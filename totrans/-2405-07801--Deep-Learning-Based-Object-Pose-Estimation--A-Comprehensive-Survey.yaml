- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:32:49'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2405.07801] Deep Learning-Based Object Pose Estimation: A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2405.07801](https://ar5iv.labs.arxiv.org/html/2405.07801)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning-Based Object Pose Estimation:'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Comprehensive Survey
  prefs: []
  type: TYPE_NORMAL
- en: Jian Liu, Wei Sun, Hui Yang, Zhiwen Zeng, Chongpei Liu, Jin Zheng, Xingyu Liu
  prefs: []
  type: TYPE_NORMAL
- en: 'Hossein Rahmani, Nicu Sebe, , and Ajmal Mian Jian Liu, Wei Sun, Hui Yang, Zhiwen
    Zeng, and Chongpei Liu are with the National Engineering Research Center for Robot
    Visual Perception and Control Technology, College of Electrical and Information
    Engineering, and the State Key Laboratory of Advanced Design and Manufacturing
    for Vehicle Body, Hunan University, Changsha 410082, China. E-mail: (jianliu,
    wei_sun, huiyang, zingaltern, chongpei56)@hnu.edu.cn Jin Zheng is with the School
    of Architecture and Art, Central South University, Changsha, 410082, China. E-mail:
    zheng.jin@csu.edu.cn Xingyu Liu is with the Department of Automation, Tsinghua
    University, Beijing 100084, China. E-mail: liuxy21@mails.tsinghua.edu.cn Hossein
    Rahmani is with the School of Computing and Communications, Lancaster University,
    LA1 4YW, United Kingdom. E-mail: h.rahmani@lancaster.ac.uk Nicu Sebe is with the
    Department of Information Engineering and Computer Science, University of Trento,
    Trento 38123, Italy. E-mail: sebe@disi.unitn.it Ajmal Mian is with the Department
    of Computer Science, The University of Western Australia, WA 6009, Australia.
    E-mail: ajmal.mian@uwa.edu.au This work was done while Jian Liu was a visiting
    Ph.D. student with The University of Western Australia, supervised by Prof. Ajmal
    Mian, and Chongpei Liu was a visiting Ph.D. student with the University of Trento,
    supervised by Prof. Nicu Sebe.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Object pose estimation is a fundamental computer vision problem with broad applications
    in augmented reality and robotics. Over the past decade, deep learning models,
    due to their superior accuracy and robustness, have increasingly supplanted conventional
    algorithms reliant on engineered point pair features. Nevertheless, several challenges
    persist in contemporary methods, including their dependency on labeled training
    data, model compactness, robustness under challenging conditions, and their ability
    to generalize to novel unseen objects. A recent survey discussing the progress
    made on different aspects of this area, outstanding challenges, and promising
    future directions, is missing. To fill this gap, we discuss the recent advances
    in deep learning-based object pose estimation, covering all three formulations
    of the problem, *i.e.*, instance-level, category-level, and unseen object pose
    estimation. Our survey also covers multiple input data modalities, degrees-of-freedom
    of output poses, object properties, and downstream tasks, providing the readers
    with a holistic understanding of this field. Additionally, it discusses training
    paradigms of different domains, inference modes, application areas, evaluation
    metrics, and benchmark datasets, as well as reports the performance of current
    state-of-the-art methods on these benchmarks, thereby facilitating the readers
    in selecting the most suitable method for their application. Finally, the survey
    identifies key challenges, reviews the prevailing trends along with their pros
    and cons, and identifies promising directions for future research. We also keep
    tracing the latest works at [Awesome-Object-Pose-Estimation](https://github.com/CNJianLiu/Awesome-Object-Pose-Estimation).
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Object pose estimation, deep learning, comprehensive survey, 3D computer vision.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object pose estimation is a fundamental computer vision problem that aims to
    estimate the pose of an object in a given image relative to the camera that captured
    the image. Object pose estimation is a crucial technology for augmented reality[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)], robotic manipulation[[4](#bib.bib4), [5](#bib.bib5)],
    hand-object interaction[[6](#bib.bib6), [7](#bib.bib7)], etc. Depending on the
    application needs, the object pose is estimated up to varying degrees of freedom
    (DoF) such as 3DoF that only includes 3D rotation, 6DoF that additionally includes
    3D translation, or 9DoF which includes estimating the 3D size of the object besides
    the 3D rotation and 3D translation.
  prefs: []
  type: TYPE_NORMAL
- en: In the pre-deep learning era, many hand-crafted feature-based approaches such
    as SIFT[[8](#bib.bib8)], FPFH[[9](#bib.bib9)], VFH[[10](#bib.bib10)], and Point
    Pair Features (PPF)[[11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13), [14](#bib.bib14)]
    were designed for object pose estimation. However, these methods exhibit deficiencies
    in accuracy and robustness when confronted with complex scenes[[15](#bib.bib15),
    [16](#bib.bib16)]. These traditional methods have now been supplanted by data
    driven deep learning-based approaches that harness the power of deep neural networks
    to learn high-dimensional feature representations from data, leading to improved
    accuracy and robustness to handle complex environments.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b04f726d0d23afc45a36714d9f34d626.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison of instance-level, category-level, and unseen object methods.
    Instance-level methods can only estimate the pose of specific object instances
    on which they are trained. Category-level methods can infer intra-class unseen
    instances rather than being limited to specific instances in the training data.
    In contrast, unseen object pose estimation methods have stronger generalization
    ability and can handle object categories not encountered during training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/78b5cf433b76a024efaac33c3a18c2aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: A taxonomy of this survey. Firstly, we review the datasets and evaluation
    metrics used to evaluate object pose estimation. Next, we review the deep learning-based
    methods by dividing them into three categories: instance-level, category-level,
    and unseen methods. Instance-level methods can be further classified into correspondence-based,
    template-based, voting-based, and regression-based methods. Category-level methods
    can be further divided into shape prior-based and shape prior-free methods. Unseen
    methods can be further classified into CAD model-based and manual reference view-based
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning-based object pose estimation methods can be divided into instance-level,
    category-level, and unseen object methods according to the problem formulation.
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey") shows a comparison of the three methods. Early methods
    were mainly instance-level [[16](#bib.bib16), [17](#bib.bib17), [18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20)], trained to estimate the pose of specific
    object instances. Instance-level methods can be further divided into correspondence-based,
    template-based, voting-based, and regression-based methods. Since instance-level
    methods are trained on instance-specific data, they can estimate pose with high
    precision for the given object instances. However, their generalization performance
    is poor because they are meant to be applied only to the instances on which they
    are trained. Moreover, many instance-level methods[[18](#bib.bib18), [21](#bib.bib21)]
    require CAD models of the objects. Recognizing these limitations, Wang *et al.*
    [[22](#bib.bib22)] proposed the first category-level object pose and size estimation
    method. They generalize to intra-class unseen objects without necessitating retraining
    and employing CAD models during inference. Subsequent category-level methods [[23](#bib.bib23),
    [24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26), [27](#bib.bib27)] can be
    divided into shape prior-based and shape prior-free methods. While improving the
    generalization ability within a category, these category-level methods still need
    to collect and label extensive training data for each object category. Moreover,
    these methods cannot generalize to unseen object categories. To this end, some
    unseen object pose estimation methods have been recently proposed [[1](#bib.bib1),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [3](#bib.bib3)], which can
    be further classified into CAD model-based and manual reference view-based methods.
    These methods further enhance the generalization of object pose estimation, *i.e.*,
    they can be generalized to unseen objects without retraining. Nevertheless, they
    still need to obtain the object CAD model or annotate a few reference images of
    the object.'
  prefs: []
  type: TYPE_NORMAL
- en: Although significant progress has been made in the area of object pose estimation,
    several challenges persist in current methods, such as the reliance on labeled
    training data, difficulty in generalizing to novel unseen objects, model compactness,
    and robustness in challenging scenarios. To enable readers to swiftly grasp the
    current state-of-the-art (SOTA) in object pose estimation and facilitate further
    research in this direction, it is crucial to provide a thorough review of all
    the relevant problem formulations. A close examination of the existing academic
    literature reveals a significant gap when reviewing the various problem formulations
    in object pose estimation. Current prevailing reviews [[31](#bib.bib31), [32](#bib.bib32),
    [33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)] tend to exhibit a narrow
    focus, either confined to particular input modalities[[32](#bib.bib32), [33](#bib.bib33)]
    or tethered to specific application domains[[34](#bib.bib34), [35](#bib.bib35)].
    Furthermore, these reviews predominantly scrutinize instance-level and category-level
    methods, thus neglecting the exploration of the most practical problem formulation
    in the domain which is unseen object pose estimation. This hinders readers from
    gaining a comprehensive understanding of the area. For instance, Fan *et al.*
    [[33](#bib.bib33)] provided valuable insights into RGB image-based object pose
    estimation. However, their focus is limited to a singular modality, hindering
    readers from comprehensively understanding methods across various input modalities.
    Conversely, Du *et al.* [[34](#bib.bib34)] exclusively examined object pose estimation
    within the context of the robotic grasping task, which limits the readers to understand
    object pose estimation only from the perspective of a single specific application.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the above problems, we present here a comprehensive survey of recent
    advancements in deep learning-based methods for object pose estimation. Our survey
    encompasses all problem formulations, including instance-level, category-level,
    and unseen object pose estimation, aiming to provide readers with a holistic understanding
    of this field. Additionally, we discuss different domain training paradigms, application
    areas, evaluation metrics, and benchmark datasets, as well as report the performance
    of state-of-the-art methods on these benchmarks, aiding readers in selecting suitable
    methods for their applications. Furthermore, we also highlight prevailing trends
    and discuss their strengths and weaknesses, as well as identify key challenges
    and promising avenues for future research. The taxonomy of this survey is shown
    in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Deep Learning-Based Object Pose
    Estimation: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our main contributions and highlights are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present a *comprehensive survey* of deep learning-based object pose estimation
    methods. This is the *first* survey that covers all three problem formulations
    in the domain, including instance-level, category-level, and unseen object pose
    estimation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our survey covers popular input data modalities (RGB images, depth images, RGBD
    images), the different degrees of freedom (3DoF, 6DoF, 9DoF) in output poses,
    object properties (rigid, articulated) for the task of pose estimation as well
    as tracking. It is crucial to cover all these aspects in a single survey to give
    a complete picture to readers, an aspect overlooked by existing surveys which
    only cover a few of these aspects.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discuss different domain training paradigms, inference modes, application
    areas, evaluation metrics, and benchmark datasets as well as report the performance
    of existing SOTA methods on these benchmarks to help readers choose the most appropriate
    ones for deployment in their application.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We highlight popular trends in the evolution of object pose estimation techniques
    over the past decade and discuss their strengths and weaknesses. We also identify
    key challenges that are still outstanding in object pose estimation along with
    promising research directions to guide future efforts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of this article is organized as follows. Sec. [2](#S2 "2 Datasets
    and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")
    reviews the datasets and metrics used to evaluate the three categories of object
    pose estimation methods. We then review instance-level methods in Sec. [3](#S3
    "3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"), category-level methods in Sec. [4](#S4 "4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"), and unseen object pose estimation methods in Sec. [5](#S5 "5 Unseen
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"). In the aforementioned three sections, we also discuss the training paradigms,
    inference modes, challenges, and popular trends associated with representative
    methods in the particular category. Next, Sec. [6](#S6 "6 Applications ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey") reviews the common
    applications of object pose estimation. Finally, Sec. [7](#S7 "7 Conclusion and
    Future Direction ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey") summarizes this article and provides an outlook on future research directions
    based on the challenges in the field.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Datasets and Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The advancement of deep learning-based object pose estimation is closely linked
    to the creation and utilization of challenging and trustworthy large-scale datasets.
    This section introduces commonly used mainstream object pose estimation datasets,
    categorized into instance-level, category-level, and unseen object pose estimation
    methods based on problem formulation. The chronological overview is shown in Fig.
    [3](#S2.F3 "Figure 3 ‣ 2.1.2 Other Datasets ‣ 2.1 Datasets for Instance-Level
    Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"). In addition, we also conduct an overview of the related
    evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Datasets for Instance-Level Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the BOP Challenge datasets[[36](#bib.bib36)] are currently the most popular
    datasets for the evaluation of instance-level methods, we divide the instance-level
    datasets into BOP Challenge and other datasets for overview.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1 BOP Challenge Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Linemod Dataset (LM)[[37](#bib.bib37)] comprises 15 RGBD sequences containing
    annotated RGBD images with ground-truth 6DoF object poses, object CAD models,
    2D bounding boxes, and binary masks. Typically following Brachmann *et al.*[[38](#bib.bib38)],
    approximately 15$\%$ of images from each sequence are allocated for training,
    with the remaining 85$\%$ reserved for testing. These sequences present challenging
    scenarios with cluttered scenes, texture-less objects, and varying lighting conditions,
    making accurate object pose estimation difficult.
  prefs: []
  type: TYPE_NORMAL
- en: Linemod Occlusion Dataset (LM-O)[[39](#bib.bib39)] is an extension of the LM
    dataset[[37](#bib.bib37)] specifically designed to evaluate the performance in
    occlusion scenarios. This dataset consists of 1214 RGBD images from the basic
    sequence in the LM dataset for 8 heavily occluded objects. It is critical for
    evaluating and improving pose estimation algorithms in complex environments characterized
    by occlusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'IC-MI [[40](#bib.bib40)] / IC-BIN Dataset [[41](#bib.bib41)] contribute to
    texture-less object pose estimation. IC-MI comprises six objects: 2 texture-less
    and 4 textured household item models. IC-BIN dataset is specifically designed
    to address challenges posed by clutter and occlusion in robot garbage bin picking
    scenarios. IC-BIN includes 2 objects from the IC-MI.'
  prefs: []
  type: TYPE_NORMAL
- en: RU-APC Dataset [[42](#bib.bib42)] aims to tackle challenges in warehouse picking
    tasks and provides rich data for evaluating and improving the perception capabilities
    of robots in a warehouse automation context. The dataset comprises 10,368 registered
    depth and RGB images, covering 24 types of objects, which are placed in various
    poses within different boxes on warehouse shelves to simulate diverse experimental
    conditions.
  prefs: []
  type: TYPE_NORMAL
- en: YCB-Video Dataset (YCB-V)[[15](#bib.bib15)] comprises 21 objects distributed
    across 92 RGBD videos, each video containing 3 to 9 objects from the YCB object
    dataset[[43](#bib.bib43)] (totaling 50 objects). It includes 133,827 frames with
    a resolution of 640$\times$480, making it well-suited for both object pose estimation
    and tracking tasks.
  prefs: []
  type: TYPE_NORMAL
- en: T-LESS Dataset [[44](#bib.bib44)] is an RGBD dataset designed for texture-less
    objects commonly found in industrial settings. It includes 30 electrical objects
    with no obvious texture or distinguishable color properties. In addition, it includes
    images of varying resolutions. In the training set, images predominantly feature
    black backgrounds, while the test set showcases diverse backgrounds with varying
    lighting conditions and occlusions. T-LESS is challenging because of the absence
    of texture on objects and the intricate environmental settings.
  prefs: []
  type: TYPE_NORMAL
- en: ITODD Dataset [[45](#bib.bib45)] includes 28 real-world industrial objects distributed
    across over 800 scenes with around 3,500 images. This dataset leverages two industrial
    3D sensors and three high-resolution grayscale cameras to enable multi-angle observation
    of the scenes, providing comprehensive and detailed data for industrial object
    analysis and evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: TYO-L / TUD-L Dataset [[36](#bib.bib36)] focus on different lighting conditions.
    Specifically, TYO-L provides observation of 3 objects under 8 lighting conditions.
    These scenes are designed to evaluate the robustness of pose estimation algorithms
    to lighting variations. Unlike TYO-L, the data collection method of TUD-L involves
    fixing the camera and manually moving the object, providing a more realistic representation
    of the object’s physical movement.
  prefs: []
  type: TYPE_NORMAL
- en: HB Dataset [[46](#bib.bib46)] covers various scenes with changes in occlusion
    and lighting conditions. It comprises 33 objects, including 17 toys, 8 household
    items, and 8 industry-related objects, distributed across 13 diverse scenes.
  prefs: []
  type: TYPE_NORMAL
- en: HOPE Dataset [[47](#bib.bib47)] is specifically designed for household objects,
    containing 28 toy grocery objects. The HOPE-Image dataset includes objects from
    50 scenes across 10 home/office environments. Each scene includes up to 5 lighting
    variations, such as backlit and obliquely directed lighting, with shadow-casting
    effects. Additionally, the HOPE-Video dataset comprises 10 video sequences totaling
    2,038 frames, with each scene showcasing between 5 to 20 objects.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Other Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'YCBInEOAT Dataset [[48](#bib.bib48)] is designed for RGBD-based object pose
    tracking in robotic manipulation. It contains the ego-centric RGBD videos of a
    dual-arm robot manipulating the YCB objects [[43](#bib.bib43)]. There are 3 types
    of manipulation: single-arm pick-and-place, within-arm manipulation, and pick-and-place
    between arms. This dataset comprises annotations of ground-truth poses across
    7449 frames, encompassing 5 distinct objects depicted in 9 videos.'
  prefs: []
  type: TYPE_NORMAL
- en: ClearPose Dataset [[49](#bib.bib49)] is designed for transparent objects, which
    are widely prevalent in daily life, presenting significant challenges to visual
    perception and sensing systems due to their indistinct texture features and unreliable
    depth information. It encompasses over 350K real-world RGBD images and 5M instance
    annotations across 63 household objects.
  prefs: []
  type: TYPE_NORMAL
- en: MP6D Dataset [[50](#bib.bib50)] is an RGBD dataset designed for object pose
    estimation of metal parts, featuring 20 texture-less metal components. It consists
    of 20,100 real-world images with object pose labels collected from various scenarios
    as well as 50K synthetic images, encompassing cluttered and occluded scenes.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c77fd5515c61bcbbfd96c093997a7567.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Chronological overview of the datasets for object pose estimation
    evaluation. Notably, the pink arrows represent the BOP Challenge datasets, which
    can be used to evaluate both instance-level and unseen object methods. The red
    references represent the datasets of articulated objects. From this, we can also
    see the development trend in the field of object pose estimation, *i.e.*, from
    instance-level methods to category-level and unseen methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Datasets for Category-Level Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this part, we divide the category-level datasets into rigid and articulated
    object datasets for elaboration.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1 Rigid Objects Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'CAMERA25 Dataset [[22](#bib.bib22)] incorporates 1085 instances across 6 object
    categories: bowl, bottle, can, camera, mug, and laptop. Notably, the object CAD
    models in CAMERA25 are sourced from the synthetic ShapeNet dataset [[51](#bib.bib51)].
    Each image within this dataset contains multiple instances, accompanied by segmentation
    masks and 9DoF pose labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'REAL275 Dataset [[22](#bib.bib22)] is a real-world dataset comprising 18 videos
    and approximately 8K RGBD images. The dataset is divided into three subsets: a
    training set (7 videos), a validation set (5 videos), and a testing set (6 videos).
    It includes 42 object instances across 6 categories, consistent with those in
    the CAMERA25 dataset. REAL275 is a prominent real-world dataset extensively used
    for category-level object pose estimation in academic research.'
  prefs: []
  type: TYPE_NORMAL
- en: kPAM Dataset [[52](#bib.bib52)] is tailored specifically for robotic applications,
    emphasizing the use of keypoints. Notably, it adopts a methodology involving 3D
    reconstruction followed by manual keypoint annotation on these reconstructions.
    With a total of 117 training sequences and 245 testing sequences, kPAM offers
    a substantial collection of data for training and evaluating algorithms related
    to robotic perception and manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: TOD Dataset [[53](#bib.bib53)] consists of 15 transparent objects categorized
    into 6 classes, each annotated with pertinent 3D keypoints. It encompasses a vast
    collection of 48K stereo and RGBD images capturing both transparent and opaque
    depth variations. The primary focus of the TOD dataset is on transparent 3D object
    applications, providing essential resources for tasks such as object detection
    and pose estimation in challenging scenarios involving transparency.
  prefs: []
  type: TYPE_NORMAL
- en: Objectron Dataset [[54](#bib.bib54)] contains 15K annotated video clips with
    over 4M labeled images belonging to categories of bottles, books, bikes, cameras,
    chairs, cereal boxes, cups, laptops, and shoes. This dataset is sourced from 10
    countries spanning 5 continents, ensuring diverse geographic representation. Due
    to its extensive content, it is highly advantageous for evaluating the RGB-based
    category-level object pose estimation and tracking methods.
  prefs: []
  type: TYPE_NORMAL
- en: Wild6D Dataset [[55](#bib.bib55)] is a substantial real-world dataset used to
    assess self-supervised category-level object pose estimation methods. It offers
    annotations exclusively for 486 test videos with diverse backgrounds, showcasing
    162 objects across 5 categories (excluding the ”can” category found in CAMERA25
    and REAL275).
  prefs: []
  type: TYPE_NORMAL
- en: PhoCaL Dataset [[56](#bib.bib56)] incorporates both RGBD and RGB-P (Polarisation)
    modalities. It consists of 60 meticulously crafted 3D models representing household
    objects, including symmetric, transparent, and reflective items. PhoCaL focuses
    on 8 specific object categories across 24 sequences, deliberately introducing
    challenges such as occlusion and clutter.
  prefs: []
  type: TYPE_NORMAL
- en: HouseCat6D Dataset [[57](#bib.bib57)] is a comprehensive dataset designed for
    multi-modal category-level object pose estimation and grasping tasks. The dataset
    encompasses a wide range of household object categories, featuring 194 high-quality
    3D models. It includes objects of varying photometric complexity, such as transparent
    and reflective items, and spans 41 scenes with diverse viewpoints. The dataset
    is specifically curated to address challenges in object pose estimation, including
    occlusions and the absence of markers, making it suitable for evaluating algorithms
    under real-world conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Articulated Objects Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'BMVC Dataset [[58](#bib.bib58)] includes 4 articulated objects: laptop, cabinet,
    cupboard, and toy train. Each object is modeled as a motion chain comprising components
    and interconnected heads. Joints are constrained to one rotational and one translational
    DoF. This dataset provides CAD models and accompanying text files detailing the
    topology of the underlying motion chain structure for each object.'
  prefs: []
  type: TYPE_NORMAL
- en: RBO Dataset [[59](#bib.bib59)] contains 14 commonly found articulated objects
    in human environments, with 358 interaction sequences, resulting in a total of
    67 minutes of manual manipulation under different experimental conditions, including
    changes in interaction type, lighting, viewpoint, and background settings.
  prefs: []
  type: TYPE_NORMAL
- en: HOI4D Dataset [[60](#bib.bib60)] is pivotal for advancing research in category-level
    human-object interactions. It comprises 2.4M RGBD self-centered video frames depicting
    interactions between over 9 participants and 800 object instances. These instances
    are divided into 16 categories, including 7 rigid and 9 articulated objects.
  prefs: []
  type: TYPE_NORMAL
- en: ReArtMix / ReArtVal Datasets [[61](#bib.bib61)] are formulated to tackle the
    challenge of partial-level multiple articulated objects pose estimation featuring
    unknown kinematic structures. The ReArtMix dataset encompasses over 100,000 RGBD
    images rendered against diverse background scenes. The ReArtVal dataset consists
    of 6 real-world desktop scenes comprising over 6,000 RGBD frames.
  prefs: []
  type: TYPE_NORMAL
- en: 'ContactArt Dataset [[62](#bib.bib62)] is generated using a remote operating
    system [[63](#bib.bib63)] to manipulate articulated objects in a simulation environment.
    This system utilizes smartphones and laptops to precisely annotate poses and contact
    information. This dataset contains 5 prevalent categories of articulated objects:
    laptops, drawers, safes, microwaves, and trash cans, for a total of 80 instances.
    All object models are sourced from the PartNet dataset[[64](#bib.bib64)], thus
    promoting scalability.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Datasets for Unseen Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The current mainstream datasets for evaluating unseen methods are the BOP Challenge
    datasets, as discussed in Sec. [2.1.1](#S2.SS1.SSS1 "2.1.1 BOP Challenge Datasets
    ‣ 2.1 Datasets for Instance-Level Methods ‣ 2 Datasets and Metrics ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey"). Besides these BOP Challenge
    datasets, there are also some datasets designed for evaluating manual reference
    view-based methods as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: MOPED Dataset [[65](#bib.bib65)] is a model-free object pose estimation dataset
    featuring 11 household objects. It includes reference and test images that encompass
    all views of the objects. Each object in the test sequences is depicted in five
    distinct environments, with approximately 300 test images per object.
  prefs: []
  type: TYPE_NORMAL
- en: GenMOP Dataset [[1](#bib.bib1)] includes 10 objects ranging from flat objects
    to thin structure objects. For each object, there are two video sequences collected
    from various backgrounds and lighting situations. Each video sequence consists
    of approximately 200 images.
  prefs: []
  type: TYPE_NORMAL
- en: OnePose Dataset [[66](#bib.bib66)] comprises over 450 real-world video sequences
    of 150 objects. These sequences are collected in a variety of background conditions
    and capture all angles of the objects. Each environment has an average duration
    of 30 seconds. The dataset is randomly partitioned into training and validation
    sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'OnePose-LowTexture Dataset [[2](#bib.bib2)] is introduced as a complement to
    the testing set of the existing OnePose dataset[[66](#bib.bib66)], which predominantly
    features textured objects. This dataset comprises 40 household objects with low
    texture. For each object, there are two video sequences: one serving as the reference
    video and the other for testing. Each video is captured at a resolution of 1920$\times$1440,
    30 Frames Per Second (FPS), and approximately 30 seconds in duration.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this part, we divide the metrics into 3DoF, 6DoF, 9DoF, and other evaluation
    metrics for a comprehensive overview.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 3DoF Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The geodesic distance [[67](#bib.bib67)] between the ground-truth and predicted
    3D rotations is a commonly used 3DoF pose estimation metric. Calculating the angle
    error between two rotation matrices can visually evaluate their relative deviation.
    It can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{array}[]{l}d\left({R_{gt}},R\right)=\arccos\left(\frac{{tr}\left({R_{gt}}^{\top}R\right)-1}{2}\right)/\pi\end{array},\vspace{-0.5em}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where ${R_{gt}}$ and $R$ denote the ground-truth and predicted 3D rotations,
    respectively. $\top$ represents matrix transpose. $tr$ denotes the trace of a
    matrix, which refers to the sum of the elements on the main diagonal. Typically,
    3D rotation estimation accuracy is defined as the percentage of objects whose
    angle error is below a specific threshold and whose predicted class is correct.
    It can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E2.m1.4" class="ltx_math_unparsed" alttext="Acc.=\left\{{\begin{array}[]{*{20}{c}}{1,}\\
    {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{d({R_{gt}},R)<\lambda\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}c=c_{gt}}\end{array}}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '{{\rm{otherwise}}}\end{array}," display="block"><semantics id="S2.E2.m1.4a"><mrow
    id="S2.E2.m1.4b"><mi id="S2.E2.m1.4.5">A</mi><mi id="S2.E2.m1.4.6">c</mi><mi id="S2.E2.m1.4.7">c</mi><mo
    lspace="0em" rspace="0.0835em" id="S2.E2.m1.4.8">.</mo><mo lspace="0.0835em" id="S2.E2.m1.4.9">=</mo><mrow
    id="S2.E2.m1.4.10"><mo id="S2.E2.m1.4.10.1">{</mo><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" id="S2.E2.m1.2.2"><mtr id="S2.E2.m1.2.2a"><mtd
    id="S2.E2.m1.2.2b"><mrow id="S2.E2.m1.1.1.1.1.1.3"><mn id="S2.E2.m1.1.1.1.1.1.1">1</mn><mo
    id="S2.E2.m1.1.1.1.1.1.3.1">,</mo></mrow></mtd></mtr><mtr id="S2.E2.m1.2.2v"><mtd
    id="S2.E2.m1.2.2w"><mrow id="S2.E2.m1.2.2.2.1.1.3"><mn id="S2.E2.m1.2.2.2.1.1.1">0</mn><mo
    id="S2.E2.m1.2.2.2.1.1.3.1">,</mo></mrow></mtd></mtr></mtable><mtable columnspacing="5pt"
    displaystyle="true" rowspacing="0pt" id="S2.E2.m1.4.4"><mtr id="S2.E2.m1.4.4a"><mtd
    id="S2.E2.m1.4.4b"><mrow id="S2.E2.m1.4.4.2.2.2"><mi id="S2.E2.m1.4.4.2.2.2.4">if</mi><mo
    lspace="0.167em" rspace="0em" id="S2.E2.m1.4.4.2.2.2.3">​</mo><mtable columnspacing="5pt"
    displaystyle="true" id="S2.E2.m1.4.4.2.2.2.2"><mtr id="S2.E2.m1.4.4.2.2.2.2a"><mtd
    id="S2.E2.m1.4.4.2.2.2.2b"><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2"><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2"><mi
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.3">d</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.2">​</mo><mrow
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1"><mo stretchy="false" id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.2">(</mo><msub
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.2">R</mi><mrow
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.3"><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.3.2">g</mi><mo
    lspace="0em" rspace="0em" id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.3.1">​</mo><mi
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.1.3.3">t</mi></mrow></msub><mo id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.3">,</mo><mi
    id="S2.E2.m1.3.3.1.1.1.1.1.1.1.1">R</mi><mo stretchy="false" id="S2.E2.m1.4.4.2.2.2.2.2.2.2.2.1.1.4">)</mo></mrow></mrow><mo
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.4"><</mo><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5"><mi
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.2">λ</mi><mo lspace="0.167em" rspace="0em" id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.1">​</mo><mtable
    columnspacing="5pt" displaystyle="true" id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3"><mtr
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3a"><mtd id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3b"><mi
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.3.1.1.1">and</mi></mtd></mtr></mtable><mo lspace="0.167em"
    rspace="0em" id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.1a">​</mo><mi id="S2.E2.m1.4.4.2.2.2.2.2.2.2.5.4">c</mi></mrow><mo
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.6">=</mo><msub id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7"><mi
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.2">c</mi><mrow id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.3"><mi
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.3.2">g</mi><mo lspace="0em" rspace="0em" id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.3.1">​</mo><mi
    id="S2.E2.m1.4.4.2.2.2.2.2.2.2.7.3.3">t</mi></mrow></msub></mrow></mtd></mtr></mtable></mrow></mtd></mtr><mtr
    id="S2.E2.m1.4.4v"><mtd id="S2.E2.m1.4.4w"><mi id="S2.E2.m1.4.4.3.1.1">otherwise</mi></mtd></mtr></mtable><mo
    lspace="0.167em" id="S2.E2.m1.4.10.2">,</mo></mrow></mrow><annotation encoding="application/x-tex"
    id="S2.E2.m1.4c">Acc.=\left\{{\begin{array}[]{*{20}{c}}{1,}\\ {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{d({R_{gt}},R)<\lambda\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}c=c_{gt}}\end{array}}\\
    {{\rm{otherwise}}}\end{array},</annotation></semantics></math> |  | (2) |'
  prefs: []
  type: TYPE_NORMAL
- en: where $c$, $c_{gt}$, and $\lambda$ denote the predicted class, ground-truth
    class and predefined threshold, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 6DoF Evaluation Metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Currently, the BOP metric (*BOP-M*)[[36](#bib.bib36)] is the most popular metric,
    which is the Average Recall (*AR*) of the Visible Surface Discrepancy (VSD), Maximum
    Symmetry-Aware Surface Distance (MSSD), and Maximum Symmetry-Aware Projection
    Distance (MSPD) metrics. Specifically, the VSD [[36](#bib.bib36)] metric treats
    poses that are indistinguishable in shape as equivalent by only measuring the
    misalignment of the visible object surface. It can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E3.m1.11" class="ltx_Math" alttext="\begin{array}[]{l}{e_{VSD}}\left({\hat{D},\bar{D},\hat{V},\bar{V},\tau}\right)=\\
    av{g_{p\in\hat{V}\cup\bar{V}}}\left\{{\begin{array}[]{*{20}{l}}{0,}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '{1,}\end{array}}\right.\begin{array}[]{*{20}{l}}{{\rm{if}}\begin{array}[]{*{20}{l}}{p\in\hat{V}\cap\bar{V}\wedge&#124;\hat{D}\left(p\right)-\bar{D}\left(p\right)&#124;<\tau}\end{array}}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '{{\rm{otherwise}}}\end{array},\end{array}\vspace{-0.5em}" display="block"><semantics
    id="S2.E3.m1.11a"><mtable displaystyle="true" rowspacing="0pt" id="S2.E3.m1.11.11"
    xref="S2.E3.m1.11.11.cmml"><mtr id="S2.E3.m1.11.11a" xref="S2.E3.m1.11.11.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S2.E3.m1.11.11b" xref="S2.E3.m1.11.11.cmml"><mrow
    id="S2.E3.m1.5.5.5.5.5" xref="S2.E3.m1.5.5.5.5.5.cmml"><mrow id="S2.E3.m1.5.5.5.5.5.7"
    xref="S2.E3.m1.5.5.5.5.5.7.cmml"><msub id="S2.E3.m1.5.5.5.5.5.7.2" xref="S2.E3.m1.5.5.5.5.5.7.2.cmml"><mi
    id="S2.E3.m1.5.5.5.5.5.7.2.2" xref="S2.E3.m1.5.5.5.5.5.7.2.2.cmml">e</mi><mrow
    id="S2.E3.m1.5.5.5.5.5.7.2.3" xref="S2.E3.m1.5.5.5.5.5.7.2.3.cmml"><mi id="S2.E3.m1.5.5.5.5.5.7.2.3.2"
    xref="S2.E3.m1.5.5.5.5.5.7.2.3.2.cmml">V</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.5.5.5.7.2.3.1"
    xref="S2.E3.m1.5.5.5.5.5.7.2.3.1.cmml">​</mo><mi id="S2.E3.m1.5.5.5.5.5.7.2.3.3"
    xref="S2.E3.m1.5.5.5.5.5.7.2.3.3.cmml">S</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.5.5.5.5.5.7.2.3.1a"
    xref="S2.E3.m1.5.5.5.5.5.7.2.3.1.cmml">​</mo><mi id="S2.E3.m1.5.5.5.5.5.7.2.3.4"
    xref="S2.E3.m1.5.5.5.5.5.7.2.3.4.cmml">D</mi></mrow></msub><mo lspace="0em" rspace="0em"
    id="S2.E3.m1.5.5.5.5.5.7.1" xref="S2.E3.m1.5.5.5.5.5.7.1.cmml">​</mo><mrow id="S2.E3.m1.5.5.5.5.5.7.3.2"
    xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml"><mo id="S2.E3.m1.5.5.5.5.5.7.3.2.1" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">(</mo><mover
    accent="true" id="S2.E3.m1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.cmml"><mi id="S2.E3.m1.1.1.1.1.1.1.2"
    xref="S2.E3.m1.1.1.1.1.1.1.2.cmml">D</mi><mo id="S2.E3.m1.1.1.1.1.1.1.1" xref="S2.E3.m1.1.1.1.1.1.1.1.cmml">^</mo></mover><mo
    id="S2.E3.m1.5.5.5.5.5.7.3.2.2" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">,</mo><mover
    accent="true" id="S2.E3.m1.2.2.2.2.2.2" xref="S2.E3.m1.2.2.2.2.2.2.cmml"><mi id="S2.E3.m1.2.2.2.2.2.2.2"
    xref="S2.E3.m1.2.2.2.2.2.2.2.cmml">D</mi><mo id="S2.E3.m1.2.2.2.2.2.2.1" xref="S2.E3.m1.2.2.2.2.2.2.1.cmml">¯</mo></mover><mo
    id="S2.E3.m1.5.5.5.5.5.7.3.2.3" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">,</mo><mover
    accent="true" id="S2.E3.m1.3.3.3.3.3.3" xref="S2.E3.m1.3.3.3.3.3.3.cmml"><mi id="S2.E3.m1.3.3.3.3.3.3.2"
    xref="S2.E3.m1.3.3.3.3.3.3.2.cmml">V</mi><mo id="S2.E3.m1.3.3.3.3.3.3.1" xref="S2.E3.m1.3.3.3.3.3.3.1.cmml">^</mo></mover><mo
    id="S2.E3.m1.5.5.5.5.5.7.3.2.4" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">,</mo><mover
    accent="true" id="S2.E3.m1.4.4.4.4.4.4" xref="S2.E3.m1.4.4.4.4.4.4.cmml"><mi id="S2.E3.m1.4.4.4.4.4.4.2"
    xref="S2.E3.m1.4.4.4.4.4.4.2.cmml">V</mi><mo id="S2.E3.m1.4.4.4.4.4.4.1" xref="S2.E3.m1.4.4.4.4.4.4.1.cmml">¯</mo></mover><mo
    id="S2.E3.m1.5.5.5.5.5.7.3.2.5" xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">,</mo><mi
    id="S2.E3.m1.5.5.5.5.5.5" xref="S2.E3.m1.5.5.5.5.5.5.cmml">τ</mi><mo id="S2.E3.m1.5.5.5.5.5.7.3.2.6"
    xref="S2.E3.m1.5.5.5.5.5.7.3.1.cmml">)</mo></mrow></mrow><mo id="S2.E3.m1.5.5.5.5.5.6"
    xref="S2.E3.m1.5.5.5.5.5.6.cmml">=</mo></mrow></mtd></mtr><mtr id="S2.E3.m1.11.11c"
    xref="S2.E3.m1.11.11.cmml"><mtd class="ltx_align_left" columnalign="left" id="S2.E3.m1.11.11d"
    xref="S2.E3.m1.11.11.cmml"><mrow id="S2.E3.m1.11.11.11.6.6.6" xref="S2.E3.m1.11.11.11.6.6.6.1.cmml"><mrow
    id="S2.E3.m1.11.11.11.6.6.6.1" xref="S2.E3.m1.11.11.11.6.6.6.1.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.2"
    xref="S2.E3.m1.11.11.11.6.6.6.1.2.cmml">a</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.11.11.11.6.6.6.1.1"
    xref="S2.E3.m1.11.11.11.6.6.6.1.1.cmml">​</mo><mi id="S2.E3.m1.11.11.11.6.6.6.1.3"
    xref="S2.E3.m1.11.11.11.6.6.6.1.3.cmml">v</mi><mo lspace="0em" rspace="0em" id="S2.E3.m1.11.11.11.6.6.6.1.1a"
    xref="S2.E3.m1.11.11.11.6.6.6.1.1.cmml">​</mo><msub id="S2.E3.m1.11.11.11.6.6.6.1.4"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.4.2"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.2.cmml">g</mi><mrow id="S2.E3.m1.11.11.11.6.6.6.1.4.3"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.4.3.2"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.2.cmml">p</mi><mo id="S2.E3.m1.11.11.11.6.6.6.1.4.3.1"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.1.cmml">∈</mo><mrow id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.cmml"><mover accent="true" id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.2"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.2.cmml">V</mi><mo id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.1"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.1.cmml">^</mo></mover><mo id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.1"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.1.cmml">∪</mo><mover accent="true" id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.cmml"><mi id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.2"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.2.cmml">V</mi><mo id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.1"
    xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.1.cmml">¯</mo></mover></mrow></mrow></msub><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.11.11.11.6.6.6.1.1b" xref="S2.E3.m1.11.11.11.6.6.6.1.1.cmml">​</mo><mrow
    id="S2.E3.m1.11.11.11.6.6.6.1.5.2" xref="S2.E3.m1.11.11.11.6.6.6.1.5.1.cmml"><mo
    id="S2.E3.m1.11.11.11.6.6.6.1.5.2.1" xref="S2.E3.m1.11.11.11.6.6.6.1.5.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S2.E3.m1.7.7.7.2.2.2"
    xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mtr id="S2.E3.m1.7.7.7.2.2.2a" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S2.E3.m1.7.7.7.2.2.2b" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mrow
    id="S2.E3.m1.6.6.6.1.1.1.1.1.1.3" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mn id="S2.E3.m1.6.6.6.1.1.1.1.1.1.1"
    xref="S2.E3.m1.6.6.6.1.1.1.1.1.1.1.cmml">0</mn><mo id="S2.E3.m1.6.6.6.1.1.1.1.1.1.3.1"
    xref="S2.E3.m1.7.7.7.2.2.2.cmml">,</mo></mrow></mtd></mtr><mtr id="S2.E3.m1.7.7.7.2.2.2v"
    xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mtd class="ltx_align_left" columnalign="left"
    id="S2.E3.m1.7.7.7.2.2.2w" xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mrow id="S2.E3.m1.7.7.7.2.2.2.2.1.1.3"
    xref="S2.E3.m1.7.7.7.2.2.2.cmml"><mn id="S2.E3.m1.7.7.7.2.2.2.2.1.1.1" xref="S2.E3.m1.7.7.7.2.2.2.2.1.1.1.cmml">1</mn><mo
    id="S2.E3.m1.7.7.7.2.2.2.2.1.1.3.1" xref="S2.E3.m1.7.7.7.2.2.2.cmml">,</mo></mrow></mtd></mtr></mtable></mrow><mo
    lspace="0.167em" rspace="0em" id="S2.E3.m1.11.11.11.6.6.6.1.1c" xref="S2.E3.m1.11.11.11.6.6.6.1.1.cmml">​</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S2.E3.m1.10.10.10.5.5.5"
    xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mtr id="S2.E3.m1.10.10.10.5.5.5a" xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S2.E3.m1.10.10.10.5.5.5b" xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mrow
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.cmml"><mi
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.5" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.5.cmml">if</mi><mo
    lspace="0.167em" rspace="0em" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.4" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.4.cmml">​</mo><mtable
    columnspacing="5pt" displaystyle="true" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"><mtr
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3a" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"><mtd
    class="ltx_align_left" columnalign="left" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3b"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.cmml"><mi id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.5"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.5.cmml">p</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.6"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.6.cmml">∈</mo><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.cmml"><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.cmml"><mover accent="true" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.cmml"><mi id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.2"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.2.cmml">V</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.1"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.1.cmml">^</mo></mover><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.1"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.1.cmml">∩</mo><mover accent="true"
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.cmml"><mi
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.2.cmml">V</mi><mo
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.1.cmml">¯</mo></mover></mrow><mo
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.2.cmml">∧</mo><mrow
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.2.cmml"><mo
    stretchy="false" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.2.1.cmml">&#124;</mo><mrow
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.cmml"><mrow
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml"><mover
    accent="true" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.cmml"><mi
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.2.cmml">D</mi><mo
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.1.cmml">^</mo></mover><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.1"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.1.cmml">​</mo><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.3.2"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml"><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.3.2.1"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml">(</mo><mi id="S2.E3.m1.8.8.8.3.3.3.1.1.1.1.1.1.1.1"
    xref="S2.E3.m1.8.8.8.3.3.3.1.1.1.1.1.1.1.1.cmml">p</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.3.2.2"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml">)</mo></mrow></mrow><mo
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.1.cmml">−</mo><mrow
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml"><mover
    accent="true" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.cmml"><mi
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.2" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.2.cmml">D</mi><mo
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.1" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.1.cmml">¯</mo></mover><mo
    lspace="0em" rspace="0em" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.1"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.1.cmml">​</mo><mrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.3.2"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml"><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.3.2.1"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml">(</mo><mi id="S2.E3.m1.9.9.9.4.4.4.2.2.2.2.2.2.2.2"
    xref="S2.E3.m1.9.9.9.4.4.4.2.2.2.2.2.2.2.2.cmml">p</mi><mo id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.3.2.2"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml">)</mo></mrow></mrow></mrow><mo
    stretchy="false" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.3" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.2.1.cmml">&#124;</mo></mrow></mrow><mo
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.7" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.7.cmml"><</mo><mi
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.8" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.8.cmml">τ</mi></mrow></mtd></mtr></mtable></mrow></mtd></mtr><mtr
    id="S2.E3.m1.10.10.10.5.5.5v" xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mtd class="ltx_align_left"
    columnalign="left" id="S2.E3.m1.10.10.10.5.5.5w" xref="S2.E3.m1.10.10.10.5.5.5.cmml"><mi
    id="S2.E3.m1.10.10.10.5.5.5.4.1.1" xref="S2.E3.m1.10.10.10.5.5.5.4.1.1.cmml">otherwise</mi></mtd></mtr></mtable></mrow><mo
    lspace="0.167em" id="S2.E3.m1.11.11.11.6.6.6.2" xref="S2.E3.m1.11.11.11.6.6.6.1.cmml">,</mo></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" id="S2.E3.m1.11b"><matrix id="S2.E3.m1.11.11.cmml" xref="S2.E3.m1.11.11"><matrixrow
    id="S2.E3.m1.11.11a.cmml" xref="S2.E3.m1.11.11"><apply id="S2.E3.m1.5.5.5.5.5.cmml"
    xref="S2.E3.m1.5.5.5.5.5"><apply id="S2.E3.m1.5.5.5.5.5.7.cmml" xref="S2.E3.m1.5.5.5.5.5.7"><apply
    id="S2.E3.m1.5.5.5.5.5.7.2.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.5.5.5.5.5.7.2.1.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2">subscript</csymbol><ci
    id="S2.E3.m1.5.5.5.5.5.7.2.2.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2.2">𝑒</ci><apply
    id="S2.E3.m1.5.5.5.5.5.7.2.3.cmml" xref="S2.E3.m1.5.5.5.5.5.7.2.3"><ci id="S2.E3.m1.5.5.5.5.5.7.2.3.2.cmml"
    xref="S2.E3.m1.5.5.5.5.5.7.2.3.2">𝑉</ci><ci id="S2.E3.m1.5.5.5.5.5.7.2.3.3.cmml"
    xref="S2.E3.m1.5.5.5.5.5.7.2.3.3">𝑆</ci><ci id="S2.E3.m1.5.5.5.5.5.7.2.3.4.cmml"
    xref="S2.E3.m1.5.5.5.5.5.7.2.3.4">𝐷</ci></apply></apply><vector id="S2.E3.m1.5.5.5.5.5.7.3.1.cmml"
    xref="S2.E3.m1.5.5.5.5.5.7.3.2"><apply id="S2.E3.m1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1"><ci
    id="S2.E3.m1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.1.1.1.1.1.1.1">^</ci><ci id="S2.E3.m1.1.1.1.1.1.1.2.cmml"
    xref="S2.E3.m1.1.1.1.1.1.1.2">𝐷</ci></apply><apply id="S2.E3.m1.2.2.2.2.2.2.cmml"
    xref="S2.E3.m1.2.2.2.2.2.2"><ci id="S2.E3.m1.2.2.2.2.2.2.1.cmml" xref="S2.E3.m1.2.2.2.2.2.2.1">¯</ci><ci
    id="S2.E3.m1.2.2.2.2.2.2.2.cmml" xref="S2.E3.m1.2.2.2.2.2.2.2">𝐷</ci></apply><apply
    id="S2.E3.m1.3.3.3.3.3.3.cmml" xref="S2.E3.m1.3.3.3.3.3.3"><ci id="S2.E3.m1.3.3.3.3.3.3.1.cmml"
    xref="S2.E3.m1.3.3.3.3.3.3.1">^</ci><ci id="S2.E3.m1.3.3.3.3.3.3.2.cmml" xref="S2.E3.m1.3.3.3.3.3.3.2">𝑉</ci></apply><apply
    id="S2.E3.m1.4.4.4.4.4.4.cmml" xref="S2.E3.m1.4.4.4.4.4.4"><ci id="S2.E3.m1.4.4.4.4.4.4.1.cmml"
    xref="S2.E3.m1.4.4.4.4.4.4.1">¯</ci><ci id="S2.E3.m1.4.4.4.4.4.4.2.cmml" xref="S2.E3.m1.4.4.4.4.4.4.2">𝑉</ci></apply><ci
    id="S2.E3.m1.5.5.5.5.5.5.cmml" xref="S2.E3.m1.5.5.5.5.5.5">𝜏</ci></vector></apply><csymbol
    cd="latexml" id="S2.E3.m1.5.5.5.5.5.8.cmml" xref="S2.E3.m1.5.5.5.5.5.8">absent</csymbol></apply></matrixrow><matrixrow
    id="S2.E3.m1.11.11b.cmml" xref="S2.E3.m1.11.11"><apply id="S2.E3.m1.11.11.11.6.6.6.1.cmml"
    xref="S2.E3.m1.11.11.11.6.6.6"><ci id="S2.E3.m1.11.11.11.6.6.6.1.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.2">𝑎</ci><ci
    id="S2.E3.m1.11.11.11.6.6.6.1.3.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.3">𝑣</ci><apply
    id="S2.E3.m1.11.11.11.6.6.6.1.4.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4"><csymbol
    cd="ambiguous" id="S2.E3.m1.11.11.11.6.6.6.1.4.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4">subscript</csymbol><ci
    id="S2.E3.m1.11.11.11.6.6.6.1.4.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.2">𝑔</ci><apply
    id="S2.E3.m1.11.11.11.6.6.6.1.4.3.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3"><ci
    id="S2.E3.m1.11.11.11.6.6.6.1.4.3.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.2">𝑝</ci><apply
    id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3"><apply
    id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2"><ci
    id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.1">^</ci><ci
    id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.2.2">𝑉</ci></apply><apply
    id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3"><ci
    id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.1">¯</ci><ci
    id="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.2.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.4.3.3.3.2">𝑉</ci></apply></apply></apply></apply><apply
    id="S2.E3.m1.11.11.11.6.6.6.1.5.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.5.2"><csymbol
    cd="latexml" id="S2.E3.m1.11.11.11.6.6.6.1.5.1.1.cmml" xref="S2.E3.m1.11.11.11.6.6.6.1.5.2.1">cases</csymbol><matrix
    id="S2.E3.m1.7.7.7.2.2.2.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><matrixrow id="S2.E3.m1.7.7.7.2.2.2a.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><cn type="integer" id="S2.E3.m1.6.6.6.1.1.1.1.1.1.1.cmml"
    xref="S2.E3.m1.6.6.6.1.1.1.1.1.1.1">0</cn><cerror id="S2.E3.m1.7.7.7.2.2.2b.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2c.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2d.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2e.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2f.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2g.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2h.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2i.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2j.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2k.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2l.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2m.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2n.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2o.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2p.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2q.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2r.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2s.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2t.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2u.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2v.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2w.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2x.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2y.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2z.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2aa.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2ab.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ac.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2ad.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ae.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2af.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ag.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2ah.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ai.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2aj.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2ak.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E3.m1.7.7.7.2.2.2al.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous" id="S2.E3.m1.7.7.7.2.2.2am.cmml"
    xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    id="S2.E3.m1.7.7.7.2.2.2an.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><cn type="integer"
    id="S2.E3.m1.7.7.7.2.2.2.2.1.1.1.cmml" xref="S2.E3.m1.7.7.7.2.2.2.2.1.1.1">1</cn><cerror
    id="S2.E3.m1.7.7.7.2.2.2ao.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2ap.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2aq.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2ar.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2as.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2at.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2au.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2av.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2aw.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2ax.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2ay.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2az.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2ba.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bb.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bc.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bd.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2be.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bf.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bg.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bh.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bi.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bj.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bk.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bl.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bm.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bn.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bo.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bp.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bq.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2br.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bs.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bt.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bu.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bv.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2bw.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bx.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.7.7.7.2.2.2by.cmml" xref="S2.E3.m1.7.7.7.2.2.2"><csymbol cd="ambiguous"
    id="S2.E3.m1.7.7.7.2.2.2bz.cmml" xref="S2.E3.m1.7.7.7.2.2.2">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><matrix
    id="S2.E3.m1.10.10.10.5.5.5.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><matrixrow id="S2.E3.m1.10.10.10.5.5.5a.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5"><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3"><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.5.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.5">if</ci><matrix id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><matrixrow id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3a.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3"><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3b.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3"><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.5.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.5">𝑝</ci><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3"><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3"><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2"><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.1.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.1">^</ci><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.2.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.2.2">𝑉</ci></apply><apply id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3"><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.1.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.1">¯</ci><ci id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.2.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.3.3.2">𝑉</ci></apply></apply><apply
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1"><apply
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1"><apply
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2"><apply
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2"><ci
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.1">^</ci><ci
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.2.2.2">𝐷</ci></apply><ci
    id="S2.E3.m1.8.8.8.3.3.3.1.1.1.1.1.1.1.1.cmml" xref="S2.E3.m1.8.8.8.3.3.3.1.1.1.1.1.1.1.1">𝑝</ci></apply><apply
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3"><apply
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2"><ci
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.1.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.1">¯</ci><ci
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.2.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.3.1.1.1.3.2.2">𝐷</ci></apply><ci
    id="S2.E3.m1.9.9.9.4.4.4.2.2.2.2.2.2.2.2.cmml" xref="S2.E3.m1.9.9.9.4.4.4.2.2.2.2.2.2.2.2">𝑝</ci></apply></apply></apply></apply></apply><apply
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3c.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3"><ci
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.8.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3.3.3.3.8">𝜏</ci></apply></apply><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3b.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3c.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3d.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3e.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3f.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3g.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3h.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3i.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3j.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3k.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3l.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3m.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3n.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3o.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3p.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3q.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3r.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3s.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3t.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3u.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3v.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3w.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3x.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3y.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3z.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3aa.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ab.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ac.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ad.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ae.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3af.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ag.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ah.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ai.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3aj.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3ak.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3al.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3"><csymbol
    cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5.3.3.3.3am.cmml" xref="S2.E3.m1.10.10.10.5.5.5.3.3.3.3">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><cerror
    id="S2.E3.m1.10.10.10.5.5.5b.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5c.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5d.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5e.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5f.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5g.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5h.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5i.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5j.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5k.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5l.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5m.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5n.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5o.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5p.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5q.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5r.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5s.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5t.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5u.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5v.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5w.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5x.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5y.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5z.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5aa.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5ab.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5ac.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5ad.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5ae.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5af.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5ag.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5ah.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5ai.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5aj.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5ak.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5al.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5am.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    id="S2.E3.m1.10.10.10.5.5.5an.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><ci id="S2.E3.m1.10.10.10.5.5.5.4.1.1.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5.4.1.1">otherwise</ci><cerror id="S2.E3.m1.10.10.10.5.5.5ao.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous" id="S2.E3.m1.10.10.10.5.5.5ap.cmml"
    xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5aq.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5ar.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5as.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5at.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5au.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5av.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5aw.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5ax.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5ay.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5az.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5ba.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bb.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bc.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bd.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5be.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bf.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bg.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bh.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bi.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bj.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bk.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bl.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bm.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bn.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bo.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bp.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bq.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5br.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bs.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bt.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bu.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bv.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5bw.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bx.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror><cerror
    id="S2.E3.m1.10.10.10.5.5.5by.cmml" xref="S2.E3.m1.10.10.10.5.5.5"><csymbol cd="ambiguous"
    id="S2.E3.m1.10.10.10.5.5.5bz.cmml" xref="S2.E3.m1.10.10.10.5.5.5">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply></matrixrow></matrix></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E3.m1.11c">\begin{array}[]{l}{e_{VSD}}\left({\hat{D},\bar{D},\hat{V},\bar{V},\tau}\right)=\\
    av{g_{p\in\hat{V}\cup\bar{V}}}\left\{{\begin{array}[]{*{20}{l}}{0,}\\ {1,}\end{array}}\right.\begin{array}[]{*{20}{l}}{{\rm{if}}\begin{array}[]{*{20}{l}}{p\in\hat{V}\cap\bar{V}\wedge&#124;\hat{D}\left(p\right)-\bar{D}\left(p\right)&#124;<\tau}\end{array}}\\
    {{\rm{otherwise}}}\end{array},\end{array}\vspace{-0.5em}</annotation></semantics></math>
    |  | (3) |'
  prefs: []
  type: TYPE_NORMAL
- en: 'where the symbols $\hat{D}$ and $\bar{D}$ represent distance maps generated
    by rendering the object model $M$ in two different poses: $\hat{P}$ (an estimated
    pose) and $\bar{P}$ (the ground-truth pose), respectively. In these maps, each
    pixel $p$ stores the distance from the camera center to a 3D point ${{\rm{x}}_{p}}$
    that projects onto $p$. These distance values are derived from depth maps, which
    are typical outputs of sensors like Kinect, containing the $Z$ coordinate of ${{\rm{x}}_{p}}$.
    These distance maps are compared with the distance map ${D_{I}}$ of the test image
    $I$ to derive visibility masks $\hat{V}$ and $\bar{V}$. These masks identify pixels
    where the model $M$ is visible in the image $I$. The parameter $\tau$ represents
    the tolerance for misalignment. In addition, the MSSD [[36](#bib.bib36)] metric
    is a suitable factor for determining the likelihood of successful robotic manipulation
    and is not significantly affected by object geometry or surface sampling density.
    It can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${e_{MSSD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)={\min_{S\in{S_{M}}}}{\max_{{\rm{x}}\in{V_{M}}}}{\left\&#124;{\hat{P}{\rm{x}}-\bar{P}S{\rm{x}}}\right\&#124;_{2}},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'where the set ${{S_{M}}}$ comprises global symmetry transformations for the
    object model $M$, while ${{V_{M}}}$ represents the vertices of the model. Furthermore,
    the MSPD [[36](#bib.bib36)] metric is ideal for evaluating RGB-only methods in
    augmented reality, focusing on perceivable discrepancies and excluding alignment
    along the optical (Z) axis, which can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{array}[]{l}{e_{MSPD}}\left({\hat{P},\bar{P},{S_{M}},{V_{M}}}\right)=\\
    {\min_{S\in{S_{M}}}}{\max_{{\rm{x}}\in{V_{M}}}}{\left\&#124;{proj\left({\hat{P}{\rm{x}}}\right)-proj\left({\bar{P}S{\rm{x}}}\right)}\right\&#124;_{2}},\end{array}\vspace{-0.5em}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where the function $proj()$ represents the 2D projection (pixel-level), and
    the other symbols have the same meanings as in MSSD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the *BOP-M*, the average point distance (ADD)[[37](#bib.bib37)] and
    average closest point distance (ADD-S)[[37](#bib.bib37)] are also commonly leveraged
    to evaluate the performance of 6DoF object pose estimation. They can intuitively
    quantify the geometric error between the estimated and the ground-truth poses
    by computing the average distance between corresponding points on the object CAD
    model. Specifically, the ADD metric is designed for asymmetric objects, while
    the ADD-S metric is designed explicitly for symmetric objects. Given the ground-truth
    rotation ${R_{gt}}$ and translation ${t_{gt}}$, as well as the estimated rotation
    $R$ and translation $t$, ADD calculates the average pairwise distance between
    the 3D model points ${x\in O}$ corresponding to the transformation between the
    ground truth and estimated pose:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ADD=\mathop{avg}\limits_{x\in O}\left\&#124;{\left({{R_{gt}}x+{t_{gt}}}\right)-\left({Rx+t}\right)}\right\&#124;.\vspace{-0.5em}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 'For symmetric objects, the matching between points in certain views is inherently
    ambiguous. Therefore, the average distance is calculated using the nearest point
    distance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $ADD{\rm{-}}S=\mathop{avg}\limits_{{x_{1}}\in O}\mathop{\min}\limits_{{x_{2}}\in
    O}\left\&#124;{\left({{R_{gt}}{x_{1}}+{t_{gt}}}\right)-\left({R{x_{2}}+t}\right)}\right\&#124;.\vspace{-0.5em}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Meanwhile, the area under the ADD and the ADD-S curve (AUC) are often leveraged
    for evaluation. Specifically, if the ADD and ADD-S are smaller than a given threshold,
    the predicted pose will be considered correct. Moreover, there are many methods
    [[68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)] that evaluate asymmetric
    and symmetric objects using ADD and ADD-S, respectively. This metric is termed
    ADD(S).
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, $n^{\circ}$$m{\rm{cm}}$ [[71](#bib.bib71)] is also currently a
    prevalent evaluation metric (especially in category-level object pose estimation).
    It directly quantifies the errors in predicted 3D rotation and 3D translation.
    An object pose prediction is deemed correct if its rotation error is below threshold
    $n^{\circ}$ and its translation error is below threshold $m{\rm{cm}}$. It can
    be defined as an indicator function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math id="S2.E8.m1.3" class="ltx_Math" alttext="\vspace{-0.5em}{I_{{n^{\circ}}m{\rm{cm}}}}({e_{R}},{e_{t}})=\left\{{\begin{array}[]{*{20}{c}}{1,}\\
    {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{{e_{R}}<{n^{\circ}}\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}{e_{t}}<m{\rm{cm}}}\end{array}}\\'
  prefs: []
  type: TYPE_NORMAL
- en: '{{\rm{otherwise}}}\end{array}," display="block"><semantics id="S2.E8.m1.3a"><mrow
    id="S2.E8.m1.3.3.1" xref="S2.E8.m1.3.3.1.1.cmml"><mrow id="S2.E8.m1.3.3.1.1" xref="S2.E8.m1.3.3.1.1.cmml"><mrow
    id="S2.E8.m1.3.3.1.1.2" xref="S2.E8.m1.3.3.1.1.2.cmml"><msub id="S2.E8.m1.3.3.1.1.2.4"
    xref="S2.E8.m1.3.3.1.1.2.4.cmml"><mi id="S2.E8.m1.3.3.1.1.2.4.2" xref="S2.E8.m1.3.3.1.1.2.4.2.cmml">I</mi><mrow
    id="S2.E8.m1.3.3.1.1.2.4.3" xref="S2.E8.m1.3.3.1.1.2.4.3.cmml"><msup id="S2.E8.m1.3.3.1.1.2.4.3.2"
    xref="S2.E8.m1.3.3.1.1.2.4.3.2.cmml"><mi id="S2.E8.m1.3.3.1.1.2.4.3.2.2" xref="S2.E8.m1.3.3.1.1.2.4.3.2.2.cmml">n</mi><mo
    id="S2.E8.m1.3.3.1.1.2.4.3.2.3" xref="S2.E8.m1.3.3.1.1.2.4.3.2.3.cmml">∘</mo></msup><mo
    lspace="0em" rspace="0em" id="S2.E8.m1.3.3.1.1.2.4.3.1" xref="S2.E8.m1.3.3.1.1.2.4.3.1.cmml">​</mo><mi
    id="S2.E8.m1.3.3.1.1.2.4.3.3" xref="S2.E8.m1.3.3.1.1.2.4.3.3.cmml">m</mi><mo lspace="0em"
    rspace="0em" id="S2.E8.m1.3.3.1.1.2.4.3.1a" xref="S2.E8.m1.3.3.1.1.2.4.3.1.cmml">​</mo><mi
    id="S2.E8.m1.3.3.1.1.2.4.3.4" xref="S2.E8.m1.3.3.1.1.2.4.3.4.cmml">cm</mi></mrow></msub><mo
    lspace="0em" rspace="0em" id="S2.E8.m1.3.3.1.1.2.3" xref="S2.E8.m1.3.3.1.1.2.3.cmml">​</mo><mrow
    id="S2.E8.m1.3.3.1.1.2.2.2" xref="S2.E8.m1.3.3.1.1.2.2.3.cmml"><mo stretchy="false"
    id="S2.E8.m1.3.3.1.1.2.2.2.3" xref="S2.E8.m1.3.3.1.1.2.2.3.cmml">(</mo><msub id="S2.E8.m1.3.3.1.1.1.1.1.1"
    xref="S2.E8.m1.3.3.1.1.1.1.1.1.cmml"><mi id="S2.E8.m1.3.3.1.1.1.1.1.1.2" xref="S2.E8.m1.3.3.1.1.1.1.1.1.2.cmml">e</mi><mi
    id="S2.E8.m1.3.3.1.1.1.1.1.1.3" xref="S2.E8.m1.3.3.1.1.1.1.1.1.3.cmml">R</mi></msub><mo
    id="S2.E8.m1.3.3.1.1.2.2.2.4" xref="S2.E8.m1.3.3.1.1.2.2.3.cmml">,</mo><msub id="S2.E8.m1.3.3.1.1.2.2.2.2"
    xref="S2.E8.m1.3.3.1.1.2.2.2.2.cmml"><mi id="S2.E8.m1.3.3.1.1.2.2.2.2.2" xref="S2.E8.m1.3.3.1.1.2.2.2.2.2.cmml">e</mi><mi
    id="S2.E8.m1.3.3.1.1.2.2.2.2.3" xref="S2.E8.m1.3.3.1.1.2.2.2.2.3.cmml">t</mi></msub><mo
    stretchy="false" id="S2.E8.m1.3.3.1.1.2.2.2.5" xref="S2.E8.m1.3.3.1.1.2.2.3.cmml">)</mo></mrow></mrow><mo
    id="S2.E8.m1.3.3.1.1.3" xref="S2.E8.m1.3.3.1.1.3.cmml">=</mo><mrow id="S2.E8.m1.3.3.1.1.4"
    xref="S2.E8.m1.3.3.1.1.4.cmml"><mrow id="S2.E8.m1.3.3.1.1.4.2.2" xref="S2.E8.m1.3.3.1.1.4.2.1.cmml"><mo
    id="S2.E8.m1.3.3.1.1.4.2.2.1" xref="S2.E8.m1.3.3.1.1.4.2.1.1.cmml">{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S2.E8.m1.2.2" xref="S2.E8.m1.2.2.cmml"><mtr
    id="S2.E8.m1.2.2a" xref="S2.E8.m1.2.2.cmml"><mtd id="S2.E8.m1.2.2b" xref="S2.E8.m1.2.2.cmml"><mrow
    id="S2.E8.m1.1.1.1.1.1.3" xref="S2.E8.m1.2.2.cmml"><mn id="S2.E8.m1.1.1.1.1.1.1"
    xref="S2.E8.m1.1.1.1.1.1.1.cmml">1</mn><mo id="S2.E8.m1.1.1.1.1.1.3.1" xref="S2.E8.m1.2.2.cmml">,</mo></mrow></mtd></mtr><mtr
    id="S2.E8.m1.2.2v" xref="S2.E8.m1.2.2.cmml"><mtd id="S2.E8.m1.2.2w" xref="S2.E8.m1.2.2.cmml"><mrow
    id="S2.E8.m1.2.2.2.1.1.3" xref="S2.E8.m1.2.2.cmml"><mn id="S2.E8.m1.2.2.2.1.1.1"
    xref="S2.E8.m1.2.2.2.1.1.1.cmml">0</mn><mo id="S2.E8.m1.2.2.2.1.1.3.1" xref="S2.E8.m1.2.2.cmml">,</mo></mrow></mtd></mtr></mtable></mrow><mo
    lspace="0.167em" rspace="0em" id="S2.E8.m1.3.3.1.1.4.1" xref="S2.E8.m1.3.3.1.1.4.1.cmml">​</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" id="S2.E8.m1.3.3.1.1.4.3"
    xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mtr id="S2.E8.m1.3.3.1.1.4.3a" xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mtd
    id="S2.E8.m1.3.3.1.1.4.3b" xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mrow id="S2.E8.m1.3.3.1.1.4.3.1.1.1"
    xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.2.cmml">if</mi><mo
    lspace="0.167em" rspace="0em" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.1" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.1.cmml">​</mo><mtable
    columnspacing="5pt" displaystyle="true" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"><mtr
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3a" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"><mtd
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3b" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml"><mrow
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.cmml"><msub
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.cmml"><mi
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.2.cmml">e</mi><mi
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.3.cmml">R</mi></msub><mo
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.3.cmml"><</mo><mrow
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.cmml"><msup
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.cmml"><mi
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.2.cmml">n</mi><mo
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.3.cmml">∘</mo></msup><mo
    lspace="0.167em" rspace="0em" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1.cmml">​</mo><mtable
    columnspacing="5pt" displaystyle="true" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"
    xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"><mtr id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3a"
    xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"><mtd id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3b"
    xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.1.1.1"
    xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.1.1.1.cmml">and</mi></mtd></mtr></mtable><mo
    lspace="0.167em" rspace="0em" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1a" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.1.cmml">​</mo><msub
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.cmml"><mi
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.2.cmml">e</mi><mi
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.3.cmml">t</mi></msub></mrow><mo
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.5" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.5.cmml"><</mo><mrow
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.cmml"><mi
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.2" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.2.cmml">m</mi><mo
    lspace="0em" rspace="0em" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.1" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.1.cmml">​</mo><mi
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.3" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.3.cmml">cm</mi></mrow></mrow></mtd></mtr></mtable></mrow></mtd></mtr><mtr
    id="S2.E8.m1.3.3.1.1.4.3v" xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mtd id="S2.E8.m1.3.3.1.1.4.3w"
    xref="S2.E8.m1.3.3.1.1.4.3.cmml"><mi id="S2.E8.m1.3.3.1.1.4.3.2.1.1" xref="S2.E8.m1.3.3.1.1.4.3.2.1.1.cmml">otherwise</mi></mtd></mtr></mtable></mrow></mrow><mo
    lspace="0.167em" id="S2.E8.m1.3.3.1.2" xref="S2.E8.m1.3.3.1.1.cmml">,</mo></mrow><annotation-xml
    encoding="MathML-Content" id="S2.E8.m1.3b"><apply id="S2.E8.m1.3.3.1.1.cmml" xref="S2.E8.m1.3.3.1"><apply
    id="S2.E8.m1.3.3.1.1.2.cmml" xref="S2.E8.m1.3.3.1.1.2"><apply id="S2.E8.m1.3.3.1.1.2.4.cmml"
    xref="S2.E8.m1.3.3.1.1.2.4"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.2.4.1.cmml"
    xref="S2.E8.m1.3.3.1.1.2.4">subscript</csymbol><ci id="S2.E8.m1.3.3.1.1.2.4.2.cmml"
    xref="S2.E8.m1.3.3.1.1.2.4.2">𝐼</ci><apply id="S2.E8.m1.3.3.1.1.2.4.3.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3"><apply
    id="S2.E8.m1.3.3.1.1.2.4.3.2.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.2"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.2.4.3.2.1.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.2">superscript</csymbol><ci
    id="S2.E8.m1.3.3.1.1.2.4.3.2.2.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.2.2">𝑛</ci></apply><ci
    id="S2.E8.m1.3.3.1.1.2.4.3.3.cmml" xref="S2.E8.m1.3.3.1.1.2.4.3.3">𝑚</ci><ci id="S2.E8.m1.3.3.1.1.2.4.3.4.cmml"
    xref="S2.E8.m1.3.3.1.1.2.4.3.4">cm</ci></apply></apply><interval closure="open"
    id="S2.E8.m1.3.3.1.1.2.2.3.cmml" xref="S2.E8.m1.3.3.1.1.2.2.2"><apply id="S2.E8.m1.3.3.1.1.1.1.1.1.cmml"
    xref="S2.E8.m1.3.3.1.1.1.1.1.1"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.1.1.1.1.1.cmml"
    xref="S2.E8.m1.3.3.1.1.1.1.1.1">subscript</csymbol><ci id="S2.E8.m1.3.3.1.1.1.1.1.1.2.cmml"
    xref="S2.E8.m1.3.3.1.1.1.1.1.1.2">𝑒</ci><ci id="S2.E8.m1.3.3.1.1.1.1.1.1.3.cmml"
    xref="S2.E8.m1.3.3.1.1.1.1.1.1.3">𝑅</ci></apply><apply id="S2.E8.m1.3.3.1.1.2.2.2.2.cmml"
    xref="S2.E8.m1.3.3.1.1.2.2.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.2.2.2.2.1.cmml"
    xref="S2.E8.m1.3.3.1.1.2.2.2.2">subscript</csymbol><ci id="S2.E8.m1.3.3.1.1.2.2.2.2.2.cmml"
    xref="S2.E8.m1.3.3.1.1.2.2.2.2.2">𝑒</ci><ci id="S2.E8.m1.3.3.1.1.2.2.2.2.3.cmml"
    xref="S2.E8.m1.3.3.1.1.2.2.2.2.3">𝑡</ci></apply></interval></apply><apply id="S2.E8.m1.3.3.1.1.4.cmml"
    xref="S2.E8.m1.3.3.1.1.4"><apply id="S2.E8.m1.3.3.1.1.4.2.1.cmml" xref="S2.E8.m1.3.3.1.1.4.2.2"><csymbol
    cd="latexml" id="S2.E8.m1.3.3.1.1.4.2.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.2.2.1">cases</csymbol><matrix
    id="S2.E8.m1.2.2.cmml" xref="S2.E8.m1.2.2"><matrixrow id="S2.E8.m1.2.2a.cmml"
    xref="S2.E8.m1.2.2"><cn type="integer" id="S2.E8.m1.1.1.1.1.1.1.cmml" xref="S2.E8.m1.1.1.1.1.1.1">1</cn><cerror
    id="S2.E8.m1.2.2b.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2c.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2d.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2e.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2f.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2g.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2h.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2i.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2j.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2k.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2l.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2m.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2n.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2o.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2p.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2q.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2r.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2s.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2t.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2u.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2v.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2w.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2x.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2y.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2z.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2aa.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2ab.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ac.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2ad.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ae.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2af.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ag.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2ah.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ai.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2aj.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ak.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2al.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2am.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    id="S2.E8.m1.2.2an.cmml" xref="S2.E8.m1.2.2"><cn type="integer" id="S2.E8.m1.2.2.2.1.1.1.cmml"
    xref="S2.E8.m1.2.2.2.1.1.1">0</cn><cerror id="S2.E8.m1.2.2ao.cmml" xref="S2.E8.m1.2.2"><csymbol
    cd="ambiguous" id="S2.E8.m1.2.2ap.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2aq.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ar.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2as.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2at.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2au.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2av.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2aw.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2ax.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2ay.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2az.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2ba.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bb.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2bc.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bd.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2be.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bf.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2bg.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bh.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bi.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bj.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2bk.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bl.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bm.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bn.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2bo.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bp.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bq.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2br.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2bs.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bt.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2bu.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bv.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.2.2bw.cmml" xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bx.cmml"
    xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.2.2by.cmml"
    xref="S2.E8.m1.2.2"><csymbol cd="ambiguous" id="S2.E8.m1.2.2bz.cmml" xref="S2.E8.m1.2.2">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><matrix
    id="S2.E8.m1.3.3.1.1.4.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><matrixrow id="S2.E8.m1.3.3.1.1.4.3a.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><apply id="S2.E8.m1.3.3.1.1.4.3.1.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1"><ci
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.2">if</ci><matrix
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><matrixrow
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3a.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><apply
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1"><apply
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1b.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1"><apply
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2">subscript</csymbol><ci
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.2">𝑒</ci><ci
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.2.3">𝑅</ci></apply><apply
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4"><apply
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2">superscript</csymbol><ci
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.2.2">𝑛</ci></apply><matrix
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><matrixrow
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3a.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><ci
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.1.1.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3.1.1.1">and</ci><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3b.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3c.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3d.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3e.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3f.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3g.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3h.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3i.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3j.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3k.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3l.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3m.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3n.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3o.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3p.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3q.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3r.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3s.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3t.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3u.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3v.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3w.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3x.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3y.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3z.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3aa.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ab.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ac.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ad.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ae.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3af.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ag.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ah.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ai.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3aj.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3ak.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3al.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3am.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.3">missing-subexpression</csymbol></cerror></matrixrow></matrix><apply
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.1.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4">subscript</csymbol><ci
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.2">𝑒</ci><ci
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.4.4.3">𝑡</ci></apply></apply></apply><apply
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1c.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1"><apply
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6"><ci
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.2.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.2">𝑚</ci><ci
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.3.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3.1.1.1.6.3">cm</ci></apply></apply></apply><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3b.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3c.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3d.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3e.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3f.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3g.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3h.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3i.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3j.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3k.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3l.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3m.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3n.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3o.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3p.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3q.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3r.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3s.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3t.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3u.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3v.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3w.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3x.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3y.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3z.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3aa.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ab.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ac.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ad.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ae.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3af.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ag.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ah.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ai.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3aj.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3ak.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3al.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3"><csymbol
    cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3.1.1.1.3am.cmml" xref="S2.E8.m1.3.3.1.1.4.3.1.1.1.3">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply><cerror
    id="S2.E8.m1.3.3.1.1.4.3b.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3c.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3d.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3e.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3f.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3g.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3h.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3i.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3j.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3k.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3l.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3m.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3n.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3o.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3p.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3q.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3r.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3s.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3t.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3u.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3v.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3w.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3x.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3y.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3z.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3aa.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3ab.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3ac.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3ad.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3ae.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3af.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3ag.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3ah.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3ai.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3aj.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3ak.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror
    id="S2.E8.m1.3.3.1.1.4.3al.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous"
    id="S2.E8.m1.3.3.1.1.4.3am.cmml" xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror></matrixrow><matrixrow
    id="S2.E8.m1.3.3.1.1.4.3an.cmml" xref="S2.E8.m1.3.3.1.1.4.3"><ci id="S2.E8.m1.3.3.1.1.4.3.2.1.1.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3.2.1.1">otherwise</ci><cerror id="S2.E8.m1.3.3.1.1.4.3ao.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ap.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3aq.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ar.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3as.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3at.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3au.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3av.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3aw.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3ax.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3ay.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3az.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3ba.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bb.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bc.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bd.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3be.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bf.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bg.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bh.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bi.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bj.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bk.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bl.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bm.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bn.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bo.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bp.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bq.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3br.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bs.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bt.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bu.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bv.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3bw.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bx.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror><cerror id="S2.E8.m1.3.3.1.1.4.3by.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3"><csymbol cd="ambiguous" id="S2.E8.m1.3.3.1.1.4.3bz.cmml"
    xref="S2.E8.m1.3.3.1.1.4.3">missing-subexpression</csymbol></cerror></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" id="S2.E8.m1.3c">\vspace{-0.5em}{I_{{n^{\circ}}m{\rm{cm}}}}({e_{R}},{e_{t}})=\left\{{\begin{array}[]{*{20}{c}}{1,}\\
    {0,}\end{array}}\right.\begin{array}[]{*{20}{c}}{{\rm{if}}\begin{array}[]{*{20}{c}}{{e_{R}}<{n^{\circ}}\begin{array}[]{*{20}{c}}{{\rm{and}}}\end{array}{e_{t}}<m{\rm{cm}}}\end{array}}\\
    {{\rm{otherwise}}}\end{array},</annotation></semantics></math> |  | (8) |'
  prefs: []
  type: TYPE_NORMAL
- en: where ${{e_{R}}}$ and ${{e_{t}}}$ represent the rotation and translation errors
    between the estimated and ground-truth values, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, compared to directly comparing 6DoF pose in 3D space, the simplicity
    and practicality of the 2D Projection metric [[38](#bib.bib38)] make it suitable
    for evaluation as well, which quantifies the average distance between CAD model
    points when projected under the estimated object pose and the ground-truth pose.
    A pose is considered correct if the projected distances are less than 5 pixels.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3 9DoF Evaluation Metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '$Io{U_{3D}}$ denotes the Intersection-over-Union (IoU)[[22](#bib.bib22)] percentage
    between the ground-truth and predicted 3D bounding boxes, which can evaluate the
    6DoF pose estimation as well as the 3DoF size estimation. It can be expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Io{U_{3D}}=\frac{{{P_{B}}\cap{G_{B}}}}{{{P_{B}}\cup{G_{B}}}},\vspace{-0.5em}$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where ${{G_{B}}}$ and ${{P_{B}}}$ represent the ground-truth and the predicted
    3D bounding boxes, respectively. The symbols $\cap$ and $\cup$ represent the intersection
    and union, respectively. The correctness of the predicted object pose is determined
    based on whether the $Io{U_{3D}}$ value exceeds a predefined threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.4 Other Metric
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since some Normalized Object Coordinate Space (NOCS) shape alignment-based
    category-level methods reconstruct the 3D object shape before estimating the object
    pose, the Chamfer Distance (CD) metric[[72](#bib.bib72)], which not only captures
    the global shape deviation but is also sensitive to local shape differences, is
    commonly leveraged to evaluate the NOCS shape reconstruction accuracy of these
    methods as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${D_{cd}}=\sum\limits_{x\in N}{\mathop{\min}\limits_{y\in{M_{gt}}}}\parallel
    x-y\parallel_{2}^{2}+\sum\limits_{y\in{M_{gt}}}{\mathop{\min}\limits_{x\in N}}\parallel
    x-y\parallel_{2}^{2},\vspace{-0.5em}$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $N$ and $M_{gt}$ represent the reconstructed and ground-truth NOCS shape,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Instance-Level Object Pose Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Instance-level object pose estimation describes the task of estimating the
    pose of the objects that have been seen during the training of the model. We classify
    existing instance-level methods into four categories: correspondence-based (Sec. [3.1](#S3.SS1
    "3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣
    Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")), template-based
    (Sec. [3.2](#S3.SS2 "3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose
    Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")),
    voting-based (Sec. [3.3](#S3.SS3 "3.3 Voting-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")), and regression-based (Sec. [3.4](#S3.SS4 "3.4 Regression-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Correspondence-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Correspondence-based object pose estimation refers to techniques that involve
    identifying correspondences between the input data and the given complete object
    CAD model. Correspondence-based methods can be divided into sparse and dense correspondences.
    Sparse correspondence-based methods (Sec. [3.1.1](#S3.SS1.SSS1 "3.1.1 Sparse Correspondence
    Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")) involve
    detecting object keypoints in the input image or point cloud to establish 2D-3D
    or 3D-3D correspondences between the input data and the object CAD model, followed
    by the utilization of the Perspective-n-Point (PnP) algorithm [[73](#bib.bib73)]
    or least square method to determine the object pose. Dense correspondence-based
    methods (Sec. [3.1.2](#S3.SS1.SSS2 "3.1.2 Dense Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) aim to establish dense 2D-3D or 3D-3D
    correspondences, ultimately leading to more accurate object pose estimation. For
    the RGB image, they leverage every pixel or multiple patches to generate pixel-wise
    correspondences, while for the point cloud, they use the entire point cloud to
    find point-wise correspondences. The illustration of these two types of methods
    is shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Sparse Correspondence Methods ‣
    3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey"). The attributes
    and performance of some representative methods are shown in Table [I](#S3.T1 "TABLE
    I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3
    Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Sparse Correspondence Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a representative method, Rad *et al.*[[74](#bib.bib74)] first used a segmentation
    method to detect the object of interest in an RGB image. Then, they predicted
    the 2D projections of the object’s 3D bounding box corners. Finally, they used
    the PnP algorithm[[73](#bib.bib73)] to estimate the object pose. Additionally,
    they employed a classifier to determine the pose range in real-time, addressing
    the issue of ambiguity in symmetric objects. Tekin *et al.*[[75](#bib.bib75)]
    proposed a CNN network inspired by YOLO[[76](#bib.bib76)] to integrate object
    detection and pose estimation, directly predicting the locations of the projected
    vertices of the 3D object bounding box. Unlike [[74](#bib.bib74)] and [[75](#bib.bib75)],
    Pavlakos *et al.*[[77](#bib.bib77)] predicted the 2D projections of predefined
    semantic keypoints. Doosti *et al.*[[78](#bib.bib78)] introduced a compact model
    comprising two adaptive graph convolutional neural networks (GCNNs)[[79](#bib.bib79)],
    collaborating to estimate object and hand poses. To further enhance the robustness
    of object pose estimation, Song *et al.*[[80](#bib.bib80)] employed a hybrid intermediate
    representation to convey geometric details in the input image, encompassing keypoints,
    edge vectors, and symmetry correspondences. Liu *et al.*[[81](#bib.bib81)] proposed
    a multi-directional feature pyramid network along with a method that calculates
    object pose estimation confidence by incorporating spatial and plane information.
    Hu et al.[[82](#bib.bib82)] introduced a single-stage hierarchical end-to-end
    trainable network to address pose estimation challenges associated with scale
    variations in aerospace objects. In a recent development, Lian *et al.*[[83](#bib.bib83)]
    increased the number of predefined 3D keypoints to enhance the establishment of
    correspondences. Moreover, they devised a hierarchical binary encoding approach
    for localizing keypoints, enabling gradual refinement of correspondences and transforming
    correspondence regression into a more efficient classification task. To estimate
    transparent object pose, Chang *et al.*[[84](#bib.bib84)] used a 3D bounding box
    prediction network and multi-view geometry techniques. The method first detects
    2D projections of 3D bounding box vertices, and then reconstructs 3D points based
    on the multi-view detected 2D projections incorporating camera motion data. Additionally,
    they introduced a generalized pose definition to address pose ambiguity for symmetric
    objects. To enhance the efficiency of pose estimation networks, Guo *et al.*[[85](#bib.bib85)]
    integrated knowledge distillation into object pose estimation by distilling the
    teacher’s distribution of local predictions into the student network. Liu *et
    al.*[[86](#bib.bib86)] argued that differentiable PnP strategies conflict with
    the averaging nature of the PnP problem, resulting in gradients that may encourage
    the network to degrade the accuracy of individual correspondences. To mitigate
    this, they introduced a linear covariance loss, which can be used for both sparse
    and dense correspondence-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate vulnerability caused by large occlusions, Crivellaro *et al.*[[87](#bib.bib87)]
    used several control points to represent each object part. Then, they predicted
    the 2D projections of these control points to calculate the object pose. Some
    researchers solved the occlusion problem by predicting keypoints using small patches.
    Oberweger *et al.*[[88](#bib.bib88)] processed each patch separately to generate
    heatmaps and then aggregated the results to achieve precise and reliable predictions.
    Additionally, they offered a straightforward but efficient strategy to resolve
    ambiguities between patches and heatmaps during training. Hu *et al.*[[89](#bib.bib89)]
    unveiled a segmentation-driven pose estimation framework in which every visible
    object part offers a local pose prediction through 2D keypoint locations. Furthermore,
    Huang *et al.*[[90](#bib.bib90)] conceptualized 2D keypoint locations as probabilistic
    distributions within the loss function and designed a confidence-based network.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the reliance on annotated real-world data is also an important task.
    Some methods exploit geometric consistency as additional information to alleviate
    the need for annotation. Zhao *et al.*[[91](#bib.bib91)] employed image pairs
    with object annotations and relative transformation between viewpoints to automatically
    identify objects’ 3D keypoints that are geometrically and visually consistent.
    In addition, Yang *et al.*[[92](#bib.bib92)] used a keypoint consistency regularization
    for dual-scale images with a labeled 2D bounding box. Using semi-supervised learning,
    Liu *et al.*[[93](#bib.bib93)] developed a unified framework for estimating 3D
    hand and object poses. They constructed a joint learning framework that conducts
    explicit contextual reasoning between hand and object representations. To generate
    pseudo labels in semi-supervised learning, they utilized the spatial-temporal
    consistency found in large-scale hand-object videos as a constraint. Synthetic
    data is also a way to solve the annotation problem. Georgakis *et al.*[[94](#bib.bib94)]
    reduced the need for expensive 3DoF pose annotations by selecting keypoints and
    maintaining viewpoint and modality invariance in RGB images and CAD model renderings.
    Sock *et al.*[[95](#bib.bib95)] utilized self-supervision to minimize the gap
    between synthetic and real data and enforced photometric consistency across different
    object views to fine-tune the model. Further, Zhang *et al.*[[96](#bib.bib96)]
    utilized the invariance of geometry relations between keypoints across real and
    synthetic domains to accomplish domain adaptation. Thalhammer *et al.*[[97](#bib.bib97)]
    introduced a specialized feature pyramid network to compute multi-scale features,
    enabling the simultaneous generation of pose hypotheses across various feature
    map resolutions.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the sparse correspondence-based methods can estimate object pose efficiently.
    However, relying on only a few control points can lead to sub-optimal accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/baba466aa3245ec421cb6de495238ea9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of the correspondence-based (Sec. [3.1](#S3.SS1 "3.1
    Correspondence-Based Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")), template-based
    (Sec. [3.2](#S3.SS2 "3.2 Template-Based Methods ‣ 3 Instance-Level Object Pose
    Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")),
    voting-based (Sec. [3.3](#S3.SS3 "3.3 Voting-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")), and regression-based (Sec. [3.4](#S3.SS4 "3.4 Regression-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) instance-level methods. Correspondence-based methods
    (Sec. [3.1](#S3.SS1 "3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) involve establishing correspondences between input data and a provided
    object CAD model. Template-based methods (Sec. [3.2](#S3.SS2 "3.2 Template-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) involve identifying the most similar
    template from a set of templates labeled with ground-truth object poses. Voting-based
    methods (Sec. [3.3](#S3.SS3 "3.3 Voting-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) determine object pose through a pixel-level or point-level voting scheme.
    Regression-based methods (Sec. [3.4](#S3.SS4 "3.4 Regression-Based Methods ‣ 3
    Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) aim to obtain the object pose directly from the learned
    features.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Representative instance-level methods. For each method, we report
    its 10 properties: published year, training input, inference input, pose DoF (3DoF,
    6DoF, and 9DoF), object property (rigid, articulated), task (estimation, tracking,
    and refinement), domain training paradigm (source domain, domain adaptation, and
    domain generalization), inference mode, application area, and its performance
    of key metrics on key datasets. Notably, for the input of training and inference,
    we only focus on the input of the pose estimation model, not the input of the
    front-end segmentation method (because it can be obtained through RGB as well
    as through depth or RGBD). D, S, C, T, V, P, and R denote object detection, instance
    segmentation, correspondence prediction, template matching, voting, pose solution/regression,
    and pose refinement, respectively. We report the average recall of ADD(S) within
    10$\%$ of the object diameter (termed ADD(S)-0.1d) of LM-O and LM datasets, and
    the AUC of ADD-S ($<$0.1m) of YCB-V dataset (Sec. [2](#S2 "2 Datasets and Metrics
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Published &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Year &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pose &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DoF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Property &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Domain Training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paradigm &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mode &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Application &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Area &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LM-O $&#124;$ LM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ADD(S)-0.1d &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; YCB-V &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ADD-S ($<$0.1m) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Correspondence-Based Methods | Sparse correspondence | Rad *et al.*[[74](#bib.bib74)]
    | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+C+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| symmetrical objects | - | 62.7 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Tekin *et al.*[[75](#bib.bib75)] | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; C+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 56.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Hu *et al.*[[89](#bib.bib89)] | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; C+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| occlusion | 27.0 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Song *et al.*[[80](#bib.bib80)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; C+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; symmetrical objects, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; occlusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 47.5 | 91.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Hu *et al.*[[82](#bib.bib82)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; C+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| large scale variations | 48.6 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Chang *et al.*[[84](#bib.bib84)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; C+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; transparent, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; symmetrical objects &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Guo *et al.*[[85](#bib.bib85)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+C+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 44.5 | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Dense correspondence | Li *et al.*[[19](#bib.bib19)] | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+C+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 89.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Hodan *et al.*[[98](#bib.bib98)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; C+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| symmetrical objects | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Shugurov *et al.*[[99](#bib.bib99)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+C+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 99.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Chen *et al.*[[100](#bib.bib100)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+C+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 95.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Haugaard *et al.*[[101](#bib.bib101)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+C+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Li *et al.*[[102](#bib.bib102)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+C+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 51.4 | 97.8 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Xu *et al.*[[103](#bib.bib103)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | refinement | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| occlusion | 60.7 | 97.4 | 85.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Template-Based Methods | RGB-based | Sundermeyer *et al.*[[104](#bib.bib104)]
    | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 31.4 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Papaioannidis *et al.*[[105](#bib.bib105)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Li *et al.*[[106](#bib.bib106)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 88.6 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Deng *et al.*[[107](#bib.bib107)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | tracking | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| symmetrical objects | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Point cloud | Li *et al.*[[70](#bib.bib70)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 70.6 | 99.5 | 96.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Jiang *et al.*[[108](#bib.bib108)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Dang *et al.*[[109](#bib.bib109)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 52.0 | 69.0 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Voting-Based Methods | Indirect voting | Peng *et al.*[[17](#bib.bib17)]
    | 2019 | RGB | RGB | 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; V+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| occlusion | 40.8 | 86.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| He *et al.*[[18](#bib.bib18)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+V+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 99.4 | 95.5 |'
  prefs: []
  type: TYPE_TB
- en: '| He *et al.*[[21](#bib.bib21)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+V+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 66.2 | 99.7 | 96.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Cao *et al.*[[110](#bib.bib110)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | 6DoF | rigid | estimation | source | end to end | general | 58.7 |
    - | 90.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Wu *et al.*[[111](#bib.bib111)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+V+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 70.2 | 99.4 | 96.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou *et al.*[[112](#bib.bib112)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+V+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 77.7 | 99.8 | 96.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Direct voting | Wang *et al.*[[16](#bib.bib16)] | 2019 | RGBD | RGBD | 6DoF
    | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 94.3 | 93.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Tian *et al.*[[113](#bib.bib113)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+V+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 92.9 | 91.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou *et al.*[[114](#bib.bib114)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 65.0 | 99.6 | 95.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Mo *et al.*[[115](#bib.bib115)] | 2022 | RGBD | RGBD | 6DoF | rigid | estimation
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - | 93.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Hong *et al.*[[116](#bib.bib116)] | 2024 | RGBD | RGBD | 6DoF | rigid | estimation
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 71.1 | 96.7 | 92.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Regression-Based Methods | Geometry-guided | Chen *et al.*[[117](#bib.bib117)]
    | 2020 | RGBD | RGBD | 6DoF | rigid | estimation | source | end to end | general
    | - | 98.7 | 92.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Hu *et al.*[[118](#bib.bib118)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | 6DoF | rigid | estimation | source | end to end | general | 43.3 |
    - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Labbé *et al.*[[119](#bib.bib119)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - | 93.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang *et al.*[[120](#bib.bib120)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 62.2 | - | 91.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Di *et al.*[[121](#bib.bib121)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | 6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| occlusion | 62.32 | 96.0 | 90.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang *et al.*[[122](#bib.bib122)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | 6DoF | rigid | estimation | adaptation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| occlusion | 59.8 | 85.6 | 90.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Direct regression | Xiang *et al.*[[15](#bib.bib15)] | 2017 | RGB | RGB |
    6DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+V+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| cluttered | 24.9 | - | 75.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Li *et al.*[[123](#bib.bib123)] | 2018 | RGBD | RGBD | 6DoF | rigid | estimation
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - | 94.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Li *et al.*[[124](#bib.bib124)] | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid |'
  prefs: []
  type: TYPE_TB
- en: '&#124; refinement, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tracking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| source | end to end | general | 55.5 | 88.6 | 81.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Manhardt *et al.*[[125](#bib.bib125)] | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid |'
  prefs: []
  type: TYPE_TB
- en: '&#124; refinement, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tracking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| generaliztion | end to end | general | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Manhardt *et al.*[[126](#bib.bib126)] | 2019 | RGB | RGB | 6DoF | rigid |
    estimation | source | end to end | symmetrical objects | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Papaioannidis *et al.*[[127](#bib.bib127)] | 2019 | RGB | RGB | 3DoF | rigid
    | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu *et al.*[[128](#bib.bib128)] | 2019 | RGB | RGB | 3DoF | rigid | estimation
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| texture-less | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wen *et al.*[[48](#bib.bib48)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 6DoF | rigid | tracking | generaliztion | end to end | general | -
    | - | 93.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang *et al.*[[68](#bib.bib68)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 6DoF | rigid | estimation | generaliztion | end to end | general |
    32.1 | 58.9 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Jiang *et al.*[[69](#bib.bib69)] | 2022 | RGBD | RGBD | 6DoF | rigid | estimation
    | source | end to end | general | 30.8 | 97.0 | 95.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Hai *et al.*[[129](#bib.bib129)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | 6DoF | rigid | refinement | source | end to end | general | 66.4 |
    99.3 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Li *et al.*[[130](#bib.bib130)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | refinement | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - | 97.0 |'
  prefs: []
  type: TYPE_TB
- en: 3.1.2 Dense Correspondence Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dense correspondence-based methods utilize a significantly larger number of
    correspondences compared to sparse correspondence-based methods. This enables
    them to achieve higher accuracy and handle occlusions more effectively. Li *et
    al.*[[19](#bib.bib19)] argued for the differentiation between rotation and translation,
    proposing the coordinates-based disentangled pose network. This network separates
    pose estimation into distinct predictions for rotation and translation. Zakharov
    *et al.*[[20](#bib.bib20)] introduced the dense multi-class 2D-3D correspondence-based
    object pose detector and a tailored deep learning-based refinement process. In
    addition, Cai *et al.*[[131](#bib.bib131)] proposed a technique to automatically
    identify and match image landmarks consistently across different views, aiming
    to enhance the process of learning 2D-3D mapping. Wang *et al.*[[132](#bib.bib132)]
    developed a pose estimation pipeline guided by reconstruction, capitalizing on
    geometric consistency. Further, Shugurov *et al.*[[99](#bib.bib99)] built upon
    Zakharov *et al.*[[20](#bib.bib20)] by developing a unified deep network capable
    of accommodating multiple image modalities (such as RGB and Depth) and integrating
    a differentiable rendering-based pose refinement method. Su *et al.*[[133](#bib.bib133)]
    introduced a discrete descriptor realized by hierarchical binary grouping, capable
    of densely representing the object surface. As a result, this method can predict
    fine-grained correspondences. Chen *et al.*[[100](#bib.bib100)] introduced a probabilistic
    PnP[[73](#bib.bib73)] layer designed for general end-to-end pose estimation. This
    layer generates a pose distribution on the SE(3) manifold. On the other hand,
    Xu *et al.*[[134](#bib.bib134)] argued that encoding pose-sensitive local features
    and modeling the statistical distribution of inlier poses are crucial for accurate
    and robust 6DoF pose estimation. Inspired by PPF[[11](#bib.bib11)], they exploited
    pose-sensitive information carried by each pair of oriented points and an ensemble
    of redundant pose predictions to achieve robust performance on severe inter-object
    occlusion and systematic noises in scene point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: Some methods recover object poses by establishing 3D-3D correspondences. Huang
    *et al.*[[135](#bib.bib135)] used an RGB image to predict 3D object coordinates
    in the camera frustum, thus establishing 3D-3D correspondences. Further, Jiang
    *et al.*[[136](#bib.bib136)] introduced a center-based decoupled framework, leveraging
    bird’s eye and front views for object center voting. They utilized feature similarity
    between the center-aligned object and the object CAD model to establish correspondences
    for Singular Value Decomposition (SVD)-based[[137](#bib.bib137)] rotation estimation.
    More recently, Lin *et al.*[[138](#bib.bib138)] utilized an RGBD image as input
    and employed point-to-surface matching to estimate the object surface correspondence.
    They establish 3D-3D correspondences by iteratively constricting the surface,
    transitioning it into a correspondence point while progressively eliminating outliers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some methods put more effort into handling challenging cases, such as symmetric
    objects[[139](#bib.bib139), [98](#bib.bib98), [140](#bib.bib140)] and texture-less
    objects [[141](#bib.bib141)]. Park *et al.*[[139](#bib.bib139)] utilized generative
    adversarial training to reconstruct occluded parts to alleviate the impact of
    occlusion. They handle symmetric objects by guiding predictions towards the nearest
    symmetric pose. In addition, Hodan *et al.*[[98](#bib.bib98)] modeled an object
    using compact surface fragments to handle symmetries in object modeling effectively.
    For each pixel, the network predicts: the likelihood of each object’s presence,
    the probability of the fragments conditional on the object’s presence, and the
    exact 3D translation of each fragment. Finally, the object pose is determined
    using a robust and efficient version of the PnP-RANSAC algorithm [[73](#bib.bib73)].
    Further, Wu *et al.*[[140](#bib.bib140)] employed a geometric-aware dense matching
    network to acquire visible dense correspondences. Additionally, they utilized
    the distance consistency of these correspondences to mitigate ambiguity in symmetrical
    objects. For texture-less objects, Wu *et al.*[[141](#bib.bib141)] leveraged information
    from the object CAD model and established 2D-3D correspondences using a pseudo-Siamese
    neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: With research development, domain adaptation, weak supervision, and self-supervision
    techniques have been introduced into pose estimation. Li *et al.*[[142](#bib.bib142)]
    noticed that images with varying levels of realism and semantics exhibit different
    transferability between synthetic and real domains. Consequently, they decomposed
    the input image into multi-level semantic representations and merged the strengths
    of these representations to mitigate the domain gap. Further, Hu *et al.*[[143](#bib.bib143)]
    introduced a method exclusively trained on synthetic images, which infers the
    necessary pose correction for refining rough poses. Haugaard *et al.*[[101](#bib.bib101)]
    utilized learned distributions to sample, score, and refine pose hypotheses. Correspondence
    distributions are learned using a contrastive loss. This method is unsupervised
    regarding visual ambiguities. More recently, Li *et al.*[[102](#bib.bib102)] introduced
    a weakly-supervised reconstruction-based pipeline. Initially, they reconstructed
    the objects from various viewpoints using an implicit neural representation. Subsequently,
    they trained a network to predict pixel-wise 2D-3D correspondences. Hai *et al.*[[144](#bib.bib144)]
    proposed a refinement strategy that uses the geometry constraint in synthetic-to-real
    image pairs captured from multiple viewpoints.
  prefs: []
  type: TYPE_NORMAL
- en: There are also methods that focus on pose refinement. Lipson *et al.*[[145](#bib.bib145)]
    iteratively refined pose and correspondences in a tightly coupled manner. They
    incorporated a differentiable layer to refine the pose by solving the bidirectional
    depth-augmented PnP problem. In addition, Xu *et al.*[[103](#bib.bib103)] formulated
    object pose refinement as a non-linear least squares problem using the estimated
    correspondence field, *i.e.*, the correspondence between the RGB image and the
    rendered image using the initial pose. The non-linear least squares problem is
    then solved by a differentiable levenberg-marquardt algorithm [[146](#bib.bib146)],
    enabling end-to-end training.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the aforementioned correspondence-based methods exhibit robustness
    to occlusion since they can utilize local correspondences to predict object pose.
    However, these methods may encounter challenges when handling objects that lack
    salient shape features or texture.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Template-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By leveraging global information from the image, template-based methods can
    effectively address the challenges posed by texture-less objects. Template-based
    methods involve identifying the most similar template from a set of templates
    labeled with ground-truth object poses. They can be categorized into RGB-based
    template (Sec. [3.2.1](#S3.SS2.SSS1 "3.2.1 RGB-Based Template Methods ‣ 3.2 Template-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) and point cloud-based template (Sec.
    [3.2.2](#S3.SS2.SSS2 "3.2.2 Point Cloud-Based Template Methods ‣ 3.2 Template-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) methods. These two methods are illustrated
    in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey"). When the input is an RGB image, the
    templates comprise 2D projections extracted from object CAD models, with annotations
    of ground-truth poses. This process transforms object pose estimation into image
    retrieval. Conversely, when dealing with a point cloud, the template comprises
    the object CAD model with the canonical pose. Notably, we classify the methods
    that directly regress the relative pose between the object CAD model and the observed
    point cloud as template-based methods. This is because these methods can be interpreted
    as seeking the optimal relative pose that aligns the observed point cloud with
    the template. Consequently, the determined relative pose serves as the object
    pose. The characteristics and performance of some representative methods are shown
    in Table [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 RGB-Based Template Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a seminal contribution, Sundermeyer *et al.*[[104](#bib.bib104)] achieved
    3D rotation estimation through a variant of denoising autoencoder, which learns
    an implicit representation of object rotation. If depth is available, it can be
    used for pose refinement. Liu *et al.*[[147](#bib.bib147)] developed a CNN akin
    to an autoencoder to reconstruct arbitrary scenes featuring the target object
    and extract the object area. In addition, Zhang *et al.*[[148](#bib.bib148)] utilized
    an object detector and a keypoint extractor to simplify the template search process.
    Papaioannidis *et al.*[[105](#bib.bib105)] suggested that estimating object poses
    in synthetic images is more straightforward. Therefore, they employed a generative
    adversarial network to convert real images into synthetic ones while preserving
    the object pose. Li *et al.*[[106](#bib.bib106)] utilized a new pose representation
    (*i.e.*, 3D location field) to guide an auto-encoder to distill pose-related features,
    thereby enhancing the handling of pose ambiguity. Stevšič *et al.*[[149](#bib.bib149)]
    proposed a spatial attention mechanism to identify and utilize spatial details
    for pose refinement. Different from the above methods, Deng *et al.*[[107](#bib.bib107)]
    addressed the 6DoF object pose tracking problem within the Rao-Blackwellized particle
    filtering [[150](#bib.bib150)] framework. They finely discretized the rotation
    space and trained an autoencoder network to build a codebook of feature embeddings
    for these discretized rotations. This method efficiently estimates the 3D translation
    along with the full distribution over the 3D rotation.
  prefs: []
  type: TYPE_NORMAL
- en: RGB cameras are widely used as visual sensors, yet they struggle to capture
    sufficient information under poor lighting conditions. This results in poor pose
    estimation performance.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Point Cloud-Based Template Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: With the popularity of consumer-grade 3D cameras, point cloud-based methods
    take full advantage of their ability to adapt to poor illumination and capture
    geometric information. Li *et al.*[[70](#bib.bib70)] adopted a feature disentanglement
    and alignment module to establish part-to-part correspondences between the partial
    point cloud and object CAD model, enhancing geometric constraint. Jiang *et al.*[[108](#bib.bib108)]
    proposed a point cloud registration framework based on the SE(3) diffusion model,
    which gradually perturbs the optimal rigid transformation of a pair of point clouds
    by continuously injecting perturbation transformation through the SE(3) forward
    diffusion process. Then, the SE(3) reverse denoising process is used to gradually
    denoise, making it closer to the optimal transformation for accurate pose estimation.
    Dang *et al.*[[109](#bib.bib109)] proposed two key contributions to enhance pose
    estimation performance on real-world data. First, they introduced a directly supervised
    loss function that bypasses the SVD[[137](#bib.bib137)] operation, mitigating
    the sensitivity of SVD-based loss functions to the rotation range between the
    input partial point cloud and the object CAD model. Second, they devised a match
    normalization strategy to address disparities in feature distributions between
    the partial point cloud and the CAD model.
  prefs: []
  type: TYPE_NORMAL
- en: In general, template-based methods leverage global information from the image,
    enabling them to effectively handle texture-less objects. However, achieving high
    pose estimation accuracy may lead to increased memory usage by the templates and
    a rapid rise in computational complexity. Additionally, they may also exhibit
    poor performance when confronted with occluded objects.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Voting-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Voting-based methods determine object pose through a pixel-level or point-level
    voting scheme, which can be categorized into two main types: indirect voting and
    direct voting. Indirect voting methods (Sec. [3.3.1](#S3.SS3.SSS1 "3.3.1 Indirect
    Voting Methods ‣ 3.3 Voting-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")) estimate
    a set of pre-defined 2D keypoints from the RGB image through pixel-level voting,
    or a set of pre-defined 3D keypoints from the point cloud via point-level voting.
    Subsequently, the object pose is determined through 2D-3D or 3D-3D keypoint correspondences
    between the input image and the CAD model. Direct voting methods (Sec. [3.3.2](#S3.SS3.SSS2
    "3.3.2 Direct Voting Methods ‣ 3.3 Voting-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) directly predict the pose and confidence at the pixel-level or point-level,
    then select the pose with the highest confidence as the object pose. The illustration
    of these types of methods is shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Sparse
    Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"). The attributes and performance of some representative methods are shown
    in Table [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Indirect Voting Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Some researchers predicted 2D keypoints and then derived the object pose through
    2D-3D keypoints correspondence. Liu *et al.*[[151](#bib.bib151)] introduced a
    continuous representation method called keypoint distance field (KDF), which extracts
    2D keypoints by voting on each KDF. Meanwhile, Cao *et al.*[[110](#bib.bib110)]
    proposed a method called dynamic graph PnP[[73](#bib.bib73)] to learn the object
    pose from 2D-3D correspondence, enabling end-to-end training. Moreover, Liu *et
    al.*[[152](#bib.bib152)] introduced a bidirectional depth residual fusion network
    to fuse RGBD information, thereby estimating 2D keypoints precisely. Inspired
    by the diffusion model, Xu *et al.*[[153](#bib.bib153)] proposed a diffusion-based
    framework to formulate 2D keypoint detection as a denoising process to establish
    more accurate 2D-3D correspondences.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the aforementioned methods that predict 2D keypoints, He *et al.*[[18](#bib.bib18)]
    proposed a depth hough voting network to predict 3D keypoints. Subsequently, they
    estimated the object pose through levenberg-marquardt algorithm[[146](#bib.bib146)].
    Furthermore, He *et al.*[[21](#bib.bib21)] introduced a bidirectional fusion network
    to complement RGB and depth heterogeneous data, thereby better predicting the
    3D keypoints. To better capture features among object points in 3D space, Mei
    *et al.*[[154](#bib.bib154)] utilized graph convolutional networks to facilitate
    feature exchange among points in 3D space, aiming to improve the accuracy of predicting
    3D keypoints. Wu *et al.*[[111](#bib.bib111)] proposed a 3D keypoint voting scheme
    based on cross-spherical surfaces, allowing for generating smaller and more dispersed
    3D keypoint sets, thus improving estimation efficiency. To obtain more accurate
    3D keypoints, Wang *et al.*[[155](#bib.bib155)] presented an iterative 3D keypoint
    voting network to refine the initial localization of 3D keypoints. Most recently,
    Zhou *et al.*[[112](#bib.bib112)] introduced a novel weighted vector 3D keypoints
    voting algorithm, which adopts a non-iterative global optimization strategy to
    precisely localize 3D keypoints, while also achieving near real-time inference
    speed.
  prefs: []
  type: TYPE_NORMAL
- en: In response to challenging scenarios such as cluttered or occlusion, Peng *et
    al.*[[17](#bib.bib17)] introduced a pixel-wise voting network to regress pixel-level
    vectors pointing to 3D keypoints. These vectors create a flexible representation
    for locating occluded or truncated 3D keypoints. Since most industrial parts are
    parameterized, Zeng *et al.*[[156](#bib.bib156)] defined 3D keypoints linked to
    parameters through driven parameters and symmetries. This approach effectively
    addresses the pose estimation of objects in stacking scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Rather than utilizing a single-view RGBD image as input, Duffhauss *et al.*[[157](#bib.bib157)]
    employed multi-view RGBD images as input. They extracted visual features from
    each RGB image, while geometric features were extracted from the object point
    cloud (generated by fusing all depth images). This multi-view RGBD feature fusion-based
    method can accurately predict object pose in cluttered scenes.
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers have proposed new training strategies to improve pose estimation
    performance. Yu *et al.*[[158](#bib.bib158)] developed a differentiable proxy
    voting loss that simulates hypothesis selection during the voting process, enabling
    end-to-end training. In addition, Lin *et al.*[[159](#bib.bib159)] proposed a
    novel learning framework, which utilizes the accurate result of the RGBD-based
    pose refinement method to supervise the RGB-based pose estimator. To bridge the
    domain gap between synthetic and real data, Ikeda *et al.*[[160](#bib.bib160)]
    introduced a method to transfer object style transfer from synthetic to realistic
    without manual intervention.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, indirect voting-based methods provide an excellent solution for instance-level
    object pose estimation. However, the accuracy of pose estimation heavily relies
    on the quality of the keypoints, which can result in lower robustness.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Direct Voting Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The performance of the indirect voting methods heavily depends on the selection
    of keypoints. Consequently, direct voting methods have been proposed as an alternative
    solution. Tian *et al.*[[113](#bib.bib113)] uniformly sampled rotation anchors
    in SO(3). Subsequently, they predicted constraint deviations for each anchor towards
    the target, using the uncertainty score to select the best prediction. Then, they
    detected the 3D translation by aggregating point-to-center vectors towards the
    object center to recover the 6DoF pose. Wang *et al.*[[16](#bib.bib16)] fused
    RGB and depth features on a per-pixel basis and utilized a pose predictor to generate
    6DoF pose and confidence for each pixel. Subsequently, they selected the pose
    of the pixel with the highest confidence as the final pose. Zhou *et al.*[[161](#bib.bib161)]
    employed CNNs[[162](#bib.bib162)] to extract RGB features, which are then integrated
    into the point cloud to obtain fused features. Unlike [[16](#bib.bib16)], the
    fused features take the form of point sets rather than feature mappings.
  prefs: []
  type: TYPE_NORMAL
- en: However, the aforementioned RGBD fusion methods merely concatenate RGB and depth
    features without delving into their intrinsic relationship. Therefore, Zhou *et
    al.*[[114](#bib.bib114)] proposed a new multi-modal fusion graph convolutional
    network to enhance the fusion of RGB and depth images, capturing the inter-modality
    correlations through local information propagation. Liu *et al.*[[163](#bib.bib163)]
    decoupled scale-related and scale-invariant information in the depth image to
    guide the network in perceiving the scene’s 3D structure and provide scene texture
    for the RGB image feature extraction. Unlike the aforementioned approaches that
    use still images, Mu *et al.*[[164](#bib.bib164)] proposed a time fusion model
    integrating temporal motion information from RGBD images for 6DoF object pose
    estimation. This method effectively captures object motion and changes, thereby
    enhancing pose estimation accuracy and stability.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric objects may have multiple true poses, leading to ambiguity in pose
    estimation. To address this issue, Mo*et al.*[[115](#bib.bib115)] designed a symmetric-invariant
    pose distance metric, which enables the network to estimate symmetric objects
    accurately. Cai *et al.*[[165](#bib.bib165)] introduced a 3D rotation representation
    to learn the object implicit symmetry, eliminating the need for additional prior
    knowledge about object symmetry.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce dependency on annotated real data, Zeng *et al.*[[166](#bib.bib166)]
    trained their model solely on synthetic dataset. Then, they utilized a sim-to-real
    learning network to improve their generalization ability. During pose estimation,
    they transformed scene points into centroid space and obtained object pose through
    clustering and voting.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, voting-based methods have demonstrated superior performance in pose
    estimation tasks. However, the voting process is time-consuming and increases
    computational complexity [[167](#bib.bib167)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Regression-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Regression-based methods aim to directly obtain the object pose from the learned
    features. They can be divided into two main types: geometry-guided regression
    and direct regression. Geometry-guided regression methods (Sec. [3.4.1](#S3.SS4.SSS1
    "3.4.1 Geometry-Guided Regression Methods ‣ 3.4 Regression-Based Methods ‣ 3 Instance-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) leverage geometric information from RGBD images (such as object 3D structural
    features or 2D-3D geometric constraints) to assist in object pose estimation.
    Direct regression methods (Sec. [3.4.2](#S3.SS4.SSS2 "3.4.2 Direct Regression
    Methods ‣ 3.4 Regression-Based Methods ‣ 3 Instance-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")) directly
    regress the object pose, utilizing RGBD image information. The illustration of
    these two types of methods is shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Sparse
    Correspondence Methods ‣ 3.1 Correspondence-Based Methods ‣ 3 Instance-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"). The attributes and performance of some representative methods are shown
    in Table [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Geometry-Guided Regression Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gao *et al.*[[168](#bib.bib168)] employed decoupled networks for rotation and
    translation regression from the object point cloud. Meanwhile, Chen *et al.*[[117](#bib.bib117)]
    introduced a rotation residual estimator to estimate the residual between the
    predicted rotation and the ground truth, enhancing the accuracy of rotation prediction.
    Lin *et al.*[[169](#bib.bib169)] used a network to extract the geometric features
    of the object point cloud. Then, they enhanced the pairwise consistency of geometric
    features by applying spectral convolution on pairwise compatibility graphs. Additionally,
    Shi *et al.*[[170](#bib.bib170)] learned geometric and contextual features within
    point cloud blocks. Then, they trained a sub-block network to predict the pose
    of each point cloud block. Finally, the most reliable block pose is selected as
    the object pose. To address the challenge of point cloud-based object pose tracking,
    Liu *et al.*[[171](#bib.bib171)] proposed a shifted point convolution operation
    between the point clouds of adjacent frames to facilitate the local context interaction.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches solely relying on object point cloud often overlook the object texture
    details. Therefore, Wen *et al.*[[172](#bib.bib172)] and An *et al.*[[173](#bib.bib173)]
    leveraged the complementary nature of RGB and depth information. They improved
    cross-modal fusion strategies by employing attention mechanisms to effectively
    align and integrate these two heterogeneous data sources, resulting in enhanced
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: In contrast to the aforementioned methods that directly derive geometric information
    from the depth image or the object CAD model, many researchers focused more on
    generating geometric constraints from the RGB image. Hu *et al.*[[118](#bib.bib118)]
    learned the 2D offset from the CAD model center to the 3D bounding box corners
    from the RGB image, and then directly regressed the object pose from the 2D-3D
    correspondence. Di *et al.*[[121](#bib.bib121)] used a shared encoder and two
    independent decoders to generate 2D-3D correspondence and self-occlusion information,
    improving the robustness of object pose estimation under occlusion. Further, Wang
    *et al.*[[120](#bib.bib120)] proposed a Geometry-Guided Direct Regression Network
    (GDR-Net) to learn object pose from dense 2D-3D correspondence in an end-to-end
    manner. Wang *et al.*[[122](#bib.bib122)] introduced noise-augmented student training
    and differentiable rendering based on GDR-Net[[120](#bib.bib120)], enabling robustness
    to occlusion scenes through self-supervised learning with multiple geometric constraints.
    Zhang *et al.*[[174](#bib.bib174)] proposed a transformer-based pose estimation
    approach that consists of a patch-aware feature fusion module and a transformer-based
    pose refinement module to address the limitation of CNN-based networks in capturing
    global dependencies. Most recently, Feng *et al.*[[175](#bib.bib175)] decoupled
    rotation into two sets of corresponding 3D normals. This decoupling strategy significantly
    improves the rotation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Given the labor-intensive nature of real-world data annotation, some methods
    leverage synthetic data training to generalize to the real world. Gao *et al.*[[176](#bib.bib176)]
    constructed a lightweight synthetic point cloud generation pipeline and leveraged
    an enhanced point cloud-based autoencoder to learn latent object pose information
    to regress object pose. To improve generalization to real-world scenes, Zhou *et
    al.* [[177](#bib.bib177)] utilized annotated synthetic data to supervise the network
    convergence. They proposed a self-supervised pipeline for unannotated real data
    by minimizing the distance between the CAD model transformed from the predicted
    pose and the input point cloud. Tan *et al.* [[178](#bib.bib178)] proposed a self-supervised
    monocular object pose estimation network consisting of teacher and student modules.
    The teacher module is trained on synthetic data for initial object pose estimation,
    and the student model predicts camera pose from the unannotated real image. The
    student module acquires knowledge of object pose estimation from the teacher module
    by imposing geometric constraints derived from the camera pose.
  prefs: []
  type: TYPE_NORMAL
- en: Geometry-guided regression methods typically require additional processing steps
    to extract and handle geometric information, which increases computational costs
    and complexity.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Direct Regression Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Direct regression methods aim to directly recover the object pose from the RGBD
    image without additional transformation steps, thus reducing complexity. These
    methods encompass various strategies, including coupled pose output, decoupled
    pose output, and 3D rotation (3DoF pose) output. Coupled pose involves predicting
    object rotation and translation together, while decoupled pose involves predicting
    them separately. Moreover, the 3DoF pose output focuses solely on predicting object
    rotation without considering translation. These strategies are discussed in detail
    below.
  prefs: []
  type: TYPE_NORMAL
- en: 'Coupled Pose: To overcome lighting variations in the environment, Rambach *et
    al.*[[179](#bib.bib179)] used a pencil filter to normalize the input image into
    light-invariant representations, and then directly regressed the object coupled
    pose using a CNN network. Additionally, Kleeberger *et al.*[[180](#bib.bib180)]
    introduced a robust framework to handle occlusions between objects and estimate
    the multiple objects pose in the image. This framework is capable of running in
    real-time at 65 FPS. Sarode *et al.*[[181](#bib.bib181)] introduced a PointNet-based[[182](#bib.bib182)]
    framework to align point clouds for pose estimation, aiming to reduce sensitivity
    to pose misalignment. Estimating object pose from a single RGB image introduces
    an inherent ambiguity problem. Manhardt *et al.*[[126](#bib.bib126)] suggested
    explicitly addressing these ambiguities. They predicted multiple 6DoF poses for
    each object to estimate specific pose distributions caused by symmetry and repetitive
    textures. Inspired by the visible surface difference metric, Bengtson *et al.*[[183](#bib.bib183)]
    relied on a differentiable renderer and the CAD model to generate multiple weighted
    poses, avoiding falling into local minima. Moreover, Park *et al.*[[184](#bib.bib184)]
    proposed a method for pose estimation based on the local grid in object space.
    The method locates the grid region of interest on a ray in camera space and transforms
    the grid into object space via the estimated pose. The transformed grid is a new
    standard for sampling mesh and estimating pose.'
  prefs: []
  type: TYPE_NORMAL
- en: For object pose tracking, Garon *et al.*[[185](#bib.bib185)] proposed a real-time
    tracking method that learns transformation relationships from consecutive frames
    during training, and used FCN [[186](#bib.bib186)] to obtain the relative pose
    between two frames for training and inference.
  prefs: []
  type: TYPE_NORMAL
- en: Coupled pose may lead to information coupling between rotation and translation,
    making it difficult to distinguish their relationship during the optimization
    process, thus affecting estimation accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Decoupled Pose: Decoupling the 6DoF object pose enables explicit modeling of
    the dependencies and independencies between object rotation and translation[[15](#bib.bib15)].'
  prefs: []
  type: TYPE_NORMAL
- en: In object pose estimation, Xiang *et al.*[[15](#bib.bib15)] estimated the 3D
    translation by locating the object center in the image and predicting the distance
    from the object center to the camera. They further estimated the 3D rotation by
    regressing to a quaternion representation, and introduced a novel loss function
    to handle symmetric objects better. Meanwhile, Kehl *et al.*[[187](#bib.bib187)]
    extended the SSD framework[[188](#bib.bib188)] to generate 2D bounding boxes,
    as well as confidence scores for each viewpoint and in-plane rotation. Then, they
    chose the 2D bounding box through non-maximum suppression, along with the highest
    confidence viewpoint and in-plane rotation to infer the 3D translation, resulting
    in the full 6DoF object pose. Wu *et al.*[[189](#bib.bib189)], Do*et al.*[[190](#bib.bib190)]
    and Bukschat *et al.*[[167](#bib.bib167)] used two parallel FCN [[186](#bib.bib186)]
    branches to regress the object rotation and translation independently. To eliminate
    the dependence on annotations of real data, Wang *et al.*[[68](#bib.bib68)] used
    synthetic RGB data for fully supervised training, and then leveraged neural rendering
    for self-supervised learning on unannotated real RGBD data. Moreover, Jiang *et
    al.*[[69](#bib.bib69)] fused RGBD, built-in 2D-pixel coordinate encoding, and
    depth normal vector features to better estimate the object rotation and translation.
    Single-view methods suffer from ambiguity, therefore, Li *et al.*[[123](#bib.bib123)]
    proposed a multi-view fusion framework to reduce the ambiguity inherent in single-view
    frameworks. Further, Labbé *et al.*[[119](#bib.bib119)] proposed a unified approach
    for multi-view, multi-object pose estimation. Initially, they utilized a single-view,
    single-object pose estimation technique to derive pose hypotheses for individual
    objects. Then, they aligned these object pose hypotheses across multiple input
    images to collectively infer both the camera viewpoints and object pose within
    a unified scene. Most recently, Hsiao *et al.*[[191](#bib.bib191)] introduced
    a score-based diffusion method to solve the pose ambiguity problem in RGB-based
    object pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: For object pose tracking, Wen *et al.*[[48](#bib.bib48)] proposed a data-driven
    optimization strategy to stabilize the 6DoF object pose tracking. Specifically,
    they predicted the 6DoF pose by predicting the relative pose between the adjacent
    frames. Liu *et al.*[[192](#bib.bib192)] proposed a new subtraction feature fusion
    module based on [[48](#bib.bib48)] to establish sufficient spatiotemporal information
    interaction between adjacent frames, improving the robustness of object pose tracking
    in complex scenes. Different from the method based on RGBD input, Ge *et al.*[[193](#bib.bib193)]
    designed a novel deep neural network architecture that integrates visual and inertial
    features to predict the relative object pose between consecutive image frames.
  prefs: []
  type: TYPE_NORMAL
- en: In object pose refinement, Li *et al.*[[124](#bib.bib124)] iteratively refined
    pose through aligning RGB image with rendered image of object CAD model. Additionally,
    they predicted optical flow and foreground masks to stabilize the training procedure.
    Manhardt *et al.*[[125](#bib.bib125)] refined the 6DoF pose by aligning object
    contour between the RGB image and rendered contour. The rendered contour is obtained
    from the object CAD model using the initial pose. Hai *et al.*[[129](#bib.bib129)]
    proposed a shape-constraint recursive matching framework to refine the initial
    pose. They first computed a pose-induced flow based on the initial and currently
    estimated pose, and then directly decoupled the 6DoF pose from the pose-induced
    flow. To address the low running efficiency of the pose refinement methods, Iwase
    *et al.*[[194](#bib.bib194)] introduced a deep texture rendering-based pose refinement
    method for fast feature extraction using an object CAD model with a learnable
    texture. Most recently, Li *et al.*[[130](#bib.bib130)] proposed a two-stage method.
    The first stage performs pose classification and renders the object CAD model
    in the classified poses. The second stage performs regression to predict fine-grained
    residual in the classified poses. This method improves robustness by guiding residual
    pose regression through pose classification.
  prefs: []
  type: TYPE_NORMAL
- en: '3DoF Pose: Some researchers pursue more efficient and practical pose estimation
    by solely regressing the 3D rotation. Papaioannidis *et al.*[[127](#bib.bib127)]
    proposed a novel quaternion-based multi-objective loss function, which integrates
    manifold learning and regression for learning 3DoF pose descriptors. They obtained
    the 3DoF pose through the regression of the learned descriptors. Liu *et al.*[[128](#bib.bib128)]
    trained a triple network based on convolutional neural networks to extract discriminative
    features from binary images. They incorporated pose-guided methods and regression
    constraints into the constructed triple network to adapt the features for the
    regression task, enhancing robustness. In addition, Josifovski *et al.*[[195](#bib.bib195)]
    estimated the camera viewpoint related to the object coordinate system by constructing
    a viewpoint estimation model, thereby obtaining the 3DoF pose appearing in the
    bounding box.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, direct regression methods simplify the object pose estimation process
    and further enhance the performance of instance-level methods. However, instance-level
    methods can only estimate specific object instances in the training data, limiting
    their generalization to unseen objects. Additionally, most instance-level methods
    require accurate object CAD models, which is a challenge, especially for objects
    with complex shapes and textures.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Category-Level Object Pose Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Research on category-level methods has garnered significant attention due to
    their potential for generalizing to unseen objects within established categories
    [[196](#bib.bib196)]. In this section, we review category-level methods by dividing
    them into shape prior-based (Sec. [4.1](#S4.SS1 "4.1 Shape Prior-Based Methods
    ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) and shape prior-free (Sec. [4.2](#S4.SS2 "4.2 Shape
    Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) methods. The illustration of
    these two categories is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey"). The characteristics and performance of some representative SOTA methods
    are shown in Table [II](#S4.T2 "TABLE II ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6eb485ec579b51ebdaefe72f772aabeb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Illustration of the shape prior-based (Sec. [4.1](#S4.SS1 "4.1 Shape
    Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) and shape prior-free (Sec. [4.2](#S4.SS2
    "4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")) category-level
    methods. The dashed arrows indicate offline training, which means that we need
    to train a model offline using the category-level model library to obtain shape
    priors. (Sec. [4.1](#S4.SS1 "4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")): Taking RGBD input as an example, NOCS shape alignment methods (Sec.
    [4.1.1](#S4.SS1.SSS1 "4.1.1 NOCS Shape Alignment Methods ‣ 4.1 Shape Prior-Based
    Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) first learn a model to predict the
    NOCS shape/map of the object, and then align the object point cloud with the NOCS
    shape/map through a non-differentiable pose solution method such as the Umeyama
    algorithm [[197](#bib.bib197)] to solve the object pose. In contrast, direct regress
    pose methods (Sec. [4.1.2](#S4.SS1.SSS2 "4.1.2 Direct Regress Pose Methods ‣ 4.1
    Shape Prior-Based Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) directly regress the object
    pose from the extracted input features. On the other hand, the shape prior-free
    methods (Sec. [4.2](#S4.SS2 "4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) do not have the process of shape priors regression: Depth-guided geometry-aware
    methods (Sec. [4.2.1](#S4.SS2.SSS1 "4.2.1 Depth-Guided Geometry-Aware Methods
    ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep
    Learning-Based Object Pose Estimation: A Comprehensive Survey")) focus on perceiving
    the global and local geometric information of the object and leverage these 3D
    geometric features to estimate the object pose. Conversely, RGBD-guided semantic
    and geometry fusion methods (Sec. [4.2.2](#S4.SS2.SSS2 "4.2.2 RGBD-Guided Semantic
    and Geometry Fusion Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) regress the object pose by fusing the 2D semantic and 3D geometric information
    of the object.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Shape Prior-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Shape prior-based methods first learn a neural network using CAD models of
    intra-class seen objects in offline mode to derive shape priors, and then utilize
    them as 3D geometry prior information to guide intra-class unseen object pose
    estimation. In this part, we divide the shape prior-based methods into two categories
    based on their approach to addressing object pose estimation. The first category
    is Normalized Object Coordinate Space (NOCS) shape alignment methods (Sec. [4.1.1](#S4.SS1.SSS1
    "4.1.1 NOCS Shape Alignment Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")). They first predict the NOCS shape/map, and then use an offline pose
    solution method (such as the Umeyama algorithm[[197](#bib.bib197)]) to align the
    object point cloud with the predicted NOCS shape/map to obtain the object pose.
    The other category is the pose regression methods (Sec. [4.1.2](#S4.SS1.SSS2 "4.1.2
    Direct Regress Pose Methods ‣ 4.1 Shape Prior-Based Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")). They directly regress the object pose from the feature level, making
    the pose acquisition process differentiable. The illustration of these two categories
    is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Representative category-level methods. For each method, we report
    its 10 properties, which have the same meanings as described in Table. [I](#S3.T1
    "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"). D, S, N, K, and P denote object detection, instance
    segmentation, NOCS shape/map regression, keypoints detection, and pose solution/regression,
    respectively. Moreover, we report the $5^{\circ}$$5{\rm{cm}}$ metric of CAMERA25
    and REAL275 datasets (Sec. [2](#S2 "2 Datasets and Metrics ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Published &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Year &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pose &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DoF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Property &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Domain Training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Paradigm &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mode &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Application &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Area &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAMERA25 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $5^{\circ}$$5{\rm{cm}}$ (mAP) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; REAL275 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $5^{\circ}$$5{\rm{cm}}$ (mAP) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Shape Prior-Based Methods | NOCS shape alignment | Tian *et al.*[[72](#bib.bib72)]
    | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 59.0 | 21.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang *et al.*[[198](#bib.bib198)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 76.4 | 34.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen *et al.*[[23](#bib.bib23)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 74.5 | 39.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Zou *et al.*[[199](#bib.bib199)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 76.7 | 41.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Fan *et al.*[[200](#bib.bib200)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wei *et al.*[[201](#bib.bib201)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Direct regress pose | Irshad *et al.*[[202](#bib.bib202)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | source | end-to-end | general | 66.2 |
    29.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Lin *et al.*[[203](#bib.bib203)] | 2022 | Depth | Depth | 9DoF | rigid |
    estimation | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 70.9 | 42.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.*[[204](#bib.bib204)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Depth | 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 75.5 | 44.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.*[[205](#bib.bib205)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Depth | 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 79.6 | 48.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu *et al.*[[206](#bib.bib206)] | 2022 | Depth | Depth | 9DoF | rigid |'
  prefs: []
  type: TYPE_TB
- en: '&#124; refinement, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tracking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 80.3 | 54.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Lin *et al.*[[24](#bib.bib24)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 45.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Ze *et al.*[[55](#bib.bib55)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | adaptation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 33.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu *et al.*[[207](#bib.bib207)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 50.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Shape Prior-Free Methods | Depth-guided geometry-aware | Li *et al.*[[208](#bib.bib208)]
    | 2020 | Depth | Depth | 9DoF | articulated | estimation | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Chen *et al.*[[209](#bib.bib209)] | 2021 | Depth | Depth | 9DoF | rigid |
    estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 28.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Weng *et al.*[[210](#bib.bib210)] | 2021 | Depth | Depth | 9DoF |'
  prefs: []
  type: TYPE_TB
- en: '&#124; rigid, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; articulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| tracking | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 62.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Di *et al.*[[25](#bib.bib25)] | 2022 | Depth | Depth | 9DoF | rigid | estimation
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 79.1 | 42.9 |'
  prefs: []
  type: TYPE_TB
- en: '| You *et al.*[[211](#bib.bib211)] | 2022 | Depth | Depth | 9DoF | rigid |
    estimation | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 16.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Zheng *et al.*[[26](#bib.bib26)] | 2023 | Depth | Depth | 9DoF | rigid |
    estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 80.5 | 55.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang *et al.*[[212](#bib.bib212)] | 2023 | Depth | Depth | 6DoF | rigid
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; estimation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tracking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 60.9 |'
  prefs: []
  type: TYPE_TB
- en: '| RGBD-guided semantic $\&amp;$ geometry fusion | Wang *et al.*[[22](#bib.bib22)]
    | 2019 | RGBD | RGBD | 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (S+N)+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 40.9 | 10.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang *et al.*[[213](#bib.bib213)] | 2020 | RGBD | RGBD | 6DoF | rigid | tracking
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; K+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 33.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Lin *et al.*[[214](#bib.bib214)] | 2021 | RGBD | RGBD | 9DoF | rigid | estimation
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 70.7 | 35.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Wen *et al.*[[215](#bib.bib215)] | 2021 | RGBD | RGBD | 9DoF | rigid | tracking
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+K+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 87.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Peng *et al.*[[216](#bib.bib216)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | adaptation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 33.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Lee *et al.*[[217](#bib.bib217)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | adaptation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 34.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Lee *et al.*[[218](#bib.bib218)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGBD | 9DoF | rigid | estimation | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 35.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Liu *et al.*[[27](#bib.bib27)] | 2023 | RGBD | RGBD | 9DoF | rigid | estimation
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 79.9 | 53.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Lin *et al.*[[219](#bib.bib219)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD or &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD or &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 81.4 | 57.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Chen *et al.*[[220](#bib.bib220)] | 2024 | RGBD | RGBD | 9DoF | rigid | estimation
    | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | 63.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Others | Lee *et al.*[[221](#bib.bib221)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RGB | 9DoF | rigid | estimation | generalization |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+N+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Lin *et al.*[[222](#bib.bib222)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Text &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Text &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 9DoF | rigid | estimation | source |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 82.2 | 58.3 |'
  prefs: []
  type: TYPE_TB
- en: 4.1.1 NOCS Shape Alignment Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As a pioneering work, Tian *et al.*[[72](#bib.bib72)] first extracted the shape
    prior in offline mode, which is used to represent the mean shape of a category
    of objects. For example, mugs are composed of a cylindrical cup body and an arc-shaped
    handle. Next, they introduced a shape prior deformation network for the intra-class
    unseen object to reconstruct its NOCS shape. Finally, the Umeyama algorithm[[197](#bib.bib197)]
    is employed to solve the object pose by aligning the NOCS shape and the object
    point cloud. Following Tian *et al.*[[72](#bib.bib72)], some methods aim to reconstruct
    the NOCS shape more accurately. Specifically, Wang *et al.*[[198](#bib.bib198)]
    designed a recurrent reconstruction network to iteratively refine the reconstructed
    NOCS shape. Further, Chen *et al.*[[23](#bib.bib23)] adjusted the shape prior
    dynamically by using the structure similarity between the RGBD image and the shape
    prior. Zou *et al.*[[199](#bib.bib199)] proposed two multi-scale transformer-based
    networks (Pixelformer and Pointformer) for extracting RGB and point cloud features,
    and subsequently merging them for shape prior deformation. Different from the
    previous methods, Fan *et al.*[[223](#bib.bib223)] introduced an adversarial canonical
    representation reconstruction framework, which includes a reconstructor and a
    discriminator of NOCS representation. Specifically, the reconstructor mainly consists
    of a pose-irrelevant module and a relational reconstruction module to reduce the
    sensitivity to rotation and translation, as well as to generate high-quality features,
    respectively. Then, the discriminator is used to guide the reconstructor to generate
    realistic NOCS representations. Nie *et al.*[[224](#bib.bib224)] improved the
    accuracy of pose estimation via geometry-informed instance-specific priors and
    multi-stage shape reconstruction. More recently, Zhou *et al.*[[225](#bib.bib225)]
    designed a two-stage pipeline consisting of deformation and registration to improve
    accuracy. Zou *et al.*[[226](#bib.bib226)] introduced a graph-guided point transformer
    consisting of a graph-guided attention encoder and an iterative non-parametric
    decoder to further extract the point cloud feature. In addition, Li *et al.*[[227](#bib.bib227)]
    leveraged discrepancies in instance-category structures alongside potential geometric-semantic
    associations to better investigate intra-class shape information. Yu *et al.*[[228](#bib.bib228)]
    further divided the NOCS shape reconstruction process into three parts: coarse
    deformation, fine deformation, and recurrent refinement to enhance the accuracy
    of NOCS shape reconstruction.'
  prefs: []
  type: TYPE_NORMAL
- en: Given that the annotation of ground-truth object pose is time-consuming, He
    *et al.*[[229](#bib.bib229)] explored a self-supervised method via enforcing the
    geometric consistency between point cloud and category prior mesh, avoiding using
    the real-world pose annotation. Further, Li *et al.*[[230](#bib.bib230)] first
    extracted semantic primitives via a part segmentation network, and leveraged semantic
    primitives to compute SIM(3)-invariant shape descriptor to generate the optimized
    shape. Then, the Umeyama algorithm[[197](#bib.bib197)] is utilized to recover
    the object pose. Through this approach, they achieved domain generalization, bridging
    the gap between synthesis and real-world application.
  prefs: []
  type: TYPE_NORMAL
- en: Depth images may be unavailable in some challenging scenes (*e.g.*, under strong
    or low light conditions). Therefore, achieving monocular category-level object
    pose estimation is of great significance across various applications. Fan *et
    al.* [[200](#bib.bib200)] directly predicted object-level depth and NOCS shape
    from a monocular RGB image by deforming the shape prior, and subsequently leveraged
    the Umeyama algorithm [[197](#bib.bib197)] to solve the object pose. Unlike [[200](#bib.bib200)],
    Wei *et al.* [[201](#bib.bib201)] estimated the 2.5D sketch and separated scale
    recovery using the shape prior. They then reconstructed the NOCS shape, employing
    the RANSAC [[73](#bib.bib73)] algorithm to remove outliers, before utilizing the
    PnP algorithm for recovering the object pose. For transparent objects, Chen *et
    al.* [[231](#bib.bib231)] proposed a new solution based on stereo vision, which
    defines a back-view NOCS map to tackle the problem of image content aliasing.
  prefs: []
  type: TYPE_NORMAL
- en: In general, although these NOCS shape alignment methods can recover the object
    pose, the alignment process is non-differentiable and is not integrated into the
    learning process. Thus, errors in predicting the NOCS shape/map have a significant
    impact on the accuracy of pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Direct Regress Pose Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the non-differentiable nature of the NOCS shape alignment process, several
    direct regression-based pose methods have been proposed recently to enable end-to-end
    training. Irshad *et al.*[[202](#bib.bib202)] treated object instances as spatial
    centers and proposed an end-to-end method that combines object detection, reconstruction,
    and pose estimation. Wang *et al.*[[232](#bib.bib232)] developed a deformable
    template field to decouple shape and pose deformation, improving the accuracy
    of shape reconstruction and pose estimation. On the other hand, Zhang *et al.*[[204](#bib.bib204)]
    proposed a symmetry-aware shape prior deformation method, which integrates shape
    prior into a direct pose estimation network. Further, Zhang *et al.*[[205](#bib.bib205)]
    introduced a geometry-guided residual object bounding box projection framework
    to address the challenge of insufficient pose-sensitive feature extraction. In
    order to obtain a more precise object pose, Liu *et al.*[[206](#bib.bib206)] designed
    CATRE, a pose refinement method based on the alignment of the shape prior and
    the object point cloud to refine the object pose estimated by the above methods.
    Zheng *et al.*[[233](#bib.bib233)] extended CATRE[[206](#bib.bib206)] to address
    the geometric variation problem by integrating hybrid scope layers and learnable
    affine transformations.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the extensive manual effort required for annotating real-world training
    data, Lin *et al.*[[203](#bib.bib203)] explored the shape alignment of each intra-class
    unseen instance against its corresponding category-level shape prior, implicitly
    representing its 3D rotation. This approach facilitates domain generalization
    from synthesis to real-world scenarios. Further, Ze *et al.*[[55](#bib.bib55)]
    proposed a novel framework based on pose and shape differentiable rendering to
    achieve domain adaptation object pose estimation. In addition, they collected
    a large Wild6D dataset for category-level object pose estimation in the wild.
    Following Ze *et al.*[[55](#bib.bib55)], Zhang *et al.*[[234](#bib.bib234)] introduced
    2D-3D and 3D-2D geometry correspondences to enhance the ability of domain adaptation.
    Different from the previous approaches, Remus *et al.*[[235](#bib.bib235)] leveraged
    instance-level methods for domain-generalized category-level object pose estimation
    via a single RGB image. Lin *et al.*[[24](#bib.bib24)] proposed a deep prior deformation-based
    network and leveraged a parallel learning scheme to achieve domain generalization.
    More recently, Liu *et al.*[[207](#bib.bib207)] designed a multi-hypothesis consistency
    learning framework. This framework addresses the uncertainty problem and reduces
    the domain gap between synthetic and real-world datasets by employing multiple
    feature extraction and fusion techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, while the shape prior-based methods mentioned above significantly improve
    pose estimation performance, obtaining the shape priors requires constructing
    category-level CAD model libraries and subsequently training a network, which
    is both cumbersome and time-consuming.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Shape Prior-Free Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Shape prior-free methods do not rely on using shape priors and thus have better
    generalization capabilities. These methods can be divided into three main categories:
    depth-guided geometry-aware (Sec. [4.2.1](#S4.SS2.SSS1 "4.2.1 Depth-Guided Geometry-Aware
    Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")), RGBD-guided
    semantic and geometry fusion (Sec. [4.2.2](#S4.SS2.SSS2 "4.2.2 RGBD-Guided Semantic
    and Geometry Fusion Methods ‣ 4.2 Shape Prior-Free Methods ‣ 4 Category-Level
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")), and other (Sec. [4.2.3](#S4.SS2.SSS3 "4.2.3 Others ‣ 4.2 Shape Prior-Free
    Methods ‣ 4 Category-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) methods. The illustration of the first
    two categories is shown in Fig. [5](#S4.F5 "Figure 5 ‣ 4 Category-Level Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Depth-Guided Geometry-Aware Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Thanks to the rapid development of 3D Graph Convolution (3DGC), Chen *et al.*[[209](#bib.bib209)]
    leveraged 3DGC and introduced a fast shape-based method, which consists of an
    RGB-based network to achieve 2D object detection, a shape-based network for 3D
    segmentation and rotation regression, and a residual-based network for translation
    and size regression. Inspired by Chen *et al.*[[209](#bib.bib209)], Liu *et al.*[[236](#bib.bib236)]
    improved the network with structure encoder and reasoning attention. Further,
    Di *et al.*[[25](#bib.bib25)] proposed a geometry-guided point-wise voting method
    that exploits geometric insights to enhance the learning of pose-sensitive features.
    Specifically, they designed a symmetry-aware point cloud reconstruction network
    and introduced a point-wise bounding box voting mechanism during training to add
    additional geometric guidance. Due to the translation and scale invariant properties
    of the 3DGC, these methods are limited in perceiving object translation and size
    information. Based on this, Zheng *et al.*[[26](#bib.bib26)] further designed
    a hybrid scope feature extraction layer, which can simultaneously perceive global
    and local geometric structures and encode size and translation information.
  prefs: []
  type: TYPE_NORMAL
- en: Besides the above 3DGC-based methods, Deng *et al.*[[237](#bib.bib237)] combined
    a category-level auto-encoder with a particle filter framework to achieve object
    pose estimation and tracking. Wang *et al.*[[238](#bib.bib238)] leveraged learnable
    sparse queries as implicit prior to perform deformation and matching for pose
    estimation. In addition, Wan *et al.*[[239](#bib.bib239)] developed a semantically-aware
    object coordinate space to address the semantically incoherent problem of NOCS[[22](#bib.bib22)].
    More recently, Zhang *et al.*[[212](#bib.bib212)] proposed a scored-based diffusion
    model to address the multi-hypothesis problem in symmetric objects and partial
    point clouds. They first leveraged the scored-based diffusion model to generate
    multiple pose candidates, and then utilized an energy-based diffusion model to
    remove abnormal poses. On the other hand, Lin *et al.*[[240](#bib.bib240)] first
    introduced an instance-adaptive keypoints detection method and then designed a
    geometric-aware global and local features aggregation network based on the detected
    keypoints for pose and size estimation. Li *et al.*[[241](#bib.bib241)] leveraged
    category-level method to determine part object poses for assembling multi-part
    multi-joint 3D shape.
  prefs: []
  type: TYPE_NORMAL
- en: To perform pose estimation on articulated objects, Li *et al.*[[208](#bib.bib208)]
    inspired by Wang *et al.*[[22](#bib.bib22)], introduced a standard representation
    for different articulated objects within a category by designing an articulation-aware
    normalized coordinate space hierarchy, which simultaneously constructs a canonical
    object space and a set of canonical part spaces. Weng *et al.*[[210](#bib.bib210)]
    further proposed CAPTRA, a unified framework that enables 9DoF pose tracking of
    rigid and articulated objects simultaneously. Due to the nearly unlimited freedom
    of garments and extreme self-occlusion, Chi *et al.*[[242](#bib.bib242)] introduce
    GarmentNets, which conceptualizes deformable object pose estimation as a shape
    completion problem within a canonical space. More recently, Liu *et al.*[[243](#bib.bib243)]
    developed a reinforcement learning-based pipeline to predict 9DoF articulated
    object pose via fitting joint states through reinforced agent training. Further,
    Liu *et al.*[[244](#bib.bib244)] learned part-level SE(3)-equivariant features
    via a pose-aware equivariant point convolution operator to address the issue of
    self-supervised articulated object pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: To avoid using extensive real-world labeled data for training, Li *et al.*[[245](#bib.bib245)]
    used SE(3) equivariant point cloud networks for self-supervised object pose estimation.
    You *et al.*[[211](#bib.bib211)] introduced a category-level point pair feature
    voting method to reduce the impact of synthetic to real-world domain gap, achieving
    generalizable object pose estimation in the wild.
  prefs: []
  type: TYPE_NORMAL
- en: In general, these methods fully extract the pose-related geometric features.
    However, the absence of semantic information limits their better performance.
    Appropriate fusion of semantic and geometric information can significantly improve
    the robustness of pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 RGBD-Guided Semantic and Geometry Fusion Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As a groundbreaking research, Wang *et al.*[[22](#bib.bib22)] designed a normalized
    object coordinate space to provide a canonical representation for a category of
    objects. They first predicted the class label, mask, and NOCS map of the intra-class
    unseen object. Then, they utilized the Umeyama algorithm [[197](#bib.bib197)]
    to solve object pose by aligning the NOCS map with the object point cloud. To
    handle various shape changes of intra-class objects, Chen *et al.*[[246](#bib.bib246)]
    learned a canonical shape space as a unified representation. On the other hand,
    Lin *et al.*[[247](#bib.bib247)] explored the applicability of sparse steerable
    convolution (SSC) to object pose estimation and proposed an SSC-based pipeline.
    Further, Lin *et al.*[[214](#bib.bib214)] proposed a dual pose network, which
    consists of a shared pose encoder and two parallel explicit and implicit pose
    decoders. The implicit decoder can enforce predicted pose consistency when there
    are no CAD models during inference. Wang *et al.*[[248](#bib.bib248)] designed
    an attention-guided network with relation-aware and structure-aware for RGB image
    and point cloud features fusion. Very recently, Liu *et al.*[[27](#bib.bib27)]
    explored the necessity of shape priors for shape reconstruction of intra-class
    unseen objects. They demonstrated that the deformation process is more important
    than the shape prior and proposed a prior-free implicit space transformation network.
    Lin *et al.*[[219](#bib.bib219)] addressed the poor rotation estimation accuracy
    by decoupling the rotation estimation into viewpoint and in-plane rotation. In
    addition, they also proposed a spherical feature pyramid network based on spatial
    spherical convolution to process spherical signals. With the rapid development
    of the Large Vision Model (LVM), Chen *et al.*[[220](#bib.bib220)] further leveraged
    the LVM DINOv2[[249](#bib.bib249)] to extract the SE(3)-consistent semantic features
    and fused them with object-specific hierarchical geometric features to encapsulate
    category-level information for rotation estimation.
  prefs: []
  type: TYPE_NORMAL
- en: Since the above methods still require a large amount of real-world annotated
    training data, their applicability in real-world scenes is limited. To this end,
    Peng *et al.*[[216](#bib.bib216)] proposed a real-world self-supervised training
    framework based on deep implicit shape representation. They leveraged the deep
    signed distance function[[250](#bib.bib250)] as a 3D representation to achieve
    domain adaptation from synthesis to the real world. In addition, Lee *et al.*[[217](#bib.bib217)]
    introduced a teacher-student self-supervised learning mechanism. They used supervised
    training in the source domain and self-supervised training in the target domain,
    effectively achieving domain adaptation. Recently, Lee *et al.*[[218](#bib.bib218)]
    further proposed a test-time adaptation framework for domain-generalized category-level
    object pose estimation. Specifically, they first trained the model using labeled
    synthetic data and then leveraged the pre-trained model for test-time adaptation
    in the real world during inference.
  prefs: []
  type: TYPE_NORMAL
- en: To improve the running speed of the object pose estimation method, once the
    object pose of the first frame is acquired, continuous spatio-temporal information
    can be utilized to track the object pose. Wang *et al.*[[213](#bib.bib213)] proposed
    an anchors-based object pose tracking method. They first detected the anchors
    of each frame as the keypoints, and then solved the relative object pose through
    the keypoints correspondence. Wen *et al.*[[215](#bib.bib215)] first obtained
    continuous frame RGBD masks through the video segmentation network, transductive-VOS
    [[251](#bib.bib251)], and then leveraged LF-Net [[252](#bib.bib252)] for generalized
    keypoints detection. Next, they matched keypoints between consecutive frames and
    performed coarse registration to estimate the initial relative pose. Finally,
    a memory-augmented pose graph optimization method is proposed for continuous pose
    tracking.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, these RGBD-guided semantic and geometry fusion methods achieve superior
    performance. However, if the input depth image contains errors, the accuracy of
    pose estimation can significantly decrease. Hence, ensuring robustness in pose
    estimation when dealing with erroneous or missing depth images is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Others
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since most mobile devices are not equipped with depth cameras, Chen *et al.*[[253](#bib.bib253)]
    incorporated a neural synthesis module with a gradient-based fitting procedure
    to simultaneously predict object shape and pose, achieving monocular object pose
    estimation. Lee *et al.*[[221](#bib.bib221)] estimated the NOCS shape and the
    metric scale shape of the object, and performed a similarity transformation between
    them to solve the object pose and size. Further, Yen-Chen *et al.*[[254](#bib.bib254)]
    inverted neural radiance fields for monocular category-level pose estimation.
    Different from the previous methods, Lin *et al.*[[255](#bib.bib255)] proposed
    a keypoint-based single-stage pipeline via a single RGB image. Guo *et al.*[[256](#bib.bib256)]
    redefined the monocular category-level object pose estimation problem from a long-horizon
    visual navigation perspective. On the other hand, Ma *et al.*[[257](#bib.bib257)]
    enhanced the robustness of the monocular method in occlusion scenes through coarse-to-fine
    rendering of neural features. Given that transparent instances lack both color
    and depth information, Zhang *et al.*[[258](#bib.bib258)] proposed to utilize
    depth completion and surface normal estimation to achieve category-level pose
    estimation for transparent instances.
  prefs: []
  type: TYPE_NORMAL
- en: In order to improve the running efficiency of the monocular method, Lin *et
    al.*[[259](#bib.bib259)] developed a keypoint-based monocular object pose tracking
    approach. This approach demonstrates the significance of integrating uncertainty
    estimation using a tracklet-conditioned deep network and probabilistic filtering.
    Following Lin *et al.*[[259](#bib.bib259)], Yu *et al.*[[260](#bib.bib260)] further
    improved the pose tracking accuracy through a network that combines convolutions
    and transformers.
  prefs: []
  type: TYPE_NORMAL
- en: To further improve the generalization of category-level methods, Goodwin *et
    al.*[[261](#bib.bib261)] introduced a reference image-based zero-shot approach,
    which first extracts spatial feature descriptors and builds cyclical descriptor
    distances. Then, they established the top-k semantic correspondences for pose
    estimation. Zaccaria *et al.*[[262](#bib.bib262)] proposed a self-supervised framework
    via optical flow consistency. Very recently, Cai *et al.*[[263](#bib.bib263)]
    developed an open-vocabulary framework that aims to generalize to unseen categories
    using textual prompts in unseen scene images. Felice *et al.*[[264](#bib.bib264)]
    explored zero-shot novel view synthesis based on a diffusion model for 3D object
    reconstruction, and recovered the object pose through correspondences. Lin *et
    al.*[[222](#bib.bib222)] used a pre-trained vision-language model to make full
    use of rich semantic knowledge and align the representations of the three modalities
    (image, point cloud, and text) in the feature space through multi-modal contrastive
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: On the whole, these shape prior-free methods circumvent the reliance on shape
    priors and further improve the generalization ability of category-level object
    pose estimation methods. Nevertheless, these methods are limited to generalizing
    within intra-class unseen objects. For objects of different categories, the training
    data need to be collected and the models need to be retrained, which remains a
    significant limitation.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Unseen Object Pose Estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unseen object pose estimation methods can generalize to unseen objects without
    the need for retraining. Point Pair Features (PPF) [[11](#bib.bib11)] is a classical
    method for unseen object pose estimation that utilizes oriented point pair features
    to build global model description and a fast voting scheme to match locally. The
    final pose is solved by pose clustering and iterative closest point [[137](#bib.bib137)]
    refinement. However, PPF suffers from low accuracy and slow runtime, limiting
    its applicability. In contrast, deep learning-based methods leverage neural networks
    to learn more complex features from data without specifically designed feature
    engineering, thus enhancing accuracy and efficiency. In this section, we review
    the deep learning-based unseen object pose estimation methods and classify them
    into CAD model-based (Sec. [5.1](#S5.SS1 "5.1 CAD Model-Based Methods ‣ 5 Unseen
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) and manual reference view-based (Sec. [5.2](#S5.SS2 "5.2 Manual Reference
    View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) methods. The illustration of these
    two categories of methods is shown in Fig. [6](#S5.F6 "Figure 6 ‣ 5.1 CAD Model-Based
    Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 CAD Model-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The CAD model-based methods involve utilizing the object CAD model as prior
    knowledge during the process of estimating the pose of an unseen object. These
    methods can be further categorized into feature matching-based and template matching-based
    methods. Feature matching-based methods (Sec. [5.1.1](#S5.SS1.SSS1 "5.1.1 Feature
    Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")) focus
    on designing a network to match features between the CAD model and the query image,
    establishing 2D-3D or 3D-3D correspondences, and solving the pose by the PnP algorithm
    or least squares method. Template matching-based methods (Sec. [5.1.2](#S5.SS1.SSS2
    "5.1.2 Template Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) utilize rendered templates from the CAD model for retrieval. The initial
    pose is acquired based on the most similar template, and further refinement is
    necessary using a refiner to obtain a more accurate pose. The illustration of
    these two categories of methods is shown in Fig. [6](#S5.F6 "Figure 6 ‣ 5.1 CAD
    Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey"). The characteristics and performance
    of some representative methods are shown in Table [III](#S5.T3 "TABLE III ‣ 5.1.1
    Feature Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8f5314bf8af0191f170ac1afb68891d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Illustration of the CAD model-based (Sec. [5.1](#S5.SS1 "5.1 CAD
    Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey")) and manual reference view-based (Sec.
    [5.2](#S5.SS2 "5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object Pose
    Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"))
    methods for unseen object pose estimation. (Sec. [5.1](#S5.SS1 "5.1 CAD Model-Based
    Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")): The feature matching-based methods (Sec. [5.1.1](#S5.SS1.SSS1
    "5.1.1 Feature Matching-Based Methods ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen
    Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")) focus on designing a network to match features between the CAD model
    and the query image, establishing correspondences (2D-3D or 3D-3D), and solving
    the pose using the PnP algorithm or least squares method. The template matching-based
    methods (Sec. [5.1.2](#S5.SS1.SSS2 "5.1.2 Template Matching-Based Methods ‣ 5.1
    CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) utilize rendered templates from
    the CAD model for retrieval. The initial pose is acquired based on the most similar
    template, and further refinement is necessary using a refiner to obtain a more
    accurate pose. (Sec. [5.2](#S5.SS2 "5.2 Manual Reference View-Based Methods ‣
    5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")): There are two types of feature matching-based methods
    (Sec. [5.2.1](#S5.SS2.SSS1 "5.2.1 Feature Matching-Based Methods ‣ 5.2 Manual
    Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")). One involves extracting features
    from reference views and the query image, obtaining 3D-3D correspondences through
    a feature matching network. The other initially reconstructs the 3D object representation
    using the reference views and establishes the 2D-3D correspondences between the
    query image and the 3D representation. The object pose is solved using correspondence-based
    algorithms, like PnP or the least squares method. Template matching-based methods
    (Sec. [5.2.2](#S5.SS2.SSS2 "5.2.2 Template Matching-Based Methods ‣ 5.2 Manual
    Reference View-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")) also have two types. One reconstructs
    the 3D object representation using the reference views and then renders multiple
    templates. The initial pose is acquired by retrieving the most similar template
    and then refining it to get the final pose. The other directly uses the reference
    views as the templates for template matching.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 Feature Matching-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As an early exploratory work, Pitteri *et al.*[[265](#bib.bib265)] proposed
    a 3DoF pose estimation approach that approximates object’s geometry using only
    the corner points of the CAD model. Nonetheless, it only works effectively on
    objects having specific corners. Hence, Pitteri *et al.*[[266](#bib.bib266)] further
    introduced an embedding that captures the local geometry of 3D points on the object
    surface. Matching these embeddings can create 2D-3D correspondences, and the pose
    is then determined using the PnP+RANSAC[[73](#bib.bib73)] algorithm. However,
    these methods only estimate the 3DoF pose.
  prefs: []
  type: TYPE_NORMAL
- en: Gou *et al.*[[267](#bib.bib267)] defined the challenge of estimating the 6DoF
    pose of unseen objects, offering a baseline solution through the identification
    of 3D correspondences between object and scene point clouds. Similarly, Hagelskjær
    *et al.*[[268](#bib.bib268)] trained a network to match keypoints from the CAD
    model to the object point cloud. Yet, it focuses on bin picking with homogeneous
    bins, which only demonstrates that generalized pose estimation can achieve outstanding
    performance in restricted scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by point cloud registration methods on unseen objects, Zhao *et al.*[[269](#bib.bib269)]
    proposed a geometry correspondence-based method using generic and object-agnostic
    geometry features to establish unambiguous and robust 3D-3D correspondences. Nevertheless,
    it still needs to get the class label and segmentation mask of unseen objects
    through other methods such as Mask-RCNN [[270](#bib.bib270)]. To this end, Chen
    *et al.*[[271](#bib.bib271)] explored a framework named ZeroPose, which realizes
    joint instance segmentation and pose estimation of unseen objects. Specifically,
    they utilized the foundation model SAM[[272](#bib.bib272)] to generate possible
    object proposals and adopted a template matching method to accomplish instance
    segmentation. After that, they developed a hierarchical geometric feature matching
    network based on GeoTransformer[[273](#bib.bib273)] to establish correspondences.
    Following ZeroPose, Lin *et al.*[[30](#bib.bib30)] devised a novel matching score
    in terms of semantics, appearance, and geometry to obtain better segmentation.
    As for pose estimation, they proposed a two-stage partial-to-partial point matching
    model to construct dense 3D-3D correspondence effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Besides these methods employing geometry features, Caraffa *et al.*[[274](#bib.bib274)]
    devised a method that fuses visual and geometric features extracted from different
    pre-trained models to enhance pose prediction stability and accuracy. It is the
    first technique to estimate the unseen object pose by utilizing the synergy between
    geometric and vision foundation models. Additionally, Huang *et al.*[[275](#bib.bib275)]
    proposed a method for object pose prediction from RGBD images by combining 2D
    texture and 3D geometric cues.
  prefs: []
  type: TYPE_NORMAL
- en: To sum up, feature matching-based methods aim to extract generic object-agnostic
    features and achieve strong correspondences by matching these features. However,
    these methods require not only robust feature matching models but also tailored
    designs to enhance the representation of object features, presenting a significant
    challenge.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Representative CAD-based methods. Since the domain training paradigm
    of most unseen methods is domain generalization, we report 9 properties for each
    method, which have the same meanings as described in the caption of Table. [I](#S3.T1
    "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based Methods
    ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"). D, S, F, T, P, R, and V denote object detection, instance
    segmentation, feature matching to build correspondences, template matching to
    retrieve pose, pose solution/regression, pose refinement, and pose voting, respectively.
    We report the *BOP-M* across the LM-O and YCB-V datasets (Sec. [2](#S2 "2 Datasets
    and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"))
    for various methods. Notably, these methods use Mask-RCNN[[270](#bib.bib270)]
    (normal font), CNOS [[276](#bib.bib276)] or their own proposed methods [[271](#bib.bib271)]
    [[30](#bib.bib30)] [[277](#bib.bib277)] (bold), and a combination of PPF and SIFT
    [[278](#bib.bib278)] (italics) for unseen object location, respectively. Moreover,
    Örnek *et al.*[[279](#bib.bib279)] and Caraffa *et al.*[[274](#bib.bib274)] don’t
    require any task-specific training, we use ”$\times$” to denote it.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Published &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Year &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pose &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DoF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Property &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mode &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Application &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Area &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LM-O &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; *BOP-M* &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; YCB-V &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; *BOP-M* &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| CAD Model-Based Methods | Feature matching | Pitteri *et al.*[[266](#bib.bib266)]
    | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+F+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao *et al.*[[269](#bib.bib269)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Depth, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Depth, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+F+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 65.2 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Chen *et al.*[[271](#bib.bib271)] | 2023 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+F+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 49.1 | 57.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Lin *et al.*[[30](#bib.bib30)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+F+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 69.9 | 84.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Huang *et al.*[[275](#bib.bib275)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+F+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 56.2 | 60.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Caraffa *et al.*[[274](#bib.bib274)] | 2024 | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+F+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 69.0 | 85.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Template matching | Sundermeyer *et al.*[[280](#bib.bib280)] | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D/S+T+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Okorn *et al.*[[278](#bib.bib278)] | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; P+V &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 59.8 | 51.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Shugurov *et al.*[[277](#bib.bib277)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+T+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 46.2 | 54.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Nguyen *et al.*[[281](#bib.bib281)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| occlusion | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Labbé *et al.*[[28](#bib.bib28)] | 2022 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid |'
  prefs: []
  type: TYPE_TB
- en: '&#124; estimation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; refinement &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 58.3 | 63.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Örnek *et al.*[[279](#bib.bib279)] | 2023 | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+T+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 61.0 | 69.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Nguyen *et al.*[[29](#bib.bib29)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 63.1 | 65.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Moon *et al.*[[282](#bib.bib282)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB/RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid | refinement |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 62.9 | 82.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang *et al.*[[283](#bib.bib283)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGB, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wen *et al.*[[3](#bib.bib3)] | 2024 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RGBD, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAD Model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 6DoF | rigid |'
  prefs: []
  type: TYPE_TB
- en: '&#124; estimation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tracking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T+R+V &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | 78.8 | 88.0 |'
  prefs: []
  type: TYPE_TB
- en: 5.1.2 Template Matching-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Template matching has been widely used in computer vision and stands as an effective
    solution for tackling the pose estimation challenges posed by unseen objects.
    Wohlhart *et al.*[[67](#bib.bib67)] and Balntas *et al.*[[284](#bib.bib284)] were
    pioneers in using deep pose descriptors for object matching and pose retrieval.
    However, their descriptors are tailored to specific orientations and categories,
    limiting their utility to objects with similar appearances. In contrast, Sundermeyer
    *et al.*[[280](#bib.bib280)] proposed a single-encoder-multi-decoder network for
    jointly estimating the 3D rotation of multiple objects. This approach eliminates
    the need to segregate views of different objects in the latent space and enables
    the sharing of common features in the encoder. Yet, it still requires training
    multiple decoders. Wen *et al.*[[285](#bib.bib285)] addressed this problem by
    decoupling object shape and pose in the latent representation, enabling auto-encoding
    without the necessity of multi-path decoders for different objects, thus enhancing
    scalability.
  prefs: []
  type: TYPE_NORMAL
- en: Instead of training a network to learn features across objects, Okorn *et al.*[[278](#bib.bib278)]
    first generated candidate poses by PPF [[11](#bib.bib11)] and projected each into
    the scene. Later, they designed a scoring network to evaluate the hypothesis by
    comparing color and geometry differences between the projected object point cloud
    and RGBD image. Busam *et al.*[[286](#bib.bib286)] reformulated 6DoF pose retrieval
    as an action decision process and determined the final pose by iteratively estimating
    probable movements. Cai *et al.*[[287](#bib.bib287)] retrieved various candidate
    viewpoints from a target object viewpoint codebook, and then conducted in-plane
    2D rotational regression on each retrieved viewpoint to obtain a set of 3D rotation
    estimates. These estimates were evaluated using a consistency score to generate
    the final rotation prediction. Meanwhile, Shugurov *et al.*[[277](#bib.bib277)]
    matched the detected objects with the rendering database for initial viewpoint
    estimation. Then, they predicted the dense 2D-2D correspondences between the template
    and the image via feature matching. Pose estimation was eventually performed by
    using PnP+RANSAC[[73](#bib.bib73)] or Kabsch[[288](#bib.bib288)]+RANSAC.
  prefs: []
  type: TYPE_NORMAL
- en: Since estimating the full 6DoF pose of an unseen object is extremely challenging,
    some works focus on estimating the 3D rotation to simplify it. Different from
    the previous works [[67](#bib.bib67), [284](#bib.bib284), [289](#bib.bib289),
    [104](#bib.bib104)] that exploited a global image representation to measure image
    similarity, Nguyen *et al.*[[281](#bib.bib281)] used CNN-extracted local features
    to compare the similarity between the input image and templates, showing better
    property and occlusion robustness over global representation. Another noteworthy
    approach is an image retrieval framework based on multi-scale local similarities
    developed by Zhao *et al.*[[290](#bib.bib290)]. They extracted feature maps of
    various sizes from the input image and devised a similarity fusion module to robustly
    predict image similarity scores from multi-scale pairwise feature maps. Further,
    Thalhammer *et al.*[[291](#bib.bib291)] and Ausserlechner *et al.*[[292](#bib.bib292)]
    extended the scheme of Nguyen *et al.*[[281](#bib.bib281)] and demonstrated that
    the pre-trained Vision-Transformer (ViT) [[293](#bib.bib293)] outperforms task-specific
    fine-tuned CNN [[162](#bib.bib162)] for template matching. However, these methods
    still have a noticeable performance gap between seen and unseen objects. To this
    end, Wang *et al.*[[283](#bib.bib283)] introduced diffusion features that show
    great potential in modeling unseen objects. Furthermore, they designed three aggregation
    networks to efficiently capture and aggregate diffusion features at different
    granularities, thus improving its generalizability.
  prefs: []
  type: TYPE_NORMAL
- en: In order to further improve the generalization and robustness of the 6DoF pose
    estimation, Labbé *et al.*[[28](#bib.bib28)] used a render-and-compare approach
    and a coarse-to-fine strategy. Notably, they leveraged a large-scale 3D model
    dataset to generate a synthetic dataset containing 2 million images and over 20,000
    models. It achieved strong generalization by training the network on this dataset.
    Compared to the non-differentiable rendering pipeline of Labbé *et al.*[[28](#bib.bib28)],
    Tremblay *et al.*[[294](#bib.bib294)] utilized recent advancements in differentiable
    rendering to design a flexible refiner, allowing fine-tuning the setup without
    retraining. On the other hand, Moon *et al.*[[282](#bib.bib282)] presented a shape-constraint
    recurrent flow framework, which predicts the optical flow between the template
    and query image and refines the pose iteratively. It took advantage of shape information
    directly to improve the accuracy and scalability. Recently, Wen *et al.*[[3](#bib.bib3)]
    inherited the idea of Labbé *et al.*[[28](#bib.bib28)] and developed a novel synthesis
    data generation pipeline using emerging large-scale 3D model databases, Large
    Language Models (LLMs), and diffusion models. It greatly expanded the amount and
    diversity of data, and ultimately achieved comparable results to instance-level
    methods in a render-and-compare manner.
  prefs: []
  type: TYPE_NORMAL
- en: It is well known that template matching methods are sensitive to occlusions
    and require considerable time to match numerous templates. Therefore, Nguyen *et
    al.*[[29](#bib.bib29)] achieved rapid and robust pose estimation by finding the
    suitable trade-off between the use of template matching and patch correspondences.
    In particular, the features of the query image and templates are extracted using
    the ViT [[293](#bib.bib293)], followed by fast template matching using a sub-linear
    nearest neighbor search. The most similar template provides two DoFs for azimuth
    and elevation, while the remaining four DoFs are obtained by constructing correspondences
    between the query image and this template. Örnek *et al.*[[279](#bib.bib279)]
    utilized DINOv2[[249](#bib.bib249)] to extract descriptors for the query image
    and templates. Moreover, they introduced a fast template retrieval method based
    on visual words constructed from DINOv2 patch descriptors, thereby decreasing
    the reliance on extensive data and enhancing matching speed compared to Labbé
    *et al.*[[28](#bib.bib28)].
  prefs: []
  type: TYPE_NORMAL
- en: In summary, template matching-based methods make full use of the advantages
    provided by a multitude of templates, enabling high accuracy and strong generalization.
    Nonetheless, they have limitations in terms of time consumption, sensitivity to
    occlusions, and challenges posed by complex backgrounds and lighting variations.
  prefs: []
  type: TYPE_NORMAL
- en: Whether the above-mentioned feature matching-based or template matching-based
    methods, they both require a CAD model of the target object to provide prior information.
    In practice, accurate CAD models often require specialized hardware to build,
    which limits the practical application of these methods to a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Manual Reference View-Based Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Aside from these CAD model-based approaches, there are some manual reference
    view-based methods that do not require the unseen object CAD model as a prior
    condition but instead require providing some manual labeled reference views with
    the target object. Similar to CAD model-based methods, these methods are also
    categorized into two types: feature matching-based (Sec. [5.2.1](#S5.SS2.SSS1
    "5.2.1 Feature Matching-Based Methods ‣ 5.2 Manual Reference View-Based Methods
    ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) and template matching-based (Sec. [5.2.2](#S5.SS2.SSS2
    "5.2.2 Template Matching-Based Methods ‣ 5.2 Manual Reference View-Based Methods
    ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")) methods. These two categories of methods are illustrated
    in Fig. [6](#S5.F6 "Figure 6 ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose
    Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey").
    The attributes and performance of some representative methods are shown in Table
    [IV](#S5.T4 "TABLE IV ‣ 5.2 Manual Reference View-Based Methods ‣ 5 Unseen Object
    Pose Estimation ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Representative manual reference view-based methods. For each method,
    we report its 9 properties, which have the same meanings as described in the caption
    of Table. [I](#S3.T1 "TABLE I ‣ 3.1.1 Sparse Correspondence Methods ‣ 3.1 Correspondence-Based
    Methods ‣ 3 Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object
    Pose Estimation: A Comprehensive Survey"). D, S, F, T, P, R, and V have the same
    meanings as Table. [III](#S5.T3 "TABLE III ‣ 5.1.1 Feature Matching-Based Methods
    ‣ 5.1 CAD Model-Based Methods ‣ 5 Unseen Object Pose Estimation ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey"). We report the average recall
    of ADD(S) within 10$\%$ of the object diameter, termed as ADD(S)-0.1d (Sec. [2](#S2
    "2 Datasets and Metrics ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive
    Survey")). Notably, ”YOLOv5” and ”GT” denote the use of YOLOv5[[295](#bib.bib295)]
    and ground-truth bounding box/segmentation mask for object localization, respectively.
    For a fair comparison, we also report the number of used reference views. ”Full”
    represents all views.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Published &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Year &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Training &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Input &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pose &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DoF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Object &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Property &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Inference &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mode &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Application &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Area &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ADD(S)-0.1d &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Manual Reference View-Based Methods | Feature matching | He *et al.*[[296](#bib.bib296)]
    | 2022 | RGBD | RGBD | 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+F+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 83.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GT+16) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Sun *et al.*[[66](#bib.bib66)] | 2022 | RGB | RGB | 6DoF | rigid | estimation
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+F+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 63.6 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (YOLOv5+Full) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| He *et al.*[[2](#bib.bib2)] | 2022 | RGB | RGB | 6DoF | rigid | estimation
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+F+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 76.9 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (YOLOv5+Full) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Castro *et al.*[[297](#bib.bib297)] | 2023 | RGB | RGB | 6DoF | rigid | estimation
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+F+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 87.5 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GT+Full) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Lee *et al.*[[298](#bib.bib298)] | 2024 | RGB | RGB | 6DoF | rigid | estimation
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+F+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 78.4 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (YOLOv5+64) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Template matching | Park *et al.*[[65](#bib.bib65)] | 2020 | RGBD | RGBD
    | 6DoF | rigid | estimation |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+T+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 87.1 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GT+16) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Nguyen *et al.*[[299](#bib.bib299)] | 2022 | RGB | RGB | 6DoF | rigid | tracking
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+S+P &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - |'
  prefs: []
  type: TYPE_TB
- en: '| Liu *et al.*[[1](#bib.bib1)] | 2022 | RGB | RGB | 6DoF | rigid | estimation
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; three-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general | - |'
  prefs: []
  type: TYPE_TB
- en: '| Gao *et al.*[[300](#bib.bib300)] | 2023 | RGB | RGB | 6DoF | rigid | estimation
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; S+D+P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| occlusion | - |'
  prefs: []
  type: TYPE_TB
- en: '| Cai *et al.*[[301](#bib.bib301)] | 2024 | RGB | RGB | 6DoF | rigid | estimation
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; two-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; P+R &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 81.7 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Full) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Wen *et al.*[[3](#bib.bib3)] | 2024 | RGBD | RGBD | 6DoF | rigid |'
  prefs: []
  type: TYPE_TB
- en: '&#124; estimation, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; tracking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; four-stage, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; D+T+R+V &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| general |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 99.9 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (GT+16) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1 Feature Matching-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Different from CAD model-based feature matching methods, manual reference view-based
    feature matching methods primarily establish 3D-3D correspondences between the
    RGBD query image and RGBD reference images, or 2D-3D correspondences between the
    query image and sparse point cloud reconstructed by reference views. Subsequently,
    the object pose is solved according to the different correspondences. He *et al.*[[296](#bib.bib296)]
    proposed the first few-shot 6DoF object pose estimation method, which can estimate
    the pose of an unseen object by a few support views without extra training. Specifically,
    they desinged a dense RGBD prototype matching framework based on transformers
    to fully explore the semantic and geometric relationship between the query image
    and reference views. Corsetti *et al.*[[302](#bib.bib302)] used a textual prompt
    for object segmentation and reformulated the problem as a relative pose estimation
    between two scenes. The relative pose was obtained via point cloud registration.
  prefs: []
  type: TYPE_NORMAL
- en: Some methods took an alternative route from the perspective of matching after
    reconstruction. Wu *et al.*[[303](#bib.bib303)] developed a global registration-based
    method that used reference and query images to reconstruct full-view and single-view
    models, and then searched for point matches between the two models. Sun *et al.*[[66](#bib.bib66)]
    drew inspiration from visual localization and revised the pipeline to adapt it
    for pose estimation. More precisely, they reconstructed a Structure from Motion
    (SfM) model of the unseen object using RGB sequences from all reference viewpoints.
    Then, they matched 2D keypoints in the query image with the 3D points in the SfM
    model by a graph attention network. Nevertheless, it performed poorly on low-textured
    objects because of its reliance on repeatably detected keypoints. To deal with
    this problem, He *et al.*[[2](#bib.bib2)] designed a new keypoint-free SfM method
    to reconstruct semi-dense point cloud models of low-textured objects based on
    the detector-free feature matching method LoFTR[[304](#bib.bib304)]. Castro *et
    al.*[[297](#bib.bib297)] pointed out that these pre-trained feature matching models
    [[305](#bib.bib305), [304](#bib.bib304)] fail to capture the optimal descriptions
    for pose estimation. Based on this, they redesigned the training pipeline based
    on a three-view system for one-shot object-to-image matching.
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned works still require dense support views (*i.e.*, $\geq 32$
    views). To address this problem, Fan *et al.*[[306](#bib.bib306)] turned the 6DoF
    object pose estimation task into relative pose estimation between the retrieved
    object in the target view and the reference view. Given only one reference view,
    they achieved it by using the DINOv2 model [[249](#bib.bib249)] for global matching
    and the LoFTR model [[304](#bib.bib304)] for local matching. Note that this method
    cannot estimate absolute translation (or object scale), as this is an ill-posed
    problem when only considering two views. Beyond that, Lee *et al.*[[298](#bib.bib298)]
    applied a powerful pre-trained technique tailored for 3D vision[[307](#bib.bib307)]
    and demonstrated geometry-oriented visual pre-training can get better generalization
    capability with fewer reference views.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, due to the lack of prior geometric information from CAD models, manual
    reference view-based feature matching methods often require special designs to
    extract the geometric features of unseen objects. The number of reference views
    also constrains the actual application of such approaches to a certain extent.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2 Template Matching-Based Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Template matching-based methods mainly adopt the strategy of retrieval and
    refinement. There are two types: one reconstructs a 3D object representation using
    reference views, renders multiple templates based on this 3D representation, and
    employs a similarity network to compare the query image with each template for
    the initial pose. A refiner is then used to refine this initial pose for increased
    accuracy. The other directly uses reference views as templates, requiring plenty
    of views for retrieval and greater reliance on a refiner for accuracy. Park *et
    al.*[[65](#bib.bib65)] introduced a novel framework for pose estimation of unseen
    objects without the CAD model. They reconstructed 3D object representations from
    a few reference views, followed by estimating translation using mask bounding
    boxes and corresponding depth values. The initial rotation was determined by sampling
    angles and refined using gradient updates via a render-and-compare approach. By
    training the network to render and reconstruct diverse 3D shapes, it achieved
    excellent generalization performance on unseen objects.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike Park *et al.*[[65](#bib.bib65)] that used the strategy of render-and-compare
    after reconstruction, Liu *et al.*[[1](#bib.bib1)] designed a pipeline for detection,
    retrieval, and refinement. They first designed a detector to identify object bounding
    boxes in the target view. Next, they compared the query and reference images at
    the pixel level to acquire the initial pose based on the similarity score. The
    pose was then refined using feature volume and multiple 3D convolution layers.
    However, object-centered reference images from cluttered scenes are constrained
    by actual segmentation or bounding box cropping, limiting its real-world applicability.
    To overcome this limitation, Gao *et al.*[[300](#bib.bib300)] proposed adaptive
    segmentation modules to learn distinguishable representations of unseen objects,
    and Zhao *et al.*[[308](#bib.bib308)] leveraged distributed reference kernels
    and translation estimator to achieve multi-scale correlation computation and object
    translation parameter prediction, thus robustly learning the prior translation
    of unseen objects.
  prefs: []
  type: TYPE_NORMAL
- en: To further enhance the robustness of the translation estimation for object detection,
    Pan *et al.*[[309](#bib.bib309)] modified the framework of Liu *et al.*[[1](#bib.bib1)].
    Precisely, they utilized pre-trained ViT[[293](#bib.bib293)] to learn robust feature
    representations and adopted a top-K pose proposal scheme for pose initialization.
    Additionally, they applied a coarse-to-fine cascaded refinement process, incorporating
    feature pyramids and adaptive discrete pose hypotheses. Besides [[309](#bib.bib309)],
    Cai *et al.*[[301](#bib.bib301)] revisited the pipeline of Liu *et al.*[[1](#bib.bib1)].
    They proposed a generic joint segmentation method and an efficient 3D Gaussian
    Splatting-based refiner, improving the performance and robustness of object localization
    and pose estimation.
  prefs: []
  type: TYPE_NORMAL
- en: In unseen object tracking, Nguyen *et al.*[[299](#bib.bib299)] proposed the
    first method that extended to invisible categories without requiring 3D information
    and extra reference images, given the ground-truth object pose in the first frame.
    Their transformer-based architecture outputs continuous relative object poses
    between consecutive frames, combined with the initial object pose, to provide
    the object pose for each frame. Wen *et al.*[[310](#bib.bib310)] used the collaborative
    design of concurrent tracking and neural object fields to perform 6DoF tracking
    from RGBD sequences. Key aspects of it include online pose graph optimization,
    concurrent neural object fields for 3D shape and appearance reconstruction, and
    a memory pool facilitating communication between the two processes.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, Nguyen *et al.*[[311](#bib.bib311)] reconsidered template matching
    from the perspective of generating new views. Given a single reference view, they
    trained a model to directly predict the discriminative embeddings of the novel
    viewpoints of the object. In contrast, Wen *et al.*[[3](#bib.bib3)] applied an
    object-centric neural field representation for object modeling and RGBD rendering.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, similar to template matching-based methods using CAD models, manual
    reference view-based methods also rely on massive templates. Moreover, due to
    limited reference views, these methods need to generate new templates or employ
    additional strategies to optimize the initial pose obtained through template matching.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the advancement of object pose estimation technology, several applications
    leveraging this progress have been deployed. In this section, we elaborate on
    the development trends of these applications. Specifically, these applications
    include robotic manipulation (Sec. [6.1](#S6.SS1 "6.1 Robotic Manipulation ‣ 6
    Applications ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")),
    Augmented Reality (AR)/Virtual Reality (VR) (Sec. [6.2](#S6.SS2 "6.2 Augmented
    Reality/Virtual Reality ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey")), aerospace (Sec. [6.3](#S6.SS3 "6.3 Aerospace ‣ 6 Applications
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")), hand-object
    interaction (Sec. [6.4](#S6.SS4 "6.4 Hand-Object Interaction ‣ 6 Applications
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey")), and autonomous
    driving (Sec. [6.5](#S6.SS5 "6.5 Autonomous Driving ‣ 6 Applications ‣ Deep Learning-Based
    Object Pose Estimation: A Comprehensive Survey")). The chronological overview
    is shown in Fig. [7](#S6.F7 "Figure 7 ‣ 6.1.1 Instance-Level Manipulation ‣ 6.1
    Robotic Manipulation ‣ 6 Applications ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Robotic Manipulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We categorize the robotic manipulation application into instance-level, category-level,
    and unseen objects. This classification helps in better understanding the challenges
    and requirements across different levels.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.1 Instance-Level Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To tackle the challenge of annotating real data during training, many works
    utilize synthetic data for training as it is easy to acquire and annotate [[312](#bib.bib312),
    [313](#bib.bib313)]. At the same time, synthetic data can simulate various scenes
    and environmental changes, thus helping to improve the adaptability of robotic
    manipulation. Li *et al.*[[314](#bib.bib314)] used a large-scale synthetic dataset
    and a small-scale weakly labeled real-world dataset to reduce the difficulty of
    system deployment. Additionally, Chen *et al.*[[315](#bib.bib315)] proposed an
    iterative self-training framework, using a teacher network trained on synthetic
    data to generate pseudo-labels for real data. Meanwhile, Fu *et al.*[[316](#bib.bib316)]
    trained only on synthetic images based on physical rendering. One of the critical
    challenges of synthetic data is bridging the gap with reality, which Tremblay
    *et al.*[[317](#bib.bib317)] addressed by combining domain randomization with
    real data.
  prefs: []
  type: TYPE_NORMAL
- en: Handling stacked occlusion scenes is another significant challenge, especially
    in industrial automation and logistics. In these scenarios, robots must accurately
    identify and localize objects stacked on each other, which requires an effective
    process of occluded objects and accurate pose estimation. Dong *et al.*[[318](#bib.bib318)]
    argued that the regression poses of points from the same object should tightly
    reside in the pose space. Therefore, these points can be clustered into different
    instances, and their corresponding object poses can be estimated simultaneously.
    This method can handle severe object occlusion. Moreover, Zhuang *et al.*[[319](#bib.bib319)]
    established an end-to-end pipeline to synchronously regress all potential object
    poses from an unsegmented point cloud. Most recently, Wada *et al.*[[320](#bib.bib320)]
    proposed a system that fully utilized identified accurate object CAD models and
    non-parametric reconstruction of unrecognized structures to estimate the occluded
    objects pose in real-time.
  prefs: []
  type: TYPE_NORMAL
- en: Low-textured objects lack object surface texture information, making robotic
    manipulation challenging. Therefore, Zhang *et al.*[[321](#bib.bib321)] proposed
    a pose estimation method for texture-less industrial parts. Poor surface texture
    and brightness make it challenging to compute discriminative local appearance
    descriptors. This method achieves more accurate results by optimizing the pose
    in the edge image. In addition, Chang *et al.*[[322](#bib.bib322)] carried out
    transparent object grasping by estimating the object pose using a proposed model-free
    method that relies on multiview geometry. In agricultural scenes, Kim *et al.*[[323](#bib.bib323)]
    constructed an automated data collection scheme based on a 3D simulator environment
    to achieve three-level ripeness classification and pose estimation of target fruits.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7a5a9b0cb8df631bb3c19206a4346199.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Chronological overview of some representative applications of object
    pose estimation methods. The black references, red references, and orange references
    represent the application of instance-level, category-level, and unseen methods,
    respectively. From this, we can also see the development trend, *i.e.*, from instance-level
    methods to category-level and unseen methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Category-Level Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To investigate the application of category-level object pose estimation for
    robotic manipulation, Liu *et al.*[[324](#bib.bib324)] introduced a fine segmentation-guided
    category-level method with difference-aware shape deformation for robotic grasping.
    Yu *et al.*[[325](#bib.bib325)] proposed a shape prior-based approach and explored
    its application for robotic grasping. Further, Liu *et al.*[[4](#bib.bib4)] developed
    a robotic continuous grasping system with a pre-defined vector orientation-based
    grasping strategy, based on shape transformer-guided object pose estimation. To
    improve efficiency and enable the pose estimation method to be applied to tasks
    with higher real-time requirements, Sun *et al.*[[326](#bib.bib326)] utilized
    the inter-frame consistent keypoints to perform object pose tracking for aerial
    manipulation. To further avoid manual data annotation in the real-world scene,
    Yu *et al.*[[327](#bib.bib327)] built a robotic grasping platform and designed
    a self-supervised-based method for category-level robotic grasping. More recently,
    Liu *et al.*[[5](#bib.bib5)] explored a contrastive learning-guided prior-free
    object pose estimation method for domain-generalized robotic picking.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Unseen Object Manipulation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Since unseen object pose estimation belongs to an emerging research, there is
    currently a lack of specialized designs for robotic. Here, we report several methods
    that validate the effectiveness of unseen object pose estimation through robotic
    manipulation. Okorn *et al.*[[278](#bib.bib278)] introduced a method for zero-shot
    object pose estimation in clutter. By scoring pose hypotheses and choosing the
    highest-scoring pose, they successfully grasped a novel drill object using a robotic
    arm. Labbé *et al.*[[28](#bib.bib28)] and Wen *et al.*[[3](#bib.bib3)] adopted
    the render-and-compare strategy and trained the network on a large-scale synthetic
    dataset, resulting in an outstanding generalization. They further verified the
    effectiveness of their methods through robotic grasping experiments.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Augmented Reality/Virtual Reality
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object pose estimation has various specific applications in AR and VR fields.
    In AR, accurate pose estimation allows for a precise overlay of virtual objects
    onto the real world. The key to VR technology lies in tracking the head-mounted
    display pose and controller in 3D space.
  prefs: []
  type: TYPE_NORMAL
- en: Su *et al.*[[328](#bib.bib328)] combined two CNN architectures[[162](#bib.bib162)]
    into a network, consisting of a state estimation branch and a pose estimation
    branch explicitly trained on synthetic images, to achieve AR assembly applications.
    Pandey *et al.*[[329](#bib.bib329)] introduced a method for automatically annotating
    the handheld objects pose in camera space, addressing the efficient 6Dof pose
    tracking problem for handheld controllers from the perspective of egocentric cameras.
    Liu *et al.*[[1](#bib.bib1)] presented a generalizable model-free 6DoF object
    pose estimator that has realized the complete object detection and pose estimation
    process. By simply capturing reference images of an unseen object and retrieving
    the poses of reference images, this method can predict the object pose on arbitrary
    query images and be easily applied to daily objects for AR/VR applications. He
    *et al.*[[2](#bib.bib2)] adopted matching after the reconstruction strategy, which
    establishes the correspondences between the query image and the reconstructed
    point cloud from reference views. This method does not rely on keypoint matching
    and allows for AR applications even on low-texture objects. Wen *et al.* [[3](#bib.bib3)]
    achieved strong generality by employing large-scale comprehensive training and
    innovative transformer-based architecture. This method has been successfully applied
    in various domains, including AR and robotic manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Aerospace
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Estimating the object pose in space presents unique challenges not commonly
    encountered in terrestrial environments. One of the most significant differences
    is the lack of atmospheric scattering, which complicates lighting conditions and
    makes objects invisible over long distances. In-orbit proximity operations for
    space rendezvous, docking, and debris removal require precise pose estimation
    under diverse lighting conditions and on high-texture backgrounds. Proença *et
    al.*[[330](#bib.bib330)] proposed URSO, a simulator developed on Unreal Engine
    4 [[331](#bib.bib331)] for generating annotated images of spacecraft orbiting
    Earth, which can be used as valuable data for aerospace application. Hu *et al.*[[82](#bib.bib82)]
    proposed an encoder-decoder architecture that reliably handles large-scale changes
    under challenging conditions, enhancing robustness. Wang *et al.*[[332](#bib.bib332)]
    introduced a counterfactual analysis framework to achieve robust pose estimation
    of spaceborne targets in complex backgrounds. Ulmer *et al.*[[333](#bib.bib333)]
    generated multiple pose hypotheses for objects and introduced a pixel-level posterior
    formula to estimate the probability of each hypothesis. This approach can handle
    extreme visual conditions, including overexposure, high contrast, and low signal-to-noise
    ratio.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Hand-Object Interaction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When humans/robots interact with the physical world, they primarily do so through
    their hands. Therefore, accurately understanding how hands interact with objects
    is crucial.
  prefs: []
  type: TYPE_NORMAL
- en: Hand-object interaction methods often rely on the object CAD model, and obtaining
    the object CAD model from daily life scenes is challenging. Patten *et al.*[[334](#bib.bib334)]
    reconstructed high-quality object CAD model to mitigate the reliance on object
    CAD model in hand-object interaction. To further enhance hand-object interaction,
    Lin *et al.*[[6](#bib.bib6)] utilized an effective attention model to improve
    the representation capability of hand and object features, thereby improving the
    accuracy of hand and object pose estimation. However, this method has limited
    utilization of the underlying geometric structures, leading to an increased reliance
    on visual features. Performance may degrade when objects lack visual features
    or when these features are occluded. Therefore, Rezazadeh *et al.*[[7](#bib.bib7)]
    introduced a hierarchical graph neural network architecture combined with multimodal
    (visual and tactile) data to compensate for visual deficiencies and improve robustness.
    Moreover, Qi *et al.*[[335](#bib.bib335)] introduced a hand-object pose estimation
    network guided by Signed Distance Fields (SDF), which jointly leverages the SDFs
    of both the hand and the object to provide a complete global implicit representation.
    This method aids in guiding the pose estimation of hands and objects in occlusion
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Autonomous Driving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object pose estimation can be used to perceive surrounding objects such as vehicles,
    pedestrians, and obstacles, aiding the autonomous driving system in making timely
    decisions.
  prefs: []
  type: TYPE_NORMAL
- en: In order to address the pose estimation problem in autonomous driving, Hoque
    *et al.*[[336](#bib.bib336)] proposed a 6DoF pose hypothesis based on a deep hybrid
    structure composed of CNNs[[162](#bib.bib162)] and RNNs[[337](#bib.bib337)]. More
    recently, Sun *et al.*[[338](#bib.bib338)] designed an effective keypoint selection
    algorithm, which takes into account the shape information of panel objects within
    the scene of robot cabin inspection, addressing the challenge of 6DoF pose estimation
    of highly variable panel objects.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion and Future Direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this survey, we have provided a systematic overview of the latest deep learning-based
    object pose estimation methods, covering a comprehensive classification, a comparison
    of their strengths and weaknesses, and an exploration of their applications. Despite
    the great success, many challenges still exist, as discussed in Sec. [3](#S3 "3
    Instance-Level Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"), Sec. [4](#S4 "4 Category-Level Object Pose Estimation
    ‣ Deep Learning-Based Object Pose Estimation: A Comprehensive Survey"), and Sec.
    [5](#S5 "5 Unseen Object Pose Estimation ‣ Deep Learning-Based Object Pose Estimation:
    A Comprehensive Survey"). Based on these challenges, we further point out some
    promising future directions aimed at advancing research in object pose estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the perspective of label-efficient learning, prevailing methodologies
    predominantly rely on the utilization of real-world labeled datasets for training
    purposes. Nevertheless, the labor-intensive nature of manually collecting and
    annotating training data is widely acknowledged. Hence, we advocate for the exploration
    of label-efficient learning techniques for object pose estimation, which can be
    pursued through the following avenues: 1) LLMs/LVMs-guided weak/self-supervised
    learning methods. With the rapid advancements in pre-trained LLMs/LVMs, their
    versatile application in various scenarios through an unsupervised manner has
    become feasible. Leveraging LLMs/LVMs as prior knowledge holds promise for exploring
    weak or self-supervised learning techniques in object pose estimation. 2) Synthesis
    to real-world domain adaptation and generalization methods. Due to the high costs
    associated with acquiring real-world training data through manual efforts, synthetic
    data generation offers a cost-effective alternative. We believe that by exploring
    domain adaptation and generalization techniques from synthetic to real-world domains,
    we can mitigate domain gaps and achieve the capability to generalize synthetic
    data-trained models for real-world applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In terms of applications, facilitating the deployment of object pose estimation
    methods on mobile devices and robots is crucial. We argue that enhancing the deployability
    of existing methods can be achieved through the following approaches: 1) End-to-end
    methods integrating detection or segmentation. Current SOTA approaches typically
    require initial object detection or segmentation using a pre-trained model before
    inputting the image into a pose estimation model (indirect pose estimation models
    even need to use non-differentiable PnP or Umeyama algorithms to solve pose),
    which complicates deployment. Future research can enhance the deployability on
    mobile devices and robots by exploring end-to-end object pose estimation methods
    that seamlessly integrate detection or segmentation. 2) Single RGB image-based
    methods. Given that most mobile devices (such as smartphones and tablets) lack
    depth cameras, achieving high-precision estimation of unseen object poses using
    a single RGB image is crucial. Due to inherent geometric limitations in 2D images,
    future research can explore LVMs-based monocular depth estimation methods to enhance
    the accuracy of monocular object pose estimation by incorporating scene-level
    depth information. 3) Model lightweighting. Existing SOTA models often have large
    parameter sizes and inefficient running performance, which presents challenges
    for deployment on mobile devices and robots with limited computational resources.
    Future work can explore effective lightweight methods, such as teacher-student
    models, to research reducing model parameter count (GPU memory) and improving
    model running efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing methods are predominantly designed for common objects and scenes,
    rendering them ineffective for challenging objects and scenes. We believe that
    the applicability can be enhanced through the following avenues: 1) Articulated
    object pose estimation. Articulated objects (such as clothing and drawers) exhibit
    multiple DoF and significant self-occlusion compared to rigid objects, making
    pose estimation challenging. Achieving high-precision pose estimation for articulated
    objects is an important research problem that remains to be addressed in the future.
    2) Transparent object pose estimation. The simultaneous absence of texture, color,
    and depth information poses a significant challenge for estimating the pose of
    transparent objects. Future research endeavors could focus on enhancing the geometric
    information of transparent objects through depth augmentation or completion techniques,
    thereby improving the accuracy of pose estimation. 3) Robust methods for handling
    occlusion. Occlusion is the most common challenge. Currently, there exists no
    object pose estimation method that can effectively handle severe occlusion. Severe
    occlusion leads to an incomplete representation of texture and geometric features
    in objects, introducing uncertainty into the pose estimation model. Hence, improving
    the model’s ability to perceive severe occlusion is crucial for enhancing its
    robustness.'
  prefs: []
  type: TYPE_NORMAL
- en: 'From the aspect of problem formulation, recent instance-level methods have
    achieved high precision but exhibit poor generalization. Category-level methods
    demonstrate good generalization for intra-class unseen objects but fail to generalize
    to unseen object categories. Unseen object pose estimation methods have the potential
    to generalize to any unseen object, yet they still rely on object CAD models or
    reference views. The following paths can be explored from the problem formulation
    to further enhance the generalization of object pose estimation: 1) Few-shot learning-based
    category-level methods for unseen categories. Since category-level methods need
    to re-obtain a large amount of annotated training data for unseen object categories,
    their generalization is severely limited. Therefore, future research could focus
    on exploring how to leverage few-shot learning to enable the rapid generalization
    of category-level methods to unseen object categories. 2) CAD model-free and sparse
    manual reference view-based unseen object pose estimation. While current unseen
    object pose estimation methods do not require retraining for unseen objects, they
    still rely on either the CAD models or extensive annotated reference views of
    unseen objects, both of which still require manual acquisition. To this end, exploring
    CAD model-free and sparse manual reference view-based unseen object pose estimation
    methods is crucial. 3) Open-vocabulary strong generalization methods. Given the
    broad applicability of object pose estimation in human-machine interaction scenes,
    future research could leverage open vocabulary provided by humans as prompts to
    enhance generalization to unseen objects and scenes.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the National Natural Science Foundation of China
    under Grant U22A2059, Shenzhen Science and Technology Foundation under Grant 2021Szvup035,
    China Scholarship Council under Grant 202306130074 and Grant 202206130048, Natural
    Science Foundation of Hunan Province under Grant 2024JJ5098, and by the State
    Key Laboratory of Advanced Design and Manufacturing for Vehicle Body Open Foundation.
    Ajmal Mian was supported by the Australian Research Council Future Fellowship
    Award funded by the Australian Government under Project FT210100268.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. Liu and Y. Wen, “Gen6d: Generalizable model-free 6-dof object pose estimation
    from rgb images,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] X. He and J. Sun, “Onepose++: Keypoint-free one-shot object pose estimation
    without cad models,” in NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] B. Wen and W. Yang, “Foundationpose: Unified 6d pose estimation and tracking
    of novel objects,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Liu and W. Sun, “Robotic continuous grasping system by shape transformer-guided
    multi-object category-level 6d pose estimation,” IEEE TII, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] J. Liu and W. Sun, “Domain-generalized robotic picking via contrastive
    learning-based 6-d pose estimation,” IEEE TII, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Z. Lin and C. Ding, “Harmonious feature learning for interactive hand-object
    pose estimation,” in CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Rezazadeh and S. Dikhale, “Hierarchical graph nural networks for proprioceptive
    6d pose estimation of in-hand objects,” in ICRA, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] D. G. Lowe, “Distinctive image features from scale-invariant keypoints,”
    IJCV, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] R. B. Rusu and N. Blodow, “Fast point feature histograms (fpfh) for 3d
    registration,” in ICRA, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] R. B. Rusu and G. Bradski, “Fast 3d recognition and pose using the viewpoint
    feature histogram,” in IROS, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] B. Drost and M. Ulrich, “Model globally, match locally: Efficient and
    robust 3d object recognition,” in CVPR, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] C. Choi and H. I. Christensen, “3d pose estimation of daily objects using
    an rgb-d camera,” in IROS, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] C. Choi and A. J. Trevor, “Rgb-d edge detection and edge-based registration,”
    in IROS, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] T. Birdal and S. Ilic, “Point pair features based object detection and
    pose estimation revisited,” in 3DV, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Xiang and T. Schmidt, “Posecnn: A convolutional neural network for
    6d object pose estimation in cluttered scenes,” arXiv preprint arXiv:1711.00199,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] C. Wang and D. Xu, “Densefusion: 6d object pose estimation by iterative
    dense fusion,” in CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] S. Peng and Y. Liu, “Pvnet: Pixel-wise voting network for 6dof pose estimation,”
    in CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Y. He and W. Sun, “Pvn3d: A deep point-wise 3d keypoints voting network
    for 6dof pose estimation,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Z. Li and G. Wang, “Cdpn: Coordinates-based disentangled pose network
    for real-time rgb-based 6-dof object pose estimation,” in ICCV, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. Zakharov and I. Shugurov, “Dpod: 6d pose object detector and refiner,”
    in ICCV, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. He and H. Huang, “Ffb6d: A full flow bidirectional fusion network for
    6d pose estimation,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Wang and S. Sridhar, “Normalized object coordinate space for category-level
    6d object pose and size estimation,” in CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] K. Chen and Q. Dou, “Sgpa: Structure-guided prior adaptation for category-level
    6d object pose estimation,” in ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Lin and Z. Wei, “Category-level 6d object pose and size estimation
    using self-supervised deep prior deformation networks,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Di and R. Zhang, “Gpv-pose: Category-level object pose estimation via
    geometry-guided point-wise voting,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] L. Zheng and C. Wang, “Hs-pose: Hybrid scope feature extraction for category-level
    object pose estimation,” in CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] J. Liu and Y. Chen, “Ist-net: Prior-free category-level pose estimation
    with implicit space transformation,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Labbé and L. Manuelli, “Megapose: 6d pose estimation of novel objects
    via render & compare,” in CoRL, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] V. N. Nguyen and T. Groueix, “Gigapose: Fast and robust novel object pose
    estimation via one correspondence,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Lin and L. Liu, “Sam-6d: Segment anything model meets zero-shot 6d
    object pose estimation,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Hoque and M. Y. Arafat, “A comprehensive review on 3d object detection
    and 6d pose estimation with deep learning,” IEEE Access, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] G. Marullo and L. Tanzi, “6d object position estimation from 2d images:
    A literature review,” Multimed. Tools Appl., 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Z. Fan and Y. Zhu, “Deep learning on monocular object pose detection and
    tracking: A comprehensive overview,” ACM Comput. Surv., 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] G. Du and K. Wang, “Vision-based robotic grasping from object localization,
    object pose estimation to grasp estimation for parallel grippers: A review,” Artif
    Intell Rev., 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Guan and Y. Hao, “A survey of 6dof object pose estimation methods for
    different application scenarios,” Sensors, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] T. Hodan and M. Sundermeyer, “Bop challenge 2023 on detection, segmentation
    and pose estimation of seen and unseen rigid objects,” arXiv preprint arXiv:2403.09799,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. Hinterstoisser and V. Lepetit, “Model based training, detection and
    pose estimation of texture-less 3d objects in heavily cluttered scenes,” in ACCV,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] E. Brachmann and F. Michel, “Uncertainty-driven 6d pose estimation of
    objects and scenes from a single rgb image,” in CVPR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] E. Brachmann and A. Krull, “Learning 6d object pose estimation using 3d
    object coordinates,” in ECCV, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. Tejani and D. Tang, “Latent-class hough forests for 3d object detection
    and pose estimation,” in ECCV, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] A. Doumanoglou and R. Kouskouridas, “Recovering 6d object pose and predicting
    next-best-view in the crowd,” in CVPR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] C. Rennie and R. Shome, “A dataset for improved rgbd-based object detection
    and pose estimation for warehouse pick-and-place,” IEEE RAL, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] B. Calli and A. Singh, “The ycb object and model set: Towards common benchmarks
    for manipulation research,” in ICRA, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] T. Hodan and P. Haluza, “T-less: An rgb-d dataset for 6d pose estimation
    of texture-less objects,” in WACV, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] B. Drost and M. Ulrich, “Introducing mvtec itodd - a dataset for 3d object
    recognition in industry,” in ICCVW, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] R. Kaskman and S. Zakharov, “Homebreweddb: Rgb-d dataset for 6d pose estimation
    of 3d objects,” in ICCVW, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] S. Tyree and J. Tremblay, “6-dof pose estimation of household objects
    for robotic manipulation: An accessible dataset and benchmark,” in IROS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] B. Wen and C. Mitash, “Se(3)-tracknet: Data-driven 6d pose tracking by
    calibrating image residuals in synthetic domains,” in IROS, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] X. Chen and H. Zhang, “Clearpose: Large-scale transparent object dataset
    and benchmark,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] L. Chen and H. Yang, “Mp6d: An rgb-d dataset for metal parts’ 6d pose
    estimation,” IEEE RAL, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. X. Chang and T. Funkhouser, “Shapenet: An information-rich 3d model
    repository,” arXiv preprint arXiv:1512.03012, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] L. Manuelli and W. Gao, “kpam: Keypoint affordances for category-level
    robotic manipulation,” in ISRR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] X. Liu and R. Jonschkowski, “Keypose: Multi-view 3d labeling and keypoint
    estimation for transparent objects,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] A. Ahmadyan and L. Zhang, “Objectron: A large scale dataset of object-centric
    videos in the wild with pose annotations,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Ze and X. Wang, “Category-level 6d object pose estimation in the wild:
    A semi-supervised learning approach and a new dataset,” in NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] P. Wang and H. Jung, “Phocal: A multi-modal dataset for category-level
    object pose estimation with photometrically challenging objects,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] H. Jung and S.-C. Wu, “Housecat6d–a large-scale multi-modal category level
    6d object pose dataset with household objects in realistic scenarios,” in CVPR,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] F. Michel and A. Krull, “Pose estimation of kinematic chain instances
    via object coordinate regression.,” in BMVC, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] R. Martín-Martín and C. Eppner, “The rbo dataset of articulated objects
    and interactions,” IJRR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Y. Liu and Y. Liu, “Hoi4d: A 4d egocentric dataset for category-level
    human-object interaction,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] L. Liu and H. Xue, “Toward real-world category-level articulation pose
    estimation,” IEEE TIP, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Z. Zhu and J. Wang, “Contactart: Learning 3d interaction priors for category-level
    articulated object and hand poses estimation,” arXiv preprint arXiv:2305.01618,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Y. Qin and H. Su, “From one hand to multiple hands: Imitation learning
    for dexterous manipulation from single-camera teleoperation,” IEEE RAL, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] K. Mo and S. Zhu, “Partnet: A large-scale benchmark for fine-grained and
    hierarchical part-level 3d object understanding,” in CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] K. Park and A. Mousavian, “Latentfusion: End-to-end differentiable reconstruction
    and rendering for unseen object pose estimation,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Sun and Z. Wang, “Onepose: One-shot object pose estimation without
    cad models,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] P. Wohlhart and V. Lepetit, “Learning descriptors for object recognition
    and 3d pose estimation,” in CVPR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] G. Wang and F. Manhardt, “Self6d: Self-supervised monocular 6d object
    pose estimation,” in ECCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] X. Jiang and D. Li, “Uni6d: A unified cnn framework without projection
    breakdown for 6d pose estimation,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] H. Li and J. Lin, “Dcl-net: Deep correspondence learning network for 6d
    pose estimation,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J. Shotton and B. Glocker, “Scene coordinate regression forests for camera
    relocalization in rgb-d images,” in CVPR, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] M. Tian and M. H. Ang, “Shape prior deformation for categorical 6d object
    pose and size estimation,” in ECCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] M. A. Fischler and R. C. Bolles, “Random sample consensus,” COMMUN ACM,
    1981.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] M. Rad and V. Lepetit, “Bb8: A scalable, accurate, robust to partial occlusion
    method for predicting the 3d poses of challenging objects without using depth,”
    in ICCV, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] B. Tekin and S. N. Sinha, “Real-time seamless single shot 6d object pose
    prediction,” in CVPR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] J. Redmon and S. Divvala, “You only look once: Unified, real-time object
    detection,” in CVPR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] G. Pavlakos and X. Zhou, “6-dof object pose from semantic keypoints,”
    in ICRA, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] B. Doosti and S. Naha, “Hope-net: A graph-based model for hand-object
    pose estimation,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” arXiv preprint arXiv:1609.02907, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] C. Song and J. Song, “Hybridpose: 6d object pose estimation under hybrid
    representations,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] P. Liu and Q. Zhang, “Mfpn-6d : Real-time one-stage pose estimation of
    objects on rgb images,” in ICRA, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Y. Hu and S. Speierer, “Wide-depth-range 6d object pose estimation in
    space,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] R. Lian and H. Ling, “Checkerpose: Progressive dense keypoint localization
    for object pose estimation with graph neural network,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] J. Chang and M. Kim, “Ghostpose: Multi-view pose estimation of transparent
    objects for robot hand grasping,” in IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Guo and Y. Hu, “Knowledge distillation for 6d pose estimation by aligning
    distributions of local predictions,” in CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] F. Liu and Y. Hu, “Linear-covariance loss for end-to-end learning of 6d
    pose estimation,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] A. Crivellaro and M. Rad, “Robust 3d object tracking from monocular images
    using stable parts,” IEEE TPAMI, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] M. Oberweger and M. Rad, “Making deep heatmaps robust to partial occlusions
    for 3d object pose estimation,” in ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Hu and J. Hugonot, “Segmentation-driven 6d object pose estimation,”
    in CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] W.-L. Huang and C.-Y. Hung, “Confidence-based 6d object pose estimation,”
    IEEE TMM, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] W. Zhao and S. Zhang, “Learning deep network for detecting 3d object keypoints
    and 6d poses,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Z. Yang and X. Yu, “Dsc-posenet: Learning 6dof object pose estimation
    via dual-scale consistency,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] S. Liu and H. Jiang, “Semi-supervised 3d hand-object poses estimation
    with interactions in time,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] G. Georgakis and S. Karanam, “Learning local rgb-to-cad correspondences
    for object pose estimation,” in ICCV, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. Sock and G. Garcia-Hernando, “Introducing pose consistency and warp-alignment
    for self-supervised 6d object pose estimation in color images,” in 3DV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] S. Zhang and W. Zhao, “Keypoint-graph-driven learning framework for object
    pose estimation,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. Thalhammer and M. Leitner, “Pyrapose: Feature pyramids for fast and
    accurate object pose estimation under domain shift,” in ICRA, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] T. Hodan and D. Barath, “Epos: Estimating 6d pose of objects with symmetries,”
    in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] I. Shugurov and S. Zakharov, “Dpodv2: Dense correspondence-based 6 dof
    pose estimation,” IEEE TPAMI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] H. Chen and P. Wang, “Epro-pnp: Generalized end-to-end probabilistic
    perspective-n-points for monocular object pose estimation,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] R. L. Haugaard and A. G. Buch, “Surfemb: Dense and continuous correspondence
    distributions for object pose estimation with learnt surface embeddings,” in CVPR,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] F. Li and S. R. Vutukur, “Nerf-pose: A first-reconstruct-then-regress
    approach for weakly-supervised 6d object pose estimation,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Xu and K.-Y. Lin, “Rnnpose: 6-dof object pose estimation via recurrent
    correspondence field estimation and pose optimization,” IEEE TPAMI, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] M. Sundermeyer and Z.-C. Marton, “Implicit 3d orientation learning for
    6d object detection from rgb images,” in ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] C. Papaioannidis and V. Mygdalis, “Domain-translated 3d object pose estimation,”
    IEEE TIP, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Z. Li and X. Ji, “Pose-guided auto-encoder and feature-based refinement
    for 6-dof object pose regression,” in ICRA, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] X. Deng and A. Mousavian, “Poserbpf: A rao–blackwellized particle filter
    for 6-d object pose tracking,” IEEE TRO, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] H. Jiang and M. Salzmann, “Se(3) diffusion model-based point cloud registration
    for robust 6d object pose estimation,” in NeurIPS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Z. Dang and L. Wang, “Match normalization: Learning-based point cloud
    registration for 6d object pose estimation in the real world,” IEEE TPAMI, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] T. Cao and F. Luo, “Dgecn: A depth-guided edge convolutional network
    for end-to-end 6d pose estimation,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Wu and M. Zand, “Vote from the center: 6 dof pose estimation in rgb-d
    images by radial keypoint voting,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] J. Zhou and K. Chen, “Deep fusion transformer network with weighted vector-wise
    keypoints voting for robust 6d object pose estimation,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. Tian and L. Pan, “Robust 6d object pose estimation by learning rgb-d
    features,” in ICRA, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] G. Zhou and H. Wang, “Pr-gcn: A deep graph convolutional network with
    point refinement for 6d pose estimation,” in ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] N. Mo and W. Gan, “Es6d: A computation efficient and symmetry-aware 6d
    pose regression framework,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] J.-X. Hong and H.-B. Zhang, “A transformer-based multi-modal fusion network
    for 6d pose estimation,” Information Fusion, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] W. Chen and X. Jia, “G2l-net: Global to local network for real-time 6d
    pose estimation with embedding vector features,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Y. Hu and P. Fua, “Single-stage 6d object pose estimation,” in CVPR,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Y. Labbé and J. Carpentier, “Cosypose: Consistent multi-view multi-object
    6d pose estimation,” in ECCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] G. Wang and F. Manhardt, “Gdr-net: Geometry-guided direct regression
    network for monocular 6d object pose estimation,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Y. Di and F. Manhardt, “So-pose: Exploiting self-occlusion for direct
    6d pose estimation,” in ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] G. Wang and F. Manhardt, “Occlusion-aware self-supervised monocular 6d
    object pose estimation,” IEEE TPAMI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] C. Li and J. Bai, “A unified framework for multi-view multi-class object
    pose estimation,” in ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Y. Li and G. Wang, “Deepim: Deep iterative matching for 6d pose estimation,”
    in ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] F. Manhardt and W. Kehl, “Deep model-based 6d pose refinement in rgb,”
    in ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] F. Manhardt and D. M. Arroyo, “Explaining the ambiguity of object detection
    and 6d pose from visual data,” in ICCV, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] C. Papaioannidis and I. Pitas, “3d object pose estimation using multi-objective
    quaternion learning,” IEEE TCSVT, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Y. Liu and L. Zhou, “Regression-based three-dimensional pose estimation
    for texture-less objects,” IEEE TMM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Y. Hai and R. Song, “Shape-constraint recurrent flow for 6d object pose
    estimation,” in CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Y. Li and Y. Mao, “Mrc-net: 6-dof pose estimation with multiscale residual
    correlation,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] M. Cai and I. Reid, “Reconstruct locally, localize globally: A model
    free method for object pose estimation,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] D. Wang and G. Zhou, “Geopose: Dense reconstruction guided 6d object
    pose estimation with geometric consistency,” IEEE TMM, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Y. Su and M. Saleh, “Zebrapose: Coarse to fine surface encoding for 6dof
    object pose estimation,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Z. Xu and Y. Zhang, “Bico-net: Regress globally, match locally for robust
    6d pose estimation,” in IJCAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] L. Huang and T. Hodan, “Neural correspondence field for object pose estimation,”
    in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] H. Jiang and Z. Dang, “Center-based decoupled point cloud registration
    for 6d object pose estimation,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] P. Besl and N. D. McKay, “A method for registration of 3-d shapes,” IEEE
    TPAMI, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Y. Lin and Y. Su, “Hipose: Hierarchical binary surface encoding and correspondence
    pruning for rgb-d 6dof object pose estimation,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] K. Park and T. Patten, “Pix2pose: Pixel-wise coordinate regression of
    objects for 6d pose estimation,” in ICCV, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] C. Wu and L. Chen, “Geometric-aware dense matching network for 6d pose
    estimation of objects from rgb-d images,” PR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] C. Wu and L. Chen, “Pseudo-siamese graph matching network for textureless
    objects’ 6-d pose estimation,” IEEE TIE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Z. Li and Y. Hu, “Sd-pose: Semantic decomposition for cross-domain 6d
    object pose estimation,” in AAAI, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Y. Hu and P. Fua, “Perspective flow aggregation for data-limited 6d object
    pose estimation,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. Hai and R. Song, “Pseudo flow consistency for self-supervised 6d object
    pose estimation,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] L. Lipson and Z. Teed, “Coupled iterative refinement for 6d multi-object
    pose estimation,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] J. J. Moré, “The levenberg-marquardt algorithm: Implementation and theory,”
    in Numerical Analysis, 1978.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] X. Liu and J. Zhang, “6dof pose estimation with object cutout based on
    a deep autoencoder,” in ISMAR-Adjunct, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Y. Zhang and C. Zhang, “6d object pose estimation algorithm using preprocessing
    of segmentation and keypoint extraction,” in I2MTC, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] S. Stevšič and O. Hilliges, “Spatial attention improves iterative 6d
    object pose estimation,” in 3DV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] K. Murphy and S. Russell, Sequential Monte Carlo Methods in Practice,
    ch. Rao-blackwellised particle filtering for dynamic bayesian networks. Springer,
    2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] X. Liu and S. Iwase, “Kdfnet: Learning keypoint distance field for 6d
    object pose estimation,” in IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] P. Liu and Q. Zhang, “Bdr6d: Bidirectional deep residual fusion network
    for 6d pose estimation,” IEEE TASE, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] L. Xu and H. Qu, “6d-diff: A keypoint diffusion framework for 6d object
    pose estimation,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] J. Mei and X. Jiang, “Spatial feature mapping for 6dof object pose estimation,”
    PR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] F. Wang and X. Zhang, “Kvnet: An iterative 3d keypoints voting network
    for real-time 6-dof object pose estimation,” Neurocomputing, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] L. Zeng and W. J. Lv, “Parametricnet: 6dof pose estimation network for
    parametric shapes in stacked scenarios,” in ICRA, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] F. Duffhauss and T. Demmler, “Mv6d: Multi-view 6d pose estimation on
    rgb-d frames using a deep point-wise voting network,” in IROS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] X. Yu and Z. Zhuang, “6dof object pose estimation via differentiable
    proxy voting loss,” arXiv preprint arXiv:2002.03923, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] H. Lin and S. Peng, “Learning to estimate object poses without real image
    annotations.,” in IJCAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] T. Ikeda and S. Tanishige, “Sim2real instance-level style transfer for
    6d pose estimation,” in IROS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] G. Zhou and Y. Yan, “A novel depth and color feature fusion framework
    for 6d object pose estimation,” IEEE TMM, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. LeCun and B. Boser, “Backpropagation applied to handwritten zip code
    recognition,” Neural Comput, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] X. Liu and X. Yuan, “A depth adaptive feature extraction and dense prediction
    network for 6-d pose estimation in robotic grasping,” IEEE TII, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] F. Mu and R. Huang, “Temporalfusion: Temporal motion reasoning with multi-frame
    fusion for 6d object pose estimation,” in IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] D. Cai and J. Heikkilä, “Sc6d: Symmetry-agnostic and correspondence-free
    6d object pose estimation,” in 3DV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] L. Zeng and W. J. Lv, “Ppr-net++: Accurate 6-d pose estimation in stacked
    scenarios,” IEEE TASE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Y. Bukschat and M. Vetter, “Efficientpose: An efficient, accurate and
    scalable end-to-end 6d multi object pose estimation approach,” arXiv preprint
    arXiv:2011.04307, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] G. Gao and M. Lauri, “6d object pose regression via supervised learning
    on point clouds,” in ICRA, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] M. Lin and V. Murali, “6d object pose estimation with pairwise compatible
    geometric features,” in ICRA, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Y. Shi and J. Huang, “Stablepose: Learning 6d object poses from geometrically
    stable patches,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Z. Liu and Q. Wang, “Pa-pose: Partial point cloud fusion based on reliable
    alignment for 6d pose tracking,” PR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Y. Wen and Y. Fang, “Gccn: Geometric constraint co-attention network
    for 6d object pose estimation,” in ACM MM, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Y. An and D. Yang, “Hft6d: Multimodal 6d object pose estimation based
    on hierarchical feature transformer,” Measurement, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Z. Zhang and W. Chen, “Trans6d: Transformer-based 6d object pose estimation
    and refinement,” in ECCVW, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] G. Feng and T.-B. Xu, “Nvr-net: Normal vector guided regression network
    for disentangled 6d pose estimation,” IEEE TCSVT, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] G. Gao and M. Lauri, “Cloudaae: Learning 6d object pose regression with
    on-line data synthesis on point clouds,” in ICRA, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] G. Zhou and D. Wang, “Semi-supervised 6d object pose estimation without
    using real annotations,” IEEE TCSVT, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] T. Tan and Q. Dong, “Smoc-net: Leveraging camera pose for self-supervised
    monocular object pose estimation,” in CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] J. Rambach and C. Deng, “Learning 6dof object poses from synthetic single
    channel images,” in ISMAR-Adjunct, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] K. Kleeberger and M. F. Huber, “Single shot 6d object pose estimation,”
    in ICRA, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] V. Sarode and X. Li, “Pcrnet: Point cloud registration network using
    pointnet encoding,” arXiv preprint arXiv:1908.07906, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] C. R. Qi and H. Su, “Pointnet: Deep learning on point sets for 3d classification
    and segmentation,” in CVPR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] S. H. Bengtson and H. Åström, “Pose estimation from rgb images of highly
    symmetric objects using a novel multi-pose loss and differential rendering,” in
    IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] J. Park and N. Cho, “Dprost: Dynamic projective spatial transformer network
    for 6d pose estimation,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] M. Garon and J.-F. Lalonde, “Deep 6-dof tracking,” IEEE TVCG, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] J. Long and E. Shelhamer, “Fully convolutional networks for semantic
    segmentation,” in CVPR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] W. Kehl and F. Manhardt, “Ssd-6d: Making rgb-based 3d detection and 6d
    pose estimation great again,” in ICCV, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in ECCV, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] J. Wu and B. Zhou, “Real-time object pose estimation with pose interpreter
    networks,” in IROS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] T.-T. Do and M. Cai, “Deep-6dpose: Recovering 6d object pose from a single
    rgb image,” arXiv preprint arXiv:1802.10367, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] T.-C. Hsiao and H.-W. Chen, “Confronting ambiguity in 6d object pose
    estimation via score-based diffusion on se(3),” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] J. Liu and W. Sun, “Hff6d: Hierarchical feature fusion network for robust
    6d object pose tracking,” IEEE TCSVT, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] R. Ge and G. Loianno, “Vipose: Real-time visual-inertial 6d object pose
    tracking,” in IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] S. Iwase and X. Liu, “Repose: Fast 6d object pose refinement via deep
    texture rendering,” in ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] J. Josifovski and M. Kerzel, “Object detection and pose estimation based
    on convolutional neural networks trained with synthetic data,” in IROS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] C. Sahin and T.-K. Kim, “Category-level 6d object pose recovery in depth
    images,” in ECCVW, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] S. Umeyama, “Least-squares estimation of transformation parameters between
    two point patterns,” IEEE TPAMI, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] J. Wang and K. Chen, “Category-level 6d object pose estimation via cascaded
    relation and recurrent reconstruction networks,” in IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] L. Zou and Z. Huang, “6d-vit: Category-level 6d object pose estimation
    via transformer-based instance representation learning,” IEEE TIP, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Z. Fan and Z. Song, “Object level depth reconstruction for category level
    6d object pose estimation from monocular rgb image,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] J. Wei and X. Song, “Rgb-based category-level object pose estimation
    via decoupled metric scale recovery,” arXiv preprint arXiv:2309.10255, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] M. Z. Irshad and T. Kollar, “Centersnap: Single-shot multi-object 3d
    shape reconstruction and categorical 6d pose and size estimation,” in ICRA, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] H. Lin and Z. Liu, “Sar-net: Shape alignment and recovery network for
    category-level 6d object pose and size estimation,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] R. Zhang and Y. Di, “Ssp-pose: Symmetry-aware shape prior deformation
    for direct category-level object pose estimation,” in IROS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] R. Zhang and Y. Di, “Rbp-pose: Residual bounding box projection for category-level
    pose estimation,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] X. Liu and G. Wang, “Catre: Iterative point clouds alignment for category-level
    object pose refinement,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] J. Liu and W. Sun, “Mh6d: Multi-hypothesis consistency learning for category-level
    6-d object pose estimation,” IEEE TNNLS, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] X. Li and H. Wang, “Category-level articulated object pose estimation,”
    in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] W. Chen and X. Jia, “Fs-net: Fast shape-based network for category-level
    6d object pose estimation with decoupled rotation mechanism,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Y. Weng and H. Wang, “Captra: Category-level pose tracking for rigid
    and articulated objects from point clouds,” in ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Y. You and R. Shi, “Cppf: Towards robust category-level 9d pose estimation
    in the wild,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] J. Zhang and M. Wu, “Generative category-level object pose estimation
    via diffusion models,” in NeurIPS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] C. Wang and Martín-Martín, “6-pack: Category-level 6d pose tracker with
    anchor-based keypoints,” in ICRA, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] J. Lin and Z. Wei, “Dualposenet: Category-level 6d object pose and size
    estimation using dual pose network with refined learning of pose consistency,”
    in ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] B. Wen and K. Bekris, “Bundletrack: 6d pose tracking for novel objects
    without instance or category-level 3d models,” in IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] W. Peng and J. Yan, “Self-supervised category-level 6d object pose estimation
    with deep implicit shape representation,” in AAAI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] T. Lee and B.-U. Lee, “Uda-cope: Unsupervised domain adaptation for category-level
    object pose estimation,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] T. Lee and J. Tremblay, “Tta-cope: Test-time adaptation for category-level
    object pose estimation,” in CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] J. Lin and Z. Wei, “Vi-net: Boosting category-level 6d object pose estimation
    via learning decoupled rotations on the spherical representations,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Y. Chen and Y. Di, “Secondpose: Se (3)-consistent dual-stream feature
    fusion for category-level pose estimation,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] T. Lee and B.-U. Lee, “Category-level metric scale object shape and pose
    estimation,” IEEE RAL, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] X. Lin and M. Zhu, “Clipose: Category-level object pose estimation with
    pre-trained vision-language knowledge,” arXiv preprint arXiv:2402.15726, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Z. Fan and Z. Song, “Acr-pose: Adversarial canonical representation reconstruction
    network for category level 6d object pose estimation,” arXiv preprint arXiv:2111.10524,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] T. Nie and J. Ma, “Category-level 6d pose estimation using geometry-guided
    instance-aware prior and multi-stage reconstruction,” IEEE RAL, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] L. Zhou and Z. Liu, “Dr-pose: A two-stage deformation-and-registration
    pipeline for category-level 6d object pose estimation,” in IROS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] L. Zou and Z. Huang, “Gpt-cope: A graph-guided point transformer for
    category-level object pose estimation,” IEEE TCSVT, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] G. Li and D. Zhu, “Sd-pose: Structural discrepancy aware category-level
    6d object pose estimation,” in WACV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] S. Yu and D.-H. Zhai, “Catformer: Category-level 6d object pose estimation
    with transformer,” in AAAI, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Y. He and H. Fan, “Towards self-supervised category-level object pose
    and size estimation,” arXiv preprint arXiv:2203.02884, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] G. Li and Y. Li, “Generative category-level shape and pose estimation
    with semantic primitives,” in CoRL, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] K. Chen and S. James, “Stereopose: Category-level 6d transparent object
    pose estimation from stereo images via back-view nocs,” in ICRA, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] H. Wang and Z. Fan, “Dtf-net: Category-level pose estimation and shape
    reconstruction via deformable template field,” in ACM MM, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] L. Zheng and T. H. E. Tse, “Georef: Geometric alignment across shape
    variation for category-level object pose refinement,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] K. Zhang and Y. Fu, “Self-supervised geometric correspondence for category-level
    6d object pose estimation in the wild,” in ICLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] A. Remus and S. D’Avella, “I2c-net: Using instance-level neural networks
    for monocular category-level 6d pose estimation,” IEEE RAL, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] J. Liu and Z. Cao, “Category-level 6d object pose estimation with structure
    encoder and reasoning attention,” IEEE TCSVT, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] X. Deng and J. Geng, “icaps: Iterative category-level object pose and
    shape estimation,” IEEE RAL, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] R. Wang and X. Wang, “Query6dof: Learning sparse queries as implicit
    shape prior for category-level 6dof pose estimation,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] B. Wan and Y. Shi, “Socs: Semantically-aware object coordinate space
    for category-level 6d object pose estimation under large shape variations,” in
    ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] X. Lin and W. Yang, “Instance-adaptive and geometric-aware keypoint learning
    for category-level 6d object pose estimation,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Y. Li and K. Mo, “Category-level multi-part multi-joint 3d shape assembly,”
    in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] C. Chi and S. Song, “Garmentnets: Category-level pose estimation for
    garments via canonical space shape completion,” in ICCV, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] L. Liu and J. Du, “Category-level articulated object 9d pose estimation
    via reinforcement learning,” in ACM MM, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] X. Liu and J. Zhang, “Self-supervised category-level articulated object
    pose estimation with part-level se (3) equivariance,” ICLR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] X. Li and Y. Weng, “Leveraging se (3) equivariance for self-supervised
    category-level object pose estimation from point clouds,” in NeurIPS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] D. Chen and J. Li, “Learning canonical shape space for category-level
    6d object pose and size estimation,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] J. Lin and H. Li, “Sparse steerable convolutions: An efficient learning
    of se (3)-equivariant features for estimation and tracking of object poses in
    3d space,” in NeurIPS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] H. Wang and W. Li, “Attention-guided rgb-d fusion network for category-level
    6d object pose estimation,” in IROS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] M. Oquab and T. Darcet, “Dinov2: Learning robust visual features without
    supervision,” arXiv preprint arXiv:2304.07193, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] J. J. Park and P. Florence, “Deepsdf: Learning continuous signed distance
    functions for shape representation,” in CVPR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] Y. Zhang and Z. Wu, “A transductive approach for video object segmentation,”
    in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] Y. Ono and E. Trulls, “Lf-net: Learning local features from images,”
    in NeurIPS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] X. Chen and Z. Dong, “Category level object pose estimation via neural
    analysis-by-synthesis,” in ECCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] L. Yen-Chen and P. Florence, “inerf: Inverting neural radiance fields
    for pose estimation,” in IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Y. Lin and J. Tremblay, “Single-stage keypoint-based category-level object
    pose estimation from an rgb image,” in ICRA, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] J. Guo and F. Zhong, “A visual navigation perspective for category-level
    object pose estimation,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] W. Ma and A. Wang, “Robust category-level 6d pose estimation with coarse-to-fine
    rendering of neural features,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] H. Zhang and A. Opipari, “Transnet: Category-level transparent object
    pose estimation,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Y. Lin and J. Tremblay, “Keypoint-based category-level object pose tracking
    from an rgb sequence with uncertainty estimation,” in ICRA, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] S. Yu and D.-H. Zhai, “Cattrack: Single-stage category-level 6d object
    pose tracking via convolution and vision transformer,” IEEE TMM, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] W. Goodwin and S. Vaze, “Zero-shot category-level object pose estimation,”
    in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] M. Zaccaria and F. Manhardt, “Self-supervised category-level 6d object
    pose estimation with optical flow consistency,” IEEE RAL, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] J. Cai and Y. He, “Ov9d: Open-vocabulary category-level 9d object pose
    and size estimation,” arXiv preprint arXiv:2403.12396, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] F. Di Felice and A. Remus, “Zero123-6d: Zero-shot novel view synthesis
    for rgb category-level 6d pose estimation,” arXiv preprint arXiv:2403.14279, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] G. Pitteri and S. Ilic, “Cornet: Generic 3d corners for 6d pose estimation
    of new objects without retraining,” in ICCVW, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] G. Pitteri and A. Bugeau, “3d object detection and pose estimation of
    unseen objects in color images with local surface embeddings,” in ACCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] M. Gou and H. Pan, “Unseen object 6d pose estimation: A benchmark and
    baselines,” arXiv preprint arXiv:2206.11808, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] F. Hagelskjær and R. L. Haugaard, “Keymatchnet: Zero-shot pose estimation
    in 3d point clouds by generalized keypoint matching,” arXiv preprint arXiv:2303.16102,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] H. Zhao and S. Wei, “Learning symmetry-aware geometry correspondences
    for 6d object pose estimation,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] K. He and G. Gkioxari, “Mask r-cnn,” in ICCV, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] J. Chen and M. Sun, “Zeropose: Cad-model-based zero-shot pose estimation,”
    arXiv preprint arXiv:2305.17934, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] A. Kirillov and E. Mintun, “Segment anything,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] Z. Qin and H. Yu, “Geometric transformer for fast and robust point cloud
    registration,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] A. Caraffa and D. Boscaini, “Freeze: Training-free zero-shot 6d pose
    estimation with geometric and vision foundation models,” arXiv preprint arXiv:2312.00947,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] J. Huang and H. Yu, “Matchu: Matching unseen objects for 6d pose estimation
    from rgb-d images,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] V. N. Nguyen and T. Groueix, “Cnos: A strong baseline for cad-based novel
    object segmentation,” in ICCV, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] I. Shugurov and F. Li, “Osop: A multi-stage one shot object pose estimation
    framework,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] B. Okorn and Q. Gu, “Zephyr: Zero-shot pose hypothesis rating,” in ICRA,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] E. P. Örnek and Y. Labbé, “Foundpose: Unseen object pose estimation with
    foundation features,” arXiv preprint arXiv:2311.18809, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] M. Sundermeyer and M. Durner, “Multi-path learning for object pose estimation
    across domains,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] V. N. Nguyen and Y. Hu, “Templates for 3d object pose estimation revisited:
    Generalization to new objects and robustness to occlusions,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] S. Moon and H. Son, “Genflow: Generalizable recurrent flow for 6d pose
    refinement of novel objects,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] T. Wang and G. Hu, “Object pose estimation via the aggregation of diffusion
    features,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] V. Balntas and A. Doumanoglou, “Pose guided rgbd feature learning for
    3d object pose estimation,” in ICCV, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] Y. Wen and X. Li, “Disp6d: Disentangled implicit shape and pose learning
    for scalable 6d pose estimation,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] B. Busam and H. J. Jung, “I like to move it: 6d pose estimation as an
    action decision process,” arXiv preprint arXiv:2009.12678, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] D. Cai and J. Heikkilä, “Ove6d: Object viewpoint encoding for depth-based
    6d object pose estimation,” in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] W. Kabsch, “A solution for the best rotation to relate two sets of vectors,”
    Acta Crystallographica Section A: Crystal Physics, Diffraction, Theoretical and
    General Crystallography, 1976.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] E. Corona and K. Kundu, “Pose estimation for objects with rotational
    symmetry,” in IROS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] C. Zhao and Y. Hu, “Fusing local similarities for retrieval-based 3d
    orientation estimation of unseen objects,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] S. Thalhammer and J.-B. Weibel, “Self-supervised vision transformers
    for 3d pose estimation of novel objects,” Image Vision Comput, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] P. Ausserlechner and D. Haberger, “Zs6d: Zero-shot 6d object pose estimation
    using vision transformers,” arXiv preprint arXiv:2309.11986, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] A. Dosovitskiy and L. Beyer, “An image is worth 16x16 words: Transformers
    for image recognition at scale,” in CoLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] J. Tremblay and B. Wen, “Diff-dope: Differentiable deep object pose estimation,”
    arXiv preprint arXiv:2310.00463, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] Ultralytics, “GitHub - ultralytics/yolov5.” [https://github.com/ultralytics/yolov5](https://github.com/ultralytics/yolov5),
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] Y. He and Y. Wang, “Fs6d: Few-shot 6d pose estimation of novel objects,”
    in CVPR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] P. Castro and T.-K. Kim, “Posematcher: One-shot 6d object pose estimation
    by deep feature matching,” in ICCVW, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] J. Lee and Y. Cabon, “Mfos: Model-free & one-shot object pose estimation,”
    in AAAI, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] Y. Du and Y. Xiao, “Pizza: A powerful image-only zero-shot zero-cad approach
    to 6 dof tracking,” in 3DV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] N. Gao and V. A. Ngo, “Sa6d: Self-adaptive few-shot 6d pose estimator
    for novel and occluded objects,” in CoRL, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] D. Cai and J. Heikkilä, “Gs-pose: Cascaded framework for generalizable
    segmentation-based 6d object pose estimation,” arXiv preprint arXiv:2403.10683,
    2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] C. Jaime and B. Davide, “Open-vocabulary object 6d pose estimation,”
    in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] J. Wu and Y. Wang, “Unseen object pose estimation via registration,”
    in RCAR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] J. Sun and Z. Shen, “Loftr: Detector-free local feature matching with
    transformers,” in CVPR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] P.-E. Sarlin and D. DeTone, “Superglue: Learning feature matching with
    graph neural networks,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] Z. Fan and P. Pan, “Pope: 6-dof promptable pose estimation of any object,
    in any scene, with one reference,” arXiv preprint arXiv:2305.15727, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] P. Weinzaepfel and V. Leroy, “Croco: Self-supervised pre-training for
    3d vision tasks by cross-view completion,” in NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] C. Zhao and Y. Hu, “Locposenet: Robust location prior for unseen object
    pose estimation,” in 3DV, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] P. Pan and Z. Fan, “Learning to estimate 6dof pose from limited data:
    A few-shot, generalizable approach using rgb images,” in 3DV, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] B. Wen and J. Tremblay, “Bundlesdf: Neural 6-dof tracking and 3d reconstruction
    of unknown objects,” in CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] V. N. Nguyen and T. Groueix, “Nope: Novel object pose estimation from
    a single image,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] K. Park and T. Patten, “Neural object learning for 6d pose estimation
    using a few cluttered images,” in ECCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] H. Chen and F. Manhardt, “Texpose: Neural texture learning for self-supervised
    6d object pose estimation,” in CVPR, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] Y. Li and J. Sun, “Weakly supervised 6d pose estimation for robotic grasping,”
    in SIGGRAPH, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] K. Chen and R. Cao, “Sim-to-real 6d object pose estimation via iterative
    self-training for robotic bin picking,” in ECCV, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] B. Fu and S. K. Leong, “6d robotic assembly based on rgb-only object
    pose estimation,” in IROS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] J. Tremblay and T. To, “Deep object pose estimation for semantic robotic
    grasping of household objects,” arXiv preprint arXiv:1809.10790, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] Z. Dong and S. Liu, “Ppr-net: Point-wise pose regression network for
    instance segmentation and 6dd pose estimation in bin-picking scenarios,” in IROS,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] C. Zhuang and H. Wang, “Attentionvote: A coarse-to-fine voting network
    of anchor-free 6d pose estimation on point cloud for robotic bin-picking application,”
    Robot Cim-int Manuf, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] K. Wada and E. Sucar, “Morefusion: Multi-object reasoning for 6d pose
    estimation from volumetric fusion,” in CVPR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] H. Zhang and Q. Cao, “Detect in rgb, optimize in edge: Accurate 6d pose
    estimation for texture-less industrial parts,” in ICRA, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] J. Chang and M. Kim, “Ghostpose: Multi-view pose estimation of transparent
    objects for robot hand grasping,” in IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] J. Kim and H. Pyo, “Tomato harvesting robotic system based on deep-tomatos:
    Deep learning network using transformation loss for 6d pose estimation of maturity
    classified tomatoes with side-stem,” Comput Electron Agric, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] C. Liu and W. Sun, “Fine segmentation and difference-aware shape adjustment
    for category-level 6dof object pose estimation,” Appl Intell, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] S. Yu and D.-H. Zhai, “Category-level 6-d object pose estimation with
    shape deformation for robotic grasp detection,” IEEE TNNLS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] J. Sun and Y. Wang, “Ick-track: A category-level 6-dof pose tracker using
    inter-frame consistent keypoints for aerial manipulation,” in IROS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] S. Yu and D.-H. Zhai, “Robotic grasp detection based on category-level
    object pose estimation with self-supervised learning,” IEEE TMEC, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] Y. Su and J. Rambach, “Deep multi-state object pose estimation for augmented
    reality assembly,” in ISMAR-Adjunct, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] R. Pandey and P. Pidlypenskyi, “Efficient 6-dof tracking of handheld
    objects from an egocentric viewpoint,” in ECCV, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] P. F. Proença and Y. Gao, “Deep learning for spacecraft pose estimation
    from photorealistic rendering,” in ICRA, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] U. E. 4, “Unreal engine 4.” [https://www.unrealengine.com](https://www.unrealengine.com).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] S. Wang and S. Wang, “Ca-spacenet: Counterfactual analysis for 6d pose
    estimation in space,” in IROS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] M. Ulmer and M. Durner, “6d object pose estimation from approximate 3d
    models for orbital robotics,” in IROS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] T. Patten and K. Park, “Object learning for 6d pose estimation and grasping
    from rgb-d videos of in-hand manipulation,” in IROS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] H. Qi and C. Zhao, “Hoisdf: Constraining 3d hand-object pose estimation
    with global signed distance fields,” in CVPR, 2024.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] S. Hoque and S. Xu, “Deep learning for 6d pose estimation of objects-a
    case study for autonomous driving,” Expert Syst Appl, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] J. L. Elman, “Finding structure in time,” Cogn Sci, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] H. Sun and P. Ni, “Panelpose: A 6d pose estimation of highly-variable
    panel object for robotic robust cockpit panel inspection,” in IROS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
