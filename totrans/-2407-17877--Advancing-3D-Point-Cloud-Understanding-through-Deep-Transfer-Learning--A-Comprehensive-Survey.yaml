- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:31:06'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2407.17877] Advancing 3D Point Cloud Understanding through Deep Transfer Learning:
    A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.17877](https://ar5iv.labs.arxiv.org/html/2407.17877)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \useunder
  prefs: []
  type: TYPE_NORMAL
- en: \ul
  prefs: []
  type: TYPE_NORMAL
- en: \cormark
  prefs: []
  type: TYPE_NORMAL
- en: '[1]'
  prefs: []
  type: TYPE_NORMAL
- en: '[] []'
  prefs: []
  type: TYPE_NORMAL
- en: 'Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shahab Saquib Sohail    Yassine Himeur yhimeur@ud.ac.ae    Hamza Kheddar   
    Abbes Amira    Fodil Fadli    Shadi Atalla    Abigail Copiaco    Wathiq Mansoor
    School of Computing Science and Engineering, VIT Bhopal University, Sehore, MP
    466114, India College of Engineering and Information Technology, University of
    Dubai, Dubai, UAE LSEA Laboratory, Department of Electrical Engineering, University
    of Medea, 26000, Algeria Department of Computer Science, University of Sharjah,
    UAE Institute of Artificial Intelligence, De Montfort University, Leicester, United
    Kingdom Department of Architecture & Urban Planning, Qatar University, Doha, 2713,
    Qatar
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The 3D point cloud (3DPC) has significantly evolved and benefited from the advance
    of deep learning (DL). However, the latter faces various issues, including the
    lack of data or annotated data, the existence of a significant gap between training
    data and test data, and the requirement for high computational resources. To that
    end, deep transfer learning (DTL), which decreases dependency and costs by utilizing
    knowledge gained from a source data/task in training a target data/task, has been
    widely investigated. Numerous DTL frameworks have been suggested for aligning
    point clouds obtained from several scans of the same scene. Additionally, DA,
    which is a subset of DTL, has been modified to enhance the point cloud data’s
    quality by dealing with noise and missing points. Ultimately, fine-tuning and
    DA approaches have demonstrated their effectiveness in addressing the distinct
    difficulties inherent in point cloud data. This paper presents the first review
    shedding light on this aspect. it provides a comprehensive overview of the latest
    techniques for understanding 3DPC using DTL and domain adaptation (DA). Accordingly,
    DTL’s background is first presented along with the datasets and evaluation metrics.
    A well-defined taxonomy is introduced, and detailed comparisons are presented,
    considering different aspects such as different knowledge transfer strategies,
    and performance. The paper covers various applications, such as 3DPC object detection,
    semantic labeling, segmentation, classification, registration, downsampling/upsampling,
    and denoising. Furthermore, the article discusses the advantages and limitations
    of the presented frameworks, identifies open challenges, and suggests potential
    research directions.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 3D point cloud \sepDeep transfer learning \sepDomain adaption \sepFine-tuning
    \sepClassification \sepSegmentation and registration
  prefs: []
  type: TYPE_NORMAL
- en: 3DPC
  prefs: []
  type: TYPE_NORMAL
- en: 3D point cloud
  prefs: []
  type: TYPE_NORMAL
- en: GAN
  prefs: []
  type: TYPE_NORMAL
- en: generative adversarial network
  prefs: []
  type: TYPE_NORMAL
- en: CV
  prefs: []
  type: TYPE_NORMAL
- en: computer vision
  prefs: []
  type: TYPE_NORMAL
- en: DL
  prefs: []
  type: TYPE_NORMAL
- en: deep learning
  prefs: []
  type: TYPE_NORMAL
- en: ML
  prefs: []
  type: TYPE_NORMAL
- en: machine learning
  prefs: []
  type: TYPE_NORMAL
- en: DTL
  prefs: []
  type: TYPE_NORMAL
- en: deep transfer learning
  prefs: []
  type: TYPE_NORMAL
- en: DA
  prefs: []
  type: TYPE_NORMAL
- en: domain adaptation
  prefs: []
  type: TYPE_NORMAL
- en: UDA
  prefs: []
  type: TYPE_NORMAL
- en: unsupervised domain adaptation
  prefs: []
  type: TYPE_NORMAL
- en: TD
  prefs: []
  type: TYPE_NORMAL
- en: target domain
  prefs: []
  type: TYPE_NORMAL
- en: SD
  prefs: []
  type: TYPE_NORMAL
- en: source omain
  prefs: []
  type: TYPE_NORMAL
- en: CNN
  prefs: []
  type: TYPE_NORMAL
- en: convolutional neural network
  prefs: []
  type: TYPE_NORMAL
- en: MAE
  prefs: []
  type: TYPE_NORMAL
- en: multi-scale masked autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: IoU
  prefs: []
  type: TYPE_NORMAL
- en: intersection over union
  prefs: []
  type: TYPE_NORMAL
- en: SSL
  prefs: []
  type: TYPE_NORMAL
- en: self-supervised learning
  prefs: []
  type: TYPE_NORMAL
- en: SVCN
  prefs: []
  type: TYPE_NORMAL
- en: sparse voxel completion network
  prefs: []
  type: TYPE_NORMAL
- en: OA
  prefs: []
  type: TYPE_NORMAL
- en: overall accuracy
  prefs: []
  type: TYPE_NORMAL
- en: CD
  prefs: []
  type: TYPE_NORMAL
- en: Chamfer distance
  prefs: []
  type: TYPE_NORMAL
- en: LiDAR
  prefs: []
  type: TYPE_NORMAL
- en: light detection and ranging
  prefs: []
  type: TYPE_NORMAL
- en: BiLSTM
  prefs: []
  type: TYPE_NORMAL
- en: bidirectional LSTM
  prefs: []
  type: TYPE_NORMAL
- en: ITL
  prefs: []
  type: TYPE_NORMAL
- en: inductive transfer learning
  prefs: []
  type: TYPE_NORMAL
- en: SFUDA
  prefs: []
  type: TYPE_NORMAL
- en: source-free unsupervised DA
  prefs: []
  type: TYPE_NORMAL
- en: PCM
  prefs: []
  type: TYPE_NORMAL
- en: point cloud mixup
  prefs: []
  type: TYPE_NORMAL
- en: MLP
  prefs: []
  type: TYPE_NORMAL
- en: multi-layer perceptron
  prefs: []
  type: TYPE_NORMAL
- en: CLIP
  prefs: []
  type: TYPE_NORMAL
- en: contrastive vision-language pretraining
  prefs: []
  type: TYPE_NORMAL
- en: TTL
  prefs: []
  type: TYPE_NORMAL
- en: transductive transfer learning
  prefs: []
  type: TYPE_NORMAL
- en: CLDA
  prefs: []
  type: TYPE_NORMAL
- en: cross-modal learning DA
  prefs: []
  type: TYPE_NORMAL
- en: AE
  prefs: []
  type: TYPE_NORMAL
- en: auto-encoder
  prefs: []
  type: TYPE_NORMAL
- en: RNN
  prefs: []
  type: TYPE_NORMAL
- en: recurrent neural network
  prefs: []
  type: TYPE_NORMAL
- en: CRF
  prefs: []
  type: TYPE_NORMAL
- en: conditional random field
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 1.1 Preliminary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \Ac
  prefs: []
  type: TYPE_NORMAL
- en: CV continues to attract significant interest as a growing branch of [machine
    learning](#id5.5.id5) ([ML](#id5.5.id5)), which targets different problems in
    smart cities, medicine, autonomous driving, medicine, video surveillance, scene
    understanding, safety, and security [[1](#bib.bib1), [2](#bib.bib2)]. With the
    rapid development of sensor technology, 3D sensors have recently been widely adopted,
    which increased the interest of the [computer vision](#id3.3.id3) ([CV](#id3.3.id3))
    research community in developing 3D sensor data processing methodologies [[3](#bib.bib3),
    [4](#bib.bib4)]. Additionally, with the use of augmented reality and virtual reality
    (AR/VR), 3D vision problems become more important since they provide much richer
    information than 2D [[5](#bib.bib5), [6](#bib.bib6)]. Typically, numerous 3D sensors
    for acquiring 3D data have been used, including depth-sensing cameras (such as
    Apple Depth, RealSense, and Kinect cameras), [light detection and ranging](#id18.18.id18)
    ([LiDAR](#id18.18.id18)), which is used for mobile mapping terrestrial laser scanning,
    aerial [LiDAR](#id18.18.id18) [[7](#bib.bib7)]. Moreover, [generative adversarial
    networks](#id2.2.id2) can be used to augment data when data scarcity problem occurs.
    In this context, 3D data can provide rich scale, shape, and geometric information
    to be complemented with 2D images for better representing the surrounding environment
    [[8](#bib.bib8), [9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: While there are different approaches to representing 3D data, such as volumetric
    grids, meshes, and depth images, the [3D point clouds](#id1.1.id1) are the most
    used. Typically, a [3DPC](#id1.1.id1) representation conserves the original geometric
    information in 3D space [[10](#bib.bib10)]. Besides, a [3DPC](#id1.1.id1) is an
    ensemble of data points representing an object’s surfaces in 3D coordinates. In
    this regard, spatial coordinates are used to represent data points, surface normals,
    and color information and format ( e.g., RGB, HSV, and others) [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, although [3DPC](#id1.1.id1) can be considered as non-Euclidean
    geometric data, in practice, delineated as small Euclidean subgroups with a standard
    coordinates system and global parametrization [[12](#bib.bib12)]. The success
    of this representation is due to its invariability to transformations and attacks,
    including rotation, scaling, translation, etc., which makes it robust to extracting
    objects’ features. Moreover, using [deep learnings](#id4.4.id4) techniques to
    extract [3DPC](#id1.1.id1) data and perform complex tasks such as detection, classification,
    recognition, and retrieval has expanded their adoption in many research and development
    areas [[13](#bib.bib13)]. Typically, [3DPCs](#id1.1.id1) are widely adopted to
    [CV](#id3.3.id3) tasks, such as object recognition, semantic segmentation, and
    scene understanding [[14](#bib.bib14), [15](#bib.bib15)]. Additionally, they are
    used to create detailed models of environments for navigation and localization,
    detailed maps and models of buildings, landscapes, and other structures, as well
    as detailed models of buildings and other structures for design and planning [[16](#bib.bib16)].
    Moreover, they can be utilized to inspect and maintain industrial equipment, infrastructure,
    and other assets [[17](#bib.bib17), [18](#bib.bib18)]. Besides, the [3DPC](#id1.1.id1)
    technology helps implement realistic and immersive experiences in AR and VR applications.
  prefs: []
  type: TYPE_NORMAL
- en: The traditional [ML](#id5.5.id5) approaches and, recently, [DL](#id4.4.id4)
    methods witnessed rapid growth and have attracted researchers because of their
    applicability in many real-life applications, including smart healthcare [[19](#bib.bib19)],
    disease diagnosis and medical image classifications [[4](#bib.bib4)], business
    and marketing [[20](#bib.bib20)], recommender systems [[21](#bib.bib21)], energy
    [[22](#bib.bib22)], agriculture [[23](#bib.bib23)], robotics [[24](#bib.bib24)],
    and more. The primary advantage of these learning algorithms is that they train
    a model that learns the hidden pattern and can be exploited for any specific purpose
    with high accuracy. However, with time, some issues have been raised by the research
    communities. First, these learning algorithms require extensive training datasets,
    especially [DL](#id4.4.id4) algorithms [[1](#bib.bib1)]. Second, most [DL](#id4.4.id4)
    models are based on supervised learning and then require huge amounts of ground-truth
    data. Third, it is a common assumption that the data that is trained and the future
    data to be processed must have the same distribution and be in the same feature
    space [[25](#bib.bib25)]. It becomes difficult and sometimes impossible to maintain
    the aforementioned assumption in several real-life scenarios. For example, when
    performing specific tasks, such as classification in a particular domain; however,
    we may have the requisite trained data in another domain, where both datasets
    may not have the same distribution or same feature space.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cdc48809bf2a69e00c992328d4267ece.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Difference between conventional ML and DTL techniques for multiple
    tasks: (a) conventional ML and (b) DTL.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, an essential factor for ensuring accurate performance of [ML](#id5.5.id5)
    algorithms is consistency in the distribution and feature space of the datasets
    used for training and testing. If the distribution of data changes, it becomes
    necessary to rebuild the model from scratch by collecting new training data [[26](#bib.bib26)].
    However, this process is not only costly but also often impractical due to the
    challenges associated with re-collecting training data. Therefore, there is a
    need for a mechanism that can reduce the expenses associated with re-collecting
    training data, minimize the cost of data labeling, and still achieve high performance
    without requiring extensive amounts of training data. To that end, knowledge transfer
    has been suggested, which would fit the above constraints and can significantly
    improve performance. This knowledge transfer mechanism is termed [deep transfer
    learning](#id6.6.id6) ([DTL](#id6.6.id6)) [[1](#bib.bib1), [26](#bib.bib26), [27](#bib.bib27)].
    Fig. [1](#S1.F1 "Figure 1 ‣ 1.1 Preliminary ‣ 1 Introduction ‣ Advancing 3D Point
    Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey") explains
    briefly the difference between conventional [ML](#id5.5.id5) and [DTL](#id6.6.id6)
    techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the one hand, employing [DL](#id4.4.id4) for [3DPC](#id1.1.id1) poses a
    rather complex challenges because of: (i) the variability in point density and
    reflective intensity, influenced by the distance between objects and [LiDAR](#id18.18.id18)
    sensors, (ii) existence of noise originated from sensors ( e.g., perturbations
    and outliers), (iii) data incompleteness caused by occlusion between objects and
    cluttered background, and (iv) confusion categories caused by shape-similar or
    reflectance-similar objects. On the other hand, challenges of [DL](#id4.4.id4)/[DTL](#id6.6.id6)
    models include (i) permutation and orientation invariance, (ii) 3D translation
    and rotation challenges, (iii) difficulty of handling large-scale datasets, (iv)
    securing computational resources, and (v) low performance. Fig. [2](#S1.F2 "Figure
    2 ‣ 1.1 Preliminary ‣ 1 Introduction ‣ Advancing 3D Point Cloud Understanding
    through Deep Transfer Learning: A Comprehensive Survey") summarizes the tasks
    and challenges related to data and [DL](#id4.4.id4)/[DTL](#id6.6.id6)-based applications
    on [3DPCs](#id1.1.id1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/216d9873281d2b6633f8bd0c799ba2dd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Tasks and challenges related to data and [DL](#id4.4.id4)/[DTL](#id6.6.id6)-based
    applications on [3DPCs](#id1.1.id1).'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Our contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Arguably, one of the top success stories of [DL](#id4.4.id4) is [DTL](#id6.6.id6).
    Accordingly, the finding that pretraining a [DL](#id4.4.id4) network on a rich
    source set (e.g., ImageNet) can help boost performance once fine-tuned on a usually
    much smaller target set has been instrumental to many applications in language
    and [CV](#id3.3.id3). Likewise, several studies have explored the potential of
    [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) applications to address the aforementioned
    challenges. This offers an opportunity to present the first review article that
    comprehensively examines the contributions of [DTL](#id6.6.id6)-based approaches
    in advancing our understanding of 3D scenes, such as [3DPC](#id1.1.id1) segmentation,
    3D object detection, 3D object classification, [3DPC](#id1.1.id1) registration,
    and more. For instance, [DTL](#id6.6.id6) has been shown to be effective in various
    ways, including (i) leveraging knowledge learned from synthetic data to improve
    semantic segmentation in real [LiDAR](#id18.18.id18) [3DPC](#id1.1.id1), (ii)
    enabling accurate classification of [3DPCs](#id1.1.id1) even with limited training
    data, (iii) mitigating the issue of overfitting in [3DPC](#id1.1.id1) classification,
    and (iv) reducing the labor-intensive process of annotating [3DPC](#id1.1.id1)
    datasets, among other benefits. In this respect, this paper first presents a background
    of [DTL](#id6.6.id6), where a well-defined taxonomy is conducted. Moving on, datasets
    and evaluation metrics used to evaluate existing [DTL](#id6.6.id6)-based [3DPCs](#id1.1.id1)
    techniques are discussed. Next, existing studies are overviewed based on different
    aspects, and their pros and cons are identified. After that, the challenges encountered
    when using [DTL](#id6.6.id6) for [3DPCs](#id1.1.id1) are identified before deriving
    future research directions. To summarize, the main contributions of this review
    can be stated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Presenting, to the best of the authors’ knowledge, the first review article
    on using [DTL](#id6.6.id6) and [domain adaptation](#id7.7.id7) ([DA](#id7.7.id7))
    for [3DPC](#id1.1.id1) applications;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discussing existing datasets and evaluation metrics used for assessing the performance
    of [3DPC](#id1.1.id1) frameworks based in [DTL](#id6.6.id6);
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing a well-defined taxonomy to overview existing [DTL](#id6.6.id6)-based
    [3DPC](#id1.1.id1) studies;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying the current challenges encountered when using [DTL](#id6.6.id6)
    for [3DPC](#id1.1.id1) understanding tasks; and
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deriving future research directions that can attract significant research interest
    in the near future.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.3 Methodology of the survey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The present review study is based on the protocols and procedure suggested
    in [[28](#bib.bib28)], which has recently been adopted widely in many studies
    such as [[29](#bib.bib29)]. This study is motivated by the recent developments
    in [3DPC](#id1.1.id1) technology. There are some reviews available on the theme
    which has exploited [ML](#id5.5.id5) [[30](#bib.bib30)], [DL](#id4.4.id4) [[31](#bib.bib31)],
    and reinforcement learning [[32](#bib.bib32)]; however, to the best of our search
    and efforts, we could not find a review exclusively exploring [DTL](#id6.6.id6)
    for different [3DPC](#id1.1.id1) tasks. The proliferation of [DTL](#id6.6.id6)
    techniques has influenced researchers by virtue of which the span of related techniques
    has been expanding and has covered much recent AI-driven research. Our study gives
    the readers an insight into how [DTL](#id6.6.id6) is being implemented for performing
    many [3DPC](#id1.1.id1) tasks. This review aims at finding the answer to research
    questions presented in Table [1](#S1.T1 "Table 1 ‣ 1.3 Methodology of the survey
    ‣ 1 Introduction ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Research questions covered in this review.'
  prefs: []
  type: TYPE_NORMAL
- en: '| No. | Question | Objective |'
  prefs: []
  type: TYPE_TB
- en: '| RQ1 | What are the key principles and theoretical foundations of [DTL](#id6.6.id6)
    and [3DPC](#id1.1.id1)? | Provide a foundational understanding of the key principles
    and theories behind [DTL](#id6.6.id6) and [3DPC](#id1.1.id1). This will aid readers
    in grasifying the basic tenets upon which the field of study is built and support
    comprehension of more complex concepts discussed later in the review. |'
  prefs: []
  type: TYPE_TB
- en: '| RQ2 | Why the application of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) is receiving
    increasing attention? | Examine and explain why [DTL](#id6.6.id6) is increasingly
    being used for [3DPC](#id1.1.id1). This will help highlight the unique benefits
    and opportunities offered by [DTL](#id6.6.id6) in handling 3D data, indicating
    its growing importance in the field. |'
  prefs: []
  type: TYPE_TB
- en: '| RQ3 | How the different [3DPC](#id1.1.id1) tasks can be implemented using
    [DTL](#id6.6.id6)? | Present a comprehensive overview of how [DTL](#id6.6.id6)
    can be utilized in various [3DPC](#id1.1.id1) tasks. This objective aims to showcase
    the versatility and wide-ranging applicability of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1)
    tasks, providing practical insights for researchers and practitioners. |'
  prefs: []
  type: TYPE_TB
- en: '| RQ4 | What are the [DTL](#id6.6.id6) mechanisms and settings for performing
    the [3DPC](#id1.1.id1) tasks and optimizing their performance? | Delve into the
    specific mechanisms and settings of [DTL](#id6.6.id6) that optimize performance
    in [3DPC](#id1.1.id1) tasks. The objective here is to provide a clear understanding
    of the operational details of [DTL](#id6.6.id6), which can serve as a guide for
    those intending to implement [DTL](#id6.6.id6) in their own [3DPC](#id1.1.id1)
    tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| RQ5 | Which [DTL](#id6.6.id6) models are better appropriate for [3DPC](#id1.1.id1)
    tasks? | Evaluate and suggest [DTL](#id6.6.id6) models that are most suitable
    for [3DPC](#id1.1.id1) tasks. By doing this, the review will provide practical
    advice for readers looking to apply [DTL](#id6.6.id6) in [3DPC](#id1.1.id1), assisting
    them in model selection. |'
  prefs: []
  type: TYPE_TB
- en: '| RQ6 | What are the issues in implementing [DTL](#id6.6.id6) for performing
    the above tasks, and how to tackle these challenges? | Identify the key challenges
    associated with implementing [DTL](#id6.6.id6) for [3DPC](#id1.1.id1) tasks and
    to propose potential solutions to these challenges. This will help improve the
    application of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) by addressing problems
    head-on, promoting a robust and reliable usage of these techniques. |'
  prefs: []
  type: TYPE_TB
- en: '| RQ7 | What are the future research directions for improving [DTL](#id6.6.id6)
    in [3DPC](#id1.1.id1)? | Forecast future research directions for [DTL](#id6.6.id6)
    in [3DPC](#id1.1.id1). This objective aims to stimulate new research initiatives
    by identifying promising avenues for further exploration, driving innovation and
    development in the field. |'
  prefs: []
  type: TYPE_TB
- en: itemize
  prefs: []
  type: TYPE_NORMAL
- en: Why the application of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) is receiving
    increasing attention?
  prefs: []
  type: TYPE_NORMAL
- en: How the different [3DPC](#id1.1.id1) tasks can be implemented using [DTL](#id6.6.id6)?
  prefs: []
  type: TYPE_NORMAL
- en: What are the [DTL](#id6.6.id6) mechanisms and settings for performing the [3DPC](#id1.1.id1)
    tasks and optimizing their performance?
  prefs: []
  type: TYPE_NORMAL
- en: Which [DTL](#id6.6.id6) models are better appropriate for [3DPC](#id1.1.id1)
    tasks?
  prefs: []
  type: TYPE_NORMAL
- en: What are the issues in implementing [DTL](#id6.6.id6) for performing the above
    tasks, and how to tackle these challenges?
  prefs: []
  type: TYPE_NORMAL
- en: What are the future research directions for improving [DTL](#id6.6.id6) in [3DPC](#id1.1.id1)?
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ef9092bb3d06ecc9812424405788867f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb83c035bd36fd36e665ced37564e6a6.png)'
  prefs: []
  type: TYPE_IMG
- en: (b)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Summary of the approach used to search and select articles included
    in the review: (a) The adopted search procedure and (b) The selection criteria.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The bibliometric research is performed in the context of a narrative review.
    The recent works related to the [3DPC](#id1.1.id1) involving [DTL](#id6.6.id6)
    techniques for performing their respective tasks have been searched. We searched
    the relevant keywords, like, "3D point cloud", "3D point cloud segmentation",
    "3D point cloud classification", "3D object detection", "deep transfer learning",
    and "domain adaptation", with different combinations on Scopus database in titles,
    abstracts, and keywords. The adopted search procedure is explained in Figs. [3](#S1.F3
    "Figure 3 ‣ 1.3 Methodology of the survey ‣ 1 Introduction ‣ Advancing 3D Point
    Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")(a)
    and [3](#S1.F3 "Figure 3 ‣ 1.3 Methodology of the survey ‣ 1 Introduction ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")(b).
    Typically, in response to the relevant queries, 176 articles were retrieved, with
    149 of them being non-duplicates. These articles underwent further scrutiny to
    determine their relevance to the theme of the present study. Ultimately, 108 papers
    were selected for inclusion in this study.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper is organized as follows: Section [2](#S2 "2 Background ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    introduces background related to [DTL](#id6.6.id6) definitions and terms, pre-trained
    models, useful datasets, and metrics used in [3DPC](#id1.1.id1). Section [3](#S3
    "3 Overview of DTL ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey") reviews relevant literature and work related
    to [DTL](#id6.6.id6) and [DA](#id7.7.id7). Section [4](#S4 "4 Applications of
    TL-based 3DPCs ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey") outlines state-of-the-art proposed applications
    based on [DTL](#id6.6.id6). Section [5](#S5 "5 Open Challenges ‣ Advancing 3D
    Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    discusses open challenges. Section [6](#S6 "6 Future Research directions ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    suggests recent future research directions. Finally, Section [7](#S7 "7 Conclusion
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey") concludes the survey. Fig. [4](#S1.F4 "Figure 4 ‣ 1.3 Methodology of
    the survey ‣ 1 Introduction ‣ Advancing 3D Point Cloud Understanding through Deep
    Transfer Learning: A Comprehensive Survey") illustrates the survey’s structure,
    enhancing readability, and offering guidance to readers.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/92dc1d2795d117db3802eed71b5e31bd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Survey structure with sections and sub-sections disctribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1 3D Point Cloud
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A 3DPC is a collection of data points in a three-dimensional coordinate system.
    Each point in the point cloud is represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P=\{p_{1},p_{2},...,p_{n}\}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where each point $p_{i}$ is a vector in $\mathbb{R}^{3}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p_{i}=(x_{i},y_{i},z_{i})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 2.1.1 Operations and Transformations
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Common operations and transformations applied to 3DPCs include:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Translation: Moving the point cloud by adding a constant vector $\mathbf{t}=(t_{x},t_{y},t_{z})$
    to each point:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $p_{i}^{\prime}=p_{i}+\mathbf{t}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rotation: Rotating the point cloud using a rotation matrix $\mathbf{R}$. The
    rotation matrix is a 3x3 matrix:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $p_{i}^{\prime}=\mathbf{R}p_{i}$ |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: Rotation matrices can be derived from Euler angles, axis-angle, or quaternion
    representations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scaling: Changing the size by scaling each point by a scalar $s$ or different
    scalars for each axis:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $p_{i}^{\prime}=s\cdot p_{i}\text{ or }p_{i}^{\prime}=(s_{x}x_{i},s_{y}y_{i},s_{z}z_{i})$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transformation: A combination of translation, rotation, and scaling, represented
    as matrix multiplication in homogeneous coordinates:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | <math   alttext="p_{i}^{\prime}=\mathbf{T}\begin{bmatrix}x_{i}\\
    y_{i}\\'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: z_{i}\\
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1\end{bmatrix}" display="block"><semantics ><mrow 
    ><msubsup  ><mi
     >p</mi><mi 
    >i</mi><mo  >′</mo></msubsup><mo
     >=</mo><mrow 
    ><mi  >𝐓</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo  >[</mo><mtable
    displaystyle="true" rowspacing="0pt"  ><mtr
     ><mtd 
    ><msub  ><mi
     >x</mi><mi
     >i</mi></msub></mtd></mtr><mtr
     ><mtd 
    ><msub  ><mi
     >y</mi><mi
     >i</mi></msub></mtd></mtr><mtr
     ><mtd 
    ><msub  ><mi
     >z</mi><mi
     >i</mi></msub></mtd></mtr><mtr
     ><mtd 
    ><mn  >1</mn></mtd></mtr></mtable><mo
     >]</mo></mrow></mrow></mrow><annotation-xml
    encoding="MathML-Content" ><apply  ><apply
     ><csymbol cd="ambiguous" 
    >superscript</csymbol><apply 
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci
     >𝑝</ci><ci 
    >𝑖</ci></apply><ci  >′</ci></apply><apply
     ><ci 
    >𝐓</ci><apply  ><csymbol
    cd="latexml"  >matrix</csymbol><matrix
     ><matrixrow 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑥</ci><ci
     >𝑖</ci></apply></matrixrow><matrixrow
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑦</ci><ci 
    >𝑖</ci></apply></matrixrow><matrixrow 
    ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑧</ci><ci
     >𝑖</ci></apply></matrixrow><matrixrow
     ><cn type="integer" 
    >1</cn></matrixrow></matrix></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >p_{i}^{\prime}=\mathbf{T}\begin{bmatrix}x_{i}\\
    y_{i}\\ z_{i}\\ 1\end{bmatrix}</annotation></semantics></math> |  |
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'where $\mathbf{T}$ is a 4x4 transformation matrix:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathbf{T}=\begin{bmatrix}\mathbf{R}&amp;\mathbf{t}\\ 0&amp;1\end{bmatrix}$
    |  |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 2.2 Definitions pertaining to DTL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This section presents the main definitions used in [DTL](#id6.6.id6)-based [3DPC](#id1.1.id1)
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Def. 1 - Domain: Consider a dataset $X$ consisting of $n$ observations, $x_{1},\cdots,x_{n}$,
    in a feature space $\chi$. The marginal probability distribution of $X$ is represented
    by $P(X)$. A domain, denoted as $\mathbb{D}$, is defined as the set containing
    $X$ and $P(X)$. In the field of [DTL](#id6.6.id6), the domain containing the initial
    knowledge is referred to as the [source omain](#id10.10.id10) ([SD](#id10.10.id10)),
    represented by $\mathbb{D}_{S}$, while the domain containing the unknown knowledge
    to be learned is called the [target domain](#id9.9.id9) ([TD](#id9.9.id9)) and
    represented by $\mathbb{D}_{T}$ .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Def. 2 - Task: The dataset $X$ contains $n$ observations, $x_{1},\cdots,x_{n}$,
    in a feature space $\chi$, and it is associated with a set of labels $Y$, $y_{1},\cdots,y_{n}$,
    in a label space $\gamma$. A task can be defined as a set containing the labels
    $Y$ and a learning objective predictive function $\mathbb{F}(X)$, represented
    by $\mathbb{T}=\{Y,\mathbb{F}(X)\}$. This function is also denoted as the conditional
    distribution $P(Y|X)$. In accordance with this definition of task, the label spaces
    of the [SD](#id10.10.id10) and [TD](#id9.9.id9) are represented as $\gamma_{S}$
    and $\gamma_{T}$ respectively [[33](#bib.bib33)].'
  prefs: []
  type: TYPE_NORMAL
- en: One way to classify [DTL](#id6.6.id6) methods is based on their approach to
    transferring knowledge, which can be broken down into what, when, and how knowledge
    is transferred.
  prefs: []
  type: TYPE_NORMAL
- en: '(a) What knowledge is transferred: The classification of [DTL](#id6.6.id6)
    methods based on "what knowledge is transferred" examines the specific characteristics
    of knowledge that can be transferred across domains or tasks. Some information
    is unique to a particular domain or task, while other knowledge is general and
    can improve the performance of the [TD](#id9.9.id9) or task. Based on this criterion,
    [DTL](#id6.6.id6) methods can be categorized as model-based, relation-based, instance-based,
    and feature-based [[34](#bib.bib34)].'
  prefs: []
  type: TYPE_NORMAL
- en: '(b) How knowledge is transferred: The classification of [DTL](#id6.6.id6) methods
    based on this question focuses on the specific algorithms or techniques used to
    transfer knowledge across domains or tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '(c) When knowledge is transferred: inquires as to when and under what circumstances
    knowledge should or should not be transferred.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Theoretical taxonomy of DTL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, a clear categorization of [DTL](#id6.6.id6) methodologies
    used for [3DPCs](#id1.1.id1) is presented. The proposed classification, shown
    in Fig. [5](#S2.F5 "Figure 5 ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey"), is arranged based on the following criteria: (i) learning style, (ii)
    methodology, (iii) DTL type, (iv) data annotation, and (v) widely used [DTL](#id6.6.id6)
    models. [DTL](#id6.6.id6) techniques can generally be divided into different groups
    depending on whether the source and [TDs](#id9.9.id9) and tasks are alike or not.
    The mathematical descriptions of the different [DTL](#id6.6.id6) groups and their
    distinctions are presented in Fig. [6](#S2.F6 "Figure 6 ‣ 2.3 Theoretical taxonomy
    of DTL ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey"). While this information is discussed in the
    literature review, aiming to simplify it and present it in a more reader-friendly
    manner in this study.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/14eba21422506cf68fecefef19db8fea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Proposed taxonomy of existing [DTL](#id6.6.id6) algorithms for [3DPCs](#id1.1.id1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6465ca58ee5f36fe55a94f3f1405d72b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The [DTL](#id6.6.id6) classification is determined by how similar
    the source and [TDs](#id9.9.id9) and tasks are. The symbol ($\varsubsetneq$) is
    used to indicate that the domains/tasks are distinct yet related. The symbol ($\exists!$)
    indicates the presence of a single unique domain/task. While ($\cong$) denotes
    that the domains, tasks, or spaces do not match.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1 Inductive DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The goal of inductive [DTL](#id6.6.id6) is to improve the target prediction
    function $\mathbb{F}T$ in the [TDs](#id9.9.id9) when compared to classical [ML](#id5.5.id5).
    This is achieved even when the target tasks $\mathbb{T}{T}$ are different from
    the source tasks $\mathbb{T}{S}$. However, the [SD](#id10.10.id10) $\mathbb{D}{S}$
    and [TD](#id9.9.id9) $\mathbb{D}_{T}$ may not always be identical (as shown in
    Fig. [6](#S2.F6 "Figure 6 ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")).
    Inductive [DTL](#id6.6.id6) can take two forms, depending on the availability
    of labeled or unlabeled data:'
  prefs: []
  type: TYPE_NORMAL
- en: '(a) Multi-task DTL: This approach is used when the [SDs](#id10.10.id10) has
    a large labeled dataset ($X_{S}$ labeled with $Y_{S}$). This is a specific form
    of multi-task learning where multiple tasks $(T_{1},T_{2},\dots,T_{n})$ are learned
    simultaneously (in parallel), including both the source and target tasks[[35](#bib.bib35)].'
  prefs: []
  type: TYPE_NORMAL
- en: '(b) Sequential learning: also known as self-taught learning, is a method used
    when the dataset in the [SD](#id10.10.id10) is unlabeled. It relies on (i) transferring
    the feature representation learned from a large collection of unlabeled datasets,
    and (ii) applying the learned representation to labeled data for classification
    tasks. This [DTL](#id6.6.id6) method involves sequentially learning multiple tasks
    where the gaps between the [SDs](#id10.10.id10) and [TDs](#id9.9.id9) may differ.
    For instance, assume we have a pre-trained model (PTM) $M$ and we apply [DTL](#id6.6.id6)
    to several tasks $(T_{1},T_{2},\dots,T_{n})$. In this approach, a specific task
    $\mathbb{T}_{T}$ is learned at each time step $t$ and it is slower than multi-task
    learning, however, when not all the tasks are present at the time of training,
    it may be advantageous. Sequential learning can be further classified into different
    types [[36](#bib.bib36)].'
  prefs: []
  type: TYPE_NORMAL
- en: 1-
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fine-tuning: involves training a new function, $\mathbb{F}_{T}$, that adapts
    the parameters of a PTM $M$ from a source task, $\mathbb{T}_{S}$, to a target
    task, $\mathbb{T}_{T}$, by translating the weights of the source task, $W_{S}$,
    to the weights of the target task, $W_{T}$. This can be done across all layers
    or just a subset of them, and the learning rate for each layer can be adjusted
    independently (known as discriminative fine-tuning). Additionally, new parameters,
    $K$, can be added to the model to improve its performance on the target task [[37](#bib.bib37)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathbb{F}_{T}(W_{T},K)=W_{S}\times K$ |  | (1) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 2-
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adapter modules: they are designed to take a PTM, $M_{S}$, and adapt its weights,
    $W_{S}$, to a target task, $\mathbb{T}_{T}$. The adapter module achieves this
    by introducing a new set of parameters, $K$, that are smaller in size compared
    to $W_{S}$, i.e. $K\ll W_{S}$. Both $K$ and $W_{S}$ are decomposed into smaller,
    more compact modules, such that $W_{S}={w}_{n}$ and $K={k}_{n}$. This allows the
    adapter module to learn a new function, $\mathbb{F}_{T}$, which adapts the model
    to the target task.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\mathbb{F}_{T}(K,W_{S})=k_{1}^{\prime}\times w_{1}\times\cdots k_{n}^{\prime}\times
    w_{n}$ |  | (2) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 'The equation ([2](#S2.E2 "In item 2- ‣ 2.3.1 Inductive DTL ‣ 2.3 Theoretical
    taxonomy of DTL ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding through
    Deep Transfer Learning: A Comprehensive Survey")) illustrates the procedure of
    adapting a model to a new task by dynamic weight adjustment, original weights
    $W_{S}={w}_{n}$ remain unchanged, whereas the set of weights $K$ are updated to
    $K^{\prime}={k^{\prime}}_{n}$. This principle of Dynamic DA is illustrated in
    Fig. [7](#S2.F7 "Figure 7 ‣ 2.3.1 Inductive DTL ‣ 2.3 Theoretical taxonomy of
    DTL ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3-
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feature based: it focuses on learning concepts and representations at various
    levels of an image, such as corners or interest points, blobs or regions of interest
    points, ridges, or edges $E$. In this approach, the collection of $E$ obtained
    from a PTM $M$ is kept unchanged and only $W^{\prime}$ is fine-tuned, such that
    the function $\mathbb{F}_{T}$ can be represented as $E\times W^{\prime}$. The
    idea is that $E$ are the feature learned from PTM and fine-tuning only the last
    layer $W^{\prime}$ to adapt to the new task.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4-
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zero-shot: is the simplest approach among all the others. It does not involve
    modifying or adding new parameters to a PTM by assuming that the existing parameters,
    denoted as $W_{S}$, cannot be changed. Essentially, zero-shot does not require
    any training to optimize or learn new parameters."'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/839f97b25fb82bfd171671e144377ca8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Example of DTL models used in [3DPCs](#id1.1.id1): (a) fine-tuning,
    and (b) deep DA.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Transductive DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In comparison to traditional [ML](#id5.5.id5), which can be used as a benchmark
    for DA and [DTL](#id6.6.id6), [DTL](#id6.6.id6) addresses the scenario where the
    [TD](#id9.9.id9) data, denoted as $\mathbb{D}{T}$, differs from the [SDs](#id10.10.id10)
    data, denoted as $\mathbb{D}{S}$. While the [SDs](#id10.10.id10) has annotated
    data ($X_{S}$ paired with $Y_{S}$), the [TD](#id9.9.id9) has no labeled data.
    The source and target tasks are similar as outlined in Fig. [6](#S2.F6 "Figure
    6 ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background ‣ Advancing 3D Point Cloud
    Understanding through Deep Transfer Learning: A Comprehensive Survey"). Transductive
    [DTL](#id6.6.id6) aims at constructing a target prediction function $\mathbb{F}_{T}$
    using the knowledge from both the [SD](#id10.10.id10) and [TD](#id9.9.id9). Additionally,
    transductive [DTL](#id6.6.id6) can be further categorized into two groups based
    on the relationship between the [SD](#id10.10.id10) and [TD](#id9.9.id9), as described
    in [[38](#bib.bib38)].'
  prefs: []
  type: TYPE_NORMAL
- en: '(a) Deep domain adaptation (DDA): refers to the situation where the feature
    spaces of the [SD](#id10.10.id10), denoted as $\chi_{S}$, and the [TDs](#id9.9.id9),
    denoted as $\chi_{T}$, are the same. However, the probability distributions of
    the input data are different, with $P(Y_{S}/X_{S})\neq P(Y_{T}/X_{T})$ as described
    in [[39](#bib.bib39)]. DDA is particularly useful when the target task has a unique
    distribution or limited labeled data, as highlighted in [[40](#bib.bib40)].'
  prefs: []
  type: TYPE_NORMAL
- en: '(b) Cross-modality DTL: most [DTL](#id6.6.id6) techniques require some form
    of relationship between feature spaces (or label spaces) of the source and [TDs](#id9.9.id9),
    i.e., $\mathbb{D}{S}$ and $\mathbb{D}{T}$. This means that [DTL](#id6.6.id6) can
    only be applied when the source and target have the same modality, such as text,
    speech, or video. In contrast, cross-modality [DTL](#id6.6.id6) is one of the
    most challenging areas of [DTL](#id6.6.id6), as it assumes that the feature spaces
    of the source and [TDs](#id9.9.id9) are completely different ($\chi_{S}\neq\chi_{T}$),
    such as speech-to-image, image-to-text, and text-to-speech. Additionally, the
    label spaces of the source $Y_{S}$ and target $Y_{S}$ domains may also differ
    ($Y_{S}\neq Y_{T}$) as described in [[41](#bib.bib41)].'
  prefs: []
  type: TYPE_NORMAL
- en: '(c) Unsupervised DTL: is a method of improving the learning of the target prediction
    function $\mathbb{F}T$ in the [TD](#id9.9.id9) $\mathbb{D}{T}$ by using knowledge
    from the [SDs](#id10.10.id10) $\mathbb{D}{S}$ and the source task $\mathbb{T}{S}$,
    even when the labels $Y_{S}$ and $Y_{T}$ are not present. It is important to note
    that the source and target tasks, $\mathbb{T}{S}$ and $\mathbb{T}{T}$, are related
    but distinct. Such kind of approach is useful when labels are not available for
    [TD](#id9.9.id9) data, as stated in [[42](#bib.bib42)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Adversarial DTL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Adversarial learning, as introduced in [[43](#bib.bib43)], is a method that
    helps to learn more transferable and discriminative representations. The first
    method using this approach, the domain-adversarial neural network (DANN) was presented
    in [[44](#bib.bib44)]. Unlike traditional methods that use predefined distance
    functions, DANN utilizes a domain-adversarial loss within the network. This approach
    has shown to improve the network’s ability to learn discriminative data and has
    been used in many visual surveillance studies [[45](#bib.bib45), [46](#bib.bib46),
    [47](#bib.bib47), [48](#bib.bib48)]. However, prior works in DANN have not considered
    the different effects of marginal and conditional distributions. An alternative
    approach, dynamic distribution alignment, was proposed in [[49](#bib.bib49)] that
    dynamically evaluates the importance of each distribution, thus providing a more
    nuanced approach." Another approach called the adversarial scoring network (ASNet)
    was introduced in [[50](#bib.bib50)] to bridge the gap between domains at different
    levels of granularity. This approach uses adversarial learning to align the [SDs](#id10.10.id10)
    with the [TD](#id9.9.id9) in the global and local feature space during the coarse-grained
    stage. The transferability of source attributes is then evaluated in fine-grained
    stage by comparing the similarity between the source and target samples at multiple
    levels, utilizing the generative probability obtained in the coarse stage. The
    transferable elements are then selectively used to assist the [DTL](#id6.6.id6)
    adaptation process. This coarse-to-fine architecture effectively reduces the problem
    of domain disparity. Specifically, photographs are encoded into density maps by
    a generator, and then classified as [SDs](#id10.10.id10) or [TD](#id9.9.id9) using
    a dual-discriminator. Adversarial training is employed between the dual-discriminator
    and generator to bring the domains’ distributions closer together. The dual-discriminator
    also generates four different scores which are used as a signal to optimize the
    density of the [SDs](#id10.10.id10) during adaptation for fine-grained transfer
    as stated in [[50](#bib.bib50)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[51](#bib.bib51)], the authors studied the adversarial robustness in [3DPC](#id1.1.id1)
    recognition using three different architectures: [multi-layer perceptron](#id23.23.id23)
    ([MLP](#id23.23.id23)) network (PointNet), convolutional network (DGCNN), and
    transformer-based network. They employed two methods: adversarial pretraining
    for fine-tuning, where [self-supervised learning](#id14.14.id14) ([SSL](#id14.14.id14))
    tasks are utilized for pretraining, and adversarial joint training, where the
    self-supervised task is trained alongside the recognition task, as illustrated
    in Fig. [8](#S2.F8 "Figure 8 ‣ 2.3.3 Adversarial DTL ‣ 2.3 Theoretical taxonomy
    of DTL ‣ 2 Background ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a7ca1fd95225088bb486d91859547ee3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Flowchart of the adversarial TL-based [3DPC](#id1.1.id1) classification
    approach proposed in [[51](#bib.bib51)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Popular DL-based PC models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The development of DL-based 3DPC approaches is characterized by a rich diversity
    of deep learning models each aimed at overcoming specific challenges related to
    point cloud data processing. While these models offer significant improvements
    in processing speed, accuracy, and applicability, they also underscore the ongoing
    need for models that can generalize across different environments and handle the
    inherent complexities of 3D point data more effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Feature Extraction and Geometric Detail Enhancement Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Models such as PointNet [[52](#bib.bib52)] and PointNet++ [[53](#bib.bib53)]
    spearheaded the direct processing of point clouds by respecting permutation invariance
    and enhancing the capture of hierarchical structures, respectively. They are pivotal
    in tasks like object classification and segmentation, though they struggle with
    local detail due to uniform point processing. PointVGG [[54](#bib.bib54)] and
    PointPAVGG [[55](#bib.bib55)] extend these capabilities by adapting image-based
    convolutional and attention mechanisms to point clouds, striving to bridge gaps
    in capturing intricate geometric details but facing challenges in computational
    efficiency. SpiderCNN [[56](#bib.bib56)] and PointCNN [[57](#bib.bib57)] further
    innovate by adapting traditional CNN transformations to unordered point data,
    enhancing the models’ ability to learn from complex datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Segmentation and Object Detection Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Models like SplatNet [[58](#bib.bib58)], SGPN [[59](#bib.bib59)], and FoldingNet
    [[60](#bib.bib60)] represent advances in segmentation and unsupervised learning.
    SplatNet leverages sparse bilateral convolutional layers for processing high-dimensional
    lattices in point clouds, suitable for segmentation but hampered by scaling issues.
    SGPN focuses on instance segmentation by predicting semantic classes and point
    groupings, which becomes complex in cluttered environments. FoldingNet explores
    unsupervised learning with a folding-based decoder that may not generalize across
    all point cloud types.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.3 Data Integration and Upsampling Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PU-Net [[61](#bib.bib61)] and PointGrid [[62](#bib.bib62)] focus on upsampling
    and integrating grid-based approaches with point processing. PU-Net enhances point
    cloud density using multi-level feature integration, while PointGrid blends point
    and grid-based methods for recognizing 3D models, though it struggles with sparse
    point clouds.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.4 Specialized Application Models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Models such as Hand PointNet [[63](#bib.bib63)] and PointNetVLAD [[64](#bib.bib64)]
    are tailored for specific applications like 3D hand pose estimation and global
    descriptor extraction, critical for place recognition. However, these models often
    face limitations when applied outside their intended scope, such as dynamic environments
    or varying object types.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.5 Innovative Approaches in Object Detection and Registration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Emerging models like PREDATOR [[65](#bib.bib65)] and 3DIoUMatch [[66](#bib.bib66)]
    showcase the potential of deep learning in object detection and registration.
    PREDATOR addresses low-overlap point cloud registration with a deep attention
    mechanism, suited for benchmark scenarios but limited in broader applications.
    3DIoUMatch leverages a semi-supervised learning approach to enhance 3D object
    detection, showing dependency on initial label quality for performance efficacy.
    Table LABEL:tab2 summarizes the most popular 3DPC models proposed in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Summary of popular DL-based [3DPC](#id1.1.id1) understanding models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | Model Name | Contribution Description | Dataset | Application | Limitation
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[52](#bib.bib52)] | PointNet | Introduced a neural network that processes
    point clouds directly, respecting permutation invariance. | Various 3D benchmarks
    | Object classification, part segmentation, semantic parsing | Does not capture
    local structural details. |'
  prefs: []
  type: TYPE_TB
- en: '| [[67](#bib.bib67)] | ScanNet | Developed an RGB-D video dataset with extensive
    annotations for deep learning applications. | ScanNet dataset | 3D object classification,
    semantic voxel labeling, CAD model retrieval | Limited diversity in scene views
    and semantic annotations. |'
  prefs: []
  type: TYPE_TB
- en: '| [[68](#bib.bib68)] | OctNet | Proposed a sparse 3D data representation that
    allows for deep, high-resolution 3D convolutional networks. | Not specified |
    3D object classification, orientation estimation, point cloud labeling | Focus
    on sparse data might not generalize to denser datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| [[53](#bib.bib53)] | PointNet++ | Extended PointNet to capture local structures
    using a hierarchical neural network. | Challenging benchmarks of 3DPCs | Enhanced
    3D recognition tasks | Performance drops with non-uniform point densities. |'
  prefs: []
  type: TYPE_TB
- en: '| [[54](#bib.bib54)] | PointVGG | Introduced point convolution and pooling
    methods to adapt image-based techniques for point clouds. | Challenging benchmarks
    of 3DPCs | Object classification, part segmentation | May not fully capture intricate
    geometric details. |'
  prefs: []
  type: TYPE_TB
- en: '| [[55](#bib.bib55)] | PointPAVGG | A VGG-based network that incorporates a
    point attention mechanism for feature extraction from point clouds. | ShapeNet,
    ModelNet | Point cloud classification, segmentation | Increased computational
    demand due to complex feature integration. |'
  prefs: []
  type: TYPE_TB
- en: '| [[58](#bib.bib58)] | SplatNet | Utilized sparse bilateral convolutional layers
    for processing point clouds in a high-dimensional lattice. | Not specified | 3D
    segmentation | Scaling issues with memory and computational cost in larger lattices.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[60](#bib.bib60)] | FoldingNet | Developed a deep auto-encoder with a folding-based
    decoder for unsupervised learning on point clouds. | Not specified | Unsupervised
    learning, 3D object reconstruction | Generic decoder structure may not work for
    all point cloud types. |'
  prefs: []
  type: TYPE_TB
- en: '| [[61](#bib.bib61)] | PU-Net | Presented a method for upsampling 3DPCs using
    multi-level features. | Synthesis and scan data | Point cloud upsampling | Focus
    on upsampling may not improve other manipulations. |'
  prefs: []
  type: TYPE_TB
- en: '| [[69](#bib.bib69)] | SO-Net | Built a permutation invariant architecture
    using Self-Organizing Maps for point cloud processing. | Not specified | Point
    cloud reconstruction, classification, segmentation, shape retrieval | Requires
    tuning of the network’s receptive field. |'
  prefs: []
  type: TYPE_TB
- en: '| [[70](#bib.bib70)] | PIXOR | Developed a real-time 3D object detection system
    from point clouds using BEV, optimized for autonomous driving. | KITTI, large-scale
    3D vehicle detection benchmark | Real-time 3D object detection in autonomous driving
    | Primarily optimized for vehicle detection, may not generalize to other object
    types. |'
  prefs: []
  type: TYPE_TB
- en: '| [[59](#bib.bib59)] | SGPN | Introduced a network for 3D instance segmentation
    by predicting point groupings and semantic classes. | Various 3D scenes | 3D instance
    segmentation, object detection, semantic segmentation | May struggle with highly
    cluttered or complex environments. |'
  prefs: []
  type: TYPE_TB
- en: '| [[71](#bib.bib71)] | VoxelNet | Unified feature extraction and bounding box
    prediction for 3DPCs into a single deep network. | KITTI | 3D detection of cars,
    pedestrians, and cyclists | High computational cost due to dense voxelization
    of point clouds. |'
  prefs: []
  type: TYPE_TB
- en: '| [[63](#bib.bib63)] | Hand PointNet | Developed a hand pose regression network
    using 3DPCs to capture complex hand structures. | Three challenging hand pose
    datasets | 3D hand pose estimation | Focuses on hand pose, limiting its applicability
    to other forms of point cloud processing. |'
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64)] | PointNetVLAD | Proposed a network for global descriptor
    extraction from point clouds for place recognition. | Created benchmark datasets
    for point cloud based retrieval | Place recognition from point clouds | May not
    be as effective in highly dynamic environments. |'
  prefs: []
  type: TYPE_TB
- en: '| [[72](#bib.bib72)] | PPFNet | Introduced a network for learning globally
    informed 3D local feature descriptors from point clouds. | Not specified | Finding
    correspondences in unorganized point clouds | Dependency on the quality of local
    features and global context understanding. |'
  prefs: []
  type: TYPE_TB
- en: '| [[62](#bib.bib62)] | PointGrid | Combined point and grid-based approaches
    to recognize 3D models from point clouds. | Popular shape recognition benchmarks
    | 3D model recognition, classification, and segmentation | Might not handle extremely
    sparse or irregular point clouds effectively. |'
  prefs: []
  type: TYPE_TB
- en: '| [[73](#bib.bib73)] | PointFusion | Leverages image and point cloud data for
    3D object detection without dataset-specific tuning. | KITTI, SUN-RGBD | Generic
    3D object detection across diverse environments | The fusion process can be complex
    and computationally intensive. |'
  prefs: []
  type: TYPE_TB
- en: '| [[74](#bib.bib74)] | Frustum PointNets | Operates on raw point clouds for
    3D object detection, combining 2D and 3D detection methods. | KITTI, SUN RGB-D
    | 3D object detection in both indoor and outdoor scenes | Might encounter difficulties
    with very sparse point clouds or heavy occlusions. |'
  prefs: []
  type: TYPE_TB
- en: '| [[75](#bib.bib75)] | 3DFeat-Net | Learns 3D feature detectors and descriptors
    for point cloud matching using weak supervision. | Outdoor Lidar datasets | Point
    cloud matching for localization and mapping | Performance highly dependent on
    the effectiveness of weak supervision learning mechanisms. |'
  prefs: []
  type: TYPE_TB
- en: '| [[56](#bib.bib56)] | SpiderCNN | Developed SpiderConv units to handle 3DPCs
    | ModelNet40 | 3DPC classification and segmentation | May struggle with very large
    or noisy datasets |'
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] | PointCNN | Introduced a X-transformation to handle unordered
    point clouds for CNNs | Multiple challenging benchmark datasets | Feature learning
    from point clouds | Dependent on the effectiveness of the X-transformation |'
  prefs: []
  type: TYPE_TB
- en: '| [[76](#bib.bib76)] | SqueezeSeg | Created a CNN pipeline for semantic segmentation
    of LiDAR data | KITTI, GTA-V (simulated) | Semantic segmentation in autonomous
    driving | Reliance on synthetic data for improved accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| [[77](#bib.bib77)] | 2DPASS | Boosted point cloud learning via 2D image fusion
    during training | SemanticKITTI, NuScenes | Semantic segmentation of point clouds
    | Requires multi-modal data during training, complex implementation |'
  prefs: []
  type: TYPE_TB
- en: '| [[65](#bib.bib65)] | PREDATOR | Focused on low-overlap point cloud registration
    with deep attention | 3DMatch benchmark | Point cloud registration | Limited to
    pairwise registration, may not generalize beyond benchmark scenarios |'
  prefs: []
  type: TYPE_TB
- en: '| [[66](#bib.bib66)] | 3DIoUMatch | Implemented a semi-supervised 3D object
    detection with teacher-student learning | ScanNet, SUN-RGBD, KITTI | 3D object
    detection | High task complexity and dependency on initial label quality |'
  prefs: []
  type: TYPE_TB
- en: '| [[78](#bib.bib78)] | PointPoseNet | Developed a pipeline for 6D object pose
    estimation from point clouds | LINEMOD, Occlusion LINEMOD | 6D pose estimation
    | Limited to known objects in complex scenes, sensitivity to occlusions |'
  prefs: []
  type: TYPE_TB
- en: '| [[79](#bib.bib79)] | ASAP-Net | Enhanced spatio-temporal modeling of point
    clouds | Synthia, SemanticKITTI | Point cloud sequence segmentation | Requires
    specific attention and structure-aware algorithms for optimal performance |'
  prefs: []
  type: TYPE_TB
- en: '| [[80](#bib.bib80)] | MUSCLE | Proposed a compression algorithm for LiDAR
    data using spatio-temporal relationships | UrbanCity, SemanticKITTI | LiDAR data
    compression | Efficiency depends on the variability of the input data |'
  prefs: []
  type: TYPE_TB
- en: '| [[81](#bib.bib81)] | PC-RGNN | Addressed sparse and partial point clouds
    for 3D detection using GNNs | KITTI | 3D object detection | Highly dependent on
    the quality of initial point cloud data |'
  prefs: []
  type: TYPE_TB
- en: 2.5 Pre-trained 2D models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Xu et al. [[82](#bib.bib82)] demonstrate how pretrained 2D image models can
    be adapted for 3DPC understanding with minimal modification. By extending 2D ConvNets
    and vision transformers to handle 3D data, the method involves inflating 2D filters
    to 3D and only finetuning specific layers like the input, output, and normalization
    layers. This approach leverages the deep feature representations learned from
    large-scale 2D image datasets, enabling the models to perform competitively on
    3DPC tasks such as classification and segmentation, while significantly reducing
    the training time and data requirements compared to training from scratch. The
    transferability is facilitated by the similarity in feature representation between
    the 2D and 3D tasks, despite their differences in data modality.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, shape classification and part segmentation pose significant challenges
    in [CV](#id3.3.id3). While trained [convolutional neural networks](#id11.11.id11)
    have shown impressive performance on regular grid data like images, accurately
    capturing shape information and geometric representation from irregular and disordered
    point clouds is problematic. To address this, [[54](#bib.bib54)] proposes point
    convolution (Pconv) and point pooling (Ppool) techniques inspired by convolution
    and pooling in image processing, specifically designed for point clouds to learn
    high-level features. Pconv gradually magnifies receptive fields to capture local
    geometric information, while Ppool tackles the disorder of point clouds using
    a symmetric function that aggregates points progressively for a more detailed
    local geometric representation. Our novel network, named PointVGG, incorporates
    Pconv, Ppool, and a graph structure for feature learning in point clouds, and
    is applied to object classification and part segmentation tasks. Experimental
    results demonstrate that PointVGG achieves state-of-the-art performance on challenging
    benchmarks of [3DPC](#id1.1.id1). Moreover, extracting high-level features from
    disordered point cloud data using pre-trained 2D [CNN](#id11.11.id11) remains
    challenging. To address this, [[55](#bib.bib55)] proposes a VGG-based network
    called point positional attention VGG (PointPAVGG), inspired by the classical
    VGG network. Our approach combines global and local features by extracting local
    geometric information from every sphere domain and analyzing the global position
    score using our point attention (PA) module. PointPAVGG, with its graph structure
    point cloud feature extraction and PA, is applied to point cloud classification
    and segmentation tasks. Through comprehensive experiments on ShapeNet and ModelNet,
    our method demonstrates superior performance, achieving state-of-the-art results
    in classification and segmentation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The work in [[83](#bib.bib83)] presents a method for automatic detection of
    manhole covers from mobile mapping point cloud data, which are large-scale spatial
    databases used for various purposes. The method uses a fully [CNN](#id11.11.id11)
    with [DTL](#id6.6.id6) and a simplified class activation mapping (CAM) location
    algorithm to accurately determine the position of manhole covers. Different source
    model architectures, such as AlexNet, VGG-16, Inception-v3, and ResNet-101, are
    assessed. Results showed that VGG-16 achieved the best detection performance among
    others, with recall, precision, and F2-score of 0.973 each. The approach also
    achieves a horizontal 95% confidence interval of 16.5 cm for location performance
    using VGG-16 architecture. The study highlights the importance of incorporating
    geometric information channels in the ground image for improved detection and
    location accuracy. Aerial imaging using drones, an efficient timely data collection
    after natural hazards for post-event management, provides detailed site characterization
    with minimal ground support, but results in large amounts of 2D orthomosaic images
    and [3DPC](#id1.1.id1). Effective data processing workflows are needed to identify
    structural damage states. Liao et al. [[84](#bib.bib84)] introduce two [DL](#id4.4.id4)
    models, based on 2D and 3D [CNNs](#id11.11.id11), for post-windstorm classification.
    [DTL](#id6.6.id6) from AlexNet and VGGNet is used for the 2D [CNNs](#id11.11.id11),
    while a 3D fully convolutional network (3DFCN) with skip connections is developed
    and trained for point cloud data. The models are compared using quantitative performance
    measures, and the 3DFCN shows greater robustness in detecting different damage
    classes. This highlights the importance of 3D datasets, particularly depth information,
    in distinguishing between different damage states in structures.
  prefs: []
  type: TYPE_NORMAL
- en: In [[85](#bib.bib85)], a [DL](#id4.4.id4)-based approach is proposed, which
    involves projecting [3DPC](#id1.1.id1) into 2D rendering views and then feeding
    them into a [CNN](#id11.11.id11) for quality score prediction. [DTL](#id6.6.id6)
    is employed to leverage the capabilities of VGG-16 trained on the ImageNet database.
    The performance of the proposed model is evaluated on two benchmark databases,
    ICIP2020 and SJTU, and the results show a strong correlation between the predicted
    and subjective quality scores, outperforming state-of-the-art point cloud quality
    assessment models. [[86](#bib.bib86)] addresses the problem of learning outdoor
    [3DPC](#id1.1.id1) from monocular data using a sparse ground-truth dataset. A
    [DL](#id4.4.id4)-based approach called Pix2Point is proposed, which uses a 2D-3D
    hybrid neural network architecture and a supervised end-to-end minimization of
    an optimal transport divergence between point clouds. The proposed approach outperformed
    efficient monocular depth methods when trained on sparse point clouds. The paper
    highlights the potential of [DL](#id4.4.id4) for monocular [3DPC](#id1.1.id1)
    prediction and its ability to handle complete and challenging outdoor scenes.
    The encoding block, in the target model, consists of convolution, pooling, and
    normalization layers to extract feature descriptions from the RGB image. These
    features are then processed by a fully connected layer to obtain a preliminary
    set of 3D point coordinates. Several source models, based on VGG, DenseNet, and
    ResNet architectures, are explored and compared, which are referred to as backbones.
  prefs: []
  type: TYPE_NORMAL
- en: Balado et al. [[87](#bib.bib87)] propose in 2020, a method that minimizes the
    use of point cloud samples for training [CNNs](#id11.11.id11) by converting point
    clouds to images (pc-images). This enables the generation of multiple samples
    per object through multi-view, and the combination of pc-images with images from
    online datasets such as ImageNet and Google Images. Results suggest keeping some
    point cloud images in training; even 10% can lead to high classification accuracy.
    The work presented in [[88](#bib.bib88)] showcases a prototypical implementation
    of a service-oriented architecture for classifying indoor point cloud scenes in
    office environments. This approach utilizes multi-view techniques for semantic
    enrichment of captured scans and subsequent classification. The approach is tested
    using a pretrained [CNN](#id11.11.id11) model, Inception V3, to classify common
    office furniture objects such as chairs, sofas, and desks in [3DPC](#id1.1.id1)
    scans. The results show that the approach can achieve acceptable accuracy in classifying
    common office furniture, based on RGB cubemap images of the octree partitioned
    areas of the [3DPC](#id1.1.id1) scan. Additional methods for web-based 3D visualization,
    editing, and annotation of point clouds are also discussed.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[89](#bib.bib89)] propose a visual recognition and location
    method for object detection in soft robotic manipulation using RGB-D information
    fusion. The method involves scanning and reconstructing the environment using
    ORB-SLAM2, constructing an object feature database, matching point clouds using
    the iterative closest point (ICP) algorithm, identifying regions of interest,
    and using the inception-v3 model and [DTL](#id6.6.id6) for object recognition.
    The position of the object relative to the camera is obtained through correspondence
    between color information and point cloud data. The method showed that objects
    belonging to the same object have much lower matching error compared to those
    not belonging to the same object, and successful object identification and location
    were achieved through color recognition. Moving on, the authors in [[90](#bib.bib90)]
    adopt the ResNet50 [[91](#bib.bib91)] pretrained on the ImageNet data set to extract
    deep features from each feature image and obtain five multi-scale and multi-view
    (MSMV) deep features per point.
  prefs: []
  type: TYPE_NORMAL
- en: Table LABEL:tab3 presents a summary of DTL-based [3DPC](#id1.1.id1) models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Summary of DTL-based [3DPC](#id1.1.id1) models.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Model | Highlights | Limitations |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Point-based methods | AltasNet [[12](#bib.bib12)] | In AtlasNet, a surface
    representation is inferred by regarding a 3D shape as a collection of parametric
    surface elements. | The reiterated many times largely determines the reconstruction.
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSN [[92](#bib.bib92)] | A sampling algorithm combines a set of parametric
    surface elements, which MSN predicts, with the partial input. | Fine-grained details
    of object shape aren’t generated successfully. |'
  prefs: []
  type: TYPE_TB
- en: '| ASHF-Net [[93](#bib.bib93)] | A hierarchical folding decoder with the gated
    skip-attention and multi-resolution completion target to exploit the local structure
    details of the incomplete inputs is proposed by ASHF-Net. | The decoder [113]
    obtains unstructured predictions, and the surface of the results doesn’t remain
    smooth. |'
  prefs: []
  type: TYPE_TB
- en: '| Point-based methods | PCN [[94](#bib.bib94)] | The coarse-to-fine completion
    is performed by PCN, which combines the fully connected network and FoldingNet.
    | The synthesis of shape details is not achievable. |'
  prefs: []
  type: TYPE_TB
- en: '| SA-Net [[95](#bib.bib95)] | Hierarchical folding in the multi-stage points
    generation decoder is proposed by SA-Net. | The implicit representation of the
    target shape from the intermediate layer, which helps refine the shape in the
    local region, is difficult to interpret and constrain. |'
  prefs: []
  type: TYPE_TB
- en: '| FoldingNet [[60](#bib.bib60)] | It is commonly assumed in a two-stage generation
    process that a 2D-manifold can recover 3D objects. | Explicitly constraining the
    implicit intermediate is challenging. |'
  prefs: []
  type: TYPE_TB
- en: '| SK-PCN [[96](#bib.bib96)] | The global structure is acquired by predicting
    the 3D skeleton with SK-PCN, and the surface completion is achieved by learning
    the displacements of skeletal points. | The overall shapes are the sole focus
    of the meso-skeleton. |'
  prefs: []
  type: TYPE_TB
- en: '| Point-based methods | GRNet [[97](#bib.bib97)] | Unordered point clouds are
    regularized by GRNet, which introduces 3D grids as intermediate representations.
    | The resolution still governs it. A significant computational cost is incurred
    when a higher resolution is used. |'
  prefs: []
  type: TYPE_TB
- en: '| VE-PCN [[98](#bib.bib98)] | The structure information is incorporated into
    the shape completion by VE-PCN through the utilization of edge generation. | High
    frequency components are the sole focus of the edges. |'
  prefs: []
  type: TYPE_TB
- en: '| Point-PEFT [[99](#bib.bib99)] | Introduces a Parameter-Efficient Fine-Tuning
    method for 3D models, minimizing adaptation costs for downstream tasks with minimal
    trainable parameters. | While effective, the specialized method’s broader applicability
    and long-term adaptability across diverse domains remain unproven. |'
  prefs: []
  type: TYPE_TB
- en: '| DAPT [[100](#bib.bib100)] | Implement a Dynamic Adapter for point cloud analysis,
    offering efficient parameter use and reducing training resources significantly.
    | Although reducing trainable parameters, the adaptability and effectiveness in
    extremely diverse environments is yet to be fully assessed. |'
  prefs: []
  type: TYPE_TB
- en: '| AgileGAN3D [[101](#bib.bib101)] | A novel framework for 3D artistic portrait
    stylization using unpaired 2D exemplars and advanced 3D GAN models. | The dependency
    on the quality and diversity of the 2D exemplars might limit the model’s versatility
    in less controlled scenarios. |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3D-TRAM [[102](#bib.bib102)] | Combines transfer learning with a memory
    component to enhance 3D reconstruction from 2D images, leveraging CAD models for
    better accuracy. | The method’s performance can vary significantly with the complexity
    of the scene and the quality of the available CAD models. |'
  prefs: []
  type: TYPE_TB
- en: 2.6 Pre-trained 3D models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Implementing a standardized approach to [3DPC](#id1.1.id1) neural network design
    has the potential to yield comparable advancements seen in the extensive research
    conducted on pre-training visual models, especially in the image domain. However,
    compared to the 2D domain, the design of neural networks for point cloud data
    is less mature, as evidenced by the numerous new architectures proposed recently.
    This is due to several factors, including the challenge of processing unordered
    sets [[103](#bib.bib103)], the choice of neighborhood aggregation mechanism, which
    could be hierarchical [[53](#bib.bib53), [104](#bib.bib104), [105](#bib.bib105)],
    spatial CNN-like [[106](#bib.bib106), [56](#bib.bib56), [57](#bib.bib57), [107](#bib.bib107)],
    spectral [[108](#bib.bib108), [109](#bib.bib109)], or graph-based [[110](#bib.bib110),
    [111](#bib.bib111)], and the fact that points are discrete samples of an underlying
    surface, which has led to the consideration of continuous convolutions [[112](#bib.bib112),
    [113](#bib.bib113)].
  prefs: []
  type: TYPE_NORMAL
- en: While pre-training image models have been successful in achieving high levels
    of prosperity, pre-training 3D models is still in the developmental stage. To
    address this issue, numerous researchers have explored various [SSL](#id14.14.id14)
    mechanisms that utilize different pretext tasks, such as solving jigsaw puzzles
    [[114](#bib.bib114)], estimating orientation [[115](#bib.bib115)], and reconstructing
    deformations [[116](#bib.bib116)]. Drawing inspiration from pre-training strategies
    in the image domain, several approaches have been proposed in the 3D domain, including
    point contrast [[117](#bib.bib117)], which uses a contrastive learning principle,
    and OcCo [[118](#bib.bib118)], Point-BERT [[119](#bib.bib119)], and Point-M2AE
    [[120](#bib.bib120)], which introduce reconstruction pretext tasks to facilitate
    better representation learning. However, the lack of available data in the 3D
    domain remains a significant obstacle in developing more effective pre-training
    strategies.
  prefs: []
  type: TYPE_NORMAL
- en: black For instance, [[117](#bib.bib117)] focuses on developing effective representation
    learning techniques for point cloud data. The authors draw inspiration from the
    success of standardized neural architectures in the 2D domain, such as VGGNet
    [simonyan2014very] and ResNet/ResNeXt [[91](#bib.bib91)], and conjecture that
    a similar standardization of point cloud neural network design could enable similar
    progress. However, compared to the 2D domain, the design of neural networks for
    point cloud data is less mature, as evidenced by the numerous new architectures
    proposed recently. This is due to several factors, including the challenge of
    processing unordered sets [ravanbakhsh2016deep, [52](#bib.bib52), [103](#bib.bib103)],
    the choice of neighborhood aggregation mechanism (e.g., hierarchical [[53](#bib.bib53),
    [104](#bib.bib104), [105](#bib.bib105)], spatial [CNN](#id11.11.id11)-like [[106](#bib.bib106),
    [56](#bib.bib56), [57](#bib.bib57), [107](#bib.bib107)], spectral [[108](#bib.bib108),
    [109](#bib.bib109)], or graph-based [[110](#bib.bib110), [111](#bib.bib111)]),
    and the fact that points are discrete samples of an underlying surface, which
    has led to the consideration of continuous convolutions [[112](#bib.bib112), [113](#bib.bib113)].
  prefs: []
  type: TYPE_NORMAL
- en: In this respect, Choy et al. proposed the Minkowski Engine [[121](#bib.bib121)],
    which is an extension of sub-manifold sparse convolutional networks [[122](#bib.bib122)]
    to higher dimensions. By facilitating the adoption of common deep architectures
    from 2D vision, sparse convolutional networks could help standardize [DL](#id4.4.id4)
    for point cloud. In [[117](#bib.bib117)], the authors use a unified UNet [[123](#bib.bib123)]
    architecture built with Minkowski Engine as the backbone network in all our experiments
    and show that it can seamlessly transfer between tasks and datasets. [[120](#bib.bib120)]
    The paper proposes Point-M2AE scheme, a [multi-scale masked autoencoder](#id12.12.id12)
    ([MAE](#id12.12.id12)) pre-training framework for [SSL](#id14.14.id14) of [3DPC](#id1.1.id1).
    Unlike standard [auto-encoder](#id27.27.id27) ([AE](#id27.27.id27))-based transformers,
    Point-M2AE modifies the encoder and decoder into pyramid architectures to model
    spatial geometries and capture fine-grained and high-level semantics of 3D shapes.
    The encoder uses a multi-scale masking strategy for consistent visible regions
    across scales and a local spatial self-attention [[124](#bib.bib124)] mechanism
    during fine-tuning. The lightweight decoder gradually upsamples point tokens with
    skip connections from the encoder, promoting reconstruction from a global-to-local
    perspective. Point-M2AE achieves 92.9% accuracy on ModelNet40 using a linear SVM.
    Fine-tuning enhances performance to 86.43% on ScanObjectNN and provides benefits
    in various tasks, such as few-shot classification, part segmentation, and 3D object
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: The reference [[125](#bib.bib125)] introduces the PointCLIP method, which encodes
    point clouds by projecting them into multi-view depth maps and aggregates view-wise
    zero-shot predictions for knowledge transfer from 2D to 3D. An inter-view adapter
    is designed to extract global features and adaptively fuse few-shot knowledge
    from 3D into CLIP pre-trained in 2D. Fine-tuning the lightweight adapter in few-shot
    settings significantly improves PointCLIP’s performance. PointCLIP also exhibits
    complementary properties with classical 3D-supervised networks, and ensembling
    with baseline models further boosts performance, surpassing state-of-the-art models.
    PointCLIP is a promising alternative for effective [3DPC](#id1.1.id1) understanding
    with low resource cost and data regime, as demonstrated through experiments on
    ModelNet10, ModelNet40, and ScanObjectNN datasets. Huang et al. [[126](#bib.bib126)]
    proposes a method called MF-PointNN for surrogate modeling of melt pool in metallic
    additive manufacturing. Melt pool modeling is important for uncertainty quantification
    and quality control in metal additive manufacturing, but finite element simulation
    for thermal modeling can be time-consuming. MF-PointNN is a multi-fidelity approach
    that combines low-fidelity analytical models and high-fidelity finite element
    simulation data using [DTL](#id6.6.id6). A basic PointNN is first trained with
    low-fidelity data to establish correlation between inputs and thermal field of
    analytical models. Then, the basic PointNN is updated and fine-tuned using a small
    amount of high-fidelity data to build the MF-PointNN. This latter efficiently
    maps input variables and spatial positions to thermal histories, allowing for
    efficient prediction of the three-dimensional melt pool. Results of melt pool
    modeling for Ti-6Al-4V in electron beam additive manufacturing under uncertainty
    show the effectiveness of the proposed approach.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7 Evaluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Several metrics have been reported to evaluate the performance of different
    [DTL](#id6.6.id6) approaches for various [3DPC](#id1.1.id1) tasks. F1 measures
    and [overall accuracy](#id16.16.id16) ([OA](#id16.16.id16)), are the most frequently
    used measures to assess the performance of algorithms over benchmark datasets.
    These metrics are used for many purposes, including segmentation, classification,
    and registration. F1 measures give an idea about the behavior of the precision
    and recall curve, whereas [OA](#id16.16.id16) conveys the mean accuracy for instances
    of the test. Besides, [intersection over union](#id13.13.id13) ([IoU](#id13.13.id13))
    and mean [IoU](#id13.13.id13) have been extensively used, especially for object
    detection and segmentation purpose. For instance, in [[127](#bib.bib127)], authors
    have used mean [IoU](#id13.13.id13) between point-wise ground truth and prediction.
    In [[128](#bib.bib128)], authors have computed [IoU](#id13.13.id13) for different
    shapes. It is calculated as the average of [IoUs](#id13.13.id13) of all parts
    in a shape. Furthermore, for some particular categories, mean IoUs (mIoUs) are
    obtained by finding out the average of all [IoUs](#id13.13.id13) shapes. Other
    metrics, instance mIoU (Ins. mIoU) and category mIoU (Cat. mIoU) are also introduced
    and computed by calculating the average of all shapes and the average of mIoUs
    for all categories, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics such as OA and mIoU provide a holistic view of the model’s performance
    across different categories. Additionally, the frame per second (FPS) metric evaluates
    the efficiency and speed of real-time applications, while root mean squared error
    (RMSE) and mean absolute percentage error (MAPE) are used for precision in measurement
    tasks. Moving on, consistency rate (CR), consistency proportion (CP), and weight
    coverage (WCov) are advanced evaluation metrics used to assess the quality of
    3D point cloud reconstructions. CR measures the rate at which a reconstructed
    scene maintains geometric consistency across different views or instances. CP
    evaluates the proportion of consistent data points within the entire dataset,
    providing a sense of how uniformly the model performs. WCov assesses the coverage
    of the weighted areas in the point cloud, indicating how well the model captures
    the essential features and structures of the scene.
  prefs: []
  type: TYPE_NORMAL
- en: To evaluate the performance or semantic segmentation, authors in [[129](#bib.bib129)]
    have used errors of commission and errors of omission metrics over the CMP façade
    dataset. Frame per second (FPS) metric is used by [[130](#bib.bib130)] on the
    ouster LiDAR-64 dataset. In addition to this, Zong et al. [[131](#bib.bib131)]
    have used average precision (mPrec) and average recall (mRec) to test their segmentation
    method. Moreover, the best performance for [OA](#id16.16.id16) on the Shapenet
    dataset is 94.5% [[132](#bib.bib132)]. However, the highest precision (85.99%)
    is achieved on CMP facade dataset [[129](#bib.bib129)]. The F1 score has been
    calculated on different datasets, viz. CMP facade, ISPRS, Semantic 3D dataset,
    ModelNet4, and other. The best performance has been reported by [[133](#bib.bib133)]
    on ModelNet40\. The aforementioned metrics are not only applicable to [3DPC](#id1.1.id1)
    tasks but also to other artificial intelligence-related tasks such as classification,
    segmentation, and other. However, subsequent subsections thoroughly elaborate
    on specific metrics relevant to [3DPC](#id1.1.id1) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.1 Distance metrics
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Distance metrics play a pivotal role in the processing of point clouds for tasks
    such as nearest-neighbor search, clustering, and segmentation. They provide a
    measure of similarity or dissimilarity between points in space, influencing the
    outcomes of many algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.2 Euclidean Distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Defined as the square root of the sum of the squared differences between the
    coordinates of two points:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d(\mathbf{p},\mathbf{q})=\sqrt{(p_{1}-q_{1})^{2}+(p_{2}-q_{2})^{2}+\cdots+(p_{n}-q_{n})^{2}}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{p}=(p_{1},p_{2},\ldots,p_{n})$ and $\mathbf{q}=(q_{1},q_{2},\ldots,q_{n})$
    represent the coordinates of two points in an $n$-dimensional space. The Euclidean
    distance $d(\mathbf{p},\mathbf{q})$ calculates the "as-the-crow-flies" distance
    between the two points, effectively measuring the length of the straight line
    segment that connects them.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.3 Manhattan Distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Also known as taxicab or city block distance, it is the sum of the absolute
    differences of their coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d(\mathbf{p},\mathbf{q})=&#124;p_{1}-q_{1}&#124;+&#124;p_{2}-q_{2}&#124;+\cdots+&#124;p_{n}-q_{n}&#124;$
    |  |'
  prefs: []
  type: TYPE_TB
- en: This metric is ideal for grid-based and urban environments where travel paths
    are constrained to grid layouts.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.4 Mahalanobis Distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Takes into account the correlations of the dataset and is scale-invariant,
    defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d(\mathbf{x},\mathbf{\mu})=\sqrt{(\mathbf{x}-\mathbf{\mu})^{T}\mathbf{S}^{-1}(\mathbf{x}-\mathbf{\mu})}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathbf{x}$ is the vector representing the point whose distance from the distribution
    is being measured.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathbf{\mu}$ is the mean vector of the distribution, representing the central
    tendency.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: $\mathbf{S}^{-1}$ is the inverse of the covariance matrix $\mathbf{S}$ of the
    distribution, which adjusts the distance measure to account for the spread and
    orientation of the data points.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This formulation accounts for the shape of the data distribution, correcting
    distances based on how data spreads and correlates across dimensions, thus providing
    a more nuanced measure of distance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.7.5 Hausdorff distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let us assume that there are two point clouds $S_{1}$ and $S_{2}$ in $R^{3}$
    with $N_{1}$ and $N_{2}$ points in each cloud, respectively. In order to compare
    these point sets, one of the earliest methods that was proposed is the Hausdorff
    distance ($\mathcal{D}_{H}$), which is based on finding the minimum distance from
    a point to a set [[134](#bib.bib134)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d(x,y)=\left\&#124;x-y\right\&#124;_{2}$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $D(x,S)=\underset{y\in S}{\text{min }}d(x,y)$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'and calculates a symmetric max min distance [[135](#bib.bib135)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{D}_{H}(S_{1},S_{2})=\max\left\{\underset{a\in S_{1}}{\max}D(a,S_{2}),\underset{b\in
    S_{2}}{\max}D(b,S_{1})\right\}$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 2.7.6 Chamfer distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The [Chamfer distance](#id17.17.id17) ([CD](#id17.17.id17)) is a metric used
    to evaluate the similarity between two sets of points in space. It considers the
    distance between each point in both sets and finds the nearest point in the other
    set, then sums the square of these distances.[CD](#id17.17.id17) is commonly used
    in the ShapeNet’s shape reconstruction challenge. The [CD](#id17.17.id17) between
    two point clouds, S1 and S2, is defined as [[136](#bib.bib136)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{D}_{CD}\left(S_{1},S_{2}\right)=\frac{1}{\left&#124;S_{1}\right&#124;}\sum_{x\in
    S_{1}}\underset{y\in S_{2}}{\min}\left\&#124;x-y\right\&#124;_{2}^{2}+\frac{1}{\left&#124;S_{2}\right&#124;}\sum_{y\in
    S_{2}}\underset{x\in S_{1}}{\min}\left\&#124;x-y\right\&#124;_{2}^{2}$ |  | (6)
    |'
  prefs: []
  type: TYPE_TB
- en: 2.7.7 Earth mover’s distance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In contrast to the Hausdorff distance and its derivative methods that involve
    identifying the closest neighboring point for each point, the Earth mover’s distance
    (EMD) or the Wasserstein distance operates by establishing a one-to-one correspondence
    (i.e., bijection represented by $\zeta$) between the two sets of points. The objective
    is to minimize the total distance between the corresponding points in the two
    sets [[134](#bib.bib134)].
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{D}_{EMD}(S_{1},S_{2})=\underset{\zeta:S_{1}\longrightarrow S_{2}}{\min}\sum_{a\in
    S_{1}}\left\&#124;a-\zeta(a)\right\&#124;_{2}$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 2.8 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There is a considerable number of datasets available for performing various
    [3DPC](#id1.1.id1) tasks using state-of-the-art techniques. A detail about datasets
    on which [DL](#id4.4.id4) has been implemented is reported in [[31](#bib.bib31)].
    However, in our study, we have considered the datasets for which [DTL](#id6.6.id6)
    has been incorporated to carry out [3DPC](#id1.1.id1) tasks. These include widely
    used benchmarks like, PointNet [[137](#bib.bib137)], ShapeNet [[117](#bib.bib117)],
    ModelNet [[128](#bib.bib128)], ScanNet [[117](#bib.bib117)], ISPRS [[138](#bib.bib138)],
    KITTI 3D object detection [[130](#bib.bib130)], Campus3D [[139](#bib.bib139)]
    and semantic 3D dataset [[140](#bib.bib140)]. Apart from these well-known benchmarks,
    in this study, newly introduced datasets are also included. These comprises of
    Scaffolds dataset [[140](#bib.bib140)], CMP façade dataset [[129](#bib.bib129)],
    DFC [[138](#bib.bib138)], Ouster LiDAR-64 [[130](#bib.bib130)], Santa Monica point
    cloud data [[141](#bib.bib141)], UTD-MHAD [[142](#bib.bib142)], PointDA-10 [[143](#bib.bib143)],
    NYU [[144](#bib.bib144)], SHREC2018 [[145](#bib.bib145)], and EndoSLAM dataset
    [[146](#bib.bib146)].
  prefs: []
  type: TYPE_NORMAL
- en: 'These datasets are used for [3DPC](#id1.1.id1) segmentation (semantic, panoptic,
    and instance), classification, and object detection and tracking and contain features
    beneficial for carrying out related tasks. For instance, datasets like ModelNet
    40, ShapeNet and ScanNet, among others, are used for shape classification and
    contain columns like sample size, classes, training, testing data percentage,
    and other useful features. In addition to this, SHREC2018 and EndoSLAM are medical-related
    datasets. The latter is created through the recording of multiple endoscope cameras
    for six porcine organs, as well as synthetically generated records. However, the
    former, the SHREC2018 protein dataset, contains 2267 protein details. To record
    the details, the protein data bank (PDB) format has been used. The datasets, references,
    and available links are given in Table [4](#S2.T4 "Table 4 ‣ 2.8 Datasets ‣ 2
    Background ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning:
    A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Dataset and evaluation metrics used by [DTL](#id6.6.id6) methods for
    various [3DPC](#id1.1.id1) tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | Dataset | Dataset availability | Evaluation metric | Best performance
    (%) |'
  prefs: []
  type: TYPE_TB
- en: '| [[117](#bib.bib117)] | ShapeNet, ScanNet | [http://www.scan-net.org/#code-and-data](http://www.scan-net.org/#code-and-data)
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[137](#bib.bib137)] | PointNet [[52](#bib.bib52)], ShapeNet [[147](#bib.bib147)],
    and ModelNet-40 | [http://stanford.edu/rqi/pointnet/](http://stanford.edu/rqi/pointnet/)
    [https://shapenet.org/](https://shapenet.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/) |
    - | - |'
  prefs: []
  type: TYPE_NORMAL
- en: '| [[148](#bib.bib148)] | UTD-MHAD | [http://www.utdallas.edu/kehtar/Kinect2Dataset.zip](http://www.utdallas.edu/kehtar/Kinect2Dataset.zip)
    | Recognition rate | 92.50 |'
  prefs: []
  type: TYPE_TB
- en: '| [[133](#bib.bib133)] | ModelNet40 | [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)
    | F1, precision, recall | F1 (91) |'
  prefs: []
  type: TYPE_TB
- en: '| [[149](#bib.bib149)] | ModelNet | [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/)
    | MAE-T, MAE-F | CA (97.6) |'
  prefs: []
  type: TYPE_TB
- en: '| [[128](#bib.bib128)] | ShapeNet, ModelNet | [https://shapenet.org/](https://shapenet.org/)
    [https://modelnet.cs.princeton.edu/](https://modelnet.cs.princeton.edu/) | OA
    | 90.40 |'
  prefs: []
  type: TYPE_TB
- en: '| [[138](#bib.bib138)] | ISPRS, DFC | [https://www.isprs.org/data/](https://www.isprs.org/data/)
    | F1, [OA](#id16.16.id16) | F1 (83.62), OA (89.84) |'
  prefs: []
  type: TYPE_TB
- en: '| [[130](#bib.bib130)] | KITTI 3D object detection, Ouster LiDAR-64 | [https://ouster.com/resources/lidar-sample-data/](https://ouster.com/resources/lidar-sample-data/)
    | FPS (frame per second) | FPS (30.6) |'
  prefs: []
  type: TYPE_TB
- en: '| [[150](#bib.bib150)] | ImageNet, KITTI 2D, VLP-16 | [http://www.image-net.org/about-stats](http://www.image-net.org/about-stats)
    | IoU, F1, mAP | mAP(81.27), IoU(65.11), F1 (0.82) |'
  prefs: []
  type: TYPE_TB
- en: '| [[140](#bib.bib140)] | Semantic3D dataset [22], Scaffolds dataset | [http://www.semantic3d.net/view_dbase.php?chl=1](http://www.semantic3d.net/view_dbase.php?chl=1)
    | F1, precision, recall | F1 (90.84) |'
  prefs: []
  type: TYPE_TB
- en: '| [[139](#bib.bib139)] | Campus3D | [https://3d.nus.app/](https://3d.nus.app/)
    | OA, IoU, mIoU, CR, CP and WCov | OA(90.9), mIoU(61.5) |'
  prefs: []
  type: TYPE_TB
- en: '| [[129](#bib.bib129)] | CMP façade dataset | [https://cmp.felk.cvut.cz/tylecr1/facade/](https://cmp.felk.cvut.cz/tylecr1/facade/)
    | Precision, recall, F1, Errors of commission, Errors of omission | Precision
    (85.97), recall (89.80), F1 (87.85) |'
  prefs: []
  type: TYPE_TB
- en: '| [[141](#bib.bib141)] | Santa Monica point cloud data, KITTI 2D, VLP-16 |
    [http://www.cvlibs.net/datasets/kitti/](http://www.cvlibs.net/datasets/kitti/)
    | F1, precision, recall | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[143](#bib.bib143)] | PointDA-10 | - | Accuracy | 49.70 |'
  prefs: []
  type: TYPE_TB
- en: '| [[145](#bib.bib145)] | SHREC2018 | - | precision, recall, NN, T1, T2, EM,
    DCG | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[146](#bib.bib146)] | EndoSLAM dataset | [https://github.com/CapsuleEndoscope/EndoSLAM](https://github.com/CapsuleEndoscope/EndoSLAM)
    | RMSE | - |'
  prefs: []
  type: TYPE_TB
- en: '| [[131](#bib.bib131)] | [3DPC](#id1.1.id1) tunnel dataset | - | Average precision
    (mPrec), average recall (mRec) | mPrec (71.2) |'
  prefs: []
  type: TYPE_TB
- en: '| [[151](#bib.bib151)] | 2014 scans | - | MAPE, RMSE | MAPE (0.416), RMSE (0.112)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[152](#bib.bib152)] | NYU | - | RMSE, Accuracy | (81.8), |'
  prefs: []
  type: TYPE_TB
- en: '| [[132](#bib.bib132)] | ShapeNetPart | [https://shapenet.org/](https://shapenet.org/)
    | OA, mIoU, Cat. mIoU, Ins. mIoU | OA (94.5) |'
  prefs: []
  type: TYPE_TB
- en: 3 Overview of DTL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Arguably one of the top success stories of [DL](#id4.4.id4) is [DTL](#id6.6.id6).
    Many applications in language and vision have benefited from the discovery that
    pretraining a network on a rich source set (e.g., ImageNet) can help boost performance
    once fine-tuned on a typically much smaller target set. Yet, very little is known
    about its usefulness in [3DPC](#id1.1.id1) understanding. We see this as an opportunity
    considering the effort required for annotating data in 3D.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Domain adaptation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \Acf
  prefs: []
  type: TYPE_NORMAL
- en: DA has shown significant improvements in various [ML](#id5.5.id5) and [CV](#id3.3.id3)
    tasks, such as classification, detection, and segmentation. However, there are
    limited methods that have achieved [DA](#id7.7.id7) directly on [3DPC](#id1.1.id1)
    data, to the best of our knowledge. The challenge of point cloud data lies in
    its rich spatial geometric information, where the semantics of the entire object
    are contributed by regional geometric structures. Most general-purpose DA methods
    that focus on global feature alignment and disregard local geometric information
    may not be suitable for 3D domain alignment. Wu and their colleagues [[153](#bib.bib153)]
    discusse the challenges of generating and annotating large amounts of real-world
    data for [DL](#id4.4.id4)-based approaches in robotics [CV](#id3.3.id3) tasks.
    To overcome this, the authors propose using simulation-to-reality (sim2real) [DTL](#id6.6.id6)
    for point cloud data in an industrial application case. They provide insights
    on generating and processing synthetic point cloud data to improve model performance
    when transferred to real-world data. The issue of imbalanced learning is also
    investigated, and the authors propose a novel patch-based attention network as
    a strategy to address this problem. Another work in [[154](#bib.bib154)], presents
    a new method called [DTL](#id6.6.id6)-based sampling-attention network (TSANet)
    for semantic segmentation of 3D urban point clouds to facilitate the development
    of smart cities. The method includes a segmentation model with point downsampling–upsampling
    structure, embedding method, attention mechanism, and focal loss for feature processing
    and learning. The [DTL](#id6.6.id6) technique aims to reduce data requirements
    and labeling efforts by leveraging prior knowledge. The method is evaluated on
    a realistic point cloud dataset of Cambridge and Birmingham cities, demonstrating
    promising performance, surpassing other state-of-the-art models in terms of accuracy
    and mean [IoU](#id13.13.id13). [[116](#bib.bib116)] introduces [SSL](#id14.14.id14)
    for [DA](#id7.7.id7) in 3D perception problems, specifically on point clouds.
    It describes a new family of pretext tasks called deformation reconstruction,
    inspired by sim-to-real transformations, and proposes a novel training procedure
    called point cloud mixup (PCM) motivated by the MixUp method for labeled point
    cloud data. Evaluations on [DA](#id7.7.id7) datasets for classification and segmentation
    show significant improvement over existing and baseline methods, demonstrating
    the effectiveness of SSL-PCM-DA on point clouds. Similarly, [[155](#bib.bib155)]
    introduces a new model called SqueezeSegV2 for point cloud segmentation that is
    more robust to dropout noise in [LiDAR](#id18.18.id18) point clouds. The improved
    model structure, training loss, batch normalization, and additional input channel
    result in significant accuracy improvement when trained on real data. To overcome
    the challenge of limited labeled point-cloud data, the proposed scheme employs
    [DA](#id7.7.id7) training pipeline, consisting of learned intensity rendering,
    geodesic correlation alignment, and progressive domain calibration. When trained
    on real data, the new model exhibits significant segmentation accuracy improvements
    over the original SqueezeSeg. Moreover, when trained on synthetic data using the
    proposed [DA](#id7.7.id7) pipeline, the test accuracy on real-world data nearly
    doubles. A similar scheme for segmentation is proposed in [[156](#bib.bib156)],
    introducing LiDARNet, a boundary-aware [DA](#id7.7.id7) model tailored for semantic
    segmentation of [LiDAR](#id18.18.id18) point cloud data. The model uses a two-branch
    structure to extract domain private and shared features, and incorporates Gated-SCNN
    to learn boundary information during segmentation. The domain gap is further reduced
    by learning a mapping between domains using shared and private features. They
    also introduce a new dataset, SemanticUSL, for [DA](#id7.7.id7) in [LiDAR](#id18.18.id18)
    semantic segmentation, which has the same format and ontology as Semantic KITTI.
    Experiments on real-world datasets show that LiDARNet achieves comparable performance
    on the [SD](#id10.10.id10) and significant performance improvement (8-22% mIoU)
    on the [TDs](#id9.9.id9) after adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other researchers have advanced unsupervised [DA](#id7.7.id7) approaches, demonstrated
    by the method outlined in [[157](#bib.bib157)], aimed at enhancing semantic labeling
    accuracy for [3DPC](#id1.1.id1) in autonomous driving scenarios. The proposed
    approach uses a complete and label approach, leveraging a [sparse voxel completion
    network](#id15.15.id15) ([SVCN](#id15.15.id15)) to recover underlying surfaces
    of sparse point clouds and transfer semantic labels across different [LiDAR](#id18.18.id18)
    sensors. The approach does not require manual labeling for training pairs and
    introduces local adversarial learning to the model surface prior. Experimental
    results on a new benchmark dataset show significant performance improvements ranging
    from 8.2% to 36.6% compared to previous [DA](#id7.7.id7) methods. Likewise, [[158](#bib.bib158)]
    introduces PointDAN, a 3D [DA](#id7.7.id7) network tailored for point cloud data,
    aligning global and local features at multiple levels to achieve domain alignment.
    It introduces a Self-adaptive node module for local alignment, which models discriminative
    local structures, and a node-attention module for hierarchical feature representation.
    For global alignment, an adversarial-training strategy is employed. PointDAN outperforms
    state-of-the-art [DA](#id7.7.id7) methods in terms of adapting [3DPC](#id1.1.id1)
    data across domains, as demonstrated on the benchmark dataset PointDA-10, created
    by the authors. Zhao et al. [[159](#bib.bib159)] proposes an end-to-end framework
    called ePointDA for simulation-to-real [DA](#id7.7.id7) (SRDA) in [LiDAR](#id18.18.id18)
    point cloud segmentation. ePointDA consists of three modules: self-supervised
    dropout noise rendering, statistics-invariant and spatially-adaptive feature alignment,
    and transferable segmentation learning. The framework bridges the domain shift
    at pixel-level and feature-level, without requiring real-world statistics. Experimental
    results on adapting from synthetic to real datasets demonstrate the superiority
    of ePointDA in [LiDAR](#id18.18.id18) point cloud segmentation. Xu and their colleagues
    [[160](#bib.bib160)] propose semantic point generation to enhance the reliability
    of LiDAR-based object detectors against domain shifts in autonomous driving. The
    scheme generates semantic points to recover missing parts of foreground objects
    caused by occlusions, low reflectance, or weather interference. By merging the
    semantic points with the original points, an augmented point cloud is obtained,
    which significantly improves the performance of modern LiDAR-based detectors in
    unsupervised [DA](#id7.7.id7) tasks. The method also benefits object detection
    in the original domain, surpassing KITTI when combined with PV-RCNN.'
  prefs: []
  type: TYPE_NORMAL
- en: Lang et al. [[161](#bib.bib161)] utilize PointNets to represent point clouds
    organized in vertical columns. The scheme, called PointPillars, achieves superior
    speed and accuracy, surpassing KITTI benchmarks while running at a significantly
    higher frame rate of 62 Hz. A faster version achieves state-of-the-art performance
    at 105 Hz, making it a suitable encoding approach for object detection in point
    clouds in robotics applications like autonomous driving. For object detection
    from [3DPC](#id1.1.id1), the authors [[162](#bib.bib162)] propose a Semi-Supervised
    [DA](#id7.7.id7) method for 3D object detection that leverages a small amount
    of labeled target data to improve adaptation performance. The method consists
    of an inter-[DA](#id7.7.id7) stage, which uses a Point-CutMix module to align
    point cloud distribution across domains, and an intra-domain generalization stage,
    which employs intra-domain Point-MixUp in semi-supervised learning to enhance
    model generalization on the unlabeled target set. Experimental results show that
    the proposed scheme, with only 10% labeled target data, outperforms a fully-supervised
    oracle model with 100% target labels on the Waymo to nuScenes domain shift. However,
    Du et al. [[163](#bib.bib163)] tackle the challenge of enhancing feature representation
    robustness in [3DPC](#id1.1.id1) by employing the [DA](#id7.7.id7) approach. The
    severe spatial occlusion and point density variance in point cloud data make designing
    robust features crucial. The proposed approach bridges the gap between the perceptual
    domain (real scene) and the conceptual domain (augmented scene with non-occluded
    point clouds), mimicking the functionality of human perception. Experimental results
    show that this simple yet effective approach significantly improves the performance
    of [3DPC](#id1.1.id1) object detection, achieving state-of-the-art results.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning stands as a ubiquitous [DTL](#id6.6.id6) approach, aiding pretrained
    models to adapt to new tasks through iterative training, often supplemented with
    additional layers known as the target model. [[164](#bib.bib164)]. The recent
    research studies, such as [[165](#bib.bib165), [166](#bib.bib166), [167](#bib.bib167)],
    have argued that even if the target task and source task are different, transferring
    features via [DTL](#id6.6.id6) approaches outperforms random features selections.
    This characteristic has led to the great success of [DTL](#id6.6.id6) in different
    spheres of real-life applications such as medical imaging [[168](#bib.bib168),
    [4](#bib.bib4)], recognition tasks like speech recognition [[26](#bib.bib26)],
    hand-gesture [[169](#bib.bib169)], face recognition [[170](#bib.bib170)], and
    rcently, in the processing of [3DPC](#id1.1.id1) data [[130](#bib.bib130), [171](#bib.bib171)].
    One of the major concerns raised in fine-tuning a pretrained model is to identify
    which layer to fine-tune. The authors in [[172](#bib.bib172)] have explicitly
    added regularization terms to the loss function for obtaining the parameters of
    the fine-tuned model close to the original pretrained model. AdaFilter [[164](#bib.bib164)],
    is an adaptive fine-tuning approach, which considers criterion for optimization
    by selecting only a part of the convolutional filters in the pretrained model.
    Furthermore, they have exploited a recurrent gated network and considered activation
    of the previous layer for carefully fine-tuning the desired convolutional filters
    only. The adaptive fine-tuning scheme considers the similarity between the source
    and target tasks and datasets to reuse more pretrained filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many works that involve processing [3DPC](#id1.1.id1) data have exploited [DTL](#id6.6.id6)
    for many tasks, include 3D reconstructions of scaffolds [[140](#bib.bib140)],
    [3DPC](#id1.1.id1) instance segmentation [[131](#bib.bib131)], classification
    of soft-story buildings using [3DPC](#id1.1.id1) data [[141](#bib.bib141)], [3DPC](#id1.1.id1)
    understanding [[117](#bib.bib117)], 3D orientation recognition [[149](#bib.bib149)],
    [SSL](#id14.14.id14) on [3DPCs](#id1.1.id1)  [[148](#bib.bib148)], and remaining
    useful life prediction [[151](#bib.bib151)], among others. Furthermore, some fine-tuning
    strategies involve all the pretrained parameters, whereas others fine-tuning the
    last few layers [[164](#bib.bib164)], as illusted in Fig. [7](#S2.F7 "Figure 7
    ‣ 2.3.1 Inductive DTL ‣ 2.3 Theoretical taxonomy of DTL ‣ 2 Background ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    (a). G Diraco et al. [[151](#bib.bib151)] have performed an extensive experiment
    for a particular scenario in which the amount of data in the [TD](#id9.9.id9)
    is small and explored how it behaves if only part of the network is fine-tuned
    on the given target dataset. They have updated the decoder of their model, which
    has been trained on [SDs](#id10.10.id10) and evaluated on the target dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: One factor contributing to the success of [DTL](#id6.6.id6) is its ability to
    perform well with smaller datasets. In support of this, the authors [[141](#bib.bib141)]
    have employed [DTL](#id6.6.id6) to address the challenge of training large parameters
    in deep convolutional networks. Their experimental results demonstrate the transferability
    of [DTL](#id6.6.id6) from [SDs](#id10.10.id10) to [TD](#id9.9.id9), as the dataset
    under consideration lacked sufficient data to train large parameters. Additionally,
    compared to traditional training methods, [DTL](#id6.6.id6) consumes less time,
    making it a more efficient approach. To support this claim, the authors noted
    that models such as VGGNet, Inception and ResNet, which were trained using fine-tuning
    techniques, exhibited shorter training times by triggering the early stopping
    mechanism. This improvement in training time has enabled the workflow to quickly
    identify soft-story buildings on a city scale. Furthermore, the authors have shown
    that [DTL](#id6.6.id6) can effectively reduce the risk of overfitting that can
    occur with [DL](#id4.4.id4) techniques. By leveraging a pretrained network, [DTL](#id6.6.id6)
    can transfer knowledge and prevent the model from memorizing the training data,
    leading to more robust and generalizable performance. Moving on, Xiu et al. [[173](#bib.bib173)]
    propose using airborne [LiDAR](#id18.18.id18) to detect collapsed buildings during
    earthquake emergency response, as [DTL](#id6.6.id6)-based damage detection with
    aerial images has limitations in detecting collapsed buildings with undamaged
    roofs. The authors develop a [3DPC](#id1.1.id1)-based dataset for building damage
    detection and propose a general extension framework and a visual explanation method
    to validate model decisions. The results conducted using PointNet [[52](#bib.bib52)],
    PointNet++ [[53](#bib.bib53)] and DGCNN [[174](#bib.bib174)] show that [3DPC](#id1.1.id1)-based
    methods can achieve high accuracy and are robust even with reduced training data.
    The model also achieves moderate accuracy on another dataset with different architectural
    styles without additional training.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Unsupervised DTL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unsupervised [DTL](#id6.6.id6) is mainly based on [unsupervised domain adaptation](#id8.8.id8)
    ([UDA](#id8.8.id8)) to remove the need or addressing the issue of lack for labeled
    data and allows any image to be used as a datapoint for any [CV](#id3.3.id3) tasks
    [[175](#bib.bib175), [176](#bib.bib176)]. Numerous methods have been proposed
    for performing [UDA](#id8.8.id8) on 2D images, which can be divided into two main
    categories: methods based on domain-invariant feature learning and methods for
    learning domain mapping. The former [[177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179),
    [180](#bib.bib180), [181](#bib.bib181)] aim to minimize the discrepancy between
    two distributions in the feature space, while the latter [[182](#bib.bib182),
    [183](#bib.bib183), [184](#bib.bib184)] use neural networks, such as CycleGAN
    [[53](#bib.bib53)], to directly learn the translation from the [SD](#id10.10.id10)
    to the [TDs](#id9.9.id9). In [[185](#bib.bib185)], 2D translation is extended
    to depth images using a differential contrastive learning strategy for preserving
    underlying geometries. Despite their differences, these methods widely exploit
    domain adversarial training. Additionally, several useful techniques, such as
    pseudo-labeling [[186](#bib.bib186)] and batch normalization tailored for [DA](#id7.7.id7)  [[187](#bib.bib187)],
    have also been proposed.'
  prefs: []
  type: TYPE_NORMAL
- en: Although there have been significant efforts made in [UDA](#id8.8.id8) for 2D
    images and depth, [UDA](#id8.8.id8) on [3DPC](#id1.1.id1) is still in its early
    stages. [UDA](#id8.8.id8) on point clouds involves extending domain adversarial
    training from 2D images to [3DPC](#id1.1.id1) to align features on both local
    and global levels [[188](#bib.bib188)]. Nevertheless, adversarial methods on [3DPC](#id1.1.id1)
    struggle to balance local geometry alignment and global semantic alignment. CycleGAN
    [[189](#bib.bib189)] is utilized by both [[159](#bib.bib159)] and [[190](#bib.bib190)]
    to generate more realistic [LiDAR](#id18.18.id18) point clouds from synthetic
    data. This Sim2Real approach is used to minimize feature distances between the
    [SD](#id10.10.id10) and [TD](#id9.9.id9). The work in [[157](#bib.bib157)], on
    the other hand, leverages segmentation on completed surface reconstructed from
    sparse point cloud for better adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: For object-level tasks, [[191](#bib.bib191), [158](#bib.bib158)] align global
    and local features, while [[155](#bib.bib155)] and [[190](#bib.bib190)] project
    point clouds to 2D and birds-eye view, respectively, to reduce sparsity. [[163](#bib.bib163)]
    creates a car model set and adapts their features for detection object features,
    but only targets general car 3D detection on a single point cloud domain. Recently,
    [[192](#bib.bib192)] published the first study targeting [UDA](#id8.8.id8) for
    3D [LiDAR](#id18.18.id18) detection. They identify the vehicle size as the domain
    gap between KITTI [[193](#bib.bib193)] and other datasets and resize the vehicles
    in the data. In contrast, [[160](#bib.bib160)] identifies point cloud quality
    as the major domain gap between Waymo’s two datasets [[194](#bib.bib194)] and
    proposes a learning-based approach to close the gap.
  prefs: []
  type: TYPE_NORMAL
- en: Recent works on [UDA](#id8.8.id8) on point clouds mainly focus on designing
    suitable self-supervised tasks on point clouds to facilitate learning domain invariant
    features, which are discussed in detail in the following subsection. In addition
    to [UDA](#id8.8.id8) on object point clouds, several methods have been proposed
    to address specific domain gaps on [LiDAR](#id18.18.id18) point clouds. These
    methods commonly address depth missing and sampling difference between sensors.
    ST3D [[195](#bib.bib195)] presents a task-specific self-training pipeline with
    curriculum data augmentation to further improve the adaptation process
  prefs: []
  type: TYPE_NORMAL
- en: Alternatively, most existing [UDA](#id8.8.id8) approaches focus on uni-modal
    data, despite the availability of multi-modal datasets. In [[196](#bib.bib196)],
    the authors propose a cross-modal [UDA](#id8.8.id8) (xMUDA) approach for 3D semantic
    segmentation, where both 2D images and [3DPC](#id1.1.id1) are utilized. This is
    challenging as the two input modalities are heterogeneous and can be affected
    differently by domain shift. In xMUDA, the modalities learn from each other through
    mutual mimicking, separate from the segmentation objective, to prevent the stronger
    modality from adopting false predictions from the weaker one. The proposed xMUDA
    approach is evaluated on various [UDA](#id8.8.id8) scenarios, such as day-to-night,
    country-to-country, and dataset-to-dataset shifts, using recent autonomous driving
    datasets. Results show that xMUDA significantly improves over uni-modal [UDA](#id8.8.id8)
    in all tested scenarios and complements state-of-the-art [UDA](#id8.8.id8) techniques.
    Saltori et al. [[197](#bib.bib197)] introduces a novel source-free [UDA](#id8.8.id8)
    (SF-UDA) framework for 3D object detection using [LiDAR](#id18.18.id18) point
    clouds, which does not require any annotations from the [SD](#id10.10.id10) or
    images/annotations from the [TD](#id9.9.id9). It addresses domain shift in [LiDAR](#id18.18.id18)
    data, which is not only due to changes in environment and object appearances,
    but also to geometry (e.g., point density variations). SF-UDA^(3D) utilizes pseudo-annotations,
    reversible scale-transformations, and motion coherency for [DA](#id7.7.id7). Experimental
    results on large-scale datasets, KITTI and nuScenes, show that SF-UDA^(3D) outperforms
    previous methods based on feature alignment and state-of-the-art 3D object detection
    methods that use few-shot target annotations or target annotation statistics.
    Also, [[198](#bib.bib198)] introduces RefRec, an approach that investigate pseudo-labels
    and self-training for [UDA](#id8.8.id8) in point cloud classification. Instead
    of relying on multi-task learning, RefRec proposes two innovations for effective
    self-training on 3D data. First, it refines noisy pseudo-labels by matching shape
    descriptors learned from the unsupervised task of shape reconstruction on both
    domains. Second, it proposes a novel self-training protocol that learns domain-specific
    decision boundaries and mitigates the negative impact of mislabelled target samples
    and in-domain intra-class variability. RefRec achieves state-of-the-art performance
    on standard benchmarks for [UDA](#id8.8.id8) in point cloud classification, demonstrating
    the effectiveness of self-training for this emerging research problem. Additionally,
    [[199](#bib.bib199)] focuses on [UDA](#id8.8.id8) in 3D [CV](#id3.3.id3) tasks,
    specifically point cloud visual tasks. The proposed approach introduces a dual-branch
    feature alignment network (DFAN) architecture that leverages the characteristics
    of local and global features in point clouds. The approach utilizes different
    strategies for feature extraction and alignment in each branch, complementing
    each other. Hierarchical alignment for local features and distribution alignment
    for global features are also introduced. Experimental results on benchmark datasets
    demonstrate that the proposed approach achieves state-of-the-art performance in
    point cloud classification and segmentation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Previous studies on unsupervised 3D learning have principally concentrated on
    ShapeNet [[147](#bib.bib147)], which is a repository of single-object computer
    aided design (CAD) models. Typically, the idea is to use ShapeNet as the ImageNet
    counterpart in 3D so that the characteristics learned on single synthetic objects
    can be transferred to other real-world applications. In this regard, numerous
    works have then been proposed. For instance, [[200](#bib.bib200)] introduces a
    point-level DA by searched transformations. Typically, transformations of [3DPCs](#id1.1.id1)
    are learned via the search of the best combination of operations on [3DPCs](#id1.1.id1),
    which transfers data from the [SD](#id10.10.id10) to the [TD](#id9.9.id9) while
    keeping the [TD](#id9.9.id9) unlabeled. Besides, most [3DPC](#id1.1.id1)-based
    [UDA](#id8.8.id8) techniques focus on extracting domain-invariant features in
    different domains for feature alignment. In this regard, [UDA](#id8.8.id8) is
    advocated for the detection of 3D objects in the context of semantic point generation,
    as proposed in [[160](#bib.bib160)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving on, in [[157](#bib.bib157)], a [UDA](#id8.8.id8) problem approach for
    the semantic labeling of [3DPCs](#id1.1.id1) is proposed, focusing on domain gap
    reduction of [LiDAR](#id18.18.id18) sensors. Based on the observation that sparse
    [3DPCs](#id1.1.id1) are sampled from 3D surfaces, a [SVCN](#id15.15.id15) was
    designed to complete the 3D surfaces of a sparse [3DPC](#id1.1.id1). Unlike semantic
    labels, obtaining training pairs for [SVCN](#id15.15.id15) requires no manual
    labeling. A local adversarial learning to model the surface prior was also introduced.
    The recovered 3D surfaces serve as a canonical domain from which semantic labels
    can transfer across different [LiDAR](#id18.18.id18) sensors. Experiments and
    ablation studies with benchmark for cross-domain semantic labeling of [LiDAR](#id18.18.id18)
    data show that this approach provides 6.3-37.6% better performance than previous
    DA methods. Fig. [9](#S3.F9 "Figure 9 ‣ 3.3 Unsupervised DTL ‣ 3 Overview of DTL
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey") portrays the block diagram of the the [UDA](#id8.8.id8) scheme introduced
    in [[157](#bib.bib157)] for semantic segmentation of [LiDAR](#id18.18.id18)  [3DPCs](#id1.1.id1).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/79770984fc4459add9e9b5abff6f519b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Flowchart of the [UDA](#id8.8.id8) approach proposed in [[157](#bib.bib157)]
    for semantic segmentation of [LiDAR](#id18.18.id18) [3DPCs](#id1.1.id1)'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Semi-supervised DTL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The integration of semi-supervised DTL into 3DPC understanding for tasks like
    object detection and segmentation is rapidly transforming the field, particularly
    in contexts where labeled data are scarce. This approach leverages both labeled
    and unlabeled data, enhancing learning efficiency and model performance across
    various applications, from autonomous driving to tunnel monitoring. Typically,
    Tang et al. [[201](#bib.bib201)] develope a semi-supervised 3D object detection
    model that utilizes a novel network to transfer knowledge from well-labeled object
    classes to weakly labeled classes, improving detection in classes with only 2D
    labels. Moving on, Ji et al. [[5](#bib.bib5)] propose the Semi-supervised Learning-based
    Point Cloud Network (SPCNet) which integrates various learning modules to enhance
    tunnel scene segmentation from 3DPCs, significantly reducing the reliance on extensive
    labeled datasets. Imad et al. [[130](#bib.bib130)]: Focused on utilizing 3D LiDAR
    data for autonomous driving perception, enhancing object detection through a semi-supervised
    learning framework that minimizes the need for large-scale annotated datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: In this same direction, Huang et al. [[202](#bib.bib202)] present a point cloud
    registration framework that minimizes a feature-metric projection error without
    needing correspondences, using a semi-supervised approach. This method is particularly
    robust to noise and density differences in point clouds. Horache et al. [[203](#bib.bib203)]
    propose a method for generalizing deep learning for 3DPC registration on entirely
    different datasets using a combination of Multi-Scale Sparse Voxel Convolution
    and an unsupervised transfer algorithm, UDGE, enhancing adaptability across varied
    real-world datasets. Li et al. [[204](#bib.bib204)] introduce a semi-supervised
    point cloud segmentation method that utilizes both labeled and unlabeled data.
    By employing adversarial architecture for confidence discrimination of label predictions
    on unlabeled point clouds, their approach enhances segmentation performance.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, Chen et al. [[205](#bib.bib205)] propose a multimodal semi-supervised
    learning framework that utilizes instance-level consistency and a novel multimodal
    contrastive prototype loss to enforce consistent representations across different
    3D data modalities of the same object. Similarly, Xiao et al. [[206](#bib.bib206)]
    tackle the synthetic-to-real data gap in 3DPCs segmentation by developing a large-scale
    synthetic dataset and a novel translation method that decomposes and addresses
    the differences in appearance and sparsity between synthetic and real datasets.
    Moving on, Mei et al. [[207](#bib.bib207)] develop a system for the semantic segmentation
    of 3D LiDAR data in dynamic scenes, using a semi-supervised learning strategy
    that combines limited manual annotations with large amounts of constraint data
    to enhance scene adaptability. Additionally, Huang et al. [[208](#bib.bib208)]
    introduce a spatio-temporal representation learning framework for learning from
    unlabeled 3DPCs in a self-supervised manner, facilitating the generalization of
    pre-trained models to a variety of downstream tasks. Moreover, Qin et al. [[209](#bib.bib209)]
    propose a weakly supervised framework for 3D object detection that leverages unsupervised
    3D proposal generation and cross-modal knowledge distillation, reducing the dependency
    on annotated 3D bounding boxes.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, Yu et al. [[210](#bib.bib210)] tackle the data scarcity
    challenge in 3D tasks by transferring knowledge from robust 2D models to augment
    RGB-D images with pseudo-labels, significantly improving the pre-training of 3D
    models with limited labeled data. Xu et al. [[211](#bib.bib211)] develop a hierarchical
    point-based active learning strategy for 3DPC segmentation, which measures uncertainty
    at multiple levels and selects important points for manual labeling, effectively
    utilizing limited annotations. Zhang et al. [[212](#bib.bib212)] study weakly
    semi-supervised 3D object detection with point annotations to generate high-quality
    pseudo-bounding boxes, enabling 3D detectors to perform comparably to fully-supervised
    models with substantially fewer labeled data. Lastly, Wang et al. [[213](#bib.bib213)]
    introduce a Semi-Supervised Domain Adaptation method for 3D object detection (SSDA3D)
    that employs an Inter-domain Point-CutMix module to align point cloud distributions
    across domains and an Intra-domain Point-MixUp for enhancing model generalization
    on unlabeled target data. Table [5](#S3.T5 "Table 5 ‣ 3.4 Semi-supervised DTL
    ‣ 3 Overview of DTL ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey") summarizes and compares the above-discussed
    studies based on several aspects. This comparison underscores the dynamic evolution
    of semi-supervised DTL techniques in handling 3DPCs, which is pivotal for applications
    ranging from autonomous vehicles to environmental scanning and medical imaging.
    These advancements promise to drive significant improvements in how 3D data is
    processed and utilized across various fields.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Comparison of Studies on Semi-Supervised DTL in 3DPCs'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | ML Model | Dataset | Application / Task | Advantage | Limitation |'
  prefs: []
  type: TYPE_TB
- en: '| [[201](#bib.bib201)] | Semi-supervised 3D Object Detection | SUN-RGBD, KITTI
    | 3D object detection from 2D and 3D labels | Efficiently transfers 3D info from
    strong to weak classes | Requires sufficient data in strong classes for effective
    transfer |'
  prefs: []
  type: TYPE_TB
- en: '| [[205](#bib.bib205)] | Multimodal Semi-supervised Learning | ModelNet10,
    ModelNet40 | 3D classification and retrieval | Improves data efficiency using
    multimodal data consistency | Performance depends on the quality of multimodal
    data integration |'
  prefs: []
  type: TYPE_TB
- en: '| [[5](#bib.bib5)] | SPCNet | Real tunnel point clouds | Multi-class object
    segmentation in tunnel scenes | Reduces reliance on labeled data, enhances segmentation
    performance | Specific to tunnel environments, may not generalize |'
  prefs: []
  type: TYPE_TB
- en: '| [[130](#bib.bib130)] | DTL based Semantic Segmentation | KITTI, Ouster LiDAR-64
    | 3D object detection in autonomous driving | Reduces need for large-scale datasets,
    fast processing | Limited by the initial data quality and transfer efficiency
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[213](#bib.bib213)] | Semi-Supervised Domain Adaptation (SSDA3D) | Waymo,
    nuScenes | 3D object detection in diverse conditions | Adapts to new domains with
    minimal labeled data | Performance may degrade with severe domain shifts |'
  prefs: []
  type: TYPE_TB
- en: '| [[206](#bib.bib206)] | DTL with PCT | SynLiDAR | 3DPC segmentation | Bridges
    the gap between synthetic and real data, extensive dataset | Focuses on segmentation;
    may not directly apply to other 3D tasks |'
  prefs: []
  type: TYPE_TB
- en: '| [[202](#bib.bib202)] | Semi-supervised or unsupervised feature-metric registration
    | N/A | Point cloud registration | Robust to noise and does not require correspondences;
    fast optimization | Limited details on specific dataset adaptability and real-world
    implementation |'
  prefs: []
  type: TYPE_TB
- en: '| [[203](#bib.bib203)] | MS-SVConv and UDGE | 3DMatch, ETH, TUM | 3DPC registration
    | Generalizes across different datasets using unsupervised DTL | May require substantial
    computational resources; specific adaptation challenges not addressed |'
  prefs: []
  type: TYPE_TB
- en: '| [[207](#bib.bib207)] | CNN-based classifier | Custom LiDAR dataset | Semantic
    segmentation of dynamic scenes | Combines few annotations with large constraint
    data for improved adaptability | Primarily tailored to dynamic scenes, may not
    generalize to static or varied environments |'
  prefs: []
  type: TYPE_TB
- en: '| [[208](#bib.bib208)] | Spatio-Temporal Representation Learning (STRL) | Synthetic,
    indoor, outdoor datasets | 3D scene understanding | Learns from unlabeled data;
    generalizes to multiple 3D tasks | Dependence on temporal correlation which may
    not be present in all datasets |'
  prefs: []
  type: TYPE_TB
- en: '| [[210](#bib.bib210)] | DTL from 2D to 3D models | ScanNet | Semantic segmentation
    | Uses pseudo-labels for pre-training 3D models; enhances data efficiency | Relies
    on the quality and relevance of 2D model training to 3D tasks |'
  prefs: []
  type: TYPE_TB
- en: '| [[209](#bib.bib209)] | Weakly supervised 3D object detection | KITTI | 3D
    object detection | Reduces need for detailed annotations; uses unsupervised proposal
    generation | Performance may lag behind fully-supervised methods; adaptation to
    other datasets not detailed |'
  prefs: []
  type: TYPE_TB
- en: '| [[204](#bib.bib204)] | Semi-supervised point cloud segmentation | N/A | Point
    cloud segmentation | Utilizes both labeled and unlabeled data; improves with self-training
    | Specifics on dataset and environmental adaptability are not detailed |'
  prefs: []
  type: TYPE_TB
- en: '| [[211](#bib.bib211)] | Hierarchical point-based active learning | S3DIS,
    ScanNetV2 | Point cloud semantic segmentation | Efficient use of very few labeled
    data; incorporates active learning | May require intricate setup for uncertainty
    measurement and point selection |'
  prefs: []
  type: TYPE_TB
- en: '| [[212](#bib.bib212)] | Vision Transformer-based WSS3D | SUN RGBD, KITTI |
    3D object detection | Low reliance on fully labeled data; uses point annotations
    effectively | Challenges with varying detector compatibility and scene diversity
    |'
  prefs: []
  type: TYPE_TB
- en: 3.5 Inductive transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In [inductive transfer learning](#id20.20.id20) ([ITL](#id20.20.id20)), usually
    [SD](#id10.10.id10) and [TD](#id9.9.id9) remain the same, however target task
    differs from the source task. In [ITL](#id20.20.id20) setting, knowledge is transferred
    from source task to attain high performance in the target task. As suggested in
    [[214](#bib.bib214), [215](#bib.bib215)], the target is labeled, whereas source
    may be labeled, unlabeled, or both. However, in the field of [3DPC](#id1.1.id1),
    it is observed that only labeled source has been explored.
  prefs: []
  type: TYPE_NORMAL
- en: For instance, Chen et al. [[141](#bib.bib141)] focus on classifying soft-story
    buildings using [CNNs](#id11.11.id11) and density features extracted from [3DPCs](#id1.1.id1).
    More specifically, Once the [3DPC](#id1.1.id1) data of Santa Monica city is collected,
    density features are extracted and converted into 2D imagery data. The task of
    identifying a soft-story building is then approached as a binary classification
    problem, which has effectively been addressed using [CNN](#id11.11.id11) models,
    including VGG, Inception, ResNet and Naive [CNN](#id11.11.id11). They have trained
    the [SD](#id10.10.id10) with labeled settings exploiting more than 1.2 million
    images and 1,000 labeled categories, and surprisingly VGGNet has proved to be
    the best in 2014, with its different versions namely, VGG19 and VGG16\. In addition
    to this, 138 million parameters are trained in the network. VGGNet uses 3 x 3
    filters, contrary to this, Inception uses 1 × 1 filters, limiting the number of
    input channels. Hence, the trainable parameters for Inception is low, approximately
    6.4 million trainable parameters. In an another work, Murtiyoso et al. [[129](#bib.bib129)],
    have taken advantage of DeepLabv3+ network which they trained on labeled dataset
    of building façade images. The trained network is deployed on [TD](#id9.9.id9),
    2D orthoimages received from photogrammetry. Similarly, a [DL](#id4.4.id4)-based
    encoder-decoder lightweight neural architecture, RandLA-Net, for the semantic
    segmentation of [3DPC](#id1.1.id1) data, has been utilized in [[140](#bib.bib140)]
    for performing per-point segmentation of large-scale [3DPCs](#id1.1.id1), as detailed
    in [[216](#bib.bib216)], and demonstrated a good semantic segmentation performance
    in the Semantic 3D benchmark [[140](#bib.bib140), [217](#bib.bib217)].
  prefs: []
  type: TYPE_NORMAL
- en: In [[218](#bib.bib218)], authors have exploited [ITL](#id20.20.id20) for [3DPC](#id1.1.id1)
    classification using an airborne laser scanning method. They have extracted deep
    features using deep residual network, and [CNN](#id11.11.id11) was dedicated to
    classification task. To exploit [CNN](#id11.11.id11) further, the unevenly distributed
    [3DPC](#id1.1.id1) is transformed into voxels [[219](#bib.bib219)]. However, this
    transformation can lead to information redundancy as well as some feature loss.PointNet
    and later PointNet++ were proposed in [[52](#bib.bib52)] and [[53](#bib.bib53)],
    respectively. They successfully directly classify the original [3DPC](#id1.1.id1),
    leading to the proliferation of PointNet-like approaches for [3DPC](#id1.1.id1)
    classification. However, classification accuracy is affected when small range
    of multi-view projection influence the detailed description on each 3D point.
    In addition, there is a need of huge computing time for generating feature maps.
  prefs: []
  type: TYPE_NORMAL
- en: Lie and his team [[138](#bib.bib138)] have used DensNet201 for obtaining deep
    features, and make use of an improved fully [CNN](#id11.11.id11) by incorporating
    it into the [ITL](#id20.20.id20). The result was very promising and superior to
    state-of-the-art on ISPRS dataset for [OA](#id16.16.id16). In many cases weight
    transfer has found to be instrumental in classification performance. For instance,
    Kamil and Marian [[142](#bib.bib142)] has suggested weights transfer while they
    use [ITL](#id20.20.id20) to improve the classification accuracy of [bidirectional
    LSTM](#id19.19.id19) ([BiLSTM](#id19.19.id19))-based approach for identifying
    human activities. Similarly, Sun et al. [[220](#bib.bib220)] has incorporated
    deep [ITL](#id20.20.id20) to explore the options for solving the issues that are
    encountered with SAE networks in the prediction of remaining useful life by utilizing
    weight and feature transfer.
  prefs: []
  type: TYPE_NORMAL
- en: For semantic labeling of heritage, Arnold et al. [[133](#bib.bib133)] has utilized
    [ITL](#id20.20.id20) by considering geometric shape characteristic for the purpose
    of segmentation of memorial objects from the scene. Further, they pretrained [CNN](#id11.11.id11)
    on a model from ModelNet’s labeled dataset. Similarly, for body measurement of
    cattle, Huang et al. [[221](#bib.bib221)] have pretrained the Kd-network by obtaining
    3D deep model’s initial parameters from ShapeNet. The authors extracted the [3DPC](#id1.1.id1)
    spatial feature information which are transferred for identifying cattle body
    silhouette. In addition, their [ITL](#id20.20.id20) method is applicable even
    if the data distribution is different for source and target data.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Transductive transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The [transductive transfer learning](#id25.25.id25) ([TTL](#id25.25.id25)) was
    introduced by Arnold et al. [[222](#bib.bib222)] to solve a task in which domains
    are different however the source and target tasks remain same . The authors advocate
    that at the training time, all unlabeled data must be available in the [TD](#id9.9.id9),
    contrary to this, Pan and Yang [[215](#bib.bib215)] urges that for finding out
    marginal probability, it would be enough to seeing part of the unlabeled target
    data at the training time.
  prefs: []
  type: TYPE_NORMAL
- en: Applying [DTL](#id6.6.id6) for different domain encounters many issues. One
    of these issues for [SDs](#id10.10.id10) and [TDs](#id9.9.id9) is distribution
    mismatching. There are few works based on [UDA](#id8.8.id8) methods that have
    been reported in the literature for the adaptation of a model to an unlabeled
    [TD](#id9.9.id9) from a labeled [SD](#id10.10.id10) for various different applications
    [[223](#bib.bib223), [224](#bib.bib224), [177](#bib.bib177), [225](#bib.bib225),
    [226](#bib.bib226)]. In addition to [UDA](#id8.8.id8), authors in [[143](#bib.bib143)]
    has suggested [source-free unsupervised DA](#id21.21.id21) ([SFUDA](#id21.21.id21)).
    They have used virtual domain modeling to address of one the most talked issue
    in [SFUDA](#id21.21.id21) without requiring original source, reducing source and
    task data mismatching. To fill the gap between [SD](#id10.10.id10) and [TD](#id9.9.id9)
    distributions, they have introduced an intermediate virtual domain, reducing the
    distribution mismatch into two steps, minimizing the domain gap between [SD](#id10.10.id10)
    and virtual domains and then between the virtual and [TDs](#id9.9.id9). To achieve
    this goal, the authors have generated the virtual domain samples in the feature
    space using an approximated Gaussian mixture model (GMM) and the pretrained source
    model, so that the virtual domain maintains a similar distribution to the [SD](#id10.10.id10)
    without access to the original source data. On the other hand, they have proposed
    an effective distribution alignment method that gradually improves the compactness
    of the [TD](#id9.9.id9) distribution through model learning to reduce the aforementioned
    distribution gap. However, the first [SFUDA](#id21.21.id21) framework was proposed
    in [[197](#bib.bib197)], where the authors exploited the source model to fine-tune
    the [TD](#id9.9.id9) data without access to the source data for 3D object detection.
    The paper [[116](#bib.bib116)] discussed the use of [SSL](#id14.14.id14) to learn
    useful representations from unlabeled data in [DA](#id7.7.id7) for 3D perception
    problems. The authors proposed a new family of pretext tasks employing [SSL](#id14.14.id14)
    for [DA](#id7.7.id7) on point clouds, called deformation reconstruction, along
    with a novel training procedure called [point cloud mixup](#id22.22.id22) ([PCM](#id22.22.id22))
    for labeled point cloud data. The results demonstrated that employing [SSL](#id14.14.id14)
    with this technique significantly improves classification and segmentation in
    [DA](#id7.7.id7) datasets.
  prefs: []
  type: TYPE_NORMAL
- en: As the research on DA for [3DPC](#id1.1.id1) is evolving, researchers are working
    to improve the method to reduce the [SD](#id10.10.id10)’ differences from [TD](#id9.9.id9),
    and to maximize the domain adaptability so that it can be generalized. However,
    most of the research addresses [DA](#id7.7.id7) through experimentation over different
    domain meant for different but related task. Table LABEL:UDA summarizes some of
    the relevant studies, compares their characteristics and identifies their pros
    and cons.
  prefs: []
  type: TYPE_NORMAL
- en: 'The works [[227](#bib.bib227), [228](#bib.bib228)] require handling preprocessing
    tasks and additional data, which in turn increases complexity. Rist et al. [[227](#bib.bib227)]
    exploit cross-sensor [DA](#id7.7.id7) via 3D voxels, while Wang et al. [[228](#bib.bib228)]
    utilize adversarial networks for global adaptation by exploiting cross-range adaptation.
    In this context, the authors in [[229](#bib.bib229)] present a potentially significant
    contribution. Their approach circumvents complex procedures and the need for additional
    data. Instead, they leverage the KITTI benchmark dataset to align strong-weak
    features and enhance feature representation, supplementing it with available data
    when necessary. Specifically, they focus on the ’car’ example from the benchmark
    dataset, considering ’near-range’ and ’far-range’ objects as the [SD](#id10.10.id10)
    and [TD](#id9.9.id9), respectively. In addition to the underlying issues in handling
    different domain for DA problems, distribution mismatch within a domain has also
    been raised by researchers. In this regard, to adapt the different scenario within
    a single domain, Zhang et al. [[230](#bib.bib230)] have taken the advantage of
    cross-dataset and considered several adaptation scenario like day-night adaptation
    and adaptation of different scenes (say, from different places) as in the case
    with nuScenes dataset consisting of driving scenes from different geographical
    locations, Boston and Singapore. Their extensive experiments enable understanding
    how researchers can leverage cross-datasets for [UDA](#id8.8.id8). Moreover, they
    have introduced range-aware and scale-aware detection mechanism for [3DPC](#id1.1.id1)
    tasks. In the similar direction, Jaritz et al. [[196](#bib.bib196)] has considered
    the same scenario, i.e., day-night adaptation and Boston-Singapore scenario, however
    hey have primarily focused on cross-modality adaptation using their xMUDA model.
    By the virtue of their proposed method, they have shown a better performance can
    be achieved for cross-modality. Moreover, A2D2 and Semantic KITTI datasets were
    used as source and target respectively. Moving forward, Nunes et al. [[231](#bib.bib231)]
    present a new contrastive learning approach for representation learning of 3DPs
    point cloud data in the context of autonomous driving. The approach extracts class-agnostic
    segments and applies contrastive loss to discriminate between similar and dissimilar
    structures. The method is applied on data recorded with a 3D [LiDAR](#id18.18.id18)
    and achieves competitive performance in comparison to other self-supervised contrastive
    point cloud methods. Fig. [10](#S3.F10 "Figure 10 ‣ 3.6 Transductive transfer
    learning ‣ 3 Overview of DTL ‣ Advancing 3D Point Cloud Understanding through
    Deep Transfer Learning: A Comprehensive Survey") presents an example of fine-tuning
    in [3DPC](#id1.1.id1) segmentation [[196](#bib.bib196)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0059369f88435ceb1aba7014d30b8d27.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Example of using fine-tuning in [3DPC](#id1.1.id1) segmentation:
    (A) Data augmentation is used to generate the augmented views $\mathcal{P}^{q}$
    and $\mathcal{P}^{k}$ from a point cloud $\mathcal{P}$, and class-agnostic segments
    $S$ are extracted from $\mathcal{P}$. The augmented segments $\mathcal{S}^{q}$
    and $\mathcal{S}^{k}$ are determined with their point-wise features using the
    point indexes of $\mathcal{S}$ extracted from $\mathcal{P}$. Point-wise features
    $\mathcal{F}^{q}$ and $\mathcal{F}^{k}$ are computed, followed by dropout and
    global max pooling over each segment. The segment feature vectors are projected
    using the projection head to obtain the final features $\textbf{s}^{q}_{m}$ and
    $\textbf{s}^{k}_{m}$ from the $M$ segments, and the contrastive loss is computed,
    and (B) The pre-trained backbone is fine-tuned for the downstream task, i.e.,
    semantic segmentation [[231](#bib.bib231)].'
  prefs: []
  type: TYPE_NORMAL
- en: A UDA-based semantic labeling scheme of [3DPCs](#id1.1.id1) is proposed in [[157](#bib.bib157)],
    where domain discrepancies induced by different [LiDAR](#id18.18.id18) sensors
    have been addressed. Typically, a complete and label technique that recovers 3D
    surfaces is developed before passing them to a segmentation network. More precisely,
    a [SVCN](#id15.15.id15) to complete the 3D surfaces of a sparse [3DPC](#id1.1.id1)
    is designed. By contrast to semantic labeling, obtaining training pairs for [SVCN](#id15.15.id15)
    does not require any manual labeling. Moreover, local adversarial learning has
    been introduced for modeling the surface priors. The recovered 3D surfaces serve
    as a canonical domain, from which semantic labels can transfer across different
    [LiDAR](#id18.18.id18) sensors.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: A summary of unsupervised DA frameworks and their characteristics
    used in [3DPC](#id1.1.id1) understanding.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Work | SD | TD | [ML](#id5.5.id5) model | SoDTs | SoDDs | Pros and cons |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[116](#bib.bib116)] | PointSegDA | PointSegDA | DefRec | S | D | The first
    study of SSL for DA on [3DPCs](#id1.1.id1). |'
  prefs: []
  type: TYPE_TB
- en: '| [[142](#bib.bib142)] | UTD-MHAD | UTD-MHAD | N/A | S | S | Computationally
    efficient and enable human activity detection using depth maps |'
  prefs: []
  type: TYPE_TB
- en: '| [[143](#bib.bib143)] | ModelNet40 | ShapeNet | VDM-DA | S | D | Enable virtual
    domain modeling for source data-free DA. |'
  prefs: []
  type: TYPE_TB
- en: '| [[155](#bib.bib155)] | GTA-LiDAR | KITTI | SqueezesegV2 | S | D | Enhance
    model structure and UDA for road-object segmentation. |'
  prefs: []
  type: TYPE_TB
- en: '| [[156](#bib.bib156)] | Semantic (KITTI, POSS, USL) | Semantic (KITTI, POSS,
    USL) | LiDARNet | S | D | Preserve almost the same performance on the SD after
    adaptation and achieve 8%-22% mIoU performance increase in the TD. |'
  prefs: []
  type: TYPE_TB
- en: '| [[157](#bib.bib157)] | Waymo | KITTI; nuScenes | [SVCN](#id15.15.id15) |
    S | D | Provide 8.2-36.6% better performance than previous DA methods. |'
  prefs: []
  type: TYPE_TB
- en: '| [[158](#bib.bib158)] | pointDA-10 | pointDA-10 | pointDAN | S | S | 3DPC
    representation using a multi-scale 3D DA network. |'
  prefs: []
  type: TYPE_TB
- en: '| [[195](#bib.bib195)] | Waymo | KITTI, nuSenses, Lyft | ST3D | S | D | Exceed
    fully supervised results on KITTI 3D object detection benchmark. |'
  prefs: []
  type: TYPE_TB
- en: '| [[197](#bib.bib197)] | nuScenes | KITTI | SF-UDA | S | D | Enable [SFUDA](#id21.21.id21)
    but requires to be tested beyond cars, to detect other types of objects. |'
  prefs: []
  type: TYPE_TB
- en: '| [[198](#bib.bib198)] | PointDA-10; ScanObjectNN | ShapeNet, ModelNet40, ScanNet,
    ScanObjectNN | RefRec | S | D | Refine pseudolabels, offline and online, by leveraging
    shape descriptors learned to solve shape reconstruction on both domains |'
  prefs: []
  type: TYPE_TB
- en: '| [[228](#bib.bib228)] | KITTI | nuScenes | CrAF | S | D | Conduct more challenging
    cross-device adaptation. |'
  prefs: []
  type: TYPE_TB
- en: '| [[230](#bib.bib230)] | PreSIL; nuScenes | KITTI; nuScenes | SRDAN | S | D
    | Demosntrate the significance of geometric characteristics for cross-dataset
    3D object detection. |'
  prefs: []
  type: TYPE_TB
- en: '| [[232](#bib.bib232)] | labeled data, VirtualKITTi | Semantic KITTI | CLDA
    | D | D | Improve segmentation performance for rare classes but still needs more
    enhancement. |'
  prefs: []
  type: TYPE_TB
- en: '| [[233](#bib.bib233)] | PointDA-10 | PointDA-10 | BADM; PointDA-10 | S | S
    | Achieve state-of-the-art performance and outperform other 3D DA techniques but
    not appropriate for different tasks or different domains. |'
  prefs: []
  type: TYPE_TB
- en: '| [[234](#bib.bib234)] | bSSFP-MRI; MM-WHS | LGE-MRI; MRI and CT | UDA-GAN
    | D | D | Provide promising performance, compared to the state-of-the-art. |'
  prefs: []
  type: TYPE_TB
- en: '| [[235](#bib.bib235)] | Video data | LiDAR | PALMAR | S | D | 63% improvement
    of multi-person tTracking than state-of-the-art frameworks while maintaining efficient
    computation on the edge devices. |'
  prefs: []
  type: TYPE_TB
- en: '| [[236](#bib.bib236)] | KITTI object; near-range | KITTI object; far-range
    | SCNET | S | S | Achieve a remarkable improvement in the adaptation capabilities
    but without enable knowledge transfer to different tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| [[237](#bib.bib237)] | GTA-V | Oxford RobotCar | vLPD-Net R; vLPD-Net V |
    S | D | Achieve state-of-the-art performance on the real-world Oxford RobotCar
    dataset but the investigation of the loop closure and re-localization in real-world
    is needed. |'
  prefs: []
  type: TYPE_TB
- en: '| [[238](#bib.bib238)] | CadData | CamData | DANN; SSLPC;'
  prefs: []
  type: TYPE_NORMAL
- en: PoinDAN | S | D | Less time-consuming for implementation in production. |
  prefs: []
  type: TYPE_NORMAL
- en: '| [[239](#bib.bib239)] | PointDA-10 | PointDA-10 | DSDAN | S | S | Exceed the
    state-of-the-art performance of cross-dataset 3DPC recognition tasks. |'
  prefs: []
  type: TYPE_TB
- en: '| [[240](#bib.bib240)] | High resolution LiDar data | Low resolution LiDar
    data | LAMAR | S | D | 94% Human activity recognition performance in multiple-inhabitant
    scenario. |'
  prefs: []
  type: TYPE_TB
- en: '| [[241](#bib.bib241)] | A2D2 | Semantic KITTI | xMUDA | S | D | Enable [cross-modal
    learning DA](#id26.26.id26) ([CLDA](#id26.26.id26)) in 3D semantic segmentation;
    however, knowledge cannot be transferred across different ttasks. |'
  prefs: []
  type: TYPE_TB
- en: '| [[242](#bib.bib242)] | KITTI | MDLS | CycleGAN; YOLOv3 | S | D | Outperformed
    other compared baseline approaches with more than 39% improvement in F 1 -Measure
    score. |'
  prefs: []
  type: TYPE_TB
- en: 'Abbreviations: Same or different tasks (SoDTs); Same or different domains (SoDDs);
    Different (D); Same(S).'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Applications of TL-based 3DPCs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TL-based [3DPC](#id1.1.id1) can be applied in various applications, including
    robotics and autonomous systems, augmented reality and virtual reality, medical
    imaging, geo-spatial data analysis, industrial inspection, building information
    modeling, etc. In this section, we focus on some of the main applications that
    attract increasing research effort. Table LABEL:3dpc_tasks summarizes some of
    the pertinent [DTL](#id6.6.id6)-based [3DPC](#id1.1.id1) frameworks proposed to
    perform different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Semantic labeling and segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Semantic labeling of [3DPC](#id1.1.id1) is crucial for performing segmentation
    tasks to represent scans accurately, using manual as well as automatic labeling.
    For instance, recently, Xie et al. [[117](#bib.bib117)] have used the Stanford
    large-scale 3D indoor spaces (S3DIS) dataset for transferring knowledge to the
    target dataset. However, it contains a much smaller repository as compared to
    source dataset (ScanNet), and manual semantic labeling has been done with 13 categories.
    Arnold et al. [[133](#bib.bib133)] used simple [MLP](#id23.23.id23) to learn the
    diversity of the feature space and acquire knowledge for semantic labeling.
  prefs: []
  type: TYPE_NORMAL
- en: One of the prime concerns is to identify similar labels and different data from
    the same labels separately. To address this, authors [[243](#bib.bib243)] have
    suggested a domain-independent approach for semantic labeling. Their proposed
    model considers similarity metrics as features to infer the correct semantic labels
    and learns a similarity function to identify if attributes have the same labels.
    Because the matching function is unrelated to specific labels, their model does
    not depend upon the label and thus are independent of domain ontologies. A similar
    approach for bridge [3DPC](#id1.1.id1) has been suggested in [[244](#bib.bib244)].
    The authors have received an improved IoU with 94.29%.
  prefs: []
  type: TYPE_NORMAL
- en: The ability of [DTL](#id6.6.id6) to reduce the need for large-scale dataset
    can be economical and hence can be exploited in [3DPC](#id1.1.id1) tasks for performing
    accurately without requiring more extensive data. For example, with the help of
    [DTL](#id6.6.id6), authors in [[245](#bib.bib245)] have explored the analogy between
    real and synthetic data for semantic segmentation and tried reducing their gap.
    Based on deep [CNN](#id11.11.id11) (DCNN), authors in [[246](#bib.bib246)] have
    suggested a weakly-supervised semantic segmentation technique. Unlike others,
    they have considered auxiliary segmentation annotations for distinct categories
    to guide segmentation on pictures with only image-level class labels. They have
    used a decoupled encoder-decoder architecture with an attention model allowing
    segmentation knowledge to be transferable across categories.
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic segmentation is conducted in [[206](#bib.bib206)] by transferring
    knowledge from synthetic to real [LiDAR](#id18.18.id18)  [3DPCs](#id1.1.id1).
    Typically, a large-scale synthetic [LiDAR](#id18.18.id18) dataset, is first collected
    from multiple virtual environments with rich layouts and scenes. It includes point-wise
    labeled [3DPCs](#id1.1.id1) with accurate geometric shapes and comprehensive semantic
    classes. Moving on, a [3DPC](#id1.1.id1) translator (PCT) is designed to mitigate
    the discrepancy between real and synthetic [3DPCs](#id1.1.id1). Typically, the
    synthetic-to-real discrepancy is decomposed into a sparsity and an appearance
    component before separately handling them. Fig. [11](#S4.F11 "Figure 11 ‣ 4.1
    Semantic labeling and segmentation ‣ 4 Applications of TL-based 3DPCs ‣ Advancing
    3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey")
    explains the PCT approach adopted in [[206](#bib.bib206)], disentangling [3DPC](#id1.1.id1)
    translation into appearance and sparsity translation tasks. Accordingly, dense
    [3DPCs](#id1.1.id1) having similar appearances are first learned within the appearance
    translation. The sparsity translation then learns real sparsity distribution in
    2D space and fuses it with the reconstructed [3DPC](#id1.1.id1) in 3D space. Then,
    real sparsity distributions in 2D space are learned within the sparsity translation
    before being fused with the reconstructed [3DPC](#id1.1.id1) in 3D space.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9301d1642ff250e328177da445ff0369.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: The PCT method separates the task of translating [3DPC](#id1.1.id1)
    into two parts: appearance translation and sparsity translation. Using synthetic
    [3DPC](#id1.1.id1) as input, the appearance translation learns to create dense
    [3DPC](#id1.1.id1) that look similar to real ones. The sparsity translation then
    learns the typical sparsity patterns found in real 2D point clouds, and combines
    these patterns with the dense [3DPC](#id1.1.id1) in 3D space. The end result is
    a [3DPC](#id1.1.id1) that looks and is sparse similarly to real [3DPC](#id1.1.id1).'
  prefs: []
  type: TYPE_NORMAL
- en: Moving forward, A. Murtiyoso et al. [[129](#bib.bib129)] has used [DTL](#id6.6.id6)
    on a photogrammetric orthoimage by exploiting labeled and rectified images of
    building facades for training neural networks on them. Next, they allow the transition
    from 2D orthoimage to [3DPC](#id1.1.id1) by another program. With their promising
    results for photogrammetric data, their proposed work is a potential option to
    assist in automatic [3DPC](#id1.1.id1) semantic segmentation. Similarly, authors
    in [[171](#bib.bib171)] have claimed that semantic segmentation can perform equally
    competently to the latest [ML](#id5.5.id5) approaches in modeling cultural heritage.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, [3DPC](#id1.1.id1) instance segmentation is equally employed
    for many applications. For example, in [[131](#bib.bib131)], authors have taken
    a problem of height detection of a catenary conductor. For [3DPC](#id1.1.id1)
    instance segmentation, they have incorporated [DTL](#id6.6.id6) and 3D-BoNet model
    with a multiscale grouping (MSG) structure on a smaller dataset, with loss curve
    converging better for their model. In addition to its applications in building,
    tunneling, and construction, [DTL](#id6.6.id6) for [3DPC](#id1.1.id1) segmentation
    has found utility in robotics. Specifically, it is utilized for automatic toolpath
    generation to enable tasks such as masking, deburring, and polishing in various
    industrial applications. Z. Xie and his team [[137](#bib.bib137)] have utilized
    [3DPC](#id1.1.id1) segmentation and classification to extract the object features.
    Furthermore, they have exploited [DTL](#id6.6.id6) to enhance performance and
    avoid investing much time in training data. Their toolpath recommendation automatically
    suggests patterns to choose from and does not require manual efforts. To identify
    human activities, authors [[142](#bib.bib142)] have carried out segmentation to
    separate the human figure from the background so that the descriptor can only
    be determined for a human figure.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[3DPC](#id1.1.id1) classification is the process of classifying the features
    of 3D objects and categorizing the classes to which they belong. The [3DPC](#id1.1.id1)
    segmentation types and classification are illustrated in Fig. [12](#S4.F12 "Figure
    12 ‣ 4.2 Classification ‣ 4 Applications of TL-based 3DPCs ‣ Advancing 3D Point
    Cloud Understanding through Deep Transfer Learning: A Comprehensive Survey").
    In the diagram, it is shown how a building, with the help of [CNN](#id11.11.id11),
    can be classified based on different classifications e.g., concerning time (period),
    region, and structure. Recently, researchers have explored different ways of directly
    classifying [3DPC](#id1.1.id1), mainly to avoid information loss during the conversion
    of [3DPC](#id1.1.id1) into non-[3DPC](#id1.1.id1). Automated classification of
    point cloud data has great potential for various applications, but a limited number
    of labelled points can lead to overfitting and poor generalization in [ML](#id5.5.id5)
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: One of the pioneered works in the field is done by Qi et al. [[52](#bib.bib52)],
    reported being the first [DL](#id4.4.id4) model to classify the original [3DPC](#id1.1.id1)
    directly. Their PointNet model is based on [CNN](#id11.11.id11), and its success
    has attracted many works to follow their proposed architecture. [CNN](#id11.11.id11)
    has been widely used for classification tasks. For example, authors [[129](#bib.bib129)]
    have used [CNN](#id11.11.id11) with [DTL](#id6.6.id6) to classify improved fisher
    vectors used to represent [3DPC](#id1.1.id1). The work [[247](#bib.bib247)] introduces
    a method called Atrous XCRF that induces controlled noise by invoking conditional
    random field similarity penalties using nearby features to improve generalization.
    The method achieves high [OA](#id16.16.id16) and F1 score in a benchmark study
    using the ISPRS 3D labeling dataset and is on par with the current best model.
    Using [DTL](#id6.6.id6) with Bergen 2018 dataset shows improvement in accuracy,
    but more work is needed to address generalization issues in [DA](#id7.7.id7) and
    [DTL](#id6.6.id6). To overcome the requirement of large dataset, Lei et al. [[138](#bib.bib138)]
    have proposed a [3DPC](#id1.1.id1) classification method that integrates an improved
    [CNN](#id11.11.id11) into [DTL](#id6.6.id6). They have used the DensNet201 model
    as a pretrained model for finding deep features able to accurately characterize
    the object. In addition, The authors [[218](#bib.bib218)] have proposed an ALS
    [3DPC](#id1.1.id1) classification method based on [DTL](#id6.6.id6) using [CNN](#id11.11.id11).
    In [[138](#bib.bib138)], an ALS [3DPC](#id1.1.id1) classification approach for
    integrating an enhanced fully-[CNN](#id11.11.id11) into [DTL](#id6.6.id6) with
    multi-view and multiscale and deep features is proposed. Typically, the shallow
    features of the ALS [3DPC](#id1.1.id1), such as height, intensity, and curvature
    change, are extracted to generate feature maps by multiscale voxel and multi-view
    projection. Second, these feature maps are fed into the pretrained DenseNet201
    model to derive deep features. Experimental results show that [OA](#id16.16.id16)
    and the average F1 scores obtained by the proposed method are 89.84% and 83.62%,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/785d8654f41e3b6e8b0c5bbea9443c82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: The flow chart of the proposed classification method.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Point-set registration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the increasing number of 3D sensors and 3D data production, consolidating
    overlapping [3DPCs](#id1.1.id1), which is called registration, has become a significant
    issue in many applications [[248](#bib.bib248)]. Registration can be used online
    (e.g., in a [LiDAR](#id18.18.id18) SLAM pipeline for loop closure detection) or
    offline (e.g., in 3D reconstructions of RGB-D indoor scenes [[249](#bib.bib249)]
    or for outdoor [LiDAR](#id18.18.id18) map building for autonomous vehicles). However,
    [3DPC](#id1.1.id1) registration can be challenging with real-world scans because
    of noise, incomplete 3D scans, outliers, etc. There are numerous existing approaches
    to this issue; however, recently, [DL](#id4.4.id4) approaches have become very
    popular, especially for learning descriptors and computing transformations. These
    data-driven approaches, especially those that involve learning on large datasets
    with the ground truth pose as supervision, have effectively solved registration
    problems [[250](#bib.bib250), [237](#bib.bib237)]. Specifically, in [[237](#bib.bib237)],
    a registration-aided DA model for [3DPC](#id1.1.id1)-based place recognition is
    developed, called vLPD-Net. The method utilizes virtual large-scale point cloud
    descriptors and a structure-aware registration network, along with adversarial
    training, to minimize the gap between synthetic and real-world domains.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Scene understanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The method proposed by Xu et al. [[82](#bib.bib82)] uses [DTL](#id6.6.id6) technique
    by inflating weights to transform an image-pretrained model into a [3DPC](#id1.1.id1)
    model. Authors in [[251](#bib.bib251)] have proposed a model ’Shape Self-Correction’
    for [3DPC](#id1.1.id1) analysis where they have explored how unlabeled data can
    be utilized for a better [3DPC](#id1.1.id1) understanding. In addition, their
    approach can identify distorted points and automatically correct them by producing
    state-of-the-art results. Moreover, this model can be used as a pretrained model
    to be fine-tuned for related tasks. Going forward, Zhang et al. [[252](#bib.bib252)]
    also tried to exploit a pretrained large-scale 2D dataset to generalize it to
    [3DPC](#id1.1.id1) understanding using a [contrastive vision-language pretraining](#id24.24.id24)
    ([CLIP](#id24.24.id24)). Their model, PointCLIP, projects [3DPC](#id1.1.id1) into
    multi-view depth maps by encoding it, and successfully, view-wise zeroshot prediction
    is aggregated to transfer knowledge from 2D to 3D. Furthermore, many works exploit
    unsupervised approaches for [3DPC](#id1.1.id1) understanding. For example, Xie
    et al. [[117](#bib.bib117)] have suggested an unsupervised pretraining framework.
    To avoid the domain gap and utilize their approach across the different domains
    in the real world, they have directly made use of a complex network to pretrain
    the model. The major contribution they have is the examination of the transferability
    of learned representations generalization across the domain for [3DPC](#id1.1.id1)
    understanding. In addition, their architecture can also represent point-level
    information by capturing local features. To further exploit the geometric structure
    of the [3DPC](#id1.1.id1) and relationships of local features, Liu et al. [[253](#bib.bib253)]
    have suggested a convolution-based geometric-aware approach. Moreover, they proposed
    a filtering mechanism that identifies noise and outliers for a high-level understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Object detection and denoising
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The exploitation of [DTL](#id6.6.id6) for various applications of [3DPC](#id1.1.id1)
    in 3D object detection and recognition has recently attracted researchers, ranging
    from medical science to biological research and robotics. For instance, authors
    in [[145](#bib.bib145)] have used [DTL](#id6.6.id6) for protein structure indexing
    using [3DPC](#id1.1.id1). The suggested approach saves training time by fine-tuning
    the pretrained network, using Euclidean distance to extract shape-matching, with
    generic 3D objects. Furthermore, authors in [[131](#bib.bib131)] have proposed
    a detection method incorporating instance segmentation and [DTL](#id6.6.id6) for
    overhead catenary height detection of a tunnel using a 3D [3DPC](#id1.1.id1) tunnel
    dataset. To recognize human activities, Sidor et al. [[142](#bib.bib142)] have
    suggested an approach by transforming depth maps into [3DPC](#id1.1.id1). The
    authors have used [DTL](#id6.6.id6) combined with multiple networks to improve
    the classification mechanism based on BiLSTM. In [[143](#bib.bib143)], [3DPC](#id1.1.id1)
    semantic segmentation based on [DTL](#id6.6.id6) is used for 3D object detection.
    The advantage of this method is that it reduces large-scale training datasets
    requirement, and consequently, the training duration is reduced as [DTL](#id6.6.id6)
    exploits knowledge from prior classification tasks, aiding object detection after
    preprocessing, thereby promoting cross-task generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Denoising is an important [3DPC](#id1.1.id1) task to remove noise usually found
    in acquiring images from scanning devices. These noises can influence many downstream
    tasks such as segmentation, detection, reconstruction and classification, etc.
    To remove these noises at different scales of [3DPC](#id1.1.id1) data, several
    algorithms have been suggested. The first [DTL](#id6.6.id6)-based [3DPC](#id1.1.id1)
    denoising scheme was proposed in [[254](#bib.bib254)]. It involved denoising patches
    of points by projecting them onto a learned local frame and repositioning the
    points onto the surface using a [CNN](#id11.11.id11). Some interesting [3DPC](#id1.1.id1)
    denoising approaches include score-based [3DPC](#id1.1.id1) denoising [[255](#bib.bib255)],
    graph Laplacian regularization [[256](#bib.bib256)], bipartite graph approximation
    and total variation [[257](#bib.bib257)], weighted multi-projection [[258](#bib.bib258)],
    and feature graph learning [[259](#bib.bib259)], etc.
  prefs: []
  type: TYPE_NORMAL
- en: 4.6 Upsampling and donwsampling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Point clouds obtained through 3D scanning are often incomplete, unevenly distributed,
    and corrupted by noise. Upsampling, on the other hand, involves generating a dense
    set of points that not only restores uniformity and proximity to the surface but
    can also address small gaps in the data, all through a single network. For instance,
    authors have proposed Segnet [[260](#bib.bib260)], and they designed decoder to
    upsamples their input feature map(s) having lower resolution enabling non-linear
    upsampling to produce dense feature maps. In a similar way, Eltner et al. [[261](#bib.bib261)]
    have used an encoder that extracts a low-resolution activation map while the decoder
    upsamples it to obtain a pixel-wise classification. More recently, an upsampling
    method for [3DPC](#id1.1.id1) to reduce human annotation cost is presented in
    [[262](#bib.bib262)]. They relied solely on the upsampling operation to perform
    effective feature learning of [3DPCs](#id1.1.id1). Simialrly, Imad et al. [[130](#bib.bib130)]
    have added more convolutional layers to the decoder stage, which has been coupled
    with upsampling layers to increase the size of the spatial tensor and generate
    high-resolution segmentation outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Down-sampling point clouds refers to the process of reducing the number of points
    contained within them. This is commonly done to decrease the processing time required,
    or to select a specific number of points for use in training, among other purposes.Moreover,
    downsampling is also used to transform [3DPC](#id1.1.id1) into 2D representations.
    Typically, in [[263](#bib.bib263)], a 2D scene modelling method is described that
    effectively and meaningfully transforms the 3D data to a 2.5D representation and
    then to a 2D grid map. In addition to this, downsampling has been used in 3D data
    processing to remove the noise. For example, [[264](#bib.bib264)] have used growing
    neural gas (GNG) based downsampling technique for noise removal. However, downsampling
    necessitates careful consideration of whether the points are significant for the
    output, so that all important points are passed to the next layer and not removed.
    To this end, critical points layer (CPL) has been proposed in [[265](#bib.bib265)],
    an adaptive downsampling layer that learns to downsize an unordered [3DPC](#id1.1.id1)
    while keeping the crucial (critical) points. Downsampling has also been used in
    building and automation, for instance, a downsampling technique to optimize Terrestrial
    laser scanning (TLS) datset is suggested in [[266](#bib.bib266)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Summary of [DTL](#id6.6.id6)-based [3DPC](#id1.1.id1) frameworks describing
    different tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | MoN | Task/application | DTL | Limitations | Contribution/advantage/key
    point |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[117](#bib.bib117)] | ResNet-34, Pointcontrast | [3DPC](#id1.1.id1) understanding
    | FT | Testing for generalizability across multiple datasets and verifying computational
    costs after applying DTL were not conducted. | transferability of learned representation
    in [3DPCs](#id1.1.id1) to high-level scene understanding. |'
  prefs: []
  type: TYPE_TB
- en: '| [[128](#bib.bib128)] | UFF | Segmentation and classification | UDTL | they
    optimize the classifiers in a single stage, a multi-stage classifier that can
    reduce the redefined cost function could enhance the result. | Propose a learning
    system with an unsupervised feedforward feature (UFF) by incorporating encoder-decoder
    architecture. |'
  prefs: []
  type: TYPE_TB
- en: '| [[129](#bib.bib129)] | DeepLabv3+ | Semantic segmentation | ITL | Due to
    the terrestrial nature of the data acquisition, several parts of the orthophoto,
    notably blind spots were distorted by the orthorectification algorithm | An improved
    automated procedure for [3DPC](#id1.1.id1) semantic segmentation, especially for
    photogrammetric data. |'
  prefs: []
  type: TYPE_TB
- en: '| [[130](#bib.bib130)] | AE | [3DPC](#id1.1.id1) semantic segmentation for
    3D object detection | ITL | The method does not work for raw [3DPCs](#id1.1.id1).
    | A [3DPC](#id1.1.id1) is projected into a birds-eye-view by which the amount
    of annotated data and time required for training has been minimized |'
  prefs: []
  type: TYPE_TB
- en: '| [[131](#bib.bib131)] | 3D BoNet + MSG structure | [3DPC](#id1.1.id1) instance
    segmentation | ITL | The proposed RKT method is costly | New tunnel dataset is
    generated for [3DPC](#id1.1.id1) segmentation. |'
  prefs: []
  type: TYPE_TB
- en: '| [[133](#bib.bib133)] | ConvPoint network, MLP | [3DPC](#id1.1.id1) segmentation
    for classification | ITL | Semantic interpretation of objects with fewer visual
    features is missing | automation of extraction and labeling of memorial objects
    from cultural heritage sites using [3DPC](#id1.1.id1) data. |'
  prefs: []
  type: TYPE_TB
- en: '| [[138](#bib.bib138)] | DenseNet201 | [3DPC](#id1.1.id1) classification |
    TTL | Algorithms are not integrated efficiently, which makes the feature maps
    generation complex | This algorithm can quickly and accurately extract the features
    and reduce the effect of ground object size on classification results. |'
  prefs: []
  type: TYPE_TB
- en: '| [[140](#bib.bib140)] | RandLA-Net | [3DPC](#id1.1.id1) semantic segmentation
    | ITL | The SLAM algorithm is not efficient for indoor spaces, [LiDAR](#id18.18.id18)
    can be located to a limited height only, which provides a minimal view for acquired
    data | Robot dog has been exploited for data acquisition instead of human which
    saves time and minimize the monetary efforts. |'
  prefs: []
  type: TYPE_TB
- en: '| [[141](#bib.bib141)] | CNN | Classification | ITL | The [3DPC](#id1.1.id1)
    data is used instead of street-view images, which needs open space. Buildings
    with irregularities and reinforcing shapes can misclassify as soft-story buildings.
    | To accurately classify the soft-story building using CNN model pretrained with
    DTL |'
  prefs: []
  type: TYPE_TB
- en: '| [[142](#bib.bib142)] | BiLSTM | Segmentation and classification | TTL | eigenvalue-based
    descriptors for human activity recognition and other state-of-the-art methods
    are not discussed | With the help of [3DPCs](#id1.1.id1) and VFH descriptor, human
    activities are recognized. |'
  prefs: []
  type: TYPE_TB
- en: '| [[143](#bib.bib143)] | SFUDMA | Object recognition | UDA | The proposed Virtual
    Domain Modeling needs to be deployed for a variety of tasks. | They successfully
    achieve the goal of distribution alignment between the SD and TD by training deep
    networks without accessing the SD data |'
  prefs: []
  type: TYPE_TB
- en: '| [[144](#bib.bib144)] | DenesNet-121 | Depth estimation | ITL | Testing for
    generalizability across multiple datasets was not performed. | Depth and surface
    estimation with a higher resolution |'
  prefs: []
  type: TYPE_TB
- en: '| [[145](#bib.bib145)] | GLT4IP, pointNet | Protein structure indexing | FT
    | The validation of real-time performance improvements through transfer learning
    was not conducted. | DTL based indexing of protein structures using [3DPCs](#id1.1.id1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[146](#bib.bib146)] | Endo-SfMLearner | Depth estimation | UTL | Unresolved
    aspects: enhancing data adaptability, resolving issues, integrating with segmentation,
    abnormality detection, and classification tasks for improved performance. | Presented
    a new dataset for endoscopy, “EndoSLAM” |'
  prefs: []
  type: TYPE_TB
- en: '| [[148](#bib.bib148)] | GMM | Segmentation and classification | ITL | Further
    research into novel pretext tasks tailored specifically to the idiosyncrasies
    of 3D data has not been conducted. | For representation learning, the method does
    not require any procedure like transformation/data augmentation. |'
  prefs: []
  type: TYPE_TB
- en: '| [[149](#bib.bib149)] | POCO | 3D orientation, and classification | ITL |
    Orientation accuracy can be improved further | Experimentally shown how DTL can
    be used for a model to serve as a platform for 3D object’s orientation representation.
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[150](#bib.bib150)] | YOLO | Object detection | ITL | Method heavily relies
    on [LiDAR](#id18.18.id18) data only and may not be that beneficial for another
    domain of image dataset | Suggest a method efficient enough and compatible for
    any [LiDARs](#id18.18.id18), even containing a different number of channels. Moreover,
    they do not require a re-training step. |'
  prefs: []
  type: TYPE_TB
- en: '| [[151](#bib.bib151)] | GoogleNet, AlexNet, VGG | Classification | ITL | verification
    of the degree of dependence of the DNN architecture on the characteristics of
    the mechanical system subject to degradation | representation of punch deformation
    with depth and normal vector maps (DNVMs) obtained from 3D scan [3DPCs](#id1.1.id1)
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[221](#bib.bib221)] | Kd-network | Shape classification, feature recognition
    | ITL | This method not applicable for any cattle and may produce heavy mistakes
    and significant errors for adult Qinghai yaks | Livestock can be observed without
    any physical involvement of human |'
  prefs: []
  type: TYPE_TB
- en: '| [[260](#bib.bib260)] | Segnet | Upsampling | UDTL | This method needs to
    separate the roles between the optimizer and the model to achieve the desired
    outcome. | It is much smaller and can be trained end-to-end using stochastic gradient
    descent. It has been engineered to be effective for memory and processing time
    during inference. |'
  prefs: []
  type: TYPE_TB
- en: '| [[262](#bib.bib262)] | UAE | upsampling | SSTL | The complexity increased
    with the double-head architecture. Additionally, mechanical systems with different
    geometry and material from the punch tool were not investigated. | It allows classification
    and segmentation tasks to benefit from pretrained models. And it reduces the human
    annotation cost. |'
  prefs: []
  type: TYPE_TB
- en: '| [[265](#bib.bib265)] | CPL | downsampling | DA | The complexity of CP-Net
    is high, potentially leading to increased computation costs. | an adaptive downsampling
    layer that learns to downsize an unordered [3DPC](#id1.1.id1) while keeping the
    crucial (critical) points |'
  prefs: []
  type: TYPE_TB
- en: '| [[254](#bib.bib254)] | PointProNets | denoising | S | The [3DPC](#id1.1.id1)
    data utilized in the method was created by adding random noise; hence it is ineffective
    for [3DPCs](#id1.1.id1) that were legitimately created using [LiDAR](#id18.18.id18).
    | The first TL based [3DPC](#id1.1.id1) denoising scheme. |'
  prefs: []
  type: TYPE_TB
- en: 'Abbreviations: Unsupervised TL (UTL); supervised DTL (STL); self- supervised
    DTL (SSTL); supervised (S); upsampling auto-encoder (UAE); model or network (MoN);
    fine-tuning (FT).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.7 Scene generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3D point cloud scene generation or reconstruction is of paramount importance
    in various fields, including robotics, autonomous driving, virtual reality, and
    architecture. By capturing the precise three-dimensional coordinates of a scene,
    3D point clouds provide a detailed and accurate representation of the environment.
    This capability is crucial for tasks such as object recognition, navigation, and
    interaction within a space, enabling robots and autonomous vehicles to perceive
    and understand their surroundings effectively. In architecture and construction,
    3D reconstruction facilitates accurate modeling and analysis of structures, aiding
    in design, inspection, and renovation processes. Furthermore, in virtual reality
    and gaming, 3D point cloud generation enhances the realism and immersion of digital
    environments, providing users with a more engaging and interactive experience.
    The ability to create detailed and accurate 3D models from point clouds revolutionizes
    various industries by improving precision, efficiency, and safety in numerous
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: The SGFormer study [[267](#bib.bib267)] introduces a Semantic Graph Transformer
    for point cloud-based 3D scene graph generation, addressing the limitations of
    graph convolutional networks (GCNs) by using Transformer layers for global information
    passing. The model includes graph embedding and semantic injection layers to enhance
    object visual features with linguistic knowledge from large-scale language models.
    Benchmarked on the 3DSSG dataset, SGFormer demonstrates significant improvements
    in relationship prediction over state-of-the-art methods. The Sat2Scene study
    [[268](#bib.bib268)] proposes a novel architecture for direct 3D scene generation
    from satellite imagery using diffusion models and neural rendering techniques.
    This method generates texture colors at the point level and transforms them into
    a scene representation for rendering arbitrary views, showing proficiency in generating
    photo-realistic street-view image sequences and cross-view urban scenes, validated
    through experiments on city-scale datasets.
  prefs: []
  type: TYPE_NORMAL
- en: The ART3D study [[269](#bib.bib269)] introduces a framework combining diffusion
    models and 3D Gaussian splatting for 3D artistic scene generation, utilizing depth
    information and initial artistic images to generate point cloud maps and enhance
    3D scene consistency. ART3D shows superior performance in content and structural
    consistency metrics compared to existing methods. Liu et al. [[16](#bib.bib16)]
    propose a high-precision 3D building model generation method using multi-source
    3D data fusion, achieving state-of-the-art performance in multi-source 3D data
    quality evaluation and large-scale, high-precision building model generation.
    Chung et al. [[270](#bib.bib270)] present LucidDreamer, a domain-free 3D scene
    generation pipeline leveraging large-scale diffusion-based generative models,
    producing highly detailed Gaussian splats and outperforming previous methods in
    reconstruction quality. The S2HGrasp study [[271](#bib.bib271)] explores generating
    human grasps from single-view scene point clouds, addressing challenges of incomplete
    object point clouds and scene points, showcasing strong generalization capabilities
    and effective grasp generation. Lastly, the SGRec3D study [[272](#bib.bib272)]
    presents a self-supervised pre-training method for 3D scene graph prediction,
    achieving significant improvements in 3D scene graph prediction with reduced labeled
    data requirements during fine-tuning, demonstrating state-of-the-art performance.
    In [[273](#bib.bib273)], Liu et al. introduce the Pyramid Discrete Diffusion model
    (PDD), a framework employing scale-varied diffusion models to generate high-quality
    large-scale 3D scenes. Using a coarse-to-fine paradigm, PDD addresses the complexity
    and size challenges of 3D scenery data, particularly for outdoor scenes. The model
    demonstrates effective generation capabilities and data compatibility, allowing
    easy fine-tuning across different datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [8](#S4.T8 "Table 8 ‣ 4.7 Scene generation ‣ 4 Applications of TL-based
    3DPCs ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning:
    A Comprehensive Survey") compares various studies on 3D scene generation and reconstruction,
    highlighting key aspects such as models used, datasets, contributions, performance
    values, and limitations. The models range from SGFormer, utilizing a Semantic
    Graph Transformer, to MS3DQE-Net for multi-source 3D data fusion, and Pyramid
    Discrete Diffusion (PDD) for large-scale scene generation. Datasets include 3DSSG,
    city-scale datasets from satellite imagery, artistic images with depth information,
    and MLS 3D point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each study offers unique contributions: SGFormer excels in relationship prediction,
    Sat2Scene integrates diffusion models for urban scenes, and ART3D bridges artistic
    and realistic images. MS3DQE-Net focuses on high-precision building models, LucidDreamer
    achieves domain-free scene generation, S2HGrasp generates human grasps from incomplete
    point clouds, and SGRec3D enhances 3D scene graph prediction. Performance metrics
    show significant improvements, such as 40.94% R@50 for SGFormer, FID of 71.98
    for Sat2Scene, and PSNR of 34.24 for LucidDreamer. Limitations include over-smoothing
    in GCNs, handling significant view changes, and managing the complexity of 3D
    data. These challenges highlight areas for future research in 3D scene generation
    and reconstruction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparison of 3D Scene Generation/Reconstruction Studies'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | Model(s) Used | Dataset/Data Type | Main Contribution | Best Performance
    Value | Limitation |'
  prefs: []
  type: TYPE_TB
- en: '| [[267](#bib.bib267)] | SGFormer | 3DSSG | Semantic Graph TransFormer for
    3D scene graph generation | 40.94% R@50, 88.36% boost in complex scenes | Over-smoothing
    in GCNs |'
  prefs: []
  type: TYPE_TB
- en: '| [[268](#bib.bib268)] | Diffusion Models, Neural Rendering | City-scale datasets,
    satellite imagery | Direct 3D scene generation from satellite imagery | FID: 71.98,
    KID: 5.91 | Handling significant view changes and scene scale |'
  prefs: []
  type: TYPE_TB
- en: '| [[269](#bib.bib269)] | Diffusion Models, 3D Gaussian Splatting | Artistic
    images, depth information | 3D artistic scene generation | PSNR: 24.041, SSIM:
    0.863, LPIPS: 0.214 | Bridging artistic and realistic images |'
  prefs: []
  type: TYPE_TB
- en: '| [[16](#bib.bib16)] | MS3DQE-Net, Deep Learning | MLS 3D point clouds, 3D
    mesh data | High-precision 3D building model generation | State-of-the-art performance
    in data quality evaluation | Balancing local accuracy and overall integrity |'
  prefs: []
  type: TYPE_TB
- en: '| [[270](#bib.bib270)] | Diffusion-based Generative Model | Various datasets,
    VR content | Domain-free 3D scene generation | PSNR: 34.24, SSIM: 0.9781, LPIPS:
    0.0164 | Limited by training on 3D scan datasets |'
  prefs: []
  type: TYPE_TB
- en: '| [[271](#bib.bib271)] | S2HGrasp (Global Perception, DiffuGrasp) | S2HGD dataset,
    single-view scene point clouds | Generating human grasps from single-view point
    clouds | Contact Ratio: 99.41%, Volume: 6.58cm³ | Handling incomplete object point
    clouds |'
  prefs: []
  type: TYPE_TB
- en: '| [[272](#bib.bib272)] | SGRec3D | Various 3D scene understanding datasets
    | Self-supervised pre-training for 3D scene graph prediction | +10% on object
    prediction, +4% on relationship prediction | Requires object-level and relationship
    labels |'
  prefs: []
  type: TYPE_TB
- en: '| [[273](#bib.bib273)] | Pyramid Discrete Diffusion (PDD) | Large-scale 3D
    scenes, outdoor scenes | Coarse-to-fine 3D scene generation | mIoU: 68.0, MA:
    85.7, F3D: 0.20 | Complexity and size of 3D scenery data |'
  prefs: []
  type: TYPE_TB
- en: 5 Open Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[DTL](#id6.6.id6) and DA can be effective techniques for [3DPC](#id1.1.id1)
    understanding, but they also come with their own set of challenges. Typically,
    [DL](#id4.4.id4) models require large amounts of labeled data to achieve high
    accuracy. However, in the case of [3DPC](#id1.1.id1) understanding, it can be
    challenging to obtain large amounts of labeled data due to the time-consuming
    and expensive process of manually annotating point clouds [[274](#bib.bib274)].
    Additionally, [3DPC](#id1.1.id1) data can be highly variable in terms of size,
    shape, and orientation. This variability can make it challenging to develop [DTL](#id6.6.id6)
    and DA models that can generalize well to new datasets [[275](#bib.bib275), [276](#bib.bib276)].
    Moreover, DA is needed when the distribution of the source and [TDs](#id9.9.id9)
    is different. In the context of [3DPC](#id1.1.id1) understanding, domain shift
    can occur due to changes in sensor modalities, lighting conditions, and other
    environmental factors. To that end, DA techniques must be used to adapt the model
    to the [TD](#id9.9.id9) and prevent performance degradation. Moving on, [3DPC](#id1.1.id1)
    data can contain complex geometric structures, such as non-uniformly distributed
    points, non-planar surfaces, and varying levels of detail. These complexities
    can make it challenging to develop effective [DL](#id4.4.id4) models that can
    accurately capture the structure and features of the data [[274](#bib.bib274)].
    [DTL](#id6.6.id6) and DA models can be computationally expensive, especially when
    dealing with large [3DPC](#id1.1.id1) datasets. This can limit their practical
    applicability and require specialized hardware to achieve acceptable performance
    [[277](#bib.bib277), [278](#bib.bib278)].'
  prefs: []
  type: TYPE_NORMAL
- en: DA methods have shown significant improvements in various [ML](#id5.5.id5) and
    [CV](#id3.3.id3) tasks, such as classification, detection, and segmentation. However,
    to date, there are only a few methods that have been successful in applying DA
    directly to [3DPC](#id1.1.id1) data. The unique challenge in working with [3DPC](#id1.1.id1)
    data is its large amount of spatial geometric information, and the object’s semantics
    which depend on the regional geometric structures. This means that general-purpose
    DA methods, which focus on global feature alignment and neglect local geometric
    information, are not effective for aligning 3D domains, as stated in [[158](#bib.bib158)].
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 The problem of negative transfer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Negative transfer refers to a situation when knowledge transfer can negatively
    influence the model’s accuracy, possibly because the [SD](#id10.10.id10) and [TD](#id9.9.id9)
    are significantly different or they are supposed to perform different tasks [[215](#bib.bib215)].
    Moreover, not taking adequate advantage of the relationship between the [SD](#id10.10.id10)
    and [TD](#id9.9.id9) can also lead to negative transfer [[151](#bib.bib151)].
    In such situations, random and forced knowledge transfer without identifying the
    specific knowledge that can be useful and without considering "what", "when" and
    "how" to transfer can further propel negative transfer [[37](#bib.bib37)]. Hence,
    it can be inferred that high affinity among source and target tasks is of great
    importance for a successful [DTL](#id6.6.id6) and to avoid the negative transfer.
    To this end, researchers have suggested clustering methods for tasks and identifying
    similar tasks by keeping them in the same cluster. In addition to this, an approach
    to group learning tasks together is suggested in [[279](#bib.bib279)]. This approach
    enables what and when to transfer by identifying tasks within a group. Similarly,
    imbalanced training samples can increase negative transfer chances in a [UDA](#id8.8.id8)
    scenario. For Example, for [3DPC](#id1.1.id1) representation, Qin et al. [[158](#bib.bib158)]
    have suggested a 3D DA network and have argued that for domain alignment, unique
    parts can induce negative transfer and hence covering standard features in 3D
    space can help to avoid the negative transfer. Furthermore, to address negative
    transfer while dealing with different domains/modalities for [3DPC](#id1.1.id1),
    Gong et al. [[280](#bib.bib280)] have suggested a multi-[SD](#id10.10.id10) adaptation
    and label unification approach. They have used attention-guided adversarial alignment
    for a strong distribution alignment between the [SD](#id10.10.id10) and [TD](#id9.9.id9).
    Additionally, their proposed uncertainty maximization module limits the self-assured
    predictions for unlabeled samples in the [SDs](#id10.10.id10). To make their approach
    more promising, they have introduced a fusion module based on pseudo-labeling.
    Especially for unlabeled samples, they perform pseudo-labeling for the [SDs](#id10.10.id10)
    and all samples in the [TD](#id9.9.id9).
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 The problem of overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The established successful model, such as PointCNN and other DNN models, despite
    their widespread use and success for [3DPC](#id1.1.id1) tasks, still face some
    potential challenges, especially when the dataset available is relatively small.
    One possible reason for this is significant parameters, even in several million.
    It requires a large amount of data to fit this many parameters adequately. Since
    [3DPC](#id1.1.id1) data suffer from the scarcity of labeled data, this limitation
    makes [3DPC](#id1.1.id1) tasks prone to overfitting, and poor generalization [[247](#bib.bib247)].
  prefs: []
  type: TYPE_NORMAL
- en: Several mechanisms help in tackling the overfitting issue. For instance, [[221](#bib.bib221)]
    have used [DTL](#id6.6.id6) on a Kd-network, they have frozen the weights of all
    network layers except the last fully connected layer, and only the fully connected
    layer is modified so that the gradients during backpropagation are not calculated,
    which can avoid the occurrence of overfitting and improve training efficiency
    [[281](#bib.bib281)]. Entropy minimization (EM), a regularization method, has
    also been suggested by [[234](#bib.bib234)] in addressing overfitting. EM helps
    improve generalization and, hence, robustness for preventing overfitting. Arief
    et al. proposed a solution to address the overfitting problem of PointCNN, a DNN-based
    model, by using the Atrous XCRF model, which is a combination of [conditional
    random field](#id29.29.id29) ([CRF](#id29.29.id29)) and [recurrent neural network](#id28.28.id28)
    ([RNN](#id28.28.id28)) [[247](#bib.bib247)]. DNN-based models such as PointCNN
    are highly susceptible to overfitting when the available dataset is relatively
    small, this is because of the large number of parameters in such models which
    requires a large number of training data. To tackle this problem, they introduced
    controlled noise during the training of a DNN-classifier, the method works by
    retraining a validated model using unlabeled test data. The training process is
    guided using the hierarchical structure of the [CRF](#id29.29.id29) penalty procedure.
    With the proposed algorithm XCRF and addition of A-XCRF layer, they were able
    to improve the model’s accuracy by utilizing the unlabelled data.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Domain discrepancies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, [UDA](#id8.8.id8) has gained a good deal of attraction by the research
    community, especially for various complex [3DPC](#id1.1.id1) tasks like semantic
    segmentation [[140](#bib.bib140)] and object recognition [[143](#bib.bib143)].
    However, domain discrepancy exists due to the domain shift  [[282](#bib.bib282)];
    hence, it becomes difficult for a model to perform across domains accurately.
    Different solutions have been proposed to minimize the domain discrepancy for
    [3DPC](#id1.1.id1) tasks. To this end, [DL](#id4.4.id4) methods based on the adversarial
    network, such as domain adversarial neural networks [18], have improved performance
    for [UDA](#id8.8.id8) tasks. The primary job of bridging the domain gap is performed
    by introducing a generator that keeps fooling the discriminator until it identifies
    the discrepancies [[238](#bib.bib238)]. Moving forward, for classification purposes,
    a model is presented in [[158](#bib.bib158)] that focuses on aligning the local
    and global features. Similarly, for 3D segmentation on [LiDAR](#id18.18.id18)
    sensors, authors in [[155](#bib.bib155)] have considered aligning activation correlation
    [[282](#bib.bib282)]. The out space alignment and entropy minimization has been
    integrated by the authors in [[234](#bib.bib234)] for addressing the above [UDA](#id8.8.id8)
    issues. Domain discrepancies that can arise due to [LiDAR](#id18.18.id18) sensors
    are handled by Yi et al. [[157](#bib.bib157)]. Accordingly, a [SSL](#id14.14.id14)
    framework to improve [UDA](#id8.8.id8) performance has been suggested in [[116](#bib.bib116)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The proliferation in related research has paved the way for an increased interest
    in domain-invariant representations (DiR). The primary aim of the DiR is to facilitate
    a way that there should be insignificant discrepancies for features coming from
    different domains. To this end, Jaritz et al. [[241](#bib.bib241)] have leveraged
    the cross-modal discrepancies while preserving the best performance of each sensor
    from self-driving data from cameras and [LiDAR](#id18.18.id18)  [3DPC](#id1.1.id1)
    sensors because domain gaps are different for different sensors. For example,
    comparing a [LiDAR](#id18.18.id18) and a camera, the former is more robust to
    lighting changes with respect to the latter. Whereas there are always dense images
    that come as output from a camera while [LiDAR](#id18.18.id18) sensing density
    is proportional to the sensor setup. Notably, ’cross-modality’ differ from multi-modal
    fusion [[241](#bib.bib241)] as multi-modal implies training a single model in
    a supervised way for combining inputs like [LiDAR](#id18.18.id18) and camera [[283](#bib.bib283)],
    RGB-D [[284](#bib.bib284)], etc. This cross-modality DiR can help avoid the limitations
    of [UDA](#id8.8.id8) across modalities in which one modality influences the performance
    of the other modality in a negative way. In Fig. [13](#S5.F13 "Figure 13 ‣ 5.3
    Domain discrepancies ‣ 5 Open Challenges ‣ Advancing 3D Point Cloud Understanding
    through Deep Transfer Learning: A Comprehensive Survey"), an overview of [CLDA](#id26.26.id26)
    is presented. Here, a prediction of 3D segmentation labels is performed through
    2D and a 3D network by providing an image and a [3DPC](#id1.1.id1) as input to
    this network. 3D predictions are converted to 3D while consistency is ensured
    through mutual mimicking. This approach is proved to be advantageous for [UDA](#id8.8.id8).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d868aca49723304d58e1e2422d310d46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The [CLDA](#id26.26.id26) proposed in [[241](#bib.bib241)], where
    consistency has been ensured through mutual mimicking.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Reproducibility of scientific results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite the growing interest in using [DTL](#id6.6.id6) for various energy applications,
    there are still several factors that hinder the widespread adoption of [DTL](#id6.6.id6)-based
    models and impact reproducibility, and the ability to compare [DTL](#id6.6.id6)-based
    solutions. Firstly, it is difficult to evaluate the generality of [DTL](#id6.6.id6)
    models as most of the frameworks are evaluated on datasets collected under similar
    conditions, such as the same climate [[285](#bib.bib285)]. Secondly, there is
    a lack of consistency in using the same datasets and benchmarks to validate new
    [DTL](#id6.6.id6) models, due to the limited availability of open-source, benchmarked
    datasets [[286](#bib.bib286)]. Finally, different metrics and parameter settings
    have been used to measure the distance between the source and [TD](#id9.9.id9)s
    and evaluate the performance of [DTL](#id6.6.id6)-based solutions on different
    datasets. This makes it challenging, and even impossible, to compare [DTL](#id6.6.id6)
    techniques uniformly [[287](#bib.bib287)]. Despite that, there are some studies
    already uploaded on GitHub and other online platforms to facilitate the reproducibility
    and comparison tasks; the effort put in this direction still needs to consider
    the challenges introduced by [DTL](#id6.6.id6) for [3DPC](#id1.1.id1)s.
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Measuring knowledge gains
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Assessing the knowledge gained when using [DTL](#id6.6.id6) models for specific
    tasks is crucial, yet this challenge has not been fully addressed in the literature.
    Bengio et al. in [[288](#bib.bib288)] proposed four measures, transfer error,
    transfer loss, transfer ratio, and in-domain ratio, to quantify the gain of knowledge
    in [DTL](#id6.6.id6). However, it is unclear how these measures would perform
    with other [DTL](#id6.6.id6)-based methods, particularly in the energy sector,
    where the class sets are different between problems. Moreover, these measures
    can lead to non-definite performance evaluations if a perfect baseline model is
    achieved. To address these issues, simpler measures such as accuracy, F1 score,
    MSE, RMSE, MAE, performance improvement ratio (PIR) or other statistically-inspired
    coefficients have been widely used to evaluate [DTL](#id6.6.id6)-based solutions
    in the energy sector. These measures provide additional information, such as class
    agreement, and are better suited to the specific requirements of the energy sector.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Unification of DTL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The evolution of [DTL](#id6.6.id6) in energy applications has been rapid and
    innovative but also rather fragmented. This is due to the range of unique mathematical
    formulations utilized to delineate [DTL](#id6.6.id6) algorithms. Numerous researchers
    have developed and proposed various versions of [DTL](#id6.6.id6), each bearing
    its unique label and implementation approach. Examples of these include "Heterogeneous
    [DTL](#id6.6.id6)" as proposed in [[289](#bib.bib289)], "Statistical investigations"
    in [[290](#bib.bib290)], and "DA-DTL" in [[291](#bib.bib291)]. This multiplicity
    of terms and methods can often lead to confusion among scholars in the field.
    It is clear that there is a pressing need to homogenize the terminologies and
    conceptual underpinnings of [DTL](#id6.6.id6) to foster better comprehension and
    collaboration within the research community. A seminal step towards this unification
    was taken in [[292](#bib.bib292)], laying a foundation upon which future studies
    can build. However, the exploration remains incomplete with respect to [3DPC](#id1.1.id1)
    applications in particular.
  prefs: []
  type: TYPE_NORMAL
- en: The unification of [DTL](#id6.6.id6) concepts entails collating, comparing,
    and reconciling the differing methodologies, with an ultimate aim to develop a
    more cohesive and comprehensive framework. A unified approach would facilitate
    the application of [DTL](#id6.6.id6) in [3DPC](#id1.1.id1) applications by standardizing
    algorithm descriptions, mitigating confusion, and making it easier for researchers
    to implement, adapt, and build upon existing algorithms. This, in turn, would
    expedite the progression of [DTL](#id6.6.id6)-based solutions, thereby accelerating
    advancements in the realm of energy applications. Therefore, it is paramount to
    expand the unification efforts initiated in [[292](#bib.bib292)] to include [3DPC](#id1.1.id1)
    applications. Future research endeavors should prioritize developing a harmonized
    approach that accommodates the diverse range of [DTL](#id6.6.id6) methodologies
    while also meeting the specific needs of [3DPC](#id1.1.id1) applications. This
    would not only streamline the use of [DTL](#id6.6.id6) in the field but also pave
    the way for more innovative developments in energy applications.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future Research directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 3DPC Transformers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transformer models have greatly improved NLP [[293](#bib.bib293)] and CV, but
    their application in [3DPC](#id1.1.id1) processing remains uncertain. Questions
    remain about their ability to handle irregular and unordered data in 3D, their
    suitability for different 3D representations and their competence in various 3D
    processing tasks. Fig. [14](#S6.F14 "Figure 14 ‣ 6.1 3DPC Transformers ‣ 6 Future
    Research directions ‣ Advancing 3D Point Cloud Understanding through Deep Transfer
    Learning: A Comprehensive Survey") illustrates an example of a Transformer encoder
    architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ecc17235b883408ab9b39fcb097a2b4d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Example of a Transformer’s encoder architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One important challenge when using transformers is that they require large
    amounts of training data. In the field of [3DPC](#id1.1.id1), obtaining large
    datasets is a challenge and this makes training Transformers for 3D tasks difficult.
    The authors in [[294](#bib.bib294)] have investigated the use of knowledge from
    multiple images for [3DPC](#id1.1.id1) understanding. They proposed a pipeline
    called Pix4Point that allows for utilizing pretrained Transformers in the image
    domain to improve downstream [3DPC](#id1.1.id1) tasks. This is achieved by using
    a modality-agnostic pure Transformer backbone with tokenizer and decoder layers
    specialized in the 3D domain. Another model named “Point-BERT” which is based
    on BERT’s architecture [[295](#bib.bib295)] to learn Transformers for generalizing
    the idea of BERT to [3DPC](#id1.1.id1) is proposed in [[119](#bib.bib119)]. Point-BERT
    is trained using a masked point modeling task. The pipeline of Point-BERT is illustrated
    in Fig. [15](#S6.F15 "Figure 15 ‣ 6.1 3DPC Transformers ‣ 6 Future Research directions
    ‣ Advancing 3D Point Cloud Understanding through Deep Transfer Learning: A Comprehensive
    Survey"), where the input [3DPC](#id1.1.id1) is first partitioned into several
    point patches.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3b9ea751190868a3d5538ad886343ce6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: An illustration of CNN based DTL for [3DPC](#id1.1.id1) segmentation
    and its types'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Further generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Enhancing the generalization capabilities of [DTL](#id6.6.id6) algorithms for
    [3DPC](#id1.1.id1) understanding involves some unique challenges and strategies
    due to the distinctive nature of 3D data. For instance, data augmentation techniques
    for 3D data can be more complex than for images or text. Techniques could include
    rotations, translations, adding Gaussian noise, or mirroring the point cloud along
    a plane. This would create a more diverse training dataset and can help the model
    generalize better. In order to reduce the domain shift between the source and
    target task, techniques such as feature alignment can be used. This involves minimizing
    the difference between the distributions of the source and target features. Moreover,
    utilizing architectures that maintain geometric invariance can be helpful as [3DPC](#id1.1.id1)
    data can be rotated or scaled. Networks, such as PointNet or PointNet++ which
    are invariant to permutations and transformations of the input points can be utilized.
    Additionally, methods such as PointNet++ use hierarchical neural networks to capture
    local structures induced by the metric space points live in, and also model global
    structures. This can help with generalization as it learns at different scales.
    Moving on, regularization methods such as dropout or weight decay can be implemented
    to prevent overfitting and ensure the model generalizes better. Lastly, designing
    loss functions that better capture the properties of 3D data can also help with
    generalization.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Multi-task learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.3.1 Segmentation and Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section focuses on frameworks that handle multiple tasks like segmentation,
    detection, and sometimes additional tasks like classification or instance identification
    within point clouds. Typically, Pham et al. 2019 [[296](#bib.bib296)] and Zou
    et al. 2022 [[297](#bib.bib297)] both explore multi-task learning frameworks that
    handle segmentation and classification simultaneously. Pham et al. focus on combining
    semantic and instance segmentation, while Zou et al. integrate classification
    and segmentation tasks to enhance each other, utilizing a Y-shaped graph neural
    network. Moving on, Chen et al. 2022 [[298](#bib.bib298)] propose a method for
    joint semantic and instance segmentation using a shared encoder and dual decoders,
    focusing on enhancing feature representation through cross-task interactions.
    similarly, Ye et al. 2023 [[299](#bib.bib299)] extend the multi-task approach
    to LiDAR-based perception tasks, unifying object detection, semantic segmentation,
    and panoptic segmentation in a single network to optimize performance across tasks.
    These studies collectively push the boundaries in efficient multi-task handling,
    demonstrating that integrating tasks can improve overall performance and reduce
    computational costs by sharing learned features across tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 Advanced Architectures for Enhanced Feature Extraction
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: MTL can be used for developing sophisticated neural network architectures to
    better handle the complexities of point cloud data. In this regard, Dubey et al.
    2022 [[300](#bib.bib300)] introduce an attention-based architecture for radar
    point cloud data, focusing on enhancing target localization and classification
    through an anchor-free network design. Similarly, Hassani et al. 2019 [[301](#bib.bib301)]
    leverage an unsupervised multi-task model to learn point and shape features effectively,
    employing a graph-based encoder that tackles tasks like clustering and reconstruction
    simultaneously. Moving forward, Lin et al. 2022 [[302](#bib.bib302)] use a multi-task
    learning framework to improve semantic segmentation efficiency in Mobile Laser
    Scanning point clouds by integrating color prediction as an auxiliary task.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 Specific Enhancements
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Many studies have focused on applying multi-task learning to achieve specific
    enhancements in processing or understanding point cloud data. In this direction,
    Zhao et al. 2024 [[303](#bib.bib303)] develop a robust network for preprocessing
    LiDAR data, tackling denoising, segmentation, and completion tasks within a shared
    framework to improve data quality. Similarly, Rios et al. 2021 [[304](#bib.bib304)]
    employ a multi-task approach in an evolutionary optimization context, using a
    3DPC autoencoder to unify different design representations in a common latent
    space. Feng et al. 2021 [[305](#bib.bib305)] propose a multi-task network to handle
    various perception tasks for autonomous driving, demonstrating how a unified approach
    can enhance both detection and road understanding tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, the study in [[306](#bib.bib306)] introduces FFT-RadNet, a novel HD
    radar sensing model that optimizes the computation of angular positions from radar
    data, facilitating vehicle detection and free driving space segmentation. Moving
    on, Shan et al. 2023 [[307](#bib.bib307)] propose a no-reference point cloud quality
    assessment metric, GPA-Net, which utilizes a multi-task framework to enhance the
    accuracy of quality regression by also predicting distortion type and degree.
    Besides, the authors in [[308](#bib.bib308)] present Point-TTA, a test-time adaptation
    framework for point cloud registration that improves model generalization and
    performance on unknown testing environments through self-supervised auxiliary
    tasks. On the other hand, Zhang et al. 2022 [[309](#bib.bib309)] - The study proposes
    an improved multi-task network for roof plane segmentation from airborne laser
    scanning point clouds, which effectively segments and identifies roof planes.
    Wei et al. 2021 [[310](#bib.bib310)] - This paper proposes a multi-task network
    for 3D keypoint detection, which efficiently captures local and global features
    for accurate keypoint localization and semantic labeling. Lastly, the work in
    [[311](#bib.bib311)] introduces a multi-task network for machining feature recognition
    from point cloud data, aiming to accurately segment and identify complex machining
    features.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Cross-Modal Transfer Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.4.1 Enhancing 3DPC Understanding Through 2D-3D Correspondences
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Many studies employ contrastive learning and knowledge distillation strategies
    to align and enhance features between 2D images and 3DPCs. These methods focus
    on leveraging the rich textual and visual information available in 2D data to
    augment the spatial understanding provided by 3DPCs, significantly boosting performance
    in tasks such as classification, segmentation, and dense captioning. For instance,
    Afham et al. [[312](#bib.bib312)] introduce CrossPoint, utilizing self-supervised
    contrastive learning to map 2D image features to 3DPCs for better object classification
    and segmentation. The PointCMT, proposed in Yan et al. [[313](#bib.bib313)], adopts
    a teacher-student framework for cross-modal training, using 2D images to enhance
    3DPC classification through knowledge distillation. Wu et al. [[314](#bib.bib314)]
    propose CrossNet, a method that aligns 3DPC features with both colored and grayscale
    images to enhance classification and segmentation tasks across different domains.
    Group 2: Cross-Modal and Cross-Domain Adaptation for Comprehensive 3D Understanding'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.2 Comprehensive 3D Understanding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CMTL can be employed in various works to tackle the challenge of transferring
    knowledge across not just modalities but also different domains, aiming to improve
    the performance of 3D perception tasks under varying conditions and datasets,
    without relying heavily on labeled data. Typically, Zhang et al. [[315](#bib.bib315)]
    develop a novel adaptation strategy that aligns features between images and point
    clouds to enhance semantic segmentation without requiring 3D labels. The X4D-SceneFormer,
    introduced by Jing et al. [[316](#bib.bib316)], uses a dual-branch Transformer
    architecture to transfer dynamic textual and visual cues from 2D video sequences
    to 4D point cloud sequences, focusing on temporal and semantic coherence. Zhou
    et al. [[317](#bib.bib317)] introduce PointCMC, which utilizes a Local-to-Local
    (L2L) module and a Cross-Modal Local-Global Contrastive (CLGC) loss to enhance
    cross-modal knowledge transfer between image and point cloud data.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.3 Innovative Cross-Modal Applications in Robotics and Dynamic Environments
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section discusses the group of studies that applies cross-modal transfer
    learning to specific, challenging scenarios such as robotic tactile recognition
    and dynamic 4D scene understanding, demonstrating the flexibility and potential
    of cross-modal learning in practical and dynamic applications. Specifically, Falco
    et al. [[318](#bib.bib318)] explore visuo-tactile object recognition, where a
    robot uses visual data to enhance its tactile recognition capabilities, effectively
    bridging the gap between seeing and touching. Additionally, the authors in [[316](#bib.bib316)]
    focus on enhancing 4D point cloud understanding by transferring knowledge from
    RGB video sequences, improving the dynamic scene comprehension significantly.
    Murali et al. [[319](#bib.bib319)] propose xAVTNet, a visuo-tactile cross-modal
    framework for object recognition by autonomous robotic systems, utilizing active
    learning and perception strategies.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.4 Enhanced Sensory Perception and Retrieval
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: CMTL can be used to improve sensory perception and retrieval tasks, enabling
    more effective and efficient recognition and classification across different sensory
    inputs. For example, Shen et al. [[320](#bib.bib320)] implement a simple cross-modal
    transfer from 2D to 3D sensors, using Vision Transformers to handle occlusions
    and improve performance in low-light scenarios. Jing et al. [[321](#bib.bib321)]
    enhances cross-modal retrieval capabilities by training a network to minimize
    feature discrepancies across 2D images, 3DPCs, and mesh data, ensuring robust
    retrieval performance across modalities.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.5 Point Cloud Completion and Enhancement
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Zhu et al. [[322](#bib.bib322)] introduce a cross-modal shape-transfer dual-refinement
    network (CSDN) designed for point cloud completion, utilizing images to guide
    the geometry generation of missing regions and refine the output through dual
    refinement processes. Li et al. [[323](#bib.bib323)] propose a model integrating
    Cross-Domain and Cross-Modal Knowledge Distillation to mitigate domain shifts
    and enhance the interaction between LiDAR and camera data, improving adaptation
    in unsupervised settings. Zhang et al. [[317](#bib.bib317)] explore PointMCD,
    a multi-view cross-modal distillation architecture, which aligns features between
    2D visual and 3D geometric domains to boost the learning capacity of point cloud
    encoders.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.6 Domain Adaptation and Generalization in Cross-Modal Settings
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Peng et al. [[324](#bib.bib324)] discuss leveraging 2D images for 3D domain
    adaptation through cross-modal learning, addressing the loss of useful 2D features
    and promoting high-level modal complementarity. Nitsch et al. [[325](#bib.bib325)]
    propose a transductive transfer learning approach that uses a multi-modal adversarial
    autoencoder to transfer knowledge from images to point clouds, focusing on object
    detection. Li et al. [[326](#bib.bib326)] introduce a bird’s-eye view approach
    for cross-modal learning under Domain Generalization (DG) for 3D semantic segmentation,
    optimizing domain-irrelevant representation modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4.7 Semantic Segmentation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The ProtoTransfer, proposed in [[327](#bib.bib327)], explores knowledge transfer
    from multi-modal sources, such as LiDAR points and images, to enhance point cloud
    semantic segmentation, using a class-wise prototype bank to fully exploit image
    representations and transfer multi-modal knowledge to point cloud features. This
    approach effectively handles the challenge of unmatched point and pixel features
    by employing a pseudo-labeling scheme to integrate these features into the prototype
    bank, demonstrating superior performance on large-scale benchmarks. Similarly,
    Jaritz et al. [[196](#bib.bib196)] explore xMUDA, a cross-modal UDA technique
    for 3D semantic segmentation, leveraging mutual mimicking between 2D images and
    3DPCs. Xing et al. [[328](#bib.bib328)] discuss enhancing domain adaptation for
    3D semantic segmentation using cross-modal contrastive learning to improve interactions
    between 2D-pixel features and 3D point features.
  prefs: []
  type: TYPE_NORMAL
- en: Table LABEL:tab:comparison2 presents a comprehensive comparison of various studies
    focused on cross-modal learning and point cloud analysis, highlighting significant
    contributions and challenges in the field. For example, CrossPoint utilizes a
    self-supervised approach to establish 3D-2D correspondence, improving 3D object
    classification and segmentation, although it faces complexities in cross-modal
    alignment. Similarly, PointCMT enhances the representation of point clouds by
    incorporating view images derived from 2D image quality, thereby boosting 3DPC
    classification capabilities. Other notable methodologies include CrossNet, which
    excels in both intra- and cross-modal learning across multiple benchmark datasets,
    and X-Trans2Cap, which demonstrates effective cross-modal knowledge transfer in
    3D dense captioning, albeit at a high computational cost.
  prefs: []
  type: TYPE_NORMAL
- en: The studies also outline several limitations inherent in current cross-modal
    and point cloud analysis techniques. For instance, many models, like the Cross-modal
    adaptation and simCrossTrans, struggle with domain shifts or performance variations
    across different 3D tasks, respectively. Additionally, while technologies such
    as the Visuotactile object recognition and xAVTNet offer high accuracy in specific
    applications such as object recognition in robotics, they are limited by their
    dependency on particular datasets and high system complexities. Other common challenges
    include the sensitivity to hyperparameters, as seen in Dual-Cross for cross-modal
    UDA, and the difficulty in feature alignment and generalization across different
    domains, which affects models like PointMCD and BEV-DG.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Comparison of Studies on Cross-Modal Learning and Point Cloud Analysis'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | ML Model | Dataset | Application / Task | Advantage | Limitation |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[312](#bib.bib312)] | CrossPoint | Varied | 3D object classification, segmentation
    | Uses 3D-2D correspondence; self-supervised | May require complex cross-modal
    alignment |'
  prefs: []
  type: TYPE_TB
- en: '| [[313](#bib.bib313)] | PointCMT | ModelNet40, ScanObjectNN | 3DPC classification
    | Enhances point-only representation; uses view-images | Depends on 2D image quality
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[314](#bib.bib314)] | CrossNet | Multiple benchmarks | 3DPC classification,
    segmentation | Intra- and cross-modal learning; versatile | Complex model structure
    for fine-tuning |'
  prefs: []
  type: TYPE_TB
- en: '| [[329](#bib.bib329)] | X-Trans2Cap | ScanRefer, Nr3D | 3D dense captioning
    | Effective cross-modal knowledge transfer; high accuracy | High computational
    cost in training |'
  prefs: []
  type: TYPE_TB
- en: '| [[315](#bib.bib315)] | Cross-modal adaptation | SemanticKITTI, KITTI360,
    GTA5 | 3DPC semantic segmentation | Leverages 2D datasets; unsupervised | May
    struggle with substantial domain shifts |'
  prefs: []
  type: TYPE_TB
- en: '| [[318](#bib.bib318)] | Visuo-tactile object recognition | 15 objects dataset
    | Object recognition | High accuracy; cross-modal transfer | Limited to specific
    tactile and visual datasets |'
  prefs: []
  type: TYPE_TB
- en: '| [[316](#bib.bib316)] | X4D-SceneFormer | HOI4D | 4D point cloud tasks | Incorporates
    temporal dynamics; high performance | Complex model, heavy on resources |'
  prefs: []
  type: TYPE_TB
- en: '| [[320](#bib.bib320)] | simCrossTrans | SUN RGB-D | 3D sensor performance
    | Utilizes ViTs; robust to occlusions | Performance may vary across different
    3D tasks |'
  prefs: []
  type: TYPE_TB
- en: '| [[321](#bib.bib321)] | Cross-modal retrieval | ModelNet10, ModelNet40 | Cross-modal
    retrieval | Efficient feature learning; high retrieval accuracy | Depends on network’s
    feature extraction ability |'
  prefs: []
  type: TYPE_TB
- en: '| [[327](#bib.bib327)] | ProtoTransfer | nuScenes, SemanticKITTI | Point cloud
    semantic segmentation | Exploits multi-modal fusion; high accuracy | May miss
    benefits for unmatched features |'
  prefs: []
  type: TYPE_TB
- en: '| [[322](#bib.bib322)] | CSDN | Varied | Point cloud completion | Coarse-to-fine
    approach with dual-refinement; cross-modal data use | Complex model structure
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[319](#bib.bib319)] | xAVTNet | Robotics system | Visuo-tactile object recognition
    | Integrates visuo-tactile data; uses active learning | Specific to robotics;
    high system complexity |'
  prefs: []
  type: TYPE_TB
- en: '| [[323](#bib.bib323)] | Dual-Cross | Multi-modal | Cross-modal UDA | Integrates
    CDKD and CMKD for domain adaptation | Highly sensitive to hyperparameters |'
  prefs: []
  type: TYPE_TB
- en: '| [[324](#bib.bib324)] | DsCML, CMAL | Multi-modal | 3D semantic segmentation
    | Enhances cross-modal learning; diverse domain adaptation | Potential feature
    mismatch |'
  prefs: []
  type: TYPE_TB
- en: '| [[325](#bib.bib325)] | Adversarial Auto Encoder | KITTI | Object detection
    | Transductive transfer from images to point clouds | Requires multi-modal data
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[330](#bib.bib330)] | PointCMC | Varied | 3D object classification, segmentation
    | Enhances fine-grained cross-modal knowledge transfer | Alignment challenges
    |'
  prefs: []
  type: TYPE_TB
- en: '| [[196](#bib.bib196)] | xMUDA | Autonomous driving datasets | 3D semantic
    segmentation | Utilizes mutual mimicking in multi-modality | Complexity in heterogeneous
    input spaces |'
  prefs: []
  type: TYPE_TB
- en: '| [[328](#bib.bib328)] | Cross-modal contrastive learning | Varied | 3D semantic
    segmentation | Improves adaptation effects; uses neighborhood feature aggregation
    | Depends on precise feature correspondence |'
  prefs: []
  type: TYPE_TB
- en: '| [[317](#bib.bib317)] | PointMCD | 3D datasets | 3D shape classification,
    segmentation | Uses cross-modal distillation; aligns features across modalities
    | Complicated feature alignment process |'
  prefs: []
  type: TYPE_TB
- en: '| [[326](#bib.bib326)] | BEV-DG | 3D datasets | 3D semantic segmentation |
    Implements BEV-based cross-modal learning; robust to misalignment | Domain generalization
    complexity |'
  prefs: []
  type: TYPE_TB
- en: 6.5 Diffusion models for 3DPC undertanding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Diffusion models are increasingly used for 3DPC applications due to their ability
    to generate high-quality, detailed structures, which is crucial for tasks such
    as 3D modeling and virtual reality. These models excel in handling complex distributions
    characteristic of 3DPCs, which are typically unordered and involve intricate spatial
    relationships. Their gradual denoising process allows them to implicitly learn
    the nuances of geometric structures, making them suitable for a range of tasks
    including reconstruction, up-sampling, and completion. Additionally, diffusion
    models are robust to noisy data, a common challenge with real-world 3D sensor
    data, and can be integrated with specialized 3D neural network architectures like
    PointNet, enhancing their effectiveness in processing 3DPCs. This flexibility
    and robustness make diffusion models a promising approach for advanced 3D applications
    across various industries.
  prefs: []
  type: TYPE_NORMAL
- en: The studies on diffusion model-based 3DPCs offer innovative solutions across
    various domains. Zheng et al. [[331](#bib.bib331)] introduce PointDif, a pre-training
    method for 3D models that employs a conditional point generator to enhance feature
    aggregation and guide point-to-point recovery, showing notable improvements in
    classification and segmentation tasks. Kasten et al. [[332](#bib.bib332)] describe
    SDS-Complete, a text-to-image diffusion model for completing 3D objects from partial
    point clouds, demonstrating its effectiveness in handling Out-Of-Distribution
    objects. Liu’s study [[333](#bib.bib333)] focuses on a Diffusion Probabilistic
    Network for point cloud segmentation, utilizing a reverse diffusion process to
    refine topological structures. Jiang et al. [[334](#bib.bib334)] develop an SE(3)
    diffusion model-based framework for 3D registration, achieving precise object
    pose alignment by manipulating noise in the SE(3) manifold. Jin introduces [[335](#bib.bib335)]
    a multiway point cloud mosaicking approach, employing a diffusion-based denoising
    process for enhanced registration accuracy. Feng’s [[336](#bib.bib336)] DiffPoint
    combines Vision Transformers with diffusion models for 2D-to-3D reconstruction,
    achieving superior results in both single and multi-view tasks. Sharma [[337](#bib.bib337)]
    proposes a class-conditioned diffusion model to generate synthetic point cloud
    embeddings, significantly enhancing classification performance. Mo’s [[338](#bib.bib338)]
    DiT-3D uses a Diffusion Transformer to generate high-quality 3D shapes from voxelized
    point clouds, integrating 3D window attention for computational efficiency. Yi’s
    [[339](#bib.bib339)] GaussianDreamer leverages both 2D and 3D diffusion models
    for rapid, high-quality 3D generation from text prompts. Lastly, Ho [[340](#bib.bib340)]
    introduces Diffusion-SS3D, integrating diffusion models into a semi-supervised
    learning framework for 3D object detection, improving the quality of pseudo-labels
    and enhancing detection accuracy in diverse 3D spaces. These advancements collectively
    highlight the versatility and effectiveness of diffusion models in enhancing 3DPC
    processing and analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Ohno et al. (2024) present a privacy-centric pedestrian tracking system using
    3D LiDARs, capturing pedestrians as anonymous point clouds and leveraging a generative
    diffusion model to predict trajectories in unmonitored areas, achieving a high
    F-measure of 0.98 [[341](#bib.bib341)]. In a different approach, Bi et al. (2024)
    introduce DiffusionEMIS, a novel paradigm that models 3-D electromagnetic inverse
    scattering as a denoising diffusion process, significantly improving performance
    and noise resistance over conventional methods [[342](#bib.bib342)]. Dutt (2024)
    develops Diff3F, which distills diffusion features onto untextured shapes, demonstrating
    robustness and reliability across several benchmarks [[343](#bib.bib343)]. Simultaneously,
    Li (2024) proposes a framework for generating stable crystal structures using
    a point cloud-based diffusion model, enhancing material design [[344](#bib.bib344)].
    Expanding the applications, Ze (2024) integrates 3D visual representations into
    diffusion models for robotic imitation learning with the 3D Diffusion Policy (DP3),
    which shows remarkable success rates and generalizability [[345](#bib.bib345)].
    Addressing point cloud registration, She (2024) utilizes graph neural PDEs and
    heat kernel signatures to enhance keypoint matching accuracy and robustness [[346](#bib.bib346)].
    Meanwhile, Li (2024) details a method for reconstructing 3D colored objects from
    images using a conditional diffusion model, achieving high fidelity in shape and
    color reconstruction [[347](#bib.bib347)]. Chen (2024) introduces V3D, adapting
    video diffusion models for 3D generation that produce high-quality outputs rapidly
    with impressive multi-view consistency [[348](#bib.bib348)]. Further, Hu (2024)
    presents a novel generative model combining latent diffusion with topological
    features, enabling diverse and adaptable 3D shape generation [[349](#bib.bib349)].
    Finally, Dong (2024) proposes a novel generative model for man-made shapes, incorporating
    diffusion processes with a quality checker to ensure geometric feasibility and
    physical stability, significantly outperforming traditional methods on ShapeNet-v2
    [[350](#bib.bib350)].
  prefs: []
  type: TYPE_NORMAL
- en: Several future directions can be suggested to further improve the use of diffusion
    models in 3DPC. These include enhancing privacy while utilizing detailed data,
    improving computational efficiency in model adaptation with minimal trainable
    parameters, and increasing robustness against data quality variations. Additionally,
    there’s a trend toward cross-domain applications and interdisciplinary approaches,
    such as integrating 2D image processing techniques. The generation of complex
    and detailed 3D structures, especially in dynamic environments, is also a focal
    point, along with the development of semi-supervised and fully automated learning
    systems. Innovations in incorporating topological and geometric features into
    diffusion processes highlight the potential for more precise and versatile models.
    Finally, there’s a growing need for standardization in benchmarking methods to
    evaluate and compare the performance of diffusion models in handling 3DPCs. These
    insights indicate a move towards more efficient, robust, and application-diverse
    uses of diffusion models in the realm of 3D data analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the rapid advancement of 3D scanning technologies, such as [LiDAR](#id18.18.id18)
    and RGB-D cameras, capturing and processing [3DPC](#id1.1.id1) data has become
    increasingly popular in various fields, including robotics, [CV](#id3.3.id3),
    virtual reality, and autonomous vehicles. On the other hand, deep [DTL](#id6.6.id6),
    a cutting-edge technique in [ML](#id5.5.id5) and [DL](#id4.4.id4), has emerged
    as a powerful approach to leverage pre-trained DNNs and transfer their knowledge
    to new [3DPC](#id1.1.id1) tasks overcome several challenges to using [DL](#id4.4.id4).
    To shed light on the latest innovations related to using [DTL](#id6.6.id6) in
    [3DPCs](#id1.1.id1), this article presented a comprehensive review of the state-of-the-art
    techniques for [3DPC](#id1.1.id1) understanding using [DTL](#id6.6.id6) and DA,
    including [3DPC](#id1.1.id1) object detection, [3DPC](#id1.1.id1) semantic labeling,
    segmentation and classification, [3DPC](#id1.1.id1) registration, downsampling/upsampling
    and [3DPC](#id1.1.id1) denoising. In doing so, a well-defined taxonomy has been
    introduced, and detailed comparisons have been conducted, presented with reference
    to different aspects, such as the types of adopted [DL](#id4.4.id4) models, obtained
    performance, and knowledge transfer strategies (fine-tuning, DA, [UDA](#id8.8.id8),
    etc.). Moving forward, the pros and cons of presented frameworks have been covered
    before identifying open challenges. Lastly, potential research directions have
    been listed.
  prefs: []
  type: TYPE_NORMAL
- en: Point cloud understanding has made significant progress through years of research,
    but as it becomes more widely used in real-world applications, it faces new challenges.
    One major challenge is partial overlap in sensor-acquired point clouds, which
    makes direct registration difficult. While some solutions have been developed
    for alignment under partial overlap, the rate of overlap is often limited. Finding
    a comprehensive solution to this problem, therefore, is a valuable and promising
    area of research. Moreover, only simple objects can currently be solved for alignment,
    despite the fact that the [3DPC](#id1.1.id1) registration techniques that combine
    [DTL](#id6.6.id6) and conventional methods have made significant advancements.
    For instance, when dealing with complex scenarios and large-scale [3DPCs](#id1.1.id1),
    these approaches fall short of the desired outcomes and continue to use the conventional
    algorithms. However, these algorithms are stochastic, and the number of iterations
    rise exponentially with outliers. By combining [DTL](#id6.6.id6) and conventional
    [ML](#id5.5.id5) methods, better results are obtained. Specifically, traditional
    [ML](#id5.5.id5) approaches are transparent, whereas [DTL](#id6.6.id6) schemes
    excel at fitting data. Hence, one of the trends for the future research is how
    to combine the benefits of both.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, the need for generalizing [3DPC](#id1.1.id1) scene understanding algorithms
    arises from the fact that different application scenarios present the algorithms
    with different challenges. Nevertheless, given the state of the research, it is
    difficult to suggest a general algorithm. For instance, an aircraft’s skin is
    very large, its surface is smooth, and it has few features with a curvature. When
    feature-based methods are used in the registration process, significant misalignment
    will consequently happen. The development of targeted, lightweight, and efficient
    algorithms for particular application scenarios is thus an attractive research
    hotspot in the near future.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. Himeur, S. Al-Maadeed, H. Kheddar, N. Al-Maadeed, K. Abualsaud, A. Mohamed,
    T. Khattab, Video surveillance using deep transfer learning and deep domain adaptation:
    Towards better generalization, Engineering Applications of Artificial Intelligence
    119 (2023) 105698.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. Copiaco, Y. Himeur, A. Amira, W. Mansoor, F. Fadli, S. Atalla, S. S.
    Sohail, An innovative deep anomaly detection of building energy consumption using
    energy time-series images, Engineering Applications of Artificial Intelligence
    119 (2023) 105775.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Zhou, A. Ji, L. Zhang, X. Xue, Attention-enhanced sampling point cloud
    network (aspcnet) for efficient 3d tunnel semantic segmentation, Automation in
    Construction 146 (2023) 104667.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Y. Habchi, Y. Himeur, H. Kheddar, A. Boukabou, S. Atalla, A. Chouchane,
    A. Ouamane, W. Mansoor, Ai in thyroid cancer diagnosis: Techniques, trends, and
    future directions, Systems 11 (10) (2023) 519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Ji, Y. Zhou, L. Zhang, R. L. Tiong, X. Xue, Semi-supervised learning-based
    point cloud network for segmentation of 3d tunnel scenes, Automation in Construction
    146 (2023) 104668.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] O. Kerdjidj, Y. Himeur, S. Atalla, A. Copiaco, A. Amira, F. Fadli, S. S.
    Sohail, W. Mansoor, A. Gawanmeh, S. Miniaoui, Exploiting 2d representations for
    enhanced indoor localization: A transfer learning approach, IEEE Sensors Journal
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C. Fotsing, P. Hahn, D. Cunningham, C. Bobda, Volumetric wall detection
    in unorganized indoor point clouds using continuous segments in 2d grids, Automation
    in Construction 141 (2022) 104462.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] X. Lai, J. Liu, L. Jiang, L. Wang, H. Zhao, S. Liu, X. Qi, J. Jia, Stratified
    transformer for 3d point cloud segmentation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, IEEE, New Orleans, LA, USA, 2022,
    pp. 8500–8509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Bechar, Y. Elmir, Y. Himeur, R. Medjoudj, A. Amira, Federated and transfer
    learning for cancer detection based on image analysis, arXiv preprint arXiv:2405.20126
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] R. Abbasi, A. K. Bashir, H. J. Alyamani, F. Amin, J. Doh, J. Chen, Lidar
    point cloud compression, processing and learning for autonomous driving, IEEE
    Transactions on Intelligent Transportation Systems (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] D. Liu, W. Hu, Imperceptible transfer attack and defense on 3d point cloud
    classification, IEEE Transactions on Pattern Analysis and Machine Intelligence
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Q. Yu, C. Yang, H. Wei, Part-wise atlasnet for 3d point cloud reconstruction
    from a single image, Knowledge-Based Systems 242 (2022) 108395.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Z. Li, Z. Chen, A. Li, L. Fang, Q. Jiang, X. Liu, J. Jiang, B. Zhou, H. Zhao,
    Simipu: Simple 2d image and 3d point cloud unsupervised pre-training for spatial-aware
    visual representations, in: Proceedings of the AAAI Conference on Artificial Intelligence,
    Vol. 36, AAAI, online/virtual, 2022, pp. 1500–1508.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] O. Elharrouss, S. Al-Maadeed, N. Subramanian, N. Ottakath, N. Almaadeed,
    Y. Himeur, Panoptic segmentation: a review, arXiv preprint arXiv:2111.10250 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. Cao, H. Zhao, P. Liu, Semantic segmentation for point clouds via semantic-based
    local aggregation and multi-scale global pyramid, Machines 11 (1) (2023) 11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] W. Liu, Y. Zang, Z. Xiong, X. Bian, C. Wen, X. Lu, C. Wang, J. M. Junior,
    W. N. Gonçalves, J. Li, 3d building model generation from mls point cloud and
    3d mesh using multi-source data fusion, International Journal of Applied Earth
    Observation and Geoinformation 116 (2023) 103171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] X. Yang, E. del Rey Castillo, Y. Zou, L. Wotherspoon, Y. Tan, Automated
    semantic segmentation of bridge components from large-scale point clouds using
    a weighted superpoint graph, Automation in Construction 142 (2022) 104519.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] W. Liu, Y. Shao, K. Chen, C. Li, H. Luo, Whale optimization algorithm-based
    point cloud data processing method for sewer pipeline inspection, Automation in
    Construction 141 (2022) 104423.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] I. Ahmed, G. Jeon, F. Piccialli, A deep-learning-based smart healthcare
    system for patient’s discomfort detection at the edge of internet of things, IEEE
    Internet of Things Journal 8 (13) (2021) 10318–10326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] N. Mehdiyev, J. Evermann, P. Fettke, A novel business process prediction
    model using a deep learning method, Business & information systems engineering
    62 (2) (2020) 143–157.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] R. Kiran, P. Kumar, B. Bhasker, Dnnrec: A novel deep learning based hybrid
    recommender system, Expert Systems with Applications 144 (2020) 113054.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Y. Himeur, K. Ghanem, A. Alsalemi, F. Bensaali, A. Amira, Artificial intelligence
    based anomaly detection of energy consumption in buildings: A review, current
    trends and new perspectives, Applied Energy 287 (2021) 116601.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] P. K. Kashyap, S. Kumar, A. Jaiswal, M. Prasad, A. H. Gandomi, Towards
    precision agriculture: Iot-enabled intelligent irrigation systems using deep learning
    neural network, IEEE Sensors Journal 21 (16) (2021) 17479–17491.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] D. De Gregorio, A. Tonioni, G. Palli, L. Di Stefano, Semiautomatic labeling
    for deep learning in robotics, IEEE Transactions on Automation Science and Engineering
    17 (2) (2019) 611–620.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. N. Sayed, Y. Himeur, F. Bensaali, Deep and transfer learning for building
    occupancy detection: A review and comparative analysis, Engineering Applications
    of Artificial Intelligence 115 (2022) 105254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] H. Kheddar, Y. Himeur, S. Al-Maadeed, A. Amira, F. Bensaali, Deep transfer
    learning for automatic speech recognition: Towards better generalization, Knowledge-Based
    Systems 277 (2023) 110851. [doi:10.1016/j.knosys.2023.110851](https://doi.org/10.1016/j.knosys.2023.110851).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] H. Kheddar, Y. Himeur, A. I. Awad, Deep transfer learning for intrusion
    detection in industrial control networks: A comprehensive review, J. Netw. Comput.
    Appl. 220 (2023) 103760.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] B. Kitchenham, Procedures for performing systematic reviews, Keele, UK,
    Keele University 33 (2004) (2004) 1–26.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. Himeur, A. Alsalemi, A. Al-Kababji, F. Bensaali, A. Amira, C. Sardianos,
    G. Dimitrakopoulos, I. Varlamis, A survey of recommender systems for energy efficiency
    in buildings: Principles, challenges and prospects, Information Fusion 72 (2021)
    1–21.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] S. A. Bello, S. Yu, C. Wang, J. M. Adam, J. Li, Deep learning on 3d point
    clouds, Remote Sensing 12 (11) (2020) 1729.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Guo, H. Wang, Q. Hu, H. Liu, L. Liu, M. Bennamoun, Deep learning for
    3d point clouds: A survey, IEEE transactions on pattern analysis and machine intelligence
    43 (12) (2020) 4338–4364.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Y. Li, Deep reinforcement learning: An overview, arXiv preprint arXiv:1701.07274
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. Z. Ramirez, A. Tonioni, S. Salti, L. D. Stefano, Learning across tasks
    and domains, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, IEEE, Seoul, Korea (South), 2019, pp. 8110–8119.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] M. A. Morid, A. Borjali, G. Del Fiol, A scoping review of transfer learning
    research on medical image analysis using imagenet, Computers in biology and medicine
    128 (2021) 104115.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] W. Li, W. Huan, B. Hou, Y. Tian, Z. Zhang, A. Song, Can emotion be transferred?–a
    review on transfer learning for eeg-based emotion recognition, IEEE Transactions
    on Cognitive and Developmental Systems (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Z. Alyafeai, M. S. AlShaibani, I. Ahmad, A survey on transfer learning
    in natural language processing, arXiv preprint arXiv:2007.04239 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] R. Ribani, M. Marengoni, A survey of transfer learning for convolutional
    neural networks, in: 2019 32nd SIBGRAPI conference on graphics, patterns and images
    tutorials (SIBGRAPI-T), IEEE, Rio de Janeiro, Brazil, 2019, pp. 47–57.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Z. Wan, R. Yang, M. Huang, N. Zeng, X. Liu, A review on transfer learning
    in eeg signal analysis, Neurocomputing 421 (2021) 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Z.-H. Liu, L.-B. Jiang, H.-L. Wei, L. Chen, X.-H. Li, Optimal transport-based
    deep domain adaptation approach for fault diagnosis of rotating machine, IEEE
    Transactions on Instrumentation and Measurement 70 (2021) 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] C. Tan, F. Sun, T. Kong, W. Zhang, C. Yang, C. Liu, A survey on deep transfer
    learning, in: International conference on artificial neural networks, Springer,
    Rhodes, Greece, 2018, pp. 270–279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Niu, Y. Jiang, B. Chen, J. Wang, Y. Liu, H. Song, Cross-modality transfer
    learning for image-text information management, ACM Transactions on Management
    Information System (TMIS) 13 (1) (2021) 1–14.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] J. Si, H. Shi, J. Chen, C. Zheng, Unsupervised deep transfer learning
    with moment matching: A new intelligent fault diagnosis approach for bearings,
    Measurement 172 (2021) 108827.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Y. Zhou, J. Yang, H. Li, T. Cao, S.-Y. Kung, Adversarial learning for
    multiscale crowd counting under complex scenes, IEEE transactions on cybernetics
    51 (11) (2020) 5423–5432.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, V. Lempitsky, Domain-adversarial training of neural networks, The
    journal of machine learning research 17 (1) (2016) 2096–2030.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Z. Shen, Y. Xu, B. Ni, M. Wang, J. Hu, X. Yang, Crowd counting via adversarial
    cross-scale consistency pursuit, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 5245–5254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] M.-I. Georgescu, R. T. Ionescu, F. S. Khan, M. Popescu, M. Shah, A background-agnostic
    framework with adversarial training for abnormal event detection in video, arXiv
    preprint arXiv:2008.12328 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] W. Choi, W. Yang, J. Na, J. Park, G. Lee, W. Nam, Unsupervised gait phase
    estimation with domain-adversarial neural network and adaptive window, IEEE Journal
    of Biomedical and Health Informatics (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] E. Soleimani, E. Nazerfard, Cross-subject transfer learning in human activity
    recognition systems using generative adversarial networks, Neurocomputing 426
    (2021) 26–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. Wang, Y. Chen, W. Feng, H. Yu, M. Huang, Q. Yang, Transfer learning
    with dynamic distribution adaptation, ACM Transactions on Intelligent Systems
    and Technology (TIST) 11 (1) (2020) 1–25.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Z. Zou, X. Qu, P. Zhou, S. Xu, X. Ye, W. Wu, J. Ye, Coarse to fine: Domain
    adaptive crowd counting via adversarial scoring network, in: Proceedings of the
    29th ACM International Conference on Multimedia, ACM, online, 2021, pp. 2185–2194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] J. Sun, Y. Cao, C. B. Choy, Z. Yu, A. Anandkumar, Z. M. Mao, C. Xiao,
    Adversarially robust 3d point cloud recognition using self-supervisions, Advances
    in Neural Information Processing Systems 34 (2021) 15498–15512.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] C. R. Qi, H. Su, K. Mo, L. J. Guibas, Pointnet: Deep learning on point
    sets for 3d classification and segmentation, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, IEEE, Honolulu, HI, USA, 2017, pp.
    652–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] C. R. Qi, L. Yi, H. Su, L. J. Guibas, Pointnet++: Deep hierarchical feature
    learning on point sets in a metric space, Advances in neural information processing
    systems 30 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] R. Li, Y. Zhang, D. Niu, G. Yang, N. Zafar, C. Zhang, X. Zhao, Pointvgg:
    Graph convolutional network with progressive aggregating features on point clouds,
    Neurocomputing 429 (2021) 187–198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Shi, C. Zhang, X. Zhang, K. Wang, Y. Zhang, X. Zhao, Pointpavgg: An
    incremental algorithm for extraction of points’ positional feature using vgg on
    point clouds, in: Intelligent Computing Theories and Application: 17th International
    Conference, ICIC 2021, Shenzhen, China, August 12–15, 2021, Proceedings, Part
    II, Springer, 2021, pp. 718–731.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Y. Xu, T. Fan, M. Xu, L. Zeng, Y. Qiao, Spidercnn: Deep learning on point
    sets with parameterized convolutional filters, in: Proceedings of the European
    conference on computer vision (ECCV), Springer, Munich, Germany, 2018, pp. 87–102.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Y. Li, R. Bu, M. Sun, W. Wu, X. Di, B. Chen, Pointcnn: Convolution on
    x-transformed points, Advances in neural information processing systems 31 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] H. Su, V. Jampani, D. Sun, S. Maji, E. Kalogerakis, M.-H. Yang, J. Kautz,
    Splatnet: Sparse lattice networks for point cloud processing, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, 2018, pp. 2530–2539.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] W. Wang, R. Yu, Q. Huang, U. Neumann, Sgpn: Similarity group proposal
    network for 3d point cloud instance segmentation, in: Proceedings of the IEEE
    conference on computer vision and pattern recognition, 2018, pp. 2569–2578.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Y. Yang, C. Feng, Y. Shen, D. Tian, Foldingnet: Point cloud auto-encoder
    via deep grid deformation, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 206–215.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] L. Yu, X. Li, C.-W. Fu, D. Cohen-Or, P.-A. Heng, Pu-net: Point cloud upsampling
    network, in: Proceedings of the IEEE conference on computer vision and pattern
    recognition, 2018, pp. 2790–2799.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] T. Le, Y. Duan, Pointgrid: A deep network for 3d shape understanding,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    2018, pp. 9204–9214.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] L. Ge, Y. Cai, J. Weng, J. Yuan, Hand pointnet: 3d hand pose estimation
    using point sets, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2018, pp. 8417–8426.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] M. A. Uy, G. H. Lee, Pointnetvlad: Deep point cloud based retrieval for
    large-scale place recognition, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 4470–4479.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. Huang, Z. Gojcic, M. Usvyatsov, A. Wieser, K. Schindler, Predator:
    Registration of 3d point clouds with low overlap, in: Proceedings of the IEEE/CVF
    Conference on computer vision and pattern recognition, 2021, pp. 4267–4276.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H. Wang, Y. Cong, O. Litany, Y. Gao, L. J. Guibas, 3dioumatch: Leveraging
    iou prediction for semi-supervised 3d object detection, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021, pp. 14615–14624.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] A. Dai, A. X. Chang, M. Savva, M. Halber, T. Funkhouser, M. Nießner, Scannet:
    Richly-annotated 3d reconstructions of indoor scenes, in: Proceedings of the IEEE
    conference on computer vision and pattern recognition, 2017, pp. 5828–5839.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] G. Riegler, A. Osman Ulusoy, A. Geiger, Octnet: Learning deep 3d representations
    at high resolutions, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2017, pp. 3577–3586.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] J. Li, B. M. Chen, G. H. Lee, So-net: Self-organizing network for point
    cloud analysis, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2018, pp. 9397–9406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] B. Yang, W. Luo, R. Urtasun, Pixor: Real-time 3d object detection from
    point clouds, in: Proceedings of the IEEE conference on Computer Vision and Pattern
    Recognition, 2018, pp. 7652–7660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Y. Zhou, O. Tuzel, Voxelnet: End-to-end learning for point cloud based
    3d object detection, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, 2018, pp. 4490–4499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] H. Deng, T. Birdal, S. Ilic, Ppfnet: Global context aware local features
    for robust 3d point matching, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 195–205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] D. Xu, D. Anguelov, A. Jain, Pointfusion: Deep sensor fusion for 3d bounding
    box estimation, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, 2018, pp. 244–253.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] C. R. Qi, W. Liu, C. Wu, H. Su, L. J. Guibas, Frustum pointnets for 3d
    object detection from rgb-d data, in: Proceedings of the IEEE conference on computer
    vision and pattern recognition, 2018, pp. 918–927.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Z. J. Yew, G. H. Lee, 3dfeat-net: Weakly supervised local 3d features
    for point cloud registration, in: Proceedings of the European conference on computer
    vision (ECCV), 2018, pp. 607–623.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] B. Wu, A. Wan, X. Yue, K. Keutzer, Squeezeseg: Convolutional neural nets
    with recurrent crf for real-time road-object segmentation from 3d lidar point
    cloud, in: 2018 IEEE international conference on robotics and automation (ICRA),
    IEEE, 2018, pp. 1887–1893.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Yan, J. Gao, C. Zheng, C. Zheng, R. Zhang, S. Cui, Z. Li, 2dpass: 2d
    priors assisted semantic segmentation on lidar point clouds, in: European Conference
    on Computer Vision, Springer, 2022, pp. 677–695.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] W. Chen, J. Duan, H. Basevi, H. J. Chang, A. Leonardis, Pointposenet:
    Point pose network for robust 6d object pose estimation, in: Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision, 2020, pp. 2824–2833.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] H. Cao, Y. Lu, C. Lu, B. Pang, G. Liu, A. Yuille, Asap-net: Attention
    and structure aware point cloud sequence segmentation, arXiv preprint arXiv:2008.05149
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] S. Biswas, J. Liu, K. Wong, S. Wang, R. Urtasun, Muscle: Multi sweep compression
    of lidar using deep entropy models, Advances in Neural Information Processing
    Systems 33 (2020) 22170–22181.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Zhang, D. Huang, Y. Wang, Pc-rgnn: Point cloud completion and graph
    neural network for 3d object detection, in: Proceedings of the AAAI conference
    on artificial intelligence, Vol. 35, 2021, pp. 3430–3437.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C. Xu, S. Yang, B. Zhai, B. Wu, X. Yue, W. Zhan, P. Vajda, K. Keutzer,
    M. Tomizuka, Image2point: 3d point-cloud understanding with 2d image pretrained
    models (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] L. Mattheuwsen, M. Vergauwen, Manhole cover detection on rasterized mobile
    mapping point cloud data using transfer learned fully convolutional neural networks,
    Remote Sensing 12 (22) (2020) 3820.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Liao, M. E. Mohammadi, R. L. Wood, Deep learning classification of
    2d orthomosaic images and 3d point clouds for post-event structural damage assessment,
    Drones 4 (2) (2020) 24.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Bourbia, A. Karine, A. Chetouani, M. El Hassouni, Blind projection-based
    3d point cloud quality assessment method using a convolutional neural network.,
    in: VISIGRAPP (4: VISAPP), SCITEPRES, online, 2022, pp. 518–525.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] R. Leroy, P. Trouvé-Peloux, F. Champagnat, B. Le Saux, M. Carvalho, Pix2point:
    Learning outdoor 3d using sparse point clouds and optimal transport, in: 2021
    17th International Conference on Machine Vision and Applications (MVA), IEEE,
    online, 2021, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] J. Balado, R. Sousa, L. Diaz-Vilarino, P. Arias, Transfer learning in
    urban object classification: Online images to recognize point clouds, Automation
    in Construction 111 (2020) 103058.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] V. Stojanovic, M. Trapp, R. Richter, J. Döllner, Service-oriented semantic
    enrichment of indoor point clouds using octree-based multiview classification,
    Graphical Models 105 (2019) 101039.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] W. Dongyu, H. Fuwen, T. Mikolajczyk, H. Yunhua, Object detection for soft
    robotic manipulation based on rgb-d sensors, in: 2018 WRC Symposium on Advanced
    Robotics and Automation (WRC SARA), IEEE, Beijing, China, 2018, pp. 52–58.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] C. Zhao, H. Guo, J. Lu, D. Yu, D. Li, X. Chen, Als point cloud classification
    with small training data set based on transfer learning, IEEE Geoscience and Remote
    Sensing Letters 17 (8) (2020) 1406–1410. [doi:10.1109/LGRS.2019.2947608](https://doi.org/10.1109/LGRS.2019.2947608).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    IEEE, Las Vegas, NV, USA, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] M. Liu, L. Sheng, S. Yang, J. Shao, S.-M. Hu, Morphing and sampling network
    for dense point cloud completion, in: Proceedings of the AAAI conference on artificial
    intelligence, Vol. 34, 2020, pp. 11596–11603.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] D. Zong, S. Sun, J. Zhao, Ashf-net: Adaptive sampling and hierarchical
    folding network for robust point cloud completion, in: Proceedings of the AAAI
    Conference on Artificial Intelligence, Vol. 35, 2021, pp. 3625–3632.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] W. Yuan, T. Khot, D. Held, C. Mertz, M. Hebert, Pcn: Point completion
    network, in: 2018 international conference on 3D vision (3DV), IEEE, 2018, pp.
    728–737.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] X. Wen, T. Li, Z. Han, Y.-S. Liu, Point cloud completion by skip-attention
    network with hierarchical folding, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2020, pp. 1939–1948.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Y. Nie, Y. Lin, X. Han, S. Guo, J. Chang, S. Cui, J. Zhang, et al., Skeleton-bridged
    point completion: From global inference to local adjustment, Advances in Neural
    Information Processing Systems 33 (2020) 16119–16130.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Y. Li, L. Ma, W. Tan, C. Sun, D. Cao, J. Li, Grnet: Geometric relation
    network for 3d object detection from point clouds, ISPRS Journal of Photogrammetry
    and Remote Sensing 165 (2020) 43–53.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] X. Wang, M. H. Ang, G. H. Lee, Voxel-based network for shape completion
    by leveraging edge generation, in: Proceedings of the IEEE/CVF international conference
    on computer vision, 2021, pp. 13189–13198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Y. Tang, R. Zhang, Z. Guo, X. Ma, B. Zhao, Z. Wang, D. Wang, X. Li, Point-peft:
    Parameter-efficient fine-tuning for 3d pre-trained models, in: Proceedings of
    the AAAI Conference on Artificial Intelligence, Vol. 38, 2024, pp. 5171–5179.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Zhou, D. Liang, W. Xu, X. Zhu, Y. Xu, Z. Zou, X. Bai, Dynamic adapter
    meets prompt tuning: Parameter-efficient transfer learning for point cloud analysis,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2024, pp. 14707–14717.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] G. Song, H. Xu, J. Liu, T. Zhi, Y. Shi, J. Zhang, Z. Jiang, J. Feng,
    S. Sang, L. Luo, Agilegan3d: Few-shot 3d portrait stylization by augmented transfer
    learning, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2024, pp. 765–774.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. A. Shoukat, A. B. Sargano, L. You, Z. Habib, 3d estimation of single-view
    2d images using shape priors and transfer learning, Multimedia Tools and Applications
    (2024) 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] M. Zaheer, S. Kottur, S. Ravanbakhsh, B. Poczos, R. R. Salakhutdinov,
    A. J. Smola, Deep sets, Advances in neural information processing systems 30 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] R. Klokov, V. Lempitsky, Escape from cells: Deep kd-networks for the
    recognition of 3d point cloud models, in: Proceedings of the IEEE international
    conference on computer vision, Venice, Italy, 2017, pp. 863–872.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] W. Zeng, T. Gevers, 3dcontextnet: Kd tree guided hierarchical learning
    of point clouds using local and global contextual cues, in: Proceedings of the
    European Conference on Computer Vision (ECCV) Workshops, Springer, Munich, Germany,,
    2018, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] B.-S. Hua, M.-K. Tran, S.-K. Yeung, Pointwise convolutional neural networks,
    in: Proceedings of the IEEE conference on computer vision and pattern recognition,
    IEEE, Salt Lake City, UT, USA, 2018, pp. 984–993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Z. Zhang, B.-S. Hua, S.-K. Yeung, Shellnet: Efficient point cloud convolutional
    neural networks using concentric shells statistics, in: Proceedings of the IEEE/CVF
    international conference on computer vision, IEEE, Seoul, Korea (South), 2019,
    pp. 1607–1616.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] G. Te, W. Hu, A. Zheng, Z. Guo, Rgcnn: Regularized graph cnn for point
    cloud segmentation, in: Proceedings of the 26th ACM international conference on
    Multimedia, ACM, Seoul Republic of Korea, 2018, pp. 746–754.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] C. Wang, B. Samari, K. Siddiqi, Local spectral graph convolution for
    point set feature learning, in: Proceedings of the European conference on computer
    vision (ECCV), Springer, Munich, Germany, 2018, pp. 52–66.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] S. Xie, S. Liu, Z. Chen, Z. Tu, Attentional shapecontextnet for point
    cloud recognition, in: Proceedings of the IEEE conference on computer vision and
    pattern recognition, IEEE, Salt Lake City, UT, USA, 2018, pp. 4606–4615.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] N. Verma, E. Boyer, J. Verbeek, Feastnet: Feature-steered graph convolutions
    for 3d shape analysis, in: Proceedings of the IEEE conference on computer vision
    and pattern recognition, IEEE, Salt Lake City, UT, USA, 2018, pp. 2598–2606.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] A. Boulch, Convpoint: Continuous convolutions for point cloud processing,
    Computers & Graphics 88 (2020) 24–34.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Z. Yang, O. Litany, T. Birdal, S. Sridhar, L. Guibas, Continuous geodesic
    convolutions for learning on 3d shapes, in: Proceedings of the IEEE/CVF winter
    conference on applications of computer vision, IEEE, online, 2021, pp. 134–144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] J. Sauder, B. Sievers, Self-supervised deep learning on point clouds
    by reconstructing space, Advances in Neural Information Processing Systems 32
    (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] O. Poursaeed, T. Jiang, H. Qiao, N. Xu, V. G. Kim, Self-supervised learning
    of point clouds via orientation estimation, in: 2020 International Conference
    on 3D Vision (3DV), IEEE, 2020, pp. 1018–1028.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] I. Achituve, H. Maron, G. Chechik, Self-supervised learning for domain
    adaptation on point clouds, in: Proceedings of the IEEE/CVF winter conference
    on applications of computer vision, IEEE, online, 2021, pp. 123–133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] S. Xie, J. Gu, D. Guo, C. R. Qi, L. Guibas, O. Litany, Pointcontrast:
    Unsupervised pre-training for 3d point cloud understanding, in: Computer Vision–ECCV
    2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings,
    Part III 16, Springer, online, 2020, pp. 574–591.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] H. Wang, Q. Liu, X. Yue, J. Lasenby, M. J. Kusner, Unsupervised point
    cloud pre-training via occlusion completion, in: Proceedings of the IEEE/CVF international
    conference on computer vision, Montreal, BC, Canada, 2021, pp. 9782–9792.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] X. Yu, L. Tang, Y. Rao, T. Huang, J. Zhou, J. Lu, Point-bert: Pre-training
    3d point cloud transformers with masked point modeling, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE, New Orleans,
    LA, USA, 2022, pp. 19313–19322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] R. Zhang, Z. Guo, P. Gao, R. Fang, B. Zhao, D. Wang, Y. Qiao, H. Li,
    Point-m2ae: multi-scale masked autoencoders for hierarchical point cloud pre-training,
    arXiv preprint arXiv:2205.14401 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] C. Choy, J. Gwak, S. Savarese, 4d spatio-temporal convnets: Minkowski
    convolutional neural networks, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, IEEE, Long Beach, CA, USA, 2019, pp. 3075–3084.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] B. Graham, M. Engelcke, L. Van Der Maaten, 3d semantic segmentation with
    submanifold sparse convolutional networks, in: Proceedings of the IEEE conference
    on computer vision and pattern recognition, IEEE, Salt Lake City, UT, USA, 2018,
    pp. 9224–9232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] O. Ronneberger, P. Fischer, T. Brox, U-net: Convolutional networks for
    biomedical image segmentation, in: Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18, Springer, 2015, pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] H. Kheddar, M. Hemis, Y. Himeur, Automatic speech recognition using advanced
    deep learning approaches: A survey, Information Fusion (2024) 102422.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    H. Li, Pointclip: Point cloud understanding by clip, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, IEEE, New Orleans, Louisiana,
    USA, 2022, pp. 8552–8562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] X. Huang, T. Xie, Z. Wang, L. Chen, Q. Zhou, Z. Hu, A transfer learning-based
    multi-fidelity point-cloud neural network approach for melt pool modeling in additive
    manufacturing, ASCE-ASME J Risk and Uncert in Engrg Sys Part B Mech Engrg 8 (1)
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] W. Zhao, H. Zhang, Y. Yan, Y. Fu, H. Wang, A semantic segmentation algorithm
    using fcn with combination of bslic, Applied Sciences 8 (4) (2018) 500.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] M. Zhang, P. Kadam, S. Liu, C.-C. J. Kuo, Unsupervised feedforward feature
    (uff) learning for point cloud classification and segmentation, in: 2020 IEEE
    International Conference on Visual Communications and Image Processing (VCIP),
    IEEE, online, 2020, pp. 144–147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] A. Murtiyoso, C. Lhenry, T. Landes, P. Grussenmeyer, E. Alby, Semantic
    segmentation for building façade 3d point cloud from 2d orthophoto images using
    transfer learning, ISPRS-International Archives of the Photogrammetry, Remote
    Sensing and Spatial Information Sciences 43 (2021) 201–206.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] M. Imad, O. Doukhi, D.-J. Lee, Transfer learning based semantic segmentation
    for 3d object detection from point cloud, Sensors 21 (12) (2021) 3964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] C. Zong, H. Wang, et al., An improved 3d point cloud instance segmentation
    method for overhead catenary height detection, Computers & Electrical Engineering
    98 (2022) 107685.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] D. Bazazian, D. Nahata, Dcg-net: Dynamic capsule graph convolutional
    network for point clouds, IEEE Access 8 (2020) 188056–188067.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] N. Arnold, P. Angelov, T. Viney, P. Atkinson, Automatic extraction and
    labelling of memorial objects from 3d point clouds, Journal of Computer Applications
    in Archaeology 4 (1) (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] D. Urbach, Y. Ben-Shabat, M. Lindenbaum, Dpdist: Comparing point clouds
    using deep point cloud distance, in: Computer Vision–ECCV 2020: 16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XI 16, Springer,
    2020, pp. 545–560.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] D. P. Huttenlocher, G. A. Klanderman, W. J. Rucklidge, Comparing images
    using the hausdorff distance, IEEE Transactions on pattern analysis and machine
    intelligence 15 (9) (1993) 850–863.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] T. Nguyen, Q.-H. Pham, T. Le, T. Pham, N. Ho, B.-S. Hua, Point-set distances
    for learning representations of 3d point clouds, in: Proceedings of the IEEE/CVF
    International Conference on Computer Vision, Montreal, BC, Canada, 2021, pp. 10478–10487.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Z. Xie, N. Somani, Y. J. S. Tan, J. C. Y. Seng, Automatic toolpath pattern
    recommendation for various industrial applications based on deep learning, in:
    2021 IEEE/SICE International Symposium on System Integration (SII), IEEE, 2021,
    pp. 60–65.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] X. Lei, H. Wang, C. Wang, Z. Zhao, J. Miao, P. Tian, Als point cloud
    classification by integrating an improved fully convolutional network into transfer
    learning with multi-scale and multi-view deep features, Sensors 20 (23) (2020)
    6969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] X. Li, C. Li, Z. Tong, A. Lim, J. Yuan, Y. Wu, J. Tang, R. Huang, Campus3d:
    A photogrammetry point cloud benchmark for hierarchical understanding of outdoor
    scene, in: Proceedings of the 28th ACM International Conference on Multimedia,
    2020, pp. 238–246.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] J. Kim, D. Chung, Y. Kim, H. Kim, Deep learning-based 3d reconstruction
    of scaffolds using a robot dog, Automation in Construction 134 (2022) 104092.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] P.-Y. Chen, Z. Y. Wu, E. Taciroglu, Classification of soft-story buildings
    using deep learning with density features extracted from 3d point clouds, Journal
    of Computing in Civil Engineering 35 (3) (2021) 04021005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] K. Sidor, M. Wysocki, Recognition of human activities using depth maps
    and the viewpoint feature histogram descriptor, Sensors 20 (10) (2020) 2940.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] J. Tian, J. Zhang, W. Li, D. Xu, Vdm-da: Virtual domain modeling for
    source data-free domain adaptation, IEEE Transactions on Circuits and Systems
    for Video Technology (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] K. Huang, X. Qu, S. Chen, Z. Chen, W. Zhang, H. Qi, F. Zhao, Superb monocular
    depth estimation based on transfer learning and surface normal guidance, Sensors
    20 (17) (2020) 4856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] H. Benhabiles, K. Hammoudi, F. Windal, M. Melkemi, A. Cabani, A transfer
    learning exploited for indexing protein structures from 3d point clouds, in: Sipaim–Miccai
    Biomedical Workshop, Springer, 2018, pp. 82–89.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] K. B. Ozyoruk, G. I. Gokceler, T. L. Bobrow, G. Coskun, K. Incetan, Y. Almalioglu,
    F. Mahmood, E. Curto, L. Perdigoto, M. Oliveira, et al., Endoslam dataset and
    an unsupervised monocular visual odometry and depth estimation approach for endoscopic
    videos, Medical image analysis 71 (2021) 102058.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] A. X. Chang, T. Funkhouser, L. Guibas, P. Hanrahan, Q. Huang, Z. Li,
    S. Savarese, M. Savva, S. Song, H. Su, et al., Shapenet: An information-rich 3d
    model repository, arXiv preprint arXiv:1512.03012 (2015).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] B. Eckart, W. Yuan, C. Liu, J. Kautz, Self-supervised learning on 3d
    point clouds by learning discrete generative models, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, IEEE, Nashville, TN, USA,
    2021, pp. 8248–8257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] S. Lee, Y. Yang, Progressive deep learning framework for recognizing
    3d orientations and object class based on point cloud representation, Sensors
    21 (18) (2021) 6108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] B. Dai, C. Le Gentil, T. Vidal-Calleja, Connecting the dots for real-time
    lidar-based object detection with yolo, in: Australasian Conference on Robotics
    and Automation, ACRA, ARAA, Lincoln, Canterbury, New Zealand, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] G. Diraco, P. Siciliano, A. Leone, Remaining useful life prediction from
    3d scan data with genetically optimized convolutional neural networks, Sensors
    21 (20) (2021) 6772.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] J. Kang, M. Körner, Y. Wang, H. Taubenböck, X. X. Zhu, Building instance
    classification using street view images, ISPRS journal of photogrammetry and remote
    sensing 145 (2018) 44–59.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] C. Wu, X. Bi, J. Pfrommer, A. Cebulla, S. Mangold, J. Beyerer, Sim2real
    transfer learning for point cloud segmentation: An industrial application case
    on autonomous disassembly, in: Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision, IEEE, Waikoloa, HI, USA, 2023, pp. 4531–4540.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Y. Zhou, A. Ji, L. Zhang, X. Xue, Sampling-attention deep learning network
    with transfer learning for large-scale urban point cloud semantic segmentation,
    Engineering Applications of Artificial Intelligence 117 (2023) 105554.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] B. Wu, X. Zhou, S. Zhao, X. Yue, K. Keutzer, Squeezesegv2: Improved model
    structure and unsupervised domain adaptation for road-object segmentation from
    a lidar point cloud, in: 2019 International Conference on Robotics and Automation
    (ICRA), IEEE, Montreal, Canada, 2019, pp. 4376–4382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] P. Jiang, S. Saripalli, Lidarnet: A boundary-aware domain adaptation
    model for point cloud semantic segmentation, in: 2021 IEEE International Conference
    on Robotics and Automation (ICRA), IEEE, Xi’an, China, 2021, pp. 2457–2464.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] L. Yi, B. Gong, T. Funkhouser, Complete & label: A domain adaptation
    approach to semantic segmentation of lidar point clouds, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, IEEE, Nashville,
    TN, USA, 2021, pp. 15363–15373.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] C. Qin, H. You, L. Wang, C.-C. J. Kuo, Y. Fu, Pointdan: A multi-scale
    3d domain adaption network for point cloud representation, Advances in Neural
    Information Processing Systems 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] S. Zhao, Y. Wang, B. Li, B. Wu, Y. Gao, P. Xu, T. Darrell, K. Keutzer,
    epointda: An end-to-end simulation-to-real domain adaptation framework for lidar
    point cloud segmentation, in: Proceedings of the AAAI Conference on Artificial
    Intelligence, Vol. 35, online, 2021, pp. 3500–3509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Q. Xu, Y. Zhou, W. Wang, C. R. Qi, D. Anguelov, Spg: Unsupervised domain
    adaptation for 3d object detection via semantic point generation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada,
    2021, pp. 15446–15456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, O. Beijbom, Pointpillars:
    Fast encoders for object detection from point clouds, in: Proceedings of the IEEE/CVF
    conference on computer vision and pattern recognition, IEEE, Long Beach, CA, USA,
    2019, pp. 12697–12705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, J. Shen, Ssda3d: Semi-supervised
    domain adaptation for 3d object detection from point cloud, arXiv preprint arXiv:2212.02845
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] L. Du, X. Ye, X. Tan, J. Feng, Z. Xu, E. Ding, S. Wen, Associate-3ddet:
    Perceptual-to-conceptual association for 3d point cloud object detection, in:
    Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    Seattle, WA, USA, 2020, pp. 13329–13338.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Y. Guo, Y. Li, L. Wang, T. Rosing, Adafilter: Adaptive filter fine-tuning
    for deep transfer learning, in: Proceedings of the AAAI Conference on Artificial
    Intelligence, Vol. 34, AAAI, New York Midtown, New York, USA, 2020, pp. 4060–4066.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] A. Kumar, P. Sattigeri, K. Wadhawan, L. Karlinsky, R. Feris, B. Freeman,
    G. Wornell, Co-regularized alignment for unsupervised domain adaptation, Advances
    in Neural Information Processing Systems 31 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Z. Li, X. Li, Y. Wei, L. Bing, Y. Zhang, Q. Yang, Transferable end-to-end
    aspect-based sentiment analysis with selective adversarial learning, arXiv preprint
    arXiv:1910.14192 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] W. Ge, Y. Yu, Borrowing treasures from the wealthy: Deep transfer learning
    through selective joint fine-tuning, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, IEEE, Honolulu, HI, USA, 2017, pp. 1086–1095.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] M. Maqsood, F. Nazir, U. Khan, F. Aadil, H. Jamal, I. Mehmood, O.-y.
    Song, Transfer learning assisted classification and detection of alzheimer’s disease
    stages using 3d mri scans, Sensors 19 (11) (2019) 2645.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] U. Côté-Allard, C. L. Fall, A. Drouin, A. Campeau-Lecours, C. Gosselin,
    K. Glette, F. Laviolette, B. Gosselin, Deep learning for electromyographic hand
    gesture signal classification using transfer learning, IEEE transactions on neural
    systems and rehabilitation engineering 27 (4) (2019) 760–771.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] C.-X. Ren, D.-Q. Dai, K.-K. Huang, Z.-R. Lai, Transfer learning of structured
    representation for face recognition, IEEE Transactions on image processing 23 (12)
    (2014) 5440–5454.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] F. Matrone, M. Martini, Transfer learning and performance enhancement
    techniques for deep semantic segmentation of built heritage point clouds, Virtual
    Archaeology Review 12 (25) (2021) 73–84.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] L. Xuhong, Y. Grandvalet, F. Davoine, Explicit inductive bias for transfer
    learning with convolutional networks, in: International Conference on Machine
    Learning, PMLR, PMLR, tockholm, Sweden, 2018, pp. 2825–2834.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] H. Xiu, T. Shinohara, M. Matsuoka, M. Inoguchi, K. Kawabe, K. Horie,
    Collapsed building detection using 3d point clouds and deep learning, Remote Sensing
    12 (24) (2020) 4057.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Y. Wang, Y. Sun, Z. Liu, S. E. Sarma, M. M. Bronstein, J. M. Solomon,
    Dynamic graph cnn for learning on point clouds, Acm Transactions On Graphics (tog)
    38 (5) (2019) 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] K. Jeon, G. Lee, S. Yang, H. D. Jeong, Named entity recognition of building
    construction defect information from text with linguistic noise, Automation in
    Construction 143 (2022) 104543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] M.-Y. Cheng, R. R. Khasani, K. Setiono, Image quality enhancement using
    hybridgan for automated railway track defect recognition, Automation in Construction
    146 (2023) 104669.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] G. Kang, L. Jiang, Y. Yang, A. G. Hauptmann, Contrastive adaptation network
    for unsupervised domain adaptation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, Long Beach, CA, USA, 2019, pp. 4893–4902.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A. S. Chung,
    L. Hauswald, V. H. Pham, M. Mühlegg, S. Dorn, T. Fernandez, M. Jänicke, S. Mirashi,
    C. Savani, M. Sturm, O. Vorobiov, M. Oelker, S. Garreis, P. Schuberth, A2d2: Audi
    autonomous driving dataset (2020). [arXiv:2004.06320](http://arxiv.org/abs/2004.06320).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] R. Wang, Z. Wu, Z. Weng, J. Chen, G.-J. Qi, Y.-G. Jiang, Cross-domain
    contrastive learning for unsupervised domain adaptation, IEEE Transactions on
    Multimedia (2022) 1–1[doi:10.1109/TMM.2022.3146744](https://doi.org/10.1109/TMM.2022.3146744).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] B. Xie, S. Li, F. Lv, C. H. Liu, G. Wang, D. Wu, A collaborative alignment
    framework of transferable knowledge extraction for unsupervised domain adaptation,
    IEEE Transactions on Knowledge and Data Engineering (2022). [doi:10.1109/TKDE.2022.3185233](https://doi.org/10.1109/TKDE.2022.3185233).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] C.-X. Ren, Y.-H. Liu, X.-W. Zhang, K.-K. Huang, Multi-source unsupervised
    domain adaptation via pseudo target domain, IEEE Transactions on Image Processing
    31 (2022) 2122–2135. [doi:10.1109/TIP.2022.3152052](https://doi.org/10.1109/TIP.2022.3152052).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] B. Fernando, A. Habrard, M. Sebban, T. Tuytelaars, Unsupervised visual
    domain adaptation using subspace alignment, in: 2013 IEEE International Conference
    on Computer Vision, IEEE, Sydney, Australia, 2013, pp. 2960–2967. [doi:10.1109/ICCV.2013.368](https://doi.org/10.1109/ICCV.2013.368).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] M. M. Bronstein, J. Bruna, Y. LeCun, A. Szlam, P. Vandergheynst, Geometric
    deep learning: Going beyond euclidean data, IEEE Signal Processing Magazine 34 (4)
    (2017) 18–42. [doi:10.1109/MSP.2017.2693418](https://doi.org/10.1109/MSP.2017.2693418).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] C.-Y. Lee, T. Batra, M. H. Baig, D. Ulbricht, Sliced wasserstein discrepancy
    for unsupervised domain adaptation, in: 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), IEEE, Long Beach, CA, USA, 2019, pp. 10277–10287.
    [doi:10.1109/CVPR.2019.01053](https://doi.org/10.1109/CVPR.2019.01053).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] C. Ledig, L. Theis, F. Huszar, J. Caballero, A. Cunningham, A. Acosta,
    A. Aitken, A. Tejani, J. Totz, Z. Wang, W. Shi, Photo-realistic single image super-resolution
    using a generative adversarial network (2017). [arXiv:1609.04802](http://arxiv.org/abs/1609.04802).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] J. Huang, H. Zhang, L. Yi, T. Funkhouser, M. NieBner, L. Guibas, Texturenet:
    Consistent local parametrizations for learning from high-resolution signals on
    meshes, in: 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR), IEEE, Long Beach, CA, USA, 2019, pp. 4435–4444.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] B. Gong, K. Grauman, F. Sha, Reshaping visual datasets for domain adaptation,
    Advances in Neural Information Processing Systems 26 (2013).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Y. Shen, Y. Yang, M. Yan, H. Wang, Y. Zheng, L. J. Guibas, Domain adaptation
    on point clouds via geometry-aware implicits, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, New Orleans, LA, USA, 2022,
    pp. 7223–7232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] J.-Y. Zhu, T. Park, P. Isola, A. A. Efros, Unpaired image-to-image translation
    using cycle-consistent adversarial networks, in: Proceedings of the IEEE international
    conference on computer vision, IEEE, Venice, Italy, 2017, pp. 2223–2232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] K. Saleh, A. Abobakr, M. Attia, J. Iskander, D. Nahavandi, M. Hossny,
    S. Nahvandi, Domain adaptation for vehicle detection from bird’s eye view lidar
    point cloud data, in: Proceedings of the IEEE/CVF International Conference on
    Computer Vision Workshops, IEEE, Seoul, Korea, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] X. Zhou, A. Karpur, C. Gan, L. Luo, Q. Huang, Unsupervised domain adaptation
    for 3d keypoint estimation via view consistency, in: Proceedings of the European
    conference on computer vision (ECCV), IEEE, Munich, Germany, 2018, pp. 137–153.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Y. Wang, X. Chen, Y. You, L. E. Li, B. Hariharan, M. Campbell, K. Q.
    Weinberger, W.-L. Chao, Train in germany, test in the usa: Making 3d object detectors
    generalize, in: Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition, IEEE, Seattle, WA, USA, 2020, pp. 11713–11723.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] A. Geiger, P. Lenz, C. Stiller, R. Urtasun, Vision meets robotics: The
    kitti dataset, The International Journal of Robotics Research 32 (11) (2013) 1231–1237.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik, P. Tsui,
    J. Guo, Y. Zhou, Y. Chai, B. Caine, et al., Scalability in perception for autonomous
    driving: Waymo open dataset, in: Proceedings of the IEEE/CVF conference on computer
    vision and pattern recognition, IEEE, Seattle, WA, USA, 2020, pp. 2446–2454.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] J. Yang, S. Shi, Z. Wang, H. Li, X. Qi, St3d: Self-training for unsupervised
    domain adaptation on 3d object detection, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, IEEE, Nashville, TN, USA, 2021, pp.
    10368–10378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] M. Jaritz, T.-H. Vu, R. d. Charette, E. Wirbel, P. Pérez, xmuda: Cross-modal
    unsupervised domain adaptation for 3d semantic segmentation, in: Proceedings of
    the IEEE/CVF conference on computer vision and pattern recognition, Seattle, WA,
    USA, 2020, pp. 12605–12614.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] C. Saltori, S. Lathuiliére, N. Sebe, E. Ricci, F. Galasso, Sf-uda 3d:
    Source-free unsupervised domain adaptation for lidar-based 3d object detection,
    in: 2020 International Conference on 3D Vision (3DV), IEEE, Fukuoka, Japan, 2020,
    pp. 771–780.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] A. Cardace, R. Spezialetti, P. Z. Ramirez, S. Salti, L. Di Stefano, Refrec:
    Pseudo-labels refinement via shape reconstruction for unsupervised 3d domain adaptation,
    in: 2021 International Conference on 3D Vision (3DV), IEEE, 2021, pp. 331–341.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] L. Shi, Z. Yuan, M. Cheng, Y. Chen, C. Wang, Dfan: Dual-branch feature
    alignment network for domain adaptation on point clouds, IEEE Transactions on
    Geoscience and Remote Sensing 60 (2022) 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] D. Kang, Y. Nam, D. Kyung, J. Choi, Unsupervised domain adaptation for
    3d point clouds by searched transformations, IEEE Access 10 (2022) 56901–56913.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Y. S. Tang, G. H. Lee, Transferable semi-supervised 3d object detection
    from rgb-d data, in: Proceedings of the IEEE/CVF international conference on computer
    vision, 2019, pp. 1931–1940.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] X. Huang, G. Mei, J. Zhang, Feature-metric registration: A fast semi-supervised
    approach for robust point cloud registration without correspondences, in: Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp.
    11366–11374.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] S. Horache, J.-E. Deschaud, F. Goulette, 3d point cloud registration
    with multi-scale architecture and unsupervised transfer learning, in: 2021 international
    conference on 3D vision (3DV), IEEE, 2021, pp. 1351–1361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] H. Li, Z. Sun, Y. Wu, Y. Song, Semi-supervised point cloud segmentation
    using self-training with label confidence prediction, Neurocomputing 437 (2021)
    227–237.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Z. Chen, L. Jing, Y. Liang, Y. Tian, B. Li, Multimodal semi-supervised
    learning for 3d objects, arXiv preprint arXiv:2110.11601 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] A. Xiao, J. Huang, D. Guan, F. Zhan, S. Lu, Transfer learning from synthetic
    to real lidar point cloud for semantic segmentation, in: Proceedings of the AAAI
    conference on artificial intelligence, Vol. 36, 2022, pp. 2795–2803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] J. Mei, B. Gao, D. Xu, W. Yao, X. Zhao, H. Zhao, Semantic segmentation
    of 3d lidar data in dynamic scene using semi-supervised learning, IEEE Transactions
    on Intelligent Transportation Systems 21 (6) (2019) 2496–2509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] S. Huang, Y. Xie, S.-C. Zhu, Y. Zhu, Spatio-temporal self-supervised
    representation learning for 3d point clouds, in: Proceedings of the IEEE/CVF International
    Conference on Computer Vision, 2021, pp. 6535–6545.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Z. Qin, J. Wang, Y. Lu, Weakly supervised 3d object detection from point
    clouds, in: Proceedings of the 28th ACM International Conference on Multimedia,
    2020, pp. 4144–4152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] P.-C. Yu, C. Sun, M. Sun, Data efficient 3d learner via knowledge transferred
    from 2d model, in: European Conference on Computer Vision, Springer, 2022, pp.
    182–198.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Z. Xu, B. Yuan, S. Zhao, Q. Zhang, X. Gao, Hierarchical point-based active
    learning for semi-supervised point cloud semantic segmentation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 18098–18108.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] D. Zhang, D. Liang, Z. Zou, J. Li, X. Ye, Z. Liu, X. Tan, X. Bai, A simple
    vision transformer for weakly semi-supervised 3d object detection, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 8373–8383.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, J. Shen, Ssda3d: Semi-supervised
    domain adaptation for 3d object detection from point cloud, in: Proceedings of
    the AAAI Conference on Artificial Intelligence, Vol. 37, 2023, pp. 2707–2715.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, Q. He, A
    comprehensive survey on transfer learning, Proceedings of the IEEE 109 (1) (2020)
    43–76.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] S. J. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions
    on knowledge and data engineering 22 (10) (2009) 1345–1359.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Q. Hu, B. Yang, L. Xie, S. Rosa, Y. Guo, Z. Wang, N. Trigoni, A. Markham,
    Randla-net: Efficient semantic segmentation of large-scale point clouds, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, Seattle,
    WA, USA, 2020, pp. 11108–11117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] T. Hackel, N. Savinov, L. Ladicky, J. D. Wegner, K. Schindler, M. Pollefeys,
    Semantic3d. net: A new large-scale point cloud classification benchmark, arXiv
    preprint arXiv:1704.03847 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] C. Zhao, H. Guo, J. Lu, D. Yu, D. Li, X. Chen, Als point cloud classification
    with small training data set based on transfer learning, IEEE Geoscience and Remote
    Sensing Letters 17 (8) (2019) 1406–1410.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Z. Liu, H. Tang, Y. Lin, S. Han, Point-voxel cnn for efficient 3d deep
    learning, Advances in Neural Information Processing Systems 32 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] C. Sun, M. Ma, Z. Zhao, S. Tian, R. Yan, X. Chen, Deep transfer learning
    based on sparse autoencoder for remaining useful life prediction of tool in manufacturing,
    IEEE Transactions on Industrial Informatics 15 (4) (2018) 2416–2425.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] L. Huang, H. Guo, Q. Rao, Z. Hou, S. Li, S. Qiu, X. Fan, H. Wang, Body
    dimension measurements of qinchuan cattle with transfer learning from lidar sensing,
    Sensors 19 (22) (2019) 5046.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] A. Arnold, R. Nallapati, W. W. Cohen, A comparative study of methods
    for transductive transfer learning, in: Seventh IEEE international conference
    on data mining workshops (ICDMW 2007), IEEE, IEEE, Omaha, NE, 2007, pp. 77–82.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] K. Bousmalis, N. Silberman, D. Dohan, D. Erhan, D. Krishnan, Unsupervised
    pixel-level domain adaptation with generative adversarial networks, in: Proceedings
    of the IEEE conference on computer vision and pattern recognition, IEEE, Honolulu,
    HI, USA, 2017, pp. 3722–3731.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] L. Duan, I. W. Tsang, D. Xu, Domain transfer multiple kernel learning,
    IEEE Transactions on Pattern Analysis and Machine Intelligence 34 (3) (2012) 465–479.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] M. Long, H. Zhu, J. Wang, M. I. Jordan, Deep transfer learning with joint
    adaptation networks, in: International conference on machine learning, PMLR, PMLR,
    Sydney, Australia, 2017, pp. 2208–2217.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] W. Zhang, W. Ouyang, W. Li, D. Xu, Collaborative and adversarial network
    for unsupervised domain adaptation, in: Proceedings of the IEEE conference on
    computer vision and pattern recognition, IEEE, Salt Lake City, UT, USA, 2018,
    pp. 3801–3809.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] C. B. Rist, M. Enzweiler, D. M. Gavrila, Cross-sensor deep domain adaptation
    for lidar detection and segmentation, in: 2019 IEEE Intelligent Vehicles Symposium
    (IV), IEEE, 2019, pp. 1535–1542.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Z. Wang, S. Ding, Y. Li, M. Zhao, S. Roychowdhury, A. Wallin, G. Sapiro,
    Q. Qiu, Range adaptation for 3d object detection in lidar, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision Workshops, Seoul, Korea,
    2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Z. Wang, L. Wang, B. Dai, Strong-weak feature alignment for 3d object
    detection, Electronics 10 (10) (2021) 1205.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] W. Zhang, W. Li, D. Xu, Srdan: Scale-aware and range-aware domain adaptation
    network for cross-dataset 3d object detection, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, Nashville, TN, USA, 2021,
    pp. 6769–6779.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] L. Nunes, R. Marcuzzi, X. Chen, J. Behley, C. Stachniss, Segcontrast:
    3d point cloud feature representation learning through self-supervised segment
    discrimination, IEEE Robotics and Automation Letters 7 (2) (2022) 2116–2123.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] W. Liu, Z. Luo, Y. Cai, Y. Yu, Y. Ke, J. M. Junior, W. N. Gonçalves,
    J. Li, Adversarial unsupervised domain adaptation for 3d semantic segmentation
    with multi-modal learning, ISPRS Journal of Photogrammetry and Remote Sensing
    176 (2021) 211–221.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] H. Tang, C. Xu, J. Yang, Bi-adversarial discrepancy minimization for
    unsupervised domain adaptation on 3d point cloud, in: 2021 International Joint
    Conference on Neural Networks (IJCNN), IEEE, IEEE, online, 2021, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] S. Vesal, M. Gu, R. Kosti, A. Maier, N. Ravikumar, Adapt everywhere:
    unsupervised adaptation of point-clouds and entropy minimization for multi-modal
    cardiac image segmentation, IEEE Transactions on Medical Imaging 40 (7) (2021)
    1838–1851.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] M. A. U. Alam, M. M. Rahman, J. Q. Widberg, Palmar: Towards adaptive
    multi-inhabitant activity recognition in point-cloud technology, in: IEEE INFOCOM
    2021-IEEE Conference on Computer Communications, IEEE, IEEE, online, 2021, pp.
    1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Z. Wang, L. Wang, L. Xiao, B. Dai, Unsupervised subcategory domain adaptive
    network for 3d object detection in lidar, Electronics 10 (8) (2021) 927.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Z. Qiao, H. Hu, W. Shi, S. Chen, Z. Liu, H. Wang, A registration-aided
    domain adaptation network for 3d point cloud based place recognition, in: 2021
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), IEEE,
    IEEE, Prague, Czech Republic, 2021, pp. 1317–1322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] X. Zhu, H. Manamasa, J. L. J. Sánchez, A. Maki, L. Hanson, Automatic
    assembly quality inspection based on an unsupervised point cloud domain adaptation
    model, Procedia CIRP 104 (2021) 1801–1806.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] F. Wang, W. Li, D. Xu, Cross-dataset point cloud recognition using deep-shallow
    domain adaptation network, IEEE Transactions on Image Processing 30 (2021) 7364–7377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] M. A. U. Alam, F. Mazzoni, M. M. Rahman, J. Widberg, Lamar: Lidar based
    multi-inhabitant activity recognition, in: MobiQuitous 2020-17th EAI International
    Conference on Mobile and Ubiquitous Systems: Computing, Networking and Services,
    EAI, online, 2020, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] M. Jaritz, T.-H. Vu, R. De Charette, É. Wirbel, P. Pérez, Cross-modal
    learning for domain adaptation in 3d semantic segmentation, IEEE Transactions
    on Pattern Analysis and Machine Intelligence 45 (2) (2022) 1533–1544.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] K. Saleh, A. Abobakr, D. Nahavandi, J. Iskander, M. Attia, M. Hossny,
    S. Nahavandi, Cyclist intent prediction using 3d lidar sensors for fully automated
    vehicles, in: 2019 IEEE Intelligent Transportation Systems Conference (ITSC),
    IEEE, IEEE, Auckland, New Zealand, 2019, pp. 2020–2026.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] M. Pham, S. Alse, C. A. Knoblock, P. Szekely, Semantic labeling: a domain-independent
    approach, in: International Semantic Web Conference, Springer, Springer, Kobe,
    Japan, 2016, pp. 446–462.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] T. Xia, J. Yang, L. Chen, Automated semantic segmentation of bridge point
    cloud based on local descriptor and machine learning, Automation in Construction
    133 (2022) 103992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] R. Sun, X. Zhu, C. Wu, C. Huang, J. Shi, L. Ma, Not all areas are equal:
    Transfer learning for semantic segmentation via hierarchical region selection,
    in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    Long Beach, CA, USA, 2019, pp. 4360–4369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] S. Hong, J. Oh, H. Lee, B. Han, Learning transferrable knowledge for
    semantic segmentation with deep convolutional neural network, in: Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, IEEE, Las Vegas,
    NV, USA, 2016, pp. 3204–3212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] H. A. Arief, U. G. Indahl, G.-H. Strand, H. Tveite, Addressing overfitting
    on point cloud classification using atrous xcrf, ISPRS Journal of Photogrammetry
    and Remote Sensing 155 (2019) 90–101.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] H. Yang, J. Shi, L. Carlone, Teaser: Fast and certifiable point cloud
    registration, IEEE Transactions on Robotics 37 (2) (2020) 314–333.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] S. Choi, Q.-Y. Zhou, V. Koltun, Robust reconstruction of indoor scenes,
    in: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    IEEE, Boston, MA, USA, 2015, pp. 5556–5565.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Y. Wang, J. M. Solomon, Deep closest point: Learning representations
    for point cloud registration, in: Proceedings of the IEEE/CVF international conference
    on computer vision, Seoul, Korea (South), 2019, pp. 3523–3532.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] Y. Chen, J. Liu, B. Ni, H. Wang, J. Yang, N. Liu, T. Li, Q. Tian, Shape
    self-correction for unsupervised point cloud understanding, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada,
    2021, pp. 8382–8391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] R. Zhang, Z. Guo, W. Zhang, K. Li, X. Miao, B. Cui, Y. Qiao, P. Gao,
    H. Li, Pointclip: Point cloud understanding by clip, arXiv preprint arXiv:2112.02413
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] K. Liu, Z. Gao, F. Lin, B. M. Chen, Fg-net: A fast and accurate framework
    for large-scale lidar point cloud understanding, IEEE Transactions on Cybernetics
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] R. Roveri, A. C. Öztireli, I. Pandele, M. Gross, Pointpronets: Consolidation
    of point clouds with convolutional neural networks, in: Computer Graphics Forum,
    Vol. 37, Wiley Online Library, 2018, pp. 87–99.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] S. Luo, W. Hu, Score-based point cloud denoising, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada,
    2021, pp. 4583–4592.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] J. Zeng, G. Cheung, M. Ng, J. Pang, C. Yang, 3d point cloud denoising
    using graph laplacian regularization of a low dimensional manifold model, IEEE
    Transactions on Image Processing 29 (2019) 3474–3489.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] C. Dinesh, G. Cheung, I. V. Bajic, C. Yang, Fast 3d point cloud denoising
    via bipartite graph approximation & total variation, arXiv preprint arXiv:1804.10831
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] C. Duan, S. Chen, J. Kovačević, Weighted multi-projection: 3d point cloud
    denoising with estimated tangent planes, arXiv preprint arXiv:1807.00253 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] W. Hu, X. Gao, G. Cheung, Z. Guo, Feature graph learning for 3d point
    cloud denoising, IEEE Transactions on Signal Processing 68 (2020) 2841–2856.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] V. Badrinarayanan, A. Kendall, R. Cipolla, Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation, IEEE transactions on pattern
    analysis and machine intelligence 39 (12) (2017) 2481–2495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] A. Eltner, P. O. Bressan, T. Akiyama, W. N. Gonçalves, J. Marcato Junior,
    Using deep learning for automatic water stage measurements, Water Resources Research
    57 (3) (2021) e2020WR027608.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] C. Zhang, J. Shi, X. Deng, Z. Wu, Upsampling autoencoder for self-supervised
    point cloud learning, arXiv preprint arXiv:2203.10768 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] L. Garrote, J. Rosa, J. Paulo, C. Premebida, P. Peixoto, U. J. Nunes,
    3d point cloud downsampling for 2d indoor scene modelling in mobile robotics,
    in: 2017 IEEE international conference on autonomous robot systems and competitions
    (ICARSC), IEEE, IEEE, Coimbra, Portugal, 2017, pp. 228–233.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] S. Orts-Escolano, V. Morell, J. García-Rodríguez, M. Cazorla, Point cloud
    data filtering and downsampling using growing neural gas, in: The 2013 International
    Joint Conference on Neural Networks (IJCNN), IEEE, IEEE, Dallas, Texas, USA, 2013,
    pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] E. Nezhadarya, E. Taghavi, R. Razani, B. Liu, J. Luo, Adaptive hierarchical
    down-sampling for point cloud classification, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, Seattle, WA, USA, 2020,
    pp. 12956–12964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] C. Suchocki, W. Błaszczak-Bąk, Down-sampling of point clouds for the
    technical diagnostics of buildings and structures, Geosciences 9 (2) (2019) 70.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] C. Lv, M. Qi, X. Li, Z. Yang, H. Ma, Sgformer: Semantic graph transformer
    for point cloud-based 3d scene graph generation, in: Proceedings of the AAAI Conference
    on Artificial Intelligence, Vol. 38, 2024, pp. 4035–4043.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] Z. Li, Z. Li, Z. Cui, M. Pollefeys, M. R. Oswald, Sat2scene: 3d urban
    scene generation from satellite images with diffusion, in: Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 7141–7150.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] P. Li, C. Tang, Q. Huang, Z. Li, Art3d: 3d gaussian splatting for text-guided
    artistic scenes generation, arXiv preprint arXiv:2405.10508 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] J. Chung, S. Lee, H. Nam, J. Lee, K. M. Lee, Luciddreamer: Domain-free
    generation of 3d gaussian splatting scenes, arXiv preprint arXiv:2311.13384 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Y.-K. Wang, C. Xing, Y.-L. Wei, X.-M. Wu, W.-S. Zheng, Single-view scene
    point cloud human grasp generation, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2024, pp. 831–841.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] S. Koch, P. Hermosilla, N. Vaskevicius, M. Colosi, T. Ropinski, Sgrec3d:
    Self-supervised 3d scene graph learning via object-level scene reconstruction,
    in: Proceedings of the IEEE/CVF Winter Conference on Applications of Computer
    Vision, 2024, pp. 3404–3414.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] Y. Liu, X. Li, X. Li, L. Qi, C. Li, M.-H. Yang, Pyramid diffusion for
    fine 3d large scene generation, arXiv preprint arXiv:2311.12085 (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] T. Mathes, D. Seidel, K.-H. Häberle, H. Pretzsch, P. Annighöfer, What
    are we missing? occlusion in laser scanning point clouds and its impact on the
    detection of single-tree morphologies and stand structural variables, Remote Sensing
    15 (2) (2023) 450.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Y. Zhang, L. Wang, Y. Dai, Plot: a 3d point cloud object detection network
    for autonomous driving, Robotica (2023) 1–17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Y. Himeur, M. Elnour, F. Fadli, N. Meskin, I. Petri, Y. Rezgui, F. Bensaali,
    A. Amira, Next-generation energy systems for sustainable smart cities: Roles of
    transfer learning, Sustainable Cities and Society (2022) 104059.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] C. Romanengo, A. Raffo, S. Biasotti, B. Falcidieno, Recognising geometric
    primitives in 3d point clouds of mechanical cad objects, Computer-Aided Design
    (2023) 103479.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] Y. Himeur, S. Al-Maadeed, I. Varlamis, N. Al-Maadeed, K. Abualsaud, A. Mohamed,
    Face mask detection in smart cities using deep and transfer learning: Lessons
    learned from the covid-19 pandemic, Systems 11 (2) (2023) 107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] A. Argyriou, A. Maurer, M. Pontil, An algorithm for transfer learning
    in a heterogeneous environment, in: Joint European Conference on Machine Learning
    and Knowledge Discovery in Databases, Springer, Springer, Antwerp, Belgium, 2008,
    pp. 71–85.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] R. Gong, D. Dai, Y. Chen, W. Li, L. Van Gool, mdalu: Multi-source domain
    adaptation and label unification with partial datasets, in: Proceedings of the
    IEEE/CVF International Conference on Computer Vision, Montreal, BC, Canada, 2021,
    pp. 8876–8885.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] L. Wang, X. Geng, X. Ma, D. Zhang, Q. Yang, Ridesharing car detection
    by transfer learning, Artificial Intelligence 273 (2019) 1–18.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] P. Morerio, J. Cavazza, V. Murino, Minimal-entropy correlation alignment
    for unsupervised deep domain adaptation, arXiv preprint arXiv:1711.10288 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[283] M. Liang, B. Yang, Y. Chen, R. Hu, R. Urtasun, Multi-task multi-sensor
    fusion for 3d object detection, in: Proceedings of the IEEE/CVF Conference on
    Computer Vision and Pattern Recognition, Long Beach, CA, USA, 2019, pp. 7345–7353.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[284] A. Valada, R. Mohan, W. Burgard, Self-supervised model adaptation for
    multimodal semantic segmentation, International Journal of Computer Vision 128 (5)
    (2020) 1239–1285.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[285] Y. Himeur, S. Al-Maadeed, N. Almadeed, K. Abualsaud, A. Mohamed, T. Khattab,
    O. Elharrouss, Deep visual social distancing monitoring to combat covid-19: A
    comprehensive survey, Sustainable Cities and Society (2022) 104064.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[286] G. Mateo-García, V. Laparra, D. López-Puigdollers, L. Gómez-Chova, Transferring
    deep learning models for cloud detection between landsat-8 and proba-v, ISPRS
    Journal of Photogrammetry and Remote Sensing 160 (2020) 1–17.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[287] X. Ling, R. Qin, A graph-matching approach for cross-view registration
    of over-view and street-view based point clouds, ISPRS Journal of Photogrammetry
    and Remote Sensing 185 (2022) 2–15.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[288] X. Glorot, A. Bordes, Y. Bengio, Domain adaptation for large-scale sentiment
    classification: A deep learning approach, in: ICML, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[289] W. Hu, Y. Luo, Z. Lu, Y. Wen, Heterogeneous transfer learning for thermal
    comfort modeling, in: Proceedings of the 6th ACM International Conference on Systems
    for Energy-Efficient Buildings, Cities, and Transportation, ACM, New York, NY,
    USA, 2019, pp. 61–70.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[290] C. Fan, Y. Sun, F. Xiao, J. Ma, D. Lee, J. Wang, Y. C. Tseng, Statistical
    investigations of transfer learning-based methodology for short-term building
    energy predictions, Applied Energy 262 (2020) 114499.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[291] J. Lin, J. Ma, J. Zhu, H. Liang, Deep domain adaptation for non-intrusive
    load monitoring based on a knowledge transfer learning network, IEEE Transactions
    on Smart Grid (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[292] N. Patricia, B. Caputo, Learning to learn, from transfer learning to
    domain adaptation: A unifying perspective, in: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, IEEE, Columbus, OH, USA, 2014, pp.
    1442–1449.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[293] N. Djeffal, H. Kheddar, D. Addou, A. C. Mazari, Y. Himeur, Automatic
    speech recognition with bert and ctc transformers: A review, in: 2023 2nd International
    Conference on Electronics, Energy and Measurement (IC2EM), Vol. 1, IEEE, 2023,
    pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[294] G. Qian, X. Zhang, A. Hamdi, B. Ghanem, Pix4point: Image pretrained transformers
    for 3d point cloud understanding, arXiv preprint arXiv:2208.12259 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[295] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep
    bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[296] Q.-H. Pham, T. Nguyen, B.-S. Hua, G. Roig, S.-K. Yeung, Jsis3d: Joint
    semantic-instance segmentation of 3d point clouds with multi-task pointwise networks
    and multi-value conditional random fields, in: Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, 2019, pp. 8827–8836.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[297] X. Zou, K. Li, Y. Li, W. Wei, C. Chen, Multi-task y-shaped graph neural
    network for point cloud learning in autonomous driving, IEEE Transactions on Intelligent
    Transportation Systems 23 (7) (2022) 9568–9579.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[298] F. Chen, F. Wu, G. Gao, Y. Ji, J. Xu, G.-P. Jiang, X.-Y. Jing, Jspnet:
    Learning joint semantic & instance segmentation of point clouds via feature self-similarity
    and cross-task probability, Pattern Recognition 122 (2022) 108250.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[299] D. Ye, Z. Zhou, W. Chen, Y. Xie, Y. Wang, P. Wang, H. Foroosh, Lidarmultinet:
    Towards a unified multi-task network for lidar perception, in: Proceedings of
    the AAAI Conference on Artificial Intelligence, Vol. 37, 2023, pp. 3231–3240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[300] A. Dubey, A. Santra, J. Fuchs, M. Lübke, R. Weigel, F. Lurz, Haradnet:
    Anchor-free target detection for radar point clouds using hierarchical attention
    and multi-task learning, Machine Learning with Applications 8 (2022) 100275.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[301] K. Hassani, M. Haley, Unsupervised multi-task feature learning on point
    clouds, in: Proceedings of the IEEE/CVF International Conference on Computer Vision,
    2019, pp. 8160–8171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[302] X. Lin, H. Luo, W. Guo, C. Wang, J. Li, A multi-task learning framework
    for semantic segmentation in mls point clouds, in: International Conference on
    Adaptive and Intelligent Systems, Springer, 2022, pp. 382–392.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[303] L. Zhao, Y. Hu, X. Yang, Z. Dou, L. Kang, Robust multi-task learning
    network for complex lidar point cloud data preprocessing, Expert Systems with
    Applications 237 (2024) 121552.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[304] T. Rios, B. van Stein, T. Bäck, B. Sendhoff, S. Menzel, Multitask shape
    optimization using a 3-d point cloud autoencoder as unified representation, IEEE
    Transactions on Evolutionary Computation 26 (2) (2021) 206–217.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[305] D. Feng, Y. Zhou, C. Xu, M. Tomizuka, W. Zhan, A simple and efficient
    multi-task network for 3d object detection and road understanding, in: 2021 IEEE/RSJ
    International Conference on Intelligent Robots and Systems (IROS), IEEE, 2021,
    pp. 7067–7074.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[306] J. Rebut, A. Ouaknine, W. Malik, P. Pérez, Raw high-definition radar
    for multi-task learning, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2022, pp. 17021–17030.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[307] Z. Shan, Q. Yang, R. Ye, Y. Zhang, Y. Xu, X. Xu, S. Liu, Gpa-net: No-reference
    point cloud quality assessment with multi-task graph convolutional network, IEEE
    Transactions on Visualization and Computer Graphics (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[308] A. Hatem, Y. Qian, Y. Wang, Point-tta: Test-time adaptation for point
    cloud registration using multitask meta-auxiliary learning, in: Proceedings of
    the IEEE/CVF International Conference on Computer Vision, 2023, pp. 16494–16504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[309] C. Zhang, H. Fan, An improved multi-task pointwise network for segmentation
    of building roofs in airborne laser scanning point clouds, The Photogrammetric
    Record 37 (179) (2022) 260–284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[310] G. Wei, L. Ma, C. Wang, C. Desrosiers, Y. Zhou, Multi-task joint learning
    of 3d keypoint saliency and correspondence estimation, Computer-Aided Design 141
    (2021) 103105.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[311] H. Zhang, S. Zhang, Y. Zhang, J. Liang, Z. Wang, Machining feature recognition
    based on a novel multi-task deep learning network, Robotics and Computer-Integrated
    Manufacturing 77 (2022) 102369.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[312] M. Afham, I. Dissanayake, D. Dissanayake, A. Dharmasiri, K. Thilakarathna,
    R. Rodrigo, Crosspoint: Self-supervised cross-modal contrastive learning for 3d
    point cloud understanding, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2022, pp. 9902–9912.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[313] X. Yan, H. Zhan, C. Zheng, J. Gao, R. Zhang, S. Cui, Z. Li, Let images
    give you more: Point cloud cross-modal training for shape analysis, Advances in
    Neural Information Processing Systems 35 (2022) 32398–32411.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[314] Y. Wu, J. Liu, M. Gong, P. Gong, X. Fan, A. Qin, Q. Miao, W. Ma, Self-supervised
    intra-modal and cross-modal contrastive learning for point cloud understanding,
    IEEE Transactions on Multimedia (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[315] J. Zhang, H. Yang, D.-J. Wu, J. Keung, X. Li, X. Zhu, Y. Ma, Cross-modal
    and cross-domain knowledge transfer for label-free 3d segmentation, in: Chinese
    Conference on Pattern Recognition and Computer Vision (PRCV), Springer, 2023,
    pp. 465–477.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[316] L. Jing, Y. Xue, X. Yan, C. Zheng, D. Wang, R. Zhang, Z. Wang, H. Fang,
    B. Zhao, Z. Li, X4d-sceneformer: Enhanced scene understanding on 4d point cloud
    videos through cross-modal knowledge transfer, in: Proceedings of the AAAI Conference
    on Artificial Intelligence, Vol. 38, 2024, pp. 2670–2678.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[317] Q. Zhang, J. Hou, Y. Qian, Pointmcd: Boosting deep point cloud encoders
    via multi-view cross-modal distillation for 3d shape recognition, IEEE Transactions
    on Multimedia (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[318] P. Falco, S. Lu, C. Natale, S. Pirozzi, D. Lee, A transfer learning approach
    to cross-modal object recognition: from visual observation to robotic haptic exploration,
    IEEE Transactions on Robotics 35 (4) (2019) 987–998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[319] P. K. Murali, C. Wang, D. Lee, R. Dahiya, M. Kaboli, Deep active cross-modal
    visuo-tactile transfer learning for robotic object recognition, IEEE Robotics
    and Automation Letters 7 (4) (2022) 9557–9564.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[320] X. Shen, I. Stamos, Simcrosstrans: A simple cross-modality transfer learning
    for object detection with convnets or vision transformers, arXiv preprint arXiv:2203.10456
    (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[321] L. Jing, E. Vahdani, J. Tan, Y. Tian, Cross-modal center loss for 3d
    cross-modal retrieval, in: Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, 2021, pp. 3142–3151.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[322] Z. Zhu, L. Nan, H. Xie, H. Chen, J. Wang, M. Wei, J. Qin, Csdn: Cross-modal
    shape-transfer dual-refinement network for point cloud completion, IEEE Transactions
    on Visualization and Computer Graphics (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[323] M. Li, Y. Zhang, Y. Xie, Z. Gao, C. Li, Z. Zhang, Y. Qu, Cross-domain
    and cross-modal knowledge distillation in domain adaptation for 3d semantic segmentation,
    in: Proceedings of the 30th ACM International Conference on Multimedia, 2022,
    pp. 3829–3837.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[324] D. Peng, Y. Lei, W. Li, P. Zhang, Y. Guo, Sparse-to-dense feature matching:
    Intra and inter domain cross-modal learning in domain adaptation for 3d semantic
    segmentation, in: Proceedings of the IEEE/CVF International Conference on Computer
    Vision, 2021, pp. 7108–7117.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[325] J. Nitsch, J. Nieto, R. Siegwart, M. Schmidt, C. Cadena, Learning common
    and transferable feature representations for multi-modal data, in: 2020 IEEE Intelligent
    Vehicles Symposium (IV), IEEE, 2020, pp. 1601–1607.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[326] M. Li, Y. Zhang, X. Ma, Y. Qu, Y. Fu, Bev-dg: Cross-modal learning under
    bird’s-eye view for domain generalization of 3d semantic segmentation, in: Proceedings
    of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 11632–11642.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[327] P. Tang, H.-M. Xu, C. Ma, Prototransfer: Cross-modal prototype transfer
    for point cloud segmentation, in: Proceedings of the IEEE/CVF International Conference
    on Computer Vision, 2023, pp. 3337–3347.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[328] B. Xing, X. Ying, R. Wang, J. Yang, T. Chen, Cross-modal contrastive
    learning for domain adaptation in 3d semantic segmentation, in: Proceedings of
    the AAAI Conference on Artificial Intelligence, Vol. 37, 2023, pp. 2974–2982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[329] Z. Yuan, X. Yan, Y. Liao, Y. Guo, G. Li, S. Cui, Z. Li, X-trans2cap:
    Cross-modal knowledge transfer using transformer for 3d dense captioning, in:
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    2022, pp. 8563–8573.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[330] H. Zhou, X. Peng, Y. Luo, Z. Wu, Pointcmc: Cross-modal multi-scale correspondences
    learning for point cloud understanding (2023).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[331] X. Zheng, X. Huang, G. Mei, Y. Hou, Z. Lyu, B. Dai, W. Ouyang, Y. Gong,
    Point cloud pre-training with diffusion models, in: Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, 2024, pp. 22935–22945.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[332] Y. Kasten, O. Rahamim, G. Chechik, Point cloud completion with pretrained
    text-to-image diffusion models, Advances in Neural Information Processing Systems
    36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[333] C. Liu, A. Jiang, Y. Tang, Y. Zhu, Q. Chen, 3d point cloud semantic segmentation
    based on diffusion model, in: ICASSP 2024-2024 IEEE International Conference on
    Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2024, pp. 4375–4379.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[334] H. Jiang, M. Salzmann, Z. Dang, J. Xie, J. Yang, Se (3) diffusion model-based
    point cloud registration for robust 6d object pose estimation, Advances in Neural
    Information Processing Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[335] S. Jin, I. Armeni, M. Pollefeys, D. Barath, Multiway point cloud mosaicking
    with diffusion and global optimization, in: Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, 2024, pp. 20838–20849.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[336] Y. Feng, X. Shi, M. Cheng, Y. Xiong, Diffpoint: Single and multi-view
    point cloud reconstruction with vit based diffusion model, arXiv preprint arXiv:2402.11241
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[337] G. Sharma, C. Gupta, A. Agarwal, L. Sharma, A. Dhall, Generating point
    cloud augmentations via class-conditioned diffusion model, in: Proceedings of
    the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024, pp. 480–488.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[338] S. Mo, E. Xie, R. Chu, L. Hong, M. Niessner, Z. Li, Dit-3d: Exploring
    plain diffusion transformers for 3d shape generation, Advances in Neural Information
    Processing Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[339] T. Yi, J. Fang, J. Wang, G. Wu, L. Xie, X. Zhang, W. Liu, Q. Tian, X. Wang,
    Gaussiandreamer: Fast generation from text to 3d gaussians by bridging 2d and
    3d diffusion models, in: Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, 2024, pp. 6796–6807.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[340] C.-J. Ho, C.-H. Tai, Y.-Y. Lin, M.-H. Yang, Y.-H. Tsai, Diffusion-ss3d:
    Diffusion model for semi-supervised 3d object detection, Advances in Neural Information
    Processing Systems 36 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[341] M. Ohno, R. Ukyo, T. Amano, H. Rizk, H. Yamaguchi, Privacy-preserving
    pedestrian tracking with path image inpainting and 3d point cloud features, Pervasive
    and Mobile Computing 100 (2024) 101914.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[342] X. Bi, Y. Chen, L. Li, Diffusionemis: Diffusion model for 3d electromagnetic
    inverse scattering, IEEE Transactions on Geoscience and Remote Sensing (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[343] N. S. Dutt, S. Muralikrishnan, N. J. Mitra, Diffusion 3d features (diff3f):
    Decorating untextured shapes with distilled semantic features, in: Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp.
    4494–4504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[344] Z. Li, R. Mrad, R. Jiao, G. Huang, J. Shan, S. Chu, Y. Chen, Generative
    design of crystal structures by point cloud representations and diffusion model,
    arXiv preprint arXiv:2401.13192 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[345] Y. Ze, G. Zhang, K. Zhang, C. Hu, M. Wang, H. Xu, 3d diffusion policy,
    arXiv preprint arXiv:2403.03954 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[346] R. She, Q. Kang, S. Wang, W. P. Tay, K. Zhao, Y. Song, T. Geng, Y. Xu,
    D. N. Navarro, A. Hartmannsgruber, Pointdifformer: Robust point cloud registration
    with neural diffusion and transformer, IEEE Transactions on Geoscience and Remote
    Sensing (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[347] B. Li, X. Wei, B. Liu, W. Wang, Z.-F. He, Y.-K. Lai, 3d colored object
    reconstruction from a single view image through diffusion, Expert Systems with
    Applications 252 (2024) 124225.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[348] Z. Chen, Y. Wang, F. Wang, Z. Wang, H. Liu, V3d: Video diffusion models
    are effective 3d generators, arXiv preprint arXiv:2403.06738 (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[349] J. Hu, B. Fei, B. Xu, F. Hou, W. Yang, S. Wang, N. Lei, C. Qian, Y. He,
    Topology-aware latent diffusion for 3d shape generation, arXiv preprint arXiv:2401.17603
    (2024).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[350] Y. Dong, Q. Zuo, X. Gu, W. Yuan, Z. Zhao, Z. Dong, L. Bo, Q. Huang, Gpld3d:
    Latent diffusion of 3d shape generative models by enforcing geometric and physical
    priors, in: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, 2024, pp. 56–66.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
