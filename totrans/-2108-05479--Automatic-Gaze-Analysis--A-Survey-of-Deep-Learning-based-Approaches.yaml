- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:52:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2108.05479] Automatic Gaze Analysis: A Survey of Deep Learning based Approaches'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.05479](https://ar5iv.labs.arxiv.org/html/2108.05479)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Automatic Gaze Analysis: A Survey of Deep Learning based Approaches'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Shreya Ghosh, Abhinav Dhall, Munawar Hayat, Jarrod Knibbe, Qiang Ji S. Ghosh
    and M. Hayat are with Monash University. (E-mail: {shreya.ghosh, munawar.hayat}@monash.edu).
    A. Dhall is with Monash University and Indian Institute of Technology Ropar, India.
    (E-mail: abhinav.dhall@monash.edu). J. Knibbe is with the University of Melbourne.
    (E-mail: jarrod.knibbe@unimelb.edu.au). Q. Ji is with the Rensselaer Polytechnic
    Institute (E-mail: jiq@rpi.edu).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Eye gaze analysis is an important research problem in the field of Computer
    Vision and Human-Computer Interaction. Even with notable progress in the last
    10 years, automatic gaze analysis still remains challenging due to the uniqueness
    of eye appearance, eye-head interplay, occlusion, image quality, and illumination
    conditions. There are several open questions, including what are the important
    cues to interpret gaze direction in an unconstrained environment without prior
    knowledge and how to encode them in real-time. We review the progress across a
    range of gaze analysis tasks and applications to elucidate these fundamental questions,
    identify effective methods in gaze analysis, and provide possible future directions.
    We analyze recent gaze estimation and segmentation methods, especially in the
    unsupervised and weakly supervised domain, based on their advantages and reported
    evaluation metrics. Our analysis shows that the development of a robust and generic
    gaze analysis method still needs to address real-world challenges such as unconstrained
    setup and learning with less supervision. We conclude by discussing future research
    directions for designing a real-world gaze analysis system that can propagate
    to other domains including Computer Vision, Augmented Reality (AR), Virtual Reality
    (VR), and Human Computer Interaction (HCI). Project Page: [https://github.com/i-am-shreya/EyeGazeSurvey](https://github.com/i-am-shreya/EyeGazeSurvey)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Gaze Analysis, Automated Gaze Estimation, Eye Segmentation, Gaze Tracking, Unsupervised
    and Self-supervised Gaze Analysis, Human Computer Interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Humans perceive their environment through voluntary or involuntary eye movement
    to receive, fixate and track visual stimuli, or in response to an auditory, or
    cognitive stimulus. The eye movements therefore can provide insights into our
    visual attention [[1](#bib.bib1)] and cognition (emotions, beliefs and desires) [[2](#bib.bib2)].
    Furthermore, we rely on these insights extensively in day-to-day communication
    and social interaction.
  prefs: []
  type: TYPE_NORMAL
- en: 'Automatic gaze analysis develops techniques to estimate the position of target
    objects by observing the eyes’ movement. However, accurate gaze analysis is a
    complex problem. An accurate method should be able to disentangle gaze, while
    being resilient to a broad array of challenges, including: eye-head interplay,
    illumination variations, eye registration errors, occlusions, and identity bias.
    Furthermore, research [[3](#bib.bib3)] has shown how human gaze follows an arbitrary
    trajectory during eye movements which poses further challenge in gaze estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Research in gaze analysis mainly involves coarse or fine-grained gaze estimation.
    There are three aspects of gaze analysis: registration, representation, and recognition.
    The first step, registration, involves the detection of the eyes (or eye-related
    key points or sometimes even just the face). In the second step, representation,
    the detected eye is projected to a meaningful feature space. In the final stage,
    recognition, the corresponding gaze direction or gaze location is predicted based
    on the features from stage 2. Research interest in automatic gaze analysis spans
    in several disciplines. One of the earliest explorations of gaze analysis was
    conducted in 1879, when Javal et al. [[4](#bib.bib4)] first studied, and coined
    the term, saccades. The broader interest in gaze analysis, however, developed
    with the advent of eye tracking technologies (initially in 1908, before gaining
    momentum in the late 70s, with systems such as ‘Purkinje Image’[[5](#bib.bib5)],
    ‘Bright Pupil’ [[6](#bib.bib6)]). Automated gaze analysis then gained traction
    in computer vision-related assistive technology [[7](#bib.bib7), [8](#bib.bib8)],
    which then propagated through HCI [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)],
    consumer behavior analysis [[12](#bib.bib12)], AR and VR [[13](#bib.bib13), [14](#bib.bib14)],
    egocentric vision [[15](#bib.bib15)], biometric systems [[16](#bib.bib16)] and
    other domains [[17](#bib.bib17), [18](#bib.bib18)]. A brief chronology of the
    seminal gaze analysis methods with important milestones is presented in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches"). The increased reliance on gaze tracking technologies, however,
    came with its own challenges, namely, the cost of such devices and the requirement
    for specific controlled settings. To overcome their limitations and handle generic
    unconstrained settings [[19](#bib.bib19), [20](#bib.bib20)], most traditional
    gaze analysis models rely on handcrafted low-level features (e.g., color [[21](#bib.bib21)],
    shape [[22](#bib.bib22), [21](#bib.bib21)] and appearance [[23](#bib.bib23)])
    and certain geometrical heuristics [[20](#bib.bib20)]. Since 2015, the approach
    to gaze analysis has changed, turning to the deep learning [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)], similar to other computer vision tasks. With
    the deep learning based models and the availability of the large training datasets,
    the challenges associated with the variation in lighting, camera setup, eye-head
    interplay, etc, are reduced greatly over the past few years. Although, these performance
    enhancements have come with the requirement of large scale annotated data, which
    is expensive to acquire. As such, more recently, deep learning with limited annotation
    has gained increasing popularity  [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b08fa6f37b120a0ce12db598381e7744.png)'
  prefs: []
  type: TYPE_IMG
- en: <svg version="1.1" width="684.93" height="70.57" overflow="visible"><g transform="translate(0,70.57)
    scale(1,-1)"><g transform="translate(-657.26,17.3)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="79.0092707900927" height="7.3336100733361"
    overflow="visible">[[4](#bib.bib4)]</foreignobject></g></g><g transform="translate(-642.04,89.94)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="105.853051058531"
    height="7.3336100733361" overflow="visible">[[30](#bib.bib30)]</foreignobject></g></g><g
    transform="translate(-560.4,89.94)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="108.205341082053" height="7.3336100733361"
    overflow="visible">[[6](#bib.bib6)]</foreignobject></g></g><g transform="translate(-575.2,31.13)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="116.645911166459"
    height="7.3336100733361" overflow="visible">[[5](#bib.bib5)]</foreignobject></g></g><g
    transform="translate(-453.85,30.44)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="82.3301508233015" height="7.3336100733361"
    overflow="visible">[[22](#bib.bib22)]</foreignobject></g></g><g transform="translate(-437.25,85.79)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="76.3802407638024"
    height="7.3336100733361" overflow="visible">[[19](#bib.bib19)]</foreignobject></g></g><g
    transform="translate(-377.75,35.98)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="116.645911166459" height="7.3336100733361"
    overflow="visible">[[31](#bib.bib31)]</foreignobject></g></g><g transform="translate(-332.09,80.25)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="81.9150408191504"
    height="7.3336100733361" overflow="visible">[[23](#bib.bib23)]</foreignobject></g></g><g
    transform="translate(-304.41,35.98)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="102.808911028089" height="7.3336100733361"
    overflow="visible">[[20](#bib.bib20)]</foreignobject></g></g><g transform="translate(-239.38,31.83)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="112.218071122181"
    height="7.3336100733361" overflow="visible">[[24](#bib.bib24)]</foreignobject></g></g><g
    transform="translate(-182.65,29.06)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="78.3174207831742" height="7.3336100733361"
    overflow="visible">[[26](#bib.bib26)]</foreignobject></g></g><g transform="translate(-232.46,87.17)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="79.4243807942438"
    height="7.3336100733361" overflow="visible">[[25](#bib.bib25)]</foreignobject></g></g><g
    transform="translate(-172.96,83.02)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="73.1977307319773" height="7.3336100733361"
    overflow="visible">[[29](#bib.bib29)]</foreignobject></g></g><g transform="translate(-115.4,83.02)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="105.299571052996"
    height="7.3336100733361" overflow="visible">[[28](#bib.bib28)]</foreignobject></g></g><g
    transform="translate(-55.35,83.02)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="100.456621004566" height="7.3336100733361"
    overflow="visible">[[32](#bib.bib32)]</foreignobject></g></g><g transform="translate(-103.78,27.67)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="113.878511138785"
    height="7.3336100733361" overflow="visible">[[33](#bib.bib33)]</foreignobject></g></g><g
    transform="translate(-85.24,8.03)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="126.746921267469" height="7.3336100733361"
    overflow="visible">[[34](#bib.bib34)]</foreignobject></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: A brief chronology of seminal gaze analysis works. The very first
    gaze pattern modelling dates back to the work of Javal et al. in 1879 [[4](#bib.bib4)].
    One of the first deep learning driven appearance based gaze estimation models
    was proposed in $2015$ [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper surveys different gaze analysis methods by isolating their fundamental
    components, and discusses how each component addresses the aforementioned challenges
    in gaze analysis. The paper discusses new trends and developments in the field
    of computer vision and the AR/VR domain, from the perspective of gaze analysis.
    We cover recent gaze analysis techniques in the un-, self-, and weakly-supervised
    domain, along with validation protocols with evaluation metrics tailored for gaze
    analysis. We also discuss various data capturing devices, including: RGB/IR camera,
    tablet/laptop’s camera, ladybug camera and other gaze trackers (including video-oculography [[21](#bib.bib21)])
    are also discussed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Due to the rapid progress in the computer vision field (Refer Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")), it is increasingly useful to get thorough guidance via exhaustive
    survey/review articles. In 2010 and 2013, Hansen et al. [[21](#bib.bib21)] and
    Chennamma et al. [[35](#bib.bib35)] reviewed the state-of-the-art eye detection
    and gaze tracking techniques. These reviews provide a holistic view of hardware,
    user interface, eye detection, and gaze mapping techniques. Since these reviews
    were before the deep learning era, they contain the relevant features leveraged
    from handcrafted techniques. Afterwards in 2016, Jing et al. [[36](#bib.bib36)]
    reviewed methods for 2-D and 3-D gaze estimation methods. In 2017, Kar et al. [[37](#bib.bib37)]
    provided insights into the issues related to algorithms, system configurations,
    and user conditions. In 2020, a more comprehensive and detailed study of deep
    learning based gaze estimation methods is presented by Cazzato et al. [[38](#bib.bib38)].
    To date, however, no comprehensive review has examined the recent trends in learning
    from less supervision. Moreover, all of the existing reviews focus only on gaze
    estimation and ignore significant works in eye segmentation, gaze zone estimation,
    gaze trajectory prediction, gaze redirection, and unconstrained gaze estimation
    in single and multiperson setting. The contributions of the paper are summarized
    below:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A comprehensive review of automated gaze analysis. We categorize and summarize
    existing methods by considering data capturing sensors, platforms, popular gaze
    estimation tasks in computer vision, level of supervision and learning paradigm.
    The proposed taxonomies aim to help researchers to get a deeper understanding
    of the key components in gaze analysis.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Different popular tasks under one framework. To the best of our knowledge, we
    are the first to put different eye and gaze related popular tasks under one framework.
    Apart from gaze estimation, we consider gaze trajectory, gaze zone estimation
    and gaze redirection tasks.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applications. We explore major applications of gaze analysis using computer
    vision i.e. Augmented and Virtual Reality [[13](#bib.bib13), [39](#bib.bib39)],
    Driver Engagement [[40](#bib.bib40), [41](#bib.bib41)] and Healthcare [[42](#bib.bib42),
    [43](#bib.bib43)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Privacy Concerns. We also provide a brief review of the privacy concerns of
    the gaze data and its possible implications.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Overview of open questions and potential research directions. We review several
    issues associated with the current gaze analysis frameworks (i.e. model design,
    dataset collection, etc.) and discuss possible future research directions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/62a2aab09d3bbbf5da4340ea057672e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Top Left: Overview of the human visual system, eye modelling and
    eye movement. For computer vision based automated gaze analysis, we consider an
    image containing eyes (left) as input. Thus, such methods analyze the visible
    eye regions (middle) and predict the 2-D/3-D gaze vector as output. However, there
    are unobservable factors by which we can predict the true gaze direction (right)
    which requires person-specific information and other factors [[44](#bib.bib44)].
    Bottom Left: Apart from static image based gaze estimation, the dynamic eye movement
    is another line of research in computer vision that provides cues regarding human
    behavioural traits. Right: The actual modelling of gaze with respect to eye anatomy.
    We only highlight the relevant parts i.e. pupil, cornea, iris, sclera, fovea,
    LOS and LOG. The angle between LOG and LOS is called angle of kappa ($\kappa$).'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The human visual system is a complex cognitive process. As such, understanding
    and modelling human gaze has become a fundamental research problem in psychology,
    neurology, cognitive science, and computer vision. To lay the foundations for
    this review, below, we provide brief descriptions of the Human Visual System and
    Eye Modelling (Sec. [2.1](#S2.SS1 "2.1 Human Visual System and Eye Modelling ‣
    2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")),
    Eye movements (Sec. [2.2](#S2.SS2 "2.2 Eye Movements and Foveal Vision ‣ 2 Preliminaries
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")), Problem
    Settings in Automated Gaze Analysis (Sec. [2.3](#S2.SS3 "2.3 Gaze Estimation:
    Problem Setting ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep
    Learning based Approaches")) and the associated Challenges (Sec. [2.4](#S2.SS4
    "2.4 Challenges ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep
    Learning based Approaches")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Human Visual System and Eye Modelling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Computer vision based human visual perception methods estimate gaze quantitatively
    from image or video data. These methods analyze the visible region of the eyes
    (the iris and sclera, see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")), and attempt to approximate
    the unobservable features of the eyes (which are integral to determining gaze
    direction). These approximations can be based on models of the human eyes, derived
    from movement patterns over time, or learned via representation learning over
    large scale data. For gaze estimation, we typically approximate the eye as a sphere
    with a radius of 12-13mm. Subsequently, we model gaze direction with respect to
    the optical axis, also called the line of gaze (LoG), or the visual axis, the
    line of sight (LoS) (see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")right). The line of
    gaze (LoG) connects the pupil, cornea and eyeball center. Conversely, the line
    of sight (LoS) is the line connecting the fovea and center of the cornea. Generally,
    the LoS is considered as the true direction of gaze. The intersection point of
    the visual and optical axis is called the nodal point of eye (anatomically the
    cornea center), which typically encodes a subject dependent angular offset. This
    individual offset is the main motivation behind having subject dependent calibration
    for gaze tracking devices. According to prior studies [[45](#bib.bib45), [46](#bib.bib46)],
    the fovea is located around 4-5° horizontally and 1.5°  vertically below the optical
    axis. Across a broader population, this can vary up to 3° between subjects [[46](#bib.bib46)].
    Additionally, head-pose also plays an important role in gaze analysis. The coarse
    gaze direction of a subject can be determined by the position (in 3-D coordinate)
    and orientation (Euler angles) of the headpose [[21](#bib.bib21)]. Most of the
    time, the combined direction of LoS and head pose provide information about where
    the person is looking.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Eye Movements and Foveal Vision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We perceive our environment through eye movements. These movements can be voluntary
    or involuntary, and help us to acquire, fixate, and track visual stimuli (see
    Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches")). Eye movements are divided into three primary
    categories: saccades, smooth pursuits, and fixations.'
  prefs: []
  type: TYPE_NORMAL
- en: Saccades. Saccades are rapid and reflexive eye movements, primarily used for
    adjustment to a new location in the visual environment. It can be executed voluntarily
    or involuntarily, as a part of optokinetic measure [[47](#bib.bib47)]. Saccades
    typically last for between 10 and 100 ms.
  prefs: []
  type: TYPE_NORMAL
- en: Smooth Pursuits. Smooth pursuit occur while tracking a moving target. This involuntary
    action depends on the range of the target’s motion as astonishingly, human eyes
    can follow the velocity of a moving target to some extend.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fixations (Microsaccades, Drifts, and Tremors). Fixations are eye movements
    in which the focus of attention stabilizes over a stationary object of interest.
    Fixations are characterized by three types of miniature eye movements: tremor,
    drift and microsaccades [[47](#bib.bib47)]. During Fixations, the miniature eye
    movements occur due to the noise present in the control system to hold gaze steadily.
    This noise occurs in the area of fixation, around 5° visual angle. For simplification
    of the underlying natural process, this noise is ignored during fixation.'
  prefs: []
  type: TYPE_NORMAL
- en: Foveal Vision. The fovea centralis region of human eye is responsible for the
    perception of sharp and high resolution human vision. In order to perceive the
    environment, it is necessary to direct the foveal vision to select region of interest
    (the process is termed as ‘foveation’). This sharp foveal vision decays rapidly
    within the range of 1-5°. Beyond this limit, human vision is blurred, and low
    resolution. This is termed as peripheral vision. Our peripheral vision plays an
    important role in our overall visual experience, especially for motion detection.
    On an abstract level, our visual perception is the result of our brains merging
    our foveal and peripheral vision.
  prefs: []
  type: TYPE_NORMAL
- en: '2.3 Gaze Estimation: Problem Setting'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main task of gaze estimation is to determine the line of sight of the pupil.
    Fig. [4](#S2.F4 "Figure 4 ‣ 2.3 Gaze Estimation: Problem Setting ‣ 2 Preliminaries
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches") depicts
    a typical visual sensor based real-time gaze estimation setup consisting of user,
    data capturing sensor(s) and visual plane. The main calibration factors in this
    setting are:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimation of camera calibration parameters, which include both intrinsic and
    extrinsic camera parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimation of geometric calibration parameters, which include the relative position
    of the camera, light source and screen.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Estimation of personal calibration, which include headpose and eye-specific
    parameters such as cornea curvature, the nodal point of the eye, etc.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In some of the applications, the calibration parameters are estimated in task-specific
    settings. For example, users are requested to fixate their gaze to some pre-defined
    points for calibration. Similarly, user-specific information is registered once
    in the devices for subject-specific calibrations. With the advances in computer
    vision and deep learning, nowadays gaze estimation techniques are developed with
    appearance based features and do not require an explicit calibration step. For
    example, CalibMe [[48](#bib.bib48)] is a fast and unsupervised calibration technique
    for gaze trackers, designed to overcome the burden of repeated calibration.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d82053596e8045fdd41a241da8d5d914.png)'
  prefs: []
  type: TYPE_IMG
- en: <svg version="1.1" width="621.24" height="97.17" overflow="visible"><g transform="translate(0,97.17)
    scale(1,-1)"><g transform="translate(-339.01,172.96)"><g transform="translate(0,36.9447903694479)
    scale(1, -1)"><foreignobject width="621.281306212813" height="7.3336100733361"
    overflow="visible">[[49](#bib.bib49), [24](#bib.bib24), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53), [26](#bib.bib26), [54](#bib.bib54), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60)]</foreignobject></g></g><g
    transform="translate(-339.01,159.13)"><g transform="translate(0,36.9447903694479)
    scale(1, -1)"><foreignobject width="619.482496194825" height="7.3336100733361"
    overflow="visible">[[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [49](#bib.bib49),
    [29](#bib.bib29), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [27](#bib.bib27)]</foreignobject></g></g><g
    transform="translate(-339.01,117.61)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="378.44195378442" height="7.3336100733361"
    overflow="visible">[[67](#bib.bib67), [68](#bib.bib68), [32](#bib.bib32), [34](#bib.bib34),
    [69](#bib.bib69)]</foreignobject></g></g><g transform="translate(-339.01,69.19)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="373.460633734606"
    height="7.3336100733361" overflow="visible">[[70](#bib.bib70), [68](#bib.bib68),
    [32](#bib.bib32), [34](#bib.bib34)]</foreignobject></g></g><g transform="translate(-339.01,20.76)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="75.1349107513491"
    height="7.3336100733361" overflow="visible">[[71](#bib.bib71)]</foreignobject></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: The plot shows the popularity of different data capturing devices
    across research articles over the past 10 years. Here, HMD: Head Mounted Device,
    RGBD: RGB Depth camera.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Role of Data Capturing Sensors. Visual stimuli provide valuable information
    for computer vision based gaze estimation techniques. A trade-off of widely used
    sensors is mentioned in Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 Gaze Estimation: Problem
    Setting ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches"). These data capturing sensors are mainly divided into two categories:
    Intrusive and Non-intrusive. An array of methods used specialized hardware which
    requires physical contact with human skin or eyes are termed as intrusive sensors.
    The widely used intrusive sensors are head-mounted devices (HMD), electrodes,
    or scleral coils [[69](#bib.bib69), [72](#bib.bib72)]. These devices may cause
    unpleasant user experience, and the accuracy of these systems depends on the tolerance,
    accurate subject-specific calibration and other factors of the devices. On the
    other hand, data capturing devices that do not require physical contact [[73](#bib.bib73)]
    are termed non-intrusive sensors. Mainly, RGB, RGBD and IR cameras fall under
    this category. These methods face several challenges, which include partial occlusion
    of the iris by the eyelid, varying illumination condition, head pose, specular
    reflection in case the user wears glasses, the inability to use standard shape
    fitting for iris boundary detection, and other effects including motion blur and
    over saturation of images [[73](#bib.bib73)]. To deal with these challenges, most
    of the existing gaze estimation methods have been performed under constrained
    environments like constrained head pose, controlled illumination conditions, and
    camera angle. Among all of the aforementioned factors, pupil visibility plays
    an important role as robust gaze estimation needs accurate pupil-center localization.
    Fast and accurate pupil-center localization is still a challenging task [[74](#bib.bib74)],
    particularly for images with low resolution. A trade-off of widely used sensors
    are mentioned in Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 Gaze Estimation: Problem Setting
    ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches").'
  prefs: []
  type: TYPE_NORMAL
- en: Role of Headpose. Gaze estimation is a challenging task due to eye-head interplay.
    Head-pose plays the most important role in gaze estimation. The gaze direction
    of a subject is determined by the combined effect of position and orientation
    of head pose and eyeball. One can change gaze direction via eyeball and pupil
    movement by maintaining stationary or dynamic head-pose or by moving both. Usually,
    this process is subject dependent. People adjust their head-pose and gaze to maintain
    a comfortable posture. Thus, the gaze estimation task needs to consider both gaze
    and head-pose at the same time for inference. As a result of this, it is more
    common to consider head-pose information in the gaze estimation methods implicitly
    or explicitly [[24](#bib.bib24), [50](#bib.bib50), [75](#bib.bib75)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8fd599e0b47c45bb730a88bcdf506881.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Overview of gaze estimation setups (See Sec. [2.3](#S2.SS3 "2.3 Gaze
    Estimation: Problem Setting ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches") for more details). A traditional gaze analysis
    setup considers the effect of head, visual plane and camera coordinates. The gaze
    analysis tasks include gaze zone, point of regard, gaze trajectory estimation
    etc. (See Sec. [3](#S3 "3 Gaze Analysis in Computer Vision ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")) The gaze vector is defined by the
    angles ($\theta,\phi$) in polar co-ordinate systems as shown in the gaze direction
    part.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Role of Visual Plane. The visual plane is the plane containing the gaze target
    point i.e. where the subject is looking, which is often termed as Point of Gaze
    (PoG). The distance between the user and the visual plane varies a lot in a real-world
    setting. Thus, recent deep learning based methods do not rely on the distance
    or placement of the visual plane. The most common gaze analysis setup uses a RGB
    camera placed at 20 - 70 cm from the user in unconstrained setting i.e. without
    any invasive sensors or fixed setup. In different real-world settings, the visual
    plane could be desktop ($\sim$ 60cm), mobile phone ($\sim$ 20cm), car ($\sim$
    50cm) etc. An overview is presented in Table [I](#S2.T1 "Table I ‣ 2.3 Gaze Estimation:
    Problem Setting ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep
    Learning based Approaches").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table I: Attributes of different platforms widely used in gaze analysis. Here,
    Dist.: distance (in cm), VA: viewing Angle (in °), HMD: Head Mounted Devices,
    FV: Free Viewing, UC: User Condition, ET: External Target.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Platform | Dist. | VA | UC | Papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Desktop, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; TV Panels &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 30-50, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 200-500 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\sim$ 40 °, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 40°-60° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Static, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sitting, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Upright &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[49](#bib.bib49), [24](#bib.bib24), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53), [26](#bib.bib26), [54](#bib.bib54), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [49](#bib.bib49),
    [29](#bib.bib29), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[76](#bib.bib76), [71](#bib.bib71), [58](#bib.bib58), [59](#bib.bib59),
    [60](#bib.bib60)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD | 2-5 | 55°-75° |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Independent &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Leanback, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sitting, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Upright) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[77](#bib.bib77), [78](#bib.bib78), [32](#bib.bib32), [34](#bib.bib34)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Automotive | 50 | 40°-60° |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Mobile, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sitting, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Upright &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[40](#bib.bib40), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Handheld | 20-40 | 5°-12° |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Leanfwd, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sitting, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Standing, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Mobile &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[25](#bib.bib25), [63](#bib.bib63), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ET/ FV | – | – |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Leanfwd, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sitting, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Standing, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Upright &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[90](#bib.bib90), [27](#bib.bib27)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data Annotation. Generally, deep learning based methods require large amount
    of annotated data for generalized representation learning. Curating large scale
    annotated gaze datasets is non-trivial [[90](#bib.bib90), [24](#bib.bib24), [87](#bib.bib87)],
    time consuming and requires expensive equipment setup. Current dataset recording
    paradigms via wearable sensors may lead to uncomfortable user experience and it
    require expert knowledge. Another common aspect of the current datasets is the
    constrained environment in which they are recorded (For example, CAVE [[23](#bib.bib23)]
    dataset is recorded on indoor environment with headpose restricted). Recently,
    a few datasets [[90](#bib.bib90), [87](#bib.bib87)] have been proposed to address
    this gap by recording in unconstrained indoor and outdoor environments. Another
    challenge associated with data annotation is participant’s cooperation. However,
    it is assumed that participants fixate their gaze as per the given instructions [[90](#bib.bib90),
    [24](#bib.bib24), [87](#bib.bib87)]. Despite these attempts [[90](#bib.bib90),
    [24](#bib.bib24), [87](#bib.bib87)], data annotation still remains complex, noisy
    and time-consuming. Self/weakly/un-supervised learning paradigms [[27](#bib.bib27),
    [28](#bib.bib28)] could be helpful to address the dataset creation and annotation
    challenges.
  prefs: []
  type: TYPE_NORMAL
- en: Subjective Bias. Another challenge for the automatic gaze analysis method is
    subjective bias. Individual differences in the nodal points of human eyes makes
    automatic and generic gaze analysis way more difficult. In an ideal scenario,
    any gaze analysis method should encode rich features corresponding to eye region
    appearance, which provides relevant information for gaze analysis. To address
    this challenge, few-shot learning based approach has been widely adapted  [[29](#bib.bib29),
    [91](#bib.bib91)], where the motivation is to adapt to new subject with minimum
    subject specific information. Another way to deal with the subjective bias is
    combining classical eye model based approaches with geometrical constraints [[92](#bib.bib92)]
    as this approach has the potential to generalize well across subjects.
  prefs: []
  type: TYPE_NORMAL
- en: Eye Blink. Blinks are an involuntary and periodic motion of the eyelids. They
    pose a challenge for gaze analysis, as blinks result in missed frames of data.
    A few recent works [[90](#bib.bib90), [40](#bib.bib40)] assume that the head pose
    information is a suitable replacement for gaze during blinking based on a common
    line of sight between a subject’s headpose and gaze. However, it is noted that
    a large shift in the gaze is possible after subject re-opens their eyes. To simplify
    the situation, some gaze analysis methods ignore eye-blink data (e.g., [[87](#bib.bib87),
    [57](#bib.bib57)]) and some treat blinks as a separate class of data (.e.g, [[93](#bib.bib93),
    [41](#bib.bib41)]). A possibility for real world deployment of such a system is
    generating gaze labels by interpolating from neighbouring frames’ labels when
    blinks are detected [[40](#bib.bib40)].
  prefs: []
  type: TYPE_NORMAL
- en: Data Attributes. Several factors, such as eye-head interplay, occlusion, blurred
    image, and illumination can influence the performance of a gaze analysis model.
    The presence of any subset of these attributes can degrade the performance of
    a system [[90](#bib.bib90), [87](#bib.bib87)]. Many methods use face alignment [[24](#bib.bib24),
    [25](#bib.bib25)] and 3-D head pose estimation [[24](#bib.bib24)] as a pre-processing
    step. However, face alignment on images captured in an unconstrained environment
    based images may introduce noise in a system. To overcome this, recent approaches [[90](#bib.bib90),
    [94](#bib.bib94), [57](#bib.bib57), [27](#bib.bib27)] avoid these pre-processing
    steps and show increase in gaze prediction performance.
  prefs: []
  type: TYPE_NORMAL
- en: Another critical challenge in gaze estimation is eye-head interplay. Prior studies
    generally address this issue via implicit training [[25](#bib.bib25), [95](#bib.bib95)]
    or provide the head pose information separately as a feature [[24](#bib.bib24)].
    Similarly, it is challenging to estimate gaze under partial occlusion. When the
    head’s yaw rotation is greater than 90°, one side of the face becomes occluded
    w.r.t. the camera. A few prior works [[25](#bib.bib25), [24](#bib.bib24)] avoid
    these scenarios by disregarding these frames. Kellnhofer et al. [[90](#bib.bib90)],
    however, argue that when the head yaw angle is in the range 90°- 135°, the partial
    visibility still provides relevant information about the gaze direction. This
    study also proposes quantile regression via pinball loss to mitigate the effect
    of partial occlusion in training data in terms of uncertainty. Despite all of
    these attempts, gaze estimation still remains challenging in presence of these
    attributes. There is still have scope to eliminate the effects of these attributes
    and make the gaze analysis model more robust for the real-world deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Application Specific Challenges. Gaze analysis also has application-specific
    requirements, for example, coarse or fine gaze estimation in AR, VR, Robotics,
    egocentric vision and HCI. Thus, a working algorithm behind any eye-tracking devices
    needs to fit in the application environment.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Gaze Analysis in Computer Vision
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We provide a breakdown of different gaze analysis tasks for vision based applications.
    Any statistical gaze modeling mainly estimates the relation between the input
    visual data and the point of regard/gaze direction.
  prefs: []
  type: TYPE_NORMAL
- en: '2-D/3-D Gaze Estimation. Most of the existing studies consider gaze estimation
    as either the gaze direction in 3-D space or as the point of regard in 2-D/3-D
    coordinates (see Fig. [4](#S2.F4 "Figure 4 ‣ 2.3 Gaze Estimation: Problem Setting
    ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")).
    We can divide the gaze estimation methods into the following types:'
  prefs: []
  type: TYPE_NORMAL
- en: '1) Geometric Methods: These geometric methods compute a gaze direction from
    the geometric model of the eye (see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")), where
    the anatomical structure of the eye is considered to get the 3-D gaze direction
    or gaze vector. These methods were widely used in prior to more deep learning
    approaches [[21](#bib.bib21)]. These recent deep learning based approaches implicitly
    model these geometric parameters during the learning process, and, as such, do
    not explicitly require the often noisy subject specific parameters, such as cornea
    radii, cornea center, angles of kappa (i.e. Refer $\kappa$ in Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")), iris radius, the distance between the pupil center and cornea
    center, etc.'
  prefs: []
  type: TYPE_NORMAL
- en: '2) Regression Methods: Regression based methods [[96](#bib.bib96), [24](#bib.bib24),
    [26](#bib.bib26), [28](#bib.bib28)] map visual stimuli (image or image-related
    features) to gaze coordinates or gaze angles in 2-D/3-D. The output mapping is
    application-specific. For example, such techniques are often used to map 2-D/3-D
    gaze coordinate mainly maps people’s focus of attention to the screen coordinates
    (for human-computer interaction based applications such as engagement or attention
    monitoring). Regression based methods can be divided into two types: the parametric
    and non-parametric approaches. Parametric approaches (e.g., [[96](#bib.bib96),
    [28](#bib.bib28)]) assume gaze trajectories as a polynomial, where the task is
    to estimate the parameters of the polynomial equation. Non-parametric approaches
    directly work on the mappings in spite of calculating the intersection between
    the gaze direction and gazed object explicitly [[24](#bib.bib24), [26](#bib.bib26),
    [57](#bib.bib57)]. The recent deep learning based approaches are non-parametric [[26](#bib.bib26),
    [97](#bib.bib97), [28](#bib.bib28), [54](#bib.bib54), [90](#bib.bib90)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trajectory Prediction. Gaze estimation has potential applications in AR/VR
    especially in Foveated Rending (FR) and Attention Tunneling (AT), where the future
    eye trajectory prediction is highly desirable. To meet this requirement, a new
    research direction (i.e. future gaze trajectory prediction) has been recently
    introduced [[34](#bib.bib34)]. Here, possible future gaze locations can be estimated
    based on the prior gaze points, content of the visual plane or their combination.
    Thus, the problem statement can be formulated as follows: given $n$ number of
    prior gaze points, the algorithm will predict the $m$ future frames’ gaze direction
    in a user-specific setting.'
  prefs: []
  type: TYPE_NORMAL
- en: Gaze Zone. In many gaze estimation based applications such as driver gaze [[40](#bib.bib40),
    [41](#bib.bib41), [93](#bib.bib93), [27](#bib.bib27)], gaming platforms [[98](#bib.bib98)],
    website designing [[99](#bib.bib99)], etc., the exact position or angle of the
    line of sight of the pupil is not required. Thus, a gaze zone approach is utilized
    in these cases for estimation. Here, the gaze zone refers to an area in 2-D or
    3-D space. For example, in a simplistic driver gaze zone estimation, the driver
    could be looking straight ahead, at the steering wheel, at the radio, or at the
    mirrors. Similarly, another example is detecting more visually salient zone/region
    during website designing [[99](#bib.bib99)].
  prefs: []
  type: TYPE_NORMAL
- en: Gaze Redirection. Due to the challenges in different posed gaze conditions,
    generation on the go is gaining popularity [[100](#bib.bib100), [34](#bib.bib34)].
    It aims to capture subject-specific signals from a few eye images of an individual
    and generate realistic eye images for the same individual under different eye
    states (gaze direction, camera position, eye openness etc.). The gaze redirection
    can be performed in both controlled and uncontrolled way [[101](#bib.bib101),
    [100](#bib.bib100), [102](#bib.bib102)]. Apart from these, eye rendering is another
    research direction to generate realistic eye given the appearance, gaze direction
    of a person. It has potential applications in virtual agents, social robotics,
    behaviour generation and in the animation industry [[103](#bib.bib103)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Unconstrained Gaze Estimation. Gaze estimation in an unconstrained setting
    can be divided into two types:'
  prefs: []
  type: TYPE_NORMAL
- en: '1) Single Person Setting: In webcam or RGB camera based gaze estimation approaches,
    geometric model based eye tracking [[104](#bib.bib104), [105](#bib.bib105)] is
    typically used, as it is fast and does not require training data. At the same
    time, however, it relies on accurate eye location and key points detection, which
    is hard to achieve in real-world environments. Deep learning based methods [[106](#bib.bib106),
    [97](#bib.bib97)] have eliminated this issue to some extent, however, it still
    remains a challenge as it does not generalize well in different settings.'
  prefs: []
  type: TYPE_NORMAL
- en: '2) Multi-Person Setting: In unconstrained multi-person settings, it is very
    difficult to track the eyes. For example, in a social interaction scenario, understanding
    the gaze behaviour of each person provides important cues to interpret social
    dynamics [[107](#bib.bib107)]. To this end, a new research direction is introduced
    where the problem is defined as whether the people are Looking At Each Other (LAEO)
    in a given video sequence [[108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110)].
    Similarly, gaze communication [[111](#bib.bib111)] and GazeOnce [[112](#bib.bib112)]
    are another line of research aligned with this field.'
  prefs: []
  type: TYPE_NORMAL
- en: Visual Attention Estimation. Human visual attention estimation is another line
    of research which mainly focuses on where the person is looking irrespective of
    eyes visibility. The popular sub-tasks in this direction are gaze following [[113](#bib.bib113),
    [114](#bib.bib114), [62](#bib.bib62), [115](#bib.bib115), [116](#bib.bib116)],
    gaze communication [[111](#bib.bib111)], human attention in goal-driven environments [[117](#bib.bib117)]
    and categorical visual search [[118](#bib.bib118)], visual scan-path analysis
    in visual question answering [[119](#bib.bib119)], and naturalistic environment [[120](#bib.bib120),
    [121](#bib.bib121)]. These methods are mostly driven by saliency in the scene,
    head orientation, or any other task at hand. Visual attention based approaches
    have the potential to localize the gaze target directly from scene information
    which in turn enhances the scalability of naturalistic gaze behaviour patterns.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Gaze Analysis Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We break down a gaze analysis framework into its fundamental components (Fig. [5](#S4.F5
    "Figure 5 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")) and discuss their
    role in terms of eye detection and segmentation (Sec. [4.1](#S4.SS1 "4.1 Eye Detection
    and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches")), Network Architecture (Sec. [4.2](#S4.SS2
    "4.2 Representative Deep Network Architectures ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")) and Level of Supervision
    (Sec. [4.3](#S4.SS3 "4.3 Level of Supervision ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")).'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Eye Detection and Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Eye registration is the first stage of gaze analysis and requires detection
    of the eye and the relevant regions of interest.
  prefs: []
  type: TYPE_NORMAL
- en: 'Eye Detection Methods. The main aim of the eye detection algorithms is to accurately
    identify the eye region from an input image. Eye detection algorithms need to
    operate in challenging conditions such as occlusion, eye openness, variability
    in eye size, head pose, illumination, and viewing angle, while balancing the trade-off
    in appearance, dynamic variation and computational complexity. Prior works on
    eye detection can be divided into three categories: shape based [[22](#bib.bib22)],
    appearance based [[122](#bib.bib122), [123](#bib.bib123), [49](#bib.bib49), [105](#bib.bib105)]
    and hybrid method [[123](#bib.bib123)]. The most popular libraries for eye and
    facial point detection are [Dlib](https://github.com/davisking/dlib) [[106](#bib.bib106)]
    [OpenFace](https://github.com/cmusatyalab/openface) [[105](#bib.bib105), [104](#bib.bib104)],
    [MTCNN](https://github.com/kpzhang93/MTCNN_face_detection_alignment) [[124](#bib.bib124)],
    [Duel Shot Face Detector](https://github.com/Tencent/FaceDetection-DSFD) [[125](#bib.bib125)],
    [FaceX-Zoo](https://github.com/JDAI-CV/FaceX-Zoo) [[126](#bib.bib126)].'
  prefs: []
  type: TYPE_NORMAL
- en: The pupil and iris region of the eye is usually darker than the sclera which
    provides an important cue to differentiate or localize the pupil. The pupil center
    localization use dedicated and costly devices [[69](#bib.bib69), [72](#bib.bib72)],
    which requires person-specific pre-calibration. To overcome this limitation, the
    deep learning based pupil localization methods use ensembles of randomized trees [[127](#bib.bib127)],
    local self similarity matching [[73](#bib.bib73)], adaptive gradient boosting [[128](#bib.bib128)],
    hough regression forests [[129](#bib.bib129)], deep learning based landmark localization
    models [[97](#bib.bib97), [106](#bib.bib106)], heterogeneous CNN models [[130](#bib.bib130)],
    etc. In prior literature, the choice of eye registration process is influenced
    by the correlation between the input image and the learning objective of the proposed
    method. Apart from this, the trade-off between the accuracy of eye localization
    and running time complexity of the algorithm is optimized in a task specific way.
    In this context, OpenFace and Dlib are the most popular. Moreover, the choice
    of eye/face registration process may also depend on their ability to detect eye
    components in different challenging real world conditions.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/326fb4daeabfc0781c534535923a70c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: A generic gaze analysis framework has different components including
    registration, gaze representations and inference. Although in the deep learning
    based approaches, there is a high overlap between the representation and inference
    module. Refer Sec. [4](#S4 "4 Gaze Analysis Framework ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eye Segmentation. The main task of eye segmentation is pixel-wise or region-wise
    differentiation of the visible eye parts. In general, the eye region is divided
    into three parts: sclera (the white region of the eyes), iris (the colour ring
    of tissue around the pupil) and pupil (the dark iris region). Prior studies [[131](#bib.bib131),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134)] on eye segmentation
    mainly explore to segment the iris and sclera region. Few studies [[32](#bib.bib32),
    [34](#bib.bib34)] include the pupil region in the segmentation task as well. Eye
    segmentation is widely used in the biometric systems [[135](#bib.bib135)] and
    prior for synthetic eye generation [[136](#bib.bib136)].'
  prefs: []
  type: TYPE_NORMAL
- en: Eye Blink Detection. Eye blinks are the involuntary and periodic activity that
    can help to judge the cognitive activity of a person (e.g. driver’s fatigue [[137](#bib.bib137)],
    lie detection [[138](#bib.bib138)]). KLT trackers and various sensors are also
    widely used to get the eye motion information to track eye blink [[139](#bib.bib139)].
    The existing eye blink detection approaches aim to solve a binary classification
    problem (blink/no blink) either in a heuristic based or data-driven way. The heuristic
    based approaches mainly include motion localization [[139](#bib.bib139)] and template
    matching [[140](#bib.bib140)]. As these methods are highly reliable on pre-defined
    thresholds, they could be sensitive to subjective bias, illumination and head
    pose. To overcome this limitation, the data-driven approaches infer on the basis
    of appearance based temporal motion features [[141](#bib.bib141), [139](#bib.bib139)]
    or spatial features [[142](#bib.bib142)]. In hybrid approach [[143](#bib.bib143)],
    multi-scale LSTM based framework is used to detect eye blink using both spatial
    and temporal information.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e15baf5d1096f3d6999e8976d1003b68.png)'
  prefs: []
  type: TYPE_IMG
- en: <svg version="1.1" width="627.35" height="37.36" overflow="visible"><g transform="translate(0,37.36)
    scale(1,-1)"><g transform="translate(-654.49,381.9)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="112.218071122181" height="7.3336100733361"
    overflow="visible">[[24](#bib.bib24)]</foreignobject></g></g><g transform="translate(-470.46,373.6)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="68.3547806835478"
    height="7.3336100733361" overflow="visible">[[51](#bib.bib51)]</foreignobject></g></g><g
    transform="translate(-232.46,386.05)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="79.9778607997786" height="7.3336100733361"
    overflow="visible">[[90](#bib.bib90)]</foreignobject></g></g><g transform="translate(-89.94,386.05)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="107.651861076519"
    height="7.3336100733361" overflow="visible">[[136](#bib.bib136)]</foreignobject></g></g><g
    transform="translate(-586.69,290.58)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="101.425211014252" height="7.3336100733361"
    overflow="visible">[[57](#bib.bib57)]</foreignobject></g></g><g transform="translate(-445.55,290.58)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="78.3174207831742"
    height="7.3336100733361" overflow="visible">[[26](#bib.bib26)]</foreignobject></g></g><g
    transform="translate(-283.66,290.58)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="79.4243807942438" height="7.3336100733361"
    overflow="visible">[[25](#bib.bib25)]</foreignobject></g></g><g transform="translate(-83.02,290.58)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="120.658641206586"
    height="7.3336100733361" overflow="visible">[[27](#bib.bib27)]</foreignobject></g></g><g
    transform="translate(-560.4,131.45)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="105.299571052996" height="7.3336100733361"
    overflow="visible">[[28](#bib.bib28)]</foreignobject></g></g><g transform="translate(-496.75,141.14)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="73.1977307319773"
    height="7.3336100733361" overflow="visible">[[29](#bib.bib29)]</foreignobject></g></g><g
    transform="translate(-242.15,141.14)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="77.7639407776394" height="7.3336100733361"
    overflow="visible">[[101](#bib.bib101)]</foreignobject></g></g></g></svg>
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: A brief overview of different pipelines used for gaze analysis tasks.
    Refer Sec. [4.2](#S4.SS2 "4.2 Representative Deep Network Architectures ‣ 4 Gaze
    Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches") for more details of the networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Representative Deep Network Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we provide a generic formulation and representation of gaze
    analysis. Given an RGB image $\mathbf{I}\in\mathbb{R}^{W\times H\times 3}$, a
    deep learning based model mapped it to task-specific label space. The input RGB
    image is usually the face or eye regions. Based on the primary network architectures
    adopted in the literature, we classify the models into the following categories:
    CNN based, Multi-Branch network based, Temporal based, Transformer Based and VAE/GAN
    based. An overview is shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and
    Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of
    Deep Learning based Approaches").'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 CNN based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most of the recent solutions adopt a CNN based architecture [[24](#bib.bib24),
    [51](#bib.bib51), [50](#bib.bib50), [25](#bib.bib25), [26](#bib.bib26), [92](#bib.bib92)],
    which aims to learn end-to-end spatial representation followed by gaze prediction.
    The adopted model is often a modified version of the popular CNNs in vision (e.g.
    AlexNet [[27](#bib.bib27)], VGG [[54](#bib.bib54)], ResNet-18 [[90](#bib.bib90),
    [67](#bib.bib67)], ResNet-50[[94](#bib.bib94)], Capsule network [[27](#bib.bib27)]).
    These CNNs learn from a single-stream of RGB images (e.g. face, left or right
    eye patch) [[24](#bib.bib24), [51](#bib.bib51)], or multiple streams of information
    (e.g. face, and eye patches) [[57](#bib.bib57), [25](#bib.bib25)], and prior knowledge
    based on eye anatomy or geometrical constraints [[26](#bib.bib26)].
  prefs: []
  type: TYPE_NORMAL
- en: 'GazeNet. It is the extended version of the first deep learning based gaze estimation
    method [[24](#bib.bib24)] which aims to capture low level and high level appearance
    feature by using convolution operation. GazeNet takes a grayscale eye patch image
    $\mathbf{I}\in\mathbb{R}^{W\times H}$ as input and maps it to angular gaze vector
    $\mathbf{g}\in\mathbb{R}^{2}$. As headpose provides relevant features for gaze
    direction, the headpose vector is also added in the FC layer for better inference
    (Refer top left image in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")). The Extended version [[50](#bib.bib50)] is adapted from the
    VGG network which further boosts the performance. To train these models, the sum
    of the individual $\ell_{2}$ losses between the predicted $\mathbf{\hat{g}}$ and
    actual gaze angle vectors $\mathbf{g}$ is considered.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Spatial Weight CNN. It is a full face appearance based gaze estimation method [[51](#bib.bib51)]
    which uses a spatial weighting mechanism for encoding the important locations
    of the facial image $\mathbf{I}\in\mathbb{R}^{W\times H\times 3}$ via the standard
    CNN architecture (Refer top row second column image in Fig. [6](#S4.F6 "Figure
    6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")). This weighting mechanism
    (aka attention) automatically assign more weight to the regions contributing more
    towards gaze estimation. It includes three additional $1\times 1$ convolutional
    layers followed by a ReLU activation. Given a $N\times H\times W$ dimensional
    activation map ($U$) as input (where $N$, $H$ and $W$ are the number of feature
    channels, height and width of the output), the spatial weights module learns the
    weight matrix $W$ from element-wise multiplication of $W$ with the original activation
    $U$ via the following function: $W\bigodot U_{c},$ across the channel dimensions.
    Thus, the model learns to assign more weight to the specific regions, which in
    turn eliminates unwanted noise in the input. For 2-D gaze estimation, the $\ell_{1}$
    distance between the predicted and ground-truth gaze positions in the target screen
    coordinate system is utilized. Similarly, the $\ell_{1}$ distance between the
    predicted and ground-truth gaze angle vectors in the normalized space is used
    for 3-D gaze estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Dilated Convolution. Another interesting architecture for gaze estimation is
    dilated-convolutional layers which preserve spatial resolution while increasing
    the size of the receptive field without compromising the number of parameters [[64](#bib.bib64)].
    It aims to capture the slight change in pixels due to eye movement. Given an input
    feature map $U$ of kernel size $N\times M\times K$ ($N$: height, $M$:width, $K$:channel
    with weights $W$ and bias $b$) and dilation rates ($r_{1}$, $r_{2}$), the output
    feature map $v$ can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $v(x,y)=\sum\limits_{k=1}^{K}\sum\limits_{m=0}^{M-1}\sum\limits_{n=0}^{N-1}u(x+nr_{1},y+mr_{2},k)w_{nmk}+b$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The dilated convolution is applied in facial and left/right eye patches before
    inferring the gaze. For training the network, cross entropy loss is used in the
    label space. Representation learning via MinENet [[144](#bib.bib144)] also relies
    on dilated and asymmetric convolutions to provide context to the segmented regions
    of the eye by increasing the receptive field capacity of the model to learn contextual
    information.
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian CNN. Another variant of CNN is Bayesian CNN which is used for robust
    and generalizable eye-tracking under different conditions [[145](#bib.bib145)].
    Instead of predicting eye gaze using a single trained eye model, it performs eye
    tracking using an ensemble of models, hence alleviating the over-fitting problem,
    is more robust under insufficient data, and can generalize better across datasets.
    Compared to the point based eye landmark estimation methods, the BNN model can
    generalize better and it is also more robust under challenging real-world conditions.
    Additionally, the extended version of the BCNN (i.e. the single-stage model to
    multi-stage, yielding the cascade BCNN) allows feeding the uncertainty information
    from the current stage to the next stage to progressively improve the gaze estimation
    accuracy. This could be an interesting area for further study.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pictorial Gaze. Pictorial gaze [[26](#bib.bib26)] aims to model the relative
    position of eyeball and iris to get the gaze direction. The network [[26](#bib.bib26)]
    consists of two parts: 1) regression from eye patch image to intermediate gazemap
    followed by 2) regression from gazemap to gaze direction vector $g$ (Refer second
    row second column image in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")). The gazemap is an intermediate representation of a simple
    model of the human eyeball and iris in terms of $m\times n$ dimensional image,
    where, the projected eyeball diameter is $2r=1.2n$ and the iris centre coordinates
    ($u_{i},v_{i}$) are as follows: $u_{i}=\frac{m}{2}-r^{\prime}sin\ \phi\ cos\ \theta,v_{i}=\frac{n}{2}-r^{\prime}sin\
    \theta$ where, $r^{\prime}=r\ cos\ (sin^{-1}\frac{1}{2})$ and gaze direction $g=(\theta,\phi)$.
    Basically, the iris is an ellipse with major-axis diameter of r and minor-axis
    diameter of $r|cos\ \theta cos\ \phi|$. The first part is implemented via a stacked
    hourglass architecture which assumed to encode complex spatial relations including
    the locations of occluded key points. Consequently, for the second part, a DenseNet
    architecture is used which maps the intermediate gazemap to the gaze vector $\hat{g}$.
    It is trained via gaze direction regression loss defined as: $||g-\hat{g}||_{2}$
    as well as cross-entropy loss between predicted and ground-truth gazemaps for
    all pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Ize-Net. This framework is used for coarse to fine gaze representation learning.
    Here, the main idea is to learn coarse gaze representation by dividing the gaze
    locations to gaze zones. Further, the gaze zone is mapped to the finer gaze vector.
    The proposed network [[27](#bib.bib27)] (Refer second row right image in Fig. [6](#S4.F6
    "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")) is a combination
    of convolution and primary capsule layer. After the convolution layers, the primary
    capsule layer is appended whose job is to take the features learned by convolution
    layers and produce combinations of the features to consider face symmetry into
    account. This network is trained for coarse gaze zone which is fine-tuned for
    downstream 2-D/3-D gaze estimation.'
  prefs: []
  type: TYPE_NORMAL
- en: EyeNet. consists of modified residual units as the backbone, attention blocks
    and multi-scale supervision architecture. This network is robust for the low resolution,
    image blur, glint, illumination, off-angles, off-axis, reflection, glasses and
    different colour of iris region challenges.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Multi-Branch network based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are several works [[54](#bib.bib54), [25](#bib.bib25), [57](#bib.bib57)]
    which utilize multiple inputs for better inference.
  prefs: []
  type: TYPE_NORMAL
- en: 'iTracker. The iTracker framework [[25](#bib.bib25)] takes the left eye, right
    eye, detected facial region and face location in the original frame as a binary
    mask (all of the size $224\times 224$) and predicts the distance from the camera
    (in cm). The model is jointly trained with Euclidean loss on the x and y gaze
    position. The overview of the framework is shown in the second row third column
    image in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze
    Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Branch Design. Similar to iTracker, Jyoti et al. [[57](#bib.bib57)] propose
    a framework which takes the full face, left and right eye, both eye patch as input
    for inferring gaze (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches") second-row left image). To train this network, mean squared
    error between the true and predicted gaze point/direction is used.'
  prefs: []
  type: TYPE_NORMAL
- en: Two-Stream VGG Network. In [[54](#bib.bib54)], a two-stream VGG network is used
    for gaze inference while taking left and right eye patch as input. Similar to
    prior works, it utilizes the sum of the individual $\ell_{2}$ losses between the
    predicted and ground truth gaze vectors to train the ensemble network.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Temporal Gaze Modelling
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The human gaze is a continuous and dynamic process. While scanning the environment,
    the concerned subject performs eye movements in terms of fixations, saccades,
    smooth pursuit, vergence, and vestibulo-ocular movements. Moreover, a certain
    image frame in time has a high correlation with the gaze direction of previous
    time steps. Based on this line of reasoning, several works [[146](#bib.bib146),
    [90](#bib.bib90), [65](#bib.bib65), [66](#bib.bib66), [56](#bib.bib56), [147](#bib.bib147)]
    have leveraged temporal information and eye movement dynamics to enhance gaze
    estimation performance as compared to image based static methods. Given a sequence
    of frames, here the task is to estimate the gaze direction of the concerned person.
    For this modelling, popular recurrent neural network structures have been explored
    (e.g. GRU [[148](#bib.bib148)], LSTM/bi-LSTM [[90](#bib.bib90)]).
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Modal Recurrent CNN. Palmero et al. [[56](#bib.bib56)] have proposed a
    multimodal recurrent CNN framework in which the learned static features of all
    the input frames of a given sequence are fed to a many-to-one recurrent module
    for predicting the 3D gaze direction of the last frame in the sequence. Their
    approach improves the state-of-the-art gaze estimation performance significantly
    (i.e. by 4% on EYEDIAP dataset).
  prefs: []
  type: TYPE_NORMAL
- en: DGTN. Wang et al. [[66](#bib.bib66)] have proposed Dynamic Gaze Transition Network
    (DGTN) based on semi-Markov approach which models human eye movement dynamics.
    DGTN first computes per-frame gaze using a CNN which is further refined using
    the learned dynamic information.
  prefs: []
  type: TYPE_NORMAL
- en: Improved iTracker + bi-LSTM. Bidirectional recurrent module based temporal modeling
    methods have been introduced in [[65](#bib.bib65)] which rely on both past and
    future frames. It is quite beneficial for low-to-mid resolution images and videos
    having a low frame rate ($\sim 30$ fps) despite its reduced applicability for
    real-time application as future frames are not usually available.
  prefs: []
  type: TYPE_NORMAL
- en: 'Pinball LSTM. Similarly to encode contextual information along temporal domain,
    pinball LSTM [[90](#bib.bib90)] is proposed. This video based gaze estimation
    model using bidirectional LSTM considers 7-frame sequence to estimate the gaze
    direction of the central frame. Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection
    and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches") first row third column image illustrates the
    architecture of the model. The facial region from each frame is provided as input
    to the backbone CNN having ResNet-18 architecture. It maps input image to 256-dimensional
    feature space. Further, a two layer bidirectional LSTMs map these features to
    label space via a FC layer with an error quantile estimation i.e. ($\theta,\phi,\sigma$),
    where ($\theta,\phi$) is the predicted gaze direction in spherical coordinates
    corresponding to the ground-truth gaze vector in the eye coordinate system $g$
    as $\theta=-arctan\ \frac{g_{x}}{g_{z}}$ and $\phi=arcsin\ g_{y}$. On the other
    hand, $\sigma$ corresponds to the offset from the predicted gaze i.e. $\theta+\sigma$
    and $\phi+\sigma$ in the 90% quantiles of its distribution and $\theta-\sigma$
    and $\phi-\sigma$ are in the 10% quantiles. The pinball loss is computed as follows:
    given the ground truth label $y=(\theta_{gt},\phi_{gt})$, the loss $L_{\tau}$
    for the quantile $\tau$ and the angle $\alpha\in\{\theta,\phi\}$ can be written
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{\tau}(\alpha,\sigma,\alpha_{gt})=max(\tau\hat{q_{\tau}},-(1-\tau)\hat{q_{\tau}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where, $\hat{q_{\tau}}=\alpha_{gt}-(\alpha-\sigma)$, for $\tau\leq 0.5$ and
    $\alpha_{gt}-(\alpha+\sigma)$ otherwise. This loss enforces $\theta$ and $\phi$
    to converge to their ground truth values.
  prefs: []
  type: TYPE_NORMAL
- en: Static + LSTM. In an interesting study, Palmero et al. [[146](#bib.bib146)]
    analyze the effect of sequential information for appearance-based gaze estimation
    using a static CNN network followed by a recurrent module to capture eye movement
    dynamics. The model is developed based on high-resolution eye-image sequences
    performing a stimulus-elicited fixation and saccade task in a VR scenario. The
    proposed model learns eye movement dynamics with accurate localization of gaze
    movement transition.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion. Despite the initial efforts that confirm the benefits of leveraging
    temporal information [[146](#bib.bib146), [90](#bib.bib90), [65](#bib.bib65),
    [66](#bib.bib66), [56](#bib.bib56)], there still have scope to explore eye dynamics
    in a task-driven real environment. Sometimes it is difficult to capture eye movement
    dynamics accurately using videos having low-resolution image frames with poor
    frame rates. Thus, it is still challenging how and why temporal information enhances
    gaze estimation performance for eye movement dynamics. Moreover, a deep understanding
    of eye movement patterns is required as the existing datasets are only based on
    task-based elicitation. It is also important to integrate the existing bio-mechanic
    eye models with data to achieve robust and data-efficient eye tracking. There
    are several open problems in this line of research in terms of eye movement dynamics
    (i.e. gaze directions, velocities, and gaze trajectories) in task-dependent as
    well as natural behaviors.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Transformer based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Transformer models have recently gotten attention for their notable performance
    on a broad range of vision tasks. Similarly, in the gaze estimation domain, there
    are two types of transformers used to date which are designed on top of the ViT
    framework. The first one is the pure transformer in gaze estimation (GazeTR-Pure) [[149](#bib.bib149)]
    and the other one is hybrid transformer in gaze estimation (GazeTR-Hybrid) [[149](#bib.bib149)].
    GazeTR-Pure [[149](#bib.bib149)] takes the cropped face as input along with an
    extra token. The extra token is a learnable embedding which aggregates the image
    features together. On the other hand, GazeTR-Hybrid [[149](#bib.bib149)] is comprised
    of CNN and transformer. It is based on the fact that gaze estimation is a regression
    task and it is quite difficult to get the perception of gaze with only local patch
    based correlation. These models take advantage of the transformer’s attention
    mechanism to improve gaze estimation performance. These are initial explorations
    using the transformer backbone. There is an immense possibility to explore this
    architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5 VAE/GAN based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Variational autoencoders and GANs have been used for unsupervised or self-supervised
    representation learning (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and
    Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of
    Deep Learning based Approaches")). Here, the latent space feature of the autoencoder
    model is used for gaze estimation inference [[29](#bib.bib29), [28](#bib.bib28),
    [101](#bib.bib101)]. Apart from representation learning, VAE and GAN based models
    are widely used for gaze redirection tasks [[150](#bib.bib150), [101](#bib.bib101),
    [102](#bib.bib102), [100](#bib.bib100)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'DT-ED. For representation learning via gaze redirection in a person independent
    manner, variational autoencoders are often utilized [[29](#bib.bib29), [101](#bib.bib101)].
    Disentangling Transforming Encoder-Decoder (DT-ED) framework [[29](#bib.bib29)]
    takes an input image $x$ and maps it to the latent space $z$ via an encoder $E$
    (i.e. $E(x):x\to z$). In the latent space, DT-ED disentangles three important
    factors relevant to gaze, i.e. gaze direction ($z_{g}$), head orientation ($z_{h}$),
    and the appearance of the eye region ($z_{a}$). Thus, $z$ can be expressed as:
    $z=\{z_{a};z_{g};z_{h}\}$. The framework disentangles these factors by explicitly
    applying constraints related to gaze and headpose rotations. Further, a decoder
    $D$ maps $z$ back to the redirected image (i.e. $D(E(x)):z\to\hat{x}$). The gaze
    direction is estimated from the $z_{g}$ part of the latent embedding. The overall
    illustration is shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches") (bottom row middle image).'
  prefs: []
  type: TYPE_NORMAL
- en: 'ST-ED. Similarly, the Self-Transforming Encoder-Decoder (ST-ED) architecture [[101](#bib.bib101)]
    (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze
    Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches") bottom row right image) takes a pair of images $x_{i}$ and $x_{t}$
    as input, disentangles the subject’s personal non-varying embeddings ($z^{0}_{i}$
    and $z^{0}_{t}$), considers pseudo-label conditions ($\hat{c_{i}}$ and $\hat{c_{t}}$)
    and embedding representations ($z_{i}$ and $z_{t}$). The learning objective for
    transformation depends on the pseudo condition labels which consider extraneous
    factors in absence of ground truth annotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gaze Redirection Network. The main motivation behind the unsupervised gaze
    redirection network [[28](#bib.bib28)] is capturing generic eye representation
    via gaze redirection (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and
    Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of
    Deep Learning based Approaches") bottom row left image). The framework takes eye
    patch $I_{i}$ as input and predict the redirected eye patch as output $I_{o}$
    while preserving the difference in rotation $\Delta_{r}=r_{i}-r_{o}=G_{\phi}(I_{i})-G_{\phi}(I_{o})$.
    In this work, gaze redirection is used as a pretext task for representation learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'RITnet. RITnet [[136](#bib.bib136)] (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1
    Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches") top row right image) is a hybrid
    version of U-Net and DenseNet based upon Fully Convolutional Networks (FCN). To
    balance the trade-off between the performance and computational complexity, it
    consists of 5 Down-Blocks in the encoder and 4 Up-Blocks in the decoder where
    the last layer of the encoder block is termed as the bottleneck layer. Each Down-Block
    has 5 convolution layers with LeakyReLU activation and the layers share connections
    with previous layers similar to DenseNet architecture. Similarly, each Up-Block
    has 4 convolution layers with LeakyReLU activation. All Up-Blocks have skip connection
    with their corresponding Down-Block which is an effective strategy to learn representation.
    To train the model, the following loss functions are used: 1) Standard cross-entropy
    loss (CEL) is applied pixel-wise to categorize each pixel into four categories
    (i.e. background, iris, sclera, and pupil). 2) Generalized Dice Loss (GDL) penalize
    the pixels on the basis of the overlap between the ground truth pixel and corresponding
    prediction. 3) Boundary Aware Loss (BAL) weights each pixel in terms of distance
    with its two nearest neighbour. This loss helps to avoid CEL confusion in boundary
    region. 4) Surface Loss (SL) helps to recover small regions and contours via distance
    based scaling. The overall loss is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\ell=\ell_{CEL}(\lambda_{1}+\lambda_{2}\ell_{BAL})+\lambda_{3}\ell_{GDL}+\lambda_{4}\ell_{SL}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Similarly, another lightweight model [[151](#bib.bib151)] uses MobileNet with
    depth-wise separable convolution for efficiency. It also utilize a squeeze and
    excitation (SE) module for performance enhancement by modelling channel independence.
    Moreover, the heuristic filtering of the connected component is utilized to enforce
    biological coherence in the network. Few works [[152](#bib.bib152), [153](#bib.bib153)]
    also use multi-class classification strategy for rich representation learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Statistical Modelling. The statistical inference based mapping is performed
    based on k-NN [[87](#bib.bib87)], support vector regression [[23](#bib.bib23),
    [87](#bib.bib87)] and random forest [[87](#bib.bib87), [20](#bib.bib20)]. A brief
    overview of these methods is summarised in Table [II](#S4.T2 "Table II ‣ 4.2.5
    VAE/GAN based ‣ 4.2 Representative Deep Network Architectures ‣ 4 Gaze Analysis
    Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches").
    Prior deep learning works on semantic eye segmentation is mainly focused on iris
    or sclera segmentation via Fuzzy C Means clustering, Otsu’s binarization, k-NN [[154](#bib.bib154)]
    etc. Sclera segmentation challenge was organized since 2015 to promote development
    in this area [[154](#bib.bib154), [133](#bib.bib133), [155](#bib.bib155)]. Recently,
    the OpenEDS challenge was organized in 2019 by Facebook Research in which eye
    segmentation was one of the sub-challenges. Most of the methods in this challenge
    use deep learning techniques [[136](#bib.bib136), [156](#bib.bib156), [144](#bib.bib144)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table II: A comparison of gaze analysis methods with respect to registration
    (Reg.), representation (Represent.), Level of Supervision, Model, Prediction,
    validation on benchmark datasets (validation), Platforms (Plat.), Publication
    venue (Publ.) and year. Here, GV: Gaze Vector, Scr.: Screen, LOSO: Leave One Subject
    Out, LPIPS: Learned Perceptual Image Patch Similarity, MM: Morphable Model, RRF:
    Random Regression Forest, AEM: Anatomic Eye Model, GRN: Gaze Regression Network,
    ET: External Target, FV: Free Viewing, HH: HandHeld Device, HMD: Head Mounted
    Device, Seg.: Segmentation and GR: Gaze Redirection, LAEO: Looking At Each Other.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Ref. | Reg. | Represent. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Level of &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Prediction | Validation | Plat. | Publ. | Year |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [[23](#bib.bib23)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face[[157](#bib.bib157)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Fully-Sup. | SVM |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Gaze locking &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[23](#bib.bib23)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; UIST &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2013 |'
  prefs: []
  type: TYPE_TB
- en: '| [[158](#bib.bib158)] | 3-D MM |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Convex Hull | 3-D GV |'
  prefs: []
  type: TYPE_TB
- en: '&#124;  [[158](#bib.bib158)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ET. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ETRA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 |'
  prefs: []
  type: TYPE_TB
- en: '| [[20](#bib.bib20)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, Eye [[20](#bib.bib20)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RRF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3-D GV |'
  prefs: []
  type: TYPE_TB
- en: '&#124;  [[20](#bib.bib20)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 |'
  prefs: []
  type: TYPE_TB
- en: '| [[159](#bib.bib159)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN+CLNF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICCV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2015 |'
  prefs: []
  type: TYPE_TB
- en: '| [[24](#bib.bib24)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, L/R Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN [[24](#bib.bib24)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2015 |'
  prefs: []
  type: TYPE_TB
- en: '| [[25](#bib.bib25)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, L/R Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; iTracker [[25](#bib.bib25)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2-D Scr. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[87](#bib.bib87), [25](#bib.bib25)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HH |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 |'
  prefs: []
  type: TYPE_TB
- en: '| [[160](#bib.bib160)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN [[25](#bib.bib25)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GR Img. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[160](#bib.bib160)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ECCV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 |'
  prefs: []
  type: TYPE_TB
- en: '| [[87](#bib.bib87)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye[[161](#bib.bib161)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SVR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2-D Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[87](#bib.bib87)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HH |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MVA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| [[54](#bib.bib54)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye [[162](#bib.bib162)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VGG-16+FC [[54](#bib.bib54)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[24](#bib.bib24), [54](#bib.bib54)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ECCV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| [[26](#bib.bib26)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eyes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ECCV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| [[57](#bib.bib57)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face [[104](#bib.bib104)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Geo.+Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN [[57](#bib.bib57)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[87](#bib.bib87), [23](#bib.bib23)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Desk. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| [[92](#bib.bib92)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Geo.+Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HGSM+c-BiGAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eye, GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| [[64](#bib.bib64)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, L/R Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dilated CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24), [25](#bib.bib25), [23](#bib.bib23)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ACCV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| [[29](#bib.bib29)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Few-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DT-ED+ML &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24), [25](#bib.bib25)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICCV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[90](#bib.bib90)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pinball LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24), [23](#bib.bib23), [87](#bib.bib87)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ET |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICCV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Eye | Appear. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SegNet [[163](#bib.bib163)] | Seg. Map |'
  prefs: []
  type: TYPE_TB
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[66](#bib.bib66)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, L/R Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DGTN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[66](#bib.bib66)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Desk. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[164](#bib.bib164)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MeNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24), [20](#bib.bib20), [25](#bib.bib25)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[145](#bib.bib145)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Semi/Unsup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; BCNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Desk. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[136](#bib.bib136)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eyes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hybrid U-net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Seg. Map &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[165](#bib.bib165)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eyes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Modified Resnet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Seg. Map &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[156](#bib.bib156)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eyes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eye-MMS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Seg. Map &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[144](#bib.bib144)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eyes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dilated CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Seg. Map &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[91](#bib.bib91)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eyes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear.+Seg. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Few-shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[24](#bib.bib24), [23](#bib.bib23)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[28](#bib.bib28)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eyes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Unsup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[24](#bib.bib24), [23](#bib.bib23)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[27](#bib.bib27)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Unsup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; IzeNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[23](#bib.bib23), [87](#bib.bib87)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| FV |'
  prefs: []
  type: TYPE_TB
- en: '&#124; IJCNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[166](#bib.bib166)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear.+Seg. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Seg2Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eye Img. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ICCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[167](#bib.bib167)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye Seq. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Unsup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Hier. HMM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eye Move. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[168](#bib.bib168)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ECCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[169](#bib.bib169)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Semi/Unsup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mSegNet+Discre. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Seg. Map &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ECCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[170](#bib.bib170)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Few-Shot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; EyeSeg &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Seg. Map &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ECCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [[101](#bib.bib101)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ST-ED &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[25](#bib.bib25), [23](#bib.bib23), [158](#bib.bib158)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; NeurIPS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[34](#bib.bib34)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Eye | Appear. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Modified ResNet | GR Img. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; [[34](#bib.bib34)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| HMD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ECCVW &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [[148](#bib.bib148)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eyes &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet-18+GRU &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PoG,3-D GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[148](#bib.bib148)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ECCV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [[94](#bib.bib94)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ResNet-50 | 3-D GV |'
  prefs: []
  type: TYPE_TB
- en: '&#124;  [[24](#bib.bib24), [25](#bib.bib25), [90](#bib.bib90), [158](#bib.bib158)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ECCV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [[171](#bib.bib171)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Semi-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GRN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[113](#bib.bib113)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| FV |'
  prefs: []
  type: TYPE_TB
- en: '&#124; WACV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [[60](#bib.bib60)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RSN+GazeNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[158](#bib.bib158), [24](#bib.bib24), [25](#bib.bib25)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; BMVC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [[59](#bib.bib59)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CA-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; AAAI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [[172](#bib.bib172)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; FAR-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[158](#bib.bib158), [24](#bib.bib24), [54](#bib.bib54)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TIP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [[102](#bib.bib102)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear.+AEM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MT c-GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eye Img. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[24](#bib.bib24), [23](#bib.bib23), [20](#bib.bib20)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; WACV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| [[89](#bib.bib89)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; AFF-Net &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Scr., GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[25](#bib.bib25), [51](#bib.bib51)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Arxiv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| [[173](#bib.bib173)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Unsup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PureGaze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Face, GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[94](#bib.bib94), [90](#bib.bib90), [24](#bib.bib24), [20](#bib.bib20)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Scr. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Arxiv &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| [[110](#bib.bib110)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Weakly-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet-18+LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[94](#bib.bib94), [90](#bib.bib90), [110](#bib.bib110), [25](#bib.bib25)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CVPR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| [[109](#bib.bib109)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fully-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LAEO-Net++ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LAEO &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[108](#bib.bib108)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TPAMI &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| [[174](#bib.bib174)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Face, Eye &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Appear. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Limited-Sup. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ResNet-50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[23](#bib.bib23), [24](#bib.bib24), [90](#bib.bib90), [40](#bib.bib40)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Any |'
  prefs: []
  type: TYPE_TB
- en: '&#124; WACV &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2022 |'
  prefs: []
  type: TYPE_TB
- en: 4.2.6 Discussion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In an attempt to summarize the recent deep network based gaze analysis methods,
    we present some main take away points as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The overall gaze estimation methods are divided into two broad categories:
    1) 2-D Gaze Estimation: In this context the proposed methods map the input image
    to 2-D Point of Regard (PoR) in the visual plane. The visual planes could either
    be the observable object or screen. Non deep learning methods or early deep learning
    methods [[21](#bib.bib21), [24](#bib.bib24), [50](#bib.bib50), [51](#bib.bib51)]
    perform these mappings. 2) 3-D Gaze Estimation: The 3-D gaze estimation basically
    considers the gaze vector instead of 2-D PoR. The gaze vector is the line joining
    the pupil center point with the point of regard. Recent works [[94](#bib.bib94),
    [26](#bib.bib26), [29](#bib.bib29), [148](#bib.bib148), [110](#bib.bib110)] mainly
    relies on 3-D gaze estimation methods. The choice of gaze estimation methods rely
    on the application and requirement.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Single branch CNN based architectures [[24](#bib.bib24), [51](#bib.bib51), [50](#bib.bib50),
    [25](#bib.bib25), [26](#bib.bib26), [92](#bib.bib92)] are widely used over the
    past few years for progressive improvements on benchmark datasets. The input to
    these networks are restricted to single eye, eye patch or face. Thus, to further
    boost the performance, multi branch networks are proposed which utilize eyes,
    face, geometric constraints, visual plane grid as input.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both single or multi branch networks depend on spatial information. However,
    eye movement is dynamic in nature. Thus, few recent proposed architectures [[90](#bib.bib90),
    [92](#bib.bib92)] use temporal information for inference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For representation learning, VAE and GAN based architectures [[29](#bib.bib29),
    [28](#bib.bib28), [101](#bib.bib101)] are explored. However, it is observed that
    these architectures could have high time complexity as compared to single or multi
    branch CNN.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prior based appearance encoding is another line of approaches for encoding
    rich feature representation. Few works have defined priors based on eye anatomy [[26](#bib.bib26)],
    geometrical constraint [[172](#bib.bib172)] as biases for better generalization.
    Despite direct appearance encoding, Park et al. [[26](#bib.bib26)] proposed an
    intermediate pictorial representation, termed a ‘gazemap’ (refer Fig. [6](#S4.F6
    "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")) of the eye to simplify
    the gaze estimation task. Similarly, the ‘two eye asymmetry’ property is utilized
    for gaze estimation [[172](#bib.bib172)] where the underlying hypothesis is that
    despite the difference in appearances of two eyes due to environmental factors,
    the gaze directions remains approximately the same. The CNN based regression model
    is assumed to be independent of identity distribution, however, due to the subject-specific
    offset of the nodal point of the eyes, gaze datasets have identity specific bias.
    Xiong et al. [[164](#bib.bib164)] inject this bias as a prior by mixing different
    models. Similarly, to handle this offset, the gaze is decomposed into the subject
    independent and dependent bias for performance enhancement and better generalization [[175](#bib.bib175)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to train the deep learning based models, $\ell_{2}$ [[24](#bib.bib24),
    [51](#bib.bib51), [50](#bib.bib50)] and cosine similarity based losses [[94](#bib.bib94),
    [148](#bib.bib148)] are used. However, a novel pinball loss [[90](#bib.bib90)]
    is proposed to model the uncertainty in gaze estimation, especially in unconstrained
    settings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Similarly for deep learning based eye segmentation approaches, the eye image
    to segmentation mapping is performed in a non-parametric way which implicitly
    encodes shape, geometry, appearance and other factors [[176](#bib.bib176), [165](#bib.bib165),
    [152](#bib.bib152), [136](#bib.bib136), [67](#bib.bib67), [32](#bib.bib32)]. The
    most popular network architectures for eye segmentation are U-net [[177](#bib.bib177)],
    modified version of SegNet [[32](#bib.bib32)], RITnet [[136](#bib.bib136)], EyeNet [[165](#bib.bib165)].
    These VAE based architechtures have high time and space complexity. However, recent
    methods [[165](#bib.bib165), [136](#bib.bib136)] do consider these factors without
    compromising the performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Level of Supervision
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the type of supervision, the training procedure can be classified
    into the following categories: fully-supervised, Semi-/Self-/weakly-/unsupervised.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.1 Fully-Supervised.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Supervised learning paradigm is the most commonly used training framework in
    gaze estimation literature [[23](#bib.bib23), [20](#bib.bib20), [24](#bib.bib24),
    [87](#bib.bib87), [26](#bib.bib26), [97](#bib.bib97)] and eye segmentation literature [[136](#bib.bib136),
    [165](#bib.bib165), [156](#bib.bib156), [144](#bib.bib144), [135](#bib.bib135),
    [154](#bib.bib154), [133](#bib.bib133), [155](#bib.bib155), [11](#bib.bib11)].
    As the fully-supervised methods require a lot of accurately annotated data. Accurate
    annotation of gaze data is a complex, noise-prone, and time-consuming task and
    sometimes it requires expensive data acquisition setups. Moreover, there is a
    high possibility of noisy or wrong annotation due to distraction in participation
    during data collection, eye blink activity and inherent measurement errors in
    data curation settings. Variation in data curation setup limits merging multiple
    datasets for supervision. Dataset specific data acquisition processes are discussed
    in Sec. [5.1](#S5.SS1 "5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches"). Thus, the research
    community is moving towards learning with less supervision.'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Task Learning. Multi-task learning incorporates different tasks which
    provide auxiliary information as a bias to improve model performance. The auxiliary
    information can be Gaze+Landmark [[61](#bib.bib61)], PoG+Screen saliency [[148](#bib.bib148),
    [178](#bib.bib178)], Gaze+Depth [[71](#bib.bib71)], Gaze+Headpose [[52](#bib.bib52)],
    Segmentation+Gaze [[67](#bib.bib67)] and Gaze-direction+Gaze-uncertainty [[90](#bib.bib90)].
    These gaze aligned tasks facilitate strong representation learning with additional
    task based supervision.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f83a4b9daf9eba9102a42b329b6f0cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Data collection procedure in different settings for benchmark datasets.
    From left to right the examples are from CAVE [[23](#bib.bib23)], Eth-XGaze [[94](#bib.bib94)],
    MPII [[50](#bib.bib50)] and Gaze360 [[90](#bib.bib90)] datasets. The leftmost
    one is more constrained and the rightmost one is less constrained. Images are
    taken from respective datasets [[90](#bib.bib90), [94](#bib.bib94), [50](#bib.bib50),
    [23](#bib.bib23)]. Refer Table [III](#S5.T3 "Table III ‣ 5.1 Datasets for Gaze
    Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3.2 Semi-/Self-/Weakly-/Unsupervised.
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To a large extent, the supervised deep learning based methods’ performance
    depends on the quality and quantity of annotated data. However, manual labeling
    of gaze data is a complex, time consuming and labor extensive process. On this
    front, Semi-/Self-/Weakly-/Unsupervised Learning paradigms provide a promising
    alternative to enable learning from a vast amount of readily available non-annotated
    data. For learning paradigms with less supervision, the important methods are
    described in detail below:'
  prefs: []
  type: TYPE_NORMAL
- en: Weakly-supervised and Learning from Pseudo Labels. Weakly supervised learning
    aims to bridge the gap between the fully-supervised and fully-unsupervised techniques.
    Till date in gaze estimation domain, the weak supervision has been performed via
    ‘Looking At Each Other (LAEO)’ [[179](#bib.bib179)] and pseudo labelling [[174](#bib.bib174)].
    For weak supervision, Kothari et al. [[179](#bib.bib179)] leverage strong gaze-related
    geometric constraints from two people interaction scenario. On the other hand,
    MTGLS [[174](#bib.bib174)] framework leverages from non-annotated facial image
    data by three complementary signals i.e. (1) the line of sight of the pupil, (2)
    the head-pose and (3) the eye dexterity.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised and Self-supervised Representation Learning. Self supervised learning
    has emerged as a popular technique for learning meaningful representations from
    vast amount of non-annotated data. It requires pseudo labels for any pre-designed
    task which is often termed as Auxiliary or Pretext Task. The pre-designed task
    is mostly aligned with the gaze estimation. Dubey et al. [[27](#bib.bib27)] propose
    a pretext task where the visual regions of the gaze are divided into zones by
    geometric constraints. These pseudo labels are utilized for representation learning.
    Yu et al. [[28](#bib.bib28)] use subject specific gaze redirection as a pretext
    task. Swapping Affine Transformations (SwAT) [[180](#bib.bib180)] is the extended
    version of Swapping Assignments Between Views (SwAV), a popular self supervised
    learning framework used for gaze representation learning using different augmentation
    techniques. The self-supervised representation learning has the potential to eliminate
    the major drawback of gaze data annotation which is quite difficult and error
    prone. Future directions in this area may include designing better pre-text tasks
    and combining multiple pre-text tasks to jointly pre-train the models [[174](#bib.bib174)].
    In addition, combining data-driven eye tracking with model-based eye tracking
    can be another future direction as model-based eye tracking can provide pseudo-labels
    or pre-train the models.
  prefs: []
  type: TYPE_NORMAL
- en: Few Shot Learning. Few-shot learning aims to adapt to a new task with very few
    examples [[29](#bib.bib29), [91](#bib.bib91)]. The main challenge in few shot
    paradigm is over-fitting issue since highly over-parameterized deep networks are
    involved to learn from only a few training samples. To this front, mainly gaze
    redirection strategy [[91](#bib.bib91)] and Few-shot Adaptive GaZE Estimation
    (FAZE) [[29](#bib.bib29)] frameworks are proposed. Among them, FAZE is shown a
    two stage adaptation strategy. In the first stage, a rotation aware latent space
    embedding is learned based on encoder-decoder framework. Further, adaptation is
    performed on top of the features using MAML which is a popular meta-learning paradigm.
    FAZE is able to adapt to a new subject with $\leq 9$ sample which is quite promising.
  prefs: []
  type: TYPE_NORMAL
- en: Learning-by-synthesis. The term ‘learning-by-synthesis’ is coined by Sugano
    et al. [[20](#bib.bib20)]. The main objective is to synthesize different gaze
    viewpoints to multiply the data from a quantitative and a qualitative perspectives
    rather than manual labelling. Few other studies [[20](#bib.bib20), [160](#bib.bib160),
    [181](#bib.bib181), [182](#bib.bib182), [91](#bib.bib91), [183](#bib.bib183),
    [92](#bib.bib92)] also adopt data generation methods which can address the diversity
    in terms of headpose and eye rotation. However, these generative models have high
    computational complexity and are constrained by the quality of generated images.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion. In gaze estimation domain, there are still very few works in semi-/self-/weakly-/unsupervised
    learning paradigms. Among these works, learning from pseudo labels have their
    limitations as the label space contains noise. On the other hand, gaze redirection
    synthesis is based on the availability of same or different person’s data with
    eye rotation or the prior knowledge of rotation angle. Thus, learning robust and
    generalizable gaze representations using minimal or no supervision still remains
    an open-ended research question. One possible direction to alleviate this problem
    is to combine data-driven eye tracking with model-based eye tracking to produce
    physically plausible eye tracking models that are data efficient during training
    and generalize better during testing.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we review the commonly followed evaluation procedures on various
    datasets along with the metrics adopted in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Datasets for Gaze Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the rapid progress in the gaze analysis domain, several datasets have
    been proposed for different gaze analysis tasks (see Sec. [3](#S3 "3 Gaze Analysis
    in Computer Vision ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches")). The dataset collection technique has evolved from constrained lab
    environments [[23](#bib.bib23)] to unconstrained indoor [[87](#bib.bib87), [50](#bib.bib50),
    [51](#bib.bib51), [54](#bib.bib54)] and outdoor settings [[90](#bib.bib90)] (Refer
    Fig. [7](#S4.F7 "Figure 7 ‣ 4.3.1 Fully-Supervised. ‣ 4.3 Level of Supervision
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")). We provide a detailed overview of the datasets in Table [III](#S5.T3
    "Table III ‣ 5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches"). Compared with early datasets [[23](#bib.bib23),
    [158](#bib.bib158)], recently released datasets [[90](#bib.bib90), [148](#bib.bib148)]
    are typically more advanced with less bias, improved complexity, and larger in
    scale. These are better suited for training and evaluation. We describe a few
    important datasets below:'
  prefs: []
  type: TYPE_NORMAL
- en: CAVE [[23](#bib.bib23)] contains 5,880 images of 56 subjects with different
    gaze directions and head poses. There are 21 different gaze directions for each
    person and the data was collected in a constrained lab environment, with 7 horizontal
    and 3 vertical gaze locations.
  prefs: []
  type: TYPE_NORMAL
- en: The Eyediap dataset [[184](#bib.bib184)] was designed to overcome the main challenges
    associated with the head pose, person and 3-D target variations along with changes
    in ambient and sensing conditions.
  prefs: []
  type: TYPE_NORMAL
- en: TabletGaze [[87](#bib.bib87)] is a large unconstrained dataset of 51 subjects
    with 4 different postures and 35 gaze locations collected using a tablet in an
    indoor environment. TabletGaze dataset is also collected in a $7\times 5$ grid
    format.
  prefs: []
  type: TYPE_NORMAL
- en: MPII [[50](#bib.bib50)] gaze dataset contains 213,659 images collected from
    15 subjects during natural everyday events in front of a laptop over a three-month
    duration. MPII gaze dataset is collected by showing random points on the laptop
    screen to the participants. Further, Zhang et al. [[51](#bib.bib51)] curate MPIIFaceGaze
    dataset with the hypothesis that gaze can be more accurately predicted when the
    entire face is considered.
  prefs: []
  type: TYPE_NORMAL
- en: RT-GENE dataset [[54](#bib.bib54)] is recorded in a more naturalistic environment
    with varied gaze and head pose angles. The ground truth annotation was done using
    a motion capture system with mobile eye-tracking glasses.
  prefs: []
  type: TYPE_NORMAL
- en: Gaze360 [[90](#bib.bib90)] is a large-scale gaze estimation dataset collected
    from 238 subjects in unconstrained indoor and outdoor settings with a wide range
    of head pose.
  prefs: []
  type: TYPE_NORMAL
- en: ETH-XGaze [[94](#bib.bib94)] is a large scale dataset collected in a constraint
    environment with a wide range of head pose, high-resolution images. The dataset
    contains images from different camera positions, illumination conditions to add
    more challenges to the data.
  prefs: []
  type: TYPE_NORMAL
- en: EVE [[148](#bib.bib148)] is also collected in constraint indoor setting with
    different camera views to map human gaze in screen co-ordinate.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to gaze estimation several benchmark datasets have been proposed over
    the past few years for eye and sclera segmentation. The datasets collected for
    sclera segmentation is in a constraint environment and with very few subjects [[185](#bib.bib185),
    [186](#bib.bib186), [187](#bib.bib187)]. A more challenging publicly available
    dataset was released in sclera recognition challenges [[154](#bib.bib154), [133](#bib.bib133),
    [155](#bib.bib155)]. Recently, a large scale dataset termed as OpenEDS: Open Eye
    Dataset [[32](#bib.bib32)], is released which contains eye images collected by
    using a VR head-mounted device. Additionally, there was two synchronized eye facing
    cameras having a frame rate of 200 Hz. The data was collected under controlled
    illumination and contains 12,759 images with eye segmentation masks collected
    from 152 participants.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table III: Datasets. A comparison of gaze datasets with respect to several
    attributes (i.e. number of subjects (# sub), gaze labels, modality, headpose and
    gaze angle in yaw and pitch axis, environment (Env.), baseline method, data statistics
    (# data), and year of publication.) The abbreviations used are: In: Indoor, Out:
    Outdoor, Both: Indoor + Outdoor, Gen.: Generation, u/k: unknown, Seq.: Sequence,
    VF: Visual Field, EB: Eye Blink, GE: Gaze Event [[179](#bib.bib179)], GBRT: Gradient
    Boosting Regression Trees, GC: Gaze Communication, GNN: Graph Neural Network and
    Seg.: Segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | # Sub | Label | Modality | Head-Pose | Gaze | Env. | Baseline |
    # Data | Year |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| [CAVE](https://www.cs.columbia.edu/CAVE/databases/columbia_gaze/) [[23](#bib.bib23)]
    | 56 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.:$5184\times 3456$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $0$°, $\pm 30$° | $\pm 15$°, $\pm 10$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SVM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:Cross-val &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:5880 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2013 |'
  prefs: []
  type: TYPE_TB
- en: '| [EYEDIAP](https://www.idiap.ch/en/dataset/eyediap) [[158](#bib.bib158)] |
    16 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: HD and VGA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 15$°, $30$° | $\pm 25$°, $20$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Convex Hull &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:237 min &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 |'
  prefs: []
  type: TYPE_TB
- en: '| [UT MV](https://www.ut-vision.org/datasets/) [[20](#bib.bib20)] | 50 | 3-D
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.:$1280\times 1024$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 36$°, $\pm 36$° | $\pm 50$°, $\pm 36$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Random Reg. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Forests &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:64,000 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2014 |'
  prefs: []
  type: TYPE_TB
- en: '| OMEG [[188](#bib.bib188)] | 50 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $1280\times 1024$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $0$°, $\pm 30$° |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $-38$°to $+36$°, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $-10$°to $+29$° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SVR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:LOSO &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:44,827 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2015 |'
  prefs: []
  type: TYPE_TB
- en: '| [MPIIGaze](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild) [[24](#bib.bib24)]
    | 15 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 15$°, $30$° | $\pm 20$°, $\pm 20$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN variant [[24](#bib.bib24)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:LOSO &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:213,659 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2015 |'
  prefs: []
  type: TYPE_TB
- en: '| [GazeFollow](http://gazefollow.csail.mit.edu/index.html) [[189](#bib.bib189)]
    | 130,339 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: Variable &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Variable | Both |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN variant [[189](#bib.bib189)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:122,143 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2015 |'
  prefs: []
  type: TYPE_TB
- en: '| [SynthesEye](https://www.cl.cam.ac.uk/research/rainbow/projects/syntheseyes/)[[159](#bib.bib159)]
    | NA | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.:$120\times 80$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 50$°, $\pm 50$° | $\pm 50$°, $\pm 50$° | Syn |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN [[159](#bib.bib159)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:11,400 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2015 |'
  prefs: []
  type: TYPE_TB
- en: '| [GazeCapture](https://gazecapture.csail.mit.edu/) [[25](#bib.bib25)] | 1450
    | 2-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.:$640\times 480$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 30$°, $40$° | $\pm 20$°, $\pm 20$° | Both |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN [[25](#bib.bib25)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:2,445,504 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 |'
  prefs: []
  type: TYPE_TB
- en: '| [UnityEyes](https://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/) [[190](#bib.bib190)]
    | NA | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.:$400\times 300$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Variable | Syn |'
  prefs: []
  type: TYPE_TB
- en: '&#124; KNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:NA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 1,000,000 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2016 |'
  prefs: []
  type: TYPE_TB
- en: '| [TabletGaze](https://sh.rice.edu/cognitive-engagement/%20tabletgaze/) [[87](#bib.bib87)]
    | 51 | 2-D Sc. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Video &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 50$°, $\pm 50$° | $\pm 20$°, $\pm 20$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SVR &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:Cross-val &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:816 Seq. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\sim$ 300,000 img. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [MPIIFaceGaze](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/its-written-all-over-your-face-full-face-appearance-based-gaze-estimation)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[51](#bib.bib51)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 15 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 15$°, $30$° | $\pm 20$°, $\pm 20$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN variant [[51](#bib.bib51)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:LOSO &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:213,659 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| [InvisibleEye](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/invisibleeye-mobile-eye-tracking-using-multiple-low-resolution-cameras-and-learning-based-gaze-estimation) [[70](#bib.bib70)]
    | 17 | 2-D Sc |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $5\times 5$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Unknown |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $2560\times 1600$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; pixel VF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ANN [[70](#bib.bib70)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:280,000 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| [RT-GENE](https://github.com/Tobias-Fischer/rt_gene) [[54](#bib.bib54)] |
    15 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.:$1920\times 1080$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 40$°, $\pm 40$° | $\pm 40$°, $-40$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNN [[54](#bib.bib54)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.:Cross val &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:122,531 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| [Gaze 360](http://gaze360.csail.mit.edu/) [[90](#bib.bib90)] | 238 | 3-D
    |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.:$4096\times 3382$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 90$°, u/k | $\pm 140$°, $-50$° |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Both &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pinball LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 172,000 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [RT-BENE](https://github.com/Tobias-Fischer/rt_gene)[[191](#bib.bib191)]
    | 17 | EB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $1920\times 1080$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 40$°, $\pm 40$° | $\pm 40$°, $-40$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CNNs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Cross val &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 243,714 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [NV Gaze](https://sites.google.com/nvidia.com/nvgaze) [[68](#bib.bib68)]
    | 30 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; 3-D, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Seg. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Image (Synthetic) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.:$1280\times 960$, &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $640\times 480$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Unknown |'
  prefs: []
  type: TYPE_TB
- en: '&#124; $30$°$\times 40$° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VF &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Both &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN [[192](#bib.bib192)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 2,500,000 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [HUST-LEBW](https://github.com/thorhu/Eyeblink-in-the-wild) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[143](#bib.bib143)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 172 | EB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Video &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Variable | Both |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MS-LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 673 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [VACATION](https://github.com/LifengFan/Human-Gaze-Communication) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;  [[111](#bib.bib111)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 206,774 | GC |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Video &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $640\times 360$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Variable | Both |'
  prefs: []
  type: TYPE_TB
- en: '&#124; GNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 96,993 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [OpenEDS-19](https://research.facebook.com/openeds-challenge/) [[32](#bib.bib32)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Track 1: Semantic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 152 | Seg. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $640\times 400$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Unknown | Unknown | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SegNet [[163](#bib.bib163)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total:12,759 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (in # SegSeq [[32](#bib.bib32)]) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [OpenEDS-19](https://research.facebook.com/openeds-challenge/) [[32](#bib.bib32)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Track 2: Synthetic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eye Generation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 152 | Gen. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $640\times 400$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Unknown | Unknown | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 252,690 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2019 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [OpenEDS-20](https://research.facebook.com/openeds-2020-challenge/) [[34](#bib.bib34)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Track 1: Gaze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Prediction &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 90 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $640\times 400$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Unknown | $\pm 20$°, $\pm 20$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Modified ResNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 8,960 Seq., &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 550,400 img. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [OpenEDS-20](https://research.facebook.com/openeds-2020-challenge/) [[34](#bib.bib34)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Track 2: Sparse &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Temporal Semantic &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Segmentation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 90 | Seg. |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $640\times 400$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Unknown | $\pm 20$°, $\pm 20$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; SegNet [[163](#bib.bib163)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; (Power &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Efficient version) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 200 Seq. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 29,500 img. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [mEBAL](https://github.com/BiDAlab/mEBAL)[[142](#bib.bib142)] | 38 | EB |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Variable | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; VGG-16 Varient &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 756,000 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [ETH-XGaze](https://ait.ethz.ch/projects/2020/ETH-XGaze/) [[94](#bib.bib94)]
    | 110 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $6000\times 4000$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 80$°, $\pm 80$° | $\pm 120$°, $\pm 70$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNet-50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 1,083,492 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [EVE](https://ait.ethz.ch/projects/2020/EVE/) [[148](#bib.bib148)] | 54 |
    3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $6000\times 4000$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| $\pm 80$°, $\pm 80$° | $\pm 80$°, $\pm 80$° | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNet-18 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 12,308,334 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [GW](http://www.cis.rit.edu/%C2%A0rsk3900/gaze-in-wild/) [[179](#bib.bib179)]
    | 19 | GE |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $1920\times 1080$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Variable | In |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: $\sim$ 5,800,000 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [LAEO](https://github.com/AVAuco/ucolaeodb) [[110](#bib.bib110)] | 485 |
    3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: Variable &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Variable | Both |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNet-18+LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 800,000 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| [GOO](https://github.com/upeee/GOO-GAZE2021) [[193](#bib.bib193)] | 100 |
    3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: Variable &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Variable | Both |'
  prefs: []
  type: TYPE_TB
- en: '&#124; ResNet-50 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 201,552 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| [OpenNEEDS](https://research.facebook.com/publications/openneeds-a-dataset-of-gaze-head-hand-and-scene-signals-during-exploration-in-open-ended-vr-environments/) [[194](#bib.bib194)]
    | 44 | 3-D |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Image &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim.: $128\times 71$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Variable | Variable | VR |'
  prefs: []
  type: TYPE_TB
- en: '&#124; GBRT &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Eval.: Hold out &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Total: 2,086,507 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 2021 |'
  prefs: []
  type: TYPE_TB
- en: 'Table IV: Cross-Dataset Study. Cross dataset generalization study on different
    gaze estimation datasets in terms of angular error (in °).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Test$\rightarrow$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Train &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; $\downarrow$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Datasets &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Pinball-LSTM [[90](#bib.bib90)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; CAVE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MPIIFace &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RT-GENE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaze360 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAVE &#124; MPIIFace &#124; RT-GENE &#124; Gaze360 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; – &#124; 12.3° &#124; 32.8° &#124; 57.9° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 12.4° &#124; – &#124; 26.5° &#124; 57.8° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 24.2° &#124; 18.9 &#124; – &#124; 56.6° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 9.0° &#124; 12.1 &#124; 13.4° &#124; – &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| ETH-X Gaze [[94](#bib.bib94)] |'
  prefs: []
  type: TYPE_TB
- en: '&#124; MPIIGaze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; EYEDIAP &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaze-Capture &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RT-GENE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaze360 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ETHXGaze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MPIIGaze &#124; EYEDIAP &#124; Gaze-Capture &#124; RT-GENE &#124; Gaze360
    &#124; ETH-X Gaze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; – &#124; 17.9° &#124; 6.3° &#124; 14.9° &#124; 31.7° &#124; 34.9° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 16.9° &#124; – &#124; 14.2° &#124; 15.6° &#124; 33.7° &#124; 41.7° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 4.5° &#124; 13.7° &#124; – &#124; 14.7° &#124; 30.2° &#124; 29.4° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 12.0° &#124; 21.2° &#124; 13.2° &#124; – &#124; 34.7° &#124; 42.6° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 10.3° &#124; 11.3° &#124; 12.9° &#124; 26.6° &#124; – &#124; 17.0° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 7.5° &#124; 11.0° &#124; 10.5° &#124; 31.2° &#124; 27.3° &#124; – &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Data Generation/Gaze Redirection. Since gaze data collection and annotation
    is an expensive and time-consuming process, the research community moves towards
    a data generation process for benchmarking with a large variation in data attributes.
    Prior works in this domain generate both synthetic and real images. The methods
    are based on Generative Adversarial Networks (GANs). To capture the possible rotational
    variation in images, gaze redirection techniques [[160](#bib.bib160), [181](#bib.bib181),
    [182](#bib.bib182), [91](#bib.bib91), [183](#bib.bib183)] are quite popular. An
    early work on gaze manipulation [[195](#bib.bib195)] uses pre-recording of several
    potential eye replacements during test time. Further, Kononenko et al. [[196](#bib.bib196)]
    propose wrapping based gaze redirection using supervised learning, which learns
    the gaze redirection via a flow field to move eye pupil and relevant pixels from
    the input image to the output image. The gaze re-direction methods may struggle
    with extrapolation since it depends on the training samples and training methods.
    Moreover, these works suffer from low-quality generation and low redirection precision.
    To overcome this, Chen et al. [[100](#bib.bib100)] propose a MultiModal-Guided
    Gaze Redirection (MGGR) framework which uses gaze-map images and target angles
    to adjust a given eye appearance via learning. The other approaches are mainly
    based on random forest [[196](#bib.bib196)] and style transfer [[197](#bib.bib197)].
    Random forest is used to decide the possible gaze direction and in style transfer,
    the appearance based feature is mainly encoded. Sela et al. [[197](#bib.bib197)]
    propose a GAN based framework to generate a large dataset of high-resolution eye
    images having diversity in subjects, head pose, camera settings and realism. However,
    the GAN based methods lack in their capability to preserve content (i.e. eye shape)
    for benchmarking. Buhler et al. [[166](#bib.bib166)] synthesize person-specific
    eye images with a given semantic segmentation mask by preserving the style and
    content of the reference images. In summary, we can say that although a lot of
    effort has been made to generate realistic eye images, but due to several limitations
    (perfect gaze direction, image quality), these images are not used for benchmarking.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Evaluation Strategy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we describe the most widely used gaze metrics in the gaze analysis
    domain.
  prefs: []
  type: TYPE_NORMAL
- en: Gaze Estimation. The most common practice to measure the gaze estimation accuracy/error
    is in terms of angular error (in °) [[26](#bib.bib26), [94](#bib.bib94), [29](#bib.bib29),
    [148](#bib.bib148)] and gaze location (in pixels or cm/mm(s)) [[87](#bib.bib87),
    [148](#bib.bib148)]. The angular error is measured between the actual gaze direction
    ($\mathbf{g}\in\mathbb{R}^{3}$) and predicted gaze direction ($\hat{\mathbf{g}}\in\mathbb{R}^{3}$)
    defined as $\frac{\mathbf{g}.\hat{\mathbf{g}}}{||\mathbf{g}||.||\hat{\mathbf{g}}||}$.
    On the other hand, Euclidean distance is measured between the original and predicted
    point of gaze (PoG).
  prefs: []
  type: TYPE_NORMAL
- en: Gaze Redirection. The gaze redirection evaluation is performed in both quantitative
    and qualitative manner [[101](#bib.bib101), [100](#bib.bib100), [102](#bib.bib102)].
    The quantitative analysis is done in terms of angular gaze redirection error estimated
    between the predicted values and their intended target values. As in this task,
    the moment of the eye pupil is pre-defined, thus, this angular error weakly quantifies
    how perfectly the eye redirection occurs, although the method for measuring the
    angle has some inherent noise. For qualitative analysis, the Learned Perceptual
    Image Patch Similarity (LPIPS) metric is used which measures the paired image
    similarity in the gaze redirection task.
  prefs: []
  type: TYPE_NORMAL
- en: Eye Segmentation. Commonly used evaluation metric for eye segmentation methods,
    is average of the mean Intersection over Union (mIoU). Although, for the recent
    OpenEDS challenge [[32](#bib.bib32)], the mIoU metric is calculated for all classes
    and model size (S) is calculated as a function of a number of trainable parameters
    in megabytes (MB).
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Cross Dataset Analysis.
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Datasets play an important role in defining the research progress made in gaze
    analysis. Apart from serving as a source for training models, it helps to quantify
    the performance measure. In the gaze analysis domain, the aim of dataset curation
    is to capture the real-world scenario setting as close as possible. Thus, it is
    necessary to evaluate the robustness and generalizability of the models across
    different data acquisition setups for better adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 'On this front, we explore two aspects: First of all, we explore the cross-dataset
    generalizability of gaze estimation methods based on two SOTA models i.e. Pinball-LSTM [[90](#bib.bib90)]
    and ETH-X-Gaze [[94](#bib.bib94)]. For this purpose, the training is performed
    on one dataset while the testing is conducted on the other dataset. (Refer Table [IV](#S5.T4
    "Table IV ‣ 5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")). Further, we explore the SOTA method’s
    performance on different datasets to show the robustness of the model (Refer Table [V](#S5.T5
    "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")). Angular error (in °) is used as
    an evaluation metric. Further to generalize across datasets, we calculate the
    mean angular error across datasets. Below are some of the important observations
    inferred from our experimental results.'
  prefs: []
  type: TYPE_NORMAL
- en: Data Collection Settings. Dataset collection setup plays an important role in
    generalizability and method’s robustness. As the CAVE dataset is collected in
    a constrained setup and it has high-resolution images, the models trained on this
    data fail to adapt well to the data with low resolution and synthetic images.
    Thus, the pinball-LSTM trained on CAVE data has a high error in Gaze360 ($\sim$
    57.9°) and RT-GENE ($\sim$ 32.8°) datasets. A similar pattern is observed in the
    case of the MPII dataset as well. Model trained on this dataset gives high error
    in adapting RT-GENE ($\sim$ 26.5°) and Gaze360 ($\sim$ 57.8°).
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross Dataset Generalization. By observing the cross dataset generalization
    performance, we can determine how diverse the training dataset is from a generalization
    perspective. From Table [IV](#S5.T4 "Table IV ‣ 5.1 Datasets for Gaze Analysis
    ‣ 5 Validation ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches"),
    we observe that Gaze360, Gaze-Capture, and ETH-X Gaze datasets are the most challenging
    datasets. Training models on these two datasets would be a good choice as it has
    better generalization performance across different datasets. In contrast, RT-GENE
    and Gaze-capture contain significant biases and training models on them will lead
    to poor cross-dataset generalization performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Robust Modelling. In order to study the robustness of a model trained on any
    dataset, we recommend evaluating the model on Gaze360, Gaze-Capture, and ETH-X
    Gaze datasets. These datasets exhibit multiple variations in terms of background
    environment, eye visibility, occlusion, low-resolution images and could serve
    as important indicators for real-world adaptation. We also recommend training
    the models on all benchmark datasets together and expect a better generalization
    than training on individual datasets in novel or in-the-wild settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table V: Where we stand now. Chronological comparison of the performance of
    different models for the gaze-related tasks on related benchmark datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Methods | Datasets | Year |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Estimation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GazeNet [[24](#bib.bib24)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dilated-Conv. [[64](#bib.bib64)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Landmark based [[97](#bib.bib97)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RT-GENE [[54](#bib.bib54)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Pinball-LSTM [[90](#bib.bib90)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CA-Net [[59](#bib.bib59)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [GazeTR-Hybrid](https://github.com/yihuacheng/GazeTR) [[149](#bib.bib149)]
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAVE &#124; MPIIGaze &#124; EYEDIAP &#124; UT MV &#124; MPIIFace &#124;
    Gaze360 &#124; ETH-X Gaze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; – &#124; 5.70° &#124; 7.13° &#124; 6.44° &#124; 5.76° &#124; – &#124;
    – &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; – &#124; 4.39° &#124; 6.57° &#124; – &#124; 4.42° &#124; 13.73° &#124;
    – &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 8.7° &#124; 8.3° &#124; 26.6° &#124; – &#124; – &#124; – &#124; – &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; – &#124; 4.61° &#124; 6.30° &#124; – &#124; 4.66° &#124; 12.26 &#124;
    – &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 9.0° &#124; 12.1° &#124; 5.58° &#124; – &#124; 12.1° &#124; 11.04° &#124;
    4.46° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; – &#124; 4.27° &#124; 5.63° &#124; – &#124; 4.27° &#124; 11.20° &#124;
    – &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2015 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2018 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2018 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2019 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2020 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2021 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CAVE                                MPIIGaze                               
    Eth-X-Gaze                                Gaze360 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; <svg version="1.1" width="708" height="299.77" overflow="visible"><g
    transform="translate(0,299.77) scale(1,-1)"><g class="makebox" transform="translate(0,0)"><g
    transform="translate(0,99) scale(1, -1)"><foreignobject width="177" height="99"
    overflow="visible">![[Uncaptioned image]](img/48a0aea43ea0880805f9894f3ed96d12.png)</foreignobject></g></g><g
    class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1,
    -1)"><foreignobject width="177" height="99" overflow="visible">![[Uncaptioned
    image]](img/840c52201db3080a9432864eab9fe3ee.png)</foreignobject></g></g><g class="makebox"
    transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignobject
    width="177" height="99" overflow="visible">![[Uncaptioned image]](img/50ce207c826a845fbd20e093d130a6fa.png)</foreignobject></g></g><g
    class="makebox" transform="translate(0,0)"><g transform="translate(0,99) scale(1,
    -1)"><foreignobject width="177" height="99" overflow="visible">![[Uncaptioned
    image]](img/7a7fa04f4edbb116ab0349c55d877340.png)</foreignobject></g></g></g></svg>
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| In the diagrams, the y-axis represents the angular error (in °) and the x-axis
    represents the timeline. |'
  prefs: []
  type: TYPE_TB
- en: '| Task | Datasets | Methods (Eval.AI) | Year |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Trajectory &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Estimation &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Team-name $\rightarrow$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; OpenEDS2020 [[34](#bib.bib34)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Team-name $\rightarrow$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; OpenNEEDS [[194](#bib.bib194)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Random_B &#124; caixin &#124; EyMazing &#124; fgp200709d &#124; vipl_gaze
    &#124; Baseline &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [3.078°](https://repushko.com/all/openeds2020/) &#124; 3.248° &#124;
    3.313° &#124; 3.347° &#124; 3.386° &#124; 5.368° &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; XiaodongWang &#124; Hebut_Lyx &#124; tetelias &#124; TCS_Research &#124;
    Baseline &#124; AnotherShot &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1.68° &#124; 1.75° &#124; 1.99° &#124; 2.05° &#124; 7.18° &#124; 7.94°
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [2020](https://eval.ai/web/challenges/challenge-page/605/leaderboard/1682)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [2021](https://eval.ai/web/challenges/challenge-page/895/leaderboard/2361)
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Dataset | Methods | Year |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaze &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Zone &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [EmotiW2020](http://users.cecs.anu.edu.au/%C2%A0Tom.Gedeon/pdfs/Emotiw%202020%20Driver%20gaze%20group%20emotion%20student%20engagement%20and%20physiological%20signal%20based%20challenges.pdf)
    $\rightarrow$ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DGW [[40](#bib.bib40)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DD_Vision &#124; SituAlgorithm &#124; Overfit &#124; DeepBlueAI &#124;
    UDECE &#124; X-AWARE &#124; Baseline &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 82.52% &#124; 81.51% &#124; 78.87% &#124; 75.88% &#124; 74.57% &#124;
    71.62% &#124; 60.98% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2020 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Datasets | Methods | Year |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Visual &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Attention &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GazeFollowing [[113](#bib.bib113)] (AUC $\uparrow$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GazeCommunication [[111](#bib.bib111)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1\. Atomic-Level (Top-1%) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2\. Event-Level (Top-1%) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; VisualSearch [[118](#bib.bib118)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 1\. Microwave (MultiMatch) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2\. Clock (MultiMatch) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SharedAttention [[114](#bib.bib114)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Human &#124; [[114](#bib.bib114)] &#124; [[62](#bib.bib62)] &#124; [[113](#bib.bib113)]
    &#124; Center &#124; Random &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.924 &#124; 0.921 &#124; 0.896 &#124; 0.878 &#124; 0.633 &#124; 0.504
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ST-GNN &#124; CNN+LSTM &#124; CNN+SVM &#124; CNN+RF &#124; CNN &#124;
    Chance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 55.02% &#124; 24.65% &#124; 36.23% &#124; 37.68% &#124; 23.05% &#124;
    16.44% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 55.90% &#124; – &#124; – &#124; – &#124; – &#124; 22.70% &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Behavioural &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Agreement &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LSTM &#124; GRU &#124; Scanpath &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.714 &#124; 0.621 &#124; 0.677 &#124; 0.684 &#124; 0.664 &#124; Direction
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 0.701 &#124; 0.633 &#124; 0.673 &#124; 0.669 &#124; 0.659 &#124; Direction
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ST-CNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ST-GNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaze+Saliency &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +LSTM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaze+Saliency &#124; GazeFollow &#124; Random &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 83.3% &#124; 71.4% &#124; 66.2% &#124; 59.4% &#124; 58.7% &#124; 22.70%
    &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2015 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2019 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2019 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 2020 &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Where We Stand Now?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [V](#S5.T5 "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣
    Automatic Gaze Analysis: A Survey of Deep Learning based Approaches"), we analyze
    dataset specific improvements made by different methods over the past few years.
    In the following, we discuss some of the important observations from Table [V](#S5.T5
    "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gaze Estimation on Constrained Setup. Most popular 2D-3D gaze estimation datasets [[50](#bib.bib50),
    [94](#bib.bib94), [23](#bib.bib23)] are collected in constrained scenarios where
    there is a certain distance between the user and the visual screen. Moreover,
    as the nodal point of the human eye has subject-specific offset (which varies
    around 2-3°), it is difficult to reduce the angular error beyond a certain limit
    using visible regions of the eyes. On this front, the performance of some of the
    gaze estimation methods [[24](#bib.bib24), [51](#bib.bib51), [94](#bib.bib94)]
    seems to have plateaued on constrained datasets such as CAVE, MPII, and Eth-X-Gaze
    (Refer to Figures in Table [V](#S5.T5 "Table V ‣ 5.3 Cross Dataset Analysis. ‣
    5 Validation ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")).'
  prefs: []
  type: TYPE_NORMAL
- en: Gaze Estimation in Unconstrained Setup. Gaze estimation in unconstrained environments
    still remains largely unresolved mainly due to the unavailability of large-scale
    annotated data. Gaze360 [[90](#bib.bib90)] and GazeCapture [[25](#bib.bib25)]
    are two popular public-domain datasets available for this purpose. Especially
    in the Gaze360 dataset, in many cases, the eyes are not visible which makes it
    more challenging to track where the person is looking. It is quite difficult to
    estimate gaze in a naturalistic environment, more exploration along this line
    is highly desirable.
  prefs: []
  type: TYPE_NORMAL
- en: Gaze Estimation with Limited Supervision. Gaze Estimation with limited supervision
    is a promising research direction. As manual annotation of gaze data is an error-prone
    process, there is a high possibility of noise in labeling. To this end, proposed
    approaches are mainly based on ‘learning-by-synthesis’ [[20](#bib.bib20)], hierarchical
    generative models [[92](#bib.bib92)], conditional random field [[198](#bib.bib198)],
    unsupervised gaze target discovery [[95](#bib.bib95)], few-shot learning [[29](#bib.bib29),
    [91](#bib.bib91)], pseudo-labelling [[174](#bib.bib174)] and self/unsupervised [[28](#bib.bib28),
    [27](#bib.bib27)]. While domain specific knowledge has been utilized for these
    approaches, developing robust methods from limited amount of annotated data with
    enhanced generalization across different real-life scenarios still largely remains
    unresolved.
  prefs: []
  type: TYPE_NORMAL
- en: Visual Attention Estimation. Eye visibility plays an important role in estimating
    the gaze direction of a person. To this end, visual attention estimation mainly
    focuses on where the person is looking irrespective of eye visibility. To facilitate
    research along this direction, GazeFollow [[113](#bib.bib113)] and VideoAttentionTarget [[114](#bib.bib114)]
    datasets have been proposed. Some important research directions to explore include
    scene saliency, visual search, and human scan path [[117](#bib.bib117), [119](#bib.bib119)].
  prefs: []
  type: TYPE_NORMAL
- en: Gaze Trajectory Modelling. Gaze Trajectory modeling and estimation is another
    line of research that requires further research attention [[34](#bib.bib34)].
    Natural gaze dynamics consist of a continuous sequence of gaze events such as
    fixations, saccades, pursuit, vestibulo-ocular reflex, optokinetic reflex, vergence,
    and blinks [rayner1995eye]. These aforementioned dynamics can be influenced by
    saliency, task-relevant information, and environmental factors. Natural eye movements
    of humans span an elliptical region with a horizontally oriented axis greater
    than $\sim 100$° and a vertically oriented axis spanning $\sim 70$°. Due to the
    lack of labeled temporal gaze trajectory data, there are only a few studies [[34](#bib.bib34),
    [146](#bib.bib146)] that focus on tasks related to the gaze trajectory.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Gaze in Augmented Reality, Virtual Reality and 360° Video Streaming
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We are witnessing great progress in the adaptation of VR, AR and 360° Video
    Streaming technology. Eye-tracking has the potential to bring revolution in the
    AR/VR and 360° video streaming for immersive video application space since it
    can enhance the device’s awareness by learning about users’ attention at any given
    point in time. Consequently, user’s focus based optimization reduces power consumption
    by these devices [[199](#bib.bib199), [200](#bib.bib200), [34](#bib.bib34), [201](#bib.bib201)].
    In this section, we will cover the importance of eye-tracking technology and how
    it enables better user experience in AR/VR and 360° Video Streaming devices including
    eye inter pupillary distance for estimating image perception quality, person identification
    or state estimation by their eye gaze pattern, improve interactions, etc.
  prefs: []
  type: TYPE_NORMAL
- en: 'Foveated Rendering (a.k.a gaze-contingent eye tracking) is a process designed
    to show the user only a portion of what they are looking at in full detail [[200](#bib.bib200),
    [32](#bib.bib32), [34](#bib.bib34)]. The focus region follows the user’s visual
    field. Graphics displayed with foveated rendering better matches the way we see
    objects. Usually, the user watches the AR/VR environment or 360° video using head
    mounted display devices. The existing platforms stream the full 360° scene while
    the user can view only a small part of the scene which spans about 90° - 120° horizontally,
    90° vertically. Quantitatively, it is less than 20% of the whole scene. Thus,
    a significant amount of power and network bandwidth is wasted for the display
    which is never utilized in viewing. In ideal condition, the display will be only
    in the user’s visual field while blurring the periphery. Following are the three
    important benefits of the user’s visual field based rendering process: 1\. Improved
    Image Quality: It can enable 4k displays on the current generation graphics processing
    units (GPUs) without degradation in performance. 2\. Lower cost: Similarly, the
    end-users can run AR/VR and 360° Video Streaming based applications on low-cost
    hardware without compromising the performance. 3\. Increased Frame Rate per Second
    (FPS): The end-user can run at a higher frame rate using the same graphical settings.
    There are two types of foveated rendering: dynamic foveated rendering and static
    foveated rendering. Dynamic foveated rendering follows the user’s gaze trajectory
    using eye-tracking and renders a sharp image in the required region, but this
    eye tracking is challenging in many scenarios. On the other hand, static foveated
    rendering considers a fixed area of the highest resolution at the center of the
    viewer’s device irrespective of the user’s gaze. It depends on the user’s head
    movements, thus, facing a challenge in eye-head interplay as the image quality
    is drastically reduced if the user looks away from the center of the field of
    view. The main key aspects of accurate eye position/visual attention estimation
    ahead of time is to enhance user experience via providing high image quality in
    the subject’s visual focus area. It requires person-specific calibration as nodal
    point of human have subject specific offset. Thus, generalizing it across user
    poses a challenge in the gaze analysis community to address [[202](#bib.bib202)].
    On the other hand, user’s viewpoint prediction ahead of time could face a lot
    of challenges as human eye movement is ballistic in nature. The visual attention
    of the user can therefore change abruptly based on the content in the screen.
    Thus, the prediction algorithm needs to take care of imperfect prediction as well
    and it needs to integrate with bit rate control. This process will enable user-specific
    recommendation and other facilities to enhance user experience [[203](#bib.bib203)].'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Driver Engagement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the progress in autonomous and smart cars, the requirement for automatic
    driver monitoring has been observed and researchers have been working on this
    problem for a few years now [[81](#bib.bib81), [79](#bib.bib79), [40](#bib.bib40),
    [204](#bib.bib204)]. In the literature, the problem is treated as a gaze zone
    estimation problem. A summary of the gaze estimation benchmarks is shown in Table
    [VI](#S6.T6 "Table VI ‣ 6.2 Driver Engagement ‣ 6 Applications ‣ Automatic Gaze
    Analysis: A Survey of Deep Learning based Approaches"). The proposed methods can
    be classified into two categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Sensor Based Tracking. These mainly utilize dedicated sensors integrated hardware
    devices for monitoring the driver’s gaze in real-time. These devices require accurate
    pre-calibration and additionally these devices are expensive. Few examples of
    these sensors are Infrared (IR) camera [[205](#bib.bib205)], head-mounted devices [[77](#bib.bib77),
    [206](#bib.bib206)] and other systems [[207](#bib.bib207), [208](#bib.bib208)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Table VI: Comparison of driver gaze estimation datasets with respect to number
    of subjects (# Sub), number of zones (# Zones), illumination conditions and labelling
    procedure.'
  prefs: []
  type: TYPE_NORMAL
- en: '| References | # Sub | # Zones | Illumination | Labelling |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Choi et al. [[84](#bib.bib84)] | 4 | 8 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Bright & &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Dim &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 3-D Gyro. |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Lee et al.  [[85](#bib.bib85)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 12 | 18 | Day | Manual |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Fridman et al.  [[82](#bib.bib82)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 50 | 6 | Day | Manual |'
  prefs: []
  type: TYPE_TB
- en: '| Tawari et al.  [[79](#bib.bib79)] | 6 | 8 | Day | Manual |'
  prefs: []
  type: TYPE_TB
- en: '| Vora et al.  [[41](#bib.bib41)] | 10 | 7 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Diff. &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; day times &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Manual |'
  prefs: []
  type: TYPE_TB
- en: '| Jha et al. [[77](#bib.bib77)] | 16 | 18 | Day |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Head- &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; band &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Wang et al. [[209](#bib.bib209)] | 3 | 9 | Day |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Motion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sensor &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DGW[[40](#bib.bib40)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 338 | 9 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Diff. day times &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| Automatic |'
  prefs: []
  type: TYPE_TB
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; MGM[[204](#bib.bib204)] &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| 60 | 21 |'
  prefs: []
  type: TYPE_TB
- en: '&#124; Diff. day times &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Multiple &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Sensors &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: 'Image processing and vision based methods. These are mainly focused on two
    types of methods: head-pose based only [[85](#bib.bib85), [80](#bib.bib80), [210](#bib.bib210),
    [209](#bib.bib209)] and both head-pose and eye-gaze based [[81](#bib.bib81), [79](#bib.bib79),
    [80](#bib.bib80), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)]. Driver’s
    head pose provides partial information regarding his/her gaze direction as there
    may be an interplay between eyeball movement and head pose [[83](#bib.bib83)].
    Hence, methods relying on head pose information may fail to disambiguate between
    the eye movement with fixed head-pose. Thus, the methods relying on both head
    pose and gaze based prediction are more robust.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Gaze in Healthcare and Wellbeing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gaze is widely used in healthcare domain to enhance the diagnosis performance.
    Generally, eye movement patterns is widely used as behavioral bio-markers of various
    mental health problems including depression [[211](#bib.bib211)], post traumatic
    stress disorder [[212](#bib.bib212)] and Parkinson’s disease [[42](#bib.bib42)].
    Similarly, individuals diagnosed with Autism Spectral Disorder display gaze avoidance
    in social scenes [[42](#bib.bib42)]. Even intoxication including alcohol consumption
    and/or other drugs usage reflects on eye and gaze properties, especially, decreased
    accuracy and speed of saccades, changes in pupil size, and an impaired ability
    to fixate on moving objects. A recent survey [[42](#bib.bib42)] discusses the
    potential applications in healthcare including concussion [[43](#bib.bib43)],
    multiple sclerosis [[213](#bib.bib213)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Physiological Signals. A gaze estimation system could be one of the communication
    methods for severely disabled people who cannot perform any type of gestures and
    speech. Sakurai et al. [[214](#bib.bib214)] developed an eye-tracking method using
    a compact and light electrooculogram (EOG) signal. Further, this prototype is
    improved via the usage of the EOG component which strongly correlated with the
    change of eye movements [[215](#bib.bib215)] (Refer Fig. [8](#S6.F8 "Figure 8
    ‣ 6.3 Gaze in Healthcare and Wellbeing ‣ 6 Applications ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")). The setup can detect object scanning
    only by eye and face muscle movements. The experimental results open the possibility
    of eye-tracking via EOG signals and a Kinect sensor. Research along this direction
    can be extremely useful for disabled people.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/761dd8e2f0ed785661cefe5971cbc9a1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Electro-oculogram (EOG) based gaze estimation method [[215](#bib.bib215)].
    This prototype opens the possibility of communication for severely disabled people.
    Refer Sec. [6.3](#S6.SS3 "6.3 Gaze in Healthcare and Wellbeing ‣ 6 Applications
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches") for more
    details.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Privacy in gaze estimation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Due to the rapid progress over the past few years, gaze estimation technologies
    have become more reliable, cheap, compact and observe increasing use in many fields,
    such as gaming, marketing, driver safety, and healthcare. Consequently, these
    expanding uses of technology raise serious privacy concerns. Gaze patterns can
    reveal much more information than a user wishes and expects to give away. By portraying
    the sensitivity of gaze tracking data, this section provides a brief overview
    of privacy concerns and consequent implications of gaze estimation and eye-tracking.
    Fig. [9](#S7.F9 "Figure 9 ‣ 7 Privacy in gaze estimation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches") shows the overview of the privacy
    concerns, including common data capturing scenarios with their possible implications.
    A recent analysis [[216](#bib.bib216)] of the literature shows that eye-tracking
    data may implicitly contain information about a user’s biometric identity [[217](#bib.bib217)],
    personal attributes (such as gender, age, ethnicity, personality traits, intoxication,
    emotional state, skills etc.) [[218](#bib.bib218), [219](#bib.bib219), [220](#bib.bib220)],
    physical and mental health [[42](#bib.bib42), [211](#bib.bib211)]. Few eye-tracking
    measures may even reveal underlying cognitive processes [[17](#bib.bib17)]. The
    widespread consideration of eye-tracking enhance the potential to improve our
    lives in many directions, but the technology can also pose a substantial threat
    to privacy. Thus, it is necessary to understand the sensitiveness of gaze data
    from a holistic perspective to prevent its misuse.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f39d86d95b4bae66736bab3d593b53a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: The possible privacy concerns related to gaze analysis framework [[216](#bib.bib216)].
    Please refer Sec. [7](#S7 "7 Privacy in gaze estimation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches") for more details.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion and Future Direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Gaze analysis is a technology in search of an application in several domains
    mainly in assistive technology and HCI. The wide applications of gaze related
    technology is growing rapidly. Thus, it opens a lot of research opportunity ahead
    of the community. Here, in this paper, we present an overall review of gaze analysis
    frameworks with different perspectives from different point of view. Beginning
    with the preliminaries of gaze modelling and eye movement, we further elaborate
    on challenges in this field, overview of gaze analysis framework and its possible
    applications in different domains. For eye analysis, mainly geometric and appearance
    properties are widely explored in prior works. Despite recent progress, the gaze
    analysis remains challenging due to eye head interplay, occlusion and other challenges
    mentioned in Sec. [2.4](#S2.SS4 "2.4 Challenges ‣ 2 Preliminaries ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches"). Thus, there is a
    scope for future development in this respect. Moreover, all of the proposed datasets
    in this domain are collected in constraint environments. In order to overcome
    these limitations, the generative adversarial network based data generation approach
    has come into play. Due to several image quality-related issues, these datasets
    are not used for benchmarking. Automatic labelling of images based on accurate
    heuristic could be explored to reduce the data annotation burden greatly. Future
    directions for the eye and gaze trackers include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gaze Analysis in Unconstrained Setup: The most precise methods for gaze estimation
    is via intrusive sensors, IR camera and RGBD camera. The main drawback of these
    systems is that their performance degrades when used in real-world settings. In
    future, gaze estimation models should consider these situations. Although several
    current efforts in this direction employ techniques, yet further research is needed.
    Moreover, most of the current gaze estimation benchmark datasets require the proper
    geometric arrangement as well as user cooperation (e.g., CAVE, TabletGaze, MPII,
    Eyediap, ETH-XGaze etc). It would be an interesting direction to explore gaze
    estimation in a more flexible setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning with Less Supervision: With the surge in unsupervised, self-supervised,
    weakly supervised techniques in this domain, more exploration in this direction
    is required to eliminate the dependency on ground truth gaze label which could
    be error-prone due to data acquisition limitations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Gaze Inference: Apart from localizing the eye and determining gaze, the gaze
    patterns provides vital cues for encoding the cognitive and affective states of
    the concerned person. More exploration and cross-domain research could be another
    direction to encode visual perception.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AR/VR: Eye tracking has potential application in AR/VR including Foveated Rending
    (FR) and Attention Tunneling. The gaze based interaction require low latency gaze
    estimation. In these applications, the visual environment presents a high-quality
    image at the point where the user is looking while blurring the other peripheral
    region. The intuition is to reduce power consumption without compromising the
    perceptual quality as well as user experience. However, eye movements are fast
    and involuntary action which restrict the use of this techniques (in FR) due to
    the subsequent delays in the eye-tracking pipelines. In order to address this
    issue, a new research direction i.e. future gaze trajectory prediction has been
    recently introduced [[34](#bib.bib34)]. More exploration along this direction
    is highly desirable.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Eye Model and Learning Based Hybrid Approaches: Traditional geometrical eye
    model based and appearance guided learning based approaches have complimentary
    advantages. The geometrical eye model based methods does not require training
    data. Moreover, it has strong generalization capability but it is highly relied
    on relevant eye landmark localization performance. Accurate localization of eye
    landmarks is quite challenging in real world settings as the subject could have
    extreme headpose, occlusion, illumination and other environmental factors. On
    the other hand, the learning based approaches can encode eye appearance feature
    but it does not generalize well across different setups. Thus, a hybrid model
    which can take the advantage of both scenarios could be a possible research direction
    for gaze estimation and eye tracking domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-modal/Cross-modal Gaze Estimation: Over the past decade, head gesture
    synthesis has become an interesting line of research. Prior works in this area
    have mainly used handcrafted audio features such as energy based features [[221](#bib.bib221)],
    MFCC (Mel Frequency Cepstral Coefficent) [[222](#bib.bib222)], LPC (Linear Predictive
    Coding) [[222](#bib.bib222)] and filter bank [[222](#bib.bib222), [223](#bib.bib223)]
    to generate realistic head gesture. The main challenge in this domain is audio
    data annotation for head motion synthesis which is a noisy and error prone process.
    Prior works approach this problem via multi-stream HMMs [[221](#bib.bib221)],
    MLP based regression modelling [[222](#bib.bib222)], bi-LSTM [[223](#bib.bib223)]
    and Conditional Variational Autoencoder (CVAE) [[224](#bib.bib224)]. In vision
    domain, mainly visual stimuli is utilized for gaze estimation. As the audio signal
    is non-trivial for gaze estimation, yet, it has the potential to coarsely define
    the gaze direction [[225](#bib.bib225)]. Research along this direction have potential
    to estimate gaze in challenging situation where visual stimuli fails.'
  prefs: []
  type: TYPE_NORMAL
- en: The techniques surveyed in this paper focus on gaze analysis from different
    perspective, however, these techniques can be useful for other computer vision
    and HCI tasks. Gaze analysis and its widespread applications is a unique and well-defined
    topic, which have already influenced recent technologies. Scholarly interest in
    gaze estimation is established in a large number of disciplines. It primarily
    originates from vision-related assistive technology which further propagates through
    other domains and attracts a lot of future research attention across various fields.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] H. Liu and I. Heynderickx, “Visual attention in objective image quality
    assessment: Based on eye-tracking data,” *IEEE Transactions on Circuits and Systems
    for Video Technology*, vol. 21, no. 7, pp. 971–982, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] A. Frischen, A. P. Bayliss, and S. P. Tipper, “Gaze cueing of attention:
    visual attention, social cognition, and individual differences.” *Psychological
    Bulletin*, vol. 133, p. 694, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] D. Purves, Y. Morgenstern, and W. T. Wojtach, “Perception and reality:
    why a wholly empirical paradigm is needed to understand vision,” *Frontiers in
    systems neuroscience*, vol. 9, p. 156, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] E. Javal, “Essai sur la physiologie de la lecture,” *Annales d’Ocilistique*,
    vol. 80, pp. 97–117, 1878.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] T. N. Cornsweet and H. D. Crane, “Accurate two-dimensional eye tracker
    using first and fourth purkinje images,” *JOSA*, vol. 63, no. 8, pp. 921–928,
    1973.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] J. Merchant, R. Morrissette, and J. L. Porterfield, “Remote measurement
    of eye direction allowing subject motion over one cubic foot of space,” *IEEE
    transactions on biomedical engineering*, no. 4, pp. 309–317, 1974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Borgestig, J. Sandqvist, R. Parsons, T. Falkmer, and H. Hemmingsson,
    “Eye gaze performance for children with severe physical impairments using gaze-based
    assistive technology—a longitudinal study,” *Assistive technology*, vol. 28, no. 2,
    pp. 93–102, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] F. Corno, L. Farinetti, and I. Signorile, “A cost-effective solution for
    eye-gaze assistive technology,” in *IEEE International Conference on Multimedia
    and Expo*, vol. 2, 2002, pp. 433–436.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. W. Joseph and R. Murugesh, “Potential eye tracking metrics and indicators
    to measure cognitive load in human-computer interaction research,” *Journal of
    Scientific Research*, vol. 64, no. 1, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. Pi, P. A. Koljonen, Y. Hu, and B. E. Shi, “Dynamic bayesian adjustment
    of dwell time for faster eye typing,” *IEEE Transactions on Neural Systems and
    Rehabilitation Engineering*, vol. 28, no. 10, pp. 2315–2324, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Z. Chen, D. Deng, J. Pi, and B. E. Shi, “Unsupervised outlier detection
    in appearance-based gaze estimation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision Workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. Wedel and R. Pieters, “A review of eye-tracking research in marketing,”
    in *Review of marketing research*.   Routledge, 2017, pp. 123–147.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A. Patney, J. Kim, M. Salvi, A. Kaplanyan, C. Wyman, N. Benty, A. Lefohn,
    and D. Luebke, “Perceptually-based foveated virtual reality,” in *SIGGRAPH Emerging
    Technologies*.   ACM, 2016, pp. 1–2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] R. T. Azuma, “A survey of augmented reality,” *Presence: Teleoperators
    & Virtual Environments*, vol. 6, no. 4, pp. 355–385, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] F. Ragusa, A. Furnari, S. Battiato, G. Signorello, and G. M. Farinella,
    “Ego-ch: Dataset and fundamental tasks for visitors behavioral understanding using
    egocentric vision,” *Pattern Recognition Letters*, vol. 131, pp. 150–157, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] A. K. Jain, R. Bolle, and S. Pankanti, *Biometrics: personal identification
    in networked society*.   Springer Science & Business Media, 2006, vol. 479.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] M. K. Eckstein, B. Guerra-Carrillo, A. T. M. Singley, and S. A. Bunge,
    “Beyond eye gaze: What else can eyetracking reveal about cognition and cognitive
    development?” *Developmental cognitive neuroscience*, vol. 25, pp. 69–91, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] M. A. Miller and M. T. Fillmore, “Persistence of attentional bias toward
    alcohol-related stimuli in intoxicated social drinkers,” *Drug and Alcohol Dependence*,
    vol. 117, no. 2-3, pp. 184–189, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Z. Zhu and Q. Ji, “Novel eye gaze tracking techniques under natural head
    movement,” *IEEE Transactions on biomedical engineering*, vol. 54, no. 12, pp.
    2246–2260, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Y. Sugano, Y. Matsushita, and Y. Sato, “Learning-by-synthesis for appearance-based
    3d gaze estimation,” in *IEEE Computer Vision and Pattern Recognition*, 2014,
    pp. 1821–1828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] D. W. Hansen and Q. Ji, “In the eye of the beholder: A survey of models
    for eyes and gaze,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 32, no. 3, pp. 478–500, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] D. Hansen and A. Pece, “Eye tracking in the wild,” *Computer Vision and
    Image Understanding*, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] B. Smith, Q. Yin, S. Feiner, and S. Nayar, “Gaze locking: passive eye
    contact detection for human-object interaction,” in *ACM User Interface Software
    & Technology*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze
    estimation in the wild,” in *IEEE Computer Vision and Pattern Recognition*, 2015,
    pp. 4511–4520.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] K. Krafka, A. Khosla, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik,
    and A. Torralba, “Eye tracking for everyone,” in *IEEE Computer Vision and Pattern
    Recognition*, 2016, pp. 2176–2184.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Park, A. Spurr, and O. Hilliges, “Deep pictorial gaze estimation,”
    in *European Conference on Computer Vision*, 2018, pp. 721–738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] N. Dubey, S. Ghosh, and A. Dhall, “Unsupervised learning of eye gaze representation
    from the web,” in *2019 International Joint Conference on Neural Networks (IJCNN)*.   IEEE,
    2019, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Yu and J. Odobez, “Unsupervised representation learning for gaze estimation,”
    *IEEE Conference on Computer Vision and Pattern Recognition*, pp. 1–13, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Park, S. D. Mello, P. Molchanov, U. Iqbal, O. Hilliges, and J. Kautz,
    “Few-shot adaptive gaze estimation,” in *IEEE International Conference on Computer
    Vision*, 2019, pp. 9368–9377.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] E. B. Huey, “The psychology and pedagogy of reading: With a review of
    the history of reading and writing and of methods, texts, and hygiene in reading,”
    1908.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Sugano, Y. Matsushita, and Y. Sato, “Appearance-based gaze estimation
    using visual saliency,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 35, no. 2, pp. 329–341, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] S. J. Garbin, Y. Shen, I. Schuetz, R. Cavin, G. Hughes, and S. S. Talathi,
    “Openeds: Open eye dataset,” *arXiv preprint arXiv:1905.03702*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] P. Kellnhofer, A. Recasens, S. Stent, W. Matusik, and A. Torralba, “Gaze360:
    Physically unconstrained gaze estimation in the wild,” in *IEEE International
    Conference on Computer Vision*, 2019, pp. 6912–6921.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] C. Palmero, A. Sharma, K. Behrendt, K. Krishnakumar, O. V. Komogortsev,
    and S. S. Talathi, “Openeds2020: Open eyes dataset,” *arXiv preprint arXiv:2005.03876*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] H. Chennamma and X. Yuan, “A survey on eye-gaze tracking techniques,”
    *arXiv preprint arXiv:1312.6410*, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] H. Jing-Yao, X. Yong-Yue, L. Lin-Na, X.-C. ZHANG, Q. Li, and C. Jian-Nan,
    “Survey on key technologies of eye gaze tracking,” *DEStech Transactions on Computer
    Science and Engineering*, no. aice-ncs, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] A. Kar and P. Corcoran, “A review and analysis of eye-gaze estimation
    systems, algorithms and performance evaluation methods in consumer platforms,”
    *IEEE Access*, vol. 5, pp. 16 495–16 519, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] D. Cazzato, M. Leo, C. Distante, and H. Voos, “When i look into your eyes:
    A survey on computer vision contributions for human gaze estimation and tracking,”
    *Sensors*, vol. 20, no. 13, p. 3739, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] V. Clay, P. König, and S. U. König, “Eye tracking in virtual reality,”
    *Journal of Eye Movement Research*, vol. 12, no. 1, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Ghosh, A. Dhall, G. Sharma, S. Gupta, and N. Sebe, “Speak2label: Using
    domain knowledge for creating a large scale driver gaze zone estimation dataset,”
    *arXiv preprint arXiv:2004.05973*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Vora, A. Rangesh, and M. M. Trivedi, “Driver gaze zone estimation using
    convolutional neural networks: A general framework and ablative analysis,” *IEEE
    Transactions on Intelligent Vehicles*, pp. 254–265, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Harezlak and P. Kasprowski, “Application of eye tracking in medicine:
    A survey, research issues and challenges,” *Computerized Medical Imaging and Graphics*,
    vol. 65, pp. 176–190, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Y. Kempinski, “System and method of diagnosis using gaze and eye tracking,”
    Apr. 21 2016, uS Patent App. 14/723,590.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. Park, “Representation learning for webcam-based gaze estimation,” Ph.D.
    dissertation, ETH Zurich, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] R. H. Carpenter, *Movements of the Eyes, 2nd Rev*.   Pion Limited, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] E. D. Guestrin and M. Eizenman, “General theory of remote gaze estimation
    using the pupil center and corneal reflections,” *IEEE Transactions on biomedical
    engineering*, vol. 53, no. 6, pp. 1124–1133, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. T. Duchowski and A. T. Duchowski, *Eye tracking methodology: Theory
    and practice*.   Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] T. Santini, W. Fuhl, and E. Kasneci, “Calibme: Fast and unsupervised eye
    tracker calibration for gaze-based pervasive human-computer interaction,” in *ACM
    Conference on Human Factors in Computing Systems*, 2017, pp. 2594–2605.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] X. Zhang, Y. Sugano, and A. Bulling, “Evaluation of appearance-based methods
    and implications for gaze-based applications,” in *Proceedings of the 2019 CHI
    Conference on Human Factors in Computing Systems*, 2019, pp. 1–13.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Mpiigaze: Real-world dataset
    and deep appearance-based gaze estimation,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] ——, “It’s written all over your face: Full-face appearance-based gaze
    estimation,” in *IEEE Computer Vision and Pattern Recognition Workshop*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] W. Zhu and H. Deng, “Monocular free-head 3d gaze tracking with deep learning
    and geometry constraints,” in *Proceedings of the IEEE International Conference
    on Computer Vision*, 2017, pp. 3143–3152.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] W. Cui, J. Cui, and H. Zha, “Specialized gaze estimation for children
    by convolutional neural network and domain adaptation,” in *2017 IEEE International
    Conference on Image Processing (ICIP)*.   IEEE, 2017, pp. 3305–3309.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] T. Fischer, H. J. Chang, and Y. Demiris, “RT-GENE: Real-Time Eye Gaze
    Estimation in Natural Environments,” in *European Conference on Computer Vision*,
    2018, pp. 339–357.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Y. Cheng, F. Lu, and X. Zhang, “Appearance-based gaze estimation via evaluation-guided
    asymmetric regression,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 100–115.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] C. Palmero, J. Selva, M. A. Bagheri, and S. Escalera, “Recurrent cnn for
    3d gaze estimation using appearance and shape cues,” *arXiv preprint arXiv:1805.03064*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] S. Jyoti and A. Dhall, “Automatic eye gaze estimation using geometric
    & texture-based networks,” in *International Conference on Pattern Recognition*.   IEEE,
    2018, pp. 2474–2479.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] G. Liu, Y. Yu, K. A. F. Mora, and J.-M. Odobez, “A differential approach
    for gaze estimation,” *IEEE transactions on pattern analysis and machine intelligence*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Y. Cheng, S. Huang, F. Wang, C. Qian, and F. Lu, “A coarse-to-fine adaptive
    network for appearance-based gaze estimation,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 34, no. 07, 2020, pp. 10 623–10 630.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] X. Zhang, Y. Sugano, A. Bulling, and O. Hilliges, “Learning-based region
    selection for end-to-end gaze estimation,” in *British Machine Vision Conference
    (BMVC 2020)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Y. Yu, G. Liu, and J.-M. Odobez, “Deep multitask gaze estimation with
    a constrained landmark-gaze model,” in *Proceedings of the European Conference
    on Computer Vision (ECCV) Workshops*, 2018, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] E. Chong, N. Ruiz, Y. Wang, Y. Zhang, A. Rozga, and J. M. Rehg, “Connecting
    gaze, scene, and attention: Generalized attention estimation via joint modeling
    of gaze and scene saliency,” in *Proceedings of the European conference on computer
    vision (ECCV)*, 2018, pp. 383–398.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] X. Zhang, M. X. Huang, Y. Sugano, and A. Bulling, “Training person-specific
    gaze estimators from user interactions with multiple devices,” in *Proceedings
    of the 2018 CHI Conference on Human Factors in Computing Systems*, 2018, pp. 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Z. Chen and B. E. Shi, “Appearance-based gaze estimation using dilated-convolutions,”
    in *Asian Conference on Computer Vision*.   Springer, 2018, pp. 309–324.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] X. Zhou, J. Lin, J. Jiang, and S. Chen, “Learning a 3d gaze estimator
    with improved itracker combined with bidirectional lstm,” in *2019 IEEE international
    conference on Multimedia and expo (ICME)*.   IEEE, 2019, pp. 850–855.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] K. Wang, H. Su, and Q. Ji, “Neuro-inspired eye tracking with eye movement
    dynamics,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 9831–9840.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Z. Wu, S. Rajendran, T. Van As, V. Badrinarayanan, and A. Rabinovich,
    “Eyenet: A multi-task deep network for off-axis eye gaze estimation,” in *2019
    IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)*.   IEEE,
    2019, pp. 3683–3687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Kim, M. Stengel, A. Majercik, S. De Mello, D. Dunn, S. Laine, M. McGuire,
    and D. Luebke, “Nvgaze: An anatomically-informed dataset for low-latency, near-eye
    gaze estimation,” in *Proceedings of the 2019 CHI Conference on Human Factors
    in Computing Systems*, 2019, pp. 1–12.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] D. Xia and Z. Ruan, “IR image based eye gaze estimation,” in *IEEE ACIS
    International Conference on Software Engineering, Artificial Intelligence, Networking,
    and Parallel/Distributed Computing*, vol. 1, 2007, pp. 220–224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] M. Tonsen, J. Steil, Y. Sugano, and A. Bulling, “Invisibleeye: Mobile
    eye tracking using multiple low-resolution cameras and learning-based gaze estimation,”
    *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    vol. 1, no. 3, pp. 1–21, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] D. Lian, Z. Zhang, W. Luo, L. Hu, M. Wu, Z. Li, J. Yu, and S. Gao, “Rgbd
    based gaze estimation via multi-task cnn,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 33, no. 01, 2019, pp. 2488–2495.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] A. Tsukada, M. Shino, M. Devyver, and T. Kanade, “Illumination-free gaze
    estimation method for first-person vision wearable device,” in *IEEE International
    Conference on Computer Vision Workshop*, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] M. Leo, D. Cazzato, T. De Marco, and C. Distante, “Unsupervised eye pupil
    localization through differential geometry and local self-similarity,” *Public
    Library of Science*, vol. 9, no. 8, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] C. Gou, Y. Wu, K. Wang, K. Wang, F. Wang, and Q. Ji, “A joint cascaded
    framework for simultaneous eye detection and eye state estimation,” *Pattern Recognition*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] J. Pi and B. E. Shi, “Task-embedded online eye-tracker calibration for
    improving robustness to head motion,” in *Proceedings of the 11th ACM Symposium
    on Eye Tracking Research & Applications*, 2019, pp. 1–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] D. Lian, L. Hu, W. Luo, Y. Xu, L. Duan, J. Yu, and S. Gao, “Multiview
    multitask gaze estimation with deep convolutional neural networks,” *IEEE transactions
    on neural networks and learning systems*, vol. 30, no. 10, pp. 3010–3023, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. Jha and C. Busso, “Probabilistic estimation of the gaze region of the
    driver using dense classification,” in *IEEE International Conference on Intelligent
    Transportation Systems*, 2018, pp. 697–702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Z. Hu, S. Li, C. Zhang, K. Yi, G. Wang, and D. Manocha, “Dgaze: Cnn-based
    gaze prediction in dynamic scenes,” *IEEE transactions on visualization and computer
    graphics*, vol. 26, no. 5, pp. 1902–1911, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. Tawari, K. H. Chen, and M. M. Trivedi, “Where is the driver looking:
    Analysis of head, eye and iris for robust gaze zone estimation,” in *IEEE Conference
    on Intelligent Transportation Systems*, 2014, pp. 988–994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Tawari and M. M. Trivedi, “Robust and continuous estimation of driver
    gaze zone by dynamic analysis of multiple face videos,” in *IEEE Intelligent Vehicles
    Symposium*, 2014, pp. 254–265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] B. Vasli, S. Martin, and M. M. Trivedi, “On driver gaze estimation: Explorations
    and fusion of geometric and data driven approaches,” in *IEEE Intelligent Transportation
    Systems*, 2016, pp. 655–660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] L. Fridman, P. Langhans, J. Lee, and B. Reimer, “Driver gaze estimation
    without using eye movement,” *IEEE Intelligent Systems*, pp. 49–56, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] L. Fridman, J. Lee, B. Reimer, and T. Victor, “‘owl’and ‘lizard’: patterns
    of head pose and eye pose in driver gaze classification,” *IET Computer Vision*,
    vol. 10, no. 4, pp. 308–314, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] I. H. Choi, S. K. Hong, and Y. G. Kim, “Real-time categorization of driver’s
    gaze zone using the deep learning techniques,” in *International Conference on
    Big Data and Smart Computing*.   IEEE, 2016, pp. 143–148.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. Lee, J. Jo, H. Jung, K. Park, and J. Kim, “Real-time gaze estimator
    based on driver’s head orientation for forward collision warning system,” *IEEE
    Transactions on Intelligent Transportation Systems*, vol. 12, no. 1, pp. 254–267,
    2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] J. He, K. Pham, N. Valliappan, P. Xu, C. Roberts, D. Lagun, and V. Navalpakkam,
    “On-device few-shot personalization for real-time gaze estimation,” in *IEEE International
    Conference on Computer Vision Workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Q. Huang, A. Veeraraghavan, and A. Sabharwal, “Tabletgaze: dataset and
    analysis for unconstrained appearance-based gaze estimation in mobile tablets,”
    *Machine Vision and Applications*, vol. 28, no. 5-6, pp. 445–461, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] T. Guo, Y. Liu, H. Zhang, X. Liu, Y. Kwak, B. In Yoo, J.-J. Han, and C. Choi,
    “A generalized and robust method towards practical gaze estimation on smart phone,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*,
    2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Y. Bao, Y. Cheng, Y. Liu, and F. Lu, “Adaptive feature fusion network
    for gaze tracking in mobile tablets,” *arXiv preprint arXiv:2103.11119*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] P. Kellnhofer, A. Recasens, S. Stent, W. Matusik, and A. Torralba, “Gaze360:
    Physically unconstrained gaze estimation in the wild,” in *IEEE International
    Conference on Computer Vision*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Yu, G. Liu, and J. Odobez, “Improving few-shot user-specific gaze adaptation
    via gaze redirection synthesis,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 11 937–11 946.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] K. Wang, R. Zhao, and Q. Ji, “A hierarchical generative model for eye
    image synthesis and eye gaze estimation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018, pp. 440–448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] S. Vora, A. Rangesh, and M. M. Trivedi, “On generalizing driver gaze zone
    estimation using convolutional neural networks,” in *IEEE Intelligent Vehicles
    Symposium (IV)*.   IEEE, 2017, pp. 849–854.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] X. Zhang, S. Park, T. Beeler, D. Bradley, S. Tang, and O. Hilliges, “Eth-xgaze:
    A large scale dataset for gaze estimation under extreme head pose and gaze variation,”
    in *European Conference on Computer Vision*.   Springer, 2020, pp. 365–381.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] X. Zhang, Y. Sugano, and A. Bulling, “Everyday eye contact detection using
    unsupervised gaze target discovery,” in *ACM User Interface Software and Technology*,
    2017, pp. 193–203.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] C. H. Morimoto and M. R. Mimica, “Eye gaze tracking techniques for interactive
    applications,” *Computer vision and image understanding*, vol. 98, no. 1, pp.
    4–24, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] S. Park, X. Zhang, A. Bulling, and O. Hilliges, “Learning to find eye
    region landmarks for remote gaze estimation in unconstrained settings,” in *Proceedings
    of the 2018 ACM Symposium on Eye Tracking Research & Applications*, 2018, pp.
    1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] P. M. Corcoran, F. Nanu, S. Petrescu, and P. Bigioi, “Real-time eye gaze
    tracking for gaming design and consumer electronics systems,” *IEEE Transactions
    on Consumer Electronics*, vol. 58, no. 2, pp. 347–355, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] S. Chu, N. Paul, and L. Ruel, “Using eye tracking technology to examine
    the effectiveness of design elements on news websites.” *Information Design Journal
    (IDJ)*, vol. 17, no. 1, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] J. Chen, J. Zhang, J. Fan, T. Chen, E. Sangineto, and N. Sebe, “Mggr:
    Multimodal-guided gaze redirection with coarse-to-fine learning,” *arXiv preprint
    arXiv:2004.03064*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Zheng, S. Park, X. Zhang, S. De Mello, and O. Hilliges, “Self-learning
    transformations for improving gaze and head redirection,” *arXiv preprint arXiv:2010.12307*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J. Chen, J. Zhang, E. Sangineto, T. Chen, J. Fan, and N. Sebe, “Coarse-to-fine
    gaze redirection with numerical and pictorial guidance,” in *Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision*, 2021, pp. 3665–3674.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] K. Ruhland, C. E. Peters, S. Andrist, J. B. Badler, N. I. Badler, M. Gleicher,
    B. Mutlu, and R. McDonnell, “A review of eye gaze in virtual agents, social robotics
    and hci: Behaviour generation, user interaction and perception,” in *Computer
    graphics forum*, vol. 34, no. 6.   Wiley Online Library, 2015, pp. 299–326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] T. Baltrušaitis, P. Robinson, and L.-P. Morency, “Openface: an open source
    facial behavior analysis toolkit,” in *IEEE Winter Conference on Applications
    of Computer Vision*, 2016, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] T. Baltrusaitis, A. Zadeh, Y. C. Lim, and L.-P. Morency, “Openface 2.0:
    Facial behavior analysis toolkit,” in *2018 13th IEEE International Conference
    on Automatic Face & Gesture Recognition (FG 2018)*.   IEEE, 2018, pp. 59–66.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] D. E. King, “Dlib-ml: A machine learning toolkit,” *Journal of Machine
    Learning Research*, pp. 1755–1758, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] P. Müller, M. X. Huang, X. Zhang, and A. Bulling, “Robust eye contact
    detection in natural multi-person interactions using gaze and speaking behaviour,”
    in *Proc. ACM International Symposium on Eye Tracking Research and Applications
    (ETRA)*, 2018, pp. 31:1–31:10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] M. J. Marin-Jimenez, V. Kalogeiton, P. Medina-Suarez, and A. Zisserman,
    “Laeo-net: revisiting people looking at each other in videos,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 3477–3485.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] ——, “LAEO-Net++: revisiting people Looking At Each Other in videos,”
    in *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] R. Kothari, S. De Mello, U. Iqbal, W. Byeon, S. Park, and J. Kautz, “Weakly-supervised
    physically unconstrained gaze estimation,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2021, pp. 9980–9989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] L. Fan, W. Wang, S. Huang, X. Tang, and S.-C. Zhu, “Understanding human
    gaze communication by spatio-temporal graph reasoning,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2019, pp. 5724–5733.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] M. Zhang, Y. Liu, and F. Lu, “Gazeonce: Real-time multi-person gaze estimation,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2022, pp. 4197–4206.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Recasens, A. Khosla, C. Vondrick, and A. Torralba, “Where are they
    looking?” in *Advances in Neural Information Processing Systems*, vol. 28.   Curran
    Associates, Inc., 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] E. Chong, Y. Wang, N. Ruiz, and J. M. Rehg, “Detecting attended visual
    targets in video,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2020, pp. 5396–5406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] D. Tu, X. Min, H. Duan, G. Guo, G. Zhai, and W. Shen, “End-to-end human-gaze-target
    detection with transformers,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2022, pp. 2202–2210.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] B. Wang, T. Hu, B. Li, X. Chen, and Z. Zhang, “Gatector: A unified framework
    for gaze object prediction,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2022, pp. 19 588–19 597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Z. Yang, L. Huang, Y. Chen, Z. Wei, S. Ahn, G. Zelinsky, D. Samaras,
    and M. Hoai, “Predicting goal-directed human attention using inverse reinforcement
    learning,” in *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*, 2020, pp. 193–202.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] G. Zelinsky, Z. Yang, L. Huang, Y. Chen, S. Ahn, Z. Wei, H. Adeli, D. Samaras,
    and M. Hoai, “Benchmarking gaze prediction for categorical visual search,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] X. Chen, M. Jiang, and Q. Zhao, “Predicting human scanpaths in visual
    question answering,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2021, pp. 10 876–10 885.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] M. Kümmerer and M. Bethge, “State-of-the-art in human scanpath prediction,”
    *arXiv preprint arXiv:2102.12239*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. Bao, B. Liu, and J. Yu, “Escnet: Gaze target detection with the understanding
    of 3d scenes,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*, June 2022, pp. 14 126–14 135.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] K. Liang, Y. Chahir, M. Molina, C. Tijus, and F. Jouen, “Appearance-based
    gaze tracking with spectral clustering and semi-supervised gaussian process regression,”
    in *Proceedings of the 2013 Conference on Eye Tracking South Africa*, 2013, pp.
    17–23.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] K. Wang and Q. Ji, “Hybrid model and appearance based eye tracking with
    kinect,” in *Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research
    & Applications*, 2016, pp. 331–332.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and alignment
    using multitask cascaded convolutional networks,” *IEEE Signal Processing Letters*,
    vol. 23, no. 10, pp. 1499–1503, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] J. Li, Y. Wang, C. Wang, Y. Tai, J. Qian, J. Yang, C. Wang, J. Li, and
    F. Huang, “Dsfd: Dual shot face detector,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Wang, Y. Liu, Y. Hu, H. Shi, and T. Mei, “Facex-zoo: A pytorh toolbox
    for face recognition,” *arXiv preprint arXiv:2101.04407*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] N. Markuš, M. Frljak, I. S. Pandžić, J. Ahlberg, and R. Forchheimer,
    “Eye pupil localization with an ensemble of randomized trees,” *Pattern Recognition*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] D. Tian, G. He, J. Wu, H. Chen, and Y. Jiang, “An accurate eye pupil
    localization approach based on adaptive gradient boosting decision tree,” in *2016
    Visual Communications and Image Processing (VCIP)*.   IEEE, 2016, pp. 1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] A. Kacete, J. Royan, R. Seguier, M. Collobert, and C. Soladie, “Real-time
    eye pupil localization using hough regression forest,” in *2016 IEEE Winter Conference
    on Applications of Computer Vision (WACV)*.   IEEE, 2016, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] J. H. Choi, K. I. Lee, Y. C. Kim, and B. C. Song, “Accurate eye pupil
    localization using heterogeneous cnn models,” in *2019 IEEE International Conference
    on Image Processing (ICIP)*.   IEEE, 2019, pp. 2179–2183.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] W. Sankowski, K. Grabowski, M. Napieralska, M. Zubert, and A. Napieralski,
    “Reliable algorithm for iris segmentation in eye image,” *Image and vision computing*,
    vol. 28, no. 2, pp. 231–237, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] P. Radu, J. Ferryman, and P. Wild, “A robust sclera segmentation algorithm,”
    in *2015 IEEE 7th International Conference on Biometrics Theory, Applications
    and Systems (BTAS)*.   IEEE, 2015, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] A. Das, U. Pal, M. A. Ferrer, M. Blumenstein, D. Štepec, P. Rot, Ž. Emeršič,
    P. Peer, V. Štruc, S. A. Kumar *et al.*, “Sserbc 2017: Sclera segmentation and
    eye recognition benchmarking competition,” in *2017 IEEE International Joint Conference
    on Biometrics (IJCB)*.   IEEE, 2017, pp. 742–747.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] D. R. Lucio, R. Laroca, E. Severo, A. S. Britto, and D. Menotti, “Fully
    convolutional networks and generative adversarial networks applied to sclera segmentation,”
    in *2018 IEEE 9th International Conference on Biometrics Theory, Applications
    and Systems (BTAS)*.   IEEE, 2018, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A. Das, U. Pal, M. Blumenstein, and M. A. F. Ballester, “Sclera recognition-a
    survey,” in *2013 2nd IAPR Asian Conference on Pattern Recognition*.   IEEE, 2013,
    pp. 917–921.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. K. Chaudhary, R. Kothari, M. Acharya, S. Dangi, N. Nair, R. Bailey,
    C. Kanan, G. Diaz, and J. B. Pelz, “Ritnet: real-time semantic segmentation of
    the eye for gaze tracking,” in *2019 IEEE/CVF International Conference on Computer
    Vision Workshop (ICCVW)*.   IEEE, 2019, pp. 3698–3702.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] N. N. Pandey and N. B. Muppalaneni, “Real-time drowsiness identification
    based on eye state analysis,” in *2021 International Conference on Artificial
    Intelligence and Smart Systems (ICAIS)*.   IEEE, 2021, pp. 1182–1187.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] M. Monaro, P. Capuozzo, F. Ragucci, A. Maffei, A. Curci, C. Scarpazza,
    A. Angrilli, and G. Sartori, “Using blink rate to detect deception: A study to
    validate an automatic blink detector and a new dataset of videos from liars and
    truth-tellers,” in *International Conference on Human-Computer Interaction*.   Springer,
    2020, pp. 494–509.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] T. Drutarovsky and A. Fogelton, “Eye blink detection using variance of
    motion vectors,” in *European Conference on Computer Vision*.   Springer, 2014,
    pp. 436–448.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] A. Królak and P. Strumiłło, “Eye-blink detection system for human–computer
    interaction,” *Universal Access in the Information Society*, vol. 11, no. 4, pp.
    409–419, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] J. Cech and T. Soukupova, “Real-time eye blink detection using facial
    landmarks,” *Cent. Mach. Perception, Dep. Cybern. Fac. Electr. Eng. Czech Tech.
    Univ. Prague*, pp. 1–8, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] R. Daza, A. Morales, J. Fierrez, and R. Tolosana, “mebal: A multimodal
    database for eye blink detection and attention level estimation,” in *Companion
    Publication of the 2020 International Conference on Multimodal Interaction*, 2020,
    pp. 32–36.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] G. Hu, Y. Xiao, Z. Cao, L. Meng, Z. Fang, J. T. Zhou, and J. Yuan, “Towards
    real-time eyeblink detection in the wild: Dataset, theory and practices,” *IEEE
    Transactions on Information Forensics and Security*, vol. 15, pp. 2194–2208, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] J. Perry and A. Fernandez, “Minenet: A dilated cnn for semantic segmentation
    of eye features,” in *Proceedings of the IEEE International Conference on Computer
    Vision Workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] K. Wang, R. Zhao, H. Su, and Q. Ji, “Generalizing eye tracking with bayesian
    adversarial learning,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 11 907–11 916.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] C. Palmero Cantarino, O. V. Komogortsev, and S. S. Talathi, “Benefits
    of temporal information for appearance-based gaze estimation,” in *ACM Symposium
    on Eye Tracking Research and Applications*, 2020, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] S. Nonaka, S. Nobuhara, and K. Nishino, “Dynamic 3d gaze from afar: Deep
    gaze estimation from temporal eye-head-body coordination,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June
    2022, pp. 2192–2201.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Park, E. Aksan, X. Zhang, and O. Hilliges, “Towards end-to-end video-based
    eye-tracking,” in *European Conference on Computer Vision (ECCV)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Y. Cheng and F. Lu, “Gaze estimation using transformer,” *arXiv preprint
    arXiv:2105.14424*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb,
    “Learning from simulated and unsupervised images through adversarial training,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2017, pp. 2107–2116.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] S.-H. Kim, G.-S. Lee, H.-J. Yang *et al.*, “Eye semantic segmentation
    with a lightweight model,” in *2019 IEEE/CVF International Conference on Computer
    Vision Workshop (ICCVW)*.   IEEE, 2019, pp. 3694–3697.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] P. Rot, Ž. Emeršič, V. Struc, and P. Peer, “Deep multi-class eye segmentation
    for ocular biometrics,” in *2018 IEEE International Work Conference on Bioinspired
    Intelligence (IWOBI)*.   IEEE, 2018, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] B. Luo, J. Shen, Y. Wang, and M. Pantic, “The ibug eye segmentation dataset,”
    in *2018 Imperial College Computing Student Workshop (ICCSW 2018)*.   Schloss
    Dagstuhl-Leibniz-Zentrum fuer Informatik, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] A. Das, U. Pal, M. A. Ferrer, and M. Blumenstein, “Ssrbc 2016: Sclera
    segmentation and recognition benchmarking competition,” in *2016 International
    Conference on Biometrics (ICB)*.   IEEE, 2016, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] A. Das, U. Pal, M. Blumenstein, C. Wang, Y. He, Y. Zhu, and Z. Sun, “Sclera
    segmentation benchmarking competition in cross-resolution environment,” in *2019
    International Conference on Biometrics (ICB)*.   IEEE, 2019, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] F. Boutros, N. Damer, F. Kirchbuchner, and A. Kuijper, “Eye-mms: Miniature
    multi-scale segmentation network of key eye-regions in embedded applications,”
    in *Proceedings of the IEEE International Conference on Computer Vision Workshops*,
    2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] “OKAO vision, howpublished = [http://www.omron.com/r_d/coretech/vision/okao.html.](http://www.omron.com/r_d/coretech/vision/okao.html.)”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] K. A. Funes Mora, F. Monay, and J.-M. Odobez, “Eyediap: A database for
    the development and evaluation of gaze estimation algorithms from rgb and rgb-d
    cameras,” in *Proceedings of the Symposium on Eye Tracking Research and Applications*,
    2014, pp. 255–258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] E. Wood, T. Baltrusaitis, X. Zhang, Y. Sugano, P. Robinson, and A. Bulling,
    “Rendering of eyes for eye-shape registration and gaze estimation,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2015, pp. 3756–3764.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Y. Ganin, D. Kononenko, D. Sungatullina, and V. Lempitsky, “Deepwarp:
    Photorealistic image resynthesis for gaze manipulation,” in *European conference
    on computer vision*.   Springer, 2016, pp. 311–326.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] “Yu, S.:Harr feature cart-tree based cascade eye detector homepage.,
    howpublished = [http://yushiqi.cn/research/eyedetection](http://yushiqi.cn/research/eyedetection).”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] J. Xiang and G. Zhu, “Joint face detection and facial expression recognition
    with mtcnn,” in *2017 4th International Conference on Information Science and
    Control Engineering (ICISCE)*.   IEEE, 2017, pp. 424–427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 39, no. 12, pp. 2481–2495, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Y. Xiong, H. J. Kim, and V. Singh, “Mixed effects neural networks (menets)
    with applications to gaze estimation,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 7743–7752.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] P. Kansal and S. Devanathan, “Eyenet: Attention based convolutional encoder-decoder
    network for eye region segmentation,” in *2019 IEEE/CVF International Conference
    on Computer Vision Workshop (ICCVW)*.   IEEE, 2019, pp. 3688–3693.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] M. Bühler, S. Park, S. De Mello, X. Zhang, and O. Hilliges, “Content-consistent
    generation of realistic eyes with style,” *arXiv preprint arXiv:1911.03346*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Y. Zhu, Y. Yan, and O. Komogortsev, “Hierarchical hmm for eye movement
    classification,” in *European Conference on Computer Vision*.   Springer, 2020,
    pp. 544–554.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] O. V. Komogortsev and A. Karpov, “Automated classification and scoring
    of smooth pursuit eye movements in the presence of fixations and saccades,” *Behavior
    research methods*, vol. 45, no. 1, pp. 203–215, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Y. Shen, O. Komogortsev, and S. S. Talathi, “Domain adaptation for eye
    segmentation,” in *European Conference on Computer Vision*.   Springer, 2020,
    pp. 555–569.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] J. Perry and A. S. Fernandez, “Eyeseg: Fast and efficient few-shot semantic
    segmentation,” in *European Conference on Computer Vision*.   Springer, 2020,
    pp. 570–582.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] P. A. Dias, D. Malafronte, H. Medeiros, and F. Odone, “Gaze estimation
    for assisted living environments,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2020, pp. 290–299.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Y. Cheng, X. Zhang, F. Lu, and Y. Sato, “Gaze estimation by exploring
    two-eye asymmetry,” *IEEE Transactions on Image Processing*, vol. 29, pp. 5259–5272,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Y. Cheng, Y. Bao, and F. Lu, “Puregaze: Purifying gaze feature for generalizable
    gaze estimation,” *arXiv preprint arXiv:2103.13173*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. Ghosh, M. Hayat, A. Dhall, and J. Knibbe, “Mtgls: Multi-task gaze
    estimation with limited supervision,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2022, pp. 3223–3234.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Z. Chen and B. Shi, “Offset calibration for appearance-based gaze estimation
    via gaze decomposition,” in *Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision*, 2020, pp. 270–279.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] B. Luo, J. Shen, S. Cheng, Y. Wang, and M. Pantic, “Shape constrained
    network for eye segmentation in the wild,” in *The IEEE Winter Conference on Applications
    of Computer Vision*, 2020, pp. 1952–1960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] W. Wang, J. Shen, X. Dong, A. Borji, and R. Yang, “Inferring salient
    objects from human fixations,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 42, no. 8, pp. 1913–1927, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] R. Kothari, Z. Yang, C. Kanan, R. Bailey, J. B. Pelz, and G. J. Diaz,
    “Gaze-in-wild: A dataset for studying eye and head coordination in everyday activities,”
    *Scientific Reports*, vol. 10, no. 1, pp. 1–18, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] A. Farkhondeh, C. Palmero, S. Scardapane, and S. Escalera, “Towards self-supervised
    gaze estimation,” *arXiv preprint arXiv:2203.10974*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Z. He, A. Spurr, X. Zhang, and O. Hilliges, “Photo-realistic monocular
    gaze redirection using generative adversarial networks,” in *Proceedings of the
    IEEE International Conference on Computer Vision*, 2019, pp. 6932–6941.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] E. Wood, T. Baltrušaitis, L.-P. Morency, P. Robinson, and A. Bulling,
    “Gazedirector: Fully articulated eye gaze redirection in video,” in *Computer
    Graphics Forum*, vol. 37, no. 2.   Wiley Online Library, 2018, pp. 217–225.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] H. Kaur and R. Manduchi, “Subject guided eye image synthesis with application
    to gaze redirection,” in *Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision*, 2021, pp. 11–20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] K. A. Funes Mora, F. Monay, and J.-M. Odobez, “Eyediap: A database for
    the development and evaluation of gaze estimation algorithms from rgb and rgb-d
    cameras,” in *ACM Symposium on Eye Tracking Research and Applications*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] R. Derakhshani, A. Ross, and S. Crihalmeanu, “A new biometric modality
    based on conjunctival vasculature,” in *Proceedings of Artificial Neural Networks
    in Engineering*, 2006, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] R. Derakhshani and A. Ross, “A texture-based neural network classifier
    for biometric identification using ocular surface vasculature,” in *2007 International
    Joint Conference on Neural Networks*.   IEEE, 2007, pp. 2982–2987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] S. Crihalmeanu, A. Ross, and R. Derakhshani, “Enhancement and registration
    schemes for matching conjunctival vasculature,” in *International Conference on
    Biometrics*.   Springer, 2009, pp. 1240–1249.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Q. He, X. Hong, X. Chai, J. Holappa, G. Zhao, X. Chen, and M. Pietikäinen,
    “Omeg: Oulu multi-pose eye gaze dataset,” in *Scandinavian Conference on Image
    Analysis*.   Springer, 2015, pp. 418–427.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] A. Recasens^∗, A. Khosla^∗, C. Vondrick, and A. Torralba, “Where are
    they looking?” in *Advances in Neural Information Processing Systems (NIPS)*,
    2015, ^∗ indicates equal contribution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] E. Wood, T. Baltrušaitis, L.-P. Morency, P. Robinson, and A. Bulling,
    “Learning an appearance-based gaze estimator from one million synthesised images,”
    in *Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &
    Applications*, 2016, pp. 131–138.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] K. Cortacero, T. Fischer, and Y. Demiris, “Rt-bene: a dataset and baselines
    for real-time blink estimation in natural environments,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision Workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] S. Laine, T. Karras, T. Aila, A. Herva, S. Saito, R. Yu, H. Li, and J. Lehtinen,
    “Production-level facial performance capture using deep convolutional neural networks,”
    in *Proceedings of the ACM SIGGRAPH/Eurographics symposium on computer animation*,
    2017, pp. 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] H. Tomas, M. Reyes, R. Dionido, M. Ty, J. Mirando, J. Casimiro, R. Atienza,
    and R. Guinto, “Goo: A dataset for gaze object prediction in retail environments,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2021, pp. 3125–3133.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] K. J. Emery, M. Zannoli, J. Warren, L. Xiao, and S. S. Talathi, “Openneeds:
    A dataset of gaze, head, hand, and scene signals during exploration in open-ended
    vr environments,” in *ACM Symposium on Eye Tracking Research and Applications*,
    2021, pp. 1–7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] L. Wolf, Z. Freund, and S. Avidan, “An eye for an eye: A single camera
    gaze-replacement method,” in *2010 IEEE Computer Society Conference on Computer
    Vision and Pattern Recognition*.   IEEE, 2010, pp. 817–824.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] D. Kononenko and V. Lempitsky, “Learning to look up: Realtime monocular
    gaze correction using machine learning,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2015, pp. 4667–4675.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] M. Sela, P. Xu, J. He, V. Navalpakkam, and D. Lagun, “Gazegan-unpaired
    adversarial image generation for gaze estimation,” *arXiv preprint arXiv:1711.09767*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] B. Benfold and I. Reid, “Unsupervised learning of a scene-specific coarse
    gaze estimator,” in *IEEE International Conference on Computer Vision*, 2011,
    pp. 2344–2351.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] T. Li, Q. Liu, and X. Zhou, “Ultra-low-power gaze tracking for virtual
    reality,” *GetMobile: Mobile Comp. and Comm.*, vol. 22, no. 3, p. 27–31, Jan.
    2019\. [Online]. Available: [https://doi.org/10.1145/3308755.3308765](https://doi.org/10.1145/3308755.3308765)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. Benty, D. Luebke,
    and A. Lefohn, “Towards foveated rendering for gaze-tracked virtual reality,”
    *ACM Transactions on Graphics (TOG)*, vol. 35, no. 6, pp. 1–12, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] S. Park, A. Bhattacharya, Z. Yang, S. R. Das, and D. Samaras, “Mosaic:
    Advancing user quality of experience in 360-degree video streaming with machine
    learning,” *IEEE Transactions on Network and Service Management*, vol. 18, no. 1,
    pp. 1000–1015, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] D. Alexandrovsky, S. Putze, M. Bonfert, S. Höffner, P. Michelmann, D. Wenig,
    R. Malaka, and J. D. Smeddinck, “Examining design choices of questionnaires in
    vr user studies,” ser. CHI ’20.   New York, NY, USA: Association for Computing
    Machinery, 2020, p. 1–21\. [Online]. Available: [https://doi.org/10.1145/3313831.3376260](https://doi.org/10.1145/3313831.3376260)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] G.-A. Koulieris, K. Akşit, C. Richardt, and R. Mantiuk, “Cutting-edge
    vr/ar display technologies (gaze-, accommodation-, motion-aware and hdr-enabled),”
    in *SIGGRAPH Asia 2018 Courses*, ser. SA ’18.   New York, NY, USA: Association
    for Computing Machinery, 2018\. [Online]. Available: [https://doi.org/10.1145/3277644.3277771](https://doi.org/10.1145/3277644.3277771)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] S. Jha, M. F. Marzban, T. Hu, M. H. Mahmoud, and N. A.-D. C. Busso, “The
    multimodal driver monitoring database: A naturalistic corpus to study driver attention,”
    *arXiv preprint arXiv:2101.04639*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] M. W. Johns, A. Tucker, R. Chapman, K. Crowley, and N. Michael, “Monitoring
    eye and eyelid movements by infrared reflectance oculography to measure drowsiness
    in drivers,” *Somnologie-Schlafforschung und Schlafmedizin*, vol. 11, no. 4, pp.
    234–242, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] S. Jha and C. Busso, “Challenges in head pose estimation of drivers in
    naturalistic recordings using existing tools,” in *IEEE International Conference
    on Intelligent Transportation Systems*, 2017, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Y. Feng, G. Cheung, W.-t. Tan, P. Le Callet, and Y. Ji, “Low-cost eye
    gaze prediction system for interactive networked video streaming,” *IEEE Transactions
    on Multimedia*, vol. 15, no. 8, pp. 1865–1879, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] C. Zhang, Q. He, J. Liu, and Z. Wang, “Exploring viewer gazing patterns
    for touch-based mobile gamecasting,” *IEEE Transactions on Multimedia*, vol. 19,
    no. 10, pp. 2333–2344, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Y. Wang, G. Yuan, Z. Mi, J. Peng, X. Ding, Z. Liang, and X. Fu, “Continuous
    driver’s gaze zone estimation using rgb-d camera,” *Sensors*, p. 1287, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] S. S. Mukherjee and N. M. Robertson, “Deep head pose: Gaze-direction
    estimation in multimodal video,” *IEEE Transactions on Multimedia*, vol. 17, no. 11,
    pp. 2094–2107, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] S. Alghowinem, R. Goecke, M. Wagner, G. Parker, and M. Breakspear, “Eye
    movement analysis for depression detection,” in *2013 IEEE International Conference
    on Image Processing*.   IEEE, 2013, pp. 4220–4224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] M. E. Milanak, M. R. Judah, H. Berenbaum, A. F. Kramer, and M. Neider,
    “Ptsd symptoms and overt attention to contextualized emotional faces: Evidence
    from eye tracking,” *Psychiatry research*, vol. 269, pp. 408–413, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] O. Avital, “Method and system of using eye tracking to evaluate subjects,”
    Oct. 8 2015, uS Patent App. 14/681,083.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] K. Sakurai, M. Yan, H. Tamura, and K. Tanno, “A study on gaze estimation
    system using the direction of eyes and face,” in *2016 World Automation Congress
    (WAC)*.   IEEE, 2016, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] K. Sakurai, M. Yan, K. Tanno, and H. Tamura, “Gaze estimation method
    using analysis of electrooculogram signals and kinect sensor,” *Computational
    intelligence and neuroscience*, vol. 2017, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] J. L. Kröger, O. H.-M. Lutz, and F. Müller, “What does your gaze reveal
    about you? on the privacy implications of eye tracking,” in *IFIP International
    Summer School on Privacy and Identity Management*.   Springer, 2019, pp. 226–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] B. John, S. Koppal, and E. Jain, “Eyeveil: degrading iris authentication
    in eye tracking headsets,” in *Proceedings of the 11th ACM Symposium on Eye Tracking
    Research & Applications*, 2019, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] M. Erbilek, M. Fairhurst, and M. C. D. C. Abreu, “Age prediction from
    iris biometrics,” in *5th International Conference on Imaging for Crime Detection
    and Prevention (ICDP 2013)*.   IET, 2013, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] F. J. M. Moss, R. Baddeley, and N. Canagarajah, “Eye movements to natural
    images as a function of sex and personality,” *PloS one*, vol. 7, no. 11, p. e47870,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] S. Hoppe, T. Loetscher, S. A. Morey, and A. Bulling, “Eye movements during
    everyday behavior predict personality traits,” *Frontiers in human neuroscience*,
    vol. 12, p. 105, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] A. Ben Youssef, H. Shimodaira, and D. A. Braude, “Articulatory features
    for speech-driven head motion synthesis,” *Proceedings of Interspeech, Lyon, France*,
    vol. 3, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] C. Ding, L. Xie, and P. Zhu, “Head motion synthesis from speech using
    deep neural networks,” *Multimedia Tools and Applications*, vol. 74, no. 22, pp.
    9871–9888, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] C. Ding, P. Zhu, and L. Xie, “Blstm neural networks for speech driven
    head motion synthesis,” in *Sixteenth Annual Conference of the International Speech
    Communication Association*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] D. Greenwood, S. Laycock, and I. Matthews, “Predicting head pose from
    speech with a conditional variational autoencoder.”   ISCA, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] S. Ghosh, A. Dhall, M. Hayat, and J. Knibbe, “Av-gaze: A study on the
    effectiveness of audio guided visual attention estimation for non-profilic faces,”
    *arXiv preprint arXiv:2207.03048*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/ea0c78a05b66f2bbd6975147856c2158.png) | Shreya
    Ghosh is currently pursuing her PhD at Monash University, Australia. She received
    MS(R) degree in Computer Science and Engineering from the Indian Institute of
    Technology Ropar, India. She received the B.Tech. in CSE from the Govt. College
    of Engineering and Textile Technology (Serampore, India). Her research interests
    include affective computing, computer vision, Deep Learning. She is a student
    member of the IEEE. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/5a06e0077b3a72e7e211785a411b49ec.png) | Abhinav
    Dhall is an Assistant Professor at Indian Institute of Technology Ropar and Adjunct
    Senior Lecturer at Monash University . He received PhD from the Australian National
    University in 2014\. Followed by postdocs at the University of Waterloo and the
    University of Canberra. He was awarded the Best Doctoral Paper Award at ACM ICMR
    2013, Best Student Paper Honourable mention at IEEE AFGR 2013 and Best Paper Nomination
    at IEEE ICME 2012\. His research interests are in computer vision for Affective
    computing and Assistive Technology. He is a member of the IEEE and Associate Editor
    of IEEE Transactions on Affective Computing. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/a62a13f26859d4780b4d83fd177939f2.png) | Munawar
    Hayat is currently a Senior Research Fellow with Monash University, Australia.
    He received his PhD from The University of Western Australia (UWA). His PhD thesis
    received multiple awards, including the Deans List Honorable Mention Award and
    the Robert Street Prize. After his PhD, he joined IBM Research as a postdoc and
    then moved to the University of Canberra as an Assistant Professor. He was a Senior
    Scientist at the Inception Institute of Artificial Intelligence, UAE. He has been
    awarded the ARC DECRA fellowship. His research interests are in computer vision,
    machine learning, deep learning, and affective computing. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/d395d166c18346e7c2cdfb3fb15a888b.png) | Jarrod
    Knibbe is currently with the University of Melbourne, Australia. He received his
    PhD from The University of Bristol in 2016\. He completed a post-doc in human-centred
    computing at the University of Copenhagen. His research interests include interaction
    design and user experience, with a focus on body based user interfaces, electric
    muscle stimulation, and virtual reality. He has published over 25 papers at top
    venues in Human-Computer Interaction, including CHI, UIST, CSCW, and Ubicomp.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/d5057ee31cdcdacb016803d3aa365eaa.png) | Qiang
    Ji is a Professor with the Department of Electrical, Computer, and Systems Engineering
    at Rensselaer Polytechnic Institute (RPI). He received his Ph.D degree in Electrical
    Engineering from the University of Washington. He was a program director at the
    National Science Foundation, where he managed NSF’s computer vision and machine
    learning programs. He also held teaching and research positions at University
    of Illinois at Urbana-Champaign, Carnegie Mellon University, and University of
    Nevada at Reno. His research interests are in computer vision, probabilistic machine
    learning, and their applications. He has published over 300 papers, received multiple
    awards for his work, serve as an editor for multiple international journals, and
    organize numerous international conferences/workshops. He is a fellow of the IEEE
    and the IAPR. |'
  prefs: []
  type: TYPE_TB
