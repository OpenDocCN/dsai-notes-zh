- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:52:08'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:52:08
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2108.05479] Automatic Gaze Analysis: A Survey of Deep Learning based Approaches'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2108.05479] 自动注视分析：深度学习方法综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2108.05479](https://ar5iv.labs.arxiv.org/html/2108.05479)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2108.05479](https://ar5iv.labs.arxiv.org/html/2108.05479)
- en: 'Automatic Gaze Analysis: A Survey of Deep Learning based Approaches'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动注视分析：深度学习方法综述
- en: 'Shreya Ghosh, Abhinav Dhall, Munawar Hayat, Jarrod Knibbe, Qiang Ji S. Ghosh
    and M. Hayat are with Monash University. (E-mail: {shreya.ghosh, munawar.hayat}@monash.edu).
    A. Dhall is with Monash University and Indian Institute of Technology Ropar, India.
    (E-mail: abhinav.dhall@monash.edu). J. Knibbe is with the University of Melbourne.
    (E-mail: jarrod.knibbe@unimelb.edu.au). Q. Ji is with the Rensselaer Polytechnic
    Institute (E-mail: jiq@rpi.edu).'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Shreya Ghosh、Abhinav Dhall、Munawar Hayat、Jarrod Knibbe、Qiang Ji S. Ghosh 和 M.
    Hayat 均隶属于蒙纳士大学。（电子邮件：{shreya.ghosh, munawar.hayat}@monash.edu）。A. Dhall 除了在蒙纳士大学外，还在印度理工学院罗帕尔分校工作。（电子邮件：abhinav.dhall@monash.edu）。J.
    Knibbe 就职于墨尔本大学。（电子邮件：jarrod.knibbe@unimelb.edu.au）。Q. Ji 在伦斯勒理工学院工作。（电子邮件：jiq@rpi.edu）。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Eye gaze analysis is an important research problem in the field of Computer
    Vision and Human-Computer Interaction. Even with notable progress in the last
    10 years, automatic gaze analysis still remains challenging due to the uniqueness
    of eye appearance, eye-head interplay, occlusion, image quality, and illumination
    conditions. There are several open questions, including what are the important
    cues to interpret gaze direction in an unconstrained environment without prior
    knowledge and how to encode them in real-time. We review the progress across a
    range of gaze analysis tasks and applications to elucidate these fundamental questions,
    identify effective methods in gaze analysis, and provide possible future directions.
    We analyze recent gaze estimation and segmentation methods, especially in the
    unsupervised and weakly supervised domain, based on their advantages and reported
    evaluation metrics. Our analysis shows that the development of a robust and generic
    gaze analysis method still needs to address real-world challenges such as unconstrained
    setup and learning with less supervision. We conclude by discussing future research
    directions for designing a real-world gaze analysis system that can propagate
    to other domains including Computer Vision, Augmented Reality (AR), Virtual Reality
    (VR), and Human Computer Interaction (HCI). Project Page: [https://github.com/i-am-shreya/EyeGazeSurvey](https://github.com/i-am-shreya/EyeGazeSurvey)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 眼动分析是计算机视觉和人机交互领域中的一个重要研究问题。尽管在过去10年中取得了显著进展，由于眼睛外观的独特性、眼头交互、遮挡、图像质量和照明条件等因素，自动注视分析仍然具有挑战性。仍然存在几个未解的问题，包括在没有先验知识的非约束环境中如何解读注视方向的重要线索，以及如何实时编码这些线索。我们回顾了在各种注视分析任务和应用中的进展，以阐明这些基本问题，识别注视分析中的有效方法，并提供可能的未来方向。我们分析了近期的注视估计和分割方法，特别是在无监督和弱监督领域，基于其优点和报告的评估指标。我们的分析表明，开发一种稳健且通用的注视分析方法仍需解决现实世界中的挑战，如非约束设置和较少监督下的学习。我们通过讨论未来研究方向来总结，以设计一个能够推广到其他领域的现实世界注视分析系统，包括计算机视觉、增强现实（AR）、虚拟现实（VR）和人机交互（HCI）。项目页面：[https://github.com/i-am-shreya/EyeGazeSurvey](https://github.com/i-am-shreya/EyeGazeSurvey)
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引术语：
- en: Gaze Analysis, Automated Gaze Estimation, Eye Segmentation, Gaze Tracking, Unsupervised
    and Self-supervised Gaze Analysis, Human Computer Interaction.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注视分析，自动注视估计，眼部分割，注视跟踪，无监督和自监督注视分析，人机交互。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Humans perceive their environment through voluntary or involuntary eye movement
    to receive, fixate and track visual stimuli, or in response to an auditory, or
    cognitive stimulus. The eye movements therefore can provide insights into our
    visual attention [[1](#bib.bib1)] and cognition (emotions, beliefs and desires) [[2](#bib.bib2)].
    Furthermore, we rely on these insights extensively in day-to-day communication
    and social interaction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通过自愿或非自愿的眼动来感知环境，从而接收、固定和跟踪视觉刺激，或对听觉或认知刺激做出反应。因此，眼动可以提供对我们视觉注意力[[1](#bib.bib1)]和认知（情感、信念和欲望）[[2](#bib.bib2)]的见解。此外，我们在日常交流和社交互动中广泛依赖这些见解。
- en: 'Automatic gaze analysis develops techniques to estimate the position of target
    objects by observing the eyes’ movement. However, accurate gaze analysis is a
    complex problem. An accurate method should be able to disentangle gaze, while
    being resilient to a broad array of challenges, including: eye-head interplay,
    illumination variations, eye registration errors, occlusions, and identity bias.
    Furthermore, research [[3](#bib.bib3)] has shown how human gaze follows an arbitrary
    trajectory during eye movements which poses further challenge in gaze estimation.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自动注视分析开发了通过观察眼睛运动来估计目标物体位置的技术。然而，准确的注视分析是一个复杂的问题。一个准确的方法应该能够解开注视，同时对包括：眼睛与头部的相互作用、光照变化、眼睛登记错误、遮挡和身份偏见在内的各种挑战具有弹性。此外，研究[[3](#bib.bib3)]显示了人类注视在眼睛运动过程中遵循任意轨迹，这进一步增加了注视估计的挑战。
- en: 'Research in gaze analysis mainly involves coarse or fine-grained gaze estimation.
    There are three aspects of gaze analysis: registration, representation, and recognition.
    The first step, registration, involves the detection of the eyes (or eye-related
    key points or sometimes even just the face). In the second step, representation,
    the detected eye is projected to a meaningful feature space. In the final stage,
    recognition, the corresponding gaze direction or gaze location is predicted based
    on the features from stage 2. Research interest in automatic gaze analysis spans
    in several disciplines. One of the earliest explorations of gaze analysis was
    conducted in 1879, when Javal et al. [[4](#bib.bib4)] first studied, and coined
    the term, saccades. The broader interest in gaze analysis, however, developed
    with the advent of eye tracking technologies (initially in 1908, before gaining
    momentum in the late 70s, with systems such as ‘Purkinje Image’[[5](#bib.bib5)],
    ‘Bright Pupil’ [[6](#bib.bib6)]). Automated gaze analysis then gained traction
    in computer vision-related assistive technology [[7](#bib.bib7), [8](#bib.bib8)],
    which then propagated through HCI [[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)],
    consumer behavior analysis [[12](#bib.bib12)], AR and VR [[13](#bib.bib13), [14](#bib.bib14)],
    egocentric vision [[15](#bib.bib15)], biometric systems [[16](#bib.bib16)] and
    other domains [[17](#bib.bib17), [18](#bib.bib18)]. A brief chronology of the
    seminal gaze analysis methods with important milestones is presented in Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches"). The increased reliance on gaze tracking technologies, however,
    came with its own challenges, namely, the cost of such devices and the requirement
    for specific controlled settings. To overcome their limitations and handle generic
    unconstrained settings [[19](#bib.bib19), [20](#bib.bib20)], most traditional
    gaze analysis models rely on handcrafted low-level features (e.g., color [[21](#bib.bib21)],
    shape [[22](#bib.bib22), [21](#bib.bib21)] and appearance [[23](#bib.bib23)])
    and certain geometrical heuristics [[20](#bib.bib20)]. Since 2015, the approach
    to gaze analysis has changed, turning to the deep learning [[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)], similar to other computer vision tasks. With
    the deep learning based models and the availability of the large training datasets,
    the challenges associated with the variation in lighting, camera setup, eye-head
    interplay, etc, are reduced greatly over the past few years. Although, these performance
    enhancements have come with the requirement of large scale annotated data, which
    is expensive to acquire. As such, more recently, deep learning with limited annotation
    has gained increasing popularity  [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)].'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 凝视分析的研究主要涉及粗略或细致的凝视估计。凝视分析有三个方面：注册、表示和识别。第一步，注册，涉及眼睛（或与眼睛相关的关键点，有时甚至只是脸）的检测。在第二步，表示，检测到的眼睛被投影到一个有意义的特征空间。在最后阶段，识别，根据第二阶段的特征预测相应的凝视方向或凝视位置。自动凝视分析的研究兴趣涵盖多个学科。早期对凝视分析的探索之一发生在1879年，当时Javal等人[[4](#bib.bib4)]首次研究并创造了“眼跳”这一术语。然而，凝视分析的广泛兴趣随着眼动追踪技术的出现而发展（最初在1908年，之后在70年代末期获得发展，如‘Purkinje
    Image’[[5](#bib.bib5)]和‘Bright Pupil’[[6](#bib.bib6)]）。随后，自动凝视分析在计算机视觉相关的辅助技术[[7](#bib.bib7),
    [8](#bib.bib8)]中获得了关注，并通过人机交互[[9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]、消费者行为分析[[12](#bib.bib12)]、增强现实和虚拟现实[[13](#bib.bib13),
    [14](#bib.bib14)]、自我中心视觉[[15](#bib.bib15)]、生物识别系统[[16](#bib.bib16)]及其他领域[[17](#bib.bib17),
    [18](#bib.bib18)]传播开来。图[1](#S1.F1 "图 1 ‣ 1 介绍 ‣ 自动凝视分析：基于深度学习的方法综述")展示了具有重要里程碑的开创性凝视分析方法的简要年表。然而，对凝视追踪技术的依赖带来了自身的挑战，即这些设备的成本和对特定受控环境的要求。为了克服这些限制并处理通用的无约束环境[[19](#bib.bib19),
    [20](#bib.bib20)]，大多数传统的凝视分析模型依赖于手工制作的低级特征（例如颜色[[21](#bib.bib21)]、形状[[22](#bib.bib22),
    [21](#bib.bib21)]和外观[[23](#bib.bib23)]）以及某些几何启发式方法[[20](#bib.bib20)]。自2015年以来，凝视分析的方法发生了变化，转向了深度学习[[24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26)]，类似于其他计算机视觉任务。随着基于深度学习的模型和大规模训练数据集的可用性，过去几年中与光照变化、相机设置、眼头互动等相关的挑战大大减少。然而，这些性能提升也伴随着对大规模注释数据的需求，这些数据获取成本高昂。因此，最近，具有有限注释的深度学习方法获得了越来越高的关注[[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)]。
- en: '![Refer to caption](img/b08fa6f37b120a0ce12db598381e7744.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b08fa6f37b120a0ce12db598381e7744.png)'
- en: <svg version="1.1" width="684.93" height="70.57" overflow="visible"><g transform="translate(0,70.57)
    scale(1,-1)"><g transform="translate(-657.26,17.3)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="79.0092707900927" height="7.3336100733361"
    overflow="visible">[[4](#bib.bib4)]</foreignobject></g></g><g transform="translate(-642.04,89.94)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="105.853051058531"
    height="7.3336100733361" overflow="visible">[[30](#bib.bib30)]</foreignobject></g></g><g
    transform="translate(-560.4,89.94)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="108.205341082053" height="7.3336100733361"
    overflow="visible">[[6](#bib.bib6)]</foreignobject></g></g><g transform="translate(-575.2,31.13)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="116.645911166459"
    height="7.3336100733361" overflow="visible">[[5](#bib.bib5)]</foreignobject></g></g><g
    transform="translate(-453.85,30.44)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="82.3301508233015" height="7.3336100733361"
    overflow="visible">[[22](#bib.bib22)]</foreignobject></g></g><g transform="translate(-437.25,85.79)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="76.3802407638024"
    height="7.3336100733361" overflow="visible">[[19](#bib.bib19)]</foreignobject></g></g><g
    transform="translate(-377.75,35.98)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="116.645911166459" height="7.3336100733361"
    overflow="visible">[[31](#bib.bib31)]</foreignobject></g></g><g transform="translate(-332.09,80.25)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="81.9150408191504"
    height="7.3336100733361" overflow="visible">[[23](#bib.bib23)]</foreignobject></g></g><g
    transform="translate(-304.41,35.98)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="102.808911028089" height="7.3336100733361"
    overflow="visible">[[20](#bib.bib20)]</foreignobject></g></g><g transform="translate(-239.38,31.83)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="112.218071122181"
    height="7.3336100733361" overflow="visible">[[24](#bib.bib24)]</foreignobject></g></g><g
    transform="translate(-182.65,29.06)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="78.3174207831742" height="7.3336100733361"
    overflow="visible">[[26](#bib.bib26)]</foreignobject></g></g><g transform="translate(-232.46,87.17)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="79.4243807942438"
    height="7.3336100733361" overflow="visible">[[25](#bib.bib25)]</foreignobject></g></g><g
    transform="translate(-172.96,83.02)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="73.1977307319773" height="7.3336100733361"
    overflow="visible">[[29](#bib.bib29)]</foreignobject></g></g><g transform="translate(-115.4,83.02)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="105.299571052996"
    height="7.3336100733361" overflow="visible">[[28](#bib.bib28)]</foreignobject></g></g><g
    transform="translate(-55.35,83.02)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="100.456621004566" height="7.3336100733361"
    overflow="visible">[[32](#bib.bib32)]</foreignobject></g></g><g transform="translate(-103.78,27.67)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="113.878511138785"
    height="7.3336100733361" overflow="visible">[[33](#bib.bib33)]</foreignobject></g></g><g
    transform="translate(-85.24,8.03)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="126.746921267469" height="7.3336100733361"
    overflow="visible">[[34](#bib.bib34)]</foreignobject></g></g></g></svg>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <svg version="1.1" width="684.93" height="70.57" overflow="visible"><g transform="translate(0,70.57)
    scale(1,-1)"><g transform="translate(-657.26,17.3)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="79.0092707900927" height="7.3336100733361"
    overflow="visible">[[4](#bib.bib4)]</foreignobject></g></g><g transform="translate(-642.04,89.94)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="105.853051058531"
    height="7.3336100733361" overflow="visible">[[30](#bib.bib30)]</foreignobject></g></g><g
    transform="translate(-560.4,89.94)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="108.205341082053" height="7.3336100733361"
    overflow="visible">[[6](#bib.bib6)]</foreignobject></g></g><g transform="translate(-575.2,31.13)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="116.645911166459"
    height="7.3336100733361" overflow="visible">[[5](#bib.bib5)]</foreignobject></g></g><g
    transform="translate(-453.85,30.44)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="82.3301508233015" height="7.3336100733361"
    overflow="visible">[[22](#bib.bib22)]</foreignobject></g></g><g transform="translate(-437.25,85.79)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="76.3802407638024"
    height="7.3336100733361" overflow="visible">[[19](#bib.bib19)]</foreignobject></g></g><g
    transform="translate(-377.75,35.98)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="116.645911166459" height="7.3336100733361"
    overflow="visible">[[31](#bib.bib31)]</foreignobject></g></g><g transform="translate(-332.09,80.25)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="81.9150408191504"
    height="7.3336100733361" overflow="visible">[[23](#bib.bib23)]</foreignobject></g></g><g
    transform="translate(-304.41,35.98)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="102.808911028089" height="7.3336100733361"
    overflow="visible">[[20](#bib.bib20)]</foreignobject></g></g><g transform="translate(-239.38,31.83)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="112.218071122181"
    height="7.3336100733361" overflow="visible">[[24](#bib.bib24)]</foreignobject></g></g><g
    transform="translate(-182.65,29.06)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="78.3174207831742" height="7.3336100733361"
    overflow="visible">[[26](#bib.bib26)]</foreignobject></g></g><g transform="translate(-232.46,87.17)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="79.4243807942438"
    height="7.3336100733361" overflow="visible">[[25](#bib.bib25)]</foreignobject></g></g><g
    transform="translate(-172.96,83.02)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="73.1977307319773" height="7.3336100733361"
    overflow="visible">[[29](#bib.bib29)]</foreignobject></g></g><g transform="translate(-115.4,83.02)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="105.299571052996"
    height="7.3336100733361" overflow="visible">[[28](#bib.bib28)]</foreignobject></g></g><g
    transform="translate(-55.35,83.02)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="100.456621004566" height="7.3336100733361"
    overflow="visible">[[32](#bib.bib32)]</foreignobject></g></g><g transform="translate(-103.78,27.67)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="113.878511138785"
    height="7.3336100733361" overflow="visible">[[33](#bib.bib33)]</foreignobject></g></g><g
    transform="translate(-85.24,8.03)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="126.746921267469" height="7.3336100733361"
    overflow="visible">[[34](#bib.bib34)]</foreignobject></g></g></g></svg>
- en: 'Figure 1: A brief chronology of seminal gaze analysis works. The very first
    gaze pattern modelling dates back to the work of Javal et al. in 1879 [[4](#bib.bib4)].
    One of the first deep learning driven appearance based gaze estimation models
    was proposed in $2015$ [[24](#bib.bib24)].'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：开创性注视分析工作的简要年表。最早的注视模式建模可以追溯到Javal等人1879年的工作[[4](#bib.bib4)]。其中一个基于深度学习的外观驱动注视估计模型是在$2015$年提出的[[24](#bib.bib24)]。
- en: 'This paper surveys different gaze analysis methods by isolating their fundamental
    components, and discusses how each component addresses the aforementioned challenges
    in gaze analysis. The paper discusses new trends and developments in the field
    of computer vision and the AR/VR domain, from the perspective of gaze analysis.
    We cover recent gaze analysis techniques in the un-, self-, and weakly-supervised
    domain, along with validation protocols with evaluation metrics tailored for gaze
    analysis. We also discuss various data capturing devices, including: RGB/IR camera,
    tablet/laptop’s camera, ladybug camera and other gaze trackers (including video-oculography [[21](#bib.bib21)])
    are also discussed.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过隔离其基本组成部分来调查不同的注视分析方法，并讨论每个组成部分如何解决上述注视分析中的挑战。本文从注视分析的角度讨论了计算机视觉和AR/VR领域的新趋势和发展。我们涵盖了近期在非监督、自监督和弱监督领域的注视分析技术，并介绍了为注视分析量身定制的验证协议和评估指标。我们还讨论了各种数据捕捉设备，包括：RGB/IR摄像头、平板电脑/笔记本电脑摄像头、天牛摄像头及其他注视追踪器（包括视频眼动描记仪[[21](#bib.bib21)]）等。
- en: 'Due to the rapid progress in the computer vision field (Refer Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")), it is increasingly useful to get thorough guidance via exhaustive
    survey/review articles. In 2010 and 2013, Hansen et al. [[21](#bib.bib21)] and
    Chennamma et al. [[35](#bib.bib35)] reviewed the state-of-the-art eye detection
    and gaze tracking techniques. These reviews provide a holistic view of hardware,
    user interface, eye detection, and gaze mapping techniques. Since these reviews
    were before the deep learning era, they contain the relevant features leveraged
    from handcrafted techniques. Afterwards in 2016, Jing et al. [[36](#bib.bib36)]
    reviewed methods for 2-D and 3-D gaze estimation methods. In 2017, Kar et al. [[37](#bib.bib37)]
    provided insights into the issues related to algorithms, system configurations,
    and user conditions. In 2020, a more comprehensive and detailed study of deep
    learning based gaze estimation methods is presented by Cazzato et al. [[38](#bib.bib38)].
    To date, however, no comprehensive review has examined the recent trends in learning
    from less supervision. Moreover, all of the existing reviews focus only on gaze
    estimation and ignore significant works in eye segmentation, gaze zone estimation,
    gaze trajectory prediction, gaze redirection, and unconstrained gaze estimation
    in single and multiperson setting. The contributions of the paper are summarized
    below:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于计算机视觉领域的快速进展（参见图[1](#S1.F1 "图1 ‣ 1 引言 ‣ 自动注视分析：深度学习方法综述")），通过详尽的调查/综述文章获得全面指导变得越来越有用。在2010年和2013年，Hansen等人[[21](#bib.bib21)]和Chennamma等人[[35](#bib.bib35)]回顾了最先进的眼睛检测和注视跟踪技术。这些综述提供了硬件、用户界面、眼睛检测和注视映射技术的全面视角。由于这些综述是在深度学习时代之前，它们包含了从手工制作技术中利用的相关特征。随后在2016年，Jing等人[[36](#bib.bib36)]回顾了2-D和3-D注视估计方法。2017年，Kar等人[[37](#bib.bib37)]提供了有关算法、系统配置和用户条件问题的见解。2020年，Cazzato等人[[38](#bib.bib38)]提出了对基于深度学习的注视估计方法的更全面和详细的研究。然而，迄今为止，还没有综合性的综述文章探讨从少量监督中学习的最新趋势。此外，所有现有的综述仅关注注视估计，而忽视了眼睛分割、注视区域估计、注视轨迹预测、注视重定向以及单人和多人设置下的无约束注视估计等重要工作。本文的贡献总结如下：
- en: '1.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: A comprehensive review of automated gaze analysis. We categorize and summarize
    existing methods by considering data capturing sensors, platforms, popular gaze
    estimation tasks in computer vision, level of supervision and learning paradigm.
    The proposed taxonomies aim to help researchers to get a deeper understanding
    of the key components in gaze analysis.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对自动注视分析的全面综述。我们通过考虑数据捕捉传感器、平台、计算机视觉中流行的注视估计任务、监督程度和学习范式，来对现有方法进行分类和总结。提出的分类法旨在帮助研究人员更深入地理解注视分析中的关键组成部分。
- en: '2.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Different popular tasks under one framework. To the best of our knowledge, we
    are the first to put different eye and gaze related popular tasks under one framework.
    Apart from gaze estimation, we consider gaze trajectory, gaze zone estimation
    and gaze redirection tasks.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在一个框架下的不同流行任务。根据我们的知识，我们是第一个将不同的眼睛和视线相关流行任务放在一个框架下的。除了视线估计，我们还考虑视线轨迹、视线区域估计和视线重定向任务。
- en: '3.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Applications. We explore major applications of gaze analysis using computer
    vision i.e. Augmented and Virtual Reality [[13](#bib.bib13), [39](#bib.bib39)],
    Driver Engagement [[40](#bib.bib40), [41](#bib.bib41)] and Healthcare [[42](#bib.bib42),
    [43](#bib.bib43)].
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 应用。我们探索了使用计算机视觉进行视线分析的主要应用，即增强现实和虚拟现实 [[13](https://example.org), [39](https://example.org)]，驾驶员参与度 [[40](https://example.org),
    [41](https://example.org)] 和医疗保健 [[42](https://example.org), [43](https://example.org)]。
- en: '4.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Privacy Concerns. We also provide a brief review of the privacy concerns of
    the gaze data and its possible implications.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐私问题。我们还简要回顾了视线数据的隐私问题及其可能的影响。
- en: '5.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Overview of open questions and potential research directions. We review several
    issues associated with the current gaze analysis frameworks (i.e. model design,
    dataset collection, etc.) and discuss possible future research directions.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开放问题和潜在研究方向的概述。我们回顾了与当前视线分析框架（即模型设计、数据集收集等）相关的若干问题，并讨论了可能的未来研究方向。
- en: '![Refer to caption](img/62a2aab09d3bbbf5da4340ea057672e7.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/62a2aab09d3bbbf5da4340ea057672e7.png)'
- en: 'Figure 2: Top Left: Overview of the human visual system, eye modelling and
    eye movement. For computer vision based automated gaze analysis, we consider an
    image containing eyes (left) as input. Thus, such methods analyze the visible
    eye regions (middle) and predict the 2-D/3-D gaze vector as output. However, there
    are unobservable factors by which we can predict the true gaze direction (right)
    which requires person-specific information and other factors [[44](#bib.bib44)].
    Bottom Left: Apart from static image based gaze estimation, the dynamic eye movement
    is another line of research in computer vision that provides cues regarding human
    behavioural traits. Right: The actual modelling of gaze with respect to eye anatomy.
    We only highlight the relevant parts i.e. pupil, cornea, iris, sclera, fovea,
    LOS and LOG. The angle between LOG and LOS is called angle of kappa ($\kappa$).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：左上角：人类视觉系统、眼睛建模和眼球运动的概述。对于基于计算机视觉的自动视线分析，我们考虑包含眼睛的图像（左）作为输入。因此，这些方法分析可见的眼睛区域（中间）并预测2D/3D视线向量作为输出。然而，有一些不可观察的因素，我们可以通过这些因素预测真实的视线方向（右），这需要个体特定的信息和其他因素 [[44](https://example.org)]。左下角：除了基于静态图像的视线估计，动态眼球运动是计算机视觉中的另一条研究方向，它提供了关于人类行为特征的线索。右侧：关于眼睛解剖学的实际视线建模。我们只突出显示相关部分，即瞳孔、角膜、虹膜、巩膜、中央凹、LOS
    和 LOG。LOG 和 LOS 之间的角度称为κ角 ($\kappa$)。
- en: 2 Preliminaries
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步工作
- en: 'The human visual system is a complex cognitive process. As such, understanding
    and modelling human gaze has become a fundamental research problem in psychology,
    neurology, cognitive science, and computer vision. To lay the foundations for
    this review, below, we provide brief descriptions of the Human Visual System and
    Eye Modelling (Sec. [2.1](#S2.SS1 "2.1 Human Visual System and Eye Modelling ‣
    2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")),
    Eye movements (Sec. [2.2](#S2.SS2 "2.2 Eye Movements and Foveal Vision ‣ 2 Preliminaries
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")), Problem
    Settings in Automated Gaze Analysis (Sec. [2.3](#S2.SS3 "2.3 Gaze Estimation:
    Problem Setting ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep
    Learning based Approaches")) and the associated Challenges (Sec. [2.4](#S2.SS4
    "2.4 Challenges ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep
    Learning based Approaches")).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 人类视觉系统是一个复杂的认知过程。因此，理解和建模人类视线已经成为心理学、神经学、认知科学和计算机视觉中的一个基本研究问题。为了为本综述奠定基础，以下是对人类视觉系统和眼睛建模（第[2.1](https://example.org)节）、眼球运动（第[2.2](https://example.org)节）、自动视线分析中的问题设置（第[2.3](https://example.org)节）以及相关挑战（第[2.4](https://example.org)节）的简要描述。
- en: 2.1 Human Visual System and Eye Modelling
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 人类视觉系统和眼睛建模
- en: 'Computer vision based human visual perception methods estimate gaze quantitatively
    from image or video data. These methods analyze the visible region of the eyes
    (the iris and sclera, see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")), and attempt to approximate
    the unobservable features of the eyes (which are integral to determining gaze
    direction). These approximations can be based on models of the human eyes, derived
    from movement patterns over time, or learned via representation learning over
    large scale data. For gaze estimation, we typically approximate the eye as a sphere
    with a radius of 12-13mm. Subsequently, we model gaze direction with respect to
    the optical axis, also called the line of gaze (LoG), or the visual axis, the
    line of sight (LoS) (see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")right). The line of
    gaze (LoG) connects the pupil, cornea and eyeball center. Conversely, the line
    of sight (LoS) is the line connecting the fovea and center of the cornea. Generally,
    the LoS is considered as the true direction of gaze. The intersection point of
    the visual and optical axis is called the nodal point of eye (anatomically the
    cornea center), which typically encodes a subject dependent angular offset. This
    individual offset is the main motivation behind having subject dependent calibration
    for gaze tracking devices. According to prior studies [[45](#bib.bib45), [46](#bib.bib46)],
    the fovea is located around 4-5° horizontally and 1.5°  vertically below the optical
    axis. Across a broader population, this can vary up to 3° between subjects [[46](#bib.bib46)].
    Additionally, head-pose also plays an important role in gaze analysis. The coarse
    gaze direction of a subject can be determined by the position (in 3-D coordinate)
    and orientation (Euler angles) of the headpose [[21](#bib.bib21)]. Most of the
    time, the combined direction of LoS and head pose provide information about where
    the person is looking.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 基于计算机视觉的人类视觉感知方法通过图像或视频数据定量估计视线。这些方法分析眼睛的可见区域（虹膜和巩膜，见图[2](#S1.F2 "图 2 ‣ 1 介绍
    ‣ 自动注视分析：基于深度学习方法的综述")），并尝试近似眼睛的不可观测特征（这些特征对确定视线方向至关重要）。这些近似可以基于人眼模型，通过时间的运动模式得出，或通过大规模数据的表示学习得出。对于视线估计，我们通常将眼睛近似为半径为12-13毫米的球体。随后，我们相对于光学轴建模视线方向，也称为注视线（LoG），或视觉轴，视线（LoS）（见图[2](#S1.F2
    "图 2 ‣ 1 介绍 ‣ 自动注视分析：基于深度学习方法的综述")右）。注视线（LoG）连接瞳孔、角膜和眼球中心。相反，视线（LoS）是连接中央凹和角膜中心的线。通常，LoS被认为是视线的真实方向。视觉轴与光学轴的交点称为眼的结点点（解剖学上为角膜中心），通常编码一个主体依赖的角度偏移。这个个体偏移是进行视线追踪设备主体依赖校准的主要动机。根据之前的研究[[45](#bib.bib45),
    [46](#bib.bib46)]，中央凹大约位于光学轴水平下方4-5°和垂直下方1.5°的位置。在更广泛的人群中，这可能在个体之间变化高达3°[[46](#bib.bib46)]。此外，头部姿势在视线分析中也发挥重要作用。通过头部姿势的三维坐标位置和方向（欧拉角），可以确定主体的粗略视线方向[[21](#bib.bib21)]。大多数情况下，LoS和头部姿势的结合方向提供了关于人们在看什么的信息。
- en: 2.2 Eye Movements and Foveal Vision
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 眼部运动与中央视觉
- en: 'We perceive our environment through eye movements. These movements can be voluntary
    or involuntary, and help us to acquire, fixate, and track visual stimuli (see
    Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches")). Eye movements are divided into three primary
    categories: saccades, smooth pursuits, and fixations.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过眼部运动感知环境。这些运动可以是自愿的或非自愿的，帮助我们获取、固定和追踪视觉刺激（见图[2](#S1.F2 "图 2 ‣ 1 介绍 ‣ 自动注视分析：基于深度学习方法的综述")）。眼部运动分为三类：眼跳、平滑追踪和注视。
- en: Saccades. Saccades are rapid and reflexive eye movements, primarily used for
    adjustment to a new location in the visual environment. It can be executed voluntarily
    or involuntarily, as a part of optokinetic measure [[47](#bib.bib47)]. Saccades
    typically last for between 10 and 100 ms.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 眼跳。眼跳是快速而反射性的眼部运动，主要用于调整到视觉环境中的新位置。它可以是自愿或非自愿的，作为光动学测量的一部分[[47](#bib.bib47)]。眼跳通常持续10到100毫秒。
- en: Smooth Pursuits. Smooth pursuit occur while tracking a moving target. This involuntary
    action depends on the range of the target’s motion as astonishingly, human eyes
    can follow the velocity of a moving target to some extend.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑追踪。平滑追踪发生在跟踪移动目标时。这种无意识动作取决于目标运动的范围，因为惊人的是，人眼能够在一定程度上跟随移动目标的速度。
- en: 'Fixations (Microsaccades, Drifts, and Tremors). Fixations are eye movements
    in which the focus of attention stabilizes over a stationary object of interest.
    Fixations are characterized by three types of miniature eye movements: tremor,
    drift and microsaccades [[47](#bib.bib47)]. During Fixations, the miniature eye
    movements occur due to the noise present in the control system to hold gaze steadily.
    This noise occurs in the area of fixation, around 5° visual angle. For simplification
    of the underlying natural process, this noise is ignored during fixation.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 凝视（微小震颤、漂移和震颤）。凝视是眼睛在一个静止的兴趣对象上稳定注意力的运动。凝视的特点是三种微小眼动：震颤、漂移和微小震颤[[47](#bib.bib47)]。在凝视过程中，由于控制系统中的噪声，微小眼动发生，以保持凝视稳定。这种噪声发生在凝视区域，约5°视角。为了简化自然过程，这种噪声在凝视过程中被忽略。
- en: Foveal Vision. The fovea centralis region of human eye is responsible for the
    perception of sharp and high resolution human vision. In order to perceive the
    environment, it is necessary to direct the foveal vision to select region of interest
    (the process is termed as ‘foveation’). This sharp foveal vision decays rapidly
    within the range of 1-5°. Beyond this limit, human vision is blurred, and low
    resolution. This is termed as peripheral vision. Our peripheral vision plays an
    important role in our overall visual experience, especially for motion detection.
    On an abstract level, our visual perception is the result of our brains merging
    our foveal and peripheral vision.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 中央视力。人眼的中央凹区域负责感知清晰和高分辨率的视觉。为了感知环境，需要将中央视力定向到选择的兴趣区域（这个过程被称为‘中央凹注视’）。这种清晰的中央视力在1-5°范围内迅速衰减。超出这个范围，人眼视力变得模糊且低分辨率。这被称为周边视力。我们的周边视力在整体视觉体验中扮演着重要角色，特别是对运动的检测。从抽象层面来看，我们的视觉感知是大脑将中央视力和周边视力合并的结果。
- en: '2.3 Gaze Estimation: Problem Setting'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 凝视估计：问题设置
- en: 'The main task of gaze estimation is to determine the line of sight of the pupil.
    Fig. [4](#S2.F4 "Figure 4 ‣ 2.3 Gaze Estimation: Problem Setting ‣ 2 Preliminaries
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches") depicts
    a typical visual sensor based real-time gaze estimation setup consisting of user,
    data capturing sensor(s) and visual plane. The main calibration factors in this
    setting are:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 凝视估计的主要任务是确定瞳孔的视线。图[4](#S2.F4 "图 4 ‣ 2.3 凝视估计：问题设置 ‣ 2 前期准备 ‣ 自动凝视分析：深度学习方法综述")
    描绘了一个典型的基于视觉传感器的实时凝视估计设置，包含用户、数据捕捉传感器和视觉平面。此设置中的主要标定因素包括：
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Estimation of camera calibration parameters, which include both intrinsic and
    extrinsic camera parameters.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 相机标定参数的估计，包括内在和外在相机参数。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Estimation of geometric calibration parameters, which include the relative position
    of the camera, light source and screen.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 几何标定参数的估计，包括相机、光源和屏幕的相对位置。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Estimation of personal calibration, which include headpose and eye-specific
    parameters such as cornea curvature, the nodal point of the eye, etc.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 个人标定的估计，包括头部姿态和眼睛特定参数，如角膜曲率、眼睛的结点等。
- en: In some of the applications, the calibration parameters are estimated in task-specific
    settings. For example, users are requested to fixate their gaze to some pre-defined
    points for calibration. Similarly, user-specific information is registered once
    in the devices for subject-specific calibrations. With the advances in computer
    vision and deep learning, nowadays gaze estimation techniques are developed with
    appearance based features and do not require an explicit calibration step. For
    example, CalibMe [[48](#bib.bib48)] is a fast and unsupervised calibration technique
    for gaze trackers, designed to overcome the burden of repeated calibration.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些应用中，标定参数在特定任务设置中进行估计。例如，用户被要求将视线固定在一些预定义的点上进行标定。类似地，用户特定的信息在设备中注册以进行个体特定的标定。随着计算机视觉和深度学习的进步，如今的凝视估计技术基于外观特征开发，不需要显式的标定步骤。例如，CalibMe[[48](#bib.bib48)]
    是一种快速且无监督的标定技术，旨在克服重复标定的负担。
- en: '![Refer to caption](img/d82053596e8045fdd41a241da8d5d914.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d82053596e8045fdd41a241da8d5d914.png)'
- en: <svg version="1.1" width="621.24" height="97.17" overflow="visible"><g transform="translate(0,97.17)
    scale(1,-1)"><g transform="translate(-339.01,172.96)"><g transform="translate(0,36.9447903694479)
    scale(1, -1)"><foreignobject width="621.281306212813" height="7.3336100733361"
    overflow="visible">[[49](#bib.bib49), [24](#bib.bib24), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53), [26](#bib.bib26), [54](#bib.bib54), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60)]</foreignobject></g></g><g
    transform="translate(-339.01,159.13)"><g transform="translate(0,36.9447903694479)
    scale(1, -1)"><foreignobject width="619.482496194825" height="7.3336100733361"
    overflow="visible">[[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [49](#bib.bib49),
    [29](#bib.bib29), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [27](#bib.bib27)]</foreignobject></g></g><g
    transform="translate(-339.01,117.61)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="378.44195378442" height="7.3336100733361"
    overflow="visible">[[67](#bib.bib67), [68](#bib.bib68), [32](#bib.bib32), [34](#bib.bib34),
    [69](#bib.bib69)]</foreignobject></g></g><g transform="translate(-339.01,69.19)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="373.460633734606"
    height="7.3336100733361" overflow="visible">[[70](#bib.bib70), [68](#bib.bib68),
    [32](#bib.bib32), [34](#bib.bib34)]</foreignobject></g></g><g transform="translate(-339.01,20.76)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="75.1349107513491"
    height="7.3336100733361" overflow="visible">[[71](#bib.bib71)]</foreignobject></g></g></g></svg>
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: <svg version="1.1" width="621.24" height="97.17" overflow="visible"><g transform="translate(0,97.17)
    scale(1,-1)"><g transform="translate(-339.01,172.96)"><g transform="translate(0,36.9447903694479)
    scale(1, -1)"><foreignobject width="621.281306212813" height="7.3336100733361"
    overflow="visible">[[49](#bib.bib49), [24](#bib.bib24), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53), [26](#bib.bib26), [54](#bib.bib54), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59), [60](#bib.bib60)]</foreignobject></g></g><g
    transform="translate(-339.01,159.13)"><g transform="translate(0,36.9447903694479)
    scale(1, -1)"><foreignobject width="619.482496194825" height="7.3336100733361"
    overflow="visible">[[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [49](#bib.bib49),
    [29](#bib.bib29), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66), [27](#bib.bib27)]</foreignobject></g></g><g
    transform="translate(-339.01,117.61)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="378.44195378442" height="7.3336100733361"
    overflow="visible">[[67](#bib.bib67), [68](#bib.bib68), [32](#bib.bib32), [34](#bib.bib34),
    [69](#bib.bib69)]</foreignobject></g></g><g transform="translate(-339.01,69.19)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="373.460633734606"
    height="7.3336100733361" overflow="visible">[[70](#bib.bib70), [68](#bib.bib68),
    [32](#bib.bib32), [34](#bib.bib34)]</foreignobject></g></g><g transform="translate(-339.01,20.76)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="75.1349107513491"
    height="7.3336100733361" overflow="visible">[[71](#bib.bib71)]</foreignobject></g></g></g></svg>
- en: 'Figure 3: The plot shows the popularity of different data capturing devices
    across research articles over the past 10 years. Here, HMD: Head Mounted Device,
    RGBD: RGB Depth camera.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：该图展示了过去 10 年间不同数据捕捉设备在研究文章中的受欢迎程度。在这里，HMD：头戴式设备，RGBD：RGB 深度相机。
- en: 'Role of Data Capturing Sensors. Visual stimuli provide valuable information
    for computer vision based gaze estimation techniques. A trade-off of widely used
    sensors is mentioned in Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 Gaze Estimation: Problem
    Setting ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches"). These data capturing sensors are mainly divided into two categories:
    Intrusive and Non-intrusive. An array of methods used specialized hardware which
    requires physical contact with human skin or eyes are termed as intrusive sensors.
    The widely used intrusive sensors are head-mounted devices (HMD), electrodes,
    or scleral coils [[69](#bib.bib69), [72](#bib.bib72)]. These devices may cause
    unpleasant user experience, and the accuracy of these systems depends on the tolerance,
    accurate subject-specific calibration and other factors of the devices. On the
    other hand, data capturing devices that do not require physical contact [[73](#bib.bib73)]
    are termed non-intrusive sensors. Mainly, RGB, RGBD and IR cameras fall under
    this category. These methods face several challenges, which include partial occlusion
    of the iris by the eyelid, varying illumination condition, head pose, specular
    reflection in case the user wears glasses, the inability to use standard shape
    fitting for iris boundary detection, and other effects including motion blur and
    over saturation of images [[73](#bib.bib73)]. To deal with these challenges, most
    of the existing gaze estimation methods have been performed under constrained
    environments like constrained head pose, controlled illumination conditions, and
    camera angle. Among all of the aforementioned factors, pupil visibility plays
    an important role as robust gaze estimation needs accurate pupil-center localization.
    Fast and accurate pupil-center localization is still a challenging task [[74](#bib.bib74)],
    particularly for images with low resolution. A trade-off of widely used sensors
    are mentioned in Fig. [3](#S2.F3 "Figure 3 ‣ 2.3 Gaze Estimation: Problem Setting
    ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据捕捉传感器的作用。视觉刺激为基于计算机视觉的注视估计技术提供了有价值的信息。图[3](#S2.F3 "图 3 ‣ 2.3 注视估计：问题设置 ‣ 2
    基础 ‣ 自动注视分析：基于深度学习的方法综述")中提到了一些广泛使用传感器的权衡。这些数据捕捉传感器主要分为两类：侵入式和非侵入式。使用专用硬件，需要与人体皮肤或眼睛有物理接触的方法被称为侵入式传感器。广泛使用的侵入式传感器包括头戴式设备（HMD）、电极或巩膜线圈 [[69](#bib.bib69),
    [72](#bib.bib72)]。这些设备可能会导致不愉快的用户体验，这些系统的准确性取决于设备的耐受性、准确的受试者特定校准和其他因素。另一方面，不需要物理接触的数据捕捉设备 [[73](#bib.bib73)]
    被称为非侵入式传感器。主要包括RGB、RGBD和IR摄像头。这些方法面临多个挑战，包括眼睑部分遮挡虹膜、不同的光照条件、头部姿势、用户佩戴眼镜时的镜面反射、虹膜边界检测无法使用标准形状拟合，以及其他效果如运动模糊和图像过度饱和 [[73](#bib.bib73)]。为了应对这些挑战，大多数现有的注视估计方法都在受限的环境下进行，例如受限的头部姿势、控制的光照条件和摄像头角度。在上述所有因素中，瞳孔的可见性起着重要作用，因为稳健的注视估计需要准确的瞳孔中心定位。快速而准确的瞳孔中心定位仍然是一个具有挑战性的任务 [[74](#bib.bib74)]，尤其对于低分辨率图像。图[3](#S2.F3
    "图 3 ‣ 2.3 注视估计：问题设置 ‣ 2 基础 ‣ 自动注视分析：基于深度学习的方法综述")中提到了一些广泛使用传感器的权衡。
- en: Role of Headpose. Gaze estimation is a challenging task due to eye-head interplay.
    Head-pose plays the most important role in gaze estimation. The gaze direction
    of a subject is determined by the combined effect of position and orientation
    of head pose and eyeball. One can change gaze direction via eyeball and pupil
    movement by maintaining stationary or dynamic head-pose or by moving both. Usually,
    this process is subject dependent. People adjust their head-pose and gaze to maintain
    a comfortable posture. Thus, the gaze estimation task needs to consider both gaze
    and head-pose at the same time for inference. As a result of this, it is more
    common to consider head-pose information in the gaze estimation methods implicitly
    or explicitly [[24](#bib.bib24), [50](#bib.bib50), [75](#bib.bib75)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 头部姿势的作用。由于眼睛与头部的相互作用，注视估计是一项具有挑战性的任务。头部姿势在注视估计中发挥着最重要的作用。一个主体的注视方向由头部姿势和眼球的位置信息和方向的综合效果决定。可以通过维持静态或动态的头部姿势或同时移动两者来改变注视方向。通常，这个过程依赖于个体。人们调整头部姿势和注视方向以保持舒适的姿势。因此，注视估计任务需要同时考虑注视和头部姿势。因此，考虑头部姿势信息在注视估计方法中是更加常见的，无论是隐式还是显式 [[24](#bib.bib24),
    [50](#bib.bib50), [75](#bib.bib75)]。
- en: '![Refer to caption](img/8fd599e0b47c45bb730a88bcdf506881.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8fd599e0b47c45bb730a88bcdf506881.png)'
- en: 'Figure 4: Overview of gaze estimation setups (See Sec. [2.3](#S2.SS3 "2.3 Gaze
    Estimation: Problem Setting ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches") for more details). A traditional gaze analysis
    setup considers the effect of head, visual plane and camera coordinates. The gaze
    analysis tasks include gaze zone, point of regard, gaze trajectory estimation
    etc. (See Sec. [3](#S3 "3 Gaze Analysis in Computer Vision ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")) The gaze vector is defined by the
    angles ($\theta,\phi$) in polar co-ordinate systems as shown in the gaze direction
    part.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '图4：注视估计设置的概述（有关更多细节，请参见第[2.3](#S2.SS3 "2.3 Gaze Estimation: Problem Setting
    ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")节）。传统的注视分析设置考虑头部、视觉平面和相机坐标的影响。注视分析任务包括注视区域、注视点、注视轨迹估计等（参见第[3](#S3
    "3 Gaze Analysis in Computer Vision ‣ Automatic Gaze Analysis: A Survey of Deep
    Learning based Approaches")节）。注视向量由极坐标系统中的角度（$\theta,\phi$）定义，如注视方向部分所示。'
- en: 'Role of Visual Plane. The visual plane is the plane containing the gaze target
    point i.e. where the subject is looking, which is often termed as Point of Gaze
    (PoG). The distance between the user and the visual plane varies a lot in a real-world
    setting. Thus, recent deep learning based methods do not rely on the distance
    or placement of the visual plane. The most common gaze analysis setup uses a RGB
    camera placed at 20 - 70 cm from the user in unconstrained setting i.e. without
    any invasive sensors or fixed setup. In different real-world settings, the visual
    plane could be desktop ($\sim$ 60cm), mobile phone ($\sim$ 20cm), car ($\sim$
    50cm) etc. An overview is presented in Table [I](#S2.T1 "Table I ‣ 2.3 Gaze Estimation:
    Problem Setting ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep
    Learning based Approaches").'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '视觉平面的作用。视觉平面是包含注视目标点的平面，即被观察者所看的地方，这通常称为注视点（PoG）。用户与视觉平面之间的距离在实际环境中差异很大。因此，最近的深度学习方法不依赖于视觉平面的距离或位置。最常见的注视分析设置使用放置在离用户20
    - 70厘米处的RGB相机，在非约束设置中，即没有任何侵入性传感器或固定设置。在不同的实际环境中，视觉平面可以是桌面（约60厘米）、手机（约20厘米）、汽车（约50厘米）等。概述见表
    [I](#S2.T1 "Table I ‣ 2.3 Gaze Estimation: Problem Setting ‣ 2 Preliminaries ‣
    Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")。'
- en: 'Table I: Attributes of different platforms widely used in gaze analysis. Here,
    Dist.: distance (in cm), VA: viewing Angle (in °), HMD: Head Mounted Devices,
    FV: Free Viewing, UC: User Condition, ET: External Target.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表I：广泛用于注视分析的不同平台的属性。这里，Dist.：距离（以厘米为单位），VA：视角（以度为单位），HMD：头戴设备，FV：自由观看，UC：用户条件，ET：外部目标。
- en: '| Platform | Dist. | VA | UC | Papers |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 平台 | 距离 | 视角 | 用户条件 | 论文 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '|'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Desktop, &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 桌面，&#124;'
- en: '&#124; TV Panels &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 电视面板 &#124;'
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 30-50, &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 30-50，&#124;'
- en: '&#124; 200-500 &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 200-500 &#124;'
- en: '|'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; $\sim$ 40 °, &#124;'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 约40 °，&#124;'
- en: '&#124; 40°-60° &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 40°-60° &#124;'
- en: '|'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Static, &#124;'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 静态，&#124;'
- en: '&#124; Sitting, &#124;'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 坐姿，&#124;'
- en: '&#124; Upright &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直立 &#124;'
- en: '|'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[49](#bib.bib49), [24](#bib.bib24), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53), [26](#bib.bib26), [54](#bib.bib54), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57)] &#124;'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[49](#bib.bib49), [24](#bib.bib24), [50](#bib.bib50), [51](#bib.bib51),
    [52](#bib.bib52), [53](#bib.bib53), [26](#bib.bib26), [54](#bib.bib54), [55](#bib.bib55),
    [56](#bib.bib56), [57](#bib.bib57)] &#124;'
- en: '&#124;  [[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [49](#bib.bib49),
    [29](#bib.bib29), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)] &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63), [49](#bib.bib49),
    [29](#bib.bib29), [64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)] &#124;'
- en: '&#124;  [[76](#bib.bib76), [71](#bib.bib71), [58](#bib.bib58), [59](#bib.bib59),
    [60](#bib.bib60)] &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[76](#bib.bib76), [71](#bib.bib71), [58](#bib.bib58), [59](#bib.bib59),
    [60](#bib.bib60)] &#124;'
- en: '|'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| HMD | 2-5 | 55°-75° |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 头戴显示器 | 2-5 | 55°-75° |'
- en: '&#124; Independent &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 独立 &#124;'
- en: '&#124; (Leanback, &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （靠背，&#124;'
- en: '&#124; Sitting, &#124;'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 坐姿，&#124;'
- en: '&#124; Upright) &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直立）&#124;'
- en: '|'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[77](#bib.bib77), [78](#bib.bib78), [32](#bib.bib32), [34](#bib.bib34)]
    &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[77](#bib.bib77), [78](#bib.bib78), [32](#bib.bib32), [34](#bib.bib34)]
    &#124;'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Automotive | 50 | 40°-60° |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 汽车 | 50 | 40°-60° |'
- en: '&#124; Mobile, &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 移动设备，&#124;'
- en: '&#124; Sitting, &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 坐姿，&#124;'
- en: '&#124; Upright &#124;'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直立 &#124;'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[40](#bib.bib40), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)] &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[40](#bib.bib40), [79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)] &#124;'
- en: '|'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Handheld | 20-40 | 5°-12° |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 手持设备 | 20-40 | 5°-12° |'
- en: '&#124; Leanfwd, &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Leanfwd，&#124;'
- en: '&#124; Sitting, &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 坐着，&#124;'
- en: '&#124; Standing, &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 站立，&#124;'
- en: '&#124; Mobile &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 移动 &#124;'
- en: '|'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[25](#bib.bib25), [63](#bib.bib63), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89)] &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[25](#bib.bib25), [63](#bib.bib63), [86](#bib.bib86), [87](#bib.bib87),
    [88](#bib.bib88), [89](#bib.bib89)] &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ET/ FV | – | – |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| ET/ FV | – | – |'
- en: '&#124; Leanfwd, &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Leanfwd，&#124;'
- en: '&#124; Sitting, &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 坐着，&#124;'
- en: '&#124; Standing, &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 站立，&#124;'
- en: '&#124; Upright &#124;'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直立 &#124;'
- en: '|'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[90](#bib.bib90), [27](#bib.bib27)] &#124;'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[90](#bib.bib90), [27](#bib.bib27)] &#124;'
- en: '|'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 2.4 Challenges
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 挑战
- en: Data Annotation. Generally, deep learning based methods require large amount
    of annotated data for generalized representation learning. Curating large scale
    annotated gaze datasets is non-trivial [[90](#bib.bib90), [24](#bib.bib24), [87](#bib.bib87)],
    time consuming and requires expensive equipment setup. Current dataset recording
    paradigms via wearable sensors may lead to uncomfortable user experience and it
    require expert knowledge. Another common aspect of the current datasets is the
    constrained environment in which they are recorded (For example, CAVE [[23](#bib.bib23)]
    dataset is recorded on indoor environment with headpose restricted). Recently,
    a few datasets [[90](#bib.bib90), [87](#bib.bib87)] have been proposed to address
    this gap by recording in unconstrained indoor and outdoor environments. Another
    challenge associated with data annotation is participant’s cooperation. However,
    it is assumed that participants fixate their gaze as per the given instructions [[90](#bib.bib90),
    [24](#bib.bib24), [87](#bib.bib87)]. Despite these attempts [[90](#bib.bib90),
    [24](#bib.bib24), [87](#bib.bib87)], data annotation still remains complex, noisy
    and time-consuming. Self/weakly/un-supervised learning paradigms [[27](#bib.bib27),
    [28](#bib.bib28)] could be helpful to address the dataset creation and annotation
    challenges.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 数据注释。通常，基于深度学习的方法需要大量的标注数据来进行泛化表示学习。整理大规模标注的注视数据集并非易事[[90](#bib.bib90), [24](#bib.bib24),
    [87](#bib.bib87)]，耗时且需要昂贵的设备设置。目前通过可穿戴传感器记录数据的模式可能会导致用户体验不适，而且需要专业知识。当前数据集的另一个常见特点是记录时的受限环境（例如，CAVE
    [[23](#bib.bib23)] 数据集是在室内环境中记录的，头部姿势受到限制）。最近，已经提出了一些数据集 [[90](#bib.bib90), [87](#bib.bib87)]
    以填补这一空白，通过在非受限的室内和室外环境中进行记录。与数据注释相关的另一个挑战是参与者的配合。然而，假设参与者会按照给定的指示固定他们的注视[[90](#bib.bib90),
    [24](#bib.bib24), [87](#bib.bib87)]。尽管有这些尝试 [[90](#bib.bib90), [24](#bib.bib24),
    [87](#bib.bib87)]，数据注释仍然复杂、嘈杂且耗时。自监督/弱监督/无监督学习范式 [[27](#bib.bib27), [28](#bib.bib28)]
    可能有助于解决数据集创建和注释的挑战。
- en: Subjective Bias. Another challenge for the automatic gaze analysis method is
    subjective bias. Individual differences in the nodal points of human eyes makes
    automatic and generic gaze analysis way more difficult. In an ideal scenario,
    any gaze analysis method should encode rich features corresponding to eye region
    appearance, which provides relevant information for gaze analysis. To address
    this challenge, few-shot learning based approach has been widely adapted  [[29](#bib.bib29),
    [91](#bib.bib91)], where the motivation is to adapt to new subject with minimum
    subject specific information. Another way to deal with the subjective bias is
    combining classical eye model based approaches with geometrical constraints [[92](#bib.bib92)]
    as this approach has the potential to generalize well across subjects.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 主观偏差。自动注视分析方法的另一个挑战是主观偏差。个体眼睛的结点点差异使得自动和通用的注视分析变得更加困难。在理想情况下，任何注视分析方法都应编码与眼睛区域外观相关的丰富特征，这为注视分析提供了相关信息。为应对这一挑战，已经广泛采用了少样本学习方法
    [[29](#bib.bib29), [91](#bib.bib91)]，其动机是以最少的个体特定信息适应新对象。应对主观偏差的另一种方法是将经典眼部模型方法与几何约束
    [[92](#bib.bib92)] 相结合，因为这种方法有可能在不同个体之间实现良好的泛化。
- en: Eye Blink. Blinks are an involuntary and periodic motion of the eyelids. They
    pose a challenge for gaze analysis, as blinks result in missed frames of data.
    A few recent works [[90](#bib.bib90), [40](#bib.bib40)] assume that the head pose
    information is a suitable replacement for gaze during blinking based on a common
    line of sight between a subject’s headpose and gaze. However, it is noted that
    a large shift in the gaze is possible after subject re-opens their eyes. To simplify
    the situation, some gaze analysis methods ignore eye-blink data (e.g., [[87](#bib.bib87),
    [57](#bib.bib57)]) and some treat blinks as a separate class of data (.e.g, [[93](#bib.bib93),
    [41](#bib.bib41)]). A possibility for real world deployment of such a system is
    generating gaze labels by interpolating from neighbouring frames’ labels when
    blinks are detected [[40](#bib.bib40)].
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 眼睑闪烁。眨眼是眼睑的非自主性和周期性运动。它们对注视分析提出了挑战，因为眨眼会导致数据帧丢失。一些近期的研究[[90](#bib.bib90), [40](#bib.bib40)]假设头部姿态信息可以作为眨眼时注视的适当替代，因为头部姿态与注视之间存在共同视线。然而，值得注意的是，受试者重新睁开眼睛后，注视可能会有很大偏移。为了简化情况，一些注视分析方法忽略了眼睑闪烁数据（例如，[[87](#bib.bib87),
    [57](#bib.bib57)]），而一些则将眨眼视为一种单独的数据类别（例如，[[93](#bib.bib93), [41](#bib.bib41)]）。这种系统在现实世界中的一个可能应用是通过从相邻帧的标签中插值生成注视标签，以应对眨眼的检测[[40](#bib.bib40)]。
- en: Data Attributes. Several factors, such as eye-head interplay, occlusion, blurred
    image, and illumination can influence the performance of a gaze analysis model.
    The presence of any subset of these attributes can degrade the performance of
    a system [[90](#bib.bib90), [87](#bib.bib87)]. Many methods use face alignment [[24](#bib.bib24),
    [25](#bib.bib25)] and 3-D head pose estimation [[24](#bib.bib24)] as a pre-processing
    step. However, face alignment on images captured in an unconstrained environment
    based images may introduce noise in a system. To overcome this, recent approaches [[90](#bib.bib90),
    [94](#bib.bib94), [57](#bib.bib57), [27](#bib.bib27)] avoid these pre-processing
    steps and show increase in gaze prediction performance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 数据属性。多个因素，如眼头互动、遮挡、模糊图像和光照，可能会影响注视分析模型的性能。这些属性的任何子集的存在都可能降低系统的性能[[90](#bib.bib90),
    [87](#bib.bib87)]。许多方法使用面部对齐[[24](#bib.bib24), [25](#bib.bib25)]和3-D头部姿态估计[[24](#bib.bib24)]作为预处理步骤。然而，在无约束环境下捕捉的图像上进行面部对齐可能会在系统中引入噪声。为了解决这个问题，近期的方法[[90](#bib.bib90),
    [94](#bib.bib94), [57](#bib.bib57), [27](#bib.bib27)]避免了这些预处理步骤，并显示出注视预测性能的提升。
- en: Another critical challenge in gaze estimation is eye-head interplay. Prior studies
    generally address this issue via implicit training [[25](#bib.bib25), [95](#bib.bib95)]
    or provide the head pose information separately as a feature [[24](#bib.bib24)].
    Similarly, it is challenging to estimate gaze under partial occlusion. When the
    head’s yaw rotation is greater than 90°, one side of the face becomes occluded
    w.r.t. the camera. A few prior works [[25](#bib.bib25), [24](#bib.bib24)] avoid
    these scenarios by disregarding these frames. Kellnhofer et al. [[90](#bib.bib90)],
    however, argue that when the head yaw angle is in the range 90°- 135°, the partial
    visibility still provides relevant information about the gaze direction. This
    study also proposes quantile regression via pinball loss to mitigate the effect
    of partial occlusion in training data in terms of uncertainty. Despite all of
    these attempts, gaze estimation still remains challenging in presence of these
    attributes. There is still have scope to eliminate the effects of these attributes
    and make the gaze analysis model more robust for the real-world deployment.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注视估计中的另一个关键挑战是眼头互动。以往的研究通常通过隐式训练[[25](#bib.bib25), [95](#bib.bib95)]来解决这个问题，或将头部姿态信息单独提供作为特征[[24](#bib.bib24)]。同样，在部分遮挡下估计注视也具有挑战性。当头部的偏航旋转角度大于90°时，面部的一侧相对于相机会被遮挡。一些以往的研究[[25](#bib.bib25),
    [24](#bib.bib24)]通过忽略这些帧来避免这些场景。然而，Kellnhofer等人[[90](#bib.bib90)]认为，当头部偏航角在90°-135°范围内时，部分可见性仍提供了有关注视方向的相关信息。该研究还提出了通过“弹球损失”进行分位回归，以减少训练数据中部分遮挡的影响。尽管进行了所有这些尝试，注视估计在这些属性存在时仍然具有挑战性。仍有空间消除这些属性的影响，使注视分析模型在实际部署中更具鲁棒性。
- en: Application Specific Challenges. Gaze analysis also has application-specific
    requirements, for example, coarse or fine gaze estimation in AR, VR, Robotics,
    egocentric vision and HCI. Thus, a working algorithm behind any eye-tracking devices
    needs to fit in the application environment.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 应用特定挑战。注视分析还有应用特定的要求，例如AR、VR、机器人、以自我为中心的视觉和HCI中的粗略或精确注视估计。因此，任何眼动追踪设备背后的工作算法需要适应应用环境。
- en: 3 Gaze Analysis in Computer Vision
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 计算机视觉中的注视分析
- en: We provide a breakdown of different gaze analysis tasks for vision based applications.
    Any statistical gaze modeling mainly estimates the relation between the input
    visual data and the point of regard/gaze direction.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供了针对视觉应用的不同注视分析任务的细分。任何统计注视建模主要估计输入视觉数据与注视点/注视方向之间的关系。
- en: '2-D/3-D Gaze Estimation. Most of the existing studies consider gaze estimation
    as either the gaze direction in 3-D space or as the point of regard in 2-D/3-D
    coordinates (see Fig. [4](#S2.F4 "Figure 4 ‣ 2.3 Gaze Estimation: Problem Setting
    ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")).
    We can divide the gaze estimation methods into the following types:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '2-D/3-D 注视估计。大多数现有研究将注视估计视为3-D空间中的注视方向或2-D/3-D坐标中的注视点（见图[4](#S2.F4 "Figure
    4 ‣ 2.3 Gaze Estimation: Problem Setting ‣ 2 Preliminaries ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")）。我们可以将注视估计方法分为以下几种类型：'
- en: '1) Geometric Methods: These geometric methods compute a gaze direction from
    the geometric model of the eye (see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")), where
    the anatomical structure of the eye is considered to get the 3-D gaze direction
    or gaze vector. These methods were widely used in prior to more deep learning
    approaches [[21](#bib.bib21)]. These recent deep learning based approaches implicitly
    model these geometric parameters during the learning process, and, as such, do
    not explicitly require the often noisy subject specific parameters, such as cornea
    radii, cornea center, angles of kappa (i.e. Refer $\kappa$ in Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")), iris radius, the distance between the pupil center and cornea
    center, etc.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '1) 几何方法：这些几何方法从眼睛的几何模型中计算注视方向（见图[2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")），其中考虑了眼睛的解剖结构以获取3-D注视方向或注视向量。这些方法在更多深度学习方法出现之前被广泛使用[[21](#bib.bib21)]。这些近期的深度学习方法在学习过程中隐式建模这些几何参数，因此不需要显式地考虑经常噪声较大的特定于个体的参数，如角膜半径、角膜中心、κ角（即图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")中的$\kappa$）、虹膜半径、瞳孔中心与角膜中心之间的距离等。'
- en: '2) Regression Methods: Regression based methods [[96](#bib.bib96), [24](#bib.bib24),
    [26](#bib.bib26), [28](#bib.bib28)] map visual stimuli (image or image-related
    features) to gaze coordinates or gaze angles in 2-D/3-D. The output mapping is
    application-specific. For example, such techniques are often used to map 2-D/3-D
    gaze coordinate mainly maps people’s focus of attention to the screen coordinates
    (for human-computer interaction based applications such as engagement or attention
    monitoring). Regression based methods can be divided into two types: the parametric
    and non-parametric approaches. Parametric approaches (e.g., [[96](#bib.bib96),
    [28](#bib.bib28)]) assume gaze trajectories as a polynomial, where the task is
    to estimate the parameters of the polynomial equation. Non-parametric approaches
    directly work on the mappings in spite of calculating the intersection between
    the gaze direction and gazed object explicitly [[24](#bib.bib24), [26](#bib.bib26),
    [57](#bib.bib57)]. The recent deep learning based approaches are non-parametric [[26](#bib.bib26),
    [97](#bib.bib97), [28](#bib.bib28), [54](#bib.bib54), [90](#bib.bib90)].'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 回归方法：基于回归的方法[[96](#bib.bib96), [24](#bib.bib24), [26](#bib.bib26), [28](#bib.bib28)]
    将视觉刺激（图像或图像相关特征）映射到2-D/3-D中的注视坐标或注视角度。输出映射是特定于应用的。例如，这些技术通常用于将2-D/3-D注视坐标主要映射到屏幕坐标（例如用于人机交互应用中的参与度或注意力监测）。基于回归的方法可以分为两种类型：参数化方法和非参数化方法。参数化方法（例如[[96](#bib.bib96),
    [28](#bib.bib28)]）假设注视轨迹为多项式，其任务是估计多项式方程的参数。非参数化方法直接处理映射，而不需要显式计算注视方向与被注视物体之间的交点[[24](#bib.bib24),
    [26](#bib.bib26), [57](#bib.bib57)]。近期的深度学习方法是非参数化的[[26](#bib.bib26), [97](#bib.bib97),
    [28](#bib.bib28), [54](#bib.bib54), [90](#bib.bib90)]。
- en: 'Trajectory Prediction. Gaze estimation has potential applications in AR/VR
    especially in Foveated Rending (FR) and Attention Tunneling (AT), where the future
    eye trajectory prediction is highly desirable. To meet this requirement, a new
    research direction (i.e. future gaze trajectory prediction) has been recently
    introduced [[34](#bib.bib34)]. Here, possible future gaze locations can be estimated
    based on the prior gaze points, content of the visual plane or their combination.
    Thus, the problem statement can be formulated as follows: given $n$ number of
    prior gaze points, the algorithm will predict the $m$ future frames’ gaze direction
    in a user-specific setting.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测。注视估计在增强现实/虚拟现实中具有潜在应用，特别是在焦点渲染（FR）和注意力隧道（AT）中，未来眼睛轨迹预测是非常期望的。为了满足这一要求，最近提出了一种新的研究方向（即未来注视轨迹预测）[[34](#bib.bib34)]。在这里，可以根据先前的注视点、视觉平面的内容或它们的组合来估计可能的未来注视位置。因此，问题陈述可以表述为：给定$n$个先前的注视点，算法将预测用户特定设置下$m$个未来帧的注视方向。
- en: Gaze Zone. In many gaze estimation based applications such as driver gaze [[40](#bib.bib40),
    [41](#bib.bib41), [93](#bib.bib93), [27](#bib.bib27)], gaming platforms [[98](#bib.bib98)],
    website designing [[99](#bib.bib99)], etc., the exact position or angle of the
    line of sight of the pupil is not required. Thus, a gaze zone approach is utilized
    in these cases for estimation. Here, the gaze zone refers to an area in 2-D or
    3-D space. For example, in a simplistic driver gaze zone estimation, the driver
    could be looking straight ahead, at the steering wheel, at the radio, or at the
    mirrors. Similarly, another example is detecting more visually salient zone/region
    during website designing [[99](#bib.bib99)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 注视区域。在许多基于注视估计的应用中，如驾驶员注视[[40](#bib.bib40), [41](#bib.bib41), [93](#bib.bib93),
    [27](#bib.bib27)]，游戏平台[[98](#bib.bib98)]，网站设计[[99](#bib.bib99)]等，不需要瞳孔视线的精确位置或角度。因此，在这些情况下使用注视区域方法进行估计。在这里，注视区域指的是二维或三维空间中的一个区域。例如，在简化的驾驶员注视区域估计中，驾驶员可能直视前方、注视方向盘、收音机或镜子。类似的例子是在网站设计中检测更为视觉显著的区域[[99](#bib.bib99)]。
- en: Gaze Redirection. Due to the challenges in different posed gaze conditions,
    generation on the go is gaining popularity [[100](#bib.bib100), [34](#bib.bib34)].
    It aims to capture subject-specific signals from a few eye images of an individual
    and generate realistic eye images for the same individual under different eye
    states (gaze direction, camera position, eye openness etc.). The gaze redirection
    can be performed in both controlled and uncontrolled way [[101](#bib.bib101),
    [100](#bib.bib100), [102](#bib.bib102)]. Apart from these, eye rendering is another
    research direction to generate realistic eye given the appearance, gaze direction
    of a person. It has potential applications in virtual agents, social robotics,
    behaviour generation and in the animation industry [[103](#bib.bib103)].
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注视重定向。由于在不同的注视条件下面临的挑战，实时生成正在获得越来越多的关注[[100](#bib.bib100), [34](#bib.bib34)]。其目的是从个体的少量眼睛图像中捕捉特定的信号，并在不同的眼睛状态（注视方向、摄像头位置、眼睛开合度等）下生成逼真的眼睛图像。注视重定向可以在控制和非控制的方式下进行[[101](#bib.bib101),
    [100](#bib.bib100), [102](#bib.bib102)]。此外，眼睛渲染是另一种研究方向，旨在根据个人的外观和注视方向生成逼真的眼睛图像。它在虚拟代理、社交机器人、行为生成以及动画产业中具有潜在应用[[103](#bib.bib103)]。
- en: 'Unconstrained Gaze Estimation. Gaze estimation in an unconstrained setting
    can be divided into two types:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 非约束注视估计。非约束设置下的注视估计可以分为两种类型：
- en: '1) Single Person Setting: In webcam or RGB camera based gaze estimation approaches,
    geometric model based eye tracking [[104](#bib.bib104), [105](#bib.bib105)] is
    typically used, as it is fast and does not require training data. At the same
    time, however, it relies on accurate eye location and key points detection, which
    is hard to achieve in real-world environments. Deep learning based methods [[106](#bib.bib106),
    [97](#bib.bib97)] have eliminated this issue to some extent, however, it still
    remains a challenge as it does not generalize well in different settings.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 单人设置：在基于网络摄像头或RGB摄像机的注视估计方法中，通常使用几何模型基础的眼睛跟踪[[104](#bib.bib104), [105](#bib.bib105)]，因为它速度快且不需要训练数据。然而，它依赖于准确的眼睛位置和关键点检测，这在实际环境中很难实现。基于深度学习的方法[[106](#bib.bib106),
    [97](#bib.bib97)]在某种程度上消除了这个问题，但由于其在不同环境中的泛化能力不足，这仍然是一个挑战。
- en: '2) Multi-Person Setting: In unconstrained multi-person settings, it is very
    difficult to track the eyes. For example, in a social interaction scenario, understanding
    the gaze behaviour of each person provides important cues to interpret social
    dynamics [[107](#bib.bib107)]. To this end, a new research direction is introduced
    where the problem is defined as whether the people are Looking At Each Other (LAEO)
    in a given video sequence [[108](#bib.bib108), [109](#bib.bib109), [110](#bib.bib110)].
    Similarly, gaze communication [[111](#bib.bib111)] and GazeOnce [[112](#bib.bib112)]
    are another line of research aligned with this field.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 多人设置：在无约束的多人环境中，追踪眼睛是非常困难的。例如，在社交互动场景中，理解每个人的凝视行为为解读社会动态提供了重要线索[[107](#bib.bib107)]。为此，引入了一个新的研究方向，其中问题定义为人们在给定的视频序列中是否相互注视（LAEO）[[108](#bib.bib108),
    [109](#bib.bib109), [110](#bib.bib110)]。类似地，凝视交流[[111](#bib.bib111)]和GazeOnce[[112](#bib.bib112)]是与此领域相关的另一研究方向。
- en: Visual Attention Estimation. Human visual attention estimation is another line
    of research which mainly focuses on where the person is looking irrespective of
    eyes visibility. The popular sub-tasks in this direction are gaze following [[113](#bib.bib113),
    [114](#bib.bib114), [62](#bib.bib62), [115](#bib.bib115), [116](#bib.bib116)],
    gaze communication [[111](#bib.bib111)], human attention in goal-driven environments [[117](#bib.bib117)]
    and categorical visual search [[118](#bib.bib118)], visual scan-path analysis
    in visual question answering [[119](#bib.bib119)], and naturalistic environment [[120](#bib.bib120),
    [121](#bib.bib121)]. These methods are mostly driven by saliency in the scene,
    head orientation, or any other task at hand. Visual attention based approaches
    have the potential to localize the gaze target directly from scene information
    which in turn enhances the scalability of naturalistic gaze behaviour patterns.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉注意力估计。人类视觉注意力估计是另一条研究方向，主要关注于人们看向何处，无论眼睛是否可见。这个方向上的流行子任务包括凝视跟踪[[113](#bib.bib113),
    [114](#bib.bib114), [62](#bib.bib62), [115](#bib.bib115), [116](#bib.bib116)]，凝视交流[[111](#bib.bib111)]，目标驱动环境中的人类注意力[[117](#bib.bib117)]，分类视觉搜索[[118](#bib.bib118)]，视觉问答中的视觉扫描路径分析[[119](#bib.bib119)]，以及自然环境[[120](#bib.bib120),
    [121](#bib.bib121)]。这些方法主要依靠场景的显著性、头部方向或其他任务。基于视觉注意力的方法具有直接从场景信息中定位凝视目标的潜力，从而增强自然凝视行为模式的可扩展性。
- en: 4 Gaze Analysis Framework
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 凝视分析框架
- en: 'We break down a gaze analysis framework into its fundamental components (Fig. [5](#S4.F5
    "Figure 5 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")) and discuss their
    role in terms of eye detection and segmentation (Sec. [4.1](#S4.SS1 "4.1 Eye Detection
    and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches")), Network Architecture (Sec. [4.2](#S4.SS2
    "4.2 Representative Deep Network Architectures ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")) and Level of Supervision
    (Sec. [4.3](#S4.SS3 "4.3 Level of Supervision ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将凝视分析框架分解为其基本组件（图 [5](#S4.F5 "图 5 ‣ 4.1 眼睛检测与分割 ‣ 4 凝视分析框架 ‣ 自动凝视分析：基于深度学习的方法概述")），并讨论它们在眼睛检测和分割（节
    [4.1](#S4.SS1 "4.1 眼睛检测与分割 ‣ 4 凝视分析框架 ‣ 自动凝视分析：基于深度学习的方法概述")）、网络架构（节 [4.2](#S4.SS2
    "4.2 代表性深度网络架构 ‣ 4 凝视分析框架 ‣ 自动凝视分析：基于深度学习的方法概述")）和监督等级（节 [4.3](#S4.SS3 "4.3 监督等级
    ‣ 4 凝视分析框架 ‣ 自动凝视分析：基于深度学习的方法概述")）中的作用。
- en: 4.1 Eye Detection and Segmentation
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 眼睛检测与分割
- en: Eye registration is the first stage of gaze analysis and requires detection
    of the eye and the relevant regions of interest.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 眼睛注册是凝视分析的第一阶段，需要检测眼睛及相关的兴趣区域。
- en: 'Eye Detection Methods. The main aim of the eye detection algorithms is to accurately
    identify the eye region from an input image. Eye detection algorithms need to
    operate in challenging conditions such as occlusion, eye openness, variability
    in eye size, head pose, illumination, and viewing angle, while balancing the trade-off
    in appearance, dynamic variation and computational complexity. Prior works on
    eye detection can be divided into three categories: shape based [[22](#bib.bib22)],
    appearance based [[122](#bib.bib122), [123](#bib.bib123), [49](#bib.bib49), [105](#bib.bib105)]
    and hybrid method [[123](#bib.bib123)]. The most popular libraries for eye and
    facial point detection are [Dlib](https://github.com/davisking/dlib) [[106](#bib.bib106)]
    [OpenFace](https://github.com/cmusatyalab/openface) [[105](#bib.bib105), [104](#bib.bib104)],
    [MTCNN](https://github.com/kpzhang93/MTCNN_face_detection_alignment) [[124](#bib.bib124)],
    [Duel Shot Face Detector](https://github.com/Tencent/FaceDetection-DSFD) [[125](#bib.bib125)],
    [FaceX-Zoo](https://github.com/JDAI-CV/FaceX-Zoo) [[126](#bib.bib126)].'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 眼睛检测方法。眼睛检测算法的主要目标是从输入图像中准确识别眼睛区域。眼睛检测算法需要在诸如遮挡、眼睛开度、眼睛大小变化、头部姿态、光照和视角等挑战条件下运行，同时平衡外观、动态变化和计算复杂度之间的权衡。以往关于眼睛检测的研究可以分为三类：基于形状的[[22](#bib.bib22)]、基于外观的[[122](#bib.bib122),
    [123](#bib.bib123), [49](#bib.bib49), [105](#bib.bib105)]和混合方法[[123](#bib.bib123)]。最受欢迎的眼睛和面部点检测库包括
    [Dlib](https://github.com/davisking/dlib) [[106](#bib.bib106)]、[OpenFace](https://github.com/cmusatyalab/openface)
    [[105](#bib.bib105), [104](#bib.bib104)]、[MTCNN](https://github.com/kpzhang93/MTCNN_face_detection_alignment)
    [[124](#bib.bib124)]、[Duel Shot Face Detector](https://github.com/Tencent/FaceDetection-DSFD)
    [[125](#bib.bib125)] 和 [FaceX-Zoo](https://github.com/JDAI-CV/FaceX-Zoo) [[126](#bib.bib126)]。
- en: The pupil and iris region of the eye is usually darker than the sclera which
    provides an important cue to differentiate or localize the pupil. The pupil center
    localization use dedicated and costly devices [[69](#bib.bib69), [72](#bib.bib72)],
    which requires person-specific pre-calibration. To overcome this limitation, the
    deep learning based pupil localization methods use ensembles of randomized trees [[127](#bib.bib127)],
    local self similarity matching [[73](#bib.bib73)], adaptive gradient boosting [[128](#bib.bib128)],
    hough regression forests [[129](#bib.bib129)], deep learning based landmark localization
    models [[97](#bib.bib97), [106](#bib.bib106)], heterogeneous CNN models [[130](#bib.bib130)],
    etc. In prior literature, the choice of eye registration process is influenced
    by the correlation between the input image and the learning objective of the proposed
    method. Apart from this, the trade-off between the accuracy of eye localization
    and running time complexity of the algorithm is optimized in a task specific way.
    In this context, OpenFace and Dlib are the most popular. Moreover, the choice
    of eye/face registration process may also depend on their ability to detect eye
    components in different challenging real world conditions.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 眼睛的瞳孔和虹膜区域通常比巩膜更暗，这提供了一个重要的线索来区分或定位瞳孔。瞳孔中心定位使用专用且昂贵的设备[[69](#bib.bib69), [72](#bib.bib72)]，这需要特定于个人的预校准。为克服这一限制，基于深度学习的瞳孔定位方法使用了随机森林集成[[127](#bib.bib127)]、局部自相似匹配[[73](#bib.bib73)]、自适应梯度提升[[128](#bib.bib128)]、霍夫回归森林[[129](#bib.bib129)]、基于深度学习的标志定位模型[[97](#bib.bib97),
    [106](#bib.bib106)]、异构CNN模型[[130](#bib.bib130)]等。在以往的文献中，眼睛注册过程的选择受输入图像与所提方法学习目标之间的相关性影响。除此之外，眼睛定位的准确性和算法运行时间复杂度之间的权衡以任务特定的方式进行优化。在这个背景下，OpenFace和Dlib是最受欢迎的。此外，眼睛/面部注册过程的选择还可能取决于它们在不同挑战性现实世界条件下检测眼部组件的能力。
- en: '![Refer to caption](img/326fb4daeabfc0781c534535923a70c0.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![请参见说明](img/326fb4daeabfc0781c534535923a70c0.png)'
- en: 'Figure 5: A generic gaze analysis framework has different components including
    registration, gaze representations and inference. Although in the deep learning
    based approaches, there is a high overlap between the representation and inference
    module. Refer Sec. [4](#S4 "4 Gaze Analysis Framework ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches") for more details.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '图 5：一个通用的注视分析框架包含不同的组件，包括注册、注视表示和推断。尽管在基于深度学习的方法中，表示和推断模块之间存在高度重叠。有关更多细节，请参见第[4](#S4
    "4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches)节。'
- en: 'Eye Segmentation. The main task of eye segmentation is pixel-wise or region-wise
    differentiation of the visible eye parts. In general, the eye region is divided
    into three parts: sclera (the white region of the eyes), iris (the colour ring
    of tissue around the pupil) and pupil (the dark iris region). Prior studies [[131](#bib.bib131),
    [132](#bib.bib132), [133](#bib.bib133), [134](#bib.bib134)] on eye segmentation
    mainly explore to segment the iris and sclera region. Few studies [[32](#bib.bib32),
    [34](#bib.bib34)] include the pupil region in the segmentation task as well. Eye
    segmentation is widely used in the biometric systems [[135](#bib.bib135)] and
    prior for synthetic eye generation [[136](#bib.bib136)].'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 眼睛分割。眼睛分割的主要任务是像素级或区域级区分可见的眼睛部分。通常，眼睛区域分为三个部分：巩膜（眼睛的白色区域），虹膜（围绕瞳孔的彩色组织环）和瞳孔（黑暗的虹膜区域）。先前的研究[[131](#bib.bib131)，[132](#bib.bib132)，[133](#bib.bib133)，[134](#bib.bib134)]主要探讨了虹膜和巩膜区域的分割。也有少数研究[[32](#bib.bib32)，[34](#bib.bib34)]将瞳孔区域纳入分割任务。眼睛分割广泛应用于生物识别系统[[135](#bib.bib135)]和合成眼睛生成的先验研究[[136](#bib.bib136)]。
- en: Eye Blink Detection. Eye blinks are the involuntary and periodic activity that
    can help to judge the cognitive activity of a person (e.g. driver’s fatigue [[137](#bib.bib137)],
    lie detection [[138](#bib.bib138)]). KLT trackers and various sensors are also
    widely used to get the eye motion information to track eye blink [[139](#bib.bib139)].
    The existing eye blink detection approaches aim to solve a binary classification
    problem (blink/no blink) either in a heuristic based or data-driven way. The heuristic
    based approaches mainly include motion localization [[139](#bib.bib139)] and template
    matching [[140](#bib.bib140)]. As these methods are highly reliable on pre-defined
    thresholds, they could be sensitive to subjective bias, illumination and head
    pose. To overcome this limitation, the data-driven approaches infer on the basis
    of appearance based temporal motion features [[141](#bib.bib141), [139](#bib.bib139)]
    or spatial features [[142](#bib.bib142)]. In hybrid approach [[143](#bib.bib143)],
    multi-scale LSTM based framework is used to detect eye blink using both spatial
    and temporal information.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 眼睑检测。眼睑是无意识且周期性的活动，可以帮助判断一个人的认知活动（例如，司机的疲劳[[137](#bib.bib137)]，谎言检测[[138](#bib.bib138)]）。KLT
    跟踪器和各种传感器也被广泛用于获取眼睛运动信息以跟踪眼睑[[139](#bib.bib139)]。现有的眼睑检测方法旨在解决一个二元分类问题（眨眼/不眨眼），无论是基于启发式还是数据驱动的方法。基于启发式的方法主要包括运动定位[[139](#bib.bib139)]和模板匹配[[140](#bib.bib140)]。由于这些方法高度依赖预定义的阈值，它们可能对主观偏见、光照和头部姿势敏感。为了克服这一局限性，数据驱动的方法基于外观的时间运动特征[[141](#bib.bib141)，[139](#bib.bib139)]或空间特征[[142](#bib.bib142)]进行推断。在混合方法[[143](#bib.bib143)]中，使用多尺度
    LSTM 基础框架通过空间和时间信息来检测眼睑。
- en: '![Refer to caption](img/e15baf5d1096f3d6999e8976d1003b68.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e15baf5d1096f3d6999e8976d1003b68.png)'
- en: <svg version="1.1" width="627.35" height="37.36" overflow="visible"><g transform="translate(0,37.36)
    scale(1,-1)"><g transform="translate(-654.49,381.9)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="112.218071122181" height="7.3336100733361"
    overflow="visible">[[24](#bib.bib24)]</foreignobject></g></g><g transform="translate(-470.46,373.6)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="68.3547806835478"
    height="7.3336100733361" overflow="visible">[[51](#bib.bib51)]</foreignobject></g></g><g
    transform="translate(-232.46,386.05)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="79.9778607997786" height="7.3336100733361"
    overflow="visible">[[90](#bib.bib90)]</foreignobject></g></g><g transform="translate(-89.94,386.05)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="107.651861076519"
    height="7.3336100733361" overflow="visible">[[136](#bib.bib136)]</foreignobject></g></g><g
    transform="translate(-586.69,290.58)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="101.425211014252" height="7.3336100733361"
    overflow="visible">[[57](#bib.bib57)]</foreignobject></g></g><g transform="translate(-445.55,290.58)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="78.3174207831742"
    height="7.3336100733361" overflow="visible">[[26](#bib.bib26)]</foreignobject></g></g><g
    transform="translate(-283.66,290.58)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="79.4243807942438" height="7.3336100733361"
    overflow="visible">[[25](#bib.bib25)]</foreignobject></g></g><g transform="translate(-83.02,290.58)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="120.658641206586"
    height="7.3336100733361" overflow="visible">[[27](#bib.bib27)]</foreignobject></g></g><g
    transform="translate(-560.4,131.45)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="105.299571052996" height="7.3336100733361"
    overflow="visible">[[28](#bib.bib28)]</foreignobject></g></g><g transform="translate(-496.75,141.14)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="73.1977307319773"
    height="7.3336100733361" overflow="visible">[[29](#bib.bib29)]</foreignobject></g></g><g
    transform="translate(-242.15,141.14)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="77.7639407776394" height="7.3336100733361"
    overflow="visible">[[101](#bib.bib101)]</foreignobject></g></g></g></svg>
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: <svg version="1.1" width="627.35" height="37.36" overflow="visible"><g transform="translate(0,37.36)
    scale(1,-1)"><g transform="translate(-654.49,381.9)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="112.218071122181" height="7.3336100733361"
    overflow="visible">[[24](#bib.bib24)]</foreignobject></g></g><g transform="translate(-470.46,373.6)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="68.3547806835478"
    height="7.3336100733361" overflow="visible">[[51](#bib.bib51)]</foreignobject></g></g><g
    transform="translate(-232.46,386.05)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="79.9778607997786" height="7.3336100733361"
    overflow="visible">[[90](#bib.bib90)]</foreignobject></g></g><g transform="translate(-89.94,386.05)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="107.651861076519"
    height="7.3336100733361" overflow="visible">[[136](#bib.bib136)]</foreignobject></g></g><g
    transform="translate(-586.69,290.58)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="101.425211014252" height="7.3336100733361"
    overflow="visible">[[57](#bib.bib57)]</foreignobject></g></g><g transform="translate(-445.55,290.58)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="78.3174207831742"
    height="7.3336100733361" overflow="visible">[[26](#bib.bib26)]</foreignobject></g></g><g
    transform="translate(-283.66,290.58)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="79.4243807942438" height="7.3336100733361"
    overflow="visible">[[25](#bib.bib25)]</foreignobject></g></g><g transform="translate(-83.02,290.58)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="120.658641206586"
    height="7.3336100733361" overflow="visible">[[27](#bib.bib27)]</foreignobject></g></g><g
    transform="translate(-560.4,131.45)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="105.299571052996" height="7.3336100733361"
    overflow="visible">[[28](#bib.bib28)]</foreignobject></g></g><g transform="translate(-496.75,141.14)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="73.1977307319773"
    height="7.3336100733361" overflow="visible">[[29](#bib.bib29)]</foreignobject></g></g><g
    transform="translate(-242.15,141.14)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="77.7639407776394" height="7.3336100733361"
    overflow="visible">[[101](#bib.bib101)]</foreignobject></g></g></g></svg>
- en: 'Figure 6: A brief overview of different pipelines used for gaze analysis tasks.
    Refer Sec. [4.2](#S4.SS2 "4.2 Representative Deep Network Architectures ‣ 4 Gaze
    Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches") for more details of the networks.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '图6：不同的注视分析任务所使用的管道的简要概述。有关网络的更多详细信息，请参见第[4.2](#S4.SS2 "4.2 Representative Deep
    Network Architectures ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A
    Survey of Deep Learning based Approaches")节。'
- en: 4.2 Representative Deep Network Architectures
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 代表性深度网络架构
- en: 'In this section, we provide a generic formulation and representation of gaze
    analysis. Given an RGB image $\mathbf{I}\in\mathbb{R}^{W\times H\times 3}$, a
    deep learning based model mapped it to task-specific label space. The input RGB
    image is usually the face or eye regions. Based on the primary network architectures
    adopted in the literature, we classify the models into the following categories:
    CNN based, Multi-Branch network based, Temporal based, Transformer Based and VAE/GAN
    based. An overview is shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and
    Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of
    Deep Learning based Approaches").'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们提供了注视分析的通用公式和表示。给定一个 RGB 图像 $\mathbf{I}\in\mathbb{R}^{W\times H\times
    3}$，一个基于深度学习的模型将其映射到特定任务的标签空间。输入的 RGB 图像通常是面部或眼部区域。根据文献中采用的主要网络架构，我们将模型分类为以下几类：基于
    CNN 的、基于多分支网络的、基于时间序列的、基于 Transformer 的和基于 VAE/GAN 的。概述见图 [6](#S4.F6 "Figure 6
    ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze
    Analysis: A Survey of Deep Learning based Approaches")。'
- en: 4.2.1 CNN based
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 基于 CNN 的
- en: Most of the recent solutions adopt a CNN based architecture [[24](#bib.bib24),
    [51](#bib.bib51), [50](#bib.bib50), [25](#bib.bib25), [26](#bib.bib26), [92](#bib.bib92)],
    which aims to learn end-to-end spatial representation followed by gaze prediction.
    The adopted model is often a modified version of the popular CNNs in vision (e.g.
    AlexNet [[27](#bib.bib27)], VGG [[54](#bib.bib54)], ResNet-18 [[90](#bib.bib90),
    [67](#bib.bib67)], ResNet-50[[94](#bib.bib94)], Capsule network [[27](#bib.bib27)]).
    These CNNs learn from a single-stream of RGB images (e.g. face, left or right
    eye patch) [[24](#bib.bib24), [51](#bib.bib51)], or multiple streams of information
    (e.g. face, and eye patches) [[57](#bib.bib57), [25](#bib.bib25)], and prior knowledge
    based on eye anatomy or geometrical constraints [[26](#bib.bib26)].
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的大多数解决方案采用了基于 CNN 的架构 [[24](#bib.bib24), [51](#bib.bib51), [50](#bib.bib50),
    [25](#bib.bib25), [26](#bib.bib26), [92](#bib.bib92)]，旨在学习端到端的空间表示，然后进行注视预测。采用的模型通常是视觉中流行的
    CNN 的修改版本（例如，AlexNet [[27](#bib.bib27)]、VGG [[54](#bib.bib54)]、ResNet-18 [[90](#bib.bib90),
    [67](#bib.bib67)]、ResNet-50 [[94](#bib.bib94)]、胶囊网络 [[27](#bib.bib27)]）。这些 CNN
    从单一流的 RGB 图像（例如，面部、左眼或右眼区域）[[24](#bib.bib24), [51](#bib.bib51)]，或多个信息流（例如，面部和眼部区域）[[57](#bib.bib57),
    [25](#bib.bib25)]，以及基于眼睛解剖学或几何约束的先验知识 [[26](#bib.bib26)] 中进行学习。
- en: 'GazeNet. It is the extended version of the first deep learning based gaze estimation
    method [[24](#bib.bib24)] which aims to capture low level and high level appearance
    feature by using convolution operation. GazeNet takes a grayscale eye patch image
    $\mathbf{I}\in\mathbb{R}^{W\times H}$ as input and maps it to angular gaze vector
    $\mathbf{g}\in\mathbb{R}^{2}$. As headpose provides relevant features for gaze
    direction, the headpose vector is also added in the FC layer for better inference
    (Refer top left image in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")). The Extended version [[50](#bib.bib50)] is adapted from the
    VGG network which further boosts the performance. To train these models, the sum
    of the individual $\ell_{2}$ losses between the predicted $\mathbf{\hat{g}}$ and
    actual gaze angle vectors $\mathbf{g}$ is considered.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 'GazeNet。它是第一个基于深度学习的注视估计方法的扩展版本 [[24](#bib.bib24)]，旨在通过使用卷积操作捕捉低级和高级外观特征。GazeNet
    以灰度眼部区域图像 $\mathbf{I}\in\mathbb{R}^{W\times H}$ 为输入，并将其映射到角度注视向量 $\mathbf{g}\in\mathbb{R}^{2}$。由于头部姿态提供了与注视方向相关的特征，因此头部姿态向量也被添加到全连接层中以进行更好的推断（参见图
    [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches) 左上图"）。扩展版本
    [[50](#bib.bib50)] 是从 VGG 网络改编而来的，进一步提升了性能。为了训练这些模型，考虑了预测的 $\mathbf{\hat{g}}$
    和实际注视角度向量 $\mathbf{g}$ 之间的 $\ell_{2}$ 损失的总和。'
- en: 'Spatial Weight CNN. It is a full face appearance based gaze estimation method [[51](#bib.bib51)]
    which uses a spatial weighting mechanism for encoding the important locations
    of the facial image $\mathbf{I}\in\mathbb{R}^{W\times H\times 3}$ via the standard
    CNN architecture (Refer top row second column image in Fig. [6](#S4.F6 "Figure
    6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")). This weighting mechanism
    (aka attention) automatically assign more weight to the regions contributing more
    towards gaze estimation. It includes three additional $1\times 1$ convolutional
    layers followed by a ReLU activation. Given a $N\times H\times W$ dimensional
    activation map ($U$) as input (where $N$, $H$ and $W$ are the number of feature
    channels, height and width of the output), the spatial weights module learns the
    weight matrix $W$ from element-wise multiplication of $W$ with the original activation
    $U$ via the following function: $W\bigodot U_{c},$ across the channel dimensions.
    Thus, the model learns to assign more weight to the specific regions, which in
    turn eliminates unwanted noise in the input. For 2-D gaze estimation, the $\ell_{1}$
    distance between the predicted and ground-truth gaze positions in the target screen
    coordinate system is utilized. Similarly, the $\ell_{1}$ distance between the
    predicted and ground-truth gaze angle vectors in the normalized space is used
    for 3-D gaze estimation.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 空间权重 CNN。这是一种基于全脸外观的注视估计方法[[51](#bib.bib51)]，通过标准 CNN 架构使用空间加权机制对面部图像 $\mathbf{I}\in\mathbb{R}^{W\times
    H\times 3}$ 的重要位置进行编码（参见图[6](#S4.F6 "图 6 ‣ 4.1 眼睛检测与分割 ‣ 4 注视分析框架 ‣ 自动注视分析：基于深度学习的方法调查")中的顶部行第二列图像）。该加权机制（又称为注意力）自动为对注视估计贡献更多的区域分配更多的权重。它包括三个额外的
    $1\times 1$ 卷积层，后接 ReLU 激活。给定一个 $N\times H\times W$ 维激活图（$U$）作为输入（其中 $N$、$H$ 和
    $W$ 是特征通道数、高度和宽度），空间权重模块通过以下函数从 $W$ 与原始激活 $U$ 的逐元素乘法中学习权重矩阵 $W$：$W\bigodot U_{c}$，跨通道维度。因此，模型学会为特定区域分配更多权重，从而消除输入中的不必要噪声。对于
    2-D 注视估计，利用预测注视位置与真实注视位置在目标屏幕坐标系中的 $\ell_{1}$ 距离。同样，对于 3-D 注视估计，使用预测注视角度向量与真实注视角度向量在归一化空间中的
    $\ell_{1}$ 距离。
- en: 'Dilated Convolution. Another interesting architecture for gaze estimation is
    dilated-convolutional layers which preserve spatial resolution while increasing
    the size of the receptive field without compromising the number of parameters [[64](#bib.bib64)].
    It aims to capture the slight change in pixels due to eye movement. Given an input
    feature map $U$ of kernel size $N\times M\times K$ ($N$: height, $M$:width, $K$:channel
    with weights $W$ and bias $b$) and dilation rates ($r_{1}$, $r_{2}$), the output
    feature map $v$ can be defined as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 扩张卷积。另一种用于注视估计的有趣架构是扩张卷积层，它在不妥协参数数量的情况下保持空间分辨率，同时增加感受野的大小[[64](#bib.bib64)]。它旨在捕捉由于眼睛运动而导致的像素微小变化。给定一个核大小为
    $N\times M\times K$ ($N$：高度，$M$：宽度，$K$：通道，带有权重 $W$ 和偏置 $b$) 的输入特征图 $U$ 和扩张率 ($r_{1}$,
    $r_{2}$)，输出特征图 $v$ 可以定义如下：
- en: '|  | $v(x,y)=\sum\limits_{k=1}^{K}\sum\limits_{m=0}^{M-1}\sum\limits_{n=0}^{N-1}u(x+nr_{1},y+mr_{2},k)w_{nmk}+b$
    |  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $v(x,y)=\sum\limits_{k=1}^{K}\sum\limits_{m=0}^{M-1}\sum\limits_{n=0}^{N-1}u(x+nr_{1},y+mr_{2},k)w_{nmk}+b$
    |  |'
- en: The dilated convolution is applied in facial and left/right eye patches before
    inferring the gaze. For training the network, cross entropy loss is used in the
    label space. Representation learning via MinENet [[144](#bib.bib144)] also relies
    on dilated and asymmetric convolutions to provide context to the segmented regions
    of the eye by increasing the receptive field capacity of the model to learn contextual
    information.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 扩张卷积在面部和左/右眼补丁中应用，然后再推断注视。对于网络训练，使用标签空间中的交叉熵损失。通过 MinENet [[144](#bib.bib144)]
    的表示学习也依赖于扩张和不对称卷积，通过增加模型的感受野容量来为眼睛的分段区域提供上下文信息。
- en: Bayesian CNN. Another variant of CNN is Bayesian CNN which is used for robust
    and generalizable eye-tracking under different conditions [[145](#bib.bib145)].
    Instead of predicting eye gaze using a single trained eye model, it performs eye
    tracking using an ensemble of models, hence alleviating the over-fitting problem,
    is more robust under insufficient data, and can generalize better across datasets.
    Compared to the point based eye landmark estimation methods, the BNN model can
    generalize better and it is also more robust under challenging real-world conditions.
    Additionally, the extended version of the BCNN (i.e. the single-stage model to
    multi-stage, yielding the cascade BCNN) allows feeding the uncertainty information
    from the current stage to the next stage to progressively improve the gaze estimation
    accuracy. This could be an interesting area for further study.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯卷积神经网络（Bayesian CNN）。另一种变体是贝叶斯卷积神经网络（Bayesian CNN），它用于在不同条件下进行鲁棒且可泛化的眼动追踪[[145](#bib.bib145)]。与使用单一训练眼部模型预测眼动不同，它使用模型集成进行眼动追踪，从而减轻了过拟合问题，在数据不足的情况下更具鲁棒性，并且可以在数据集之间更好地泛化。与基于点的眼部标志估计方法相比，BNN模型的泛化能力更强，在挑战性的实际条件下也更具鲁棒性。此外，BCNN的扩展版本（即单阶段模型到多阶段模型，形成级联BCNN）允许将当前阶段的不确定性信息传递到下一个阶段，从而逐步提高注视估计的准确性。这可能是一个有趣的进一步研究领域。
- en: 'Pictorial Gaze. Pictorial gaze [[26](#bib.bib26)] aims to model the relative
    position of eyeball and iris to get the gaze direction. The network [[26](#bib.bib26)]
    consists of two parts: 1) regression from eye patch image to intermediate gazemap
    followed by 2) regression from gazemap to gaze direction vector $g$ (Refer second
    row second column image in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")). The gazemap is an intermediate representation of a simple
    model of the human eyeball and iris in terms of $m\times n$ dimensional image,
    where, the projected eyeball diameter is $2r=1.2n$ and the iris centre coordinates
    ($u_{i},v_{i}$) are as follows: $u_{i}=\frac{m}{2}-r^{\prime}sin\ \phi\ cos\ \theta,v_{i}=\frac{n}{2}-r^{\prime}sin\
    \theta$ where, $r^{\prime}=r\ cos\ (sin^{-1}\frac{1}{2})$ and gaze direction $g=(\theta,\phi)$.
    Basically, the iris is an ellipse with major-axis diameter of r and minor-axis
    diameter of $r|cos\ \theta cos\ \phi|$. The first part is implemented via a stacked
    hourglass architecture which assumed to encode complex spatial relations including
    the locations of occluded key points. Consequently, for the second part, a DenseNet
    architecture is used which maps the intermediate gazemap to the gaze vector $\hat{g}$.
    It is trained via gaze direction regression loss defined as: $||g-\hat{g}||_{2}$
    as well as cross-entropy loss between predicted and ground-truth gazemaps for
    all pixels.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '图像注视（Pictorial Gaze）。图像注视[[26](#bib.bib26)]旨在建模眼球和虹膜的相对位置以获取注视方向。该网络[[26](#bib.bib26)]由两部分组成：1)
    从眼部图像到中间注视图的回归，其次是2) 从注视图到注视方向向量$g$的回归（参考图[6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection
    and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches")中的第二行第二列图像）。注视图是对人眼和虹膜的简单模型的中间表示，图像尺寸为$m\times
    n$，其中投影的眼球直径为$2r=1.2n$，虹膜中心坐标为($u_{i},v_{i}$)，如下：$u_{i}=\frac{m}{2}-r^{\prime}sin\
    \phi\ cos\ \theta,v_{i}=\frac{n}{2}-r^{\prime}sin\ \theta$，其中$r^{\prime}=r\ cos\
    (sin^{-1}\frac{1}{2})$，注视方向为$g=(\theta,\phi)$。基本上，虹膜是一个椭圆，其主轴直径为r，副轴直径为$r|cos\
    \theta cos\ \phi|$。第一部分通过一个堆叠的沙漏架构实现，该架构假设能够编码复杂的空间关系，包括遮挡关键点的位置。因此，对于第二部分，使用了DenseNet架构，该架构将中间注视图映射到注视向量$\hat{g}$。它通过定义为$||g-\hat{g}||_{2}$的注视方向回归损失以及预测的注视图和真实注视图之间的交叉熵损失进行训练。'
- en: 'Ize-Net. This framework is used for coarse to fine gaze representation learning.
    Here, the main idea is to learn coarse gaze representation by dividing the gaze
    locations to gaze zones. Further, the gaze zone is mapped to the finer gaze vector.
    The proposed network [[27](#bib.bib27)] (Refer second row right image in Fig. [6](#S4.F6
    "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")) is a combination
    of convolution and primary capsule layer. After the convolution layers, the primary
    capsule layer is appended whose job is to take the features learned by convolution
    layers and produce combinations of the features to consider face symmetry into
    account. This network is trained for coarse gaze zone which is fine-tuned for
    downstream 2-D/3-D gaze estimation.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ize-Net。该框架用于从粗到细的视线表示学习。其主要思想是通过将视线位置划分为视线区域来学习粗略的视线表示。此外，将视线区域映射到更精细的视线向量。所提出的网络
    [[27](#bib.bib27)]（参见图 [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches") 第二行右侧图像）是卷积层和初级胶囊层的组合。在卷积层之后，附加了初级胶囊层，其作用是将卷积层学到的特征组合起来，以考虑面部对称性。该网络为粗略视线区域进行训练，并在下游的
    2-D/3-D 视线估计中进行微调。'
- en: EyeNet. consists of modified residual units as the backbone, attention blocks
    and multi-scale supervision architecture. This network is robust for the low resolution,
    image blur, glint, illumination, off-angles, off-axis, reflection, glasses and
    different colour of iris region challenges.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: EyeNet。由修改过的残差单元、注意力块和多尺度监督结构组成。该网络在低分辨率、图像模糊、光斑、光照、角度偏差、偏轴、反射、眼镜和不同虹膜颜色等挑战下表现稳健。
- en: 4.2.2 Multi-Branch network based
  id: totrans-157
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 基于多分支网络
- en: There are several works [[54](#bib.bib54), [25](#bib.bib25), [57](#bib.bib57)]
    which utilize multiple inputs for better inference.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 有几个工作 [[54](#bib.bib54), [25](#bib.bib25), [57](#bib.bib57)] 使用多个输入以获得更好的推断结果。
- en: 'iTracker. The iTracker framework [[25](#bib.bib25)] takes the left eye, right
    eye, detected facial region and face location in the original frame as a binary
    mask (all of the size $224\times 224$) and predicts the distance from the camera
    (in cm). The model is jointly trained with Euclidean loss on the x and y gaze
    position. The overview of the framework is shown in the second row third column
    image in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze
    Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches").'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 'iTracker。iTracker 框架 [[25](#bib.bib25)] 将左眼、右眼、检测到的面部区域和原始帧中的面部位置作为二进制掩模（大小均为
    $224\times 224$），并预测与相机的距离（单位为厘米）。该模型与 x 和 y 视线位置的欧几里得损失共同训练。框架的概述如图 [6](#S4.F6
    "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches") 第二行第三列图像所示。'
- en: 'Multi-Branch Design. Similar to iTracker, Jyoti et al. [[57](#bib.bib57)] propose
    a framework which takes the full face, left and right eye, both eye patch as input
    for inferring gaze (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches") second-row left image). To train this network, mean squared
    error between the true and predicted gaze point/direction is used.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '多分支设计。类似于 iTracker，Jyoti 等人 [[57](#bib.bib57)] 提出了一个框架，该框架以全脸、左右眼以及两个眼部补丁作为输入来推断视线（参见图
    [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches") 第二行左侧图像）。训练该网络时，使用真实视线点/方向与预测视线点/方向之间的均方误差。'
- en: Two-Stream VGG Network. In [[54](#bib.bib54)], a two-stream VGG network is used
    for gaze inference while taking left and right eye patch as input. Similar to
    prior works, it utilizes the sum of the individual $\ell_{2}$ losses between the
    predicted and ground truth gaze vectors to train the ensemble network.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 双流 VGG 网络。在 [[54](#bib.bib54)] 中，使用双流 VGG 网络进行视线推断，同时以左右眼部补丁作为输入。与先前的工作类似，它利用预测和真实视线向量之间的
    $\ell_{2}$ 损失的总和来训练集成网络。
- en: 4.2.3 Temporal Gaze Modelling
  id: totrans-162
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 时间视线建模
- en: The human gaze is a continuous and dynamic process. While scanning the environment,
    the concerned subject performs eye movements in terms of fixations, saccades,
    smooth pursuit, vergence, and vestibulo-ocular movements. Moreover, a certain
    image frame in time has a high correlation with the gaze direction of previous
    time steps. Based on this line of reasoning, several works [[146](#bib.bib146),
    [90](#bib.bib90), [65](#bib.bib65), [66](#bib.bib66), [56](#bib.bib56), [147](#bib.bib147)]
    have leveraged temporal information and eye movement dynamics to enhance gaze
    estimation performance as compared to image based static methods. Given a sequence
    of frames, here the task is to estimate the gaze direction of the concerned person.
    For this modelling, popular recurrent neural network structures have been explored
    (e.g. GRU [[148](#bib.bib148)], LSTM/bi-LSTM [[90](#bib.bib90)]).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 人眼注视是一个连续且动态的过程。在扫描环境时，相关主体执行各种眼动，包括注视、扫视、平滑追踪、聚焦以及前庭眼动。此外，某一时刻的图像帧与之前时间步骤的注视方向具有高度相关性。基于这一推理，多项工作[[146](#bib.bib146),
    [90](#bib.bib90), [65](#bib.bib65), [66](#bib.bib66), [56](#bib.bib56), [147](#bib.bib147)]
    利用时间信息和眼动动态来提升注视估计性能，相比于基于图像的静态方法。给定一系列帧，任务是估计相关人员的注视方向。为此建模，探索了流行的递归神经网络结构（如
    GRU [[148](#bib.bib148)], LSTM/bi-LSTM [[90](#bib.bib90)]）。
- en: Multi-Modal Recurrent CNN. Palmero et al. [[56](#bib.bib56)] have proposed a
    multimodal recurrent CNN framework in which the learned static features of all
    the input frames of a given sequence are fed to a many-to-one recurrent module
    for predicting the 3D gaze direction of the last frame in the sequence. Their
    approach improves the state-of-the-art gaze estimation performance significantly
    (i.e. by 4% on EYEDIAP dataset).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态递归 CNN。Palmero 等人[[56](#bib.bib56)] 提出了一个多模态递归 CNN 框架，其中将所有输入帧的静态特征输入到一个多对一的递归模块，以预测序列中最后一帧的
    3D 注视方向。他们的方法显著提升了最先进的注视估计性能（即在 EYEDIAP 数据集上提高了 4%）。
- en: DGTN. Wang et al. [[66](#bib.bib66)] have proposed Dynamic Gaze Transition Network
    (DGTN) based on semi-Markov approach which models human eye movement dynamics.
    DGTN first computes per-frame gaze using a CNN which is further refined using
    the learned dynamic information.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: DGTN。Wang 等人[[66](#bib.bib66)] 提出了基于半马尔可夫方法的动态注视转移网络（DGTN），该方法建模了人眼运动动态。DGTN
    首先使用 CNN 计算每帧的注视，然后利用学习到的动态信息进一步优化。
- en: Improved iTracker + bi-LSTM. Bidirectional recurrent module based temporal modeling
    methods have been introduced in [[65](#bib.bib65)] which rely on both past and
    future frames. It is quite beneficial for low-to-mid resolution images and videos
    having a low frame rate ($\sim 30$ fps) despite its reduced applicability for
    real-time application as future frames are not usually available.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 改进版 iTracker + bi-LSTM。引入了双向递归模块基础的时间建模方法[[65](#bib.bib65)]，这些方法依赖于过去和未来的帧。尽管由于未来帧通常不可用，这些方法在实时应用中的适用性较低，但对于低到中等分辨率的图像和视频（帧率约为
    $\sim 30$ fps）却非常有益。
- en: 'Pinball LSTM. Similarly to encode contextual information along temporal domain,
    pinball LSTM [[90](#bib.bib90)] is proposed. This video based gaze estimation
    model using bidirectional LSTM considers 7-frame sequence to estimate the gaze
    direction of the central frame. Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection
    and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches") first row third column image illustrates the
    architecture of the model. The facial region from each frame is provided as input
    to the backbone CNN having ResNet-18 architecture. It maps input image to 256-dimensional
    feature space. Further, a two layer bidirectional LSTMs map these features to
    label space via a FC layer with an error quantile estimation i.e. ($\theta,\phi,\sigma$),
    where ($\theta,\phi$) is the predicted gaze direction in spherical coordinates
    corresponding to the ground-truth gaze vector in the eye coordinate system $g$
    as $\theta=-arctan\ \frac{g_{x}}{g_{z}}$ and $\phi=arcsin\ g_{y}$. On the other
    hand, $\sigma$ corresponds to the offset from the predicted gaze i.e. $\theta+\sigma$
    and $\phi+\sigma$ in the 90% quantiles of its distribution and $\theta-\sigma$
    and $\phi-\sigma$ are in the 10% quantiles. The pinball loss is computed as follows:
    given the ground truth label $y=(\theta_{gt},\phi_{gt})$, the loss $L_{\tau}$
    for the quantile $\tau$ and the angle $\alpha\in\{\theta,\phi\}$ can be written
    as:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '钢珠 LSTM。为了在时间域内编码上下文信息，提出了钢珠 LSTM [[90](#bib.bib90)]。这个基于视频的凝视估计模型使用双向 LSTM，考虑了
    7 帧序列来估计中央帧的凝视方向。图 [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣
    4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches") 第一行第三列的图像展示了模型的架构。每帧的面部区域作为输入提供给具有 ResNet-18 架构的骨干 CNN。它将输入图像映射到
    256 维特征空间。此外，两个层的双向 LSTM 将这些特征映射到标签空间，通过一个带有误差分位估计的全连接层，即 ($\theta,\phi,\sigma$)，其中
    ($\theta,\phi$) 是在球坐标系中预测的凝视方向，对应于眼睛坐标系中的真实凝视向量 $g$，表示为 $\theta=-arctan\ \frac{g_{x}}{g_{z}}$
    和 $\phi=arcsin\ g_{y}$。另一方面，$\sigma$ 对应于从预测凝视的偏移，即 $\theta+\sigma$ 和 $\phi+\sigma$
    在其分布的 90% 分位数中，而 $\theta-\sigma$ 和 $\phi-\sigma$ 则在 10% 分位数中。钢珠损失计算如下：给定真实标签 $y=(\theta_{gt},\phi_{gt})$，对于分位数
    $\tau$ 和角度 $\alpha\in\{\theta,\phi\}$，损失 $L_{\tau}$ 可以表示为：'
- en: '|  | $L_{\tau}(\alpha,\sigma,\alpha_{gt})=max(\tau\hat{q_{\tau}},-(1-\tau)\hat{q_{\tau}})$
    |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{\tau}(\alpha,\sigma,\alpha_{gt})=max(\tau\hat{q_{\tau}},-(1-\tau)\hat{q_{\tau}})$
    |  |'
- en: where, $\hat{q_{\tau}}=\alpha_{gt}-(\alpha-\sigma)$, for $\tau\leq 0.5$ and
    $\alpha_{gt}-(\alpha+\sigma)$ otherwise. This loss enforces $\theta$ and $\phi$
    to converge to their ground truth values.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\hat{q_{\tau}}=\alpha_{gt}-(\alpha-\sigma)$，对于 $\tau\leq 0.5$，否则为 $\alpha_{gt}-(\alpha+\sigma)$。该损失强制
    $\theta$ 和 $\phi$ 收敛到其真实值。
- en: Static + LSTM. In an interesting study, Palmero et al. [[146](#bib.bib146)]
    analyze the effect of sequential information for appearance-based gaze estimation
    using a static CNN network followed by a recurrent module to capture eye movement
    dynamics. The model is developed based on high-resolution eye-image sequences
    performing a stimulus-elicited fixation and saccade task in a VR scenario. The
    proposed model learns eye movement dynamics with accurate localization of gaze
    movement transition.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 静态 + LSTM。在一项有趣的研究中，Palmero 等人 [[146](#bib.bib146)] 分析了使用静态 CNN 网络结合递归模块来捕捉眼动动态的序列信息对基于外观的凝视估计的影响。该模型基于高分辨率眼部图像序列，执行在
    VR 场景中的刺激引发的注视和扫视任务。所提出的模型通过准确定位凝视运动过渡来学习眼动动态。
- en: Discussion. Despite the initial efforts that confirm the benefits of leveraging
    temporal information [[146](#bib.bib146), [90](#bib.bib90), [65](#bib.bib65),
    [66](#bib.bib66), [56](#bib.bib56)], there still have scope to explore eye dynamics
    in a task-driven real environment. Sometimes it is difficult to capture eye movement
    dynamics accurately using videos having low-resolution image frames with poor
    frame rates. Thus, it is still challenging how and why temporal information enhances
    gaze estimation performance for eye movement dynamics. Moreover, a deep understanding
    of eye movement patterns is required as the existing datasets are only based on
    task-based elicitation. It is also important to integrate the existing bio-mechanic
    eye models with data to achieve robust and data-efficient eye tracking. There
    are several open problems in this line of research in terms of eye movement dynamics
    (i.e. gaze directions, velocities, and gaze trajectories) in task-dependent as
    well as natural behaviors.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论。尽管最初的研究确认了利用时间信息的好处 [[146](#bib.bib146), [90](#bib.bib90), [65](#bib.bib65),
    [66](#bib.bib66), [56](#bib.bib56)]，但在任务驱动的实际环境中探索眼动学仍然有一定的空间。有时，使用分辨率低、帧率差的视频很难准确捕捉眼动的动态。因此，如何以及为什么时间信息能提升眼动动态的注视估计性能仍然是一个挑战。此外，需要对眼动模式有深入的理解，因为现有的数据集仅基于任务驱动的诱发。将现有的生物力学眼动模型与数据整合，以实现稳健且数据高效的眼动追踪也很重要。在任务相关和自然行为的眼动动态（即注视方向、速度和注视轨迹）方面，这一研究领域仍存在若干未解决的问题。
- en: 4.2.4 Transformer based
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 基于 Transformer
- en: Transformer models have recently gotten attention for their notable performance
    on a broad range of vision tasks. Similarly, in the gaze estimation domain, there
    are two types of transformers used to date which are designed on top of the ViT
    framework. The first one is the pure transformer in gaze estimation (GazeTR-Pure) [[149](#bib.bib149)]
    and the other one is hybrid transformer in gaze estimation (GazeTR-Hybrid) [[149](#bib.bib149)].
    GazeTR-Pure [[149](#bib.bib149)] takes the cropped face as input along with an
    extra token. The extra token is a learnable embedding which aggregates the image
    features together. On the other hand, GazeTR-Hybrid [[149](#bib.bib149)] is comprised
    of CNN and transformer. It is based on the fact that gaze estimation is a regression
    task and it is quite difficult to get the perception of gaze with only local patch
    based correlation. These models take advantage of the transformer’s attention
    mechanism to improve gaze estimation performance. These are initial explorations
    using the transformer backbone. There is an immense possibility to explore this
    architecture.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 模型最近因其在广泛视觉任务中的显著表现而受到关注。同样，在注视估计领域，迄今为止有两种类型的 Transformer 被用于 ViT
    框架之上。第一种是纯 Transformer 的注视估计（GazeTR-Pure）[[149](#bib.bib149)]，另一种是混合 Transformer
    的注视估计（GazeTR-Hybrid）[[149](#bib.bib149)]。GazeTR-Pure [[149](#bib.bib149)] 将裁剪后的面部图像作为输入，并加上一个额外的标记。这个额外的标记是一个可学习的嵌入，它将图像特征汇聚在一起。另一方面，GazeTR-Hybrid
    [[149](#bib.bib149)] 由 CNN 和 Transformer 组成。它基于这样的事实：注视估计是一个回归任务，仅通过局部补丁相关性很难获得注视的感知。这些模型利用了
    Transformer 的注意机制来提升注视估计性能。这些都是利用 Transformer 主干的初步探索，还有巨大的潜力去探索这种架构。
- en: 4.2.5 VAE/GAN based
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5 基于 VAE/GAN
- en: 'Variational autoencoders and GANs have been used for unsupervised or self-supervised
    representation learning (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and
    Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of
    Deep Learning based Approaches")). Here, the latent space feature of the autoencoder
    model is used for gaze estimation inference [[29](#bib.bib29), [28](#bib.bib28),
    [101](#bib.bib101)]. Apart from representation learning, VAE and GAN based models
    are widely used for gaze redirection tasks [[150](#bib.bib150), [101](#bib.bib101),
    [102](#bib.bib102), [100](#bib.bib100)].'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '变分自编码器和 GAN 已被用于无监督或自监督的表示学习（参见图 [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and
    Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of
    Deep Learning based Approaches")）。在这里，自编码器模型的潜在空间特征用于注视估计推断 [[29](#bib.bib29),
    [28](#bib.bib28), [101](#bib.bib101)]。除了表示学习之外，基于 VAE 和 GAN 的模型在注视重定向任务中也被广泛使用 [[150](#bib.bib150),
    [101](#bib.bib101), [102](#bib.bib102), [100](#bib.bib100)]。'
- en: 'DT-ED. For representation learning via gaze redirection in a person independent
    manner, variational autoencoders are often utilized [[29](#bib.bib29), [101](#bib.bib101)].
    Disentangling Transforming Encoder-Decoder (DT-ED) framework [[29](#bib.bib29)]
    takes an input image $x$ and maps it to the latent space $z$ via an encoder $E$
    (i.e. $E(x):x\to z$). In the latent space, DT-ED disentangles three important
    factors relevant to gaze, i.e. gaze direction ($z_{g}$), head orientation ($z_{h}$),
    and the appearance of the eye region ($z_{a}$). Thus, $z$ can be expressed as:
    $z=\{z_{a};z_{g};z_{h}\}$. The framework disentangles these factors by explicitly
    applying constraints related to gaze and headpose rotations. Further, a decoder
    $D$ maps $z$ back to the redirected image (i.e. $D(E(x)):z\to\hat{x}$). The gaze
    direction is estimated from the $z_{g}$ part of the latent embedding. The overall
    illustration is shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches") (bottom row middle image).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 'DT-ED。为了通过目光重定向进行无关人的表征学习，常常利用变分自编码器 [[29](#bib.bib29), [101](#bib.bib101)]。Disentangling
    Transforming Encoder-Decoder (DT-ED) 框架 [[29](#bib.bib29)] 接受输入图像 $x$，通过编码器 $E$（即
    $E(x):x\to z$）将其映射到潜在空间 $z$。在潜在空间中，DT-ED 解开了与目光相关的三个重要因素，即目光方向 ($z_{g}$)、头部方向
    ($z_{h}$) 和眼睛区域的外观 ($z_{a}$)。因此，$z$ 可以表示为：$z=\{z_{a};z_{g};z_{h}\}$。该框架通过明确应用与目光和头部姿态旋转相关的约束来解开这些因素。此外，解码器
    $D$ 将 $z$ 映射回重定向图像（即 $D(E(x)):z\to\hat{x}$）。目光方向是从潜在嵌入的 $z_{g}$ 部分估计的。整体示意图见图 [6](#S4.F6
    "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")（底行中图）。'
- en: 'ST-ED. Similarly, the Self-Transforming Encoder-Decoder (ST-ED) architecture [[101](#bib.bib101)]
    (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze
    Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches") bottom row right image) takes a pair of images $x_{i}$ and $x_{t}$
    as input, disentangles the subject’s personal non-varying embeddings ($z^{0}_{i}$
    and $z^{0}_{t}$), considers pseudo-label conditions ($\hat{c_{i}}$ and $\hat{c_{t}}$)
    and embedding representations ($z_{i}$ and $z_{t}$). The learning objective for
    transformation depends on the pseudo condition labels which consider extraneous
    factors in absence of ground truth annotation.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 'ST-ED。类似地，Self-Transforming Encoder-Decoder (ST-ED) 架构 [[101](#bib.bib101)]（参见图 [6](#S4.F6
    "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches") 底行右图）将一对图像 $x_{i}$
    和 $x_{t}$ 作为输入，解开主体的个人不变嵌入 ($z^{0}_{i}$ 和 $z^{0}_{t}$)，考虑伪标签条件 ($\hat{c_{i}}$
    和 $\hat{c_{t}}$) 和嵌入表示 ($z_{i}$ 和 $z_{t}$)。变换的学习目标依赖于伪条件标签，这些标签考虑了缺少真实注释的外部因素。'
- en: 'Gaze Redirection Network. The main motivation behind the unsupervised gaze
    redirection network [[28](#bib.bib28)] is capturing generic eye representation
    via gaze redirection (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection and
    Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of
    Deep Learning based Approaches") bottom row left image). The framework takes eye
    patch $I_{i}$ as input and predict the redirected eye patch as output $I_{o}$
    while preserving the difference in rotation $\Delta_{r}=r_{i}-r_{o}=G_{\phi}(I_{i})-G_{\phi}(I_{o})$.
    In this work, gaze redirection is used as a pretext task for representation learning.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '目光重定向网络。无监督目光重定向网络的主要动机 [[28](#bib.bib28)] 是通过目光重定向捕捉通用的眼睛表征（参见图 [6](#S4.F6
    "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches") 底行左图）。该框架接受眼部图像 $I_{i}$
    作为输入，并预测重定向后的眼部图像 $I_{o}$ 作为输出，同时保留旋转差异 $\Delta_{r}=r_{i}-r_{o}=G_{\phi}(I_{i})-G_{\phi}(I_{o})$。在这项工作中，目光重定向被用作表征学习的前置任务。'
- en: 'RITnet. RITnet [[136](#bib.bib136)] (Refer Fig. [6](#S4.F6 "Figure 6 ‣ 4.1
    Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches") top row right image) is a hybrid
    version of U-Net and DenseNet based upon Fully Convolutional Networks (FCN). To
    balance the trade-off between the performance and computational complexity, it
    consists of 5 Down-Blocks in the encoder and 4 Up-Blocks in the decoder where
    the last layer of the encoder block is termed as the bottleneck layer. Each Down-Block
    has 5 convolution layers with LeakyReLU activation and the layers share connections
    with previous layers similar to DenseNet architecture. Similarly, each Up-Block
    has 4 convolution layers with LeakyReLU activation. All Up-Blocks have skip connection
    with their corresponding Down-Block which is an effective strategy to learn representation.
    To train the model, the following loss functions are used: 1) Standard cross-entropy
    loss (CEL) is applied pixel-wise to categorize each pixel into four categories
    (i.e. background, iris, sclera, and pupil). 2) Generalized Dice Loss (GDL) penalize
    the pixels on the basis of the overlap between the ground truth pixel and corresponding
    prediction. 3) Boundary Aware Loss (BAL) weights each pixel in terms of distance
    with its two nearest neighbour. This loss helps to avoid CEL confusion in boundary
    region. 4) Surface Loss (SL) helps to recover small regions and contours via distance
    based scaling. The overall loss is defined as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 'RITnet。RITnet[[136](#bib.bib136)]（参见图 [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection
    and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches") 顶部右侧图像）是基于完全卷积网络（FCN）的U-Net和DenseNet的混合版本。为了平衡性能和计算复杂性之间的权衡，它由编码器中的5个下采样块和解码器中的4个上采样块组成，其中编码器块的最后一层被称为瓶颈层。每个下采样块具有5个带LeakyReLU激活的卷积层，并且这些层与之前的层共享连接，类似于DenseNet架构。同样，每个上采样块具有4个带LeakyReLU激活的卷积层。所有上采样块与对应的下采样块有跳跃连接，这是一种有效的表征学习策略。为了训练模型，使用了以下损失函数：1）标准交叉熵损失（CEL）在每个像素上应用，将每个像素分类为四个类别（即背景、虹膜、巩膜和瞳孔）。2）广义Dice损失（GDL）根据地面真实像素与相应预测之间的重叠惩罚像素。3）边界感知损失（BAL）根据距离加权每个像素与其两个最近邻的像素。该损失有助于避免CEL在边界区域的混淆。4）表面损失（SL）通过基于距离的缩放来帮助恢复小区域和轮廓。总体损失定义如下：'
- en: '|  | $\ell=\ell_{CEL}(\lambda_{1}+\lambda_{2}\ell_{BAL})+\lambda_{3}\ell_{GDL}+\lambda_{4}\ell_{SL}.$
    |  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\ell=\ell_{CEL}(\lambda_{1}+\lambda_{2}\ell_{BAL})+\lambda_{3}\ell_{GDL}+\lambda_{4}\ell_{SL}.$
    |  |'
- en: Similarly, another lightweight model [[151](#bib.bib151)] uses MobileNet with
    depth-wise separable convolution for efficiency. It also utilize a squeeze and
    excitation (SE) module for performance enhancement by modelling channel independence.
    Moreover, the heuristic filtering of the connected component is utilized to enforce
    biological coherence in the network. Few works [[152](#bib.bib152), [153](#bib.bib153)]
    also use multi-class classification strategy for rich representation learning.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，另一种轻量级模型[[151](#bib.bib151)]使用具有深度可分离卷积的MobileNet来提高效率。它还利用了一个挤压和激励（SE）模块，通过建模通道独立性来提升性能。此外，还利用了连接组件的启发式过滤，以确保网络的生物学一致性。一些工作[[152](#bib.bib152),
    [153](#bib.bib153)]也使用了多类别分类策略来进行丰富的表征学习。
- en: 'Other Statistical Modelling. The statistical inference based mapping is performed
    based on k-NN [[87](#bib.bib87)], support vector regression [[23](#bib.bib23),
    [87](#bib.bib87)] and random forest [[87](#bib.bib87), [20](#bib.bib20)]. A brief
    overview of these methods is summarised in Table [II](#S4.T2 "Table II ‣ 4.2.5
    VAE/GAN based ‣ 4.2 Representative Deep Network Architectures ‣ 4 Gaze Analysis
    Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches").
    Prior deep learning works on semantic eye segmentation is mainly focused on iris
    or sclera segmentation via Fuzzy C Means clustering, Otsu’s binarization, k-NN [[154](#bib.bib154)]
    etc. Sclera segmentation challenge was organized since 2015 to promote development
    in this area [[154](#bib.bib154), [133](#bib.bib133), [155](#bib.bib155)]. Recently,
    the OpenEDS challenge was organized in 2019 by Facebook Research in which eye
    segmentation was one of the sub-challenges. Most of the methods in this challenge
    use deep learning techniques [[136](#bib.bib136), [156](#bib.bib156), [144](#bib.bib144)].'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '其他统计建模。基于统计推断的映射是基于k-NN [[87](#bib.bib87)]、支持向量回归 [[23](#bib.bib23), [87](#bib.bib87)]
    和随机森林 [[87](#bib.bib87), [20](#bib.bib20)] 进行的。这些方法的简要概述总结在表 [II](#S4.T2 "Table
    II ‣ 4.2.5 VAE/GAN based ‣ 4.2 Representative Deep Network Architectures ‣ 4 Gaze
    Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches") 中。以往关于语义眼部分割的深度学习工作主要集中在通过模糊C均值聚类、Otsu二值化、k-NN [[154](#bib.bib154)]
    等进行虹膜或巩膜分割。自2015年以来组织了巩膜分割挑战，以促进该领域的发展 [[154](#bib.bib154), [133](#bib.bib133),
    [155](#bib.bib155)]。最近，Facebook Research于2019年组织了OpenEDS挑战，其中眼部分割是子挑战之一。该挑战中的大多数方法使用了深度学习技术
    [[136](#bib.bib136), [156](#bib.bib156), [144](#bib.bib144)]。'
- en: 'Table II: A comparison of gaze analysis methods with respect to registration
    (Reg.), representation (Represent.), Level of Supervision, Model, Prediction,
    validation on benchmark datasets (validation), Platforms (Plat.), Publication
    venue (Publ.) and year. Here, GV: Gaze Vector, Scr.: Screen, LOSO: Leave One Subject
    Out, LPIPS: Learned Perceptual Image Patch Similarity, MM: Morphable Model, RRF:
    Random Regression Forest, AEM: Anatomic Eye Model, GRN: Gaze Regression Network,
    ET: External Target, FV: Free Viewing, HH: HandHeld Device, HMD: Head Mounted
    Device, Seg.: Segmentation and GR: Gaze Redirection, LAEO: Looking At Each Other.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：关于注册（Reg.）、表示（Represent.）、监督级别、模型、预测、在基准数据集上的验证（validation）、平台（Plat.）、出版场所（Publ.）和年份的注视分析方法比较。这里，GV：注视向量，Scr.：屏幕，LOSO：留一人法，LPIPS：学习的感知图像块相似度，MM：可变形模型，RRF：随机回归森林，AEM：解剖眼模型，GRN：注视回归网络，ET：外部目标，FV：自由观看，HH：手持设备，HMD：头戴设备，Seg.：分割，GR：注视重定向，LAEO：彼此注视。
- en: '| Ref. | Reg. | Represent. |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 注册 | 表示 |'
- en: '&#124; Level of &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 级别 &#124;'
- en: '&#124; Sup. &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 监督 &#124;'
- en: '| Model | Prediction | Validation | Plat. | Publ. | Year |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 预测 | 验证 | 平台 | 出版 | 年份 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| [[23](#bib.bib23)] |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| [[23](#bib.bib23)] |'
- en: '&#124; Face[[157](#bib.bib157)] &#124;'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部[[157](#bib.bib157)] &#124;'
- en: '|'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '| Fully-Sup. | SVM |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 完全监督 | SVM |'
- en: '&#124; Gaze locking &#124;'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注视锁定 &#124;'
- en: '|'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[23](#bib.bib23)] &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[23](#bib.bib23)] &#124;'
- en: '| Scr. |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 屏幕 |'
- en: '&#124; UIST &#124;'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; UIST &#124;'
- en: '| 2013 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 2013 |'
- en: '| [[158](#bib.bib158)] | 3-D MM |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| [[158](#bib.bib158)] | 3-D MM |'
- en: '&#124; Appear. &#124;'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '| Convex Hull | 3-D GV |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 凸包 | 3-D GV |'
- en: '&#124;  [[158](#bib.bib158)] &#124;'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[158](#bib.bib158)] &#124;'
- en: '| ET. |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| ET. |'
- en: '&#124; ETRA &#124;'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ETRA &#124;'
- en: '| 2014 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 2014 |'
- en: '| [[20](#bib.bib20)] |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| [[20](#bib.bib20)] |'
- en: '&#124; Face, Eye [[20](#bib.bib20)] &#124;'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部，眼 [[20](#bib.bib20)] &#124;'
- en: '|'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RRF &#124;'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RRF &#124;'
- en: '| 3-D GV |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 3-D GV |'
- en: '&#124;  [[20](#bib.bib20)] &#124;'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[20](#bib.bib20)] &#124;'
- en: '| Any |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 任意 |'
- en: '&#124; CVPR &#124;'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2014 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 2014 |'
- en: '| [[159](#bib.bib159)] |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| [[159](#bib.bib159)] |'
- en: '&#124; Eye &#124;'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼 &#124;'
- en: '|'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CNN+CLNF &#124;'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN+CLNF &#124;'
- en: '|'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24)] &#124;'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[24](#bib.bib24)] &#124;'
- en: '| Any |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 任意 |'
- en: '&#124; ICCV &#124;'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICCV &#124;'
- en: '| 2015 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '| [[24](#bib.bib24)] |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| [[24](#bib.bib24)] |'
- en: '&#124; Face, L/R Eye &#124;'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部，L/R 眼 &#124;'
- en: '|'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CNN [[24](#bib.bib24)] &#124;'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN [[24](#bib.bib24)] &#124;'
- en: '|'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24)] &#124;'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[24](#bib.bib24)] &#124;'
- en: '| Scr. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 屏幕 |'
- en: '&#124; CVPR &#124;'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2015 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '| [[25](#bib.bib25)] |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| [[25](#bib.bib25)] |'
- en: '&#124; Face, L/R Eye &#124;'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Face, L/R Eye &#124;'
- en: '|'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; iTracker [[25](#bib.bib25)] &#124;'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; iTracker [[25](#bib.bib25)] &#124;'
- en: '|'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2-D Scr. &#124;'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2-D Scr. &#124;'
- en: '|'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[87](#bib.bib87), [25](#bib.bib25)] &#124;'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[87](#bib.bib87), [25](#bib.bib25)] &#124;'
- en: '| HH |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| HH |'
- en: '&#124; CVPR &#124;'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2016 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 2016 |'
- en: '| [[160](#bib.bib160)] |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| [[160](#bib.bib160)] |'
- en: '&#124; Eye &#124;'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Eye &#124;'
- en: '|'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CNN [[25](#bib.bib25)] &#124;'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN [[25](#bib.bib25)] &#124;'
- en: '|'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GR Img. &#124;'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GR Img. &#124;'
- en: '|'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[160](#bib.bib160)] &#124;'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[160](#bib.bib160)] &#124;'
- en: '| Any |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Any |'
- en: '&#124; ECCV &#124;'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ECCV &#124;'
- en: '| 2016 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 2016 |'
- en: '| [[87](#bib.bib87)] |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| [[87](#bib.bib87)] |'
- en: '&#124; Eye[[161](#bib.bib161)] &#124;'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Eye[[161](#bib.bib161)] &#124;'
- en: '|'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; SVR &#124;'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SVR &#124;'
- en: '| 2-D Scr. |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 2-D Scr. |'
- en: '&#124; [[87](#bib.bib87)] &#124;'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[87](#bib.bib87)] &#124;'
- en: '| HH |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| HH |'
- en: '&#124; MVA &#124;'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MVA &#124;'
- en: '| 2017 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 2017 |'
- en: '| [[54](#bib.bib54)] |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| [[54](#bib.bib54)] |'
- en: '&#124; Eye [[162](#bib.bib162)] &#124;'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Eye [[162](#bib.bib162)] &#124;'
- en: '|'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; VGG-16+FC [[54](#bib.bib54)] &#124;'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VGG-16+FC [[54](#bib.bib54)] &#124;'
- en: '|'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[24](#bib.bib24), [54](#bib.bib54)] &#124;'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[24](#bib.bib24), [54](#bib.bib54)] &#124;'
- en: '| Scr. |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; ECCV &#124;'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ECCV &#124;'
- en: '| 2018 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 2018 |'
- en: '| [[26](#bib.bib26)] |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| [[26](#bib.bib26)] |'
- en: '&#124; Eyes &#124;'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Eyes &#124;'
- en: '|'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CNN &#124;'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN &#124;'
- en: '|'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
- en: '| Scr. |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; ECCV &#124;'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ECCV &#124;'
- en: '| 2018 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| 2018 |'
- en: '| [[57](#bib.bib57)] |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| [[57](#bib.bib57)] |'
- en: '&#124; Face [[104](#bib.bib104)] &#124;'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Face [[104](#bib.bib104)] &#124;'
- en: '|'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Geo.+Appear. &#124;'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Geo.+Appear. &#124;'
- en: '|'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CNN [[57](#bib.bib57)] &#124;'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN [[57](#bib.bib57)] &#124;'
- en: '|'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[87](#bib.bib87), [23](#bib.bib23)] &#124;'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[87](#bib.bib87), [23](#bib.bib23)] &#124;'
- en: '| Desk. |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| Desk. |'
- en: '&#124; ICPR &#124;'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICPR &#124;'
- en: '| 2018 |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| 2018 |'
- en: '| [[92](#bib.bib92)] |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| [[92](#bib.bib92)] |'
- en: '&#124; Eye &#124;'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Eye &#124;'
- en: '|'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Geo.+Appear. &#124;'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Geo.+Appear. &#124;'
- en: '|'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; HGSM+c-BiGAN &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; HGSM+c-BiGAN &#124;'
- en: '|'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Eye, GV &#124;'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Eye, GV &#124;'
- en: '|'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
- en: '| Any |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Any |'
- en: '&#124; CVPR &#124;'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2018 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 2018 |'
- en: '| [[64](#bib.bib64)] |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| [[64](#bib.bib64)] |'
- en: '&#124; Face, L/R Eye &#124;'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Face, L/R Eye &#124;'
- en: '|'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dilated CNN &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Dilated CNN &#124;'
- en: '|'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24), [25](#bib.bib25), [23](#bib.bib23)] &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[24](#bib.bib24), [25](#bib.bib25), [23](#bib.bib23)] &#124;'
- en: '| Scr. |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; ACCV &#124;'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ACCV &#124;'
- en: '| 2018 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 2018 |'
- en: '| [[29](#bib.bib29)] |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| [[29](#bib.bib29)] |'
- en: '&#124; Face &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Face &#124;'
- en: '|'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Few-Shot &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Few-Shot &#124;'
- en: '|'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DT-ED+ML &#124;'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DT-ED+ML &#124;'
- en: '|'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24), [25](#bib.bib25)] &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[24](#bib.bib24), [25](#bib.bib25)] &#124;'
- en: '| Scr. |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; ICCV &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICCV &#124;'
- en: '| 2019 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[90](#bib.bib90)] |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| [[90](#bib.bib90)] |'
- en: '&#124; Face &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Face &#124;'
- en: '|'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pinball LSTM &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Pinball LSTM &#124;'
- en: '|'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24), [23](#bib.bib23), [87](#bib.bib87)] &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[24](#bib.bib24), [23](#bib.bib23), [87](#bib.bib87)] &#124;'
- en: '| ET |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| ET |'
- en: '&#124; ICCV &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICCV &#124;'
- en: '| 2019 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '|'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[32](#bib.bib32)] &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[32](#bib.bib32)] &#124;'
- en: '| Eye | Appear. |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| Eye | Appear. |'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '| SegNet [[163](#bib.bib163)] | Seg. Map |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| SegNet [[163](#bib.bib163)] | Seg. Map |'
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[32](#bib.bib32)] &#124;'
- en: '| HMD |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| HMD |'
- en: '&#124; ICCVW &#124;'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICCVW &#124;'
- en: '| 2019 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[66](#bib.bib66)] |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| [[66](#bib.bib66)] |'
- en: '&#124; Face, L/R Eye &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Face, L/R Eye &#124;'
- en: '|'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DGTN &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DGTN &#124;'
- en: '|'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GV &#124;'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GV &#124;'
- en: '|'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[66](#bib.bib66)] &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[66](#bib.bib66)] &#124;'
- en: '| Desk. |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| Desk. |'
- en: '&#124; CVPR &#124;'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2019 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[164](#bib.bib164)] |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| [[164](#bib.bib164)] |'
- en: '&#124; Face &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Face &#124;'
- en: '|'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Appear. &#124;'
- en: '|'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fully-Sup. &#124;'
- en: '|'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MeNet &#124;'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MeNet &#124;'
- en: '|'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24), [20](#bib.bib20), [25](#bib.bib25)] &#124;'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[24](#bib.bib24), [20](#bib.bib20), [25](#bib.bib25)] &#124;'
- en: '| Scr. |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| 屏幕 |'
- en: '&#124; CVPR &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2019 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[145](#bib.bib145)] |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| [[145](#bib.bib145)] |'
- en: '&#124; Face, Eye &#124;'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 脸部, 眼睛 &#124;'
- en: '|'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Semi/Unsup. &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 半监督/无监督 &#124;'
- en: '|'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; BCNN &#124;'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BCNN &#124;'
- en: '|'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
- en: '| Desk. |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| 桌面 |'
- en: '&#124; CVPR &#124;'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2019 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[136](#bib.bib136)] |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| [[136](#bib.bib136)] |'
- en: '&#124; Eyes &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hybrid U-net &#124;'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 混合 U-net &#124;'
- en: '|'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Seg. Map &#124;'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割图 &#124;'
- en: '|'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[32](#bib.bib32)] &#124;'
- en: '| HMD |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '| HMD |'
- en: '&#124; ICCVW &#124;'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICCVW &#124;'
- en: '| 2019 |'
  id: totrans-468
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[165](#bib.bib165)] |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '| [[165](#bib.bib165)] |'
- en: '&#124; Eyes &#124;'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Modified Resnet &#124;'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 修改版 Resnet &#124;'
- en: '|'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Seg. Map &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割图 &#124;'
- en: '|'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[32](#bib.bib32)] &#124;'
- en: '| HMD |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| HMD |'
- en: '&#124; ICCVW &#124;'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICCVW &#124;'
- en: '| 2019 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[156](#bib.bib156)] |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| [[156](#bib.bib156)] |'
- en: '&#124; Eyes &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Eye-MMS &#124;'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Eye-MMS &#124;'
- en: '|'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Seg. Map &#124;'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割图 &#124;'
- en: '|'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[32](#bib.bib32)] &#124;'
- en: '| HMD |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| HMD |'
- en: '&#124; ICCVW &#124;'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICCVW &#124;'
- en: '| 2019 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[144](#bib.bib144)] |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| [[144](#bib.bib144)] |'
- en: '&#124; Eyes &#124;'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Dilated CNN &#124;'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 膨胀 CNN &#124;'
- en: '|'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Seg. Map &#124;'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割图 &#124;'
- en: '|'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[32](#bib.bib32)] &#124;'
- en: '| HMD |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| HMD |'
- en: '&#124; ICCVW &#124;'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICCVW &#124;'
- en: '| 2019 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[91](#bib.bib91)] |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| [[91](#bib.bib91)] |'
- en: '&#124; Eyes &#124;'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear.+Seg. &#124;'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观+分割 &#124;'
- en: '|'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Few-shot &#124;'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 少样本 &#124;'
- en: '|'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GR &#124;'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GR &#124;'
- en: '|'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2-D GV &#124;'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2-D GV &#124;'
- en: '|'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[24](#bib.bib24), [23](#bib.bib23)] &#124;'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[24](#bib.bib24), [23](#bib.bib23)] &#124;'
- en: '| Any |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 任何 |'
- en: '&#124; CVPR &#124;'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2019 |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[28](#bib.bib28)] |'
  id: totrans-529
  prefs: []
  type: TYPE_TB
  zh: '| [[28](#bib.bib28)] |'
- en: '&#124; Eyes &#124;'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Unsup. &#124;'
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无监督 &#124;'
- en: '|'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GR &#124;'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GR &#124;'
- en: '|'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2-D GV &#124;'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2-D GV &#124;'
- en: '|'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[24](#bib.bib24), [23](#bib.bib23)] &#124;'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[24](#bib.bib24), [23](#bib.bib23)] &#124;'
- en: '| Any |'
  id: totrans-541
  prefs: []
  type: TYPE_TB
  zh: '| 任何 |'
- en: '&#124; CVPR &#124;'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2019 |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[27](#bib.bib27)] |'
  id: totrans-544
  prefs: []
  type: TYPE_TB
  zh: '| [[27](#bib.bib27)] |'
- en: '&#124; Face, Eye &#124;'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 脸部, 眼睛 &#124;'
- en: '|'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Unsup. &#124;'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无监督 &#124;'
- en: '|'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; IzeNet &#124;'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IzeNet &#124;'
- en: '|'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 3-D GV &#124;'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D GV &#124;'
- en: '|'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[23](#bib.bib23), [87](#bib.bib87)] &#124;'
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[23](#bib.bib23), [87](#bib.bib87)] &#124;'
- en: '| FV |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '| FV |'
- en: '&#124; IJCNN &#124;'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; IJCNN &#124;'
- en: '| 2019 |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[166](#bib.bib166)] |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '| [[166](#bib.bib166)] |'
- en: '&#124; Eye &#124;'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear.+Seg. &#124;'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观+分割 &#124;'
- en: '|'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Seg2Eye &#124;'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Seg2Eye &#124;'
- en: '|'
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Eye Img. &#124;'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛图像 &#124;'
- en: '|'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[32](#bib.bib32)] &#124;'
- en: '| HMD |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| HMD |'
- en: '&#124; ICCVW &#124;'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ICCVW &#124;'
- en: '| 2019 |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[167](#bib.bib167)] |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| [[167](#bib.bib167)] |'
- en: '&#124; Eye Seq. &#124;'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛序列 &#124;'
- en: '|'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Unsup. &#124;'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无监督 &#124;'
- en: '|'
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hier. HMM &#124;'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分层 HMM &#124;'
- en: '|'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Eye Move. &#124;'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛运动 &#124;'
- en: '|'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[168](#bib.bib168)] &#124;'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[168](#bib.bib168)] &#124;'
- en: '| Any |'
  id: totrans-586
  prefs: []
  type: TYPE_TB
  zh: '| 任何 |'
- en: '&#124; ECCVW &#124;'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ECCVW &#124;'
- en: '| 2019 |'
  id: totrans-588
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[169](#bib.bib169)] |'
  id: totrans-589
  prefs: []
  type: TYPE_TB
  zh: '| [[169](#bib.bib169)] |'
- en: '&#124; Eye &#124;'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Semi/Unsup. &#124;'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 半监督/无监督 &#124;'
- en: '|'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; mSegNet+Discre. &#124;'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; mSegNet+Discre. &#124;'
- en: '|'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Seg. Map &#124;'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割图 &#124;'
- en: '|'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[32](#bib.bib32)] &#124;'
- en: '| HMD |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '| HMD |'
- en: '&#124; ECCVW &#124;'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ECCVW &#124;'
- en: '| 2019 |'
  id: totrans-603
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[170](#bib.bib170)] |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '| [[170](#bib.bib170)] |'
- en: '&#124; Eye &#124;'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Few-Shot &#124;'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 少样本 &#124;'
- en: '|'
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; EyeSeg &#124;'
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EyeSeg &#124;'
- en: '|'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Seg. Map &#124;'
  id: totrans-613
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割图 &#124;'
- en: '|'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[32](#bib.bib32)] &#124;'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[32](#bib.bib32)] &#124;'
- en: '| HMD |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| HMD |'
- en: '&#124; ECCVW &#124;'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ECCVW &#124;'
- en: '| 2019 |'
  id: totrans-618
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [[101](#bib.bib101)] |'
  id: totrans-619
  prefs: []
  type: TYPE_TB
  zh: '| [[101](#bib.bib101)] |'
- en: '&#124; Face &#124;'
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 脸部 &#124;'
- en: '|'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 外观 &#124;'
- en: '|'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ST-ED &#124;'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ST-ED &#124;'
- en: '|'
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GR &#124;'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GR &#124;'
- en: '|'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[25](#bib.bib25), [23](#bib.bib23), [158](#bib.bib158)] &#124;'
  id: totrans-630
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[25](#bib.bib25), [23](#bib.bib23), [158](#bib.bib158)] &#124;'
- en: '| Scr. |'
  id: totrans-631
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; NeurIPS &#124;'
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; NeurIPS &#124;'
- en: '| 2020 |'
  id: totrans-633
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '|'
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[34](#bib.bib34)] &#124;'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[34](#bib.bib34)] &#124;'
- en: '| Eye | Appear. |'
  id: totrans-636
  prefs: []
  type: TYPE_TB
  zh: '| 眼睛 | 出现 |'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '| Modified ResNet | GR Img. |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '| 修改版 ResNet | GR 图像 |'
- en: '&#124; [[34](#bib.bib34)] &#124;'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[34](#bib.bib34)] &#124;'
- en: '| HMD |'
  id: totrans-640
  prefs: []
  type: TYPE_TB
  zh: '| HMD |'
- en: '&#124; ECCVW &#124;'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ECCVW &#124;'
- en: '| 2020 |'
  id: totrans-642
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [[148](#bib.bib148)] |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '| [[148](#bib.bib148)] |'
- en: '&#124; Eyes &#124;'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ResNet-18+GRU &#124;'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-18+GRU &#124;'
- en: '|'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PoG,3-D GV &#124;'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PoG,3-D GV &#124;'
- en: '|'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[148](#bib.bib148)] &#124;'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[148](#bib.bib148)] &#124;'
- en: '| Scr. |'
  id: totrans-655
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; ECCV &#124;'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ECCV &#124;'
- en: '| 2020 |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [[94](#bib.bib94)] |'
  id: totrans-658
  prefs: []
  type: TYPE_TB
  zh: '| [[94](#bib.bib94)] |'
- en: '&#124; Face &#124;'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部 &#124;'
- en: '|'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '| ResNet-50 | 3-D GV |'
  id: totrans-664
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 | 3-D GV |'
- en: '&#124;  [[24](#bib.bib24), [25](#bib.bib25), [90](#bib.bib90), [158](#bib.bib158)]
    &#124;'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[24](#bib.bib24), [25](#bib.bib25), [90](#bib.bib90), [158](#bib.bib158)]
    &#124;'
- en: '| Scr. |'
  id: totrans-666
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; ECCV &#124;'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ECCV &#124;'
- en: '| 2020 |'
  id: totrans-668
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [[171](#bib.bib171)] |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| [[171](#bib.bib171)] |'
- en: '&#124; Face &#124;'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部 &#124;'
- en: '|'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Semi-Sup. &#124;'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 半监督 &#124;'
- en: '|'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GRN &#124;'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GRN &#124;'
- en: '|'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GV &#124;'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GV &#124;'
- en: '|'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[113](#bib.bib113)] &#124;'
  id: totrans-680
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[113](#bib.bib113)] &#124;'
- en: '| FV |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '| FV |'
- en: '&#124; WACV &#124;'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WACV &#124;'
- en: '| 2020 |'
  id: totrans-683
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [[60](#bib.bib60)] |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '| [[60](#bib.bib60)] |'
- en: '&#124; Face, Eye &#124;'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部, 眼睛 &#124;'
- en: '|'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; RSN+GazeNet &#124;'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RSN+GazeNet &#124;'
- en: '|'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GV &#124;'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GV &#124;'
- en: '|'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[158](#bib.bib158), [24](#bib.bib24), [25](#bib.bib25)] &#124;'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[158](#bib.bib158), [24](#bib.bib24), [25](#bib.bib25)] &#124;'
- en: '| Scr. |'
  id: totrans-696
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; BMVC &#124;'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; BMVC &#124;'
- en: '| 2020 |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [[59](#bib.bib59)] |'
  id: totrans-699
  prefs: []
  type: TYPE_TB
  zh: '| [[59](#bib.bib59)] |'
- en: '&#124; Face, Eye &#124;'
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部, 眼睛 &#124;'
- en: '|'
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CA-Net &#124;'
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CA-Net &#124;'
- en: '|'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GV &#124;'
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GV &#124;'
- en: '|'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
  id: totrans-710
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[24](#bib.bib24), [158](#bib.bib158)] &#124;'
- en: '| Scr. |'
  id: totrans-711
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; AAAI &#124;'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AAAI &#124;'
- en: '| 2020 |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [[172](#bib.bib172)] |'
  id: totrans-714
  prefs: []
  type: TYPE_TB
  zh: '| [[172](#bib.bib172)] |'
- en: '&#124; Face, Eye &#124;'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部, 眼睛 &#124;'
- en: '|'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-717
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; FAR-Net &#124;'
  id: totrans-721
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; FAR-Net &#124;'
- en: '|'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GV &#124;'
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GV &#124;'
- en: '|'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [[158](#bib.bib158), [24](#bib.bib24), [54](#bib.bib54)] &#124;'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[158](#bib.bib158), [24](#bib.bib24), [54](#bib.bib54)] &#124;'
- en: '| Scr. |'
  id: totrans-726
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; TIP &#124;'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TIP &#124;'
- en: '| 2020 |'
  id: totrans-728
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [[102](#bib.bib102)] |'
  id: totrans-729
  prefs: []
  type: TYPE_TB
  zh: '| [[102](#bib.bib102)] |'
- en: '&#124; Eye &#124;'
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛 &#124;'
- en: '|'
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear.+AEM &#124;'
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现+AEM &#124;'
- en: '|'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MT c-GAN &#124;'
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MT c-GAN &#124;'
- en: '|'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Eye Img. &#124;'
  id: totrans-738
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛图像 &#124;'
- en: '|'
  id: totrans-739
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[24](#bib.bib24), [23](#bib.bib23), [20](#bib.bib20)] &#124;'
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[24](#bib.bib24), [23](#bib.bib23), [20](#bib.bib20)] &#124;'
- en: '| Scr. |'
  id: totrans-741
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; WACV &#124;'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WACV &#124;'
- en: '| 2021 |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
  zh: '| 2021 |'
- en: '| [[89](#bib.bib89)] |'
  id: totrans-744
  prefs: []
  type: TYPE_TB
  zh: '| [[89](#bib.bib89)] |'
- en: '&#124; Face, Eye &#124;'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部, 眼睛 &#124;'
- en: '|'
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; AFF-Net &#124;'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; AFF-Net &#124;'
- en: '|'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Scr., GV &#124;'
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Scr., GV &#124;'
- en: '|'
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[25](#bib.bib25), [51](#bib.bib51)] &#124;'
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[25](#bib.bib25), [51](#bib.bib51)] &#124;'
- en: '| Scr. |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; Arxiv &#124;'
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Arxiv &#124;'
- en: '| 2021 |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '| 2021 |'
- en: '| [[173](#bib.bib173)] |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
  zh: '| [[173](#bib.bib173)] |'
- en: '&#124; Face &#124;'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部 &#124;'
- en: '|'
  id: totrans-761
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Unsup. &#124;'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无监督 &#124;'
- en: '|'
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; PureGaze &#124;'
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; PureGaze &#124;'
- en: '|'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Face, GV &#124;'
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部, GV &#124;'
- en: '|'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[94](#bib.bib94), [90](#bib.bib90), [24](#bib.bib24), [20](#bib.bib20)]
    &#124;'
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[94](#bib.bib94), [90](#bib.bib90), [24](#bib.bib24), [20](#bib.bib20)]
    &#124;'
- en: '| Scr. |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
  zh: '| Scr. |'
- en: '&#124; Arxiv &#124;'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Arxiv &#124;'
- en: '| 2021 |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
  zh: '| 2021 |'
- en: '| [[110](#bib.bib110)] |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
  zh: '| [[110](#bib.bib110)] |'
- en: '&#124; Face &#124;'
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部 &#124;'
- en: '|'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Weakly-Sup. &#124;'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 弱监督 &#124;'
- en: '|'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ResNet-18+LSTM &#124;'
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-18+LSTM &#124;'
- en: '|'
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GV &#124;'
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GV &#124;'
- en: '|'
  id: totrans-784
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[94](#bib.bib94), [90](#bib.bib90), [110](#bib.bib110), [25](#bib.bib25)]
    &#124;'
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[94](#bib.bib94), [90](#bib.bib90), [110](#bib.bib110), [25](#bib.bib25)]
    &#124;'
- en: '| Any |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
  zh: '| 任意 |'
- en: '&#124; CVPR &#124;'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CVPR &#124;'
- en: '| 2021 |'
  id: totrans-788
  prefs: []
  type: TYPE_TB
  zh: '| 2021 |'
- en: '| [[109](#bib.bib109)] |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
  zh: '| [[109](#bib.bib109)] |'
- en: '&#124; Face &#124;'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部 &#124;'
- en: '|'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-793
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully-Sup. &#124;'
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全监督 &#124;'
- en: '|'
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LAEO-Net++ &#124;'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LAEO-Net++ &#124;'
- en: '|'
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; LAEO &#124;'
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LAEO &#124;'
- en: '|'
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[108](#bib.bib108)] &#124;'
  id: totrans-800
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[108](#bib.bib108)] &#124;'
- en: '| Any |'
  id: totrans-801
  prefs: []
  type: TYPE_TB
  zh: '| 任意 |'
- en: '&#124; TPAMI &#124;'
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; TPAMI &#124;'
- en: '| 2021 |'
  id: totrans-803
  prefs: []
  type: TYPE_TB
  zh: '| 2021 |'
- en: '| [[174](#bib.bib174)] |'
  id: totrans-804
  prefs: []
  type: TYPE_TB
  zh: '| [[174](#bib.bib174)] |'
- en: '&#124; Face, Eye &#124;'
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 面部, 眼睛 &#124;'
- en: '|'
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Appear. &#124;'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 出现 &#124;'
- en: '|'
  id: totrans-808
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Limited-Sup. &#124;'
  id: totrans-809
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 有限监督 &#124;'
- en: '|'
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ResNet-50 &#124;'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-50 &#124;'
- en: '|'
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GV &#124;'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GV &#124;'
- en: '|'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[23](#bib.bib23), [24](#bib.bib24), [90](#bib.bib90), [40](#bib.bib40)]
    &#124;'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[23](#bib.bib23), [24](#bib.bib24), [90](#bib.bib90), [40](#bib.bib40)]
    &#124;'
- en: '| Any |'
  id: totrans-816
  prefs: []
  type: TYPE_TB
  zh: '| 任意 |'
- en: '&#124; WACV &#124;'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; WACV &#124;'
- en: '| 2022 |'
  id: totrans-818
  prefs: []
  type: TYPE_TB
  zh: '| 2022 |'
- en: 4.2.6 Discussion
  id: totrans-819
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.6 讨论
- en: 'In an attempt to summarize the recent deep network based gaze analysis methods,
    we present some main take away points as follows:'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结最近基于深度网络的注视分析方法，我们提出了一些主要的要点如下：
- en: •
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The overall gaze estimation methods are divided into two broad categories:
    1) 2-D Gaze Estimation: In this context the proposed methods map the input image
    to 2-D Point of Regard (PoR) in the visual plane. The visual planes could either
    be the observable object or screen. Non deep learning methods or early deep learning
    methods [[21](#bib.bib21), [24](#bib.bib24), [50](#bib.bib50), [51](#bib.bib51)]
    perform these mappings. 2) 3-D Gaze Estimation: The 3-D gaze estimation basically
    considers the gaze vector instead of 2-D PoR. The gaze vector is the line joining
    the pupil center point with the point of regard. Recent works [[94](#bib.bib94),
    [26](#bib.bib26), [29](#bib.bib29), [148](#bib.bib148), [110](#bib.bib110)] mainly
    relies on 3-D gaze estimation methods. The choice of gaze estimation methods rely
    on the application and requirement.'
  id: totrans-822
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 总体注视估计方法分为两大类：1) 2-D 注视估计：在这种情况下，提出的方法将输入图像映射到视觉平面上的 2-D 注视点（PoR）。视觉平面可以是可观察的对象或屏幕。非深度学习方法或早期深度学习方法
    [[21](#bib.bib21), [24](#bib.bib24), [50](#bib.bib50), [51](#bib.bib51)] 执行这些映射。2)
    3-D 注视估计：3-D 注视估计基本上考虑注视向量而不是 2-D PoR。注视向量是连接瞳孔中心点与注视点的直线。近期的工作 [[94](#bib.bib94),
    [26](#bib.bib26), [29](#bib.bib29), [148](#bib.bib148), [110](#bib.bib110)] 主要依赖于
    3-D 注视估计方法。注视估计方法的选择依赖于应用和需求。
- en: •
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Single branch CNN based architectures [[24](#bib.bib24), [51](#bib.bib51), [50](#bib.bib50),
    [25](#bib.bib25), [26](#bib.bib26), [92](#bib.bib92)] are widely used over the
    past few years for progressive improvements on benchmark datasets. The input to
    these networks are restricted to single eye, eye patch or face. Thus, to further
    boost the performance, multi branch networks are proposed which utilize eyes,
    face, geometric constraints, visual plane grid as input.
  id: totrans-824
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于单支路 CNN 的架构 [[24](#bib.bib24), [51](#bib.bib51), [50](#bib.bib50), [25](#bib.bib25),
    [26](#bib.bib26), [92](#bib.bib92)] 在过去几年中被广泛使用，以在基准数据集上进行渐进式改进。这些网络的输入限制为单只眼睛、眼罩或面部。因此，为了进一步提升性能，提出了多支路网络，这些网络利用眼睛、面部、几何约束、视觉平面网格作为输入。
- en: •
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Both single or multi branch networks depend on spatial information. However,
    eye movement is dynamic in nature. Thus, few recent proposed architectures [[90](#bib.bib90),
    [92](#bib.bib92)] use temporal information for inference.
  id: totrans-826
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单支路或多支路网络都依赖于空间信息。然而，眼动具有动态特性。因此，少数最近提出的架构 [[90](#bib.bib90), [92](#bib.bib92)]
    使用时间信息进行推断。
- en: •
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: For representation learning, VAE and GAN based architectures [[29](#bib.bib29),
    [28](#bib.bib28), [101](#bib.bib101)] are explored. However, it is observed that
    these architectures could have high time complexity as compared to single or multi
    branch CNN.
  id: totrans-828
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于表示学习，基于 VAE 和 GAN 的架构 [[29](#bib.bib29), [28](#bib.bib28), [101](#bib.bib101)]
    已被探索。然而，观察到这些架构相较于单支路或多支路 CNN 可能具有较高的时间复杂度。
- en: •
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Prior based appearance encoding is another line of approaches for encoding
    rich feature representation. Few works have defined priors based on eye anatomy [[26](#bib.bib26)],
    geometrical constraint [[172](#bib.bib172)] as biases for better generalization.
    Despite direct appearance encoding, Park et al. [[26](#bib.bib26)] proposed an
    intermediate pictorial representation, termed a ‘gazemap’ (refer Fig. [6](#S4.F6
    "Figure 6 ‣ 4.1 Eye Detection and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")) of the eye to simplify
    the gaze estimation task. Similarly, the ‘two eye asymmetry’ property is utilized
    for gaze estimation [[172](#bib.bib172)] where the underlying hypothesis is that
    despite the difference in appearances of two eyes due to environmental factors,
    the gaze directions remains approximately the same. The CNN based regression model
    is assumed to be independent of identity distribution, however, due to the subject-specific
    offset of the nodal point of the eyes, gaze datasets have identity specific bias.
    Xiong et al. [[164](#bib.bib164)] inject this bias as a prior by mixing different
    models. Similarly, to handle this offset, the gaze is decomposed into the subject
    independent and dependent bias for performance enhancement and better generalization [[175](#bib.bib175)].'
  id: totrans-830
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '基于先验的外观编码是编码丰富特征表示的另一种方法。少数工作基于眼睛解剖结构[[26](#bib.bib26)]、几何约束[[172](#bib.bib172)]定义了先验作为改进泛化的偏差。尽管直接的外观编码，Park
    等人[[26](#bib.bib26)] 提出了一个中间图示表示，称为“注视图”（参见图 [6](#S4.F6 "Figure 6 ‣ 4.1 Eye Detection
    and Segmentation ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches)）来简化注视估计任务。类似地，‘双眼不对称’属性被用于注视估计[[172](#bib.bib172)]，其基本假设是尽管由于环境因素两个眼睛的外观有所不同，注视方向仍大致相同。基于
    CNN 的回归模型被假定为与身份分布无关，然而，由于眼睛的节点点的主体特定偏差，注视数据集存在身份特定的偏差。Xiong 等人[[164](#bib.bib164)]
    通过混合不同的模型将这种偏差注入为先验。类似地，为了处理这种偏差，注视被分解为主体无关和依赖偏差，以提高性能和更好的泛化[[175](#bib.bib175)]。'
- en: •
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: In order to train the deep learning based models, $\ell_{2}$ [[24](#bib.bib24),
    [51](#bib.bib51), [50](#bib.bib50)] and cosine similarity based losses [[94](#bib.bib94),
    [148](#bib.bib148)] are used. However, a novel pinball loss [[90](#bib.bib90)]
    is proposed to model the uncertainty in gaze estimation, especially in unconstrained
    settings.
  id: totrans-832
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了训练基于深度学习的模型，使用了 $\ell_{2}$ [[24](#bib.bib24), [51](#bib.bib51), [50](#bib.bib50)]
    和余弦相似度基于的损失[[94](#bib.bib94), [148](#bib.bib148)]。然而，提出了一种新型的弹球损失[[90](#bib.bib90)]，用于建模注视估计中的不确定性，特别是在无约束的设置中。
- en: •
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Similarly for deep learning based eye segmentation approaches, the eye image
    to segmentation mapping is performed in a non-parametric way which implicitly
    encodes shape, geometry, appearance and other factors [[176](#bib.bib176), [165](#bib.bib165),
    [152](#bib.bib152), [136](#bib.bib136), [67](#bib.bib67), [32](#bib.bib32)]. The
    most popular network architectures for eye segmentation are U-net [[177](#bib.bib177)],
    modified version of SegNet [[32](#bib.bib32)], RITnet [[136](#bib.bib136)], EyeNet [[165](#bib.bib165)].
    These VAE based architechtures have high time and space complexity. However, recent
    methods [[165](#bib.bib165), [136](#bib.bib136)] do consider these factors without
    compromising the performance.
  id: totrans-834
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于基于深度学习的眼睛分割方法，眼睛图像到分割的映射以非参数方式进行，这种方式隐式编码了形状、几何、外观和其他因素[[176](#bib.bib176),
    [165](#bib.bib165), [152](#bib.bib152), [136](#bib.bib136), [67](#bib.bib67),
    [32](#bib.bib32)]。眼睛分割最流行的网络架构是 U-net [[177](#bib.bib177)]、SegNet 的修改版本[[32](#bib.bib32)]、RITnet
    [[136](#bib.bib136)]、EyeNet [[165](#bib.bib165)]。这些基于 VAE 的架构具有较高的时间和空间复杂度。然而，最近的方法[[165](#bib.bib165),
    [136](#bib.bib136)]确实在不妨碍性能的情况下考虑了这些因素。
- en: 4.3 Level of Supervision
  id: totrans-835
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 监督级别
- en: 'Based on the type of supervision, the training procedure can be classified
    into the following categories: fully-supervised, Semi-/Self-/weakly-/unsupervised.'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 根据监督类型，训练过程可以分为以下几类：完全监督、半监督/自监督/弱监督/无监督。
- en: 4.3.1 Fully-Supervised.
  id: totrans-837
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1 完全监督
- en: 'Supervised learning paradigm is the most commonly used training framework in
    gaze estimation literature [[23](#bib.bib23), [20](#bib.bib20), [24](#bib.bib24),
    [87](#bib.bib87), [26](#bib.bib26), [97](#bib.bib97)] and eye segmentation literature [[136](#bib.bib136),
    [165](#bib.bib165), [156](#bib.bib156), [144](#bib.bib144), [135](#bib.bib135),
    [154](#bib.bib154), [133](#bib.bib133), [155](#bib.bib155), [11](#bib.bib11)].
    As the fully-supervised methods require a lot of accurately annotated data. Accurate
    annotation of gaze data is a complex, noise-prone, and time-consuming task and
    sometimes it requires expensive data acquisition setups. Moreover, there is a
    high possibility of noisy or wrong annotation due to distraction in participation
    during data collection, eye blink activity and inherent measurement errors in
    data curation settings. Variation in data curation setup limits merging multiple
    datasets for supervision. Dataset specific data acquisition processes are discussed
    in Sec. [5.1](#S5.SS1 "5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches"). Thus, the research
    community is moving towards learning with less supervision.'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '监督学习范式是注视估计文献 [[23](#bib.bib23), [20](#bib.bib20), [24](#bib.bib24), [87](#bib.bib87),
    [26](#bib.bib26), [97](#bib.bib97)] 和眼睛分割文献 [[136](#bib.bib136), [165](#bib.bib165),
    [156](#bib.bib156), [144](#bib.bib144), [135](#bib.bib135), [154](#bib.bib154),
    [133](#bib.bib133), [155](#bib.bib155), [11](#bib.bib11)] 中最常用的训练框架。由于完全监督的方法需要大量准确标注的数据，准确标注注视数据是一个复杂、容易出错且耗时的任务，有时需要昂贵的数据采集设置。此外，由于参与者在数据收集期间的干扰、眼睛眨动活动以及数据整理设置中的固有测量误差，存在高噪声或错误标注的可能性。数据整理设置的变化限制了多个数据集的合并以进行监督。数据集特定的数据采集过程在第[5.1节](#S5.SS1
    "5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis: A Survey
    of Deep Learning based Approaches")中讨论。因此，研究社区正逐步向少监督学习方向发展。'
- en: Multi-Task Learning. Multi-task learning incorporates different tasks which
    provide auxiliary information as a bias to improve model performance. The auxiliary
    information can be Gaze+Landmark [[61](#bib.bib61)], PoG+Screen saliency [[148](#bib.bib148),
    [178](#bib.bib178)], Gaze+Depth [[71](#bib.bib71)], Gaze+Headpose [[52](#bib.bib52)],
    Segmentation+Gaze [[67](#bib.bib67)] and Gaze-direction+Gaze-uncertainty [[90](#bib.bib90)].
    These gaze aligned tasks facilitate strong representation learning with additional
    task based supervision.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务学习。多任务学习结合了不同的任务，这些任务提供辅助信息作为偏置，以提高模型性能。辅助信息可以是 Gaze+Landmark [[61](#bib.bib61)]、PoG+屏幕显著性
    [[148](#bib.bib148), [178](#bib.bib178)]、Gaze+深度 [[71](#bib.bib71)]、Gaze+头部姿态
    [[52](#bib.bib52)]、分割+Gaze [[67](#bib.bib67)] 和 Gaze-方向+Gaze-不确定性 [[90](#bib.bib90)]。这些与注视对齐的任务通过额外的任务基础监督促进了强大的表示学习。
- en: '![Refer to caption](img/7f83a4b9daf9eba9102a42b329b6f0cb.png)'
  id: totrans-840
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/7f83a4b9daf9eba9102a42b329b6f0cb.png)'
- en: 'Figure 7: Data collection procedure in different settings for benchmark datasets.
    From left to right the examples are from CAVE [[23](#bib.bib23)], Eth-XGaze [[94](#bib.bib94)],
    MPII [[50](#bib.bib50)] and Gaze360 [[90](#bib.bib90)] datasets. The leftmost
    one is more constrained and the rightmost one is less constrained. Images are
    taken from respective datasets [[90](#bib.bib90), [94](#bib.bib94), [50](#bib.bib50),
    [23](#bib.bib23)]. Refer Table [III](#S5.T3 "Table III ‣ 5.1 Datasets for Gaze
    Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches") for more details.'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: '图7：基准数据集不同设置下的数据收集程序。从左到右的示例来自 CAVE [[23](#bib.bib23)]、Eth-XGaze [[94](#bib.bib94)]、MPII
    [[50](#bib.bib50)] 和 Gaze360 [[90](#bib.bib90)] 数据集。最左边的是更具约束性的，而最右边的是较少约束性的。图像取自各自的数据集
    [[90](#bib.bib90), [94](#bib.bib94), [50](#bib.bib50), [23](#bib.bib23)]。有关更多细节，请参见表
    [III](#S5.T3 "Table III ‣ 5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")。'
- en: 4.3.2 Semi-/Self-/Weakly-/Unsupervised.
  id: totrans-842
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2 半监督/自监督/弱监督/无监督。
- en: 'To a large extent, the supervised deep learning based methods’ performance
    depends on the quality and quantity of annotated data. However, manual labeling
    of gaze data is a complex, time consuming and labor extensive process. On this
    front, Semi-/Self-/Weakly-/Unsupervised Learning paradigms provide a promising
    alternative to enable learning from a vast amount of readily available non-annotated
    data. For learning paradigms with less supervision, the important methods are
    described in detail below:'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 在很大程度上，监督深度学习方法的性能依赖于标注数据的质量和数量。然而，注视数据的手动标注是一个复杂、耗时且劳动密集的过程。在这方面，半监督/自监督/弱监督/无监督学习范式提供了一个有前景的替代方案，以从大量现成的未标注数据中进行学习。对于监督较少的学习范式，以下方法被详细描述：
- en: Weakly-supervised and Learning from Pseudo Labels. Weakly supervised learning
    aims to bridge the gap between the fully-supervised and fully-unsupervised techniques.
    Till date in gaze estimation domain, the weak supervision has been performed via
    ‘Looking At Each Other (LAEO)’ [[179](#bib.bib179)] and pseudo labelling [[174](#bib.bib174)].
    For weak supervision, Kothari et al. [[179](#bib.bib179)] leverage strong gaze-related
    geometric constraints from two people interaction scenario. On the other hand,
    MTGLS [[174](#bib.bib174)] framework leverages from non-annotated facial image
    data by three complementary signals i.e. (1) the line of sight of the pupil, (2)
    the head-pose and (3) the eye dexterity.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督和伪标签学习。弱监督学习旨在弥合完全监督和完全无监督技术之间的差距。在注视估计领域，至今弱监督主要通过‘相互注视 (LAEO)’[[179](#bib.bib179)]和伪标注[[174](#bib.bib174)]进行。对于弱监督，Kothari等人[[179](#bib.bib179)]利用了两人互动场景中的强注视相关几何约束。另一方面，MTGLS[[174](#bib.bib174)]框架利用了来自非标注面部图像数据的三种互补信号，即（1）瞳孔的视线，（2）头部姿态以及（3）眼睛灵活性。
- en: Unsupervised and Self-supervised Representation Learning. Self supervised learning
    has emerged as a popular technique for learning meaningful representations from
    vast amount of non-annotated data. It requires pseudo labels for any pre-designed
    task which is often termed as Auxiliary or Pretext Task. The pre-designed task
    is mostly aligned with the gaze estimation. Dubey et al. [[27](#bib.bib27)] propose
    a pretext task where the visual regions of the gaze are divided into zones by
    geometric constraints. These pseudo labels are utilized for representation learning.
    Yu et al. [[28](#bib.bib28)] use subject specific gaze redirection as a pretext
    task. Swapping Affine Transformations (SwAT) [[180](#bib.bib180)] is the extended
    version of Swapping Assignments Between Views (SwAV), a popular self supervised
    learning framework used for gaze representation learning using different augmentation
    techniques. The self-supervised representation learning has the potential to eliminate
    the major drawback of gaze data annotation which is quite difficult and error
    prone. Future directions in this area may include designing better pre-text tasks
    and combining multiple pre-text tasks to jointly pre-train the models [[174](#bib.bib174)].
    In addition, combining data-driven eye tracking with model-based eye tracking
    can be another future direction as model-based eye tracking can provide pseudo-labels
    or pre-train the models.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督和自监督表征学习。自监督学习已成为从大量未标注数据中学习有意义表征的热门技术。它需要为任何预设计任务提供伪标签，这通常被称为辅助任务或前置任务。预设计任务通常与注视估计对齐。Dubey等人[[27](#bib.bib27)]提出了一种前置任务，其中注视的视觉区域通过几何约束划分为不同区域。这些伪标签用于表征学习。Yu等人[[28](#bib.bib28)]使用特定主体的注视重定向作为前置任务。Swapping
    Affine Transformations (SwAT) [[180](#bib.bib180)]是Swapping Assignments Between
    Views (SwAV)的扩展版本，SwAV是一个流行的自监督学习框架，用于利用不同的数据增强技术进行注视表征学习。自监督表征学习有潜力消除注视数据标注的主要缺点，即困难且容易出错。该领域的未来方向可能包括设计更好的前置任务，并将多个前置任务结合起来共同预训练模型[[174](#bib.bib174)]。此外，将数据驱动的眼动追踪与基于模型的眼动追踪相结合，也可能是未来的一个方向，因为基于模型的眼动追踪可以提供伪标签或预训练模型。
- en: Few Shot Learning. Few-shot learning aims to adapt to a new task with very few
    examples [[29](#bib.bib29), [91](#bib.bib91)]. The main challenge in few shot
    paradigm is over-fitting issue since highly over-parameterized deep networks are
    involved to learn from only a few training samples. To this front, mainly gaze
    redirection strategy [[91](#bib.bib91)] and Few-shot Adaptive GaZE Estimation
    (FAZE) [[29](#bib.bib29)] frameworks are proposed. Among them, FAZE is shown a
    two stage adaptation strategy. In the first stage, a rotation aware latent space
    embedding is learned based on encoder-decoder framework. Further, adaptation is
    performed on top of the features using MAML which is a popular meta-learning paradigm.
    FAZE is able to adapt to a new subject with $\leq 9$ sample which is quite promising.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习。少样本学习旨在通过极少的样本适应新任务[[29](#bib.bib29), [91](#bib.bib91)]。在少样本范式中，主要挑战是过拟合问题，因为高度过参数化的深度网络需要仅从少量训练样本中学习。为此，主要提出了注视重定向策略[[91](#bib.bib91)]和少样本自适应注视估计（FAZE）[[29](#bib.bib29)]框架。其中，FAZE展示了一种两阶段适应策略。在第一阶段，基于编码器-解码器框架学习旋转感知的潜在空间嵌入。随后，在特征上进行适应，使用MAML这一个流行的元学习范式。FAZE能够通过$\leq
    9$样本适应新主体，表现相当有前景。
- en: Learning-by-synthesis. The term ‘learning-by-synthesis’ is coined by Sugano
    et al. [[20](#bib.bib20)]. The main objective is to synthesize different gaze
    viewpoints to multiply the data from a quantitative and a qualitative perspectives
    rather than manual labelling. Few other studies [[20](#bib.bib20), [160](#bib.bib160),
    [181](#bib.bib181), [182](#bib.bib182), [91](#bib.bib91), [183](#bib.bib183),
    [92](#bib.bib92)] also adopt data generation methods which can address the diversity
    in terms of headpose and eye rotation. However, these generative models have high
    computational complexity and are constrained by the quality of generated images.
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 学习合成。‘学习合成’这一术语由Sugano等人[[20](#bib.bib20)]提出。其主要目的是从定量和定性角度合成不同的凝视视角，以扩充数据，而非依赖手动标注。其他一些研究[[20](#bib.bib20)、[160](#bib.bib160)、[181](#bib.bib181)、[182](#bib.bib182)、[91](#bib.bib91)、[183](#bib.bib183)、[92](#bib.bib92)]也采用了数据生成方法，以应对头部姿态和眼睛旋转的多样性。然而，这些生成模型具有较高的计算复杂性，并且受到生成图像质量的限制。
- en: Discussion. In gaze estimation domain, there are still very few works in semi-/self-/weakly-/unsupervised
    learning paradigms. Among these works, learning from pseudo labels have their
    limitations as the label space contains noise. On the other hand, gaze redirection
    synthesis is based on the availability of same or different person’s data with
    eye rotation or the prior knowledge of rotation angle. Thus, learning robust and
    generalizable gaze representations using minimal or no supervision still remains
    an open-ended research question. One possible direction to alleviate this problem
    is to combine data-driven eye tracking with model-based eye tracking to produce
    physically plausible eye tracking models that are data efficient during training
    and generalize better during testing.
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论。在凝视估计领域，仍然很少有关于半监督、自监督、弱监督或无监督学习范式的研究。在这些研究中，从伪标签学习存在一定局限性，因为标签空间中包含噪声。另一方面，凝视重定向合成基于相同或不同个体的眼睛旋转数据或旋转角度的先验知识。因此，使用最少或无需监督来学习稳健且可泛化的凝视表示仍然是一个悬而未决的研究问题。缓解这个问题的一个可能方向是将数据驱动的眼动追踪与基于模型的眼动追踪相结合，以生成在训练过程中数据高效且在测试时泛化更好的物理上可信的眼动追踪模型。
- en: 5 Validation
  id: totrans-849
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 验证
- en: In this section, we review the commonly followed evaluation procedures on various
    datasets along with the metrics adopted in the literature.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们回顾了文献中对各种数据集常见的评估程序以及采用的指标。
- en: 5.1 Datasets for Gaze Analysis
  id: totrans-851
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 凝视分析数据集
- en: 'With the rapid progress in the gaze analysis domain, several datasets have
    been proposed for different gaze analysis tasks (see Sec. [3](#S3 "3 Gaze Analysis
    in Computer Vision ‣ Automatic Gaze Analysis: A Survey of Deep Learning based
    Approaches")). The dataset collection technique has evolved from constrained lab
    environments [[23](#bib.bib23)] to unconstrained indoor [[87](#bib.bib87), [50](#bib.bib50),
    [51](#bib.bib51), [54](#bib.bib54)] and outdoor settings [[90](#bib.bib90)] (Refer
    Fig. [7](#S4.F7 "Figure 7 ‣ 4.3.1 Fully-Supervised. ‣ 4.3 Level of Supervision
    ‣ 4 Gaze Analysis Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")). We provide a detailed overview of the datasets in Table [III](#S5.T3
    "Table III ‣ 5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches"). Compared with early datasets [[23](#bib.bib23),
    [158](#bib.bib158)], recently released datasets [[90](#bib.bib90), [148](#bib.bib148)]
    are typically more advanced with less bias, improved complexity, and larger in
    scale. These are better suited for training and evaluation. We describe a few
    important datasets below:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: '随着凝视分析领域的快速进展，已经提出了多个用于不同凝视分析任务的数据集（见第[3](#S3 "3 Gaze Analysis in Computer
    Vision ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")节）。数据集收集技术已经从受限的实验室环境[[23](#bib.bib23)]发展到无约束的室内[[87](#bib.bib87)、[50](#bib.bib50)、[51](#bib.bib51)、[54](#bib.bib54)]和室外环境[[90](#bib.bib90)]（参见图[7](#S4.F7
    "Figure 7 ‣ 4.3.1 Fully-Supervised. ‣ 4.3 Level of Supervision ‣ 4 Gaze Analysis
    Framework ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")）。我们在表[III](#S5.T3
    "Table III ‣ 5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")中提供了数据集的详细概述。与早期数据集[[23](#bib.bib23)、[158](#bib.bib158)]相比，最近发布的数据集[[90](#bib.bib90)、[148](#bib.bib148)]通常更先进，偏差较小，复杂性更高，规模也更大。这些数据集更适合用于训练和评估。以下是一些重要数据集的描述：'
- en: CAVE [[23](#bib.bib23)] contains 5,880 images of 56 subjects with different
    gaze directions and head poses. There are 21 different gaze directions for each
    person and the data was collected in a constrained lab environment, with 7 horizontal
    and 3 vertical gaze locations.
  id: totrans-853
  prefs: []
  type: TYPE_NORMAL
  zh: CAVE [[23](#bib.bib23)] 包含 56 个受试者的 5,880 张图像，具有不同的注视方向和头部姿态。每个人有 21 个不同的注视方向，数据是在受限的实验室环境中收集的，具有
    7 个水平和 3 个垂直的注视位置。
- en: The Eyediap dataset [[184](#bib.bib184)] was designed to overcome the main challenges
    associated with the head pose, person and 3-D target variations along with changes
    in ambient and sensing conditions.
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: Eyediap 数据集 [[184](#bib.bib184)] 旨在克服与头部姿态、个人和三维目标变化相关的主要挑战，同时应对环境和传感条件的变化。
- en: TabletGaze [[87](#bib.bib87)] is a large unconstrained dataset of 51 subjects
    with 4 different postures and 35 gaze locations collected using a tablet in an
    indoor environment. TabletGaze dataset is also collected in a $7\times 5$ grid
    format.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: TabletGaze [[87](#bib.bib87)] 是一个大规模的非约束数据集，包含 51 个受试者的 4 种不同姿势和 35 个注视位置，这些数据是在室内环境中使用平板电脑收集的。TabletGaze
    数据集还以 $7\times 5$ 的网格格式进行收集。
- en: MPII [[50](#bib.bib50)] gaze dataset contains 213,659 images collected from
    15 subjects during natural everyday events in front of a laptop over a three-month
    duration. MPII gaze dataset is collected by showing random points on the laptop
    screen to the participants. Further, Zhang et al. [[51](#bib.bib51)] curate MPIIFaceGaze
    dataset with the hypothesis that gaze can be more accurately predicted when the
    entire face is considered.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: MPII [[50](#bib.bib50)] 注视数据集包含 213,659 张图像，这些图像是在三个月的时间里，从 15 个受试者在笔记本电脑前的自然日常事件中收集的。MPII
    注视数据集通过向参与者展示笔记本电脑屏幕上的随机点来收集。此外，Zhang 等人 [[51](#bib.bib51)] 组织了 MPIIFaceGaze 数据集，假设在考虑整个面部时，注视可以被更准确地预测。
- en: RT-GENE dataset [[54](#bib.bib54)] is recorded in a more naturalistic environment
    with varied gaze and head pose angles. The ground truth annotation was done using
    a motion capture system with mobile eye-tracking glasses.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: RT-GENE 数据集 [[54](#bib.bib54)] 记录在一个更自然的环境中，具有多样的注视角度和头部姿态。地面真实度注释是使用运动捕捉系统和移动眼动追踪眼镜完成的。
- en: Gaze360 [[90](#bib.bib90)] is a large-scale gaze estimation dataset collected
    from 238 subjects in unconstrained indoor and outdoor settings with a wide range
    of head pose.
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: Gaze360 [[90](#bib.bib90)] 是一个大规模注视估计数据集，从 238 个受试者处收集，涵盖了非约束的室内和室外环境，并具有广泛的头部姿态。
- en: ETH-XGaze [[94](#bib.bib94)] is a large scale dataset collected in a constraint
    environment with a wide range of head pose, high-resolution images. The dataset
    contains images from different camera positions, illumination conditions to add
    more challenges to the data.
  id: totrans-859
  prefs: []
  type: TYPE_NORMAL
  zh: ETH-XGaze [[94](#bib.bib94)] 是一个大规模数据集，收集于一个受限环境中，具有广泛的头部姿态和高分辨率图像。该数据集包含来自不同摄像机位置和照明条件的图像，以增加数据的挑战性。
- en: EVE [[148](#bib.bib148)] is also collected in constraint indoor setting with
    different camera views to map human gaze in screen co-ordinate.
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: EVE [[148](#bib.bib148)] 也是在受限的室内环境中收集的，具有不同的摄像机视角，以在屏幕坐标中映射人类注视。
- en: 'Similar to gaze estimation several benchmark datasets have been proposed over
    the past few years for eye and sclera segmentation. The datasets collected for
    sclera segmentation is in a constraint environment and with very few subjects [[185](#bib.bib185),
    [186](#bib.bib186), [187](#bib.bib187)]. A more challenging publicly available
    dataset was released in sclera recognition challenges [[154](#bib.bib154), [133](#bib.bib133),
    [155](#bib.bib155)]. Recently, a large scale dataset termed as OpenEDS: Open Eye
    Dataset [[32](#bib.bib32)], is released which contains eye images collected by
    using a VR head-mounted device. Additionally, there was two synchronized eye facing
    cameras having a frame rate of 200 Hz. The data was collected under controlled
    illumination and contains 12,759 images with eye segmentation masks collected
    from 152 participants.'
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: '与注视估计类似，近年来已经提出了多个基准数据集用于眼睛和巩膜分割。用于巩膜分割的数据集是在受限环境中收集的，并且受试者数量非常少 [[185](#bib.bib185),
    [186](#bib.bib186), [187](#bib.bib187)]。一个更具挑战性的公开数据集是在巩膜识别挑战中发布的 [[154](#bib.bib154),
    [133](#bib.bib133), [155](#bib.bib155)]。最近，发布了一个名为 OpenEDS: Open Eye Dataset [[32](#bib.bib32)]
    的大规模数据集，其中包含使用 VR 头戴设备收集的眼睛图像。此外，还有两台同步的眼部对摄像机，帧率为 200 Hz。数据是在受控的照明条件下收集的，包含 12,759
    张带有眼部分割掩码的图像，来自 152 名参与者。'
- en: 'Table III: Datasets. A comparison of gaze datasets with respect to several
    attributes (i.e. number of subjects (# sub), gaze labels, modality, headpose and
    gaze angle in yaw and pitch axis, environment (Env.), baseline method, data statistics
    (# data), and year of publication.) The abbreviations used are: In: Indoor, Out:
    Outdoor, Both: Indoor + Outdoor, Gen.: Generation, u/k: unknown, Seq.: Sequence,
    VF: Visual Field, EB: Eye Blink, GE: Gaze Event [[179](#bib.bib179)], GBRT: Gradient
    Boosting Regression Trees, GC: Gaze Communication, GNN: Graph Neural Network and
    Seg.: Segmentation.'
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: '表 III: 数据集。不同注视数据集在多个属性（即受试者数量（# sub）、注视标签、模态、头部姿势和注视角度在偏航和俯仰轴上、环境（Env.）、基线方法、数据统计（#
    data）、以及出版年份）方面的比较。使用的缩写包括：In: 室内，Out: 室外，Both: 室内 + 室外，Gen.: 生成，u/k: 未知，Seq.:
    序列，VF: 视觉场，EB: 眼睑眨动，GE: 注视事件 [[179](#bib.bib179)]，GBRT: 梯度提升回归树，GC: 注视交流，GNN:
    图神经网络 和 Seg.: 分割。'
- en: '| Dataset | # Sub | Label | Modality | Head-Pose | Gaze | Env. | Baseline |
    # Data | Year |'
  id: totrans-863
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | # 受试者 | 标签 | 模态 | 头部姿势 | 注视 | 环境 | 基线 | # 数据 | 年份 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-864
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| [CAVE](https://www.cs.columbia.edu/CAVE/databases/columbia_gaze/) [[23](#bib.bib23)]
    | 56 | 3-D |'
  id: totrans-865
  prefs: []
  type: TYPE_TB
  zh: '| [CAVE](https://www.cs.columbia.edu/CAVE/databases/columbia_gaze/) [[23](#bib.bib23)]
    | 56 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.:$5184\times 3456$ &#124;'
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸:$5184\times 3456$ &#124;'
- en: '| $0$°, $\pm 30$° | $\pm 15$°, $\pm 10$° | In |'
  id: totrans-868
  prefs: []
  type: TYPE_TB
  zh: '| $0$°, $\pm 30$° | $\pm 15$°, $\pm 10$° | 室内 |'
- en: '&#124; SVM &#124;'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SVM &#124;'
- en: '&#124; Eval.:Cross-val &#124;'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估:交叉验证 &#124;'
- en: '|'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:5880 &#124;'
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计:5880 &#124;'
- en: '| 2013 |'
  id: totrans-873
  prefs: []
  type: TYPE_TB
  zh: '| 2013 |'
- en: '| [EYEDIAP](https://www.idiap.ch/en/dataset/eyediap) [[158](#bib.bib158)] |
    16 | 3-D |'
  id: totrans-874
  prefs: []
  type: TYPE_TB
  zh: '| [EYEDIAP](https://www.idiap.ch/en/dataset/eyediap) [[158](#bib.bib158)] |
    16 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-875
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: HD and VGA &#124;'
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸: HD 和 VGA &#124;'
- en: '| $\pm 15$°, $30$° | $\pm 25$°, $20$° | In |'
  id: totrans-877
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 15$°, $30$° | $\pm 25$°, $20$° | 室内 |'
- en: '&#124; Convex Hull &#124;'
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 凸包 &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估: Hold out &#124;'
- en: '|'
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:237 min &#124;'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计:237 min &#124;'
- en: '| 2014 |'
  id: totrans-882
  prefs: []
  type: TYPE_TB
  zh: '| 2014 |'
- en: '| [UT MV](https://www.ut-vision.org/datasets/) [[20](#bib.bib20)] | 50 | 3-D
    |'
  id: totrans-883
  prefs: []
  type: TYPE_TB
  zh: '| [UT MV](https://www.ut-vision.org/datasets/) [[20](#bib.bib20)] | 50 | 3-D
    |'
- en: '&#124; Image &#124;'
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.:$1280\times 1024$ &#124;'
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸:$1280\times 1024$ &#124;'
- en: '| $\pm 36$°, $\pm 36$° | $\pm 50$°, $\pm 36$° | In |'
  id: totrans-886
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 36$°, $\pm 36$° | $\pm 50$°, $\pm 36$° | 室内 |'
- en: '&#124; Random Reg. &#124;'
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 随机回归 &#124;'
- en: '&#124; Forests &#124;'
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 森林 &#124;'
- en: '&#124; Eval.:Hold out &#124;'
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估:Hold out &#124;'
- en: '|'
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:64,000 &#124;'
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计:64,000 &#124;'
- en: '| 2014 |'
  id: totrans-892
  prefs: []
  type: TYPE_TB
  zh: '| 2014 |'
- en: '| OMEG [[188](#bib.bib188)] | 50 | 3-D |'
  id: totrans-893
  prefs: []
  type: TYPE_TB
  zh: '| OMEG [[188](#bib.bib188)] | 50 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $1280\times 1024$ &#124;'
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸: $1280\times 1024$ &#124;'
- en: '| $0$°, $\pm 30$° |'
  id: totrans-896
  prefs: []
  type: TYPE_TB
  zh: '| $0$°, $\pm 30$° |'
- en: '&#124; $-38$°to $+36$°, &#124;'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $-38$°到 $+36$°， &#124;'
- en: '&#124; $-10$°to $+29$° &#124;'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $-10$°到 $+29$° &#124;'
- en: '| In |'
  id: totrans-899
  prefs: []
  type: TYPE_TB
  zh: '| 室内 |'
- en: '&#124; SVR &#124;'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SVR &#124;'
- en: '&#124; Eval.:LOSO &#124;'
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估:LOSO &#124;'
- en: '|'
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:44,827 &#124;'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计:44,827 &#124;'
- en: '| 2015 |'
  id: totrans-904
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '| [MPIIGaze](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild) [[24](#bib.bib24)]
    | 15 | 3-D |'
  id: totrans-905
  prefs: []
  type: TYPE_TB
  zh: '| [MPIIGaze](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/appearance-based-gaze-estimation-in-the-wild)
    [[24](#bib.bib24)] | 15 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸: $1280\times 720$ &#124;'
- en: '| $\pm 15$°, $30$° | $\pm 20$°, $\pm 20$° | In |'
  id: totrans-908
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 15$°, $30$° | $\pm 20$°, $\pm 20$° | 室内 |'
- en: '&#124; CNN variant [[24](#bib.bib24)] &#124;'
  id: totrans-909
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN 变体 [[24](#bib.bib24)] &#124;'
- en: '&#124; Eval.:LOSO &#124;'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估:LOSO &#124;'
- en: '|'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:213,659 &#124;'
  id: totrans-912
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计:213,659 &#124;'
- en: '| 2015 |'
  id: totrans-913
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '| [GazeFollow](http://gazefollow.csail.mit.edu/index.html) [[189](#bib.bib189)]
    | 130,339 | 3-D |'
  id: totrans-914
  prefs: []
  type: TYPE_TB
  zh: '| [GazeFollow](http://gazefollow.csail.mit.edu/index.html) [[189](#bib.bib189)]
    | 130,339 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-915
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: Variable &#124;'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸: 变量 &#124;'
- en: '| Variable | Variable | Both |'
  id: totrans-917
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 变量 | 室内和室外 |'
- en: '&#124; CNN variant [[189](#bib.bib189)] &#124;'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN 变体 [[189](#bib.bib189)] &#124;'
- en: '&#124; Eval.:Hold out &#124;'
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估:Hold out &#124;'
- en: '|'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:122,143 &#124;'
  id: totrans-921
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计:122,143 &#124;'
- en: '| 2015 |'
  id: totrans-922
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '| [SynthesEye](https://www.cl.cam.ac.uk/research/rainbow/projects/syntheseyes/)[[159](#bib.bib159)]
    | NA | 3-D |'
  id: totrans-923
  prefs: []
  type: TYPE_TB
  zh: '| [SynthesEye](https://www.cl.cam.ac.uk/research/rainbow/projects/syntheseyes/)
    [[159](#bib.bib159)] | NA | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.:$120\times 80$ &#124;'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸:$120\times 80$ &#124;'
- en: '| $\pm 50$°, $\pm 50$° | $\pm 50$°, $\pm 50$° | Syn |'
  id: totrans-926
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 50$°, $\pm 50$° | $\pm 50$°, $\pm 50$° | Syn |'
- en: '&#124; CNN [[159](#bib.bib159)] &#124;'
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN [[159](#bib.bib159)] &#124;'
- en: '&#124; Eval.:Hold out &#124;'
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估:Hold out &#124;'
- en: '|'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:11,400 &#124;'
  id: totrans-930
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计:11,400 &#124;'
- en: '| 2015 |'
  id: totrans-931
  prefs: []
  type: TYPE_TB
  zh: '| 2015 |'
- en: '| [GazeCapture](https://gazecapture.csail.mit.edu/) [[25](#bib.bib25)] | 1450
    | 2-D |'
  id: totrans-932
  prefs: []
  type: TYPE_TB
  zh: '| [GazeCapture](https://gazecapture.csail.mit.edu/) [[25](#bib.bib25)] | 1450
    | 2-D |'
- en: '&#124; Image &#124;'
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.:$640\times 480$ &#124;'
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$640\times 480$ &#124;'
- en: '| $\pm 30$°, $40$° | $\pm 20$°, $\pm 20$° | Both |'
  id: totrans-935
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 30$°, $40$° | $\pm 20$°, $\pm 20$° | 两者 |'
- en: '&#124; CNN [[25](#bib.bib25)] &#124;'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN [[25](#bib.bib25)] &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:2,445,504 &#124;'
  id: totrans-939
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：2,445,504 &#124;'
- en: '| 2016 |'
  id: totrans-940
  prefs: []
  type: TYPE_TB
  zh: '| 2016 |'
- en: '| [UnityEyes](https://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/) [[190](#bib.bib190)]
    | NA | 3-D |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
  zh: '| [UnityEyes](https://www.cl.cam.ac.uk/research/rainbow/projects/unityeyes/)
    [[190](#bib.bib190)] | NA | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-942
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.:$400\times 300$ &#124;'
  id: totrans-943
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$400\times 300$ &#124;'
- en: '| Variable | Variable | Syn |'
  id: totrans-944
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 变量 | 同步 |'
- en: '&#124; KNN &#124;'
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; KNN &#124;'
- en: '&#124; Eval.:NA &#124;'
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：NA &#124;'
- en: '|'
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 1,000,000 &#124;'
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：1,000,000 &#124;'
- en: '| 2016 |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
  zh: '| 2016 |'
- en: '| [TabletGaze](https://sh.rice.edu/cognitive-engagement/%20tabletgaze/) [[87](#bib.bib87)]
    | 51 | 2-D Sc. |'
  id: totrans-950
  prefs: []
  type: TYPE_TB
  zh: '| [TabletGaze](https://sh.rice.edu/cognitive-engagement/%20tabletgaze/) [[87](#bib.bib87)]
    | 51 | 2-D Sc. |'
- en: '&#124; Video &#124;'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$1280\times 720$ &#124;'
- en: '| $\pm 50$°, $\pm 50$° | $\pm 20$°, $\pm 20$° | In |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 50$°, $\pm 50$° | $\pm 20$°, $\pm 20$° | 在 |'
- en: '&#124; SVR &#124;'
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SVR &#124;'
- en: '&#124; Eval.:Cross-val &#124;'
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：交叉验证 &#124;'
- en: '|'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:816 Seq. &#124;'
  id: totrans-957
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：816 序列 &#124;'
- en: '&#124; $\sim$ 300,000 img. &#124;'
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\sim$ 300,000 图像 &#124;'
- en: '| 2017 |'
  id: totrans-959
  prefs: []
  type: TYPE_TB
  zh: '| 2017 |'
- en: '|'
  id: totrans-960
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [MPIIFaceGaze](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/its-written-all-over-your-face-full-face-appearance-based-gaze-estimation)
    &#124;'
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [MPIIFaceGaze](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/its-written-all-over-your-face-full-face-appearance-based-gaze-estimation)
    &#124;'
- en: '&#124;  [[51](#bib.bib51)] &#124;'
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[51](#bib.bib51)] &#124;'
- en: '| 15 | 3-D |'
  id: totrans-963
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-964
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$1280\times 720$ &#124;'
- en: '| $\pm 15$°, $30$° | $\pm 20$°, $\pm 20$° | In |'
  id: totrans-966
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 15$°, $30$° | $\pm 20$°, $\pm 20$° | 在 |'
- en: '&#124; CNN variant [[51](#bib.bib51)] &#124;'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN 变体 [[51](#bib.bib51)] &#124;'
- en: '&#124; Eval.:LOSO &#124;'
  id: totrans-968
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：LOSO &#124;'
- en: '|'
  id: totrans-969
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:213,659 &#124;'
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：213,659 &#124;'
- en: '| 2017 |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
  zh: '| 2017 |'
- en: '| [InvisibleEye](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/invisibleeye-mobile-eye-tracking-using-multiple-low-resolution-cameras-and-learning-based-gaze-estimation) [[70](#bib.bib70)]
    | 17 | 2-D Sc |'
  id: totrans-972
  prefs: []
  type: TYPE_TB
  zh: '| [InvisibleEye](https://www.mpi-inf.mpg.de/departments/computer-vision-and-machine-learning/research/gaze-based-human-computer-interaction/invisibleeye-mobile-eye-tracking-using-multiple-low-resolution-cameras-and-learning-based-gaze-estimation)
    [[70](#bib.bib70)] | 17 | 2-D Sc |'
- en: '&#124; Image &#124;'
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $5\times 5$ &#124;'
  id: totrans-974
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$5\times 5$ &#124;'
- en: '| Unknown |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
  zh: '| 未知 |'
- en: '&#124; $2560\times 1600$ &#124;'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $2560\times 1600$ &#124;'
- en: '&#124; pixel VF &#124;'
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 像素视场 &#124;'
- en: '| In |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
  zh: '| 在 |'
- en: '&#124; ANN [[70](#bib.bib70)] &#124;'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ANN [[70](#bib.bib70)] &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-981
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:280,000 &#124;'
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：280,000 &#124;'
- en: '| 2017 |'
  id: totrans-983
  prefs: []
  type: TYPE_TB
  zh: '| 2017 |'
- en: '| [RT-GENE](https://github.com/Tobias-Fischer/rt_gene) [[54](#bib.bib54)] |
    15 | 3-D |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
  zh: '| [RT-GENE](https://github.com/Tobias-Fischer/rt_gene) [[54](#bib.bib54)] |
    15 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.:$1920\times 1080$ &#124;'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$1920\times 1080$ &#124;'
- en: '| $\pm 40$°, $\pm 40$° | $\pm 40$°, $-40$° | In |'
  id: totrans-987
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 40$°, $\pm 40$° | $\pm 40$°, $-40$° | 在 |'
- en: '&#124; CNN [[54](#bib.bib54)] &#124;'
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN [[54](#bib.bib54)] &#124;'
- en: '&#124; Eval.:Cross val &#124;'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：交叉验证 &#124;'
- en: '|'
  id: totrans-990
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:122,531 &#124;'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：122,531 &#124;'
- en: '| 2018 |'
  id: totrans-992
  prefs: []
  type: TYPE_TB
  zh: '| 2018 |'
- en: '| [Gaze 360](http://gaze360.csail.mit.edu/) [[90](#bib.bib90)] | 238 | 3-D
    |'
  id: totrans-993
  prefs: []
  type: TYPE_TB
  zh: '| [Gaze 360](http://gaze360.csail.mit.edu/) [[90](#bib.bib90)] | 238 | 3-D
    |'
- en: '&#124; Image &#124;'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.:$4096\times 3382$ &#124;'
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$4096\times 3382$ &#124;'
- en: '| $\pm 90$°, u/k | $\pm 140$°, $-50$° |'
  id: totrans-996
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 90$°, u/k | $\pm 140$°, $-50$° |'
- en: '&#124; Both &#124;'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两者 &#124;'
- en: '|'
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pinball LSTM &#124;'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Pinball LSTM &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 172,000 &#124;'
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：172,000 &#124;'
- en: '| 2019 |'
  id: totrans-1003
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [RT-BENE](https://github.com/Tobias-Fischer/rt_gene)[[191](#bib.bib191)]
    | 17 | EB |'
  id: totrans-1004
  prefs: []
  type: TYPE_TB
  zh: '| [RT-BENE](https://github.com/Tobias-Fischer/rt_gene) [[191](#bib.bib191)]
    | 17 | EB |'
- en: '&#124; Image &#124;'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $1920\times 1080$ &#124;'
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$1920\times 1080$ &#124;'
- en: '| $\pm 40$°, $\pm 40$° | $\pm 40$°, $-40$° | In |'
  id: totrans-1007
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 40$°, $\pm 40$° | $\pm 40$°, $-40$° | 在 |'
- en: '&#124; CNNs &#124;'
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNNs &#124;'
- en: '&#124; Eval.: Cross val &#124;'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：交叉验证 &#124;'
- en: '|'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 243,714 &#124;'
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：243,714 &#124;'
- en: '| 2019 |'
  id: totrans-1012
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '| [NV Gaze](https://sites.google.com/nvidia.com/nvgaze) [[68](#bib.bib68)]
    | 30 |'
  id: totrans-1013
  prefs: []
  type: TYPE_TB
  zh: '| [NV Gaze](https://sites.google.com/nvidia.com/nvgaze) [[68](#bib.bib68)]
    | 30 |'
- en: '&#124; 3-D, &#124;'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 3-D, &#124;'
- en: '&#124; Seg. &#124;'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '|'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image (Synthetic) &#124;'
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像（合成） &#124;'
- en: '&#124; Dim.:$1280\times 960$, &#124;'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$1280\times 960$ &#124;'
- en: '&#124; $640\times 480$ &#124;'
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $640\times 480$ &#124;'
- en: '| Unknown |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '| 未知 |'
- en: '&#124; $30$°$\times 40$° &#124;'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $30$°$\times 40$° &#124;'
- en: '&#124; VF &#124;'
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VF &#124;'
- en: '|'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Both &#124;'
  id: totrans-1024
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 两者 &#124;'
- en: '|'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CNN [[192](#bib.bib192)] &#124;'
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN [[192](#bib.bib192)] &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1028
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 2,500,000 &#124;'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计：2,500,000 &#124;'
- en: '| 2019 |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '|'
  id: totrans-1031
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [HUST-LEBW](https://github.com/thorhu/Eyeblink-in-the-wild) &#124;'
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [HUST-LEBW](https://github.com/thorhu/Eyeblink-in-the-wild) &#124;'
- en: '&#124;  [[143](#bib.bib143)] &#124;'
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[143](#bib.bib143)] &#124;'
- en: '| 172 | EB |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '| 172 | EB |'
- en: '&#124; Video &#124;'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$1280\times 720$ &#124;'
- en: '| Variable | Variable | Both |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 变量 | 两者 |'
- en: '&#124; MS-LSTM &#124;'
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MS-LSTM &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1040
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 673 &#124;'
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计：673 &#124;'
- en: '| 2019 |'
  id: totrans-1042
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '|'
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [VACATION](https://github.com/LifengFan/Human-Gaze-Communication) &#124;'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [VACATION](https://github.com/LifengFan/Human-Gaze-Communication) &#124;'
- en: '&#124;  [[111](#bib.bib111)] &#124;'
  id: totrans-1045
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[111](#bib.bib111)] &#124;'
- en: '| 206,774 | GC |'
  id: totrans-1046
  prefs: []
  type: TYPE_TB
  zh: '| 206,774 | GC |'
- en: '&#124; Video &#124;'
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视频 &#124;'
- en: '&#124; Dim.: $640\times 360$ &#124;'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$640\times 360$ &#124;'
- en: '| Variable | Variable | Both |'
  id: totrans-1049
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 变量 | 两者 |'
- en: '&#124; GNN &#124;'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GNN &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 96,993 &#124;'
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计：96,993 &#124;'
- en: '| 2019 |'
  id: totrans-1054
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '|'
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [OpenEDS-19](https://research.facebook.com/openeds-challenge/) [[32](#bib.bib32)]
    &#124;'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [OpenEDS-19](https://research.facebook.com/openeds-challenge/) [[32](#bib.bib32)]
    &#124;'
- en: '&#124; Track 1: Semantic &#124;'
  id: totrans-1057
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 路径 1：语义 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '| 152 | Seg. |'
  id: totrans-1059
  prefs: []
  type: TYPE_TB
  zh: '| 152 | 分割 |'
- en: '&#124; Image &#124;'
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $640\times 400$ &#124;'
  id: totrans-1061
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$640\times 400$ &#124;'
- en: '| Unknown | Unknown | In |'
  id: totrans-1062
  prefs: []
  type: TYPE_TB
  zh: '| 未知 | 未知 | 在 |'
- en: '&#124; SegNet [[163](#bib.bib163)] &#124;'
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SegNet [[163](#bib.bib163)] &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total:12,759 &#124;'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计：12,759 &#124;'
- en: '&#124; (in # SegSeq [[32](#bib.bib32)]) &#124;'
  id: totrans-1067
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （在 # SegSeq [[32](#bib.bib32)]) &#124;'
- en: '| 2019 |'
  id: totrans-1068
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '|'
  id: totrans-1069
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [OpenEDS-19](https://research.facebook.com/openeds-challenge/) [[32](#bib.bib32)]
    &#124;'
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [OpenEDS-19](https://research.facebook.com/openeds-challenge/) [[32](#bib.bib32)]
    &#124;'
- en: '&#124; Track 2: Synthetic &#124;'
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 路径 2：合成 &#124;'
- en: '&#124; Eye Generation &#124;'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 眼睛生成 &#124;'
- en: '| 152 | Gen. |'
  id: totrans-1073
  prefs: []
  type: TYPE_TB
  zh: '| 152 | 生成 |'
- en: '&#124; Image &#124;'
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $640\times 400$ &#124;'
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$640\times 400$ &#124;'
- en: '| Unknown | Unknown | In |'
  id: totrans-1076
  prefs: []
  type: TYPE_TB
  zh: '| 未知 | 未知 | 在 |'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 252,690 &#124;'
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计：252,690 &#124;'
- en: '| 2019 |'
  id: totrans-1080
  prefs: []
  type: TYPE_TB
  zh: '| 2019 |'
- en: '|'
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [OpenEDS-20](https://research.facebook.com/openeds-2020-challenge/) [[34](#bib.bib34)]
    &#124;'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [OpenEDS-20](https://research.facebook.com/openeds-2020-challenge/) [[34](#bib.bib34)]
    &#124;'
- en: '&#124; Track 1: Gaze &#124;'
  id: totrans-1083
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 路径 1：注视 &#124;'
- en: '&#124; Prediction &#124;'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预测 &#124;'
- en: '| 90 | 3-D |'
  id: totrans-1085
  prefs: []
  type: TYPE_TB
  zh: '| 90 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $640\times 400$ &#124;'
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$640\times 400$ &#124;'
- en: '| Unknown | $\pm 20$°, $\pm 20$° | In |'
  id: totrans-1088
  prefs: []
  type: TYPE_TB
  zh: '| 未知 | $\pm 20$°, $\pm 20$° | 在 |'
- en: '&#124; Modified ResNet &#124;'
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 修改的 ResNet &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 8,960 Seq., &#124;'
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计：8,960 序列，&#124;'
- en: '&#124; 550,400 img. &#124;'
  id: totrans-1093
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 550,400 张图像 &#124;'
- en: '| 2020 |'
  id: totrans-1094
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '|'
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [OpenEDS-20](https://research.facebook.com/openeds-2020-challenge/) [[34](#bib.bib34)]
    &#124;'
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [OpenEDS-20](https://research.facebook.com/openeds-2020-challenge/) [[34](#bib.bib34)]
    &#124;'
- en: '&#124; Track 2: Sparse &#124;'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 路径 2：稀疏 &#124;'
- en: '&#124; Temporal Semantic &#124;'
  id: totrans-1098
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 时序语义 &#124;'
- en: '&#124; Segmentation &#124;'
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分割 &#124;'
- en: '| 90 | Seg. |'
  id: totrans-1100
  prefs: []
  type: TYPE_TB
  zh: '| 90 | 分割 |'
- en: '&#124; Image &#124;'
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $640\times 400$ &#124;'
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$640\times 400$ &#124;'
- en: '| Unknown | $\pm 20$°, $\pm 20$° | In |'
  id: totrans-1103
  prefs: []
  type: TYPE_TB
  zh: '| 未知 | $\pm 20$°, $\pm 20$° | 在 |'
- en: '&#124; SegNet [[163](#bib.bib163)] &#124;'
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SegNet [[163](#bib.bib163)] &#124;'
- en: '&#124; (Power &#124;'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; （功率 &#124;'
- en: '&#124; Efficient version) &#124;'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高效版本) &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 200 Seq. &#124;'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计：200 序列 &#124;'
- en: '&#124; 29,500 img. &#124;'
  id: totrans-1110
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 29,500 张图像 &#124;'
- en: '| 2020 |'
  id: totrans-1111
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [mEBAL](https://github.com/BiDAlab/mEBAL)[[142](#bib.bib142)] | 38 | EB |'
  id: totrans-1112
  prefs: []
  type: TYPE_TB
  zh: '| [mEBAL](https://github.com/BiDAlab/mEBAL)[[142](#bib.bib142)] | 38 | EB |'
- en: '&#124; Image &#124;'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $1280\times 720$ &#124;'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$1280\times 720$ &#124;'
- en: '| Variable | Variable | In |'
  id: totrans-1115
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 变量 | 在 |'
- en: '&#124; VGG-16 Varient &#124;'
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VGG-16 变体 &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 756,000 &#124;'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总计：756,000 &#124;'
- en: '| 2020 |'
  id: totrans-1120
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [ETH-XGaze](https://ait.ethz.ch/projects/2020/ETH-XGaze/) [[94](#bib.bib94)]
    | 110 | 3-D |'
  id: totrans-1121
  prefs: []
  type: TYPE_TB
  zh: '| [ETH-XGaze](https://ait.ethz.ch/projects/2020/ETH-XGaze/) [[94](#bib.bib94)]
    | 110 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Dim.: $6000\times 4000$ &#124;'
  id: totrans-1123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$6000\times 4000$ &#124;'
- en: '| $\pm 80$°, $\pm 80$° | $\pm 120$°, $\pm 70$° | In |'
  id: totrans-1124
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 80$°, $\pm 80$° | $\pm 120$°, $\pm 70$° | 在 |'
- en: '&#124; ResNet-50 &#124;'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-50 &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 1,083,492 &#124;'
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：1,083,492 &#124;'
- en: '| 2020 |'
  id: totrans-1129
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [EVE](https://ait.ethz.ch/projects/2020/EVE/) [[148](#bib.bib148)] | 54 |
    3-D |'
  id: totrans-1130
  prefs: []
  type: TYPE_TB
  zh: '| [EVE](https://ait.ethz.ch/projects/2020/EVE/) [[148](#bib.bib148)] | 54 |
    3-D |'
- en: '&#124; Image &#124;'
  id: totrans-1131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图片 &#124;'
- en: '&#124; Dim.: $6000\times 4000$ &#124;'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$6000\times 4000$ &#124;'
- en: '| $\pm 80$°, $\pm 80$° | $\pm 80$°, $\pm 80$° | In |'
  id: totrans-1133
  prefs: []
  type: TYPE_TB
  zh: '| $\pm 80$°, $\pm 80$° | $\pm 80$°, $\pm 80$° | 在 |'
- en: '&#124; ResNet-18 &#124;'
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-18 &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1136
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 12,308,334 &#124;'
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：12,308,334 &#124;'
- en: '| 2020 |'
  id: totrans-1138
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [GW](http://www.cis.rit.edu/%C2%A0rsk3900/gaze-in-wild/) [[179](#bib.bib179)]
    | 19 | GE |'
  id: totrans-1139
  prefs: []
  type: TYPE_TB
  zh: '| [GW](http://www.cis.rit.edu/%C2%A0rsk3900/gaze-in-wild/) [[179](#bib.bib179)]
    | 19 | GE |'
- en: '&#124; Image &#124;'
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图片 &#124;'
- en: '&#124; Dim.: $1920\times 1080$ &#124;'
  id: totrans-1141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$1920\times 1080$ &#124;'
- en: '| Variable | Variable | In |'
  id: totrans-1142
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 变量 | 在 |'
- en: '&#124; RNN &#124;'
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RNN &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: $\sim$ 5,800,000 &#124;'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：$\sim$ 5,800,000 &#124;'
- en: '| 2020 |'
  id: totrans-1147
  prefs: []
  type: TYPE_TB
  zh: '| 2020 |'
- en: '| [LAEO](https://github.com/AVAuco/ucolaeodb) [[110](#bib.bib110)] | 485 |
    3-D |'
  id: totrans-1148
  prefs: []
  type: TYPE_TB
  zh: '| [LAEO](https://github.com/AVAuco/ucolaeodb) [[110](#bib.bib110)] | 485 |
    3-D |'
- en: '&#124; Image &#124;'
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图片 &#124;'
- en: '&#124; Dim.: Variable &#124;'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：变量 &#124;'
- en: '| Variable | Variable | Both |'
  id: totrans-1151
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 变量 | 两者 |'
- en: '&#124; ResNet-18+LSTM &#124;'
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-18+LSTM &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 800,000 &#124;'
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：800,000 &#124;'
- en: '| 2021 |'
  id: totrans-1156
  prefs: []
  type: TYPE_TB
  zh: '| 2021 |'
- en: '| [GOO](https://github.com/upeee/GOO-GAZE2021) [[193](#bib.bib193)] | 100 |
    3-D |'
  id: totrans-1157
  prefs: []
  type: TYPE_TB
  zh: '| [GOO](https://github.com/upeee/GOO-GAZE2021) [[193](#bib.bib193)] | 100 |
    3-D |'
- en: '&#124; Image &#124;'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图片 &#124;'
- en: '&#124; Dim.: Variable &#124;'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：变量 &#124;'
- en: '| Variable | Variable | Both |'
  id: totrans-1160
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 变量 | 两者 |'
- en: '&#124; ResNet-50 &#124;'
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ResNet-50 &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 201,552 &#124;'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：201,552 &#124;'
- en: '| 2021 |'
  id: totrans-1165
  prefs: []
  type: TYPE_TB
  zh: '| 2021 |'
- en: '| [OpenNEEDS](https://research.facebook.com/publications/openneeds-a-dataset-of-gaze-head-hand-and-scene-signals-during-exploration-in-open-ended-vr-environments/) [[194](#bib.bib194)]
    | 44 | 3-D |'
  id: totrans-1166
  prefs: []
  type: TYPE_TB
  zh: '| [OpenNEEDS](https://research.facebook.com/publications/openneeds-a-dataset-of-gaze-head-hand-and-scene-signals-during-exploration-in-open-ended-vr-environments/)
    [[194](#bib.bib194)] | 44 | 3-D |'
- en: '&#124; Image &#124;'
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图片 &#124;'
- en: '&#124; Dim.: $128\times 71$ &#124;'
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺寸：$128\times 71$ &#124;'
- en: '| Variable | Variable | VR |'
  id: totrans-1169
  prefs: []
  type: TYPE_TB
  zh: '| 变量 | 变量 | VR |'
- en: '&#124; GBRT &#124;'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GBRT &#124;'
- en: '&#124; Eval.: Hold out &#124;'
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 评估：保留 &#124;'
- en: '|'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Total: 2,086,507 &#124;'
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 总数：2,086,507 &#124;'
- en: '| 2021 |'
  id: totrans-1174
  prefs: []
  type: TYPE_TB
  zh: '| 2021 |'
- en: 'Table IV: Cross-Dataset Study. Cross dataset generalization study on different
    gaze estimation datasets in terms of angular error (in °).'
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：跨数据集研究。不同凝视估计数据集在角度误差（以°为单位）上的跨数据集泛化研究。
- en: '| Model |'
  id: totrans-1176
  prefs: []
  type: TYPE_TB
  zh: '| 模型 |'
- en: '&#124; Test$\rightarrow$ &#124;'
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 测试$\rightarrow$ &#124;'
- en: '&#124; Train &#124;'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练 &#124;'
- en: '&#124; $\downarrow$ &#124;'
  id: totrans-1179
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; $\downarrow$ &#124;'
- en: '|'
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Datasets &#124;'
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据集 &#124;'
- en: '|'
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| --- | --- | --- |'
  id: totrans-1183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Pinball-LSTM [[90](#bib.bib90)] |'
  id: totrans-1184
  prefs: []
  type: TYPE_TB
  zh: '| Pinball-LSTM [[90](#bib.bib90)] |'
- en: '&#124; CAVE &#124;'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAVE &#124;'
- en: '&#124; MPIIFace &#124;'
  id: totrans-1186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPIIFace &#124;'
- en: '&#124; RT-GENE &#124;'
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RT-GENE &#124;'
- en: '&#124; Gaze360 &#124;'
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Gaze360 &#124;'
- en: '|'
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CAVE &#124; MPIIFace &#124; RT-GENE &#124; Gaze360 &#124;'
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAVE &#124; MPIIFace &#124; RT-GENE &#124; Gaze360 &#124;'
- en: '&#124; – &#124; 12.3° &#124; 32.8° &#124; 57.9° &#124;'
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; – &#124; 12.3° &#124; 32.8° &#124; 57.9° &#124;'
- en: '&#124; 12.4° &#124; – &#124; 26.5° &#124; 57.8° &#124;'
  id: totrans-1192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 12.4° &#124; – &#124; 26.5° &#124; 57.8° &#124;'
- en: '&#124; 24.2° &#124; 18.9 &#124; – &#124; 56.6° &#124;'
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 24.2° &#124; 18.9 &#124; – &#124; 56.6° &#124;'
- en: '&#124; 9.0° &#124; 12.1 &#124; 13.4° &#124; – &#124;'
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 9.0° &#124; 12.1 &#124; 13.4° &#124; – &#124;'
- en: '|'
  id: totrans-1195
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| ETH-X Gaze [[94](#bib.bib94)] |'
  id: totrans-1196
  prefs: []
  type: TYPE_TB
  zh: '| ETH-X Gaze [[94](#bib.bib94)] |'
- en: '&#124; MPIIGaze &#124;'
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPIIGaze &#124;'
- en: '&#124; EYEDIAP &#124;'
  id: totrans-1198
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; EYEDIAP &#124;'
- en: '&#124; Gaze-Capture &#124;'
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Gaze-Capture &#124;'
- en: '&#124; RT-GENE &#124;'
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RT-GENE &#124;'
- en: '&#124; Gaze360 &#124;'
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Gaze360 &#124;'
- en: '&#124; ETHXGaze &#124;'
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ETHXGaze &#124;'
- en: '|'
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MPIIGaze &#124; EYEDIAP &#124; Gaze-Capture &#124; RT-GENE &#124; Gaze360
    &#124; ETH-X Gaze &#124;'
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MPIIGaze &#124; EYEDIAP &#124; Gaze-Capture &#124; RT-GENE &#124; Gaze360
    &#124; ETH-X Gaze &#124;'
- en: '&#124; – &#124; 17.9° &#124; 6.3° &#124; 14.9° &#124; 31.7° &#124; 34.9° &#124;'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; – &#124; 17.9° &#124; 6.3° &#124; 14.9° &#124; 31.7° &#124; 34.9° &#124;'
- en: '&#124; 16.9° &#124; – &#124; 14.2° &#124; 15.6° &#124; 33.7° &#124; 41.7° &#124;'
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 16.9° &#124; – &#124; 14.2° &#124; 15.6° &#124; 33.7° &#124; 41.7° &#124;'
- en: '&#124; 4.5° &#124; 13.7° &#124; – &#124; 14.7° &#124; 30.2° &#124; 29.4° &#124;'
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 4.5° &#124; 13.7° &#124; – &#124; 14.7° &#124; 30.2° &#124; 29.4° &#124;'
- en: '&#124; 12.0° &#124; 21.2° &#124; 13.2° &#124; – &#124; 34.7° &#124; 42.6° &#124;'
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 12.0° &#124; 21.2° &#124; 13.2° &#124; – &#124; 34.7° &#124; 42.6° &#124;'
- en: '&#124; 10.3° &#124; 11.3° &#124; 12.9° &#124; 26.6° &#124; – &#124; 17.0° &#124;'
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 10.3° &#124; 11.3° &#124; 12.9° &#124; 26.6° &#124; – &#124; 17.0° &#124;'
- en: '&#124; 7.5° &#124; 11.0° &#124; 10.5° &#124; 31.2° &#124; 27.3° &#124; – &#124;'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 7.5° &#124; 11.0° &#124; 10.5° &#124; 31.2° &#124; 27.3° &#124; – &#124;'
- en: '|'
  id: totrans-1211
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Data Generation/Gaze Redirection. Since gaze data collection and annotation
    is an expensive and time-consuming process, the research community moves towards
    a data generation process for benchmarking with a large variation in data attributes.
    Prior works in this domain generate both synthetic and real images. The methods
    are based on Generative Adversarial Networks (GANs). To capture the possible rotational
    variation in images, gaze redirection techniques [[160](#bib.bib160), [181](#bib.bib181),
    [182](#bib.bib182), [91](#bib.bib91), [183](#bib.bib183)] are quite popular. An
    early work on gaze manipulation [[195](#bib.bib195)] uses pre-recording of several
    potential eye replacements during test time. Further, Kononenko et al. [[196](#bib.bib196)]
    propose wrapping based gaze redirection using supervised learning, which learns
    the gaze redirection via a flow field to move eye pupil and relevant pixels from
    the input image to the output image. The gaze re-direction methods may struggle
    with extrapolation since it depends on the training samples and training methods.
    Moreover, these works suffer from low-quality generation and low redirection precision.
    To overcome this, Chen et al. [[100](#bib.bib100)] propose a MultiModal-Guided
    Gaze Redirection (MGGR) framework which uses gaze-map images and target angles
    to adjust a given eye appearance via learning. The other approaches are mainly
    based on random forest [[196](#bib.bib196)] and style transfer [[197](#bib.bib197)].
    Random forest is used to decide the possible gaze direction and in style transfer,
    the appearance based feature is mainly encoded. Sela et al. [[197](#bib.bib197)]
    propose a GAN based framework to generate a large dataset of high-resolution eye
    images having diversity in subjects, head pose, camera settings and realism. However,
    the GAN based methods lack in their capability to preserve content (i.e. eye shape)
    for benchmarking. Buhler et al. [[166](#bib.bib166)] synthesize person-specific
    eye images with a given semantic segmentation mask by preserving the style and
    content of the reference images. In summary, we can say that although a lot of
    effort has been made to generate realistic eye images, but due to several limitations
    (perfect gaze direction, image quality), these images are not used for benchmarking.
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: 数据生成/注视重定向。由于注视数据的收集和标注是一个昂贵且耗时的过程，研究社区转向了一种具有大量数据属性变异的数据生成过程来进行基准测试。该领域的先前工作生成了合成和真实图像。这些方法基于生成对抗网络（GANs）。为了捕捉图像中的可能旋转变化，注视重定向技术[[160](#bib.bib160),
    [181](#bib.bib181), [182](#bib.bib182), [91](#bib.bib91), [183](#bib.bib183)]非常流行。早期的注视操控工作[[195](#bib.bib195)]使用了测试期间对几个潜在眼睛替代物的预录制。此外，Kononenko
    等人[[196](#bib.bib196)]提出了基于包装的注视重定向方法，该方法通过流场来学习注视重定向，以将眼球和相关像素从输入图像移动到输出图像。由于依赖于训练样本和训练方法，注视重定向方法可能在外推时遇到困难。此外，这些工作还存在生成质量低和重定向精度低的问题。为了解决这个问题，Chen
    等人[[100](#bib.bib100)]提出了一个多模态引导注视重定向（MGGR）框架，该框架利用注视图像和目标角度来通过学习调整给定的眼睛外观。其他方法主要基于随机森林[[196](#bib.bib196)]和风格迁移[[197](#bib.bib197)]。随机森林用于决定可能的注视方向，而风格迁移则主要编码基于外观的特征。Sela
    等人[[197](#bib.bib197)]提出了一个基于GAN的框架，用于生成具有受试者、头部姿势、相机设置和真实性多样性的高分辨率眼睛图像的大型数据集。然而，基于GAN的方法在保持内容（即眼睛形状）方面的能力不足以进行基准测试。Buhler
    等人[[166](#bib.bib166)]通过保持参考图像的风格和内容，使用给定的语义分割掩码合成特定于人的眼睛图像。总之，尽管为了生成逼真的眼睛图像付出了很多努力，但由于若干限制（完美的注视方向、图像质量），这些图像尚未用于基准测试。
- en: 5.2 Evaluation Strategy
  id: totrans-1213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 评估策略
- en: In this section, we describe the most widely used gaze metrics in the gaze analysis
    domain.
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了注视分析领域中最广泛使用的注视指标。
- en: Gaze Estimation. The most common practice to measure the gaze estimation accuracy/error
    is in terms of angular error (in °) [[26](#bib.bib26), [94](#bib.bib94), [29](#bib.bib29),
    [148](#bib.bib148)] and gaze location (in pixels or cm/mm(s)) [[87](#bib.bib87),
    [148](#bib.bib148)]. The angular error is measured between the actual gaze direction
    ($\mathbf{g}\in\mathbb{R}^{3}$) and predicted gaze direction ($\hat{\mathbf{g}}\in\mathbb{R}^{3}$)
    defined as $\frac{\mathbf{g}.\hat{\mathbf{g}}}{||\mathbf{g}||.||\hat{\mathbf{g}}||}$.
    On the other hand, Euclidean distance is measured between the original and predicted
    point of gaze (PoG).
  id: totrans-1215
  prefs: []
  type: TYPE_NORMAL
  zh: 注视估计。测量注视估计准确性/误差的最常见做法是以角度误差（以 ° 为单位）[[26](#bib.bib26), [94](#bib.bib94), [29](#bib.bib29),
    [148](#bib.bib148)] 和注视位置（以像素或厘米/毫米为单位）[[87](#bib.bib87), [148](#bib.bib148)]
    来衡量。角度误差是实际注视方向（$\mathbf{g}\in\mathbb{R}^{3}$）和预测注视方向（$\hat{\mathbf{g}}\in\mathbb{R}^{3}$）之间的测量，定义为
    $\frac{\mathbf{g}.\hat{\mathbf{g}}}{||\mathbf{g}||.||\hat{\mathbf{g}}||}$。另一方面，欧几里得距离是测量原始注视点（PoG）和预测点之间的距离。
- en: Gaze Redirection. The gaze redirection evaluation is performed in both quantitative
    and qualitative manner [[101](#bib.bib101), [100](#bib.bib100), [102](#bib.bib102)].
    The quantitative analysis is done in terms of angular gaze redirection error estimated
    between the predicted values and their intended target values. As in this task,
    the moment of the eye pupil is pre-defined, thus, this angular error weakly quantifies
    how perfectly the eye redirection occurs, although the method for measuring the
    angle has some inherent noise. For qualitative analysis, the Learned Perceptual
    Image Patch Similarity (LPIPS) metric is used which measures the paired image
    similarity in the gaze redirection task.
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: 注视重定向。注视重定向评估以定量和定性方式进行 [[101](#bib.bib101), [100](#bib.bib100), [102](#bib.bib102)]。定量分析是通过估计预测值和其目标值之间的角度注视重定向误差来完成的。在这项任务中，眼睛瞳孔的运动是预定义的，因此这个角度误差在一定程度上量化了眼睛重定向的准确性，尽管测量角度的方法存在一些固有噪声。对于定性分析，使用学习的感知图像块相似性（LPIPS）指标，该指标用于衡量注视重定向任务中配对图像的相似性。
- en: Eye Segmentation. Commonly used evaluation metric for eye segmentation methods,
    is average of the mean Intersection over Union (mIoU). Although, for the recent
    OpenEDS challenge [[32](#bib.bib32)], the mIoU metric is calculated for all classes
    and model size (S) is calculated as a function of a number of trainable parameters
    in megabytes (MB).
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 眼睛分割。眼睛分割方法常用的评估指标是平均交并比（mIoU）。尽管如此，对于最近的 OpenEDS 挑战 [[32](#bib.bib32)]，mIoU
    指标是针对所有类别计算的，而模型大小（S）则作为可训练参数数量（以兆字节 MB 为单位）的函数来计算。
- en: 5.3 Cross Dataset Analysis.
  id: totrans-1218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 跨数据集分析。
- en: Datasets play an important role in defining the research progress made in gaze
    analysis. Apart from serving as a source for training models, it helps to quantify
    the performance measure. In the gaze analysis domain, the aim of dataset curation
    is to capture the real-world scenario setting as close as possible. Thus, it is
    necessary to evaluate the robustness and generalizability of the models across
    different data acquisition setups for better adaptation.
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在定义注视分析研究进展中发挥着重要作用。除了作为训练模型的来源外，它还帮助量化性能度量。在注视分析领域，数据集策划的目标是尽可能接近真实世界场景设置。因此，有必要评估模型在不同数据采集设置下的鲁棒性和泛化能力，以便更好地适应。
- en: 'On this front, we explore two aspects: First of all, we explore the cross-dataset
    generalizability of gaze estimation methods based on two SOTA models i.e. Pinball-LSTM [[90](#bib.bib90)]
    and ETH-X-Gaze [[94](#bib.bib94)]. For this purpose, the training is performed
    on one dataset while the testing is conducted on the other dataset. (Refer Table [IV](#S5.T4
    "Table IV ‣ 5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")). Further, we explore the SOTA method’s
    performance on different datasets to show the robustness of the model (Refer Table [V](#S5.T5
    "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")). Angular error (in °) is used as
    an evaluation metric. Further to generalize across datasets, we calculate the
    mean angular error across datasets. Below are some of the important observations
    inferred from our experimental results.'
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: '在这方面，我们探讨了两个方面：首先，我们探讨了基于两个最先进模型（即Pinball-LSTM [[90](#bib.bib90)]和ETH-X-Gaze [[94](#bib.bib94)]）的注视估计方法的跨数据集泛化能力。为此，训练是在一个数据集上进行，而测试则在另一个数据集上进行。（参见表[IV](#S5.T4
    "Table IV ‣ 5.1 Datasets for Gaze Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")）。进一步地，我们探讨了最先进方法在不同数据集上的表现，以展示模型的鲁棒性（参见表[V](#S5.T5
    "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")）。角度误差（以°为单位）被用作评估指标。为了在数据集间泛化，我们计算了跨数据集的平均角度误差。以下是从我们的实验结果中得出的一些重要观察结果。'
- en: Data Collection Settings. Dataset collection setup plays an important role in
    generalizability and method’s robustness. As the CAVE dataset is collected in
    a constrained setup and it has high-resolution images, the models trained on this
    data fail to adapt well to the data with low resolution and synthetic images.
    Thus, the pinball-LSTM trained on CAVE data has a high error in Gaze360 ($\sim$
    57.9°) and RT-GENE ($\sim$ 32.8°) datasets. A similar pattern is observed in the
    case of the MPII dataset as well. Model trained on this dataset gives high error
    in adapting RT-GENE ($\sim$ 26.5°) and Gaze360 ($\sim$ 57.8°).
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: 数据收集设置。数据集收集设置在泛化能力和方法的鲁棒性中发挥着重要作用。由于CAVE数据集是在受限设置下收集的，并且具有高分辨率图像，因此在此数据上训练的模型无法很好地适应低分辨率和合成图像的数据。因此，在CAVE数据上训练的pinball-LSTM在Gaze360（$\sim$
    57.9°）和RT-GENE（$\sim$ 32.8°）数据集中误差较大。在MPII数据集中也观察到了类似的模式。在该数据集上训练的模型在适应RT-GENE（$\sim$
    26.5°）和Gaze360（$\sim$ 57.8°）时误差较大。
- en: 'Cross Dataset Generalization. By observing the cross dataset generalization
    performance, we can determine how diverse the training dataset is from a generalization
    perspective. From Table [IV](#S5.T4 "Table IV ‣ 5.1 Datasets for Gaze Analysis
    ‣ 5 Validation ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches"),
    we observe that Gaze360, Gaze-Capture, and ETH-X Gaze datasets are the most challenging
    datasets. Training models on these two datasets would be a good choice as it has
    better generalization performance across different datasets. In contrast, RT-GENE
    and Gaze-capture contain significant biases and training models on them will lead
    to poor cross-dataset generalization performance.'
  id: totrans-1222
  prefs: []
  type: TYPE_NORMAL
  zh: '跨数据集泛化。通过观察跨数据集的泛化表现，我们可以确定训练数据集从泛化角度的多样性。从表[IV](#S5.T4 "Table IV ‣ 5.1 Datasets
    for Gaze Analysis ‣ 5 Validation ‣ Automatic Gaze Analysis: A Survey of Deep Learning
    based Approaches")中，我们观察到Gaze360、Gaze-Capture和ETH-X Gaze数据集是最具挑战性的数据集。在这两个数据集上训练模型是一个不错的选择，因为它在不同数据集上的泛化表现更好。相反，RT-GENE和Gaze-Capture数据集包含显著的偏差，在它们上面训练的模型将导致较差的跨数据集泛化表现。'
- en: Robust Modelling. In order to study the robustness of a model trained on any
    dataset, we recommend evaluating the model on Gaze360, Gaze-Capture, and ETH-X
    Gaze datasets. These datasets exhibit multiple variations in terms of background
    environment, eye visibility, occlusion, low-resolution images and could serve
    as important indicators for real-world adaptation. We also recommend training
    the models on all benchmark datasets together and expect a better generalization
    than training on individual datasets in novel or in-the-wild settings.
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: 鲁棒建模。为了研究在任何数据集上训练的模型的鲁棒性，我们建议在Gaze360、Gaze-Capture和ETH-X Gaze数据集上评估模型。这些数据集在背景环境、眼睛可见度、遮挡、低分辨率图像等方面表现出多种变化，并可能作为实际应用的重要指标。我们还建议将模型在所有基准数据集上一起训练，并期望比在单个数据集上训练获得更好的泛化效果，无论是在新环境还是实际环境中。
- en: 'Table V: Where we stand now. Chronological comparison of the performance of
    different models for the gaze-related tasks on related benchmark datasets.'
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：我们目前的情况。不同模型在相关基准数据集上的注视相关任务的性能的时间比较。
- en: '| Task | Methods | Datasets | Year |'
  id: totrans-1225
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 方法 | 数据集 | 年份 |'
- en: '|'
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Gaze &#124;'
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注视 &#124;'
- en: '&#124; Estimation &#124;'
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计 &#124;'
- en: '|'
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GazeNet [[24](#bib.bib24)] &#124;'
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GazeNet [[24](#bib.bib24)] &#124;'
- en: '&#124; Dilated-Conv. [[64](#bib.bib64)] &#124;'
  id: totrans-1231
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Dilated-Conv. [[64](#bib.bib64)] &#124;'
- en: '&#124; Landmark based [[97](#bib.bib97)] &#124;'
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于地标 [[97](#bib.bib97)] &#124;'
- en: '&#124; RT-GENE [[54](#bib.bib54)] &#124;'
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RT-GENE [[54](#bib.bib54)] &#124;'
- en: '&#124; Pinball-LSTM [[90](#bib.bib90)] &#124;'
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Pinball-LSTM [[90](#bib.bib90)] &#124;'
- en: '&#124; CA-Net [[59](#bib.bib59)] &#124;'
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CA-Net [[59](#bib.bib59)] &#124;'
- en: '&#124; [GazeTR-Hybrid](https://github.com/yihuacheng/GazeTR) [[149](#bib.bib149)]
    &#124;'
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [GazeTR-Hybrid](https://github.com/yihuacheng/GazeTR) [[149](#bib.bib149)]
    &#124;'
- en: '|'
  id: totrans-1237
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CAVE &#124; MPIIGaze &#124; EYEDIAP &#124; UT MV &#124; MPIIFace &#124;
    Gaze360 &#124; ETH-X Gaze &#124;'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAVE &#124; MPIIGaze &#124; EYEDIAP &#124; UT MV &#124; MPIIFace &#124;
    Gaze360 &#124; ETH-X Gaze &#124;'
- en: '&#124; – &#124; 5.70° &#124; 7.13° &#124; 6.44° &#124; 5.76° &#124; – &#124;
    – &#124;'
  id: totrans-1239
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; – &#124; 5.70° &#124; 7.13° &#124; 6.44° &#124; 5.76° &#124; – &#124;
    – &#124;'
- en: '&#124; – &#124; 4.39° &#124; 6.57° &#124; – &#124; 4.42° &#124; 13.73° &#124;
    – &#124;'
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; – &#124; 4.39° &#124; 6.57° &#124; – &#124; 4.42° &#124; 13.73° &#124;
    – &#124;'
- en: '&#124; 8.7° &#124; 8.3° &#124; 26.6° &#124; – &#124; – &#124; – &#124; – &#124;'
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 8.7° &#124; 8.3° &#124; 26.6° &#124; – &#124; – &#124; – &#124; – &#124;'
- en: '&#124; – &#124; 4.61° &#124; 6.30° &#124; – &#124; 4.66° &#124; 12.26 &#124;
    – &#124;'
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; – &#124; 4.61° &#124; 6.30° &#124; – &#124; 4.66° &#124; 12.26 &#124;
    – &#124;'
- en: '&#124; 9.0° &#124; 12.1° &#124; 5.58° &#124; – &#124; 12.1° &#124; 11.04° &#124;
    4.46° &#124;'
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 9.0° &#124; 12.1° &#124; 5.58° &#124; – &#124; 12.1° &#124; 11.04° &#124;
    4.46° &#124;'
- en: '&#124; – &#124; 4.27° &#124; 5.63° &#124; – &#124; 4.27° &#124; 11.20° &#124;
    – &#124;'
  id: totrans-1244
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; – &#124; 4.27° &#124; 5.63° &#124; – &#124; 4.27° &#124; 11.20° &#124;
    – &#124;'
- en: '|'
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2015 &#124;'
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2015 &#124;'
- en: '&#124; 2018 &#124;'
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2018 &#124;'
- en: '&#124; 2018 &#124;'
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2018 &#124;'
- en: '&#124; 2019 &#124;'
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2019 &#124;'
- en: '&#124; 2020 &#124;'
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2020 &#124;'
- en: '&#124; 2021 &#124;'
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2021 &#124;'
- en: '|'
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-1253
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; CAVE                                MPIIGaze                               
    Eth-X-Gaze                                Gaze360 &#124;'
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CAVE                                MPIIGaze                               
    Eth-X-Gaze                                Gaze360 &#124;'
- en: '&#124; <svg version="1.1" width="708" height="299.77" overflow="visible"><g
    transform="translate(0,299.77) scale(1,-1)"><g  transform="translate(0,0)"><g
    transform="translate(0,99) scale(1, -1)"><foreignobject width="177" height="99"
    overflow="visible">![[Uncaptioned image]](img/48a0aea43ea0880805f9894f3ed96d12.png)</foreignobject></g></g><g
    transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignobject
    width="177" height="99" overflow="visible">![[Uncaptioned image]](img/840c52201db3080a9432864eab9fe3ee.png)</foreignobject></g></g><g
    transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignobject
    width="177" height="99" overflow="visible">![[Uncaptioned image]](img/50ce207c826a845fbd20e093d130a6fa.png)</foreignobject></g></g><g
    transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignobject
    width="177" height="99" overflow="visible">![[Uncaptioned image]](img/7a7fa04f4edbb116ab0349c55d877340.png)</foreignobject></g></g></g></svg>
    &#124;'
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; <svg version="1.1" width="708" height="299.77" overflow="visible"><g
    transform="translate(0,299.77) scale(1,-1)"><g  transform="translate(0,0)"><g
    transform="translate(0,99) scale(1, -1)"><foreignobject width="177" height="99"
    overflow="visible">![[无标题图像]](img/48a0aea43ea0880805f9894f3ed96d12.png)</foreignobject></g></g><g
    transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignobject
    width="177" height="99" overflow="visible">![[无标题图像]](img/840c52201db3080a9432864eab9fe3ee.png)</foreignobject></g></g><g
    transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignobject
    width="177" height="99" overflow="visible">![[无标题图像]](img/50ce207c826a845fbd20e093d130a6fa.png)</foreignobject></g></g><g
    transform="translate(0,0)"><g transform="translate(0,99) scale(1, -1)"><foreignobject
    width="177" height="99" overflow="visible">![[无标题图像]](img/7a7fa04f4edbb116ab0349c55d877340.png)</foreignobject></g></g></g></svg>
    &#124;'
- en: '|'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| In the diagrams, the y-axis represents the angular error (in °) and the x-axis
    represents the timeline. |'
  id: totrans-1257
  prefs: []
  type: TYPE_TB
  zh: '| 在图表中，y轴表示角度误差（以°为单位），x轴表示时间轴。 |'
- en: '| Task | Datasets | Methods (Eval.AI) | Year |'
  id: totrans-1258
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 方法（评估AI） | 年份 |'
- en: '|'
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Trajectory &#124;'
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 轨迹 &#124;'
- en: '&#124; Estimation &#124;'
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计 &#124;'
- en: '|'
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Team-name $\rightarrow$ &#124;'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 团队名称 $\rightarrow$ &#124;'
- en: '&#124; OpenEDS2020 [[34](#bib.bib34)] &#124;'
  id: totrans-1264
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; OpenEDS2020 [[34](#bib.bib34)] &#124;'
- en: '&#124; Team-name $\rightarrow$ &#124;'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 团队名称 $\rightarrow$ &#124;'
- en: '&#124; OpenNEEDS [[194](#bib.bib194)] &#124;'
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; OpenNEEDS [[194](#bib.bib194)] &#124;'
- en: '|'
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Random_B &#124; caixin &#124; EyMazing &#124; fgp200709d &#124; vipl_gaze
    &#124; Baseline &#124;'
  id: totrans-1268
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Random_B &#124; caixin &#124; EyMazing &#124; fgp200709d &#124; vipl_gaze
    &#124; 基线 &#124;'
- en: '&#124; [3.078°](https://repushko.com/all/openeds2020/) &#124; 3.248° &#124;
    3.313° &#124; 3.347° &#124; 3.386° &#124; 5.368° &#124;'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [3.078°](https://repushko.com/all/openeds2020/) &#124; 3.248° &#124;
    3.313° &#124; 3.347° &#124; 3.386° &#124; 5.368° &#124;'
- en: '&#124; XiaodongWang &#124; Hebut_Lyx &#124; tetelias &#124; TCS_Research &#124;
    Baseline &#124; AnotherShot &#124;'
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; XiaodongWang &#124; Hebut_Lyx &#124; tetelias &#124; TCS_Research &#124;
    Baseline &#124; AnotherShot &#124;'
- en: '&#124; 1.68° &#124; 1.75° &#124; 1.99° &#124; 2.05° &#124; 7.18° &#124; 7.94°
    &#124;'
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1.68° &#124; 1.75° &#124; 1.99° &#124; 2.05° &#124; 7.18° &#124; 7.94°
    &#124;'
- en: '|'
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [2020](https://eval.ai/web/challenges/challenge-page/605/leaderboard/1682)
    &#124;'
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [2020](https://eval.ai/web/challenges/challenge-page/605/leaderboard/1682)
    &#124;'
- en: '&#124; [2021](https://eval.ai/web/challenges/challenge-page/895/leaderboard/2361)
    &#124;'
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [2021](https://eval.ai/web/challenges/challenge-page/895/leaderboard/2361)
    &#124;'
- en: '|'
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Task | Dataset | Methods | Year |'
  id: totrans-1276
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 方法 | 年份 |'
- en: '|'
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Gaze &#124;'
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 目光 &#124;'
- en: '&#124; Zone &#124;'
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 区域 &#124;'
- en: '|'
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; [EmotiW2020](http://users.cecs.anu.edu.au/%C2%A0Tom.Gedeon/pdfs/Emotiw%202020%20Driver%20gaze%20group%20emotion%20student%20engagement%20and%20physiological%20signal%20based%20challenges.pdf)
    $\rightarrow$ &#124;'
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [EmotiW2020](http://users.cecs.anu.edu.au/%C2%A0Tom.Gedeon/pdfs/Emotiw%202020%20Driver%20gaze%20group%20emotion%20student%20engagement%20and%20physiological%20signal%20based%20challenges.pdf)
    $\rightarrow$ &#124;'
- en: '&#124; DGW [[40](#bib.bib40)] &#124;'
  id: totrans-1282
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DGW [[40](#bib.bib40)] &#124;'
- en: '|'
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DD_Vision &#124; SituAlgorithm &#124; Overfit &#124; DeepBlueAI &#124;
    UDECE &#124; X-AWARE &#124; Baseline &#124;'
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DD_Vision &#124; SituAlgorithm &#124; Overfit &#124; DeepBlueAI &#124;
    UDECE &#124; X-AWARE &#124; Baseline &#124;'
- en: '&#124; 82.52% &#124; 81.51% &#124; 78.87% &#124; 75.88% &#124; 74.57% &#124;
    71.62% &#124; 60.98% &#124;'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 82.52% &#124; 81.51% &#124; 78.87% &#124; 75.88% &#124; 74.57% &#124;
    71.62% &#124; 60.98% &#124;'
- en: '|'
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2020 &#124;'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2020 &#124;'
- en: '|'
  id: totrans-1288
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Task | Datasets | Methods | Year |'
  id: totrans-1289
  prefs: []
  type: TYPE_TB
  zh: '| 任务 | 数据集 | 方法 | 年份 |'
- en: '|'
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Visual &#124;'
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉 &#124;'
- en: '&#124; Attention &#124;'
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Attention &#124;'
- en: '|'
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; GazeFollowing [[113](#bib.bib113)] (AUC $\uparrow$) &#124;'
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GazeFollowing [[113](#bib.bib113)] (AUC $\uparrow$) &#124;'
- en: '&#124; GazeCommunication [[111](#bib.bib111)] &#124;'
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; GazeCommunication [[111](#bib.bib111)] &#124;'
- en: '&#124; 1\. Atomic-Level (Top-1%) &#124;'
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1\. 原子级别（Top-1%） &#124;'
- en: '&#124; 2\. Event-Level (Top-1%) &#124;'
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2\. 事件级别（Top-1%） &#124;'
- en: '&#124; VisualSearch [[118](#bib.bib118)] &#124;'
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; VisualSearch [[118](#bib.bib118)] &#124;'
- en: '&#124; 1\. Microwave (MultiMatch) &#124;'
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 1\. 微波炉（多重匹配） &#124;'
- en: '&#124; 2\. Clock (MultiMatch) &#124;'
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2\. 时钟（多重匹配） &#124;'
- en: '&#124; SharedAttention [[114](#bib.bib114)] &#124;'
  id: totrans-1301
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; SharedAttention [[114](#bib.bib114)] &#124;'
- en: '|'
  id: totrans-1302
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Human &#124; [[114](#bib.bib114)] &#124; [[62](#bib.bib62)] &#124; [[113](#bib.bib113)]
    &#124; Center &#124; Random &#124;'
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人工 &#124; [[114](#bib.bib114)] &#124; [[62](#bib.bib62)] &#124; [[113](#bib.bib113)]
    &#124; Center &#124; Random &#124;'
- en: '&#124; 0.924 &#124; 0.921 &#124; 0.896 &#124; 0.878 &#124; 0.633 &#124; 0.504
    &#124;'
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.924 &#124; 0.921 &#124; 0.896 &#124; 0.878 &#124; 0.633 &#124; 0.504
    &#124;'
- en: '&#124; ST-GNN &#124; CNN+LSTM &#124; CNN+SVM &#124; CNN+RF &#124; CNN &#124;
    Chance &#124;'
  id: totrans-1305
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ST-GNN &#124; CNN+LSTM &#124; CNN+SVM &#124; CNN+RF &#124; CNN &#124;
    Chance &#124;'
- en: '&#124; 55.02% &#124; 24.65% &#124; 36.23% &#124; 37.68% &#124; 23.05% &#124;
    16.44% &#124;'
  id: totrans-1306
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 55.02% &#124; 24.65% &#124; 36.23% &#124; 37.68% &#124; 23.05% &#124;
    16.44% &#124;'
- en: '&#124; 55.90% &#124; – &#124; – &#124; – &#124; – &#124; 22.70% &#124;'
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 55.90% &#124; – &#124; – &#124; – &#124; – &#124; 22.70% &#124;'
- en: '&#124;'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; Behavioural &#124;'
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行为 &#124;'
- en: '&#124; Agreement &#124;'
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 协议 &#124;'
- en: '&#124; CNN &#124;'
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; CNN &#124;'
- en: '&#124; RNN &#124;'
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; RNN &#124;'
- en: '&#124; LSTM &#124; GRU &#124; Scanpath &#124;'
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; LSTM &#124; GRU &#124; 扫描路径 &#124;'
- en: '&#124; 0.714 &#124; 0.621 &#124; 0.677 &#124; 0.684 &#124; 0.664 &#124; Direction
    &#124;'
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.714 &#124; 0.621 &#124; 0.677 &#124; 0.684 &#124; 0.664 &#124; Direction
    &#124;'
- en: '&#124; 0.701 &#124; 0.633 &#124; 0.673 &#124; 0.669 &#124; 0.659 &#124; Direction
    &#124;'
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 0.701 &#124; 0.633 &#124; 0.673 &#124; 0.669 &#124; 0.659 &#124; Direction
    &#124;'
- en: '&#124;'
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;'
- en: '&#124; ST-CNN &#124;'
  id: totrans-1317
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ST-CNN &#124;'
- en: '&#124; +LSTM &#124;'
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; +LSTM &#124;'
- en: '&#124; ST-GNN &#124;'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ST-GNN &#124;'
- en: '&#124; Gaze+Saliency &#124;'
  id: totrans-1320
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Gaze+Saliency &#124;'
- en: '&#124; +LSTM &#124;'
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; +LSTM &#124;'
- en: '&#124; Gaze+Saliency &#124; GazeFollow &#124; Random &#124;'
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Gaze+Saliency &#124; GazeFollow &#124; Random &#124;'
- en: '&#124; 83.3% &#124; 71.4% &#124; 66.2% &#124; 59.4% &#124; 58.7% &#124; 22.70%
    &#124;'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 83.3% &#124; 71.4% &#124; 66.2% &#124; 59.4% &#124; 58.7% &#124; 22.70%
    &#124;'
- en: '|'
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; 2015 &#124;'
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2015 &#124;'
- en: '&#124; 2019 &#124;'
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2019 &#124;'
- en: '&#124; 2019 &#124;'
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2019 &#124;'
- en: '&#124; 2020 &#124;'
  id: totrans-1328
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 2020 &#124;'
- en: '|'
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 5.4 Where We Stand Now?
  id: totrans-1330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 我们现在的现状？
- en: 'In Table [V](#S5.T5 "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣
    Automatic Gaze Analysis: A Survey of Deep Learning based Approaches"), we analyze
    dataset specific improvements made by different methods over the past few years.
    In the following, we discuss some of the important observations from Table [V](#S5.T5
    "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches").'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: '在表[Table V](#S5.T5 "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣
    Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")中，我们分析了过去几年不同方法在数据集特定改进方面的表现。接下来，我们讨论一些来自表[Table
    V](#S5.T5 "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣ Automatic Gaze
    Analysis: A Survey of Deep Learning based Approaches")的重要观察结果。'
- en: 'Gaze Estimation on Constrained Setup. Most popular 2D-3D gaze estimation datasets [[50](#bib.bib50),
    [94](#bib.bib94), [23](#bib.bib23)] are collected in constrained scenarios where
    there is a certain distance between the user and the visual screen. Moreover,
    as the nodal point of the human eye has subject-specific offset (which varies
    around 2-3°), it is difficult to reduce the angular error beyond a certain limit
    using visible regions of the eyes. On this front, the performance of some of the
    gaze estimation methods [[24](#bib.bib24), [51](#bib.bib51), [94](#bib.bib94)]
    seems to have plateaued on constrained datasets such as CAVE, MPII, and Eth-X-Gaze
    (Refer to Figures in Table [V](#S5.T5 "Table V ‣ 5.3 Cross Dataset Analysis. ‣
    5 Validation ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches")).'
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
  zh: '受限设置下的注视估计。大多数流行的2D-3D注视估计数据集[[50](#bib.bib50), [94](#bib.bib94), [23](#bib.bib23)]是在用户与视觉屏幕之间存在一定距离的受限场景中收集的。此外，由于人眼的结点点具有个体特定的偏移（大约2-3°），使用眼睛的可见区域很难将角度误差降低到某一极限。在这方面，一些注视估计方法[[24](#bib.bib24),
    [51](#bib.bib51), [94](#bib.bib94)]在受限数据集如CAVE、MPII和Eth-X-Gaze上的表现似乎已经达到了瓶颈（参见表格中的图[Table
    V](#S5.T5 "Table V ‣ 5.3 Cross Dataset Analysis. ‣ 5 Validation ‣ Automatic Gaze
    Analysis: A Survey of Deep Learning based Approaches")）。'
- en: Gaze Estimation in Unconstrained Setup. Gaze estimation in unconstrained environments
    still remains largely unresolved mainly due to the unavailability of large-scale
    annotated data. Gaze360 [[90](#bib.bib90)] and GazeCapture [[25](#bib.bib25)]
    are two popular public-domain datasets available for this purpose. Especially
    in the Gaze360 dataset, in many cases, the eyes are not visible which makes it
    more challenging to track where the person is looking. It is quite difficult to
    estimate gaze in a naturalistic environment, more exploration along this line
    is highly desirable.
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: 无受限设置下的注视估计。在无受限环境下的注视估计仍然大多未解决，主要由于缺乏大规模标注数据。Gaze360[[90](#bib.bib90)]和GazeCapture[[25](#bib.bib25)]是用于此目的的两个流行的公共数据集。特别是在Gaze360数据集中，很多情况下眼睛不可见，这使得追踪人的注视方向更加具有挑战性。在自然环境中估计注视非常困难，因此在这方面进行更多的探索是非常有必要的。
- en: Gaze Estimation with Limited Supervision. Gaze Estimation with limited supervision
    is a promising research direction. As manual annotation of gaze data is an error-prone
    process, there is a high possibility of noise in labeling. To this end, proposed
    approaches are mainly based on ‘learning-by-synthesis’ [[20](#bib.bib20)], hierarchical
    generative models [[92](#bib.bib92)], conditional random field [[198](#bib.bib198)],
    unsupervised gaze target discovery [[95](#bib.bib95)], few-shot learning [[29](#bib.bib29),
    [91](#bib.bib91)], pseudo-labelling [[174](#bib.bib174)] and self/unsupervised [[28](#bib.bib28),
    [27](#bib.bib27)]. While domain specific knowledge has been utilized for these
    approaches, developing robust methods from limited amount of annotated data with
    enhanced generalization across different real-life scenarios still largely remains
    unresolved.
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: 有限监督下的注视估计。有限监督下的注视估计是一个有前途的研究方向。由于人工注释注视数据是一个容易出错的过程，标注中有较高的噪声可能性。为此，提出的方法主要基于“学习-合成”[[20](#bib.bib20)]、层次生成模型[[92](#bib.bib92)]、条件随机场[[198](#bib.bib198)]、无监督注视目标发现[[95](#bib.bib95)]、少样本学习[[29](#bib.bib29),
    [91](#bib.bib91)]、伪标注[[174](#bib.bib174)]以及自我/无监督[[28](#bib.bib28), [27](#bib.bib27)]。尽管这些方法已利用了领域特定知识，但从有限标注数据中开发出具有增强泛化能力的鲁棒方法，在不同真实场景中仍然大多未解决。
- en: Visual Attention Estimation. Eye visibility plays an important role in estimating
    the gaze direction of a person. To this end, visual attention estimation mainly
    focuses on where the person is looking irrespective of eye visibility. To facilitate
    research along this direction, GazeFollow [[113](#bib.bib113)] and VideoAttentionTarget [[114](#bib.bib114)]
    datasets have been proposed. Some important research directions to explore include
    scene saliency, visual search, and human scan path [[117](#bib.bib117), [119](#bib.bib119)].
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉注意力估计。眼睛的可见性在估计一个人的注视方向中起着重要作用。为此，视觉注意力估计主要关注于个人的注视方向，而不考虑眼睛的可见性。为了推动这一方向的研究，提出了GazeFollow[[113](#bib.bib113)]和VideoAttentionTarget[[114](#bib.bib114)]数据集。一些重要的研究方向包括场景显著性、视觉搜索和人类扫描路径[[117](#bib.bib117),
    [119](#bib.bib119)]。
- en: Gaze Trajectory Modelling. Gaze Trajectory modeling and estimation is another
    line of research that requires further research attention [[34](#bib.bib34)].
    Natural gaze dynamics consist of a continuous sequence of gaze events such as
    fixations, saccades, pursuit, vestibulo-ocular reflex, optokinetic reflex, vergence,
    and blinks [rayner1995eye]. These aforementioned dynamics can be influenced by
    saliency, task-relevant information, and environmental factors. Natural eye movements
    of humans span an elliptical region with a horizontally oriented axis greater
    than $\sim 100$° and a vertically oriented axis spanning $\sim 70$°. Due to the
    lack of labeled temporal gaze trajectory data, there are only a few studies [[34](#bib.bib34),
    [146](#bib.bib146)] that focus on tasks related to the gaze trajectory.
  id: totrans-1336
  prefs: []
  type: TYPE_NORMAL
  zh: 注视轨迹建模。注视轨迹建模和估计是另一个需要进一步研究的方向[[34](#bib.bib34)]。自然注视动态包括一系列连续的注视事件，如注视、扫视、追踪、前庭眼动反射、视动反射、调节和眨眼[rayner1995eye]。这些前述的动态可以受到显著性、任务相关信息和环境因素的影响。人类的自然眼动范围为一个椭圆区域，水平轴大于
    $\sim 100$°，垂直轴跨度为 $\sim 70$°。由于缺乏标注的时间注视轨迹数据，目前只有少数几项研究[[34](#bib.bib34), [146](#bib.bib146)]关注与注视轨迹相关的任务。
- en: 6 Applications
  id: totrans-1337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 应用
- en: 6.1 Gaze in Augmented Reality, Virtual Reality and 360° Video Streaming
  id: totrans-1338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 增强现实、虚拟现实和360°视频流中的注视
- en: We are witnessing great progress in the adaptation of VR, AR and 360° Video
    Streaming technology. Eye-tracking has the potential to bring revolution in the
    AR/VR and 360° video streaming for immersive video application space since it
    can enhance the device’s awareness by learning about users’ attention at any given
    point in time. Consequently, user’s focus based optimization reduces power consumption
    by these devices [[199](#bib.bib199), [200](#bib.bib200), [34](#bib.bib34), [201](#bib.bib201)].
    In this section, we will cover the importance of eye-tracking technology and how
    it enables better user experience in AR/VR and 360° Video Streaming devices including
    eye inter pupillary distance for estimating image perception quality, person identification
    or state estimation by their eye gaze pattern, improve interactions, etc.
  id: totrans-1339
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在见证虚拟现实（VR）、增强现实（AR）和360°视频流技术的巨大进展。眼动追踪有可能在AR/VR和360°视频流的沉浸式视频应用领域带来革命，因为它可以通过了解用户在任何给定时间的注意力来增强设备的感知。因此，基于用户焦点的优化减少了这些设备的功耗[[199](#bib.bib199),
    [200](#bib.bib200), [34](#bib.bib34), [201](#bib.bib201)]。在这一部分，我们将探讨眼动追踪技术的重要性，以及它如何提升AR/VR和360°视频流设备的用户体验，包括通过眼间瞳距来估计图像感知质量、通过眼动模式进行人物识别或状态估计、改善互动等。
- en: 'Foveated Rendering (a.k.a gaze-contingent eye tracking) is a process designed
    to show the user only a portion of what they are looking at in full detail [[200](#bib.bib200),
    [32](#bib.bib32), [34](#bib.bib34)]. The focus region follows the user’s visual
    field. Graphics displayed with foveated rendering better matches the way we see
    objects. Usually, the user watches the AR/VR environment or 360° video using head
    mounted display devices. The existing platforms stream the full 360° scene while
    the user can view only a small part of the scene which spans about 90° - 120° horizontally,
    90° vertically. Quantitatively, it is less than 20% of the whole scene. Thus,
    a significant amount of power and network bandwidth is wasted for the display
    which is never utilized in viewing. In ideal condition, the display will be only
    in the user’s visual field while blurring the periphery. Following are the three
    important benefits of the user’s visual field based rendering process: 1\. Improved
    Image Quality: It can enable 4k displays on the current generation graphics processing
    units (GPUs) without degradation in performance. 2\. Lower cost: Similarly, the
    end-users can run AR/VR and 360° Video Streaming based applications on low-cost
    hardware without compromising the performance. 3\. Increased Frame Rate per Second
    (FPS): The end-user can run at a higher frame rate using the same graphical settings.
    There are two types of foveated rendering: dynamic foveated rendering and static
    foveated rendering. Dynamic foveated rendering follows the user’s gaze trajectory
    using eye-tracking and renders a sharp image in the required region, but this
    eye tracking is challenging in many scenarios. On the other hand, static foveated
    rendering considers a fixed area of the highest resolution at the center of the
    viewer’s device irrespective of the user’s gaze. It depends on the user’s head
    movements, thus, facing a challenge in eye-head interplay as the image quality
    is drastically reduced if the user looks away from the center of the field of
    view. The main key aspects of accurate eye position/visual attention estimation
    ahead of time is to enhance user experience via providing high image quality in
    the subject’s visual focus area. It requires person-specific calibration as nodal
    point of human have subject specific offset. Thus, generalizing it across user
    poses a challenge in the gaze analysis community to address [[202](#bib.bib202)].
    On the other hand, user’s viewpoint prediction ahead of time could face a lot
    of challenges as human eye movement is ballistic in nature. The visual attention
    of the user can therefore change abruptly based on the content in the screen.
    Thus, the prediction algorithm needs to take care of imperfect prediction as well
    and it needs to integrate with bit rate control. This process will enable user-specific
    recommendation and other facilities to enhance user experience [[203](#bib.bib203)].'
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 注视渲染（也称为视线依赖眼动追踪）是一种旨在仅以完整细节向用户显示他们所注视的部分内容的过程[[200](#bib.bib200), [32](#bib.bib32),
    [34](#bib.bib34)]。焦点区域跟随用户的视觉范围。使用注视渲染显示的图像更符合我们看物体的方式。通常，用户通过头戴显示设备观看AR/VR环境或360°视频。现有平台会流式传输整个360°场景，而用户只能查看场景中的小部分，水平约为90°-120°，垂直为90°。定量而言，这不到整个场景的20%。因此，大量的电力和网络带宽被浪费在从未利用的显示上。在理想条件下，显示将仅出现在用户的视觉范围内，而周围部分则模糊。以下是基于用户视觉范围的渲染过程的三个重要好处：1.
    提高图像质量：它可以在当前一代图形处理单元（GPU）上启用4k显示而不会性能下降。2. 降低成本：同样，最终用户可以在低成本硬件上运行AR/VR和360°视频流应用，而不会影响性能。3.
    增加每秒帧数（FPS）：最终用户可以在相同图形设置下以更高的帧率运行。有两种类型的注视渲染：动态注视渲染和静态注视渲染。动态注视渲染使用眼动追踪跟踪用户的视线轨迹，并在所需区域渲染清晰图像，但在许多场景中，眼动追踪是具有挑战性的。另一方面，静态注视渲染考虑到视图设备中心的固定区域的最高分辨率，而不管用户的视线如何。它依赖于用户的头部运动，因此，在用户从视场中心偏离时，图像质量会显著下降。准确的眼睛位置/视觉注意力估计的主要关键方面是通过提供高图像质量来增强用户体验。这需要个性化校准，因为人类的节点点有特定的偏移。因此，将其泛化到不同用户在注视分析社区中面临挑战[[202](#bib.bib202)]。另一方面，预测用户视点可能会面临许多挑战，因为人眼运动本质上是弹道性的。用户的视觉注意力因此可以根据屏幕上的内容突然变化。因此，预测算法还需要处理不完美的预测，并且需要与比特率控制集成。此过程将使用户特定的推荐和其他设施成为可能，以增强用户体验[[203](#bib.bib203)]。
- en: 6.2 Driver Engagement
  id: totrans-1341
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 驱动程序参与
- en: 'With the progress in autonomous and smart cars, the requirement for automatic
    driver monitoring has been observed and researchers have been working on this
    problem for a few years now [[81](#bib.bib81), [79](#bib.bib79), [40](#bib.bib40),
    [204](#bib.bib204)]. In the literature, the problem is treated as a gaze zone
    estimation problem. A summary of the gaze estimation benchmarks is shown in Table
    [VI](#S6.T6 "Table VI ‣ 6.2 Driver Engagement ‣ 6 Applications ‣ Automatic Gaze
    Analysis: A Survey of Deep Learning based Approaches"). The proposed methods can
    be classified into two categories:'
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
  zh: '随着自动驾驶和智能汽车的发展，对自动驾驶员监测的需求已经显现，研究人员已经在这一问题上工作了几年 [[81](#bib.bib81), [79](#bib.bib79),
    [40](#bib.bib40), [204](#bib.bib204)]。在文献中，这个问题被视为注视区域估计问题。注视估计基准的总结见表 [VI](#S6.T6
    "Table VI ‣ 6.2 Driver Engagement ‣ 6 Applications ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")。提出的方法可以分为两类：'
- en: Sensor Based Tracking. These mainly utilize dedicated sensors integrated hardware
    devices for monitoring the driver’s gaze in real-time. These devices require accurate
    pre-calibration and additionally these devices are expensive. Few examples of
    these sensors are Infrared (IR) camera [[205](#bib.bib205)], head-mounted devices [[77](#bib.bib77),
    [206](#bib.bib206)] and other systems [[207](#bib.bib207), [208](#bib.bib208)].
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 基于传感器的跟踪。这些方法主要利用专用传感器集成的硬件设备来实时监测驾驶员的注视。这些设备需要准确的预校准，并且这些设备价格昂贵。一些传感器的例子包括红外
    (IR) 相机 [[205](#bib.bib205)]、头戴式设备 [[77](#bib.bib77), [206](#bib.bib206)] 和其他系统
    [[207](#bib.bib207), [208](#bib.bib208)]。
- en: 'Table VI: Comparison of driver gaze estimation datasets with respect to number
    of subjects (# Sub), number of zones (# Zones), illumination conditions and labelling
    procedure.'
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: '表 VI: 驾驶员注视估计数据集的比较，涉及受试者数量 (# 受试者)、区域数量 (# 区域)、照明条件和标注过程。'
- en: '| References | # Sub | # Zones | Illumination | Labelling |'
  id: totrans-1345
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | # 受试者 | # 区域 | 照明 | 标注 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-1346
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Choi et al. [[84](#bib.bib84)] | 4 | 8 |'
  id: totrans-1347
  prefs: []
  type: TYPE_TB
  zh: '| Choi 等 [[84](#bib.bib84)] | 4 | 8 |'
- en: '&#124; Bright & &#124;'
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 明亮 & &#124;'
- en: '&#124; Dim &#124;'
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 昏暗 &#124;'
- en: '| 3-D Gyro. |'
  id: totrans-1350
  prefs: []
  type: TYPE_TB
  zh: '| 3-D 陀螺仪 |'
- en: '|'
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Lee et al.  [[85](#bib.bib85)] &#124;'
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Lee 等 [[85](#bib.bib85)] &#124;'
- en: '| 12 | 18 | Day | Manual |'
  id: totrans-1353
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 18 | 日间 | 手动 |'
- en: '|'
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fridman et al.  [[82](#bib.bib82)] &#124;'
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Fridman 等 [[82](#bib.bib82)] &#124;'
- en: '| 50 | 6 | Day | Manual |'
  id: totrans-1356
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 6 | 日间 | 手动 |'
- en: '| Tawari et al.  [[79](#bib.bib79)] | 6 | 8 | Day | Manual |'
  id: totrans-1357
  prefs: []
  type: TYPE_TB
  zh: '| Tawari 等 [[79](#bib.bib79)] | 6 | 8 | 日间 | 手动 |'
- en: '| Vora et al.  [[41](#bib.bib41)] | 10 | 7 |'
  id: totrans-1358
  prefs: []
  type: TYPE_TB
  zh: '| Vora 等 [[41](#bib.bib41)] | 10 | 7 |'
- en: '&#124; Diff. &#124;'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 差异 &#124;'
- en: '&#124; day times &#124;'
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 日间时间 &#124;'
- en: '| Manual |'
  id: totrans-1361
  prefs: []
  type: TYPE_TB
  zh: '| 手动 |'
- en: '| Jha et al. [[77](#bib.bib77)] | 16 | 18 | Day |'
  id: totrans-1362
  prefs: []
  type: TYPE_TB
  zh: '| Jha 等 [[77](#bib.bib77)] | 16 | 18 | 日间 |'
- en: '&#124; Head- &#124;'
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 头部- &#124;'
- en: '&#124; band &#124;'
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 带 &#124;'
- en: '|'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Wang et al. [[209](#bib.bib209)] | 3 | 9 | Day |'
  id: totrans-1366
  prefs: []
  type: TYPE_TB
  zh: '| Wang 等 [[209](#bib.bib209)] | 3 | 9 | 日间 |'
- en: '&#124; Motion &#124;'
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运动 &#124;'
- en: '&#124; Sensor &#124;'
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传感器 &#124;'
- en: '|'
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; DGW[[40](#bib.bib40)] &#124;'
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DGW[[40](#bib.bib40)] &#124;'
- en: '| 338 | 9 |'
  id: totrans-1372
  prefs: []
  type: TYPE_TB
  zh: '| 338 | 9 |'
- en: '&#124; Diff. day times &#124;'
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 差异 日间时间 &#124;'
- en: '| Automatic |'
  id: totrans-1374
  prefs: []
  type: TYPE_TB
  zh: '| 自动 |'
- en: '|'
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; MGM[[204](#bib.bib204)] &#124;'
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MGM[[204](#bib.bib204)] &#124;'
- en: '| 60 | 21 |'
  id: totrans-1377
  prefs: []
  type: TYPE_TB
  zh: '| 60 | 21 |'
- en: '&#124; Diff. day times &#124;'
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 差异 日间时间 &#124;'
- en: '|'
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multiple &#124;'
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多个 &#124;'
- en: '&#124; Sensors &#124;'
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传感器 &#124;'
- en: '|'
  id: totrans-1382
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Image processing and vision based methods. These are mainly focused on two
    types of methods: head-pose based only [[85](#bib.bib85), [80](#bib.bib80), [210](#bib.bib210),
    [209](#bib.bib209)] and both head-pose and eye-gaze based [[81](#bib.bib81), [79](#bib.bib79),
    [80](#bib.bib80), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)]. Driver’s
    head pose provides partial information regarding his/her gaze direction as there
    may be an interplay between eyeball movement and head pose [[83](#bib.bib83)].
    Hence, methods relying on head pose information may fail to disambiguate between
    the eye movement with fixed head-pose. Thus, the methods relying on both head
    pose and gaze based prediction are more robust.'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理和基于视觉的方法。这些方法主要集中在两种类型：仅基于头部姿势的方法 [[85](#bib.bib85), [80](#bib.bib80), [210](#bib.bib210),
    [209](#bib.bib209)] 和同时基于头部姿势和眼动注视的方法 [[81](#bib.bib81), [79](#bib.bib79), [80](#bib.bib80),
    [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84)]。驾驶员的头部姿势提供了关于其注视方向的部分信息，因为眼球运动与头部姿势之间可能存在相互作用
    [[83](#bib.bib83)]。因此，依赖头部姿势信息的方法可能无法区分固定头部姿势下的眼球运动。因此，依赖头部姿势和注视预测的结合方法更为可靠。
- en: 6.3 Gaze in Healthcare and Wellbeing
  id: totrans-1384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 健康护理与福祉中的注视
- en: Gaze is widely used in healthcare domain to enhance the diagnosis performance.
    Generally, eye movement patterns is widely used as behavioral bio-markers of various
    mental health problems including depression [[211](#bib.bib211)], post traumatic
    stress disorder [[212](#bib.bib212)] and Parkinson’s disease [[42](#bib.bib42)].
    Similarly, individuals diagnosed with Autism Spectral Disorder display gaze avoidance
    in social scenes [[42](#bib.bib42)]. Even intoxication including alcohol consumption
    and/or other drugs usage reflects on eye and gaze properties, especially, decreased
    accuracy and speed of saccades, changes in pupil size, and an impaired ability
    to fixate on moving objects. A recent survey [[42](#bib.bib42)] discusses the
    potential applications in healthcare including concussion [[43](#bib.bib43)],
    multiple sclerosis [[213](#bib.bib213)].
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
  zh: 注视在医疗领域被广泛应用，以提高诊断性能。通常，眼动模式被广泛用作各种心理健康问题的行为生物标记，包括抑郁症 [[211](#bib.bib211)]、创伤后应激障碍 [[212](#bib.bib212)]
    和帕金森病 [[42](#bib.bib42)]。同样，诊断为自闭症谱系障碍的个体在社交场合中表现出注视回避 [[42](#bib.bib42)]。即使是饮酒和/或其他药物使用的醉酒状态也会反映在眼睛和注视特性上，特别是眼跳的准确性和速度下降、瞳孔大小的变化以及在移动物体上固定的能力受损。近期的调查 [[42](#bib.bib42)]
    讨论了包括脑震荡 [[43](#bib.bib43)] 和多发性硬化症 [[213](#bib.bib213)] 在内的医疗潜在应用。
- en: 'Physiological Signals. A gaze estimation system could be one of the communication
    methods for severely disabled people who cannot perform any type of gestures and
    speech. Sakurai et al. [[214](#bib.bib214)] developed an eye-tracking method using
    a compact and light electrooculogram (EOG) signal. Further, this prototype is
    improved via the usage of the EOG component which strongly correlated with the
    change of eye movements [[215](#bib.bib215)] (Refer Fig. [8](#S6.F8 "Figure 8
    ‣ 6.3 Gaze in Healthcare and Wellbeing ‣ 6 Applications ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")). The setup can detect object scanning
    only by eye and face muscle movements. The experimental results open the possibility
    of eye-tracking via EOG signals and a Kinect sensor. Research along this direction
    can be extremely useful for disabled people.'
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: '生理信号。一个注视估计系统可能成为一种用于严重残疾人士的沟通方式，他们无法进行任何类型的手势和语言。Sakurai 等人 [[214](#bib.bib214)]
    开发了一种使用紧凑且轻便的电眼图（EOG）信号的眼动追踪方法。此外，这个原型通过使用与眼动变化强相关的 EOG 组件得到了改进 [[215](#bib.bib215)]（参见图 [8](#S6.F8
    "Figure 8 ‣ 6.3 Gaze in Healthcare and Wellbeing ‣ 6 Applications ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches")）。该设备仅通过眼睛和面部肌肉的运动即可检测物体扫描。实验结果展示了通过
    EOG 信号和 Kinect 传感器进行眼动追踪的可能性。沿着这个方向的研究对残疾人士非常有用。'
- en: '![Refer to caption](img/761dd8e2f0ed785661cefe5971cbc9a1.png)'
  id: totrans-1387
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/761dd8e2f0ed785661cefe5971cbc9a1.png)'
- en: 'Figure 8: Electro-oculogram (EOG) based gaze estimation method [[215](#bib.bib215)].
    This prototype opens the possibility of communication for severely disabled people.
    Refer Sec. [6.3](#S6.SS3 "6.3 Gaze in Healthcare and Wellbeing ‣ 6 Applications
    ‣ Automatic Gaze Analysis: A Survey of Deep Learning based Approaches") for more
    details.'
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 基于电眼图（EOG）的注视估计方法 [[215](#bib.bib215)]。这个原型为严重残疾人士提供了沟通的可能性。有关更多细节，请参见第 [6.3](#S6.SS3
    "6.3 Gaze in Healthcare and Wellbeing ‣ 6 Applications ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches")节。'
- en: 7 Privacy in gaze estimation
  id: totrans-1389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 注视估计中的隐私
- en: 'Due to the rapid progress over the past few years, gaze estimation technologies
    have become more reliable, cheap, compact and observe increasing use in many fields,
    such as gaming, marketing, driver safety, and healthcare. Consequently, these
    expanding uses of technology raise serious privacy concerns. Gaze patterns can
    reveal much more information than a user wishes and expects to give away. By portraying
    the sensitivity of gaze tracking data, this section provides a brief overview
    of privacy concerns and consequent implications of gaze estimation and eye-tracking.
    Fig. [9](#S7.F9 "Figure 9 ‣ 7 Privacy in gaze estimation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches") shows the overview of the privacy
    concerns, including common data capturing scenarios with their possible implications.
    A recent analysis [[216](#bib.bib216)] of the literature shows that eye-tracking
    data may implicitly contain information about a user’s biometric identity [[217](#bib.bib217)],
    personal attributes (such as gender, age, ethnicity, personality traits, intoxication,
    emotional state, skills etc.) [[218](#bib.bib218), [219](#bib.bib219), [220](#bib.bib220)],
    physical and mental health [[42](#bib.bib42), [211](#bib.bib211)]. Few eye-tracking
    measures may even reveal underlying cognitive processes [[17](#bib.bib17)]. The
    widespread consideration of eye-tracking enhance the potential to improve our
    lives in many directions, but the technology can also pose a substantial threat
    to privacy. Thus, it is necessary to understand the sensitiveness of gaze data
    from a holistic perspective to prevent its misuse.'
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: 由于过去几年内的快速进展，注视估计技术变得更加可靠、便宜、紧凑，并在许多领域中得到越来越多的应用，如游戏、营销、驾驶安全和医疗保健。因此，这些技术的扩展应用引发了严重的隐私担忧。注视模式可以揭示用户不愿意和意料之外的更多信息。通过描绘注视跟踪数据的敏感性，本节提供了隐私问题及其后果的简要概述。图
    [9](#S7.F9 "图 9 ‣ 7 注视估计中的隐私 ‣ 自动注视分析：基于深度学习的方法概述") 显示了隐私问题的概述，包括常见的数据捕获场景及其可能的影响。最近的分析 [[216](#bib.bib216)]
    显示，眼动追踪数据可能隐含着有关用户生物识别身份的信息 [[217](#bib.bib217)]，个人属性（如性别、年龄、种族、性格特征、醉酒状态、情感状态、技能等） [[218](#bib.bib218),
    [219](#bib.bib219), [220](#bib.bib220)]，身体和心理健康 [[42](#bib.bib42), [211](#bib.bib211)]。一些眼动追踪措施甚至可能揭示潜在的认知过程 [[17](#bib.bib17)]。眼动追踪的广泛应用提升了在许多方面改善我们生活的潜力，但该技术也可能对隐私构成实质性威胁。因此，有必要从整体角度理解注视数据的敏感性，以防止其被滥用。
- en: '![Refer to caption](img/4f39d86d95b4bae66736bab3d593b53a.png)'
  id: totrans-1391
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4f39d86d95b4bae66736bab3d593b53a.png)'
- en: 'Figure 9: The possible privacy concerns related to gaze analysis framework [[216](#bib.bib216)].
    Please refer Sec. [7](#S7 "7 Privacy in gaze estimation ‣ Automatic Gaze Analysis:
    A Survey of Deep Learning based Approaches") for more details.'
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 与注视分析框架相关的潜在隐私问题 [[216](#bib.bib216)]。有关更多细节，请参见第 [7](#S7 "7 注视估计中的隐私
    ‣ 自动注视分析：基于深度学习的方法概述") 节。'
- en: 8 Conclusion and Future Direction
  id: totrans-1393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论与未来方向
- en: 'Gaze analysis is a technology in search of an application in several domains
    mainly in assistive technology and HCI. The wide applications of gaze related
    technology is growing rapidly. Thus, it opens a lot of research opportunity ahead
    of the community. Here, in this paper, we present an overall review of gaze analysis
    frameworks with different perspectives from different point of view. Beginning
    with the preliminaries of gaze modelling and eye movement, we further elaborate
    on challenges in this field, overview of gaze analysis framework and its possible
    applications in different domains. For eye analysis, mainly geometric and appearance
    properties are widely explored in prior works. Despite recent progress, the gaze
    analysis remains challenging due to eye head interplay, occlusion and other challenges
    mentioned in Sec. [2.4](#S2.SS4 "2.4 Challenges ‣ 2 Preliminaries ‣ Automatic
    Gaze Analysis: A Survey of Deep Learning based Approaches"). Thus, there is a
    scope for future development in this respect. Moreover, all of the proposed datasets
    in this domain are collected in constraint environments. In order to overcome
    these limitations, the generative adversarial network based data generation approach
    has come into play. Due to several image quality-related issues, these datasets
    are not used for benchmarking. Automatic labelling of images based on accurate
    heuristic could be explored to reduce the data annotation burden greatly. Future
    directions for the eye and gaze trackers include:'
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: '视线分析是一项在多个领域（主要是辅助技术和人机交互）寻找应用的技术。视线相关技术的广泛应用正在迅速增长。因此，为社区提供了许多研究机会。在本文中，我们从不同角度对视线分析框架进行了全面审查。我们首先介绍了视线建模和眼动的基本知识，接着详细阐述了该领域的挑战、视线分析框架的概述及其在不同领域的潜在应用。对于眼部分析，主要几何和外观特性在先前的工作中被广泛探索。尽管有近期进展，但由于眼睛头部互动、遮挡和其他在第[2.4节](#S2.SS4
    "2.4 Challenges ‣ 2 Preliminaries ‣ Automatic Gaze Analysis: A Survey of Deep
    Learning based Approaches")提到的挑战，视线分析仍然具有挑战性。因此，在这方面未来有发展空间。此外，所有在该领域提出的数据集都是在受限环境中收集的。为了克服这些限制，生成对抗网络基础的数据生成方法已经开始发挥作用。由于若干图像质量相关的问题，这些数据集未被用于基准测试。基于准确启发式的自动标记图像可以大大减轻数据注释负担。未来眼部和视线跟踪器的方向包括：'
- en: 'Gaze Analysis in Unconstrained Setup: The most precise methods for gaze estimation
    is via intrusive sensors, IR camera and RGBD camera. The main drawback of these
    systems is that their performance degrades when used in real-world settings. In
    future, gaze estimation models should consider these situations. Although several
    current efforts in this direction employ techniques, yet further research is needed.
    Moreover, most of the current gaze estimation benchmark datasets require the proper
    geometric arrangement as well as user cooperation (e.g., CAVE, TabletGaze, MPII,
    Eyediap, ETH-XGaze etc). It would be an interesting direction to explore gaze
    estimation in a more flexible setting.'
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 非约束设置中的视线分析：最精确的视线估计方法是通过侵入式传感器、红外相机和RGBD相机进行的。这些系统的主要缺点是它们在实际环境中的性能会下降。未来，视线估计模型应考虑这些情况。尽管目前在这方面的几项努力采用了技术，但仍需进一步研究。此外，大多数当前的视线估计基准数据集需要适当的几何安排以及用户配合（例如，CAVE、TabletGaze、MPII、Eyediap、ETH-XGaze等）。探索在更灵活的设置中进行视线估计将是一个有趣的方向。
- en: 'Learning with Less Supervision: With the surge in unsupervised, self-supervised,
    weakly supervised techniques in this domain, more exploration in this direction
    is required to eliminate the dependency on ground truth gaze label which could
    be error-prone due to data acquisition limitations.'
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: 学习与更少监督：随着在这个领域无监督、自监督、弱监督技术的激增，需要更多探索以消除对真实视线标签的依赖，因为真实标签可能因数据采集限制而出现错误。
- en: 'Gaze Inference: Apart from localizing the eye and determining gaze, the gaze
    patterns provides vital cues for encoding the cognitive and affective states of
    the concerned person. More exploration and cross-domain research could be another
    direction to encode visual perception.'
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: 视线推断：除了定位眼睛和确定视线外，视线模式提供了编码相关人员认知和情感状态的重要线索。更多探索和跨领域研究可能是编码视觉感知的另一个方向。
- en: 'AR/VR: Eye tracking has potential application in AR/VR including Foveated Rending
    (FR) and Attention Tunneling. The gaze based interaction require low latency gaze
    estimation. In these applications, the visual environment presents a high-quality
    image at the point where the user is looking while blurring the other peripheral
    region. The intuition is to reduce power consumption without compromising the
    perceptual quality as well as user experience. However, eye movements are fast
    and involuntary action which restrict the use of this techniques (in FR) due to
    the subsequent delays in the eye-tracking pipelines. In order to address this
    issue, a new research direction i.e. future gaze trajectory prediction has been
    recently introduced [[34](#bib.bib34)]. More exploration along this direction
    is highly desirable.'
  id: totrans-1398
  prefs: []
  type: TYPE_NORMAL
  zh: AR/VR：眼动追踪在AR/VR中的潜在应用包括聚焦渲染（FR）和注意力隧道。基于注视的交互需要低延迟的注视估计。在这些应用中，视觉环境在用户注视的点上呈现高质量的图像，同时模糊其他外围区域。直观的目标是降低功耗而不影响感知质量以及用户体验。然而，眼动是快速且不自主的动作，这限制了这种技术（在FR中）的使用，因为眼动追踪管道中的延迟。为了应对这个问题，最近引入了一个新的研究方向，即未来注视轨迹预测[[34](#bib.bib34)]。沿着这个方向的更多探索是非常可取的。
- en: 'Eye Model and Learning Based Hybrid Approaches: Traditional geometrical eye
    model based and appearance guided learning based approaches have complimentary
    advantages. The geometrical eye model based methods does not require training
    data. Moreover, it has strong generalization capability but it is highly relied
    on relevant eye landmark localization performance. Accurate localization of eye
    landmarks is quite challenging in real world settings as the subject could have
    extreme headpose, occlusion, illumination and other environmental factors. On
    the other hand, the learning based approaches can encode eye appearance feature
    but it does not generalize well across different setups. Thus, a hybrid model
    which can take the advantage of both scenarios could be a possible research direction
    for gaze estimation and eye tracking domain.'
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
  zh: 眼睛模型和基于学习的混合方法：传统的几何眼睛模型和基于外观的学习方法各有其互补优势。几何眼睛模型的方法不需要训练数据。此外，它具有强大的泛化能力，但高度依赖于眼睛标志点的定位性能。在现实世界中，准确定位眼睛标志点具有相当大的挑战，因为受试者可能会有极端的头部姿势、遮挡、光照和其他环境因素。另一方面，基于学习的方法可以编码眼睛外观特征，但在不同环境下的泛化能力较差。因此，能够利用这两种情况优势的混合模型可能是注视估计和眼动追踪领域的一个研究方向。
- en: 'Multi-modal/Cross-modal Gaze Estimation: Over the past decade, head gesture
    synthesis has become an interesting line of research. Prior works in this area
    have mainly used handcrafted audio features such as energy based features [[221](#bib.bib221)],
    MFCC (Mel Frequency Cepstral Coefficent) [[222](#bib.bib222)], LPC (Linear Predictive
    Coding) [[222](#bib.bib222)] and filter bank [[222](#bib.bib222), [223](#bib.bib223)]
    to generate realistic head gesture. The main challenge in this domain is audio
    data annotation for head motion synthesis which is a noisy and error prone process.
    Prior works approach this problem via multi-stream HMMs [[221](#bib.bib221)],
    MLP based regression modelling [[222](#bib.bib222)], bi-LSTM [[223](#bib.bib223)]
    and Conditional Variational Autoencoder (CVAE) [[224](#bib.bib224)]. In vision
    domain, mainly visual stimuli is utilized for gaze estimation. As the audio signal
    is non-trivial for gaze estimation, yet, it has the potential to coarsely define
    the gaze direction [[225](#bib.bib225)]. Research along this direction have potential
    to estimate gaze in challenging situation where visual stimuli fails.'
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 多模态/跨模态注视估计：在过去十年里，头部姿势合成成为了一个有趣的研究方向。该领域的先前工作主要使用了手工制作的音频特征，如基于能量的特征[[221](#bib.bib221)]、MFCC（梅尔频率倒谱系数）[[222](#bib.bib222)]、LPC（线性预测编码）[[222](#bib.bib222)]和滤波器组[[222](#bib.bib222),
    [223](#bib.bib223)]来生成逼真的头部姿势。该领域的主要挑战是头部运动合成的音频数据注释，这是一个噪声多且容易出错的过程。先前的工作通过多流HMMs[[221](#bib.bib221)]、MLP回归建模[[222](#bib.bib222)]、bi-LSTM[[223](#bib.bib223)]和条件变分自编码器（CVAE）[[224](#bib.bib224)]来解决这个问题。在视觉领域，主要利用视觉刺激进行注视估计。尽管音频信号对于注视估计来说并非简单，但它有可能粗略定义注视方向[[225](#bib.bib225)]。沿着这个方向的研究有可能在视觉刺激失效的挑战性情况下估计注视。
- en: The techniques surveyed in this paper focus on gaze analysis from different
    perspective, however, these techniques can be useful for other computer vision
    and HCI tasks. Gaze analysis and its widespread applications is a unique and well-defined
    topic, which have already influenced recent technologies. Scholarly interest in
    gaze estimation is established in a large number of disciplines. It primarily
    originates from vision-related assistive technology which further propagates through
    other domains and attracts a lot of future research attention across various fields.
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
  zh: 本文调查的技术侧重于从不同角度分析注视，但这些技术对其他计算机视觉和人机交互任务也可能有用。注视分析及其广泛应用是一个独特且定义明确的主题，已经影响了近期的技术。学术界对注视估计的兴趣在大量学科中建立。它主要源于与视觉相关的辅助技术，随后传播到其他领域，并在各个领域引起了大量未来研究的关注。
- en: References
  id: totrans-1402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] H. Liu and I. Heynderickx, “Visual attention in objective image quality
    assessment: Based on eye-tracking data,” *IEEE Transactions on Circuits and Systems
    for Video Technology*, vol. 21, no. 7, pp. 971–982, 2011.'
  id: totrans-1403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] H. Liu 和 I. Heynderickx，“基于眼动数据的客观图像质量评估中的视觉注意力，” *IEEE Transactions on
    Circuits and Systems for Video Technology*，第 21 卷，第 7 期，页码 971–982，2011年。'
- en: '[2] A. Frischen, A. P. Bayliss, and S. P. Tipper, “Gaze cueing of attention:
    visual attention, social cognition, and individual differences.” *Psychological
    Bulletin*, vol. 133, p. 694, 2007.'
  id: totrans-1404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] A. Frischen, A. P. Bayliss 和 S. P. Tipper，“注意力的注视提示：视觉注意力、社会认知和个体差异。” *Psychological
    Bulletin*，第 133 卷，页码 694，2007年。'
- en: '[3] D. Purves, Y. Morgenstern, and W. T. Wojtach, “Perception and reality:
    why a wholly empirical paradigm is needed to understand vision,” *Frontiers in
    systems neuroscience*, vol. 9, p. 156, 2015.'
  id: totrans-1405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] D. Purves, Y. Morgenstern 和 W. T. Wojtach，“感知与现实：为何需要完全经验范式来理解视觉，” *Frontiers
    in systems neuroscience*，第 9 卷，页码 156，2015年。'
- en: '[4] E. Javal, “Essai sur la physiologie de la lecture,” *Annales d’Ocilistique*,
    vol. 80, pp. 97–117, 1878.'
  id: totrans-1406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] E. Javal，“阅读生理学试验，” *Annales d’Ocilistique*，第 80 卷，页码 97–117，1878年。'
- en: '[5] T. N. Cornsweet and H. D. Crane, “Accurate two-dimensional eye tracker
    using first and fourth purkinje images,” *JOSA*, vol. 63, no. 8, pp. 921–928,
    1973.'
  id: totrans-1407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. N. Cornsweet 和 H. D. Crane，“使用第一和第四普尔金捷像的精确二维眼动仪，” *JOSA*，第 63 卷，第 8
    期，页码 921–928，1973年。'
- en: '[6] J. Merchant, R. Morrissette, and J. L. Porterfield, “Remote measurement
    of eye direction allowing subject motion over one cubic foot of space,” *IEEE
    transactions on biomedical engineering*, no. 4, pp. 309–317, 1974.'
  id: totrans-1408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Merchant, R. Morrissette 和 J. L. Porterfield，“允许受试者在一立方英尺空间内运动的远程眼动方向测量，”
    *IEEE transactions on biomedical engineering*，第 4 期，页码 309–317，1974年。'
- en: '[7] M. Borgestig, J. Sandqvist, R. Parsons, T. Falkmer, and H. Hemmingsson,
    “Eye gaze performance for children with severe physical impairments using gaze-based
    assistive technology—a longitudinal study,” *Assistive technology*, vol. 28, no. 2,
    pp. 93–102, 2016.'
  id: totrans-1409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] M. Borgestig, J. Sandqvist, R. Parsons, T. Falkmer 和 H. Hemmingsson，“使用基于注视的辅助技术的严重身体残疾儿童的眼动表现——一项纵向研究，”
    *Assistive technology*，第 28 卷，第 2 期，页码 93–102，2016年。'
- en: '[8] F. Corno, L. Farinetti, and I. Signorile, “A cost-effective solution for
    eye-gaze assistive technology,” in *IEEE International Conference on Multimedia
    and Expo*, vol. 2, 2002, pp. 433–436.'
  id: totrans-1410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] F. Corno, L. Farinetti 和 I. Signorile，“眼动辅助技术的经济有效解决方案，” 见 *IEEE国际多媒体与博览会议*，第
    2 卷，2002年，页码 433–436。'
- en: '[9] A. W. Joseph and R. Murugesh, “Potential eye tracking metrics and indicators
    to measure cognitive load in human-computer interaction research,” *Journal of
    Scientific Research*, vol. 64, no. 1, 2020.'
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. W. Joseph 和 R. Murugesh，“潜在的眼动跟踪指标和测量认知负荷的指标在人机交互研究中的应用，” *Journal of
    Scientific Research*，第 64 卷，第 1 期，2020年。'
- en: '[10] J. Pi, P. A. Koljonen, Y. Hu, and B. E. Shi, “Dynamic bayesian adjustment
    of dwell time for faster eye typing,” *IEEE Transactions on Neural Systems and
    Rehabilitation Engineering*, vol. 28, no. 10, pp. 2315–2324, 2020.'
  id: totrans-1412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Pi, P. A. Koljonen, Y. Hu 和 B. E. Shi，“动态贝叶斯调整停留时间以加快眼动输入，” *IEEE Transactions
    on Neural Systems and Rehabilitation Engineering*，第 28 卷，第 10 期，页码 2315–2324，2020年。'
- en: '[11] Z. Chen, D. Deng, J. Pi, and B. E. Shi, “Unsupervised outlier detection
    in appearance-based gaze estimation,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision Workshops*, 2019, pp. 0–0.'
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Z. Chen, D. Deng, J. Pi 和 B. E. Shi，“基于外观的注视估计中的无监督异常检测，” 见 *IEEE/CVF国际计算机视觉会议论文集*，2019年，页码
    0–0。'
- en: '[12] M. Wedel and R. Pieters, “A review of eye-tracking research in marketing,”
    in *Review of marketing research*.   Routledge, 2017, pp. 123–147.'
  id: totrans-1414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Wedel 和 R. Pieters，“市场营销中眼动研究的综述，” 见 *市场营销研究综述*。 Routledge，2017年，页码
    123–147。'
- en: '[13] A. Patney, J. Kim, M. Salvi, A. Kaplanyan, C. Wyman, N. Benty, A. Lefohn,
    and D. Luebke, “Perceptually-based foveated virtual reality,” in *SIGGRAPH Emerging
    Technologies*.   ACM, 2016, pp. 1–2.'
  id: totrans-1415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. Patney, J. Kim, M. Salvi, A. Kaplanyan, C. Wyman, N. Benty, A. Lefohn,
    和 D. Luebke，“基于感知的视网膜虚拟现实”，发表于 *SIGGRAPH 新兴技术*。ACM，2016年，页1–2。'
- en: '[14] R. T. Azuma, “A survey of augmented reality,” *Presence: Teleoperators
    & Virtual Environments*, vol. 6, no. 4, pp. 355–385, 1997.'
  id: totrans-1416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] R. T. Azuma，“增强现实调查”，*存在感：遥操作与虚拟环境*，第6卷，第4期，页355–385，1997年。'
- en: '[15] F. Ragusa, A. Furnari, S. Battiato, G. Signorello, and G. M. Farinella,
    “Ego-ch: Dataset and fundamental tasks for visitors behavioral understanding using
    egocentric vision,” *Pattern Recognition Letters*, vol. 131, pp. 150–157, 2020.'
  id: totrans-1417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] F. Ragusa, A. Furnari, S. Battiato, G. Signorello, 和 G. M. Farinella，“Ego-ch：使用自我中心视觉理解访客行为的数据集和基本任务”，*模式识别快报*，第131卷，页150–157，2020年。'
- en: '[16] A. K. Jain, R. Bolle, and S. Pankanti, *Biometrics: personal identification
    in networked society*.   Springer Science & Business Media, 2006, vol. 479.'
  id: totrans-1418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] A. K. Jain, R. Bolle, 和 S. Pankanti，*生物识别：网络社会中的个人身份识别*。Springer Science
    & Business Media，2006年，第479卷。'
- en: '[17] M. K. Eckstein, B. Guerra-Carrillo, A. T. M. Singley, and S. A. Bunge,
    “Beyond eye gaze: What else can eyetracking reveal about cognition and cognitive
    development?” *Developmental cognitive neuroscience*, vol. 25, pp. 69–91, 2017.'
  id: totrans-1419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] M. K. Eckstein, B. Guerra-Carrillo, A. T. M. Singley, 和 S. A. Bunge，“超越眼动：眼动追踪还能揭示关于认知和认知发展的什么？”*发展性认知神经科学*，第25卷，页69–91，2017年。'
- en: '[18] M. A. Miller and M. T. Fillmore, “Persistence of attentional bias toward
    alcohol-related stimuli in intoxicated social drinkers,” *Drug and Alcohol Dependence*,
    vol. 117, no. 2-3, pp. 184–189, 2011.'
  id: totrans-1420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] M. A. Miller 和 M. T. Fillmore，“饮酒社交者对与酒精相关刺激的注意力偏向的持续性”，*药物与酒精依赖*，第117卷，第2-3期，页184–189，2011年。'
- en: '[19] Z. Zhu and Q. Ji, “Novel eye gaze tracking techniques under natural head
    movement,” *IEEE Transactions on biomedical engineering*, vol. 54, no. 12, pp.
    2246–2260, 2007.'
  id: totrans-1421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] Z. Zhu 和 Q. Ji，“自然头部运动下的新型眼动追踪技术”，*IEEE 生物医学工程学报*，第54卷，第12期，页2246–2260，2007年。'
- en: '[20] Y. Sugano, Y. Matsushita, and Y. Sato, “Learning-by-synthesis for appearance-based
    3d gaze estimation,” in *IEEE Computer Vision and Pattern Recognition*, 2014,
    pp. 1821–1828.'
  id: totrans-1422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Sugano, Y. Matsushita, 和 Y. Sato，“基于外观的3D凝视估计的合成学习”，发表于 *IEEE 计算机视觉与模式识别*，2014年，页1821–1828。'
- en: '[21] D. W. Hansen and Q. Ji, “In the eye of the beholder: A survey of models
    for eyes and gaze,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 32, no. 3, pp. 478–500, 2009.'
  id: totrans-1423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] D. W. Hansen 和 Q. Ji，“在观察者的眼中：眼睛和凝视模型的调查”，*IEEE 模式分析与机器智能学报*，第32卷，第3期，页478–500，2009年。'
- en: '[22] D. Hansen and A. Pece, “Eye tracking in the wild,” *Computer Vision and
    Image Understanding*, 2005.'
  id: totrans-1424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] D. Hansen 和 A. Pece，“野外中的眼动追踪”，*计算机视觉与图像理解*，2005年。'
- en: '[23] B. Smith, Q. Yin, S. Feiner, and S. Nayar, “Gaze locking: passive eye
    contact detection for human-object interaction,” in *ACM User Interface Software
    & Technology*, 2013.'
  id: totrans-1425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] B. Smith, Q. Yin, S. Feiner, 和 S. Nayar，“凝视锁定：用于人机交互的被动眼睛接触检测”，发表于 *ACM
    用户界面软件与技术*，2013年。'
- en: '[24] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Appearance-based gaze
    estimation in the wild,” in *IEEE Computer Vision and Pattern Recognition*, 2015,
    pp. 4511–4520.'
  id: totrans-1426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] X. Zhang, Y. Sugano, M. Fritz, 和 A. Bulling，“野外中的基于外观的凝视估计”，发表于 *IEEE
    计算机视觉与模式识别*，2015年，页4511–4520。'
- en: '[25] K. Krafka, A. Khosla, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik,
    and A. Torralba, “Eye tracking for everyone,” in *IEEE Computer Vision and Pattern
    Recognition*, 2016, pp. 2176–2184.'
  id: totrans-1427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] K. Krafka, A. Khosla, P. Kellnhofer, H. Kannan, S. Bhandarkar, W. Matusik,
    和 A. Torralba，“面向所有人的眼动追踪”，发表于 *IEEE 计算机视觉与模式识别*，2016年，页2176–2184。'
- en: '[26] S. Park, A. Spurr, and O. Hilliges, “Deep pictorial gaze estimation,”
    in *European Conference on Computer Vision*, 2018, pp. 721–738.'
  id: totrans-1428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Park, A. Spurr, 和 O. Hilliges，“深度图像化凝视估计”，发表于 *欧洲计算机视觉会议*，2018年，页721–738。'
- en: '[27] N. Dubey, S. Ghosh, and A. Dhall, “Unsupervised learning of eye gaze representation
    from the web,” in *2019 International Joint Conference on Neural Networks (IJCNN)*.   IEEE,
    2019, pp. 1–7.'
  id: totrans-1429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] N. Dubey, S. Ghosh, 和 A. Dhall，“从网络中无监督学习眼睛凝视表示”，发表于 *2019国际神经网络联合会议（IJCNN）*。IEEE，2019年，页1–7。'
- en: '[28] Y. Yu and J. Odobez, “Unsupervised representation learning for gaze estimation,”
    *IEEE Conference on Computer Vision and Pattern Recognition*, pp. 1–13, 2020.'
  id: totrans-1430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Yu 和 J. Odobez，“用于凝视估计的无监督表示学习”，*IEEE 计算机视觉与模式识别会议*，页1–13，2020年。'
- en: '[29] S. Park, S. D. Mello, P. Molchanov, U. Iqbal, O. Hilliges, and J. Kautz,
    “Few-shot adaptive gaze estimation,” in *IEEE International Conference on Computer
    Vision*, 2019, pp. 9368–9377.'
  id: totrans-1431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Park, S. D. Mello, P. Molchanov, U. Iqbal, O. Hilliges 和 J. Kautz，“少样本自适应凝视估计，”
    见 *IEEE国际计算机视觉会议*，2019年，第9368–9377页。'
- en: '[30] E. B. Huey, “The psychology and pedagogy of reading: With a review of
    the history of reading and writing and of methods, texts, and hygiene in reading,”
    1908.'
  id: totrans-1432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] E. B. Huey，“阅读的心理学和教学法：回顾阅读和写作的历史以及阅读方法、文本和卫生，” 1908年。'
- en: '[31] Y. Sugano, Y. Matsushita, and Y. Sato, “Appearance-based gaze estimation
    using visual saliency,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 35, no. 2, pp. 329–341, 2012.'
  id: totrans-1433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Y. Sugano, Y. Matsushita 和 Y. Sato，“基于外观的凝视估计使用视觉显著性，” *IEEE模式分析与机器智能汇刊*，第35卷，第2期，第329–341页，2012年。'
- en: '[32] S. J. Garbin, Y. Shen, I. Schuetz, R. Cavin, G. Hughes, and S. S. Talathi,
    “Openeds: Open eye dataset,” *arXiv preprint arXiv:1905.03702*, 2019.'
  id: totrans-1434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] S. J. Garbin, Y. Shen, I. Schuetz, R. Cavin, G. Hughes 和 S. S. Talathi，“Openeds:
    开放眼睛数据集，” *arXiv预印本 arXiv:1905.03702*，2019年。'
- en: '[33] P. Kellnhofer, A. Recasens, S. Stent, W. Matusik, and A. Torralba, “Gaze360:
    Physically unconstrained gaze estimation in the wild,” in *IEEE International
    Conference on Computer Vision*, 2019, pp. 6912–6921.'
  id: totrans-1435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] P. Kellnhofer, A. Recasens, S. Stent, W. Matusik 和 A. Torralba，“Gaze360:
    在野外进行物理上不受约束的凝视估计，” 见 *IEEE国际计算机视觉会议*，2019年，第6912–6921页。'
- en: '[34] C. Palmero, A. Sharma, K. Behrendt, K. Krishnakumar, O. V. Komogortsev,
    and S. S. Talathi, “Openeds2020: Open eyes dataset,” *arXiv preprint arXiv:2005.03876*,
    2020.'
  id: totrans-1436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] C. Palmero, A. Sharma, K. Behrendt, K. Krishnakumar, O. V. Komogortsev
    和 S. S. Talathi，“Openeds2020: 开放眼睛数据集，” *arXiv预印本 arXiv:2005.03876*，2020年。'
- en: '[35] H. Chennamma and X. Yuan, “A survey on eye-gaze tracking techniques,”
    *arXiv preprint arXiv:1312.6410*, 2013.'
  id: totrans-1437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] H. Chennamma 和 X. Yuan，“眼动追踪技术综述，” *arXiv预印本 arXiv:1312.6410*，2013年。'
- en: '[36] H. Jing-Yao, X. Yong-Yue, L. Lin-Na, X.-C. ZHANG, Q. Li, and C. Jian-Nan,
    “Survey on key technologies of eye gaze tracking,” *DEStech Transactions on Computer
    Science and Engineering*, no. aice-ncs, 2016.'
  id: totrans-1438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] H. Jing-Yao, X. Yong-Yue, L. Lin-Na, X.-C. ZHANG, Q. Li 和 C. Jian-Nan，“眼动追踪关键技术综述，”
    *DEStech计算机科学与工程学报*，第aice-ncs期，2016年。'
- en: '[37] A. Kar and P. Corcoran, “A review and analysis of eye-gaze estimation
    systems, algorithms and performance evaluation methods in consumer platforms,”
    *IEEE Access*, vol. 5, pp. 16 495–16 519, 2017.'
  id: totrans-1439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] A. Kar 和 P. Corcoran，“消费者平台中眼动估计系统、算法和性能评估方法的综述与分析，” *IEEE Access*，第5卷，第16,495–16,519页，2017年。'
- en: '[38] D. Cazzato, M. Leo, C. Distante, and H. Voos, “When i look into your eyes:
    A survey on computer vision contributions for human gaze estimation and tracking,”
    *Sensors*, vol. 20, no. 13, p. 3739, 2020.'
  id: totrans-1440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] D. Cazzato, M. Leo, C. Distante 和 H. Voos，“当我看着你的眼睛时：关于计算机视觉对人类凝视估计和跟踪的贡献的综述，”
    *传感器*，第20卷，第13期，第3739页，2020年。'
- en: '[39] V. Clay, P. König, and S. U. König, “Eye tracking in virtual reality,”
    *Journal of Eye Movement Research*, vol. 12, no. 1, 2019.'
  id: totrans-1441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] V. Clay, P. König 和 S. U. König，“虚拟现实中的眼动追踪，” *眼动研究期刊*，第12卷，第1期，2019年。'
- en: '[40] S. Ghosh, A. Dhall, G. Sharma, S. Gupta, and N. Sebe, “Speak2label: Using
    domain knowledge for creating a large scale driver gaze zone estimation dataset,”
    *arXiv preprint arXiv:2004.05973*, 2020.'
  id: totrans-1442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Ghosh, A. Dhall, G. Sharma, S. Gupta 和 N. Sebe，“Speak2label：利用领域知识创建大规模驾驶员凝视区域估计数据集，”
    *arXiv预印本 arXiv:2004.05973*，2020年。'
- en: '[41] S. Vora, A. Rangesh, and M. M. Trivedi, “Driver gaze zone estimation using
    convolutional neural networks: A general framework and ablative analysis,” *IEEE
    Transactions on Intelligent Vehicles*, pp. 254–265, 2018.'
  id: totrans-1443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Vora, A. Rangesh 和 M. M. Trivedi，“使用卷积神经网络进行驾驶员凝视区域估计：通用框架和消融分析，” *IEEE智能车辆汇刊*，第254–265页，2018年。'
- en: '[42] K. Harezlak and P. Kasprowski, “Application of eye tracking in medicine:
    A survey, research issues and challenges,” *Computerized Medical Imaging and Graphics*,
    vol. 65, pp. 176–190, 2018.'
  id: totrans-1444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] K. Harezlak 和 P. Kasprowski，“眼动追踪在医学中的应用：调查、研究问题和挑战，” *计算机化医学成像与图形*，第65卷，第176–190页，2018年。'
- en: '[43] Y. Kempinski, “System and method of diagnosis using gaze and eye tracking,”
    Apr. 21 2016, uS Patent App. 14/723,590.'
  id: totrans-1445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Kempinski，“利用凝视和眼动追踪进行诊断的系统和方法，” 2016年4月21日，uS专利申请14/723,590。'
- en: '[44] S. Park, “Representation learning for webcam-based gaze estimation,” Ph.D.
    dissertation, ETH Zurich, 2020.'
  id: totrans-1446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. Park，“基于网络摄像头的凝视估计的表示学习，” 博士学位论文，苏黎世联邦理工学院，2020年。'
- en: '[45] R. H. Carpenter, *Movements of the Eyes, 2nd Rev*.   Pion Limited, 1988.'
  id: totrans-1447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] R. H. Carpenter，*眼睛运动，第2版修订版*。 Pion Limited，1988年。'
- en: '[46] E. D. Guestrin and M. Eizenman, “General theory of remote gaze estimation
    using the pupil center and corneal reflections,” *IEEE Transactions on biomedical
    engineering*, vol. 53, no. 6, pp. 1124–1133, 2006.'
  id: totrans-1448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] E. D. Guestrin 和 M. Eizenman，"使用瞳孔中心和角膜反射的远程注视估计的通用理论"，*IEEE生物医学工程学报*，第53卷，第6期，pp.
    1124–1133，2006年。'
- en: '[47] A. T. Duchowski and A. T. Duchowski, *Eye tracking methodology: Theory
    and practice*.   Springer, 2017.'
  id: totrans-1449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] A. T. Duchowski 和 A. T. Duchowski，*眼动追踪方法论：理论与实践*。Springer，2017年。'
- en: '[48] T. Santini, W. Fuhl, and E. Kasneci, “Calibme: Fast and unsupervised eye
    tracker calibration for gaze-based pervasive human-computer interaction,” in *ACM
    Conference on Human Factors in Computing Systems*, 2017, pp. 2594–2605.'
  id: totrans-1450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] T. Santini, W. Fuhl 和 E. Kasneci，"Calibme：用于注视基础的普适人机交互的快速无监督眼动追踪校准"，发表于*ACM计算机系统中的人因会议*，2017年，pp.
    2594–2605。'
- en: '[49] X. Zhang, Y. Sugano, and A. Bulling, “Evaluation of appearance-based methods
    and implications for gaze-based applications,” in *Proceedings of the 2019 CHI
    Conference on Human Factors in Computing Systems*, 2019, pp. 1–13.'
  id: totrans-1451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] X. Zhang, Y. Sugano 和 A. Bulling，"基于外观的方法评估及其对注视应用的影响"，发表于*2019年计算机系统中的人因（CHI）会议论文集*，2019年，pp.
    1–13。'
- en: '[50] X. Zhang, Y. Sugano, M. Fritz, and A. Bulling, “Mpiigaze: Real-world dataset
    and deep appearance-based gaze estimation,” *IEEE Transactions on Pattern Analysis
    and Machine Intelligence*, 2017.'
  id: totrans-1452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. Zhang, Y. Sugano, M. Fritz 和 A. Bulling，"Mpiigaze：真实世界数据集与深度外观基础注视估计"，*IEEE模式分析与机器智能学报*，2017年。'
- en: '[51] ——, “It’s written all over your face: Full-face appearance-based gaze
    estimation,” in *IEEE Computer Vision and Pattern Recognition Workshop*, 2017.'
  id: totrans-1453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] ——，"全脸外观基础注视估计：写在你脸上的所有信息"，发表于*IEEE计算机视觉与模式识别研讨会*，2017年。'
- en: '[52] W. Zhu and H. Deng, “Monocular free-head 3d gaze tracking with deep learning
    and geometry constraints,” in *Proceedings of the IEEE International Conference
    on Computer Vision*, 2017, pp. 3143–3152.'
  id: totrans-1454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] W. Zhu 和 H. Deng，"单目自由头部3D注视追踪结合深度学习与几何约束"，发表于*IEEE国际计算机视觉会议论文集*，2017年，pp.
    3143–3152。'
- en: '[53] W. Cui, J. Cui, and H. Zha, “Specialized gaze estimation for children
    by convolutional neural network and domain adaptation,” in *2017 IEEE International
    Conference on Image Processing (ICIP)*.   IEEE, 2017, pp. 3305–3309.'
  id: totrans-1455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] W. Cui, J. Cui 和 H. Zha，"通过卷积神经网络和领域适应的儿童专用注视估计"，发表于*2017年IEEE国际图像处理会议（ICIP）*。IEEE，2017年，pp.
    3305–3309。'
- en: '[54] T. Fischer, H. J. Chang, and Y. Demiris, “RT-GENE: Real-Time Eye Gaze
    Estimation in Natural Environments,” in *European Conference on Computer Vision*,
    2018, pp. 339–357.'
  id: totrans-1456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] T. Fischer, H. J. Chang 和 Y. Demiris，"RT-GENE：自然环境中的实时眼动注视估计"，发表于*欧洲计算机视觉会议*，2018年，pp.
    339–357。'
- en: '[55] Y. Cheng, F. Lu, and X. Zhang, “Appearance-based gaze estimation via evaluation-guided
    asymmetric regression,” in *Proceedings of the European Conference on Computer
    Vision (ECCV)*, 2018, pp. 100–115.'
  id: totrans-1457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Y. Cheng, F. Lu 和 X. Zhang，"基于外观的注视估计通过评估引导的非对称回归"，发表于*欧洲计算机视觉会议（ECCV）*，2018年，pp.
    100–115。'
- en: '[56] C. Palmero, J. Selva, M. A. Bagheri, and S. Escalera, “Recurrent cnn for
    3d gaze estimation using appearance and shape cues,” *arXiv preprint arXiv:1805.03064*,
    2018.'
  id: totrans-1458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] C. Palmero, J. Selva, M. A. Bagheri 和 S. Escalera，"用于3D注视估计的递归CNN结合外观和形状线索"，*arXiv预印本arXiv:1805.03064*，2018年。'
- en: '[57] S. Jyoti and A. Dhall, “Automatic eye gaze estimation using geometric
    & texture-based networks,” in *International Conference on Pattern Recognition*.   IEEE,
    2018, pp. 2474–2479.'
  id: totrans-1459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] S. Jyoti 和 A. Dhall，"使用几何与纹理基础网络的自动眼动注视估计"，发表于*国际模式识别会议*。IEEE，2018年，pp.
    2474–2479。'
- en: '[58] G. Liu, Y. Yu, K. A. F. Mora, and J.-M. Odobez, “A differential approach
    for gaze estimation,” *IEEE transactions on pattern analysis and machine intelligence*,
    2019.'
  id: totrans-1460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] G. Liu, Y. Yu, K. A. F. Mora 和 J.-M. Odobez，"一种用于注视估计的差分方法"，*IEEE模式分析与机器智能学报*，2019年。'
- en: '[59] Y. Cheng, S. Huang, F. Wang, C. Qian, and F. Lu, “A coarse-to-fine adaptive
    network for appearance-based gaze estimation,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 34, no. 07, 2020, pp. 10 623–10 630.'
  id: totrans-1461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Y. Cheng, S. Huang, F. Wang, C. Qian 和 F. Lu，"一种从粗到细的自适应网络用于基于外观的注视估计"，发表于*AAAI人工智能会议论文集*，第34卷，第07期，2020年，pp.
    10 623–10 630。'
- en: '[60] X. Zhang, Y. Sugano, A. Bulling, and O. Hilliges, “Learning-based region
    selection for end-to-end gaze estimation,” in *British Machine Vision Conference
    (BMVC 2020)*, 2020.'
  id: totrans-1462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] X. Zhang, Y. Sugano, A. Bulling 和 O. Hilliges，"基于学习的区域选择用于端到端注视估计"，发表于*英国机器视觉会议（BMVC
    2020）*，2020年。'
- en: '[61] Y. Yu, G. Liu, and J.-M. Odobez, “Deep multitask gaze estimation with
    a constrained landmark-gaze model,” in *Proceedings of the European Conference
    on Computer Vision (ECCV) Workshops*, 2018, pp. 0–0.'
  id: totrans-1463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Yu, G. Liu, 和 J.-M. Odobez， “基于约束地标-视线模型的深度多任务视线估计，” 见于 *欧洲计算机视觉会议
    (ECCV) 研讨会*，2018 年，第 0–0 页。'
- en: '[62] E. Chong, N. Ruiz, Y. Wang, Y. Zhang, A. Rozga, and J. M. Rehg, “Connecting
    gaze, scene, and attention: Generalized attention estimation via joint modeling
    of gaze and scene saliency,” in *Proceedings of the European conference on computer
    vision (ECCV)*, 2018, pp. 383–398.'
  id: totrans-1464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] E. Chong, N. Ruiz, Y. Wang, Y. Zhang, A. Rozga, 和 J. M. Rehg， “连接视线、场景和注意力：通过视线和场景显著性联合建模的一般化注意力估计，”
    见于 *欧洲计算机视觉会议 (ECCV) 论文集*，2018 年，第 383–398 页。'
- en: '[63] X. Zhang, M. X. Huang, Y. Sugano, and A. Bulling, “Training person-specific
    gaze estimators from user interactions with multiple devices,” in *Proceedings
    of the 2018 CHI Conference on Human Factors in Computing Systems*, 2018, pp. 1–12.'
  id: totrans-1465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] X. Zhang, M. X. Huang, Y. Sugano, 和 A. Bulling， “从用户与多个设备的交互中训练特定于个人的视线估计器，”
    见于 *2018 年计算机系统人因会议 (CHI) 会议论文集*，2018 年，第 1–12 页。'
- en: '[64] Z. Chen and B. E. Shi, “Appearance-based gaze estimation using dilated-convolutions,”
    in *Asian Conference on Computer Vision*.   Springer, 2018, pp. 309–324.'
  id: totrans-1466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Z. Chen 和 B. E. Shi， “基于外观的视线估计使用扩张卷积，” 见于 *亚洲计算机视觉会议*。   Springer，2018
    年，第 309–324 页。'
- en: '[65] X. Zhou, J. Lin, J. Jiang, and S. Chen, “Learning a 3d gaze estimator
    with improved itracker combined with bidirectional lstm,” in *2019 IEEE international
    conference on Multimedia and expo (ICME)*.   IEEE, 2019, pp. 850–855.'
  id: totrans-1467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] X. Zhou, J. Lin, J. Jiang, 和 S. Chen， “结合改进的 Itracker 和双向 LSTM 学习 3D 视线估计器，”
    见于 *2019 IEEE 国际多媒体与展会会议 (ICME)*。   IEEE，2019 年，第 850–855 页。'
- en: '[66] K. Wang, H. Su, and Q. Ji, “Neuro-inspired eye tracking with eye movement
    dynamics,” in *Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 9831–9840.'
  id: totrans-1468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] K. Wang, H. Su, 和 Q. Ji， “基于神经启发的眼动追踪，” 见于 *IEEE 计算机视觉与模式识别会议论文集*，2019
    年，第 9831–9840 页。'
- en: '[67] Z. Wu, S. Rajendran, T. Van As, V. Badrinarayanan, and A. Rabinovich,
    “Eyenet: A multi-task deep network for off-axis eye gaze estimation,” in *2019
    IEEE/CVF International Conference on Computer Vision Workshop (ICCVW)*.   IEEE,
    2019, pp. 3683–3687.'
  id: totrans-1469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Z. Wu, S. Rajendran, T. Van As, V. Badrinarayanan, 和 A. Rabinovich， “Eyenet：一种用于离轴眼动估计的多任务深度网络，”
    见于 *2019 IEEE/CVF 国际计算机视觉会议研讨会 (ICCVW)*。   IEEE，2019 年，第 3683–3687 页。'
- en: '[68] J. Kim, M. Stengel, A. Majercik, S. De Mello, D. Dunn, S. Laine, M. McGuire,
    and D. Luebke, “Nvgaze: An anatomically-informed dataset for low-latency, near-eye
    gaze estimation,” in *Proceedings of the 2019 CHI Conference on Human Factors
    in Computing Systems*, 2019, pp. 1–12.'
  id: totrans-1470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Kim, M. Stengel, A. Majercik, S. De Mello, D. Dunn, S. Laine, M. McGuire,
    和 D. Luebke， “Nvgaze：一个用于低延迟、近眼视线估计的解剖学数据集，” 见于 *2019 年计算机系统人因会议 (CHI) 会议论文集*，2019
    年，第 1–12 页。'
- en: '[69] D. Xia and Z. Ruan, “IR image based eye gaze estimation,” in *IEEE ACIS
    International Conference on Software Engineering, Artificial Intelligence, Networking,
    and Parallel/Distributed Computing*, vol. 1, 2007, pp. 220–224.'
  id: totrans-1471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] D. Xia 和 Z. Ruan， “基于红外图像的眼动估计，” 见于 *IEEE ACIS 软件工程、人工智能、网络及并行/分布式计算国际会议*，第
    1 卷，2007 年，第 220–224 页。'
- en: '[70] M. Tonsen, J. Steil, Y. Sugano, and A. Bulling, “Invisibleeye: Mobile
    eye tracking using multiple low-resolution cameras and learning-based gaze estimation,”
    *Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies*,
    vol. 1, no. 3, pp. 1–21, 2017.'
  id: totrans-1472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] M. Tonsen, J. Steil, Y. Sugano, 和 A. Bulling， “Invisibleeye：使用多个低分辨率摄像头和基于学习的视线估计进行移动眼动追踪，”
    *ACM 交互式、移动、可穿戴和无处不在技术会议论文集*，第 1 卷，第 3 期，第 1–21 页，2017 年。'
- en: '[71] D. Lian, Z. Zhang, W. Luo, L. Hu, M. Wu, Z. Li, J. Yu, and S. Gao, “Rgbd
    based gaze estimation via multi-task cnn,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 33, no. 01, 2019, pp. 2488–2495.'
  id: totrans-1473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] D. Lian, Z. Zhang, W. Luo, L. Hu, M. Wu, Z. Li, J. Yu, 和 S. Gao， “基于 RGBD
    的视线估计通过多任务 CNN，” 见于 *AAAI 人工智能会议论文集*，第 33 卷，第 01 期，2019 年，第 2488–2495 页。'
- en: '[72] A. Tsukada, M. Shino, M. Devyver, and T. Kanade, “Illumination-free gaze
    estimation method for first-person vision wearable device,” in *IEEE International
    Conference on Computer Vision Workshop*, 2011.'
  id: totrans-1474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] A. Tsukada, M. Shino, M. Devyver, 和 T. Kanade， “用于第一人称视觉可穿戴设备的无光照视线估计方法，”
    见于 *IEEE 国际计算机视觉会议研讨会*，2011 年。'
- en: '[73] M. Leo, D. Cazzato, T. De Marco, and C. Distante, “Unsupervised eye pupil
    localization through differential geometry and local self-similarity,” *Public
    Library of Science*, vol. 9, no. 8, 2014.'
  id: totrans-1475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] M. Leo, D. Cazzato, T. De Marco, 和 C. Distante， “通过微分几何和局部自相似性进行无监督眼瞳定位，”
    *公共科学图书馆*，第 9 卷，第 8 期，2014 年。'
- en: '[74] C. Gou, Y. Wu, K. Wang, K. Wang, F. Wang, and Q. Ji, “A joint cascaded
    framework for simultaneous eye detection and eye state estimation,” *Pattern Recognition*,
    2017.'
  id: totrans-1476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] C. Gou, Y. Wu, K. Wang, K. Wang, F. Wang, 和 Q. Ji, “一种用于同时眼睛检测和眼睛状态估计的联合级联框架，”
    *模式识别*，2017。'
- en: '[75] J. Pi and B. E. Shi, “Task-embedded online eye-tracker calibration for
    improving robustness to head motion,” in *Proceedings of the 11th ACM Symposium
    on Eye Tracking Research & Applications*, 2019, pp. 1–9.'
  id: totrans-1477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] J. Pi 和 B. E. Shi, “任务嵌入的在线眼动仪标定以提高对头部运动的鲁棒性，” 见 *第11届ACM眼动研究与应用研讨会论文集*，2019，页1–9。'
- en: '[76] D. Lian, L. Hu, W. Luo, Y. Xu, L. Duan, J. Yu, and S. Gao, “Multiview
    multitask gaze estimation with deep convolutional neural networks,” *IEEE transactions
    on neural networks and learning systems*, vol. 30, no. 10, pp. 3010–3023, 2018.'
  id: totrans-1478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] D. Lian, L. Hu, W. Luo, Y. Xu, L. Duan, J. Yu, 和 S. Gao, “基于深度卷积神经网络的多视角多任务凝视估计，”
    *IEEE神经网络与学习系统汇刊*，第30卷，第10期，页3010–3023，2018。'
- en: '[77] S. Jha and C. Busso, “Probabilistic estimation of the gaze region of the
    driver using dense classification,” in *IEEE International Conference on Intelligent
    Transportation Systems*, 2018, pp. 697–702.'
  id: totrans-1479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] S. Jha 和 C. Busso, “使用密集分类的驾驶员凝视区域概率估计，” 见 *IEEE国际智能交通系统会议*，2018，页697–702。'
- en: '[78] Z. Hu, S. Li, C. Zhang, K. Yi, G. Wang, and D. Manocha, “Dgaze: Cnn-based
    gaze prediction in dynamic scenes,” *IEEE transactions on visualization and computer
    graphics*, vol. 26, no. 5, pp. 1902–1911, 2020.'
  id: totrans-1480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Z. Hu, S. Li, C. Zhang, K. Yi, G. Wang, 和 D. Manocha, “Dgaze: 基于CNN的动态场景中凝视预测，”
    *IEEE可视化与计算机图形学汇刊*，第26卷，第5期，页1902–1911，2020。'
- en: '[79] A. Tawari, K. H. Chen, and M. M. Trivedi, “Where is the driver looking:
    Analysis of head, eye and iris for robust gaze zone estimation,” in *IEEE Conference
    on Intelligent Transportation Systems*, 2014, pp. 988–994.'
  id: totrans-1481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] A. Tawari, K. H. Chen, 和 M. M. Trivedi, “驾驶员在看哪里: 头部、眼睛和虹膜的分析用于鲁棒的凝视区域估计，”
    见 *IEEE智能交通系统会议*，2014，页988–994。'
- en: '[80] A. Tawari and M. M. Trivedi, “Robust and continuous estimation of driver
    gaze zone by dynamic analysis of multiple face videos,” in *IEEE Intelligent Vehicles
    Symposium*, 2014, pp. 254–265.'
  id: totrans-1482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] A. Tawari 和 M. M. Trivedi, “通过对多个面部视频的动态分析实现鲁棒且连续的驾驶员凝视区域估计，” 见 *IEEE智能车辆研讨会*，2014，页254–265。'
- en: '[81] B. Vasli, S. Martin, and M. M. Trivedi, “On driver gaze estimation: Explorations
    and fusion of geometric and data driven approaches,” in *IEEE Intelligent Transportation
    Systems*, 2016, pp. 655–660.'
  id: totrans-1483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] B. Vasli, S. Martin, 和 M. M. Trivedi, “关于驾驶员凝视估计: 几何和数据驱动方法的探索与融合，” 见
    *IEEE智能交通系统*，2016，页655–660。'
- en: '[82] L. Fridman, P. Langhans, J. Lee, and B. Reimer, “Driver gaze estimation
    without using eye movement,” *IEEE Intelligent Systems*, pp. 49–56, 2015.'
  id: totrans-1484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] L. Fridman, P. Langhans, J. Lee, 和 B. Reimer, “无需使用眼动的驾驶员凝视估计，” *IEEE智能系统*，页49–56，2015。'
- en: '[83] L. Fridman, J. Lee, B. Reimer, and T. Victor, “‘owl’and ‘lizard’: patterns
    of head pose and eye pose in driver gaze classification,” *IET Computer Vision*,
    vol. 10, no. 4, pp. 308–314, 2016.'
  id: totrans-1485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] L. Fridman, J. Lee, B. Reimer, 和 T. Victor, “‘owl’和‘lizard’: 驾驶员凝视分类中的头部姿势和眼睛姿势模式，”
    *IET计算机视觉*，第10卷，第4期，页308–314，2016。'
- en: '[84] I. H. Choi, S. K. Hong, and Y. G. Kim, “Real-time categorization of driver’s
    gaze zone using the deep learning techniques,” in *International Conference on
    Big Data and Smart Computing*.   IEEE, 2016, pp. 143–148.'
  id: totrans-1486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] I. H. Choi, S. K. Hong, 和 Y. G. Kim, “使用深度学习技术的驾驶员凝视区域实时分类，” 见 *国际大数据与智能计算会议*。
    IEEE，2016，页143–148。'
- en: '[85] S. Lee, J. Jo, H. Jung, K. Park, and J. Kim, “Real-time gaze estimator
    based on driver’s head orientation for forward collision warning system,” *IEEE
    Transactions on Intelligent Transportation Systems*, vol. 12, no. 1, pp. 254–267,
    2011.'
  id: totrans-1487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. Lee, J. Jo, H. Jung, K. Park, 和 J. Kim, “基于驾驶员头部朝向的实时凝视估计器用于前方碰撞警告系统，”
    *IEEE智能交通系统汇刊*，第12卷，第1期，页254–267，2011。'
- en: '[86] J. He, K. Pham, N. Valliappan, P. Xu, C. Roberts, D. Lagun, and V. Navalpakkam,
    “On-device few-shot personalization for real-time gaze estimation,” in *IEEE International
    Conference on Computer Vision Workshops*, 2019, pp. 0–0.'
  id: totrans-1488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] J. He, K. Pham, N. Valliappan, P. Xu, C. Roberts, D. Lagun, 和 V. Navalpakkam,
    “设备上的少样本个性化以实现实时凝视估计，” 见 *IEEE计算机视觉研讨会*，2019，页0–0。'
- en: '[87] Q. Huang, A. Veeraraghavan, and A. Sabharwal, “Tabletgaze: dataset and
    analysis for unconstrained appearance-based gaze estimation in mobile tablets,”
    *Machine Vision and Applications*, vol. 28, no. 5-6, pp. 445–461, 2017.'
  id: totrans-1489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Q. Huang, A. Veeraraghavan, 和 A. Sabharwal, “Tabletgaze: 用于移动平板的无约束外观基础凝视估计的数据集与分析，”
    *机器视觉与应用*，第28卷，第5-6期，页445–461，2017。'
- en: '[88] T. Guo, Y. Liu, H. Zhang, X. Liu, Y. Kwak, B. In Yoo, J.-J. Han, and C. Choi,
    “A generalized and robust method towards practical gaze estimation on smart phone,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision Workshops*,
    2019, pp. 0–0.'
  id: totrans-1490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] T. Guo, Y. Liu, H. Zhang, X. Liu, Y. Kwak, B. In Yoo, J.-J. Han, 和 C.
    Choi, “面向智能手机的通用且稳健的实际注视估计方法，” 见于 *IEEE/CVF国际计算机视觉会议论文集*，2019年，页码0–0。'
- en: '[89] Y. Bao, Y. Cheng, Y. Liu, and F. Lu, “Adaptive feature fusion network
    for gaze tracking in mobile tablets,” *arXiv preprint arXiv:2103.11119*, 2021.'
  id: totrans-1491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Y. Bao, Y. Cheng, Y. Liu, 和 F. Lu, “用于移动平板的自适应特征融合网络进行注视跟踪，” *arXiv 预印本
    arXiv:2103.11119*，2021年。'
- en: '[90] P. Kellnhofer, A. Recasens, S. Stent, W. Matusik, and A. Torralba, “Gaze360:
    Physically unconstrained gaze estimation in the wild,” in *IEEE International
    Conference on Computer Vision*, 2019.'
  id: totrans-1492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] P. Kellnhofer, A. Recasens, S. Stent, W. Matusik, 和 A. Torralba, “Gaze360：野外物理上不受限的注视估计，”
    见于 *IEEE国际计算机视觉会议*，2019年。'
- en: '[91] Y. Yu, G. Liu, and J. Odobez, “Improving few-shot user-specific gaze adaptation
    via gaze redirection synthesis,” in *IEEE Conference on Computer Vision and Pattern
    Recognition*, 2019, pp. 11 937–11 946.'
  id: totrans-1493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. Yu, G. Liu, 和 J. Odobez, “通过注视重定向合成改善少样本用户特定注视适应，” 见于 *IEEE计算机视觉与模式识别会议*，2019年，页码11 937–11 946。'
- en: '[92] K. Wang, R. Zhao, and Q. Ji, “A hierarchical generative model for eye
    image synthesis and eye gaze estimation,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2018, pp. 440–448.'
  id: totrans-1494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] K. Wang, R. Zhao, 和 Q. Ji, “用于眼睛图像合成和注视估计的层次生成模型，” 见于 *IEEE计算机视觉与模式识别会议论文集*，2018年，页码440–448。'
- en: '[93] S. Vora, A. Rangesh, and M. M. Trivedi, “On generalizing driver gaze zone
    estimation using convolutional neural networks,” in *IEEE Intelligent Vehicles
    Symposium (IV)*.   IEEE, 2017, pp. 849–854.'
  id: totrans-1495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] S. Vora, A. Rangesh, 和 M. M. Trivedi, “使用卷积神经网络进行驾驶员注视区域估计的泛化，” 见于 *IEEE智能车辆研讨会（IV）*。IEEE，2017年，页码849–854。'
- en: '[94] X. Zhang, S. Park, T. Beeler, D. Bradley, S. Tang, and O. Hilliges, “Eth-xgaze:
    A large scale dataset for gaze estimation under extreme head pose and gaze variation,”
    in *European Conference on Computer Vision*.   Springer, 2020, pp. 365–381.'
  id: totrans-1496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] X. Zhang, S. Park, T. Beeler, D. Bradley, S. Tang, 和 O. Hilliges, “Eth-xgaze：用于极端头部姿势和注视变化下的注视估计的大规模数据集，”
    见于 *欧洲计算机视觉会议*。Springer，2020年，页码365–381。'
- en: '[95] X. Zhang, Y. Sugano, and A. Bulling, “Everyday eye contact detection using
    unsupervised gaze target discovery,” in *ACM User Interface Software and Technology*,
    2017, pp. 193–203.'
  id: totrans-1497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] X. Zhang, Y. Sugano, 和 A. Bulling, “利用无监督注视目标发现的日常眼神接触检测，” 见于 *ACM用户界面软件与技术*，2017年，页码193–203。'
- en: '[96] C. H. Morimoto and M. R. Mimica, “Eye gaze tracking techniques for interactive
    applications,” *Computer vision and image understanding*, vol. 98, no. 1, pp.
    4–24, 2005.'
  id: totrans-1498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. H. Morimoto 和 M. R. Mimica, “用于交互应用的眼动追踪技术，” *计算机视觉与图像理解*，第98卷，第1期，页码4–24，2005年。'
- en: '[97] S. Park, X. Zhang, A. Bulling, and O. Hilliges, “Learning to find eye
    region landmarks for remote gaze estimation in unconstrained settings,” in *Proceedings
    of the 2018 ACM Symposium on Eye Tracking Research & Applications*, 2018, pp.
    1–10.'
  id: totrans-1499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] S. Park, X. Zhang, A. Bulling, 和 O. Hilliges, “学习在无约束环境下为远程注视估计找到眼睛区域标志点，”
    见于 *2018 ACM眼动追踪研究与应用研讨会论文集*，2018年，页码1–10。'
- en: '[98] P. M. Corcoran, F. Nanu, S. Petrescu, and P. Bigioi, “Real-time eye gaze
    tracking for gaming design and consumer electronics systems,” *IEEE Transactions
    on Consumer Electronics*, vol. 58, no. 2, pp. 347–355, 2012.'
  id: totrans-1500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] P. M. Corcoran, F. Nanu, S. Petrescu, 和 P. Bigioi, “用于游戏设计和消费电子系统的实时眼动追踪，”
    *IEEE消费电子学报*，第58卷，第2期，页码347–355，2012年。'
- en: '[99] S. Chu, N. Paul, and L. Ruel, “Using eye tracking technology to examine
    the effectiveness of design elements on news websites.” *Information Design Journal
    (IDJ)*, vol. 17, no. 1, 2009.'
  id: totrans-1501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] S. Chu, N. Paul, 和 L. Ruel, “使用眼动追踪技术检查新闻网站设计元素的有效性。” *信息设计杂志（IDJ）*，第17卷，第1期，2009年。'
- en: '[100] J. Chen, J. Zhang, J. Fan, T. Chen, E. Sangineto, and N. Sebe, “Mggr:
    Multimodal-guided gaze redirection with coarse-to-fine learning,” *arXiv preprint
    arXiv:2004.03064*, 2020.'
  id: totrans-1502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] J. Chen, J. Zhang, J. Fan, T. Chen, E. Sangineto, 和 N. Sebe, “Mggr：基于多模态引导的注视重定向与粗到细学习，”
    *arXiv 预印本 arXiv:2004.03064*，2020年。'
- en: '[101] Y. Zheng, S. Park, X. Zhang, S. De Mello, and O. Hilliges, “Self-learning
    transformations for improving gaze and head redirection,” *arXiv preprint arXiv:2010.12307*,
    2020.'
  id: totrans-1503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Y. Zheng, S. Park, X. Zhang, S. De Mello, 和 O. Hilliges, “自学习变换以改善注视和头部重定向，”
    *arXiv 预印本 arXiv:2010.12307*，2020年。'
- en: '[102] J. Chen, J. Zhang, E. Sangineto, T. Chen, J. Fan, and N. Sebe, “Coarse-to-fine
    gaze redirection with numerical and pictorial guidance,” in *Proceedings of the
    IEEE/CVF Winter Conference on Applications of Computer Vision*, 2021, pp. 3665–3674.'
  id: totrans-1504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Chen, J. Zhang, E. Sangineto, T. Chen, J. Fan, 和 N. Sebe，“粗到细的注视重定向与数字和图示指导，”发表于
    *IEEE/CVF计算机视觉冬季会议*，2021年，第3665–3674页。'
- en: '[103] K. Ruhland, C. E. Peters, S. Andrist, J. B. Badler, N. I. Badler, M. Gleicher,
    B. Mutlu, and R. McDonnell, “A review of eye gaze in virtual agents, social robotics
    and hci: Behaviour generation, user interaction and perception,” in *Computer
    graphics forum*, vol. 34, no. 6.   Wiley Online Library, 2015, pp. 299–326.'
  id: totrans-1505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] K. Ruhland, C. E. Peters, S. Andrist, J. B. Badler, N. I. Badler, M.
    Gleicher, B. Mutlu, 和 R. McDonnell，“虚拟代理、社交机器人和人机交互中眼神的回顾：行为生成、用户交互和感知，”发表于 *计算机图形学论坛*，第34卷，第6期。
    Wiley在线图书馆，2015年，第299–326页。'
- en: '[104] T. Baltrušaitis, P. Robinson, and L.-P. Morency, “Openface: an open source
    facial behavior analysis toolkit,” in *IEEE Winter Conference on Applications
    of Computer Vision*, 2016, pp. 1–10.'
  id: totrans-1506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] T. Baltrušaitis, P. Robinson, 和 L.-P. Morency，“OpenFace：一个开源面部行为分析工具包，”发表于
    *IEEE冬季计算机视觉应用会议*，2016年，第1–10页。'
- en: '[105] T. Baltrusaitis, A. Zadeh, Y. C. Lim, and L.-P. Morency, “Openface 2.0:
    Facial behavior analysis toolkit,” in *2018 13th IEEE International Conference
    on Automatic Face & Gesture Recognition (FG 2018)*.   IEEE, 2018, pp. 59–66.'
  id: totrans-1507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] T. Baltrusaitis, A. Zadeh, Y. C. Lim, 和 L.-P. Morency，“OpenFace 2.0：面部行为分析工具包，”发表于
    *2018年第13届IEEE国际自动面部与姿态识别会议 (FG 2018)*。 IEEE，2018年，第59–66页。'
- en: '[106] D. E. King, “Dlib-ml: A machine learning toolkit,” *Journal of Machine
    Learning Research*, pp. 1755–1758, 2009.'
  id: totrans-1508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] D. E. King，“Dlib-ml：一个机器学习工具包，” *机器学习研究杂志*，第1755–1758页，2009年。'
- en: '[107] P. Müller, M. X. Huang, X. Zhang, and A. Bulling, “Robust eye contact
    detection in natural multi-person interactions using gaze and speaking behaviour,”
    in *Proc. ACM International Symposium on Eye Tracking Research and Applications
    (ETRA)*, 2018, pp. 31:1–31:10.'
  id: totrans-1509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] P. Müller, M. X. Huang, X. Zhang, 和 A. Bulling，“在自然多人交互中使用注视和语言行为的鲁棒眼神接触检测，”发表于
    *ACM国际眼动研究与应用研讨会 (ETRA)*，2018年，第31:1–31:10页。'
- en: '[108] M. J. Marin-Jimenez, V. Kalogeiton, P. Medina-Suarez, and A. Zisserman,
    “Laeo-net: revisiting people looking at each other in videos,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 3477–3485.'
  id: totrans-1510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] M. J. Marin-Jimenez, V. Kalogeiton, P. Medina-Suarez, 和 A. Zisserman，“Laeo-net：重新审视视频中彼此注视的人，”发表于
    *IEEE/CVF计算机视觉与模式识别会议*，2019年，第3477–3485页。'
- en: '[109] ——, “LAEO-Net++: revisiting people Looking At Each Other in videos,”
    in *IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, 2021.'
  id: totrans-1511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] ——，“LAEO-Net++：重新审视视频中彼此注视的人，”发表于 *IEEE模式分析与机器智能汇刊 (TPAMI)*，2021年。'
- en: '[110] R. Kothari, S. De Mello, U. Iqbal, W. Byeon, S. Park, and J. Kautz, “Weakly-supervised
    physically unconstrained gaze estimation,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR)*, June 2021, pp. 9980–9989.'
  id: totrans-1512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] R. Kothari, S. De Mello, U. Iqbal, W. Byeon, S. Park, 和 J. Kautz，“弱监督物理无约束的注视估计，”发表于
    *IEEE/CVF计算机视觉与模式识别会议 (CVPR)*，2021年6月，第9980–9989页。'
- en: '[111] L. Fan, W. Wang, S. Huang, X. Tang, and S.-C. Zhu, “Understanding human
    gaze communication by spatio-temporal graph reasoning,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2019, pp. 5724–5733.'
  id: totrans-1513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] L. Fan, W. Wang, S. Huang, X. Tang, 和 S.-C. Zhu，“通过时空图推理理解人类注视交流，”发表于
    *IEEE/CVF国际计算机视觉大会*，2019年，第5724–5733页。'
- en: '[112] M. Zhang, Y. Liu, and F. Lu, “Gazeonce: Real-time multi-person gaze estimation,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    (CVPR)*, June 2022, pp. 4197–4206.'
  id: totrans-1514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] M. Zhang, Y. Liu, 和 F. Lu，“GazeOnce：实时多人注视估计，”发表于 *IEEE/CVF计算机视觉与模式识别会议
    (CVPR)*，2022年6月，第4197–4206页。'
- en: '[113] A. Recasens, A. Khosla, C. Vondrick, and A. Torralba, “Where are they
    looking?” in *Advances in Neural Information Processing Systems*, vol. 28.   Curran
    Associates, Inc., 2015.'
  id: totrans-1515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Recasens, A. Khosla, C. Vondrick, 和 A. Torralba，“他们在看哪里？”发表于 *神经信息处理系统进展*，第28卷。
    Curran Associates, Inc.，2015年。'
- en: '[114] E. Chong, Y. Wang, N. Ruiz, and J. M. Rehg, “Detecting attended visual
    targets in video,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2020, pp. 5396–5406.'
  id: totrans-1516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] E. Chong, Y. Wang, N. Ruiz, 和 J. M. Rehg，“在视频中检测注意到的视觉目标，”发表于 *IEEE/CVF计算机视觉与模式识别会议*，2020年，第5396–5406页。'
- en: '[115] D. Tu, X. Min, H. Duan, G. Guo, G. Zhai, and W. Shen, “End-to-end human-gaze-target
    detection with transformers,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2022, pp. 2202–2210.'
  id: totrans-1517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] D. Tu, X. Min, H. Duan, G. Guo, G. Zhai, 和 W. Shen, “基于变换器的端到端人类注视目标检测”，发表于
    *IEEE/CVF计算机视觉与模式识别会议 (CVPR)*，2022年6月，页码2202–2210。'
- en: '[116] B. Wang, T. Hu, B. Li, X. Chen, and Z. Zhang, “Gatector: A unified framework
    for gaze object prediction,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR)*, June 2022, pp. 19 588–19 597.'
  id: totrans-1518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] B. Wang, T. Hu, B. Li, X. Chen, 和 Z. Zhang, “Gatector: 一个统一的注视对象预测框架”，发表于
    *IEEE/CVF计算机视觉与模式识别会议 (CVPR)*，2022年6月，页码19,588–19,597。'
- en: '[117] Z. Yang, L. Huang, Y. Chen, Z. Wei, S. Ahn, G. Zelinsky, D. Samaras,
    and M. Hoai, “Predicting goal-directed human attention using inverse reinforcement
    learning,” in *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*, 2020, pp. 193–202.'
  id: totrans-1519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Z. Yang, L. Huang, Y. Chen, Z. Wei, S. Ahn, G. Zelinsky, D. Samaras,
    和 M. Hoai, “使用逆强化学习预测目标导向的人类注意力”，发表于 *IEEE/CVF计算机视觉与模式识别会议*，2020年，页码193–202。'
- en: '[118] G. Zelinsky, Z. Yang, L. Huang, Y. Chen, S. Ahn, Z. Wei, H. Adeli, D. Samaras,
    and M. Hoai, “Benchmarking gaze prediction for categorical visual search,” in
    *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition
    Workshops*, 2019, pp. 0–0.'
  id: totrans-1520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] G. Zelinsky, Z. Yang, L. Huang, Y. Chen, S. Ahn, Z. Wei, H. Adeli, D.
    Samaras, 和 M. Hoai, “类别视觉搜索中的注视预测基准测试”，发表于 *IEEE/CVF计算机视觉与模式识别会议研讨会*，2019年，页码0–0。'
- en: '[119] X. Chen, M. Jiang, and Q. Zhao, “Predicting human scanpaths in visual
    question answering,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2021, pp. 10 876–10 885.'
  id: totrans-1521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] X. Chen, M. Jiang, 和 Q. Zhao, “在视觉问答中预测人类扫描路径”，发表于 *IEEE/CVF计算机视觉与模式识别会议*，2021年，页码10,876–10,885。'
- en: '[120] M. Kümmerer and M. Bethge, “State-of-the-art in human scanpath prediction,”
    *arXiv preprint arXiv:2102.12239*, 2021.'
  id: totrans-1522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] M. Kümmerer 和 M. Bethge, “人类扫描路径预测的最新进展”，*arXiv预印本 arXiv:2102.12239*，2021年。'
- en: '[121] J. Bao, B. Liu, and J. Yu, “Escnet: Gaze target detection with the understanding
    of 3d scenes,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition (CVPR)*, June 2022, pp. 14 126–14 135.'
  id: totrans-1523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] J. Bao, B. Liu, 和 J. Yu, “Escnet: 通过理解3D场景进行注视目标检测”，发表于 *IEEE/CVF计算机视觉与模式识别会议
    (CVPR)*，2022年6月，页码14,126–14,135。'
- en: '[122] K. Liang, Y. Chahir, M. Molina, C. Tijus, and F. Jouen, “Appearance-based
    gaze tracking with spectral clustering and semi-supervised gaussian process regression,”
    in *Proceedings of the 2013 Conference on Eye Tracking South Africa*, 2013, pp.
    17–23.'
  id: totrans-1524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] K. Liang, Y. Chahir, M. Molina, C. Tijus, 和 F. Jouen, “基于外观的注视追踪结合光谱聚类和半监督高斯过程回归”，发表于
    *2013年南非眼动追踪会议*，2013年，页码17–23。'
- en: '[123] K. Wang and Q. Ji, “Hybrid model and appearance based eye tracking with
    kinect,” in *Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research
    & Applications*, 2016, pp. 331–332.'
  id: totrans-1525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] K. Wang 和 Q. Ji, “基于Kinect的混合模型和外观基础眼动追踪”，发表于 *第九届ACM眼动研究与应用双年会*，2016年，页码331–332。'
- en: '[124] K. Zhang, Z. Zhang, Z. Li, and Y. Qiao, “Joint face detection and alignment
    using multitask cascaded convolutional networks,” *IEEE Signal Processing Letters*,
    vol. 23, no. 10, pp. 1499–1503, 2016.'
  id: totrans-1526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] K. Zhang, Z. Zhang, Z. Li, 和 Y. Qiao, “使用多任务级联卷积网络的联合面部检测与对齐”，*IEEE信号处理快报*，第23卷，第10期，页码1499–1503，2016年。'
- en: '[125] J. Li, Y. Wang, C. Wang, Y. Tai, J. Qian, J. Yang, C. Wang, J. Li, and
    F. Huang, “Dsfd: Dual shot face detector,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2019.'
  id: totrans-1527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] J. Li, Y. Wang, C. Wang, Y. Tai, J. Qian, J. Yang, C. Wang, J. Li, 和
    F. Huang, “Dsfd: 双镜头面部检测器”，发表于 *IEEE计算机视觉与模式识别会议*，2019年。'
- en: '[126] J. Wang, Y. Liu, Y. Hu, H. Shi, and T. Mei, “Facex-zoo: A pytorh toolbox
    for face recognition,” *arXiv preprint arXiv:2101.04407*, 2021.'
  id: totrans-1528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] J. Wang, Y. Liu, Y. Hu, H. Shi, 和 T. Mei, “Facex-zoo: 一个用于面部识别的pytorch工具箱”，*arXiv预印本
    arXiv:2101.04407*，2021年。'
- en: '[127] N. Markuš, M. Frljak, I. S. Pandžić, J. Ahlberg, and R. Forchheimer,
    “Eye pupil localization with an ensemble of randomized trees,” *Pattern Recognition*,
    2014.'
  id: totrans-1529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] N. Markuš, M. Frljak, I. S. Pandžić, J. Ahlberg, 和 R. Forchheimer, “基于随机森林集成的瞳孔定位”，*模式识别*，2014年。'
- en: '[128] D. Tian, G. He, J. Wu, H. Chen, and Y. Jiang, “An accurate eye pupil
    localization approach based on adaptive gradient boosting decision tree,” in *2016
    Visual Communications and Image Processing (VCIP)*.   IEEE, 2016, pp. 1–4.'
  id: totrans-1530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] D. Tian, G. He, J. Wu, H. Chen, 和 Y. Jiang, “基于自适应梯度提升决策树的精确瞳孔定位方法”，发表于
    *2016视觉通信与图像处理 (VCIP)*。IEEE，2016年，页码1–4。'
- en: '[129] A. Kacete, J. Royan, R. Seguier, M. Collobert, and C. Soladie, “Real-time
    eye pupil localization using hough regression forest,” in *2016 IEEE Winter Conference
    on Applications of Computer Vision (WACV)*.   IEEE, 2016, pp. 1–8.'
  id: totrans-1531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] A. Kacete, J. Royan, R. Seguier, M. Collobert, 和 C. Soladie, “使用霍夫回归森林的实时眼瞳定位,”
    见于 *2016 IEEE 冬季计算机视觉应用会议 (WACV)*. IEEE, 2016, 第 1–8 页。'
- en: '[130] J. H. Choi, K. I. Lee, Y. C. Kim, and B. C. Song, “Accurate eye pupil
    localization using heterogeneous cnn models,” in *2019 IEEE International Conference
    on Image Processing (ICIP)*.   IEEE, 2019, pp. 2179–2183.'
  id: totrans-1532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] J. H. Choi, K. I. Lee, Y. C. Kim, 和 B. C. Song, “使用异构 CNN 模型的精确眼瞳定位,”
    见于 *2019 IEEE 国际图像处理会议 (ICIP)*. IEEE, 2019, 第 2179–2183 页。'
- en: '[131] W. Sankowski, K. Grabowski, M. Napieralska, M. Zubert, and A. Napieralski,
    “Reliable algorithm for iris segmentation in eye image,” *Image and vision computing*,
    vol. 28, no. 2, pp. 231–237, 2010.'
  id: totrans-1533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] W. Sankowski, K. Grabowski, M. Napieralska, M. Zubert, 和 A. Napieralski,
    “眼部图像中虹膜分割的可靠算法,” *图像与视觉计算*, 卷 28, 第 2 期, 第 231–237 页, 2010。'
- en: '[132] P. Radu, J. Ferryman, and P. Wild, “A robust sclera segmentation algorithm,”
    in *2015 IEEE 7th International Conference on Biometrics Theory, Applications
    and Systems (BTAS)*.   IEEE, 2015, pp. 1–6.'
  id: totrans-1534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] P. Radu, J. Ferryman, 和 P. Wild, “一种鲁棒的巩膜分割算法,” 见于 *2015 IEEE 第七届国际生物特征识别理论、应用与系统会议
    (BTAS)*. IEEE, 2015, 第 1–6 页。'
- en: '[133] A. Das, U. Pal, M. A. Ferrer, M. Blumenstein, D. Štepec, P. Rot, Ž. Emeršič,
    P. Peer, V. Štruc, S. A. Kumar *et al.*, “Sserbc 2017: Sclera segmentation and
    eye recognition benchmarking competition,” in *2017 IEEE International Joint Conference
    on Biometrics (IJCB)*.   IEEE, 2017, pp. 742–747.'
  id: totrans-1535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] A. Das, U. Pal, M. A. Ferrer, M. Blumenstein, D. Štepec, P. Rot, Ž. Emeršič,
    P. Peer, V. Štruc, S. A. Kumar *等*, “Sserbc 2017: 巩膜分割与眼睛识别基准竞赛,” 见于 *2017 IEEE
    国际联合生物特征识别会议 (IJCB)*. IEEE, 2017, 第 742–747 页。'
- en: '[134] D. R. Lucio, R. Laroca, E. Severo, A. S. Britto, and D. Menotti, “Fully
    convolutional networks and generative adversarial networks applied to sclera segmentation,”
    in *2018 IEEE 9th International Conference on Biometrics Theory, Applications
    and Systems (BTAS)*.   IEEE, 2018, pp. 1–7.'
  id: totrans-1536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] D. R. Lucio, R. Laroca, E. Severo, A. S. Britto, 和 D. Menotti, “全卷积网络和生成对抗网络在巩膜分割中的应用,”
    见于 *2018 IEEE 第九届国际生物特征识别理论、应用与系统会议 (BTAS)*. IEEE, 2018, 第 1–7 页。'
- en: '[135] A. Das, U. Pal, M. Blumenstein, and M. A. F. Ballester, “Sclera recognition-a
    survey,” in *2013 2nd IAPR Asian Conference on Pattern Recognition*.   IEEE, 2013,
    pp. 917–921.'
  id: totrans-1537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. Das, U. Pal, M. Blumenstein, 和 M. A. F. Ballester, “巩膜识别-综述,” 见于 *2013
    第二届 IAPR 亚洲模式识别会议*. IEEE, 2013, 第 917–921 页。'
- en: '[136] A. K. Chaudhary, R. Kothari, M. Acharya, S. Dangi, N. Nair, R. Bailey,
    C. Kanan, G. Diaz, and J. B. Pelz, “Ritnet: real-time semantic segmentation of
    the eye for gaze tracking,” in *2019 IEEE/CVF International Conference on Computer
    Vision Workshop (ICCVW)*.   IEEE, 2019, pp. 3698–3702.'
  id: totrans-1538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] A. K. Chaudhary, R. Kothari, M. Acharya, S. Dangi, N. Nair, R. Bailey,
    C. Kanan, G. Diaz, 和 J. B. Pelz, “Ritnet: 实时语义分割眼睛用于注视跟踪,” 见于 *2019 IEEE/CVF 国际计算机视觉会议研讨会
    (ICCVW)*. IEEE, 2019, 第 3698–3702 页。'
- en: '[137] N. N. Pandey and N. B. Muppalaneni, “Real-time drowsiness identification
    based on eye state analysis,” in *2021 International Conference on Artificial
    Intelligence and Smart Systems (ICAIS)*.   IEEE, 2021, pp. 1182–1187.'
  id: totrans-1539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] N. N. Pandey 和 N. B. Muppalaneni, “基于眼睛状态分析的实时困倦识别,” 见于 *2021 国际人工智能与智能系统会议
    (ICAIS)*. IEEE, 2021, 第 1182–1187 页。'
- en: '[138] M. Monaro, P. Capuozzo, F. Ragucci, A. Maffei, A. Curci, C. Scarpazza,
    A. Angrilli, and G. Sartori, “Using blink rate to detect deception: A study to
    validate an automatic blink detector and a new dataset of videos from liars and
    truth-tellers,” in *International Conference on Human-Computer Interaction*.   Springer,
    2020, pp. 494–509.'
  id: totrans-1540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] M. Monaro, P. Capuozzo, F. Ragucci, A. Maffei, A. Curci, C. Scarpazza,
    A. Angrilli, 和 G. Sartori, “利用眨眼频率检测欺骗：验证自动眨眼检测器和新的撒谎者与诚实者视频数据集的研究,” 见于 *国际人机交互会议*.
    Springer, 2020, 第 494–509 页。'
- en: '[139] T. Drutarovsky and A. Fogelton, “Eye blink detection using variance of
    motion vectors,” in *European Conference on Computer Vision*.   Springer, 2014,
    pp. 436–448.'
  id: totrans-1541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] T. Drutarovsky 和 A. Fogelton, “使用运动矢量的方差进行眼睛眨动检测,” 见于 *欧洲计算机视觉会议*. Springer,
    2014, 第 436–448 页。'
- en: '[140] A. Królak and P. Strumiłło, “Eye-blink detection system for human–computer
    interaction,” *Universal Access in the Information Society*, vol. 11, no. 4, pp.
    409–419, 2012.'
  id: totrans-1542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] A. Królak 和 P. Strumiłło, “用于人机交互的眨眼检测系统,” *信息社会中的普遍访问*, 卷 11, 第 4 期,
    第 409–419 页, 2012。'
- en: '[141] J. Cech and T. Soukupova, “Real-time eye blink detection using facial
    landmarks,” *Cent. Mach. Perception, Dep. Cybern. Fac. Electr. Eng. Czech Tech.
    Univ. Prague*, pp. 1–8, 2016.'
  id: totrans-1543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] J. Cech 和 T. Soukupova，“使用面部标志点进行实时眼睑检测，” *中央机器感知，电气工程系，捷克理工大学布拉格*，第1–8页，2016年。'
- en: '[142] R. Daza, A. Morales, J. Fierrez, and R. Tolosana, “mebal: A multimodal
    database for eye blink detection and attention level estimation,” in *Companion
    Publication of the 2020 International Conference on Multimodal Interaction*, 2020,
    pp. 32–36.'
  id: totrans-1544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] R. Daza, A. Morales, J. Fierrez, 和 R. Tolosana，“mebal：用于眼睑检测和注意水平估计的多模态数据库，”
    见 *2020年国际多模态交互会议伴刊*，2020年，第32–36页。'
- en: '[143] G. Hu, Y. Xiao, Z. Cao, L. Meng, Z. Fang, J. T. Zhou, and J. Yuan, “Towards
    real-time eyeblink detection in the wild: Dataset, theory and practices,” *IEEE
    Transactions on Information Forensics and Security*, vol. 15, pp. 2194–2208, 2019.'
  id: totrans-1545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] G. Hu, Y. Xiao, Z. Cao, L. Meng, Z. Fang, J. T. Zhou, 和 J. Yuan，“迈向实时眼睑检测：数据集、理论与实践，”
    *IEEE 信息取证与安全汇刊*，第15卷，第2194–2208页，2019年。'
- en: '[144] J. Perry and A. Fernandez, “Minenet: A dilated cnn for semantic segmentation
    of eye features,” in *Proceedings of the IEEE International Conference on Computer
    Vision Workshops*, 2019, pp. 0–0.'
  id: totrans-1546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] J. Perry 和 A. Fernandez，“Minenet：用于眼部特征语义分割的膨胀卷积神经网络，” 见 *IEEE 国际计算机视觉会议研讨会论文集*，2019年，第0–0页。'
- en: '[145] K. Wang, R. Zhao, H. Su, and Q. Ji, “Generalizing eye tracking with bayesian
    adversarial learning,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2019, pp. 11 907–11 916.'
  id: totrans-1547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] K. Wang, R. Zhao, H. Su, 和 Q. Ji，“通过贝叶斯对抗学习泛化眼动追踪，” 见 *IEEE/CVF 计算机视觉与模式识别会议*，2019年，第11 907–11 916页。'
- en: '[146] C. Palmero Cantarino, O. V. Komogortsev, and S. S. Talathi, “Benefits
    of temporal information for appearance-based gaze estimation,” in *ACM Symposium
    on Eye Tracking Research and Applications*, 2020, pp. 1–5.'
  id: totrans-1548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] C. Palmero Cantarino, O. V. Komogortsev, 和 S. S. Talathi，“基于外观的注视估计中的时间信息优势，”
    见 *ACM 眼动追踪研究与应用研讨会*，2020年，第1–5页。'
- en: '[147] S. Nonaka, S. Nobuhara, and K. Nishino, “Dynamic 3d gaze from afar: Deep
    gaze estimation from temporal eye-head-body coordination,” in *Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, June
    2022, pp. 2192–2201.'
  id: totrans-1549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] S. Nonaka, S. Nobuhara, 和 K. Nishino，“远程动态三维注视：基于时间的眼头体协调的深度注视估计，” 见
    *IEEE/CVF 计算机视觉与模式识别会议（CVPR）*，2022年6月，第2192–2201页。'
- en: '[148] S. Park, E. Aksan, X. Zhang, and O. Hilliges, “Towards end-to-end video-based
    eye-tracking,” in *European Conference on Computer Vision (ECCV)*, 2020.'
  id: totrans-1550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] S. Park, E. Aksan, X. Zhang, 和 O. Hilliges，“迈向端到端视频眼动追踪，” 见 *欧洲计算机视觉大会（ECCV）*，2020年。'
- en: '[149] Y. Cheng and F. Lu, “Gaze estimation using transformer,” *arXiv preprint
    arXiv:2105.14424*, 2021.'
  id: totrans-1551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Cheng 和 F. Lu，“使用变压器进行注视估计，” *arXiv 预印本 arXiv:2105.14424*，2021年。'
- en: '[150] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, and R. Webb,
    “Learning from simulated and unsupervised images through adversarial training,”
    in *Proceedings of the IEEE conference on computer vision and pattern recognition*,
    2017, pp. 2107–2116.'
  id: totrans-1552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] A. Shrivastava, T. Pfister, O. Tuzel, J. Susskind, W. Wang, 和 R. Webb，“通过对抗训练从模拟和无监督图像中学习，”
    见 *IEEE 计算机视觉与模式识别会议*，2017年，第2107–2116页。'
- en: '[151] S.-H. Kim, G.-S. Lee, H.-J. Yang *et al.*, “Eye semantic segmentation
    with a lightweight model,” in *2019 IEEE/CVF International Conference on Computer
    Vision Workshop (ICCVW)*.   IEEE, 2019, pp. 3694–3697.'
  id: totrans-1553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] S.-H. Kim, G.-S. Lee, H.-J. Yang *等*，“使用轻量级模型进行眼部语义分割，” 见 *2019 IEEE/CVF
    国际计算机视觉会议研讨会（ICCVW）*。 IEEE，2019年，第3694–3697页。'
- en: '[152] P. Rot, Ž. Emeršič, V. Struc, and P. Peer, “Deep multi-class eye segmentation
    for ocular biometrics,” in *2018 IEEE International Work Conference on Bioinspired
    Intelligence (IWOBI)*.   IEEE, 2018, pp. 1–8.'
  id: totrans-1554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] P. Rot, Ž. Emeršič, V. Struc, 和 P. Peer，“用于眼部生物识别的深度多类眼睛分割，” 见 *2018
    IEEE 国际生物启发智能工作会议（IWOBI）*。 IEEE，2018年，第1–8页。'
- en: '[153] B. Luo, J. Shen, Y. Wang, and M. Pantic, “The ibug eye segmentation dataset,”
    in *2018 Imperial College Computing Student Workshop (ICCSW 2018)*.   Schloss
    Dagstuhl-Leibniz-Zentrum fuer Informatik, 2019.'
  id: totrans-1555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] B. Luo, J. Shen, Y. Wang, 和 M. Pantic，“ibug 眼睛分割数据集，” 见 *2018 帝国学院计算学生研讨会（ICCSW
    2018）*。 Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik，2019年。'
- en: '[154] A. Das, U. Pal, M. A. Ferrer, and M. Blumenstein, “Ssrbc 2016: Sclera
    segmentation and recognition benchmarking competition,” in *2016 International
    Conference on Biometrics (ICB)*.   IEEE, 2016, pp. 1–6.'
  id: totrans-1556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] A. Das, U. Pal, M. A. Ferrer, 和 M. Blumenstein，“SSRB 2016：巩膜分割与识别基准竞赛，”
    见 *2016年国际生物识别会议（ICB）*。 IEEE，2016年，第1–6页。'
- en: '[155] A. Das, U. Pal, M. Blumenstein, C. Wang, Y. He, Y. Zhu, and Z. Sun, “Sclera
    segmentation benchmarking competition in cross-resolution environment,” in *2019
    International Conference on Biometrics (ICB)*.   IEEE, 2019, pp. 1–7.'
  id: totrans-1557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] A. Das, U. Pal, M. Blumenstein, C. Wang, Y. He, Y. Zhu, 和 Z. Sun, “跨分辨率环境下的巩膜分割基准竞赛，”发表于
    *2019年生物识别国际会议 (ICB)*。   IEEE，2019年，第1–7页。'
- en: '[156] F. Boutros, N. Damer, F. Kirchbuchner, and A. Kuijper, “Eye-mms: Miniature
    multi-scale segmentation network of key eye-regions in embedded applications,”
    in *Proceedings of the IEEE International Conference on Computer Vision Workshops*,
    2019, pp. 0–0.'
  id: totrans-1558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] F. Boutros, N. Damer, F. Kirchbuchner, 和 A. Kuijper, “Eye-mms: 嵌入式应用中关键眼睛区域的微型多尺度分割网络，”发表于
    *IEEE国际计算机视觉会议研讨会论文集*，2019年，第0–0页。'
- en: '[157] “OKAO vision, howpublished = [http://www.omron.com/r_d/coretech/vision/okao.html.](http://www.omron.com/r_d/coretech/vision/okao.html.)”'
  id: totrans-1559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] “OKAO vision, 公开地址 = [http://www.omron.com/r_d/coretech/vision/okao.html.](http://www.omron.com/r_d/coretech/vision/okao.html.)”'
- en: '[158] K. A. Funes Mora, F. Monay, and J.-M. Odobez, “Eyediap: A database for
    the development and evaluation of gaze estimation algorithms from rgb and rgb-d
    cameras,” in *Proceedings of the Symposium on Eye Tracking Research and Applications*,
    2014, pp. 255–258.'
  id: totrans-1560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] K. A. Funes Mora, F. Monay, 和 J.-M. Odobez, “Eyediap: 用于从RGB和RGB-D相机开发和评估注视估计算法的数据库，”发表于
    *眼动追踪研究与应用研讨会论文集*，2014年，第255–258页。'
- en: '[159] E. Wood, T. Baltrusaitis, X. Zhang, Y. Sugano, P. Robinson, and A. Bulling,
    “Rendering of eyes for eye-shape registration and gaze estimation,” in *Proceedings
    of the IEEE International Conference on Computer Vision*, 2015, pp. 3756–3764.'
  id: totrans-1561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] E. Wood, T. Baltrusaitis, X. Zhang, Y. Sugano, P. Robinson, 和 A. Bulling,
    “眼睛渲染用于眼形注册和注视估计，”发表于 *IEEE国际计算机视觉会议论文集*，2015年，第3756–3764页。'
- en: '[160] Y. Ganin, D. Kononenko, D. Sungatullina, and V. Lempitsky, “Deepwarp:
    Photorealistic image resynthesis for gaze manipulation,” in *European conference
    on computer vision*.   Springer, 2016, pp. 311–326.'
  id: totrans-1562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Y. Ganin, D. Kononenko, D. Sungatullina, 和 V. Lempitsky, “Deepwarp: 用于注视操控的光照真实图像重合成，”发表于
    *欧洲计算机视觉会议*。   Springer，2016年，第311–326页。'
- en: '[161] “Yu, S.:Harr feature cart-tree based cascade eye detector homepage.,
    howpublished = [http://yushiqi.cn/research/eyedetection](http://yushiqi.cn/research/eyedetection).”'
  id: totrans-1563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] “Yu, S.:Harr特征卡特树级联眼睛检测器主页， 公开地址 = [http://yushiqi.cn/research/eyedetection](http://yushiqi.cn/research/eyedetection).”'
- en: '[162] J. Xiang and G. Zhu, “Joint face detection and facial expression recognition
    with mtcnn,” in *2017 4th International Conference on Information Science and
    Control Engineering (ICISCE)*.   IEEE, 2017, pp. 424–427.'
  id: totrans-1564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] J. Xiang 和 G. Zhu, “使用MTCNN的联合人脸检测和面部表情识别，”发表于 *2017年第四届信息科学与控制工程国际会议
    (ICISCE)*。   IEEE，2017年，第424–427页。'
- en: '[163] V. Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep convolutional
    encoder-decoder architecture for image segmentation,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 39, no. 12, pp. 2481–2495, 2017.'
  id: totrans-1565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] V. Badrinarayanan, A. Kendall, 和 R. Cipolla, “Segnet: 用于图像分割的深度卷积编码解码架构，”
    *IEEE模式分析与机器智能汇刊*，第39卷，第12期，第2481–2495页，2017年。'
- en: '[164] Y. Xiong, H. J. Kim, and V. Singh, “Mixed effects neural networks (menets)
    with applications to gaze estimation,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2019, pp. 7743–7752.'
  id: totrans-1566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Y. Xiong, H. J. Kim, 和 V. Singh, “混合效应神经网络（MENETS）及其在注视估计中的应用，”发表于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2019年，第7743–7752页。'
- en: '[165] P. Kansal and S. Devanathan, “Eyenet: Attention based convolutional encoder-decoder
    network for eye region segmentation,” in *2019 IEEE/CVF International Conference
    on Computer Vision Workshop (ICCVW)*.   IEEE, 2019, pp. 3688–3693.'
  id: totrans-1567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] P. Kansal 和 S. Devanathan, “Eyenet: 基于注意力的卷积编码解码网络用于眼睛区域分割，”发表于 *2019
    IEEE/CVF国际计算机视觉会议研讨会 (ICCVW)*。   IEEE，2019年，第3688–3693页。'
- en: '[166] M. Bühler, S. Park, S. De Mello, X. Zhang, and O. Hilliges, “Content-consistent
    generation of realistic eyes with style,” *arXiv preprint arXiv:1911.03346*, 2019.'
  id: totrans-1568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] M. Bühler, S. Park, S. De Mello, X. Zhang, 和 O. Hilliges, “具有风格的现实眼睛的内容一致生成，”
    *arXiv预印本 arXiv:1911.03346*，2019年。'
- en: '[167] Y. Zhu, Y. Yan, and O. Komogortsev, “Hierarchical hmm for eye movement
    classification,” in *European Conference on Computer Vision*.   Springer, 2020,
    pp. 544–554.'
  id: totrans-1569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Y. Zhu, Y. Yan, 和 O. Komogortsev, “用于眼动分类的层次HMM，”发表于 *欧洲计算机视觉会议*。   Springer，2020年，第544–554页。'
- en: '[168] O. V. Komogortsev and A. Karpov, “Automated classification and scoring
    of smooth pursuit eye movements in the presence of fixations and saccades,” *Behavior
    research methods*, vol. 45, no. 1, pp. 203–215, 2013.'
  id: totrans-1570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] O. V. Komogortsev 和 A. Karpov, “在注视和眼动过程中自动分类和评分平滑追踪眼动,” *行为研究方法*, 第45卷，第1期,
    页码203–215, 2013年。'
- en: '[169] Y. Shen, O. Komogortsev, and S. S. Talathi, “Domain adaptation for eye
    segmentation,” in *European Conference on Computer Vision*.   Springer, 2020,
    pp. 555–569.'
  id: totrans-1571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Y. Shen, O. Komogortsev, 和 S. S. Talathi, “眼睛分割的领域适应,” 收录于 *欧洲计算机视觉会议*.
    Springer, 2020年, 页码555–569。'
- en: '[170] J. Perry and A. S. Fernandez, “Eyeseg: Fast and efficient few-shot semantic
    segmentation,” in *European Conference on Computer Vision*.   Springer, 2020,
    pp. 570–582.'
  id: totrans-1572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] J. Perry 和 A. S. Fernandez, “Eyeseg: 快速高效的少样本语义分割,” 收录于 *欧洲计算机视觉会议*.
    Springer, 2020年, 页码570–582。'
- en: '[171] P. A. Dias, D. Malafronte, H. Medeiros, and F. Odone, “Gaze estimation
    for assisted living environments,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2020, pp. 290–299.'
  id: totrans-1573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] P. A. Dias, D. Malafronte, H. Medeiros, 和 F. Odone, “用于辅助生活环境的注视估计,”
    收录于 *IEEE/CVF计算机视觉冬季会议论文集*, 2020年, 页码290–299。'
- en: '[172] Y. Cheng, X. Zhang, F. Lu, and Y. Sato, “Gaze estimation by exploring
    two-eye asymmetry,” *IEEE Transactions on Image Processing*, vol. 29, pp. 5259–5272,
    2020.'
  id: totrans-1574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Y. Cheng, X. Zhang, F. Lu, 和 Y. Sato, “通过探索双眼不对称进行注视估计,” *IEEE图像处理汇刊*,
    第29卷, 页码5259–5272, 2020年。'
- en: '[173] Y. Cheng, Y. Bao, and F. Lu, “Puregaze: Purifying gaze feature for generalizable
    gaze estimation,” *arXiv preprint arXiv:2103.13173*, 2021.'
  id: totrans-1575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Y. Cheng, Y. Bao, 和 F. Lu, “Puregaze: 用于可泛化注视估计的注视特征净化,” *arXiv预印本 arXiv:2103.13173*,
    2021年。'
- en: '[174] S. Ghosh, M. Hayat, A. Dhall, and J. Knibbe, “Mtgls: Multi-task gaze
    estimation with limited supervision,” in *Proceedings of the IEEE/CVF Winter Conference
    on Applications of Computer Vision*, 2022, pp. 3223–3234.'
  id: totrans-1576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] S. Ghosh, M. Hayat, A. Dhall, 和 J. Knibbe, “Mtgls: 限制监督下的多任务注视估计,” 收录于
    *IEEE/CVF计算机视觉冬季会议论文集*, 2022年, 页码3223–3234。'
- en: '[175] Z. Chen and B. Shi, “Offset calibration for appearance-based gaze estimation
    via gaze decomposition,” in *Proceedings of the IEEE/CVF Winter Conference on
    Applications of Computer Vision*, 2020, pp. 270–279.'
  id: totrans-1577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Z. Chen 和 B. Shi, “通过注视分解进行基于外观的注视估计的偏移校准,” 收录于 *IEEE/CVF计算机视觉冬季会议论文集*,
    2020年, 页码270–279。'
- en: '[176] B. Luo, J. Shen, S. Cheng, Y. Wang, and M. Pantic, “Shape constrained
    network for eye segmentation in the wild,” in *The IEEE Winter Conference on Applications
    of Computer Vision*, 2020, pp. 1952–1960.'
  id: totrans-1578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] B. Luo, J. Shen, S. Cheng, Y. Wang, 和 M. Pantic, “用于野外眼睛分割的形状约束网络,” 收录于
    *IEEE冬季计算机视觉会议*, 2020年, 页码1952–1960。'
- en: '[177] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  id: totrans-1579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] O. Ronneberger, P. Fischer, 和 T. Brox, “U-net: 生物医学图像分割的卷积网络,” 收录于 *国际医学图像计算与计算机辅助干预会议*.
    Springer, 2015年, 页码234–241。'
- en: '[178] W. Wang, J. Shen, X. Dong, A. Borji, and R. Yang, “Inferring salient
    objects from human fixations,” *IEEE transactions on pattern analysis and machine
    intelligence*, vol. 42, no. 8, pp. 1913–1927, 2019.'
  id: totrans-1580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] W. Wang, J. Shen, X. Dong, A. Borji, 和 R. Yang, “从人类注视中推断显著对象,” *IEEE模式分析与机器智能汇刊*,
    第42卷，第8期, 页码1913–1927, 2019年。'
- en: '[179] R. Kothari, Z. Yang, C. Kanan, R. Bailey, J. B. Pelz, and G. J. Diaz,
    “Gaze-in-wild: A dataset for studying eye and head coordination in everyday activities,”
    *Scientific Reports*, vol. 10, no. 1, pp. 1–18, 2020.'
  id: totrans-1581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] R. Kothari, Z. Yang, C. Kanan, R. Bailey, J. B. Pelz, 和 G. J. Diaz, “Gaze-in-wild:
    研究日常活动中眼睛与头部协调的数据集,” *Scientific Reports*, 第10卷，第1期, 页码1–18, 2020年。'
- en: '[180] A. Farkhondeh, C. Palmero, S. Scardapane, and S. Escalera, “Towards self-supervised
    gaze estimation,” *arXiv preprint arXiv:2203.10974*, 2022.'
  id: totrans-1582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] A. Farkhondeh, C. Palmero, S. Scardapane, 和 S. Escalera, “朝向自监督注视估计,”
    *arXiv预印本 arXiv:2203.10974*, 2022年。'
- en: '[181] Z. He, A. Spurr, X. Zhang, and O. Hilliges, “Photo-realistic monocular
    gaze redirection using generative adversarial networks,” in *Proceedings of the
    IEEE International Conference on Computer Vision*, 2019, pp. 6932–6941.'
  id: totrans-1583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Z. He, A. Spurr, X. Zhang, 和 O. Hilliges, “使用生成对抗网络进行照片级真实的单目注视重定向,”
    收录于 *IEEE国际计算机视觉会议论文集*, 2019年, 页码6932–6941。'
- en: '[182] E. Wood, T. Baltrušaitis, L.-P. Morency, P. Robinson, and A. Bulling,
    “Gazedirector: Fully articulated eye gaze redirection in video,” in *Computer
    Graphics Forum*, vol. 37, no. 2.   Wiley Online Library, 2018, pp. 217–225.'
  id: totrans-1584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] E. Wood, T. Baltrušaitis, L.-P. Morency, P. Robinson, 和 A. Bulling, “Gazedirector:
    视频中的全关节眼动重定向，” 见于 *计算机图形学论坛*，第37卷，第2期。Wiley Online Library，2018年，页码217–225。'
- en: '[183] H. Kaur and R. Manduchi, “Subject guided eye image synthesis with application
    to gaze redirection,” in *Proceedings of the IEEE/CVF Winter Conference on Applications
    of Computer Vision*, 2021, pp. 11–20.'
  id: totrans-1585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] H. Kaur 和 R. Manduchi, “基于主题的眼部图像合成及其在注视重定向中的应用，” 见于 *IEEE/CVF计算机视觉应用冬季会议论文集*，2021年，页码11–20。'
- en: '[184] K. A. Funes Mora, F. Monay, and J.-M. Odobez, “Eyediap: A database for
    the development and evaluation of gaze estimation algorithms from rgb and rgb-d
    cameras,” in *ACM Symposium on Eye Tracking Research and Applications*, 2014.'
  id: totrans-1586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] K. A. Funes Mora, F. Monay, 和 J.-M. Odobez, “Eyediap: 一个用于从RGB和RGB-D相机开发和评估注视估计算法的数据库，”
    见于 *ACM眼动追踪研究与应用研讨会*，2014年。'
- en: '[185] R. Derakhshani, A. Ross, and S. Crihalmeanu, “A new biometric modality
    based on conjunctival vasculature,” in *Proceedings of Artificial Neural Networks
    in Engineering*, 2006, pp. 1–8.'
  id: totrans-1587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] R. Derakhshani, A. Ross, 和 S. Crihalmeanu, “一种基于结膜血管的新型生物特征识别模态，” 见于
    *工程中的人工神经网络会议论文集*，2006年，页码1–8。'
- en: '[186] R. Derakhshani and A. Ross, “A texture-based neural network classifier
    for biometric identification using ocular surface vasculature,” in *2007 International
    Joint Conference on Neural Networks*.   IEEE, 2007, pp. 2982–2987.'
  id: totrans-1588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] R. Derakhshani 和 A. Ross, “一种基于纹理的神经网络分类器，用于使用眼表血管进行生物特征识别，” 见于 *2007年国际神经网络联合会议*。IEEE，2007年，页码2982–2987。'
- en: '[187] S. Crihalmeanu, A. Ross, and R. Derakhshani, “Enhancement and registration
    schemes for matching conjunctival vasculature,” in *International Conference on
    Biometrics*.   Springer, 2009, pp. 1240–1249.'
  id: totrans-1589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] S. Crihalmeanu, A. Ross, 和 R. Derakhshani, “结膜血管匹配的增强与配准方案，” 见于 *国际生物特征识别会议*。Springer，2009年，页码1240–1249。'
- en: '[188] Q. He, X. Hong, X. Chai, J. Holappa, G. Zhao, X. Chen, and M. Pietikäinen,
    “Omeg: Oulu multi-pose eye gaze dataset,” in *Scandinavian Conference on Image
    Analysis*.   Springer, 2015, pp. 418–427.'
  id: totrans-1590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Q. He, X. Hong, X. Chai, J. Holappa, G. Zhao, X. Chen, 和 M. Pietikäinen,
    “Omeg: Oulu多姿态眼动数据集，” 见于 *斯堪的纳维亚图像分析会议*。Springer，2015年，页码418–427。'
- en: '[189] A. Recasens^∗, A. Khosla^∗, C. Vondrick, and A. Torralba, “Where are
    they looking?” in *Advances in Neural Information Processing Systems (NIPS)*,
    2015, ^∗ indicates equal contribution.'
  id: totrans-1591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] A. Recasens^∗, A. Khosla^∗, C. Vondrick, 和 A. Torralba, “他们在看哪里？” 见于
    *神经信息处理系统（NIPS）进展*，2015年，^∗表示同等贡献。'
- en: '[190] E. Wood, T. Baltrušaitis, L.-P. Morency, P. Robinson, and A. Bulling,
    “Learning an appearance-based gaze estimator from one million synthesised images,”
    in *Proceedings of the Ninth Biennial ACM Symposium on Eye Tracking Research &
    Applications*, 2016, pp. 131–138.'
  id: totrans-1592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] E. Wood, T. Baltrušaitis, L.-P. Morency, P. Robinson, 和 A. Bulling, “从一百万张合成图像中学习基于外观的注视估计器，”
    见于 *第九届ACM双年会眼动追踪研究与应用研讨会论文集*，2016年，页码131–138。'
- en: '[191] K. Cortacero, T. Fischer, and Y. Demiris, “Rt-bene: a dataset and baselines
    for real-time blink estimation in natural environments,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision Workshops*, 2019, pp. 0–0.'
  id: totrans-1593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] K. Cortacero, T. Fischer, 和 Y. Demiris, “Rt-bene: 一个用于自然环境中实时眨眼估计的数据集和基准，”
    见于 *IEEE/CVF国际计算机视觉会议工作坊论文集*，2019年，页码0–0。'
- en: '[192] S. Laine, T. Karras, T. Aila, A. Herva, S. Saito, R. Yu, H. Li, and J. Lehtinen,
    “Production-level facial performance capture using deep convolutional neural networks,”
    in *Proceedings of the ACM SIGGRAPH/Eurographics symposium on computer animation*,
    2017, pp. 1–10.'
  id: totrans-1594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] S. Laine, T. Karras, T. Aila, A. Herva, S. Saito, R. Yu, H. Li, 和 J.
    Lehtinen, “使用深度卷积神经网络的生产级面部性能捕捉，” 见于 *ACM SIGGRAPH/Eurographics计算机动画研讨会论文集*，2017年，页码1–10。'
- en: '[193] H. Tomas, M. Reyes, R. Dionido, M. Ty, J. Mirando, J. Casimiro, R. Atienza,
    and R. Guinto, “Goo: A dataset for gaze object prediction in retail environments,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2021, pp. 3125–3133.'
  id: totrans-1595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] H. Tomas, M. Reyes, R. Dionido, M. Ty, J. Mirando, J. Casimiro, R. Atienza,
    和 R. Guinto, “Goo: 用于零售环境中注视对象预测的数据集，” 见于 *IEEE/CVF计算机视觉与模式识别会议论文集*，2021年，页码3125–3133。'
- en: '[194] K. J. Emery, M. Zannoli, J. Warren, L. Xiao, and S. S. Talathi, “Openneeds:
    A dataset of gaze, head, hand, and scene signals during exploration in open-ended
    vr environments,” in *ACM Symposium on Eye Tracking Research and Applications*,
    2021, pp. 1–7.'
  id: totrans-1596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] K. J. Emery, M. Zannoli, J. Warren, L. Xiao, 和 S. S. Talathi，“Openneeds:
    一套关于在开放式虚拟现实环境中探索期间的注视、头部、手部和场景信号的数据集，” 收录于 *ACM眼动追踪研究与应用研讨会*，2021年，第1–7页。'
- en: '[195] L. Wolf, Z. Freund, and S. Avidan, “An eye for an eye: A single camera
    gaze-replacement method,” in *2010 IEEE Computer Society Conference on Computer
    Vision and Pattern Recognition*.   IEEE, 2010, pp. 817–824.'
  id: totrans-1597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] L. Wolf, Z. Freund, 和 S. Avidan，“以眼还眼：一种单摄像头注视替代方法，” 收录于 *2010 IEEE计算机学会计算机视觉与模式识别会议*。IEEE，2010年，第817–824页。'
- en: '[196] D. Kononenko and V. Lempitsky, “Learning to look up: Realtime monocular
    gaze correction using machine learning,” in *Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition*, 2015, pp. 4667–4675.'
  id: totrans-1598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] D. Kononenko 和 V. Lempitsky，“学习向上看：使用机器学习进行实时单目注视校正，” 收录于 *IEEE计算机视觉与模式识别会议论文集*，2015年，第4667–4675页。'
- en: '[197] M. Sela, P. Xu, J. He, V. Navalpakkam, and D. Lagun, “Gazegan-unpaired
    adversarial image generation for gaze estimation,” *arXiv preprint arXiv:1711.09767*,
    2017.'
  id: totrans-1599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] M. Sela, P. Xu, J. He, V. Navalpakkam, 和 D. Lagun，“Gazegan-无配对对抗图像生成用于注视估计，”
    *arXiv预印本 arXiv:1711.09767*，2017年。'
- en: '[198] B. Benfold and I. Reid, “Unsupervised learning of a scene-specific coarse
    gaze estimator,” in *IEEE International Conference on Computer Vision*, 2011,
    pp. 2344–2351.'
  id: totrans-1600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] B. Benfold 和 I. Reid，“无监督学习的场景特定粗略注视估计器，” 收录于 *IEEE国际计算机视觉大会*，2011年，第2344–2351页。'
- en: '[199] T. Li, Q. Liu, and X. Zhou, “Ultra-low-power gaze tracking for virtual
    reality,” *GetMobile: Mobile Comp. and Comm.*, vol. 22, no. 3, p. 27–31, Jan.
    2019\. [Online]. Available: [https://doi.org/10.1145/3308755.3308765](https://doi.org/10.1145/3308755.3308765)'
  id: totrans-1601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] T. Li, Q. Liu, 和 X. Zhou，“虚拟现实的超低功耗注视追踪，” *GetMobile: 移动计算与通信*，第22卷，第3期，第27–31页，2019年1月。[在线]。可用链接：[https://doi.org/10.1145/3308755.3308765](https://doi.org/10.1145/3308755.3308765)'
- en: '[200] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. Benty, D. Luebke,
    and A. Lefohn, “Towards foveated rendering for gaze-tracked virtual reality,”
    *ACM Transactions on Graphics (TOG)*, vol. 35, no. 6, pp. 1–12, 2016.'
  id: totrans-1602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] A. Patney, M. Salvi, J. Kim, A. Kaplanyan, C. Wyman, N. Benty, D. Luebke,
    和 A. Lefohn，“朝向视网膜渲染的注视跟踪虚拟现实，” *ACM图形学汇刊（TOG）*，第35卷，第6期，第1–12页，2016年。'
- en: '[201] S. Park, A. Bhattacharya, Z. Yang, S. R. Das, and D. Samaras, “Mosaic:
    Advancing user quality of experience in 360-degree video streaming with machine
    learning,” *IEEE Transactions on Network and Service Management*, vol. 18, no. 1,
    pp. 1000–1015, 2021.'
  id: totrans-1603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] S. Park, A. Bhattacharya, Z. Yang, S. R. Das, 和 D. Samaras，“Mosaic: 利用机器学习提升360度视频流的用户体验，”
    *IEEE网络与服务管理汇刊*，第18卷，第1期，第1000–1015页，2021年。'
- en: '[202] D. Alexandrovsky, S. Putze, M. Bonfert, S. Höffner, P. Michelmann, D. Wenig,
    R. Malaka, and J. D. Smeddinck, “Examining design choices of questionnaires in
    vr user studies,” ser. CHI ’20.   New York, NY, USA: Association for Computing
    Machinery, 2020, p. 1–21\. [Online]. Available: [https://doi.org/10.1145/3313831.3376260](https://doi.org/10.1145/3313831.3376260)'
  id: totrans-1604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] D. Alexandrovsky, S. Putze, M. Bonfert, S. Höffner, P. Michelmann, D.
    Wenig, R. Malaka, 和 J. D. Smeddinck，“考察虚拟现实用户研究中问卷设计的选择，” 第CHI ’20卷。纽约，NY，美国：计算机协会，2020年，第1–21页。[在线]。可用链接：[https://doi.org/10.1145/3313831.3376260](https://doi.org/10.1145/3313831.3376260)'
- en: '[203] G.-A. Koulieris, K. Akşit, C. Richardt, and R. Mantiuk, “Cutting-edge
    vr/ar display technologies (gaze-, accommodation-, motion-aware and hdr-enabled),”
    in *SIGGRAPH Asia 2018 Courses*, ser. SA ’18.   New York, NY, USA: Association
    for Computing Machinery, 2018\. [Online]. Available: [https://doi.org/10.1145/3277644.3277771](https://doi.org/10.1145/3277644.3277771)'
  id: totrans-1605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] G.-A. Koulieris, K. Akşit, C. Richardt, 和 R. Mantiuk，“前沿虚拟现实/增强现实显示技术（注视、适应、运动感知和HDR支持），”
    收录于 *SIGGRAPH Asia 2018课程*，第SA ’18卷。纽约，NY，美国：计算机协会，2018年。[在线]。可用链接：[https://doi.org/10.1145/3277644.3277771](https://doi.org/10.1145/3277644.3277771)'
- en: '[204] S. Jha, M. F. Marzban, T. Hu, M. H. Mahmoud, and N. A.-D. C. Busso, “The
    multimodal driver monitoring database: A naturalistic corpus to study driver attention,”
    *arXiv preprint arXiv:2101.04639*, 2020.'
  id: totrans-1606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] S. Jha, M. F. Marzban, T. Hu, M. H. Mahmoud, 和 N. A.-D. C. Busso，“多模态驾驶员监测数据库：一个用于研究驾驶员注意力的自然语料库，”
    *arXiv预印本 arXiv:2101.04639*，2020年。'
- en: '[205] M. W. Johns, A. Tucker, R. Chapman, K. Crowley, and N. Michael, “Monitoring
    eye and eyelid movements by infrared reflectance oculography to measure drowsiness
    in drivers,” *Somnologie-Schlafforschung und Schlafmedizin*, vol. 11, no. 4, pp.
    234–242, 2007.'
  id: totrans-1607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] M. W. Johns, A. Tucker, R. Chapman, K. Crowley, 和 N. Michael, “通过红外反射眼动仪监测眼睛和眼睑运动以测量驾驶员的瞌睡情况，”
    *Somnologie-Schlafforschung und Schlafmedizin*, vol. 11, no. 4, pp. 234–242, 2007.'
- en: '[206] S. Jha and C. Busso, “Challenges in head pose estimation of drivers in
    naturalistic recordings using existing tools,” in *IEEE International Conference
    on Intelligent Transportation Systems*, 2017, pp. 1–6.'
  id: totrans-1608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] S. Jha 和 C. Busso, “使用现有工具对自然驾驶记录中的头部姿态估计的挑战，” 在 *IEEE智能交通系统国际会议*，2017,
    pp. 1–6.'
- en: '[207] Y. Feng, G. Cheung, W.-t. Tan, P. Le Callet, and Y. Ji, “Low-cost eye
    gaze prediction system for interactive networked video streaming,” *IEEE Transactions
    on Multimedia*, vol. 15, no. 8, pp. 1865–1879, 2013.'
  id: totrans-1609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Y. Feng, G. Cheung, W.-t. Tan, P. Le Callet, 和 Y. Ji, “用于互动网络视频流的低成本眼动预测系统，”
    *IEEE Transactions on Multimedia*, vol. 15, no. 8, pp. 1865–1879, 2013.'
- en: '[208] C. Zhang, Q. He, J. Liu, and Z. Wang, “Exploring viewer gazing patterns
    for touch-based mobile gamecasting,” *IEEE Transactions on Multimedia*, vol. 19,
    no. 10, pp. 2333–2344, 2017.'
  id: totrans-1610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] C. Zhang, Q. He, J. Liu, 和 Z. Wang, “探索触摸式移动游戏广播中的观众注视模式，” *IEEE Transactions
    on Multimedia*, vol. 19, no. 10, pp. 2333–2344, 2017.'
- en: '[209] Y. Wang, G. Yuan, Z. Mi, J. Peng, X. Ding, Z. Liang, and X. Fu, “Continuous
    driver’s gaze zone estimation using rgb-d camera,” *Sensors*, p. 1287, 2019.'
  id: totrans-1611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Y. Wang, G. Yuan, Z. Mi, J. Peng, X. Ding, Z. Liang, 和 X. Fu, “使用 rgb-d
    摄像头进行连续驾驶员注视区域估计，” *Sensors*, p. 1287, 2019.'
- en: '[210] S. S. Mukherjee and N. M. Robertson, “Deep head pose: Gaze-direction
    estimation in multimodal video,” *IEEE Transactions on Multimedia*, vol. 17, no. 11,
    pp. 2094–2107, 2015.'
  id: totrans-1612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] S. S. Mukherjee 和 N. M. Robertson, “深度头部姿态：多模态视频中的注视方向估计，” *IEEE Transactions
    on Multimedia*, vol. 17, no. 11, pp. 2094–2107, 2015.'
- en: '[211] S. Alghowinem, R. Goecke, M. Wagner, G. Parker, and M. Breakspear, “Eye
    movement analysis for depression detection,” in *2013 IEEE International Conference
    on Image Processing*.   IEEE, 2013, pp. 4220–4224.'
  id: totrans-1613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] S. Alghowinem, R. Goecke, M. Wagner, G. Parker, 和 M. Breakspear, “用于抑郁症检测的眼动分析，”
    在 *2013年IEEE国际图像处理会议*。IEEE, 2013, pp. 4220–4224.'
- en: '[212] M. E. Milanak, M. R. Judah, H. Berenbaum, A. F. Kramer, and M. Neider,
    “Ptsd symptoms and overt attention to contextualized emotional faces: Evidence
    from eye tracking,” *Psychiatry research*, vol. 269, pp. 408–413, 2018.'
  id: totrans-1614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] M. E. Milanak, M. R. Judah, H. Berenbaum, A. F. Kramer, 和 M. Neider,
    “创伤后应激障碍症状与对情境化情感面孔的显性注意：来自眼动追踪的证据，” *Psychiatry research*, vol. 269, pp. 408–413,
    2018.'
- en: '[213] O. Avital, “Method and system of using eye tracking to evaluate subjects,”
    Oct. 8 2015, uS Patent App. 14/681,083.'
  id: totrans-1615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] O. Avital, “使用眼动追踪评估受试者的方法和系统，” 2015年10月8日, 美国专利申请 14/681,083.'
- en: '[214] K. Sakurai, M. Yan, H. Tamura, and K. Tanno, “A study on gaze estimation
    system using the direction of eyes and face,” in *2016 World Automation Congress
    (WAC)*.   IEEE, 2016, pp. 1–6.'
  id: totrans-1616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] K. Sakurai, M. Yan, H. Tamura, 和 K. Tanno, “基于眼睛和面部方向的注视估计系统研究，” 在 *2016年世界自动化大会
    (WAC)*. IEEE, 2016, pp. 1–6.'
- en: '[215] K. Sakurai, M. Yan, K. Tanno, and H. Tamura, “Gaze estimation method
    using analysis of electrooculogram signals and kinect sensor,” *Computational
    intelligence and neuroscience*, vol. 2017, 2017.'
  id: totrans-1617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] K. Sakurai, M. Yan, K. Tanno, 和 H. Tamura, “基于眼动电图信号和kinect传感器的注视估计方法，”
    *Computational intelligence and neuroscience*, vol. 2017, 2017.'
- en: '[216] J. L. Kröger, O. H.-M. Lutz, and F. Müller, “What does your gaze reveal
    about you? on the privacy implications of eye tracking,” in *IFIP International
    Summer School on Privacy and Identity Management*.   Springer, 2019, pp. 226–241.'
  id: totrans-1618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] J. L. Kröger, O. H.-M. Lutz, 和 F. Müller, “你的注视透露了什么？关于眼动追踪的隐私影响，” 在
    *IFIP国际隐私与身份管理暑期学校*。Springer, 2019, pp. 226–241.'
- en: '[217] B. John, S. Koppal, and E. Jain, “Eyeveil: degrading iris authentication
    in eye tracking headsets,” in *Proceedings of the 11th ACM Symposium on Eye Tracking
    Research & Applications*, 2019, pp. 1–5.'
  id: totrans-1619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] B. John, S. Koppal, 和 E. Jain, “Eyeveil：在眼动追踪头戴设备中降低虹膜认证的可靠性，” 在 *第11届ACM眼动研究与应用研讨会论文集*，2019,
    pp. 1–5.'
- en: '[218] M. Erbilek, M. Fairhurst, and M. C. D. C. Abreu, “Age prediction from
    iris biometrics,” in *5th International Conference on Imaging for Crime Detection
    and Prevention (ICDP 2013)*.   IET, 2013, pp. 1–5.'
  id: totrans-1620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] M. Erbilek, M. Fairhurst, 和 M. C. D. C. Abreu, “基于虹膜生物特征的年龄预测，” 在 *第五届国际犯罪检测与预防成像会议
    (ICDP 2013)*. IET, 2013, pp. 1–5.'
- en: '[219] F. J. M. Moss, R. Baddeley, and N. Canagarajah, “Eye movements to natural
    images as a function of sex and personality,” *PloS one*, vol. 7, no. 11, p. e47870,
    2012.'
  id: totrans-1621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] F. J. M. Moss, R. Baddeley, 和 N. Canagarajah, “性别和个性对自然图像的眼动影响，” *PloS
    one*, vol. 7, no. 11, p. e47870, 2012.'
- en: '[220] S. Hoppe, T. Loetscher, S. A. Morey, and A. Bulling, “Eye movements during
    everyday behavior predict personality traits,” *Frontiers in human neuroscience*,
    vol. 12, p. 105, 2018.'
  id: totrans-1622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] S. Hoppe, T. Loetscher, S. A. Morey, 和 A. Bulling，“日常行为中的眼动预测人格特质，” *人类神经科学前沿*，第12卷，第105页，2018。'
- en: '[221] A. Ben Youssef, H. Shimodaira, and D. A. Braude, “Articulatory features
    for speech-driven head motion synthesis,” *Proceedings of Interspeech, Lyon, France*,
    vol. 3, 2013.'
  id: totrans-1623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] A. Ben Youssef, H. Shimodaira, 和 D. A. Braude，“用于语音驱动的头部运动合成的发音特征，” *国际语音会议论文集，法国里昂*，第3卷，2013。'
- en: '[222] C. Ding, L. Xie, and P. Zhu, “Head motion synthesis from speech using
    deep neural networks,” *Multimedia Tools and Applications*, vol. 74, no. 22, pp.
    9871–9888, 2015.'
  id: totrans-1624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] C. Ding, L. Xie, 和 P. Zhu，“使用深度神经网络从语音合成头部运动，” *多媒体工具与应用*，第74卷，第22期，第9871–9888页，2015。'
- en: '[223] C. Ding, P. Zhu, and L. Xie, “Blstm neural networks for speech driven
    head motion synthesis,” in *Sixteenth Annual Conference of the International Speech
    Communication Association*, 2015.'
  id: totrans-1625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] C. Ding, P. Zhu, 和 L. Xie，“用于语音驱动的头部运动合成的Blstm神经网络，” 见于 *第十六届国际语音通信协会年会*，2015。'
- en: '[224] D. Greenwood, S. Laycock, and I. Matthews, “Predicting head pose from
    speech with a conditional variational autoencoder.”   ISCA, 2017.'
  id: totrans-1626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] D. Greenwood, S. Laycock, 和 I. Matthews，“使用条件变分自编码器预测头部姿态。” ISCA，2017。'
- en: '[225] S. Ghosh, A. Dhall, M. Hayat, and J. Knibbe, “Av-gaze: A study on the
    effectiveness of audio guided visual attention estimation for non-profilic faces,”
    *arXiv preprint arXiv:2207.03048*, 2022.'
  id: totrans-1627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] S. Ghosh, A. Dhall, M. Hayat, 和 J. Knibbe，“Av-gaze：一种关于音频引导的视觉注意力估计在非特征面部中的有效性的研究，”
    *arXiv预印本 arXiv:2207.03048*，2022。'
- en: '| ![[Uncaptioned image]](img/ea0c78a05b66f2bbd6975147856c2158.png) | Shreya
    Ghosh is currently pursuing her PhD at Monash University, Australia. She received
    MS(R) degree in Computer Science and Engineering from the Indian Institute of
    Technology Ropar, India. She received the B.Tech. in CSE from the Govt. College
    of Engineering and Textile Technology (Serampore, India). Her research interests
    include affective computing, computer vision, Deep Learning. She is a student
    member of the IEEE. |'
  id: totrans-1628
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/ea0c78a05b66f2bbd6975147856c2158.png) | 舒雷亚·戈什目前在澳大利亚蒙纳士大学攻读博士学位。她在印度罗帕尔印度理工学院获得计算机科学与工程硕士（研究型）学位。在印度政府工程与纺织技术学院（塞兰布尔）获得计算机科学与工程学士学位。她的研究兴趣包括情感计算、计算机视觉和深度学习。她是IEEE的学生会员。'
- en: '| ![[Uncaptioned image]](img/5a06e0077b3a72e7e211785a411b49ec.png) | Abhinav
    Dhall is an Assistant Professor at Indian Institute of Technology Ropar and Adjunct
    Senior Lecturer at Monash University . He received PhD from the Australian National
    University in 2014\. Followed by postdocs at the University of Waterloo and the
    University of Canberra. He was awarded the Best Doctoral Paper Award at ACM ICMR
    2013, Best Student Paper Honourable mention at IEEE AFGR 2013 and Best Paper Nomination
    at IEEE ICME 2012\. His research interests are in computer vision for Affective
    computing and Assistive Technology. He is a member of the IEEE and Associate Editor
    of IEEE Transactions on Affective Computing. |'
  id: totrans-1629
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/5a06e0077b3a72e7e211785a411b49ec.png) | 阿比纳夫·达哈尔是印度罗帕尔印度理工学院的助理教授，同时也是蒙纳士大学的兼职高级讲师。他于2014年在澳大利亚国立大学获得博士学位，随后在滑铁卢大学和堪培拉大学进行了博士后研究。他在ACM
    ICMR 2013获得了最佳博士论文奖，在IEEE AFGR 2013获得了最佳学生论文荣誉提名，并在IEEE ICME 2012获得了最佳论文提名。他的研究兴趣包括用于情感计算和辅助技术的计算机视觉。他是IEEE会员，同时是IEEE《情感计算汇刊》的副编辑。'
- en: '| ![[Uncaptioned image]](img/a62a13f26859d4780b4d83fd177939f2.png) | Munawar
    Hayat is currently a Senior Research Fellow with Monash University, Australia.
    He received his PhD from The University of Western Australia (UWA). His PhD thesis
    received multiple awards, including the Deans List Honorable Mention Award and
    the Robert Street Prize. After his PhD, he joined IBM Research as a postdoc and
    then moved to the University of Canberra as an Assistant Professor. He was a Senior
    Scientist at the Inception Institute of Artificial Intelligence, UAE. He has been
    awarded the ARC DECRA fellowship. His research interests are in computer vision,
    machine learning, deep learning, and affective computing. |'
  id: totrans-1630
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注图片]](img/a62a13f26859d4780b4d83fd177939f2.png) | 穆纳瓦尔·哈亚特目前是澳大利亚蒙纳士大学的高级研究员。他在西澳大学（UWA）获得了博士学位。他的博士论文获得了多个奖项，包括院长名单荣誉奖和罗伯特·街奖。在获得博士学位后，他加入了IBM研究所担任博士后，随后转到堪培拉大学担任助理教授。他曾是阿联酋启迪人工智能研究院的高级科学家。他获得了ARC
    DECRA奖学金。他的研究兴趣包括计算机视觉、机器学习、深度学习和情感计算。'
- en: '| ![[Uncaptioned image]](img/d395d166c18346e7c2cdfb3fb15a888b.png) | Jarrod
    Knibbe is currently with the University of Melbourne, Australia. He received his
    PhD from The University of Bristol in 2016\. He completed a post-doc in human-centred
    computing at the University of Copenhagen. His research interests include interaction
    design and user experience, with a focus on body based user interfaces, electric
    muscle stimulation, and virtual reality. He has published over 25 papers at top
    venues in Human-Computer Interaction, including CHI, UIST, CSCW, and Ubicomp.
    |'
  id: totrans-1631
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/d395d166c18346e7c2cdfb3fb15a888b.png) | 贾罗德·克尼布目前在澳大利亚墨尔本大学工作。他于2016年获得布里斯托大学的博士学位。之后，他在哥本哈根大学完成了以人为本计算的博士后研究。他的研究兴趣包括交互设计和用户体验，重点关注基于身体的用户界面、电肌肉刺激和虚拟现实。他在《人机交互》领域的顶级会议上发表了超过25篇论文，包括CHI、UIST、CSCW和Ubicomp。
    |'
- en: '| ![[Uncaptioned image]](img/d5057ee31cdcdacb016803d3aa365eaa.png) | Qiang
    Ji is a Professor with the Department of Electrical, Computer, and Systems Engineering
    at Rensselaer Polytechnic Institute (RPI). He received his Ph.D degree in Electrical
    Engineering from the University of Washington. He was a program director at the
    National Science Foundation, where he managed NSF’s computer vision and machine
    learning programs. He also held teaching and research positions at University
    of Illinois at Urbana-Champaign, Carnegie Mellon University, and University of
    Nevada at Reno. His research interests are in computer vision, probabilistic machine
    learning, and their applications. He has published over 300 papers, received multiple
    awards for his work, serve as an editor for multiple international journals, and
    organize numerous international conferences/workshops. He is a fellow of the IEEE
    and the IAPR. |'
  id: totrans-1632
  prefs: []
  type: TYPE_TB
  zh: '| ![[无标题图片]](img/d5057ee31cdcdacb016803d3aa365eaa.png) | 姜强是伦斯勒理工学院（RPI）电气、计算机与系统工程系的教授。他在华盛顿大学获得电气工程博士学位。他曾担任国家科学基金会的项目主任，负责管理NSF的计算机视觉和机器学习项目。他还曾在伊利诺伊大学香槟分校、卡内基梅隆大学和内华达大学里诺分校担任教学和研究职位。他的研究兴趣包括计算机视觉、概率机器学习及其应用。他已发表超过300篇论文，因其工作获得多项奖项，担任多个国际期刊的编辑，并组织了众多国际会议/研讨会。他是IEEE和IAPR的会士。'
