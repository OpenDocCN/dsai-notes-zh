- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:50:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:50:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.10436] A Survey on Deep-Learning Approaches for Vehicle Trajectory Prediction
    in Autonomous Driving'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.10436] 关于自动驾驶中车辆轨迹预测的深度学习方法综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.10436](https://ar5iv.labs.arxiv.org/html/2110.10436)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.10436](https://ar5iv.labs.arxiv.org/html/2110.10436)
- en: A Survey on Deep-Learning Approaches for Vehicle Trajectory Prediction in Autonomous
    Driving
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于自动驾驶中车辆轨迹预测的深度学习方法综述
- en: Jianbang Liu^(∗1), Xinyu Mao^(∗1), Yuqi Fang², Delong Zhu¹, and Max Q.-H. Meng^(†3),
    Fellow, IEEE $*$Equal contribution. $\dagger$The corresponding author.¹Jianbang
    Liu, Xinyu Mao and Delong Zhu are with the Department of Electronic Engineering,
    The Chinese University of Hong Kong, Hong Kong. ({henryliu, maoxinyu, zhudelong}@link.cuhk.edu.hk)²Yuqi
    Fang is with the Department of Biomedical Engineering, The Chinese University
    of Hong Kong, Hong Kong. (fangyuqi@link.cuhk.edu.hk) ³Max Q.-H. Meng is with the
    Department of Electronic and Electrical Engineering of the Southern University
    of Science and Technology in Shenzhen, China, on leave from the Department of
    Electronic Engineering, The Chinese University of Hong Kong, Hong Kong, and also
    with the Shenzhen Research Institute of the Chinese University of Hong Kong in
    Shenzhen, China. (max.meng@ieee.org.)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 刘建邦^(∗1)、毛鑫宇^(∗1)、方宇琦²、朱德龙¹，以及**IEEE会士**孟庆华^(†3)，$*$等贡献。$\dagger$通讯作者。¹刘建邦、毛鑫宇和朱德龙均在香港中文大学电子工程系工作，香港。({henryliu,
    maoxinyu, zhudelong}@link.cuhk.edu.hk)²方宇琦在香港中文大学生物医学工程系工作，香港。(fangyuqi@link.cuhk.edu.hk)³孟庆华在中国深圳南方科技大学电子与电气工程系工作，同时从香港中文大学电子工程系休假，并且还在中国深圳香港中文大学深圳研究院工作。(max.meng@ieee.org.)
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the rapid development of machine learning, autonomous driving has become
    a hot issue, making urgent demands for more intelligent perception and planning
    systems. Self-driving cars can avoid traffic crashes with precisely predicted
    future trajectories of surrounding vehicles. In this work, we review and categorize
    existing learning-based trajectory forecasting methods from perspectives of representation,
    modeling, and learning. Moreover, we make our implementation of Target-driveN
    Trajectory Prediction publicly available at [https://github.com/Henry1iu/TNT-Trajectory-Predition](https://github.com/Henry1iu/TNT-Trajectory-Predition),
    demonstrating its outstanding performance whereas its original codes are withheld.
    Enlightenment is expected for researchers seeking to improve trajectory prediction
    performance based on the achievement we have made.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习的快速发展，自动驾驶已成为一个热点问题，对更智能的感知和规划系统提出了迫切需求。自动驾驶汽车可以通过精确预测周围车辆的未来轨迹来避免交通事故。在这项工作中，我们从表示、建模和学习的角度回顾和分类了现有的基于学习的轨迹预测方法。此外，我们将Target-driveN轨迹预测的实现公开于[https://github.com/Henry1iu/TNT-Trajectory-Predition](https://github.com/Henry1iu/TNT-Trajectory-Predition)，展示了其出色的性能，而其原始代码被保留。希望这些成果能为寻求提高轨迹预测性能的研究者提供启示。
- en: I Introduction
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: Human depends increasingly on the autonomous system to be freed from tedious
    tasks, a typical instance of which is autonomous driving. In the self-driving
    scenario, safety is the first concern. Predicting the future trajectories of participants
    helps the autonomous system find the most promising local path planning solution
    and prevent possible collision.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 人们越来越依赖自动系统来摆脱繁琐的任务，其中一个典型实例就是自动驾驶。在自动驾驶场景中，安全是首要关注点。预测参与者的未来轨迹有助于自动系统找到最有前景的局部路径规划解决方案，防止可能的碰撞。
- en: Pioneers have predicted the motion of dynamic objects with Kalman filter[[1](#bib.bib1)],
    linear trajectory avoidance model[[2](#bib.bib2)], and social force model[[3](#bib.bib3)].
    Compared to these traditional modeling techniques based on hand-crafted features,
    deep learning algorithms that learn features automatically via optimizing loss
    functions have recently attracted researchers’ attention. In this work, we survey
    some recent approaches for vehicle trajectory prediction and present some innovative
    ideas. We implement the approach presented by Zhao et al. in [[4](#bib.bib4)]
    since its vectorized representation is innovative and efficient. By publishing
    this survey along with our code, we hope that new breakthroughs can be made in
    this rapidly expanding research field.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 先驱者们利用卡尔曼滤波器[[1](#bib.bib1)]、线性轨迹规避模型[[2](#bib.bib2)]和社会力模型[[3](#bib.bib3)]预测动态物体的运动。与这些基于手工特征的传统建模技术相比，近年来通过优化损失函数自动学习特征的深度学习算法吸引了研究人员的注意。在这项工作中，我们综述了一些最近的车辆轨迹预测方法，并提出了一些创新的想法。我们实现了赵等人提出的在[[4](#bib.bib4)]中的方法，因为它的向量化表示是创新且高效的。通过发布这项调查和我们的代码，我们希望在这个快速扩展的研究领域取得新的突破。
- en: 'The two main contributions are as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 主要有以下两个贡献：
- en: '1.'
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Recent deep-learning approaches tackling trajectory prediction problems in driving
    scenarios are reviewed and discussed.
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最近针对驾驶场景中的轨迹预测问题的深度学习方法进行了回顾和讨论。
- en: '2.'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: We implement the prediction model introduced by Zhao et al. [[4](#bib.bib4)]
    and release our code to the research community.
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们实现了赵等人介绍的预测模型[[4](#bib.bib4)]并向研究社区发布了我们的代码。
- en: II Problem Formulation
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 问题表述
- en: A self-driving system is assumed to be equipped with detection and tracking
    modules that observe state $\displaystyle{\mathbb{S}}$ accurately for all the
    involved agents $\displaystyle{\mathbb{A}}$. Given a scene, the prediction target
    is denoted as $\displaystyle{\bm{a}}_{tar}$ and the surrounding agents are denoted
    as $\displaystyle{\mathbb{A}}_{nbrs}=\{{\bm{a}}_{1},{\bm{a}}_{2},...,{\bm{a}}_{m}\}$.
    The state of agent $\displaystyle{\bm{a}}_{i}\in{\mathbb{A}}$ at frame $t$ is
    denoted as $\displaystyle s^{t}_{i}$, including features such as position, velocity,
    heading angle, actor type, and $\displaystyle{\bm{s}}_{i}=\{s^{-T_{obs}+1}_{i},s^{-T_{obs}+2}_{i},...,s^{0}_{i}\}$
    denote the sequence of states sampled at different timestamps throughout the observation
    period $\displaystyle T_{obs}$. The objective of a predictive framework is to
    predict future trajectories $\uptau_{tar}=\{\tau_{i}|i=1,2,...,K\}$ of the target
    agent $\displaystyle{\bm{a}}_{tar}$, where $\displaystyle\tau_{i}=\{(x^{1}_{i},y^{1}_{i}),(x^{2}_{i},y^{2}_{i}),...,(x^{T_{pred}}_{i},y^{T_{pred}}_{i})\}$
    denotes the predicted trajectories for the target agent up to the prediction horizon
    $\displaystyle T_{pred}$. Besides, the predicted trajectories $\displaystyle\tau_{i}\in\uptau_{tar}$
    need to satisfy the no-collision constraint and target agent’s kinematic constraints.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个自动驾驶系统配备了检测和跟踪模块，能够准确观察所有涉及的代理的状态$\displaystyle{\mathbb{S}}$。给定一个场景，预测目标记作$\displaystyle{\bm{a}}_{tar}$，周围代理记作$\displaystyle{\mathbb{A}}_{nbrs}=\{{\bm{a}}_{1},{\bm{a}}_{2},...,{\bm{a}}_{m}\}$。代理$\displaystyle{\bm{a}}_{i}\in{\mathbb{A}}$在帧$t$的状态记作$\displaystyle
    s^{t}_{i}$，包括位置、速度、航向角、演员类型等特征，$\displaystyle{\bm{s}}_{i}=\{s^{-T_{obs}+1}_{i},s^{-T_{obs}+2}_{i},...,s^{0}_{i}\}$表示在观察期$\displaystyle
    T_{obs}$内不同时间戳采样的状态序列。预测框架的目标是预测目标代理$\displaystyle{\bm{a}}_{tar}$的未来轨迹$\uptau_{tar}=\{\tau_{i}|i=1,2,...,K\}$，其中$\displaystyle\tau_{i}=\{(x^{1}_{i},y^{1}_{i}),(x^{2}_{i},y^{2}_{i}),...,(x^{T_{pred}}_{i},y^{T_{pred}}_{i})\}$表示预测的目标代理在预测视野$\displaystyle
    T_{pred}$内的轨迹。此外，预测的轨迹$\displaystyle\tau_{i}\in\uptau_{tar}$需要满足无碰撞约束和目标代理的运动学约束。
- en: III Methods
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 方法
- en: In this section, we review the data representation, model structure, learning
    techniques, and objective functions of some representative approaches.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们回顾了某些代表性方法的数据表示、模型结构、学习技术和目标函数。
- en: 'Table I: Comparison of Modality and Modeling: $\displaystyle CSDTS_{multi}$,
    $\vec{Polyline}$, $\Delta_{dist}$, $\mathcal{G}_{Lane}$, $\uptau_{cond}$, $Anchor_{T}$,
    $\Delta$, $State\_Map^{F}$, and $\mathcal{L}$ denote CSDTS containing position
    under multiple coordinate system, the vectorized polyline, the relative distance,
    the lane connectivity graph, the conditioned future trajectory, the trajectory
    anchors, the offset, the future state map, and the Laplacian distribution, respectively.
    S-Pool indicates the social pooling[[5](#bib.bib5), [6](#bib.bib6)], Spatial&Temporal
    indicates the spatial and temporal learning[[7](#bib.bib7)], SA and DPA indicate
    the self-attention and the dot-product attention. XFMR is the abbreviation of
    transformer.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Input Modality | Output Modality | Modeling |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '| Model | $\displaystyle{\mathbb{S}}$ Repre. | Scene Repre. | Extra | Output
    | $\displaystyle{\mathbb{S}}$ Enc. | Scene Enc. | Interaction |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| DESIRE[[8](#bib.bib8)] | CSDTS | BEV | - | $\uptau_{tar}$, $\Delta$, Score
    | GRU, CVAE | ConvNet | S-Pool |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| MTP[[9](#bib.bib9)] | BEV | State | $\uptau_{tar}$, $\Pr(\cdot)$ | MobileNetV2[[10](#bib.bib10)]
    | - |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: '| MultiPath[[11](#bib.bib11)] | BEV | - | $\uptau_{tar}$, $\Pr(\cdot)$, $\mu$,
    $\sigma$ | ResNet[[12](#bib.bib12)] | CNN |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
- en: '| CoverNet[[13](#bib.bib13)] | BEV | State | $\uptau_{tar}$, $\Pr(\cdot)$ |
    ResNet[[12](#bib.bib12)] | - |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
- en: '| VectorNet[[14](#bib.bib14)] | $\vec{Polyline}$ | - | $\uptau_{tar}$ | PointNet[[15](#bib.bib15)]
    | SA-GNN |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
- en: '| TPNet[[16](#bib.bib16)] | BEV | - | $P(t)$, $\Delta$, $\Pr(\cdot)$ | ResNet[[12](#bib.bib12)]
    | - |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
- en: '| SAMMP[[17](#bib.bib17)] | CSDTS | - | - | $\uptau_{tar}$ | LSTM | - | SA,
    RNN |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
- en: '| Luo[[18](#bib.bib18)] | CSDTS | Lanes, Points | - | Lanes, $\mu$, $\sigma$
    | LSTM | 1D-Conv, MLP | DPA |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
- en: '| BaiduUS[[19](#bib.bib19)] | CSDTS | Lanes, Points | $\Delta_{dist}$ | Lanes,
    $\mu$, $\sigma$ | LSTM | LSTM | ST-G |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
- en: '| LaneGCN[[20](#bib.bib20)] | CSDTS | $\vec{Polyline}$ | $\mathcal{G}_{Lane}$
    | $\uptau_{tar}$, Score | 1D-Conv | LaneGCN | SA |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
- en: '| DATF[[21](#bib.bib21)] | CSDTS | BEV | - | $\uptau_{tar}$ | LSTM | ConvNet
    | SA |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
- en: '| SMART[[22](#bib.bib22)] | BEV | - | $State\_Map^{F}$ | U-Net, CVAE | S-Pool
    |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
- en: '| PiP[[23](#bib.bib23)] | CSDTS | - | $\uptau_{cond}$ | $\uptau_{tar}$ | LSTM
    | - | S-Pool |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
- en: '| TNT[[4](#bib.bib4)] | $\vec{Polyline}$ | - | $\uptau_{tar}$, Score | PointNet[[15](#bib.bib15)]
    | SA-GNN |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
- en: '| WIMP[[24](#bib.bib24)] | CSDTS | $\vec{Polyline}$ | - | $\uptau_{tar}$ |
    RNN | SA | SA |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
- en: '| HOME[[25](#bib.bib25)] | CSDTS | BEV | - | Heatmap, $\uptau_{tar}$ | 1D-Conv,
    RNN | CNN | SA |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
- en: '| TPCN[[7](#bib.bib7)] | CSDTS | Points | Tables | $\uptau_{tar}$, $\Delta$
    | PointNet++, Spatial&Temporal | - |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
- en: '| LaPred[[26](#bib.bib26)] | CSDTS | Points | - | $\uptau_{tar}$ | 1D-Conv,
    LSTM, MLP | SA |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| MMTrans[[27](#bib.bib27)] | CSDTS | $\vec{Polyline}$ | - | $\uptau_{tar}$
    | XFMR | VectorNet | XFMR |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: '| ALAN[[28](#bib.bib28)] | $\displaystyle CSDTS_{multi}$ | Points, BEV | -
    | $\uptau_{tar}$ | MLP, LSTM | 1D-Conv | S-Pool |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ALAN[[28](#bib.bib28)] | $\displaystyle CSDTS_{multi}$ | 点, BEV | - | $\uptau_{tar}$
    | MLP, LSTM | 1D-Conv | S-Pool |'
- en: '| LaneRCNN[[29](#bib.bib29)] | CSDTS | Points, $\mathcal{G}_{Lane}$ | LaneRoI
    | $\Pr(\cdot)$, $\Delta$, $\Delta\theta$ | LaneConv, LanePool | LaneRoI |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| LaneRCNN[[29](#bib.bib29)] | CSDTS | 点, $\mathcal{G}_{Lane}$ | LaneRoI |
    $\Pr(\cdot)$, $\Delta$, $\Delta\theta$ | LaneConv, LanePool | LaneRoI |'
- en: '| PRIME[[30](#bib.bib30)] | $\displaystyle CSDTS_{multi}$ | $\mathcal{G}_{Lane}$
    | $Anchor_{T}$ | Score | 1D-Conv, RNN | bi-LSTM | SA |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| PRIME[[30](#bib.bib30)] | $\displaystyle CSDTS_{multi}$ | $\mathcal{G}_{Lane}$
    | $Anchor_{T}$ | Score | 1D-Conv, RNN | bi-LSTM | SA |'
- en: '| SceneTrans[[31](#bib.bib31)] | CSDTS | $\vec{Polyline}$ | - | $\uptau_{tar}$,
    $\mathcal{L}$, $\theta$ | XFMR, PointNet[[15](#bib.bib15)] | SA |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| SceneTrans[[31](#bib.bib31)] | CSDTS | $\vec{Polyline}$ | - | $\uptau_{tar}$,
    $\mathcal{L}$, $\theta$ | XFMR, PointNet[[15](#bib.bib15)] | SA |'
- en: III-A Representations
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 表示方法
- en: There are mainly two types of representation, i.e., image and continuous-space
    samples, to describe the historical observation and future prediction in each
    vehicle trajectory prediction case. Images are commonly used to carry the agents
    and road observations [[8](#bib.bib8), [9](#bib.bib9), [11](#bib.bib11), [13](#bib.bib13),
    [16](#bib.bib16), [21](#bib.bib21), [22](#bib.bib22), [25](#bib.bib25), [28](#bib.bib28)]
    due to its dense characteristic. Some researchers [[14](#bib.bib14), [4](#bib.bib4),
    [7](#bib.bib7), [26](#bib.bib26)] prefer utilizing sparse points or polylines
    when describing the historical trajectories or the scene context.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 主要有两种表示类型，即图像和连续空间样本，用于描述每个车辆轨迹预测案例中的历史观察和未来预测。由于其密集特性，图像通常用于承载代理和道路观察 [[8](#bib.bib8),
    [9](#bib.bib9), [11](#bib.bib11), [13](#bib.bib13), [16](#bib.bib16), [21](#bib.bib21),
    [22](#bib.bib22), [25](#bib.bib25), [28](#bib.bib28)]。一些研究人员 [[14](#bib.bib14),
    [4](#bib.bib4), [7](#bib.bib7), [26](#bib.bib26)] 更喜欢在描述历史轨迹或场景上下文时使用稀疏点或多边形线。
- en: III-A1 Agent State Representations
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 代理状态表示
- en: Due to the limitation of practical detection and tracking system, the agent
    state can only be recorded periodically. It’s natural to represent the agent state
    with continuous-space discrete-time samples (CSDTS), which are vectors (or arrays)
    containing the agent state features. In [[8](#bib.bib8), [14](#bib.bib14), [17](#bib.bib17),
    [18](#bib.bib18), [19](#bib.bib19), [21](#bib.bib21), [23](#bib.bib23), [4](#bib.bib4),
    [24](#bib.bib24), [25](#bib.bib25), [7](#bib.bib7), [26](#bib.bib26), [27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)], features
    mainly include the timestamp, the position of the agent under a bird-eye-view
    (BEV) coordinate system, the displacement to the last timestamp, the velocity
    of the agent, and the relative heading angle. Nevertheless, researchers also explore
    sketching the trajectories on a rasterized BEV image in [[9](#bib.bib9), [11](#bib.bib11),
    [13](#bib.bib13), [16](#bib.bib16), [22](#bib.bib22)]. The image representation
    is not widely adopted to describe agent states in recent papers.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于实际检测和跟踪系统的限制，代理状态只能周期性地记录。用连续空间离散时间样本（CSDTS）表示代理状态是自然的，这些样本是包含代理状态特征的向量（或数组）。在
    [[8](#bib.bib8), [14](#bib.bib14), [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [21](#bib.bib21), [23](#bib.bib23), [4](#bib.bib4), [24](#bib.bib24), [25](#bib.bib25),
    [7](#bib.bib7), [26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29),
    [30](#bib.bib30), [31](#bib.bib31)] 中，特征主要包括时间戳、在鸟瞰视图（BEV）坐标系统下的代理位置、到上一个时间戳的位移、代理的速度以及相对航向角。然而，研究人员也探讨了在
    [[9](#bib.bib9), [11](#bib.bib11), [13](#bib.bib13), [16](#bib.bib16), [22](#bib.bib22)]
    中在光栅化的BEV图像上勾画轨迹。近年来，图像表示在描述代理状态时并不广泛采用。
- en: III-A2 Scene Context Representations
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 场景上下文表示
- en: In contrast with the continuous-space sample, image is the most straightforward
    representation to carry the scene context and has been studied in many works [[8](#bib.bib8),
    [9](#bib.bib9), [11](#bib.bib11), [13](#bib.bib13), [16](#bib.bib16), [21](#bib.bib21),
    [22](#bib.bib22), [25](#bib.bib25), [28](#bib.bib28)]. The shape and status of
    the roads are visualized on the BEV image as detailedly as possible. Sometimes,
    this kind of rasterized image is also called the high definition (HD) map in the
    paper [[14](#bib.bib14), [4](#bib.bib4)]. Gao et al. [[14](#bib.bib14)] propose
    to discretize the scene context and represent it by the vectors. This novel idea
    is quickly appreciated and adopted by other researchers [[4](#bib.bib4), [24](#bib.bib24),
    [27](#bib.bib27), [31](#bib.bib31)] because it provides a unified representation
    and the computation cost of vectorized representation is less expensive. In advance,
    some works[[20](#bib.bib20), [7](#bib.bib7), [29](#bib.bib29)] construct extra
    tables or graphs to indicate the temporal and spatial correspondence of the vectorized
    scene context, which assist the feature extraction and interaction modeling in
    their prediction model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 与连续空间样本相比，图像是传达场景上下文的最直接表示方式，并且在许多研究中得到了探讨 [[8](#bib.bib8), [9](#bib.bib9),
    [11](#bib.bib11), [13](#bib.bib13), [16](#bib.bib16), [21](#bib.bib21), [22](#bib.bib22),
    [25](#bib.bib25), [28](#bib.bib28)]。道路的形状和状态在BEV图像中尽可能详细地可视化。有时，这种栅格化图像也被称为高分辨率（HD）地图
    [[14](#bib.bib14), [4](#bib.bib4)]。高等人 [[14](#bib.bib14)] 提出了对场景上下文进行离散化并用向量表示的方案。这个新颖的想法得到了其他研究人员的迅速认可和采纳
    [[4](#bib.bib4), [24](#bib.bib24), [27](#bib.bib27), [31](#bib.bib31)]，因为它提供了一种统一的表示方式，向量化表示的计算成本也较低。此外，一些研究
    [[20](#bib.bib20), [7](#bib.bib7), [29](#bib.bib29)] 构建了额外的表格或图表来指示向量化场景上下文的时间和空间对应关系，这有助于在其预测模型中进行特征提取和交互建模。
- en: III-A3 Output Representations
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 输出表示
- en: Some approaches focus on improving the input data representation[[14](#bib.bib14)],
    the modeling [[18](#bib.bib18), [23](#bib.bib23), [26](#bib.bib26), [27](#bib.bib27)]
    as well as the objective function [[28](#bib.bib28)], and merely output future
    trajectory represented by single modality, which denotes sets of future position
    $\uptau_{tar}$ . In order to produce a promising result, approaches estimate additional
    modalities, such as the offset of the predicted trajectories from the ground truth
    [[8](#bib.bib8), [16](#bib.bib16), [29](#bib.bib29)], the score of the prediction
    according to a pre-defined scoring metric [[8](#bib.bib8), [20](#bib.bib20), [4](#bib.bib4),
    [30](#bib.bib30)], the probability $\Pr(\cdot)$ of each trajectory prediction
    or anchor [[9](#bib.bib9), [11](#bib.bib11), [13](#bib.bib13), [16](#bib.bib16),
    [29](#bib.bib29)], the probability heat-map of final position[[25](#bib.bib25)],
    and the heading $\theta$ or the angular offset $\Delta\theta$ of the target agent
    at the destination[[29](#bib.bib29), [31](#bib.bib31)]. MultiPath[[11](#bib.bib11)]
    models the control uncertainty with the Gaussian mixture model (GMM) and predicts
    the mean $\mu$ and variance $\sigma$ of the GMM as the model output. TPNet[[16](#bib.bib16)]
    and PRIME[[30](#bib.bib30)] decide to express the trajectory anchors or the output
    trajectories in the form of polynomials $P(t)$ other than the usual point sets
    $\uptau_{tar}$.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法专注于改善输入数据表示 [[14](#bib.bib14)]、建模 [[18](#bib.bib18), [23](#bib.bib23), [26](#bib.bib26),
    [27](#bib.bib27)] 以及目标函数 [[28](#bib.bib28)]，仅仅输出由单一模态表示的未来轨迹，表示未来位置集合 $\uptau_{tar}$。为了生成有前景的结果，方法估计额外的模态，如预测轨迹与真实值之间的偏差
    [[8](#bib.bib8), [16](#bib.bib16), [29](#bib.bib29)]、根据预定义评分指标的预测得分 [[8](#bib.bib8),
    [20](#bib.bib20), [4](#bib.bib4), [30](#bib.bib30)]、每个轨迹预测或锚点的概率 $\Pr(\cdot)$
    [[9](#bib.bib9), [11](#bib.bib11), [13](#bib.bib13), [16](#bib.bib16), [29](#bib.bib29)]、最终位置的概率热图
    [[25](#bib.bib25)]，以及目标代理在目的地的航向 $\theta$ 或角度偏差 $\Delta\theta$ [[29](#bib.bib29),
    [31](#bib.bib31)]。MultiPath [[11](#bib.bib11)] 使用高斯混合模型（GMM）建模控制不确定性，并将GMM的均值
    $\mu$ 和方差 $\sigma$ 作为模型输出。TPNet [[16](#bib.bib16)] 和 PRIME [[30](#bib.bib30)]
    决定以多项式 $P(t)$ 的形式表达轨迹锚点或输出轨迹，而非通常的点集 $\uptau_{tar}$。
- en: III-B Modeling
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 建模
- en: To achieve better prediction performance, researchers propose novel models with
    various architectures, such as Multi-Layer Perception (MLP), Convolution Neural
    Network (CNN), Recurrent Neural Network (RNN), Graph Neural Network (GNN). We
    summarise some common design choices and their differences regarding feature encoding,
    interaction modeling, and prediction head. Some approaches that adopt the generative
    model are also reviewed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更好的预测性能，研究人员提出了具有各种架构的新模型，例如多层感知机（**MLP**）、卷积神经网络（**CNN**）、递归神经网络（**RNN**）、图神经网络（**GNN**）。我们总结了一些常见的设计选择及其在特征编码、交互建模和预测头方面的差异。还回顾了一些采用生成模型的方法。
- en: III-B1 Feature Encoding
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B1 特征编码
- en: Taking the trajectory as sequential data, plenty of papers [[8](#bib.bib8),
    [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [21](#bib.bib21), [23](#bib.bib23),
    [24](#bib.bib24)] propose feature encoder consisting of RNN units, like the gate
    recurrent unit (GRU)[[32](#bib.bib32)] and the long short-term memory. Some combine
    MLP or 1D-Conv with RNN unit to extract hidden space features of the trajectory
    input[[25](#bib.bib25), [26](#bib.bib26), [28](#bib.bib28), [30](#bib.bib30)]
    or the scene context input[[26](#bib.bib26)]. The great success of transformer
    in natural language processing attracts some researchers’ attention and they apply
    the transformer module in feature extraction[[24](#bib.bib24), [27](#bib.bib27),
    [31](#bib.bib31)]. To handle the rasterized input, some work[[8](#bib.bib8), [9](#bib.bib9),
    [11](#bib.bib11), [13](#bib.bib13), [21](#bib.bib21), [22](#bib.bib22), [25](#bib.bib25)]
    directly borrow convolutional feature encoder from object detection [[12](#bib.bib12),
    [10](#bib.bib10)] and image segmentation tasks[[33](#bib.bib33)]. VectorNet[[14](#bib.bib14)]
    follows the idea of PointNet[[15](#bib.bib15)] and extracts instant-level features
    for the vectorized input. TNT [[4](#bib.bib4)] directly inherits the feature extraction
    backbone of VectorNet. Ye et al. take the discrete input as the point cloud and
    mimic the point cloud encoding in their work[[7](#bib.bib7)]. LaneGCN [[20](#bib.bib20)]
    and LaneRCNN[[29](#bib.bib29)] introduce graph convolution modules at the encoding
    stage and aggregate the features across the graph constructed based on the road
    connectivity or temporal sequence.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将轨迹视为序列数据，许多论文[[8](#bib.bib8), [17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [21](#bib.bib21), [23](#bib.bib23), [24](#bib.bib24)] 提出了由**RNN** 单元组成的特征编码器，如门控递归单元（**GRU**）[[32](#bib.bib32)]
    和长短期记忆网络（**LSTM**）。有些将**MLP** 或1D卷积与**RNN** 单元结合，以提取轨迹输入[[25](#bib.bib25), [26](#bib.bib26),
    [28](#bib.bib28), [30](#bib.bib30)] 或场景上下文输入[[26](#bib.bib26)] 的隐空间特征。**Transformer**
    在自然语言处理中的巨大成功引起了部分研究人员的关注，他们将**Transformer** 模块应用于特征提取[[24](#bib.bib24), [27](#bib.bib27),
    [31](#bib.bib31)]。为了处理栅格化输入，一些工作[[8](#bib.bib8), [9](#bib.bib9), [11](#bib.bib11),
    [13](#bib.bib13), [21](#bib.bib21), [22](#bib.bib22), [25](#bib.bib25)] 直接借用了对象检测[[12](#bib.bib12),
    [10](#bib.bib10)] 和图像分割任务[[33](#bib.bib33)] 中的卷积特征编码器。**VectorNet**[[14](#bib.bib14)]
    遵循**PointNet**[[15](#bib.bib15)] 的思路，为矢量化输入提取瞬时级特征。**TNT** [[4](#bib.bib4)] 直接继承了**VectorNet**
    的特征提取骨干网。**Ye** 等人将离散输入视为点云，并在他们的工作中模拟点云编码[[7](#bib.bib7)]。**LaneGCN** [[20](#bib.bib20)]
    和**LaneRCNN**[[29](#bib.bib29)] 在编码阶段引入了图卷积模块，并在基于道路连通性或时间序列构建的图中聚合特征。
- en: III-B2 Interaction Modeling
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B2 交互建模
- en: The interaction among vehicles, pedestrians and road elements is extremely important
    yet complicated. To model the agent-to-agent interaction and agent-to-scene interaction
    at the same time, researchers enhance the social pooling mechanism proposed in
    [[5](#bib.bib5), [6](#bib.bib6)] by including the scene context feature maps[[8](#bib.bib8),
    [22](#bib.bib22), [28](#bib.bib28)]. A variety of researchers[[14](#bib.bib14),
    [17](#bib.bib17), [18](#bib.bib18), [20](#bib.bib20), [21](#bib.bib21), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [30](#bib.bib30)] motivated by the success
    of the attention mechanism design the interaction modeling module. Furthermore,
    Liu et al.[[27](#bib.bib27)] and Ngiam et al.[[31](#bib.bib31)] build the entire
    interaction module with the multi-head attention. The graph modeling and the GNN
    layers are also frequently involved because the message passing of GNN can fuse
    the features of different agents (and road elements). We believe the feature fusion
    behavior is essentially the same as the interaction between agents and the road
    elements. There exists an exception that TPCN[[7](#bib.bib7)] doesn’t explicitly
    consider modeling the interaction but still achieves a good performance.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 车辆、行人和道路元素之间的互动非常重要且复杂。为了同时建模代理之间的互动和代理与场景的互动，研究人员通过引入场景上下文特征图[[8](#bib.bib8),
    [22](#bib.bib22), [28](#bib.bib28)]来增强[[5](#bib.bib5), [6](#bib.bib6)]提出的社会汇聚机制。受到注意力机制成功的启发，许多研究人员[[14](#bib.bib14),
    [17](#bib.bib17), [18](#bib.bib18), [20](#bib.bib20), [21](#bib.bib21), [24](#bib.bib24),
    [25](#bib.bib25), [26](#bib.bib26), [30](#bib.bib30)]设计了互动建模模块。此外，刘等[[27](#bib.bib27)]和Ngiam等[[31](#bib.bib31)]用多头注意力构建了整个互动模块。图建模和GNN层也经常涉及，因为GNN的消息传递可以融合不同代理（和道路元素）的特征。我们认为特征融合行为本质上与代理与道路元素之间的互动相同。唯一的例外是TPCN[[7](#bib.bib7)]没有明确考虑互动建模，但仍然实现了良好的性能。
- en: III-B3 Prediction Head
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B3 预测头
- en: Some researchers characterise the prediction by hidden Markov model and generate
    the trajectory prediction with RNN [[8](#bib.bib8), [17](#bib.bib17), [18](#bib.bib18),
    [21](#bib.bib21), [23](#bib.bib23), [24](#bib.bib24)]. Other researchers take
    the prediction as a regression process and decode the features with MLP in [[14](#bib.bib14),
    [9](#bib.bib9), [19](#bib.bib19), [20](#bib.bib20), [4](#bib.bib4), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)].
    Significantly influenced by the concept of anchor proposal[[34](#bib.bib34), [35](#bib.bib35)]
    in the computer vision field, many researchers add a classification branch in
    their prediction head to predict a probability or a confidence score for each
    proposed trajectory anchor [[11](#bib.bib11), [16](#bib.bib16), [18](#bib.bib18),
    [20](#bib.bib20), [23](#bib.bib23), [4](#bib.bib4), [26](#bib.bib26), [27](#bib.bib27),
    [31](#bib.bib31), [29](#bib.bib29)]. Motivated by the prediction uncertainty of
    the deep-learning model against the unseen cases, CoverNet[[13](#bib.bib13)] and
    PRIME[[30](#bib.bib30)] further deploy a model-based planner to propose trajectory
    anchors that satisfy the constraints imposed by the kinematics of the vehicle
    and the scene context.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员通过隐马尔可夫模型表征预测，并使用RNN生成轨迹预测[[8](#bib.bib8), [17](#bib.bib17), [18](#bib.bib18),
    [21](#bib.bib21), [23](#bib.bib23), [24](#bib.bib24)]。其他研究人员将预测视为回归过程，并在[[14](#bib.bib14),
    [9](#bib.bib9), [19](#bib.bib19), [20](#bib.bib20), [4](#bib.bib4), [25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27), [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]中用MLP解码特征。受到计算机视觉领域锚点提议概念[[34](#bib.bib34),
    [35](#bib.bib35)]的显著影响，许多研究人员在其预测头中添加了分类分支，以预测每个提议轨迹锚点的概率或置信度分数[[11](#bib.bib11),
    [16](#bib.bib16), [18](#bib.bib18), [20](#bib.bib20), [23](#bib.bib23), [4](#bib.bib4),
    [26](#bib.bib26), [27](#bib.bib27), [31](#bib.bib31), [29](#bib.bib29)]。受到深度学习模型对未见情况预测不确定性的启发，CoverNet[[13](#bib.bib13)]和PRIME[[30](#bib.bib30)]进一步部署了基于模型的规划器，以提出满足车辆和场景上下文运动学约束的轨迹锚点。
- en: III-B4 Generative Model
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-B4 生成模型
- en: DESIRE[[8](#bib.bib8)] and SMART[[22](#bib.bib22)] treat the trajectory prediction
    as the conditional sampling and selection process. The recognition module firstly
    projects the trajectory observation and prediction ground truth to the latent
    space. The latent variables $z$ are assumed to satisfy a distribution prior parameterized
    by the encoding of the recognition module. Then, the conditional decoder recovers
    the future trajectory from the given conditional input and the latent variable
    $z$. However, one cannot obtain the likelihood of each trajectory sampled from
    the generative model. DESIRE designs the ranking and refinement module to evaluate
    the trajectories generated from the CVAE module.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: DESIRE[[8](#bib.bib8)]和SMART[[22](#bib.bib22)]将轨迹预测视为条件采样和选择过程。识别模块首先将轨迹观察和预测真实值投影到潜在空间。潜在变量
    $z$ 被假设满足由识别模块编码的分布先验。然后，条件解码器从给定的条件输入和潜在变量 $z$ 中恢复未来轨迹。然而，无法获得从生成模型中采样的每个轨迹的可能性。DESIRE
    设计了排名和精炼模块来评估从CVAE模块生成的轨迹。
- en: III-C Learning and Objective Functions
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 学习与目标函数
- en: Most of the proposed deep-learning models are trained in a supervised manner.
    The cross-entropy (CE) loss, smooth-L1 (Huber) loss, negative log-likelihood (NLL)
    loss and mean-square-error (MSE) loss are all commonly adopted as the objective
    function during the training process. Researchers use some training skills and
    propose new techniques or objective functions so that the model can converge to
    a better optimal.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数提出的深度学习模型都是以监督的方式进行训练的。交叉熵（CE）损失、平滑-L1（Huber）损失、负对数似然（NLL）损失和均方误差（MSE）损失都是训练过程中常用的目标函数。研究人员使用一些训练技巧并提出新技术或目标函数，以便模型能够收敛到更优的解。
- en: MultiPath[[11](#bib.bib11)] adopts an unsupervised learning method to find the
    appropriate trajectory anchors and teaches their model with imitation learning.
    VectorNet[[14](#bib.bib14)] designs an auxiliary graph completion loss to encourage
    the interaction module to capture a better insight. TNT[[4](#bib.bib4)] trains
    the trajectory regressor by a teacher-forcing technique[[36](#bib.bib36)] based
    on their uni-modal assumption.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: MultiPath[[11](#bib.bib11)] 采用无监督学习方法来寻找合适的轨迹锚点，并通过模仿学习对其模型进行训练。VectorNet[[14](#bib.bib14)]
    设计了一种辅助图补全损失，以鼓励互动模块捕捉更好的见解。TNT[[4](#bib.bib4)] 基于其单模态假设通过教师强迫技术[[36](#bib.bib36)]
    训练轨迹回归器。
- en: 'Models which label the ground truth as one certain trajectory while predicting
    diverse outputs suffer from mode collapse problem [[37](#bib.bib37), [38](#bib.bib38)],
    resulting in the failure of making the multi-modal predictions. To overcome this
    problem, some researchers deploy a novel objective function or introduce new functions.
    MTP[[9](#bib.bib9)] targets at minimizing the multiple-trajectory prediction loss,
    which can be regarded as a variant of winner-takes-all (WTA) loss proposed in
    [[39](#bib.bib39)]:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 将真实轨迹标记为某一特定轨迹，同时预测不同输出的模型会遇到模式崩溃问题[[37](#bib.bib37), [38](#bib.bib38)]，导致无法进行多模态预测。为了解决这个问题，一些研究人员提出了新颖的目标函数或引入了新函数。MTP[[9](#bib.bib9)]旨在最小化多轨迹预测损失，可以视为[[39](#bib.bib39)]中提出的赢家通吃（WTA）损失的一种变体：
- en: '|  | $\mathcal{L}^{MTP}=-\sum_{i=1}^{K}I_{i^{*}}\log{p_{i}}+\alpha\sum_{i=1}^{K}I_{i^{*}}L(\tau_{gt},\tau_{i}),$
    |  | (1) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}^{MTP}=-\sum_{i=1}^{K}I_{i^{*}}\log{p_{i}}+\alpha\sum_{i=1}^{K}I_{i^{*}}L(\tau_{gt},\tau_{i}),$
    |  | (1) |'
- en: where $I_{i^{*}}$ is a binary indicator equal to 1 if the $i^{*}$ mode is closest
    to the GT trajectory according to an arbitrary trajectory distance function or
    0 otherwise, $p_{i}$ is the probability of the best mode $i^{*}$, $L(\cdot,\cdot)$
    indicates an arbitrary displacement error function, and $\alpha$ is a coefficient
    weighting the regression loss. Some followers also incorporate the WTA loss in
    their work[[20](#bib.bib20), [24](#bib.bib24), [31](#bib.bib31)]. Narayanan et
    al.[[28](#bib.bib28)] propose a divide-and-conquer (DAC) initialization technique
    for stabilizing the training with WTA loss. The model trained with DAC beats other
    competitors in their work.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{i^{*}}$ 是一个二进制指示符，如果 $i^{*}$ 模式根据任意轨迹距离函数最接近真实轨迹则为1，否则为0，$p_{i}$ 是最佳模式
    $i^{*}$ 的概率，$L(\cdot,\cdot)$ 表示任意位移误差函数，$\alpha$ 是一个加权回归损失的系数。一些研究人员也在他们的工作中引入了WTA损失[[20](#bib.bib20),
    [24](#bib.bib24), [31](#bib.bib31)]。Narayanan等人[[28](#bib.bib28)]提出了一种分治（DAC）初始化技术，以稳定带有WTA损失的训练。使用DAC训练的模型在他们的工作中超越了其他竞争者。
- en: IV Implementation of TNT Approach
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV TNT 方法的实现
- en: TNT[[4](#bib.bib4)], as the extension of VectorNet[[14](#bib.bib14)], draws
    our interest with its good model interpretability and outstanding performance.
    However, the official code is disclosed to the public due to the intellectual
    property issue. Aiming at facilitating the exploration of using the vector and
    graph representation in vehicle trajectory prediction, we implement our version
    of TNT[[4](#bib.bib4)] with our understanding of the TNT[[4](#bib.bib4)] approach
    and make our implementation publicly available.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: TNT[[4](#bib.bib4)]作为VectorNet[[14](#bib.bib14)]的扩展，以其良好的模型可解释性和出色的性能引起了我们的兴趣。然而，由于知识产权问题，官方代码已公开。为了促进在车辆轨迹预测中使用向量和图表示的探索，我们根据对TNT[[4](#bib.bib4)]方法的理解实现了我们版本的TNT[[4](#bib.bib4)]，并公开了我们的实现。
- en: Our code is implemented with Pytorch and Pytorch Geometric libraries. Strictly
    following the details in [[14](#bib.bib14), [4](#bib.bib4)], we implement the
    VectorNet feature extraction backbone and TNT prediction heads. The three prediction
    heads, target candidate prediction head, target-conditioned trajectory prediction
    head, and scoring head are modeled by 2-layer MLPs. The target candidate prediction
    module consists of two 2-layer MLPs, one for predicting the discrete distribution
    over target locations, the other one for predicting the most likely offset corresponding
    to each target candidate. In our design, the target-conditioned trajectory prediction
    module is implemented by a 2-layer MLP, but it is involved twice at the loss computation
    of each batch. For the first time, the MLP predicts the trajectories given the
    ground truth final position regarded as the teacher-forcing training. Then, the
    MLP predicts M trajectories, which will be scored by the scoring module, given
    the M-selected target candidates.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码是用Pytorch和Pytorch Geometric库实现的。严格按照[[14](#bib.bib14), [4](#bib.bib4)]中的细节，我们实现了VectorNet特征提取骨干网和TNT预测头。这三个预测头，即目标候选预测头、目标条件轨迹预测头和评分头，都由2层MLP模型建模。目标候选预测模块由两个2层MLP组成，一个用于预测目标位置的离散分布，另一个用于预测与每个目标候选者对应的最可能的偏移量。在我们的设计中，目标条件轨迹预测模块由一个2层MLP实现，但它在每个批次的损失计算中涉及两次。第一次，MLP根据被视为教师强制训练的实际最终位置预测轨迹。然后，MLP预测M条轨迹，这些轨迹将由评分模块根据M选定的目标候选者进行评分。
- en: 'During the implementation of TNT[[4](#bib.bib4)], we found the descriptions
    of several design choices and hyper-parameters are vague or missing. We propose
    proper solutions to fill in these gaps and introduce them briefly in the remaining
    of this section:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现TNT[[4](#bib.bib4)]的过程中，我们发现对一些设计选择和超参数的描述模糊或缺失。我们提出了适当的解决方案来填补这些空白，并在本节剩余部分简要介绍：
- en: 'Data Normalization: As mentioned in [[14](#bib.bib14)], we centralize the coordinates
    at the last observed position of the target agent for each data sequence. Additionally,
    we apply the heading normalization as described in [[17](#bib.bib17), [29](#bib.bib29),
    [31](#bib.bib31)]. The trajectories and scene context representations are rotated
    to ensure that the heading of the target agent at the last observed position is
    aligned with the x-axis.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 数据归一化：如[[14](#bib.bib14)]中提到的，我们将每个数据序列中的坐标集中在目标代理的最后观察位置。此外，我们应用了[[17](#bib.bib17),
    [29](#bib.bib29), [31](#bib.bib31)]中描述的航向归一化。轨迹和场景上下文表示被旋转，以确保目标代理在最后观察位置的航向与x轴对齐。
- en: 'Target Candidates: According to the description in [[4](#bib.bib4)], TNT deploys
    an equal-distance target candidate sampling along with the candidate center lines.
    The authors take $1000$ target candidates as an example. It’s worth noting that
    the number of target candidates can vary across the sequences since the possible
    travel distance and area of the target agent in the future can be diverse. A unified
    number of candidates can fail to cover a sufficient area for predicting the future
    target. In our code, we implement the equal-distance sampling strategy and sample
    different numbers of target candidates for each sequence.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 目标候选者：根据[[4](#bib.bib4)]中的描述，TNT采用等距离的目标候选者采样方法，并沿候选中心线进行。作者以$1000$个目标候选者作为示例。值得注意的是，目标候选者的数量在序列中可能会有所不同，因为未来目标代理的可能旅行距离和区域可能会有所不同。统一的候选者数量可能无法覆盖足够的区域来预测未来目标。在我们的代码中，我们实现了等距离采样策略，并为每个序列采样不同数量的目标候选者。
- en: 'Non-Maximum Suppression (NMS): Inspired by object detection, TNT[[4](#bib.bib4)]
    filters M multi-modal predictions concerning the predicted score and their distance
    to the selected ones. The M trajectories are sorted by the predicted score firstly
    then picked greedily if their distances to the selected ones exceed a certain
    threshold. However, the exact distance threshold that determines the near-duplicate
    trajectories is not given in their paper. Here we implement a hard-threshold strategy.
    Specifically, only trajectories that exceed the distance threshold are selected
    and all-zero trajectories are padded to the remaining slots if no more trajectory
    is distant enough from all the selected ones.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 非极大值抑制（NMS）：受到物体检测的启发，TNT[[4](#bib.bib4)] 根据预测分数和与选定轨迹的距离筛选 M 个多模态预测。这些 M 个轨迹首先按预测分数排序，然后如果它们与选定轨迹的距离超过某个阈值，则被贪婪地选择。然而，决定近似重复轨迹的精确距离阈值在他们的论文中没有给出。在这里，我们实现了一个硬阈值策略。具体而言，只有超过距离阈值的轨迹才会被选择，如果没有更多轨迹的距离足够远，则用全零轨迹填充剩余的槽位。
- en: Although exactly the same results as [[4](#bib.bib4)] have not yet been achieved,
    our implementation still demonstrates outstanding performance. The results achieved
    so far are shown in Section [V](#S5 "V Experiments ‣ A Survey on Deep-Learning
    Approaches for Vehicle Trajectory Prediction in Autonomous Driving").
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管尚未获得与[[4](#bib.bib4)]完全相同的结果，但我们的实现仍然表现出色。迄今为止取得的结果见于第 [V](#S5 "V Experiments
    ‣ A Survey on Deep-Learning Approaches for Vehicle Trajectory Prediction in Autonomous
    Driving")节。
- en: V Experiments
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 实验
- en: This section is organized as follows. Firstly, several frequently used datasets
    are introduced. Secondly, we present existing metrics that the experiments are
    evaluated with. In addition, we list the commonly adopted preprocessing tricks
    and data augmentation strategies. Finally, a detailed comparison of recently proposed
    methods is demonstrated.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 本节组织如下。首先，介绍几个常用的数据集。其次，我们展示实验评估所使用的现有指标。此外，我们列出了常用的预处理技巧和数据增强策略。最后，展示了最近提出方法的详细比较。
- en: 'Table II: Trajectory Prediction Performance on Argoverse Dataset'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：Argoverse 数据集上的轨迹预测性能
- en: 'Validation Set Test Set k=1 k=6 k=1 k=6 Model minADE^† minFDE^† MR^‡ minADE
    minFDE MR minADE minFDE MR minADE minFDE MR DAC VectorNet[[14](#bib.bib14)] 1.66
    3.67 - - - - 1.81 4.01 - - - - - TPNet[[16](#bib.bib16)] 1.75 3.88 - - - - 2.23
    4.70 - 1.61 3.28 - 0.96 Luo[[18](#bib.bib18)] 1.60 3.64 - 1.35 2.68 - 1.91 4.31
    0.66 0.99 1.71 0.19 0.98 LaneGCN[[20](#bib.bib20)] 1.35 2.97 - 0.71 1.08 - 1.71
    3.78 0.59 0.87 1.36 0.16 - SMART[[22](#bib.bib22)] - - - 1.44 2.47 - - - - - -
    - - TNT[[4](#bib.bib4)] - - - 0.73 1.29 0.09 - - - 0.94 1.54 0.13 - WIMP[[24](#bib.bib24)]
    1.45 3.19 - 0.75 1.14 0.12 1.82 4.03 - 0.90 1.42 0.17 - HOME[[25](#bib.bib25)]
    - 3.02 0.51 - 1.28 0.07 1.73 3.73 0.58 0.94 1.45 0.10 - TPCN[[7](#bib.bib7)] 1.34
    2.95 0.50 0.73 1.15 0.11 1.66 3.69 0.59 0.87 1.38 0.16 - LaPred[[26](#bib.bib26)]
    1.48 3.29 - 0.71 1.44 - - - - - - - - MMTrans[[27](#bib.bib27)] - - - 0.71 1.15
    0.11 - - - 0.84 1.34 0.15 - LaneRCNN[[29](#bib.bib29)] 1.33 2.85 - 0.77 1.19 0.08
    1.69 3.69 0.57 0.90 1.45 0.12 - PRIME[[30](#bib.bib30)] - - - - - - 1.91 3.82
    0.59 1.22 1.56 0.12 - SceneTrans[[31](#bib.bib31)] - - - - - - - - - 0.80 1.23
    0.13 - Ours - - - 1.11 2.12 0.31 - - - - - - - $\star$ minADE/ minFDE: in meters
    $\ddagger$ MR: the threshold for endpoint error is 2m'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '验证集 测试集 k=1 k=6 k=1 k=6 模型 minADE^† minFDE^† MR^‡ minADE minFDE MR minADE minFDE
    MR minADE minFDE MR DAC VectorNet[[14](#bib.bib14)] 1.66 3.67 - - - - 1.81 4.01
    - - - - - TPNet[[16](#bib.bib16)] 1.75 3.88 - - - - 2.23 4.70 - 1.61 3.28 - 0.96
    Luo[[18](#bib.bib18)] 1.60 3.64 - 1.35 2.68 - 1.91 4.31 0.66 0.99 1.71 0.19 0.98
    LaneGCN[[20](#bib.bib20)] 1.35 2.97 - 0.71 1.08 - 1.71 3.78 0.59 0.87 1.36 0.16
    - SMART[[22](#bib.bib22)] - - - 1.44 2.47 - - - - - - - - TNT[[4](#bib.bib4)]
    - - - 0.73 1.29 0.09 - - - 0.94 1.54 0.13 - WIMP[[24](#bib.bib24)] 1.45 3.19 -
    0.75 1.14 0.12 1.82 4.03 - 0.90 1.42 0.17 - HOME[[25](#bib.bib25)] - 3.02 0.51
    - 1.28 0.07 1.73 3.73 0.58 0.94 1.45 0.10 - TPCN[[7](#bib.bib7)] 1.34 2.95 0.50
    0.73 1.15 0.11 1.66 3.69 0.59 0.87 1.38 0.16 - LaPred[[26](#bib.bib26)] 1.48 3.29
    - 0.71 1.44 - - - - - - - - MMTrans[[27](#bib.bib27)] - - - 0.71 1.15 0.11 - -
    - 0.84 1.34 0.15 - LaneRCNN[[29](#bib.bib29)] 1.33 2.85 - 0.77 1.19 0.08 1.69
    3.69 0.57 0.90 1.45 0.12 - PRIME[[30](#bib.bib30)] - - - - - - 1.91 3.82 0.59
    1.22 1.56 0.12 - SceneTrans[[31](#bib.bib31)] - - - - - - - - - 0.80 1.23 0.13
    - 我们 - - - 1.11 2.12 0.31 - - - - - - - $\star$ minADE/ minFDE: 单位为米 $\ddagger$
    MR: 端点误差的阈值为2米'
- en: V-A Datasets
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 数据集
- en: Argoverse The Argoverse Dataset[[40](#bib.bib40)] is the most frequently used
    dataset for motion forecasting experiments. It contains over 300K scenarios in
    Pittsburgh and Miami. Each scenario contains a 2D birds-eye-view centroid of different
    objects at 10 Hz. The task of motion forecasting is to predict the trajectories
    of the sole agent type object in the next 3 seconds, given the preceding 2-second
    long trajectories together with HD map features which can be accessed through
    Argoverse API. The whole dataset can be split into 208,272 training sequences,
    40,127 validation sequences, and 79,391 testing sequences. For training and validation,
    full 5-second trajectories are provided. While for testing, only the first 2 seconds
    trajectories are given.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Argoverse Argoverse 数据集[[40](#bib.bib40)]是最常用于运动预测实验的数据集。它包含了匹兹堡和迈阿密的超过30万场景。每个场景包含了不同对象在10
    Hz下的二维鸟瞰图心点。运动预测的任务是根据前2秒的轨迹以及通过Argoverse API访问的高清地图特征，预测接下来3秒内的单一代理类型对象的轨迹。整个数据集可以分为208,272个训练序列，40,127个验证序列和79,391个测试序列。在训练和验证中，提供完整的5秒轨迹。而在测试中，只提供前2秒的轨迹。
- en: NuScenes NuScenes is a public large-scale dataset for autonomous driving[[41](#bib.bib41)].
    The trajectories are represented in the x-y coordinate system at 2 Hz. The original
    driving scenarios are collected in Boston and Singapore, where right-hand and
    left-hand traffic rules apply respectively. Up to 2 seconds of past history can
    be utilized to predict 6-second long future trajectories for each agent.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: NuScenes NuScenes是一个公开的大规模自动驾驶数据集[[41](#bib.bib41)]。轨迹以2 Hz的频率在x-y坐标系统中表示。原始驾驶场景在波士顿和新加坡收集，分别适用右侧和左侧交通规则。最多可以利用2秒的过去历史来预测每个代理的6秒长的未来轨迹。
- en: NGSIM Next Generation SIMulation[[42](#bib.bib42)] dataset is extracted from
    real-world highway driving scenarios using high mounted digital video cameras.
    The precise location of each vehicle is recorded at 10 Hz. Since the dataset is
    not published purely for trajectory prediction, the length of past observations
    and predictions can be customized.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: NGSIM 下一代模拟[[42](#bib.bib42)]数据集是通过高架数字视频摄像机从实际高速公路驾驶场景中提取的。每辆车的位置以10 Hz的频率记录。由于数据集不是纯粹为了轨迹预测而发布的，因此过去观察和预测的长度可以自定义。
- en: V-B Metrics
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 指标
- en: Some commonly used metrics are as follows.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常用的指标如下。
- en: FDE(K) Minimum Final Displacement Error over K refers to the L2 distance between
    the endpoint of predicted trajectory and that of ground truth over the best K
    predictions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: FDE(K) 最终位移误差（K）指的是预测轨迹终点与真实值终点之间的L2距离，在最佳K次预测中计算。
- en: ADE(K) Minimum Average Displacement Error over K denotes the average pointwise
    L2 distance between the whole forecasted trajectory and the ground truth over
    the best K predictions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ADE(K) 平均位移误差（K）指的是在最佳K次预测中，整个预测轨迹与真实值之间的逐点L2距离的平均值。
- en: MR Miss Rate evaluates the proportion of unacceptable outputs in all proposed
    solutions. One scenario is usually defined as a miss when the endpoint error for
    the best trajectory is greater than 2.0m.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: MR 漏检率评估所有提出的解决方案中不可接受输出的比例。当最佳轨迹的终点误差大于2.0米时，通常将一个场景定义为漏检。
- en: Some papers have proposed several specific metrics corresponding to their innovative
    methods.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一些论文提出了与其创新方法相对应的若干特定指标。
- en: DAC Drivable Area Compliance[[16](#bib.bib16), [18](#bib.bib18), [21](#bib.bib21)]
    equals to the count of future trajectories within the drivable area divided by
    the number of all possible trajectories. DAC evaluates the feasibility of proposed
    solutions.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: DAC 可驱动区域合规性[[16](#bib.bib16), [18](#bib.bib18), [21](#bib.bib21)]等于在可驱动区域内的未来轨迹数量除以所有可能轨迹的数量。DAC评估提出解决方案的可行性。
- en: V-C Preprocessing and Augmentation
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 预处理和增强
- en: Centralization Aiming at reducing the complexity of prediction, the origin of
    the coordinate system to represent trajectories is chosen to be the position of
    the predicted agent at the last observed timestamp[[13](#bib.bib13), [14](#bib.bib14),
    [17](#bib.bib17), [29](#bib.bib29), [31](#bib.bib31)].
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 中心化 为了减少预测的复杂性，选择将代表轨迹的坐标系统原点设置为最后观察时间戳的预测代理位置[[13](#bib.bib13), [14](#bib.bib14),
    [17](#bib.bib17), [29](#bib.bib29), [31](#bib.bib31)]。
- en: Heading Normalization Heading normalization means that the coordinate system
    where trajectories are represented is rotated[[13](#bib.bib13), [17](#bib.bib17),
    [29](#bib.bib29), [31](#bib.bib31)], such that the orientation of the predicted
    agent at the last observed timestamp is aligned with the x-axis.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**标题归一化** 标题归一化意味着轨迹表示的坐标系统被旋转[[13](#bib.bib13), [17](#bib.bib17), [29](#bib.bib29),
    [31](#bib.bib31)]，以使最后观察时间戳的预测代理的方向与x轴对齐。'
- en: Scene Rotation In order to combat overfitting and improve generalization capability,
    random rotation[[8](#bib.bib8), [25](#bib.bib25), [29](#bib.bib29), [31](#bib.bib31)]
    is frequently applied to the whole scenario and the trajectory coordinates following
    centralization.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景旋转** 为了对抗过拟合并提高泛化能力，通常对整个场景及其中心化后的轨迹坐标进行随机旋转[[8](#bib.bib8), [25](#bib.bib25),
    [29](#bib.bib29), [31](#bib.bib31)]。'
- en: Agent Dropout As is often the case, there are excessive agents in one scenario,
    some of which are not worthy of attention. In [[31](#bib.bib31)], non-predicted
    agents are artificially removed with probability of 0.1.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**代理丢弃** 如常见的情况一样，一个场景中可能存在过多的代理，其中一些并不值得关注。在[[31](#bib.bib31)]中，非预测代理以0.1的概率被人工移除。'
- en: V-D Results
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**第V-D部分 结果**'
- en: This section makes a comparison among the results of the fore-mentioned methods.
    Since Argoverse is the most frequently used data set, for a fair comparison we
    only compare experimental results in Argoverse. All data is collected originally
    from the published papers and listed in TABLE [II](#S5.T2 "Table II ‣ V Experiments
    ‣ A Survey on Deep-Learning Approaches for Vehicle Trajectory Prediction in Autonomous
    Driving").
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本节对上述方法的结果进行了比较。由于Argoverse是最常用的数据集，为了公平比较，我们仅比较Argoverse中的实验结果。所有数据均来自已发布的论文，并列在表[II](#S5.T2
    "Table II ‣ V Experiments ‣ A Survey on Deep-Learning Approaches for Vehicle Trajectory
    Prediction in Autonomous Driving")中。
- en: Scene Transformer[[31](#bib.bib31)] ranks first when sorting results by minADE(K=6)
    and minFDE(K=6), while the first prize belongs to [[25](#bib.bib25)] when ranked
    by MR(K=6). Both methods use the attention module to model the interaction between
    agents and environments, which provides a promising solution to interaction representation.
    A performance drop is usually observed among other approaches (TNT[[4](#bib.bib4)],
    WIMP[[24](#bib.bib24)], HOME[[25](#bib.bib25)], TPCN[[7](#bib.bib7)], MMTrans[[27](#bib.bib27)],
    annd laneRCNN[[29](#bib.bib29)]) when applying them on the test set. We evaluate
    our TNT implementation on the validation set of Argoverse. Compared with the state-of-the-art,
    our performance is 0.4 meter worse in minADE, 1.04 meters in minFDE, 0.24 in MR.
    Improvement is expected by introducing the WTA loss or the lane attention strategy
    in our design.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景变换器**[[31](#bib.bib31)]在按minADE(K=6)和minFDE(K=6)排序的结果中排名第一，而在按MR(K=6)排序时，第一名属于[[25](#bib.bib25)]。这两种方法都使用了注意力模块来建模代理与环境之间的交互，这为交互表示提供了有前途的解决方案。通常会观察到其他方法（TNT[[4](#bib.bib4)],
    WIMP[[24](#bib.bib24)], HOME[[25](#bib.bib25)], TPCN[[7](#bib.bib7)], MMTrans[[27](#bib.bib27)],
    和 laneRCNN[[29](#bib.bib29])）在测试集上应用时性能下降。我们在Argoverse的验证集上评估了我们的TNT实现。与最先进的方法相比，我们在minADE上差0.4米，在minFDE上差1.04米，在MR上差0.24。预计通过引入WTA损失或车道注意力策略可以改善性能。'
- en: VI Conclusion
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**第VI部分 结论**'
- en: 'In this work, we make a thorough review of existing methods on trajectory prediction
    for vehicles. Starting from the mathematical formulation of the prediction problem,
    we divide the complicated forecasting task into three components: representation,
    modeling as well as learning and objective functions. Modules varying from MLP
    to CNN, RNN, GNN form constituent parts of prediction networks, namely feature
    encoding, interaction modeling, and prediction header. We additionally make our
    version of TNT publicly available and present the implementation in detail. A
    fair comparison of results on Argoverse Dataset alongside with list of metrics
    and preprocessing tricks are illustrated at the end of this work. Further improvement
    of trajectory prediction, e.g., accuracy or efficiency, is expected with the assistance
    of our work.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们对现有的车辆轨迹预测方法进行了彻底的回顾。从预测问题的数学公式出发，我们将复杂的预测任务分解为三个组成部分：表示、建模以及学习和目标函数。从MLP到CNN、RNN、GNN的模块组成了预测网络的组成部分，即特征编码、交互建模和预测头。我们还将我们的TNT版本公开，并详细介绍了实现方法。对Argoverse数据集结果的公平比较以及指标和预处理技巧的列表在本工作末尾进行了说明。预计在我们工作的帮助下，轨迹预测的准确性或效率将有所提升。
- en: Acknowledgment
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**致谢**'
- en: 'This work is partially supported by Shenzhen Key Laboratory of Robotics Perception
    and Intelligence, Southern University of Science and Technology, Shenzhen 518055,
    China, Hong Kong RGC CRF grant C4063-18G, Hong Kong RGC GRF grant #14200618.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] A. Ess, K. Schindler, B. Leibe, and L. Van Gool, “Object detection and
    tracking for autonomous navigation in dynamic environments,” *The International
    Journal of Robotics Research*, vol. 29, no. 14, pp. 1707–1725, 2010.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] S. Pellegrini, A. Ess, and L. Van Gool, “Predicting pedestrian trajectories,”
    in *Visual Analysis of Humans*.   Springer, 2011, pp. 473–491.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] M. Luber, J. A. Stork, G. D. Tipaldi, and K. O. Arras, “People tracking
    with human motion predictions from social forces,” in *2010 IEEE International
    Conference on Robotics and Automation*.   IEEE, 2010, pp. 464–469.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] H. Zhao, J. Gao, T. Lan, C. Sun, B. Sapp, B. Varadarajan, Y. Shen, Y. Shen,
    Y. Chai, C. Schmid *et al.*, “Tnt: Target-driven trajectory prediction,” *arXiv
    preprint arXiv:2008.08294*, 2020.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese,
    “Social lstm: Human trajectory prediction in crowded spaces,” in *Proceedings
    of the IEEE conference on computer vision and pattern recognition*, 2016, pp.
    961–971.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] A. Gupta, J. Johnson, L. Fei-Fei, S. Savarese, and A. Alahi, “Social gan:
    Socially acceptable trajectories with generative adversarial networks,” in *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*, 2018, pp.
    2255–2264.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Ye, T. Cao, and Q. Chen, “Tpcn: Temporal point cloud networks for motion
    forecasting,” in *Proceedings of the IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, 2021, pp. 11 318–11 327.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] N. Lee, W. Choi, P. Vernaza, C. B. Choy, P. H. Torr, and M. Chandraker,
    “Desire: Distant future prediction in dynamic scenes with interacting agents,”
    in *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    2017, pp. 336–345.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] H. Cui, V. Radosavljevic, F.-C. Chou, T.-H. Lin, T. Nguyen, T.-K. Huang,
    J. Schneider, and N. Djuric, “Multimodal trajectory predictions for autonomous
    driving using deep convolutional networks,” in *2019 International Conference
    on Robotics and Automation (ICRA)*.   IEEE, 2019, pp. 2090–2096.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, “Mobilenetv2:
    Inverted residuals and linear bottlenecks,” in *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, 2018, pp. 4510–4520.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Y. Chai, B. Sapp, M. Bansal, and D. Anguelov, “Multipath: Multiple probabilistic
    anchor trajectory hypotheses for behavior prediction,” *arXiv preprint arXiv:1910.05449*,
    2019.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *Proceedings of the IEEE conference on computer vision and pattern
    recognition*, 2016, pp. 770–778.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] T. Phan-Minh, E. C. Grigore, F. A. Boulton, O. Beijbom, and E. M. Wolff,
    “Covernet: Multimodal behavior prediction using trajectory sets,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020,
    pp. 14 074–14 083.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] T. Phan-Minh, E. C. Grigore, F. A. Boulton, O. Beijbom, 和 E. M. Wolff,
    “Covernet: 使用轨迹集进行多模态行为预测，”在*IEEE/CVF计算机视觉与模式识别会议论文集*中，2020，第14 074–14 083页。'
- en: '[14] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C. Schmid, “Vectornet:
    Encoding hd maps and agent dynamics from vectorized representation,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020,
    pp. 11 525–11 533.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, 和 C. Schmid, “Vectornet:
    从矢量化表示编码高清地图和智能体动态，”在*IEEE/CVF计算机视觉与模式识别会议论文集*中，2020，第11 525–11 533页。'
- en: '[15] C. R. Qi, H. Su, K. Mo, and L. J. Guibas, “Pointnet: Deep learning on
    point sets for 3d classification and segmentation,” in *Proceedings of the IEEE
    conference on computer vision and pattern recognition*, 2017, pp. 652–660.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] C. R. Qi, H. Su, K. Mo, 和 L. J. Guibas, “Pointnet: 深度学习点集用于3D分类和分割，”在*IEEE计算机视觉与模式识别会议论文集*中，2017，第652–660页。'
- en: '[16] L. Fang, Q. Jiang, J. Shi, and B. Zhou, “Tpnet: Trajectory proposal network
    for motion prediction,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2020, pp. 6797–6806.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] L. Fang, Q. Jiang, J. Shi, 和 B. Zhou, “Tpnet: 用于运动预测的轨迹提议网络，”在*IEEE/CVF计算机视觉与模式识别会议论文集*中，2020，第6797–6806页。'
- en: '[17] J. Mercat, T. Gilles, N. El Zoghby, G. Sandou, D. Beauvois, and G. P.
    Gil, “Multi-head attention for multi-modal joint vehicle motion forecasting,”
    in *2020 IEEE International Conference on Robotics and Automation (ICRA)*.   IEEE,
    2020, pp. 9638–9644.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. Mercat, T. Gilles, N. El Zoghby, G. Sandou, D. Beauvois, 和 G. P. Gil,
    “用于多模态联合车辆运动预测的多头注意力，”在*2020 IEEE国际机器人与自动化会议（ICRA）*中。 IEEE, 2020，第9638–9644页。'
- en: '[18] C. Luo, L. Sun, D. Dabiri, and A. Yuille, “Probabilistic multi-modal trajectory
    prediction with lane attention for autonomous vehicles,” in *2020 IEEE/RSJ International
    Conference on Intelligent Robots and Systems (IROS)*.   IEEE, 2020, pp. 2370–2376.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] C. Luo, L. Sun, D. Dabiri, 和 A. Yuille, “具有车道注意力的概率性多模态轨迹预测用于自动驾驶车辆，”在*2020
    IEEE/RSJ国际智能机器人与系统会议（IROS）*中。 IEEE, 2020，第2370–2376页。'
- en: '[19] J. Pan, H. Sun, K. Xu, Y. Jiang, X. Xiao, J. Hu, and J. Miao, “Lane-attention:
    Predicting vehicles’ moving trajectories by learning their attention over lanes,”
    in *2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*.   IEEE,
    2020, pp. 7949–7956.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] J. Pan, H. Sun, K. Xu, Y. Jiang, X. Xiao, J. Hu, 和 J. Miao, “车道注意力: 通过学习车道上的注意力预测车辆移动轨迹，”在*2020
    IEEE/RSJ国际智能机器人与系统会议（IROS）*中。 IEEE, 2020，第7949–7956页。'
- en: '[20] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, and R. Urtasun, “Learning
    lane graph representations for motion forecasting,” in *European Conference on
    Computer Vision*.   Springer, 2020, pp. 541–556.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao, S. Feng, 和 R. Urtasun, “学习车道图表示进行运动预测，”在*欧洲计算机视觉会议*上。
    Springer, 2020, 第541–556页。'
- en: '[21] S. H. Park, G. Lee, J. Seo, M. Bhat, M. Kang, J. Francis, A. Jadhav, P. P.
    Liang, and L.-P. Morency, “Diverse and admissible trajectory forecasting through
    multimodal context understanding,” in *European Conference on Computer Vision*.   Springer,
    2020, pp. 282–298.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. H. Park, G. Lee, J. Seo, M. Bhat, M. Kang, J. Francis, A. Jadhav, P.
    P. Liang, 和 L.-P. Morency, “通过多模态上下文理解实现多样化且可接受的轨迹预测，”在*欧洲计算机视觉会议*上。 Springer,
    2020, 第282–298页。'
- en: '[22] N. Sriram, B. Liu, F. Pittaluga, and M. Chandraker, “Smart: Simultaneous
    multi-agent recurrent trajectory prediction,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 463–479.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] N. Sriram, B. Liu, F. Pittaluga, 和 M. Chandraker, “Smart: 同时多智能体递归轨迹预测，”在*欧洲计算机视觉会议*上。
    Springer, 2020, 第463–479页。'
- en: '[23] H. Song, W. Ding, Y. Chen, S. Shen, M. Y. Wang, and Q. Chen, “Pip: Planning-informed
    trajectory prediction for autonomous driving,” in *European Conference on Computer
    Vision*.   Springer, 2020, pp. 598–614.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] H. Song, W. Ding, Y. Chen, S. Shen, M. Y. Wang, 和 Q. Chen, “Pip: 基于规划的轨迹预测用于自动驾驶，”在*欧洲计算机视觉会议*上。
    Springer, 2020, 第598–614页。'
- en: '[24] S. Khandelwal, W. Qi, J. Singh, A. Hartnett, and D. Ramanan, “What-if
    motion prediction for autonomous driving,” *arXiv preprint arXiv:2008.10587*,
    2020.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Khandelwal, W. Qi, J. Singh, A. Hartnett, 和 D. Ramanan, “自动驾驶的‘如果-那么’运动预测，”*arXiv预印本
    arXiv:2008.10587*，2020。'
- en: '[25] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, and F. Moutarde,
    “Home: Heatmap output for future motion estimation,” *arXiv preprint arXiv:2105.10968*,
    2021.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] T. Gilles, S. Sabatini, D. Tsishkou, B. Stanciulescu, 和 F. Moutarde, “Home:
    用于未来运动估计的热图输出，”*arXiv预印本 arXiv:2105.10968*，2021。'
- en: '[26] B. Kim, S. H. Park, S. Lee, E. Khoshimjonov, D. Kum, J. Kim, J. S. Kim,
    and J. W. Choi, “Lapred: Lane-aware prediction of multi-modal future trajectories
    of dynamic agents,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2021, pp. 14 636–14 645.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Liu, J. Zhang, L. Fang, Q. Jiang, and B. Zhou, “Multimodal motion prediction
    with stacked transformers,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2021, pp. 7577–7586.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Narayanan, R. Moslemi, F. Pittaluga, B. Liu, and M. Chandraker, “Divide-and-conquer
    for lane-aware diverse trajectory prediction,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2021, pp. 15 799–15 808.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] W. Zeng, M. Liang, R. Liao, and R. Urtasun, “Lanercnn: Distributed representations
    for graph-centric motion forecasting,” *arXiv preprint arXiv:2101.06653*, 2021.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. Song, D. Luan, W. Ding, M. Y. Wang, and Q. Chen, “Learning to predict
    vehicle trajectories with model-based planning,” *arXiv preprint arXiv:2103.04027*,
    2021.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Ngiam, B. Caine, V. Vasudevan, Z. Zhang, H.-T. L. Chiang, J. Ling,
    R. Roelofs, A. Bewley, C. Liu, A. Venugopal *et al.*, “Scene transformer: A unified
    multi-task model for behavior prediction and planning,” *arXiv preprint arXiv:2106.08417*,
    2021.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for
    statistical machine translation,” *arXiv preprint arXiv:1406.1078*, 2014.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] D. Erhan, C. Szegedy, A. Toshev, and D. Anguelov, “Scalable object detection
    using deep neural networks,” in *Proceedings of the IEEE conference on computer
    vision and pattern recognition*, 2014, pp. 2147–2154.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” *Advances in neural information
    processing systems*, vol. 28, pp. 91–99, 2015.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] R. J. Williams and D. Zipser, “A learning algorithm for continually running
    fully recurrent neural networks,” *Neural computation*, vol. 1, no. 2, pp. 270–280,
    1989.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] N. Rhinehart, K. M. Kitani, and P. Vernaza, “R2p2: A reparameterized pushforward
    policy for diverse, precise generative path forecasting,” in *Proceedings of the
    European Conference on Computer Vision (ECCV)*, 2018, pp. 772–788.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. Hong, B. Sapp, and J. Philbin, “Rules of the road: Predicting driving
    behavior with a convolutional model of semantic interactions,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019,
    pp. 8454–8462.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] S. Lee, S. P. S. Prakash, M. Cogswell, V. Ranjan, D. Crandall, and D. Batra,
    “Stochastic multiple choice learning for training diverse deep ensembles,” in
    *Advances in Neural Information Processing Systems*, 2016, pp. 2119–2127.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] S. Lee, S. P. S. Prakash, M. Cogswell, V. Ranjan, D. Crandall, 和 D. Batra，
    “用于训练多样化深度集成的随机多选学习，” 收录于*神经信息处理系统进展*，2016年，第2119–2127页。'
- en: '[40] M.-F. Chang, J. W. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
    D. Wang, P. Carr, S. Lucey, D. Ramanan, and J. Hays, “Argoverse: 3d tracking and
    forecasting with rich maps,” in *Conference on Computer Vision and Pattern Recognition
    (CVPR)*, 2019.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] M.-F. Chang, J. W. Lambert, P. Sangkloy, J. Singh, S. Bak, A. Hartnett,
    D. Wang, P. Carr, S. Lucey, D. Ramanan, 和 J. Hays， “Argoverse：具有丰富地图的3D跟踪与预测，”
    收录于*计算机视觉与模式识别会议（CVPR）*，2019年。'
- en: '[41] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, and O. Beijbom, “nuscenes: A multimodal dataset for autonomous
    driving,” in *Proceedings of the IEEE/CVF conference on computer vision and pattern
    recognition*, 2020, pp. 11 621–11 631.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan,
    Y. Pan, G. Baldan, 和 O. Beijbom， “nuscenes：用于自动驾驶的多模态数据集，” 收录于*IEEE/CVF计算机视觉与模式识别会议论文集*，2020年，第11 621–11 631页。'
- en: '[42] J. C. John Halkias, “Next generation simulation fact sheet,” Federal Highway
    Administration (FHWA), 2006.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] J. C. John Halkias， “下一代模拟信息表，” 联邦公路管理局（FHWA），2006年。'
