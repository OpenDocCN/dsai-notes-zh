- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:50:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:50:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.12638] Deep Learning for UAV-based Object Detection and Tracking: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.12638] 基于深度学习的无人机目标检测与跟踪：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.12638](https://ar5iv.labs.arxiv.org/html/2110.12638)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.12638](https://ar5iv.labs.arxiv.org/html/2110.12638)
- en: 'Deep Learning for UAV-based Object Detection and Tracking: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的无人机目标检测与跟踪：综述
- en: 'Xin Wu,  Wei Li,  Danfeng Hong,  Ran Tao,  and Qian Du This work was supported,
    in part by the National Natural Science Foundation of China under Grant 61922013,
    62101045, and U1833203, and partly by the China Postdoctoral Science Foundation
    Funded Project No. 2021M690385.X. Wu, W. Li, and R. Tao are with the School of
    Information and Electronics, Beijing Institute of Technology, 100081 Beijing,
    China, and Beijing Key Laboratory of Fractional Signals and Systems, 100081 Beijing,
    China. (e-mail: 040251522wuxin@163.com; liwei089@ieee.org; rantao@bit.edu.cn)D.
    Hong is with the Key Laboratory of Digital Earth Science, Aerospace Information
    Research Institute, Chinese Academy of Sciences, 100094 Beijing, China. (e-mail:
    hongdf@aircas.ac.cn)Q. Du is with the Department of Electrical and Computer Engineering,
    Mississippi State University, Starkville, MS 39762, USA. (e-mail: du@ece.msstate.edu)'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Xin Wu, Wei Li, Danfeng Hong, Ran Tao, 和 Qian Du 本工作部分得到中国国家自然科学基金资助（资助号61922013、62101045、U1833203），部分由中国博士后科学基金资助项目（资助号2021M690385.X）支持。X.
    Wu, W. Li 和 R. Tao 供职于北京理工大学信息与电子学院（100081，北京，中国）以及北京分数信号与系统重点实验室（100081，北京，中国）。(电子邮箱：040251522wuxin@163.com;
    liwei089@ieee.org; rantao@bit.edu.cn) D. Hong 供职于中国科学院空间信息研究所数字地球科学重点实验室（100094，北京，中国）。(电子邮箱：hongdf@aircas.ac.cn)
    Q. Du 供职于密西西比州立大学电气与计算机工程系（39762，MS，美国）。(电子邮箱：du@ece.msstate.edu)
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'This is the pre-acceptance version, to read the final version please go to
    IEEE Geoscience and Remote Sensing Magazine on IEEE Xplore. Owing to effective
    and flexible data acquisition, unmanned aerial vehicle (UAV) has recently become
    a hotspot across the fields of computer vision (CV) and remote sensing (RS). Inspired
    by recent success of deep learning (DL), many advanced object detection and tracking
    approaches have been widely applied to various UAV-related tasks, such as environmental
    monitoring, precision agriculture, traffic management. This paper provides a comprehensive
    survey on the research progress and prospects of DL-based UAV object detection
    and tracking methods. More specifically, we first outline the challenges, statistics
    of existing methods, and provide solutions from the perspectives of DL-based models
    in three research topics: object detection from the image, object detection from
    the video, and object tracking from the video. Open datasets related to UAV-dominated
    object detection and tracking are exhausted, and four benchmark datasets are employed
    for performance evaluation using some state-of-the-art methods. Finally, prospects
    and considerations for the future work are discussed and summarized. It is expected
    that this survey can facilitate those researchers who come from remote sensing
    field with an overview of DL-based UAV object detection and tracking methods,
    along with some thoughts on their further developments.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预接受版本，欲阅读最终版本，请访问IEEE Xplore上的《IEEE地球科学与遥感杂志》。由于高效而灵活的数据采集，无人机（UAV）近期在计算机视觉（CV）和遥感（RS）领域成为了热点。受到深度学习（DL）最近成功的启发，许多先进的目标检测和跟踪方法已被广泛应用于各种无人机相关任务，如环境监测、精准农业、交通管理。本文提供了基于深度学习的无人机目标检测和跟踪方法的研究进展与前景的全面调查。更具体地，我们首先概述了挑战、现有方法的统计数据，并从深度学习模型的角度提供了解决方案，涵盖三个研究主题：图像中的目标检测、视频中的目标检测以及视频中的目标跟踪。我们彻底梳理了与无人机主导的目标检测和跟踪相关的开放数据集，并采用四个基准数据集使用一些最先进的方法进行性能评估。最后，我们讨论和总结了未来工作的前景和考虑。希望这项调查能为遥感领域的研究人员提供一个基于深度学习的无人机目标检测和跟踪方法的概述，并对其进一步发展提出一些思考。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep learning, object detection, object tracking, remote sensing, unmanned aerial
    vehicle, video.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、目标检测、目标跟踪、遥感、无人机、视频。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Object detection and tracking, as an important research topic in the field of
    remote sensing, has been widely investigated and applied to various civil and
    military tasks, such as environmental monitoring, geological hazard detection,
    precision agriculture, and urban planning. Traditional object acquisition methods
    derive mainly from satellites and manned aircraft. Normally, the two types of
    platforms run on a fixed rail or follow a predetermined path, or temporarily change
    the running route and hover according to a commissioned task, e.g., city planning
    and mapping, or performing object observation in a harsh and inhospitable environment,
    e.g., remote sensing in the cryosphere. However, the cost of satellites and manned
    aircraft, and the potential safety issues of pilots inevitably limit the application
    scope of such platforms.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测和跟踪作为遥感领域的一个重要研究课题，已被广泛研究并应用于各种民用和军事任务，如环境监测、地质灾害检测、精密农业和城市规划。传统的目标获取方法主要来源于卫星和载人飞机。通常，这两种平台运行在固定轨道上或按照预定路径运行，或根据委托任务临时改变运行路线并悬停，例如城市规划和制图，或在严酷且不适宜的环境中进行目标观察，例如在冰冻圈进行遥感。然而，卫星和载人飞机的成本以及飞行员的潜在安全问题不可避免地限制了这些平台的应用范围。
- en: '![Refer to caption](img/d767be96a2948ded785a2ceab1a7a72f.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d767be96a2948ded785a2ceab1a7a72f.png)'
- en: 'Figure 1: A complex urban scenario for UAV object detection and tracking. For
    simplicity, only bounding boxes and class names for certain objects are drawn
    in the imagery.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：无人机目标检测和跟踪的复杂城市场景。为简单起见，图像中仅绘制了某些对象的边界框和类别名称。
- en: With the development of microelectronic software and hardware, navigation and
    communication technology renewal, and breakthroughs in materials and energy technology,
    unmanned aerial vehicle (UAV) platform already an international research hotspot
    in remote sensing has rapidly emerged. A UAV remote sensing system is a high-tech
    combination of science and technology integrated UAVs, remote sensing, global
    positioning system (GPS) positioning, and inertial measurement unit (IMU) attitude
    determination means. It is a dedicated remote sensing system with the goal of
    obtaining low-altitude high-resolution remote sensing images. Compared with traditional
    platforms, UAV makes up for information loss caused by weather, time, and other
    limitations. In addition, the high mobility of UAVs enables it to flexibly collect
    video data without geographic restriction. These data, either in contents or time,
    are extremely informative, and thus object detection and tracking have entered
    the era of mass UAV [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)], which has
    played an increasingly important role in land cover mapping [[4](#bib.bib4), [5](#bib.bib5)],
    smart agriculture [[6](#bib.bib6), [7](#bib.bib7)], smart city [[8](#bib.bib8)],
    traffic monitoring [[9](#bib.bib9)], and disaster monitoring [[10](#bib.bib10)],
    among other topics.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 随着微电子软件和硬件的发展、导航和通信技术的更新，以及材料和能源技术的突破，无人机（UAV）平台已经迅速成为遥感领域的国际研究热点。无人机遥感系统是一个高科技的综合体，集成了无人机、遥感、全球定位系统（GPS）定位和惯性测量单元（IMU）姿态确定手段。它是一个专用的遥感系统，旨在获取低空高分辨率的遥感图像。与传统平台相比，无人机弥补了天气、时间等限制所造成的信息丢失。此外，无人机的高机动性使其能够在没有地理限制的情况下灵活地收集视频数据。这些数据无论是在内容还是时间上都极具信息量，因此目标检测和跟踪已进入了大规模无人机时代[[1](#bib.bib1),
    [2](#bib.bib2), [3](#bib.bib3)]，在土地覆盖制图[[4](#bib.bib4), [5](#bib.bib5)]、智能农业[[6](#bib.bib6),
    [7](#bib.bib7)]、智慧城市[[8](#bib.bib8)]、交通监测[[9](#bib.bib9)]和灾害监测[[10](#bib.bib10)]等主题中发挥了越来越重要的作用。
- en: 'As one of the fundamental computer vision problems, object detection and tracking
    employ classic, i.e., statistically-based, methods [[11](#bib.bib11), [12](#bib.bib12)].
    However, today’s massive quantities of data impact the performance of these traditional
    methods, which poses a problem for feature dimension explosion, yielding higher
    storage space and time costs. Owing to the emergence of deep neural network (DL)
    techniques [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)], hierarchical
    feature representations with enough sample data can be learned with deep and complex
    networks. Since 2015, the deep neural network has become a mainstream framework
    used for UAV object detection and tracking [[16](#bib.bib16), [17](#bib.bib17)].
    Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") shows an example of object detection and tracking
    in an urban areas with UAV remote sensing. Classic deep neural networks are divided
    into two major categories: two-stage and one-stage networks. Among them, the two-stage
    networks, such as RCNN [[18](#bib.bib18)], Fast RCNN [[19](#bib.bib19)], and Faster
    RCNN [[20](#bib.bib20)], first need to generate a region proposal (RP), and then
    classify and locate candidate regions. A series of work [[21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23)] has demonstrated that a two-stage network is suitable for applications
    with higher detection accuracy. A one-stage network, such as SSD [[24](#bib.bib24)]
    and YOLO [[16](#bib.bib16), [25](#bib.bib25), [26](#bib.bib26)], directly generates
    class probability and coordinate position, and is faster than a two-stage network.
    Similarly, there are some faster light weight networks, such as mobilenet SSD
    [[27](#bib.bib27)], YOLOv3 [[28](#bib.bib28)], ESPnet_v2 [[29](#bib.bib29)], etc.
    Therefore, one-stage and faster light weight networks are the final winners for
    UAV remote sensing practical applications with high-speed requirements. But for
    low-resolution data, it fails to produce good results without preprocessing images
    or modifying the classic neural network structure.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算机视觉的基础问题之一，目标检测与跟踪采用了经典的，即基于统计的方法 [[11](#bib.bib11), [12](#bib.bib12)]。然而，如今海量的数据对这些传统方法的性能产生了影响，导致特征维度爆炸，从而带来更高的存储空间和时间成本。由于深度神经网络（DL）技术的出现
    [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)]，可以通过深层复杂的网络学习到具有足够样本数据的层次特征表示。自2015年以来，深度神经网络已成为用于无人机目标检测和跟踪的主流框架
    [[16](#bib.bib16), [17](#bib.bib17)]。图[1](#S1.F1 "图 1 ‣ 引言 ‣ 基于无人机的目标检测与跟踪的深度学习综述")展示了一个在城市区域使用无人机遥感进行目标检测和跟踪的示例。经典的深度神经网络分为两大类：两阶段网络和一阶段网络。其中，两阶段网络，如RCNN
    [[18](#bib.bib18)]、Fast RCNN [[19](#bib.bib19)]和Faster RCNN [[20](#bib.bib20)]，首先需要生成一个区域提议（RP），然后对候选区域进行分类和定位。一系列工作
    [[21](#bib.bib21), [22](#bib.bib22), [23](#bib.bib23)] 已证明，两阶段网络适用于需要更高检测精度的应用。一阶段网络，如SSD
    [[24](#bib.bib24)] 和YOLO [[16](#bib.bib16), [25](#bib.bib25), [26](#bib.bib26)]，直接生成类别概率和坐标位置，比两阶段网络更快。类似地，还有一些更快的轻量级网络，如mobilenet
    SSD [[27](#bib.bib27)]、YOLOv3 [[28](#bib.bib28)]、ESPnet_v2 [[29](#bib.bib29)]
    等。因此，一阶段和更快的轻量级网络在具有高速要求的无人机遥感实际应用中是最终的赢家。但对于低分辨率数据，如果不对图像进行预处理或修改经典神经网络结构，则无法产生良好的结果。
- en: This paper focuses on UAV with a maximum take-off weight of fewer than 30 kilograms,
    and provides a comprehensive review of deep learning (DL)-based UAV object detection
    and tracking methods by summarizing the latest published work, discussing the
    key issues and difficult problems, and delineating areas of future development.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文聚焦于最大起飞重量低于30公斤的无人机，并通过总结最新发布的工作，讨论关键问题和难点，划定未来发展的领域，提供了基于深度学习（DL）的无人机目标检测和跟踪方法的综合综述。
- en: '![Refer to caption](img/2a24fe06ba77091fc480a8291ddf1ffb.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2a24fe06ba77091fc480a8291ddf1ffb.png)'
- en: 'Figure 2: Partial statistical analysis results of light and small UAVs currently
    in use.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：当前使用的轻小型无人机的部分统计分析结果。
- en: 'The remainder of this paper is organized as follows. Section [II](#S2 "II Related
    Surveys and Brief Statistics ‣ Deep Learning for UAV-based Object Detection and
    Tracking: A Survey") briefly summarizes the statistics of UAV aircraft and related
    publications. Section [VI](#S6 "VI UAV-based Benchmark Dataset ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey") describes the existing
    UAV-based remote sensing datasets. Section [III](#S3 "III Object Detection from
    UAV-borne Images ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey")-[V](#S5 "V Multiple Object Tracking from UAV-borne Video ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey") reviews the existing DL-based
    work closely related to UAV-based object detection and tracking for the three
    sub-branches. Section [VIII](#S8 "VIII Discussion and Conclusion ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey") discusses conclusions.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本文其余部分的组织结构如下。第[II](#S2 "II 相关调查与简要统计 ‣ 基于无人机的目标检测与跟踪的深度学习：综述")节简要总结了无人机飞行器及相关出版物的统计数据。第[VI](#S6
    "VI 基于无人机的基准数据集 ‣ 基于无人机的目标检测与跟踪的深度学习：综述")节描述了现有的基于无人机的遥感数据集。第[III](#S3 "III 无人机图像中的目标检测
    ‣ 基于无人机的目标检测与跟踪的深度学习：综述")至[V](#S5 "V 无人机视频中的多目标跟踪 ‣ 基于无人机的目标检测与跟踪的深度学习：综述")节回顾了与无人机目标检测和跟踪密切相关的现有深度学习工作。第[VIII](#S8
    "VIII 讨论与结论 ‣ 基于无人机的目标检测与跟踪的深度学习：综述")节讨论了结论。
- en: II Related Surveys and Brief Statistics
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 相关调查与简要统计
- en: II-A UAV Aircraft Statistics
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 无人机飞行器统计
- en: 'Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") shows the classification of UAVs in present
    use through statistical analysis. From the perspective of power supply, battery
    power is used more often than fuel power; for the aerodynamic shape, multi-rotor
    is more common than fixed-wing; for the weight of the aircraft, the majority is
    under 30 kilograms, which is considered small light UAV; and the flight time for
    most UAVs is less than 1 hour. The quantitative analysis results show that small
    light UAVs have become the main type used for study and application, and have
    more market weight. In addition, the “Small light UAV remote sensing development
    report” published in 2016 [[30](#bib.bib30)] shows that China has more than 3,000
    professional small light UAVs for remote sensing applications. This type of UAV
    exhibits the following five main characteristics.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图[2](#S1.F2 "图 2 ‣ I 引言 ‣ 基于无人机的目标检测与跟踪的深度学习：综述")通过统计分析展示了当前使用的无人机分类。从电源的角度来看，电池电力使用频率高于燃料电力；从空气动力学形状来看，多旋翼无人机比固定翼无人机更常见；从飞机的重量来看，大多数在30公斤以下，被认为是小型轻量化无人机；大多数无人机的飞行时间少于1小时。定量分析结果显示，小型轻量化无人机已成为研究和应用的主要类型，并且具有更大的市场份额。此外，2016年发布的《小型轻量化无人机遥感发展报告》[[30](#bib.bib30)]显示，中国拥有超过3000架用于遥感应用的专业小型轻量化无人机。这类无人机具有以下五个主要特征。
- en: 1) Long flight time. As new energy technology, energy management technology,
    and lightweight composite material research technology have developed, UAV flight
    time has been continuously extended.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 长时间飞行。随着新能源技术、能源管理技术和轻量化复合材料研究技术的发展，无人机的飞行时间不断延长。
- en: 2) Low comprehensive cost and high technical content. On the one hand, the use
    of low-cost and lightweight materials reduces the production cost of UAV and remote
    sensors. On the other hand, the increase of mass users promotes the mass production
    of components and structural parts, further reducing the production cost of UAV
    and remote sensors.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 低综合成本和高技术含量。一方面，低成本和轻量化材料的使用降低了无人机和遥感器的生产成本。另一方面，大量用户的增加促进了组件和结构部件的批量生产，进一步降低了无人机和遥感器的生产成本。
- en: 3) Small, light-weight, diversified remote sensing cameras. All remote sensing
    loads on small light UAVs are developed to below 30 kilograms, and optical and
    infrared loads are even reduced to less than a half kilogram. In addition, multi-angle
    photography, tilt photography, sensor integration, hyperspectral imaging interference
    [[31](#bib.bib31)], and other technologies have been used in UAV remote sensing.
    Commercial high-end cameras have been widely used for professional aerial missions,
    and popular cameras are used for mass entertainment and general applications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 小型、轻便、多样化的遥感相机。所有小型轻型无人机上的遥感负载都开发到30公斤以下，光学和红外负载甚至减少到半公斤以下。此外，无人机遥感中使用了多角度摄影、倾斜摄影、传感器集成、高光谱成像干扰[[31](#bib.bib31)]等技术。商业高端相机已广泛用于专业空中任务，流行相机则用于大众娱乐和一般应用。
- en: 4) Real-time data transmission. Advances in wireless communication and information
    compression technology have powerfully impelled image resolution with a higher
    data rate and longer transmission distance. Almost no-delay data link transmission
    makes real-time observation possible.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 实时数据传输。无线通信和信息压缩技术的进步极大推动了图像分辨率的提升，实现了更高的数据传输速率和更长的传输距离。几乎无延迟的数据链路传输使得实时观察成为可能。
- en: II-B Challenges
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 挑战
- en: Object detection and tracking tasks in the UAV remote sensing video face many
    challenges, such as image degradation, uneven object intensity, small object size,
    and real-time problems like perspective specificity, background complexity, scale,
    and direction diversity problems in satellite and manned aircraft objects.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机遥感视频中的目标检测和跟踪任务面临许多挑战，如图像退化、目标强度不均、目标尺寸小以及实时问题，如视角特异性、背景复杂性、尺度和方向多样性问题。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Image degradation problem. The load that a mini-UAV platform carries is strictly
    limited in terms of weight, volume, and power. Rapid movement changes in the external
    environment (such as light, cloud, fog, rain, etc.) cause aerial images to be
    fuzzy and noisy, which inevitably leads to image degradation [[32](#bib.bib32)].
    In addition, high-speed flight or camera rotation also increases the complexity
    of object detection. Thus, it is necessary to carry out image pre-processing,
    such as noise reduction, camera distortion correction, etc., to ensure the effectiveness
    of the object detection model.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像退化问题。迷你无人机平台所承载的负载在重量、体积和功率方面都有严格限制。外部环境的快速变化（如光线、云层、雾霾、雨水等）会导致航空图像模糊和噪声，这不可避免地导致图像退化[[32](#bib.bib32)]。此外，高速飞行或相机旋转也增加了目标检测的复杂性。因此，有必要进行图像预处理，如噪声减少、相机畸变校正等，以确保目标检测模型的有效性。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Uneven object intensity problem. The image acquisition equipment of a UAV typically
    uses a large aperture, fixed focal, and wide-angle lens. In addition, flexible
    camera movement results in an uneven density of captured objects. Some of them
    are densely arranged and overlap many times, so that it is easy to repeat detection.
    Some are sparse and unevenly distributed, so that it is prone to missed detection.
    In addition, most objects occupy a small number of pixels, which makes it difficult
    to separate them from their surroundings.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标强度不均问题。无人机的图像采集设备通常使用大光圈、固定焦距和广角镜头。此外，灵活的相机运动导致捕获的目标密度不均。一些目标密集排列并重叠多次，容易重复检测；一些目标稀疏且分布不均，容易漏检。此外，大多数目标占据的像素很少，使其从背景中分离变得困难。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Object size problem. UAV remote sensing images can be acquired at different
    altitudes, yielding photographs containing any size of ground objects. This challenges
    the classical DL-based method. In addition, ground objects in UAV remote sensing
    are primarily shown as images with an area smaller than $32\times 32$ pixels.
    MS COCO dataset[[33](#bib.bib33)] defines small objects due to their less distinct
    features, yielding more false and missed detection targets.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 目标尺寸问题。无人机遥感图像可以在不同的高度上获得，拍摄的地面目标大小各异。这对经典的基于深度学习的方法提出了挑战。此外，无人机遥感中的地面目标主要表现为面积小于
    $32\times 32$ 像素的图像。MS COCO 数据集[[33](#bib.bib33)] 定义了小目标，由于其特征不明显，容易导致更多的误检和漏检目标。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Real-time problem. Object detection or tracking in a video obtained by a drone
    needs to quickly and accurately locate moving ground objects, so real-time processing
    performance is highly essential.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 实时问题。无人机获取的视频中的目标检测或跟踪需要快速准确地定位移动的地面物体，因此实时处理性能至关重要。
- en: II-C Contribution
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 贡献
- en: Up to now, reviews concerning object detection and tracking from airborne and
    spaceborne datasets can be found [[34](#bib.bib34), [35](#bib.bib35)]. For UAV
    data, several representative surveys have been published in the literature, which
    include surveys on the UAV image processing and application [[36](#bib.bib36),
    [37](#bib.bib37)], the UAV system[[38](#bib.bib38)]. However, less attention has
    been paid to the advance of object and tracking techniques both in image and video
    acquired by UAV. Although reviews in [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)] present some DL-based static
    object detection for UAV image and the one in [[45](#bib.bib45)] presents traditional
    object tracking for UAV video, there still lacks a complete survey for object
    and tracking and the most recent advances.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，有关来自空中和太空数据集的目标检测和跟踪的综述可以在 [[34](#bib.bib34), [35](#bib.bib35)] 中找到。对于无人机数据，文献中已经发布了一些具有代表性的综述，包括无人机图像处理和应用的综述
    [[36](#bib.bib36), [37](#bib.bib37)]，无人机系统 [[38](#bib.bib38)]。然而，对于无人机获取的图像和视频中目标检测与跟踪技术的进展关注较少。虽然
    [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)] 中的综述介绍了无人机图像的某些基于深度学习的静态目标检测，[[45](#bib.bib45)] 中的综述则介绍了无人机视频的传统目标跟踪，但仍缺乏对目标检测和跟踪以及最新进展的完整综述。
- en: 'Therefore, it is imperative to provide a comprehensive survey of DL-based object
    detection and tracking for UAV data, focusing on static object detection (SOD),
    video object detection (VID), and multiple object tracking (MOT). In the following
    discussion, we limit this review to DL-based methods based on corresponding publications.
    We hope that this survey will provide readers and practitioners with instructive
    information. Fig. [3](#S2.F3 "Figure 3 ‣ II-C Contribution ‣ II Related Surveys
    and Brief Statistics ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows the typical DL-based learning mode for these three research topics.
    For the choice of the DL method, SOD object detection focuses on detection head
    design to assign positive and negative samples, such as RPN+ ROI Pooling in Faster
    RCNN, detection outputs is classification and bounding box. VID and MOT are about
    UAV video data and the difference between them is how to use temporal information.
    The former focuses on modifying the missed detection results of the current frame
    by using temporal context in adjacent frames, while the latter focuses on predicting
    the trajectory in the next frame to obtain the moving state of objects.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，必须对基于深度学习的无人机数据目标检测与跟踪进行全面调查，重点关注静态目标检测（SOD）、视频目标检测（VID）和多目标跟踪（MOT）。在以下讨论中，我们将此综述限制在基于相应出版物的深度学习方法。我们希望这项调查能够为读者和从业者提供有益的信息。图
    [3](#S2.F3 "图 3 ‣ II-C 贡献 ‣ II 相关综述和简要统计 ‣ 基于无人机的目标检测与跟踪的深度学习：综述") 显示了这三个研究主题的典型深度学习学习模式。对于深度学习方法的选择，SOD
    目标检测侧重于检测头设计，以分配正负样本，例如 Faster RCNN 中的 RPN+ ROI Pooling，检测输出包括分类和边界框。VID 和 MOT
    处理无人机视频数据，它们之间的区别在于如何利用时间信息。前者侧重于通过使用相邻帧中的时间上下文来修改当前帧的漏检结果，而后者则侧重于预测下一帧中的轨迹，以获得物体的运动状态。
- en: '![Refer to caption](img/18b2a6eb332cf4efd7ed56c6a7fcf2af.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/18b2a6eb332cf4efd7ed56c6a7fcf2af.png)'
- en: 'Figure 3: An illustration of three UAV topics based on deep learning methods.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：基于深度学习方法的三种无人机研究主题示意图。
- en: '![Refer to caption](img/e2d31f13fa54c5d49017c6f7b0ccd227.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e2d31f13fa54c5d49017c6f7b0ccd227.png)'
- en: 'Figure 4: The development of typical methods for UAV static object detection.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：无人机静态目标检测的典型方法的发展。
- en: '![Refer to caption](img/8ed23fe354195dd7419338724134efd8.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8ed23fe354195dd7419338724134efd8.png)'
- en: 'Figure 5: Deep learning-based scale diversity and direction diversity strategies.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于深度学习的尺度多样性和方向多样性策略。
- en: III Object Detection from UAV-borne Images
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 无人机搭载图像中的目标检测
- en: 'Although deep learning-based object detection methods for UAV remote sensing
    images are mainly borrowed from traditional digital images in the computer vision
    community, the limitation of small UAV platform and imaging acquisition condition
    inevitably causes problems of particularity perspective, complex background, scale
    and direction diversity, and issues related to small sizes. In the following,
    some solutions based on DL methods have been summarized according to recent publications.
    Fig. [6](#S3.F6 "Figure 6 ‣ III-F Object Detection on Others ‣ III Object Detection
    from UAV-borne Images ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows the development of typical methods for SOD. Among them, some
    methods specially designed for UAV data are listed in Table. [I](#S3.T1 "TABLE
    I ‣ III-B Object Detection on Scale Diversity ‣ III Object Detection from UAV-borne
    Images ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey").
    Other methods that can solve the above problem, but not specifically for UAV data,
    are briefly introduced in the text. The remainder of this section introduces DL-based
    SOD methods to solve five representative problems, including data processing,
    scale diversity, small objects, direction diversity and detection speed.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然基于深度学习的无人机遥感图像目标检测方法主要来自计算机视觉领域传统数字图像，但小型无人机平台和成像获取条件的限制不可避免地引起了特殊视角、复杂背景、尺度和方向多样性以及与小尺寸相关的问题。接下来，根据最近的出版物总结了一些基于深度学习方法的解决方案。图[6](#S3.F6
    "图6 ‣ III-F 其他目标检测方法 ‣ III 无人机载图像目标检测 ‣ 无人机基于深度学习的目标检测与跟踪：一项调查")显示了SOD的典型方法的发展。其中，一些专门针对无人机数据设计的方法列在表[I](#S3.T1
    "表I ‣ III-B 尺度多样性目标检测 ‣ III 无人机载图像目标检测 ‣ 无人机基于深度学习的目标检测与跟踪：一项调查")中。其他可以解决上述问题的方法，但不专门针对无人机数据的方法，在文本中简要介绍。本节其余部分介绍了基于DL的SOD方法来解决五个典型问题，包括数据处理、尺度多样性、小对象、方向多样性和检测速度。
- en: III-A Data Processing
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 数据处理
- en: Two types of data processing are typically applied preprocessing before data
    acquisition and postprocessing after data acquisition.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在数据获取前和数据获取后应用两种类型的数据处理预处理。
- en: The latter is more commonly used in DL-based techniques. Most of the existing
    UAV-based remote sensing works present an experimental dataset and appropriate
    data processing techniques [[46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)],
    and all of them carry out image postprocessing procedure after image acquisition,
    such as increasing the number of training samples, enlarging the diversity of
    sample size and direction, and expanding the illumination change of samples. However,
    their effectiveness is variable.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 后者在基于DL的技术中更常见。大多数现有的基于无人机遥感的研究作品提供了实验数据集和适当的数据处理技术[[46](#bib.bib46), [47](#bib.bib47),
    [48](#bib.bib48)]，并且它们都在图像获取后进行图像后处理程序，例如增加训练样本数量，扩大样本大小和方向的多样性，以及扩展样本的照明变化。然而，它们的有效性是不确定的。
- en: Due to the limitation of UAV flight altitude and load, there is inevitably ground
    object overlapping, coverage, and displacement. Xia et al. [[49](#bib.bib49)]
    took optical cameras as an example, focusing on various difficulties and problems
    in the process of UAV remote sensing data acquisition, and systematically discussed
    the key techniques of data processing.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无人机飞行高度和负载的限制，不可避免地存在地面物体重叠、覆盖和位移。夏等人[[49](#bib.bib49)]以光学相机为例，关注了无人机遥感数据获取过程中的各种困难和问题，并系统地讨论了数据处理的关键技术。
- en: III-B Object Detection on Scale Diversity
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 尺度多样性目标检测
- en: 'UAV remote sensing images can be acquired at different altitudes and ground
    objects can be any size, even for intraclass. Therefore, solutions to scale diversity
    are cross-referenced in this review. There are two main approaches to solving
    this problem through deep learning, as illustrated in Fig. [5](#S2.F5 "Figure
    5 ‣ II-C Contribution ‣ II Related Surveys and Brief Statistics ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey")(a). The most commonly
    used is the multi-scale feature map [[23](#bib.bib23)], which is the output of
    multiple filters on multiple feature maps (MFM) or multiple filters on a single
    feature map (SFM) [[50](#bib.bib50), [51](#bib.bib51), [24](#bib.bib24), [52](#bib.bib52),
    [22](#bib.bib22), [53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)]. The other is a dilated/deformable
    convolution kernel [[60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63)].
    It points out that systematic expansion supports the exponential expansion of
    the receptive field without loss of resolution or coverage. Chen et al. [[60](#bib.bib60)]
    introduced an extended convolution filter to obtain the ResNeXt-d combination
    structure on the basis of ResNeXt[[64](#bib.bib64)] architecture, which can expand
    the receptive field.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机遥感图像可以在不同的高度获取，地面物体可以是任何大小，甚至同类物体也是如此。因此，本综述中交叉引用了解决尺度多样性的问题。通过深度学习解决这个问题主要有两种方法，如图[5](#S2.F5
    "图 5 ‣ II-C 贡献 ‣ II 相关调查与简要统计 ‣ 基于无人机的物体检测与跟踪的深度学习：综述")(a)所示。最常用的是多尺度特征图[[23](#bib.bib23)]，这是多个特征图（MFM）上多个滤波器的输出，或者单个特征图（SFM）上的多个滤波器[[50](#bib.bib50),
    [51](#bib.bib51), [24](#bib.bib24), [52](#bib.bib52), [22](#bib.bib22), [53](#bib.bib53),
    [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56), [57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59)]。另一种方法是膨胀/变形卷积核[[60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62),
    [63](#bib.bib63)]。它指出系统扩展支持感受野的指数扩展，而不会丧失分辨率或覆盖范围。Chen等人[[60](#bib.bib60)]在ResNeXt[[64](#bib.bib64)]架构的基础上引入了扩展卷积滤波器，以获得ResNeXt-d组合结构，这可以扩展感受野。
- en: 'TABLE I: DL-based Static Object Detection Approaches for UAV exclusive'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 基于深度学习的无人机静态物体检测方法'
- en: '| Static Object Detection |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 静态物体检测 |'
- en: '| --- |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Reference | Challenge | Dataset Used | Journal/Conf. | Year | Code.link |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 挑战 | 使用的数据集 | 期刊/会议 | 年份 | 代码链接 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| RRNet[[53](#bib.bib53)] | Small objects, scale variation | VisDrone | ICCV
    Workshops | 2019 | [https://github.com/ouc-ocean-group/RRNet](https://github.com/ouc-ocean-group/RRNet)
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| RRNet[[53](#bib.bib53)] | 小物体，尺度变化 | VisDrone | ICCV Workshops | 2019 | [https://github.com/ouc-ocean-group/RRNet](https://github.com/ouc-ocean-group/RRNet)
    |'
- en: '| SlimYOLOv3[[65](#bib.bib65)] | Real-time | Visdrone | ICCV | 2019 | [https://github.com/PengyiZhang/SlimYOLOv3.](https://github.com/PengyiZhang/SlimYOLOv3.)
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| SlimYOLOv3[[65](#bib.bib65)] | 实时 | Visdrone | ICCV | 2019 | [https://github.com/PengyiZhang/SlimYOLOv3.](https://github.com/PengyiZhang/SlimYOLOv3.)
    |'
- en: '| Zhang et al[[62](#bib.bib62)] | Small objects | VisDrone | ICCV Workshops
    | 2019 | - |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al[[62](#bib.bib62)] | 小物体 | VisDrone | ICCV Workshops | 2019 |
    - |'
- en: '| FS-SSD[[66](#bib.bib66)] | Small objects | Stanford Drone | IEEE TCSVT |
    2019 | - |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| FS-SSD[[66](#bib.bib66)] | 小物体 | Stanford Drone | IEEE TCSVT | 2019 | - |'
- en: '| SAMFR[[67](#bib.bib67)] | Scale variation | Visdrone | ICCV Workshop | 2019
    | - |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| SAMFR[[67](#bib.bib67)] | 尺度变化 | Visdrone | ICCV Workshop | 2019 | - |'
- en: '| ClusDet[[57](#bib.bib57)] | Scale variation | VisDrone, UAVDT | ICCV | 2019
    | [https://github.com/fyangneil](https://github.com/fyangneil) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| ClusDet[[57](#bib.bib57)] | 尺度变化 | VisDrone, UAVDT | ICCV | 2019 | [https://github.com/fyangneil](https://github.com/fyangneil)
    |'
- en: '| CenterNet[[58](#bib.bib58)] | Scale variation | VisDrone | - | 2019 | - |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| CenterNet[[58](#bib.bib58)] | 尺度变化 | VisDrone | - | 2019 | - |'
- en: '| Yang et al[[59](#bib.bib59)] | Scale variation | Stanford Drone | IEEE Access
    | 2019 | - |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al[[59](#bib.bib59)] | 尺度变化 | Stanford Drone | IEEE Access | 2019
    | - |'
- en: '| Wu et al[[68](#bib.bib68)] | Real-time | CARPK | DDCLS | 2019 | - |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al[[68](#bib.bib68)] | 实时 | CARPK | DDCLS | 2019 | - |'
- en: '| NDFT[[69](#bib.bib69)] | UAV-specific nuisances | VisDrone, UAVDT | ICCV
    | 2019 | [https://github.com/VITA-Group/UAV-NDFT](https://github.com/VITA-Group/UAV-NDFT)
    |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| NDFT[[69](#bib.bib69)] | 无人机特有干扰 | VisDrone, UAVDT | ICCV | 2019 | [https://github.com/VITA-Group/UAV-NDFT](https://github.com/VITA-Group/UAV-NDFT)
    |'
- en: '| MSOA-Net[[56](#bib.bib56)] | Scale variation | UVSD | Remote Sens. | 2020
    | - |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| MSOA-Net[[56](#bib.bib56)] | 尺度变化 | UVSD | Remote Sens. | 2020 | - |'
- en: '| GDF-Net[[61](#bib.bib61)] | Scale variation | VisDrone, UAVDT | Remote Sens.
    | 2020 | - |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| GDF-Net[[61](#bib.bib61)] | 尺度变化 | VisDrone, UAVDT | Remote Sens. | 2020
    | - |'
- en: '| HRDNet[[70](#bib.bib70)] | Scale variation | VisDrone | CVPR | 2020 | - |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| HRDNet[[70](#bib.bib70)] | 尺度变化 | VisDrone | CVPR | 2020 | - |'
- en: '| D-A-FS SSD[[63](#bib.bib63)][[63](#bib.bib63)] | Scale variation | VisDrone
    | ICIT | 2020 | - |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| D-A-FS SSD[[63](#bib.bib63)][[63](#bib.bib63)] | 尺度变化 | VisDrone | ICIT |
    2020 | - |'
- en: '| UAV-YOLO[[16](#bib.bib16)] | Small scale | UAV123, Own | Sensors | 2020 |
    - |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| UAV-YOLO[[16](#bib.bib16)] | 小尺度 | UAV123, 自有 | Sensors | 2020 | - |'
- en: '| SyNet[[71](#bib.bib71)] | Class imbalance | VisDrone | ICPR | 2020 | [https://github.com/mertalbaba/SyNet](https://github.com/mertalbaba/SyNet)
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| SyNet[[71](#bib.bib71)] | 类别不平衡 | VisDrone | ICPR | 2020 | [https://github.com/mertalbaba/SyNet](https://github.com/mertalbaba/SyNet)
    |'
- en: '| ComNet[[72](#bib.bib72)] | Blurred edges, low contrast | Own | IEEE TGRS
    | 2020 | - |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| ComNet[[72](#bib.bib72)] | 模糊边缘，低对比度 | 自有 | IEEE TGRS | 2020 | - |'
- en: '| MPFPN[[73](#bib.bib73)] | Small objects | VisDrone | IEEE Access | 2020 |
    - |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| MPFPN[[73](#bib.bib73)] | 小物体 | VisDrone | IEEE Access | 2020 | - |'
- en: '| D2Det[[74](#bib.bib74)], | Localization, classification | UAVDT | CVPR |
    2020 | [https://github.com/JialeCao001/D2Det.](https://github.com/JialeCao001/D2Det.)
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| D2Det[[74](#bib.bib74)] | 定位，分类 | UAVDT | CVPR | 2020 | [https://github.com/JialeCao001/D2Det.](https://github.com/JialeCao001/D2Det.)
    |'
- en: '| DAGN[[75](#bib.bib75)] | Small objects | VEDAI | IEEE GRSL | 2020 | - |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| DAGN[[75](#bib.bib75)] | 小物体 | VEDAI | IEEE GRSL | 2020 | - |'
- en: '| GANet[[76](#bib.bib76)] | Small objects | UAVDT, CARPK, PUCPR+ | MM | 2020
    | [https://isrc.iscas.ac.cn/gitlab/research/ganet](https://isrc.iscas.ac.cn/gitlab/research/ganet)
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| GANet[[76](#bib.bib76)] | 小物体 | UAVDT, CARPK, PUCPR+ | MM | 2020 | [https://isrc.iscas.ac.cn/gitlab/research/ganet](https://isrc.iscas.ac.cn/gitlab/research/ganet)
    |'
- en: '| DAN[[77](#bib.bib77)] | Dense distribution, small object | Visdrone-det |
    NCC | 2020 | - |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| DAN[[77](#bib.bib77)] | 密集分布，小物体 | Visdrone-det | NCC | 2020 | - |'
- en: '| Zhang et al[[78](#bib.bib78)] | Real-time | Stanford drone | Neurocomputing
    | 2020 | - |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al[[78](#bib.bib78)] | 实时 | Stanford drone | Neurocomputing | 2020
    | - |'
- en: '| DNOD Eifficientdet[[79](#bib.bib79)] | Dense objects,small objects | Visdrone-DET,
    UAVDT | Neurocomputing | 2021 | - |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| DNOD Eifficientdet[[79](#bib.bib79)] | 密集物体，小物体 | Visdrone-DET, UAVDT | Neurocomputing
    | 2021 | - |'
- en: '| ECascade-RCNN[[54](#bib.bib54)] | Scale variation | VisDrone | ICARA | 2021
    | - |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| ECascade-RCNN[[54](#bib.bib54)] | 尺度变化 | VisDrone | ICARA | 2021 | - |'
- en: '| Cas_RCNN+FPN[[80](#bib.bib80)] | Cost | Visdrone | Transp. Res. Rec. | 2021
    | - |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| Cas_RCNN+FPN[[80](#bib.bib80)] | 成本 | Visdrone | Transp. Res. Rec. | 2021
    | - |'
- en: '| DSYolov3[[55](#bib.bib55)] | Scale variation | VisDrone, UAVDT | J. Vis.
    Commun. Image Represent. | 2021 | - |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| DSYolov3[[55](#bib.bib55)] | 尺度变化 | VisDrone, UAVDT | J. Vis. Commun. Image
    Represent. | 2021 | - |'
- en: '| DSHNet[[81](#bib.bib81)] | Long-tail distribution | VisDrone, UAVDT | WACV
    | 2021 | [https://github.com/we1pingyu/DSHNet](https://github.com/we1pingyu/DSHNet)
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| DSHNet[[81](#bib.bib81)] | 长尾分布 | VisDrone, UAVDT | WACV | 2021 | [https://github.com/we1pingyu/DSHNet](https://github.com/we1pingyu/DSHNet)
    |'
- en: III-C Object Detection on Small Objects
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 小物体检测
- en: The UAV flying altitude inevitably causes most objects to be shown in scale
    diversity, small object size and dense arrangement, resulting in less feature
    information that can be extracted. Many work deal with the small object detection
    problem through the same network designing for scale diversity, including RRNet
    [[53](#bib.bib53)], HRDNet[[70](#bib.bib70)], Cascade network [[62](#bib.bib62)],
    UAV-YOLO [[16](#bib.bib16)], MPFPN [[73](#bib.bib73)], depthwise-separable attention-guided
    network (DAGN) [[75](#bib.bib75)], GANet [[76](#bib.bib76)], and FS-SSD [[66](#bib.bib66)],
    ResNeXt-d [[60](#bib.bib60)], et al. In these methods, accurate feature information
    learned by small objects is highly important. In addition, some new networks are
    based on YOLOv4 or Eifficientdet-D7 networks, e.g., DNOD [[79](#bib.bib79)], which
    are developed to improve the detection speed.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机飞行高度不可避免地导致大多数物体在尺度上存在多样性，小物体尺寸和密集排列，结果是可以提取的特征信息较少。许多工作通过为尺度多样性设计相同的网络来解决小物体检测问题，包括
    RRNet [[53](#bib.bib53)]，HRDNet[[70](#bib.bib70)]，Cascade 网络 [[62](#bib.bib62)]，UAV-YOLO
    [[16](#bib.bib16)]，MPFPN [[73](#bib.bib73)]，深度可分离注意力引导网络 (DAGN) [[75](#bib.bib75)]，GANet
    [[76](#bib.bib76)] 和 FS-SSD [[66](#bib.bib66)]，ResNeXt-d [[60](#bib.bib60)] 等。在这些方法中，小物体学习到的准确特征信息非常重要。此外，一些新网络基于
    YOLOv4 或 Eifficientdet-D7 网络，例如 DNOD [[79](#bib.bib79)]，旨在提高检测速度。
- en: To further improve the distinguish ability of small objects, Li et al. [[82](#bib.bib82)]
    proposed a perceptual GAN to generate a super-resolved representation of small
    objects. This method uses the structural correlativity of large and small objects
    to enhance the representation of small objects and give them a similar expression
    to large objects. Hu et al. [[83](#bib.bib83)] found that the structure of small
    objects after pooling was typically distorted, and proposed a new context-aware
    region of interest (ROI) pooling method. Chen et al. [[60](#bib.bib60)] proposed
    an ResNeXt-d combination structure to enhance the perception of small size objects.
    There are other methods, including changing the anchor information, or cropping
    multiple subset tiles from the original high-resolution images, to improve the
    detection performance of small and dense object. Jadhav et al. [[77](#bib.bib77)]
    modified the anchor scale and Tang et al. [[84](#bib.bib84)] designed a coarse
    anchor-free detector (CPEN) to address dense small object detection. In [[85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87)], the authors proposed effective solutions
    to small object detection from high-resolution images by cropping multiple subset
    tiles from the original high-resolution images, and learning them through use
    of a CNN network without degrading the resolution.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步提高对小物体的区分能力，Li 等人 [[82](#bib.bib82)] 提出了一个感知 GAN 以生成小物体的超分辨率表示。这种方法利用大物体和小物体的结构相关性来增强小物体的表现，并使其具有类似于大物体的表达。Hu
    等人 [[83](#bib.bib83)] 发现小物体在池化后的结构通常会失真，并提出了一种新的上下文感知感兴趣区域（ROI）池化方法。Chen 等人 [[60](#bib.bib60)]
    提出了一个 ResNeXt-d 组合结构来增强对小尺寸物体的感知。还有其他方法，包括改变锚点信息或从原始高分辨率图像中裁剪多个子集块，以提高对小物体和密集物体的检测性能。Jadhav
    等人 [[77](#bib.bib77)] 修改了锚点尺度，而 Tang 等人 [[84](#bib.bib84)] 设计了一个粗略的无锚点检测器（CPEN）来解决密集小物体检测的问题。在
    [[85](#bib.bib85), [86](#bib.bib86), [87](#bib.bib87)] 中，作者提出了从高分辨率图像中裁剪多个子集块并通过使用
    CNN 网络进行学习，以有效解决小物体检测的问题，而不会降低分辨率。
- en: Alternately, the flexible movement of the camera results in an uneven density
    of captured objects. Tightly packed objects in an image, especially smaller size
    ground objects, inevitably overlap. Mekhalfi et al. [[88](#bib.bib88)] introduced
    Capsnets to model the relationship between objects.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，相机的灵活运动导致捕获物体的密度不均匀。图像中紧密排列的物体，尤其是较小的地面物体，难免会重叠。Mekhalfi 等人 [[88](#bib.bib88)]
    引入了 Capsnets 来建模物体之间的关系。
- en: III-D Object Detection on Direction Diversity
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 方向多样性的物体检测
- en: 'Objection direction from an optical remote sensing image is related to its
    actual parking location. The classic CNNs, which benefit from using a rectangular
    convolution kernel, are sensitive to object direction. Fig. [5](#S2.F5 "Figure
    5 ‣ II-C Contribution ‣ II Related Surveys and Brief Statistics ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey")(b) shows four commonly
    used solutions based on deep learning.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '光学遥感图像中的物体方向与其实际停车位置相关。经典的 CNN 由于使用了矩形卷积核，对物体方向非常敏感。图 [5](#S2.F5 "Figure 5
    ‣ II-C Contribution ‣ II Related Surveys and Brief Statistics ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey")(b) 展示了基于深度学习的四种常用解决方案。'
- en: The simplest and most common solution is data augmentation, which can make CNNs
    rotation-invariant though rotation transformation of different angles to extend
    the training set [[89](#bib.bib89), [90](#bib.bib90), [22](#bib.bib22)]. Cheng
    et al. [[89](#bib.bib89)] added regularization constraints on the basis of existing
    CNN architecture to build a rotation-invariant CNN (RICNN). With further researches,
    Fisher discriminative CNN related rotation-invariant network, called RIFD-CNN,
    have been proposed to further boost object detection performance [[91](#bib.bib91),
    [92](#bib.bib92)]. Laptev et al. [[90](#bib.bib90)] added a rotation-invariant
    pool operator to the penultimate layer of output. The shortcoming of data augmentation
    is the increased cost of network training and the risk of over fitting.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单且最常见的解决方案是数据增强，它可以通过旋转不同角度来扩展训练集，从而使 CNN 对旋转变换具有不变性 [[89](#bib.bib89), [90](#bib.bib90),
    [22](#bib.bib22)]。Cheng 等人 [[89](#bib.bib89)] 在现有的 CNN 架构基础上添加了正则化约束，以构建一个旋转不变
    CNN（RICNN）。随着进一步的研究，提出了与旋转不变网络相关的 Fisher 判别 CNN，称为 RIFD-CNN，以进一步提高物体检测性能 [[91](#bib.bib91),
    [92](#bib.bib92)]。Laptev 等人 [[90](#bib.bib90)] 在输出的倒数第二层添加了一个旋转不变池化操作符。数据增强的缺点是增加了网络训练的成本和过拟合的风险。
- en: Some work directly used additional network modules such as oriented proposal
    boxes to achieve object detection [[93](#bib.bib93), [94](#bib.bib94)], or upgraded
    the general convolutional filter to a directional channel filter to achieve rotation
    invariant of texture [[95](#bib.bib95)]. The region proposal network (RPN) [[96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98)] added to the anchor boxed with multiple angles
    in order to cover the oriented object. Additionally, inspired by text detection
    methods [[99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102)],
    Xia et al. [[103](#bib.bib103)] designed a direction insensitive FR-O network
    by adding a direction box detection sub-network to Faster RCNN. Li et al. [[104](#bib.bib104)]
    proposed RADet to acquire a rotation bounding box with a shape mask. However,
    the drawback of additional network modules is that the transform parameter estimation
    is non-adaptive.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作直接使用了额外的网络模块，例如定向建议框，以实现目标检测[[93](#bib.bib93), [94](#bib.bib94)]，或将普通卷积滤波器升级为方向通道滤波器，以实现纹理的旋转不变性[[95](#bib.bib95)]。区域提议网络（RPN）[[96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98)] 被添加到具有多个角度的锚框中，以覆盖定向对象。此外，受到文本检测方法的启发[[99](#bib.bib99),
    [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102)]，Xia等人[[103](#bib.bib103)]
    设计了一个方向不敏感的FR-O网络，通过向Faster RCNN中添加方向框检测子网络实现。Li等人[[104](#bib.bib104)] 提出了RADet，以获取带有形状掩膜的旋转边界框。然而，额外网络模块的缺点在于变换参数估计是不自适应的。
- en: Approaches like, Oriented Response Networks (ORNs) [[105](#bib.bib105)], Polar
    Transformer Network (PTN) [[106](#bib.bib106)], and Equivariant Transformer Networks
    (ETNs) [[107](#bib.bib107)], which were proposed for object detection from natural
    scenes, also provided a qualitative or qualitative analysis of rotation invariant
    features. On the basis of these techniques, Zhou et al. [[108](#bib.bib108)] developed
    a rotated feature network (RFN) using encoder–encoder architecture for object
    detection in remote-sensing images. It is worth mentioning that some rotation-invariant
    methods based on theoretical analysis can cover the intrinsic properties of rotations
    [[109](#bib.bib109), [110](#bib.bib110)] to extract real rotation- invariant features.
    Up to now, these methods have have not been widely used in deep learning.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 像定向响应网络（ORNs）[[105](#bib.bib105)]、极坐标变换网络（PTN）[[106](#bib.bib106)] 和等变变换网络（ETNs）[[107](#bib.bib107)]
    这些为自然场景目标检测提出的方法，也提供了旋转不变特征的定性或定量分析。在这些技术的基础上，Zhou等人[[108](#bib.bib108)] 开发了一个旋转特征网络（RFN），使用编码器–编码器架构进行遥感图像的目标检测。值得一提的是，一些基于理论分析的旋转不变方法可以涵盖旋转的内在特性[[109](#bib.bib109),
    [110](#bib.bib110)]，以提取真正的旋转不变特征。迄今为止，这些方法尚未在深度学习中得到广泛应用。
- en: III-E Object Detection on Detection Speed
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 检测速度上的目标检测
- en: Limited by flight stability and the load capacity of a micro-mini UAV, the altitude
    of an airborne remote sensing sensor needs to be adjusted quickly and accurately
    in real time so that ground objects are always in the monitoring field of vision.
    Meanwhile, rapid processing and analysis of high-quality remote sensing images
    obtained by a UAV system in real time is the key for miniature UAV remote sensing.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 受限于飞行稳定性和微型无人机的负载能力，机载遥感传感器的高度需要实时快速而准确地调整，以使地面物体始终处于监测视野中。同时，实时快速处理和分析无人机系统获得的高质量遥感图像是微型无人机遥感的关键。
- en: When considering all the deep learning methods, the most direct way is to choose
    the right platform, including an ARM, mobile, and embedded platform, or to trim
    up the classic network architecture to minimize unnecessary channels in the convolutional
    layer. In [[85](#bib.bib85), [86](#bib.bib86), [68](#bib.bib68)], the authors
    adopted a YOLO and even a tiny-YOLO network to achieve real-time object detection.
    Zhang et al. [[65](#bib.bib65)] trimmed the update YOLOv3, and proposed slimYOLOv3,
    which balanced the number of parameters, memory usage, and inference time to achieve
    real-time object detection. [[87](#bib.bib87), [78](#bib.bib78)] modified the
    feature resolution of the lightweight Pelee network [[111](#bib.bib111)] to meet
    real-time needs. Due to the efficiency and power of YOLOv4, many object detection
    models [[112](#bib.bib112), [44](#bib.bib44)] are based on this network. Ammar
    et al. [[44](#bib.bib44)] used YOLOv3 and newly released YOLOv4 to detect vehicles
    with inference processing speed from 12 fps for $608\times 608$ up to 23 fps for
    $320\times 320$. Furthermore, Wang et al. [[113](#bib.bib113)] designed a Strip
    Bottleneck with YOLO network (SPB-YOLO) based on YOLOv5 for engineering application.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有深度学习方法，最直接的方法是选择合适的平台，包括ARM、移动和嵌入式平台，或对经典网络架构进行修剪，以减少卷积层中的不必要通道。在[[85](#bib.bib85)、[86](#bib.bib86)、[68](#bib.bib68)]中，作者采用了YOLO甚至tiny-YOLO网络来实现实时目标检测。Zhang等人[[65](#bib.bib65)]修剪了更新的YOLOv3，并提出了slimYOLOv3，平衡了参数数量、内存使用和推理时间，以实现实时目标检测。[[87](#bib.bib87)、[78](#bib.bib78)]修改了轻量级Pelee网络[[111](#bib.bib111)]的特征分辨率，以满足实时需求。由于YOLOv4的效率和性能，许多目标检测模型[[112](#bib.bib112)、[44](#bib.bib44)]基于该网络。Ammar等人[[44](#bib.bib44)]使用YOLOv3和新发布的YOLOv4来检测车辆，推理处理速度从$608\times
    608$的12 fps提升到$320\times 320$的23 fps。此外，Wang等人[[113](#bib.bib113)]设计了基于YOLOv5的Strip
    Bottleneck YOLO网络（SPB-YOLO）用于工程应用。
- en: 'In addition, real-time object detection from images is also a necessary condition
    for detection-based object detection and object tracking from videos, which will
    be discussed in Section [IV](#S4 "IV Object Detection from UAV-borne Video ‣ Deep
    Learning for UAV-based Object Detection and Tracking: A Survey") and [V](#S5 "V
    Multiple Object Tracking from UAV-borne Video ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey"), respectively.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，从图像中实时目标检测也是基于检测的目标检测和视频中的目标跟踪的必要条件，这将在第[IV](#S4 "IV UAV视频目标检测 ‣ 基于深度学习的UAV目标检测和跟踪：综述")和第[V](#S5
    "V UAV视频中的多目标跟踪 ‣ 基于深度学习的UAV目标检测和跟踪：综述")节中分别讨论。
- en: III-F Object Detection on Others
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-F 其他对象检测
- en: Besides the aforementioned main challenges, other problems in object detection
    in UAV images are addressed, such as Nuisance Disentangled Feature Transform (NDFT)
    [[69](#bib.bib69)] for a large number of fine-grained domains, D2Det [[74](#bib.bib74)]
    for precise localization and accurate classification, combinational neural network
    (ComNet) [[72](#bib.bib72)] for blurred edges and low contrast, an ensemble network
    (SyNet) [[71](#bib.bib71)] for class imbalance problem and the scaling problem,
    and Dual Sampler and Head detection Network (DSHNet) for long-tail distribution.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述主要挑战外，UAV图像中的目标检测还解决了其他问题，例如用于大量细粒度领域的Nuisance Disentangled Feature Transform
    (NDFT) [[69](#bib.bib69)]、用于精确定位和准确分类的D2Det [[74](#bib.bib74)]、用于模糊边缘和低对比度的组合神经网络（ComNet）
    [[72](#bib.bib72)]、用于类别不平衡问题和缩放问题的集成网络（SyNet） [[71](#bib.bib71)]，以及用于长尾分布的Dual
    Sampler and Head detection Network (DSHNet)。
- en: '![Refer to caption](img/64a45bb20ef19ae876c31a194b77a4fd.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/64a45bb20ef19ae876c31a194b77a4fd.png)'
- en: 'Figure 6: The development of typical methods for UAV object detection from
    video.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：UAV对象检测的典型方法发展。
- en: 'TABLE II: DL-based Video Object Detection Approaches for UAV exclusive'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表II：基于深度学习的UAV专用视频目标检测方法
- en: '| Video Object Detection |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 视频目标检测 |'
- en: '| --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Reference | Challenge | Dataset Used | Journal/Conf. | Year | Code.link |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 挑战 | 使用数据集 | 期刊/会议 | 年份 | 代码链接 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| STCA[[114](#bib.bib114)] | Defocus, motion blur, occlusion | VisDrone-VID
    | ICCV Workshop | 2019 | - |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| STCA[[114](#bib.bib114)] | 虚焦、运动模糊、遮挡 | VisDrone-VID | ICCV Workshop | 2019
    | - |'
- en: '| SCNN[[115](#bib.bib115)] | Temporal and contextual correlation | DAC | AAAI
    | 2019 | - |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| SCNN[[115](#bib.bib115)] | 时序和上下文关联 | DAC | AAAI | 2019 | - |'
- en: '| Nousi et al.[[116](#bib.bib116)] | Real-time | own-recorded | RCAR | 2019
    | - |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Nousi等人[[116](#bib.bib116)] | 实时 | 自录 | RCAR | 2019 | - |'
- en: '| Abughalieh et al[[117](#bib.bib117)] | Varying resolutions | Own | Multimed.
    Tools. Appl. | 2019 | - |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| Abughalieh et al[[117](#bib.bib117)] | 分辨率变化 | 自有 | 多媒体工具与应用 | 2019 | - |'
- en: '| Zhang et al[[118](#bib.bib118)] | Appearance deterioration, occlusion, motion
    blur | VisDrone-VID | MIPR | 2020 | - |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al[[118](#bib.bib118)] | 外观退化、遮挡、运动模糊 | VisDrone-VID | MIPR | 2020
    | - |'
- en: '| MOR-UAVNet[[119](#bib.bib119)] | Moving object | MOR-UAV | MM | 2020 | [https://visionintelligence.github.io/Datasets.html](https://visionintelligence.github.io/Datasets.html)
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| MOR-UAVNet[[119](#bib.bib119)] | 移动物体 | MOR-UAV | MM | 2020 | [https://visionintelligence.github.io/Datasets.html](https://visionintelligence.github.io/Datasets.html)
    |'
- en: '| TDFA[[120](#bib.bib120)] | Small-scale | Okutama, VisDrone-VID | Multidim
    Syst Sign P | 2021 | - |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| TDFA[[120](#bib.bib120)] | 小规模 | 奥多玛，VisDrone-VID | 多维系统信号处理 | 2021 | - |'
- en: '| STDnet-ST[[121](#bib.bib121)] | Small object | USC-GRAD-STDdb,UAVDT,VisDrone-VID
    | PR | 2021 | - |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| STDnet-ST[[121](#bib.bib121)] | 小物体 | USC-GRAD-STDdb, UAVDT, VisDrone-VID
    | PR | 2021 | - |'
- en: IV Object Detection from UAV-borne Video
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 无人机视频中的目标检测
- en: 'Video object detection (VID) becomes a hot topic after ImageNet VID challenge
    2015\. It is widely used on UAV data until 2017, and also brings some new challenges,
    e.g., camera change and motion blur in a drone platforms. In the following, some
    solutions based on DL methods are summarized according to recent publications.
    Fig. [6](#S3.F6 "Figure 6 ‣ III-F Object Detection on Others ‣ III Object Detection
    from UAV-borne Images ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows the development of typical methods for VID. Among them, methods
    specially designed for UAV data are listed in Table [II](#S3.T2 "TABLE II ‣ III-F
    Object Detection on Others ‣ III Object Detection from UAV-borne Images ‣ Deep
    Learning for UAV-based Object Detection and Tracking: A Survey"). Other methods
    that can solve the above problem, but not specifically for UAV data, are described
    in the text.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '视频目标检测（VID）在ImageNet VID挑战赛2015后成为一个热门话题。它在2017年之前被广泛应用于无人机数据，并且带来了一些新的挑战，例如无人机平台上的相机变更和运动模糊。接下来，根据近期出版的文献，总结了一些基于深度学习（DL）方法的解决方案。图[6](#S3.F6
    "Figure 6 ‣ III-F Object Detection on Others ‣ III Object Detection from UAV-borne
    Images ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey")展示了VID典型方法的发展。其中，专门针对无人机数据设计的方法列在表[II](#S3.T2
    "TABLE II ‣ III-F Object Detection on Others ‣ III Object Detection from UAV-borne
    Images ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey")中。其他可以解决上述问题但不专门针对无人机数据的方法在文中进行了描述。'
- en: The main steps of VID are summarized below.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: VID的主要步骤总结如下。
- en: •
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Single frame image object detection: Static object detection or object detection
    from images. Each frame in the video is an independent image, and the object detection
    from the image can be achieved by using a method in Section [III](#S3 "III Object
    Detection from UAV-borne Images ‣ Deep Learning for UAV-based Object Detection
    and Tracking: A Survey").'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '单帧图像目标检测：静态目标检测或图像目标检测。视频中的每一帧都是一张独立的图像，图像中的目标检测可以通过使用第[III](#S3 "III Object
    Detection from UAV-borne Images ‣ Deep Learning for UAV-based Object Detection
    and Tracking: A Survey")节中的方法实现。'
- en: •
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Detection results amendment: The above missed detection results are compensated
    by temporal information and context information of the video.'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检测结果修正：上述遗漏的检测结果通过视频的时间信息和上下文信息进行了补偿。
- en: The early mainstream method for VID is a multi-stage pipeline method, such as
    tubeless with convolutional neural network (TCNN) [[122](#bib.bib122), [123](#bib.bib123)]
    and sequence non-maximum suppression (Seq-NMS) [[124](#bib.bib124)], which is
    object detection from each frame where the modified detection results using a
    temporal context are performed separately. With the depth of research, many approaches
    have started to cast VID as a classic object detection problem. For example, a
    network model of feature enhancement module integration called SSD with comprehensive
    feature enhancement (CFE-SSDv2) [[125](#bib.bib125)] has been proposed to improve
    the accuracy of small size VID. F-SSD [[114](#bib.bib114)], based on SSD and FCOS,
    improved the robustness of the model through decision fusion of each frame detection
    result. EODST[[126](#bib.bib126)], based on SSD, adopted ECO tracking methods
    to associate object detection from a single frame. Similarly, benefiting from
    some advanced detectors, like HRDet [[127](#bib.bib127)], Cascade R-CNN [[128](#bib.bib128)],
    CenterNet [[129](#bib.bib129)], RetinaNet [[130](#bib.bib130)], and FPN [[131](#bib.bib131)],
    statistical convolutional neural network (SCNN) [[115](#bib.bib115)], several
    networks specifically developed for VID have been proposed. Some literature focus
    on the performance and real-time of these networks, so as to develop them on mobile
    [[117](#bib.bib117)] or embedded systems [[116](#bib.bib116)]. However, these
    methods are difficult to cover the context information for video. Although there
    are some methods to integrate Spatio-temporal information, e.g., Spatio-temporal
    neural network built on STDnet (STDnet-ST) [[121](#bib.bib121)], the problems
    of missed and false inspection still persist.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 早期主流的视频目标检测（VID）方法是多阶段管道方法，例如无管道的卷积神经网络（TCNN）[[122](#bib.bib122), [123](#bib.bib123)]
    和序列非最大抑制（Seq-NMS）[[124](#bib.bib124)]，这是从每一帧中进行目标检测，经过时间上下文修改的检测结果分别执行。随着研究的深入，许多方法开始将VID视为经典的目标检测问题。例如，提出了一种名为SSD的特征增强模块集成网络模型，称为全面特征增强（CFE-SSDv2）[[125](#bib.bib125)]，旨在提高小尺寸VID的准确性。基于SSD和FCOS的F-SSD
    [[114](#bib.bib114)]通过对每一帧检测结果进行决策融合，提高了模型的鲁棒性。基于SSD的EODST [[126](#bib.bib126)]采用ECO跟踪方法来关联单帧的目标检测。类似地，借助一些先进的检测器，如HRDet
    [[127](#bib.bib127)], Cascade R-CNN [[128](#bib.bib128)], CenterNet [[129](#bib.bib129)],
    RetinaNet [[130](#bib.bib130)] 和 FPN [[131](#bib.bib131)]，统计卷积神经网络（SCNN）[[115](#bib.bib115)]，已经提出了几种专门为VID开发的网络。一些文献关注这些网络的性能和实时性，以便在移动设备[[117](#bib.bib117)]
    或嵌入式系统[[116](#bib.bib116)]上进行开发。然而，这些方法很难覆盖视频的上下文信息。尽管有一些方法集成了时空信息，例如基于STDnet构建的时空神经网络（STDnet-ST）[[121](#bib.bib121)]，但遗漏和误检的问题仍然存在。
- en: The remainder of this section introduces three mainstream DL-based VID methods,
    including optical flow-based network, memory network-based network and tracking-based
    network, which integrate temporal context information into the DL-based methods
    to yield a better detection performance of VID and correct false alarms and missed
    detection.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节余下部分介绍了三种主流的基于深度学习的VID方法，包括基于光流的网络、基于记忆网络的网络和基于跟踪的网络，这些方法将时间上下文信息集成到基于深度学习的方法中，以提高VID的检测性能，并纠正虚假警报和漏检。
- en: IV-A Optical Flow-based Network
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 基于光流的网络
- en: In order to build the relationship between consecutive frames, some researchers
    estimate motion information. The most commonly used motion estimation method is
    optical flow.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建立连续帧之间的关系，一些研究人员估计运动信息。最常用的运动估计方法是光流。
- en: '[[118](#bib.bib118)] and [[120](#bib.bib120)] used the effective CNN model
    for optical flow (PWC-Net) [[132](#bib.bib132)] method and spatial pyramid network
    (SPyNet) [[133](#bib.bib133)] to obtain the motion information of two neighbor
    frames, respectively. Zhu et al. [[134](#bib.bib134)] designed fusion feature
    maps to achieve VID using deep feature flow (DFF) by learning the feature maps
    of key frames using feature extracting and of non-key frames using FlowNet. FlowNet
    was 11.8 times faster than Mobilenet, and even the smallest FlowNet-Xception was
    1.6 times faster. The flow guided feature aggregation (FGFA) [[135](#bib.bib135)]
    proposed by the MSRA visual computing group is also an early attempt based on
    optical flow. FGFA enhances the features of each frame by aggregating the features
    of multiple frames, finally using FlowNet to warp the features to solve video
    degradation. While FGFA is helpful for medium and fast speed VID, it is less effective
    for slow-speed VID. Subsequently, FGFA+ achieved better results by merging several
    data expansion strategies. Ref. [[136](#bib.bib136)] proposed an impression network,
    that can perform multi-frame feature fusion between sparse key frames, solving
    problems like defocus, motion, blur, and other issues in VID, while balancing
    detection speed and accuracy. Built upon [[135](#bib.bib135), [134](#bib.bib134)],
    Zhu et al. [[137](#bib.bib137)] adapted the flow network to learn multi-frame
    features and estimate cross-frame motion. Zhu et al. [[138](#bib.bib138)] subsequently
    designed a more lightweight optical flow network on mobiles. The entire network
    is trained end-to-end, reaching a mean average precision (MAP) of 60.2 in VID,
    and running to a speed of 25 frames on a Huawei Mate 8 cellphone. Due to a large
    amount of optical flow calculation using multiple frames, the network cannot perform
    back propagation revision during the training phase.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '[[118](#bib.bib118)] 和 [[120](#bib.bib120)] 分别使用了有效的 CNN 模型用于光流（PWC-Net）[[132](#bib.bib132)]
    方法和空间金字塔网络（SPyNet）[[133](#bib.bib133)] 来获取两个相邻帧的运动信息。Zhu 等人 [[134](#bib.bib134)]
    设计了融合特征图，通过学习关键帧的特征图和非关键帧的 FlowNet 特征图，使用深度特征流（DFF）实现 VID。FlowNet 比 Mobilenet
    快 11.8 倍，即便是最小的 FlowNet-Xception 也快了 1.6 倍。MSRA 视觉计算组提出的流引导特征聚合（FGFA）[[135](#bib.bib135)]
    也是基于光流的早期尝试。FGFA 通过聚合多帧的特征来增强每一帧的特征，最后使用 FlowNet 来扭曲特征以解决视频退化问题。虽然 FGFA 对于中等和快速速度的
    VID 有帮助，但对慢速 VID 的效果较差。随后，FGFA+ 通过合并几种数据扩展策略获得了更好的结果。参考文献 [[136](#bib.bib136)]
    提出了一个印象网络，该网络能够在稀疏关键帧之间执行多帧特征融合，解决了 VID 中如失焦、运动、模糊等问题，同时平衡了检测速度和准确性。基于 [[135](#bib.bib135),
    [134](#bib.bib134)]，Zhu 等人 [[137](#bib.bib137)] 适应了流网络来学习多帧特征并估计跨帧运动。Zhu 等人 [[138](#bib.bib138)]
    随后设计了一个更轻量级的移动光流网络。整个网络进行了端到端训练，在 VID 中达到了 60.2 的平均精度（MAP），并在华为 Mate 8 手机上运行到
    25 帧的速度。由于使用多个帧进行大量光流计算，网络在训练阶段无法进行反向传播修正。'
- en: IV-B Memory Networks-based Network
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 基于记忆网络的网络
- en: Since a video sequence has a strong long-term correlation, researchers introduced
    a memory network to fully learn the time information in a video sequence, such
    as a recurrent neural network (RNN) [[139](#bib.bib139)], long short-term memory
    (LSTM), and gated recurrent unit (GRU).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于视频序列具有强烈的长期相关性，研究人员引入了记忆网络，以充分学习视频序列中的时间信息，如递归神经网络（RNN）[[139](#bib.bib139)]、长短期记忆（LSTM）和门控递归单元（GRU）。
- en: In [[140](#bib.bib140)], Lu et al. proposed an association LSTM that fundamentally
    modeled object association between consecutive frames, and prompted LSTM to supply
    high quality association features. Refs. [[136](#bib.bib136)] and [[141](#bib.bib141)]
    both used ConvLSTM for efficient fusion of multi-frame features, improving video
    object detection accuracy while ensuring timeliness. In particular, [[141](#bib.bib141)]
    developed a new cross framework that used two feature extractors to run on different
    frames to improve the robustness of detectors. Liu et al [[142](#bib.bib142)]
    proposed an inter woven recurrent-convolutional architecture by designing the
    Bottleneck-LSTM layer to ensure real-time detection. Inspired by [[137](#bib.bib137)]
    and [[136](#bib.bib136)], Jiang et al. [[143](#bib.bib143)] adopted a brain-inspired
    memory mechanism to design a locally weighted deformable neighbors method for
    video object detection. Tripathi et al. [[144](#bib.bib144)] trained RNN through
    the content information of adjacent frames to optimize VID. Unlike the motion
    information learning of adjacent frames, Xiao et al [[145](#bib.bib145)] proposed
    a spatio-temporal memory network (STMN) to model and align the long-term sequence
    appearance and motion dynamics of objects in an end-to-end manner by learning
    multiple frames information. Wang et al. [[145](#bib.bib145)] proposed a motion-aware
    network (MANet) to directly learn motion information over a long period of time
    by fusing multiple frame features.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[140](#bib.bib140)]中，Lu等人提出了一种关联LSTM，基本上建模了连续帧之间的对象关联，并促使LSTM提供高质量的关联特征。参考文献[[136](#bib.bib136)]和[[141](#bib.bib141)]都使用了ConvLSTM来高效融合多帧特征，提高了视频对象检测的准确性，同时确保了及时性。特别地，[[141](#bib.bib141)]开发了一个新的跨框架，使用两个特征提取器在不同帧上运行，以提高检测器的鲁棒性。Liu等人[[142](#bib.bib142)]通过设计Bottleneck-LSTM层提出了一种交织的递归卷积架构，以确保实时检测。受[[137](#bib.bib137)]和[[136](#bib.bib136)]的启发，Jiang等人[[143](#bib.bib143)]采用了受脑启发的记忆机制，设计了一种局部加权变形邻域方法用于视频对象检测。Tripathi等人[[144](#bib.bib144)]通过相邻帧的内容信息训练RNN以优化VID。与相邻帧的运动信息学习不同，Xiao等人[[145](#bib.bib145)]提出了一种时空记忆网络（STMN），通过学习多个帧的信息以端到端的方式建模和对齐对象的长期序列外观和运动动态。Wang等人[[145](#bib.bib145)]提出了一种运动感知网络（MANet），通过融合多个帧特征直接学习长期的运动信息。
- en: IV-C Tracking-based Network
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 基于跟踪的网络
- en: In view of the high similarity between VID and object tracking in video discussed
    in the next section, there are still some methods to achieve VID by means of a
    tracking method [[114](#bib.bib114)] or to achieve object detection and tracking
    at the same time [[146](#bib.bib146)]. [[114](#bib.bib114)] proposed a novel spatial
    and temporal context-aware approach based on tracking for drone-based video object
    detection. In [[146](#bib.bib146)], the authors designed a scheduler network as
    a generalization of siamese trackers determined to detect or track at a certain
    frame. Actually, detection and tracking always coexist in actual scenarios.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于VID和下一节讨论的视频目标跟踪之间的高度相似性，仍有一些方法通过跟踪方法实现VID[[114](#bib.bib114)]或同时实现对象检测和跟踪[[146](#bib.bib146)]。[[114](#bib.bib114)]提出了一种基于跟踪的新的空间和时间上下文感知方法用于无人机视频对象检测。在[[146](#bib.bib146)]中，作者设计了一个调度网络作为siamese跟踪器的泛化，以决定在某一帧上进行检测或跟踪。实际上，检测和跟踪在实际场景中总是共存的。
- en: '![Refer to caption](img/a2e0eb283381a7537c412eb7cff46fee.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/a2e0eb283381a7537c412eb7cff46fee.png)'
- en: 'Figure 7: An illustration of the difference between VID and MOT in up/down
    frames.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：VID与MOT在上下帧中的区别说明。
- en: '![Refer to caption](img/076fb677c2b493d1081bd35c46f410db.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/076fb677c2b493d1081bd35c46f410db.png)'
- en: 'Figure 8: The development of typical methods for UAV object tracking from video.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：UAV视频中典型目标跟踪方法的发展。
- en: 'TABLE III: DL-based Multiple Object Tracking Approaches for UAV exclusive'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表III：基于DL的多目标跟踪方法（仅限UAV）
- en: '| Multiple Object Tracking |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 多目标跟踪 |'
- en: '| --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- |'
- en: '| Reference | Challenge | Dataset Used | Journal/Proc. | Year | Code/Link |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 参考 | 挑战 | 使用的数据集 | 期刊/会议 | 年份 | 代码/链接 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Deep SORT [[147](#bib.bib147)] | Occlusion | VisDrone-MOT | ICIP | 2017 |
    [https://github.com/nwojke/deep_sort](https://github.com/nwojke/deep_sort) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Deep SORT [[147](#bib.bib147)] | 遮挡 | VisDrone-MOT | ICIP | 2017 | [https://github.com/nwojke/deep_sort](https://github.com/nwojke/deep_sort)
    |'
- en: '| SCTrack [[148](#bib.bib148)] | Missed detection, occlusions | VisDrone |
    AVSS | 2018 | - |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| SCTrack [[148](#bib.bib148)] | 漏检，遮挡 | VisDrone | AVSS | 2018 | - |'
- en: '| Zhou et al[[149](#bib.bib149)] | Occlusion | VisDrone-MOT | Comput. Electr.
    Eng. | 2019 | - |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| Zhou 等[[149](#bib.bib149)] | 遮挡 | VisDrone-MOT | Comput. Electr. Eng. | 2019
    | - |'
- en: '| OSIM[[150](#bib.bib150)] | Orientation, scale | UAVDT | Remote Sens. | 2019
    | - |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| OSIM[[150](#bib.bib150)] | 方向、尺度 | UAVDT | Remote Sens. | 2019 | - |'
- en: '| Flow-tracker[[151](#bib.bib151)] | ID Switches, error detection | VisDrone-MOT
    | ICCV | 2019 | - |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| Flow-tracker[[151](#bib.bib151)] | ID 切换、错误检测 | VisDrone-MOT | ICCV | 2019
    | - |'
- en: '| TNT[[152](#bib.bib152)] | Camera motion, occlusion, pose variation | VisDrone-MOT,
    Own | ACM-MM | 2019 | - |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| TNT[[152](#bib.bib152)] | 相机运动、遮挡、姿态变化 | VisDrone-MOT, 自有 | ACM-MM | 2019
    | - |'
- en: '| HMTT[[153](#bib.bib153)] | Target motion, shape, appearance changes | VisDrone-MOT
    | ICCV | 2019 | - |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| HMTT[[153](#bib.bib153)] | 目标运动、形状、外观变化 | VisDrone-MOT | ICCV | 2019 | -
    |'
- en: '| Yang et al[[154](#bib.bib154)] | Target position changes | Own | RS | 2019
    | [https://frank804.github.io/](https://frank804.github.io/) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等[[154](#bib.bib154)] | 目标位置变化 | 自有 | RS | 2019 | [https://frank804.github.io/](https://frank804.github.io/)
    |'
- en: '| GGD[[155](#bib.bib155)] | False alarms, missed detections | VisDrone-MOT
    | ICCV | 2019 | [https://github.com/hakanardo/ggdtrack](https://github.com/hakanardo/ggdtrack)
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| GGD[[155](#bib.bib155)] | 错误警报、漏检 | VisDrone-MOT | ICCV | 2019 | [https://github.com/hakanardo/ggdtrack](https://github.com/hakanardo/ggdtrack)
    |'
- en: '| COMET[[156](#bib.bib156)] | Small object | UAVDT, VisDrone-MOT, Small-90
    | ICCV | 2019 | - |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| COMET[[156](#bib.bib156)] | 小物体 | UAVDT, VisDrone-MOT, Small-90 | ICCV |
    2019 | - |'
- en: '| Self-balance[[157](#bib.bib157)] | Appearance, motion | UAVDT | Multimedia
    Asia | 2019 | - |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 自平衡[[157](#bib.bib157)] | 外观、运动 | UAVDT | Multimedia Asia | 2019 | - |'
- en: '| Abughalieh et al[[117](#bib.bib117)] | Low detailed targets | DARPA, VIVID,
    Own | Multimed. Tools.Appl. | 2019 | - |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Abughalieh 等[[117](#bib.bib117)] | 低细节目标 | DARPA, VIVID, 自有 | Multimed. Tools.Appl.
    | 2019 | - |'
- en: '| Tracktor++[[158](#bib.bib158)] | Occlusions, crowded scenes | VisDrone-MOT
    | ICCV | 2019 | [https://git.io/fjQr8](https://git.io/fjQr8) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| Tracktor++[[158](#bib.bib158)] | 遮挡、拥挤场景 | VisDrone-MOT | ICCV | 2019 | [https://git.io/fjQr8](https://git.io/fjQr8)
    |'
- en: '| IPGAT[[159](#bib.bib159)] | Small object, appearance unreliable | UAVDT,
    Stanford Drone | PRL | 2020 | - |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| IPGAT[[159](#bib.bib159)] | 小物体、外观不可靠 | UAVDT, Stanford Drone | PRL | 2020
    | - |'
- en: '| Kapania et al[[160](#bib.bib160)] | Real-time | VisDrone-MOT | AIMS | 2020
    | - |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| Kapania 等[[160](#bib.bib160)] | 实时 | VisDrone-MOT | AIMS | 2020 | - |'
- en: '| PAS tracker[[161](#bib.bib161)] | False detections | VisDrone-MOT | ECCV
    | 2020 | - |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| PAS tracker[[161](#bib.bib161)] | 错误检测 | VisDrone-MOT | ECCV | 2020 | - |'
- en: '| DAN[[77](#bib.bib77)] | Dense distribution, small object | VisDrone-MOT |
    NCC | 2020 | - |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| DAN[[77](#bib.bib77)] | 密集分布、小物体 | VisDrone-MOT | NCC | 2020 | - |'
- en: '| DQN[[162](#bib.bib162)] | Small target | UAVDT | Electronics | 2021 | - |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| DQN[[162](#bib.bib162)] | 小目标 | UAVDT | Electronics | 2021 | - |'
- en: '| Cas_RCNN+ FPN[[80](#bib.bib80)] | Complex background | VisDrone-MOT | Transp.
    Res. Rec. | 2021 | - |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| Cas_RCNN+ FPN[[80](#bib.bib80)] | 复杂背景 | VisDrone-MOT | Transp. Res. Rec.
    | 2021 | - |'
- en: '| HDHNet[[163](#bib.bib163)] | Small object,class imbalance | VisDrone-MOT
    | Multimed. Tools. Appl. | 2021 | - |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| HDHNet[[163](#bib.bib163)] | 小物体、类别不平衡 | VisDrone-MOT | Multimed. Tools.
    Appl. | 2021 | - |'
- en: V Multiple Object Tracking from UAV-borne Video
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 从无人机载视频中进行多目标跟踪
- en: 'Multiple object tracking (MOT) for UAV video attract increasing research interest
    in recent years due to the flexibility of the camera in the drone platform. The
    popular DL-based MOT methods are not usually optimal for drone video data, due
    to new challenges, e.g., large viewpoint change and scales in drone platforms.
    Fig. [7](#S4.F7 "Figure 7 ‣ IV-C Tracking-based Network ‣ IV Object Detection
    from UAV-borne Video ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows a brief process to clarify the differences between VID and MOT.
    Both VID (Section [IV](#S4 "IV Object Detection from UAV-borne Video ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey")) and MOT need accurate
    object location,and the difference in MOT lies in predicting the trajectory in
    the next frame, in order to obtain the moving state of objects. In contrast, VID
    only needs to modify the detection results of the current frame by using temporal
    context in adjacent frames. In the following, DL-based solutions are summarized
    according to recently published literature. Fig. [8](#S4.F8 "Figure 8 ‣ IV-C Tracking-based
    Network ‣ IV Object Detection from UAV-borne Video ‣ Deep Learning for UAV-based
    Object Detection and Tracking: A Survey") shows the development of typical methods
    for MOT. Among them, methods specially designed for UAV data are listed in Table
    [III](#S4.T3 "TABLE III ‣ IV-C Tracking-based Network ‣ IV Object Detection from
    UAV-borne Video ‣ Deep Learning for UAV-based Object Detection and Tracking: A
    Survey"). Others methods that can solve the above problem, but not specifically
    for UAV data, are described directly in the paragraph. The remainder of this section
    introduce three mainstream DL-based MOT methods, i.e., tracking-by-detection,
    single object tracking assisted method, and memory networks.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机视频的多目标跟踪（MOT）由于无人机平台相机的灵活性，近年来吸引了越来越多的研究兴趣。由于无人机平台的新挑战，如视角的大幅度变化和尺度变化，流行的基于DL的MOT方法通常不太适用于无人机视频数据。图[7](#S4.F7
    "图7 ‣ IV-C 基于跟踪的网络 ‣ IV 无人机视频目标检测 ‣ 无人机基于深度学习的目标检测和跟踪：综述")简要说明了澄清VID和MOT之间的差异的过程。
    VID（第[IV节](#S4 "IV 无人机视频目标检测 ‣ 无人机基于深度学习的目标检测和跟踪：综述"））和MOT都需要准确的对象位置，MOT的差异在于预测下一帧中的轨迹，以获取对象的移动状态。相反，VID只需要使用相邻帧的时间上下文修改当前帧的检测结果。接下来，根据最近发表的文献总结了基于DL的解决方案。图[8](#S4.F8
    "图8 ‣ IV-C 基于跟踪的网络 ‣ IV 无人机视频目标检测 ‣ 无人机基于深度学习的目标检测和跟踪：综述")展示了MOT的典型方法的发展。其中，专门设计用于无人机数据的方法列在表[III](#S4.T3
    "表III ‣ IV-C 基于跟踪的网络 ‣ IV 无人机视频目标检测 ‣ 无人机基于深度学习的目标检测和跟踪：综述")中。其他可以解决上述问题，但并非专门针对无人机数据的方法，直接在段落中描述。本节的其余部分介绍了三种主流的基于DL的MOT方法，即检测跟踪，辅助单目标跟踪方法和记忆网络。
- en: V-A Tracking-by-Detection
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 检测跟踪
- en: Tracking-by-Detection (TBD) is the mainstream method of MOT [[159](#bib.bib159),
    [164](#bib.bib164), [165](#bib.bib165), [147](#bib.bib147), [166](#bib.bib166),
    [167](#bib.bib167), [167](#bib.bib167), [168](#bib.bib168)]. The main steps of
    TBDs are to first detect all objects of interests for the current frame, and then
    perform data associated with the previous frame for tracking. This method has
    the virtue of tracking newly arising objects in the whole video, but detection
    accuracy has a decisive effect on tracking results. In the TBD method, MOT is
    considered a data-dependent problem.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 检测跟踪（TBD）是MOT的主流方法[[159](#bib.bib159)，[164](#bib.bib164)，[165](#bib.bib165)，[147](#bib.bib147)，[166](#bib.bib166)，[167](#bib.bib167)，[167](#bib.bib167)，[168](#bib.bib168)]。TBD的主要步骤是首先检测当前帧中所有感兴趣的对象，然后执行与前一帧相关的数据进行跟踪。该方法具有在整个视频中跟踪新出现的对象的优点，但检测精度对跟踪结果具有决定性影响。在TBD方法中，MOT被视为一种依赖数据的问题。
- en: The commonly used TBD is CMOT [[164](#bib.bib164)], MDP [[165](#bib.bib165)],
    SORT [[169](#bib.bib169)] and DSORT [[147](#bib.bib147), [160](#bib.bib160), [77](#bib.bib77)],
    GOG [[166](#bib.bib166)], CEM [[170](#bib.bib170)], SMOT [[167](#bib.bib167)],
    and IOUT [[168](#bib.bib168), [151](#bib.bib151)]. For these methods, DL is only
    responsible for object detection, and traditional data-related methods are for
    data association. Recently, many learning-based data association approaches have
    been proposed. For example, Schulter et al. [[171](#bib.bib171)] designed an end-to-end
    network to solve the association problem. Son et al. [[172](#bib.bib172)] proposed
    a quadruplet convolutional neural network (Quad-CNN) with learning data association
    across frames by quadruplet losses. Feichtenhofer et al. [[173](#bib.bib173)]
    introduced correlation features and produced data association cross frames by
    linking the frame-level detection, which could simultaneously achieve object detection
    and tracking. Sun et al. [[127](#bib.bib127)] adopted a depth network to realize
    end-to-end feature extraction and data association. Jadhav et al. [[174](#bib.bib174)]
    proposed multiple object tracking methods by training a custom deep association
    network. Zhang et al. [[152](#bib.bib152)] developed a UAV tracking system, which
    is an integration of RetinaNet and TrackletNet Tracker (TNT). Huang et al. [[163](#bib.bib163)]
    proposed a hierarchical deep high-resolution network (HDHNet) to achieve an end-to-end
    online MOT system. Stadler et al. [[161](#bib.bib161)] proposed a PAS tracker
    that employs a novel similarity measure and Cascade RCNN to make full use of object
    representations. Yang et al. [[154](#bib.bib154)] designed dense-optical-flow-trajectory
    voting to measure the similarity of objects in adjacent frames, and integrated
    YOLOv3 to realize MOT.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 常用的待定（TBD）方法包括 CMOT [[164](#bib.bib164)]、MDP [[165](#bib.bib165)]、SORT [[169](#bib.bib169)]
    和 DSORT [[147](#bib.bib147), [160](#bib.bib160), [77](#bib.bib77)]、GOG [[166](#bib.bib166)]、CEM
    [[170](#bib.bib170)]、SMOT [[167](#bib.bib167)] 以及 IOUT [[168](#bib.bib168), [151](#bib.bib151)]。对于这些方法，深度学习（DL）仅负责目标检测，而传统的数据相关方法用于数据关联。最近，许多基于学习的数据关联方法被提出。例如，Schulter
    等 [[171](#bib.bib171)] 设计了一种端到端的网络来解决关联问题。Son 等 [[172](#bib.bib172)] 提出了一个四重卷积神经网络（Quad-CNN），通过四重损失实现跨帧的数据关联。Feichtenhofer
    等 [[173](#bib.bib173)] 引入了相关特征，通过连接帧级检测来实现跨帧的数据关联，从而可以同时实现目标检测和跟踪。Sun 等 [[127](#bib.bib127)]
    采用深度网络实现了端到端的特征提取和数据关联。Jadhav 等 [[174](#bib.bib174)] 通过训练一个定制的深度关联网络提出了多目标跟踪方法。Zhang
    等 [[152](#bib.bib152)] 开发了一个无人机跟踪系统，该系统集成了 RetinaNet 和 TrackletNet Tracker（TNT）。Huang
    等 [[163](#bib.bib163)] 提出了一个分层的深度高分辨率网络（HDHNet），以实现端到端的在线 MOT 系统。Stadler 等 [[161](#bib.bib161)]
    提出了一个 PAS 跟踪器，该跟踪器采用了新颖的相似性度量和 Cascade RCNN，以充分利用目标表示。Yang 等 [[154](#bib.bib154)]
    设计了稠密光流轨迹投票方法来测量相邻帧中对象的相似性，并集成了 YOLOv3 来实现 MOT。
- en: Another way to optimize the track association is Siamese network [[175](#bib.bib175)],
    which is a similarity measurement method that is especially suitable for object
    classification when there are more object classes but small quantities in each
    class. It has been widely applied in multiple object tracking [[176](#bib.bib176),
    [177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180),
    [181](#bib.bib181)]. For example, LEE et al. [[176](#bib.bib176)] proposed an
    on online object tracking using rule distillated Siamese random forest. Jin et
    al. [[177](#bib.bib177)] proposed online MOT with Siamese network and optical
    flow (Siamese-OF). Shuai et al. [[178](#bib.bib178)] proposed MOT with Siamese
    Track-RCNN. Bea et al. [[179](#bib.bib179)] proposed an updated Siamese network
    to learn discriminative deep feature representations for MOT. Leal-Taixé et al.
    [[180](#bib.bib180)] developed a multi-modal MOT method by learning the local
    features of RGB images and optical flow maps using a Siamese network. Al-Shakarji
    et al. [[148](#bib.bib148)] designed a time-efficient detection-based multi-object
    tracking system using a three step cascaded data association scheme. Dike et al.
    [[162](#bib.bib162)] proposed a quadruplet network to track prediction objects
    from crowded environments. Yu et al. [[157](#bib.bib157)] proposed a self-balance
    method integrating appearance similarity and motion consistency. Youssef et al.
    [[80](#bib.bib80)] achieve MOT by cascade region-based convolutional neural networks
    and feature pyramid networks.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种优化轨迹关联的方法是孪生网络 [[175](#bib.bib175)]，这是一种相似度测量方法，特别适用于目标分类当目标类别较多但每个类别样本量较少时。它已广泛应用于多目标跟踪
    [[176](#bib.bib176), [177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179),
    [180](#bib.bib180), [181](#bib.bib181)]。例如，LEE 等人 [[176](#bib.bib176)] 提出了使用规则提炼的孪生随机森林进行在线目标跟踪。Jin
    等人 [[177](#bib.bib177)] 提出了基于孪生网络和光流（Siamese-OF）的在线MOT。Shuai 等人 [[178](#bib.bib178)]
    提出了基于孪生 Track-RCNN 的MOT方法。Bea 等人 [[179](#bib.bib179)] 提出了更新的孪生网络，以学习区分性的深层特征表示用于MOT。Leal-Taixé
    等人 [[180](#bib.bib180)] 开发了一种多模态MOT方法，通过使用孪生网络学习RGB图像和光流图的局部特征。Al-Shakarji 等人
    [[148](#bib.bib148)] 设计了一种高效的基于检测的多目标跟踪系统，使用了三级级联数据关联方案。Dike 等人 [[162](#bib.bib162)]
    提出了一个四重网络，以跟踪来自拥挤环境的预测目标。Yu 等人 [[157](#bib.bib157)] 提出了一个自平衡方法，将外观相似性和运动一致性集成在一起。Youssef
    等人 [[80](#bib.bib80)] 通过级联区域卷积神经网络和特征金字塔网络实现了多目标跟踪。
- en: It should be noted that if we directly use video data acquired by UAV during
    the flight for MOT, the detection result often contain high noise, false alarm’s
    and missed detection due to changes in the UAV aircraft’s motion, the inevitable
    ”jitter”, and ambient light. Therefore, it is necessary to pre-process the UAV
    video. In addition, Tracking-by-Detection would fail to efficiently match when
    the front and back frames of the object in the video move too fast.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，如果我们直接使用无人机在飞行过程中获取的视频数据进行多目标跟踪（MOT），检测结果往往会因为无人机的运动、不可避免的“抖动”以及环境光线的变化而包含高噪声、误报和漏检。因此，有必要对无人机视频进行预处理。此外，当视频中目标的前后帧移动过快时，基于检测的跟踪方法会效率低下。
- en: V-B Single Object Tracking Assisted Multiple Object Tracking
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 单目标跟踪辅助多目标跟踪
- en: Trajectory prediction can address the failings of Tracking-by-Detection identified
    above well, and the most commonly used method is the single object tracking (SOT)
    assisted method [[182](#bib.bib182), [183](#bib.bib183), [165](#bib.bib165), [184](#bib.bib184),
    [185](#bib.bib185), [153](#bib.bib153)]. With significant progress in this approach
    recently, SOT has been successfully applied to complex scenes [[186](#bib.bib186),
    [187](#bib.bib187), [188](#bib.bib188)], but directly applying SOT to MOT would
    encounter calculation inefficiency and tracking drift caused by occlusion. For
    this reason, Pan et al.[[153](#bib.bib153)] propose a hierarchical multi-target
    tracker (HMTT) incorporating SOT and Kalman filtering to improve the MOT performance.
    Li et al. [[182](#bib.bib182)] designed an multiple vehicle tracking approach
    to effectively integrate SOT based forward position prediction with IOUT to enhance
    the detection results in the association phase. Yan et al. [[183](#bib.bib183)]
    associated detector and SOT trackers as candidate objects, and then candidates
    were selected through an ensemble framework. Xiang et al. [[165](#bib.bib165)]
    adopted the Markov Decision Processes (MDP) method to track objects in a tracked
    state with optical flow. Chu et al. [[184](#bib.bib184)] treated all detection
    output as SOT proposals, and designed MOT network architecture by considering
    multiple objective interactions, yielding a significant improvement for MOT. Ref.
    [[189](#bib.bib189)] proposed a novel instance-aware tracker to effectively integrate
    SOT in to MOT. In [[190](#bib.bib190)], the authors adopted a Siamese-RPN [[191](#bib.bib191)]
    SOT tracker and re-identification (ReID) network to extract short-term and long-term
    clues, respectively. Better data association method called Switcher-aware classification
    (SAC) was then proposed to improve the tracking results while solving the offset
    problem. In the above methods, the SOT tracker is independent of data association,
    which raises a potential issue that the two steps do not collaborate well to reinforce
    each other. To this end, Zhu et al. [[192](#bib.bib192)] proposed Dual Matching
    Attention Networks (DMAN) to deal with intra-class distractors and frequent interactions
    between objects though integrating a unified framework by single object ECO tracking
    and data association.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 轨迹预测可以很好地解决上述通过检测跟踪识别的问题，而最常用的方法是单目标跟踪（SOT）辅助方法[[182](#bib.bib182)、[183](#bib.bib183)、[165](#bib.bib165)、[184](#bib.bib184)、[185](#bib.bib185)、[153](#bib.bib153)]。最近这一方法取得了显著进展，SOT已经成功应用于复杂场景[[186](#bib.bib186)、[187](#bib.bib187)、[188](#bib.bib188)]，但直接将SOT应用于MOT会遇到计算效率低和由于遮挡造成的跟踪漂移。因此，Pan等人[[153](#bib.bib153)]提出了一种层次化的多目标跟踪器（HMTT），将SOT和卡尔曼滤波结合起来，以提高MOT性能。Li等人[[182](#bib.bib182)]设计了一种多车辆跟踪方法，以有效整合基于SOT的前向位置预测与IOUT，以增强关联阶段的检测结果。Yan等人[[183](#bib.bib183)]将检测器和SOT跟踪器关联为候选对象，然后通过集成框架选择候选者。Xiang等人[[165](#bib.bib165)]采用了马尔可夫决策过程（MDP）方法，通过光流跟踪处于跟踪状态的物体。Chu等人[[184](#bib.bib184)]将所有检测输出视为SOT提议，并通过考虑多个目标交互设计了MOT网络架构，从而显著改善了MOT性能。文献[[189](#bib.bib189)]提出了一种新颖的实例感知跟踪器，以有效整合SOT到MOT中。在[[190](#bib.bib190)]中，作者采用了Siamese-RPN[[191](#bib.bib191)]
    SOT跟踪器和重新识别（ReID）网络，分别提取短期和长期线索。随后提出了一种更好的数据关联方法，称为Switcher-aware classification（SAC），以改善跟踪结果，同时解决偏移问题。在上述方法中，SOT跟踪器与数据关联是独立的，这带来了一个潜在问题，即两个步骤之间的协作不够紧密，无法相互增强。为此，Zhu等人[[192](#bib.bib192)]提出了双重匹配注意力网络（DMAN），通过将单目标ECO跟踪和数据关联整合到统一框架中，以处理类内干扰物和物体之间的频繁交互。
- en: In addition, for real-time analysis of the SOT-assisted method, offline-trained
    SOT trackers like the Siamese-RPN can achieve high-speed accuracy of more than
    80 frames per second, while an online SOT update consumes a lot of CPU resources.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于SOT辅助方法的实时分析，离线训练的SOT跟踪器如Siamese-RPN可以实现每秒超过80帧的高速准确性，而在线SOT更新会消耗大量CPU资源。
- en: V-C Multiple Object Tracking Based on Memory Networks
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于记忆网络的V-C多目标跟踪
- en: Similar to the VID, MOT may judge new object status through historical trajectory
    information. Therefore, it is a feasible framework for designing a network structure
    that can memorize historical information and learn matching similarity measurement
    based on this historical information to enhance the performance of MOT [[193](#bib.bib193)].
    Among all the RNNs, the LSTM network has shown reliable performance on many sequence
    problems, and can overcome the gradient disappearance and explosion problems of
    standard RNNs. The special structures of LSTM enable it to remember information
    for a long time. Recently, some methods [[194](#bib.bib194), [195](#bib.bib195),
    [196](#bib.bib196)] employing have achieved impressive performances by LSTM networks.
    Milan et al. [[193](#bib.bib193)] trained an end-to-end LSTM network for online
    MOT. Sadeghian et al. [[185](#bib.bib185)] integrated appearance, action, and
    interaction cues into a unified RNN, and designed feature fusion based on LSTM
    to express motion interaction, so as to learn the matching similarity between
    the trajectory history information and the current detection. After designing
    and analyzing each gate function in LSTM, Kim et al. [[197](#bib.bib197)] proposed
    a novel RNN model called bilinear LSTM based on multiplication so as to improve
    the learning ability of long-term appearance models. Yu et al. [[159](#bib.bib159)]
    estimated the individual motion and global motion by LSTM and Siamese network.
    In [[190](#bib.bib190)], the short-term clues obtained by the Siamese-RPN network
    and the long-term clues obtained by ReID were introduced to meet complex scenarios
    and achieved state-of-the-art tracking performance.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与 VID 相似，MOT 可能通过历史轨迹信息判断新对象状态。因此，这是一种可行的框架，用于设计能够记住历史信息并基于这些历史信息学习匹配相似度测量的网络结构，以增强
    MOT 的性能 [[193](#bib.bib193)]。在所有 RNN 中，LSTM 网络在许多序列问题上表现出可靠的性能，并能够克服标准 RNN 的梯度消失和爆炸问题。LSTM
    的特殊结构使其能够长期记住信息。最近，一些方法 [[194](#bib.bib194), [195](#bib.bib195), [196](#bib.bib196)]
    通过 LSTM 网络取得了令人印象深刻的成绩。Milan 等人 [[193](#bib.bib193)] 为在线 MOT 训练了一个端到端的 LSTM 网络。Sadeghian
    等人 [[185](#bib.bib185)] 将外观、动作和互动线索整合到一个统一的 RNN 中，并设计了基于 LSTM 的特征融合以表达运动交互，从而学习轨迹历史信息与当前检测之间的匹配相似度。在设计和分析
    LSTM 中的每个门函数后，Kim 等人 [[197](#bib.bib197)] 提出了一个名为双线性 LSTM 的新型 RNN 模型，该模型基于乘法，以提高长期外观模型的学习能力。Yu
    等人 [[159](#bib.bib159)] 通过 LSTM 和 Siamese 网络估计个体运动和全局运动。在 [[190](#bib.bib190)]
    中，引入了 Siamese-RPN 网络获得的短期线索和 ReID 获得的长期线索，以应对复杂场景，并实现了最先进的跟踪性能。
- en: V-D Multiple Object Tracking Based on Others
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 基于其他方法的多目标跟踪
- en: Besides the aforementioned methods, other methods for multiple object tracking
    are also available, such as generalized graph differences (GGD) for network flow
    optimization [[155](#bib.bib155)] with an efficient representation of differences
    between graphs, context-aware IoU-guided tracker (COMET) [[156](#bib.bib156)]
    with offline proposal generation and multitask two-stream network. There is also
    literature focusing on designing MOT patrol [[149](#bib.bib149)] or mobile [[117](#bib.bib117)]
    systems for UAV video.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述方法，还有其他多目标跟踪方法，例如用于网络流优化的广义图差异（GGD） [[155](#bib.bib155)]，其高效表示了图之间的差异，基于上下文感知的
    IoU 引导跟踪器（COMET） [[156](#bib.bib156)]，它具有离线提案生成和多任务双流网络。还有文献关注于为 UAV 视频设计 MOT
    巡逻 [[149](#bib.bib149)] 或移动 [[117](#bib.bib117)] 系统。
- en: VI UAV-based Benchmark Dataset
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 基于 UAV 的基准数据集
- en: With the development of data-driven deep learning methods, researchers have
    made a lot of contributions to develop a variety of reference datasets for object
    detection (including images and videos) and tracking in UAV remote sensing, to
    help further study and performance comparison. In this section, we have reviewed
    some of the most commonly used open and classic UAV-based remote sensing datasets
    for detection and tracking.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据驱动的深度学习方法的发展，研究人员在开发各种参考数据集方面做出了大量贡献，这些数据集用于目标检测（包括图像和视频）和 UAV 遥感中的跟踪，以帮助进一步研究和性能比较。在这一部分，我们回顾了一些最常用的开放和经典的基于
    UAV 的遥感数据集，用于检测和跟踪。
- en: 'TABLE IV: Comparison of Current State-of-the-Art UAV Benchmarks and Datasets.
    The tasks SOD, VID, SOT, and MOT stands for object detection from images, object
    detection from video, single object tracking, and multiple objects tracking respectively.
    S: Single camera view, M: Multiple camera view, C_View: Camera View'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 当前最先进的无人机基准和数据集比较。任务SOD、VID、SOT和MOT分别代表从图像中检测物体、从视频中检测物体、单目标跟踪和多目标跟踪。S:
    单摄像头视图，M: 多摄像头视图，C_View: 摄像头视图'
- en: '| Object Detection from Image | Attributes |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 从图像中检测物体 | 属性 |  |'
- en: '| Modality | Images | Boxes | Tasks | Image Size | Annotation | Occlusion |
    Weather | C_View | Year |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Modality | Images | Boxes | Tasks | Image Size | Annotation | Occlusion |
    Weather | C_View | Year |'
- en: '| CARPK[[198](#bib.bib198)] | RGB | 1.5k | 90k | SOD | $1280\times 720$ | HBB
    |  |  | S | 2017 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| CARPK[[198](#bib.bib198)] | RGB | 1.5k | 90k | SOD | $1280\times 720$ | HBB
    |  |  | S | 2017 |'
- en: '| UAVDT[[199](#bib.bib199)] | RGB | 40k | 841.5k | SOD | $1080\times 540$ |
    HBB | $\surd$ | $\surd$ | M | 2018 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| UAVDT[[199](#bib.bib199)] | RGB | 40k | 841.5k | SOD | $1080\times 540$ |
    HBB | $\surd$ | $\surd$ | M | 2018 |'
- en: '| DAC-SDC [[200](#bib.bib200)] | RGB | 150k | - | SOD | $640\times 360$ | HBB
    |  |  | M | 2019 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| DAC-SDC [[200](#bib.bib200)] | RGB | 150k | - | SOD | $640\times 360$ | HBB
    |  |  | M | 2019 |'
- en: '| VisDrone-2018[[21](#bib.bib21)] | RGB | 40.0k | 183.3k | SOD | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2018 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| VisDrone-2018[[21](#bib.bib21)] | RGB | 40.0k | 183.3k | SOD | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2018 |'
- en: '| VisDrone-2019[[201](#bib.bib201)] | RGB | 261.9k | 2.6m | SOD | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2019 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| VisDrone-2019[[201](#bib.bib201)] | RGB | 261.9k | 2.6m | SOD | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2019 |'
- en: '| DroneVehicle [[202](#bib.bib202)] | RGB + Infrared | 31.064k | 88.3k | SOD
    | $840\times 712$ | OBB |  | $\surd$ | M | 2020 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| DroneVehicle [[202](#bib.bib202)] | RGB + 红外 | 31.064k | 88.3k | SOD | $840\times
    712$ | OBB |  | $\surd$ | M | 2020 |'
- en: '| AU-AIR [[203](#bib.bib203)] | Multi-modal | 32.823k | - | SOD | $1920\times
    1080$ | HBB |  | $\surd$ | M | 2020 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| AU-AIR [[203](#bib.bib203)] | 多模态 | 32.823k | - | SOD | $1920\times 1080$
    | HBB |  | $\surd$ | M | 2020 |'
- en: '| BIRDSAI [[204](#bib.bib204)] | Thermal-IR | - | 270k | SOD | $640\times 480$
    | HBB |  |  | M | 2020 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| BIRDSAI [[204](#bib.bib204)] | 热成像-红外 | - | 270k | SOD | $640\times 480$
    | HBB |  |  | M | 2020 |'
- en: '| UVSD [[56](#bib.bib56)] | RGB | 5.8k |  | SOD | $960\times 540$ to $5280\times
    2970$ | HBB/OBB |  |  | M | 2020 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| UVSD [[56](#bib.bib56)] | RGB | 5.8k |  | SOD | $960\times 540$ 到 $5280\times
    2970$ | HBB/OBB |  |  | M | 2020 |'
- en: '| MOHR[[205](#bib.bib205)] | RGB | - | 90k | SOD | $5482\times 3078$/$7360\times
    4912$ | HBB |  |  | M | 2021 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| MOHR[[205](#bib.bib205)] | RGB | - | 90k | SOD | $5482\times 3078$/$7360\times
    4912$ | HBB |  |  | M | 2021 |'
- en: '| $8688\times 5792$ |  |  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| $8688\times 5792$ |  |  |'
- en: '| Object Detection from Video | Attributes |  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 从视频中检测物体 | 属性 |  |'
- en: '| Modality | Images | Boxes | Tasks | Image Size | Annotation | Occlusion |
    Weather | C_View | Year |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| Modality | Images | Boxes | Tasks | Image Size | Annotation | Occlusion |
    Weather | C_View | Year |'
- en: '| Okutama-Action[[206](#bib.bib206)] | RGB | 77.4k | 422.1k | VID | $3840\times
    2160$ | HBB |  |  | M | 2017 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| Okutama-Action[[206](#bib.bib206)] | RGB | 77.4k | 422.1k | VID | $3840\times
    2160$ | HBB |  |  | M | 2017 |'
- en: '| UAVDT[[199](#bib.bib199)] | RGB | 80k | 841.5k | VID | $1080\times 540$ |
    HBB | $\surd$ | $\surd$ | M | 2018 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| UAVDT[[199](#bib.bib199)] | RGB | 80k | 841.5k | VID | $1080\times 540$ |
    HBB | $\surd$ | $\surd$ | M | 2018 |'
- en: '| VisDrone-2018[[21](#bib.bib21)] | RGB | 40.0k | 183.3k | VID | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2018 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| VisDrone-2018[[21](#bib.bib21)] | RGB | 40.0k | 183.3k | VID | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2018 |'
- en: '| VisDrone-2019[[201](#bib.bib201)] | RGB | 261.9k | 2.6m | VID | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2019 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| VisDrone-2019[[201](#bib.bib201)] | RGB | 261.9k | 2.6m | VID | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2019 |'
- en: '| MOR-UAV [[119](#bib.bib119)] | RGB | 10k | 90k | VID | $1280\times 720$/$1920\times
    1080$ | HBB | $\surd$ | $\surd$ | M | 2020 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| MOR-UAV [[119](#bib.bib119)] | RGB | 10k | 90k | VID | $1280\times 720$/$1920\times
    1080$ | HBB | $\surd$ | $\surd$ | M | 2020 |'
- en: '| Object Tracking from Image | Attributes |  |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 从图像中跟踪物体 | 属性 |  |'
- en: '| Modality | Images | Boxes | Tasks | Image Size | Annotation | Occlusion |
    Weather | C_View | Year |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| Modality | Images | Boxes | Tasks | Image Size | Annotation | Occlusion |
    Weather | C_View | Year |'
- en: '| UAV123[[207](#bib.bib207)] | RGB | 110k | 110k | SOT | $720\times 720$ |
    HBB |  |  | M | 2016 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| UAV123[[207](#bib.bib207)] | RGB | 110k | 110k | SOT | $720\times 720$ |
    HBB |  |  | M | 2016 |'
- en: '| DTB70[[208](#bib.bib208)] | RGB | - | - | SOT | $1280\times 720$ | HBB |  |  |
    M | 2017 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| DTB70[[208](#bib.bib208)] | RGB | - | - | SOT | $1280\times 720$ | HBB |  |  |
    M | 2017 |'
- en: '| Stanford[[209](#bib.bib209)] | RGB | 929.5k | 19.5k | MOT | $1417\times 2019$
    | HBB | $\surd$ |  | S | 2016 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| Stanford[[209](#bib.bib209)] | RGB | 929.5k | 19.5k | MOT | $1417\times 2019$
    | HBB | $\surd$ |  | S | 2016 |'
- en: '| UAVDT[[199](#bib.bib199)] | RGB | 80k | 841.5k | MOT | $1080\times 540$ |
    HBB | $\surd$ | $\surd$ | M | 2018 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| UAVDT[[199](#bib.bib199)] | RGB | 80k | 841.5k | MOT | $1080\times 540$ |
    HBB | $\surd$ | $\surd$ | M | 2018 |'
- en: '| VisDrone-2018[[21](#bib.bib21)] | RGB | 40.0k | 183.3k | MOT | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2018 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| VisDrone-2018[[21](#bib.bib21)] | RGB | 40.0k | 183.3k | MOT | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2018 |'
- en: '| VisDrone-2019[[201](#bib.bib201)] | RGB | 261.9k | 2.6m | MOT | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2019 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| VisDrone-2019[[201](#bib.bib201)] | RGB | 261.9k | 2.6m | MOT | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2019 |'
- en: '| BIRDSAI[[204](#bib.bib204)] | Thermal-IR | 162k | 270k | MOT | $640\times
    480$ | HBB |  |  | M | 2020 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| BIRDSAI[[204](#bib.bib204)] | 热红外 | 162k | 270k | MOT | $640\times 480$ |
    HBB |  |  | M | 2020 |'
- en: 'Stanford Drone Dataset[[209](#bib.bib209)]¹¹1[http://cvgl.stanford.edu/projects/uav_data/](http://cvgl.stanford.edu/projects/uav_data/):
    The Stanford Drone Dataset is a large-scale object tracking dataset, that was
    made public by Stanford University in 2016\. These video sequences were captured
    in a real campus environment by a 4k camera on a quadcopter, which hovered above
    various intersections on campus with a flight height of about 80 meters. This
    dataset contains 10 object types with more than 19,000 objects, including 112,000
    pedestrians, 64,000 bicycles, 13,000 cars, 33,000 skateboarders, 22,000 golf carts,
    and 11,000 public cars, all of which can be used for multiple object tracking.
    Although this dataset only has videos of a college campus, the data has enough
    pluralism to be applied in various scenarios.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 斯坦福无人机数据集[[209](#bib.bib209)]¹¹1[http://cvgl.stanford.edu/projects/uav_data/](http://cvgl.stanford.edu/projects/uav_data/)：斯坦福无人机数据集是一个大规模的目标跟踪数据集，由斯坦福大学在2016年公布。这些视频序列由一台4k相机安装在四旋翼无人机上拍摄，该无人机在校园内的多个交叉路口上方悬停，飞行高度约为80米。该数据集包含10种目标类型，共有超过19,000个目标，包括112,000名行人、64,000辆自行车、13,000辆汽车、33,000名滑板运动员、22,000辆高尔夫球车和11,000辆公共汽车，所有这些数据可用于多目标跟踪。尽管该数据集仅有大学校园的视频，但数据的多样性足以应用于各种场景。
- en: 'UAV123 Dataset[[207](#bib.bib207)]²²2[https://cemse.kaust.edu.sa/ivul/uav123](https://cemse.kaust.edu.sa/ivul/uav123):
    The UAV123 dataset is a long-term aerial object tracking dataset, which was designated
    as public by King Abdullah University of Science and Technology in 2016\. It contains
    123 video sequences and more than 110,000 representative frames. The label information
    of each sequence adopts a horizontal bounding box (i.e., upper left and lower
    right), and the bounding box size and aspect ratio show significant differences
    from the first frame. These video sequences were captured by three different UAVs:
    an off-the-shelf professional-grade UAV (DJIS1000) with a flight height of 5-25
    meters, a small low-cost UAV, and a UAV simulator. The UAV123 dataset has multiple
    variations of scenes, objects, and their corresponding attitudes, making it better
    suited for a deep learning framework.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: UAV123 数据集[[207](#bib.bib207)]²²2[https://cemse.kaust.edu.sa/ivul/uav123](https://cemse.kaust.edu.sa/ivul/uav123)：UAV123
    数据集是一个长期的空中目标跟踪数据集，由国王阿卜杜拉科技大学在2016年公布。它包含123个视频序列和超过110,000个代表性帧。每个序列的标签信息采用水平边界框（即左上角和右下角），且边界框的大小和纵横比与第一帧有显著差异。这些视频序列由三种不同的无人机拍摄：一台5-25米飞行高度的现货专业无人机（DJIS1000），一台小型低成本无人机，以及一台无人机模拟器。UAV123
    数据集具有多种场景、目标及其对应姿态的变体，使其更适合深度学习框架。
- en: 'Drone Tracking Benchmark (DTB70)[[208](#bib.bib208)]³³3[https://link.zhihu.com/?target=https%3A//github.com/flyers/drone-tracking](https://link.zhihu.com/?target=https%3A//github.com/flyers/drone-tracking):
    The DTB70 dataset includes both short-term and long-term aerial objects, which
    were provided by the Hong Kong University of Science and Technology in 2017\.
    It contains 70 video sequences. Some of these video sequences were captured in
    a real outdoor environment by a DJI Phantom 2 Vision+ drone, which hovered over
    the university campus with a flight altitude of lower than 120 meters. The others
    were intercepted through YouTube to increase the diversity of samples. Each frame
    contains $1280\times 720$ and its label information adopts a horizontal bounding
    box (i.e., upper left and lower right).'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机跟踪基准（DTB70）[[208](#bib.bib208)]³³3[https://link.zhihu.com/?target=https%3A//github.com/flyers/drone-tracking](https://link.zhihu.com/?target=https%3A//github.com/flyers/drone-tracking)：DTB70
    数据集包含了短期和长期的空中目标，这些数据由香港科技大学在2017年提供。它包含70个视频序列。其中一些视频序列是由 DJI Phantom 2 Vision+
    无人机在实际户外环境中拍摄的，该无人机在大学校园上方悬停，飞行高度低于120米。其他序列则是通过YouTube获取的，以增加样本的多样性。每帧包含 $1280\times
    720$ 的分辨率，其标签信息采用水平边界框（即左上角和右下角）。
- en: 'Car Parking Lot Dataset (CARPK)[[198](#bib.bib198)]⁴⁴4[https://lafi.github.io/LPN/](https://lafi.github.io/LPN/):
    The CARPK dataset is a large-scale vehicle detection and counting dataset, which
    was designated as public by the National Taiwan University in 2017\. In particular,
    it is the first and largest parking lot dataset acquired by drone views and is
    used for vehicle counting parked in a different parking lot. The dataset was acquired
    by a Phantom 3 Professional drone with a flight height of 40 meters, covering
    nearly 90,000 cars in four different parking lots. The maximum size of vehicles
    in the CARPK dataset is much larger than $64\times 64$, and the maximum number
    of cars in a single scenario in the CARPK dataset is 188\. The label information
    of each vehicle adopts a horizontal bounding box (i.e., upper left and lower right).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 汽车停车场数据集 (CARPK)[[198](#bib.bib198)]⁴⁴4[https://lafi.github.io/LPN/](https://lafi.github.io/LPN/)：CARPK
    数据集是一个大规模的车辆检测与计数数据集，由国立台湾大学于 2017 年公开。特别地，这是首个也是最大的无人机视角停车场数据集，用于计算不同停车场中的停放车辆数量。该数据集由
    Phantom 3 Professional 无人机以 40 米的飞行高度获取，涵盖了四个不同停车场中的近 90,000 辆汽车。CARPK 数据集中车辆的最大尺寸远大于
    $64\times 64$，而单一场景中车辆的最大数量为 188。每辆车的标签信息采用水平边界框（即左上角和右下角）。
- en: 'Okutama-Action Dataset[[206](#bib.bib206)]⁵⁵5[http://okutama-action.org/](http://okutama-action.org/):
    Okutama dataset is a large-scale human action detection dataset, which was designated
    as public by five universities, including Munich University of Technology and
    the Royal Institute of Technology of Sweden, in 2017\. It contains 43 video sequences
    with 77,365 representative frames. These video sequences were captured at 45-
    or 90-degree camera angles using two drones with a flight height of 10-45 meters.
    In addition, the position and orientation of the UAV are flexible and changeable
    in order to acquire the diversification of the object. This dataset covers 12
    action types, such as reading, handshaking, drinking, and carrying. The speed
    of recorded videos is 30 frames per second (fps), and the image size is $3840\times
    2160$.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 奥多摩行动数据集 (Okutama-Action Dataset)[[206](#bib.bib206)]⁵⁵5[http://okutama-action.org/](http://okutama-action.org/)：奥多摩数据集是一个大规模的人类动作检测数据集，由包括慕尼黑工业大学和瑞典皇家理工学院在内的五所大学于
    2017 年公开。它包含 43 个视频序列和 77,365 张代表性帧。这些视频序列使用两个无人机在 10-45 米的飞行高度下以 45 度或 90 度的相机角度拍摄。此外，无人机的位置和方向是灵活可变的，以获取对象的多样性。该数据集涵盖
    12 种动作类型，如阅读、握手、喝水和搬运。记录视频的速度为每秒 30 帧 (fps)，图像大小为 $3840\times 2160$。
- en: 'UAV Detection and Tracking (UAVDT) Dataset[[199](#bib.bib199)]⁶⁶6[https://sites.google.com/site/daviddo0323/projects/uavdt](https://sites.google.com/site/daviddo0323/projects/uavdt):
    The UAVDT dataset is a large-scale vehicle detection and tracking dataset, which
    was designated as public by the University of the Chinese Academy of Sciences
    in 2018\. It contains 100 video sequences with 80,000 representative frames, approximately
    2,700 vehicles with 0.84 million bounding boxes, covering a range of weather conditions,
    occlusion, and flying heights. This dataset presents all sorts of common scenarios,
    including squares, arterial roads, toll stations, highways, intersections, and
    T-junctions. The speed of recorded videos is 30 frames per second (fps), and the
    image size is $1080\times 540$ pixels, which can be used for multiple tasks, such
    as vehicle detection, single vehicle tracking, and multiple vehicle tracking.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: UAV 检测与跟踪 (UAVDT) 数据集[[199](#bib.bib199)]⁶⁶6[https://sites.google.com/site/daviddo0323/projects/uavdt](https://sites.google.com/site/daviddo0323/projects/uavdt)：UAVDT
    数据集是一个大规模的车辆检测与跟踪数据集，由中国科学院大学于 2018 年公开。它包含 100 个视频序列，80,000 张代表性帧，约 2,700 辆车辆和
    84 万个边界框，涵盖了各种天气条件、遮挡情况和飞行高度。该数据集呈现了各种常见场景，包括广场、主干道、收费站、高速公路、交叉口和 T 型交叉路口。记录视频的速度为每秒
    30 帧 (fps)，图像大小为 $1080\times 540$ 像素，可用于多种任务，如车辆检测、单车跟踪和多车跟踪。
- en: 'DAC-SDC dataset[[200](#bib.bib200)]⁷⁷7[www.github.com/xyzxinyizhang/2018-DAC-System-Design-Contest](www.github.com/xyzxinyizhang/2018-DAC-System-Design-Contest):
    The Design Automation Conference (DAC) is a challenging object detection dataset
    collected by UAV, which was designated as public by the University of Notre Dame
    in 2018\. It contains 95 categories and 150k images captured with different points
    of UAV view. Each extracted frame includes $640\times 360$ pixels.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: DAC-SDC 数据集[[200](#bib.bib200)]⁷⁷7[www.github.com/xyzxinyizhang/2018-DAC-System-Design-Contest](www.github.com/xyzxinyizhang/2018-DAC-System-Design-Contest)：设计自动化会议
    (DAC) 是一个由无人机收集的挑战性目标检测数据集，于 2018 年由圣母大学公开。它包含 95 个类别和 150k 张图像，这些图像是从不同的无人机视角拍摄的。每帧提取图像的分辨率为
    $640\times 360$ 像素。
- en: 'VisDrone2018 Dataset[[21](#bib.bib21)]⁸⁸8[https://github.com/VisDrone/VisDrone-Dataset](https://github.com/VisDrone/VisDrone-Dataset):
    The VisDrone2018 dataset is a large-scale visual object detection and tracking
    dataset, which was designated as public by three universities, Tianjin University,
    GE Global Research, and Temple University, in 2018\. It contains 263 video sequences
    with 179,264 representative frames and 10,209 static images. These video sequences
    were captured by various camera devices using multiple drones (i.e., DJI Mavic
    and Phantom series (3, 3A, 3SE, 3P, 4, 4A, 4P)), which hovered above 14 cities
    in China. This dataset covers multiple common objects, such as pedestrians, cars,
    bicycles, and tricycles. The maximum image size of each video are much larger
    than $2000\times 1500$, and they can be used for multiple tasks, particularly
    object detection, single object tracking, and multiple object tracking. There
    are over 2.5 million objects with their label information in a horizontal bounding
    box.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: VisDrone2018 数据集[[21](#bib.bib21)]⁸⁸8[https://github.com/VisDrone/VisDrone-Dataset](https://github.com/VisDrone/VisDrone-Dataset)：VisDrone2018
    数据集是一个大规模的视觉对象检测和跟踪数据集，由天津大学、GE 全球研究中心和天普大学于 2018 年公开。它包含 263 个视频序列、179,264 帧代表性图像和
    10,209 张静态图像。这些视频序列是由多种摄像设备通过多架无人机（即 DJI Mavic 和 Phantom 系列 (3, 3A, 3SE, 3P, 4,
    4A, 4P)）在中国 14 个城市上空拍摄的。该数据集涵盖了多种常见物体，如行人、汽车、自行车和三轮车。每个视频的最大图像尺寸远大于 $2000\times
    1500$，可用于多种任务，特别是对象检测、单目标跟踪和多目标跟踪。数据集中有超过 250 万个带标签的水平边界框中的物体。
- en: 'VisDrone2019 Dataset[[201](#bib.bib201)]⁹⁹9[https://github.com/VisDrone/VisDrone-Dataset](https://github.com/VisDrone/VisDrone-Dataset):
    Compared to VisDrone2018, VisDrone2019 added 25 long-term tracking video sequences
    with a total of 82,644 frames, of which 12 clips were acquired in the daytime,
    and the rest were by night. Therefore, this dataset contains 288 video sequences
    with 261,908 representative frames and 10,209 static images. For each target,
    the scaling is much smaller and the disturbance factor is much greater.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: VisDrone2019 数据集[[201](#bib.bib201)]⁹⁹9[https://github.com/VisDrone/VisDrone-Dataset](https://github.com/VisDrone/VisDrone-Dataset)：与
    VisDrone2018 相比，VisDrone2019 增加了 25 个长期跟踪视频序列，共计 82,644 帧，其中 12 个片段是在白天获取的，其余为夜间获取。因此，该数据集包含
    288 个视频序列、261,908 帧代表性图像和 10,209 张静态图像。对于每个目标，缩放比例要小得多，干扰因素要大得多。
- en: 'Moving Object Recognition (MOR-UAV) Dataset[[119](#bib.bib119)]^(10)^(10)10[https://arxiv.org/abs/2008.01699](https://arxiv.org/abs/2008.01699):
    The MOR-UAV dataset is a large-scale video dataset for moving object recognition
    in UAV videos, which was designated as public by the Malaviya National Institute
    of Technology Jaipur in 2020\. It contains 30 video sequences with 10,948 representative
    frames, and approximately 89,783 moving object instances, covering various challenging
    scenarios such as night time, occlusion, camera motion, weather conditions, camera
    views, and so on. MOR-UAV can be used as the benchmark for both MOR and moving
    object detection (MOD) in UAV videos. The videos are recorded at 30 frames per
    second (fps) and the image size varies from $1280\times 720$ to $1920\times 1080$
    pixels. The moving objects are labeled using the Yolo-mark1 tool, and about 10,948
    frames are annotated representing moving vehicles. There are two categories of
    vehicles: cars and heavy vehicles.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 移动物体识别（MOR-UAV）数据集[[119](#bib.bib119)]^(10)^(10)10[https://arxiv.org/abs/2008.01699](https://arxiv.org/abs/2008.01699)：MOR-UAV
    数据集是一个大规模的视频数据集，用于无人机视频中的移动物体识别，由马拉维亚国家技术学院斋浦尔在2020年公开发布。该数据集包含30个视频序列，10,948帧代表性图像，约89,783个移动物体实例，涵盖各种挑战性场景，如夜间、遮挡、相机运动、天气条件、相机视角等。MOR-UAV
    可作为无人机视频中移动物体识别（MOR）和移动物体检测（MOD）的基准。视频以每秒30帧（fps）录制，图像大小从$1280\times 720$ 到$1920\times
    1080$ 像素不等。移动物体使用Yolo-mark1工具标注，约10,948帧被标注为移动车辆。车辆分为两类：汽车和重型车辆。
- en: 'DroneVehicle dataset[[202](#bib.bib202)]^(11)^(11)11[https://github.com/VisDrone/DroneVehicle](https://github.com/VisDrone/DroneVehicle):
    The DroneVehicle dataset is large-scale object detection and counting dataset
    with both RGB and thermal infrared (RGBT) images captured by camera-equipped drones,
    which was designated as public by the Tianjin University in 2020\. It contains
    15,532 pairs of images, i.e., RGB and infrared images, covering challenging scenarios
    with illumination, occlusion, and scale variations. DroneVehicle dataset can be
    used as the benchmark for both object detection and counting on the UAV platform.
    The images in this dataset were captured over various urban areas, including urban
    roads, residential areas, parking lots, highways, etc., from day to night. The
    image size is $840\times 712$ pixels.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: DroneVehicle 数据集[[202](#bib.bib202)]^(11)^(11)11[https://github.com/VisDrone/DroneVehicle](https://github.com/VisDrone/DroneVehicle)：DroneVehicle
    数据集是一个大规模的目标检测和计数数据集，包含由配备摄像头的无人机拍摄的RGB和热红外（RGBT）图像，由天津大学在2020年公开发布。该数据集包含15,532对图像，即RGB和红外图像，涵盖了具有照明、遮挡和尺度变化的挑战性场景。DroneVehicle
    数据集可以作为无人机平台上目标检测和计数的基准。该数据集中的图像拍摄于各种城市区域，包括城市道路、住宅区、停车场、高速公路等，从白天到夜晚。图像大小为$840\times
    712$ 像素。
- en: 'AU-AIR dataset[[203](#bib.bib203)]^(12)^(12)12[https://bozcani.github.io/auairdataset](https://bozcani.github.io/auairdataset):
    The multi-purpose aerial dataset (AU-AIR) is a large-scale object detection dataset
    from multi-modal sensors (i.e., visual, time, location, altitude, IMU, velocity)
    captured by camera-equipped drones, which was designated as public by the Aarhus
    University in 2020\. It contains 8 video sequences with 32,823 extracted frames
    at the intersection of Skejby Nordlandsvej and P.O Pedersensvej (Aarhus, Denmark)
    on windless days with various lighting and weather conditions. This dataset contains
    8 object types, including person, car, bus, van, truck, bike, motorbike, and trailer,
    all of which can be used for static or video object detection. Each frame contains
    $1920\times 1080$ pixels.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: AU-AIR 数据集[[203](#bib.bib203)]^(12)^(12)12[https://bozcani.github.io/auairdataset](https://bozcani.github.io/auairdataset)：多用途空中数据集（AU-AIR）是一个大规模目标检测数据集，来自多模态传感器（即视觉、时间、位置、海拔、IMU、速度），由配备摄像头的无人机拍摄，由奥胡斯大学在2020年公开发布。该数据集包含8个视频序列，32,823帧提取图像，拍摄于丹麦奥胡斯的Skejby
    Nordlandsvej和P.O Pedersensvej交叉口的无风日，具有各种照明和天气条件。该数据集包含8种目标类型，包括人、车、公交车、面包车、卡车、自行车、摩托车和拖车，可用于静态或视频目标检测。每帧图像大小为$1920\times
    1080$ 像素。
- en: 'BIRDSAI dataset[[204](#bib.bib204)]^(13)^(13)13[https://sites.google.com/view/elizabethbondi/dataset](https://sites.google.com/view/elizabethbondi/dataset):
    The benchmarking IR dataset for surveillance with aerial intelligence (BIRDSAI)
    is a challenging object detection and tracking dataset collected using a TIR camera
    mounted on a fixed-wing UAV in multiple African protected areas, which was designated
    as public by the Harvard University in 2020\. It contains 48 real aerial TIR videos
    of varying lengths and 124 synthetic aerial TIR videos generated from AirSim-W.
    This dataset contains humans and animals with scale variations, background clutter,
    large camera rotations, and motion blur, etc. Each frame contains $640\times 480$
    pixels.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: BIRDSAI 数据集[[204](#bib.bib204)]^(13)^(13)13[https://sites.google.com/view/elizabethbondi/dataset](https://sites.google.com/view/elizabethbondi/dataset)：用于监视的空中智能（BIRDSAI）基准红外数据集是一个具有挑战性的对象检测和跟踪数据集，该数据集使用固定翼无人机上的TIR相机在多个非洲保护区收集，由哈佛大学于2020年公开发布。它包含48个真实的空中TIR视频（长度各异）和124个从AirSim-W生成的合成空中TIR视频。该数据集包含具有尺度变化、背景杂乱、大幅相机旋转和运动模糊等特征的人类和动物。每帧图像包含$640\times
    480$像素。
- en: 'MOHR dataset[[205](#bib.bib205)]: The benchmarking IR dataset is a large-scale
    benchmark object detection dataset collected at different altitudes by employing
    three cameras, i.e., DJI Phantom 4Pro, Sonny RX1rM2, and Nikon D800\. The dataset
    includes 3,048 images of size $5482\times 3078$, 5,192 images of size $7360\times
    4912$, and 2,390 images of size $8688\times 5792$, respectively. It contains 90,014
    object instances with labels and bounding boxes were annotated, which includes
    25,575 cars, 12,957 trucks, 41,468 buildings, 7,718 flood damages, and 2,296 collapses,
    covering the challenging of scale variations.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: MOHR 数据集[[205](#bib.bib205)]：该基准红外数据集是一个大规模的基准对象检测数据集，通过使用三台相机（即DJI Phantom
    4Pro、Sony RX1rM2和Nikon D800）在不同高度采集的。数据集包括3,048张尺寸为$5482\times 3078$的图像、5,192张尺寸为$7360\times
    4912$的图像和2,390张尺寸为$8688\times 5792$的图像。它包含90,014个带标签和边界框的目标实例，其中包括25,575辆汽车、12,957辆卡车、41,468座建筑、7,718处洪水损坏和2,296处倒塌，涵盖了尺度变化的挑战。
- en: 'UVSD dataset[[210](#bib.bib210)]^(14)^(14)14[https://github.com/liuchunsense/UVSD](https://github.com/liuchunsense/UVSD):
    The UAV-based vehicle segmentation dataset (UVSD) is a large-scale benchmark object
    detection, counting, and segmentation dataset. The dataset includes 5,874 images,
    with 98,600 object instances with high-quality instance-level semantic annotations.
    These images are captured by DJI matrice 200 quadcopter integrated with a zenmuse
    X5S gimbal and camera, and image size varies from $960\times 540$ to $5280\times
    2970$ pixels. In particular, UVSD has multiple format annotations, including pixel-level
    semantic, OBB and HBB.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: UVSD 数据集[[210](#bib.bib210)]^(14)^(14)14[https://github.com/liuchunsense/UVSD](https://github.com/liuchunsense/UVSD)：基于无人机的车辆分割数据集（UVSD）是一个大规模的基准对象检测、计数和分割数据集。该数据集包含5,874张图像，具有98,600个高质量实例级语义注释的目标实例。这些图像由DJI
    Matrice 200四轴飞行器配备的Zenmuse X5S云台和相机捕捉，图像尺寸从$960\times 540$到$5280\times 2970$像素不等。特别地，UVSD具有多种格式的注释，包括像素级语义、OBB和HBB。
- en: '![Refer to caption](img/be67c289cfab2446f4dd4ab2f491bb99.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/be67c289cfab2446f4dd4ab2f491bb99.png)'
- en: 'Figure 9: Visual samples of annotated images taken from benchmark datasets.
    The first, second, and third rows stand for UAVDT, VisDrone, and Okutama-Action
    datasets, respectively.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：来自基准数据集的标注图像的视觉样本。第一、第二和第三行分别代表UAVDT、VisDrone和Okutama-Action数据集。
- en: VII Experiment Results And Analysis
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 实验结果与分析
- en: 'In this section, we take four benchmark datasets, including VisDrone, UAVDT,
    Okutama-Action and Stanford UAV datasets, to illustrate the performance of representative
    object detection and tracking methods. Fig. [9](#S6.F9 "Figure 9 ‣ VI UAV-based
    Benchmark Dataset ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows examples of annotated images in these four datasets.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们采用了四个基准数据集，包括VisDrone、UAVDT、Okutama-Action和Stanford UAV数据集，以展示代表性对象检测和跟踪方法的性能。图[9](#S6.F9
    "图 9 ‣ VI 基于无人机的基准数据集 ‣ 基于无人机的对象检测与跟踪的深度学习：综述")显示了这些四个数据集中标注图像的示例。
- en: VII-A Evaluation of Object Detection from UAV-borne Images
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A 基于无人机图像的对象检测评估
- en: For object detection from UAV-borne images, the common performance metrics are
    Average Precision (AP) and Average Recall (AR). AP is used as a global measure.
    More precisely, the value of AP and AR are related to the rate between the overlap
    of the detection bounding box and the ground-truth box exceeds a certain percentages.
    The most frequently used is $AP^{IoU=0.50:0.05:0.95}$, $AP^{IoU=0.50}$, $AP^{IoU=0.75}$,
    $AR^{max=1}$, $AR^{max=10}$, $AR^{max=100}$ and $AR^{max=500}$. Specifically,
    $AP^{IoU=0.50:0.05:0.95}$ denotes the mean average precision (mAP), that is, the
    average value of the multiple intersection over union (IOU) threshold, which is
    defined as the geometric overlap between predictions and ground truths, of all
    categories with step size of 0.05\. $AP^{IoU=0.50}$ and $AP^{IoU=0.75}$ are computed
    at a certain IOU threshold over all categories. Moreover, $AP^{s}=AP^{small}$,
    $AP^{m}=AP^{medium}$, $AP^{l}=AP^{large}$ represent the average precision at different
    scales. $AR^{max=1}$, $AR^{max=10}$, $AR^{max=100}$ , and $AR^{max=500}$ are the
    maximum recalls number of 1, 10, 100, and 500 detected objects in each image.
    For more details please refer to [[21](#bib.bib21), [201](#bib.bib201)].
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 对于来自无人机图像的目标检测，常用的性能指标是平均精度（AP）和平均召回率（AR）。AP 作为全局度量被使用。更准确地说，AP 和 AR 的值与检测边界框与真实框重叠的比例超过某些百分比有关。最常用的是
    $AP^{IoU=0.50:0.05:0.95}$、$AP^{IoU=0.50}$、$AP^{IoU=0.75}$、$AR^{max=1}$、$AR^{max=10}$、$AR^{max=100}$
    和 $AR^{max=500}$。具体来说，$AP^{IoU=0.50:0.05:0.95}$ 表示平均精度（mAP），即在所有类别中，多个交并比（IOU）阈值的平均值，其定义为预测与真实值之间的几何重叠，步长为
    0.05。$AP^{IoU=0.50}$ 和 $AP^{IoU=0.75}$ 是在所有类别中在特定 IOU 阈值下计算的。此外，$AP^{s}=AP^{small}$、$AP^{m}=AP^{medium}$、$AP^{l}=AP^{large}$
    表示在不同尺度下的平均精度。$AR^{max=1}$、$AR^{max=10}$、$AR^{max=100}$ 和 $AR^{max=500}$ 是每张图像中检测到的
    1、10、100 和 500 个对象的最大召回数。有关更多详细信息，请参考 [[21](#bib.bib21), [201](#bib.bib201)]。
- en: 'Table [V](#S7.T5 "TABLE V ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") lists quantitative results of several state-of-the-art
    detection methods. Their experiment results are distributed in different UAV object
    detection datasets and most of them just use $AP=AP^{IoU=0.50:0.05:0.95}$ as the
    only evaluation criteria. To be fair, the performance of these works is compared
    according to their AP value under a specific dataset.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [V](#S7.T5 "TABLE V ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") 列出了几种最先进检测方法的定量结果。它们的实验结果分布在不同的无人机目标检测数据集上，其中大多数仅使用
    $AP=AP^{IoU=0.50:0.05:0.95}$ 作为唯一的评估标准。为了公平起见，这些工作的性能是根据其在特定数据集下的 AP 值进行比较的。'
- en: 'VisDrone dataset: This dataset has severe sample imbalance and occlusion problems
    between small objects. NDFT with domain-robust features, which transfers the learned
    NDFT through UAVDT to VisDrone dataset, achieves the best performance among all
    comparative methods, i.e., 52.77% AP score on the VisDrone-DET validation set
    due to the testing set has been closed after the ICCV2019 conference. The possible
    reason is that NDFT could achieve a substantial gain in robustness to many UAV-specific
    nuisances, such as varying flying altitudes, adverse weather conditions, dynamically
    changing viewing angles, etc. SAMFR with spatial-refinement module and receptive
    field expansion block (RFEB), MPFPN with parallel branch becomes the second and
    third with 33.72% and 29.05% AP scores.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: VisDrone 数据集：该数据集存在严重的样本不平衡和小对象遮挡问题。具有领域鲁棒特征的 NDFT，通过无人机检测传递到 VisDrone 数据集，在所有比较方法中表现最佳，即在
    VisDrone-DET 验证集上获得了 52.77% 的 AP 分数，因为测试集在 ICCV2019 会议后已关闭。可能的原因是 NDFT 在面对许多无人机特有的干扰（如飞行高度变化、不利天气条件、动态变化的视角等）时能够显著提高鲁棒性。具有空间精炼模块和感受野扩展块（RFEB）的
    SAMFR，具有并行分支的 MPFPN 分别以 33.72% 和 29.05% 的 AP 分数成为第二和第三。
- en: 'Table [V](#S7.T5 "TABLE V ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") shows the results of 10 baseline methods in
    the VisDrone-DET2019 Challenge, i.e., FPN [[131](#bib.bib131)], R-FCN [[19](#bib.bib19)],
    Faster R-CNN (FRCNN) [[20](#bib.bib20)], SSD [[24](#bib.bib24)], Cascade CNN [[128](#bib.bib128)],
    RetinaNet [[130](#bib.bib130)], CornetNet [[211](#bib.bib211)], RefineNet [[212](#bib.bib212)],
    DetNet [[213](#bib.bib213)], and Light Faster R-CNN (Light-RCNN) [[214](#bib.bib214)].
    The samples are in strict accordance, with 6,471 for training, 548 for validation
    and 1,580 for testing. For the parameters of these networks, we adjust them within
    a reasonable range or directly adopted the default values. CornerNet achieves
    the best performance, while SSD^∗ performs the worst.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [V](#S7.T5 "TABLE V ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") 显示了 VisDrone-DET2019 挑战赛中 10 种基线方法的结果，即 FPN
    [[131](#bib.bib131)]、R-FCN [[19](#bib.bib19)]、Faster R-CNN (FRCNN) [[20](#bib.bib20)]、SSD
    [[24](#bib.bib24)]、Cascade CNN [[128](#bib.bib128)]、RetinaNet [[130](#bib.bib130)]、CornetNet
    [[211](#bib.bib211)]、RefineNet [[212](#bib.bib212)]、DetNet [[213](#bib.bib213)]
    和 Light Faster R-CNN (Light-RCNN) [[214](#bib.bib214)]。样本严格一致，训练集 6,471 个，验证集
    548 个，测试集 1,580 个。对于这些网络的参数，我们在合理范围内调整或直接采用默认值。CornerNet 表现最佳，而 SSD^∗ 表现最差。'
- en: 'UAVDT dataset: With different locations but similar environments to the VisDrone
    dataset, UAVDT has higher complexity due to its images collected from a variety
    of scenes. Moreover, the weather condition would increase the difficulty of single,
    multiple, or overlapping small object detection. D2Det published in CVPR2020 with
    dense local regression achieves the best performance among all methods, i.e.,
    56.92% AP score on the testing set. NDFT with domain-robust features, FPN employed
    ResNet101 becomes the second and third with 52.03% and 49.05% AP scores. We also
    report the detection results of 8 baseline DL-based networks, including R-FCN
    [[19](#bib.bib19)], Faster R-CNN (FRCNN) [[20](#bib.bib20)], FRCNN plus FPN [[131](#bib.bib131)],
    SSD [[24](#bib.bib24)], Cascade CNN [[128](#bib.bib128)], Reverse connection with
    Objectness prior Networks (RON) [[130](#bib.bib130)], ClusDet [[215](#bib.bib215)],
    and DMDet [[216](#bib.bib216)], are shown in Table [VI](#S7.T6 "TABLE VI ‣ VII-A
    Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment Results
    And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey").
    Among them, the image size for UAVDT was $1024\times 540$ pixels, while the sample
    size of some methods varied. The network parameters were the same as the VisDrone
    dataset. FPN achieves the best performance, while RON performs the worst.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'UAVDT 数据集：虽然位置不同，但环境类似于 VisDrone 数据集，UAVDT 由于其图像来自各种场景，因此具有更高的复杂性。此外，天气条件会增加对单个、多重或重叠小物体检测的难度。D2Det
    在 CVPR2020 上发布，采用密集局部回归，在所有方法中表现最佳，即测试集上的 56.92% AP 分数。具有领域鲁棒特征的 NDFT，FPN 采用 ResNet101
    以 52.03% 和 49.05% AP 分数分别排名第二和第三。我们还报告了 8 个基线 DL 基于网络的检测结果，包括 R-FCN [[19](#bib.bib19)]、Faster
    R-CNN (FRCNN) [[20](#bib.bib20)]、FRCNN 加 FPN [[131](#bib.bib131)]、SSD [[24](#bib.bib24)]、Cascade
    CNN [[128](#bib.bib128)]、带有 Objectness 优先网络 (RON) [[130](#bib.bib130)]、ClusDet
    [[215](#bib.bib215)] 和 DMDet [[216](#bib.bib216)]，结果见表 [VI](#S7.T6 "TABLE VI ‣
    VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment Results
    And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey")。其中，UAVDT
    的图像尺寸为 $1024\times 540$ 像素，而某些方法的样本尺寸有所不同。网络参数与 VisDrone 数据集相同。FPN 达到了最佳性能，而 RON
    表现最差。'
- en: 'TABLE V: Performance comparisons of UAV exclusive detection networks and classic
    detection networks. The best performers are highlighted in bold.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 表 V：无人机专用检测网络与经典检测网络的性能比较。最佳表现者以**粗体**标出。
- en: '| Method | Network | Train/Test | Image Size | AP | AP_50 | AP_75 | AR_1 |
    AR_10 | AR_100 | AR_500 | Exp. Data |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 网络 | 训练/测试 | 图像大小 | AP | AP_50 | AP_75 | AR_1 | AR_10 | AR_100 | AR_500
    | 试验数据 |'
- en: '| Yang et al[[59](#bib.bib59)] | VGG-16 | 3,475/869 | - | 92.00 | - | - | -
    | - | - | - | Own vehicle |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al[[59](#bib.bib59)] | VGG-16 | 3,475/869 | - | 92.00 | - | - | -
    | - | - | - | 自有车辆 |'
- en: '| UAV-YOLO[[16](#bib.bib16)] | YOLOv3 | 3,776/630 | $608\times 608$ | 90.86
    | - | - | - | - | - | - | UAV123+Own |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| UAV-YOLO[[16](#bib.bib16)] | YOLOv3 | 3,776/630 | $608\times 608$ | 90.86
    | - | - | - | - | - | - | UAV123+Own |'
- en: '| FS-SSD[[66](#bib.bib66)] | VGG16 | 989/459 | $512\times 512$ | 89.52 | -
    | - | - | - | - | - | CARPK |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| FS-SSD[[66](#bib.bib66)] | VGG16 | 989/459 | $512\times 512$ | 89.52 | -
    | - | - | - | - | - | CARPK |'
- en: '| FS-SSD[[66](#bib.bib66)] | VGG16 | 69,673/53,224 | $512\times 512$ | 65.84
    |  |  |  |  |  |  | Stanford Drone |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| FS-SSD[[66](#bib.bib66)] | VGG16 | 69,673/53,224 | $512\times 512$ | 65.84
    |  |  |  |  |  |  | Stanford Drone |'
- en: '| Yang et al[[59](#bib.bib59)] | VGG-16 | 3,500/831 | $320\times 320$ | 90.40
    | - | - | - | - | - | - | Stanford Drone |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al[[59](#bib.bib59)] | VGG-16 | 3,500/831 | $320\times 320$ | 90.40
    | - | - | - | - | - | - | Stanford Drone |'
- en: '| MSOA-Net[[56](#bib.bib56)] | ResNet50 | 3,564/1,725 | $1333\times 800$ |
    77.00 | 91.50 | 83.30 |  |  |  |  | UAVDT |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| MSOA-Net[[56](#bib.bib56)] | ResNet50 | 3,564/1,725 | $1333\times 800$ |
    77.00 | 91.50 | 83.30 |  |  |  |  | UAVDT |'
- en: '| GDF-Net[[61](#bib.bib61)] | ResNet50 | 11,915/16,580 | $1200\times 675$ |
    15.40 | 26.10 | 17.00 | 13.20 | 23.10 | 27.60 |  | UAVDT |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| GDF-Net[[61](#bib.bib61)] | ResNet50 | 11,915/16,580 | $1200\times 675$ |
    15.40 | 26.10 | 17.00 | 13.20 | 23.10 | 27.60 |  | UAVDT |'
- en: '| DSYolov3[[55](#bib.bib55)] | Yolov3 | 24,143/16,592 | $1200\times 540$ |
    9.80 | 23.40 | 5.00 | - | - | - |  | UAVDT |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| DSYolov3[[55](#bib.bib55)] | Yolov3 | 24,143/16,592 | $1200\times 540$ |
    9.80 | 23.40 | 5.00 | - | - | - |  | UAVDT |'
- en: '| ClusDet[[57](#bib.bib57)] | ResNeXt101 | 23,238/15,069 | $1080\times 540$
    | 13.70 | 25.50 | 12.50 | - | - | - | - | UAVDT |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| ClusDet[[57](#bib.bib57)] | ResNeXt101 | 23,238/15,069 | $1080\times 540$
    | 13.70 | 25.50 | 12.50 | - | - | - | - | UAVDT |'
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 23,258/15,069 | $1080\times 540$ |
    17.80 | 30.40 | 19.70 | - | - | - | - | UAVDT |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 23,258/15,069 | $1080\times 540$ |
    17.80 | 30.40 | 19.70 | - | - | - | - | UAVDT |'
- en: '| D2Det[[74](#bib.bib74)] | ResNet101 | 23,258/15,069 | $1,333\times 800$ |
    56.92 | - | - | - | - | - | - | UAVDT |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| D2Det[[74](#bib.bib74)] | ResNet101 | 23,258/15,069 | $1,333\times 800$ |
    56.92 | - | - | - | - | - | - | UAVDT |'
- en: '| NDFT[[69](#bib.bib69)] | ResNet101 | 23,258/15,069 | $1,333\times 800$ |
    52.03 | - | - | - | - | - | - | UAVDT |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| NDFT[[69](#bib.bib69)] | ResNet101 | 23,258/15,069 | $1,333\times 800$ |
    52.03 | - | - | - | - | - | - | UAVDT |'
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 23,258/15,069 | $1,080\times 540$ |
    17.80 | 30.4 | 19.7 | - | - | - | - | UAVDT |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 23,258/15,069 | $1,080\times 540$ |
    17.80 | 30.4 | 19.7 | - | - | - | - | UAVDT |'
- en: '| DNOD[[79](#bib.bib79)] | YOLOv4 | 23,258/15,069 | $1080\times 540$ | 14.20
    | 31.90 | 11.00 |  | - | - | - | UAVDT |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| DNOD[[79](#bib.bib79)] | YOLOv4 | 23,258/15,069 | $1080\times 540$ | 14.20
    | 31.90 | 11.00 |  | - | - | - | UAVDT |'
- en: '| DNOD[[79](#bib.bib79)] | EfficientDet-D7 | 23,258/15,069 | $1080\times 540$
    | 12.90 | 32.00 | 10.90 | - | - | - | - | UAVDT |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| DNOD[[79](#bib.bib79)] | EfficientDet-D7 | 23,258/15,069 | $1080\times 540$
    | 12.90 | 32.00 | 10.90 | - | - | - | - | UAVDT |'
- en: '| FPN^∗[[131](#bib.bib131)] | FPN | 23,258/15,069 | $1080\times 540$ | 49.05
    | - |  | - | - | - | - | UAVDT |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| FPN^∗[[131](#bib.bib131)] | FPN | 23,258/15,069 | $1080\times 540$ | 49.05
    | - |  | - | - | - | - | UAVDT |'
- en: '| RON[[217](#bib.bib217)] | VGG16 | 23,258/15,069 | $1080\times 540$ | 5.0
    | 15.9 | 1.7 | - | - | - | - | UAVDT |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| RON[[217](#bib.bib217)] | VGG16 | 23,258/15,069 | $1080\times 540$ | 5.0
    | 15.9 | 1.7 | - | - | - | - | UAVDT |'
- en: '| RetinaNet[[130](#bib.bib130)] | RetinaNet | 23,258/15,069 | $1080\times 540$
    | 33.95 | - | - | - | - | - | - | UAVDT |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| RetinaNet[[130](#bib.bib130)] | RetinaNet | 23,258/15,069 | $1080\times 540$
    | 33.95 | - | - | - | - | - | - | UAVDT |'
- en: '| ECas_RCNN[[54](#bib.bib54)] | ResNet50 | 6,371/521 | $1450\times 800$ | 28.40
    | - | - | - | - | - | - | VisDrone-Val |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| ECas_RCNN[[54](#bib.bib54)] | ResNet50 | 6,371/521 | $1450\times 800$ | 28.40
    | - | - | - | - | - | - | VisDrone-Val |'
- en: '| GDF-Net[[61](#bib.bib61)] | ResNet50 | 6,471/11,610 | $1200\times 675$ |
    18.20 | 30.80 | 19.20 | 8.10 | 24.10 | 28.70 |  | VisDrone-Val |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| GDF-Net[[61](#bib.bib61)] | ResNet50 | 6,471/11,610 | $1200\times 675$ |
    18.20 | 30.80 | 19.20 | 8.10 | 24.10 | 28.70 |  | VisDrone-Val |'
- en: '| HRDNet[[70](#bib.bib70)] | ResNeXt50+101 | 3,564/1,725 | $3800\times 3800$
    | 35.51 | 62.00 | 35.13 | 0.39 | 3.38 | 30.91 | 46.62 | VisDrone-Val |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| HRDNet[[70](#bib.bib70)] | ResNeXt50+101 | 3,564/1,725 | $3800\times 3800$
    | 35.51 | 62.00 | 35.13 | 0.39 | 3.38 | 30.91 | 46.62 | VisDrone-Val |'
- en: '| D-A-FS SSD[[63](#bib.bib63)] | - | - | - | - | - | - | - | - | - | - | VisDrone-Val
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| D-A-FS SSD[[63](#bib.bib63)] | - | - | - | - | - | - | - | - | - | - | VisDrone-Val
    |'
- en: '| ClusDet[[57](#bib.bib57)] | ResNeXt101 | 6,471/548 | $2000\times 1500$ |
    32.40 | 56.20 | 31.60 | - | - | - | - | VisDrone-Val |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| ClusDet[[57](#bib.bib57)] | ResNeXt101 | 6,471/548 | $2000\times 1500$ |
    32.40 | 56.20 | 31.60 | - | - | - | - | VisDrone-Val |'
- en: '| CenterNet[[58](#bib.bib58)] | HourGlass-104 | 3,564/1,725 | $1024\times 1024$
    | 21.58 | 48.09 | 16.76 | 12.04 | 29.60 | 39.63 | 40.42 | VisDrone-Val |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| CenterNet[[58](#bib.bib58)] | HourGlass-104 | 3,564/1,725 | $1024\times 1024$
    | 21.58 | 48.09 | 16.76 | 12.04 | 29.60 | 39.63 | 40.42 | VisDrone-Val |'
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 6,471/548 | $2000\times 1500$ | 30.30
    | 51.80 | 30.90 | - | - | - | - | VisDrone-Val |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 6,471/548 | $2000\times 1500$ | 30.30
    | 51.80 | 30.90 | - | - | - | - | VisDrone-Val |'
- en: '| NDFT[[69](#bib.bib69)] | ResNet101 | 6,471/548 | $2,000\times 1,500$ | 52.77
    | - | - | - | - | - | - | VisDrone-Val |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| NDFT[[69](#bib.bib69)] | ResNet101 | 6,471/548 | $2,000\times 1,500$ | 52.77
    | - | - | - | - | - | - | VisDrone-Val |'
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 6,471/548 | $2,000\times 1,500$ | 24.60
    | 44.40 | 24.10 | - | - | - | - | VisDrone-Val |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 6,471/548 | $2,000\times 1,500$ | 24.60
    | 44.40 | 24.10 | - | - | - | - | VisDrone-Val |'
- en: '| MPFPN[[73](#bib.bib73)] | ResNet101 | 6,471/1,580 | $1440\times 800$ | 29.05
    | 54.38 | 26.99 | 0.55 | 5.81 | 35.57 | 45.69 | VisDrone-Val |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| MPFPN[[73](#bib.bib73)] | ResNet101 | 6,471/1,580 | $1440\times 800$ | 29.05
    | 54.38 | 26.99 | 0.55 | 5.81 | 35.57 | 45.69 | VisDrone-Val |'
- en: '| SAMFR[[67](#bib.bib67)] | DetNet59 | 6,471/548 | $512\times 512$ | 33.72
    | 58.62 | 33.88 | 0.53 | 3.40 | 22.60 | 46.03 | VisDrone-Val |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| SAMFR[[67](#bib.bib67)] | DetNet59 | 6,471/548 | $512\times 512$ | 33.72
    | 58.62 | 33.88 | 0.53 | 3.40 | 22.60 | 46.03 | VisDrone-Val |'
- en: '| DANN[[77](#bib.bib77)] | RetinaNet | 6,471/548 | $1500\times 1000$ | 11.19
    | 25.65 | 8.78 | 0.56 | 4.87 | 17.19 | 24.09 | VisDrone-Val |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| DANN[[77](#bib.bib77)] | RetinaNet | 6,471/548 | $1500\times 1000$ | 11.19
    | 25.65 | 8.78 | 0.56 | 4.87 | 17.19 | 24.09 | VisDrone-Val |'
- en: '| Cas_RCNN+FPN[[80](#bib.bib80)] | ResNet101 | 4,960/1,534 | $1500\times 2000$
    | 20.46 | 38.58 | 18.83 | 1.32 | 11.32 | 25.82 | 25.84 | VisDrone-Val |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| Cas_RCNN+FPN[[80](#bib.bib80)] | ResNet101 | 4,960/1,534 | $1500\times 2000$
    | 20.46 | 38.58 | 18.83 | 1.32 | 11.32 | 25.82 | 25.84 | VisDrone-Val |'
- en: '| DNOD[[79](#bib.bib79)] | YOLOv4 | 6,471/1,610 | $1260\times 765$ | 54.88
    | - | - | - | - | - | - | VisDrone-Val |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| DNOD[[79](#bib.bib79)] | YOLOv4 | 6,471/1,610 | $1260\times 765$ | 54.88
    | - | - | - | - | - | - | VisDrone-Val |'
- en: '| DNOD[[79](#bib.bib79)] | EfficientDet-D7 | 6,471/1,610 | $1260\times 765$
    | 53.76 | - | - | - | - | - | - | VisDrone-Val |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| DNOD[[79](#bib.bib79)] | EfficientDet-D7 | 6,471/1,610 | $1260\times 765$
    | 53.76 | - | - | - | - | - | - | VisDrone-Val |'
- en: '| RRNet[[53](#bib.bib53)] | HourGlass | 6,741/1580 | $512\times 512$ | 29.13
    | 55.82 | 27.23 | 1.02 | 8.50 | 35.19 | 46.05 | VisDrone-Det |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| RRNet[[53](#bib.bib53)] | HourGlass | 6,741/1580 | $512\times 512$ | 29.13
    | 55.82 | 27.23 | 1.02 | 8.50 | 35.19 | 46.05 | VisDrone-Det |'
- en: '| DSYolov3[[55](#bib.bib55)] | Yolov3 | 6,471/548 | $1920\times 1080$ | 22.30
    | 44.50 | 20.30 | - | - | - | - | VisDrone-Det |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| DSYolov3[[55](#bib.bib55)] | Yolov3 | 6,471/548 | $1920\times 1080$ | 22.30
    | 44.50 | 20.30 | - | - | - | - | VisDrone-Det |'
- en: '| SAMFR[[67](#bib.bib67)] | DetNet59 | 6,471/1,580 | $512\times 512$ | 20.18
    | 40.03 | 18.42 | 0.46 | 3.49 | 21.6 | 30.82 | VisDrone-Det |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| SAMFR[[67](#bib.bib67)] | DetNet59 | 6,471/1,580 | $512\times 512$ | 20.18
    | 40.03 | 18.42 | 0.46 | 3.49 | 21.6 | 30.82 | VisDrone-Det |'
- en: '| SyNet[[71](#bib.bib71)] | CenterNet | 6,471/1,580 | $2000\times 1500$ | 25.10
    | 48.40 | 26.20 | - | - | - | - | VisDrone-Det |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| SyNet[[71](#bib.bib71)] | CenterNet | 6,471/1,580 | $2000\times 1500$ | 25.10
    | 48.40 | 26.20 | - | - | - | - | VisDrone-Det |'
- en: '| SlimYOLOv3[[65](#bib.bib65)] | YOLOv3-SPP3-90 | 6,471/548 | $832\times 832$
    | 23.90 | - | - | - | - | - | - | VisDrone-Det |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| SlimYOLOv3[[65](#bib.bib65)] | YOLOv3-SPP3-90 | 6,471/548 | $832\times 832$
    | 23.90 | - | - | - | - | - | - | VisDrone-Det |'
- en: '| Zhang et al[[62](#bib.bib62)] | ResNet50+RPN | 6,471/1580 | $2000\times 1500$
    | 22.61 | 45.16 | 19.94 | 0.42 | 2.84 | 17.1 | 35.27 | VisDrone-Det |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al[[62](#bib.bib62)] | ResNet50+RPN | 6,471/1580 | $2000\times 1500$
    | 22.61 | 45.16 | 19.94 | 0.42 | 2.84 | 17.1 | 35.27 | VisDrone-Det |'
- en: '| CornerNet^∗[[211](#bib.bib211)] | CornetNet | 6,471/1,580 | $2000\times 1500$
    | 17.41 | 34.12 | 15.78 | 0.39 | 3.32 | 24.37 | 26.11 | VisDrone-Det |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| CornerNet^∗[[211](#bib.bib211)] | CornetNet | 6,471/1,580 | $2000\times 1500$
    | 17.41 | 34.12 | 15.78 | 0.39 | 3.32 | 24.37 | 26.11 | VisDrone-Det |'
- en: '| FPN^∗[[131](#bib.bib131)] | FPN | 6,471/1,580 | $2000\times 1500$ | 16.51
    | 32.20 | 14.91 | 0.33 | 3.03 | 20.72 | 24.93 | VisDrone-Det |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| FPN^∗[[131](#bib.bib131)] | FPN | 6,471/1,580 | $2000\times 1500$ | 16.51
    | 32.20 | 14.91 | 0.33 | 3.03 | 20.72 | 24.93 | VisDrone-Det |'
- en: '| Light-RCNN^∗[[214](#bib.bib214)] | Light-RCNN | 6,471/1580 | $2000\times
    1500$ | 16.53 | 32.78 | 15.13 | 0.35 | 3.16 | 23.09 | 25.07 | VisDrone-Det |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Light-RCNN^∗[[214](#bib.bib214)] | Light-RCNN | 6,471/1580 | $2000\times
    1500$ | 16.53 | 32.78 | 15.13 | 0.35 | 3.16 | 23.09 | 25.07 | VisDrone-Det |'
- en: '| Cas_RCNN^∗[[128](#bib.bib128)] | Cascade R-CNN | 6,471/1,580 | $2000\times
    1500$ | 16.09 | 31.91 | 15.01 | 0.28 | 2.79 | 21.37 | 28.43 | VisDrone-Det |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| Cas_RCNN^∗[[128](#bib.bib128)] | Cascade R-CNN | 6,471/1,580 | $2000\times
    1500$ | 16.09 | 31.91 | 15.01 | 0.28 | 2.79 | 21.37 | 28.43 | VisDrone-Det |'
- en: '| DetNet59^∗[[213](#bib.bib213)] | DetNet-59 | 6,471/1,580 | $2000\times 1500$
    | 15.26 | 29.23 | 14.34 | 0.26 | 2.57 | 20.87 | 22.28 | VisDrone-Det |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| DetNet59^∗[[213](#bib.bib213)] | DetNet-59 | 6,471/1,580 | $2000\times 1500$
    | 15.26 | 29.23 | 14.34 | 0.26 | 2.57 | 20.87 | 22.28 | VisDrone-Det |'
- en: '| RefineNet[[212](#bib.bib212)] | RefineNet | 6,471/1,580 | $2000\times 1500$
    | 14.90 | 28.76 | 14.08 | 0.24 | 2.41 | 18.13 | 25.69 | VisDrone-Det |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| RefineNet[[212](#bib.bib212)] | RefineNet | 6,471/1,580 | $2000\times 1500$
    | 14.90 | 28.76 | 14.08 | 0.24 | 2.41 | 18.13 | 25.69 | VisDrone-Det |'
- en: '| RetinaNet^∗[[130](#bib.bib130)] | RetinaNet | 6,471/1,580 | $2000\times 1500$
    | 11.81 | 21.37 | 11.62 | 0.21 | 1.21 | 5.31 | 19.29 | VisDrone-Det |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| RetinaNet^∗[[130](#bib.bib130)] | RetinaNet | 6,471/1,580 | $2000\times 1500$
    | 11.81 | 21.37 | 11.62 | 0.21 | 1.21 | 5.31 | 19.29 | VisDrone-Det |'
- en: '| R-FCN^∗[[19](#bib.bib19)] | R-FCN | 6,471/1,580 | $2000\times 1500$ | 7.20
    | 15.17 | 6.38 | 0.88 | 5.35 | 12.04 | 13.95 | VisDrone-Det |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| R-FCN^∗[[19](#bib.bib19)] | R-FCN | 6,471/1,580 | $2000\times 1500$ | 7.20
    | 15.17 | 6.38 | 0.88 | 5.35 | 12.04 | 13.95 | VisDrone-Det |'
- en: '| FRCNN^∗[[20](#bib.bib20)] | FRCNN | 6,471/1,580 | $2000\times 1500$ | 3.55
    | 8.75 | 2.43 | 0.66 | 3.49 | 6.51 | 6.53 | VisDrone-Det |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| FRCNN^∗[[20](#bib.bib20)] | FRCNN | 6,471/1,580 | $2000\times 1500$ | 3.55
    | 8.75 | 2.43 | 0.66 | 3.49 | 6.51 | 6.53 | VisDrone-Det |'
- en: '| SSD^∗[[24](#bib.bib24)] | SSD | 6,471/1,580 | $2000\times 1500$ | 2.52 |
    4.78 | 2.47 | 0.58 | 2.81 | 4.51 | 6.41 | VisDrone-Det |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| SSD^∗[[24](#bib.bib24)] | SSD | 6,471/1,580 | $2000\times 1500$ | 2.52 |
    4.78 | 2.47 | 0.58 | 2.81 | 4.51 | 6.41 | VisDrone-Det |'
- en: 'TABLE VI: Object detection results on the UAVDT-DET testing set. The best performers
    are highlighted in bold.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：UAVDT-DET 测试集上的目标检测结果。表现最好的方法以粗体突出显示。
- en: '| Method | Backbone | Train/Test | Image Size | AP | AP_50 | AP_75 | AP_s |
    AP_m | AP_l | Exp. Data |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 主干网络 | 训练/测试 | 图像尺寸 | AP | AP_50 | AP_75 | AP_s | AP_m | AP_l | 实验数据
    |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| R-FCN [[19](#bib.bib19)] | ResNet50 | 23,258/15,069 | $1080\times 540$ |
    7.0 | 17.5 | 3.9 | 4.4 | 14.7 | 12.1 | UAVDT |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| R-FCN [[19](#bib.bib19)] | ResNet50 | 23,258/15,069 | $1080\times 540$ |
    7.0 | 17.5 | 3.9 | 4.4 | 14.7 | 12.1 | UAVDT |'
- en: '| SSD [[24](#bib.bib24)] | VGG16 | 23,258/15,069 | $1080\times 540$ | 9.3 |
    21.4 | 6.7 | 7.1 | 17.1 | 12.0 | UAVDT |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| SSD [[24](#bib.bib24)] | VGG16 | 23,258/15,069 | $1080\times 540$ | 9.3 |
    21.4 | 6.7 | 7.1 | 17.1 | 12.0 | UAVDT |'
- en: '| FRCNN [[20](#bib.bib20)] | VGG16 | 23,258/15,069 | $1080\times 540$ | 5.8
    | 17.4 | 2.5 | 3.8 | 12.3 | 9.4 | UAVDT |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| FRCNN [[20](#bib.bib20)] | VGG16 | 23,258/15,069 | $1080\times 540$ | 5.8
    | 17.4 | 2.5 | 3.8 | 12.3 | 9.4 | UAVDT |'
- en: '| FRCNN [[20](#bib.bib20)]+FPN [[131](#bib.bib131)] | ResNet50 | 23,258/15,069
    | $1080\times 540$ | 11.0 | 23.4 | 8.4 | 8.1 | 20.2 | 26.5 | UAVDT |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| FRCNN [[20](#bib.bib20)]+FPN [[131](#bib.bib131)] | ResNet50 | 23,258/15,069
    | $1080\times 540$ | 11.0 | 23.4 | 8.4 | 8.1 | 20.2 | 26.5 | UAVDT |'
- en: '| ClusDet [[215](#bib.bib215)] | ResNet50 | -/25,427 | $1080\times 540$ | 13.7
    | 26.5 | 12.5 | 9.1 | 25.1 | 31.2 | UAVDT |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| ClusDet [[215](#bib.bib215)] | ResNet50 | -/25,427 | $1080\times 540$ | 13.7
    | 26.5 | 12.5 | 9.1 | 25.1 | 31.2 | UAVDT |'
- en: '| DMDet [[216](#bib.bib216)] | ResNet50 | -/32,764 | $1080\times 540$ | 14.7
    | 24.6 | 16.3 | 9.3 | 26.2 | 35.2 | UAVDT |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| DMDet [[216](#bib.bib216)] | ResNet50 | -/32,764 | $1080\times 540$ | 14.7
    | 24.6 | 16.3 | 9.3 | 26.2 | 35.2 | UAVDT |'
- en: 'TABLE VII: Performance comparisons of UAV exclusive detection networks and
    classic detection networks for the VisDrone-VID testing set. The best performers
    are highlighted in bold.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：VisDrone-VID 测试集上 UAV 专用检测网络与经典检测网络的性能比较。表现最好的方法以粗体突出显示。
- en: '| Method | Framework | Train/Test | Image Size | AP | AP_50 | AP_75 | AR_1
    | AR_10 | AR_100 | AR_500 | Exp. Data |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 框架 | 训练/测试 | 图像尺寸 | AP | AP_50 | AP_75 | AR_1 | AR_10 | AR_100 | AR_500
    | 实验数据 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| TDFA[[120](#bib.bib120)] | FlowNet+Fea_Agg | 54,503/14,114 | $720\times 1280$
    | - | 87.18 | - | - | - | - | - | Okutama |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| TDFA[[120](#bib.bib120)] | FlowNet+Fea_Agg | 54,503/14,114 | $720\times 1280$
    | - | 87.18 | - | - | - | - | - | Okutama |'
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+RCN | 23,829/16,580 | $1024\times
    540$ | 13.30 | 36.40 | - | - | - | - | - | UAVDT |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| STDnet-ST[[121](#bib.bib121)] | STDnet+RCN | 23,829/16,580 | $1024\times
    540$ | 13.30 | 36.40 | - | - | - | - | - | UAVDT |'
- en: '| Zhang et al[[118](#bib.bib118)] | Cas_RCNN+IRR-PWC | 17,268/5,397 | $1280\times
    720$ | 65.20 | 88.80 | 74.60 | - | - | - | - | VisDrone-VID |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al[[118](#bib.bib118)] | Cas_RCNN+IRR-PWC | 17,268/5,397 | $1280\times
    720$ | 65.20 | 88.80 | 74.60 | - | - | - | - | VisDrone-VID |'
- en: '| STCA[[114](#bib.bib114)] | F-SSD+FCOS | 24,198/6,322 | - | 18.73 | 44.38
    | 12.68 | - | - | - | - | VisDrone-VID |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| STCA[[114](#bib.bib114)] | F-SSD+FCOS | 24,198/6,322 | - | 18.73 | 44.38
    | 12.68 | - | - | - | - | VisDrone-VID |'
- en: '| TDFA[[120](#bib.bib120)] | FlowNet+Fea_Agg | 24,201/2,819 | $720\times 1280$
    | 27.27 | 50.73 | 27.94 | - | - | - | - | VisDrone-VID |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| TDFA[[120](#bib.bib120)] | FlowNet+Fea_Agg | 24,201/2,819 | $720\times 1280$
    | 27.27 | 50.73 | 27.94 | - | - | - | - | VisDrone-VID |'
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+RCN | 24,201/6,635 | $1,920\times
    1,080$ | 7.50 | 22.40 | - | - | - | - | - | VisDrone-VID |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| STDnet-ST[[121](#bib.bib121)] | STDnet+RCN | 24,201/6,635 | $1,920\times
    1,080$ | 7.50 | 22.40 | - | - | - | - | - | VisDrone-VID |'
- en: '| FGFA^∗ [[135](#bib.bib135), [218](#bib.bib218)] | VGG16 | 24,198/6,322 |
    3840$\times$ 2160 | 18.33 | 39.71 | 14.39 | 10.09 | 26.25 | 34.49 | 34.89 | VisDrone-VID
    |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| FGFA^∗ [[135](#bib.bib135), [218](#bib.bib218)] | VGG16 | 24,198/6,322 |
    3840$\times$ 2160 | 18.33 | 39.71 | 14.39 | 10.09 | 26.25 | 34.49 | 34.89 | VisDrone-VID
    |'
- en: '| CFE-SSDv2 [[219](#bib.bib219), [218](#bib.bib218)] | SSD | 24,198/6,322 |
    3840$\times$ 2160 | 21.57 | 44.75 | 17.95 | 11.85 | 30.46 | 41.89 | 44.82 | VisDrone-VID
    |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| CFE-SSDv2 [[219](#bib.bib219), [218](#bib.bib218)] | SSD | 24,198/6,322 |
    3840$\times$ 2160 | 21.57 | 44.75 | 17.95 | 11.85 | 30.46 | 41.89 | 44.82 | VisDrone-VID
    |'
- en: '| D&T (R-FCN) [[173](#bib.bib173), [218](#bib.bib218)] | Hourglass | 24,198/6,322
    | 3840$\times$ 2160 | 17.04 | 35.37 | 14.11 | 10.47 | 25.76 | 31.86 | 32.03 |
    VisDrone-VID |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| D&T (R-FCN) [[173](#bib.bib173), [218](#bib.bib218)] | Hourglass | 24,198/6,322
    | 3840$\times$ 2160 | 17.04 | 35.37 | 14.11 | 10.47 | 25.76 | 31.86 | 32.03 |
    VisDrone-VID |'
- en: '| FPN^∗ [[131](#bib.bib131), [218](#bib.bib218)] | ResNet-101 | 24,198/6,322
    | 3840$\times$ 2160 | 16.72 | 39.12 | 11.80 | 5.56 | 20.48 | 28.42 | 28.42 | VisDrone-VID
    |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| FPN^∗ [[131](#bib.bib131), [218](#bib.bib218)] | ResNet-101 | 24,198/6,322
    | 3840$\times$ 2160 | 16.72 | 39.12 | 11.80 | 5.56 | 20.48 | 28.42 | 28.42 | VisDrone-VID
    |'
- en: '| CornerNet^∗ [[211](#bib.bib211), [218](#bib.bib218)] | Hourglass-59 | 24,198/6,322
    | 3840$\times$ 2160 | 16.49 | 35.79 | 12.89 | 9.47 | 24.07 | 30.68 | 30.68 | VisDrone-VID
    |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| CornerNet^∗ [[211](#bib.bib211), [218](#bib.bib218)] | Hourglass-59 | 24,198/6,322
    | 3840$\times$ 2160 | 16.49 | 35.79 | 12.89 | 9.47 | 24.07 | 30.68 | 30.68 | VisDrone-VID
    |'
- en: '| CenterNet^∗ [[129](#bib.bib129), [218](#bib.bib218)] | Hourglass | 24,198/6,322
    | 3840$\times$ 2160 | 15.75 | 34.53 | 12.10 | 8.90 | 22.80 | 29.20 | 29.20 | VisDrone-VID
    |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| CenterNet^∗ [[129](#bib.bib129), [218](#bib.bib218)] | Hourglass | 24,198/6,322
    | 3840$\times$ 2160 | 15.75 | 34.53 | 12.10 | 8.90 | 22.80 | 29.20 | 29.20 | VisDrone-VID
    |'
- en: '| Faster R-CNN^∗ [[20](#bib.bib20), [218](#bib.bib218)] | VGG16 | 24,198/6,322
    | 3840$\times$ 2160 | 14.46 | 31.80 | 11.20 | 8.55 | 21.31 | 26.77 | 26.77 | VisDrone-VID
    |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| Faster R-CNN^∗ [[20](#bib.bib20), [218](#bib.bib218)] | VGG16 | 24,198/6,322
    | 3840$\times$ 2160 | 14.46 | 31.80 | 11.20 | 8.55 | 21.31 | 26.77 | 26.77 | VisDrone-VID
    |'
- en: '| RD [[212](#bib.bib212), [220](#bib.bib220)] | RefineDet | 24,198/6,322 |
    3840$\times$ 2160 | 14.95 | 35.25 | 10.11 | 9.67 | 24.60 | 29.72 | 29.91 | VisDrone-VID
    |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| RD [[212](#bib.bib212), [220](#bib.bib220)] | RefineDet | 24,198/6,322 |
    3840$\times$ 2160 | 14.95 | 35.25 | 10.11 | 9.67 | 24.60 | 29.72 | 29.91 | VisDrone-VID
    |'
- en: '| RetinaNet_s [[130](#bib.bib130), [220](#bib.bib220)] | RetinaNet | 24,198/6,322
    | 3840$\times$ 2160 | 8.63 | 21.83 | 4.98 | 5.80 | 12.91 | 15.15 | 15.15 | VisDrone-VID
    |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| RetinaNet_s [[130](#bib.bib130), [220](#bib.bib220)] | RetinaNet | 24,198/6,322
    | 3840$\times$ 2160 | 8.63 | 21.83 | 4.98 | 5.80 | 12.91 | 15.15 | 15.15 | VisDrone-VID
    |'
- en: 'TABLE VIII: Video object detection results on the Okutama-Action and UAVDT
    testing set. “ #vid” is the number of videos that send to the detector. The best
    performers are highlighted in bold.'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VIII：Okutama-Action 和 UAVDT 测试集的视频目标检测结果。"#vid" 是发送给检测器的视频数量。表现最佳的结果以**粗体**标出。
- en: '| Method | Backbone | #vid | Image Size | AP_50 | Exp.Data | Method | Train/Test
    | Image Size | AP | AP_50 | Exp.Data |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 骨干网络 | #vid | 图像尺寸 | AP_50 | 实验数据 | 方法 | 训练/测试 | 图像尺寸 | AP | AP_50 |
    实验数据 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| SSD [[24](#bib.bib24)] | VGG | 10 | $960\times 540$ | 18.80 | Okutama-Action
    | Faster RCNN [[199](#bib.bib199)] | 23,829/76,215 | $1080\times 540$ | 6.6 |
    26.00 | UAVDT |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| SSD [[24](#bib.bib24)] | VGG | 10 | $960\times 540$ | 18.80 | Okutama-Action
    | Faster RCNN [[199](#bib.bib199)] | 23,829/76,215 | $1080\times 540$ | 6.6 |
    26.00 | UAVDT |'
- en: '| SSD [[24](#bib.bib24)] | ResNet50 | 10 | $608\times 608$ | 52.30 | Okutama-Action
    | SSD [[199](#bib.bib199)] | 23,829/76,215 | $1080\times 540$ | 6.0 | 23.50 |
    UAVDT |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| SSD [[24](#bib.bib24)] | ResNet50 | 10 | $608\times 608$ | 52.30 | Okutama-Action
    | SSD [[199](#bib.bib199)] | 23,829/76,215 | $1080\times 540$ | 6.0 | 23.50 |
    UAVDT |'
- en: '| R-FCN [[19](#bib.bib19)] | ResNet50 | 10 | $608\times 608$ | 53.50 | Okutama-Action
    | R-FCN [[199](#bib.bib199)] | 23,829/76,215 | $1080\times 540$ | 9.2 | 32.50
    | UAVDT |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| R-FCN [[19](#bib.bib19)] | ResNet50 | 10 | $608\times 608$ | 53.50 | Okutama-Action
    | R-FCN [[199](#bib.bib199)] | 23,829/76,215 | $1080\times 540$ | 9.2 | 32.50
    | UAVDT |'
- en: '| Retinanet [[130](#bib.bib130)] | ResNet50 | 10 | $608\times 608$ | 56.30
    | Okutama-Action | FGFA [[135](#bib.bib135)] | 23,829/76,215 | $1080\times 540$
    | 6.3 | 20.70 | UAVDT |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| Retinanet [[130](#bib.bib130)] | ResNet50 | 10 | $608\times 608$ | 56.30
    | Okutama-Action | FGFA [[135](#bib.bib135)] | 23,829/76,215 | $1080\times 540$
    | 6.3 | 20.70 | UAVDT |'
- en: '| YOLOv3_tiny [[221](#bib.bib221)] | DarkNet-53 | 10 | $608\times 608$ | 52.40
    | Okutama-Action | FPN[[131](#bib.bib131)] | 23,829/76,215 | $1080\times 540$
    | 11.8 | 29.70 | UAVDT |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| YOLOv3_tiny [[221](#bib.bib221)] | DarkNet-53 | 10 | $608\times 608$ | 52.40
    | Okutama-Action | FPN[[131](#bib.bib131)] | 23,829/76,215 | $1080\times 540$
    | 11.8 | 29.70 | UAVDT |'
- en: 'TABLE IX: Performance comparisons of UAV exclusive tracking networks and classic
    tracking networks for the VisDrone-MOT testing set taken MOTA, MOTP, etc as evaluation
    indexes. The best performers are highlighted in bold.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IX：基于 MOTA、MOTP 等评价指标对 UAV 专用跟踪网络和经典跟踪网络的性能比较。表现最佳的结果以**粗体**标出。
- en: '| Method | Framework | Train/Test(seq) | Image Size | MOTA | MOTP | IDF1 |
    FAF | MT | ML | FP | FN | IDS | FM | Exp. Data |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 框架 | 训练/测试(序列) | 图像尺寸 | MOTA | MOTP | IDF1 | FAF | MT | ML | FP | FN
    | IDS | FM | 实验数据 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '| TNT[[152](#bib.bib152)] | RetinaNet50 | 56/33 | $3840\times 2160$ | 48.6
    | - | 58.1 | - | 281 | 478 | 5,349 | 76,402 | 468 | - | VisDrone-MOT |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| TNT[[152](#bib.bib152)] | RetinaNet50 | 56/33 | $3840\times 2160$ | 48.6
    | - | 58.1 | - | 281 | 478 | 5,349 | 76,402 | 468 | - | VisDrone-MOT |'
- en: '| +TrackletNet |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| +TrackletNet |'
- en: '| HDHNet[[163](#bib.bib163)] | HRNet+DLA | 56/7 | $3840\times 2160$ | 32.9
    | 76.9 | 42.3 | - | - | - | 80,454 | 35,686 | 1,056 | 1,242 | VisDrone-MOT |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
  zh: '| HDHNet[[163](#bib.bib163)] | HRNet+DLA | 56/7 | $3840\times 2160$ | 32.9
    | 76.9 | 42.3 | - | - | - | 80,454 | 35,686 | 1,056 | 1,242 | VisDrone-MOT |'
- en: '| Flow-Tracker[[151](#bib.bib151)] | IOU+Optical flow | 56/7 | $3840\times
    2160$ | 26.4 | 78.1 | 41.9 | - | 115 | 246 | 9,987 | 43,766 | 127 | 428 | VisDrone-MOT
    |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
  zh: '| Flow-Tracker[[151](#bib.bib151)] | IOU+光流 | 56/7 | $3840\times 2160$ | 26.4
    | 78.1 | 41.9 | - | 115 | 246 | 9,987 | 43,766 | 127 | 428 | VisDrone-MOT |'
- en: '| CMOT^∗  [[164](#bib.bib164)] | Faster RCNN | 56/16 | $3840\times 2160$ |
    31.5 | 73.3 | 51.3 | 1.42 | 282 | 435 | 26,851 | 72,382 | 789 | 2,257 | VisDrone-MOT
    |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
  zh: '| CMOT^∗  [[164](#bib.bib164)] | Faster RCNN | 56/16 | $3840\times 2160$ |
    31.5 | 73.3 | 51.3 | 1.42 | 282 | 435 | 26,851 | 72,382 | 789 | 2,257 | VisDrone-MOT
    |'
- en: '| TBD^∗  [[222](#bib.bib222)] | Faster RCNN | 56/16 | $3840\times 2160$ | 35.6
    | 74.1 | 45.9 | 1.17 | 302 | 419 | 22,086 | 70,083 | 1,834 | 2,307 | VisDrone-MOT
    |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '| TBD^∗  [[222](#bib.bib222)] | Faster RCNN | 56/16 | $3840\times 2160$ | 35.6
    | 74.1 | 45.9 | 1.17 | 302 | 419 | 22,086 | 70,083 | 1,834 | 2,307 | VisDrone-MOT
    |'
- en: '| $H^{2}T^{*}$  [[223](#bib.bib223)] | Faster RCNN | 56/16 | $3840\times 2160$
    | 32.2 | 73.3 | 44.4 | 0.95 | 214 | 494 | 17,889 | 79,801 | 1,269 | 2,035 | VisDrone-MOT
    |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
  zh: '| $H^{2}T^{*}$  [[223](#bib.bib223)] | Faster RCNN | 56/16 | $3840\times 2160$
    | 32.2 | 73.3 | 44.4 | 0.95 | 214 | 494 | 17,889 | 79,801 | 1,269 | 2,035 | VisDrone-MOT
    |'
- en: '| IHTLS^∗  [[167](#bib.bib167)] | Faster RCNN | 56/16 | $3840\times 2160$ |
    36.5 | 74.8 | 43.0 | 0.94 | 245 | 446 | 14,564 | 75,361 | 1,435 | 2,662 | VisDrone-MOT
    |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '| IHTLS^∗  [[167](#bib.bib167)] | Faster RCNN | 56/16 | $3840\times 2160$ |
    36.5 | 74.8 | 43.0 | 0.94 | 245 | 446 | 14,564 | 75,361 | 1,435 | 2,662 | VisDrone-MOT
    |'
- en: '| Ctrack [[224](#bib.bib224)] | Faster RCNN | 56/16 | $3840\times 2160$ | 30.8
    | 73.5 | 51.9 | 1.95 | 369 | 375 | 36,930 | 62,819 | 1,376 | 2,190 | VisDrone-MOT
    |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '| Ctrack [[224](#bib.bib224)] | Faster RCNN | 56/16 | $3840\times 2160$ | 30.8
    | 73.5 | 51.9 | 1.95 | 369 | 375 | 36,930 | 62,819 | 1,376 | 2,190 | VisDrone-MOT
    |'
- en: '| CEM^∗  [[170](#bib.bib170)] | Faster RCNN | 56/16 | $3840\times 2160$ | 5.1
    | 72.3 | 19.2 | 1.12 | 105 | 752 | 21,180 | 116,363 | 1,002 | 1,858 | VisDrone-MOT
    |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
  zh: '| CEM^∗  [[170](#bib.bib170)] | Faster RCNN | 56/16 | $3840\times 2160$ | 5.1
    | 72.3 | 19.2 | 1.12 | 105 | 752 | 21,180 | 116,363 | 1,002 | 1,858 | VisDrone-MOT
    |'
- en: '| GOG^∗  [[166](#bib.bib166)] | Faster RCNN | 56/16 | $3840\times 2160$ | 38.4
    | 75.1 | 45.1 | 0.54 | 244 | 496 | 10,179 | 78,724 | 1,114 | 2,012 | VisDrone-MOT
    |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '| GOG^∗  [[166](#bib.bib166)] | Faster RCNN | 56/16 | $3840\times 2160$ | 38.4
    | 75.1 | 45.1 | 0.54 | 244 | 496 | 10,179 | 78,724 | 1,114 | 2,012 | VisDrone-MOT
    |'
- en: 'TABLE X: Performance comparisons of UAV exclusive tracking networks and classic
    tracking networks for the VisDrone-MOT testing set taken AP as evaluation indexes.
    The best performers are highlighted in bold.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '表 X: UAV 专用跟踪网络和经典跟踪网络在 VisDrone-MOT 测试集上的性能比较，使用 AP 作为评估指标。最佳性能的结果用**粗体**标出。'
- en: '| Method | Framework | Train/Test(seq) | Image Size | AP | AP_0.25 | AP_0.5
    | AP_0.75 | AP_car | AP_bus | AP_truck | AP_ped | AP_van | Exp. Data |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 框架 | 训练/测试（序列） | 图像大小 | AP | AP_0.25 | AP_0.5 | AP_0.75 | AP_car | AP_bus
    | AP_truck | AP_ped | AP_van | 实验数据 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
- en: '| PAS Tracker[[161](#bib.bib161)] | CenterNet+IOU | 56/7 | $608\times 608$
    | 50.80 | 66.10 | 52.50 | 33.80 | 62.7 | 81.20 | 43.90 | 30.30 | 35.90 | VisDrone-MOT
    |'
  id: totrans-339
  prefs: []
  type: TYPE_TB
  zh: '| PAS Tracker[[161](#bib.bib161)] | CenterNet+IOU | 56/7 | $608\times 608$
    | 50.80 | 66.10 | 52.50 | 33.80 | 62.7 | 81.20 | 43.90 | 30.30 | 35.90 | VisDrone-MOT
    |'
- en: '| HMTT[[153](#bib.bib153)] | CenterNet+IOU | 56/7 | $608\times 608$ | 28.67
    | 39.05 | 27.88 | 19.08 | 44.35 | 30.56 | 18.75 | 26.49 | 23.19 | VisDrone-MOT
    |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| HMTT[[153](#bib.bib153)] | CenterNet+IOU | 56/7 | $608\times 608$ | 28.67
    | 39.05 | 27.88 | 19.08 | 44.35 | 30.56 | 18.75 | 26.49 | 23.19 | VisDrone-MOT
    |'
- en: '| DAN[[77](#bib.bib77)] | RetinaNet+DAN | - | $1500\times 1000$ | 13.88 | 23.19
    | 12.81 | 5.64 | 32.20 | 8.83 | 6.61 | 18.61 | 3.16 | VisDrone-MOT |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| DAN[[77](#bib.bib77)] | RetinaNet+DAN | - | $1500\times 1000$ | 13.88 | 23.19
    | 12.81 | 5.64 | 32.20 | 8.83 | 6.61 | 18.61 | 3.16 | VisDrone-MOT |'
- en: '| GGD[[155](#bib.bib155)] | Faster RCNN | 56/33 | $3840\times 2160$ | 23.09
    | 31.01 | 22.70 | 15.55 | 35.45 | 28.57 | 11.90 | 17.20 | 22.34 | VisDrone-MOT
    |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| GGD[[155](#bib.bib155)] | Faster RCNN | 56/33 | $3840\times 2160$ | 23.09
    | 31.01 | 22.70 | 15.55 | 35.45 | 28.57 | 11.90 | 17.20 | 22.34 | VisDrone-MOT
    |'
- en: '| Cas_RCNN+FPNCas_RCNN+FPN[[80](#bib.bib80)] | Cascade R-CNN | - | $2000\times
    1500$ | 28.51 | 44.76 | 30.38 | 10.40 | 35.09 | 34.58 | 18.20 | - | 26.18 | VisDrone-MOT
    |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| Cas_RCNN+FPNCas_RCNN+FPN[[80](#bib.bib80)] | Cascade R-CNN | - | $2000\times
    1500$ | 28.51 | 44.76 | 30.38 | 10.40 | 35.09 | 34.58 | 18.20 | - | 26.18 | VisDrone-MOT
    |'
- en: 'TABLE XI: Performance comparisons of UAV exclusive tracking networks and classic
    tracking networks for the Stanford Droned dataset taken MOTA, MOTP, etc as evaluation
    indexes. The best performers are highlighted in bold.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XI: UAV 专用跟踪网络和经典跟踪网络在 Stanford Droned 数据集上的性能比较，使用 MOTA、MOTP 等作为评估指标。最佳性能的结果用**粗体**标出。'
- en: '| Method | Framework | Train/Test(seq) | Image Size | MOTA | MOTP | IDF1 |
    IDP | MT% | ML% | FP | FN | IDS | FM | Exp. Data |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 框架 | 训练/测试（序列） | 图像大小 | MOTA | MOTP | IDF1 | IDP | MT% | ML% | FP |
    FN | IDS | FM | 实验数据 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '| IPGAT[[159](#bib.bib159)] | LSTM+ CGAN | 36/24 | $1080\times 540$ | 99.9
    | 99.9 | 90.0 | 90.0 | 99.8 | 0.13 | 3 | 833 | 3, 395 | 905 | Stanford Drone |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| IPGAT[[159](#bib.bib159)] | LSTM+ CGAN | 36/24 | $1080\times 540$ | 99.9
    | 99.9 | 90.0 | 90.0 | 99.8 | 0.13 | 3 | 833 | 3, 395 | 905 | Stanford Drone |'
- en: '| +Siamese |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| +Siamese |'
- en: '| CEM [[170](#bib.bib170)] | Faster RCNN | 36/24 | $1417\times 2019$ | 3.0
    | 81.8 | 5.4 | 47.6 | 2.7 | 90.25 | 972,646 | 348,495 | 3,103 | 5,997 | Stanford
    Drone |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| CEM [[170](#bib.bib170)] | Faster RCNN | 36/24 | $1417\times 2019$ | 3.0
    | 81.8 | 5.4 | 47.6 | 2.7 | 90.25 | 972,646 | 348,495 | 3,103 | 5,997 | Stanford
    Drone |'
- en: '| GOG [[166](#bib.bib166)] | Faster RCNN | 36/24 | $1417\times 2019$ | 98.9
    | 100.0 | 86.3 | 86.7 | 100.0 | 96.2 | 3 | 66,625 | 4,928 | 2,621 | Stanford Drone
    |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| GOG [[166](#bib.bib166)] | Faster RCNN | 36/24 | $1417\times 2019$ | 98.9
    | 100.0 | 86.3 | 86.7 | 100.0 | 96.2 | 3 | 66,625 | 4,928 | 2,621 | Stanford Drone
    |'
- en: '| IOUT [[168](#bib.bib168)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.9
    | 100.0 | 93.2 | 93.2 | 98.9 | 1.04 | 0 | 2,497 | 1,170 | 949 | Stanford Drone
    |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| IOUT [[168](#bib.bib168)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.9
    | 100.0 | 93.2 | 93.2 | 98.9 | 1.04 | 0 | 2,497 | 1,170 | 949 | Stanford Drone
    |'
- en: '| SMOT [[167](#bib.bib167)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.1
    | 100.0 | 91.8 | 91.9 | 97.3 | 1.38 | 17,212 | 38,846 | 2,275 | 3,926 | Stanford
    Drone |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| SMOT [[167](#bib.bib167)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.1
    | 100.0 | 91.8 | 91.9 | 97.3 | 1.38 | 17,212 | 38,846 | 2,275 | 3,926 | Stanford
    Drone |'
- en: '| SORT [[169](#bib.bib169)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.5
    | 98.1 | 95.7 | 96.0 | 98.0 | 1.05 | 20 | 32,436 | 957 | 952 | Stanford Drone
    |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| SORT [[169](#bib.bib169)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.5
    | 98.1 | 95.7 | 96.0 | 98.0 | 1.05 | 20 | 32,436 | 957 | 952 | Stanford Drone
    |'
- en: '| SLSTM [[195](#bib.bib195)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.3
    | 99.9 | 89.6 | 89.6 | 99.8 | 0.13 | 11 | 841 | 3, 630 | 906 | Stanford Drone
    |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| SLSTM [[195](#bib.bib195)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.3
    | 99.9 | 89.6 | 89.6 | 99.8 | 0.13 | 11 | 841 | 3, 630 | 906 | Stanford Drone
    |'
- en: 'TABLE XII: Performance comparisons of UAV exclusive tracking networks and classic
    tracking networks for the UAVDT dataset taken MOTA, MOTP, etc as evaluation indexes.
    The best performers are highlighted in bold.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 表 XII：针对 UAVDT 数据集的 UAV 专用跟踪网络和经典跟踪网络的性能比较，以 MOTA、MOTP 等作为评估指标。最佳表现者以 **粗体**
    标出。
- en: '| Method | Framework | Train/Test(seq) | Image Size | MOTA | MOTP | IDF1 |
    IDP | MT(%) | ML(%) | FP | FN | IDS | FM | Exp. Data |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 框架 | 训练/测试（序列） | 图像大小 | MOTA | MOTP | IDF1 | IDP | MT（%） | ML（%） | FP
    | FN | IDS | FM | 试验数据 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
- en: '| IPGAT[[159](#bib.bib159)] | LSTM+ CGAN | 30/20 | $1080\times 540$ | 39.0
    | 72.2 | 49.4 | 63.2 | 37.4 | 25.2 | 42, 135 | 163, 837 | 2,091 | 10,057 | UAVDT
    |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| IPGAT[[159](#bib.bib159)] | LSTM+ CGAN | 30/20 | $1080\times 540$ | 39.0
    | 72.2 | 49.4 | 63.2 | 37.4 | 25.2 | 42, 135 | 163, 837 | 2,091 | 10,057 | UAVDT
    |'
- en: '| +Siamese |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| +Siamese |'
- en: '| OSIM[[150](#bib.bib150)] | YOLOv3 | - | $1080\times 540$ | 88.7 | - | - |
    - | - | - | 8 | 610 | - | - | UAVDT |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| OSIM[[150](#bib.bib150)] | YOLOv3 | - | $1080\times 540$ | 88.7 | - | - |
    - | - | - | 8 | 610 | - | - | UAVDT |'
- en: '| Self-balance[[157](#bib.bib157)] | LSTM | 30/20 | $1080\times 540$ | 38.6
    | 72.1 | 48.5 | 61.1 | 38.9 | 24.4 | 44,724 | 160,950 | 3,489 | 11,796 | UAVDT
    |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| Self-balance[[157](#bib.bib157)] | LSTM | 30/20 | $1080\times 540$ | 38.6
    | 72.1 | 48.5 | 61.1 | 38.9 | 24.4 | 44,724 | 160,950 | 3,489 | 11,796 | UAVDT
    |'
- en: '| UAV_MOT1 [[162](#bib.bib162)] | Faster RCNN | 30/20 | $1417\times 2019$ |
    40.3 | 74.0 | 55.0 | 67.0 | - | - | 30,065 | 150,837 | 1,091 | 3,057 | UAVDT |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| UAV_MOT1 [[162](#bib.bib162)] | Faster RCNN | 30/20 | $1417\times 2019$ |
    40.3 | 74.0 | 55.0 | 67.0 | - | - | 30,065 | 150,837 | 1,091 | 3,057 | UAVDT |'
- en: '| RLSTM[[225](#bib.bib225)] | Faster RCNN | 30/20 | $1080\times 540$ | 25.6
    | 69.1 | 31.3 | 38.6 | 36.7 | 25.7 | 71,955 | 180,461 | 1,333 | 13,088 | UAVDT
    |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| RLSTM[[225](#bib.bib225)] | Faster RCNN | 30/20 | $1080\times 540$ | 25.6
    | 69.1 | 31.3 | 38.6 | 36.7 | 25.7 | 71,955 | 180,461 | 1,333 | 13,088 | UAVDT
    |'
- en: '| SLSTM [[195](#bib.bib195)] | Faster RCNN | 30/20 | $1080\times 540$ | 37.9
    | 72.0 | 37.2 | 46.8 | 38.2 | 24.4 | 44,783 | 161,009 | 6,048 | 12,051 | UAVDT
    |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| SLSTM [[195](#bib.bib195)] | Faster RCNN | 30/20 | $1080\times 540$ | 37.9
    | 72.0 | 37.2 | 46.8 | 38.2 | 24.4 | 44,783 | 161,009 | 6,048 | 12,051 | UAVDT
    |'
- en: '| SORT[[169](#bib.bib169)] | Faster RCNN | 30/20 | $1080\times 540$ | 39.0
    | 74.3 | 43.7 | 58.9 | 33.9 | 28.0 | 33,037 | 172,628 | 2,350 | 5,787 | UAVDT
    |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| SORT[[169](#bib.bib169)] | Faster RCNN | 30/20 | $1080\times 540$ | 39.0
    | 74.3 | 43.7 | 58.9 | 33.9 | 28.0 | 33,037 | 172,628 | 2,350 | 5,787 | UAVDT
    |'
- en: '| RMOT[[226](#bib.bib226)] | Faster RCNN | 30/20 | $1080\times 540$ | -39.8
    | 72.3 | 33.3 | 27.8 | 36.7 | 25.7 | 319,008 | 151,485 | 5,973 | 5,897 | UAVDT
    |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| RMOT[[226](#bib.bib226)] | Faster RCNN | 30/20 | $1080\times 540$ | -39.8
    | 72.3 | 33.3 | 27.8 | 36.7 | 25.7 | 319,008 | 151,485 | 5,973 | 5,897 | UAVDT
    |'
- en: '| SMOT[[167](#bib.bib167)] | Faster RCNN | 30/20 | $1080\times 540$ | 33.9
    | 72.2 | 45.0 | 55.7 | 36.7 | 25.7 | 57,112 | 166,528 | 1,752 | 9,577 | UAVDT
    |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| SMOT[[167](#bib.bib167)] | Faster RCNN | 30/20 | $1080\times 540$ | 33.9
    | 72.2 | 45.0 | 55.7 | 36.7 | 25.7 | 57,112 | 166,528 | 1,752 | 9,577 | UAVDT
    |'
- en: '| CEM[[170](#bib.bib170)] | Faster RCNN | 30/20 | $1080\times 540$ | -7.3 |
    69.6 | 10.2 | 19.4 | 7.3 | 68.6 | 72,378 | 290,962 | 2,488 | 4,248 | UAVDT |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| CEM[[170](#bib.bib170)] | Faster RCNN | 30/20 | $1080\times 540$ | -7.3 |
    69.6 | 10.2 | 19.4 | 7.3 | 68.6 | 72,378 | 290,962 | 2,488 | 4,248 | UAVDT |'
- en: '| GOG[[166](#bib.bib166)] | Faster RCNN | 30/20 | $1080\times 540$ | 34.4 |
    72.2 | 18.0 | 23.3 | 35.5 | 25.3 | 41,126 | 168,194 | 14,301 | 12,516 | UAVDT
    |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| GOG[[166](#bib.bib166)] | Faster RCNN | 30/20 | $1080\times 540$ | 34.4 |
    72.2 | 18.0 | 23.3 | 35.5 | 25.3 | 41,126 | 168,194 | 14,301 | 12,516 | UAVDT
    |'
- en: '| IOUT[[168](#bib.bib168)] | Faster RCNN | 30/20 | $1080\times 540$ | 36.6
    | 72.1 | 23.7 | 30.3 | 37.4 | 25.0 | 42,245 | 163,881 | 9,938 | 10,463 | UAVDT
    |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| IOUT[[168](#bib.bib168)] | Faster RCNN | 30/20 | $1080\times 540$ | 36.6
    | 72.1 | 23.7 | 30.3 | 37.4 | 25.0 | 42,245 | 163,881 | 9,938 | 10,463 | UAVDT
    |'
- en: VII-B Evaluation of Object Detection from UAV-borne Video
  id: totrans-371
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B 无人机载视频中的目标检测评估
- en: 'For object detection from UAV-borne video, the common indicators to evaluate
    object detection methods are the same as UAV-borne image, including $AP^{IoU=0.50:0.05:0.95}$,
    $AP^{IoU=0.50}$,$AP^{IoU=0.75}$, $AR^{max=1}$, $AR^{max=10}$, $AR^{max=100}$ and
    $AR^{max=500}$. Table [V](#S7.T5 "TABLE V ‣ VII-A Evaluation of Object Detection
    from UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for
    UAV-based Object Detection and Tracking: A Survey") lists the public quantitative
    results of some state-of-the-art and baseline detection works. Among them, four
    works are the UAV preserves object detection works, and the experimental results
    are mainly focused on the VisDrone dataset. TDFA with a two-stream refined flowNet
    (SPyNet) pipeline, which is robust to small-scale objects and can achieve the
    best performance among all comparison methods, i.e., 27.27% AP score on the VisDrone-VID
    validation set. MPFPN with parallel branch is ranked as the second and third with
    33.72% and 29.05% AP scores.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '对于无人机载视频中的目标检测，评估目标检测方法的常用指标与无人机载图像相同，包括 $AP^{IoU=0.50:0.05:0.95}$、$AP^{IoU=0.50}$、$AP^{IoU=0.75}$、$AR^{max=1}$、$AR^{max=10}$、$AR^{max=100}$
    和 $AR^{max=500}$。表 [V](#S7.T5 "TABLE V ‣ VII-A Evaluation of Object Detection
    from UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for
    UAV-based Object Detection and Tracking: A Survey") 列出了某些最先进和基准检测工作的公开定量结果。其中，四项工作是无人机保持目标检测的工作，实验结果主要集中在
    VisDrone 数据集上。TDFA 采用双流精细化的 flowNet (SPyNet) 流水线，对小规模目标具有鲁棒性，并且在所有比较方法中表现最佳，即
    VisDrone-VID 验证集上的 27.27% AP 分数。MPFPN 具有并行分支，排名第二和第三，AP 分数分别为 33.72% 和 29.05%。'
- en: We have also summarized the results of 9 baseline methods in the VisDrone-VID
    challenge, including CFE-SSDv2 [[219](#bib.bib219)], FGFA^∗ [[135](#bib.bib135)],
    RefineDet [[212](#bib.bib212), [220](#bib.bib220)], RetinaNet [[130](#bib.bib130)],
    detection and tracking (D&T) [[173](#bib.bib173)], FPN^∗ [[131](#bib.bib131)],
    CornerNet^∗ [[211](#bib.bib211)], CenterNet^∗ [[129](#bib.bib129)], and Faster
    R-CNN^∗ [[20](#bib.bib20)]. The experiment results were instructed in accordance,
    with three non-overlapping subsets, 56 video sequences with 24,198 frames for
    the training set, 16 video sequences with 6,322 frames for testing, and the remaining
    sequences are for validation. Obviously, the detection performance of object detection
    in video yields better object detection in an image, and detection results amendment
    by context information plays a decisive role. Moreover, a small object is an inevitable
    problem of object detection in video. Therefore, CFE-SSD with small objects friendliness,
    FGFA assisting the current frame by adopting the front and back frames information,
    and D&T with ROI tracking to associate adjacent frames, obtain a better detection
    performance.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还总结了 VisDrone-VID 挑战中的 9 种基准方法的结果，包括 CFE-SSDv2 [[219](#bib.bib219)]、FGFA^∗
    [[135](#bib.bib135)]、RefineDet [[212](#bib.bib212), [220](#bib.bib220)]、RetinaNet
    [[130](#bib.bib130)]、检测与跟踪 (D&T) [[173](#bib.bib173)]、FPN^∗ [[131](#bib.bib131)]、CornerNet^∗
    [[211](#bib.bib211)]、CenterNet^∗ [[129](#bib.bib129)] 和 Faster R-CNN^∗ [[20](#bib.bib20)]。实验结果是按要求进行的，包含三个不重叠的子集：训练集有
    56 个视频序列和 24,198 帧，测试集有 16 个视频序列和 6,322 帧，其余序列用于验证。显然，视频中的目标检测性能优于图像中的目标检测，而上下文信息对检测结果的修正起着决定性作用。此外，小目标是视频目标检测中不可避免的问题。因此，CFE-SSD
    对小目标友好，FGFA 通过采用前后帧信息辅助当前帧，而 D&T 通过 ROI 跟踪关联相邻帧，从而获得更好的检测性能。
- en: 'Besides the VisDrone dataset, some other datasets are also used, such as Okutama-Action
    and UAVDT. Compared with five baseline works in Table [VIII](#S7.T8 "TABLE VIII
    ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey"), TDFA experimented Okutama-Action dataset has achieved the best detection
    performance, i.e., 87.18% AP_50 value on the Okutama-Action test dataset. STDnet-ST
    with Spatio-temporal ConvNet and STDnet experimented Okutama-Action dataset, have
    achieved 34.60% AP for objects under $16\times 16$ pixels.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '除了 VisDrone 数据集，还使用了一些其他数据集，如 Okutama-Action 和 UAVDT。与表 [VIII](#S7.T8 "TABLE
    VIII ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") 中的五个基准工作相比，TDFA 在 Okutama-Action 数据集上的实验取得了最佳检测性能，即在 Okutama-Action
    测试数据集上的 AP_50 值为 87.18%。STDnet-ST 和 STDnet 在 Okutama-Action 数据集上的实验，取得了 $16\times
    16$ 像素下对象的 34.60% AP。'
- en: 'TABLE XIII: Computation Cost of Statistical Object Detection Approaches for
    UAV exclusive'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XIII: UAV 专用统计对象检测方法的计算成本'
- en: '| Reference | Network Pipeline | Image Size | Exp. environment | Times/fps
    | Year |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 网络管道 | 图像尺寸 | 实验环境 | 次数/帧 | 年份 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| RRNet[[53](#bib.bib53)] | HourGlass | $512\times 512$ | - | - | 2019 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| RRNet[[53](#bib.bib53)] | HourGlass | $512\times 512$ | - | - | 2019 |'
- en: '| Wu et al[[68](#bib.bib68)] | YOLOv3 | $1080\times 640$ | Workstation(NVIDIA
    Tesla K80/-) | 15 | 2019 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| Wu et al[[68](#bib.bib68)] | YOLOv3 | $1080\times 640$ | 工作站（NVIDIA Tesla
    K80/-） | 15 | 2019 |'
- en: '| SlimYOLOv3[[65](#bib.bib65)] | YOLOv3-SPP3-90 | $832\times 832$ | Workstation(NVIDIA
    GTX 1080Ti/-) | 28.3 | 2019 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| SlimYOLOv3[[65](#bib.bib65)] | YOLOv3-SPP3-90 | $832\times 832$ | 工作站（NVIDIA
    GTX 1080Ti/-） | 28.3 | 2019 |'
- en: '| NDFT[[69](#bib.bib69)] | ResNet101 | - | Workstation(-/-) | - | 2019 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| NDFT[[69](#bib.bib69)] | ResNet101 | - | 工作站（-/-） | - | 2019 |'
- en: '| ClusDet[[57](#bib.bib57)] | ResNeXt101 | $2000\times 1500$ | Workstation(NVIDIA
    GTX 1080 Ti/-) | 1.3 | 2019 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| ClusDet[[57](#bib.bib57)] | ResNeXt101 | $2000\times 1500$ | 工作站（NVIDIA GTX
    1080 Ti/-） | 1.3 | 2019 |'
- en: '| CenterNet[[58](#bib.bib58)] | HourGlass104 | $1024\times 1024$ | - | - |
    2019 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| CenterNet[[58](#bib.bib58)] | HourGlass104 | $1024\times 1024$ | - | - |
    2019 |'
- en: '| Yang et al[[59](#bib.bib59)] | VGG16 | $320\times 320$ | Worstation(NVIDIA
    GTX-1080Ti/12GB) | 58 | 2019 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| Yang et al[[59](#bib.bib59)] | VGG16 | $320\times 320$ | 工作站（NVIDIA GTX-1080Ti/12GB）
    | 58 | 2019 |'
- en: '| Zhang et al[[62](#bib.bib62)] | ResNet50 | - | Workstation(NVIDIA GeForce
    1060/6GB) | - | 2019 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al[[62](#bib.bib62)] | ResNet50 | - | 工作站（NVIDIA GeForce 1060/6GB）
    | - | 2019 |'
- en: '| FS-SSD[[66](#bib.bib66)] | VGG16 | $512\times 512$ | Workstation(NVIDIA TITAN
    X (Pascal)/12GB) | 18.3 | 2019 |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| FS-SSD[[66](#bib.bib66)] | VGG16 | $512\times 512$ | 工作站（NVIDIA TITAN X (Pascal)/12GB）
    | 18.3 | 2019 |'
- en: '| SAMFR[[67](#bib.bib67)] | DetNet59 | $419\times 419$ | - | 10 | 2019 |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| SAMFR[[67](#bib.bib67)] | DetNet59 | $419\times 419$ | - | 10 | 2019 |'
- en: '| MSOA-Net[[56](#bib.bib56)] | ResNet50 | $1333\times 800$ | Workstation(NVIDIA
    TITAN-Xp/-) |  | 2020 |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| MSOA-Net[[56](#bib.bib56)] | ResNet50 | $1333\times 800$ | 工作站（NVIDIA TITAN-Xp/-）
    |  | 2020 |'
- en: '| GDF-Net[[61](#bib.bib61)] | ResNet50 | $1200\times 675$ | Workstation(NVIDIA
    Geforce RTX 2080ti/11GB) | 17.9 | 2020 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| GDF-Net[[61](#bib.bib61)] | ResNet50 | $1200\times 675$ | 工作站（NVIDIA Geforce
    RTX 2080ti/11GB） | 17.9 | 2020 |'
- en: '| HRDNet[[70](#bib.bib70)] | ResNeXt50+101 | $960\times 1360$ | Workstation(NVIDIA
    GTX 2080Ti/-) | 0.7 | 2020 |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| HRDNet[[70](#bib.bib70)] | ResNeXt50+101 | $960\times 1360$ | 工作站（NVIDIA
    GTX 2080Ti/-） | 0.7 | 2020 |'
- en: '| D-A-FS SSD[[63](#bib.bib63)] | - | - | - | - | 2020 |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| D-A-FS SSD[[63](#bib.bib63)] | - | - | - | - | 2020 |'
- en: '| UAV-YOLO[[16](#bib.bib16)] | YOLOv3 | $608\times 608$ | Workstation(NVIDIA
    GTX Titan XP/64GB) | 20 | 2020 |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| UAV-YOLO[[16](#bib.bib16)] | YOLOv3 | $608\times 608$ | 工作站（NVIDIA GTX Titan
    XP/64GB） | 20 | 2020 |'
- en: '| SyNet[[71](#bib.bib71)] | CenterNet | - | - | - | 2020 |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| SyNet[[71](#bib.bib71)] | CenterNet | - | - | - | 2020 |'
- en: '| ComNet[[72](#bib.bib72)] | YOLOv3 | $416\times 416$ | Workstation(NVIDIA
    GTX 1080Ti/12GB) | 20 | 2020 |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| ComNet[[72](#bib.bib72)] | YOLOv3 | $416\times 416$ | 工作站（NVIDIA GTX 1080Ti/12GB）
    | 20 | 2020 |'
- en: '| ComNet[[72](#bib.bib72)] | YOLOv3 | $416\times 416$ | laptop(Intel Core i5-8300/4GB)
    | 3 | 2020 |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| ComNet[[72](#bib.bib72)] | YOLOv3 | $416\times 416$ | 笔记本电脑（Intel Core i5-8300/4GB）
    | 3 | 2020 |'
- en: '| ComNet[[72](#bib.bib72)] | YOLOv3 | $416\times 416$ | Jason Nano(Tegra X1/4GB)
    | 2 | 2020 |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| ComNet[[72](#bib.bib72)] | YOLOv3 | $416\times 416$ | Jason Nano（Tegra X1/4GB）
    | 2 | 2020 |'
- en: '| MPFPN[[73](#bib.bib73)] | ResNets101 | $1440\times 800$ | Workstation(NVIDIA
    GTX 1080Ti/-) | 2.1 | 2020 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| MPFPN[[73](#bib.bib73)] | ResNets101 | $1440\times 800$ | 工作站（NVIDIA GTX
    1080Ti/-） | 2.1 | 2020 |'
- en: '| D2Det[[74](#bib.bib74)] | ResNet101 | $1333\times 800$ | Workstation(NVIDIA
    GTX Titan Xp/-) | 5.9 | 2020 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| D2Det[[74](#bib.bib74)] | ResNet101 | $1333\times 800$ | 工作站（NVIDIA GTX Titan
    Xp/-） | 5.9 | 2020 |'
- en: '| DAGN[[75](#bib.bib75)] | YOLOv3 | $512\times 512$ | Workstation(NVIDIA GeForce
    GTX 1080Ti/11GB) | 25.1 | 2020 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| DAGN[[75](#bib.bib75)] | YOLOv3 | $512\times 512$ | 工作站（NVIDIA GeForce GTX
    1080Ti/11GB） | 25.1 | 2020 |'
- en: '| GANet[[76](#bib.bib76)] | VGG-16/ResNet50 | $512\times 512$ | Workstation(NVIDIA
    GTX Titan Xp/-) | - | 2020 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| GANet[[76](#bib.bib76)] | VGG-16/ResNet50 | $512\times 512$ | Workstation(NVIDIA
    GTX Titan Xp/-) | - | 2020 |'
- en: '| DAN[[77](#bib.bib77)] | Resnet-50 | $1500\times 1000$ | - | - | 2020 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| DAN[[77](#bib.bib77)] | Resnet-50 | $1500\times 1000$ | - | - | 2020 |'
- en: '| Zhang et al[[78](#bib.bib78)] | PeleeNet | $304\times 304$ | Workstation(NVIDIA
    TITAN X (Pascal)/12GB) | 23.6 | 2020 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al[[78](#bib.bib78)] | PeleeNet | $304\times 304$ | Workstation(NVIDIA
    TITAN X (Pascal)/12GB) | 23.6 | 2020 |'
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | $2000\times 1500$ | Workstation(NVIDIA
    1080Ti/11GB) | 10.8 | 2021 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| DSHNet[[81](#bib.bib81)] | ResNet50 | $2000\times 1500$ | Workstation(NVIDIA
    1080Ti/11GB) | 10.8 | 2021 |'
- en: '| DNOD[[79](#bib.bib79)] | VGG19+CSPDarknet53 | $608\times 608$ | Workstation(NVIDIA
    GeForce RTX 2080ti/6GB) | 38.3 | 2021 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| DNOD[[79](#bib.bib79)] | VGG19+CSPDarknet53 | $608\times 608$ | Workstation(NVIDIA
    GeForce RTX 2080ti/6GB) | 38.3 | 2021 |'
- en: '| ECas_RCNN[[54](#bib.bib54)] | ResNet50 | $1450\times 800$ | Workstation(NVIDIA
    RTX 2080Ti/-) | - | 2021 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| ECas_RCNN[[54](#bib.bib54)] | ResNet50 | $1450\times 800$ | Workstation(NVIDIA
    RTX 2080Ti/-) | - | 2021 |'
- en: '| DSYolov3[[55](#bib.bib55)] | Yolov3 | $416\times 416$ | Workstation(NVIDIA
    GTX 1080Ti/-) | 13.7 | 2021 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| DSYolov3[[55](#bib.bib55)] | Yolov3 | $416\times 416$ | Workstation(NVIDIA
    GTX 1080Ti/-) | 13.7 | 2021 |'
- en: '| Cas_RCNN+FPN[[80](#bib.bib80)] | ResNet101 | $1500\times 2000$ | - | - |
    2021 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| Cas_RCNN+FPN[[80](#bib.bib80)] | ResNet101 | $1500\times 2000$ | - | - |
    2021 |'
- en: 'TABLE XIV: Computation Cost of DL-based Video Object Detection Approaches for
    UAV exclusive'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XIV: 专为无人机设计的基于深度学习的视频目标检测方法的计算成本'
- en: '| Reference | Network Pipeline | Image Size | Exp. environment | Times/fps
    | Year |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| Reference | Network Pipeline | Image Size | Exp. environment | Times/fps
    | Year |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| STCA[[114](#bib.bib114)] | SSD+FCOS+SiamFC | $300\times 300$ | Workstation(NVIDIA
    GeForce GTX 1080/-) | - | 2019 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| STCA[[114](#bib.bib114)] | SSD+FCOS+SiamFC | $300\times 300$ | Workstation(NVIDIA
    GeForce GTX 1080/-) | - | 2019 |'
- en: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | laptop(Core
    i7-2670QM/6GB) | 26.3 | 2019 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | 笔记本电脑(Core
    i7-2670QM/6GB) | 26.3 | 2019 |'
- en: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | Embedded(Raspberry
    Pi 2/1GB) | 10.8 | 2019 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | 嵌入式(Raspberry
    Pi 2/1GB) | 10.8 | 2019 |'
- en: '| Nousi et al[[116](#bib.bib116)] | Tity YOLO | $288\times 288$ | Embedded(Robot
    Operating System) | 23 | 2019 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| Nousi et al[[116](#bib.bib116)] | Tity YOLO | $288\times 288$ | 嵌入式(Robot
    Operating System) | 23 | 2019 |'
- en: '| SCNN[[115](#bib.bib115)] | ResNet34 | $224\times 224$ | Workstation(NVIDIA
    GeForce GTX 1080/-) | 246 | 2019 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| SCNN[[115](#bib.bib115)] | ResNet34 | $224\times 224$ | Workstation(NVIDIA
    GeForce GTX 1080/-) | 246 | 2019 |'
- en: '| Zhang et al[[118](#bib.bib118)] | Cas_R-CNN+IRR-PWC [[132](#bib.bib132)]
    | $720\times 1280$ | - | - | 2020 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| Zhang et al[[118](#bib.bib118)] | Cas_R-CNN+IRR-PWC [[132](#bib.bib132)]
    | $720\times 1280$ | - | - | 2020 |'
- en: '| MOR-UAVNet[[119](#bib.bib119)] | MOR-UAVNetv14 | $608\times 608$ | Workstation(NVIDIA
    RTX 2080 Ti/11GB) | 10.5 | 2020 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| MOR-UAVNet[[119](#bib.bib119)] | MOR-UAVNetv14 | $608\times 608$ | Workstation(NVIDIA
    RTX 2080 Ti/11GB) | 10.5 | 2020 |'
- en: '| TDFA[[120](#bib.bib120)] | FlowNet+ Fea_Aggregation | $720\times 1280$ |
    Workstation(NVIDIA GeForce GTX TITAN X/12GB) | 3.8 | 2021 |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '| TDFA[[120](#bib.bib120)] | FlowNet+ Fea_Aggregation | $720\times 1280$ |
    Workstation(NVIDIA GeForce GTX TITAN X/12GB) | 3.8 | 2021 |'
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+ConvNet | $1280\times 720$ | - | -
    | 2021 |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '| STDnet-ST[[121](#bib.bib121)] | STDnet+ConvNet | $1280\times 720$ | - | -
    | 2021 |'
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+ConvNet | $1920\times 1080$ | - |
    - | 2021 |'
  id: totrans-420
  prefs: []
  type: TYPE_TB
  zh: '| STDnet-ST[[121](#bib.bib121)] | STDnet+ConvNet | $1920\times 1080$ | - |
    - | 2021 |'
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+ConvNet | $1024\times 540$ | - | -
    | 2021 |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| STDnet-ST[[121](#bib.bib121)] | STDnet+ConvNet | $1024\times 540$ | - | -
    | 2021 |'
- en: VII-C Evaluation of Object Tracking from UAV-borne Video
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 无人机视频中的目标跟踪评估
- en: For object tracking from UAV-borne video, the common way to evaluate object
    detection methods, including multiple object tracking accuracy (MOTA), multiple
    object tracking precision (MOTP), identification precision (IDP), identification
    F1 score (IDF1), false alarms per frame (FAF), the number of mostly tracked targets
    (MT, more than 80% of trajectories being covered by the ground truth), the number
    of mostly lost targets (ML, less than 20% of trajectories being covered by the
    ground truth), the number of false positives (FP), the number of false negatives
    (FN), the number of ID switches (IDS), and the number of times a trajectory is
    Fragmented (FM).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 对于无人机视频中的目标跟踪，评估目标检测方法的常见方式包括：多目标跟踪准确率（MOTA）、多目标跟踪精度（MOTP）、识别精度（IDP）、识别F1得分（IDF1）、每帧虚警数（FAF）、大部分跟踪目标的数量（MT，地面真实情况覆盖率超过80%的轨迹数量）、大部分丢失目标的数量（ML，地面真实情况覆盖率不到20%的轨迹数量）、假阳性数量（FP）、假阴性数量（FN）、ID切换数量（IDS）以及轨迹碎片化的次数（FM）。
- en: The IDF1 score is defined as
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: The IDF1 score is defined as
- en: '|  | $IDF1=\frac{2IDTP}{2IDTP+IDFP+IDFN},$ |  | (1) |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '|  | $IDF1=\frac{2IDTP}{2IDTP+IDFP+IDFN},$ |  | (1) |'
- en: where IDTP is the number of true positive IDs, IDFP is the number of false positive
    IDs, and IDFN is the number of false negative IDs. In addition, some literature
    have still adopted the detection evaluation metrics, including $AP^{IoU=0.50:0.05:0.95}$,
    $AP^{IoU=0.25}$, $AP^{IoU=0.50}$, $AP^{IoU=0.75}$.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，IDTP 是真实正例 ID 的数量，IDFP 是假阳性 ID 的数量，IDFN 是假阴性 ID 的数量。此外，一些文献仍然采用检测评估指标，包括
    $AP^{IoU=0.50:0.05:0.95}$、$AP^{IoU=0.25}$、$AP^{IoU=0.50}$ 和 $AP^{IoU=0.75}$。
- en: 'Tables [IX](#S7.T9 "TABLE IX ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey")-[XII](#S7.T12 "TABLE XII ‣ VII-A Evaluation
    of Object Detection from UAV-borne Images ‣ VII Experiment Results And Analysis
    ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey") summarize
    the quantitative comparison of several multiple object tracking methods on the
    challenging public UAV dataset. In Table [IX](#S7.T9 "TABLE IX ‣ VII-A Evaluation
    of Object Detection from UAV-borne Images ‣ VII Experiment Results And Analysis
    ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey"), the average
    rank of 10 metrics (i.e., MOTA, MOTP, IDF1, FAF, MT, ML, FP, FN, IDS, and FM)
    is used to rank these approaches. TrackletNet Tracker (TNT), wins the VisDrone-MOT
    challenge dataset by the highest MOTA, IDF1, FP, IDS. We also report the accuracy
    of the trackers in AP as well as different object categories, including AP_car,
    AP_bus, AP_trk, AP_ped and AP_van in Table [X](#S7.T10 "TABLE X ‣ VII-A Evaluation
    of Object Detection from UAV-borne Images ‣ VII Experiment Results And Analysis
    ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey"). The PAS
    tracker followed by the tracking-by-detection paradigm achieves the best performance,
    i.e., 50.80% AP score on the VisDrone-MOT testing set. HMTT based on SOT achieves
    a 28.67% AP score on the VisDrone-MOT validation set. With the exception of VisDrone
    dataset, IPGAT achieves the best tracking performance for the UAVDT and Stanford
    Drone testing dataset in Tables [XI](#S7.T11 "TABLE XI ‣ VII-A Evaluation of Object
    Detection from UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey") and [XII](#S7.T12 "TABLE
    XII ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey"), in terms of IDF1, MT, ML, and FN, by estimating object motion and
    UAV movement as individual and global motions, respectively.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [IX](#S7.T9 "TABLE IX ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey")-[XII](#S7.T12 "TABLE XII ‣ VII-A Evaluation
    of Object Detection from UAV-borne Images ‣ VII Experiment Results And Analysis
    ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey") 总结了在具有挑战性的公开
    UAV 数据集上，几种多目标跟踪方法的定量比较。在表格 [IX](#S7.T9 "TABLE IX ‣ VII-A Evaluation of Object
    Detection from UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey") 中，使用了 10 项指标（即 MOTA、MOTP、IDF1、FAF、MT、ML、FP、FN、IDS
    和 FM）的平均排名来对这些方法进行排序。TrackletNet Tracker (TNT) 在 VisDrone-MOT 挑战数据集中获得了最高的 MOTA、IDF1、FP
    和 IDS。我们还在表格 [X](#S7.T10 "TABLE X ‣ VII-A Evaluation of Object Detection from
    UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based
    Object Detection and Tracking: A Survey") 中报告了跟踪器在 AP 以及不同物体类别（包括 AP_car、AP_bus、AP_trk、AP_ped
    和 AP_van）上的准确性。PAS 跟踪器采用基于检测的跟踪范式，获得了 VisDrone-MOT 测试集上的最佳性能，即 50.80% 的 AP 分数。基于
    SOT 的 HMTT 在 VisDrone-MOT 验证集上获得了 28.67% 的 AP 分数。除了 VisDrone 数据集外，IPGAT 在表格 [XI](#S7.T11
    "TABLE XI ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") 和 [XII](#S7.T12 "TABLE XII ‣ VII-A Evaluation of Object Detection from
    UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based
    Object Detection and Tracking: A Survey") 中，对于 UAVDT 和 Stanford Drone 测试数据集，在
    IDF1、MT、ML 和 FN 方面表现最佳，通过将物体运动和 UAV 运动分别估计为个体和全局运动。'
- en: 'The rest of Tables [IX](#S7.T9 "TABLE IX ‣ VII-A Evaluation of Object Detection
    from UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for
    UAV-based Object Detection and Tracking: A Survey") to [XII](#S7.T12 "TABLE XII
    ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") are the results from baseline methods for the three MOT datasets. The
    results are based on Faster RCNN detection input for convenient comparison. For
    VisDrone-MOT dataset consists of 79 video sequences in total, including 56 video
    sequences for the training set, 16 video sequences for testing, and the remainder
    is for validation. Under these settings, Ctrack with recovering long-time disappearance
    objects in the crowded scenes achieves the best tracking performance among all
    methods in the VisDrone testing dataset, in terms of the IDF1, MT, ML, and FN.
    For the UAVDT dataset under 50 sequences recorded in the traffic scenario from
    UAVs, 60% for training and 40% for testing. From Table [XII](#S7.T12 "TABLE XII
    ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey"), it can be seen that SORT is superior on most metrics. Although these
    results are far from the requirements of practical application, they can provide
    feasible direction (e.g., the association of moving objects) and a reliable theoretical
    basis for future research. For the Stanford Drone dataset, the performance gap
    of the listed baseline method is minimal, maybe IOUT slightly better.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '表格 [IX](#S7.T9 "TABLE IX ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") 到 [XII](#S7.T12 "TABLE XII ‣ VII-A Evaluation
    of Object Detection from UAV-borne Images ‣ VII Experiment Results And Analysis
    ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey") 显示的是三个MOT数据集的基线方法结果。结果基于Faster
    RCNN检测输入以方便比较。对于VisDrone-MOT数据集，总共有79个视频序列，包括56个用于训练，16个用于测试，其余用于验证。在这些设置下，Ctrack在拥挤场景中恢复长期消失目标的能力使其在VisDrone测试数据集中在IDF1、MT、ML和FN方面的跟踪性能优于所有方法。对于UAVDT数据集，记录了50个交通场景序列，其中60%用于训练，40%用于测试。从表
    [XII](#S7.T12 "TABLE XII ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") 可以看出，SORT在大多数指标上表现更佳。尽管这些结果远未满足实际应用的要求，但它们为未来的研究提供了可行的方向（例如，移动目标的关联）和可靠的理论基础。对于斯坦福无人机数据集，列出的基线方法的性能差距最小，也许IOUT略微更好。'
- en: VII-D Estimation of Computation Cost
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D 计算成本估算
- en: 'In this survey, all the reviewed methods have their own experimental environment,
    experimental data, and even source code. Considering the computation cost is directly
    related to speed, GPU, and backbone model, we list these three indexes of the
    UAV exclusive methods for the above three topics in Tables [XIII](#S7.T13 "TABLE
    XIII ‣ VII-B Evaluation of Object Detection from UAV-borne Video ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey")- [XV](#S7.T15 "TABLE XV ‣ VII-D Estimation of Computation Cost ‣ VII
    Experiment Results And Analysis ‣ Deep Learning for UAV-based Object Detection
    and Tracking: A Survey"). Depending on the computing power of NVIDIA’s GPU ^(15)^(15)15[https://www.pianshen.com/article/13711825712/](https://www.pianshen.com/article/13711825712/),
    the computation cost can be estimated with backbone network in the corresponding
    method.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '在本调查中，所有回顾的方法都有各自的实验环境、实验数据甚至源代码。考虑到计算成本与速度、GPU和主干模型直接相关，我们在表格 [XIII](#S7.T13
    "TABLE XIII ‣ VII-B Evaluation of Object Detection from UAV-borne Video ‣ VII
    Experiment Results And Analysis ‣ Deep Learning for UAV-based Object Detection
    and Tracking: A Survey")- [XV](#S7.T15 "TABLE XV ‣ VII-D Estimation of Computation
    Cost ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") 中列出了上述三个主题的UAV专属方法的这三个指标。根据NVIDIA GPU的计算能力^(15)^(15)15[https://www.pianshen.com/article/13711825712/](https://www.pianshen.com/article/13711825712/)，可以估算出相应方法中主干网络的计算成本。'
- en: 'TABLE XV: Computation Cost of DL-based Multiple Object Tracking for UAV exclusive.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '表 XV: 针对UAV专用的DL-based多目标跟踪的计算成本。'
- en: '| Reference | Network Pipeline | Image Size | Exp. environment | Times/fps
    | Year |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| 参考文献 | 网络管道 | 图像尺寸 | 实验环境 | 次数/秒 | 年份 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| Deep SORT[[147](#bib.bib147)] | Faster R-CNN+ Sort | $1920\times 1080$ |
    Workstation(NVIDIA GeForce GTX 1050/-) | - | 2017 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| Deep SORT[[147](#bib.bib147)] | Faster R-CNN+ Sort | $1920\times 1080$ |
    工作站(NVIDIA GeForce GTX 1050/-) | - | 2017 |'
- en: '| SCTrack[[148](#bib.bib148)] | Faster R-CNN+YOLOv3 | - | - | - | 2018 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| SCTrack[[148](#bib.bib148)] | Faster R-CNN+YOLOv3 | - | - | - | 2018 |'
- en: '| TNT[[152](#bib.bib152)] | Faster-RCNN+ SVO+MVS+3d loc | - | - | - | 2019
    |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| TNT[[152](#bib.bib152)] | Faster-RCNN+ SVO+MVS+3d loc | - | - | - | 2019
    |'
- en: '| Zhou et al[[149](#bib.bib149)] | Faster RCNN+ SPHP | $1080\times 540$ | Workstation(NVIDIA
    RTX 2080Ti/-) | - | 2019 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| Zhou 等[[149](#bib.bib149)] | Faster RCNN+ SPHP | $1080\times 540$ | 工作站（NVIDIA
    RTX 2080Ti/-） | - | 2019 |'
- en: '| OSIM[[150](#bib.bib150)] | YOLOv3+Kalman filtering | $2720\times 1530$ |
    Workstation(NVIDIA GeForce GTX 1080 Ti/-) | 30 | 2019 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| OSIM[[150](#bib.bib150)] | YOLOv3+Kalman filtering | $2720\times 1530$ |
    工作站（NVIDIA GeForce GTX 1080 Ti/-） | 30 | 2019 |'
- en: '| +deep appearance feature | Workstation(Intel UHD Graphics 630/-) |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| +deep appearance feature | 工作站（Intel UHD Graphics 630/-） |'
- en: '| Self-balance[[157](#bib.bib157)] | LSTM | $1080\times 540$ | Workstation(NVIDIA
    Titan X/32GB) | - | 2019 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| Self-balance[[157](#bib.bib157)] | LSTM | $1080\times 540$ | 工作站（NVIDIA Titan
    X/32GB） | - | 2019 |'
- en: '| Flow-tracker[[151](#bib.bib151)] | Optical Flownet+IOU | - | Workstation(NVIDIA
    GTX 1080Ti/-) | 5 | 2019 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| Flow-tracker[[151](#bib.bib151)] | Optical Flownet+IOU | - | 工作站（NVIDIA GTX
    1080Ti/-） | 5 | 2019 |'
- en: '| HMTT[[153](#bib.bib153)] | CenterNet+IOU+OSNet | - | - | - | 2019 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| HMTT[[153](#bib.bib153)] | CenterNet+IOU+OSNet | - | - | - | 2019 |'
- en: '| Yang et al[[154](#bib.bib154)] | YOLOv3 + dense-trajectory-Voting | $1920\times
    1080$ | Workstation(NVIDIA GeForce GTX1080Ti/6GB) | 8.6 | 2019 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| Yang 等[[154](#bib.bib154)] | YOLOv3 + dense-trajectory-Voting | $1920\times
    1080$ | 工作站（NVIDIA GeForce GTX1080Ti/6GB） | 8.6 | 2019 |'
- en: '| GGD[[155](#bib.bib155)] | Faster RCNN+GGD | - | Workstation(NVIDIA GTX 1080/-)
    | - | 2019 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| GGD[[155](#bib.bib155)] | Faster RCNN+GGD | - | 工作站（NVIDIA GTX 1080/-） |
    - | 2019 |'
- en: '| COMET[[156](#bib.bib156)] | ResNet-50+Two-stream network | $1080\times 540$
    | Workstation(NVIDIA Tesla V100/16GB) | 24 | 2019 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| COMET[[156](#bib.bib156)] | ResNet-50+Two-stream network | $1080\times 540$
    | 工作站（NVIDIA Tesla V100/16GB） | 24 | 2019 |'
- en: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | Laptop(Core
    i7-2670QM/6GB) | 26.3 | 2019 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| Abughalieh 等[[117](#bib.bib117)] | FAST | $320\times 240$ | 笔记本（Core i7-2670QM/6GB）
    | 26.3 | 2019 |'
- en: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | Embedded(Raspberry
    Pi 2/1GB) | 10.8 | 2019 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| Abughalieh 等[[117](#bib.bib117)] | FAST | $320\times 240$ | 嵌入式（Raspberry
    Pi 2/1GB） | 10.8 | 2019 |'
- en: '| IPGAT[[159](#bib.bib159)] | SiameseNet+LSTM+CGAN | - | Workstation(NVIDIA
    Titan X/32GB) | - | 2020 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| IPGAT[[159](#bib.bib159)] | SiameseNet+LSTM+CGAN | - | 工作站（NVIDIA Titan X/32GB）
    | - | 2020 |'
- en: '| Kapania et al[[160](#bib.bib160)] | YOLOv3+RetinaNet | - | - | - | 2020 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| Kapania 等[[160](#bib.bib160)] | YOLOv3+RetinaNet | - | - | - | 2020 |'
- en: '| PAS tracker[[161](#bib.bib161)] | Cascade R-CNN+Similarity | - | - | - |
    2020 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| PAS tracker[[161](#bib.bib161)] | Cascade R-CNN+Similarity | - | - | - |
    2020 |'
- en: '| DAN[[77](#bib.bib77)] | RetinaNet+DeepSORT | $1500\times 1000$ | - |  | 2020
    |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '| DAN[[77](#bib.bib77)] | RetinaNet+DeepSORT | $1500\times 1000$ | - |  | 2020
    |'
- en: '| DQN[[162](#bib.bib162)] | Faster R-CNN | - | - | - | 2021 |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| DQN[[162](#bib.bib162)] | Faster R-CNN | - | - | - | 2021 |'
- en: '| Youssef et al[[80](#bib.bib80)] | Cascade R-CNN+FPN | $2000\times 1500$ |
    Workstation(NVIDIA Quadro RTX5000/16GB) | - | 2021 |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| Youssef 等[[80](#bib.bib80)] | Cascade R-CNN+FPN | $2000\times 1500$ | 工作站（NVIDIA
    Quadro RTX5000/16GB） | - | 2021 |'
- en: '| HDHNet[[163](#bib.bib163)] | HDHNet+DeepSORT+Cas_RCNN | $1280\times 720$
    | Workstation(NVIDIA TITAN RTX/24GB) | 4.3 | 2021 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| HDHNet[[163](#bib.bib163)] | HDHNet+DeepSORT+Cas_RCNN | $1280\times 720$
    | 工作站（NVIDIA TITAN RTX/24GB） | 4.3 | 2021 |'
- en: '| $2880\times 1620$ | 2.1 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| $2880\times 1620$ | 2.1 |'
- en: VIII Discussion and Conclusion
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 讨论与结论
- en: In this paper, deep learning approached in object and tracking of the remote
    sensing field has been systematically analyzed according to three UAV topics,
    i.e., SOD, VID, and MOT. The conclusions were drawn as follows.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇论文中，深度学习在遥感领域的目标检测与跟踪方法已根据三个无人机主题（即SOD、VID和MOT）进行了系统分析。得出的结论如下。
- en: 'UAV data: The public UAV-borne datasets for object detection and tracking are
    mainly visible data, and the largest image size is $3840\times 2160$ (VisDrone
    dataset). There are only one multiple source data called Vehicle dataset with
    visible-thermal infrared cameras equipped with drones. In label terms, the bounding
    box is not limited to horizontal bounding box strongly dependent on robustness
    to direction, even have oriented bounding box, e.g., in the Vehicle Dataset.'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 无人机数据：公开的无人机载数据集主要用于目标检测和跟踪，最大图像尺寸为$3840\times 2160$（VisDrone数据集）。只有一个多源数据集称为Vehicle数据集，配备有可见光-热红外相机的无人机。在标注方面，边界框不限于水平边界框，而是依赖于方向的鲁棒性，甚至有定向边界框，例如Vehicle数据集中的情况。
- en: 'DL Method: This survey reviews DL-based object detection and tracking methods
    for UAV acquired data from three topics. In general, most classical DL methods,
    by appending extra modules available to UAV challenges, can be applied to these
    three topics. Specifically, considering different requirements for precision and
    speed, the existing static object detection methods especially for UAV are mainly
    based on YOLO (e.g., UAV-YOLO, ComNet, SlimYOLOv3, DAGN, etc), Faster RCNN (e.g.,
    Dshnet, NDFT, D2det, etc), and SSD (e.g., FS SSD). Among them, YOLO, and SSD based
    methods are advantageous in speed. For VID and MOT, there are few methods especially
    designed for UAV data. Most literature are still about classical methods for natural
    scene data, such as Flownet, LSTM for VID, and DeepSort, SiamRPN for MOT. As a
    consequence, their performance is far from perfect, e.g., the highest AP for VisDrone-VID
    is just 65.2%, and for VisDrone-MOT is just 50.80%. Further effort is needed to
    solve the interaction of ground objects and the complexity of the tracking scenes.
    As for systems, the existing UAV object detection and tracking systems are mainly
    based on the classic DL method, where the speed can be guaranteed but the accuracy
    still needs to be improved.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: DL 方法：这项调查回顾了基于深度学习的无人机数据目标检测和跟踪方法，涉及三个主题。一般来说，大多数经典深度学习方法，通过附加额外的模块，可以应用于这些三大主题。具体而言，根据精度和速度的不同要求，目前针对无人机的静态目标检测方法主要基于
    YOLO（例如 UAV-YOLO、ComNet、SlimYOLOv3、DAGN 等）、Faster RCNN（例如 Dshnet、NDFT、D2det 等）和
    SSD（例如 FS SSD）。其中，基于 YOLO 和 SSD 的方法在速度上具有优势。对于 VID 和 MOT，专门针对无人机数据的方法较少。大多数文献仍然是关于自然场景数据的经典方法，例如
    VID 的 Flownet、LSTM 和 MOT 的 DeepSort、SiamRPN。因此，它们的性能仍然远未完美，例如，VisDrone-VID 的最高
    AP 仅为 65.2%，VisDrone-MOT 的最高 AP 仅为 50.80%。需要进一步努力解决地面物体的交互和跟踪场景的复杂性。至于系统，现有的无人机目标检测和跟踪系统主要基于经典的深度学习方法，其中速度可以得到保证，但准确性仍需改进。
- en: 'Computer platforms: The image/video acquired by UAV in this review mainly belongs
    to the remote sensing community. In this community, DL-based methods are mainly
    carried out on various NVIDIA series GPU, e.g., TITAN Xp, RTX 2080Ti, GTX 1080Ti,
    etc. Their processing speed is roughly within the range of $0.2fps\sim 50fps$
    with different image size. Although the research reviewed in [[117](#bib.bib117),
    [116](#bib.bib116)] designs object detection and tracking system using a Raspberry
    Pi 2 minicomputer process for object detection with 11fps, or using a Jetson TX2
    embedded platform for object detection with 8.5fps, even 4.5fps [[227](#bib.bib227)]
    and object tracking with 15fps, but lacks generality.'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 计算平台：本综述中获取的无人机图像/视频主要属于遥感领域。在该领域，基于深度学习的方法主要在各种 NVIDIA 系列 GPU 上进行，例如 TITAN
    Xp、RTX 2080Ti、GTX 1080Ti 等。它们的处理速度大致在 $0.2fps\sim 50fps$ 的范围内，具体取决于图像大小。尽管综述中[[117](#bib.bib117),
    [116](#bib.bib116)]设计了使用 Raspberry Pi 2 微型计算机进行目标检测，速度为 11fps，或使用 Jetson TX2 嵌入式平台进行目标检测，速度为
    8.5fps，甚至 4.5fps [[227](#bib.bib227)] 和目标跟踪 15fps，但缺乏通用性。
- en: Object detection with tracking reflects a perfect union in engineering practice.
    With tracking assistance, detection becomes stable and exhibits no jitter. Meanwhile,
    fine labels and ID information of objects with the same class are also given.
    Through automatic analysis and extraction of trajectory features, false and missed
    detection rates can be significantly reduced. In the near future development of
    object detection and tracking in UAV remote sensing is expected and new techniques
    will emerge to improve these metrics even further. In addition, efficiently processing
    massive multi-source UAV remote sensing data are worth consideration. UAVs equipped
    with different sensors, e.g., visible, infrared, thermal infrared, multispectral,
    hyperspectral sensors, can integrate a variety of sensing modalities to make use
    of their complementary properties, which further realizes more robust and accurate
    object tracking and detection.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 带有跟踪功能的目标检测在工程实践中体现了完美的结合。借助跟踪，检测变得更加稳定且不会出现抖动。同时，还提供了相同类别物体的精细标签和 ID 信息。通过自动分析和提取轨迹特征，可以显著减少虚假和漏检率。在不久的将来，预计无人机遥感中的目标检测和跟踪将会有新的发展，新的技术将进一步提高这些指标。此外，高效处理海量多源无人机遥感数据也是值得考虑的。配备了不同传感器的无人机，例如可见光、红外、热红外、多光谱、超光谱传感器，可以集成多种传感方式，以利用它们的互补特性，从而实现更加稳健和准确的目标跟踪和检测。
- en: References
  id: totrans-462
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Shahbazi, J. Théau, and P. Ménard, “Recent applications of unmanned
    aerial imagery in natural resource management,” GISci. Remote Sens., vol. 51,
    no. 4, pp. 339–365, 2014.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Shahbazi, J. Théau 和 P. Ménard，“无人机影像在自然资源管理中的近期应用”，GISci. Remote Sens.,
    vol. 51, no. 4, pp. 339–365, 2014。'
- en: '[2] H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita, I. Khalil,
    N. S. Othman, A. Khreishah, and M. Guizani, “Unmanned aerial vehicles (uavs):
    A survey on civil applications and key research challenges,” IEEE Access, vol. 7,
    pp. 48572–48634, 2019.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita, I. Khalil,
    N. S. Othman, A. Khreishah 和 M. Guizani，“无人机（uavs）：民用应用与主要研究挑战的调查”，IEEE Access,
    vol. 7, pp. 48572–48634, 2019。'
- en: '[3] M. Shahbazi, J. Théau, and P. Ménard, “Recent applications of unmanned
    aerial imagery in natural resource management,” GISci. Remote Sens., vol. 51,
    no. 4, pp. 339–365, 2014.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] M. Shahbazi, J. Théau 和 P. Ménard，“无人机影像在自然资源管理中的近期应用”，GISci. Remote Sens.,
    vol. 51, no. 4, pp. 339–365, 2014。'
- en: '[4] D. Hong, N. Yokoya, N. Ge, J. Chanussot, and X. Zhu, “Learnable manifold
    alignment (LeMA): A semi-supervised cross-modality learning framework for land
    cover and land use classification,” ISPRS J. Photogramm. Remote Sens., vol. 147,
    pp. 193–205, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] D. Hong, N. Yokoya, N. Ge, J. Chanussot 和 X. Zhu，“可学习的流形对齐（LeMA）：用于土地覆盖和土地利用分类的半监督跨模态学习框架”，ISPRS
    J. Photogramm. Remote Sens., vol. 147, pp. 193–205, 2019。'
- en: '[5] D. Hong, L. Gao, N. Yokoya, J. Yao, J. Chanussot, Q. Du, and B. Zhang,
    “More diverse means better: Multimodal deep learning meets remote-sensing imagery
    classification,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 5, pp. 4340–4354,
    2021.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] D. Hong, L. Gao, N. Yokoya, J. Yao, J. Chanussot, Q. Du 和 B. Zhang，“更多的多样性意味着更好：多模态深度学习与遥感影像分类的结合”，IEEE
    Trans. Geosci. Remote Sens., vol. 59, no. 5, pp. 4340–4354, 2021。'
- en: '[6] E. Honkavaara, H. Saari, J. K. nd I. Pölönen, T. Hakala, P. Litkey, J. Mäkynen,
    and L. Pesonen, “Processing and assessment of spectrometric, stereoscopic imagery
    collected using a lightweight uav spectral camera for precision agriculture,”
    Remote Sens., vol. 5, no. 10, pp. 5006–5039, 2013.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] E. Honkavaara, H. Saari, J. K. 和 I. Pölönen, T. Hakala, P. Litkey, J. Mäkynen
    和 L. Pesonen，“利用轻型无人机光谱相机收集的光谱和立体影像在精确农业中的处理与评估”，Remote Sens., vol. 5, no. 10,
    pp. 5006–5039, 2013。'
- en: '[7] R. Francisco, Q. Zhang, and J. Reid, “Stereo vision three-dimensional terrain
    maps for precision agriculture,” Comput. Electron. Agr., vol. 60, no. 2, pp. 133–143,
    2008.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] R. Francisco, Q. Zhang 和 J. Reid，“用于精确农业的立体视觉三维地形图”，Comput. Electron. Agr.,
    vol. 60, no. 2, pp. 133–143, 2008。'
- en: '[8] H. Menouar, I. Guvenc, K. Akkaya, A. S. Uluagac, A. Kadri, and A. Tuncer,
    “Uav-enabled intelligent transportation systems for the smart city: Applications
    and challenges,” IEEE Commun. Mag., vol. 55, no. 3, pp. 22–28, 2017.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] H. Menouar, I. Guvenc, K. Akkaya, A. S. Uluagac, A. Kadri 和 A. Tuncer，“面向智能城市的无人机智能交通系统：应用与挑战”，IEEE
    Commun. Mag., vol. 55, no. 3, pp. 22–28, 2017。'
- en: '[9] E. Honkavaara, H. Saari, J. Kaivosoja, I. Pölönen, T. Hakala, P. Litkey,
    J. Mäkynen, and L. Pesonen, “Processing and assessment of spectrometric, stereoscopic
    imagery collected using a lightweight uav spectral camera for precision agriculture,”
    Remote Sens., vol. 5, no. 10, pp. 5006–5039, 2013.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] E. Honkavaara, H. Saari, J. Kaivosoja, I. Pölönen, T. Hakala, P. Litkey,
    J. Mäkynen 和 L. Pesonen，“利用轻型无人机光谱相机收集的光谱和立体影像在精确农业中的处理与评估”，Remote Sens., vol.
    5, no. 10, pp. 5006–5039, 2013。'
- en: '[10] M. Erdelj, E. Natalizio, K. R. Chowdhury, and I. F. Akyildiz, “Help from
    the sky: Leveraging uavs for disaster management,” IEEE Pervasive Comput., vol. 16,
    no. 1, pp. 24–32, 2017.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Erdelj, E. Natalizio, K. R. Chowdhury 和 I. F. Akyildiz，“来自天空的帮助：利用无人机进行灾难管理”，IEEE
    Pervasive Comput., vol. 16, no. 1, pp. 24–32, 2017。'
- en: '[11] L. Li, W. Huang, I. Y.-H. Gu, and Q. Tian, “Statistical modeling of complex
    backgrounds for foreground object detection,” IEEE Trans. Image Process., vol. 13,
    no. 11, pp. 1459–1472, 2004.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] L. Li, W. Huang, I. Y.-H. Gu 和 Q. Tian，“复杂背景的统计建模用于前景物体检测”，IEEE Trans.
    Image Process., vol. 13, no. 11, pp. 1459–1472, 2004。'
- en: '[12] M. Haag and H.-H. Nagel, “Combination of edge element and optical flow
    estimates for 3d-model-based vehicle tracking in traffic image sequences,” Int.
    J. Comput. Vis., vol. 35, no. 3, pp. 295–319, 1999.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] M. Haag 和 H.-H. Nagel，“边缘元素与光流估计的结合用于交通图像序列中的3D模型基础车辆跟踪”，Int. J. Comput.
    Vis., vol. 35, no. 3, pp. 295–319, 1999。'
- en: '[13] B. Rasti, D. Hong, R. Hang, P. Ghamisi, X. Kang, J. Chanussot, and J. Benediktsson,
    “Feature extraction for hyperspectral imagery: The evolution from shallow to deep:
    Overview and toolbox,” IEEE Geosci. Remote Sens. Mag., vol. 8, no. 4, pp. 60–88,
    2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] B. Rasti, D. Hong, R. Hang, P. Ghamisi, X. Kang, J. Chanussot 和 J. Benediktsson，“高光谱影像的特征提取：从浅层到深层的演变：概述与工具箱”，IEEE
    Geosci. Remote Sens. Mag., vol. 8, no. 4, pp. 60–88, 2020。'
- en: '[14] X. Wu, W. Li, D. Hong, J. Tian, R. Tao, and Q. Du, “Vehicle detection
    of multi-source remote sensing data using active fine-tuning network,” ISPRS J.
    Photogramm. Remote Sens., vol. 167, pp. 39–53, 2020.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] X. Wu, W. Li, D. Hong, J. Tian, R. Tao 和 Q. Du，“基于主动微调网络的多源遥感数据车辆检测”，ISPRS
    摄影测量与遥感学杂志，卷 167，页码 39–53，2020年。'
- en: '[15] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza, and J. Chanussot, “Graph
    convolutional networks for hyperspectral image classification,” IEEE Trans. Geosci.
    Remote Sens., vol. 59, no. 7, pp. 5966–5978, 2021.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza 和 J. Chanussot，“用于高光谱图像分类的图卷积网络”，IEEE
    地球科学与遥感学报，卷 59，第 7 期，页码 5966–5978，2021年。'
- en: '[16] M. Liu, X. Wang, A. Zhou, X. Fu, Y. Ma, and C. Piao, “Uav-yolo: Small
    object detection on unmanned aerial vehicle perspective,” Sensor, vol. 20, no. 8,
    p. 2238, 2020.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] M. Liu, X. Wang, A. Zhou, X. Fu, Y. Ma 和 C. Piao，“UAV-YOLO：无人机视角的小物体检测”，传感器，卷
    20，第 8 期，页码 2238，2020年。'
- en: '[17] Y. Cai, D. Du, L. Zhang, L. Wen, W. Wang, Y. Wu, and S. Lyu, “Guided attention
    network for object detection and counting on drones,” arXiv preprint arXiv:1909.11307,
    2019.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Y. Cai, D. Du, L. Zhang, L. Wen, W. Wang, Y. Wu 和 S. Lyu，“用于无人机上的目标检测和计数的引导注意网络”，arXiv
    预印本 arXiv:1909.11307，2019年。'
- en: '[18] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in Proc. CVPR, pp. 580–587,
    2014.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] R. Girshick, J. Donahue, T. Darrell 和 J. Malik，“准确的对象检测和语义分割的丰富特征层次”，发表于
    CVPR 会议论文集，页码 580–587，2014年。'
- en: '[19] R. Girshick, “Fast r-cnn,” in Proc. ICCV, pp. 1440–1448, 2015.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] R. Girshick，“Fast R-CNN”，发表于 ICCV 会议论文集，页码 1440–1448，2015年。'
- en: '[20] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in Proc. NIPS, pp. 91–99, 2015.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] S. Ren, K. He, R. Girshick 和 J. Sun，“Faster R-CNN：基于区域提议网络的实时目标检测”，发表于
    NIPS 会议论文集，页码 91–99，2015年。'
- en: '[21] P. Zhu, L. Wen, X. Bian, H. Ling, and Q. Hu, “Vision meets drones: A challenge,”
    arXiv preprint arXiv:1804.07437, 2018.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] P. Zhu, L. Wen, X. Bian, H. Ling 和 Q. Hu，“视觉遇见无人机：一个挑战”，arXiv 预印本 arXiv:1804.07437，2018年。'
- en: '[22] Z. Deng, H. Sun, S. Zhou, J. Zhao, L. Lei, and H. Zou, “Multi-scale object
    detection in remote sensing imagery with convolutional neural networks,” ISPRS
    J. Photogramm. Remote Sens., vol. 145, pp. 3–22, 2018.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Z. Deng, H. Sun, S. Zhou, J. Zhao, L. Lei 和 H. Zou，“基于卷积神经网络的遥感图像中的多尺度目标检测”，ISPRS
    摄影测量与遥感学杂志，卷 145，页码 3–22，2018年。'
- en: '[23] X. Wu, D. Hong, P. Ghamisi, W. Li, and R. Tao, “Msri-ccf: Multi-scale
    and rotation-insensitive convolutional channel features for geospatial object
    detection,” Remote Sens., vol. 10, no. 12, p. 1990, 2018.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] X. Wu, D. Hong, P. Ghamisi, W. Li 和 R. Tao，“MSRI-CCF：用于地理空间目标检测的多尺度和旋转不变卷积通道特征”，遥感学报，卷
    10，第 12 期，页码 1990，2018年。'
- en: '[24] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, and A. Berg,
    “SSD: Single shot multibox detector,” in Proc. ECCV, pp. 21–37, Springer, 2016.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu 和 A. Berg，“SSD：单次多框检测器”，发表于
    ECCV 会议论文集，页码 21–37，Springer，2016年。'
- en: '[25] X. Luo, X. Tian, H. Zhang, W. Hou, G. Leng, W. Xu, H. Jia, X. He, M. Wang,
    and J. Zhang, “Fast automatic vehicle detection in uav images using convolutional
    neural networks,” Remote Sens., vol. 12, no. 12, p. 1994, 2020.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] X. Luo, X. Tian, H. Zhang, W. Hou, G. Leng, W. Xu, H. Jia, X. He, M. Wang
    和 J. Zhang，“基于卷积神经网络的无人机图像中的快速自动车辆检测”，遥感学报，卷 12，第 12 期，页码 1994，2020年。'
- en: '[26] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in Proc. CVPR, pp. 779–788, 2016.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] J. Redmon, S. Divvala, R. Girshick 和 A. Farhadi，“你只看一次：统一的实时目标检测”，发表于
    CVPR 会议论文集，页码 779–788，2016年。'
- en: '[27] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” arXiv preprint arXiv:1704.04861, 2017.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto
    和 H. Adam，“MobileNets：用于移动视觉应用的高效卷积神经网络”，arXiv 预印本 arXiv:1704.04861，2017年。'
- en: '[28] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” arXiv
    preprint arXiv:1804.02767, 2018.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] J. Redmon 和 A. Farhadi，“YOLOv3：一种增量改进”，arXiv 预印本 arXiv:1804.02767，2018年。'
- en: '[29] S. Mehta, M. Rastegari, L. Shapiro, and H. Hajishirzi, “Espnetv2: A light-weight,
    power efficient, and general purpose convolutional neural network,” in Proc. CVPR,
    pp. 9190–9200, 2019.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] S. Mehta, M. Rastegari, L. Shapiro 和 H. Hajishirzi，“ESPNetv2：一种轻量级、节能和通用的卷积神经网络”，发表于
    CVPR 会议论文集，页码 9190–9200，2019年。'
- en: '[30] X. H. Liao and C. H. Zhou, Light- small UAV remote sensing development
    report. 2016.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] X. H. Liao 和 C. H. Zhou，《轻型小型无人机遥感发展报告》，2016年。'
- en: '[31] D. Hong, N. Yokoya, J. Chanussot, and X. X. Zhu, “CoSpace: Common subspace
    learning from hyperspectral-multispectral correspondences,” IEEE Trans. Geosci.
    Remote Sens., vol. 57, no. 7, pp. 4349–4359, 2019.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] D. Hong, N. Yokoya, J. Chanussot, 和 X. X. Zhu, “CoSpace: 从高光谱-多光谱对应中学习常见子空间,”
    IEEE Trans. Geosci. Remote Sens., 卷57, 期7, 页码4349–4359, 2019。'
- en: '[32] D. Hong, N. Yokoya, J. Chanussot, and X. X. Zhu, “An augmented linear
    mixing model to address spectral variability for hyperspectral unmixing,” IEEE
    Trans. Image Process., vol. 28, no. 4, pp. 1923–1938, 2019.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] D. Hong, N. Yokoya, J. Chanussot, 和 X. X. Zhu, “一种增强线性混合模型以解决高光谱解混的光谱变异性,”
    IEEE Trans. Image Process., 卷28, 期4, 页码1923–1938, 2019。'
- en: '[33] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and L. Zitnick, “Microsoft coco: Common objects in context,” in Proc. ECCV, pp. 740–755,
    Springer, 2014.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    和 L. Zitnick, “Microsoft coco: 语境中的常见物体,” 发表在Proc. ECCV, 页码740–755, Springer,
    2014。'
- en: '[34] J. Han, D. Zhang, G. Cheng, N. Liu, and D. Xu, “Advanced deep-learning
    techniques for salient and category-specific object detection: a survey,” IEEE
    Signal Process. Mag., vol. 35, no. 1, pp. 84–100, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] J. Han, D. Zhang, G. Cheng, N. Liu, 和 D. Xu, “突出及类别特定物体检测的高级深度学习技术：综述,”
    IEEE Signal Process. Mag., 卷35, 期1, 页码84–100, 2018。'
- en: '[35] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, “Object detection in optical
    remote sensing images: A survey and a new benchmark,” ISPRS J. Photogramm. Remote
    Sens., vol. 159, pp. 296–307, 2020.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] K. Li, G. Wan, G. Cheng, L. Meng, 和 J. Han, “光学遥感图像中的物体检测：综述及新基准,” ISPRS
    J. Photogramm. Remote Sens., 卷159, 页码296–307, 2020。'
- en: '[36] T. Z. Xiang, G. S. Xia, and L. Zhang, “Mini-uav-based remote sensing:
    Techniques, applications and prospectives,” IEEE GEOSC REM SEN M., vol. 7, no. 3,
    pp. 29–63, 2018.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] T. Z. Xiang, G. S. Xia, 和 L. Zhang, “基于迷你无人机的遥感：技术、应用及前景,” IEEE GEOSC
    REM SEN M., 卷7, 期3, 页码29–63, 2018。'
- en: '[37] H. Yao, R. Qin, and X. Chen, “Unmanned aerial vehicle for remote sensing
    applications—a review,” Remote Sens, vol. 11, no. 12, 2019.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] H. Yao, R. Qin, 和 X. Chen, “无人机在遥感应用中的使用综述,” Remote Sens, 卷11, 期12, 2019。'
- en: '[38] N. Yin, R. Liu, B. Zeng, and N. Liu, “A review: Uav-based remote sensing,”
    in Proc. IOP, vol. 490, p. 062014, IOP Publishing, 2019.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] N. Yin, R. Liu, B. Zeng, 和 N. Liu, “综述：基于无人机的遥感,” 发表在Proc. IOP, 卷490,
    页码062014, IOP Publishing, 2019。'
- en: '[39] L. P. Osco, J. M. Junior, A. P. M. Ramos, L. A. d. C. Jorge, S. N. Fatholahi,
    J. d. A. Silva, E. T. Matsubara, H. Pistori, W. N. Gonçalves, and J. Li, “A review
    on deep learning in uav remote sensing,” arXiv preprint arXiv:2101.10861, 2021.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] L. P. Osco, J. M. Junior, A. P. M. Ramos, L. A. d. C. Jorge, S. N. Fatholahi,
    J. d. A. Silva, E. T. Matsubara, H. Pistori, W. N. Gonçalves, 和 J. Li, “无人机遥感中深度学习综述,”
    arXiv preprint arXiv:2101.10861, 2021。'
- en: '[40] P. Mittal, A. Sharma, and R. Singh, “Deep learning-based object detection
    in low-altitude uav datasets: A survey,” Image Vis Comput, p. 104046, 2020.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] P. Mittal, A. Sharma, 和 R. Singh, “基于深度学习的低空无人机数据集中的物体检测：综述,” Image Vis
    Comput, 页码104046, 2020。'
- en: '[41] S. Srivastava, S. Narayan, and S. Mittal, “A survey of deep learning techniques
    for vehicle detection from uav images,” J. Syst. Archit., 2021.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] S. Srivastava, S. Narayan, 和 S. Mittal, “基于无人机图像的深度学习技术车辆检测综述,” J. Syst.
    Archit., 2021。'
- en: '[42] A. Ayalew and Pooja, “A review on object detection from unmanned aerial
    vehicle using cnn,” IJARIIT, vol. 5, pp. 241–243, 2019.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Ayalew 和 Pooja, “基于CNN的无人机物体检测综述,” IJARIIT, 卷5, 页码241–243, 2019。'
- en: '[43] D. Cazzato, C. Cimarelli, J. L. Sanchez-Lopez, H. Voos, and M. Leo, “A
    survey of computer vision methods for 2d object detection from unmanned aerial
    vehicles,” Journal of Imaging, vol. 6, no. 8, 2020.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] D. Cazzato, C. Cimarelli, J. L. Sanchez-Lopez, H. Voos, 和 M. Leo, “基于无人机的2D物体检测计算机视觉方法综述,”
    Journal of Imaging, 卷6, 期8, 2020。'
- en: '[44] A. Ammar, A. Koubaa, M. Ahmed, A. Saad, and B. Benjdira, “Vehicle detection
    from aerial images using deep learning: A comparative study,” Electronics, vol. 10,
    no. 7, 2021.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Ammar, A. Koubaa, M. Ahmed, A. Saad, 和 B. Benjdira, “使用深度学习进行航空图像中的车辆检测：比较研究,”
    Electronics, 卷10, 期7, 2021。'
- en: '[45] J. Hao, Y. Zhou, G. Zhang, Q. Lv, and Q. Wu, “A review of target tracking
    algorithm based on uav,” in Proc. CBS, pp. 328–333, IEEE, 2018.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J. Hao, Y. Zhou, G. Zhang, Q. Lv, 和 Q. Wu, “基于无人机的目标跟踪算法综述,” 发表在Proc.
    CBS, 页码328–333, IEEE, 2018。'
- en: '[46] G. Pajares, “Overview and current status of remote sensing applications
    based on unmanned aerial vehicles (UAVs),” Photogramm. Eng. Remote Sens., vol. 81,
    no. 4, pp. 281–330, 2015.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] G. Pajares, “基于无人机的遥感应用概述及现状,” Photogramm. Eng. Remote Sens., 卷81, 期4,
    页码281–330, 2015。'
- en: '[47] T. Adão, J. Hruška, L. Pádua, J. Bessa, E. Peres, R. Morais, and J. Sousa,
    “Hyperspectral imaging: A review on uav-based sensors, data processing and applications
    for agriculture and forestry,” Remote Sens,, vol. 9, no. 11, p. 1110, 2017.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] T. Adão，J. Hruška，L. Pádua，J. Bessa，E. Peres，R. Morais，和 J. Sousa，“高光谱成像:
    关于无人机传感器、数据处理和农业及林业应用的综述，”《遥感》，第9卷，第11期，第1110页，2017年。'
- en: '[48] Y. Chen, Y. Zhang, J. Xin, G. Wang, L. Mu, Y. Yi, H. Liu, and D. Liu,
    “UAV image-based forest fire detection approach using convolutional neural network,”
    in Proc. ICIEA, pp. 2118–2123, June 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Y. 陈，Y. 张，J. 辛，G. 王，L. 穆，Y. 易，H. 刘，和 D. 刘，“基于无人机图像的森林火灾检测方法，采用卷积神经网络，”在
    ICIEA 会议上，pp. 2118–2123，2019年6月。'
- en: '[49] T. Xiang, G. Xia, and L. Zhang, “Mini-UAV-based remote sensing: techniques,
    applications and prospectives,” arXiv preprint arXiv:1812.07770, 2018.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] T. 向，G. 夏，和 L. 张，“基于迷你无人机的遥感: 技术、应用与前景，”arXiv 预印本 arXiv:1812.07770，2018年。'
- en: '[50] X. Chen, S. Xiang, C. Liu, and C. Pan, “Vehicle detection in satellite
    images by hybrid deep convolutional neural networks,” IEEE Geosci. Remote Sens.
    Lett., vol. 11, no. 10, pp. 1797–1801, 2014.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] X. 陈，S. 向，C. 刘，和 C. 潘，“通过混合深度卷积神经网络进行卫星图像中的车辆检测，”《IEEE地球科学与遥感快报》，第11卷，第10期，第1797–1801页，2014年。'
- en: '[51] X. Liu, C. Deng, J. Chanussot, D. Hong, and B. Zhao, “Stfnet: A two-stream
    convolutional neural network for spatiotemporal image fusion,” IEEE Trans. Geosci.
    Remote Sens., vol. 57, no. 9, pp. 6552–6564, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] X. 刘，C. 邓，J. Chanussot，D. 洪，和 B. 赵，“STFNet: 一种用于时空图像融合的双流卷积神经网络，”《IEEE地球科学与遥感快报》，第57卷，第9期，第6552–6564页，2019年。'
- en: '[52] Z. Cai, Q. Fan, R. Feris, and N. Vasconcelos, “A unified multi-scale deep
    convolutional neural network for fast object detection,” in Proc. ECCV, pp. 354–370,
    Springer, 2016.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Z. 蔡，Q. 范，R. Feris，和 N. Vasconcelos，“一种用于快速物体检测的统一多尺度深度卷积神经网络，”在 ECCV
    会议上，pp. 354–370，Springer，2016年。'
- en: '[53] C. Chen, Y. Zhang, Q. Lv, S. Wei, X. Wang, X. Sun, and J. Dong, “Rrnet:
    A hybrid detector for object detection in drone-captured images,” in Proc. ICCV
    Workshops, pp. 0–0, 2019.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] C. 陈，Y. 张，Q. 吕，S. 魏，X. 王，X. 孙，和 J. 董，“RRNet: 一种用于无人机捕获图像的混合检测器，”在 ICCV
    Workshops 会议上，pp. 0–0，2019年。'
- en: '[54] Q. Lin, Y. Ding, H. Xu, W. Lin, J. Li, and X. Xie, “Ecascade-rcnn: Enhanced
    cascade rcnn for multi-scale object detection in uav images,” in Proc. ICARA,
    pp. 268–272, 2021.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Q. 林，Y. 丁，H. 徐，W. 林，J. 李，和 X. 谢，“Ecascade-RCNN: 增强的 Cascade RCNN 用于无人机图像中的多尺度物体检测，”在
    ICARA 会议上，pp. 268–272，2021年。'
- en: '[55] Z. Li, X. Liu, Y. Zhao, B. Liu, Z. Huang, and R. Hong, “A lightweight
    multi-scale aggregated model for detecting aerial images captured by uavs,” J.
    Vis. Commun. Image Represent., vol. 77, p. 103058, 2021.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Z. 李，X. 刘，Y. 赵，B. 刘，Z. 黄，和 R. 洪，“用于检测无人机捕获航空图像的轻量级多尺度聚合模型，”《视觉通信与图像表示杂志》，第77卷，第103058页，2021年。'
- en: '[56] W. Zhang, C. Liu, F. Chang, and Y. Song, “Multi-scale and occlusion aware
    network for vehicle detection and segmentation on uav aerial images,” Remote Sens.,
    vol. 12, no. 11, p. 1760, 2020.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] W. 张，C. 刘，F. 常，和 Y. 宋，“针对无人机航空图像中的车辆检测和分割的多尺度和遮挡感知网络，”《遥感》，第12卷，第11期，第1760页，2020年。'
- en: '[57] F. Yang, H. Fan, P. Chu, E. Blasch, and H. Ling, “Clustered object detection
    in aerial images,” in Proc. ICCV, pp. 8311–8320, 2019.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] F. 杨，H. 范，P. 朱，E. Blasch，和 H. Ling，“航空图像中的聚类物体检测，”在 ICCV 会议上，pp. 8311–8320，2019年。'
- en: '[58] D. R. Pailla, V. Kollerathu, and S. S. Chennamsetty, “Object detection
    on aerial imagery using centernet,” arXiv preprint arXiv:1908.08244, 2019.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] D. R. Pailla，V. Kollerathu，和 S. S. Chennamsetty，“使用 CenterNet 对航空图像进行物体检测，”arXiv
    预印本 arXiv:1908.08244，2019年。'
- en: '[59] J. Yang, X. Xie, and W. Yang, “Effective contexts for uav vehicle detection,”
    IEEE Access, vol. 7, pp. 85042–85054, 2019.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] J. 杨，X. 谢，和 W. 杨，“有效的上下文用于无人机车辆检测，”《IEEE Access》，第7卷，第85042–85054页，2019年。'
- en: '[60] C. Chen, W. Gong, Y. Chen, and W. Li, “Object detection in remote sensing
    images based on a scene-contextual feature pyramid network,” Remote Sens., vol. 11,
    no. 3, p. 339, 2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] C. 陈，W. 龚，Y. 陈，和 W. 李，“基于场景上下文特征金字塔网络的遥感图像中的物体检测，”《遥感》，第11卷，第3期，第339页，2019年。'
- en: '[61] R. Zhang, Z. Shao, X. Huang, J. Wang, and D. Li, “Object detection in
    uav images via global density fused convolutional network,” Remote Sens., vol. 12,
    no. 19, p. 3140, 2020.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] R. 张，Z. 邵，X. 黄，J. 王，和 D. 李，“通过全局密度融合卷积网络在无人机图像中进行物体检测，”《遥感》，第12卷，第19期，第3140页，2020年。'
- en: '[62] X. Zhang, E. Izquierdo, and K. Chandramouli, “Dense and small object detection
    in uav vision based on cascade network,” in Proc. ICCV Workshops, pp. 0–0, 2019.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] X. 张，E. Izquierdo，和 K. Chandramouli，“基于级联网络的无人机视觉中的密集和小物体检测，”在 ICCV Workshops
    会议上，pp. 0–0，2019年。'
- en: '[63] Y. Liu, Z. Ding, Y. Cao, and M. Chang, “Multi-scale feature fusion uav
    image object detection method based on dilated convolution and attention mechanism,”
    in Proc. ICIT, pp. 125–132, 2020.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Y. 刘，Z. 丁，Y. 曹，和 M. 常，"基于膨胀卷积和注意力机制的多尺度特征融合无人机图像物体检测方法"，在 Proc. ICIT，第125–132页，2020年。'
- en: '[64] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in Proc. CVPR, pp. 1492–1500, 2017.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] S. 谢，R. 吉尔什克，P. 多拉尔，Z. 图，和 K. 何，"用于深度神经网络的聚合残差变换"，在 Proc. CVPR，第1492–1500页，2017年。'
- en: '[65] P. Zhang, Y. Zhong, and X. Li, “Slimyolov3: Narrower, faster and better
    for real-time uav applications,” in Proc. ICCV, pp. 0–0, 2019.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] P. 张，Y. 钟，和 X. 李，"Slimyolov3: 更窄、更快且更适合实时无人机应用"，在 Proc. ICCV，第0–0页，2019年。'
- en: '[66] X. Liang, J. Zhang, L. Zhuo, Y. Li, and Q. Tian, “Small object detection
    in unmanned aerial vehicle images using feature fusion and scaling-based single
    shot detector with spatial context analysis,” IEEE Trans. Circuits Syst. Video
    Technol., vol. 30, no. 6, pp. 1758–1770, 2019.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] X. 梁，J. 张，L. 卓，Y. 李，和 Q. 田，"使用特征融合和基于尺度的单次检测器及空间上下文分析的无人机图像小物体检测"，IEEE电路与系统视频技术学报，第30卷，第6期，第1758–1770页，2019年。'
- en: '[67] H. Wang, Z. Wang, M. Jia, A. Li, T. Feng, W. Zhang, and L. Jiao, “Spatial
    attention for multi-scale feature refinement for object detection,” in Proc. ICCV
    Workshops, pp. 0–0, 2019.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] H. 王，Z. 王，M. 贾，A. 李，T. 冯，W. 张，和 L. 焦，"用于物体检测的多尺度特征细化的空间注意力"，在 Proc. ICCV
    Workshops，第0–0页，2019年。'
- en: '[68] Q. Wu and Y. Zhou, “Real-time object detection based on unmanned aerial
    vehicle,” in Proc. DDCLS, pp. 574–579, 2019.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Q. 吴 和 Y. 周，"基于无人机的实时物体检测"，在 Proc. DDCLS，第574–579页，2019年。'
- en: '[69] Z. Wu, K. Suresh, P. Narayanan, H. Xu, H. Kwon, and Z. Wang, “Delving
    into robust object detection from unmanned aerial vehicles: A deep nuisance disentanglement
    approach,” in Proc. ICCV, pp. 1201–1210, 2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Z. 吴，K. 苏雷什，P. 纳拉扬，H. 许，H. 權，和 Z. 王，"深入探讨无人机的鲁棒物体检测：一种深度干扰解耦方法"，在 Proc.
    ICCV，第1201–1210页，2019年。'
- en: '[70] Z. Liu, G. Gao, L. Sun, and Z. Fang, “Hrdnet: high-resolution detection
    network for small objects,” arXiv preprint arXiv:2006.07607, 2020.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Z. 刘，G. 高，L. 孙，和 Z. 方，"Hrdnet: 高分辨率小物体检测网络"，arXiv预印本 arXiv:2006.07607，2020年。'
- en: '[71] B. M. Albaba and S. Ozer, “Synet: An ensemble network for object detection
    in uav images,” arXiv preprint arXiv:2012.12991, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] B. M. 阿尔巴巴 和 S. 奥泽尔，"Synet: 用于无人机图像的物体检测的集成网络"，arXiv预印本 arXiv:2012.12991，2020年。'
- en: '[72] M. Li, X. Zhao, J. Li, and L. Nan, “Comnet: Combinational neural network
    for object detection in uav-borne thermal images,” IEEE Trans. Geosci. Remote.
    Sens., 2020.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. 李，X. 赵，J. 李，和 L. 南，"Comnet: 用于无人机搭载热成像的物体检测的组合神经网络"，IEEE地球科学与遥感学报，2020年。'
- en: '[73] Y. Liu, F. Yang, and P. Hu, “Small-object detection in uav-captured images
    via multi-branch parallel feature pyramid networks,” IEEE Access, vol. 8, pp. 145740–145750,
    2020.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. 刘，F. 杨，和 P. 胡，"通过多分支并行特征金字塔网络在无人机捕获图像中检测小物体"，IEEE Access，第8卷，第145740–145750页，2020年。'
- en: '[74] J. Cao, H. Cholakkal, R. M. Anwer, F. S. Khan, Y. Pang, and L. Shao, “D2det:
    Towards high quality object detection and instance segmentation,” in Proc. CVPR,
    pp. 11485–11494, 2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] J. 曹，H. 乔拉卡尔，R. M. 安瓦尔，F. S. 韩，Y. 庞，和 L. 邵，"D2det: 面向高质量物体检测和实例分割"，在 Proc.
    CVPR，第11485–11494页，2020年。'
- en: '[75] Z. Zhang, Y. Liu, T. Liu, Z. Lin, and S. Wang, “Dagn: A real-time uav
    remote sensing image vehicle detection framework,” IEEE Geosci. Remote Sens. Lett.,
    vol. 17, no. 11, pp. 1884–1888, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Z. 张，Y. 刘，T. 刘，Z. 林，和 S. 王，"Dagn: 一种实时无人机遥感图像车辆检测框架"，IEEE地球科学与遥感快报，第17卷，第11期，第1884–1888页，2019年。'
- en: '[76] C. YuanQiang, D. Du, L. Zhang, L. Wen, W. Wang, Y. Wu, and S. Lyu, “Guided
    attention network for object detection and counting on drones,” in Proc. ACM MM,
    pp. 709–717, 2020.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] C. 袁强，D. 杜，L. 张，L. 温，W. 王，Y. 吴，和 S. 吕，"用于无人机物体检测和计数的引导注意力网络"，在 Proc. ACM
    MM， 第709–717页，2020年。'
- en: '[77] A. Jadhav, P. Mukherjee, V. Kaushik, and B. Lall, “Aerial multi-object
    tracking by detection using deep association networks,” in Proc. NCC, pp. 1–6,
    2020.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] A. 贾达夫，P. 穆克吉，V. 考什克，和 B. 拉尔，"通过深度关联网络进行的空中多物体跟踪"，在 Proc. NCC，第1–6页，2020年。'
- en: '[78] J. Zhang, X. Liang, M. Wang, L. Yang, and L. Zhuo, “Coarse-to-fine object
    detection in unmanned aerial vehicle imagery using lightweight convolutional neural
    network and deep motion saliency,” Neurocomputing, vol. 398, pp. 555–565, 2020.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] J. 张，X. 梁，M. 王，L. 杨，和 L. 卓，"使用轻量级卷积神经网络和深度运动显著性进行无人机图像中的粗到精物体检测"，Neurocomputing，第398卷，第555–565页，2020年。'
- en: '[79] G. Tian, J. Liu, and W. Yang, “A dual neural network for object detection
    in uav images,” Neurocomputing, vol. 443, pp. 292–301, 2021.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] G. Tian, J. Liu 和 W. Yang, “用于无人机图像中物体检测的双神经网络,” Neurocomputing, 卷 443,
    页码 292–301, 2021年。'
- en: '[80] Y. Youssef and M. Elshenawy, “Automatic vehicle counting and tracking
    in aerial video feeds using cascade region-based convolutional neural networks
    and feature pyramid networks,” Transp. Res. Rec., p. 0361198121997833, 2021.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Y. Youssef 和 M. Elshenawy, “使用级联区域卷积神经网络和特征金字塔网络在空中视频流中进行自动车辆计数和跟踪,” Transp.
    Res. Rec., 页码 0361198121997833, 2021年。'
- en: '[81] W. Yu, T. Yang, and C. Chen, “Towards resolving the challenge of long-tail
    distribution in uav images for object detection,” in Proc. WACV, pp. 3258–3267,
    2021.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] W. Yu, T. Yang 和 C. Chen, “针对无人机图像中长尾分布挑战的解决方案,” 在 Proc. WACV, 页码 3258–3267,
    2021年。'
- en: '[82] J. Li, X. Liang, Y. Wei, T. Xu, J. Feng, and S. Yan, “Perceptual generative
    adversarial networks for small object detection,” in Proc. CVPR, pp. 1222–1230,
    2017.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] J. Li, X. Liang, Y. Wei, T. Xu, J. Feng 和 S. Yan, “用于小物体检测的感知生成对抗网络,”
    在 Proc. CVPR, 页码 1222–1230, 2017年。'
- en: '[83] X. Hu, X. Xu, Y. Xiao, H. Chen, S. He, J. Qin, and P. Heng, “Sinet: A
    scale-insensitive convolutional neural network for fast vehicle detection,” IEEE
    Trans. Intell. Transp. Syst., vol. 20, no. 3, pp. 1010–1019, 2018.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] X. Hu, X. Xu, Y. Xiao, H. Chen, S. He, J. Qin 和 P. Heng, “Sinet: 一种对尺度不敏感的卷积神经网络用于快速车辆检测,”
    IEEE Trans. Intell. Transp. Syst., 卷 20, 期 3, 页码 1010–1019, 2018年。'
- en: '[84] Z. Tang, X. Liu, G. Shen, and B. Yang, “Penet: Object detection using
    points estimation in aerial images,” arXiv preprint arXiv:2001.08247, 2020.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Z. Tang, X. Liu, G. Shen 和 B. Yang, “Penet: 使用点估计在空中图像中进行物体检测,” arXiv
    预印本 arXiv:2001.08247, 2020年。'
- en: '[85] V. Ruzicka and F. Franchetti, “Fast and accurate object detection in high
    resolution 4k and 8k video using gpus,” in Proc. HPEC, pp. 1–7, IEEE, 2018.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] V. Ruzicka 和 F. Franchetti, “使用 GPU 在高分辨率 4K 和 8K 视频中进行快速准确的物体检测,” 在 Proc.
    HPEC, 页码 1–7, IEEE, 2018年。'
- en: '[86] G. Plastiras, C. Kyrkou, and T. Theocharides, “Efficient convnet-based
    object detection for unmanned aerial vehicles by selective tile processing,” in
    Proc. ICDSC, pp. 1–6, 2018.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] G. Plastiras, C. Kyrkou 和 T. Theocharides, “通过选择性切片处理实现无人机的高效卷积网络物体检测,”
    在 Proc. ICDSC, 页码 1–6, 2018年。'
- en: '[87] O. Unel, B. Ozkalayci, and C. Cigla, “The power of tiling for small object
    detection,” in Proc. CVPR Workshops, June 2019.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] O. Unel, B. Ozkalayci 和 C. Cigla, “小物体检测中的切片技术的力量,” 在 Proc. CVPR Workshops,
    2019年6月。'
- en: '[88] M. L. Mekhalfi, M. B. Bejiga, D. Soresina, F. Melgani, and B. Demir, “Capsule
    networks for object detection in uav imagery,” Remote Sens., vol. 11, no. 14,
    p. 1694, 2019.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] M. L. Mekhalfi, M. B. Bejiga, D. Soresina, F. Melgani 和 B. Demir, “用于无人机图像中物体检测的胶囊网络,”
    Remote Sens., 卷 11, 期 14, 页码 1694, 2019年。'
- en: '[89] G. Cheng, P. Zhou, and J. Han, “Learning rotation-invariant convolutional
    neural networks for object detection in vhr optical remote sensing images,” IEEE
    Trans. Geosci. Remote Sens., vol. 54, no. 12, pp. 7405–7415, 2016.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] G. Cheng, P. Zhou 和 J. Han, “用于 VHR 光学遥感图像中物体检测的旋转不变卷积神经网络学习,” IEEE Trans.
    Geosci. Remote Sens., 卷 54, 期 12, 页码 7405–7415, 2016年。'
- en: '[90] D. Laptev, N. Savinov, J. Buhmann, and M. Pollefeys, “Ti-pooling: transformation-invariant
    pooling for feature learning in convolutional neural networks,” in Proc. CVPR,
    pp. 289–297, 2016.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] D. Laptev, N. Savinov, J. Buhmann 和 M. Pollefeys, “Ti-pooling: 转换不变的池化用于卷积神经网络中的特征学习,”
    在 Proc. CVPR, 页码 289–297, 2016年。'
- en: '[91] G. Cheng, P. Zhou, and J. Han, “Rifd-cnn: Rotation-invariant and fisher
    discriminative convolutional neural networks for object detection,” in Proc. CVPR,
    pp. 2884–2893, 2016.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] G. Cheng, P. Zhou 和 J. Han, “Rifd-cnn: 用于物体检测的旋转不变和 Fisher 判别卷积神经网络,”
    在 Proc. CVPR, 页码 2884–2893, 2016年。'
- en: '[92] G. Cheng, J. Han, P. Zhou, and D. Xu, “Learning rotation-invariant and
    fisher discriminative convolutional neural networks for object detection,” IEEE
    Trans. Image Process, vol. 28, no. 1, pp. 265–278, 2018.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] G. Cheng, J. Han, P. Zhou 和 D. Xu, “学习旋转不变和 Fisher 判别卷积神经网络用于物体检测,” IEEE
    Trans. Image Process, 卷 28, 期 1, 页码 265–278, 2018年。'
- en: '[93] Z. Liu, H. Wang, L. Weng, and Y. Yang, “Ship rotated bounding box space
    for ship extraction from high-resolution optical satellite images with complex
    backgrounds,” IEEE Geosci. Remote Sens.Lett., vol. 13, no. 8, pp. 1074–1078, 2016.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Z. Liu, H. Wang, L. Weng 和 Y. Yang, “用于从高分辨率光学卫星图像中提取船只的旋转边界框空间,” IEEE
    Geosci. Remote Sens. Lett., 卷 13, 期 8, 页码 1074–1078, 2016年。'
- en: '[94] Z. Liu, J. Hu, L. Weng, and Y. Yang, “Rotated region based cnn for ship
    detection,” in Proc. ICIP, pp. 900–904, IEEE, 2017.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Z. Liu, J. Hu, L. Weng 和 Y. Yang, “用于船只检测的旋转区域卷积神经网络,” 在 Proc. ICIP, 页码
    900–904, IEEE, 2017年。'
- en: '[95] D. Marcos, M. Volpi, and D. Tuia, “Learning rotation invariant convolutional
    filters for texture classification,” in Proc. ICPR, pp. 2012–2017, IEEE, 2016.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] D. Marcos, M. Volpi, 和 D. Tuia, “学习旋转不变卷积滤波器用于纹理分类,” 在 Proc. ICPR, pp. 2012–2017,
    IEEE, 2016。'
- en: '[96] K. Li, G. Cheng, S. Bu, and X. You, “Rotation-insensitive and context-augmented
    object detection in remote sensing images,” IEEE Trans. Geosci. Remote Sens.,
    vol. 56, no. 4, pp. 2337–2348, 2017.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] K. Li, G. Cheng, S. Bu, 和 X. You, “旋转不敏感和上下文增强的遥感图像目标检测,” IEEE Trans.
    Geosci. Remote Sens., vol. 56, no. 4, pp. 2337–2348, 2017。'
- en: '[97] K. Fu, Z. Chang, Y. Zhang, G. Xu, K. Zhang, and X. Sun, “Rotation-aware
    and multi-scale convolutional neural network for object detection in remote sensing
    images,” ISPRS J. Photogramm. Remote Sens., vol. 161, pp. 294–308, 2020.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] K. Fu, Z. Chang, Y. Zhang, G. Xu, K. Zhang, 和 X. Sun, “旋转感知和多尺度卷积神经网络用于遥感图像中的目标检测,”
    ISPRS J. Photogramm. Remote Sens., vol. 161, pp. 294–308, 2020。'
- en: '[98] Y. Yu, H. Guan, D. Li, T. Gu, E. Tang, and A. Li, “Orientation guided
    anchoring for geospatial object detection from remote sensing imagery,” ISPRS
    J. Photogramm. Remote Sens., vol. 160, pp. 67–82, 2020.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Y. Yu, H. Guan, D. Li, T. Gu, E. Tang, 和 A. Li, “面向导向的锚点用于从遥感图像中检测地理空间对象,”
    ISPRS J. Photogramm. Remote Sens., vol. 160, pp. 67–82, 2020。'
- en: '[99] M. Liao, Z. Zhu, B. Shi, G.-s. Xia, and X. Bai, “Rotation-sensitive regression
    for oriented scene text detection,” in Proc. CVPR, pp. 5909–5918, 2018.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] M. Liao, Z. Zhu, B. Shi, G.-s. Xia, 和 X. Bai, “旋转敏感回归用于定向场景文本检测,” 在 Proc.
    CVPR, pp. 5909–5918, 2018。'
- en: '[100] M. Liao, B. Shi, and X. Bai, “Textboxes$++$: A single-shot oriented scene
    text detector,” IEEE Trans. Image Process., vol. 27, no. 8, pp. 3676–3690, 2018.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] M. Liao, B. Shi, 和 X. Bai, “Textboxes$++$: 一种单次检测定向场景文本的检测器,” IEEE Trans.
    Image Process., vol. 27, no. 8, pp. 3676–3690, 2018。'
- en: '[101] Y. Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu, and Z. Luo,
    “R 2 cnn: Rotational region cnn for arbitrarily-oriented scene text detection,”
    in Proc. ICPR, pp. 3610–3615, IEEE, 2018.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Y. Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu, 和 Z. Luo,
    “R 2 cnn: 用于任意定向场景文本检测的旋转区域cnn,” 在 Proc. ICPR, pp. 3610–3615, IEEE, 2018。'
- en: '[102] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, and X. Xue, “Arbitrary-oriented
    scene text detection via rotation proposals,” IEEE Trans. Multimedia, vol. 20,
    no. 11, pp. 3111–3122, 2018.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, 和 X. Xue, “通过旋转提案进行任意方向场景文本检测,”
    IEEE Trans. Multimedia, vol. 20, no. 11, pp. 3111–3122, 2018。'
- en: '[103] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo,
    and L. Zhang, “Dota: A large-scale dataset for object detection in aerial images,”
    in Proc. CVPR, pp. 3974–3983, 2018.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo,
    和 L. Zhang, “Dota: 一个用于航空图像目标检测的大规模数据集,” 在 Proc. CVPR, pp. 3974–3983, 2018。'
- en: '[104] Y. Li, Q. Huang, X. Pei, L. Jiao, and R. Shang, “Radet: Refine feature
    pyramid network and multi-layer attention network for arbitrary-oriented object
    detection of remote sensing images,” Remote Sens., vol. 12, no. 3, p. 389, 2020.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Y. Li, Q. Huang, X. Pei, L. Jiao, 和 R. Shang, “Radet: 改进特征金字塔网络和多层注意力网络用于遥感图像中的任意方向目标检测,”
    Remote Sens., vol. 12, no. 3, p. 389, 2020。'
- en: '[105] Y. Zhou, Q. Ye, Q. Qiu, and J. Jiao, “Oriented response networks,” in
    Proc. CVPR, pp. 519–528, 2017.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Y. Zhou, Q. Ye, Q. Qiu, 和 J. Jiao, “定向响应网络,” 在 Proc. CVPR, pp. 519–528,
    2017。'
- en: '[106] C. Esteves, C. Allen-Blanchette, X. Zhou, and K. Daniilidis, “Polar transformer
    networks,” arXiv preprint arXiv:1709.01889, 2017.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] C. Esteves, C. Allen-Blanchette, X. Zhou, 和 K. Daniilidis, “极坐标变换网络,”
    arXiv 预印本 arXiv:1709.01889, 2017。'
- en: '[107] K. S. Tai, P. Bailis, and G. Valiant, “Equivariant transformer networks,”
    arXiv preprint arXiv:1901.11399, 2019.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] K. S. Tai, P. Bailis, 和 G. Valiant, “等变变换网络,” arXiv 预印本 arXiv:1901.11399,
    2019。'
- en: '[108] K. Zhou, Z. Zhang, C. Gao, and J. Liu, “Rotated feature network for multiorientation
    object detection of remote-sensing images,” IEEE Geosci. Remote Sensing Lett.,
    pp. 1–5, 2020.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] K. Zhou, Z. Zhang, C. Gao, 和 J. Liu, “用于多方向目标检测的旋转特征网络,” IEEE Geosci.
    Remote Sensing Lett., pp. 1–5, 2020。'
- en: '[109] U. Schmidt and S. Roth, “Learning rotation-aware features: From invariant
    priors to equivariant descriptors,” in Proc. CVPR, pp. 2050–2057, IEEE, 2012.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] U. Schmidt 和 S. Roth, “学习旋转感知特征：从不变先验到等变描述符,” 在 Proc. CVPR, pp. 2050–2057,
    IEEE, 2012。'
- en: '[110] X. Wu, D. Hong, J. Tian, J. Chanussot, W. Li, and R. Tao, “ORSIm Detector:
    A novel object detection framework in optical remote sensing imagery using spatial-frequency
    channel features,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 7, pp. 5146–5158,
    2019.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] X. Wu, D. Hong, J. Tian, J. Chanussot, W. Li, 和 R. Tao, “ORSIm 检测器：一种用于光学遥感图像的空间频率通道特征的新型目标检测框架,”
    IEEE Trans. Geosci. Remote Sens., vol. 57, no. 7, pp. 5146–5158, 2019。'
- en: '[111] R. J. Wang, X. Li, and C. X. Ling, “Pelee: A real-time object detection
    system on mobile devices,” in Proc. NIPS, pp. 1963–1972, 2018.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] R. J. Wang, X. Li, 和 C. X. Ling，“Pelee: 一种实时的移动设备对象检测系统，”发表于Proc. NIPS，第1963–1972页，2018年。'
- en: '[112] Y. Yang, G. Xie, and Y. Qu, “Real-time detection of aircraft objects
    in remote sensing images based on improved yolov4,” in Proc. IAEAC, vol. 5, pp. 1156–1164,
    2021.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Y. Yang, G. Xie, 和 Y. Qu，“基于改进的yolov4的遥感图像中飞机对象的实时检测，”发表于Proc. IAEAC，第5卷，第1156–1164页，2021年。'
- en: '[113] X. Wang, W. Li, W. Guo, and K. Cao, “Spb-yolo: An efficient real-time
    detector for unmanned aerial vehicle images,” in Prof. ICAIIC, pp. 099–104, 2021.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] X. Wang, W. Li, W. Guo, 和 K. Cao，“Spb-yolo: 一种高效的实时无人机图像检测器，”发表于Prof.
    ICAIIC，第099–104页，2021年。'
- en: '[114] Z. Pi, Y. Lian, X. Chen, Y. Wu, Y. Li, and L. Jiao, “A novel spatial
    and temporal context-aware approach for drone-based video object detection,” in
    Proc. ICCV Workshops, Oct 2019.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Z. Pi, Y. Lian, X. Chen, Y. Wu, Y. Li, 和 L. Jiao，“一种新颖的空间和时间上下文感知方法用于基于无人机的视频对象检测，”发表于Proc.
    ICCV Workshops，2019年10月。'
- en: '[115] T. Wang, J. Xiong, X. Xu, and Y. Shi, “Scnn: a general distribution based
    statistical convolutional neural network with application to video object detection,”
    in Proc. AAAI, vol. 33, pp. 5321–5328, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] T. Wang, J. Xiong, X. Xu, 和 Y. Shi，“Scnn: 一种基于通用分布的统计卷积神经网络及其在视频对象检测中的应用，”发表于Proc.
    AAAI，第33卷，第5321–5328页，2019年。'
- en: '[116] P. Nousi, I. Mademlis, I. Karakostas, A. Tefas, and I. Pitas, “Embedded
    uav real-time visual object detection and tracking,” in Proc. RCAR, pp. 708–713,
    2019.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] P. Nousi, I. Mademlis, I. Karakostas, A. Tefas, 和 I. Pitas，“嵌入式无人机实时视觉对象检测与跟踪，”发表于Proc.
    RCAR，第708–713页，2019年。'
- en: '[117] K. M. Abughalieh, B. H. Sababha, and N. A. Rawashdeh, “A video-based
    object detection and tracking system for weight sensitive uavs,” Multimed. Tools.
    Appl., vol. 78, no. 7, pp. 9149–9167, 2019.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] K. M. Abughalieh, B. H. Sababha, 和 N. A. Rawashdeh，“一种基于视频的对象检测与跟踪系统，针对重量敏感的无人机，”Multimed.
    Tools. Appl., 卷78，第7期，第9149–9167页，2019年。'
- en: '[118] Y. Zhang, L. Shen, X. Wang, and H.-M. Hu, “Drone video object detection
    using convolutional neural networks with time domain motion features,” in Proc.
    MIPR, pp. 153–156, 2020.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Y. Zhang, L. Shen, X. Wang, 和 H.-M. Hu，“使用时间域运动特征的卷积神经网络进行无人机视频对象检测，”发表于Proc.
    MIPR，第153–156页，2020年。'
- en: '[119] M. Mandal, L. K. Kumar, and S. K. Vipparthi, “Mor-uav: A benchmark dataset
    and baselines for moving object recognition in uav videos,” in Proc. ACM MM, pp. 2626–2635,
    2020.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Mandal, L. K. Kumar, 和 S. K. Vipparthi，“Mor-uav: 一个用于无人机视频中移动对象识别的基准数据集和基线，”发表于Proc.
    ACM MM，第2626–2635页，2020年。'
- en: '[120] H. Xie and H. Shin, “Two-stream small-scale pedestrian detection network
    with feature aggregation for drone-view videos,” Multidim. Syst. Sign. P., pp. 1–17,
    2021.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] H. Xie 和 H. Shin，“一种用于无人机视角视频的小尺度行人检测网络，结合了特征聚合，”Multidim. Syst. Sign.
    P., 第1–17页，2021年。'
- en: '[121] B. Bosquet, M. Mucientes, and V. M. Brea, “Stdnet-st: Spatio-temporal
    convnet for small object detection,” Pattern Recognit., p. 107929, 2021.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] B. Bosquet, M. Mucientes, 和 V. M. Brea，“Stdnet-st: 小物体检测的时空卷积网络，”Pattern
    Recognit., 第107929页，2021年。'
- en: '[122] K. Kang, W. Ouyang, H. Li, and X. Wang, “Object detection from video
    tubelets with convolutional neural networks,” in Proc. CVPR, pp. 817–825, 2016.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] K. Kang, W. Ouyang, H. Li, 和 X. Wang，“从视频Tubelets中检测物体的卷积神经网络，”发表于Proc.
    CVPR，第817–825页，2016年。'
- en: '[123] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang,
    R. Wang, X. Wang, et al., “T-cnn: Tubelets with convolutional neural networks
    for object detection from videos,” IEEE Trans. Circuits Syst. Video Technol.,
    vol. 28, no. 10, pp. 2896–2907, 2017.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang,
    R. Wang, X. Wang等，“T-cnn: 用于从视频中检测物体的卷积神经网络Tubelets，”IEEE Trans. Circuits Syst.
    Video Technol., 卷28，第10期，第2896–2907页，2017年。'
- en: '[124] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh, H. Shi,
    J. Li, S. Yan, and T. S. Huang, “Seq-nms for video object detection,” arXiv preprint
    arXiv:1602.08465, 2016.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh, H.
    Shi, J. Li, S. Yan, 和 T. S. Huang，“Seq-nms用于视频对象检测，”arXiv预印本arXiv:1602.08465，2016年。'
- en: '[125] Q. Zhao, Y. Wang, T. Sheng, and Z. Tang, “Comprehensive feature enhancement
    module for single-shot object detector,” in Proc. ACCV, pp. 325–340, Springer,
    2018.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Q. Zhao, Y. Wang, T. Sheng, 和 Z. Tang，“单次检测器的综合特征增强模块，”发表于Proc. ACCV，第325–340页，Springer，2018年。'
- en: '[126] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, “Eco: Efficient
    convolution operators for tracking,” in Proc. CVPR, pp. 6638–6646, 2017.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] M. Danelljan, G. Bhat, F. Shahbaz Khan, 和 M. Felsberg，“Eco: 用于跟踪的高效卷积操作符，”发表于Proc.
    CVPR，第6638–6646页，2017年。'
- en: '[127] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution representation
    learning for human pose estimation,” in Proc. CVPR, pp. 5693–5703, 2019.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] K. Sun, B. Xiao, D. Liu 和 J. Wang, “用于人体姿态估计的深度高分辨率表示学习，” 见 Proc. CVPR,
    pp. 5693–5703, 2019.'
- en: '[128] Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality
    object detection,” in Proc. CVPR, pp. 6154–6162, 2018.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Z. Cai 和 N. Vasconcelos, “Cascade r-cnn: 深入探讨高质量目标检测，” 见 Proc. CVPR,
    pp. 6154–6162, 2018.'
- en: '[129] X. Zhou, D. Wang, and P. Krähenbühl, “Objects as points,” arXiv preprint
    arXiv:1904.07850, 2019.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] X. Zhou, D. Wang 和 P. Krähenbühl, “对象作为点，” arXiv 预印本 arXiv:1904.07850,
    2019.'
- en: '[130] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in Proc. ICCV, pp. 2980–2988, 2017.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] T.-Y. Lin, P. Goyal, R. Girshick, K. He 和 P. Dollár, “用于密集目标检测的焦点损失，”
    见 Proc. ICCV, pp. 2980–2988, 2017.'
- en: '[131] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in Proc. CVPR, pp. 2117–2125,
    2017.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan 和 S. Belongie,
    “用于目标检测的特征金字塔网络，” 见 Proc. CVPR, pp. 2117–2125, 2017.'
- en: '[132] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, “Pwc-net: Cnns for optical
    flow using pyramid, warping, and cost volume,” in Prof. CVPR, pp. 8934–8943, 2018.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] D. Sun, X. Yang, M.-Y. Liu 和 J. Kautz, “Pwc-net: 使用金字塔、扭曲和成本体积的光流 CNNs，”
    见 Prof. CVPR, pp. 8934–8943, 2018.'
- en: '[133] A. Ranjan and M. J. Black, “Optical flow estimation using a spatial pyramid
    network,” in Prof. CVPR, pp. 2720–2729, 2017.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] A. Ranjan 和 M. J. Black, “使用空间金字塔网络进行光流估计，” 见 Prof. CVPR, pp. 2720–2729,
    2017.'
- en: '[134] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, “Deep feature flow for
    video recognition,” in Proc. CVPR, pp. 2349–2358, 2017.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] X. Zhu, Y. Xiong, J. Dai, L. Yuan 和 Y. Wei, “用于视频识别的深度特征流，” 见 Proc. CVPR,
    pp. 2349–2358, 2017.'
- en: '[135] X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei, “Flow-guided feature aggregation
    for video object detection,” in Proc. ICCV, pp. 408–417, 2017.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] X. Zhu, Y. Wang, J. Dai, L. Yuan 和 Y. Wei, “基于流引导的特征聚合用于视频目标检测，” 见 Proc.
    ICCV, pp. 408–417, 2017.'
- en: '[136] C. Hetang, H. Qin, S. Liu, and J. Yan, “Impression network for video
    object detection,” arXiv preprint arXiv:1712.05896, 2017.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] C. Hetang, H. Qin, S. Liu 和 J. Yan, “用于视频目标检测的印象网络，” arXiv 预印本 arXiv:1712.05896,
    2017.'
- en: '[137] X. Zhu, J. Dai, L. Yuan, and Y. Wei, “Towards high performance video
    object detection,” in Proc. CVPR, pp. 7210–7218, 2018.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] X. Zhu, J. Dai, L. Yuan 和 Y. Wei, “朝着高性能视频目标检测迈进，” 见 Proc. CVPR, pp.
    7210–7218, 2018.'
- en: '[138] X. Zhu, J. Dai, X. Zhu, Y. Wei, and L. Yuan, “Towards high performance
    video object detection for mobiles,” arXiv preprint arXiv:1804.05830, 2018.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] X. Zhu, J. Dai, X. Zhu, Y. Wei 和 L. Yuan, “朝着高性能移动视频目标检测迈进，” arXiv 预印本
    arXiv:1804.05830, 2018.'
- en: '[139] R. Hang, Q. Liu, D. Hong, and P. Ghamisi, “Cascaded recurrent neural
    networks for hyperspectral image classification,” IEEE Trans. Geosci. Remote Sens.,
    vol. 57, no. 8, pp. 5384–5394, 2019.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] R. Hang, Q. Liu, D. Hong 和 P. Ghamisi, “用于高光谱图像分类的级联递归神经网络，” IEEE Trans.
    Geosci. Remote Sens., vol. 57, no. 8, pp. 5384–5394, 2019.'
- en: '[140] Y. Lu, C. Lu, and C.-K. Tang, “Online video object detection using association
    lstm,” in Proc. ICCV, pp. 2344–2352, 2017.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Y. Lu, C. Lu 和 C.-K. Tang, “使用关联 LSTM 的在线视频目标检测，” 见 Proc. ICCV, pp. 2344–2352,
    2017.'
- en: '[141] M. Liu, M. Zhu, M. White, Y. Li, and D. Kalenichenko, “Looking fast and
    slow: Memory-guided mobile video object detection,” arXiv preprint arXiv:1903.10172,
    2019.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] M. Liu, M. Zhu, M. White, Y. Li 和 D. Kalenichenko, “快速与缓慢：记忆引导的移动视频目标检测，”
    arXiv 预印本 arXiv:1903.10172, 2019.'
- en: '[142] M. Liu and M. Zhu, “Mobile video object detection with temporally-aware
    feature maps,” in Proc. CVPR, pp. 5686–5695, 2018.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] M. Liu 和 M. Zhu, “具有时间感知特征图的移动视频目标检测，” 见 Proc. CVPR, pp. 5686–5695, 2018.'
- en: '[143] Z. Jiang, P. Gao, C. Guo, Q. Zhang, S. Xiang, and C. Pan, “Video object
    detection with locally-weighted deformable neighbors,” in Proc. AAAI, vol. 33,
    pp. 8529–8536, 2019.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Z. Jiang, P. Gao, C. Guo, Q. Zhang, S. Xiang 和 C. Pan, “具有局部加权可变形邻居的视频目标检测，”
    见 Proc. AAAI, vol. 33, pp. 8529–8536, 2019.'
- en: '[144] S. Tripathi, Z. C. Lipton, S. Belongie, and T. Nguyen, “Context matters:
    Refining object detection in video with recurrent neural networks,” arXiv preprint
    arXiv:1607.04648, 2016.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] S. Tripathi, Z. C. Lipton, S. Belongie 和 T. Nguyen, “上下文很重要：用递归神经网络改进视频中的目标检测，”
    arXiv 预印本 arXiv:1607.04648, 2016.'
- en: '[145] F. Xiao and Y. Jae Lee, “Video object detection with an aligned spatial-temporal
    memory,” in Proc. ECCV, pp. 485–501, 2018.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] F. Xiao 和 Y. Jae Lee, “具有对齐时空记忆的视频目标检测，” 见 Proc. ECCV, pp. 485–501, 2018.'
- en: '[146] H. Luo, W. Xie, X. Wang, and W. Zeng, “Detect or track: Towards cost-effective
    video object detection/tracking,” in Proc. AAAI, vol. 33, pp. 8803–8810, 2019.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] H. Luo, W. Xie, X. Wang 和 W. Zeng, “检测还是跟踪：朝着具有成本效益的视频目标检测/跟踪方向发展，” 见
    Proc. AAAI, vol. 33, pp. 8803–8810, 2019.'
- en: '[147] N. Wojke, A. Bewley, and D. Paulus, “Simple online and realtime tracking
    with a deep association metric,” in Proc. ICIP, pp. 3645–3649, IEEE, 2017.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] N. Wojke, A. Bewley, 和 D. Paulus, “基于深度关联度量的简单在线实时跟踪,” 在 ICIP 会议上, 页
    3645–3649, IEEE, 2017。'
- en: '[148] N. M. Al-Shakarji, F. Bunyak, G. Seetharaman, and K. Palaniappan, “Multi-object
    tracking cascade with multi-step data association and occlusion handling,” in
    Proc. AVSS, pp. 1–6, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] N. M. Al-Shakarji, F. Bunyak, G. Seetharaman, 和 K. Palaniappan, “具有多步数据关联和遮挡处理的多目标跟踪级联,”
    在 AVSS 会议上, 页 1–6, 2018。'
- en: '[149] Y. Zhou, T. Rui, Y. Li, and X. Zuo, “A uav patrol system using panoramic
    stitching and object detection,” Comput. Electr. Eng., vol. 80, p. 106473, 2019.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Y. Zhou, T. Rui, Y. Li, 和 X. Zuo, “使用全景拼接和物体检测的无人机巡逻系统,” 计算机与电气工程, 卷
    80, 页 106473, 2019。'
- en: '[150] J. Wang, S. Simeonova, and M. Shahbazi, “Orientation- and scale-invariant
    multi-vehicle detection and tracking from unmanned aerial videos,” Remote Sens,
    vol. 11, no. 18, 2019.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] J. Wang, S. Simeonova, 和 M. Shahbazi, “从无人机视频中进行方向和尺度不变的多车辆检测与跟踪,” 遥感,
    卷 11, 第 18 期, 2019。'
- en: '[151] W. Li, J. Mu, and G. Liu, “Multiple object tracking with motion and appearance
    cues,” in Proc. ICCV Workshops, pp. 0–0, 2019.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] W. Li, J. Mu, 和 G. Liu, “结合运动和外观线索的多目标跟踪,” 在 ICCV 研讨会上, 页 0–0, 2019。'
- en: '[152] H. Zhang, G. Wang, Z. Lei, and J.-N. Hwang, “Eye in the sky: Drone-based
    object tracking and 3d localization,” in Proc. ACM MM, pp. 899–907, 2019.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] H. Zhang, G. Wang, Z. Lei, 和 J.-N. Hwang, “天上的眼睛: 基于无人机的物体跟踪与 3D 定位,”
    在 ACM MM 会议上, 页 899–907, 2019。'
- en: '[153] S. Pan, Z. Tong, Y. Zhao, Z. Zhao, F. Su, and B. Zhuang, “Multi-object
    tracking hierarchically in visual data taken from drones,” in Proc. ICCV Workshops,
    pp. 0–0, 2019.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] S. Pan, Z. Tong, Y. Zhao, Z. Zhao, F. Su, 和 B. Zhuang, “从无人机拍摄的视觉数据中分层多目标跟踪,”
    在 ICCV 研讨会上, 页 0–0, 2019。'
- en: '[154] T. Yang, D. Li, Y. Bai, F. Zhang, S. Li, M. Wang, Z. Zhang, and J. Li,
    “Multiple-object-tracking algorithm based on dense trajectory voting in aerial
    videos,” Remote Sens., vol. 11, no. 19, p. 2278, 2019.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] T. Yang, D. Li, Y. Bai, F. Zhang, S. Li, M. Wang, Z. Zhang, 和 J. Li,
    “基于密集轨迹投票的空中视频多目标跟踪算法,” 遥感, 卷 11, 第 19 期, 页 2278, 2019。'
- en: '[155] H. Ardo and M. Nilsson, “Multi target tracking from drones by learning
    from generalized graph differences,” in Proc. ICCV Workshops, pp. 0–0, 2019.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] H. Ardo 和 M. Nilsson, “通过学习广义图差异进行无人机多目标跟踪,” 在 ICCV 研讨会上, 页 0–0, 2019。'
- en: '[156] S. M. Marvasti-Zadeh, J. Khaghani, H. Ghanei-Yakhdan, S. Kasaei, and
    L. Cheng, “Comet: context-aware iou-guided network for small object tracking,”
    in Proc. ACCV, 2020.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] S. M. Marvasti-Zadeh, J. Khaghani, H. Ghanei-Yakhdan, S. Kasaei, 和 L.
    Cheng, “Comet: 基于上下文的 IOU 引导网络用于小物体跟踪,” 在 ACCV 会议上, 2020。'
- en: '[157] H. Yu, G. Li, W. Zhang, H. Yao, and Q. Huang, “Self-balance motion and
    appearance model for multi-object tracking in uav,” in Proc. ACM MM Asia, pp. 1–6,
    2019.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] H. Yu, G. Li, W. Zhang, H. Yao, 和 Q. Huang, “用于 UAV 中多目标跟踪的自平衡运动和外观模型,”
    在 ACM MM Asia 会议上, 页 1–6, 2019。'
- en: '[158] P. Bergmann, T. Meinhardt, and L. Leal-Taixe, “Tracking without bells
    and whistles,” in Proc. ICCV, pp. 941–951, 2019.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] P. Bergmann, T. Meinhardt, 和 L. Leal-Taixe, “无需花哨装置的跟踪,” 在 ICCV 会议上,
    页 941–951, 2019。'
- en: '[159] H. Yu, G. Li, L. Su, B. Zhong, H. Yao, and Q. Huang, “Conditional gan
    based individual and global motion fusion for multiple object tracking in uav
    videos,” Pattern Recognit. Lett., vol. 131, pp. 219–226, 2020.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] H. Yu, G. Li, L. Su, B. Zhong, H. Yao, 和 Q. Huang, “基于条件 GAN 的个体和全局运动融合用于
    UAV 视频中的多目标跟踪,” 模式识别快报, 卷 131, 页 219–226, 2020。'
- en: '[160] S. Kapania, D. Saini, S. Goyal, N. Thakur, R. Jain, and P. Nagrath, “Multi
    object tracking with uavs using deep sort and yolov3 retinanet detection framework,”
    in Proc. ACM. AIMS, pp. 1–6, 2020.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] S. Kapania, D. Saini, S. Goyal, N. Thakur, R. Jain, 和 P. Nagrath, “使用深度
    SORT 和 YOLOv3 RetinaNet 检测框架的无人机多目标跟踪,” 在 ACM AIMS 会议上, 页 1–6, 2020。'
- en: '[161] D. Stadler, L. W. Sommer, and J. Beyerer, “Pas tracker: position-, appearance-and
    size-aware multi-object tracking in drone videos,” in Proc. ECCV, pp. 604–620,
    Springer, 2020.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] D. Stadler, L. W. Sommer, 和 J. Beyerer, “PAS 跟踪器: 位置、外观和尺寸感知的无人机视频多目标跟踪,”
    在 ECCV 会议上, 页 604–620, Springer, 2020。'
- en: '[162] H. U. Dike and Y. Zhou, “A robust quadruplet and faster region-based
    cnn for uav video-based multiple object tracking in crowded environment,” Electronics,
    vol. 10, no. 7, p. 795, 2021.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] H. U. Dike 和 Y. Zhou, “一种强健的四重体和更快的基于区域的 CNN 用于拥挤环境中的 UAV 视频多目标跟踪,” 电子学,
    卷 10, 第 7 期, 页 795, 2021。'
- en: '[163] W. Huang, X. Zhou, M. Dong, and H. Xu, “Multiple objects tracking in
    the uav system based on hierarchical deep high-resolution network,” Multimed.
    Tools. Appl., pp. 1–19, 2021.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] W. Huang, X. Zhou, M. Dong, 和 H. Xu, “基于分层深度高分辨率网络的 UAV 系统中的多目标跟踪,” 多媒体工具与应用,
    页 1–19, 2021。'
- en: '[164] S.-H. Bae and K.-J. Yoon, “Robust online multi-object tracking based
    on tracklet confidence and online discriminative appearance learning,” in Proc.
    CVPR, pp. 1218–1225, 2014.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] S.-H. Bae 和 K.-J. Yoon，“基于轨迹置信度和在线判别外观学习的鲁棒在线多对象跟踪，”在 Proc. CVPR, pp.
    1218–1225, 2014。'
- en: '[165] Y. Xiang, A. Alahi, and S. Savarese, “Learning to track: Online multi-object
    tracking by decision making,” in Proc. ICCV, pp. 4705–4713, 2015.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Y. Xiang, A. Alahi, 和 S. Savarese，“学习跟踪：通过决策制定进行在线多对象跟踪，”在 Proc. ICCV,
    pp. 4705–4713, 2015。'
- en: '[166] H. Pirsiavash, D. Ramanan, and C. C. Fowlkes, “Globally-optimal greedy
    algorithms for tracking a variable number of objects,” in Proc. CVPR, pp. 1201–1208,
    IEEE, 2011.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] H. Pirsiavash, D. Ramanan, 和 C. C. Fowlkes，“用于跟踪可变数量对象的全局最优贪婪算法，”在 Proc.
    CVPR, pp. 1201–1208, IEEE, 2011。'
- en: '[167] C. Dicle, O. I. Camps, and M. Sznaier, “The way they move: Tracking multiple
    targets with similar appearance,” in Proc. ICCV, pp. 2304–2311, 2013.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] C. Dicle, O. I. Camps, 和 M. Sznaier，“他们移动的方式：跟踪具有相似外观的多个目标，”在 Proc. ICCV,
    pp. 2304–2311, 2013。'
- en: '[168] E. Bochinski, V. Eiselein, and T. Sikora, “High-speed tracking-by-detection
    without using image information,” in Proc. AVSS, pp. 1–6, IEEE, 2017.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] E. Bochinski, V. Eiselein, 和 T. Sikora，“无需使用图像信息的高速检测跟踪，”在 Proc. AVSS,
    pp. 1–6, IEEE, 2017。'
- en: '[169] A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online and
    realtime tracking,” in Proc. ICIP, pp. 3464–3468, IEEE, 2016.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] A. Bewley, Z. Ge, L. Ott, F. Ramos, 和 B. Upcroft，“简单的在线和实时跟踪，”在 Proc.
    ICIP, pp. 3464–3468, IEEE, 2016。'
- en: '[170] A. Milan, S. Roth, and K. Schindler, “Continuous energy minimization
    for multitarget tracking,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 1,
    pp. 58–72, 2013.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] A. Milan, S. Roth, 和 K. Schindler，“用于多目标跟踪的连续能量最小化，” IEEE Trans. Pattern
    Anal. Mach. Intell., vol. 36, no. 1, pp. 58–72, 2013。'
- en: '[171] S. Schulter, P. Vernaza, W. Choi, and M. Chandraker, “Deep network flow
    for multi-object tracking,” in Proc. CVPR, pp. 6951–6960, 2017.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. Schulter, P. Vernaza, W. Choi, 和 M. Chandraker，“用于多对象跟踪的深度网络流，”在 Proc.
    CVPR, pp. 6951–6960, 2017。'
- en: '[172] J. Son, M. Baek, M. Cho, and B. Han, “Multi-object tracking with quadruplet
    convolutional neural networks,” in Proc. CVPR, pp. 5620–5629, 2017.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] J. Son, M. Baek, M. Cho, 和 B. Han，“使用四重卷积神经网络的多对象跟踪，”在 Proc. CVPR, pp.
    5620–5629, 2017。'
- en: '[173] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Detect to track and track
    to detect,” in Proc. ICCV, pp. 3038–3046, 2017.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] C. Feichtenhofer, A. Pinz, 和 A. Zisserman，“检测跟踪与跟踪检测，”在 Proc. ICCV, pp.
    3038–3046, 2017。'
- en: '[174] A. Jadhav, P. Mukherjee, V. Kaushik, and B. Lall, “Aerial multi-object
    tracking by detection using deep association networks,” arXiv preprint arXiv:1909.01547,
    2019.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] A. Jadhav, P. Mukherjee, V. Kaushik, 和 B. Lall，“通过检测的空中多对象跟踪使用深度关联网络，”
    arXiv preprint arXiv:1909.01547, 2019。'
- en: '[175] S. Chopra, R. Hadsell, and Y. LeCun, “Learning a similarity metric discriminatively,
    with application to face verification,” in Proc. CVPR, vol. 1, pp. 539–546, IEEE,
    2005.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] S. Chopra, R. Hadsell, 和 Y. LeCun，“通过判别学习相似度度量，并应用于人脸验证，”在 Proc. CVPR,
    vol. 1, pp. 539–546, IEEE, 2005。'
- en: '[176] J. Lee, S. Kim, and B. C. Ko, “Online multiple object tracking using
    rule distillated siamese random forest,” IEEE Access, vol. 8, pp. 182828–182841,
    2020.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] J. Lee, S. Kim, 和 B. C. Ko，“使用规则提炼的孪生随机森林进行在线多对象跟踪，” IEEE Access, vol.
    8, pp. 182828–182841, 2020。'
- en: '[177] J. Jin, X. Li, X. Li, and S. Guan, “Online multi-object tracking with
    siamese network and optical flow,” in Proc. ICIVC, pp. 193–198, IEEE, 2020.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] J. Jin, X. Li, X. Li, 和 S. Guan，“使用孪生网络和光流的在线多对象跟踪，”在 Proc. ICIVC, pp.
    193–198, IEEE, 2020。'
- en: '[178] B. Shuai, A. G. Berneshawi, D. Modolo, and J. Tighe, “Multi-object tracking
    with siamese track-rcnn,” arXiv preprint arXiv:2004.07786, 2020.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] B. Shuai, A. G. Berneshawi, D. Modolo, 和 J. Tighe，“使用孪生Track-RCNN的多对象跟踪，”
    arXiv preprint arXiv:2004.07786, 2020。'
- en: '[179] S.-H. Bae and K.-J. Yoon, “Confidence-based data association and discriminative
    deep appearance learning for robust online multi-object tracking,” IEEE Trans.
    Pattern Anal. Mach. Intell., vol. 40, no. 3, pp. 595–610, 2017.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] S.-H. Bae 和 K.-J. Yoon，“基于置信度的数据关联和判别深度外观学习用于鲁棒在线多对象跟踪，” IEEE Trans.
    Pattern Anal. Mach. Intell., vol. 40, no. 3, pp. 595–610, 2017。'
- en: '[180] L. Leal-Taixé, C. Canton-Ferrer, and K. Schindler, “Learning by tracking:
    Siamese cnn for robust target association,” in Proc. CVPR Workshops, pp. 33–40,
    2016.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] L. Leal-Taixé, C. Canton-Ferrer, 和 K. Schindler，“通过跟踪学习：用于鲁棒目标关联的孪生CNN，”在
    Proc. CVPR Workshops, pp. 33–40, 2016。'
- en: '[181] B. Wang, L. Wang, B. Shuai, Z. Zuo, T. Liu, K. Luk Chan, and G. Wang,
    “Joint learning of convolutional neural networks and temporally constrained metrics
    for tracklet association,” in Proc. CVPR Workshops, pp. 1–8, 2016.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] B. Wang, L. Wang, B. Shuai, Z. Zuo, T. Liu, K. Luk Chan, 和 G. Wang，“卷积神经网络与时间约束度量的联合学习用于轨迹关联，”在
    Proc. CVPR Workshops, pp. 1–8, 2016。'
- en: '[182] A. Li, L. Luo, and S. Tang, “Real-time tracking of vehicles with siamese
    network and backward prediction,” in Proc. ICME, pp. 1–6, IEEE, 2020.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] A. Li，L. Luo 和 S. Tang，"使用 Siamese 网络和反向预测的实时车辆跟踪"，见《ICME 会议录》，第 1–6
    页，IEEE，2020 年。'
- en: '[183] X. Yan, X. Wu, I. A. Kakadiaris, and S. K. Shah, “To track or to detect?
    an ensemble framework for optimal selection,” in Proc. ECCV, pp. 594–607, Springer,
    2012.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] X. Yan，X. Wu，I. A. Kakadiaris 和 S. K. Shah，"跟踪还是检测？最优选择的集成框架"，见《ECCV
    会议录》，第 594–607 页，Springer，2012 年。'
- en: '[184] Q. Chu, W. Ouyang, H. Li, X. Wang, B. Liu, and N. Yu, “Online multi-object
    tracking using cnn-based single object tracker with spatial-temporal attention
    mechanism,” in Proc. ICCV, pp. 4836–4845, 2017.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Q. Chu，W. Ouyang，H. Li，X. Wang，B. Liu 和 N. Yu，"基于 CNN 的单目标跟踪器结合时空注意力机制的在线多目标跟踪"，见《ICCV
    会议录》，第 4836–4845 页，2017 年。'
- en: '[185] A. Sadeghian, A. Alahi, and S. Savarese, “Tracking the untrackable: Learning
    to track multiple cues with long-term dependencies,” in Proc. ICCV, pp. 300–311,
    2017.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] A. Sadeghian，A. Alahi 和 S. Savarese，"跟踪不可跟踪的对象：学习跟踪具有长期依赖的多种线索"，见《ICCV
    会议录》，第 300–311 页，2017 年。'
- en: '[186] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and P. H. Torr, “Staple:
    Complementary learners for real-time tracking,” in Proc. CVPR, pp. 1401–1409,
    2016.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] L. Bertinetto，J. Valmadre，S. Golodetz，O. Miksik 和 P. H. Torr，"Staple:
    实时跟踪的互补学习器"，见《CVPR 会议录》，第 1401–1409 页，2016 年。'
- en: '[187] H. Fan and H. Ling, “Parallel tracking and verifying: A framework for
    real-time and high accuracy visual tracking,” in Proc. ICCV, pp. 5486–5494, 2017.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] H. Fan 和 H. Ling，"并行跟踪与验证：实时和高精度视觉跟踪的框架"，见《ICCV 会议录》，第 5486–5494 页，2017
    年。'
- en: '[188] H. Fan and H. Ling, “Sanet: Structure-aware network for visual tracking,”
    in Proc. CVPR workshops, pp. 42–49, 2017.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] H. Fan 和 H. Ling，"Sanet: 结构感知网络用于视觉跟踪"，见《CVPR 工作坊》，第 42–49 页，2017 年。'
- en: '[189] P. Chu, H. Fan, C. C. Tan, and H. Ling, “Online multi-object tracking
    with instance-aware tracker and dynamic model refreshment,” in Proc. WACV, pp. 161–170,
    IEEE, 2019.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] P. Chu，H. Fan，C. C. Tan 和 H. Ling，"基于实例感知跟踪器和动态模型更新的在线多目标跟踪"，见《WACV 会议录》，第
    161–170 页，IEEE，2019 年。'
- en: '[190] W. Feng, Z. Hu, W. Wu, J. Yan, and W. Ouyang, “Multi-object tracking
    with multiple cues and switcher-aware classification,” arXiv preprint arXiv:1901.06129,
    2019.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] W. Feng，Z. Hu，W. Wu，J. Yan 和 W. Ouyang，"结合多种线索和切换器感知分类的多目标跟踪"，arXiv 预印本
    arXiv:1901.06129，2019 年。'
- en: '[191] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual tracking
    with siamese region proposal network,” in Proc. CVPR, pp. 8971–8980, 2018.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] B. Li，J. Yan，W. Wu，Z. Zhu 和 X. Hu，"基于 Siamese 区域提议网络的高性能视觉跟踪"，见《CVPR
    会议录》，第 8971–8980 页，2018 年。'
- en: '[192] J. Zhu, H. Yang, N. Liu, M. Kim, W. Zhang, and M.-H. Yang, “Online multi-object
    tracking with dual matching attention networks,” in Proc. ECCV, pp. 366–382, 2018.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] J. Zhu，H. Yang，N. Liu，M. Kim，W. Zhang 和 M.-H. Yang，"基于双重匹配注意力网络的在线多目标跟踪"，见《ECCV
    会议录》，第 366–382 页，2018 年。'
- en: '[193] A. Milan, S. H. Rezatofighi, A. Dick, I. Reid, and K. Schindler, “Online
    multi-target tracking using recurrent neural networks,” in Proc. AAAI, pp. 4225–4232,
    2017.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] A. Milan，S. H. Rezatofighi，A. Dick，I. Reid 和 K. Schindler，"使用递归神经网络的在线多目标跟踪"，见《AAAI
    会议录》，第 4225–4232 页，2017 年。'
- en: '[194] Q. Li, X. Zhao, and K. Huang, “Learning temporally correlated representations
    using lstms for visual tracking,” in Proc. ICIP, pp. 1614–1618, IEEE, 2016.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Q. Li，X. Zhao 和 K. Huang，"使用 LSTMs 学习时间相关的表示以进行视觉跟踪"，见《ICIP 会议录》，第 1614–1618
    页，IEEE，2016 年。'
- en: '[195] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese,
    “Social lstm: Human trajectory prediction in crowded spaces,” in Proc. CVPR, pp. 961–971,
    2016.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] A. Alahi，K. Goel，V. Ramanathan，A. Robicquet，L. Fei-Fei 和 S. Savarese，"Social
    LSTM: 拥挤空间中的人类轨迹预测"，见《CVPR 会议录》，第 961–971 页，2016 年。'
- en: '[196] Y. Liang and Y. Zhou, “Lstm multiple object tracker combining multiple
    cues,” in Proc. ICIP, pp. 2351–2355, IEEE, 2018.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Y. Liang 和 Y. Zhou，"Lstm 多目标跟踪器结合多种线索"，见《ICIP 会议录》，第 2351–2355 页，IEEE，2018
    年。'
- en: '[197] C. Kim, F. Li, and J. M. Rehg, “Multi-object tracking with neural gating
    using bilinear lstm,” in Proc. ECCV, pp. 200–215, 2018.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] C. Kim，F. Li 和 J. M. Rehg，"使用双线性 LSTM 的多目标跟踪与神经门控"，见《ECCV 会议录》，第 200–215
    页，2018 年。'
- en: '[198] H. Meng-Ru, L. Yen-Liang, and H. Winston, “Drone-based object counting
    by spatially regularized regional proposal networks,” in Proc. ICCV, IEEE, 2017.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] H. Meng-Ru，L. Yen-Liang 和 H. Winston，"基于无人机的物体计数，通过空间正则化区域提议网络"，见《ICCV
    会议录》，IEEE，2017 年。'
- en: '[199] D. Du, Y. Qi, H. Yu, Y. Yang, K. Duan, G. Li, W. Zhang, Q. Huang, and
    Q. Tian, “The unmanned aerial vehicle benchmark: Object detection and tracking,”
    in Proc. ECCV, pp. 370–386, 2018.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] D. Du，Y. Qi，H. Yu，Y. Yang，K. Duan，G. Li，W. Zhang，Q. Huang 和 Q. Tian，"无人机基准测试：物体检测与跟踪"，见《ECCV
    会议录》，第 370–386 页，2018 年。'
- en: '[200] X. Xu, X. Zhang, B. Yu, X. S. Hu, C. Rowen, J. Hu, and Y. Shi, “Dac-sdc
    low power object detection challenge for uav applications,” IEEE Trans. Pattern
    Anal. Mach. Intell., 2019.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] X. Xu, X. Zhang, B. Yu, X. S. Hu, C. Rowen, J. Hu, 和 Y. Shi, “DAC-SDC
    低功耗物体检测挑战用于无人机应用,” IEEE Trans. Pattern Anal. Mach. Intell., 2019.'
- en: '[201] P. Zhu, L. Wen, D. Du, X. Bian, Q. Hu, and H. Ling, “Vision meets drones:
    Past, present and future,” arXiv preprint arXiv:2001.06303, 2020.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] P. Zhu, L. Wen, D. Du, X. Bian, Q. Hu, 和 H. Ling, “视觉与无人机的结合：过去、现在与未来,”
    arXiv 预印本 arXiv:2001.06303, 2020.'
- en: '[202] P. Zhu, Y. Sun, L. Wen, Y. Feng, and Q. Hu, “Drone based rgbt vehicle
    detection and counting: A challenge,” arXiv preprint arXiv:2003.02437, 2020.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] P. Zhu, Y. Sun, L. Wen, Y. Feng, 和 Q. Hu, “基于无人机的 RGBT 车辆检测与计数：一个挑战,”
    arXiv 预印本 arXiv:2003.02437, 2020.'
- en: '[203] I. Bozcan and E. Kayacan, “Au-air: A multi-modal unmanned aerial vehicle
    dataset for low altitude traffic surveillance,” in Proc. ICRA, pp. 8504–8510,
    IEEE, 2020.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] I. Bozcan 和 E. Kayacan, “Au-air: 一个用于低高度交通监控的多模态无人机数据集,” 见 Proc. ICRA,
    页 8504–8510, IEEE, 2020.'
- en: '[204] E. Bondi, R. Jain, P. Aggrawal, S. Anand, R. Hannaford, A. Kapoor, J. Piavis,
    S. Shah, L. Joppa, B. Dilkina, et al., “Birdsai: A dataset for detection and tracking
    in aerial thermal infrared videos,” in Proc. WACV, pp. 1747–1756, 2020.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] E. Bondi, R. Jain, P. Aggrawal, S. Anand, R. Hannaford, A. Kapoor, J.
    Piavis, S. Shah, L. Joppa, B. Dilkina, 等, “Birdsai: 一个用于检测和跟踪航拍热红外视频的数据集,” 见 Proc.
    WACV, 页 1747–1756, 2020.'
- en: '[205] H. Zhang, M. Sun, Q. Li, L. Liu, M. Liu, and Y. Ji, “An empirical study
    of multi-scale object detection in high resolution uav images,” Neurocomputing,
    vol. 421, pp. 173–182, 2021.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] H. Zhang, M. Sun, Q. Li, L. Liu, M. Liu, 和 Y. Ji, “高分辨率无人机图像中多尺度物体检测的实证研究,”
    Neurocomputing, 第 421 卷, 页 173–182, 2021.'
- en: '[206] M. Barekatain, M. Martí, H. Shih, S. Murray, K. Nakayama, Y. Matsuo,
    and H. Prendinger, “Okutama-action: An aerial view video dataset for concurrent
    human action detection,” in Proc. CVPR Workshops, pp. 28–35, 2017.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] M. Barekatain, M. Martí, H. Shih, S. Murray, K. Nakayama, Y. Matsuo,
    和 H. Prendinger, “Okutama-action: 一个用于同时检测人类动作的航拍视频数据集,” 见 Proc. CVPR Workshops,
    页 28–35, 2017.'
- en: '[207] M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simulator for uav
    tracking,” in Proc. ECCV, pp. 445–461, Springer, 2016.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] M. Mueller, N. Smith, 和 B. Ghanem, “一个用于无人机跟踪的基准和模拟器,” 见 Proc. ECCV,
    页 445–461, Springer, 2016.'
- en: '[208] S. Li and D. Yeung, “Visual object tracking for unmanned aerial vehicles:
    A benchmark and new motion models,” in Proc. AAAI, 2017.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] S. Li 和 D. Yeung, “无人机的视觉物体跟踪：基准和新运动模型,” 见 Proc. AAAI, 2017.'
- en: '[209] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learning social
    etiquette: Human trajectory understanding in crowded scenes,” in Proc. ECCV, vol. 9912,
    pp. 549–565, Sep 2016.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] A. Robicquet, A. Sadeghian, A. Alahi, 和 S. Savarese, “学习社会礼仪：在拥挤场景中理解人类轨迹,”
    见 Proc. ECCV, 第 9912 卷, 页 549–565, 2016 年 9 月.'
- en: '[210] W. Zhang, C. Liu, F. Chang, and Y. Song, “Multi-scale and occlusion aware
    network for vehicle detection and segmentation on uav aerial images,” Remote Sens.,
    vol. 12, no. 11, p. 1760, 2020.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] W. Zhang, C. Liu, F. Chang, 和 Y. Song, “多尺度和遮挡感知网络用于无人机航拍图像中的车辆检测与分割,”
    Remote Sens., 第 12 卷，第 11 期, 页 1760, 2020.'
- en: '[211] H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,”
    in Proc. ECCV, pp. 734–750, 2018.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] H. Law 和 J. Deng, “Cornernet: 将物体检测为配对的关键点,” 见 Proc. ECCV, 页 734–750,
    2018.'
- en: '[212] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, “Single-shot refinement
    neural network for object detection,” in Proc. CVPR, pp. 4203–4212, 2018.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] S. Zhang, L. Wen, X. Bian, Z. Lei, 和 S. Z. Li, “用于物体检测的单次细化神经网络,” 见 Proc.
    CVPR, 页 4203–4212, 2018.'
- en: '[213] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Detnet: A backbone
    network for object detection,” arXiv preprint arXiv:1804.06215, 2018.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, 和 J. Sun, “Detnet: 一种用于物体检测的骨干网络,”
    arXiv 预印本 arXiv:1804.06215, 2018.'
- en: '[214] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Light-head r-cnn:
    In defense of two-stage object detector,” arXiv preprint arXiv:1711.07264, 2017.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, 和 J. Sun, “Light-head r-cnn:
    为两阶段物体检测器辩护,” arXiv 预印本 arXiv:1711.07264, 2017.'
- en: '[215] F. Yang, H. Fan, P. Chu, E. Blasch, and H. Ling, “Clustered object detection
    in aerial images,” in Proc. ICCV, pp. 8311–8320, 2019.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] F. Yang, H. Fan, P. Chu, E. Blasch, 和 H. Ling, “航拍图像中的聚类物体检测,” 见 Proc.
    ICCV, 页 8311–8320, 2019.'
- en: '[216] C. Li, T. Yang, S. Zhu, C. Chen, and S. Guan, “Density map guided object
    detection in aerial images,” in Proc. CVPR Workshops, pp. 190–191, 2020.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] C. Li, T. Yang, S. Zhu, C. Chen, 和 S. Guan, “密度图引导的航拍图像中的物体检测,” 见 Proc.
    CVPR Workshops, 页 190–191, 2020.'
- en: '[217] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, and Y. Chen, “Ron: Reverse connection
    with objectness prior networks for object detection,” in Proc. CVPR, pp. 5936–5944,
    2017.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, 和 Y. Chen, “Ron: 具有物体性先验网络的反向连接用于物体检测,”
    见 Proc. CVPR, 页 5936–5944, 2017.'
- en: '[218] P. Zhu, D. Du, L. Wen, X. Bian, H. Ling, Q. Hu, T. Peng, J. Zheng, X. Wang,
    Y. Zhang, et al., “Visdrone-vid2019: The vision meets drone object detection in
    video challenge results,” in Proc. ICCV Workshops, pp. 0–0, 2019.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] P. Zhu、D. Du、L. Wen、X. Bian、H. Ling、Q. Hu、T. Peng、J. Zheng、X. Wang、Y.
    Zhang 等，"Visdrone-vid2019：视觉遇见无人机视频中的目标检测挑战结果"，发表于 ICCV 研讨会，页码 0–0，2019年。'
- en: '[219] Q. Zhao, T. Sheng, Y. Wang, F. Ni, and L. Cai, “Cfenet: An accurate and
    efficient single-shot object detector for autonomous driving,” arXiv preprint
    arXiv:1806.09790, 2018.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Q. Zhao、T. Sheng、Y. Wang、F. Ni 和 L. Cai，"Cfenet：一种精确高效的单次检测器用于自动驾驶"，arXiv
    预印本 arXiv:1806.09790，2018年。'
- en: '[220] P. Zhu, L. Wen, D. Du, X. Bian, H. Ling, Q. Hu, H. Wu, Q. Nie, H. Cheng,
    C. Liu, et al., “Visdrone-vdt2018: The vision meets drone video detection and
    tracking challenge results,” in Proc. ECCV, pp. 0–0, 2018.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] P. Zhu、L. Wen、D. Du、X. Bian、H. Ling、Q. Hu、H. Wu、Q. Nie、H. Cheng、C. Liu
    等，"Visdrone-vdt2018：视觉遇见无人机视频检测与跟踪挑战结果"，发表于 ECCV 会议，页码 0–0，2018年。'
- en: '[221] P. Adarsh, P. Rathi, and M. Kumar, “Yolo v3-tiny: Object detection and
    recognition using one stage improved model,” in Proc. ICACCS, pp. 687–694, IEEE,
    2020.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] P. Adarsh、P. Rathi 和 M. Kumar，"Yolo v3-tiny：使用单阶段改进模型的目标检测与识别"，发表于 ICACCS
    会议，页码 687–694，IEEE，2020年。'
- en: '[222] A. Geiger, M. Lauer, C. Wojek, C. Stiller, and R. Urtasun, “3d traffic
    scene understanding from movable platforms,” IEEE Trans. Pattern Anal. Mach. Intell.,
    vol. 36, no. 5, pp. 1012–1025, 2013.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] A. Geiger、M. Lauer、C. Wojek、C. Stiller 和 R. Urtasun，"从可移动平台进行 3D 交通场景理解"，《IEEE
    模式分析与机器智能》期刊，第 36 卷，第 5 期，页码 1012–1025，2013年。'
- en: '[223] L. Wen, W. Li, J. Yan, Z. Lei, D. Yi, and S. Z. Li, “Multiple target
    tracking based on undirected hierarchical relation hypergraph,” in Proc. CVPR,
    pp. 1282–1289, 2014.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] L. Wen、W. Li、J. Yan、Z. Lei、D. Yi 和 S. Z. Li，"基于无向分层关系超图的多目标跟踪"，发表于 CVPR
    会议，页码 1282–1289，2014年。'
- en: '[224] N. M. Al-Shakarji, G. Seetharaman, F. Bunyak, and K. Palaniappan, “Robust
    multi-object tracking with semantic color correlation,” in Proc. AVSS, pp. 1–7,
    IEEE, 2017.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] N. M. Al-Shakarji、G. Seetharaman、F. Bunyak 和 K. Palaniappan，"基于语义色彩关联的鲁棒多物体跟踪"，发表于
    AVSS 会议，页码 1–7，IEEE，2017年。'
- en: '[225] A. Milan, S. H. Rezatofighi, A. Dick, I. Reid, and K. Schindler, “Online
    multi-target tracking using recurrent neural networks,” arXiv preprint arXiv:1604.03635,
    2016.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] A. Milan、S. H. Rezatofighi、A. Dick、I. Reid 和 K. Schindler，"基于递归神经网络的在线多目标跟踪"，arXiv
    预印本 arXiv:1604.03635，2016年。'
- en: '[226] J. H. Yoon, M.-H. Yang, J. Lim, and K.-J. Yoon, “Bayesian multi-object
    tracking using motion context from multiple objects,” in Proc. WACV, pp. 33–40,
    IEEE, 2015.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] J. H. Yoon、M.-H. Yang、J. Lim 和 K.-J. Yoon，"基于多物体运动上下文的贝叶斯多物体跟踪"，发表于 WACV
    会议，页码 33–40，IEEE，2015年。'
- en: '[227] J. Deng, Z. Zhong, H. Huang, Y. Lan, Y. Han, and Y. Zhang, “Lightweight
    semantic segmentation network for real-time weed mapping using unmanned aerial
    vehicles,” Appl, vol. 10, no. 20, p. 7132, 2020.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] J. Deng、Z. Zhong、H. Huang、Y. Lan、Y. Han 和 Y. Zhang，"用于实时杂草映射的轻量级语义分割网络，使用无人驾驶飞行器"，《应用》期刊，第
    10 卷，第 20 期，页码 7132，2020年。'
- en: '| ![[Uncaptioned image]](img/87a66d0fb93d97553797c90637f90347.png) | Xin Wu
    (S’19–M’20) received the M.Sc. degree in Computer Science and Technology from
    the College of Information Engineering, Qingdao University, Qingdao, China, in
    2014, the Ph.D. degree from the School of Information and Electronics, Beijing
    Institute of Technology (BIT), Beijing, China, in 2020. In 2018, she was a visiting
    student at the Photogrammetry and Image Analysis department of the Remote Sensing
    Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany.
    She is currently a Postdoctoral researcher in the School of Information and Electronics,
    BIT, Beijing, China. Her research interests include signal / image processing,
    fractional Fourier transform, deep learning and their applications in biometrics
    and geospatial object detection. She was a recipient of the Jose Bioucas Dias
    award for recognizing the outstanding paper at the Workshop on Hyperspectral Imaging
    and Signal Processing: Evolution in Remote Sensing (WHISPERS) in 2021. |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图像]](img/87a66d0fb93d97553797c90637f90347.png) | Xin Wu（S’19–M’20）于2014年获得中国青岛大学信息工程学院计算机科学与技术硕士学位，2020年获得北京理工大学（BIT）信息与电子学院博士学位。2018年，她在德国航天中心（DLR）遥感技术研究所（IMF）摄影测量与图像分析部门担任访问学生。她目前是北京理工大学信息与电子学院的博士后研究员。她的研究兴趣包括信号/图像处理、分数傅里叶变换、深度学习及其在生物特征识别和地理空间物体检测中的应用。她曾因其在
    2021 年高光谱成像与信号处理研讨会（WHISPERS）上发表的杰出论文获得 Jose Bioucas Dias 奖。 |'
- en: '| ![[Uncaptioned image]](img/4abd286bc978e90e89d1483831deffed.png) | Wei Li
    (S’11–M’13–SM’16) received the B.E.degree in telecommunications engineering from
    Xidian University, Xi’an, China, in 2007, the M.S. degree in information science
    and technology from Sun Yat-Sen University, Guangzhou, China, in 2009, and the
    Ph.D. degree in electrical and computer engineering from Mississippi State University,
    Starkville, MS, USA, in 2012. Subsequently, he spent 1 year as a Postdoctoral
    Researcher at the University of California, Davis, CA, USA. He is currently a
    professor with the School of Information and Electronics, Beijing Institute of
    Technology. His research interests include hyperspectral image analysis, pattern
    recognition, and data compression. He is currently serving as Associate Editor
    for the IEEE Transactions on Geoscience and Remote Sensing (TGRS), IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS),
    and IEEE Signal Processing Letters (SPL). He has published more than 150 peer-reviewed
    articles and 100 conference papers totally cited by 7500 times (Google Scholar).
    He received the JSTARS Best Reviewer in 2016 and TGRS Best Reviewer award in 2020
    from IEEE Geoscience and Remote Sensing Society (GRSS), and the Outstanding Paper
    award at IEEE International Workshop on Hyperspectral Image and Signal Processing:
    Evolution in Remote Sensing (Whispers), 2019. |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/4abd286bc978e90e89d1483831deffed.png) | 魏黎（S’11–M’13–SM’16）于2007年在中国西安的西电大学获得电信工程学士学位，2009年在中国广州的中山大学获得信息科学与技术硕士学位，并于2012年在美国密西西比州的密西西比州立大学获得电气与计算机工程博士学位。随后，他在美国加州大学戴维斯分校担任博士后研究员一年。他目前是北京理工大学信息与电子学院的教授。其研究兴趣包括高光谱图像分析、模式识别和数据压缩。他目前担任IEEE地球科学与遥感汇刊（TGRS）、IEEE应用地球观测与遥感期刊（JSTARS）以及IEEE信号处理快报（SPL）的副主编。他已发表150余篇同行评审文章和100篇会议论文，总被引用7500次（谷歌学术）。他于2016年获得JSTARS最佳审稿人奖，2020年获得IEEE地球科学与遥感学会（GRSS）的TGRS最佳审稿人奖，并在2019年IEEE国际高光谱图像与信号处理研讨会（Whispers）上获得杰出论文奖。
    |'
- en: '| ![[Uncaptioned image]](img/b128cd56439fcf5ce9eb6982dd0207f5.png) | Danfeng
    Hong (S’16–M’19–SM’21) received the M.Sc. degree (summa cum laude) in computer
    vision from the College of Information Engineering, Qingdao University, Qingdao,
    China, in 2015, the Dr. -Ing degree (summa cum laude) from the Signal Processing
    in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany,
    in 2019. Since 2015, he has been a Research Associate at the Remote Sensing Technology
    Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany. He
    is currently a Research Scientist and leads a Spectral Vision Working Group at
    IMF, DLR. He is also an Adjunct Scientist at GIPSA-lab, Grenoble INP, CNRS, Univ.
    Grenoble Alpes, Grenoble, France. His research interests include signal / image
    processing and analysis, hyperspectral remote sensing, machine / deep learning,
    artificial intelligence, and their applications in Earth Vision. Dr. Hong is an
    Editorial Board Member of Remote Sensing and a Topical Associate Editor of the
    IEEE Transactions on Geoscience and Remote Sensing (TGRS). He was a recipient
    of the Best Reviewer Award of the IEEE TGRS in 2021 and the Jose Bioucas Dias
    award for recognizing the outstanding paper at the Workshop on Hyperspectral Imaging
    and Signal Processing: Evolution in Remote Sensing (WHISPERS) in 2021\. He is
    also a Leading Guest Editor of the International Journal of Applied Earth Observation
    and Geoinformation, the IEEE Journal of Selected Topics in Applied Earth Observations,
    and Remote Sensing. |'
  id: totrans-692
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/b128cd56439fcf5ce9eb6982dd0207f5.png) | 洪丹凤（S’16–M’19–SM’21）于2015年在中国青岛的青岛大学信息工程学院获得计算机视觉硕士学位（优等），2019年在德国慕尼黑工业大学（TUM）的地球观测信号处理（SiPEO）获得工程博士学位（优等）。自2015年以来，他一直在德国航天中心（DLR）遥感技术研究所（IMF）担任研究助理。目前，他是IMF,
    DLR的研究科学家，并领导一个光谱视觉工作组。他还是法国格勒诺布尔INP-CNRS-格勒诺布尔阿尔卑斯大学GIPSA-lab的兼职科学家。他的研究兴趣包括信号/图像处理与分析、高光谱遥感、机器/深度学习、人工智能及其在地球视觉中的应用。洪博士是《遥感》编辑委员会成员和IEEE地球科学与遥感汇刊（TGRS）的专题副主编。他曾获得2021年IEEE
    TGRS最佳审稿人奖和2021年高光谱成像与信号处理研讨会（WHISPERS）杰出论文的Jose Bioucas Dias奖。他还是《应用地球观测与地理信息国际期刊》、IEEE应用地球观测期刊和《遥感》的主编。'
- en: '| ![[Uncaptioned image]](img/496bf313e9860058b44f716c14f12909.png) | Ran Tao
    (M’00–-SM’04) received the B.S. degree from the Electronic Engineering Institute
    of PLA, Hefei, China, in 1985, and the M.S. and Ph.D. degrees from the Harbin
    Institute of Technology, Harbin, China, in 1990 and 1993, respectively. In 2001,
    he was a Senior Visiting Scholar with the University of Michigan, Ann Arbor, MI,
    USA. He is currently a Professor with the School of Information and Electronics,
    Beijing Institute of Technology, Beijing, China. He has authored 3 books and more
    than 180 peer-reviewed journal articles. His current research interests include
    fractional signal and information processing with applications. Dr. Tao was the
    recipient of the National Science Foundation of China for Distinguished Young
    Scholars in 2006, and the First Prize of Science and Technology Progress in 2006
    and 2007, and the First Prize of Natural Science in 2013, both awarded by the
    Ministry of Education. He was a Distinguished Professor of the Changjiang Scholars
    Program in 2009\. He was a Chief Professor of the Program for Changjiang Scholars
    and Innovative Research Team in University from 2010 to 2012\. He has been a Chief
    Professor of the Creative Research Groups of the National Natural Science Foundation
    of China since 2014\. He has been awarded the Famous Teachers of Higher Education
    in Beijing in 2018\. He is currently the Vice Chair of the International Union
    of Radio Science China Council. |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/496bf313e9860058b44f716c14f12909.png) | Ran Tao（M’00–SM’04）于1985年获得中国合肥中国人民解放军电子工程学院的工学学士学位，1990年和1993年分别获得中国哈尔滨工业大学的硕士和博士学位。2001年，他曾在美国密歇根大学安娜堡分校担任高级访问学者。他目前是北京理工大学信息与电子学院的教授。他著有3本书籍和180多篇同行评审的期刊文章。他的当前研究兴趣包括分数阶信号与信息处理及其应用。陶博士于2006年获得中国国家自然科学基金杰出青年科学基金，2006年和2007年获得科技进步一等奖，2013年获得自然科学一等奖，这些奖项均由教育部颁发。他于2009年成为长江学者特聘教授，2010年至2012年担任长江学者与创新研究团队首席教授，自2014年以来担任国家自然科学基金创新研究群体首席教授。2018年，他获得了北京市高等教育名师奖。他目前是国际电波科学联合会中国委员会副主席。
    |'
- en: '| ![[Uncaptioned image]](img/6c8b049330bcf5816e7f86356a4615a1.png) | Qian Du
    (M’00–SM’05–F’18) received the Ph.D. degree in electrical engineering from the
    University of Maryland at Baltimore County, Baltimore, MD, USA, in 2000\. She
    is currently the Bobby Shackouls Professor with the Department of Electrical and
    Computer Engineering, Mississippi State University, MS, USA. Her research interests
    include hyperspectral remote sensing image analysis and applications, pattern
    classification, data compression, and neural networks. Dr. Du is a fellow of the
    SPIE-International Society for Optics and Photonics. She received the 2010 Best
    Reviewer Award from the IEEE Geoscience and Remote Sensing Society. She was the
    Co-Chair of the Data Fusion Technical Committee of the IEEE Geoscience and Remote
    Sensing Society from 2009 to 2013, and the Chair of the Remote Sensing and Mapping
    Technical Committee of the International Association for Pattern Recognition from
    2010 to 2014\. She has served as an Associate Editor of the IEEE JOURNAL OF SELECTED
    TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, the Journal of Applied
    Remote Sensing, and the IEEE SIGNAL PROCESSING LETTERS. She is the Editor-in-Chief
    of the IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
    SENSING from 2016 to 2020. |'
  id: totrans-694
  prefs: []
  type: TYPE_TB
  zh: '| ![[未标注的图片]](img/6c8b049330bcf5816e7f86356a4615a1.png) | Qian Du（M’00–SM’05–F’18）于2000年获得美国马里兰大学巴尔的摩县分校的电气工程博士学位。她目前是美国密西西比州立大学电气与计算机工程系的Bobby
    Shackouls教授。她的研究兴趣包括高光谱遥感图像分析及应用、模式分类、数据压缩和神经网络。杜博士是SPIE—国际光学与光子学学会的会士。她获得了2010年IEEE地球科学与遥感学会最佳审稿人奖。她曾在2009年至2013年担任IEEE地球科学与遥感学会数据融合技术委员会的联合主席，并在2010年至2014年担任国际模式识别协会遥感与制图技术委员会的主席。她还曾担任IEEE《应用地球观测与遥感精选主题期刊》的副编辑、《应用遥感期刊》和《IEEE信号处理快报》的副编辑。她在2016年至2020年担任IEEE《应用地球观测与遥感精选主题期刊》的主编。'
