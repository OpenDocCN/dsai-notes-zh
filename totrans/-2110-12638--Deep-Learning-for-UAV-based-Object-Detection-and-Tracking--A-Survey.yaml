- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:50:21'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2110.12638] Deep Learning for UAV-based Object Detection and Tracking: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.12638](https://ar5iv.labs.arxiv.org/html/2110.12638)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning for UAV-based Object Detection and Tracking: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Xin Wu,  Wei Li,  Danfeng Hong,  Ran Tao,  and Qian Du This work was supported,
    in part by the National Natural Science Foundation of China under Grant 61922013,
    62101045, and U1833203, and partly by the China Postdoctoral Science Foundation
    Funded Project No. 2021M690385.X. Wu, W. Li, and R. Tao are with the School of
    Information and Electronics, Beijing Institute of Technology, 100081 Beijing,
    China, and Beijing Key Laboratory of Fractional Signals and Systems, 100081 Beijing,
    China. (e-mail: 040251522wuxin@163.com; liwei089@ieee.org; rantao@bit.edu.cn)D.
    Hong is with the Key Laboratory of Digital Earth Science, Aerospace Information
    Research Institute, Chinese Academy of Sciences, 100094 Beijing, China. (e-mail:
    hongdf@aircas.ac.cn)Q. Du is with the Department of Electrical and Computer Engineering,
    Mississippi State University, Starkville, MS 39762, USA. (e-mail: du@ece.msstate.edu)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This is the pre-acceptance version, to read the final version please go to
    IEEE Geoscience and Remote Sensing Magazine on IEEE Xplore. Owing to effective
    and flexible data acquisition, unmanned aerial vehicle (UAV) has recently become
    a hotspot across the fields of computer vision (CV) and remote sensing (RS). Inspired
    by recent success of deep learning (DL), many advanced object detection and tracking
    approaches have been widely applied to various UAV-related tasks, such as environmental
    monitoring, precision agriculture, traffic management. This paper provides a comprehensive
    survey on the research progress and prospects of DL-based UAV object detection
    and tracking methods. More specifically, we first outline the challenges, statistics
    of existing methods, and provide solutions from the perspectives of DL-based models
    in three research topics: object detection from the image, object detection from
    the video, and object tracking from the video. Open datasets related to UAV-dominated
    object detection and tracking are exhausted, and four benchmark datasets are employed
    for performance evaluation using some state-of-the-art methods. Finally, prospects
    and considerations for the future work are discussed and summarized. It is expected
    that this survey can facilitate those researchers who come from remote sensing
    field with an overview of DL-based UAV object detection and tracking methods,
    along with some thoughts on their further developments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning, object detection, object tracking, remote sensing, unmanned aerial
    vehicle, video.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Object detection and tracking, as an important research topic in the field of
    remote sensing, has been widely investigated and applied to various civil and
    military tasks, such as environmental monitoring, geological hazard detection,
    precision agriculture, and urban planning. Traditional object acquisition methods
    derive mainly from satellites and manned aircraft. Normally, the two types of
    platforms run on a fixed rail or follow a predetermined path, or temporarily change
    the running route and hover according to a commissioned task, e.g., city planning
    and mapping, or performing object observation in a harsh and inhospitable environment,
    e.g., remote sensing in the cryosphere. However, the cost of satellites and manned
    aircraft, and the potential safety issues of pilots inevitably limit the application
    scope of such platforms.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d767be96a2948ded785a2ceab1a7a72f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A complex urban scenario for UAV object detection and tracking. For
    simplicity, only bounding boxes and class names for certain objects are drawn
    in the imagery.'
  prefs: []
  type: TYPE_NORMAL
- en: With the development of microelectronic software and hardware, navigation and
    communication technology renewal, and breakthroughs in materials and energy technology,
    unmanned aerial vehicle (UAV) platform already an international research hotspot
    in remote sensing has rapidly emerged. A UAV remote sensing system is a high-tech
    combination of science and technology integrated UAVs, remote sensing, global
    positioning system (GPS) positioning, and inertial measurement unit (IMU) attitude
    determination means. It is a dedicated remote sensing system with the goal of
    obtaining low-altitude high-resolution remote sensing images. Compared with traditional
    platforms, UAV makes up for information loss caused by weather, time, and other
    limitations. In addition, the high mobility of UAVs enables it to flexibly collect
    video data without geographic restriction. These data, either in contents or time,
    are extremely informative, and thus object detection and tracking have entered
    the era of mass UAV [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3)], which has
    played an increasingly important role in land cover mapping [[4](#bib.bib4), [5](#bib.bib5)],
    smart agriculture [[6](#bib.bib6), [7](#bib.bib7)], smart city [[8](#bib.bib8)],
    traffic monitoring [[9](#bib.bib9)], and disaster monitoring [[10](#bib.bib10)],
    among other topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'As one of the fundamental computer vision problems, object detection and tracking
    employ classic, i.e., statistically-based, methods [[11](#bib.bib11), [12](#bib.bib12)].
    However, today’s massive quantities of data impact the performance of these traditional
    methods, which poses a problem for feature dimension explosion, yielding higher
    storage space and time costs. Owing to the emergence of deep neural network (DL)
    techniques [[13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15)], hierarchical
    feature representations with enough sample data can be learned with deep and complex
    networks. Since 2015, the deep neural network has become a mainstream framework
    used for UAV object detection and tracking [[16](#bib.bib16), [17](#bib.bib17)].
    Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") shows an example of object detection and tracking
    in an urban areas with UAV remote sensing. Classic deep neural networks are divided
    into two major categories: two-stage and one-stage networks. Among them, the two-stage
    networks, such as RCNN [[18](#bib.bib18)], Fast RCNN [[19](#bib.bib19)], and Faster
    RCNN [[20](#bib.bib20)], first need to generate a region proposal (RP), and then
    classify and locate candidate regions. A series of work [[21](#bib.bib21), [22](#bib.bib22),
    [23](#bib.bib23)] has demonstrated that a two-stage network is suitable for applications
    with higher detection accuracy. A one-stage network, such as SSD [[24](#bib.bib24)]
    and YOLO [[16](#bib.bib16), [25](#bib.bib25), [26](#bib.bib26)], directly generates
    class probability and coordinate position, and is faster than a two-stage network.
    Similarly, there are some faster light weight networks, such as mobilenet SSD
    [[27](#bib.bib27)], YOLOv3 [[28](#bib.bib28)], ESPnet_v2 [[29](#bib.bib29)], etc.
    Therefore, one-stage and faster light weight networks are the final winners for
    UAV remote sensing practical applications with high-speed requirements. But for
    low-resolution data, it fails to produce good results without preprocessing images
    or modifying the classic neural network structure.'
  prefs: []
  type: TYPE_NORMAL
- en: This paper focuses on UAV with a maximum take-off weight of fewer than 30 kilograms,
    and provides a comprehensive review of deep learning (DL)-based UAV object detection
    and tracking methods by summarizing the latest published work, discussing the
    key issues and difficult problems, and delineating areas of future development.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2a24fe06ba77091fc480a8291ddf1ffb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Partial statistical analysis results of light and small UAVs currently
    in use.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of this paper is organized as follows. Section [II](#S2 "II Related
    Surveys and Brief Statistics ‣ Deep Learning for UAV-based Object Detection and
    Tracking: A Survey") briefly summarizes the statistics of UAV aircraft and related
    publications. Section [VI](#S6 "VI UAV-based Benchmark Dataset ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey") describes the existing
    UAV-based remote sensing datasets. Section [III](#S3 "III Object Detection from
    UAV-borne Images ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey")-[V](#S5 "V Multiple Object Tracking from UAV-borne Video ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey") reviews the existing DL-based
    work closely related to UAV-based object detection and tracking for the three
    sub-branches. Section [VIII](#S8 "VIII Discussion and Conclusion ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey") discusses conclusions.'
  prefs: []
  type: TYPE_NORMAL
- en: II Related Surveys and Brief Statistics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A UAV Aircraft Statistics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Fig. [2](#S1.F2 "Figure 2 ‣ I Introduction ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") shows the classification of UAVs in present
    use through statistical analysis. From the perspective of power supply, battery
    power is used more often than fuel power; for the aerodynamic shape, multi-rotor
    is more common than fixed-wing; for the weight of the aircraft, the majority is
    under 30 kilograms, which is considered small light UAV; and the flight time for
    most UAVs is less than 1 hour. The quantitative analysis results show that small
    light UAVs have become the main type used for study and application, and have
    more market weight. In addition, the “Small light UAV remote sensing development
    report” published in 2016 [[30](#bib.bib30)] shows that China has more than 3,000
    professional small light UAVs for remote sensing applications. This type of UAV
    exhibits the following five main characteristics.'
  prefs: []
  type: TYPE_NORMAL
- en: 1) Long flight time. As new energy technology, energy management technology,
    and lightweight composite material research technology have developed, UAV flight
    time has been continuously extended.
  prefs: []
  type: TYPE_NORMAL
- en: 2) Low comprehensive cost and high technical content. On the one hand, the use
    of low-cost and lightweight materials reduces the production cost of UAV and remote
    sensors. On the other hand, the increase of mass users promotes the mass production
    of components and structural parts, further reducing the production cost of UAV
    and remote sensors.
  prefs: []
  type: TYPE_NORMAL
- en: 3) Small, light-weight, diversified remote sensing cameras. All remote sensing
    loads on small light UAVs are developed to below 30 kilograms, and optical and
    infrared loads are even reduced to less than a half kilogram. In addition, multi-angle
    photography, tilt photography, sensor integration, hyperspectral imaging interference
    [[31](#bib.bib31)], and other technologies have been used in UAV remote sensing.
    Commercial high-end cameras have been widely used for professional aerial missions,
    and popular cameras are used for mass entertainment and general applications.
  prefs: []
  type: TYPE_NORMAL
- en: 4) Real-time data transmission. Advances in wireless communication and information
    compression technology have powerfully impelled image resolution with a higher
    data rate and longer transmission distance. Almost no-delay data link transmission
    makes real-time observation possible.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Challenges
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Object detection and tracking tasks in the UAV remote sensing video face many
    challenges, such as image degradation, uneven object intensity, small object size,
    and real-time problems like perspective specificity, background complexity, scale,
    and direction diversity problems in satellite and manned aircraft objects.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Image degradation problem. The load that a mini-UAV platform carries is strictly
    limited in terms of weight, volume, and power. Rapid movement changes in the external
    environment (such as light, cloud, fog, rain, etc.) cause aerial images to be
    fuzzy and noisy, which inevitably leads to image degradation [[32](#bib.bib32)].
    In addition, high-speed flight or camera rotation also increases the complexity
    of object detection. Thus, it is necessary to carry out image pre-processing,
    such as noise reduction, camera distortion correction, etc., to ensure the effectiveness
    of the object detection model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uneven object intensity problem. The image acquisition equipment of a UAV typically
    uses a large aperture, fixed focal, and wide-angle lens. In addition, flexible
    camera movement results in an uneven density of captured objects. Some of them
    are densely arranged and overlap many times, so that it is easy to repeat detection.
    Some are sparse and unevenly distributed, so that it is prone to missed detection.
    In addition, most objects occupy a small number of pixels, which makes it difficult
    to separate them from their surroundings.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Object size problem. UAV remote sensing images can be acquired at different
    altitudes, yielding photographs containing any size of ground objects. This challenges
    the classical DL-based method. In addition, ground objects in UAV remote sensing
    are primarily shown as images with an area smaller than $32\times 32$ pixels.
    MS COCO dataset[[33](#bib.bib33)] defines small objects due to their less distinct
    features, yielding more false and missed detection targets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time problem. Object detection or tracking in a video obtained by a drone
    needs to quickly and accurately locate moving ground objects, so real-time processing
    performance is highly essential.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: II-C Contribution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Up to now, reviews concerning object detection and tracking from airborne and
    spaceborne datasets can be found [[34](#bib.bib34), [35](#bib.bib35)]. For UAV
    data, several representative surveys have been published in the literature, which
    include surveys on the UAV image processing and application [[36](#bib.bib36),
    [37](#bib.bib37)], the UAV system[[38](#bib.bib38)]. However, less attention has
    been paid to the advance of object and tracking techniques both in image and video
    acquired by UAV. Although reviews in [[39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43), [44](#bib.bib44)] present some DL-based static
    object detection for UAV image and the one in [[45](#bib.bib45)] presents traditional
    object tracking for UAV video, there still lacks a complete survey for object
    and tracking and the most recent advances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, it is imperative to provide a comprehensive survey of DL-based object
    detection and tracking for UAV data, focusing on static object detection (SOD),
    video object detection (VID), and multiple object tracking (MOT). In the following
    discussion, we limit this review to DL-based methods based on corresponding publications.
    We hope that this survey will provide readers and practitioners with instructive
    information. Fig. [3](#S2.F3 "Figure 3 ‣ II-C Contribution ‣ II Related Surveys
    and Brief Statistics ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows the typical DL-based learning mode for these three research topics.
    For the choice of the DL method, SOD object detection focuses on detection head
    design to assign positive and negative samples, such as RPN+ ROI Pooling in Faster
    RCNN, detection outputs is classification and bounding box. VID and MOT are about
    UAV video data and the difference between them is how to use temporal information.
    The former focuses on modifying the missed detection results of the current frame
    by using temporal context in adjacent frames, while the latter focuses on predicting
    the trajectory in the next frame to obtain the moving state of objects.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/18b2a6eb332cf4efd7ed56c6a7fcf2af.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: An illustration of three UAV topics based on deep learning methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2d31f13fa54c5d49017c6f7b0ccd227.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: The development of typical methods for UAV static object detection.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8ed23fe354195dd7419338724134efd8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Deep learning-based scale diversity and direction diversity strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: III Object Detection from UAV-borne Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Although deep learning-based object detection methods for UAV remote sensing
    images are mainly borrowed from traditional digital images in the computer vision
    community, the limitation of small UAV platform and imaging acquisition condition
    inevitably causes problems of particularity perspective, complex background, scale
    and direction diversity, and issues related to small sizes. In the following,
    some solutions based on DL methods have been summarized according to recent publications.
    Fig. [6](#S3.F6 "Figure 6 ‣ III-F Object Detection on Others ‣ III Object Detection
    from UAV-borne Images ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows the development of typical methods for SOD. Among them, some
    methods specially designed for UAV data are listed in Table. [I](#S3.T1 "TABLE
    I ‣ III-B Object Detection on Scale Diversity ‣ III Object Detection from UAV-borne
    Images ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey").
    Other methods that can solve the above problem, but not specifically for UAV data,
    are briefly introduced in the text. The remainder of this section introduces DL-based
    SOD methods to solve five representative problems, including data processing,
    scale diversity, small objects, direction diversity and detection speed.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A Data Processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Two types of data processing are typically applied preprocessing before data
    acquisition and postprocessing after data acquisition.
  prefs: []
  type: TYPE_NORMAL
- en: The latter is more commonly used in DL-based techniques. Most of the existing
    UAV-based remote sensing works present an experimental dataset and appropriate
    data processing techniques [[46](#bib.bib46), [47](#bib.bib47), [48](#bib.bib48)],
    and all of them carry out image postprocessing procedure after image acquisition,
    such as increasing the number of training samples, enlarging the diversity of
    sample size and direction, and expanding the illumination change of samples. However,
    their effectiveness is variable.
  prefs: []
  type: TYPE_NORMAL
- en: Due to the limitation of UAV flight altitude and load, there is inevitably ground
    object overlapping, coverage, and displacement. Xia et al. [[49](#bib.bib49)]
    took optical cameras as an example, focusing on various difficulties and problems
    in the process of UAV remote sensing data acquisition, and systematically discussed
    the key techniques of data processing.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Object Detection on Scale Diversity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'UAV remote sensing images can be acquired at different altitudes and ground
    objects can be any size, even for intraclass. Therefore, solutions to scale diversity
    are cross-referenced in this review. There are two main approaches to solving
    this problem through deep learning, as illustrated in Fig. [5](#S2.F5 "Figure
    5 ‣ II-C Contribution ‣ II Related Surveys and Brief Statistics ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey")(a). The most commonly
    used is the multi-scale feature map [[23](#bib.bib23)], which is the output of
    multiple filters on multiple feature maps (MFM) or multiple filters on a single
    feature map (SFM) [[50](#bib.bib50), [51](#bib.bib51), [24](#bib.bib24), [52](#bib.bib52),
    [22](#bib.bib22), [53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55), [56](#bib.bib56),
    [57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)]. The other is a dilated/deformable
    convolution kernel [[60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63)].
    It points out that systematic expansion supports the exponential expansion of
    the receptive field without loss of resolution or coverage. Chen et al. [[60](#bib.bib60)]
    introduced an extended convolution filter to obtain the ResNeXt-d combination
    structure on the basis of ResNeXt[[64](#bib.bib64)] architecture, which can expand
    the receptive field.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: DL-based Static Object Detection Approaches for UAV exclusive'
  prefs: []
  type: TYPE_NORMAL
- en: '| Static Object Detection |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reference | Challenge | Dataset Used | Journal/Conf. | Year | Code.link |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RRNet[[53](#bib.bib53)] | Small objects, scale variation | VisDrone | ICCV
    Workshops | 2019 | [https://github.com/ouc-ocean-group/RRNet](https://github.com/ouc-ocean-group/RRNet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| SlimYOLOv3[[65](#bib.bib65)] | Real-time | Visdrone | ICCV | 2019 | [https://github.com/PengyiZhang/SlimYOLOv3.](https://github.com/PengyiZhang/SlimYOLOv3.)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al[[62](#bib.bib62)] | Small objects | VisDrone | ICCV Workshops
    | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| FS-SSD[[66](#bib.bib66)] | Small objects | Stanford Drone | IEEE TCSVT |
    2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SAMFR[[67](#bib.bib67)] | Scale variation | Visdrone | ICCV Workshop | 2019
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| ClusDet[[57](#bib.bib57)] | Scale variation | VisDrone, UAVDT | ICCV | 2019
    | [https://github.com/fyangneil](https://github.com/fyangneil) |'
  prefs: []
  type: TYPE_TB
- en: '| CenterNet[[58](#bib.bib58)] | Scale variation | VisDrone | - | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al[[59](#bib.bib59)] | Scale variation | Stanford Drone | IEEE Access
    | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al[[68](#bib.bib68)] | Real-time | CARPK | DDCLS | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| NDFT[[69](#bib.bib69)] | UAV-specific nuisances | VisDrone, UAVDT | ICCV
    | 2019 | [https://github.com/VITA-Group/UAV-NDFT](https://github.com/VITA-Group/UAV-NDFT)
    |'
  prefs: []
  type: TYPE_TB
- en: '| MSOA-Net[[56](#bib.bib56)] | Scale variation | UVSD | Remote Sens. | 2020
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| GDF-Net[[61](#bib.bib61)] | Scale variation | VisDrone, UAVDT | Remote Sens.
    | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| HRDNet[[70](#bib.bib70)] | Scale variation | VisDrone | CVPR | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| D-A-FS SSD[[63](#bib.bib63)][[63](#bib.bib63)] | Scale variation | VisDrone
    | ICIT | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| UAV-YOLO[[16](#bib.bib16)] | Small scale | UAV123, Own | Sensors | 2020 |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| SyNet[[71](#bib.bib71)] | Class imbalance | VisDrone | ICPR | 2020 | [https://github.com/mertalbaba/SyNet](https://github.com/mertalbaba/SyNet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| ComNet[[72](#bib.bib72)] | Blurred edges, low contrast | Own | IEEE TGRS
    | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MPFPN[[73](#bib.bib73)] | Small objects | VisDrone | IEEE Access | 2020 |
    - |'
  prefs: []
  type: TYPE_TB
- en: '| D2Det[[74](#bib.bib74)], | Localization, classification | UAVDT | CVPR |
    2020 | [https://github.com/JialeCao001/D2Det.](https://github.com/JialeCao001/D2Det.)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DAGN[[75](#bib.bib75)] | Small objects | VEDAI | IEEE GRSL | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| GANet[[76](#bib.bib76)] | Small objects | UAVDT, CARPK, PUCPR+ | MM | 2020
    | [https://isrc.iscas.ac.cn/gitlab/research/ganet](https://isrc.iscas.ac.cn/gitlab/research/ganet)
    |'
  prefs: []
  type: TYPE_TB
- en: '| DAN[[77](#bib.bib77)] | Dense distribution, small object | Visdrone-det |
    NCC | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al[[78](#bib.bib78)] | Real-time | Stanford drone | Neurocomputing
    | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DNOD Eifficientdet[[79](#bib.bib79)] | Dense objects,small objects | Visdrone-DET,
    UAVDT | Neurocomputing | 2021 | - |'
  prefs: []
  type: TYPE_TB
- en: '| ECascade-RCNN[[54](#bib.bib54)] | Scale variation | VisDrone | ICARA | 2021
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Cas_RCNN+FPN[[80](#bib.bib80)] | Cost | Visdrone | Transp. Res. Rec. | 2021
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| DSYolov3[[55](#bib.bib55)] | Scale variation | VisDrone, UAVDT | J. Vis.
    Commun. Image Represent. | 2021 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DSHNet[[81](#bib.bib81)] | Long-tail distribution | VisDrone, UAVDT | WACV
    | 2021 | [https://github.com/we1pingyu/DSHNet](https://github.com/we1pingyu/DSHNet)
    |'
  prefs: []
  type: TYPE_TB
- en: III-C Object Detection on Small Objects
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The UAV flying altitude inevitably causes most objects to be shown in scale
    diversity, small object size and dense arrangement, resulting in less feature
    information that can be extracted. Many work deal with the small object detection
    problem through the same network designing for scale diversity, including RRNet
    [[53](#bib.bib53)], HRDNet[[70](#bib.bib70)], Cascade network [[62](#bib.bib62)],
    UAV-YOLO [[16](#bib.bib16)], MPFPN [[73](#bib.bib73)], depthwise-separable attention-guided
    network (DAGN) [[75](#bib.bib75)], GANet [[76](#bib.bib76)], and FS-SSD [[66](#bib.bib66)],
    ResNeXt-d [[60](#bib.bib60)], et al. In these methods, accurate feature information
    learned by small objects is highly important. In addition, some new networks are
    based on YOLOv4 or Eifficientdet-D7 networks, e.g., DNOD [[79](#bib.bib79)], which
    are developed to improve the detection speed.
  prefs: []
  type: TYPE_NORMAL
- en: To further improve the distinguish ability of small objects, Li et al. [[82](#bib.bib82)]
    proposed a perceptual GAN to generate a super-resolved representation of small
    objects. This method uses the structural correlativity of large and small objects
    to enhance the representation of small objects and give them a similar expression
    to large objects. Hu et al. [[83](#bib.bib83)] found that the structure of small
    objects after pooling was typically distorted, and proposed a new context-aware
    region of interest (ROI) pooling method. Chen et al. [[60](#bib.bib60)] proposed
    an ResNeXt-d combination structure to enhance the perception of small size objects.
    There are other methods, including changing the anchor information, or cropping
    multiple subset tiles from the original high-resolution images, to improve the
    detection performance of small and dense object. Jadhav et al. [[77](#bib.bib77)]
    modified the anchor scale and Tang et al. [[84](#bib.bib84)] designed a coarse
    anchor-free detector (CPEN) to address dense small object detection. In [[85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87)], the authors proposed effective solutions
    to small object detection from high-resolution images by cropping multiple subset
    tiles from the original high-resolution images, and learning them through use
    of a CNN network without degrading the resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Alternately, the flexible movement of the camera results in an uneven density
    of captured objects. Tightly packed objects in an image, especially smaller size
    ground objects, inevitably overlap. Mekhalfi et al. [[88](#bib.bib88)] introduced
    Capsnets to model the relationship between objects.
  prefs: []
  type: TYPE_NORMAL
- en: III-D Object Detection on Direction Diversity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Objection direction from an optical remote sensing image is related to its
    actual parking location. The classic CNNs, which benefit from using a rectangular
    convolution kernel, are sensitive to object direction. Fig. [5](#S2.F5 "Figure
    5 ‣ II-C Contribution ‣ II Related Surveys and Brief Statistics ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey")(b) shows four commonly
    used solutions based on deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: The simplest and most common solution is data augmentation, which can make CNNs
    rotation-invariant though rotation transformation of different angles to extend
    the training set [[89](#bib.bib89), [90](#bib.bib90), [22](#bib.bib22)]. Cheng
    et al. [[89](#bib.bib89)] added regularization constraints on the basis of existing
    CNN architecture to build a rotation-invariant CNN (RICNN). With further researches,
    Fisher discriminative CNN related rotation-invariant network, called RIFD-CNN,
    have been proposed to further boost object detection performance [[91](#bib.bib91),
    [92](#bib.bib92)]. Laptev et al. [[90](#bib.bib90)] added a rotation-invariant
    pool operator to the penultimate layer of output. The shortcoming of data augmentation
    is the increased cost of network training and the risk of over fitting.
  prefs: []
  type: TYPE_NORMAL
- en: Some work directly used additional network modules such as oriented proposal
    boxes to achieve object detection [[93](#bib.bib93), [94](#bib.bib94)], or upgraded
    the general convolutional filter to a directional channel filter to achieve rotation
    invariant of texture [[95](#bib.bib95)]. The region proposal network (RPN) [[96](#bib.bib96),
    [97](#bib.bib97), [98](#bib.bib98)] added to the anchor boxed with multiple angles
    in order to cover the oriented object. Additionally, inspired by text detection
    methods [[99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101), [102](#bib.bib102)],
    Xia et al. [[103](#bib.bib103)] designed a direction insensitive FR-O network
    by adding a direction box detection sub-network to Faster RCNN. Li et al. [[104](#bib.bib104)]
    proposed RADet to acquire a rotation bounding box with a shape mask. However,
    the drawback of additional network modules is that the transform parameter estimation
    is non-adaptive.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches like, Oriented Response Networks (ORNs) [[105](#bib.bib105)], Polar
    Transformer Network (PTN) [[106](#bib.bib106)], and Equivariant Transformer Networks
    (ETNs) [[107](#bib.bib107)], which were proposed for object detection from natural
    scenes, also provided a qualitative or qualitative analysis of rotation invariant
    features. On the basis of these techniques, Zhou et al. [[108](#bib.bib108)] developed
    a rotated feature network (RFN) using encoder–encoder architecture for object
    detection in remote-sensing images. It is worth mentioning that some rotation-invariant
    methods based on theoretical analysis can cover the intrinsic properties of rotations
    [[109](#bib.bib109), [110](#bib.bib110)] to extract real rotation- invariant features.
    Up to now, these methods have have not been widely used in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: III-E Object Detection on Detection Speed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Limited by flight stability and the load capacity of a micro-mini UAV, the altitude
    of an airborne remote sensing sensor needs to be adjusted quickly and accurately
    in real time so that ground objects are always in the monitoring field of vision.
    Meanwhile, rapid processing and analysis of high-quality remote sensing images
    obtained by a UAV system in real time is the key for miniature UAV remote sensing.
  prefs: []
  type: TYPE_NORMAL
- en: When considering all the deep learning methods, the most direct way is to choose
    the right platform, including an ARM, mobile, and embedded platform, or to trim
    up the classic network architecture to minimize unnecessary channels in the convolutional
    layer. In [[85](#bib.bib85), [86](#bib.bib86), [68](#bib.bib68)], the authors
    adopted a YOLO and even a tiny-YOLO network to achieve real-time object detection.
    Zhang et al. [[65](#bib.bib65)] trimmed the update YOLOv3, and proposed slimYOLOv3,
    which balanced the number of parameters, memory usage, and inference time to achieve
    real-time object detection. [[87](#bib.bib87), [78](#bib.bib78)] modified the
    feature resolution of the lightweight Pelee network [[111](#bib.bib111)] to meet
    real-time needs. Due to the efficiency and power of YOLOv4, many object detection
    models [[112](#bib.bib112), [44](#bib.bib44)] are based on this network. Ammar
    et al. [[44](#bib.bib44)] used YOLOv3 and newly released YOLOv4 to detect vehicles
    with inference processing speed from 12 fps for $608\times 608$ up to 23 fps for
    $320\times 320$. Furthermore, Wang et al. [[113](#bib.bib113)] designed a Strip
    Bottleneck with YOLO network (SPB-YOLO) based on YOLOv5 for engineering application.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, real-time object detection from images is also a necessary condition
    for detection-based object detection and object tracking from videos, which will
    be discussed in Section [IV](#S4 "IV Object Detection from UAV-borne Video ‣ Deep
    Learning for UAV-based Object Detection and Tracking: A Survey") and [V](#S5 "V
    Multiple Object Tracking from UAV-borne Video ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: III-F Object Detection on Others
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides the aforementioned main challenges, other problems in object detection
    in UAV images are addressed, such as Nuisance Disentangled Feature Transform (NDFT)
    [[69](#bib.bib69)] for a large number of fine-grained domains, D2Det [[74](#bib.bib74)]
    for precise localization and accurate classification, combinational neural network
    (ComNet) [[72](#bib.bib72)] for blurred edges and low contrast, an ensemble network
    (SyNet) [[71](#bib.bib71)] for class imbalance problem and the scaling problem,
    and Dual Sampler and Head detection Network (DSHNet) for long-tail distribution.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/64a45bb20ef19ae876c31a194b77a4fd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The development of typical methods for UAV object detection from
    video.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: DL-based Video Object Detection Approaches for UAV exclusive'
  prefs: []
  type: TYPE_NORMAL
- en: '| Video Object Detection |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reference | Challenge | Dataset Used | Journal/Conf. | Year | Code.link |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| STCA[[114](#bib.bib114)] | Defocus, motion blur, occlusion | VisDrone-VID
    | ICCV Workshop | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SCNN[[115](#bib.bib115)] | Temporal and contextual correlation | DAC | AAAI
    | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Nousi et al.[[116](#bib.bib116)] | Real-time | own-recorded | RCAR | 2019
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Abughalieh et al[[117](#bib.bib117)] | Varying resolutions | Own | Multimed.
    Tools. Appl. | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al[[118](#bib.bib118)] | Appearance deterioration, occlusion, motion
    blur | VisDrone-VID | MIPR | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| MOR-UAVNet[[119](#bib.bib119)] | Moving object | MOR-UAV | MM | 2020 | [https://visionintelligence.github.io/Datasets.html](https://visionintelligence.github.io/Datasets.html)
    |'
  prefs: []
  type: TYPE_TB
- en: '| TDFA[[120](#bib.bib120)] | Small-scale | Okutama, VisDrone-VID | Multidim
    Syst Sign P | 2021 | - |'
  prefs: []
  type: TYPE_TB
- en: '| STDnet-ST[[121](#bib.bib121)] | Small object | USC-GRAD-STDdb,UAVDT,VisDrone-VID
    | PR | 2021 | - |'
  prefs: []
  type: TYPE_TB
- en: IV Object Detection from UAV-borne Video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Video object detection (VID) becomes a hot topic after ImageNet VID challenge
    2015\. It is widely used on UAV data until 2017, and also brings some new challenges,
    e.g., camera change and motion blur in a drone platforms. In the following, some
    solutions based on DL methods are summarized according to recent publications.
    Fig. [6](#S3.F6 "Figure 6 ‣ III-F Object Detection on Others ‣ III Object Detection
    from UAV-borne Images ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows the development of typical methods for VID. Among them, methods
    specially designed for UAV data are listed in Table [II](#S3.T2 "TABLE II ‣ III-F
    Object Detection on Others ‣ III Object Detection from UAV-borne Images ‣ Deep
    Learning for UAV-based Object Detection and Tracking: A Survey"). Other methods
    that can solve the above problem, but not specifically for UAV data, are described
    in the text.'
  prefs: []
  type: TYPE_NORMAL
- en: The main steps of VID are summarized below.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Single frame image object detection: Static object detection or object detection
    from images. Each frame in the video is an independent image, and the object detection
    from the image can be achieved by using a method in Section [III](#S3 "III Object
    Detection from UAV-borne Images ‣ Deep Learning for UAV-based Object Detection
    and Tracking: A Survey").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Detection results amendment: The above missed detection results are compensated
    by temporal information and context information of the video.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The early mainstream method for VID is a multi-stage pipeline method, such as
    tubeless with convolutional neural network (TCNN) [[122](#bib.bib122), [123](#bib.bib123)]
    and sequence non-maximum suppression (Seq-NMS) [[124](#bib.bib124)], which is
    object detection from each frame where the modified detection results using a
    temporal context are performed separately. With the depth of research, many approaches
    have started to cast VID as a classic object detection problem. For example, a
    network model of feature enhancement module integration called SSD with comprehensive
    feature enhancement (CFE-SSDv2) [[125](#bib.bib125)] has been proposed to improve
    the accuracy of small size VID. F-SSD [[114](#bib.bib114)], based on SSD and FCOS,
    improved the robustness of the model through decision fusion of each frame detection
    result. EODST[[126](#bib.bib126)], based on SSD, adopted ECO tracking methods
    to associate object detection from a single frame. Similarly, benefiting from
    some advanced detectors, like HRDet [[127](#bib.bib127)], Cascade R-CNN [[128](#bib.bib128)],
    CenterNet [[129](#bib.bib129)], RetinaNet [[130](#bib.bib130)], and FPN [[131](#bib.bib131)],
    statistical convolutional neural network (SCNN) [[115](#bib.bib115)], several
    networks specifically developed for VID have been proposed. Some literature focus
    on the performance and real-time of these networks, so as to develop them on mobile
    [[117](#bib.bib117)] or embedded systems [[116](#bib.bib116)]. However, these
    methods are difficult to cover the context information for video. Although there
    are some methods to integrate Spatio-temporal information, e.g., Spatio-temporal
    neural network built on STDnet (STDnet-ST) [[121](#bib.bib121)], the problems
    of missed and false inspection still persist.
  prefs: []
  type: TYPE_NORMAL
- en: The remainder of this section introduces three mainstream DL-based VID methods,
    including optical flow-based network, memory network-based network and tracking-based
    network, which integrate temporal context information into the DL-based methods
    to yield a better detection performance of VID and correct false alarms and missed
    detection.
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Optical Flow-based Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to build the relationship between consecutive frames, some researchers
    estimate motion information. The most commonly used motion estimation method is
    optical flow.
  prefs: []
  type: TYPE_NORMAL
- en: '[[118](#bib.bib118)] and [[120](#bib.bib120)] used the effective CNN model
    for optical flow (PWC-Net) [[132](#bib.bib132)] method and spatial pyramid network
    (SPyNet) [[133](#bib.bib133)] to obtain the motion information of two neighbor
    frames, respectively. Zhu et al. [[134](#bib.bib134)] designed fusion feature
    maps to achieve VID using deep feature flow (DFF) by learning the feature maps
    of key frames using feature extracting and of non-key frames using FlowNet. FlowNet
    was 11.8 times faster than Mobilenet, and even the smallest FlowNet-Xception was
    1.6 times faster. The flow guided feature aggregation (FGFA) [[135](#bib.bib135)]
    proposed by the MSRA visual computing group is also an early attempt based on
    optical flow. FGFA enhances the features of each frame by aggregating the features
    of multiple frames, finally using FlowNet to warp the features to solve video
    degradation. While FGFA is helpful for medium and fast speed VID, it is less effective
    for slow-speed VID. Subsequently, FGFA+ achieved better results by merging several
    data expansion strategies. Ref. [[136](#bib.bib136)] proposed an impression network,
    that can perform multi-frame feature fusion between sparse key frames, solving
    problems like defocus, motion, blur, and other issues in VID, while balancing
    detection speed and accuracy. Built upon [[135](#bib.bib135), [134](#bib.bib134)],
    Zhu et al. [[137](#bib.bib137)] adapted the flow network to learn multi-frame
    features and estimate cross-frame motion. Zhu et al. [[138](#bib.bib138)] subsequently
    designed a more lightweight optical flow network on mobiles. The entire network
    is trained end-to-end, reaching a mean average precision (MAP) of 60.2 in VID,
    and running to a speed of 25 frames on a Huawei Mate 8 cellphone. Due to a large
    amount of optical flow calculation using multiple frames, the network cannot perform
    back propagation revision during the training phase.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Memory Networks-based Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since a video sequence has a strong long-term correlation, researchers introduced
    a memory network to fully learn the time information in a video sequence, such
    as a recurrent neural network (RNN) [[139](#bib.bib139)], long short-term memory
    (LSTM), and gated recurrent unit (GRU).
  prefs: []
  type: TYPE_NORMAL
- en: In [[140](#bib.bib140)], Lu et al. proposed an association LSTM that fundamentally
    modeled object association between consecutive frames, and prompted LSTM to supply
    high quality association features. Refs. [[136](#bib.bib136)] and [[141](#bib.bib141)]
    both used ConvLSTM for efficient fusion of multi-frame features, improving video
    object detection accuracy while ensuring timeliness. In particular, [[141](#bib.bib141)]
    developed a new cross framework that used two feature extractors to run on different
    frames to improve the robustness of detectors. Liu et al [[142](#bib.bib142)]
    proposed an inter woven recurrent-convolutional architecture by designing the
    Bottleneck-LSTM layer to ensure real-time detection. Inspired by [[137](#bib.bib137)]
    and [[136](#bib.bib136)], Jiang et al. [[143](#bib.bib143)] adopted a brain-inspired
    memory mechanism to design a locally weighted deformable neighbors method for
    video object detection. Tripathi et al. [[144](#bib.bib144)] trained RNN through
    the content information of adjacent frames to optimize VID. Unlike the motion
    information learning of adjacent frames, Xiao et al [[145](#bib.bib145)] proposed
    a spatio-temporal memory network (STMN) to model and align the long-term sequence
    appearance and motion dynamics of objects in an end-to-end manner by learning
    multiple frames information. Wang et al. [[145](#bib.bib145)] proposed a motion-aware
    network (MANet) to directly learn motion information over a long period of time
    by fusing multiple frame features.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Tracking-based Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In view of the high similarity between VID and object tracking in video discussed
    in the next section, there are still some methods to achieve VID by means of a
    tracking method [[114](#bib.bib114)] or to achieve object detection and tracking
    at the same time [[146](#bib.bib146)]. [[114](#bib.bib114)] proposed a novel spatial
    and temporal context-aware approach based on tracking for drone-based video object
    detection. In [[146](#bib.bib146)], the authors designed a scheduler network as
    a generalization of siamese trackers determined to detect or track at a certain
    frame. Actually, detection and tracking always coexist in actual scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a2e0eb283381a7537c412eb7cff46fee.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: An illustration of the difference between VID and MOT in up/down
    frames.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/076fb677c2b493d1081bd35c46f410db.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The development of typical methods for UAV object tracking from video.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: DL-based Multiple Object Tracking Approaches for UAV exclusive'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multiple Object Tracking |'
  prefs: []
  type: TYPE_TB
- en: '| --- |'
  prefs: []
  type: TYPE_TB
- en: '| Reference | Challenge | Dataset Used | Journal/Proc. | Year | Code/Link |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Deep SORT [[147](#bib.bib147)] | Occlusion | VisDrone-MOT | ICIP | 2017 |
    [https://github.com/nwojke/deep_sort](https://github.com/nwojke/deep_sort) |'
  prefs: []
  type: TYPE_TB
- en: '| SCTrack [[148](#bib.bib148)] | Missed detection, occlusions | VisDrone |
    AVSS | 2018 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al[[149](#bib.bib149)] | Occlusion | VisDrone-MOT | Comput. Electr.
    Eng. | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| OSIM[[150](#bib.bib150)] | Orientation, scale | UAVDT | Remote Sens. | 2019
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Flow-tracker[[151](#bib.bib151)] | ID Switches, error detection | VisDrone-MOT
    | ICCV | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| TNT[[152](#bib.bib152)] | Camera motion, occlusion, pose variation | VisDrone-MOT,
    Own | ACM-MM | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| HMTT[[153](#bib.bib153)] | Target motion, shape, appearance changes | VisDrone-MOT
    | ICCV | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al[[154](#bib.bib154)] | Target position changes | Own | RS | 2019
    | [https://frank804.github.io/](https://frank804.github.io/) |'
  prefs: []
  type: TYPE_TB
- en: '| GGD[[155](#bib.bib155)] | False alarms, missed detections | VisDrone-MOT
    | ICCV | 2019 | [https://github.com/hakanardo/ggdtrack](https://github.com/hakanardo/ggdtrack)
    |'
  prefs: []
  type: TYPE_TB
- en: '| COMET[[156](#bib.bib156)] | Small object | UAVDT, VisDrone-MOT, Small-90
    | ICCV | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Self-balance[[157](#bib.bib157)] | Appearance, motion | UAVDT | Multimedia
    Asia | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Abughalieh et al[[117](#bib.bib117)] | Low detailed targets | DARPA, VIVID,
    Own | Multimed. Tools.Appl. | 2019 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Tracktor++[[158](#bib.bib158)] | Occlusions, crowded scenes | VisDrone-MOT
    | ICCV | 2019 | [https://git.io/fjQr8](https://git.io/fjQr8) |'
  prefs: []
  type: TYPE_TB
- en: '| IPGAT[[159](#bib.bib159)] | Small object, appearance unreliable | UAVDT,
    Stanford Drone | PRL | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Kapania et al[[160](#bib.bib160)] | Real-time | VisDrone-MOT | AIMS | 2020
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| PAS tracker[[161](#bib.bib161)] | False detections | VisDrone-MOT | ECCV
    | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DAN[[77](#bib.bib77)] | Dense distribution, small object | VisDrone-MOT |
    NCC | 2020 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DQN[[162](#bib.bib162)] | Small target | UAVDT | Electronics | 2021 | - |'
  prefs: []
  type: TYPE_TB
- en: '| Cas_RCNN+ FPN[[80](#bib.bib80)] | Complex background | VisDrone-MOT | Transp.
    Res. Rec. | 2021 | - |'
  prefs: []
  type: TYPE_TB
- en: '| HDHNet[[163](#bib.bib163)] | Small object,class imbalance | VisDrone-MOT
    | Multimed. Tools. Appl. | 2021 | - |'
  prefs: []
  type: TYPE_TB
- en: V Multiple Object Tracking from UAV-borne Video
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Multiple object tracking (MOT) for UAV video attract increasing research interest
    in recent years due to the flexibility of the camera in the drone platform. The
    popular DL-based MOT methods are not usually optimal for drone video data, due
    to new challenges, e.g., large viewpoint change and scales in drone platforms.
    Fig. [7](#S4.F7 "Figure 7 ‣ IV-C Tracking-based Network ‣ IV Object Detection
    from UAV-borne Video ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows a brief process to clarify the differences between VID and MOT.
    Both VID (Section [IV](#S4 "IV Object Detection from UAV-borne Video ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey")) and MOT need accurate
    object location,and the difference in MOT lies in predicting the trajectory in
    the next frame, in order to obtain the moving state of objects. In contrast, VID
    only needs to modify the detection results of the current frame by using temporal
    context in adjacent frames. In the following, DL-based solutions are summarized
    according to recently published literature. Fig. [8](#S4.F8 "Figure 8 ‣ IV-C Tracking-based
    Network ‣ IV Object Detection from UAV-borne Video ‣ Deep Learning for UAV-based
    Object Detection and Tracking: A Survey") shows the development of typical methods
    for MOT. Among them, methods specially designed for UAV data are listed in Table
    [III](#S4.T3 "TABLE III ‣ IV-C Tracking-based Network ‣ IV Object Detection from
    UAV-borne Video ‣ Deep Learning for UAV-based Object Detection and Tracking: A
    Survey"). Others methods that can solve the above problem, but not specifically
    for UAV data, are described directly in the paragraph. The remainder of this section
    introduce three mainstream DL-based MOT methods, i.e., tracking-by-detection,
    single object tracking assisted method, and memory networks.'
  prefs: []
  type: TYPE_NORMAL
- en: V-A Tracking-by-Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tracking-by-Detection (TBD) is the mainstream method of MOT [[159](#bib.bib159),
    [164](#bib.bib164), [165](#bib.bib165), [147](#bib.bib147), [166](#bib.bib166),
    [167](#bib.bib167), [167](#bib.bib167), [168](#bib.bib168)]. The main steps of
    TBDs are to first detect all objects of interests for the current frame, and then
    perform data associated with the previous frame for tracking. This method has
    the virtue of tracking newly arising objects in the whole video, but detection
    accuracy has a decisive effect on tracking results. In the TBD method, MOT is
    considered a data-dependent problem.
  prefs: []
  type: TYPE_NORMAL
- en: The commonly used TBD is CMOT [[164](#bib.bib164)], MDP [[165](#bib.bib165)],
    SORT [[169](#bib.bib169)] and DSORT [[147](#bib.bib147), [160](#bib.bib160), [77](#bib.bib77)],
    GOG [[166](#bib.bib166)], CEM [[170](#bib.bib170)], SMOT [[167](#bib.bib167)],
    and IOUT [[168](#bib.bib168), [151](#bib.bib151)]. For these methods, DL is only
    responsible for object detection, and traditional data-related methods are for
    data association. Recently, many learning-based data association approaches have
    been proposed. For example, Schulter et al. [[171](#bib.bib171)] designed an end-to-end
    network to solve the association problem. Son et al. [[172](#bib.bib172)] proposed
    a quadruplet convolutional neural network (Quad-CNN) with learning data association
    across frames by quadruplet losses. Feichtenhofer et al. [[173](#bib.bib173)]
    introduced correlation features and produced data association cross frames by
    linking the frame-level detection, which could simultaneously achieve object detection
    and tracking. Sun et al. [[127](#bib.bib127)] adopted a depth network to realize
    end-to-end feature extraction and data association. Jadhav et al. [[174](#bib.bib174)]
    proposed multiple object tracking methods by training a custom deep association
    network. Zhang et al. [[152](#bib.bib152)] developed a UAV tracking system, which
    is an integration of RetinaNet and TrackletNet Tracker (TNT). Huang et al. [[163](#bib.bib163)]
    proposed a hierarchical deep high-resolution network (HDHNet) to achieve an end-to-end
    online MOT system. Stadler et al. [[161](#bib.bib161)] proposed a PAS tracker
    that employs a novel similarity measure and Cascade RCNN to make full use of object
    representations. Yang et al. [[154](#bib.bib154)] designed dense-optical-flow-trajectory
    voting to measure the similarity of objects in adjacent frames, and integrated
    YOLOv3 to realize MOT.
  prefs: []
  type: TYPE_NORMAL
- en: Another way to optimize the track association is Siamese network [[175](#bib.bib175)],
    which is a similarity measurement method that is especially suitable for object
    classification when there are more object classes but small quantities in each
    class. It has been widely applied in multiple object tracking [[176](#bib.bib176),
    [177](#bib.bib177), [178](#bib.bib178), [179](#bib.bib179), [180](#bib.bib180),
    [181](#bib.bib181)]. For example, LEE et al. [[176](#bib.bib176)] proposed an
    on online object tracking using rule distillated Siamese random forest. Jin et
    al. [[177](#bib.bib177)] proposed online MOT with Siamese network and optical
    flow (Siamese-OF). Shuai et al. [[178](#bib.bib178)] proposed MOT with Siamese
    Track-RCNN. Bea et al. [[179](#bib.bib179)] proposed an updated Siamese network
    to learn discriminative deep feature representations for MOT. Leal-Taixé et al.
    [[180](#bib.bib180)] developed a multi-modal MOT method by learning the local
    features of RGB images and optical flow maps using a Siamese network. Al-Shakarji
    et al. [[148](#bib.bib148)] designed a time-efficient detection-based multi-object
    tracking system using a three step cascaded data association scheme. Dike et al.
    [[162](#bib.bib162)] proposed a quadruplet network to track prediction objects
    from crowded environments. Yu et al. [[157](#bib.bib157)] proposed a self-balance
    method integrating appearance similarity and motion consistency. Youssef et al.
    [[80](#bib.bib80)] achieve MOT by cascade region-based convolutional neural networks
    and feature pyramid networks.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that if we directly use video data acquired by UAV during
    the flight for MOT, the detection result often contain high noise, false alarm’s
    and missed detection due to changes in the UAV aircraft’s motion, the inevitable
    ”jitter”, and ambient light. Therefore, it is necessary to pre-process the UAV
    video. In addition, Tracking-by-Detection would fail to efficiently match when
    the front and back frames of the object in the video move too fast.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Single Object Tracking Assisted Multiple Object Tracking
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Trajectory prediction can address the failings of Tracking-by-Detection identified
    above well, and the most commonly used method is the single object tracking (SOT)
    assisted method [[182](#bib.bib182), [183](#bib.bib183), [165](#bib.bib165), [184](#bib.bib184),
    [185](#bib.bib185), [153](#bib.bib153)]. With significant progress in this approach
    recently, SOT has been successfully applied to complex scenes [[186](#bib.bib186),
    [187](#bib.bib187), [188](#bib.bib188)], but directly applying SOT to MOT would
    encounter calculation inefficiency and tracking drift caused by occlusion. For
    this reason, Pan et al.[[153](#bib.bib153)] propose a hierarchical multi-target
    tracker (HMTT) incorporating SOT and Kalman filtering to improve the MOT performance.
    Li et al. [[182](#bib.bib182)] designed an multiple vehicle tracking approach
    to effectively integrate SOT based forward position prediction with IOUT to enhance
    the detection results in the association phase. Yan et al. [[183](#bib.bib183)]
    associated detector and SOT trackers as candidate objects, and then candidates
    were selected through an ensemble framework. Xiang et al. [[165](#bib.bib165)]
    adopted the Markov Decision Processes (MDP) method to track objects in a tracked
    state with optical flow. Chu et al. [[184](#bib.bib184)] treated all detection
    output as SOT proposals, and designed MOT network architecture by considering
    multiple objective interactions, yielding a significant improvement for MOT. Ref.
    [[189](#bib.bib189)] proposed a novel instance-aware tracker to effectively integrate
    SOT in to MOT. In [[190](#bib.bib190)], the authors adopted a Siamese-RPN [[191](#bib.bib191)]
    SOT tracker and re-identification (ReID) network to extract short-term and long-term
    clues, respectively. Better data association method called Switcher-aware classification
    (SAC) was then proposed to improve the tracking results while solving the offset
    problem. In the above methods, the SOT tracker is independent of data association,
    which raises a potential issue that the two steps do not collaborate well to reinforce
    each other. To this end, Zhu et al. [[192](#bib.bib192)] proposed Dual Matching
    Attention Networks (DMAN) to deal with intra-class distractors and frequent interactions
    between objects though integrating a unified framework by single object ECO tracking
    and data association.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, for real-time analysis of the SOT-assisted method, offline-trained
    SOT trackers like the Siamese-RPN can achieve high-speed accuracy of more than
    80 frames per second, while an online SOT update consumes a lot of CPU resources.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Multiple Object Tracking Based on Memory Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Similar to the VID, MOT may judge new object status through historical trajectory
    information. Therefore, it is a feasible framework for designing a network structure
    that can memorize historical information and learn matching similarity measurement
    based on this historical information to enhance the performance of MOT [[193](#bib.bib193)].
    Among all the RNNs, the LSTM network has shown reliable performance on many sequence
    problems, and can overcome the gradient disappearance and explosion problems of
    standard RNNs. The special structures of LSTM enable it to remember information
    for a long time. Recently, some methods [[194](#bib.bib194), [195](#bib.bib195),
    [196](#bib.bib196)] employing have achieved impressive performances by LSTM networks.
    Milan et al. [[193](#bib.bib193)] trained an end-to-end LSTM network for online
    MOT. Sadeghian et al. [[185](#bib.bib185)] integrated appearance, action, and
    interaction cues into a unified RNN, and designed feature fusion based on LSTM
    to express motion interaction, so as to learn the matching similarity between
    the trajectory history information and the current detection. After designing
    and analyzing each gate function in LSTM, Kim et al. [[197](#bib.bib197)] proposed
    a novel RNN model called bilinear LSTM based on multiplication so as to improve
    the learning ability of long-term appearance models. Yu et al. [[159](#bib.bib159)]
    estimated the individual motion and global motion by LSTM and Siamese network.
    In [[190](#bib.bib190)], the short-term clues obtained by the Siamese-RPN network
    and the long-term clues obtained by ReID were introduced to meet complex scenarios
    and achieved state-of-the-art tracking performance.
  prefs: []
  type: TYPE_NORMAL
- en: V-D Multiple Object Tracking Based on Others
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Besides the aforementioned methods, other methods for multiple object tracking
    are also available, such as generalized graph differences (GGD) for network flow
    optimization [[155](#bib.bib155)] with an efficient representation of differences
    between graphs, context-aware IoU-guided tracker (COMET) [[156](#bib.bib156)]
    with offline proposal generation and multitask two-stream network. There is also
    literature focusing on designing MOT patrol [[149](#bib.bib149)] or mobile [[117](#bib.bib117)]
    systems for UAV video.
  prefs: []
  type: TYPE_NORMAL
- en: VI UAV-based Benchmark Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the development of data-driven deep learning methods, researchers have
    made a lot of contributions to develop a variety of reference datasets for object
    detection (including images and videos) and tracking in UAV remote sensing, to
    help further study and performance comparison. In this section, we have reviewed
    some of the most commonly used open and classic UAV-based remote sensing datasets
    for detection and tracking.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Comparison of Current State-of-the-Art UAV Benchmarks and Datasets.
    The tasks SOD, VID, SOT, and MOT stands for object detection from images, object
    detection from video, single object tracking, and multiple objects tracking respectively.
    S: Single camera view, M: Multiple camera view, C_View: Camera View'
  prefs: []
  type: TYPE_NORMAL
- en: '| Object Detection from Image | Attributes |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modality | Images | Boxes | Tasks | Image Size | Annotation | Occlusion |
    Weather | C_View | Year |'
  prefs: []
  type: TYPE_TB
- en: '| CARPK[[198](#bib.bib198)] | RGB | 1.5k | 90k | SOD | $1280\times 720$ | HBB
    |  |  | S | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| UAVDT[[199](#bib.bib199)] | RGB | 40k | 841.5k | SOD | $1080\times 540$ |
    HBB | $\surd$ | $\surd$ | M | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| DAC-SDC [[200](#bib.bib200)] | RGB | 150k | - | SOD | $640\times 360$ | HBB
    |  |  | M | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| VisDrone-2018[[21](#bib.bib21)] | RGB | 40.0k | 183.3k | SOD | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| VisDrone-2019[[201](#bib.bib201)] | RGB | 261.9k | 2.6m | SOD | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| DroneVehicle [[202](#bib.bib202)] | RGB + Infrared | 31.064k | 88.3k | SOD
    | $840\times 712$ | OBB |  | $\surd$ | M | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| AU-AIR [[203](#bib.bib203)] | Multi-modal | 32.823k | - | SOD | $1920\times
    1080$ | HBB |  | $\surd$ | M | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| BIRDSAI [[204](#bib.bib204)] | Thermal-IR | - | 270k | SOD | $640\times 480$
    | HBB |  |  | M | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| UVSD [[56](#bib.bib56)] | RGB | 5.8k |  | SOD | $960\times 540$ to $5280\times
    2970$ | HBB/OBB |  |  | M | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| MOHR[[205](#bib.bib205)] | RGB | - | 90k | SOD | $5482\times 3078$/$7360\times
    4912$ | HBB |  |  | M | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| $8688\times 5792$ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Object Detection from Video | Attributes |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modality | Images | Boxes | Tasks | Image Size | Annotation | Occlusion |
    Weather | C_View | Year |'
  prefs: []
  type: TYPE_TB
- en: '| Okutama-Action[[206](#bib.bib206)] | RGB | 77.4k | 422.1k | VID | $3840\times
    2160$ | HBB |  |  | M | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| UAVDT[[199](#bib.bib199)] | RGB | 80k | 841.5k | VID | $1080\times 540$ |
    HBB | $\surd$ | $\surd$ | M | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| VisDrone-2018[[21](#bib.bib21)] | RGB | 40.0k | 183.3k | VID | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| VisDrone-2019[[201](#bib.bib201)] | RGB | 261.9k | 2.6m | VID | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| MOR-UAV [[119](#bib.bib119)] | RGB | 10k | 90k | VID | $1280\times 720$/$1920\times
    1080$ | HBB | $\surd$ | $\surd$ | M | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| Object Tracking from Image | Attributes |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modality | Images | Boxes | Tasks | Image Size | Annotation | Occlusion |
    Weather | C_View | Year |'
  prefs: []
  type: TYPE_TB
- en: '| UAV123[[207](#bib.bib207)] | RGB | 110k | 110k | SOT | $720\times 720$ |
    HBB |  |  | M | 2016 |'
  prefs: []
  type: TYPE_TB
- en: '| DTB70[[208](#bib.bib208)] | RGB | - | - | SOT | $1280\times 720$ | HBB |  |  |
    M | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| Stanford[[209](#bib.bib209)] | RGB | 929.5k | 19.5k | MOT | $1417\times 2019$
    | HBB | $\surd$ |  | S | 2016 |'
  prefs: []
  type: TYPE_TB
- en: '| UAVDT[[199](#bib.bib199)] | RGB | 80k | 841.5k | MOT | $1080\times 540$ |
    HBB | $\surd$ | $\surd$ | M | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| VisDrone-2018[[21](#bib.bib21)] | RGB | 40.0k | 183.3k | MOT | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| VisDrone-2019[[201](#bib.bib201)] | RGB | 261.9k | 2.6m | MOT | $3840\times
    2160$ | HBB | $\surd$ | $\surd$ | M | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| BIRDSAI[[204](#bib.bib204)] | Thermal-IR | 162k | 270k | MOT | $640\times
    480$ | HBB |  |  | M | 2020 |'
  prefs: []
  type: TYPE_TB
- en: 'Stanford Drone Dataset[[209](#bib.bib209)]¹¹1[http://cvgl.stanford.edu/projects/uav_data/](http://cvgl.stanford.edu/projects/uav_data/):
    The Stanford Drone Dataset is a large-scale object tracking dataset, that was
    made public by Stanford University in 2016\. These video sequences were captured
    in a real campus environment by a 4k camera on a quadcopter, which hovered above
    various intersections on campus with a flight height of about 80 meters. This
    dataset contains 10 object types with more than 19,000 objects, including 112,000
    pedestrians, 64,000 bicycles, 13,000 cars, 33,000 skateboarders, 22,000 golf carts,
    and 11,000 public cars, all of which can be used for multiple object tracking.
    Although this dataset only has videos of a college campus, the data has enough
    pluralism to be applied in various scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'UAV123 Dataset[[207](#bib.bib207)]²²2[https://cemse.kaust.edu.sa/ivul/uav123](https://cemse.kaust.edu.sa/ivul/uav123):
    The UAV123 dataset is a long-term aerial object tracking dataset, which was designated
    as public by King Abdullah University of Science and Technology in 2016\. It contains
    123 video sequences and more than 110,000 representative frames. The label information
    of each sequence adopts a horizontal bounding box (i.e., upper left and lower
    right), and the bounding box size and aspect ratio show significant differences
    from the first frame. These video sequences were captured by three different UAVs:
    an off-the-shelf professional-grade UAV (DJIS1000) with a flight height of 5-25
    meters, a small low-cost UAV, and a UAV simulator. The UAV123 dataset has multiple
    variations of scenes, objects, and their corresponding attitudes, making it better
    suited for a deep learning framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Drone Tracking Benchmark (DTB70)[[208](#bib.bib208)]³³3[https://link.zhihu.com/?target=https%3A//github.com/flyers/drone-tracking](https://link.zhihu.com/?target=https%3A//github.com/flyers/drone-tracking):
    The DTB70 dataset includes both short-term and long-term aerial objects, which
    were provided by the Hong Kong University of Science and Technology in 2017\.
    It contains 70 video sequences. Some of these video sequences were captured in
    a real outdoor environment by a DJI Phantom 2 Vision+ drone, which hovered over
    the university campus with a flight altitude of lower than 120 meters. The others
    were intercepted through YouTube to increase the diversity of samples. Each frame
    contains $1280\times 720$ and its label information adopts a horizontal bounding
    box (i.e., upper left and lower right).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Car Parking Lot Dataset (CARPK)[[198](#bib.bib198)]⁴⁴4[https://lafi.github.io/LPN/](https://lafi.github.io/LPN/):
    The CARPK dataset is a large-scale vehicle detection and counting dataset, which
    was designated as public by the National Taiwan University in 2017\. In particular,
    it is the first and largest parking lot dataset acquired by drone views and is
    used for vehicle counting parked in a different parking lot. The dataset was acquired
    by a Phantom 3 Professional drone with a flight height of 40 meters, covering
    nearly 90,000 cars in four different parking lots. The maximum size of vehicles
    in the CARPK dataset is much larger than $64\times 64$, and the maximum number
    of cars in a single scenario in the CARPK dataset is 188\. The label information
    of each vehicle adopts a horizontal bounding box (i.e., upper left and lower right).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Okutama-Action Dataset[[206](#bib.bib206)]⁵⁵5[http://okutama-action.org/](http://okutama-action.org/):
    Okutama dataset is a large-scale human action detection dataset, which was designated
    as public by five universities, including Munich University of Technology and
    the Royal Institute of Technology of Sweden, in 2017\. It contains 43 video sequences
    with 77,365 representative frames. These video sequences were captured at 45-
    or 90-degree camera angles using two drones with a flight height of 10-45 meters.
    In addition, the position and orientation of the UAV are flexible and changeable
    in order to acquire the diversification of the object. This dataset covers 12
    action types, such as reading, handshaking, drinking, and carrying. The speed
    of recorded videos is 30 frames per second (fps), and the image size is $3840\times
    2160$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'UAV Detection and Tracking (UAVDT) Dataset[[199](#bib.bib199)]⁶⁶6[https://sites.google.com/site/daviddo0323/projects/uavdt](https://sites.google.com/site/daviddo0323/projects/uavdt):
    The UAVDT dataset is a large-scale vehicle detection and tracking dataset, which
    was designated as public by the University of the Chinese Academy of Sciences
    in 2018\. It contains 100 video sequences with 80,000 representative frames, approximately
    2,700 vehicles with 0.84 million bounding boxes, covering a range of weather conditions,
    occlusion, and flying heights. This dataset presents all sorts of common scenarios,
    including squares, arterial roads, toll stations, highways, intersections, and
    T-junctions. The speed of recorded videos is 30 frames per second (fps), and the
    image size is $1080\times 540$ pixels, which can be used for multiple tasks, such
    as vehicle detection, single vehicle tracking, and multiple vehicle tracking.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DAC-SDC dataset[[200](#bib.bib200)]⁷⁷7[www.github.com/xyzxinyizhang/2018-DAC-System-Design-Contest](www.github.com/xyzxinyizhang/2018-DAC-System-Design-Contest):
    The Design Automation Conference (DAC) is a challenging object detection dataset
    collected by UAV, which was designated as public by the University of Notre Dame
    in 2018\. It contains 95 categories and 150k images captured with different points
    of UAV view. Each extracted frame includes $640\times 360$ pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'VisDrone2018 Dataset[[21](#bib.bib21)]⁸⁸8[https://github.com/VisDrone/VisDrone-Dataset](https://github.com/VisDrone/VisDrone-Dataset):
    The VisDrone2018 dataset is a large-scale visual object detection and tracking
    dataset, which was designated as public by three universities, Tianjin University,
    GE Global Research, and Temple University, in 2018\. It contains 263 video sequences
    with 179,264 representative frames and 10,209 static images. These video sequences
    were captured by various camera devices using multiple drones (i.e., DJI Mavic
    and Phantom series (3, 3A, 3SE, 3P, 4, 4A, 4P)), which hovered above 14 cities
    in China. This dataset covers multiple common objects, such as pedestrians, cars,
    bicycles, and tricycles. The maximum image size of each video are much larger
    than $2000\times 1500$, and they can be used for multiple tasks, particularly
    object detection, single object tracking, and multiple object tracking. There
    are over 2.5 million objects with their label information in a horizontal bounding
    box.'
  prefs: []
  type: TYPE_NORMAL
- en: 'VisDrone2019 Dataset[[201](#bib.bib201)]⁹⁹9[https://github.com/VisDrone/VisDrone-Dataset](https://github.com/VisDrone/VisDrone-Dataset):
    Compared to VisDrone2018, VisDrone2019 added 25 long-term tracking video sequences
    with a total of 82,644 frames, of which 12 clips were acquired in the daytime,
    and the rest were by night. Therefore, this dataset contains 288 video sequences
    with 261,908 representative frames and 10,209 static images. For each target,
    the scaling is much smaller and the disturbance factor is much greater.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving Object Recognition (MOR-UAV) Dataset[[119](#bib.bib119)]^(10)^(10)10[https://arxiv.org/abs/2008.01699](https://arxiv.org/abs/2008.01699):
    The MOR-UAV dataset is a large-scale video dataset for moving object recognition
    in UAV videos, which was designated as public by the Malaviya National Institute
    of Technology Jaipur in 2020\. It contains 30 video sequences with 10,948 representative
    frames, and approximately 89,783 moving object instances, covering various challenging
    scenarios such as night time, occlusion, camera motion, weather conditions, camera
    views, and so on. MOR-UAV can be used as the benchmark for both MOR and moving
    object detection (MOD) in UAV videos. The videos are recorded at 30 frames per
    second (fps) and the image size varies from $1280\times 720$ to $1920\times 1080$
    pixels. The moving objects are labeled using the Yolo-mark1 tool, and about 10,948
    frames are annotated representing moving vehicles. There are two categories of
    vehicles: cars and heavy vehicles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DroneVehicle dataset[[202](#bib.bib202)]^(11)^(11)11[https://github.com/VisDrone/DroneVehicle](https://github.com/VisDrone/DroneVehicle):
    The DroneVehicle dataset is large-scale object detection and counting dataset
    with both RGB and thermal infrared (RGBT) images captured by camera-equipped drones,
    which was designated as public by the Tianjin University in 2020\. It contains
    15,532 pairs of images, i.e., RGB and infrared images, covering challenging scenarios
    with illumination, occlusion, and scale variations. DroneVehicle dataset can be
    used as the benchmark for both object detection and counting on the UAV platform.
    The images in this dataset were captured over various urban areas, including urban
    roads, residential areas, parking lots, highways, etc., from day to night. The
    image size is $840\times 712$ pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AU-AIR dataset[[203](#bib.bib203)]^(12)^(12)12[https://bozcani.github.io/auairdataset](https://bozcani.github.io/auairdataset):
    The multi-purpose aerial dataset (AU-AIR) is a large-scale object detection dataset
    from multi-modal sensors (i.e., visual, time, location, altitude, IMU, velocity)
    captured by camera-equipped drones, which was designated as public by the Aarhus
    University in 2020\. It contains 8 video sequences with 32,823 extracted frames
    at the intersection of Skejby Nordlandsvej and P.O Pedersensvej (Aarhus, Denmark)
    on windless days with various lighting and weather conditions. This dataset contains
    8 object types, including person, car, bus, van, truck, bike, motorbike, and trailer,
    all of which can be used for static or video object detection. Each frame contains
    $1920\times 1080$ pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'BIRDSAI dataset[[204](#bib.bib204)]^(13)^(13)13[https://sites.google.com/view/elizabethbondi/dataset](https://sites.google.com/view/elizabethbondi/dataset):
    The benchmarking IR dataset for surveillance with aerial intelligence (BIRDSAI)
    is a challenging object detection and tracking dataset collected using a TIR camera
    mounted on a fixed-wing UAV in multiple African protected areas, which was designated
    as public by the Harvard University in 2020\. It contains 48 real aerial TIR videos
    of varying lengths and 124 synthetic aerial TIR videos generated from AirSim-W.
    This dataset contains humans and animals with scale variations, background clutter,
    large camera rotations, and motion blur, etc. Each frame contains $640\times 480$
    pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MOHR dataset[[205](#bib.bib205)]: The benchmarking IR dataset is a large-scale
    benchmark object detection dataset collected at different altitudes by employing
    three cameras, i.e., DJI Phantom 4Pro, Sonny RX1rM2, and Nikon D800\. The dataset
    includes 3,048 images of size $5482\times 3078$, 5,192 images of size $7360\times
    4912$, and 2,390 images of size $8688\times 5792$, respectively. It contains 90,014
    object instances with labels and bounding boxes were annotated, which includes
    25,575 cars, 12,957 trucks, 41,468 buildings, 7,718 flood damages, and 2,296 collapses,
    covering the challenging of scale variations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'UVSD dataset[[210](#bib.bib210)]^(14)^(14)14[https://github.com/liuchunsense/UVSD](https://github.com/liuchunsense/UVSD):
    The UAV-based vehicle segmentation dataset (UVSD) is a large-scale benchmark object
    detection, counting, and segmentation dataset. The dataset includes 5,874 images,
    with 98,600 object instances with high-quality instance-level semantic annotations.
    These images are captured by DJI matrice 200 quadcopter integrated with a zenmuse
    X5S gimbal and camera, and image size varies from $960\times 540$ to $5280\times
    2970$ pixels. In particular, UVSD has multiple format annotations, including pixel-level
    semantic, OBB and HBB.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/be67c289cfab2446f4dd4ab2f491bb99.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Visual samples of annotated images taken from benchmark datasets.
    The first, second, and third rows stand for UAVDT, VisDrone, and Okutama-Action
    datasets, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: VII Experiment Results And Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we take four benchmark datasets, including VisDrone, UAVDT,
    Okutama-Action and Stanford UAV datasets, to illustrate the performance of representative
    object detection and tracking methods. Fig. [9](#S6.F9 "Figure 9 ‣ VI UAV-based
    Benchmark Dataset ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") shows examples of annotated images in these four datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Evaluation of Object Detection from UAV-borne Images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For object detection from UAV-borne images, the common performance metrics are
    Average Precision (AP) and Average Recall (AR). AP is used as a global measure.
    More precisely, the value of AP and AR are related to the rate between the overlap
    of the detection bounding box and the ground-truth box exceeds a certain percentages.
    The most frequently used is $AP^{IoU=0.50:0.05:0.95}$, $AP^{IoU=0.50}$, $AP^{IoU=0.75}$,
    $AR^{max=1}$, $AR^{max=10}$, $AR^{max=100}$ and $AR^{max=500}$. Specifically,
    $AP^{IoU=0.50:0.05:0.95}$ denotes the mean average precision (mAP), that is, the
    average value of the multiple intersection over union (IOU) threshold, which is
    defined as the geometric overlap between predictions and ground truths, of all
    categories with step size of 0.05\. $AP^{IoU=0.50}$ and $AP^{IoU=0.75}$ are computed
    at a certain IOU threshold over all categories. Moreover, $AP^{s}=AP^{small}$,
    $AP^{m}=AP^{medium}$, $AP^{l}=AP^{large}$ represent the average precision at different
    scales. $AR^{max=1}$, $AR^{max=10}$, $AR^{max=100}$ , and $AR^{max=500}$ are the
    maximum recalls number of 1, 10, 100, and 500 detected objects in each image.
    For more details please refer to [[21](#bib.bib21), [201](#bib.bib201)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [V](#S7.T5 "TABLE V ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") lists quantitative results of several state-of-the-art
    detection methods. Their experiment results are distributed in different UAV object
    detection datasets and most of them just use $AP=AP^{IoU=0.50:0.05:0.95}$ as the
    only evaluation criteria. To be fair, the performance of these works is compared
    according to their AP value under a specific dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'VisDrone dataset: This dataset has severe sample imbalance and occlusion problems
    between small objects. NDFT with domain-robust features, which transfers the learned
    NDFT through UAVDT to VisDrone dataset, achieves the best performance among all
    comparative methods, i.e., 52.77% AP score on the VisDrone-DET validation set
    due to the testing set has been closed after the ICCV2019 conference. The possible
    reason is that NDFT could achieve a substantial gain in robustness to many UAV-specific
    nuisances, such as varying flying altitudes, adverse weather conditions, dynamically
    changing viewing angles, etc. SAMFR with spatial-refinement module and receptive
    field expansion block (RFEB), MPFPN with parallel branch becomes the second and
    third with 33.72% and 29.05% AP scores.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [V](#S7.T5 "TABLE V ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey") shows the results of 10 baseline methods in
    the VisDrone-DET2019 Challenge, i.e., FPN [[131](#bib.bib131)], R-FCN [[19](#bib.bib19)],
    Faster R-CNN (FRCNN) [[20](#bib.bib20)], SSD [[24](#bib.bib24)], Cascade CNN [[128](#bib.bib128)],
    RetinaNet [[130](#bib.bib130)], CornetNet [[211](#bib.bib211)], RefineNet [[212](#bib.bib212)],
    DetNet [[213](#bib.bib213)], and Light Faster R-CNN (Light-RCNN) [[214](#bib.bib214)].
    The samples are in strict accordance, with 6,471 for training, 548 for validation
    and 1,580 for testing. For the parameters of these networks, we adjust them within
    a reasonable range or directly adopted the default values. CornerNet achieves
    the best performance, while SSD^∗ performs the worst.'
  prefs: []
  type: TYPE_NORMAL
- en: 'UAVDT dataset: With different locations but similar environments to the VisDrone
    dataset, UAVDT has higher complexity due to its images collected from a variety
    of scenes. Moreover, the weather condition would increase the difficulty of single,
    multiple, or overlapping small object detection. D2Det published in CVPR2020 with
    dense local regression achieves the best performance among all methods, i.e.,
    56.92% AP score on the testing set. NDFT with domain-robust features, FPN employed
    ResNet101 becomes the second and third with 52.03% and 49.05% AP scores. We also
    report the detection results of 8 baseline DL-based networks, including R-FCN
    [[19](#bib.bib19)], Faster R-CNN (FRCNN) [[20](#bib.bib20)], FRCNN plus FPN [[131](#bib.bib131)],
    SSD [[24](#bib.bib24)], Cascade CNN [[128](#bib.bib128)], Reverse connection with
    Objectness prior Networks (RON) [[130](#bib.bib130)], ClusDet [[215](#bib.bib215)],
    and DMDet [[216](#bib.bib216)], are shown in Table [VI](#S7.T6 "TABLE VI ‣ VII-A
    Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment Results
    And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey").
    Among them, the image size for UAVDT was $1024\times 540$ pixels, while the sample
    size of some methods varied. The network parameters were the same as the VisDrone
    dataset. FPN achieves the best performance, while RON performs the worst.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Performance comparisons of UAV exclusive detection networks and classic
    detection networks. The best performers are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Network | Train/Test | Image Size | AP | AP_50 | AP_75 | AR_1 |
    AR_10 | AR_100 | AR_500 | Exp. Data |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al[[59](#bib.bib59)] | VGG-16 | 3,475/869 | - | 92.00 | - | - | -
    | - | - | - | Own vehicle |'
  prefs: []
  type: TYPE_TB
- en: '| UAV-YOLO[[16](#bib.bib16)] | YOLOv3 | 3,776/630 | $608\times 608$ | 90.86
    | - | - | - | - | - | - | UAV123+Own |'
  prefs: []
  type: TYPE_TB
- en: '| FS-SSD[[66](#bib.bib66)] | VGG16 | 989/459 | $512\times 512$ | 89.52 | -
    | - | - | - | - | - | CARPK |'
  prefs: []
  type: TYPE_TB
- en: '| FS-SSD[[66](#bib.bib66)] | VGG16 | 69,673/53,224 | $512\times 512$ | 65.84
    |  |  |  |  |  |  | Stanford Drone |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al[[59](#bib.bib59)] | VGG-16 | 3,500/831 | $320\times 320$ | 90.40
    | - | - | - | - | - | - | Stanford Drone |'
  prefs: []
  type: TYPE_TB
- en: '| MSOA-Net[[56](#bib.bib56)] | ResNet50 | 3,564/1,725 | $1333\times 800$ |
    77.00 | 91.50 | 83.30 |  |  |  |  | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| GDF-Net[[61](#bib.bib61)] | ResNet50 | 11,915/16,580 | $1200\times 675$ |
    15.40 | 26.10 | 17.00 | 13.20 | 23.10 | 27.60 |  | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| DSYolov3[[55](#bib.bib55)] | Yolov3 | 24,143/16,592 | $1200\times 540$ |
    9.80 | 23.40 | 5.00 | - | - | - |  | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| ClusDet[[57](#bib.bib57)] | ResNeXt101 | 23,238/15,069 | $1080\times 540$
    | 13.70 | 25.50 | 12.50 | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 23,258/15,069 | $1080\times 540$ |
    17.80 | 30.40 | 19.70 | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| D2Det[[74](#bib.bib74)] | ResNet101 | 23,258/15,069 | $1,333\times 800$ |
    56.92 | - | - | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| NDFT[[69](#bib.bib69)] | ResNet101 | 23,258/15,069 | $1,333\times 800$ |
    52.03 | - | - | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 23,258/15,069 | $1,080\times 540$ |
    17.80 | 30.4 | 19.7 | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| DNOD[[79](#bib.bib79)] | YOLOv4 | 23,258/15,069 | $1080\times 540$ | 14.20
    | 31.90 | 11.00 |  | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| DNOD[[79](#bib.bib79)] | EfficientDet-D7 | 23,258/15,069 | $1080\times 540$
    | 12.90 | 32.00 | 10.90 | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| FPN^∗[[131](#bib.bib131)] | FPN | 23,258/15,069 | $1080\times 540$ | 49.05
    | - |  | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| RON[[217](#bib.bib217)] | VGG16 | 23,258/15,069 | $1080\times 540$ | 5.0
    | 15.9 | 1.7 | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| RetinaNet[[130](#bib.bib130)] | RetinaNet | 23,258/15,069 | $1080\times 540$
    | 33.95 | - | - | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| ECas_RCNN[[54](#bib.bib54)] | ResNet50 | 6,371/521 | $1450\times 800$ | 28.40
    | - | - | - | - | - | - | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| GDF-Net[[61](#bib.bib61)] | ResNet50 | 6,471/11,610 | $1200\times 675$ |
    18.20 | 30.80 | 19.20 | 8.10 | 24.10 | 28.70 |  | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| HRDNet[[70](#bib.bib70)] | ResNeXt50+101 | 3,564/1,725 | $3800\times 3800$
    | 35.51 | 62.00 | 35.13 | 0.39 | 3.38 | 30.91 | 46.62 | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| D-A-FS SSD[[63](#bib.bib63)] | - | - | - | - | - | - | - | - | - | - | VisDrone-Val
    |'
  prefs: []
  type: TYPE_TB
- en: '| ClusDet[[57](#bib.bib57)] | ResNeXt101 | 6,471/548 | $2000\times 1500$ |
    32.40 | 56.20 | 31.60 | - | - | - | - | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| CenterNet[[58](#bib.bib58)] | HourGlass-104 | 3,564/1,725 | $1024\times 1024$
    | 21.58 | 48.09 | 16.76 | 12.04 | 29.60 | 39.63 | 40.42 | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 6,471/548 | $2000\times 1500$ | 30.30
    | 51.80 | 30.90 | - | - | - | - | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| NDFT[[69](#bib.bib69)] | ResNet101 | 6,471/548 | $2,000\times 1,500$ | 52.77
    | - | - | - | - | - | - | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | 6,471/548 | $2,000\times 1,500$ | 24.60
    | 44.40 | 24.10 | - | - | - | - | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| MPFPN[[73](#bib.bib73)] | ResNet101 | 6,471/1,580 | $1440\times 800$ | 29.05
    | 54.38 | 26.99 | 0.55 | 5.81 | 35.57 | 45.69 | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| SAMFR[[67](#bib.bib67)] | DetNet59 | 6,471/548 | $512\times 512$ | 33.72
    | 58.62 | 33.88 | 0.53 | 3.40 | 22.60 | 46.03 | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| DANN[[77](#bib.bib77)] | RetinaNet | 6,471/548 | $1500\times 1000$ | 11.19
    | 25.65 | 8.78 | 0.56 | 4.87 | 17.19 | 24.09 | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| Cas_RCNN+FPN[[80](#bib.bib80)] | ResNet101 | 4,960/1,534 | $1500\times 2000$
    | 20.46 | 38.58 | 18.83 | 1.32 | 11.32 | 25.82 | 25.84 | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| DNOD[[79](#bib.bib79)] | YOLOv4 | 6,471/1,610 | $1260\times 765$ | 54.88
    | - | - | - | - | - | - | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| DNOD[[79](#bib.bib79)] | EfficientDet-D7 | 6,471/1,610 | $1260\times 765$
    | 53.76 | - | - | - | - | - | - | VisDrone-Val |'
  prefs: []
  type: TYPE_TB
- en: '| RRNet[[53](#bib.bib53)] | HourGlass | 6,741/1580 | $512\times 512$ | 29.13
    | 55.82 | 27.23 | 1.02 | 8.50 | 35.19 | 46.05 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| DSYolov3[[55](#bib.bib55)] | Yolov3 | 6,471/548 | $1920\times 1080$ | 22.30
    | 44.50 | 20.30 | - | - | - | - | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| SAMFR[[67](#bib.bib67)] | DetNet59 | 6,471/1,580 | $512\times 512$ | 20.18
    | 40.03 | 18.42 | 0.46 | 3.49 | 21.6 | 30.82 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| SyNet[[71](#bib.bib71)] | CenterNet | 6,471/1,580 | $2000\times 1500$ | 25.10
    | 48.40 | 26.20 | - | - | - | - | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| SlimYOLOv3[[65](#bib.bib65)] | YOLOv3-SPP3-90 | 6,471/548 | $832\times 832$
    | 23.90 | - | - | - | - | - | - | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al[[62](#bib.bib62)] | ResNet50+RPN | 6,471/1580 | $2000\times 1500$
    | 22.61 | 45.16 | 19.94 | 0.42 | 2.84 | 17.1 | 35.27 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| CornerNet^∗[[211](#bib.bib211)] | CornetNet | 6,471/1,580 | $2000\times 1500$
    | 17.41 | 34.12 | 15.78 | 0.39 | 3.32 | 24.37 | 26.11 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| FPN^∗[[131](#bib.bib131)] | FPN | 6,471/1,580 | $2000\times 1500$ | 16.51
    | 32.20 | 14.91 | 0.33 | 3.03 | 20.72 | 24.93 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| Light-RCNN^∗[[214](#bib.bib214)] | Light-RCNN | 6,471/1580 | $2000\times
    1500$ | 16.53 | 32.78 | 15.13 | 0.35 | 3.16 | 23.09 | 25.07 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| Cas_RCNN^∗[[128](#bib.bib128)] | Cascade R-CNN | 6,471/1,580 | $2000\times
    1500$ | 16.09 | 31.91 | 15.01 | 0.28 | 2.79 | 21.37 | 28.43 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| DetNet59^∗[[213](#bib.bib213)] | DetNet-59 | 6,471/1,580 | $2000\times 1500$
    | 15.26 | 29.23 | 14.34 | 0.26 | 2.57 | 20.87 | 22.28 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| RefineNet[[212](#bib.bib212)] | RefineNet | 6,471/1,580 | $2000\times 1500$
    | 14.90 | 28.76 | 14.08 | 0.24 | 2.41 | 18.13 | 25.69 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| RetinaNet^∗[[130](#bib.bib130)] | RetinaNet | 6,471/1,580 | $2000\times 1500$
    | 11.81 | 21.37 | 11.62 | 0.21 | 1.21 | 5.31 | 19.29 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| R-FCN^∗[[19](#bib.bib19)] | R-FCN | 6,471/1,580 | $2000\times 1500$ | 7.20
    | 15.17 | 6.38 | 0.88 | 5.35 | 12.04 | 13.95 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| FRCNN^∗[[20](#bib.bib20)] | FRCNN | 6,471/1,580 | $2000\times 1500$ | 3.55
    | 8.75 | 2.43 | 0.66 | 3.49 | 6.51 | 6.53 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: '| SSD^∗[[24](#bib.bib24)] | SSD | 6,471/1,580 | $2000\times 1500$ | 2.52 |
    4.78 | 2.47 | 0.58 | 2.81 | 4.51 | 6.41 | VisDrone-Det |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VI: Object detection results on the UAVDT-DET testing set. The best performers
    are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Backbone | Train/Test | Image Size | AP | AP_50 | AP_75 | AP_s |
    AP_m | AP_l | Exp. Data |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| R-FCN [[19](#bib.bib19)] | ResNet50 | 23,258/15,069 | $1080\times 540$ |
    7.0 | 17.5 | 3.9 | 4.4 | 14.7 | 12.1 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| SSD [[24](#bib.bib24)] | VGG16 | 23,258/15,069 | $1080\times 540$ | 9.3 |
    21.4 | 6.7 | 7.1 | 17.1 | 12.0 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| FRCNN [[20](#bib.bib20)] | VGG16 | 23,258/15,069 | $1080\times 540$ | 5.8
    | 17.4 | 2.5 | 3.8 | 12.3 | 9.4 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| FRCNN [[20](#bib.bib20)]+FPN [[131](#bib.bib131)] | ResNet50 | 23,258/15,069
    | $1080\times 540$ | 11.0 | 23.4 | 8.4 | 8.1 | 20.2 | 26.5 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| ClusDet [[215](#bib.bib215)] | ResNet50 | -/25,427 | $1080\times 540$ | 13.7
    | 26.5 | 12.5 | 9.1 | 25.1 | 31.2 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| DMDet [[216](#bib.bib216)] | ResNet50 | -/32,764 | $1080\times 540$ | 14.7
    | 24.6 | 16.3 | 9.3 | 26.2 | 35.2 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VII: Performance comparisons of UAV exclusive detection networks and
    classic detection networks for the VisDrone-VID testing set. The best performers
    are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Framework | Train/Test | Image Size | AP | AP_50 | AP_75 | AR_1
    | AR_10 | AR_100 | AR_500 | Exp. Data |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TDFA[[120](#bib.bib120)] | FlowNet+Fea_Agg | 54,503/14,114 | $720\times 1280$
    | - | 87.18 | - | - | - | - | - | Okutama |'
  prefs: []
  type: TYPE_TB
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+RCN | 23,829/16,580 | $1024\times
    540$ | 13.30 | 36.40 | - | - | - | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al[[118](#bib.bib118)] | Cas_RCNN+IRR-PWC | 17,268/5,397 | $1280\times
    720$ | 65.20 | 88.80 | 74.60 | - | - | - | - | VisDrone-VID |'
  prefs: []
  type: TYPE_TB
- en: '| STCA[[114](#bib.bib114)] | F-SSD+FCOS | 24,198/6,322 | - | 18.73 | 44.38
    | 12.68 | - | - | - | - | VisDrone-VID |'
  prefs: []
  type: TYPE_TB
- en: '| TDFA[[120](#bib.bib120)] | FlowNet+Fea_Agg | 24,201/2,819 | $720\times 1280$
    | 27.27 | 50.73 | 27.94 | - | - | - | - | VisDrone-VID |'
  prefs: []
  type: TYPE_TB
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+RCN | 24,201/6,635 | $1,920\times
    1,080$ | 7.50 | 22.40 | - | - | - | - | - | VisDrone-VID |'
  prefs: []
  type: TYPE_TB
- en: '| FGFA^∗ [[135](#bib.bib135), [218](#bib.bib218)] | VGG16 | 24,198/6,322 |
    3840$\times$ 2160 | 18.33 | 39.71 | 14.39 | 10.09 | 26.25 | 34.49 | 34.89 | VisDrone-VID
    |'
  prefs: []
  type: TYPE_TB
- en: '| CFE-SSDv2 [[219](#bib.bib219), [218](#bib.bib218)] | SSD | 24,198/6,322 |
    3840$\times$ 2160 | 21.57 | 44.75 | 17.95 | 11.85 | 30.46 | 41.89 | 44.82 | VisDrone-VID
    |'
  prefs: []
  type: TYPE_TB
- en: '| D&T (R-FCN) [[173](#bib.bib173), [218](#bib.bib218)] | Hourglass | 24,198/6,322
    | 3840$\times$ 2160 | 17.04 | 35.37 | 14.11 | 10.47 | 25.76 | 31.86 | 32.03 |
    VisDrone-VID |'
  prefs: []
  type: TYPE_TB
- en: '| FPN^∗ [[131](#bib.bib131), [218](#bib.bib218)] | ResNet-101 | 24,198/6,322
    | 3840$\times$ 2160 | 16.72 | 39.12 | 11.80 | 5.56 | 20.48 | 28.42 | 28.42 | VisDrone-VID
    |'
  prefs: []
  type: TYPE_TB
- en: '| CornerNet^∗ [[211](#bib.bib211), [218](#bib.bib218)] | Hourglass-59 | 24,198/6,322
    | 3840$\times$ 2160 | 16.49 | 35.79 | 12.89 | 9.47 | 24.07 | 30.68 | 30.68 | VisDrone-VID
    |'
  prefs: []
  type: TYPE_TB
- en: '| CenterNet^∗ [[129](#bib.bib129), [218](#bib.bib218)] | Hourglass | 24,198/6,322
    | 3840$\times$ 2160 | 15.75 | 34.53 | 12.10 | 8.90 | 22.80 | 29.20 | 29.20 | VisDrone-VID
    |'
  prefs: []
  type: TYPE_TB
- en: '| Faster R-CNN^∗ [[20](#bib.bib20), [218](#bib.bib218)] | VGG16 | 24,198/6,322
    | 3840$\times$ 2160 | 14.46 | 31.80 | 11.20 | 8.55 | 21.31 | 26.77 | 26.77 | VisDrone-VID
    |'
  prefs: []
  type: TYPE_TB
- en: '| RD [[212](#bib.bib212), [220](#bib.bib220)] | RefineDet | 24,198/6,322 |
    3840$\times$ 2160 | 14.95 | 35.25 | 10.11 | 9.67 | 24.60 | 29.72 | 29.91 | VisDrone-VID
    |'
  prefs: []
  type: TYPE_TB
- en: '| RetinaNet_s [[130](#bib.bib130), [220](#bib.bib220)] | RetinaNet | 24,198/6,322
    | 3840$\times$ 2160 | 8.63 | 21.83 | 4.98 | 5.80 | 12.91 | 15.15 | 15.15 | VisDrone-VID
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE VIII: Video object detection results on the Okutama-Action and UAVDT
    testing set. “ #vid” is the number of videos that send to the detector. The best
    performers are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Backbone | #vid | Image Size | AP_50 | Exp.Data | Method | Train/Test
    | Image Size | AP | AP_50 | Exp.Data |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| SSD [[24](#bib.bib24)] | VGG | 10 | $960\times 540$ | 18.80 | Okutama-Action
    | Faster RCNN [[199](#bib.bib199)] | 23,829/76,215 | $1080\times 540$ | 6.6 |
    26.00 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| SSD [[24](#bib.bib24)] | ResNet50 | 10 | $608\times 608$ | 52.30 | Okutama-Action
    | SSD [[199](#bib.bib199)] | 23,829/76,215 | $1080\times 540$ | 6.0 | 23.50 |
    UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| R-FCN [[19](#bib.bib19)] | ResNet50 | 10 | $608\times 608$ | 53.50 | Okutama-Action
    | R-FCN [[199](#bib.bib199)] | 23,829/76,215 | $1080\times 540$ | 9.2 | 32.50
    | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| Retinanet [[130](#bib.bib130)] | ResNet50 | 10 | $608\times 608$ | 56.30
    | Okutama-Action | FGFA [[135](#bib.bib135)] | 23,829/76,215 | $1080\times 540$
    | 6.3 | 20.70 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| YOLOv3_tiny [[221](#bib.bib221)] | DarkNet-53 | 10 | $608\times 608$ | 52.40
    | Okutama-Action | FPN[[131](#bib.bib131)] | 23,829/76,215 | $1080\times 540$
    | 11.8 | 29.70 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IX: Performance comparisons of UAV exclusive tracking networks and classic
    tracking networks for the VisDrone-MOT testing set taken MOTA, MOTP, etc as evaluation
    indexes. The best performers are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Framework | Train/Test(seq) | Image Size | MOTA | MOTP | IDF1 |
    FAF | MT | ML | FP | FN | IDS | FM | Exp. Data |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TNT[[152](#bib.bib152)] | RetinaNet50 | 56/33 | $3840\times 2160$ | 48.6
    | - | 58.1 | - | 281 | 478 | 5,349 | 76,402 | 468 | - | VisDrone-MOT |'
  prefs: []
  type: TYPE_TB
- en: '| +TrackletNet |'
  prefs: []
  type: TYPE_TB
- en: '| HDHNet[[163](#bib.bib163)] | HRNet+DLA | 56/7 | $3840\times 2160$ | 32.9
    | 76.9 | 42.3 | - | - | - | 80,454 | 35,686 | 1,056 | 1,242 | VisDrone-MOT |'
  prefs: []
  type: TYPE_TB
- en: '| Flow-Tracker[[151](#bib.bib151)] | IOU+Optical flow | 56/7 | $3840\times
    2160$ | 26.4 | 78.1 | 41.9 | - | 115 | 246 | 9,987 | 43,766 | 127 | 428 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| CMOT^∗  [[164](#bib.bib164)] | Faster RCNN | 56/16 | $3840\times 2160$ |
    31.5 | 73.3 | 51.3 | 1.42 | 282 | 435 | 26,851 | 72,382 | 789 | 2,257 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| TBD^∗  [[222](#bib.bib222)] | Faster RCNN | 56/16 | $3840\times 2160$ | 35.6
    | 74.1 | 45.9 | 1.17 | 302 | 419 | 22,086 | 70,083 | 1,834 | 2,307 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| $H^{2}T^{*}$  [[223](#bib.bib223)] | Faster RCNN | 56/16 | $3840\times 2160$
    | 32.2 | 73.3 | 44.4 | 0.95 | 214 | 494 | 17,889 | 79,801 | 1,269 | 2,035 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| IHTLS^∗  [[167](#bib.bib167)] | Faster RCNN | 56/16 | $3840\times 2160$ |
    36.5 | 74.8 | 43.0 | 0.94 | 245 | 446 | 14,564 | 75,361 | 1,435 | 2,662 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| Ctrack [[224](#bib.bib224)] | Faster RCNN | 56/16 | $3840\times 2160$ | 30.8
    | 73.5 | 51.9 | 1.95 | 369 | 375 | 36,930 | 62,819 | 1,376 | 2,190 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| CEM^∗  [[170](#bib.bib170)] | Faster RCNN | 56/16 | $3840\times 2160$ | 5.1
    | 72.3 | 19.2 | 1.12 | 105 | 752 | 21,180 | 116,363 | 1,002 | 1,858 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| GOG^∗  [[166](#bib.bib166)] | Faster RCNN | 56/16 | $3840\times 2160$ | 38.4
    | 75.1 | 45.1 | 0.54 | 244 | 496 | 10,179 | 78,724 | 1,114 | 2,012 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE X: Performance comparisons of UAV exclusive tracking networks and classic
    tracking networks for the VisDrone-MOT testing set taken AP as evaluation indexes.
    The best performers are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Framework | Train/Test(seq) | Image Size | AP | AP_0.25 | AP_0.5
    | AP_0.75 | AP_car | AP_bus | AP_truck | AP_ped | AP_van | Exp. Data |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- |'
  prefs: []
  type: TYPE_TB
- en: '| PAS Tracker[[161](#bib.bib161)] | CenterNet+IOU | 56/7 | $608\times 608$
    | 50.80 | 66.10 | 52.50 | 33.80 | 62.7 | 81.20 | 43.90 | 30.30 | 35.90 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| HMTT[[153](#bib.bib153)] | CenterNet+IOU | 56/7 | $608\times 608$ | 28.67
    | 39.05 | 27.88 | 19.08 | 44.35 | 30.56 | 18.75 | 26.49 | 23.19 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| DAN[[77](#bib.bib77)] | RetinaNet+DAN | - | $1500\times 1000$ | 13.88 | 23.19
    | 12.81 | 5.64 | 32.20 | 8.83 | 6.61 | 18.61 | 3.16 | VisDrone-MOT |'
  prefs: []
  type: TYPE_TB
- en: '| GGD[[155](#bib.bib155)] | Faster RCNN | 56/33 | $3840\times 2160$ | 23.09
    | 31.01 | 22.70 | 15.55 | 35.45 | 28.57 | 11.90 | 17.20 | 22.34 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: '| Cas_RCNN+FPNCas_RCNN+FPN[[80](#bib.bib80)] | Cascade R-CNN | - | $2000\times
    1500$ | 28.51 | 44.76 | 30.38 | 10.40 | 35.09 | 34.58 | 18.20 | - | 26.18 | VisDrone-MOT
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XI: Performance comparisons of UAV exclusive tracking networks and classic
    tracking networks for the Stanford Droned dataset taken MOTA, MOTP, etc as evaluation
    indexes. The best performers are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Framework | Train/Test(seq) | Image Size | MOTA | MOTP | IDF1 |
    IDP | MT% | ML% | FP | FN | IDS | FM | Exp. Data |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| IPGAT[[159](#bib.bib159)] | LSTM+ CGAN | 36/24 | $1080\times 540$ | 99.9
    | 99.9 | 90.0 | 90.0 | 99.8 | 0.13 | 3 | 833 | 3, 395 | 905 | Stanford Drone |'
  prefs: []
  type: TYPE_TB
- en: '| +Siamese |'
  prefs: []
  type: TYPE_TB
- en: '| CEM [[170](#bib.bib170)] | Faster RCNN | 36/24 | $1417\times 2019$ | 3.0
    | 81.8 | 5.4 | 47.6 | 2.7 | 90.25 | 972,646 | 348,495 | 3,103 | 5,997 | Stanford
    Drone |'
  prefs: []
  type: TYPE_TB
- en: '| GOG [[166](#bib.bib166)] | Faster RCNN | 36/24 | $1417\times 2019$ | 98.9
    | 100.0 | 86.3 | 86.7 | 100.0 | 96.2 | 3 | 66,625 | 4,928 | 2,621 | Stanford Drone
    |'
  prefs: []
  type: TYPE_TB
- en: '| IOUT [[168](#bib.bib168)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.9
    | 100.0 | 93.2 | 93.2 | 98.9 | 1.04 | 0 | 2,497 | 1,170 | 949 | Stanford Drone
    |'
  prefs: []
  type: TYPE_TB
- en: '| SMOT [[167](#bib.bib167)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.1
    | 100.0 | 91.8 | 91.9 | 97.3 | 1.38 | 17,212 | 38,846 | 2,275 | 3,926 | Stanford
    Drone |'
  prefs: []
  type: TYPE_TB
- en: '| SORT [[169](#bib.bib169)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.5
    | 98.1 | 95.7 | 96.0 | 98.0 | 1.05 | 20 | 32,436 | 957 | 952 | Stanford Drone
    |'
  prefs: []
  type: TYPE_TB
- en: '| SLSTM [[195](#bib.bib195)] | Faster RCNN | 36/24 | $1417\times 2019$ | 99.3
    | 99.9 | 89.6 | 89.6 | 99.8 | 0.13 | 11 | 841 | 3, 630 | 906 | Stanford Drone
    |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XII: Performance comparisons of UAV exclusive tracking networks and classic
    tracking networks for the UAVDT dataset taken MOTA, MOTP, etc as evaluation indexes.
    The best performers are highlighted in bold.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Framework | Train/Test(seq) | Image Size | MOTA | MOTP | IDF1 |
    IDP | MT(%) | ML(%) | FP | FN | IDS | FM | Exp. Data |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | ---
    | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| IPGAT[[159](#bib.bib159)] | LSTM+ CGAN | 30/20 | $1080\times 540$ | 39.0
    | 72.2 | 49.4 | 63.2 | 37.4 | 25.2 | 42, 135 | 163, 837 | 2,091 | 10,057 | UAVDT
    |'
  prefs: []
  type: TYPE_TB
- en: '| +Siamese |'
  prefs: []
  type: TYPE_TB
- en: '| OSIM[[150](#bib.bib150)] | YOLOv3 | - | $1080\times 540$ | 88.7 | - | - |
    - | - | - | 8 | 610 | - | - | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| Self-balance[[157](#bib.bib157)] | LSTM | 30/20 | $1080\times 540$ | 38.6
    | 72.1 | 48.5 | 61.1 | 38.9 | 24.4 | 44,724 | 160,950 | 3,489 | 11,796 | UAVDT
    |'
  prefs: []
  type: TYPE_TB
- en: '| UAV_MOT1 [[162](#bib.bib162)] | Faster RCNN | 30/20 | $1417\times 2019$ |
    40.3 | 74.0 | 55.0 | 67.0 | - | - | 30,065 | 150,837 | 1,091 | 3,057 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| RLSTM[[225](#bib.bib225)] | Faster RCNN | 30/20 | $1080\times 540$ | 25.6
    | 69.1 | 31.3 | 38.6 | 36.7 | 25.7 | 71,955 | 180,461 | 1,333 | 13,088 | UAVDT
    |'
  prefs: []
  type: TYPE_TB
- en: '| SLSTM [[195](#bib.bib195)] | Faster RCNN | 30/20 | $1080\times 540$ | 37.9
    | 72.0 | 37.2 | 46.8 | 38.2 | 24.4 | 44,783 | 161,009 | 6,048 | 12,051 | UAVDT
    |'
  prefs: []
  type: TYPE_TB
- en: '| SORT[[169](#bib.bib169)] | Faster RCNN | 30/20 | $1080\times 540$ | 39.0
    | 74.3 | 43.7 | 58.9 | 33.9 | 28.0 | 33,037 | 172,628 | 2,350 | 5,787 | UAVDT
    |'
  prefs: []
  type: TYPE_TB
- en: '| RMOT[[226](#bib.bib226)] | Faster RCNN | 30/20 | $1080\times 540$ | -39.8
    | 72.3 | 33.3 | 27.8 | 36.7 | 25.7 | 319,008 | 151,485 | 5,973 | 5,897 | UAVDT
    |'
  prefs: []
  type: TYPE_TB
- en: '| SMOT[[167](#bib.bib167)] | Faster RCNN | 30/20 | $1080\times 540$ | 33.9
    | 72.2 | 45.0 | 55.7 | 36.7 | 25.7 | 57,112 | 166,528 | 1,752 | 9,577 | UAVDT
    |'
  prefs: []
  type: TYPE_TB
- en: '| CEM[[170](#bib.bib170)] | Faster RCNN | 30/20 | $1080\times 540$ | -7.3 |
    69.6 | 10.2 | 19.4 | 7.3 | 68.6 | 72,378 | 290,962 | 2,488 | 4,248 | UAVDT |'
  prefs: []
  type: TYPE_TB
- en: '| GOG[[166](#bib.bib166)] | Faster RCNN | 30/20 | $1080\times 540$ | 34.4 |
    72.2 | 18.0 | 23.3 | 35.5 | 25.3 | 41,126 | 168,194 | 14,301 | 12,516 | UAVDT
    |'
  prefs: []
  type: TYPE_TB
- en: '| IOUT[[168](#bib.bib168)] | Faster RCNN | 30/20 | $1080\times 540$ | 36.6
    | 72.1 | 23.7 | 30.3 | 37.4 | 25.0 | 42,245 | 163,881 | 9,938 | 10,463 | UAVDT
    |'
  prefs: []
  type: TYPE_TB
- en: VII-B Evaluation of Object Detection from UAV-borne Video
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For object detection from UAV-borne video, the common indicators to evaluate
    object detection methods are the same as UAV-borne image, including $AP^{IoU=0.50:0.05:0.95}$,
    $AP^{IoU=0.50}$,$AP^{IoU=0.75}$, $AR^{max=1}$, $AR^{max=10}$, $AR^{max=100}$ and
    $AR^{max=500}$. Table [V](#S7.T5 "TABLE V ‣ VII-A Evaluation of Object Detection
    from UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for
    UAV-based Object Detection and Tracking: A Survey") lists the public quantitative
    results of some state-of-the-art and baseline detection works. Among them, four
    works are the UAV preserves object detection works, and the experimental results
    are mainly focused on the VisDrone dataset. TDFA with a two-stream refined flowNet
    (SPyNet) pipeline, which is robust to small-scale objects and can achieve the
    best performance among all comparison methods, i.e., 27.27% AP score on the VisDrone-VID
    validation set. MPFPN with parallel branch is ranked as the second and third with
    33.72% and 29.05% AP scores.'
  prefs: []
  type: TYPE_NORMAL
- en: We have also summarized the results of 9 baseline methods in the VisDrone-VID
    challenge, including CFE-SSDv2 [[219](#bib.bib219)], FGFA^∗ [[135](#bib.bib135)],
    RefineDet [[212](#bib.bib212), [220](#bib.bib220)], RetinaNet [[130](#bib.bib130)],
    detection and tracking (D&T) [[173](#bib.bib173)], FPN^∗ [[131](#bib.bib131)],
    CornerNet^∗ [[211](#bib.bib211)], CenterNet^∗ [[129](#bib.bib129)], and Faster
    R-CNN^∗ [[20](#bib.bib20)]. The experiment results were instructed in accordance,
    with three non-overlapping subsets, 56 video sequences with 24,198 frames for
    the training set, 16 video sequences with 6,322 frames for testing, and the remaining
    sequences are for validation. Obviously, the detection performance of object detection
    in video yields better object detection in an image, and detection results amendment
    by context information plays a decisive role. Moreover, a small object is an inevitable
    problem of object detection in video. Therefore, CFE-SSD with small objects friendliness,
    FGFA assisting the current frame by adopting the front and back frames information,
    and D&T with ROI tracking to associate adjacent frames, obtain a better detection
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Besides the VisDrone dataset, some other datasets are also used, such as Okutama-Action
    and UAVDT. Compared with five baseline works in Table [VIII](#S7.T8 "TABLE VIII
    ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey"), TDFA experimented Okutama-Action dataset has achieved the best detection
    performance, i.e., 87.18% AP_50 value on the Okutama-Action test dataset. STDnet-ST
    with Spatio-temporal ConvNet and STDnet experimented Okutama-Action dataset, have
    achieved 34.60% AP for objects under $16\times 16$ pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XIII: Computation Cost of Statistical Object Detection Approaches for
    UAV exclusive'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Network Pipeline | Image Size | Exp. environment | Times/fps
    | Year |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| RRNet[[53](#bib.bib53)] | HourGlass | $512\times 512$ | - | - | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Wu et al[[68](#bib.bib68)] | YOLOv3 | $1080\times 640$ | Workstation(NVIDIA
    Tesla K80/-) | 15 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| SlimYOLOv3[[65](#bib.bib65)] | YOLOv3-SPP3-90 | $832\times 832$ | Workstation(NVIDIA
    GTX 1080Ti/-) | 28.3 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| NDFT[[69](#bib.bib69)] | ResNet101 | - | Workstation(-/-) | - | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| ClusDet[[57](#bib.bib57)] | ResNeXt101 | $2000\times 1500$ | Workstation(NVIDIA
    GTX 1080 Ti/-) | 1.3 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| CenterNet[[58](#bib.bib58)] | HourGlass104 | $1024\times 1024$ | - | - |
    2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al[[59](#bib.bib59)] | VGG16 | $320\times 320$ | Worstation(NVIDIA
    GTX-1080Ti/12GB) | 58 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al[[62](#bib.bib62)] | ResNet50 | - | Workstation(NVIDIA GeForce
    1060/6GB) | - | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| FS-SSD[[66](#bib.bib66)] | VGG16 | $512\times 512$ | Workstation(NVIDIA TITAN
    X (Pascal)/12GB) | 18.3 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| SAMFR[[67](#bib.bib67)] | DetNet59 | $419\times 419$ | - | 10 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| MSOA-Net[[56](#bib.bib56)] | ResNet50 | $1333\times 800$ | Workstation(NVIDIA
    TITAN-Xp/-) |  | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| GDF-Net[[61](#bib.bib61)] | ResNet50 | $1200\times 675$ | Workstation(NVIDIA
    Geforce RTX 2080ti/11GB) | 17.9 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| HRDNet[[70](#bib.bib70)] | ResNeXt50+101 | $960\times 1360$ | Workstation(NVIDIA
    GTX 2080Ti/-) | 0.7 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| D-A-FS SSD[[63](#bib.bib63)] | - | - | - | - | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| UAV-YOLO[[16](#bib.bib16)] | YOLOv3 | $608\times 608$ | Workstation(NVIDIA
    GTX Titan XP/64GB) | 20 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| SyNet[[71](#bib.bib71)] | CenterNet | - | - | - | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| ComNet[[72](#bib.bib72)] | YOLOv3 | $416\times 416$ | Workstation(NVIDIA
    GTX 1080Ti/12GB) | 20 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| ComNet[[72](#bib.bib72)] | YOLOv3 | $416\times 416$ | laptop(Intel Core i5-8300/4GB)
    | 3 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| ComNet[[72](#bib.bib72)] | YOLOv3 | $416\times 416$ | Jason Nano(Tegra X1/4GB)
    | 2 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| MPFPN[[73](#bib.bib73)] | ResNets101 | $1440\times 800$ | Workstation(NVIDIA
    GTX 1080Ti/-) | 2.1 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| D2Det[[74](#bib.bib74)] | ResNet101 | $1333\times 800$ | Workstation(NVIDIA
    GTX Titan Xp/-) | 5.9 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| DAGN[[75](#bib.bib75)] | YOLOv3 | $512\times 512$ | Workstation(NVIDIA GeForce
    GTX 1080Ti/11GB) | 25.1 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| GANet[[76](#bib.bib76)] | VGG-16/ResNet50 | $512\times 512$ | Workstation(NVIDIA
    GTX Titan Xp/-) | - | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| DAN[[77](#bib.bib77)] | Resnet-50 | $1500\times 1000$ | - | - | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al[[78](#bib.bib78)] | PeleeNet | $304\times 304$ | Workstation(NVIDIA
    TITAN X (Pascal)/12GB) | 23.6 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| DSHNet[[81](#bib.bib81)] | ResNet50 | $2000\times 1500$ | Workstation(NVIDIA
    1080Ti/11GB) | 10.8 | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| DNOD[[79](#bib.bib79)] | VGG19+CSPDarknet53 | $608\times 608$ | Workstation(NVIDIA
    GeForce RTX 2080ti/6GB) | 38.3 | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| ECas_RCNN[[54](#bib.bib54)] | ResNet50 | $1450\times 800$ | Workstation(NVIDIA
    RTX 2080Ti/-) | - | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| DSYolov3[[55](#bib.bib55)] | Yolov3 | $416\times 416$ | Workstation(NVIDIA
    GTX 1080Ti/-) | 13.7 | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| Cas_RCNN+FPN[[80](#bib.bib80)] | ResNet101 | $1500\times 2000$ | - | - |
    2021 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XIV: Computation Cost of DL-based Video Object Detection Approaches for
    UAV exclusive'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Network Pipeline | Image Size | Exp. environment | Times/fps
    | Year |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| STCA[[114](#bib.bib114)] | SSD+FCOS+SiamFC | $300\times 300$ | Workstation(NVIDIA
    GeForce GTX 1080/-) | - | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | laptop(Core
    i7-2670QM/6GB) | 26.3 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | Embedded(Raspberry
    Pi 2/1GB) | 10.8 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Nousi et al[[116](#bib.bib116)] | Tity YOLO | $288\times 288$ | Embedded(Robot
    Operating System) | 23 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| SCNN[[115](#bib.bib115)] | ResNet34 | $224\times 224$ | Workstation(NVIDIA
    GeForce GTX 1080/-) | 246 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al[[118](#bib.bib118)] | Cas_R-CNN+IRR-PWC [[132](#bib.bib132)]
    | $720\times 1280$ | - | - | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| MOR-UAVNet[[119](#bib.bib119)] | MOR-UAVNetv14 | $608\times 608$ | Workstation(NVIDIA
    RTX 2080 Ti/11GB) | 10.5 | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| TDFA[[120](#bib.bib120)] | FlowNet+ Fea_Aggregation | $720\times 1280$ |
    Workstation(NVIDIA GeForce GTX TITAN X/12GB) | 3.8 | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+ConvNet | $1280\times 720$ | - | -
    | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+ConvNet | $1920\times 1080$ | - |
    - | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| STDnet-ST[[121](#bib.bib121)] | STDnet+ConvNet | $1024\times 540$ | - | -
    | 2021 |'
  prefs: []
  type: TYPE_TB
- en: VII-C Evaluation of Object Tracking from UAV-borne Video
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For object tracking from UAV-borne video, the common way to evaluate object
    detection methods, including multiple object tracking accuracy (MOTA), multiple
    object tracking precision (MOTP), identification precision (IDP), identification
    F1 score (IDF1), false alarms per frame (FAF), the number of mostly tracked targets
    (MT, more than 80% of trajectories being covered by the ground truth), the number
    of mostly lost targets (ML, less than 20% of trajectories being covered by the
    ground truth), the number of false positives (FP), the number of false negatives
    (FN), the number of ID switches (IDS), and the number of times a trajectory is
    Fragmented (FM).
  prefs: []
  type: TYPE_NORMAL
- en: The IDF1 score is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $IDF1=\frac{2IDTP}{2IDTP+IDFP+IDFN},$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where IDTP is the number of true positive IDs, IDFP is the number of false positive
    IDs, and IDFN is the number of false negative IDs. In addition, some literature
    have still adopted the detection evaluation metrics, including $AP^{IoU=0.50:0.05:0.95}$,
    $AP^{IoU=0.25}$, $AP^{IoU=0.50}$, $AP^{IoU=0.75}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Tables [IX](#S7.T9 "TABLE IX ‣ VII-A Evaluation of Object Detection from UAV-borne
    Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for UAV-based Object
    Detection and Tracking: A Survey")-[XII](#S7.T12 "TABLE XII ‣ VII-A Evaluation
    of Object Detection from UAV-borne Images ‣ VII Experiment Results And Analysis
    ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey") summarize
    the quantitative comparison of several multiple object tracking methods on the
    challenging public UAV dataset. In Table [IX](#S7.T9 "TABLE IX ‣ VII-A Evaluation
    of Object Detection from UAV-borne Images ‣ VII Experiment Results And Analysis
    ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey"), the average
    rank of 10 metrics (i.e., MOTA, MOTP, IDF1, FAF, MT, ML, FP, FN, IDS, and FM)
    is used to rank these approaches. TrackletNet Tracker (TNT), wins the VisDrone-MOT
    challenge dataset by the highest MOTA, IDF1, FP, IDS. We also report the accuracy
    of the trackers in AP as well as different object categories, including AP_car,
    AP_bus, AP_trk, AP_ped and AP_van in Table [X](#S7.T10 "TABLE X ‣ VII-A Evaluation
    of Object Detection from UAV-borne Images ‣ VII Experiment Results And Analysis
    ‣ Deep Learning for UAV-based Object Detection and Tracking: A Survey"). The PAS
    tracker followed by the tracking-by-detection paradigm achieves the best performance,
    i.e., 50.80% AP score on the VisDrone-MOT testing set. HMTT based on SOT achieves
    a 28.67% AP score on the VisDrone-MOT validation set. With the exception of VisDrone
    dataset, IPGAT achieves the best tracking performance for the UAVDT and Stanford
    Drone testing dataset in Tables [XI](#S7.T11 "TABLE XI ‣ VII-A Evaluation of Object
    Detection from UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning
    for UAV-based Object Detection and Tracking: A Survey") and [XII](#S7.T12 "TABLE
    XII ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey"), in terms of IDF1, MT, ML, and FN, by estimating object motion and
    UAV movement as individual and global motions, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of Tables [IX](#S7.T9 "TABLE IX ‣ VII-A Evaluation of Object Detection
    from UAV-borne Images ‣ VII Experiment Results And Analysis ‣ Deep Learning for
    UAV-based Object Detection and Tracking: A Survey") to [XII](#S7.T12 "TABLE XII
    ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey") are the results from baseline methods for the three MOT datasets. The
    results are based on Faster RCNN detection input for convenient comparison. For
    VisDrone-MOT dataset consists of 79 video sequences in total, including 56 video
    sequences for the training set, 16 video sequences for testing, and the remainder
    is for validation. Under these settings, Ctrack with recovering long-time disappearance
    objects in the crowded scenes achieves the best tracking performance among all
    methods in the VisDrone testing dataset, in terms of the IDF1, MT, ML, and FN.
    For the UAVDT dataset under 50 sequences recorded in the traffic scenario from
    UAVs, 60% for training and 40% for testing. From Table [XII](#S7.T12 "TABLE XII
    ‣ VII-A Evaluation of Object Detection from UAV-borne Images ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey"), it can be seen that SORT is superior on most metrics. Although these
    results are far from the requirements of practical application, they can provide
    feasible direction (e.g., the association of moving objects) and a reliable theoretical
    basis for future research. For the Stanford Drone dataset, the performance gap
    of the listed baseline method is minimal, maybe IOUT slightly better.'
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Estimation of Computation Cost
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this survey, all the reviewed methods have their own experimental environment,
    experimental data, and even source code. Considering the computation cost is directly
    related to speed, GPU, and backbone model, we list these three indexes of the
    UAV exclusive methods for the above three topics in Tables [XIII](#S7.T13 "TABLE
    XIII ‣ VII-B Evaluation of Object Detection from UAV-borne Video ‣ VII Experiment
    Results And Analysis ‣ Deep Learning for UAV-based Object Detection and Tracking:
    A Survey")- [XV](#S7.T15 "TABLE XV ‣ VII-D Estimation of Computation Cost ‣ VII
    Experiment Results And Analysis ‣ Deep Learning for UAV-based Object Detection
    and Tracking: A Survey"). Depending on the computing power of NVIDIA’s GPU ^(15)^(15)15[https://www.pianshen.com/article/13711825712/](https://www.pianshen.com/article/13711825712/),
    the computation cost can be estimated with backbone network in the corresponding
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XV: Computation Cost of DL-based Multiple Object Tracking for UAV exclusive.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Reference | Network Pipeline | Image Size | Exp. environment | Times/fps
    | Year |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Deep SORT[[147](#bib.bib147)] | Faster R-CNN+ Sort | $1920\times 1080$ |
    Workstation(NVIDIA GeForce GTX 1050/-) | - | 2017 |'
  prefs: []
  type: TYPE_TB
- en: '| SCTrack[[148](#bib.bib148)] | Faster R-CNN+YOLOv3 | - | - | - | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| TNT[[152](#bib.bib152)] | Faster-RCNN+ SVO+MVS+3d loc | - | - | - | 2019
    |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al[[149](#bib.bib149)] | Faster RCNN+ SPHP | $1080\times 540$ | Workstation(NVIDIA
    RTX 2080Ti/-) | - | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| OSIM[[150](#bib.bib150)] | YOLOv3+Kalman filtering | $2720\times 1530$ |
    Workstation(NVIDIA GeForce GTX 1080 Ti/-) | 30 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| +deep appearance feature | Workstation(Intel UHD Graphics 630/-) |'
  prefs: []
  type: TYPE_TB
- en: '| Self-balance[[157](#bib.bib157)] | LSTM | $1080\times 540$ | Workstation(NVIDIA
    Titan X/32GB) | - | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Flow-tracker[[151](#bib.bib151)] | Optical Flownet+IOU | - | Workstation(NVIDIA
    GTX 1080Ti/-) | 5 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| HMTT[[153](#bib.bib153)] | CenterNet+IOU+OSNet | - | - | - | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Yang et al[[154](#bib.bib154)] | YOLOv3 + dense-trajectory-Voting | $1920\times
    1080$ | Workstation(NVIDIA GeForce GTX1080Ti/6GB) | 8.6 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| GGD[[155](#bib.bib155)] | Faster RCNN+GGD | - | Workstation(NVIDIA GTX 1080/-)
    | - | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| COMET[[156](#bib.bib156)] | ResNet-50+Two-stream network | $1080\times 540$
    | Workstation(NVIDIA Tesla V100/16GB) | 24 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | Laptop(Core
    i7-2670QM/6GB) | 26.3 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Abughalieh et al[[117](#bib.bib117)] | FAST | $320\times 240$ | Embedded(Raspberry
    Pi 2/1GB) | 10.8 | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| IPGAT[[159](#bib.bib159)] | SiameseNet+LSTM+CGAN | - | Workstation(NVIDIA
    Titan X/32GB) | - | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| Kapania et al[[160](#bib.bib160)] | YOLOv3+RetinaNet | - | - | - | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| PAS tracker[[161](#bib.bib161)] | Cascade R-CNN+Similarity | - | - | - |
    2020 |'
  prefs: []
  type: TYPE_TB
- en: '| DAN[[77](#bib.bib77)] | RetinaNet+DeepSORT | $1500\times 1000$ | - |  | 2020
    |'
  prefs: []
  type: TYPE_TB
- en: '| DQN[[162](#bib.bib162)] | Faster R-CNN | - | - | - | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| Youssef et al[[80](#bib.bib80)] | Cascade R-CNN+FPN | $2000\times 1500$ |
    Workstation(NVIDIA Quadro RTX5000/16GB) | - | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| HDHNet[[163](#bib.bib163)] | HDHNet+DeepSORT+Cas_RCNN | $1280\times 720$
    | Workstation(NVIDIA TITAN RTX/24GB) | 4.3 | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| $2880\times 1620$ | 2.1 |'
  prefs: []
  type: TYPE_TB
- en: VIII Discussion and Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, deep learning approached in object and tracking of the remote
    sensing field has been systematically analyzed according to three UAV topics,
    i.e., SOD, VID, and MOT. The conclusions were drawn as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'UAV data: The public UAV-borne datasets for object detection and tracking are
    mainly visible data, and the largest image size is $3840\times 2160$ (VisDrone
    dataset). There are only one multiple source data called Vehicle dataset with
    visible-thermal infrared cameras equipped with drones. In label terms, the bounding
    box is not limited to horizontal bounding box strongly dependent on robustness
    to direction, even have oriented bounding box, e.g., in the Vehicle Dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DL Method: This survey reviews DL-based object detection and tracking methods
    for UAV acquired data from three topics. In general, most classical DL methods,
    by appending extra modules available to UAV challenges, can be applied to these
    three topics. Specifically, considering different requirements for precision and
    speed, the existing static object detection methods especially for UAV are mainly
    based on YOLO (e.g., UAV-YOLO, ComNet, SlimYOLOv3, DAGN, etc), Faster RCNN (e.g.,
    Dshnet, NDFT, D2det, etc), and SSD (e.g., FS SSD). Among them, YOLO, and SSD based
    methods are advantageous in speed. For VID and MOT, there are few methods especially
    designed for UAV data. Most literature are still about classical methods for natural
    scene data, such as Flownet, LSTM for VID, and DeepSort, SiamRPN for MOT. As a
    consequence, their performance is far from perfect, e.g., the highest AP for VisDrone-VID
    is just 65.2%, and for VisDrone-MOT is just 50.80%. Further effort is needed to
    solve the interaction of ground objects and the complexity of the tracking scenes.
    As for systems, the existing UAV object detection and tracking systems are mainly
    based on the classic DL method, where the speed can be guaranteed but the accuracy
    still needs to be improved.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Computer platforms: The image/video acquired by UAV in this review mainly belongs
    to the remote sensing community. In this community, DL-based methods are mainly
    carried out on various NVIDIA series GPU, e.g., TITAN Xp, RTX 2080Ti, GTX 1080Ti,
    etc. Their processing speed is roughly within the range of $0.2fps\sim 50fps$
    with different image size. Although the research reviewed in [[117](#bib.bib117),
    [116](#bib.bib116)] designs object detection and tracking system using a Raspberry
    Pi 2 minicomputer process for object detection with 11fps, or using a Jetson TX2
    embedded platform for object detection with 8.5fps, even 4.5fps [[227](#bib.bib227)]
    and object tracking with 15fps, but lacks generality.'
  prefs: []
  type: TYPE_NORMAL
- en: Object detection with tracking reflects a perfect union in engineering practice.
    With tracking assistance, detection becomes stable and exhibits no jitter. Meanwhile,
    fine labels and ID information of objects with the same class are also given.
    Through automatic analysis and extraction of trajectory features, false and missed
    detection rates can be significantly reduced. In the near future development of
    object detection and tracking in UAV remote sensing is expected and new techniques
    will emerge to improve these metrics even further. In addition, efficiently processing
    massive multi-source UAV remote sensing data are worth consideration. UAVs equipped
    with different sensors, e.g., visible, infrared, thermal infrared, multispectral,
    hyperspectral sensors, can integrate a variety of sensing modalities to make use
    of their complementary properties, which further realizes more robust and accurate
    object tracking and detection.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. Shahbazi, J. Théau, and P. Ménard, “Recent applications of unmanned
    aerial imagery in natural resource management,” GISci. Remote Sens., vol. 51,
    no. 4, pp. 339–365, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] H. Shakhatreh, A. H. Sawalmeh, A. Al-Fuqaha, Z. Dou, E. Almaita, I. Khalil,
    N. S. Othman, A. Khreishah, and M. Guizani, “Unmanned aerial vehicles (uavs):
    A survey on civil applications and key research challenges,” IEEE Access, vol. 7,
    pp. 48572–48634, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] M. Shahbazi, J. Théau, and P. Ménard, “Recent applications of unmanned
    aerial imagery in natural resource management,” GISci. Remote Sens., vol. 51,
    no. 4, pp. 339–365, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] D. Hong, N. Yokoya, N. Ge, J. Chanussot, and X. Zhu, “Learnable manifold
    alignment (LeMA): A semi-supervised cross-modality learning framework for land
    cover and land use classification,” ISPRS J. Photogramm. Remote Sens., vol. 147,
    pp. 193–205, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] D. Hong, L. Gao, N. Yokoya, J. Yao, J. Chanussot, Q. Du, and B. Zhang,
    “More diverse means better: Multimodal deep learning meets remote-sensing imagery
    classification,” IEEE Trans. Geosci. Remote Sens., vol. 59, no. 5, pp. 4340–4354,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] E. Honkavaara, H. Saari, J. K. nd I. Pölönen, T. Hakala, P. Litkey, J. Mäkynen,
    and L. Pesonen, “Processing and assessment of spectrometric, stereoscopic imagery
    collected using a lightweight uav spectral camera for precision agriculture,”
    Remote Sens., vol. 5, no. 10, pp. 5006–5039, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] R. Francisco, Q. Zhang, and J. Reid, “Stereo vision three-dimensional terrain
    maps for precision agriculture,” Comput. Electron. Agr., vol. 60, no. 2, pp. 133–143,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] H. Menouar, I. Guvenc, K. Akkaya, A. S. Uluagac, A. Kadri, and A. Tuncer,
    “Uav-enabled intelligent transportation systems for the smart city: Applications
    and challenges,” IEEE Commun. Mag., vol. 55, no. 3, pp. 22–28, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] E. Honkavaara, H. Saari, J. Kaivosoja, I. Pölönen, T. Hakala, P. Litkey,
    J. Mäkynen, and L. Pesonen, “Processing and assessment of spectrometric, stereoscopic
    imagery collected using a lightweight uav spectral camera for precision agriculture,”
    Remote Sens., vol. 5, no. 10, pp. 5006–5039, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. Erdelj, E. Natalizio, K. R. Chowdhury, and I. F. Akyildiz, “Help from
    the sky: Leveraging uavs for disaster management,” IEEE Pervasive Comput., vol. 16,
    no. 1, pp. 24–32, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] L. Li, W. Huang, I. Y.-H. Gu, and Q. Tian, “Statistical modeling of complex
    backgrounds for foreground object detection,” IEEE Trans. Image Process., vol. 13,
    no. 11, pp. 1459–1472, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] M. Haag and H.-H. Nagel, “Combination of edge element and optical flow
    estimates for 3d-model-based vehicle tracking in traffic image sequences,” Int.
    J. Comput. Vis., vol. 35, no. 3, pp. 295–319, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] B. Rasti, D. Hong, R. Hang, P. Ghamisi, X. Kang, J. Chanussot, and J. Benediktsson,
    “Feature extraction for hyperspectral imagery: The evolution from shallow to deep:
    Overview and toolbox,” IEEE Geosci. Remote Sens. Mag., vol. 8, no. 4, pp. 60–88,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] X. Wu, W. Li, D. Hong, J. Tian, R. Tao, and Q. Du, “Vehicle detection
    of multi-source remote sensing data using active fine-tuning network,” ISPRS J.
    Photogramm. Remote Sens., vol. 167, pp. 39–53, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] D. Hong, L. Gao, J. Yao, B. Zhang, A. Plaza, and J. Chanussot, “Graph
    convolutional networks for hyperspectral image classification,” IEEE Trans. Geosci.
    Remote Sens., vol. 59, no. 7, pp. 5966–5978, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] M. Liu, X. Wang, A. Zhou, X. Fu, Y. Ma, and C. Piao, “Uav-yolo: Small
    object detection on unmanned aerial vehicle perspective,” Sensor, vol. 20, no. 8,
    p. 2238, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Y. Cai, D. Du, L. Zhang, L. Wen, W. Wang, Y. Wu, and S. Lyu, “Guided attention
    network for object detection and counting on drones,” arXiv preprint arXiv:1909.11307,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] R. Girshick, J. Donahue, T. Darrell, and J. Malik, “Rich feature hierarchies
    for accurate object detection and semantic segmentation,” in Proc. CVPR, pp. 580–587,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] R. Girshick, “Fast r-cnn,” in Proc. ICCV, pp. 1440–1448, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in Proc. NIPS, pp. 91–99, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] P. Zhu, L. Wen, X. Bian, H. Ling, and Q. Hu, “Vision meets drones: A challenge,”
    arXiv preprint arXiv:1804.07437, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Z. Deng, H. Sun, S. Zhou, J. Zhao, L. Lei, and H. Zou, “Multi-scale object
    detection in remote sensing imagery with convolutional neural networks,” ISPRS
    J. Photogramm. Remote Sens., vol. 145, pp. 3–22, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] X. Wu, D. Hong, P. Ghamisi, W. Li, and R. Tao, “Msri-ccf: Multi-scale
    and rotation-insensitive convolutional channel features for geospatial object
    detection,” Remote Sens., vol. 10, no. 12, p. 1990, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C. Fu, and A. Berg,
    “SSD: Single shot multibox detector,” in Proc. ECCV, pp. 21–37, Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] X. Luo, X. Tian, H. Zhang, W. Hou, G. Leng, W. Xu, H. Jia, X. He, M. Wang,
    and J. Zhang, “Fast automatic vehicle detection in uav images using convolutional
    neural networks,” Remote Sens., vol. 12, no. 12, p. 1994, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi, “You only look once:
    Unified, real-time object detection,” in Proc. CVPR, pp. 779–788, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “Mobilenets: Efficient convolutional neural networks for mobile vision
    applications,” arXiv preprint arXiv:1704.04861, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Redmon and A. Farhadi, “Yolov3: An incremental improvement,” arXiv
    preprint arXiv:1804.02767, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Mehta, M. Rastegari, L. Shapiro, and H. Hajishirzi, “Espnetv2: A light-weight,
    power efficient, and general purpose convolutional neural network,” in Proc. CVPR,
    pp. 9190–9200, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] X. H. Liao and C. H. Zhou, Light- small UAV remote sensing development
    report. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] D. Hong, N. Yokoya, J. Chanussot, and X. X. Zhu, “CoSpace: Common subspace
    learning from hyperspectral-multispectral correspondences,” IEEE Trans. Geosci.
    Remote Sens., vol. 57, no. 7, pp. 4349–4359, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] D. Hong, N. Yokoya, J. Chanussot, and X. X. Zhu, “An augmented linear
    mixing model to address spectral variability for hyperspectral unmixing,” IEEE
    Trans. Image Process., vol. 28, no. 4, pp. 1923–1938, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] T. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and L. Zitnick, “Microsoft coco: Common objects in context,” in Proc. ECCV, pp. 740–755,
    Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. Han, D. Zhang, G. Cheng, N. Liu, and D. Xu, “Advanced deep-learning
    techniques for salient and category-specific object detection: a survey,” IEEE
    Signal Process. Mag., vol. 35, no. 1, pp. 84–100, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] K. Li, G. Wan, G. Cheng, L. Meng, and J. Han, “Object detection in optical
    remote sensing images: A survey and a new benchmark,” ISPRS J. Photogramm. Remote
    Sens., vol. 159, pp. 296–307, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] T. Z. Xiang, G. S. Xia, and L. Zhang, “Mini-uav-based remote sensing:
    Techniques, applications and prospectives,” IEEE GEOSC REM SEN M., vol. 7, no. 3,
    pp. 29–63, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] H. Yao, R. Qin, and X. Chen, “Unmanned aerial vehicle for remote sensing
    applications—a review,” Remote Sens, vol. 11, no. 12, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] N. Yin, R. Liu, B. Zeng, and N. Liu, “A review: Uav-based remote sensing,”
    in Proc. IOP, vol. 490, p. 062014, IOP Publishing, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] L. P. Osco, J. M. Junior, A. P. M. Ramos, L. A. d. C. Jorge, S. N. Fatholahi,
    J. d. A. Silva, E. T. Matsubara, H. Pistori, W. N. Gonçalves, and J. Li, “A review
    on deep learning in uav remote sensing,” arXiv preprint arXiv:2101.10861, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] P. Mittal, A. Sharma, and R. Singh, “Deep learning-based object detection
    in low-altitude uav datasets: A survey,” Image Vis Comput, p. 104046, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Srivastava, S. Narayan, and S. Mittal, “A survey of deep learning techniques
    for vehicle detection from uav images,” J. Syst. Archit., 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Ayalew and Pooja, “A review on object detection from unmanned aerial
    vehicle using cnn,” IJARIIT, vol. 5, pp. 241–243, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] D. Cazzato, C. Cimarelli, J. L. Sanchez-Lopez, H. Voos, and M. Leo, “A
    survey of computer vision methods for 2d object detection from unmanned aerial
    vehicles,” Journal of Imaging, vol. 6, no. 8, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Ammar, A. Koubaa, M. Ahmed, A. Saad, and B. Benjdira, “Vehicle detection
    from aerial images using deep learning: A comparative study,” Electronics, vol. 10,
    no. 7, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Hao, Y. Zhou, G. Zhang, Q. Lv, and Q. Wu, “A review of target tracking
    algorithm based on uav,” in Proc. CBS, pp. 328–333, IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] G. Pajares, “Overview and current status of remote sensing applications
    based on unmanned aerial vehicles (UAVs),” Photogramm. Eng. Remote Sens., vol. 81,
    no. 4, pp. 281–330, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] T. Adão, J. Hruška, L. Pádua, J. Bessa, E. Peres, R. Morais, and J. Sousa,
    “Hyperspectral imaging: A review on uav-based sensors, data processing and applications
    for agriculture and forestry,” Remote Sens,, vol. 9, no. 11, p. 1110, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Chen, Y. Zhang, J. Xin, G. Wang, L. Mu, Y. Yi, H. Liu, and D. Liu,
    “UAV image-based forest fire detection approach using convolutional neural network,”
    in Proc. ICIEA, pp. 2118–2123, June 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] T. Xiang, G. Xia, and L. Zhang, “Mini-UAV-based remote sensing: techniques,
    applications and prospectives,” arXiv preprint arXiv:1812.07770, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] X. Chen, S. Xiang, C. Liu, and C. Pan, “Vehicle detection in satellite
    images by hybrid deep convolutional neural networks,” IEEE Geosci. Remote Sens.
    Lett., vol. 11, no. 10, pp. 1797–1801, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] X. Liu, C. Deng, J. Chanussot, D. Hong, and B. Zhao, “Stfnet: A two-stream
    convolutional neural network for spatiotemporal image fusion,” IEEE Trans. Geosci.
    Remote Sens., vol. 57, no. 9, pp. 6552–6564, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Z. Cai, Q. Fan, R. Feris, and N. Vasconcelos, “A unified multi-scale deep
    convolutional neural network for fast object detection,” in Proc. ECCV, pp. 354–370,
    Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] C. Chen, Y. Zhang, Q. Lv, S. Wei, X. Wang, X. Sun, and J. Dong, “Rrnet:
    A hybrid detector for object detection in drone-captured images,” in Proc. ICCV
    Workshops, pp. 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Q. Lin, Y. Ding, H. Xu, W. Lin, J. Li, and X. Xie, “Ecascade-rcnn: Enhanced
    cascade rcnn for multi-scale object detection in uav images,” in Proc. ICARA,
    pp. 268–272, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Z. Li, X. Liu, Y. Zhao, B. Liu, Z. Huang, and R. Hong, “A lightweight
    multi-scale aggregated model for detecting aerial images captured by uavs,” J.
    Vis. Commun. Image Represent., vol. 77, p. 103058, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] W. Zhang, C. Liu, F. Chang, and Y. Song, “Multi-scale and occlusion aware
    network for vehicle detection and segmentation on uav aerial images,” Remote Sens.,
    vol. 12, no. 11, p. 1760, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] F. Yang, H. Fan, P. Chu, E. Blasch, and H. Ling, “Clustered object detection
    in aerial images,” in Proc. ICCV, pp. 8311–8320, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] D. R. Pailla, V. Kollerathu, and S. S. Chennamsetty, “Object detection
    on aerial imagery using centernet,” arXiv preprint arXiv:1908.08244, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] J. Yang, X. Xie, and W. Yang, “Effective contexts for uav vehicle detection,”
    IEEE Access, vol. 7, pp. 85042–85054, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] C. Chen, W. Gong, Y. Chen, and W. Li, “Object detection in remote sensing
    images based on a scene-contextual feature pyramid network,” Remote Sens., vol. 11,
    no. 3, p. 339, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] R. Zhang, Z. Shao, X. Huang, J. Wang, and D. Li, “Object detection in
    uav images via global density fused convolutional network,” Remote Sens., vol. 12,
    no. 19, p. 3140, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] X. Zhang, E. Izquierdo, and K. Chandramouli, “Dense and small object detection
    in uav vision based on cascade network,” in Proc. ICCV Workshops, pp. 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Y. Liu, Z. Ding, Y. Cao, and M. Chang, “Multi-scale feature fusion uav
    image object detection method based on dilated convolution and attention mechanism,”
    in Proc. ICIT, pp. 125–132, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in Proc. CVPR, pp. 1492–1500, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] P. Zhang, Y. Zhong, and X. Li, “Slimyolov3: Narrower, faster and better
    for real-time uav applications,” in Proc. ICCV, pp. 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] X. Liang, J. Zhang, L. Zhuo, Y. Li, and Q. Tian, “Small object detection
    in unmanned aerial vehicle images using feature fusion and scaling-based single
    shot detector with spatial context analysis,” IEEE Trans. Circuits Syst. Video
    Technol., vol. 30, no. 6, pp. 1758–1770, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] H. Wang, Z. Wang, M. Jia, A. Li, T. Feng, W. Zhang, and L. Jiao, “Spatial
    attention for multi-scale feature refinement for object detection,” in Proc. ICCV
    Workshops, pp. 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Q. Wu and Y. Zhou, “Real-time object detection based on unmanned aerial
    vehicle,” in Proc. DDCLS, pp. 574–579, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Z. Wu, K. Suresh, P. Narayanan, H. Xu, H. Kwon, and Z. Wang, “Delving
    into robust object detection from unmanned aerial vehicles: A deep nuisance disentanglement
    approach,” in Proc. ICCV, pp. 1201–1210, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Z. Liu, G. Gao, L. Sun, and Z. Fang, “Hrdnet: high-resolution detection
    network for small objects,” arXiv preprint arXiv:2006.07607, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] B. M. Albaba and S. Ozer, “Synet: An ensemble network for object detection
    in uav images,” arXiv preprint arXiv:2012.12991, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] M. Li, X. Zhao, J. Li, and L. Nan, “Comnet: Combinational neural network
    for object detection in uav-borne thermal images,” IEEE Trans. Geosci. Remote.
    Sens., 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Liu, F. Yang, and P. Hu, “Small-object detection in uav-captured images
    via multi-branch parallel feature pyramid networks,” IEEE Access, vol. 8, pp. 145740–145750,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] J. Cao, H. Cholakkal, R. M. Anwer, F. S. Khan, Y. Pang, and L. Shao, “D2det:
    Towards high quality object detection and instance segmentation,” in Proc. CVPR,
    pp. 11485–11494, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Z. Zhang, Y. Liu, T. Liu, Z. Lin, and S. Wang, “Dagn: A real-time uav
    remote sensing image vehicle detection framework,” IEEE Geosci. Remote Sens. Lett.,
    vol. 17, no. 11, pp. 1884–1888, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] C. YuanQiang, D. Du, L. Zhang, L. Wen, W. Wang, Y. Wu, and S. Lyu, “Guided
    attention network for object detection and counting on drones,” in Proc. ACM MM,
    pp. 709–717, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] A. Jadhav, P. Mukherjee, V. Kaushik, and B. Lall, “Aerial multi-object
    tracking by detection using deep association networks,” in Proc. NCC, pp. 1–6,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] J. Zhang, X. Liang, M. Wang, L. Yang, and L. Zhuo, “Coarse-to-fine object
    detection in unmanned aerial vehicle imagery using lightweight convolutional neural
    network and deep motion saliency,” Neurocomputing, vol. 398, pp. 555–565, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] G. Tian, J. Liu, and W. Yang, “A dual neural network for object detection
    in uav images,” Neurocomputing, vol. 443, pp. 292–301, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Y. Youssef and M. Elshenawy, “Automatic vehicle counting and tracking
    in aerial video feeds using cascade region-based convolutional neural networks
    and feature pyramid networks,” Transp. Res. Rec., p. 0361198121997833, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] W. Yu, T. Yang, and C. Chen, “Towards resolving the challenge of long-tail
    distribution in uav images for object detection,” in Proc. WACV, pp. 3258–3267,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] J. Li, X. Liang, Y. Wei, T. Xu, J. Feng, and S. Yan, “Perceptual generative
    adversarial networks for small object detection,” in Proc. CVPR, pp. 1222–1230,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] X. Hu, X. Xu, Y. Xiao, H. Chen, S. He, J. Qin, and P. Heng, “Sinet: A
    scale-insensitive convolutional neural network for fast vehicle detection,” IEEE
    Trans. Intell. Transp. Syst., vol. 20, no. 3, pp. 1010–1019, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Z. Tang, X. Liu, G. Shen, and B. Yang, “Penet: Object detection using
    points estimation in aerial images,” arXiv preprint arXiv:2001.08247, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] V. Ruzicka and F. Franchetti, “Fast and accurate object detection in high
    resolution 4k and 8k video using gpus,” in Proc. HPEC, pp. 1–7, IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] G. Plastiras, C. Kyrkou, and T. Theocharides, “Efficient convnet-based
    object detection for unmanned aerial vehicles by selective tile processing,” in
    Proc. ICDSC, pp. 1–6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] O. Unel, B. Ozkalayci, and C. Cigla, “The power of tiling for small object
    detection,” in Proc. CVPR Workshops, June 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] M. L. Mekhalfi, M. B. Bejiga, D. Soresina, F. Melgani, and B. Demir, “Capsule
    networks for object detection in uav imagery,” Remote Sens., vol. 11, no. 14,
    p. 1694, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] G. Cheng, P. Zhou, and J. Han, “Learning rotation-invariant convolutional
    neural networks for object detection in vhr optical remote sensing images,” IEEE
    Trans. Geosci. Remote Sens., vol. 54, no. 12, pp. 7405–7415, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] D. Laptev, N. Savinov, J. Buhmann, and M. Pollefeys, “Ti-pooling: transformation-invariant
    pooling for feature learning in convolutional neural networks,” in Proc. CVPR,
    pp. 289–297, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] G. Cheng, P. Zhou, and J. Han, “Rifd-cnn: Rotation-invariant and fisher
    discriminative convolutional neural networks for object detection,” in Proc. CVPR,
    pp. 2884–2893, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] G. Cheng, J. Han, P. Zhou, and D. Xu, “Learning rotation-invariant and
    fisher discriminative convolutional neural networks for object detection,” IEEE
    Trans. Image Process, vol. 28, no. 1, pp. 265–278, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Z. Liu, H. Wang, L. Weng, and Y. Yang, “Ship rotated bounding box space
    for ship extraction from high-resolution optical satellite images with complex
    backgrounds,” IEEE Geosci. Remote Sens.Lett., vol. 13, no. 8, pp. 1074–1078, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Z. Liu, J. Hu, L. Weng, and Y. Yang, “Rotated region based cnn for ship
    detection,” in Proc. ICIP, pp. 900–904, IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] D. Marcos, M. Volpi, and D. Tuia, “Learning rotation invariant convolutional
    filters for texture classification,” in Proc. ICPR, pp. 2012–2017, IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] K. Li, G. Cheng, S. Bu, and X. You, “Rotation-insensitive and context-augmented
    object detection in remote sensing images,” IEEE Trans. Geosci. Remote Sens.,
    vol. 56, no. 4, pp. 2337–2348, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] K. Fu, Z. Chang, Y. Zhang, G. Xu, K. Zhang, and X. Sun, “Rotation-aware
    and multi-scale convolutional neural network for object detection in remote sensing
    images,” ISPRS J. Photogramm. Remote Sens., vol. 161, pp. 294–308, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Y. Yu, H. Guan, D. Li, T. Gu, E. Tang, and A. Li, “Orientation guided
    anchoring for geospatial object detection from remote sensing imagery,” ISPRS
    J. Photogramm. Remote Sens., vol. 160, pp. 67–82, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] M. Liao, Z. Zhu, B. Shi, G.-s. Xia, and X. Bai, “Rotation-sensitive regression
    for oriented scene text detection,” in Proc. CVPR, pp. 5909–5918, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] M. Liao, B. Shi, and X. Bai, “Textboxes$++$: A single-shot oriented scene
    text detector,” IEEE Trans. Image Process., vol. 27, no. 8, pp. 3676–3690, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Y. Jiang, X. Zhu, X. Wang, S. Yang, W. Li, H. Wang, P. Fu, and Z. Luo,
    “R 2 cnn: Rotational region cnn for arbitrarily-oriented scene text detection,”
    in Proc. ICPR, pp. 3610–3615, IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J. Ma, W. Shao, H. Ye, L. Wang, H. Wang, Y. Zheng, and X. Xue, “Arbitrary-oriented
    scene text detection via rotation proposals,” IEEE Trans. Multimedia, vol. 20,
    no. 11, pp. 3111–3122, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] G.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo,
    and L. Zhang, “Dota: A large-scale dataset for object detection in aerial images,”
    in Proc. CVPR, pp. 3974–3983, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Y. Li, Q. Huang, X. Pei, L. Jiao, and R. Shang, “Radet: Refine feature
    pyramid network and multi-layer attention network for arbitrary-oriented object
    detection of remote sensing images,” Remote Sens., vol. 12, no. 3, p. 389, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Y. Zhou, Q. Ye, Q. Qiu, and J. Jiao, “Oriented response networks,” in
    Proc. CVPR, pp. 519–528, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] C. Esteves, C. Allen-Blanchette, X. Zhou, and K. Daniilidis, “Polar transformer
    networks,” arXiv preprint arXiv:1709.01889, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] K. S. Tai, P. Bailis, and G. Valiant, “Equivariant transformer networks,”
    arXiv preprint arXiv:1901.11399, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] K. Zhou, Z. Zhang, C. Gao, and J. Liu, “Rotated feature network for multiorientation
    object detection of remote-sensing images,” IEEE Geosci. Remote Sensing Lett.,
    pp. 1–5, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] U. Schmidt and S. Roth, “Learning rotation-aware features: From invariant
    priors to equivariant descriptors,” in Proc. CVPR, pp. 2050–2057, IEEE, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] X. Wu, D. Hong, J. Tian, J. Chanussot, W. Li, and R. Tao, “ORSIm Detector:
    A novel object detection framework in optical remote sensing imagery using spatial-frequency
    channel features,” IEEE Trans. Geosci. Remote Sens., vol. 57, no. 7, pp. 5146–5158,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] R. J. Wang, X. Li, and C. X. Ling, “Pelee: A real-time object detection
    system on mobile devices,” in Proc. NIPS, pp. 1963–1972, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. Yang, G. Xie, and Y. Qu, “Real-time detection of aircraft objects
    in remote sensing images based on improved yolov4,” in Proc. IAEAC, vol. 5, pp. 1156–1164,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] X. Wang, W. Li, W. Guo, and K. Cao, “Spb-yolo: An efficient real-time
    detector for unmanned aerial vehicle images,” in Prof. ICAIIC, pp. 099–104, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Z. Pi, Y. Lian, X. Chen, Y. Wu, Y. Li, and L. Jiao, “A novel spatial
    and temporal context-aware approach for drone-based video object detection,” in
    Proc. ICCV Workshops, Oct 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] T. Wang, J. Xiong, X. Xu, and Y. Shi, “Scnn: a general distribution based
    statistical convolutional neural network with application to video object detection,”
    in Proc. AAAI, vol. 33, pp. 5321–5328, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] P. Nousi, I. Mademlis, I. Karakostas, A. Tefas, and I. Pitas, “Embedded
    uav real-time visual object detection and tracking,” in Proc. RCAR, pp. 708–713,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] K. M. Abughalieh, B. H. Sababha, and N. A. Rawashdeh, “A video-based
    object detection and tracking system for weight sensitive uavs,” Multimed. Tools.
    Appl., vol. 78, no. 7, pp. 9149–9167, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Y. Zhang, L. Shen, X. Wang, and H.-M. Hu, “Drone video object detection
    using convolutional neural networks with time domain motion features,” in Proc.
    MIPR, pp. 153–156, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Mandal, L. K. Kumar, and S. K. Vipparthi, “Mor-uav: A benchmark dataset
    and baselines for moving object recognition in uav videos,” in Proc. ACM MM, pp. 2626–2635,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] H. Xie and H. Shin, “Two-stream small-scale pedestrian detection network
    with feature aggregation for drone-view videos,” Multidim. Syst. Sign. P., pp. 1–17,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] B. Bosquet, M. Mucientes, and V. M. Brea, “Stdnet-st: Spatio-temporal
    convnet for small object detection,” Pattern Recognit., p. 107929, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] K. Kang, W. Ouyang, H. Li, and X. Wang, “Object detection from video
    tubelets with convolutional neural networks,” in Proc. CVPR, pp. 817–825, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] K. Kang, H. Li, J. Yan, X. Zeng, B. Yang, T. Xiao, C. Zhang, Z. Wang,
    R. Wang, X. Wang, et al., “T-cnn: Tubelets with convolutional neural networks
    for object detection from videos,” IEEE Trans. Circuits Syst. Video Technol.,
    vol. 28, no. 10, pp. 2896–2907, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] W. Han, P. Khorrami, T. L. Paine, P. Ramachandran, M. Babaeizadeh, H. Shi,
    J. Li, S. Yan, and T. S. Huang, “Seq-nms for video object detection,” arXiv preprint
    arXiv:1602.08465, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Q. Zhao, Y. Wang, T. Sheng, and Z. Tang, “Comprehensive feature enhancement
    module for single-shot object detector,” in Proc. ACCV, pp. 325–340, Springer,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] M. Danelljan, G. Bhat, F. Shahbaz Khan, and M. Felsberg, “Eco: Efficient
    convolution operators for tracking,” in Proc. CVPR, pp. 6638–6646, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] K. Sun, B. Xiao, D. Liu, and J. Wang, “Deep high-resolution representation
    learning for human pose estimation,” in Proc. CVPR, pp. 5693–5703, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Z. Cai and N. Vasconcelos, “Cascade r-cnn: Delving into high quality
    object detection,” in Proc. CVPR, pp. 6154–6162, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] X. Zhou, D. Wang, and P. Krähenbühl, “Objects as points,” arXiv preprint
    arXiv:1904.07850, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Dollár, “Focal loss for
    dense object detection,” in Proc. ICCV, pp. 2980–2988, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] T.-Y. Lin, P. Dollár, R. Girshick, K. He, B. Hariharan, and S. Belongie,
    “Feature pyramid networks for object detection,” in Proc. CVPR, pp. 2117–2125,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] D. Sun, X. Yang, M.-Y. Liu, and J. Kautz, “Pwc-net: Cnns for optical
    flow using pyramid, warping, and cost volume,” in Prof. CVPR, pp. 8934–8943, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] A. Ranjan and M. J. Black, “Optical flow estimation using a spatial pyramid
    network,” in Prof. CVPR, pp. 2720–2729, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] X. Zhu, Y. Xiong, J. Dai, L. Yuan, and Y. Wei, “Deep feature flow for
    video recognition,” in Proc. CVPR, pp. 2349–2358, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] X. Zhu, Y. Wang, J. Dai, L. Yuan, and Y. Wei, “Flow-guided feature aggregation
    for video object detection,” in Proc. ICCV, pp. 408–417, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] C. Hetang, H. Qin, S. Liu, and J. Yan, “Impression network for video
    object detection,” arXiv preprint arXiv:1712.05896, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] X. Zhu, J. Dai, L. Yuan, and Y. Wei, “Towards high performance video
    object detection,” in Proc. CVPR, pp. 7210–7218, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] X. Zhu, J. Dai, X. Zhu, Y. Wei, and L. Yuan, “Towards high performance
    video object detection for mobiles,” arXiv preprint arXiv:1804.05830, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] R. Hang, Q. Liu, D. Hong, and P. Ghamisi, “Cascaded recurrent neural
    networks for hyperspectral image classification,” IEEE Trans. Geosci. Remote Sens.,
    vol. 57, no. 8, pp. 5384–5394, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Y. Lu, C. Lu, and C.-K. Tang, “Online video object detection using association
    lstm,” in Proc. ICCV, pp. 2344–2352, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M. Liu, M. Zhu, M. White, Y. Li, and D. Kalenichenko, “Looking fast and
    slow: Memory-guided mobile video object detection,” arXiv preprint arXiv:1903.10172,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] M. Liu and M. Zhu, “Mobile video object detection with temporally-aware
    feature maps,” in Proc. CVPR, pp. 5686–5695, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Z. Jiang, P. Gao, C. Guo, Q. Zhang, S. Xiang, and C. Pan, “Video object
    detection with locally-weighted deformable neighbors,” in Proc. AAAI, vol. 33,
    pp. 8529–8536, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] S. Tripathi, Z. C. Lipton, S. Belongie, and T. Nguyen, “Context matters:
    Refining object detection in video with recurrent neural networks,” arXiv preprint
    arXiv:1607.04648, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] F. Xiao and Y. Jae Lee, “Video object detection with an aligned spatial-temporal
    memory,” in Proc. ECCV, pp. 485–501, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] H. Luo, W. Xie, X. Wang, and W. Zeng, “Detect or track: Towards cost-effective
    video object detection/tracking,” in Proc. AAAI, vol. 33, pp. 8803–8810, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] N. Wojke, A. Bewley, and D. Paulus, “Simple online and realtime tracking
    with a deep association metric,” in Proc. ICIP, pp. 3645–3649, IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] N. M. Al-Shakarji, F. Bunyak, G. Seetharaman, and K. Palaniappan, “Multi-object
    tracking cascade with multi-step data association and occlusion handling,” in
    Proc. AVSS, pp. 1–6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Y. Zhou, T. Rui, Y. Li, and X. Zuo, “A uav patrol system using panoramic
    stitching and object detection,” Comput. Electr. Eng., vol. 80, p. 106473, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] J. Wang, S. Simeonova, and M. Shahbazi, “Orientation- and scale-invariant
    multi-vehicle detection and tracking from unmanned aerial videos,” Remote Sens,
    vol. 11, no. 18, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] W. Li, J. Mu, and G. Liu, “Multiple object tracking with motion and appearance
    cues,” in Proc. ICCV Workshops, pp. 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] H. Zhang, G. Wang, Z. Lei, and J.-N. Hwang, “Eye in the sky: Drone-based
    object tracking and 3d localization,” in Proc. ACM MM, pp. 899–907, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] S. Pan, Z. Tong, Y. Zhao, Z. Zhao, F. Su, and B. Zhuang, “Multi-object
    tracking hierarchically in visual data taken from drones,” in Proc. ICCV Workshops,
    pp. 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] T. Yang, D. Li, Y. Bai, F. Zhang, S. Li, M. Wang, Z. Zhang, and J. Li,
    “Multiple-object-tracking algorithm based on dense trajectory voting in aerial
    videos,” Remote Sens., vol. 11, no. 19, p. 2278, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] H. Ardo and M. Nilsson, “Multi target tracking from drones by learning
    from generalized graph differences,” in Proc. ICCV Workshops, pp. 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. M. Marvasti-Zadeh, J. Khaghani, H. Ghanei-Yakhdan, S. Kasaei, and
    L. Cheng, “Comet: context-aware iou-guided network for small object tracking,”
    in Proc. ACCV, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] H. Yu, G. Li, W. Zhang, H. Yao, and Q. Huang, “Self-balance motion and
    appearance model for multi-object tracking in uav,” in Proc. ACM MM Asia, pp. 1–6,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] P. Bergmann, T. Meinhardt, and L. Leal-Taixe, “Tracking without bells
    and whistles,” in Proc. ICCV, pp. 941–951, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] H. Yu, G. Li, L. Su, B. Zhong, H. Yao, and Q. Huang, “Conditional gan
    based individual and global motion fusion for multiple object tracking in uav
    videos,” Pattern Recognit. Lett., vol. 131, pp. 219–226, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] S. Kapania, D. Saini, S. Goyal, N. Thakur, R. Jain, and P. Nagrath, “Multi
    object tracking with uavs using deep sort and yolov3 retinanet detection framework,”
    in Proc. ACM. AIMS, pp. 1–6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] D. Stadler, L. W. Sommer, and J. Beyerer, “Pas tracker: position-, appearance-and
    size-aware multi-object tracking in drone videos,” in Proc. ECCV, pp. 604–620,
    Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] H. U. Dike and Y. Zhou, “A robust quadruplet and faster region-based
    cnn for uav video-based multiple object tracking in crowded environment,” Electronics,
    vol. 10, no. 7, p. 795, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] W. Huang, X. Zhou, M. Dong, and H. Xu, “Multiple objects tracking in
    the uav system based on hierarchical deep high-resolution network,” Multimed.
    Tools. Appl., pp. 1–19, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] S.-H. Bae and K.-J. Yoon, “Robust online multi-object tracking based
    on tracklet confidence and online discriminative appearance learning,” in Proc.
    CVPR, pp. 1218–1225, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Y. Xiang, A. Alahi, and S. Savarese, “Learning to track: Online multi-object
    tracking by decision making,” in Proc. ICCV, pp. 4705–4713, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] H. Pirsiavash, D. Ramanan, and C. C. Fowlkes, “Globally-optimal greedy
    algorithms for tracking a variable number of objects,” in Proc. CVPR, pp. 1201–1208,
    IEEE, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] C. Dicle, O. I. Camps, and M. Sznaier, “The way they move: Tracking multiple
    targets with similar appearance,” in Proc. ICCV, pp. 2304–2311, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] E. Bochinski, V. Eiselein, and T. Sikora, “High-speed tracking-by-detection
    without using image information,” in Proc. AVSS, pp. 1–6, IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] A. Bewley, Z. Ge, L. Ott, F. Ramos, and B. Upcroft, “Simple online and
    realtime tracking,” in Proc. ICIP, pp. 3464–3468, IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] A. Milan, S. Roth, and K. Schindler, “Continuous energy minimization
    for multitarget tracking,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 36, no. 1,
    pp. 58–72, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] S. Schulter, P. Vernaza, W. Choi, and M. Chandraker, “Deep network flow
    for multi-object tracking,” in Proc. CVPR, pp. 6951–6960, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] J. Son, M. Baek, M. Cho, and B. Han, “Multi-object tracking with quadruplet
    convolutional neural networks,” in Proc. CVPR, pp. 5620–5629, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] C. Feichtenhofer, A. Pinz, and A. Zisserman, “Detect to track and track
    to detect,” in Proc. ICCV, pp. 3038–3046, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] A. Jadhav, P. Mukherjee, V. Kaushik, and B. Lall, “Aerial multi-object
    tracking by detection using deep association networks,” arXiv preprint arXiv:1909.01547,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] S. Chopra, R. Hadsell, and Y. LeCun, “Learning a similarity metric discriminatively,
    with application to face verification,” in Proc. CVPR, vol. 1, pp. 539–546, IEEE,
    2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] J. Lee, S. Kim, and B. C. Ko, “Online multiple object tracking using
    rule distillated siamese random forest,” IEEE Access, vol. 8, pp. 182828–182841,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] J. Jin, X. Li, X. Li, and S. Guan, “Online multi-object tracking with
    siamese network and optical flow,” in Proc. ICIVC, pp. 193–198, IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] B. Shuai, A. G. Berneshawi, D. Modolo, and J. Tighe, “Multi-object tracking
    with siamese track-rcnn,” arXiv preprint arXiv:2004.07786, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] S.-H. Bae and K.-J. Yoon, “Confidence-based data association and discriminative
    deep appearance learning for robust online multi-object tracking,” IEEE Trans.
    Pattern Anal. Mach. Intell., vol. 40, no. 3, pp. 595–610, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] L. Leal-Taixé, C. Canton-Ferrer, and K. Schindler, “Learning by tracking:
    Siamese cnn for robust target association,” in Proc. CVPR Workshops, pp. 33–40,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] B. Wang, L. Wang, B. Shuai, Z. Zuo, T. Liu, K. Luk Chan, and G. Wang,
    “Joint learning of convolutional neural networks and temporally constrained metrics
    for tracklet association,” in Proc. CVPR Workshops, pp. 1–8, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] A. Li, L. Luo, and S. Tang, “Real-time tracking of vehicles with siamese
    network and backward prediction,” in Proc. ICME, pp. 1–6, IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] X. Yan, X. Wu, I. A. Kakadiaris, and S. K. Shah, “To track or to detect?
    an ensemble framework for optimal selection,” in Proc. ECCV, pp. 594–607, Springer,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Q. Chu, W. Ouyang, H. Li, X. Wang, B. Liu, and N. Yu, “Online multi-object
    tracking using cnn-based single object tracker with spatial-temporal attention
    mechanism,” in Proc. ICCV, pp. 4836–4845, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] A. Sadeghian, A. Alahi, and S. Savarese, “Tracking the untrackable: Learning
    to track multiple cues with long-term dependencies,” in Proc. ICCV, pp. 300–311,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] L. Bertinetto, J. Valmadre, S. Golodetz, O. Miksik, and P. H. Torr, “Staple:
    Complementary learners for real-time tracking,” in Proc. CVPR, pp. 1401–1409,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] H. Fan and H. Ling, “Parallel tracking and verifying: A framework for
    real-time and high accuracy visual tracking,” in Proc. ICCV, pp. 5486–5494, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] H. Fan and H. Ling, “Sanet: Structure-aware network for visual tracking,”
    in Proc. CVPR workshops, pp. 42–49, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] P. Chu, H. Fan, C. C. Tan, and H. Ling, “Online multi-object tracking
    with instance-aware tracker and dynamic model refreshment,” in Proc. WACV, pp. 161–170,
    IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] W. Feng, Z. Hu, W. Wu, J. Yan, and W. Ouyang, “Multi-object tracking
    with multiple cues and switcher-aware classification,” arXiv preprint arXiv:1901.06129,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] B. Li, J. Yan, W. Wu, Z. Zhu, and X. Hu, “High performance visual tracking
    with siamese region proposal network,” in Proc. CVPR, pp. 8971–8980, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] J. Zhu, H. Yang, N. Liu, M. Kim, W. Zhang, and M.-H. Yang, “Online multi-object
    tracking with dual matching attention networks,” in Proc. ECCV, pp. 366–382, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] A. Milan, S. H. Rezatofighi, A. Dick, I. Reid, and K. Schindler, “Online
    multi-target tracking using recurrent neural networks,” in Proc. AAAI, pp. 4225–4232,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Q. Li, X. Zhao, and K. Huang, “Learning temporally correlated representations
    using lstms for visual tracking,” in Proc. ICIP, pp. 1614–1618, IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] A. Alahi, K. Goel, V. Ramanathan, A. Robicquet, L. Fei-Fei, and S. Savarese,
    “Social lstm: Human trajectory prediction in crowded spaces,” in Proc. CVPR, pp. 961–971,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Y. Liang and Y. Zhou, “Lstm multiple object tracker combining multiple
    cues,” in Proc. ICIP, pp. 2351–2355, IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] C. Kim, F. Li, and J. M. Rehg, “Multi-object tracking with neural gating
    using bilinear lstm,” in Proc. ECCV, pp. 200–215, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] H. Meng-Ru, L. Yen-Liang, and H. Winston, “Drone-based object counting
    by spatially regularized regional proposal networks,” in Proc. ICCV, IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] D. Du, Y. Qi, H. Yu, Y. Yang, K. Duan, G. Li, W. Zhang, Q. Huang, and
    Q. Tian, “The unmanned aerial vehicle benchmark: Object detection and tracking,”
    in Proc. ECCV, pp. 370–386, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] X. Xu, X. Zhang, B. Yu, X. S. Hu, C. Rowen, J. Hu, and Y. Shi, “Dac-sdc
    low power object detection challenge for uav applications,” IEEE Trans. Pattern
    Anal. Mach. Intell., 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] P. Zhu, L. Wen, D. Du, X. Bian, Q. Hu, and H. Ling, “Vision meets drones:
    Past, present and future,” arXiv preprint arXiv:2001.06303, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] P. Zhu, Y. Sun, L. Wen, Y. Feng, and Q. Hu, “Drone based rgbt vehicle
    detection and counting: A challenge,” arXiv preprint arXiv:2003.02437, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] I. Bozcan and E. Kayacan, “Au-air: A multi-modal unmanned aerial vehicle
    dataset for low altitude traffic surveillance,” in Proc. ICRA, pp. 8504–8510,
    IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] E. Bondi, R. Jain, P. Aggrawal, S. Anand, R. Hannaford, A. Kapoor, J. Piavis,
    S. Shah, L. Joppa, B. Dilkina, et al., “Birdsai: A dataset for detection and tracking
    in aerial thermal infrared videos,” in Proc. WACV, pp. 1747–1756, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] H. Zhang, M. Sun, Q. Li, L. Liu, M. Liu, and Y. Ji, “An empirical study
    of multi-scale object detection in high resolution uav images,” Neurocomputing,
    vol. 421, pp. 173–182, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] M. Barekatain, M. Martí, H. Shih, S. Murray, K. Nakayama, Y. Matsuo,
    and H. Prendinger, “Okutama-action: An aerial view video dataset for concurrent
    human action detection,” in Proc. CVPR Workshops, pp. 28–35, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] M. Mueller, N. Smith, and B. Ghanem, “A benchmark and simulator for uav
    tracking,” in Proc. ECCV, pp. 445–461, Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] S. Li and D. Yeung, “Visual object tracking for unmanned aerial vehicles:
    A benchmark and new motion models,” in Proc. AAAI, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] A. Robicquet, A. Sadeghian, A. Alahi, and S. Savarese, “Learning social
    etiquette: Human trajectory understanding in crowded scenes,” in Proc. ECCV, vol. 9912,
    pp. 549–565, Sep 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] W. Zhang, C. Liu, F. Chang, and Y. Song, “Multi-scale and occlusion aware
    network for vehicle detection and segmentation on uav aerial images,” Remote Sens.,
    vol. 12, no. 11, p. 1760, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] H. Law and J. Deng, “Cornernet: Detecting objects as paired keypoints,”
    in Proc. ECCV, pp. 734–750, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] S. Zhang, L. Wen, X. Bian, Z. Lei, and S. Z. Li, “Single-shot refinement
    neural network for object detection,” in Proc. CVPR, pp. 4203–4212, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Detnet: A backbone
    network for object detection,” arXiv preprint arXiv:1804.06215, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Z. Li, C. Peng, G. Yu, X. Zhang, Y. Deng, and J. Sun, “Light-head r-cnn:
    In defense of two-stage object detector,” arXiv preprint arXiv:1711.07264, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] F. Yang, H. Fan, P. Chu, E. Blasch, and H. Ling, “Clustered object detection
    in aerial images,” in Proc. ICCV, pp. 8311–8320, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] C. Li, T. Yang, S. Zhu, C. Chen, and S. Guan, “Density map guided object
    detection in aerial images,” in Proc. CVPR Workshops, pp. 190–191, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] T. Kong, F. Sun, A. Yao, H. Liu, M. Lu, and Y. Chen, “Ron: Reverse connection
    with objectness prior networks for object detection,” in Proc. CVPR, pp. 5936–5944,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] P. Zhu, D. Du, L. Wen, X. Bian, H. Ling, Q. Hu, T. Peng, J. Zheng, X. Wang,
    Y. Zhang, et al., “Visdrone-vid2019: The vision meets drone object detection in
    video challenge results,” in Proc. ICCV Workshops, pp. 0–0, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Q. Zhao, T. Sheng, Y. Wang, F. Ni, and L. Cai, “Cfenet: An accurate and
    efficient single-shot object detector for autonomous driving,” arXiv preprint
    arXiv:1806.09790, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] P. Zhu, L. Wen, D. Du, X. Bian, H. Ling, Q. Hu, H. Wu, Q. Nie, H. Cheng,
    C. Liu, et al., “Visdrone-vdt2018: The vision meets drone video detection and
    tracking challenge results,” in Proc. ECCV, pp. 0–0, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] P. Adarsh, P. Rathi, and M. Kumar, “Yolo v3-tiny: Object detection and
    recognition using one stage improved model,” in Proc. ICACCS, pp. 687–694, IEEE,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] A. Geiger, M. Lauer, C. Wojek, C. Stiller, and R. Urtasun, “3d traffic
    scene understanding from movable platforms,” IEEE Trans. Pattern Anal. Mach. Intell.,
    vol. 36, no. 5, pp. 1012–1025, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] L. Wen, W. Li, J. Yan, Z. Lei, D. Yi, and S. Z. Li, “Multiple target
    tracking based on undirected hierarchical relation hypergraph,” in Proc. CVPR,
    pp. 1282–1289, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] N. M. Al-Shakarji, G. Seetharaman, F. Bunyak, and K. Palaniappan, “Robust
    multi-object tracking with semantic color correlation,” in Proc. AVSS, pp. 1–7,
    IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] A. Milan, S. H. Rezatofighi, A. Dick, I. Reid, and K. Schindler, “Online
    multi-target tracking using recurrent neural networks,” arXiv preprint arXiv:1604.03635,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] J. H. Yoon, M.-H. Yang, J. Lim, and K.-J. Yoon, “Bayesian multi-object
    tracking using motion context from multiple objects,” in Proc. WACV, pp. 33–40,
    IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] J. Deng, Z. Zhong, H. Huang, Y. Lan, Y. Han, and Y. Zhang, “Lightweight
    semantic segmentation network for real-time weed mapping using unmanned aerial
    vehicles,” Appl, vol. 10, no. 20, p. 7132, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/87a66d0fb93d97553797c90637f90347.png) | Xin Wu
    (S’19–M’20) received the M.Sc. degree in Computer Science and Technology from
    the College of Information Engineering, Qingdao University, Qingdao, China, in
    2014, the Ph.D. degree from the School of Information and Electronics, Beijing
    Institute of Technology (BIT), Beijing, China, in 2020. In 2018, she was a visiting
    student at the Photogrammetry and Image Analysis department of the Remote Sensing
    Technology Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany.
    She is currently a Postdoctoral researcher in the School of Information and Electronics,
    BIT, Beijing, China. Her research interests include signal / image processing,
    fractional Fourier transform, deep learning and their applications in biometrics
    and geospatial object detection. She was a recipient of the Jose Bioucas Dias
    award for recognizing the outstanding paper at the Workshop on Hyperspectral Imaging
    and Signal Processing: Evolution in Remote Sensing (WHISPERS) in 2021. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/4abd286bc978e90e89d1483831deffed.png) | Wei Li
    (S’11–M’13–SM’16) received the B.E.degree in telecommunications engineering from
    Xidian University, Xi’an, China, in 2007, the M.S. degree in information science
    and technology from Sun Yat-Sen University, Guangzhou, China, in 2009, and the
    Ph.D. degree in electrical and computer engineering from Mississippi State University,
    Starkville, MS, USA, in 2012. Subsequently, he spent 1 year as a Postdoctoral
    Researcher at the University of California, Davis, CA, USA. He is currently a
    professor with the School of Information and Electronics, Beijing Institute of
    Technology. His research interests include hyperspectral image analysis, pattern
    recognition, and data compression. He is currently serving as Associate Editor
    for the IEEE Transactions on Geoscience and Remote Sensing (TGRS), IEEE Journal
    of Selected Topics in Applied Earth Observations and Remote Sensing (JSTARS),
    and IEEE Signal Processing Letters (SPL). He has published more than 150 peer-reviewed
    articles and 100 conference papers totally cited by 7500 times (Google Scholar).
    He received the JSTARS Best Reviewer in 2016 and TGRS Best Reviewer award in 2020
    from IEEE Geoscience and Remote Sensing Society (GRSS), and the Outstanding Paper
    award at IEEE International Workshop on Hyperspectral Image and Signal Processing:
    Evolution in Remote Sensing (Whispers), 2019. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/b128cd56439fcf5ce9eb6982dd0207f5.png) | Danfeng
    Hong (S’16–M’19–SM’21) received the M.Sc. degree (summa cum laude) in computer
    vision from the College of Information Engineering, Qingdao University, Qingdao,
    China, in 2015, the Dr. -Ing degree (summa cum laude) from the Signal Processing
    in Earth Observation (SiPEO), Technical University of Munich (TUM), Munich, Germany,
    in 2019. Since 2015, he has been a Research Associate at the Remote Sensing Technology
    Institute (IMF), German Aerospace Center (DLR), Oberpfaffenhofen, Germany. He
    is currently a Research Scientist and leads a Spectral Vision Working Group at
    IMF, DLR. He is also an Adjunct Scientist at GIPSA-lab, Grenoble INP, CNRS, Univ.
    Grenoble Alpes, Grenoble, France. His research interests include signal / image
    processing and analysis, hyperspectral remote sensing, machine / deep learning,
    artificial intelligence, and their applications in Earth Vision. Dr. Hong is an
    Editorial Board Member of Remote Sensing and a Topical Associate Editor of the
    IEEE Transactions on Geoscience and Remote Sensing (TGRS). He was a recipient
    of the Best Reviewer Award of the IEEE TGRS in 2021 and the Jose Bioucas Dias
    award for recognizing the outstanding paper at the Workshop on Hyperspectral Imaging
    and Signal Processing: Evolution in Remote Sensing (WHISPERS) in 2021\. He is
    also a Leading Guest Editor of the International Journal of Applied Earth Observation
    and Geoinformation, the IEEE Journal of Selected Topics in Applied Earth Observations,
    and Remote Sensing. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/496bf313e9860058b44f716c14f12909.png) | Ran Tao
    (M’00–-SM’04) received the B.S. degree from the Electronic Engineering Institute
    of PLA, Hefei, China, in 1985, and the M.S. and Ph.D. degrees from the Harbin
    Institute of Technology, Harbin, China, in 1990 and 1993, respectively. In 2001,
    he was a Senior Visiting Scholar with the University of Michigan, Ann Arbor, MI,
    USA. He is currently a Professor with the School of Information and Electronics,
    Beijing Institute of Technology, Beijing, China. He has authored 3 books and more
    than 180 peer-reviewed journal articles. His current research interests include
    fractional signal and information processing with applications. Dr. Tao was the
    recipient of the National Science Foundation of China for Distinguished Young
    Scholars in 2006, and the First Prize of Science and Technology Progress in 2006
    and 2007, and the First Prize of Natural Science in 2013, both awarded by the
    Ministry of Education. He was a Distinguished Professor of the Changjiang Scholars
    Program in 2009\. He was a Chief Professor of the Program for Changjiang Scholars
    and Innovative Research Team in University from 2010 to 2012\. He has been a Chief
    Professor of the Creative Research Groups of the National Natural Science Foundation
    of China since 2014\. He has been awarded the Famous Teachers of Higher Education
    in Beijing in 2018\. He is currently the Vice Chair of the International Union
    of Radio Science China Council. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/6c8b049330bcf5816e7f86356a4615a1.png) | Qian Du
    (M’00–SM’05–F’18) received the Ph.D. degree in electrical engineering from the
    University of Maryland at Baltimore County, Baltimore, MD, USA, in 2000\. She
    is currently the Bobby Shackouls Professor with the Department of Electrical and
    Computer Engineering, Mississippi State University, MS, USA. Her research interests
    include hyperspectral remote sensing image analysis and applications, pattern
    classification, data compression, and neural networks. Dr. Du is a fellow of the
    SPIE-International Society for Optics and Photonics. She received the 2010 Best
    Reviewer Award from the IEEE Geoscience and Remote Sensing Society. She was the
    Co-Chair of the Data Fusion Technical Committee of the IEEE Geoscience and Remote
    Sensing Society from 2009 to 2013, and the Chair of the Remote Sensing and Mapping
    Technical Committee of the International Association for Pattern Recognition from
    2010 to 2014\. She has served as an Associate Editor of the IEEE JOURNAL OF SELECTED
    TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING, the Journal of Applied
    Remote Sensing, and the IEEE SIGNAL PROCESSING LETTERS. She is the Editor-in-Chief
    of the IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE
    SENSING from 2016 to 2020. |'
  prefs: []
  type: TYPE_TB
