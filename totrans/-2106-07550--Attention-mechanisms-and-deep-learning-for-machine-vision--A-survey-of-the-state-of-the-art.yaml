- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:54:06'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:54:06
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2106.07550] Attention mechanisms and deep learning for machine vision: A survey
    of the state of the art'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2106.07550] 注意机制与深度学习在机器视觉中的应用：最新研究概述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2106.07550](https://ar5iv.labs.arxiv.org/html/2106.07550)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2106.07550](https://ar5iv.labs.arxiv.org/html/2106.07550)
- en: ∎
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ∎
- en: '¹¹institutetext: Abdul Mueed Hafiz ²²institutetext: Dept of Electronics & Communication
    Engineering, Institute of Technology, University of Kashmir (Zakura Campus), Srinagar,
    J&K, 190006 India'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构：Abdul Mueed Hafiz ²²机构：电子与通信工程系，克什米尔大学（扎库拉校区），斯利那加，J&K，190006 印度
- en: 'Tel.: +91-7006474254'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 电话：+91-7006474254
- en: '²²email: mueedhafiz@uok.edu.in ³³institutetext: Shabir Ahmad Parah ⁴⁴institutetext:
    Department of Electronics and Instrumentation Technology, University of Kashmir
    (Main Campus), Srinagar, J&K, 190006 India ⁵⁵institutetext: Rouf Ul Alam Bhat
    ⁶⁶institutetext: Dept of Electronics & Communication Engineering, Institute of
    Technology, University of Kashmir (Zakura Campus), Srinagar, J&K, 190006 India'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ²²电子邮件：mueedhafiz@uok.edu.in ³³机构：Shabir Ahmad Parah ⁴⁴机构：电子与仪器技术系，克什米尔大学（主校区），斯利那加，J&K，190006
    印度 ⁵⁵机构：Rouf Ul Alam Bhat ⁶⁶机构：电子与通信工程系，克什米尔大学（扎库拉校区），斯利那加，J&K，190006 印度
- en: 'Attention mechanisms and deep learning for machine vision: A survey of the
    state of the art'
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 注意机制与深度学习在机器视觉中的应用：最新研究概述
- en: 'Abdul Mueed Hafiz    Shabir Ahmad Parah    Rouf Ul Alam Bhat(Received: date
    / Accepted: date)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Abdul Mueed Hafiz    Shabir Ahmad Parah    Rouf Ul Alam Bhat（收到：日期 / 接受：日期）
- en: Abstract
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the advent of state of the art nature-inspired pure attention based models
    i.e. transformers, and their success in natural language processing (NLP), their
    extension to machine vision (MV) tasks was inevitable and much felt. Subsequently,
    vision transformers (ViTs) were introduced which are giving quite a challenge
    to the established deep learning based machine vision techniques. However, pure
    attention based models/architectures like transformers require huge data, large
    training times and large computational resources. Some recent works suggest that
    combinations of these two varied fields can prove to build systems which have
    the advantages of both these fields. Accordingly, this state of the art survey
    paper is introduced which hopefully will help readers get useful information about
    this interesting and potential research area. A gentle introduction to attention
    mechanisms is given, followed by a discussion of the popular attention based deep
    architectures. Subsequently, the major categories of the intersection of attention
    mechanisms and deep learning for machine vision (MV) based are discussed. Afterwards,
    the major algorithms, issues and trends within the scope of the paper are discussed.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 随着基于自然启发的最先进纯注意力模型（即变换器）的出现，以及它们在自然语言处理（NLP）中的成功，它们向机器视觉（MV）任务的扩展是不可避免的并且非常必要。随后，视觉变换器（ViTs）被引入，这对现有的深度学习机器视觉技术提出了相当大的挑战。然而，像变换器这样的纯注意力模型/架构需要大量的数据、大的训练时间和大量的计算资源。一些近期的工作表明，这两个不同领域的结合可以构建具有这两个领域优点的系统。因此，本文介绍了这一最新研究领域的概述，希望能帮助读者获取关于这一有趣且潜力巨大的研究领域的有用信息。文章首先对注意机制进行了温和的介绍，然后讨论了流行的基于注意力的深度架构。接着，讨论了注意机制与深度学习在机器视觉（MV）中的交集的主要类别。之后，讨论了论文范围内的主要算法、问题和趋势。
- en: 'Keywords:'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Attention vision transformers CNNs deep learning machine vision
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 视觉 变换器 CNN 深度学习 机器视觉
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Recently attention-based mechanisms like transformers [tv_1](#bib.bib93) have
    been successfully applied to various machine vision tasks by using them as vision
    transformers (ViTs) [tv_11](#bib.bib20) in image recognition [tv_12](#bib.bib90)
    , object detection [tv_13](#bib.bib8) ; [tv_14](#bib.bib119) , segmentation[tv_15](#bib.bib112)
    , image super-resolution [tv_16](#bib.bib109) , video understanding [tv_17](#bib.bib86)
    ; [tv_18](#bib.bib26) , image generation [tv_19](#bib.bib10) , text-image synthesis
    [tv_20](#bib.bib75) and visual question answering [tv_21](#bib.bib87) ; [tv_22](#bib.bib85)
    , among others [tv_23](#bib.bib97) ; [tv_24](#bib.bib50) ; [tv_25](#bib.bib18)
    ; [tv_26](#bib.bib111) achieving at par as well as even better results as compared
    to the established CNN models [tv](#bib.bib45) . However, transformers have various
    issues like being ’data-hungry’ and requiring large training times. Deep learning
    [dl_main](#bib.bib54) ; [dl_book](#bib.bib27) ; [dl_review](#bib.bib82) based
    convolutional neural networks (CNNs) [cnn1](#bib.bib55) ; [cnn2](#bib.bib56) on
    the other hand do not have such problems significantly. Accordingly, techniques
    have emerged which are at the intersection of pure attention based models and
    the established pure CNNs which have best of the both features. Machine vision
    (MV) has also benefitted from this merger of the two important vision models viz.
    ViTs and CNNs. In the this section we will discuss the source of power of ViTs
    and transformers in general i.e. attention and its types [tv](#bib.bib45) briefly
    for the readers to have an idea of the new type of machine vision (MV) models
    i.e. ViTs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，像变压器这样的基于注意力的机制 [tv_1](#bib.bib93) 已成功应用于各种机器视觉任务，通过将它们作为视觉变压器（ViTs） [tv_11](#bib.bib20)
    进行图像识别 [tv_12](#bib.bib90)、目标检测 [tv_13](#bib.bib8)；[tv_14](#bib.bib119)、分割 [tv_15](#bib.bib112)、图像超分辨率
    [tv_16](#bib.bib109)、视频理解 [tv_17](#bib.bib86)；[tv_18](#bib.bib26)、图像生成 [tv_19](#bib.bib10)、文本-图像合成
    [tv_20](#bib.bib75) 和视觉问答 [tv_21](#bib.bib87)；[tv_22](#bib.bib85) 等，取得了与既有 CNN
    模型 [tv](#bib.bib45) 相当甚至更好的结果。然而，变压器存在各种问题，例如“数据需求量大”和训练时间长。另一方面，基于深度学习 [dl_main](#bib.bib54)；[dl_book](#bib.bib27)；[dl_review](#bib.bib82)
    的卷积神经网络（CNNs） [cnn1](#bib.bib55)；[cnn2](#bib.bib56) 则没有如此显著的问题。因此，出现了一些技术，这些技术位于纯注意力模型和纯
    CNN 之间的交集，结合了两者的最佳特性。机器视觉（MV）也从这两种重要视觉模型，即 ViTs 和 CNNs 的融合中受益。在本节中，我们将简要讨论 ViTs
    和变压器的一般优势，即注意力及其类型 [tv](#bib.bib45)，以便读者了解新型机器视觉（MV）模型，即 ViTs。
- en: 1.1 Self-attention
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 自注意力
- en: For a given a sequence of elements, the self-attention process gives a measurable
    estimate of the relevance of one element others. For example, which elements like
    words can come together in a sequence like a sentence. The self-attention process
    is an important unit of attention-based models like transformers, that models
    the dependencies among all elements of the sequence for formal/structured prediction
    applications. Plainly stated, a self-attention model layer assigns a value to
    every element in a structure/sequence by combining information globally from the
    input vector/sequence.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的元素序列，自注意力过程提供了一个可测量的估计，以确定一个元素与其他元素的相关性。例如，哪些元素如单词可以在句子这样的序列中组合在一起。自注意力过程是像变压器这样的基于注意力的模型的重要单元，它建模了序列中所有元素之间的依赖关系，用于形式化/结构化预测应用。简单来说，自注意力模型层通过从输入向量/序列中全局地结合信息，为结构/序列中的每个元素分配一个值。
- en: 'Denoting a sequence of n entities (x[1], x[2], $\ldots$ x[n]) by $\textbf{X}\in\mathbb{R}^{n\times
    d}$ , d being the dimension which embeds dependency of every element. The purpose
    of self-attention is capturing the dependency between all n elements after encoding
    every element inside the overall contextual knowledge. This process is achieved 
    by the definition of 3 weight matrices which have to be learnt for transforming:
    Queries $\left(\textbf{W}^{Q}\in\mathbb{R}^{n\times d_{q}}\right)$, Keys $\left(\textbf{W}^{K}\in\mathbb{R}^{n\times
    d_{k}}\right)$ and Values $\left(\textbf{W}^{V}\in\mathbb{R}^{n\times d_{v}}\right)$.
    First the input vector X is projected to the 3 weight matrices for obtaining $\textbf{Q}=\textbf{XW}^{Q}$,
    $\textbf{K}=\textbf{XW}^{K}$ and $\textbf{V}=\textbf{XW}^{V}$. The output $\textbf{Z}\in\mathbb{R}^{n\times
    d_{v}}$ in the self-attention layer is next expressed as,'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 $\textbf{X}\in\mathbb{R}^{n\times d}$ 表示 n 个实体的序列 (x[1], x[2], $\ldots$ x[n])，其中
    d 是表示每个元素依赖性的维度。自注意力的目的是捕捉所有 n 个元素之间的依赖关系，在对每个元素进行整体上下文知识编码后。这一过程通过定义 3 个权重矩阵来实现，这些矩阵需要学习以进行变换：查询
    $\left(\textbf{W}^{Q}\in\mathbb{R}^{n\times d_{q}}\right)$、键 $\left(\textbf{W}^{K}\in\mathbb{R}^{n\times
    d_{k}}\right)$ 和值 $\left(\textbf{W}^{V}\in\mathbb{R}^{n\times d_{v}}\right)$。首先，将输入向量
    X 投影到 3 个权重矩阵上，以获得 $\textbf{Q}=\textbf{XW}^{Q}$、$\textbf{K}=\textbf{XW}^{K}$ 和
    $\textbf{V}=\textbf{XW}^{V}$。在自注意力层中，输出 $\textbf{Z}\in\mathbb{R}^{n\times d_{v}}$
    随后表示为，
- en: '|  | $Z=\textbf{softmax}\left(\frac{\textbf{QK}^{T}}{\sqrt[]{d_{q}}}\right)\textbf{V}.$
    |  | (1) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | $Z=\textbf{softmax}\left(\frac{\textbf{QK}^{T}}{\sqrt[]{d_{q}}}\right)\textbf{V}.$
    |  | (1) |'
- en: For a certain element in the vector/sequence, the self-attention mechanism fundamentally
    finds the dot product of query with all the keys, this product being subsequently
    normalized by the softmax function for obtaining the attention-map scores. Every
    element now assumes the value of the weighted summation for all elements inside
    the vector/sequence, wherein all weights are equal to the attention map scores.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于向量/序列中的某个元素，自注意力机制基本上是计算查询与所有键的点积，这一乘积随后通过 softmax 函数进行归一化，以获得注意力图得分。现在每个元素都假设为向量/序列中所有元素的加权总和，其中所有权重都等于注意力图得分。
- en: 1.2 Masked self-attention
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 掩码自注意力
- en: '{justify}'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: The self-attention layer applies to every element/entity. For the transformer
    [tv_1](#bib.bib93) having been trained for prediction of the next entity in the
    vector/sequence, the self-attention units inside the decoder are then masked for
    prevention of their application to the entities coming in future. This technique
    is achieved by calculating the element-wise product with a mask $\textbf{M}\in\mathbb{R}^{n\times
    n}$ , where M is the upper triangular matrix. Thus masked self-attention is calculated
    as,
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力层适用于每个元素/实体。对于已被训练用于预测向量/序列中下一个实体的变换器 [tv_1](#bib.bib93)，解码器内部的自注意力单元会被掩码以防止它们对未来的实体进行应用。这一技术通过与掩码
    $\textbf{M}\in\mathbb{R}^{n\times n}$ 进行逐元素乘积计算来实现，其中 M 是上三角矩阵。因此，掩码自注意力的计算方式为，
- en: '|  | $\textbf{softmax}\left(\frac{\textbf{QK}^{T}}{\sqrt[]{d_{q}}}{\circ}~{}\textbf{M}\right),$
    |  | (2) |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{softmax}\left(\frac{\textbf{QK}^{T}}{\sqrt[]{d_{q}}}{\circ}~{}\textbf{M}\right),$
    |  | (2) |'
- en: where ${\circ}$ is the Hadamard product. During prediction of an element in
    the vector/sequence, the attention map scores of the future elements are set to
    0 in the masked self-attention.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\circ}$ 是哈达玛积。在向量/序列中的某个元素进行预测时，未来元素的注意力图得分在掩码自注意力中被设置为0。
- en: 1.3 Multi-head attention
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 多头注意力
- en: '{justify}'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: For encapsulation of various complicated dependencies between various elements
    / entities in the vector/sequence, the multi-head attention process consists of
    multiple self-attention units with h = 8 inside the original transformer architecture
    [tv_1](#bib.bib93) . Every unit contains its own learnable weight-matrices $\left\{\textbf{W}^{Q_{i}},\textbf{W}^{K_{i}},\textbf{W}^{V_{i}}\right\}$,
    where i = 0, 1, 2, … (h - 1). For a particular input X, outputs of h self-attention
    units in the multi-head attention process are combined into one matrix $[\textbf{Z}\textsubscript{0},\textbf{Z}\textsubscript{1},$
    …$\textbf{Z}\textsubscript{h$-$ 1}]\in\mathbb{R}^{n\times h\times d_{v}}$ and
    are subsequently projected to another weight matrix W $\in$ $\mathbb{R}^{h\cdot
    d_{v}\times d}.$
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了封装向量/序列中各个元素/实体之间的复杂依赖关系，多头注意力过程包含了原始变换器架构 [tv_1](#bib.bib93) 中的多个自注意力单元，h
    = 8。每个单元包含其自身的可学习权重矩阵 $\left\{\textbf{W}^{Q_{i}},\textbf{W}^{K_{i}},\textbf{W}^{V_{i}}\right\}$，其中
    i = 0, 1, 2, … (h - 1)。对于特定输入 X，多头注意力过程中 h 个自注意力单元的输出被组合成一个矩阵 $[\textbf{Z}\textsubscript{0},\textbf{Z}\textsubscript{1},$
    …$\textbf{Z}\textsubscript{h$-$ 1}]\in\mathbb{R}^{n\times h\times d_{v}}$，随后被投影到另一个权重矩阵
    W $\in$ $\mathbb{R}^{h\cdot d_{v}\times d}.$
- en: 'The notable difference of the self-attention process with the convolutional
    operation is that every weight is dynamically computed as against static weights
    which remain fixed for various inputs as for convolution. Also that the self-attention
    process is invariable to permutation and change for different number of inputs
    with the result that it has a convenient operation over irregularity as against
    the convolutional operator which needs a grid array. See [1](#S1.F1 "Figure 1
    ‣ 1.3 Multi-head attention ‣ 1 Introduction ‣ Attention mechanisms and deep learning
    for machine vision: A survey of the state of the art") for illustration of these
    concepts.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力过程与卷积操作的显著区别在于，每个权重是动态计算的，而卷积操作中的静态权重在面对各种输入时保持不变。此外，自注意力过程对不同输入数量的排列和变化是不可变的，从而在不规则性上的操作更为方便，而卷积操作需要一个网格数组。有关这些概念的说明请参见
    [1](#S1.F1 "图 1 ‣ 1.3 多头注意力 ‣ 1 介绍 ‣ 机器视觉中的注意力机制和深度学习：技术现状的调查")。
- en: '![Refer to caption](img/8a1df55ea3eeb9a8d2efe225439098a2.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8a1df55ea3eeb9a8d2efe225439098a2.png)'
- en: 'Figure 1: Illustration of various attention mechanisms [tv](#bib.bib45)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '图 1: 各种注意力机制的示意图 [tv](#bib.bib45)'
- en: 2 Attention based deep learning architectures
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 注意力基于的深度学习架构
- en: 'In this section, some common deep learning architectures of deep attention
    models are discussed [DeepVisualAttentionPred](#bib.bib96) and a graphical illustration
    is presented in [2](#S2.F2 "Figure 2 ‣ 2 Attention based deep learning architectures
    ‣ Attention mechanisms and deep learning for machine vision: A survey of the state
    of the art"). The architectures of the prevalent deep attention based models are
    categorized into the following important classes as given below:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，讨论了深度学习模型中一些常见的深度注意力模型架构 [DeepVisualAttentionPred](#bib.bib96)，并在 [2](#S2.F2
    "图 2 ‣ 2 基于注意力的深度学习架构 ‣ 机器视觉中的注意力机制和深度学习：技术现状的调查") 中展示了图示。当前流行的深度基于注意力的模型架构被分类为以下重要类别：
- en: '1.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Single channel model
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单通道模型
- en: '2.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Multi-channel model feeding on multi-scale data
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多通道模型处理多尺度数据
- en: '3.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Skip-layer model
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跳层模型
- en: '4.'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: Bottom-up/ top-down model
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自下而上/自上而下模型
- en: '5.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Skip-layer model with multi-scale saliency single network
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 具有多尺度显著性单网络的跳层模型
- en: '![Refer to caption](img/005b2490b3d4f1d2f4246dc9d7dc751b.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/005b2490b3d4f1d2f4246dc9d7dc751b.png)'
- en: 'Figure 2: (a)-(c) Depiction of 3 common classes of deep learning configurations
    used in attention prediction: (a) single-channel model configuration, (b) multi-channel
    model configuration with multi-scale input data, and (c) skip-layer model configuration.
    (d) bottom-up/top-down model configuration used in attention-based object segmentation
    and instance segmentation. (e) modified skip-layer model using multi-scale attention
    information in a single network. [DeepVisualAttentionPred](#bib.bib96)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: (a)-(c) 描述了在注意力预测中使用的 3 种常见深度学习配置：(a) 单通道模型配置，(b) 具有多尺度输入数据的多通道模型配置，以及
    (c) 跳层模型配置。 (d) 用于基于注意力的对象分割和实例分割的自下而上/自上而下模型配置。 (e) 使用多尺度注意力信息的修改跳层模型。 [DeepVisualAttentionPred](#bib.bib96)'
- en: 2.1 Single-channel model
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 单通道模型
- en: As demonstrated in Figure 2(a), the single channel model is the predominant
    configuration of various CNN-based attention models also being used by many attention-based
    works [a51](#bib.bib48) ; [a14](#bib.bib43) ; [a16](#bib.bib49) ; [a15](#bib.bib72)
    . Almost all the other types of CNN configurations can be considered as variants
    of the single channel model. It has been demonstrated that attention cues on various
    levels and scales are vital for attention [a28](#bib.bib108) . Using multi-scale
    features of CNNs into attention-based models is an obvious choice. In the next
    type of single channel model, namely multi-channel model, the changes are done
    along this line.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2(a)所示，单通道模型是各种基于CNN的注意力模型的主要配置，许多基于注意力的工作也在使用[a51](#bib.bib48) ; [a14](#bib.bib43)
    ; [a16](#bib.bib49) ; [a15](#bib.bib72) 。几乎所有其他类型的CNN配置都可以被视为单通道模型的变体。已经证明，各级别和尺度上的注意力提示对注意力至关重要[a28](#bib.bib108)
    。在基于注意力的模型中使用CNN的多尺度特征是显而易见的选择。在下一种单通道模型，即多通道模型中，沿着这条线进行了改变。
- en: 2.2 Multi-channel model
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 多通道模型
- en: Some implementations of this model include [a52](#bib.bib41) ; [a57](#bib.bib116)
    ; [a32](#bib.bib61) ; [a4](#bib.bib67) . The basic concept in the multi-channel
    model is shown in Figure 2(b). This type of model learns multi-scale attention
    information by training multiple models with multi-scale data inputs. The multiple
    model channels are in parallel and can have varying configurations with different
    scales. As shown in [a58](#bib.bib105) , input data is fed via multiple channels
    simultaneously, and then the features from different channels are fused and fed
    into a unified output layer for producing the final attention map. We observe
    that in the multi-channel model, multi-scale learning takes place outside the
    individual models. In the next configuration discussed, the multi-scale learning
    is inside the model, and this is achieved by combining feature maps from various
    convolutional layer hierarchies.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的一些实现包括[a52](#bib.bib41) ; [a57](#bib.bib116) ; [a32](#bib.bib61) ; [a4](#bib.bib67)
    。多通道模型的基本概念如图2(b)所示。这种类型的模型通过训练多个模型以处理多尺度数据输入来学习多尺度注意力信息。多个模型通道是并行的，可以具有不同尺度的各种配置。如[a58](#bib.bib105)所示，输入数据通过多个通道同时输入，然后将不同通道的特征融合并输入统一的输出层，以生成最终的注意力图。在多通道模型中，我们观察到多尺度学习发生在各个模型之外。在下一种配置中讨论的多尺度学习发生在模型内部，这通过结合来自不同卷积层层次的特征图来实现。
- en: 2.3 Skip-layer model
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 跳层模型
- en: A common skip-layer model is shown in Figure 2(c) being used in [a48](#bib.bib51)
    ; [a50](#bib.bib52) ; [a59](#bib.bib14) . Instead of learning from many parallel
    channels on multiple-scale images, the skip-layer model learns multi-scale feature
    maps inside a primary channel. Multi-scale outputs are learned from various layers
    with increasingly larger reception fields and down-sampling ratios. Next, these
    outputs are fused for outputting final attention map.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如图2(c)所示，常见的跳层模型被用于[a48](#bib.bib51) ; [a50](#bib.bib52) ; [a59](#bib.bib14)
    。跳层模型不是从多个尺度图像的多个并行通道中学习，而是在一个主要通道内部学习多尺度特征图。从具有逐渐增大的接收场和下采样比率的各个层中学习到多尺度输出。接下来，这些输出被融合以输出最终的注意力图。
- en: 2.4 Bottom-up/top-down model
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 自底向上/自顶向下模型
- en: This relatively newer model configuration called top-down/bottom-up model has
    been used in attention-based object segmentation [a60](#bib.bib110) and also in
    instance segmentation [a61](#bib.bib73) ; [hafizmmir](#bib.bib31) ; [mmir_review](#bib.bib29)
    . The architecture of the model is shown in Figure 2(d), wherein segmentation
    feature maps are first obtained by common bottom-up convolution techniques, and
    next a top-down refinement is done for fusing the data from deep to shallow layers
    into the mask. The main motivation behind this configuration is to produce high-fidelity
    segmentation masks because deep CNN layers lose fine image detail. The bottom-up/top-down
    model is like a type of skip-layer model since different layers are connected
    to each other.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这种相对较新的模型配置称为自顶向下/自底向上的模型，已在基于注意力的物体分割[a60](#bib.bib110)和实例分割[a61](#bib.bib73)
    ; [hafizmmir](#bib.bib31) ; [mmir_review](#bib.bib29)中使用。该模型的架构如图2(d)所示，其中分割特征图首先通过常见的自底向上的卷积技术获得，然后通过自顶向下的细化将数据从深层到浅层融合到掩码中。这种配置的主要动机是生成高保真度的分割掩码，因为深度CNN层会丢失细节图像。自底向上/自顶向下模型类似于一种跳层模型，因为不同层之间是相互连接的。
- en: 2.5 Skip-layer Model with Multi-scale Saliency Single Network
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 带有多尺度显著性单网络的跳层模型
- en: This model [DeepVisualAttentionPred](#bib.bib96) shown in Fig. 2(e), is inspired
    by the model in [a58](#bib.bib105) and the deeply-supervised model in [a22](#bib.bib57)
    . The model uses multi-scale and multi-level attention-based information from
    various layers, and learns via the deeply supervised technique. An important difference
    between this model and the previous models is that the former provides combined
    straightforward supervision of the hidden layers instead of the common approach
    of supervising only the last output layer and then propagating the supervised
    output back to the previous layers. It uses the merit of the skip-layer model
    (Figure 2(c)) which does not learn from multiple model channels with multi-scale
    input data. Also, it is lighter than the multi-channel model (Figure 2(b)) and
    bottom-up/top-down model (Figure 2(d)). It has been found that the bottom-up/top-down
    model faces training difficulties while as the deeply supervised model shows high
    training efficiency.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型 [DeepVisualAttentionPred](#bib.bib96) 如图 2(e) 所示，受到 [a58](#bib.bib105) 中模型和
    [a22](#bib.bib57) 中深度监督模型的启发。该模型使用来自各种层的多尺度和多层次的基于注意力的信息，并通过深度监督技术进行学习。该模型与之前模型的重要区别在于，前者提供了对隐藏层的直接监督，而不是仅对最后输出层进行监督并将监督输出传播回前面的层。它利用了跳过层模型的优点（图
    2(c)），该模型不会从具有多尺度输入数据的多个模型通道中学习。此外，它比多通道模型（图 2(b)）和自下而上/自上而下模型（图 2(d)）更轻量。研究发现，自下而上/自上而下模型在训练过程中面临困难，而深度监督模型则显示出高效的训练效果。
- en: In the next section we turn to the categorization of various techniques of attention
    mechanisms and deep learning in machine vision, and discuss each category in detail.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论注意力机制和深度学习在机器视觉中的各种技术的分类，并详细讨论每个类别。
- en: '3 Attention and deep learning in machine vision: Broad categories'
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 注意力与深度学习在机器视觉中的应用：广泛类别
- en: 'In this section, we discuss category-wise the various techniques of attention
    mechanisms and deep learning applied to machine vision. Three broad categories
    are:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将按类别讨论应用于机器视觉的各种注意力机制和深度学习技术。三大类是：
- en: '1.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Attention-based CNNs
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于注意力的CNNs
- en: '2.'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: CNN transformer pipelines
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CNN 变换器管道
- en: '3.'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Hybrid transformers
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 混合变换器
- en: These categories are discussed in the following sub-sections one by one. First
    we discuss attention-based CNNs in the following subsection.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类别将在以下小节中逐一讨论。首先，我们将在以下小节中讨论基于注意力的CNNs。
- en: 3.1 Attention-based CNNs
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 基于注意力的CNNs
- en: Recently attention mechanisms have been applied in deep learning for machine
    vision applications, e.g. object detection [ac1_3](#bib.bib5) ; [ac1_31](#bib.bib83)
    ; [ac1_28](#bib.bib76) , image captioning [ac1_35](#bib.bib106) ; [ac1_39](#bib.bib113)
    ; [ac1_2](#bib.bib3) and action recognition [ac1_30](#bib.bib81) . The central
    idea of the attention mechanisms is locating the most salient components of the
    feature maps in convolutional neural networks (CNNs) in a manner that the redundancy
    is removed for machine vision applications. Generally, attention is embedded in
    the CNN by using attention maps. Particularly the attention-based maps in [ac1_31](#bib.bib83)
    ; [ac1_28](#bib.bib76) ; [ac1_35](#bib.bib106) ; [ac1_30](#bib.bib81) yield in
    a self learned manner having other information with weak supervision of the attention
    maps. Other techniques cited in literature [ac1_39](#bib.bib113) ; [ac1_37](#bib.bib107)
    proceed by utilization of human attention data or guidance of the CNNs by focusing
    on the regions of interest (ROIs). In the following subsections, we proceed with
    discussing some noteworthy techniques in the general area of machine vision which
    use attention-based CNNs e.g. those used in image classification/retrieval, object
    detection, sign language recognition, denoising and facial expression recognition.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，注意力机制已被应用于深度学习的机器视觉应用中，例如对象检测 [ac1_3](#bib.bib5) ; [ac1_31](#bib.bib83) ;
    [ac1_28](#bib.bib76) ，图像描述 [ac1_35](#bib.bib106) ; [ac1_39](#bib.bib113) ; [ac1_2](#bib.bib3)
    和动作识别 [ac1_30](#bib.bib81) 。注意力机制的核心思想是在卷积神经网络（CNNs）中定位特征图的最显著部分，以便为机器视觉应用去除冗余。通常，通过使用注意力图将注意力嵌入到CNN中。特别是，[ac1_31](#bib.bib83)
    ; [ac1_28](#bib.bib76) ; [ac1_35](#bib.bib106) ; [ac1_30](#bib.bib81) 中基于注意力的图以自学习的方式产生，具备其他信息并对注意力图进行弱监督。文献中引用的其他技术
    [ac1_39](#bib.bib113) ; [ac1_37](#bib.bib107) 通过利用人类注意力数据或通过关注兴趣区域（ROIs）来指导CNNs。接下来的小节中，我们将讨论一些在机器视觉的一般领域中值得注意的技术，这些技术使用基于注意力的CNNs，例如在图像分类/检索、对象检测、手语识别、去噪和面部表情识别中的应用。
- en: 3.1.1 Image classification/retrieval and object detection
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 图像分类/检索与目标检测
- en: 'It is well established that attention contributes to human perception in an
    important manner [ac2_2](#bib.bib47) ; [ac2_24](#bib.bib78) ; [ac2_25](#bib.bib13)
    . One important characteristic of a human vision system is that it does not attempt
    to address the whole visual scene at one go. Instead, in the same, a sequence
    of partial glimpses is exploited and focusing is done selectively on various parts
    for capturing the visual structure in a better manner [ac2_26](#bib.bib53) . Recently,
    several attempts have been made [ac2_27](#bib.bib95) ; [ac2_28](#bib.bib38) for
    incorporation of attention processing mechanisms in order to improve the classification
    accuracy of CNNs on large scale classification tasks. Wang et al. [ac2_27](#bib.bib95)
    have proposed a residual attention network having an encoder-decoder style attention
    mechanism unit. By refinement of the features, the network gives good accuracy
    as well as shows robustness to noise. Without directly computing the three dimensional
    attention map, the process is decomposed such that it learns channel-attention
    and spatial-attention exclusively. The exclusive attention map generation technique
    for 3D features is computationally inexpensive and parameter restricted, and hence
    can be used as a plug and play unit for existing CNN networks. In their work [ac2_28](#bib.bib38)
    , the authors have introduced a compact unit for exploitation of the relationship
    between various channels. In this ’Squeeze and Excitation’ unit, the authors have
    used global average-pooling of feature maps for computation of each channel’s
    attention. However, the authors of [ac2](#bib.bib101) show that the features used
    in [ac2_28](#bib.bib38) are suboptimal for inferring fine-channel attention. Accordingly
    the authors of [ac2](#bib.bib101) use max-pooled feature maps also. According
    to [ac2](#bib.bib101) in [ac2_28](#bib.bib38) spatial attention is missed which
    contributes in an important manner to deciding the focusing region as brought
    out in [ac2_29](#bib.bib11) . The authors of [ac2](#bib.bib101) thus proposed
    the convolutional block attention module (CBAM) for exploitation of both the spatial
    as well as channel-wise attention with the help of a robust network and proceed
    to verify that exploitation of both these mechanisms is better than use of only
    the channel-wise attention mechanism [ac2_28](#bib.bib38) by using it for image
    classification in ImageNet-1K dataset [ac2_imagenet](#bib.bib15) . The authors
    of [ac2](#bib.bib101) experimentally demonstrate that their module is effective
    also in object detection tasks using two popular datasets viz. MS-COCO [ac2_coco](#bib.bib66)
    and VOC [ac2_voc](#bib.bib23) . They achieve impressive results by inserting their
    module in the pre-existing one-shot object detector [ac2_30](#bib.bib100) in the
    VOC-2007 testing set. [3](#S3.F3 "Figure 3 ‣ 3.1.1 Image classification/retrieval
    and object detection ‣ 3.1 Attention-based CNNs ‣ 3 Attention and deep learning
    in machine vision: Broad categories ‣ Attention mechanisms and deep learning for
    machine vision: A survey of the state of the art") shows the CBAM for both channel
    and spatial-attention processes. Here we attempt to briefly explain the attention
    mechanism in CBAM.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '已经明确指出，注意力对人类感知有重要作用 [ac2_2](#bib.bib47) ; [ac2_24](#bib.bib78) ; [ac2_25](#bib.bib13)
    。人类视觉系统的一个重要特点是，它不会一次性处理整个视觉场景。而是，通过一系列局部的观察，选择性地关注各个部分，以更好地捕捉视觉结构 [ac2_26](#bib.bib53)
    。最近，已有几项尝试 [ac2_27](#bib.bib95) ; [ac2_28](#bib.bib38) ，致力于将注意力处理机制整合到大规模分类任务的
    CNN 中，以提高分类准确性。Wang 等人 [ac2_27](#bib.bib95) 提出了一个具有编码器-解码器风格注意力机制单元的残差注意力网络。通过特征的精炼，该网络不仅提供了良好的准确性，还显示出对噪声的鲁棒性。该过程在不直接计算三维注意力图的情况下被分解，以便它独立地学习通道注意力和空间注意力。用于
    3D 特征的独特注意力图生成技术计算开销小且参数受限，因此可以作为插件单元用于现有的 CNN 网络。在他们的研究 [ac2_28](#bib.bib38)
    中，作者引入了一个紧凑单元，用于利用各通道之间的关系。在这个‘压缩与激励’单元中，作者使用了特征图的全局平均池化来计算每个通道的注意力。然而，[ac2](#bib.bib101)
    的作者表明，[ac2_28](#bib.bib38) 中使用的特征对于推断精细通道注意力来说是不够的。因此，[ac2](#bib.bib101) 的作者还使用了最大池化的特征图。[ac2](#bib.bib101)
    中提到，[ac2_28](#bib.bib38) 中缺失的空间注意力对决定关注区域有重要贡献，正如 [ac2_29](#bib.bib11) 所示。因此，[ac2](#bib.bib101)
    的作者提出了卷积块注意力模块（CBAM），用于同时利用空间和通道注意力，通过强大的网络验证了同时使用这两种机制优于仅使用通道注意力机制 [ac2_28](#bib.bib38)
    。在 ImageNet-1K 数据集 [ac2_imagenet](#bib.bib15) 上进行图像分类时使用了该模块。[ac2](#bib.bib101)
    的作者还通过在两个流行的数据集 MS-COCO [ac2_coco](#bib.bib66) 和 VOC [ac2_voc](#bib.bib23) 中进行实验，证明了他们的模块在目标检测任务中的有效性。他们通过将模块插入到预先存在的单次目标检测器
    [ac2_30](#bib.bib100) 中，在 VOC-2007 测试集中取得了令人印象深刻的结果。[3](#S3.F3 "Figure 3 ‣ 3.1.1
    Image classification/retrieval and object detection ‣ 3.1 Attention-based CNNs
    ‣ 3 Attention and deep learning in machine vision: Broad categories ‣ Attention
    mechanisms and deep learning for machine vision: A survey of the state of the
    art") 显示了 CBAM 对于通道和空间注意力过程的应用。这里我们尝试简要解释 CBAM 中的注意力机制。'
- en: '![Refer to caption](img/d4fec9c04bb5b43d764a8adbfed338de.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d4fec9c04bb5b43d764a8adbfed338de.png)'
- en: 'Figure 3: Illustration of both attention sub-modules in CBAM [ac2](#bib.bib101)
    . As shown, the channel-wise sub-module utilizes the max-pooling of the feature
    output as well as the average-pooling of the feature output with the help of a
    shared network. On the other hand, the spatial-wise sub-module uses two identical
    feature outputs by pooling them along their channel axes and then forwarding them
    to the convolutional layer. [ac2](#bib.bib101)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：CBAM [ac2](#bib.bib101) 中两个注意力子模块的示意图。如图所示，通道-wise 子模块利用特征输出的最大池化以及特征输出的平均池化，借助一个共享网络。另一方面，空间-wise
    子模块通过沿通道轴对两个相同的特征输出进行池化，然后将其传递到卷积层。 [ac2](#bib.bib101)
- en: '{justify}'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: 'For a given input feature map $\textbf{F}\in\mathbb{R}^{C\times H\times W}$
    , CBAM [ac2](#bib.bib101) produces a one-dimensional attention map $\textbf{M}_{c}\in\mathbb{R}^{C\times
    1\times 1}$ and a two-dimensional spatial attention map $\textbf{M}_{s}\in\mathbb{R}^{1\times
    H\times W}$ as shown in Figure 3\. This attention mechanism operation can be put
    as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的输入特征图 $\textbf{F}\in\mathbb{R}^{C\times H\times W}$，CBAM [ac2](#bib.bib101)
    生成一维的注意力图 $\textbf{M}_{c}\in\mathbb{R}^{C\times 1\times 1}$ 和二维的空间注意力图 $\textbf{M}_{s}\in\mathbb{R}^{1\times
    H\times W}$，如图 3 所示。该注意力机制操作可以表示为：
- en: '|  | $\textbf{F}^{{}^{\prime}}=\textbf{M}_{c}\left(\textbf{F}\right)\bigotimes\textbf{F},$
    |  | (3) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{F}^{{}^{\prime}}=\textbf{M}_{c}\left(\textbf{F}\right)\bigotimes\textbf{F},$
    |  | (3) |'
- en: '|  | $\textbf{F}^{{}^{\prime\prime}}=\textbf{M}_{s}\left(\textbf{F}^{\prime}\right)\bigotimes\textbf{F}^{\prime}.$
    |  | (4) |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{F}^{{}^{\prime\prime}}=\textbf{M}_{s}\left(\textbf{F}^{\prime}\right)\bigotimes\textbf{F}^{\prime}.$
    |  | (4) |'
- en: '{justify}'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: 'where $\bigotimes$ is the multiplication operator for elements. {justify} Channel
    attention is mathematically computed as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\bigotimes$ 是元素的乘法运算符。{justify} 通道注意力的数学计算如下：
- en: '|  | $\textbf{M}_{c}\left(\textbf{F}\right)=\sigma\left(MLP\left(AvgPool\left(\textbf{F}\right)\right)+MLP\left(MaxPool\left(\textbf{F}\right)\right)\right)\\
    =\sigma\left(\textbf{W}_{1}\left(\textbf{W}_{0}\left(\textbf{F}_{\mathrm{avg}}^{c}\right)\right)+\textbf{W}_{1}\left(\textbf{W}_{0}\left(\textbf{F}_{\mathrm{\max}}^{c}\right)\right)\right)$
    |  | (5) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{M}_{c}\left(\textbf{F}\right)=\sigma\left(MLP\left(AvgPool\left(\textbf{F}\right)\right)+MLP\left(MaxPool\left(\textbf{F}\right)\right)\right)\\
    =\sigma\left(\textbf{W}_{1}\left(\textbf{W}_{0}\left(\textbf{F}_{\mathrm{avg}}^{c}\right)\right)+\textbf{W}_{1}\left(\textbf{W}_{0}\left(\textbf{F}_{\mathrm{\max}}^{c}\right)\right)\right)$
    |  | (5) |'
- en: '{justify}'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: where $\sigma$ is the sigmoid function, $\textbf{W}_{0}\in\mathbb{R}^{C/r\times
    C}$ and $\textbf{W}_{1}\in\mathbb{R}^{C\times C/r}$ . The Multi-layer Perceptron
    (MLP) weights, $\textbf{W}_{0}$ and $\textbf{W}_{1}$ , are shared for both the
    inputs. $\textbf{W}_{0}~{}$ comes after the ReLU activation function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 是 sigmoid 函数，$\textbf{W}_{0}\in\mathbb{R}^{C/r\times C}$ 和 $\textbf{W}_{1}\in\mathbb{R}^{C\times
    C/r}$。多层感知器（MLP）权重 $\textbf{W}_{0}$ 和 $\textbf{W}_{1}$ 对两个输入是共享的。$\textbf{W}_{0}~{}$
    在 ReLU 激活函数之后。
- en: '|  | $\textbf{M}_{s}\left(\textbf{F}\right)=\sigma\left(f^{7\times 7}\left(\left[AvgPool\left(\textbf{F}\right);MaxPool\left(\textbf{F}\right)\right]\right)\right)=\sigma\left(f^{7\times
    7}\left(\left[\textbf{F}_{\mathrm{avg}}^{s};\textbf{F}_{\mathrm{\max}}^{s}\right]\right)\right)$
    |  | (6) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{M}_{s}\left(\textbf{F}\right)=\sigma\left(f^{7\times 7}\left(\left[AvgPool\left(\textbf{F}\right);MaxPool\left(\textbf{F}\right)\right]\right)\right)=\sigma\left(f^{7\times
    7}\left(\left[\textbf{F}_{\mathrm{avg}}^{s};\textbf{F}_{\mathrm{\max}}^{s}\right]\right)\right)$
    |  | (6) |'
- en: '{justify}'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: 'where $\sigma$ is the sigmoid function and $f^{7\times 7}$ is the convolutional
    operator with a $7\times 7$ filter. The authors of [ac2](#bib.bib101) have used
    their technique for both image classification/retrieval on the ImageNet-1K dataset
    and object detection on both MS-COCO and VOC2007 datasets. The results obtained
    using their CBAM integrated networks outperform other contemporary networks. They
    also have demonstrated the superiority of their technique as compared to others
    also via grad-CAM [ac2_18](#bib.bib79) visualizations obtained on images from
    ImageNet validation set. [4](#S3.F4 "Figure 4 ‣ 3.1.1 Image classification/retrieval
    and object detection ‣ 3.1 Attention-based CNNs ‣ 3 Attention and deep learning
    in machine vision: Broad categories ‣ Attention mechanisms and deep learning for
    machine vision: A survey of the state of the art") shows the same.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 是 sigmoid 函数，而 $f^{7\times 7}$ 是一个 $7\times 7$ 滤波器的卷积算子。[ac2](#bib.bib101)
    的作者已将他们的技术应用于 ImageNet-1K 数据集上的图像分类/检索和 MS-COCO 及 VOC2007 数据集上的目标检测。使用他们的 CBAM
    集成网络获得的结果超越了其他同时期的网络。他们还通过 grad-CAM [ac2_18](#bib.bib79) 可视化展示了他们技术相对于其他技术的优越性，所用图像来自
    ImageNet 验证集。[4](#S3.F4 "图 4 ‣ 3.1.1 图像分类/检索和目标检测 ‣ 3.1 基于注意力的 CNN ‣ 3 注意力与深度学习在机器视觉中的应用：广泛类别
    ‣ 机器视觉中的注意力机制与深度学习：最前沿调查") 也显示了这一点。
- en: '![Refer to caption](img/a1307a59e8cdfac6f4b957c74d5c0c7b.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a1307a59e8cdfac6f4b957c74d5c0c7b.png)'
- en: 'Figure 4: Heat map visualizations using Grad-CAM [ac2_18](#bib.bib79) . The
    visualizations are shown for those of the CBAM-fitted CNNs, viz. {ResNet50 + CBAM},
    baseline {ResNet50} [ac2_5](#bib.bib33) , and Squeeze and Excitation method [ac2_28](#bib.bib38)
    (SE)-integrated architecture {ResNet50 + SE}. Grad-CAM visualization has been
    obtained with feature maps of last conv layer outputs. The GT label has been shown
    on top of every image, where P is the softmax score of every network for the GT
    category. (Reproduced by permission from publisher of [ac2](#bib.bib101) )'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 使用 Grad-CAM [ac2_18](#bib.bib79) 的热力图可视化。这些可视化展示了 CBAM 适配的 CNN，如 {ResNet50
    + CBAM}、基线 {ResNet50} [ac2_5](#bib.bib33) 和 Squeeze 与激励方法 [ac2_28](#bib.bib38)（SE）集成的架构
    {ResNet50 + SE}。Grad-CAM 可视化是通过最后卷积层输出的特征图获得的。GT 标签显示在每张图像的顶部，其中 P 是每个网络对于 GT
    类别的 softmax 得分。（经 [ac2](#bib.bib101) 出版商许可再现）'
- en: 'Another novel and related work in the area of image classification/retrieval
    by using attention-based CNNs is given by the authors of [ac1](#bib.bib63) for
    glaucoma detection from the area of medical image analysis [hafizmedical](#bib.bib30)
    . They call their network attention-based CNN for glaucoma detection AG-CNN. It
    includes a novel attention-prediction subnet along with other subnets. They achieve
    ’end to end’ training on an attention-based CNN architecture by supervision of
    the training through 3 separate loss functions based on: i) attention-prediction,
    ii) feature-visualization and iii) glaucoma-classification. Based on the work
    of authors in [ac1_16](#bib.bib42) , the authors use the Kullback Leibler (KL)
    divergence function as an equivalent of the nature-inspired attention-loss $Loss_{a}$
    given by:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用基于注意力的 CNN 进行图像分类/检索领域中，[ac1](#bib.bib63) 的作者提供了一个新颖且相关的工作，用于从医学图像分析中检测青光眼
    [hafizmedical](#bib.bib30)。他们称其网络为用于青光眼检测的注意力基 CNN，简称 AG-CNN。该网络包括一个新颖的注意力预测子网以及其他子网。他们通过基于以下三个独立损失函数的监督实现了“端到端”训练：i)
    注意力预测，ii) 特征可视化，iii) 青光眼分类。基于 [ac1_16](#bib.bib42) 的作者的工作，他们使用 Kullback Leibler
    (KL) 散度函数作为自然启发的注意力损失 $Loss_{a}$ 的等效函数，计算公式为：
- en: '|  | $\mathrm{Loss}_{a}=\frac{1}{I\cdot J}\sum_{i=1}^{I}\sum_{j=1}^{J}A_{ij}\log\left(\frac{A_{ij}}{\hat{A}_{ij}}\right)$
    |  | (7) |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathrm{Loss}_{a}=\frac{1}{I\cdot J}\sum_{i=1}^{I}\sum_{j=1}^{J}A_{ij}\log\left(\frac{A_{ij}}{\hat{A}_{ij}}\right)$
    |  | (7) |'
- en: '{justify}'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: where $\hat{A}$ (with its elements $\hat{A}_{ij}\in\left[0,1\right]$ ) is the
    attention map, and, I and J are the attention-map length and width respectively.
    By incorporating these novel features, the authors of [ac1](#bib.bib63) demonstrate
    that their proposed AG-CNN technique significantly improves the state of the art
    in glaucoma detection.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{A}$（其元素 $\hat{A}_{ij}\in\left[0,1\right]$）是注意力图，I 和 J 分别是注意力图的长度和宽度。通过融入这些新特性，[ac1](#bib.bib63)
    的作者展示了他们提出的 AG-CNN 技术显著提升了青光眼检测的最前沿水平。
- en: For more interesting techniques on image classification/retrieval using attention-based
    CNNs the readers may refer to some of the recent outstanding works in this area
    as given in [pb](#bib.bib24) ; [hic](#bib.bib32) , etc.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用基于注意力的 CNNs 进行图像分类/检索的更多有趣技术，读者可以参考该领域一些近期的杰出工作，如 [pb](#bib.bib24)；[hic](#bib.bib32)
    等。
- en: 3.1.2 Sign Language Recognition
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 手语识别
- en: 'Sign language recognition (SLR) is a valuable and challenging research area
    in machine vision related multimedia field. Conventionally, SLR relies on hand-crafted
    features with low performance. In their novel work [sl](#bib.bib40) , the authors
    propose to use attention based 3D CNNs for SLR. Their model has 2 advantages.
    First, it learns spatial and temporal features from video frames without any pre-processing
    or prior knowledge. Attention mechanisms help the model to select the clues. During
    training for capturing the features, spatial attention is used in the model for
    focusing on the ROIs. After this, temporal attention is used for selection of
    the important motions for determining the action-class. Their method has been
    benchmarked on a self-made large Chinese SL dataset having 500 classes, and also
    on the ChaLearn14 benchmark [sl_6](#bib.bib22) . The authors demonstrate that
    their technique outperforms other state of the art techniques on the datasets
    used. We discuss this interesting technique in more detail below. {justify} The
    spatial attention map is calculated as follows. They use an attention-based mask
    for denotation of the value of each image pixel. Let $x_{i,k}\in\mathbb{R}^{2}$
    denote the position of a viewpoint k in an image i, the value of the location
    $p\in\mathbb{R}^{2}$ inside the attention map $M_{i,k}\in\mathbb{R}^{w\times h}$
    for k is given by:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 手语识别（SLR）是计算机视觉相关多媒体领域中的一个有价值且具有挑战性的研究领域。传统上，SLR 依赖于手工制作的特征，其性能较低。在他们的创新工作中，[sl](#bib.bib40)
    的作者提议使用基于注意力的 3D CNNs 进行 SLR。他们的模型有两个优点。首先，它从视频帧中学习空间和时间特征，而无需任何预处理或先验知识。注意力机制帮助模型选择线索。在捕获特征的训练过程中，模型使用空间注意力来集中关注感兴趣区域（ROIs）。之后，使用时间注意力来选择重要的运动，以确定动作类别。他们的方法在一个自制的大型中文
    SL 数据集（拥有 500 个类别）和 ChaLearn14 基准 [sl_6](#bib.bib22) 上进行了基准测试。作者展示了他们的技术在所用数据集上优于其他最先进的技术。我们将在下面更详细地讨论这一有趣的技术。{justify}
    空间注意力图的计算方法如下。他们使用基于注意力的掩码来表示每个图像像素的值。令 $x_{i,k}\in\mathbb{R}^{2}$ 表示图像 i 中视点
    k 的位置，注意力图 $M_{i,k}\in\mathbb{R}^{w\times h}$ 中位置 $p\in\mathbb{R}^{2}$ 的值由下式给出：
- en: '|  | $M_{i,k}\left(p\right)=\exp\left(-\frac{\&#124;p-x_{i,k}\&#124;_{2}^{2}}{\sigma}\right),$
    |  | (8) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | $M_{i,k}\left(p\right)=\exp\left(-\frac{\&#124;p-x_{i,k}\&#124;_{2}^{2}}{\sigma}\right),$
    |  | (8) |'
- en: '{justify}'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: where $\sigma$ is experimentally chosen, and w and h are image dimensions. The
    attention mask is formed by aggregating the peaks of various viewpoints obtained
    previously with the help of a max operator,
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\sigma$ 是实验选择的，w 和 h 是图像的维度。注意力掩码通过聚合先前获得的不同视点的峰值形成，利用了最大操作符，
- en: '|  | $M_{i}\left(p\right)=\mathop{\max}_{k}M_{i,k}\left(p\right).$ |  | (9)
    |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | $M_{i}\left(p\right)=\mathop{\max}_{k}M_{i,k}\left(p\right).$ |  | (9)
    |'
- en: '{justify}'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: Consequently the i^(th) attention weighed image I[i] is the element-wise product
    given by,
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第 i^(th) 个注意力加权图像 I[i] 是如下所示的逐元素乘积，
- en: '|  | $I_{i}\left(p\right)=I_{i}\left(p\right)\times M_{i}\left(p\right).$ |  |
    (10) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $I_{i}\left(p\right)=I_{i}\left(p\right)\times M_{i}\left(p\right).$ |  |
    (10) |'
- en: '{justify}'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: Based on the video feature obtained above, the use a Support Vector Machine
    (SVM) based classifier [sl_18](#bib.bib92) for classification by clubbing it to
    another temporal attention-based pipeline. As done earlier in [sl_43](#bib.bib21)
    , the features are fed to a bi-directional LSTM for generation of an attention
    vector $s\in\mathbb{R}^{8192}$ . The features are also fed to a one-layer MLP
    which gives the hidden vector $H=\{h_{1},h_{2},\ldots,h_{n}\}$ , $h_{i}\in\mathbb{R}^{8192}$
    . This vector is an integration of the sequence of clip features by attention
    pooling. This technique measures the value of each clip feature by determining
    its relation with the attention vector s. Finally, they combine the video and
    trajectory features and use softmax based classification. Although an effective
    technique, the authors still admit that the work focuses on isolated SLR. For
    dealing with continuous SLR, which translates a clip into a sentence, RNN based
    methods are going to give results as admitted by the authors of the above work,
    and they want to work in that direction.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述获得的视频特征，使用基于支持向量机（SVM）的分类器[sl_18](#bib.bib92)，通过将其与另一个时间注意力机制的管道结合来进行分类。与[sl_43](#bib.bib21)中的做法相同，特征被输入到一个双向LSTM中，以生成一个注意力向量
    $s\in\mathbb{R}^{8192}$ 。特征还被输入到一个单层MLP中，该MLP生成一个隐藏向量 $H=\{h_{1},h_{2},\ldots,h_{n}\}$
    ，其中 $h_{i}\in\mathbb{R}^{8192}$ 。该向量是通过注意力池化整合剪辑特征序列。这种技术通过确定每个剪辑特征与注意力向量s的关系来衡量每个剪辑特征的值。最后，他们结合视频和轨迹特征，并使用基于softmax的分类。尽管这是一种有效的技术，作者仍然承认该工作专注于孤立的SLR。对于处理连续SLR，即将剪辑转换为句子的情况，RNN方法将给出结果，正如上述工作作者所承认的，他们希望在这方面进行研究。
- en: 3.1.3 Image denoising
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 图像去噪
- en: 'Image denoising is a low-level machine vision (MV) task. Deep CNNs are quite
    popular in low-level MV. Research has been done to improve the performance in
    the area by using very deep networks. However, as the network depth increases,
    the effects of the shallow layers on deep layers decrease. Accordingly the authors
    of [ac4](#bib.bib89) have proposed an attention-based denoising CNN named ADNet  featuring
    an attention block (AB). The AB has been used for fine extraction of the noise
    data hidden in complex backgrounds. This technique has been proved by the authors
    of [ac4](#bib.bib89) to be very effective for denoising images with complex noise
    e.g. real noise-induced images. Various experiments demonstrate that ADNet delivers
    very good performance for 3 tasks viz. denoising of synthetic images, denoising
    of real noisy images, and also blind denoising. Here, the AB guides the previous
    network section by using the current network section in order to learn the noise
    nature. This is particularly useful for unknown images having noise, i.e. real
    noisy images and blind denoising. The AB uses 2 successive steps for implementation
    of its attention mechanism. First a 1x1 convolution is done on the output from
    the 17^(th) CNN layer output in order to compress the feature map into a weight
    vector for adjustment of the previous section. Next, the weights thus obtained
    are used to multiply the feature map output of the 16^(th) CNN layer for extraction
    of more refined noise feature maps. It should be noted that inspired by this novel
    effort more complex attention mechanisms can be used along with more dedicated
    ’denoising’ deep CNNs. The code of ADNet is available at: https://github.com/hellloxiaotian/ADNet.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图像去噪是一个低层次的机器视觉（MV）任务。深度卷积神经网络（CNN）在低层次MV中非常受欢迎。通过使用非常深的网络，研究已经在这一领域提高了性能。然而，随着网络深度的增加，浅层对深层的影响会减小。因此，[ac4](#bib.bib89)的作者提出了一种基于注意力机制的去噪CNN，称为ADNet，具有一个注意力块（AB）。AB用于细致提取隐藏在复杂背景中的噪声数据。该技术已经被[ac4](#bib.bib89)的作者证明对于去噪具有复杂噪声的图像非常有效，例如真实噪声引起的图像。各种实验表明，ADNet在三项任务中表现非常出色，即合成图像的去噪、真实噪声图像的去噪以及盲去噪。在这里，AB通过使用当前网络部分来指导前一网络部分，以便学习噪声的性质。这对于具有噪声的未知图像，即真实噪声图像和盲去噪尤为有用。AB使用两个连续步骤来实现其注意力机制。首先，对第17层CNN输出的结果进行1x1卷积，以将特征图压缩成一个权重向量，用于调整前一部分。接下来，利用获得的权重乘以第16层CNN的特征图输出，以提取更精细的噪声特征图。需要注意的是，受到这一新颖努力的启发，可以使用更复杂的注意力机制，并结合更专门的“去噪”深度CNN。ADNet的代码可在以下网址获取：https://github.com/hellloxiaotian/ADNet。
- en: 3.1.4 Facial expression recognition
  id: totrans-106
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 面部表情识别
- en: 'One hot topic in Machine Vision (MV) is facial expression recognition (FER)
    which can be used in various MV fields like human computer interaction (HCI),
    affective computing, etc. In their work [ac31](#bib.bib62) , the authors have
    proposed an end to end CNN network featuring an attention mechanism for auto FER.
    It has 4 main parts viz. feature extraction unit, attention unit, reconstruction
    unit and classification unit. The attention mechanism incorporated guides the
    CNN for paying more attention to important features extracted from earlier unit.
    The authors have combined their LBP features and their attention mechanism for
    enhancing the attention mechanism for obtaining better performance. They have
    applied their technique to their own dataset and 4 others, i.e., JAFFE [ac31_13](#bib.bib70)
    , CK+ [ac31_11](#bib.bib69) , FER2013 [ac31_39](#bib.bib1) and Oulu-CASIA [ac31_25](#bib.bib115)
    , and have experimentally demonstrated that their technique performs better than
    other contemporary techniques. The attention mechanism used in the work has been
    proved to be valuable in pixelwise MV tasks. Their attention unit consists of
    two branches. The first is used to obtain feature map F[p], and the second combines
    the LBP feature maps for obtaining the attention maps F[m]. In the next step,
    the element wise multiplication is done for the attention maps F[m] and the feature
    maps F[p] to obtain the final feature maps F[m] as:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 机器视觉（MV）中的一个热门话题是面部表情识别（FER），它可以用于各种MV领域，如人机交互（HCI）、情感计算等。在他们的工作中，[ac31](#bib.bib62)
    作者提出了一种端到端的CNN网络，具有用于自动FER的注意机制。它包含4个主要部分，即特征提取单元、注意单元、重建单元和分类单元。引入的注意机制指导CNN更多地关注从前一个单元提取的重要特征。作者结合了他们的LBP特征和注意机制，以增强注意机制以获得更好的性能。他们将其技术应用于自己的数据集和其他4个数据集，即JAFFE
    [ac31_13](#bib.bib70)、CK+ [ac31_11](#bib.bib69)、FER2013 [ac31_39](#bib.bib1) 和Oulu-CASIA
    [ac31_25](#bib.bib115)，并通过实验表明他们的技术优于其他现有技术。该工作中使用的注意机制在逐像素MV任务中被证明是有价值的。他们的注意单元由两个分支组成。第一个用于获取特征图F[p]，第二个结合LBP特征图以获取注意图F[m]。下一步，对注意图F[m]和特征图F[p]进行逐元素乘法，以获得最终特征图F[m]，其计算公式为：
- en: '|  | $F_{final}=F_{p}F_{m}$ |  | (11) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{final}=F_{p}F_{m}$ |  | (11) |'
- en: '{justify}'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '{justify}'
- en: 'Supposing that input of previous layer in the second branch is f[m], then the
    attention maps F[m] are given by:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设第二个分支的前一层输入为f[m]，则注意图F[m]为：
- en: '|  | $F_{m}=sigmoid\left(Wf_{m}+b\right)$ |  | (12) |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '|  | $F_{m}=sigmoid\left(Wf_{m}+b\right)$ |  | (12) |'
- en: where w and b are denotations for weights and bias of conv layer, respectively.
    The technique is suitable for 2D images and its architecture needs to be modified
    to extend its application to video, 3D facial data, depth-image data. The authors
    also state that they are considering using more robust and efficient machine learning
    (ML) techniques for enhancement of the architecture.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 w 和 b 分别表示卷积层的权重和偏置。该技术适用于2D图像，其架构需要修改以扩展到视频、3D面部数据、深度图像数据。作者还表示他们正在考虑使用更稳健和高效的机器学习（ML）技术来增强该架构。
- en: In another valuable work in the area of FER given in [ac32](#bib.bib65) ,the
    authors state that in spite of the fact that conventional FER systems are almost
    perfect for analyzing constrained poses however they cannot perform well for partially
    occluded poses which are common in the real world. Accordingly, they have proposed
    an attention-based CNN (ACNN) for perception of facial occlusion part which focuses
    on the highly discriminative unoccluded parts. Learning in their model is end
    to end. For various Regions of Interest (ROIs), they have introduced two types
    of ACNN viz. patch based type and global-local based type. The first type uses
    attention only for local patches in face regions. The second type combines local
    features at the patch level with global features at the imagelevel. Evaluation
    is done on their own face expression dataset having in-the-wild occlusions, 2
    of the largest in-the-wild face expression datasets i.e. RAF-DB [ac32_4](#bib.bib64)
    and AffectNet [ac32_5](#bib.bib71) and many other datasets. They show experimentally
    that using ACNNs improves the FER performance wherein the ACNNs shift attention
    from occluded facial regions to others which are not. They also show that their
    ACNN outperforms other state of the art techniques on several important FER datasets.
    However, the technique relies on landmarks. The authors intend to address this
    issue, as according them, ACNNs rely on face landmark localization units. Hence
    ACNNs have to be made more robust for generation of attention maps without landmarks,
    and this is an open area for research.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在[ac32](#bib.bib65)中，作者指出，尽管传统的面部表情识别（FER）系统在分析受限姿势时几乎是完美的，但对于现实世界中常见的部分遮挡姿势却表现不佳。因此，他们提出了一种基于注意力的卷积神经网络（ACNN），用于感知面部遮挡部位，重点关注高度具有区分性的未遮挡部分。他们的模型是端到端学习的。对于不同的兴趣区域（ROIs），他们引入了两种类型的ACNN，即基于补丁的类型和全局-局部类型。第一种类型仅在面部区域的局部补丁上使用注意力。第二种类型将局部特征（在补丁级别）与全局特征（在图像级别）结合起来。评估是在他们自己的面部表情数据集上进行的，该数据集具有自然场景中的遮挡，还有两个最大的自然场景面部表情数据集，即RAF-DB
    [ac32_4](#bib.bib64) 和 AffectNet [ac32_5](#bib.bib71) 以及其他多个数据集。他们通过实验证明，使用ACNN可以提高FER性能，其中ACNN将注意力从遮挡的面部区域转移到其他未遮挡的区域。他们还展示了他们的ACNN在多个重要的FER数据集上超越了其他最先进的技术。然而，该技术依赖于地标。作者打算解决这个问题，因为他们认为ACNN依赖于面部地标定位单元。因此，需要使ACNN在没有地标的情况下更具鲁棒性，以生成注意力图，这也是一个开放的研究领域。
- en: In the next sub-section, we turn to another important category of techniques
    of attention mechanisms and deep learning in machine vision, namely CNN transformer
    pipelines.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将转向机器视觉中注意力机制和深度学习的另一重要技术类别，即CNN transformer流水线。
- en: 3.2 CNN transformer Pipelines
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 CNN transformer 流水线
- en: In this sub-section, we discuss another important category of techniques of
    attention and deep learning in machine vision, namely the CNN transformer pipeline.
    Here a CNN is used to feed feature maps to a transformer, and acts like a teacher
    to the transformer, as will be discussed. The notable works falling under this
    category have been discussed below for each area of machine vision (MV).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们讨论了机器视觉中注意力和深度学习的另一重要技术类别，即CNN transformer流水线。在这里，CNN用于将特征图输入到transformer中，并作为transformer的“教师”，正如将要讨论的那样。下面讨论了在机器视觉（MV）各个领域下的这一类别的显著工作。
- en: 3.2.1 Image recognition
  id: totrans-117
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 图像识别
- en: Transformers are ’data-hungry’ in nature. For example a large-scale dataset
    like ImageNet [ac2_imagenet](#bib.bib15) is not sufficient to train a vision transformer
    from scratch. To address this issue, the work in [tv_12](#bib.bib90) proposes
    to distill information from a teacher CNN to a student transformer, in turn allowing
    training of the transformer only on ImageNet sans additional data. The data-efficient
    image transformer (DeiT) [tv_12](#bib.bib90) is a first in large scale image classification/retrieval
    without using a large-scale dataset like JFT [tv_39](#bib.bib2) . DeiT shows that
    transformers (requiring very large amounts of training data) can also be trained
    successfully on medium-sized datasets (e.g., 1.2M images as against 100M+ images
    used in ViT [tv_11](#bib.bib20) ) with shorter training time. An important contribution
    of DeiT is its novel native distillation technique [tv_78](#bib.bib36) which uses
    a teacher CNN (RegNetY-16GF [tv_79](#bib.bib74) ) whose outputs are fed to the
    transformer for training. The feature map outputs from the teacher CNN help the
    transformer (DeiT) in effectively finding important representations in input data
    images. The representations learned by DeiT are as good as top-performing CNNs
    like EfficientNet [tv_80](#bib.bib88) and also are efficiently applicable to various
    downstream image recognition tasks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器本质上是“数据饥饿型”的。例如，大规模数据集如 ImageNet [ac2_imagenet](#bib.bib15) 并不足以从零开始训练一个视觉变换器。为了解决这个问题，[tv_12](#bib.bib90)
    的工作提出了从教师 CNN 中提炼信息到学生变换器中，从而仅在 ImageNet 上训练变换器而无需额外数据。数据高效的图像变换器（DeiT） [tv_12](#bib.bib90)
    是在没有使用像 JFT [tv_39](#bib.bib2) 这样的规模数据集的大规模图像分类/检索中的首例。DeiT 表明，变换器（需要大量训练数据）也可以在中等规模的数据集上（例如，1.2M
    图像，与 ViT [tv_11](#bib.bib20) 使用的 100M+ 图像相比）成功训练，并且训练时间较短。DeiT 的一个重要贡献是其新颖的原生蒸馏技术
    [tv_78](#bib.bib36)，它使用教师 CNN（RegNetY-16GF [tv_79](#bib.bib74)）的输出供变换器进行训练。来自教师
    CNN 的特征图输出帮助变换器（DeiT）有效地在输入数据图像中找到重要的表示。DeiT 学到的表示与顶级 CNN，如 EfficientNet [tv_80](#bib.bib88)，一样优秀，并且在各种下游图像识别任务中也能高效应用。
- en: 3.2.2 Object detection
  id: totrans-119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 目标检测
- en: 'Like image classification/retrieval, transformers can be applied to image feature-map
    sets obtained from CNNs for precise object detection which involves prediction
    of object bounding boxes (BBoxes) and their corresponding category labels. In
    DETR [tv_13](#bib.bib8) , given spatial features obtained from a CNN backbone,
    the transformer encoder flattens the spatial axes along a single axis as shown
    in [5](#S3.F5 "Figure 5 ‣ 3.2.2 Object detection ‣ 3.2 CNN transformer Pipelines
    ‣ 3 Attention and deep learning in machine vision: Broad categories ‣ Attention
    mechanisms and deep learning for machine vision: A survey of the state of the
    art") which is feature map flattening from 3D to 1D. A sequence of features $(d\times
    n)$ is obtained with $d=$ feature dimension, and $n=h\times w$ ($[h,w]$ being
    the size of the feature map). Next, the 1D flattened features are encoded and
    decoded by the multi-head self-attention units as given in the work of [tv_1](#bib.bib93)
    .'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '类似于图像分类/检索，变换器也可以应用于从 CNNs 获得的图像特征图集，以实现精确的目标检测，其中涉及对目标边界框（BBoxes）及其对应类别标签的预测。在
    DETR [tv_13](#bib.bib8) 中，给定从 CNN 主干网络获得的空间特征，变换器编码器沿单一轴将空间轴展平，如[5](#S3.F5 "Figure
    5 ‣ 3.2.2 Object detection ‣ 3.2 CNN transformer Pipelines ‣ 3 Attention and deep
    learning in machine vision: Broad categories ‣ Attention mechanisms and deep learning
    for machine vision: A survey of the state of the art")所示，这是一种从 3D 到 1D 的特征图展平。得到的特征序列为$(d\times
    n)$，其中$d=$特征维度，$n=h\times w$（$[h,w]$为特征图的大小）。接下来，1D 展平的特征由多头自注意力单元进行编码和解码，如 [tv_1](#bib.bib93)
    的工作所述。'
- en: '![Refer to caption](img/fc4be473955df907268d222c6a963fcf.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/fc4be473955df907268d222c6a963fcf.png)'
- en: 'Figure 5: Overview of the DETR pipeline. [tv_13](#bib.bib8)'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：DETR 流水线概述。 [tv_13](#bib.bib8)
- en: 3.2.3 Multi-modal machine vision tasks
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 多模态机器视觉任务
- en: The machine vision (MV) tasks in this category include vision-language tasks
    like visual question-answering (VQA) [tv_135](#bib.bib4) , visual commonsense-reasoning
    (VSR) [tv_136](#bib.bib114) , crossmodal retrieval [tv_137](#bib.bib58) and image-captioning
    [tv_138](#bib.bib94) . There is a body of work for these areas within the scope
    of this paper, and the notable works have been mentioned here. In their work [tv_22](#bib.bib85)
    , the authors propose VL-BERT [tv_22](#bib.bib85) , one such technique for learning
    features which can be generalized to multi-modal MV downstream tasks like VSR
    and VQA. This technique involves aligning both visual as well as linguistic cues
    in order for learning compositely and effectively. For this, [tv_22](#bib.bib85)
    uses the BERT (Bidirectional encoder representations from transformers) [tv_3](#bib.bib17)
    architecture, and feeds it the features obtained from both visual and language
    domains. The language-features are the tokens in the input text sequences and
    the visual-features are the ROIs obtained from the input image by using a standard
    faster R-CNN model [tv_83](#bib.bib77) . Their performance on various multi-modal
    MV tasks shows the advantage of the proposed technique over conventional ’language
    only’ pre-training as done in the BERT [tv_3](#bib.bib17) .
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这一类别中的机器视觉（MV）任务包括视觉-语言任务，如视觉问答（VQA） [tv_135](#bib.bib4) 、视觉常识推理（VSR） [tv_136](#bib.bib114)
    、跨模态检索 [tv_137](#bib.bib58) 和图像描述 [tv_138](#bib.bib94) 。在本文的范围内，这些领域已有一定的研究工作，并在此提及了显著的研究成果。在他们的工作
    [tv_22](#bib.bib85) 中，作者提出了VL-BERT [tv_22](#bib.bib85) ，这是一种用于学习特征的技术，可以推广到多模态MV下游任务，如VSR和VQA。该技术涉及对齐视觉和语言线索，以便进行综合和有效的学习。为此，[tv_22](#bib.bib85)
    使用了BERT（Bidirectional Encoder Representations from Transformers） [tv_3](#bib.bib17)
    架构，并将从视觉和语言领域获得的特征输入到该架构中。语言特征是输入文本序列中的标记，而视觉特征是通过使用标准的faster R-CNN模型 [tv_83](#bib.bib77)
    从输入图像中获得的ROIs。他们在各种多模态MV任务中的表现显示了所提技术相对于传统的“仅语言”预训练（如在BERT [tv_3](#bib.bib17)
    中所做的）的优势。
- en: 3.2.4 Video understanding
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 视频理解
- en: Videos which are audiovisual data are abundantly found. In spite of this, the
    contemporary techniques tend to learn from short videos (up to few seconds) allowing
    them to interpret usually short-range relationships [tv_1](#bib.bib93) ; [tv_29](#bib.bib37)
    . Long-range relationship learning is needed in different uni-modal and multi-modal
    MV tasks like activity recognition [tv_67](#bib.bib44) ; [tv_150](#bib.bib9) ;
    [tv_151](#bib.bib25) ; [tv_152](#bib.bib80) ; [tv_153](#bib.bib98) . In this section,
    we highlight some recent techniques from the CNN transformer pipeline domain which
    seek to address this issue better than transformer networks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 视频作为视听数据非常普遍。尽管如此，现代技术通常从短视频（最多几秒钟）中学习，使其能够解释通常的短距离关系 [tv_1](#bib.bib93) ; [tv_29](#bib.bib37)
    。然而，在不同的单模态和多模态MV任务中，如活动识别 [tv_67](#bib.bib44) ; [tv_150](#bib.bib9) ; [tv_151](#bib.bib25)
    ; [tv_152](#bib.bib80) ; [tv_153](#bib.bib98) ，需要学习长距离关系。本节中，我们重点介绍了一些来自CNN变换器管道领域的近期技术，这些技术旨在比变换器网络更好地解决这一问题。
- en: 'In their work [tv_154](#bib.bib118) , the authors study the problem of dense-video
    captioning with transformers. This requires producing language data for every
    event occurring in the video. The earlier techniques used for the same usually
    proceed sequentially: event-detection followed by caption-generation inside distinct
    sub-blocks. The authors of [tv_154](#bib.bib118) propose a unified transformer
    architecture which learns one model for tackling both the aforementioned tasks
    jointly. Thus the proposed technique combines both the multi-modal MV tasks of
    event-detection and caption-generation. In the first stage, a video-encoder has
    been used for obtain frame wise features, which is followed by 2 decoder units
    which propose relevant events and related captions. As a matter of fact, [tv_154](#bib.bib118)
    is the first technique for dense-video captioning without using recurrent models.
    It uses self-attention based encoder which is fed CNN output features. Experimentation
    on ActivityNet Captions [tv_155](#bib.bib46) and YouCookII [tv_156](#bib.bib117)
    datasets reported valuable improvement over earlier RNN and double-staged techniques.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的工作 [tv_154](#bib.bib118) 中，作者研究了使用变换器进行密集视频字幕生成的问题。这需要为视频中发生的每个事件生成语言数据。早期用于同一问题的技术通常按顺序进行：事件检测，然后在不同的子块中生成字幕。
    [tv_154](#bib.bib118) 的作者提出了一种统一的变换器架构，该架构学习一个模型以联合处理上述两个任务。因此，所提出的技术结合了事件检测和字幕生成的多模态
    MV 任务。在第一阶段，使用视频编码器获取逐帧特征，接着是两个解码器单元，提出相关事件和相关字幕。实际上，[tv_154](#bib.bib118) 是首个不使用递归模型的密集视频字幕生成技术。它使用基于自注意力的编码器，并将
    CNN 输出特征作为输入。对 ActivityNet Captions [tv_155](#bib.bib46) 和 YouCookII [tv_156](#bib.bib117)
    数据集的实验报告显示，相比早期的 RNN 和双阶段技术有了显著的改进。
- en: In their work [tv_134](#bib.bib59) , the authors have noted in their work that
    in the multi-modal MV task learning techniques like VideoBERT [tv_17](#bib.bib86)
    and ViLBERT [tv_133](#bib.bib68) the language-processing part is generally kept
    fixed for a pre-trained model like BERT [tv_3](#bib.bib17) for reducing the training
    complexity. As an alternative and also as a first, they have proposed PEMT, a
    multi-modal bidirectional transformer which can learn end-to-end audio-visual
    video data. In their model, short-term dependencies are first learnt using CNNs,
    and this is followed by a long-term dependency learning unit. The technique uses
    CNN features learned during its training for selection of negative samples which
    are similar to positive samples. The results obtained show that the concept has
    good implications on multi-modal task model performance.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的工作 [tv_134](#bib.bib59) 中，作者指出在多模态 MV 任务学习技术中，如 VideoBERT [tv_17](#bib.bib86)
    和 ViLBERT [tv_133](#bib.bib68)，语言处理部分通常固定为预训练模型 BERT [tv_3](#bib.bib17) 以减少训练复杂性。作为替代方案和首次尝试，他们提出了
    PEMT，一种多模态双向变换器，可以学习端到端的音视频数据。在他们的模型中，首先使用 CNN 学习短期依赖，然后是长期依赖学习单元。该技术在训练过程中使用学习到的
    CNN 特征来选择与正样本类似的负样本。结果表明，这一概念对多模态任务模型的性能有良好的影响。
- en: 'Traditionally, CNN-based techniques for video classification usually performed
    3D spatio-temporal manipulation on relatively small intervals for video understanding.
    In their work [tv_160](#bib.bib7) , the authors have proposed the video transformer
    network (VTN) which first obtains frame features from a 2D CNN then applies a
    transformer encoder for learning temporal relationships. There are 2 advantages
    of using transformer encoder for the spatial features: (i) whole video is processed
    in a single pass, and (ii) training and efficiency are improved considerably by
    avoiding 3D convolution which is expensive.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，基于 CNN 的视频分类技术通常在相对较小的时间间隔内进行 3D 时空操作以理解视频。在他们的工作 [tv_160](#bib.bib7) 中，作者提出了视频变换器网络（VTN），该网络首先通过
    2D CNN 获取帧特征，然后应用变换器编码器以学习时间关系。使用变换器编码器处理空间特征有 2 个优点：（i）整个视频在一次传递中处理，（ii）通过避免成本高昂的
    3D 卷积，训练和效率得到了显著改善。
- en: '![Refer to caption](img/3b8cac8f8e6422e48b6949d6a5a6c901.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![参考字幕](img/3b8cac8f8e6422e48b6949d6a5a6c901.png)'
- en: 'Figure 6: Video transformer network (VTN) architecture [tv_160](#bib.bib7)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 视频变换器网络（VTN）架构 [tv_160](#bib.bib7)'
- en: 'These feats make VTN suitable for learning from long videos in which inter-entity
    interactions are spread length-wise. The experiments of the authors on the Kinetics-400
    dataset [tv_67](#bib.bib44) with various CNN and non-CNN backbones e.g. ResNet
    [tv_59](#bib.bib34) , ViT [tv_11](#bib.bib20) and DeiT [tv_12](#bib.bib90) , show
    good performance. [6](#S3.F6 "Figure 6 ‣ 3.2.4 Video understanding ‣ 3.2 CNN transformer
    Pipelines ‣ 3 Attention and deep learning in machine vision: Broad categories
    ‣ Attention mechanisms and deep learning for machine vision: A survey of the state
    of the art") shows the overall schematic of the proposed model.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '这些成就使得VTN适合于从长视频中学习，其中实体间的交互在长度上分布。作者在Kinetics-400数据集 [tv_67](#bib.bib44) 上进行的实验，使用了各种CNN和非CNN骨干网络，例如ResNet
    [tv_59](#bib.bib34) 、ViT [tv_11](#bib.bib20) 和DeiT [tv_12](#bib.bib90) ，表现良好。[6](#S3.F6
    "Figure 6 ‣ 3.2.4 Video understanding ‣ 3.2 CNN transformer Pipelines ‣ 3 Attention
    and deep learning in machine vision: Broad categories ‣ Attention mechanisms and
    deep learning for machine vision: A survey of the state of the art") 显示了所提出模型的整体示意图。'
- en: In the next sub-section, we turn to third category of techniques of attention
    mechanisms and deep learning in machine vision, i.e. hybrid transformers.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个子节中，我们转向机器视觉中注意机制和深度学习的第三类技术，即混合变换器。
- en: 3.3 Hybrid transformers
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 混合变换器
- en: Transformers used to be exclusively attention based networks. However, some
    recent works have introduced two new variants i.e. convolutional vision transformers
    (CvTs) and hybrid CNN-transformer models. These variants are discussed below.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器曾经是专门基于注意力的网络。然而，最近一些研究引入了两种新的变体，即卷积视觉变换器（CvTs）和混合CNN-变换器模型。下面讨论这些变体。
- en: 3.3.1 Convolutional vision transformers
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 卷积视觉变换器
- en: In natural language processing (NLP) and speech recognition (SR), convolutional
    operations were used for modification of the transformer unit, either by changing
    the multi-head attention blocks with convolutional layers [ct_38](#bib.bib102)
    , or by adding more parallel convolutional layers [ct_39](#bib.bib104) or more
    sequential convolutional layers [ct_13](#bib.bib28) , in order to capture local
    dependencies. Earlier research [ct_37](#bib.bib99) proposed propagation of the
    attention maps to following layers by residual connections being transformed by
    convolutional operations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理（NLP）和语音识别（SR）中，卷积操作被用于修改变换器单元，无论是通过用卷积层替换多头注意力块 [ct_38](#bib.bib102)
    ，还是通过增加更多并行卷积层 [ct_39](#bib.bib104) 或更多顺序卷积层 [ct_13](#bib.bib28) 来捕捉局部依赖。早期的研究
    [ct_37](#bib.bib99) 提出了通过卷积操作变换的残差连接将注意力图传播到后续层。
- en: Convolutional vision transformers (CvTs) [ct](#bib.bib103) improve the vision
    transformer (ViT) both in terms of performance and efficiency with the introduction
    of convolutions into ViT for yielding the best of both architectures. This has
    been achieved through 2 important modifications. First, a range of transformers
    with a novel convolutional token-embedding and second, a convolutional transformer
    unit giving convolutional projections. Thus they propose introduction of convolutional
    operations to 2 primary parts of the ViT viz., first, replacement of the ’linear
    projection’ used for every position in the attention mechanism with their novel
    ’convolutional projection’, and second, use of their hierarchical multistage architecture
    for enabling variable resolution of two-dimensional reshaped tokens just like
    CNNs. These fundamental changes have introduced desirable properties of CNNs to
    ViTs i.e., shift-, scale-, and distortion-invariance, while at the same time have
    maintained the merits of transformers i.e. global context, dynamic attention,
    and higher level of generalization. The authors validate CvT through extensive
    experimentation showing that their technique achieves state of the art performance
    as compared to other ViTs and ResNets on the ImageNet-1k dataset, with lesser
    parameters and lesser FLOPs. Also, the performance gains stay when CvT is pre-trained
    on larger datasets like ImageNet-22k [ct_9](#bib.bib16) and is subsequently fine-tuned
    for downstream tasks. Pre-training on ImageNet-22k leads to top-1 accuracy of
    87.7% for the ImageNet-1k validation set. Lastly, their results demonstrate that
    positional encoding which is an important component in existing ViTs, can be suitably
    removed in CvT thus simplifying its architecture for higher resolution MV tasks.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积视觉变换器（CvTs）[ct](#bib.bib103) 通过将卷积引入视觉变换器（ViT），在性能和效率方面都得到了提升，从而融合了这两种架构的优点。这是通过两项重要修改实现的。首先，一系列变换器采用了新型卷积的
    token 嵌入；其次，卷积变换单元提供了卷积投影。因此，他们建议将卷积操作引入 ViT 的两个主要部分：首先，将注意机制中每个位置使用的‘线性投影’替换为他们的新型‘卷积投影’；其次，使用他们的层次多阶段架构，实现二维重塑
    token 的可变分辨率，就像 CNNs 一样。这些根本性的变化将 CNN 的期望属性引入了 ViTs，即位移、缩放和畸变不变性，同时保留了变换器的优点，即全局上下文、动态注意力和更高的泛化能力。作者通过大量实验验证了
    CvT，结果表明，与其他 ViTs 和 ResNets 在 ImageNet-1k 数据集上的表现相比，他们的技术在参数数量和 FLOPs 上都更少，并且达到了最先进的性能。此外，当
    CvT 在更大的数据集如 ImageNet-22k [ct_9](#bib.bib16) 上进行预训练后，并随后进行下游任务的微调时，性能提升依然保持。预训练在
    ImageNet-22k 上使得 ImageNet-1k 验证集的 top-1 准确率达到了 87.7%。最后，他们的结果表明，现有 ViTs 中重要的成分——位置编码，在
    CvT 中可以适当去除，从而简化了其架构，以更好地适应高分辨率 MV 任务。
- en: 3.3.2 Hybrid CNN-transformer models
  id: totrans-139
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 混合 CNN-Transformer 模型
- en: A wide range of recent developments in handcrafted neural network models for
    machine vision tasks have asserted the important need for exploration of hybrid
    models which consist of diverse building blocks. At the same time, neural network
    model searching techniques are surging with expectations of reduction in human
    effort. In evidence brought out by some works [hc_19](#bib.bib19) ; [hc_61](#bib.bib84)
    ; [hc_3](#bib.bib6) it is stated that hybrids of convolutional neural networks
    (CNNs) and transformers can perform better that both pure CNNs and pure transformers.
    In spite of this, the question that whether neural architecture search (NAS) methods
    can handle different search spaces with different candidates like CNNs and transformers,
    effectively and efficiently, leads to an open research area. In their work [hc](#bib.bib60)
    , the authors propose the ’block-wisely self-supervised neural architecture search’
    (BossNAS) which is an unsupervised NAS technique which addresses the issue of
    inaccurate model rating due to large weight-sharing space and supervision with
    bias as undertaken in earlier techniques. Going into specifics, they factorize
    the search-space into smaller blocks and also utilize a new self-supervision based
    training technique called ’ensemble bootstrapping’, for training every block individually
    prior to search. Also, they propose a search-space called HyTra which is like
    a hybrid search-space fabric of CNNs and transformers. The fabric like search-space
    consists of model architectures similar to the common ViTs [hc_19](#bib.bib19)
    ; [hc_65](#bib.bib91) ; [hc_13](#bib.bib12) , CNNs [hc_24](#bib.bib35) ; [hc_30](#bib.bib39)
    and hybrid CNN-transformers [hc_61](#bib.bib84) at various scales. Over the same
    difficult search-space, their searched hybrid model viz. BossNet-T yields 82.2%
    accuracy for ImageNet, going beyond EfficientNet by a margin of 2.1% with similar
    computation time. Also, they report that their technique achieves better model
    rating accuracy on the MBConv search-space for ImageNet and on NATS-Bench size
    search-space for CIFAR-100 than the state of the art NAS techniques. The code
    and the pre-trained models are available online at https://github.com/changlin31/BossNAS.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在手工设计的神经网络模型用于机器视觉任务方面有了广泛的发展，这突显了探索由多样化构建模块组成的混合模型的必要性。同时，神经网络模型搜索技术的兴起寄托了减少人工努力的期望。一些研究表明，卷积神经网络（CNNs）和变换器的混合体可能比纯CNNs和纯变换器表现更好。尽管如此，神经架构搜索（NAS）方法是否能够有效和高效地处理具有不同候选者（如CNNs和变换器）的不同搜索空间仍然是一个开放的研究领域。在他们的工作中，作者提出了“块级自监督神经架构搜索”（BossNAS），这是一种无监督的NAS技术，解决了由于大型权重共享空间和监督偏差导致的模型评分不准确的问题。具体来说，他们将搜索空间因子化为较小的块，并利用一种称为“集成自举”的新自监督训练技术，对每个块进行单独训练，然后再进行搜索。此外，他们提出了一种称为HyTra的搜索空间，这类似于CNNs和变换器的混合搜索空间。该搜索空间包含了类似于常见ViTs、CNNs和混合CNN-变换器的模型架构，涵盖了不同的尺度。在同样困难的搜索空间中，他们搜索到的混合模型BossNet-T在ImageNet上达到了82.2%的准确率，超越了EfficientNet
    2.1%的准确率差距，同时计算时间相似。他们还报告称，在MBConv搜索空间和CIFAR-100的NATS-Bench搜索空间上，他们的技术在模型评分准确性上优于现有的NAS技术。代码和预训练模型可以在
    https://github.com/changlin31/BossNAS 在线获取。
- en: In the next section, we discuss the major research algorithms, issues and trends
    in techniques of attention and deep learning in machine vision.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们讨论机器视觉中注意力机制和深度学习技术的主要研究算法、问题和趋势。
- en: 4 Major research algorithms, issues and trends
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 主要研究算法、问题和趋势
- en: In the field of machine vision (MV), recently attention based mechanisms are
    generating a lot of interest. Pure attention based architectures/models are slowly
    and steadily proving worthy of loosening the grip of deep learning over MV as
    interesting and efficient attention based models continue to be built. However,
    pure attention based models come with their own set of issues. They are quite
    ’data-hungry’ as they require huge amounts of data to pre-train before being able
    to be applied to MV downstream tasks after fine-tuning. As an example, vision
    transformers have to be pre-trained on the JFT dataset [tv_39](#bib.bib2) which
    consists of 300 million images, and subsequently have to be fine-tuned on ImageNet-1K
    [ac2_imagenet](#bib.bib15) before they can be used for MV tasks like image classification/retrieval.
    Also, the training times are exceedingly long for pre-training in transformers.
    Hence, reducing the ’hunger/appetite’ of transformers is an open research area.
    Also, reducing the training time of transformers by using efficient architectures
    and training techniques is also an open research area. Reducing the computational
    load/resources for training of vision based transformers is also an open research
    area besides finding novel ways to port them to limited hardware/resource (portable)
    platforms available in the industry. A very large body of research work is present
    on deep learning and CNN based architectures and transformers can benefit from
    the same, as CNN based models have taken a foothold in MV. The industrial footprint
    of deep learning and CNN based models is also large. Attention based models can
    benefit from the work done and industrial footprint of deep learning based models.
    Some works [hc_19](#bib.bib19) ; [hc_61](#bib.bib84) ; [hc_3](#bib.bib6) state
    that hybrids of convolutional neural networks (CNNs) and transformers can perform
    better that both pure CNNs and pure transformers.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器视觉（MV）领域，最近基于注意力的机制引起了大量关注。纯粹基于注意力的架构/模型正在稳步证明其在MV领域中值得摆脱深度学习的控制，因为有趣且高效的基于注意力的模型不断被构建。然而，纯粹基于注意力的模型有其自身的问题。它们非常“数据饥饿”，需要大量的数据进行预训练，然后才能在微调后应用于MV下游任务。例如，视觉变换器必须在包含3亿张图像的JFT数据集
    [tv_39](#bib.bib2) 上进行预训练，并且随后必须在ImageNet-1K [ac2_imagenet](#bib.bib15) 上进行微调，才能用于图像分类/检索等MV任务。此外，变换器的预训练时间极其漫长。因此，减少变换器的“饥饿/需求”是一个开放的研究领域。此外，通过使用高效的架构和训练技术来减少变换器的训练时间也是一个开放的研究领域。减少视觉变换器训练的计算负担/资源也是一个开放的研究领域，此外还需要找到新的方法将其移植到工业中有限的硬件/资源（便携）平台上。深度学习和CNN基础架构的研究工作非常丰富，变换器可以从中受益，因为CNN基础模型在MV中已占据了一席之地。深度学习和CNN基础模型的工业足迹也很大。基于注意力的模型可以从深度学习模型的工作和工业足迹中受益。一些研究
    [hc_19](#bib.bib19) ; [hc_61](#bib.bib84) ; [hc_3](#bib.bib6) 表明，卷积神经网络（CNNs）和变换器的混合模型可能比纯CNN和纯变换器表现得更好。
- en: Currently, the algorithms applicable to transformers benefitting from deep learning
    and CNN architecture are present in three main categories as discussed earlier.
    The first category being attention-based CNNs. The algorithms in this category
    aim to augment the performance of classical CNN architectures by plugging into
    them attention-based components/units in order to refine the features as and when
    they are used. Attention based CNN plugins like CBAM have been used successfully
    in various CNNs models/architectures to boost their performance at relatively
    small computational time overhead. In spite of this, the amount of attention available
    in this category is limited and the CNNs use the attention based mechanisms sparingly.
    Deeper integration and merging of attention based mechanisms and CNNs are required
    before outstanding and record breaking performances can be achieved. Coming to
    the second category of CNN transformer pipelines which has also been discussed,
    the pipeline is just like the earlier hybrid two-stage classifiers wherein a feature
    map generated by a ’teacher’ CNN is fed to a waiting ’student’ classifier which
    operates on this feature map. In this two-stage model, it is safe to say that
    the performance of the second-stage model depends on the image/video interpretation
    capability/capacity of the CNN. As such the architecture/design of the first-stage
    CNN is in question regarding its design-based efficacy at efficiently interpreting
    the image/video data. And it is known that there are currently a large number
    of CNN architectures available and making the correct choice is an open research
    field. Coming to the third category of Hybrid CNN-transformers, the merging of
    these two different techniques is a difficult one. Network architecture search
    (NAS) has been used to search through hybrid CNN-transformer search-space fabric.
    However, given its exhaustive nature requiring large computational resources and
    careful fabric design, the optimization of the same is also an open research area.
    In spite of the limitations and issues mentioned above, attention based mechanisms
    like vision transformers (ViTs) are considered having potential to impact the
    MV research and industrial body in the future. Combined with the power and experience
    of deep learning, the merger of the two techniques can prove to be revolutionary
    for both the existing and as well as the upcoming machine vision (MV) tasks/applications,
    as new, larger and more efficient computational hardware and software continue
    to be developed.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，适用于变换器的算法，受益于深度学习和 CNN 架构，如之前讨论的，主要分为三大类。第一类是基于注意力的 CNN。该类别的算法旨在通过将基于注意力的组件/单元插入到经典
    CNN 架构中，以在使用过程中提炼特征，从而提升经典 CNN 架构的性能。像 CBAM 这样的基于注意力的 CNN 插件已经在各种 CNN 模型/架构中成功应用，以在相对较小的计算时间开销下提升其性能。尽管如此，这一类别中的注意力量仍然有限，CNN
    对基于注意力的机制的使用也较为节制。在实现卓越和打破记录的性能之前，需要更深入地整合和融合基于注意力的机制和 CNN。第二类 CNN 变换器流水线，也如前所述，该流水线类似于早期的混合两阶段分类器，其中由“教师”CNN
    生成的特征图被输入到等待中的“学生”分类器，该分类器在此特征图上操作。在这一两阶段模型中，可以安全地说，第二阶段模型的性能依赖于 CNN 的图像/视频解释能力/容量。因此，第一阶段
    CNN 的架构/设计在高效解释图像/视频数据方面的设计效果受到质疑。目前已知存在大量的 CNN 架构，选择正确的架构仍是一个开放的研究领域。第三类是混合 CNN-变换器，将这两种不同技术融合起来是一个困难的任务。网络架构搜索（NAS）已被用来在混合
    CNN-变换器搜索空间中进行搜索。然而，考虑到其需要大量计算资源和精心设计的全面性，优化这一点也是一个开放的研究领域。尽管存在上述限制和问题，基于注意力的机制，如视觉变换器（ViTs），被认为在未来对机器视觉（MV）研究和工业领域具有潜在影响。结合深度学习的力量和经验，这两种技术的融合可能对现有和即将出现的机器视觉（MV）任务/应用产生革命性的影响，随着新的、更大、更高效的计算硬件和软件的不断发展。
- en: 5 Conclusion
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: In this paper, the merger of attention based mechanisms and deep learning for
    various machine vision (MV) tasks/applications has been discussed. In the beginning
    of the paper, various types of attention mechanism were briefly discussed. Next,
    various attention based architectures were discussed. This was followed by discussing
    various categories of combinations of attention mechanisms and deep learning techniques
    for machine vision (MV). The various architectures and their associated machine
    vision tasks/applications were discussed. Afterwards, major research algorithms,
    issues and trends within the scope of the paper were discussed. By using 110+
    papers as research reference in this survey, the readers of this paper are expected
    to form a knowledge-base and get a head-start in the area of combinational techniques
    of attention based mechanisms and deep learning for machine vision.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本文探讨了基于注意力机制和深度学习的机器视觉（MV）任务/应用的融合。在论文的开始部分，简要讨论了各种类型的注意力机制。接下来，讨论了各种基于注意力的架构。随后，讨论了注意力机制和深度学习技术在机器视觉（MV）中的各种组合类别。讨论了各种架构及其相关的机器视觉任务/应用。之后，讨论了论文范围内主要的研究算法、问题和趋势。通过使用110+篇论文作为研究参考，期望读者能够形成知识库，并在基于注意力机制和深度学习的机器视觉组合技术领域获得一个良好的起点。
- en: Conflict of interest
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 利益冲突
- en: The authors declare no conflict of interest.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 作者声明没有利益冲突。
- en: References
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) URL https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) URL https://www.kaggle.com/c/challenges-in-representation-learning-facial-expression-recognition-challenge/data
- en: (2) Revisiting the unreasonable effectiveness of data. URL https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) 重新审视数据的非凡有效性。URL https://ai.googleblog.com/2017/07/revisiting-unreasonable-effectiveness.html
- en: '(3) Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
    L.: Bottom-up and top-down attention for image captioning and visual question
    answering. In: 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pp. 6077–6086 (2018). DOI 10.1109/CVPR.2018.00636'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Anderson, P., He, X., Buehler, C., Teney, D., Johnson, M., Gould, S., Zhang,
    L.：自下而上和自上而下的注意力机制用于图像描述和视觉问答。在：2018 IEEE/CVF 计算机视觉与模式识别会议，页码6077–6086（2018年）。DOI
    10.1109/CVPR.2018.00636
- en: '(4) Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L.,
    Parikh, D.: Vqa: Visual question answering. In: 2015 IEEE International Conference
    on Computer Vision (ICCV), pp. 2425–2433 (2015). DOI 10.1109/ICCV.2015.279'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) Antol, S., Agrawal, A., Lu, J., Mitchell, M., Batra, D., Zitnick, C.L.,
    Parikh, D.：Vqa：视觉问答。在：2015 IEEE 国际计算机视觉会议（ICCV），页码2425–2433（2015年）。DOI 10.1109/ICCV.2015.279
- en: '(5) Ba, J., Mnih, V., Kavukcuoglu, K.: Multiple object recognition with visual
    attention (2015)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) Ba, J., Mnih, V., Kavukcuoglu, K.：基于视觉注意力的多目标识别（2015年）
- en: '(6) Bello, I.: Lambdanetworks: Modeling long-range interactions without attention.
    In: International Conference on Learning Representations (2021). URL https://openreview.net/forum?id=xTJEN-ggl1b'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) Bello, I.：LambdaNetworks：无注意力建模长程交互。在：国际学习表示会议（2021年）。URL https://openreview.net/forum?id=xTJEN-ggl1b
- en: '(7) Berg, A., O’Connor, M., Cruz, M.T.: Keyword transformer: A self-attention
    model for keyword spotting (2021)'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) Berg, A., O’Connor, M., Cruz, M.T.：关键词变换器：用于关键词检测的自注意力模型（2021年）
- en: '(8) Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko,
    S.: End-to-end object detection with transformers. In: A. Vedaldi, H. Bischof,
    T. Brox, J.M. Frahm (eds.) Computer Vision - ECCV 2020, pp. 213–229\. Springer
    International Publishing, Cham (2020)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov, A., Zagoruyko,
    S.：基于变换器的端到端目标检测。在：A. Vedaldi, H. Bischof, T. Brox, J.M. Frahm（编）计算机视觉 - ECCV
    2020，页码213–229。施普林格国际出版社，尚（2020年）
- en: '(9) Carreira, J., Noland, E., Hillier, C., Zisserman, A.: A short note on the
    kinetics-700 human action dataset (2019)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) Carreira, J., Noland, E., Hillier, C., Zisserman, A.：关于 kinetics-700 人类动作数据集的简短说明（2019年）
- en: '(10) Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C.,
    Xu, C., Gao, W.: Pre-trained image processing transformer (2020)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) Chen, H., Wang, Y., Guo, T., Xu, C., Deng, Y., Liu, Z., Ma, S., Xu, C.,
    Xu, C., Gao, W.：预训练的图像处理变换器（2020年）
- en: '(11) Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S.:
    Sca-cnn: Spatial and channel-wise attention in convolutional networks for image
    captioning. In: 2017 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 6298–6306 (2017). DOI 10.1109/CVPR.2017.667'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Chen, L., Zhang, H., Xiao, J., Nie, L., Shao, J., Liu, W., Chua, T.S.：SCA-CNN：用于图像描述的空间和通道注意力的卷积网络。在：2017
    IEEE 计算机视觉与模式识别会议（CVPR），页码6298–6306（2017年）。DOI 10.1109/CVPR.2017.667
- en: '(12) Chu, X., Tian, Z., Zhang, B., Wang, X., Wei, X., Xia, H., Shen, C.: Conditional
    positional encodings for vision transformers (2021)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(12) Chu, X., Tian, Z., Zhang, B., Wang, X., Wei, X., Xia, H., Shen, C.: 面向视觉变换器的条件位置编码
    (2021)'
- en: '(13) Corbetta, M., Shulman, G.L.: Control of goal-directed and stimulus-driven
    attention in the brain. Nature reviews neuroscience 3(3), 201–215 (2002)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(13) Corbetta, M., Shulman, G.L.: 大脑中目标导向和刺激驱动注意力的控制。《自然评论神经科学》3(3), 201–215
    (2002)'
- en: '(14) Cornia, M., Baraldi, L., Serra, G., Cucchiara, R.: A deep multi-level
    network for saliency prediction. In: 2016 23rd International Conference on Pattern
    Recognition (ICPR), pp. 3488–3493 (2016). DOI 10.1109/ICPR.2016.7900174'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(14) Cornia, M., Baraldi, L., Serra, G., Cucchiara, R.: 一种深度多层网络用于显著性预测。在：2016年第23届国际模式识别大会（ICPR），页码3488–3493
    (2016)。DOI 10.1109/ICPR.2016.7900174'
- en: '(15) Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet:
    A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 248–255 (2009). DOI 10.1109/CVPR.2009.5206848'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(15) Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet：大规模层次图像数据库。在：2009年IEEE计算机视觉与模式识别会议，页码248–255
    (2009)。DOI 10.1109/CVPR.2009.5206848'
- en: '(16) Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet:
    A large-scale hierarchical image database. In: 2009 IEEE Conference on Computer
    Vision and Pattern Recognition, pp. 248–255 (2009). DOI 10.1109/CVPR.2009.5206848'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(16) Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L.: Imagenet：大规模层次图像数据库。在：2009年IEEE计算机视觉与模式识别会议，页码248–255
    (2009)。DOI 10.1109/CVPR.2009.5206848'
- en: '(17) Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT: Pre-training of
    deep bidirectional transformers for language understanding. In: Proceedings of
    the 2019 Conference of the North American Chapter of the Association for Computational
    Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.
    4171–4186\. Association for Computational Linguistics, Minneapolis, Minnesota
    (2019). DOI 10.18653/v1/N19-1423. URL https://www.aclweb.org/anthology/N19-1423'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(17) Devlin, J., Chang, M.W., Lee, K., Toutanova, K.: BERT：用于语言理解的深度双向变换器的预训练。在：2019年北美计算语言学协会人类语言技术会议论文集，第1卷（长篇和短篇论文），页码4171–4186。计算语言学协会，明尼阿波利斯，明尼苏达州
    (2019)。DOI 10.18653/v1/N19-1423。URL https://www.aclweb.org/anthology/N19-1423'
- en: '(18) Doersch, C., Gupta, A., Zisserman, A.: Crosstransformers: spatially-aware
    few-shot transfer (2021)'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(18) Doersch, C., Gupta, A., Zisserman, A.: Crosstransformers：空间感知的少样本迁移 (2021)'
- en: '(19) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., Dehghani, M., Mindere, M., Heigold, G., Gelly, S., Uszkoreit,
    J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition
    at scale (2020)'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(19) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., Dehghani, M., Mindere, M., Heigold, G., Gelly, S., Uszkoreit,
    J., Houlsby, N.: 一张图像值16x16个单词：大规模图像识别的变换器 (2020)'
- en: '(20) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit,
    J., Houlsby, N.: An image is worth 16x16 words: Transformers for image recognition
    at scale (2020)'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(20) Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X.,
    Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit,
    J., Houlsby, N.: 一张图像值16x16个单词：大规模图像识别的变换器 (2020)'
- en: '(21) Er, M.J., Zhang, Y., Wang, N., Pratama, M.: Attention pooling-based convolutional
    neural network for sentence modelling. Information Sciences 373, 388–403 (2016).
    DOI 10.1016/j.ins.2016.08.084. URL https://www.sciencedirect.com/science/article/pii/S0020025516306673'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(21) Er, M.J., Zhang, Y., Wang, N., Pratama, M.: 基于注意力池化的卷积神经网络用于句子建模。《信息科学》373,
    388–403 (2016)。DOI 10.1016/j.ins.2016.08.084。URL https://www.sciencedirect.com/science/article/pii/S0020025516306673'
- en: '(22) Escalera, S., Baró, X., Gonzàlez, J., Bautista, M.A., Madadi, M., Reyes,
    M., Ponce-López, V., Escalante, H.J., Shotton, J., Guyon, I.: Chalearn looking
    at people challenge 2014: Dataset and results. In: L. Agapito, M.M. Bronstein,
    C. Rother (eds.) Computer Vision - ECCV 2014 Workshops, pp. 459–473\. Springer
    International Publishing, Cham (2015)'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(22) Escalera, S., Baró, X., Gonzàlez, J., Bautista, M.A., Madadi, M., Reyes,
    M., Ponce-López, V., Escalante, H.J., Shotton, J., Guyon, I.: Chalearn 2014年关注人类挑战：数据集和结果。在：L.
    Agapito, M.M. Bronstein, C. Rother (编辑)《计算机视觉 - ECCV 2014工作坊》，页码459–473。施普林格国际出版社，香农
    (2015)'
- en: '(23) Everingham, M., Williams, C.K.: The pascal visual object classes challenge
    2007 (voc2007) results'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(23) Everingham, M., Williams, C.K.: Pascal视觉对象类别挑战2007 (voc2007) 结果'
- en: '(24) Gessert, N., Sentker, T., Madesta, F., Schmitz, R., Kniep, H., Baltruschat,
    I., Werner, R., Schlaefer, A.: Skin lesion classification using cnns with patch-based
    attention and diagnosis-guided loss weighting. IEEE Transactions on Biomedical
    Engineering 67(2), 495–503 (2020). DOI 10.1109/TBME.2019.2915839'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(24) Gessert, N., Sentker, T., Madesta, F., Schmitz, R., Kniep, H., Baltruschat,
    I., Werner, R., Schlaefer, A.: 使用带有基于补丁的注意力和诊断指导损失加权的卷积神经网络进行皮肤病变分类。IEEE生物医学工程汇刊
    67(2), 495–503（2020）。DOI 10.1109/TBME.2019.2915839'
- en: '(25) Ging, S., Zolfaghari, M., Pirsiavash, H., Brox, T.: Coot: Cooperative
    hierarchical transformer for video-text representation learning (2020)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(25) Ging, S., Zolfaghari, M., Pirsiavash, H., Brox, T.: Coot: 协作层次变换器用于视频-文本表示学习（2020）'
- en: '(26) Girdhar, R., João Carreira, J., Doersch, C., Zisserman, A.: Video action
    transformer network. In: 2019 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), pp. 244–253 (2019). DOI 10.1109/CVPR.2019.00033'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(26) Girdhar, R., João Carreira, J., Doersch, C., Zisserman, A.: 视频动作转换网络。见：2019
    IEEE/CVF计算机视觉与模式识别会议（CVPR），第244–253页（2019）。DOI 10.1109/CVPR.2019.00033'
- en: '(27) Goodfellow, I., Bengio, Y., Courville, A.: Deep Learning. MIT Press (2016)'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(27) Goodfellow, I., Bengio, Y., Courville, A.: 深度学习。麻省理工学院出版社（2016）'
- en: '(28) Gulati, A., Qin, J., Chiu, C.C., Parmar, N., Zhang, Y., Yu, J., Han, W.,
    Wang, S., Zhang, Z., Wu, Y., Pang, R.: Conformer: Convolution-augmented Transformer
    for Speech Recognition. In: Proc. Interspeech 2020, pp. 5036–5040 (2020). DOI 10.21437/Interspeech.2020-3015.
    URL ’http://dx.doi.org/10.21437/Interspeech.2020-3015'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(28) Gulati, A., Qin, J., Chiu, C.C., Parmar, N., Zhang, Y., Yu, J., Han, W.,
    Wang, S., Zhang, Z., Wu, Y., Pang, R.: Conformer: 卷积增强的变换器用于语音识别。见：Proc. Interspeech
    2020，第5036–5040页（2020）。DOI 10.21437/Interspeech.2020-3015。网址 ’http://dx.doi.org/10.21437/Interspeech.2020-3015'
- en: '(29) Guo, Y., Liu, Y., Georgiou, T., Lew, M.S.: A review of semantic segmentation
    using deep neural networks. International journal of multimedia information retrieval
    7(2), 87–93 (2018). DOI 10.1007/s13735-017-0141-z'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(29) Guo, Y., Liu, Y., Georgiou, T., Lew, M.S.: 使用深度神经网络的语义分割综述。国际多媒体信息检索期刊
    7(2), 87–93（2018）。DOI 10.1007/s13735-017-0141-z'
- en: '(30) Hafiz, A.M., Bhat, G.M.: A survey of deep learning techniques for medical
    diagnosis. In: M. Tuba, S. Akashe, A. Joshi (eds.) Information and Communication
    Technology for Sustainable Development, pp. 161–170\. Springer Singapore, Singapore
    (2020)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(30) Hafiz, A.M., Bhat, G.M.: 深度学习技术在医学诊断中的综述。见：M. Tuba, S. Akashe, A. Joshi（编）《可持续发展的信息与通信技术》，第161–170页。新加坡：Springer
    Singapore（2020）'
- en: '(31) Hafiz, A.M., Bhat, G.M.: A survey on instance segmentation: state of the
    art. International Journal of Multimedia Information Retrieval 9, 171–189 (2020).
    DOI 10.1007/s13735-020-00195-x'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(31) Hafiz, A.M., Bhat, G.M.: 实例分割的综述：现状。国际多媒体信息检索期刊 9, 171–189（2020）。DOI 10.1007/s13735-020-00195-x'
- en: '(32) Hang, R., Li, Z., Liu, Q., Ghamisi, P., Bhattacharyya, S.S.: Hyperspectral
    image classification with attention-aided cnns. IEEE Transactions on Geoscience
    and Remote Sensing 59(3), 2281–2293 (2021). DOI 10.1109/TGRS.2020.3007921'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(32) Hang, R., Li, Z., Liu, Q., Ghamisi, P., Bhattacharyya, S.S.: 利用注意力辅助的卷积神经网络进行高光谱图像分类。IEEE地球科学与遥感汇刊
    59(3), 2281–2293（2021）。DOI 10.1109/TGRS.2020.3007921'
- en: '(33) He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
    recognition. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 770–778\. IEEE Computer Society, Los Alamitos, CA, USA (2016). DOI 10.1109/CVPR.2016.90.
    URL https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.90'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(33) He, K., Zhang, X., Ren, S., Sun, J.: 图像识别的深度残差学习。见：2016 IEEE计算机视觉与模式识别会议（CVPR），第770–778页。IEEE计算机学会，美国加利福尼亚州洛斯阿拉米托斯（2016）。DOI
    10.1109/CVPR.2016.90。网址 https://doi.ieeecomputersociety.org/10.1109/CVPR.2016.90'
- en: '(34) He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
    recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR) (2016)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(34) He, K., Zhang, X., Ren, S., Sun, J.: 图像识别的深度残差学习。见：IEEE计算机视觉与模式识别会议（CVPR）论文集（2016）'
- en: '(35) He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
    recognition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR) (2016)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(35) He, K., Zhang, X., Ren, S., Sun, J.: 图像识别的深度残差学习。见：IEEE计算机视觉与模式识别会议（CVPR）论文集（2016）'
- en: '(36) Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural
    network (2015)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(36) Hinton, G., Vinyals, O., Dean, J.: 提炼神经网络中的知识（2015）'
- en: '(37) Hochreiter, S., Schmidhuber, J.: Long Short-Term Memory. Neural Computation
    9(8), 1735–1780 (1997). DOI 10.1162/neco.1997.9.8.1735. URL https://doi.org/10.1162/neco.1997.9.8.1735'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(37) Hochreiter, S., Schmidhuber, J.: 长短期记忆。神经计算 9(8), 1735–1780（1997）。DOI
    10.1162/neco.1997.9.8.1735。网址 https://doi.org/10.1162/neco.1997.9.8.1735'
- en: '(38) Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E.: Squeeze-and-excitation
    networks. IEEE Transactions on Pattern Analysis and Machine Intelligence 42(8),
    2011–2023 (2020). DOI 10.1109/TPAMI.2019.2913372'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(38) Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E.: Squeeze-and-excitation
    网络。IEEE 模式分析与机器智能学报 42(8)，2011–2023（2020）。DOI 10.1109/TPAMI.2019.2913372'
- en: '(39) Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E.: Squeeze-and-excitation
    networks. IEEE Trans. Pattern Anal. Mach. Intell. 42(8), 2011–2023 (2020). DOI 10.1109/TPAMI.2019.2913372.
    URL https://doi.org/10.1109/TPAMI.2019.2913372'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(39) Hu, J., Shen, L., Albanie, S., Sun, G., Wu, E.: Squeeze-and-excitation
    网络。IEEE 模式分析与机器智能学报 42(8)，2011–2023（2020）。DOI 10.1109/TPAMI.2019.2913372. URL
    https://doi.org/10.1109/TPAMI.2019.2913372'
- en: '(40) Huang, J., Zhou, W., Li, H., Li, W.: Attention-based 3d-cnns for large-vocabulary
    sign language recognition. IEEE Transactions on Circuits and Systems for Video
    Technology 29(9), 2822–2832 (2019). DOI 10.1109/TCSVT.2018.2870740'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(40) Huang, J., Zhou, W., Li, H., Li, W.: 基于注意力的 3D-CNN 用于大词汇量手语识别。IEEE 电路与系统视频技术学报
    29(9)，2822–2832（2019）。DOI 10.1109/TCSVT.2018.2870740'
- en: '(41) Huang, X., Shen, C., Boix, X., Zhao, Q.: Salicon: Reducing the semantic
    gap in saliency prediction by adapting deep neural networks. In: 2015 IEEE International
    Conference on Computer Vision (ICCV), pp. 262–270 (2015). DOI 10.1109/ICCV.2015.38'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(41) Huang, X., Shen, C., Boix, X., Zhao, Q.: Salicon：通过调整深度神经网络减少显著性预测中的语义差距。见：2015
    IEEE 国际计算机视觉会议（ICCV），第 262–270 页（2015）。DOI 10.1109/ICCV.2015.38'
- en: '(42) Huang, X., Shen, C., Boix, X., Zhao, Q.: Salicon: Reducing the semantic
    gap in saliency prediction by adapting deep neural networks. In: 2015 IEEE International
    Conference on Computer Vision (ICCV), pp. 262–270 (2015). DOI 10.1109/ICCV.2015.38'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(42) Huang, X., Shen, C., Boix, X., Zhao, Q.: Salicon：通过调整深度神经网络减少显著性预测中的语义差距。见：2015
    IEEE 国际计算机视觉会议（ICCV），第 262–270 页（2015）。DOI 10.1109/ICCV.2015.38'
- en: '(43) Jetley, S., Murray, N., Vig, E.: End-to-end saliency mapping via probability
    distribution prediction. In: 2016 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), pp. 5753–5761 (2016). DOI 10.1109/CVPR.2016.620'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(43) Jetley, S., Murray, N., Vig, E.: 通过概率分布预测进行端到端的显著性映射。见：2016 IEEE 计算机视觉与模式识别会议（CVPR），第
    5753–5761 页（2016）。DOI 10.1109/CVPR.2016.620'
- en: '(44) Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
    S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.: The
    kinetics human action video dataset (2017)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(44) Kay, W., Carreira, J., Simonyan, K., Zhang, B., Hillier, C., Vijayanarasimhan,
    S., Viola, F., Green, T., Back, T., Natsev, P., Suleyman, M., Zisserman, A.: The
    kinetics 人类动作视频数据集（2017）'
- en: '(45) Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.: Transformers
    in vision: A survey (2021)'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(45) Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M.: 视觉中的变换器：综述（2021）'
- en: '(46) Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C.: Dense-captioning
    events in videos. In: 2017 IEEE International Conference on Computer Vision (ICCV),
    pp. 706–715 (2017). DOI 10.1109/ICCV.2017.83'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(46) Krishna, R., Hata, K., Ren, F., Fei-Fei, L., Niebles, J.C.: 视频中的密集描述事件。见：2017
    IEEE 国际计算机视觉会议（ICCV），第 706–715 页（2017）。DOI 10.1109/ICCV.2017.83'
- en: '(47) Krizhevsky, A., et al.: Learning multiple layers of features from tiny
    images (2009)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (47) Krizhevsky, A., 等：从微小图像中学习多层特征（2009）
- en: '(48) Kruthiventi, S.S.S., Ayush, K., Babu, R.V.: Deepfix: A fully convolutional
    neural network for predicting human eye fixations. IEEE Transactions on Image
    Processing 26(9), 4446–4456 (2017). DOI 10.1109/TIP.2017.2710620'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(48) Kruthiventi, S.S.S., Ayush, K., Babu, R.V.: Deepfix：一种用于预测人类眼动注视的全卷积神经网络。IEEE
    图像处理学报 26(9)，4446–4456（2017）。DOI 10.1109/TIP.2017.2710620'
- en: '(49) Kruthiventi, S.S.S., Gudisa, V., Dholakiya, J.H., Babu, R.V.: Saliency
    unified: A deep architecture for simultaneous eye fixation prediction and salient
    object segmentation. In: 2016 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 5781–5790 (2016). DOI 10.1109/CVPR.2016.623'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(49) Kruthiventi, S.S.S., Gudisa, V., Dholakiya, J.H., Babu, R.V.: Saliency
    unified：一种用于同时预测眼动注视和显著性目标分割的深度架构。见：2016 IEEE 计算机视觉与模式识别会议（CVPR），第 5781–5790 页（2016）。DOI
    10.1109/CVPR.2016.623'
- en: '(50) Kumar, M., Weissenborn, D., Kalchbrenner, N.: Colorization transformer
    (2021)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(50) Kumar, M., Weissenborn, D., Kalchbrenner, N.: 色彩化变换器（2021）'
- en: '(51) Kümmerer, M., Theis, L., Bethge, M.: Deep gaze i: Boosting saliency prediction
    with feature maps trained on imagenet (2015)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(51) Kümmerer, M., Theis, L., Bethge, M.: Deep gaze i：通过在 ImageNet 上训练的特征图提升显著性预测（2015）'
- en: '(52) Kümmerer, M., Wallis, T.S.A., Bethge, M.: Deepgaze ii: Reading fixations
    from deep features trained on object recognition (2016)'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(52) Kümmerer, M., Wallis, T.S.A., Bethge, M.: Deepgaze ii：从对象识别训练的深度特征中读取注视（2016）'
- en: '(53) Larochelle, H., Hinton, G.: Learning to combine foveal glimpses with a
    third-order boltzmann machine. In: Proceedings of the 23rd International Conference
    on Neural Information Processing Systems - Volume 1, NIPS’10, pp. 1243–1251\.
    Curran Associates Inc., Red Hook, NY, USA (2010)'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (53) Larochelle, H., Hinton, G.：**学习将中心视野的瞬间与三阶玻尔兹曼机结合**。见：第23届国际神经信息处理系统会议论文集
    - 第1卷，NIPS’10，第1243–1251页。Curran Associates Inc.，Red Hook，NY，美国（2010）
- en: '(54) LeCun, Y., Bengio, Y., Hinton, G.: Deep learning. nature 521(7553), 436–444
    (2015)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (54) LeCun, Y., Bengio, Y., Hinton, G.：**深度学习**。自然 521(7553)，436–444（2015）
- en: '(55) Lecun, Y., Bottou, L., Bengio, Y., Haffner, P.: Gradient-based learning
    applied to document recognition. Proceedings of the IEEE 86(11), 2278–2324 (1998).
    DOI 10.1109/5.726791'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (55) Lecun, Y., Bottou, L., Bengio, Y., Haffner, P.：**应用于文档识别的基于梯度的学习**。IEEE汇刊
    86(11)，2278–2324（1998）。DOI 10.1109/5.726791
- en: '(56) LeCun, Y., Kavukcuoglu, K., Farabet, C.: Convolutional networks and applications
    in vision. In: Proceedings of 2010 IEEE International Symposium on Circuits and
    Systems, pp. 253–256 (2010). DOI 10.1109/ISCAS.2010.5537907'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (56) LeCun, Y., Kavukcuoglu, K., Farabet, C.：**卷积网络及其在视觉中的应用**。见：2010年IEEE国际电路与系统研讨会论文集，第253–256页（2010）。DOI
    10.1109/ISCAS.2010.5537907
- en: '(57) Lee, C.Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z.: Deeply-Supervised
    Nets. In: G. Lebanon, S.V.N. Vishwanathan (eds.) Proceedings of the Eighteenth
    International Conference on Artificial Intelligence and Statistics, *Proceedings
    of Machine Learning Research*, vol. 38, pp. 562–570\. PMLR, San Diego, California,
    USA (2015). URL http://proceedings.mlr.press/v38/lee15a.html'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (57) Lee, C.Y., Xie, S., Gallagher, P., Zhang, Z., Tu, Z.：**深度监督网络**。见：G. Lebanon,
    S.V.N. Vishwanathan（编）《第十八届人工智能与统计国际会议论文集》，*机器学习研究论文集*，第38卷，第562–570页。PMLR，加州圣地亚哥，美国（2015）。URL
    http://proceedings.mlr.press/v38/lee15a.html
- en: '(58) Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.: Stacked cross attention
    for image-text matching. In: Proceedings of the European Conference on Computer
    Vision (ECCV) (2018)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (58) Lee, K.H., Chen, X., Hua, G., Hu, H., He, X.：**用于图像-文本匹配的堆叠交叉注意力**。见：欧洲计算机视觉会议（ECCV）论文集（2018）
- en: '(59) Lee, S., Yu, Y., Kim, G., Breuel, T., Kautz, J., Song, Y.: Parameter efficient
    multimodal transformers for video representation learning (2020)'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (59) Lee, S., Yu, Y., Kim, G., Breuel, T., Kautz, J., Song, Y.：**参数高效的多模态变换器用于视频表示学习**（2020）
- en: '(60) Li, C., Tang, T., Wang, G., Peng, J., Wang, B., Liang, X., Chang, X.:
    Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural
    architecture search (2021)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (60) Li, C., Tang, T., Wang, G., Peng, J., Wang, B., Liang, X., Chang, X.：**Bossnas：探索具有块级自监督神经架构搜索的混合CNN-变换器**（2021）
- en: '(61) Li, G., Yu, Y.: Visual saliency based on multiscale deep features. In:
    2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 5455–5463
    (2015). DOI 10.1109/CVPR.2015.7299184'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (61) Li, G., Yu, Y.：**基于多尺度深度特征的视觉显著性**。见：2015年IEEE计算机视觉与模式识别会议（CVPR）论文集，第5455–5463页（2015）。DOI
    10.1109/CVPR.2015.7299184
- en: '(62) Li, J., Jin, K., Zhou, D., Kubota, N., Ju, Z.: Attention mechanism-based
    cnn for facial expression recognition. Neurocomputing 411, 340–350 (2020). DOI https://doi.org/10.1016/j.neucom.2020.06.014.
    URL https://www.sciencedirect.com/science/article/pii/S0925231220309838'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (62) Li, J., Jin, K., Zhou, D., Kubota, N., Ju, Z.：**基于注意力机制的CNN用于面部表情识别**。神经计算
    411，340–350（2020）。DOI https://doi.org/10.1016/j.neucom.2020.06.014。URL https://www.sciencedirect.com/science/article/pii/S0925231220309838
- en: '(63) Li, L., Xu, M., Wang, X., Jiang, L., Liu, H.: Attention based glaucoma
    detection: A large-scale database and cnn model. In: 2019 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 10563–10572 (2019). DOI 10.1109/CVPR.2019.01082'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (63) Li, L., Xu, M., Wang, X., Jiang, L., Liu, H.：**基于注意力的青光眼检测：一个大规模数据库和CNN模型**。见：2019年IEEE/CVF计算机视觉与模式识别会议（CVPR）论文集，第10563–10572页（2019）。DOI
    10.1109/CVPR.2019.01082
- en: '(64) Li, S., Deng, W., Du, J.: Reliable crowdsourcing and deep locality-preserving
    learning for expression recognition in the wild. In: Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR) (2017)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (64) Li, S., Deng, W., Du, J.：**可靠的众包和深度局部保持学习用于野外表情识别**。见：IEEE计算机视觉与模式识别会议（CVPR）论文集（2017）
- en: '(65) Li, Y., Zeng, J., Shan, S., Chen, X.: Occlusion aware facial expression
    recognition using cnn with attention mechanism. IEEE Transactions on Image Processing
    28(5), 2439–2450 (2019). DOI 10.1109/TIP.2018.2886767'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (65) Li, Y., Zeng, J., Shan, S., Chen, X.：**基于CNN的遮挡感知面部表情识别与注意力机制**。IEEE图像处理汇刊
    28(5)，2439–2450（2019）。DOI 10.1109/TIP.2018.2886767
- en: '(66) Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
    Dollár, P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: D. Fleet,
    T. Pajdla, B. Schiele, T. Tuytelaars (eds.) Computer Vision - ECCV 2014, pp. 740–755\.
    Springer International Publishing, Cham (2014)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (66) Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D.,
    Dollár, P., Zitnick, C.L.：Microsoft COCO：上下文中的常见对象。在：D. Fleet, T. Pajdla, B. Schiele,
    T. Tuytelaars（编）计算机视觉 - ECCV 2014，页码 740–755。Springer 国际出版公司，Cham（2014）
- en: '(67) Liu, N., Han, J., Liu, T., Li, X.: Learning to predict eye fixations via
    multiresolution convolutional neural networks. IEEE Transactions on Neural Networks
    and Learning Systems 29(2), 392–404 (2018). DOI 10.1109/TNNLS.2016.2628878'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (67) Liu, N., Han, J., Liu, T., Li, X.：通过多分辨率卷积神经网络学习预测眼动。IEEE 神经网络与学习系统学报 29(2)，392–404（2018）。DOI
    10.1109/TNNLS.2016.2628878
- en: '(68) Lu, J., Batra, D., Parikh, D., Lee, S.: Vilbert: Pretraining task-agnostic
    visiolinguistic representations for vision-and-language tasks (2019)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (68) Lu, J., Batra, D., Parikh, D., Lee, S.：Vilbert：为视觉和语言任务预训练任务无关的视觉语言表示（2019）
- en: '(69) Lucey, P., Cohn, J.F., Kanade, T., Saragih, J., Ambadar, Z., Matthews,
    I.: The extended cohn-kanade dataset (ck+): A complete dataset for action unit
    and emotion-specified expression. In: 2010 IEEE Computer Society Conference on
    Computer Vision and Pattern Recognition - Workshops, pp. 94–101 (2010). DOI 10.1109/CVPRW.2010.5543262'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (69) Lucey, P., Cohn, J.F., Kanade, T., Saragih, J., Ambadar, Z., Matthews,
    I.：扩展的Cohn-Kanade数据集（CK+）：用于动作单元和情感特定表情的完整数据集。在：2010年IEEE计算机视觉与模式识别会议 - 研讨会，页码
    94–101（2010）。DOI 10.1109/CVPRW.2010.5543262
- en: '(70) Lyons, M., Akamatsu, S., Kamachi, M., Gyoba, J.: Coding facial expressions
    with gabor wavelets. In: Proceedings Third IEEE International Conference on Automatic
    Face and Gesture Recognition, pp. 200–205 (1998). DOI 10.1109/AFGR.1998.670949'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (70) Lyons, M., Akamatsu, S., Kamachi, M., Gyoba, J.：用Gabor小波编码面部表情。在：第三届IEEE国际自动面部和姿态识别会议论文集，页码
    200–205（1998）。DOI 10.1109/AFGR.1998.670949
- en: '(71) Mollahosseini, A., Hasani, B., Mahoor, M.H.: Affectnet: A database for
    facial expression, valence, and arousal computing in the wild. IEEE Transactions
    on Affective Computing 10(1), 18–31 (2019). DOI 10.1109/TAFFC.2017.2740923'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (71) Mollahosseini, A., Hasani, B., Mahoor, M.H.：Affectnet：一个用于面部表情、情感价值和激发计算的数据库。IEEE
    情感计算学报 10(1)，18–31（2019）。DOI 10.1109/TAFFC.2017.2740923
- en: '(72) Pan, J., Sayrol, E., Giro-I-Nieto, X., McGuinness, K., O’Connor, N.E.:
    Shallow and deep convolutional networks for saliency prediction. In: 2016 IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 598–606 (2016).
    DOI 10.1109/CVPR.2016.71'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (72) Pan, J., Sayrol, E., Giro-I-Nieto, X., McGuinness, K., O’Connor, N.E.：用于显著性预测的浅层和深层卷积网络。在：2016年IEEE计算机视觉与模式识别会议（CVPR），页码
    598–606（2016）。DOI 10.1109/CVPR.2016.71
- en: '(73) Pinheiro, P.O., Lin, T.Y., Collobert, R., Dollár, P.: Learning to refine
    object segments. In: B. Leibe, J. Matas, N. Sebe, M. Welling (eds.) Computer Vision
    - ECCV 2016, pp. 75–91\. Springer International Publishing, Cham (2016)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (73) Pinheiro, P.O., Lin, T.Y., Collobert, R., Dollár, P.：学习精细化目标分割。在：B. Liebe,
    J. Matas, N. Sebe, M. Welling（编）计算机视觉 - ECCV 2016，页码 75–91。Springer 国际出版公司，Cham（2016）
- en: '(74) Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Dollar, P.: Designing
    network design spaces. In: 2020 IEEE/CVF Conference on Computer Vision and Pattern
    Recognition (CVPR), pp. 10425–10433 (2020). DOI 10.1109/CVPR42600.2020.01044'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (74) Radosavovic, I., Kosaraju, R.P., Girshick, R., He, K., Dollar, P.：设计网络设计空间。在：2020年IEEE/CVF计算机视觉与模式识别会议（CVPR），页码
    10425–10433（2020）。DOI 10.1109/CVPR42600.2020.01044
- en: '(75) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Chen, M., Child, R., Misra,
    V., Mishkin, P., Krueger, G., Agarwal, S., et al.: Dall· e: Creating images from
    text. OpenAI blog. https://openai. com/blog/dall-e (2021)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (75) Ramesh, A., Pavlov, M., Goh, G., Gray, S., Chen, M., Child, R., Misra,
    V., Mishkin, P., Krueger, G., Agarwal, S.，等：Dall· e：从文本创建图像。OpenAI 博客。https://openai.com/blog/dall-e（2021）
- en: '(76) Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time
    object detection with region proposal networks. In: Proceedings of the 28th International
    Conference on Neural Information Processing Systems - Volume 1, NIPS’15, pp. 91–99\.
    MIT Press, Cambridge, MA, USA (2015)'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (76) Ren, S., He, K., Girshick, R., Sun, J.：Faster r-cnn：面向实时目标检测的区域提议网络。在：第28届国际神经信息处理系统会议论文集
    - 第1卷，NIPS’15，页码 91–99。MIT Press，Cambridge，MA，USA（2015）
- en: '(77) Ren, S., He, K., Girshick, R., Sun, J.: Faster r-cnn: Towards real-time
    object detection with region proposal networks. IEEE Transactions on Pattern Analysis
    and Machine Intelligence 39(6), 1137–1149 (2017). DOI 10.1109/TPAMI.2016.2577031'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (77) Ren, S., He, K., Girshick, R., Sun, J.：Faster r-cnn：面向实时目标检测的区域提议网络。IEEE
    模式分析与机器智能学报 39(6)，1137–1149（2017）。DOI 10.1109/TPAMI.2016.2577031
- en: '(78) Rensink, R.A.: The dynamic representation of scenes. Visual Cognition
    7(1-3), 17–42 (2000). DOI 10.1080/135062800394667. URL https://doi.org/10.1080/135062800394667'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (78) Rensink, R.A.：场景的动态表示。Visual Cognition 7（1-3），17-42（2000年）。DOI 10.1080/135062800394667。网址https://doi.org/10.1080/135062800394667
- en: '(79) Selvaraju, R.R., Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra,
    D.: Grad-cam: Visual explanations from deep networks via gradient-based localization.
    In: 2017 IEEE International Conference on Computer Vision (ICCV), pp. 618–626
    (2017). DOI 10.1109/ICCV.2017.74'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (79) Selvaraju, R.R.，Cogswell, M., Das, A., Vedantam, R., Parikh, D., Batra,
    D.：Grad-cam：基于梯度的深度网络的可视化解释。在：2017年IEEE国际计算机视觉大会（ICCV），pp.618-626（2017年）。DOI 10.1109/ICCV.2017.74
- en: '(80) Seong, H., Hyun, J., Kim, E.: Video multitask transformer network. In:
    2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pp.
    1553–1561 (2019). DOI 10.1109/ICCVW.2019.00194'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (80) Seong, H., Hyun, J., Kim, E.：视频多任务变换器网络。在：2019年IEEE/CVF国际计算机视觉会议研讨会（ICCVW），pp.1553-1561（2019年）。DOI 10.1109/ICCVW.2019.00194
- en: '(81) Sharma, S., Kiros, R., Salakhutdinov, R.: Action recognition using visual
    attention. In: International Conference on Learning Representations (ICLR) Workshop
    (2016). URL https://arxiv.org/abs/1511.04119'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (81) Sharma, S., Kiros, R., Salakhutdinov, R.：使用视觉注意力进行动作识别。在：国际学习表示会议（ICLR）研讨会（2016年）。网址https://arxiv.org/abs/1511.04119
- en: '(82) Shrestha, A., Mahmood, A.: Review of deep learning algorithms and architectures.
    IEEE Access 7, 53040–53065 (2019). DOI 10.1109/ACCESS.2019.2912200'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(82) Shrestha, A., Mahmood, A.: 深度学习算法和架构综述。IEEE Access 7，53040-53065（2019年）。DOI 10.1109/ACCESS.2019.2912200'
- en: '(83) Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale
    image recognition (2015)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(83) Simonyan, K., Zisserman, A.: 非常深的卷积网络用于大规模图像识别（2015年）'
- en: '(84) Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani,
    A.: Bottleneck transformers for visual recognition (2021)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (84) Srinivas, A., Lin, T.Y., Parmar, N., Shlens, J., Abbeel, P., Vaswani, A.：用于视觉识别的瓶颈变换器（2021年）
- en: '(85) Su, W., Zhu, X., Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.: Vl-bert: Pre-training
    of generic visual-linguistic representations. In: International Conference on
    Learning Representations (2020). URL https://openreview.net/forum?id=SygXPaEYvH'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (85) Su, W., Zhu, X.，Cao, Y., Li, B., Lu, L., Wei, F., Dai, J.：Vl-bert：通用视觉语言表示的预训练。在：国际学习表示会议（2020年）。网址https://openreview.net/forum?id=SygXPaEYvH
- en: '(86) Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C.: Videobert: A
    joint model for video and language representation learning. In: 2019 IEEE/CVF
    International Conference on Computer Vision (ICCV), pp. 7463–7472 (2019). DOI 10.1109/ICCV.2019.00756'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (86) Sun, C., Myers, A., Vondrick, C., Murphy, K., Schmid, C.：Videobert：用于视频和语言表示学习的联合模型。在：2019年IEEE/CVF国际计算机视觉大会（ICCV），pp.7463-7472（2019年）。DOI 10.1109/ICCV.2019.00756
- en: '(87) Tan, H., Bansal, M.: LXMERT: Learning cross-modality encoder representations
    from transformers. In: Proceedings of the 2019 Conference on Empirical Methods
    in Natural Language Processing and the 9th International Joint Conference on Natural
    Language Processing (EMNLP-IJCNLP), pp. 5100–5111\. Association for Computational
    Linguistics, Hong Kong, China (2019). DOI 10.18653/v1/D19-1514. URL https://www.aclweb.org/anthology/D19-1514'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (87) Tan, H., Bansal, M.：LXMERT：从变换器中学习跨模态编码器表示。在：2019年经验方法自然语言处理大会和第9届国际自然语言处理联合会议论文集（EMNLP-IJCNLP），pp.5100-5111。计算语言学协会，中国香港（2019年）。DOI 10.18653/v1/D19-1514。网址https://www.aclweb.org/anthology/D19-1514
- en: '(88) Tan, M., Le, Q.: EfficientNet: Rethinking model scaling for convolutional
    neural networks. In: K. Chaudhuri, R. Salakhutdinov (eds.) Proceedings of the
    36th International Conference on Machine Learning, *Proceedings of Machine Learning
    Research*, vol. 97, pp. 6105–6114\. PMLR (2019). URL http://proceedings.mlr.press/v97/tan19a.html'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '(88) Tan, M., Le, Q.: EfficientNet：重新思考卷积神经网络的模型缩放。在：K. Chaudhuri, R. Salakhutdinov（编辑）第36届国际机器学习大会论文集，*机器学习研究论文集*，卷97，pp.6105-6114。PMLR（2019年）。网址http://proceedings.mlr.press/v97/tan19a.html'
- en: '(89) Tian, C., Xu, Y., Li, Z., Zuo, W., Fei, L., Liu, H.: Attention-guided
    cnn for image denoising. Neural Networks 124, 117–129 (2020). DOI https://doi.org/10.1016/j.neunet.2019.12.024.
    URL https://www.sciencedirect.com/science/article/pii/S0893608019304241'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (89) Tian, C., Xu, Y., Li, Z., Zuo, W., Fei, L., Liu, H.：用于图像去噪的注意力引导的CNN。神经网络124，117-129（2020年）。DOI https://doi.org/10.1016/j.neunet.2019.12.024。网址https://www.sciencedirect.com/science/article/pii/S0893608019304241
- en: '(90) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou,
    H.: Training data-efficient image transformers & distillation through attention
    (2021)'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (90) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.：训练数据高效的图像变换器和通过注意力进行蒸馏（2021年）
- en: '(91) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou,
    H.: Training data-efficient image transformers & distillation through attention
    (2021)'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (91) Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H.：通过注意力训练数据高效的图像变换器与蒸馏（2021）
- en: '(92) Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.: Learning
    spatiotemporal features with 3d convolutional networks. In: Proceedings of the
    IEEE International Conference on Computer Vision (ICCV) (2015)'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (92) Tran, D., Bourdev, L., Fergus, R., Torresani, L., Paluri, M.：利用3D卷积网络学习时空特征。见：IEEE国际计算机视觉会议（ICCV）论文集（2015）
- en: '(93) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A.N., Kaiser, u., Polosukhin, I.: Attention is all you need. In: Proceedings of
    the 31st International Conference on Neural Information Processing Systems, NIPS’17,
    pp. 6000–6010\. Curran Associates Inc., Red Hook, NY, USA (2017)'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (93) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A.N., Kaiser, u., Polosukhin, I.：注意力即你所需。见：第31届国际神经信息处理系统会议论文集，NIPS’17，第6000–6010页。Curran
    Associates Inc., Red Hook, NY, USA（2017）
- en: '(94) Vinyals, O., Toshev, A., Bengio, S., Erhan, D.: Show and tell: A neural
    image caption generator. In: 2015 IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), pp. 3156–3164 (2015). DOI 10.1109/CVPR.2015.7298935'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (94) Vinyals, O., Toshev, A., Bengio, S., Erhan, D.：展示与讲述：一个神经图像字幕生成器。见：2015
    IEEE计算机视觉与模式识别会议（CVPR），第3156–3164页（2015）。DOI 10.1109/CVPR.2015.7298935
- en: '(95) Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X.,
    Tang, X.: Residual attention network for image classification. In: 2017 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 6450–6458 (2017). DOI 10.1109/CVPR.2017.683'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (95) Wang, F., Jiang, M., Qian, C., Yang, S., Li, C., Zhang, H., Wang, X., Tang,
    X.：用于图像分类的残差注意力网络。见：2017 IEEE计算机视觉与模式识别会议（CVPR），第6450–6458页（2017）。DOI 10.1109/CVPR.2017.683
- en: '(96) Wang, W., Shen, J.: Deep visual attention prediction. IEEE Transactions
    on Image Processing 27(5), 2368–2378 (2018). DOI 10.1109/TIP.2017.2787612'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (96) Wang, W., Shen, J.：深度视觉注意力预测。IEEE图像处理汇刊27(5)，2368–2378（2018）。DOI 10.1109/TIP.2017.2787612
- en: '(97) Wang, X., Yeshwanth, C., Niebner, M.: Sceneformer: Indoor scene generation
    with transformers (2021)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (97) Wang, X., Yeshwanth, C., Niebner, M.：Sceneformer：使用变换器生成室内场景（2021）
- en: '(98) Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.: End-to-end
    video instance segmentation with transformers (2021)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (98) Wang, Y., Xu, Z., Wang, X., Shen, C., Cheng, B., Shen, H., Xia, H.：端到端视频实例分割与变换器（2021）
- en: '(99) Wang, Y., Yang, Y., Bai, J., Zhang, M., Bai, J., Yu, J., Zhang, C., Huang,
    G., Tong, Y.: Evolving attention with residual convolutions (2021)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (99) Wang, Y., Yang, Y., Bai, J., Zhang, M., Bai, J., Yu, J., Zhang, C., Huang,
    G., Tong, Y.：通过残差卷积进化的注意力（2021）
- en: '(100) Woo, S., Hwang, S., Kweon, I.S.: Stairnet: Top-down semantic aggregation
    for accurate one shot detection. In: 2018 IEEE Winter Conference on Applications
    of Computer Vision (WACV), pp. 1093–1102 (2018). DOI 10.1109/WACV.2018.00125'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (100) Woo, S., Hwang, S., Kweon, I.S.：StairNet：用于精确一次性检测的自上而下的语义聚合。见：2018 IEEE冬季计算机视觉应用会议（WACV），第1093–1102页（2018）。DOI
    10.1109/WACV.2018.00125
- en: '(101) Woo, S., Park, J., Lee, J.Y., Kweon, I.S.: Cbam: Convolutional block
    attention module. In: V. Ferrari, M. Hebert, C. Sminchisescu, Y. Weiss (eds.)
    Computer Vision - ECCV 2018, pp. 3–19\. Springer International Publishing, Cham
    (2018)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (101) Woo, S., Park, J., Lee, J.Y., Kweon, I.S.：CBAM：卷积块注意力模块。见：V. Ferrari,
    M. Hebert, C. Sminchisescu, Y. Weiss（编辑）计算机视觉 - ECCV 2018，第3–19页。Springer国际出版，Cham（2018）
- en: '(102) Wu, F., Fan, A., Baevski, A., Dauphin, Y.N., Auli, M.: Pay less attention
    with lightweight and dynamic convolutions (2019)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (102) Wu, F., Fan, A., Baevski, A., Dauphin, Y.N., Auli, M.：用轻量级和动态卷积减少注意力（2019）
- en: '(103) Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.:
    Cvt: Introducing convolutions to vision transformers (2021)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (103) Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L.：CVT：将卷积引入视觉变换器（2021）
- en: '(104) Wu, Z., Liu, Z., Lin, J., Lin, Y., Han, S.: Lite transformer with long-short
    range attention (2020)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (104) Wu, Z., Liu, Z., Lin, J., Lin, Y., Han, S.：具有长短距离注意力的轻量变换器（2020）
- en: '(105) Xie, S., Tu, Z.: Holistically-nested edge detection. In: 2015 IEEE International
    Conference on Computer Vision (ICCV), pp. 1395–1403 (2015). DOI 10.1109/ICCV.2015.164'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (105) Xie, S., Tu, Z.：整体嵌套边缘检测。见：2015 IEEE国际计算机视觉会议（ICCV），第1395–1403页（2015）。DOI
    10.1109/ICCV.2015.164
- en: '(106) Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R.,
    Zemel, R., Bengio, Y.: Show, attend and tell: Neural image caption generation
    with visual attention. In: F. Bach, D. Blei (eds.) Proceedings of the 32nd International
    Conference on Machine Learning, *Proceedings of Machine Learning Research*, vol. 37,
    pp. 2048–2057\. PMLR, Lille, France (2015). URL http://proceedings.mlr.press/v37/xuc15.html'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (106) Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel,
    R., Bengio, Y.：《展示、关注与讲述：具有视觉注意力的神经图像字幕生成》。发表于：F. Bach, D. Blei（编辑）《第32届国际机器学习会议论文集》，*机器学习研究论文集*，第37卷，第2048–2057页。PMLR，法国里尔（2015年）。网址
    http://proceedings.mlr.press/v37/xuc15.html
- en: '(107) Xu, M., Li, C., Liu, Y., Deng, X., Lu, J.: A subjective visual quality
    assessment method of panoramic videos. In: 2017 IEEE International Conference
    on Multimedia and Expo (ICME), pp. 517–522 (2017). DOI 10.1109/ICME.2017.8019351'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (107) Xu, M., Li, C., Liu, Y., Deng, X., Lu, J.：《全景视频的主观视觉质量评估方法》。发表于：2017年IEEE国际多媒体与展览会议（ICME），第517–522页（2017年）。DOI
    10.1109/ICME.2017.8019351
- en: '(108) Yang, C., Zhang, L., Lu, H., Ruan, X., Yang, M.H.: Saliency detection
    via graph-based manifold ranking. In: 2013 IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 3166–3173 (2013). DOI 10.1109/CVPR.2013.407'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (108) Yang, C., Zhang, L., Lu, H., Ruan, X., Yang, M.H.：《通过基于图的流形排序进行显著性检测》。发表于：2013年IEEE计算机视觉与模式识别会议，第3166–3173页（2013年）。DOI
    10.1109/CVPR.2013.407
- en: '(109) Yang, F., Yang, H., Fu, J., Lu, H., Guo, B.: Learning texture transformer
    network for image super-resolution. In: 2020 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 5790–5799 (2020). DOI 10.1109/CVPR42600.2020.00583'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (109) Yang, F., Yang, H., Fu, J., Lu, H., Guo, B.：《学习纹理变换网络用于图像超分辨率》。发表于：2020年IEEE/CVF计算机视觉与模式识别会议（CVPR），第5790–5799页（2020年）。DOI
    10.1109/CVPR42600.2020.00583
- en: '(110) Yao, X., Han, J., Zhang, D., Nie, F.: Revisiting co-saliency detection:
    A novel approach based on two-stage multi-view spectral rotation co-clustering.
    IEEE Transactions on Image Processing 26(7), 3196–3209 (2017). DOI 10.1109/TIP.2017.2694222'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (110) Yao, X., Han, J., Zhang, D., Nie, F.：《重新审视共同显著性检测：一种基于两阶段多视角谱旋转共同聚类的新方法》。IEEE图像处理汇刊
    26(7)，第3196–3209页（2017年）。DOI 10.1109/TIP.2017.2694222
- en: '(111) Ye, H.J., Hu, H., Zhan, D.C., Sha, F.: Few-shot learning via embedding
    adaptation with set-to-set functions. In: 2020 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition (CVPR), pp. 8805–8814 (2020). DOI 10.1109/CVPR42600.2020.00883'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (111) Ye, H.J., Hu, H., Zhan, D.C., Sha, F.：《通过嵌入适应进行少样本学习，采用集合到集合函数》。发表于：2020年IEEE/CVF计算机视觉与模式识别会议（CVPR），第8805–8814页（2020年）。DOI
    10.1109/CVPR42600.2020.00883
- en: '(112) Ye, L., Rochan, M., Liu, Z., Wang, Y.: Cross-modal self-attention network
    for referring image segmentation. In: 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 10494–10503 (2019). DOI 10.1109/CVPR.2019.01075'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (112) Ye, L., Rochan, M., Liu, Z., Wang, Y.：《用于引用图像分割的跨模态自注意网络》。发表于：2019年IEEE/CVF计算机视觉与模式识别会议（CVPR），第10494–10503页（2019年）。DOI
    10.1109/CVPR.2019.01075
- en: '(113) Yu, Y., Choi, J., Kim, Y., Yoo, K., Lee, S.H., Kim, G.: Supervising neural
    attention models for video captioning by human gaze data. In: 2017 IEEE Conference
    on Computer Vision and Pattern Recognition (CVPR), pp. 6119–6127 (2017). DOI 10.1109/CVPR.2017.648'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (113) Yu, Y., Choi, J., Kim, Y., Yoo, K., Lee, S.H., Kim, G.：《通过人类注视数据监督视频字幕生成的神经注意力模型》。发表于：2017年IEEE计算机视觉与模式识别会议（CVPR），第6119–6127页（2017年）。DOI
    10.1109/CVPR.2017.648
- en: '(114) Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.: From recognition to cognition:
    Visual commonsense reasoning. In: 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), pp. 6713–6724 (2019). DOI 10.1109/CVPR.2019.00688'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (114) Zellers, R., Bisk, Y., Farhadi, A., Choi, Y.：《从识别到认知：视觉常识推理》。发表于：2019年IEEE/CVF计算机视觉与模式识别会议（CVPR），第6713–6724页（2019年）。DOI
    10.1109/CVPR.2019.00688
- en: '(115) Zhao, G., Huang, X., Taini, M., Li, S.Z., Pietikäinen, M.: Facial expression
    recognition from near-infrared videos. Image and Vision Computing 29(9), 607–619
    (2011). DOI https://doi.org/10.1016/j.imavis.2011.07.002. URL https://www.sciencedirect.com/science/article/pii/S0262885611000515'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (115) Zhao, G., Huang, X., Taini, M., Li, S.Z., Pietikäinen, M.：《从近红外视频中进行面部表情识别》。图像与视觉计算
    29(9)，第607–619页（2011年）。DOI https://doi.org/10.1016/j.imavis.2011.07.002。网址 https://www.sciencedirect.com/science/article/pii/S0262885611000515
- en: '(116) Zhao, R., Ouyang, W., Li, H., Wang, X.: Saliency detection by multi-context
    deep learning. In: 2015 IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pp. 1265–1274 (2015). DOI 10.1109/CVPR.2015.7298731'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (116) Zhao, R., Ouyang, W., Li, H., Wang, X.：《通过多上下文深度学习进行显著性检测》。发表于：2015年IEEE计算机视觉与模式识别会议（CVPR），第1265–1274页（2015年）。DOI
    10.1109/CVPR.2015.7298731
- en: '(117) Zhou, L., Xu, C., Corso, J.: Towards automatic learning of procedures
    from web instructional videos. Proceedings of the AAAI Conference on Artificial
    Intelligence 32(1) (2018). URL https://ojs.aaai.org/index.php/AAAI/article/view/12342'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (117) 周亮、许畅、科索：《从网络教学视频中自动学习流程的探索》。美国人工智能协会会议论文集 32(1) (2018)。网址 https://ojs.aaai.org/index.php/AAAI/article/view/12342
- en: '(118) Zhou, L., Zhou, Y., Corso, J.J., Socher, R., Xiong, C.: End-to-end dense
    video captioning with masked transformer. In: 2018 IEEE/CVF Conference on Computer
    Vision and Pattern Recognition, pp. 8739–8748 (2018). DOI 10.1109/CVPR.2018.00911'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (118) 周亮、周勇、科索·杰、索切尔、熊超：《基于遮罩变换器的端到端密集视频字幕生成》。发表于：2018 IEEE/CVF计算机视觉与模式识别会议，页码
    8739–8748 (2018)。DOI 10.1109/CVPR.2018.00911
- en: '(119) Zhu, X., Su, W., Lu, L., Li, B., Wang, X., Dai, J.: Deformable detr:
    Deformable transformers for end-to-end object detection (2021)'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (119) 朱晓、苏伟、陆林、李斌、王晓、戴俊：《可变形DETR：用于端到端目标检测的可变形变换器》（2021）
