- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 20:06:26'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:06:26
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1904.09146] Salient Object Detection in the Deep Learning Era: An In-depth
    Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1904.09146] 深度学习时代的显著目标检测：深入调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1904.09146](https://ar5iv.labs.arxiv.org/html/1904.09146)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1904.09146](https://ar5iv.labs.arxiv.org/html/1904.09146)
- en: Salient Object Detection in the
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 显著目标检测
- en: 'Deep Learning Era: An In-depth Survey'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习时代：深入调查
- en: Wenguan Wang, , Qiuxia Lai, Huazhu Fu, ,
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Wenguan Wang、Qiuxia Lai、Huazhu Fu、
- en: 'Jianbing Shen, , Haibin Ling, and Ruigang Yang, W. Wang is with ETH Zurich,
    Switzerland. (Email: wenguanwang.ai@gmail.com) Q. Lai is with the Department of
    Computer Science and Engineering, the Chinese University of Hong Kong, Hong Kong,
    China. (Email: qxlai@cse.cuhk.edu.hk) H. Fu is with Inception Institute of Artificial
    Intelligence, UAE. (Email: hzfu@ieee.org) J. Shen is with Beijing Laboratory of
    Intelligent Information Technology, School of Computer Science, Beijing Institute
    of Technology, China. (Email: shenjianbing@bit.edu.cn) H. Ling is with the Department
    of Computer and Information Sciences, Temple University, Philadelphia, PA, USA.
    (Email: hbling@temple.edu) R. Yang is with the University of Kentucky, Lexington,
    KY 40507\. (Email: ryang@cs.uky.edu) Corresponding author: Jianbing Shen'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Jianbing Shen、Haibin Ling和Ruigang Yang，W. Wang与瑞士苏黎世联邦理工学院（ETH Zurich）有关。 （电子邮件：wenguanwang.ai@gmail.com）
    Q. Lai与香港中文大学计算机科学与工程系有关。（电子邮件：qxlai@cse.cuhk.edu.hk） H. Fu与阿联酋的Inception人工智能研究所有关。（电子邮件：hzfu@ieee.org）
    J. Shen与北京理工大学计算机学院北京智能信息技术实验室有关。（电子邮件：shenjianbing@bit.edu.cn） H. Ling与美国宾夕法尼亚州费城的天普大学计算机与信息科学系有关。（电子邮件：hbling@temple.edu）
    R. Yang与肯塔基大学，肯塔基州列克星敦，邮政编码40507有关。（电子邮件：ryang@cs.uky.edu） 通讯作者：Jianbing Shen
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: As an essential problem in computer vision, salient object detection (SOD) has
    attracted an increasing amount of research attention over the years. Recent advances
    in SOD are predominantly led by deep learning-based solutions (named deep SOD).
    To enable in-depth understanding of deep SOD, in this paper, we provide a comprehensive
    survey covering various aspects, ranging from algorithm taxonomy to unsolved issues.
    In particular, we first review deep SOD algorithms from different perspectives,
    including network architecture, level of supervision, learning paradigm, and object-/instance-level
    detection. Following that, we summarize and analyze existing SOD datasets and
    evaluation metrics. Then, we benchmark a large group of representative SOD models,
    and provide detailed analyses of the comparison results. Moreover, we study the
    performance of SOD algorithms under different attribute settings, which has not
    been thoroughly explored previously, by constructing a novel SOD dataset with
    rich attribute annotations covering various salient object types, challenging
    factors, and scene categories. We further analyze, for the first time in the field,
    the robustness of SOD models to random input perturbations and adversarial attacks.
    We also look into the generalization and difficulty of existing SOD datasets.
    Finally, we discuss several open issues of SOD and outline future research directions.
    All the saliency prediction maps, our constructed dataset with annotations, and
    codes for evaluation are publicly available at [https://github.com/wenguanwang/SODsurvey](https://github.com/wenguanwang/SODsurvey).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 作为计算机视觉中的一个重要问题，显著目标检测（SOD）近年来吸引了越来越多的研究关注。近年来在SOD领域的进展主要由基于深度学习的解决方案（即深度SOD）主导。为了深入了解深度SOD，本文提供了一个全面的调查，涵盖了从算法分类到未解决问题的各个方面。特别是，我们首先从不同的角度回顾了深度SOD算法，包括网络架构、监督水平、学习范式以及对象/实例级检测。接下来，我们总结和分析了现有的SOD数据集和评估指标。然后，我们对一组具有代表性的SOD模型进行了基准测试，并提供了比较结果的详细分析。此外，我们通过构建一个包含各种显著目标类型、挑战因素和场景类别的丰富属性注释的创新SOD数据集，研究了SOD算法在不同属性设置下的性能，这是之前没有深入探索过的。我们首次在该领域分析了SOD模型对随机输入扰动和对抗攻击的鲁棒性。我们还探讨了现有SOD数据集的泛化能力和难度。最后，我们讨论了SOD的若干开放问题，并概述了未来的研究方向。所有的显著性预测图、我们构建的带注释的数据集以及评估代码都可以在[https://github.com/wenguanwang/SODsurvey](https://github.com/wenguanwang/SODsurvey)上公开获取。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Salient Object Detection, Deep Learning, Benchmark, Image Saliency.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 显著目标检测、深度学习、基准、图像显著性。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Salient object detection (SOD) aims at highlighting visually salient object
    regions in images. Here, ‘visually salient’ describes the property of an object
    or a region to attract human observers’ attention. SOD is driven by and applied
    to a wide spectrum of object-level applications in various areas. In computer
    vision, representative applications include image understanding [[1](#bib.bib1),
    [2](#bib.bib2)], image captioning [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)],
    object detection [[6](#bib.bib6), [7](#bib.bib7)], unsupervised video object segmentation [[8](#bib.bib8),
    [9](#bib.bib9)], semantic segmentation [[10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12)],
    person re-identification [[13](#bib.bib13), [14](#bib.bib14)], and video summarization [[15](#bib.bib15),
    [16](#bib.bib16)]. In computer graphics, SOD also plays an essential role in various
    tasks, including non-photorealistic rendering​ [[17](#bib.bib17), [18](#bib.bib18)],
    image cropping​ [[19](#bib.bib19), [20](#bib.bib20)], image retargeting [[21](#bib.bib21)],
    *etc*. Several applications in robotics, such as human-robot interaction [[22](#bib.bib22),
    [23](#bib.bib23)], and object discovery [[24](#bib.bib24), [25](#bib.bib25)],
    also benefit from SOD for better scene/object understanding.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 突出物体检测（SOD）的目标是突出图像中视觉显著的物体区域。在这里，“视觉显著”描述了一个物体或区域吸引人类观察者注意的属性。SOD 驱动并应用于各种领域中的广泛物体级应用。在计算机视觉中，代表性的应用包括图像理解 [[1](#bib.bib1),
    [2](#bib.bib2)]、图像描述 [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)]、物体检测 [[6](#bib.bib6),
    [7](#bib.bib7)]、无监督视频物体分割 [[8](#bib.bib8), [9](#bib.bib9)]、语义分割 [[10](#bib.bib10),
    [11](#bib.bib11), [12](#bib.bib12)]、人物再识别 [[13](#bib.bib13), [14](#bib.bib14)]
    和视频总结 [[15](#bib.bib15), [16](#bib.bib16)]。在计算机图形学中，SOD 也在各种任务中发挥着重要作用，包括非现实渲染 [[17](#bib.bib17),
    [18](#bib.bib18)]、图像裁剪 [[19](#bib.bib19), [20](#bib.bib20)]、图像重定向 [[21](#bib.bib21)]、*等等*。在机器人领域，如人机交互 [[22](#bib.bib22),
    [23](#bib.bib23)] 和物体发现 [[24](#bib.bib24), [25](#bib.bib25)]，SOD 也为更好的场景/物体理解提供了帮助。
- en: Though inspired by eye fixation prediction (FP) [[26](#bib.bib26)], which originated
    from cognitive and psychology research communities to investigate the human attention
    mechanism by predicting eye fixation positions in visual scenes, SOD differs in
    that it aims to detect the whole attentive object regions. Since the renaissance
    of deep learning techniques, significant improvement for SOD has been achieved
    in recent years, thanks to the powerful representation learning methods. Since
    the first introduction in 2015 ​[[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29)],
    deep learning-based SOD (or *deep SOD*) algorithms have quickly shown superior
    performance over traditional solutions, and have continued to improve the state-of-the-art.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管受到了眼动预测（FP）[[26](#bib.bib26)] 的启发，该方法源于认知和心理学研究界，通过预测视觉场景中的眼动位置来研究人类注意机制，但
    SOD 的不同之处在于它旨在检测整个关注物体区域。自深度学习技术复兴以来，SOD 在近年来取得了显著的进展，这要归功于强大的表示学习方法。自2015年首次介绍以来 [[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29)]，基于深度学习的 SOD（或 *深度 SOD*）算法迅速表现出优于传统解决方案的卓越性能，并持续推动了技术的最前沿。
- en: This paper provides a comprehensive and in-depth survey of SOD in the deep learning
    era. In addition to taxonomically reviewing existing deep SOD methods, it provides
    in-depth analyses of representative datasets and evaluation metrics, and investigates
    crucial but largely under-explored issues, such as the robustness and transferability
    of deep SOD models, their strengths and weaknesses under certain scenarios (*i.e*.,
    scene/salient object categories, challenging factors), as well as the generalizability
    and difficulty of SOD datasets. The saliency maps used for benchmarking, our constructed
    dataset, and evaluation codes are available at [https://github.com/wenguanwang/SODsurvey](https://github.com/wenguanwang/SODsurvey).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本文提供了对深度学习时代 SOD 的全面而深入的调查。除了分类回顾现有的深度 SOD 方法外，还提供了对代表性数据集和评估指标的深入分析，并调查了重要但大多未被深入探索的问题，如深度
    SOD 模型的鲁棒性和迁移性，它们在某些场景（*即*，场景/显著物体类别、挑战因素）下的优缺点，以及 SOD 数据集的一般性和难度。用于基准测试的显著性图、我们构建的数据集和评估代码可以在
    [https://github.com/wenguanwang/SODsurvey](https://github.com/wenguanwang/SODsurvey)
    找到。
- en: '![Refer to caption](img/3f5f283ff837292842364affecf6e7c7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/3f5f283ff837292842364affecf6e7c7.png)'
- en: <svg version="1.1" width="638.95" height="59.5" overflow="visible"><g transform="translate(0,59.5)
    scale(1,-1)"><g transform="translate(-668.33,69.19)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="53.964300539643" height="7.3336100733361"
    overflow="visible">[[30](#bib.bib30)]</foreignobject></g></g><g transform="translate(-567.32,69.19)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="113.32503113325"
    height="7.3336100733361" overflow="visible">[[31](#bib.bib31)]</foreignobject></g></g><g
    transform="translate(-467.69,69.19)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="89.1102808911028" height="7.3336100733361"
    overflow="visible">[[32](#bib.bib32)]</foreignobject></g></g><g transform="translate(-410.96,69.19)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="102.808911028089"
    height="7.3336100733361" overflow="visible">[[33](#bib.bib33)]</foreignobject></g></g><g
    transform="translate(-355.61,91.32)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="88.4184308841843" height="7.3336100733361"
    overflow="visible">[[34](#bib.bib34)]</foreignobject></g></g><g transform="translate(-361.15,68.49)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="103.639131036391"
    height="7.3336100733361" overflow="visible">[[35](#bib.bib35)]</foreignobject></g></g><g
    transform="translate(-309.95,68.49)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="87.7265808772658" height="7.3336100733361"
    overflow="visible">[[36](#bib.bib36)]</foreignobject></g></g><g transform="translate(-254.6,14.94)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="92.2927909229279"
    height="7.3336100733361" overflow="visible">[[29](#bib.bib29)]</foreignobject></g></g><g
    transform="translate(-208.94,14.94)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="123.841151238412" height="7.3336100733361"
    overflow="visible">[[37](#bib.bib37)]</foreignobject></g></g><g transform="translate(-204.23,71.4)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="78.5941607859416"
    height="7.3336100733361" overflow="visible">[[38](#bib.bib38)]</foreignobject></g></g><g
    transform="translate(-159.96,79.15)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="82.6068908260689" height="7.3336100733361"
    overflow="visible">[[39](#bib.bib39)]</foreignobject></g></g><g transform="translate(-105.71,88.56)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="81.2231908122319"
    height="7.3336100733361" overflow="visible">[[40](#bib.bib40)]</foreignobject></g></g><g
    transform="translate(-44.28,69.19)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="93.6764909367649" height="7.3336100733361"
    overflow="visible">[[41](#bib.bib41)]</foreignobject></g></g><g transform="translate(-85.24,14.94)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="70.0152207001522"
    height="7.3336100733361" overflow="visible">[[42](#bib.bib42)]</foreignobject></g></g></g></svg>
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: <svg version="1.1" width="638.95" height="59.5" overflow="visible"><g transform="translate(0,59.5)
    scale(1,-1)"><g transform="translate(-668.33,69.19)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="53.964300539643" height="7.3336100733361"
    overflow="visible">[[30](#bib.bib30)]</foreignobject></g></g><g transform="translate(-567.32,69.19)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="113.32503113325"
    height="7.3336100733361" overflow="visible">[[31](#bib.bib31)]</foreignobject></g></g><g
    transform="translate(-467.69,69.19)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="89.1102808911028" height="7.3336100733361"
    overflow="visible">[[32](#bib.bib32)]</foreignobject></g></g><g transform="translate(-410.96,69.19)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="102.808911028089"
    height="7.3336100733361" overflow="visible">[[33](#bib.bib33)]</foreignobject></g></g><g
    transform="translate(-355.61,91.32)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="88.4184308841843" height="7.3336100733361"
    overflow="visible">[[34](#bib.bib34)]</foreignobject></g></g><g transform="translate(-361.15,68.49)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="103.639131036391"
    height="7.3336100733361" overflow="visible">[[35](#bib.bib35)]</foreignobject></g></g><g
    transform="translate(-309.95,68.49)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="87.7265808772658" height="7.3336100733361"
    overflow="visible">[[36](#bib.bib36)]</foreignobject></g></g><g transform="translate(-254.6,14.94)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="92.2927909229279"
    height="7.3336100733361" overflow="visible">[[29](#bib.bib29)]</foreignobject></g></g><g
    transform="translate(-208.94,14.94)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="123.841151238412" height="7.3336100733361"
    overflow="visible">[[37](#bib.bib37)]</foreignobject></g></g><g transform="translate(-204.23,71.4)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="78.5941607859416"
    height="7.3336100733361" overflow="visible">[[38](#bib.bib38)]</foreignobject></g></g><g
    transform="translate(-159.96,79.15)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="82.6068908260689" height="7.3336100733361"
    overflow="visible">[[39](#bib.bib39)]</foreignobject></g></g><g transform="translate(-105.71,88.56)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="81.2231908122319"
    height="7.3336100733361" overflow="visible">[[40](#bib.bib40)]</foreignobject></g></g><g
    transform="translate(-44.28,69.19)"><g transform="translate(0,9.8242700982427)
    scale(1, -1)"><foreignobject width="93.6764909367649" height="7.3336100733361"
    overflow="visible">[[41](#bib.bib41)]</foreignobject></g></g><g transform="translate(-85.24,14.94)"><g
    transform="translate(0,9.8242700982427) scale(1, -1)"><foreignobject width="70.0152207001522"
    height="7.3336100733361" overflow="visible">[[42](#bib.bib42)]</foreignobject></g></g></g></svg>
- en: 'Figure 1: A brief chronology of SOD. The very first SOD models date back to
    the work of Liu *et al*. [[30](#bib.bib30)] and Achanta *et al*. [[31](#bib.bib31)].
    The first incorporation of deep learning techniques into SOD models was in $2015$.
    Listed methods are milestones, which are typically highly cited. See §[1.1](#S1.SS1
    "1.1 History and Scope ‣ 1 Introduction ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey") for more details.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '图1：SOD的简要年表。最早的SOD模型可以追溯到刘*等人* [[30](#bib.bib30)] 和Achanta*等人* [[31](#bib.bib31)]
    的工作。深度学习技术首次被应用到SOD模型中是在$2015$年。列出的这些方法都是里程碑，通常被高度引用。有关详细信息，请参见§[1.1](#S1.SS1
    "1.1 History and Scope ‣ 1 Introduction ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")。'
- en: 'TABLE I: Summary of previous reviews. For each work, the publication information
    and coverage are provided. See §[1.2](#S1.SS2 "1.2 Related Previous Reviews and
    Surveys ‣ 1 Introduction ‣ Salient Object Detection in the Deep Learning Era:
    An In-depth Survey") for more detailed descriptions.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '表I：之前综述的总结。每项工作的出版信息和覆盖范围均提供。有关更详细的描述，请参见§[1.2](#S1.SS2 "1.2 Related Previous
    Reviews and Surveys ‣ 1 Introduction ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey")。'
- en: '|  Title | Year | Venue | Description |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  标题 | 年份 | 发表期刊 | 描述 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| State-of-the-Art in Visual Attention Modeling [[43](#bib.bib43)] | 2013 |
    TPAMI | This paper reviews visual attention (*i.e*. fixation prediction) models
    before 2013. |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 视觉注意建模的最新进展 [[43](#bib.bib43)] | 2013 | TPAMI | 本文回顾了2013年前的视觉注意力（*即* 注视预测）模型。
    |'
- en: '| Salient Object Detection: A Benchmark [[44](#bib.bib44)] | 2015 | TIP | This
    paper benchmarks 29 heuristic SOD models and 10 FP methods over 7 datasets. |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 突出物体检测：基准 [[44](#bib.bib44)] | 2015 | TIP | 本文对29种启发式SOD模型和10种FP方法在7个数据集上的表现进行了基准测试。
    |'
- en: '| Attentive Systems: A Survey [[45](#bib.bib45)] | 2017 | IJCV | This paper
    reviews applications that utilize visual saliency cues. |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 专注系统：综述 [[45](#bib.bib45)] | 2017 | IJCV | 本文回顾了利用视觉显著性线索的应用。 |'
- en: '|'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A Review of Co-Saliency Detection Algorithms: &#124;'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 共显著性检测算法综述：&#124;'
- en: '&#124; Fundamentals, Applications, and Challenges [[46](#bib.bib46)] &#124;'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基础、应用和挑战 [[46](#bib.bib46)] &#124;'
- en: '| 2018 | TIST | This paper reviews the fundamentals, challenges, and applications
    of co-saliency detection. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | TIST | 本文回顾了共显著性检测的基础、挑战和应用。 |'
- en: '|'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Review of Visual Saliency Detection with Comprehen- &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉显著性检测的综合回顾 &#124;'
- en: '&#124; sive Information [[47](#bib.bib47)] &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息[[47](#bib.bib47)] &#124;'
- en: '| 2018 | TCSVT | This paper reviews RGB-D SOD, co-saliency detection and video
    SOD. |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | TCSVT | 本文回顾了RGB-D SOD、共显著性检测和视频SOD。 |'
- en: '|'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Advanced Deep-Learning Techniques for Salient and &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高级深度学习技术用于显著性和 &#124;'
- en: '&#124; Category-Specific Object Detection: A Survey [[48](#bib.bib48)] &#124;'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 类别特定的物体检测：综述 [[48](#bib.bib48)] &#124;'
- en: '| 2018 | SPM |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | SPM |'
- en: '&#124; This paper reviews several sub-directions of object detection, namely
    objectness detection, SOD &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文回顾了物体检测的几个子方向，即物体性检测、SOD &#124;'
- en: '&#124; and category-specific object detection. &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和类别特定的物体检测。 &#124;'
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Saliency Prediction in the Deep Learning Era: Successes &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 深度学习时代的显著性预测：成功案例 &#124;'
- en: '&#124; and Limitations [[49](#bib.bib49)] &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和局限性 [[49](#bib.bib49)] &#124;'
- en: '| 2019 | TPAMI | This paper reviews image and video fixation prediction models
    and analyzes specific questions. |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | TPAMI | 本文回顾了图像和视频注视预测模型，并分析了具体问题。 |'
- en: '| Salient Object Detection: A Survey [[50](#bib.bib50)] | 2019 | CVM |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 突出物体检测：综述 [[50](#bib.bib50)] | 2019 | CVM |'
- en: '&#124; This paper reviews 65 heuristic and 21 deep SOD models up to 2017 and
    discusses closely related &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文回顾了截至2017年的65种启发式和21种深度SOD模型，并讨论了密切相关的 &#124;'
- en: '&#124; areas like object detection, fixation prediction, segmentation, *etc*.
    &#124;'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 如物体检测、注视预测、分割等。 &#124;'
- en: '|'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 1.1 History and Scope
  id: totrans-50
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 历史与范围
- en: Humans are able to quickly allocate attention to important regions in visual
    scenes. Understanding and modeling such an astonishing ability, *i.e*., visual
    attention or visual saliency, is a fundamental research problem in psychology,
    neurobiology, cognitive science, and computer vision. There are two categories
    of computational models for visual saliency, namely FP and SOD. FP originated
    from cognitive and psychology communities [[26](#bib.bib26), [51](#bib.bib51),
    [52](#bib.bib52)], and targets at predicting where people look in images.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 人类能够迅速将注意力分配到视觉场景中的重要区域。理解和建模这种令人惊叹的能力，即视觉注意力或视觉显著性，是心理学、神经生物学、认知科学和计算机视觉中的一个基础研究问题。视觉显著性的计算模型分为两类，即
    FP 和 SOD。FP 起源于认知和心理学领域[[26](#bib.bib26), [51](#bib.bib51), [52](#bib.bib52)]，旨在预测人们在图像中查看的地方。
- en: The history of SOD is relatively short and can be traced back to [[30](#bib.bib30)]
    and [[31](#bib.bib31)]. The rise of SOD has been driven by a wide range of object-level
    computer vision applications. Instead of FP models only predicting sparse eye
    fixation locations, SOD models aim to detect the whole entities of the visually
    attractive objects with precise boundaries. Most traditional, non-deep SOD models [[53](#bib.bib53),
    [36](#bib.bib36)] rely on low-level features and certain heuristics (*e.g*., *color
    contrast* [[32](#bib.bib32)], *background prior* [[54](#bib.bib54)]). To obtain
    uniformly highlighted salient objects and clear object boundaries, an over-segmentation
    process that generates regions [[55](#bib.bib55)], super-pixels [[56](#bib.bib56),
    [57](#bib.bib57)], or object proposals [[58](#bib.bib58)] is often integrated
    into these models. Please see [[44](#bib.bib44)] for a more comprehensive overview.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: SOD 的历史相对较短，可追溯至[[30](#bib.bib30)]和[[31](#bib.bib31)]。SOD 的兴起受到各种对象级计算机视觉应用的推动。与仅预测稀疏眼动位置的
    FP 模型不同，SOD 模型旨在检测视觉吸引力对象的整个实体及其精确边界。大多数传统的非深度 SOD 模型[[53](#bib.bib53), [36](#bib.bib36)]依赖于低级特征和某些启发式方法（*例如*，*颜色对比*[[32](#bib.bib32)]，*背景先验*[[54](#bib.bib54)]）。为了获得均匀高亮的显著对象和清晰的对象边界，通常会将过度分割过程（生成区域[[55](#bib.bib55)]、超像素[[56](#bib.bib56),
    [57](#bib.bib57)]或对象提议[[58](#bib.bib58)]）集成到这些模型中。有关更全面的概述，请参见[[44](#bib.bib44)]。
- en: 'With the compelling success of deep learning technologies in computer vision,
    more and more deep SOD methods have begun springing up since $2015$. Earlier deep
    SOD models utilized multi-layer perceptron (MLP) classifiers to predict the saliency
    score of deep features extracted from each image processing unit [[29](#bib.bib29),
    [28](#bib.bib28), [27](#bib.bib27)]. Later, a more effective and efficient form,
    *i.e*., fully convolutional network (FCN)-based model, became the mainstream SOD
    architecture. Some recent methods [[42](#bib.bib42), [41](#bib.bib41)] also introduced
    Capsule [[59](#bib.bib59)] into SOD to comprehensively address object property
    modeling. A brief chronology of SOD is shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1
    Introduction ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '随着深度学习技术在计算机视觉中的巨大成功，越来越多的深度 SOD 方法自 $2015$ 年起开始涌现。早期的深度 SOD 模型利用多层感知器（MLP）分类器来预测从每个图像处理单元中提取的深度特征的显著性分数[[29](#bib.bib29),
    [28](#bib.bib28), [27](#bib.bib27)]。随后，一种更有效的形式，即基于全卷积网络（FCN）的模型，成为了主流 SOD 架构。一些最近的方法[[42](#bib.bib42),
    [41](#bib.bib41)]还将 Capsule[[59](#bib.bib59)] 引入 SOD，以全面解决对象属性建模。SOD 的简要年表见图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Salient Object Detection in the Deep Learning Era:
    An In-depth Survey")。'
- en: Scope of the survey. Despite its short history, research in deep SOD has produced
    hundreds of papers, making it impractical (and fortunately unnecessary) to review
    all of them. Instead, we comprehensively select influential papers published in
    prestigious journals and conferences. This survey mainly focuses on the major
    progress in the last five years, but for completeness and better readability,
    some early related works are also included. Due to limitations on space and our
    knowledge, we apologize to those authors whose works are not included in this
    paper. It is worth noting that we restrict this survey to single-image SOD methods,
    and leave RGB-D SOD, co-saliency detection, video SOD, *etc*., as separate topics.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 调查的范围。尽管历史较短，深度显著目标检测的研究已经产生了数百篇论文，因此回顾所有论文是不切实际的（幸运的是不必要的）。我们综合选择了在权威期刊和会议上发表的有影响力的论文。本调查主要关注过去五年的重大进展，但为了完整性和更好的可读性，也包含了一些早期相关工作。由于空间和知识限制，对未能纳入本文的作者表示歉意。值得注意的是，我们将本调查限制在单图像显著目标检测方法上，将RGB-D显著目标检测、共同显著性检测、视频显著目标检测等作为单独主题。
- en: 1.2 Related Previous Reviews and Surveys
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 相关的先前综述和调查
- en: 'Table [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Salient Object Detection in the
    Deep Learning Era: An In-depth Survey") lists existing surveys that are related
    to ours. Among them, Borji *et al*. [[44](#bib.bib44)] reviewed SOD methods preceding
    2015, thus do not refer to recent deep learning-based solutions. Zhang *et al*. [[46](#bib.bib46)]
    reviewed methods for co-saliency detection, *i.e*., detecting common salient objects
    from multiple relevant images. Cong *et al*. [[47](#bib.bib47)] reviewed several
    extended SOD tasks including RGB-D SOD, co-saliency detection and video SOD. Han *et
    al*. [[48](#bib.bib48)] looked into several sub-directions of object detection,
    and outlined recent progress in objectness detection, SOD, and category-specific
    object detection. Borji *et al*. summarized both heuristic [[43](#bib.bib43)]
    and deep models [[49](#bib.bib49)] for FP. Nguyen *et al*. [[45](#bib.bib45)]
    focused on categorizing the applications of visual saliency (including both SOD
    and FP) in different areas. Finally, a more recently published survey [[50](#bib.bib50)]
    covers both traditional non-deep SOD methods and deep ones until 2017, and discusses
    their relation to several other closely-related research areas, such as special-purpose
    object detection and segmentation.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '表[I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")列出了与我们工作相关的现有调查。在这些调查中，Borji *et al* [[44](#bib.bib44)]
    回顾了2015年前的显著目标检测方法，因此未涉及近期基于深度学习的解决方案。Zhang *et al* [[46](#bib.bib46)] 回顾了共同显著性检测的方法，即从多个相关图像中检测共同的显著目标。Cong
    *et al* [[47](#bib.bib47)] 回顾了几种扩展的显著目标检测任务，包括RGB-D显著目标检测、共同显著性检测和视频显著目标检测。Han
    *et al* [[48](#bib.bib48)] 探讨了目标检测的几个子方向，并概述了在目标性检测、显著目标检测和类别特定目标检测方面的最新进展。Borji
    *et al* 总结了FP的启发式 [[43](#bib.bib43)] 和深度模型 [[49](#bib.bib49)]。Nguyen *et al* [[45](#bib.bib45)]
    侧重于分类视觉显著性（包括显著目标检测和FP）在不同领域的应用。最后，一项最近发表的调查 [[50](#bib.bib50)] 涵盖了直到2017年的传统非深度显著目标检测方法和深度方法，并讨论了它们与其他几个密切相关的研究领域，如专用目标检测和分割的关系。'
- en: 'TABLE II: Taxonomies and representative publications of deep SOD methods. See
    §[2](#S2 "2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey") for more detailed descriptions.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '表II：深度显著目标检测方法的分类和代表性出版物。有关更详细的描述，请参见§[2](#S2 "2 Deep Learning based SOD Models
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")。'
- en: '|       Category | Publications |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '|       分类 | 出版物 |'
- en: '| Network Architectures (§[2.1](#S2.SS1 "2.1 Representative Network Architectures
    for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")) | Multi-layer perceptron | 1) Super-pixel/patch-based
    | [[29](#bib.bib29)], [[60](#bib.bib60)], [[27](#bib.bib27)], [[61](#bib.bib61)]
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 网络架构 (§[2.1](#S2.SS1 "2.1 Representative Network Architectures for SOD ‣
    2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey")) | 多层感知器 | 1) 超像素/补丁基础 | [[29](#bib.bib29)], [[60](#bib.bib60)],
    [[27](#bib.bib27)], [[61](#bib.bib61)] |'
- en: '| (MLP)-based | 2) Object proposal based | [[28](#bib.bib28)], [[37](#bib.bib37)],
    [[62](#bib.bib62)] |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| (MLP) 基于 | 2) 基于目标提议 | [[28](#bib.bib28)], [[37](#bib.bib37)], [[62](#bib.bib62)]
    |'
- en: '| Fully convolutional network (FCN)-based | 1) Single-stream | [[63](#bib.bib63)],
    [[64](#bib.bib64)], [[65](#bib.bib65)], [[66](#bib.bib66)], [[67](#bib.bib67)],
    [[68](#bib.bib68)], [[69](#bib.bib69)] |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 基于全卷积网络 (FCN) 的 | 1) 单流 | [[63](#bib.bib63)], [[64](#bib.bib64)], [[65](#bib.bib65)],
    [[66](#bib.bib66)], [[67](#bib.bib67)], [[68](#bib.bib68)], [[69](#bib.bib69)]
    |'
- en: '| 2) Multi-stream | [[70](#bib.bib70)], [[71](#bib.bib71)], [[72](#bib.bib72)],
    [[73](#bib.bib73)], [[74](#bib.bib74)] |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 2) 多流 | [[70](#bib.bib70)], [[71](#bib.bib71)], [[72](#bib.bib72)], [[73](#bib.bib73)],
    [[74](#bib.bib74)] |'
- en: '| 3) Side-fusion | [[39](#bib.bib39)], [[75](#bib.bib75)], [[76](#bib.bib76)],
    [[77](#bib.bib77)], [[78](#bib.bib78)], [[79](#bib.bib79)], [[80](#bib.bib80)],
    [[81](#bib.bib81)], [[82](#bib.bib82)] |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 3) 侧融合 | [[39](#bib.bib39)], [[75](#bib.bib75)], [[76](#bib.bib76)], [[77](#bib.bib77)],
    [[78](#bib.bib78)], [[79](#bib.bib79)], [[80](#bib.bib80)], [[81](#bib.bib81)],
    [[82](#bib.bib82)] |'
- en: '| 4) Bottom-up/top-down | [[38](#bib.bib38)], [[83](#bib.bib83)], [[84](#bib.bib84)],
    [[85](#bib.bib85)], [[86](#bib.bib86)], [[87](#bib.bib87)], [[40](#bib.bib40)],
    [[88](#bib.bib88)], [[89](#bib.bib89)], [[90](#bib.bib90)], [[91](#bib.bib91)],
    [[92](#bib.bib92)], [[93](#bib.bib93)], [[94](#bib.bib94)], [[95](#bib.bib95)]
    |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 4) 自下而上/自上而下 | [[38](#bib.bib38)], [[83](#bib.bib83)], [[84](#bib.bib84)],
    [[85](#bib.bib85)], [[86](#bib.bib86)], [[87](#bib.bib87)], [[40](#bib.bib40)],
    [[88](#bib.bib88)], [[89](#bib.bib89)], [[90](#bib.bib90)], [[91](#bib.bib91)],
    [[92](#bib.bib92)], [[93](#bib.bib93)], [[94](#bib.bib94)], [[95](#bib.bib95)]
    |'
- en: '| 5) Branched | [[96](#bib.bib96)], [[97](#bib.bib97)], [[98](#bib.bib98)],
    [[99](#bib.bib99)], [[100](#bib.bib100)], [[101](#bib.bib101)], [[102](#bib.bib102)],
    [[103](#bib.bib103)] |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 5) 分支 | [[96](#bib.bib96)], [[97](#bib.bib97)], [[98](#bib.bib98)], [[99](#bib.bib99)],
    [[100](#bib.bib100)], [[101](#bib.bib101)], [[102](#bib.bib102)], [[103](#bib.bib103)]
    |'
- en: '| Hybrid network-based |  | [[104](#bib.bib104)], [[105](#bib.bib105)] |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 混合网络基础 |  | [[104](#bib.bib104)], [[105](#bib.bib105)] |'
- en: '| Capsule-based |  | [[41](#bib.bib41)], [[42](#bib.bib42)] |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 胶囊基础 |  | [[41](#bib.bib41)], [[42](#bib.bib42)] |'
- en: '| Level of Supervision (§[2.2](#S2.SS2 "2.2 Level of Supervision ‣ 2 Deep Learning
    based SOD Models ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey")) | Fully-supervised |  | All others |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 监督水平 (§[2.2](#S2.SS2 "2.2 监督水平 ‣ 2 基于深度学习的显著性目标检测模型 ‣ 深度学习时代的显著性目标检测：深入调查"))
    | 全监督 |  | 其他 |'
- en: '| Un-/Weakly-supervised | 1) Category-level | [[97](#bib.bib97)], [[68](#bib.bib68)],
    [[69](#bib.bib69)], [[81](#bib.bib81)] |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 无监督/弱监督 | 1) 类别级 | [[97](#bib.bib97)], [[68](#bib.bib68)], [[69](#bib.bib69)],
    [[81](#bib.bib81)] |'
- en: '| 2) Pseudo pixel-level | [[83](#bib.bib83)], [[98](#bib.bib98)], [[67](#bib.bib67)],
    [[99](#bib.bib99)] |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 2) 伪像素级 | [[83](#bib.bib83)], [[98](#bib.bib98)], [[67](#bib.bib67)], [[99](#bib.bib99)]
    |'
- en: '| Learning Paradigm (§[2.3](#S2.SS3 "2.3 Learning Paradigm ‣ 2 Deep Learning
    based SOD Models ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey")) | Single-task learning (STL) |  | All others |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 学习范式 (§[2.3](#S2.SS3 "2.3 学习范式 ‣ 2 基于深度学习的显著性目标检测模型 ‣ 深度学习时代的显著性目标检测：深入调查"))
    | 单任务学习 (STL) |  | 其他 |'
- en: '| Mingle-task learning (MTL) | 1) Salient object subitizing | [[37](#bib.bib37)],
    [[77](#bib.bib77)], [[79](#bib.bib79)] |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 混合任务学习 (MTL) | 1) 显著性目标计数 | [[37](#bib.bib37)], [[77](#bib.bib77)], [[79](#bib.bib79)]
    |'
- en: '| 2) Fixation prediction | [[96](#bib.bib96)], [[87](#bib.bib87)] |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 2) 注视点预测 | [[96](#bib.bib96)], [[87](#bib.bib87)] |'
- en: '| 3) Image classification | [[97](#bib.bib97)], [[98](#bib.bib98)] |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| 3) 图像分类 | [[97](#bib.bib97)], [[98](#bib.bib98)] |'
- en: '| 4) Semantic segmentation | [[63](#bib.bib63)], [[103](#bib.bib103)] |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 4) 语义分割 | [[63](#bib.bib63)], [[103](#bib.bib103)] |'
- en: '| 5) Contour/edge detection | [[75](#bib.bib75)], [[99](#bib.bib99)], [[89](#bib.bib89)],
    [[91](#bib.bib91)], [[92](#bib.bib92)], [[93](#bib.bib93)], [[101](#bib.bib101)],
    [[82](#bib.bib82)], [[102](#bib.bib102)] |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 5) 轮廓/边缘检测 | [[75](#bib.bib75)], [[99](#bib.bib99)], [[89](#bib.bib89)],
    [[91](#bib.bib91)], [[92](#bib.bib92)], [[93](#bib.bib93)], [[101](#bib.bib101)],
    [[82](#bib.bib82)], [[102](#bib.bib102)] |'
- en: '| 6) Image captioning | [[100](#bib.bib100)] |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 6) 图像描述 | [[100](#bib.bib100)] |'
- en: '| Object-/Instance- Level (§[2.4](#S2.SS4 "2.4 Object-/Instance-Level SOD ‣
    2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey")) | Object-level |  | All others |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 对象/实例级 (§[2.4](#S2.SS4 "2.4 对象/实例级显著性目标检测 ‣ 2 基于深度学习的显著性目标检测模型 ‣ 深度学习时代的显著性目标检测：深入调查"))
    | 对象级 |  | 其他 |'
- en: '| Instance-level |  | [[37](#bib.bib37)], [[70](#bib.bib70)] |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 实例级 |  | [[37](#bib.bib37)], [[70](#bib.bib70)] |'
- en: 'Different from previous SOD surveys, which focus on earlier non-deep learning
    SOD methods [[44](#bib.bib44)], other related fields [[47](#bib.bib47), [48](#bib.bib48),
    [43](#bib.bib43), [49](#bib.bib49)], practical applications [[45](#bib.bib45)]
    or a limited number of deep SOD models [[50](#bib.bib50)], this work systematically
    and comprehensively reviews recent advances in the field. It features in-depth
    analyses and discussions on various aspects, many of which, to the best of our
    knowledge, have never been explored in this field. In particular, we comprehensively
    summarize and discuss existing deep SOD methods under several proposed taxonomies
    (§[2](#S2 "2 Deep Learning based SOD Models ‣ Salient Object Detection in the
    Deep Learning Era: An In-depth Survey")); review datasets (§[3](#S3 "3 SOD Datasets
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")) and
    evaluation metrics (§[4](#S4 "4 Evaluation Metrics ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey")) with their pros and cons; provide
    a deeper understanding of SOD models through an attribute-based evaluation (§[5.3](#S5.SS3
    "5.3 Attribute-Based Study ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey")); discuss the influence
    of input perturbation (§[5.4](#S5.SS4 "5.4 Robustness Against General Input Perturbations
    ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")); analyze the robustness of deep SOD models
    to adversarial attacks (§[5.5](#S5.SS5 "5.5 Robustness Against Manually Designed
    Input Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey")); study the generalization and
    difficulty of existing SOD datasets (§[5.6](#S5.SS6 "5.6 Cross-Dataset Generalization
    Evaluation ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey")); and offer insight into essential
    open issues, challenges, and future directions (§[6](#S6 "6 More Discussions ‣
    Salient Object Detection in the Deep Learning Era: An In-depth Survey")). We expect
    our survey to provide novel insight and inspiration that will facilitate the understanding
    of deep SOD, and foster research on the open issues raised.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '与以往关注早期非深度学习SOD方法[[44](#bib.bib44)]、其他相关领域[[47](#bib.bib47)、[48](#bib.bib48)、[43](#bib.bib43)、[49](#bib.bib49)]、实际应用[[45](#bib.bib45)]或有限数量的深度SOD模型[[50](#bib.bib50)]的SOD调查不同，本工作系统而全面地回顾了该领域的最新进展。它对各种方面进行了深入分析和讨论，其中许多方面据我们所知，在这一领域尚未被探索。特别是，我们在几个提出的分类体系下综合总结和讨论现有深度SOD方法（§[2](#S2
    "2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey)）；回顾数据集（§[3](#S3 "3 SOD Datasets ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey)") 和评估指标（§[4](#S4 "4 Evaluation Metrics
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey)") 的优缺点；通过基于属性的评估（§[5.3](#S5.SS3
    "5.3 Attribute-Based Study ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey)") 提供对SOD模型的更深刻理解；讨论输入扰动的影响（§[5.4](#S5.SS4
    "5.4 Robustness Against General Input Perturbations ‣ 5 Benchmarking and Empirical
    Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey)")；分析深度SOD模型对对抗攻击的鲁棒性（§[5.5](#S5.SS5
    "5.5 Robustness Against Manually Designed Input Perturbations ‣ 5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey)")；研究现有SOD数据集的泛化能力和难度（§[5.6](#S5.SS6 "5.6 Cross-Dataset Generalization
    Evaluation ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey)")；并提供对重要开放问题、挑战和未来方向的见解（§[6](#S6
    "6 More Discussions ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey)")。我们期望我们的综述能够提供新颖的见解和灵感，促进对深度SOD的理解，并推动对提出的开放问题的研究。'
- en: 1.3 Our Contributions
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 我们的贡献
- en: 'Our contributions in this paper are summarized as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本文中的贡献总结如下：
- en: '1.'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: A systematic review of deep SOD models from various perspectives. We categorize
    and summarize existing deep SOD models according to network architecture, level
    of supervision, learning paradigm, *etc*. The proposed taxonomies aim to help
    researchers gain a deeper understanding of the key features of deep SOD models.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对各种深度显著性目标检测（SOD）模型进行系统性综述。我们根据网络架构、监督级别、学习范式、*等等*对现有的深度SOD模型进行分类和总结。所提出的分类体系旨在帮助研究人员更深入地理解深度SOD模型的关键特征。
- en: '2.'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: An attribute-based performance evaluation of SOD models. We compile a hybrid
    dataset and provide annotated attributes for object categories, scene categories,
    and challenging factors. By evaluating several representative SOD models on it,
    we uncover the strengths and weaknesses of deep and non-deep approaches, opening
    up promising directions for future efforts.
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于属性的SOD模型性能评估。我们编译了一个混合数据集，并提供了物体类别、场景类别和挑战因素的标注属性。通过对几个具有代表性的SOD模型进行评估，我们揭示了深度和非深度方法的优缺点，为未来的研究开辟了有希望的方向。
- en: '3.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: An analysis of the robustness of SOD models against general input perturbations.
    To study the robustness of SOD models, we investigate the effects of various perturbations
    on the final performance of deep and non-deep SOD models. Some results are somewhat
    unexpected.
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SOD模型对一般输入扰动的鲁棒性分析。为了研究SOD模型的鲁棒性，我们调查了各种扰动对深度和非深度SOD模型最终性能的影响。一些结果有些出乎意料。
- en: '4.'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '4.'
- en: The first known adversarial attack analysis for SOD models. We further examine
    the robustness of SOD models against intentionally designed perturbations, *i.e*.,
    adversarial attacks. The specially designed attacks and evaluations can serve
    as baselines for further studying the robustness and transferability of deep SOD
    models.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一次已知的SOD模型对抗性攻击分析。我们进一步检验了SOD模型对有意设计的扰动，即对抗性攻击的鲁棒性。特别设计的攻击和评估可以作为进一步研究深度SOD模型鲁棒性和可迁移性的基线。
- en: '5.'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '5.'
- en: Cross-dataset generalization study. To analyze the generalization and difficulty
    of existing SOD datasets in-depth, we conduct a cross-dataset generalization study
    that quantitatively reveals the dataset bias.
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 跨数据集泛化研究。为了深入分析现有SOD数据集的泛化能力和难度，我们进行了一项跨数据集泛化研究，定量揭示了数据集的偏差。
- en: '6.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '6.'
- en: Overview of open issues and future directions. We thoroughly look over several
    essential issues (*i.e*., model design, dataset collection, *etc*.), shedding
    light on potential directions for future research.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 开放问题和未来方向概述。我们全面审视了几个关键问题（*即*，模型设计、数据集收集，*等*），阐明了未来研究的潜在方向。
- en: These contributions together comprise an exhaustive, up-to-date, and in-depth
    survey, and differentiate it from previous review papers significantly.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这些贡献共同组成了一份详尽、最新和深入的综述，并与以往的综述论文有显著区别。
- en: 'TABLE III: Summary of essential characteristics for popular SOD methods. Here,
    ‘#Training’ is the number of training images, and ‘CRF’ denotes whether the predictions
    are post-processed by conditional random field [[106](#bib.bib106)]. See §[2](#S2
    "2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey") for more detailed descriptions.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '表III：流行SOD方法的基本特征总结。这里，“#Training”是训练图像的数量，“CRF”表示预测是否经过条件随机场后处理 [[106](#bib.bib106)]。有关详细描述，请参见§[2](#S2
    "2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey")。'
- en: '|   | Methods | Publ. | Architecture | Backbone |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|   | Methods | Publ. | Architecture | Backbone |'
- en: '&#124; Level of &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Level of &#124;'
- en: '&#124; Supervision &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Supervision &#124;'
- en: '|'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Learning &#124;'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Learning &#124;'
- en: '&#124; Paradigm &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Paradigm &#124;'
- en: '|'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Obj.-/Inst.- &#124;'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Obj.-/Inst.- &#124;'
- en: '&#124; Level SOD &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; Level SOD &#124;'
- en: '| Training Dataset | #Training | CRF |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Training Dataset | #Training | CRF |'
- en: '| 2015 | SuperCNN [[61](#bib.bib61)] | IJCV | MLP+super-pixel | - | Fully-Sup.
    | STL | Object | ECSSD [[55](#bib.bib55)] | 800 |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | SuperCNN [[61](#bib.bib61)] | IJCV | MLP+super-pixel | - | Fully-Sup.
    | STL | Object | ECSSD [[55](#bib.bib55)] | 800 |  |'
- en: '| MCDL [[29](#bib.bib29)] | CVPR | MLP+super-pixel | GoogleNet | Fully-Sup.
    | STL | Object | MSRA10K [[107](#bib.bib107)] | 8,000 |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| MCDL [[29](#bib.bib29)] | CVPR | MLP+super-pixel | GoogleNet | Fully-Sup.
    | STL | Object | MSRA10K [[107](#bib.bib107)] | 8,000 |  |'
- en: '| LEGS [[28](#bib.bib28)] | CVPR | MLP+segment | - | Fully-Sup. | STL | Object
    | MSRA-B [[30](#bib.bib30)]+PASCAL-S [[108](#bib.bib108)] | 3,000+340 |  |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| LEGS [[28](#bib.bib28)] | CVPR | MLP+segment | - | Fully-Sup. | STL | Object
    | MSRA-B [[30](#bib.bib30)]+PASCAL-S [[108](#bib.bib108)] | 3,000+340 |  |'
- en: '| MDF [[27](#bib.bib27)] | CVPR | MLP+segment | - | Fully-Sup. | STL | Object
    | MSRA-B [[30](#bib.bib30)] | 2,500 |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| MDF [[27](#bib.bib27)] | CVPR | MLP+segment | - | Fully-Sup. | STL | Object
    | MSRA-B [[30](#bib.bib30)] | 2,500 |  |'
- en: '| 2016 | ELD [[60](#bib.bib60)] | CVPR | MLP+super-pixel | VGGNet | Fully-Sup.
    | STL | Object | MSRA10K [[107](#bib.bib107)] | $\sim$9,000 |  |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | ELD [[60](#bib.bib60)] | CVPR | MLP+super-pixel | VGGNet | Fully-Sup.
    | STL | Object | MSRA10K [[107](#bib.bib107)] | $\sim$9,000 |  |'
- en: '| DHSNet [[38](#bib.bib38)] | CVPR | FCN | VGGNet | Fully-Sup. | STL | Object
    | MSRA10K [[107](#bib.bib107)]+DUT-OMRON [[56](#bib.bib56)] | 6,000+3,500 |  |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| DHSNet [[38](#bib.bib38)] | CVPR | FCN | VGGNet | 全监督 | STL | 物体 | MSRA10K [[107](#bib.bib107)]+DUT-OMRON [[56](#bib.bib56)]
    | 6,000+3,500 |  |'
- en: '| DCL [[104](#bib.bib104)] | CVPR | FCN | VGGNet | Fully-Sup. | STL | Object
    | MSRA-B [[30](#bib.bib30)] | 2,500 | ✓ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| DCL [[104](#bib.bib104)] | CVPR | FCN | VGGNet | 全监督 | STL | 物体 | MSRA-B [[30](#bib.bib30)]
    | 2,500 | ✓ |'
- en: '| RACDNN [[64](#bib.bib64)] | CVPR | FCN | VGGNet | Fully-Sup. | STL | Object
    | DUT-OMRON [[56](#bib.bib56)]+NJU2000 [[109](#bib.bib109)]+RGBD [[110](#bib.bib110)]
    | 10,565 |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| RACDNN [[64](#bib.bib64)] | CVPR | FCN | VGGNet | 全监督 | STL | 物体 | DUT-OMRON [[56](#bib.bib56)]+NJU2000 [[109](#bib.bib109)]+RGBD [[110](#bib.bib110)]
    | 10,565 |  |'
- en: '| SU [[96](#bib.bib96)] | CVPR | FCN | VGGNet | Fully-Sup. | MTL | Object |
    MSRA10K [[107](#bib.bib107)]+SALICON [[111](#bib.bib111)] | 10,000+15,000 | ✓
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| SU [[96](#bib.bib96)] | CVPR | FCN | VGGNet | 全监督 | MTL | 物体 | MSRA10K [[107](#bib.bib107)]+SALICON [[111](#bib.bib111)]
    | 10,000+15,000 | ✓ |'
- en: '| MAP [[37](#bib.bib37)] | CVPR | MLP+obj. prop. | VGGNet | Fully-Sup. | MTL
    | Instance | SOS [[112](#bib.bib112)] | $\sim$5,500 |  |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| MAP [[37](#bib.bib37)] | CVPR | MLP+obj. prop. | VGGNet | 全监督 | MTL | 实例
    | SOS [[112](#bib.bib112)] | $\sim$5,500 |  |'
- en: '| SSD [[62](#bib.bib62)] | ECCV | MLP+obj. prop. | AlexNet | Fully-Sup. | STL
    | Object | MSRA-B [[30](#bib.bib30)] | 2,500 |  |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| SSD [[62](#bib.bib62)] | ECCV | MLP+obj. prop. | AlexNet | 全监督 | STL | 物体
    | MSRA-B [[30](#bib.bib30)] | 2,500 |  |'
- en: '| CRPSD [[105](#bib.bib105)] | ECCV | FCN | VGGNet | Fully-Sup. | STL | Object
    | MSRA10K [[107](#bib.bib107)] | 10,000 |  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| CRPSD [[105](#bib.bib105)] | ECCV | FCN | VGGNet | 全监督 | STL | 物体 | MSRA10K [[107](#bib.bib107)]
    | 10,000 |  |'
- en: '| RFCN [[63](#bib.bib63)] | ECCV | FCN | VGGNet | Fully-Sup. | MTL | Object
    | PASCAL VOC 2010 [[113](#bib.bib113)]+MSRA10K [[107](#bib.bib107)] | 10,103+10,000
    |  |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| RFCN [[63](#bib.bib63)] | ECCV | FCN | VGGNet | 全监督 | MTL | 物体 | PASCAL VOC
    2010 [[113](#bib.bib113)]+MSRA10K [[107](#bib.bib107)] | 10,103+10,000 |  |'
- en: '| 2017 | MSRNet [[70](#bib.bib70)] | CVPR | FCN | VGGNet | Fully-Sup. | STL
    | Instance | MSRA-B [[30](#bib.bib30)]+HKU-IS [[27](#bib.bib27)] (+ILSO [[70](#bib.bib70)])
    | 2,500+2,500 (+500) | ✓ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 2017 | MSRNet [[70](#bib.bib70)] | CVPR | FCN | VGGNet | 全监督 | STL | 实例 |
    MSRA-B [[30](#bib.bib30)]+HKU-IS [[27](#bib.bib27)] (+ILSO [[70](#bib.bib70)])
    | 2,500+2,500 (+500) | ✓ |'
- en: '| DSS [[39](#bib.bib39)] | CVPR | FCN | VGGNet | Fully-Sup. | STL | Object
    | MSRA-B [[30](#bib.bib30)]+HKU-IS [[27](#bib.bib27)] | 2,500 | ✓ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| DSS [[39](#bib.bib39)] | CVPR | FCN | VGGNet | 全监督 | STL | 物体 | MSRA-B [[30](#bib.bib30)]+HKU-IS [[27](#bib.bib27)]
    | 2,500 | ✓ |'
- en: '| WSS [[97](#bib.bib97)] | CVPR | FCN | VGGNet | Weakly-Sup. | MTL | Object
    | ImageNet [[114](#bib.bib114)] | 456k | ✓ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| WSS [[97](#bib.bib97)] | CVPR | FCN | VGGNet | 弱监督 | MTL | 物体 | ImageNet [[114](#bib.bib114)]
    | 456k | ✓ |'
- en: '| DLS [[65](#bib.bib65)] | CVPR | FCN | VGGNet | Fully-Sup. | STL | Object
    | MSRA10K [[107](#bib.bib107)] | 10,000 |  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| DLS [[65](#bib.bib65)] | CVPR | FCN | VGGNet | 全监督 | STL | 物体 | MSRA10K [[107](#bib.bib107)]
    | 10,000 |  |'
- en: '| NLDF [[75](#bib.bib75)] | CVPR | FCN | VGGNet | Fully-Sup. | MTL | Object
    | MSRA-B [[30](#bib.bib30)] | 2,500 | ✓ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| NLDF [[75](#bib.bib75)] | CVPR | FCN | VGGNet | 全监督 | MTL | 物体 | MSRA-B [[30](#bib.bib30)]
    | 2,500 | ✓ |'
- en: '| DSOS [[77](#bib.bib77)] | ICCV | FCN | VGGNet | Fully-Sup. | MTL | Object
    | SOS [[112](#bib.bib112)] | 6,900 |  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| DSOS [[77](#bib.bib77)] | ICCV | FCN | VGGNet | 全监督 | MTL | 物体 | SOS [[112](#bib.bib112)]
    | 6,900 |  |'
- en: '| Amulet [[76](#bib.bib76)] | ICCV | FCN | VGGNet | Fully-Sup. | STL | Object
    | MSRA10K [[107](#bib.bib107)] | 10,000 |  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| Amulet [[76](#bib.bib76)] | ICCV | FCN | VGGNet | 全监督 | STL | 物体 | MSRA10K [[107](#bib.bib107)]
    | 10,000 |  |'
- en: '| FSN [[72](#bib.bib72)] | ICCV | FCN | VGGNet | Fully-Sup. | STL | Object
    | MSRA10K [[107](#bib.bib107)] | 10,000 |  |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| FSN [[72](#bib.bib72)] | ICCV | FCN | VGGNet | 全监督 | STL | 物体 | MSRA10K [[107](#bib.bib107)]
    | 10,000 |  |'
- en: '| SBF [[83](#bib.bib83)] | ICCV | FCN | VGGNet | Un-Sup. | STL | Object | MSRA10K [[107](#bib.bib107)]
    | 10,000 |  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| SBF [[83](#bib.bib83)] | ICCV | FCN | VGGNet | 无监督 | STL | 物体 | MSRA10K [[107](#bib.bib107)]
    | 10,000 |  |'
- en: '| SRM [[71](#bib.bib71)] | ICCV | FCN | ResNet | Fully-Sup. | STL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| SRM [[71](#bib.bib71)] | ICCV | FCN | ResNet | 全监督 | STL | 物体 | DUTS [[97](#bib.bib97)]
    | 10,553 |  |'
- en: '| UCF [[66](#bib.bib66)] | ICCV | FCN | VGGNet | Fully-Sup. | STL | Object
    | MSRA10K [[107](#bib.bib107)] | 10,000 |  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| UCF [[66](#bib.bib66)] | ICCV | FCN | VGGNet | 全监督 | STL | 物体 | MSRA10K [[107](#bib.bib107)]
    | 10,000 |  |'
- en: '| 2018 | RADF [[78](#bib.bib78)] | AAAI | FCN | VGGNet | Fully-Sup. | STL |
    Object | MSRA10K [[107](#bib.bib107)] | 10,000 | ✓ |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | RADF [[78](#bib.bib78)] | AAAI | FCN | VGGNet | 全监督 | STL | 物体 | MSRA10K [[107](#bib.bib107)]
    | 10,000 | ✓ |'
- en: '| ASMO [[98](#bib.bib98)] | AAAI | FCN | ResNet101 | Weakly-Sup. | MTL | Object
    | MS COCO [[115](#bib.bib115)]+MSRA-B [[30](#bib.bib30)]+HKU-IS [[27](#bib.bib27)]
    | 82,783+2,500+2,500 | ✓ |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ASMO [[98](#bib.bib98)] | AAAI | FCN | ResNet101 | 弱监督 | MTL | 物体 | MS COCO [[115](#bib.bib115)]+MSRA-B [[30](#bib.bib30)]+HKU-IS [[27](#bib.bib27)]
    | 82,783+2,500+2,500 | ✓ |'
- en: '| LICNN [[68](#bib.bib68)] | AAAI | FCN | VGGNet | Weakly-Sup. | STL | Object
    | ImageNet [[114](#bib.bib114)] | 456k |  |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
- en: '| BDMP [[84](#bib.bib84)] | CVPR | FCN | VGGNet | Fully-Sup. | STL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
- en: '| DUS [[67](#bib.bib67)] | CVPR | FCN | ResNet101 | Un-Sup. | MTL | Object
    | MSRA-B [[30](#bib.bib30)] | 2,500 |  |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
- en: '| DGRL [[85](#bib.bib85)] | CVPR | FCN | ResNet50 | Fully-Sup. | STL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
- en: '| PAGR [[86](#bib.bib86)] | CVPR | FCN | VGGNet19 | Fully-Sup. | STL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
- en: '| RSDNet [[79](#bib.bib79)] | CVPR | FCN | ResNet101 | Fully-Sup. | MTL | Object
    | PASCAL-S [[108](#bib.bib108)] | 425 |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
- en: '| ASNet [[87](#bib.bib87)] | CVPR | FCN | VGGNet | Fully-Sup. | MTL | Object
    | SALICON [[111](#bib.bib111)]+MSRA10K [[107](#bib.bib107)]+DUT-OMRON [[56](#bib.bib56)]
    | 15,000+10,000+5,168 |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
- en: '| PiCANet [[40](#bib.bib40)] | CVPR | FCN | VGGNet/ResNet50 | Fully-Sup. |
    STL | Object | DUTS [[97](#bib.bib97)] | 10,553 | ✓ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
- en: '| C2S-Net [[99](#bib.bib99)] | ECCV | FCN | VGGNet | Weakly-Sup. | MTL | Object
    | MSRA10K [[107](#bib.bib107)]+Web | 10,000+20,000 |  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
- en: '|  | RAS [[88](#bib.bib88)] | ECCV | FCN | VGGNet | Fully-Sup. | STL | Object
    | MSRA-B [[30](#bib.bib30)] | 2,500 |  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
- en: '| 2019 | SuperVAE[[69](#bib.bib69)] | AAAI | FCN | N/A | Un-Sup. | STL | Object
    | N/A | N/A |  |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
- en: '| DEF[[74](#bib.bib74)] | AAAI | FCN | ResNet101 | Fully-Sup. | STL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
- en: '| AFNet[[89](#bib.bib89)] | CVPR | FCN | VGGNet16 | Fully-Sup. | MTL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
- en: '| BASNet[[90](#bib.bib90)] | CVPR | FCN | ResNet-34 | Fully-Sup. | STL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
- en: '| CapSal[[100](#bib.bib100)] | CVPR | FCN | ResNet101 | Fully-Sup. | MTL |
    Object | COCO-CapSal [[100](#bib.bib100)]/DUTS [[97](#bib.bib97)] | 5,265/10,553
    |  |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
- en: '| CPD-R[[80](#bib.bib80)] | CVPR | FCN | ResNet50 | Fully-Sup. | STL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
- en: '| MLSLNet[[91](#bib.bib91)] | CVPR | FCN | VGG16 | Fully-Sup. | MTL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
- en: '| ^†MWS[[81](#bib.bib81)] | CVPR | FCN | N/A | Weakly-Sup. | STL | Object |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
- en: '&#124; ImageNet DET [[114](#bib.bib114)]+MS COCO [[115](#bib.bib115)] &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +ImageNet [[116](#bib.bib116)]+DUTS [[97](#bib.bib97)] &#124;'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 456k+82,783 &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; +300,000+10,553 &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '|  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| PAGE-Net[[92](#bib.bib92)] | CVPR | FCN | VGGNet16 | Fully-Sup. | MTL | Object
    | MSRA10K [[107](#bib.bib107)] | 10,000 | ✓ |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| PS[[94](#bib.bib94)] | CVPR | FCN | ResNet50 | Fully-Sup. | STL | Object
    | MSRA10K [[107](#bib.bib107)] | 10,000 | ✓ |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| PoolNet[[93](#bib.bib93)] | CVPR | FCN | ResNet50 | Fully-Sup. | STL/MTL
    | Object | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: '| BANet[[101](#bib.bib101)] | ICCV | FCN | ResNet50 | Fully-Sup. | MTL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: '| EGNet[[82](#bib.bib82)] | ICCV | FCN | VGGNet/ResNet | Fully-Sup. | MTL |
    Object | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| EGNet[[82](#bib.bib82)] | ICCV | FCN | VGGNet/ResNet | 完全监督 | MTL | 目标 |
    DUTS [[97](#bib.bib97)] | 10,553 |  |'
- en: '| HRSOD[[73](#bib.bib73)] | ICCV | FCN | VGGNet | Fully-Sup. | STL | Object
    | DUTS [[97](#bib.bib97)]/HRSOD [[73](#bib.bib73)]+DUTS [[97](#bib.bib97)] | 10,553/12,163
    |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| HRSOD[[73](#bib.bib73)] | ICCV | FCN | VGGNet | 完全监督 | STL | 目标 | DUTS [[97](#bib.bib97)]/HRSOD
    [[73](#bib.bib73)]+DUTS [[97](#bib.bib97)] | 10,553/12,163 |  |'
- en: '| JDFPR[[95](#bib.bib95)] | ICCV | FCN | VGG | Fully-Sup. | STL | Object |
    MSRA-B [[30](#bib.bib30)] | 2,500 | ✓ |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| JDFPR[[95](#bib.bib95)] | ICCV | FCN | VGG | 完全监督 | STL | 目标 | MSRA-B [[30](#bib.bib30)]
    | 2,500 | ✓ |'
- en: '| SCRN[[102](#bib.bib102)] | ICCV | FCN | ResNet50 | Fully-Sup. | MTL | Object
    | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| SCRN[[102](#bib.bib102)] | ICCV | FCN | ResNet50 | 完全监督 | MTL | 目标 | DUTS
    [[97](#bib.bib97)] | 10,553 |  |'
- en: '| SSNet[[103](#bib.bib103)] | ICCV | FCN | Desenet169 | Fully-Sup. | MTL |
    Object | PASCAL VOC 2012 [[113](#bib.bib113)]+DUTS [[97](#bib.bib97)] | 1,464+10,553
    | ✓ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| SSNet[[103](#bib.bib103)] | ICCV | FCN | Desenet169 | 完全监督 | MTL | 目标 | PASCAL
    VOC 2012 [[113](#bib.bib113)]+DUTS [[97](#bib.bib97)] | 1,464+10,553 | ✓ |'
- en: '| TSPOANet[[41](#bib.bib41)] | ICCV | Capsule | FLNet | Fully-Sup. | STL |
    Object | DUTS [[97](#bib.bib97)] | 10,553 |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| TSPOANet[[41](#bib.bib41)] | ICCV | Capsule | FLNet | 完全监督 | STL | 目标 | DUTS
    [[97](#bib.bib97)] | 10,553 |  |'
- en: 'The rest of the paper is organized as follows. §[2](#S2 "2 Deep Learning based
    SOD Models ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")
    explains the proposed taxonomies, each accompanied with one or two most representative
    models. §[3](#S3 "3 SOD Datasets ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey") examines the most notable SOD datasets, whereas §[4](#S4
    "4 Evaluation Metrics ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey") describes several widely used SOD metrics. §[5](#S5 "5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey") benchmarks several deep SOD models and provides in-depth analyses.
    §[6](#S6 "6 More Discussions ‣ Salient Object Detection in the Deep Learning Era:
    An In-depth Survey") provides further discussions and presents open issues and
    future research directions of the field. Finally, §[7](#S7 "7 Conclusion ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey") concludes the
    paper.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下。§[2](#S2 "2 基于深度学习的显著性目标检测模型 ‣ 深度学习时代的显著性目标检测：深入调查") 解释了提出的分类法，每种分类法都配有一两个最具代表性的模型。§[3](#S3
    "3 显著性目标检测数据集 ‣ 深度学习时代的显著性目标检测：深入调查") 考察了最重要的显著性目标检测数据集，而 §[4](#S4 "4 评估指标 ‣ 深度学习时代的显著性目标检测：深入调查")
    描述了几种广泛使用的显著性目标检测指标。§[5](#S5 "5 基准测试和实证分析 ‣ 深度学习时代的显著性目标检测：深入调查") 对几种深度显著性目标检测模型进行了基准测试，并提供了深入分析。§[6](#S6
    "6 更多讨论 ‣ 深度学习时代的显著性目标检测：深入调查") 提供了进一步的讨论，提出了领域中的开放问题和未来的研究方向。最后，§[7](#S7 "7 结论
    ‣ 深度学习时代的显著性目标检测：深入调查") 总结了论文。
- en: 2 Deep Learning based SOD Models
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 基于深度学习的显著性目标检测模型
- en: 'Before reviewing recent deep SOD models in details, we first provide a common
    formulation of the image-based SOD problem. Given an input image $\bm{I}\!\in\!\mathbb{R}^{W\!\times\!H\!\times\!3\!}$
    of size $W\!\times\!H$, an SOD model $f$ maps the input image $\bm{I}$ to a continuous
    saliency map $\bm{S}\!=\!f(\bm{I})\!\in\![0,1]^{W\!\times\!H\!}$. For learning-based
    SOD, the model $f$ is learned through a set of training samples. Given a set of
    static images $\mathcal{I}\!\!=\!\!\{\bm{I}_{n\!}\!\in\!\mathbb{R}^{W\!\times\!H\!\times\!3}\}_{n}$
    and corresponding binary SOD ground-truth masks $\mathcal{G}\!\!=\!\!\{\bm{G}_{n\!}\!\in\!\{0,1\}^{W\!\times\!H}\}_{n}$,
    the goal of learning is to find $f\!\in\!\mathcal{F}$ that minimizes the prediction
    error, *i.e*., $\sum_{n\!}\ell(\bm{S}_{n},\bm{G}_{n})$, where $\ell$ is a certain
    distance measure (*e.g*., defined in §[4](#S4 "4 Evaluation Metrics ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey")), $\bm{S}_{n\!}\!=_{\!}\!f(\bm{I}_{n})$,
    and $\mathcal{F}$ is the set of potential mapping functions. Deep SOD methods
    typically model $f$ through modern deep learning techniques, as will be reviewed
    later in this section. The ground-truths $\mathcal{G}$ can be collected by different
    methodologies, *i.e*., direct human-annotation or eye-fixation-guided labeling,
    and may have different formats, *i.e*., pixel-wise or bounding-box annotations,
    which will be discussed in §[3](#S3 "3 SOD Datasets ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey").'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '在详细回顾最近的深度 SOD 模型之前，我们首先提供图像基础 SOD 问题的通用表述。给定一个大小为 $W\!\times\!H$ 的输入图像 $\bm{I}\!\in\!\mathbb{R}^{W\!\times\!H\!\times\!3\!}$，一个
    SOD 模型 $f$ 将输入图像 $\bm{I}$ 映射到一个连续的显著性图 $\bm{S}\!=\!f(\bm{I})\!\in\![0,1]^{W\!\times\!H\!}$。对于基于学习的
    SOD，模型 $f$ 是通过一组训练样本来学习的。给定一组静态图像 $\mathcal{I}\!\!=\!\!\{\bm{I}_{n\!}\!\in\!\mathbb{R}^{W\!\times\!H\!\times\!3}\}_{n}$
    和相应的二进制 SOD 真实掩码 $\mathcal{G}\!\!=\!\!\{\bm{G}_{n\!}\!\in\!\{0,1\}^{W\!\times\!H}\}_{n}$，学习的目标是找到
    $f\!\in\!\mathcal{F}$，使得预测误差最小，即 $\sum_{n\!}\ell(\bm{S}_{n},\bm{G}_{n})$，其中 $\ell$
    是某种距离度量（*例如*，定义在 §[4](#S4 "4 Evaluation Metrics ‣ Salient Object Detection in
    the Deep Learning Era: An In-depth Survey")），$\bm{S}_{n\!}\!=_{\!}\!f(\bm{I}_{n})$，而
    $\mathcal{F}$ 是潜在映射函数的集合。深度 SOD 方法通常通过现代深度学习技术来建模 $f$，这将在本节后续部分回顾。真实值 $\mathcal{G}$
    可以通过不同的方法收集，即直接人工标注或眼动引导标注，并且可能具有不同的格式，即像素级或边界框注释，这将在 §[3](#S3 "3 SOD Datasets
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey") 中讨论。'
- en: 'In Table [II](#S1.T2 "TABLE II ‣ 1.2 Related Previous Reviews and Surveys ‣
    1 Introduction ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey"), we categorize recent deep SOD models according to four taxonomies, considering
    network architecture (§[2.1](#S2.SS1 "2.1 Representative Network Architectures
    for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")), level of supervision (§[2.2](#S2.SS2 "2.2
    Level of Supervision ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey")), learning paradigm (§[2.3](#S2.SS3
    "2.3 Learning Paradigm ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey")), and whether they works at an
    object or instance level (§[2.4](#S2.SS4 "2.4 Object-/Instance-Level SOD ‣ 2 Deep
    Learning based SOD Models ‣ Salient Object Detection in the Deep Learning Era:
    An In-depth Survey")). In the following, each category is elaborated on and exemplified
    by one or two most representative models. Table [III](#S1.T3 "TABLE III ‣ 1.3
    Our Contributions ‣ 1 Introduction ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey") summarizes essential characteristics of recent SOD models.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '在表 [II](#S1.T2 "TABLE II ‣ 1.2 Related Previous Reviews and Surveys ‣ 1 Introduction
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey") 中，我们根据四个分类对最近的深度
    SOD 模型进行了分类，考虑了网络架构 (§[2.1](#S2.SS1 "2.1 Representative Network Architectures
    for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey"))、监督级别 (§[2.2](#S2.SS2 "2.2 Level of Supervision
    ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey"))、学习范式 (§[2.3](#S2.SS3 "2.3 Learning Paradigm ‣ 2 Deep
    Learning based SOD Models ‣ Salient Object Detection in the Deep Learning Era:
    An In-depth Survey")) 和是否在对象或实例级别工作 (§[2.4](#S2.SS4 "2.4 Object-/Instance-Level
    SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey"))。接下来，每个类别将进行详细阐述，并通过一个或两个最具代表性的模型进行示例。表 [III](#S1.T3
    "TABLE III ‣ 1.3 Our Contributions ‣ 1 Introduction ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey") 总结了最近 SOD 模型的基本特征。'
- en: 2.1 Representative Network Architectures for SOD
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 代表性网络架构用于 SOD
- en: 'Based on the primary network architectures adopted, we classify deep SOD models
    into four categories, namely MLP-based (§[2.1.1](#S2.SS1.SSS1 "2.1.1 Multi-Layer
    Perceptron (MLP)-Based Methods ‣ 2.1 Representative Network Architectures for
    SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")), FCN-based (§[2.1.2](#S2.SS1.SSS2 "2.1.2 Fully
    Convolutional Network (FCN)-Based Methods ‣ 2.1 Representative Network Architectures
    for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")), hybrid network-based (§[2.1.3](#S2.SS1.SSS3
    "2.1.3 Hybrid Network-Based Methods ‣ 2.1 Representative Network Architectures
    for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")) and Capsule-based (§[2.1.4](#S2.SS1.SSS4 "2.1.4
    Capsule-Based Methods ‣ 2.1 Representative Network Architectures for SOD ‣ 2 Deep
    Learning based SOD Models ‣ Salient Object Detection in the Deep Learning Era:
    An In-depth Survey")).'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所采用的主要网络架构，我们将深度SOD模型分为四类，即基于MLP的（§[2.1.1](#S2.SS1.SSS1 "2.1.1 多层感知器（MLP）-
    基于的方法 ‣ 2.1 SOD的代表性网络架构 ‣ 2 深度学习基于SOD的模型 ‣ 深度学习时代的显著对象检测：深入调查")），基于FCN的（§[2.1.2](#S2.SS1.SSS2
    "2.1.2 完全卷积网络（FCN）- 基于的方法 ‣ 2.1 SOD的代表性网络架构 ‣ 2 深度学习基于SOD的模型 ‣ 深度学习时代的显著对象检测：深入调查")），基于混合网络的（§[2.1.3](#S2.SS1.SSS3
    "2.1.3 混合网络- 基于的方法 ‣ 2.1 SOD的代表性网络架构 ‣ 2 深度学习基于SOD的模型 ‣ 深度学习时代的显著对象检测：深入调查")）和基于胶囊的（§[2.1.4](#S2.SS1.SSS4
    "2.1.4 胶囊- 基于的方法 ‣ 2.1 SOD的代表性网络架构 ‣ 2 深度学习基于SOD的模型 ‣ 深度学习时代的显著对象检测：深入调查")）。
- en: '![Refer to caption](img/bc20759ccc2e0c7609e56c4365c73309.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bc20759ccc2e0c7609e56c4365c73309.png)'
- en: 'Figure 2: Categorization of previous deep SOD models according to the adopted
    network architecture. (a) MLP-based methods. (b)-(f) FCN-based methods, mainly
    using (b) single-stream network, (c) multi-stream network, (d) side-out fusion
    network, (e) bottom-up/top-down network, and (f) branch network architectures.
    (g) Hybrid network-based methods. (h) Capsule-based methods. See §[2.1](#S2.SS1
    "2.1 Representative Network Architectures for SOD ‣ 2 Deep Learning based SOD
    Models ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")
    for more detailed descriptions.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：根据采用的网络架构对之前的深度SOD模型进行分类。(a) 基于MLP的方法。(b)-(f) 基于FCN的方法，主要使用(b) 单流网络，(c) 多流网络，(d)
    侧向融合网络，(e) 自下而上/自上而下网络，和(f) 分支网络架构。(g) 基于混合网络的方法。(h) 基于胶囊的方法。有关更详细的描述，请参见§[2.1](#S2.SS1
    "2.1 SOD的代表性网络架构 ‣ 2 深度学习基于SOD的模型 ‣ 深度学习时代的显著对象检测：深入调查")。
- en: 2.1.1 Multi-Layer Perceptron (MLP)-Based Methods
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1 多层感知器（MLP）- 基于的方法
- en: 'MLP-based methods leverage image subunits (*i.e*., *super-pixels/patches* [[29](#bib.bib29),
    [61](#bib.bib61), [60](#bib.bib60)] and generic *object proposals* [[28](#bib.bib28),
    [27](#bib.bib27), [37](#bib.bib37), [62](#bib.bib62)]) as processing units. They
    feed deep features extracted from the subunits into an MLP-classifier for saliency
    score prediction (Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Representative Network Architectures
    for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")(a)).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 基于MLP的方法利用图像子单元（*即*，*超像素/补丁* [[29](#bib.bib29), [61](#bib.bib61), [60](#bib.bib60)]
    和通用*目标提议* [[28](#bib.bib28), [27](#bib.bib27), [37](#bib.bib37), [62](#bib.bib62)])作为处理单元。它们将从子单元中提取的深度特征输入到MLP分类器中以预测显著性得分（图[2](#S2.F2
    "图2 ‣ 2.1 SOD的代表性网络架构 ‣ 2 深度学习基于SOD的模型 ‣ 深度学习时代的显著对象检测：深入调查")(a)）。
- en: 1) Super-pixel/patch-based methods use regular (patch) or nearly-regular (super-pixel)
    image decomposition. As an example of regular decomposition, MCDL [[29](#bib.bib29)]
    uses two pathways to extract local and global context from two super-pixel-centered
    windows of different sizes. The global and local feature vectors are fed into
    an MLP for classifying background and saliency. In contrast, SuperCNN [[61](#bib.bib61)]
    constructs two hand-crafted input feature sequences for each irregular super-pixel,
    and use two separate CNN columns to produce saliency scores from the feature sequences,
    respectively. Regular image decomposition can accelerate the processing speed,
    thus most of the methods in this category are based on regular decompostion.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 超像素/基于补丁的方法使用规则（补丁）或近乎规则（超像素）的图像分解。作为规则分解的一个例子，MCDL [[29](#bib.bib29)] 使用两条路径从两个不同大小的超像素中心窗口中提取局部和全局上下文。全局和局部特征向量被送入一个MLP进行背景和显著性的分类。相比之下，SuperCNN
    [[61](#bib.bib61)] 为每个不规则的超像素构造两个手工设计的输入特征序列，并使用两个独立的CNN列从特征序列中分别生成显著性分数。规则的图像分解可以加快处理速度，因此该类别中的大多数方法基于规则分解。
- en: 2) Object proposal-based methods leverage object proposals [[28](#bib.bib28),
    [27](#bib.bib27)] or bounding-boxes [[37](#bib.bib37), [62](#bib.bib62)] as basic
    processing units in order to better encode object information. For instance, MAP [[37](#bib.bib37)]
    uses a CNN model to generate a set of scored bounding-boxes, then selects an optimized
    compact subset of bounding-boxes as the salient objects. Note that this kind of
    methods typically produce coarse SOD results due to the lack of object boundary
    information.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 基于目标提议的方法利用目标提议 [[28](#bib.bib28), [27](#bib.bib27)] 或边界框 [[37](#bib.bib37),
    [62](#bib.bib62)] 作为基本处理单元，以更好地编码目标信息。例如，MAP [[37](#bib.bib37)] 使用CNN模型生成一组评分的边界框，然后选择一个优化的紧凑边界框子集作为显著目标。需要注意的是，这种方法通常由于缺乏目标边界信息而产生粗糙的显著性目标检测结果。
- en: Though MLP-based SOD methods greatly outperform their non-deep counterparts,
    they cannot fully leverage essential spatial information and are quite time-consuming,
    as they need to process all visual subunits one-by-one.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于MLP的显著性检测方法在性能上远超其非深度学习对手，但由于需要逐个处理所有视觉子单元，它们无法充分利用重要的空间信息，并且相当耗时。
- en: 2.1.2 Fully Convolutional Network (FCN)-Based Methods
  id: totrans-180
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2 完全卷积网络（FCN）基方法
- en: 'To address the limitations of MLP-based methods, recent solutions adopt FCN
    architecture [[117](#bib.bib117)], leading to end-to-end spatial saliency representation
    learning and fast saliency prediction, within a single feed-forward process. FCN-based
    methods are now dominant in the field. Typical architectures can be further classified
    as: *single-stream*, *multi-stream*, *side-fusion*, *bottom-up/top-down*, and
    *branched networks*.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对基于MLP的方法的局限性，近期的解决方案采用了FCN架构 [[117](#bib.bib117)]，实现了端到端的空间显著性表示学习和快速显著性预测，在单次前向传播过程中完成。基于FCN的方法目前在这一领域占据主导地位。典型架构可以进一步分类为：*单流*、*多流*、*侧面融合*、*自下而上/自上而下*
    和 *分支网络*。
- en: '1) Single-stream network is the most standard architecture, having a stack
    of convolutional layers, interleaved with pooling and non-linear activation operations
    (see Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Representative Network Architectures for
    SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")(b)). It takes a whole image as input, and directly
    outputs a pixel-wise probabilistic map highlighting salient objects. For example,
    UCF [[66](#bib.bib66)] makes use of an encoder-decoder network architecture for
    finer-resolution saliency prediction. It incorporates a reformulated dropout in
    the encoder to learn uncertain features, and a hybrid upsampling scheme in the
    decoder to avoid checkerboard artifacts.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 单流网络是最标准的架构，拥有一系列卷积层，交替进行池化和非线性激活操作（见图 [2](#S2.F2 "图 2 ‣ 2.1 代表性网络架构 ‣ 2
    深度学习基于SOD模型 ‣ 深度学习时代的显著目标检测：深度调查")(b)）。它以整张图像作为输入，直接输出一个像素级的概率图，突出显示显著目标。例如，UCF
    [[66](#bib.bib66)] 使用编码-解码网络架构进行更精细分辨率的显著性预测。它在编码器中结合了一种重新设计的丢弃法来学习不确定特征，并在解码器中采用混合上采样方案以避免棋盘状伪影。
- en: '2) Multi-stream network, as depicted in Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Representative
    Network Architectures for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey")(c), typically consists
    of multiple network streams to explicitly learn multi-scale saliency features
    from multi-resolution inputs. Multi-stream outputs are fused to form a final prediction.
    DCL [[104](#bib.bib104)], as one of the earliest attempts towards this direction,
    contains two streams, which produce pixel- and region-level SOD estimations, respectively.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 多流网络，如图[2](#S2.F2 "图 2 ‣ 2.1 代表性网络架构用于SOD ‣ 2 基于深度学习的SOD模型 ‣ 深度学习时代的显著目标检测：深入调查")(c)所示，通常由多个网络流组成，以显式地从多分辨率输入中学习多尺度显著性特征。多流输出被融合以形成最终预测。DCL [[104](#bib.bib104)]，作为早期对此方向的尝试之一，包含两个流，分别生成像素级和区域级的SOD估计。
- en: '3) Side-fusion network fuses multi-layer responses of a backbone network together
    for SOD prediction, making use of the complementary information of the inherent
    multi-scale representations of the CNN hierarchy (Fig. [2](#S2.F2 "Figure 2 ‣
    2.1 Representative Network Architectures for SOD ‣ 2 Deep Learning based SOD Models
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")(d)).
    Side-outputs are typically supervised by the ground-truth, leading to a deep supervision
    strategy [[118](#bib.bib118)]. As a well-known side-fusion network based SOD model,
    DSS [[39](#bib.bib39)]${}_{\!}$ adds${}_{\!}$ short connections from deeper side-outputs
    to shallower ones. In this way, higher-level features help lower side-outputs
    to better locate salient regions, and lower-level${}_{\!}$ features${}_{\!}$ can${}_{\!}$
    enrich${}_{\!}$ deeper${}_{\!}$ side-outputs${}_{\!}$ with${}_{\!}$ finer${}_{\!}$
    details.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 3) 边融合网络将主干网络的多层响应融合在一起以进行SOD预测，利用CNN层次中固有多尺度表示的互补信息（图[2](#S2.F2 "图 2 ‣ 2.1
    代表性网络架构用于SOD ‣ 2 基于深度学习的SOD模型 ‣ 深度学习时代的显著目标检测：深入调查")(d)）。边输出通常由真实值监督，形成深度监督策略 [[118](#bib.bib118)]。作为一个知名的基于边融合网络的SOD模型，DSS [[39](#bib.bib39)]${}_{\!}$
    添加${}_{\!}$ 从更深层边输出到较浅层的短连接。通过这种方式，高层特征帮助低层边输出更好地定位显著区域，低层特征则可以为更深层边输出提供更细致的细节。
- en: '4) Bottom-up/top-down network refines rough saliency maps in the feed-forward
    pass by gradually incorporating spatial-detail-rich features from lower layers,
    and produces the finest saliency maps at the top-most layer (Fig. [2](#S2.F2 "Figure
    2 ‣ 2.1 Representative Network Architectures for SOD ‣ 2 Deep Learning based SOD
    Models ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")(e)),
    which resembles the U-Net [[119](#bib.bib119)] for semantic segmentation. This
    network architectures is first adopted by PiCANet [[40](#bib.bib40)], which hierarchically
    embeds global and local pixel-wise attention modules to selectively attend to
    informative context.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 自下而上/自上而下网络通过逐步融合来自低层的空间细节丰富的特征，在前馈传递中细化粗略的显著性图，并在最上层生成最精细的显著性图（图[2](#S2.F2
    "图 2 ‣ 2.1 代表性网络架构用于SOD ‣ 2 基于深度学习的SOD模型 ‣ 深度学习时代的显著目标检测：深入调查")(e)），这类似于用于语义分割的U-Net [[119](#bib.bib119)]。该网络架构首次由PiCANet [[40](#bib.bib40)]采用，层次嵌入全局和局部像素级注意力模块，以选择性地关注信息丰富的上下文。
- en: '5) Branched network typically addresses multi-task learning for more robust
    saliency pattern modeling. They have a single-input-multiple-output structure,
    where bottom layers are shared to process a common input and top ones are specialized
    for different tasks (Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Representative Network Architectures
    for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")(f)). For example, C2S-Net [[99](#bib.bib99)]
    is constructed by adding a pre-trained contour detection model [[120](#bib.bib120)]
    to a main SOD branch. Then the two branches are alternately trained for the two
    tasks, *i.e*., SOD and contour detection.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 5) 分支网络通常用于多任务学习，以便更稳健地建模显著性模式。它们具有单输入多输出结构，其中底层共享以处理公共输入，顶部层则专门用于不同任务（图[2](#S2.F2
    "图 2 ‣ 2.1 代表性网络架构用于SOD ‣ 2 基于深度学习的SOD模型 ‣ 深度学习时代的显著目标检测：深入调查")(f)）。例如，C2S-Net [[99](#bib.bib99)]是通过将预训练的轮廓检测模型 [[120](#bib.bib120)]
    添加到主SOD分支中构建的。然后，两个分支交替训练这两个任务，*即*，SOD和轮廓检测。
- en: 2.1.3 Hybrid Network-Based Methods
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3 混合网络方法
- en: 'Some other models combine both MLP- and FCN-based subnets to produce edge-preserving
    results with multi-scale context (Fig. [2](#S2.F2 "Figure 2 ‣ 2.1 Representative
    Network Architectures for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey")(g)). Combining pixel-level
    and region-level saliency cues is a promising strategy to yield improved performance,
    though it introduces extra computational costs. CRPSD [[105](#bib.bib105)] consolidates
    this idea. It combines pixel- and region-level saliency. The former is generated
    by fusing the last and penultimate side-output features of an FCN, while the latter
    is obtained by applying an existing SOD model [[29](#bib.bib29)] to image regions.
    Only the FCN and fusion layers are trainable.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '一些其他模型结合了MLP和FCN基础子网，以在多尺度背景下产生边缘保持的结果（图 [2](#S2.F2 "Figure 2 ‣ 2.1 Representative
    Network Architectures for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey")(g)）。结合像素级和区域级显著性线索是一种有前途的策略，可以提高性能，尽管这会带来额外的计算成本。CRPSD [[105](#bib.bib105)]巩固了这一观点。它结合了像素级和区域级显著性。其中，像素级显著性是通过融合FCN的最后和倒数第二侧输出特征生成的，而区域级显著性是通过对图像区域应用现有SOD模型 [[29](#bib.bib29)]得到的。只有FCN和融合层是可训练的。'
- en: 2.1.4 Capsule-Based Methods
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4 胶囊网络方法
- en: 'Recently, Hinton *et al*. [[59](#bib.bib59)] proposed a new family of neural
    networks, named *Capsules*. Capsules are made up of a group of neurons which accept
    and output vectors as opposed to scalar values of CNNs, allowing entity properties
    to be comprehensively modeled. Some researchers have thus been inspired to explore
    Capsules in SOD [[42](#bib.bib42), [41](#bib.bib41)] (Fig. [2](#S2.F2 "Figure
    2 ‣ 2.1 Representative Network Architectures for SOD ‣ 2 Deep Learning based SOD
    Models ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")(h)).
    For instance, TSPOANet [[41](#bib.bib41)] emphasizes part-object relations using
    a two-stream capsule network. The input features of capsules are extracted from
    a CNN, and transformed into low-level capsules. These are then assigned to high-level
    capsules, and finally recognized to be salient or background.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '最近，Hinton *等人* [[59](#bib.bib59)] 提出了一个新的神经网络家族，称为*胶囊网络*。胶囊网络由一组神经元组成，这些神经元接受和输出向量，而不是CNN的标量值，从而允许实体属性得到全面建模。因此，一些研究人员受此启发，开始探索胶囊网络在SOD中的应用 [[42](#bib.bib42),
    [41](#bib.bib41)]（图 [2](#S2.F2 "Figure 2 ‣ 2.1 Representative Network Architectures
    for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")(h)）。例如，TSPOANet [[41](#bib.bib41)] 强调了使用双流胶囊网络的部分对象关系。胶囊的输入特征是从CNN中提取的，并转换为低级胶囊。这些胶囊随后被分配到高级胶囊中，最后被识别为显著或背景。'
- en: 2.2 Level of Supervision
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 监督级别
- en: Based on the type of supervision, deep SOD models can be classified into either
    *fully-supervised* or *weakly-/unsupervised*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 基于监督类型，深度SOD模型可以分为*完全监督*或*弱监督/无监督*两类。
- en: 2.2.1 Fully-Supervised Methods
  id: totrans-193
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1 完全监督方法
- en: Most deep SOD models are trained with large-scale pixel-level human annotations,
    which are time-consuming and expensive to acquire. Moreover, models trained on
    fine-labeled datasets tend to overfit and generalize poorly to real-life images [[67](#bib.bib67)].
    Thus, training SOD with weaker annotations has become an increasingly popular
    research direction.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度SOD模型都是使用大规模像素级人工标注进行训练的，这种标注方式既费时又昂贵。此外，在细致标注的数据集上训练的模型往往会过拟合，并且对真实图像的泛化能力较差 [[67](#bib.bib67)]。因此，使用较弱标注的SOD训练方法已成为一个越来越受欢迎的研究方向。
- en: 2.2.2 Weakly-/Unsupervised Methods
  id: totrans-195
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2 弱监督/无监督方法
- en: To get rid of laborious manual labeling, several weak supervision forms have
    been explored in SOD, including *image-level category* labels [[97](#bib.bib97),
    [68](#bib.bib68)], *object contours* [[99](#bib.bib99)], *image captions* [[81](#bib.bib81)]
    and *pseudo ground-truth* masks generated by non-learning SOD methods [[83](#bib.bib83),
    [67](#bib.bib67), [98](#bib.bib98)].
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了摆脱繁琐的人工标注，已经在SOD中探索了几种弱监督形式，包括*图像级类别*标签 [[97](#bib.bib97), [68](#bib.bib68)]，*对象轮廓* [[99](#bib.bib99)]，*图像标题* [[81](#bib.bib81)]和*伪真值*掩码，这些掩码是由非学习SOD方法生成的 [[83](#bib.bib83),
    [67](#bib.bib67), [98](#bib.bib98)]。
- en: 1) Category-level supervision. It has been shown that deep features trained
    with only image-level labels also provide information on object locations [[121](#bib.bib121),
    [122](#bib.bib122)], making them promising supervision signals for SOD training.
    WSS [[97](#bib.bib97)], as a typical example, first pre-trains a two-branch network,
    where one branch is used to predict image labels based on ImageNet [[114](#bib.bib114)],
    and the other estimates SOD maps. The estimated maps are refined by CRF and used
    to further fine-tune the SOD branch.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 类别级监督。研究表明，仅使用图像级标签训练的深度特征也能提供对象位置的信息[[121](#bib.bib121), [122](#bib.bib122)]，使其成为SOD训练中有前景的监督信号。WSS [[97](#bib.bib97)]作为一个典型例子，首先预训练一个双分支网络，其中一个分支用于基于ImageNet [[114](#bib.bib114)]预测图像标签，另一个用于估计SOD图。估计的图通过CRF进行细化，并用于进一步微调SOD分支。
- en: 2) Pseudo pixel-level supervision. Though informative, image-level labels are
    weak. Some researchers therefore instead use traditional non-learning SOD methods [[83](#bib.bib83),
    [67](#bib.bib67), [98](#bib.bib98)], or contour information [[99](#bib.bib99)],
    to generate noisy yet finer-grained cues for training. For instance, SBF [[83](#bib.bib83)]
    fuses weak saliency maps from a set of prior heuristic SOD models [[35](#bib.bib35),
    [123](#bib.bib123), [124](#bib.bib124)] at intra- and inter-image levels, to generate
    supervision signals. C2S-Net [[99](#bib.bib99)] trains the SOD branch with the
    pixel-wise salient object masks generated from the outputs of the contour branch [[125](#bib.bib125)]
    using CEDN [[120](#bib.bib120)]. The contour and SOD branches alternatively update
    each other and progressively output finer SOD predictions.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 伪像素级监督。尽管信息丰富，图像级标签仍然较弱。因此，一些研究人员使用传统的非学习SOD方法[[83](#bib.bib83), [67](#bib.bib67),
    [98](#bib.bib98)]或轮廓信息[[99](#bib.bib99)]，生成噪声较大但更细粒度的训练线索。例如，SBF [[83](#bib.bib83)]融合了一组先验启发式SOD模型[[35](#bib.bib35),
    [123](#bib.bib123), [124](#bib.bib124)]生成的弱显著图，在图像内部和图像之间生成监督信号。C2S-Net [[99](#bib.bib99)]使用CEDN [[120](#bib.bib120)]生成的轮廓分支输出的像素级显著对象掩码训练SOD分支。轮廓分支和SOD分支交替更新，并逐渐输出更精细的SOD预测。
- en: 2.3 Learning Paradigm
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 学习范式
- en: From the perspective of learning paradigms, SOD networks can be divided into
    *single-task learning (STL)* and *multi-task learning (MTL)* methods.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从学习范式的角度来看，SOD网络可以分为*单任务学习（STL）*和*多任务学习（MTL）*方法。
- en: 2.3.1 Single-Task Learning (STL) Based Methods
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1 单任务学习（STL）方法
- en: In machine learning, the standard practice is to learn one task at a time [[126](#bib.bib126)],
    *i.e*., STL. Most deep SOD methods belong to this realm of learning, *i.e*., they
    utilize supervision from a single knowledge domain (SOD or anther related field
    such as image classification [[68](#bib.bib68)]) for training.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，标准做法是一次学习一个任务[[126](#bib.bib126)]，*即*，STL。大多数深度SOD方法属于这种学习范畴，*即*，它们利用来自单一知识领域（SOD或其他相关领域，如图像分类[[68](#bib.bib68)]）的监督进行训练。
- en: 2.3.2 Multi-Task Learning (MTL) Based Methods
  id: totrans-203
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2 多任务学习（MTL）方法
- en: 'Inspired by the human learning process, where knowledge learned from related
    tasks can assist the learning of a new task, MTL [[126](#bib.bib126)] aims to
    improve the performance of multiple related tasks by learning them simultaneously.
    Benefiting from extra knowledge from related tasks, models can gain improved generalizability.
    An extra advantage lies in the sharing of samples among tasks, which alleviates
    the lack of data for training heavily parameterized models. These are the core
    motivations of MTL based SOD models, and branched architectures (see §[2.1.2](#S2.SS1.SSS2
    "2.1.2 Fully Convolutional Network (FCN)-Based Methods ‣ 2.1 Representative Network
    Architectures for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey")) are usually adopted.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '受到人类学习过程的启发，MTL [[126](#bib.bib126)]旨在通过同时学习多个相关任务来提高这些任务的性能，从而利用相关任务中的额外知识来增强模型的泛化能力。另一个优势在于任务之间样本的共享，这缓解了训练大量参数模型时数据不足的问题。这些是基于MTL的SOD模型的核心动机，通常采用分支架构（见§[2.1.2](#S2.SS1.SSS2
    "2.1.2 Fully Convolutional Network (FCN)-Based Methods ‣ 2.1 Representative Network
    Architectures for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey")）。'
- en: 1) Salient object subitizing. The ability of humans to rapidly enumerate a small
    number of items is known as subitizing [[127](#bib.bib127), [112](#bib.bib112)].
    Inspired by this, some works learn salient object subitizing and detection simultaneously [[37](#bib.bib37),
    [77](#bib.bib77), [79](#bib.bib79)]. RSDNet [[79](#bib.bib79)] represents the
    latest advance in this direction. It addresses detection, ranking and subitizing
    of salient objects in a unified framework.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 1) 突出物体的瞬时计数。人类迅速枚举少量物品的能力被称为瞬时计数 [[127](#bib.bib127), [112](#bib.bib112)]。受此启发，一些研究同时学习突出物体的瞬时计数和检测 [[37](#bib.bib37),
    [77](#bib.bib77), [79](#bib.bib79)]。RSDNet [[79](#bib.bib79)] 代表了这一方向的最新进展。它在一个统一框架中处理突出物体的检测、排序和瞬时计数。
- en: 2) Fixation prediction aims to predict human eye-fixation locations in visual
    scenes. Due to its close relation with SOD, learning shared knowledge from these
    two tasks can improve the performance of both. For example, ASNet [[87](#bib.bib87)]
    derives fixation information as a high-level understanding of the scene, from
    upper network layers. Then, fine-grained object-level saliency is progressively
    optimized under the guidance of the fixation in a top-down manner. 3) Image classification.
    Image-level tags are valuable for SOD, as they provide the category information
    of dominant objects in the images which are very likely to be the salient regions [[97](#bib.bib97)].
    Inspired by this, some SOD models learn image classification as an auxiliary task.
    For example, ASMO [[98](#bib.bib98)] leverages class activation maps from a neural
    classifier and saliency maps from previous non-learning SOD methods to train the
    SOD network, in an iterative manner.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 2) 注视预测旨在预测视觉场景中的人眼注视位置。由于其与SOD的密切关系，从这两个任务中学习共享知识可以提高两者的性能。例如，ASNet [[87](#bib.bib87)]
    从上层网络层提取注视信息作为对场景的高级理解。然后，在自上而下的指导下，逐步优化细粒度的物体级显著性。3) 图像分类。图像级标签对SOD非常有价值，因为它们提供了图像中主导物体的类别信息，这些物体很可能是显著区域 [[97](#bib.bib97)]。受此启发，一些SOD模型将图像分类作为辅助任务。例如，ASMO [[98](#bib.bib98)]
    利用神经分类器的类激活图和先前非学习SOD方法的显著性图，以迭代的方式训练SOD网络。
- en: 4) Semantic segmentation is for per-pixel semantic prediction. Though SOD is
    class-agnostic, high-level semantics play a crucial role in saliency modeling.
    Thus, the task of semantic segmentation can also be integrated into SOD learning.
    A recent SOD model, SSNet [[103](#bib.bib103)], is developed upon this idea. It
    uses a saliency aggregation module to predict a saliency score of each category.
    Then, a segmentation network is used to produce segmentation masks of all the
    categories. These masks are finally aggregated (according to corresponding saliency
    scores) to produce a SOD map.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 4) 语义分割用于逐像素语义预测。虽然SOD是类无关的，但高层次的语义在显著性建模中发挥着至关重要的作用。因此，语义分割的任务也可以集成到SOD学习中。一个最近的SOD模型，SSNet [[103](#bib.bib103)]，是基于这一想法开发的。它使用显著性聚合模块预测每个类别的显著性分数。然后，使用分割网络生成所有类别的分割掩码。这些掩码最终根据相应的显著性分数进行聚合，生成SOD图。
- en: 'TABLE IV: Statistics of popular SOD datasets, including the number of images,
    number of salient objects per image, area ratio of the salient objects in images,
    annotation type, image resolution, and existence of fixation data. See §[3](#S3
    "3 SOD Datasets ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey") for more detailed descriptions.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '表 IV: 流行SOD数据集的统计信息，包括图像数量、每张图像的显著物体数量、显著物体在图像中的面积比、注释类型、图像分辨率以及是否存在注视数据。有关更详细的描述，请参见§[3](#S3
    "3 SOD Datasets ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey")。'
- en: '|   | Dataset | Year | Publ. | #Img. | #Obj. | Obj. Area(%) | SOD Annotation
    | Resolution | Fix. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|   | 数据集 | 年份 | 发表 | 图像数量 | 物体数量 | 物体面积(%) | SOD 注释 | 分辨率 | Fix. |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Early | MSRA-A [[30](#bib.bib30)] | 2007 | CVPR | 1,000/20,840 | 1-2 | -
    | bounding-box object-level |                            - |  |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 早期 | MSRA-A [[30](#bib.bib30)] | 2007 | CVPR | 1,000/20,840 | 1-2 | - | 边框物体级别
    |                            - |  |  |'
- en: '| MSRA-B [[30](#bib.bib30)] | 2007 | CVPR | 5,000 | 1-2 | 20.82[±10.29] | bounding-box
    object-level, pixel-wise object-level | $\max(w,h)\!=\!400,$ | $\min(w,h)\!=\!126$
    |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| MSRA-B [[30](#bib.bib30)] | 2007 | CVPR | 5,000 | 1-2 | 20.82[±10.29] | 边框物体级别,
    像素级物体级别 | $\max(w,h)\!=\!400,$ | $\min(w,h)\!=\!126$ |  |'
- en: '| SED1 [[128](#bib.bib128)] | 2007 | CVPR | 100 | 1 | 26.70[±14.26] | pixel-wise
    object-level | $\max(w,h)\!=\!465,$ | $\min(w,h)\!=\!125$ |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
- en: '| SED2 [[128](#bib.bib128)] | 2007 | CVPR | 100 | 2 | 21.42[±18.41] | pixel-wise
    object-level | $\max(w,h)\!=\!300,$ | $\min(w,h)\!=\!144$ |  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: '| ASD [[31](#bib.bib31)] | 2009 | CVPR | 1,000 | 1-2 | 19.89[±9.53] | pixel-wise
    object-level | $\max(w,h)\!=\!400,$ | $\min(w,h)\!=\!142$ |  |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
- en: '| Modern&Popular | SOD [[129](#bib.bib129)] | 2010 | CVPR-W | 300 | 1-4+ |
    27.99[±19.36] | pixel-wise object-level | $\max(w,h)\!=\!481,$ | $\min(w,h)\!=\!321$
    |  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: '| MSRA10K [[107](#bib.bib107)] | 2015 | TPAMI | 10,000 | 1-2 | 22.21[±10.09]
    | pixel-wise object-level | $\max(w,h)\!=\!400,$ | $\min(w,h)\!=\!144$ |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
- en: '| ECSSD [[55](#bib.bib55)] | 2015 | TPAMI | 1,000 | 1-4+ | 23.51[±14.02] |
    pixel-wise object-level | $\max(w,h)\!=\!400,$ | $\min(w,h)\!=\!139$ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: '| DUT-OMRON [[56](#bib.bib56)] | 2013 | CVPR | 5,168 | 1-4+ | 14.85[±12.15]
    | pixel-wise object-level | $\max(w,h)\!=\!401,$ | $\min(w,h)\!=\!139$ | ✓ |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
- en: '| PASCAL-S [[108](#bib.bib108)] | 2014 | CVPR | 850 | 1-4+ | 24.23[±16.70]
    | pixel-wise object-level | $\max(w,h)\!=\!500,$ | $\min(w,h)\!=\!139$ | ✓ |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
- en: '| HKU-IS [[27](#bib.bib27)] | 2015 | CVPR | 4,447 | 1-4+ | 19.13[±10.90] |
    pixel-wise object-level | $\max(w,h)\!=\!500,$ | $\min(w,h)\!=\!100$ |  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
- en: '| DUTS [[97](#bib.bib97)] | 2017 | CVPR | 15,572 | 1-4+ | 23.17[±15.52] | pixel-wise
    object-level | $\max(w,h)\!=\!500,$ | $\min(w,h)\!=\!100$ |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
- en: '| Special | SOS [[112](#bib.bib112)] | 2015 | CVPR | 6,900 | 0-4+ | 41.22[±25.35]
    | object number, bounding-box (train set) | $\max(w,h)\!=\!6132,$ | $\min(w,h)\!=\!80~{}~{}$
    |  |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
- en: '| MSO [[112](#bib.bib112)] | 2015 | CVPR | 1,224 | 0-4+ | 39.51[±24.85] | object
    number, bounding-box instance-level | $\max(w,h)\!=\!3888,$ | $\min(w,h)\!=\!120$
    |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
- en: '| ILSO [[70](#bib.bib70)] | 2017 | CVPR | 1,000 | 1-4+ | 24.89[±12.59] | pixel-wise
    instance-level | $\max(w,h)\!=\!400,$ | $\min(w,h)\!=\!142$ |  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
- en: '| XPIE [[130](#bib.bib130)] | 2017 | CVPR | 10,000 | 1-4+ | 19.42[±14.39] |
    pixel-wise object-level, geographic information | $\max(w,h)\!=\!500,$ | $\min(w,h)\!=\!130$
    | ✓ |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
- en: '| SOC [[131](#bib.bib131)] | 2018 | ECCV | 6,000 | 0-4+ | 21.36[±16.88] | pixel-wise
    instance-level, object category, attribute | $\max(w,h)\!=\!849,$ | $\min(w,h)\!=\!161$
    |  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
- en: '| COCO-CapSal [[100](#bib.bib100)] | 2019 | CVPR | 6,724 | 1-4+ | 23.74[±17.00]
    | pixel-wise object-level, image caption | $\max(w,h)\!=\!640,$ | $\min(w,h)\!=\!480$
    |  |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
- en: '| HRSOD [[73](#bib.bib73)] | 2019 | ICCV | 2,010 | 1-4+ | 21.13[±15.14] | pixel-wise
    object-level | $\max(w,h)\!=\!10240,$ | $\min(w,h)\!=\!600$ |  |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
- en: 5) Contour/edge detection refers to the task of detecting obvious object boundaries
    in images, which are informative of salient objects. Thus, it is also explored
    in SOD modeling. For example, PAGE-Net [[92](#bib.bib92)] learns an edge detection
    module and embeds edge cues into the main SOD stream in a top-down manner, leading
    to better edge-preserving results.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 6) Image Captioning can provide extra knowledge about the main content of visual
    scenes, enabling SOD models to better capture high-level semantics. This has been
    explored in CapSal [[100](#bib.bib100)], which incorporates semantic context from
    a captioning network with local-global visual cues to achieve improved performance
    for detecting salient objects.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Object-/Instance-Level SOD
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to whether or not they can identify different salient object instances,
    current deep SOD models can be categorized into object-level and instance-level
    methods.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1 Object-Level Methods
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most deep SOD models are object-level methods, *i.e*., designed to detect pixels
    that belong to salient objects without being aware of individual object instances.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2 Instance-Level Methods
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Instance-level SOD methods further identify individual object instances in the
    detected salient regions, which is crucial for practical applications that need
    finer distinctions, such as semantic segmentation [[132](#bib.bib132)] and multi-human
    parsing [[133](#bib.bib133)]. As an early attempt, MSRNet [[70](#bib.bib70)] performs
    salient instance detection by decomposing it into three sub-tasks, *i.e*., pixel-level
    saliency prediction, salient object contour detection and salient instance identification.
    It jointly performs the first two sub-tasks by integrating deep features for several
    different scaled versions of the input image. The last sub-task is solved by multi-scale
    combinatorial grouping [[125](#bib.bib125)] to generate salient object proposals
    from the detected contours and filter out noisy or overlapping ones.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 3 SOD Datasets
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the rapid development of SOD, numerous datasets have been introduced.
    Table [IV](#S2.T4 "TABLE IV ‣ 2.3.2 Multi-Task Learning (MTL) Based Methods ‣
    2.3 Learning Paradigm ‣ 2 Deep Learning based SOD Models ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey") summarizes $19$ SOD datasets, which
    are highly representative and widely used for training or benchmarking, or collected
    with specific properties.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Quick Overview
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In an attempt to facilitate understanding of SOD datasets, we present some main
    take-away points of this section.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: • Compared with early datasets [[30](#bib.bib30), [128](#bib.bib128), [31](#bib.bib31)],
    recent ones [[107](#bib.bib107), [56](#bib.bib56), [27](#bib.bib27), [97](#bib.bib97)]
    are typically more advanced with less center bias, improved complexity, and increased
    scale. They are thus better-suited for training and evaluation, and likely to
    have longer life-spans.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: • Some other recent datasets [[112](#bib.bib112), [70](#bib.bib70), [130](#bib.bib130),
    [131](#bib.bib131), [100](#bib.bib100), [73](#bib.bib73)] are enriched with more
    diverse annotations (*e.g*., subitizing, captioning), representing new trends
    in the field.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 'More${}_{\!}$ in-depth${}_{\!}$ discussions${}_{\!}$ regarding${}_{\!}$ generalizability${}_{\!}$
    and${}_{\!}$ difficulty${}_{\!}$ of${}_{\!}$ several famous${}_{\!}$ datasets
    will${}_{\!}$ be${}_{\!}$ presented${}_{\!}$ in${}_{\!}$ §[5.6](#S5.SS6 "5.6 Cross-Dataset
    Generalization Evaluation ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey").​​'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Early SOD Datasets
  id: totrans-245
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Early SOD datasets typically contain simple scenes where 1-2 salient objects
    stand out from a clear background.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: • MSRA-A [[30](#bib.bib30)] contains 20,840 images. Each image has only one
    noticeable and eye-catching object, annotated by a bounding-box. As a subset of
    MSRA-A, MSRA-B has 5,000 images and less ambiguity w.r.t. the salient object.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: • SED [[128](#bib.bib128)]¹¹1[http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB](http://www.wisdom.weizmann.ac.il/~vision/Seg_Evaluation_DB)
    comprises a single-object subset and a two-object subset; each has 100 images
    with mask annotations.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: • ASD [[31](#bib.bib31)]²²2[https://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/](https://ivrlwww.epfl.ch/supplementary_material/RK_CVPR09/),
    also a subset of MSRA-A, has 1,000 images with pixel-wise ground-truths.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Popular Modern SOD Datasets
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recent SOD datasets tend to include more challenging and general scenes with
    relatively complex backgrounds and${}_{\!}$ multiple${}_{\!}$ salient${}_{\!}$
    objects. All${}_{\!}$ have${}_{\!}$ pixel-wise${}_{\!}$ annotations.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: • SOD [[129](#bib.bib129)]³³3[http://elderlab.yorku.ca/SOD/](http://elderlab.yorku.ca/SOD/)
    consists of 300 images, constructed from [[134](#bib.bib134)]. Many images have
    more than one salient object that is similar to the background or touches image
    boundaries.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '• MSRA10K [[107](#bib.bib107)]⁴⁴4[https://mmcheng.net/zh/msra10k/](https://mmcheng.net/zh/msra10k/),
    also known as THUS10K, contains 10,000 images selected from MSRA-A and covers
    all the images in ASD. Due to its large scale, MSRA10K is widely used to train
    deep SOD models (see Table [III](#S1.T3 "TABLE III ‣ 1.3 Our Contributions ‣ 1
    Introduction ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey")).'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: • ECSSD [[55](#bib.bib55)]⁵⁵5[http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency](http://www.cse.cuhk.edu.hk/leojia/projects/hsaliency)
    is composed of 1,000 images with semantically meaningful but structurally complex
    natural contents.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: • DUT-OMRON [[56](#bib.bib56)]⁶⁶6[http://saliencydetection.net/dut-omron/](http://saliencydetection.net/dut-omron/)
    has 5,168 images of complex backgrounds and diverse content, with pixel-wise annotations.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: • PASCAL-S [[108](#bib.bib108)]⁷⁷7[http://cbi.gatech.edu/salobj/](http://cbi.gatech.edu/salobj/)​
    comprises 850 challenging images selected from the PASCAL VOC2010 val set [[113](#bib.bib113)].
    With${}_{\!}$ eye-fixation${}_{\!}$ records, non-binary${}_{\!}$ salient-object${}_{\!}$
    mask${}_{\!}$ annotations${}_{\!}$ are provided. Note that${}_{\!}$ the${}_{\!}$
    saliency${}_{\!}$ value${}_{\!}$ of${}_{\!}$ a${}_{\!}$ pixel${}_{\!}$ is${}_{\!}$
    calculated${}_{\!}$ as${}_{\!}$ the${}_{\!}$ ratio${}_{\!}$ of${}_{\!}$ subjects${}_{\!}$
    that${}_{\!}$ select${}_{\!}$ the${}_{\!}$ segment${}_{\!}$ containing${}_{\!}$
    this${}_{\!}$ pixel${}_{\!}$ as${}_{\!}$ salient${}_{\!}$.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: • HKU-IS [[27](#bib.bib27)]⁸⁸8[https://i.cs.hku.hk/~gbli/deep_saliency.html](https://i.cs.hku.hk/~gbli/deep_saliency.html)
    has $4,447$ complex scenes that typically contain multiple disconnected objects
    with diverse spatial distributions and similar fore-/background appearances.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: '• DUTS [[97](#bib.bib97)]⁹⁹9[http://saliencydetection.net/duts/](http://saliencydetection.net/duts/)
    is a large-scale dataset, where the $10,553$ training images were selected from
    the ImageNet train/val set [[114](#bib.bib114)], and the $5,019$ test images are
    from the ImageNet test set and SUN [[135](#bib.bib135)]. Since 2017, SOD models
    are typically trained on DUTS (Table [III](#S1.T3 "TABLE III ‣ 1.3 Our Contributions
    ‣ 1 Introduction ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey")).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Other Special SOD Datasets
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the${}_{\!}$ above “standard” SOD datasets, some special ones
    have also recently been proposed, leading to new research directions.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '• SOS [[112](#bib.bib112)]^(10)^(10)10[http://cs-people.bu.edu/jmzhang/sos.html](http://cs-people.bu.edu/jmzhang/sos.html)
    is created for SOD subitizing [[127](#bib.bib127)]. It contains 6,900 images (training
    set: 5,520, test set: 1,380). Each image is labeled as containing 0, 1, 2, 3 or
    4+ salient objects.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: • MSO [[112](#bib.bib112)]^(11)^(11)11[http://cs-people.bu.edu/jmzhang/sos.html](http://cs-people.bu.edu/jmzhang/sos.html)
    is a subset of SOS-test [[112](#bib.bib112)], covering 1,224 images. It has a
    more balanced distribution of the number of salient objects. Each object has a
    bounding-box annotation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: • ILSO [[70](#bib.bib70)]^(12)^(12)12[http://www.sysu-hcp.net/instance-level-salient-object-segmentation/](http://www.sysu-hcp.net/instance-level-salient-object-segmentation/)
    contains 1,000 images with precise instance-level annotations and coarse contour
    labeling.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: '• XPIE [[130](#bib.bib130)]^(13)^(13)13[http://cvteam.net/projects/CVPR17-ELE/ELE.html](http://cvteam.net/projects/CVPR17-ELE/ELE.html)​
    has 10,000 images with pixel-wise labels. It has three subsets: Set-P has 625
    images of places-of-interest with geographic information; Set-I 8,799 images with
    object tags; and Set-E 576 images with eye-fixation records.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: • SOC [[131](#bib.bib131)]^(14)^(14)14[http://mmcheng.net/SOCBenchmark/](http://mmcheng.net/SOCBenchmark/)
    consists of 6,000 images with 80 common categories. Half of the images contain
    salient objects, while the remaining have none. Each image containing salient
    objects is annotated with an instance-level ground-truth mask, object category,
    and challenging factors. The non-salient object subset has 783 texture images
    and 2,217 real-scene images.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: • SOC [[131](#bib.bib131)]^(14)^(14)14[http://mmcheng.net/SOCBenchmark/](http://mmcheng.net/SOCBenchmark/)
    包含 6,000 张图像，覆盖 80 个常见类别。其中一半图像包含显著物体，而其余的则不含。每张包含显著物体的图像都有实例级真实掩码、物体类别和挑战因素的注释。非显著物体子集包含
    783 张纹理图像和 2,217 张真实场景图像。
- en: • COCO-CapSal [[100](#bib.bib100)]^(15)^(15)15[https://github.com/yi94code/HRSOD](https://github.com/yi94code/HRSOD)
    is built from COCO [[115](#bib.bib115)] and SALICON [[111](#bib.bib111)]. Salient
    objects were first roughly localized using the mouse-click data in SALICON, then
    precisely annotated according to the instance masks in COCO. The dataset has 5,265
    and 1,459 images for training and testing, respectively.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: • COCO-CapSal [[100](#bib.bib100)]^(15)^(15)15[https://github.com/yi94code/HRSOD](https://github.com/yi94code/HRSOD)
    基于 COCO [[115](#bib.bib115)] 和 SALICON [[111](#bib.bib111)] 构建。首先使用 SALICON 中的鼠标点击数据大致定位显著物体，然后根据
    COCO 中的实例掩码进行精确注释。该数据集包括 5,265 张训练图像和 1,459 张测试图像。
- en: • HRSOD [[73](#bib.bib73)]^(16)^(16)16[https://github.com/zhangludl/code-and-dataset-for-CapSal](https://github.com/zhangludl/code-and-dataset-for-CapSal)
    is the first *high-resolution* dataset for SOD. It contains 1,610 training and
    400 testing images collected from websites. Pixel-wise ground-truths are provided.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: • HRSOD [[73](#bib.bib73)]^(16)^(16)16[https://github.com/zhangludl/code-and-dataset-for-CapSal](https://github.com/zhangludl/code-and-dataset-for-CapSal)
    是首个 *高分辨率* 的 SOD 数据集。它包含 1,610 张训练图像和 400 张测试图像，均从网站收集。提供了逐像素的真实值。
- en: '![Refer to caption](img/81af8b350a777c5e8f6ac14d4ee64443.png)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/81af8b350a777c5e8f6ac14d4ee64443.png)'
- en: 'Figure 3: Annotation distributions of SOD datasets (see §[3](#S3 "3 SOD Datasets
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey") for
    details).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3：SOD 数据集的注释分布（详见 §[3](#S3 "3 SOD Datasets ‣ Salient Object Detection in
    the Deep Learning Era: An In-depth Survey")）。'
- en: 3.5 Discussion
  id: totrans-270
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 讨论
- en: 'As shown in Table [IV](#S2.T4 "TABLE IV ‣ 2.3.2 Multi-Task Learning (MTL) Based
    Methods ‣ 2.3 Learning Paradigm ‣ 2 Deep Learning based SOD Models ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey"), early SOD datasets [[30](#bib.bib30),
    [128](#bib.bib128), [31](#bib.bib31)] are comprised of simple images with 1-2
    salient objects per image, and only provide rough bounding-box annotations, which
    are insufficient for reliable evaluations [[31](#bib.bib31), [136](#bib.bib136)].
    Performance on these datasets has become saturated. Modern datasets [[107](#bib.bib107),
    [55](#bib.bib55), [56](#bib.bib56), [27](#bib.bib27), [97](#bib.bib97)] are typically
    large-scale and offer precise pixel-wise ground-truths. The scenes are more complex
    and general, and usually contain multiple salient objects. Some special datasets contain
    challenging scenes with background only [[112](#bib.bib112), [131](#bib.bib131)],
    provide more fine-grained, instance-level SOD ground-truths [[70](#bib.bib70),
    [131](#bib.bib131)] or include other annotations such as image captions [[100](#bib.bib100)],
    inspiring new research directions and applications. Fig. [3](#S3.F3 "Figure 3
    ‣ 3.4 Other Special SOD Datasets ‣ 3 SOD Datasets ‣ Salient Object Detection in
    the Deep Learning Era: An In-depth Survey") depicts the annotation distributions
    of $18$ SOD datasets. Here are some essential conclusions: 1) Some datasets [[30](#bib.bib30),
    [31](#bib.bib31), [107](#bib.bib107), [97](#bib.bib97)] have significant center
    bias; 2) Datasets [[27](#bib.bib27), [70](#bib.bib70), [100](#bib.bib100)] have
    more balanced location distributions for salient objects; and 3) MSO [[112](#bib.bib112)]
    has less center bias, as only bounding-box annotations are provided. We analyze
    the generalizability and difficulty of several famous SOD datasets in-depth in
    §[5.6](#S5.SS6 "5.6 Cross-Dataset Generalization Evaluation ‣ 5 Benchmarking and
    Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey").'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation Metrics
  id: totrans-272
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section reviews popular object-level SOD evaluation metrics, *i.e*., Precision-Recall
    (PR), F-measure [[31](#bib.bib31)], Mean Absolute Error (MAE) [[33](#bib.bib33)],
    weighted $F_{\beta}$ measure (Fbw) [[137](#bib.bib137)], Structural measure (S-measure) [[138](#bib.bib138)],
    and Enhanced-alignment measure (E-measure) [[139](#bib.bib139)].
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Quick Overview
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better understand the characteristics of different metrics, a quick overview
    of the main conclusions for this section are provided as follows.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: • PR, F-measure, MAE, and Fbw address pixel-wise errors, while S-measure and
    E-measure consider structure cues.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: • Among pixel-level metrics, PR, F-measure, and Fbw fail to consider true negative
    pixels, while MAE can remedy this.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: • Among${}_{\!}$ structured${}_{\!}$ metrics, S-measure${}_{\!}$ is${}_{\!}$
    more${}_{\!}$ favored${}_{\!}$ than${}_{\!}$ E-measure,${}_{\!}$ as${}_{\!}$ SOD${}_{\!}$
    addresses${}_{\!}$ continuous${}_{\!}$ saliency${}_{\!}$ estimates.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: '• Considering popularity, advantages and completeness, F-measure, S-measure
    and MAE are the most recommended and${}_{\!}$ are${}_{\!}$ thus${}_{\!}$ used${}_{\!}$
    for${}_{\!}$ our${}_{\!}$ performance${}_{\!}$ benchmarking${}_{\!}$ in${}_{\!}$
    §[5.2](#S5.SS2 "5.2 Performance Benchmarking ‣ 5 Benchmarking and Empirical Analysis
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey").'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Metric Details
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '• PR is calculated based on the binarized salient object mask and ground-truth:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\text{Precision}=\frac{\text{TP}}{\text{TP}+\text{FP}},~{}~{}~{}~{}\text{Recall}=\frac{\text{TP}}{\text{TP}+\text{FN}},$
    |  | (1) |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: where TP, TN, FP, FN denote true-positive, true-negative, false-positive, and
    false-negative, respectively. A set of thresholds ([$0\!-\!255$]) is applied to
    binarize the prediction. Each threshold produces a pair of precision/recall values
    to form a PR curve for describing model performance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '• F-measure [[31](#bib.bib31)] comprehensively considers both precision and
    recall by computing the weighted harmonic mean:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small F_{\beta}=\frac{(1+\beta^{2})\text{Precision}\times\text{Recall}}{\beta^{2}\text{Precision}+\text{Recall}}.$
    |  | (2) |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: Empirically, $\beta^{2}$ is set to $0.3$ [[31](#bib.bib31)] to put more emphasis
    on precision. Instead of plotting the whole F-measure curve, some methods only
    report *maximal* $F_{\beta}$, or binarize the predicted saliency map by an adaptive
    threshold, *i.e*., twice the mean value of the saliency prediction, and report
    *mean F*.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '• MAE [[33](#bib.bib33)] measures the average pixel-wise absolute error between
    normalized saliency prediction map $\bm{S}\!\in\![0,1]^{W\!\times\!H}$ and binary
    ground-truth mask $\bm{G}\!\in\!\{0,1\}^{W\!\times\!H}$:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\text{MAE}=\frac{1}{W\!\times\!H}\sum\nolimits_{i=1}^{W}\sum\nolimits_{j=1}^{H}\lvert\bm{G}(i,j)-\bm{S}(i,j)\rvert.$
    |  | (3) |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Benchmarking results of $44$ state-of-the-art deep SOD models and
    $3$ top-performing classic SOD methods on $6$ famous datasets (§[5.2](#S5.SS2
    "5.2 Performance Benchmarking ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey")). Here max F,
    S, and M indicate *maximal* $F_{\beta}$, S-measure, and MAE, respectively. The
    three best scores are marked in red, blue, and green, respectively.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: '|                                                                          
                                   Dataset | ECSSD [[55](#bib.bib55)] | DUT-OMRON [[56](#bib.bib56)]
    | PASCAL-S [[108](#bib.bib108)] | HKU-IS [[27](#bib.bib27)] | DUTS-test [[97](#bib.bib97)]
    | SOD [[129](#bib.bib129)] |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| Metric | max F$\uparrow$ |    S$\uparrow$ |    M$\downarrow$ | max F$\uparrow$
    |    S$\uparrow$ |    M$\downarrow$ | max F$\uparrow$ |    S$\uparrow$ |    M$\downarrow$
    | max F$\uparrow$ |    S$\uparrow$ |    M$\downarrow$ | max F$\uparrow$ |    S$\uparrow$
    |    M$\downarrow$ | max F$\uparrow$ |    S$\uparrow$ |    M$\downarrow$ |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '| 2013-14 | ^∗HS[[35](#bib.bib35)] | .673 | .685 | .228 | .561 | .633 | .227
    | .569 | .624 | .262 | .652 | .674 | .215 | .504 | .601 | .243 | .756 | .711 |
    .222 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| ^∗DRFI[[53](#bib.bib53)] | .751 | .732 | .170 | .623 | .696 | .150 | .639
    | .658 | .207 | .745 | .740 | .145 | .600 | .676 | .155 | .658 | .619 | .228 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| ^∗wCtr[[36](#bib.bib36)] | .684 | .714 | .165 | .541 | .653 | .171 | .599
    | .656 | .196 | .695 | .729 | .138 | .522 | .639 | .176 | .615 | .638 | .213 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '| 2015 | MCDL[[29](#bib.bib29)] | .816 | .803 | .101 | .670 | .752 | .089 |
    .706 | .721 | .143 | .787 | .786 | .092 | .634 | .713 | .105 | .689 | .651 | .182
    |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '| LEGS[[28](#bib.bib28)] | .805 | .786 | .118 | .631 | .714 | .133 | $\ddagger$
    | $\ddagger$ | $\ddagger$ | .736 | .742 | .119 | .612 | .696 | .137 | .685 | .658
    | .197 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: '| MDF[[27](#bib.bib27)] | .797 | .776 | .105 | .643 | .721 | .092 | .704 |
    .696 | .142 | .839 | .810 | .129 | .657 | .728 | .114 | .736 | .674 | .160 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
- en: '| 2016 | ELD[[60](#bib.bib60)] | .849 | .841 | .078 | .677 | .751 | .091 |
    .782 | .799 | .111 | .868 | .868 | .063 | .697 | .754 | .092 | .717 | .705 | .155
    |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
- en: '| DHSNet[[38](#bib.bib38)] | .893 | .884 | .060 | $\ddagger$ | $\ddagger$ |
    $\ddagger$ | .799 | .810 | .092 | .875 | .870 | .053 | .776 | .818 | .067 | .790
    | .749 | .129 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
- en: '| DCL[[104](#bib.bib104)] | .882 | .868 | .075 | .699 | .771 | .086 | .787
    | .796 | .113 | .885 | .877 | .055 | .742 | .796 | .149 | .786 | .747 | .195 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
- en: '| ^⋄MAP[[37](#bib.bib37)] | .556 | .611 | .213 | .448 | .598 | .159 | .521
    | .593 | .207 | .552 | .624 | .182 | .453 | .583 | .181 | .509 | .557 | .236 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| CRPSD[[105](#bib.bib105)] | .915 | .895 | .048 | - | - | - | .864 | .852
    | .064 | .906 | .885 | .043 | - | - | - | - | - | - |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| RFCN[[63](#bib.bib63)] | .875 | .852 | .107 | .707 | .764 | .111 | .800 |
    .798 | .132 | .881 | .859 | .089 | .755 | .859 | .090 | .769 | .794 | .170 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| 2017 | MSRNet[[70](#bib.bib70)] | .900 | .895 | .054 | .746 | .808 | .073
    | .828 | .838 | .081 | $\ddagger$ | $\ddagger$ | $\ddagger$ | .804 | .839 | .061
    | .802 | .779 | .113 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| DSS[[39](#bib.bib39)] | .906 | .882 | .052 | .737 | .790 | .063 | .805 |
    .798 | .093 | $\ddagger$ | $\ddagger$ | $\ddagger$ | .796 | .824 | .057 | .805
    | .751 | .122 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| ^†WSS[[97](#bib.bib97)] | .879 | .811 | .104 | .725 | .730 | .110 | .804
    | .744 | .139 | .878 | .822 | .079 | .878 | .822 | .079 | .807 | .675 | .170 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| DLS[[65](#bib.bib65)] | .826 | .806 | .086 | .644 | .725 | .090 | .712 |
    .723 | .130 | .807 | .799 | .069 | - | - | - | - | - | - |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| NLDF[[75](#bib.bib75)] | .889 | .875 | .063 | .699 | .770 | .080 | .795 |
    .805 | .098 | .888 | .879 | .048 | .777 | .816 | .065 | .808 | .889 | .125 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: '| Amulet[[76](#bib.bib76)] | .905 | .894 | .059 | .715 | .780 | .098 | .805
    | .818 | .100 | .887 | .886 | .051 | .750 | .804 | .085 | .773 | .757 | .142 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
- en: '| FSN[[72](#bib.bib72)] | .897 | .884 | .053 | .736 | .802 | .066 | .800 |
    .804 | .093 | .884 | .877 | .044 | .761 | .808 | .066 | .781 | .755 | .127 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| SBF[[83](#bib.bib83)] | .833 | .832 | .091 | .649 | .748 | .110 | .726 |
    .758 | .133 | .821 | .829 | .078 | .657 | .743 | .109 | .740 | .708 | .159 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| SRM[[71](#bib.bib71)] | .905 | .895 | .054 | .725 | .798 | .069 | .817 |
    .834 | .084 | .893 | .887 | .046 | .798 | .836 | .059 | .792 | .741 | .128 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| UCF[[66](#bib.bib66)] | .890 | .883 | .069 | .698 | .760 | .120 | .787 |
    .805 | .115 | .874 | .875 | .062 | .742 | .782 | .112 | .763 | .753 | .165 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| 2018 | RADF[[78](#bib.bib78)] | .911 | .894 | .049 | .761 | .817 | .055 |
    .800 | .802 | .097 | .902 | .888 | .039 | .792 | .826 | .061 | .804 | .757 | .126
    |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| BDMP[[84](#bib.bib84)] | .917 | .911 | .045 | .734 | .809 | .064 | .830 |
    .845 | .074 | .910 | .907 | .039 | .827 | .862 | .049 | .806 | .786 | .108 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| DGRL[[85](#bib.bib85)] | .916 | .906 | .043 | .741 | .810 | .063 | .830 |
    .839 | .074 | .902 | .897 | .037 | .805 | .842 | .050 | .802 | .771 | .105 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| PAGR[[86](#bib.bib86)] | .904 | .889 | .061 | .707 | .775 | .071 | .814 |
    .822 | .089 | .897 | .887 | .048 | .817 | .838 | .056 | .761 | .716 | .147 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| RSDNet[[79](#bib.bib79)] | .880 | .788 | .173 | .715 | .644 | .178 | $\ddagger$
    | $\ddagger$ | $\ddagger$ | .871 | .787 | .156 | .798 | .720 | .161 | .790 | .668
    | .226 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| ASNet[[87](#bib.bib87)] | .925 | .915 | .047 | $\ddagger$ | $\ddagger$ |
    $\ddagger$ | .848 | .861 | .070 | .912 | .906 | .041 | .806 | .843 | .061 | .801
    | .762 | .121 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: '| PiCANet[[40](#bib.bib40)] | .929 | .916 | .035 | .767 | .825 | .054 | .838
    | .846 | .064 | .913 | .905 | .031 | .840 | .863 | .040 | .814 | .776 | .096 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
- en: '| ^†C2S-Net[[99](#bib.bib99)] | .902 | .896 | .053 | .722 | .799 | .072 | .827
    | .839 | .081 | .887 | .889 | .046 | .784 | .831 | .062 | .786 | .760 | .124 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
- en: '| RAS[[88](#bib.bib88)] | .908 | .893 | .056 | .753 | .814 | .062 | .800 |
    .799 | .101 | .901 | .887 | .045 | .807 | .839 | .059 | .810 | .764 | .124 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| 2019 | AFNet[[89](#bib.bib89)] | .924 | .913 | .042 | .759 | .826 | .057
    | .844 | .849 | .070 | .910 | .905 | .036 | .838 | .867 | .046 | .809 | .774 |
    .111 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| BASNet[[90](#bib.bib90)] | .931 | .916 | .037 | .779 | .836 | .057 | .835
    | .838 | .076 | .919 | .909 | .032 | .838 | .866 | .048 | .805 | .769 | .114 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| CapSal[[100](#bib.bib100)] | .813 | .826 | .077 | .535 | .674 | .101 | .827
    | .837 | .073 | .842 | .851 | .057 | .772 | .818 | .061 | .669 | .694 | .148 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| CPD[[80](#bib.bib80)] | .926 | .918 | .037 | .753 | .825 | .056 | .833 |
    .848 | .071 | .911 | .905 | .034 | .840 | .869 | .043 | .814 | .767 | .112 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| MLSLNet[[91](#bib.bib91)] | .917 | .911 | .045 | .734 | .809 | .064 | .835
    | .844 | .074 | .910 | .907 | .039 | .828 | .862 | .049 | .806 | .786 | .108 |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| ^†MWS[[81](#bib.bib81)] | .859 | .827 | .099 | .676 | .756 | .108 | .753
    | .768 | .134 | .835 | .818 | .086 | .720 | .759 | .092 | .772 | .700 | .170 |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| PAGE-Net[[92](#bib.bib92)] | .926 | .910 | .037 | .760 | .819 | .059 | .829
    | .835 | .073 | .910 | .901 | .031 | .816 | .848 | .048 | .795 | .763 | .108 |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| PS[[94](#bib.bib94)] | .930 | .918 | .041 | .789 | .837 | .061 | .837 | .850
    | .071 | .913 | .907 | .038 | .835 | .865 | .048 | .824 | .800 | .103 |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| PoolNet[[93](#bib.bib93)] | .937 | .926 | .035 | .762 | .831 | .054 | .858
    | .865 | .065 | .923 | .919 | .030 | .865 | .886 | .037 | .831 | .788 | .106 |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '| BANet-R[[101](#bib.bib101)] | .939 | .924 | .035 | .782 | .832 | .059 | .847
    | .852 | .070 | .923 | .913 | .032 | .858 | .879 | .040 | .842 | .791 | .106 |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| EGNet-R[[82](#bib.bib82)] | .936 | .925 | .037 | .777 | .841 | .053 | .841
    | .852 | .074 | .924 | .918 | .031 | .866 | .887 | .039 | .854 | .802 | .099 |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| HRSOD-DH[[73](#bib.bib73)] | .911 | .888 | .052 | .692 | .762 | .065 | .810
    | .817 | .079 | .890 | .877 | .042 | .800 | .824 | .050 | .735 | .705 | .139 |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '| JDFPR[[95](#bib.bib95)] | .915 | .907 | .049 | .755 | .821 | .057 | .827
    | .841 | .082 | .905 | .903 | .039 | .792 | .836 | .059 | .792 | .763 | .123 |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: '| SCRN[[102](#bib.bib102)] | .937 | .927 | .037 | .772 | .836 | .056 | .856
    | .869 | .063 | .921 | .916 | .034 | .864 | .885 | .040 | .826 | .787 | .107 |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
- en: '| SSNet[[103](#bib.bib103)] | .889 | .867 | .046 | .708 | .773 | .056 | .793
    | .807 | .072 | .876 | .854 | .041 | .769 | .784 | .049 | .713 | .700 | .118 |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
- en: '| TSPOANet[[41](#bib.bib41)] | .919 | .907 | .047 | .749 | .818 | .061 | .830
    | .842 | .078 | .909 | .902 | .039 | .828 | .860 | .049 | .810 | .772 | .118 |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
- en: ^∗ Non-deep learning model. ^† Weakly-supervised model. ^⋄ Bounding-box output.
    $\ddagger$ Training on subset. - Results not available.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '• Fbw [[137](#bib.bib137)] intuitively generalizes F-measure by alternating
    the way of calculating precision and recall. It extends the four basic quantities
    TP, TN, FP and FN to real values, and assigns different weights ($\omega$) to
    different errors at different locations, considering the neighborhood information:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small F_{\beta}^{\omega}=\frac{(1+\beta^{2})\text{Precision}^{\omega}\times\text{Recall}^{\omega}}{\beta^{2}\text{Precision}^{\omega}+\text{Recall}^{\omega}}.$
    |  | (4) |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
- en: '• S-measure [[138](#bib.bib138)] evaluates the structural similarity between
    the real-valued saliency map and the binary ground-truth. It considers object-aware
    ($S_{o}$) and region-aware ($S_{r}$) structure similarities:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small S=\alpha\times S_{o}+(1-\alpha)\times S_{r},$ |  | (5) |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
- en: where $\alpha$ is empirically set to $0.5$.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
- en: '• E-measure [[139](#bib.bib139)] considers global means of the image and local
    pixel matching simultaneously:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small Q_{\bm{S}}=\frac{1}{W\!\times\!H}\sum\nolimits_{i=1}^{W}\sum\nolimits_{j=1}^{H}\phi_{\bm{S}}(i,j),$
    |  | (6) |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
- en: where $\phi_{\bm{S}}$ is the enhanced alignment matrix, reflecting the correlation
    between $\bm{S}$ and $\bm{G}$ after subtracting their global means, respectively.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Discussion
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: These measures are typically based on pixel-wise errors while ignoring${}_{\!}$
    structural${}_{\!}$ similarities, with S-measure${}_{\!}$ and E-measure being
    the only exceptions. F-measure and E-measure are designed for assessing binarized
    saliency prediction maps, while PR, MAE, Fbw, and S-measure are for non-binary
    map evaluation.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Among pixel-level metrics, the PR curve is classic. However, precision and recall
    cannot fully assess the quality of saliency predictions, since high-precision
    predictions may only highlight a part of salient objects, while high-recall predictions
    are typically meaningless if all the pixels are predicted as being salient. In
    general, a high-recall response may come at the expense of reduced precision,
    and vice versa. F-measure and Fbw are thus used to consider precision and recall
    simultaneously. However, overlap-based metrics (*i.e*., PR, F-measure, and Fbw)
    do not consider the true negative saliency assignments, *i.e*., the pixels correctly
    marked as non-salient. Thus, these metrics favor methods that successfully assign
    high saliency to salient pixels but fail to detect non-salient regions [[50](#bib.bib50)].
    MAE can remedy this, but it performs poorly when salient objects are small. For
    the structure-/image-level metrics, S-measure is more popular than E-measure,
    as SOD focuses on continuous predictions.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: Considering${}_{\!}$ the${}_{\!}$ popularity${}_{\!}$ and${}_{\!}$ characteristics${}_{\!}$
    of${}_{\!}$ existing${}_{\!}$ metrics${}_{\!}$ and${}_{\!}$ completeness${}_{\!}$
    of${}_{\!}$ evaluation, F-measure (*maximal*${}_{\!}$ $F_{\beta}$), S-measure${}_{\!}$
    and${}_{\!}$ MAE${}_{\!}$ are${}_{\!}$ our${}_{\!}$ top${}_{\!}$ recommendations.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/33f396b361b84818ee48e6c8660fc84f.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Sample images from the hybrid benchmark consisting of images randomly
    selected from $6$ SOD datasets. Salient regions are uniformly highlighted. Corresponding
    attributes are listed. See §[5.3](#S5.SS3 "5.3 Attribute-Based Study ‣ 5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey") for more detailed descriptions.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: 5 Benchmarking and Empirical Analysis
  id: totrans-354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides empirical analyses to shed light on some key challenges
    in the field. Specifically, with our large-scale benchmarking (§[5.2](#S5.SS2
    "5.2 Performance Benchmarking ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey")), we first conduct
    an attribute-based study to better understand the benefits and limitations of${}_{\!}$
    current${}_{\!}$ arts${}_{\!}$ (§[5.3](#S5.SS3 "5.3 Attribute-Based Study ‣ 5
    Benchmarking and Empirical Analysis ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey")). Then, we${}_{\!}$ study${}_{\!}$ the${}_{\!}$ robustness${}_{\!}$
    of SOD${}_{\!}$ models${}_{\!}$ against${}_{\!}$ input${}_{\!}$ perturbations,
    *i.e*., random${}_{\!}$ exerted​​ noises${}_{\!}$ (§[5.4](#S5.SS4 "5.4 Robustness
    Against General Input Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣
    Salient Object Detection in the Deep Learning Era: An In-depth Survey")) and${}_{\!}$
    manually designed adversarial samples${}_{\!}$ (§[5.5](#S5.SS5 "5.5 Robustness
    Against Manually Designed Input Perturbations ‣ 5 Benchmarking and Empirical Analysis
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")). Finally,
    we${}_{\!}$ quantitatively${}_{\!}$ assess${}_{\!}$ the${}_{\!}$ generalizability${}_{\!}$
    and${}_{\!}$ difficulty${}_{\!}$ of${}_{\!}$ current${}_{\!}$ mainstream${}_{\!}$
    SOD datasets${}_{\!}$ (§[5.6](#S5.SS6 "5.6 Cross-Dataset Generalization Evaluation
    ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey")).'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE 7: Attribute-based study w.r.t. salient object categories, challenges
    and scene categories. $(\cdot)$ indicates the percentage of images with a specific
    attribute. ND-avg indicates the average score of three heuristic models: HS [[35](#bib.bib35)],
    DRFI [[53](#bib.bib53)] and wCtr [[36](#bib.bib36)]. D-avg indicates the average
    score of three deep learning models: DGRL [[85](#bib.bib85)], PAGR [[86](#bib.bib86)]
    and PiCANet [[40](#bib.bib40)]. Best in red, and worst with underline. See §[5.3](#S5.SS3
    "5.3 Attribute-Based Study ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey") for more details.'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
- en: '|  Metric | Method | Salient object categories | Challenges | Scene categories
    |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
- en: '| Human | Animal | Artifact | NatObj | $\mathcal{MO}$ | $\mathcal{HO}$ | $\mathcal{OV}$
    | $\mathcal{OC}$ | $\mathcal{CS}$ | $\mathcal{BC}$ | $\mathcal{CT}$ | $\mathcal{SO}$
    | $\mathcal{LO}$ | Indoor | Urban | Natural |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
- en: '| (26.61) | (38.44) | (45.67) | (10.56) | (11.39) | (66.39) | (28.72) | (46.50)
    | (40.44) | (47.22) | (74.11) | (21.61) | (12.61) | (20.28) | (22.22) | (57.50)
    |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
- en: '| max F$\uparrow$ | ^∗HS[[35](#bib.bib35)] | .587 | .650 | .636 | .704 | .663
    | .637 | .631 | .645 | .558 | .647 | .629 | .493 | .737 | .594 | .627 | .650 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
- en: '| ^∗DRFI[[53](#bib.bib53)] | .635 | .692 | .673 | .713 | .674 | .688 | .658
    | .675 | .599 | .662 | .677 | .566 | .747 | .609 | .661 | .697 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
- en: '| ^∗wCtr[[36](#bib.bib36)] | .557 | .621 | .624 | .682 | .639 | .625 | .605
    | .620 | .522 | .612 | .606 | .469 | .689 | .578 | .613 | .618 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
- en: '| DGRL[[85](#bib.bib85)] | .820 | .881 | .830 | .728 | .783 | .846 | .829 |
    .830 | .781 | .842 | .834 | .724 | .873 | .800 | .848 | .840 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
- en: '| PAGR[[86](#bib.bib86)] | .834 | .890 | .787 | .725 | .743 | .819 | .778 |
    .809 | .770 | .797 | .822 | .760 | .802 | .788 | .796 | .828 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
- en: '| PiCANet[[40](#bib.bib40)] | .840 | .897 | .846 | .669 | .791 | .861 | .843
    | .845 | .797 | .848 | .850 | .763 | .889 | .806 | .862 | .859 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
- en: '| ^∗ND-avg | .593 | .654 | .644 | .700 | .659 | .650 | .631 | .647 | .560 |
    .640 | .637 | .509 | .724 | .594 | .634 | .655 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
- en: '| D-avg | .831 | .889 | .821 | .708 | .772 | .842 | .817 | .828 | .783 | .829
    | .836 | .749 | .855 | .798 | .836 | .842 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
- en: ^∗  Non-deep learning model.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.1 Quick Overview
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For ease of understanding, we compile important observations and conclusions
    from subsequent experiments below.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '• Overall benchmarks (§[5.2](#S5.SS2 "5.2 Performance Benchmarking ‣ 5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey")). As shown in Table [V](#S4.T5 "TABLE V ‣ 4.2 Metric Details
    ‣ 4 Evaluation Metrics ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey"), deep SOD models significantly outperform heuristic ones, and
    the performance on some datasets [[55](#bib.bib55), [27](#bib.bib27)] has become
    saturated. [[93](#bib.bib93), [101](#bib.bib101), [82](#bib.bib82), [102](#bib.bib102)]
    are current state-of-the-arts.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
- en: '• Attribute-based analysis (§[5.3](#S5.SS3 "5.3 Attribute-Based Study ‣ 5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey")). Results in Table [7](#S5.T6 "TABLE 7 ‣ 5 Benchmarking and
    Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey") reveal that deep methods show significant advantages in detecting semantic-rich
    objects, such as animal. Both deep and non-deep methods face difficulties with
    small salient objects. For application scenarios, indoor scenes pose great challenges,
    highlighting potential directions for future efforts.'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '• Robustness against random perturbations (§[5.4](#S5.SS4 "5.4 Robustness Against
    General Input Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey")). As shown in
    Table [IX](#S5.T9 "TABLE IX ‣ 5.3.2 Analysis ‣ 5.3 Attribute-Based Study ‣ 5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey"), surprisingly, deep methods are more sensitive than heuristic
    ones to random input perturbations. Both types of methods demonstrate more robustness
    against Rotation, while being fragile towards Gaussian blur and Gaussian noise.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: '• Adversarial attack (§[5.5](#S5.SS5 "5.5 Robustness Against Manually Designed
    Input Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey")). Table [X](#S5.T10 "TABLE X ‣
    5.5.2 Transferability Across Networks ‣ 5.5 Robustness Against Manually Designed
    Input Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey") suggests that adversarial attacks
    cause drastic degradation in performance for deep SOD models, and are even worse
    than that of random perturbations. However, attacks rarely transfers between different
    SOD networks.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: '• Generalizability and difficulty of datasets (§[5.6](#S5.SS6 "5.6 Cross-Dataset
    Generalization Evaluation ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey")). Table [XI](#S5.T11
    "TABLE XI ‣ 5.6 Cross-Dataset Generalization Evaluation ‣ 5 Benchmarking and Empirical
    Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")
    shows that DUTS-train [[97](#bib.bib97)] is a good choice for training deep SOD
    models as it has the best generalizability, while SOC [[131](#bib.bib131)], DUT-OMRON [[56](#bib.bib56)],
    and DUTS-test [[97](#bib.bib97)] are more suitable for evaluation due to their
    difficulty.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE 6: Descriptions of attributes that often bring difficulties to SOD (see
    §[5.3](#S5.SS3 "5.3 Attribute-Based Study ‣ 5 Benchmarking and Empirical Analysis
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")).'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: '|  Attr |                                           Description |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{MO}$ | Multiple Objects. There exist more than two salient objects.
    |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{HO}$ | Heterogeneous Object. Salient object regions have distinct
    |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
- en: '|  | colors or illuminations. |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{OV}$ | Out-of-View. Salient objects are partially clipped by image
    |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
- en: '|  | boundaries. |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{OC}$ | Occlusion. Salient objects are occluded by other objects.
    |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{CS}$ | Complex Scene. Background regions contain confusing |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
- en: '|  | objects or rich details. |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{BC}$ | Background Clutter. Foreground and background regions |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
- en: '|  | around the salient object boundaries have similar colors ($\chi^{2}$ |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
- en: '|  | between RGB histograms less than $0.9$). |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{CT}$ | Complex Topology. Salient objects have complex shapes, *e.g*.,
    |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
- en: '|  | thin parts or holes. |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{SO}$ | Small Object. Ratio between salient object area and image
    is |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
- en: '|  | less than $0.1$. |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
- en: '| $\mathcal{LO}$ | Large Object. Ratio between salient object area and image
    is |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
- en: '|  | larger than $0.5$. |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
- en: 5.2 Performance Benchmarking
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Table [V](#S4.T5 "TABLE V ‣ 4.2 Metric Details ‣ 4 Evaluation Metrics ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey") shows the performances
    of $44$ state-of-the-art deep SOD models and three top-performing classic methods
    (suggested by [[44](#bib.bib44)]) on six most popular modern datasets. The performance
    is measured by three metrics, *i.e*., *maximal* $F_{\beta}$, S-measure and MAE,
    as recommended in §[4.3](#S4.SS3 "4.3 Discussion ‣ 4 Evaluation Metrics ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey"). All the benchmarked
    models are representative, and have publicly available implementations or saliency
    prediction results. For performance benchmarking, we either use saliency maps
    provided by the authors or run their official codes. It is worth mentioning that,
    for some methods, our benchmarking results are inconsistent with their reported
    scores. There are several reasons. First, our community long lacked an open, universally-adopted
    evaluation tool, while there are many implementation factors would influence the
    evaluation scores, such as input image resolution, threshold step, *etc*. Second,
    some methods [[66](#bib.bib66), [85](#bib.bib85), [76](#bib.bib76), [100](#bib.bib100),
    [69](#bib.bib69), [74](#bib.bib74)] use *mean* F-measure instead of *maximal*
    F-measure for performance evaluation. Third, for some methods [[76](#bib.bib76),
    [39](#bib.bib39)], the evaluation scores of finally released saliency maps are
    inconsistent with the ones reported in papers. We hope that our performance benchmarking,
    publicly released evaluation tools and SOD maps could help our community build
    an open and standardized evaluation system and ensure consistency and procedural
    correctness for results and conclusions produced by different parties.'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: Not surprisingly, data-driven models greatly outperform conventional heuristic
    ones, due to their strong learning ability for visually salient pattern modeling.
    In addition, the performance has gradually increased since 2015, demonstrating
    well the advancement of deep learning techniques. However, after 2018, the rate
    of improvement began decrasing, calling for more effective model designs and new
    machine learning technologies. We also find that the performances tend to be saturated
    on older SOD datasets such as ECSSD [[55](#bib.bib55)] and HKU-IS [[27](#bib.bib27)].
    Hence, among the $44$ famous deep SOD models, we would like to nominate PoolNet [[93](#bib.bib93)],
    BANet [[101](#bib.bib101)], EGNet [[82](#bib.bib82)], and SCRN [[102](#bib.bib102)]
    as the four state-of-the-art methods, which consistently show promising performance
    over diverse datasets.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Attribute-Based Study
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although the community has witnessed the great advances made by deep SOD models,
    it is still unclear under which specific aspects these models perform well. As
    there are numerous factors affecting the performance of a SOD algorithm, such
    as object/scene category, occlusion, *etc*., it is crucial to evaluate the performance
    under different scenarios. This can help reveal the strengths and weaknesses of
    deep SOD models, identify pending challenges, and highlight future research directions
    towards more robust algorithms.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.1 Hybrid Benchmark Dataset with Attribute Annotations
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'TABLE VIII: Attribute statistics of top and bottom 100 images based on F-measure.
    $(\cdot)$ indicates the percentage of the images with a specific attribute. ND-avg
    indicates the average results of three heuristic models: HS [[35](#bib.bib35)],
    DRFI [[53](#bib.bib53)] and wCtr [[36](#bib.bib36)]. D-avg indicates the average
    results of three deep models: DGRL [[85](#bib.bib85)], PAGR [[86](#bib.bib86)]
    and PiCANet [[40](#bib.bib40)]. Two largest changes in red if positive, and blue
    if negative. See §[5.3](#S5.SS3 "5.3 Attribute-Based Study ‣ 5 Benchmarking and
    Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey") for more details.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
- en: '|  Method | Cases | Salient object categories | Challenges | Scene categories
    |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
- en: '| Human | Animal | Artifact | NatObj | $\mathcal{MO}$ | $\mathcal{HO}$ | $\mathcal{OV}$
    | $\mathcal{OC}$ | $\mathcal{CS}$ | $\mathcal{BC}$ | $\mathcal{CT}$ | $\mathcal{SO}$
    | $\mathcal{LO}$ | Indoor | Urban | Natural |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
- en: '| (26.61) | (38.44) | (45.67) | (10.56) | (11.39) | (66.39) | (28.72) | (46.50)
    | (40.44) | (47.22) | (74.11) | (21.61) | (12.61) | (20.28) | (22.22) | (57.50)
    |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
- en: '| ND-avg | Best ($\%$) | 13.00 | 25.00 | 46.00 | 27.00 | 5.00 | 61.00 | 12.00
    | 26.00 | 10.00 | 20.00 | 63.00 | 5.00 | 18.00 | 17.00 | 6.00 | 12.00 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
- en: '| change | -13.61 | -13.44 | +0.33 | +14.44 | -6.39 | -5.39 | -16.72 | -20.50
    | -30.44 | -27.22 | -11.11 | -16.61 | +5.39 | -3.28 | -16.22 | -45.50 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
- en: '| Worst ($\%$) | 36.00 | 30.00 | 41.00 | 5.00 | 6.00 | 54.00 | 15.00 | 34.00
    | 70.00 | 31.00 | 71.00 | 76.00 | 0.00 | 22.00 | 37.00 | 37.00 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
- en: '| change | +9.39 | -8.44 | -4.67 | -5.56 | -5.39 | -12.39 | -13.72 | -12.50
    | +29.56 | -16.22 | -3.11 | +54.39 | -12.61 | +1.72 | +14.78 | -20.50 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
- en: '| D-avg | Best ($\%$) | 24.00 | 30.00 | 49.00 | 17.00 | 3.00 | 69.00 | 33.00
    | 28.00 | 26.00 | 35.00 | 49.00 | 2.00 | 18.00 | 24.00 | 23.00 | 53.00 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
- en: '| change | -2.61 | -8.44 | +3.33 | +6.44 | -8.39 | +2.61 | +4.28 | -18.50 |
    -14.44 | -12.22 | -25.11 | -19.61 | +5.39 | +3.72 | +0.78 | -4.50 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
- en: '| Worst ($\%$) | 30.00 | 10.00 | 49.00 | 33.00 | 20.00 | 52.00 | 28.00 | 46.00
    | 70.00 | 42.00 | 59.00 | 50.00 | 3.00 | 32.00 | 23.00 | 45.00 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
- en: '| change | +3.39 | -28.44 | +3.33 | +22.44 | +8.61 | -14.39 | -0.72 | -0.50
    | +29.56 | -5.22 | -15.11 | +28.39 | -9.61 | +11.72 | +0.78 | -12.50 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
- en: 'To enable a deeper analysis and understanding of the performance of an algorithm,
    it is essential to identify the key factors and circumstances influencing it [[140](#bib.bib140)].
    To this end, we construct a hybrid benchmark with rich attribute annotations.
    It consists of 1,800 images randomly selected from six SOD datasets (300 for each),
    namely SOD [[129](#bib.bib129)], ECSSD [[55](#bib.bib55)], DUT-OMRON [[56](#bib.bib56)],
    PASCAL-S [[108](#bib.bib108)], HKU-IS [[27](#bib.bib27)] and DUTS test set [[97](#bib.bib97)].
    Inspired by [[108](#bib.bib108), [140](#bib.bib140)], we annotate each image with
    an extensive set of attributes covering typical object types, challenging factors
    and diverse scene categories. Specifically, the annotated salient objects are
    categorized into Human, Animal, Artifact and NatObj (Natural Objects), where NatObj
    includes natural objects such as fruit, plant, mountains, icebergs, lakes, *etc*.
    The challenging factors describe specific situations that often bring difficulties
    to SOD, such as occlusions, background clutter, and complex shapes (see Table [6](#S5.T7
    "TABLE 6 ‣ 5.1 Quick Overview ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey")). The image scenes
    include Indoor, Urban and Natural, where the last two indicate different outdoor
    environments. It is worth mentioning that the attributes are not mutually exclusive.
    Some sample images with attribute annotations are shown in Fig. [4](#S4.F4 "Figure
    4 ‣ 4.3 Discussion ‣ 4 Evaluation Metrics ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey"). Please note that this benchmark will also
    be used in §[5.4](#S5.SS4 "5.4 Robustness Against General Input Perturbations
    ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey") and §[5.5](#S5.SS5 "5.5 Robustness Against
    Manually Designed Input Perturbations ‣ 5 Benchmarking and Empirical Analysis
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey").'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: For the baselines in our attribute-based analysis, we choose the three top-performing
    heuristic models again, *i.e*., HS [[35](#bib.bib35)], DRFI [[53](#bib.bib53)]
    and wCtr [[36](#bib.bib36)], and three recent famous deep methods, *i.e*., DGRL [[85](#bib.bib85)],
    PAGR [[86](#bib.bib86)] and PiCANet [[40](#bib.bib40)]. All three deep models
    are trained on DUTS-train [[97](#bib.bib97)] and have publicly released implementations.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
- en: 5.3.2 Analysis
  id: totrans-416
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In Table [7](#S5.T6 "TABLE 7 ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey"), we report the
    performance on subsets of our hybrid dataset characterized by a particular attribute.
    To provide better insight, in Table [VIII](#S5.T8 "TABLE VIII ‣ 5.3.1 Hybrid Benchmark
    Dataset with Attribute Annotations ‣ 5.3 Attribute-Based Study ‣ 5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey"), we select images with the best-100 and worst-100 model predictions,
    and compare the portion distributions of attributes w.r.t. the ones over the whole
    dataset. Below are some important observations drawn from these experiments.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
- en: '• ‘Easy’ and ‘hard’ object categories. Deep and non-deep SOD models view object
    categories differently (Table [7](#S5.T6 "TABLE 7 ‣ 5 Benchmarking and Empirical
    Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")).
    For the deep methods (D-avg), NatObj is clearly the most challenging one which
    is probably due to its small number of training samples and complex topologies.
    Animal appears to be the easiest, which can be attributed to its significant semantics.
    By contrast, traditional methods (ND-avg) struggle with Human, revealing their
    limitations in capturing high-level semantics. We are surprised to find that the
    deep models significantly outperform the non-deep ones over almost all the object
    categories, except NatObj. This demonstrates the value of heuristic assumptions
    in certain scenes and the potential of embedding human prior knowledge into current
    deep learning schemes.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: '• Most and least challenging factors. Table [7](#S5.T6 "TABLE 7 ‣ 5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey") shows that, interestingly, both deep and non-deep methods handle
    $\mathcal{LO}$ well. In addition, both types of methods face difficulties with
    $\mathcal{SO}$, highlighting a promising direction for future efforts. Besides,
    we find that $\mathcal{CS}$ and $\mathcal{MO}$ are challenging for deep models,
    showing that current solutions still fall short at determining the relative importance
    of different objects.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: '• Most and least difficult scenes. Deep and heuristic methods perform similarly
    when faced with different scenes (Table [7](#S5.T6 "TABLE 7 ‣ 5 Benchmarking and
    Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey")). For both types of methods, Natural is the easiest, which is reasonable
    as the scenes are typically simple. Further, though both contain numerous objects,
    Indoor is more challenging than Urban as it often suffers from highly unevenly
    distributed illumination and more complex scenes. Our experiments also show that
    the utility of SOD models in real, and especially complex, environments is still
    limited.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: '• Additional advantages of deep models. As shown in Table [7](#S5.T6 "TABLE
    7 ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey"), deep models achieve great improvements on
    semantically rich objects (Human, Animal and Artifact), demonstrating advantages
    in semantic modeling. This is verified again by their good performance on complex
    object shapes ($\mathcal{HO}$, $\mathcal{OV}$, $\mathcal{OC}$, $\mathcal{CT}$).
    Deep models also narrow the gap between different scene categories (Indoor v.s.
    Natural), indicating an improved robustness against various backgrounds.'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
- en: '• Best and worst predictions. From Table [VIII](#S5.T8 "TABLE VIII ‣ 5.3.1
    Hybrid Benchmark Dataset with Attribute Annotations ‣ 5.3 Attribute-Based Study
    ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey"), in addition to similar conclusions drawn from
    Table [7](#S5.T6 "TABLE 7 ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey"), some unique and interesting
    observations can be made. First, for deep methods, NatObj spans a large range
    of challenge, containing both the simplest and hardest samples. Thus, future efforts
    should pay more attention to the hard samples in NatObj. In addition, after considering
    data distribution bias, $\mathcal{CS}$ is the most challenging factor for deep
    models.'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/57cb01ba4fc1c7b1aac685357634c399.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Examples of saliency prediction under various input perturbations.
    The max F values are denoted in red. See §[5.4](#S5.SS4 "5.4 Robustness Against
    General Input Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey") for more details.'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IX: Input perturbation study on the hybrid benchmark. ND-avg indicates
    the average score of three heuristic models: HS [[35](#bib.bib35)], DRFI [[53](#bib.bib53)]
    and wCtr [[36](#bib.bib36)]. D-avg indicates the average score of three deep learning
    models: SRM [[71](#bib.bib71)], DGRL [[85](#bib.bib85)] and PiCANet [[40](#bib.bib40)].
    Best in red and worst with underline. See §[5.4](#S5.SS4 "5.4 Robustness Against
    General Input Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey") for more details.'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '|  Metric | Method | Original |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
- en: '&#124; Gaus. blur &#124;'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ( $\sigma$= ) &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Gaus. noise &#124;'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ( var= ) &#124;'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '| Rotation |  Gray |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
- en: '|    $2$ | $4$ |  $0.01$ |  $0.08$ |   $15^{\circ}$ | $-15^{\circ}$ |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
- en: '| max F$\uparrow$ | ^∗HS[[35](#bib.bib35)] | .600 | -.012 | -.096 | -.022 |
    -.057 | +.015 | +.009 | -.104 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
- en: '| ^∗DRFI[[53](#bib.bib53)] | .670 | -.040 | -.103 | -.035 | -.120 | -.009 |
    -.009 | -.086 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
- en: '| ^∗wCtr[[36](#bib.bib36)] | .611 | +.006 | -.000 | -.024 | -.136 | -.004 |
    -.003 | -.070 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
- en: '| SRM [[71](#bib.bib71)] | .817 | -.090 | -.229 | -.025 | -.297 | -.028 | -.029
    | -.042 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
- en: '| DGRL [[85](#bib.bib85)] | .831 | -.088 | -.365 | -.050 | -.402 | -.031 |
    -.022 | -.026 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
- en: '| PiCANet [[40](#bib.bib40)] | .848 | -.048 | -.175 | -.014 | -.148 | -.005
    | -.008 | -.039 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
- en: '| ^∗ND-avg | .627 | -.015 | -.066 | -.027 | -.104 | -.000 | -.001 | -.087 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
- en: '| D-avg | .832 | -.075 | -.256 | -.041 | -.282 | -.021 | -.020 | -.037 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
- en: ^∗ Non-deep learning model.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 5.4 Robustness Against General Input Perturbations
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The robustness of a model lies in its stability against corrupt inputs. Intuitively,
    the outputs of a robust SOD model should be repeatable on slightly different images
    with the same content. However, the recently introduced adversarial examples,
    *i.e*. maliciously constructed inputs that fool machine learning models, can degrade
    the performance of deep image classifiers significantly. Current deep SOD models
    likely face a similar challenge. Therefore, in this section, we examine the robustness
    of SOD models by comparing their outputs for randomly perturbed inputs, such as
    noisy or blurred images. Then, in §[5.5](#S5.SS5 "5.5 Robustness Against Manually
    Designed Input Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey"), we will study
    the robustness to manually designed adversarial examples.'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: The input perturbations investigated include Gaussian blur, Gaussian noise,
    Rotation, and Gray. For blurring, we employ Gaussian blur kernels with a sigma
    of $2$ or $4$. For noise, we select two variance values, *i.e*., $0.01$ and $0.08$,
    to cover both tiny and medium magnitudes. For rotation, we rotate the images by
    $+15^{\circ}$ and $-15^{\circ}$, respectively, and cut out the largest box with
    the original aspect ratio. The gray images are generated using the Matlab rgb2gray
    function.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: 'As in §[5.3](#S5.SS3 "5.3 Attribute-Based Study ‣ 5 Benchmarking and Empirical
    Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey"),
    we include three popular heuristic models [[35](#bib.bib35), [53](#bib.bib53),
    [36](#bib.bib36)] and three deep methods [[71](#bib.bib71), [85](#bib.bib85),
    [40](#bib.bib40)] in our experiments. Table [IX](#S5.T9 "TABLE IX ‣ 5.3.2 Analysis
    ‣ 5.3 Attribute-Based Study ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey") shows the results.
    Overall, compared with deep models, heuristic methods are less sensitive towards
    input perturbations. The compactness and abstractness of superpixels likely explains
    much of this. Specifically, heuristic methods are rarely affected by Rotation,
    but perform worse under strong Gaussian blur, strong Gaussian noise and Gray.
    Deep methods suffer the most under Gaussian blur and strong Gaussian noise, which
    may be caused by the damage to shallow-layer features. Deep methods are relatively
    robust against Rotation, revealing the rotation invariance of DNNs brought by
    the pooling operation. Interestingly, we further find that, among the three deep
    models, PiCANet [[40](#bib.bib40)] demonstrates excellent robustness against a
    wide range of input perturbations, including Gaussian blur, Gaussian noise, and
    Rotation. We attribute this to its effective non-local operation. This reveals
    that effective network designs can improve the robustness to random perturbations.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Robustness Against Manually Designed Input Perturbations
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given the significant concerns with model robustness to random perturbations,
    this section presents an analysis focusing specifically on manually designed adversarial
    perturbations. Recent years have witnessed great advance in SOD driven by the
    progress of deep learning. However, whether the deep SOD models are as powerful
    as they seem is a question to worth pondering. Meanwhile, DNNs have been previously
    found to be susceptible to adversarial attacks, where visually imperceptible perturbations
    lead to completely different predictions [[141](#bib.bib141)]. Though intensively
    studied in classification tasks, adversarial${}_{\!}$ attacks${}_{\!}$ in${}_{\!}$
    SOD${}_{\!}$ are${}_{\!}$ rarely${}_{\!}$ explored. As SOD has been integrated
    as a critical part in many security systems and commercial projects, SOD models
    also have potential risks of being attacked. Specifically, SOD plays a significant
    role in many security systems, for detecting the candidates of interest targets
    from remote sensing images [[142](#bib.bib142)], video surveillance data [[143](#bib.bib143)],
    or sensor signals of autonomous vehicles [[144](#bib.bib144)]. In such situation,
    examining the robustness of SOD models is rather important because the insecurity
    of SOD modules may cause severe losses, *e.g*., the criminals may use inconspicuous
    adversarial perturbations to fool SOD modules and then cheat the surveillance
    systems. Besides, SOD has benefited many commercial projects such as photo editing [[20](#bib.bib20)],
    and image/video compression [[145](#bib.bib145)]. The adversarial attacks launched
    by hackers on the embedded SOD modules would inevitably affect the functioning
    of commercial products and impacting users, causing losses for the developers
    and companies. Therefore, studying the robustness of SOD models is crucial for
    defending these applications against malicious attacks. In this section, we study
    the robustness against adversarial attacks and transferability of adversarial
    examples targeting different SOD models. Our observations are expected to shed
    light on adversarial attacks and defenses for SOD, providing a better understanding
    of vulnerabilities of deep SOD models and improving the robustness of SOD involved
    practical applications.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/383204c1047e7c8d80edcdd965b0b6db.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Examples of SOD prediction under adversarial perturbations of different
    target networks. The perturbations are magnified by $10$ for better visualization.
    Red for max F. See §[5.5](#S5.SS5 "5.5 Robustness Against Manually Designed Input
    Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey") for details.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.1 Robustness of SOD Against Adversarial Attacks
  id: totrans-451
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'For measuring the robustness of deep SOD models, we adopt and modify an adversarial
    attack algorithm designed for semantic segmentation, *i.e*., Dense Adversary Generation
    (DAG) [[146](#bib.bib146)]. We choose three representative deep models, *i.e*.,
    SRM​ [[71](#bib.bib71)], DGRL​ [[85](#bib.bib85)] and PiCANet​ [[40](#bib.bib40)]
    for our study. The experiment is conducted on the hybrid benchmark introduced
    in §[5.3](#S5.SS3 "5.3 Attribute-Based Study ‣ 5 Benchmarking and Empirical Analysis
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey"). Following [[146](#bib.bib146)],
    we measure the perceptibility of the adversarial examples by computing the average
    perceptibility of the adversarial perturbations generated from the hybrid benchmark.
    The values for the three models are $3.54\!\times\!10^{-3}$, $3.57\!\times\!10^{-3}$,
    and $3.51\!\times\!10^{-3}$, respectively.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: 'Exemplar adversarial cases are shown in Fig. [6](#S5.F6 "Figure 6 ‣ 5.5 Robustness
    Against Manually Designed Input Perturbations ‣ 5 Benchmarking and Empirical Analysis
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey"). As
    can be seen, the adversarial attacks can prevent the SOD models from producing
    reliable salient object candidates. Quantitative results are listed in Table [X](#S5.T10
    "TABLE X ‣ 5.5.2 Transferability Across Networks ‣ 5.5 Robustness Against Manually
    Designed Input Perturbations ‣ 5 Benchmarking and Empirical Analysis ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey"). The underlined
    entries of Table [X](#S5.T10 "TABLE X ‣ 5.5.2 Transferability Across Networks
    ‣ 5.5 Robustness Against Manually Designed Input Perturbations ‣ 5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey") reveal that the three deep SOD models investigated are vulnerable
    to adversarial perturbations of the inputs. However, as can be observed by comparing
    Tables [IX](#S5.T9 "TABLE IX ‣ 5.3.2 Analysis ‣ 5.3 Attribute-Based Study ‣ 5
    Benchmarking and Empirical Analysis ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey") and [X](#S5.T10 "TABLE X ‣ 5.5.2 Transferability Across
    Networks ‣ 5.5 Robustness Against Manually Designed Input Perturbations ‣ 5 Benchmarking
    and Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An
    In-depth Survey"), the models are more robust to random input perturbations. These
    differences in robustness might be interpretated by the the distance from the
    inputs to the decision boundary in high dimensional space. The intentionally designed
    adversarial inputs often lie closer to the decision boundary than the random inputs [[147](#bib.bib147)],
    and can thus more easily cause pixel-wise misclassification.'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: 5.5.2 Transferability Across Networks
  id: totrans-454
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Previous research has revealed that adversarial perturbations can be transferred
    across networks, *i.e*. adversarial examples targeting one model can mislead another
    without any modification [[148](#bib.bib148)]. This transferability is widely
    used for black-box attacks against real-world systems. To investigate the transferability
    of perturbations for deep SOD models, we use the adversarial perturbation computed
    on one SOD model to attack another.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [X](#S5.T10 "TABLE X ‣ 5.5.2 Transferability Across Networks ‣ 5.5 Robustness
    Against Manually Designed Input Perturbations ‣ 5 Benchmarking and Empirical Analysis
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey") shows
    the experimental results for the three models under investigation (SRM [[71](#bib.bib71)],
    DGRL [[85](#bib.bib85)] and PiCANet [[40](#bib.bib40)]). While the DAG attack
    leads to severe performance drops for the targeted model (see the diagonal), it
    causes much less degradation to other models, *i.e*., the transferability between
    models of different network structures is weak for SOD task, which is similar
    to the transferability observed for semantic segmentation, as analyzed in [[146](#bib.bib146)].
    This may be because the gradient directions of different models are orthogonal
    to each other [[149](#bib.bib149)], so the gradient-based attack in the experiment
    transfers poorly to non-targeted models. However, adversarial images generated
    from an ensemble of multiple models might generate non-targeted adversarial instances
    with better transferability [[149](#bib.bib149)], which would be a great threat
    to deep SOD models.'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE X: Results for adversarial attack experiments. Max F$\uparrow$ on the
    hybrid benchmark is presented when exerting adversarial perturbations from different
    models. Worst results are underline. See §[5.5](#S5.SS5 "5.5 Robustness Against
    Manually Designed Input Perturbations ‣ 5 Benchmarking and Empirical Analysis
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey") for
    details.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: '|  Attack from |    SRM [[71](#bib.bib71)] |   DGRL [[85](#bib.bib85)] | PiCANet [[40](#bib.bib40)]
    |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
- en: '| None | .817 | .831 | .848 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
- en: '| SRM [[71](#bib.bib71)] | .263 | .780 | .842 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
- en: '| DGRL [[85](#bib.bib85)] | .778 | .248 | .844 |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
- en: '| PiCANet [[40](#bib.bib40)] | .772 | .799 | .253 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
- en: 5.6 Cross-Dataset Generalization Evaluation
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Datasets are responsible for much of the recent progress in SOD, not just as
    sources for training deep models, but also as means for measuring and comparing
    performance. Datasets are collected with the goal of representing the visual world,
    and to summarize the algorithm as a single number (*i.e*., benchmark score). A
    concern thus arises: it is necessary to evaluate how well a particular dataset
    represents the real world; or, more specifically, to quantitatively measuring
    the dataset’s generalization ability. Unfortunately, previous studies [[44](#bib.bib44)]
    are quite limited – mainly concerning the degrees of center bias in different
    SOD datasets. Here, we follow [[150](#bib.bib150)] to assess how general SOD datasets
    are. We study the generalization and difficulty of several mainstream SOD datasets
    by performing a cross-dataset analysis, *i.e*., training on one dataset, and testing
    on the others. We expect our experiments to stimulate discussion in the community
    regarding this essential but largely neglected issue.'
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
- en: 'We first train a typical SOD model on one dataset, and then explore how well
    it generalizes to a representative set of other datasets, compared with its performance
    on the “native” test set. Specifically, we implement the typical SOD model as
    a bottom-up/top-down structure, which has been the most standard and popular SOD
    architecture these years and is the basis of many current top-performing models [[93](#bib.bib93),
    [101](#bib.bib101), [82](#bib.bib82), [102](#bib.bib102)]. As shown in Fig. [7](#S5.F7
    "Figure 7 ‣ 5.6 Cross-Dataset Generalization Evaluation ‣ 5 Benchmarking and Empirical
    Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey"),
    the encoder part is borrowed from VGG16 [[151](#bib.bib151)], and the decoder
    consists of three convolutional layers that gradually refine the saliency prediction.
    We pick six representative datasets ​[[107](#bib.bib107), [55](#bib.bib55), [56](#bib.bib56),
    [27](#bib.bib27), [97](#bib.bib97), [131](#bib.bib131)]. For each dataset, we
    train the SOD model with $800$ randomly selected training images and test it on
    $200$ other validation images. Please note that a total of $1,000$ is the maximum
    possible number of images considering the size of the smallest selected dataset,
    ECSSD [[55](#bib.bib55)].'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/273548cde49698318c5e201435ad78f5.png)'
  id: totrans-467
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Network architecture of the SOD model used in cross-dataset generalization
    evaluation. See §[5.6](#S5.SS6 "5.6 Cross-Dataset Generalization Evaluation ‣
    5 Benchmarking and Empirical Analysis ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey") for more detailed descriptions.'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE XI: Results for cross-dataset generalization experiment. Max F$\uparrow$
    for saliency prediction when training on one dataset (rows) and testing on another
    (columns). “Self” refers to training and testing on the same dataset (same as
    diagonal). “Mean Others” indicates average performance on all except self. See
    §[5.6](#S5.SS6 "5.6 Cross-Dataset Generalization Evaluation ‣ 5 Benchmarking and
    Empirical Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey") for details.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
- en: '|   <svg version="1.1" height="38.74" width="110.7" overflow="visible"><g transform="translate(0,38.74)
    scale(1,-1)"><g  transform="translate(0,0)"><g transform="translate(0,9.46) scale(1,
    -1)"><foreignobject width="48.85" height="9.46" overflow="visible">​Train on:​​</foreignobject></g></g>
    <g  transform="translate(66.42,29.28)"><g transform="translate(0,9.46) scale(1,
    -1)"><foreignobject width="44.28" height="9.46" overflow="visible">​​Test on:</foreignobject></g></g></g></svg>
    |'
  id: totrans-470
  prefs: []
  type: TYPE_TB
- en: '&#124; MSRA- &#124;'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; 10K[[107](#bib.bib107)] &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ECSSD &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[55](#bib.bib55)] &#124;'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DUT-OM &#124;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RON[[56](#bib.bib56)] &#124;'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; HKU- &#124;'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; IS[[27](#bib.bib27)] &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DUTS &#124;'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[97](#bib.bib97)] &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SOC &#124;'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; [[131](#bib.bib131)] &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
- en: '|  Self |'
  id: totrans-488
  prefs: []
  type: TYPE_TB
- en: '&#124; Mean &#124;'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; others &#124;'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Percent &#124;'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; drop$\downarrow$ &#124;'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
- en: '| MSRA10K[[107](#bib.bib107)] | .875 | .818 | .660 | .849 | .671 | .617 | .875
    | .723 | 17% |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
- en: '| ECSSD[[55](#bib.bib55)] | .844 | .831 | .630 | .833 | .646 | .616 | .831
    | .714 | 14% |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
- en: '| DUT-OMRON[[56](#bib.bib56)] | .795 | .752 | .673 | .779 | .623 | .567 | .673
    | .703 | -5% |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
- en: '| HKU-IS[[27](#bib.bib27)] | .857 | .838 | .695 | .880 | .719 | .639 | .880
    | .750 | 15% |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
- en: '| DUTS[[97](#bib.bib97)] | .857 | .834 | .647 | .860 | .665 | .654 | .665 |
    .770 | -16% |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
- en: '| SOC[[131](#bib.bib131)] | .700 | .670 | .517 | .666 | .514 | .593 | .593
    | .613 | -3% |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
- en: '| Mean others | .821 | .791 | .637 | .811 | .640 | .614 | - | - | - |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
- en: 'Table [XI](#S5.T11 "TABLE XI ‣ 5.6 Cross-Dataset Generalization Evaluation
    ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object Detection in the Deep
    Learning Era: An In-depth Survey") summarizes the results of cross-dataset generalization,
    measured by max F. Each column corresponds to the performance when training on
    all the datasets separately and testing on one. Each row indicates training on
    one dataset and testing on all of them. Since our training/testing protocol is
    different from the one used in the benchmarks mentioned in previous sections,
    the actual performance numbers are not meaningful. Rather, it is the relative
    performance difference that matters. Not surprisingly, we observe that the best
    results are achieved when training and testing on the same dataset. By looking
    at the numbers across each column, we can determine how easy a dataset is for
    models trained on the other datasets. By looking at the numbers across one row,
    we can determine how good a dataset is at generalizing to the others. We find
    that SOC [[131](#bib.bib131)] is the most difficult dataset (lowest column, Mean
    others $0.614$). MSRA10K [[107](#bib.bib107)] appears to be the easiest one (highest
    column, Mean others $0.811$), and generalizes the worst (highest row, Percent
    drop $17\%$). DUTS [[97](#bib.bib97)] is shown to have the best generalization
    ability (lowest row, Percent drop $-16\%$).'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on these analyses, we would make the following recommendations for SOD
    datasets: 1) For training deep models, DUTS [[97](#bib.bib97)] is a good choice
    because it has the best generalizability. 2) For testing, SOC [[131](#bib.bib131)]
    is good for assessing the worst-case performances, since it is the most challenging
    dataset. DUT-OMRON [[56](#bib.bib56)] and DUTS-test [[97](#bib.bib97)] deserve
    more considerations as they are also very difficult.'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
- en: 6 More Discussions
  id: totrans-505
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our previous systematic review and empirical studies characterized the models
    (§[2](#S2 "2 Deep Learning based SOD Models ‣ Salient Object Detection in the
    Deep Learning Era: An In-depth Survey")), datasets (§[3](#S3 "3 SOD Datasets ‣
    Salient Object Detection in the Deep Learning Era: An In-depth Survey")), metrics
    (§[4](#S4 "4 Evaluation Metrics ‣ Salient Object Detection in the Deep Learning
    Era: An In-depth Survey")), and challenges (§[5](#S5 "5 Benchmarking and Empirical
    Analysis ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey"))
    of deep SOD. Here we further posit active research directions, and outline several
    open issues.'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Model Design
  id: totrans-507
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on the review of deep SOD network architectures in §[2.1](#S2.SS1 "2.1
    Representative Network Architectures for SOD ‣ 2 Deep Learning based SOD Models
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey"), as
    well as recent advances in related fields, we here discuss several essential directions
    for SOD model design.'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
- en: '• Network topology. Network topology determines the within-network information
    flow, which directly affects model capacity and training difficulty and thus influences
    the best possible performance. To figure out an effective network topology for
    SOD, diverse architectures have been explored (§[2.1](#S2.SS1 "2.1 Representative
    Network Architectures for SOD ‣ 2 Deep Learning based SOD Models ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey")), such as multi-stream
    networks, side-out fusion networks, as well as bottom-up/top-down networks. However,
    all these network architectures are hand-designed. Thus, a promising direction
    would be to use automated machine learning (AutoML) algorithms, such as neural
    architecture search [[152](#bib.bib152)], to automatically search for the best-performing
    SOD network topology.'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
- en: • Loss function. Most deep SOD methods are trained with the standard binary
    cross-entropy loss, which may fail to fully capture the quality factors for the
    SOD task. Only a few efforts have been made to derive losses from SOD evaluation
    metrics [[87](#bib.bib87)]. Thus, it is worth exploring more effective SOD loss
    functions, such as the mean intersection-over-union loss [[153](#bib.bib153)]
    and affinity field matching loss [[154](#bib.bib154)].
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
- en: '• Adaptive computation. Currently, all deep SOD models are fixed feed-forward
    structures. However, most parameters model high-level features that, in contrast
    to low-level and many mid-level concepts, cannot be broadly shared across categories/scenes.
    As such, we would like to ask the following question: What if a SOD model could
    directly execute certain layers that can best explain the saliency patterns in
    a given scene? To answer this, one could leverage adaptive computation techniques [[155](#bib.bib155),
    [156](#bib.bib156)] to vary the amount of computation on-the-fly, *i.e*., by selectively
    activating part of the network in an input-dependent fashion. This could bring
    a better trade-off between network depth and computational cost. On the other
    hand, adapting inference pathways for different inputs would provide finer-grained
    discriminative ability for various attributes. Therefore, exploring dynamic network
    structures in SOD is promising for improving both efficiency and effectiveness.'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Data Collection
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our previous discussions (§[3](#S3 "3 SOD Datasets ‣ Salient Object Detection
    in the Deep Learning Era: An In-depth Survey")) and analyses (§[5.3](#S5.SS3 "5.3
    Attribute-Based Study ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey") and §[5.6](#S5.SS6 "5.6
    Cross-Dataset Generalization Evaluation ‣ 5 Benchmarking and Empirical Analysis
    ‣ Salient Object Detection in the Deep Learning Era: An In-depth Survey")) on
    current SOD datasets revealed several factors that are essential for future dataset
    collection.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
- en: '• Annotation inconsistency. Though existing SOD datasets play a critical role
    in training and evaluating modern SOD models, annotation inconsistencies among
    different SOD datasets have essentially been ignored by the community. The inconsistencies
    are mainly caused by separate subjects and rules/conditions during dataset annotation
    (see Fig. [8](#S6.F8 "Figure 8 ‣ 6.2 Data Collection ‣ 6 More Discussions ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey")). To ease annotation
    burdens, most current SOD datasets only have a few human annotators directly identify
    the salient objects, instead of considering real human eye fixation behavior.
    Maintaining annotation consistency among newly collected datasets is an important
    consideration.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/333ca6af4b6fd16ba68083d5ae6734f2.png)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Examples for annotation inconsistency. Each row shows two exemplar
    image pairs. See §[6.2](#S6.SS2 "6.2 Data Collection ‣ 6 More Discussions ‣ Salient
    Object Detection in the Deep Learning Era: An In-depth Survey") for more detailed
    descriptions.'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
- en: '• Coarse v.s. fine annotation. Modern SOD datasets all have pixel-level annotations,
    which greatly boosts the performance of deep SOD models. However, pixel-wise ground-truths
    are very costly to collect considering the complex object boundaries and the intense
    data requirement. Further, the annotation qualities of different datasets are
    different (see bicycles in Fig. [8](#S6.F8 "Figure 8 ‣ 6.2 Data Collection ‣ 6
    More Discussions ‣ Salient Object Detection in the Deep Learning Era: An In-depth
    Survey")). Finer labels are believed to be essential for high-quality saliency
    prediction, but usually take more time to collect. Thus, given a limited budget,
    finding the optimal annotation strategy is an open problem. Some works have studied
    the relationship between label quality and model performance in semantic segmentation [[157](#bib.bib157)],
    highlighting a possible research direction for SOD dataset collection. In addition,
    current SOD models typically assume that the annotations are perfect. Thus, it
    would also be of value to explore robust SOD models that can learn saliency patterns
    from imperfectly annotated data.'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
- en: • Domain-specific SOD datasets. SOD has shown potential in a wide range of applications,
    such as autonomous vehicles, video games, medical image processing, *etc*. Due
    to the different visual appearances and semantic components, the saliency mechanisms
    in these applications are quite different from that of conventional natural images.
    Thus, collecting domain-specific datasets might benefit the application of SOD
    in certain scenarios, as observed in FP for crowds [[158](#bib.bib158)], webpages [[159](#bib.bib159)]
    or driving [[160](#bib.bib160)], and better connect SOD to the biological top-down
    visual attention mechanism and human mental state.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Saliency Ranking and Relative Saliency
  id: totrans-519
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Current algorithms seems over-focused on directly regressing the saliency map
    to pursue a high benchmarking number, while neglecting the fact that the absolute
    magnitude of values in a saliency map might be less important than the relative
    saliency values among objects [[108](#bib.bib108)]. Though the relative value/rank
    order is rarely considered in the context of benchmarking metrics (with the exception
    of [[79](#bib.bib79)]), it is crucial for better modeling human visual attention
    behavior. This is, in essence, a selection process that centers our attention
    on certain important elements of the surroundings, while blending other relatively
    unimportant things into the background. This not only hints at one shortcoming
    of existing benchmarking paradigms and data collection strategies, but also reveals
    a common limitation of current methods. Current state-of-the-arts fall short at
    determining the relative importance of objects, such as identifying the most important
    person in a crowded room. This is also evidenced by the experiments in §[5.3](#S5.SS3
    "5.3 Attribute-Based Study ‣ 5 Benchmarking and Empirical Analysis ‣ Salient Object
    Detection in the Deep Learning Era: An In-depth Survey"), which show that deep
    models face great difficulties in complex ($\mathcal{CS}$), indoor (Indoor) or
    multi-object ($\mathcal{MO}$) scenes. In other words, deep SOD models, though
    good at semantic modeling, require higher-level image understanding. Exploring
    more powerful network designs that explicitly reason the relative saliency and
    revisiting classic cognitive theories are both promising directions to overcome
    this issue.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Linking SOD to Visual Fixations
  id: totrans-521
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The strong correlation between eye movements (implicit saliency) and explicit
    object saliency has been explored throughout history [[161](#bib.bib161), [162](#bib.bib162),
    [108](#bib.bib108), [44](#bib.bib44), [163](#bib.bib163)]. However, despite the
    deep connections between the problems of FP and SOD, the major computational models
    of the two tasks remain largely distinct; only a few SOD models consider both
    tasks simultaneously [[72](#bib.bib72), [87](#bib.bib87), [96](#bib.bib96)]. This
    is mainly due to the overemphasis on the specific setting of SOD and the design
    bias of current SOD datasets, which overlooks the connection to eye fixations
    during data annotation. As stated in [[108](#bib.bib108)], such dataset design
    bias not only creates a discomforting disconnection between FP and SOD, but also
    further misleads the algorithm designing. Exploring classic visual attention theories
    in SOD is a promising and crucial direction which could make SOD models more consistent
    with the visual processing of human visual system and provide better explainability.
    In addition, the ultimate goal of visual saliency modeling is to understand the
    underlying rationale of the visual attention mechanism. However, with the current
    focus on exploring more powerful neural network architectures and beating the
    latest benchmark numbers on different datasets, have we perhaps lost sight of
    the original purpose? The solution to these problems requires dense collaborations
    between the FP and SOD communities.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 Learning SOD in a Weakly-/Unsupervised Manner
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep SOD methods are typically trained in a fully-supervised manner with a plethora
    of finely-annotated pixel-level ground-truths. However, it is highly costly and
    time-consuming to construct a large-scale, well-annotated SOD dataset. Though
    some efforts have been made to achieve SOD with limited supervision, *i.e*., by
    leveraging category-level labels [[97](#bib.bib97), [68](#bib.bib68), [69](#bib.bib69)]
    or pseudo pixel-wise annotations [[83](#bib.bib83), [98](#bib.bib98), [67](#bib.bib67),
    [99](#bib.bib99), [81](#bib.bib81)], there is still a notable gap with the fully-supervised
    counterparts. In contrast, humans usually learn with little or even no supervision.
    Since the ultimate goal of visual saliency modeling is to understand the visual
    attention mechanism, learning SOD in an weakly-/unsupervised manner would be of
    great value to both the research community and real-world applications. Further,
    it would also help us understand which factors truly drive our attention mechanism
    and saliency pattern understanding. Given the massive number of algorithmic breakthroughs
    over the past few years, we can expect a flurry of innovation towards this promising
    direction.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
- en: 6.6 Pre-training with Self-Supervised Visual Features
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current deep SOD methods are typically built on ImageNet-pretrained networks,
    and fine-tuned on SOD datasets. It is believed that parameters trained on ImageNet
    can serve as a good starting point to accelerate the convergence of training and
    prevent overfitting on smaller-scale SOD datasets. Besides pre-training deep SOD
    models on the de facto dataset, ImageNet, another option is to leverage self-supervised
    learning techniques [[164](#bib.bib164)] to learn effective visual features from
    a vast amount of unlabeled images/videos. The visual features can be learned through
    various pretext tasks like image inpainting [[165](#bib.bib165)], colorization [[166](#bib.bib166)],
    clustering [[167](#bib.bib167)], *etc*., and can be generalized to other vision
    tasks. Fine-tuning the SOD models on parameters trained from self-supervised learning
    is promising to yield better performance compared to the ImageNet initialization.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
- en: 6.7 Efficient SOD for Real-World Application
  id: totrans-527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current top-leading deep SOD models are designed to be complicated in order
    to achieve increased learning capacity and improved performance. However, more
    ingenuous and light-weight architectures are required to fulfill the requirements
    of mobile and embedded applications, such as robotics, autonomous driving, augmented
    reality, *etc*. The degradation of accuracy and generalization ability caused
    by model scale deduction should be minimal. To facilitate the application of SOD
    in real-world scenarios, it is possible to utilize model compression [[168](#bib.bib168)]
    or knowledge distillation [[169](#bib.bib169), [170](#bib.bib170)] techniques
    to develop compact and fast SOD models with competitive performance. Such compression
    techniques have already been shown effective in improving generalization ability
    and alleviating under-fitting for training efficient object detection models [[171](#bib.bib171)].
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper we present, to the best of our knowledge, the first comprehensive
    review of SOD focusing on deep learning techniques. We first provide novel testimonies
    for categorizing deep SOD models from several distinct perspectives, including
    network architecture, level of supervision, *etc*. We then cover the contemporary
    literature on popular SOD datasets and evaluation criteria, providing a thorough
    performance benchmarking of major SOD methods and offering recommendations for
    several datasets and metrics that can be used to consistently assess different
    models. Next, we consider several previously under-explored issues related to
    benchmarking and baselines. In particular, we study the strengths and weaknesses
    of deep and non-deep SOD models by compiling and annotating a new dataset and
    evaluating several representative models on it, revealing promising directions
    for future efforts. We also study the robustness of SOD methods by analyzing the
    effects of various perturbations on the final performance. Moreover, for the first
    time in the field, we investigate the robustness of deep SOD models to maliciously
    designed adversarial perturbations and the transferability of these adversarial
    examples, providing baselines for future research. In addition, we analyze the
    generalization and difficulty of existing SOD datasets through a cross-dataset
    generalization study, and quantitatively reveal the dataset bias. We finally introduce
    several open issues and challenges of SOD in the deep learning era, providing
    insightful discussions and identifying a number of potentially fruitful directions
    forward.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, SOD has achieved notable progress thanks to the striking development
    of deep learning techniques. However, there are still under-explored problems
    on achieving more efficient model designs, training, and inference for both academic
    research and real-world applications. We expect this survey to provide an effective
    way to understand current state-of-the-arts and, more importantly, insight for
    the future exploration of SOD.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-532
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] J.-Y. Zhu, J. Wu, Y. Xu, E. Chang, and Z. Tu, “Unsupervised object class
    discovery via saliency-guided multiple class learning,” *IEEE Trans. Pattern Anal.
    Mach. Intell.*, vol. 37, no. 4, pp. 862–875, 2015.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] F. Zhang, B. Du, and L. Zhang, “Saliency-guided unsupervised feature learning
    for scene classification,” *IEEE Trans. Geosci. Remote Sens.*, vol. 53, no. 4,
    pp. 2175–2184, 2015.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] K. Xu, J. Ba, R. Kiros, K. Cho, A. Courville, R. Salakhudinov, R. Zemel,
    and Y. Bengio, “Show, attend and tell: Neural image caption generation with visual
    attention,” in *Proc. ACM Int. Conf. Mach. Learn.*, 2015, pp. 2048–2057.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] H. Fang, S. Gupta, F. Iandola, R. K. Srivastava, L. Deng, P. Dollár, J. Gao,
    X. He, M. Mitchell, J. C. Platt *et al.*, “From captions to visual concepts and
    back,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015, pp. 1473–1482.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. Das, H. Agrawal, L. Zitnick, D. Parikh, and D. Batra, “Human attention
    in visual question answering: Do humans and deep networks look at the same regions?”
    *Computer Vision and Image Understanding*, vol. 163, pp. 90–100, 2017.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Z. Ren, S. Gao, L.-T. Chia, and I. W.-H. Tsang, “Region-based saliency
    detection and its application in object recognition.” *IEEE Trans. Circuits Syst.
    Video Technol.*, vol. 24, no. 5, pp. 769–779, 2014.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. Zhang, D. Meng, L. Zhao, and J. Han, “Bridging saliency detection to
    weakly supervised object detection based on self-paced curriculum learning,” in
    *International Joint Conferences on Artificial Intelligence*, 2016.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] W. Wang, J. Shen, R. Yang, and F. Porikli, “Saliency-aware video object
    segmentation,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 40, no. 1, pp.
    20–33, 2018.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] H. Song, W. Wang, S. Zhao, J. Shen, and K.-M. Lam, “Pyramid dilated deeper
    convlstm for video salient object detection,” in *Proc. Eur. Conf. Comput. Vis.*,
    2018.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. Wei, J. Feng, X. Liang, M.-M. Cheng, Y. Zhao, and S. Yan, “Object region
    mining with adversarial erasing: A simple classification to semantic segmentation
    approach,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] X. Wang, S. You, X. Li, and H. Ma, “Weakly-supervised semantic segmentation
    by iteratively mining common object features,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] G. Sun, W. Wang, J. Dai, and L. Van Gool, “Mining cross-image semantics
    for weakly supervised semantic segmentation,” in *Proc. Eur. Conf. Comput. Vis.*,
    2020, pp. 347–365.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] R. Zhao, W. Ouyang, and X. Wang, “Unsupervised salience learning for person
    re-identification,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013,
    pp. 3586–3593.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] S. Bi, G. Li, and Y. Yu, “Person re-identification using multiple experts
    with random subspaces,” *Journal of Image and Graphics*, vol. 2, no. 2, 2014.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y.-F. Ma, L. Lu, H.-J. Zhang, and M. Li, “A user attention model for video
    summarization,” in *Proc. ACM Int. Conf. Multimedia*, 2002, pp. 533–542.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. Simakov, Y. Caspi, E. Shechtman, and M. Irani, “Summarizing visual
    data using bidirectional similarity,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2008, pp. 1–8.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. Han, E. J. Pauwels, and P. De Zeeuw, “Fast saliency-aware multi-modality
    image fusion,” *Neurocomputing*, vol. 111, pp. 70–80, 2013.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] P. L. Rosin and Y.-K. Lai, “Artistic minimal rendering with lines and
    blocks,” *Graphical Models*, vol. 75, no. 4, pp. 208–229, 2013.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] W. Wang, J. Shen, Y. Yu, and K.-L. Ma, “Stereoscopic thumbnail creation
    via efficient stereo saliency detection,” *IEEE Trans. Visualization and Comput.
    Graphics*, vol. 23, no. 8, pp. 2014–2027, 2016.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] W. Wang, J. Shen, and H. Ling, “A deep network solution for attention
    and aesthetics aware photo cropping,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    2018.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S. Avidan and A. Shamir, “Seam carving for content-aware image resizing,”
    in *ACM Trans. Graph.*, vol. 26, no. 3, 2007, p. 10.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Y. Sugano, Y. Matsushita, and Y. Sato, “Calibration-free gaze sensing
    using saliency maps,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2010,
    pp. 2667–2674.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Borji and L. Itti, “Defending yarbus: Eye movements reveal observers’
    task,” *Journal of Vision*, vol. 14, no. 3, pp. 29–29, 2014.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] A. Karpathy, S. Miller, and L. Fei-Fei, “Object discovery in 3d scenes
    via shape analysis,” in *Proc. IEEE Conf. Robot. Autom.*, 2013, pp. 2088–2095.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] S. Frintrop, G. M. García, and A. B. Cremers, “A cognitive approach for
    object discovery,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2014,
    pp. 2329–2334.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. M. Treisman and G. Gelade, “A feature-integration theory of attention,”
    *Cognitive psychology*, vol. 12, no. 1, pp. 97–136, 1980.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] G. Li and Y. Yu, “Visual saliency based on multiscale deep features,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015, pp. 5455–5463.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] L. Wang, H. Lu, X. Ruan, and M.-H. Yang, “Deep networks for saliency detection
    via local estimation and global search,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2015, pp. 3183–3192.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] R. Zhao, W. Ouyang, H. Li, and X. Wang, “Saliency detection by multi-context
    deep learning,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015, pp.
    1265–1274.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] T. Liu, J. Sun, N.-N. Zheng, X. Tang, and H.-Y. Shum, “Learning to detect
    a salient object,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2007,
    pp. 1–8.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] R. Achanta, S. Hemami, F. Estrada, and S. Susstrunk, “Frequency-tuned
    salient region detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2009, pp. 1597–1604.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] M.-M. Cheng, G.-X. Zhang, N. J. Mitra, X. Huang, and S.-M. Hu, “Global
    contrast based salient region detection,” *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2011.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] F. Perazzi, P. Krähenbühl, Y. Pritch, and A. Hornung, “Saliency filters:
    Contrast based filtering for salient region detection,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*   IEEE, 2012, pp. 733–740.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] J. Wang, H. Jiang, Z. Yuan, M.-M. Cheng, X. Hu, and N. Zheng, “Salient
    object detection: A discriminative regional feature integration approach,” *Int.
    J. Comput. Vis.*, vol. 123, no. 2, pp. 251–268, 2017.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Q. Yan, L. Xu, J. Shi, and J. Jia, “Hierarchical saliency detection,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013, pp. 1155–1162.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] W. Zhu, S. Liang, Y. Wei, and J. Sun, “Saliency optimization from robust
    background detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2014,
    pp. 2814–2821.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price, and R. Mech, “Unconstrained
    salient object detection via proposal subset optimization,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2016, pp. 5733–5742.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] N. Liu and J. Han, “DHSNet: Deep hierarchical saliency network for salient
    object detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 678–686.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Q. Hou, M.-M. Cheng, X. Hu, A. Borji, Z. Tu, and P. Torr, “Deeply supervised
    salient object detection with short connections,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2017, pp. 3203–3212.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] N. Liu, J. Han, and M.-H. Yang, “Picanet: Learning pixel-wise contextual
    attention for saliency detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, pp. 3089–3098.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Y. Liu, Q. Zhang, D. Zhang, and J. Han, “Employing deep part-object relationships
    for salient object detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019,
    pp. 1232–1241.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Q. Qi, S. Zhao, J. Shen, and K.-M. Lam, “Multi-scale capsule attention-based
    salient object detection with multi-crossed layer connections,” in *IEEE International
    Conference on Multimedia and Expo*, 2019, pp. 1762–1767.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. Borji and L. Itti, “State-of-the-art in visual attention modeling,”
    *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 35, no. 1, pp. 185–207, 2013.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Borji, M.-M. Cheng, H. Jiang, and J. Li, “Salient object detection:
    A benchmark,” *IEEE Trans. Image Process.*, vol. 24, no. 12, pp. 5706–5722, 2015.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] T. V. Nguyen, Q. Zhao, and S. Yan, “Attentive systems: A survey,” *Int.
    J. Comput. Vis.*, vol. 126, no. 1, pp. 86–110, 2018.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] D. Zhang, H. Fu, J. Han, A. Borji, and X. Li, “A review of co-saliency
    detection algorithms: fundamentals, applications, and challenges,” *ACM Trans.
    Intell. Syst. Technol.*, vol. 9, no. 4, p. 38, 2018.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] R. Cong, J. Lei, H. Fu, M.-M. Cheng, W. Lin, and Q. Huang, “Review of
    visual saliency detection with comprehensive information,” *IEEE Trans. Circuits
    Syst. Video Technol.*, 2018.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Han, D. Zhang, G. Cheng, N. Liu, and D. Xu, “Advanced deep-learning
    techniques for salient and category-specific object detection: a survey,” *IEEE
    Signal Processing Magazine*, vol. 35, no. 1, pp. 84–100, 2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] A. Borji, “Saliency prediction in the deep learning era: Successes and
    limitations,” *IEEE Trans. Pattern Anal. Mach. Intell.*, 2019.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] A. Borji, M.-M. Cheng, Q. Hou, H. Jiang, and J. Li, “Salient object detection:
    A survey,” *Computational Visual Media*, pp. 1–34, 2019.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] C. Koch and S. Ullman, “Shifts in selective visual attention: Towards
    the underlying neural circuitry,” *Human neurobiology*, vol. 4, no. 4, p. 219,
    1985.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] L. Itti, C. Koch, and E. Niebur, “A model of saliency-based visual attention
    for rapid scene analysis,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 20,
    no. 11, pp. 1254–1259, 1998.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] H. Jiang, J. Wang, Z. Yuan, Y. Wu, N. Zheng, and S. Li, “Salient object
    detection: A discriminative regional feature integration approach,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2013, pp. 2083–2090.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Y. Wei, F. Wen, W. Zhu, and J. Sun, “Geodesic saliency using background
    priors,” *Proc. Eur. Conf. Comput. Vis.*, pp. 29–42, 2012.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] J. Shi, Q. Yan, L. Xu, and J. Jia, “Hierarchical image saliency detection
    on extended cssd,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 38, no. 4,
    pp. 717–729, 2015.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] C. Yang, L. Zhang, H. Lu, X. Ruan, and M. H. Yang, “Saliency detection
    via graph-based manifold ranking,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2013, pp. 3166–3173.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] W. Wang, J. Shen, L. Shao, and F. Porikli, “Correspondence driven saliency
    transfer,” *IEEE Trans. Image Process.*, vol. 25, no. 11, pp. 5025–5034, 2016.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] F. Guo, W. Wang, J. Shen, L. Shao, J. Yang, D. Tao, and Y. Y. Tang, “Video
    saliency detection using object proposals,” *IEEE Trans. Cybernetics*, 2017.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] G. E. Hinton, A. Krizhevsky, and S. D. Wang, “Transforming auto-encoders,”
    in *Proc. Int. Conf. Artificial Neural Netw.*, 2011, pp. 44–51.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] G. Lee, Y.-W. Tai, and J. Kim, “Deep saliency with encoded low level distance
    map and high level features,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2016, pp. 660–668.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] S. He, R. W. Lau, W. Liu, Z. Huang, and Q. Yang, “Supercnn: A superpixelwise
    convolutional neural network for salient object detection,” *Int. J. Comput. Vis.*,
    vol. 115, no. 3, pp. 330–344, 2015.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] J. Kim and V. Pavlovic, “A shape-based approach for salient object detection
    using deep learning,” in *Proc. Eur. Conf. Comput. Vis.*, 2016, pp. 455–470.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] L. Wang, L. Wang, H. Lu, P. Zhang, and X. Ruan, “Saliency detection with
    recurrent fully convolutional networks,” in *Proc. Eur. Conf. Comput. Vis.*, 2016,
    pp. 825–841.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Kuen, Z. Wang, and G. Wang, “Recurrent attentional networks for saliency
    detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016, pp. 3668–3677.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] P. Hu, B. Shuai, J. Liu, and G. Wang, “Deep level sets for salient object
    detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017, pp. 540–549.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] P. Zhang, D. Wang, H. Lu, H. Wang, and B. Yin, “Learning uncertain convolutional
    features for accurate saliency detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2017, pp. 212–221.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. Zhang, T. Zhang, Y. Dai, M. Harandi, and R. Hartley, “Deep unsupervised
    saliency detection: A multiple noisy labeling perspective,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2018, pp. 9029–9038.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] C. Cao, Y. Hunag, Z. Wang, L. Wang, N. Xu, and T. Tan, “Lateral inhibition-inspired
    convolutional neural network for visual attention and saliency detection,” in
    *AAAI Conference on Artificial Intelligence*, 2018.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] B. Li, Z. Sun, and Y. Guo, “Supervae: Superpixelwise variational autoencoder
    for salient object detection,” in *AAAI Conference on Artificial Intelligence*,
    2019, pp. 8569–8576.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] G. Li, Y. Xie, L. Lin, and Y. Yu, “Instance-level salient object segmentation,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017, pp. 247–256.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] T. Wang, A. Borji, L. Zhang, P. Zhang, and H. Lu, “A stagewise refinement
    model for detecting salient objects in images,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2017, pp. 4039–4048.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] X. Chen, A. Zheng, J. Li, and F. Lu, “Look, perceive and segment: Finding
    the salient objects in images via two-stream fixation-semantic CNNs,” in *Proc.
    IEEE Int. Conf. Comput. Vis.*, 2017, pp. 1050–1058.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Y. Zeng, P. Zhang, J. Zhang, Z. Lin, and H. Lu, “Towards high-resolution
    salient object detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019, pp.
    7234–7243.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Y. Zhuge, Y. Zeng, and H. Lu, “Deep embedding features for salient object
    detection,” in *AAAI Conference on Artificial Intelligence*, vol. 33, 2019, pp.
    9340–9347.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Z. Luo, A. Mishra, A. Achkar, J. Eichel, S. Li, and P.-M. Jodoin, “Non-local
    deep features for salient object detection,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2017, pp. 6593–6601.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] P. Zhang, D. Wang, H. Lu, H. Wang, and X. Ruan, “Amulet: Aggregating multi-level
    convolutional features for salient object detection,” in *Proc. IEEE Int. Conf.
    Comput. Vis.*, 2017, pp. 202–211.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. He, J. Jiao, X. Zhang, G. Han, and R. W. Lau, “Delving into salient
    object subitizing and detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2017,
    pp. 1059–1067.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] X. Hu, L. Zhu, J. Qin, C.-W. Fu, and P.-A. Heng, “Recurrently aggregating
    deep features for salient object detection.” in *AAAI Conference on Artificial
    Intelligence*, 2018.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] M. Amirul Islam, M. Kalash, and N. D. B. Bruce, “Revisiting salient object
    detection: Simultaneous detection, ranking, and subitizing of multiple salient
    objects,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Z. Wu, L. Su, and Q. Huang, “Cascaded partial decoder for fast and accurate
    salient object detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2019, pp. 3907–3916.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Y. Zeng, Y. Zhuge, H. Lu, L. Zhang, M. Qian, and Y. Yu, “Multi-source
    weak supervision for saliency detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2019, pp. 6074–6083.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] J.-X. Zhao, J.-J. Liu, D.-P. Fan, Y. Cao, J. Yang, and M.-M. Cheng, “Egnet:
    Edge guidance network for salient object detection,” in *Proc. IEEE Int. Conf.
    Comput. Vis.*, 2019, pp. 8779–8788.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] D. Zhang, J. Han, and Y. Zhang, “Supervision by fusion: Towards unsupervised
    learning of deep salient object detector,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    vol. 1, no. 2, 2017, p. 3.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] L. Zhang, J. Dai, H. Lu, Y. He, and G. Wang, “A bi-directional message
    passing model for salient object detection,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2018, pp. 1741–1750.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] T. Wang, L. Zhang, S. Wang, H. Lu, G. Yang, X. Ruan, and A. Borji, “Detect
    globally, refine locally: A novel approach to saliency detection,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2018, pp. 3127–3135.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] X. Zhang, T. Wang, J. Qi, H. Lu, and G. Wang, “Progressive attention guided
    recurrent network for salient object detection,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2018, pp. 714–722.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] W. Wang, J. Shen, X. Dong, and A. Borji, “Salient object detection driven
    by fixation prediction,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2018, pp. 1171–1720.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] S. Chen, X. Tan, B. Wang, and X. Hu, “Reverse attention for salient object
    detection,” in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 236–252.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] M. Feng, H. Lu, and E. Ding, “Attentive feedback network for boundary-aware
    salient object detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2019, pp. 1623–1632.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] X. Qin, Z. Zhang, C. Huang, C. Gao, M. Dehghan, and M. Jagersand, “Basnet:
    Boundary-aware salient object detection,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2019, pp. 7479–7489.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] R. Wu, M. Feng, W. Guan, D. Wang, H. Lu, and E. Ding, “A mutual learning
    method for salient object detection with intertwined multi-supervision,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 8150–8159.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] W. Wang, S. Zhao, J. Shen, S. C. Hoi, and A. Borji, “Salient object detection
    with pyramid attention and salient edges,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2019, pp. 1448–1457.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J.-J. Liu, Q. Hou, M.-M. Cheng, J. Feng, and J. Jiang, “A simple pooling-based
    design for real-time salient object detection,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2019, pp. 3917–3926.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] W. Wang, J. Shen, M.-M. Cheng, and L. Shao, “An iterative and cooperative
    top-down and bottom-up inference network for salient object detection,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2019, pp. 5968–5977.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Y. Xu, D. Xu, X. Hong, W. Ouyang, R. Ji, M. Xu, and G. Zhao, “Structured
    modeling of joint deep feature and prediction refinement for salient object detection,”
    in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019, pp. 3789–3798.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] S. S. Kruthiventi, V. Gudisa, J. H. Dholakiya, and R. Venkatesh Babu,
    “Saliency unified: A deep architecture for simultaneous eye fixation prediction
    and salient object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2016, pp. 5781–5790.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] L. Wang, H. Lu, Y. Wang, M. Feng, D. Wang, B. Yin, and X. Ruan, “Learning
    to detect salient objects with image-level supervision,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] G. Li, Y. Xie, and L. Lin, “Weakly supervised salient object detection
    using image labels,” in *AAAI Conference on Artificial Intelligence*, 2018.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] X. Li, F. Yang, H. Cheng, W. Liu, and D. Shen, “Contour knowledge transfer
    for salient object detection,” in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 370–385.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] L. Zhang, J. Zhang, Z. Lin, H. Lu, and Y. He, “Capsal: Leveraging captioning
    to boost semantics for salient object detection,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2019, pp. 6024–6033.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] J. Su, J. Li, Y. Zhang, C. Xia, and Y. Tian, “Selectivity or invariance:
    Boundary-aware salient object detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2019, pp. 3799–3808.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Z. Wu, L. Su, and Q. Huang, “Stacked cross refinement network for edge-aware
    salient object detection,” in *Proc. IEEE Int. Conf. Comput. Vis.*, 2019, pp.
    7264–7273.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Zeng, Y. Zhuge, H. Lu, and L. Zhang, “Joint learning of saliency detection
    and weakly supervised semantic segmentation,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2019, pp. 7223–7233.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] G. Li and Y. Yu, “Deep contrast learning for salient object detection,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016, pp. 478–487.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Y. Tang and X. Wu, “Saliency detection via combining region-level and
    pixel-level predictions with cnns,” in *Proc. Eur. Conf. Comput. Vis.*, 2016,
    pp. 809–825.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] P. Krähenbühl and V. Koltun, “Efficient inference in fully connected
    crfs with gaussian edge potentials,” in *Proc. Advances Neural Inf. Process. Syst.*,
    2011, pp. 109–117.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] M.-M. Cheng, N. J. Mitra, X. Huang, P. H. S. Torr, and S.-M. Hu, “Global
    contrast based salient region detection,” *IEEE Trans. Pattern Anal. Mach. Intell.*,
    vol. 37, no. 3, pp. 569–582, 2015.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Y. Li, X. Hou, C. Koch, J. M. Rehg, and A. L. Yuille, “The secrets of
    salient object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, pp. 280–287.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] R. Ju, Y. Liu, T. Ren, L. Ge, and G. Wu, “Depth-aware salient object
    detection using anisotropic center-surround difference,” *Signal Processing: Image
    Communication*, vol. 38, pp. 115–126, 2015.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] H. Peng, B. Li, W. Xiong, W. Hu, and R. Ji, “Rgbd salient object detection:
    a benchmark and algorithms,” in *Proc. Eur. Conf. Comput. Vis.*, 2014, pp. 92–109.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] M. Jiang, S. Huang, J. Duan, and Q. Zhao, “SALICON: Saliency in context,”
    in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2015, pp. 1072–1080.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] J. Zhang, S. Ma, M. Sameki, S. Sclaroff, M. Betke, Z. Lin, X. Shen, B. Price,
    and R. Mech, “Salient object subitizing,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2015, pp. 4045–4054.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman,
    “The pascal visual object classes (voc) challenge,” *Int. J. Comput. Vis.*, vol. 88,
    no. 2, pp. 303–338, 2010.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2009, pp. 248–255.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Dollár,
    and C. L. Zitnick, “Microsoft coco: Common objects in context,” in *Proc. Eur.
    Conf. Comput. Vis.*, 2014, pp. 740–755.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *Proc. Advances Neural Inf. Process.
    Syst.*, 2012, pp. 1097–1105.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] J. Long, E. Shelhamer, and T. Darrell, “Fully convolutional networks
    for semantic segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2015, pp. 3431–3440.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] S. Xie and Z. Tu, “Holistically-nested edge detection,” in *Proc. IEEE
    Int. Conf. Comput. Vis.*, 2015, pp. 1395–1403.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical Image
    Computing and Computer-Assisted Intervention*, 2015, pp. 234–241.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Yang, B. Price, S. Cohen, H. Lee, and M.-H. Yang, “Object contour
    detection with a fully convolutional encoder-decoder network,” in *Proc. IEEE
    Conf. Comput. Vis. Pattern Recognit.*, 2016, pp. 193–202.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. L. Long, N. Zhang, and T. Darrell, “Do convnets learn correspondence?”
    in *Proc. Advances Neural Inf. Process. Syst.*, 2014, pp. 1601–1609.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba, “Learning
    deep features for discriminative localization,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2016, pp. 2921–2929.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] J. Zhang, S. Sclaroff, Z. Lin, X. Shen, B. Price, and R. Mech, “Minimum
    barrier salient object detection at 80 fps,” in *Proc. IEEE Int. Conf. Comput.
    Vis.*, 2015, pp. 1404–1412.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] J. Zhang and S. Sclaroff, “Exploiting surroundedness for saliency detection:
    a boolean map approach,” *IEEE Trans. Pattern Anal. Mach. Intell.*, no. 5, pp.
    889–902, 2016.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] P. Arbeláez, J. Pont-Tuset, J. T. Barron, F. Marques, and J. Malik, “Multiscale
    combinatorial grouping,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2014, pp. 328–335.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] R. Caruana, “Multitask learning,” *Machine learning*, vol. 28, no. 1,
    pp. 41–75, 1997.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] E. L. Kaufman, M. W. Lord, T. W. Reese, and J. Volkmann, “The discrimination
    of visual number,” *The American Journal of Psychology*, vol. 62, no. 4, pp. 498–525,
    1949.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] S. Alpert, M. Galun, R. Basri, and A. Brandt, “Image segmentation by
    probabilistic bottom-up aggregation and cue integration,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2007, pp. 1–8.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] V. Movahedi and J. H. Elder, “Design and perceptual validation of performance
    measures for salient object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit. - Workshops*, 2010.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] C. Xia, J. Li, X. Chen, A. Zheng, and Y. Zhang, “What is and what is
    not a salient object? learning salient object detector by ensembling linear exemplar
    regressors,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2017, pp. 4321–4329.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] D.-P. Fan, M.-M. Cheng, J.-J. Liu, S.-H. Gao, Q. Hou, and A. Borji, “Salient
    objects in clutter: Bringing salient object detection to the foreground,” in *The
    Proc. Eur. Conf. Comput. Vis.*, 2018.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] R. Fan, Q. Hou, M.-M. Cheng, G. Yu, R. R. Martin, and S.-M. Hu, “Associating
    inter-image salient instances for weakly supervised semantic segmentation,” in
    *The Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 367–383.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Zhao, J. Li, H. Liu, S. Yan, and J. Feng, “Fine-grained multi-human
    parsing,” *Int. J. Comput. Vis.*, pp. 1–19, 2019.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] D. Martin, C. Fowlkes, D. Tal, and J. Malik, “A database of human segmented
    natural images and its application to evaluating segmentation algorithms and measuring
    ecological statistics,” in *Proc. IEEE Int. Conf. Comput. Vis.*, vol. 2, 2001,
    pp. 416–423.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] J. Xiao, J. Hays, K. A. Ehinger, A. Oliva, and A. Torralba, “Sun database:
    Large-scale scene recognition from abbey to zoo,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2010, pp. 3485–3492.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Z. Wang and B. Li, “A two-stage approach to saliency detection in images,”
    in *Proc. IEEE Conf. Acoust. Speech Signal Process.*, 2008, pp. 965–968.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] R. Margolin, L. Zelnik-Manor, and A. Tal, “How to evaluate foreground
    maps?” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2014, pp. 248–255.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] D.-P. Fan, M.-M. Cheng, Y. Liu, T. Li, and A. Borji, “Structure-measure:
    A new way to evaluate foreground maps,” in *Proc. IEEE Int. Conf. Comput. Vis.*,
    2017.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] D.-P. Fan, M.-M. Cheng, J.-J. Liu, S.-H. Gao, Q. Hou, and A. Borji, “Enhanced-alignment
    measure for binary foreground map evaluation,” in *International Joint Conferences
    on Artificial Intelligence*, 2018.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] F. Perazzi, J. Pont-Tuset, B. McWilliams, L. Van Gool, M. Gross, and
    A. Sorkine-Hornung, “A benchmark dataset and evaluation methodology for video
    object segmentation,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2016,
    pp. 724–732.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” in *Proc. Int. Conf.
    Learn. Representations*, 2014.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] C. Li, R. Cong, J. Hou, S. Zhang, Y. Qian, and S. Kwong, “Nested network
    with two-stream pyramid for salient object detection in optical remote sensing
    images,” *IEEE Trans. Geosci. Remote Sens.*, vol. 57, no. 11, pp. 9156–9166, 2019.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] I. Mehmood, M. Sajjad, W. Ejaz, and S. W. Baik, “Saliency-directed prioritization
    of visual data in wireless surveillance networks,” *Information Fusion*, vol. 24,
    pp. 16–30, 2015.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Z. Zhang, S. Fidler, and R. Urtasun, “Instance-level segmentation for
    autonomous driving with deep densely connected mrfs,” in *Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit.*, 2016, pp. 669–677.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] C. Guo and L. Zhang, “A novel multiresolution spatiotemporal saliency
    detection model and its applications in image and video compression,” *IEEE Trans.
    Image Process.*, vol. 19, no. 1, pp. 185–198, 2009.'
  id: totrans-677
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] C. Xie, J. Wang, Z. Zhang, Y. Zhou, L. Xie, and A. Yuille, “Adversarial
    examples for semantic segmentation and object detection,” in *Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit.*, 2017, pp. 1369–1378.'
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] A. Fawzi, S.-M. Moosavi-Dezfooli, and P. Frossard, “Robustness of classifiers:
    from adversarial to random noise,” in *Proc. Advances Neural Inf. Process. Syst.*,
    2016, pp. 1632–1640.'
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] N. Papernot, P. McDaniel, and I. Goodfellow, “Transferability in machine
    learning: from phenomena to black-box attacks using adversarial samples,” *arXiv
    preprint arXiv:1605.07277*, 2016.'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Y. Liu, X. Chen, C. Liu, and D. Song, “Delving into transferable adversarial
    examples and black-box attacks,” in *Proc. Int. Conf. Learn. Representations*,
    2017.'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] A. Torralba and A. A. Efros, “Unbiased look at dataset bias,” in *Proc.
    IEEE Conf. Comput. Vis. Pattern Recognit.*, 2011, pp. 1521–1528.'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *Proc. Int. Conf. Learn. Representations*, 2015.'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] B. Zoph and Q. V. Le, “Neural architecture search with reinforcement
    learning,” in *Proc. Int. Conf. Learn. Representations*, 2017.'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] M. Berman, A. Rannen Triki, and M. B. Blaschko, “The lovász-softmax loss:
    A tractable surrogate for the optimization of the intersection-over-union measure
    in neural networks,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*, 2018,
    pp. 4413–4421.'
  id: totrans-685
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] T.-W. Ke, J.-J. Hwang, Z. Liu, and S. X. Yu, “Adaptive affinity fields
    for semantic segmentation,” in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 587–602.'
  id: totrans-686
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] E. Bengio, P.-L. Bacon, J. Pineau, and D. Precup, “Conditional computation
    in neural networks for faster models,” in *Proc. Int. Conf. Learn. Representations*,
    2016.'
  id: totrans-687
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] A. Veit and S. Belongie, “Convolutional networks with adaptive inference
    graphs,” in *Proc. Eur. Conf. Comput. Vis.*, 2018.'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] A. Zlateski, R. Jaroensri, P. Sharma, and F. Durand, “On the importance
    of label quality for semantic segmentation,” in *Proc. IEEE Conf. Comput. Vis.
    Pattern Recognit.*, 2018.'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] M. Jiang, J. Xu, and Q. Zhao, “Saliency in crowd,” in *Proc. Eur. Conf.
    Comput. Vis.*, 2014, pp. 17–32.'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Q. Zheng, J. Jiao, Y. Cao, and R. W. Lau, “Task-driven webpage saliency,”
    in *Proc. Eur. Conf. Comput. Vis.*, 2018, pp. 287–302.'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Palazzi, F. Solera, S. Calderara, S. Alletto, and R. Cucchiara, “Learning
    where to attend like a human driver,” in *IEEE Intelligent Vehicles Symposium*,
    2017, pp. 920–925.'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] A. K. Mishra, Y. Aloimonos, L. F. Cheong, and A. Kassim, “Active visual
    segmentation,” *IEEE Trans. Pattern Anal. Mach. Intell.*, vol. 34, no. 4, pp.
    639–653, 2012.'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] C. M. Masciocchi, S. Mihalas, D. Parkhurst, and E. Niebur, “Everyone
    knows what is interesting: Salient locations which should be fixated,” *Journal
    of Vision*, vol. 9, no. 11, pp. 25–25, 2009.'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] A. Borji, “What is a salient object? A dataset and a baseline model for
    salient object detection,” *IEEE Trans. Image Process.*, vol. 24, no. 2, pp. 742–756,
    2015.'
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] L. Jing and Y. Tian, “Self-supervised visual feature learning with deep
    neural networks: A survey,” *IEEE Trans. Pattern Anal. Mach. Intell.*, 2020.'
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] D. Pathak, P. Krahenbuhl, J. Donahue, T. Darrell, and A. A. Efros, “Context
    encoders: Feature learning by inpainting,” in *Proc. IEEE Conf. Comput. Vis. Pattern
    Recognit.*, 2016, pp. 2536–2544.'
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] G. Larsson, M. Maire, and G. Shakhnarovich, “Colorization as a proxy
    task for visual understanding,” in *Proc. IEEE Conf. Comput. Vis. Pattern Recognit.*,
    2017, pp. 6874–6883.'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] M. Caron, P. Bojanowski, A. Joulin, and M. Douze, “Deep clustering for
    unsupervised learning of visual features,” in *Proc. Eur. Conf. Comput. Vis.*,
    2018, pp. 132–149.'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] C. Bucilua, R. Caruana, and A. Niculescu-Mizil, “Model compression,”
    in *Proceedings of SIGKDD international conference on Knowledge discovery and
    data mining*, 2006, pp. 535–541.'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” in *Proc. Advances Neural Inf. Process. Syst. - workshops*, 2014.'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] A. Romero, N. Ballas, S. E. Kahou, A. Chassang, C. Gatta, and Y. Bengio,
    “Fitnets: Hints for thin deep nets,” *arXiv preprint arXiv:1412.6550*, 2014.'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] G. Chen, W. Choi, X. Yu, T. Han, and M. Chandraker, “Learning efficient
    object detection models with knowledge distillation,” in *Proc. Advances Neural
    Inf. Process. Syst.*, 2017, pp. 742–751.'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| Wenguan Wang received his Ph.D. degree from Beijing Institute of Technology
    in 2018\. He is currently a postdoc scholar at ETH Zurich, Switzerland. From 2016
    to 2018, he was a visiting Ph.D. student in University of California, Los Angeles.
    From 2018 to 2019, he was a senior scientist at Inception Institute of Artificial
    Intelligence, UAE. His current research interests include computer vision and
    deep learning. |'
  id: totrans-704
  prefs: []
  type: TYPE_TB
- en: '| Qiuxia Lai received the B.E. and M.S. degrees in the School of Automation
    from Huazhong University of Science and Technology in 2013 and 2016, respectively.
    She is currently pursuing the Ph.D. degree in The Chinese University of Hong Kong.
    Her research interests include image/video processing and deep learning. |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
- en: '| Huazhu Fu (SM’18) received the Ph.D. degree from Tianjin University, China,
    in 2013\. He was a Research Fellow with Nanyang Technological University, Singapore
    for two years. From 2015 to 2018, he was a Research Scientist with the Institute
    for Infocomm Research, Agency for Science, Technology and Research, Singapore.
    He is currently a Senior Scientist with Inception Institute of Artificial Intelligence,
    UAE. His research interests include computer vision and medical image analysis.
    He is an Associate Editor of IEEE TMI and IEEE Access. |'
  id: totrans-706
  prefs: []
  type: TYPE_TB
- en: '| Jianbing Shen (M’11-SM’12) is a Professor with the School of Computer Science,
    Beijing Institute of Technology. He has published about 100 journal and conference
    papers such as TPAMI, CVPR, and ICCV. He has obtained many honors including the
    Fok Ying Tung Education Foundation from Ministry of Education, the Program for
    Beijing Excellent Youth Talents from Beijing Municipal Education Commission, and
    the Program for New Century Excellent Talents from Ministry of Education. His
    research interests include computer vision and deep learning. He is an Associate
    Editor of IEEE TNNLS, IEEE TIP and Neurocomputing. |'
  id: totrans-707
  prefs: []
  type: TYPE_TB
- en: '| Haibin Ling received the PhD degree from University of Maryland in 2006\.
    From 2000 to 2001, he was an assistant researcher at Microsoft Research Asia.
    From 2006 to 2007, he worked as a postdoc at University of California Los Angeles.
    After that, he joined Siemens Corporate Research as a research scientist. Since
    2008, he has been with Temple University where he is now an Associate Professor.
    He received the Best Student Paper Award at the ACM UIST in 2003, and the NSF
    CAREER Award in 2014\. He is an Associate Editor of IEEE TPAMI, PR, and CVIU,
    and served as Area Chairs for CVPR 2014, 2016 and 2019. |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
- en: '| Ruigang Yang is currently a full professor of Computer Science at the University
    of Kentucky. His research interests span over computer vision and computer graphics,
    in particular in 3D reconstruction and 3D data analysis. He has received a number
    of awards, including the US National Science Foundation Faculty Early Career Development
    (CAREER) Program Award in 2004, and the best Demonstration Award at CVPR 2007.
    He is currently an associate editor of IEEE TPAMI. |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
