- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:34:46'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2401.15167] The Zwicky Transient Facility Bright Transient Survey. III. BTSbot:
    Automated Identification and Follow-up of Bright Transients with Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2401.15167](https://ar5iv.labs.arxiv.org/html/2401.15167)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[Nabeel Rehemtulla](https://orcid.org/0000-0002-5683-2389) Department of Physics
    and Astronomy, Northwestern University, 2145 Sheridan Road, Evanston, IL 60208,
    USA Center for Interdisciplinary Exploration and Research in Astrophysics (CIERA),
    1800 Sherman Ave., Evanston, IL 60201, USA [Adam A. Miller](https://orcid.org/0000-0001-9515-478X)
    Department of Physics and Astronomy, Northwestern University, 2145 Sheridan Road,
    Evanston, IL 60208, USA Center for Interdisciplinary Exploration and Research
    in Astrophysics (CIERA), 1800 Sherman Ave., Evanston, IL 60201, USA [Theophile
    Jegou Du Laz](https://orcid.org/0009-0003-6181-4526) Division of Physics, Mathematics,
    and Astronomy 249-17, California Institute of Technology, Pasadena, CA 91125,
    USA [Michael W. Coughlin](https://orcid.org/0000-0002-8262-2924) School of Physics
    and Astronomy, University of Minnesota, Minneapolis, Minnesota 55455, USA [Christoffer
    Fremling](https://orcid.org/0000-0002-4223-103X) Division of Physics, Mathematics,
    and Astronomy 249-17, California Institute of Technology, Pasadena, CA 91125,
    USA Caltech Optical Observatories, California Institute of Technology, Pasadena,
    CA 91125, USA [Daniel A. Perley](https://orcid.org/0000-0001-8472-1996) Astrophysics
    Research Institute, Liverpool John Moores University, IC2, Liverpool Science Park,
    146 Brownlow Hill, Liverpool L3 5RF, UK [Yu-Jing Qin](https://orcid.org/0000-0003-3658-6026)
    Division of Physics, Mathematics, and Astronomy 249-17, California Institute of
    Technology, Pasadena, CA 91125, USA [Jesper Sollerman](https://orcid.org/0000-0003-1546-6615)
    Department of Astronomy, The Oskar Klein Center, Stockholm University, AlbaNova,
    SE-10691 Stockholm, Sweden [Ashish A. Mahabal](https://orcid.org/0000-0003-2242-0244)
    Division of Physics, Mathematics, and Astronomy 249-17, California Institute of
    Technology, Pasadena, CA 91125, USA Center for Data Driven Discovery, California
    Institute of Technology, Pasadena, CA 91125, USA [Russ R. Laher](https://orcid.org/0000-0003-2451-5482)
    IPAC, California Institute of Technology, 1200 E. California Blvd, Pasadena, CA
    91125, USA [Reed Riddle](https://orcid.org/0000-0002-0387-370X) Caltech Optical
    Observatories, California Institute of Technology, Pasadena, CA 91125, USA [Ben
    Rusholme](https://orcid.org/0000-0001-7648-4142) IPAC, California Institute of
    Technology, 1200 E. California Blvd, Pasadena, CA 91125, USA [Shrinivas R. Kulkarni](https://orcid.org/0000-0001-5390-8563)
    Division of Physics, Mathematics, and Astronomy 249-17, California Institute of
    Technology, Pasadena, CA 91125, USA'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The Bright Transient Survey (BTS) aims to obtain a classification spectrum for
    all bright ($m_{\mathrm{peak}}\,\leq\,18.5$ mag) extragalactic transients found
    in the Zwicky Transient Facility (ZTF) public survey. BTS critically relies on
    visual inspection (“scanning”) to select targets for spectroscopic follow-up,
    which, while effective, has required a significant time investment over the past
    $\sim$5 yr of ZTF operations. We present BTSbot, a multi-modal convolutional neural
    network, which provides a bright transient score to individual ZTF detections
    using their image data and 25 extracted features. BTSbot is able to eliminate
    the need for daily human scanning by automatically identifying and requesting
    spectroscopic follow-up observations of new bright transient candidates. BTSbot recovers
    all bright transients in our test split and performs on par with scanners in terms
    of identification speed (on average, $\sim$1 hour quicker than scanners). We also
    find that BTSbot is not significantly impacted by any data shift by comparing
    performance across a concealed test split and a sample of very recent BTS candidates.
    BTSbot has been integrated into Fritz and Kowalski, ZTF’s first-party marshal
    and alert broker, and now sends automatic spectroscopic follow-up requests for
    the new transients it identifies. During the month of October 2023, BTSbot selected
    296 sources in real-time, 93% of which were real extragalactic transients. With
    BTSbot and other automation tools, the BTS workflow has produced the first fully
    automatic end-to-end discovery and classification of a transient, representing
    a significant reduction in the human-time needed to scan. Future development has
    tremendous potential for creating similar models to identify and request follow-up
    observations for specific types of transients.
  prefs: []
  type: TYPE_NORMAL
- en: 'Time domain astronomy (2109) — Sky surveys (1464) — Supernovae (1668) — Convolutional
    neural networks (1938)^†^†facilities: PO:1.2m, PO:1.5m^†^†software: Astropy (Astropy
    Collaboration et al., [2013](#bib.bib2), [2018](#bib.bib3)), corner (Foreman-Mackey,
    [2016](#bib.bib30)), Jupyter (Kluyver et al., [2016](#bib.bib55)), Keras (Chollet
    et al., [2015](#bib.bib18)), Matplotlib (Hunter, [2007](#bib.bib45)), NumPy (Harris
    et al., [2020](#bib.bib41)), pandas (pandas development team, [2020](#bib.bib67);
    Wes McKinney, [2010](#bib.bib101)), penquins (Duev et al., [2021](#bib.bib26)),
    scikit-learn (Pedregosa et al., [2011](#bib.bib70)), SciPy (Virtanen et al., [2020](#bib.bib98)),
    SkyPortal (van der Walt et al., [2019](#bib.bib94); Coughlin et al., [2023](#bib.bib21)),
    Tensorflow (Abadi et al., [2015](#bib.bib1)), tqdm (da Costa-Luis, [2019](#bib.bib22)),
    and the Weights and Biases platform (Biewald, [2020](#bib.bib7))'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Large, wide-field surveys like Pan-STARRS (Panoramic Survey Telescope and Rapid
    Response System; Kaiser et al., [2002](#bib.bib50)), ASAS-SN (All-Sky Automated
    Survey for Supernovae; Shappee et al., [2014](#bib.bib80)), ATLAS (Asteroid Terrestrial
    Last-Alert System; Tonry, [2011](#bib.bib90); Tonry et al., [2018](#bib.bib91);
    Smith et al., [2020](#bib.bib84)), and ZTF (Zwicky Transient Facility; Bellm et al.,
    [2019a](#bib.bib5), [b](#bib.bib6); Graham et al., [2019](#bib.bib40); Dekany
    et al., [2020](#bib.bib23); Masci et al., [2019](#bib.bib59)) have made immense
    contributions to time-domain astronomy by repeatedly imaging the entire night
    sky. Some surveys interface with the community through the output of an alert
    stream. Alert packets comprising the stream are intended to notify the community
    of the statistically significant brightening or dimming of some source with respect
    to a historical reference image. The alert stream provides time-series for a wide
    range of astrophysical, and non-astrophysical, phenomena. We are interested in
    the study of supernovae (SNe) using these alert streams, although only a small
    fraction of alerts in the unfiltered stream originate from genuine SNe. Thus,
    alert filters are deployed to identify candidate sources of interest. It is practically
    impossible to design an alert filter that rejects every irrelevant alert and accepts
    all alerts related to sources of interest. Therefore, manual candidate vetting,
    or “scanning,” of the filtered alerts is required. The term scanning can refer
    to different actions depending on the relevant survey and science case, but, in
    general, scanning has been used extensively and very successfully in the SN community
    to identify SNe for follow-up observations and further study.
  prefs: []
  type: TYPE_NORMAL
- en: The nightly data rate of the Vera C. Rubin Observatory’s Legacy Survey of Space
    and Time (LSST; Ivezić et al., [2019](#bib.bib48)) will significantly surpass
    our collective capacity for human scanning. Previous and ongoing surveys employ
    machine learning (ML) techniques for real/bogus classification (e.g., Bailey et al.,
    [2007](#bib.bib4); Bloom et al., [2012](#bib.bib9); Brink et al., [2013](#bib.bib13);
    Wright et al., [2015](#bib.bib102); Goldstein et al., [2015](#bib.bib36); Cabrera-Vives
    et al., [2017](#bib.bib15); Mahabal et al., [2019](#bib.bib58); Duev et al., [2019](#bib.bib28);
    Turpin et al., [2020](#bib.bib93); Killestein et al., [2021](#bib.bib52)), and
    adopting ML will be near-compulsory to efficiently extract knowledge from the
    next generation of surveys.
  prefs: []
  type: TYPE_NORMAL
- en: Beyond real/bogus classification, ML models have also been applied to a variety
    of tasks in astronomy including photometric transient classification (e.g., Boone,
    [2019](#bib.bib10); Muthukrishna et al., [2019](#bib.bib64); Hosseinzadeh et al.,
    [2020](#bib.bib44); Villar et al., [2019](#bib.bib96), [2020](#bib.bib97); Gagliano
    et al., [2023](#bib.bib35)), photometric redshift estimation (e.g., Carrasco Kind
    & Brunner, [2013](#bib.bib17); Sadeh et al., [2016](#bib.bib79); Pasquet et al.,
    [2019](#bib.bib68)), and many others. Some have also been developed to, in part,
    reduce scanning load by encoding phenomenological or taxonomic information of
    a source in their model’s output (e.g., Bailey et al., [2007](#bib.bib4); Duev
    & van der Walt, [2021](#bib.bib27); Carrasco-Davis et al., [2021](#bib.bib16);
    Gomez et al., [2020](#bib.bib37), [2023](#bib.bib38); Stein et al., [2023](#bib.bib86)).
  prefs: []
  type: TYPE_NORMAL
- en: For the most part, ML models in astronomy perform their tasks using extracted
    numeric features with architectures like random forests (Breiman, [2001](#bib.bib12)),
    or fully-connected neural networks (McCulloch & Pitts, [1943](#bib.bib60); LeCun
    et al., [2015](#bib.bib57)). While appropriate in some cases, limiting these models
    to extracted features alone ignores potentially valuable information present in
    the images from which the features are extracted. A comparatively small number
    of convolutional neural networks (CNNs; Fukushima & Miyake, [1982](#bib.bib34))
    have been built that make use of the information embedded in astronomical images,
    and they have generally had great success (e.g., Dieleman et al., [2015](#bib.bib24);
    Lanusse et al., [2018](#bib.bib56); Domínguez Sánchez et al., [2018](#bib.bib25);
    Duev et al., [2019](#bib.bib28); Walmsley et al., [2020](#bib.bib100)). CNNs are
    particularly well suited to astronomy because they can capture properties, like
    galaxy morphology, which often remain largely obscured to other image processing
    techniques (Walmsley et al., [2019](#bib.bib99)). Only a very small subset of
    these CNNs are multi-modal, meaning they take in images and input of another type,
    like extracted features or a light curve (e.g., van Roestel et al., [2021](#bib.bib95);
    Duev & van der Walt, [2021](#bib.bib27); Carrasco-Davis et al., [2021](#bib.bib16);
    Morgan et al., [2022](#bib.bib62), [2023](#bib.bib63); Stoppa et al., [2023](#bib.bib87)).
    We have developed a multi-modal CNN (MM-CNN) to automate scanning for a filtered
    stream from ZTF.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Bright Transient Survey (BTS; Fremling et al., [2020](#bib.bib32); Perley
    et al., [2020](#bib.bib72)) aims to spectroscopically classify all bright ($m_{\mathrm{peak}}\leq
    18.5\,\mathrm{mag}$ in $g$- or $r$-band) extragalactic transients¹¹1Henceforth,
    we use “supernova” and “transient” interchangeably with “extragalactic transient.”
    These are not equivalent, but it simplifies the prose. from the ZTF public alert
    stream (Patterson et al., [2019](#bib.bib69)). The ZTF public survey produces
    $>10^{5}$ alert packets per night (Mahabal et al., [2019](#bib.bib58)). The majority
    of these are from variable sources or are bogus alerts, those arising from non-astrophysical
    phenomena. The BTS alert filter (Perley et al., [2020](#bib.bib72)) removes from
    consideration most bogus alerts and many alerts from asteroids, active galactic
    nuclei (AGN), cataclysmic variables (CVs), variable stars (VarStars), and alerts
    from sources with $m>19.0\,\mathrm{mag}$ in $g$- and $r$-band. This filter makes
    use of braai (Duev et al., [2019](#bib.bib28)) and sgscore (Tachibana & Miller,
    [2018](#bib.bib89)) to filter the stream down to $\sim$50 new candidate BTS sources
    per night, of which $\sim$7 are new real bright transients. The other BTS candidates
    are mostly dim ($m_{\mathrm{peak}}>18.5\,\mathrm{mag}$) SNe or AGN, CVs, and VarStars
    that were not removed by the alert filter. All nightly BTS candidates are scanned
    by experts (“scanners”) who catalog (“save”) the real bright transients and request
    spectroscopic observations for classification. Scanning is performed on Fritz,²²2[https://github.com/fritz-marshal/fritz](https://github.com/fritz-marshal/fritz)
    ZTF’s first party marshal and a SkyPortal instance (van der Walt et al., [2019](#bib.bib94);
    Coughlin et al., [2023](#bib.bib21)). BTS primarily executes its follow-up observations
    with robotic spectrographs like the SED machine (SEDM; Blagorodnova et al., [2018](#bib.bib8);
    Rigault et al., [2019](#bib.bib77); Kim et al., [2022](#bib.bib53)), and, soon,
    the SED machine Kitt Peak³³3[https://sites.astro.caltech.edu/sedmkp/](https://sites.astro.caltech.edu/sedmkp/)
    (SEDM-KP). The resulting classifications, whether assigned automatically by SNIascore
    (Fremling et al., [2021](#bib.bib33)) or manually by a scanner, are promptly reported
    to the public via the Transient Name Server⁴⁴4[https://www.wis-tns.org](https://www.wis-tns.org)
    (TNS), as visualized in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ The Zwicky
    Transient Facility Bright Transient Survey. III. BTSbot: Automated Identification
    and Follow-up of Bright Transients with Deep Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/230a6140ce4587c3690b77e7e473533f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Diagram of the BTS workflow. The BTS alert filter ingests the ZTF
    public survey alert stream and, using upstream tools like braai and sgscore, removes
    bogus alerts, dim sources, and sources that are trivially not bright transients.
    Selection of real bright transients from the pool of candidates is done by visual
    inspection (“scanning”); in this study we develop an ML-based alternative: BTSbot.
    The selected bright transients receive spectroscopic follow-up and classification,
    and are promptly reported to the public.'
  prefs: []
  type: TYPE_NORMAL
- en: The BTS catalog is available online⁵⁵5[https://sites.astro.caltech.edu/ztf/bts](https://sites.astro.caltech.edu/ztf/bts)
    and is updated in real-time. Since its origin in May 2018, BTS has maintained
    exceptionally high spectroscopic completeness of relevant sources ($95.4\%$⁶⁶6Computed
    for sources passing the BTS alert filter with $m_{\mathrm{peak}}\leq 18.5\,\mathrm{mag}$,
    good light curve coverage, and displaying a SN-like light curve or a galaxy cross-match
    from May 2018 to September 2023\. We adopt this value for the BTS sample completeness
    henceforth. See Perley et al. ([2020](#bib.bib72)) for detailed definition of
    sample cuts.), and BTS serves the community by rapidly releasing their classifications
    to the public. As of October 2023, the BTS sample contains more than 8,300 publicly
    classified SNe.
  prefs: []
  type: TYPE_NORMAL
- en: BTS enables a large amount of science, notably including some of the largest
    SN population studies conducted to date (e.g., Perley et al., [2020](#bib.bib72);
    Irani et al., [2022](#bib.bib46); Sharon & Kushnir, [2022](#bib.bib82); Sollerman
    et al., [2022](#bib.bib85); Rodríguez et al., [2023](#bib.bib78); Cold & Hjorth,
    [2023](#bib.bib19); Sharma et al., [2023](#bib.bib81)). The survey also provides
    unique discoveries (e.g., Goobar et al., [2023](#bib.bib39); Yang et al., [2021](#bib.bib103))
    and is paving the way for using SNe to study large scale structure (Tsaprazi et al.,
    [2022](#bib.bib92)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Our new model, BTSbot, enables the automation of scanning and spectroscopic
    follow-up for BTS by performing binary classification: bright transient / not
    bright transient. BTSbot produces a unit-interval bright transient score for an
    input ZTF Avro⁷⁷7[https://avro.apache.org](https://avro.apache.org) alert packet
    (Masci et al., [2019](#bib.bib59); Patterson et al., [2019](#bib.bib69)) augmented
    with some custom metadata features. It is trained to run only on alerts from sources
    with at least one alert passing the BTS alert filter (candidate BTS sources),
    the same alerts which are provided to scanners. From image cutouts and extracted
    features, BTSbot must simultaneously separate transients from variable sources
    and, while $m>18.5\,\mathrm{mag}$, predict whether or not the source will attain
    $m_{\mathrm{peak}}\leq 18.5\,\mathrm{mag}$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'BTSbot is now integrated into Kowalski (Duev et al., [2019](#bib.bib28); Coughlin
    et al., [2023](#bib.bib21)), ZTF’s first-party alert broker, and Fritz, where
    BTSbot has now entered the BTS workflow. BTSbot is able to automatically save
    sources to BTS catalogs and conditionally trigger SEDM/SEDM-KP. BTSbot joins a
    rich collection of ML models and automation tools central to daily BTS operations,
    namely braai (Duev et al., [2019](#bib.bib28)), sgscore (Tachibana & Miller, [2018](#bib.bib89)),
    pySEDM (Rigault et al., [2019](#bib.bib77)), and SNIascore (Fremling et al., [2021](#bib.bib33)).
    Together, this workflow has yielded the first transient to be fully automatically
    detected, identified, spectroscopically classified, and publicly reported: SN 2023tyk
    (Rehemtulla et al., [2023b](#bib.bib75)). Zero human action was involved from
    the first detection to the publicly reported spectroscopic classification. Combining
    complementary ML models allows for the automation of a significant fraction of
    the tasks necessary to maintain BTS. This has the side-effect of freeing up time
    otherwise spent on repetitive, well-understood tasks, allowing for the better
    allocation of expert time and resources. We make the BTSbot source code and trained
    model publicly available on GitHub ([https://github.com/nabeelre/BTSbot](https://github.com/nabeelre/BTSbot)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Training data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The quality of an ML model’s training set is a key factor in determining its
    performance. ML models, especially deep learning models like BTSbot, are known
    to behave unpredictably when exposed to data unlike what they were trained on
    (Szegedy et al., [2013](#bib.bib88); Hendrycks & Gimpel, [2016](#bib.bib42)).
    Thus, a model’s training set must be fully representative of the model’s input
    domain. BTSbot’s domain is ZTF alert packets from BTS candidates. These alerts
    come from SNe, AGN, CVs, VarStars, novae in very nearby systems, and a small number
    of other miscellaneous events including bogus alerts of many types (e.g., those
    due to poor image subtractions, high proper motion stars, saturated stars, etc.).
    We compile an extensive list of ZTF identifiers (ZTF-IDs) for sources that fall
    into these categories by drawing from a number of repositories. With these selections,
    we fully represent the wide variety of astrophysical phenomena BTSbot is exposed
    to in production.
  prefs: []
  type: TYPE_NORMAL
- en: An internal version of the BTS Sample Explorer is the most significant contributor
    to our initial list of ZTF-IDs. This internal BTS Sample Explorer operates identically
    to the public-facing version presented in Appendix F of Perley et al. [2020](#bib.bib72),
    but it adds information from internal ZTF catalogs. One of these catalogs is assembled
    by scanning an alert filter nearly identical to the BTS alert filter but altered
    to include sources with $19<m_{\mathrm{peak}}\,[\mathrm{mag}]<19.8$. Although
    these are not strictly BTS candidates, we allow them into the training set to
    increase the number of faint alerts.
  prefs: []
  type: TYPE_NORMAL
- en: We draw from these catalogs with three queries. The first query, “trues” for
    short, selects bright ($m_{\mathrm{peak}}\leq 18.5\,\mathrm{mag}$) spectroscopically
    confirmed extragalactic transients that pass the purity cut (i.e. have a galaxy
    cross-match or a SN-like light curve, see Sec. 2.4 of Perley et al. [2020](#bib.bib72)
    for details). These bright confirmed transients make up the entire positive/true
    class of our training set. The second query, “vars” for short, selects any source
    classified as an AGN, CV, or quasar (QSO) in any of the internal BTS catalogs.
    These sources are all considered non-transients or are not extragalactic and thus
    are part of the negative/false class regardless of their peak magnitude. The third
    query, “dims” for short, selects any source that passes the purity cut with pre-
    and post-peak light curve coverage (see Sec. 2.3 of Perley et al. [2020](#bib.bib72)
    for coverage definitions) and $m_{\mathrm{peak}}>18.5\,\mathrm{mag}$. This is
    designed to broadly select dim SNe, which, because of their peak magnitudes, are
    part of the negative class. The dims query is dominated by SNe but also includes
    a small number of other sources, e.g., tidal disruption events, CVs, asteroids,
    VarStars, etc. The requirement of pre- and post-peak light curve coverage is added
    to avoid selecting bright SNe with poor photometric coverage near peak, which
    could then appear as having dimmer peak magnitudes. Because peak magnitudes presented
    on the BTS sample explorer are computed from public data alone, there is a small
    population of sources which, due to alerts from partnership data, have $m_{\mathrm{peak}}\leq
    18.5\,\mathrm{mag}$ but appear in dims. These are all removed entirely from the
    training set because many of them are unclassified.⁸⁸8These were identified after
    the test split was revealed so, to preserve the same splits, they are removed
    after train/validation/test splitting is performed. By definition, neither vars
    nor dims overlap with trues, but the imperfect nature of the selections done for
    vars and dims creates some overlap between the two. Sources in the overlap are
    removed from dims and kept only in vars.
  prefs: []
  type: TYPE_NORMAL
- en: Any source queried from the BTS Sample Explorer was once marked as a bright
    transient by a human scanner. While we do build a sizeable list of non-bright transients
    saved by the human scanners, there is a missing population of BTS candidates that
    are never saved. Our fourth query, “rejects” for short, represents these sources.
    This list is limited to sources with alerts that pass the BTS alert filter between
    January 1st 2021 and January 1st 2023 UTC⁹⁹9All dates and times here are in UTC
    unless otherwise specified.. Many candidates from before January 1st 2021 encountered
    earlier versions of the BTS alert filter, which was last improved in late 2020;
    many candidates from after January 1st 2023 were still evolving at the time the
    sources were queried and thus have potentially uncertain types. By not saving
    them, scanners implicitly mark these sources as non-bright transients and thus
    part of our negative class. This assumption is reasonable given BTS’s very high
    photometric completeness. An extremely small fraction of the sources in the rejects
    list may be bright transients and inject a small amount of label noise into our
    training set, but we find that this effect, if present at all, is small enough
    to be ignored.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is an additional population of BTS candidates, which we do not include
    in our training set: “junk” for short. These sources, which frequently pass the
    BTS alert filter, have been identified by scanners to clearly not be bright transients.
    They are mostly AGN and VarStars but also include smaller numbers of sources like
    high-proper motion stars and long-lived CVs. Experiments trained on these sources
    frequently yielded worse performance than equivalent models which excluded them.
    Some sources selected by our other queries are also cataloged in junk; these sources
    remain in the other query and are not removed with other junk sources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once we have a list of ZTF-IDs and their corresponding labels, we query Kowalski
    to retrieve all alert packets from each source. BTS only uses data from the ZTF
    public survey, but we include data from partnership time to increase the size
    of our training set. We arrange the science, reference, and difference image cutouts
    of each alert packet to form a $63\times 63\times 3$ image, or a triplet. Image
    cutouts are individually normalized with Euclidean-normalization^(10)^(10)10Also
    called $L^{2}$-normalization.; pixels in masked regions are uniformly given values
    of 0; cutouts smaller than $63\times 63$ (due to being near the edge of a CCD)
    are padded to $63\times 63$ with pixels of value $10^{-9}$. This image pre-processing
    is identical to braai’s (Duev et al., [2019](#bib.bib28)). Some cutouts have pixel
    values uniformly set to NaN or exactly zero. These cutouts are corrupted, and
    alerts with any corrupted cutouts are removed. The distribution of alerts and
    sources in the training set at this stage is shown in Table [1](#S2.T1 "Table
    1 ‣ 2 Training data ‣ The Zwicky Transient Facility Bright Transient Survey. III.
    BTSbot: Automated Identification and Follow-up of Bright Transients with Deep
    Learning") under the initial queries header.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Training set size before/after cleaning cuts'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name of Query | Number of Sources | Number of Alerts |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Initial queries |'
  prefs: []
  type: TYPE_TB
- en: '| trues^a^aSpectroscopically confirmed bright ($m_{\mathrm{peak}}\,\leq\,18.5\,\mathrm{mag}$)
    extragalactic transients. | 5,212 | 308,934 |'
  prefs: []
  type: TYPE_TB
- en: '| vars^b^bSources classified as AGN, CVs, VarStars, or QSOs. | 1,127 | 150,017
    |'
  prefs: []
  type: TYPE_TB
- en: '| dims^c^cDim ($m_{\mathrm{peak}}\,>\,18.5\,\mathrm{mag}$) sources with transient-like
    light curves. | 8,979 | 249,087 |'
  prefs: []
  type: TYPE_TB
- en: '| rejects^d^dSources not marked as bright extragalactic transients by BTS scanners.
    | 4,417 | 407,357 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 19,735 | 1,115,395 |'
  prefs: []
  type: TYPE_TB
- en: '| Cleaned training set |'
  prefs: []
  type: TYPE_TB
- en: '| trues^a^aSpectroscopically confirmed bright ($m_{\mathrm{peak}}\,\leq\,18.5\,\mathrm{mag}$)
    extragalactic transients. | 5,206 | 264,317 |'
  prefs: []
  type: TYPE_TB
- en: '| vars^b^bSources classified as AGN, CVs, VarStars, or QSOs. | 1,126 | 109,934
    |'
  prefs: []
  type: TYPE_TB
- en: '| dims^c^cDim ($m_{\mathrm{peak}}\,>\,18.5\,\mathrm{mag}$) sources with transient-like
    light curves. | 8,824 | 223,934 |'
  prefs: []
  type: TYPE_TB
- en: '| rejects^d^dSources not marked as bright extragalactic transients by BTS scanners.
    | 4,402 | 241,478 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | 19,558 | 839,663 |'
  prefs: []
  type: TYPE_TB
- en: Note. — Alerts are removed from the training set if they (i) have a corrupted
    image cutout; (ii) come from a source with an ambiguous label; (iii) are missing
    Pan-STARRS1 cross-match information; (iv) are an $i$-band observation; (v) have
    a negative difference image; or (vi) come from a source with a transient present
    in the reference image.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aa950313d0cd93f3fe2b216edbfbeb3b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Custom metadata feature definitions depicted for the light curve
    of ZTF20acjlkpe. Teal ($g$-band) and red ($r$-band) circles indicate detections.
    days_to_peak (purple), days_since_peak (green), age (navy), peakmag_so_far (upper
    dashed gray), and maxmag_so_far (lower dashed gray) are presented for the latest
    detection shown. Together, these features make simplified information of the light
    curve phase and shape available to BTSbot (shown as the orange line, which assumes
    that maxmag_so_far corresponds to the first detection).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We then augment the alert packets with custom metadata features that are not
    already present in the ZTF Avro alert packets: days_to_peak, days_since_peak,
    age, peakmag_so_far, maxmag_so_far, and nnondet. The feature days_to_peak encodes
    the number of days from the first alert to the alert with the brightest magnitude
    thus far for the source in question. Closely related is days_since_peak, which
    represents the number of days from the current alert to the alert with the brightest
    magnitude. The sum of these quantities is the age. The features peakmag_so_far
    and maxmag_so_far encode the brightest and dimmest magnitude of all alert packets
    from this source thus far; peakmag_so_far is particularly crucial for correctly
    classifying late-time alerts of SNe because BTSbot would otherwise have almost
    no information on the brightness history of the source. These features are shown
    over an example light curve in Figure [2](#S2.F2 "Figure 2 ‣ 2 Training data ‣
    The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning"). They clearly
    do not recover the light curve perfectly, but they are adopted because of their
    simplicity and effectiveness across a very large variety of light curves. We find
    that giving BTSbot high-level information on the source’s current phase and the
    rough shape of its light curve with these features yields a significant boost
    in performance. Many methods exist to more faithfully represent SNe light curves,
    but many are not conditioned to model light curves of the other sources in BTSbot’s
    domain, e.g. AGN, VarStars, and others. Notably, these features are defined for
    all sources with as few as one detection or detections in only one photometric
    passband. One could extract more information from the light curve by decoupling
    these features to passband-dependent equivalents, however, this would create instances
    of alerts having missing features potentially compromising the model’s performance.
    Lastly, the feature nnondet is an estimation of the number of non-detections at
    the source’s location. This feature was used in the ALeRCE real-time stamp classifier
    (Carrasco-Davis et al., [2021](#bib.bib16)) and was found to have high importance
    for distinguishing alerts between their five classes; we find that including it
    similarly boosts BTSbot’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: All of these features are computed for each alert packet from its perspective,
    i.e., only using information already available at the time the alert packet was
    created, including alerts from before each source passed the BTS alert filter.
    Additionally, some alerts were created before the latest version of braai went
    into production and thus have a deep real-bogus score (drb) from an outdated version
    of braai or no drb at all. We rerun all alert packets through the d6_m9 version
    of braai and replace all drb scores with these new scores.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we clean our training set with a series of cuts. First, we remove alerts
    from $i$-band observations and alerts with negative difference images. The $i$-band
    alerts are removed primarily because BTS draws from the ZTF public survey, which
    only takes observations in $g$- and $r$-band. Negative difference images are identified
    using the isdiffpos flag included in the ZTF alert packets. Alerts with negative
    difference images are removed because they frequently represent instances of SNe
    or other transients visible in the reference image: a configuration different
    than what is typical for an alert. As an extra precaution, we compile a list of
    sources with SNe present in the reference images and remove them from the training
    set. Additionally, some sources fit into neither or both of our classes and are
    correspondingly removed from our training set. For example, ZTF18abdiasx appears
    to be a bright supernova projected on top of an AGN, and thus fits in both of
    our classes so it is removed from the training set. Some of the metadata features
    we choose to include reference the Pan-STARRS1 catalog (PS1; Kaiser et al., [2002](#bib.bib50)):
    sgscore{1,2} and distpsnr{1,2}. When there are no PS1 sources within 30 arcseconds
    of a given ZTF alert, these features are set to $-999$. We remove all alerts with
    $-999$ in any of these four fields. Table [1](#S2.T1 "Table 1 ‣ 2 Training data
    ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning") shows the
    counts of sources and alerts at this stage under the cleaned training set header.
    Each of the four queries loses some alerts in the cleaning process, however, the
    rejects query loses more than $150,000$ alerts. These $150,000$ alerts are almost
    entirely negative difference image alerts, which come from sources like binary
    stars, AGN, and high-proper motion stars.'
  prefs: []
  type: TYPE_NORMAL
- en: We employ standard practices for splitting the full training set into train,
    validation, and test splits. We randomly assign sources to these splits with probabilities
    of 81%, 9%, and 10% respectively^(11)^(11)1190% of the training data is seen by
    the model during development and the remaining 10% is concealed until hyperparameters
    are finalized.. This splitting is done on sources rather than alerts to prevent
    any source from having alerts in multiple splits, which would produce a validation
    and test bias that overestimates the true performance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2 Training data ‣ The Zwicky Transient Facility
    Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up of
    Bright Transients with Deep Learning") illustrates that, as expected, the typical
    number of alerts per source is variable over the four different queries. Some
    AGN in vars have $>10^{3}$ alerts, while some SNe have as few as two alerts. We
    correct this imbalance by defining a hyperparameter $N_{\mathrm{max}}$: the maximum
    number of alerts per source allowed in the training set. Sources with more than
    $N_{\mathrm{max}}$ alerts will have some alerts removed and those with $N_{\mathrm{max}}$
    or fewer alerts will remain untouched.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e9e6ea1121fc096c9e752c6e153d3c2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Gaussian kernel density estimations showing the number of alerts
    per source for each query comprising our cleaned training set. Top: positive class
    examples; Bottom: negative class examples. Sources with many alerts are thinned
    down to $N_{\mathrm{max}}=100$ alerts per source. This prevents long-lived sources
    like AGN (in Variables) and bright SNe (in Bright transients) from being over-represented
    in training.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/467732361a1f4fd57495a9f85cdad463.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Bar charts showing the distribution of sources and alerts from 4
    queries into train/validation/test (81%/9%/10%) splits. Certain sources have an
    upper limit set on their number of alerts, excess alerts are removed. Our training
    set is imbalanced favoring not-BTS sources $\sim$3:1 and favoring the not-BTS
    alerts $\sim$2:1\.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We apply $N_{\mathrm{max}}$ alert thinning to sources based on which query
    they originate from and which split they are present in. For sources in the trues,
    dims, and rejects queries (many of which are galactic or extragalactic transients),
    we want BTSbot to learn and identify their properties at all phases of their evolution,
    so we take a simple uniform random selection of $N_{\mathrm{max}}$ alerts to preserve
    and discard all others. The remaining alerts are roughly uniformly distributed
    over the rise, peak, and fade of these transients^(12)^(12)12Observational coverage
    of transients is typically greater near peak, so this simple random selection
    will slightly favor removing alerts near peak. We do not expect significantly
    different results if we attempted to correct for this.. For sources in the vars
    query, we keep only the most recent $N_{\mathrm{max}}$ alerts. Metadata of long-lived
    sources look systematically different in the present than they did years ago.
    For example, AGN were typically first detected near the beginning of ZTF and have
    been repeatedly detected since. New incoming alerts from these AGN, those that
    BTSbot will encounter in production, will typically have very large age and ndethist
    (approximately, number of previous detections), but those nearer to when they
    were first detected will have much smaller age and ndethist. By selecting the
    latest $N_{\mathrm{max}}$ alerts, we capture those which are most representative
    of today’s BTS candidates, and we avoid a data shift that would dramatically worsen
    BTSbot’s performance in production. Early versions of BTSbot used random $N_{\mathrm{max}}$
    thinning on vars sources and yielded markedly worse performance. We find the optimal
    value to be $N_{\mathrm{max}}=100$ (see Sec. [3.1](#S3.SS1 "3.1 Training and hyperparameter
    optimization ‣ 3 BTSbot Scope, Architecture, and Training ‣ The Zwicky Transient
    Facility Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up
    of Bright Transients with Deep Learning") for optimization details).'
  prefs: []
  type: TYPE_NORMAL
- en: The CVs in vars are transients like the sources in trues and dims, and we want
    to capture alerts from all stages of their evolution, rather than just late-time.
    The selection of the latest $N_{\mathrm{max}}=100$ alerts from vars sources does
    not significantly impact our coverage of CV evolution because most CVs are very
    rapidly evolving and have fewer than $N_{\mathrm{max}}=100$ alerts, so they typically
    receive no thinning at all.
  prefs: []
  type: TYPE_NORMAL
- en: Sources from all queries in the train split get $N_{\mathrm{max}}$ alert thinning
    to avoid the over-representation problem. In the validation and test splits, however,
    only sources from the vars query get alert thinning. The other sources are left
    un-thinned because (i) over-representation is not an issue, and (ii) we need all
    alerts from these sources to most accurately measure completeness and purity of
    the BTSbot predictions. We keep alert thinning in place for the vars sources in
    the validation and test splits because the very old alerts from these long-lived
    AGN are unlike what appears in present day scanning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S2.F4 "Figure 4 ‣ 2 Training data ‣ The Zwicky Transient Facility
    Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up of
    Bright Transients with Deep Learning") shows the number of sources and alerts
    in each split and the query which they originated from after all cuts and thinning
    are applied. Our two classes are moderately imbalanced: $\sim$35% BTS alerts to
    $\sim$65% not-BTS alerts. We discuss techniques for mitigating the effects of
    class imbalance in Sec. [3](#S3 "3 BTSbot Scope, Architecture, and Training ‣
    The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning"). In total,
    our production training set comprises $608\mathrm{,}943$ total alerts from $19\mathrm{,}558$
    total sources totalling $>$60 GB. Despite BTSbot’s relatively narrow domain, this
    is significantly more alerts than other models with similar architectures, such
    as the ALeRCE stamp classifier ($\sim$52,000; Carrasco-Davis et al., [2021](#bib.bib16))
    and the ACAI models ($\sim$200,000; Duev & van der Walt, [2021](#bib.bib27)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3 BTSbot Scope, Architecture, and Training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c3ff9f7aa2716dfdb2e6f5297924d0e6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Diagram of our multi-modal convolutional neural network, BTSbot,
    performing bright extragalactic transient / not-bright extragalactic transient
    binary classification on ZTF alert packets. Image input is processed through the
    convolutional branch and then flattened to a 1D vector and concatenated with the
    output of dense layers in the metadata branch. After another dense layer, a single-neuron
    layer produces the final prediction: a unit-interval bright transient score.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Our MM-CNN, BTSbot, automates source identification for BTS by assigning each
    ZTF alert packet a bright transient score. Figure [5](#S3.F5 "Figure 5 ‣ 3 BTSbot
    Scope, Architecture, and Training ‣ The Zwicky Transient Facility Bright Transient
    Survey. III. BTSbot: Automated Identification and Follow-up of Bright Transients
    with Deep Learning") shows how input is fed into BTSbot and how information is
    combined to produce the output score; BTSbot contains three main components. (1)
    The convolutional branch processes the science, reference, and difference cutouts
    as a three-channel image through a VGG-like architecture (Simonyan & Zisserman,
    [2014](#bib.bib83)). (2) The metadata branch processes the 25 extracted features
    (see Table [4](#S3.T4 "Table 4 ‣ 3.1 Training and hyperparameter optimization
    ‣ 3 BTSbot Scope, Architecture, and Training ‣ The Zwicky Transient Facility Bright
    Transient Survey. III. BTSbot: Automated Identification and Follow-up of Bright
    Transients with Deep Learning")) through a batch normalization layer and two dense
    layers. (3) The combined section concatenates the output of the two branches and
    passes it through two more dense layers, the second of which produces the prediction
    using a softmax activation function. The output is a unit-interval score where
    higher scores represent increased confidence that the source in the input alert
    packet is, or will become, a bright extragalactic transient. The parameters for
    each layer are shown in Table [2](#S3.T2 "Table 2 ‣ 3 BTSbot Scope, Architecture,
    and Training ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot:
    Automated Identification and Follow-up of Bright Transients with Deep Learning").
    Other than the final layer, activation functions are all the rectified linear
    unit (ReLU; Nair & Hinton, [2010](#bib.bib65)), and all convolutional layers have
    symmetric unit stride and same padding.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: BTSbot layer configurations'
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer type | Layer parameters | Hyperparameter search range |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Convolutional branch |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2D Conv. | 32 filters, $5\times 5$ kernel | 8 – 128 filters^a^aAll 2D Convolutional
    (Conv.) layers have the same search range for filter counts and kernel size. |'
  prefs: []
  type: TYPE_TB
- en: '| 2D Conv. | 32 filters, $5\times 5$ kernel | [3, 5, 7] kernel size^a^aAll
    2D Convolutional (Conv.) layers have the same search range for filter counts and
    kernel size. |'
  prefs: []
  type: TYPE_TB
- en: '| Max pool | $2\times 2$ kernel | - |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | 0.50 | 0.1 – 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| 2D Conv. | 64 filters, $5\times 5$ kernel | 8 – 128 filters^a^aAll 2D Convolutional
    (Conv.) layers have the same search range for filter counts and kernel size. |'
  prefs: []
  type: TYPE_TB
- en: '| 2D Conv. | 64 filters, $5\times 5$ kernel | [3, 5, 7] kernel size^a^aAll
    2D Convolutional (Conv.) layers have the same search range for filter counts and
    kernel size. |'
  prefs: []
  type: TYPE_TB
- en: '| Max pool | $4\times 4$ kernel | - |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | 0.55 | 0.1 – 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Metadata branch |  |'
  prefs: []
  type: TYPE_TB
- en: '| Batch norm. | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | 128 units | 32 – 256 units |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | 0.25 | 0.1 – 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | 128 units | 32 – 256 units |'
  prefs: []
  type: TYPE_TB
- en: '| Combined section |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | 8 units | 8 – 128 units |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | 0.20 | 0.1 – 0.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | 1 unit | - |'
  prefs: []
  type: TYPE_TB
- en: Note. — Images and metadata are passed through their respective branches, and
    the output of either is concatenated and sent to the combined branch. Dropout
    and batch normalization (batch norm.) layers are included to regularize.
  prefs: []
  type: TYPE_NORMAL
- en: 'The choice of a MM-CNN is motivated by the fact that the images and the extracted
    features provide complementary information for performing our task. For example,
    the extracted feature distpsnr1 represents the angular distance to the PS1 cataloged
    source nearest to this ZTF source, and sgscore1 represents the star-galaxy score
    (Tachibana & Miller, [2018](#bib.bib89)) of this PS1 source. While new transients
    are not present in PS1, the host galaxies of supernovae (SNe) often are. Thus,
    alerts from SNe tend to have moderate distpsnr1 and small sgscore1 values, indicating
    a galaxy projected nearby to the source. Most AGN and some CVs that pass the BTS
    alert filter are cataloged in PS1 and thus have distpsnr1 very near to zero. The
    images also provide important information following a similar heuristic. Bright
    SNe tend to be associated with prominent (bright with large angular size) off-center
    extended sources, their host galaxies; faint SNe tend to have less prominent host
    galaxies because they tend to be farther away; AGN will appear as exactly centered
    extended sources; CVs will often appear surrounded by many bright point sources
    because they tend to occur near the galactic plane around many other stars. Carrasco-Davis
    et al. ([2021](#bib.bib16)) give more detailed descriptions of how AGN, SNe, and
    VarStars can be distinguished using ZTF image cutouts and metadata. A MM-CNN is
    able to pool information from all input types and consider them together when
    making a prediction. We also experiment with uni-modal alternatives to BTSbot in
    Appendix [B](#A2 "Appendix B Comparison with uni-modal architectures ‣ The Zwicky
    Transient Facility Bright Transient Survey. III. BTSbot: Automated Identification
    and Follow-up of Bright Transients with Deep Learning").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given the scope and architecture of BTSbot we encounter a number of challenges.
    First, we are requiring BTSbot to learn multiple complex separations. BTSbot must
    learn to separate SNe from other sources without using redshift information because
    it is not always known a priori. It must also learn to identify bright SNe with
    limited time-series information irrespective of the SN’s current phase. Early
    in its rise or late in its fade, a bright SN can appear very similar to a near-peak
    dim SN. Additionally, BTSbot does not have explicit information stating the BTS
    threshold is 18.5 mag. Rather, it must learn the position of this threshold drawn
    through the continuous distribution of SNe peak magnitudes. Lastly, BTSbot does
    not have direct access to full light curve information, i.e., information on every
    previous individual detection. Instead, it has access to the basic custom metadata
    features we compute from the full light curve. We omit full light curve information,
    in part, because there is no established method in the literature for representing
    partial light curves of the wide variety our model encounters in a way that is
    fit for input into a neural network. There has been a great deal of work to accomplish
    this for SNe alone (e.g., Villar et al., [2020](#bib.bib97)), but these methods
    are not applicable to all the types of sources that BTSbot encounters. This choice
    supports the possibility of tuning BTSbot to identify extragalactic transients
    very rapidly, potentially on their first detection, which Section [5.3](#S5.SS3
    "5.3 An adaptation of BTSbot: Automatic, very rapid follow-up ‣ 5 Discussion ‣
    The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning") explores.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Training and hyperparameter optimization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BTSbot is implemented with TensorFlow (Abadi et al., [2015](#bib.bib1)) and
    the Keras API (Chollet et al., [2015](#bib.bib18)). We adopt the Adam optimizer
    (Kingma & Ba, [2014](#bib.bib54)) and the binary cross-entropy loss function.
    In addition to thinning a source’s alerts by $N_{\mathrm{max}}$ (see Sec. [2](#S2
    "2 Training data ‣ The Zwicky Transient Facility Bright Transient Survey. III.
    BTSbot: Automated Identification and Follow-up of Bright Transients with Deep
    Learning")), we make use of a number of training techniques to mitigate overfitting.
    We employ data augmentation, which executes random rotations of 0^∘, 90^∘, 180^∘,
    and 270^∘ and random horizontal and vertical flipping on the image cutouts. These
    also help ensure that BTSbot is invariant to these transformations. We weight
    contributions to the loss function by the inverse of the relative size of the
    input alert’s class (i.e. misclassifications of BTS alerts contribute more loss
    than that of not-BTS alerts). This size is computed by the count of alerts per
    class in the train split, but we also experiment with computing these weights
    based on the number of sources per class. The learning rate $\alpha$ decreases
    after 20 epochs without improved validation loss and training terminates after
    75 epochs without improved validation loss.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: BTSbot hyperparameters'
  prefs: []
  type: TYPE_NORMAL
- en: '| Parameter name | Optimized value | Hyperparameter search range |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| batch size | 64 | 8 – 64 |'
  prefs: []
  type: TYPE_TB
- en: '| Adam $\beta_{1}$ | 0.99 | 0.81 – 0.999 |'
  prefs: []
  type: TYPE_TB
- en: '| Adam $\beta_{2}$ | 0.99 | 0.9 – 0.9999 |'
  prefs: []
  type: TYPE_TB
- en: '| learning rate ($\alpha$) | $10^{-4}$ | $10^{-2}$ – $5\times 10^{-6}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $\alpha$ decrease factor | $0.4$ | 0.25 – 0.75 |'
  prefs: []
  type: TYPE_TB
- en: '| $\alpha_{\mathrm{min}}$ | $5\times 10^{-10}$ | $10^{-10}$ – $10^{-5}$ |'
  prefs: []
  type: TYPE_TB
- en: '| $N_{\mathrm{max}}$ | 100 | 1 – $\infty$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: BTSbot metadata features'
  prefs: []
  type: TYPE_NORMAL
- en: '| Feature name | Definition [unit] |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Alert packet metadata |'
  prefs: []
  type: TYPE_TB
- en: '| sgscore{1,2} | Star/Galaxy score of nearest two PS1 sources |'
  prefs: []
  type: TYPE_TB
- en: '| distpsnr{1,2} | Distance to nearest two PS1 sources [arcsec] |'
  prefs: []
  type: TYPE_TB
- en: '| fwhm | Full Width Half Max [pixels] |'
  prefs: []
  type: TYPE_TB
- en: '| magpsf | magnitude of PSF-fit photometry [mag] |'
  prefs: []
  type: TYPE_TB
- en: '| sigmapsf | 1-$\sigma$ uncertainty in magpsf [mag] |'
  prefs: []
  type: TYPE_TB
- en: '| chipsf | Reduced $\chi^{2}$ of PSF-fit |'
  prefs: []
  type: TYPE_TB
- en: '| ra | Right ascension of source [deg] |'
  prefs: []
  type: TYPE_TB
- en: '| dec | Declination of source [deg] |'
  prefs: []
  type: TYPE_TB
- en: '| diffmaglim | 5-$\sigma$ magnitude detection threshold [mag] |'
  prefs: []
  type: TYPE_TB
- en: '| ndethist | Number of previous detections of source |'
  prefs: []
  type: TYPE_TB
- en: '| nmtchps | $\#$ of PS1 cross-matches within 30 arcsec |'
  prefs: []
  type: TYPE_TB
- en: '| drb | Deep learning-based real/bogus score |'
  prefs: []
  type: TYPE_TB
- en: '| ncovhist | $\#$ of times source on a field and read channel |'
  prefs: []
  type: TYPE_TB
- en: '| chinr | $\chi$ parameter of nearest source in reference |'
  prefs: []
  type: TYPE_TB
- en: '| sharpnr | sharp parameter of nearest source in reference |'
  prefs: []
  type: TYPE_TB
- en: '| scorr | Peak-pixel S/N in detection image |'
  prefs: []
  type: TYPE_TB
- en: '| sky | Local sky background estimate [DN] |'
  prefs: []
  type: TYPE_TB
- en: '| Custom metadata |'
  prefs: []
  type: TYPE_TB
- en: '| days_since_peak | Time since brightest alert [days] |'
  prefs: []
  type: TYPE_TB
- en: '| days_to_peak | Time from first to brightest alert [days] |'
  prefs: []
  type: TYPE_TB
- en: '| age | days_since_peak + days_to_peak |'
  prefs: []
  type: TYPE_TB
- en: '| peakmag_so_far | Source’s minimum magpsf thusfar [mag] |'
  prefs: []
  type: TYPE_TB
- en: '| maxmag_so_far | Source’s maximum magpsf thusfar [mag] |'
  prefs: []
  type: TYPE_TB
- en: '| nnondet^a^aAdopted from Carrasco-Davis et al. ([2021](#bib.bib16)). | ncovhist
    - ndethist |'
  prefs: []
  type: TYPE_TB
- en: Note. — The 25 metadata features passed into BTSbot’s metadata branch. Full
    definitions of alert packet features can be found at [https://zwickytransientfacility.github.io/ztf-avro-alert/schema.html](https://zwickytransientfacility.github.io/ztf-avro-alert/schema.html).
  prefs: []
  type: TYPE_NORMAL
- en: 'We utilize the Weights and Biases platform (Biewald, [2020](#bib.bib7)) to
    perform multiple extensive Bayesian hyperparameter sweeps. A Bayesian hyperparameter
    sweep guides searching through a large grid of hyperparameters intelligently by
    leveraging correlations between previous hyperparameter inputs and the corresponding
    trained model’s performance in a specified metric. This is expected to be more
    efficient than traditional grid search sweeps because, for example, the Bayesian
    sweep can learn to avoid certain regions of the hyperparameter space that frequently
    yield poor performing models. We find the optimal configuration of hyperparameters
    for BTSbot layers (see Table [2](#S3.T2 "Table 2 ‣ 3 BTSbot Scope, Architecture,
    and Training ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot:
    Automated Identification and Follow-up of Bright Transients with Deep Learning")),
    the selection of metadata features (see Table [4](#S3.T4 "Table 4 ‣ 3.1 Training
    and hyperparameter optimization ‣ 3 BTSbot Scope, Architecture, and Training ‣
    The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning")), and other
    values (see Table [3](#S3.T3 "Table 3 ‣ 3.1 Training and hyperparameter optimization
    ‣ 3 BTSbot Scope, Architecture, and Training ‣ The Zwicky Transient Facility Bright
    Transient Survey. III. BTSbot: Automated Identification and Follow-up of Bright
    Transients with Deep Learning")). Tables [2](#S3.T2 "Table 2 ‣ 3 BTSbot Scope,
    Architecture, and Training ‣ The Zwicky Transient Facility Bright Transient Survey.
    III. BTSbot: Automated Identification and Follow-up of Bright Transients with
    Deep Learning") and [3](#S3.T3 "Table 3 ‣ 3.1 Training and hyperparameter optimization
    ‣ 3 BTSbot Scope, Architecture, and Training ‣ The Zwicky Transient Facility Bright
    Transient Survey. III. BTSbot: Automated Identification and Follow-up of Bright
    Transients with Deep Learning") also include the range of values searched over
    for each hyperparameter in our sweeps. The search over which alert packet metadata
    features to provide to BTSbot included many other features which we found to not
    improve performance: sgscore3, distpsnr3, fid, maggaia, neargaia, magdiff, magap,
    sigmaap, magapbig, sigmaapbig, magnr, ssnrms, dsnrms, seeratio, nneg, magzpsci,
    jdstarthist, and classtar.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 BTSbot performance and comparison to human scanners
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Performance on the test split
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0f5f9eb986ca274cc9d39803c0d13e97.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: ROC curve (top) and confusion matrix (bottom) of the BTSbot production
    model and 19 other trials with different initializations. Top: The production
    BTSbot model (orange curve) and 19 other trials (gray curves) yield very similar
    ROC curves and ROCAUCs, indicating that BTSbot’s excellent performance is robust
    to different initializations. Bottom: The production model’s confusion matrix
    makes clear that BTSbot has greater TN rate (upper-left quadrant) than TP rate
    (lower-right quadrant). We find that models which trade-off TP rate in favor of
    TN rate tend to improve in other key performance metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: To characterize BTSbot’s generalizable performance, we present performance metrics
    on test split data. We train 20 trials of BTSbot with identical optimal hyperparameters
    and different random initializations of the model’s learnable parameters. The
    trial which yields the best test split performance is chosen as the production
    model. Performance metrics are reported for the chosen production model and the
    median of the metric for the 20 trials with uncertainties representing the metric’s
    1-$\sigma$ bounds.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final BTSbot models yield test accuracy of $94.1\%\pm 0.28$ and $94.9\%$ for
    the production model. The upper panel of Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Performance
    on the test split ‣ 4 BTSbot performance and comparison to human scanners ‣ The
    Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated Identification
    and Follow-up of Bright Transients with Deep Learning") shows BTSbot’s receiver
    operating characteristic (ROC) curves. ROC curves visualize the balance of true
    and false positives at various classification thresholds, and the area under the
    ROC curve (ROCAUC) is frequently used as a summary statistic for the performance
    of a classifier. A perfect classifier’s ROC curve has ROCAUC equal to unity while
    ROCAUC=0.5 corresponds to random guessing (on a balanced dataset). The final BTSbot models
    yield ROCAUC=$0.984\pm 0.001$ and ROCAUC=$0.985$ for the production model. The
    results of the production model and the 19 other trials are extremely similar,
    indicating stability in BTSbot’s performance over different initializations. The
    lower panel of Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Performance on the test split
    ‣ 4 BTSbot performance and comparison to human scanners ‣ The Zwicky Transient
    Facility Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up
    of Bright Transients with Deep Learning") shows the production model’s confusion
    matrix. A binary classifier’s confusion matrix visualizes the frequencies of the
    four classification outcomes: true negative ($\mathrm{TN}$), false positive ($\mathrm{FP}$),
    false negative ($\mathrm{FN}$), and true positive ($\mathrm{TP}$). The production
    model shows higher accuracy on not-BTS alerts (TN rate) than on BTS alerts (TP
    rate). We could attempt to train BTSbot to better balance the TP and TN rates,
    for example by adjusting the class-weights, but we find that models which favor
    TN rate perform better overall.'
  prefs: []
  type: TYPE_NORMAL
- en: While we fare excellently in traditional ML performance metrics, they are not
    necessarily representative of BTSbot’s real-world performance. Because BTSbot produces
    scores on alert packets (alert-based classification) but sources must be chosen
    for follow-up (source-based classification), we must define a mapping from a sequence
    of alert predictions to a source prediction. We call these mappings “policies,”
    which we define to be analogous to the criteria BTS scanners use when deciding
    whether or not to save a source and request a spectrum of it. By simulating our
    policies on BTS candidates, we can compute performance metrics which realistically
    represent how BTSbot performs as a scanner. Further, we can compare the resulting
    figures against human scanners to contextualize BTSbot’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We define two policies which closely emulate their human scanning analogs:
    bts_p1 and bts_p2. The policy bts_p1 requires that a source have at least two
    alerts with high ($\geq 0.5$) bright transient score and $\texttt{magpsf}\leq
    19~{}\mathrm{mag}$ before being saved and having an SEDM trigger sent at priority
    1\. The policy bts_p2 requires that a source meet bts_p1 as well as having at
    least one alert with $\texttt{magpsf}\leq 18.5~{}\mathrm{mag}$ before a trigger
    being sent with priority 2\. Priority is a parameter of the requests sent to SEDM,
    where larger values indicate the request is more urgent. Higher priority requests
    are typically fulfilled before lower priority requests, although other factors,
    e.g. observability, are also taken into account. Most follow-up requests sent
    by BTS scanners are with priority 1 or 2, and priorities 2 and greater are typically
    reserved for sources which have already attained $m_{\mathrm{peak}}\leq 18.5~{}\mathrm{mag}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0560330ba9e9d18e9ad0242deae60aa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Completeness and purity of BTSbot actions (left) and speed comparison
    with human scanners (right) for sources in our test split. Left: The completeness
    curve (dash-dot navy) is 100% in all bins. BTSbot’s perfect completeness is conducive
    to BTS’s science efforts which require a highly-complete, unbiased sample. The
    small variations in the purity curve (solid orange) are due to small number statistics.
    The overall purity of bts_p1 and bts_p2 are 84.6% and 93.0% respectively. bts_p1
    has extra contamination from SNe with $m_{\mathrm{peak}}$ slightly greater than
    18.5 mag, sources which are acceptable targets for spectroscopic observation.
    Right: Histograms comparing BTSbot’s speed in saving (blue) and triggering (red)
    with that of scanners. Both distributions peak very near to zero indicating that
    BTSbot acts as quickly as human scanners on new bright transients.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We quantify the performance of these policies with completeness, purity, $\Delta
    t_{\mathrm{save}}$, and $\Delta t_{\mathrm{trigger}}$. Completeness (or “recall”)
    is the fraction of bright transients which are correctly classified by the policy:
    $\mathrm{TP}/(\mathrm{TP}+\mathrm{FN})$. The completeness of bts_p1 and bts_p2
    are identical, so we report a single value for both policies. Purity (or “precision”)
    is the fraction of predicted bright transients that are actually bright transients:
    $\mathrm{TP}/(\mathrm{TP}+\mathrm{FP})$. $\Delta t_{\mathrm{save/trigger}}$, is
    the difference between the Julian Date ($\mathrm{JD}$) at which BTSbot saved and
    triggered on a source with the $\mathrm{JD}$ that scanners did the same. The $\mathrm{JD}$s
    for scanners are queried from Fritz for all sources in the trues set (see Sec. [2](#S2
    "2 Training data ‣ The Zwicky Transient Facility Bright Transient Survey. III.
    BTSbot: Automated Identification and Follow-up of Bright Transients with Deep
    Learning")). Sources that were saved before January 1st 2021 are removed from
    this analysis because many of them were scanned using the GROWTH Marshal (Kasliwal
    et al., [2019](#bib.bib51)), so the save and trigger $\mathrm{JD}$s available
    on Fritz are unreliable. For saving, we use the JD at which a scanner added a
    source to the BTS catalog on Fritz. For triggering, we only consider sources which
    had an SEDM integral field unit (IFU) request sent before their first spectrum
    was uploaded to Fritz. The JD used for comparison is the JD at which the first
    IFU follow-up request was created. This restriction is applied because the time
    at which scanners decided to trigger is not available for the other facilities
    BTS uses for classification. BTSbot’s $\mathrm{JD}$ for saving and triggering
    on a source is the JD associated with the alert that first made the source satisfy
    bts_p1. The BTSbot JDs correspond to either policy, but we select bts_p1 as it
    makes for a more direct comparison to scanners.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When computing completeness and purity for these policies, we make two additional
    minor cuts. First, 70 sources in the test split also appear in junk (see Sec. [2](#S2
    "2 Training data ‣ The Zwicky Transient Facility Bright Transient Survey. III.
    BTSbot: Automated Identification and Follow-up of Bright Transients with Deep
    Learning")). This catalog is a list of sources that are unambiguously not bright
    transients which frequently pass the BTS alert filter. When scanning, they are
    typically hidden and not considered for saving or triggering; they are excluded
    from completeness and purity calculations. We also identify 59 sources which have
    only a single alert remaining after the cleaning cuts (see Sec. [2](#S2 "2 Training
    data ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning")). These
    sources will never pass either policy, so they are also excluded from completeness
    and purity calculations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We use estimates of the scanners’ completeness, purity, and speed as benchmarks
    against which to judge BTSbot’s performance. We compute a lower-limit on the scanners’
    saving completeness of bright transients from values presented in Table 1 and
    Section 3 of Perley et al. ([2020](#bib.bib72)), which give 99.6% completeness.
    It is not straightforward to compute the BTS scanners’ purity for saving or triggering
    on bright transients. Scanners will often intentionally act on transients which
    they know will not attain $m_{\mathrm{peak}}\,\leq\,18.5\,\mathrm{mag}$,^(13)^(13)13These
    sources, while not “bright” transients, are still of interest to BTS and the individuals
    within BTS. so straightforward estimates of the scanners’ bright transient purity
    would underestimate their ability to select only bright transients. Instead, we
    estimate their saving and triggering purity for selecting any extragalactic transient
    and rejecting other sources. We limit this analysis to saves and triggers performed
    between $2460175.5<\mathrm{JD}<2460216.5$ (see Sec. [4.2](#S4.SS2 "4.2 Performance
    on very recent BTS candidates ‣ 4 BTSbot performance and comparison to human scanners
    ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning") for explanation
    of JD bounds). During this time, 266 sources were saved by scanners to the primary
    internal BTS catalog, only 4 of which were not extragalactic transients: $98.5\%$
    scanner saving purity. Similarly, scanners sent SEDM IFU for 327 unique sources
    of which 11 were non-extragalactic transients: $96.7\%$ scanner triggering purity.
    Further, scanners saving and triggering on transients with $m_{\mathrm{peak}}>18.5~{}\mathrm{mag}$
    complicates $\Delta t_{\mathrm{save/trigger}}$ estimates. Scanners will save and
    trigger on transients slightly faster than otherwise because they need not wait
    for a transient to unambiguously demonstrate that it will soon have $m_{\mathrm{peak}}\leq
    18.5~{}\mathrm{mag}$. In addition, BTS scanners are occasionally aided in identifying
    bright transients by TNS reports from scanners working with other surveys or other
    ZTF alert brokers. For these reasons, $\Delta t_{\mathrm{save/trigger}}$ comparisons
    will not be perfectly direct, but we still adopt them as a basic benchmark for
    BTSbot’s speed.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc6ca578d517224ecda58d56aa6bd711.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Light curves of three sources depicting typical evolution of BTSbot scores.
    Teal ($g$-band) and red ($r$-band) points indicate detections, and filled and
    open circles represent alerts which received score $\geq 0.5$ and $<0.5$ respectively.
    Left: TPs may have low scoring alerts while still dim, but scores increase once
    they near the 18.5 mag threshold (dotted gray line). After fading well below the
    peak magnitude, scores remain high, in part, due to information provided by custom
    metadata features. Center: Almost all bts_p1 FPs are dim ($m_{\mathrm{peak}}>18.5\,\mathrm{mag}$)
    transients whose alerts receive high scores when near the BTS threshold. Right:
    Many bts_p2 FPs are CVs, which could be better rejected by increasing the score
    threshold to $0.8$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The left panel of Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Performance on the test
    split ‣ 4 BTSbot performance and comparison to human scanners ‣ The Zwicky Transient
    Facility Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up
    of Bright Transients with Deep Learning") shows BTSbot’s completeness and purity
    under our policies as a function of peak magnitude. The completeness curve is
    exactly 100% in all peak magnitude bins, giving perfect overall completeness.
    This compares well to the BTS scanners’ saving completeness: 99.6%. Very high
    completeness is essential for maintaining the quality of the BTS sample and ensuring
    that BTSbot does not inject any significant selection biases into the BTS sample.
    The purity curve is $>90\%$ in all peak magnitude bins. Most bins have $<10$ FPs,
    so the variations between bins is likely due to small number statistics. Overall
    purity for bts_p2 is 93.0%, and overall purity for bts_p1 is 84.6%. The bts_p1
    purity is not aligned with the purity curve because bts_p1 also considers a large
    number of sources with $m_{\mathrm{peak}}>18.5$.^(14)^(14)14Completeness and purity
    are not shown for $m_{\mathrm{peak}}>18.5$ because they are undefined and uniformly
    zero respectively. Contamination from dim ($m_{\mathrm{peak}}>18.5$) transients
    is the reason why the bts_p1 purity is much less than the bts_p2 purity; they
    make up 53 of the 54 FPs unique to bts_p1. These 53 transients have median $m_{\mathrm{peak}}$
    of 18.57 mag and many received spectroscopic follow-up requests by BTS. They are
    still of interest to members of BTS, so they represent a reasonable use of follow-up
    resources. BTSbot’s purity falls slightly ($\sim 2\%-6\%$) short of the scanners’
    ($98.5\%$ saving, $95.6\%$ triggering). Likely at the cost of completeness or
    speed, alternative policies could be designed to more conservatively allocate
    spectroscopic resources, increasing BTSbot’s purity to compare favorably with
    scanners. BTSbot’s behavior is dependent on braai and sgscore because they contribute
    to determining what sources pass the BTS alert filter: the only sources which
    BTSbot trains and triggers on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The right panel of Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Performance on the test
    split ‣ 4 BTSbot performance and comparison to human scanners ‣ The Zwicky Transient
    Facility Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up
    of Bright Transients with Deep Learning") shows histograms comparing the time
    at which BTSbot and the human scanners saved or triggered on a source. Negative
    values indicate that BTSbot was faster, and positive values indicate that the
    scanners were faster. Both histograms peak sharply at 0 days, suggesting that
    scanners and BTSbot act on new transients at the same time. The median of $\Delta
    t_{\mathrm{save}}$ and $\Delta t_{\mathrm{trigger}}$ are $-0.0381$ days and $-0.0147$ days
    respectively; BTSbot acts marginally quicker than scanners. Much of this performance
    is likely due to BTSbot’s decisions being made immediately as new alerts filter
    through the ZTF and BTS pipelines, although BTS does benefit from consistent real-time
    scanning thanks to members in European time-zones.'
  prefs: []
  type: TYPE_NORMAL
- en: The tails of this distribution include 6 (11) sources which are saved by BTSbot a
    week or more before (after) scanners did. Nuclear and hostless SNe make up most
    of the cases where BTSbot was faster. This suggests BTSbot is less hesitant to
    claim these challenging sources to be bright transients. Most of the cases where
    BTSbot is slower than scanners are slowly evolving SNe with a history of detections
    down to $\sim$20 mag. These suggest that scanners can better use the evolution
    prior to reaching 19 mag to identify transients, and BTSbot needs brighter detections
    to identify sources. Late identifications of these sorts are unlikely an issue
    because BTSbot still consistently identifies these sources before or near peak.
    Sources with poor light curve coverage, especially around 19 mag, cause large
    $\Delta t_{\mathrm{save}}$ and $\Delta t_{\mathrm{trigger}}$ and appear in either
    of these groups. Overall, BTSbot fares very well in time to save and trigger when
    compared to scanners.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Analysis of misclassifications in test split
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tracking and categorizing misclassifications is a key part of the development
    of ML models. Misclassifications are particularly important to understand in the
    case of BTSbot because mistakes could introduce biases into the BTS sample and
    waste valuable spectroscopic resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'On test split data, bts_p1 selects 92 FPs and 0 FNs. The majority of the FPs
    (53/92) are real dim transients. These are sources outside of BTSbot’s positive
    class but are still of interest to BTS, so they are acceptable FPs. Nearly all
    of the remaining FPs are CVs, AGN or QSOs and are shared with bts_p2. The center
    panel of Figure [8](#S4.F8 "Figure 8 ‣ 4.1 Performance on the test split ‣ 4 BTSbot
    performance and comparison to human scanners ‣ The Zwicky Transient Facility Bright
    Transient Survey. III. BTSbot: Automated Identification and Follow-up of Bright
    Transients with Deep Learning") presents an instance of a typical bts_p1 FP. Alerts
    are classified as not belonging to a bright transient until the source nears the
    18.5 mag threshold, at which point a small number of high-scoring alerts cause
    the source to pass bts_p1. The right panel shows a FP CV misclassified by both
    bts_p1 and bts_p2. It shows high- and low-scoring alerts interspersed with each
    other, although some other CV FPs begin receiving exclusively low scores once
    the CV has faded well beyond peak. The completeness is perfect for bts_p1 and
    bts_p2, so they have zero FNs.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the non-SN misclassifications are dominated by a relatively small number
    of CVs, AGN, and QSOs. Improved policies may be able to improve the rejection
    of these frequent FPs by, e.g., better leveraging existing information in external
    catalogs. The majority of the total misclassifications (dim SNe selected by bts_p1)
    are not problematic for BTS and represent an appropriate allocation of spectroscopic
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Performance on very recent BTS candidates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Performance diagnostics computed on test split data tend to be robust and representative
    of real-world performance, but, in some cases, can have associated biases. Our
    test split includes many alerts that are years old and a subtle data shift (caused
    by, e.g., maintenance to the camera or the optics) may have occurred since then.
    To characterize BTSbot’s present-day performance we conduct an additional analysis
    using alerts from very recent BTS candidates.
  prefs: []
  type: TYPE_NORMAL
- en: 'We perform this analysis in two parts: (i) we present alert-based performance
    metrics on alerts that recently passed the BTS alert filter and our cleaning cuts
    (see Sec. [2](#S2 "2 Training data ‣ The Zwicky Transient Facility Bright Transient
    Survey. III. BTSbot: Automated Identification and Follow-up of Bright Transients
    with Deep Learning")); (ii) we also present policy-based performance metrics on
    sources that recently passed bts_p1 or recently peaked. The date boundaries for
    these analyses are determined by the final date our training data was queried
    (19 August 2023) and the date BTSbot went into production (29 September 2023):
    $2460175.5<\mathrm{JD}<2460216.5$. This cut on JD minimizes the bias in this analysis
    from transient alerts which BTSbot trained on and instances where BTSbot’s actions,
    which are visible to scanners, influenced scanners’ decisions. Some very long-lived
    sources, like certain AGN, do have alerts in our training data and in this cleaned
    present-day sample. We do not remove these sources because they are encountered
    by BTSbot in production, and thus should be accounted for in this analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf83cd0155a41ba810005ca9194d3c0e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Same as Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Performance on the test
    split ‣ 4 BTSbot performance and comparison to human scanners ‣ The Zwicky Transient
    Facility Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up
    of Bright Transients with Deep Learning") for a sample of alerts from very recent
    BTS candidates. The results of the ROC (present-day: thick green; test split:
    narrow orange), the ROCAUC, and the confusion matrix are all very similar to their
    test split analogs. All metrics are marginally improved for the present-day sample
    but not significantly so. These suggest that no data shift has occurred that significantly
    decreases BTSbot’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We begin this analysis by applying the cleaning cuts described in Sec. [2](#S2
    "2 Training data ‣ The Zwicky Transient Facility Bright Transient Survey. III.
    BTSbot: Automated Identification and Follow-up of Bright Transients with Deep
    Learning") on the public alerts that passed the BTS filter with $2460175.5<\mathrm{JD}<2460216.5$.
    After cuts, this sample totals 4,031 alerts from 251 bright transients and 15,159
    alerts from 1,652 non-bright transients. Figure [9](#S4.F9 "Figure 9 ‣ 4.2 Performance
    on very recent BTS candidates ‣ 4 BTSbot performance and comparison to human scanners
    ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning") shows that
    the production BTSbot model yields $96.2\%$ accuracy and ROCAUC=$0.988$ on the
    present-day sample. TP rate and TN rate are 91.9% and 97.4%, respectively. The
    resulting performance is very similar to the metrics computed from test split
    data in Sec. [4.1](#S4.SS1 "4.1 Performance on the test split ‣ 4 BTSbot performance
    and comparison to human scanners ‣ The Zwicky Transient Facility Bright Transient
    Survey. III. BTSbot: Automated Identification and Follow-up of Bright Transients
    with Deep Learning") and shown in Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Performance
    on the test split ‣ 4 BTSbot performance and comparison to human scanners ‣ The
    Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated Identification
    and Follow-up of Bright Transients with Deep Learning"). Here, we observe marginally
    higher performance across all alert-based metrics than in the test split results.
    These variations are most likely due to the relatively small size of the present-day
    sample and are not indicative of a data shift affecting BTSbot’s performance.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9232211ca7cdc60d170a8eb081df270a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Same as Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Performance on the test
    split ‣ 4 BTSbot performance and comparison to human scanners ‣ The Zwicky Transient
    Facility Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up
    of Bright Transients with Deep Learning") for a sample of very recent BTS candidates.
    Left: The completeness (dash-dot navy) and purity (solid orange) curves are perfect
    in most bins. The overall purity of bts_p1 and bts_p2 are $81.0\%$ and $92.0\%$ respectively.
    Contamination unique to bts_p1 is almost entirely real SNe with $m_{\mathrm{peak}}$
    slightly greater than 18.5, acceptable false positives. The relatively low purity
    in two bins is due to small number statistics. Right: Both distributions still
    peak very near to zero. Likely due to differences in the cuts creating the input
    samples, $\Delta t_{\mathrm{save}}$ ($0.0525$ days) and $\Delta t_{\mathrm{trigger}}$
    ($1.01$ days) are larger than test split equivalents.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As in Sec. [4.1](#S4.SS1 "4.1 Performance on the test split ‣ 4 BTSbot performance
    and comparison to human scanners ‣ The Zwicky Transient Facility Bright Transient
    Survey. III. BTSbot: Automated Identification and Follow-up of Bright Transients
    with Deep Learning"), any source in junk or having only one alert after cleaning
    is removed when computing policy completeness and purity. In Sec. [2](#S2 "2 Training
    data ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning"), the vars
    query received alert thinning down to $N_{\mathrm{max}}=100$ alerts per source.
    We emulate this by thinning sources classified on Fritz or manually identified
    as an AGN, CV, or QSO down to their latest $N_{\mathrm{max}}=100$ alerts. We run
    all public alerts passing our Sec. [2](#S2 "2 Training data ‣ The Zwicky Transient
    Facility Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up
    of Bright Transients with Deep Learning") cleaning cuts from all sources through
    both policies and select only the sources which satisfy bts_p1 or reach their
    peak magnitude between $2460175.5<\mathrm{JD}<2460216.5$. This representatively
    simulates the actions BTSbot would have taken during this time period should it
    have been fully operational.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similar to Figure [7](#S4.F7 "Figure 7 ‣ 4.1 Performance on the test split
    ‣ 4 BTSbot performance and comparison to human scanners ‣ The Zwicky Transient
    Facility Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up
    of Bright Transients with Deep Learning"), Figure [10](#S4.F10 "Figure 10 ‣ 4.2
    Performance on very recent BTS candidates ‣ 4 BTSbot performance and comparison
    to human scanners ‣ The Zwicky Transient Facility Bright Transient Survey. III.
    BTSbot: Automated Identification and Follow-up of Bright Transients with Deep
    Learning") shows the completeness, purity, and speed comparison of the production
    BTSbot model on the present-day BTS sample. Both the completeness and purity curve,
    shown in the left panel, are at or near 100% in most peak magnitude bins. The
    overall completeness is $98.7\%$. This remains near-perfect and supports BTSbot’s
    ability to scan without imposing significant selection biases into the BTS sample.
    Variations in either curve are due to small number statistics; there are 0-3 FPs
    and FNs in each bin shown. Overall bts_p1 purity is $81.0\%$, and overall bts_p2
    purity is $92.0\%$. Similar to the test split results, the sources selected by
    bts_p1 but not bts_p2 are dominated by SNe with $m_{\mathrm{peak}}$ slightly dimmer
    than 18.5 mag (19/22 sources): non-problematic FPs because they are typically
    triggered on by BTS. The completeness and purity estimates of the present-day
    sample are very similar to their test split analogs, $\leq\pm 4\%$ difference
    in all three metrics, further suggesting that no data shift has affected BTSbot’s
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The right panel of Figure [10](#S4.F10 "Figure 10 ‣ 4.2 Performance on very
    recent BTS candidates ‣ 4 BTSbot performance and comparison to human scanners
    ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning") compares
    BTSbot’s speed to act with that of the human scanners on our present-day sample.
    The medians of $\Delta t_{\mathrm{save}}$ and $\Delta t_{\mathrm{trigger}}$ are
    larger than their test split counterparts: $0.0525$ days and $1.01$ days respectively.
    We attribute this increase to two main factors. (i) We have excluded ZTF partnership
    data from the present-day sample, but scanners typically view data from both the
    ZTF public and partnership observations when scanning. This frequently provides
    them with additional detections for BTS candidates, which BTSbot is blinded from,
    aiding in more quickly identify bright transients. (ii) The present-day sample
    is very small, so the median is volatile to changes. There are just is 65 sources
    with $\Delta t_{\mathrm{save}}$ values in the present-day sample, while there
    were $>250$ sources for the test split. The shapes of the $\Delta t$ distributions,
    however, is consistent between the present-day and test split analysis. Together,
    these suggest that the differences observed are not due to a data shift but rather
    to differences in the sample characteristics and in how the experiments were conducted.'
  prefs: []
  type: TYPE_NORMAL
- en: With a sample of BTS candidates contiguous in time, we can now easily compute
    the median number of saves and triggers performed by BTSbot per night. Over the
    41 nights in the present-day sample, 184 sources satisfied bts_p1. This is a median
    of $\sim 4.5$ sources per night which would have been saved and sent to SEDM.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Analysis of misclassifications in present-day sample
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Analogously to Sec. [4.1.1](#S4.SS1.SSS1 "4.1.1 Analysis of misclassifications
    in test split ‣ 4.1 Performance on the test split ‣ 4 BTSbot performance and comparison
    to human scanners ‣ The Zwicky Transient Facility Bright Transient Survey. III.
    BTSbot: Automated Identification and Follow-up of Bright Transients with Deep
    Learning"), we investigate the types of sources which BTSbot misclassifies in
    the present-day sample.'
  prefs: []
  type: TYPE_NORMAL
- en: Our policies select 35 FPs and 2 FNs. Similarly to the test split results, the
    majority of FPs (19/35) are real dim transients. The other FPs are again dominated
    by CVs and AGN but do include other sources like asteroids. One of the FNs (ZTF23aaxtplp)
    has an extremely bright host galaxy and is projected very near to its nucleus.
    The other FN (ZTF23abeuope) shows two bright stars overlapping the host galaxy
    and projected very near to the supernova. It is not certain that these properties
    caused these sources to receive low scores, but they are clearly very uncommon.
  prefs: []
  type: TYPE_NORMAL
- en: As is the case for the test split, the majority of the misclassifications in
    the present-day sample, dim SNe, have little negative consequence for BTS.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Integration of BTSbot into ZTF and the BTS workflow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BTSbot has been deployed in Kowalski to enable running in real-time on incoming
    alert packets from IPAC’s alert-producing and brokering system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Kowalski performs three distinct operations on every alert packet. First, it
    separates the alert packet from its prv_candidates field (a 30-day history of
    detections and non-detections) to concatenate it to the full list of prv_candidates
    from all previous alerts with the same objectId. This forms a full light curve
    for a given ZTF object. The product of this concatenation is then used to compute
    the custom metadata features BTSbot takes as input (see Sec. [2](#S2 "2 Training
    data ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning") for list
    and definitions). Kowalski then cross-matches every alert with a large number
    of catalogs such as the NASA Extragalactic Database^(15)^(15)15[https://ned.ipac.caltech.edu/](https://ned.ipac.caltech.edu/)
    (NED), CLU (Census of the Local Universe; Cook et al., [2019](#bib.bib20)), Milliquas
    (Flesch, [2019](#bib.bib29)), and others. Lastly, Kowalski runs several ML models:
    braai, the five ACAI classifiers (Duev & van der Walt, [2021](#bib.bib27)), and
    BTSbot. The outputs of all models are injected into the alert packet along with
    their corresponding version numbers allowing alert filters to use this information
    when identifying candidate transients.'
  prefs: []
  type: TYPE_NORMAL
- en: The enriched alert packets are then stored in a non-relational database (MongoDB^(16)^(16)16[https://www.mongodb.com](https://www.mongodb.com)),
    allowing users to design custom, potentially complex, filters as pure database
    queries for maximum flexibility. These filters, including the BTS alert filter,
    run on every alert.
  prefs: []
  type: TYPE_NORMAL
- en: If an alert passes a filter, it will be sent as a candidate to Fritz, a SkyPortal
    instance that serves as the ZTF collaboration’s Marshal. Two additional layers
    of filtering are applied to allow the automated saving and triggering of any instrument
    for which SkyPortal has a corresponding application programming interface (API).
    The first filtering layer assesses if the candidate should be saved, and the second
    assesses whether an instrument should be triggered for follow-up. Auto-triggering
    is only run on sources which have passed the auto-saving filter. In the case of
    BTSbot, two auto-saving and auto-triggering filters implemented, one for each
    of the policies bts_p1 and bts_p2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional features are implemented in SkyPortal to ensure that redundant triggers
    are not sent. A new, automatic follow-up request is prevented if a source already
    has a spectum, a classification, or if the instrument has already been triggered
    for that source. Instruments like SEDM which can conduct both spectroscopic and
    photometric observations get additional rules to define whether the new trigger
    is redundant with an existing one. Some teams direct scanners to maintain lists
    of sources that frequently pass their alert filter but are not of interest, e.g.
    the junk set (see Sec. [2](#S2 "2 Training data ‣ The Zwicky Transient Facility
    Bright Transient Survey. III. BTSbot: Automated Identification and Follow-up of
    Bright Transients with Deep Learning")). These can optionally be used to prevent
    a source from being auto-saved. The payload used for triggering an instrument,
    which contains the triggering instructions like the priority for SEDM, is set
    in advance alongside the Kowalski auto-triggering filter, but the priority assigned
    to a target can be dynamically increased as new alerts are posted. In particular,
    a source triggered on after passing bts_p1 can have its payload updated to priority
    2 if it passes bts_p2 before the trigger is completed.'
  prefs: []
  type: TYPE_NORMAL
- en: When auto-saving or auto-triggering actions are taken, comments recording these
    actions are posted to the relevant source page on Fritz. Beyond bookkeeping, this
    is crucial to facilitate scanners working alongside BTSbot by displaying BTSbot’s
    actions in the same interface where manual scanning is performed. Seamless integration
    with the tools that scanners already rely on enables joint-working that is more
    efficient and more reliable than either separately. This also allows scanners
    to modify triggers sent by BTSbot, for example, increasing the trigger’s priority
    or adding photometry to the request. These make real automated triggers safer
    and more dependable, making the automated aspects of BTS more easily monitored.
    The results from the SEDM observations and associated data products are uploaded
    back to Fritz for visualization.
  prefs: []
  type: TYPE_NORMAL
- en: 'BTSbot saved 296 sources to an internal BTS catalog while running in production
    during October 2023,^(17)^(17)17An older version of BTSbot (v1.0) was running
    in production during October 2023\. The production model presented here (v1.0.1)
    corrects for the sources with $m_{\mathrm{peak}}\leq 18.5$ in the dims query (see
    Sec. [2](#S2 "2 Training data ‣ The Zwicky Transient Facility Bright Transient
    Survey. III. BTSbot: Automated Identification and Follow-up of Bright Transients
    with Deep Learning")). Both versions yield nearly identical performance. and 92.6%
    of these were confirmed as extragalactic transients. SN 2023tyk (ZTF23abhvlji)
    is a Type Ia supernova that was identified and triggered on by BTSbot. The data
    collected by SEDM was then reduced by pySEDM (Rigault et al., [2019](#bib.bib77)),
    the spectrum classified as belonging to a Type Ia SN by SNIascore (Fremling et al.,
    [2021](#bib.bib33)), and the classification automatically reported to TNS. As
    detailed by Rehemtulla et al. ([2023b](#bib.bib75)), SN 2023tyk is the first transient
    to be fully automatically detected, identified, spectroscopically classified,
    and publicly reported. As of December 2023, more than a dozen additional Type
    Ia SNe have been both triggered on by BTSbot and spectroscopically classified
    by SNIascore: SN 2023vcz, SN 2023uxa, SN 2023uti, SN 2023vcx, SN 2023uty, SN 2023vtp,
    SN 2023vpd, SN 2023vip, SN 2023vwz, SN 2023wts, SN 2023xms, SN 2023xhc, SN 2023xkq.'
  prefs: []
  type: TYPE_NORMAL
- en: Operating BTSbot and SNIascore alongside each other allows for the full-automation
    of a significant amount of day-to-day tasks in BTS. About 70% of bright transients
    found by BTS are Type Ia supernovae (Fremling et al., [2020](#bib.bib32)), nearly
    all of which we expect to be identified by BTSbot and 80-90% of which will be
    classified and reported to TNS by SNIascore. If no human scanning took place,
    this BTS workflow could fully-automatically handle more than half of the bright
    transients in consideration by BTS. In practice, there are additional tasks which
    remain to be automated or made more efficient. Namely, scanning also involves
    the retrieving host galaxy redshifts from NED and classifications from TNS. We
    expect that much of these processes can be automated and planning for doing so
    is underway. While a comprehensive transition to automated scanning for BTS is
    unlikely (and not necessarily desirable), these tools mitigate the dependence
    of BTS operations on the natural fluctuations of scanners’ lives. Further, a reallocation
    of some human effort invested in BTS is possible when many tasks are automated.
    BTS experts are able to spend more time, e.g., analyzing bulk properties of SN
    samples or searching for rare or very young events.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Comparison with similar models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ALeRCE^(18)^(18)18[https://alerce.online/](https://alerce.online/) (Automatic
    Learning for the Rapid Classification of Events; Förster et al., [2021](#bib.bib31))
    real-time stamp classifier (Carrasco-Davis et al., [2021](#bib.bib16)) and the
    ACAI (Alert-Classifying Artificial Intelligence; Duev & van der Walt, [2021](#bib.bib27))
    framework are other image- and extracted feature-based MM-CNNs that perform classification
    on ZTF alert packets. BTSbot is very similar to these models in architecture (indeed,
    some aspects of BTSbot are inspired by them), but they are quite different in
    application.
  prefs: []
  type: TYPE_NORMAL
- en: 'The real-time stamp classifier predicts which one of five high-level classes
    (SN, AGN, VarStar, asteroid, bogus) the source in a ZTF alert belongs to. This
    is done, primarily, with the goal of automatic and rapid identification of SNe.
    To this end, the stamp-classifier is trained exclusively on the first alert packet
    from a given source. While rapid identification of SNe is an interest of ours
    (see Sec. [5.3](#S5.SS3 "5.3 An adaptation of BTSbot: Automatic, very rapid follow-up
    ‣ 5 Discussion ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot:
    Automated Identification and Follow-up of Bright Transients with Deep Learning")),
    we instead train BTSbot to function on any alert packet from relevant ZTF sources.
    This grants BTSbot the extra utility of being able to identify SNe at any phase
    of their evolution, albeit with the additional complications associated with doing
    so.'
  prefs: []
  type: TYPE_NORMAL
- en: The ACAI framework, a union of five independent binary classifiers, predicts
    which of five phenomenological features (hosted, orphan, nuclear, variable star,
    bogus) characterizes the source in a ZTF alert packet. These models are trained
    on any alert from any ZTF source and thus learn a much broader domain than BTSbot.
    Narrowing the input domain of our model reduces its broader utility relative to
    ACAI but unlocks greater performance for our particular task.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike BTSbot, neither the stamp classifier nor ACAI learn class definitions
    that are sensitive to the source’s brightness. BTSbot must learn to select just
    a subset of extragalactic transients, those with $m_{\mathrm{peak}}\,\leq\,18.5\,\mathrm{mag}$,
    and reject other extragalactic transients and all other sources. While BTSbot performs
    binary classification, effectively marginalizing over all non-bright transient
    classes, the stamp classifier performs five-class classification and the ACAI
    models perform five binary-classifications. This requires these other models,
    the stamp classifier in particular, to learn more discriminatory information between
    its five classes. This is not necessary for BTSbot because our primary interest
    is automating BTS scanning, for which only bright transients are relevant. Thus,
    we can justify combining non-bright transients into a single class, simplifying
    BTSbot’s task.
  prefs: []
  type: TYPE_NORMAL
- en: '5.3 An adaptation of BTSbot: Automatic, very rapid follow-up'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The development of BTSbot and the design of our policies prioritized completeness
    of bright transients over other metrics like purity and speed. After operating
    in production for a few months a different set of policies was implemented, which
    are focused on increasing purity to operate BTSbot with minimal intervention.
    These priorities are directed by the needs of BTS, but applications of a BTSbot-like
    model to other science efforts could prefer to prioritize other metrics.
  prefs: []
  type: TYPE_NORMAL
- en: BTSbot’s architecture is particularly well suited for the automated identification
    of very young transients. At early times (i.e., the night of the first detection),
    the source’s light curve is uninformative because it is comprised of a very small
    number of data points. Instead, much of the information available on the new transient
    is embedded in associated images. In this regime, one would expect that an image-based
    model, like BTSbot, would have better access to constraining information than
    a purely light-curve based model.
  prefs: []
  type: TYPE_NORMAL
- en: We take SN 2023ixf as an example illustrative of the additional discovery potential
    of a model like BTSbot. SN 2023ixf was a Type II SN in Messier 101\. Owing to
    its proximity, it provided a unique laboratory to study pre-supernova mass loss
    events of red supergiants, allowed the assembly of a comprehensive time-dependent
    description of the event, and much more (e.g.; Bostroem et al., [2023](#bib.bib11);
    Hiramatsu et al., [2023](#bib.bib43); Jacobson-Galán et al., [2023](#bib.bib49);
    Qin et al., [2023](#bib.bib73); Zimmerman et al., [2023](#bib.bib104)). SN 2023ixf
    was reported to TNS by Koichi Itagaki at 21:42 UTC on 19 May 2023 (Itagaki, [2023](#bib.bib47)).
    The earliest published spectrum was collected less than an hour later by Perley
    et al. ([2023](#bib.bib71)). About 14 hours before the first TNS report, SN 2023ixf
    was detected by ZTF, and, just minutes later, this alert packet was assigned a
    bright transient score of 0.840 by an early version of BTSbot.^(19)^(19)19The
    early version of BTSbot referenced here (v0.3) is presented in Rehemtulla et al.
    [2023a](#bib.bib74). A variant of BTSbot could be trained to isolate such sources.
    With an alert filter and policy suited for the search of young transients, this
    could have allowed for a more rapid identification and spectroscopic follow-up
    of SN 2023ixf. About half the observing night was remaining for SEDM at the time
    the first detection would have passed the auto-triggering filters, enough time
    to obtain a spectrum if triggered at high enough priority. If the spectrum is
    collected near the end of observing that night at Palomar Observatory ($\sim$12:00 UTC),
    this represents a $\sim$10 hour speed-up over the otherwise earliest spectrum
    obtained. In this example, BTSbot and the associated integration tools presented
    here allow the probing of early, rapidly-evolving explosion physics typically
    unavailable to traditional triggering methods.
  prefs: []
  type: TYPE_NORMAL
- en: There are a number of challenges related to this adaptation of BTSbot. Namely,
    the BTS alert filter and the bts_p1 and bts_p2 policies presented here would likely
    be inappropriate or sub-optimal following this new definition of the model’s priorities.
    A thorough exploration of how to best assemble a training set for this goal would
    also be required.
  prefs: []
  type: TYPE_NORMAL
- en: Automatic spectroscopic follow-up of infant SNe in ZTF data has been successfully
    implemented before in AMPEL (Nordin et al., [2019](#bib.bib66)). Target selection
    was driven by SNGuess (Miranda et al., [2022](#bib.bib61)), a decision tree-based
    ML system for identifying SNe. Their automated triggering was designed to observe
    nearby infant SNe, which was successfully done for a number of sources.
  prefs: []
  type: TYPE_NORMAL
- en: 'BTSbot’s source code and the production BTSbot model are publicly available
    on GitHub ([https://github.com/nabeelre/BTSbot](https://github.com/nabeelre/BTSbot)).
    This repository includes all codes necessary for assembling BTSbot’s training
    set, training the model, creating figures visualizing validation or test split
    performance, and more. It is written specifically with adaptability and flexibility
    in mind. Additional functionalities, e.g. training models with alternative architectures
    (see Appendix [B](#A2 "Appendix B Comparison with uni-modal architectures ‣ The
    Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated Identification
    and Follow-up of Bright Transients with Deep Learning")), are embedded in the
    scripts with minimal added complexity and no repeated code. This is done to facilitate
    ease in recycling the BTSbot code-base for other applications. One could, for
    example, quickly explore solving a problem with a simple fully-connected neural
    network and later advance to a MM-CNN with powerful features like data augmentation
    and Weights and Biases hyperparameter sweeps integration already built-in. We
    encourage the use of this resource by the community.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We have presented BTSbot, a new multi-modal convolutional neural network to
    automate scanning for the ZTF Bright Transient Survey. BTSbot uses ZTF image cutouts
    and metadata to produce a bright transient score for an individual ZTF alert packet.
    It achieves $\sim$95% accuracy on input alerts and identified 100% of bright transients
    in our test split with 93% purity. Performance compares very closely to that of
    human scanners in terms of completeness, purity, and speed to act on new transients.
    BTSbot only falls slightly short of scanners in terms of purity of the bright
    transient sample it produces: $93\%$ vs. $\sim 97\%$. We also perform an additional
    analysis with very recent BTS candidates to demonstrate that BTSbot is not impacted
    by a significant data shift.'
  prefs: []
  type: TYPE_NORMAL
- en: 'BTSbot has been fully integrated into Kowalski and Fritz, ZTF’s first-party
    alert broker and marshal, and now automatically sends spectroscopic follow-up
    requests for the new bright transients it identifies. BTSbot joins a family of
    automation tools in the BTS workflow (braai, sgscore, pySEDM, and SNIascore) which
    aid in running BTS efficiently. These models, coordinated by Fritz and Kowalski,
    have enabled the first fully automatic detection, identification, spectroscopic
    classification, and public reporting of a transient: SN 2023tyk. This milestone
    represents a boost in efficiency for BTS and an image of what time-domain astronomy
    could look like during the Rubin-era. It also hints towards the discovery potential
    of adapting similar technology to other areas of time-domain astronomy.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A great number of people have contributed to BTS and BTS scanning over the
    years. We thank the following people who have saved 10 or more sources to internal
    BTS catalogs on Fritz as of October 2023: Ivan Altunin, Raphael Baer-Way, Pallas
    A. Beddow, Ofek Bengiat, Joshua S. Bloom, Ola Bochenek, Emma Born, Kate Bostow,
    Victoria Mei Brendel, Rachel Bruch, Vidhi Chander, Matthew Chu, Elma Chuang, Aishwarya
    Dahiwale, Asia deGraw, Dmitry Duev, Kingsley Ehrich, Eli Gendreau-Distler, Nachiket
    Girish, Xander Hall, K-Ryan Hinds, Ido Irani, Cooper Jacobus, Connor Jennings,
    Joel Johansson, Snehaa Ganesh Kumar, Michael May, William Meynardie, Shaunak Modak,
    Kishore Patra, Neil Pichay, Sophia Risin, Yashvi Sharma, Gabrielle Stewart, Nora
    Linn Strotjohann, James Sunseri, Edgar Vidal, Jacob Wise, Abel Yagubyan, Yoomee
    Zeng, and Erez A. Zimmerman.'
  prefs: []
  type: TYPE_NORMAL
- en: We also thank Jakob Nordin for discussions relating to AMPEL.
  prefs: []
  type: TYPE_NORMAL
- en: The material contained in this document is based upon work supported by a National
    Aeronautics and Space Administration (NASA) grant or cooperative agreement. Any
    opinions, findings, conclusions, or recommendations expressed in this material
    are those of the author and do not necessarily reflect the views of NASA. This
    work was supported through a NASA grant awarded to the Illinois/NASA Space Grant
    Consortium.
  prefs: []
  type: TYPE_NORMAL
- en: This research was supported in part through the computational resources and
    staff contributions provided for the Quest high performance computing facility
    at Northwestern University which is jointly supported by the Office of the Provost,
    the Office for Research, and Northwestern University Information Technology.
  prefs: []
  type: TYPE_NORMAL
- en: Based on observations obtained with the Samuel Oschin Telescope 48-inch and
    the 60-inch Telescope at the Palomar Observatory as part of the Zwicky Transient
    Facility project. ZTF is supported by the National Science Foundation under Grants
    No. AST-1440341 and AST-2034437 and a collaboration including current partners
    Caltech, IPAC, the Oskar Klein Center at Stockholm University, the University
    of Maryland, University of California, Berkeley , the University of Wisconsin
    at Milwaukee, University of Warwick, Ruhr University, Cornell University, Northwestern
    University and Drexel University. Operations are conducted by COO, IPAC, and UW.
  prefs: []
  type: TYPE_NORMAL
- en: SED Machine is based upon work supported by the National Science Foundation
    under Grant No. 1106171
  prefs: []
  type: TYPE_NORMAL
- en: The Gordon and Betty Moore Foundation, through both the Data-Driven Investigator
    Program and a dedicated grant, provided critical funding for SkyPortal.
  prefs: []
  type: TYPE_NORMAL
- en: N. Rehemtulla and A. A. Miller are partially supported by LBNL Subcontract NO. 7707915.
  prefs: []
  type: TYPE_NORMAL
- en: M. W. Coughlin acknowledges support from the National Science Foundation with
    grant numbers PHY-2308862 and PHY-2117997.
  prefs: []
  type: TYPE_NORMAL
- en: SRK thanks the Heising-Simons Foundation for supporting his research.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix A Accuracy and loss evolution during training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3fd8fe3440fc9345fb670561f055dac1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Accuracy (top) and loss (bottom) for the final BTSbot models (production
    model: bold curve; other trials: narrow curves) of the train (blue) and validation
    (green) splits during training as a function of epoch. Gray vertical lines indicate
    epochs at which the learning rate decreased. The discrepancies in these metrics
    between the train and validation splits cannot be interpreted as overfitting alone
    because they are partially due to differences in alert thinning and class weighting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [11](#A1.F11 "Figure 11 ‣ Appendix A Accuracy and loss evolution during
    training ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot:
    Automated Identification and Follow-up of Bright Transients with Deep Learning")
    shows how the accuracy and loss evolve during the training of the final BTSbot models,
    including the production model. Differences in how the $N_{\mathrm{max}}$ alert
    thinning is applied between train and validation (see Sec. [2](#S2 "2 Training
    data ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning")) and the
    use of class weights during training can explain much of the difference between
    their respective loss curves. Thus, the train loss being significantly lower than
    validation loss cannot be interpreted as overfitting. There does appear to be
    overfitting in the production model (bold curves) past epoch $\sim$30, evidenced
    by the validation loss beginning to increase while the train loss continues to
    decrease. We avoid this by selecting the model that produced the minimum validation
    loss during training, rather than the model from the final epoch of training.
    Figure [11](#A1.F11 "Figure 11 ‣ Appendix A Accuracy and loss evolution during
    training ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot:
    Automated Identification and Follow-up of Bright Transients with Deep Learning")
    marks the epochs where the learning rate decreased due to a plateau in the validation
    loss as vertical gray lines. The learning rate decreases three times, spanning
    from $\alpha=10^{-4}$ to $\alpha=6\times 10^{-6}$. We also observe that the selected
    production model is not the model that produced the least validation loss or had
    the greatest validation accuracy. The production model is selected by considering
    best performance in policy-based metrics, those most relevant to how BTSbot is
    used.'
  prefs: []
  type: TYPE_NORMAL
- en: Training this production model took $\sim$32 hours on an Intel Xeon 6230 CPU,
    and inference on a single alert packet takes less than a hundredth of a second
    on a laptop CPU.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Comparison with uni-modal architectures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An exploration of simpler, alternative architectures and a characterization
    of their performance is necessary to justify BTSbot’s chosen multi-modal architecture.
    We present alert- and policy-based performance metrics produced by two uni-modal
    model architectures: a uni-modal convolutional neural network (UM-CNN) and a fully-connected
    neural network (NN). These networks perform inference using only images and only
    metadata respectively. The training data, image preprocessing, and feature extraction
    are, where relevant, performed identically for all three architectures. The ordering
    and types of layers of the uni-modal architectures are identical to those of the
    multi-modal architecture with the other branch entirely removed. The hyperparameters
    of the layers, the Adam optimizer, and the value of $N_{\mathrm{max}}$ are searched
    over in a Bayesian hyperparameter sweep similarly to the sweeps executed for the
    MM-CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#A2.T5 "Table 5 ‣ Appendix B Comparison with uni-modal architectures
    ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning") shows the
    optimal layer hyperparameters for either architecture. Compared to the MM-CNN’s
    convolutional branch, the UM-CNN has smaller convolutional kernels and a much
    larger dense layer following the flattening. The UM-CNN’s first two convolutional
    layers have more filters than the final two convolutional layers, whereas the
    pattern is reversed for the MM-CNN. We also find that the optimized UM-CNN has
    $\textrm{batch size}=32$, and Adam parameters $\beta_{1}=\beta_{2}=0.999$. Compared
    to the MM-CNN’s metadata branch, the optimized NN has half as many neurons in
    the first two dense layers but eight times as many in the third layer. We also
    find that the NN performs best with $\textrm{batch size}=128$, Adam parameters
    $\beta_{1}=\beta_{2}=0.999$, and $N_{\mathrm{max}}=30$. The hyperparameter values
    which are not explicitly mentioned can be assumed to be identical to that of the
    MM-CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Architectures of uni-modal BTSbot alternatives'
  prefs: []
  type: TYPE_NORMAL
- en: '| Uni-modal convolutional neural network | Fully-connected neural network |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Layer type | Layer parameters | Layer type | Layer parameters |'
  prefs: []
  type: TYPE_TB
- en: '| 2D Convolution | 64 filters, $3\times 3$ kernel | Batch normalization | -
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2D Convolution | 64 filters, $3\times 3$ kernel | Dense | 64 units |'
  prefs: []
  type: TYPE_TB
- en: '| Max pooling | $2\times 2$ kernel | Dropout | 0.40 |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | 0.45 | Dense | 64 units |'
  prefs: []
  type: TYPE_TB
- en: '| 2D Convolution | 16 filters, $3\times 3$ kernel | Dense | 64 units |'
  prefs: []
  type: TYPE_TB
- en: '| 2D Convolution | 16 filters, $3\times 3$ kernel | Dropout | 0.70 |'
  prefs: []
  type: TYPE_TB
- en: '| Max pooling | $4\times 4$ kernel | Dense | 1 unit |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | 0.65 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Flattening | - |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | 128 units |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dropout | 0.45 |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Dense | 1 unit |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Note. — Layer parameters of alternative BTSbot architectures: uni-modal convolutional
    neural network (left two columns) and fully-connected neural network (right two
    columns). Optimal hyperparameters are determined by large hyperparameter sweeps.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Performance metrics of uni-modal BTSbot alternatives'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture type | Alert-based metrics | Policy-based metrics |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Accuracy | ROCAUC | Completeness | bts_p1 purity | bts_p2 purity | $\Delta
    t_{\mathrm{save}}$ | $\Delta t_{\mathrm{trigger}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| NN | 95.5% | 0.988 | 99.8% | 82.8% | 91.2% | -0.0400 days | -0.0200 days
    |'
  prefs: []
  type: TYPE_TB
- en: '| UM-CNN | 82.4% | 0.902 | 94.3% | 69.5% | 92.5% | -0.0485 days | -0.0174 days
    |'
  prefs: []
  type: TYPE_TB
- en: '| MM-CNN | $94.9\%$ | $0.985$ | 100.0% | 84.6% | 93.0% | -0.0381 days | -0.0147 days
    |'
  prefs: []
  type: TYPE_TB
- en: 'Note. — Comparison of performance metrics on test split data across three different
    model architectures: a fully-connected neural network (NN), a uni-modal convolutional
    neural network (UM-CNN), and a multi-modal convolutional neural network (MM-CNN).
    Each network is the highest performing in at least one metric. The UM-CNN is less
    complete than the MM-CNN and both the NN and the UM-CNN fall short of the MM-CNN
    in purity. Overall, the MM-CNN is the best performing architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [6](#A2.T6 "Table 6 ‣ Appendix B Comparison with uni-modal architectures
    ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated
    Identification and Follow-up of Bright Transients with Deep Learning") compares
    test split performance metrics from each of the three architectures. The NN slightly
    outperforms the MM-CNN in the alert-based metrics while the UM-CNN falls well
    short of the others. The UM-CNN presented here uses the full-size image cutouts,
    but Appendix [C](#A3 "Appendix C Convolutional neural networks with cropped image
    cutouts ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot:
    Automated Identification and Follow-up of Bright Transients with Deep Learning")
    shows that slightly increased performance can be realized through the reduction
    of input image’s size. In policy-based metrics, the MM-CNN’s marginal advantage
    in completeness over the NN is due to having just a single fewer FN. The MM-CNN
    demonstrates a larger advantage in purity, however. Its advantage is $\sim 0.5-2\%$
    over the others in bts_p2 purity; even small boosts in purity are valuable when
    applied over the large samples and baselines in consideration by BTS. Each model
    performs very similarly in the speed metrics, although the NN and the UM-CNN marginally
    outperform the others in $\Delta t_{\mathrm{trigger}}$ and $\Delta t_{\mathrm{save}}$
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Each architecture outperforms the others in some metrics, but the MM-CNN delivers
    the best overall performance. It delivers the highest completeness and purity
    while sacrificing well less than an hour in $\Delta t_{\mathrm{save/trigger}}$.
  prefs: []
  type: TYPE_NORMAL
- en: Despite having worse purity than the MM-CNN, there is still valuable utility
    in the NN because we find that it trains $\sim 60$ times faster than the MM-CNN.
    This discrepancy in training time is partially due to the NN adopting $N_{\mathrm{max}}=30$
    instead of $N_{\mathrm{max}}=100$ as the MM-CNN does, and could be also decreased
    by training the MM-CNN on a GPU. These factors aside, the NN would very likely
    remain many factors quicker to train, and it is thus better suited for experimenting
    with adapting BTSbot to alternative use cases. In developing adaptations of BTSbot,
    being able to very quickly design and execute experiments will be key. Excluding
    images may also lend advantages in the ease of transferring BTSbot adaptations
    to other surveys, for example, LSST or the upcoming La Silla Schmidt Southern
    Survey (LS4^(20)^(20)20[https://www.snowmass21.org/docs/files/summaries/CF/SNOWMASS21-CF6_CF4_Peter_Nugent-171.pdf](https://www.snowmass21.org/docs/files/summaries/CF/SNOWMASS21-CF6_CF4_Peter_Nugent-171.pdf)).
    Although effort has been made to develop survey-agnostic CNNs (e.g., Cabrera-Vives
    et al., [2023](#bib.bib14)), BTSbot is not designed or expected to perform consistently
    on image data from other surveys. Instead, one could more easily design a NN which
    exclusively uses survey-agnostic features, increasing its potential impact by
    allowing it to be applied more widely.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix C Convolutional neural networks with cropped image cutouts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The properties of the image cutouts produced by large wide-field surveys are
    critical to a number of the survey’s functions. While the pixel scale, typically
    measured in arcseconds (”) per pixel, is a fixed property of the telescope’s optics,
    many choices can be made in software that determine the nature of the cutouts
    sent to alert brokers. One must choose the angular and pixel size of the cutouts
    in light of the pixel scale and whether or not to decrease resolution by binning
    the pixels. A tension arises because scanners generally prefer having more information,
    i.e. cutouts with higher resolution and greater angular size, but the surveys
    producing and disseminating the alerts prefer minimal network bandwidth costs,
    i.e. lower resolution and smaller angular size. Smaller cutouts with less information
    may compromise the performance of these key ML models, however, possibly reducing
    the scientific potential of the entire survey. Only a small number of studies
    developing CNNs for large wide-field surveys comment on performance over a range
    of cutout sizes (e.g.; Killestein et al., [2021](#bib.bib52); Carrasco-Davis et al.,
    [2021](#bib.bib16); Reyes-Jainaga et al., [2023](#bib.bib76)). It remains to be
    seen, then, what the minimum cutout size is to maintain acceptable performance
    with current CNNs. Adding further complexity, CNNs performing different tasks
    will be impacted differently by reduced cutout sizes, and MM-CNNs may be more
    resilient to reduced cutout sizes due to the availability of metadata information.
    Multi-scale cutouts, where resolution progressively decreases moving away from
    the cutout center, are a compelling alternative because they dramatically increase
    the available field-of-view without an increase to the data volume of the cutouts
    (Reyes-Jainaga et al., [2023](#bib.bib76)). Further study of (MM-)CNN performance
    over a range of cutout sizes is necessary to better characterize their correlation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b1ea0fd2b701e30bf690eaad82f9fa5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Test set accuracy of BTSbot UM-CNNs as a function of input image
    cutout size. Very small cutouts ($3\times 3$ to $13\times 13$ pixels) clearly
    under-perform relative to models with larger cutouts ($35\times 35$ to $63\times
    63$ pixels). The highest performing model is notably not that which uses the full-size
    images but rather the model which uses $49\times 49$ pixel cutouts. Cropping cutouts
    could allow for the significant decrease of a survey’s data rate and possibly
    network bandwidth costs, in this case, with no decrease in the performance of
    ML-based transient detection tools.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To explore this important issue, we train multiple UM-CNN versions of BTSbot to
    identify which cutout size produces the most accurate model. We create new training
    sets from BTSbot’s original training set by keeping only the innermost $N_{\mathrm{pix}}\times
    N_{\mathrm{pix}}$ pixels of each cutout and renormalizing them individually. We
    train $\sim$30-60 trials of BTSbot with a given value of $N_{\mathrm{pix}}$ (3,
    5, 7, 13, 21, 35, or 49) in Bayesian hyperparameter sweeps similar to Section [3.1](#S3.SS1
    "3.1 Training and hyperparameter optimization ‣ 3 BTSbot Scope, Architecture,
    and Training ‣ The Zwicky Transient Facility Bright Transient Survey. III. BTSbot:
    Automated Identification and Follow-up of Bright Transients with Deep Learning")
    and Appendix [B](#A2 "Appendix B Comparison with uni-modal architectures ‣ The
    Zwicky Transient Facility Bright Transient Survey. III. BTSbot: Automated Identification
    and Follow-up of Bright Transients with Deep Learning"). These trials are unlikely
    sufficient to yield optimal hyperparameters, but they do give a representative
    view of the performance for each value of $N_{\mathrm{pix}}$. For models with
    $N_{\mathrm{pix}}\leq 13$, we remove the pooling layers entirely to maintain the
    image’s size through the hidden layers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [12](#A3.F12 "Figure 12 ‣ Appendix C Convolutional neural networks with
    cropped image cutouts ‣ The Zwicky Transient Facility Bright Transient Survey.
    III. BTSbot: Automated Identification and Follow-up of Bright Transients with
    Deep Learning") presents the highest performing models judged by accuracy on the
    test set. We find that the $N_{\mathrm{pix}}=49$ UM-CNN yields the best performance,
    and that both the $N_{\mathrm{pix}}=49$ and $N_{\mathrm{pix}}=35$ UM-CNNs marginally
    outperform the UM-CNN with uncropped cutouts ($N_{\mathrm{pix}}=63$). It is not
    unambiguously clear what the reason for this is from this simple experiment alone,
    however, these results suggest that information $\sim 0.5^{\prime}$ away from
    the source in question tends to be irrelevant or noisy when performing our task.
    We notably do not observe the same trend for the MM-CNN, where the $N_{\mathrm{pix}}=63$
    model is the highest performing. Carrasco-Davis et al. ([2021](#bib.bib16)) find
    that the ALeRCE stamp classifier (a MM-CNN), performs best with $N_{\mathrm{pix}}=21$
    cutouts. Together, this indicates that more work is necessary to better understand
    the optimal cutout size for upcoming surveys.'
  prefs: []
  type: TYPE_NORMAL
- en: The $N_{\mathrm{pix}}=35$ model roughly matches the $N_{\mathrm{pix}}=63$ model
    in accuracy but has $35^{2}/63^{2}\approx 30\%$ the number of pixels. In this
    case, shrinking cutouts dramatically quickens BTSbot’s training without compromising
    performance. Shrinking cutouts survey-wide is an option to reduce a survey’s network
    bandwidth costs significantly as image cutouts typically comprise a large fraction
    of the bytes in an alert packet. Although $35\times 35$ cutouts are appropriate
    for BTSbot, such small cutouts may reduce scanners’ ability to distinguish sources
    of different types. Experiments of this sort are most relevant to the alert stream
    design of upcoming surveys like LSST and LS4\. LSST’s planned alert packet cutout
    size is at least $30\times 30$ pixels^(21)^(21)21[https://dmtn-102.lsst.io](https://dmtn-102.lsst.io)
    at a pixel scale of 0.2”/pixel (Ivezić et al., [2019](#bib.bib48)), thus spanning
    6” on a side. The $N_{\mathrm{pix}}=5$ and $N_{\mathrm{pix}}=7$ models are nearest
    to this in terms of field-of-view, however, ZTF’s much larger pixel scale (1”/pixel)
    makes this an unreasonable comparison. Further study is required to assess how
    small cutouts can be without compromising, e.g., real/bogus performance at LSST-like
    resolutions. LS4 will have a pixel scale very similar to that of ZTF, but, at
    the time of writing, the alert packet cutout size is undetermined (R. Knop, private
    communications). These results provide the preliminary guidance that LS4 cutouts
    should be no smaller than $21\times 21$ in order to maintain the performance of
    BTSbot-like UM-CNN models.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abadi et al. (2015) Abadi, M., Agarwal, A., Barham, P., et al. 2015, TensorFlow:
    Large-Scale Machine Learning on Heterogeneous Systems. [https://www.tensorflow.org/](https://www.tensorflow.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Astropy Collaboration et al. (2013) Astropy Collaboration, Robitaille, T. P.,
    Tollerud, E. J., et al. 2013, A&A, 558, A33, doi: [10.1051/0004-6361/201322068](http://doi.org/10.1051/0004-6361/201322068)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Astropy Collaboration et al. (2018) Astropy Collaboration, Price-Whelan, A. M.,
    Sipőcz, B. M., et al. 2018, AJ, 156, 123, doi: [10.3847/1538-3881/aabc4f](http://doi.org/10.3847/1538-3881/aabc4f)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bailey et al. (2007) Bailey, S., Aragon, C., Romano, R., et al. 2007, ApJ, 665,
    1246, doi: [10.1086/519832](http://doi.org/10.1086/519832)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellm et al. (2019a) Bellm, E. C., Kulkarni, S. R., Graham, M. J., et al. 2019a,
    PASP, 131, 018002, doi: [10.1088/1538-3873/aaecbe](http://doi.org/10.1088/1538-3873/aaecbe)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bellm et al. (2019b) Bellm, E. C., Kulkarni, S. R., Barlow, T., et al. 2019b,
    PASP, 131, 068003, doi: [10.1088/1538-3873/ab0c2a](http://doi.org/10.1088/1538-3873/ab0c2a)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biewald (2020) Biewald, L. 2020, Experiment Tracking with Weights and Biases.
    [https://www.wandb.com/](https://www.wandb.com/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blagorodnova et al. (2018) Blagorodnova, N., Neill, J. D., Walters, R., et al.
    2018, PASP, 130, 035003, doi: [10.1088/1538-3873/aaa53f](http://doi.org/10.1088/1538-3873/aaa53f)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bloom et al. (2012) Bloom, J. S., Richards, J. W., Nugent, P. E., et al. 2012,
    PASP, 124, 1175, doi: [10.1086/668468](http://doi.org/10.1086/668468)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Boone (2019) Boone, K. 2019, AJ, 158, 257, doi: [10.3847/1538-3881/ab5182](http://doi.org/10.3847/1538-3881/ab5182)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bostroem et al. (2023) Bostroem, K. A., Pearson, J., Shrestha, M., et al. 2023,
    ApJ, 956, L5, doi: [10.3847/2041-8213/acf9a4](http://doi.org/10.3847/2041-8213/acf9a4)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breiman (2001) Breiman, L. 2001, Machine Learning, 45, 5, doi: [10.1023/A:1010933404324](http://doi.org/10.1023/A:1010933404324)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brink et al. (2013) Brink, H., Richards, J. W., Poznanski, D., et al. 2013,
    MNRAS, 435, 1047, doi: [10.1093/mnras/stt1306](http://doi.org/10.1093/mnras/stt1306)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cabrera-Vives et al. (2023) Cabrera-Vives, G., Bolivar, C., Förster, F., et al.
    2023, arXiv e-prints, arXiv:2308.07538, doi: [10.48550/arXiv.2308.07538](http://doi.org/10.48550/arXiv.2308.07538)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cabrera-Vives et al. (2017) Cabrera-Vives, G., Reyes, I., Förster, F., Estévez,
    P. A., & Maureira, J.-C. 2017, ApJ, 836, 97, doi: [10.3847/1538-4357/836/1/97](http://doi.org/10.3847/1538-4357/836/1/97)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carrasco-Davis et al. (2021) Carrasco-Davis, R., Reyes, E., Valenzuela, C.,
    et al. 2021, AJ, 162, 231, doi: [10.3847/1538-3881/ac0ef1](http://doi.org/10.3847/1538-3881/ac0ef1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carrasco Kind & Brunner (2013) Carrasco Kind, M., & Brunner, R. J. 2013, MNRAS,
    432, 1483, doi: [10.1093/mnras/stt574](http://doi.org/10.1093/mnras/stt574)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chollet et al. (2015) Chollet, F., et al. 2015, Keras, [https://keras.io](https://keras.io)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cold & Hjorth (2023) Cold, C., & Hjorth, J. 2023, A&A, 670, A48, doi: [10.1051/0004-6361/202244867](http://doi.org/10.1051/0004-6361/202244867)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cook et al. (2019) Cook, D. O., Kasliwal, M. M., Sistine, A. V., et al. 2019,
    The Astrophysical Journal, 880, 7, doi: [10.3847/1538-4357/ab2131](http://doi.org/10.3847/1538-4357/ab2131)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Coughlin et al. (2023) Coughlin, M. W., Bloom, J. S., Nir, G., et al. 2023,
    arXiv e-prints, arXiv:2305.00108, doi: [10.48550/arXiv.2305.00108](http://doi.org/10.48550/arXiv.2305.00108)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: da Costa-Luis (2019) da Costa-Luis, C. 2019, The Journal of Open Source Software,
    4, 1277, doi: [10.21105/joss.01277](http://doi.org/10.21105/joss.01277)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dekany et al. (2020) Dekany, R., Smith, R. M., Riddle, R., et al. 2020, PASP,
    132, 038001, doi: [10.1088/1538-3873/ab4ca2](http://doi.org/10.1088/1538-3873/ab4ca2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dieleman et al. (2015) Dieleman, S., Willett, K. W., & Dambre, J. 2015, MNRAS,
    450, 1441, doi: [10.1093/mnras/stv632](http://doi.org/10.1093/mnras/stv632)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Domínguez Sánchez et al. (2018) Domínguez Sánchez, H., Huertas-Company, M.,
    Bernardi, M., Tuccillo, D., & Fischer, J. L. 2018, MNRAS, 476, 3661, doi: [10.1093/mnras/sty338](http://doi.org/10.1093/mnras/sty338)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Duev et al. (2021) Duev, D., Shin, K. M., & Singer, L. 2021, dmitryduev/penquins:
    a python client for dmitryduev/kowalski, v2.1.2, Zenodo, doi: [10.5281/zenodo.5651471](http://doi.org/10.5281/zenodo.5651471)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duev & van der Walt (2021) Duev, D. A., & van der Walt, S. J. 2021, arXiv e-prints,
    arXiv:2111.12142, doi: [10.48550/arXiv.2111.12142](http://doi.org/10.48550/arXiv.2111.12142)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Duev et al. (2019) Duev, D. A., Mahabal, A., Masci, F. J., et al. 2019, MNRAS,
    489, 3582, doi: [10.1093/mnras/stz2357](http://doi.org/10.1093/mnras/stz2357)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flesch (2019) Flesch, E. W. 2019, VizieR Online Data Catalog, VII/283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Foreman-Mackey (2016) Foreman-Mackey, D. 2016, The Journal of Open Source Software,
    1, 24, doi: [10.21105/joss.00024](http://doi.org/10.21105/joss.00024)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Förster et al. (2021) Förster, F., Cabrera-Vives, G., Castillo-Navarrete, E.,
    et al. 2021, AJ, 161, 242, doi: [10.3847/1538-3881/abe9bc](http://doi.org/10.3847/1538-3881/abe9bc)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fremling et al. (2020) Fremling, C., Miller, A. A., Sharma, Y., et al. 2020,
    ApJ, 895, 32, doi: [10.3847/1538-4357/ab8943](http://doi.org/10.3847/1538-4357/ab8943)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fremling et al. (2021) Fremling, C., Hall, X. J., Coughlin, M. W., et al. 2021,
    ApJ, 917, L2, doi: [10.3847/2041-8213/ac116f](http://doi.org/10.3847/2041-8213/ac116f)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fukushima & Miyake (1982) Fukushima, K., & Miyake, S. 1982, Pattern Recognition,
    15, 455, doi: [10.1016/0031-3203(82)90024-3](http://doi.org/10.1016/0031-3203(82)90024-3)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gagliano et al. (2023) Gagliano, A., Contardo, G., Foreman Mackey, D., Malz,
    A. I., & Aleo, P. D. 2023, arXiv e-prints, arXiv:2305.08894. [https://arxiv.org/abs/2305.08894](https://arxiv.org/abs/2305.08894)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goldstein et al. (2015) Goldstein, D. A., D’Andrea, C. B., Fischer, J. A., et al.
    2015, AJ, 150, 82, doi: [10.1088/0004-6256/150/3/82](http://doi.org/10.1088/0004-6256/150/3/82)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gomez et al. (2020) Gomez, S., Berger, E., Blanchard, P. K., et al. 2020, ApJ,
    904, 74, doi: [10.3847/1538-4357/abbf49](http://doi.org/10.3847/1538-4357/abbf49)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gomez et al. (2023) Gomez, S., Villar, V. A., Berger, E., et al. 2023, ApJ,
    949, 113, doi: [10.3847/1538-4357/acc535](http://doi.org/10.3847/1538-4357/acc535)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goobar et al. (2023) Goobar, A., Johansson, J., Schulze, S., et al. 2023, Nature
    Astronomy, 7, 1137, doi: [10.1038/s41550-023-02034-5](http://doi.org/10.1038/s41550-023-02034-5)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graham et al. (2019) Graham, M. J., Kulkarni, S. R., Bellm, E. C., et al. 2019,
    PASP, 131, 078001, doi: [10.1088/1538-3873/ab006c](http://doi.org/10.1088/1538-3873/ab006c)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Harris et al. (2020) Harris, C. R., Millman, K. J., van der Walt, S. J., et al.
    2020, Nature, 585, 357–362, doi: [10.1038/s41586-020-2649-2](http://doi.org/10.1038/s41586-020-2649-2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks & Gimpel (2016) Hendrycks, D., & Gimpel, K. 2016, arXiv e-prints,
    arXiv:1610.02136, doi: [10.48550/arXiv.1610.02136](http://doi.org/10.48550/arXiv.1610.02136)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hiramatsu et al. (2023) Hiramatsu, D., Tsuna, D., Berger, E., et al. 2023, ApJ,
    955, L8, doi: [10.3847/2041-8213/acf299](http://doi.org/10.3847/2041-8213/acf299)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hosseinzadeh et al. (2020) Hosseinzadeh, G., Dauphin, F., Villar, V. A., et al.
    2020, ApJ, 905, 93, doi: [10.3847/1538-4357/abc42b](http://doi.org/10.3847/1538-4357/abc42b)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hunter (2007) Hunter, J. D. 2007, Computing in Science and Engineering, 9, 90,
    doi: [10.1109/MCSE.2007.55](http://doi.org/10.1109/MCSE.2007.55)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irani et al. (2022) Irani, I., Prentice, S. J., Schulze, S., et al. 2022, ApJ,
    927, 10, doi: [10.3847/1538-4357/ac4709](http://doi.org/10.3847/1538-4357/ac4709)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Itagaki (2023) Itagaki, K. 2023, Transient Name Server Discovery Report, 2023-1158,
    1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivezić et al. (2019) Ivezić, Ž., Kahn, S. M., Tyson, J. A., et al. 2019, ApJ,
    873, 111, doi: [10.3847/1538-4357/ab042c](http://doi.org/10.3847/1538-4357/ab042c)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jacobson-Galán et al. (2023) Jacobson-Galán, W. V., Dessart, L., Margutti, R.,
    et al. 2023, ApJ, 954, L42, doi: [10.3847/2041-8213/acf2ec](http://doi.org/10.3847/2041-8213/acf2ec)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaiser et al. (2002) Kaiser, N., Aussel, H., Burke, B. E., et al. 2002, in Society
    of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, Vol. 4836,
    Survey and Other Telescope Technologies and Discoveries, ed. J. A. Tyson & S. Wolff,
    154–164, doi: [10.1117/12.457365](http://doi.org/10.1117/12.457365)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kasliwal et al. (2019) Kasliwal, M. M., Cannella, C., Bagdasaryan, A., et al.
    2019, PASP, 131, 038003, doi: [10.1088/1538-3873/aafbc2](http://doi.org/10.1088/1538-3873/aafbc2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Killestein et al. (2021) Killestein, T. L., Lyman, J., Steeghs, D., et al. 2021,
    MNRAS, 503, 4838, doi: [10.1093/mnras/stab633](http://doi.org/10.1093/mnras/stab633)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kim et al. (2022) Kim, Y. L., Rigault, M., Neill, J. D., et al. 2022, PASP,
    134, 024505, doi: [10.1088/1538-3873/ac50a0](http://doi.org/10.1088/1538-3873/ac50a0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Ba (2014) Kingma, D. P., & Ba, J. 2014, arXiv e-prints, arXiv:1412.6980,
    doi: [10.48550/arXiv.1412.6980](http://doi.org/10.48550/arXiv.1412.6980)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kluyver et al. (2016) Kluyver, T., Ragan-Kelley, B., Pérez, F., et al. 2016,
    in IOS Press, 87–90, doi: [10.3233/978-1-61499-649-1-87](http://doi.org/10.3233/978-1-61499-649-1-87)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lanusse et al. (2018) Lanusse, F., Ma, Q., Li, N., et al. 2018, MNRAS, 473,
    3895, doi: [10.1093/mnras/stx1665](http://doi.org/10.1093/mnras/stx1665)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (2015) LeCun, Y., Bengio, Y., & Hinton, G. 2015, Nature, 521, 436,
    doi: [10.1038/nature14539](http://doi.org/10.1038/nature14539)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahabal et al. (2019) Mahabal, A., Rebbapragada, U., Walters, R., et al. 2019,
    PASP, 131, 038002, doi: [10.1088/1538-3873/aaf3fa](http://doi.org/10.1088/1538-3873/aaf3fa)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masci et al. (2019) Masci, F. J., Laher, R. R., Rusholme, B., et al. 2019, PASP,
    131, 018003, doi: [10.1088/1538-3873/aae8ac](http://doi.org/10.1088/1538-3873/aae8ac)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: McCulloch & Pitts (1943) McCulloch, W. S., & Pitts, W. 1943, The Bulletin of
    Mathematical Biophysics, 5, 115, doi: [10.1007/bf02478259](http://doi.org/10.1007/bf02478259)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Miranda et al. (2022) Miranda, N., Freytag, J. C., Nordin, J., et al. 2022,
    A&A, 665, A99, doi: [10.1051/0004-6361/202243668](http://doi.org/10.1051/0004-6361/202243668)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morgan et al. (2022) Morgan, R., Nord, B., Bechtol, K., et al. 2022, ApJ, 927,
    109, doi: [10.3847/1538-4357/ac5178](http://doi.org/10.3847/1538-4357/ac5178)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Morgan et al. (2023) —. 2023, ApJ, 943, 19, doi: [10.3847/1538-4357/ac721b](http://doi.org/10.3847/1538-4357/ac721b)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muthukrishna et al. (2019) Muthukrishna, D., Narayan, G., Mandel, K. S., Biswas,
    R., & Hložek, R. 2019, PASP, 131, 118002, doi: [10.1088/1538-3873/ab1609](http://doi.org/10.1088/1538-3873/ab1609)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nair & Hinton (2010) Nair, V., & Hinton, G. E. 2010, in Proceedings of the
    27th International Conference on International Conference on Machine Learning,
    ICML’10 (Madison, WI, USA: Omnipress), 807–814'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nordin et al. (2019) Nordin, J., Brinnel, V., van Santen, J., et al. 2019, A&A,
    631, A147, doi: [10.1051/0004-6361/201935634](http://doi.org/10.1051/0004-6361/201935634)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'pandas development team (2020) pandas development team, T. 2020, pandas-dev/pandas:
    Pandas, latest, Zenodo, doi: [10.5281/zenodo.3509134](http://doi.org/10.5281/zenodo.3509134)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pasquet et al. (2019) Pasquet, J., Bertin, E., Treyer, M., Arnouts, S., & Fouchez,
    D. 2019, A&A, 621, A26, doi: [10.1051/0004-6361/201833617](http://doi.org/10.1051/0004-6361/201833617)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patterson et al. (2019) Patterson, M. T., Bellm, E. C., Rusholme, B., et al.
    2019, PASP, 131, 018001, doi: [10.1088/1538-3873/aae904](http://doi.org/10.1088/1538-3873/aae904)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pedregosa et al. (2011) Pedregosa, F., Varoquaux, G., Gramfort, A., et al. 2011,
    Journal of Machine Learning Research, 12, 2825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perley et al. (2023) Perley, D. A., Gal-Yam, A., Irani, I., & Zimmerman, E.
    2023, Transient Name Server AstroNote, 119, 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perley et al. (2020) Perley, D. A., Fremling, C., Sollerman, J., et al. 2020,
    ApJ, 904, 35, doi: [10.3847/1538-4357/abbd98](http://doi.org/10.3847/1538-4357/abbd98)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qin et al. (2023) Qin, Y.-J., Zhang, K., Bloom, J., et al. 2023, arXiv e-prints,
    arXiv:2309.10022, doi: [10.48550/arXiv.2309.10022](http://doi.org/10.48550/arXiv.2309.10022)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rehemtulla et al. (2023a) Rehemtulla, N., Miller, A. A., Coughlin, M. W., &
    Jegou du Laz, T. 2023a, arXiv e-prints, arXiv:2307.07618, doi: [10.48550/arXiv.2307.07618](http://doi.org/10.48550/arXiv.2307.07618)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rehemtulla et al. (2023b) Rehemtulla, N., Miller, A., Fremling, C., et al. 2023b,
    Transient Name Server AstroNote, 265, 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reyes-Jainaga et al. (2023) Reyes-Jainaga, I., Förster, F., Muñoz Arancibia,
    A. M., et al. 2023, ApJ, 952, L43, doi: [10.3847/2041-8213/ace77e](http://doi.org/10.3847/2041-8213/ace77e)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rigault et al. (2019) Rigault, M., Neill, J. D., Blagorodnova, N., et al. 2019,
    A&A, 627, A115, doi: [10.1051/0004-6361/201935344](http://doi.org/10.1051/0004-6361/201935344)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rodríguez et al. (2023) Rodríguez, Ó., Maoz, D., & Nakar, E. 2023, ApJ, 955,
    71, doi: [10.3847/1538-4357/ace2bd](http://doi.org/10.3847/1538-4357/ace2bd)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sadeh et al. (2016) Sadeh, I., Abdalla, F. B., & Lahav, O. 2016, PASP, 128,
    104502, doi: [10.1088/1538-3873/128/968/104502](http://doi.org/10.1088/1538-3873/128/968/104502)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shappee et al. (2014) Shappee, B. J., Prieto, J. L., Grupe, D., et al. 2014,
    ApJ, 788, 48, doi: [10.1088/0004-637X/788/1/48](http://doi.org/10.1088/0004-637X/788/1/48)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. (2023) Sharma, Y., Sollerman, J., Fremling, C., et al. 2023, ApJ,
    948, 52, doi: [10.3847/1538-4357/acbc16](http://doi.org/10.3847/1538-4357/acbc16)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharon & Kushnir (2022) Sharon, A., & Kushnir, D. 2022, MNRAS, 509, 5275, doi: [10.1093/mnras/stab3380](http://doi.org/10.1093/mnras/stab3380)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan & Zisserman (2014) Simonyan, K., & Zisserman, A. 2014, arXiv e-prints,
    arXiv:1409.1556, doi: [10.48550/arXiv.1409.1556](http://doi.org/10.48550/arXiv.1409.1556)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Smith et al. (2020) Smith, K. W., Smartt, S. J., Young, D. R., et al. 2020,
    PASP, 132, 085002, doi: [10.1088/1538-3873/ab936e](http://doi.org/10.1088/1538-3873/ab936e)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sollerman et al. (2022) Sollerman, J., Yang, S., Perley, D., et al. 2022, A&A,
    657, A64, doi: [10.1051/0004-6361/202142049](http://doi.org/10.1051/0004-6361/202142049)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stein et al. (2023) Stein, R., Mahabal, A., Reusch, S., et al. 2023, arXiv e-prints,
    arXiv:2312.00139, doi: [10.48550/arXiv.2312.00139](http://doi.org/10.48550/arXiv.2312.00139)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stoppa et al. (2023) Stoppa, F., Bhattacharyya, S., Ruiz de Austri, R., et al.
    2023, arXiv e-prints, arXiv:2307.14456, doi: [10.48550/arXiv.2307.14456](http://doi.org/10.48550/arXiv.2307.14456)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. (2013) Szegedy, C., Zaremba, W., Sutskever, I., et al. 2013,
    arXiv e-prints, arXiv:1312.6199, doi: [10.48550/arXiv.1312.6199](http://doi.org/10.48550/arXiv.1312.6199)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tachibana & Miller (2018) Tachibana, Y., & Miller, A. A. 2018, PASP, 130, 128001,
    doi: [10.1088/1538-3873/aae3d9](http://doi.org/10.1088/1538-3873/aae3d9)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tonry (2011) Tonry, J. L. 2011, PASP, 123, 58, doi: [10.1086/657997](http://doi.org/10.1086/657997)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tonry et al. (2018) Tonry, J. L., Denneau, L., Heinze, A. N., et al. 2018, PASP,
    130, 064505, doi: [10.1088/1538-3873/aabadf](http://doi.org/10.1088/1538-3873/aabadf)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tsaprazi et al. (2022) Tsaprazi, E., Jasche, J., Goobar, A., et al. 2022, MNRAS,
    510, 366, doi: [10.1093/mnras/stab3525](http://doi.org/10.1093/mnras/stab3525)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Turpin et al. (2020) Turpin, D., Ganet, M., Antier, S., et al. 2020, MNRAS,
    497, 2641, doi: [10.1093/mnras/staa2046](http://doi.org/10.1093/mnras/staa2046)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van der Walt et al. (2019) van der Walt, S. J., Crellin-Quick, A., & Bloom,
    J. S. 2019, Journal of Open Source Software, 4, 1247, doi: [10.21105/joss.01247](http://doi.org/10.21105/joss.01247)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van Roestel et al. (2021) van Roestel, J., Duev, D. A., Mahabal, A. A., et al.
    2021, AJ, 161, 267, doi: [10.3847/1538-3881/abe853](http://doi.org/10.3847/1538-3881/abe853)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Villar et al. (2019) Villar, V. A., Berger, E., Miller, G., et al. 2019, ApJ,
    884, 83, doi: [10.3847/1538-4357/ab418c](http://doi.org/10.3847/1538-4357/ab418c)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Villar et al. (2020) Villar, V. A., Hosseinzadeh, G., Berger, E., et al. 2020,
    ApJ, 905, 94, doi: [10.3847/1538-4357/abc6fd](http://doi.org/10.3847/1538-4357/abc6fd)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Virtanen et al. (2020) Virtanen, P., Gommers, R., Oliphant, T. E., et al. 2020,
    Nature Methods, 17, 261, doi: [10.1038/s41592-019-0686-2](http://doi.org/10.1038/s41592-019-0686-2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walmsley et al. (2019) Walmsley, M., Ferguson, A. M. N., Mann, R. G., & Lintott,
    C. J. 2019, MNRAS, 483, 2968, doi: [10.1093/mnras/sty3232](http://doi.org/10.1093/mnras/sty3232)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Walmsley et al. (2020) Walmsley, M., Smith, L., Lintott, C., et al. 2020, MNRAS,
    491, 1554, doi: [10.1093/mnras/stz2816](http://doi.org/10.1093/mnras/stz2816)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wes McKinney (2010) Wes McKinney. 2010, in Proceedings of the 9th Python in
    Science Conference, ed. Stéfan van der Walt & Jarrod Millman, 56 – 61, doi: [10.25080/Majora-92bf1922-00a](http://doi.org/10.25080/Majora-92bf1922-00a)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wright et al. (2015) Wright, D. E., Smartt, S. J., Smith, K. W., et al. 2015,
    MNRAS, 449, 451, doi: [10.1093/mnras/stv292](http://doi.org/10.1093/mnras/stv292)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2021) Yang, S., Sollerman, J., Strotjohann, N. L., et al. 2021,
    A&A, 655, A90, doi: [10.1051/0004-6361/202141244](http://doi.org/10.1051/0004-6361/202141244)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zimmerman et al. (2023) Zimmerman, E. A., Irani, I., Chen, P., et al. 2023,
    arXiv e-prints, arXiv:2310.10727, doi: [10.48550/arXiv.2310.10727](http://doi.org/10.48550/arXiv.2310.10727)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
