- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:49:27'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:49:27
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2112.01800] A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2112.01800] 调查：少量标记样本下的深度学习在高光谱图像分类中的应用'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.01800](https://ar5iv.labs.arxiv.org/html/2112.01800)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2112.01800](https://ar5iv.labs.arxiv.org/html/2112.01800)
- en: 'A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调查：少量标记样本下的深度学习在高光谱图像分类中的应用
- en: Sen Jia Shuguo Jiang Zhijie Lin Nanying Li Meng Xu Shiqi Yu [yusq@sustech.edu.cn](mailto:yusq@sustech.edu.cn)
    College of Computer Science and Software Engineering, Shenzhen University, China
    SZU Branch, Shenzhen Institute of Artificial Intelligence and Robotics for Society,
    China Department of Computer Science and Engineering, Southern University of Science
    and Technology, China
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 参者：Sen Jia Shuguo Jiang Zhijie Lin Nanying Li Meng Xu Shiqi Yu [yusq@sustech.edu.cn](mailto:yusq@sustech.edu.cn)
    深圳大学计算机科学与软件工程学院，中国 SZU 分部，深圳社会人工智能与机器人研究所，中国 南方科技大学计算机科学与工程系，中国
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: With the rapid development of deep learning technology and improvement in computing
    capability, deep learning has been widely used in the field of hyperspectral image
    (HSI) classification. In general, deep learning models often contain many trainable
    parameters and require a massive number of labeled samples to achieve optimal
    performance. However, in regard to HSI classification, a large number of labeled
    samples is generally difficult to acquire due to the difficulty and time-consuming
    nature of manual labeling. Therefore, many research works focus on building a
    deep learning model for HSI classification with few labeled samples. In this article,
    we concentrate on this topic and provide a systematic review of the relevant literature.
    Specifically, the contributions of this paper are twofold. First, the research
    progress of related methods is categorized according to the learning paradigm,
    including transfer learning, active learning and few-shot learning. Second, a
    number of experiments with various state-of-the-art approaches has been carried
    out, and the results are summarized to reveal the potential research directions.
    More importantly, it is notable that although there is a vast gap between deep
    learning models (that usually need sufficient labeled samples) and the HSI scenario
    with few labeled samples, the issues of small-sample sets can be well characterized
    by fusion of deep learning methods and related techniques, such as transfer learning
    and a lightweight model. For reproducibility, the source codes of the methods
    assessed in the paper can be found at [https://github.com/ShuGuoJ/HSI-Classification.git](https://github.com/ShuGuoJ/HSI-Classification.git).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习技术的快速发展和计算能力的提升，深度学习已广泛应用于高光谱图像（HSI）分类领域。通常，深度学习模型往往包含大量可训练的参数，并需要大量的标记样本才能达到最佳性能。然而，在HSI分类中，由于人工标记的困难和耗时，获得大量标记样本通常比较困难。因此，许多研究工作集中于构建少量标记样本的HSI分类深度学习模型。本文着重于这一主题，并提供了相关文献的系统性综述。具体而言，本文的贡献有两个方面。首先，相关方法的研究进展根据学习范式进行分类，包括迁移学习、主动学习和少样本学习。其次，进行了大量使用各种先进方法的实验，并总结了结果以揭示潜在的研究方向。更重要的是，虽然深度学习模型（通常需要足够的标记样本）与少标记样本的HSI场景之间存在巨大差距，但通过深度学习方法和相关技术（如迁移学习和轻量模型）的融合，可以很好地表征小样本集问题。为了可重复性，本文评估的方法的源代码可以在
    [https://github.com/ShuGuoJ/HSI-Classification.git](https://github.com/ShuGuoJ/HSI-Classification.git)
    找到。
- en: 'keywords:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'hyperspectral image classification, deep learning, transfer learning, few-shot
    learning^†^†journal: Neurocomputing^(mytitlenote)^(mytitlenote)footnotetext: The
    work is supported by the National Natural Science Foundation of China (Grant No.
    41971300, 61901278 and 61976144), the National Key Research and Development Program
    of China (Grant No. 2020AAA0140002), the Program for Young Changjiang Scholars,
    the Key Project of Department of Education of Guangdong Province (Grant No. 2020ZDZX3045)
    and the Shenzhen Scientific Research and Development Funding Program under (Grant
    No. JCYJ20180305124802421 and JCYJ20180305125902403).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '高光谱图像分类、深度学习、迁移学习、少样本学习^†^†期刊: Neurocomputing^(mytitlenote)^(mytitlenote)脚注:
    本研究得到了中国国家自然科学基金（资助号 41971300, 61901278 和 61976144）、中国国家重点研发计划（资助号 2020AAA0140002）、长江学者奖励计划、广东省教育厅重点项目（资助号
    2020ZDZX3045）以及深圳市科研开发资金计划（资助号 JCYJ20180305124802421 和 JCYJ20180305125902403）的支持。'
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: Hyperspectral remote sensing technology is a method that organically combines
    the spectrum of ground objects determined by their unique material composition
    with the spatial image reflecting the shape, texture and layout of ground objects,
    to realize the accurate detection, recognition and attribute analysis of ground
    objects. The resultant hyperspectral images (HSIs) not only contain abundant spectral
    information reflecting the unique physical properties of the ground features but
    also provide rich spatial information of the ground features. Therefore, HSIs
    can be utilized to solve problems that cannot be solved well in multispectral
    or natural images, such as the precise identification of each pixel. Since different
    materials exhibit specific spectral characteristics, the classification performance
    of HSI can be more accurate. Due to these advantages, hyperspectral remote sensing
    has been widely used in many applications, such as precision agriculture [[1](#bib.bib1)],
    crop monitoring [[2](#bib.bib2)], and land resources [[3](#bib.bib3), [4](#bib.bib4)].
    In environmental protection, HSI has been employed to detect gas [[5](#bib.bib5)],
    oil spills [[6](#bib.bib6)], water quality [[7](#bib.bib7), [8](#bib.bib8)] and
    vegetation coverage [[9](#bib.bib9), [10](#bib.bib10)], to better protect our
    living environment. In the medical field, HSI has been utilized for skin testing
    to examine the health of human skin [[11](#bib.bib11)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 高光谱遥感技术是一种将地物的独特材质组成决定的光谱与反映地物形状、纹理和布局的空间图像有机结合的方法，以实现对地物的准确检测、识别和属性分析。得到的高光谱图像（HSI）不仅包含丰富的光谱信息，反映了地物的独特物理属性，还提供了地物的丰富空间信息。因此，HSI
    可用于解决在多光谱或自然图像中无法很好解决的问题，例如每个像素的精确识别。由于不同材料表现出特定的光谱特征，高光谱图像的分类性能可以更为准确。由于这些优势，高光谱遥感已广泛应用于许多领域，如精准农业 [[1](#bib.bib1)]、作物监测 [[2](#bib.bib2)]
    和土地资源 [[3](#bib.bib3), [4](#bib.bib4)]。在环境保护方面，HSI 被用于检测气体 [[5](#bib.bib5)]、石油泄漏 [[6](#bib.bib6)]、水质 [[7](#bib.bib7),
    [8](#bib.bib8)] 和植被覆盖 [[9](#bib.bib9), [10](#bib.bib10)]，以更好地保护我们的生活环境。在医学领域，HSI
    被用于皮肤测试，以检查人体皮肤的健康状况 [[11](#bib.bib11)]。
- en: As a general pattern recognition problem, HSI classification has received a
    substantial amount of attention, and a large number of research results have been
    achieved in the past several decades. According to the previous work [[12](#bib.bib12)],
    all researches can be divided into the spectral-feature method, spatial-feature
    method, and spectral-spatial-feature method. The spectral feature is the primitive
    characteristic of the hyperspectral image, which is also called the spectral vector
    or spectral curve. And the spatial feature [[13](#bib.bib13)] means the relationship
    between the central pixel and its context, which can greatly increase the robustness
    of the model. In the early period of the study on HSI classification, researchers
    mainly focused on the pure spectral feature-based methods, which simply apply
    classifiers to pixel vectors, such as support vector machines (SVM) [[14](#bib.bib14)],
    neural networks [[15](#bib.bib15)], logistic regression [[16](#bib.bib16)], to
    obtain classification results without any feature extraction. But raw spectra
    contain much redundant information and the relation between spectra and ground
    objects is non-linear, which enlarges the difficulty of the model classification.
    Therefore, most later methods give more attention to dimension reduction and feature
    extraction to learn the more discriminative feature. For the approaches based
    on dimension reduction, principle component analysis [[17](#bib.bib17)], independent
    component analysis [[18](#bib.bib18)], linear discriminant analysis [[19](#bib.bib19)],
    and low-rank [[20](#bib.bib20)] are widely used. Nevertheless, the performance
    of those models is still unsatisfactory. Because, there is a common phenomenon
    in the hyperspectral image which is that different surface objects may have the
    same spectral characteristic and, otherwise, the same surface objects may have
    different spectral characteristics. The variability of spectra of ground objects
    is caused by illumination, environmental, atmospheric, and temporal conditions.
    Those enlarge the probability of misclassification. Thus, those methods are only
    based on spectral information, and ignore spatial information, resulting in unsatisfactory
    classification performance. The spatial characteristic of ground objects supply
    abundant information of shape, context, and layout about ground objects, and neighboring
    pixels belong to the same class with high probability, which is useful for improving
    classification accuracy and robustness of methods. Then, a large number of feature
    extraction methods that integrate the spatial structural and texture information
    with the spectral features have been developed, including morphological [[21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)], filtering [[24](#bib.bib24), [25](#bib.bib25)],
    coding [[26](#bib.bib26)], etc. Since deep learning-based methods are mainly concerned
    in this paper, the readers are referred to [[27](#bib.bib27)] for more details
    on these conventional techniques.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个通用的模式识别问题，高光谱图像（HSI）分类受到了大量关注，并且在过去几十年里取得了大量的研究成果。根据以往的研究[[12](#bib.bib12)]，所有研究可以分为光谱特征方法、空间特征方法和光谱-空间特征方法。光谱特征是高光谱图像的原始特性，也称为光谱向量或光谱曲线。而空间特征[[13](#bib.bib13)]指的是中央像素与其上下文之间的关系，这可以大大提高模型的鲁棒性。在高光谱图像分类研究的早期阶段，研究人员主要关注基于纯光谱特征的方法，这些方法仅仅将分类器应用于像素向量，例如支持向量机（SVM）[[14](#bib.bib14)]、神经网络[[15](#bib.bib15)]、逻辑回归[[16](#bib.bib16)]，以获得分类结果，而无需任何特征提取。但原始光谱包含大量冗余信息，并且光谱与地面物体之间的关系是非线性的，这增加了模型分类的难度。因此，后来大多数方法更加关注于降维和特征提取，以学习更具区分性的特征。对于基于降维的方法，主成分分析[[17](#bib.bib17)]、独立成分分析[[18](#bib.bib18)]、线性判别分析[[19](#bib.bib19)]和低秩[[20](#bib.bib20)]被广泛使用。然而，这些模型的性能仍然不令人满意。因为，高光谱图像中存在一个普遍现象，即不同的地面物体可能具有相同的光谱特征，反之，相同的地面物体可能具有不同的光谱特征。地面物体光谱的变异性是由光照、环境、大气和时间条件引起的。这些因素增加了误分类的概率。因此，这些方法仅基于光谱信息，而忽略了空间信息，导致分类性能不佳。地面物体的空间特征提供了关于地面物体的形状、上下文和布局的丰富信息，并且相邻的像素很可能属于同一类，这对于提高分类准确性和方法的鲁棒性非常有用。随后，许多将空间结构和纹理信息与光谱特征相结合的特征提取方法被开发出来，包括形态学[[21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)]、滤波[[24](#bib.bib24), [25](#bib.bib25)]、编码[[26](#bib.bib26)]等。由于本文主要关注基于深度学习的方法，读者可以参考[[27](#bib.bib27)]以获取更多关于这些传统技术的细节。
- en: 'In the past decade, deep learning technology has developed rapidly and received
    widespread attention. Compared with traditional machine learning model, deep learning
    technology does not need to artificially design feature patterns and can automatically
    learn patterns from data. Therefore, it has been successfully applied in the fields
    of natural language processing, speech recognition, semantic segmentation, autonomous
    driving, and object detection, and gained excellent performance. Recently, it
    also has been introduced into the field of HSI classification. Researchers have
    proposed a number of new deep learning-based HIS classification approaches, as
    shown in the left part of Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples").
    Currently, all methods, based on the joint spectral-spatial feature, can be divided
    into two categories—Two-Stream and Single-Stream, according to whether they simultaneously
    extract the joint spectral-spatial feature. The architecture of two-stream usually
    includes two branches—spectral branch and spatial branch. The former is to extract
    the spectral feature of the pixel, and the latter is to capture the spatial relation
    of the central pixel with its neighbor pixels. And the existing methods have covered
    all deep learning modules, such as fully connected layer, convolutional layer,
    and recurrent unit.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十
- en: 'In the general deep learning framework, a large number of training samples
    should be provided to well train the model and tune the numerous parameters. However,
    in practice, manually labeling is often very time-consuming and expensive due
    to the need for expert knowledge, and thus, a sufficient training set is often
    unavailable. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    (here the widely used Kennedy Space Center (KSC) hyperspectral image is utilized
    for illustration), the left figure randomly selects 10 samples per class and contains
    130 labeled samples in total, which is very scattered and can hardly be seen.
    Alternatively, the right figure in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") displays 50% of labeled samples, which is more suitable for deep learning-based
    methods. Hence, there is a vast gap between the training samples required by deep
    learning models and the labeled samples that can be collected in practice. And
    there are many learning paradigms proposed for solving the problem of few label
    samples, as shown in the right part of Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples"). In section 2, we will discuss them in detail. And they can be integrated
    with any model architecture. Some pioneering works such as [[28](#bib.bib28)]
    started the topic by training a deep model with good generalization only using
    few labeled samples. However, there are still many challenges for this topic.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '在一般的深度学习框架中，需要提供大量的训练样本来充分训练模型并调整众多参数。然而，在实践中，手动标注通常非常耗时且昂贵，因为需要专家知识，因此，通常无法获得足够的训练集。如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") 所示（这里使用了广泛应用的肯尼迪航天中心（KSC）高光谱图像进行说明），左图随机选择每个类别的10个样本，总共包含130个标注样本，这些样本非常分散，几乎难以辨认。相反，图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") 右图展示了50%的标注样本，这更适合基于深度学习的方法。因此，深度学习模型所需的训练样本与实际可以收集的标注样本之间存在巨大差距。为了解决少量标注样本的问题，已经提出了许多学习范式，如图
    [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") 右侧所示。第2节将详细讨论这些范式，并且它们可以与任何模型架构进行集成。一些开创性的工作如
    [[28](#bib.bib28)] 已经通过仅使用少量标注样本训练具有良好泛化能力的深度模型开启了这一话题。然而，这一领域仍然面临许多挑战。'
- en: '![Refer to caption](img/9c2a417741b859faf8aa968e4ec124c3.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9c2a417741b859faf8aa968e4ec124c3.png)'
- en: 'Figure 1: Illustration of the massive gap between practical situations (i.e.,
    few labeled samples) and a large number of labeled samples of deep learning-based
    methods. Here, the widely used Kennedy Space Center (KSC) hyperspectral image
    is employed, which contains 13 land covers and 5211 labeled samples (detailed
    information can be found in the experimental section). Generally, sufficient samples
    are required to well train a deep learning model (as illustrated in the right
    figure), which is hard to be achieved in practice due to the difficulty of manually
    labeling (as shown in the left figure).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：实际情况（即，少量标注样本）与深度学习方法所需的大量标注样本之间的巨大差距。这里采用了广泛应用的肯尼迪航天中心（KSC）高光谱图像，其中包含13种土地覆盖类型和5211个标注样本（详细信息可以在实验部分找到）。通常，需要足够的样本来充分训练深度学习模型（如右图所示），但由于手动标注的难度，这在实践中很难实现（如左图所示）。
- en: In this paper, we hope to provide a comprehensive review of the state-of-the-art
    deep learning-based methods for HSI classification with few labeled samples. First,
    instead of separating the various methods according to feature fusion manner,
    such as spectral-based, spatial-based, and joint spectral-spatial-based methods,
    the research progress of methods related to few training samples is categorized
    according to the learning paradigm, including transfer learning, active learning,
    and few-shot learning. Second, a number of experiments with various state-of-the-art
    approaches have been carried out, and the results are summarized to reveal the
    potential research directions. Further, it should be noted that different from
    the previous review papers [[12](#bib.bib12), [29](#bib.bib29)], this paper mainly
    focuses on the few labeled sample issue, which is considered as the most challenging
    problem in the HSI classification scenario. For reproducibility, the source codes
    of the methods conducted in the paper can be found at the web site for the paper¹¹1[https://github.com/ShuGuoJ/HSI-Classification.git](https://github.com/ShuGuoJ/HSI-Classification.git).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们希望提供对高光谱图像（HSI）分类中基于深度学习的最先进方法的全面综述，尤其是在标签样本较少的情况下。首先，我们不按照特征融合方式（如基于光谱、基于空间和光谱-空间联合的方法）来区分各种方法，而是根据学习范式（包括迁移学习、主动学习和少样本学习）来分类涉及少量训练样本的方法的研究进展。其次，我们进行了大量的实验，比较了多种最先进的方法，并总结了结果以揭示潜在的研究方向。此外，需要注意的是，与之前的综述论文[[12](#bib.bib12),
    [29](#bib.bib29)]不同，本文主要关注少量标注样本的问题，这是高光谱图像分类场景中最具挑战性的难题。为了重现性，本文中使用的方法的源代码可以在网站¹¹1[https://github.com/ShuGuoJ/HSI-Classification.git](https://github.com/ShuGuoJ/HSI-Classification.git)上找到。
- en: 'The remainder of this paper is organized as follows. Section [2](#S2 "2 Deep
    learning models for HSI classification ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") introduces the deep models that
    are popular in recent years. In Section [3](#S3 "3 Deep learning paradigms for
    HSI classification with few labeled samples ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples"), we divide the previous works
    into four mainstream learning paradigms, including transfer learning, active learning,
    and few-shot learning. In Section [4](#S4 "4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples"), we performed
    many experiments, and a number of representative deep learning-based classification
    methods are compared on several real hyperspectral image data sets. Finally, conclusions
    and suggestions are provided in Section [5](#S5 "5 Conclusions ‣ A Survey: Deep
    Learning for Hyperspectral Image Classification with Few Labeled Samples").'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的其余部分组织如下。第[2](#S2 "2 深度学习模型在高光谱图像分类中的应用 ‣ 调查：基于深度学习的高光谱图像分类方法")节介绍了近年来流行的深度模型。在第[3](#S3
    "3 用于高光谱图像分类的深度学习范式 ‣ 调查：基于深度学习的高光谱图像分类方法")节，我们将之前的工作划分为四种主流学习范式，包括迁移学习、主动学习和少样本学习。在第[4](#S4
    "4 实验 ‣ 调查：基于深度学习的高光谱图像分类方法")节，我们进行了许多实验，并比较了若干代表性的基于深度学习的分类方法在几个实际高光谱图像数据集上的表现。最后，第[5](#S5
    "5 结论 ‣ 调查：基于深度学习的高光谱图像分类方法")节提供了结论和建议。
- en: '![Refer to caption](img/c83a1055948d73fada507706b8f628a8.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c83a1055948d73fada507706b8f628a8.png)'
- en: 'Figure 2: The category of deep learning-based methods for hyperspectral image
    classification. The left is from the model architecture point of view, while the
    right is from the learning paradigm point of view. It is worth noting that the
    both kinds of methods can be combined arbitrarily.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的高光谱图像分类方法的分类。左侧从模型架构的角度出发，而右侧则从学习范式的角度出发。值得注意的是，这两种方法可以任意结合。
- en: 2 Deep learning models for HSI classification
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 深度学习模型在高光谱图像分类中的应用
- en: In this section, three classical deep learning models, including the autoencoder,
    convolutional neural network (CNN), and recurrent neural network (RNN), for HSI
    classification are respectively described, and the relevant references are reviewed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，分别描述了三种经典的深度学习模型，包括自编码器、卷积神经网络（CNN）和递归神经网络（RNN），用于高光谱图像分类，并回顾了相关文献。
- en: 2.1 Autoencoder for HSI classification
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 用于高光谱图像分类的自编码器
- en: 'An autoencoder [[30](#bib.bib30)] is a classic neural network, which consists
    of two parts: an encoder and a decoder. The encoder $p_{encoder}(\bm{h}|\bm{x})$
    maps the input $\bm{x}$ as a hidden representation $\bm{h}$, and then, the decoder
    $p_{decoder}(\hat{\bm{x}}|\bm{h})$ reconstructs $\hat{\bm{x}}$ from $\bm{h}$.
    It aims to make the input and output as similar as possible. The loss function
    can be formulated as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器[[30](#bib.bib30)]是一种经典的神经网络，由两部分组成：编码器和解码器。编码器 $p_{encoder}(\bm{h}|\bm{x})$
    将输入 $\bm{x}$ 映射为隐藏表示 $\bm{h}$，然后解码器 $p_{decoder}(\hat{\bm{x}}|\bm{h})$ 从 $\bm{h}$
    重建 $\hat{\bm{x}}$。其目标是使输入和输出尽可能相似。损失函数可以表述如下：
- en: '|  | $\mathcal{L}(\bm{x},\hat{\bm{x}})=\min&#124;\bm{x}-\hat{\bm{x}}&#124;$
    |  | (1) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\bm{x},\hat{\bm{x}})=\min\|\bm{x}-\hat{\bm{x}}\|$ |  | (1)
    |'
- en: 'where $\mathcal{L}$ is the similarity measure. If the dimension of $\bm{h}$
    is smaller than $\bm{x}$, the autoencoder procedure is undercomplete and can be
    used to reduce the data dimension. Evidently, if there is not any constraint on
    $\bm{h}$, the autoencoder is the simplest identical function. In other words,
    the network does not learn anything. To avoid such a situation, the usual way
    is to add the normalization term $\Omega(h)$ to the loss. In [[31](#bib.bib31),
    zeng2018facial], the normalization of the autoencoder, referred as a sparse autoencoder,
    is $\Omega(h)=\lambda\sum_{i}h_{i}$, which will make most of the parameters of
    the network very close to zero. Therefore, it is equipped with a certain degree
    of noise immunity and can produce the sparsest representation of the input. Another
    way to avoid the identical mapping is by adding some noise into $\bm{x}$ to make
    the damaged input $\bm{x_{noise}}$ and then forcing the decoder to reconstruct
    the $\bm{x}$. In this situation, it becomes the denoising autoencoder [[32](#bib.bib32)],
    which can remove the additional noise from $\bm{x_{noise}}$ and produce a powerful
    hidden representation of the input. In general, the autoencoder plays the role
    of feature extractor [[33](#bib.bib33)] to learn the internal pattern of data
    without labeled samples. Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Autoencoder for HSI
    classification ‣ 2 Deep learning models for HSI classification ‣ A Survey: Deep
    Learning for Hyperspectral Image Classification with Few Labeled Samples") illustrates
    the basic architecture of the autoencoder model.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{L}$ 是相似度度量。如果 $\bm{h}$ 的维度小于 $\bm{x}$，则自编码器过程为欠完备的，可以用于降低数据维度。显然，如果对
    $\bm{h}$ 没有任何约束，自编码器就是最简单的恒等函数。换句话说，网络什么也没有学到。为了避免这种情况，通常的做法是将归一化项 $\Omega(h)$
    添加到损失中。在[[31](#bib.bib31), zeng2018facial]中，自编码器的归一化，被称为稀疏自编码器，其为 $\Omega(h)=\lambda\sum_{i}h_{i}$，这将使网络的大多数参数非常接近于零。因此，它具有一定程度的噪声免疫力，并且可以生成输入的最稀疏表示。另一种避免恒等映射的方法是向
    $\bm{x}$ 中添加一些噪声，生成损坏的输入 $\bm{x_{noise}}$，然后强制解码器重建 $\bm{x}$。在这种情况下，它变成了去噪自编码器[[32](#bib.bib32)]，可以去除
    $\bm{x_{noise}}$ 中的额外噪声，并生成输入的强大隐藏表示。一般来说，自编码器作为特征提取器[[33](#bib.bib33)]，在没有标记样本的情况下学习数据的内部模式。图[3](#S2.F3
    "图 3 ‣ 2.1 自编码器用于HSI分类 ‣ 2 深度学习模型用于HSI分类 ‣ 调查：少量标记样本的高光谱图像分类中的深度学习") 展示了自编码器模型的基本架构。
- en: '![Refer to caption](img/85775aa5c15d070e6222732fbb44c76a.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/85775aa5c15d070e6222732fbb44c76a.png)'
- en: 'Figure 3: The architecture of the autoencoder. The solid line represents training,
    while the dashed line represents inference.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：自编码器的架构。实线表示训练，虚线表示推断。
- en: 'Therefore, Chen *et al.* [[34](#bib.bib34)] used an autoencoder for the first
    time for feature extraction and classification of HSIs. First, in the pretraining
    stage, the spectral vector of each pixel directly inputs the encoder module, and
    then, the decoder is used to reconstruct it so that the encoder has the ability
    to extract spectral features. Alternatively, to obtain the spatial features, principal
    component analysis (PCA) is utilized to reduce the dimensionality of the hyperspectral
    image, and then, the image patch is flattened into a vector. Another autoencoder
    is employed to learn the spatial features. Finally, the spatial-spectral joint
    information obtained above is fused and classified. Subsequently, a large number
    of hyperspectral image classification methods [[35](#bib.bib35), [36](#bib.bib36)]
    based on autoencoders appeared. Most of these methods adopt the same training
    strategy as [[34](#bib.bib34)], which is divided into two modules: fully training
    the encoder in an unsupervised manner and fine-tuning the classifier in a supervised
    manner. Each of these methods attempts different types of encoders or preprocessing
    methods to adapt to HSI classification under the condition of small samples. For
    example, Xing *et al.* [[36](#bib.bib36)] stack multiple denoising autoencoders
    to form a feature extractor, which has a stronger anti-noise ability to extract
    more robust representations. Given that the same ground objects may have different
    spectra while different ground objects may exhibit similar spectra, spectral-based
    classification methods often fail to achieve satisfactory performance, and spatial
    structural information of objects provides an effective supplement. To gain a
    better spatial description of an object, some autoencoder models combined with
    convolutional neural networks (CNNs) have been developed [[37](#bib.bib37), [38](#bib.bib38)].
    Concretely, the autoencoder module is able to extract spectral features on large
    unlabeled samples, while the CNN is proven to be able to extract spatial features
    well. After fusion, the spatial-spectral features can be achieved. Further, to
    reduce the number of trainable parameters, some researchers use the lightweight
    models, such as SVMs [[39](#bib.bib39), [40](#bib.bib40)], random forests [[41](#bib.bib41),
    [42](#bib.bib42)] or logistic regression [[34](#bib.bib34), [43](#bib.bib43)],
    to serve as the classifier.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，陈*等*[[34](#bib.bib34)]首次使用自编码器进行高光谱图像的特征提取和分类。首先，在预训练阶段，每个像素的光谱向量直接输入编码器模块，然后使用解码器进行重建，以便编码器具备提取光谱特征的能力。或者，为了获取空间特征，采用主成分分析（PCA）来降低高光谱图像的维度，然后将图像块展平为一个向量。另一个自编码器被用来学习空间特征。最后，将上述获得的空间-光谱联合信息进行融合和分类。随后，出现了大量基于自编码器的高光谱图像分类方法[[35](#bib.bib35),
    [36](#bib.bib36)]。这些方法大多采用与[[34](#bib.bib34)]相同的训练策略，即将训练分为两个模块：以无监督方式充分训练编码器，并以有监督方式微调分类器。这些方法中的每一个都尝试不同类型的编码器或预处理方法，以适应小样本下的高光谱图像分类。例如，邢*等*[[36](#bib.bib36)]堆叠多个去噪自编码器以形成特征提取器，该提取器具有更强的抗噪声能力以提取更鲁棒的表示。由于相同的地物可能具有不同的光谱，而不同的地物可能表现出相似的光谱，基于光谱的分类方法往往无法获得令人满意的性能，而对象的空间结构信息提供了有效的补充。为了获得更好的空间描述，一些与卷积神经网络（CNN）结合的自编码器模型被开发出来[[37](#bib.bib37),
    [38](#bib.bib38)]。具体而言，自编码器模块能够在大量未标记样本上提取光谱特征，而CNN被证明能够很好地提取空间特征。经过融合后，可以得到空间-光谱特征。此外，为了减少可训练参数的数量，一些研究人员使用轻量级模型，如SVMs[[39](#bib.bib39),
    [40](#bib.bib40)]、随机森林[[41](#bib.bib41), [42](#bib.bib42)]或逻辑回归[[34](#bib.bib34),
    [43](#bib.bib43)]，作为分类器。
- en: Due to the three-dimensional (3D) pattern of hyperspectral images, it is desirable
    to simultaneously investigate the spectral and spatial information such that the
    joint spatial-spectral correlation can be better examined. Some three-dimensional
    operators and methods have been proposed. In the preprocessing stage, Li *et al.* [[44](#bib.bib44)]
    utilized the 3D Gabor operator to fuse spatial information and spectral information
    to obtain spatial-spectral joint features, which were then fed into the autoencoder
    to obtain more abstract features. Mei *et al.* [[40](#bib.bib40)] used a 3D convolutional
    operator to construct an autoencoder to extract spatial-spectral features directly.
    In addition, image segmentation has been introduced to characterize the region
    structure of objects to avoid misclassification of pixels at the boundary [[45](#bib.bib45)].
    Therefore, Liu *et al.* [[46](#bib.bib46)] utilized superpixel segmentation technology
    as a postprocessing method to perform boundary regularization on the classification
    map.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 由于高光谱图像的三维（3D）模式，建议同时研究光谱和空间信息，以便更好地检查联合空间-光谱相关性。已有一些三维算子和方法被提出。在预处理阶段，李*等人*
    [[44](#bib.bib44)] 利用3D Gabor算子融合空间信息和光谱信息，以获得空间-光谱联合特征，然后将其输入自编码器以获得更抽象的特征。梅*等人*
    [[40](#bib.bib40)] 使用了3D卷积算子构建自编码器，直接提取空间-光谱特征。此外，图像分割技术已被引入，以表征对象的区域结构，从而避免边界像素的误分类
    [[45](#bib.bib45)]。因此，刘*等人* [[46](#bib.bib46)] 利用超像素分割技术作为后处理方法，对分类图进行边界正则化。
- en: 2.2 Convolutional Neural Networks (CNNs) for HSI classification
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 用于高光谱图像分类的卷积神经网络（CNNs）
- en: 'In theory, the CNN uses a group of parameters that refer to a kernel function
    or kernel to scan the image and produce a specified feature. It has three main
    characteristics that make it very powerful for feature representation, and thus,
    the CNN has been successfully applied in many research fields. The first one is
    the local connection that greatly decreases the number of trainable parameters
    and makes itself suitable for processing large images. This is the most obvious
    difference from the fully connected network, which has a full connection between
    two neighboring neural layers and is unfriendly for large spatial images. To further
    reduce the number of parameters, the same convolutional kernel shares the same
    parameters, which is the second characteristic of CNNs. In contrast, in the traditional
    neural network, the parameters of the output are independent from each other.
    However, the CNN applies the same parameters for all of the output to cut back
    the number of parameters, leading to the third characteristic: shift invariance.
    It means that even if the feature of an object has shifted from one position to
    another, the CNN model still has the capacity to capture it regardless of where
    it appears. Specifically, a common convolutional layer consists of three traditional
    components: linear mapping, the activation function and the pooling function.
    Similar to other modern neural network architectures, activation functions are
    used to bring a nonlinear mapping feature into the network. Generally, the rectified
    linear unit (ReLU) is the prior choice. Pooling makes use of the statistical characteristic
    of the local region to represent the output of a specified position. Taking the
    max pooling step as an example, it employs the max value to replace the region
    of input. Clearly, the pooling operation is robust to small changes and noise
    interfere, which could be smoothed out by the pooling operation in the output,
    and thus, more abstract features can be reserved.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，卷积神经网络（CNN）使用一组参数，这些参数指代一个卷积核函数或卷积核来扫描图像并产生指定的特征。它具有三大主要特征，使其在特征表示方面非常强大，因此，CNN
    已在许多研究领域成功应用。第一个特征是局部连接，这大大减少了可训练参数的数量，使其适合处理大图像。这是与全连接网络最明显的区别，全连接网络在两个相邻的神经层之间有全连接，不适合处理大型空间图像。为了进一步减少参数数量，相同的卷积核共享相同的参数，这是
    CNN 的第二个特征。相比之下，在传统神经网络中，输出的参数是相互独立的。然而，CNN 为所有输出应用相同的参数，以减少参数数量，从而带来第三个特征：平移不变性。这意味着即使一个物体的特征从一个位置移动到另一个位置，CNN
    模型仍然能够捕捉到它，无论它出现在哪里。具体而言，一个常见的卷积层由三个传统组件组成：线性映射、激活函数和池化函数。类似于其他现代神经网络架构，激活函数用于将非线性映射特性引入网络中。通常，修正线性单元（ReLU）是首选。池化利用局部区域的统计特征来表示指定位置的输出。以最大池化步骤为例，它使用最大值替代输入区域。显然，池化操作对小的变化和噪声干扰具有鲁棒性，这些干扰可以通过池化操作在输出中平滑掉，从而保留更多的抽象特征。
- en: In the early works of applying CNNs for HSI classification, two-dimensional
    convolution was the most widely used method, which is mainly employed to extract
    spatial texture information [[47](#bib.bib47), [28](#bib.bib28), [48](#bib.bib48)],
    but the redundant bands greatly enlarge the size of the convolutional kernel,
    especially the channel dimensionality. Later, a combination of one-dimensional
    convolution and two-dimensional convolution appeared [[49](#bib.bib49)] to solve
    the above problem. Concretely, one-dimensional and two-dimensional convolutions
    are responsible for extracting spectral and spatial features, respectively. The
    two types of features are then fused before being input to the classifier. For
    the small training sample problem, due to insufficient labeled samples, it is
    difficult for CNNs to learn effective features. For this reason, some researchers
    usually introduced traditional machine learning methods, such as attribute profiles [[50](#bib.bib50)],
    GLCM [[51](#bib.bib51)], hash learning [[52](#bib.bib52)], and Markov Random fields [[53](#bib.bib53)],
    to introduce prior information to the convolutional network and improve the performance
    of the network. Similar to the trend of autoencoder-based classification methods,
    three-dimensional CNN models have also been applied to HSI classification in recent
    years and have shown better feature fusion capabilities [[54](#bib.bib54), [55](#bib.bib55)].
    However, due to the large number of parameters, three-dimensional convolution
    is not suitable for solving small-sample classification problems under supervised
    learning. To reduce the number of parameters of 3D convolution, Fang *et al.* [[56](#bib.bib56)]
    designed a 3D separable convolution. In contrast, Mou *et al.* [[57](#bib.bib57),
    [58](#bib.bib58)] introduced an autoencoder scheme into the three-dimensional
    convolution module to solve this problem. By a combination with the classic autoencoder
    training method, the three-dimensional convolution autoencoder can be trained
    in an unsupervised learning manner, and then, the decoder is replaced with a classifier,
    while the parameters of the encoder are frozen. Finally, a small classifier is
    trained by supervised learning. Moreover, due to the success of ResNet [[59](#bib.bib59)],
    scholars have studied the HSI classification problem based on convolutional residuals [[57](#bib.bib57),
    [58](#bib.bib58), [60](#bib.bib60), [61](#bib.bib61)]. These methods try to use
    jump connections to enable the network to learn complex features with a small
    number of labeled samples. Similarly, CNNs with dense connections have also been
    introduced into this field [[62](#bib.bib62), [63](#bib.bib63)]. In addition,
    the attention mechanism is another hotpot for fully mining sample features. Concretely,
    Haut and Xiong *et al.* [[64](#bib.bib64), [65](#bib.bib65)] incorporated the
    attention mechanism with CNNs for HSI classification. Although the above models
    can work well on HSI, they cannot overcome the disadvantage of the low spatial
    resolution of HSIs, which may cause mixed pixels. To make up for this shortcoming,
    multimodality CNN models have been proposed. These methods [[66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68)] combine HSIs and LiDAR data together to increase
    the discriminability of sample features. Moreover, to achieve good performance
    under the small-sample scenario, Yu *et al.* [[28](#bib.bib28)] enlarged the training
    set through data augmentation by implementing rotation and flipping. On the one
    hand, this method increases the number of samples and improves their diversity.
    On the other hand, it enhances the model’s ability of rotation invariance, which
    is important in some fields such as remote sensing. Subsequently, Li *et al.* [[69](#bib.bib69),
    [70](#bib.bib70)] designed a data augmentation scheme for HSI classification.
    They combined the samples in pairs so that the model no longer learns the characteristics
    of the samples themselves but learns the differences between the samples. Different
    combinations make the scale of the training set larger, which is more conducive
    for model training.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期应用CNN进行HSI分类的研究中，二维卷积是最广泛使用的方法，主要用于提取空间纹理信息[[47](#bib.bib47), [28](#bib.bib28),
    [48](#bib.bib48)]，但冗余的波段大大增加了卷积核的大小，尤其是通道维度。后来，出现了一维卷积和二维卷积的组合[[49](#bib.bib49)]来解决上述问题。具体来说，一维和二维卷积分别负责提取光谱和空间特征。然后，这两种特征在输入分类器之前进行融合。对于小样本训练问题，由于标记样本不足，CNN很难学习到有效的特征。为此，一些研究人员通常引入传统的机器学习方法，如属性谱[[50](#bib.bib50)]、GLCM[[51](#bib.bib51)]、哈希学习[[52](#bib.bib52)]和马尔可夫随机场[[53](#bib.bib53)]，以引入先验信息来改善网络的性能。与基于自编码器的分类方法趋势类似，近年来，三维CNN模型也被应用于HSI分类，并展示了更好的特征融合能力[[54](#bib.bib54),
    [55](#bib.bib55)]。然而，由于参数数量庞大，三维卷积不适合解决监督学习中的小样本分类问题。为了减少3D卷积的参数数量，Fang *et al.*
    [[56](#bib.bib56)] 设计了一种3D可分离卷积。相比之下，Mou *et al.* [[57](#bib.bib57), [58](#bib.bib58)]
    将自编码器方案引入三维卷积模块来解决这个问题。通过与经典自编码器训练方法的结合，三维卷积自编码器可以以无监督学习的方式进行训练，然后用分类器替代解码器，同时固定编码器的参数。最后，通过监督学习训练一个小型分类器。此外，由于ResNet的成功[[59](#bib.bib59)]，学者们研究了基于卷积残差的HSI分类问题[[57](#bib.bib57),
    [58](#bib.bib58), [60](#bib.bib60), [61](#bib.bib61)]。这些方法尝试利用跳跃连接使网络能够在少量标记样本的情况下学习复杂特征。类似地，具有密集连接的CNN也被引入这一领域[[62](#bib.bib62),
    [63](#bib.bib63)]。此外，注意机制是另一个热门领域，用于充分挖掘样本特征。具体来说，Haut和Xiong *et al.* [[64](#bib.bib64),
    [65](#bib.bib65)] 将注意机制与CNN结合用于HSI分类。尽管上述模型在HSI上表现良好，但它们不能克服HSI低空间分辨率的缺陷，这可能导致混合像素。为弥补这一缺点，提出了多模态CNN模型。这些方法[[66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68)] 将HSI和LiDAR数据结合起来，以增加样本特征的可分辨性。此外，为了在小样本场景下实现良好性能，Yu
    *et al.* [[28](#bib.bib28)] 通过实施旋转和翻转的数据增强来扩大训练集。一方面，这种方法增加了样本数量并提高了样本的多样性。另一方面，它增强了模型的旋转不变性，这在一些领域如遥感中非常重要。随后，Li
    *et al.* [[69](#bib.bib69), [70](#bib.bib70)] 设计了一种用于HSI分类的数据增强方案。他们将样本成对组合，使模型不再学习样本自身的特征，而是学习样本之间的差异。不同的组合使训练集的规模更大，这更有利于模型训练。
- en: 2.3 Recurrent neural network (RNN) for HSI classification
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 HSI分类的递归神经网络（RNN）
- en: 'Compared with other forms of neural networks, recurrent neural networks (RNNs) [[71](#bib.bib71)]
    have memory capabilities and can record the context information of sequential
    data. Because of this memory characteristic, recurrent neural networks are widely
    used in tasks such as speech recognition and machine translation. More precisely,
    the input of a recurrent neural network is usually a sequence of vectors. At each
    time step $t$, the network receives an element $\bm{x}_{t}$ in a sequence and
    the state $\bm{h}_{t-1}$ of the previous time step, and produces an output $\bm{y}_{t}$
    and a state $\bm{h}_{t}$ representing the context information at the current moment.
    This process can be formulated as:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他形式的神经网络相比，递归神经网络（RNNs） [[71](#bib.bib71)]具有记忆能力，可以记录序列数据的上下文信息。由于这种记忆特性，递归神经网络在语音识别和机器翻译等任务中得到了广泛应用。更准确地说，递归神经网络的输入通常是一个向量序列。在每个时间步
    $t$，网络接收序列中的一个元素 $\bm{x}_{t}$ 和前一个时间步的状态 $\bm{h}_{t-1}$，并生成一个输出 $\bm{y}_{t}$ 和一个表示当前时刻上下文信息的状态
    $\bm{h}_{t}$。这个过程可以表示为：
- en: '|  | $\bm{h}_{t}=f(\mathbf{W}_{hh}\bm{h}_{t-1}+\mathbf{W}_{xh}\bm{x}_{t}+\mathbf{b})$
    |  | (2) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{h}_{t}=f(\mathbf{W}_{hh}\bm{h}_{t-1}+\mathbf{W}_{xh}\bm{x}_{t}+\mathbf{b})$
    |  | (2) |'
- en: where $\mathbf{W}_{xh}$ represents the weight matrix from the input layer to
    the hidden layer, $\mathbf{W}_{hh}$ denotes the state transition weight in the
    hidden layer, and $\mathbf{b}$ is the bias. It can be seen that the current state
    of the recurrent neural network is controlled by both the state of the previous
    time step and the current input. This mechanism allows the recurrent neural network
    to capture the contextual semantic information implicitly between the input vectors.
    For example, in the machine translation task, it can enable the network to understand
    the semantic relationship between words in a sentence.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{W}_{xh}$ 代表从输入层到隐藏层的权重矩阵，$\mathbf{W}_{hh}$ 表示隐藏层中的状态转移权重，$\mathbf{b}$
    是偏置。可以看出，递归神经网络的当前状态由前一个时间步的状态和当前输入共同控制。这种机制使得递归神经网络能够隐式地捕捉输入向量之间的上下文语义信息。例如，在机器翻译任务中，它可以使网络理解句子中词汇之间的语义关系。
- en: However, the classic RNN is prone to encounter gradient explosion or gradient
    vanishing problems during the training process. When there are too many inputs,
    the derivation chain of the RNN will become too long, making the gradient value
    close to infinity or zero. Therefore, the classic RNN model is replaced by a long
    short-term memory (LSTM) network [[71](#bib.bib71)] or a gated recurrent unit
    (GRU) [[72](#bib.bib72)] in the HSI classification task.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，经典的RNN在训练过程中容易遇到梯度爆炸或梯度消失的问题。当输入过多时，RNN的导数链会变得过长，导致梯度值接近无穷大或零。因此，在HSI分类任务中，经典的RNN模型被长短期记忆（LSTM）网络 [[71](#bib.bib71)]或门控递归单元（GRU） [[72](#bib.bib72)]替代。
- en: 'Both LSTM and GRU use gating technology to filter the input and the previous
    state so that the network can forget unnecessary information and retain the most
    valuable context. LSTM maintains an internal memory state, and there are three
    gates: input gate $\bm{i}_{t}$, forget gate $\bm{f}_{t}$ and output gate $\bm{o}_{t}$,
    which are formulated as:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM和GRU都使用门控技术来过滤输入和前一个状态，以便网络能够忘记不必要的信息并保留最有价值的上下文。LSTM保持内部记忆状态，并且有三个门：输入门
    $\bm{i}_{t}$、遗忘门 $\bm{f}_{t}$ 和输出门 $\bm{o}_{t}$，它们的公式为：
- en: '|  | $\bm{i}_{t}=\sigma(\mathbf{W_{i}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (3) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{i}_{t}=\sigma(\mathbf{W_{i}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (3) |'
- en: '|  | $\bm{f}_{t}=\sigma(\mathbf{W_{f}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (4) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{f}_{t}=\sigma(\mathbf{W_{f}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (4) |'
- en: '|  | $\bm{o}_{t}=\sigma(\mathbf{W_{io}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (5) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{o}_{t}=\sigma(\mathbf{W_{io}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (5) |'
- en: 'It can be found that the three gates are generated based on the current input
    and the previous state. First, the current input and the previous state will be
    spliced and mapped to a new input $\bm{g}_{t}$ according to the following formula:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 可以发现，这三个门是基于当前输入和前一个状态生成的。首先，当前输入和前一个状态将被拼接并映射到一个新的输入 $\bm{g}_{t}$，公式如下：
- en: '|  | $\bm{g}_{t}=\tanh(\mathbf{W_{g}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (6) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{g}_{t}=\tanh(\mathbf{W_{g}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (6) |'
- en: Subsequently, the input gate, the forget gate, the new input $\bm{g}_{t}$ and
    the internal memory unit $\hat{\bm{h}}_{t-1}$ update the internal memory state
    tegother. In this process, LSTM discards invalid information and adds new semantic
    information.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，输入门、遗忘门、新输入$\bm{g}_{t}$和内部记忆单元$\hat{\bm{h}}_{t-1}$一起更新内部记忆状态。在这个过程中，LSTM丢弃无效的信息并添加新的语义信息。
- en: '|  | $\hat{\bm{h}}_{t}=\bm{f}_{t}\odot\hat{\bm{h}}_{t-1}+\bm{i}_{t}\odot\bm{g}_{t}$
    |  | (7) |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\bm{h}}_{t}=\bm{f}_{t}\odot\hat{\bm{h}}_{t-1}+\bm{i}_{t}\odot\bm{g}_{t}$
    |  | (7) |'
- en: Finally, the new internal memory state is filtered by the output gate to form
    the output of the current time step
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，新的内部记忆状态通过输出门进行过滤，形成当前时间步的输出。
- en: '|  | $\bm{h}_{t}=\bm{o}_{t}\odot\tanh(\hat{\bm{h}}_{t})$ |  | (8) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{h}_{t}=\bm{o}_{t}\odot\tanh(\hat{\bm{h}}_{t})$ |  | (8) |'
- en: Concerning HSI processing, each spectral image is a high-dimensional vector
    and can be regarded as a sequence of data. There are many works using LSTM for
    HSI classification tasks. For instance, Mou *et al.* [[73](#bib.bib73)] proposed
    an LSTM-based HSI classification method for the first time, and their work only
    focused on spectral information. For each sample pixel vector, each band is input
    into the LSTM step by step. To improve the performance of the model, spatial information
    is considered in subsequent research. For example, Liu *et al.* fully considered
    the spatial neighborhood of the sample and used a multilayer LSTM to extract spatial
    spectrum features [[74](#bib.bib74)]. Specifically, in each time step, the sampling
    points of the neighborhood are sequentially input into the network to deeply mine
    the context information in the spatial neighborhood. In [[75](#bib.bib75)], Zhou
    *et al.* used two LSTMs to extract spectral features and spatial features. In
    particular, for the extraction of spatial features, PCA is first used to extract
    principal components from the sample rectangular space neighborhood. Then, the
    first principal component is divided into several lines to form a set of sequence
    data, and gradually input into the network. In contrast, Ma and Zhang *et al.* [[76](#bib.bib76),
    [77](#bib.bib77)] measures the similarity between the sample point in the spatial
    neighborhood and the center point. The sample points in the neighborhood will
    be reordered according to the similarity and then input into the network step
    by step. This approach allows the network to focus on learning sample points that
    are highly similar to the center point, and the memory of the internal hidden
    state can thus be enhanced. Erting Pan *et al.* [[78](#bib.bib78)] proposed an
    effective tiny model for spectral-spatial classification on HSIs based on a single
    gate recurrent unit (GRU). In this work, the rectangular space neighborhood is
    flattened into a vector, which is used to initialize the hidden vector $h_{0}$
    of GRU, and the center point pixel vector is input into the network to learn features.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 关于HSI处理，每个光谱图像是一个高维向量，可以视为数据序列。许多研究使用LSTM进行HSI分类任务。例如，Mou *et al.* [[73](#bib.bib73)]
    首次提出了一种基于LSTM的HSI分类方法，他们的工作仅关注于光谱信息。对于每个样本像素向量，每个波段逐步输入到LSTM中。为了提高模型性能，后续研究考虑了空间信息。例如，Liu
    *et al.* 充分考虑了样本的空间邻域，并使用多层LSTM来提取空间光谱特征[[74](#bib.bib74)]。具体来说，在每个时间步，邻域的采样点按顺序输入到网络中，以深入挖掘空间邻域中的上下文信息。在[[75](#bib.bib75)]中，Zhou
    *et al.* 使用两个LSTM来提取光谱特征和空间特征。特别地，针对空间特征的提取，首先使用PCA从样本矩形空间邻域中提取主成分。然后，将第一个主成分分成几条线，形成一组序列数据，并逐步输入到网络中。相比之下，Ma
    和 Zhang *et al.* [[76](#bib.bib76), [77](#bib.bib77)] 测量空间邻域内样本点与中心点之间的相似性。邻域内的样本点将根据相似性重新排序，然后逐步输入到网络中。这种方法使网络能够集中学习与中心点高度相似的样本点，从而增强了内部隐状态的记忆。Erting
    Pan *et al.* [[78](#bib.bib78)] 提出了一个有效的基于单门循环单元（GRU）的光谱-空间分类的小型模型。在这项工作中，矩形空间邻域被展平为一个向量，用于初始化GRU的隐藏向量$h_{0}$，并将中心点像素向量输入到网络中进行特征学习。
- en: In addition, Wu and Saurabh argue that it is difficult to dig out the internal
    features of the sample by directly inputting a single original spectral vector
    into the RNN [[79](#bib.bib79), [80](#bib.bib80)]. The authors use a one-dimensional
    convolution operator to extract multiple feature vectors from the spectrum vector,
    which form a feature sequence and are then input to the RNN. Finally, the fully
    connected layer and the softmax function are adopted to obtain the classification
    result. It can be seen that only using recurrent neural networks or one-dimensional
    convolution to extract the spatial-spectrum joint features is actually not efficient
    because this will cause the loss of spatial structure information. Therefore,
    some researchers combine two-dimensional/three-dimensional CNNs with an RNN and
    use convolution operators to extract spatial-spectral joint features. For example,
    Hao *et al.* [[81](#bib.bib81)] utilized U-Net to extract features and input them
    into an LSTM or GRU so that the contextual information between features could
    be explored. Moreover, Shi *et al.* [[82](#bib.bib82)] introduced the concept
    of the directional sequence to fully extract the spatial structure information
    of HSIs. First, the rectangular area of the sampling point is divided into nine
    overlapping patches. Second, the patch will be mapped to a set of feature vectors
    through a three-dimensional convolutional network, and the relative position of
    the patch can generate 8 combinations of directions (for example, top, middle,
    bottom, left, center, and right) to form a direction sequence. Finally, the sequence
    is input into the LSTM or GRU to obtain the classification result. In this way,
    the spatial distribution and structural characteristics of the features can be
    explored.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，吴和Saurabh认为，直接将单一原始光谱向量输入到RNN中，很难挖掘样本的内部特征[[79](#bib.bib79), [80](#bib.bib80)]。作者使用一维卷积操作从光谱向量中提取多个特征向量，这些特征向量形成特征序列，然后输入到RNN中。最后，采用全连接层和softmax函数来获得分类结果。可以看出，仅使用递归神经网络或一维卷积来提取空间-光谱联合特征实际上效率不高，因为这会导致空间结构信息的丢失。因此，一些研究人员将二维/三维卷积神经网络（CNN）与RNN结合使用，并利用卷积操作提取空间-光谱联合特征。例如，Hao
    *等人* [[81](#bib.bib81)] 利用U-Net提取特征并将其输入到LSTM或GRU中，从而可以探索特征之间的上下文信息。此外，Shi *等人*
    [[82](#bib.bib82)] 引入了方向序列的概念，以充分提取高光谱图像的空间结构信息。首先，将采样点的矩形区域划分为九个重叠的补丁。其次，通过三维卷积网络将补丁映射为一组特征向量，补丁的相对位置可以生成8种方向组合（例如，上、中、下、左、中间和右），以形成一个方向序列。最后，将该序列输入到LSTM或GRU中，以获得分类结果。通过这种方式，可以探索特征的空间分布和结构特征。
- en: 3 Deep learning paradigms for HSI classification with few labeled samples
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 少量标注样本的高光谱图像分类深度学习范式
- en: Although different HSI classification methods have different specific designs,
    they all follow some learning paradigms. In this section, we mainly introduce
    several learning paradigms that are applied to HSI classification with few labeled
    training samples. These learning paradigms are based on specific learning theories.
    We hope to provide a general guide for researchers to design algorithms.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不同的高光谱图像（HSI）分类方法有不同的具体设计，但它们都遵循某些学习范式。在这一节中，我们主要介绍几种应用于高光谱图像分类且具有少量标注训练样本的学习范式。这些学习范式基于特定的学习理论。我们希望为研究人员设计算法提供一个通用的指南。
- en: 3.1 Deep Transfer Learning for HSI classification
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 高光谱图像分类的深度迁移学习
- en: 'Transfer learning [[83](#bib.bib83)] is an effective method to deal with the
    small-sample problem. Transfer learning tries to transfer knowledge learned from
    one domain to another. First, there are two data sets/domains, one is called a
    source domain that contains abundant labeled samples, and the other is called
    a target domain and only contains few labeled samples. To facilitate the subsequent
    description, we define the source domain as $\mathbf{D}_{s}$, the target domain
    as $\mathbf{D}_{t}$, and their label spaces as $\mathbf{Y}_{s}$ and $\mathbf{Y}_{t}$,
    respectively. Usually, the data distribution of the source domain and the target
    domain are inconsistent: $P(\bm{X}_{s})\neq P(\bm{X}_{t})$. Therefore, the purpose
    of transfer learning is to use the knowledge learned from $\mathbf{D}_{s}$ to
    identify the labels of samples in $\mathbf{D}_{t}$.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习 [[83](#bib.bib83)] 是解决小样本问题的有效方法。迁移学习试图将从一个领域学到的知识转移到另一个领域。首先，有两个数据集/领域，一个是包含大量标记样本的源领域，另一个是只包含少量标记样本的目标领域。为了便于后续描述，我们将源领域定义为
    $\mathbf{D}_{s}$，目标领域定义为 $\mathbf{D}_{t}$，它们的标签空间分别为 $\mathbf{Y}_{s}$ 和 $\mathbf{Y}_{t}$。通常，源领域和目标领域的数据分布是不一致的：$P(\bm{X}_{s})\neq
    P(\bm{X}_{t})$。因此，迁移学习的目的是利用从 $\mathbf{D}_{s}$ 中学到的知识来识别 $\mathbf{D}_{t}$ 中样本的标签。
- en: Fine-tuning is a general method in transfer learning that uses $\mathbf{D}_{s}$
    to train the model and adjust it by $\mathbf{D}_{t}$. Its original motivation
    is to reduce the number of samples needed during the training process. Since deep
    learning models generally contain a vast number of parameters and if it is trained
    on the target domain $\mathbf{D}_{t}$, it is easy to overfit and perform poorly
    in practice. However, fine-tuning allows the model parameters to reach a suboptimal
    state, and a small number of training samples of the target domain can tune the
    model to reach the optimal state. It involves two steps. First, the specific model
    will be fully trained on the source domain $\mathbf{D}_{s}$ with abundant labeled
    samples to make the model parameters arrive at a good state. Then, the model is
    transferred to the target domain $\mathbf{D}_{t}$, except for some task-related
    modules, and slightly tuned on $\mathbf{D}_{t}$ so that the model fits the data
    distribution of the target domain $\mathbf{D}_{t}$.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是迁移学习中的一种通用方法，它使用 $\mathbf{D}_{s}$ 来训练模型，并通过 $\mathbf{D}_{t}$ 进行调整。其最初动机是减少训练过程中所需的样本数量。由于深度学习模型通常包含大量参数，如果在目标领域
    $\mathbf{D}_{t}$ 上进行训练，容易出现过拟合现象，实际表现不佳。然而，微调允许模型参数达到次优状态，并且少量目标领域的训练样本可以将模型调整到最佳状态。它包括两个步骤。首先，使用大量标记样本在源领域
    $\mathbf{D}_{s}$ 上对特定模型进行全面训练，使模型参数达到良好状态。然后，除了某些与任务相关的模块外，将模型转移到目标领域 $\mathbf{D}_{t}$，并在
    $\mathbf{D}_{t}$ 上进行轻微调整，以使模型适应目标领域 $\mathbf{D}_{t}$ 的数据分布。
- en: '![Refer to caption](img/e7e11e3a60a62fa97b36ff2befbf2611.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e7e11e3a60a62fa97b36ff2befbf2611.png)'
- en: 'Figure 4: Flowchart of the fine-tuning method. The solid line represents pretraining,
    and the dashed line represents fine-tuning. $f_{\omega}$ is a learning function.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：微调方法的流程图。实线代表预训练，虚线代表微调。$f_{\omega}$ 是一个学习函数。
- en: Because the fine-tuning method is relatively simple, it is widely used in the
    transfer learning method for hyperspectral image classification. To our knowledge,
    Yang *et al.* [[84](#bib.bib84)] are the first to combine deep learning with transfer
    learning to classify hyperspectral images. The model consists of two convolutional
    neural networks, which are used to extract spectral features and spatial features.
    Then, the joint spectral-spatial feature will be input into the fully connected
    layer to gain a final result. According to fine-tuning, the model is first fully
    trained on the hyperspectral image of the source domain. Next, the fully connected
    layer is replaced and the parameters of the convolutional network are reserved.
    Finally, the transfer model will be trained on the target hyperspectral image
    to adapt to the new data distribution. The later transfer learning models based
    on fine-tuning basically follow that architecture [[85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88)]. It is worth noting that Deng *et al.* [[89](#bib.bib89)]
    combined transfer learning with active learning to classify HSI.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于微调方法相对简单，广泛用于高光谱图像分类的迁移学习方法中。据我们所知，杨等人[[84](#bib.bib84)]是第一个将深度学习与迁移学习结合起来对高光谱图像进行分类的人。该模型由两个卷积神经网络组成，用于提取光谱特征和空间特征。然后，联合光谱-空间特征将输入到全连接层中得到最终结果。根据微调，该模型首先在源域的高光谱图像上进行全面训练。接下来，完全连接层被替换，卷积网络的参数被保留。最后，迁移模型将在目标高光谱图像上进行训练，以适应新的数据分布。后续基于微调的迁移学习模型基本遵循该结构[[85](#bib.bib85),
    [86](#bib.bib86), [87](#bib.bib87), [88](#bib.bib88)]。值得注意的是，邓等人[[89](#bib.bib89)]将迁移学习与主动学习相结合，用于高光谱图像的分类。
- en: Data distribution adaptation is another commonly used transfer learning method.
    The basic idea of this theory is that in the original feature space, the data
    probability distributions of the source domain and the target domain are usually
    different. However, they can be mapped to a common feature space together. In
    this space, their data probability distributions become similar. In 2014, Ghifary
    *et al.* [[90](#bib.bib90)] first proposed a shadow neural network-based domain
    adaptation model, called DaNN. The innovation of this work is that a maximum mean
    discrepancy (MMD) adaptation layer is added to calculate the distance between
    the source domain and the target domain. Moreover, the distance is merged into
    the loss function to reduce the difference between the two data distributions.
    Subsequently, Tzeng *et al.* [[91](#bib.bib91)] extended this work with a deeper
    network and proposed deep domain confusion to solve the adaptive problem of deep
    networks.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分布自适应是另一种常用的迁移学习方法。该理论的基本思想是，在原始特征空间中，源域和目标域的数据概率分布通常是不同的。然而，它们可以一起映射到一个共同的特征空间中。在这个空间中，它们的数据概率分布变得相似。2014年，Ghifary等人[[90](#bib.bib90)]首次提出了基于方阵神经网络的领域自适应模型DaNN。这项工作的创新之处在于添加了最大均值差异（MMD）适应层，用于计算源域和目标域之间的距离。此外，该距离被合并到损失函数中，以减小两个数据分布之间的差异。随后，Tzeng等人[[91](#bib.bib91)]基于更深的网络对这项工作进行了扩展，并提出了深度领域混乱来解决深度网络的自适应问题。
- en: '![Refer to caption](img/df2392978c69fc2eba6e8ebe39aaf3f7.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/df2392978c69fc2eba6e8ebe39aaf3f7.png)'
- en: 'Figure 5: Flowchart of DANN.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：DANN的流程图。
- en: Wang *et al.* [[92](#bib.bib92)] introduced the deep domain adaptation model
    to the field of hyperspectral image classification for the first time. In [[92](#bib.bib92)],
    two hyperspectral images from different scenes will be mapped to two low-dimensional
    subspaces by the deep neural network, in which the samples are represented as
    manifolds. MMD is used to measure the distance between two low-dimensional subspaces
    and is added to the loss function to make two low-dimensional subspaces have high
    similarity. In addition, they still add the sum of the distances between samples
    and their neighbor into the loss function to ensure that the low-dimensional manifold
    is discriminative.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人*et al.* [[92](#bib.bib92)]首次将深度领域自适应模型引入到高光谱图像分类领域。在[[92](#bib.bib92)]中，两个来自不同场景的高光谱图像将通过深度神经网络映射到两个低维子空间中，其中样本被表示为流形。使用最大均值差异（MMD）来度量两个低维子空间之间的距离，并将其添加到损失函数中，使得两个低维子空间之间具有高相似性。此外，他们还将样本与其邻居之间的距离之和加入到损失函数中，以确保低维流形具有判别性。
- en: Motivated by the excellent performance of generative adversarial net (GAN),
    Yaroslav *et al.* [[93](#bib.bib93)] first introduced it into transfer learning.
    The network is named DANN (domain-adversarial neural network), which is different
    from DaNN proposed by Ghifary *et al.* [[90](#bib.bib90)]. The generator $\mathbf{G}_{f}$
    and the discriminator $\mathbf{G}_{d}$ compete with each other until they have
    converged. In transfer learning, the data in one of the domains (usually the target
    domain) are regarded as the generated sample. The generator aims to learn the
    characteristics of the target domain sample so that the discriminator cannot distinguish
    which domain the sample comes from to achieve the purpose of domain adaptation.
    Therefore, $\mathbf{G}_{f}$ is used to represent the feature extractor here.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 受生成对抗网络（GAN）卓越性能的启发，Yaroslav *等人* [[93](#bib.bib93)] 首次将其引入迁移学习。该网络被命名为DANN（领域对抗神经网络），不同于Ghifary
    *等人* [[90](#bib.bib90)] 提出的DaNN。生成器$\mathbf{G}_{f}$和鉴别器$\mathbf{G}_{d}$互相竞争，直到它们收敛。在迁移学习中，一个领域的数据（通常是目标领域）被视为生成样本。生成器旨在学习目标领域样本的特征，使鉴别器无法区分样本来自哪个领域，从而实现领域适应的目的。因此，$\mathbf{G}_{f}$
    在这里用来表示特征提取器。
- en: Elshamli *et al.* [[94](#bib.bib94)] first introduced the concept of DANN to
    the task of hyperspectral image classification. Compared to general GNN, it has
    two discriminators. One is the class discriminator predicting the class labels
    of samples, and the other is the domain discriminator predicting the source of
    the samples. Different from the two-stage method, DANN is an end-to-end model
    that can perform representation learning and classification tasks simultaneously.
    Moreover, it is easy to train. Further, it outperforms two-stage frameworks such
    as the denoising autoencoder and traditional approaches such as PCA in hyperspectral
    image classification.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Elshamli *等人* [[94](#bib.bib94)] 首次将DANN概念引入高光谱图像分类任务。与一般的GNN相比，它有两个鉴别器。一个是预测样本类别标签的类别鉴别器，另一个是预测样本来源的领域鉴别器。不同于两阶段方法，DANN是一个端到端的模型，能够同时进行表示学习和分类任务。此外，它容易训练。进一步地，它在高光谱图像分类中优于两阶段框架，如去噪自编码器和传统方法，如PCA。
- en: 3.2 Deep Active Learning for HSI classification
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 高光谱图像分类的深度主动学习
- en: 'Active learning [[95](#bib.bib95)] in the supervised learning method can efficiently
    deal with small-sample problems. It can effectively learn discriminative features
    by autonomously selecting representative or high-information samples from the
    training set, especially when the labeled samples are scarce. Generally speaking,
    active learning consists of five components, $A=(C,L,U,Q,S)$. Among them, $C$
    represents one or a group of classifiers. $L$ and $U$ represent the labeled samples
    and unlabeled samples, respectively. $Q$ is the query function, which is used
    to query the samples with a large amount of information among the unlabeled samples.
    $S$ is an expert and can label unlabeled samples. In general, active learning
    has two stages. The first stage is the initialization stage. In this stage, a
    small number of samples will be randomly selected to form the training set $L$
    and be labeled by experts to train the classifier. The second stage is the iterative
    query. $Q$ will select new samples from the unlabeled sample set $U$ for $S$ to
    mark them based on the results of the previous iteration and add them to the training
    set $L$. The active learning method applied to hyperspectral image classification
    is mainly based on the active learning algorithm of the committee and the active
    learning algorithm based on the posterior probability. In the committee-based
    active learning algorithm, the EQB method uses entropy to measure the amount of
    information in unlabeled samples. Specifically, the training set L will be divided
    into $k$ subsets to train $k$ classifiers and then use these $k$ classifiers to
    classify all unlabeled samples. Therefore, each unlabeled sample corresponds to
    k predicted labels. The entropy value is calculated from this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 主动学习 [[95](#bib.bib95)] 在监督学习方法中可以有效处理小样本问题。它可以通过从训练集中自主选择具有代表性或高信息量的样本来有效学习区分特征，特别是在标注样本稀缺的情况下。一般来说，主动学习由五个组成部分构成，$A=(C,L,U,Q,S)$。其中，$C$
    代表一个或一组分类器。$L$ 和 $U$ 分别代表标注样本和未标注样本。$Q$ 是查询函数，用于查询未标注样本中信息量大的样本。$S$ 是专家，可以对未标注样本进行标注。一般来说，主动学习有两个阶段。第一个阶段是初始化阶段。在此阶段，将随机选择少量样本来形成训练集
    $L$ 并由专家标注以训练分类器。第二个阶段是迭代查询。$Q$ 将从未标注样本集 $U$ 中选择新样本供 $S$ 根据前一次迭代的结果进行标注，并将其添加到训练集
    $L$ 中。应用于高光谱图像分类的主动学习方法主要基于委员会的主动学习算法和基于后验概率的主动学习算法。在基于委员会的主动学习算法中，EQB 方法使用熵来衡量未标注样本的信息量。具体而言，训练集
    $L$ 将被划分为 $k$ 个子集以训练 $k$ 个分类器，然后使用这 $k$ 个分类器对所有未标注样本进行分类。因此，每个未标注样本对应 $k$ 个预测标签。熵值由此计算得出：
- en: '|  | $\bm{x}^{EQB}=\mathop{\arg\min}_{x_{i}\in U}\frac{H^{EQB}(x_{i})}{log(N_{i})}$
    |  | (9) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x}^{EQB}=\mathop{\arg\min}_{x_{i}\in U}\frac{H^{EQB}(x_{i})}{log(N_{i})}$
    |  | (9) |'
- en: where $H$ represents the entropy value, and $N_{i}$ represents the number of
    classes predicted by the sample $x_{i}$. Samples with large entropy will be selected
    and manually labeled [[96](#bib.bib96)]. In [[97](#bib.bib97)], the deep belief
    network is used to generate the mapping feature $h$ of the input $x$ in an unsupervised
    way, and then, $h$ will be used to calculate the information entropy. At the same
    time, sparse representation is used to estimate the representations of the sample.
    In the process of selecting samples for active learning, the information entropy
    and representations of the samples are comprehensively considered.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$H$ 代表熵值，$N_{i}$ 代表样本 $x_{i}$ 预测的类别数量。具有较大熵值的样本将被选择并手动标注 [[96](#bib.bib96)]。在
    [[97](#bib.bib97)] 中，深度置信网络被用于以无监督的方式生成输入 $x$ 的映射特征 $h$，然后用 $h$ 计算信息熵。同时，稀疏表示用于估计样本的表示。在选择主动学习样本的过程中，将综合考虑样本的信息熵和表示。
- en: 'In contrast, the active learning method based on posterior probability [[98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100)] is more widely used. Breaking ties belongs
    to the active learning method of posterior probability, which is widely used in
    hyperspectral classification tasks. This method first uses specifies models, such
    as convolutional networks, maximum likelihood estimation classifiers, support
    vector machines, etc., to estimate the posterior probabilities of all samples
    in the candidate pool. Then, the approach uses the posterior probability to input
    the following formula to produce a measure of sample uncertainty:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，基于后验概率的主动学习方法[[98](#bib.bib98), [99](#bib.bib99), [100](#bib.bib100)]使用得更为广泛。打破平衡属于后验概率的主动学习方法，这在高光谱分类任务中被广泛使用。这种方法首先使用指定的模型，例如卷积网络、最大似然估计分类器、支持向量机等，来估计候选池中所有样本的后验概率。然后，该方法利用后验概率输入以下公式来生成样本不确定性的度量：
- en: '|  | $\bm{x}^{BT}=\mathop{\arg\min}_{x_{i}\in U}\left\{\mathop{\max}_{w\in
    N}p\left(y_{i}^{*}=w&#124;x_{i}\right)-\mathop{\max}_{w\in N\setminus w^{+}}p(y_{i}^{*}=w&#124;x_{i})\right\}$
    |  | (10) |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{x}^{BT}=\mathop{\arg\min}_{x_{i}\in U}\left\{\mathop{\max}_{w\in
    N}p\left(y_{i}^{*}=w&#124;x_{i}\right)-\mathop{\max}_{w\in N\setminus w^{+}}p(y_{i}^{*}=w&#124;x_{i})\right\}$
    |  | (10) |'
- en: In the above formula, we first calculate the difference between the largest
    probability and the second-largest probability among the posterior probabilities
    of all candidate samples and select the sample with the minimum difference to
    join the valuable data set. The lower $x^{BT}$ is, the more uncertain is the sample.
    In [[98](#bib.bib98)], Li *et al.* first used an autoencoder to construct an active
    learning model for hyperspectral image classification tasks. At the same time,
    Sun *et al.* [[99](#bib.bib99)] also proposed a similar method. However, this
    method only uses spectral features. Because of the effectiveness of spatial information,
    in [[101](#bib.bib101)], when generating the posterior probability, the space-spectrum
    joint features are considered at the same time. In contrast, Cao *et al.* [[100](#bib.bib100)]
    use convolutional neural networks to generate the posterior probability.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，我们首先计算所有候选样本的后验概率中最大概率与第二大概率之间的差异，并选择差异最小的样本加入有价值的数据集中。$x^{BT}$ 越低，样本的不确定性越大。在[[98](#bib.bib98)]中，Li
    *等*人首次使用自编码器构建了一个用于高光谱图像分类任务的主动学习模型。同时，Sun *等*人[[99](#bib.bib99)]也提出了类似的方法。然而，该方法仅使用光谱特征。由于空间信息的有效性，在[[101](#bib.bib101)]中，在生成后验概率时，同时考虑了空间-光谱联合特征。相比之下，Cao
    *等*人[[100](#bib.bib100)]使用卷积神经网络生成后验概率。
- en: In general, the active learning method can automatically select effective samples
    according to certain criteria, reduce inefficient redundant samples, and thus
    well alleviate the problem of missing training samples in the small-sample problem.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，主动学习方法可以根据一定的标准自动选择有效样本，减少无效的冗余样本，从而有效缓解小样本问题中缺少训练样本的问题。
- en: '![Refer to caption](img/a73a2b22983b87aabce95ac19960487b.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a73a2b22983b87aabce95ac19960487b.png)'
- en: 'Figure 6: Architecture of active learning.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：主动学习的架构。
- en: 3.3 Deep Few-shot Learning for HSI classification
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 高光谱图像分类的深度少样本学习
- en: 'Few-shot learning is among meta-learning approaches and aims to study the difference
    between the samples instead of directly learning what the sample is, different
    from most other deep learning methods. It makes the model learn to learn. In few-shot
    classification, given a small support set with N labeled samples $S_{N}^{k}=\{(\bm{x}_{1},y_{1}),\cdots,(\bm{x}_{N},y_{N})\}$,
    which have $k$ categories, the classifier will mask the query sample with the
    label of the largest similarity sample among $S_{N}^{k}$. To achieve this target,
    many learning frameworks have been proposed and they can be divided into two categories:
    meta-based model and metric-based model.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 少样本学习属于元学习方法之一，旨在研究样本之间的差异，而不是直接学习样本是什么，这与大多数其他深度学习方法不同。它使模型学会学习。在少样本分类中，给定一个包含N个标记样本$S_{N}^{k}=\{(\bm{x}_{1},y_{1}),\cdots,(\bm{x}_{N},y_{N})\}$的小支持集，这些样本具有$k$个类别，分类器将用与$S_{N}^{k}$中最大相似度样本的标签来掩盖查询样本。为了实现这一目标，提出了许多学习框架，它们可以分为两类：基于元模型的模型和基于度量的模型。
- en: The prototype network [[102](#bib.bib102)] is one of the metric-based models
    of few-shot learning. Its basic idea is that every class can be depicted by a
    prototype representation, and the samples that belong to the same category should
    be around the class prototype. First, all samples will be transformed into a metric
    space through an embedding function $f_{\phi}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M}$
    and represented by the embedding vector $\mathbf{c}_{k}\in\mathbb{R}^{M}$. Due
    to the powerful ability of the convolutional network, it is used as the embedding
    function. Moreover, the prototype vector is usually the mean of the embedding
    vector of the samples in the support set for each class $c_{i}$.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 原型网络[[102](#bib.bib102)]是少样本学习的度量模型之一。其基本思想是每个类别可以通过一个原型表示来描绘，属于同一类别的样本应该接近于类别原型。首先，所有样本将通过嵌入函数
    $f_{\phi}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M}$ 转换到度量空间，并通过嵌入向量 $\mathbf{c}_{k}\in\mathbb{R}^{M}$
    表示。由于卷积网络的强大能力，它被用作嵌入函数。此外，原型向量通常是每个类别 $c_{i}$ 在支持集中的样本嵌入向量的均值。
- en: '|  | $\bm{c}_{i}=\frac{1}{&#124;S^{i}&#124;}\sum_{(\bm{x}_{j},y_{j})\in S^{i}}f_{\phi}(\bm{x}_{j})$
    |  | (11) |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{c}_{i}=\frac{1}{\vert S^{i} \vert}\sum_{(\bm{x}_{j},y_{j})\in S^{i}}f_{\phi}(\bm{x}_{j})$
    |  | (11) |'
- en: In [[103](#bib.bib103)], Liu *et al.* simply introduce the prototype network
    into hyperspectral image classification task and use ResNet [[59](#bib.bib59)]
    to serve as a feature extractor that maps the samples into a metric space. Then,
    the prototype network is significantly improved for the hyperspectral image classification
    task by [[104](#bib.bib104)]. In the paper, the spatial-spectral feature is first
    integrated by the local pattern coding, and the 1D-CNN converts it to an embedding
    vector. The prototype is the weighted mean of these embedding vectors, which is
    contrary to the general prototype network. In [[105](#bib.bib105)] Xi *et al.*
    replace the mapping function with hybrid residual attention [[106](#bib.bib106)]
    and introduce a new loss function to force the network to increase the interclass
    distance and decrease the intraclass distance.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[103](#bib.bib103)]中，Liu *等*人简单地将原型网络引入到高光谱图像分类任务中，并使用ResNet[[59](#bib.bib59)]作为特征提取器，将样本映射到度量空间。随后，通过[[104](#bib.bib104)]，原型网络在高光谱图像分类任务中得到了显著改进。论文中，空间光谱特征首先通过局部模式编码进行集成，然后1D-CNN将其转换为嵌入向量。原型是这些嵌入向量的加权均值，这与一般的原型网络相反。在[[105](#bib.bib105)]中，Xi
    *等*人用混合残差注意力[[106](#bib.bib106)]替代了映射函数，并引入了一种新的损失函数，迫使网络增加类间距离，减少类内距离。
- en: '![Refer to caption](img/9e18f27d7395fa13eebb8777b6bd8a69.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9e18f27d7395fa13eebb8777b6bd8a69.png)'
- en: 'Figure 7: Architecture of a prototype network.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：原型网络的结构。
- en: 'The relation network [[107](#bib.bib107)] is another metric-based model of
    few-shot learning. In general, it has two modules: the embedding function $f_{\phi}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M}$
    and relation function $f_{\psi}:\mathbb{R}^{2M}\rightarrow\mathbb{R}$. The function
    of the embedding module is the same as the prototype network, and its key idea
    is the relation module. The relation module is to calculate the similarity of
    samples. It is a learnable module that is different from the Euclidean distance
    or cosine distance. In other words, the relation network introduces a learnable
    metric function based on the prototype network. The relation module can more precisely
    describe the difference of samples by the study. During inference, the query embedding
    $f_{\psi}(x_{i})$ will be combined with the support embedding $f_{\psi}(\bm{x}_{j})$
    as $\mathcal{C}(f_{\psi}(\bm{x}_{i}),f_{\psi}(\bm{x}_{j}))$. Usually, $\mathcal{C}(\cdot,\cdot)$
    is a concatenation operation. Then, the relation function will transform the splicing
    vector to a relation score $r_{i,j}$, which indicates the similarity between $x_{i}$
    and $x_{j}$.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 关系网络[[107](#bib.bib107)]是另一种基于度量的少样本学习模型。一般来说，它有两个模块：嵌入函数 $f_{\phi}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M}$
    和关系函数 $f_{\psi}:\mathbb{R}^{2M}\rightarrow\mathbb{R}$。嵌入模块的功能与原型网络相同，其关键思想在于关系模块。关系模块用于计算样本的相似性。它是一个可学习的模块，不同于欧氏距离或余弦距离。换句话说，关系网络引入了一种基于原型网络的可学习度量函数。关系模块可以通过研究更精确地描述样本的差异。在推理过程中，查询嵌入
    $f_{\psi}(x_{i})$ 将与支持嵌入 $f_{\psi}(\bm{x}_{j})$ 结合成 $\mathcal{C}(f_{\psi}(\bm{x}_{i}),f_{\psi}(\bm{x}_{j}))$。通常，$\mathcal{C}(\cdot,\cdot)$
    是一个拼接操作。然后，关系函数将拼接向量转换为关系分数 $r_{i,j}$，该分数表示 $x_{i}$ 和 $x_{j}$ 之间的相似性。
- en: '|  | $r_{i,j}=f_{\psi}(\mathcal{C}(f_{\psi}(\bm{x}_{i}),f_{\psi}(\bm{x}_{j})))$
    |  | (12) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{i,j}=f_{\psi}(\mathcal{C}(f_{\psi}(\bm{x}_{i}),f_{\psi}(\bm{x}_{j})))$
    |  | (12) |'
- en: Several works have introduced the relation network into hyperspectral image
    classification to solve the small sample set problem. Deng *et al.* [[108](#bib.bib108)]
    first introduced the relation network into HSI. They use a 2-dimensional convolutional
    neural network to construct both the embedding function and relation function.
    Gao *et al.* [[109](#bib.bib109)] and Ma *et al.* [[110](#bib.bib110)] have also
    proposed a similar architecture. In [[111](#bib.bib111)], to extract the joint
    spatial-spectral feature, Rao *et al.* implemented the embedding function with
    a 3-dimensional convolutional neural network.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究将关系网络引入高光谱图像分类中，以解决小样本集问题。邓*等人*[[108](#bib.bib108)]首次将关系网络引入HSI。他们使用二维卷积神经网络构建嵌入函数和关系函数。高*等人*[[109](#bib.bib109)]和马*等人*[[110](#bib.bib110)]也提出了类似的架构。在[[111](#bib.bib111)]中，为了提取联合空间-光谱特征，拉奥*等人*使用三维卷积神经网络实现了嵌入函数。
- en: '![Refer to caption](img/f340f2e315b44336e4330bfd0a4ec3cb.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/f340f2e315b44336e4330bfd0a4ec3cb.png)'
- en: 'Figure 8: Architecture of relation network.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图8: 关系网络的架构。'
- en: The Siamese network [[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114)]
    is a typical network in few-shot learning. Compared with the above network, its
    input is a sample pair. Thus, it is composed by two parallel subnetworks $f_{\phi
    1}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M}$ with the same structure and sharing
    parameters. The subnetworks respectively accept an input sample and map it to
    a low-dimensional metric space to generate their own embedding $f_{\phi 1}(\bm{x}_{i})$
    and $f_{\phi 1}(\bm{x}_{j})$. The Euclidean distances $D(\bm{x}_{i},\bm{x}_{j})$
    is used to measure their similarity.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 孪生网络[[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114)]是少样本学习中的典型网络。与上述网络相比，它的输入是样本对。因此，它由两个结构相同并共享参数的并行子网络
    $f_{\phi 1}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M}$ 组成。子网络分别接收一个输入样本，并将其映射到低维度度量空间，生成各自的嵌入
    $f_{\phi 1}(\bm{x}_{i})$ 和 $f_{\phi 1}(\bm{x}_{j})$。欧氏距离 $D(\bm{x}_{i},\bm{x}_{j})$
    用于度量它们的相似性。
- en: '|  | $D(\bm{x}_{i},\bm{x}_{j})=\&#124;f_{\phi 1}(\bm{x}_{i})-f_{\phi 1}(\bm{x}_{j})\&#124;_{2}$
    |  | (13) |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | $D(\bm{x}_{i},\bm{x}_{j})=\&#124;f_{\phi 1}(\bm{x}_{i})-f_{\phi 1}(\bm{x}_{j})\&#124;_{2}$
    |  | (13) |'
- en: The higher the similarity between the two samples is, the more likely they are
    to belong to the same class. Recently, the Siamese network was introduced into
    HSI classification. Usually, a 2-dimensional convolutional neural network [[115](#bib.bib115),
    [116](#bib.bib116)] is used to serve as the embedding function, as in the above
    two networks. In the same way, several methods combined the 1-dimensional convolution
    neural network with the 2-dimensional one [[117](#bib.bib117), [118](#bib.bib118)]
    or use a 3-dimensional network [[119](#bib.bib119)] for the joint spectral-spatial
    feature. Moreover, Miao *et al.* [[120](#bib.bib120)] have tried to use the stack
    autoencoder to construct the embedding function $f_{\phi 1}$. After training,
    the model has the ability to identify the difference between samples. To obtain
    the final classification result, we still need a classifier to classify the embedding
    feature of the sample, which is different from the prototype network and the relation
    network. To avoid overfitting under limited labeled samples, an SVM is usually
    used as a classifier since it is famous for its lightweight.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 两个样本之间的相似性越高，它们更可能属于同一类。最近，孪生网络被引入HSI分类中。通常，使用二维卷积神经网络[[115](#bib.bib115), [116](#bib.bib116)]作为嵌入函数，与上述两个网络类似。同样，一些方法将一维卷积神经网络与二维卷积神经网络[[117](#bib.bib117),
    [118](#bib.bib118)]结合使用，或使用三维网络[[119](#bib.bib119)]来获取联合光谱-空间特征。此外，苗*等人*[[120](#bib.bib120)]尝试使用堆叠自编码器构建嵌入函数
    $f_{\phi 1}$。经过训练，该模型具有识别样本差异的能力。为了获得最终分类结果，我们仍然需要一个分类器来对样本的嵌入特征进行分类，这与原型网络和关系网络不同。为了避免在有限标记样本下的过拟合，通常使用SVM作为分类器，因为它以轻量级著称。
- en: '![Refer to caption](img/ac7c46311408b42df553f4564d0b552d.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/ac7c46311408b42df553f4564d0b552d.png)'
- en: 'Figure 9: Architecture of the Siamese network.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '图9: 孪生网络的架构。'
- en: 4 Experiments
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 实验
- en: In most papers, comprehensive experiments and analysis are introduced to describe
    the advantages and disadvantages of the methods in the paper. However, the problem
    is that different papers may choose different experimental settings. For example,
    the same number of samples for training or test is used in the experiments, and
    the chosen samples are normally different since they are chosen randomly. To evaluate
    different methods fairly, we should use the exact same experimental setting. That
    is the reason why we design experiments to evaluate different methods.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数论文中，通过综合实验和分析来描述论文中方法的优缺点。然而，问题在于不同的论文可能选择不同的实验设置。例如，实验中使用了相同数量的训练或测试样本，但选择的样本通常不同，因为它们是随机选择的。为了公平地评估不同的方法，我们应该使用完全相同的实验设置。这就是为什么我们设计实验来评估不同方法的原因。
- en: As described above, the main methods of small-sample learning currently include
    the autoencoder, few-shot learning, transfer learning, active learning, and data
    augmentation. Therefore, some representative networks of the following methods–S-DMM [[121](#bib.bib121)],
    SSDL [[37](#bib.bib37)], 3DCAE [[40](#bib.bib40)], TwoCnn [[122](#bib.bib122)],
    SSLstm [[75](#bib.bib75)] and 3DVSCNN [[123](#bib.bib123)], which contain convolutional
    network models and recurrent network models, are selected to conduct experiments
    on three benchmark data sets–PaviaU, Salinas and KSC. All models are based on
    deep learning. Here, we only focus on the robustness of the model on a small-sample
    data set, so they classify hyperspectral images based on joint spectral-spatial
    features.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所述，目前的小样本学习主要包括自编码器、少样本学习、迁移学习、主动学习和数据增强。因此，选择了一些代表性网络，如S-DMM [[121](#bib.bib121)]、SSDL [[37](#bib.bib37)]、3DCAE [[40](#bib.bib40)]、TwoCnn [[122](#bib.bib122)]、SSLstm [[75](#bib.bib75)]和3DVSCNN [[123](#bib.bib123)]，这些网络包含卷积网络模型和递归网络模型，并在三个基准数据集——PaviaU、Salinas和KSC上进行实验。所有模型都基于深度学习。这里我们仅关注模型在小样本数据集上的鲁棒性，因此它们基于联合光谱-空间特征对高光谱图像进行分类。
- en: According to the sample size per category in the training data set, the experiment
    is divided into three groups. The first has 10 samples for each category, the
    second has 50 samples for each category and the third has 100 samples for each
    category. At the same time, to ensure the stability of the model, each group of
    experiments is performed ten times, and the training data set is different each
    time. Finally, models are evaluated by average accuracy (AA) and overall accuracy
    (OA).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 根据训练数据集中每个类别的样本数量，实验分为三组。第一组每个类别有10个样本，第二组每个类别有50个样本，第三组每个类别有100个样本。同时，为了确保模型的稳定性，每组实验进行十次，每次训练数据集不同。最后，通过平均准确率（AA）和总体准确率（OA）来评估模型。
- en: 4.1 Introduction of data sets
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 数据集介绍
- en: '1.'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Pavia University (PaviaU): The Pavia University data set consists of hyperspectral
    images, each with 610*340 pixels and a spatial resolution of 1.3 meters, which
    was taken by the ROSIS sensor above Pavia University in Italy. The spectral imagery
    continuously images 115 wavelengths in the range of 0.43$\sim$0.86 um. Since 12
    of the wavelengths are polluted by noise, each pixel in the final data set contains
    103 bands. It contains 42,776 labeled samples in total, covering 9 objects. In
    addition, its sample size of each object is shown in Table [1](#S4.T1 "Table 1
    ‣ 4.1 Introduction of data sets ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples").'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '帕维亚大学（PaviaU）：帕维亚大学数据集由高光谱图像组成，每张图像有610*340像素，空间分辨率为1.3米，由意大利帕维亚大学上方的ROSIS传感器拍摄。光谱图像连续成像115个波长，范围为0.43$\sim$0.86
    um。由于12个波长被噪声污染，最终数据集中的每个像素包含103个波段。它总共包含42,776个标记样本，覆盖9个对象。此外，每个对象的样本大小见表[1](#S4.T1
    "Table 1 ‣ 4.1 Introduction of data sets ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples")。'
- en: '2.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Salinas: The Salinas data set consists of hyperspectral images with 512*217
    pixels and a spatial resolution of 3.7 meters, taken over the Salinas Valley in
    California by the AVIRIS sensor. The spectral imagery continuously images 224
    wavelengths in the range of 0.2$\sim$2.4 um. Since 20 of the bands cannot be reflected
    by water, each pixel in the final data set contains 204 bands. It contains 54,129
    labeled samples in total, covering 16 objects. In addition, its sample size of
    each object is shown in Table [2](#S4.T2 "Table 2 ‣ 4.1 Introduction of data sets
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples").'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 萨利纳斯：萨利纳斯数据集由 AVIRIS 传感器拍摄，包含 512*217 像素和 3.7 米的空间分辨率的高光谱图像，拍摄于加利福尼亚州的萨利纳斯谷。光谱图像连续成像
    0.2$\sim$2.4 微米范围内的 224 个波长。由于 20 个波段不能被水反射，最终数据集中每个像素包含 204 个波段。总共有 54,129 个标记样本，涵盖了
    16 个对象。此外，每个对象的样本大小见表 [2](#S4.T2 "表 2 ‣ 4.1 数据集介绍 ‣ 4 实验 ‣ 调查：深度学习用于少量标记样本的高光谱图像分类")。
- en: '3.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Kennedy Space Center (KSC): The KSC data set was taken at the Kennedy Space
    Center (KSC), above Florida, and used the AVIRIS sensor. Its hyperspectral images
    contain 512*641 pixels, and the spatial resolution is 18 meters. The spectral
    imagery continuously images 224 wavelengths in the range of 400$\sim$2500 nm.
    Similarly, after removing 48 bands that are absorbed by water and have a low signal-to-noise
    ratio, each pixel in the final data set contains 176 bands. It contains 5211 label
    samples, covering 13 objects. Moreover, its sample size of each object is shown
    in Table [3](#S4.T3 "Table 3 ‣ 4.1 Introduction of data sets ‣ 4 Experiments ‣
    A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples").'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 肯尼迪航天中心 (KSC)：KSC 数据集采集于佛罗里达州上空的肯尼迪航天中心 (KSC)，使用了 AVIRIS 传感器。其高光谱图像包含 512*641
    像素，空间分辨率为 18 米。光谱图像连续成像 400$\sim$2500 nm 范围内的 224 个波长。同样，在去除 48 个被水吸收且信噪比低的波段后，最终数据集中的每个像素包含
    176 个波段。数据集中包含 5211 个标签样本，涵盖了 13 个对象。此外，每个对象的样本大小见表 [3](#S4.T3 "表 3 ‣ 4.1 数据集介绍
    ‣ 4 实验 ‣ 调查：深度学习用于少量标记样本的高光谱图像分类")。
- en: 'Table 1: Pavia University. It contains 9 objects. The second column and last
    column represent the name of objects and sample number, respectively.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：帕维亚大学。它包含 9 个对象。第二列和最后一列分别表示对象的名称和样本数量。
- en: '| NO. | Class | Total |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| NO. | 类别 | 总数 |'
- en: '| --- | --- | --- |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| C1 | Asphalt | 6631 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| C1 | 沥青 | 6631 |'
- en: '| C2 | Meadows | 18649 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| C2 | 草地 | 18649 |'
- en: '| C3 | Gravel | 2099 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 碎石 | 2099 |'
- en: '| C4 | Trees | 3064 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| C4 | 树木 | 3064 |'
- en: '| C5 | Painted metal sheets | 1345 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| C5 | 涂漆金属板 | 1345 |'
- en: '| C6 | Bare Soil | 5029 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| C6 | 裸土 | 5029 |'
- en: '| C7 | Bitumen | 1330 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| C7 | 沥青 | 1330 |'
- en: '| C8 | Self-Blocking Bricks | 3682 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| C8 | 自封砖 | 3682 |'
- en: '| C9 | Shadows | 947 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| C9 | 阴影 | 947 |'
- en: 'Table 2: Salinas. It contains 16 objects. The second column and last column
    represent the name of objects and sample number, respectively.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：萨利纳斯。它包含 16 个对象。第二列和最后一列分别表示对象的名称和样本数量。
- en: '| NO. | Class | Total |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| NO. | 类别 | 总数 |'
- en: '| --- | --- | --- |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| C1 | Broccoli green weeds 1 | 2009 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| C1 | 青花菜绿色杂草 1 | 2009 |'
- en: '| C2 | Broccoli green weeds 22 | 3726 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| C2 | 青花菜绿色杂草 22 | 3726 |'
- en: '| C3 | Fallow | 1976 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 休耕地 | 1976 |'
- en: '| C4 | Fallow rough plow | 1394 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| C4 | 休耕粗耕 | 1394 |'
- en: '| C5 | Fallow smooth | 2678 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| C5 | 休耕平整 | 2678 |'
- en: '| C6 | Stubble | 3959 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| C6 | 作物秸秆 | 3959 |'
- en: '| C7 | Celery | 3579 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| C7 | 芹菜 | 3579 |'
- en: '| C8 | Grapes untrained | 11271 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| C8 | 未经训练的葡萄 | 11271 |'
- en: '| C9 | Soil vineyard develop | 6203 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| C9 | 土壤葡萄园开发 | 6203 |'
- en: '| C10 | Corn senesced green weeds | 3278 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| C10 | 玉米衰老绿色杂草 | 3278 |'
- en: '| C11 | Lettuce romaine 4wk | 1068 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| C11 | 罗马生菜 4 周 | 1068 |'
- en: '| C12 | Lettuce romaine 5wk | 1927 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| C12 | 罗马生菜 5 周 | 1927 |'
- en: '| C13 | Lettuce romaine 6wk | 916 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| C13 | 罗马生菜 6 周 | 916 |'
- en: '| C14 | Lettuce romaine 7wk | 1070 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| C14 | 罗马生菜 7 周 | 1070 |'
- en: '| C15 | Vineyard untrained | 7268 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| C15 | 葡萄园未经训练 | 7268 |'
- en: '| C16 | Vineyard vertical trellis | 1807 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| C16 | 葡萄园竖立架 | 1807 |'
- en: 'Table 3: KSC. It contains 13 objects. The second column and last column represent
    the name of objects and sample number, respectively.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：KSC。它包含 13 个对象。第二列和最后一列分别表示对象的名称和样本数量。
- en: '| NO. | Class | Total |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| NO. | 类别 | 总数 |'
- en: '| --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| C1 | Scrub | 761 |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| C1 | 灌木丛 | 761 |'
- en: '| C2 | Willow swamp | 243 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| C2 | 柳树沼泽 | 243 |'
- en: '| C3 | Cabbage palm hammock | 256 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| C3 | 棕榈草地 | 256 |'
- en: '| C4 | Cabbage palm/oak hammock | 252 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| C4 | 棕榈草地/橡树草地 | 252 |'
- en: '| C5 | Slash pine | 161 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| C5 | 刺叶松 | 161 |'
- en: '| C6 | Oak/broadleaf hammock | 229 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| C6 | 橡树/阔叶草地 | 229 |'
- en: '| C7 | Hardwood swamp | 105 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| C7 | 硬木沼泽 | 105 |'
- en: '| C8 | Graminoid marsh | 431 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| C8 | 草本沼泽 | 431 |'
- en: '| C9 | Spartina marsh | 520 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| C9 | Spartina 沼泽 | 520 |'
- en: '| C10 | Cattail marsh | 404 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| C10 | 芦苇荡 | 404 |'
- en: '| C11 | Salt marsh | 419 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| C11 | 盐沼 | 419 |'
- en: '| C12 | Mud flats | 503 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| C12 | 泥滩 | 503 |'
- en: '| C13 | Water | 927 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| C13 | 水域 | 927 |'
- en: 4.2 Selected models
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 选择的模型
- en: 'Some state-of-the-art methods are choose to evaluate their performance. They
    were trained using different platforms, including Caffe, PyTorch, etc. Some platforms
    such Caffe are not well supported by the new development environments. Most models
    are our re-implementations and are trained using the exact same setting. Most
    of the above model settings are based on the original paper, and some are modified
    slightly based on the experiment. All models are trained and tested on the same
    training data set that is picked randomly based on pixels and the test data set,
    and their settings have been optimally tuned. The implementation situation of
    the code is shown in Table [4](#S4.T4 "Table 4 ‣ 4.2 Selected models ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples"). The descriptions of the chosen models are provided in the following
    part.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '选择了一些最先进的方法来评估它们的性能。它们使用不同的平台进行训练，包括 Caffe、PyTorch 等。一些平台，如 Caffe，在新开发环境中的支持不好。大多数模型是我们的重新实现，并使用完全相同的设置进行训练。上述大多数模型设置基于原始论文，有些根据实验略微修改。所有模型都在同一训练数据集上进行训练和测试，该数据集是基于像素随机挑选的，测试数据集也如此，且其设置已被优化。代码的实现情况见表
    [4](#S4.T4 "Table 4 ‣ 4.2 Selected models ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples")。所选模型的描述将在以下部分提供。'
- en: 'Table 4: Originators of model implementations. F denotes that the code of the
    model comes from the original paper. T denotes our implemented model.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：模型实现的来源。F 表示模型的代码来自原始论文。T 表示我们的实现模型。
- en: '| S-DMM | SSDL | 3DCAE | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI | SAE_LR |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| S-DMM | SSDL | 3DCAE | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI | SAE_LR |'
- en: '| F | T | F | T | T | T | T | T |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| F | T | F | T | T | T | T | T |'
- en: '1.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: SAE_LR [[34](#bib.bib34)]. This is the first paper to introduce the autoencoder
    into hyperspectral image classification, opening a new era of hyperspectral image
    processing. It adopts a raw autoencoder composed of linear layers to extract the
    feature. The size of the neighbor region is $5\times 5$, and the first 4 components
    of PCA are chosen. Subsequently, we can gain a spatial feature vector. Before
    inputting into the model, the raw spatial feature and the spatial feature are
    stacked to form a joint feature. To reduce the difficulty of training, it uses
    a greedy layerwise pretraining method to train each layer, and the parameters
    of the encoder and decoder are symmetric. Then, the encoder concatenates a linear
    classifier for fine tuning. According to [[34](#bib.bib34)], the hidden size is
    set to 60, 20, and 20 for PaviaU, Salinas, and KSC, respectively.
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SAE_LR [[34](#bib.bib34)]。这是第一篇将自编码器引入高光谱图像分类的论文，开启了高光谱图像处理的新纪元。它采用由线性层组成的原始自编码器来提取特征。邻域区域的大小为$5\times
    5$，选择了PCA的前4个组件。随后，我们可以获得一个空间特征向量。在输入模型之前，将原始空间特征和空间特征堆叠以形成联合特征。为了降低训练难度，采用贪婪的逐层预训练方法来训练每一层，且编码器和解码器的参数是对称的。然后，编码器连接一个线性分类器进行微调。根据 [[34](#bib.bib34)]，隐藏层的大小分别为PaviaU、Salinas和KSC的60、20和20。
- en: '2.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: S-DMM [[121](#bib.bib121)]. This is a relation network that contains an embedding
    module and relation module implemented by 2D convolutional networks. The model
    aims to make samples in the feature space have a small intraclass distance and
    a large interclass distance through a learnable feature embedding function and
    a metric function. After training, all samples will be assigned to the corresponding
    clusters. Finally, a simple KNN is used to classify the query sample. In the experiment,
    the neighbor region of the pixel is fixed as $5\times 5$ and the feature dimension
    is set to 64.
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: S-DMM [[121](#bib.bib121)]。这是一个包含嵌入模块和关系模块的关系网络，使用2D卷积网络实现。该模型旨在通过可学习的特征嵌入函数和度量函数，使特征空间中的样本具有小的类内距离和大的类间距离。训练后，所有样本将被分配到相应的簇中。最后，使用简单的KNN对查询样本进行分类。在实验中，像素的邻域区域固定为$5\times
    5$，特征维度设置为64。
- en: '3.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 3DCAE [[40](#bib.bib40)]. This is a 3D convolutional autoencoder adopting a
    3D convolution layer to extract the joint spectral-spatial feature. First, 3DCAE
    is trained by the traditional method, and then, an SVM classifier is adopted to
    classify the hidden features on the top of 3DCAE. In the experiment, the neighbor
    region of the pixel is set to $5\times 5$ and 90% of the samples are used to train
    the 3D autoencoder. There are two different hyperparameter settings corresponding
    to Salinas and PaviaU, and the model has not been tested on KSC in [[121](#bib.bib121)].
    Therefore, on the KSC, the model uses the same hyperparameter configuration as
    on the Salinas because they are collected by the same sensor.
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3DCAE [[40](#bib.bib40)]。这是一种 3D 卷积自编码器，采用 3D 卷积层来提取联合光谱-空间特征。首先，3DCAE 通过传统方法进行训练，然后采用
    SVM 分类器对 3DCAE 顶层的隐藏特征进行分类。在实验中，像素的邻域区域设置为 $5\times 5$，90% 的样本用于训练 3D 自编码器。对 Salinas
    和 PaviaU 有两种不同的超参数设置，而模型在 [[121](#bib.bib121)] 中没有在 KSC 上测试。因此，在 KSC 上，该模型使用与
    Salinas 相同的超参数配置，因为它们是由同一传感器收集的。
- en: '4.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: SSDL [[37](#bib.bib37)]. This is a typical two-stream structure extracting the
    spectral and spatial feature separately through two different branches and merging
    them at the end. Inspired by [[34](#bib.bib34)], the author adopts a 1D autoencoder
    to extract the spectral feature. In the branch of spatial feature extraction,
    the model uses a spatial pyramid pooling layer to replace the traditional pooling
    layer on the top convolutional layer. The spatial pyramid pooling layer enables
    the deep convolutional neural network to generate a fixed-length feature. On the
    one hand, it enables the model to convert the input of different sizes into a
    fixed-length, which is good for the module that is sensitive to the input size;
    on the other hand, it is useful for the model to better adapt to objects of different
    scales, and the output will include features from coarse to fine, achieving multiscale
    feature fusion. Then, a simple logistic classifier is used to classify the spectra-spatial
    feature. In the experiment, 80% of the data are used to train the autoencoder
    through the method of greedy layer-wise pretraining. Moreover, in the spatial
    branch, the size of the neighbor region is set to 42*42 and PCA is used to extract
    the first component. Then, the overall model is trained together.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SSDL [[37](#bib.bib37)]。这是一种典型的双流结构，通过两个不同的分支分别提取光谱和空间特征，并在最后进行融合。受到 [[34](#bib.bib34)]
    的启发，作者采用了一维自编码器来提取光谱特征。在空间特征提取的分支中，该模型使用了空间金字塔池化层来替代传统的池化层。空间金字塔池化层使得深度卷积神经网络能够生成固定长度的特征。一方面，它使得模型能够将不同大小的输入转换为固定长度，这对于对输入大小敏感的模块很有帮助；另一方面，它有助于模型更好地适应不同尺度的对象，输出将包括从粗到细的特征，实现多尺度特征融合。然后，使用一个简单的逻辑回归分类器对光谱-空间特征进行分类。在实验中，80%
    的数据用于通过贪婪分层预训练方法训练自编码器。此外，在空间分支中，邻域区域的大小设置为 42*42，并使用 PCA 提取第一主成分。然后，整体模型进行联合训练。
- en: '5.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: TwoCnn [[122](#bib.bib122)]. This is a two-stream structure based on fine-tuning.
    In the spectral branch, it adopts a 1D convolutional layer to capture local information
    of spectral features, which is entirely different from SSDL. In particular, transfer
    learning is used to pretrain parameters of the model and endow it with good robustness
    on limited samples. The pairs of the source data set and target data set are Pavia
    Center–PavaU, Indian pines-Salinas, and Indian pines-KSC. In [[122](#bib.bib122)],
    they also did not test the model on KSC. Thus, we regard Indian pines as the source
    domain for KSC, given that both data sets come from the same type of sensor. The
    neighbor region of the pixel is set to 21*21\. Additionally, it averages along
    the spectral channel to reduce the input dimension, instead of PCA. In the pretraining
    process, 15% of samples of each category of Pavia and 90% of samples of each category
    of Indian pines are treated as the training data set, and the rest serve as the
    test data set. To make the number of bands in the source data set and target data
    set the same, we filter out the band that has the smaller variance. According
    to [[122](#bib.bib122)], all other layers are transferred except for the softmax
    layer. Finally, the model is fine-tuned on the target data set with the same configuration.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TwoCnn [[122](#bib.bib122)]。这是一个基于微调的双流结构。在光谱分支中，它采用1D卷积层来捕获光谱特征的局部信息，这与SSDL完全不同。特别是，使用迁移学习对模型进行预训练，使其在有限样本上具有良好的鲁棒性。源数据集和目标数据集的配对分别是Pavia
    Center–PavaU、Indian pines-Salinas和Indian pines-KSC。在[[122](#bib.bib122)]中，他们也没有在KSC上测试模型。因此，我们将Indian
    pines视为KSC的源领域，因为这两个数据集来自相同类型的传感器。像素的邻域区域设为21*21。此外，它沿光谱通道取平均以减少输入维度，而不是使用PCA。在预训练过程中，将Pavia每类15%的样本和Indian
    pines每类90%的样本作为训练数据集，其余作为测试数据集。为了使源数据集和目标数据集中的波段数量相同，我们过滤掉方差较小的波段。根据[[122](#bib.bib122)]，除了softmax层外，所有其他层都被迁移。最后，模型在目标数据集上使用相同的配置进行微调。
- en: '6.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '6.'
- en: '3DVSCNN [[123](#bib.bib123)]. This is a general CNN-based image classification
    model, but it uses a 3D convolutional network to extract spectral-spatial features
    simultaneously followed by a fully connected network for classification. The main
    idea of [[123](#bib.bib123)] is the usage of active learning. The process can
    be divided into two steps: the selection of valuable samples and the training
    of the model. In [[123](#bib.bib123)], an SVM serves as a selector to iteratively
    select some of the most valuable samples according to Eq.([10](#S3.E10 "In 3.2
    Deep Active Learning for HSI classification ‣ 3 Deep learning paradigms for HSI
    classification with few labeled samples ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples")). Then, the 3DVSCNN is trained
    on the valuable data set. The size of its neighbor region is set to 13*13\. During
    data preprocessing, it uses PCA to extract the top 10 components for PaviaU and
    Salinas, and the top 30 components for KSC, which contain more than 99% of the
    original spectral information and still keep a clear spatial geometry. In the
    experiment, 80% of samples will be picked by the SVM to form a valuable data set
    for 4 samples in each iteration. Then, the model is trained on the valuable data
    set.'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '3DVSCNN [[123](#bib.bib123)]。这是一个基于CNN的通用图像分类模型，但它使用3D卷积网络来同时提取光谱-空间特征，然后通过一个全连接网络进行分类。[[123](#bib.bib123)]的主要思想是使用主动学习。这个过程可以分为两个步骤：选择有价值的样本和训练模型。在[[123](#bib.bib123)]中，SVM作为选择器，根据公式([10](#S3.E10
    "In 3.2 Deep Active Learning for HSI classification ‣ 3 Deep learning paradigms
    for HSI classification with few labeled samples ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples"))迭代地选择一些最有价值的样本。然后，3DVSCNN在这些有价值的数据集上进行训练。其邻域区域的大小设为13*13。在数据预处理过程中，使用PCA提取PaviaU和Salinas的前10个组件，以及KSC的前30个组件，这些组件包含了原始光谱信息的99%以上，并保持了清晰的空间几何。在实验中，80%的样本将由SVM选择，形成每次迭代中4个样本的有价值数据集。然后，在这个有价值的数据集上训练模型。'
- en: '7.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '7.'
- en: CNN_HSI [[28](#bib.bib28)]. The model combines multilayer $1\times 1$ 2D convolutions
    followed by local response normalization to capture the feature of hyperspectral
    images. To avoid the loss of information after PCA, it uses 2D convolution to
    extract spectral and spatial joint features directly, instead of 3D convolution.
    At the same time, it also adopts a dropout layer and data augmentation, including
    rotation and flipping, to improve the generalization of the model and reduce overfitting.
    After data augmentation, an image can generate eight different orientation images.
    Moreover, the model removes the linear classifier to decrease the number of trainable
    parameters. According to [[28](#bib.bib28)], the dropout rate is set to 0.6, the
    size of the neighbor region is $5\times 5$, and the batch size is 16 in the experiment.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CNN_HSI [[28](#bib.bib28)]。该模型结合了多层 $1\times 1$ 的二维卷积，随后进行局部响应归一化，以捕捉高光谱图像的特征。为了避免PCA后的信息损失，它使用二维卷积直接提取光谱和空间联合特征，而不是三维卷积。同时，它还采用了丢弃层和数据增强，包括旋转和翻转，以提高模型的泛化能力并减少过拟合。经过数据增强后，一张图像可以生成八种不同方向的图像。此外，该模型移除了线性分类器，以减少可训练参数的数量。根据 [[28](#bib.bib28)]，在实验中丢弃率设置为0.6，邻域区域的大小为
    $5\times 5$，批量大小为16。
- en: '8.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '8.'
- en: SSLstm [[75](#bib.bib75)]. Unlike the above methods, SSLstm adopts recurrent
    networks to process spectral and spatial features simultaneously. In the spectral
    branch, called SeLstm, the spectral vector is seen as a sequence. In the spatial
    branch, called SaLstm, it treats each line of the image patch as a sequence element.
    Therefore, along the column direction, the image patch can be well converted into
    a sequence. In particular, it fuses the predictions of the two branches in the
    label space to obtain the final prediction result, which is defined as
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SSLstm [[75](#bib.bib75)]。与上述方法不同，SSLstm 采用递归网络同时处理光谱和空间特征。在光谱分支中，称为SeLstm，光谱向量被视为一个序列。在空间分支中，称为SaLstm，它将图像补丁的每一行视为一个序列元素。因此，沿着列方向，图像补丁可以很好地转换为一个序列。特别是，它在标签空间中融合两个分支的预测，以获得最终的预测结果，定义为
- en: '|  | $\begin{split}P(y=j&#124;x_{i})=w_{spe}P_{spe}(y=j&#124;x_{i})+w_{spa}P_{spa}(y=j&#124;x_{i})\end{split}$
    |  | (14) |'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $\begin{split}P(y=j&#124;x_{i})=w_{spe}P_{spe}(y=j&#124;x_{i})+w_{spa}P_{spa}(y=j&#124;x_{i})\end{split}$
    |  | (14) |'
- en: where $P(y=j|x_{i})$ denotes the final posterior probability, $P_{spe}(y=j|x_{i})$
    and $P_{spa}(y=j|x_{i})$ denote the posterior probabilities from spectral and
    spatial modules, respectively, and $w_{spe}$ and $w_{spa}$ are fusion weights
    that satisfy the sum of 1\. In the experiment, the size of the neighbor region
    is set to 32*32 for PaviaU and Salinas. In addition, for KSC, it is set to 64*64\.
    Next, the first component of PCA is reserved on all data sets. The number of hidden
    nodes of the spectral branch and the spatial branch are 128 and 256, respectively.
    In addition, $w_{spe}$ and $w_{spa}$ are set to 0.5 and 0.5 separately.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $P(y=j|x_{i})$ 表示最终的后验概率，$P_{spe}(y=j|x_{i})$ 和 $P_{spa}(y=j|x_{i})$ 分别表示来自光谱模块和空间模块的后验概率，而
    $w_{spe}$ 和 $w_{spa}$ 是满足总和为1的融合权重。在实验中，邻域区域的大小对于PaviaU和Salinas设置为32*32。而对于KSC，设置为64*64。接下来，所有数据集中保留PCA的第一个成分。光谱分支和空间分支的隐藏节点数量分别为128和256。此外，$w_{spe}$
    和 $w_{spa}$ 分别设置为0.5和0.5。
- en: 4.3 Experimental results and analysis
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 实验结果与分析
- en: 'The accuracy of the test data set is shown in Table [5](#S4.T5 "Table 5 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples"), Table [6](#S4.T6
    "Table 6 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep
    Learning for Hyperspectral Image Classification with Few Labeled Samples"), and
    Table [7](#S4.T7 "Table 7 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples"). Corresponding classification maps are shown in Figure [11](#S4.F11
    "Figure 11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")$\sim$[19](#S4.F19
    "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples").
    The final classification result of the pixel is decided by the voting result of
    10 experiments.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '测试数据集的准确性如表[5](#S4.T5 "Table 5 ‣ 4.3 Experimental results and analysis ‣ 4
    Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification with
    Few Labeled Samples")、表[6](#S4.T6 "Table 6 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples")和表[7](#S4.T7 "Table 7 ‣ 4.3 Experimental results and
    analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples")中显示。对应的分类图见图[11](#S4.F11 "Figure 11 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples")$\sim$[19](#S4.F19 "Figure 19 ‣
    4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples")。像素的最终分类结果由10次实验的投票结果决定。'
- en: 'Taking Table [5](#S4.T5 "Table 5 ‣ 4.3 Experimental results and analysis ‣
    4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") as an example, the experiment is divided into three
    groups, and the sample sizes in each group are 10, 50, and 100, respectively.
    The aforementioned models are conducted 10 times in every experiment sets. Then,
    we count the average of their class classification accuracy, AA, and OA for comparing
    their performance. When sample size is 10, S-DMM has the highest AA and OA, which
    are 91.08% and 84.45% respectively, in comparison with the AA and OA of 71.58%
    and 60.00%, 75.34 % and 74.79%, 74.60% and 78.61%, 75.64% and 75.17%, 72.77% and
    69.59%, 85.12% and 82.13%, 72.40% and 66.05% for 3DCAE, SSDL, TwoCnn, 3DVSCNN,
    SSLstm, CNN_HSI and SAE_LR. Besides, S-DMM has the largest number of class classification
    accuracy. When the sample size is 50, S-DMM and CNN_HSI have the highest AA and
    OA respectively, which are 96.47% and 95.21%. In the last group, 3DVSCNN and CNN_HSI
    have the highest AA and OA, which are 97.13% and 97.35%. According to the other
    two tables, we can conclude with a similar result.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '以表[5](#S4.T5 "Table 5 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples")为例，实验分为三组，每组的样本量分别为10、50和100。上述模型在每个实验组中进行10次实验。然后，我们计算它们的分类精度、AA和OA的平均值，以比较它们的性能。当样本量为10时，S-DMM的AA和OA最高，分别为91.08%和84.45%，而3DCAE、SSDL、TwoCnn、3DVSCNN、SSLstm、CNN_HSI和SAE_LR的AA和OA分别为71.58%和60.00%、75.34%和74.79%、74.60%和78.61%、75.64%和75.17%、72.77%和69.59%、85.12%和82.13%、72.40%和66.05%。此外，S-DMM的分类精度数量最多。当样本量为50时，S-DMM和CNN_HSI的AA和OA分别最高，为96.47%和95.21%。在最后一组中，3DVSCNN和CNN_HSI的AA和OA分别最高，为97.13%和97.35%。根据其他两个表格，我们可以得出类似的结论。'
- en: 'As shown in Table [5](#S4.T5 "Table 5 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples"), Table [6](#S4.T6 "Table 6 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") and Table [7](#S4.T7 "Table 7 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples"), we can conclude
    that most models’ performance on KSC, except for 3DCAE, is better than the other
    two data sets. Especially when the data set contains few samples, the accuracy
    of S-DMM is up to 94%, superior to other data sets. This is because the surface
    objects on the KSC itself have a discriminating border between each other, regardless
    of its higher spatial resolution than that of the other data sets, as shown in
    Figure [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples")$\sim$[19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples"). In the other data sets, models easily misclassify
    the objects that have a similar spatial structure, as illustrated in Meadows (class
    2) and Bare soil (class 6) in PaviaU and Fallow rough plow (class 4) and Grapes
    untrained (class 8) in Salinas, as shown in [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples")$\sim$[16](#S4.F16 "Figure 16 ‣
    4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples"). The accuracy
    of all models on Grapes untrained is lower than other classes in Salinas. In Figure [10](#S4.F10
    "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples"),
    on all data sets, as the number of samples increases, the accuracy of all models
    will improve together.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '如表[5](#S4.T5 "Table 5 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples")、表[6](#S4.T6 "Table 6 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples")和表[7](#S4.T7 "Table 7 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples")所示，我们可以得出结论，除了3DCAE之外，大多数模型在KSC上的表现优于其他两个数据集。特别是当数据集样本较少时，S-DMM的准确率高达94%，优于其他数据集。这是因为KSC上的表面物体之间具有明显的区分边界，无论其空间分辨率是否高于其他数据集，如图[17](#S4.F17
    "Figure 17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")$\sim$[19](#S4.F19
    "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")所示。在其他数据集中，模型容易将具有相似空间结构的物体误分类，例如在PaviaU中的Meadows（类2）和Bare
    soil（类6），以及在Salinas中的Fallow rough plow（类4）和Grapes untrained（类8），如图[11](#S4.F11
    "Figure 11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")$\sim$[16](#S4.F16
    "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")所示。所有模型在Grapes
    untrained上的准确率低于Salinas中的其他类别。在图[10](#S4.F10 "Figure 10 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples")中，随着样本数量的增加，所有数据集上所有模型的准确率都会一起提高。'
- en: 'As shown in Figure [10](#S4.F10 "Figure 10 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples"), when the sample size of each category is 10, S-DMM
    and CNN_HSI have achieved stable and excellent performance on all data sets. They
    are not sensitive to the size of the data set. In Figure [10](#S4.F10 "Figure
    10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") and Figure [10](#S4.F10
    "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples"),
    with increasing sample size, the accuracy of S-DMM and CNN_HSI have improved slightly,
    but their increase is lower than that of others. In Figure [10](#S4.F10 "Figure
    10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples"), when the sample
    size increases from 50 to 100, we can obtain the same conclusion. This result
    shows that both of them can be applied to solve the small-sample problem in hyperspectral
    images. Especially for S-DMM, it has gained the best performance on the metric
    of AA and OA on Salinas and KSC in the experiment with a sample size of 10\. On
    PaviaU, it still wins the third place. This result also proves that it can work
    well on a few samples. Although TwoCnn, 3DVSCNN, and SSLstm achieve good performance
    on all data sets, when the data set contains fewer samples, they will not work
    well. It is worth mentioning that 3DVSNN uses fewer samples to train than other
    models for selecting valuable samples. This approach may not be beneficial for
    those classes with few samples. As shown in [7](#S4.T7 "Table 7 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples"), 3DVSCNN has a good performance
    on OA, but a bad performance on AA. For class 7, when its sample size increases
    from 10 to 50 and 100, its accuracy drops. This is because the total sample size
    of it is the smallest on KSC. Therefore, it contains few valuable samples. Moreover,
    the step of selecting valuable samples would cause an imbalance between the classes,
    which leads to the accuracy of class 7 decreasing. On almost all data sets, autoencoder-based
    models achieve poor performance compared with other models. Although unsupervised
    learning does not need to label samples, if there are no constraints, the autoencoder
    might actually learn nothing. Moreover, since it has a symmetric architecture,
    it would result in a vast number of parameters and increase the difficulty of
    training. Therefore, SSDL and SAE_LR use a greedy layerwise pretraining method
    to solve this problem. However, 3DCAE does not adopt this approach and achieves
    the worst performance on all data sets. As shown in Figure [10](#S4.F10 "Figure
    10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples"), it still has
    considerable room for improvement.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '如图[10](#S4.F10 "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples")所示，当每个类别的样本量为10时，S-DMM和CNN_HSI在所有数据集上均表现出稳定且优异的性能。它们对数据集的大小不敏感。在图[10](#S4.F10
    "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")和图[10](#S4.F10
    "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")中，随着样本量的增加，S-DMM和CNN_HSI的准确率略有提高，但其增幅低于其他方法。在图[10](#S4.F10
    "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")中，当样本量从50增加到100时，我们可以得出相同的结论。这个结果表明它们都可以用来解决高光谱图像中的小样本问题。特别是对于S-DMM，它在样本量为10的实验中，在Salinas和KSC的AA和OA指标上取得了最佳性能。在PaviaU上，它仍然获得了第三名。这个结果也证明它在少量样本上表现良好。尽管TwoCnn、3DVSCNN和SSLstm在所有数据集上表现良好，但当数据集包含较少样本时，它们表现不佳。值得一提的是，3DVSNN使用比其他模型更少的样本来选择有价值的样本。这种方法可能对样本较少的类别不利。如表[7](#S4.T7
    "Table 7 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep
    Learning for Hyperspectral Image Classification with Few Labeled Samples")所示，3DVSCNN在OA上的表现良好，但在AA上的表现较差。对于类别7，当其样本量从10增加到50和100时，其准确率下降。这是因为在KSC上，它的总样本量最小。因此，它包含的有价值样本较少。此外，选择有价值样本的步骤会导致类别之间的不平衡，从而导致类别7的准确率下降。在几乎所有数据集中，基于自编码器的模型相比其他模型表现较差。虽然无监督学习不需要标记样本，但如果没有约束，自编码器实际上可能什么都学不到。此外，由于其对称结构，它会导致大量参数，增加训练难度。因此，SSDL和SAE_LR使用贪婪的逐层预训练方法来解决这个问题。然而，3DCAE没有采用这种方法，并且在所有数据集上表现最差。如图[10](#S4.F10
    "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")所示，它仍然有相当大的改进空间。'
- en: Overall, classification results based on few-shot learning, active learning,
    transfer learning, and data augmentation are better than autoencoder-based unsupervised
    learning methods on the limited sample in all experiments. Few-shot learning benefits
    from the exploration of the relationship between samples to find a discriminative
    decision boarder. Active learning benefits from the selection of valuable samples,
    which enables the model to focus more attention to indistinguishable samples.
    Transfer learning makes good use of the similarity between different data sets,
    which reduces the quantity of data required for training and trainable parameters,
    improving the model’s robustness. According to raw data, the method of data augmentation
    generates more samples to expand the diversity of samples. Although the autoencoder
    can learn the internal structure of the unlabeled data set, the final feature
    representation might not have task-related characteristics. This is the reason
    why its performance on a small-sample data set is inferior to supervised learning.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: PaviaU. Classification accuracy obtained by S-DMM [[121](#bib.bib121)],
    3DCAE [[40](#bib.bib40)], SSDL [[37](#bib.bib37)], TwoCnn [[122](#bib.bib122)],
    3DVSCNN [[123](#bib.bib123)], SSLstm [[75](#bib.bib75)], CNN_HSI [[28](#bib.bib28)]
    and SAE_LR [[34](#bib.bib34)] on PaviaU. The best accuracies are marked in bold.
    The ”size” in the first line denotes the sample size per category.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '| size | classes | S-DMM | 3DCAE | SSDL | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI
    | SAE_LR |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 94.34 | 49.41 | 68.33 | 71.80 | 63.03 | 72.59 | 84.60 | 66.67 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
- en: '| 2 | 73.13 | 51.60 | 72.94 | 88.27 | 69.22 | 68.86 | 67.57 | 56.68 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
- en: '| 3 | 86.85 | 54.06 | 53.71 | 47.58 | 71.77 | 48.08 | 72.80 | 46.37 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
- en: '| 4 | 95.04 | 94.81 | 88.58 | 96.29 | 85.10 | 79.06 | 93.65 | 80.10 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
- en: '| 5 | 99.98 | 99.86 | 97.21 | 94.99 | 98.61 | 93.80 | 99.84 | 98.81 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
- en: '| 6 | 85.58 | 57.40 | 66.21 | 49.75 | 75.17 | 62.53 | 78.35 | 55.87 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
- en: '| 7 | 98.55 | 80.34 | 68.17 | 58.65 | 65.61 | 65.39 | 92.14 | 81.42 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
- en: '| 8 | 86.47 | 57.97 | 64.07 | 66.95 | 55.77 | 67.60 | 78.17 | 66.83 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.81 | 98.76 | 98.83 | 97.15 | 96.48 | 97.02 | 98.92 | 98.90 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
- en: '| AA | 91.08 | 71.58 | 75.34 | 74.60 | 75.64 | 72.77 | 85.12 | 72.40 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
- en: '| OA | 84.55 | 60.00 | 74.79 | 78.61 | 75.17 | 69.59 | 82.13 | 66.05 |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
- en: '| 50 | 1 | 97.08 | 80.76 | 76.11 | 88.50 | 90.60 | 82.96 | 93.66 | 78.83 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
- en: '| 2 | 90.09 | 63.14 | 87.39 | 86.43 | 93.68 | 82.42 | 94.82 | 65.36 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
- en: '| 3 | 95.15 | 62.57 | 70.28 | 69.21 | 90.64 | 81.59 | 94.87 | 65.50 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
- en: '| 4 | 97.35 | 97.33 | 89.27 | 98.80 | 93.47 | 91.31 | 94.49 | 92.43 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
- en: '| 5 | 100.00 | 100.00 | 98.14 | 99.81 | 99.92 | 99.67 | 100.00 | 99.47 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
- en: '| 6 | 96.32 | 80.15 | 75.12 | 84.93 | 94.15 | 82.58 | 88.14 | 72.30 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: '| 7 | 99.31 | 88.45 | 75.80 | 83.12 | 94.98 | 92.34 | 97.21 | 86.04 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
- en: '| 8 | 92.97 | 75.11 | 70.57 | 83.57 | 91.55 | 84.75 | 87.52 | 79.74 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.98 | 99.69 | 99.61 | 99.91 | 98.72 | 99.39 | 99.78 | 99.29 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 99.98 | 99.69 | 99.61 | 99.91 | 98.72 | 99.39 | 99.78 | 99.29 |'
- en: '| AA | 96.47 | 83.02 | 82.48 | 88.25 | 94.19 | 88.56 | 94.50 | 82.10 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| AA | 96.47 | 83.02 | 82.48 | 88.25 | 94.19 | 88.56 | 94.50 | 82.10 |'
- en: '| OA | 94.04 | 64.17 | 84.92 | 90.69 | 94.23 | 84.50 | 95.21 | 77.42 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| OA | 94.04 | 64.17 | 84.92 | 90.69 | 94.23 | 84.50 | 95.21 | 77.42 |'
- en: '| 100 | 1 | 97.11 | 83.05 | 85.59 | 92.21 | 94.38 | 90.84 | 94.44 | 78.64 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 1 | 97.11 | 83.05 | 85.59 | 92.21 | 94.38 | 90.84 | 94.44 | 78.64 |'
- en: '| 2 | 91.64 | 73.45 | 86.17 | 76.86 | 95.90 | 83.26 | 97.75 | 74.28 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 91.64 | 73.45 | 86.17 | 76.86 | 95.90 | 83.26 | 97.75 | 74.28 |'
- en: '| 3 | 94.23 | 73.02 | 80.29 | 72.24 | 95.96 | 80.66 | 95.37 | 79.87 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 94.23 | 73.02 | 80.29 | 72.24 | 95.96 | 80.66 | 95.37 | 79.87 |'
- en: '| 4 | 98.70 | 97.87 | 97.14 | 99.28 | 97.65 | 92.54 | 95.88 | 93.54 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 98.70 | 97.87 | 97.14 | 99.28 | 97.65 | 92.54 | 95.88 | 93.54 |'
- en: '| 5 | 100.00 | 100.00 | 99.06 | 99.89 | 99.95 | 99.57 | 99.99 | 99.24 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 100.00 | 100.00 | 99.06 | 99.89 | 99.95 | 99.57 | 99.99 | 99.24 |'
- en: '| 6 | 93.51 | 86.82 | 83.16 | 95.90 | 97.92 | 87.61 | 91.01 | 69.83 |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 93.51 | 86.82 | 83.16 | 95.90 | 97.92 | 87.61 | 91.01 | 69.83 |'
- en: '| 7 | 99.21 | 90.17 | 94.08 | 89.88 | 98.39 | 93.45 | 98.37 | 89.42 |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 99.21 | 90.17 | 94.08 | 89.88 | 98.39 | 93.45 | 98.37 | 89.42 |'
- en: '| 8 | 92.73 | 88.31 | 88.43 | 90.03 | 94.21 | 90.08 | 92.41 | 85.05 |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 92.73 | 88.31 | 88.43 | 90.03 | 94.21 | 90.08 | 92.41 | 85.05 |'
- en: '| 9 | 99.99 | 99.82 | 99.65 | 99.98 | 99.85 | 99.80 | 99.70 | 99.55 |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 99.99 | 99.82 | 99.65 | 99.98 | 99.85 | 99.80 | 99.70 | 99.55 |'
- en: '| AA | 96.35 | 88.06 | 90.40 | 90.70 | 97.13 | 90.87 | 96.10 | 85.49 |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| AA | 96.35 | 88.06 | 90.40 | 90.70 | 97.13 | 90.87 | 96.10 | 85.49 |'
- en: '| OA | 94.65 | 70.15 | 89.33 | 94.76 | 97.05 | 87.19 | 97.35 | 81.44 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| OA | 94.65 | 70.15 | 89.33 | 94.76 | 97.05 | 87.19 | 97.35 | 81.44 |'
- en: 'Table 6: Salinas. Classification accuracy obtained by S-DMM [[121](#bib.bib121)],
    3DCAE [[40](#bib.bib40)], SSDL [[37](#bib.bib37)], TwoCnn [[122](#bib.bib122)],
    3DVSCNN [[123](#bib.bib123)], SSLstm [[75](#bib.bib75)], CNN_HSI [[28](#bib.bib28)]
    and SAE_LR [[34](#bib.bib34)] on Salinas. The best accuracies are marked in bold.
    The ”size” in the first line denotes the sample size per category.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：Salinas。S-DMM [[121](#bib.bib121)]、3DCAE [[40](#bib.bib40)]、SSDL [[37](#bib.bib37)]、TwoCnn [[122](#bib.bib122)]、3DVSCNN
    [[123](#bib.bib123)]、SSLstm [[75](#bib.bib75)]、CNN_HSI [[28](#bib.bib28)] 和 SAE_LR [[34](#bib.bib34)]
    在 Salinas 数据集上获得的分类准确率。最佳准确率以**粗体**标记。第一行中的“size”表示每个类别的样本数量。
- en: '| size | classes | S-DMM | 3DCAE | SSDL | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI
    | SAE_LR |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| size | classes | S-DMM | 3DCAE | SSDL | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI
    | SAE_LR |'
- en: '| 10 | 1 | 99.45 | 99.28 | 76.01 | 88.22 | 97.92 | 79.38 | 98.80 | 86.01 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 1 | 99.45 | 99.28 | 76.01 | 88.22 | 97.92 | 79.38 | 98.80 | 86.01 |'
- en: '| 2 | 99.21 | 59.04 | 69.24 | 78.09 | 99.71 | 72.49 | 98.77 | 44.21 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 99.21 | 59.04 | 69.24 | 78.09 | 99.71 | 72.49 | 98.77 | 44.21 |'
- en: '| 3 | 96.70 | 66.54 | 69.89 | 74.80 | 95.09 | 86.83 | 95.48 | 44.72 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 96.70 | 66.54 | 69.89 | 74.80 | 95.09 | 86.83 | 95.48 | 44.72 |'
- en: '| 4 | 99.56 | 98.65 | 94.96 | 98.19 | 99.28 | 99.45 | 98.36 | 97.40 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 99.56 | 98.65 | 94.96 | 98.19 | 99.28 | 99.45 | 98.36 | 97.40 |'
- en: '| 5 | 97.12 | 81.94 | 89.43 | 96.54 | 93.35 | 94.95 | 92.55 | 83.93 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 97.12 | 81.94 | 89.43 | 96.54 | 93.35 | 94.95 | 92.55 | 83.93 |'
- en: '| 6 | 89.64 | 98.52 | 96.19 | 98.96 | 99.81 | 93.65 | 99.96 | 87.28 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 89.64 | 98.52 | 96.19 | 98.96 | 99.81 | 93.65 | 99.96 | 87.28 |'
- en: '| 7 | 99.82 | 97.31 | 76.83 | 92.52 | 96.73 | 87.82 | 99.61 | 96.94 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 99.82 | 97.31 | 76.83 | 92.52 | 96.73 | 87.82 | 99.61 | 96.94 |'
- en: '| 8 | 70.53 | 68.11 | 42.58 | 54.35 | 67.89 | 61.64 | 77.51 | 41.58 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 70.53 | 68.11 | 42.58 | 54.35 | 67.89 | 61.64 | 77.51 | 41.58 |'
- en: '| 9 | 99.02 | 95.06 | 89.58 | 81.22 | 99.42 | 90.47 | 97.19 | 78.45 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 99.02 | 95.06 | 89.58 | 81.22 | 99.42 | 90.47 | 97.19 | 78.45 |'
- en: '| 10 | 91.13 | 9.43 | 76.40 | 75.18 | 91.75 | 86.66 | 89.23 | 30.75 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 91.13 | 9.43 | 76.40 | 75.18 | 91.75 | 86.66 | 89.23 | 30.75 |'
- en: '| 11 | 97.56 | 72.26 | 93.04 | 92.26 | 95.26 | 91.37 | 95.45 | 23.52 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 97.56 | 72.26 | 93.04 | 92.26 | 95.26 | 91.37 | 95.45 | 23.52 |'
- en: '| 12 | 99.87 | 72.16 | 86.60 | 86.40 | 96.65 | 95.38 | 99.96 | 82.63 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 99.87 | 72.16 | 86.60 | 86.40 | 96.65 | 95.38 | 99.96 | 82.63 |'
- en: '| 13 | 99.25 | 99.78 | 95.46 | 98.18 | 96.64 | 96.90 | 99.22 | 92.88 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 99.25 | 99.78 | 95.46 | 98.18 | 96.64 | 96.90 | 99.22 | 92.88 |'
- en: '| 14 | 96.30 | 89.93 | 90.50 | 96.10 | 99.68 | 91.68 | 96.80 | 62.40 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 96.30 | 89.93 | 90.50 | 96.10 | 99.68 | 91.68 | 96.80 | 62.40 |'
- en: '| 15 | 72.28 | 56.98 | 65.40 | 55.60 | 83.86 | 75.55 | 72.03 | 57.10 |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 72.28 | 56.98 | 65.40 | 55.60 | 83.86 | 75.55 | 72.03 | 57.10 |'
- en: '| 16 | 95.29 | 44.35 | 75.89 | 92.39 | 92.03 | 88.43 | 94.07 | 76.75 |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 95.29 | 44.35 | 75.89 | 92.39 | 92.03 | 88.43 | 94.07 | 76.75 |'
- en: '| AA | 93.92 | 75.58 | 80.50 | 84.94 | 94.07 | 87.04 | 94.06 | 67.91 |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| AA | 93.92 | 75.58 | 80.50 | 84.94 | 94.07 | 87.04 | 94.06 | 67.91 |'
- en: '| OA | 89.69 | 71.50 | 74.29 | 77.54 | 90.18 | 81.20 | 91.31 | 67.43 |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| OA | 89.69 | 71.50 | 74.29 | 77.54 | 90.18 | 81.20 | 91.31 | 67.43 |'
- en: '| 50 | 1 | 99.97 | 98.81 | 92.70 | 97.99 | 99.99 | 94.18 | 99.20 | 85.37 |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 1 | 99.97 | 98.81 | 92.70 | 97.99 | 99.99 | 94.18 | 99.20 | 85.37 |'
- en: '| 2 | 99.84 | 86.97 | 88.30 | 91.35 | 99.94 | 92.34 | 99.57 | 92.51 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 99.84 | 86.97 | 88.30 | 91.35 | 99.94 | 92.34 | 99.57 | 92.51 |'
- en: '| 3 | 99.84 | 54.83 | 87.50 | 94.87 | 99.74 | 97.02 | 99.62 | 81.25 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 99.84 | 54.83 | 87.50 | 94.87 | 99.74 | 97.02 | 99.62 | 81.25 |'
- en: '| 4 | 99.93 | 98.87 | 99.41 | 99.96 | 99.89 | 99.95 | 99.63 | 98.40 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 99.93 | 98.87 | 99.41 | 99.96 | 99.89 | 99.95 | 99.63 | 98.40 |'
- en: '| 5 | 99.40 | 95.62 | 95.83 | 98.96 | 99.38 | 98.34 | 98.79 | 95.12 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 99.40 | 95.62 | 95.83 | 98.96 | 99.38 | 98.34 | 98.79 | 95.12 |'
- en: '| 6 | 99.92 | 99.62 | 98.95 | 99.87 | 100.00 | 98.78 | 99.98 | 98.86 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 99.92 | 99.62 | 98.95 | 99.87 | 100.00 | 98.78 | 99.98 | 98.86 |'
- en: '| 7 | 99.92 | 98.17 | 96.47 | 96.60 | 99.85 | 97.80 | 99.78 | 98.55 |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 99.92 | 98.17 | 96.47 | 96.60 | 99.85 | 97.80 | 99.78 | 98.55 |'
- en: '| 8 | 68.92 | 81.74 | 62.99 | 68.05 | 85.35 | 77.17 | 77.93 | 46.04 |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 68.92 | 81.74 | 62.99 | 68.05 | 85.35 | 77.17 | 77.93 | 46.04 |'
- en: '| 9 | 99.76 | 94.87 | 95.34 | 86.01 | 99.99 | 96.15 | 99.71 | 94.84 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 99.76 | 94.87 | 95.34 | 86.01 | 99.99 | 96.15 | 99.71 | 94.84 |'
- en: '| 10 | 97.18 | 12.87 | 95.31 | 93.94 | 98.23 | 97.23 | 97.33 | 77.69 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 97.18 | 12.87 | 95.31 | 93.94 | 98.23 | 97.23 | 97.33 | 77.69 |'
- en: '| 11 | 99.57 | 75.82 | 97.73 | 97.10 | 98.59 | 97.71 | 99.54 | 77.14 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 99.57 | 75.82 | 97.73 | 97.10 | 98.59 | 97.71 | 99.54 | 77.14 |'
- en: '| 12 | 99.90 | 58.18 | 97.51 | 97.16 | 99.89 | 98.88 | 99.84 | 96.87 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 99.90 | 58.18 | 97.51 | 97.16 | 99.89 | 98.88 | 99.84 | 96.87 |'
- en: '| 13 | 99.84 | 99.98 | 98.55 | 98.60 | 100.00 | 99.12 | 99.87 | 97.33 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 99.84 | 99.98 | 98.55 | 98.60 | 100.00 | 99.12 | 99.87 | 97.33 |'
- en: '| 14 | 98.15 | 93.80 | 97.54 | 99.37 | 99.91 | 99.24 | 99.53 | 91.49 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 98.15 | 93.80 | 97.54 | 99.37 | 99.91 | 99.24 | 99.53 | 91.49 |'
- en: '| 15 | 76.12 | 41.84 | 69.04 | 67.21 | 88.77 | 86.24 | 83.39 | 65.15 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 76.12 | 41.84 | 69.04 | 67.21 | 88.77 | 86.24 | 83.39 | 65.15 |'
- en: '| 16 | 98.87 | 69.00 | 94.34 | 97.78 | 98.55 | 97.64 | 98.15 | 91.94 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 98.87 | 69.00 | 94.34 | 97.78 | 98.55 | 97.64 | 98.15 | 91.94 |'
- en: '| AA | 96.07 | 78.81 | 91.72 | 92.80 | 98.00 | 95.49 | 96.99 | 86.78 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| AA | 96.07 | 78.81 | 91.72 | 92.80 | 98.00 | 95.49 | 96.99 | 86.78 |'
- en: '| OA | 90.92 | 74.73 | 85.79 | 87.01 | 95.30 | 91.37 | 95.08 | 79.49 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| OA | 90.92 | 74.73 | 85.79 | 87.01 | 95.30 | 91.37 | 95.08 | 79.49 |'
- en: '| 100 | 1 | 99.86 | 98.81 | 98.22 | 98.74 | 99.99 | 97.86 | 99.77 | 92.44 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 1 | 99.86 | 98.81 | 98.22 | 98.74 | 99.99 | 97.86 | 99.77 | 92.44 |'
- en: '| 2 | 99.74 | 91.88 | 96.54 | 96.70 | 99.99 | 97.74 | 99.86 | 89.46 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 99.74 | 91.88 | 96.54 | 96.70 | 99.99 | 97.74 | 99.86 | 89.46 |'
- en: '| 3 | 99.99 | 63.20 | 95.40 | 97.47 | 99.16 | 98.91 | 99.79 | 92.05 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 99.99 | 63.20 | 95.40 | 97.47 | 99.16 | 98.91 | 99.79 | 92.05 |'
- en: '| 4 | 99.84 | 99.12 | 99.29 | 99.95 | 99.85 | 99.78 | 99.44 | 99.03 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 99.84 | 99.12 | 99.29 | 99.95 | 99.85 | 99.78 | 99.44 | 99.03 |'
- en: '| 5 | 99.58 | 98.24 | 98.09 | 99.61 | 99.70 | 98.89 | 99.54 | 96.32 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 99.58 | 98.24 | 98.09 | 99.61 | 99.70 | 98.89 | 99.54 | 96.32 |'
- en: '| 6 | 99.99 | 99.95 | 99.12 | 99.79 | 100.00 | 99.62 | 100.00 | 98.96 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 99.99 | 99.95 | 99.12 | 99.79 | 100.00 | 99.62 | 100.00 | 98.96 |'
- en: '| 7 | 99.93 | 98.71 | 97.14 | 97.94 | 99.88 | 98.97 | 99.86 | 98.42 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 99.93 | 98.71 | 97.14 | 97.94 | 99.88 | 98.97 | 99.86 | 98.42 |'
- en: '| 8 | 67.88 | 71.43 | 59.51 | 66.83 | 90.54 | 86.00 | 79.90 | 39.73 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 67.88 | 71.43 | 59.51 | 66.83 | 90.54 | 86.00 | 79.90 | 39.73 |'
- en: '| 9 | 99.81 | 95.51 | 94.87 | 90.65 | 99.98 | 98.15 | 99.75 | 96.34 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 99.81 | 95.51 | 94.87 | 90.65 | 99.98 | 98.15 | 99.75 | 96.34 |'
- en: '| 10 | 96.54 | 22.92 | 96.97 | 96.21 | 97.77 | 98.55 | 97.29 | 84.35 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 96.54 | 22.92 | 96.97 | 96.21 | 97.77 | 98.55 | 97.29 | 84.35 |'
- en: '| 11 | 99.75 | 76.67 | 99.28 | 99.25 | 99.82 | 99.39 | 99.70 | 92.76 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 99.75 | 76.67 | 99.28 | 99.25 | 99.82 | 99.39 | 99.70 | 92.76 |'
- en: '| 12 | 100.00 | 64.12 | 99.39 | 98.01 | 99.99 | 99.84 | 99.99 | 96.97 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 100.00 | 64.12 | 99.39 | 98.01 | 99.99 | 99.84 | 99.99 | 96.97 |'
- en: '| 13 | 99.87 | 99.98 | 98.74 | 99.34 | 99.98 | 99.38 | 99.75 | 97.48 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 99.87 | 99.98 | 98.74 | 99.34 | 99.98 | 99.38 | 99.75 | 97.48 |'
- en: '| 14 | 98.66 | 94.73 | 98.62 | 99.72 | 99.91 | 99.44 | 99.67 | 93.52 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 98.66 | 94.73 | 98.62 | 99.72 | 99.91 | 99.44 | 99.67 | 93.52 |'
- en: '| 15 | 78.73 | 63.65 | 83.03 | 70.16 | 91.31 | 86.77 | 91.86 | 69.09 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 78.73 | 63.65 | 83.03 | 70.16 | 91.31 | 86.77 | 91.86 | 69.09 |'
- en: '| 16 | 99.27 | 79.70 | 96.65 | 99.26 | 99.26 | 98.69 | 99.10 | 93.21 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 99.27 | 79.70 | 96.65 | 99.26 | 99.26 | 98.69 | 99.10 | 93.21 |'
- en: '| AA | 96.21 | 82.41 | 94.43 | 94.35 | 98.57 | 97.37 | 97.83 | 89.38 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| AA | 96.21 | 82.41 | 94.43 | 94.35 | 98.57 | 97.37 | 97.83 | 89.38 |'
- en: '| OA | 91.56 | 76.61 | 88.67 | 90.25 | 96.89 | 94.41 | 96.28 | 81.95 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| OA | 91.56 | 76.61 | 88.67 | 90.25 | 96.89 | 94.41 | 96.28 | 81.95 |'
- en: 'Table 7: KSC. Classification accuracy obtained by S-DMM [[121](#bib.bib121)],
    3DCAE [[40](#bib.bib40)], SSDL [[37](#bib.bib37)], TwoCnn [[122](#bib.bib122)],
    3DVSCNN [[123](#bib.bib123)], SSLstm [[75](#bib.bib75)], CNN_HSI [[28](#bib.bib28)]
    and SAE_LR [[34](#bib.bib34)] on KSC. The best accuracies are marked in bold.
    The ”size” in the first line denotes the sample size per category.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：KSC。S-DMM [[121](#bib.bib121)]、3DCAE [[40](#bib.bib40)]、SSDL [[37](#bib.bib37)]、TwoCnn [[122](#bib.bib122)]、3DVSCNN [[123](#bib.bib123)]、SSLstm [[75](#bib.bib75)]、CNN_HSI [[28](#bib.bib28)]
    和 SAE_LR [[34](#bib.bib34)] 在 KSC 上获得的分类准确率。最佳准确率用**粗体**标记。第一行中的”size”表示每个类别的样本大小。
- en: '| size | classes | S-DMM | 3DCAE | SSDL | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI
    | SAE_LR |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| size | classes | S-DMM | 3DCAE | SSDL | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI
    | SAE_LR |'
- en: '| 10 | 1 | 93.49 | 35.46 | 79.21 | 67.11 | 95.33 | 73.58 | 92.17 | 83.95 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 1 | 93.49 | 35.46 | 79.21 | 67.11 | 95.33 | 73.58 | 92.17 | 83.95 |'
- en: '| 2 | 89.74 | 49.40 | 67.68 | 58.37 | 40.39 | 68.45 | 81.67 | 69.01 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 89.74 | 49.40 | 67.68 | 58.37 | 40.39 | 68.45 | 81.67 | 69.01 |'
- en: '| 3 | 95.16 | 40.41 | 76.87 | 77.20 | 75.41 | 81.59 | 86.91 | 50.61 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 95.16 | 40.41 | 76.87 | 77.20 | 75.41 | 81.59 | 86.91 | 50.61 |'
- en: '| 4 | 58.72 | 5.54 | 70.33 | 75.12 | 35.87 | 76.16 | 60.83 | 20.21 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 58.72 | 5.54 | 70.33 | 75.12 | 35.87 | 76.16 | 60.83 | 20.21 |'
- en: '| 5 | 87.95 | 33.38 | 81.26 | 88.08 | 47.42 | 87.22 | 64.37 | 23.11 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 87.95 | 33.38 | 81.26 | 88.08 | 47.42 | 87.22 | 64.37 | 23.11 |'
- en: '| 6 | 93.42 | 51.05 | 79.18 | 66.44 | 64.29 | 76.71 | 66.16 | 45.39 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 93.42 | 51.05 | 79.18 | 66.44 | 64.29 | 76.71 | 66.16 | 45.39 |'
- en: '| 7 | 98.63 | 16.32 | 95.26 | 92.74 | 57.79 | 96.42 | 96.00 | 63.58 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 98.63 | 16.32 | 95.26 | 92.74 | 57.79 | 96.42 | 96.00 | 63.58 |'
- en: '| 8 | 97.93 | 46.44 | 72.42 | 61.92 | 71.88 | 52.95 | 85.77 | 58.05 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 97.93 | 46.44 | 72.42 | 61.92 | 71.88 | 52.95 | 85.77 | 58.05 |'
- en: '| 9 | 94.88 | 86.25 | 87.00 | 92.31 | 79.00 | 90.65 | 91.06 | 76.24 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 94.88 | 86.25 | 87.00 | 92.31 | 79.00 | 90.65 | 91.06 | 76.24 |'
- en: '| 10 | 98.12 | 8.76 | 72.59 | 86.27 | 56.57 | 89.04 | 85.13 | 63.12 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 98.12 | 8.76 | 72.59 | 86.27 | 56.57 | 89.04 | 85.13 | 63.12 |'
- en: '| 11 | 97.51 | 76.21 | 88.68 | 78.17 | 86.99 | 89.32 | 95.60 | 89.98 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 97.51 | 76.21 | 88.68 | 78.17 | 86.99 | 89.32 | 95.60 | 89.98 |'
- en: '| 12 | 93.69 | 8.54 | 83.65 | 78.09 | 60.79 | 83.96 | 89.66 | 69.59 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 93.69 | 8.54 | 83.65 | 78.09 | 60.79 | 83.96 | 89.66 | 69.59 |'
- en: '| 13 | 100.00 | 46.95 | 99.98 | 100.00 | 84.92 | 100.00 | 99.95 | 97.90 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 100.00 | 46.95 | 99.98 | 100.00 | 84.92 | 100.00 | 99.95 | 97.90 |'
- en: '| AA | 92.25 | 38.82 | 81.09 | 78.60 | 65.90 | 82.00 | 84.25 | 62.36 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| AA | 92.25 | 38.82 | 81.09 | 78.60 | 65.90 | 82.00 | 84.25 | 62.36 |'
- en: '| OA | 94.48 | 49.73 | 83.71 | 82.29 | 77.40 | 83.07 | 91.13 | 72.68 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| OA | 94.48 | 49.73 | 83.71 | 82.29 | 77.40 | 83.07 | 91.13 | 72.68 |'
- en: '| 50 | 1 | 97.99 | 22.53 | 96.12 | 72.95 | 98.45 | 96.77 | 94.40 | 88.21 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 1 | 97.99 | 22.53 | 96.12 | 72.95 | 98.45 | 96.77 | 94.40 | 88.21 |'
- en: '| 2 | 98.24 | 30.98 | 94.56 | 94.04 | 39.90 | 98.19 | 91.50 | 78.50 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 98.24 | 30.98 | 94.56 | 94.04 | 39.90 | 98.19 | 91.50 | 78.50 |'
- en: '| 3 | 98.69 | 45.10 | 96.55 | 90.10 | 99.13 | 99.47 | 94.47 | 83.06 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 98.69 | 45.10 | 96.55 | 90.10 | 99.13 | 99.47 | 94.47 | 83.06 |'
- en: '| 4 | 78.22 | 3.86 | 93.51 | 92.33 | 74.01 | 98.32 | 76.49 | 43.07 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 78.22 | 3.86 | 93.51 | 92.33 | 74.01 | 98.32 | 76.49 | 43.07 |'
- en: '| 5 | 92.16 | 40.54 | 96.94 | 97.12 | 64.32 | 99.55 | 87.03 | 53.33 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 92.16 | 40.54 | 96.94 | 97.12 | 64.32 | 99.55 | 87.03 | 53.33 |'
- en: '| 6 | 98.49 | 62.07 | 96.70 | 93.80 | 77.21 | 99.72 | 70.89 | 51.90 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 98.49 | 62.07 | 96.70 | 93.80 | 77.21 | 99.72 | 70.89 | 51.90 |'
- en: '| 7 | 98.36 | 18.00 | 99.64 | 97.82 | 20.36 | 100.00 | 98.00 | 84.73 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 98.36 | 18.00 | 99.64 | 97.82 | 20.36 | 100.00 | 98.00 | 84.73 |'
- en: '| 8 | 99.21 | 43.04 | 91.92 | 90.60 | 96.25 | 97.40 | 93.86 | 77.77 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 99.21 | 43.04 | 91.92 | 90.60 | 96.25 | 97.40 | 93.86 | 77.77 |'
- en: '| 9 | 99.96 | 89.77 | 98.57 | 89.55 | 63.91 | 98.83 | 98.77 | 86.47 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 99.96 | 89.77 | 98.57 | 89.55 | 63.91 | 98.83 | 98.77 | 86.47 |'
- en: '| 10 | 99.92 | 12.12 | 93.70 | 95.56 | 54.72 | 99.52 | 91.67 | 85.28 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 99.92 | 12.12 | 93.70 | 95.56 | 54.72 | 99.52 | 91.67 | 85.28 |'
- en: '| 11 | 98.62 | 80.38 | 97.86 | 98.40 | 90.95 | 99.11 | 87.75 | 96.56 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 98.62 | 80.38 | 97.86 | 98.40 | 90.95 | 99.11 | 87.75 | 96.56 |'
- en: '| 12 | 99.07 | 19.85 | 94.99 | 95.01 | 87.37 | 99.67 | 89.54 | 82.19 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 99.07 | 19.85 | 94.99 | 95.01 | 87.37 | 99.67 | 89.54 | 82.19 |'
- en: '| 13 | 100.00 | 91.24 | 100.00 | 90.00 | 96.77 | 99.46 | 98.95 | 99.44 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 100.00 | 91.24 | 100.00 | 90.00 | 96.77 | 99.46 | 98.95 | 99.44 |'
- en: '| AA | 96.84 | 43.04 | 96.24 | 92.10 | 74.10 | 98.92 | 90.25 | 77.73 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| AA | 96.84 | 43.04 | 96.24 | 92.10 | 74.10 | 98.92 | 90.25 | 77.73 |'
- en: '| OA | 98.68 | 54.01 | 96.88 | 96.61 | 96.03 | 98.72 | 97.39 | 84.93 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| OA | 98.68 | 54.01 | 96.88 | 96.61 | 96.03 | 98.72 | 97.39 | 84.93 |'
- en: '| 100 | 1 | 98.17 | 19.03 | 97.41 | 96.51 | 98.94 | 99.74 | 93.93 | 89.77 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 1 | 98.17 | 19.03 | 97.41 | 96.51 | 98.94 | 99.74 | 93.93 | 89.77 |'
- en: '| 2 | 98.74 | 34.13 | 98.60 | 99.58 | 56.50 | 99.79 | 89.93 | 80.77 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 98.74 | 34.13 | 98.60 | 99.58 | 56.50 | 99.79 | 89.93 | 80.77 |'
- en: '| 3 | 99.55 | 57.18 | 96.67 | 99.42 | 99.81 | 99.23 | 98.33 | 82.88 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 99.55 | 57.18 | 96.67 | 99.42 | 99.81 | 99.23 | 98.33 | 82.88 |'
- en: '| 4 | 88.29 | 1.38 | 97.96 | 98.68 | 88.29 | 99.14 | 85.86 | 53.95 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 88.29 | 1.38 | 97.96 | 98.68 | 88.29 | 99.14 | 85.86 | 53.95 |'
- en: '| 5 | 93.11 | 52.46 | 99.51 | 100.00 | 76.23 | 100.00 | 93.77 | 58.52 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 93.11 | 52.46 | 99.51 | 100.00 | 76.23 | 100.00 | 93.77 | 58.52 |'
- en: '| 6 | 99.61 | 59.77 | 98.68 | 97.36 | 80.62 | 99.53 | 74.96 | 58.22 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 99.61 | 59.77 | 98.68 | 97.36 | 80.62 | 99.53 | 74.96 | 58.22 |'
- en: '| 7 | 100.00 | 8.00 | 100.00 | 100.00 | 32.00 | 100.00 | 98.00 | 86.00 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 100.00 | 8.00 | 100.00 | 100.00 | 32.00 | 100.00 | 98.00 | 86.00 |'
- en: '| 8 | 99.79 | 51.81 | 95.53 | 98.07 | 98.91 | 99.40 | 97.37 | 83.96 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 99.79 | 51.81 | 95.53 | 98.07 | 98.91 | 99.40 | 97.37 | 83.96 |'
- en: '| 9 | 99.74 | 87.40 | 98.74 | 98.74 | 63.93 | 99.12 | 99.76 | 91.95 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 99.74 | 87.40 | 98.74 | 98.74 | 63.93 | 99.12 | 99.76 | 91.95 |'
- en: '| 10 | 100.00 | 13.16 | 98.22 | 99.61 | 72.47 | 100.00 | 97.70 | 91.28 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 100.00 | 13.16 | 98.22 | 99.61 | 72.47 | 100.00 | 97.70 | 91.28 |'
- en: '| 11 | 99.91 | 83.76 | 99.06 | 99.97 | 94.42 | 99.81 | 99.84 | 97.81 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 99.91 | 83.76 | 99.06 | 99.97 | 94.42 | 99.81 | 99.84 | 97.81 |'
- en: '| 12 | 99.33 | 24.94 | 97.99 | 99.03 | 94.32 | 99.80 | 95.31 | 85.73 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| 12 | 99.33 | 24.94 | 97.99 | 99.03 | 94.32 | 99.80 | 95.31 | 85.73 |'
- en: '| 13 | 100.00 | 90.07 | 99.96 | 99.94 | 97.62 | 99.94 | 99.85 | 99.58 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| 13 | 100.00 | 90.07 | 99.96 | 99.94 | 97.62 | 99.94 | 99.85 | 99.58 |'
- en: '| AA | 98.17 | 44.85 | 98.33 | 98.99 | 81.08 | 99.65 | 94.20 | 81.57 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| AA | 98.17 | 44.85 | 98.33 | 98.99 | 81.08 | 99.65 | 94.20 | 81.57 |'
- en: '| OA | 98.96 | 59.63 | 98.75 | 99.15 | 98.55 | 99.68 | 98.05 | 89.15 | ![Refer
    to caption](img/dd49ff06da46a1a02251de8a056d0c52.png)![Refer to caption](img/fec4c877383056b9f66d72dacbbfbfa0.png)![Refer
    to caption](img/e5b192702570579d7b9d4cbc39d56eb6.png)'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '| OA | 98.96 | 59.63 | 98.75 | 99.15 | 98.55 | 99.68 | 98.05 | 89.15 | ![参见说明](img/dd49ff06da46a1a02251de8a056d0c52.png)![参见说明](img/fec4c877383056b9f66d72dacbbfbfa0.png)![参见说明](img/e5b192702570579d7b9d4cbc39d56eb6.png)'
- en: 'Figure 10: Change in accuracy over the number of samples for each category.
    [10](#S4.F10 "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") PaviaU. [10](#S4.F10 "Figure 10 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") Salinas. [10](#S4.F10 "Figure 10 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") KSC.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：每个类别样本数量变化对准确度的影响。 [10](#S4.F10 "图 10 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类深度学习")
    PaviaU. [10](#S4.F10 "图 10 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类深度学习") Salinas.
    [10](#S4.F10 "图 10 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类深度学习") KSC.
- en: '![Refer to caption](img/457af78222db50938fbf59246aa04851.png)![Refer to caption](img/0c422b45dbb99a47fe8818abaa9a3879.png)![Refer
    to caption](img/79be6492ac53069022d2deda479fbeb8.png)![Refer to caption](img/234d6333038b37f01c264dc204d8ea48.png)![Refer
    to caption](img/be9d1b5bbb273db4bce02c800b2eb878.png)![Refer to caption](img/4cd2c9522f788b687e95aafbf8a1e21d.png)![Refer
    to caption](img/21c2936dc0be3018615d7038412f5e26.png)![Refer to caption](img/97dde8237dd40ec84f311976a9c82111.png)![Refer
    to caption](img/2fbf03b337299db99905d2b6b8766d07.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/457af78222db50938fbf59246aa04851.png)![参见说明](img/0c422b45dbb99a47fe8818abaa9a3879.png)![参见说明](img/79be6492ac53069022d2deda479fbeb8.png)![参见说明](img/234d6333038b37f01c264dc204d8ea48.png)![参见说明](img/be9d1b5bbb273db4bce02c800b2eb878.png)![参见说明](img/4cd2c9522f788b687e95aafbf8a1e21d.png)![参见说明](img/21c2936dc0be3018615d7038412f5e26.png)![参见说明](img/97dde8237dd40ec84f311976a9c82111.png)![参见说明](img/2fbf03b337299db99905d2b6b8766d07.png)'
- en: 'Figure 11: Classification maps on the PaviaU data set (10 samples per class).
    [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [11](#S4.F11 "Figure 11 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [11](#S4.F11
    "Figure 11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [11](#S4.F11 "Figure
    11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：PaviaU 数据集上的分类图（每类 10 个样本）。 [11](#S4.F11 "图 11 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类的深度学习")
    原始。 [11](#S4.F11 "图 11 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类的深度学习") S-DMM。
    [11](#S4.F11 "图 11 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类的深度学习") 3DCAE。 [11](#S4.F11
    "图 11 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类的深度学习") SSDL。 [11](#S4.F11 "图 11
    ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类的深度学习") TwoCnn。 [11](#S4.F11 "图 11 ‣ 4.3
    实验结果与分析 ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类的深度学习") 3DVSCNN。 [11](#S4.F11 "图 11 ‣ 4.3 实验结果与分析
    ‣ 4 实验 ‣ 调查：少量标记样本的高光谱图像分类的深度学习") SSLstm。 [11](#S4.F11 "图 11 ‣ 4.3 实验结果与分析 ‣ 4
    实验 ‣ 调查：少量标记样本的高光谱图像分类的深度学习") CNN_HSI。 [11](#S4.F11 "图 11 ‣ 4.3 实验结果与分析 ‣ 4 实验
    ‣ 调查：少量标记样本的高光谱图像分类的深度学习") SAE_LR。
- en: '![Refer to caption](img/53210bbe28e3269788ae65e7f8383140.png)![Refer to caption](img/2e5d9057629e38657b69bbd1ddead6f9.png)![Refer
    to caption](img/5f28e554f52eda3dac9789320aa6283c.png)![Refer to caption](img/7eaf321881768657931cb971003bed7c.png)![Refer
    to caption](img/54b2b9d40e8b1e99628ea10e4a2c3134.png)![Refer to caption](img/45d3317fa95c44f835cf911bdb9d54b3.png)![Refer
    to caption](img/a2bd0045490d2c7022a5f4c6d55fe3d9.png)![Refer to caption](img/825426001b1254f6e45aa612ac28ada3.png)![Refer
    to caption](img/14c842b14a5a9b63f7fee0d21c1c3deb.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/53210bbe28e3269788ae65e7f8383140.png)![参见说明](img/2e5d9057629e38657b69bbd1ddead6f9.png)![参见说明](img/5f28e554f52eda3dac9789320aa6283c.png)![参见说明](img/7eaf321881768657931cb971003bed7c.png)![参见说明](img/54b2b9d40e8b1e99628ea10e4a2c3134.png)![参见说明](img/45d3317fa95c44f835cf911bdb9d54b3.png)![参见说明](img/a2bd0045490d2c7022a5f4c6d55fe3d9.png)![参见说明](img/825426001b1254f6e45aa612ac28ada3.png)![参见说明](img/14c842b14a5a9b63f7fee0d21c1c3deb.png)'
- en: 'Figure 12: Classification maps on the PaviaU data set (50 samples per class).
    [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [12](#S4.F12 "Figure 12 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [12](#S4.F12
    "Figure 12 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [12](#S4.F12 "Figure
    12 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '图 12: PaviaU 数据集上的分类图（每类 50 个样本）。 [12](#S4.F12 "图 12 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣
    综述：少量标注样本的高光谱图像分类的深度学习") 原始。 [12](#S4.F12 "图 12 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标注样本的高光谱图像分类的深度学习")
    S-DMM。 [12](#S4.F12 "图 12 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标注样本的高光谱图像分类的深度学习") 3DCAE。
    [12](#S4.F12 "图 12 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标注样本的高光谱图像分类的深度学习") SSDL。 [12](#S4.F12
    "图 12 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标注样本的高光谱图像分类的深度学习") TwoCnn。 [12](#S4.F12 "图
    12 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标注样本的高光谱图像分类的深度学习") 3DVSCNN。 [12](#S4.F12 "图 12
    ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标注样本的高光谱图像分类的深度学习") SSLstm。 [12](#S4.F12 "图 12 ‣ 4.3
    实验结果与分析 ‣ 4 实验 ‣ 综述：少量标注样本的高光谱图像分类的深度学习") CNN_HSI。 [12](#S4.F12 "图 12 ‣ 4.3 实验结果与分析
    ‣ 4 实验 ‣ 综述：少量标注样本的高光谱图像分类的深度学习") SAE_LR。'
- en: '![Refer to caption](img/cf8375b94847966f4e8a483f02beed18.png)![Refer to caption](img/048b8c8f336f995572908d2c38615310.png)![Refer
    to caption](img/b5ee6228a621a20a69b78ea9fedfd0df.png)![Refer to caption](img/3fc5acccf04b233ccc949c2c1a9c24b9.png)![Refer
    to caption](img/85f13326cc7ac671145cc8576995abf1.png)![Refer to caption](img/c0ef83f8142ae573fac9f4cfd17744ca.png)![Refer
    to caption](img/fda22dcd985a6443be87fd8c9e74aa67.png)![Refer to caption](img/b82acf4cd7f4b94e491782fd8d5f4522.png)![Refer
    to caption](img/6b0df880e1e37eea17a2721c650e3a46.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cf8375b94847966f4e8a483f02beed18.png)![参见说明](img/048b8c8f336f995572908d2c38615310.png)![参见说明](img/b5ee6228a621a20a69b78ea9fedfd0df.png)![参见说明](img/3fc5acccf04b233ccc949c2c1a9c24b9.png)![参见说明](img/85f13326cc7ac671145cc8576995abf1.png)![参见说明](img/c0ef83f8142ae573fac9f4cfd17744ca.png)![参见说明](img/fda22dcd985a6443be87fd8c9e74aa67.png)![参见说明](img/b82acf4cd7f4b94e491782fd8d5f4522.png)![参见说明](img/6b0df880e1e37eea17a2721c650e3a46.png)'
- en: 'Figure 13: Classification maps on the PaviaU data set (100 samples per class).
    [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [13](#S4.F13 "Figure 13 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [13](#S4.F13
    "Figure 13 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [13](#S4.F13 "Figure
    13 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：PaviaU 数据集上的分类图（每类 100 个样本）。 [13](#S4.F13 "图 13 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣
    调查：深度学习在少量标记样本下的高光谱图像分类") 原始。 [13](#S4.F13 "图 13 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本下的高光谱图像分类")
    S-DMM。 [13](#S4.F13 "图 13 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本下的高光谱图像分类") 3DCAE。
    [13](#S4.F13 "图 13 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本下的高光谱图像分类") SSDL。 [13](#S4.F13
    "图 13 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本下的高光谱图像分类") TwoCnn。 [13](#S4.F13 "图
    13 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本下的高光谱图像分类") 3DVSCNN。 [13](#S4.F13 "图 13
    ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本下的高光谱图像分类") SSLstm。 [13](#S4.F13 "图 13 ‣
    4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本下的高光谱图像分类") CNN_HSI。 [13](#S4.F13 "图 13 ‣ 4.3
    实验结果与分析 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本下的高光谱图像分类") SAE_LR。
- en: '![Refer to caption](img/45f3f33aff344f770282e858b6b081e9.png)![Refer to caption](img/242379d2501a01c140fcbff386baefdc.png)![Refer
    to caption](img/eb768fabec326ef59daa82f5d4c3b7d4.png)![Refer to caption](img/ca86cd6e1e9027023bc4b40ebe489a2b.png)![Refer
    to caption](img/8ebd28a970b41f23cb1ce8a3eebc3dc3.png)![Refer to caption](img/439af2f5c20f569f00bf4aef8dcca610.png)![Refer
    to caption](img/905bb4c3080b1df23a7ab060c65c6be9.png)![Refer to caption](img/ffaa74c07706ffb9ee72c8241a89705c.png)![Refer
    to caption](img/a38fe27b7ce0d8705579d4b2e1f6915c.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/45f3f33aff344f770282e858b6b081e9.png)![参见说明](img/242379d2501a01c140fcbff386baefdc.png)![参见说明](img/eb768fabec326ef59daa82f5d4c3b7d4.png)![参见说明](img/ca86cd6e1e9027023bc4b40ebe489a2b.png)![参见说明](img/8ebd28a970b41f23cb1ce8a3eebc3dc3.png)![参见说明](img/439af2f5c20f569f00bf4aef8dcca610.png)![参见说明](img/905bb4c3080b1df23a7ab060c65c6be9.png)![参见说明](img/ffaa74c07706ffb9ee72c8241a89705c.png)![参见说明](img/a38fe27b7ce0d8705579d4b2e1f6915c.png)'
- en: 'Figure 14: Classification maps on the Salinas data set (10 samples per class).
    [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [14](#S4.F14 "Figure 14 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [14](#S4.F14
    "Figure 14 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [14](#S4.F14 "Figure
    14 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：Salinas 数据集上的分类图（每类 10 个样本）。 [14](#S4.F14 "图 14 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标记样本的高光谱图像分类深度学习")
    原始。 [14](#S4.F14 "图 14 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标记样本的高光谱图像分类深度学习") S-DMM。 [14](#S4.F14
    "图 14 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标记样本的高光谱图像分类深度学习") 3DCAE。 [14](#S4.F14 "图 14
    ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标记样本的高光谱图像分类深度学习") SSDL。 [14](#S4.F14 "图 14 ‣ 4.3
    实验结果与分析 ‣ 4 实验 ‣ 综述：少量标记样本的高光谱图像分类深度学习") TwoCnn。 [14](#S4.F14 "图 14 ‣ 4.3 实验结果与分析
    ‣ 4 实验 ‣ 综述：少量标记样本的高光谱图像分类深度学习") 3DVSCNN。 [14](#S4.F14 "图 14 ‣ 4.3 实验结果与分析 ‣ 4
    实验 ‣ 综述：少量标记样本的高光谱图像分类深度学习") SSLstm。 [14](#S4.F14 "图 14 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣
    综述：少量标记样本的高光谱图像分类深度学习") CNN_HSI。 [14](#S4.F14 "图 14 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 综述：少量标记样本的高光谱图像分类深度学习")
    SAE_LR。
- en: '![Refer to caption](img/2e54fc7491f1ad912af2746de63e9674.png)![Refer to caption](img/a7ea03cfa709e3633938258288b9c95f.png)![Refer
    to caption](img/63a7eb0f2bf96cc0324d45239304ad44.png)![Refer to caption](img/0fcf4987788c27d206b8674175963d8a.png)![Refer
    to caption](img/d90a130e74206516e326a60845970635.png)![Refer to caption](img/ac353133206ebe2e7ea6e071f2bb5583.png)![Refer
    to caption](img/a543deb9002e7dc3aea22ad62b7a7e12.png)![Refer to caption](img/76e7608b9347d8dc920fb7ef4ab45ca8.png)![Refer
    to caption](img/11eae55fc0f61baf7601ab865f5130ad.png)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2e54fc7491f1ad912af2746de63e9674.png)![参见说明](img/a7ea03cfa709e3633938258288b9c95f.png)![参见说明](img/63a7eb0f2bf96cc0324d45239304ad44.png)![参见说明](img/0fcf4987788c27d206b8674175963d8a.png)![参见说明](img/d90a130e74206516e326a60845970635.png)![参见说明](img/ac353133206ebe2e7ea6e071f2bb5583.png)![参见说明](img/a543deb9002e7dc3aea22ad62b7a7e12.png)![参见说明](img/76e7608b9347d8dc920fb7ef4ab45ca8.png)![参见说明](img/11eae55fc0f61baf7601ab865f5130ad.png)'
- en: 'Figure 15: Classification maps on the Salinas (50 samples per class). [15](#S4.F15
    "Figure 15 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    Original. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental results and analysis ‣ 4
    Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification with
    Few Labeled Samples") S-DMM. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [15](#S4.F15 "Figure 15 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [15](#S4.F15
    "Figure 15 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [15](#S4.F15 "Figure
    15 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：Salinas 数据集上的分类图（每类50个样本）。 [15](#S4.F15 "图15 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：用于少量标记样本的高光谱图像分类的深度学习")
    原始。 [15](#S4.F15 "图15 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：用于少量标记样本的高光谱图像分类的深度学习") S-DMM。
    [15](#S4.F15 "图15 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：用于少量标记样本的高光谱图像分类的深度学习") 3DCAE。 [15](#S4.F15
    "图15 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：用于少量标记样本的高光谱图像分类的深度学习") SSDL。 [15](#S4.F15 "图15
    ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：用于少量标记样本的高光谱图像分类的深度学习") TwoCnn。 [15](#S4.F15 "图15 ‣
    4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：用于少量标记样本的高光谱图像分类的深度学习") 3DVSCNN。 [15](#S4.F15 "图15 ‣ 4.3
    实验结果与分析 ‣ 4 实验 ‣ 调查：用于少量标记样本的高光谱图像分类的深度学习") SSLstm。 [15](#S4.F15 "图15 ‣ 4.3 实验结果与分析
    ‣ 4 实验 ‣ 调查：用于少量标记样本的高光谱图像分类的深度学习") CNN_HSI。 [15](#S4.F15 "图15 ‣ 4.3 实验结果与分析 ‣
    4 实验 ‣ 调查：用于少量标记样本的高光谱图像分类的深度学习") SAE_LR。
- en: '![Refer to caption](img/476033eeec589663cdf974c38fec18de.png)![Refer to caption](img/80e5b13c3dcbfdc953d5080d62d8f19a.png)![Refer
    to caption](img/904489726a8c81a2b589e95027cfe590.png)![Refer to caption](img/498879c5b8bce7e86885bcc27268287b.png)![Refer
    to caption](img/04b51f7573fe53c6621c2ce44e316c22.png)![Refer to caption](img/7df0da058528a9bbaa9d61d54fcab08b.png)![Refer
    to caption](img/e5ae726db365fd54f979aa80bd53accd.png)![Refer to caption](img/6d062e82b8c7fa03d097bd2ca5483391.png)![Refer
    to caption](img/62dfeef7d2a99f616559b8179ad9ac81.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/476033eeec589663cdf974c38fec18de.png)![参见说明](img/80e5b13c3dcbfdc953d5080d62d8f19a.png)![参见说明](img/904489726a8c81a2b589e95027cfe590.png)![参见说明](img/498879c5b8bce7e86885bcc27268287b.png)![参见说明](img/04b51f7573fe53c6621c2ce44e316c22.png)![参见说明](img/7df0da058528a9bbaa9d61d54fcab08b.png)![参见说明](img/e5ae726db365fd54f979aa80bd53accd.png)![参见说明](img/6d062e82b8c7fa03d097bd2ca5483391.png)![参见说明](img/62dfeef7d2a99f616559b8179ad9ac81.png)'
- en: 'Figure 16: Classification maps on the Salinas data set (100 samples per class).
    [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [16](#S4.F16 "Figure 16 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [16](#S4.F16
    "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [16](#S4.F16 "Figure
    16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '图16：Salinas 数据集上的分类图（每类 100 个样本）。[16](#S4.F16 "Figure 16 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") 原始。[16](#S4.F16 "Figure 16 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") S-DMM。[16](#S4.F16
    "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    3DCAE。[16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") SSDL。[16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") TwoCnn。[16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DVSCNN。[16](#S4.F16 "Figure 16 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSLstm。[16](#S4.F16
    "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    CNN_HSI。[16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") SAE_LR。'
- en: '![Refer to caption](img/130655677ba4dc1b690dc23b7b16c9e6.png)![Refer to caption](img/0e0dee93ddec946989c833e4455c2dd6.png)![Refer
    to caption](img/8ddf84d4743f60deaa1c747a68b515da.png)![Refer to caption](img/e24515612a1c1d0cc588e93e19bd8adf.png)![Refer
    to caption](img/ba8d01cac7fe4fe0917e5b18cefa0ebd.png)![Refer to caption](img/cf5418fcca5c4b0525b8941da92137c9.png)![Refer
    to caption](img/8df95f2cebc9d6ac8e19f77ba4e24feb.png)![Refer to caption](img/b5c9b1730a2d46fe8155f36f5d14c94c.png)![Refer
    to caption](img/8032aae21ea33c850ea46afcaabd5d64.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/130655677ba4dc1b690dc23b7b16c9e6.png)![参考说明](img/0e0dee93ddec946989c833e4455c2dd6.png)![参考说明](img/8ddf84d4743f60deaa1c747a68b515da.png)![参考说明](img/e24515612a1c1d0cc588e93e19bd8adf.png)![参考说明](img/ba8d01cac7fe4fe0917e5b18cefa0ebd.png)![参考说明](img/cf5418fcca5c4b0525b8941da92137c9.png)![参考说明](img/8df95f2cebc9d6ac8e19f77ba4e24feb.png)![参考说明](img/b5c9b1730a2d46fe8155f36f5d14c94c.png)![参考说明](img/8032aae21ea33c850ea46afcaabd5d64.png)'
- en: 'Figure 17: Classification maps on the KSC data set (10 samples per class).
    [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [17](#S4.F17 "Figure 17 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [17](#S4.F17
    "Figure 17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [17](#S4.F17 "Figure
    17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图17：KSC数据集上的分类地图（每类10个样本）。[17](#S4.F17 "图17 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 深度学习在少标注样本下的高光谱图像分类调查")
    原始图像。[17](#S4.F17 "图17 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 深度学习在少标注样本下的高光谱图像分类调查") S-DMM。[17](#S4.F17
    "图17 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 深度学习在少标注样本下的高光谱图像分类调查") 3DCAE。[17](#S4.F17 "图17 ‣
    4.3 实验结果与分析 ‣ 4 实验 ‣ 深度学习在少标注样本下的高光谱图像分类调查") SSDL。[17](#S4.F17 "图17 ‣ 4.3 实验结果与分析
    ‣ 4 实验 ‣ 深度学习在少标注样本下的高光谱图像分类调查") TwoCnn。[17](#S4.F17 "图17 ‣ 4.3 实验结果与分析 ‣ 4 实验
    ‣ 深度学习在少标注样本下的高光谱图像分类调查") 3DVSCNN。[17](#S4.F17 "图17 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 深度学习在少标注样本下的高光谱图像分类调查")
    SSLstm。[17](#S4.F17 "图17 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 深度学习在少标注样本下的高光谱图像分类调查") CNN_HSI。[17](#S4.F17
    "图17 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 深度学习在少标注样本下的高光谱图像分类调查") SAE_LR。
- en: '![Refer to caption](img/0265b16b8037b38b2834f536f94418e7.png)![Refer to caption](img/4fd942dbdf13abfcf772862bd3a68f4b.png)![Refer
    to caption](img/88d11be55d09ef23769e40a392f3f64c.png)![Refer to caption](img/245a69d4fc6fe8c92a9033e5647a060f.png)![Refer
    to caption](img/8ef8f782c257d0ccaf7559e5b484658c.png)![Refer to caption](img/c59378f92391d212ba10465dd1b76b75.png)![Refer
    to caption](img/d19f74eb09888632ff365893fc58b37b.png)![Refer to caption](img/24c70588a75199fcc3c5307360ac6069.png)![Refer
    to caption](img/e97b56a307863e8b2c05ca07c54fdd8d.png)'
  id: totrans-335
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/0265b16b8037b38b2834f536f94418e7.png)![参考标题](img/4fd942dbdf13abfcf772862bd3a68f4b.png)![参考标题](img/88d11be55d09ef23769e40a392f3f64c.png)![参考标题](img/245a69d4fc6fe8c92a9033e5647a060f.png)![参考标题](img/8ef8f782c257d0ccaf7559e5b484658c.png)![参考标题](img/c59378f92391d212ba10465dd1b76b75.png)![参考标题](img/d19f74eb09888632ff365893fc58b37b.png)![参考标题](img/24c70588a75199fcc3c5307360ac6069.png)![参考标题](img/e97b56a307863e8b2c05ca07c54fdd8d.png)'
- en: 'Figure 18: Classification maps on the KSC data set (50 samples per class).
    [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [18](#S4.F18 "Figure 18 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [18](#S4.F18
    "Figure 18 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [18](#S4.F18 "Figure
    18 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18：KSC 数据集上的分类图（每类 50 个样本）。 [18](#S4.F18 "图 18 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少标记样本的高光谱图像分类深度学习")
    原始。 [18](#S4.F18 "图 18 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少标记样本的高光谱图像分类深度学习") S-DMM。 [18](#S4.F18
    "图 18 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少标记样本的高光谱图像分类深度学习") 3DCAE。 [18](#S4.F18 "图 18
    ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少标记样本的高光谱图像分类深度学习") SSDL。 [18](#S4.F18 "图 18 ‣ 4.3 实验结果与分析
    ‣ 4 实验 ‣ 调查：少标记样本的高光谱图像分类深度学习") TwoCnn。 [18](#S4.F18 "图 18 ‣ 4.3 实验结果与分析 ‣ 4 实验
    ‣ 调查：少标记样本的高光谱图像分类深度学习") 3DVSCNN。 [18](#S4.F18 "图 18 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少标记样本的高光谱图像分类深度学习")
    SSLstm。 [18](#S4.F18 "图 18 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少标记样本的高光谱图像分类深度学习") CNN_HSI。
    [18](#S4.F18 "图 18 ‣ 4.3 实验结果与分析 ‣ 4 实验 ‣ 调查：少标记样本的高光谱图像分类深度学习") SAE_LR。
- en: '![Refer to caption](img/888fe67fe507d7e0c014cb182928fe8e.png)![Refer to caption](img/204011acd85cd7c0da05806cee893f66.png)![Refer
    to caption](img/bc8037c940c4c89d5a874ad228e74b16.png)![Refer to caption](img/ddafa0a0290dce47ff5b486872956b80.png)![Refer
    to caption](img/cee87d31ffb9b5bef7e6cb455d01b31e.png)![Refer to caption](img/39cb92939100bcac628dfbaf57193775.png)![Refer
    to caption](img/ac8096cc1fc7100fb33c5d04424ff86e.png)![Refer to caption](img/ac3100b0a491fb6a30919b02e8b1bc3c.png)![Refer
    to caption](img/3249812dfac161c703f6dcdb29d2765a.png)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/888fe67fe507d7e0c014cb182928fe8e.png)![参见说明](img/204011acd85cd7c0da05806cee893f66.png)![参见说明](img/bc8037c940c4c89d5a874ad228e74b16.png)![参见说明](img/ddafa0a0290dce47ff5b486872956b80.png)![参见说明](img/cee87d31ffb9b5bef7e6cb455d01b31e.png)![参见说明](img/39cb92939100bcac628dfbaf57193775.png)![参见说明](img/ac8096cc1fc7100fb33c5d04424ff86e.png)![参见说明](img/ac3100b0a491fb6a30919b02e8b1bc3c.png)![参见说明](img/3249812dfac161c703f6dcdb29d2765a.png)'
- en: 'Figure 19: Classification maps on the KSC data set (100 samples per class).
    [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [19](#S4.F19 "Figure 19 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [19](#S4.F19
    "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [19](#S4.F19 "Figure
    19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '图 19：KSC 数据集上的分类地图（每类 100 个样本）。[19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 原始。[19](#S4.F19 "Figure 19 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") S-DMM。[19](#S4.F19 "Figure 19
    ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") 3DCAE。[19](#S4.F19
    "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    SSDL。[19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") TwoCnn。[19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") 3DVSCNN。[19](#S4.F19 "Figure 19 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") SSLstm。[19](#S4.F19 "Figure 19
    ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") CNN_HSI。[19](#S4.F19
    "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    SAE_LR。'
- en: 4.4 Model parameters
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 模型参数
- en: 'To further explore the reasons why the model has achieved different results
    on the benchmark data set, we also counted the number of trainable parameters
    of each framework (including the decoder module) on different data sets, which
    are shown in Table [8](#S4.T8 "Table 8 ‣ 4.4 Model parameters ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples"). On all data sets, the model with the least number of training parameters
    is the SAE_LR, the second is the CNN_HSI and the most is the TwoCnn. SAE_LR is
    a lightweight architecture in all models for the simple linear layer, but its
    performance is poor. Different from other 2D convolution approaches in HSI, CNN_HSI
    solely uses a $1\times 1$ kernel to process an image. Moreover, it uses a $1\times
    1$ convolution layer to serve as a classifier instead of the linear layer, which
    greatly reduces the number of trainable parameters. The next is the S-DMM. This
    also explains why S-DMM and CNN_HSI are less affected by augmentation in sample
    size but very effective on few samples. Additionally, the problem of overfitting
    is of little concern in these approaches. Stacking the spectral and spatial feature
    to generate the final fused feature is the main reason for the large number of
    parameters of TwoCnn. However, regardless of its potentially millions of trainable
    parameters, it can work well on limited samples, benefiting from transfer learning,
    which decreases trainable parameters and achieves good performance on all target
    data sets. Next, the models with the most parameters are successively 3DCAE and
    SSLstm. 3DCAE’s trainable parameters are at most eight times those of SSDL, which
    contains not only a 1D autoencoder in the spectral branch but also a spatial branch
    based on a 2D convolutional network, but 3DCAE is still worse than SSDL. Although
    3D convolutional and pooling modules can greatly avoid the problem of data structure
    information loss caused by the flattening operation, the complexity of the 3D
    structure and the symmetric structure of the autoencoder increase the number of
    model parameters, which make it easy to overfit the model. 3DVSCNN also uses a
    3D convolutional module and is better than 3DCAE, which first reduces the number
    of redundant bands by PCA. That may also be applied to 3DCAE to decrease the number
    of model parameters and make good use of characteristics of 3D convolution, extracting
    spectral and spatial information simultaneously. The main contribution of the
    parameter of SSLstm comes from the spatial branch. Although the gate structure
    of LSTM improves the model’s capabilities of long and short memory, it increases
    the complexity of the model. When the number of hidden layer units increases,
    the model’s parameters will also skyrocket greatly. Perhaps it is the coupling
    between the spectral features and recurrent network that make performance of SSLstm
    not as bad as that of 3DCAE on all data sets, which has a similar number of parameters
    and even achieved superior results on KSC. Moreover, there are no methods that
    were adopted for solving the problem of few samples. This finding also shows that
    supervised learning is better than unsupervised learning in some tasks.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步探讨模型在基准数据集上取得不同结果的原因，我们还统计了每个框架在不同数据集上的可训练参数数量（包括解码器模块），见表格 [8](#S4.T8
    "表格 8 ‣ 4.4 模型参数 ‣ 4 实验 ‣ 调查：少量标注样本的高光谱图像分类的深度学习"). 在所有数据集中，具有最少训练参数的模型是SAE_LR，其次是CNN_HSI，最多的是TwoCnn。SAE_LR是所有模型中最轻量的架构，只有简单的线性层，但其性能较差。与HSI中的其他2D卷积方法不同，CNN_HSI仅使用$1\times
    1$的卷积核处理图像。此外，它使用$1\times 1$的卷积层作为分类器，而不是线性层，这大大减少了可训练参数的数量。接下来是S-DMM。这也解释了为什么S-DMM和CNN_HSI在样本量增加时影响较小，但在少量样本上非常有效。此外，这些方法对过拟合问题关注较少。堆叠光谱和空间特征以生成最终融合特征是TwoCnn参数数量多的主要原因。然而，尽管其可训练参数可能达到数百万，但凭借迁移学习，它可以在有限样本上表现良好，减少了可训练参数，并在所有目标数据集上实现了良好的性能。接下来，参数最多的模型依次是3DCAE和SSLstm。3DCAE的可训练参数最多是SSDL的八倍，SSDL不仅包含光谱分支中的1D自编码器，还包含基于2D卷积网络的空间分支，但3DCAE仍然逊色于SSDL。尽管3D卷积和池化模块可以大大避免由扁平化操作引起的数据结构信息丢失问题，但3D结构的复杂性和自编码器的对称结构增加了模型参数的数量，使得模型容易过拟合。3DVSCNN也使用了3D卷积模块，且优于3DCAE，首先通过PCA减少了冗余波段。这也可以应用于3DCAE，以减少模型参数数量，并充分利用3D卷积的特性，同时提取光谱和空间信息。SSLstm参数的主要贡献来自空间分支。尽管LSTM的门结构提升了模型的长短期记忆能力，但增加了模型的复杂性。当隐藏层单元数量增加时，模型的参数也会大幅增加。也许是光谱特征与递归网络的耦合使得SSLstm在所有数据集上的表现不如3DCAE那么糟糕，尽管两者参数数量相似，SSLstm在KSC数据集上甚至取得了更优的结果。此外，解决少样本问题的方法没有被采用。这一发现也表明，在某些任务中，监督学习优于无监督学习。
- en: 'Table 8: The number of trainable parameters'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 表8：可训练参数的数量
- en: '|  | PaviaU | Salinas | KSC |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '|  | PaviaU | Salinas | KSC |'
- en: '| --- | --- | --- | --- |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| S-DMM | 33921 | 40385 | 38593 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| S-DMM | 33921 | 40385 | 38593 |'
- en: '| 3DCAE | 256563 | 447315 | 425139 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| 3DCAE | 256563 | 447315 | 425139 |'
- en: '| SSDL | 35650 | 48718 | 44967 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| SSDL | 35650 | 48718 | 44967 |'
- en: '| TwoCnn | 1379399 | 1542206 | 1501003 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| TwoCnn | 1379399 | 1542206 | 1501003 |'
- en: '| 3DVSCNN | 42209 | 42776 | 227613 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 3DVSCNN | 42209 | 42776 | 227613 |'
- en: '| SSLstm | 367506 | 370208 | 401818 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| SSLstm | 367506 | 370208 | 401818 |'
- en: '| CNN_HSI | 22153 | 33536 | 31753 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| CNN_HSI | 22153 | 33536 | 31753 |'
- en: '| SAE_LR | 21426 | 5969 | 5496 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| SAE_LR | 21426 | 5969 | 5496 |'
- en: 4.5 The speed of model convergence
  id: totrans-352
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 模型收敛速度
- en: 'In addition, we compare the convergence speed of the model according to the
    changes in training loss of each model in the first 200 epochs on each group of
    experiments (see Figure [20](#S4.F20 "Figure 20 ‣ 4.5 The speed of model convergence
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples")$\sim$[22](#S4.F22 "Figure 22 ‣ 4.5 The speed of model
    convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples")). Because the autoencoder and classifier
    of 3DCAE are be trained separately, and all data are used during training the
    autoencoder, it is not comparable to other models. Therefore, it is not be listed
    here. On all data sets, S-DMM has the fastest convergence speed. After approximately
    3 epochs, the training loss tends to become stable given its fewer parameters.
    Although CNN_HSI has a similar performance to S-DMM and fewer parameters, the
    learning curve of CNN_HSI’s convergence rate is slower than that of S-DMM and
    is sometimes accompanied by turbulence. The second place regarding performance
    is held by TwoCnn, which is mainly due to transfer learning to better position
    the initial parameters, and it actually has fewer parameters requiring training.
    Thus, it just needs a few epochs to fine-tune on the target data set. Moreover,
    the training curve of most models stabilizes after 100 epochs. The training loss
    of the SSLstm has severe oscillations in all data sets. This is especially noted
    in the SeLstm, where the loss sometimes has difficulty in decreasing. When the
    sequence is very long, the challenge might be that the recurrent neural network
    is more susceptible to a vanishing or exploding gradient. Moreover, the pixels
    of the hyperspectral image usually contain hundreds of bands, which is the reason
    why the training loss has difficulty decreasing or oscillations occur in SeLstm.
    In the spatial branch, it does not have this serious condition because the length
    of the spatial sequence depending on patch size is shorter than spectral sequences.
    During training, the LSTM-based model spent a considerable amount of time because
    it cannot train in parallel.'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们根据每组实验中前200个周期的训练损失变化来比较模型的收敛速度（见图[20](#S4.F20 "Figure 20 ‣ 4.5 The speed
    of model convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples")$\sim$[22](#S4.F22 "Figure 22 ‣
    4.5 The speed of model convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples")）。由于3DCAE的自编码器和分类器是分开训练的，并且在训练自编码器时使用了所有数据，这使得它与其他模型不可比。因此，此处未列出。在所有数据集中，S-DMM具有最快的收敛速度。由于其参数较少，经过大约3个周期后，训练损失趋于稳定。尽管CNN_HSI具有类似于S-DMM的性能且参数较少，但CNN_HSI的收敛率学习曲线比S-DMM慢，有时还伴有波动。第二名的性能由TwoCnn保持，这主要得益于迁移学习更好地定位了初始参数，实际上它需要训练的参数更少。因此，只需在目标数据集上进行几个周期的微调。此外，大多数模型的训练曲线在100个周期后稳定。SSLstm的训练损失在所有数据集中都有严重的波动。这在SeLstm中尤为明显，其中损失有时难以减少。当序列非常长时，挑战可能是递归神经网络更容易出现梯度消失或爆炸。此外，高光谱图像的像素通常包含数百个波段，这也是为什么SeLstm的训练损失难以减少或出现波动的原因。在空间分支中，由于空间序列的长度（取决于补丁大小）比光谱序列短，因此没有这种严重的情况。在训练期间，基于LSTM的模型花费了相当多的时间，因为它不能并行训练。'
- en: '![Refer to caption](img/92ff38140d8432c58a271ba8ff5dd534.png)![Refer to caption](img/202d1fa3f6ae3cd4333adad1bd58fa2d.png)![Refer
    to caption](img/8aefb9fed52ddb72415634ddecd17138.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/92ff38140d8432c58a271ba8ff5dd534.png)![参见说明](img/202d1fa3f6ae3cd4333adad1bd58fa2d.png)![参见说明](img/8aefb9fed52ddb72415634ddecd17138.png)'
- en: 'Figure 20: Training Loss on the PaviaU data set. [20](#S4.F20 "Figure 20 ‣
    4.5 The speed of model convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") 10 samples per class.
    [20](#S4.F20 "Figure 20 ‣ 4.5 The speed of model convergence ‣ 4 Experiments ‣
    A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 50 samples per class. [20](#S4.F20 "Figure 20 ‣ 4.5 The speed of model
    convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 100 samples per class.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '图 20: PaviaU 数据集上的训练损失。 [20](#S4.F20 "图 20 ‣ 4.5 模型收敛速度 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本的高光谱图像分类中的应用")
    每类 10 个样本。 [20](#S4.F20 "图 20 ‣ 4.5 模型收敛速度 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本的高光谱图像分类中的应用")
    每类 50 个样本。 [20](#S4.F20 "图 20 ‣ 4.5 模型收敛速度 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本的高光谱图像分类中的应用")
    每类 100 个样本。'
- en: '![Refer to caption](img/ad9c41242f4700eb1518939a53c404e3.png)![Refer to caption](img/4156238aa0edc5095f71ea494659f4a5.png)![Refer
    to caption](img/37243663eecf46a8fe843691f75780b1.png)'
  id: totrans-356
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad9c41242f4700eb1518939a53c404e3.png)![参见说明](img/4156238aa0edc5095f71ea494659f4a5.png)![参见说明](img/37243663eecf46a8fe843691f75780b1.png)'
- en: 'Figure 21: Training Loss on the Salinas data set.[21](#S4.F21 "Figure 21 ‣
    4.5 The speed of model convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") 10 samples per class.
    [21](#S4.F21 "Figure 21 ‣ 4.5 The speed of model convergence ‣ 4 Experiments ‣
    A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 50 samples per class. [21](#S4.F21 "Figure 21 ‣ 4.5 The speed of model
    convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 100 samples per class.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '图 21: Salinas 数据集上的训练损失。 [21](#S4.F21 "图 21 ‣ 4.5 模型收敛速度 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本的高光谱图像分类中的应用")
    每类 10 个样本。 [21](#S4.F21 "图 21 ‣ 4.5 模型收敛速度 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本的高光谱图像分类中的应用")
    每类 50 个样本。 [21](#S4.F21 "图 21 ‣ 4.5 模型收敛速度 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本的高光谱图像分类中的应用")
    每类 100 个样本。'
- en: '![Refer to caption](img/da1df544f757786e0ad4ef451db1f805.png)![Refer to caption](img/e0c9764502ce59980024a6ede1724f62.png)![Refer
    to caption](img/5048fc735bf6485c368c9247b9d1f03a.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/da1df544f757786e0ad4ef451db1f805.png)![参见说明](img/e0c9764502ce59980024a6ede1724f62.png)![参见说明](img/5048fc735bf6485c368c9247b9d1f03a.png)'
- en: 'Figure 22: Training Loss on the KSC data set. [22](#S4.F22 "Figure 22 ‣ 4.5
    The speed of model convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") 10 samples per class. [22](#S4.F22
    "Figure 22 ‣ 4.5 The speed of model convergence ‣ 4 Experiments ‣ A Survey: Deep
    Learning for Hyperspectral Image Classification with Few Labeled Samples") 50
    samples per class. [22](#S4.F22 "Figure 22 ‣ 4.5 The speed of model convergence
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") 100 samples per class.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '图 22: KSC 数据集上的训练损失。 [22](#S4.F22 "图 22 ‣ 4.5 模型收敛速度 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本的高光谱图像分类中的应用")
    每类 10 个样本。 [22](#S4.F22 "图 22 ‣ 4.5 模型收敛速度 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本的高光谱图像分类中的应用")
    每类 50 个样本。 [22](#S4.F22 "图 22 ‣ 4.5 模型收敛速度 ‣ 4 实验 ‣ 调查：深度学习在少量标记样本的高光谱图像分类中的应用")
    每类 100 个样本。'
- en: 5 Conclusions
  id: totrans-360
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: 'In this paper, we introduce the current research difficulties, namely, few
    samples, in the field of hyperspectral image classification and discuss popular
    learning frameworks. Furthermore, we also introduce several popular learning algorithms
    to solve the small-sample problem, such as autoencoders, few-shot learning, transfer
    learning, activate learning, and data augmentation. According to the above methods,
    we select some representative models to conduct experiments on hyperspectral benchmark
    data sets. We developed three different experiments to explore the performance
    of the models on small-sample data sets and documented their changes with increasing
    sample size, finally evaluating their effectiveness and robustness through AA
    and OA. Then, we also compared the number of parameters and convergence speeds
    of various models to further analyze their differences. Ultimately, we also highlight
    several possible future directions of hyperspectral image classification on small
    samples:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们介绍了高光谱图像分类领域当前的研究难点，即样本稀少，并讨论了流行的学习框架。此外，我们还介绍了几种流行的学习算法来解决小样本问题，如自编码器、少样本学习、迁移学习、主动学习和数据增强。根据上述方法，我们选择了一些具有代表性的模型在高光谱基准数据集上进行实验。我们设计了三种不同的实验来探索模型在小样本数据集上的表现，并记录了样本量增加时的变化，最终通过
    AA 和 OA 评估其有效性和鲁棒性。随后，我们还比较了各种模型的参数数量和收敛速度，以进一步分析它们的差异。最终，我们还突出了高光谱图像分类在小样本上的几个可能的未来方向：
- en: '1.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: Autoencoders, including linear autoencoders and 3D convolutional autoencoders,
    have been widely explored and applied to solve the sample problem in HSI. Nevertheless,
    their performance does not approach excellence. The future development trend should
    be focused on few-shot learning, transfer learning, and active learning.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自编码器，包括线性自编码器和 3D 卷积自编码器，已经被广泛探索和应用于解决 HSI 中的样本问题。然而，它们的表现还未达到卓越的水平。未来的发展趋势应集中在少样本学习、迁移学习和主动学习上。
- en: '2.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: We can fuse some learning paradigms to make good use of the advantages of each
    approach. For example, regarding the fusion of transfer learning and active learning,
    such an approach can select the valuable samples on the source data set and transfer
    the model to the target data set to avoid the imbalance of the class sample size.
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以融合一些学习范式，以充分利用每种方法的优势。例如，关于迁移学习和主动学习的融合，这种方法可以在源数据集中选择有价值的样本，并将模型迁移到目标数据集，以避免类别样本大小的不平衡。
- en: '3.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: According to the experimental results, the RNN is also suitable for hyperspectral
    image classification. However, there is little work focused on combining the learning
    paradigms with RNN. Recently, the transformer, as an alternative to the RNN that
    is capable of processing in parallel, has been introduced into the computer vision
    domain and has achieved good performance on some tasks such as object detection.
    Therefore, we can also employ this method in hyperspectral image classification
    and combine it with some learning paradigms.
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 根据实验结果，RNN 也适用于高光谱图像分类。然而，关于将学习范式与 RNN 结合的工作较少。近年来，作为 RNN 的一种并行处理替代方案，transformer
    已被引入计算机视觉领域，并在某些任务如目标检测中取得了良好的表现。因此，我们也可以在高光谱图像分类中应用这种方法，并将其与一些学习范式结合起来。
- en: '4.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: Graph convolution network has been growing more and more interested in hyperspectral
    image classification. Fully connected network, convolution network, and recurrent
    network are just suitable for processing the euclidean data and do not solve with
    the non-euclidean data directly. And image can be regarded as a special case of
    the euclidean-data. Thus, there are many researches [[124](#bib.bib124), [125](#bib.bib125),
    [126](#bib.bib126)] utilizing graph convolution networks to classify HSI.
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图卷积网络在高光谱图像分类中越来越受到关注。全连接网络、卷积网络和递归网络仅适用于处理欧几里得数据，并不能直接处理非欧几里得数据。而图像可以视为欧几里得数据的特殊情况。因此，许多研究[[124](#bib.bib124)、[125](#bib.bib125)、[126](#bib.bib126)]利用图卷积网络对
    HSI 进行分类。
- en: '5.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: The reason for requiring a large amount of label samples is the tremendous trainable
    parameters of the deep learning model. There are many methods proposed, such as
    group convolution [[127](#bib.bib127)], to light the weight of a deep neural network.
    So, how to construct a light-weight model further is also a future direction.
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 需要大量标签样本的原因是深度学习模型中庞大的可训练参数。已有许多方法被提出，例如组卷积[[127](#bib.bib127)]，以减轻深度神经网络的权重。因此，如何进一步构建轻量级模型也是未来的一个方向。
- en: Although few label classification can save much time and labor force to collect
    and label diverse samples, the models are easy to suffer from over-fit and gaining
    a weak generalization. Thus, how to avoid the over-fitting and improve model’s
    generalization is the huge challenge of HSI few label classification in the application
    potential.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管少量标签分类可以节省大量时间和劳动来收集和标记不同样本，但模型容易遭受过拟合并获得较弱的泛化能力。因此，如何避免过拟合并提高模型的泛化能力是超光谱图像（HSI）少标签分类应用潜力中的巨大挑战。
- en: Acknowledgments
  id: totrans-373
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The work is partly supported by the National Natural Science Foundation of China
    (Grant No. 61976144).
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 该项工作部分由中国国家自然科学基金（资助号61976144）支持。
- en: References
  id: totrans-375
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Teke, H. Deveci, O. Haliloğlu, S. Gürbüz, U. Sakarya, A short survey
    of hyperspectral remote sensing applications in agriculture, in: 2013 6th International
    Conference on Recent Advances in Space Technologies (RAST), IEEE, 2013, pp. 171–176.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Teke, H. Deveci, O. Haliloğlu, S. Gürbüz, U. Sakarya, 超光谱遥感在农业中的应用简述,
    载于：2013年第六届空间技术近期进展国际会议（RAST），IEEE，2013，第171–176页。'
- en: '[2] I. Strachan, E. Pattey, J. Boisvert, Impact of nitrogen and environmental
    conditions on corn as detected by hyperspectral reflectance, Remote Sens. Environ.
    80 (2) (2002) 213–224.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] I. Strachan, E. Pattey, J. Boisvert, 氮和环境条件对玉米的影响通过超光谱反射率检测, 遥感环境 80 (2)
    (2002) 213–224。'
- en: '[3] A. Bannari, A. Pacheco, K. Staenz, H. McNairn, K. Omari, Estimating and
    mapping crop residues cover on agricultural lands using hyperspectral and ikonos
    data, Remote Sens Environ 104 (4) (2006) 447–459.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Bannari, A. Pacheco, K. Staenz, H. McNairn, K. Omari, 利用超光谱和IKONOS数据估算和映射农业用地上的作物残留物覆盖,
    遥感环境 104 (4) (2006) 447–459。'
- en: '[4] C. Sabine, M. Robert, S. Thomas, R. Manuel, E. Paula, P. Marta, P. Alicia,
    Potential of hyperspectral imagery for the spatial assessment of soil erosion
    stages in agricultural semi-arid spain at different scales, in: 2014 IEEE Geoscience
    and Remote Sensing Symposium, IEEE, 2014, pp. 2918–2921.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] C. Sabine, M. Robert, S. Thomas, R. Manuel, E. Paula, P. Marta, P. Alicia,
    超光谱影像在不同尺度下对西班牙半干旱农业区土壤侵蚀阶段的空间评估潜力, 载于：2014 IEEE地球科学与遥感研讨会，IEEE，2014，第2918–2921页。'
- en: '[5] P. Kuflik, S. Rotman, Band selection for gas detection in hyperspectral
    images, in: 2012 IEEE 27th Convention of Electrical and Electronics Engineers
    in Israel, 2012, pp. 1–4. [doi:10.1109/EEEI.2012.6376973](https://doi.org/10.1109/EEEI.2012.6376973).'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] P. Kuflik, S. Rotman, 超光谱图像中气体检测的波段选择, 载于：2012 IEEE第27届以色列电气与电子工程师大会，2012，第1–4页。[doi:10.1109/EEEI.2012.6376973](https://doi.org/10.1109/EEEI.2012.6376973)。'
- en: '[6] S. Foudan, K. Menas, E. Tarek, G. Richard, Y. Ruixin, Hyperspectral image
    analysis for oil spill detection, in: Summaries of NASA/JPL Airborne Earth Science
    Workshop, Pasadena, CA, 2001, pp. 5–9.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] S. Foudan, K. Menas, E. Tarek, G. Richard, Y. Ruixin, 超光谱图像分析用于石油泄漏检测,
    载于：NASA/JPL空中地球科学研讨会总结，帕萨迪纳，加州，2001，第5–9页。'
- en: '[7] A. Mohamad, Sea water chlorophyll-a estimation using hyperspectral images
    and supervised artificial neural network, Ecol Inf 24 (2014) 60–68. [doi:10.1016/j.ecoinf.2014.07.004](https://doi.org/10.1016/j.ecoinf.2014.07.004).'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] A. Mohamad, 使用超光谱图像和监督人工神经网络估算海水叶绿素a, 生态信息 24 (2014) 60–68。[doi:10.1016/j.ecoinf.2014.07.004](https://doi.org/10.1016/j.ecoinf.2014.07.004)。'
- en: '[8] J. Sylvain, G. Mireille, A novel maximum likelihood based method for mapping
    depth and water quality from hyperspectral remote-sensing data, Remote Sens Environ
    147 (2014) 121–132. [doi:10.1016/j.rse.2014.01.026](https://doi.org/10.1016/j.rse.2014.01.026).'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] J. Sylvain, G. Mireille, 一种基于最大似然的超光谱遥感数据深度和水质映射新方法, 遥感环境 147 (2014) 121–132。[doi:10.1016/j.rse.2014.01.026](https://doi.org/10.1016/j.rse.2014.01.026)。'
- en: '[9] C. Jänicke, A. Okujeni, S. Cooper, M. Clark, P. Hostert, S. van der Linden,
    Brightness gradient-corrected hyperspectral image mosaics for fractional vegetation
    cover mapping in northern california, Remote Sensing Letters 11 (1) (2020) 1–10.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] C. Jänicke, A. Okujeni, S. Cooper, M. Clark, P. Hostert, S. van der Linden,
    明度梯度修正的超光谱图像马赛克用于加州北部植被覆盖率映射, 遥感快报 11 (1) (2020) 1–10。'
- en: '[10] J. Li, Y. Pang, Z. Li, W. Jia, Tree species classification of airborne
    hyperspectral image in cloud shadow area, in: International Symposium of Space
    Optical Instrument and Application, Springer, 2018, pp. 389–398.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] J. Li, Y. Pang, Z. Li, W. Jia, 云影区域航空高光谱图像的树种分类, 在：国际空间光学仪器与应用研讨会，Springer，2018
    年，第 389–398 页。'
- en: '[11] Z. Du, M. Jeong, S. Kong, Band selection of hyperspectral images for automatic
    detection of poultry skin tumors, IEEE Transactions on Automation Science and
    Engineering 4 (3) (2007) 332–339. [doi:10.1109/TASE.2006.888048](https://doi.org/10.1109/TASE.2006.888048).'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Z. Du, M. Jeong, S. Kong, 高光谱图像的波段选择用于自动检测禽类皮肤肿瘤, IEEE Transactions on
    Automation Science and Engineering 4 (3) (2007) 332–339. [doi:10.1109/TASE.2006.888048](https://doi.org/10.1109/TASE.2006.888048)。'
- en: '[12] S. Li, W. Song, L. Fang, Y. Chen, J. Benediktsson, Deep learning for hyperspectral
    image classification: An overview, IEEE Transactions on Geoscience and Remote
    Sensing PP (99) (2019) 1–20.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] S. Li, W. Song, L. Fang, Y. Chen, J. Benediktsson, 深度学习在高光谱图像分类中的应用概述,
    IEEE Transactions on Geoscience and Remote Sensing PP (99) (2019) 1–20.'
- en: '[13] A. Plaza, J. Plaza, G. Martin, Incorporation of spatial constraints into
    spectral mixture analysis of remotely sensed hyperspectral data, Machine Learning
    for Signal Processing .mlsp .ieee International Workshop on (2009) 1 – 6.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. Plaza, J. Plaza, G. Martin, 将空间约束引入遥感高光谱数据的光谱混合分析中, 机器学习信号处理 .mlsp
    .ieee 国际研讨会（2009 年）第 1 – 6 页。'
- en: '[14] F. Melgani, L. Bruzzone, Classification of hyperspectral remote sensing
    images with support vector machines, IEEE Transactions on Geoence and Remote Sensing
    42 (8) (2004) 1778–1790. [doi:10.1109/TGRS.2004.831865](https://doi.org/10.1109/TGRS.2004.831865).'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] F. Melgani, L. Bruzzone, 使用支持向量机对高光谱遥感图像进行分类, IEEE Transactions on Geoence
    and Remote Sensing 42 (8) (2004) 1778–1790. [doi:10.1109/TGRS.2004.831865](https://doi.org/10.1109/TGRS.2004.831865)。'
- en: '[15] Y. Zhong, L. Zhang, An adaptive artificial immune network for supervised
    classification of multi-/hyperspectral remote sensing imagery, IEEE Trans. Geosci.
    Remote Sens. 50 (3) (2011) 894–909.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. Zhong, L. Zhang, 一种用于监督分类多光谱/高光谱遥感影像的自适应人工免疫网络, IEEE Trans. Geosci.
    Remote Sens. 50 (3) (2011) 894–909.'
- en: '[16] J. Li, J. Bioucas-Dias, A. Plaza, Semisupervised hyperspectral image classification
    using soft sparse multinomial logistic regression, IEEE Geosci. Remote Sens. Lett.
    10 (2) (2012) 318–322.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] J. Li, J. Bioucas-Dias, A. Plaza, 使用软稀疏多项式逻辑回归进行半监督高光谱图像分类, IEEE Geosci.
    Remote Sens. Lett. 10 (2) (2012) 318–322.'
- en: '[17] G. Licciardi, P. Marpu, J. Chanussot, J. Benediktsson, Linear versus nonlinear
    pca for the classification of hyperspectral data based on the extended morphological
    profiles, IEEE Geosci. Remote Sens. Lett. 9 (3) (2011) 447–451.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] G. Licciardi, P. Marpu, J. Chanussot, J. Benediktsson, 基于扩展形态学轮廓的线性与非线性主成分分析在高光谱数据分类中的比较,
    IEEE Geosci. Remote Sens. Lett. 9 (3) (2011) 447–451.'
- en: '[18] A. Villa, J. Chanussot, C. Jutten, J. Benediktsson, S. Moussaoui, On the
    use of ICA for hyperspectral image analysis, in: Proc. Geoscience and Remote Sensing
    Symp.,2009 IEEE Int.,IGARSS 2009, Vol. 4, 2009, pp. IV–97. [doi:10.1109/IGARSS.2009.5417363](https://doi.org/10.1109/IGARSS.2009.5417363).'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. Villa, J. Chanussot, C. Jutten, J. Benediktsson, S. Moussaoui, 关于 ICA
    在高光谱图像分析中的应用, 在：2009 IEEE 国际地球科学与遥感研讨会（IGARSS 2009）会议记录，卷 4，2009 年，第 IV–97 页。
    [doi:10.1109/IGARSS.2009.5417363](https://doi.org/10.1109/IGARSS.2009.5417363)。'
- en: '[19] C. Zhang, Y. Zheng, Hyperspectral remote sensing image classification
    based on combined SVM and LDA, in: SPIE Asia Pacific Remote Sensing, 2014, p.
    92632P.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] C. Zhang, Y. Zheng, 基于 SVM 和 LDA 组合的高光谱遥感图像分类, 在：SPIE 亚太遥感，2014 年，第 92632P
    页。'
- en: '[20] L. He, J. Li, A. Plaza, Y. Li, Discriminative low-rank Gabor filtering
    for spectral-spatial hyperspectral image classification, IEEE Transactions on
    Geoence and Remote Sensing PP (99) (2016) 1–15. [doi:10.1109/TGRS.2016.2623742](https://doi.org/10.1109/TGRS.2016.2623742).'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] L. He, J. Li, A. Plaza, Y. Li, 用于光谱空间高光谱图像分类的判别低秩 Gabor 滤波, IEEE Transactions
    on Geoence and Remote Sensing PP (99) (2016) 1–15. [doi:10.1109/TGRS.2016.2623742](https://doi.org/10.1109/TGRS.2016.2623742)。'
- en: '[21] M. D. Mura, J. A. Benediktsson, B. Waske, L. Bruzzone, Extended profiles
    with morphological attribute filters for the analysis of hyperspectral data, Int.
    J. Remote Sens. 31 (22) (2010) 5975–5991.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] M. D. Mura, J. A. Benediktsson, B. Waske, L. Bruzzone, 用于高光谱数据分析的扩展轮廓与形态学属性滤波器,
    Int. J. Remote Sens. 31 (22) (2010) 5975–5991.'
- en: '[22] N. Falco, J. Atli Benediktsson, L. Bruzzone, Spectral and spatial classification
    of hyperspectral images based on ICA and reduced morphological attribute profiles,
    IEEE Transactions on Geoscience and Remote Sensing 53 (11) (2015) 6223–6240.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] N. Falco, J. Atli Benediktsson, L. Bruzzone, 基于 ICA 和缩减形态学属性轮廓的高光谱图像光谱和空间分类,
    IEEE Transactions on Geoscience and Remote Sensing 53 (11) (2015) 6223–6240.'
- en: '[23] M. Dalla Mura, A. Villa, J. Atli Benediktsson, J. Chanussot, L. Bruzzone,
    Classification of hyperspectral images by using extended morphological attribute
    profiles and independent component analysis, IEEE Geoscience and Remote Sensing
    Letters 8 (3) (2011) 542–546.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] M. Dalla Mura, A. Villa, J. Atli Benediktsson, J. Chanussot, L. Bruzzone,
    使用扩展形态学属性轮廓和独立成分分析对高光谱图像进行分类，IEEE Geoscience and Remote Sensing Letters 8 (3)
    (2011) 542–546。'
- en: '[24] S. Jia, L. Shen, Q. Li, Gabor feature-based collaborative representation
    for hyperspectral imagery classification, IEEE Transactions on Geoscience and
    Remote Sensing 53 (2) (2015) 1118–1129.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] S. Jia, L. Shen, Q. Li, 基于Gabor特征的高光谱影像分类协作表示，IEEE Transactions on Geoscience
    and Remote Sensing 53 (2) (2015) 1118–1129。'
- en: '[25] Y. Qian, M. Ye, J. Zhou, Hyperspectral image classification based on structured
    sparse logistic regression and three-dimensional wavelet texture features, IEEE
    Transactions on Geoscience and Remote Sensing 51 (4) (2013) 2276–2291.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Y. Qian, M. Ye, J. Zhou, 基于结构稀疏逻辑回归和三维小波纹理特征的高光谱图像分类，IEEE Transactions
    on Geoscience and Remote Sensing 51 (4) (2013) 2276–2291。'
- en: '[26] W. Li, C. Chen, H. Su, Q. Du, Local binary patterns and extreme learning
    machine for hyperspectral imagery classification, IEEE Trans Geosci Remote Sens
    53 (7) (2015) 3681–3693.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] W. Li, C. Chen, H. Su, Q. Du, 本地二值模式和极限学习机在高光谱影像分类中的应用，IEEE Trans Geosci
    Remote Sens 53 (7) (2015) 3681–3693。'
- en: '[27] P. Ghamisi, M. Dalla Mura, J. Atli Benediktsson, A survey on spectral–spatial
    classification techniques based on attribute profiles, IEEE Transactions on Geoscience
    and Remote Sensing 53 (5) (2015) 2335–2353.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] P. Ghamisi, M. Dalla Mura, J. Atli Benediktsson, 基于属性轮廓的光谱-空间分类技术综述，IEEE
    Transactions on Geoscience and Remote Sensing 53 (5) (2015) 2335–2353。'
- en: '[28] S. Yu, S. Jia, C. Xu, Convolutional neural networks for hyperspectral
    image classification, Neurocomputing 219 (2017) 88–98.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] S. Yu, S. Jia, C. Xu, 卷积神经网络在高光谱图像分类中的应用，Neurocomputing 219 (2017) 88–98。'
- en: '[29] M. Paoletti, J. Haut, J. Plaza, A. Plaza, Deep learning classifiers for
    hyperspectral imaging: A review, ISPRS J. Photogramm. Remote Sens. 158 (Dec.)
    (2019) 279–317.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. Paoletti, J. Haut, J. Plaza, A. Plaza, 深度学习分类器在高光谱成像中的应用综述，ISPRS J.
    Photogramm. Remote Sens. 158 (12月) (2019) 279–317。'
- en: '[30] G. Hinton, R. Salakhutdinov, Reducing the dimensionality of data with
    neural networks, science 313 (5786) (2006) 504–507.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] G. Hinton, R. Salakhutdinov, 使用神经网络减少数据的维度，science 313 (5786) (2006) 504–507。'
- en: '[31] A. Coates, A. Ng, H. Lee, An analysis of single-layer networks in unsupervised
    feature learning, Journal of Machine Learning Research 15 (2011) 215–223.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] A. Coates, A. Ng, H. Lee, 单层网络在无监督特征学习中的分析，Journal of Machine Learning
    Research 15 (2011) 215–223。'
- en: '[32] P. Vincent, H. Larochelle, Y. Bengio, P. Manzagol, Extracting and composing
    robust features with denoising autoencoders, in: International Conference on Machine
    Learning, 2008, pp. 1096–1103.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] P. Vincent, H. Larochelle, Y. Bengio, P. Manzagol, 使用去噪自编码器提取和组合鲁棒特征，国际机器学习会议，2008年，页1096–1103。'
- en: '[33] L. Windrim, R. Ramakrishnan, A. Melkumyan, R. Murphy, A. Chlingaryan,
    Unsupervised feature-learning for hyperspectral data with autoencoders, Remote
    Sensing 11 (7) (2019) 864.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] L. Windrim, R. Ramakrishnan, A. Melkumyan, R. Murphy, A. Chlingaryan,
    使用自编码器进行高光谱数据的无监督特征学习，Remote Sensing 11 (7) (2019) 864。'
- en: '[34] Y. Chen, Z. Lin, X. Zhao, G. Wang, Y. Gu, Deep learning-based classification
    of hyperspectral data, IEEE J Sel Topics Appl Earth Observ Remote Sens 7 (6) (2014)
    2094–2107.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Y. Chen, Z. Lin, X. Zhao, G. Wang, Y. Gu, 基于深度学习的高光谱数据分类，IEEE J Sel Topics
    Appl Earth Observ Remote Sens 7 (6) (2014) 2094–2107。'
- en: '[35] A. Ghasem, S. Farhad, R. Peter, Spectral–spatial feature learning for
    hyperspectral imagery classification using deep stacked sparse autoencoder, J.
    Appl. Remote Sens. 11 (4) (2017) 042604.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] A. Ghasem, S. Farhad, R. Peter, 使用深度堆叠稀疏自编码器进行高光谱影像分类的光谱-空间特征学习，J. Appl.
    Remote Sens. 11 (4) (2017) 042604。'
- en: '[36] C. Xing, L. Ma, X. Yang, Stacked denoise autoencoder based feature extraction
    and classification for hyperspectral images, Journal of Sensors 2016 (2016).'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C. Xing, L. Ma, X. Yang, 基于堆叠去噪自编码器的特征提取与高光谱图像分类，Journal of Sensors 2016
    (2016)。'
- en: '[37] J. Yue, S. Mao, M. Li, A deep learning framework for hyperspectral image
    classification using spatial pyramid pooling, Remote Sensing Letters 7 (9) (2016)
    875–884.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Yue, S. Mao, M. Li, 使用空间金字塔池化的高光谱图像分类深度学习框架，Remote Sensing Letters
    7 (9) (2016) 875–884。'
- en: '[38] S. Hao, W. Wang, Y. Ye, T. Nie, B. Lorenzo, Two-stream deep architecture
    for hyperspectral image classification, IEEE Trans. Geosci. Remote Sens. 56 (4)
    (2017) 2349–2361.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] S. Hao, W. Wang, Y. Ye, T. Nie, B. Lorenzo, 用于高光谱图像分类的双流深度架构，IEEE Trans.
    Geosci. Remote Sens. 56 (4) (2017) 2349–2361。'
- en: '[39] X. Sun, F. Zhou, J. Dong, F. Gao, Q. Mu, X. Wang, Encoding spectral and
    spatial context information for hyperspectral image classification, IEEE Geosci.
    Remote Sens. Lett. 14 (12) (2017) 2250–2254.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] X. Sun, F. Zhou, J. Dong, F. Gao, Q. Mu, X. Wang, 为高光谱图像分类编码光谱和空间上下文信息，《IEEE
    地球科学与遥感通讯》14(12)（2017年）2250–2254。'
- en: '[40] S. Mei, J. Ji, Y. Geng, Z. Zhang, X. Li, Q. Du, Unsupervised spatial–spectral
    feature learning by 3d convolutional autoencoder for hyperspectral classification,
    IEEE Trans. Geosci. Remote Sens. 57 (9) (2019) 6808–6820.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] S. Mei, J. Ji, Y. Geng, Z. Zhang, X. Li, Q. Du, 通过3D卷积自编码器进行无监督空间-光谱特征学习的高光谱分类，《IEEE
    地球科学与遥感学报》57(9)（2019年）6808–6820。'
- en: '[41] C. Zhao, X. Wan, G. Zhao, B. Cui, W. Liu, B. Qi, Spectral-spatial classification
    of hyperspectral imagery based on stacked sparse autoencoder and random forest,
    European journal of remote sensing 50 (1) (2017) 47–63.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] C. Zhao, X. Wan, G. Zhao, B. Cui, W. Liu, B. Qi, 基于堆叠稀疏自编码器和随机森林的高光谱图像光谱-空间分类，《欧洲遥感杂志》50(1)（2017年）47–63。'
- en: '[42] X. Wan, C. Zhao, Y. Wang, W. Liu, Stacked sparse autoencoder in hyperspectral
    data classification using spectral-spatial, higher order statistics and multifractal
    spectrum features, Infrared Physics & Technology 86 (2017) 77–89.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] X. Wan, C. Zhao, Y. Wang, W. Liu, 在高光谱数据分类中使用光谱-空间、阶数统计和多分形谱特征的堆叠稀疏自编码器，《红外物理与技术》86（2017年）77–89。'
- en: '[43] C. Wang, P. Zhang, Y. Zhang, L. Zhang, W. Wei, A multi-label hyperspectral
    image classification method with deep learning features, in: Proceedings of the
    International Conference on Internet Multimedia Computing and Service, 2016, pp.
    127–131.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] C. Wang, P. Zhang, Y. Zhang, L. Zhang, W. Wei, 一种基于深度学习特征的多标签高光谱图像分类方法，收录于：国际互联网多媒体计算与服务会议论文集，2016年，页127–131。'
- en: '[44] J. Li, B. Lorenzo, S. Liu, Deep feature representation for hyperspectral
    image classification, in: 2015 IEEE International Geoscience and Remote Sensing
    Symposium (IGARSS), IEEE, 2015, pp. 4951–4954.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] J. Li, B. Lorenzo, S. Liu, 高光谱图像分类的深度特征表示，收录于：2015 IEEE 国际地球科学与遥感研讨会（IGARSS），IEEE，2015年，页4951–4954。'
- en: '[45] M. Atif, L. Tao, Efficient deep auto-encoder learning for the classification
    of hyperspectral images, in: 2016 International Conference on Virtual Reality
    and Visualization (ICVRV), IEEE, 2016, pp. 44–51.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] M. Atif, L. Tao, 高效的深度自编码器学习用于高光谱图像分类，收录于：2016 国际虚拟现实与可视化会议（ICVRV），IEEE，2016年，页44–51。'
- en: '[46] Y. Liu, G. Cao, Q. Sun, S. Mel, Hyperspectral classification via learnt
    features, in: 2015 IEEE International Conference on Image Processing (ICIP), IEEE,
    2015, pp. 2591–2595.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Y. Liu, G. Cao, Q. Sun, S. Mel, 基于学习特征的高光谱分类，收录于：2015 IEEE 国际图像处理会议（ICIP），IEEE，2015年，页2591–2595。'
- en: '[47] H. Lee, K. Heesung, Going deeper with contextual cnn for hyperspectral
    image classification, IEEE Trans. Image Process. 26 (10) (2017) 4843–4855.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] H. Lee, K. Heesung, 使用上下文 CNN 进行高光谱图像分类的深入研究，《IEEE 图像处理学报》26(10)（2017年）4843–4855。'
- en: '[48] J. Leng, T. Li, G. Bai, Q. Dong, H. Dong, Cube-cnn-svm: a novel hyperspectral
    image classification method, in: 2016 IEEE 28th International Conference on Tools
    with Artificial Intelligence (ICTAI), IEEE, 2016, pp. 1027–1034.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] J. Leng, T. Li, G. Bai, Q. Dong, H. Dong, Cube-cnn-svm：一种新型高光谱图像分类方法，收录于：2016
    IEEE 第28届人工智能工具国际会议（ICTAI），IEEE，2016年，页1027–1034。'
- en: '[49] H. Zhang, Y. Li, Y. Zhang, Q. Shen, Spectral-spatial classification of
    hyperspectral imagery using a dual-channel convolutional neural network, Remote
    Sensing Letters 8 (5) (2017) 438–447.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] H. Zhang, Y. Li, Y. Zhang, Q. Shen, 使用双通道卷积神经网络进行高光谱图像的光谱-空间分类，《遥感通讯》8(5)（2017年）438–447。'
- en: '[50] E. Aptoula, M. Ozdemir, B. Yanikoglu, Deep learning with attribute profiles
    for hyperspectral image classification, IEEE Geosci. Remote Sens. Lett. 13 (12)
    (2016) 1970–1974.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] E. Aptoula, M. Ozdemir, B. Yanikoglu, 使用属性配置文件进行高光谱图像分类的深度学习，《IEEE 地球科学与遥感通讯》13(12)（2016年）1970–1974。'
- en: '[51] W. Zhao, S. Li, A. Li, B. Zhang, Y. Li, Hyperspectral images classification
    with convolutional neural network and textural feature using limited training
    samples, Remote Sensing Letters 10 (5) (2019) 449–458.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] W. Zhao, S. Li, A. Li, B. Zhang, Y. Li, 使用有限训练样本的卷积神经网络和纹理特征进行高光谱图像分类，《遥感通讯》10(5)（2019年）449–458。'
- en: '[52] C. Yu, M. Zhao, M. Song, Y. Wang, F. Li, R. Han, C. Chang, Hyperspectral
    image classification method based on cnn architecture embedding with hashing semantic
    feature, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 12 (6) (2019) 1866–1881.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] C. Yu, M. Zhao, M. Song, Y. Wang, F. Li, R. Han, C. Chang, 基于嵌入哈希语义特征的
    CNN 结构的高光谱图像分类方法，《IEEE 选择性主题应用地球观测与遥感杂志》12(6)（2019年）1866–1881。'
- en: '[53] C. Qing, J. Ruan, X. Xu, J. Ren, J. Zabalza, Spatial-spectral classification
    of hyperspectral images: a deep learning framework with markov random fields based
    modelling, IET Image Proc. 13 (2) (2018) 235–245.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] C. Qing, J. Ruan, X. Xu, J. Ren, J. Zabalza, 高光谱图像的空间-光谱分类：一个基于马尔可夫随机场建模的深度学习框架，IET
    图像处理 13 (2) (2018) 235–245。'
- en: '[54] Z. Zhong, J. Li, Z. Luo, M. Chapman, Spectral-spatial residual network
    for hyperspectral image classification: A 3-d deep learning framework, IEEE Trans.
    Geosci. Remote Sens. 56 (2) (2017) 847–858.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Z. Zhong, J. Li, Z. Luo, M. Chapman, 用于高光谱图像分类的光谱–空间残差网络：一个 3-d 深度学习框架，IEEE
    地球科学与遥感学报 56 (2) (2017) 847–858。'
- en: '[55] B. Liu, X. Yu, P. Zhang, X. Tan, R. Wang, L. Zhi, Spectral–spatial classification
    of hyperspectral image using three-dimensional convolution network, J. Appl. Remote
    Sens. 12 (1) (2018) 016005.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] B. Liu, X. Yu, P. Zhang, X. Tan, R. Wang, L. Zhi, 使用三维卷积网络的高光谱图像的光谱–空间分类，J.
    Appl. Remote Sens. 12 (1) (2018) 016005。'
- en: '[56] B. Fang, Y. Li, H. Zhang, J. Chan, Collaborative learning of lightweight
    convolutional neural network and deep clustering for hyperspectral image semi-supervised
    classification with limited training samples, ISPRS J Photogramm Remote Sens 161
    (2020) 164–178.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] B. Fang, Y. Li, H. Zhang, J. Chan, 轻量级卷积神经网络与深度聚类的协作学习，用于有限训练样本的高光谱图像半监督分类，ISPRS
    J Photogramm Remote Sens 161 (2020) 164–178。'
- en: '[57] L. Mou, P. Ghamisi, X. Zhu, Unsupervised spectral–spatial feature learning
    via deep residual conv–deconv network for hyperspectral image classification,
    IEEE Trans. Geosci. Remote Sens. 56 (1) (2017) 391–406.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] L. Mou, P. Ghamisi, X. Zhu, 通过深度残差卷积–解卷积网络进行无监督光谱–空间特征学习，用于高光谱图像分类，IEEE
    地球科学与遥感学报 56 (1) (2017) 391–406。'
- en: '[58] A. Sellami, M. Farah, I. Farah, B. Solaiman, Hyperspectral imagery classification
    based on semi-supervised 3-d deep neural network and adaptive band selection,
    Expert Syst. Appl. 129 (2019) 246–259.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Sellami, M. Farah, I. Farah, B. Solaiman, 基于半监督 3-d 深度神经网络和自适应带选择的高光谱图像分类，Expert
    Syst. Appl. 129 (2019) 246–259。'
- en: '[59] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770–778.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] K. He, X. Zhang, S. Ren, J. Sun, 图像识别的深度残差学习，见：Proc. IEEE Conf. Comput.
    Vis. Pattern Recognit., 2016，第 770–778 页。'
- en: '[60] M. Paoletti, J. Haut, R. Fernandez-Beltran, J. Plaza, A. Plaza, F. Pla,
    Deep pyramidal residual networks for spectral–spatial hyperspectral image classification,
    IEEE Transactions on Geoscience and Remote Sensing 57 (2) (2018) 740–754.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] M. Paoletti, J. Haut, R. Fernandez-Beltran, J. Plaza, A. Plaza, F. Pla,
    用于光谱–空间高光谱图像分类的深度金字塔残差网络，IEEE 地球科学与遥感学报 57 (2) (2018) 740–754。'
- en: '[61] X. Ma, A. Fu, J. Wang, H. Wang, B. Yin, Hyperspectral image classification
    based on deep deconvolution network with skip architecture, IEEE Trans. Geosci.
    Remote Sens. 56 (8) (2018) 4781–4791.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] X. Ma, A. Fu, J. Wang, H. Wang, B. Yin, 基于跳跃架构的深度解卷积网络的高光谱图像分类，IEEE 地球科学与遥感学报
    56 (8) (2018) 4781–4791。'
- en: '[62] M. Paoletti, J. Haut, J. Plaza, A. Plaza, Deep&dense convolutional neural
    network for hyperspectral image classification, Remote Sensing 10 (9) (2018) 1454.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] M. Paoletti, J. Haut, J. Plaza, A. Plaza, 用于高光谱图像分类的深度&稠密卷积神经网络，Remote
    Sensing 10 (9) (2018) 1454。'
- en: '[63] W. Wang, S. Dou, Z. Jiang, L. Sun, A fast dense spectral–spatial convolution
    network framework for hyperspectral images classification, Remote Sensing 10 (7)
    (2018) 1068.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] W. Wang, S. Dou, Z. Jiang, L. Sun, 高光谱图像分类的快速稠密光谱–空间卷积网络框架，Remote Sensing
    10 (7) (2018) 1068。'
- en: '[64] J. Haut, M. Paoletti, J. Plaza, A. Plaza, J. Li, Visual attention-driven
    hyperspectral image classification, IEEE Transactions on Geoscience and Remote
    Sensing 57 (10) (2019) 8065–8080.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Haut, M. Paoletti, J. Plaza, A. Plaza, J. Li, 基于视觉注意力的高光谱图像分类，IEEE
    地球科学与遥感学报 57 (10) (2019) 8065–8080。'
- en: '[65] Z. Xiong, Y. Yuan, Q. Wang, Ai-net: attention inception neural networks
    for hyperspectral image classification, in: IGARSS 2018-2018 IEEE International
    Geoscience and Remote Sensing Symposium, IEEE, 2018, pp. 2647–2650.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Z. Xiong, Y. Yuan, Q. Wang, Ai-net：用于高光谱图像分类的注意力 inception 神经网络，见：IGARSS
    2018-2018 IEEE 国际地球科学与遥感学会年会，IEEE，2018，第 2647–2650 页。'
- en: '[66] Q. Feng, D. Zhu, J. Yang, B. Li, Multisource hyperspectral and lidar data
    fusion for urban land-use mapping based on a modified two-branch convolutional
    neural network, ISPRS International Journal of Geo-Information 8 (1) (2019) 28.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Q. Feng, D. Zhu, J. Yang, B. Li, 基于改进的双分支卷积神经网络的城市土地利用制图的多源高光谱和激光雷达数据融合，ISPRS
    国际地理信息期刊 8 (1) (2019) 28。'
- en: '[67] X. Xu, W. Li, Q. Ran, Q. Du, L. Gao, B. Zhang, Multisource remote sensing
    data classification based on convolutional neural network, IEEE Transactions on
    Geoscience and Remote Sensing 56 (2) (2017) 937–949.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] X. Xu, W. Li, Q. Ran, Q. Du, L. Gao, B. Zhang, 基于卷积神经网络的多源遥感数据分类，《IEEE地球科学与遥感学报》56
    (2) (2017) 937–949。'
- en: '[68] H. Li, G. Pedram, S. Uwe, X. Zhu, Hyperspectral and lidar fusion using
    deep three-stream convolutional neural networks, Remote Sensing 10 (10) (2018)
    1649.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] H. Li, G. Pedram, S. Uwe, X. Zhu, 使用深度三流卷积神经网络的高光谱与激光雷达融合，《遥感》10 (10)
    (2018) 1649。'
- en: '[69] W. Li, C. Chen, M. Zhang, H. Li, Q. Du, Data augmentation for hyperspectral
    image classification with deep cnn, IEEE Geoscience and Remote Sensing Letters
    16 (4) (2018) 593–597.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] W. Li, C. Chen, M. Zhang, H. Li, Q. Du, 基于深度CNN的高光谱图像分类的数据增强，《IEEE地球科学与遥感学报快报》16
    (4) (2018) 593–597。'
- en: '[70] W. Wei, J. Zhang, L. Zhang, C. Tian, Y. Zhang, Deep cube-pair network
    for hyperspectral imagery classification, Remote Sensing 10 (5) (2018) 783.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] W. Wei, J. Zhang, L. Zhang, C. Tian, Y. Zhang, 用于高光谱图像分类的深度立方对网络，《遥感》10
    (5) (2018) 783。'
- en: '[71] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation
    9 (8) (1997) 1735–1780.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] S. Hochreiter, J. Schmidhuber, 长短期记忆，《神经计算》9 (8) (1997) 1735–1780。'
- en: '[72] C. Kyunghyun, V. Bart, G. Caglar, B. Dzmitry, B. Fethi, S. Holger, B. Yoshua,
    Learning phrase representations using rnn encoder-decoder for statistical machine
    translation, arXiv preprint arXiv:1406.1078 (2014).'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] C. Kyunghyun, V. Bart, G. Caglar, B. Dzmitry, B. Fethi, S. Holger, B.
    Yoshua, 使用RNN编码器-解码器进行短语表示学习的统计机器翻译，《arXiv预印本》arXiv:1406.1078 (2014)。'
- en: '[73] L. Mou, P. Ghamisi, X. Zhu, Deep recurrent neural networks for hyperspectral
    image classification, IEEE Trans. Geosci. Remote Sens. 55 (7) (2017) 3639–3655.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] L. Mou, P. Ghamisi, X. Zhu, 深度递归神经网络用于高光谱图像分类，《IEEE地球科学与遥感学报》55 (7) (2017)
    3639–3655。'
- en: '[74] B. Liu, X. Yu, A. Yu, P. Zhang, G. Wan, Spectral-spatial classification
    of hyperspectral imagery based on recurrent neural networks, Remote Sensing Letters
    9 (12) (2018) 1118–1127.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] B. Liu, X. Yu, A. Yu, P. Zhang, G. Wan, 基于递归神经网络的高光谱图像光谱空间分类，《遥感快报》9 (12)
    (2018) 1118–1127。'
- en: '[75] F. Zhou, R. Hang, Q. Liu, X. Yuan, Hyperspectral image classification
    using spectral-spatial lstms, Neurocomputing 328 (2019) 39–47.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] F. Zhou, R. Hang, Q. Liu, X. Yuan, 使用光谱空间LSTM进行高光谱图像分类，《神经计算》328 (2019)
    39–47。'
- en: '[76] M. Andong, F. A. M, Z. Wang, Z. Yin, Hyperspectral image classification
    using similarity measurements-based deep recurrent neural networks, Remote Sensing
    11 (2) (2019) 194.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] M. Andong, F. A. M, Z. Wang, Z. Yin, 使用基于相似性测量的深度递归神经网络进行高光谱图像分类，《遥感》11
    (2) (2019) 194。'
- en: '[77] X. Zhang, Y. Sun, K. Jiang, C. Li, L. Jiao, H. Zhou, Spatial sequential
    recurrent neural network for hyperspectral image classification, IEEE J. Sel.
    Topics Appl. Earth Observ. Remote Sens. 11 (11) (2018) 4141–4155.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] X. Zhang, Y. Sun, K. Jiang, C. Li, L. Jiao, H. Zhou, 用于高光谱图像分类的空间序列递归神经网络，《IEEE选择主题应用地球观测遥感学报》11
    (11) (2018) 4141–4155。'
- en: '[78] E. Pan, X. Mei, Q. Wang, Y. Ma, J. Ma, Spectral-spatial classification
    for hyperspectral image based on a single gru, Neurocomputing 387 (2020) 150–160.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] E. Pan, X. Mei, Q. Wang, Y. Ma, J. Ma, 基于单一GRU的高光谱图像光谱空间分类，《神经计算》387 (2020)
    150–160。'
- en: '[79] H. Wu, P. Saurabh, Semi-supervised deep learning using pseudo labels for
    hyperspectral image classification, IEEE Trans. Image Process. 27 (3) (2017) 1259–1270.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] H. Wu, P. Saurabh, 使用伪标签的半监督深度学习进行高光谱图像分类，《IEEE图像处理学报》27 (3) (2017) 1259–1270。'
- en: '[80] H. Wu, P. Saurabh, Convolutional recurrent neural networks forhyperspectral
    data classification, Remote Sensing 9 (3) (2017) 298.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] H. Wu, P. Saurabh, 用于高光谱数据分类的卷积递归神经网络，《遥感》9 (3) (2017) 298。'
- en: '[81] S. Hao, W. Wang, S. Mathieu, Geometry-aware deep recurrent neural networks
    for hyperspectral image classification, IEEE Trans. Geosci. Remote Sens. (2020).'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] S. Hao, W. Wang, S. Mathieu, 关注几何的深度递归神经网络用于高光谱图像分类，《IEEE地球科学与遥感学报》 (2020)。'
- en: '[82] C. Shi, P. Chi-Man, Multi-scale hierarchical recurrent neural networks
    for hyperspectral image classification, Neurocomputing 294 (2018) 82–93.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] C. Shi, P. Chi-Man, 多尺度层次递归神经网络用于高光谱图像分类，《神经计算》294 (2018) 82–93。'
- en: '[83] S. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions on Knowledge
    and Data Engineering 22 (10) (2010) 1345–1359. [doi:10.1109/tkde.2009.191](https://doi.org/10.1109/tkde.2009.191).'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S. Pan, Q. Yang, 转移学习综述，《IEEE知识与数据工程学报》22 (10) (2010) 1345–1359。 [doi:10.1109/tkde.2009.191](https://doi.org/10.1109/tkde.2009.191)。'
- en: '[84] J. Yang, Y. Zhao, J. Chan, C. Yi, Hyperspectral image classification using
    two-channel deep convolutional neural network, in: 2016 IEEE International Geoscience
    and Remote Sensing Symposium (IGARSS), IEEE, 2016, pp. 5079–5082.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Yang, Y. Zhao, J. Chan, Learning and transferring deep joint spectral–spatial
    features for hyperspectral classification, IEEE Trans. Geosci. Remote Sens. 55 (8)
    (2017) 4729–4742.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] L. Lin, C. Chen, J. Yang, S. Zhang, Deep transfer hsi classification method
    based on information measure and optimal neighborhood noise reduction, Electronics
    8 (10) (2019) 1112.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] H. Zhang, Y. Li, Y. Jiang, P. Wang, Q. Shen, C. Shen, Hyperspectral classification
    based on lightweight 3-d-cnn with transfer learning, IEEE Trans. Geosci. Remote
    Sens. 57 (8) (2019) 5813–5828.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Jiang, Y. Li, H. Zhang, Hyperspectral image classification based on
    3-d separable resnet and transfer learning, IEEE Geosci. Remote Sens. Lett. 16 (12)
    (2019) 1949–1953.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] C. Deng, Y. Xue, X. Liu, C. Li, D. Tao, Active transfer learning network:
    A unified deep joint spectral–spatial feature learning model for hyperspectral
    image classification, IEEE Trans. Geosci. Remote Sens. 57 (3) (2018) 1741–1754.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. Ghifary, W. Kleijn, M. Zhang, Domain adaptive neural networks for object
    recognition, in: Pacific Rim international conference on artificial intelligence,
    Springer, 2014, pp. 898–904.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, T. Darrell, Deep domain confusion:
    Maximizing for domain invariance, arXiv preprint arXiv:1412.3474 (2014).'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Z. Wang, B. Du, Q. Shi, W. Tu, Domain adaptation with discriminative distribution
    and manifold embedding for hyperspectral image classification, IEEE Geosci. Remote
    Sens. Lett. 16 (7) (2019) 1155–1159.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, V. Lempitsky, Domain-adversarial training of neural networks, The
    Journal of Machine Learning Research 17 (1) (2016) 2096–2030.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Elshamli, G. Taylor, A. Berg, S. Areibi, Domain adaptation using representation
    learning for the classification of remote sensing images, IEEE J. Sel. Topics
    Appl. Earth Observ. Remote Sens. 10 (9) (2017) 4198–4209.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] B. Settles, Active learning literature survey, Tech. rep., University
    of Wisconsin-Madison Department of Computer Sciences (2009).'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Haut, M. Paoletti, J. Plaza, J. Li, A. Plaza, Active learning with
    convolutional neural networks for hyperspectral image classification using a new
    bayesian approach, IEEE Trans. Geosci. Remote Sens. 56 (11) (2018) 6440–6461.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] P. Liu, H. Zhang, K. Eom, Active deep learning for classification of hyperspectral
    images, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 10 (2) (2016) 712–724.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] J. Li, Active learning for hyperspectral image classification with a stacked
    autoencoders based neural network, in: 2015 7th Workshop on Hyperspectral Image
    and Signal Processing: Evolution in Remote Sensing (WHISPERS), IEEE, 2015, pp.
    1–4.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] J. Li, 基于堆叠自编码器的高光谱图像分类主动学习，见：2015年第七届高光谱图像与信号处理：遥感演进研讨会（WHISPERS），IEEE，2015年，第1–4页。'
- en: '[99] Y. Sun, J. Li, W. Wang, P. Antonio, Z. Chen, Active learning based autoencoder
    for hyperspectral imagery classification, in: 2016 IEEE International Geoscience
    and Remote Sensing Symposium (IGARSS), IEEE, 2016, pp. 469–472.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Y. Sun, J. Li, W. Wang, P. Antonio, Z. Chen, 基于主动学习的自编码器用于高光谱图像分类，见：2016
    IEEE国际地球科学与遥感会议（IGARSS），IEEE，2016年，第469–472页。'
- en: '[100] X. Cao, J. Yao, Z. Xu, D. Meng, Hyperspectral image classification with
    convolutional neural network and active learning, IEEE Trans. Geosci. Remote Sens.
    (2020).'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] X. Cao, J. Yao, Z. Xu, D. Meng, 使用卷积神经网络和主动学习的高光谱图像分类，IEEE Trans. Geosci.
    Remote Sens. (2020)。'
- en: '[101] C. Deng, Y. Xue, X. Liu, C. Li, D. Tao, Active transfer learning network:
    A unified deep joint spectral–spatial feature learning model for hyperspectral
    image classification, IEEE Trans. Geosci. Remote Sens. 57 (3) (2018) 1741–1754.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] C. Deng, Y. Xue, X. Liu, C. Li, D. Tao, 主动迁移学习网络：一种统一的深度联合光谱-空间特征学习模型用于高光谱图像分类，IEEE
    Trans. Geosci. Remote Sens. 57 (3) (2018) 1741–1754。'
- en: '[102] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learning,
    in: Advances in neural information processing systems, 2017, pp. 4077–4087.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] J. Snell, K. Swersky, R. Zemel, 原型网络用于少样本学习，见：神经信息处理系统进展，2017年，第4077–4087页。'
- en: '[103] Y. Liu, M. Su, L. Liu, C. Li, Y. Peng, J. Hou, T. Jiang, Deep residual
    prototype learning network for hyperspectral image classification, in: Second
    Target Recognition and Artificial Intelligence Summit Forum, Vol. 11427, International
    Society for Optics and Photonics, 2020, p. 1142705.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Y. Liu, M. Su, L. Liu, C. Li, Y. Peng, J. Hou, T. Jiang, 深度残差原型学习网络用于高光谱图像分类，见：第二届目标识别与人工智能峰会论坛，卷11427，国际光学与光子学学会，2020年，第1142705页。'
- en: '[104] H. Tang, Y. Li, X. Han, Q. Huang, W. Xie, A spatial–spectral prototypical
    network for hyperspectral remote sensing image, IEEE Geosci. Remote Sens. Lett.
    17 (1) (2019) 167–171.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] H. Tang, Y. Li, X. Han, Q. Huang, W. Xie, 一种空间-光谱原型网络用于高光谱遥感图像，IEEE Geosci.
    Remote Sens. Lett. 17 (1) (2019) 167–171。'
- en: '[105] B. Xi, J. Li, Y. Li, R. Song, Y. Shi, S. Liu, Q. Du, Deep prototypical
    networks with hybrid residual attention for hyperspectral image classification,
    IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 13 (2020) 3683–3700.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] B. Xi, J. Li, Y. Li, R. Song, Y. Shi, S. Liu, Q. Du, 具有混合残差注意力的深度原型网络用于高光谱图像分类，IEEE
    J. Sel. Topics Appl. Earth Observ. Remote Sens. 13 (2020) 3683–3700。'
- en: '[106] A. Muqeet, M. Iqbal, S. Bae, Hran: Hybrid residual attention network
    for single image super-resolution, IEEE Access 7 (2019) 137020–137029.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] A. Muqeet, M. Iqbal, S. Bae, Hran: 混合残差注意力网络用于单幅图像超分辨率，IEEE Access 7
    (2019) 137020–137029。'
- en: '[107] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, T. M. Hospedales,
    Learning to compare: Relation network for few-shot learning, in: Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit., 2018, pp. 1199–1208.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, T. M. Hospedales,
    学习比较：少样本学习的关系网络，见：IEEE计算机视觉与模式识别会议论文集，2018年，第1199–1208页。'
- en: '[108] B. Deng, D. Shi, Relation network for hyperspectral image classification,
    in: 2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),
    IEEE, 2019, pp. 483–488.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] B. Deng, D. Shi, 关系网络用于高光谱图像分类，见：2019 IEEE国际多媒体与博览会研讨会（ICMEW），IEEE，2019年，第483–488页。'
- en: '[109] K. Gao, B. Liu, X. Yu, J. Qin, P. Zhang, X. Tan, Deep relation network
    for hyperspectral image few-shot classification, Remote Sensing 12 (6) (2020)
    923.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] K. Gao, B. Liu, X. Yu, J. Qin, P. Zhang, X. Tan, 深度关系网络用于高光谱图像的少样本分类，遥感
    12 (6) (2020) 923。'
- en: '[110] X. Ma, S. Ji, J. Wang, J. Geng, H. Wang, Hyperspectral image classification
    based on two-phase relation learning network, IEEE Trans. Geosci. Remote Sens.
    57 (12) (2019) 10398–10409.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] X. Ma, S. Ji, J. Wang, J. Geng, H. Wang, 基于两阶段关系学习网络的高光谱图像分类，IEEE Trans.
    Geosci. Remote Sens. 57 (12) (2019) 10398–10409。'
- en: '[111] M. Rao, P. Tang, Z. Zhang, Spatial–spectral relation network for hyperspectral
    image classification with limited training samples, IEEE J. Sel. Topics Appl.
    Earth Observ. Remote Sens. 12 (12) (2019) 5086–5100.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] M. Rao, P. Tang, Z. Zhang, 空间-光谱关系网络用于有限训练样本的高光谱图像分类，IEEE J. Sel. Topics
    Appl. Earth Observ. Remote Sens. 12 (12) (2019) 5086–5100。'
- en: '[112] B. Jane, G. Isabelle, L. Yann, S. Eduard, S. Roopak, Signature verification
    using a” siamese” time delay neural network, in: Advances in neural information
    processing systems, 1994, pp. 737–744.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] B. Jane, G. Isabelle, L. Yann, S. Eduard, S. Roopak, 使用“孪生”时延神经网络进行签名验证，见：神经信息处理系统进展，1994，pp.
    737–744。'
- en: '[113] C. Sumit, H. Raia, L. Yann, Learning a similarity metric discriminatively,
    with application to face verification, in: Proc.IEEE Conf. Comput. Vis. Pattern
    Recognit., Vol. 1, IEEE, 2005, pp. 539–546.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] C. Sumit, H. Raia, L. Yann, 通过判别方法学习相似性度量，并应用于人脸验证，见：Proc.IEEE Conf.
    Comput. Vis. Pattern Recognit., Vol. 1, IEEE, 2005, pp. 539–546。'
- en: '[114] M. Norouzi, D. Fleet, R. Salakhutdinov, Hamming distance metric learning,
    in: Advances in neural information processing systems, 2012, pp. 1061–1069.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] M. Norouzi, D. Fleet, R. Salakhutdinov, 汉明距离度量学习，见：神经信息处理系统进展，2012，pp.
    1061–1069。'
- en: '[115] B. Liu, X. Yu, P. Zhang, A. Yu, Q. Fu, X. Wei, Supervised deep feature
    extraction for hyperspectral image classification, IEEE Trans. Geosci. Remote
    Sens. 56 (4) (2017) 1909–1921.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] B. Liu, X. Yu, P. Zhang, A. Yu, Q. Fu, X. Wei, 用于高光谱图像分类的监督深度特征提取，IEEE
    Trans. Geosci. Remote Sens. 56 (4) (2017) 1909–1921。'
- en: '[116] B. Liu, X. Yu, A. Yu, G. Wan, Deep convolutional recurrent neural network
    with transfer learning for hyperspectral image classification, J. Appl. Remote
    Sens. 12 (2) (2018) 026028.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] B. Liu, X. Yu, A. Yu, G. Wan, 带有迁移学习的深度卷积递归神经网络用于高光谱图像分类，J. Appl. Remote
    Sens. 12 (2) (2018) 026028。'
- en: '[117] Z. Li, X. Tang, W. Li, C. Wang, C. Liu, J. He, A two-stage deep domain
    adaptation method for hyperspectral image classification, Remote Sensing 12 (7)
    (2020) 1054.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Z. Li, X. Tang, W. Li, C. Wang, C. Liu, J. He, 一种用于高光谱图像分类的双阶段深度领域适应方法，Remote
    Sensing 12 (7) (2020) 1054。'
- en: '[118] L. Huang, Y. Chen, Dual-path siamese cnn for hyperspectral image classification
    with limited training samples, IEEE Geosci. Remote Sens. Lett. (2020).'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] L. Huang, Y. Chen, 针对有限训练样本的高光谱图像分类的双路径孪生 CNN，IEEE Geosci. Remote Sens.
    Lett. (2020)。'
- en: '[119] M. Rao, P. Tang, Z. Zhang, A developed siamese cnn with 3d adaptive spatial-spectral
    pyramid pooling for hyperspectral image classification, Remote Sensing 12 (12)
    (2020) 1964.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] M. Rao, P. Tang, Z. Zhang, 一种开发的孪生 CNN，具有 3D 自适应空间-光谱金字塔池化用于高光谱图像分类，Remote
    Sensing 12 (12) (2020) 1964。'
- en: '[120] J. Miao, B. Wang, X. Wu, L. Zhang, B. Hu, J. Zhang, Deep feature extraction
    based on siamese network and auto-encoder for hyperspectral image classification,
    in: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium,
    IEEE, 2019, pp. 397–400.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] J. Miao, B. Wang, X. Wu, L. Zhang, B. Hu, J. Zhang, 基于孪生网络和自编码器的深度特征提取用于高光谱图像分类，见：IGARSS
    2019-2019 IEEE 国际地球科学与遥感研讨会，IEEE，2019，pp. 397–400。'
- en: '[121] B. Deng, S. Jia, D. Shi, Deep metric learning-based feature embedding
    for hyperspectral image classification, IEEE Transactions on Geoence and Remote
    Sensing 58 (2) (2020) 1422–1435.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] B. Deng, S. Jia, D. Shi, 基于深度度量学习的高光谱图像分类特征嵌入，IEEE Transactions on Geoence
    and Remote Sensing 58 (2) (2020) 1422–1435。'
- en: '[122] J. Yang, Y. Zhao, J. Chan, Learning and transferring deep joint spectral-spatial
    features for hyperspectral classification, IEEE Trans. Geosci. Remote Sens. 55 (8)
    (2017) 4729–4742. [doi:10.1109/TGRS.2017.2698503](https://doi.org/10.1109/TGRS.2017.2698503).'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] J. Yang, Y. Zhao, J. Chan, 学习和转移深度联合光谱-空间特征用于高光谱分类，IEEE Trans. Geosci.
    Remote Sens. 55 (8) (2017) 4729–4742. [doi:10.1109/TGRS.2017.2698503](https://doi.org/10.1109/TGRS.2017.2698503)。'
- en: '[123] L. Hu, X. Luo, Y. Wei, Hyperspectral image classification of convolutional
    neural network combined with valuable samples, Journal of Physics Conference Series
    1549 (2020) 052011.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] L. Hu, X. Luo, Y. Wei, 结合有价值样本的卷积神经网络用于高光谱图像分类，Journal of Physics Conference
    Series 1549 (2020) 052011。'
- en: '[124] S. Wan, C. Gong, P. Zhong, B. Du, L. Zhang, J. Yang, Multiscale dynamic
    graph convolutional network for hyperspectral image classification, IEEE Transactions
    on Geoscience and Remote Sensing 58 (5) (2019) 3162–3177.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] S. Wan, C. Gong, P. Zhong, B. Du, L. Zhang, J. Yang, 用于高光谱图像分类的多尺度动态图卷积网络，IEEE
    Transactions on Geoscience and Remote Sensing 58 (5) (2019) 3162–3177。'
- en: '[125] B. Liu, K. Gao, A. Yu, W. Guo, R. Wang, X. Zuo, Semisupervised graph
    convolutional network for hyperspectral image classification, J. Appl. Remote
    Sens. 14 (2) (2020) 026516.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] B. Liu, K. Gao, A. Yu, W. Guo, R. Wang, X. Zuo, 用于高光谱图像分类的半监督图卷积网络，J.
    Appl. Remote Sens. 14 (2) (2020) 026516。'
- en: '[126] S. Wan, C. Gong, P. Zhong, S. Pan, G. Li, J. Yang, Hyperspectral image
    classification with context-aware dynamic graph convolutional network, IEEE Transactions
    on Geoscience and Remote Sensing 59 (1) (2020) 597–612.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] S. Wan, C. Gong, P. Zhong, S. Pan, G. Li, J. Yang, 使用上下文感知动态图卷积网络的高光谱图像分类，IEEE
    Transactions on Geoscience and Remote Sensing 59 (1) (2020) 597–612。'
- en: '[127] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    H. Adam, Mobilenets: Efficient convolutional neural networks for mobile vision
    applications, arXiv preprint arXiv:1704.04861 (2017).'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    H. Adam, Mobilenets: 高效的卷积神经网络用于移动视觉应用, arXiv 预印本 arXiv:1704.04861 (2017)。'
