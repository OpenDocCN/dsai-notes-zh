- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:49:27'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2112.01800] A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.01800](https://ar5iv.labs.arxiv.org/html/2112.01800)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sen Jia Shuguo Jiang Zhijie Lin Nanying Li Meng Xu Shiqi Yu [yusq@sustech.edu.cn](mailto:yusq@sustech.edu.cn)
    College of Computer Science and Software Engineering, Shenzhen University, China
    SZU Branch, Shenzhen Institute of Artificial Intelligence and Robotics for Society,
    China Department of Computer Science and Engineering, Southern University of Science
    and Technology, China
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the rapid development of deep learning technology and improvement in computing
    capability, deep learning has been widely used in the field of hyperspectral image
    (HSI) classification. In general, deep learning models often contain many trainable
    parameters and require a massive number of labeled samples to achieve optimal
    performance. However, in regard to HSI classification, a large number of labeled
    samples is generally difficult to acquire due to the difficulty and time-consuming
    nature of manual labeling. Therefore, many research works focus on building a
    deep learning model for HSI classification with few labeled samples. In this article,
    we concentrate on this topic and provide a systematic review of the relevant literature.
    Specifically, the contributions of this paper are twofold. First, the research
    progress of related methods is categorized according to the learning paradigm,
    including transfer learning, active learning and few-shot learning. Second, a
    number of experiments with various state-of-the-art approaches has been carried
    out, and the results are summarized to reveal the potential research directions.
    More importantly, it is notable that although there is a vast gap between deep
    learning models (that usually need sufficient labeled samples) and the HSI scenario
    with few labeled samples, the issues of small-sample sets can be well characterized
    by fusion of deep learning methods and related techniques, such as transfer learning
    and a lightweight model. For reproducibility, the source codes of the methods
    assessed in the paper can be found at [https://github.com/ShuGuoJ/HSI-Classification.git](https://github.com/ShuGuoJ/HSI-Classification.git).
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'hyperspectral image classification, deep learning, transfer learning, few-shot
    learning^†^†journal: Neurocomputing^(mytitlenote)^(mytitlenote)footnotetext: The
    work is supported by the National Natural Science Foundation of China (Grant No.
    41971300, 61901278 and 61976144), the National Key Research and Development Program
    of China (Grant No. 2020AAA0140002), the Program for Young Changjiang Scholars,
    the Key Project of Department of Education of Guangdong Province (Grant No. 2020ZDZX3045)
    and the Shenzhen Scientific Research and Development Funding Program under (Grant
    No. JCYJ20180305124802421 and JCYJ20180305125902403).'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hyperspectral remote sensing technology is a method that organically combines
    the spectrum of ground objects determined by their unique material composition
    with the spatial image reflecting the shape, texture and layout of ground objects,
    to realize the accurate detection, recognition and attribute analysis of ground
    objects. The resultant hyperspectral images (HSIs) not only contain abundant spectral
    information reflecting the unique physical properties of the ground features but
    also provide rich spatial information of the ground features. Therefore, HSIs
    can be utilized to solve problems that cannot be solved well in multispectral
    or natural images, such as the precise identification of each pixel. Since different
    materials exhibit specific spectral characteristics, the classification performance
    of HSI can be more accurate. Due to these advantages, hyperspectral remote sensing
    has been widely used in many applications, such as precision agriculture [[1](#bib.bib1)],
    crop monitoring [[2](#bib.bib2)], and land resources [[3](#bib.bib3), [4](#bib.bib4)].
    In environmental protection, HSI has been employed to detect gas [[5](#bib.bib5)],
    oil spills [[6](#bib.bib6)], water quality [[7](#bib.bib7), [8](#bib.bib8)] and
    vegetation coverage [[9](#bib.bib9), [10](#bib.bib10)], to better protect our
    living environment. In the medical field, HSI has been utilized for skin testing
    to examine the health of human skin [[11](#bib.bib11)].
  prefs: []
  type: TYPE_NORMAL
- en: As a general pattern recognition problem, HSI classification has received a
    substantial amount of attention, and a large number of research results have been
    achieved in the past several decades. According to the previous work [[12](#bib.bib12)],
    all researches can be divided into the spectral-feature method, spatial-feature
    method, and spectral-spatial-feature method. The spectral feature is the primitive
    characteristic of the hyperspectral image, which is also called the spectral vector
    or spectral curve. And the spatial feature [[13](#bib.bib13)] means the relationship
    between the central pixel and its context, which can greatly increase the robustness
    of the model. In the early period of the study on HSI classification, researchers
    mainly focused on the pure spectral feature-based methods, which simply apply
    classifiers to pixel vectors, such as support vector machines (SVM) [[14](#bib.bib14)],
    neural networks [[15](#bib.bib15)], logistic regression [[16](#bib.bib16)], to
    obtain classification results without any feature extraction. But raw spectra
    contain much redundant information and the relation between spectra and ground
    objects is non-linear, which enlarges the difficulty of the model classification.
    Therefore, most later methods give more attention to dimension reduction and feature
    extraction to learn the more discriminative feature. For the approaches based
    on dimension reduction, principle component analysis [[17](#bib.bib17)], independent
    component analysis [[18](#bib.bib18)], linear discriminant analysis [[19](#bib.bib19)],
    and low-rank [[20](#bib.bib20)] are widely used. Nevertheless, the performance
    of those models is still unsatisfactory. Because, there is a common phenomenon
    in the hyperspectral image which is that different surface objects may have the
    same spectral characteristic and, otherwise, the same surface objects may have
    different spectral characteristics. The variability of spectra of ground objects
    is caused by illumination, environmental, atmospheric, and temporal conditions.
    Those enlarge the probability of misclassification. Thus, those methods are only
    based on spectral information, and ignore spatial information, resulting in unsatisfactory
    classification performance. The spatial characteristic of ground objects supply
    abundant information of shape, context, and layout about ground objects, and neighboring
    pixels belong to the same class with high probability, which is useful for improving
    classification accuracy and robustness of methods. Then, a large number of feature
    extraction methods that integrate the spatial structural and texture information
    with the spectral features have been developed, including morphological [[21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)], filtering [[24](#bib.bib24), [25](#bib.bib25)],
    coding [[26](#bib.bib26)], etc. Since deep learning-based methods are mainly concerned
    in this paper, the readers are referred to [[27](#bib.bib27)] for more details
    on these conventional techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the past decade, deep learning technology has developed rapidly and received
    widespread attention. Compared with traditional machine learning model, deep learning
    technology does not need to artificially design feature patterns and can automatically
    learn patterns from data. Therefore, it has been successfully applied in the fields
    of natural language processing, speech recognition, semantic segmentation, autonomous
    driving, and object detection, and gained excellent performance. Recently, it
    also has been introduced into the field of HSI classification. Researchers have
    proposed a number of new deep learning-based HIS classification approaches, as
    shown in the left part of Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples").
    Currently, all methods, based on the joint spectral-spatial feature, can be divided
    into two categories—Two-Stream and Single-Stream, according to whether they simultaneously
    extract the joint spectral-spatial feature. The architecture of two-stream usually
    includes two branches—spectral branch and spatial branch. The former is to extract
    the spectral feature of the pixel, and the latter is to capture the spatial relation
    of the central pixel with its neighbor pixels. And the existing methods have covered
    all deep learning modules, such as fully connected layer, convolutional layer,
    and recurrent unit.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the general deep learning framework, a large number of training samples
    should be provided to well train the model and tune the numerous parameters. However,
    in practice, manually labeling is often very time-consuming and expensive due
    to the need for expert knowledge, and thus, a sufficient training set is often
    unavailable. As shown in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    (here the widely used Kennedy Space Center (KSC) hyperspectral image is utilized
    for illustration), the left figure randomly selects 10 samples per class and contains
    130 labeled samples in total, which is very scattered and can hardly be seen.
    Alternatively, the right figure in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") displays 50% of labeled samples, which is more suitable for deep learning-based
    methods. Hence, there is a vast gap between the training samples required by deep
    learning models and the labeled samples that can be collected in practice. And
    there are many learning paradigms proposed for solving the problem of few label
    samples, as shown in the right part of Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples"). In section 2, we will discuss them in detail. And they can be integrated
    with any model architecture. Some pioneering works such as [[28](#bib.bib28)]
    started the topic by training a deep model with good generalization only using
    few labeled samples. However, there are still many challenges for this topic.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9c2a417741b859faf8aa968e4ec124c3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of the massive gap between practical situations (i.e.,
    few labeled samples) and a large number of labeled samples of deep learning-based
    methods. Here, the widely used Kennedy Space Center (KSC) hyperspectral image
    is employed, which contains 13 land covers and 5211 labeled samples (detailed
    information can be found in the experimental section). Generally, sufficient samples
    are required to well train a deep learning model (as illustrated in the right
    figure), which is hard to be achieved in practice due to the difficulty of manually
    labeling (as shown in the left figure).'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we hope to provide a comprehensive review of the state-of-the-art
    deep learning-based methods for HSI classification with few labeled samples. First,
    instead of separating the various methods according to feature fusion manner,
    such as spectral-based, spatial-based, and joint spectral-spatial-based methods,
    the research progress of methods related to few training samples is categorized
    according to the learning paradigm, including transfer learning, active learning,
    and few-shot learning. Second, a number of experiments with various state-of-the-art
    approaches have been carried out, and the results are summarized to reveal the
    potential research directions. Further, it should be noted that different from
    the previous review papers [[12](#bib.bib12), [29](#bib.bib29)], this paper mainly
    focuses on the few labeled sample issue, which is considered as the most challenging
    problem in the HSI classification scenario. For reproducibility, the source codes
    of the methods conducted in the paper can be found at the web site for the paper¹¹1[https://github.com/ShuGuoJ/HSI-Classification.git](https://github.com/ShuGuoJ/HSI-Classification.git).
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of this paper is organized as follows. Section [2](#S2 "2 Deep
    learning models for HSI classification ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") introduces the deep models that
    are popular in recent years. In Section [3](#S3 "3 Deep learning paradigms for
    HSI classification with few labeled samples ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples"), we divide the previous works
    into four mainstream learning paradigms, including transfer learning, active learning,
    and few-shot learning. In Section [4](#S4 "4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples"), we performed
    many experiments, and a number of representative deep learning-based classification
    methods are compared on several real hyperspectral image data sets. Finally, conclusions
    and suggestions are provided in Section [5](#S5 "5 Conclusions ‣ A Survey: Deep
    Learning for Hyperspectral Image Classification with Few Labeled Samples").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c83a1055948d73fada507706b8f628a8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The category of deep learning-based methods for hyperspectral image
    classification. The left is from the model architecture point of view, while the
    right is from the learning paradigm point of view. It is worth noting that the
    both kinds of methods can be combined arbitrarily.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep learning models for HSI classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, three classical deep learning models, including the autoencoder,
    convolutional neural network (CNN), and recurrent neural network (RNN), for HSI
    classification are respectively described, and the relevant references are reviewed.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Autoencoder for HSI classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'An autoencoder [[30](#bib.bib30)] is a classic neural network, which consists
    of two parts: an encoder and a decoder. The encoder $p_{encoder}(\bm{h}|\bm{x})$
    maps the input $\bm{x}$ as a hidden representation $\bm{h}$, and then, the decoder
    $p_{decoder}(\hat{\bm{x}}|\bm{h})$ reconstructs $\hat{\bm{x}}$ from $\bm{h}$.
    It aims to make the input and output as similar as possible. The loss function
    can be formulated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}(\bm{x},\hat{\bm{x}})=\min&#124;\bm{x}-\hat{\bm{x}}&#124;$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{L}$ is the similarity measure. If the dimension of $\bm{h}$
    is smaller than $\bm{x}$, the autoencoder procedure is undercomplete and can be
    used to reduce the data dimension. Evidently, if there is not any constraint on
    $\bm{h}$, the autoencoder is the simplest identical function. In other words,
    the network does not learn anything. To avoid such a situation, the usual way
    is to add the normalization term $\Omega(h)$ to the loss. In [[31](#bib.bib31),
    zeng2018facial], the normalization of the autoencoder, referred as a sparse autoencoder,
    is $\Omega(h)=\lambda\sum_{i}h_{i}$, which will make most of the parameters of
    the network very close to zero. Therefore, it is equipped with a certain degree
    of noise immunity and can produce the sparsest representation of the input. Another
    way to avoid the identical mapping is by adding some noise into $\bm{x}$ to make
    the damaged input $\bm{x_{noise}}$ and then forcing the decoder to reconstruct
    the $\bm{x}$. In this situation, it becomes the denoising autoencoder [[32](#bib.bib32)],
    which can remove the additional noise from $\bm{x_{noise}}$ and produce a powerful
    hidden representation of the input. In general, the autoencoder plays the role
    of feature extractor [[33](#bib.bib33)] to learn the internal pattern of data
    without labeled samples. Figure [3](#S2.F3 "Figure 3 ‣ 2.1 Autoencoder for HSI
    classification ‣ 2 Deep learning models for HSI classification ‣ A Survey: Deep
    Learning for Hyperspectral Image Classification with Few Labeled Samples") illustrates
    the basic architecture of the autoencoder model.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/85775aa5c15d070e6222732fbb44c76a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The architecture of the autoencoder. The solid line represents training,
    while the dashed line represents inference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, Chen *et al.* [[34](#bib.bib34)] used an autoencoder for the first
    time for feature extraction and classification of HSIs. First, in the pretraining
    stage, the spectral vector of each pixel directly inputs the encoder module, and
    then, the decoder is used to reconstruct it so that the encoder has the ability
    to extract spectral features. Alternatively, to obtain the spatial features, principal
    component analysis (PCA) is utilized to reduce the dimensionality of the hyperspectral
    image, and then, the image patch is flattened into a vector. Another autoencoder
    is employed to learn the spatial features. Finally, the spatial-spectral joint
    information obtained above is fused and classified. Subsequently, a large number
    of hyperspectral image classification methods [[35](#bib.bib35), [36](#bib.bib36)]
    based on autoencoders appeared. Most of these methods adopt the same training
    strategy as [[34](#bib.bib34)], which is divided into two modules: fully training
    the encoder in an unsupervised manner and fine-tuning the classifier in a supervised
    manner. Each of these methods attempts different types of encoders or preprocessing
    methods to adapt to HSI classification under the condition of small samples. For
    example, Xing *et al.* [[36](#bib.bib36)] stack multiple denoising autoencoders
    to form a feature extractor, which has a stronger anti-noise ability to extract
    more robust representations. Given that the same ground objects may have different
    spectra while different ground objects may exhibit similar spectra, spectral-based
    classification methods often fail to achieve satisfactory performance, and spatial
    structural information of objects provides an effective supplement. To gain a
    better spatial description of an object, some autoencoder models combined with
    convolutional neural networks (CNNs) have been developed [[37](#bib.bib37), [38](#bib.bib38)].
    Concretely, the autoencoder module is able to extract spectral features on large
    unlabeled samples, while the CNN is proven to be able to extract spatial features
    well. After fusion, the spatial-spectral features can be achieved. Further, to
    reduce the number of trainable parameters, some researchers use the lightweight
    models, such as SVMs [[39](#bib.bib39), [40](#bib.bib40)], random forests [[41](#bib.bib41),
    [42](#bib.bib42)] or logistic regression [[34](#bib.bib34), [43](#bib.bib43)],
    to serve as the classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the three-dimensional (3D) pattern of hyperspectral images, it is desirable
    to simultaneously investigate the spectral and spatial information such that the
    joint spatial-spectral correlation can be better examined. Some three-dimensional
    operators and methods have been proposed. In the preprocessing stage, Li *et al.* [[44](#bib.bib44)]
    utilized the 3D Gabor operator to fuse spatial information and spectral information
    to obtain spatial-spectral joint features, which were then fed into the autoencoder
    to obtain more abstract features. Mei *et al.* [[40](#bib.bib40)] used a 3D convolutional
    operator to construct an autoencoder to extract spatial-spectral features directly.
    In addition, image segmentation has been introduced to characterize the region
    structure of objects to avoid misclassification of pixels at the boundary [[45](#bib.bib45)].
    Therefore, Liu *et al.* [[46](#bib.bib46)] utilized superpixel segmentation technology
    as a postprocessing method to perform boundary regularization on the classification
    map.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Convolutional Neural Networks (CNNs) for HSI classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In theory, the CNN uses a group of parameters that refer to a kernel function
    or kernel to scan the image and produce a specified feature. It has three main
    characteristics that make it very powerful for feature representation, and thus,
    the CNN has been successfully applied in many research fields. The first one is
    the local connection that greatly decreases the number of trainable parameters
    and makes itself suitable for processing large images. This is the most obvious
    difference from the fully connected network, which has a full connection between
    two neighboring neural layers and is unfriendly for large spatial images. To further
    reduce the number of parameters, the same convolutional kernel shares the same
    parameters, which is the second characteristic of CNNs. In contrast, in the traditional
    neural network, the parameters of the output are independent from each other.
    However, the CNN applies the same parameters for all of the output to cut back
    the number of parameters, leading to the third characteristic: shift invariance.
    It means that even if the feature of an object has shifted from one position to
    another, the CNN model still has the capacity to capture it regardless of where
    it appears. Specifically, a common convolutional layer consists of three traditional
    components: linear mapping, the activation function and the pooling function.
    Similar to other modern neural network architectures, activation functions are
    used to bring a nonlinear mapping feature into the network. Generally, the rectified
    linear unit (ReLU) is the prior choice. Pooling makes use of the statistical characteristic
    of the local region to represent the output of a specified position. Taking the
    max pooling step as an example, it employs the max value to replace the region
    of input. Clearly, the pooling operation is robust to small changes and noise
    interfere, which could be smoothed out by the pooling operation in the output,
    and thus, more abstract features can be reserved.'
  prefs: []
  type: TYPE_NORMAL
- en: In the early works of applying CNNs for HSI classification, two-dimensional
    convolution was the most widely used method, which is mainly employed to extract
    spatial texture information [[47](#bib.bib47), [28](#bib.bib28), [48](#bib.bib48)],
    but the redundant bands greatly enlarge the size of the convolutional kernel,
    especially the channel dimensionality. Later, a combination of one-dimensional
    convolution and two-dimensional convolution appeared [[49](#bib.bib49)] to solve
    the above problem. Concretely, one-dimensional and two-dimensional convolutions
    are responsible for extracting spectral and spatial features, respectively. The
    two types of features are then fused before being input to the classifier. For
    the small training sample problem, due to insufficient labeled samples, it is
    difficult for CNNs to learn effective features. For this reason, some researchers
    usually introduced traditional machine learning methods, such as attribute profiles [[50](#bib.bib50)],
    GLCM [[51](#bib.bib51)], hash learning [[52](#bib.bib52)], and Markov Random fields [[53](#bib.bib53)],
    to introduce prior information to the convolutional network and improve the performance
    of the network. Similar to the trend of autoencoder-based classification methods,
    three-dimensional CNN models have also been applied to HSI classification in recent
    years and have shown better feature fusion capabilities [[54](#bib.bib54), [55](#bib.bib55)].
    However, due to the large number of parameters, three-dimensional convolution
    is not suitable for solving small-sample classification problems under supervised
    learning. To reduce the number of parameters of 3D convolution, Fang *et al.* [[56](#bib.bib56)]
    designed a 3D separable convolution. In contrast, Mou *et al.* [[57](#bib.bib57),
    [58](#bib.bib58)] introduced an autoencoder scheme into the three-dimensional
    convolution module to solve this problem. By a combination with the classic autoencoder
    training method, the three-dimensional convolution autoencoder can be trained
    in an unsupervised learning manner, and then, the decoder is replaced with a classifier,
    while the parameters of the encoder are frozen. Finally, a small classifier is
    trained by supervised learning. Moreover, due to the success of ResNet [[59](#bib.bib59)],
    scholars have studied the HSI classification problem based on convolutional residuals [[57](#bib.bib57),
    [58](#bib.bib58), [60](#bib.bib60), [61](#bib.bib61)]. These methods try to use
    jump connections to enable the network to learn complex features with a small
    number of labeled samples. Similarly, CNNs with dense connections have also been
    introduced into this field [[62](#bib.bib62), [63](#bib.bib63)]. In addition,
    the attention mechanism is another hotpot for fully mining sample features. Concretely,
    Haut and Xiong *et al.* [[64](#bib.bib64), [65](#bib.bib65)] incorporated the
    attention mechanism with CNNs for HSI classification. Although the above models
    can work well on HSI, they cannot overcome the disadvantage of the low spatial
    resolution of HSIs, which may cause mixed pixels. To make up for this shortcoming,
    multimodality CNN models have been proposed. These methods [[66](#bib.bib66),
    [67](#bib.bib67), [68](#bib.bib68)] combine HSIs and LiDAR data together to increase
    the discriminability of sample features. Moreover, to achieve good performance
    under the small-sample scenario, Yu *et al.* [[28](#bib.bib28)] enlarged the training
    set through data augmentation by implementing rotation and flipping. On the one
    hand, this method increases the number of samples and improves their diversity.
    On the other hand, it enhances the model’s ability of rotation invariance, which
    is important in some fields such as remote sensing. Subsequently, Li *et al.* [[69](#bib.bib69),
    [70](#bib.bib70)] designed a data augmentation scheme for HSI classification.
    They combined the samples in pairs so that the model no longer learns the characteristics
    of the samples themselves but learns the differences between the samples. Different
    combinations make the scale of the training set larger, which is more conducive
    for model training.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Recurrent neural network (RNN) for HSI classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Compared with other forms of neural networks, recurrent neural networks (RNNs) [[71](#bib.bib71)]
    have memory capabilities and can record the context information of sequential
    data. Because of this memory characteristic, recurrent neural networks are widely
    used in tasks such as speech recognition and machine translation. More precisely,
    the input of a recurrent neural network is usually a sequence of vectors. At each
    time step $t$, the network receives an element $\bm{x}_{t}$ in a sequence and
    the state $\bm{h}_{t-1}$ of the previous time step, and produces an output $\bm{y}_{t}$
    and a state $\bm{h}_{t}$ representing the context information at the current moment.
    This process can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{h}_{t}=f(\mathbf{W}_{hh}\bm{h}_{t-1}+\mathbf{W}_{xh}\bm{x}_{t}+\mathbf{b})$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{W}_{xh}$ represents the weight matrix from the input layer to
    the hidden layer, $\mathbf{W}_{hh}$ denotes the state transition weight in the
    hidden layer, and $\mathbf{b}$ is the bias. It can be seen that the current state
    of the recurrent neural network is controlled by both the state of the previous
    time step and the current input. This mechanism allows the recurrent neural network
    to capture the contextual semantic information implicitly between the input vectors.
    For example, in the machine translation task, it can enable the network to understand
    the semantic relationship between words in a sentence.
  prefs: []
  type: TYPE_NORMAL
- en: However, the classic RNN is prone to encounter gradient explosion or gradient
    vanishing problems during the training process. When there are too many inputs,
    the derivation chain of the RNN will become too long, making the gradient value
    close to infinity or zero. Therefore, the classic RNN model is replaced by a long
    short-term memory (LSTM) network [[71](#bib.bib71)] or a gated recurrent unit
    (GRU) [[72](#bib.bib72)] in the HSI classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Both LSTM and GRU use gating technology to filter the input and the previous
    state so that the network can forget unnecessary information and retain the most
    valuable context. LSTM maintains an internal memory state, and there are three
    gates: input gate $\bm{i}_{t}$, forget gate $\bm{f}_{t}$ and output gate $\bm{o}_{t}$,
    which are formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{i}_{t}=\sigma(\mathbf{W_{i}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\bm{f}_{t}=\sigma(\mathbf{W_{f}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\bm{o}_{t}=\sigma(\mathbf{W_{io}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (5) |'
  prefs: []
  type: TYPE_TB
- en: 'It can be found that the three gates are generated based on the current input
    and the previous state. First, the current input and the previous state will be
    spliced and mapped to a new input $\bm{g}_{t}$ according to the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{g}_{t}=\tanh(\mathbf{W_{g}}\cdot[\bm{x}_{t},\bm{h}_{t-1}])$ |  |
    (6) |'
  prefs: []
  type: TYPE_TB
- en: Subsequently, the input gate, the forget gate, the new input $\bm{g}_{t}$ and
    the internal memory unit $\hat{\bm{h}}_{t-1}$ update the internal memory state
    tegother. In this process, LSTM discards invalid information and adds new semantic
    information.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\bm{h}}_{t}=\bm{f}_{t}\odot\hat{\bm{h}}_{t-1}+\bm{i}_{t}\odot\bm{g}_{t}$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Finally, the new internal memory state is filtered by the output gate to form
    the output of the current time step
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{h}_{t}=\bm{o}_{t}\odot\tanh(\hat{\bm{h}}_{t})$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Concerning HSI processing, each spectral image is a high-dimensional vector
    and can be regarded as a sequence of data. There are many works using LSTM for
    HSI classification tasks. For instance, Mou *et al.* [[73](#bib.bib73)] proposed
    an LSTM-based HSI classification method for the first time, and their work only
    focused on spectral information. For each sample pixel vector, each band is input
    into the LSTM step by step. To improve the performance of the model, spatial information
    is considered in subsequent research. For example, Liu *et al.* fully considered
    the spatial neighborhood of the sample and used a multilayer LSTM to extract spatial
    spectrum features [[74](#bib.bib74)]. Specifically, in each time step, the sampling
    points of the neighborhood are sequentially input into the network to deeply mine
    the context information in the spatial neighborhood. In [[75](#bib.bib75)], Zhou
    *et al.* used two LSTMs to extract spectral features and spatial features. In
    particular, for the extraction of spatial features, PCA is first used to extract
    principal components from the sample rectangular space neighborhood. Then, the
    first principal component is divided into several lines to form a set of sequence
    data, and gradually input into the network. In contrast, Ma and Zhang *et al.* [[76](#bib.bib76),
    [77](#bib.bib77)] measures the similarity between the sample point in the spatial
    neighborhood and the center point. The sample points in the neighborhood will
    be reordered according to the similarity and then input into the network step
    by step. This approach allows the network to focus on learning sample points that
    are highly similar to the center point, and the memory of the internal hidden
    state can thus be enhanced. Erting Pan *et al.* [[78](#bib.bib78)] proposed an
    effective tiny model for spectral-spatial classification on HSIs based on a single
    gate recurrent unit (GRU). In this work, the rectangular space neighborhood is
    flattened into a vector, which is used to initialize the hidden vector $h_{0}$
    of GRU, and the center point pixel vector is input into the network to learn features.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Wu and Saurabh argue that it is difficult to dig out the internal
    features of the sample by directly inputting a single original spectral vector
    into the RNN [[79](#bib.bib79), [80](#bib.bib80)]. The authors use a one-dimensional
    convolution operator to extract multiple feature vectors from the spectrum vector,
    which form a feature sequence and are then input to the RNN. Finally, the fully
    connected layer and the softmax function are adopted to obtain the classification
    result. It can be seen that only using recurrent neural networks or one-dimensional
    convolution to extract the spatial-spectrum joint features is actually not efficient
    because this will cause the loss of spatial structure information. Therefore,
    some researchers combine two-dimensional/three-dimensional CNNs with an RNN and
    use convolution operators to extract spatial-spectral joint features. For example,
    Hao *et al.* [[81](#bib.bib81)] utilized U-Net to extract features and input them
    into an LSTM or GRU so that the contextual information between features could
    be explored. Moreover, Shi *et al.* [[82](#bib.bib82)] introduced the concept
    of the directional sequence to fully extract the spatial structure information
    of HSIs. First, the rectangular area of the sampling point is divided into nine
    overlapping patches. Second, the patch will be mapped to a set of feature vectors
    through a three-dimensional convolutional network, and the relative position of
    the patch can generate 8 combinations of directions (for example, top, middle,
    bottom, left, center, and right) to form a direction sequence. Finally, the sequence
    is input into the LSTM or GRU to obtain the classification result. In this way,
    the spatial distribution and structural characteristics of the features can be
    explored.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep learning paradigms for HSI classification with few labeled samples
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although different HSI classification methods have different specific designs,
    they all follow some learning paradigms. In this section, we mainly introduce
    several learning paradigms that are applied to HSI classification with few labeled
    training samples. These learning paradigms are based on specific learning theories.
    We hope to provide a general guide for researchers to design algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Deep Transfer Learning for HSI classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Transfer learning [[83](#bib.bib83)] is an effective method to deal with the
    small-sample problem. Transfer learning tries to transfer knowledge learned from
    one domain to another. First, there are two data sets/domains, one is called a
    source domain that contains abundant labeled samples, and the other is called
    a target domain and only contains few labeled samples. To facilitate the subsequent
    description, we define the source domain as $\mathbf{D}_{s}$, the target domain
    as $\mathbf{D}_{t}$, and their label spaces as $\mathbf{Y}_{s}$ and $\mathbf{Y}_{t}$,
    respectively. Usually, the data distribution of the source domain and the target
    domain are inconsistent: $P(\bm{X}_{s})\neq P(\bm{X}_{t})$. Therefore, the purpose
    of transfer learning is to use the knowledge learned from $\mathbf{D}_{s}$ to
    identify the labels of samples in $\mathbf{D}_{t}$.'
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning is a general method in transfer learning that uses $\mathbf{D}_{s}$
    to train the model and adjust it by $\mathbf{D}_{t}$. Its original motivation
    is to reduce the number of samples needed during the training process. Since deep
    learning models generally contain a vast number of parameters and if it is trained
    on the target domain $\mathbf{D}_{t}$, it is easy to overfit and perform poorly
    in practice. However, fine-tuning allows the model parameters to reach a suboptimal
    state, and a small number of training samples of the target domain can tune the
    model to reach the optimal state. It involves two steps. First, the specific model
    will be fully trained on the source domain $\mathbf{D}_{s}$ with abundant labeled
    samples to make the model parameters arrive at a good state. Then, the model is
    transferred to the target domain $\mathbf{D}_{t}$, except for some task-related
    modules, and slightly tuned on $\mathbf{D}_{t}$ so that the model fits the data
    distribution of the target domain $\mathbf{D}_{t}$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e7e11e3a60a62fa97b36ff2befbf2611.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Flowchart of the fine-tuning method. The solid line represents pretraining,
    and the dashed line represents fine-tuning. $f_{\omega}$ is a learning function.'
  prefs: []
  type: TYPE_NORMAL
- en: Because the fine-tuning method is relatively simple, it is widely used in the
    transfer learning method for hyperspectral image classification. To our knowledge,
    Yang *et al.* [[84](#bib.bib84)] are the first to combine deep learning with transfer
    learning to classify hyperspectral images. The model consists of two convolutional
    neural networks, which are used to extract spectral features and spatial features.
    Then, the joint spectral-spatial feature will be input into the fully connected
    layer to gain a final result. According to fine-tuning, the model is first fully
    trained on the hyperspectral image of the source domain. Next, the fully connected
    layer is replaced and the parameters of the convolutional network are reserved.
    Finally, the transfer model will be trained on the target hyperspectral image
    to adapt to the new data distribution. The later transfer learning models based
    on fine-tuning basically follow that architecture [[85](#bib.bib85), [86](#bib.bib86),
    [87](#bib.bib87), [88](#bib.bib88)]. It is worth noting that Deng *et al.* [[89](#bib.bib89)]
    combined transfer learning with active learning to classify HSI.
  prefs: []
  type: TYPE_NORMAL
- en: Data distribution adaptation is another commonly used transfer learning method.
    The basic idea of this theory is that in the original feature space, the data
    probability distributions of the source domain and the target domain are usually
    different. However, they can be mapped to a common feature space together. In
    this space, their data probability distributions become similar. In 2014, Ghifary
    *et al.* [[90](#bib.bib90)] first proposed a shadow neural network-based domain
    adaptation model, called DaNN. The innovation of this work is that a maximum mean
    discrepancy (MMD) adaptation layer is added to calculate the distance between
    the source domain and the target domain. Moreover, the distance is merged into
    the loss function to reduce the difference between the two data distributions.
    Subsequently, Tzeng *et al.* [[91](#bib.bib91)] extended this work with a deeper
    network and proposed deep domain confusion to solve the adaptive problem of deep
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/df2392978c69fc2eba6e8ebe39aaf3f7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Flowchart of DANN.'
  prefs: []
  type: TYPE_NORMAL
- en: Wang *et al.* [[92](#bib.bib92)] introduced the deep domain adaptation model
    to the field of hyperspectral image classification for the first time. In [[92](#bib.bib92)],
    two hyperspectral images from different scenes will be mapped to two low-dimensional
    subspaces by the deep neural network, in which the samples are represented as
    manifolds. MMD is used to measure the distance between two low-dimensional subspaces
    and is added to the loss function to make two low-dimensional subspaces have high
    similarity. In addition, they still add the sum of the distances between samples
    and their neighbor into the loss function to ensure that the low-dimensional manifold
    is discriminative.
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by the excellent performance of generative adversarial net (GAN),
    Yaroslav *et al.* [[93](#bib.bib93)] first introduced it into transfer learning.
    The network is named DANN (domain-adversarial neural network), which is different
    from DaNN proposed by Ghifary *et al.* [[90](#bib.bib90)]. The generator $\mathbf{G}_{f}$
    and the discriminator $\mathbf{G}_{d}$ compete with each other until they have
    converged. In transfer learning, the data in one of the domains (usually the target
    domain) are regarded as the generated sample. The generator aims to learn the
    characteristics of the target domain sample so that the discriminator cannot distinguish
    which domain the sample comes from to achieve the purpose of domain adaptation.
    Therefore, $\mathbf{G}_{f}$ is used to represent the feature extractor here.
  prefs: []
  type: TYPE_NORMAL
- en: Elshamli *et al.* [[94](#bib.bib94)] first introduced the concept of DANN to
    the task of hyperspectral image classification. Compared to general GNN, it has
    two discriminators. One is the class discriminator predicting the class labels
    of samples, and the other is the domain discriminator predicting the source of
    the samples. Different from the two-stage method, DANN is an end-to-end model
    that can perform representation learning and classification tasks simultaneously.
    Moreover, it is easy to train. Further, it outperforms two-stage frameworks such
    as the denoising autoencoder and traditional approaches such as PCA in hyperspectral
    image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Deep Active Learning for HSI classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Active learning [[95](#bib.bib95)] in the supervised learning method can efficiently
    deal with small-sample problems. It can effectively learn discriminative features
    by autonomously selecting representative or high-information samples from the
    training set, especially when the labeled samples are scarce. Generally speaking,
    active learning consists of five components, $A=(C,L,U,Q,S)$. Among them, $C$
    represents one or a group of classifiers. $L$ and $U$ represent the labeled samples
    and unlabeled samples, respectively. $Q$ is the query function, which is used
    to query the samples with a large amount of information among the unlabeled samples.
    $S$ is an expert and can label unlabeled samples. In general, active learning
    has two stages. The first stage is the initialization stage. In this stage, a
    small number of samples will be randomly selected to form the training set $L$
    and be labeled by experts to train the classifier. The second stage is the iterative
    query. $Q$ will select new samples from the unlabeled sample set $U$ for $S$ to
    mark them based on the results of the previous iteration and add them to the training
    set $L$. The active learning method applied to hyperspectral image classification
    is mainly based on the active learning algorithm of the committee and the active
    learning algorithm based on the posterior probability. In the committee-based
    active learning algorithm, the EQB method uses entropy to measure the amount of
    information in unlabeled samples. Specifically, the training set L will be divided
    into $k$ subsets to train $k$ classifiers and then use these $k$ classifiers to
    classify all unlabeled samples. Therefore, each unlabeled sample corresponds to
    k predicted labels. The entropy value is calculated from this:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x}^{EQB}=\mathop{\arg\min}_{x_{i}\in U}\frac{H^{EQB}(x_{i})}{log(N_{i})}$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $H$ represents the entropy value, and $N_{i}$ represents the number of
    classes predicted by the sample $x_{i}$. Samples with large entropy will be selected
    and manually labeled [[96](#bib.bib96)]. In [[97](#bib.bib97)], the deep belief
    network is used to generate the mapping feature $h$ of the input $x$ in an unsupervised
    way, and then, $h$ will be used to calculate the information entropy. At the same
    time, sparse representation is used to estimate the representations of the sample.
    In the process of selecting samples for active learning, the information entropy
    and representations of the samples are comprehensively considered.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast, the active learning method based on posterior probability [[98](#bib.bib98),
    [99](#bib.bib99), [100](#bib.bib100)] is more widely used. Breaking ties belongs
    to the active learning method of posterior probability, which is widely used in
    hyperspectral classification tasks. This method first uses specifies models, such
    as convolutional networks, maximum likelihood estimation classifiers, support
    vector machines, etc., to estimate the posterior probabilities of all samples
    in the candidate pool. Then, the approach uses the posterior probability to input
    the following formula to produce a measure of sample uncertainty:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{x}^{BT}=\mathop{\arg\min}_{x_{i}\in U}\left\{\mathop{\max}_{w\in
    N}p\left(y_{i}^{*}=w&#124;x_{i}\right)-\mathop{\max}_{w\in N\setminus w^{+}}p(y_{i}^{*}=w&#124;x_{i})\right\}$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: In the above formula, we first calculate the difference between the largest
    probability and the second-largest probability among the posterior probabilities
    of all candidate samples and select the sample with the minimum difference to
    join the valuable data set. The lower $x^{BT}$ is, the more uncertain is the sample.
    In [[98](#bib.bib98)], Li *et al.* first used an autoencoder to construct an active
    learning model for hyperspectral image classification tasks. At the same time,
    Sun *et al.* [[99](#bib.bib99)] also proposed a similar method. However, this
    method only uses spectral features. Because of the effectiveness of spatial information,
    in [[101](#bib.bib101)], when generating the posterior probability, the space-spectrum
    joint features are considered at the same time. In contrast, Cao *et al.* [[100](#bib.bib100)]
    use convolutional neural networks to generate the posterior probability.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the active learning method can automatically select effective samples
    according to certain criteria, reduce inefficient redundant samples, and thus
    well alleviate the problem of missing training samples in the small-sample problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a73a2b22983b87aabce95ac19960487b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Architecture of active learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Deep Few-shot Learning for HSI classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Few-shot learning is among meta-learning approaches and aims to study the difference
    between the samples instead of directly learning what the sample is, different
    from most other deep learning methods. It makes the model learn to learn. In few-shot
    classification, given a small support set with N labeled samples $S_{N}^{k}=\{(\bm{x}_{1},y_{1}),\cdots,(\bm{x}_{N},y_{N})\}$,
    which have $k$ categories, the classifier will mask the query sample with the
    label of the largest similarity sample among $S_{N}^{k}$. To achieve this target,
    many learning frameworks have been proposed and they can be divided into two categories:
    meta-based model and metric-based model.'
  prefs: []
  type: TYPE_NORMAL
- en: The prototype network [[102](#bib.bib102)] is one of the metric-based models
    of few-shot learning. Its basic idea is that every class can be depicted by a
    prototype representation, and the samples that belong to the same category should
    be around the class prototype. First, all samples will be transformed into a metric
    space through an embedding function $f_{\phi}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M}$
    and represented by the embedding vector $\mathbf{c}_{k}\in\mathbb{R}^{M}$. Due
    to the powerful ability of the convolutional network, it is used as the embedding
    function. Moreover, the prototype vector is usually the mean of the embedding
    vector of the samples in the support set for each class $c_{i}$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\bm{c}_{i}=\frac{1}{&#124;S^{i}&#124;}\sum_{(\bm{x}_{j},y_{j})\in S^{i}}f_{\phi}(\bm{x}_{j})$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: In [[103](#bib.bib103)], Liu *et al.* simply introduce the prototype network
    into hyperspectral image classification task and use ResNet [[59](#bib.bib59)]
    to serve as a feature extractor that maps the samples into a metric space. Then,
    the prototype network is significantly improved for the hyperspectral image classification
    task by [[104](#bib.bib104)]. In the paper, the spatial-spectral feature is first
    integrated by the local pattern coding, and the 1D-CNN converts it to an embedding
    vector. The prototype is the weighted mean of these embedding vectors, which is
    contrary to the general prototype network. In [[105](#bib.bib105)] Xi *et al.*
    replace the mapping function with hybrid residual attention [[106](#bib.bib106)]
    and introduce a new loss function to force the network to increase the interclass
    distance and decrease the intraclass distance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9e18f27d7395fa13eebb8777b6bd8a69.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Architecture of a prototype network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The relation network [[107](#bib.bib107)] is another metric-based model of
    few-shot learning. In general, it has two modules: the embedding function $f_{\phi}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M}$
    and relation function $f_{\psi}:\mathbb{R}^{2M}\rightarrow\mathbb{R}$. The function
    of the embedding module is the same as the prototype network, and its key idea
    is the relation module. The relation module is to calculate the similarity of
    samples. It is a learnable module that is different from the Euclidean distance
    or cosine distance. In other words, the relation network introduces a learnable
    metric function based on the prototype network. The relation module can more precisely
    describe the difference of samples by the study. During inference, the query embedding
    $f_{\psi}(x_{i})$ will be combined with the support embedding $f_{\psi}(\bm{x}_{j})$
    as $\mathcal{C}(f_{\psi}(\bm{x}_{i}),f_{\psi}(\bm{x}_{j}))$. Usually, $\mathcal{C}(\cdot,\cdot)$
    is a concatenation operation. Then, the relation function will transform the splicing
    vector to a relation score $r_{i,j}$, which indicates the similarity between $x_{i}$
    and $x_{j}$.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $r_{i,j}=f_{\psi}(\mathcal{C}(f_{\psi}(\bm{x}_{i}),f_{\psi}(\bm{x}_{j})))$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: Several works have introduced the relation network into hyperspectral image
    classification to solve the small sample set problem. Deng *et al.* [[108](#bib.bib108)]
    first introduced the relation network into HSI. They use a 2-dimensional convolutional
    neural network to construct both the embedding function and relation function.
    Gao *et al.* [[109](#bib.bib109)] and Ma *et al.* [[110](#bib.bib110)] have also
    proposed a similar architecture. In [[111](#bib.bib111)], to extract the joint
    spatial-spectral feature, Rao *et al.* implemented the embedding function with
    a 3-dimensional convolutional neural network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f340f2e315b44336e4330bfd0a4ec3cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Architecture of relation network.'
  prefs: []
  type: TYPE_NORMAL
- en: The Siamese network [[112](#bib.bib112), [113](#bib.bib113), [114](#bib.bib114)]
    is a typical network in few-shot learning. Compared with the above network, its
    input is a sample pair. Thus, it is composed by two parallel subnetworks $f_{\phi
    1}:\mathbb{R}^{D}\rightarrow\mathbb{R}^{M}$ with the same structure and sharing
    parameters. The subnetworks respectively accept an input sample and map it to
    a low-dimensional metric space to generate their own embedding $f_{\phi 1}(\bm{x}_{i})$
    and $f_{\phi 1}(\bm{x}_{j})$. The Euclidean distances $D(\bm{x}_{i},\bm{x}_{j})$
    is used to measure their similarity.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D(\bm{x}_{i},\bm{x}_{j})=\&#124;f_{\phi 1}(\bm{x}_{i})-f_{\phi 1}(\bm{x}_{j})\&#124;_{2}$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: The higher the similarity between the two samples is, the more likely they are
    to belong to the same class. Recently, the Siamese network was introduced into
    HSI classification. Usually, a 2-dimensional convolutional neural network [[115](#bib.bib115),
    [116](#bib.bib116)] is used to serve as the embedding function, as in the above
    two networks. In the same way, several methods combined the 1-dimensional convolution
    neural network with the 2-dimensional one [[117](#bib.bib117), [118](#bib.bib118)]
    or use a 3-dimensional network [[119](#bib.bib119)] for the joint spectral-spatial
    feature. Moreover, Miao *et al.* [[120](#bib.bib120)] have tried to use the stack
    autoencoder to construct the embedding function $f_{\phi 1}$. After training,
    the model has the ability to identify the difference between samples. To obtain
    the final classification result, we still need a classifier to classify the embedding
    feature of the sample, which is different from the prototype network and the relation
    network. To avoid overfitting under limited labeled samples, an SVM is usually
    used as a classifier since it is famous for its lightweight.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ac7c46311408b42df553f4564d0b552d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Architecture of the Siamese network.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In most papers, comprehensive experiments and analysis are introduced to describe
    the advantages and disadvantages of the methods in the paper. However, the problem
    is that different papers may choose different experimental settings. For example,
    the same number of samples for training or test is used in the experiments, and
    the chosen samples are normally different since they are chosen randomly. To evaluate
    different methods fairly, we should use the exact same experimental setting. That
    is the reason why we design experiments to evaluate different methods.
  prefs: []
  type: TYPE_NORMAL
- en: As described above, the main methods of small-sample learning currently include
    the autoencoder, few-shot learning, transfer learning, active learning, and data
    augmentation. Therefore, some representative networks of the following methods–S-DMM [[121](#bib.bib121)],
    SSDL [[37](#bib.bib37)], 3DCAE [[40](#bib.bib40)], TwoCnn [[122](#bib.bib122)],
    SSLstm [[75](#bib.bib75)] and 3DVSCNN [[123](#bib.bib123)], which contain convolutional
    network models and recurrent network models, are selected to conduct experiments
    on three benchmark data sets–PaviaU, Salinas and KSC. All models are based on
    deep learning. Here, we only focus on the robustness of the model on a small-sample
    data set, so they classify hyperspectral images based on joint spectral-spatial
    features.
  prefs: []
  type: TYPE_NORMAL
- en: According to the sample size per category in the training data set, the experiment
    is divided into three groups. The first has 10 samples for each category, the
    second has 50 samples for each category and the third has 100 samples for each
    category. At the same time, to ensure the stability of the model, each group of
    experiments is performed ten times, and the training data set is different each
    time. Finally, models are evaluated by average accuracy (AA) and overall accuracy
    (OA).
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Introduction of data sets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pavia University (PaviaU): The Pavia University data set consists of hyperspectral
    images, each with 610*340 pixels and a spatial resolution of 1.3 meters, which
    was taken by the ROSIS sensor above Pavia University in Italy. The spectral imagery
    continuously images 115 wavelengths in the range of 0.43$\sim$0.86 um. Since 12
    of the wavelengths are polluted by noise, each pixel in the final data set contains
    103 bands. It contains 42,776 labeled samples in total, covering 9 objects. In
    addition, its sample size of each object is shown in Table [1](#S4.T1 "Table 1
    ‣ 4.1 Introduction of data sets ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Salinas: The Salinas data set consists of hyperspectral images with 512*217
    pixels and a spatial resolution of 3.7 meters, taken over the Salinas Valley in
    California by the AVIRIS sensor. The spectral imagery continuously images 224
    wavelengths in the range of 0.2$\sim$2.4 um. Since 20 of the bands cannot be reflected
    by water, each pixel in the final data set contains 204 bands. It contains 54,129
    labeled samples in total, covering 16 objects. In addition, its sample size of
    each object is shown in Table [2](#S4.T2 "Table 2 ‣ 4.1 Introduction of data sets
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kennedy Space Center (KSC): The KSC data set was taken at the Kennedy Space
    Center (KSC), above Florida, and used the AVIRIS sensor. Its hyperspectral images
    contain 512*641 pixels, and the spatial resolution is 18 meters. The spectral
    imagery continuously images 224 wavelengths in the range of 400$\sim$2500 nm.
    Similarly, after removing 48 bands that are absorbed by water and have a low signal-to-noise
    ratio, each pixel in the final data set contains 176 bands. It contains 5211 label
    samples, covering 13 objects. Moreover, its sample size of each object is shown
    in Table [3](#S4.T3 "Table 3 ‣ 4.1 Introduction of data sets ‣ 4 Experiments ‣
    A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Table 1: Pavia University. It contains 9 objects. The second column and last
    column represent the name of objects and sample number, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| NO. | Class | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C1 | Asphalt | 6631 |'
  prefs: []
  type: TYPE_TB
- en: '| C2 | Meadows | 18649 |'
  prefs: []
  type: TYPE_TB
- en: '| C3 | Gravel | 2099 |'
  prefs: []
  type: TYPE_TB
- en: '| C4 | Trees | 3064 |'
  prefs: []
  type: TYPE_TB
- en: '| C5 | Painted metal sheets | 1345 |'
  prefs: []
  type: TYPE_TB
- en: '| C6 | Bare Soil | 5029 |'
  prefs: []
  type: TYPE_TB
- en: '| C7 | Bitumen | 1330 |'
  prefs: []
  type: TYPE_TB
- en: '| C8 | Self-Blocking Bricks | 3682 |'
  prefs: []
  type: TYPE_TB
- en: '| C9 | Shadows | 947 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Salinas. It contains 16 objects. The second column and last column
    represent the name of objects and sample number, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| NO. | Class | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C1 | Broccoli green weeds 1 | 2009 |'
  prefs: []
  type: TYPE_TB
- en: '| C2 | Broccoli green weeds 22 | 3726 |'
  prefs: []
  type: TYPE_TB
- en: '| C3 | Fallow | 1976 |'
  prefs: []
  type: TYPE_TB
- en: '| C4 | Fallow rough plow | 1394 |'
  prefs: []
  type: TYPE_TB
- en: '| C5 | Fallow smooth | 2678 |'
  prefs: []
  type: TYPE_TB
- en: '| C6 | Stubble | 3959 |'
  prefs: []
  type: TYPE_TB
- en: '| C7 | Celery | 3579 |'
  prefs: []
  type: TYPE_TB
- en: '| C8 | Grapes untrained | 11271 |'
  prefs: []
  type: TYPE_TB
- en: '| C9 | Soil vineyard develop | 6203 |'
  prefs: []
  type: TYPE_TB
- en: '| C10 | Corn senesced green weeds | 3278 |'
  prefs: []
  type: TYPE_TB
- en: '| C11 | Lettuce romaine 4wk | 1068 |'
  prefs: []
  type: TYPE_TB
- en: '| C12 | Lettuce romaine 5wk | 1927 |'
  prefs: []
  type: TYPE_TB
- en: '| C13 | Lettuce romaine 6wk | 916 |'
  prefs: []
  type: TYPE_TB
- en: '| C14 | Lettuce romaine 7wk | 1070 |'
  prefs: []
  type: TYPE_TB
- en: '| C15 | Vineyard untrained | 7268 |'
  prefs: []
  type: TYPE_TB
- en: '| C16 | Vineyard vertical trellis | 1807 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: KSC. It contains 13 objects. The second column and last column represent
    the name of objects and sample number, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| NO. | Class | Total |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| C1 | Scrub | 761 |'
  prefs: []
  type: TYPE_TB
- en: '| C2 | Willow swamp | 243 |'
  prefs: []
  type: TYPE_TB
- en: '| C3 | Cabbage palm hammock | 256 |'
  prefs: []
  type: TYPE_TB
- en: '| C4 | Cabbage palm/oak hammock | 252 |'
  prefs: []
  type: TYPE_TB
- en: '| C5 | Slash pine | 161 |'
  prefs: []
  type: TYPE_TB
- en: '| C6 | Oak/broadleaf hammock | 229 |'
  prefs: []
  type: TYPE_TB
- en: '| C7 | Hardwood swamp | 105 |'
  prefs: []
  type: TYPE_TB
- en: '| C8 | Graminoid marsh | 431 |'
  prefs: []
  type: TYPE_TB
- en: '| C9 | Spartina marsh | 520 |'
  prefs: []
  type: TYPE_TB
- en: '| C10 | Cattail marsh | 404 |'
  prefs: []
  type: TYPE_TB
- en: '| C11 | Salt marsh | 419 |'
  prefs: []
  type: TYPE_TB
- en: '| C12 | Mud flats | 503 |'
  prefs: []
  type: TYPE_TB
- en: '| C13 | Water | 927 |'
  prefs: []
  type: TYPE_TB
- en: 4.2 Selected models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Some state-of-the-art methods are choose to evaluate their performance. They
    were trained using different platforms, including Caffe, PyTorch, etc. Some platforms
    such Caffe are not well supported by the new development environments. Most models
    are our re-implementations and are trained using the exact same setting. Most
    of the above model settings are based on the original paper, and some are modified
    slightly based on the experiment. All models are trained and tested on the same
    training data set that is picked randomly based on pixels and the test data set,
    and their settings have been optimally tuned. The implementation situation of
    the code is shown in Table [4](#S4.T4 "Table 4 ‣ 4.2 Selected models ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples"). The descriptions of the chosen models are provided in the following
    part.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Originators of model implementations. F denotes that the code of the
    model comes from the original paper. T denotes our implemented model.'
  prefs: []
  type: TYPE_NORMAL
- en: '| S-DMM | SSDL | 3DCAE | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI | SAE_LR |'
  prefs: []
  type: TYPE_TB
- en: '| F | T | F | T | T | T | T | T |'
  prefs: []
  type: TYPE_TB
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SAE_LR [[34](#bib.bib34)]. This is the first paper to introduce the autoencoder
    into hyperspectral image classification, opening a new era of hyperspectral image
    processing. It adopts a raw autoencoder composed of linear layers to extract the
    feature. The size of the neighbor region is $5\times 5$, and the first 4 components
    of PCA are chosen. Subsequently, we can gain a spatial feature vector. Before
    inputting into the model, the raw spatial feature and the spatial feature are
    stacked to form a joint feature. To reduce the difficulty of training, it uses
    a greedy layerwise pretraining method to train each layer, and the parameters
    of the encoder and decoder are symmetric. Then, the encoder concatenates a linear
    classifier for fine tuning. According to [[34](#bib.bib34)], the hidden size is
    set to 60, 20, and 20 for PaviaU, Salinas, and KSC, respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: S-DMM [[121](#bib.bib121)]. This is a relation network that contains an embedding
    module and relation module implemented by 2D convolutional networks. The model
    aims to make samples in the feature space have a small intraclass distance and
    a large interclass distance through a learnable feature embedding function and
    a metric function. After training, all samples will be assigned to the corresponding
    clusters. Finally, a simple KNN is used to classify the query sample. In the experiment,
    the neighbor region of the pixel is fixed as $5\times 5$ and the feature dimension
    is set to 64.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 3DCAE [[40](#bib.bib40)]. This is a 3D convolutional autoencoder adopting a
    3D convolution layer to extract the joint spectral-spatial feature. First, 3DCAE
    is trained by the traditional method, and then, an SVM classifier is adopted to
    classify the hidden features on the top of 3DCAE. In the experiment, the neighbor
    region of the pixel is set to $5\times 5$ and 90% of the samples are used to train
    the 3D autoencoder. There are two different hyperparameter settings corresponding
    to Salinas and PaviaU, and the model has not been tested on KSC in [[121](#bib.bib121)].
    Therefore, on the KSC, the model uses the same hyperparameter configuration as
    on the Salinas because they are collected by the same sensor.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSDL [[37](#bib.bib37)]. This is a typical two-stream structure extracting the
    spectral and spatial feature separately through two different branches and merging
    them at the end. Inspired by [[34](#bib.bib34)], the author adopts a 1D autoencoder
    to extract the spectral feature. In the branch of spatial feature extraction,
    the model uses a spatial pyramid pooling layer to replace the traditional pooling
    layer on the top convolutional layer. The spatial pyramid pooling layer enables
    the deep convolutional neural network to generate a fixed-length feature. On the
    one hand, it enables the model to convert the input of different sizes into a
    fixed-length, which is good for the module that is sensitive to the input size;
    on the other hand, it is useful for the model to better adapt to objects of different
    scales, and the output will include features from coarse to fine, achieving multiscale
    feature fusion. Then, a simple logistic classifier is used to classify the spectra-spatial
    feature. In the experiment, 80% of the data are used to train the autoencoder
    through the method of greedy layer-wise pretraining. Moreover, in the spatial
    branch, the size of the neighbor region is set to 42*42 and PCA is used to extract
    the first component. Then, the overall model is trained together.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TwoCnn [[122](#bib.bib122)]. This is a two-stream structure based on fine-tuning.
    In the spectral branch, it adopts a 1D convolutional layer to capture local information
    of spectral features, which is entirely different from SSDL. In particular, transfer
    learning is used to pretrain parameters of the model and endow it with good robustness
    on limited samples. The pairs of the source data set and target data set are Pavia
    Center–PavaU, Indian pines-Salinas, and Indian pines-KSC. In [[122](#bib.bib122)],
    they also did not test the model on KSC. Thus, we regard Indian pines as the source
    domain for KSC, given that both data sets come from the same type of sensor. The
    neighbor region of the pixel is set to 21*21\. Additionally, it averages along
    the spectral channel to reduce the input dimension, instead of PCA. In the pretraining
    process, 15% of samples of each category of Pavia and 90% of samples of each category
    of Indian pines are treated as the training data set, and the rest serve as the
    test data set. To make the number of bands in the source data set and target data
    set the same, we filter out the band that has the smaller variance. According
    to [[122](#bib.bib122)], all other layers are transferred except for the softmax
    layer. Finally, the model is fine-tuned on the target data set with the same configuration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3DVSCNN [[123](#bib.bib123)]. This is a general CNN-based image classification
    model, but it uses a 3D convolutional network to extract spectral-spatial features
    simultaneously followed by a fully connected network for classification. The main
    idea of [[123](#bib.bib123)] is the usage of active learning. The process can
    be divided into two steps: the selection of valuable samples and the training
    of the model. In [[123](#bib.bib123)], an SVM serves as a selector to iteratively
    select some of the most valuable samples according to Eq.([10](#S3.E10 "In 3.2
    Deep Active Learning for HSI classification ‣ 3 Deep learning paradigms for HSI
    classification with few labeled samples ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples")). Then, the 3DVSCNN is trained
    on the valuable data set. The size of its neighbor region is set to 13*13\. During
    data preprocessing, it uses PCA to extract the top 10 components for PaviaU and
    Salinas, and the top 30 components for KSC, which contain more than 99% of the
    original spectral information and still keep a clear spatial geometry. In the
    experiment, 80% of samples will be picked by the SVM to form a valuable data set
    for 4 samples in each iteration. Then, the model is trained on the valuable data
    set.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: CNN_HSI [[28](#bib.bib28)]. The model combines multilayer $1\times 1$ 2D convolutions
    followed by local response normalization to capture the feature of hyperspectral
    images. To avoid the loss of information after PCA, it uses 2D convolution to
    extract spectral and spatial joint features directly, instead of 3D convolution.
    At the same time, it also adopts a dropout layer and data augmentation, including
    rotation and flipping, to improve the generalization of the model and reduce overfitting.
    After data augmentation, an image can generate eight different orientation images.
    Moreover, the model removes the linear classifier to decrease the number of trainable
    parameters. According to [[28](#bib.bib28)], the dropout rate is set to 0.6, the
    size of the neighbor region is $5\times 5$, and the batch size is 16 in the experiment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSLstm [[75](#bib.bib75)]. Unlike the above methods, SSLstm adopts recurrent
    networks to process spectral and spatial features simultaneously. In the spectral
    branch, called SeLstm, the spectral vector is seen as a sequence. In the spatial
    branch, called SaLstm, it treats each line of the image patch as a sequence element.
    Therefore, along the column direction, the image patch can be well converted into
    a sequence. In particular, it fuses the predictions of the two branches in the
    label space to obtain the final prediction result, which is defined as
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $\begin{split}P(y=j&#124;x_{i})=w_{spe}P_{spe}(y=j&#124;x_{i})+w_{spa}P_{spa}(y=j&#124;x_{i})\end{split}$
    |  | (14) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $P(y=j|x_{i})$ denotes the final posterior probability, $P_{spe}(y=j|x_{i})$
    and $P_{spa}(y=j|x_{i})$ denote the posterior probabilities from spectral and
    spatial modules, respectively, and $w_{spe}$ and $w_{spa}$ are fusion weights
    that satisfy the sum of 1\. In the experiment, the size of the neighbor region
    is set to 32*32 for PaviaU and Salinas. In addition, for KSC, it is set to 64*64\.
    Next, the first component of PCA is reserved on all data sets. The number of hidden
    nodes of the spectral branch and the spatial branch are 128 and 256, respectively.
    In addition, $w_{spe}$ and $w_{spa}$ are set to 0.5 and 0.5 separately.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4.3 Experimental results and analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The accuracy of the test data set is shown in Table [5](#S4.T5 "Table 5 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples"), Table [6](#S4.T6
    "Table 6 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep
    Learning for Hyperspectral Image Classification with Few Labeled Samples"), and
    Table [7](#S4.T7 "Table 7 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples"). Corresponding classification maps are shown in Figure [11](#S4.F11
    "Figure 11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")$\sim$[19](#S4.F19
    "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples").
    The final classification result of the pixel is decided by the voting result of
    10 experiments.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Taking Table [5](#S4.T5 "Table 5 ‣ 4.3 Experimental results and analysis ‣
    4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") as an example, the experiment is divided into three
    groups, and the sample sizes in each group are 10, 50, and 100, respectively.
    The aforementioned models are conducted 10 times in every experiment sets. Then,
    we count the average of their class classification accuracy, AA, and OA for comparing
    their performance. When sample size is 10, S-DMM has the highest AA and OA, which
    are 91.08% and 84.45% respectively, in comparison with the AA and OA of 71.58%
    and 60.00%, 75.34 % and 74.79%, 74.60% and 78.61%, 75.64% and 75.17%, 72.77% and
    69.59%, 85.12% and 82.13%, 72.40% and 66.05% for 3DCAE, SSDL, TwoCnn, 3DVSCNN,
    SSLstm, CNN_HSI and SAE_LR. Besides, S-DMM has the largest number of class classification
    accuracy. When the sample size is 50, S-DMM and CNN_HSI have the highest AA and
    OA respectively, which are 96.47% and 95.21%. In the last group, 3DVSCNN and CNN_HSI
    have the highest AA and OA, which are 97.13% and 97.35%. According to the other
    two tables, we can conclude with a similar result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Table [5](#S4.T5 "Table 5 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples"), Table [6](#S4.T6 "Table 6 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") and Table [7](#S4.T7 "Table 7 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples"), we can conclude
    that most models’ performance on KSC, except for 3DCAE, is better than the other
    two data sets. Especially when the data set contains few samples, the accuracy
    of S-DMM is up to 94%, superior to other data sets. This is because the surface
    objects on the KSC itself have a discriminating border between each other, regardless
    of its higher spatial resolution than that of the other data sets, as shown in
    Figure [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples")$\sim$[19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples"). In the other data sets, models easily misclassify
    the objects that have a similar spatial structure, as illustrated in Meadows (class
    2) and Bare soil (class 6) in PaviaU and Fallow rough plow (class 4) and Grapes
    untrained (class 8) in Salinas, as shown in [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples")$\sim$[16](#S4.F16 "Figure 16 ‣
    4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples"). The accuracy
    of all models on Grapes untrained is lower than other classes in Salinas. In Figure [10](#S4.F10
    "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples"),
    on all data sets, as the number of samples increases, the accuracy of all models
    will improve together.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [10](#S4.F10 "Figure 10 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples"), when the sample size of each category is 10, S-DMM
    and CNN_HSI have achieved stable and excellent performance on all data sets. They
    are not sensitive to the size of the data set. In Figure [10](#S4.F10 "Figure
    10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") and Figure [10](#S4.F10
    "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples"),
    with increasing sample size, the accuracy of S-DMM and CNN_HSI have improved slightly,
    but their increase is lower than that of others. In Figure [10](#S4.F10 "Figure
    10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples"), when the sample
    size increases from 50 to 100, we can obtain the same conclusion. This result
    shows that both of them can be applied to solve the small-sample problem in hyperspectral
    images. Especially for S-DMM, it has gained the best performance on the metric
    of AA and OA on Salinas and KSC in the experiment with a sample size of 10\. On
    PaviaU, it still wins the third place. This result also proves that it can work
    well on a few samples. Although TwoCnn, 3DVSCNN, and SSLstm achieve good performance
    on all data sets, when the data set contains fewer samples, they will not work
    well. It is worth mentioning that 3DVSNN uses fewer samples to train than other
    models for selecting valuable samples. This approach may not be beneficial for
    those classes with few samples. As shown in [7](#S4.T7 "Table 7 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples"), 3DVSCNN has a good performance
    on OA, but a bad performance on AA. For class 7, when its sample size increases
    from 10 to 50 and 100, its accuracy drops. This is because the total sample size
    of it is the smallest on KSC. Therefore, it contains few valuable samples. Moreover,
    the step of selecting valuable samples would cause an imbalance between the classes,
    which leads to the accuracy of class 7 decreasing. On almost all data sets, autoencoder-based
    models achieve poor performance compared with other models. Although unsupervised
    learning does not need to label samples, if there are no constraints, the autoencoder
    might actually learn nothing. Moreover, since it has a symmetric architecture,
    it would result in a vast number of parameters and increase the difficulty of
    training. Therefore, SSDL and SAE_LR use a greedy layerwise pretraining method
    to solve this problem. However, 3DCAE does not adopt this approach and achieves
    the worst performance on all data sets. As shown in Figure [10](#S4.F10 "Figure
    10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples"), it still has
    considerable room for improvement.'
  prefs: []
  type: TYPE_NORMAL
- en: Overall, classification results based on few-shot learning, active learning,
    transfer learning, and data augmentation are better than autoencoder-based unsupervised
    learning methods on the limited sample in all experiments. Few-shot learning benefits
    from the exploration of the relationship between samples to find a discriminative
    decision boarder. Active learning benefits from the selection of valuable samples,
    which enables the model to focus more attention to indistinguishable samples.
    Transfer learning makes good use of the similarity between different data sets,
    which reduces the quantity of data required for training and trainable parameters,
    improving the model’s robustness. According to raw data, the method of data augmentation
    generates more samples to expand the diversity of samples. Although the autoencoder
    can learn the internal structure of the unlabeled data set, the final feature
    representation might not have task-related characteristics. This is the reason
    why its performance on a small-sample data set is inferior to supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: PaviaU. Classification accuracy obtained by S-DMM [[121](#bib.bib121)],
    3DCAE [[40](#bib.bib40)], SSDL [[37](#bib.bib37)], TwoCnn [[122](#bib.bib122)],
    3DVSCNN [[123](#bib.bib123)], SSLstm [[75](#bib.bib75)], CNN_HSI [[28](#bib.bib28)]
    and SAE_LR [[34](#bib.bib34)] on PaviaU. The best accuracies are marked in bold.
    The ”size” in the first line denotes the sample size per category.'
  prefs: []
  type: TYPE_NORMAL
- en: '| size | classes | S-DMM | 3DCAE | SSDL | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI
    | SAE_LR |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 94.34 | 49.41 | 68.33 | 71.80 | 63.03 | 72.59 | 84.60 | 66.67 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 73.13 | 51.60 | 72.94 | 88.27 | 69.22 | 68.86 | 67.57 | 56.68 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 86.85 | 54.06 | 53.71 | 47.58 | 71.77 | 48.08 | 72.80 | 46.37 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 95.04 | 94.81 | 88.58 | 96.29 | 85.10 | 79.06 | 93.65 | 80.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 99.98 | 99.86 | 97.21 | 94.99 | 98.61 | 93.80 | 99.84 | 98.81 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 85.58 | 57.40 | 66.21 | 49.75 | 75.17 | 62.53 | 78.35 | 55.87 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 98.55 | 80.34 | 68.17 | 58.65 | 65.61 | 65.39 | 92.14 | 81.42 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 86.47 | 57.97 | 64.07 | 66.95 | 55.77 | 67.60 | 78.17 | 66.83 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.81 | 98.76 | 98.83 | 97.15 | 96.48 | 97.02 | 98.92 | 98.90 |'
  prefs: []
  type: TYPE_TB
- en: '| AA | 91.08 | 71.58 | 75.34 | 74.60 | 75.64 | 72.77 | 85.12 | 72.40 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 84.55 | 60.00 | 74.79 | 78.61 | 75.17 | 69.59 | 82.13 | 66.05 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 1 | 97.08 | 80.76 | 76.11 | 88.50 | 90.60 | 82.96 | 93.66 | 78.83 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 90.09 | 63.14 | 87.39 | 86.43 | 93.68 | 82.42 | 94.82 | 65.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 95.15 | 62.57 | 70.28 | 69.21 | 90.64 | 81.59 | 94.87 | 65.50 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 97.35 | 97.33 | 89.27 | 98.80 | 93.47 | 91.31 | 94.49 | 92.43 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 100.00 | 100.00 | 98.14 | 99.81 | 99.92 | 99.67 | 100.00 | 99.47 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 96.32 | 80.15 | 75.12 | 84.93 | 94.15 | 82.58 | 88.14 | 72.30 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 99.31 | 88.45 | 75.80 | 83.12 | 94.98 | 92.34 | 97.21 | 86.04 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 92.97 | 75.11 | 70.57 | 83.57 | 91.55 | 84.75 | 87.52 | 79.74 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.98 | 99.69 | 99.61 | 99.91 | 98.72 | 99.39 | 99.78 | 99.29 |'
  prefs: []
  type: TYPE_TB
- en: '| AA | 96.47 | 83.02 | 82.48 | 88.25 | 94.19 | 88.56 | 94.50 | 82.10 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 94.04 | 64.17 | 84.92 | 90.69 | 94.23 | 84.50 | 95.21 | 77.42 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 1 | 97.11 | 83.05 | 85.59 | 92.21 | 94.38 | 90.84 | 94.44 | 78.64 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 91.64 | 73.45 | 86.17 | 76.86 | 95.90 | 83.26 | 97.75 | 74.28 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 94.23 | 73.02 | 80.29 | 72.24 | 95.96 | 80.66 | 95.37 | 79.87 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 98.70 | 97.87 | 97.14 | 99.28 | 97.65 | 92.54 | 95.88 | 93.54 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 100.00 | 100.00 | 99.06 | 99.89 | 99.95 | 99.57 | 99.99 | 99.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 93.51 | 86.82 | 83.16 | 95.90 | 97.92 | 87.61 | 91.01 | 69.83 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 99.21 | 90.17 | 94.08 | 89.88 | 98.39 | 93.45 | 98.37 | 89.42 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 92.73 | 88.31 | 88.43 | 90.03 | 94.21 | 90.08 | 92.41 | 85.05 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.99 | 99.82 | 99.65 | 99.98 | 99.85 | 99.80 | 99.70 | 99.55 |'
  prefs: []
  type: TYPE_TB
- en: '| AA | 96.35 | 88.06 | 90.40 | 90.70 | 97.13 | 90.87 | 96.10 | 85.49 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 94.65 | 70.15 | 89.33 | 94.76 | 97.05 | 87.19 | 97.35 | 81.44 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 6: Salinas. Classification accuracy obtained by S-DMM [[121](#bib.bib121)],
    3DCAE [[40](#bib.bib40)], SSDL [[37](#bib.bib37)], TwoCnn [[122](#bib.bib122)],
    3DVSCNN [[123](#bib.bib123)], SSLstm [[75](#bib.bib75)], CNN_HSI [[28](#bib.bib28)]
    and SAE_LR [[34](#bib.bib34)] on Salinas. The best accuracies are marked in bold.
    The ”size” in the first line denotes the sample size per category.'
  prefs: []
  type: TYPE_NORMAL
- en: '| size | classes | S-DMM | 3DCAE | SSDL | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI
    | SAE_LR |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 99.45 | 99.28 | 76.01 | 88.22 | 97.92 | 79.38 | 98.80 | 86.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 99.21 | 59.04 | 69.24 | 78.09 | 99.71 | 72.49 | 98.77 | 44.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 96.70 | 66.54 | 69.89 | 74.80 | 95.09 | 86.83 | 95.48 | 44.72 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 99.56 | 98.65 | 94.96 | 98.19 | 99.28 | 99.45 | 98.36 | 97.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 97.12 | 81.94 | 89.43 | 96.54 | 93.35 | 94.95 | 92.55 | 83.93 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 89.64 | 98.52 | 96.19 | 98.96 | 99.81 | 93.65 | 99.96 | 87.28 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 99.82 | 97.31 | 76.83 | 92.52 | 96.73 | 87.82 | 99.61 | 96.94 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 70.53 | 68.11 | 42.58 | 54.35 | 67.89 | 61.64 | 77.51 | 41.58 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.02 | 95.06 | 89.58 | 81.22 | 99.42 | 90.47 | 97.19 | 78.45 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 91.13 | 9.43 | 76.40 | 75.18 | 91.75 | 86.66 | 89.23 | 30.75 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 97.56 | 72.26 | 93.04 | 92.26 | 95.26 | 91.37 | 95.45 | 23.52 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 99.87 | 72.16 | 86.60 | 86.40 | 96.65 | 95.38 | 99.96 | 82.63 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 99.25 | 99.78 | 95.46 | 98.18 | 96.64 | 96.90 | 99.22 | 92.88 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 96.30 | 89.93 | 90.50 | 96.10 | 99.68 | 91.68 | 96.80 | 62.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 72.28 | 56.98 | 65.40 | 55.60 | 83.86 | 75.55 | 72.03 | 57.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 95.29 | 44.35 | 75.89 | 92.39 | 92.03 | 88.43 | 94.07 | 76.75 |'
  prefs: []
  type: TYPE_TB
- en: '| AA | 93.92 | 75.58 | 80.50 | 84.94 | 94.07 | 87.04 | 94.06 | 67.91 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 89.69 | 71.50 | 74.29 | 77.54 | 90.18 | 81.20 | 91.31 | 67.43 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 1 | 99.97 | 98.81 | 92.70 | 97.99 | 99.99 | 94.18 | 99.20 | 85.37 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 99.84 | 86.97 | 88.30 | 91.35 | 99.94 | 92.34 | 99.57 | 92.51 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 99.84 | 54.83 | 87.50 | 94.87 | 99.74 | 97.02 | 99.62 | 81.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 99.93 | 98.87 | 99.41 | 99.96 | 99.89 | 99.95 | 99.63 | 98.40 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 99.40 | 95.62 | 95.83 | 98.96 | 99.38 | 98.34 | 98.79 | 95.12 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 99.92 | 99.62 | 98.95 | 99.87 | 100.00 | 98.78 | 99.98 | 98.86 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 99.92 | 98.17 | 96.47 | 96.60 | 99.85 | 97.80 | 99.78 | 98.55 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 68.92 | 81.74 | 62.99 | 68.05 | 85.35 | 77.17 | 77.93 | 46.04 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.76 | 94.87 | 95.34 | 86.01 | 99.99 | 96.15 | 99.71 | 94.84 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 97.18 | 12.87 | 95.31 | 93.94 | 98.23 | 97.23 | 97.33 | 77.69 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 99.57 | 75.82 | 97.73 | 97.10 | 98.59 | 97.71 | 99.54 | 77.14 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 99.90 | 58.18 | 97.51 | 97.16 | 99.89 | 98.88 | 99.84 | 96.87 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 99.84 | 99.98 | 98.55 | 98.60 | 100.00 | 99.12 | 99.87 | 97.33 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 98.15 | 93.80 | 97.54 | 99.37 | 99.91 | 99.24 | 99.53 | 91.49 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 76.12 | 41.84 | 69.04 | 67.21 | 88.77 | 86.24 | 83.39 | 65.15 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 98.87 | 69.00 | 94.34 | 97.78 | 98.55 | 97.64 | 98.15 | 91.94 |'
  prefs: []
  type: TYPE_TB
- en: '| AA | 96.07 | 78.81 | 91.72 | 92.80 | 98.00 | 95.49 | 96.99 | 86.78 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 90.92 | 74.73 | 85.79 | 87.01 | 95.30 | 91.37 | 95.08 | 79.49 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 1 | 99.86 | 98.81 | 98.22 | 98.74 | 99.99 | 97.86 | 99.77 | 92.44 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 99.74 | 91.88 | 96.54 | 96.70 | 99.99 | 97.74 | 99.86 | 89.46 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 99.99 | 63.20 | 95.40 | 97.47 | 99.16 | 98.91 | 99.79 | 92.05 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 99.84 | 99.12 | 99.29 | 99.95 | 99.85 | 99.78 | 99.44 | 99.03 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 99.58 | 98.24 | 98.09 | 99.61 | 99.70 | 98.89 | 99.54 | 96.32 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 99.99 | 99.95 | 99.12 | 99.79 | 100.00 | 99.62 | 100.00 | 98.96 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 99.93 | 98.71 | 97.14 | 97.94 | 99.88 | 98.97 | 99.86 | 98.42 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 67.88 | 71.43 | 59.51 | 66.83 | 90.54 | 86.00 | 79.90 | 39.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.81 | 95.51 | 94.87 | 90.65 | 99.98 | 98.15 | 99.75 | 96.34 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 96.54 | 22.92 | 96.97 | 96.21 | 97.77 | 98.55 | 97.29 | 84.35 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 99.75 | 76.67 | 99.28 | 99.25 | 99.82 | 99.39 | 99.70 | 92.76 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 100.00 | 64.12 | 99.39 | 98.01 | 99.99 | 99.84 | 99.99 | 96.97 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 99.87 | 99.98 | 98.74 | 99.34 | 99.98 | 99.38 | 99.75 | 97.48 |'
  prefs: []
  type: TYPE_TB
- en: '| 14 | 98.66 | 94.73 | 98.62 | 99.72 | 99.91 | 99.44 | 99.67 | 93.52 |'
  prefs: []
  type: TYPE_TB
- en: '| 15 | 78.73 | 63.65 | 83.03 | 70.16 | 91.31 | 86.77 | 91.86 | 69.09 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 99.27 | 79.70 | 96.65 | 99.26 | 99.26 | 98.69 | 99.10 | 93.21 |'
  prefs: []
  type: TYPE_TB
- en: '| AA | 96.21 | 82.41 | 94.43 | 94.35 | 98.57 | 97.37 | 97.83 | 89.38 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 91.56 | 76.61 | 88.67 | 90.25 | 96.89 | 94.41 | 96.28 | 81.95 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: KSC. Classification accuracy obtained by S-DMM [[121](#bib.bib121)],
    3DCAE [[40](#bib.bib40)], SSDL [[37](#bib.bib37)], TwoCnn [[122](#bib.bib122)],
    3DVSCNN [[123](#bib.bib123)], SSLstm [[75](#bib.bib75)], CNN_HSI [[28](#bib.bib28)]
    and SAE_LR [[34](#bib.bib34)] on KSC. The best accuracies are marked in bold.
    The ”size” in the first line denotes the sample size per category.'
  prefs: []
  type: TYPE_NORMAL
- en: '| size | classes | S-DMM | 3DCAE | SSDL | TwoCnn | 3DVSCNN | SSLstm | CNN_HSI
    | SAE_LR |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 1 | 93.49 | 35.46 | 79.21 | 67.11 | 95.33 | 73.58 | 92.17 | 83.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 89.74 | 49.40 | 67.68 | 58.37 | 40.39 | 68.45 | 81.67 | 69.01 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 95.16 | 40.41 | 76.87 | 77.20 | 75.41 | 81.59 | 86.91 | 50.61 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 58.72 | 5.54 | 70.33 | 75.12 | 35.87 | 76.16 | 60.83 | 20.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 87.95 | 33.38 | 81.26 | 88.08 | 47.42 | 87.22 | 64.37 | 23.11 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 93.42 | 51.05 | 79.18 | 66.44 | 64.29 | 76.71 | 66.16 | 45.39 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 98.63 | 16.32 | 95.26 | 92.74 | 57.79 | 96.42 | 96.00 | 63.58 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 97.93 | 46.44 | 72.42 | 61.92 | 71.88 | 52.95 | 85.77 | 58.05 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 94.88 | 86.25 | 87.00 | 92.31 | 79.00 | 90.65 | 91.06 | 76.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 98.12 | 8.76 | 72.59 | 86.27 | 56.57 | 89.04 | 85.13 | 63.12 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 97.51 | 76.21 | 88.68 | 78.17 | 86.99 | 89.32 | 95.60 | 89.98 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 93.69 | 8.54 | 83.65 | 78.09 | 60.79 | 83.96 | 89.66 | 69.59 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 100.00 | 46.95 | 99.98 | 100.00 | 84.92 | 100.00 | 99.95 | 97.90 |'
  prefs: []
  type: TYPE_TB
- en: '| AA | 92.25 | 38.82 | 81.09 | 78.60 | 65.90 | 82.00 | 84.25 | 62.36 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 94.48 | 49.73 | 83.71 | 82.29 | 77.40 | 83.07 | 91.13 | 72.68 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 1 | 97.99 | 22.53 | 96.12 | 72.95 | 98.45 | 96.77 | 94.40 | 88.21 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 98.24 | 30.98 | 94.56 | 94.04 | 39.90 | 98.19 | 91.50 | 78.50 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 98.69 | 45.10 | 96.55 | 90.10 | 99.13 | 99.47 | 94.47 | 83.06 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 78.22 | 3.86 | 93.51 | 92.33 | 74.01 | 98.32 | 76.49 | 43.07 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 92.16 | 40.54 | 96.94 | 97.12 | 64.32 | 99.55 | 87.03 | 53.33 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 98.49 | 62.07 | 96.70 | 93.80 | 77.21 | 99.72 | 70.89 | 51.90 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 98.36 | 18.00 | 99.64 | 97.82 | 20.36 | 100.00 | 98.00 | 84.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 99.21 | 43.04 | 91.92 | 90.60 | 96.25 | 97.40 | 93.86 | 77.77 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.96 | 89.77 | 98.57 | 89.55 | 63.91 | 98.83 | 98.77 | 86.47 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 99.92 | 12.12 | 93.70 | 95.56 | 54.72 | 99.52 | 91.67 | 85.28 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 98.62 | 80.38 | 97.86 | 98.40 | 90.95 | 99.11 | 87.75 | 96.56 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 99.07 | 19.85 | 94.99 | 95.01 | 87.37 | 99.67 | 89.54 | 82.19 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 100.00 | 91.24 | 100.00 | 90.00 | 96.77 | 99.46 | 98.95 | 99.44 |'
  prefs: []
  type: TYPE_TB
- en: '| AA | 96.84 | 43.04 | 96.24 | 92.10 | 74.10 | 98.92 | 90.25 | 77.73 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 98.68 | 54.01 | 96.88 | 96.61 | 96.03 | 98.72 | 97.39 | 84.93 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 1 | 98.17 | 19.03 | 97.41 | 96.51 | 98.94 | 99.74 | 93.93 | 89.77 |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 98.74 | 34.13 | 98.60 | 99.58 | 56.50 | 99.79 | 89.93 | 80.77 |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 99.55 | 57.18 | 96.67 | 99.42 | 99.81 | 99.23 | 98.33 | 82.88 |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | 88.29 | 1.38 | 97.96 | 98.68 | 88.29 | 99.14 | 85.86 | 53.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | 93.11 | 52.46 | 99.51 | 100.00 | 76.23 | 100.00 | 93.77 | 58.52 |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | 99.61 | 59.77 | 98.68 | 97.36 | 80.62 | 99.53 | 74.96 | 58.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | 100.00 | 8.00 | 100.00 | 100.00 | 32.00 | 100.00 | 98.00 | 86.00 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 99.79 | 51.81 | 95.53 | 98.07 | 98.91 | 99.40 | 97.37 | 83.96 |'
  prefs: []
  type: TYPE_TB
- en: '| 9 | 99.74 | 87.40 | 98.74 | 98.74 | 63.93 | 99.12 | 99.76 | 91.95 |'
  prefs: []
  type: TYPE_TB
- en: '| 10 | 100.00 | 13.16 | 98.22 | 99.61 | 72.47 | 100.00 | 97.70 | 91.28 |'
  prefs: []
  type: TYPE_TB
- en: '| 11 | 99.91 | 83.76 | 99.06 | 99.97 | 94.42 | 99.81 | 99.84 | 97.81 |'
  prefs: []
  type: TYPE_TB
- en: '| 12 | 99.33 | 24.94 | 97.99 | 99.03 | 94.32 | 99.80 | 95.31 | 85.73 |'
  prefs: []
  type: TYPE_TB
- en: '| 13 | 100.00 | 90.07 | 99.96 | 99.94 | 97.62 | 99.94 | 99.85 | 99.58 |'
  prefs: []
  type: TYPE_TB
- en: '| AA | 98.17 | 44.85 | 98.33 | 98.99 | 81.08 | 99.65 | 94.20 | 81.57 |'
  prefs: []
  type: TYPE_TB
- en: '| OA | 98.96 | 59.63 | 98.75 | 99.15 | 98.55 | 99.68 | 98.05 | 89.15 | ![Refer
    to caption](img/dd49ff06da46a1a02251de8a056d0c52.png)![Refer to caption](img/fec4c877383056b9f66d72dacbbfbfa0.png)![Refer
    to caption](img/e5b192702570579d7b9d4cbc39d56eb6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Change in accuracy over the number of samples for each category.
    [10](#S4.F10 "Figure 10 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") PaviaU. [10](#S4.F10 "Figure 10 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") Salinas. [10](#S4.F10 "Figure 10 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") KSC.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/457af78222db50938fbf59246aa04851.png)![Refer to caption](img/0c422b45dbb99a47fe8818abaa9a3879.png)![Refer
    to caption](img/79be6492ac53069022d2deda479fbeb8.png)![Refer to caption](img/234d6333038b37f01c264dc204d8ea48.png)![Refer
    to caption](img/be9d1b5bbb273db4bce02c800b2eb878.png)![Refer to caption](img/4cd2c9522f788b687e95aafbf8a1e21d.png)![Refer
    to caption](img/21c2936dc0be3018615d7038412f5e26.png)![Refer to caption](img/97dde8237dd40ec84f311976a9c82111.png)![Refer
    to caption](img/2fbf03b337299db99905d2b6b8766d07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Classification maps on the PaviaU data set (10 samples per class).
    [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [11](#S4.F11 "Figure 11 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [11](#S4.F11
    "Figure 11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [11](#S4.F11 "Figure 11 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [11](#S4.F11 "Figure
    11 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/53210bbe28e3269788ae65e7f8383140.png)![Refer to caption](img/2e5d9057629e38657b69bbd1ddead6f9.png)![Refer
    to caption](img/5f28e554f52eda3dac9789320aa6283c.png)![Refer to caption](img/7eaf321881768657931cb971003bed7c.png)![Refer
    to caption](img/54b2b9d40e8b1e99628ea10e4a2c3134.png)![Refer to caption](img/45d3317fa95c44f835cf911bdb9d54b3.png)![Refer
    to caption](img/a2bd0045490d2c7022a5f4c6d55fe3d9.png)![Refer to caption](img/825426001b1254f6e45aa612ac28ada3.png)![Refer
    to caption](img/14c842b14a5a9b63f7fee0d21c1c3deb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Classification maps on the PaviaU data set (50 samples per class).
    [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [12](#S4.F12 "Figure 12 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [12](#S4.F12
    "Figure 12 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [12](#S4.F12 "Figure 12 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [12](#S4.F12 "Figure
    12 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cf8375b94847966f4e8a483f02beed18.png)![Refer to caption](img/048b8c8f336f995572908d2c38615310.png)![Refer
    to caption](img/b5ee6228a621a20a69b78ea9fedfd0df.png)![Refer to caption](img/3fc5acccf04b233ccc949c2c1a9c24b9.png)![Refer
    to caption](img/85f13326cc7ac671145cc8576995abf1.png)![Refer to caption](img/c0ef83f8142ae573fac9f4cfd17744ca.png)![Refer
    to caption](img/fda22dcd985a6443be87fd8c9e74aa67.png)![Refer to caption](img/b82acf4cd7f4b94e491782fd8d5f4522.png)![Refer
    to caption](img/6b0df880e1e37eea17a2721c650e3a46.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Classification maps on the PaviaU data set (100 samples per class).
    [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [13](#S4.F13 "Figure 13 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [13](#S4.F13
    "Figure 13 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [13](#S4.F13 "Figure 13 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [13](#S4.F13 "Figure
    13 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/45f3f33aff344f770282e858b6b081e9.png)![Refer to caption](img/242379d2501a01c140fcbff386baefdc.png)![Refer
    to caption](img/eb768fabec326ef59daa82f5d4c3b7d4.png)![Refer to caption](img/ca86cd6e1e9027023bc4b40ebe489a2b.png)![Refer
    to caption](img/8ebd28a970b41f23cb1ce8a3eebc3dc3.png)![Refer to caption](img/439af2f5c20f569f00bf4aef8dcca610.png)![Refer
    to caption](img/905bb4c3080b1df23a7ab060c65c6be9.png)![Refer to caption](img/ffaa74c07706ffb9ee72c8241a89705c.png)![Refer
    to caption](img/a38fe27b7ce0d8705579d4b2e1f6915c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Classification maps on the Salinas data set (10 samples per class).
    [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [14](#S4.F14 "Figure 14 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [14](#S4.F14
    "Figure 14 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [14](#S4.F14 "Figure 14 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [14](#S4.F14 "Figure
    14 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e54fc7491f1ad912af2746de63e9674.png)![Refer to caption](img/a7ea03cfa709e3633938258288b9c95f.png)![Refer
    to caption](img/63a7eb0f2bf96cc0324d45239304ad44.png)![Refer to caption](img/0fcf4987788c27d206b8674175963d8a.png)![Refer
    to caption](img/d90a130e74206516e326a60845970635.png)![Refer to caption](img/ac353133206ebe2e7ea6e071f2bb5583.png)![Refer
    to caption](img/a543deb9002e7dc3aea22ad62b7a7e12.png)![Refer to caption](img/76e7608b9347d8dc920fb7ef4ab45ca8.png)![Refer
    to caption](img/11eae55fc0f61baf7601ab865f5130ad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Classification maps on the Salinas (50 samples per class). [15](#S4.F15
    "Figure 15 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    Original. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental results and analysis ‣ 4
    Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification with
    Few Labeled Samples") S-DMM. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [15](#S4.F15 "Figure 15 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [15](#S4.F15
    "Figure 15 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [15](#S4.F15 "Figure 15 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [15](#S4.F15 "Figure
    15 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/476033eeec589663cdf974c38fec18de.png)![Refer to caption](img/80e5b13c3dcbfdc953d5080d62d8f19a.png)![Refer
    to caption](img/904489726a8c81a2b589e95027cfe590.png)![Refer to caption](img/498879c5b8bce7e86885bcc27268287b.png)![Refer
    to caption](img/04b51f7573fe53c6621c2ce44e316c22.png)![Refer to caption](img/7df0da058528a9bbaa9d61d54fcab08b.png)![Refer
    to caption](img/e5ae726db365fd54f979aa80bd53accd.png)![Refer to caption](img/6d062e82b8c7fa03d097bd2ca5483391.png)![Refer
    to caption](img/62dfeef7d2a99f616559b8179ad9ac81.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Classification maps on the Salinas data set (100 samples per class).
    [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [16](#S4.F16 "Figure 16 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [16](#S4.F16
    "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [16](#S4.F16 "Figure 16 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [16](#S4.F16 "Figure
    16 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/130655677ba4dc1b690dc23b7b16c9e6.png)![Refer to caption](img/0e0dee93ddec946989c833e4455c2dd6.png)![Refer
    to caption](img/8ddf84d4743f60deaa1c747a68b515da.png)![Refer to caption](img/e24515612a1c1d0cc588e93e19bd8adf.png)![Refer
    to caption](img/ba8d01cac7fe4fe0917e5b18cefa0ebd.png)![Refer to caption](img/cf5418fcca5c4b0525b8941da92137c9.png)![Refer
    to caption](img/8df95f2cebc9d6ac8e19f77ba4e24feb.png)![Refer to caption](img/b5c9b1730a2d46fe8155f36f5d14c94c.png)![Refer
    to caption](img/8032aae21ea33c850ea46afcaabd5d64.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: Classification maps on the KSC data set (10 samples per class).
    [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [17](#S4.F17 "Figure 17 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [17](#S4.F17
    "Figure 17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [17](#S4.F17 "Figure 17 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [17](#S4.F17 "Figure
    17 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0265b16b8037b38b2834f536f94418e7.png)![Refer to caption](img/4fd942dbdf13abfcf772862bd3a68f4b.png)![Refer
    to caption](img/88d11be55d09ef23769e40a392f3f64c.png)![Refer to caption](img/245a69d4fc6fe8c92a9033e5647a060f.png)![Refer
    to caption](img/8ef8f782c257d0ccaf7559e5b484658c.png)![Refer to caption](img/c59378f92391d212ba10465dd1b76b75.png)![Refer
    to caption](img/d19f74eb09888632ff365893fc58b37b.png)![Refer to caption](img/24c70588a75199fcc3c5307360ac6069.png)![Refer
    to caption](img/e97b56a307863e8b2c05ca07c54fdd8d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: Classification maps on the KSC data set (50 samples per class).
    [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [18](#S4.F18 "Figure 18 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [18](#S4.F18
    "Figure 18 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [18](#S4.F18 "Figure 18 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [18](#S4.F18 "Figure
    18 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/888fe67fe507d7e0c014cb182928fe8e.png)![Refer to caption](img/204011acd85cd7c0da05806cee893f66.png)![Refer
    to caption](img/bc8037c940c4c89d5a874ad228e74b16.png)![Refer to caption](img/ddafa0a0290dce47ff5b486872956b80.png)![Refer
    to caption](img/cee87d31ffb9b5bef7e6cb455d01b31e.png)![Refer to caption](img/39cb92939100bcac628dfbaf57193775.png)![Refer
    to caption](img/ac8096cc1fc7100fb33c5d04424ff86e.png)![Refer to caption](img/ac3100b0a491fb6a30919b02e8b1bc3c.png)![Refer
    to caption](img/3249812dfac161c703f6dcdb29d2765a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Classification maps on the KSC data set (100 samples per class).
    [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") Original. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") S-DMM. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results
    and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 3DCAE. [19](#S4.F19 "Figure 19 ‣ 4.3
    Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") SSDL. [19](#S4.F19
    "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey:
    Deep Learning for Hyperspectral Image Classification with Few Labeled Samples")
    TwoCnn. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 3DVSCNN. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental results and analysis
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") SSLstm. [19](#S4.F19 "Figure 19 ‣ 4.3 Experimental
    results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") CNN_HSI. [19](#S4.F19 "Figure
    19 ‣ 4.3 Experimental results and analysis ‣ 4 Experiments ‣ A Survey: Deep Learning
    for Hyperspectral Image Classification with Few Labeled Samples") SAE_LR.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Model parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further explore the reasons why the model has achieved different results
    on the benchmark data set, we also counted the number of trainable parameters
    of each framework (including the decoder module) on different data sets, which
    are shown in Table [8](#S4.T8 "Table 8 ‣ 4.4 Model parameters ‣ 4 Experiments
    ‣ A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples"). On all data sets, the model with the least number of training parameters
    is the SAE_LR, the second is the CNN_HSI and the most is the TwoCnn. SAE_LR is
    a lightweight architecture in all models for the simple linear layer, but its
    performance is poor. Different from other 2D convolution approaches in HSI, CNN_HSI
    solely uses a $1\times 1$ kernel to process an image. Moreover, it uses a $1\times
    1$ convolution layer to serve as a classifier instead of the linear layer, which
    greatly reduces the number of trainable parameters. The next is the S-DMM. This
    also explains why S-DMM and CNN_HSI are less affected by augmentation in sample
    size but very effective on few samples. Additionally, the problem of overfitting
    is of little concern in these approaches. Stacking the spectral and spatial feature
    to generate the final fused feature is the main reason for the large number of
    parameters of TwoCnn. However, regardless of its potentially millions of trainable
    parameters, it can work well on limited samples, benefiting from transfer learning,
    which decreases trainable parameters and achieves good performance on all target
    data sets. Next, the models with the most parameters are successively 3DCAE and
    SSLstm. 3DCAE’s trainable parameters are at most eight times those of SSDL, which
    contains not only a 1D autoencoder in the spectral branch but also a spatial branch
    based on a 2D convolutional network, but 3DCAE is still worse than SSDL. Although
    3D convolutional and pooling modules can greatly avoid the problem of data structure
    information loss caused by the flattening operation, the complexity of the 3D
    structure and the symmetric structure of the autoencoder increase the number of
    model parameters, which make it easy to overfit the model. 3DVSCNN also uses a
    3D convolutional module and is better than 3DCAE, which first reduces the number
    of redundant bands by PCA. That may also be applied to 3DCAE to decrease the number
    of model parameters and make good use of characteristics of 3D convolution, extracting
    spectral and spatial information simultaneously. The main contribution of the
    parameter of SSLstm comes from the spatial branch. Although the gate structure
    of LSTM improves the model’s capabilities of long and short memory, it increases
    the complexity of the model. When the number of hidden layer units increases,
    the model’s parameters will also skyrocket greatly. Perhaps it is the coupling
    between the spectral features and recurrent network that make performance of SSLstm
    not as bad as that of 3DCAE on all data sets, which has a similar number of parameters
    and even achieved superior results on KSC. Moreover, there are no methods that
    were adopted for solving the problem of few samples. This finding also shows that
    supervised learning is better than unsupervised learning in some tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: The number of trainable parameters'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | PaviaU | Salinas | KSC |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| S-DMM | 33921 | 40385 | 38593 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DCAE | 256563 | 447315 | 425139 |'
  prefs: []
  type: TYPE_TB
- en: '| SSDL | 35650 | 48718 | 44967 |'
  prefs: []
  type: TYPE_TB
- en: '| TwoCnn | 1379399 | 1542206 | 1501003 |'
  prefs: []
  type: TYPE_TB
- en: '| 3DVSCNN | 42209 | 42776 | 227613 |'
  prefs: []
  type: TYPE_TB
- en: '| SSLstm | 367506 | 370208 | 401818 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN_HSI | 22153 | 33536 | 31753 |'
  prefs: []
  type: TYPE_TB
- en: '| SAE_LR | 21426 | 5969 | 5496 |'
  prefs: []
  type: TYPE_TB
- en: 4.5 The speed of model convergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition, we compare the convergence speed of the model according to the
    changes in training loss of each model in the first 200 epochs on each group of
    experiments (see Figure [20](#S4.F20 "Figure 20 ‣ 4.5 The speed of model convergence
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples")$\sim$[22](#S4.F22 "Figure 22 ‣ 4.5 The speed of model
    convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples")). Because the autoencoder and classifier
    of 3DCAE are be trained separately, and all data are used during training the
    autoencoder, it is not comparable to other models. Therefore, it is not be listed
    here. On all data sets, S-DMM has the fastest convergence speed. After approximately
    3 epochs, the training loss tends to become stable given its fewer parameters.
    Although CNN_HSI has a similar performance to S-DMM and fewer parameters, the
    learning curve of CNN_HSI’s convergence rate is slower than that of S-DMM and
    is sometimes accompanied by turbulence. The second place regarding performance
    is held by TwoCnn, which is mainly due to transfer learning to better position
    the initial parameters, and it actually has fewer parameters requiring training.
    Thus, it just needs a few epochs to fine-tune on the target data set. Moreover,
    the training curve of most models stabilizes after 100 epochs. The training loss
    of the SSLstm has severe oscillations in all data sets. This is especially noted
    in the SeLstm, where the loss sometimes has difficulty in decreasing. When the
    sequence is very long, the challenge might be that the recurrent neural network
    is more susceptible to a vanishing or exploding gradient. Moreover, the pixels
    of the hyperspectral image usually contain hundreds of bands, which is the reason
    why the training loss has difficulty decreasing or oscillations occur in SeLstm.
    In the spatial branch, it does not have this serious condition because the length
    of the spatial sequence depending on patch size is shorter than spectral sequences.
    During training, the LSTM-based model spent a considerable amount of time because
    it cannot train in parallel.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/92ff38140d8432c58a271ba8ff5dd534.png)![Refer to caption](img/202d1fa3f6ae3cd4333adad1bd58fa2d.png)![Refer
    to caption](img/8aefb9fed52ddb72415634ddecd17138.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 20: Training Loss on the PaviaU data set. [20](#S4.F20 "Figure 20 ‣
    4.5 The speed of model convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") 10 samples per class.
    [20](#S4.F20 "Figure 20 ‣ 4.5 The speed of model convergence ‣ 4 Experiments ‣
    A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 50 samples per class. [20](#S4.F20 "Figure 20 ‣ 4.5 The speed of model
    convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 100 samples per class.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ad9c41242f4700eb1518939a53c404e3.png)![Refer to caption](img/4156238aa0edc5095f71ea494659f4a5.png)![Refer
    to caption](img/37243663eecf46a8fe843691f75780b1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 21: Training Loss on the Salinas data set.[21](#S4.F21 "Figure 21 ‣
    4.5 The speed of model convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for
    Hyperspectral Image Classification with Few Labeled Samples") 10 samples per class.
    [21](#S4.F21 "Figure 21 ‣ 4.5 The speed of model convergence ‣ 4 Experiments ‣
    A Survey: Deep Learning for Hyperspectral Image Classification with Few Labeled
    Samples") 50 samples per class. [21](#S4.F21 "Figure 21 ‣ 4.5 The speed of model
    convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image
    Classification with Few Labeled Samples") 100 samples per class.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da1df544f757786e0ad4ef451db1f805.png)![Refer to caption](img/e0c9764502ce59980024a6ede1724f62.png)![Refer
    to caption](img/5048fc735bf6485c368c9247b9d1f03a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 22: Training Loss on the KSC data set. [22](#S4.F22 "Figure 22 ‣ 4.5
    The speed of model convergence ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral
    Image Classification with Few Labeled Samples") 10 samples per class. [22](#S4.F22
    "Figure 22 ‣ 4.5 The speed of model convergence ‣ 4 Experiments ‣ A Survey: Deep
    Learning for Hyperspectral Image Classification with Few Labeled Samples") 50
    samples per class. [22](#S4.F22 "Figure 22 ‣ 4.5 The speed of model convergence
    ‣ 4 Experiments ‣ A Survey: Deep Learning for Hyperspectral Image Classification
    with Few Labeled Samples") 100 samples per class.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this paper, we introduce the current research difficulties, namely, few
    samples, in the field of hyperspectral image classification and discuss popular
    learning frameworks. Furthermore, we also introduce several popular learning algorithms
    to solve the small-sample problem, such as autoencoders, few-shot learning, transfer
    learning, activate learning, and data augmentation. According to the above methods,
    we select some representative models to conduct experiments on hyperspectral benchmark
    data sets. We developed three different experiments to explore the performance
    of the models on small-sample data sets and documented their changes with increasing
    sample size, finally evaluating their effectiveness and robustness through AA
    and OA. Then, we also compared the number of parameters and convergence speeds
    of various models to further analyze their differences. Ultimately, we also highlight
    several possible future directions of hyperspectral image classification on small
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders, including linear autoencoders and 3D convolutional autoencoders,
    have been widely explored and applied to solve the sample problem in HSI. Nevertheless,
    their performance does not approach excellence. The future development trend should
    be focused on few-shot learning, transfer learning, and active learning.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can fuse some learning paradigms to make good use of the advantages of each
    approach. For example, regarding the fusion of transfer learning and active learning,
    such an approach can select the valuable samples on the source data set and transfer
    the model to the target data set to avoid the imbalance of the class sample size.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to the experimental results, the RNN is also suitable for hyperspectral
    image classification. However, there is little work focused on combining the learning
    paradigms with RNN. Recently, the transformer, as an alternative to the RNN that
    is capable of processing in parallel, has been introduced into the computer vision
    domain and has achieved good performance on some tasks such as object detection.
    Therefore, we can also employ this method in hyperspectral image classification
    and combine it with some learning paradigms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graph convolution network has been growing more and more interested in hyperspectral
    image classification. Fully connected network, convolution network, and recurrent
    network are just suitable for processing the euclidean data and do not solve with
    the non-euclidean data directly. And image can be regarded as a special case of
    the euclidean-data. Thus, there are many researches [[124](#bib.bib124), [125](#bib.bib125),
    [126](#bib.bib126)] utilizing graph convolution networks to classify HSI.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The reason for requiring a large amount of label samples is the tremendous trainable
    parameters of the deep learning model. There are many methods proposed, such as
    group convolution [[127](#bib.bib127)], to light the weight of a deep neural network.
    So, how to construct a light-weight model further is also a future direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Although few label classification can save much time and labor force to collect
    and label diverse samples, the models are easy to suffer from over-fit and gaining
    a weak generalization. Thus, how to avoid the over-fitting and improve model’s
    generalization is the huge challenge of HSI few label classification in the application
    potential.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The work is partly supported by the National Natural Science Foundation of China
    (Grant No. 61976144).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. Teke, H. Deveci, O. Haliloğlu, S. Gürbüz, U. Sakarya, A short survey
    of hyperspectral remote sensing applications in agriculture, in: 2013 6th International
    Conference on Recent Advances in Space Technologies (RAST), IEEE, 2013, pp. 171–176.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] I. Strachan, E. Pattey, J. Boisvert, Impact of nitrogen and environmental
    conditions on corn as detected by hyperspectral reflectance, Remote Sens. Environ.
    80 (2) (2002) 213–224.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Bannari, A. Pacheco, K. Staenz, H. McNairn, K. Omari, Estimating and
    mapping crop residues cover on agricultural lands using hyperspectral and ikonos
    data, Remote Sens Environ 104 (4) (2006) 447–459.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Sabine, M. Robert, S. Thomas, R. Manuel, E. Paula, P. Marta, P. Alicia,
    Potential of hyperspectral imagery for the spatial assessment of soil erosion
    stages in agricultural semi-arid spain at different scales, in: 2014 IEEE Geoscience
    and Remote Sensing Symposium, IEEE, 2014, pp. 2918–2921.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] P. Kuflik, S. Rotman, Band selection for gas detection in hyperspectral
    images, in: 2012 IEEE 27th Convention of Electrical and Electronics Engineers
    in Israel, 2012, pp. 1–4. [doi:10.1109/EEEI.2012.6376973](https://doi.org/10.1109/EEEI.2012.6376973).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] S. Foudan, K. Menas, E. Tarek, G. Richard, Y. Ruixin, Hyperspectral image
    analysis for oil spill detection, in: Summaries of NASA/JPL Airborne Earth Science
    Workshop, Pasadena, CA, 2001, pp. 5–9.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] A. Mohamad, Sea water chlorophyll-a estimation using hyperspectral images
    and supervised artificial neural network, Ecol Inf 24 (2014) 60–68. [doi:10.1016/j.ecoinf.2014.07.004](https://doi.org/10.1016/j.ecoinf.2014.07.004).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Sylvain, G. Mireille, A novel maximum likelihood based method for mapping
    depth and water quality from hyperspectral remote-sensing data, Remote Sens Environ
    147 (2014) 121–132. [doi:10.1016/j.rse.2014.01.026](https://doi.org/10.1016/j.rse.2014.01.026).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] C. Jänicke, A. Okujeni, S. Cooper, M. Clark, P. Hostert, S. van der Linden,
    Brightness gradient-corrected hyperspectral image mosaics for fractional vegetation
    cover mapping in northern california, Remote Sensing Letters 11 (1) (2020) 1–10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] J. Li, Y. Pang, Z. Li, W. Jia, Tree species classification of airborne
    hyperspectral image in cloud shadow area, in: International Symposium of Space
    Optical Instrument and Application, Springer, 2018, pp. 389–398.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Z. Du, M. Jeong, S. Kong, Band selection of hyperspectral images for automatic
    detection of poultry skin tumors, IEEE Transactions on Automation Science and
    Engineering 4 (3) (2007) 332–339. [doi:10.1109/TASE.2006.888048](https://doi.org/10.1109/TASE.2006.888048).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] S. Li, W. Song, L. Fang, Y. Chen, J. Benediktsson, Deep learning for hyperspectral
    image classification: An overview, IEEE Transactions on Geoscience and Remote
    Sensing PP (99) (2019) 1–20.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] A. Plaza, J. Plaza, G. Martin, Incorporation of spatial constraints into
    spectral mixture analysis of remotely sensed hyperspectral data, Machine Learning
    for Signal Processing .mlsp .ieee International Workshop on (2009) 1 – 6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] F. Melgani, L. Bruzzone, Classification of hyperspectral remote sensing
    images with support vector machines, IEEE Transactions on Geoence and Remote Sensing
    42 (8) (2004) 1778–1790. [doi:10.1109/TGRS.2004.831865](https://doi.org/10.1109/TGRS.2004.831865).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Y. Zhong, L. Zhang, An adaptive artificial immune network for supervised
    classification of multi-/hyperspectral remote sensing imagery, IEEE Trans. Geosci.
    Remote Sens. 50 (3) (2011) 894–909.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Li, J. Bioucas-Dias, A. Plaza, Semisupervised hyperspectral image classification
    using soft sparse multinomial logistic regression, IEEE Geosci. Remote Sens. Lett.
    10 (2) (2012) 318–322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] G. Licciardi, P. Marpu, J. Chanussot, J. Benediktsson, Linear versus nonlinear
    pca for the classification of hyperspectral data based on the extended morphological
    profiles, IEEE Geosci. Remote Sens. Lett. 9 (3) (2011) 447–451.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. Villa, J. Chanussot, C. Jutten, J. Benediktsson, S. Moussaoui, On the
    use of ICA for hyperspectral image analysis, in: Proc. Geoscience and Remote Sensing
    Symp.,2009 IEEE Int.,IGARSS 2009, Vol. 4, 2009, pp. IV–97. [doi:10.1109/IGARSS.2009.5417363](https://doi.org/10.1109/IGARSS.2009.5417363).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] C. Zhang, Y. Zheng, Hyperspectral remote sensing image classification
    based on combined SVM and LDA, in: SPIE Asia Pacific Remote Sensing, 2014, p.
    92632P.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] L. He, J. Li, A. Plaza, Y. Li, Discriminative low-rank Gabor filtering
    for spectral-spatial hyperspectral image classification, IEEE Transactions on
    Geoence and Remote Sensing PP (99) (2016) 1–15. [doi:10.1109/TGRS.2016.2623742](https://doi.org/10.1109/TGRS.2016.2623742).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] M. D. Mura, J. A. Benediktsson, B. Waske, L. Bruzzone, Extended profiles
    with morphological attribute filters for the analysis of hyperspectral data, Int.
    J. Remote Sens. 31 (22) (2010) 5975–5991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] N. Falco, J. Atli Benediktsson, L. Bruzzone, Spectral and spatial classification
    of hyperspectral images based on ICA and reduced morphological attribute profiles,
    IEEE Transactions on Geoscience and Remote Sensing 53 (11) (2015) 6223–6240.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] M. Dalla Mura, A. Villa, J. Atli Benediktsson, J. Chanussot, L. Bruzzone,
    Classification of hyperspectral images by using extended morphological attribute
    profiles and independent component analysis, IEEE Geoscience and Remote Sensing
    Letters 8 (3) (2011) 542–546.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Jia, L. Shen, Q. Li, Gabor feature-based collaborative representation
    for hyperspectral imagery classification, IEEE Transactions on Geoscience and
    Remote Sensing 53 (2) (2015) 1118–1129.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Qian, M. Ye, J. Zhou, Hyperspectral image classification based on structured
    sparse logistic regression and three-dimensional wavelet texture features, IEEE
    Transactions on Geoscience and Remote Sensing 51 (4) (2013) 2276–2291.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] W. Li, C. Chen, H. Su, Q. Du, Local binary patterns and extreme learning
    machine for hyperspectral imagery classification, IEEE Trans Geosci Remote Sens
    53 (7) (2015) 3681–3693.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] P. Ghamisi, M. Dalla Mura, J. Atli Benediktsson, A survey on spectral–spatial
    classification techniques based on attribute profiles, IEEE Transactions on Geoscience
    and Remote Sensing 53 (5) (2015) 2335–2353.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] S. Yu, S. Jia, C. Xu, Convolutional neural networks for hyperspectral
    image classification, Neurocomputing 219 (2017) 88–98.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] M. Paoletti, J. Haut, J. Plaza, A. Plaza, Deep learning classifiers for
    hyperspectral imaging: A review, ISPRS J. Photogramm. Remote Sens. 158 (Dec.)
    (2019) 279–317.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] G. Hinton, R. Salakhutdinov, Reducing the dimensionality of data with
    neural networks, science 313 (5786) (2006) 504–507.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] A. Coates, A. Ng, H. Lee, An analysis of single-layer networks in unsupervised
    feature learning, Journal of Machine Learning Research 15 (2011) 215–223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] P. Vincent, H. Larochelle, Y. Bengio, P. Manzagol, Extracting and composing
    robust features with denoising autoencoders, in: International Conference on Machine
    Learning, 2008, pp. 1096–1103.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] L. Windrim, R. Ramakrishnan, A. Melkumyan, R. Murphy, A. Chlingaryan,
    Unsupervised feature-learning for hyperspectral data with autoencoders, Remote
    Sensing 11 (7) (2019) 864.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Y. Chen, Z. Lin, X. Zhao, G. Wang, Y. Gu, Deep learning-based classification
    of hyperspectral data, IEEE J Sel Topics Appl Earth Observ Remote Sens 7 (6) (2014)
    2094–2107.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Ghasem, S. Farhad, R. Peter, Spectral–spatial feature learning for
    hyperspectral imagery classification using deep stacked sparse autoencoder, J.
    Appl. Remote Sens. 11 (4) (2017) 042604.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] C. Xing, L. Ma, X. Yang, Stacked denoise autoencoder based feature extraction
    and classification for hyperspectral images, Journal of Sensors 2016 (2016).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Yue, S. Mao, M. Li, A deep learning framework for hyperspectral image
    classification using spatial pyramid pooling, Remote Sensing Letters 7 (9) (2016)
    875–884.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] S. Hao, W. Wang, Y. Ye, T. Nie, B. Lorenzo, Two-stream deep architecture
    for hyperspectral image classification, IEEE Trans. Geosci. Remote Sens. 56 (4)
    (2017) 2349–2361.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] X. Sun, F. Zhou, J. Dong, F. Gao, Q. Mu, X. Wang, Encoding spectral and
    spatial context information for hyperspectral image classification, IEEE Geosci.
    Remote Sens. Lett. 14 (12) (2017) 2250–2254.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] S. Mei, J. Ji, Y. Geng, Z. Zhang, X. Li, Q. Du, Unsupervised spatial–spectral
    feature learning by 3d convolutional autoencoder for hyperspectral classification,
    IEEE Trans. Geosci. Remote Sens. 57 (9) (2019) 6808–6820.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] C. Zhao, X. Wan, G. Zhao, B. Cui, W. Liu, B. Qi, Spectral-spatial classification
    of hyperspectral imagery based on stacked sparse autoencoder and random forest,
    European journal of remote sensing 50 (1) (2017) 47–63.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] X. Wan, C. Zhao, Y. Wang, W. Liu, Stacked sparse autoencoder in hyperspectral
    data classification using spectral-spatial, higher order statistics and multifractal
    spectrum features, Infrared Physics & Technology 86 (2017) 77–89.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] C. Wang, P. Zhang, Y. Zhang, L. Zhang, W. Wei, A multi-label hyperspectral
    image classification method with deep learning features, in: Proceedings of the
    International Conference on Internet Multimedia Computing and Service, 2016, pp.
    127–131.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] J. Li, B. Lorenzo, S. Liu, Deep feature representation for hyperspectral
    image classification, in: 2015 IEEE International Geoscience and Remote Sensing
    Symposium (IGARSS), IEEE, 2015, pp. 4951–4954.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] M. Atif, L. Tao, Efficient deep auto-encoder learning for the classification
    of hyperspectral images, in: 2016 International Conference on Virtual Reality
    and Visualization (ICVRV), IEEE, 2016, pp. 44–51.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. Liu, G. Cao, Q. Sun, S. Mel, Hyperspectral classification via learnt
    features, in: 2015 IEEE International Conference on Image Processing (ICIP), IEEE,
    2015, pp. 2591–2595.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] H. Lee, K. Heesung, Going deeper with contextual cnn for hyperspectral
    image classification, IEEE Trans. Image Process. 26 (10) (2017) 4843–4855.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] J. Leng, T. Li, G. Bai, Q. Dong, H. Dong, Cube-cnn-svm: a novel hyperspectral
    image classification method, in: 2016 IEEE 28th International Conference on Tools
    with Artificial Intelligence (ICTAI), IEEE, 2016, pp. 1027–1034.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] H. Zhang, Y. Li, Y. Zhang, Q. Shen, Spectral-spatial classification of
    hyperspectral imagery using a dual-channel convolutional neural network, Remote
    Sensing Letters 8 (5) (2017) 438–447.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] E. Aptoula, M. Ozdemir, B. Yanikoglu, Deep learning with attribute profiles
    for hyperspectral image classification, IEEE Geosci. Remote Sens. Lett. 13 (12)
    (2016) 1970–1974.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] W. Zhao, S. Li, A. Li, B. Zhang, Y. Li, Hyperspectral images classification
    with convolutional neural network and textural feature using limited training
    samples, Remote Sensing Letters 10 (5) (2019) 449–458.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] C. Yu, M. Zhao, M. Song, Y. Wang, F. Li, R. Han, C. Chang, Hyperspectral
    image classification method based on cnn architecture embedding with hashing semantic
    feature, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 12 (6) (2019) 1866–1881.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] C. Qing, J. Ruan, X. Xu, J. Ren, J. Zabalza, Spatial-spectral classification
    of hyperspectral images: a deep learning framework with markov random fields based
    modelling, IET Image Proc. 13 (2) (2018) 235–245.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Z. Zhong, J. Li, Z. Luo, M. Chapman, Spectral-spatial residual network
    for hyperspectral image classification: A 3-d deep learning framework, IEEE Trans.
    Geosci. Remote Sens. 56 (2) (2017) 847–858.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] B. Liu, X. Yu, P. Zhang, X. Tan, R. Wang, L. Zhi, Spectral–spatial classification
    of hyperspectral image using three-dimensional convolution network, J. Appl. Remote
    Sens. 12 (1) (2018) 016005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] B. Fang, Y. Li, H. Zhang, J. Chan, Collaborative learning of lightweight
    convolutional neural network and deep clustering for hyperspectral image semi-supervised
    classification with limited training samples, ISPRS J Photogramm Remote Sens 161
    (2020) 164–178.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] L. Mou, P. Ghamisi, X. Zhu, Unsupervised spectral–spatial feature learning
    via deep residual conv–deconv network for hyperspectral image classification,
    IEEE Trans. Geosci. Remote Sens. 56 (1) (2017) 391–406.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] A. Sellami, M. Farah, I. Farah, B. Solaiman, Hyperspectral imagery classification
    based on semi-supervised 3-d deep neural network and adaptive band selection,
    Expert Syst. Appl. 129 (2019) 246–259.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] K. He, X. Zhang, S. Ren, J. Sun, Deep residual learning for image recognition,
    in: Proc. IEEE Conf. Comput. Vis. Pattern Recognit., 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] M. Paoletti, J. Haut, R. Fernandez-Beltran, J. Plaza, A. Plaza, F. Pla,
    Deep pyramidal residual networks for spectral–spatial hyperspectral image classification,
    IEEE Transactions on Geoscience and Remote Sensing 57 (2) (2018) 740–754.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] X. Ma, A. Fu, J. Wang, H. Wang, B. Yin, Hyperspectral image classification
    based on deep deconvolution network with skip architecture, IEEE Trans. Geosci.
    Remote Sens. 56 (8) (2018) 4781–4791.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] M. Paoletti, J. Haut, J. Plaza, A. Plaza, Deep&dense convolutional neural
    network for hyperspectral image classification, Remote Sensing 10 (9) (2018) 1454.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] W. Wang, S. Dou, Z. Jiang, L. Sun, A fast dense spectral–spatial convolution
    network framework for hyperspectral images classification, Remote Sensing 10 (7)
    (2018) 1068.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Haut, M. Paoletti, J. Plaza, A. Plaza, J. Li, Visual attention-driven
    hyperspectral image classification, IEEE Transactions on Geoscience and Remote
    Sensing 57 (10) (2019) 8065–8080.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Z. Xiong, Y. Yuan, Q. Wang, Ai-net: attention inception neural networks
    for hyperspectral image classification, in: IGARSS 2018-2018 IEEE International
    Geoscience and Remote Sensing Symposium, IEEE, 2018, pp. 2647–2650.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Q. Feng, D. Zhu, J. Yang, B. Li, Multisource hyperspectral and lidar data
    fusion for urban land-use mapping based on a modified two-branch convolutional
    neural network, ISPRS International Journal of Geo-Information 8 (1) (2019) 28.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] X. Xu, W. Li, Q. Ran, Q. Du, L. Gao, B. Zhang, Multisource remote sensing
    data classification based on convolutional neural network, IEEE Transactions on
    Geoscience and Remote Sensing 56 (2) (2017) 937–949.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] H. Li, G. Pedram, S. Uwe, X. Zhu, Hyperspectral and lidar fusion using
    deep three-stream convolutional neural networks, Remote Sensing 10 (10) (2018)
    1649.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] W. Li, C. Chen, M. Zhang, H. Li, Q. Du, Data augmentation for hyperspectral
    image classification with deep cnn, IEEE Geoscience and Remote Sensing Letters
    16 (4) (2018) 593–597.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] W. Wei, J. Zhang, L. Zhang, C. Tian, Y. Zhang, Deep cube-pair network
    for hyperspectral imagery classification, Remote Sensing 10 (5) (2018) 783.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] S. Hochreiter, J. Schmidhuber, Long short-term memory, Neural computation
    9 (8) (1997) 1735–1780.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] C. Kyunghyun, V. Bart, G. Caglar, B. Dzmitry, B. Fethi, S. Holger, B. Yoshua,
    Learning phrase representations using rnn encoder-decoder for statistical machine
    translation, arXiv preprint arXiv:1406.1078 (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] L. Mou, P. Ghamisi, X. Zhu, Deep recurrent neural networks for hyperspectral
    image classification, IEEE Trans. Geosci. Remote Sens. 55 (7) (2017) 3639–3655.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] B. Liu, X. Yu, A. Yu, P. Zhang, G. Wan, Spectral-spatial classification
    of hyperspectral imagery based on recurrent neural networks, Remote Sensing Letters
    9 (12) (2018) 1118–1127.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] F. Zhou, R. Hang, Q. Liu, X. Yuan, Hyperspectral image classification
    using spectral-spatial lstms, Neurocomputing 328 (2019) 39–47.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] M. Andong, F. A. M, Z. Wang, Z. Yin, Hyperspectral image classification
    using similarity measurements-based deep recurrent neural networks, Remote Sensing
    11 (2) (2019) 194.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Zhang, Y. Sun, K. Jiang, C. Li, L. Jiao, H. Zhou, Spatial sequential
    recurrent neural network for hyperspectral image classification, IEEE J. Sel.
    Topics Appl. Earth Observ. Remote Sens. 11 (11) (2018) 4141–4155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] E. Pan, X. Mei, Q. Wang, Y. Ma, J. Ma, Spectral-spatial classification
    for hyperspectral image based on a single gru, Neurocomputing 387 (2020) 150–160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] H. Wu, P. Saurabh, Semi-supervised deep learning using pseudo labels for
    hyperspectral image classification, IEEE Trans. Image Process. 27 (3) (2017) 1259–1270.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] H. Wu, P. Saurabh, Convolutional recurrent neural networks forhyperspectral
    data classification, Remote Sensing 9 (3) (2017) 298.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] S. Hao, W. Wang, S. Mathieu, Geometry-aware deep recurrent neural networks
    for hyperspectral image classification, IEEE Trans. Geosci. Remote Sens. (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C. Shi, P. Chi-Man, Multi-scale hierarchical recurrent neural networks
    for hyperspectral image classification, Neurocomputing 294 (2018) 82–93.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S. Pan, Q. Yang, A survey on transfer learning, IEEE Transactions on Knowledge
    and Data Engineering 22 (10) (2010) 1345–1359. [doi:10.1109/tkde.2009.191](https://doi.org/10.1109/tkde.2009.191).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] J. Yang, Y. Zhao, J. Chan, C. Yi, Hyperspectral image classification using
    two-channel deep convolutional neural network, in: 2016 IEEE International Geoscience
    and Remote Sensing Symposium (IGARSS), IEEE, 2016, pp. 5079–5082.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] J. Yang, Y. Zhao, J. Chan, Learning and transferring deep joint spectral–spatial
    features for hyperspectral classification, IEEE Trans. Geosci. Remote Sens. 55 (8)
    (2017) 4729–4742.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] L. Lin, C. Chen, J. Yang, S. Zhang, Deep transfer hsi classification method
    based on information measure and optimal neighborhood noise reduction, Electronics
    8 (10) (2019) 1112.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] H. Zhang, Y. Li, Y. Jiang, P. Wang, Q. Shen, C. Shen, Hyperspectral classification
    based on lightweight 3-d-cnn with transfer learning, IEEE Trans. Geosci. Remote
    Sens. 57 (8) (2019) 5813–5828.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Jiang, Y. Li, H. Zhang, Hyperspectral image classification based on
    3-d separable resnet and transfer learning, IEEE Geosci. Remote Sens. Lett. 16 (12)
    (2019) 1949–1953.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] C. Deng, Y. Xue, X. Liu, C. Li, D. Tao, Active transfer learning network:
    A unified deep joint spectral–spatial feature learning model for hyperspectral
    image classification, IEEE Trans. Geosci. Remote Sens. 57 (3) (2018) 1741–1754.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] M. Ghifary, W. Kleijn, M. Zhang, Domain adaptive neural networks for object
    recognition, in: Pacific Rim international conference on artificial intelligence,
    Springer, 2014, pp. 898–904.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, T. Darrell, Deep domain confusion:
    Maximizing for domain invariance, arXiv preprint arXiv:1412.3474 (2014).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Z. Wang, B. Du, Q. Shi, W. Tu, Domain adaptation with discriminative distribution
    and manifold embedding for hyperspectral image classification, IEEE Geosci. Remote
    Sens. Lett. 16 (7) (2019) 1155–1159.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Y. Ganin, E. Ustinova, H. Ajakan, P. Germain, H. Larochelle, F. Laviolette,
    M. Marchand, V. Lempitsky, Domain-adversarial training of neural networks, The
    Journal of Machine Learning Research 17 (1) (2016) 2096–2030.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Elshamli, G. Taylor, A. Berg, S. Areibi, Domain adaptation using representation
    learning for the classification of remote sensing images, IEEE J. Sel. Topics
    Appl. Earth Observ. Remote Sens. 10 (9) (2017) 4198–4209.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] B. Settles, Active learning literature survey, Tech. rep., University
    of Wisconsin-Madison Department of Computer Sciences (2009).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Haut, M. Paoletti, J. Plaza, J. Li, A. Plaza, Active learning with
    convolutional neural networks for hyperspectral image classification using a new
    bayesian approach, IEEE Trans. Geosci. Remote Sens. 56 (11) (2018) 6440–6461.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] P. Liu, H. Zhang, K. Eom, Active deep learning for classification of hyperspectral
    images, IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 10 (2) (2016) 712–724.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] J. Li, Active learning for hyperspectral image classification with a stacked
    autoencoders based neural network, in: 2015 7th Workshop on Hyperspectral Image
    and Signal Processing: Evolution in Remote Sensing (WHISPERS), IEEE, 2015, pp.
    1–4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Y. Sun, J. Li, W. Wang, P. Antonio, Z. Chen, Active learning based autoencoder
    for hyperspectral imagery classification, in: 2016 IEEE International Geoscience
    and Remote Sensing Symposium (IGARSS), IEEE, 2016, pp. 469–472.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] X. Cao, J. Yao, Z. Xu, D. Meng, Hyperspectral image classification with
    convolutional neural network and active learning, IEEE Trans. Geosci. Remote Sens.
    (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] C. Deng, Y. Xue, X. Liu, C. Li, D. Tao, Active transfer learning network:
    A unified deep joint spectral–spatial feature learning model for hyperspectral
    image classification, IEEE Trans. Geosci. Remote Sens. 57 (3) (2018) 1741–1754.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J. Snell, K. Swersky, R. Zemel, Prototypical networks for few-shot learning,
    in: Advances in neural information processing systems, 2017, pp. 4077–4087.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Y. Liu, M. Su, L. Liu, C. Li, Y. Peng, J. Hou, T. Jiang, Deep residual
    prototype learning network for hyperspectral image classification, in: Second
    Target Recognition and Artificial Intelligence Summit Forum, Vol. 11427, International
    Society for Optics and Photonics, 2020, p. 1142705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] H. Tang, Y. Li, X. Han, Q. Huang, W. Xie, A spatial–spectral prototypical
    network for hyperspectral remote sensing image, IEEE Geosci. Remote Sens. Lett.
    17 (1) (2019) 167–171.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] B. Xi, J. Li, Y. Li, R. Song, Y. Shi, S. Liu, Q. Du, Deep prototypical
    networks with hybrid residual attention for hyperspectral image classification,
    IEEE J. Sel. Topics Appl. Earth Observ. Remote Sens. 13 (2020) 3683–3700.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] A. Muqeet, M. Iqbal, S. Bae, Hran: Hybrid residual attention network
    for single image super-resolution, IEEE Access 7 (2019) 137020–137029.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] F. Sung, Y. Yang, L. Zhang, T. Xiang, P. H. S. Torr, T. M. Hospedales,
    Learning to compare: Relation network for few-shot learning, in: Proc. IEEE Conf.
    Comput. Vis. Pattern Recognit., 2018, pp. 1199–1208.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] B. Deng, D. Shi, Relation network for hyperspectral image classification,
    in: 2019 IEEE International Conference on Multimedia & Expo Workshops (ICMEW),
    IEEE, 2019, pp. 483–488.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] K. Gao, B. Liu, X. Yu, J. Qin, P. Zhang, X. Tan, Deep relation network
    for hyperspectral image few-shot classification, Remote Sensing 12 (6) (2020)
    923.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] X. Ma, S. Ji, J. Wang, J. Geng, H. Wang, Hyperspectral image classification
    based on two-phase relation learning network, IEEE Trans. Geosci. Remote Sens.
    57 (12) (2019) 10398–10409.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] M. Rao, P. Tang, Z. Zhang, Spatial–spectral relation network for hyperspectral
    image classification with limited training samples, IEEE J. Sel. Topics Appl.
    Earth Observ. Remote Sens. 12 (12) (2019) 5086–5100.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] B. Jane, G. Isabelle, L. Yann, S. Eduard, S. Roopak, Signature verification
    using a” siamese” time delay neural network, in: Advances in neural information
    processing systems, 1994, pp. 737–744.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] C. Sumit, H. Raia, L. Yann, Learning a similarity metric discriminatively,
    with application to face verification, in: Proc.IEEE Conf. Comput. Vis. Pattern
    Recognit., Vol. 1, IEEE, 2005, pp. 539–546.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] M. Norouzi, D. Fleet, R. Salakhutdinov, Hamming distance metric learning,
    in: Advances in neural information processing systems, 2012, pp. 1061–1069.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] B. Liu, X. Yu, P. Zhang, A. Yu, Q. Fu, X. Wei, Supervised deep feature
    extraction for hyperspectral image classification, IEEE Trans. Geosci. Remote
    Sens. 56 (4) (2017) 1909–1921.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] B. Liu, X. Yu, A. Yu, G. Wan, Deep convolutional recurrent neural network
    with transfer learning for hyperspectral image classification, J. Appl. Remote
    Sens. 12 (2) (2018) 026028.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Z. Li, X. Tang, W. Li, C. Wang, C. Liu, J. He, A two-stage deep domain
    adaptation method for hyperspectral image classification, Remote Sensing 12 (7)
    (2020) 1054.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] L. Huang, Y. Chen, Dual-path siamese cnn for hyperspectral image classification
    with limited training samples, IEEE Geosci. Remote Sens. Lett. (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] M. Rao, P. Tang, Z. Zhang, A developed siamese cnn with 3d adaptive spatial-spectral
    pyramid pooling for hyperspectral image classification, Remote Sensing 12 (12)
    (2020) 1964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Miao, B. Wang, X. Wu, L. Zhang, B. Hu, J. Zhang, Deep feature extraction
    based on siamese network and auto-encoder for hyperspectral image classification,
    in: IGARSS 2019-2019 IEEE International Geoscience and Remote Sensing Symposium,
    IEEE, 2019, pp. 397–400.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] B. Deng, S. Jia, D. Shi, Deep metric learning-based feature embedding
    for hyperspectral image classification, IEEE Transactions on Geoence and Remote
    Sensing 58 (2) (2020) 1422–1435.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] J. Yang, Y. Zhao, J. Chan, Learning and transferring deep joint spectral-spatial
    features for hyperspectral classification, IEEE Trans. Geosci. Remote Sens. 55 (8)
    (2017) 4729–4742. [doi:10.1109/TGRS.2017.2698503](https://doi.org/10.1109/TGRS.2017.2698503).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] L. Hu, X. Luo, Y. Wei, Hyperspectral image classification of convolutional
    neural network combined with valuable samples, Journal of Physics Conference Series
    1549 (2020) 052011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] S. Wan, C. Gong, P. Zhong, B. Du, L. Zhang, J. Yang, Multiscale dynamic
    graph convolutional network for hyperspectral image classification, IEEE Transactions
    on Geoscience and Remote Sensing 58 (5) (2019) 3162–3177.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] B. Liu, K. Gao, A. Yu, W. Guo, R. Wang, X. Zuo, Semisupervised graph
    convolutional network for hyperspectral image classification, J. Appl. Remote
    Sens. 14 (2) (2020) 026516.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] S. Wan, C. Gong, P. Zhong, S. Pan, G. Li, J. Yang, Hyperspectral image
    classification with context-aware dynamic graph convolutional network, IEEE Transactions
    on Geoscience and Remote Sensing 59 (1) (2020) 597–612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] A. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    H. Adam, Mobilenets: Efficient convolutional neural networks for mobile vision
    applications, arXiv preprint arXiv:1704.04861 (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
