- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:31:25'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2407.08865] Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.08865](https://ar5iv.labs.arxiv.org/html/2407.08865)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \WarningFilter
  prefs: []
  type: TYPE_NORMAL
- en: latexFont shape \WarningFilterlatexfontFont shape
  prefs: []
  type: TYPE_NORMAL
- en: 'Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lanqing Guo, Chong Wang, Yufei Wang, Yi Yu, Siyu Huang, Wenhan Yang, Member,
    IEEE,
  prefs: []
  type: TYPE_NORMAL
- en: 'Alex C. Kot, Life Fellow, IEEE, Bihan Wen, Senior Member, IEEE Lanqing Guo,
    Chong Wang, Yufei Wang, Yi Yu, Alex C. Kot, and Bihan Wen are with the School
    of Electrical & Electronic Engineering, Nanyang Technological University, Singapore
    639798\. E-mail: {lanqing001, wang1711, yufei001, yuyi0010, eackot, bihan.wen}@ntu.edu.sg.
    Siyu Huang is with the Visual Computing Division, School of Computing, Clemson
    University, Clemson, SC 29631 USA. E-mail: siyuh@clemson.edu. Wenhan Yang is with
    PengCheng Laboratory, Shenzhen, China. E-mail: yangwh@pcl.ac.cn.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Shadow removal aims at restoring the image content within shadow regions, pursuing
    a uniform distribution of illumination that is consistent between shadow and non-shadow
    regions. Comparing to other image restoration tasks, there are two unique challenges
    in shadow removal: 1) The patterns of shadows are arbitrary, varied, and often
    have highly complex trace structures, making “trace-less” image recovery difficult.
    2) The degradation caused by shadows is spatially non-uniform, resulting in inconsistencies
    in illumination and color between shadow and non-shadow areas. Recent developments
    in this field are primarily driven by deep learning-based solutions, employing
    a variety of learning strategies, network architectures, loss functions, and training
    data. Nevertheless, a thorough and insightful review of deep learning-based shadow
    removal techniques is still lacking. In this paper, we are the first to provide
    a comprehensive survey to cover various aspects ranging from technical details
    to applications. We highlight the major advancements in deep learning-based single-image
    shadow removal methods, thoroughly review previous research across various categories,
    and provide insights into the historical progression of these developments. Additionally,
    we summarize performance comparisons both quantitatively and qualitatively. Beyond
    the technical aspects of shadow removal methods, we also explore potential future
    directions for this field. The related repository is released at: [https://github.com/GuoLanqing/Awesome-Shadow-Removal](https://github.com/GuoLanqing/Awesome-Shadow-Removal).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Shadow Removal, Low-Level Vision, Deep Learning, Image Enhancement, Computational
    Photography
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Shadowing is a natural occurrence observed when certain regions of a surface
    receive less illumination compared to their neighboring areas. This happens when
    an opaque object obstructs the direct path of light between the surface and the
    light source. Shadows in images and videos hinder both the human perception [[1](#bib.bib1),
    [2](#bib.bib2)] as well as many subsequent vision tasks, e.g., object detection,
    tracking, and semantic segmentation [[3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/264ad576fbcfb126a5d7c0e2a22062a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Milestones in deep learning-based single-image shadow removal methods
    include the exploration of various technologies over time, such as deep CNNs,
    GANs, RNNs, unrolling, transformers, and diffusion models.'
  prefs: []
  type: TYPE_NORMAL
- en: Shadow degradations manifest in diverse types and exhibit highly non-uniform
    properties, presenting unique challenges for shadow removal compared to other
    image restoration tasks. Traditional shadow removal methods leverage hand-crafted
    prior information such as illumination [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)], gradient [[12](#bib.bib12),
    [13](#bib.bib13), [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16), [17](#bib.bib17)],
    and color [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22)] to restore illumination in shadowed areas. Typically, some methods [[13](#bib.bib13),
    [23](#bib.bib23)] eliminate the image gradient along shadow edges and then reintegrate
    the modified gradient to produce a shadow-free image. However, these approaches
    are based on ideal assumptions, resulting in noticeable shadow boundary artifacts
    in real-world scenarios. They always require accurate and fine-grained shadow
    edges in the detection output as the strong prior for shadow removal, which is
    impractical in applications; another group [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)] treat shadow removal as a
    relighting problem, seeking a factor to enhance the brightness of shadow pixels.
    The main challenge is to determine different scale factors for umbra and penumbra
    shadows, along with addressing shadow boundary corrections due to non-uniform
    illumination degradation.
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, deep learning-based shadow removal methods have demonstrated
    superior performance, owing to the availability of extensive training data. Figure [1](#S1.F1
    "Figure 1 ‣ I Introduction ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey") presents a concise overview of the milestones achieved
    by deep learning-based shadow removal methods in the past years. In 2017, Qu *et
    al.* [[24](#bib.bib24)] first proposed a multi-branch fusion framework as the
    pioneering work to adopt a deep convolutional neural networks (CNNs) [[25](#bib.bib25),
    [26](#bib.bib26), [27](#bib.bib27)] to this field. Subsequently, starting from
    2018, the advancement of Generative Adversarial Networks (GANs) [[28](#bib.bib28)]
    in the realm of low-level vision has led to widespread adoption of GAN-based networks
    in shadow removal tasks. These networks are not only effective in addressing amplified
    artifacts but also enable the development of unsupervised learning methods using
    unpaired shadow and shadow-free data. During this period, some methods tried to
    inject constraints, e.g., physical illumination model and mask information, into
    networks to learn more effective features, by unrolling [[29](#bib.bib29), [30](#bib.bib30)]
    or recurrent [[31](#bib.bib31)] strategies. In the most recent developments, novel
    techniques such as the transformer mechanism [[32](#bib.bib32), [33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)] and diffusion model [[36](#bib.bib36), [37](#bib.bib37),
    [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40)] have shown remarkable advancements
    in shadow removal. The transformer mechanism exploits long-range dependencies
    within contexts, facilitating more effective extraction of context information
    from non-shadow regions. Meanwhile, the diffusion model offers a potent diffusive
    generative prior for generating natural shadow-free images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite the dominance of deep learning in shadow removal research, there is
    a lack of an in-depth and comprehensive survey on deep learning-based solutions.
    The existing surveys for shadow removal [[41](#bib.bib41), [42](#bib.bib42), [43](#bib.bib43)]
    only cover traditional methods based on earlier publications, but omit the recent
    works via deep learning approaches. In this paper, we are the first to review
    and summarize deep learning-based single-image shadow removal methods, aiming
    to offer a well-structured and comprehensive knowledge base to support and advance
    the field of shadow removal. The rest of the paper is organized as follows. Section [I](#S1
    "I Introduction ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") introduces the shadow formulation model and the problem definition for
    the shadow removal task. Section [II](#S2 "II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey") presents a detailed
    shadow formation model and outlines the various shadow categories, highlighting
    the different challenges associated with each. Section [III](#S3 "III Deep Learning
    Based Shadow Removal ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") provides a detailed survey of deep learning based single-image shadow
    removal of different learning strategies, including supervised, semi-supervised,
    unsupervised, and zero-shot learning, as well as summarises the challenges of
    analysis of shadow removal. Subsequently, Section [IV](#S4 "IV Technical Review
    and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") reviews the related technical aspects and explores solutions to the aforementioned
    challenges through various technical designs and the integration of priors, including
    network architectures, basic blocks, framework designs, loss functions, datasets,
    and evaluation metrics. Section [V](#S5 "V Benchmarking and Empirical Analysis
    ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey") summarizes
    the quantitative and qualitative comparisons of a number of single-image shadow
    removal methods among various benchmarks. Section [VI](#S6 "VI Shadow Removal
    Applications ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") presents the related shadow removal applications, including shadow generation
    and shadow-related attack in practice. Section [VII](#S7 "VII Future Research
    Directions ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") provides detailed discussions of the future research directions on generalized
    shadow removal and interactive shadow removal. Finally, the paper is concluded
    in Section [VIII](#S8 "VIII Conclusion ‣ Single-Image Shadow Removal Using Deep
    Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: II Analysis on Shadow Formation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Shadow Formation Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to the Retinex theory [[44](#bib.bib44)], the formation process of
    a shadow-free image $\mathbf{I}_{sf}$ with normal lightness can be formulated
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{I}_{sf}=\mathbf{L}_{sf}\circ\mathbf{R}$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{L}$ denotes the illumination, $\mathbf{R}$ denotes the reflectance,
    and $\circ$ denotes element-wise multiplication. Shadows occur when an occluder
    blocks light between the surface and the light source, causing partial light degradations.
    Thus, the corresponding shadow image $\mathbf{I}_{s}$ with degraded illumination
    $\mathbf{L}_{s}$ can be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{I}_{s}=\mathbf{L}_{s}\circ\mathbf{R}=\mathbf{A}\circ\mathbf{L}_{sf}\circ\mathbf{R},$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{A}$ denotes the spatially variant shadow degradation degrees
    at each location.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Shadow Category
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Shadow degradations have diverse types and highly non-uniform properties, which
    present unique challenges for shadow removal compared to other image restoration
    tasks. Shadows can be classified as self shadows if they are part of an object
    or as cast shadows if they belong to the background of the scene as shown in Figure [2](#S2.F2
    "Figure 2 ‣ II-B Shadow Category ‣ II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(a). For a non-point
    source of light, a cast shadow is further sub-divided into soft and hard shadows
    based on the illumination intensity (i.e., darkness) as shown in Figure [2](#S2.F2
    "Figure 2 ‣ II-B Shadow Category ‣ II Analysis on Shadow Formation ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(b). Specifically,
    shadows produced by the obstruction of a non-point light source by an object exhibit
    two distinct regions: the umbra and the penumbra. The umbra denotes the high-density
    region, located toward the inner shadow area, whereas the penumbra represents
    the low-density region, situated toward the outer shadow area. Hard shadows exhibit
    umbra regions with a high density, often resulting in nearly obliterated surface
    texture. In contrast, soft shadows display penumbra regions towards the outer
    shadow area, with lower density, and their boundaries typically blend into the
    non-shadowed surroundings. The distance between the occluder and receiver, as
    well as the type of light source, determines whether hard or soft shadows are
    produced, as depicted in Figure [2](#S2.F2 "Figure 2 ‣ II-B Shadow Category ‣
    II Analysis on Shadow Formation ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey")(b).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Shadow degradations vary widely in type and exhibit highly non-uniform characteristics,
    requiring better generalization ability to handle varying cases in the real world.
    Different shadow types present distinct challenges: hard shadows with sharp boundaries
    can leave noticeable traces, while soft shadows and self-shadows, with ambiguous
    boundaries, are more difficult to detect and mask accurately.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/46659b9203e6982aa96b1f43d111316b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: How a shadow is formed? (a) Shadows are then classified as cast shadows
    if they belong to the background of the scene or as self shadows if they are part
    of an occluder object. (b) The cast shadows can be further classified as hard
    shadows and soft shadows. A crisp edged one (hard shadow) formed by a point light
    source, a rather more fuzzy one (soft shadow) that is formed by the area light,
    and otherwise the occluder is very close to the receiver.'
  prefs: []
  type: TYPE_NORMAL
- en: III Deep Learning Based Shadow Removal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we first provide the problem definition of single-image shadow
    removal. Then, we review and discuss existing shadow removal methods according
    to the categories of general learning strategies. After that, we summarize the
    challenges and provide an analysis of existing methods.
  prefs: []
  type: TYPE_NORMAL
- en: III-A Problem Definition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We begin by presenting the common formulation of the deep learning-based image
    shadow removal problem. For a shadow image $\mathbf{I}_{s}\in\mathbb{R}^{H\times
    W\times 3}$ with width $W$ and height $H$, the process can be modeled as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{I}}_{sf}=\mathcal{G}(\mathbf{I}_{s};\theta),$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\hat{\mathbf{I}}_{sf}\in\mathbb{R}^{H\times W\times 3}$ is the restored
    shadow-free result and $\mathcal{G}$ represents the shadow removal network with
    trainable parameters $\theta$. Different from other image restoration tasks, shadow
    removal is a partial corruption problem that requires identifying the shadow regions
    and restoring them. To aid this process, an optional shadow mask $\mathbf{M}\in\mathbb{R}^{H\times
    W}$ can be used as auxiliary information, indicating shadow regions either manually
    annotated or detected by a pre-trained shadow detector. The uniqueness property
    motivates numerous explorations into how shadow mask information can assist in
    shadow removal, facilitating the development of shadow mask-guided removal process
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{I}}_{sf}=\mathcal{G}(\mathbf{I}_{s},\mathbf{M};\theta)\;.$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: III-B General Learning Strategies
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Based on different learning strategies, we generally categorize existing image
    shadow removal methods into supervised learning, unsupervised learning, zero-shot
    learning, and semi-supervised learning. Figure [3](#S3.F3 "Figure 3 ‣ III-B General
    Learning Strategies ‣ III Deep Learning Based Shadow Removal ‣ Single-Image Shadow
    Removal Using Deep Learning: A Comprehensive Survey") illustrates the brief pipeline
    of different learning strategies. In the following section, we review some representative
    methods for each strategy.'
  prefs: []
  type: TYPE_NORMAL
- en: Supervised Learning (SL) For shadow removal, supervised learning refers to the
    model training with paired shadow and shadow-free images captured in the same
    scene, but under different illumination conditions. The pioneering deep learning-based
    work DeShadowNet [[24](#bib.bib24)] propose an automatic and end-to-end deep neural
    network to unify shadow detection, umbra/penumbra region classification, and shadow
    removal, which directly learns the mapping function between the shadow image and
    its shadow matte. Subsequently, Wang *et al.* [[45](#bib.bib45)] developed a stacked
    conditional generative adversarial network (ST-CGAN) to simultaneously learn shadow
    detection and shadow removal. The ST-CGAN organizes all tasks in a way that allows
    them to focus on one task at a time during different stages, while also benefiting
    from mutual improvements through the exchange of information in both forward and
    backward directions. Hu *et al.* [[46](#bib.bib46)] solve the shadow removal problem
    by a new perspective, which employs the spatial recurrent neural network with
    the direction-aware attention mechanism to exploit the context information. After
    that, Zhang *et al.* [[47](#bib.bib47)] design a coarse-to-fine pipeline with
    four generators as well as three discriminators to explore the residual and illumination
    information separately. A dual hierarchically aggregation network, DHAN [[48](#bib.bib48)],
    is proposed for shadow removal. The DHAN contains a series of growth dilated convolutions
    as the backbone and hierarchically aggregates multi-context features for attention
    and prediction. Lin *et al.* [[49](#bib.bib49)] specifically design BEDSR-Net
    for document image shadow removal and take advantage of specific properties of
    document images, consisting of the global background color and attention map estimation.
    Recently, a two-stage context-aware network [[50](#bib.bib50)] is proposed, which
    transfers contextual information from non-shadow to shadow regions at different
    scales with a contextual patch matching module.
  prefs: []
  type: TYPE_NORMAL
- en: In comparison to jointly learning shadow detection and shadow removal to explore
    both the semantic and fine-structural levels, some recent methods enjoy better
    performance and more lightweight models by relying on an external pre-trained
    shadow detector or manually annotated mask information. A group of methods explicitly
    combine the physical shadow illumination model into the deep network to increase
    interpretability. For instance, Le *et al.* [[51](#bib.bib51)] propose to integrate
    the linear illumination transformation into the deep network to model the shadow
    effects. This work uses a regression model to estimate the scaling factor and
    additive constant parameters for linear transformation. Following that, Fu *et
    al.* [[52](#bib.bib52)] re-formulate shadow removal as an exposure fusion challenge.
    They reconstruct shadow-free results by intelligently blending over-exposed shadow
    images with shadow images using an adaptive per-pixel kernel weight map. Zhu *et
    al.* [[29](#bib.bib29)] design an iterative algorithm and unfold pipeline to ensure
    the identity mapping among non-shadow regions and adaptively perform the fine-grained
    spatial mapping between shadow regions. Moreover, the shadow images can be divided
    into shadow and non-shadow regions based on the pre-defined shadow mask, which
    is very effective prior for shadow removal. A Mask-ShadowNet [[53](#bib.bib53)]
    is proposed, which includes a masked adaptive instance normalization layer with
    embedded aligners and processes the shadow and non-shadow regions separately by
    different statistics. To ensure the consistency between shadow removal and generation,
    Zhu *et al.* [[54](#bib.bib54)] propose a bijective mapping network (BMNet) to
    recover the underlying background contents during the forward procedure under
    the guidance of a color map and shadow mask. Guo *et al.* [[55](#bib.bib55)] propose
    ShadowFormer to exploit the illumination information from non-shadow regions to
    help shadow regions restoration via the customized transformer blocks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f65be8df6ef628777b2a9bc6c64f9312.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of common deep learning strategies for single-image
    shadow removal. Details refer to Section [III-B](#S3.SS2 "III-B General Learning
    Strategies ‣ III Deep Learning Based Shadow Removal ‣ Single-Image Shadow Removal
    Using Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Semi-Supervised Learning (SSL) In recent years, semi-supervised learning has
    emerged as an approach to leverage the strengths of both supervised learning and
    unsupervised learning. It leverages both paired data and unpaired data to boost
    the model performance and improve generalization ability. Ding *et al.* [[31](#bib.bib31)]
    propose a semi-supervised deep attentive recurrent generative adversarial network
    (ARGAN) to progressively detect and remove shadows. The generator of ARGAN performs
    multiple progressive steps in a coarse-to-fine manner, serving the dual purpose
    of generating shadow attention maps and recovering the shadow-removal images.
    The semi-supervised strategy is implemented by incorporating a significant number
    of unsupervised shadow images available online during the training process, which
    achieves better robustness and performance to complicated environment.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Learning (UL) While supervised learning methods have shown remarkable
    success in various scenarios, they heavily rely on large amounts of paired training
    data, which can be costly and time-consuming to obtain. Besides, training a deep
    model on paired data is typically designed to solve specific tasks, which struggle
    to adapt to new or out-of-distribution cases without retraining. To address this
    issue, inspired by unsupervised image translation methods, Hu *et al.* [[56](#bib.bib56)]
    propose the mask guided cycle Generative Adversarial Network (GAN) using dual-generator,
    named Mask-ShadowGAN, to employ the unpaired data for shadow removal. Similarly,
    Liu *et al.* [[57](#bib.bib57)] also follow the cycle consistency pipeline while
    integrate the lightness information into the generator. Different from the previous
    works requiring the pre-defined shadow mask, the DC-ShadowNet [[58](#bib.bib58)]
    is proposed to employ the shadow/shadow-free domain classifier as the guidance
    and a series of loss functions to enable the generator focus on the shadow regions.
    He *et al.* [[59](#bib.bib59)] focus on portrait shadow removal instead of general
    one, which proposes to solve shadow removal task as a layer decomposition problem
    and leverages the generative facial priors.
  prefs: []
  type: TYPE_NORMAL
- en: Zero-Shot Learning (ZSL) Supervised learning, and unsupervised learning methods
    exhibit limitations in terms of their ability to generalize or undergo stable
    training. To address these challenges, the concept of zero-shot learning has been
    introduced in low-level vision tasks. In this context, zero-shot learning refers
    to the approach of learning image enhancement solely from testing images, without
    the need for paired or unpaired training data. It is important to note that the
    definition of zero-shot learning in low-level vision tasks differs from its definition
    in high-level visual tasks. The goal is to emphasize that the method does not
    rely on specific training data arrangements, aiming to overcome the limitations
    and instability associated with other learning methods in low-level vision tasks [[60](#bib.bib60),
    [61](#bib.bib61)]. Shadows, in fact, only form partial degradation in images,
    while their non-shadow regions provide rich structural and illumination information
    for shadow regions’ restoration. Based on that, Le *et al.* [[62](#bib.bib62)]
    propose to crop image patches with and without shadows from shadow images as the
    unpaired training samples, which can be trained using shadow images themselves.
    Then they introduce a set of physics-based constraints that enables the adversarial
    training. After that, Liu *et al.* [[63](#bib.bib63)] propose the G2R-ShadowNet
    to leverage shadow generation to simulate shadow degradation in non-shadow regions,
    leading to paired data for training.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/95c38babdffc11a0c321bd7fcc96169a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Illustration of the three major artifacts/challenges in deep learning-based
    shadow removal, i.e., boundary trace (top), color and illumination inconsistency
    (middle), and structure distortion (bottom). Results by the representative method
    for each challenge are shown, i.e., BMNet [[54](#bib.bib54)], ShadowDiffusion [[30](#bib.bib30)],
    and DC-ShadowNet [[58](#bib.bib58)], respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: III-C Challenges and Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The goal of single-image shadow removal is to eliminate shadows from images
    while maintaining or restoring the original appearance of the scene. However,
    there are a few challenges to accomplishing the goal (refer to Figure [4](#S3.F4
    "Figure 4 ‣ III-B General Learning Strategies ‣ III Deep Learning Based Shadow
    Removal ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey")):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58fcf5786e1dbc11af6f0b1d6fb3412a.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Learning Strategy
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/23c6057d474be5fe564a8eb4398b12f0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Network Structure
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4ae3562ee5bec741348ecefdd2182638.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Mask Input
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/24a82972d45092b4113ba3b9b884ee41.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Physic Model
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e03dd0fc4f6be51f1d9b9c0514c6022d.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Training Dataset
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b5c234f8ed0c0cf6912497ac4598550e.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Testing Dataset
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12bbb381b33a10de8eb07461dc9014e4.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) Loss Function
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3502ed5342267dbc5e9a8f7dac44368e.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) Evaluation Metric
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: A statistical analysis of the number of deep learning-based shadow
    removal methods, according to their learning strategy, network characteristic,
    mask input, physic model, training dataset, testing dataset, loss function, and
    evaluation metric.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trace-less result. The patterns of shadows are characterized by their arbitrary
    nature, diversity, and occasionally, by their highly intricate trace structures,
    which present substantial challenges in achieving “trace-less” image restoration.
    Consequently, the fabrication of a shadow mask that accurately and meticulously
    captures the shadow patterns proves to be a formidable task. Moreover, the variations
    in illumination around the shadow boundaries are pronounced, complicating the
    modeling of the sophisticated and abrupt changes in these boundary regions. As
    a consequence of these complexities, the removal of shadows, particularly those
    with hard, sharp edges, is prone to the generation of artifacts along the edges
    where the shadow interfaces with the non-shadowed areas. In addressing this challenge,
    conventional approaches [[16](#bib.bib16), [17](#bib.bib17)] typically involve
    smoothing the gradient around the boundary. Similarly, deep learning-based methods [[58](#bib.bib58),
    [64](#bib.bib64)] have employed customized Total Variation (TV) loss functions
    in boundary regions to mitigate boundary artifacts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Illumination and color consistency. Shadows represent more than just areas of
    darker pixels; they encompass complex visual phenomena characterized by variations
    in color, texture, and brightness. These characteristics are influenced by the
    properties of both the light source and the surfaces they interact with. Furthermore,
    shadows, despite their impact on image quality, constitute only partial degradation
    within images. Conversely, the non-shadowed regions offer rich and accurate color
    and illumination information, serving as a reference standard. As a result, the
    goal of shadow removal is not only to enhance the shadow regions but also to pursue
    consistent illumination and color compared to non-corrupted (non-shadow) regions.
    Several existing deep learning-based methodologies have endeavored to expand the
    receptive field to incorporate valuable information from non-shadowed regions
    as guidance. This has been achieved through various techniques, including the
    fusion of multi-level features [[24](#bib.bib24)], employment of dilated convolutions [[48](#bib.bib48)],
    utilization of patch matching algorithms [[50](#bib.bib50)], and incorporation
    of transformer mechanisms [[55](#bib.bib55)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Structure distortion. Data fidelity is a prevalent concern in various image
    restoration tasks, including shadow removal, where the aim is to produce high-fidelity
    results that closely resemble the original scene. Particularly within the context
    of shadow removal, unique challenges arise. On the one hand, objects with intricate
    textures or reflective surfaces can exhibit complex shadow patterns that are challenging
    to remove without affecting the underlying structure. The removal process must
    distinguish between shadow-related texture and genuine object details to avoid
    distortion. Besides, the boundaries between shadowed and non-shadowed areas can
    be ambiguous or ill-defined, particularly in scenes with diffuse lighting or complex
    geometry. Errors in delineating these boundaries can lead to inaccuracies in shadow
    removal and subsequent structure distortion. On the other hand, some works [[45](#bib.bib45),
    [56](#bib.bib56)] applied adversarial losses for training shadow removal networks
    and tried to model the image distribution explicitly. However, these models usually
    hallucinate unrealistic image contents and structures. To mitigate this issue,
    some existing methods have adopted perceptual loss techniques to ensure structural
    consistency between input shadow images and the resulting outputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: IV Technical Review and Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we review and discuss the technical details and employed priors
    designed to address the aforementioned challenges in Section [III-C](#S3.SS3 "III-C
    Challenges and Analysis ‣ III Deep Learning Based Shadow Removal ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey") from different perspectives,
    such as network structure, integrating physical illumination models, exploiting
    shadow detection for removal, and using deep generative models for shadow removal.
    We then summarize the widely used loss functions, training and testing datasets,
    and evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A Network Structure
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As deep learning techniques advance, there’s a noticeable trend in newly proposed
    methods to incorporate more sophisticated basic blocks with enhanced modeling
    capabilities. These blocks are subsequently integrated to build more complex shadow
    removal networks. In the following section, we provide detailed designs and rationales
    for various basic blocks employed in deep learning-based shadow removal networks,
    as illustrated in Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'U-Net and Multi-Scale Addressing shadow removal as a partial corruption problem
    brings forth its own set of unique challenges, prompting us to emphasize the importance
    of maximizing the receptive field of the model to encompass as much non-corrupted
    context information as possible. The widely adopted U-Net-like structure [[65](#bib.bib65)]
    (Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and
    Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")(a)) serves as a fundamental backbone in shadow removal tasks, representing
    approximately 69% of the approaches, as depicted in Figure [5](#S3.F5 "Figure
    5 ‣ III-C Challenges and Analysis ‣ III Deep Learning Based Shadow Removal ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(b). This architecture
    has garnered widespread usage due to its ability to preserve both low-level structural
    details and high-level semantic information, facilitating high-quality restoration
    of shadow-free images while accurately identifying shadow regions. Moreover, alternative
    structural designs incorporate multi-scale information using various dilated kernels
    (Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and
    Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")(b)) or by leveraging different feature maps extracted from pre-trained
    models, such as VGG16 (Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣
    IV Technical Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey")(f)). This approach also enables the capture of diverse
    information across multiple levels and effectively enlarging the receptive field.'
  prefs: []
  type: TYPE_NORMAL
- en: One-Stage v.s. Multi-Stage A few methods incorporate the use of two-stage or
    multi-stage pipelines for shadow removal. Such a strategy enables the application
    of fine-grained constraints at different stages of the pipeline, facilitating
    more precise and effective restoration. By dividing the whole restoration process
    into multiple stages, each stage can conduct its specific set of constraints.
    For instance, certain techniques [[51](#bib.bib51), [52](#bib.bib52)] integrate
    an additional refinement module following the initial processing stage. This refinement
    step aims to further enhance the structural details or mitigate any artifacts
    present in the output produced by the preceding stage. Nevertheless, in contrast
    to single-stage methods, they often demand double or even multiple times the number
    of parameters and inference time.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7f604e22ffb9e82f121750a851930a73.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The network architectures of the building blocks that are widely
    used in deep shadow removal algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CNN v.s. Transformer The era of deep learning-based shadow removal began in
    the year 2017 with the work of Qu *et al.* [[24](#bib.bib24)]. They introduced
    a pioneering multi-branch fusion architecture (Figure [6](#S4.F6 "Figure 6 ‣ IV-A
    Network Structure ‣ IV Technical Review and Discussion ‣ Single-Image Shadow Removal
    Using Deep Learning: A Comprehensive Survey")(f)) designed to independently extract
    both local and global information using cascaded CNNs across different layers.
    Following this, a series of CNN-based networks [[51](#bib.bib51), [62](#bib.bib62),
    [29](#bib.bib29), [48](#bib.bib48), [58](#bib.bib58)] are constructed to improve
    the performance of shadow removal. After that, the transformer mechanism [[32](#bib.bib32)]
    capitalizes on long-range dependencies within contexts, leading to notable enhancements
    across various vision tasks, including image restoration and enhancement. In scenarios
    involving partial corruption, such as shadow removal, transformer-based networks
    exhibit greater potential due to the limited availability of corrupted contextual
    information to guide restoration. As a result, transformer-based models (Figure [6](#S4.F6
    "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review and Discussion ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")(c)) are becoming
    increasingly prevalent in the field of shadow removal [[66](#bib.bib66), [55](#bib.bib55)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recurrent and Recursive Recurrent neural networks (RNNs) can be used in an
    iterative manner to refine the restoration results over multiple iterations. This
    iterative refinement process allows RNN-based models to effectively address complex
    restoration tasks that require fine-grained adjustments and corrections. Certain
    existing methods [[31](#bib.bib31)] have employed recurrent networks, depicted
    in Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical Review
    and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")(d), to gradually detect and eliminate shadows using a sequential process.
    Hence, employing a recurrent unit like Long Short-Term Memory (LSTM) can ensure
    the preservation of valuable and detailed information, thereby enhancing the accuracy
    of detected shadow regions and producing increasingly realistic shadow-removal
    images. By dividing the process into multiple stages, each with its specific set
    of constraints, the system can better address various aspects of shadow removal,
    resulting in enhanced performance and accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention Mechanism Shadows often occur in specific regions of an image, and
    the attention mechanism allows the model to selectively focus on these shadowed
    area. By attending to relevant parts of the image, the model can effectively target
    shadow regions for removal while preserving non-shadowed areas. Some methods [[45](#bib.bib45),
    [56](#bib.bib56)] directly serve the shadow mask as the attention for the removal.
    Others predict a learnable attentive map as the auxiliary module by considering
    the direction (Figure [6](#S4.F6 "Figure 6 ‣ IV-A Network Structure ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")(g)) or recurrent attention [[67](#bib.bib67)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Summary of key characteristics of notable deep learning-based methods.
    “Retinex” denotes whether the models are based on the Retinex theory. “Simulated”
    indicates whether the testing data are simulated in the same manner as the synthetic
    training data. “Mask” shows whether the models require a mask. “#P” represents
    the number of trainable parameters.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Learning | Network Structure | Loss Function | Training Data
    | Testing Data | Evaluation Metric | Platform | Physic | Mask |'
  prefs: []
  type: TYPE_TB
- en: '| 2017 | DeShadowNet [[24](#bib.bib24)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; multi-branch fusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; semantic network, end-to-end &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| MSE loss (log space) | SRD | UIUC LRSS SRD | RMSE SSIM (LAB space) | Caffe
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | ST-CGAN [[45](#bib.bib45)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; joint learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN, end-to-end &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; detection loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ISTD SBU | SBU UCF ISTD | RMSE (LAB space) | PyTorch |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | SP+M-Net [[51](#bib.bib51)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; regression model(ResNeXt) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; two subnetworks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; regression loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ISTD SBU | SBU UCF ISTD | RMSE (LAB space) | PyTorch | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | DSC [[46](#bib.bib46)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; spatial RNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; direction-aware attention &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; colour loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ISTD SBU | SBU UCF ISTD | RMSE (LAB space) | Caffe |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mask-ShadowGAN [[56](#bib.bib56)] | UL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; dual-generator &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cycle-consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; identity loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SRD ISTD USR | SRD ISTD USR |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | ARGAN [[31](#bib.bib31)] | SSL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; recursive network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; detection loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SRD ISTD | SRD ISTD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TensorFlow |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | RIS-GAN [[47](#bib.bib47)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; four subnetworks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN joint discriminator &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; residual loss illumination loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cross loss adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ISTD SRD | ISTD SRD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; User Study &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TensorFlow |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | DHAN [[48](#bib.bib48)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; dilated convolution, GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; hierarchical attention &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; spatial pooling pyramid &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; binary cross entropy loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; simulated &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LPIPS SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR RMSE &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| TensorFlow |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Le *et al.* [[62](#bib.bib62)] | ZSL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; regression network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; matting loss smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; boundary loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Video Dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Video Dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| RMSE (LAB space) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; TensorFlow &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Fu *et al.* [[52](#bib.bib52)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; regression model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multi-stage, fusion network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; boundary gradient loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SRD ISTD ISTD+ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SRD ISTD ISTD+ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mask-ShadowNet [[53](#bib.bib53)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adaptive instance normalization &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; end-to-end &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ISTD | RMSE (LAB space) |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | CANet [[50](#bib.bib50)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; DenseNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; patch matching mechanism &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; contextual feature transfer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; DenseUNet &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multi-stage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; regression loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($L_{2}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cross-entropy loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; gradient loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| SRD ISTD | SRD ISTD | RMSE (LAB space) | PyTorch |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | G2R-ShadowNet [[63](#bib.bib63)] | ZSL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; three subnetworks &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; identity loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cycle-consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; refinement loss area loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Video Dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Video Dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | LG-ShadowNet [[57](#bib.bib57)] | UL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; multi-stage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; identity loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; cycle-consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; colour loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ISTD ISTD+ USR | ISTD ISTD+ USR |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM User Study &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SSEQ NIQE DBCNN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Runtime #P FLOPs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | DC-ShadowNet [[58](#bib.bib58)] | UL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; GAN &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; chromaticity loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; classification loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; boundary smoothness loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss identity loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SRD ISTD+ ISTD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; USR LRSS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SRD ISTD+ ISTD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; USR LRSS &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Zhu *et al.* [[29](#bib.bib29)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; unfolding &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; recursive network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; regularization loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Runtime #P FLOPs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | BMNet [[54](#bib.bib54)] | SL | invertible network |'
  prefs: []
  type: TYPE_TB
- en: '&#124; colour loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | CNSNet [[66](#bib.bib66)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; transformer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adaptive normalization &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; boundary gradient loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; soft mask loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | SG-ShadowNet [[68](#bib.bib68)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; style transfer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; prototypical normalization &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; area loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; spatial consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Video Dataset &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LPIPS RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | BA-ShadowNet [[69](#bib.bib69)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; encoder-decoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multi-branch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; boundary loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; similarity loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; shadow-free consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | DMTN [[70](#bib.bib70)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; encoder-decoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multi-branch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | ShadowFormer [[55](#bib.bib55)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; transformer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; encoder-decoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; #P RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | SHARDS [[71](#bib.bib71)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; two-stage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ISTD SFHQ | ISTD SFHQ | RMSE (LAB space) | PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | LRA&LDRA [[72](#bib.bib72)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; residual learning &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| ISTD+ SRD | ISTD+ SRD |'
  prefs: []
  type: TYPE_TB
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Runtime #P FLOPs &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | ShadowDiffusion [[30](#bib.bib30)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; diffusion model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; unfolding &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mask loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | FSR-Net [[73](#bib.bib73)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; multi-stage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mask loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | Inpaint4Shadow [[74](#bib.bib74)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; fusion network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Resnet, inpainting network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; mask loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | BCDiff [[64](#bib.bib64)] | ZSL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; diffusion model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; decomposition model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; boundary loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ Video &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ Video &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2024 | Liu *et al.* [[75](#bib.bib75)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; encoder-decoder &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multi-branch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; boundary loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; similarity loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; shadow-free consistency loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PyTorch &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | DeS3 [[76](#bib.bib76)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; diffusion model &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; attention &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; classifier guidance &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; perceptual loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss ($\ell_{1}$) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; adversarial loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; LRSS UCF UIUC &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM User Study &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mei *et al.* [[77](#bib.bib77)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; latent diffusion &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; multi-stage &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD ISTD+ &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; SRD DeSOBA &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM Runtime &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; #P RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '|  | HomoFormer [[78](#bib.bib78)] | SL |'
  prefs: []
  type: TYPE_TB
- en: '&#124; transformer &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; U-Net like network &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; reconstruction loss &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; ISTD+ SRD &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; PSNR SSIM Runtime &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; #P RMSE (LAB space) &#124;'
  prefs: []
  type: TYPE_NORMAL
- en: '| PyTorch |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: IV-B Integrating Physical Illumination Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The physical properties of shadows are effective prior to shadow removal, which
    guarantees that the networks would learn physically plausible transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 'Parameter estimation. Some studies [[51](#bib.bib51), [62](#bib.bib62)] employ
    a linear function to model the relationship between illuminated and shadowed pixels.
    They assume this linear relationship remains consistent during the camera’s color
    acquisition process [[79](#bib.bib79)]. Consequently, the illuminated image can
    be represented as a linear function of its shadowed values:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathbf{I}_{lit}=\omega\circ\mathbf{I}_{s}+b,$ |  | (5)
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle{\color[rgb]{0,0,0}{\hat{\mathbf{I}}_{sf}}}=\bm{\alpha}\circ\mathbf{I}_{s}+(\mathbf{1}-\bm{\alpha})\circ\mathbf{I}_{lit},$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $\omega=\left[\omega_{R},\omega_{G},\omega_{B}\right],b=\left[b_{R},b_{G},b_{B}\right]$
    for different color channels. $b$ represents the camera’s response to direct illumination,
    while $\omega$ denotes the attenuation factor of ambient illumination for a given
    pixel in a particular color channel. With this assumption in mind, they employ
    a deep regression model to estimate the parameters $\omega$ and $b$ based on input
    data comprising both shadow and shadow-free images. Subsequently, an additional
    network, specialized for matte prediction, is applied to generate the pixel-wise
    shadow matte $\bm{\alpha}$. This matte effectively combines information from the
    illuminated and shadowed regions of the images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep unrolling. Another group of works [[29](#bib.bib29), [30](#bib.bib30)]
    employs deep unrolling to enhance the interpretability of models. This popular
    and powerful approach incorporates known physical priors into deep networks, achieving
    superior performance across various image restoration problems [[80](#bib.bib80),
    [81](#bib.bib81), [82](#bib.bib82), [83](#bib.bib83), [84](#bib.bib84), [85](#bib.bib85)].
    The shadow formation model ([2](#S2.E2 "In II-A Shadow Formation Model ‣ II Analysis
    on Shadow Formation ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")) can be re-formulated as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{I}_{s}=\mathbf{A}\circ\mathbf{I}_{sf}\;.$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'Based on that, the shadow removal can be formulated as a degradation prior
    guided model, where regularization terms $\mathcal{R}(\cdot)$ are inferred by
    the learnable conditional deep CNN networks [[29](#bib.bib29)] or generative diffusion
    model [[30](#bib.bib30)] as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{\mathbf{I}}_{sf}=\operatorname*{argmin}_{\mathbf{I}_{sf}}\&#124;\mathbf{I}_{s}-\mathbf{A}\circ\mathbf{I}_{sf}\&#124;_{F}^{2}+\mathcal{R}(\mathbf{I}_{sf})\;.$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: IV-C Exploiting Shadow Detection for Removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Shadow removal often relies on accurate shadow detection. Before removing shadows
    from an image, it’s essential to identify where the shadows are present. Shadow
    detection algorithms help locate regions of shadow within an image, providing
    the necessary information for subsequent removal processes. Most existing shadow
    removal methods leverage rich semantic and structural information learnt from
    shadow detection as prior knowledge, which can roughly be divided into three widely
    used frameworks as shown in Figure [7](#S4.F7 "Figure 7 ‣ IV-C Exploiting Shadow
    Detection for Removal ‣ IV Technical Review and Discussion ‣ Single-Image Shadow
    Removal Using Deep Learning: A Comprehensive Survey"):'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Separate learning: Most shadow removal techniques (refer to the methods that
    require mask input in Table [I](#S4.T1 "TABLE I ‣ IV-A Network Structure ‣ IV
    Technical Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning:
    A Comprehensive Survey")) incorporate a pre-trained shadow detection model before
    the removal stage. This model generates a corresponding shadow mask for the input
    shadow image, which serves as a conditional input for the shadow removal network.
    The shadow detection model remains fixed and operates as a pre-processing step
    for the shadow removal process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Joint learning: The adopted frozen shadow detection model may encounter a domain
    gap when tested on cases with distinct distributions from the training data. Inaccurate
    detection of shadows could misguide the subsequent shadow removal network. To
    address this, some approaches [[45](#bib.bib45)] have employed a joint training
    strategy, optimizing both the shadow detection and removal networks simultaneously.
    This allows for fine-tuning the shadow detector to the current data domain, effectively
    mitigating the domain gap issue.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Multi-task learning: Another category of shadow removal methods [[48](#bib.bib48)]
    has adopted the multi-task learning framework to exploit shared representations
    and simultaneously output shadow mask and shadow-free results. This approach minimizes
    redundancy in feature representation learning and parameter sharing, resulting
    in more efficient utilization of computational resources and decreased inference
    time.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f83b2e1e8e5644a1d8b81de50357d88e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Illustration of the three strategies of combining shadow removal
    and detection, i.e., separate learning, joint learning, and multi-task learning
    strategies.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Deep Generative Model for Shadow Removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generative models, by design, learn the underlying data distribution of training
    images. This allows them to generate or reconstruct images that are close to the
    real shadow-free data distribution, effectively filling in missing textures or
    colors and avoiding amplified artifacts, such as over-smoothness. This is particularly
    useful in shadow removal, where the model must understand the natural image prior
    to effectively removing shadows without leaving traces. Here, we primarily discuss
    two types of generative models that are widely used in shadow removal: Generative
    Adversarial Networks (GANs) [[86](#bib.bib86)] and diffusion models [[37](#bib.bib37)].'
  prefs: []
  type: TYPE_NORMAL
- en: GAN-based methods. GAN-based models for shadow removal leverage the adversarial
    training framework to distinguish between shadow and non-shadow regions, aiming
    to generate shadow-free images that are indistinguishable from real, shadowless
    photos. While the integration of adversarial loss in shadow removal models can
    mitigate some issues such as boundary artifacts, such as those proposed by Wang *et
    al.* [[45](#bib.bib45)] and Hu *et al.* [[56](#bib.bib56)] necessitate careful
    adjustment during training. Furthermore, these methods may exhibit tendencies
    to overfit specific visual features or data distributions, potentially leading
    to the generation of hallucinated content and artifacts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Diffusion-based methods. Recently, numerous diffusion models, including the
    widely recognized diffusion denoising probability model (DDPM)[[37](#bib.bib37)],
    have garnered significant interest in the field of low-level vision[[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39)]. Guo *et al.* [[30](#bib.bib30)]
    pioneered the integration of diffusion models into the domain of shadow removal.
    Subsequently, a series of methods [[76](#bib.bib76), [64](#bib.bib64), [77](#bib.bib77)]
    tend to employ the diffusion model as the backbone. While diffusion-based methods
    have demonstrated their efficacy in producing realistic shadow-free results, they
    are often accompanied by a significant drawback: the time-consuming nature of
    the inference process, primarily attributable to the iterative sampling procedures
    involved. In some applications, such as real-time processing, there may be strict
    requirements on the latency of inference.'
  prefs: []
  type: TYPE_NORMAL
- en: IV-E Loss Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The commonly adopted loss functions in shadow removal methods are the full-reference
    loss, e.g., reconstruction loss (${\color[rgb]{0,0,0}{\ell}}_{1}$, ${\color[rgb]{0,0,0}{\ell}}_{2}$),
    and perceptual loss. Additionally, depending on specific settings and formulations,
    some non-reference loss functions like boundary loss, adversarial loss, and color
    loss are also utilized. We provide a detailed explanation of some representative
    loss functions as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstruction Loss. Reconstruction loss refers to the measure of dissimilarity
    between the ground truth image and the restored image produced by the model, which
    is typically computed by comparing the pixel values of the restored image with
    the corresponding pixel values of the clean image. Different reconstruction losses
    have their advantages and disadvantages [[87](#bib.bib87), [88](#bib.bib88)].
    ${\color[rgb]{0,0,0}{\ell}}_{2}$ loss, also known as mean squared error (MSE)
    loss, penalizes larger errors more than smaller errors. While ${\color[rgb]{0,0,0}{\ell}}_{2}$
    loss is effective at capturing global trends and minimizing overall reconstruction
    errors, it can sometimes blur or smooth out fine details due to its emphasis on
    larger errors. On the other hand, ${\color[rgb]{0,0,0}{\ell}}_{1}$ loss, also
    known as mean absolute error (MAE) loss, treats all errors equally regardless
    of their magnitude. It is less sensitive to outliers and can preserve colors and
    illuminance well, as it does not disproportionately penalize local deviations
    in the image structure. Consequently, ${\color[rgb]{0,0,0}{\ell}}_{1}$ loss can
    result in sharper edges and better preservation of high-frequency details. Structural
    Similarity Index (SSIM) loss is designed to preserve the structure and texture
    of the image, leading to visually pleasing results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Boundary Loss. The illumination surrounding the shadow boundary of shadow images
    often exhibits abrupt variations, easily leading to boundary artifacts in the
    restored results. In order to address this issue, a boundary-aware smoothness
    loss is intuitively employed to encourage smoother transitions along the boundaries
    in methods that require shadow masks. Typically, the boundary-aware smoothness
    loss focuses on minimizing the gradients in the horizontal and vertical directions
    within the boundary regions during the training process [[58](#bib.bib58), [64](#bib.bib64)],
    which is a type of non-reference loss and does not require ground truth shadow-free
    image as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{boundary }}=\left\&#124;\mathrm{~{}B}\left(\mathbf{M}\right)\circ\left&#124;\nabla\left(\hat{\mathbf{I}}_{sf}\right)\right&#124;\right\&#124;_{1},$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $\nabla$ is the gradient operation, $\mathrm{B}$ is a noise-robust function [[89](#bib.bib89),
    [90](#bib.bib90)] to compute the boundaries of the shadow regions from the corresponding
    shadow mask $\mathbf{M}$.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial Loss. Adversarial learning is a technique that involves solving
    a maximization-minimization optimization problem [[86](#bib.bib86)] to foster
    enhanced results that are perceptually indistinguishable from reference images.
    It leverages the interplay between a generator and a discriminator to generate
    enhanced results that closely resemble the reference images, enabling the generation
    of high-quality and realistic outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptual Loss. Perceptual loss is a technique used to encourage the generated
    results to closely resemble the ground truth in the feature space. The loss enhances
    the visual quality of the outputs by considering high-level features. It is computed
    as the Euclidean distance between the feature representations of an enhanced result
    and the corresponding ground truth. Typically, these feature representations are
    extracted from pre-trained networks, such as VGG [[91](#bib.bib91)], which enables
    the model to leverage the knowledge learned from a large dataset like ImageNet [[92](#bib.bib92)],
    making the perceptual loss more effective in capturing meaningful features.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\text{perceptual }}=\&#124;\phi(\mathbf{I}_{sf})-\phi(\hat{\mathbf{I}}_{sf})\&#124;_{2}^{2}\;.$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE II: Summary of shadow datasets. ‘Syn’ represents Synthetic.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Number | Resolution | Format | Real/Syn | Mask | Paired | Type |'
  prefs: []
  type: TYPE_TB
- en: '| Simulated by Physic Model [[17](#bib.bib17), [93](#bib.bib93), [94](#bib.bib94)]
    | +$\infty$ | $\infty$ | RGB | Syn |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Simulated by GAN [[51](#bib.bib51), [56](#bib.bib56), [48](#bib.bib48)] |
    +$\infty$ | $\infty$ | RGB | Syn |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| UIUC [[10](#bib.bib10)] | 108 | $640\times 425$ | RGB | Real | ✓ | ✓ | soft,
    hard, self |'
  prefs: []
  type: TYPE_TB
- en: '| LRSS [[17](#bib.bib17)] | 137 | $5184\times 3456$ | raw | Real |  | ✓ | soft
    |'
  prefs: []
  type: TYPE_TB
- en: '| SRD [[24](#bib.bib24)] | 3,088 | $800\times 640$ | RGB | Real |  | ✓ | soft,
    hard |'
  prefs: []
  type: TYPE_TB
- en: '| ISTD & ISTD+ [[45](#bib.bib45), [51](#bib.bib51)] | 1,870 | $640\times 480$
    | RGB | Real | ✓ | ✓ | hard |'
  prefs: []
  type: TYPE_TB
- en: '| USR [[56](#bib.bib56)] | 2,445 | $600\times 450$ | RGB | Real |  |  | soft,
    hard |'
  prefs: []
  type: TYPE_TB
- en: '| SBU [[95](#bib.bib95)] | 5,000 | Various | RGB | Real | ✓ |  | soft, self,
    hard |'
  prefs: []
  type: TYPE_TB
- en: 'Color Loss. Some methods [[96](#bib.bib96), [57](#bib.bib57)] encourage the
    colour in restored result $\hat{\mathbf{I}}_{sf}$ to be the same with the shadow-free
    reference $\mathbf{I}_{sf}$ according to the extra colour loss as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{\text{color }}=\sum_{p}\left(1-\cos<(\mathbf{I}_{sf}^{LAB})_{p},(\hat{\mathbf{I}}_{sf}^{LAB})_{p}>\right),$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $(\cdot)_{p}$ denotes the pixel with index $p$ and and $\cos<\cdot,\cdot>$
    represents the cosine angle between vectors. Each pixel in the restored image
    or input image is treated as a 3D vector representing the Lab color space. The
    cosine similarity between two color vectors equals $1$ when the vectors are perfectly
    aligned.
  prefs: []
  type: TYPE_NORMAL
- en: IV-F Training and Testing Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: UIUC [[10](#bib.bib10)] comprises 108 images, each featuring a shadow image
    and its corresponding ground truth shadow-free image. Of these, 72 images are
    designated for training, and the remaining 76 for testing. The dataset includes
    various shadow types, such as soft shadows and self-shadows. Among the 76 test
    image pairs, 46 pairs are created by removing the shadow source while keeping
    the light source unchanged. In the remaining 30 pairs, shadows are cast by objects
    within the scene, with the shadow-free image generated by blocking the light source,
    enveloping the entire scene in shadow. The corresponding shadow mask is generated
    by thresholding the ratio between the paired shadow and shadow-free images.
  prefs: []
  type: TYPE_NORMAL
- en: 'LRSS [[17](#bib.bib17)] is a collection of real soft shadow test photographs.
    It includes 137 images, 37 of which have corresponding ground truth shadow-free
    versions (along with mattes) to benchmark shadow removal methods. These ground
    truth images were produced by setting up a camera on a tripod and taking two photos:
    one with the shadow and one without, by removing the object casting the shadow.'
  prefs: []
  type: TYPE_NORMAL
- en: SRD [[24](#bib.bib24)] comprises 2,680 training and 408 testing pairs of shadow
    and shadow-free images. This dataset does not feature manually annotated masks.
    Like ISTD, SRD includes shadows cast by various objects, then removing the shadow
    source to capture the corresponding shadow-free image. Shadow images are captured
    under different illumination conditions, such as varied weather conditions or
    different times of the day, to incorporate both hard and soft shadows into the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: ISTD and ISTD+ [[45](#bib.bib45), [51](#bib.bib51)] The ISTD dataset [[45](#bib.bib45)]
    comprises 1330 training and 540 testing triplets, consisting of shadow images,
    corresponding manually annotated masks, and shadow-free images. This dataset primarily
    comprises outdoor scenes and predominantly contains hard shadows. Shadows are
    cast by various objects not present in the scene, and to capture the corresponding
    shadow-free image, the shadow source is removed. The Adjusted ISTD (ISTD+) dataset [[51](#bib.bib51)]
    addresses illumination inconsistencies between the shadow and shadow-free images
    present in the original ISTD dataset.
  prefs: []
  type: TYPE_NORMAL
- en: USR [[56](#bib.bib56)] is an unpaired shadow removal dataset consisting of 2,445
    shadow images and 1,770 shadow-free images. It features a diverse range of scenes,
    encompassing over a thousand different environments where shadows are cast by
    various objects such as trees, buildings, traffic signs, people, umbrellas, railings,
    and more. The dataset includes 1,956 images for training and 489 for testing,
    utilizing all 1,770 shadow-free images exclusively for training.
  prefs: []
  type: TYPE_NORMAL
- en: SBU [[95](#bib.bib95)] comprises 5,000 images depicting scenes with shadows,
    spanning diverse environments such as urban areas, beaches, mountains, roads,
    parks, snowscapes, animals, vehicles, and houses. The collection includes various
    types of photographs, including aerial shots, landscapes, close-ups, and selfies.
    Approximately 85% of the images are allocated to the training set, with the remaining
    15% reserved for testing. Among the 700 test images, detailed annotations of shadow
    masks ensure pixel accuracy. During training image annotation, a quick method
    involving sparse strokes on shadow areas was employed for efficient labeling of
    a large image set.
  prefs: []
  type: TYPE_NORMAL
- en: Video [[62](#bib.bib62)] shadow removal dataset comprises 8 videos featuring
    static scenes with unchanging backgrounds. Each video includes a corresponding
    $V_{max}$ frame, which serves as the pseudo shadow-free reference, created by
    capturing the maximum intensity values at each pixel location throughout the video.
    The mask for moving shadows includes pixels found in both shadowed and non-shadowed
    areas, defining the evaluation region. The moving shadow mask is generated by
    applying a threshold of 80.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Quantitative comparisons of the shadow removal results on SRD dataset [[24](#bib.bib24)]
    in terms of RMSE, PSNR (in dB), and SSIM. Highlighted in red, yellow, and green
    cells are the best, second-best, and third-best results, respectively. ^§ indicates
    that the results are directly quoted from the original paper as their code implementation
    was not made available.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | Shadow Region (S) | Non-Shadow Region (NS) | All Image
    (ALL) |'
  prefs: []
  type: TYPE_TB
- en: '| PSNR$\uparrow$ | SSIM $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM
    $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | RMSE$\downarrow$
    | LPIPS$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | input | 18.96 | 0.871 | 36.69 | 31.47 | 0.975 | 4.83 | 18.19 | 0.830 |
    14.05 | 0.1899 |'
  prefs: []
  type: TYPE_TB
- en: '| TR | Guo *et al.* [[10](#bib.bib10)](TPAMI’12) | - | - | 29.89 | - | - |
    6.47 | - | - | 12.60 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SL | DeshadowNet [[24](#bib.bib24)](CVPR’17) | - | - | 11.78 | - | - | 4.84
    | - | - | 6.64 | - |'
  prefs: []
  type: TYPE_TB
- en: '| DSC [[46](#bib.bib46)](TPAMI’19) | 30.65 | 0.960 | 8.62 | 31.94 | 0.965 |
    4.41 | 27.76 | 0.903 | 5.71 | 0.1141 |'
  prefs: []
  type: TYPE_TB
- en: '| DHAN [[48](#bib.bib48)](AAAI’21) | 33.67 | 0.978 | 8.94 | 34.79 | 0.979 |
    4.80 | 30.51 | 0.949 | 5.67 | 0.0666 |'
  prefs: []
  type: TYPE_TB
- en: '| AEF [[52](#bib.bib52)](CVPR’21) | 32.26 | 0.966 | 9.55 | 31.87 | 0.945 |
    5.74 | 28.40 | 0.893 | 6.50 | 0.0894 |'
  prefs: []
  type: TYPE_TB
- en: '| EMDN [[29](#bib.bib29)](AAAI’21) | 34.94 | 0.980 | 7.44 | 35.85 | 0.982 |
    3.74 | 31.72 | 0.952 | 4.79 | 0.0980 |'
  prefs: []
  type: TYPE_TB
- en: '| BMNet [[54](#bib.bib54)](CVPR’22) | 35.05 | 0.981 | 6.61 | 36.02 | 0.982
    | 3.61 | 31.69 | 0.956 | 4.46 | 0.0549 |'
  prefs: []
  type: TYPE_TB
- en: '| ShadowFormer [[55](#bib.bib55)](AAAI’23) | 35.55 | 0.982 | 6.14 | 36.82 |
    0.983 | 3.54 | 32.46 | 0.957 | 4.28 | 0.0572 |'
  prefs: []
  type: TYPE_TB
- en: '| ShadowDiffusion [[30](#bib.bib30)](CVPR’23) | 38.72 | 0.987 | 4.98 | 37.78
    | 0.985 | 3.44 | 34.73 | 0.970 | 3.63 | 0.0359 |'
  prefs: []
  type: TYPE_TB
- en: '| Inpaint4Shadow [[74](#bib.bib74)](ICCV’23) | 36.73 | 0.985 | 5.70 | 36.70
    | 0.985 | 3.27 | 33.27 | 0.967 | 3.81 | 0.0475 |'
  prefs: []
  type: TYPE_TB
- en: '| DeS3 [[76](#bib.bib76)](AAAI’24) | 37.91 | 0.986 | 5.27 | 37.45 | 0.984 |
    3.03 | 34.11 | 0.968 | 3.56 | 0.0338 |'
  prefs: []
  type: TYPE_TB
- en: '| HomoFormer [[78](#bib.bib78)](CVPR’24) | 38.81 | 0.987 | 4.25 | 39.45 | 0.988
    | 2.85 | 35.37 | 0.972 | 3.33 | 0.0419 |'
  prefs: []
  type: TYPE_TB
- en: '| UL | DC-ShadowNet [[58](#bib.bib58)](ICCV’21) | 34.00 | 0.975 | 7.70 | 35.53
    | 0.981 | 3.65 | 31.53 | 0.955 | 4.65 | 0.1285 |'
  prefs: []
  type: TYPE_TB
- en: '| SSL | ARGAN^§ [[31](#bib.bib31)](ICCV’19) | - | - | 6.35 | - | - | 4.46 |
    - | - | 5.31 | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IV: Quantitative comparisons of the shadow removal results on ISTD+ dataset [[45](#bib.bib45),
    [51](#bib.bib51)] in terms of RMSE, PSNR (in dB), SSIM, and LPIPS. ^† indicates
    that the methods use the ground truth shadow mask provided by ISTD+ dataset as
    input.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | Shadow Region (S) | Non-Shadow Region (NS) | All Image
    (ALL) |'
  prefs: []
  type: TYPE_TB
- en: '| PSNR$\uparrow$ | SSIM $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM
    $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM$\uparrow$ | RMSE$\downarrow$
    | LPIPS$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | input | 20.83 | 0.93 | 39.01 | 37.46 | 0.985 | 2.40 | 20.46 | 0.894 |
    8.40 | 0.1431 |'
  prefs: []
  type: TYPE_TB
- en: '| TR | Guo *et al.* [[10](#bib.bib10)](TPAMI’12) | 26.89 | 0.960 | 21.38 |
    35.56 | 0.976 | 3.09 | 25.52 | 0.925 | 6.09 | 0.0966 |'
  prefs: []
  type: TYPE_TB
- en: '| SL | SP+M-Net [[51](#bib.bib51)](ICCV’19) | 37.60 | 0.990 | 5.91 | 36.02
    | 0.976 | 2.99 | 32.94 | 0.962 | 3.46 | 0.0586 |'
  prefs: []
  type: TYPE_TB
- en: '| Param+M+D-Net [[62](#bib.bib62)] | 33.09 | 0.983 | 9.67 | 35.35 | 0.978 |
    2.82 | 30.15 | 0.951 | 3.94 | 0.0662 |'
  prefs: []
  type: TYPE_TB
- en: '| AEF [[52](#bib.bib52)](CVPR’21) | 36.04 | 0.978 | 6.55 | 31.16 | 0.892 |
    3.77 | 29.45 | 0.861 | 4.23 | 0.0615 |'
  prefs: []
  type: TYPE_TB
- en: '| BMNet^† [[54](#bib.bib54)](CVPR’22) | 37.87 | 0.991 | 5.62 | 37.51 | 0.985
    | 2.45 | 33.98 | 0.972 | 2.97 | 0.0305 |'
  prefs: []
  type: TYPE_TB
- en: '| SG-ShadowNet [[68](#bib.bib68)](ECCV’22) | 36.80 | 0.990 | 5.93 | 35.57 |
    0.978 | 2.92 | 32.46 | 0.962 | 3.41 | 0.0430 |'
  prefs: []
  type: TYPE_TB
- en: '| Inpaint4Shadow [[74](#bib.bib74)](CVPR’23) | 38.10 | 0.990 | 6.09 | 37.66
    | 0.981 | 2.82 | 34.16 | 0.967 | 3.35 | 0.0675 |'
  prefs: []
  type: TYPE_TB
- en: '| ShadowFormer^† [[55](#bib.bib55)](AAAI’23) | 39.48 | 0.992 | 5.23 | 38.82
    | 0.983 | 2.30 | 35.46 | 0.971 | 2.78 | 0.0260 |'
  prefs: []
  type: TYPE_TB
- en: '| ShadowDiffusion^† [[30](#bib.bib30)](CVPR’23) | 39.69 | 0.992 | 4.97 | 38.89
    | 0.987 | 2.28 | 35.67 | 0.975 | 2.72 | 0.0234 |'
  prefs: []
  type: TYPE_TB
- en: '| HomoFormer^† [[78](#bib.bib78)](CVPR’24) | 39.49 | 0.993 | 4.73 | 38.75 |
    0.984 | 2.23 | 35.35 | 0.975 | 2.64 | 0.0259 |'
  prefs: []
  type: TYPE_TB
- en: '| UL | DC-ShadowNet [[58](#bib.bib58)](ICCV’21) | 31.06 | 0.976 | 12.62 | 27.03
    | 0.961 | 6.82 | 25.03 | 0.926 | 7.77 | 0.1420 |'
  prefs: []
  type: TYPE_TB
- en: '| ZSL | G2R-ShadowNet [[63](#bib.bib63)](CVPR’21) | 33.58 | 0.979 | 8.82 |
    35.52 | 0.976 | 2.89 | 30.52 | 0.944 | 3.86 | 0.0662 |'
  prefs: []
  type: TYPE_TB
- en: '| BCDiff [[64](#bib.bib64)](ICCV’23) | 35.71 | 0.986 | 7.61 | 36.39 | 0.981
    | 2.66 | 32.11 | 0.959 | 3.47 | 0.0518 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE V: Quantitative comparisons on LRSS dataset [[17](#bib.bib17)] and UIUC [[10](#bib.bib10)]
    in terms of RMSE, PSNR (in dB), SSIM, and LPIPS.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | LRSS | UIUC |'
  prefs: []
  type: TYPE_TB
- en: '| PSNR$\uparrow$ | SSIM $\uparrow$ | RMSE$\downarrow$ | PSNR$\uparrow$ | SSIM
    $\uparrow$ | RMSE$\downarrow$ |'
  prefs: []
  type: TYPE_TB
- en: '|  | input | 16.43 | 0.886 | 15.98 | 20.85 | 0.803 | 13.97 |'
  prefs: []
  type: TYPE_TB
- en: '|  | SP+M-Net [[51](#bib.bib51)](ICCV’19) | 21.77 | 0.927 | 11.18 | 28.23 |
    0.866 | 7.13 |'
  prefs: []
  type: TYPE_TB
- en: '|  | EMDN [[29](#bib.bib29)](AAAI’22) | 19.26 | 0.882 | 15.71 | 25.56 | 0.775
    | 13.07 |'
  prefs: []
  type: TYPE_TB
- en: '|  | BMNet [[54](#bib.bib54)](CVPR’22) | 14.19 | 0.048 | 50.79 | 16.86 | 0.679
    | 45.68 |'
  prefs: []
  type: TYPE_TB
- en: '| SL | Inpaint4Shadow [[74](#bib.bib74)](CVPR’23) | 19.59 | 0.805 | 13.79 |
    27.77 | 0.849 | 7.82 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ShadowFormer [[55](#bib.bib55)](AAAI’23) | 27.01 | 0.957 | 6.37 | 28.96
    | 0.874 | 6.57 |'
  prefs: []
  type: TYPE_TB
- en: '|  | ShadowDiffusion [[30](#bib.bib30)](CVPR’23) | 28.37 | 0.955 | 6.01 | 29.02
    | 0.880 | 7.05 |'
  prefs: []
  type: TYPE_TB
- en: '|  | HomoFormer [[78](#bib.bib78)](CVPR’24) | 25.86 | 0.951 | 6.98 | 29.08
    | 0.870 | 6.81 |'
  prefs: []
  type: TYPE_TB
- en: '| UL | DC-ShadowNet [[58](#bib.bib58)](ICCV’21) | 20.89 | 0.902 | 12.55 | 24.85
    | 0.849 | 9.51 |'
  prefs: []
  type: TYPE_TB
- en: '| ZSL | G2R-ShadowNet [[63](#bib.bib63)](CVPR’21) | 20.90 | 0.901 | 9.99 |
    27.56 | 0.858 | 7.43 |'
  prefs: []
  type: TYPE_TB
- en: '| BCDiff [[64](#bib.bib64)](ICCV’23) | 22.13 | 0.922 | 10.68 | 26.81 | 0.852
    | 7.96 |'
  prefs: []
  type: TYPE_TB
- en: IV-G Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PSNR and RMSE. PSNR measures the ratio between the maximum signal power and
    the power of noise affecting image fidelity, while RMSE calculates the square
    root of the average squared difference between corresponding pixels of the reference
    and reconstructed images. Both metrics quantify the discrepancy between images,
    with lower values indicating higher distortion or error.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MAE measures the average magnitude of errors between predicted shadow-free image
    and ground truth. It calculates the absolute difference between each predicted
    value and its corresponding ground truth value, then averages these absolute differences
    to provide a single score.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SSIM [[97](#bib.bib97)] is a metric evaluating similarity between images, considering
    perceptual differences, and treating image degradation as a perceived alteration
    in structural information. It measures luminance, contrast, and structure, providing
    a score between -1 and 1, with 1 indicating perfect structural similarity.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LPIPS [[98](#bib.bib98)] is a perceptual similarity metric that evaluates differences
    between images based on their perceived similarity by a deep neural network. It
    measures the distance between feature representations of images, capturing both
    low-level and high-level features. Lower LPIPS scores indicate greater perceptual
    similarity, making it effective for tasks where human judgment is crucial, like
    image generation and restoration.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NIQE [[99](#bib.bib99)] is a no-reference metric for estimating image quality.
    It analyzes local statistics like luminance and contrast, comparing them to those
    of natural images to compute a quality score. Higher NIQE scores indicate lower
    image quality. It’s useful for tasks like compression and denoising but may not
    perform as well on structured or synthetic images.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: V Benchmarking and Empirical Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This section provides empirical analysis and highlights some key challenges
    in deep learning based shadow removal methods. We conduct extensive evaluations
    on several public benchmarks. We select a number of recent shadow removal algorithms
    from different categories to be evaluated:'
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multi-context Embedding, DeShadowNet [[24](#bib.bib24)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Attentive Recurrent GAN, ARGAN [[31](#bib.bib31)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Direction-aware Spatial Context, DSC [[46](#bib.bib46)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Shadow Parameter Estimation and Matte Prediction Network, SP+M-Net [[51](#bib.bib51)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dual Hierarchical Aggregation Network, DHAN [[48](#bib.bib48)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Generation to Removal, G2R-ShadowNet [[63](#bib.bib63)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '7.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Domain Classifier Guided Net, DC-ShadowNet [[58](#bib.bib58)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '8.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Auto-Exposure Fusion, AEF [[52](#bib.bib52)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '9.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Efficient Model-Driven Network, EMDN [[29](#bib.bib29)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '10.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Bijective Mapping Network, BMNet [[54](#bib.bib54)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '11.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context Transformer, ShadowFormer [[55](#bib.bib55)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '12.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unrolling-inspired Diffusion, ShadowDiffusion [[30](#bib.bib30)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '13.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Boundary-aware Conditional Diffusion, BCDiff [[64](#bib.bib64)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '14.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Diffusion-based Soft & Self Shadow Removal, DeS3 [[76](#bib.bib76)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '15.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Homogenized Transformer, HomoFormer [[78](#bib.bib78)]
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: including eleven supervised learning-based methods (DeShadowNet, DSC, SP+M-Net,
    DHAN, AEF, EMDN, BMNet, ShadowFormer, ShadowDiffusion, SeS3, HomoFormer), two
    zero-shot learning-based methods (G2R-ShadowNet, BCDiff), one unsupervised learning-based
    method (DC-ShadowNet), one semi-supervised learning-based method (ARGAN). To ensure
    fair comparisons, we utilize publicly available code for all the methods under
    consideration to generate their results. We leverage full-reference metrics including
    PSNR, SSIM, RMSE, and LPIPS for those testsets with ground truth shadow-free images,
    i.e., ISTD+ [[45](#bib.bib45), [51](#bib.bib51)], SRD [[24](#bib.bib24)], LRSS [[17](#bib.bib17)],
    and UIUC [[10](#bib.bib10)]. The results of shadow removal from all methods are
    either sourced from their original papers or replicated using their official implementations.
    We standardize the evaluation of shadow removal results to a resolution of $256\times
    256$ following most shadow removal methods [[52](#bib.bib52), [29](#bib.bib29),
    [54](#bib.bib54), [30](#bib.bib30)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/36061119af2bd87986f0171df380187f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Input
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1a9fb15d9808f2e280a35797cb21f031.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Mask (Residual)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e6c7b2dcf9c15fa0e9798c06d3fc545e.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Mask (DHAN [[48](#bib.bib48)])
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/335fae97ca2ada978ab5eeb146777950.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) DSC [[46](#bib.bib46)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7cd9439435e1f01cf8ffcc65f2cd7d38.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) DHAN [[48](#bib.bib48)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d50ae3a84eab2dbf89ac18da7a021cc.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) AEF [[52](#bib.bib52)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ed9d7429f286b397568fd3c8baeb44d7.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) EMDN [[29](#bib.bib29)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0d9b065bb29dd27be32160147834c073.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) BMNet [[54](#bib.bib54)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6cc8695e044d7d0a83af85af848e131f.png)'
  prefs: []
  type: TYPE_IMG
- en: (i) DC-ShadowNet [[58](#bib.bib58)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3cb5a823c67c2d1636720c575c4327b9.png)'
  prefs: []
  type: TYPE_IMG
- en: (j) SG-ShadowNet [[68](#bib.bib68)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d6601046b57ab63380bc2bd4faf4dd86.png)'
  prefs: []
  type: TYPE_IMG
- en: (k) Inpaint4Shadow [[74](#bib.bib74)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a00979bcd3c75bcdb31de6a3e75b400a.png)'
  prefs: []
  type: TYPE_IMG
- en: (l) ShadowDiffusion [[30](#bib.bib30)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1f899686566d05a9a1a15951b4002d6e.png)'
  prefs: []
  type: TYPE_IMG
- en: (m) DeS3 [[76](#bib.bib76)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f367682b39686cf551da671986cd9c46.png)'
  prefs: []
  type: TYPE_IMG
- en: (n) HomoFormer [[78](#bib.bib78)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/278f852c85b7786ff16eab0e14be7fc0.png)'
  prefs: []
  type: TYPE_IMG
- en: (o) Reference
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Visual comparison with different shadow removal methods sampled from
    test set of SRD [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: V-A Benchmarking Results
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Tables [III](#S4.T3 "TABLE III ‣ IV-F Training and Testing Datasets ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey")&[IV](#S4.T4 "TABLE IV ‣ IV-F Training and Testing Datasets ‣ IV Technical
    Review and Discussion ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey") show the quantitative results on the testsets over ISTD+, and SRD, respectively.
    When the distribution of testing data is very similar to that of training data,
    supervised methods achieve better performance. Typically, some recent transformer
    or diffusion based methods achieved state-of-the-art over ISTD+ and SRD datasets.
    To further demonstrate the advantages and disadvantages of different methods,
    Figures [8](#S5.F8 "Figure 8 ‣ V Benchmarking and Empirical Analysis ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey")&[9](#S5.F9 "Figure
    9 ‣ V-A Benchmarking Results ‣ V Benchmarking and Empirical Analysis ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey") present the visual
    examples of the shadow removal results on SRD dataset. For hard shadow removal,
    the results often exhibit noticeable boundary artifacts due to the difficulty
    in distinguishing sharp shadow edges from structural edges in the original scene,
    as illustrated in the lower right corner of Figure [8](#S5.F8 "Figure 8 ‣ V Benchmarking
    and Empirical Analysis ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey"). It also highlights that in complex scenarios involving multiple shadow
    types, such as self-shadows and cast shadows, certain shadow regions are often
    overlooked by shadow detectors. This oversight results in significant residual
    shadows evident in the outputs of most methods. Inaccurate masks can readily yield
    incorrect results across all types of shadows. Existing methods have utilized
    a range of techniques, including joint mask refinement [[30](#bib.bib30)], regional
    classification [[76](#bib.bib76)], shadow matte prediction [[51](#bib.bib51)],
    to tackle the issue of mask inaccuracy, which enable better processing of ignored
    regions.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b994192fd425fc75566b112671a2fd34.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Input
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a2a87d05ce9a328c010a92384c950b4f.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Mask (Residual)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/65cb4062256870d7cde50476c29aadae.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Mask (DHAN [[48](#bib.bib48)])
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d20d4ac7256608947e25afccc9275978.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) DSC [[46](#bib.bib46)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ed15cfdbb056c48ef38f8250ab74c90.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) DHAN [[48](#bib.bib48)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/402f09795c139d27f18e030e99d760b6.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) AEF [[52](#bib.bib52)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/94c14e387cd259507e99c0f8654c97bf.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) EMDN [[29](#bib.bib29)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/118f72ed1ffc838019186b42f3de95be.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) BMNet [[54](#bib.bib54)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a50621aef090d75f6d6077f3263dfa8c.png)'
  prefs: []
  type: TYPE_IMG
- en: (i) DC-ShadowNet [[58](#bib.bib58)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ec4983532a016f01cd55d1071f40fb89.png)'
  prefs: []
  type: TYPE_IMG
- en: (j) SG-ShadowNet [[68](#bib.bib68)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/17d94b44740597088d6d2b756ab8ee87.png)'
  prefs: []
  type: TYPE_IMG
- en: (k) Inpaint4Shadow [[74](#bib.bib74)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bdaea7cf04154e1225941b39948656e6.png)'
  prefs: []
  type: TYPE_IMG
- en: (l) ShadowDiffusion [[30](#bib.bib30)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1af1ef7cc4bca9c3d301090ec84690a3.png)'
  prefs: []
  type: TYPE_IMG
- en: (m) DeS3 [[76](#bib.bib76)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c07e13b646f97149e52ecc9d88bfe675.png)'
  prefs: []
  type: TYPE_IMG
- en: (n) HomoFormer [[78](#bib.bib78)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f05b845c94f1563d2f222fc9fa9525b1.png)'
  prefs: []
  type: TYPE_IMG
- en: (o) Reference
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 9: Visual comparison with different shadow removal methods sampled from
    test set of SRD [[24](#bib.bib24)].'
  prefs: []
  type: TYPE_NORMAL
- en: V-B Generalization Capability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further assess the generalization capabilities of current methods, we utilize
    the LRSS and UIUC datasets as our evaluation sets. These datasets are specifically
    chosen because they lack large-scale paired training data, enabling us to rigorously
    test how well the methods perform in scenarios where there is a distinct distribution
    difference between training and testing data. For the LRSS dataset, we binarize
    the residuals of shadow-free images and shadow images to generate mask inputs
    for methods that require a shadow mask. For the UIUC dataset, we utilize the ground
    truth masks provided by the dataset. To ensure a fair comparison, we consistently
    use the ISTD+ training set to train all methods. The performances of most shadow
    removal methods significantly deteriorate when transferring to testing data that
    differs markedly from the training data. For instance, models trained on pairs
    from ISTD+ encounter difficulties in effectively handling shadow images from the
    soft shadow dataset LRSS, as illustrated in Table [V](#S4.T5 "TABLE V ‣ IV-F Training
    and Testing Datasets ‣ IV Technical Review and Discussion ‣ Single-Image Shadow
    Removal Using Deep Learning: A Comprehensive Survey"). This is because the ISTD+
    dataset predominantly consists of hard shadow types and lacks soft shadow and
    self-shadow samples. On the contrary, those unsupervised or zero-shot learning
    methods may have better generalization ability to the unseen scenarios. Some methods,
    like DC-ShadowNet, considered the classification of shadow types can have a better
    robustness cross various shadow types. ShadowDiffusion employed the mask refinement
    as the auxiliary branch to achieve a better robustness to soft shadow processing.'
  prefs: []
  type: TYPE_NORMAL
- en: V-C Computational Complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In order to provide a comprehensive assessment of deep learning-based shadow
    removal techniques, we conduct a comparative analysis of the computational complexity
    of various state-of-the-art methods, as detailed in Table [VI](#S5.T6 "TABLE VI
    ‣ V-C Computational Complexity ‣ V Benchmarking and Empirical Analysis ‣ Single-Image
    Shadow Removal Using Deep Learning: A Comprehensive Survey"), including runtime,
    training parameters, and FLOPs over 100 images of size $256\times 256$ using one
    NVIDIA RTX A5000 GPU. Inpaint4Shadow [[74](#bib.bib74)] and DC-ShadowNet [[58](#bib.bib58)]
    have the shortest runtimes. BMNet [[54](#bib.bib54)] employs a highly efficient
    architecture based on lightweight invertible blocks, resulting in significantly
    fewer model parameters and lower FLOPs. Some methods use multiple sub-networks
    to perform multi-stage processing, which inevitably requires multiples of the
    parameters and computational load, such as SP+M-Net [[51](#bib.bib51)]. Specially,
    diffusion-based methods, such as ShadowDiffusion [[30](#bib.bib30)] and BCDiff [[64](#bib.bib64)],
    require iterative denoising steps. Although multi-stage methods can achieve great
    performance, they require significantly increased computational complexity and
    processing time, making them impractical for many applications.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Quantitative comparisons of the algorithms’ computational efficiency
    in terms of runtime (in second), number of trainable parameters (#Parameters)
    (in M), and FLOPs (in G).'
  prefs: []
  type: TYPE_NORMAL
- en: '| Learning | Method | RunTime$\downarrow$ | #Parameters $\downarrow$ | FLOPs$\downarrow$
    | Platform |'
  prefs: []
  type: TYPE_TB
- en: '|  | SP+M-Net [[51](#bib.bib51)] | 0.023 | 141.18 | 61.52 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '|  | DHAN [[48](#bib.bib48)] | 0.283 | 21.75 | 262.87 | TensorFlow |'
  prefs: []
  type: TYPE_TB
- en: '|  | EMDN [[29](#bib.bib29)] | 0.015 | 10.06 | 56.29 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '|  | BMNet [[54](#bib.bib54)] | 0.036 | 0.37 | 10.99 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| SL | ShadowFormer [[55](#bib.bib55)] | 0.031 | 11.35 | 64.60 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '|  | Inpaint4Shadow [[74](#bib.bib74)] | 0.006 | 14.98 | 81.18 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '|  | ShadowDiffusion [[30](#bib.bib30)] | 0.723 | 60.74 | 937.15 | PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | HomoFormer [[78](#bib.bib78)] | 0.042 | 17.81 | 35.63 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| UL | DC-ShadowNet [[58](#bib.bib58)] | 0.008 | 10.59 | 52.55 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: '| ZSL | G2R-ShadowNet [[63](#bib.bib63)] | 0.010 | 22.76 | 113.93 | PyTorch
    |'
  prefs: []
  type: TYPE_TB
- en: '| BCDiff [[64](#bib.bib64)] | 31.446 | 276.41 | 139218.75 | PyTorch |'
  prefs: []
  type: TYPE_TB
- en: VI Shadow Removal Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section provides an overview of applications related to shadow removal
    and discusses its necessity, including shadow generation and shadow-related attacks.
  prefs: []
  type: TYPE_NORMAL
- en: VI-A Shadow Generation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Shadow generation and synthesis typically involve the process of creating or
    simulating shadows in digital images or computer graphics. Similarly to shadow
    removal, shadow generation serves as an important application, which can primarily
    be divided into the following two aspects according to their subsequent applications:
    (1) Incorporating a foreground object into a background image to create a composite
    image relies heavily on the presence of shadows. Shadows are pivotal in rendering
    scenes realistically as they convey depth, perspective, and illumination. (2)
    Additionally, shadow generation can aid in simulating large-scale paired images
    that include both shadows and shadow-free counterparts. This process is achieved
    at a minimal expense, contributing significantly to data augmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3f7516bbc4a939f578e53ea1c76cd806.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Input
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/425ff3a45c7ea92ffe2fda4fc085d3e5.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Mask (Residual)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5234ad42fcdd98827c094cf1836261a2.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) SP+M-Net [[51](#bib.bib51)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6144d8908091f102d432d1a803cfbb8e.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) EMDN [[29](#bib.bib29)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9acc7b3689f16a9514f6ec609cb532b5.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) G2R-ShadowNet [[63](#bib.bib63)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/259eec93654474500547f34b310b76f4.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Inpaint4Shadow [[74](#bib.bib74)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4997729c4fa6337457df728976f31676.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) ShadowFormer [[55](#bib.bib55)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fdd9464b462094761d3e5551e8cc8a3c.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) ShadowDiffusion [[30](#bib.bib30)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/29daff33996d1b1b6b6e9e1ff677890f.png)'
  prefs: []
  type: TYPE_IMG
- en: (i) HomoFormer [[78](#bib.bib78)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ac1faea8eb45e1153f6379aa5eab5beb.png)'
  prefs: []
  type: TYPE_IMG
- en: (j) GT
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 10: Visual comparison with different shadow removal methods sampled
    from test set of LRSS [[17](#bib.bib17)].'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c878251d70059c596d2d71c2f39cd206.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Input
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5ddf34b6e63a38b1093f14d9d1fa5f3a.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Mask (Residual)
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f95b2bf03a88638cce663ed6fcb6626.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) SP+M-Net [[51](#bib.bib51)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0041e9bbbbdec59d9604f0daabd9f382.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) EMDN [[29](#bib.bib29)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8008f2550ed31bacc032421c5bd1741b.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) G2R-ShadowNet [[63](#bib.bib63)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f51f1149f7b8defd20f6ea25bdb0ac12.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Inpaint4Shadow [[74](#bib.bib74)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dc211fb205c50ac3ff753b5ca0616c68.png)'
  prefs: []
  type: TYPE_IMG
- en: (g) ShadowFormer [[55](#bib.bib55)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cdf7e58149dc8ffade68c2cbdd4fa6cd.png)'
  prefs: []
  type: TYPE_IMG
- en: (h) ShadowDiffusion [[30](#bib.bib30)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c721281b51e61b80f1912556b5a18827.png)'
  prefs: []
  type: TYPE_IMG
- en: (i) HomoFormer [[78](#bib.bib78)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e4400568717577552733dde8036bd94f.png)'
  prefs: []
  type: TYPE_IMG
- en: (j) GT
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 11: Visual comparison of the results using different shadow removal
    methods, sampled from test set of UIUC [[10](#bib.bib10)].'
  prefs: []
  type: TYPE_NORMAL
- en: Image composition. Image composition involves inserting a foreground object
    into a background image to create a composite image. However, the quality of composite
    images can be greatly affected by inconsistencies between the foreground and background,
    including appearance, geometric, and semantic disparities. Shadow discrepancies
    represent a common issue in image composition, contributing to appearance inconsistencies.
    Therefore, shadow generation becomes essential to produce plausible shadows for
    foreground objects, enhancing the realism of the composite image. Hong *et al.* [[100](#bib.bib100)]
    introduced a manually annotated dataset alongside a shadow synthesis technique
    capable of predicting shadow masks. This method leverages illumination information
    from the background and employs an illumination model to estimate shadow parameters.
    However, manually removing shadows from scenes entails considerable cost. Subsequently,
    Tao *et al.* [[101](#bib.bib101)] proposed utilizing rendering techniques for
    3D scenes to generate large-scale paired shadow/shadow-free images. They developed
    a two-stage network incorporating decomposed mask prediction and attentive shadow
    filling.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset augmentation. Due to the difficulty in obtaining extensive datasets
    for shadow removal, numerous methods in the field have opted to use the generation
    or synthesis process to supplement their training data. Gryka *et al.* [[17](#bib.bib17)]
    utilized graphics tools to produce shadow images, while Sidorov *et al.* [[93](#bib.bib93)]
    constructed a shadow dataset by manipulating lighting within a video game. Nevertheless,
    computer-generated scenes and shadows often exhibit notable disparities from natural
    scenes [[51](#bib.bib51)]. To bridge this disparity, several methodologies [[51](#bib.bib51),
    [56](#bib.bib56), [48](#bib.bib48)] have suggested employing Generative Adversarial
    Networks (GANs) to produce adversarial shadow images, aiming to enhance the realism
    and authenticity of synthesized shadows. This approach leverages the adversarial
    training process to refine the quality and fidelity of synthesized shadows, ultimately
    enhancing their realism and alignment with natural scenes. Another category of
    methods (referenced in [[94](#bib.bib94), [64](#bib.bib64)]) has utilized physical
    illumination models to simulate shadows with varying intensities, operating under
    the practical assumption that all occluding objects are situated outside the camera’s
    field of view. This non-learning approach can be seamlessly integrated into existing
    training paradigms on-the-fly, offering a computationally inexpensive solution.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Shadow-related Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With deep learning-based methods becoming the predominant solution for shadow
    removal, concerns regarding model-related robustness, such as adversarial robustness,
    have increasingly emerged within the community.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, adversarial robustness in the physical domain has gained popularity
    due to its practical applications. However, existing adversarial examples are
    generated in a “sticker-pasting” strategy, which often falls short in producing
    imperceptible perturbation. In contrast, shadow, as a ubiquitous natural phenomenon,
    has the potential to serve as a non-invasive adversarial perturbation. Zhong *et
    al.* [[102](#bib.bib102)] propose a novel optical adversarial attack that generates
    naturalistic shadow patterns on real-world images under a black box setting. As
    a consequence, they successfully demonstrated that shadow could achieve stealthy
    attacks that significantly misled the prediction of deep networks in both digital-
    and physical- settings. Another line of research focuses on evaluating the robustness
    of existing deep shadow removal networks. Due to the significant inconsistency
    of illumination between shadow and non-shadow regions, traditional adversarial
    attacks such as PGD [[103](#bib.bib103)] fail to generate visually imperceptible
    noise, especially in the shadow region. Motivated by this, Wang *et al.* [[104](#bib.bib104)]
    propose a shadow-adaptive attack where the attack budget is adaptively aligned
    with each pixel’s intensity. Thus, the generated adversarial perturbation could
    be more aggressive in non-shadow regions while remaining stealthy in the shadow
    region. With the merit of such shadow-adaptive attacks, they further establish
    the first benchmark evaluating the adversarial robustness of existing deep shadow
    removal networks.
  prefs: []
  type: TYPE_NORMAL
- en: VII Future Research Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As observed from the experimental results presented in Section [V](#S5 "V Benchmarking
    and Empirical Analysis ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive
    Survey"), shadow removal remains a challenging research topic, particularly concerning
    generalization ability. There remains considerable potential for improvement.
    This section suggests potential future research directions as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Generalized Shadow Removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: From supervised to unsupervised/zero-shot learning. While supervised learning-based
    shadow removal methods have demonstrated superior performance within specific
    datasets, their effectiveness diminishes significantly when applied to testing
    cases with distributions distinct from the training data. However, unsupervised/zero-shot
    learning methods offer a promising alternative. By harnessing the ability to capture
    the underlying structure and characteristics of data without necessitating labeled
    examples, these methods facilitate enhanced generalization to unseen, real-world
    scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exploiting knowledge from pre-trained model. Leveraging knowledge from pre-trained
    models, such as large language models [[32](#bib.bib32), [105](#bib.bib105)],
    stable diffusion [[106](#bib.bib106), [107](#bib.bib107)], and SAM (Spatially
    Adaptive Denoising Network) [[108](#bib.bib108)], holds immense potential for
    advancing shadow removal techniques. These models, having been trained on vast
    amounts of diverse data, encode rich representations of the underlying structure
    and semantics of the input data. By fine-tuning or incorporating these pre-trained
    models into shadow removal frameworks, researchers can tap into this wealth of
    knowledge to improve the quality and robustness of shadow removal algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: VII-B Interactive Shadow Removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Interactive shadow removal involves users actively participating in removing
    shadows from images through a user-friendly interface, typically utilizing tools
    or algorithms enabling real-time adjustments and feedback. This method grants
    users enhanced control and customization over the shadow removal process, allowing
    them to tailor results to their specific preferences and requirements.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: User-friendly input. Existing methods have overly stringent requirements for
    the input mask, or they rely on the off-the-shelf shadow detectors to generate
    the mask. However, using a pre-trained shadow detector may lead to a domain gap
    when applied to cases with different distributions from the training data, resulting
    in inaccurate shadow detection that could mislead the subsequent shadow removal
    network. Therefore, by utilizing simple user inputs such as clicks, strokes, or
    painted masks as prompts, the shadow removal network can effectively guide the
    process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Real-time adjustment. The shadow removal process is optimized for real-time
    performance, ensuring that the results are generated quickly enough to be displayed
    without noticeable delay. This may involve leveraging hardware acceleration, parallel
    processing, or optimized algorithms to achieve fast computation speeds. Additionally,
    in interactive shadow removal systems, users may provide feedback or make adjustments
    in real-time to refine the shadow removal results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Multiple shadows process. In real-world scenarios, it’s common to encounter
    multiple shadow regions, stemming from various sources such as multiple occluders
    or diverse light sources. When tackling these scenarios, users often seek the
    flexibility to selectively remove portions of these shadows based on user-defined
    input cues. This could involve removing shadows cast by specific objects or adjusting
    the intensity of shadows in certain areas, empowering users to tailor the shadow
    removal process to their specific needs and preferences.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: VII-C More Comprehensive Benchmark
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Currently, the field of shadow removal faces a significant challenge due to
    the absence of a standardized evaluation benchmark. Existing test datasets for
    shadow removal may not adequately represent the diversity and complexity of real-world
    shadow scenarios. Some test data may be relatively straightforward to handle,
    whose distributions are distinct from real-world cases. This discrepancy further
    complicates the evaluation process and undermines the reliability of comparative
    studies. Hence, there is a clear need for a comprehensive benchmark that encompasses
    large-scale test samples, representing a diverse range of scenes including indoor
    and outdoor environments, as well as challenging lighting conditions such as various
    types of light sources and multiple shadows. Establishing such a benchmark would
    enable researchers to conduct fair and rigorous evaluations of shadow removal
    methods, facilitating advancements in the field.
  prefs: []
  type: TYPE_NORMAL
- en: VII-D Non-Reference Evaluation Metric
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Collecting paired shadow and shadow-free images poses a significant challenge
    in practical scenarios, mainly due to the complexities involved in capturing images
    under diverse lighting conditions. Furthermore, prevailing benchmarks heavily
    rely on reference metrics such as PSNR, RMSE, and SSIM for evaluation, which may
    not adequately account for scenarios where ground truth data or paired images
    are scarce or inaccessible. In instances where large-scale shadow datasets featuring
    unpaired images, or those primarily collected for shadow detection purposes (e.g.,
    SBU [[56](#bib.bib56)] and USR [[95](#bib.bib95)]), are available, leveraging
    them to the fullest extent for benchmarking becomes crucial. However, in such
    cases, the necessity for new non-reference metrics becomes apparent. These metrics
    offer a vital solution to the limitations posed by traditional evaluation methods,
    providing a more holistic and adaptable approach to image assessment. Introducing
    novel non-reference metrics addresses the shortcomings of traditional evaluation
    methods, offering a more comprehensive and adaptable approach to image assessment.
  prefs: []
  type: TYPE_NORMAL
- en: VII-E Extension to Video Shadow Removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current video shadow removal methods typically involve processing each frame
    of a video independently using image shadow removal algorithms, as mentioned by [[63](#bib.bib63),
    [64](#bib.bib64)]. This approach, however, tends to neglect the temporal dimension
    of the video data. By treating each frame in isolation, these methods may fail
    to capture and exploit the temporal coherence present in video sequences. When
    image-based algorithms are directly applied to videos, they often produce artifacts
    and inconsistencies across frames. This is because they do not account for the
    temporal context and variations in lighting conditions that occur over time. As
    a result, the shadow removal process may introduce flickering or other temporal
    artifacts, leading to unsatisfactory results. Therefore, there remains a significant
    gap in the field of video shadow removal, calling for further research and development.
    Efforts are needed to explore novel algorithms that can effectively leverage the
    temporal dimension of video data.
  prefs: []
  type: TYPE_NORMAL
- en: VII-F Large-Scale Training Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While numerous training datasets are available for shadow removal, their scale
    and diversity in representing various shadow categories do not fully capture the
    complexity of real shadow degradations, as indicated in Table [II](#S4.T2 "TABLE
    II ‣ IV-E Loss Function ‣ IV Technical Review and Discussion ‣ Single-Image Shadow
    Removal Using Deep Learning: A Comprehensive Survey"). Current deep shadow removal
    models, as discussed in Section [V](#S5 "V Benchmarking and Empirical Analysis
    ‣ Single-Image Shadow Removal Using Deep Learning: A Comprehensive Survey"), encounter
    challenges in achieving satisfactory performance when confronted with shadow images
    captured in real-world scenarios. Moreover, the resolution of existing datasets
    is typically low, such as $480\times 640$, whereas real-world scenarios often
    involve high-resolution imagery, such as $2k$ or $4k$. Models trained on low-resolution
    data often struggle to generalize directly to high-resolution scenarios. Therefore,
    additional efforts are needed to explore the acquisition of large-scale, high-resolution,
    and diverse real-world paired shadow removal training datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: VIII Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we present a comprehensive survey of deep learning-based single-image
    shadow removal approaches. We first discuss the formation of shadows in the real
    world and summarize the challenges of the shadow removal task. Building upon this,
    we explain how existing methods address these challenges from various perspectives,
    including training strategies, model architecture design, integration of physical
    models, and leveraging knowledge from detection models. We also conduct a comprehensive
    evaluation of existing methods with different learning strategies to explore their
    advantages and disadvantages. Based on the experimental results, we identify several
    open problems that still require further development, such as improving generalization
    to real-world cases, interactive shadow removal, more comprehensive benchmarks,
    video shadow removal, and large-scale training datasets.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] R. Cucchiara, C. Grana, M. Piccardi, and A. Prati, “Detecting moving objects,
    ghosts, and shadows in video streams,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 25, no. 10, pp. 1337–1342, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] S. Nadimi and B. Bhanu, “Physical models for moving shadow and object detection
    in video,” *IEEE transactions on pattern analysis and machine intelligence*, vol. 26,
    no. 8, pp. 1079–1087, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] C. R. Jung, “Efficient background subtraction and shadow removal for monochromatic
    video sequences,” *IEEE Transactions on Multimedia*, vol. 11, no. 3, pp. 571–577,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] A. Sanin, C. Sanderson, and B. C. Lovell, “Improved shadow removal for
    robust person tracking in surveillance scenarios,” in *2010 20th International
    Conference on Pattern Recognition*.   IEEE, 2010, pp. 141–144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] W. Zhang, X. Zhao, J.-M. Morvan, and L. Chen, “Improving shadow suppression
    for illumination robust face recognition,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 41, no. 3, pp. 611–624, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] E. Arbel and H. Hel-Or, “Texture-preserving shadow removal in color images
    containing curved surfaces,” in *2007 IEEE Conference on Computer Vision and Pattern
    Recognition*.   IEEE, 2007, pp. 1–8.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] C. Fredembach and G. Finlayson, “Simple shadow remova,” in *18th International
    Conference on Pattern Recognition (ICPR’06)*, vol. 1.   IEEE, 2006, pp. 832–835.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] N. Salamati, A. Germain, and S. Siisstrunk, “Removing shadows from images
    using color and near-infrared,” in *2011 18th IEEE International Conference on
    Image Processing*.   IEEE, 2011, pp. 1713–1716.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] L. Zhang, Q. Zhang, and C. Xiao, “Shadow remover: Image shadow removal
    based on illumination recovering optimization,” *IEEE Transactions on Image Processing*,
    vol. 24, no. 11, pp. 4623–4636, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] R. Guo, Q. Dai, and D. Hoiem, “Paired regions for shadow detection and
    removal,” *IEEE transactions on pattern analysis and machine intelligence*, vol. 35,
    no. 12, pp. 2956–2967, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] T. F. Y. Vicente and D. Samaras, “Single image shadow removal via neighbor-based
    region relighting,” in *European Conference on Computer Vision*.   Springer, 2014,
    pp. 309–320.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] G. D. Finlayson, S. D. Hordley, and M. S. Drew, “Removing shadows from
    images,” in *Computer Vision—ECCV 2002: 7th European Conference on Computer Vision
    Copenhagen, Denmark, May 28–31, 2002 Proceedings, Part IV 7*.   Springer, 2002,
    pp. 823–836.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] G. D. Finlayson, S. D. Hordley, C. Lu, and M. S. Drew, “On the removal
    of shadows from images,” *IEEE transactions on pattern analysis and machine intelligence*,
    vol. 28, no. 1, pp. 59–68, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] G. D. Finlayson, S. D. Hordley, and M. S. Drew, “Removing shadows from
    images using retinex,” in *Color and imaging conference*, vol. 10.   Society of
    Imaging Science and Technology, 2002, pp. 73–79.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] C. Fredembach and G. Finlayson, “Hamiltonian path-based shadow removal,”
    in *Proceedings of the 16th British Machine Vision Conference (BMVC)*, vol. 2,
    2005, pp. 502–511.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] C. Fredembach and G. D. Finlayson, “Fast re-integration of shadow free
    images,” in *12th Color Imaging Conference: Color Science and Engineering Systems,
    Technologies, and Applications*, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] M. Gryka, M. Terry, and G. J. Brostow, “Learning to remove soft shadows,”
    *ACM Transactions on Graphics*, vol. 34, no. 5, pp. 1–15, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] T.-P. Wu and C.-K. Tang, “A bayesian approach for shadow extraction from
    a single image,” in *Tenth IEEE International Conference on Computer Vision (ICCV’05)
    Volume 1*, vol. 1.   IEEE, 2005, pp. 480–487.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Y. Shor and D. Lischinski, “The shadow meets the mask: Pyramid-based shadow
    removal,” in *Computer Graphics Forum*, vol. 27, no. 2.   Wiley Online Library,
    2008, pp. 577–586.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] C. Xiao, R. She, D. Xiao, and K.-L. Ma, “Fast shadow removal using adaptive
    multi-scale illumination transfer,” in *Computer Graphics Forum*, vol. 32.   Wiley
    Online Library, 2013, pp. 207–218.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] C. Xiao, D. Xiao, L. Zhang, and L. Chen, “Efficient shadow removal using
    subregion matching illumination transfer,” in *Computer Graphics Forum*, vol. 32,
    no. 7.   Wiley Online Library, 2013, pp. 421–430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] S. H. Khan, M. Bennamoun, F. Sohel, and R. Togneri, “Automatic shadow
    detection and removal from a single image,” *IEEE transactions on pattern analysis
    and machine intelligence*, vol. 38, no. 3, pp. 431–446, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] G. D. Finlayson, M. S. Drew, and C. Lu, “Entropy minimization for shadow
    removal,” *International Journal of Computer Vision*, vol. 85, no. 1, pp. 35–57,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] L. Qu, J. Tian, S. He, Y. Tang, and R. W. Lau, “Deshadownet: A multi-context
    embedding deep network for shadow removal,” in *CVPR*, 2017, pp. 4067–4075.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Handwritten digit recognition with a back-propagation network,”
    *Advances in neural information processing systems*, vol. 2, pp. 396–404, 1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” *Advances in neural information processing
    systems*, vol. 25, pp. 1097–1105, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] R. Venkatesan and B. Li, *Convolutional neural networks in visual computing:
    a concise guide*.   CRC Press, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial networks,” *Communications
    of the ACM*, vol. 63, no. 11, pp. 139–144, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Y. Zhu, Z. Xiao, Y. Fang, X. Fu, Z. Xiong, and Z.-J. Zha, “Efficient model-driven
    network for shadow removal,” in *Proceedings of the AAAI conference on artificial
    intelligence*, vol. 36, no. 3, 2022, pp. 3635–3643.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] L. Guo, C. Wang, W. Yang, S. Huang, Y. Wang, H. Pfister, and B. Wen, “Shadowdiffusion:
    When degradation prior meets diffusion model for shadow removal,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2023,
    pp. 14 049–14 058.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] B. Ding, C. Long, L. Zhang, and C. Xiao, “Argan: Attentive recurrent generative
    adversarial network for shadow detection and removal,” in *Proceedings of the
    IEEE/CVF international conference on computer vision*, 2019, pp. 10 213–10 222.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” 2017, pp. 5998–6008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” in *International Conference
    on Learning Representations*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo, “Swin
    transformer: Hierarchical vision transformer using shifted windows,” in *Proceedings
    of the IEEE/CVF international conference on computer vision*, 2021, pp. 10 012–10 022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] J. Liang, J. Cao, G. Sun, K. Zhang, L. Van Gool, and R. Timofte, “Swinir:
    Image restoration using swin transformer,” in *Proceedings of the IEEE/CVF International
    Conference on Computer Vision*, 2021, pp. 1833–1844.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep
    unsupervised learning using nonequilibrium thermodynamics,” in *International
    Conference on Machine Learning*.   PMLR, 2015, pp. 2256–2265.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    *Advances in Neural Information Processing Systems*, vol. 33, pp. 6840–6851, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] B. Kawar, M. Elad, S. Ermon, and J. Song, “Denoising diffusion restoration
    models,” *Advances in Neural Information Processing Systems*, vol. 35, pp. 23 593–23 606,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] C. Saharia, J. Ho, W. Chan, T. Salimans, D. J. Fleet, and M. Norouzi,
    “Image super-resolution via iterative refinement,” *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. Lugmayr, M. Danelljan, A. Romero, F. Yu, R. Timofte, and L. Van Gool,
    “Repaint: Inpainting using denoising diffusion probabilistic models,” in *Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2022,
    pp. 11 461–11 471.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Murali, V. Govindan, and S. Kalady, “A survey on shadow removal techniques
    for single image,” *International Journal of Image, Graphics and Signal Processing*,
    vol. 8, no. 12, p. 38, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Tiwari, P. K. Singh, and S. Amin, “A survey on shadow detection and
    removal in images and video sequences,” in *2016 6th International Conference-Cloud
    System and Big Data Engineering (Confluence)*.   IEEE, 2016, pp. 518–523.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] S. Das and A. Aery, “A review: shadow detection and shadow removal from
    images,” *International Journal of Engineering Trends and Technology (IJETT)*,
    vol. 4, no. 5, pp. 1764–1767, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] E. H. Land, “The retinex theory of color vision,” *Scientific american*,
    vol. 237, no. 6, pp. 108–129, 1977.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Wang, X. Li, and J. Yang, “Stacked conditional generative adversarial
    networks for jointly learning shadow detection and shadow removal,” in *CVPR*,
    2018, pp. 1788–1797.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] X. Hu, C.-W. Fu, L. Zhu, J. Qin, and P.-A. Heng, “Direction-aware spatial
    context features for shadow detection and removal,” *IEEE transactions on pattern
    analysis and machine intelligence*, vol. 42, no. 11, pp. 2795–2808, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] L. Zhang, C. Long, X. Zhang, and C. Xiao, “Ris-gan: Explore residual and
    illumination with generative adversarial networks for shadow removal,” in *Proceedings
    of the AAAI Conference on Artificial Intelligence*, vol. 34, no. 07, 2020, pp.
    12 829–12 836.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] X. Cun, C.-M. Pun, and C. Shi, “Towards ghost-free shadow removal via
    dual hierarchical aggregation network and shadow matting gan.” in *AAAI*, 2020,
    pp. 10 680–10 687.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Y.-H. Lin, W.-C. Chen, and Y.-Y. Chuang, “Bedsr-net: A deep shadow removal
    network from a single document image,” in *Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, 2020, pp. 12 905–12 914.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Z. Chen, C. Long, L. Zhang, and C. Xiao, “Canet: A context-aware network
    for shadow removal,” in *Proceedings of the IEEE/CVF international conference
    on computer vision*, 2021, pp. 4743–4752.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] H. Le and D. Samaras, “Shadow removal via shadow image decomposition,”
    in *Proceedings of the IEEE/CVF International Conference on Computer Vision*,
    2019, pp. 8578–8587.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] L. Fu, C. Zhou, Q. Guo, F. Juefei-Xu, H. Yu, W. Feng, Y. Liu, and S. Wang,
    “Auto-exposure fusion for single-image shadow removal,” in *Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition*, 2021, pp. 10 571–10 580.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] S. He, B. Peng, J. Dong, and Y. Du, “Mask-shadownet: Toward shadow removal
    via masked adaptive instance normalization,” *IEEE Signal Processing Letters*,
    vol. 28, pp. 957–961, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Y. Zhu, J. Huang, X. Fu, F. Zhao, Q. Sun, and Z.-J. Zha, “Bijective mapping
    network for shadow removal,” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, 2022, pp. 5627–5636.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] L. Guo, S. Huang, D. Liu, H. Cheng, and B. Wen, “Shadowformer: Global
    context helps shadow removal,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 37, no. 1, 2023, pp. 710–718.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] X. Hu, Y. Jiang, C.-W. Fu, and P.-A. Heng, “Mask-shadowgan: Learning to
    remove shadows from unpaired data,” in *Proceedings of the IEEE/CVF international
    conference on computer vision*, 2019, pp. 2472–2481.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Z. Liu, H. Yin, Y. Mi, M. Pu, and S. Wang, “Shadow removal by a lightness-guided
    network with training on unpaired data,” *IEEE Transactions on Image Processing*,
    vol. 30, pp. 1853–1865, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Y. Jin, A. Sharma, and R. T. Tan, “Dc-shadownet: Single-image hard and
    soft shadow removal using unsupervised domain-classifier guided network,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2021, pp. 5027–5036.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Y. He, Y. Xing, T. Zhang, and Q. Chen, “Unsupervised portrait shadow removal
    via generative priors,” in *ACM International Conference on Multimedia (ACM MM)*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] A. Shocher, N. Cohen, and M. Irani, ““zero-shot” super-resolution using
    deep internal learning,” in *Proceedings of the IEEE conference on computer vision
    and pattern recognition*, 2018, pp. 3118–3126.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] C. Guo, C. Li, J. Guo, C. C. Loy, J. Hou, S. Kwong, and R. Cong, “Zero-reference
    deep curve estimation for low-light image enhancement,” in *Proceedings of the
    IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020, pp.
    1780–1789.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] H. Le and D. Samaras, “From shadow segmentation to shadow removal,” in
    *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Z. Liu, H. Yin, X. Wu, Z. Wu, Y. Mi, and S. Wang, “From shadow generation
    to shadow removal,” in *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*, 2021, pp. 4927–4936.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] L. Guo, C. Wang, W. Yang, Y. Wang, and B. Wen, “Boundary-aware divide
    and conquer: A diffusion-based solution for unsupervised shadow removal,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2023, pp. 13 045–13 054.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks
    for biomedical image segmentation,” in *International Conference on Medical image
    computing and computer-assisted intervention*.   Springer, 2015, pp. 234–241.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Q. Yu, N. Zheng, J. Huang, and F. Zhao, “Cnsnet: A cleanness-navigated-shadow
    network for shadow removal,” in *Computer Vision–ECCV 2022 Workshops: Tel Aviv,
    Israel, October 23–27, 2022, Proceedings, Part II*.   Springer, 2023, pp. 221–238.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] L. Zhu, Z. Deng, X. Hu, C.-W. Fu, X. Xu, J. Qin, and P.-A. Heng, “Bidirectional
    feature pyramid network with recurrent attention residual modules for shadow detection,”
    in *ECCV*, 2018, pp. 121–136.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Wan, H. Yin, Z. Wu, X. Wu, Y. Liu, and S. Wang, “Style-guided shadow
    removal,” in *Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
    October 23–27, 2022, Proceedings, Part XIX*.   Springer, 2022, pp. 361–378.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] K. Niu, Y. Liu, E. Wu, and G. Xing, “A boundary-aware network for shadow
    removal,” *IEEE Transactions on Multimedia*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] J. Liu, Q. Wang, H. Fan, W. Li, L. Qu, and Y. Tang, “A decoupled multi-task
    network for shadow removal,” *IEEE Transactions on Multimedia*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Sen, S. P. Chermala, N. N. Nagori, V. Peddigari, P. Mathur, B. Prasad,
    and M. Jeong, “Shards: Efficient shadow removal using dual stage network for high-resolution
    images,” in *Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision*, 2023, pp. 1809–1817.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] M. K. Yücel, V. Dimaridou, B. Manganelli, M. Ozay, A. Drosou, and A. Saà-Garriga,
    “Lra&ldra: Rethinking residual predictions for efficient shadow detection and
    removal,” in *Proceedings of the IEEE/CVF Winter Conference on Applications of
    Computer Vision*, 2023, pp. 4925–4935.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] J. Yu, P. He, and Z. Peng, “Fsr-net: Deep fourier network for shadow removal,”
    in *Proceedings of the 31st ACM International Conference on Multimedia*, 2023,
    pp. 2335–2343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] X. Li, Q. Guo, R. Abdelfattah, D. Lin, W. Feng, I. Tsang, and S. Wang,
    “Leveraging inpainting for single-image shadow removal,” in *Proceedings of the
    IEEE/CVF International Conference on Computer Vision*, 2023, pp. 13 055–13 064.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Y. Liu, Z. Ke, K. Xu, F. Liu, Z. Wang, and R. W. Lau, “Recasting regional
    lighting for shadow removal,” in *Proceedings of the AAAI Conference on Artificial
    Intelligence*, vol. 38, no. 4, 2024, pp. 3810–3818.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Y. Jin, W. Ye, W. Yang, Y. Yuan, and R. T. Tan, “Des3: Adaptive attention-driven
    self and soft shadow removal using vit similarity,” in *Proceedings of the AAAI
    Conference on Artificial Intelligence*, vol. 38, no. 3, 2024, pp. 2634–2642.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] K. Mei, L. Figueroa, Z. Lin, Z. Ding, S. Cohen, and V. M. Patel, “Latent
    feature-guided diffusion models for shadow removal,” in *Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision*, 2024, pp. 4313–4322.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] J. Xiao, X. Fu, Y. Zhu, D. Li, J. Huang, K. Zhu, and Z.-J. Zha, “Homoformer:
    Homogenized transformer for image shadow removal,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR)*, June 2024, pp.
    25 617–25 626.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] G. Finlayson, M. M. Darrodi, and M. Mackiewicz, “Rank-based camera spectral
    sensitivity estimation,” *JOSA A*, vol. 33, no. 4, pp. 589–599, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] K. Zhang, L. V. Gool, and R. Timofte, “Deep unfolding network for image
    super-resolution,” in *Proceedings of the IEEE/CVF conference on computer vision
    and pattern recognition*, 2020, pp. 3217–3226.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] C. Wang, L. Guo, Y. Wang, H. Cheng, Y. Yu, and B. Wen, “Progressive divide-and-conquer
    via subsampling decomposition for accelerated mri,” in *Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, 2024, pp. 25 128–25 137.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] H. Zheng, H. Yong, and L. Zhang, “Unfolded deep kernel estimation for
    blind image super-resolution,” in *European Conference on Computer Vision*.   Springer,
    2022, pp. 502–518.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] C. Wang, R. Zhang, G. Maliakal, S. Ravishankar, and B. Wen, “Deep reinforcement
    learning based unrolling network for mri reconstruction,” in *2023 IEEE 20th International
    Symposium on Biomedical Imaging (ISBI)*.   IEEE, 2023, pp. 1–5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] S. Kong, W. Wang, X. Feng, and X. Jia, “Deep red unfolding network for
    image restoration,” *IEEE Transactions on Image Processing*, vol. 31, pp. 852–867,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] C. Mou, Q. Wang, and J. Zhang, “Deep generalized unfolding networks for
    image restoration,” in *Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition*, 2022, pp. 17 399–17 410.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *Advances in neural
    information processing systems*, 2014, pp. 2672–2680.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] H. Zhao, O. Gallo, I. Frosio, and J. Kautz, “Loss functions for image
    restoration with neural networks,” *IEEE Transactions on computational imaging*,
    vol. 3, no. 1, pp. 47–57, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] C. Li, C. Guo, L. Han, J. Jiang, M.-M. Cheng, J. Gu, and C. C. Loy, “Low-light
    image and video enhancement using deep learning: A survey,” *IEEE transactions
    on pattern analysis and machine intelligence*, vol. 44, no. 12, pp. 9396–9416,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] L. Xu, Q. Yan, Y. Xia, and J. Jia, “Structure extraction from texture
    via relative total variation,” *ACM transactions on graphics (TOG)*, vol. 31,
    no. 6, pp. 1–10, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. Sharma and L.-F. Cheong, “Into the twilight zone: Depth estimation
    using joint structure-stereo optimization,” in *Proceedings of the European Conference
    on Computer Vision (ECCV)*, 2018, pp. 103–118.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *3rd International Conference on Learning Representations,
    ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings*, 2015\.
    [Online]. Available: [http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *2009 IEEE Conference on Computer
    Vision and Pattern Recognition*.   Ieee, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] O. Sidorov, “Conditional gans for multi-illuminant color constancy: Revolution
    or yet another approach?” in *Proceedings of the IEEE/CVF Conference on Computer
    Vision and Pattern Recognition Workshops*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] N. Inoue and T. Yamasaki, “Learning from synthetic shadows for shadow
    detection and removal,” *IEEE Transactions on Circuits and Systems for Video Technology*,
    vol. 31, no. 11, pp. 4187–4197, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] T. F. Y. Vicente, L. Hou, C.-P. Yu, M. Hoai, and D. Samaras, “Large-scale
    training of shadow detectors with noisily-annotated shadow examples,” in *Computer
    Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October
    11-14, 2016, Proceedings, Part VI 14*.   Springer, 2016, pp. 816–832.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] R. Wang, Q. Zhang, C.-W. Fu, X. Shen, W.-S. Zheng, and J. Jia, “Underexposed
    photo enhancement using deep illumination estimation,” in *Proceedings of the
    IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2019, pp. 6849–6857.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli, “Image quality
    assessment: from error visibility to structural similarity,” *IEEE transactions
    on image processing*, vol. 13, no. 4, pp. 600–612, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in *Proceedings of the
    IEEE conference on computer vision and pattern recognition*, 2018, pp. 586–595.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Mittal, R. Soundararajan, and A. C. Bovik, “Making a “completely blind”
    image quality analyzer,” *IEEE Signal processing letters*, vol. 20, no. 3, pp.
    209–212, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Y. Hong, L. Niu, and J. Zhang, “Shadow generation for composite image
    in real-world scenes,” in *Proceedings of the AAAI conference on artificial intelligence*,
    vol. 36, no. 1, 2022, pp. 914–922.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Tao, J. Cao, Y. Hong, and L. Niu, “Shadow generation with decomposed
    mask prediction and attentive shadow filling,” in *Proceedings of the AAAI Conference
    on Artificial Intelligence*, vol. 38, no. 6, 2024, pp. 5198–5206.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Y. Zhong, X. Liu, D. Zhai, J. Jiang, and X. Ji, “Shadows can be dangerous:
    Stealthy and effective physical-world adversarial attack by natural phenomenon,”
    in *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    2022, pp. 15 345–15 354.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards
    deep learning models resistant to adversarial attacks,” in *International Conference
    on Learning Representations*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] C. Wang, Y. Yu, L. Guo, and B. Wen, “Benchmarking adversarial robustness
    of image shadow removal with shadow-adaptive attacks,” in *ICASSP 2024-2024 IEEE
    International Conference on Acoustics, Speech and Signal Processing (ICASSP)*.   IEEE,
    2024, pp. 13 126–13 130.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] E. Kasneci, K. Seßler, S. Küchemann, M. Bannert, D. Dementieva, F. Fischer,
    U. Gasser, G. Groh, S. Günnemann, E. Hüllermeier *et al.*, “Chatgpt for good?
    on opportunities and challenges of large language models for education,” *Learning
    and individual differences*, vol. 103, p. 102274, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] S. Diffusion, “Stable diffusion 2-1 base,” [https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt](https://huggingface.co/stabilityai/stable-diffusion-2-1-base/blob/main/v2-1_512-ema-pruned.ckpt),
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] D. Podell, Z. English, K. Lacey, A. Blattmann, T. Dockhorn, J. Müller,
    J. Penna, and R. Rombach, “SDXL: Improving latent diffusion models for high-resolution
    image synthesis,” in *The Twelfth International Conference on Learning Representations*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] A. Kirillov, E. Mintun, N. Ravi, H. Mao, C. Rolland, L. Gustafson, T. Xiao,
    S. Whitehead, A. C. Berg, W.-Y. Lo *et al.*, “Segment anything,” in *Proceedings
    of the IEEE/CVF International Conference on Computer Vision*, 2023, pp. 4015–4026.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
