- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:51:05'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2110.02511] A Survey on Recent Deep Learning-driven Singing Voice Synthesis
    Systems'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.02511](https://ar5iv.labs.arxiv.org/html/2110.02511)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yin-Ping Cho, Fu-Rong Yang, Yung-Chuan Chang, Ching-Ting Cheng, Xiao-Han Wang,
    Yi-Wen Liu Department of Electrical Engineering
  prefs: []
  type: TYPE_NORMAL
- en: National Tsing Hua University Hsinchu, Taiwan
  prefs: []
  type: TYPE_NORMAL
- en: yinping.cho@outlook.com; ywliu@ee.nthu.edu.tw
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Singing voice synthesis (SVS) is a task that aims to generate audio signals
    according to musical scores and lyrics. With its multifaceted nature concerning
    music and language, producing singing voices indistinguishable from that of human
    singers has always remained an unfulfilled pursuit. Nonetheless, the advancements
    of deep learning techniques have brought about a substantial leap in the quality
    and naturalness of synthesized singing voice. This paper aims to review some of
    the state-of-the-art deep learning-driven SVS systems. We intend to summarize
    their deployed model architectures and identify the strengths and limitations
    for each of the introduced systems. Thereby, we picture the recent advancement
    trajectory of this field and conclude the challenges left to be resolved both
    in commercial applications and academic research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: sining voice synthesis, deep learning, review paper
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A singing voice synthesis (SVS) system is able to generate singing voice from
    a given musical score. For the future of music composition, one can imagine that
    a song can be listened to immediately after the song has been composed without
    recording. Recently, several approaches have been proposed to build a natural
    singing voice synthesis system in Japanese, English, Korean, Spanish, and so on
    [[14](#bib.bib14), [18](#bib.bib18), [1](#bib.bib1)]. Different from speech synthesis,
    the synthesized singing voice needs to follow the musical scores; performance
    of pitch and rhythm synthesis would directly influence the perceived quality.
  prefs: []
  type: TYPE_NORMAL
- en: Before neural networks were widely used, unit concatenation [[28](#bib.bib28)]
    and hidden Markov Model (HMM) [[16](#bib.bib16)] approaches were adopted for SVS.
    The unit concatenation synthesizer generates the singing voice by selecting the
    voice elements in the database and concatenation is performed consecutively. Commercially
    available tools such as Vocaloid [[15](#bib.bib15)] and Synthesizer V¹¹1https://synthesizerv.com/en/
    have successfully gathered loyal groups of users. In contrast, HMM-based SVS [[16](#bib.bib16)]
    can model the spectral envelopes, excitation, and the singing voice duration separately.
    Then, speech parameter generation algorithms [[29](#bib.bib29)] are used to produce
    singing voice parameter trajectories. As HMM predates the advances in deep learning,
    the naturalness of HMM-based SVS is outperformed by what could now be achieved
    by neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: Over the past few years, several types of neural networks have been employed
    for SVS, such as generic deep neural networks (DNN) [[17](#bib.bib17)], convolutional
    neural networks [[20](#bib.bib20)], a recurrent neural network with long-short
    term memory (LSTM) [[18](#bib.bib18)], and generative adversarial networks (GAN)
    [[19](#bib.bib19)]. Besides, taking advantage of the similarity to text-to-speech
    (TTS), some autoregressive sequence-to-sequence(Seq2Seq) models have been proposed
    [[21](#bib.bib21), [22](#bib.bib22)]. In recent time, state-of-the-art deep learning
    architectures are adopted to tackle with the SVS task, such as the Transformer-based
    [[23](#bib.bib23)] XiaoicSing [[2](#bib.bib2)], HifiSinger [[6](#bib.bib6)] and
    diffusion denoising probabilistic model [[9](#bib.bib9)] like DiffSinger [[8](#bib.bib8)].
    However, these models typically need a large corpus for training; meanwhile, systems
    designed for lower data consumption, such as LiteSing [[10](#bib.bib10)] and Sinsy
    [[14](#bib.bib14)], are now a heated research direction.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate the one-to-many difficulty of directly predicting the raw waveform
    from a musical score, recent deep learning-driven SVS systems mostly employ an
    acoustic model-vocoder architecture as in Fig [1](#S1.F1 "Figure 1 ‣ I Introduction
    ‣ A Survey on Recent Deep Learning-driven Singing Voice Synthesis Systems"). In
    this setup, the vocoder maps frame-level parameters to the waveform, and the acoustic
    model only has to predict a parameter sequence with a length of the framed target
    waveform. This way, the mapping from the score to waveform is broken down into
    two simpler sub-tasks of lower dimension discrepancy between the mapping. Depending
    on the chosen vocoder, the frame-level synthesis parameters can be Mel-spectrograms
    [[6](#bib.bib6), [8](#bib.bib8)] or parameters designed with more insights into
    the human voice, such as those with explicit and separate F0 values [[2](#bib.bib2),
    [14](#bib.bib14), [10](#bib.bib10)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of this paper is organized as follows: Section II overviews three
    landmark deep learning SVS systems with high fidelity and naturalness. Two SVS
    systems with particular designs for low data resource training are described in
    section III. Challenges and future directions of this topic are reported in Section
    IV, and the conclusions are summarized in V.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2b06940171482c7637bff4f36a89123f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: General architecture of recent deep learning-driven SVS systems.'
  prefs: []
  type: TYPE_NORMAL
- en: II Naturalness and Audio Quality Milestones
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'II-A XiaoiceSing: Transformer + WORLD'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: XiaoiceSing [[2](#bib.bib2)] was among the first of the deep learning-driven
    SVS systems that saw commercial deployment. It aimed to generate a singing voice
    with accurate pitch and rhythm that sounded natural and human-like. This system
    adopts the architecture of the FastSpeech [[3](#bib.bib3)] augmented with singing-specific
    modifications for the acoustic model and uses WORLD as the vocoder [[4](#bib.bib4)].
    The singing-specific modifications mainly concern the addition of note duration
    and note pitch information. To ensure the correctness of rhythm, the authors proposed
    adding a syllable-level duration loss instead of relying solely on the phoneme-level
    duration as the original FastSpeech. As for a more robust pitch, the note pitch
    residually connects to the F0 output; therefore, the acoustic model only has to
    predict the relative F0 variations in the human singing voice and add it onto
    the given note pitch contour. The experiments showed that XiaoiceSing outperformed
    the baseline system [[5](#bib.bib5)] in both subjective and objective evaluations.
    In particular, the A/B tests showed a dominant preference favoring XiaoiceSing
    in terms of F0 and rhythm naturalness, proving the effectiveness of the proposed
    modifications. However, the authors remarked that the mean opinions scores (MOS)
    of XiaoiceSing were held back by the waveform quality bound of the WORLD vocoder,
    although the acoustic model performed well in objective metrics. Also, this system
    consumed a large total of 74 hours of single professional singer’s training data,
    which would be inaccessible in a purely academic context.
  prefs: []
  type: TYPE_NORMAL
- en: 'II-B HiFiSinger: Transformer + Neural Vocoder'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Building on the foundation of XiaoiceSing, HiFiSinger [[6](#bib.bib6)] aims
    to defy its waveform quality limitations. While HiFiSinger adopted the same FastSpeech-based
    acoustic model of XiaoiceSing, it swapped out WORLD and employed a Parallel WaveGAN
    (PW-GAN) [[7](#bib.bib7)] neural vocoder to generate waveforms at a high-fidelity
    48kHz sample rate. Therefore, the acoustic model is modified to predict a Mel-spectrogram
    as the main synthesis parameter and F0 contour with voiced/unvoiced (U/UV) flags
    as auxiliaries for the neural vocoder. To prompt better Mel-spectrogram fidelity,
    the authors proposed a sub-frequency GAN (SF-GAN) scheme where three discriminator
    networks operate on three overlapping sub-bands. This way, the GAN network should
    avoid the time-frequency resolution tradeoff. On the vocoder side, the main addition
    is the multi-length GAN (ML-GAN) that deploys multiple discriminator networks
    working on different waveform input lengths. This should make the discriminator
    handle both the long-time structures and short-time details better than the original
    single network. Also, predicted F0 and U/UV flags are integrated as auxiliary
    features for the modified PW-GAN alongside the Mel-spectrogram. The comparative
    mean opinion score ablation test confirmed that these modifications all positively
    contributed to the final system. In terms of perceived naturalness and audio quality,
    this system proved to be a significant improvement over XiaoiceSing in the MOS
    test. In the 48kHz generation configuration, HiFiGAN’s MOS (3.76) approached that
    of the ground truth high-fidelity recordings (4.03).
  prefs: []
  type: TYPE_NORMAL
- en: 'II-C DiffSinger: Denoising Diffusion Probabilistic Model + Neural Vocoder'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To further enhance the acoustic model’s prediction accuracy and robustness
    of Mel-spectrograms, DiffSinger [[8](#bib.bib8)] utilizes a generative model paradigm
    novel at the time of this writing: the denoising diffusion probabilistic model
    [[9](#bib.bib9)]. Instead of directly optimizing the acoustic model to generate
    Mel-spectrograms, DiffSinger formulates the generation task into a parameterized
    Markov chain conditioned on the musical score. The diffusion process of this Markov
    chain gradually scales the Mel-spectrogram and applies noise until it becomes
    Gaussian noise. Conversely, the denoising process iteratively subtracts a portion
    of noise from the noisy input and rescales it until it becomes a Mel-spectrogram.
    To improve the speed and robustness of the denoising process, the authors proposed
    a shallow diffusion mechanism for denoising inference. This mechanism utilizes
    a simple auxiliary decoder seen in Transformer-based acoustic models [[2](#bib.bib2),
    [6](#bib.bib6)] to generate a rough Mel-spectrogram from the musical score. The
    denoising process can use this rough approximation as a starting point much closer
    to the Mel-spectrogram end of the Markov chain. Therefore, the inference process
    starts denoising from a noisy input with resemblance to the target Mel-spectrogram
    and requires much fewer steps to complete. The resulting acoustic model showed
    substantial quality improvements in MOS over its state-of-the-art counterparts
    in both text-to-speech (TTS) and SVS tasks. Also, with the shallow diffusion mechanism,
    it achieved a real-time factor (RTF) of 0.191 on a single RTX V100, meaning that
    it has real-time applicability.'
  prefs: []
  type: TYPE_NORMAL
- en: III Towards Data Efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'III-A Sinsy: DNN + Neural Vocoder'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Sinsy [[14](#bib.bib14)] is designed to synthesize singing voices at appropriate
    timing from a musical score. It contains four modules: 1) a “time-lag model” which
    controls vocal start timing of each note for adapting human singing habits; 2)
    a “duration model” which estimates phoneme durations for pre-expanding features
    into the frame level; 3) a DNN-based “acoustic model” which plays the role of
    a mapping function from the score feature sequences to the acoustic feature sequences;
    4) a PeriodNet “neural vocoder” which generates time-domain waveform samples conditioned
    on acoustic features. Besides, Sinsy applied singing-specific techniques including
    accurately modeling pitch by predicting the residual connection between the note
    pitch and the output F0 in log scale, and the vibrato modeling which expresses
    fluctuations with the difference between the original F0 sequence and the smoothed
    one. Moreover, Sinsy proposed two automatic pitch correction strategies, the prior
    distribution of pitch and the pseudo-note pitch, to prevent singing voices from
    becoming out of tune. Sinsy adopted 1-hour Japanese children’s songs performed
    by a female singer for training. The mean opinion scores in subjective evaluation
    tests concluded that the proposed system could synthesize a singing voice with
    better start timing of vocal, more natural vibrato, and more accurate singing
    pitch.'
  prefs: []
  type: TYPE_NORMAL
- en: 'III-B LiteSing: WaveNet + WORLD'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'LiteSing [[10](#bib.bib10)] was designed to be a fast, high-quality SVS system
    with an efficient architecture that requires little training data. Instead of
    pursuing marginal audio quality gain with neural vocoders, LiteSing goes back
    to using WORLD as the vocoder. This is a conscious choice meant to utilize WORLD’s
    characteristic of separating instantaneous spectral envelopes from F0, making
    the prediction task simpler for the acoustic model. Furthermore, LiteSing employed
    a condition predictor that separately predicts dynamic acoustic energy, V/UV flags,
    and dynamic pitch curve, which means the decoder only has to predict the spectral
    envelope and aperiodic components with a much lower variance. That is, LiteSing
    disentangles the compound and complicated prediction task into well-defined and
    simpler sub-tasks for the acoustic model. Therefore, the authors could employ
    a relatively small and fast non-autoregressive WaveNet [[11](#bib.bib11)] as the
    model’s backbone and still expect robust and high-quality synthesis. In addition,
    the authors added a Wasserstein GAN (WGAN) [[12](#bib.bib12)] for the predicted
    acoustic features to combat the over-smoothing problem typical in WORLD parameter
    prediction tasks. The experiments limited the dataset to 48 minutes of audio to
    test LiteSing’s data efficiency. In the MOS evaluation, LiteSing achieved a score
    (3.60) comparable to that of the WORLD-resynthesized human singer’s audio (3.86).
    More importantly, although the comparing FastSpeech2 baseline yielded a marginally
    higher score (3.63), LiteSing used only one-fifteenth of the number of parameters
    (3.8M vs. 57.0M), proving its superior data efficiency; also, LiteSing was significantly
    preferred over FastSpeech2 in the A/B test for expressiveness. These results demonstrated
    the quality and efficiency of the proposed acoustic model. However, the tradeoff
    of using WORLD was obvious: the original audio received a MOS of 4.20, substantially
    higher than the WORLD-resynthesized version (3.86), and the audio fully synthesized
    by LiteSing (3.60).'
  prefs: []
  type: TYPE_NORMAL
- en: IV Challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: IV-A Data Efficiency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Table [I](#S4.T1 "TABLE I ‣ IV-A Data Efficiency ‣ IV Challenges ‣ A Survey
    on Recent Deep Learning-driven Singing Voice Synthesis Systems"), we can observe
    a tradeoff between the amount of data and the synthesis quality. Systems that
    achieved nearly human-level quality [[2](#bib.bib2), [6](#bib.bib6), [8](#bib.bib8)]
    were those designed to be more data-driven and consumed multiple hours of a singing
    voice to train. On the other hand, although some systems may function with around
    or less than one hour of data [[14](#bib.bib14), [10](#bib.bib10)], their measures
    of simplification usually come with degradations that sum to a compromised overall
    synthesis quality. Since composing a singing voice dataset requires high-fidelity
    recording equipment, a good singer, and substantial post-editing and score annotating
    efforts, data for SVS systems are costly to obtain. Therefore, the data efficiency
    of an SVS system may dictate its real-world applicability. To enhance data efficiency,
    the strategy usually combines two techniques: simplifying the neural network modules
    [[14](#bib.bib14), [10](#bib.bib10)] and decomposing the SVS task into lower-dimension
    sub-tasks by expert knowledge in singing voices [[14](#bib.bib14)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Summary of data usage'
  prefs: []
  type: TYPE_NORMAL
- en: '| System | Amount of Singing voice data consumed |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| XiaoiceSing | 74 hours |'
  prefs: []
  type: TYPE_TB
- en: '| HiFiSinger | 11 hours |'
  prefs: []
  type: TYPE_TB
- en: '| DiffSinger | 6 hours |'
  prefs: []
  type: TYPE_TB
- en: '| Sinsy | 1 hour |'
  prefs: []
  type: TYPE_TB
- en: '| LiteSing | 48 minutes |'
  prefs: []
  type: TYPE_TB
- en: IV-B The lack of unified open datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Producing an SVS dataset requires high cost of recording and annotating. Furthermore,
    due to copyright concerns, there are very few such datasets and those that are
    publicly available are usually confined to singing old folk songs without copyright
    [[24](#bib.bib24), [25](#bib.bib25), [26](#bib.bib26)]. Consequently, it is difficult
    for new researchers without resources to enter the field, and it is tricky for
    the community to objectively compare and evaluate the quality of different systems,
    as each of them is usually trained on a unique, proprietary dataset. Also, SVS
    systems that facilitate these open datasets may suffer from domain discrepancies,
    where the training dataset contains old folk songs while the songs of synthesis
    interest are of modern styles. As a comparison, text-to-speech (TTS) is a generation
    task with similar characteristics and complexity; however, an ample amount of
    free, open datasets is available for TTS [[30](#bib.bib30), [31](#bib.bib31)],
    which allows the field to progress rapidly and see a wide array of successful
    architectures and readily available commercial products. Although this issue is
    not a technical one, it is a significant limiting factor for the present and the
    prospect of SVS research.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C The lack of interpretability and transparency
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning has pushed the envelope of SVS systems in terms of synthesized
    audio quality. Nevertheless, deep learning systems’ complexity makes it almost
    impossible to analytically understand the learned mapping from the input musical
    score to the end output waveform. This characteristic prevents researchers from
    extracting knowledge about the mechanisms that comprise the process of singing.
    Consequently, it substantially weakens the motivation of doing SVS research as
    a reverse-engineering means looking to gain insights about human vocalization
    and perception of music. Although this issue does not prevent application-driven
    research, it terminates these works’ potential to extend into fields such as medicine
    or psychoacoustics. Considering the high cost of constructing SVS systems, improving
    the interpretability and transparency of deep learning-driven SVS systems is worthy
    of research effort.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Absence of emotion and singing technique control and variations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The high variation and diversity of emotion and vocalization techniques are
    quintessential to singing voices. However, although state-of-the-art deep learning-driven
    SVS systems can synthesize singing voices with reasonable naturalness and sound
    quality, they offer no means to condition these systems to synthesize with specific
    emotions or singing techniques. This is an advanced issue we surmise will become
    a research hot spot for two reasons: 1) In a commercial context, being able to
    sing according to the user’s desired style for the songs is basic functionality
    of professional singers. Without similar controllability or customizability, an
    SVS system lacks completeness to enter the real-world market. 2) This is an issue
    entangled with data efficiency. It is proven in a similar high-variance speech
    synthesis research [[27](#bib.bib27)] that conditioning the system on implicit
    features such as emotion and prosodic styles are instrumental to enhancing the
    system’s robustness and naturalness given the same training data. In cases that
    these implicit features are statistically dominant to the synthesis target, the
    absence of these conditions may lead to failure of convergence. Therefore, we
    are confident that this issue is one that should be put in the spotlight of future
    SVS research.'
  prefs: []
  type: TYPE_NORMAL
- en: V Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we reviewed some of the deep learning-driven SVS systems that
    are representative of this research topic. We have shown that these methods have
    demonstrated synthesis quality and naturalness comparable to real human singers.
    Despite these achievements, challenges yet to be resolved for this topic were
    identified — namely, the need for better data efficiency of the systems, the lack
    of open and unified benchmark datasets like those available in TTS research, deep
    neural networks’ inherent absence of interpretability, and the lack of control
    and explicit variations of emotion and singing techniques quintessential for singing
    voice synthesis. These issues hurdle the commercial deployment of SVS systems
    and are of high academic interest. Therefore, we can expect them to be the center
    of advancements in the coming times.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. Blaauw and J. Bonada, “A neural parametric singing synthesizer modeling
    timbre and expression from natural songs,” in *Applied Sciences*, 2017, p. 1313.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] P. Lu, J. Wu, J. Luan, X. Tan, and L. Zhou, “XiaoiceSing: A high-quality
    and integrated singing voice synthesis system,” *arXiv* preprint arXiv:2006.06261,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Y. Ren, Y. Ruan, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, “FastSpeech:
    Fast, robust and controllable text to speech,” in *Conference on Neural Information
    Processing Systems (NeurIPS)*, 2019, pp. 3165–3174.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. Morise, F. Yokomori, and K. Ozawa, “WORLD: A vocoder-based high-quality
    speech synthesis system for real-time applications,” in *IEICE transactions on
    information and systems*, 2016, pp. 1877-1884.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] K. Nakamura, S. Takaki, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda,
    “Fast and high-quality singing voice synthesis system based on convolutional neural
    networks,” *arXiv* preprint arXiv:1910.11690, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] J. Chen, X. Tan, J. Luan, T. Qin, and T. Liu, “HiFiSinger: Towards high-fidelity
    neural singing voice synthesis,” *arXiv* preprint arXiv:2009.01776, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] R. Yamamoto, E. Song and J. Kim, “Parallel WaveGAN: A fast waveform generation
    model based on generative adversarial networks with multi-resolution spectrogram,”
    in *International Conference on Acoustics, Speech and Signal Processing (ICASSP)*,
    2020, pp. 6199-6203.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. Liu, C. Li, Y. Ren, F. Chen, P. Liu, and Z. Zhao, “Diffsinger: Diffusion
    acoustic model for singing voice synthesis,” *arXiv* preprint arXiv:2105.02446,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Ho, Jonathan, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic
    models,” in *Conference on Neural Information Processing Systems (NeurIPS)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] X. Zhuang, T. Jiang, S. -Y. Chou, B. Wu, P. Hu, and S. Lui, “Litesing:
    Towards fast, lightweight and expressive singing voice synthesis,” in *ICASSP*,
    2021, pp. 7078-7082.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves,
    N. Kalchbrenner, A. Senior, and K. Kavukcuoglu, “WaveNet: A generative model for
    raw audio,” *arXiv* preprint arXiv:1609.03499, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] A. Martin, S. Chintala, and L. Bottou, “Wasserstein generative adversarial
    networks,” *International conference on machine learning (PMLR)*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Ren, C. Hu, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T. Liu, “Fastspeech
    2: Fast and high-quality end-to-end text to speech,” *arXiv* preprint arXiv:2006.04558,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Y. Hono, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda, “Sinsy: A deep
    neural network-based singing voice synthesis system,” in *IEEE/ACM Transactions
    on Audio, Speech, and Language Processing*, 2021, pp. 2803-2815.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] H. Kenmochi and H. Ohshita, “Vocaloid-commercial singing synthesizer based
    on sample concatenation,” in *Eighth Annual Conference of the International Speech
    Communication Association*, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] K. Saino, H. Zen, Y. Nankaku, A. Lee, and K. Tokuda, “An HMM-based singing
    voice synthesis system,” in *International Conference on Spoken Language Processing*,
    2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] M. Nishimura, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda, “Singing
    voice synthesis based on deep neural networks,” in *Conference of the International
    Speech Communication Association (INTERSPEECH)*, 2016, pp. 2478–2482.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] J. Kim, H. Choi, J. Park, M. Hahn, S. J. Kim, and J. J. Kim, “Korean singing
    voice synthesis based on an LSTM recurrent neural network,” in *INTERSPEECH*,
    2018, pp. 1551–1555.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Y. Hono, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda, “Singing voice
    synthesis based on generative adversarial networks,” in *IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP)*, 2019, pp. 6955–6959.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] K. Nakamura, K. Hashimoto, K. Oura, Y. Nankaku, and K. Tokuda, “Singing
    voice synthesis based on convolutional neural networks,” *arXiv* preprint arXiv:1904.06868,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Y. Gu, X. Yin, Y. Rao, Y. Wan, B. Tang, Y. Zhang, J. Chen, Y. Wang, and
    Z. Ma, “ByteSing: A chinese singing voice synthesis system using duration allocated
    encoder-decoder acoustic models and WaveRNN vocoders,” in *International Symposium
    on Chinese Spoken Language Processing (ISCSLP)*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] J. Lee, H. Choi, C. Jeon, J. Koo, and K. Lee, “Adversarially trained end-to-end
    Korean singing voice synthesis system,” in *INTERSPEECH*, 2019, pp. 2588–2592.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *Advances in neural
    information processing systems*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Choi, W. Kim, S. Park, S. Yong, and J. Nam, “Children’s Song Dataset
    for Singing Voice Research,” in *International Society for Music Information Retrieval
    Conference (ISMIR)*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Nagoya Institute of Technology, “NIT-SONG070-F001,” http://hts.sp.nitech.ac.jp/archives/2.3/HTSdemo\_NIT-SONG070-F001.tar.bz2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] J. Wilkins, P. Seetharaman, A. Wahl, and B. Pardo, “VocalSet: A singing
    voice dataset,” in *International Society for Music Information Retrieval Conference
    (ISMIR)*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Y. Wang, D. Stanton, Y. Zhang, R. S. Ryan, E. Battenberg, J. Shor, Y.
    Xiao, F. Ren, Y. Jia, and R. A. Saurous, “Style Tokens: Unsupervised Style Modeling,
    Control and Transfer in End-to-End Speech Synthesis,” *arXiv* preprint arXiv:1803.09017,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Bonada, X. Serra, “Synthesis of the singing voice by performance sampling
    and spectral models,” *IEEE Signal Processing Magazine*, vol. 24, pp. 69-79, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] K. Tokuda, T. Yoshimura, T. Masuko, T. Kobayashi and T. Kitamura, “Speech
    parameter generation algorithms for HMM-based speech synthesis,” in *IEEE International
    Conference on Acoustics, Speech, and Signal Processing. Proceedings*, 2000, pp.1315-1318.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Bakhturina, Evelina and Lavrukhin, Vitaly and Ginsburg, Boris and Zhang,
    Yang, ”Hi-Fi Multi-Speaker English TTS Dataset,” in *arXiv preprint arXiv:2104.01497*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Keith Ito and Linda Johnson, ”The LJ Speech Dataset,” https://keithito.com/LJ-Speech-Dataset/,
    2017'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
