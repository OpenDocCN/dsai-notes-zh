- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:39:38'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2305.10862] How Deep Learning Sees the World: A Survey on Adversarial Attacks
    & Defenses'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.10862](https://ar5iv.labs.arxiv.org/html/2305.10862)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Joana C. Costa, Tiago Roxo, Hugo Proença,  Pedro R. M. Inácio The authors are
    with Instituto de Telecomunicações, Universidade da Beira Interior, Portugal.Manuscript
    received XX, 2023; revised XX, 2023.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep Learning is currently used to perform multiple tasks, such as object recognition,
    face recognition, and natural language processing. However, Deep Neural Networks
    (DNNs) are vulnerable to perturbations that alter the network prediction (adversarial
    examples), raising concerns regarding its usage in critical areas, such as self-driving
    vehicles, malware detection, and healthcare. This paper compiles the most recent
    adversarial attacks, grouped by the attacker capacity, and modern defenses clustered
    by protection strategies. We also present the new advances regarding Vision Transformers,
    summarize the datasets and metrics used in the context of adversarial settings,
    and compare the state-of-the-art results under different attacks, finishing with
    the identification of open issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Adversarial attacks, adversarial defenses, datasets, evaluation metrics, survey,
    vision transformers.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine Learning (ML) algorithms have been able to solve various types of problems,
    namely highly complex ones, through the usage of Deep Neural Networks (DNNs) [[1](#bib.bib1)],
    achieving results similar to, or better than, humans in multiple tasks, such as
    object recognition [[2](#bib.bib2), [3](#bib.bib3)], face recognition [[4](#bib.bib4),
    [5](#bib.bib5)], and natural language processing [[6](#bib.bib6), [7](#bib.bib7)].
    These networks have also been employed in critical areas, such as self-driving
    vehicles [[8](#bib.bib8), [9](#bib.bib9)], malware detection [[10](#bib.bib10),
    [11](#bib.bib11)], and healthcare [[12](#bib.bib12), [13](#bib.bib13)], whose
    application and impaired functioning can severely impact their users.
  prefs: []
  type: TYPE_NORMAL
- en: 'Promising results shown by DNNs lead to the sense that these networks could
    correctly generalize in the local neighborhood of an input (image). These results
    motivate the adoption and integration of these networks in real-time image analysis,
    such as traffic sign recognition and vehicle segmentation, making malicious entities
    target these techniques. However, it was discovered that DNNs are susceptible
    to small perturbations in their input [[14](#bib.bib14)], which entirely alter
    their prediction, making it harder for them to be applied in critical areas. These
    perturbations have two main characteristics: 1) invisible to the Human eye or
    slight noise that does not alter Human prediction; and 2) significantly increase
    the confidence of erroneous output, the DNNs predict the wrong class with higher
    confidence than all other classes. As a result of these assertions, the effect
    of the perturbations has been analyzed with more focus on object recognition,
    which will also be the main target of this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Papernot et al. [[15](#bib.bib15)] distinguishes four types of adversaries
    depending on the information they have access to: (i) training data and network
    architecture, (ii) only training data or only network, (iii) oracle, and (iv)
    only pairs of input and output. In almost all real scenarios, the attacker does
    not have access to the training data or the network architecture, which diminishes
    the strength of the attack performed on a network, leaving the adversary with
    access to the responses given by the network, either by asking questions directly
    to it or by having pairs of input and prediction. Furthermore, the queries to
    a model are usually limited or very expensive [[16](#bib.bib16)], making it harder
    for an attacker to produce adversarial examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multiple mechanisms [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)] were proposed to defend against legacy attacks, already displaying
    their weakened effect when adequately protected, which are clustered based on
    six different domains in this survey. Regardless of the attacks and defenses already
    proposed, there is no assurance about the effective robustness of these networks
    and if they can be trusted in critical areas, clearly raising the need to make
    the DNNs inherently robust or easy to be updated every time a new vulnerability
    is encountered. This motivates the presented work, whose main contributions are
    summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We present the most recent adversarial attacks grouped by the adversary capacity,
    accompanied by an illustration of the differences between black-box and white-box
    attacks;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We propose six different domains for adversarial defense grouping, assisted
    by exemplificative figures of each of these domains, and describe the effects
    of adversarial examples in ViTs;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We detail the most widely used metrics and datasets, present state-of-the-art
    results on CIFAR-10, CIFAR-100, and ImageNet, and propose directions for future
    works.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The remaining of the paper is organized as follows: Section [II](#S2 "II Background
    for Adversarial Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") provides background information; Section [III](#S3 "III Related
    Surveys ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses") compares this review with others; Section [IV](#S4 "IV Adversarial
    Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses") presents the set of adversarial attacks; Section [V](#S5 "V Adversarial
    Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses") shows a collection of defenses to overcome these attacks; Section [VII](#S7
    "VII Datasets ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks
    & Defenses") displays the commonly used datasets; Section [VIII](#S8 "VIII Metrics
    and State-of-the-art Results ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") lists and elaborates on metrics and presents state-of-the-art
    results; and Section [IX](#S9 "IX Future Directions ‣ How Deep Learning Sees the
    World: A Survey on Adversarial Attacks & Defenses") presents future directions,
    with the concluding remarks included in Section [X](#S10 "X Conclusions ‣ How
    Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses").'
  prefs: []
  type: TYPE_NORMAL
- en: II Background for Adversarial Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: II-A Neural Network Architectures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'When an input image is fed into a CNN, it is converted into a matrix containing
    the numeric values representing the image or, if the image is colored, a set of
    matrices containing the numeric values for each color channel. Then, the Convolutions
    apply filters to these matrixes and calculate a set of reduced-size features.
    Finally, these features have an array format fed into the Fully Connected that
    classifies the provided image. Figure [1](#S2.F1 "Figure 1 ‣ II-A Neural Network
    Architectures ‣ II Background for Adversarial Attacks ‣ How Deep Learning Sees
    the World: A Survey on Adversarial Attacks & Defenses") shows an elementary example
    of CNNs used to classify images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/092f7c6eee0959ca31da9a53493bc6e9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Schematic example of the Convolutional Neural Networks mechanism
    to classify images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Contrary to the CNN, ViT does not receive the image as a whole as input; instead,
    it is pre-processed to be divided into Patches, which are smaller parts of the
    original image, as displayed in Figure [2](#S2.F2 "Figure 2 ‣ II-A Neural Network
    Architectures ‣ II Background for Adversarial Attacks ‣ How Deep Learning Sees
    the World: A Survey on Adversarial Attacks & Defenses"). These Patches are not
    fed randomly to the Transformer Encoder, they are ordered by their position, and
    both the Patches and their position are fed into the Transformer Encoder. Finally,
    the output resulting from the Transformer Encoder is fed into the Multi-Layer
    Perceptron (MLP) Head that classifies the image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9ca28e9d38fdaa51b6ae2a7ef3057dad.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Schematic example of a simplified vision transformer used to classify
    images.'
  prefs: []
  type: TYPE_NORMAL
- en: II-B Adversarial Example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Misclassification might be justified if the object contained in the image is
    not visible even to Humans. However, adversarial examples do not fit this scope.
    These examples add a perturbation to an image that causes the DNNs to misclassify
    the object in the image, yet Humans can correctly classify the same object.
  prefs: []
  type: TYPE_NORMAL
- en: 'The adversarial attacks described throughout this survey focus on identifying
    the adversarial examples that make DNNs misclassify. These attacks identify specific
    perturbations that modify the DNN classification while being correctly classified
    by Humans. The calculation of these perturbations is an optimization problem formally
    defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\arg\min_{\delta\textbf{X}}\&#124;\delta_{\textbf{X}}\&#124;\textbf{
    s.t. }\textbf{f}(\textbf{X}+\delta_{\textbf{X}})=\textbf{Y}\ast,$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: 'where $f$ is the is the classifier, $\delta_{\textbf{X}}$ is the perturbation,
    X is the original/benign image, and $\textbf{Y}\ast$ is the adversarial output.
    Furthermore, the adversarial example is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\textbf{X}\ast=\textbf{X}+\delta_{\textbf{X}},$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $\textbf{X}\ast$ is the adversarial image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ II-B Adversarial Example ‣ II Background for
    Adversarial Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") displays adversarial examples generated using different attacks.
    Mainly, the first row is the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) [[14](#bib.bib14)]
    attack, the second row is the DeepFool [[21](#bib.bib21)] attack, and the third
    row is the SmoothFool [[22](#bib.bib22)] attack. When observing the L-BFGS, the
    perturbation applies noise to almost the entirety of the adversarial image. The
    DeepFool attack only perturbs the area of the whale but not all the pixels in
    that area. Finally, the SmoothFool attack slightly disturbs the pixels in the
    area of the image. These three attacks display the evolution of adversarial attacks
    in decreasing order of detectability and, consequently, increasing order of strength.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c9efbaecc40ded3d0fdcbe55127c3bec.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Adversarial Examples created using different state-of-the-art adversarial
    attacks. The first column represents the original image; the second represents
    the perturbation used to generate the adversarial images in the third column.
    The images were resized for better visualization. Images withdrawn from [[14](#bib.bib14),
    [21](#bib.bib21), [22](#bib.bib22)]. The first perturbation follows the edges
    of the building, the second is concentrated in the area of the whale, and the
    third is more smooth and greater in area.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To limit the noise that each perturbation can add to an image, the adversarial
    attacks are divided into $L_{0}$, $L_{2}$, and $L_{p}$ norms, known as Vector
    Norms. Furthermore, commonly used terminologies in the context of adversarial
    examples are defined in Table [I](#S2.T1 "TABLE I ‣ II-D Adversary Goals and Capacity
    ‣ II Background for Adversarial Attacks ‣ How Deep Learning Sees the World: A
    Survey on Adversarial Attacks & Defenses").'
  prefs: []
  type: TYPE_NORMAL
- en: II-C Vector Norms and $\epsilon$ Constraint
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Vector Norms are functions that take a vector as input and output a positive
    value (scalar). These functions are essential to ML and allow the backpropagation
    algorithms to compute the loss value as a scalar. The family of these functions
    is known as the p-norm, and, in the context of adversarial attacks, the considered
    values for $p$ are $0$, $2$, and $\infty$.
  prefs: []
  type: TYPE_NORMAL
- en: '$L_{0}$ norm consists of counting the number of non-zero elements in the vector
    and is formally given as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $&#124;&#124;x&#124;&#124;_{0}=(&#124;x_{1}&#124;^{0}+&#124;x_{2}&#124;^{0}+...+&#124;x_{n}&#124;^{0}),$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $x_{1}$ to $x_{n}$ are the elements of the vector $x$.
  prefs: []
  type: TYPE_NORMAL
- en: '$L_{2}$ norm, also known as the Euclidean distance, measures the vector distance
    to the origin and is formally defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $&#124;&#124;x&#124;&#124;_{2}=(&#124;x_{1}&#124;^{2}+&#124;x_{2}&#124;^{2}+...+&#124;x_{n}&#124;^{2})^{\frac{1}{2}},$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $x_{1}$ to $x_{n}$ are the elements of the vector $x$.
  prefs: []
  type: TYPE_NORMAL
- en: '$L_{\infty}$ norm represents the maximum hypothetical value that $p$ can have
    and returns the absolute value of the element with the largest magnitude, formally
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $&#124;&#124;x&#124;&#124;_{\infty}=\max_{i}&#124;x_{i}&#124;,$ |  | (5)
    |'
  prefs: []
  type: TYPE_TB
- en: where $x_{i}$ is each element of the vector $x$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/145be088936a368d947b9ff7b1ec4b58.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Geometric representation of the $l_{0}$, $l_{2}$, and $l_{\infty}$
    norms, from left to right, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A geometric representation of the area of exploitation for the three considered
    p-norm is displayed in Figure [4](#S2.F4 "Figure 4 ‣ II-C Vector Norms and ϵ Constraint
    ‣ II Background for Adversarial Attacks ‣ How Deep Learning Sees the World: A
    Survey on Adversarial Attacks & Defenses"). One relevant property of the p-norm
    is: the higher $p$ is, the more important the contribution of large errors; the
    lower $p$ is, the higher the contribution of small errors. This translates into
    a large $p$ benefiting small maximal errors (minimal perturbations along multiple
    pixels) and a small $p$ encouraging larger spikes in fewer places (abrupt perturbations
    along minimal pixels). Therefore, $l_{2}$ and $l_{0}$ attacks have greater detectability
    than $l_{\infty}$ attacks, with the latter being more threatening.'
  prefs: []
  type: TYPE_NORMAL
- en: Another constraint normally seen in the context of adversarial attacks is $\epsilon$,
    which is a constant that controls the amount of noise, via generated perturbation,
    that can be added to an image. Usually, it is a tiny number and varies depending
    on the used dataset, decreasing when the task increases in difficulty. According
    to the literature, for MNIST, $\epsilon=0.1$, for CIFAR-10 and CIFAR-100, $\epsilon=8/255$,
    and for ImageNet, $\epsilon=4/255$.
  prefs: []
  type: TYPE_NORMAL
- en: II-D Adversary Goals and Capacity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Besides the restriction imposed by the different Vector Norms, the adversarial
    attacks are also divided by their impact on the networks. Depending on the goals
    of the attacker, the designation is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Confidence Reduction, the classifier outputs the original label with less confidence;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Untargeted, the classifier outputs any class besides the original label;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Targeted, the classifier outputs a particular class besides the original label.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Another important aspect of adversarial attacks is the amount of knowledge
    the attacker has access to. As defined by Papernot et al. [[15](#bib.bib15)],
    who proposed the first threat model for deep learning, the attackers can have
    access to: 1) data training and network architecture; 2) only network architecture;
    3) only data training; 4) an oracle that replies to all the inputs given; and
    5) only have pairs of input and corresponding output (samples). However, to simplify
    this classification, these capacities were divided into:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: White-box, which considers that the attacker has access to either the architecture
    or data;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Black-box, when the attacker can only access samples from an oracle or in pairs
    of input and output.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The attackers goals and capacity are essential to classify the strength of an
    attack. For example, the easiest is a Confidence Reduction White-box attack, and
    the strongest is a Targeted Black-box attack.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Common terminologies used in the context of adversarial attacks and
    their definition.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Terminology | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Original/ Clean Example | Original image presented in a dataset |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial/ Perturbed Example | Image that an adversary has manipulated
    to fool the classifier |'
  prefs: []
  type: TYPE_TB
- en: '| Perturbation | Set of changes (for each pixel and color channel) that are
    performed on the image |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial Attack | Technique used to calculate the perturbation that generates
    an adversarial example |'
  prefs: []
  type: TYPE_TB
- en: '| Transferability | Capability of an adversarial example being transferred
    from a known network to an unknown network |'
  prefs: []
  type: TYPE_TB
- en: '| White-box | Attacks that have access to DNN weights and datasets |'
  prefs: []
  type: TYPE_TB
- en: '| Black-box | Attacks that do not have access to the DNN weights and datasets
    |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial Training | Inclusion of adversarial examples in the training
    phase of the model |'
  prefs: []
  type: TYPE_TB
- en: III Related Surveys
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The first attempt to summarize and display the recent developments in this
    area was made by Akhtar and Mian [[23](#bib.bib23)]. These authors studied adversarial
    attacks in computer vision, extensively referring to attacks for classification
    and providing a brief overview of attacks beyond the classification problem. Furthermore,
    the survey presents a set of attacks performed in the real world and provides
    insight into the existence of adversarial examples. Finally, the authors present
    the defenses distributed through three categories: modified training or input,
    modifying networks, and add-on networks.'
  prefs: []
  type: TYPE_NORMAL
- en: From a broader perspective, Liu et al. [[24](#bib.bib24)] studied the security
    threats and possible defenses in ML scope, considering the different phases of
    an ML algorithm. For example, the training phase is only susceptible to poisoning
    attacks; however, the testing phase is vulnerable to evasion, impersonation, and
    inversion attacks, making it harder to defend. The authors provide their insight
    on the currently used techniques. Additionally, focusing more on the object recognition
    task, Serban et al. [[25](#bib.bib25)] extensively analyzed the adversarial attacks
    and defenses proposed under this context, providing conjectures for the existence
    of adversarial examples and evaluating the capacity of adversarial examples transferring
    between different DNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Qui et al. [[26](#bib.bib26)] extensively explains background concepts in Adversarial
    Attacks, mentioning adversary goals, capabilities, and characteristics. It also
    displays applications for adversarial attacks and presents some of the most relevant
    adversarial defenses. Furthermore, it explains a set of attacks divided by the
    stage in which they occur, referring to the most relevant attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Xu et al. [[27](#bib.bib27)] also describes background concepts, describing
    the adversary goals and knowledge. This review summarizes the most relevant adversarial
    attacks at the time of that work and presents physical world examples. Furthermore,
    the authors present a batch of defenses grouped by the underlying methodology.
    Finally, there is an outline of adversarial attacks in graphs, text, and audio
    networks, culminating in the possible applications of these attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Chakraborty et al. [[28](#bib.bib28)] provides insight into commonly used ML
    algorithms and presents the adversary capabilities and goals. The presented adversarial
    attacks are divided based on the stage of the attack (train or test). Additionally,
    the authors present relevant defenses used in adversarial settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Characteristics shown in state-of-the-art surveys on Adversarial
    Attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Survey | Year | White & | Survey | Grouping of | Future | Datasets | Metrics
    and | State-of-the-art | Vision |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Black-Box | Comparison | Defenses | Directions | Overview | Architectures
    | Comparison | Transformers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Akhtar and Mian [[23](#bib.bib23)] | 2018 | ✓ | $\times$ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Qiu et al. [[26](#bib.bib26)] | 2019 | ✓ | $\times$ | $\times$ | $\times$
    | $\times$ | $\times$ | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Serban et al. [[25](#bib.bib25)] | 2020 | ✓ | $\times$ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[27](#bib.bib27)] | 2020 | ✓ | $\times$ | ✓ | $\times$ | $\times$
    | $\times$ | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Chakraborty et al. [[28](#bib.bib28)] | 2021 | ✓ | $\times$ | $\times$ |
    $\times$ | $\times$ | $\times$ | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Long et al. [[29](#bib.bib29)] | 2022 | ✓ | ✓ | $\times$ | ✓ | $\times$ |
    $\times$ | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[30](#bib.bib30)] | 2022 | ✓ | $\times$ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[31](#bib.bib31)] | 2022 | ✓ | $\times$ | ✓ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ |'
  prefs: []
  type: TYPE_TB
- en: '| This survey | 2023 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: Long et al. [[29](#bib.bib29)] discusses a set of preliminary concepts of Computer
    Vision and adversarial context, providing a set of adversarial attacks grouped
    by adversary goals and capabilities. Finally, the authors provide a set of research
    directions that readers can use to continue the development of robust networks.
  prefs: []
  type: TYPE_NORMAL
- en: Liang et al. [[30](#bib.bib30)] discuss the most significant attacks and defenses
    in the literature, with the latter being grouped by the underlying technique.
    This review finishes with a presentation of the challenges currently existing
    in the adversarial context.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, Zhou et al. [[31](#bib.bib31)] provides insight into Deep Learning
    and Threat Models, focusing on the Cybersecurity perspective. Therefore, the authors
    identify multiple stages based on Advanced Persistent Threats and explain which
    adversarial attacks are adequate for each stage. Similarly, the same structure
    is followed to present the appropriate defenses for each stage. Furthermore, this
    survey presents the commonly used datasets in adversarial settings and provides
    a set of future directions from a Cybersecurity perspective.
  prefs: []
  type: TYPE_NORMAL
- en: 'From the analysis of the previous surveys, some concepts have already been
    standardized, such as adversary goals and capabilities and the existence of adversarial
    attacks and defenses. However, due to the recent inception of this area, there
    still needs to be more standardization in datasets and metrics. Therefore, with
    this survey, we also analyze datasets and metrics to provide insight to novice
    researchers. Furthermore, this survey consolidates the state-of-the-art results
    and identifies which datasets can be further explored. Finally, similarly to other
    reviews, this paper provides a set of future directions that researchers and practitioners
    can follow to start their work. A comparison between the several surveys discussed
    in this section is summarized in Table [II](#S3.T2 "TABLE II ‣ III Related Surveys
    ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses").'
  prefs: []
  type: TYPE_NORMAL
- en: IV Adversarial Attacks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Adversarial attacks are commonly divided by the amount of knowledge the adversaries
    have access to, white-box and black-box, as can be seen in Figure [5](#S4.F5 "Figure
    5 ‣ IV Adversarial Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e45b9c3e238a2921889e24c406a08ef5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Schematic overview of an Adversarial Attack under White-box Settings
    (left) and Black-box Settings (right). The first one uses the classifier predictions
    and network gradients to create perturbations (similar to noise), which can fool
    this classifier. These perturbations are added to the original images, creating
    adversarial images, which are fed to the network and cause misclassification.
    In the Black-box Settings, the same process is applied to a known classifier,
    and the obtained images are used to attack another classifier (represented as
    Target Architecture).'
  prefs: []
  type: TYPE_NORMAL
- en: IV-A White-box Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial examples were first proposed by Szegedy et al. [[14](#bib.bib14)],
    which discovered that DNNs do not generalize well in the vicinity of an input.
    The same authors proposed L-BFGS, the first adversarial attack, to create adversarial
    examples and raised awareness in the scientific community for this generalization
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fast Gradient Sign Method (FGSM) [[32](#bib.bib32)] is a one-step method to
    find adversarial examples, which is based on the linear explanation for the existence
    of adversarial examples, and is calculated using the model cost function, the
    gradient, and the radius epsilon. This attack is formally defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x-\epsilon\cdot\text{sign}(\nabla\text{loss}_{F,t}(x)),$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where $x$ is the original image, $\epsilon$ is the amount of changes to the
    image, and $t$ is the target label. The value for $\epsilon$ should be very small
    to make the attack undetectable.
  prefs: []
  type: TYPE_NORMAL
- en: Jacobian-based Saliency Maps (JSM) [[15](#bib.bib15)] explore the forward derivates
    to calculate the model gradients, replacing the gradient descent approaches, and
    discover which input regions are likely to yield adversarial examples. Then it
    uses saliency maps to construct the adversarial saliency maps, which display the
    features the adversary must perturb. Finally, to prove the effectiveness of JSM,
    only the adversarial examples correctly classified by humans were used to fool
    neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: DeepFool [[21](#bib.bib21)] is an iterative attack that stops when the minimal
    perturbation that alters the model output is found, exploiting its decision boundaries.
    It finds the minimum perturbation for an input $x_{0}$, corresponding to the vector
    orthogonal to the hyperplane representing the decision boundary.
  prefs: []
  type: TYPE_NORMAL
- en: Kurakin et al. [[33](#bib.bib33)] was the first to demonstrate that adversarial
    examples can also exist in the physical world, by using three different methods
    to generate the adversarial examples. Basic Iterative Method (BIM) applies the
    FGSM multiple times with a small step size between iterations and clips the intermediate
    values after each step. Iterative Least-likely Class Method (ILCM) uses the least-likely
    class, according to the prediction of the model, as the target class and uses
    BIM to calculate the adversarial example that outputs the target class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Carlini and Wagner (C&W) [[34](#bib.bib34)] attack is one of the most powerful
    attacks, which uses three different vector norms: 1) the $L_{2}$ attack uses a
    smoothing of clipped gradient descent approach, displaying low distortion; 2)
    the $L_{0}$ attack uses an iterative algorithm that, at each iteration, fixes
    the pixels that do not have much effect on the classifier and finds the minimum
    amount of pixels that need to be altered; and 3) the $L_{\infty}$ attack also
    uses an iterative algorithm with an associated penalty, penalizing every perturbation
    that exceeds a predefined value, formally defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{min}\quad c\cdot f(x+\delta)+\sum_{i}[(\delta_{i}-\tau)^{+}],$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: where $\delta$ is the perturbation, $\tau$ is the penalty threshold (initially
    1, decreasing in each iteration), and $c$ is a constant. The value for $c$ starts
    as a very low value (e.g., $10^{-4}$), and each time the attack fails, the value
    for $c$ is doubled. If $c$ exceeds a threshold (e.g., $10^{10}$), it aborts the
    search.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Aligned Adversarial Subspace (GAAS) [[35](#bib.bib35)] is an attack
    that directly estimates the dimensionality of the adversarial subspace using the
    first-order approximation of the loss function. Through the experiments, GAAS
    proved the most successful at finding many orthogonal attack directions, indicating
    that neural networks generalize linearly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Projected Gradient Descent (PGD) [[36](#bib.bib36)] is an iterative attack
    that uses saddle point formulation, viewed as an inner maximization problem and
    an outer minimization problem, to find a strong perturbation. It uses the inner
    maximization problem to find an adversarial version of a given input that achieves
    a high loss and the outer minimization problem to find model parameters that minimize
    the loss in the inner maximization problem. The saddle point problem used by PGD
    is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\theta}\rho(\theta),\text{where}~{}\rho(\theta)=\mathbb{E}_{(x,y)\sim\mathcal{D}}\Bigl{[}\max_{\delta\in\mathcal{S}}L(\theta,x+\delta,y)\Bigr{]},$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $x$ is the original image, $y$ is the corresponding label, and $\mathcal{S}$
    is the set of allowed perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: AdvGAN [[37](#bib.bib37)] uses Generative Adversarial Networks (GAN) [[38](#bib.bib38)]
    to create adversarial examples that are realistic and have high attack success
    rate. The generator receives the original instance and creates a perturbation,
    the discriminator distinguishes the original instance from the perturbed instance,
    and the target neural network is used to measure the distance between the prediction
    and the target class.
  prefs: []
  type: TYPE_NORMAL
- en: Motivated by the inability to achieve a high success rate in black-box settings,
    the Momentum Iterative FGSM (MI-FGSM) [[39](#bib.bib39)] was proposed. It introduces
    momentum, a technique for accelerating gradient descent algorithms, into the already
    proposed Iterative FGSM (I-FGSM), showing that the attack success rate in black-box
    settings increases almost double that of previous attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Croce and Hein [[40](#bib.bib40)] noted that the perturbations generated by
    $l_{0}$ attacks are sparse and by $l_{\infty}$ attacks are smooth on all pixels,
    proposing Sparse and Imperceivable Adversarial Attacks (SIAA). This attack creates
    sporadic and imperceptible perturbations by applying the standard deviation of
    each color channel in both axis directions, calculated using the two immediate
    neighboring pixels and the original pixel.
  prefs: []
  type: TYPE_NORMAL
- en: SmoothFool (SF) [[22](#bib.bib22)] is a geometry-inspired framework for computing
    smooth adversarial perturbations, exploiting the decision boundaries of a model.
    It is an iterative algorithm that uses DeepFool to calculate the initial perturbation
    and smoothly rectifies the resulting perturbation until the adversarial example
    fools the classifier. This attack provides smoother perturbations which improve
    the transferability of the adversarial examples, and their impact varies with
    the different categories in a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of exploring the adversarial examples in the physical world,
    the Adversarial Camouflage (AdvCam) [[41](#bib.bib41)], which crafts physical-world
    adversarial examples that are legitimate to human observers, was proposed. It
    uses the target image, region, and style to perform a physical adaptation (creating
    a realistic adversarial example), which is provided into a target neural network
    to evaluate the success rate of the adversarial example.
  prefs: []
  type: TYPE_NORMAL
- en: Feature Importance-aware Attack (FIA) [[42](#bib.bib42)] considers the object-aware
    features that dominate the model decisions, using the aggregate gradient (gradients
    average concerning the feature maps). This approach avoids local optimum, represents
    transferable feature importance, and uses the aggregate gradient to assign weights
    identifying the essential features. Furthermore, FIA generates highly transferable
    adversarial examples when extracting the feature importance from multiple classification
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Meta Gradient Adversarial Attack (MGAA) [[43](#bib.bib43)] is a novel architecture
    that can be integrated into any existing gradient-based attack method to improve
    cross-model transferability. This approach consists of multiple iterations, and,
    in each iteration, various models are samples from a model zoo to generate adversarial
    perturbations using the selected model, which are added to the previously generated
    perturbations. In addition, using multiple models simulates both white- and black-box
    settings, making the attacks more successful.
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Universal Adversarial Perturbations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Moosavi-Dezfooli et al. [[44](#bib.bib44)] discovered that some perturbations
    are image-agnostic (universal) and cause misclassification with high probability,
    labeled as Universal Adversarial Perturbations (UAPs). The authors found that
    these perturbations also generalize well across multiple neural networks, by searching
    for a vector of perturbations that cause misclassification in almost all the data
    drawn from a distribution of images. The optimization problem that Moosavi-Dezfooli
    et al. are trying to solve is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Delta v_{i}\xleftarrow{}\textrm{arg}\min_{r}\&#124;r\&#124;_{2}\quad\textrm{s.t.}\quad\hat{k}(x_{i}+v+r)\neq\hat{k}(x_{i}),$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $\Delta v_{i}$ is the minimal perturbation to fool the classifier, $v$
    is the universal perturbation, and $x_{i}$ is the original image. This optimization
    problem is calculated for each image in a dataset, and the vector containing the
    universal perturbation is updated.
  prefs: []
  type: TYPE_NORMAL
- en: The Universal Adversarial Networks (UAN) [[45](#bib.bib45)] are Generative Networks
    that are capable of fooling a classifier when their output is added to an image.
    These networks were inspired by the discovery of UAPs, which were used as the
    training set and can create perturbations for any given input, demonstrating more
    outstanding results than the original UAPs.
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Black-box Settings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Specifically considering black-box setup, Ilyas et al. [[46](#bib.bib46)] define
    three realistic threat models that are more faithful to real-world settings: query-limited,
    partial information, and label-only settings. The first one suggests the development
    of query-efficient algorithms, using Natural Evolutionary Strategies to estimate
    the gradients used to perform the PGD attack. When only having the probabilities
    for the top-k labels, the algorithm alternates between blending in the original
    image and maximizing the likelihood of the target class and, when the attacker
    only obtains the top-k predicted labels, the attack uses noise robustness to mount
    a targeted attack.'
  prefs: []
  type: TYPE_NORMAL
- en: Feature-Guided Black-Box (FGBB) [[47](#bib.bib47)] uses the features extracted
    from images to guide the creation of adversarial perturbations, by using Scale
    Invariant Feature Transform. High probability is assigned to pixels that impact
    the composition of an image in the Human visual system and the creation of adversarial
    examples is viewed as a two-player game, where the first player minimizes the
    distance to an adversarial example, and the second one can have different roles,
    leading to minimal adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: Square Attack [[48](#bib.bib48)] is an adversarial attack that does not need
    local gradient information, meaning that gradient masking does not affect it.
    Furthermore, this attack uses a randomized search scheme that selects localized
    square-shaped updates in random positions, causing the perturbation to be situated
    at the decision boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: IV-D Auto-Attack
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Auto-Attack [[49](#bib.bib49)] was proposed to test adversarial robustness
    in a parameter-free, computationally affordable, and user-independent way. As
    such, Croce et al. proposed two variations of PGD to overcome suboptimal step
    sizes of the objective function, namely APGD-CE and APGD-DLR, for a step size-free
    version of PGD using cross-entropy (CE) and Difference of Logits Ratio (DLR) loss,
    respectively. DLR is a loss proposed by Croce et al. which is both shift and rescaling
    invariant and thus has the same degrees of freedom as the decision of the classifier,
    not suffering from the issues of the cross-entropy loss [[49](#bib.bib49)]. Then,
    they combine these new PGD variations with two other existing attacks to create
    Auto-Attack, which is composed by:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APGD-CE, step size-free version of PGD on the cross-entropy;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: APGD-DLR, step size-free version of PGD on the DLR loss;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fast Adaptive Boundary (FAB) [[50](#bib.bib50)], which minimizes the norm of
    the adversarial perturbations;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Square [[48](#bib.bib48)] Attack, a query-efficient black-box attack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Given the main motivation of the Auto-Attack proposal, the FAB attack is the
    targeted version of FAB [[50](#bib.bib50)] since the untargeted version computes
    each iteration of the Jacobian matrix of the classifier, which scales linearly
    with the number of classes of the dataset. Although this is feasible for datasets
    with a low number of classes (e.g., MNIST and CIFAR-10), it becomes both computationally
    and memory-wise challenging with an increased number of classes (e.g., CIFAR-100
    and ImageNet).
  prefs: []
  type: TYPE_NORMAL
- en: 'As such, Auto-Attack is an ensemble of attacks with important fundamental properties:
    APGD is a white-box attack aiming at any adversarial example within an $L_{p}$-ball
    (Section [II-C](#S2.SS3 "II-C Vector Norms and ϵ Constraint ‣ II Background for
    Adversarial Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses")), FAB minimizes the norm of the perturbation necessary to
    achieve a misclassification, and Square Attack is a score-based black-box attack
    for norm bounded perturbations which use random search and do not exploit any
    gradient approximation, competitive with white-box attacks [[48](#bib.bib48)].'
  prefs: []
  type: TYPE_NORMAL
- en: V Adversarial Defenses
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: V-A Adversarial Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Szegedy et al. [[14](#bib.bib14)] proposed that training on a mixture of adversarial
    and clean examples could regularize a neural network, as shown in Figure [6](#S5.F6
    "Figure 6 ‣ V-A Adversarial Training ‣ V Adversarial Defenses ‣ How Deep Learning
    Sees the World: A Survey on Adversarial Attacks & Defenses"). Goodfellow et al. [[32](#bib.bib32)]
    evaluated the impact of Adversarial Training as a regularizer by including it
    in the objective function, showing that this approach is a reliable defense that
    can be applied to every neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: Kurakin et al. [[51](#bib.bib51)] demonstrates that it is possible to perform
    adversarial training in more massive datasets (ImageNet), displaying that the
    robustness significantly increases for one-step methods. When training the model
    with one-step attacks using the ground-truth labels, the model has significantly
    higher accuracy on the adversarial images than on the clean images, an effect
    denominated as Label Leaking, suggesting that the adversarial training should
    not make use of the ground-truth labels.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial Training in large datasets implies using fast single-step methods,
    which converge to a degenerate global minimum, meaning that models trained with
    this technique remain vulnerable to black-box attacks. Therefore, Ensemble Adversarial
    Training [[52](#bib.bib52)] uses adversarial examples crafted on other static
    pre-trained models to augment the training data, preventing the trained model
    from influencing the strength of the adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: Shared Adversarial Training [[53](#bib.bib53)] is an extension of adversarial
    training aiming to maximize robustness against universal perturbations. It splits
    the mini-batch of images used in training into a set of stacks and obtains the
    loss gradients concerning these stacks. Afterward, the gradients for each stack
    are processed to create a shared perturbation that is applied to the whole stack.
    After every iteration, these perturbations are added and clipped to constrain
    them into a predefined magnitude. Finally, these perturbations are added to the
    images and used for adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization (TRADES) [[54](#bib.bib54)]
    is inspired by the presumption that robustness can be at odds with accuracy [[55](#bib.bib55),
    [56](#bib.bib56)]. The authors show that the robust error can be tightly bounded
    by using natural error measured by the surrogate loss function and the likelihood
    of input features being close to the decision boundary (boundary error). These
    assumptions make the model weights biased toward natural or boundary errors.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/edc89bab58bccb9668a5779c37218162.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Schematic overview of Adversarial Training. A subset of the original
    images of a dataset is fed into an adversarial attack (e.g., PGD, FGSM, or C&W),
    which creates adversarial images. Each batch contains original and adversarial
    images, with the Classifier being normally trained.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the idea that gradient magnitude is directly linked to model robustness,
    Bilateral Adversarial Training (BAT) [[57](#bib.bib57)] proposes to perturb not
    only the images but also the manipulation of labels (adversarial labels) during
    the training phase. The adversarial labels are derived from a closed-form heuristic
    solution, and the adversarial images are generated from a one-step targeted attack.
  prefs: []
  type: TYPE_NORMAL
- en: Despite the popularity of adversarial training to defend models, it has a high
    cost of generating strong adversarial examples, namely for large datasets such
    as ImageNet. Therefore, Free Adversarial Training (Free-AT) [[58](#bib.bib58)]
    uses the gradient information when updating model parameters to generate the adversarial
    examples, eliminating the previously mentioned overhead.
  prefs: []
  type: TYPE_NORMAL
- en: Considering the same issue presented in Free-AT, the authors analyze Pontryagin’s
    Maximum Principle [[59](#bib.bib59)] of this problem and observe that the adversary
    update is only related to the first layer of the network. Thus, You Only Propagate
    Once (YOPO) [[60](#bib.bib60)] only considers the first layer of the network for
    forward and backpropagation, effectively reducing the amount of propagation to
    one in each update.
  prefs: []
  type: TYPE_NORMAL
- en: Misclassification Aware adveRsarial Training (MART) [[61](#bib.bib61)] is an
    algorithm that explicitly differentiates the misclassified and correctly classified
    examples during training. This proposal is motivated by the finding that different
    maximization techniques are negligible, but minimization ones are crucial when
    looking at the misclassified examples.
  prefs: []
  type: TYPE_NORMAL
- en: Defense against Occlusion Attacks (DOA) [[62](#bib.bib62)] is a defense mechanism
    that uses abstract adversarial attacks, Rectangular Occlusion Attack (ROA) [[62](#bib.bib62)],
    and applies the standard adversarial training. This attack considers including
    physically realizable attacks that are “normal” in the real world, such as eyeglasses
    and stickers on stop signs.
  prefs: []
  type: TYPE_NORMAL
- en: The proposal of Smooth Adversarial Training (SAT) [[63](#bib.bib63)] considers
    the evolution normally seen in curriculum learning, where the difficulty increases
    with time (age), using two difficulty metrics. These metrics are based on the
    maximal Hessian eigenvalue (H-SAT) and the softmax Probability (P-SAT), which
    are used to stabilize the networks for large perturbations while having high clean
    accuracy. In the same context, Friendly Adversarial Training (Friend-AT) [[64](#bib.bib64)]
    minimizes the loss considering the least adversarial data (friendly) among the
    adversarial data that is confidently misclassified. This method can be employed
    by early stopping PGD attacks when performing adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: Contrary to the idea of Free-AT [[58](#bib.bib58)], Cheap Adversarial Training
    (Cheap-AT) [[65](#bib.bib65)] proposes the use of weaker and cheaper adversaries
    (FGSM) combined with random initialization to train robust networks effectively.
    This method can be further accelerated by applying techniques that efficiently
    train networks.
  prefs: []
  type: TYPE_NORMAL
- en: In a real-world context, the attacks are not limited by the imperceptibility
    constraint ($\epsilon$ value); there are, in fact, multiple perturbations (for
    models) that have visible sizes. The main idea of Oracle-Aligned Adversarial Training
    (OA-AT) [[66](#bib.bib66)] is to create a model that is robust to high perturbation
    bounds by aligning the network predictions with ones of an Oracle during adversarial
    training. The key aspect of OA-AT is the use of Learned Perceptual Image Patch
    Similarity [[67](#bib.bib67)] to generate Oracle-Invariant attacks and convex
    combination of clean and adversarial predictions as targets for Oracle-Sensitive
    samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometry-aware Instance-reweighted Adversarial Training (GI-AT) [[68](#bib.bib68)]
    has two foundations: 1) over-parameterized models still lack capacity; and 2)
    a natural data point closer to the class boundary is less robust, translating
    into assigning the corresponding adversarial data a larger weight. Therefore,
    this defense proposes using standard adversarial training, considering that weights
    are based on how difficult it is to attack a natural data point.'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial training leads to unfounded increases in the margin along decision
    boundaries, reducing clean accuracy. To tackle this issue, Helper-based Adversarial
    Training (HAT) [[69](#bib.bib69)] incorporates additional wrongly labeled examples
    during training, achieving a good trade-off between accuracy and robustness.
  prefs: []
  type: TYPE_NORMAL
- en: As a result of the good results achieved by applying random initialization,
    Fast Adversarial Training (FAT) [[70](#bib.bib70)] performs randomized smoothing
    to optimize the inner maximization problem efficiently, and proposes a new initialization
    strategy, named backward smoothing. This strategy helps to improve the stability
    and robustness of a model using single-step robust training methods, solving the
    overfitting issue.
  prefs: []
  type: TYPE_NORMAL
- en: V-B Modify the Training Process
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gu and Rigazio [[71](#bib.bib71)] proposed using three preprocessing techniques
    to recover from the adversarial noise, namely, noise injection, autoencoder, and
    denoising autoencoder, discovering that the adversarial noise is mainly distributed
    in the high-frequency domain. Solving the adversarial problem corresponds to encountering
    adequate training techniques and objective functions to increase the distortion
    of the smallest adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another defense against adversarial examples is Defensive Distillation [[72](#bib.bib72)],
    which uses the predictions from a previously trained neural network, as displayed
    in Figure [7](#S5.F7 "Figure 7 ‣ V-B Modify the Training Process ‣ V Adversarial
    Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses"). This approach trains the initial neural network with the original
    training data and labels, producing the probability of the predictions, which
    replace the original training labels to train a smaller and resilient distilled
    network. Additionally, to improve the results obtained by Defensive Distillation,
    Papernot and McDaniel [[73](#bib.bib73)] propose to change the vector used to
    train the distilled network by combining the original label with the first model
    uncertainty.'
  prefs: []
  type: TYPE_NORMAL
- en: To solve the vulnerabilities of the neural network to adversarial examples,
    the Visual Causal Feature Learning [[74](#bib.bib74)] method uses causal reasoning
    to perform data augmentation. This approach uses manipulator functions that return
    an image similar to the original one with the desired causal effect.
  prefs: []
  type: TYPE_NORMAL
- en: Learning with a Strong Adversary [[75](#bib.bib75)] is a training procedure
    that formulates as a min-max problem, making the classifier inherently robust.
    This approach considers that the adversary applies perturbations to each data
    point to maximize the classification error, and the learning procedure attempts
    to minimize the misclassification error against the adversary. The greatest advantage
    of this procedure is the significant increase in robustness while maintaining
    clean high accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Zheng et al. [[76](#bib.bib76)] proposes the use of compression, rescaling,
    and cropping in benign images to increase the stability of DNNs, denominated as
    Image Processing, without changing the objective functions. A Gaussian perturbation
    sampler perturbs the benign image, which is fed to the DNN, and its feature representation
    of benign images is used to 1) minimize the standard CE loss; and 2) minimize
    the stability loss.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d57ab39871f2a1ff6627e02fb7eaf04f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Method proposed by Defensive Distillation [[72](#bib.bib72)]. An
    Initial Network is trained on the dataset images and labels (discrete values).
    Then, the predictions given by the Initial Network are fed into another network,
    replacing the dataset labels. These predictions are continuous values, making
    the Distilled Network more resilient to adversarial attacks.'
  prefs: []
  type: TYPE_NORMAL
- en: Zantedeschi et al. [[77](#bib.bib77)] explored the standard architectures, which
    usually employ Rectified Linear Units (ReLU) [[78](#bib.bib78), [79](#bib.bib79)]
    to ease the training process, and discovered that this function makes a small
    perturbation in the input accumulate with multiple layers (unbounded). Therefore,
    the authors propose the use of bounded ReLU (BReLU) [[80](#bib.bib80)] to prevent
    this accumulation and Gaussian Data Augmentation to perform data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Zhang and Wang [[19](#bib.bib19)] suggest that adversarial examples are generated
    through Feature Scattering (FS) in the latent space to avoid the label leaking
    effect, which considers the inter-example relationships. The adversarial examples
    are generated by maximizing the feature-matching distance between the clean and
    perturbed examples, FS produces a perturbed empirical distribution, and the DNN
    performs standard adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: 'PGD attack causes the internal representation to shift closer to the “false”
    class, Triplet Loss Adversarial (TLA) [[81](#bib.bib81)] includes an additional
    term in the loss function that pulls natural and adversarial images of a specific
    class closer and the remaining classes further apart. This method was tested with
    different samples: Random Negative (TLA-RN), which refers to a randomly sampled
    negative example, and Switch Anchor (TLA-SA), which sets the anchor as a natural
    example and the positive to be adversarial examples.'
  prefs: []
  type: TYPE_NORMAL
- en: Kumari et al. [[82](#bib.bib82)] analyzes the previously adversarial-trained
    models to test their vulnerability against adversarial attacks at the level of
    latent layers, concluding that the latent layer of these models is significantly
    vulnerable to adversarial perturbations of small magnitude. Latent Adversarial
    Training (LAT) [[82](#bib.bib82)] consists of finetuning adversarial-trained models
    to ensure robustness at the latent level.
  prefs: []
  type: TYPE_NORMAL
- en: Curvature Regularization (CR) [[83](#bib.bib83)] minimizes the curvature of
    the loss surface, which induces a more ”natural” behavior of the network. The
    theoretical foundation behind this defense uses a locally quadratic approximation
    that demonstrates a strong relation between large robustness and small curvature.
    Furthermore, the proposed regularizer confirms the assumption that exhibiting
    quasi-linear behavior in the proximity of data points is essential to achieve
    robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised Adversarial Training (UAT) [[84](#bib.bib84)] enables the training
    with unlabeled data considering two different approaches, UAT with Online Target
    (UAT-OT) that minimizes a differentiable surrogate of the smoothness loss, and
    UAT with Fixed Targets (UAT-FT) that trains an external classifier to predict
    the labels on the unsupervised data and uses its predictions as labels.
  prefs: []
  type: TYPE_NORMAL
- en: Robust Self-Training (RST) [[85](#bib.bib85)], an extension of Self-Training [[86](#bib.bib86),
    [87](#bib.bib87)], uses a standard supervised training to obtain pseudo-labels
    and then feeds them into a supervised training algorithm that targets adversarial
    robustness. This approach bridges the gap between standard and robust accuracy,
    using the unlabeled data, achieving high robustness using the same number of labels
    as required for high standard accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: SENSEI [[88](#bib.bib88)] and SENSEI-SA [[88](#bib.bib88)] use the methodologies
    employed in software testing to perform data augmentation, enhancing the robustness
    of DNNs. SENSEI implements the strategy of replacing each data point with a suitable
    variant or leaving it unchanged. SENSEI-SA improves the previous one by identifying
    which opportunities are suitable for skipping the augmentation process.
  prefs: []
  type: TYPE_NORMAL
- en: 'Bit Plane Feature Consistency (BPFC) [[89](#bib.bib89)] regularizer forces
    the DNNs to give more importance to the higher bit planes, inspired by the Human
    visual system perception. This regularizer uses the original image and a preprocessed
    version to calculate the $l_{2}$ norm between them and regularize the loss function,
    as the scheme shown in Figure [8](#S5.F8 "Figure 8 ‣ V-B Modify the Training Process
    ‣ V Adversarial Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses").'
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial Weight Perturbation (AWP) [[90](#bib.bib90)] explicitly regularizes
    the flatness of weight loss landscape and robustness gap, using a double-perturbation
    mechanism that disturbs both inputs and weights. This defense boosts the robustness
    of multiple existing adversarial training methods, confirming that it can be applied
    to other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Self-Adaptive Training (SAT) [[91](#bib.bib91)] dynamically calibrates the training
    process with the model predictions without extra computational cost, improving
    the generalization of corrupted data. In contrast with the double-descent phenomenon,
    SAT exhibits a single-descent error-capacity curve, mitigating the overfitting
    effect.
  prefs: []
  type: TYPE_NORMAL
- en: HYDRA [[92](#bib.bib92)] is another technique that explores the effects of pruning
    on the robustness of models, which proposes using pruning techniques that are
    aware of the robust training objective, allowing this objective to guide the search
    for connections to prune. This approach reaches compressed models that are state-of-the-art
    in standard and robust accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d79d0fa10ab29316dc913ce0739e75e5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Schematic overview of the Bit Plane Feature Consistency [[89](#bib.bib89)].
    This method applies multiple operations to input images, simulating adversarial
    images. Then, the loss is changed to include a regularizer (new term), which compares
    the original images with these manipulated images.'
  prefs: []
  type: TYPE_NORMAL
- en: Based on the promising results demonstrated by previous distillation methods,
    the Robust Soft Label Adversarial Distillation (RSLAD) [[93](#bib.bib93)] method
    uses soft labels to train robust small student DNNs. This method uses the Robust
    Soft Labels (RSLs) produced by the teacher DNN to supervise the student training
    on natural and adversarial examples. An essential aspect of this method is that
    the student DNN does not access the original complex labels through the training
    process.
  prefs: []
  type: TYPE_NORMAL
- en: The most sensitive neurons in each layer make significant non-trivial contributions
    to the model predictions under adversarial settings, which means that increasing
    adversarial robustness stabilizes the most sensitive neurons. Sensitive Neuron
    Stabilizing (SNS) [[94](#bib.bib94)] includes an objective function dedicated
    explicitly to maximizing the similarities of sensitive neuron behaviors when providing
    clean and adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Network Rewiring (DNR) [[95](#bib.bib95)] generates pruned DNNs that
    have high robust and standard accuracy, which employs a unified constrained optimization
    formulation using a hybrid loss function that merges ultra-high model compression
    with robust adversarial training. Furthermore, the authors propose a one-shot
    training method that achieves high compression, standard accuracy, and robustness,
    which has a practical inference 10 times faster than traditional methods.
  prefs: []
  type: TYPE_NORMAL
- en: Manifold Regularization for Locally Stable (MRLS) [[96](#bib.bib96)] DNNs exploit
    the continuous piece-wise linear nature of ReLU to learn a function that is smooth
    over both predictions and decision boundaries. This method is based on approximating
    the graph Laplacian when the data is sparse.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by the motivation behind distillation, Learnable Boundary Guided Adversarial
    Training (LBGAT) [[97](#bib.bib97)], assuming that models trained on clean data
    embed their most discriminative features, constrains the logits from the robust
    model to make them similar to the model trained on natural data. This approach
    makes the robust model inherit the decision boundaries of the clean model, preserving
    high standard and robust accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Low Temperature Distillation (LTD) [[98](#bib.bib98)], which uses previous distillation
    frameworks to generate labels, uses relatively low temperatures in the teacher
    model and employs different fixed temperatures for the teacher and student models.
    The main benefit of this mechanism is that the generated soft labels can be integrated
    into existing works without additional costs.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, literature [[99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101)]
    demonstrated that neural Ordinary Differential Equations (ODE) are naturally more
    robust to adversarial attacks than vanilla DNNs. Therefore, Stable neural ODE
    for deFending against adversarial attacks (SODEF) [[102](#bib.bib102)] uses optimization
    formulation to force the extracted feature points to be within the vicinity of
    Lyapunov-stable equilibrium points, which suppresses the input perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: Self-COnsistent Robust Error (SCORE) [[103](#bib.bib103)] employs local equivariance
    to describe the ideal behavior of a robust model, facilitating the reconciliation
    between robustness and accuracy while still dealing with worst-case uncertainty.
    This method was inspired by the discovery that the trade-off between adversarial
    and clean accuracy imposes a bias toward smoothness.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the impact of activation shape on robustness, Dai et al. [[104](#bib.bib104)]
    observes that activation has positive outputs on negative inputs, and a high finite
    curvature can improve robustness. Therefore, Parametric Shifted Sigmoidal Linear
    Unit (PSSiLU) [[104](#bib.bib104)] combines these properties and parameterized
    activation functions with adversarial training.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Use of Supplementary Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MagNet [[105](#bib.bib105)] considers two reasons for the misclassification
    of an adversarial example: 1) incapacity of the classifier to reject an adversarial
    example distant from the boundary; and 2) classifier generalizes poorly when the
    adversarial example is close to the boundary. MagNet considers multiple detectors
    trained based on the reconstruction error, detecting significantly perturbed examples
    and detecting slightly perturbed examples based on probability divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4f50921c5b7b5febe85053cd0e4dad33.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Schematic overview of the Use of Supplementary Networks. The Detector
    Network was previously trained to detect adversarial images and is included between
    the input images and the classifier. This network receives the input images and
    determines if these images are Adversarial or Not. If they are not, they are redirected
    to the Classifier; If they are, they are susceptible to Human evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversary Detection Network (ADN) [[106](#bib.bib106)] is a subnetwork that
    detects if the input example is adversarial or not, trained using adversarial
    images generated for a classification network which are classified as clean (0)
    or adversarial (1). Figure [9](#S5.F9 "Figure 9 ‣ V-C Use of Supplementary Networks
    ‣ V Adversarial Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") displays a schematic overview of this network. However, this
    defense mechanism deeply correlates to the datasets and classification networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Xu et al. found that the inclusion of Feature Squeezing (FS) [[107](#bib.bib107)]
    is highly reliable in detecting adversarial examples by reducing the search space
    available for the adversary to modify. This method compares the predictions of
    a standard network with a squeezed one, detecting adversarial examples with high
    accuracy and having few false positives.
  prefs: []
  type: TYPE_NORMAL
- en: High-level representation Guided Denoiser (HGD) [[17](#bib.bib17)] uses the
    distance between original and adversarial images to guide an image denoiser and
    suppress the impact of adversarial examples. HGD uses a Denoising AutoEncoder [[108](#bib.bib108)]
    with additional lateral connections and considers the difference between the representations
    as the loss function at a specific layer that is activated by the normal and adversarial
    examples.
  prefs: []
  type: TYPE_NORMAL
- en: Defense-GAN [[18](#bib.bib18)] explores the use of GANs to effectively represent
    the set of original training examples, making this defense independent from the
    attack used. Defense-GAN considers the usage of Wasserstein GANs (WGANs) [[109](#bib.bib109)]
    to learn the representation of the original data and denoise the adversarial examples,
    which start by minimizing the $l_{2}$ difference between the generator representation
    and the input image. Reverse Attacks [[110](#bib.bib110)] can be applied to each
    attack during the testing phase, by finding the suitable additive perturbation
    to repair the adversarial example similar to the adversarial attacks, which is
    highly difficult due to the unknown original label.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding Regularized Classifier (ER-Classifier) [[111](#bib.bib111)] is composed
    of a classifier, an encoder, and a discriminator, which uses the encoder to generate
    code vectors by reducing the dimensional space of the inputs and the discriminator
    to separate these vectors from the ideal code vectors (sampled from a prior distribution).
    This technique allows pushing adversarial examples into the benign image data
    distribution, removing the adversarial perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: Class Activation Feature-based Denoiser (CAFD) [[112](#bib.bib112)] is a self-supervised
    approach trained to remove the noise from adversarial examples, using a set of
    examples generated by the Class Activation Feature-based Attack (CAFA) [[112](#bib.bib112)].
    This defense mechanism is trained to minimize the distance of the class activation
    features between the adversarial and natural examples, being robust to unseen
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Detector Graph (DG) [[113](#bib.bib113)] considers graphs to detect the adversarial
    examples by constructing a Latent Neighborhood Graph (LNG) for each original example
    and using Graph Neural Networks (GNNs) [[114](#bib.bib114)] to exploit the relationship
    and distinguish between original and adversarial examples. This method maintains
    an additional reference dataset to retrieve the manifold information and uses
    embedding representation of image pixel values, making the defense robust to unseen
    attacks.
  prefs: []
  type: TYPE_NORMAL
- en: Images in the real world are represented in a continuous manner, yet machines
    can only store these images in discrete 2D arrays. Local Implicit Image Function
    (LIIF) [[115](#bib.bib115)] takes an image coordinate and the deep features around
    this coordinate as inputs, predicting the corresponding RGB value. This method
    of pre-processing input images can filter adversarial images by reducing their
    perturbations, which are subsequently fed to a classifier.
  prefs: []
  type: TYPE_NORMAL
- en: ADversarIal defenSe with local impliCit functiOns (DISCO) [[116](#bib.bib116)]
    is an additional network to the classifier that removes adversarial perturbations
    using localized manifold projections, which receives an adversarial image and
    a query pixel location. This defense mechanism comprises an encoder that creates
    per-pixel deep features and a local implicit module that uses these features to
    predict the clean RGB value.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c5cbf901ebd22623d47c3df9d44789a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Overview of a Feature Denoising Block [[117](#bib.bib117)], which
    can be included in the intermediate layers to make networks more robust. This
    method is an example of Change Network Architecture.'
  prefs: []
  type: TYPE_NORMAL
- en: V-D Change Network Architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To identify the type of layers and their order, Guo et al. [[118](#bib.bib118)]
    proposes the use of Neural Architecture Search (NAS) to identify the networks
    that are more robust to adversarial attacks, finding that densely connected patterns
    improve the robustness and adding convolution operations to direct connection
    edge is effective, combined to create the RobNets [[118](#bib.bib118)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature Denoising [[117](#bib.bib117)] intends to address this problem by applying
    feature-denoising operations, consisting of non-local means, bilateral, mean,
    median filters, followed by 1x1 Convolution and an identity skip connection, as
    illustrated in Figure [10](#S5.F10 "Figure 10 ‣ V-C Use of Supplementary Networks
    ‣ V Adversarial Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses"). These blocks are added to the intermediate layers of CNNs.'
  prefs: []
  type: TYPE_NORMAL
- en: Input Random [[119](#bib.bib119)] propose the addition of layers at the beginning
    of the classifier, consisting of 1) a random resizing layer, which resizes the
    width and height of the original image to a random width and height, and 2) a
    random padding layer, which pads zeros around the resized image in a random manner.
  prefs: []
  type: TYPE_NORMAL
- en: Controlling Neural Level Sets (CNLS) [[120](#bib.bib120)] uses samples obtained
    from the neural level sets and relates their positions to the network parameters,
    which allows modifying the decision boundaries of the network. The relation between
    position and parameters is achieved by constructing a sample network with an additional
    single fixed linear layer, which can incorporate the level set samples into a
    loss function.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Transformation Layer (STL) [[121](#bib.bib121)], included between the
    input image and the network first layer, transforms the received images into a
    low-dimensional quasi-natural image space, which approximates the natural image
    space and removes adversarial perturbations. This creates an attack-agnostic adversarial
    defense that gets the original and adversarial images closer.
  prefs: []
  type: TYPE_NORMAL
- en: Benz et al. [[122](#bib.bib122)] found that BN [[123](#bib.bib123)] and other
    normalization techniques make DNN more vulnerable to adversarial examples, suggesting
    the use of a framework that makes DNN more robust by learning Robust Features
    first and, then, Non-Robust Features (which are the ones learned when using BN).
  prefs: []
  type: TYPE_NORMAL
- en: V-E Perform Network Validation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most of the datasets store their images using the Joint Photographic Experts
    Group (JPEG) [[124](#bib.bib124)] compression, yet no one had evaluated the impact
    of this process on the network performance. Dziugaite et al. [[125](#bib.bib125)]
    (named as JPG) varies the magnitude of FGSM perturbations, discovering that smaller
    ones often reverse the drop in classification by a large extent and, when the
    perturbations increase in magnitude, this effect is nullified.
  prefs: []
  type: TYPE_NORMAL
- en: Regarding formal verification, a tool [[126](#bib.bib126)] for automatic Safety
    Verification of the decisions made during the classification process was created
    using Satisfiability Modulo Theory (SMT). This approach assumes that a decision
    is safe when, after applying transformations in the input, the model decision
    does not change. It is applied to every layer individually in the network, using
    a finite space of transformations.
  prefs: []
  type: TYPE_NORMAL
- en: DeepXplore [[127](#bib.bib127)] is the first white-box framework to perform
    a wide test coverage, introducing the concepts of neuron coverage, which are parts
    of the DNN that are exercised by test inputs. DeepXplore uses multiple DNNs as
    cross-referencing oracles to avoid manual checking for each test input and inputs
    that trigger different behaviors and achieve high neuron coverage is a joint optimization
    problem solved by gradient-based search techniques.
  prefs: []
  type: TYPE_NORMAL
- en: DeepGauge [[128](#bib.bib128)] intends to identify a testbed containing multi-faceted
    representations using a set of multi-granularity testing criteria. DeepGauge evaluates
    the resilience of DNNs using two different strategies, namely, primary function
    and corner-case behaviors, considering neuron- and layer-level coverage criteria.
  prefs: []
  type: TYPE_NORMAL
- en: Surprise Adequacy for Deep Learning Systems (SADL) [[129](#bib.bib129)] is based
    on the behavior of DNN on the training data, by introducing the surprise of an
    input, which is the difference between the DNN behavior when given the input and
    the learned training data. The surprise of input is used as an adequacy criterion
    (Surprise Adequacy), which is used as a metric for the Surprise Coverage to ensure
    the input surprise range coverage.
  prefs: []
  type: TYPE_NORMAL
- en: The most recent data augmentation techniques, such as cutout [[130](#bib.bib130)]
    and mixup [[131](#bib.bib131)], fail to prevent overfitting and, sometimes, make
    the model over-regularized, concluding that, to achieve substantial improvements,
    the combination of early stopping and semi-supervised data augmentation, Overfit
    Reduction (OR) [[132](#bib.bib132)], is the best method.
  prefs: []
  type: TYPE_NORMAL
- en: 'When creating a model, multiple implementation details influence its performance;
    Pang et al. [[133](#bib.bib133)] is the first one to provide insights on how these
    details influence the model robustness, herein named as Bag of Tricks (BT). Some
    conclusions drawn from this study are: 1) The robustness of the models is significantly
    affected by weight decay; 2) Early stopping of the adversarial attacks may deteriorate
    worst-case robustness; and 3) Smooth activation benefits lower capacity models.'
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is a known problem that affects model robustness; Rebuffi et al. [[134](#bib.bib134)]
    focuses on reducing this robust overfitting by using different data augmentation
    techniques. Fixing Data Augmentation (FDA) [[134](#bib.bib134)] demonstrates that
    model weight averaging combined with data augmentation schemes can significantly
    increase robustness, which is enhanced when using spatial composition techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Gowal et al. [[135](#bib.bib135)] systematically studies the effect of multiple
    training losses, model sizes, activation functions, the addition of unlabeled
    data, and other aspects. The main conclusion drawn by this analysis is that larger
    models with Swish/SiLU [[136](#bib.bib136)] activation functions and model weight
    averaging can reliably achieve state-of-the-art results in robust accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2e73b1e158e124e285f4e21fcee722b0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Overview of Adversarial Purification using Denoising Diffusion Probabilistic
    Models, adapted from [[137](#bib.bib137)]. The diffusion process is applied to
    an adversarial image, consisting of adding noise for a certain number of steps.
    During the denoising procedure, this noise is iteratively removed by the same
    amount of steps, resulting in a purified image (without perturbations).'
  prefs: []
  type: TYPE_NORMAL
- en: V-F Adversarial Purification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Adversarial Purification consists of defense mechanisms that remove adversarial
    perturbations using a generative model. Improving Robustness Using Generated Data
    (IRUGD) [[138](#bib.bib138)] explores how generative models trained on the original
    images can be leveraged to increase the size of the original datasets. Through
    extensive experiments, they concluded that Denoising Diffusion Probabilistic Models
    (DDPM) [[137](#bib.bib137)], a progression of diffusion probabilistic models [[139](#bib.bib139)],
    is the model that more closely resembles real data. Figure [11](#S5.F11 "Figure
    11 ‣ V-E Perform Network Validation ‣ V Adversarial Defenses ‣ How Deep Learning
    Sees the World: A Survey on Adversarial Attacks & Defenses") presents the main
    idea behind the DDPM process.'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the great results in image synthesis displayed by the DDPM, Sehwag et
    al. [[140](#bib.bib140)] (Proxy) uses proxy distributions to significantly improve
    the performance of adversarial training by generating additional examples, demonstrating
    that the best generative models for proxy distribution are DDPM.
  prefs: []
  type: TYPE_NORMAL
- en: Inspired by previous works on adversarial purification [[141](#bib.bib141),
    [142](#bib.bib142)], DiffPure [[143](#bib.bib143)] uses DDPM for adversarial purification,
    receiving as input an adversarial example and recovering the clean image through
    a reverse generative process. Since this discovery, multiple improvements regarding
    the use of DDPM for Adversarial Purification have been studied. Guided Diffusion
    Model for Adversarial Purification (GDMAP) [[144](#bib.bib144)] receives as initial
    input pure Gaussian noise and gradually denoises it with guidance to an adversarial
    image.
  prefs: []
  type: TYPE_NORMAL
- en: DensePure [[145](#bib.bib145)] employs iterative denoising to an input image,
    with different random seeds, to get multiple reversed samples, which are given
    to the classifier and the final prediction is based on majority voting. Furthermore,
    Wang et al. [[146](#bib.bib146)] uses the most recent diffusion models [[147](#bib.bib147)]
    to demonstrate that diffusion models with higher efficiency and image quality
    directly translate into better robust accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: VI Adversarial Effects on Vision Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like CNNs [[148](#bib.bib148)], the ViTs are also susceptible to adversarial
    perturbations that alter a patch in an image [[149](#bib.bib149)], and ViTs demonstrate
    higher robustness, almost double, compared with ResNet-50 [[150](#bib.bib150)].
  prefs: []
  type: TYPE_NORMAL
- en: To further evaluate the robustness of ViT to adversarial examples, Mahmood et
    al. [[151](#bib.bib151)] used multiple adversarial attacks in CNNs, namely FGSM,
    PGD, MIM, C&W, and MI-FGSM. The ViT has increased robustness (compared with ResNet)
    for the first four attacks and has no resilience to the C&W and MI-FGSM attacks.
    Additionally, to complement the results obtained from the performance of ViTs,
    an extensive study [[152](#bib.bib152)] using feature maps, attention maps, and
    Gradient-weighted Class Activation Mapping (Grad-CAM) [[153](#bib.bib153)] intends
    to explain this performance visually.
  prefs: []
  type: TYPE_NORMAL
- en: The transferability of adversarial examples from CNNs to ViTs was also evaluated,
    suggesting that the examples from CNNs do not instantly transfer to ViTs [[151](#bib.bib151)].
    Furthermore, Self-Attention blended Gradient Attack (SAGA) [[151](#bib.bib151)]
    was proposed to misclassify both ViTs and CNNs. The Pay No Attention (PNA) [[154](#bib.bib154)]
    attack, which ignores the gradients of attention, and the PatchOut [[154](#bib.bib154)]
    attack, which randomly samples subsets of patches, demonstrate high transferability.
  prefs: []
  type: TYPE_NORMAL
- en: To detect adversarial examples that might affect the ViTs, PatchVeto [[20](#bib.bib20)]
    uses different transformers with different attention masks that output the encoding
    of the class. An image is considered valid if all transformers reach a consensus
    in the voted class, overall the masked predictions (provided by masked transformers).
  prefs: []
  type: TYPE_NORMAL
- en: Smoothed ViTs [[155](#bib.bib155)] perform preprocessing techniques to the images
    before feeding them into the ViT, by generating image ablations (images composed
    of only one column of the original image, and the remaining columns are black),
    which are converted into tokens, and droping the fully masked tokens. The remaining
    tokens are fed into a ViT, which predicts a class for each ablation, and the class
    with the most predictions of overall ablations is considered the correct one.
  prefs: []
  type: TYPE_NORMAL
- en: Bai et al. [[156](#bib.bib156)] demonstrates that ViTs and CNNs are being unfairly
    evaluated because they do not have the same training details. Therefore, this
    work provides a fair and in-depth comparison between ViTs and CNNs, indicating
    that ViTs are as vulnerable to adversarial perturbations as CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Architecture-oriented Transferable Attacking (ATA) [[157](#bib.bib157)] is a
    framework that generates transferable adversarial examples by considering the
    common characteristics among different ViT architectures, such as self-attention
    and image-embedding. Specifically, it discovers the most attentional patch-wise
    regions significantly influencing the model decision and searches pixel-wise attacking
    positions using sensitive embedding perturbation.
  prefs: []
  type: TYPE_NORMAL
- en: Patch-fool [[158](#bib.bib158)] explores the perturbations that turn ViTs more
    vulnerable learners than CNNs, proposing a dedicated attack framework that fools
    the self-attention mechanism by attacking a single patch with multiple attention-aware
    optimization techniques. This attack mechanism demonstrates, for the first time,
    that ViTs can be more vulnerable than CNNs if attacked with proper techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Gu et al. [[159](#bib.bib159)] evaluates the robustness of ViT to patch-wise
    perturbations, concluding that these models are more robust to naturally corrupted
    patches than CNNs while being more vulnerable to adversarially generated ones.
    Inspired by the observed results, the authors propose a simple Temperature Scaling
    based method that improves the robustness of ViTs.
  prefs: []
  type: TYPE_NORMAL
- en: As previously observed for CNNs, improving the robust accuracy sacrifices the
    standard accuracy of ViTs, which may limit their applicability in the real context.
    Derandomized Smoothing [[160](#bib.bib160)] uses a progressive smoothed image
    modeling task to train the ViTs, making them capture the more discriminating local
    context while preserving global semantic information, improving both robust and
    standard accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: VeinGuard [[161](#bib.bib161)] is a defense framework that helps ViTs be more
    robust against adversarial palm-vein image attacks, with practical applicability
    in the real world. Namely, VeinGuard is composed of a local transformer-based
    GAN that learns the distribution of unperturbed vein images and a purifier that
    automatically removes a variety of adversarial perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: VII Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VII-A MNIST and F-MNIST
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most used datasets is the MNIST [[162](#bib.bib162)] dataset, which
    contains images of handwritten digits collected from approximately 250 writers
    in shades of black and white, withdrawn from two different databases. This dataset
    is divided into training and test sets, with the first one containing 60,000 examples
    and a second one containing 10,000 examples.
  prefs: []
  type: TYPE_NORMAL
- en: Xiao et al. propose the creation of the Fashion-MNIST [[163](#bib.bib163)] dataset
    by using figures from a fashion website, which has a total size of 70,000 images,
    contains ten classes, uses greyscale images, and each image has a size of 28x28\.
    The Fashion-MNIST dataset is divided into train and test sets, containing 60,000
    and 10,000 examples, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b1186d35032e37fad611482b29edc594.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Images withdrew from the MNIST dataset [[162](#bib.bib162)] in the
    first five columns and from the Fashion-MNIST dataset [[163](#bib.bib163)] in
    the last five columns. The images were resized for better visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [12](#S7.F12 "Figure 12 ‣ VII-A MNIST and F-MNIST ‣ VII Datasets ‣ How
    Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses") displays
    the 10 digits (from 0 to 9) from the MNIST dataset in the first five columns and
    the 10 fashion objects from Fashion-MNIST dataset in the last five columns. MNIST
    is one of the most widely studied datasets in the earlier works of adversarial
    examples, with defense mechanisms already displaying high robustness on this dataset.
    The same does not apply to Fashion-MNIST, which has not been as widely studied,
    despite having similar characteristics to MNIST.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c1c62caba63b55c35f31c43b2e4b32a9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: Images withdrew from the CIFAR-10 dataset [[164](#bib.bib164)] in
    the first five columns and from the CIFAR-100 dataset [[164](#bib.bib164)] in
    the last five columns. The images were resized for better visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: VII-B CIFAR-10 and CIFAR-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another widely studied dataset is the CIFAR-10, which, in conjunction with the
    CIFAR-100 dataset, are subsets from a vast database containing 80 million tiny
    images [[165](#bib.bib165)], 32x32, and three color channels 75,062 different
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: CIFAR-10 [[164](#bib.bib164)] contains only ten classes from this large database,
    with 6,000 images for each class, distributed into 50,000 training images and
    10,000 test images. This dataset considers different objects, namely, animals
    and vehicles, usually found in different environments.
  prefs: []
  type: TYPE_NORMAL
- en: CIFAR-100 [[164](#bib.bib164)] contains 100 classes with only 600 images for
    each one with the same size and amount of color channels as the CIFAR-10 dataset.
    CIFAR-100 groups its 100 classes into 20 superclasses, located in different contexts/environments,
    making this dataset much harder to achieve high results.
  prefs: []
  type: TYPE_NORMAL
- en: 'Examples from the CIFAR-10 dataset are shown in Fig. [13](#S7.F13 "Figure 13
    ‣ VII-A MNIST and F-MNIST ‣ VII Datasets ‣ How Deep Learning Sees the World: A
    Survey on Adversarial Attacks & Defenses") in the first five columns, and the
    remaining columns display examples of the superclasses from CIFAR-100\. Due to
    the unsatisfactory results demonstrated by models trained on CIFAR-10, the CIFAR-100
    dataset has not been included in most studies under the context of adversarial
    examples, suggesting that solving the issue of adversarial-perturbed images is
    still at its inception.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b92258bb80d5e2b1e0cb05e3f4ac6f7e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Images withdrew from the Street View House Numbers dataset [[166](#bib.bib166)]
    in the first three columns and from the German Traffic Sign Recognition Benchmark
    dataset [[167](#bib.bib167)] in the last three columns. The images were resized
    for better visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d5872d66900528109e7bd8bcacb044cf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Images withdrew from the ImageNet dataset [[168](#bib.bib168)] in
    the top left, from the ImageNet-A dataset [[169](#bib.bib169)] in the top right,
    from the ImageNet-C and ImageNet-P datasets [[170](#bib.bib170)] in the bottom
    left, and ImageNet-COLORDISTORT [[171](#bib.bib171)] in the bottom right. The
    images were resized for better visualization.'
  prefs: []
  type: TYPE_NORMAL
- en: VII-C Street View Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Street View House Numbers (SVHN) [[166](#bib.bib166)] dataset provides
    the same challenge as MNIST: identifying which digits are present in a colored
    image, containing ten classes, 0 to 9 digits, and an image size of 32x32 centered
    around a single character, with multiple digits in a single image. Regarding the
    dataset size, it has 630,420 digit images, but only 73,257 images are used for
    training, 26,032 images are used for testing, and the remaining 531,131 images
    can be used as additional training data.'
  prefs: []
  type: TYPE_NORMAL
- en: German Traffic Sign Recognition Benchmark (GTSRB) [[167](#bib.bib167)] is a
    dataset containing 43 classes of different traffic signs, has 50,000 images, and
    demonstrates realistic scenarios. The dataset has 51,840 images, whose size varies
    from 15x15 to 222x193, divided into training, validation, and test sets with 50%,
    25%, and 25%, respectively, of the total images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The difficulties associated with the SVHN dataset are displayed in the first
    three rows of Fig. [14](#S7.F14 "Figure 14 ‣ VII-B CIFAR-10 and CIFAR-100 ‣ VII
    Datasets ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses"), showing unique digits that occupy the whole image and multiple digits
    on different backgrounds. Furthermore, the same figure presents the different
    types of traffic signs in the GTSRB dataset, such as prohibition, warning, mandatory,
    and end of prohibition.'
  prefs: []
  type: TYPE_NORMAL
- en: VII-D ImageNet and Variants
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: ImageNet [[168](#bib.bib168)] is one of the largest datasets for object recognition,
    containing 1,461,406 colored images and 1,000 classes, with images being resized
    to 224x224\. This dataset collected photographs from Flickr, and other search
    engines, divided into 1.2 million training images, 50,000 validation images, and
    100,000 test images.
  prefs: []
  type: TYPE_NORMAL
- en: A possible alternative to ImageNet, when the dataset size is an important factor,
    is called Tiny ImageNet [[172](#bib.bib172)], a subset of ImageNet that contains
    fewer classes and images. This dataset contains only 200 classes (from the 1,000
    classes in ImageNet), 100,000 training images, 10,000 validation images, and 10,000
    test images. These classes include animals, vehicles, household items, insects,
    and clothing, considering the variety of contexts/environments that these objects
    can be found. Their images have a size of 64x64 and are colored.
  prefs: []
  type: TYPE_NORMAL
- en: ImageNet-A [[169](#bib.bib169)] is a subset of ImageNet, containing only 200
    classes from the 1,000 classes, covering the broadest categories in ImageNet.
    ImageNet-A is a dataset composed of real-world adversarially filtered images,
    which were obtained by deleting the correctly predicted images by ResNet-50 classifiers.
    Despite ImageNet-A being based on the deficiency of ResNet-50, it also demonstrates
    transferability to unseen models, making this dataset suitable for evaluating
    the robustness of multiple classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: Two additional benchmarks, ImageNet-C [[170](#bib.bib170)] and ImageNet-P [[170](#bib.bib170)],
    were designed to evaluate the robustness of DNNs. The ImageNet-C standardizes
    and expands the corruption robustness topic, consisting of 75 corruptions applied
    to each image in the ImageNet validation set. ImageNet-P applies distortions to
    the images, though it differs from ImageNet-C because it contains perturbation
    sequences using only ten common perturbations.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Relevant characteristics to the context of adversarial examples
    of the state-of-the-art datasets. #Classes means the number of classes in the
    dataset. Empty Color column means that the images in that dataset use greyscale
    or black and white shades. Datasets with $*$ are only used for testing purposes.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Size | #Classes | Classes | Color |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST | 70,000 | 10 | Digits |  |'
  prefs: []
  type: TYPE_TB
- en: '| Fashion-MNIST | 70,000 | 10 | Clothing |  |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-10 | 60,000 | 10 | Animals | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Vehicles |'
  prefs: []
  type: TYPE_TB
- en: '| SVHN | 630,420 | 10 | Digits | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| GTSRB | 51,840 | 43 | Traffic Signs | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| CIFAR-100 | 60,000 | 100 | Household Items | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Outdoor Scenes |'
  prefs: []
  type: TYPE_TB
- en: '| Tiny ImageNet | 120,000 | 200 | Animals | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Household Items |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet-A^∗ | 7,500 | 200 | Vehicles | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Food |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet-C^∗ | 3,750,000 | 200 | Vehicles | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Food |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet-P^∗ | 15,000,000 | 200 | Vehicles | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Food |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet | 1,431,167 | 1,000 | Vehicles | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Electronic devices |'
  prefs: []
  type: TYPE_TB
- en: '| ImageNet-CD^∗ | 736,515 | 1,000 | Vehicles | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Electronic devices |'
  prefs: []
  type: TYPE_TB
- en: Another benchmark to evaluate the model generalization capability is the ImageNet-COLORDISTORT
    (ImageNet-CD) [[171](#bib.bib171)], which considers multiple distortions in the
    color of an image using different color space representations. This dataset contains
    the 1,000 classes from ImageNet, removing images without color channels, and the
    same image considers multiple color distortions under the Red Green Blue (RGB),
    Hue-Saturation-Value (HSV), CIELAB, and YCbCr color spaces considered common transformations
    used in image processing.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to observe a set of images withdrawn from ImageNet in the top
    left of Fig. [15](#S7.F15 "Figure 15 ‣ VII-B CIFAR-10 and CIFAR-100 ‣ VII Datasets
    ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses").
    Additionally, some images misclassified by multiple classifiers (ImageNet-A) are
    shown in the top right of the same figure. The bottom represents the ImageNet
    with common corruptions and perturbations and is manipulated by multiple image
    techniques on the left and right, respectively. Table [III](#S7.T3 "TABLE III
    ‣ VII-D ImageNet and Variants ‣ VII Datasets ‣ How Deep Learning Sees the World:
    A Survey on Adversarial Attacks & Defenses") summarizes the main characteristics
    of the datasets presented throughout this section.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE IV: Accuracy comparison of different defense mechanisms on CIFAR-10 under
    PGD attack, $l_{\infty}$ and $\epsilon=8/255$. Clean and Robust refers to accuracy
    Without and With Adversarial Attacks, respectively. Defenses with “-” on clean
    accuracy do not have a clean accuracy reported.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Defense Method | Year | Architecture | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Clean | Robust |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| BPFC [[89](#bib.bib89)] | 2020 | ResNet-18 | 82.4 | 34.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SNS [[94](#bib.bib94)] | 2021 | VGG-16 | 86.0 | 39.6 |'
  prefs: []
  type: TYPE_TB
- en: '| AT-MIFGSM [[51](#bib.bib51)] | 2017 | Inception v3 | 85.3 | 45.9 |'
  prefs: []
  type: TYPE_TB
- en: '| AT-PGD [[36](#bib.bib36)] | 2018 | ResNet-18 | 87.3 | 47.0 |'
  prefs: []
  type: TYPE_TB
- en: '| RobNets [[118](#bib.bib118)] | 2020 | RobNet-free | 82.8 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '| HGD [[17](#bib.bib17)] | 2018 | DUNET | 92.4 | 53.1 |'
  prefs: []
  type: TYPE_TB
- en: '| RSLAD [[93](#bib.bib93)] | 2021 | ResNet-18 | 83.4 | 54.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MART [[61](#bib.bib61)] | 2020 | WRN-28-10 | 83.1 | 55.6 |'
  prefs: []
  type: TYPE_TB
- en: '| TRADES [[54](#bib.bib54)] | 2019 | WRN-34-10 | 84.9 | 56.4 |'
  prefs: []
  type: TYPE_TB
- en: '| BagT [[133](#bib.bib133)] | 2020 | WRN-34-10 | - | 56.4 |'
  prefs: []
  type: TYPE_TB
- en: '| RO [[132](#bib.bib132)] | 2020 | ResNet-18 | - | 56.8 |'
  prefs: []
  type: TYPE_TB
- en: '| DOA [[62](#bib.bib62)] | 2019 | VGGFace | 93.6 | 61.0 |'
  prefs: []
  type: TYPE_TB
- en: '| AWP [[90](#bib.bib90)] | 2020 | WRN-28-10 | - | 63.6 |'
  prefs: []
  type: TYPE_TB
- en: '| FS [[19](#bib.bib19)] | 2019 | WRN-28-10 | 90.0 | 68.6 |'
  prefs: []
  type: TYPE_TB
- en: '| CAFD [[112](#bib.bib112)] | 2021 | DUNET | 91.1 | 87.2 |'
  prefs: []
  type: TYPE_TB
- en: VIII Metrics and State-of-the-art Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: VIII-A Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Due to the nature of adversarial examples, they need specific metrics to be
    correctly evaluated and constructed. Following this direction, multiple works
    have been proposing different metrics that calculate the percentage of adversarial
    examples that make a model misclassify (fooling rate), measure the amount of perturbation
    made in an image (destruction rate), and calculate the model robustness to adversarial
    examples (average robustness).
  prefs: []
  type: TYPE_NORMAL
- en: VIII-A1 Accuracy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'This metric measures the number of samples that are correctly predicted by
    the model, which is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\text{accuracy}=\frac{TP+TN}{TP+TN+FP+FN},$ |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $TP$ refers to True Positive, $TN$ to True Negative, $FP$ to False Positive,
    and $FN$ to False Negative. The True Positive and True Negative are the samples
    whose network prediction is the same as the label (correct), and the False Positive
    and False Negative are the samples whose network prediction differs from the label
    (incorrect). When considering original images, this metric is denominated as Clean
    Accuracy and, when using adversarial images, is named as Robust Accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-A2 Fooling Rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: After being perturbed to change the classifier label, the fooling rate $FR$ [[173](#bib.bib173)]
    was proposed to calculate the percentage of images.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-A3 Average Robustness
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To objectively evaluate the robustness to adversarial perturbations of a classifier
    $f$, the average robustness $\hat{p}_{\text{adv}}(f)$ is defined as [[21](#bib.bib21)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{p}_{\text{adv}}(f)=\frac{1}{\mathcal{D}}\sum_{x\in\mathcal{D}}\frac{\&#124;\hat{r}(x)\&#124;_{2}}{\&#124;x\&#124;_{2}},$
    |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $\hat{r}(x)$ is the estimated minimal perturbation obtained using the
    attack, and $\mathcal{D}$ denotes the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE V: Accuracy comparison of different defense mechanisms on CIFAR-10 under
    Auto-Attack attack, $l_{\infty}$ and $\epsilon=8/255$ . Clean and Robust refers
    to accuracy Without and With Adversarial Attacks, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Defense Method | Year | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Clean | Robust |'
  prefs: []
  type: TYPE_TB
- en: '| WRN28-10 | Input Random [[119](#bib.bib119)] | 2017 | 94.3 | 8.6 |'
  prefs: []
  type: TYPE_TB
- en: '| BAT [[57](#bib.bib57)] | 2019 | 92.8 | 29.4 |'
  prefs: []
  type: TYPE_TB
- en: '| FS [[19](#bib.bib19)] | 2019 | 90.0 | 36.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 83.9 | 50.7 |'
  prefs: []
  type: TYPE_TB
- en: '| Pretrain [[174](#bib.bib174)] | 2019 | 87.1 | 54.9 |'
  prefs: []
  type: TYPE_TB
- en: '| UAT [[84](#bib.bib84)] | 2019 | 86.5 | 56.0 |'
  prefs: []
  type: TYPE_TB
- en: '| MART [[61](#bib.bib61)] | 2020 | 87.5 | 56.3 |'
  prefs: []
  type: TYPE_TB
- en: '| HYDRA [[92](#bib.bib92)] | 2020 | 89.0 | 57.1 |'
  prefs: []
  type: TYPE_TB
- en: '| RST [[85](#bib.bib85)] | 2019 | 89.7 | 59.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GI-AT [[68](#bib.bib68)] | 2020 | 89.4 | 59.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Proxy [[140](#bib.bib140)] | 2021 | 89.5 | 59.7 |'
  prefs: []
  type: TYPE_TB
- en: '| AWP [[90](#bib.bib90)] | 2020 | 88.3 | 60.0 |'
  prefs: []
  type: TYPE_TB
- en: '| FDA [[134](#bib.bib134)] | 2021 | 87.3 | 60.8 |'
  prefs: []
  type: TYPE_TB
- en: '| HAT [[69](#bib.bib69)] | 2021 | 88.2 | 61.0 |'
  prefs: []
  type: TYPE_TB
- en: '| SCORE [[103](#bib.bib103)] | 2022 | 88.6 | 61.0 |'
  prefs: []
  type: TYPE_TB
- en: '| PSSiLU [[104](#bib.bib104)] | 2022 | 87.0 | 61.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Gowal et al. [[135](#bib.bib135)] | 2020 | 89.5 | 62.8 |'
  prefs: []
  type: TYPE_TB
- en: '| IRUGD [[138](#bib.bib138)] | 2021 | 87.5 | 63.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[146](#bib.bib146)] | 2023 | 92.4 | 67.3 |'
  prefs: []
  type: TYPE_TB
- en: '| STL [[121](#bib.bib121)] | 2019 | 82.2 | 67.9 |'
  prefs: []
  type: TYPE_TB
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 89.3 | 85.6 |'
  prefs: []
  type: TYPE_TB
- en: '| WRN34-10 | Free-AT [[58](#bib.bib58)] | 2019 | 86.1 | 41.5 |'
  prefs: []
  type: TYPE_TB
- en: '| AT-PGD [[36](#bib.bib36)] | 2018 | 87.1 | 44.0 |'
  prefs: []
  type: TYPE_TB
- en: '| YOPO [[60](#bib.bib60)] | 2019 | 87.2 | 44.8 |'
  prefs: []
  type: TYPE_TB
- en: '| TLA [[81](#bib.bib81)] | 2019 | 86.2 | 47.4 |'
  prefs: []
  type: TYPE_TB
- en: '| LAT [[82](#bib.bib82)] | 2019 | 87.8 | 49.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SAT [[63](#bib.bib63)] | 2020 | 86.8 | 50.7 |'
  prefs: []
  type: TYPE_TB
- en: '| FAT [[70](#bib.bib70)] | 2022 | 85.3 | 51.1 |'
  prefs: []
  type: TYPE_TB
- en: '| LBGAT [[97](#bib.bib97)] | 2021 | 88.2 | 52.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TRADES [[54](#bib.bib54)] | 2019 | 84.9 | 53.1 |'
  prefs: []
  type: TYPE_TB
- en: '| SAT [[91](#bib.bib91)] | 2020 | 83.5 | 53.3 |'
  prefs: []
  type: TYPE_TB
- en: '| Friend-AT [[64](#bib.bib64)] | 2020 | 84.5 | 55.5 |'
  prefs: []
  type: TYPE_TB
- en: '| AWP [[90](#bib.bib90)] | 2020 | 85.4 | 56.2 |'
  prefs: []
  type: TYPE_TB
- en: '| LTD [[98](#bib.bib98)] | 2021 | 85.2 | 56.9 |'
  prefs: []
  type: TYPE_TB
- en: '| OA-AT [[66](#bib.bib66)] | 2021 | 85.3 | 58.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Proxy [[140](#bib.bib140)] | 2022 | 86.7 | 60.3 |'
  prefs: []
  type: TYPE_TB
- en: '| HAT [[69](#bib.bib69)] | 2021 | 91.5 | 62.8 |'
  prefs: []
  type: TYPE_TB
- en: '| WRN-70-16 | SCORE [[103](#bib.bib103)] | 2022 | 89.0 | 63.4 |'
  prefs: []
  type: TYPE_TB
- en: '| IRUGD [[138](#bib.bib138)] | 2021 | 91.1 | 65.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Gowal et al. [[135](#bib.bib135)] | 2020 | 88.7 | 66.1 |'
  prefs: []
  type: TYPE_TB
- en: '| FDA [[134](#bib.bib134)] | 2021 | 92.2 | 66.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[146](#bib.bib146)] | 2023 | 93.3 | 70.7 |'
  prefs: []
  type: TYPE_TB
- en: '| SODEF [[102](#bib.bib102)] | 2021 | 93.7 | 71.3 |'
  prefs: []
  type: TYPE_TB
- en: VIII-A4 Destruction Rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To evaluate the impact of arbitrary transformations on adversarial images,
    the notion of destruction rate $d$ is introduced and formally defined as [[33](#bib.bib33)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $d=\frac{\sum^{n}_{k=1}C(\textbf{X}^{k},y^{k}_{\text{true}})\neg C(\textbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})C(T(\textbf{X}^{k}_{\text{adv}}),y^{k}_{\text{true}})}{\sum^{n}_{k=1}(\textbf{X}^{k},y^{k}_{\text{true}})C(\textbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})},$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: 'where $n$ is the number of images, $\mathbf{X}^{k}$ is the original image from
    the dataset, $y^{k}_{\text{true}}$ is the true class of this image, $\mathbf{X}^{k}_{\text{adv}}$
    is the adversarial image corresponding to that image, and $T$ is an arbitrary
    image transformation. $\neg C(\mathbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})$
    is defined as the binary negation of $C(\mathbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})$.
    Finally, the function $C(\mathbf{X},y)$ is defined as [[33](#bib.bib33)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $C(\mathbf{X},y)=\begin{cases}1,&amp;\textrm{if image }\textbf{X}\textrm{
    is classified as }y;\\ 0,&amp;\textrm{otherwise}.\end{cases}$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: VIII-B Defense Mechanisms Robustness
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The metric used to evaluate models is accuracy, which evaluates the results
    on both original (Clean Accuracy) and adversarially perturbed (Robust Accuracy)
    datasets. One of the earliest and strongest adversarial attacks proposed was PGD,
    which was used by multiple defenses to evaluate their robustness. Table [IV](#S7.T4
    "TABLE IV ‣ VII-D ImageNet and Variants ‣ VII Datasets ‣ How Deep Learning Sees
    the World: A Survey on Adversarial Attacks & Defenses") displays defenses evaluated
    on CIFAR-10 under multiple steps PGD attack, ordered by increasing robustness.
    For the PGD attack, the best performing defenses are from approaches that use
    supplementary networks (CAFD) or modify the training process (FS and AWP). Overall,
    Wide ResNets [[175](#bib.bib175)] have better robust accuracy, due to high-capacity
    networks exhibiting greater adversarial robustness [[51](#bib.bib51), [36](#bib.bib36)],
    suggesting the usage of these networks in future developments of adversarial attacks
    and defenses.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To assess the robustness of defenses for white and black-box settings, Auto-Attack
    has gained increased interest over PGD in recent works. Tables [V](#S8.T5 "TABLE
    V ‣ VIII-A3 Average Robustness ‣ VIII-A Evaluation Metrics ‣ VIII Metrics and
    State-of-the-art Results ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses"), [VI](#S8.T6 "TABLE VI ‣ VIII-B Defense Mechanisms Robustness
    ‣ VIII Metrics and State-of-the-art Results ‣ How Deep Learning Sees the World:
    A Survey on Adversarial Attacks & Defenses"), and [VII](#S9.T7 "TABLE VII ‣ IX
    Future Directions ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") present a set of defenses that are evaluated under Auto-Attack,
    on CIFAR-10, CIFAR-100, and ImageNet, respectively, ordered by increasing Robust
    Accuracy. The most used networks are Wide ResNets with different sizes, with the
    biggest Wide ResNet displaying better results overall, and the most resilient
    defense derives from the use of supplementary networks (DISCO), followed by modifying
    the train process (SODEF) and changing network architecture (STL). The results
    suggest that the inclusion of additional components to sanitize inputs of the
    targeted model (use of supplementary networks) is the most resilient approach
    for model robustness in white and black-box settings. The updated results for
    defenses under Auto-Attack can be found on the RobustBench [[176](#bib.bib176)]
    website.'
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VI: Accuracy comparison of different defense mechanisms on CIFAR-100
    under Auto-Attack attack, $l_{\infty}$ and $\epsilon=8/255$ . Clean and Robust
    refers to accuracy Without and With Adversarial Attacks, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Defense Method | Year | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Clean | Robust |'
  prefs: []
  type: TYPE_TB
- en: '| WRN28-10 | Input Random [[119](#bib.bib119)] | 2017 | 73.6 | 3.3 |'
  prefs: []
  type: TYPE_TB
- en: '| LIIF [[115](#bib.bib115)] | 2021 | 80.3 | 3.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Bit Reduction [[107](#bib.bib107)] | 2017 | 76.9 | 3.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Pretrain [[174](#bib.bib174)] | 2019 | 59.2 | 28.4 |'
  prefs: []
  type: TYPE_TB
- en: '| SCORE [[103](#bib.bib103)] | 2022 | 63.7 | 31.1 |'
  prefs: []
  type: TYPE_TB
- en: '| FDA [[134](#bib.bib134)] | 2021 | 62.4 | 32.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[146](#bib.bib146)] | 2023 | 78.6 | 38.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 61.9 | 39.6 |'
  prefs: []
  type: TYPE_TB
- en: '| STL [[121](#bib.bib121)] | 2019 | 67.4 | 46.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 72.1 | 67.9 |'
  prefs: []
  type: TYPE_TB
- en: '| WRN34-10 | SAT [[63](#bib.bib63)] | 2020 | 62.8 | 24.6 |'
  prefs: []
  type: TYPE_TB
- en: '| AWP [[90](#bib.bib90)] | 2020 | 60.4 | 28.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LBGAT [[97](#bib.bib97)] | 2021 | 60.6 | 29.3 |'
  prefs: []
  type: TYPE_TB
- en: '| OA-AT [[66](#bib.bib66)] | 2021 | 65.7 | 30.4 |'
  prefs: []
  type: TYPE_TB
- en: '| LTD [[98](#bib.bib98)] | 2021 | 64.1 | 30.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Proxy [[140](#bib.bib140)] | 2022 | 65.9 | 31.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 71.6 | 69.0 |'
  prefs: []
  type: TYPE_TB
- en: '| WRN-70-16 | SCORE [[103](#bib.bib103)] | 2022 | 65.6 | 33.1 |'
  prefs: []
  type: TYPE_TB
- en: '| FDA [[134](#bib.bib134)] | 2021 | 63.6 | 34.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Gowal et al. [[135](#bib.bib135)] | 2020 | 69.2 | 36.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Wang et al. [[146](#bib.bib146)] | 2023 | 75.2 | 42.7 |'
  prefs: []
  type: TYPE_TB
- en: IX Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Following the de facto standards adopted by the literature, we suggest that
    future proposals of defense mechanisms should be evaluated on Auto-Attack, using
    the robust accuracy as a metric for comparison purposes. The adversarial defense
    that demonstrates better results is Adversarial Training, which should be a requirement
    when evaluating attacks and defenses.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Accuracy comparison of different defense mechanisms on ImageNet
    under Auto-Attack attack, $l_{\infty}$ and $\epsilon=4/255$. Clean and Robust
    refers to accuracy Without and With Adversarial Attacks, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Defense Method | Year | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Clean | Robust |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-18 | Bit Reduction [[107](#bib.bib107)] | 2017 | 67.6 | 4.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 67.2 | 13.1 |'
  prefs: []
  type: TYPE_TB
- en: '| Input Random [[119](#bib.bib119)] | 2017 | 64.0 | 17.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Salman et al. [[177](#bib.bib177)] | 2020 | 52.9 | 25.3 |'
  prefs: []
  type: TYPE_TB
- en: '| STL [[121](#bib.bib121)] | 2019 | 65.6 | 32.9 |'
  prefs: []
  type: TYPE_TB
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 68.0 | 60.9 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-50 | Bit Reduction [[107](#bib.bib107)] | 2017 | 73.8 | 1.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Input Random [[119](#bib.bib119)] | 2017 | 74.0 | 18.8 |'
  prefs: []
  type: TYPE_TB
- en: '| Cheap-AT [[65](#bib.bib65)] | 2020 | 55.6 | 26.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 73.6 | 33.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Salman et al. [[177](#bib.bib177)] | 2020 | 64.0 | 35.0 |'
  prefs: []
  type: TYPE_TB
- en: '| STL [[121](#bib.bib121)] | 2019 | 68.3 | 50.2 |'
  prefs: []
  type: TYPE_TB
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 72.6 | 68.2 |'
  prefs: []
  type: TYPE_TB
- en: '| WRN-50-2 | Bit Reduction [[107](#bib.bib107)] | 2017 | 75.1 | 5.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Input Random [[119](#bib.bib119)] | 2017 | 71.7 | 23.6 |'
  prefs: []
  type: TYPE_TB
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 75.4 | 24.9 |'
  prefs: []
  type: TYPE_TB
- en: '| Salman et al. [[177](#bib.bib177)] | 2020 | 68.5 | 38.1 |'
  prefs: []
  type: TYPE_TB
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 75.1 | 69.5 |'
  prefs: []
  type: TYPE_TB
- en: 'The state-of-the-art results show that MNIST and CIFAR-10 datasets are already
    saturated. Other datasets should be further evaluated, namely: 1) CIFAR-100 and
    ImageNet since adversarial defenses do not achieve state-of-the-art clean accuracy
    (91% and 95%, respectively); 2) GTSRB and SVHN, depicting harder scenarios with
    greater variations of background, inclination, and luminosity; and 3) Fashion-MNIST
    that would allow better comprehension of which image properties influence DNNs
    performance (e.g., type of task, image shades, number of classes).'
  prefs: []
  type: TYPE_NORMAL
- en: Most works present their results using accuracy as the evaluation metric and,
    more recently, evaluate their defenses on the Auto-Attack. Furthermore, the values
    given for $\epsilon$ in each dataset were standardized by recurrent use. However,
    there should be an effort to develop a metric/process that quantifies the amount
    of perturbation added to the original image. This would ease the expansion of
    adversarial attacks to other datasets that do not have a standardized $\epsilon$
    value.
  prefs: []
  type: TYPE_NORMAL
- en: There has been a greater focus on the development of white-box attacks, which
    consider that the adversary has access to the network and training data, yet this
    is not feasible in real contexts, translating into the need of focusing more on
    the development of black-box attacks. A unique black-box set, physical attacks,
    also require additional evaluation, considering the properties of the real world
    and perturbations commonly found in it. Considering the increasing liberation
    of ML in the real world, end-users can partially control the training phase of
    DNNs, suggesting that gray-box attacks will intensify (access only to network
    or data).
  prefs: []
  type: TYPE_NORMAL
- en: The different network architectures are designed to increase the clean accuracy
    of DNNs in particular object recognition datasets, yet there should be further
    evaluation on the impact of the different layers and their structure. ViTs introduce
    a new paradigm in image analysis and are more robust against natural corruptions,
    suggesting that building ViT inherently robust to adversarial examples might be
    a possible solution.
  prefs: []
  type: TYPE_NORMAL
- en: DDPM are generative models that perform adversarial purification of images,
    but they can not be applied in real-time since they take up to dozens of seconds
    to create a single purified image. Therefore, an effort on developing close to
    real-time adversarial purification strategies is a viable strategy for future
    works.
  prefs: []
  type: TYPE_NORMAL
- en: X Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DNNs are vulnerable to a set of inputs, denominated as adversarial examples,
    that drastically modify the output of the considered network and are constructed
    by adding a perturbation to the original image. This survey presents background
    concepts, such as adversary capacity and vector norms, essential to comprehend
    adversarial settings, providing a comparison with existing surveys in the literature.
    Adversarial attacks are organized based on the adversary knowledge, highlighting
    the emphasis of current works toward white box settings, and adversarial defenses
    are clustered into six domains, with most works exploring the adversarial training
    strategy. We also present the latest developments of adversarial settings in ViTs
    and describe the commonly used datasets, providing the state-of-the-art results
    in CIFAR-10, CIFAR-100, and ImageNet. Finally, we propose a set of open issues
    that can be explored for subsequent future works.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported in part by the Portuguese FCT/MCTES through National
    Funds and co-funded by EU funds under Project UIDB/50008/2020; in part by the
    FCT Doctoral Grant 2020.09847.BD and Grant 2021.04905.BD;
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” IJCV, vol. 128, no. 2,
    pp. 261–318, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] H.-B. Zhang, Y.-X. Zhang, B. Zhong, Q. Lei, L. Yang, J.-X. Du, and D.-S.
    Chen, “A comprehensive survey of vision-based human action recognition methods,”
    Sensors, vol. 19, no. 5, p. 1005, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] I. Masi, Y. Wu, T. Hassner, and P. Natarajan, “Deep face recognition: A
    survey,” in 2018 31st SIBGRAPI, pp. 471–478, IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] M. Wang and W. Deng, “Deep face recognition: A survey,” Neurocomputing,
    vol. 429, pp. 215–244, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
    T. Rault, R. Louf, M. Funtowicz, et al., “Transformers: State-of-the-art natural
    language processing,” in Proceedings of the 2020 conference on empirical methods
    in natural language processing: system demonstrations, pp. 38–45, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. W. Otter, J. R. Medina, and J. K. Kalita, “A survey of the usages of
    deep learning for natural language processing,” IEEE Transactions on Neural Networks
    and Learning Systems, vol. 32, no. 2, pp. 604–624, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] A. I. Maqueda, A. Loquercio, G. Gallego, N. García, and D. Scaramuzza,
    “Event-based vision meets deep learning on steering prediction for self-driving
    cars,” in Proceedings of the IEEE Conference on CVPR, June 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Ndikumana, N. H. Tran, D. H. Kim, K. T. Kim, and C. S. Hong, “Deep learning
    based caching for self-driving cars in multi-access edge computing,” IEEE Transactions
    on Intelligent Transportation Systems, vol. 22, no. 5, pp. 2862–2877, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Z. Yuan, Y. Lu, Z. Wang, and Y. Xue, “Droid-sec: Deep learning in android
    malware detection,” in Proceedings of the 2014 ACM Conference on SIGCOMM, SIGCOMM
    ’14, (New York, NY, USA), p. 371–372, Association for Computing Machinery, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] R. Vinayakumar, M. Alazab, K. P. Soman, P. Poornachandran, and S. Venkatraman,
    “Robust intelligent malware detection using deep learning,” IEEE Access, vol. 7,
    pp. 46717–46738, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] X. Zhou, W. Liang, I. Kevin, K. Wang, H. Wang, L. T. Yang, and Q. Jin,
    “Deep-learning-enhanced human activity recognition for internet of healthcare
    things,” IEEE Internet of Things Journal, vol. 7, no. 7, pp. 6429–6438, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Z. Liang, G. Zhang, J. X. Huang, and Q. V. Hu, “Deep learning for healthcare
    decision making with emrs,” in 2014 IEEE International Conference on BIBM, pp. 556–559,
    IEEE, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” ArXiv, vol. abs/1312.6199,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami,
    “The limitations of deep learning in adversarial settings,” in 2016 IEEE EuroS&P,
    pp. 372–387, IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Google, “Vertex ai pricing,” 2022. [Online] Accessed on 10th May 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] F. Liao, M. Liang, Y. Dong, T. Pang, J. Zhu, and X. Hu, “Defense against
    adversarial attacks using high-level representation guided denoiser,” 2018 IEEE/CVF
    Conference on CVPR, pp. 1778–1787, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] P. Samangouei, M. Kabkab, and R. Chellappa, “Defense-gan: Protecting classifiers
    against adversarial attacks using generative models,” in International Conference
    on Learning Representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] H. Zhang and J. Wang, “Defense against adversarial attacks using feature
    scattering-based adversarial training,” in NeurIPS, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Y. Huang and Y. Li, “Zero-shot certified defense against adversarial patches
    with vision transformers,” ArXiv, vol. abs/2111.10481, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: A simple
    and accurate method to fool deep neural networks,” 2016 IEEE Conference on CVPR,
    pp. 2574–2582, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Dabouei, S. Soleymani, F. Taherkhani, J. M. Dawson, and N. M. Nasrabadi,
    “Smoothfool: An efficient framework for computing smooth adversarial perturbations,”
    2020 IEEE WACV, pp. 2654–2663, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep learning
    in computer vision: A survey,” IEEE Access, vol. 6, pp. 14410–14430, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Q. Liu, P. Li, W. Zhao, W. Cai, S. Yu, and V. C. M. Leung, “A survey on
    security threats and defensive techniques of machine learning: A data driven view,”
    IEEE Access, vol. 6, pp. 12103–12117, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] A. Serban, E. Poll, and J. Visser, “Adversarial examples on object recognition:
    A comprehensive survey,” ACM Computing Surveys, vol. 53, no. 3, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Qiu, Q. Liu, S. Zhou, and C. Wu, “Review of artificial intelligence
    adversarial attack and defense technologies,” Applied Sciences, vol. 9, no. 5,
    p. 909, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] H. Xu, Y. Ma, H.-C. Liu, D. Deb, H. Liu, J.-L. Tang, and A. K. Jain, “Adversarial
    attacks and defenses in images, graphs and text: A review,” International Journal
    of Automation and Computing, vol. 17, pp. 151–178, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay, and D. Mukhopadhyay,
    “A survey on adversarial attacks and defences,” CAAI Transactions on Intelligence
    Technology, vol. 6, no. 1, pp. 25–45, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] T. Long, Q. Gao, L. Xu, and Z. Zhou, “A survey on adversarial attacks
    in computer vision: Taxonomy, visualization and future directions,” Computers
    & Security, p. 102847, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] H. Liang, E. He, Y. Zhao, Z. Jia, and H. Li, “Adversarial attack and defense:
    A survey,” Electronics, vol. 11, no. 8, p. 1283, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] S. Zhou, C. Liu, D. Ye, T. Zhu, W. Zhou, and P. S. Yu, “Adversarial attacks
    and defenses in deep learning: From a perspective of cybersecurity,” ACM Computing
    Surveys, vol. 55, no. 8, pp. 1–39, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” ArXiv, vol. abs/1412.6572, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples in
    the physical world,” ArXiv, vol. abs/1607.02533, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
    networks,” in 2017 ieee symposium on sp, pp. 39–57, Ieee, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] F. Tramèr, N. Papernot, I. J. Goodfellow, D. Boneh, and P. Mcdaniel, “The
    space of transferable adversarial examples,” ArXiv, vol. abs/1704.03453, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep
    learning models resistant to adversarial attacks,” ArXiv, vol. abs/1706.06083,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song, “Generating adversarial
    examples with adversarial networks,” in Proceedings of the 27th International
    Joint Conference on Artificial Intelligence, pp. 3905–3911, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. C. Courville, and Y. Bengio, “Generative adversarial nets,” in NIPS,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li, “Boosting
    adversarial attacks with momentum,” 2018 IEEE/CVF Conference on CVPR, pp. 9185–9193,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] F. Croce and M. Hein, “Sparse and imperceivable adversarial attacks,”
    2019 IEEE/CVF ICCV, pp. 4723–4731, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] R. Duan, X. Ma, Y. Wang, J. Bailey, A. K. Qin, and Y. Yang, “Adversarial
    camouflage: Hiding physical-world attacks with natural styles,” 2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 997–1005, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Z. Wang, H. Guo, Z. Zhang, W. Liu, Z. Qin, and K. Ren, “Feature importance-aware
    transferable adversarial attacks,” in Proceedings of the IEEE/CVF ICCV, pp. 7639–7648,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Z. Yuan, J. Zhang, Y. Jia, C. Tan, T. Xue, and S. Shan, “Meta gradient
    adversarial attack,” in Proceedings of the IEEE/CVF ICCV, pp. 7748–7757, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal
    adversarial perturbations,” 2017 IEEE Conference on CVPR, pp. 86–94, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Hayes and G. Danezis, “Learning universal adversarial perturbations
    with generative models,” 2018 IEEE SPW, pp. 43–49, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin, “Black-box adversarial
    attacks with limited queries and information,” in International Conference on
    Machine Learning, pp. 2137–2146, PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] M. Wicker, X. Huang, and M. Kwiatkowska, “Feature-guided black-box safety
    testing of deep neural networks,” in TACAS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] M. Andriushchenko, F. Croce, N. Flammarion, and M. Hein, “Square attack:
    a query-efficient black-box adversarial attack via random search,” in Computer
    Vision – ECCV 2020, pp. 484–501, Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] F. Croce and M. Hein, “Reliable evaluation of adversarial robustness with
    an ensemble of diverse parameter-free attacks,” in International conference on
    machine learning, pp. 2206–2216, PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] F. Croce and M. Hein, “Minimally distorted adversarial examples with a
    fast adaptive boundary attack,” in International Conference on Machine Learning,
    pp. 2196–2205, PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial machine learning
    at scale,” ArXiv, vol. abs/1611.01236, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel,
    “Ensemble adversarial training: Attacks and defenses,” in International Conference
    on Learning Representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] C. K. Mummadi, T. Brox, and J. H. Metzen, “Defending against universal
    perturbations with shared adversarial training,” 2019 IEEE/CVF ICCV, pp. 4927–4936,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan, “Theoretically
    principled trade-off between robustness and accuracy,” in International conference
    on machine learning, pp. 7472–7482, PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry, “Robustness
    may be at odds with accuracy,” in International Conference on Learning Representations,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] D. Su, H. Zhang, H. Chen, J. Yi, P.-Y. Chen, and Y. Gao, “Is robustness
    the cost of accuracy?–a comprehensive study on the robustness of 18 deep image
    classification models,” in Proceedings of the ECCV, pp. 631–648, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] J. Wang and H. Zhang, “Bilateral adversarial training: Towards fast training
    of more robust models against adversarial attacks,” in Proceedings of the IEEE/CVF
    ICCV, pp. 6629–6638, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L. S.
    Davis, G. Taylor, and T. Goldstein, “Adversarial training for free!,” Advances
    in Neural Information Processing Systems, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] R. E. Kopp, “Pontryagin maximum principle,” in Mathematics in Science
    and Engineering, vol. 5, pp. 255–279, Elsevier, 1962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] D. Zhang, T. Zhang, Y. Lu, Z. Zhu, and B. Dong, “You only propagate once:
    Accelerating adversarial training via maximal principle,” Advances in Neural Information
    Processing Systems, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Y. Wang, D. Zou, J. Yi, J. Bailey, X. Ma, and Q. Gu, “Improving adversarial
    robustness requires revisiting misclassified examples,” in International Conference
    on Learning Representations, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] T. Wu, L. Tong, and Y. Vorobeychik, “Defending against physically realizable
    attacks on image classification,” arXiv preprint arXiv:1909.09552, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] C. Sitawarin, S. Chakraborty, and D. Wagner, “Improving adversarial robustness
    through progressive hardening,” arXiv preprint arXiv:2003.09347, vol. 4, no. 5,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] J. Zhang, X. Xu, B. Han, G. Niu, L. Cui, M. Sugiyama, and M. Kankanhalli,
    “Attacks which do not kill training make adversarial learning stronger,” in International
    conference on machine learning, pp. 11278–11287, PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] E. Wong, L. Rice, and J. Z. Kolter, “Fast is better than free: Revisiting
    adversarial training,” arXiv preprint arXiv:2001.03994, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. Addepalli, S. Jain, G. Sriramanan, S. Khare, and V. B. Radhakrishnan,
    “Towards achieving adversarial robustness beyond perceptual limits,” in ICML 2021
    Workshop on Adversarial Machine Learning, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in Proceedings of the
    IEEE conference on CVPR, pp. 586–595, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Zhang, J. Zhu, G. Niu, B. Han, M. Sugiyama, and M. Kankanhalli, “Geometry-aware
    instance-reweighted adversarial training,” arXiv preprint arXiv:2010.01736, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] R. Rade and S.-M. Moosavi-Dezfooli, “Helper-based adversarial training:
    Reducing excessive margin to achieve a better accuracy vs. robustness trade-off,”
    in ICML 2021 Workshop on Adversarial Machine Learning, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] J. Chen, Y. Cheng, Z. Gan, Q. Gu, and J. Liu, “Efficient robust training
    via backward smoothing,” in Proceedings of the AAAI Conference on Artificial Intelligence,
    vol. 36, pp. 6222–6230, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] S. S. Gu and L. Rigazio, “Towards deep neural network architectures robust
    to adversarial examples,” ArXiv, vol. abs/1412.5068, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] N. Papernot, P. Mcdaniel, X. Wu, S. Jha, and A. Swami, “Distillation as
    a defense to adversarial perturbations against deep neural networks,” 2016 IEEE
    Symposium on SP, pp. 582–597, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] N. Papernot and P. Mcdaniel, “Extending defensive distillation,” ArXiv,
    vol. abs/1705.05264, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] K. Chalupka, P. Perona, and F. Eberhardt, “Visual causal feature learning,”
    in UAI, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] R. Huang, B. Xu, D. Schuurmans, and C. Szepesvari, “Learning with a strong
    adversary,” ArXiv, vol. abs/1511.03034, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] S. Zheng, Y. Song, T. Leung, and I. J. Goodfellow, “Improving the robustness
    of deep neural networks via stability training,” 2016 IEEE Conference on CVPR,
    pp. 4480–4488, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] V. Zantedeschi, M.-I. Nicolae, and A. Rawat, “Efficient defenses against
    adversarial attacks,” Proceedings of the 10th ACM Workshop on Artificial Intelligence
    and Security, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] A. F. Agarap, “Deep learning using rectified linear units (relu),” ArXiv,
    vol. abs/1803.08375, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] R. H. Hahnloser, R. Sarpeshkar, M. A. Mahowald, R. J. Douglas, and H. S.
    Seung, “Digital selection and analogue amplification coexist in a cortex-inspired
    silicon circuit,” Nature, vol. 405, no. 6789, pp. 947–951, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] S. S. Liew, M. Khalil-Hani, and R. Bakhteri, “Bounded activation functions
    for enhanced training stability of deep neural networks on visual pattern recognition
    problems,” Neurocomputing, vol. 216, pp. 718–734, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] C. Mao, Z. Zhong, J. Yang, C. Vondrick, and B. Ray, “Metric learning for
    adversarial robustness,” Advances in Neural Information Processing Systems, vol. 32,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] N. Kumari, M. Singh, A. Sinha, H. Machiraju, B. Krishnamurthy, and V. N.
    Balasubramanian, “Harnessing the vulnerability of latent layers in adversarially
    trained models,” in Proceedings of the 28th International Joint Conference on
    Artificial Intelligence, pp. 2779–2785, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] S.-M. Moosavi-Dezfooli, A. Fawzi, J. Uesato, and P. Frossard, “Robustness
    via curvature regularization, and vice versa,” in Proceedings of the IEEE/CVF
    Conference on CVPR, pp. 9078–9086, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] J.-B. Alayrac, J. Uesato, P.-S. Huang, A. Fawzi, R. Stanforth, and P. Kohli,
    “Are labels required for improving adversarial robustness?,” Advances in Neural
    Information Processing Systems, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Y. Carmon, A. Raghunathan, L. Schmidt, J. C. Duchi, and P. S. Liang, “Unlabeled
    data improves adversarial robustness,” Advances in neural information processing
    systems, vol. 32, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] H. Scudder, “Probability of error of some adaptive pattern-recognition
    machines,” IEEE Transactions on Information Theory, vol. 11, no. 3, pp. 363–371,
    1965.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] J. Cohen, E. Rosenfeld, and Z. Kolter, “Certified adversarial robustness
    via randomized smoothing,” in international conference on machine learning, pp. 1310–1320,
    PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] X. Gao, R. K. Saha, M. R. Prasad, and A. Roychoudhury, “Fuzz testing based
    data augmentation to improve robustness of deep neural networks,” 2020 IEEE/ACM
    42nd ICSE, pp. 1147–1158, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] S. Addepalli, S. VivekB., A. Baburaj, G. Sriramanan, and R. V. Babu, “Towards
    achieving adversarial robustness by enforcing feature consistency across bit planes,”
    2020 IEEE/CVF CVPR, pp. 1017–1026, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] D. Wu, S.-T. Xia, and Y. Wang, “Adversarial weight perturbation helps
    robust generalization,” Advances in Neural Information Processing Systems, vol. 33,
    pp. 2958–2969, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] L. Huang, C. Zhang, and H. Zhang, “Self-adaptive training: beyond empirical
    risk minimization,” Advances in neural information processing systems, vol. 33,
    pp. 19365–19376, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] V. Sehwag, S. Wang, P. Mittal, and S. Jana, “Hydra: Pruning adversarially
    robust neural networks,” Advances in Neural Information Processing Systems, vol. 33,
    pp. 19655–19666, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] B. Zi, S. Zhao, X. Ma, and Y.-G. Jiang, “Revisiting adversarial robustness
    distillation: Robust soft labels make student better,” in Proceedings of the IEEE/CVF
    ICCV, pp. 16443–16452, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] C. Zhang, A. Liu, X. Liu, Y. Xu, H. Yu, Y. Ma, and T. Li, “Interpreting
    and improving adversarial robustness of deep neural networks with neuron sensitivity,”
    IEEE Transactions on Image Processing, vol. 30, pp. 1291–1304, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] S. Kundu, M. Nazemi, P. A. Beerel, and M. Pedram, “Dnr: A tunable robust
    pruning framework through dynamic network rewiring of dnns,” in Proceedings of
    the 26th Asia and South Pacific Design Automation Conference, pp. 344–350, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] C. Jin and M. Rinard, “Manifold regularization for locally stable deep
    neural networks,” arXiv preprint arXiv:2003.04286, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Cui, S. Liu, L. Wang, and J. Jia, “Learnable boundary guided adversarial
    training,” in Proceedings of the IEEE/CVF ICCV, pp. 15721–15730, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] E.-C. Chen and C.-R. Lee, “Ltd: Low temperature distillation for robust
    adversarial training,” arXiv preprint arXiv:2111.02331, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] H. Yan, J. Du, V. Y. Tan, and J. Feng, “On robustness of neural ordinary
    differential equations,” arXiv preprint arXiv:1910.05513, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] E. Haber and L. Ruthotto, “Stable architectures for deep neural networks,”
    Inverse problems, vol. 34, no. 1, p. 014004, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] X. Liu, T. Xiao, S. Si, Q. Cao, S. Kumar, and C.-J. Hsieh, “How does
    noise help robustness? explanation and exploration under the neural sde framework,”
    in Proceedings of the IEEE/CVF Conference on CVPR, pp. 282–290, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Q. Kang, Y. Song, Q. Ding, and W. P. Tay, “Stable neural ode with lyapunov-stable
    equilibrium points for defending against adversarial attacks,” Advances in Neural
    Information Processing Systems, vol. 34, pp. 14925–14937, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T. Pang, M. Lin, X. Yang, J. Zhu, and S. Yan, “Robustness and accuracy
    could be reconcilable by (proper) definition,” in International Conference on
    Machine Learning, pp. 17258–17277, PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Dai, S. Mahloujifar, and P. Mittal, “Parameterizing activation functions
    for adversarial robustness,” in 2022 IEEE SPW, pp. 80–87, IEEE, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] D. Meng and H. Chen, “Magnet: A two-pronged defense against adversarial
    examples,” Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
    Security, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff, “On detecting
    adversarial perturbations,” 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
    examples in deep neural networks,” arXiv preprint arXiv:1704.01155, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extracting
    and composing robust features with denoising autoencoders,” in ICML ’08, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” ArXiv, vol. abs/1701.07875,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] C. Mao, M. Chiquier, H. Wang, J. Yang, and C. Vondrick, “Adversarial
    attacks are reversible with natural supervision,” in Proceedings of the IEEE/CVF
    ICCV, pp. 661–671, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Y. Li, M. R. Min, T. Lee, W. Yu, E. Kruus, W. Wang, and C.-J. Hsieh,
    “Towards robustness of deep neural networks via regularization,” in Proceedings
    of the IEEE/CVF ICCV, pp. 7496–7505, October 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] D. Zhou, N. Wang, C. Peng, X. Gao, X. Wang, J. Yu, and T. Liu, “Removing
    adversarial noise in class activation feature space,” in Proceedings of the IEEE/CVF
    ICCV, pp. 7878–7887, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] A. Abusnaina, Y. Wu, S. Arora, Y. Wang, F. Wang, H. Yang, and D. Mohaisen,
    “Adversarial example detection using latent neighborhood graph,” in Proceedings
    of the IEEE/CVF ICCV, pp. 7687–7696, October 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” IEEE Transactions on Neural Networks, vol. 20,
    no. 1, pp. 61–80, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Y. Chen, S. Liu, and X. Wang, “Learning continuous image representation
    with local implicit image function,” in Proceedings of the IEEE/CVF conference
    on CVPR, pp. 8628–8638, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] C.-H. Ho and N. Vasconcelos, “Disco: Adversarial defense with local implicit
    functions,” in Advances in Neural Information Processing Systems (S. Koyejo, S. Mohamed,
    A. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds.), vol. 35, pp. 23818–23837, Curran
    Associates, Inc., 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] C. Xie, Y. Wu, L. van der Maaten, A. L. Yuille, and K. He, “Feature denoising
    for improving adversarial robustness,” 2019 IEEE/CVF Conference on CVPR, pp. 501–509,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] M. Guo, Y. Yang, R. Xu, and Z. Liu, “When nas meets robustness: In search
    of robust architectures against adversarial attacks,” 2020 IEEE/CVF Conference
    on CVPR, pp. 628–637, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille, “Mitigating adversarial
    effects through randomization,” in International Conference on Learning Representations,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] M. Atzmon, N. Haim, L. Yariv, O. Israelov, H. Maron, and Y. Lipman, “Controlling
    neural level sets,” Advances in Neural Information Processing Systems, vol. 32,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] B. Sun, N.-h. Tsai, F. Liu, R. Yu, and H. Su, “Adversarial defense by
    stratified convolutional sparse coding,” in Proceedings of the IEEE/CVF Conference
    on CVPR, pp. 11447–11456, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] P. Benz, C. Zhang, and I. S. Kweon, “Batch normalization increases adversarial
    vulnerability: Disentangling usefulness and robustness of model features,” ArXiv,
    vol. abs/2010.03316, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” in International conference on
    machine learning, pp. 448–456, PMLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] W. F. Good, G. S. Maitz, and D. Gur, “Joint photographic experts group
    (jpeg) compatible data compression of mammograms,” Journal of Digital Imaging,
    vol. 7, no. 3, pp. 123–132, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] G. K. Dziugaite, Z. Ghahramani, and D. M. Roy, “A study of the effect
    of jpg compression on adversarial images,” arXiv preprint arXiv:1608.00853, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, “Safety verification of
    deep neural networks,” in CAV, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] K. Pei, Y. Cao, J. Yang, and S. S. Jana, “Deepxplore: Automated whitebox
    testing of deep learning systems,” Proceedings of the 26th Symposium on Operating
    Systems Principles, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,
    L. Li, Y. Liu, J. Zhao, and Y. Wang, “Deepgauge: Multi-granularity testing criteria
    for deep learning systems,” 2018 33rd IEEE/ACM International Conference on ASE,
    pp. 120–131, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] J. Kim, R. Feldt, and S. Yoo, “Guiding deep learning system testing using
    surprise adequacy,” 2019 IEEE/ACM 41st ICSE, pp. 1039–1049, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] T. DeVries and G. W. Taylor, “Improved regularization of convolutional
    neural networks with cutout,” arXiv preprint arXiv:1708.04552, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical
    risk minimization,” in International Conference on Learning Representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] L. Rice, E. Wong, and Z. Kolter, “Overfitting in adversarially robust
    deep learning,” in International Conference on Machine Learning, pp. 8093–8104,
    PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] T. Pang, X. Yang, Y. Dong, H. Su, and J. Zhu, “Bag of tricks for adversarial
    training,” arXiv preprint arXiv:2010.00467, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] S.-A. Rebuffi, S. Gowal, D. A. Calian, F. Stimberg, O. Wiles, and T. Mann,
    “Fixing data augmentation to improve adversarial robustness,” arXiv preprint arXiv:2103.01946,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] S. Gowal, C. Qin, J. Uesato, T. Mann, and P. Kohli, “Uncovering the limits
    of adversarial training against norm-bounded adversarial examples,” arXiv preprint
    arXiv:2010.03593, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] S. Elfwing, E. Uchibe, and K. Doya, “Sigmoid-weighted linear units for
    neural network function approximation in reinforcement learning,” Neural Networks,
    vol. 107, pp. 3–11, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    Advances in Neural Information Processing Systems, vol. 33, pp. 6840–6851, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] S. Gowal, S.-A. Rebuffi, O. Wiles, F. Stimberg, D. A. Calian, and T. A.
    Mann, “Improving robustness using generated data,” Advances in Neural Information
    Processing Systems, vol. 34, pp. 4218–4233, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep
    unsupervised learning using nonequilibrium thermodynamics,” in International Conference
    on Machine Learning, pp. 2256–2265, PMLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] V. Sehwag, S. Mahloujifar, T. Handina, S. Dai, C. Xiang, M. Chiang, and
    P. Mittal, “Robust learning meets generative models: Can proxy distributions improve
    adversarial robustness?,” in International Conference on Learning Representations,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] C. Shi, C. Holtz, and G. Mishne, “Online adversarial purification based
    on self-supervised learning,” in International Conference on Learning Representations,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] J. Yoon, S. J. Hwang, and J. Lee, “Adversarial purification with score-based
    generative models,” in International Conference on Machine Learning, pp. 12062–12072,
    PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] W. Nie, B. Guo, Y. Huang, C. Xiao, A. Vahdat, and A. Anandkumar, “Diffusion
    models for adversarial purification,” in International Conference on Machine Learning,
    pp. 16805–16827, PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Q. Wu, H. Ye, and Y. Gu, “Guided diffusion model for adversarial purification
    from random noise,” arXiv e-prints, pp. arXiv–2206, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] C. Xiao, Z. Chen, K. Jin, J. Wang, W. Nie, M. Liu, A. Anandkumar, B. Li,
    and D. Song, “Densepure: Understanding diffusion models towards adversarial robustness,”
    arXiv preprint arXiv:2211.00322, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Z. Wang, T. Pang, C. Du, M. Lin, W. Liu, and S. Yan, “Better diffusion
    models further improve adversarial training,” arXiv preprint arXiv:2302.04638,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] T. Karras, M. Aittala, T. Aila, and S. Laine, “Elucidating the design
    space of diffusion-based generative models,” in Advances in Neural Information
    Processing Systems (A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, eds.), 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in Advances in Neural Information Processing
    Systems (F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, eds.),
    vol. 25, Curran Associates, Inc., 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. S. Khan, and M. Yang,
    “Intriguing properties of vision transformers,” ArXiv, vol. abs/2105.10497, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” ArXiv, vol. abs/1512.03385, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] K. Mahmood, R. Mahmood, and M. van Dijk, “On the robustness of vision
    transformers to adversarial examples,” in Proceedings of the IEEE/CVF ICCV, pp. 7838–7847,
    October 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] A. Aldahdooh, W. Hamidouche, and O. Déforges, “Reveal of vision transformers
    robustness against adversarial attacks,” ArXiv, vol. abs/2106.03734, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-cam: Visual explanations from deep networks via gradient-based localization,”
    in Proceedings of the IEEE ICCV, Oct 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Z. Wei, J. Chen, M. Goldblum, Z. Wu, T. Goldstein, and Y. Jiang, “Towards
    transferable adversarial attacks on vision transformers,” ArXiv, vol. abs/2109.04176,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] H. Salman, S. Jain, E. Wong, and A. Madry, “Certified patch robustness
    via smoothed vision transformers,” ArXiv, vol. abs/2110.07719, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Y. Bai, J. Mei, A. L. Yuille, and C. Xie, “Are transformers more robust
    than cnns?,” Advances in Neural Information Processing Systems, vol. 34, pp. 26831–26843,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Y. Wang, J. Wang, Z. Yin, R. Gong, J. Wang, A. Liu, and X. Liu, “Generating
    transferable adversarial examples against vision transformers,” in Proceedings
    of the 30th ACM International Conference on Multimedia, pp. 5181–5190, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Y. Fu, S. Zhang, S. Wu, C. Wan, and Y. Lin, “Patch-fool: Are vision transformers
    always robust against adversarial perturbations?,” in International Conference
    on Learning Representations, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] J. Gu, V. Tresp, and Y. Qin, “Are vision transformers robust to patch
    perturbations?,” in Computer Vision – ECCV 2022, pp. 404–421, Springer, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Z. Chen, B. Li, J. Xu, S. Wu, S. Ding, and W. Zhang, “Towards practical
    certifiable patch defense with vision transformer,” in Proceedings of the IEEE/CVF
    Conference on CVPR, pp. 15148–15158, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Y. Li, S. Ruan, H. Qin, S. Deng, and M. A. El-Yacoubi, “Transformer based
    defense gan against palm-vein adversarial attacks,” IEEE Transactions on Information
    Forensics and Security, vol. 18, pp. 1509–1523, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324,
    1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset
    for benchmarking machine learning algorithms,” ArXiv, vol. abs/1708.07747, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features
    from tiny images,” 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] A. Torralba, R. Fergus, and W. T. Freeman, “80 million tiny images: A
    large data set for nonparametric object and scene recognition,” IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 30, no. 11, pp. 1958–1970,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading
    digits in natural images with unsupervised feature learning,” in NIPS Workshop
    on Deep Learning and Unsupervised Feature Learning 2011, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “Man vs. computer:
    Benchmarking machine learning algorithms for traffic sign recognition,” Neural
    networks, vol. 32, pp. 323–332, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, et al., “Imagenet large scale visual recognition
    challenge,” IJCV, vol. 115, no. 3, pp. 211–252, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song, “Natural
    adversarial examples,” in Proceedings of the IEEE/CVF Conference on CVPR, pp. 15262–15271,
    June 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] D. Hendrycks and T. Dietterich, “Benchmarking neural network robustness
    to common corruptions and perturbations,” in International Conference on Learning
    Representations, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] K. De and M. Pedersen, “Impact of colour on robustness of deep neural
    networks,” in 2021 IEEE/CVF International Conference on Computer Vision Workshops
    (ICCVW), pp. 21–30, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Y. Le and X. Yang, “Tiny imagenet visual recognition challenge,” CS 231N,
    vol. 7, no. 7, p. 3, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Z. Huan, Y. Wang, X. Zhang, L. Shang, C. Fu, and J. Zhou, “Data-free
    adversarial perturbations for practical black-box attack,” in Pacific-Asia conference
    on knowledge discovery and data mining, pp. 127–138, Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] D. Hendrycks, K. Lee, and M. Mazeika, “Using pre-training can improve
    model robustness and uncertainty,” in International Conference on Machine Learning,
    pp. 2712–2721, PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in British Machine
    Vision Conference 2016, British Machine Vision Association, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] F. Croce, M. Andriushchenko, V. Sehwag, E. Debenedetti, N. Flammarion,
    M. Chiang, P. Mittal, and M. Hein, “Robustbench: a standardized adversarial robustness
    benchmark,” in Thirty-fifth Conference on Neural Information Processing Systems
    Datasets and Benchmarks Track (Round 2), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] H. Salman, A. Ilyas, L. Engstrom, A. Kapoor, and A. Madry, “Do adversarially
    robust imagenet models transfer better?,” Advances in Neural Information Processing
    Systems, vol. 33, pp. 3533–3545, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
