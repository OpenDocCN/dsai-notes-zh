- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:39:38'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:39:38
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2305.10862] How Deep Learning Sees the World: A Survey on Adversarial Attacks
    & Defenses'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2305.10862] 深度学习如何看待世界：对对抗攻击与防御的调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.10862](https://ar5iv.labs.arxiv.org/html/2305.10862)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2305.10862](https://ar5iv.labs.arxiv.org/html/2305.10862)
- en: 'How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习如何看待世界：对对抗攻击与防御的调查
- en: Joana C. Costa, Tiago Roxo, Hugo Proença,  Pedro R. M. Inácio The authors are
    with Instituto de Telecomunicações, Universidade da Beira Interior, Portugal.Manuscript
    received XX, 2023; revised XX, 2023.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Joana C. Costa, Tiago Roxo, Hugo Proença, Pedro R. M. Inácio 作者均来自葡萄牙贝拉内里大学电信研究所。手稿收到时间为2023年XX月；修订时间为2023年XX月。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep Learning is currently used to perform multiple tasks, such as object recognition,
    face recognition, and natural language processing. However, Deep Neural Networks
    (DNNs) are vulnerable to perturbations that alter the network prediction (adversarial
    examples), raising concerns regarding its usage in critical areas, such as self-driving
    vehicles, malware detection, and healthcare. This paper compiles the most recent
    adversarial attacks, grouped by the attacker capacity, and modern defenses clustered
    by protection strategies. We also present the new advances regarding Vision Transformers,
    summarize the datasets and metrics used in the context of adversarial settings,
    and compare the state-of-the-art results under different attacks, finishing with
    the identification of open issues.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习目前用于执行多个任务，如物体识别、人脸识别和自然语言处理。然而，深度神经网络（DNNs）容易受到扰动的影响，这会改变网络的预测（对抗性样本），这引发了对其在关键领域使用的担忧，如自动驾驶车辆、恶意软件检测和医疗保健。本文汇编了最新的对抗攻击，按攻击者能力分组，并按保护策略对现代防御进行了分类。我们还介绍了关于视觉变换器的新进展，总结了在对抗环境中使用的数据集和指标，并比较了不同攻击下的最新成果，最后识别了存在的开放问题。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Adversarial attacks, adversarial defenses, datasets, evaluation metrics, survey,
    vision transformers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击，对抗防御，数据集，评估指标，调查，视觉变换器。
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Machine Learning (ML) algorithms have been able to solve various types of problems,
    namely highly complex ones, through the usage of Deep Neural Networks (DNNs) [[1](#bib.bib1)],
    achieving results similar to, or better than, humans in multiple tasks, such as
    object recognition [[2](#bib.bib2), [3](#bib.bib3)], face recognition [[4](#bib.bib4),
    [5](#bib.bib5)], and natural language processing [[6](#bib.bib6), [7](#bib.bib7)].
    These networks have also been employed in critical areas, such as self-driving
    vehicles [[8](#bib.bib8), [9](#bib.bib9)], malware detection [[10](#bib.bib10),
    [11](#bib.bib11)], and healthcare [[12](#bib.bib12), [13](#bib.bib13)], whose
    application and impaired functioning can severely impact their users.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习（ML）算法通过使用深度神经网络（DNNs）已经能够解决各种类型的问题，即使是高度复杂的问题，取得了与人类相似或更好的结果，涉及多个任务，如物体识别[[2](#bib.bib2)、[3](#bib.bib3)]、人脸识别[[4](#bib.bib4)、[5](#bib.bib5)]和自然语言处理[[6](#bib.bib6)、[7](#bib.bib7)]。这些网络还被应用于关键领域，如自动驾驶车辆[[8](#bib.bib8)、[9](#bib.bib9)]、恶意软件检测[[10](#bib.bib10)、[11](#bib.bib11)]和医疗保健[[12](#bib.bib12)、[13](#bib.bib13)]，这些应用的功能受损可能严重影响其用户。
- en: 'Promising results shown by DNNs lead to the sense that these networks could
    correctly generalize in the local neighborhood of an input (image). These results
    motivate the adoption and integration of these networks in real-time image analysis,
    such as traffic sign recognition and vehicle segmentation, making malicious entities
    target these techniques. However, it was discovered that DNNs are susceptible
    to small perturbations in their input [[14](#bib.bib14)], which entirely alter
    their prediction, making it harder for them to be applied in critical areas. These
    perturbations have two main characteristics: 1) invisible to the Human eye or
    slight noise that does not alter Human prediction; and 2) significantly increase
    the confidence of erroneous output, the DNNs predict the wrong class with higher
    confidence than all other classes. As a result of these assertions, the effect
    of the perturbations has been analyzed with more focus on object recognition,
    which will also be the main target of this survey.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络（DNNs）显示出的有希望的结果使人们感到这些网络能够在输入（图像）的局部邻域中正确泛化。这些结果促使了这些网络在实时图像分析中的应用和集成，如交通标志识别和车辆分割，从而使恶意实体将这些技术作为攻击目标。然而，发现
    DNNs 对输入中的微小扰动敏感 [[14](#bib.bib14)]，这些扰动会完全改变其预测，使其在关键领域的应用变得更加困难。这些扰动具有两个主要特征：1)
    对人眼不可见或轻微的噪声不会改变人类的预测；2) 显著增加错误输出的置信度，DNNs 以比其他所有类别更高的置信度预测错误的类别。因此，扰动的影响已被重点分析于目标识别，这也是本调查的主要目标。
- en: 'Papernot et al. [[15](#bib.bib15)] distinguishes four types of adversaries
    depending on the information they have access to: (i) training data and network
    architecture, (ii) only training data or only network, (iii) oracle, and (iv)
    only pairs of input and output. In almost all real scenarios, the attacker does
    not have access to the training data or the network architecture, which diminishes
    the strength of the attack performed on a network, leaving the adversary with
    access to the responses given by the network, either by asking questions directly
    to it or by having pairs of input and prediction. Furthermore, the queries to
    a model are usually limited or very expensive [[16](#bib.bib16)], making it harder
    for an attacker to produce adversarial examples.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Papernot 等人 [[15](#bib.bib15)] 根据攻击者所拥有的信息将对手分为四类：(i) 训练数据和网络架构，(ii) 仅有训练数据或仅有网络，(iii)
    神谕型攻击者，以及 (iv) 仅有输入和输出对。在几乎所有的实际场景中，攻击者无法访问训练数据或网络架构，这削弱了对网络进行攻击的力度，使得对手只能访问网络给出的响应，要么直接向网络提问，要么通过输入和预测对来获得响应。此外，对模型的查询通常是有限的或非常昂贵的
    [[16](#bib.bib16)]，这使得攻击者更难生成对抗样本。
- en: 'Multiple mechanisms [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)] were proposed to defend against legacy attacks, already displaying
    their weakened effect when adequately protected, which are clustered based on
    six different domains in this survey. Regardless of the attacks and defenses already
    proposed, there is no assurance about the effective robustness of these networks
    and if they can be trusted in critical areas, clearly raising the need to make
    the DNNs inherently robust or easy to be updated every time a new vulnerability
    is encountered. This motivates the presented work, whose main contributions are
    summarized as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 提出了多种机制 [[17](#bib.bib17), [18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)]
    来防御传统攻击，这些机制在得到适当保护时已经显示出其效果减弱，这些机制在本调查中根据六个不同领域进行了分类。尽管已有攻击和防御方案，但对这些网络的有效鲁棒性以及它们在关键领域是否值得信赖尚无保障，明确地提出了使
    DNNs 本质上具有鲁棒性或每当发现新漏洞时易于更新的需求。这激发了所呈现的工作，其主要贡献总结如下：
- en: •
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We present the most recent adversarial attacks grouped by the adversary capacity,
    accompanied by an illustration of the differences between black-box and white-box
    attacks;
  id: totrans-18
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们展示了按攻击者能力分组的最新对抗攻击，并附有黑盒攻击和白盒攻击之间差异的插图；
- en: •
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We propose six different domains for adversarial defense grouping, assisted
    by exemplificative figures of each of these domains, and describe the effects
    of adversarial examples in ViTs;
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提出了六个不同的对抗防御领域，并附有每个领域的示例图，描述了对抗样本在 ViTs 中的影响；
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We detail the most widely used metrics and datasets, present state-of-the-art
    results on CIFAR-10, CIFAR-100, and ImageNet, and propose directions for future
    works.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们详细介绍了最常用的指标和数据集，展示了在 CIFAR-10、CIFAR-100 和 ImageNet 上的最新成果，并提出了未来工作的方向。
- en: 'The remaining of the paper is organized as follows: Section [II](#S2 "II Background
    for Adversarial Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") provides background information; Section [III](#S3 "III Related
    Surveys ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses") compares this review with others; Section [IV](#S4 "IV Adversarial
    Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses") presents the set of adversarial attacks; Section [V](#S5 "V Adversarial
    Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses") shows a collection of defenses to overcome these attacks; Section [VII](#S7
    "VII Datasets ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks
    & Defenses") displays the commonly used datasets; Section [VIII](#S8 "VIII Metrics
    and State-of-the-art Results ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") lists and elaborates on metrics and presents state-of-the-art
    results; and Section [IX](#S9 "IX Future Directions ‣ How Deep Learning Sees the
    World: A Survey on Adversarial Attacks & Defenses") presents future directions,
    with the concluding remarks included in Section [X](#S10 "X Conclusions ‣ How
    Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses").'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '论文的其余部分组织如下：第 [II](#S2 "II Background for Adversarial Attacks ‣ How Deep Learning
    Sees the World: A Survey on Adversarial Attacks & Defenses") 节提供背景信息；第 [III](#S3
    "III Related Surveys ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") 节将此评述与其他评述进行比较；第 [IV](#S4 "IV Adversarial Attacks ‣ How Deep
    Learning Sees the World: A Survey on Adversarial Attacks & Defenses") 节介绍了一系列对抗性攻击；第 [V](#S5
    "V Adversarial Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") 节展示了克服这些攻击的防御措施；第 [VII](#S7 "VII Datasets ‣ How Deep Learning
    Sees the World: A Survey on Adversarial Attacks & Defenses") 节展示了常用的数据集；第 [VIII](#S8
    "VIII Metrics and State-of-the-art Results ‣ How Deep Learning Sees the World:
    A Survey on Adversarial Attacks & Defenses") 节列出了指标并详细阐述，并展示了最先进的结果；第 [IX](#S9
    "IX Future Directions ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") 节介绍了未来的方向，结论见第 [X](#S10 "X Conclusions ‣ How Deep Learning
    Sees the World: A Survey on Adversarial Attacks & Defenses") 节。'
- en: II Background for Adversarial Attacks
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 对抗性攻击背景
- en: II-A Neural Network Architectures
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 神经网络架构
- en: 'When an input image is fed into a CNN, it is converted into a matrix containing
    the numeric values representing the image or, if the image is colored, a set of
    matrices containing the numeric values for each color channel. Then, the Convolutions
    apply filters to these matrixes and calculate a set of reduced-size features.
    Finally, these features have an array format fed into the Fully Connected that
    classifies the provided image. Figure [1](#S2.F1 "Figure 1 ‣ II-A Neural Network
    Architectures ‣ II Background for Adversarial Attacks ‣ How Deep Learning Sees
    the World: A Survey on Adversarial Attacks & Defenses") shows an elementary example
    of CNNs used to classify images.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '当输入图像被输入到 CNN 时，它会被转换为一个矩阵，该矩阵包含表示图像的数值，或者如果图像是彩色的，则为包含每个颜色通道数值的多个矩阵。然后，卷积操作会对这些矩阵应用滤波器，并计算出一组缩小尺寸的特征。最后，这些特征以数组格式输入到全连接层中，对提供的图像进行分类。图 [1](#S2.F1
    "Figure 1 ‣ II-A Neural Network Architectures ‣ II Background for Adversarial
    Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses") 展示了一个基础的 CNN 图像分类示例。'
- en: '![Refer to caption](img/092f7c6eee0959ca31da9a53493bc6e9.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/092f7c6eee0959ca31da9a53493bc6e9.png)'
- en: 'Figure 1: Schematic example of the Convolutional Neural Networks mechanism
    to classify images.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：卷积神经网络机制分类图像的示意图。
- en: 'Contrary to the CNN, ViT does not receive the image as a whole as input; instead,
    it is pre-processed to be divided into Patches, which are smaller parts of the
    original image, as displayed in Figure [2](#S2.F2 "Figure 2 ‣ II-A Neural Network
    Architectures ‣ II Background for Adversarial Attacks ‣ How Deep Learning Sees
    the World: A Survey on Adversarial Attacks & Defenses"). These Patches are not
    fed randomly to the Transformer Encoder, they are ordered by their position, and
    both the Patches and their position are fed into the Transformer Encoder. Finally,
    the output resulting from the Transformer Encoder is fed into the Multi-Layer
    Perceptron (MLP) Head that classifies the image.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '与 CNN 相反，ViT 不将整个图像作为输入；相反，它会将图像预处理为多个 Patches，这些 Patches 是原始图像的较小部分，如图 [2](#S2.F2
    "Figure 2 ‣ II-A Neural Network Architectures ‣ II Background for Adversarial
    Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses")所示。这些 Patches 不是随机输入到 Transformer 编码器中，而是按位置排序，Patches 及其位置都被输入到 Transformer
    编码器中。最后，Transformer 编码器的输出被输入到分类图像的多层感知器 (MLP) 头中。'
- en: '![Refer to caption](img/9ca28e9d38fdaa51b6ae2a7ef3057dad.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9ca28e9d38fdaa51b6ae2a7ef3057dad.png)'
- en: 'Figure 2: Schematic example of a simplified vision transformer used to classify
    images.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：用于分类图像的简化视觉变换器的示意图。
- en: II-B Adversarial Example
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 对抗示例
- en: Misclassification might be justified if the object contained in the image is
    not visible even to Humans. However, adversarial examples do not fit this scope.
    These examples add a perturbation to an image that causes the DNNs to misclassify
    the object in the image, yet Humans can correctly classify the same object.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图像中的物体连人眼也无法看到，可能会解释为误分类。然而，对抗示例并不符合这一范围。这些示例在图像中添加了扰动，使得DNN将图像中的物体误分类，但人类可以正确分类相同的物体。
- en: 'The adversarial attacks described throughout this survey focus on identifying
    the adversarial examples that make DNNs misclassify. These attacks identify specific
    perturbations that modify the DNN classification while being correctly classified
    by Humans. The calculation of these perturbations is an optimization problem formally
    defined as:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查中描述的对抗攻击侧重于识别使DNN误分类的对抗示例。这些攻击识别特定的扰动，这些扰动修改了DNN的分类，同时被人类正确分类。这些扰动的计算是一个优化问题，正式定义为：
- en: '|  | $\arg\min_{\delta\textbf{X}}\&#124;\delta_{\textbf{X}}\&#124;\textbf{
    s.t. }\textbf{f}(\textbf{X}+\delta_{\textbf{X}})=\textbf{Y}\ast,$ |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\arg\min_{\delta\textbf{X}}\&#124;\delta_{\textbf{X}}\&#124;\textbf{
    s.t. }\textbf{f}(\textbf{X}+\delta_{\textbf{X}})=\textbf{Y}\ast,$ |  | (1) |'
- en: 'where $f$ is the is the classifier, $\delta_{\textbf{X}}$ is the perturbation,
    X is the original/benign image, and $\textbf{Y}\ast$ is the adversarial output.
    Furthermore, the adversarial example is defined as:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$f$是分类器，$\delta_{\textbf{X}}$是扰动，X是原始/良性图像，$\textbf{Y}\ast$是对抗输出。此外，对抗示例定义为：
- en: '|  | $\textbf{X}\ast=\textbf{X}+\delta_{\textbf{X}},$ |  | (2) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\textbf{X}\ast=\textbf{X}+\delta_{\textbf{X}},$ |  | (2) |'
- en: where $\textbf{X}\ast$ is the adversarial image.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$\textbf{X}\ast$是对抗图像。
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ II-B Adversarial Example ‣ II Background for
    Adversarial Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") displays adversarial examples generated using different attacks.
    Mainly, the first row is the Limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) [[14](#bib.bib14)]
    attack, the second row is the DeepFool [[21](#bib.bib21)] attack, and the third
    row is the SmoothFool [[22](#bib.bib22)] attack. When observing the L-BFGS, the
    perturbation applies noise to almost the entirety of the adversarial image. The
    DeepFool attack only perturbs the area of the whale but not all the pixels in
    that area. Finally, the SmoothFool attack slightly disturbs the pixels in the
    area of the image. These three attacks display the evolution of adversarial attacks
    in decreasing order of detectability and, consequently, increasing order of strength.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S2.F3 "图3 ‣ II-B 对抗示例 ‣ II 对抗攻击背景 ‣ 深度学习如何看待世界：对抗攻击与防御的调查")展示了使用不同攻击生成的对抗示例。主要地，第一行是有限记忆Broyden-Fletcher-Goldfarb-Shanno
    (L-BFGS) [[14](#bib.bib14)]攻击，第二行是DeepFool [[21](#bib.bib21)]攻击，第三行是SmoothFool
    [[22](#bib.bib22)]攻击。在观察L-BFGS时，扰动几乎应用于对抗图像的整个区域。DeepFool攻击仅对鲸鱼的区域施加扰动，而不是该区域内的所有像素。最后，SmoothFool攻击轻微地扰动了图像区域中的像素。这三种攻击展示了对抗攻击的发展，从可检测性递减到强度递增。
- en: '![Refer to caption](img/c9efbaecc40ded3d0fdcbe55127c3bec.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c9efbaecc40ded3d0fdcbe55127c3bec.png)'
- en: 'Figure 3: Adversarial Examples created using different state-of-the-art adversarial
    attacks. The first column represents the original image; the second represents
    the perturbation used to generate the adversarial images in the third column.
    The images were resized for better visualization. Images withdrawn from [[14](#bib.bib14),
    [21](#bib.bib21), [22](#bib.bib22)]. The first perturbation follows the edges
    of the building, the second is concentrated in the area of the whale, and the
    third is more smooth and greater in area.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用不同最先进对抗攻击生成的对抗示例。第一列代表原始图像；第二列代表用于生成第三列对抗图像的扰动。图像经过缩放以便更好地可视化。图像摘自[[14](#bib.bib14)、[21](#bib.bib21)、[22](#bib.bib22)]。第一种扰动沿建筑物的边缘，第二种集中在鲸鱼的区域，第三种则更平滑且面积更大。
- en: 'To limit the noise that each perturbation can add to an image, the adversarial
    attacks are divided into $L_{0}$, $L_{2}$, and $L_{p}$ norms, known as Vector
    Norms. Furthermore, commonly used terminologies in the context of adversarial
    examples are defined in Table [I](#S2.T1 "TABLE I ‣ II-D Adversary Goals and Capacity
    ‣ II Background for Adversarial Attacks ‣ How Deep Learning Sees the World: A
    Survey on Adversarial Attacks & Defenses").'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 为了限制每次扰动对图像所能增加的噪声，对抗攻击被分为$L_{0}$、$L_{2}$和$L_{p}$范数，即向量范数。此外，在对抗示例的背景下，常用术语在表[I](#S2.T1
    "表 I ‣ II-D 对手目标与能力 ‣ II 对抗攻击的背景 ‣ 深度学习如何看待世界：对对抗攻击与防御的调查")中进行了定义。
- en: II-C Vector Norms and $\epsilon$ Constraint
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 向量范数与$\epsilon$约束
- en: Vector Norms are functions that take a vector as input and output a positive
    value (scalar). These functions are essential to ML and allow the backpropagation
    algorithms to compute the loss value as a scalar. The family of these functions
    is known as the p-norm, and, in the context of adversarial attacks, the considered
    values for $p$ are $0$, $2$, and $\infty$.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 向量范数是将向量作为输入并输出一个正值（标量）的函数。这些函数在机器学习中至关重要，并允许反向传播算法计算作为标量的损失值。这些函数的家族被称为p-范数，在对抗攻击的背景下，考虑的$p$值是$0$、$2$和$\infty$。
- en: '$L_{0}$ norm consists of counting the number of non-zero elements in the vector
    and is formally given as:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: $L_{0}$范数由计算向量中非零元素的数量组成，并正式定义为：
- en: '|  | $&#124;&#124;x&#124;&#124;_{0}=(&#124;x_{1}&#124;^{0}+&#124;x_{2}&#124;^{0}+...+&#124;x_{n}&#124;^{0}),$
    |  | (3) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $&#124;&#124;x&#124;&#124;_{0}=(&#124;x_{1}&#124;^{0}+&#124;x_{2}&#124;^{0}+...+&#124;x_{n}&#124;^{0}),$
    |  | (3) |'
- en: where $x_{1}$ to $x_{n}$ are the elements of the vector $x$.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$x_{1}$到$x_{n}$是向量$x$的元素。
- en: '$L_{2}$ norm, also known as the Euclidean distance, measures the vector distance
    to the origin and is formally defined as:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: $L_{2}$范数，也称为欧几里得距离，衡量向量到原点的距离，并正式定义为：
- en: '|  | $&#124;&#124;x&#124;&#124;_{2}=(&#124;x_{1}&#124;^{2}+&#124;x_{2}&#124;^{2}+...+&#124;x_{n}&#124;^{2})^{\frac{1}{2}},$
    |  | (4) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $&#124;&#124;x&#124;&#124;_{2}=(&#124;x_{1}&#124;^{2}+&#124;x_{2}&#124;^{2}+...+&#124;x_{n}&#124;^{2})^{\frac{1}{2}},$
    |  | (4) |'
- en: where $x_{1}$ to $x_{n}$ are the elements of the vector $x$.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$x_{1}$到$x_{n}$是向量$x$的元素。
- en: '$L_{\infty}$ norm represents the maximum hypothetical value that $p$ can have
    and returns the absolute value of the element with the largest magnitude, formally
    as:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: $L_{\infty}$范数表示$p$可能具有的最大假设值，并返回具有最大幅度的元素的绝对值，正式定义为：
- en: '|  | $&#124;&#124;x&#124;&#124;_{\infty}=\max_{i}&#124;x_{i}&#124;,$ |  | (5)
    |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $&#124;&#124;x&#124;&#124;_{\infty}=\max_{i}&#124;x_{i}&#124;,$ |  | (5)
    |'
- en: where $x_{i}$ is each element of the vector $x$.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$x_{i}$是向量$x$的每个元素。
- en: '![Refer to caption](img/145be088936a368d947b9ff7b1ec4b58.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/145be088936a368d947b9ff7b1ec4b58.png)'
- en: 'Figure 4: Geometric representation of the $l_{0}$, $l_{2}$, and $l_{\infty}$
    norms, from left to right, respectively.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：$l_{0}$、$l_{2}$和$l_{\infty}$范数的几何表示，从左到右分别为。
- en: 'A geometric representation of the area of exploitation for the three considered
    p-norm is displayed in Figure [4](#S2.F4 "Figure 4 ‣ II-C Vector Norms and ϵ Constraint
    ‣ II Background for Adversarial Attacks ‣ How Deep Learning Sees the World: A
    Survey on Adversarial Attacks & Defenses"). One relevant property of the p-norm
    is: the higher $p$ is, the more important the contribution of large errors; the
    lower $p$ is, the higher the contribution of small errors. This translates into
    a large $p$ benefiting small maximal errors (minimal perturbations along multiple
    pixels) and a small $p$ encouraging larger spikes in fewer places (abrupt perturbations
    along minimal pixels). Therefore, $l_{2}$ and $l_{0}$ attacks have greater detectability
    than $l_{\infty}$ attacks, with the latter being more threatening.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图[4](#S2.F4 "图 4 ‣ II-C 向量范数与ϵ约束 ‣ II 对抗攻击的背景 ‣ 深度学习如何看待世界：对对抗攻击与防御的调查")显示了三种p-范数的利用区域的几何表示。p-范数的一个相关属性是：$p$值越高，大误差的贡献越重要；$p$值越低，小误差的贡献越高。这意味着，较大的$p$值有利于小的最大误差（在多个像素上最小的扰动），而较小的$p$值则鼓励在较少的地方出现较大的突变（在最少像素上急剧的扰动）。因此，$l_{2}$和$l_{0}$攻击比$l_{\infty}$攻击具有更高的可检测性，而后者更具威胁性。
- en: Another constraint normally seen in the context of adversarial attacks is $\epsilon$,
    which is a constant that controls the amount of noise, via generated perturbation,
    that can be added to an image. Usually, it is a tiny number and varies depending
    on the used dataset, decreasing when the task increases in difficulty. According
    to the literature, for MNIST, $\epsilon=0.1$, for CIFAR-10 and CIFAR-100, $\epsilon=8/255$,
    and for ImageNet, $\epsilon=4/255$.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击中通常看到的另一个约束是 $\epsilon$，这是一个控制通过生成扰动添加到图像中的噪声量的常数。通常，它是一个微小的数字，并根据使用的数据集而变化，当任务难度增加时会减少。根据文献，对于
    MNIST，$\epsilon=0.1$，对于 CIFAR-10 和 CIFAR-100，$\epsilon=8/255$，对于 ImageNet，$\epsilon=4/255$。
- en: II-D Adversary Goals and Capacity
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 对手目标和能力
- en: 'Besides the restriction imposed by the different Vector Norms, the adversarial
    attacks are also divided by their impact on the networks. Depending on the goals
    of the attacker, the designation is as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 除了由不同向量范数施加的限制之外，对抗攻击还根据对网络的影响进行分类。根据攻击者的目标，分类如下：
- en: •
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Confidence Reduction, the classifier outputs the original label with less confidence;
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 信心减少，分类器以较低的信心输出原始标签；
- en: •
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Untargeted, the classifier outputs any class besides the original label;
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 非定向攻击，分类器输出任何类别而非原始标签；
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Targeted, the classifier outputs a particular class besides the original label.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定向攻击，分类器输出特定类别而非原始标签。
- en: 'Another important aspect of adversarial attacks is the amount of knowledge
    the attacker has access to. As defined by Papernot et al. [[15](#bib.bib15)],
    who proposed the first threat model for deep learning, the attackers can have
    access to: 1) data training and network architecture; 2) only network architecture;
    3) only data training; 4) an oracle that replies to all the inputs given; and
    5) only have pairs of input and corresponding output (samples). However, to simplify
    this classification, these capacities were divided into:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击的另一个重要方面是攻击者可以访问的知识量。根据 Papernot 等人的定义 [[15](#bib.bib15)]，他们提出了深度学习的第一个威胁模型，攻击者可以访问：1）数据训练和网络架构；2）仅网络架构；3）仅数据训练；4）回复所有输入的预言机；5）仅有输入和相应输出（样本）对。然而，为了简化分类，这些能力被分为：
- en: •
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: White-box, which considers that the attacker has access to either the architecture
    or data;
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 白盒，考虑到攻击者可以访问架构或数据；
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Black-box, when the attacker can only access samples from an oracle or in pairs
    of input and output.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 黑盒，当攻击者只能访问来自预言机的样本或输入输出对时。
- en: The attackers goals and capacity are essential to classify the strength of an
    attack. For example, the easiest is a Confidence Reduction White-box attack, and
    the strongest is a Targeted Black-box attack.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者的目标和能力对于分类攻击的强度至关重要。例如，最简单的是信心减少白盒攻击，最强的是定向黑盒攻击。
- en: 'TABLE I: Common terminologies used in the context of adversarial attacks and
    their definition.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 表 I：对抗攻击背景下使用的常见术语及其定义。
- en: '| Terminology | Definition |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | 定义 |'
- en: '| --- | --- |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Original/ Clean Example | Original image presented in a dataset |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 原始/干净样本 | 数据集中呈现的原始图像 |'
- en: '| Adversarial/ Perturbed Example | Image that an adversary has manipulated
    to fool the classifier |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 对抗/扰动样本 | 对手已操控以欺骗分类器的图像 |'
- en: '| Perturbation | Set of changes (for each pixel and color channel) that are
    performed on the image |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 扰动 | 对图像进行的更改集合（针对每个像素和颜色通道） |'
- en: '| Adversarial Attack | Technique used to calculate the perturbation that generates
    an adversarial example |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 对抗攻击 | 用于计算生成对抗样本的扰动的技术 |'
- en: '| Transferability | Capability of an adversarial example being transferred
    from a known network to an unknown network |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 可迁移性 | 对抗样本从已知网络转移到未知网络的能力 |'
- en: '| White-box | Attacks that have access to DNN weights and datasets |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 白盒 | 具有对深度神经网络权重和数据集访问权限的攻击 |'
- en: '| Black-box | Attacks that do not have access to the DNN weights and datasets
    |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 黑盒 | 没有对深度神经网络权重和数据集访问权限的攻击 |'
- en: '| Adversarial Training | Inclusion of adversarial examples in the training
    phase of the model |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 对抗训练 | 在模型训练阶段包含对抗样本 |'
- en: III Related Surveys
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 相关调查
- en: 'The first attempt to summarize and display the recent developments in this
    area was made by Akhtar and Mian [[23](#bib.bib23)]. These authors studied adversarial
    attacks in computer vision, extensively referring to attacks for classification
    and providing a brief overview of attacks beyond the classification problem. Furthermore,
    the survey presents a set of attacks performed in the real world and provides
    insight into the existence of adversarial examples. Finally, the authors present
    the defenses distributed through three categories: modified training or input,
    modifying networks, and add-on networks.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对该领域最新进展的首次总结和展示由 Akhtar 和 Mian [[23](#bib.bib23)] 完成。这些作者研究了计算机视觉中的对抗攻击，广泛涉及了分类攻击，并简要概述了超出分类问题的攻击。此外，调查展示了在现实世界中进行的一组攻击，并深入了解了对抗样本的存在。最后，作者将防御措施分为三类：修改训练或输入、修改网络和附加网络。
- en: From a broader perspective, Liu et al. [[24](#bib.bib24)] studied the security
    threats and possible defenses in ML scope, considering the different phases of
    an ML algorithm. For example, the training phase is only susceptible to poisoning
    attacks; however, the testing phase is vulnerable to evasion, impersonation, and
    inversion attacks, making it harder to defend. The authors provide their insight
    on the currently used techniques. Additionally, focusing more on the object recognition
    task, Serban et al. [[25](#bib.bib25)] extensively analyzed the adversarial attacks
    and defenses proposed under this context, providing conjectures for the existence
    of adversarial examples and evaluating the capacity of adversarial examples transferring
    between different DNNs.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从更广泛的角度来看，Liu 等人 [[24](#bib.bib24)] 研究了机器学习领域的安全威胁和可能的防御措施，考虑了机器学习算法的不同阶段。例如，训练阶段只容易受到投毒攻击；然而，测试阶段容易受到规避、冒充和反转攻击，使得防御更加困难。作者提供了对当前使用技术的见解。此外，Serban
    等人 [[25](#bib.bib25)] 更加专注于对象识别任务，广泛分析了在此背景下提出的对抗攻击和防御，提供了对对抗样本存在的推测，并评估了对抗样本在不同深度神经网络间转移的能力。
- en: Qui et al. [[26](#bib.bib26)] extensively explains background concepts in Adversarial
    Attacks, mentioning adversary goals, capabilities, and characteristics. It also
    displays applications for adversarial attacks and presents some of the most relevant
    adversarial defenses. Furthermore, it explains a set of attacks divided by the
    stage in which they occur, referring to the most relevant attacks.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Qui 等人 [[26](#bib.bib26)] 广泛解释了对抗攻击的背景概念，提到对手的目标、能力和特征。还展示了对抗攻击的应用，并介绍了一些最相关的对抗防御。此外，解释了一组按攻击阶段划分的攻击，涉及最相关的攻击。
- en: Xu et al. [[27](#bib.bib27)] also describes background concepts, describing
    the adversary goals and knowledge. This review summarizes the most relevant adversarial
    attacks at the time of that work and presents physical world examples. Furthermore,
    the authors present a batch of defenses grouped by the underlying methodology.
    Finally, there is an outline of adversarial attacks in graphs, text, and audio
    networks, culminating in the possible applications of these attacks.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人 [[27](#bib.bib27)] 也描述了背景概念，阐述了对手的目标和知识。这篇综述总结了当时最相关的对抗攻击，并展示了物理世界中的实例。此外，作者呈现了一批按底层方法分类的防御措施。最后，综述了图形、文本和音频网络中的对抗攻击，并总结了这些攻击的潜在应用。
- en: Chakraborty et al. [[28](#bib.bib28)] provides insight into commonly used ML
    algorithms and presents the adversary capabilities and goals. The presented adversarial
    attacks are divided based on the stage of the attack (train or test). Additionally,
    the authors present relevant defenses used in adversarial settings.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Chakraborty 等人 [[28](#bib.bib28)] 透视了常用的机器学习算法，并展示了对手的能力和目标。呈现的对抗攻击根据攻击阶段（训练或测试）进行划分。此外，作者还介绍了在对抗环境中使用的相关防御措施。
- en: 'TABLE II: Characteristics shown in state-of-the-art surveys on Adversarial
    Attacks.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表 II：最新调查中的对抗攻击特征。
- en: '| Survey | Year | White & | Survey | Grouping of | Future | Datasets | Metrics
    and | State-of-the-art | Vision |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 调查 | 年份 | 白盒 & | 调查 | 分组 | 未来 | 数据集 | 指标和 | 最新技术 | 视觉 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Black-Box | Comparison | Defenses | Directions | Overview | Architectures
    | Comparison | Transformers |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 黑箱 | 比较 | 防御 | 方向 | 概述 | 架构 | 比较 | 变压器 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| Akhtar and Mian [[23](#bib.bib23)] | 2018 | ✓ | $\times$ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ | $\times$ |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| Akhtar and Mian [[23](#bib.bib23)] | 2018 | ✓ | $\times$ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ | $\times$ |'
- en: '| Qiu et al. [[26](#bib.bib26)] | 2019 | ✓ | $\times$ | $\times$ | $\times$
    | $\times$ | $\times$ | $\times$ | $\times$ |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| Qiu et al. [[26](#bib.bib26)] | 2019 | ✓ | $\times$ | $\times$ | $\times$
    | $\times$ | $\times$ | $\times$ | $\times$ |'
- en: '| Serban et al. [[25](#bib.bib25)] | 2020 | ✓ | $\times$ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ | $\times$ |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Serban et al. [[25](#bib.bib25)] | 2020 | ✓ | $\times$ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ | $\times$ |'
- en: '| Xu et al. [[27](#bib.bib27)] | 2020 | ✓ | $\times$ | ✓ | $\times$ | $\times$
    | $\times$ | $\times$ | $\times$ |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. [[27](#bib.bib27)] | 2020 | ✓ | $\times$ | ✓ | $\times$ | $\times$
    | $\times$ | $\times$ | $\times$ |'
- en: '| Chakraborty et al. [[28](#bib.bib28)] | 2021 | ✓ | $\times$ | $\times$ |
    $\times$ | $\times$ | $\times$ | $\times$ | $\times$ |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| Chakraborty et al. [[28](#bib.bib28)] | 2021 | ✓ | $\times$ | $\times$ |
    $\times$ | $\times$ | $\times$ | $\times$ | $\times$ |'
- en: '| Long et al. [[29](#bib.bib29)] | 2022 | ✓ | ✓ | $\times$ | ✓ | $\times$ |
    $\times$ | $\times$ | $\times$ |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| Long et al. [[29](#bib.bib29)] | 2022 | ✓ | ✓ | $\times$ | ✓ | $\times$ |
    $\times$ | $\times$ | $\times$ |'
- en: '| Liang et al. [[30](#bib.bib30)] | 2022 | ✓ | $\times$ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ | $\times$ |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| Liang et al. [[30](#bib.bib30)] | 2022 | ✓ | $\times$ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ | $\times$ |'
- en: '| Zhou et al. [[31](#bib.bib31)] | 2022 | ✓ | $\times$ | ✓ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| Zhou et al. [[31](#bib.bib31)] | 2022 | ✓ | $\times$ | ✓ | ✓ | ✓ | $\times$
    | $\times$ | $\times$ |'
- en: '| This survey | 2023 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 本调查 | 2023 | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: Long et al. [[29](#bib.bib29)] discusses a set of preliminary concepts of Computer
    Vision and adversarial context, providing a set of adversarial attacks grouped
    by adversary goals and capabilities. Finally, the authors provide a set of research
    directions that readers can use to continue the development of robust networks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Long et al. [[29](#bib.bib29)] 讨论了计算机视觉和对抗性背景的一组初步概念，提供了一组按对手目标和能力分组的对抗性攻击。最后，作者提供了一系列研究方向，读者可以利用这些方向继续发展鲁棒网络。
- en: Liang et al. [[30](#bib.bib30)] discuss the most significant attacks and defenses
    in the literature, with the latter being grouped by the underlying technique.
    This review finishes with a presentation of the challenges currently existing
    in the adversarial context.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Liang et al. [[30](#bib.bib30)] 讨论了文献中最重要的攻击和防御，其中防御按基本技术分组。此综述以展示当前存在的对抗性背景中的挑战作为结束。
- en: More recently, Zhou et al. [[31](#bib.bib31)] provides insight into Deep Learning
    and Threat Models, focusing on the Cybersecurity perspective. Therefore, the authors
    identify multiple stages based on Advanced Persistent Threats and explain which
    adversarial attacks are adequate for each stage. Similarly, the same structure
    is followed to present the appropriate defenses for each stage. Furthermore, this
    survey presents the commonly used datasets in adversarial settings and provides
    a set of future directions from a Cybersecurity perspective.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，Zhou et al. [[31](#bib.bib31)] 对深度学习和威胁模型提供了洞察，重点关注网络安全的视角。因此，作者基于高级持续威胁（APT）确定了多个阶段，并解释了哪些对抗性攻击适合每个阶段。同样，遵循相同的结构来展示每个阶段的适当防御。此外，本调查展示了对抗性设置中常用的数据集，并提供了从网络安全视角出发的一系列未来方向。
- en: 'From the analysis of the previous surveys, some concepts have already been
    standardized, such as adversary goals and capabilities and the existence of adversarial
    attacks and defenses. However, due to the recent inception of this area, there
    still needs to be more standardization in datasets and metrics. Therefore, with
    this survey, we also analyze datasets and metrics to provide insight to novice
    researchers. Furthermore, this survey consolidates the state-of-the-art results
    and identifies which datasets can be further explored. Finally, similarly to other
    reviews, this paper provides a set of future directions that researchers and practitioners
    can follow to start their work. A comparison between the several surveys discussed
    in this section is summarized in Table [II](#S3.T2 "TABLE II ‣ III Related Surveys
    ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses").'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '从先前调查的分析来看，一些概念已经标准化，例如对手目标和能力以及对抗攻击和防御的存在。然而，由于这一领域的近期兴起，数据集和指标的标准化仍然不足。因此，通过本次调查，我们还分析了数据集和指标，以为新手研究人员提供见解。此外，本次调查汇总了最新的研究成果，并确定了哪些数据集可以进一步探索。最后，类似于其他评审文章，本文提供了一系列未来方向，供研究人员和从业者在开展工作时参考。有关本节中讨论的几项调查的比较总结在表[II](#S3.T2
    "TABLE II ‣ III Related Surveys ‣ How Deep Learning Sees the World: A Survey on
    Adversarial Attacks & Defenses")中。'
- en: IV Adversarial Attacks
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 对抗攻击
- en: 'Adversarial attacks are commonly divided by the amount of knowledge the adversaries
    have access to, white-box and black-box, as can be seen in Figure [5](#S4.F5 "Figure
    5 ‣ IV Adversarial Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '对抗攻击通常根据对手的知识量进行分类，分为白盒和黑盒，如图[5](#S4.F5 "Figure 5 ‣ IV Adversarial Attacks
    ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses")所示。'
- en: '![Refer to caption](img/e45b9c3e238a2921889e24c406a08ef5.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅标题](img/e45b9c3e238a2921889e24c406a08ef5.png)'
- en: 'Figure 5: Schematic overview of an Adversarial Attack under White-box Settings
    (left) and Black-box Settings (right). The first one uses the classifier predictions
    and network gradients to create perturbations (similar to noise), which can fool
    this classifier. These perturbations are added to the original images, creating
    adversarial images, which are fed to the network and cause misclassification.
    In the Black-box Settings, the same process is applied to a known classifier,
    and the obtained images are used to attack another classifier (represented as
    Target Architecture).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：对抗攻击在白盒设置（左）和黑盒设置（右）下的示意图。第一个图使用分类器的预测和网络梯度来生成扰动（类似于噪声），这些扰动可以欺骗分类器。这些扰动被添加到原始图像中，形成对抗图像，然后输入到网络中导致分类错误。在黑盒设置中，相同的过程应用于已知分类器，得到的图像用于攻击另一个分类器（表示为目标架构）。
- en: IV-A White-box Settings
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 白盒设置
- en: Adversarial examples were first proposed by Szegedy et al. [[14](#bib.bib14)],
    which discovered that DNNs do not generalize well in the vicinity of an input.
    The same authors proposed L-BFGS, the first adversarial attack, to create adversarial
    examples and raised awareness in the scientific community for this generalization
    problem.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本最早由Szegedy等人提出[[14](#bib.bib14)]，他们发现深度神经网络在输入附近的泛化能力较差。相同的作者提出了L-BFGS，第一种对抗攻击方法，用于生成对抗样本，并在科学界引起了对这一泛化问题的关注。
- en: 'Fast Gradient Sign Method (FGSM) [[32](#bib.bib32)] is a one-step method to
    find adversarial examples, which is based on the linear explanation for the existence
    of adversarial examples, and is calculated using the model cost function, the
    gradient, and the radius epsilon. This attack is formally defined as:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 快速梯度符号方法（FGSM）[[32](#bib.bib32)]是一种一步法，用于寻找对抗样本，基于对抗样本存在的线性解释，使用模型成本函数、梯度和半径ε进行计算。这种攻击形式上定义为：
- en: '|  | $x-\epsilon\cdot\text{sign}(\nabla\text{loss}_{F,t}(x)),$ |  | (6) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $x-\epsilon\cdot\text{sign}(\nabla\text{loss}_{F,t}(x)),$ |  | (6) |'
- en: where $x$ is the original image, $\epsilon$ is the amount of changes to the
    image, and $t$ is the target label. The value for $\epsilon$ should be very small
    to make the attack undetectable.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x$ 是原始图像，$\epsilon$ 是对图像的变化量，$t$ 是目标标签。$\epsilon$ 的值应非常小，以使攻击不可检测。
- en: Jacobian-based Saliency Maps (JSM) [[15](#bib.bib15)] explore the forward derivates
    to calculate the model gradients, replacing the gradient descent approaches, and
    discover which input regions are likely to yield adversarial examples. Then it
    uses saliency maps to construct the adversarial saliency maps, which display the
    features the adversary must perturb. Finally, to prove the effectiveness of JSM,
    only the adversarial examples correctly classified by humans were used to fool
    neural networks.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 基于雅可比矩阵的显著性图（JSM）[[15](#bib.bib15)] 探索前向导数来计算模型梯度，替代梯度下降方法，并发现哪些输入区域可能产生对抗样本。然后，它使用显著性图来构建对抗显著性图，显示对手必须扰动的特征。最后，为了证明JSM的有效性，只使用那些被人类正确分类的对抗样本来欺骗神经网络。
- en: DeepFool [[21](#bib.bib21)] is an iterative attack that stops when the minimal
    perturbation that alters the model output is found, exploiting its decision boundaries.
    It finds the minimum perturbation for an input $x_{0}$, corresponding to the vector
    orthogonal to the hyperplane representing the decision boundary.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: DeepFool [[21](#bib.bib21)] 是一种迭代攻击，当找到最小扰动并改变模型输出时停止，利用其决策边界。它为输入$x_{0}$找到最小扰动，对应于与表示决策边界的超平面正交的向量。
- en: Kurakin et al. [[33](#bib.bib33)] was the first to demonstrate that adversarial
    examples can also exist in the physical world, by using three different methods
    to generate the adversarial examples. Basic Iterative Method (BIM) applies the
    FGSM multiple times with a small step size between iterations and clips the intermediate
    values after each step. Iterative Least-likely Class Method (ILCM) uses the least-likely
    class, according to the prediction of the model, as the target class and uses
    BIM to calculate the adversarial example that outputs the target class.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Kurakin 等人 [[33](#bib.bib33)] 首次证明了对抗样本也可以存在于物理世界中，使用三种不同的方法生成对抗样本。基本迭代方法（BIM）多次应用FGSM，每次迭代之间的步长较小，并在每一步后裁剪中间值。迭代最不可能类别方法（ILCM）根据模型的预测，使用最不可能类别作为目标类别，并使用BIM计算输出目标类别的对抗样本。
- en: 'Carlini and Wagner (C&W) [[34](#bib.bib34)] attack is one of the most powerful
    attacks, which uses three different vector norms: 1) the $L_{2}$ attack uses a
    smoothing of clipped gradient descent approach, displaying low distortion; 2)
    the $L_{0}$ attack uses an iterative algorithm that, at each iteration, fixes
    the pixels that do not have much effect on the classifier and finds the minimum
    amount of pixels that need to be altered; and 3) the $L_{\infty}$ attack also
    uses an iterative algorithm with an associated penalty, penalizing every perturbation
    that exceeds a predefined value, formally defined as:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Carlini 和 Wagner（C&W）[[34](#bib.bib34)] 攻击是最强大的攻击之一，它使用三种不同的向量范数：1) $L_{2}$
    攻击使用裁剪梯度下降方法的平滑，显示出低失真；2) $L_{0}$ 攻击使用一种迭代算法，在每次迭代中，修复对分类器影响不大的像素，并找到需要改变的最小像素数量；3)
    $L_{\infty}$ 攻击也使用带有相关惩罚的迭代算法，对每个超过预定义值的扰动进行惩罚，正式定义如下：
- en: '|  | $\text{min}\quad c\cdot f(x+\delta)+\sum_{i}[(\delta_{i}-\tau)^{+}],$
    |  | (7) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{min}\quad c\cdot f(x+\delta)+\sum_{i}[(\delta_{i}-\tau)^{+}],$
    |  | (7) |'
- en: where $\delta$ is the perturbation, $\tau$ is the penalty threshold (initially
    1, decreasing in each iteration), and $c$ is a constant. The value for $c$ starts
    as a very low value (e.g., $10^{-4}$), and each time the attack fails, the value
    for $c$ is doubled. If $c$ exceeds a threshold (e.g., $10^{10}$), it aborts the
    search.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\delta$ 是扰动，$\tau$ 是惩罚阈值（初始值为1，每次迭代时减少），$c$ 是一个常数。$c$ 的值从一个非常低的值开始（例如，$10^{-4}$），每次攻击失败时，$c$
    的值翻倍。如果$c$ 超过阈值（例如，$10^{10}$），则终止搜索。
- en: Gradient Aligned Adversarial Subspace (GAAS) [[35](#bib.bib35)] is an attack
    that directly estimates the dimensionality of the adversarial subspace using the
    first-order approximation of the loss function. Through the experiments, GAAS
    proved the most successful at finding many orthogonal attack directions, indicating
    that neural networks generalize linearly.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度对齐对抗子空间（GAAS）[[35](#bib.bib35)] 是一种直接使用损失函数的一阶近似来估计对抗子空间维度的攻击。通过实验，GAAS证明在找到许多正交攻击方向方面最成功，这表明神经网络线性泛化。
- en: 'Projected Gradient Descent (PGD) [[36](#bib.bib36)] is an iterative attack
    that uses saddle point formulation, viewed as an inner maximization problem and
    an outer minimization problem, to find a strong perturbation. It uses the inner
    maximization problem to find an adversarial version of a given input that achieves
    a high loss and the outer minimization problem to find model parameters that minimize
    the loss in the inner maximization problem. The saddle point problem used by PGD
    is defined as:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 投影梯度下降（PGD）[[36](#bib.bib36)] 是一种迭代攻击方法，使用鞍点公式，将其视为内层最大化问题和外层最小化问题，以寻找强扰动。它利用内层最大化问题来找到实现高损失的对抗版本输入，而外层最小化问题则找到最小化内层最大化问题损失的模型参数。PGD
    使用的鞍点问题定义为：
- en: '|  | $\min_{\theta}\rho(\theta),\text{where}~{}\rho(\theta)=\mathbb{E}_{(x,y)\sim\mathcal{D}}\Bigl{[}\max_{\delta\in\mathcal{S}}L(\theta,x+\delta,y)\Bigr{]},$
    |  | (8) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\theta}\rho(\theta),\text{where}~{}\rho(\theta)=\mathbb{E}_{(x,y)\sim\mathcal{D}}\Bigl{[}\max_{\delta\in\mathcal{S}}L(\theta,x+\delta,y)\Bigr{]},$
    |  | (8) |'
- en: where $x$ is the original image, $y$ is the corresponding label, and $\mathcal{S}$
    is the set of allowed perturbations.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x$ 是原始图像，$y$ 是对应的标签，$\mathcal{S}$ 是允许的扰动集合。
- en: AdvGAN [[37](#bib.bib37)] uses Generative Adversarial Networks (GAN) [[38](#bib.bib38)]
    to create adversarial examples that are realistic and have high attack success
    rate. The generator receives the original instance and creates a perturbation,
    the discriminator distinguishes the original instance from the perturbed instance,
    and the target neural network is used to measure the distance between the prediction
    and the target class.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: AdvGAN [[37](#bib.bib37)] 使用生成对抗网络（GAN）[[38](#bib.bib38)] 来创建现实且具有高攻击成功率的对抗样本。生成器接收原始实例并创建扰动，判别器区分原始实例和扰动实例，目标神经网络用于衡量预测与目标类别之间的距离。
- en: Motivated by the inability to achieve a high success rate in black-box settings,
    the Momentum Iterative FGSM (MI-FGSM) [[39](#bib.bib39)] was proposed. It introduces
    momentum, a technique for accelerating gradient descent algorithms, into the already
    proposed Iterative FGSM (I-FGSM), showing that the attack success rate in black-box
    settings increases almost double that of previous attacks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 受到在黑箱设置中无法实现高成功率的激励，提出了动量迭代 FGSM（MI-FGSM）[[39](#bib.bib39)]。它引入了动量，这是一种加速梯度下降算法的技术，应用于已提出的迭代
    FGSM（I-FGSM），显示在黑箱设置中攻击成功率几乎是以前攻击的两倍。
- en: Croce and Hein [[40](#bib.bib40)] noted that the perturbations generated by
    $l_{0}$ attacks are sparse and by $l_{\infty}$ attacks are smooth on all pixels,
    proposing Sparse and Imperceivable Adversarial Attacks (SIAA). This attack creates
    sporadic and imperceptible perturbations by applying the standard deviation of
    each color channel in both axis directions, calculated using the two immediate
    neighboring pixels and the original pixel.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Croce 和 Hein [[40](#bib.bib40)] 指出，$l_{0}$ 攻击生成的扰动是稀疏的，而 $l_{\infty}$ 攻击生成的扰动在所有像素上是平滑的，因此提出了稀疏且不可察觉的对抗攻击（SIAA）。这种攻击通过应用每个颜色通道在两个方向上的标准偏差来创建零星且不可察觉的扰动，这些偏差是通过使用两个相邻像素和原始像素计算得出的。
- en: SmoothFool (SF) [[22](#bib.bib22)] is a geometry-inspired framework for computing
    smooth adversarial perturbations, exploiting the decision boundaries of a model.
    It is an iterative algorithm that uses DeepFool to calculate the initial perturbation
    and smoothly rectifies the resulting perturbation until the adversarial example
    fools the classifier. This attack provides smoother perturbations which improve
    the transferability of the adversarial examples, and their impact varies with
    the different categories in a dataset.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: SmoothFool (SF) [[22](#bib.bib22)] 是一个受几何启发的框架，用于计算平滑对抗扰动，利用模型的决策边界。它是一个迭代算法，使用
    DeepFool 计算初始扰动，并平滑修正结果扰动，直到对抗样本欺骗分类器。这种攻击提供了更平滑的扰动，从而提高了对抗样本的迁移性，其影响因数据集中不同类别的变化而异。
- en: In the context of exploring the adversarial examples in the physical world,
    the Adversarial Camouflage (AdvCam) [[41](#bib.bib41)], which crafts physical-world
    adversarial examples that are legitimate to human observers, was proposed. It
    uses the target image, region, and style to perform a physical adaptation (creating
    a realistic adversarial example), which is provided into a target neural network
    to evaluate the success rate of the adversarial example.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索物理世界中的对抗样本的背景下，提出了对抗伪装（AdvCam） [[41](#bib.bib41)]，它制作对人体观察者合法的物理世界对抗样本。它使用目标图像、区域和风格进行物理适应（创建现实的对抗样本），然后输入目标神经网络以评估对抗样本的成功率。
- en: Feature Importance-aware Attack (FIA) [[42](#bib.bib42)] considers the object-aware
    features that dominate the model decisions, using the aggregate gradient (gradients
    average concerning the feature maps). This approach avoids local optimum, represents
    transferable feature importance, and uses the aggregate gradient to assign weights
    identifying the essential features. Furthermore, FIA generates highly transferable
    adversarial examples when extracting the feature importance from multiple classification
    models.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性感知攻击（FIA） [[42](#bib.bib42)] 考虑了主导模型决策的对象感知特征，使用汇总梯度（相对于特征图的梯度平均）。这种方法避免了局部最优，表示可迁移的特征重要性，并使用汇总梯度来分配权重以识别关键特征。此外，FIA
    在从多个分类模型中提取特征重要性时生成高度可迁移的对抗样本。
- en: Meta Gradient Adversarial Attack (MGAA) [[43](#bib.bib43)] is a novel architecture
    that can be integrated into any existing gradient-based attack method to improve
    cross-model transferability. This approach consists of multiple iterations, and,
    in each iteration, various models are samples from a model zoo to generate adversarial
    perturbations using the selected model, which are added to the previously generated
    perturbations. In addition, using multiple models simulates both white- and black-box
    settings, making the attacks more successful.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 元梯度对抗攻击（MGAA） [[43](#bib.bib43)] 是一种新型架构，可以集成到任何现有的基于梯度的攻击方法中，以提高跨模型的迁移性。这种方法包含多个迭代，在每次迭代中，从模型库中抽样各种模型，利用所选模型生成对抗扰动，并将其添加到先前生成的扰动中。此外，使用多个模型可以模拟白盒和黑盒设置，使攻击更加成功。
- en: IV-B Universal Adversarial Perturbations
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 通用对抗扰动
- en: 'Moosavi-Dezfooli et al. [[44](#bib.bib44)] discovered that some perturbations
    are image-agnostic (universal) and cause misclassification with high probability,
    labeled as Universal Adversarial Perturbations (UAPs). The authors found that
    these perturbations also generalize well across multiple neural networks, by searching
    for a vector of perturbations that cause misclassification in almost all the data
    drawn from a distribution of images. The optimization problem that Moosavi-Dezfooli
    et al. are trying to solve is the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Moosavi-Dezfooli 等人 [[44](#bib.bib44)] 发现一些扰动是图像无关的（通用的），并且以高概率导致误分类，这些扰动被称为通用对抗扰动（UAPs）。作者发现，这些扰动在多个神经网络中也能很好地泛化，通过寻找一个扰动向量来导致几乎所有从图像分布中抽取的数据的误分类。Moosavi-Dezfooli
    等人试图解决的优化问题如下：
- en: '|  | $\Delta v_{i}\xleftarrow{}\textrm{arg}\min_{r}\&#124;r\&#124;_{2}\quad\textrm{s.t.}\quad\hat{k}(x_{i}+v+r)\neq\hat{k}(x_{i}),$
    |  | (9) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Delta v_{i}\xleftarrow{}\textrm{arg}\min_{r}\&#124;r\&#124;_{2}\quad\textrm{s.t.}\quad\hat{k}(x_{i}+v+r)\neq\hat{k}(x_{i}),$
    |  | (9) |'
- en: where $\Delta v_{i}$ is the minimal perturbation to fool the classifier, $v$
    is the universal perturbation, and $x_{i}$ is the original image. This optimization
    problem is calculated for each image in a dataset, and the vector containing the
    universal perturbation is updated.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Delta v_{i}$ 是欺骗分类器的最小扰动，$v$ 是通用扰动，$x_{i}$ 是原始图像。这个优化问题对数据集中的每张图像进行计算，并更新包含通用扰动的向量。
- en: The Universal Adversarial Networks (UAN) [[45](#bib.bib45)] are Generative Networks
    that are capable of fooling a classifier when their output is added to an image.
    These networks were inspired by the discovery of UAPs, which were used as the
    training set and can create perturbations for any given input, demonstrating more
    outstanding results than the original UAPs.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通用对抗网络（UAN） [[45](#bib.bib45)] 是生成网络，它们能够欺骗分类器，当其输出添加到图像中时。这些网络受到 UAPs 发现的启发，使用
    UAPs 作为训练集，并且能够为任何给定的输入创建扰动，展示了比原始 UAPs 更优秀的结果。
- en: IV-C Black-box Settings
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 黑盒设置
- en: 'Specifically considering black-box setup, Ilyas et al. [[46](#bib.bib46)] define
    three realistic threat models that are more faithful to real-world settings: query-limited,
    partial information, and label-only settings. The first one suggests the development
    of query-efficient algorithms, using Natural Evolutionary Strategies to estimate
    the gradients used to perform the PGD attack. When only having the probabilities
    for the top-k labels, the algorithm alternates between blending in the original
    image and maximizing the likelihood of the target class and, when the attacker
    only obtains the top-k predicted labels, the attack uses noise robustness to mount
    a targeted attack.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 特别考虑黑盒设置，Ilyas 等人 [[46](#bib.bib46)] 定义了三种更符合现实世界设置的现实威胁模型：查询有限、部分信息和仅标签设置。第一个模型建议开发查询高效的算法，使用自然进化策略来估计用于执行
    PGD 攻击的梯度。当仅有前 k 个标签的概率时，算法在混合原始图像和最大化目标类别的可能性之间交替进行；当攻击者仅获得前 k 个预测标签时，攻击使用噪声鲁棒性进行针对性攻击。
- en: Feature-Guided Black-Box (FGBB) [[47](#bib.bib47)] uses the features extracted
    from images to guide the creation of adversarial perturbations, by using Scale
    Invariant Feature Transform. High probability is assigned to pixels that impact
    the composition of an image in the Human visual system and the creation of adversarial
    examples is viewed as a two-player game, where the first player minimizes the
    distance to an adversarial example, and the second one can have different roles,
    leading to minimal adversarial examples.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Feature-Guided Black-Box (FGBB) [[47](#bib.bib47)] 使用从图像中提取的特征来指导对抗扰动的创建，通过使用尺度不变特征转换。对影响图像组成的像素赋予高概率，并且对抗样本的创建被视为一个双人游戏，其中第一个玩家最小化与对抗样本的距离，而第二个玩家可以有不同的角色，从而导致最小的对抗样本。
- en: Square Attack [[48](#bib.bib48)] is an adversarial attack that does not need
    local gradient information, meaning that gradient masking does not affect it.
    Furthermore, this attack uses a randomized search scheme that selects localized
    square-shaped updates in random positions, causing the perturbation to be situated
    at the decision boundaries.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Square Attack [[48](#bib.bib48)] 是一种不需要局部梯度信息的对抗攻击，这意味着梯度屏蔽不会影响它。此外，这种攻击使用了一种随机搜索方案，选择随机位置的局部化方形更新，使得扰动位于决策边界上。
- en: IV-D Auto-Attack
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D Auto-Attack
- en: 'Auto-Attack [[49](#bib.bib49)] was proposed to test adversarial robustness
    in a parameter-free, computationally affordable, and user-independent way. As
    such, Croce et al. proposed two variations of PGD to overcome suboptimal step
    sizes of the objective function, namely APGD-CE and APGD-DLR, for a step size-free
    version of PGD using cross-entropy (CE) and Difference of Logits Ratio (DLR) loss,
    respectively. DLR is a loss proposed by Croce et al. which is both shift and rescaling
    invariant and thus has the same degrees of freedom as the decision of the classifier,
    not suffering from the issues of the cross-entropy loss [[49](#bib.bib49)]. Then,
    they combine these new PGD variations with two other existing attacks to create
    Auto-Attack, which is composed by:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Auto-Attack [[49](#bib.bib49)] 被提出用于以一种无参数、计算成本低且不依赖用户的方式测试对抗鲁棒性。因此，Croce 等人提出了
    PGD 的两个变体来克服目标函数的次优步长，即 APGD-CE 和 APGD-DLR，用于 PGD 的无步长版本，分别使用交叉熵 (CE) 和对数比率差异
    (DLR) 损失。DLR 是 Croce 等人提出的一种损失函数，它在平移和缩放方面都是不变的，因此与分类器的决策具有相同的自由度，不会受到交叉熵损失的影响
    [[49](#bib.bib49)]。然后，他们将这些新的 PGD 变体与另外两种现有攻击方法结合，创建了 Auto-Attack，其组成包括：
- en: •
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: APGD-CE, step size-free version of PGD on the cross-entropy;
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: APGD-CE，交叉熵上的无步长版本的 PGD；
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: APGD-DLR, step size-free version of PGD on the DLR loss;
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: APGD-DLR，DLR 损失上的无步长版本的 PGD；
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fast Adaptive Boundary (FAB) [[50](#bib.bib50)], which minimizes the norm of
    the adversarial perturbations;
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Fast Adaptive Boundary (FAB) [[50](#bib.bib50)]，它最小化对抗扰动的范数；
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Square [[48](#bib.bib48)] Attack, a query-efficient black-box attack.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Square [[48](#bib.bib48)] Attack，一种查询高效的黑盒攻击。
- en: Given the main motivation of the Auto-Attack proposal, the FAB attack is the
    targeted version of FAB [[50](#bib.bib50)] since the untargeted version computes
    each iteration of the Jacobian matrix of the classifier, which scales linearly
    with the number of classes of the dataset. Although this is feasible for datasets
    with a low number of classes (e.g., MNIST and CIFAR-10), it becomes both computationally
    and memory-wise challenging with an increased number of classes (e.g., CIFAR-100
    and ImageNet).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于 Auto-Attack 提案的主要动机，FAB 攻击是 FAB [[50](#bib.bib50)] 的有目标版本，因为无目标版本计算分类器的雅可比矩阵的每次迭代，该矩阵的计算量与数据集的类别数量线性相关。虽然这对于类别数量较少的数据集（例如
    MNIST 和 CIFAR-10）是可行的，但随着类别数量的增加（例如 CIFAR-100 和 ImageNet），它在计算和内存方面都会变得具有挑战性。
- en: 'As such, Auto-Attack is an ensemble of attacks with important fundamental properties:
    APGD is a white-box attack aiming at any adversarial example within an $L_{p}$-ball
    (Section [II-C](#S2.SS3 "II-C Vector Norms and ϵ Constraint ‣ II Background for
    Adversarial Attacks ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses")), FAB minimizes the norm of the perturbation necessary to
    achieve a misclassification, and Square Attack is a score-based black-box attack
    for norm bounded perturbations which use random search and do not exploit any
    gradient approximation, competitive with white-box attacks [[48](#bib.bib48)].'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，Auto-Attack 是一个具有重要基本属性的攻击集：APGD 是一种白箱攻击，旨在针对 $L_{p}$-球体内的任何对抗样本（见第 [II-C](#S2.SS3
    "II-C 向量范数和 ϵ 约束 ‣ II 对抗攻击的背景 ‣ 深度学习如何看待世界：对抗攻击与防御的调查") 节），FAB 通过最小化实现错误分类所需的扰动范数，而
    Square Attack 是一种基于评分的黑箱攻击，用于范数约束扰动，采用随机搜索且不利用任何梯度近似，与白箱攻击 [[48](#bib.bib48)]
    竞争。
- en: V Adversarial Defenses
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 对抗防御
- en: V-A Adversarial Training
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-A 对抗训练
- en: 'Szegedy et al. [[14](#bib.bib14)] proposed that training on a mixture of adversarial
    and clean examples could regularize a neural network, as shown in Figure [6](#S5.F6
    "Figure 6 ‣ V-A Adversarial Training ‣ V Adversarial Defenses ‣ How Deep Learning
    Sees the World: A Survey on Adversarial Attacks & Defenses"). Goodfellow et al. [[32](#bib.bib32)]
    evaluated the impact of Adversarial Training as a regularizer by including it
    in the objective function, showing that this approach is a reliable defense that
    can be applied to every neural network.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Szegedy 等人 [[14](#bib.bib14)] 提出了在对抗样本和干净样本的混合上训练可以对神经网络进行正则化，如图 [6](#S5.F6
    "图 6 ‣ V-A 对抗训练 ‣ V 对抗防御 ‣ 深度学习如何看待世界：对抗攻击与防御的调查") 所示。Goodfellow 等人 [[32](#bib.bib32)]
    通过将对抗训练纳入目标函数来评估其作为正则化器的影响，显示这种方法是一种可靠的防御，可以应用于每个神经网络。
- en: Kurakin et al. [[51](#bib.bib51)] demonstrates that it is possible to perform
    adversarial training in more massive datasets (ImageNet), displaying that the
    robustness significantly increases for one-step methods. When training the model
    with one-step attacks using the ground-truth labels, the model has significantly
    higher accuracy on the adversarial images than on the clean images, an effect
    denominated as Label Leaking, suggesting that the adversarial training should
    not make use of the ground-truth labels.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Kurakin 等人 [[51](#bib.bib51)] 证明在更大规模的数据集（ImageNet）上进行对抗训练是可能的，显示出一阶方法的鲁棒性显著增加。当使用真实标签进行一阶攻击训练模型时，模型在对抗图像上的准确率显著高于在干净图像上的准确率，这种现象被称为标签泄露，表明对抗训练不应使用真实标签。
- en: Adversarial Training in large datasets implies using fast single-step methods,
    which converge to a degenerate global minimum, meaning that models trained with
    this technique remain vulnerable to black-box attacks. Therefore, Ensemble Adversarial
    Training [[52](#bib.bib52)] uses adversarial examples crafted on other static
    pre-trained models to augment the training data, preventing the trained model
    from influencing the strength of the adversarial examples.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据集上进行对抗训练意味着使用快速的一阶方法，这些方法收敛到一个退化的全局最小值，这意味着使用这种技术训练的模型仍然容易受到黑箱攻击。因此，集成对抗训练 [[52](#bib.bib52)]
    使用在其他静态预训练模型上制作的对抗样本来增强训练数据，防止训练后的模型影响对抗样本的强度。
- en: Shared Adversarial Training [[53](#bib.bib53)] is an extension of adversarial
    training aiming to maximize robustness against universal perturbations. It splits
    the mini-batch of images used in training into a set of stacks and obtains the
    loss gradients concerning these stacks. Afterward, the gradients for each stack
    are processed to create a shared perturbation that is applied to the whole stack.
    After every iteration, these perturbations are added and clipped to constrain
    them into a predefined magnitude. Finally, these perturbations are added to the
    images and used for adversarial training.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 共享对抗训练 [[53](#bib.bib53)] 是对抗训练的一种扩展，旨在最大化对普遍扰动的鲁棒性。它将训练中使用的迷你批次图像分割成一组堆叠，并获得关于这些堆叠的损失梯度。之后，对每个堆叠的梯度进行处理，生成应用于整个堆叠的共享扰动。每次迭代后，这些扰动会被累加并裁剪，以限制它们到预定义的幅度。最后，这些扰动会被添加到图像中，用于对抗训练。
- en: TRadeoff-inspired Adversarial DEfense via Surrogate-loss minimization (TRADES) [[54](#bib.bib54)]
    is inspired by the presumption that robustness can be at odds with accuracy [[55](#bib.bib55),
    [56](#bib.bib56)]. The authors show that the robust error can be tightly bounded
    by using natural error measured by the surrogate loss function and the likelihood
    of input features being close to the decision boundary (boundary error). These
    assumptions make the model weights biased toward natural or boundary errors.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 受到权衡灵感启发的对抗防御通过替代损失最小化（TRADES） [[54](#bib.bib54)] 的灵感来源于鲁棒性可能与准确性相矛盾的假设 [[55](#bib.bib55),
    [56](#bib.bib56)]。作者展示了通过使用由替代损失函数测量的自然误差和输入特征接近决策边界的可能性（边界误差）可以严格界定鲁棒误差。这些假设使得模型权重偏向自然误差或边界误差。
- en: '![Refer to caption](img/edc89bab58bccb9668a5779c37218162.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/edc89bab58bccb9668a5779c37218162.png)'
- en: 'Figure 6: Schematic overview of Adversarial Training. A subset of the original
    images of a dataset is fed into an adversarial attack (e.g., PGD, FGSM, or C&W),
    which creates adversarial images. Each batch contains original and adversarial
    images, with the Classifier being normally trained.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：对抗训练的示意图。数据集中原始图像的一个子集被送入对抗攻击（例如，PGD、FGSM 或 C&W），生成对抗图像。每个批次包含原始图像和对抗图像，分类器则进行正常训练。
- en: Based on the idea that gradient magnitude is directly linked to model robustness,
    Bilateral Adversarial Training (BAT) [[57](#bib.bib57)] proposes to perturb not
    only the images but also the manipulation of labels (adversarial labels) during
    the training phase. The adversarial labels are derived from a closed-form heuristic
    solution, and the adversarial images are generated from a one-step targeted attack.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度幅度与模型鲁棒性直接相关的想法，双边对抗训练（BAT） [[57](#bib.bib57)] 提出了在训练阶段不仅扰动图像，还对标签（对抗标签）进行操作。对抗标签源自闭式启发式解法，对抗图像则通过一步目标攻击生成。
- en: Despite the popularity of adversarial training to defend models, it has a high
    cost of generating strong adversarial examples, namely for large datasets such
    as ImageNet. Therefore, Free Adversarial Training (Free-AT) [[58](#bib.bib58)]
    uses the gradient information when updating model parameters to generate the adversarial
    examples, eliminating the previously mentioned overhead.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管对抗训练在防御模型中很受欢迎，但生成强对抗样本的成本很高，特别是对于像 ImageNet 这样的庞大数据集。因此，自由对抗训练（Free-AT） [[58](#bib.bib58)]
    使用梯度信息在更新模型参数时生成对抗样本，从而消除了上述开销。
- en: Considering the same issue presented in Free-AT, the authors analyze Pontryagin’s
    Maximum Principle [[59](#bib.bib59)] of this problem and observe that the adversary
    update is only related to the first layer of the network. Thus, You Only Propagate
    Once (YOPO) [[60](#bib.bib60)] only considers the first layer of the network for
    forward and backpropagation, effectively reducing the amount of propagation to
    one in each update.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 针对 Free-AT 中提出的相同问题，作者分析了这个问题的庞特里亚金最大值原理 [[59](#bib.bib59)]，并观察到对抗更新仅与网络的第一层相关。因此，“只传播一次”（YOPO） [[60](#bib.bib60)]
    仅考虑网络的第一层进行前向和反向传播，有效减少了每次更新的传播量。
- en: Misclassification Aware adveRsarial Training (MART) [[61](#bib.bib61)] is an
    algorithm that explicitly differentiates the misclassified and correctly classified
    examples during training. This proposal is motivated by the finding that different
    maximization techniques are negligible, but minimization ones are crucial when
    looking at the misclassified examples.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 误分类感知对抗训练（MART）[[61](#bib.bib61)]是一种算法，在训练过程中明确区分误分类和正确分类的样本。这一提案的动机是发现不同的最大化技术微不足道，但在查看误分类样本时，最小化技术至关重要。
- en: Defense against Occlusion Attacks (DOA) [[62](#bib.bib62)] is a defense mechanism
    that uses abstract adversarial attacks, Rectangular Occlusion Attack (ROA) [[62](#bib.bib62)],
    and applies the standard adversarial training. This attack considers including
    physically realizable attacks that are “normal” in the real world, such as eyeglasses
    and stickers on stop signs.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 针对遮挡攻击的防御（DOA）[[62](#bib.bib62)]是一种防御机制，使用抽象对抗攻击、矩形遮挡攻击（ROA）[[62](#bib.bib62)]，并应用标准对抗训练。这种攻击考虑包括在现实世界中“正常”的物理可实现攻击，如眼镜和停车标志上的贴纸。
- en: The proposal of Smooth Adversarial Training (SAT) [[63](#bib.bib63)] considers
    the evolution normally seen in curriculum learning, where the difficulty increases
    with time (age), using two difficulty metrics. These metrics are based on the
    maximal Hessian eigenvalue (H-SAT) and the softmax Probability (P-SAT), which
    are used to stabilize the networks for large perturbations while having high clean
    accuracy. In the same context, Friendly Adversarial Training (Friend-AT) [[64](#bib.bib64)]
    minimizes the loss considering the least adversarial data (friendly) among the
    adversarial data that is confidently misclassified. This method can be employed
    by early stopping PGD attacks when performing adversarial training.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑对抗训练（SAT）[[63](#bib.bib63)]的提案考虑了通常在课程学习中看到的演变，即难度随时间（年龄）增加，使用两个难度度量标准。这些度量标准基于最大Hessian特征值（H-SAT）和softmax概率（P-SAT），用于在存在大扰动的情况下稳定网络，同时保持高的干净准确率。在同一背景下，友好对抗训练（Friend-AT）[[64](#bib.bib64)]在对抗数据中最少考虑自信误分类的对抗数据（友好），以最小化损失。这种方法可以通过在进行对抗训练时提前停止PGD攻击来应用。
- en: Contrary to the idea of Free-AT [[58](#bib.bib58)], Cheap Adversarial Training
    (Cheap-AT) [[65](#bib.bib65)] proposes the use of weaker and cheaper adversaries
    (FGSM) combined with random initialization to train robust networks effectively.
    This method can be further accelerated by applying techniques that efficiently
    train networks.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 与Free-AT [[58](#bib.bib58)]的观点相反，便宜对抗训练（Cheap-AT）[[65](#bib.bib65)]提出使用较弱且更便宜的对抗者（FGSM）结合随机初始化来有效地训练鲁棒网络。通过应用高效训练网络的技术，这种方法可以进一步加速。
- en: In a real-world context, the attacks are not limited by the imperceptibility
    constraint ($\epsilon$ value); there are, in fact, multiple perturbations (for
    models) that have visible sizes. The main idea of Oracle-Aligned Adversarial Training
    (OA-AT) [[66](#bib.bib66)] is to create a model that is robust to high perturbation
    bounds by aligning the network predictions with ones of an Oracle during adversarial
    training. The key aspect of OA-AT is the use of Learned Perceptual Image Patch
    Similarity [[67](#bib.bib67)] to generate Oracle-Invariant attacks and convex
    combination of clean and adversarial predictions as targets for Oracle-Sensitive
    samples.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，攻击不仅仅受限于不可察觉性约束（$\epsilon$值）；实际上，有多种扰动（针对模型）具有可见大小。Oracle对齐对抗训练（OA-AT）[[66](#bib.bib66)]的主要思想是通过在对抗训练期间将网络预测与Oracle的预测对齐，创建一个对高扰动边界鲁棒的模型。OA-AT的关键方面是使用学习的感知图像补丁相似性[[67](#bib.bib67)]生成Oracle不变攻击和清洁与对抗预测的凸组合，作为Oracle敏感样本的目标。
- en: 'Geometry-aware Instance-reweighted Adversarial Training (GI-AT) [[68](#bib.bib68)]
    has two foundations: 1) over-parameterized models still lack capacity; and 2)
    a natural data point closer to the class boundary is less robust, translating
    into assigning the corresponding adversarial data a larger weight. Therefore,
    this defense proposes using standard adversarial training, considering that weights
    are based on how difficult it is to attack a natural data point.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 几何感知实例加权对抗训练（GI-AT）[[68](#bib.bib68)]有两个基础：1）过度参数化的模型仍然缺乏容量；2）更接近类别边界的自然数据点更不鲁棒，这意味着给相应的对抗数据分配更大的权重。因此，这种防御建议使用标准对抗训练，考虑权重是基于攻击自然数据点的难度。
- en: Adversarial training leads to unfounded increases in the margin along decision
    boundaries, reducing clean accuracy. To tackle this issue, Helper-based Adversarial
    Training (HAT) [[69](#bib.bib69)] incorporates additional wrongly labeled examples
    during training, achieving a good trade-off between accuracy and robustness.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗训练导致决策边界上的边距不合理地增加，从而降低了清晰准确率。为了解决这个问题，基于助手的对抗训练（HAT）[[69](#bib.bib69)] 在训练过程中加入了额外的错误标记样本，实现了准确性和鲁棒性之间的良好折中。
- en: As a result of the good results achieved by applying random initialization,
    Fast Adversarial Training (FAT) [[70](#bib.bib70)] performs randomized smoothing
    to optimize the inner maximization problem efficiently, and proposes a new initialization
    strategy, named backward smoothing. This strategy helps to improve the stability
    and robustness of a model using single-step robust training methods, solving the
    overfitting issue.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于随机初始化取得了良好的结果，快速对抗训练（FAT）[[70](#bib.bib70)] 通过随机平滑来有效地优化内在的最大化问题，并提出了一种新的初始化策略，称为反向平滑。该策略有助于提高使用单步鲁棒训练方法的模型的稳定性和鲁棒性，解决了过拟合问题。
- en: V-B Modify the Training Process
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-B 修改训练过程
- en: Gu and Rigazio [[71](#bib.bib71)] proposed using three preprocessing techniques
    to recover from the adversarial noise, namely, noise injection, autoencoder, and
    denoising autoencoder, discovering that the adversarial noise is mainly distributed
    in the high-frequency domain. Solving the adversarial problem corresponds to encountering
    adequate training techniques and objective functions to increase the distortion
    of the smallest adversarial examples.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Gu 和 Rigazio [[71](#bib.bib71)] 提出了使用三种预处理技术来恢复对抗噪声，即噪声注入、自编码器和去噪自编码器，并发现对抗噪声主要分布在高频域。解决对抗问题对应于采用合适的训练技术和目标函数，以增加最小对抗样本的失真。
- en: 'Another defense against adversarial examples is Defensive Distillation [[72](#bib.bib72)],
    which uses the predictions from a previously trained neural network, as displayed
    in Figure [7](#S5.F7 "Figure 7 ‣ V-B Modify the Training Process ‣ V Adversarial
    Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses"). This approach trains the initial neural network with the original
    training data and labels, producing the probability of the predictions, which
    replace the original training labels to train a smaller and resilient distilled
    network. Additionally, to improve the results obtained by Defensive Distillation,
    Papernot and McDaniel [[73](#bib.bib73)] propose to change the vector used to
    train the distilled network by combining the original label with the first model
    uncertainty.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '针对对抗样本的另一种防御方法是防御性蒸馏[[72](#bib.bib72)]，该方法使用从先前训练的神经网络中获得的预测，如图 [7](#S5.F7
    "Figure 7 ‣ V-B Modify the Training Process ‣ V Adversarial Defenses ‣ How Deep
    Learning Sees the World: A Survey on Adversarial Attacks & Defenses")所示。这种方法用原始训练数据和标签训练初始神经网络，生成预测的概率，这些概率替换原始训练标签来训练一个更小、更鲁棒的蒸馏网络。此外，为了改善防御性蒸馏的结果，Papernot
    和 McDaniel [[73](#bib.bib73)] 提出了通过将原始标签与第一个模型不确定性结合来改变用于训练蒸馏网络的向量。'
- en: To solve the vulnerabilities of the neural network to adversarial examples,
    the Visual Causal Feature Learning [[74](#bib.bib74)] method uses causal reasoning
    to perform data augmentation. This approach uses manipulator functions that return
    an image similar to the original one with the desired causal effect.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决神经网络对对抗样本的脆弱性，视觉因果特征学习[[74](#bib.bib74)]方法使用因果推理来进行数据增强。该方法使用操控函数返回与原始图像相似且具有所需因果效果的图像。
- en: Learning with a Strong Adversary [[75](#bib.bib75)] is a training procedure
    that formulates as a min-max problem, making the classifier inherently robust.
    This approach considers that the adversary applies perturbations to each data
    point to maximize the classification error, and the learning procedure attempts
    to minimize the misclassification error against the adversary. The greatest advantage
    of this procedure is the significant increase in robustness while maintaining
    clean high accuracy.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 与强敌学习 [[75](#bib.bib75)]是一种将问题形式化为最小-最大问题的训练过程，使分类器具有固有的鲁棒性。该方法考虑到对手对每个数据点施加扰动以最大化分类错误，而学习过程则试图最小化对对手的误分类错误。这种方法的最大优点是显著提高了鲁棒性，同时保持了高准确率。
- en: Zheng et al. [[76](#bib.bib76)] proposes the use of compression, rescaling,
    and cropping in benign images to increase the stability of DNNs, denominated as
    Image Processing, without changing the objective functions. A Gaussian perturbation
    sampler perturbs the benign image, which is fed to the DNN, and its feature representation
    of benign images is used to 1) minimize the standard CE loss; and 2) minimize
    the stability loss.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: Zheng 等人[[76](#bib.bib76)] 提出了在良性图像中使用压缩、重新缩放和裁剪来提高深度神经网络的稳定性，称之为图像处理，而不改变目标函数。高斯扰动采样器扰动良性图像，该图像被输入到
    DNN 中，并使用其良性图像的特征表示来 1) 最小化标准交叉熵损失；以及 2) 最小化稳定性损失。
- en: '![Refer to caption](img/d57ab39871f2a1ff6627e02fb7eaf04f.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/d57ab39871f2a1ff6627e02fb7eaf04f.png)'
- en: 'Figure 7: Method proposed by Defensive Distillation [[72](#bib.bib72)]. An
    Initial Network is trained on the dataset images and labels (discrete values).
    Then, the predictions given by the Initial Network are fed into another network,
    replacing the dataset labels. These predictions are continuous values, making
    the Distilled Network more resilient to adversarial attacks.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：防御性蒸馏方法[[72](#bib.bib72)]。初始网络在数据集图像和标签（离散值）上进行训练。然后，将初始网络给出的预测输入到另一个网络中，替代数据集标签。这些预测是连续值，使得蒸馏网络对对抗攻击更加稳健。
- en: Zantedeschi et al. [[77](#bib.bib77)] explored the standard architectures, which
    usually employ Rectified Linear Units (ReLU) [[78](#bib.bib78), [79](#bib.bib79)]
    to ease the training process, and discovered that this function makes a small
    perturbation in the input accumulate with multiple layers (unbounded). Therefore,
    the authors propose the use of bounded ReLU (BReLU) [[80](#bib.bib80)] to prevent
    this accumulation and Gaussian Data Augmentation to perform data augmentation.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Zantedeschi 等人[[77](#bib.bib77)] 探索了标准架构，这些架构通常使用修正线性单元 (ReLU) [[78](#bib.bib78),
    [79](#bib.bib79)] 来简化训练过程，并发现该函数使得输入中的小扰动在多个层中累积（无界）。因此，作者提出使用有界 ReLU (BReLU)
    [[80](#bib.bib80)] 来防止这种累积，并进行高斯数据增强以执行数据增强。
- en: Zhang and Wang [[19](#bib.bib19)] suggest that adversarial examples are generated
    through Feature Scattering (FS) in the latent space to avoid the label leaking
    effect, which considers the inter-example relationships. The adversarial examples
    are generated by maximizing the feature-matching distance between the clean and
    perturbed examples, FS produces a perturbed empirical distribution, and the DNN
    performs standard adversarial training.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 和 Wang[[19](#bib.bib19)] 建议通过在潜在空间中进行特征散射 (FS) 来生成对抗样本，以避免标签泄漏效应，这种方法考虑了样本之间的关系。对抗样本通过最大化干净样本和扰动样本之间的特征匹配距离来生成，FS
    产生了一个扰动的经验分布，DNN 执行标准对抗训练。
- en: 'PGD attack causes the internal representation to shift closer to the “false”
    class, Triplet Loss Adversarial (TLA) [[81](#bib.bib81)] includes an additional
    term in the loss function that pulls natural and adversarial images of a specific
    class closer and the remaining classes further apart. This method was tested with
    different samples: Random Negative (TLA-RN), which refers to a randomly sampled
    negative example, and Switch Anchor (TLA-SA), which sets the anchor as a natural
    example and the positive to be adversarial examples.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: PGD 攻击使内部表示更接近“错误”类别，Triplet Loss Adversarial (TLA) [[81](#bib.bib81)] 在损失函数中包含额外的项，使特定类别的自然图像和对抗图像更接近，而其他类别的图像则进一步分离。这种方法在不同样本上进行了测试：随机负样本
    (TLA-RN)，即随机采样的负样本，以及交换锚点 (TLA-SA)，即将锚点设为自然样本，而正样本设为对抗样本。
- en: Kumari et al. [[82](#bib.bib82)] analyzes the previously adversarial-trained
    models to test their vulnerability against adversarial attacks at the level of
    latent layers, concluding that the latent layer of these models is significantly
    vulnerable to adversarial perturbations of small magnitude. Latent Adversarial
    Training (LAT) [[82](#bib.bib82)] consists of finetuning adversarial-trained models
    to ensure robustness at the latent level.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Kumari 等人[[82](#bib.bib82)] 分析了以前经过对抗训练的模型，以测试它们在潜在层级别对对抗攻击的脆弱性，得出这些模型的潜在层对小幅度的对抗扰动显著脆弱。潜在对抗训练
    (LAT) [[82](#bib.bib82)] 旨在微调对抗训练模型，以确保在潜在层级别的鲁棒性。
- en: Curvature Regularization (CR) [[83](#bib.bib83)] minimizes the curvature of
    the loss surface, which induces a more ”natural” behavior of the network. The
    theoretical foundation behind this defense uses a locally quadratic approximation
    that demonstrates a strong relation between large robustness and small curvature.
    Furthermore, the proposed regularizer confirms the assumption that exhibiting
    quasi-linear behavior in the proximity of data points is essential to achieve
    robustness.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 曲率正则化（CR）[[83](#bib.bib83)] 最小化损失表面的曲率，从而引导网络表现出更“自然”的行为。该防御方法的理论基础使用局部二次近似，展示了大鲁棒性与小曲率之间的强关系。此外，提出的正则化器确认了在数据点附近表现出准线性行为对于实现鲁棒性的重要性。
- en: Unsupervised Adversarial Training (UAT) [[84](#bib.bib84)] enables the training
    with unlabeled data considering two different approaches, UAT with Online Target
    (UAT-OT) that minimizes a differentiable surrogate of the smoothness loss, and
    UAT with Fixed Targets (UAT-FT) that trains an external classifier to predict
    the labels on the unsupervised data and uses its predictions as labels.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督对抗训练（UAT）[[84](#bib.bib84)] 通过考虑两种不同的方法来进行无标签数据的训练：一种是通过最小化光滑度损失的可微替代品的UAT与在线目标（UAT-OT），另一种是通过训练一个外部分类器来预测无监督数据的标签，并使用其预测作为标签的UAT与固定目标（UAT-FT）。
- en: Robust Self-Training (RST) [[85](#bib.bib85)], an extension of Self-Training [[86](#bib.bib86),
    [87](#bib.bib87)], uses a standard supervised training to obtain pseudo-labels
    and then feeds them into a supervised training algorithm that targets adversarial
    robustness. This approach bridges the gap between standard and robust accuracy,
    using the unlabeled data, achieving high robustness using the same number of labels
    as required for high standard accuracy.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 稳健自训练（RST）[[85](#bib.bib85)]，是自训练（Self-Training）[[86](#bib.bib86), [87](#bib.bib87)]
    的扩展，使用标准监督训练来获得伪标签，然后将这些伪标签输入到一个以对抗鲁棒性为目标的监督训练算法中。这种方法弥合了标准准确性与鲁棒准确性之间的差距，利用无标签数据，在使用与高标准准确性所需的标签数相同的标签下实现高鲁棒性。
- en: SENSEI [[88](#bib.bib88)] and SENSEI-SA [[88](#bib.bib88)] use the methodologies
    employed in software testing to perform data augmentation, enhancing the robustness
    of DNNs. SENSEI implements the strategy of replacing each data point with a suitable
    variant or leaving it unchanged. SENSEI-SA improves the previous one by identifying
    which opportunities are suitable for skipping the augmentation process.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: SENSEI [[88](#bib.bib88)] 和 SENSEI-SA [[88](#bib.bib88)] 使用软件测试中采用的方法进行数据增强，提升了
    DNN 的鲁棒性。SENSEI 实施了将每个数据点替换为适当变体或保持不变的策略。SENSEI-SA 通过识别哪些机会适合跳过增强过程来改进前者。
- en: 'Bit Plane Feature Consistency (BPFC) [[89](#bib.bib89)] regularizer forces
    the DNNs to give more importance to the higher bit planes, inspired by the Human
    visual system perception. This regularizer uses the original image and a preprocessed
    version to calculate the $l_{2}$ norm between them and regularize the loss function,
    as the scheme shown in Figure [8](#S5.F8 "Figure 8 ‣ V-B Modify the Training Process
    ‣ V Adversarial Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses").'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '位平面特征一致性（BPFC）[[89](#bib.bib89)] 正则化器迫使深度神经网络（DNN）更加重视高位平面，这一设计灵感来源于人类视觉系统的感知。该正则化器使用原始图像和预处理版本来计算它们之间的
    $l_{2}$ 范数，并对损失函数进行正则化，如图[8](#S5.F8 "Figure 8 ‣ V-B Modify the Training Process
    ‣ V Adversarial Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") 所示的方案。'
- en: Adversarial Weight Perturbation (AWP) [[90](#bib.bib90)] explicitly regularizes
    the flatness of weight loss landscape and robustness gap, using a double-perturbation
    mechanism that disturbs both inputs and weights. This defense boosts the robustness
    of multiple existing adversarial training methods, confirming that it can be applied
    to other methods.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性权重扰动（AWP）[[90](#bib.bib90)] 明确地对权重损失景观的平坦性和鲁棒性差距进行正则化，采用双重扰动机制扰动输入和权重。这种防御提升了多种现有对抗训练方法的鲁棒性，确认它可以应用于其他方法。
- en: Self-Adaptive Training (SAT) [[91](#bib.bib91)] dynamically calibrates the training
    process with the model predictions without extra computational cost, improving
    the generalization of corrupted data. In contrast with the double-descent phenomenon,
    SAT exhibits a single-descent error-capacity curve, mitigating the overfitting
    effect.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应训练（SAT）[[91](#bib.bib91)] 动态地校准训练过程，与模型预测相结合且无需额外的计算成本，从而提高了对受损数据的泛化能力。与双重下降现象相比，SAT
    展示了单一下降的误差-容量曲线，减轻了过拟合效应。
- en: HYDRA [[92](#bib.bib92)] is another technique that explores the effects of pruning
    on the robustness of models, which proposes using pruning techniques that are
    aware of the robust training objective, allowing this objective to guide the search
    for connections to prune. This approach reaches compressed models that are state-of-the-art
    in standard and robust accuracy.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '**HYDRA** [[92](#bib.bib92)] 是另一种探索修剪对模型鲁棒性影响的技术，提出使用意识到稳健训练目标的修剪技术，使该目标能够指导连接的修剪搜索。这种方法达到了在标准和鲁棒准确性方面的先进压缩模型。'
- en: '![Refer to caption](img/d79d0fa10ab29316dc913ce0739e75e5.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d79d0fa10ab29316dc913ce0739e75e5.png)'
- en: 'Figure 8: Schematic overview of the Bit Plane Feature Consistency [[89](#bib.bib89)].
    This method applies multiple operations to input images, simulating adversarial
    images. Then, the loss is changed to include a regularizer (new term), which compares
    the original images with these manipulated images.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：**位平面特征一致性**的示意图[[89](#bib.bib89)]。该方法对输入图像应用多个操作，模拟对抗图像。然后，损失函数被更改为包含一个正则化项（新术语），该项将原始图像与这些操作过的图像进行比较。
- en: Based on the promising results demonstrated by previous distillation methods,
    the Robust Soft Label Adversarial Distillation (RSLAD) [[93](#bib.bib93)] method
    uses soft labels to train robust small student DNNs. This method uses the Robust
    Soft Labels (RSLs) produced by the teacher DNN to supervise the student training
    on natural and adversarial examples. An essential aspect of this method is that
    the student DNN does not access the original complex labels through the training
    process.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 基于先前蒸馏方法展示的良好结果，**鲁棒软标签对抗蒸馏**（RSLAD）[[93](#bib.bib93)] 方法使用软标签来训练稳健的小型学生DNNs。该方法使用教师DNN生成的**鲁棒软标签**（RSLs）来监督学生在自然和对抗样本上的训练。该方法的一个重要方面是学生DNN在训练过程中无法访问原始复杂标签。
- en: The most sensitive neurons in each layer make significant non-trivial contributions
    to the model predictions under adversarial settings, which means that increasing
    adversarial robustness stabilizes the most sensitive neurons. Sensitive Neuron
    Stabilizing (SNS) [[94](#bib.bib94)] includes an objective function dedicated
    explicitly to maximizing the similarities of sensitive neuron behaviors when providing
    clean and adversarial examples.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在对抗环境下，每层中最敏感的神经元对模型预测做出了显著的非平凡贡献，这意味着提高对抗鲁棒性会稳定最敏感的神经元。**敏感神经元稳定化**（SNS）[[94](#bib.bib94)]
    包括一个目标函数，专门致力于最大化在提供干净和对抗样本时敏感神经元行为的相似性。
- en: Dynamic Network Rewiring (DNR) [[95](#bib.bib95)] generates pruned DNNs that
    have high robust and standard accuracy, which employs a unified constrained optimization
    formulation using a hybrid loss function that merges ultra-high model compression
    with robust adversarial training. Furthermore, the authors propose a one-shot
    training method that achieves high compression, standard accuracy, and robustness,
    which has a practical inference 10 times faster than traditional methods.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**动态网络重接线**（DNR）[[95](#bib.bib95)] 生成具有高稳健性和标准准确性的剪枝DNNs，采用统一的约束优化公式，使用融合超高模型压缩与稳健对抗训练的混合损失函数。此外，作者提出了一种一次性训练方法，实现了高压缩率、标准准确性和稳健性，其实际推理速度比传统方法快10倍。'
- en: Manifold Regularization for Locally Stable (MRLS) [[96](#bib.bib96)] DNNs exploit
    the continuous piece-wise linear nature of ReLU to learn a function that is smooth
    over both predictions and decision boundaries. This method is based on approximating
    the graph Laplacian when the data is sparse.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部稳定流形正则化**（MRLS）[[96](#bib.bib96)] DNNs利用ReLU的连续分段线性特性来学习一个在预测和决策边界上都光滑的函数。该方法基于在数据稀疏时近似图拉普拉斯算子。'
- en: Inspired by the motivation behind distillation, Learnable Boundary Guided Adversarial
    Training (LBGAT) [[97](#bib.bib97)], assuming that models trained on clean data
    embed their most discriminative features, constrains the logits from the robust
    model to make them similar to the model trained on natural data. This approach
    makes the robust model inherit the decision boundaries of the clean model, preserving
    high standard and robust accuracy.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 受到蒸馏背后动机的启发，**可学习边界引导对抗训练**（LBGAT）[[97](#bib.bib97)]，假设在干净数据上训练的模型嵌入了它们最具辨别力的特征，将稳健模型的logits约束为与在自然数据上训练的模型相似。这种方法使稳健模型继承了干净模型的决策边界，保持了高标准和稳健的准确性。
- en: Low Temperature Distillation (LTD) [[98](#bib.bib98)], which uses previous distillation
    frameworks to generate labels, uses relatively low temperatures in the teacher
    model and employs different fixed temperatures for the teacher and student models.
    The main benefit of this mechanism is that the generated soft labels can be integrated
    into existing works without additional costs.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 低温蒸馏 (LTD) [[98](#bib.bib98)] 使用以前的蒸馏框架来生成标签，在教师模型中使用相对较低的温度，并为教师和学生模型使用不同的固定温度。该机制的主要优点是生成的软标签可以无额外成本地集成到现有工作中。
- en: Recently, literature [[99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101)]
    demonstrated that neural Ordinary Differential Equations (ODE) are naturally more
    robust to adversarial attacks than vanilla DNNs. Therefore, Stable neural ODE
    for deFending against adversarial attacks (SODEF) [[102](#bib.bib102)] uses optimization
    formulation to force the extracted feature points to be within the vicinity of
    Lyapunov-stable equilibrium points, which suppresses the input perturbations.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的文献 [[99](#bib.bib99), [100](#bib.bib100), [101](#bib.bib101)] 证明，神经常微分方程
    (ODE) 自然比普通 DNN 对对抗攻击更具鲁棒性。因此，稳定的神经 ODE 用于对抗对抗攻击 (SODEF) [[102](#bib.bib102)]
    使用优化公式来强制提取的特征点位于李雅普诺夫稳定平衡点的附近，从而抑制输入扰动。
- en: Self-COnsistent Robust Error (SCORE) [[103](#bib.bib103)] employs local equivariance
    to describe the ideal behavior of a robust model, facilitating the reconciliation
    between robustness and accuracy while still dealing with worst-case uncertainty.
    This method was inspired by the discovery that the trade-off between adversarial
    and clean accuracy imposes a bias toward smoothness.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 自一致鲁棒误差 (SCORE) [[103](#bib.bib103)] 采用局部等变性来描述鲁棒模型的理想行为，促进了鲁棒性与准确性之间的协调，同时仍然处理最坏情况的不确定性。该方法的灵感来源于发现对抗准确性与清洁准确性之间的权衡对平滑性产生了偏差。
- en: Analyzing the impact of activation shape on robustness, Dai et al. [[104](#bib.bib104)]
    observes that activation has positive outputs on negative inputs, and a high finite
    curvature can improve robustness. Therefore, Parametric Shifted Sigmoidal Linear
    Unit (PSSiLU) [[104](#bib.bib104)] combines these properties and parameterized
    activation functions with adversarial training.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析激活形状对鲁棒性的影响时，Dai 等人 [[104](#bib.bib104)] 观察到激活对负输入具有正输出，高有限曲率可以提高鲁棒性。因此，参数化的移位
    sigmoid 线性单元 (PSSiLU) [[104](#bib.bib104)] 结合了这些特性和参数化激活函数与对抗训练。
- en: V-C Use of Supplementary Networks
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-C 补充网络的使用
- en: 'MagNet [[105](#bib.bib105)] considers two reasons for the misclassification
    of an adversarial example: 1) incapacity of the classifier to reject an adversarial
    example distant from the boundary; and 2) classifier generalizes poorly when the
    adversarial example is close to the boundary. MagNet considers multiple detectors
    trained based on the reconstruction error, detecting significantly perturbed examples
    and detecting slightly perturbed examples based on probability divergence.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: MagNet [[105](#bib.bib105)] 考虑了对抗样本分类错误的两个原因：1）分类器无法拒绝远离边界的对抗样本；2）当对抗样本接近边界时，分类器泛化能力较差。MagNet
    考虑了多个基于重建误差训练的检测器，检测显著扰动的样本和基于概率偏差检测轻微扰动的样本。
- en: '![Refer to caption](img/4f50921c5b7b5febe85053cd0e4dad33.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4f50921c5b7b5febe85053cd0e4dad33.png)'
- en: 'Figure 9: Schematic overview of the Use of Supplementary Networks. The Detector
    Network was previously trained to detect adversarial images and is included between
    the input images and the classifier. This network receives the input images and
    determines if these images are Adversarial or Not. If they are not, they are redirected
    to the Classifier; If they are, they are susceptible to Human evaluation.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：补充网络使用的示意图。检测器网络之前已被训练以检测对抗图像，并且被包括在输入图像和分类器之间。该网络接收输入图像并确定这些图像是否为对抗样本。如果不是，它们会被重定向到分类器；如果是，它们会接受人工评估。
- en: 'Adversary Detection Network (ADN) [[106](#bib.bib106)] is a subnetwork that
    detects if the input example is adversarial or not, trained using adversarial
    images generated for a classification network which are classified as clean (0)
    or adversarial (1). Figure [9](#S5.F9 "Figure 9 ‣ V-C Use of Supplementary Networks
    ‣ V Adversarial Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") displays a schematic overview of this network. However, this
    defense mechanism deeply correlates to the datasets and classification networks.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '对抗检测网络（ADN）[[106](#bib.bib106)] 是一个子网络，用于检测输入样本是否为对抗样本，通过使用为分类网络生成的对抗图像进行训练，这些图像被分类为干净（0）或对抗（1）。图[9](#S5.F9
    "Figure 9 ‣ V-C Use of Supplementary Networks ‣ V Adversarial Defenses ‣ How Deep
    Learning Sees the World: A Survey on Adversarial Attacks & Defenses")展示了该网络的示意图。然而，这种防御机制与数据集和分类网络有深刻的关联。'
- en: Xu et al. found that the inclusion of Feature Squeezing (FS) [[107](#bib.bib107)]
    is highly reliable in detecting adversarial examples by reducing the search space
    available for the adversary to modify. This method compares the predictions of
    a standard network with a squeezed one, detecting adversarial examples with high
    accuracy and having few false positives.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人发现，包含特征压缩（FS）[[107](#bib.bib107)] 在检测对抗样本时非常可靠，因为它通过减少对抗者可以修改的搜索空间来提高检测效果。该方法将标准网络的预测与压缩网络的预测进行比较，以高准确率检测对抗样本，并且假阳性率较低。
- en: High-level representation Guided Denoiser (HGD) [[17](#bib.bib17)] uses the
    distance between original and adversarial images to guide an image denoiser and
    suppress the impact of adversarial examples. HGD uses a Denoising AutoEncoder [[108](#bib.bib108)]
    with additional lateral connections and considers the difference between the representations
    as the loss function at a specific layer that is activated by the normal and adversarial
    examples.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 高级表示引导去噪器（HGD）[[17](#bib.bib17)] 利用原始图像与对抗图像之间的距离来引导图像去噪器，抑制对抗样本的影响。HGD 使用带有额外侧向连接的去噪自编码器
    [[108](#bib.bib108)]，并考虑特定层中由正常和对抗样本激活的表示差异作为损失函数。
- en: Defense-GAN [[18](#bib.bib18)] explores the use of GANs to effectively represent
    the set of original training examples, making this defense independent from the
    attack used. Defense-GAN considers the usage of Wasserstein GANs (WGANs) [[109](#bib.bib109)]
    to learn the representation of the original data and denoise the adversarial examples,
    which start by minimizing the $l_{2}$ difference between the generator representation
    and the input image. Reverse Attacks [[110](#bib.bib110)] can be applied to each
    attack during the testing phase, by finding the suitable additive perturbation
    to repair the adversarial example similar to the adversarial attacks, which is
    highly difficult due to the unknown original label.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Defense-GAN [[18](#bib.bib18)] 探索了使用生成对抗网络（GANs）有效表示原始训练样本集，使得此防御机制与使用的攻击无关。Defense-GAN
    考虑使用Wasserstein GANs（WGANs）[[109](#bib.bib109)] 来学习原始数据的表示并去噪对抗样本，其开始时最小化生成器表示与输入图像之间的
    $l_{2}$ 差异。反向攻击 [[110](#bib.bib110)] 可以在测试阶段应用于每次攻击，通过找到适当的附加扰动来修复对抗样本，这类似于对抗攻击，由于原始标签未知，操作极为困难。
- en: Embedding Regularized Classifier (ER-Classifier) [[111](#bib.bib111)] is composed
    of a classifier, an encoder, and a discriminator, which uses the encoder to generate
    code vectors by reducing the dimensional space of the inputs and the discriminator
    to separate these vectors from the ideal code vectors (sampled from a prior distribution).
    This technique allows pushing adversarial examples into the benign image data
    distribution, removing the adversarial perturbations.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入正则化分类器（ER-Classifier）[[111](#bib.bib111)] 由分类器、编码器和判别器组成，使用编码器通过减少输入的维度空间来生成代码向量，并使用判别器将这些向量与理想代码向量（从先验分布中采样）分开。这种技术可以将对抗样本推送到正常图像数据分布中，去除对抗扰动。
- en: Class Activation Feature-based Denoiser (CAFD) [[112](#bib.bib112)] is a self-supervised
    approach trained to remove the noise from adversarial examples, using a set of
    examples generated by the Class Activation Feature-based Attack (CAFA) [[112](#bib.bib112)].
    This defense mechanism is trained to minimize the distance of the class activation
    features between the adversarial and natural examples, being robust to unseen
    attacks.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 基于类别激活特征的去噪器（CAFD）[[112](#bib.bib112)] 是一种自监督方法，通过使用由基于类别激活特征攻击（CAFA）[[112](#bib.bib112)]
    生成的一组示例来训练，以去除对抗样本中的噪声。该防御机制通过最小化对抗样本与自然样本之间的类别激活特征的距离进行训练，对未见过的攻击具有鲁棒性。
- en: Detector Graph (DG) [[113](#bib.bib113)] considers graphs to detect the adversarial
    examples by constructing a Latent Neighborhood Graph (LNG) for each original example
    and using Graph Neural Networks (GNNs) [[114](#bib.bib114)] to exploit the relationship
    and distinguish between original and adversarial examples. This method maintains
    an additional reference dataset to retrieve the manifold information and uses
    embedding representation of image pixel values, making the defense robust to unseen
    attacks.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 探测器图（DG）[[113](#bib.bib113)] 通过为每个原始示例构建潜在邻域图（LNG），并使用图神经网络（GNNs）[[114](#bib.bib114)]
    来挖掘关系和区分原始与对抗性示例。该方法维护了一个额外的参考数据集以检索流形信息，并使用图像像素值的嵌入表示，使得防御对于未知攻击具有鲁棒性。
- en: Images in the real world are represented in a continuous manner, yet machines
    can only store these images in discrete 2D arrays. Local Implicit Image Function
    (LIIF) [[115](#bib.bib115)] takes an image coordinate and the deep features around
    this coordinate as inputs, predicting the corresponding RGB value. This method
    of pre-processing input images can filter adversarial images by reducing their
    perturbations, which are subsequently fed to a classifier.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现实世界中的图像以连续方式表示，但机器只能以离散的二维数组存储这些图像。局部隐式图像函数（LIIF）[[115](#bib.bib115)] 以图像坐标和该坐标周围的深层特征作为输入，预测相应的RGB值。这种预处理输入图像的方法可以通过减少其扰动来过滤对抗图像，然后将其输入分类器。
- en: ADversarIal defenSe with local impliCit functiOns (DISCO) [[116](#bib.bib116)]
    is an additional network to the classifier that removes adversarial perturbations
    using localized manifold projections, which receives an adversarial image and
    a query pixel location. This defense mechanism comprises an encoder that creates
    per-pixel deep features and a local implicit module that uses these features to
    predict the clean RGB value.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用局部隐式函数（DISCO）[[116](#bib.bib116)] 的对抗性防御是对分类器的附加网络，通过局部流形投影去除对抗扰动，该网络接收对抗图像和查询像素位置。该防御机制包括一个创建每像素深层特征的编码器和一个使用这些特征预测干净RGB值的局部隐式模块。
- en: '![Refer to caption](img/c5cbf901ebd22623d47c3df9d44789a9.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c5cbf901ebd22623d47c3df9d44789a9.png)'
- en: 'Figure 10: Overview of a Feature Denoising Block [[117](#bib.bib117)], which
    can be included in the intermediate layers to make networks more robust. This
    method is an example of Change Network Architecture.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：特征去噪块概述[[117](#bib.bib117)]，可以包含在中间层中以增强网络的鲁棒性。这种方法是更改网络架构的一个示例。
- en: V-D Change Network Architecture
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-D 更改网络架构
- en: To identify the type of layers and their order, Guo et al. [[118](#bib.bib118)]
    proposes the use of Neural Architecture Search (NAS) to identify the networks
    that are more robust to adversarial attacks, finding that densely connected patterns
    improve the robustness and adding convolution operations to direct connection
    edge is effective, combined to create the RobNets [[118](#bib.bib118)].
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别层的类型及其顺序，Guo 等人[[118](#bib.bib118)] 提出了使用神经架构搜索（NAS）来识别对抗攻击更为鲁棒的网络，发现密集连接模式提高了鲁棒性，并且将卷积操作添加到直接连接边缘是有效的，从而结合创建了RobNets[[118](#bib.bib118)]。
- en: 'Feature Denoising [[117](#bib.bib117)] intends to address this problem by applying
    feature-denoising operations, consisting of non-local means, bilateral, mean,
    median filters, followed by 1x1 Convolution and an identity skip connection, as
    illustrated in Figure [10](#S5.F10 "Figure 10 ‣ V-C Use of Supplementary Networks
    ‣ V Adversarial Defenses ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses"). These blocks are added to the intermediate layers of CNNs.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 特征去噪[[117](#bib.bib117)] 旨在通过应用特征去噪操作来解决这个问题，包括非局部均值、双边、均值、中值滤波器，随后是1x1卷积和一个恒等跳跃连接，如图[10](#S5.F10
    "图10 ‣ V-C 使用补充网络 ‣ V 对抗性防御 ‣ 深度学习如何看待世界：对抗攻击与防御的调查")所示。这些块被添加到CNN的中间层中。
- en: Input Random [[119](#bib.bib119)] propose the addition of layers at the beginning
    of the classifier, consisting of 1) a random resizing layer, which resizes the
    width and height of the original image to a random width and height, and 2) a
    random padding layer, which pads zeros around the resized image in a random manner.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 输入随机化[[119](#bib.bib119)] 提出了在分类器开始处添加层的方案，包括1）一个随机缩放层，该层将原始图像的宽度和高度调整为随机的宽度和高度，以及2）一个随机填充层，该层以随机方式在缩放后的图像周围填充零。
- en: Controlling Neural Level Sets (CNLS) [[120](#bib.bib120)] uses samples obtained
    from the neural level sets and relates their positions to the network parameters,
    which allows modifying the decision boundaries of the network. The relation between
    position and parameters is achieved by constructing a sample network with an additional
    single fixed linear layer, which can incorporate the level set samples into a
    loss function.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 控制神经层集（CNLS）[[120](#bib.bib120)] 使用从神经层集中获得的样本，并将其位置与网络参数相关联，从而允许修改网络的决策边界。位置与参数之间的关系通过构建一个包含附加固定线性层的样本网络来实现，这样可以将层集样本纳入损失函数中。
- en: Sparse Transformation Layer (STL) [[121](#bib.bib121)], included between the
    input image and the network first layer, transforms the received images into a
    low-dimensional quasi-natural image space, which approximates the natural image
    space and removes adversarial perturbations. This creates an attack-agnostic adversarial
    defense that gets the original and adversarial images closer.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏变换层（STL）[[121](#bib.bib121)]，位于输入图像和网络第一层之间，将接收到的图像转换为低维的准自然图像空间，该空间近似自然图像空间并去除对抗性扰动。这种方法创建了一种与攻击无关的对抗性防御，使原始图像和对抗图像更加接近。
- en: Benz et al. [[122](#bib.bib122)] found that BN [[123](#bib.bib123)] and other
    normalization techniques make DNN more vulnerable to adversarial examples, suggesting
    the use of a framework that makes DNN more robust by learning Robust Features
    first and, then, Non-Robust Features (which are the ones learned when using BN).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: Benz 等人[[122](#bib.bib122)]发现，BN[[123](#bib.bib123)]及其他归一化技术使 DNN 更容易受到对抗性样本的攻击，建议使用一种框架，通过先学习**鲁棒特征**，然后再学习**非鲁棒特征**（即使用
    BN 学到的特征），使 DNN 更加鲁棒。
- en: V-E Perform Network Validation
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-E 执行网络验证
- en: Most of the datasets store their images using the Joint Photographic Experts
    Group (JPEG) [[124](#bib.bib124)] compression, yet no one had evaluated the impact
    of this process on the network performance. Dziugaite et al. [[125](#bib.bib125)]
    (named as JPG) varies the magnitude of FGSM perturbations, discovering that smaller
    ones often reverse the drop in classification by a large extent and, when the
    perturbations increase in magnitude, this effect is nullified.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据集使用联合图像专家组（JPEG）[[124](#bib.bib124)] 压缩存储图像，但尚未评估此过程对网络性能的影响。Dziugaite
    等人[[125](#bib.bib125)]（称为 JPG）改变 FGSM 扰动的幅度，发现较小的扰动往往会大幅度逆转分类下降，当扰动幅度增加时，这种效果被消除。
- en: Regarding formal verification, a tool [[126](#bib.bib126)] for automatic Safety
    Verification of the decisions made during the classification process was created
    using Satisfiability Modulo Theory (SMT). This approach assumes that a decision
    is safe when, after applying transformations in the input, the model decision
    does not change. It is applied to every layer individually in the network, using
    a finite space of transformations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 关于形式验证，使用满足性模理论（SMT）创建了一种用于自动安全验证分类过程决策的工具[[126](#bib.bib126)]。该方法假设，当对输入进行转换后，模型决策不发生变化时，该决策是安全的。它应用于网络中的每一层，使用有限的转换空间。
- en: DeepXplore [[127](#bib.bib127)] is the first white-box framework to perform
    a wide test coverage, introducing the concepts of neuron coverage, which are parts
    of the DNN that are exercised by test inputs. DeepXplore uses multiple DNNs as
    cross-referencing oracles to avoid manual checking for each test input and inputs
    that trigger different behaviors and achieve high neuron coverage is a joint optimization
    problem solved by gradient-based search techniques.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: DeepXplore [[127](#bib.bib127)] 是第一个执行广泛测试覆盖的白盒框架，引入了神经元覆盖的概念，即由测试输入激活的 DNN
    部分。DeepXplore 使用多个 DNN 作为交叉参考的神谕，避免手动检查每个测试输入，并通过梯度搜索技术解决高神经元覆盖率的联合优化问题。
- en: DeepGauge [[128](#bib.bib128)] intends to identify a testbed containing multi-faceted
    representations using a set of multi-granularity testing criteria. DeepGauge evaluates
    the resilience of DNNs using two different strategies, namely, primary function
    and corner-case behaviors, considering neuron- and layer-level coverage criteria.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: DeepGauge [[128](#bib.bib128)] 旨在使用一组多粒度测试标准识别包含多方面表示的测试平台。DeepGauge 通过两种不同策略，即**主要功能**和**边角案例行为**，评估
    DNN 的鲁棒性，并考虑神经元和层级覆盖标准。
- en: Surprise Adequacy for Deep Learning Systems (SADL) [[129](#bib.bib129)] is based
    on the behavior of DNN on the training data, by introducing the surprise of an
    input, which is the difference between the DNN behavior when given the input and
    the learned training data. The surprise of input is used as an adequacy criterion
    (Surprise Adequacy), which is used as a metric for the Surprise Coverage to ensure
    the input surprise range coverage.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习系统的惊讶适应性 (SADL) [[129](#bib.bib129)] 基于 DNN 在训练数据上的行为，通过引入输入的惊讶，即给定输入时 DNN
    行为与学习的训练数据之间的差异。输入的惊讶用作适应性标准（惊讶适应性），用于惊讶覆盖的度量，以确保输入惊讶范围的覆盖。
- en: The most recent data augmentation techniques, such as cutout [[130](#bib.bib130)]
    and mixup [[131](#bib.bib131)], fail to prevent overfitting and, sometimes, make
    the model over-regularized, concluding that, to achieve substantial improvements,
    the combination of early stopping and semi-supervised data augmentation, Overfit
    Reduction (OR) [[132](#bib.bib132)], is the best method.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最新的数据增强技术，如 cutout [[130](#bib.bib130)] 和 mixup [[131](#bib.bib131)]，未能防止过拟合，有时还会导致模型过度正则化，因此得出的结论是，要实现实质性改进，结合早期停止和半监督数据增强的
    Overfit Reduction (OR) [[132](#bib.bib132)] 是最佳方法。
- en: 'When creating a model, multiple implementation details influence its performance;
    Pang et al. [[133](#bib.bib133)] is the first one to provide insights on how these
    details influence the model robustness, herein named as Bag of Tricks (BT). Some
    conclusions drawn from this study are: 1) The robustness of the models is significantly
    affected by weight decay; 2) Early stopping of the adversarial attacks may deteriorate
    worst-case robustness; and 3) Smooth activation benefits lower capacity models.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建模型时，多种实现细节会影响其性能；Pang 等人 [[133](#bib.bib133)] 是第一个提供这些细节如何影响模型鲁棒性的见解的研究，称之为
    Bag of Tricks (BT)。该研究得出的结论有：1) 模型的鲁棒性受到权重衰减的显著影响；2) 对抗攻击的早期停止可能会恶化最坏情况下的鲁棒性；3)
    平滑激活有利于较低容量的模型。
- en: Overfitting is a known problem that affects model robustness; Rebuffi et al. [[134](#bib.bib134)]
    focuses on reducing this robust overfitting by using different data augmentation
    techniques. Fixing Data Augmentation (FDA) [[134](#bib.bib134)] demonstrates that
    model weight averaging combined with data augmentation schemes can significantly
    increase robustness, which is enhanced when using spatial composition techniques.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是一个已知的问题，它影响模型鲁棒性；Rebuffi 等人 [[134](#bib.bib134)] 通过使用不同的数据增强技术来减少这种鲁棒过拟合。Fixing
    Data Augmentation (FDA) [[134](#bib.bib134)] 表明，模型权重平均结合数据增强方案可以显著提高鲁棒性，并且当使用空间组合技术时，鲁棒性会进一步增强。
- en: Gowal et al. [[135](#bib.bib135)] systematically studies the effect of multiple
    training losses, model sizes, activation functions, the addition of unlabeled
    data, and other aspects. The main conclusion drawn by this analysis is that larger
    models with Swish/SiLU [[136](#bib.bib136)] activation functions and model weight
    averaging can reliably achieve state-of-the-art results in robust accuracy.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Gowal 等人 [[135](#bib.bib135)] 系统地研究了多个训练损失、模型大小、激活函数、未标记数据的增加以及其他方面的影响。该分析得出的主要结论是，具有
    Swish/SiLU [[136](#bib.bib136)] 激活函数和模型权重平均的大型模型可以可靠地在鲁棒准确性上实现最先进的结果。
- en: '![Refer to caption](img/2e73b1e158e124e285f4e21fcee722b0.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2e73b1e158e124e285f4e21fcee722b0.png)'
- en: 'Figure 11: Overview of Adversarial Purification using Denoising Diffusion Probabilistic
    Models, adapted from [[137](#bib.bib137)]. The diffusion process is applied to
    an adversarial image, consisting of adding noise for a certain number of steps.
    During the denoising procedure, this noise is iteratively removed by the same
    amount of steps, resulting in a purified image (without perturbations).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：使用去噪扩散概率模型的对抗净化概述，改编自 [[137](#bib.bib137)]。扩散过程应用于对抗图像，包括在一定步骤中添加噪声。在去噪过程中，这些噪声通过相同数量的步骤迭代去除，从而得到一个净化后的图像（无扰动）。
- en: V-F Adversarial Purification
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: V-F 对抗净化
- en: 'Adversarial Purification consists of defense mechanisms that remove adversarial
    perturbations using a generative model. Improving Robustness Using Generated Data
    (IRUGD) [[138](#bib.bib138)] explores how generative models trained on the original
    images can be leveraged to increase the size of the original datasets. Through
    extensive experiments, they concluded that Denoising Diffusion Probabilistic Models
    (DDPM) [[137](#bib.bib137)], a progression of diffusion probabilistic models [[139](#bib.bib139)],
    is the model that more closely resembles real data. Figure [11](#S5.F11 "Figure
    11 ‣ V-E Perform Network Validation ‣ V Adversarial Defenses ‣ How Deep Learning
    Sees the World: A Survey on Adversarial Attacks & Defenses") presents the main
    idea behind the DDPM process.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性净化包括使用生成模型去除对抗性扰动的防御机制。通过生成数据提高鲁棒性（IRUGD）[[138](#bib.bib138)]探讨了如何利用在原始图像上训练的生成模型来增加原始数据集的规模。通过广泛的实验，他们得出结论，去噪扩散概率模型（DDPM）[[137](#bib.bib137)]，作为扩散概率模型[[139](#bib.bib139)]的进展，是最接近真实数据的模型。图[11](#S5.F11
    "图 11 ‣ V-E 执行网络验证 ‣ V 对抗防御 ‣ 深度学习如何看待世界：对对抗攻击与防御的调查")展示了DDPM过程的主要思路。
- en: Due to the great results in image synthesis displayed by the DDPM, Sehwag et
    al. [[140](#bib.bib140)] (Proxy) uses proxy distributions to significantly improve
    the performance of adversarial training by generating additional examples, demonstrating
    that the best generative models for proxy distribution are DDPM.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 由于DDPM在图像合成中显示出的卓越效果，Sehwag等人[[140](#bib.bib140)]（代理）使用代理分布通过生成额外的示例显著提升了对抗训练的性能，证明了DDPM是代理分布的最佳生成模型。
- en: Inspired by previous works on adversarial purification [[141](#bib.bib141),
    [142](#bib.bib142)], DiffPure [[143](#bib.bib143)] uses DDPM for adversarial purification,
    receiving as input an adversarial example and recovering the clean image through
    a reverse generative process. Since this discovery, multiple improvements regarding
    the use of DDPM for Adversarial Purification have been studied. Guided Diffusion
    Model for Adversarial Purification (GDMAP) [[144](#bib.bib144)] receives as initial
    input pure Gaussian noise and gradually denoises it with guidance to an adversarial
    image.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 受到先前对抗性净化工作[[141](#bib.bib141), [142](#bib.bib142)]的启发，DiffPure[[143](#bib.bib143)]使用DDPM进行对抗性净化，输入对抗性示例，并通过反向生成过程恢复干净图像。自此发现以来，已经研究了关于使用DDPM进行对抗性净化的多项改进。对抗性净化的引导扩散模型（GDMAP）[[144](#bib.bib144)]以纯高斯噪声作为初始输入，并逐渐用引导去噪至对抗性图像。
- en: DensePure [[145](#bib.bib145)] employs iterative denoising to an input image,
    with different random seeds, to get multiple reversed samples, which are given
    to the classifier and the final prediction is based on majority voting. Furthermore,
    Wang et al. [[146](#bib.bib146)] uses the most recent diffusion models [[147](#bib.bib147)]
    to demonstrate that diffusion models with higher efficiency and image quality
    directly translate into better robust accuracy.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: DensePure[[145](#bib.bib145)]对输入图像进行迭代去噪，使用不同的随机种子，得到多个反向样本，这些样本被提供给分类器，最终预测基于多数投票。此外，Wang等人[[146](#bib.bib146)]使用最新的扩散模型[[147](#bib.bib147)]证明了具有更高效率和图像质量的扩散模型直接转化为更好的鲁棒准确性。
- en: VI Adversarial Effects on Vision Transformers
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VI 对视觉变换器的对抗性影响
- en: Like CNNs [[148](#bib.bib148)], the ViTs are also susceptible to adversarial
    perturbations that alter a patch in an image [[149](#bib.bib149)], and ViTs demonstrate
    higher robustness, almost double, compared with ResNet-50 [[150](#bib.bib150)].
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 像CNN[[148](#bib.bib148)]一样，ViTs也容易受到扰动的影响，这些扰动会改变图像中的一个补丁[[149](#bib.bib149)]，且与ResNet-50[[150](#bib.bib150)]相比，ViTs显示出几乎双倍的更高鲁棒性。
- en: To further evaluate the robustness of ViT to adversarial examples, Mahmood et
    al. [[151](#bib.bib151)] used multiple adversarial attacks in CNNs, namely FGSM,
    PGD, MIM, C&W, and MI-FGSM. The ViT has increased robustness (compared with ResNet)
    for the first four attacks and has no resilience to the C&W and MI-FGSM attacks.
    Additionally, to complement the results obtained from the performance of ViTs,
    an extensive study [[152](#bib.bib152)] using feature maps, attention maps, and
    Gradient-weighted Class Activation Mapping (Grad-CAM) [[153](#bib.bib153)] intends
    to explain this performance visually.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步评估ViT对对抗样本的鲁棒性，Mahmood等人[[151](#bib.bib151)]在CNN中使用了多种对抗攻击方法，即FGSM、PGD、MIM、C&W和MI-FGSM。相比于ResNet，ViT在前四种攻击中的鲁棒性有所提高，但对C&W和MI-FGSM攻击没有抵御能力。此外，为了补充ViTs性能的结果，进行了一项广泛的研究[[152](#bib.bib152)]，使用特征图、注意力图和梯度加权类别激活映射（Grad-CAM）[[153](#bib.bib153)]，旨在从视觉上解释这些性能。
- en: The transferability of adversarial examples from CNNs to ViTs was also evaluated,
    suggesting that the examples from CNNs do not instantly transfer to ViTs [[151](#bib.bib151)].
    Furthermore, Self-Attention blended Gradient Attack (SAGA) [[151](#bib.bib151)]
    was proposed to misclassify both ViTs and CNNs. The Pay No Attention (PNA) [[154](#bib.bib154)]
    attack, which ignores the gradients of attention, and the PatchOut [[154](#bib.bib154)]
    attack, which randomly samples subsets of patches, demonstrate high transferability.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本从CNNs到ViTs的迁移性也被评估，结果表明，来自CNNs的样本不会立即迁移到ViTs[[151](#bib.bib151)]。此外，提出了自注意力混合梯度攻击（SAGA）[[151](#bib.bib151)]，旨在对ViTs和CNNs进行错误分类。忽略注意力梯度的攻击（PNA）[[154](#bib.bib154)]和随机采样补丁子集的攻击（PatchOut）[[154](#bib.bib154)]展示了较高的迁移性。
- en: To detect adversarial examples that might affect the ViTs, PatchVeto [[20](#bib.bib20)]
    uses different transformers with different attention masks that output the encoding
    of the class. An image is considered valid if all transformers reach a consensus
    in the voted class, overall the masked predictions (provided by masked transformers).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检测可能影响ViTs的对抗样本，PatchVeto[[20](#bib.bib20)]使用不同的变换器和不同的注意力掩码来输出类别编码。如果所有变换器在投票类别上达成一致，则图像被认为是有效的，总体上是由遮蔽的变换器提供的预测。
- en: Smoothed ViTs [[155](#bib.bib155)] perform preprocessing techniques to the images
    before feeding them into the ViT, by generating image ablations (images composed
    of only one column of the original image, and the remaining columns are black),
    which are converted into tokens, and droping the fully masked tokens. The remaining
    tokens are fed into a ViT, which predicts a class for each ablation, and the class
    with the most predictions of overall ablations is considered the correct one.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑的ViTs[[155](#bib.bib155)]在将图像输入ViT之前进行预处理，通过生成图像消融（由原始图像的单列组成，其他列为黑色）来实现，这些消融图像被转换为令牌，并丢弃完全遮蔽的令牌。剩余的令牌被输入到ViT中，ViT对每个消融图像进行类别预测，最终所有消融图像的预测最多的类别被认为是正确的。
- en: Bai et al. [[156](#bib.bib156)] demonstrates that ViTs and CNNs are being unfairly
    evaluated because they do not have the same training details. Therefore, this
    work provides a fair and in-depth comparison between ViTs and CNNs, indicating
    that ViTs are as vulnerable to adversarial perturbations as CNNs.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: Bai等人[[156](#bib.bib156)]表明ViTs和CNNs的评估存在不公平，因为它们的训练细节不同。因此，这项工作提供了对ViTs和CNNs的公平和深入的比较，表明ViTs对对抗扰动的脆弱性与CNNs相当。
- en: Architecture-oriented Transferable Attacking (ATA) [[157](#bib.bib157)] is a
    framework that generates transferable adversarial examples by considering the
    common characteristics among different ViT architectures, such as self-attention
    and image-embedding. Specifically, it discovers the most attentional patch-wise
    regions significantly influencing the model decision and searches pixel-wise attacking
    positions using sensitive embedding perturbation.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 面向架构的可转移攻击（ATA）[[157](#bib.bib157)]是一个框架，通过考虑不同ViT架构之间的共同特征，如自注意力和图像嵌入，生成可转移的对抗样本。具体来说，它发现对模型决策有显著影响的最具注意力的补丁区域，并使用敏感的嵌入扰动搜索像素级攻击位置。
- en: Patch-fool [[158](#bib.bib158)] explores the perturbations that turn ViTs more
    vulnerable learners than CNNs, proposing a dedicated attack framework that fools
    the self-attention mechanism by attacking a single patch with multiple attention-aware
    optimization techniques. This attack mechanism demonstrates, for the first time,
    that ViTs can be more vulnerable than CNNs if attacked with proper techniques.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Patch-fool [[158](#bib.bib158)] 探索了使 ViTs 更容易受到攻击的扰动，提出了一种专门的攻击框架，通过多种注意力感知优化技术攻击单个补丁，从而欺骗自注意力机制。这种攻击机制首次证明了如果采用适当的技术，ViTs
    可能比 CNNs 更脆弱。
- en: Gu et al. [[159](#bib.bib159)] evaluates the robustness of ViT to patch-wise
    perturbations, concluding that these models are more robust to naturally corrupted
    patches than CNNs while being more vulnerable to adversarially generated ones.
    Inspired by the observed results, the authors propose a simple Temperature Scaling
    based method that improves the robustness of ViTs.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: Gu 等人 [[159](#bib.bib159)] 评估了 ViT 对于补丁级扰动的鲁棒性，得出结论认为这些模型对自然腐蚀的补丁比 CNNs 更加鲁棒，但对对抗生成的补丁则更为脆弱。受观察结果启发，作者提出了一种简单的基于温度缩放的方法，来提高
    ViT 的鲁棒性。
- en: As previously observed for CNNs, improving the robust accuracy sacrifices the
    standard accuracy of ViTs, which may limit their applicability in the real context.
    Derandomized Smoothing [[160](#bib.bib160)] uses a progressive smoothed image
    modeling task to train the ViTs, making them capture the more discriminating local
    context while preserving global semantic information, improving both robust and
    standard accuracy.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如之前对 CNNs 的观察所示，提高鲁棒性准确率会牺牲 ViT 的标准准确率，这可能限制了它们在实际环境中的适用性。Derandomized Smoothing [[160](#bib.bib160)]
    使用逐步平滑的图像建模任务来训练 ViTs，使其在保留全局语义信息的同时捕捉到更具辨别力的局部上下文，从而提高了鲁棒性和标准准确率。
- en: VeinGuard [[161](#bib.bib161)] is a defense framework that helps ViTs be more
    robust against adversarial palm-vein image attacks, with practical applicability
    in the real world. Namely, VeinGuard is composed of a local transformer-based
    GAN that learns the distribution of unperturbed vein images and a purifier that
    automatically removes a variety of adversarial perturbations.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: VeinGuard [[161](#bib.bib161)] 是一个防御框架，帮助 ViTs 更加鲁棒地抵御对抗性的掌静脉图像攻击，在实际应用中具有实际意义。具体而言，VeinGuard
    由一个基于局部变换器的 GAN 组成，它学习未受扰动的静脉图像的分布，以及一个自动去除各种对抗性扰动的净化器。
- en: VII Datasets
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VII 数据集
- en: VII-A MNIST and F-MNIST
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-A MNIST 和 F-MNIST
- en: One of the most used datasets is the MNIST [[162](#bib.bib162)] dataset, which
    contains images of handwritten digits collected from approximately 250 writers
    in shades of black and white, withdrawn from two different databases. This dataset
    is divided into training and test sets, with the first one containing 60,000 examples
    and a second one containing 10,000 examples.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的数据集之一是 MNIST [[162](#bib.bib162)] 数据集，它包含了约250名书写者的手写数字图像，图像为黑白色调，从两个不同的数据库中提取。该数据集分为训练集和测试集，前者包含60,000个样本，后者包含10,000个样本。
- en: Xiao et al. propose the creation of the Fashion-MNIST [[163](#bib.bib163)] dataset
    by using figures from a fashion website, which has a total size of 70,000 images,
    contains ten classes, uses greyscale images, and each image has a size of 28x28\.
    The Fashion-MNIST dataset is divided into train and test sets, containing 60,000
    and 10,000 examples, respectively.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Xiao 等人提出通过使用时尚网站上的图形创建 Fashion-MNIST [[163](#bib.bib163)] 数据集，该数据集总大小为70,000张图像，包含十个类别，使用灰度图像，每张图像的大小为28x28。Fashion-MNIST
    数据集分为训练集和测试集，分别包含60,000和10,000个样本。
- en: '![Refer to caption](img/b1186d35032e37fad611482b29edc594.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b1186d35032e37fad611482b29edc594.png)'
- en: 'Figure 12: Images withdrew from the MNIST dataset [[162](#bib.bib162)] in the
    first five columns and from the Fashion-MNIST dataset [[163](#bib.bib163)] in
    the last five columns. The images were resized for better visualization.'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：从 MNIST 数据集 [[162](#bib.bib162)] 中提取的前五列图像和从 Fashion-MNIST 数据集 [[163](#bib.bib163)]
    中提取的最后五列图像。这些图像已被调整大小以便更好地可视化。
- en: 'Fig. [12](#S7.F12 "Figure 12 ‣ VII-A MNIST and F-MNIST ‣ VII Datasets ‣ How
    Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses") displays
    the 10 digits (from 0 to 9) from the MNIST dataset in the first five columns and
    the 10 fashion objects from Fashion-MNIST dataset in the last five columns. MNIST
    is one of the most widely studied datasets in the earlier works of adversarial
    examples, with defense mechanisms already displaying high robustness on this dataset.
    The same does not apply to Fashion-MNIST, which has not been as widely studied,
    despite having similar characteristics to MNIST.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [12](#S7.F12 "Figure 12 ‣ VII-A MNIST 和 F-MNIST ‣ VII 数据集 ‣ 深度学习如何看待世界：对对抗攻击与防御的调查")
    显示了MNIST数据集中前五列的10个数字（从0到9）和Fashion-MNIST数据集中最后五列的10个时尚物体。MNIST是对抗样本早期研究中最广泛研究的数据集之一，防御机制在该数据集上已显示出较高的鲁棒性。而Fashion-MNIST虽然具有类似于MNIST的特征，但尚未被广泛研究。
- en: '![Refer to caption](img/c1c62caba63b55c35f31c43b2e4b32a9.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c1c62caba63b55c35f31c43b2e4b32a9.png)'
- en: 'Figure 13: Images withdrew from the CIFAR-10 dataset [[164](#bib.bib164)] in
    the first five columns and from the CIFAR-100 dataset [[164](#bib.bib164)] in
    the last five columns. The images were resized for better visualization.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：前五列展示了从CIFAR-10数据集 [[164](#bib.bib164)] 中提取的图像，最后五列展示了从CIFAR-100数据集 [[164](#bib.bib164)]
    中提取的图像。这些图像已调整大小以便更好地可视化。
- en: VII-B CIFAR-10 and CIFAR-100
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-B CIFAR-10 和 CIFAR-100
- en: Another widely studied dataset is the CIFAR-10, which, in conjunction with the
    CIFAR-100 dataset, are subsets from a vast database containing 80 million tiny
    images [[165](#bib.bib165)], 32x32, and three color channels 75,062 different
    classes.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个被广泛研究的数据集是CIFAR-10，它与CIFAR-100数据集一起，是一个包含8,000万张小图像 [[165](#bib.bib165)]，每张图像32x32像素和三种颜色通道的庞大数据库的子集，共有75,062个不同的类别。
- en: CIFAR-10 [[164](#bib.bib164)] contains only ten classes from this large database,
    with 6,000 images for each class, distributed into 50,000 training images and
    10,000 test images. This dataset considers different objects, namely, animals
    and vehicles, usually found in different environments.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10 [[164](#bib.bib164)] 仅包含这个大型数据库中的十个类别，每个类别有6,000张图像，分为50,000张训练图像和10,000张测试图像。该数据集考虑了不同的物体，即动物和车辆，通常出现在不同的环境中。
- en: CIFAR-100 [[164](#bib.bib164)] contains 100 classes with only 600 images for
    each one with the same size and amount of color channels as the CIFAR-10 dataset.
    CIFAR-100 groups its 100 classes into 20 superclasses, located in different contexts/environments,
    making this dataset much harder to achieve high results.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-100 [[164](#bib.bib164)] 包含100个类别，每个类别只有600张图像，图像大小和颜色通道与CIFAR-10数据集相同。CIFAR-100将其100个类别分为20个超类，分布在不同的上下文/环境中，使得该数据集更难以获得高结果。
- en: 'Examples from the CIFAR-10 dataset are shown in Fig. [13](#S7.F13 "Figure 13
    ‣ VII-A MNIST and F-MNIST ‣ VII Datasets ‣ How Deep Learning Sees the World: A
    Survey on Adversarial Attacks & Defenses") in the first five columns, and the
    remaining columns display examples of the superclasses from CIFAR-100\. Due to
    the unsatisfactory results demonstrated by models trained on CIFAR-10, the CIFAR-100
    dataset has not been included in most studies under the context of adversarial
    examples, suggesting that solving the issue of adversarial-perturbed images is
    still at its inception.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10数据集中的示例显示在图 [13](#S7.F13 "Figure 13 ‣ VII-A MNIST 和 F-MNIST ‣ VII 数据集
    ‣ 深度学习如何看待世界：对对抗攻击与防御的调查") 的前五列中，其余列显示了CIFAR-100超类的示例。由于在CIFAR-10上训练的模型结果不理想，CIFAR-100数据集在大多数关于对抗样本的研究中未被包括，这表明解决对抗扰动图像的问题仍处于起步阶段。
- en: '![Refer to caption](img/b92258bb80d5e2b1e0cb05e3f4ac6f7e.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b92258bb80d5e2b1e0cb05e3f4ac6f7e.png)'
- en: 'Figure 14: Images withdrew from the Street View House Numbers dataset [[166](#bib.bib166)]
    in the first three columns and from the German Traffic Sign Recognition Benchmark
    dataset [[167](#bib.bib167)] in the last three columns. The images were resized
    for better visualization.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：前三列展示了从Street View House Numbers数据集 [[166](#bib.bib166)] 中提取的图像，后三列展示了从German
    Traffic Sign Recognition Benchmark数据集 [[167](#bib.bib167)] 中提取的图像。这些图像已调整大小以便更好地可视化。
- en: '![Refer to caption](img/d5872d66900528109e7bd8bcacb044cf.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d5872d66900528109e7bd8bcacb044cf.png)'
- en: 'Figure 15: Images withdrew from the ImageNet dataset [[168](#bib.bib168)] in
    the top left, from the ImageNet-A dataset [[169](#bib.bib169)] in the top right,
    from the ImageNet-C and ImageNet-P datasets [[170](#bib.bib170)] in the bottom
    left, and ImageNet-COLORDISTORT [[171](#bib.bib171)] in the bottom right. The
    images were resized for better visualization.'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：左上角是从ImageNet数据集[[168](#bib.bib168)]中提取的图像，右上角是从ImageNet-A数据集[[169](#bib.bib169)]中提取的图像，左下角是从ImageNet-C和ImageNet-P数据集[[170](#bib.bib170)]中提取的图像，右下角是ImageNet-COLORDISTORT[[171](#bib.bib171)]中的图像。图像已被调整大小以便更好地观察。
- en: VII-C Street View Datasets
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-C 街景数据集
- en: 'The Street View House Numbers (SVHN) [[166](#bib.bib166)] dataset provides
    the same challenge as MNIST: identifying which digits are present in a colored
    image, containing ten classes, 0 to 9 digits, and an image size of 32x32 centered
    around a single character, with multiple digits in a single image. Regarding the
    dataset size, it has 630,420 digit images, but only 73,257 images are used for
    training, 26,032 images are used for testing, and the remaining 531,131 images
    can be used as additional training data.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 街景房屋号码（SVHN）[[166](#bib.bib166)]数据集提供了与MNIST相同的挑战：识别彩色图像中存在的数字，包含十个类别，即0到9的数字，图像尺寸为32x32，围绕一个字符，图像中可能包含多个数字。关于数据集的大小，它有630,420张数字图像，但仅有73,257张用于训练，26,032张用于测试，其余531,131张可用作额外的训练数据。
- en: German Traffic Sign Recognition Benchmark (GTSRB) [[167](#bib.bib167)] is a
    dataset containing 43 classes of different traffic signs, has 50,000 images, and
    demonstrates realistic scenarios. The dataset has 51,840 images, whose size varies
    from 15x15 to 222x193, divided into training, validation, and test sets with 50%,
    25%, and 25%, respectively, of the total images.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 德国交通标志识别基准（GTSRB）[[167](#bib.bib167)]是一个包含43类不同交通标志的数据集，拥有50,000张图像，并展示了现实场景。该数据集有51,840张图像，尺寸从15x15到222x193不等，分为训练集、验证集和测试集，分别占总图像的50%、25%和25%。
- en: 'The difficulties associated with the SVHN dataset are displayed in the first
    three rows of Fig. [14](#S7.F14 "Figure 14 ‣ VII-B CIFAR-10 and CIFAR-100 ‣ VII
    Datasets ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses"), showing unique digits that occupy the whole image and multiple digits
    on different backgrounds. Furthermore, the same figure presents the different
    types of traffic signs in the GTSRB dataset, such as prohibition, warning, mandatory,
    and end of prohibition.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '与SVHN数据集相关的困难显示在图[14](#S7.F14 "Figure 14 ‣ VII-B CIFAR-10 and CIFAR-100 ‣ VII
    Datasets ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks &
    Defenses")的前三行中，显示了占据整个图像的独特数字和不同背景上的多个数字。此外，同一图形展示了GTSRB数据集中不同类型的交通标志，如禁止、警告、强制和禁止结束。'
- en: VII-D ImageNet and Variants
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VII-D ImageNet及其变体
- en: ImageNet [[168](#bib.bib168)] is one of the largest datasets for object recognition,
    containing 1,461,406 colored images and 1,000 classes, with images being resized
    to 224x224\. This dataset collected photographs from Flickr, and other search
    engines, divided into 1.2 million training images, 50,000 validation images, and
    100,000 test images.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet[[168](#bib.bib168)]是最大的目标识别数据集之一，包含1,461,406张彩色图像和1,000个类别，图像被调整为224x224。该数据集从Flickr和其他搜索引擎收集了照片，分为120万张训练图像、50,000张验证图像和100,000张测试图像。
- en: A possible alternative to ImageNet, when the dataset size is an important factor,
    is called Tiny ImageNet [[172](#bib.bib172)], a subset of ImageNet that contains
    fewer classes and images. This dataset contains only 200 classes (from the 1,000
    classes in ImageNet), 100,000 training images, 10,000 validation images, and 10,000
    test images. These classes include animals, vehicles, household items, insects,
    and clothing, considering the variety of contexts/environments that these objects
    can be found. Their images have a size of 64x64 and are colored.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集大小是一个重要因素时，一个可能的替代方案是Tiny ImageNet[[172](#bib.bib172)]，这是ImageNet的一个子集，包含较少的类别和图像。该数据集仅包含200个类别（从ImageNet的1,000个类别中挑选），有100,000张训练图像、10,000张验证图像和10,000张测试图像。这些类别包括动物、车辆、家居用品、昆虫和服装，考虑到这些物体可能出现的多种背景/环境。图像的尺寸为64x64，并且是彩色的。
- en: ImageNet-A [[169](#bib.bib169)] is a subset of ImageNet, containing only 200
    classes from the 1,000 classes, covering the broadest categories in ImageNet.
    ImageNet-A is a dataset composed of real-world adversarially filtered images,
    which were obtained by deleting the correctly predicted images by ResNet-50 classifiers.
    Despite ImageNet-A being based on the deficiency of ResNet-50, it also demonstrates
    transferability to unseen models, making this dataset suitable for evaluating
    the robustness of multiple classifiers.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet-A [[169](#bib.bib169)] 是 ImageNet 的一个子集，仅包含 1,000 个类别中的 200 个类别，覆盖了
    ImageNet 中最广泛的类别。ImageNet-A 是一个由真实世界对抗性过滤图像组成的数据集，这些图像是通过删除 ResNet-50 分类器正确预测的图像获得的。尽管
    ImageNet-A 是基于 ResNet-50 的不足之处，但它也展示了对未见模型的可转移性，使得这个数据集适合用于评估多个分类器的鲁棒性。
- en: Two additional benchmarks, ImageNet-C [[170](#bib.bib170)] and ImageNet-P [[170](#bib.bib170)],
    were designed to evaluate the robustness of DNNs. The ImageNet-C standardizes
    and expands the corruption robustness topic, consisting of 75 corruptions applied
    to each image in the ImageNet validation set. ImageNet-P applies distortions to
    the images, though it differs from ImageNet-C because it contains perturbation
    sequences using only ten common perturbations.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 另外两个基准，ImageNet-C [[170](#bib.bib170)] 和 ImageNet-P [[170](#bib.bib170)]，旨在评估深度神经网络的鲁棒性。ImageNet-C
    标准化并扩展了破坏鲁棒性话题，包括对 ImageNet 验证集中的每张图像应用 75 种破坏。ImageNet-P 对图像应用了失真，虽然它与 ImageNet-C
    不同，因为它包含的扰动序列仅使用十种常见的扰动。
- en: 'TABLE III: Relevant characteristics to the context of adversarial examples
    of the state-of-the-art datasets. #Classes means the number of classes in the
    dataset. Empty Color column means that the images in that dataset use greyscale
    or black and white shades. Datasets with $*$ are only used for testing purposes.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 表 III：与最新数据集的对抗样本上下文相关的特征。#类别表示数据集中类别的数量。空的颜色列表示该数据集中的图像使用灰度或黑白色调。带 $*$ 的数据集仅用于测试目的。
- en: '| Dataset | Size | #Classes | Classes | Color |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 大小 | #类别 | 类别 | 颜色 |'
- en: '| MNIST | 70,000 | 10 | Digits |  |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| MNIST | 70,000 | 10 | 数字 |  |'
- en: '| Fashion-MNIST | 70,000 | 10 | Clothing |  |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Fashion-MNIST | 70,000 | 10 | 服装 |  |'
- en: '| CIFAR-10 | 60,000 | 10 | Animals | ✓ |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-10 | 60,000 | 10 | 动物 | ✓ |'
- en: '| Vehicles |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 车辆 |'
- en: '| SVHN | 630,420 | 10 | Digits | ✓ |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| SVHN | 630,420 | 10 | 数字 | ✓ |'
- en: '| GTSRB | 51,840 | 43 | Traffic Signs | ✓ |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| GTSRB | 51,840 | 43 | 交通标志 | ✓ |'
- en: '| CIFAR-100 | 60,000 | 100 | Household Items | ✓ |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| CIFAR-100 | 60,000 | 100 | 家庭用品 | ✓ |'
- en: '| Outdoor Scenes |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 户外场景 |'
- en: '| Tiny ImageNet | 120,000 | 200 | Animals | ✓ |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| Tiny ImageNet | 120,000 | 200 | 动物 | ✓ |'
- en: '| Household Items |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 家庭用品 |'
- en: '| ImageNet-A^∗ | 7,500 | 200 | Vehicles | ✓ |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet-A^∗ | 7,500 | 200 | 车辆 | ✓ |'
- en: '| Food |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 食物 |'
- en: '| ImageNet-C^∗ | 3,750,000 | 200 | Vehicles | ✓ |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet-C^∗ | 3,750,000 | 200 | 车辆 | ✓ |'
- en: '| Food |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 食物 |'
- en: '| ImageNet-P^∗ | 15,000,000 | 200 | Vehicles | ✓ |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet-P^∗ | 15,000,000 | 200 | 车辆 | ✓ |'
- en: '| Food |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 食物 |'
- en: '| ImageNet | 1,431,167 | 1,000 | Vehicles | ✓ |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet | 1,431,167 | 1,000 | 车辆 | ✓ |'
- en: '| Electronic devices |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 电子设备 |'
- en: '| ImageNet-CD^∗ | 736,515 | 1,000 | Vehicles | ✓ |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| ImageNet-CD^∗ | 736,515 | 1,000 | 车辆 | ✓ |'
- en: '| Electronic devices |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 电子设备 |'
- en: Another benchmark to evaluate the model generalization capability is the ImageNet-COLORDISTORT
    (ImageNet-CD) [[171](#bib.bib171)], which considers multiple distortions in the
    color of an image using different color space representations. This dataset contains
    the 1,000 classes from ImageNet, removing images without color channels, and the
    same image considers multiple color distortions under the Red Green Blue (RGB),
    Hue-Saturation-Value (HSV), CIELAB, and YCbCr color spaces considered common transformations
    used in image processing.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个评估模型泛化能力的基准是 ImageNet-COLORDISTORT (ImageNet-CD) [[171](#bib.bib171)]，它通过不同的颜色空间表示考虑图像颜色的多种失真。该数据集包含了
    ImageNet 的 1,000 个类别，去除了没有颜色通道的图像，并且同一图像在红绿蓝 (RGB)、色相-饱和度-明度 (HSV)、CIELAB 和 YCbCr
    颜色空间下考虑了多种颜色失真，这些颜色空间是图像处理中的常见变换。
- en: 'It is possible to observe a set of images withdrawn from ImageNet in the top
    left of Fig. [15](#S7.F15 "Figure 15 ‣ VII-B CIFAR-10 and CIFAR-100 ‣ VII Datasets
    ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses").
    Additionally, some images misclassified by multiple classifiers (ImageNet-A) are
    shown in the top right of the same figure. The bottom represents the ImageNet
    with common corruptions and perturbations and is manipulated by multiple image
    techniques on the left and right, respectively. Table [III](#S7.T3 "TABLE III
    ‣ VII-D ImageNet and Variants ‣ VII Datasets ‣ How Deep Learning Sees the World:
    A Survey on Adversarial Attacks & Defenses") summarizes the main characteristics
    of the datasets presented throughout this section.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '可以在图 [15](#S7.F15 "Figure 15 ‣ VII-B CIFAR-10 and CIFAR-100 ‣ VII Datasets
    ‣ How Deep Learning Sees the World: A Survey on Adversarial Attacks & Defenses")
    的左上角观察到一组从ImageNet中提取的图像。此外，图像分类器误分类的图像（ImageNet-A）显示在同一图像的右上角。底部展示了常见破坏和扰动的ImageNet，并通过多种图像技术进行操控，分别位于左侧和右侧。表
    [III](#S7.T3 "TABLE III ‣ VII-D ImageNet and Variants ‣ VII Datasets ‣ How Deep
    Learning Sees the World: A Survey on Adversarial Attacks & Defenses") 总结了本节中介绍的数据集的主要特征。'
- en: 'TABLE IV: Accuracy comparison of different defense mechanisms on CIFAR-10 under
    PGD attack, $l_{\infty}$ and $\epsilon=8/255$. Clean and Robust refers to accuracy
    Without and With Adversarial Attacks, respectively. Defenses with “-” on clean
    accuracy do not have a clean accuracy reported.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 表 IV：在PGD攻击下，不同防御机制在CIFAR-10上的准确性比较，$l_{\infty}$ 和 $\epsilon=8/255$。Clean 和
    Robust 分别指没有对抗攻击和有对抗攻击下的准确性。Clean准确性为“-”的防御方法没有报告干净准确性。
- en: '| Defense Method | Year | Architecture | Accuracy |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 防御方法 | 年份 | 架构 | 准确性 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Clean | Robust |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| Clean | Robust |'
- en: '| --- | --- |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| BPFC [[89](#bib.bib89)] | 2020 | ResNet-18 | 82.4 | 34.4 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| BPFC [[89](#bib.bib89)] | 2020 | ResNet-18 | 82.4 | 34.4 |'
- en: '| SNS [[94](#bib.bib94)] | 2021 | VGG-16 | 86.0 | 39.6 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| SNS [[94](#bib.bib94)] | 2021 | VGG-16 | 86.0 | 39.6 |'
- en: '| AT-MIFGSM [[51](#bib.bib51)] | 2017 | Inception v3 | 85.3 | 45.9 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| AT-MIFGSM [[51](#bib.bib51)] | 2017 | Inception v3 | 85.3 | 45.9 |'
- en: '| AT-PGD [[36](#bib.bib36)] | 2018 | ResNet-18 | 87.3 | 47.0 |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| AT-PGD [[36](#bib.bib36)] | 2018 | ResNet-18 | 87.3 | 47.0 |'
- en: '| RobNets [[118](#bib.bib118)] | 2020 | RobNet-free | 82.8 | 52.6 |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '| RobNets [[118](#bib.bib118)] | 2020 | RobNet-free | 82.8 | 52.6 |'
- en: '| HGD [[17](#bib.bib17)] | 2018 | DUNET | 92.4 | 53.1 |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| HGD [[17](#bib.bib17)] | 2018 | DUNET | 92.4 | 53.1 |'
- en: '| RSLAD [[93](#bib.bib93)] | 2021 | ResNet-18 | 83.4 | 54.2 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| RSLAD [[93](#bib.bib93)] | 2021 | ResNet-18 | 83.4 | 54.2 |'
- en: '| MART [[61](#bib.bib61)] | 2020 | WRN-28-10 | 83.1 | 55.6 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| MART [[61](#bib.bib61)] | 2020 | WRN-28-10 | 83.1 | 55.6 |'
- en: '| TRADES [[54](#bib.bib54)] | 2019 | WRN-34-10 | 84.9 | 56.4 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| TRADES [[54](#bib.bib54)] | 2019 | WRN-34-10 | 84.9 | 56.4 |'
- en: '| BagT [[133](#bib.bib133)] | 2020 | WRN-34-10 | - | 56.4 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| BagT [[133](#bib.bib133)] | 2020 | WRN-34-10 | - | 56.4 |'
- en: '| RO [[132](#bib.bib132)] | 2020 | ResNet-18 | - | 56.8 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| RO [[132](#bib.bib132)] | 2020 | ResNet-18 | - | 56.8 |'
- en: '| DOA [[62](#bib.bib62)] | 2019 | VGGFace | 93.6 | 61.0 |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '| DOA [[62](#bib.bib62)] | 2019 | VGGFace | 93.6 | 61.0 |'
- en: '| AWP [[90](#bib.bib90)] | 2020 | WRN-28-10 | - | 63.6 |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '| AWP [[90](#bib.bib90)] | 2020 | WRN-28-10 | - | 63.6 |'
- en: '| FS [[19](#bib.bib19)] | 2019 | WRN-28-10 | 90.0 | 68.6 |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '| FS [[19](#bib.bib19)] | 2019 | WRN-28-10 | 90.0 | 68.6 |'
- en: '| CAFD [[112](#bib.bib112)] | 2021 | DUNET | 91.1 | 87.2 |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '| CAFD [[112](#bib.bib112)] | 2021 | DUNET | 91.1 | 87.2 |'
- en: VIII Metrics and State-of-the-art Results
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VIII 指标与最新结果
- en: VIII-A Evaluation Metrics
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-A 评估指标
- en: Due to the nature of adversarial examples, they need specific metrics to be
    correctly evaluated and constructed. Following this direction, multiple works
    have been proposing different metrics that calculate the percentage of adversarial
    examples that make a model misclassify (fooling rate), measure the amount of perturbation
    made in an image (destruction rate), and calculate the model robustness to adversarial
    examples (average robustness).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对抗样本的特殊性质，它们需要特定的指标来进行正确评估和构建。遵循这一方向，多个研究提出了不同的指标，用于计算使模型误分类的对抗样本的百分比（欺骗率），衡量图像中的扰动量（破坏率），以及计算模型对对抗样本的鲁棒性（平均鲁棒性）。
- en: VIII-A1 Accuracy
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-A1 准确性
- en: 'This metric measures the number of samples that are correctly predicted by
    the model, which is defined as:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 该指标衡量模型正确预测的样本数量，定义为：
- en: '|  | $\text{accuracy}=\frac{TP+TN}{TP+TN+FP+FN},$ |  | (10) |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{accuracy}=\frac{TP+TN}{TP+TN+FP+FN},$ |  | (10) |'
- en: where $TP$ refers to True Positive, $TN$ to True Negative, $FP$ to False Positive,
    and $FN$ to False Negative. The True Positive and True Negative are the samples
    whose network prediction is the same as the label (correct), and the False Positive
    and False Negative are the samples whose network prediction differs from the label
    (incorrect). When considering original images, this metric is denominated as Clean
    Accuracy and, when using adversarial images, is named as Robust Accuracy.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $TP$ 指真实正例，$TN$ 指真实负例，$FP$ 指假正例，$FN$ 指假负例。真实正例和真实负例是网络预测与标签一致（正确）的样本，而假正例和假负例是网络预测与标签不一致（错误）的样本。在考虑原始图像时，该指标称为清洁准确率，而在使用对抗图像时，则称为鲁棒准确率。
- en: VIII-A2 Fooling Rate
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-A2 欺骗率
- en: After being perturbed to change the classifier label, the fooling rate $FR$ [[173](#bib.bib173)]
    was proposed to calculate the percentage of images.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在被扰动以改变分类器标签之后，提出了欺骗率 $FR$ [[173](#bib.bib173)] 来计算图像的百分比。
- en: VIII-A3 Average Robustness
  id: totrans-336
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-A3 平均鲁棒性
- en: 'To objectively evaluate the robustness to adversarial perturbations of a classifier
    $f$, the average robustness $\hat{p}_{\text{adv}}(f)$ is defined as [[21](#bib.bib21)]:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了客观评估分类器 $f$ 对对抗扰动的鲁棒性，定义了平均鲁棒性 $\hat{p}_{\text{adv}}(f)$ [[21](#bib.bib21)]：
- en: '|  | $\hat{p}_{\text{adv}}(f)=\frac{1}{\mathcal{D}}\sum_{x\in\mathcal{D}}\frac{\&#124;\hat{r}(x)\&#124;_{2}}{\&#124;x\&#124;_{2}},$
    |  | (11) |'
  id: totrans-338
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{p}_{\text{adv}}(f)=\frac{1}{\mathcal{D}}\sum_{x\in\mathcal{D}}\frac{\&#124;\hat{r}(x)\&#124;_{2}}{\&#124;x\&#124;_{2}},$
    |  | (11) |'
- en: where $\hat{r}(x)$ is the estimated minimal perturbation obtained using the
    attack, and $\mathcal{D}$ denotes the test set.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{r}(x)$ 是使用攻击获得的估计最小扰动，$\mathcal{D}$ 表示测试集。
- en: 'TABLE V: Accuracy comparison of different defense mechanisms on CIFAR-10 under
    Auto-Attack attack, $l_{\infty}$ and $\epsilon=8/255$ . Clean and Robust refers
    to accuracy Without and With Adversarial Attacks, respectively.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '表 V: 不同防御机制在 CIFAR-10 上的准确率比较，在 Auto-Attack 攻击下，$l_{\infty}$ 和 $\epsilon=8/255$。清洁和鲁棒指的是没有对抗攻击和有对抗攻击下的准确率。'
- en: '| Architecture | Defense Method | Year | Accuracy |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| 体系结构 | 防御方法 | 年份 | 准确率 |'
- en: '| Clean | Robust |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| 清洁 | 鲁棒 |'
- en: '| WRN28-10 | Input Random [[119](#bib.bib119)] | 2017 | 94.3 | 8.6 |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| WRN28-10 | 输入随机 [[119](#bib.bib119)] | 2017 | 94.3 | 8.6 |'
- en: '| BAT [[57](#bib.bib57)] | 2019 | 92.8 | 29.4 |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| BAT [[57](#bib.bib57)] | 2019 | 92.8 | 29.4 |'
- en: '| FS [[19](#bib.bib19)] | 2019 | 90.0 | 36.6 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| FS [[19](#bib.bib19)] | 2019 | 90.0 | 36.6 |'
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 83.9 | 50.7 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Jpeg [[125](#bib.bib125)] | 2016 | 83.9 | 50.7 |'
- en: '| Pretrain [[174](#bib.bib174)] | 2019 | 87.1 | 54.9 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| Pretrain [[174](#bib.bib174)] | 2019 | 87.1 | 54.9 |'
- en: '| UAT [[84](#bib.bib84)] | 2019 | 86.5 | 56.0 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| UAT [[84](#bib.bib84)] | 2019 | 86.5 | 56.0 |'
- en: '| MART [[61](#bib.bib61)] | 2020 | 87.5 | 56.3 |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| MART [[61](#bib.bib61)] | 2020 | 87.5 | 56.3 |'
- en: '| HYDRA [[92](#bib.bib92)] | 2020 | 89.0 | 57.1 |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| HYDRA [[92](#bib.bib92)] | 2020 | 89.0 | 57.1 |'
- en: '| RST [[85](#bib.bib85)] | 2019 | 89.7 | 59.5 |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| RST [[85](#bib.bib85)] | 2019 | 89.7 | 59.5 |'
- en: '| GI-AT [[68](#bib.bib68)] | 2020 | 89.4 | 59.6 |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| GI-AT [[68](#bib.bib68)] | 2020 | 89.4 | 59.6 |'
- en: '| Proxy [[140](#bib.bib140)] | 2021 | 89.5 | 59.7 |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| Proxy [[140](#bib.bib140)] | 2021 | 89.5 | 59.7 |'
- en: '| AWP [[90](#bib.bib90)] | 2020 | 88.3 | 60.0 |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| AWP [[90](#bib.bib90)] | 2020 | 88.3 | 60.0 |'
- en: '| FDA [[134](#bib.bib134)] | 2021 | 87.3 | 60.8 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| FDA [[134](#bib.bib134)] | 2021 | 87.3 | 60.8 |'
- en: '| HAT [[69](#bib.bib69)] | 2021 | 88.2 | 61.0 |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| HAT [[69](#bib.bib69)] | 2021 | 88.2 | 61.0 |'
- en: '| SCORE [[103](#bib.bib103)] | 2022 | 88.6 | 61.0 |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| SCORE [[103](#bib.bib103)] | 2022 | 88.6 | 61.0 |'
- en: '| PSSiLU [[104](#bib.bib104)] | 2022 | 87.0 | 61.6 |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| PSSiLU [[104](#bib.bib104)] | 2022 | 87.0 | 61.6 |'
- en: '| Gowal et al. [[135](#bib.bib135)] | 2020 | 89.5 | 62.8 |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| Gowal et al. [[135](#bib.bib135)] | 2020 | 89.5 | 62.8 |'
- en: '| IRUGD [[138](#bib.bib138)] | 2021 | 87.5 | 63.4 |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| IRUGD [[138](#bib.bib138)] | 2021 | 87.5 | 63.4 |'
- en: '| Wang et al. [[146](#bib.bib146)] | 2023 | 92.4 | 67.3 |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[146](#bib.bib146)] | 2023 | 92.4 | 67.3 |'
- en: '| STL [[121](#bib.bib121)] | 2019 | 82.2 | 67.9 |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| STL [[121](#bib.bib121)] | 2019 | 82.2 | 67.9 |'
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 89.3 | 85.6 |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| DISCO [[116](#bib.bib116)] | 2022 | 89.3 | 85.6 |'
- en: '| WRN34-10 | Free-AT [[58](#bib.bib58)] | 2019 | 86.1 | 41.5 |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| WRN34-10 | Free-AT [[58](#bib.bib58)] | 2019 | 86.1 | 41.5 |'
- en: '| AT-PGD [[36](#bib.bib36)] | 2018 | 87.1 | 44.0 |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| AT-PGD [[36](#bib.bib36)] | 2018 | 87.1 | 44.0 |'
- en: '| YOPO [[60](#bib.bib60)] | 2019 | 87.2 | 44.8 |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| YOPO [[60](#bib.bib60)] | 2019 | 87.2 | 44.8 |'
- en: '| TLA [[81](#bib.bib81)] | 2019 | 86.2 | 47.4 |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| TLA [[81](#bib.bib81)] | 2019 | 86.2 | 47.4 |'
- en: '| LAT [[82](#bib.bib82)] | 2019 | 87.8 | 49.1 |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| LAT [[82](#bib.bib82)] | 2019 | 87.8 | 49.1 |'
- en: '| SAT [[63](#bib.bib63)] | 2020 | 86.8 | 50.7 |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| SAT [[63](#bib.bib63)] | 2020 | 86.8 | 50.7 |'
- en: '| FAT [[70](#bib.bib70)] | 2022 | 85.3 | 51.1 |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| FAT [[70](#bib.bib70)] | 2022 | 85.3 | 51.1 |'
- en: '| LBGAT [[97](#bib.bib97)] | 2021 | 88.2 | 52.3 |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| LBGAT [[97](#bib.bib97)] | 2021 | 88.2 | 52.3 |'
- en: '| TRADES [[54](#bib.bib54)] | 2019 | 84.9 | 53.1 |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| TRADES [[54](#bib.bib54)] | 2019 | 84.9 | 53.1 |'
- en: '| SAT [[91](#bib.bib91)] | 2020 | 83.5 | 53.3 |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| SAT [[91](#bib.bib91)] | 2020 | 83.5 | 53.3 |'
- en: '| Friend-AT [[64](#bib.bib64)] | 2020 | 84.5 | 55.5 |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| Friend-AT [[64](#bib.bib64)] | 2020 | 84.5 | 55.5 |'
- en: '| AWP [[90](#bib.bib90)] | 2020 | 85.4 | 56.2 |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| AWP [[90](#bib.bib90)] | 2020 | 85.4 | 56.2 |'
- en: '| LTD [[98](#bib.bib98)] | 2021 | 85.2 | 56.9 |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| LTD [[98](#bib.bib98)] | 2021 | 85.2 | 56.9 |'
- en: '| OA-AT [[66](#bib.bib66)] | 2021 | 85.3 | 58.0 |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| OA-AT [[66](#bib.bib66)] | 2021 | 85.3 | 58.0 |'
- en: '| Proxy [[140](#bib.bib140)] | 2022 | 86.7 | 60.3 |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| Proxy [[140](#bib.bib140)] | 2022 | 86.7 | 60.3 |'
- en: '| HAT [[69](#bib.bib69)] | 2021 | 91.5 | 62.8 |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| HAT [[69](#bib.bib69)] | 2021 | 91.5 | 62.8 |'
- en: '| WRN-70-16 | SCORE [[103](#bib.bib103)] | 2022 | 89.0 | 63.4 |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| WRN-70-16 | SCORE [[103](#bib.bib103)] | 2022 | 89.0 | 63.4 |'
- en: '| IRUGD [[138](#bib.bib138)] | 2021 | 91.1 | 65.9 |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| IRUGD [[138](#bib.bib138)] | 2021 | 91.1 | 65.9 |'
- en: '| Gowal et al. [[135](#bib.bib135)] | 2020 | 88.7 | 66.1 |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| Gowal et al. [[135](#bib.bib135)] | 2020 | 88.7 | 66.1 |'
- en: '| FDA [[134](#bib.bib134)] | 2021 | 92.2 | 66.6 |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| FDA [[134](#bib.bib134)] | 2021 | 92.2 | 66.6 |'
- en: '| Wang et al. [[146](#bib.bib146)] | 2023 | 93.3 | 70.7 |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[146](#bib.bib146)] | 2023 | 93.3 | 70.7 |'
- en: '| SODEF [[102](#bib.bib102)] | 2021 | 93.7 | 71.3 |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| SODEF [[102](#bib.bib102)] | 2021 | 93.7 | 71.3 |'
- en: VIII-A4 Destruction Rate
  id: totrans-386
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: VIII-A4 破坏率
- en: 'To evaluate the impact of arbitrary transformations on adversarial images,
    the notion of destruction rate $d$ is introduced and formally defined as [[33](#bib.bib33)]:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估任意转换对对抗图像的影响，引入了破坏率 $d$ 并正式定义为 [[33](#bib.bib33)]：
- en: '|  | $d=\frac{\sum^{n}_{k=1}C(\textbf{X}^{k},y^{k}_{\text{true}})\neg C(\textbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})C(T(\textbf{X}^{k}_{\text{adv}}),y^{k}_{\text{true}})}{\sum^{n}_{k=1}(\textbf{X}^{k},y^{k}_{\text{true}})C(\textbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})},$
    |  | (12) |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | $d=\frac{\sum^{n}_{k=1}C(\textbf{X}^{k},y^{k}_{\text{true}})\neg C(\textbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})C(T(\textbf{X}^{k}_{\text{adv}}),y^{k}_{\text{true}})}{\sum^{n}_{k=1}(\textbf{X}^{k},y^{k}_{\text{true}})C(\textbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})},$
    |  | (12) |'
- en: 'where $n$ is the number of images, $\mathbf{X}^{k}$ is the original image from
    the dataset, $y^{k}_{\text{true}}$ is the true class of this image, $\mathbf{X}^{k}_{\text{adv}}$
    is the adversarial image corresponding to that image, and $T$ is an arbitrary
    image transformation. $\neg C(\mathbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})$
    is defined as the binary negation of $C(\mathbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})$.
    Finally, the function $C(\mathbf{X},y)$ is defined as [[33](#bib.bib33)]:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $n$ 是图像的数量，$\mathbf{X}^{k}$ 是数据集中的原始图像，$y^{k}_{\text{true}}$ 是该图像的真实类别，$\mathbf{X}^{k}_{\text{adv}}$
    是与该图像对应的对抗图像，$T$ 是任意图像转换。 $\neg C(\mathbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})$
    被定义为 $C(\mathbf{X}^{k}_{\text{adv}},y^{k}_{\text{true}})$ 的二进制否定。最后，函数 $C(\mathbf{X},y)$
    被定义为 [[33](#bib.bib33)]：
- en: '|  | $C(\mathbf{X},y)=\begin{cases}1,&amp;\textrm{if image }\textbf{X}\textrm{
    is classified as }y;\\ 0,&amp;\textrm{otherwise}.\end{cases}$ |  | (13) |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '|  | $C(\mathbf{X},y)=\begin{cases}1,&amp;\textrm{如果图像 }\textbf{X}\textrm{
    被分类为 }y;\\ 0,&amp;\textrm{否则}.\end{cases}$ |  | (13) |'
- en: VIII-B Defense Mechanisms Robustness
  id: totrans-391
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: VIII-B 防御机制的稳健性
- en: 'The metric used to evaluate models is accuracy, which evaluates the results
    on both original (Clean Accuracy) and adversarially perturbed (Robust Accuracy)
    datasets. One of the earliest and strongest adversarial attacks proposed was PGD,
    which was used by multiple defenses to evaluate their robustness. Table [IV](#S7.T4
    "TABLE IV ‣ VII-D ImageNet and Variants ‣ VII Datasets ‣ How Deep Learning Sees
    the World: A Survey on Adversarial Attacks & Defenses") displays defenses evaluated
    on CIFAR-10 under multiple steps PGD attack, ordered by increasing robustness.
    For the PGD attack, the best performing defenses are from approaches that use
    supplementary networks (CAFD) or modify the training process (FS and AWP). Overall,
    Wide ResNets [[175](#bib.bib175)] have better robust accuracy, due to high-capacity
    networks exhibiting greater adversarial robustness [[51](#bib.bib51), [36](#bib.bib36)],
    suggesting the usage of these networks in future developments of adversarial attacks
    and defenses.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '用于评估模型的指标是准确率，它评估原始数据集（清洁准确率）和对抗扰动数据集（稳健准确率）上的结果。最早且最强的对抗攻击之一是 PGD，它被多个防御方法用来评估其稳健性。表 [IV](#S7.T4
    "TABLE IV ‣ VII-D ImageNet and Variants ‣ VII Datasets ‣ How Deep Learning Sees
    the World: A Survey on Adversarial Attacks & Defenses") 显示了在 CIFAR-10 上经过多个步骤
    PGD 攻击评估的防御，按稳健性递增排序。对于 PGD 攻击，表现最佳的防御方法来自使用补充网络（CAFD）或修改训练过程（FS 和 AWP）的方案。总体而言，Wide
    ResNets [[175](#bib.bib175)] 具有更好的稳健准确率，因为高容量网络表现出更大的对抗稳健性 [[51](#bib.bib51),
    [36](#bib.bib36)]，这建议未来在对抗攻击和防御的发展中使用这些网络。'
- en: 'To assess the robustness of defenses for white and black-box settings, Auto-Attack
    has gained increased interest over PGD in recent works. Tables [V](#S8.T5 "TABLE
    V ‣ VIII-A3 Average Robustness ‣ VIII-A Evaluation Metrics ‣ VIII Metrics and
    State-of-the-art Results ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses"), [VI](#S8.T6 "TABLE VI ‣ VIII-B Defense Mechanisms Robustness
    ‣ VIII Metrics and State-of-the-art Results ‣ How Deep Learning Sees the World:
    A Survey on Adversarial Attacks & Defenses"), and [VII](#S9.T7 "TABLE VII ‣ IX
    Future Directions ‣ How Deep Learning Sees the World: A Survey on Adversarial
    Attacks & Defenses") present a set of defenses that are evaluated under Auto-Attack,
    on CIFAR-10, CIFAR-100, and ImageNet, respectively, ordered by increasing Robust
    Accuracy. The most used networks are Wide ResNets with different sizes, with the
    biggest Wide ResNet displaying better results overall, and the most resilient
    defense derives from the use of supplementary networks (DISCO), followed by modifying
    the train process (SODEF) and changing network architecture (STL). The results
    suggest that the inclusion of additional components to sanitize inputs of the
    targeted model (use of supplementary networks) is the most resilient approach
    for model robustness in white and black-box settings. The updated results for
    defenses under Auto-Attack can be found on the RobustBench [[176](#bib.bib176)]
    website.'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 为评估白盒和黑盒设置下防御的鲁棒性，Auto-Attack 在最近的研究中比 PGD 更受关注。表 [V](#S8.T5 "TABLE V ‣ VIII-A3
    平均鲁棒性 ‣ VIII-A 评估指标 ‣ VIII 指标和最先进的结果 ‣ 深度学习如何看待世界：对抗攻击与防御的调查")、[VI](#S8.T6 "TABLE
    VI ‣ VIII-B 防御机制鲁棒性 ‣ VIII 指标和最先进的结果 ‣ 深度学习如何看待世界：对抗攻击与防御的调查") 和 [VII](#S9.T7
    "TABLE VII ‣ IX 未来方向 ‣ 深度学习如何看待世界：对抗攻击与防御的调查") 展示了一组在 Auto-Attack 下评估的防御机制，分别在
    CIFAR-10、CIFAR-100 和 ImageNet 上按鲁棒准确性递增排序。最常用的网络是不同规模的 Wide ResNets，其中最大的 Wide
    ResNet 展示了整体更好的结果，而最具韧性的防御源于使用补充网络（DISCO），其次是修改训练过程（SODEF）和改变网络架构（STL）。结果表明，为目标模型清理输入的附加组件（使用补充网络）是白盒和黑盒设置下模型鲁棒性的最具韧性的方法。有关
    Auto-Attack 下防御的更新结果，可以在 RobustBench [[176](#bib.bib176)] 网站上找到。
- en: 'TABLE VI: Accuracy comparison of different defense mechanisms on CIFAR-100
    under Auto-Attack attack, $l_{\infty}$ and $\epsilon=8/255$ . Clean and Robust
    refers to accuracy Without and With Adversarial Attacks, respectively.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VI：在 Auto-Attack 攻击下，不同防御机制在 CIFAR-100 上的准确性比较，$l_{\infty}$ 和 $\epsilon=8/255$。Clean
    和 Robust 分别指没有和有对抗攻击的准确性。
- en: '| Architecture | Defense Method | Year | Accuracy |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 防御方法 | 年份 | 准确性 |'
- en: '| Clean | Robust |'
  id: totrans-396
  prefs: []
  type: TYPE_TB
  zh: '| Clean | Robust |'
- en: '| WRN28-10 | Input Random [[119](#bib.bib119)] | 2017 | 73.6 | 3.3 |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| WRN28-10 | Input Random [[119](#bib.bib119)] | 2017 | 73.6 | 3.3 |'
- en: '| LIIF [[115](#bib.bib115)] | 2021 | 80.3 | 3.4 |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| LIIF [[115](#bib.bib115)] | 2021 | 80.3 | 3.4 |'
- en: '| Bit Reduction [[107](#bib.bib107)] | 2017 | 76.9 | 3.8 |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| Bit Reduction [[107](#bib.bib107)] | 2017 | 76.9 | 3.8 |'
- en: '| Pretrain [[174](#bib.bib174)] | 2019 | 59.2 | 28.4 |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| Pretrain [[174](#bib.bib174)] | 2019 | 59.2 | 28.4 |'
- en: '| SCORE [[103](#bib.bib103)] | 2022 | 63.7 | 31.1 |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| SCORE [[103](#bib.bib103)] | 2022 | 63.7 | 31.1 |'
- en: '| FDA [[134](#bib.bib134)] | 2021 | 62.4 | 32.1 |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| FDA [[134](#bib.bib134)] | 2021 | 62.4 | 32.1 |'
- en: '| Wang et al. [[146](#bib.bib146)] | 2023 | 78.6 | 38.8 |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[146](#bib.bib146)] | 2023 | 78.6 | 38.8 |'
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 61.9 | 39.6 |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '| Jpeg [[125](#bib.bib125)] | 2016 | 61.9 | 39.6 |'
- en: '| STL [[121](#bib.bib121)] | 2019 | 67.4 | 46.1 |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| STL [[121](#bib.bib121)] | 2019 | 67.4 | 46.1 |'
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 72.1 | 67.9 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| DISCO [[116](#bib.bib116)] | 2022 | 72.1 | 67.9 |'
- en: '| WRN34-10 | SAT [[63](#bib.bib63)] | 2020 | 62.8 | 24.6 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| WRN34-10 | SAT [[63](#bib.bib63)] | 2020 | 62.8 | 24.6 |'
- en: '| AWP [[90](#bib.bib90)] | 2020 | 60.4 | 28.9 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| AWP [[90](#bib.bib90)] | 2020 | 60.4 | 28.9 |'
- en: '| LBGAT [[97](#bib.bib97)] | 2021 | 60.6 | 29.3 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| LBGAT [[97](#bib.bib97)] | 2021 | 60.6 | 29.3 |'
- en: '| OA-AT [[66](#bib.bib66)] | 2021 | 65.7 | 30.4 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| OA-AT [[66](#bib.bib66)] | 2021 | 65.7 | 30.4 |'
- en: '| LTD [[98](#bib.bib98)] | 2021 | 64.1 | 30.6 |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '| LTD [[98](#bib.bib98)] | 2021 | 64.1 | 30.6 |'
- en: '| Proxy [[140](#bib.bib140)] | 2022 | 65.9 | 31.2 |'
  id: totrans-412
  prefs: []
  type: TYPE_TB
  zh: '| Proxy [[140](#bib.bib140)] | 2022 | 65.9 | 31.2 |'
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 71.6 | 69.0 |'
  id: totrans-413
  prefs: []
  type: TYPE_TB
  zh: '| DISCO [[116](#bib.bib116)] | 2022 | 71.6 | 69.0 |'
- en: '| WRN-70-16 | SCORE [[103](#bib.bib103)] | 2022 | 65.6 | 33.1 |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '| WRN-70-16 | SCORE [[103](#bib.bib103)] | 2022 | 65.6 | 33.1 |'
- en: '| FDA [[134](#bib.bib134)] | 2021 | 63.6 | 34.6 |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '| FDA [[134](#bib.bib134)] | 2021 | 63.6 | 34.6 |'
- en: '| Gowal et al. [[135](#bib.bib135)] | 2020 | 69.2 | 36.9 |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '| Gowal et al. [[135](#bib.bib135)] | 2020 | 69.2 | 36.9 |'
- en: '| Wang et al. [[146](#bib.bib146)] | 2023 | 75.2 | 42.7 |'
  id: totrans-417
  prefs: []
  type: TYPE_TB
  zh: '| Wang et al. [[146](#bib.bib146)] | 2023 | 75.2 | 42.7 |'
- en: IX Future Directions
  id: totrans-418
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IX 未来方向
- en: Following the de facto standards adopted by the literature, we suggest that
    future proposals of defense mechanisms should be evaluated on Auto-Attack, using
    the robust accuracy as a metric for comparison purposes. The adversarial defense
    that demonstrates better results is Adversarial Training, which should be a requirement
    when evaluating attacks and defenses.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 根据文献中采用的实际标准，我们建议未来的防御机制方案应在 Auto-Attack 上进行评估，使用稳健准确率作为比较指标。表现更好的对抗防御是对抗训练，这应该是评估攻击和防御时的要求。
- en: 'TABLE VII: Accuracy comparison of different defense mechanisms on ImageNet
    under Auto-Attack attack, $l_{\infty}$ and $\epsilon=4/255$. Clean and Robust
    refers to accuracy Without and With Adversarial Attacks, respectively.'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 表 VII：在 Auto-Attack 攻击下，不同防御机制在 ImageNet 上的准确率比较，$l_{\infty}$ 和 $\epsilon=4/255$。清洁和稳健分别指没有和有对抗攻击的准确率。
- en: '| Architecture | Defense Method | Year | Accuracy |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '| 体系结构 | 防御方法 | 年份 | 准确率 |'
- en: '| Clean | Robust |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| 清洁 | 稳健 |'
- en: '| ResNet-18 | Bit Reduction [[107](#bib.bib107)] | 2017 | 67.6 | 4.0 |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-18 | 位减少 [[107](#bib.bib107)] | 2017 | 67.6 | 4.0 |'
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 67.2 | 13.1 |'
  id: totrans-424
  prefs: []
  type: TYPE_TB
  zh: '| Jpeg [[125](#bib.bib125)] | 2016 | 67.2 | 13.1 |'
- en: '| Input Random [[119](#bib.bib119)] | 2017 | 64.0 | 17.8 |'
  id: totrans-425
  prefs: []
  type: TYPE_TB
  zh: '| 输入随机 [[119](#bib.bib119)] | 2017 | 64.0 | 17.8 |'
- en: '| Salman et al. [[177](#bib.bib177)] | 2020 | 52.9 | 25.3 |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '| Salman et al. [[177](#bib.bib177)] | 2020 | 52.9 | 25.3 |'
- en: '| STL [[121](#bib.bib121)] | 2019 | 65.6 | 32.9 |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| STL [[121](#bib.bib121)] | 2019 | 65.6 | 32.9 |'
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 68.0 | 60.9 |'
  id: totrans-428
  prefs: []
  type: TYPE_TB
  zh: '| DISCO [[116](#bib.bib116)] | 2022 | 68.0 | 60.9 |'
- en: '| ResNet-50 | Bit Reduction [[107](#bib.bib107)] | 2017 | 73.8 | 1.9 |'
  id: totrans-429
  prefs: []
  type: TYPE_TB
  zh: '| ResNet-50 | 位减少 [[107](#bib.bib107)] | 2017 | 73.8 | 1.9 |'
- en: '| Input Random [[119](#bib.bib119)] | 2017 | 74.0 | 18.8 |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '| 输入随机 [[119](#bib.bib119)] | 2017 | 74.0 | 18.8 |'
- en: '| Cheap-AT [[65](#bib.bib65)] | 2020 | 55.6 | 26.2 |'
  id: totrans-431
  prefs: []
  type: TYPE_TB
  zh: '| Cheap-AT [[65](#bib.bib65)] | 2020 | 55.6 | 26.2 |'
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 73.6 | 33.4 |'
  id: totrans-432
  prefs: []
  type: TYPE_TB
  zh: '| Jpeg [[125](#bib.bib125)] | 2016 | 73.6 | 33.4 |'
- en: '| Salman et al. [[177](#bib.bib177)] | 2020 | 64.0 | 35.0 |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '| Salman et al. [[177](#bib.bib177)] | 2020 | 64.0 | 35.0 |'
- en: '| STL [[121](#bib.bib121)] | 2019 | 68.3 | 50.2 |'
  id: totrans-434
  prefs: []
  type: TYPE_TB
  zh: '| STL [[121](#bib.bib121)] | 2019 | 68.3 | 50.2 |'
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 72.6 | 68.2 |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| DISCO [[116](#bib.bib116)] | 2022 | 72.6 | 68.2 |'
- en: '| WRN-50-2 | Bit Reduction [[107](#bib.bib107)] | 2017 | 75.1 | 5.0 |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| WRN-50-2 | 位减少 [[107](#bib.bib107)] | 2017 | 75.1 | 5.0 |'
- en: '| Input Random [[119](#bib.bib119)] | 2017 | 71.7 | 23.6 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 输入随机 [[119](#bib.bib119)] | 2017 | 71.7 | 23.6 |'
- en: '| Jpeg [[125](#bib.bib125)] | 2016 | 75.4 | 24.9 |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| Jpeg [[125](#bib.bib125)] | 2016 | 75.4 | 24.9 |'
- en: '| Salman et al. [[177](#bib.bib177)] | 2020 | 68.5 | 38.1 |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| Salman et al. [[177](#bib.bib177)] | 2020 | 68.5 | 38.1 |'
- en: '| DISCO [[116](#bib.bib116)] | 2022 | 75.1 | 69.5 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| DISCO [[116](#bib.bib116)] | 2022 | 75.1 | 69.5 |'
- en: 'The state-of-the-art results show that MNIST and CIFAR-10 datasets are already
    saturated. Other datasets should be further evaluated, namely: 1) CIFAR-100 and
    ImageNet since adversarial defenses do not achieve state-of-the-art clean accuracy
    (91% and 95%, respectively); 2) GTSRB and SVHN, depicting harder scenarios with
    greater variations of background, inclination, and luminosity; and 3) Fashion-MNIST
    that would allow better comprehension of which image properties influence DNNs
    performance (e.g., type of task, image shades, number of classes).'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 最先进的结果表明，MNIST 和 CIFAR-10 数据集已经达到饱和。其他数据集应进一步评估，即：1）CIFAR-100 和 ImageNet，因为对抗防御未能达到最先进的干净准确率（分别为
    91% 和 95%）；2）GTSRB 和 SVHN，描述了具有更大背景、倾斜和光照变化的更困难场景；以及 3）Fashion-MNIST，这将有助于更好地理解哪些图像属性影响
    DNN 的性能（例如，任务类型、图像阴影、类别数量）。
- en: Most works present their results using accuracy as the evaluation metric and,
    more recently, evaluate their defenses on the Auto-Attack. Furthermore, the values
    given for $\epsilon$ in each dataset were standardized by recurrent use. However,
    there should be an effort to develop a metric/process that quantifies the amount
    of perturbation added to the original image. This would ease the expansion of
    adversarial attacks to other datasets that do not have a standardized $\epsilon$
    value.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数工作使用准确率作为评估指标，并且最近在 Auto-Attack 上评估他们的防御。此外，每个数据集的 $\epsilon$ 值通过反复使用进行了标准化。然而，应努力开发一种量化添加到原始图像中的扰动量的度量/过程。这将有助于将对抗攻击扩展到没有标准化
    $\epsilon$ 值的其他数据集。
- en: There has been a greater focus on the development of white-box attacks, which
    consider that the adversary has access to the network and training data, yet this
    is not feasible in real contexts, translating into the need of focusing more on
    the development of black-box attacks. A unique black-box set, physical attacks,
    also require additional evaluation, considering the properties of the real world
    and perturbations commonly found in it. Considering the increasing liberation
    of ML in the real world, end-users can partially control the training phase of
    DNNs, suggesting that gray-box attacks will intensify (access only to network
    or data).
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 对白盒攻击的关注度有所增加，这类攻击假设对手可以访问网络和训练数据，但在实际情况中这并不现实，这意味着需要更多关注黑盒攻击的开发。一类独特的黑盒攻击，即物理攻击，也需要额外的评估，考虑到现实世界的特性和常见的扰动。鉴于机器学习在现实世界中的逐渐解放，最终用户可以部分控制深度神经网络的训练阶段，这表明灰盒攻击将会加剧（仅访问网络或数据）。
- en: The different network architectures are designed to increase the clean accuracy
    of DNNs in particular object recognition datasets, yet there should be further
    evaluation on the impact of the different layers and their structure. ViTs introduce
    a new paradigm in image analysis and are more robust against natural corruptions,
    suggesting that building ViT inherently robust to adversarial examples might be
    a possible solution.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的网络架构旨在提高深度神经网络在特定物体识别数据集上的清晰准确性，但应进一步评估不同层和其结构的影响。ViTs引入了图像分析的新范式，并且对自然腐蚀更为鲁棒，这表明构建对抗样本固有鲁棒的ViT可能是一种解决方案。
- en: DDPM are generative models that perform adversarial purification of images,
    but they can not be applied in real-time since they take up to dozens of seconds
    to create a single purified image. Therefore, an effort on developing close to
    real-time adversarial purification strategies is a viable strategy for future
    works.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: DDPM是生成模型，通过对图像进行对抗性净化，但由于其生成单张净化图像需要数十秒，因此无法实时应用。因此，开发接近实时的对抗性净化策略是未来工作的一个可行方向。
- en: X Conclusions
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: X 结论
- en: DNNs are vulnerable to a set of inputs, denominated as adversarial examples,
    that drastically modify the output of the considered network and are constructed
    by adding a perturbation to the original image. This survey presents background
    concepts, such as adversary capacity and vector norms, essential to comprehend
    adversarial settings, providing a comparison with existing surveys in the literature.
    Adversarial attacks are organized based on the adversary knowledge, highlighting
    the emphasis of current works toward white box settings, and adversarial defenses
    are clustered into six domains, with most works exploring the adversarial training
    strategy. We also present the latest developments of adversarial settings in ViTs
    and describe the commonly used datasets, providing the state-of-the-art results
    in CIFAR-10, CIFAR-100, and ImageNet. Finally, we propose a set of open issues
    that can be explored for subsequent future works.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络对一组输入，即对抗性样本，存在脆弱性，这些样本通过对原始图像添加扰动来严重修改网络的输出。本综述介绍了背景概念，如对手能力和向量范数，这些都是理解对抗性设置的关键，并与现有文献中的综述进行比较。对抗性攻击根据对手的知识进行组织，突出当前工作对白盒设置的重视，对抗性防御则被分为六个领域，大多数工作探讨了对抗性训练策略。我们还介绍了在视觉变换器（ViTs）中的对抗性设置的最新进展，并描述了常用的数据集，提供了CIFAR-10、CIFAR-100和ImageNet中的最新成果。最后，我们提出了一组可供后续研究探索的开放问题。
- en: Acknowledgments
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work was supported in part by the Portuguese FCT/MCTES through National
    Funds and co-funded by EU funds under Project UIDB/50008/2020; in part by the
    FCT Doctoral Grant 2020.09847.BD and Grant 2021.04905.BD;
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分得到了葡萄牙FCT/MCTES通过国家资金支持，并由欧盟资金在项目UIDB/50008/2020下共同资助；部分得到了FCT博士奖学金2020.09847.BD和奖学金2021.04905.BD的支持；
- en: References
  id: totrans-450
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] I. Goodfellow, Y. Bengio, and A. Courville, Deep learning. MIT press, 2016.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] I. Goodfellow, Y. Bengio, 和 A. Courville, 《深度学习》。MIT出版社，2016年。'
- en: '[2] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, and M. Pietikäinen,
    “Deep learning for generic object detection: A survey,” IJCV, vol. 128, no. 2,
    pp. 261–318, 2020.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] L. Liu, W. Ouyang, X. Wang, P. Fieguth, J. Chen, X. Liu, 和 M. Pietikäinen,
    “通用物体检测的深度学习：综述”，《国际计算机视觉杂志》，第128卷，第2期，页261-318，2020年。'
- en: '[3] H.-B. Zhang, Y.-X. Zhang, B. Zhong, Q. Lei, L. Yang, J.-X. Du, and D.-S.
    Chen, “A comprehensive survey of vision-based human action recognition methods,”
    Sensors, vol. 19, no. 5, p. 1005, 2019.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] H.-B. Zhang, Y.-X. Zhang, B. Zhong, Q. Lei, L. Yang, J.-X. Du, 和 D.-S.
    Chen, “基于视觉的人体动作识别方法综述,” Sensors, 第19卷，第5期，第1005页, 2019.'
- en: '[4] I. Masi, Y. Wu, T. Hassner, and P. Natarajan, “Deep face recognition: A
    survey,” in 2018 31st SIBGRAPI, pp. 471–478, IEEE, 2018.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] I. Masi, Y. Wu, T. Hassner, 和 P. Natarajan, “深度人脸识别：综述,” 见 2018年第31届SIBGRAPI,
    第471–478页, IEEE, 2018.'
- en: '[5] M. Wang and W. Deng, “Deep face recognition: A survey,” Neurocomputing,
    vol. 429, pp. 215–244, 2021.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] M. Wang 和 W. Deng, “深度人脸识别：综述,” Neurocomputing, 第429卷，第215–244页, 2021.'
- en: '[6] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
    T. Rault, R. Louf, M. Funtowicz, et al., “Transformers: State-of-the-art natural
    language processing,” in Proceedings of the 2020 conference on empirical methods
    in natural language processing: system demonstrations, pp. 38–45, 2020.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac,
    T. Rault, R. Louf, M. Funtowicz 等, “变换器：最先进的自然语言处理,” 见 2020年自然语言处理经验方法会议论文集：系统演示,
    第38–45页, 2020.'
- en: '[7] D. W. Otter, J. R. Medina, and J. K. Kalita, “A survey of the usages of
    deep learning for natural language processing,” IEEE Transactions on Neural Networks
    and Learning Systems, vol. 32, no. 2, pp. 604–624, 2020.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. W. Otter, J. R. Medina, 和 J. K. Kalita, “深度学习在自然语言处理中的应用综述,” IEEE神经网络与学习系统汇刊,
    第32卷，第2期，第604–624页, 2020.'
- en: '[8] A. I. Maqueda, A. Loquercio, G. Gallego, N. García, and D. Scaramuzza,
    “Event-based vision meets deep learning on steering prediction for self-driving
    cars,” in Proceedings of the IEEE Conference on CVPR, June 2018.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] A. I. Maqueda, A. Loquercio, G. Gallego, N. García, 和 D. Scaramuzza, “基于事件的视觉与深度学习在自驾车方向预测中的应用,”
    见 IEEE CVPR会议论文集, 2018年6月.'
- en: '[9] A. Ndikumana, N. H. Tran, D. H. Kim, K. T. Kim, and C. S. Hong, “Deep learning
    based caching for self-driving cars in multi-access edge computing,” IEEE Transactions
    on Intelligent Transportation Systems, vol. 22, no. 5, pp. 2862–2877, 2021.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Ndikumana, N. H. Tran, D. H. Kim, K. T. Kim, 和 C. S. Hong, “基于深度学习的多接入边缘计算自驾车缓存,”
    IEEE智能交通系统汇刊, 第22卷，第5期，第2862–2877页, 2021.'
- en: '[10] Z. Yuan, Y. Lu, Z. Wang, and Y. Xue, “Droid-sec: Deep learning in android
    malware detection,” in Proceedings of the 2014 ACM Conference on SIGCOMM, SIGCOMM
    ’14, (New York, NY, USA), p. 371–372, Association for Computing Machinery, 2014.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Z. Yuan, Y. Lu, Z. Wang, 和 Y. Xue, “Droid-sec：深度学习在安卓恶意软件检测中的应用,” 见 2014年ACM
    SIGCOMM会议论文集, SIGCOMM ’14, (纽约, NY, USA), 第371–372页, 计算机协会, 2014.'
- en: '[11] R. Vinayakumar, M. Alazab, K. P. Soman, P. Poornachandran, and S. Venkatraman,
    “Robust intelligent malware detection using deep learning,” IEEE Access, vol. 7,
    pp. 46717–46738, 2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] R. Vinayakumar, M. Alazab, K. P. Soman, P. Poornachandran, 和 S. Venkatraman,
    “基于深度学习的鲁棒智能恶意软件检测,” IEEE Access, 第7卷，第46717–46738页, 2019.'
- en: '[12] X. Zhou, W. Liang, I. Kevin, K. Wang, H. Wang, L. T. Yang, and Q. Jin,
    “Deep-learning-enhanced human activity recognition for internet of healthcare
    things,” IEEE Internet of Things Journal, vol. 7, no. 7, pp. 6429–6438, 2020.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] X. Zhou, W. Liang, I. Kevin, K. Wang, H. Wang, L. T. Yang, 和 Q. Jin, “深度学习增强的互联网医疗物联网中的人类活动识别,”
    IEEE物联网期刊, 第7卷，第7期，第6429–6438页, 2020.'
- en: '[13] Z. Liang, G. Zhang, J. X. Huang, and Q. V. Hu, “Deep learning for healthcare
    decision making with emrs,” in 2014 IEEE International Conference on BIBM, pp. 556–559,
    IEEE, 2014.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Z. Liang, G. Zhang, J. X. Huang, 和 Q. V. Hu, “基于电子病历的深度学习医疗决策,” 见 2014年IEEE国际BIBM会议,
    第556–559页, IEEE, 2014.'
- en: '[14] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow,
    and R. Fergus, “Intriguing properties of neural networks,” ArXiv, vol. abs/1312.6199,
    2014.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. J. Goodfellow,
    和 R. Fergus, “神经网络的引人注目的特性,” ArXiv, 第abs/1312.6199卷, 2014.'
- en: '[15] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, and A. Swami,
    “The limitations of deep learning in adversarial settings,” in 2016 IEEE EuroS&P,
    pp. 372–387, IEEE, 2016.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, 和 A. Swami,
    “深度学习在对抗环境中的局限性,” 见 2016年IEEE EuroS&P, 第372–387页, IEEE, 2016.'
- en: '[16] Google, “Vertex ai pricing,” 2022. [Online] Accessed on 10th May 2023.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Google, “Vertex AI定价,” 2022年. [在线] 访问日期：2023年5月10日.'
- en: '[17] F. Liao, M. Liang, Y. Dong, T. Pang, J. Zhu, and X. Hu, “Defense against
    adversarial attacks using high-level representation guided denoiser,” 2018 IEEE/CVF
    Conference on CVPR, pp. 1778–1787, 2018.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] F. Liao, M. Liang, Y. Dong, T. Pang, J. Zhu, 和 X. Hu, “使用高层表示引导去噪器防御对抗攻击,”
    2018年IEEE/CVF CVPR会议, 第1778–1787页, 2018.'
- en: '[18] P. Samangouei, M. Kabkab, and R. Chellappa, “Defense-gan: Protecting classifiers
    against adversarial attacks using generative models,” in International Conference
    on Learning Representations, 2018.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] P. Samangouei, M. Kabkab, 和 R. Chellappa, “Defense-gan: 使用生成模型保护分类器免受对抗攻击，”
    发表在国际学习表征会议，2018年。'
- en: '[19] H. Zhang and J. Wang, “Defense against adversarial attacks using feature
    scattering-based adversarial training,” in NeurIPS, 2019.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] H. Zhang 和 J. Wang, “基于特征散布的对抗训练防御对抗攻击，” 发表在NeurIPS，2019年。'
- en: '[20] Y. Huang and Y. Li, “Zero-shot certified defense against adversarial patches
    with vision transformers,” ArXiv, vol. abs/2111.10481, 2021.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Huang 和 Y. Li, “基于视觉变换器的零样本认证防御对抗补丁，” ArXiv，卷abs/2111.10481, 2021年。'
- en: '[21] S.-M. Moosavi-Dezfooli, A. Fawzi, and P. Frossard, “Deepfool: A simple
    and accurate method to fool deep neural networks,” 2016 IEEE Conference on CVPR,
    pp. 2574–2582, 2016.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S.-M. Moosavi-Dezfooli, A. Fawzi, 和 P. Frossard, “Deepfool: 一种简单而准确的方法来欺骗深度神经网络，”
    2016 IEEE CVPR会议，第2574–2582页, 2016年。'
- en: '[22] A. Dabouei, S. Soleymani, F. Taherkhani, J. M. Dawson, and N. M. Nasrabadi,
    “Smoothfool: An efficient framework for computing smooth adversarial perturbations,”
    2020 IEEE WACV, pp. 2654–2663, 2020.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] A. Dabouei, S. Soleymani, F. Taherkhani, J. M. Dawson, 和 N. M. Nasrabadi,
    “Smoothfool: 高效计算平滑对抗扰动的框架，” 2020 IEEE WACV，第2654–2663页, 2020年。'
- en: '[23] N. Akhtar and A. Mian, “Threat of adversarial attacks on deep learning
    in computer vision: A survey,” IEEE Access, vol. 6, pp. 14410–14430, 2018.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] N. Akhtar 和 A. Mian, “深度学习在计算机视觉中的对抗攻击威胁：综述，” IEEE Access，第6卷，第14410–14430页,
    2018年。'
- en: '[24] Q. Liu, P. Li, W. Zhao, W. Cai, S. Yu, and V. C. M. Leung, “A survey on
    security threats and defensive techniques of machine learning: A data driven view,”
    IEEE Access, vol. 6, pp. 12103–12117, 2018.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Q. Liu, P. Li, W. Zhao, W. Cai, S. Yu, 和 V. C. M. Leung, “机器学习的安全威胁与防御技术调查：数据驱动的视角，”
    IEEE Access，第6卷，第12103–12117页, 2018年。'
- en: '[25] A. Serban, E. Poll, and J. Visser, “Adversarial examples on object recognition:
    A comprehensive survey,” ACM Computing Surveys, vol. 53, no. 3, 2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] A. Serban, E. Poll, 和 J. Visser, “物体识别中的对抗样本：综合调查，” ACM Computing Surveys，第53卷，第3期,
    2020年。'
- en: '[26] S. Qiu, Q. Liu, S. Zhou, and C. Wu, “Review of artificial intelligence
    adversarial attack and defense technologies,” Applied Sciences, vol. 9, no. 5,
    p. 909, 2019.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Qiu, Q. Liu, S. Zhou, 和 C. Wu, “人工智能对抗攻击与防御技术的回顾，” Applied Sciences，第9卷，第5期，第909页,
    2019年。'
- en: '[27] H. Xu, Y. Ma, H.-C. Liu, D. Deb, H. Liu, J.-L. Tang, and A. K. Jain, “Adversarial
    attacks and defenses in images, graphs and text: A review,” International Journal
    of Automation and Computing, vol. 17, pp. 151–178, 2020.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] H. Xu, Y. Ma, H.-C. Liu, D. Deb, H. Liu, J.-L. Tang, 和 A. K. Jain, “图像、图形和文本中的对抗攻击与防御：综述，”
    International Journal of Automation and Computing，第17卷，第151–178页, 2020年。'
- en: '[28] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay, and D. Mukhopadhyay,
    “A survey on adversarial attacks and defences,” CAAI Transactions on Intelligence
    Technology, vol. 6, no. 1, pp. 25–45, 2021.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. Chakraborty, M. Alam, V. Dey, A. Chattopadhyay, 和 D. Mukhopadhyay,
    “对抗攻击与防御的综述，” CAAI Transactions on Intelligence Technology，第6卷，第1期，第25–45页, 2021年。'
- en: '[29] T. Long, Q. Gao, L. Xu, and Z. Zhou, “A survey on adversarial attacks
    in computer vision: Taxonomy, visualization and future directions,” Computers
    & Security, p. 102847, 2022.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] T. Long, Q. Gao, L. Xu, 和 Z. Zhou, “计算机视觉中的对抗攻击调查：分类、可视化和未来方向，” Computers
    & Security，第102847页, 2022年。'
- en: '[30] H. Liang, E. He, Y. Zhao, Z. Jia, and H. Li, “Adversarial attack and defense:
    A survey,” Electronics, vol. 11, no. 8, p. 1283, 2022.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] H. Liang, E. He, Y. Zhao, Z. Jia, 和 H. Li, “对抗攻击与防御：综述，” Electronics，第11卷，第8期，第1283页,
    2022年。'
- en: '[31] S. Zhou, C. Liu, D. Ye, T. Zhu, W. Zhou, and P. S. Yu, “Adversarial attacks
    and defenses in deep learning: From a perspective of cybersecurity,” ACM Computing
    Surveys, vol. 55, no. 8, pp. 1–39, 2022.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] S. Zhou, C. Liu, D. Ye, T. Zhu, W. Zhou, 和 P. S. Yu, “深度学习中的对抗攻击与防御：从网络安全的角度，”
    ACM Computing Surveys，第55卷，第8期，第1–39页, 2022年。'
- en: '[32] I. J. Goodfellow, J. Shlens, and C. Szegedy, “Explaining and harnessing
    adversarial examples,” ArXiv, vol. abs/1412.6572, 2015.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] I. J. Goodfellow, J. Shlens, 和 C. Szegedy, “解释和利用对抗样本，” ArXiv，卷abs/1412.6572,
    2015年。'
- en: '[33] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial examples in
    the physical world,” ArXiv, vol. abs/1607.02533, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. Kurakin, I. J. Goodfellow, 和 S. Bengio, “物理世界中的对抗样本，” ArXiv，卷abs/1607.02533,
    2017年。'
- en: '[34] N. Carlini and D. Wagner, “Towards evaluating the robustness of neural
    networks,” in 2017 ieee symposium on sp, pp. 39–57, Ieee, 2017.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] N. Carlini 和 D. Wagner, “评估神经网络鲁棒性的进展，” 发表在2017 IEEE SP研讨会，第39–57页, IEEE,
    2017年。'
- en: '[35] F. Tramèr, N. Papernot, I. J. Goodfellow, D. Boneh, and P. Mcdaniel, “The
    space of transferable adversarial examples,” ArXiv, vol. abs/1704.03453, 2017.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] F. Tramèr, N. Papernot, I. J. Goodfellow, D. Boneh, 和 P. Mcdaniel, “可转移对抗样本的空间”，ArXiv，卷号abs/1704.03453，2017年。'
- en: '[36] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu, “Towards deep
    learning models resistant to adversarial attacks,” ArXiv, vol. abs/1706.06083,
    2018.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, 和 A. Vladu, “朝着对抗攻击抗性的深度学习模型”，ArXiv，卷号abs/1706.06083，2018年。'
- en: '[37] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, and D. Song, “Generating adversarial
    examples with adversarial networks,” in Proceedings of the 27th International
    Joint Conference on Artificial Intelligence, pp. 3905–3911, 2018.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] C. Xiao, B. Li, J.-Y. Zhu, W. He, M. Liu, 和 D. Song, “利用对抗网络生成对抗样本”，发表于第27届国际联合人工智能会议，页码3905–3911，2018年。'
- en: '[38] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. C. Courville, and Y. Bengio, “Generative adversarial nets,” in NIPS,
    2014.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. C. Courville, 和 Y. Bengio, “生成对抗网络”，发表于NIPS，2014年。'
- en: '[39] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, and J. Li, “Boosting
    adversarial attacks with momentum,” 2018 IEEE/CVF Conference on CVPR, pp. 9185–9193,
    2018.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Y. Dong, F. Liao, T. Pang, H. Su, J. Zhu, X. Hu, 和 J. Li, “通过动量提升对抗攻击”，2018
    IEEE/CVF CVPR会议，页码9185–9193，2018年。'
- en: '[40] F. Croce and M. Hein, “Sparse and imperceivable adversarial attacks,”
    2019 IEEE/CVF ICCV, pp. 4723–4731, 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] F. Croce 和 M. Hein, “稀疏和不可感知的对抗攻击”，2019 IEEE/CVF ICCV，页码4723–4731，2019年。'
- en: '[41] R. Duan, X. Ma, Y. Wang, J. Bailey, A. K. Qin, and Y. Yang, “Adversarial
    camouflage: Hiding physical-world attacks with natural styles,” 2020 IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (CVPR), pp. 997–1005, 2020.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] R. Duan, X. Ma, Y. Wang, J. Bailey, A. K. Qin, 和 Y. Yang, “对抗伪装：用自然风格隐藏物理世界攻击”，2020
    IEEE/CVF计算机视觉与模式识别会议（CVPR），页码997–1005，2020年。'
- en: '[42] Z. Wang, H. Guo, Z. Zhang, W. Liu, Z. Qin, and K. Ren, “Feature importance-aware
    transferable adversarial attacks,” in Proceedings of the IEEE/CVF ICCV, pp. 7639–7648,
    2021.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Z. Wang, H. Guo, Z. Zhang, W. Liu, Z. Qin, 和 K. Ren, “特征重要性感知的可转移对抗攻击”，发表于IEEE/CVF
    ICCV会议，页码7639–7648，2021年。'
- en: '[43] Z. Yuan, J. Zhang, Y. Jia, C. Tan, T. Xue, and S. Shan, “Meta gradient
    adversarial attack,” in Proceedings of the IEEE/CVF ICCV, pp. 7748–7757, 2021.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Z. Yuan, J. Zhang, Y. Jia, C. Tan, T. Xue, 和 S. Shan, “元梯度对抗攻击”，发表于IEEE/CVF
    ICCV会议，页码7748–7757，2021年。'
- en: '[44] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, and P. Frossard, “Universal
    adversarial perturbations,” 2017 IEEE Conference on CVPR, pp. 86–94, 2017.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S.-M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, 和 P. Frossard, “通用对抗扰动”，2017
    IEEE会议CVPR，页码86–94，2017年。'
- en: '[45] J. Hayes and G. Danezis, “Learning universal adversarial perturbations
    with generative models,” 2018 IEEE SPW, pp. 43–49, 2018.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J. Hayes 和 G. Danezis, “利用生成模型学习通用对抗扰动”，2018 IEEE SPW，页码43–49，2018年。'
- en: '[46] A. Ilyas, L. Engstrom, A. Athalye, and J. Lin, “Black-box adversarial
    attacks with limited queries and information,” in International Conference on
    Machine Learning, pp. 2137–2146, PMLR, 2018.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] A. Ilyas, L. Engstrom, A. Athalye, 和 J. Lin, “黑箱对抗攻击：有限查询和信息下的攻击”，发表于国际机器学习会议，页码2137–2146，PMLR，2018年。'
- en: '[47] M. Wicker, X. Huang, and M. Kwiatkowska, “Feature-guided black-box safety
    testing of deep neural networks,” in TACAS, 2018.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] M. Wicker, X. Huang, 和 M. Kwiatkowska, “特征引导的黑箱深度神经网络安全测试”，发表于TACAS，2018年。'
- en: '[48] M. Andriushchenko, F. Croce, N. Flammarion, and M. Hein, “Square attack:
    a query-efficient black-box adversarial attack via random search,” in Computer
    Vision – ECCV 2020, pp. 484–501, Springer, 2020.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] M. Andriushchenko, F. Croce, N. Flammarion, 和 M. Hein, “平方攻击：通过随机搜索实现的查询高效黑箱对抗攻击”，发表于计算机视觉
    – ECCV 2020，页码484–501，Springer，2020年。'
- en: '[49] F. Croce and M. Hein, “Reliable evaluation of adversarial robustness with
    an ensemble of diverse parameter-free attacks,” in International conference on
    machine learning, pp. 2206–2216, PMLR, 2020.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] F. Croce 和 M. Hein, “通过多样化的无参数攻击集进行对抗鲁棒性的可靠评估”，发表于国际机器学习会议，页码2206–2216，PMLR，2020年。'
- en: '[50] F. Croce and M. Hein, “Minimally distorted adversarial examples with a
    fast adaptive boundary attack,” in International Conference on Machine Learning,
    pp. 2196–2205, PMLR, 2020.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] F. Croce 和 M. Hein, “通过快速自适应边界攻击最小化扭曲的对抗样本”，发表于国际机器学习会议，页码2196–2205，PMLR，2020年。'
- en: '[51] A. Kurakin, I. J. Goodfellow, and S. Bengio, “Adversarial machine learning
    at scale,” ArXiv, vol. abs/1611.01236, 2017.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] A. Kurakin, I. J. Goodfellow, 和 S. Bengio, “大规模对抗机器学习”，ArXiv，卷号abs/1611.01236，2017年。'
- en: '[52] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel,
    “Ensemble adversarial training: Attacks and defenses,” in International Conference
    on Learning Representations, 2018.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] F. Tramèr, A. Kurakin, N. Papernot, I. Goodfellow, D. Boneh 和 P. McDaniel,
    “集成对抗训练：攻击与防御，”发表于2018年国际学习表征会议。'
- en: '[53] C. K. Mummadi, T. Brox, and J. H. Metzen, “Defending against universal
    perturbations with shared adversarial training,” 2019 IEEE/CVF ICCV, pp. 4927–4936,
    2019.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] C. K. Mummadi, T. Brox 和 J. H. Metzen, “通过共享对抗训练防御通用扰动，”2019年IEEE/CVF
    ICCV，页码4927–4936，2019年。'
- en: '[54] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui, and M. Jordan, “Theoretically
    principled trade-off between robustness and accuracy,” in International conference
    on machine learning, pp. 7472–7482, PMLR, 2019.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] H. Zhang, Y. Yu, J. Jiao, E. Xing, L. El Ghaoui 和 M. Jordan, “理论上原则性的鲁棒性与准确性权衡，”发表于国际机器学习会议，页码7472–7482，PMLR，2019年。'
- en: '[55] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner, and A. Madry, “Robustness
    may be at odds with accuracy,” in International Conference on Learning Representations,
    2019.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] D. Tsipras, S. Santurkar, L. Engstrom, A. Turner 和 A. Madry, “鲁棒性可能与准确性相矛盾，”发表于2019年国际学习表征会议。'
- en: '[56] D. Su, H. Zhang, H. Chen, J. Yi, P.-Y. Chen, and Y. Gao, “Is robustness
    the cost of accuracy?–a comprehensive study on the robustness of 18 deep image
    classification models,” in Proceedings of the ECCV, pp. 631–648, 2018.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] D. Su, H. Zhang, H. Chen, J. Yi, P.-Y. Chen 和 Y. Gao, “鲁棒性是否是准确性的代价？–对18种深度图像分类模型鲁棒性的综合研究，”发表于ECCV会议，页码631–648，2018年。'
- en: '[57] J. Wang and H. Zhang, “Bilateral adversarial training: Towards fast training
    of more robust models against adversarial attacks,” in Proceedings of the IEEE/CVF
    ICCV, pp. 6629–6638, 2019.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] J. Wang 和 H. Zhang, “双边对抗训练：实现对抗攻击下更鲁棒模型的快速训练，”发表于IEEE/CVF ICCV会议，页码6629–6638，2019年。'
- en: '[58] A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L. S.
    Davis, G. Taylor, and T. Goldstein, “Adversarial training for free!,” Advances
    in Neural Information Processing Systems, vol. 32, 2019.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] A. Shafahi, M. Najibi, M. A. Ghiasi, Z. Xu, J. Dickerson, C. Studer, L.
    S. Davis, G. Taylor 和 T. Goldstein, “免费的对抗训练！，”发表于《神经信息处理系统进展》，第32卷，2019年。'
- en: '[59] R. E. Kopp, “Pontryagin maximum principle,” in Mathematics in Science
    and Engineering, vol. 5, pp. 255–279, Elsevier, 1962.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] R. E. Kopp, “庞特里亚金最大原理，”发表于《科学与工程中的数学》，第5卷，页码255–279，Elsevier，1962年。'
- en: '[60] D. Zhang, T. Zhang, Y. Lu, Z. Zhu, and B. Dong, “You only propagate once:
    Accelerating adversarial training via maximal principle,” Advances in Neural Information
    Processing Systems, vol. 32, 2019.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] D. Zhang, T. Zhang, Y. Lu, Z. Zhu 和 B. Dong, “你只需传播一次：通过最大原理加速对抗训练，”发表于《神经信息处理系统进展》，第32卷，2019年。'
- en: '[61] Y. Wang, D. Zou, J. Yi, J. Bailey, X. Ma, and Q. Gu, “Improving adversarial
    robustness requires revisiting misclassified examples,” in International Conference
    on Learning Representations, 2020.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Y. Wang, D. Zou, J. Yi, J. Bailey, X. Ma 和 Q. Gu, “提高对抗鲁棒性需要重新审视误分类示例，”发表于2020年国际学习表征会议。'
- en: '[62] T. Wu, L. Tong, and Y. Vorobeychik, “Defending against physically realizable
    attacks on image classification,” arXiv preprint arXiv:1909.09552, 2019.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] T. Wu, L. Tong 和 Y. Vorobeychik, “防御对图像分类的物理可实现攻击，”arXiv预印本 arXiv:1909.09552，2019年。'
- en: '[63] C. Sitawarin, S. Chakraborty, and D. Wagner, “Improving adversarial robustness
    through progressive hardening,” arXiv preprint arXiv:2003.09347, vol. 4, no. 5,
    2020.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] C. Sitawarin, S. Chakraborty 和 D. Wagner, “通过渐进硬化提高对抗鲁棒性，”arXiv预印本 arXiv:2003.09347，第4卷，第5期，2020年。'
- en: '[64] J. Zhang, X. Xu, B. Han, G. Niu, L. Cui, M. Sugiyama, and M. Kankanhalli,
    “Attacks which do not kill training make adversarial learning stronger,” in International
    conference on machine learning, pp. 11278–11287, PMLR, 2020.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] J. Zhang, X. Xu, B. Han, G. Niu, L. Cui, M. Sugiyama 和 M. Kankanhalli,
    “不会终止训练的攻击使对抗学习更强，”发表于国际机器学习会议，页码11278–11287，PMLR，2020年。'
- en: '[65] E. Wong, L. Rice, and J. Z. Kolter, “Fast is better than free: Revisiting
    adversarial training,” arXiv preprint arXiv:2001.03994, 2020.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] E. Wong, L. Rice 和 J. Z. Kolter, “快速比免费的更好：重新审视对抗训练，”arXiv预印本 arXiv:2001.03994，2020年。'
- en: '[66] S. Addepalli, S. Jain, G. Sriramanan, S. Khare, and V. B. Radhakrishnan,
    “Towards achieving adversarial robustness beyond perceptual limits,” in ICML 2021
    Workshop on Adversarial Machine Learning, 2021.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Addepalli, S. Jain, G. Sriramanan, S. Khare 和 V. B. Radhakrishnan,
    “实现超越感知极限的对抗鲁棒性，”发表于2021年ICML对抗机器学习研讨会。'
- en: '[67] R. Zhang, P. Isola, A. A. Efros, E. Shechtman, and O. Wang, “The unreasonable
    effectiveness of deep features as a perceptual metric,” in Proceedings of the
    IEEE conference on CVPR, pp. 586–595, 2018.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] R. Zhang、P. Isola、A. A. Efros、E. Shechtman 和 O. Wang，“深度特征作为感知度量的非凡有效性”，发表于IEEE
    CVPR 会议论文集，页码：586–595，2018年。'
- en: '[68] J. Zhang, J. Zhu, G. Niu, B. Han, M. Sugiyama, and M. Kankanhalli, “Geometry-aware
    instance-reweighted adversarial training,” arXiv preprint arXiv:2010.01736, 2020.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Zhang、J. Zhu、G. Niu、B. Han、M. Sugiyama 和 M. Kankanhalli，“几何感知实例重加权对抗训练”，arXiv
    预印本 arXiv:2010.01736，2020年。'
- en: '[69] R. Rade and S.-M. Moosavi-Dezfooli, “Helper-based adversarial training:
    Reducing excessive margin to achieve a better accuracy vs. robustness trade-off,”
    in ICML 2021 Workshop on Adversarial Machine Learning, 2021.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] R. Rade 和 S.-M. Moosavi-Dezfooli，“基于助手的对抗训练：减少过度边际以实现更好的准确性与鲁棒性权衡”，发表于ICML
    2021对抗机器学习研讨会，2021年。'
- en: '[70] J. Chen, Y. Cheng, Z. Gan, Q. Gu, and J. Liu, “Efficient robust training
    via backward smoothing,” in Proceedings of the AAAI Conference on Artificial Intelligence,
    vol. 36, pp. 6222–6230, 2022.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Chen、Y. Cheng、Z. Gan、Q. Gu 和 J. Liu，“通过反向平滑实现高效的鲁棒训练”，发表于AAAI人工智能会议论文集，卷号：36，页码：6222–6230，2022年。'
- en: '[71] S. S. Gu and L. Rigazio, “Towards deep neural network architectures robust
    to adversarial examples,” ArXiv, vol. abs/1412.5068, 2015.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] S. S. Gu 和 L. Rigazio，“朝向对抗样本鲁棒的深度神经网络架构”，ArXiv，卷号：abs/1412.5068，2015年。'
- en: '[72] N. Papernot, P. Mcdaniel, X. Wu, S. Jha, and A. Swami, “Distillation as
    a defense to adversarial perturbations against deep neural networks,” 2016 IEEE
    Symposium on SP, pp. 582–597, 2016.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] N. Papernot、P. Mcdaniel、X. Wu、S. Jha 和 A. Swami，“作为对抗扰动防御的蒸馏”，2016 IEEE
    SP 研讨会，页码：582–597，2016年。'
- en: '[73] N. Papernot and P. Mcdaniel, “Extending defensive distillation,” ArXiv,
    vol. abs/1705.05264, 2017.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] N. Papernot 和 P. Mcdaniel，“扩展防御蒸馏”，ArXiv，卷号：abs/1705.05264，2017年。'
- en: '[74] K. Chalupka, P. Perona, and F. Eberhardt, “Visual causal feature learning,”
    in UAI, 2015.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] K. Chalupka、P. Perona 和 F. Eberhardt，“视觉因果特征学习”，发表于UAI，2015年。'
- en: '[75] R. Huang, B. Xu, D. Schuurmans, and C. Szepesvari, “Learning with a strong
    adversary,” ArXiv, vol. abs/1511.03034, 2015.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] R. Huang、B. Xu、D. Schuurmans 和 C. Szepesvari，“与强大对手的学习”，ArXiv，卷号：abs/1511.03034，2015年。'
- en: '[76] S. Zheng, Y. Song, T. Leung, and I. J. Goodfellow, “Improving the robustness
    of deep neural networks via stability training,” 2016 IEEE Conference on CVPR,
    pp. 4480–4488, 2016.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] S. Zheng、Y. Song、T. Leung 和 I. J. Goodfellow，“通过稳定性训练提高深度神经网络的鲁棒性”，2016
    IEEE CVPR 会议，页码：4480–4488，2016年。'
- en: '[77] V. Zantedeschi, M.-I. Nicolae, and A. Rawat, “Efficient defenses against
    adversarial attacks,” Proceedings of the 10th ACM Workshop on Artificial Intelligence
    and Security, 2017.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] V. Zantedeschi、M.-I. Nicolae 和 A. Rawat，“对抗攻击的有效防御”，第10届ACM人工智能与安全研讨会论文集，2017年。'
- en: '[78] A. F. Agarap, “Deep learning using rectified linear units (relu),” ArXiv,
    vol. abs/1803.08375, 2018.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] A. F. Agarap，“使用整流线性单元（relu）的深度学习”，ArXiv，卷号：abs/1803.08375，2018年。'
- en: '[79] R. H. Hahnloser, R. Sarpeshkar, M. A. Mahowald, R. J. Douglas, and H. S.
    Seung, “Digital selection and analogue amplification coexist in a cortex-inspired
    silicon circuit,” Nature, vol. 405, no. 6789, pp. 947–951, 2000.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] R. H. Hahnloser、R. Sarpeshkar、M. A. Mahowald、R. J. Douglas 和 H. S. Seung，“数字选择和模拟放大在类皮层硅电路中共存”，《自然》，卷号：405，期号：6789，页码：947–951，2000年。'
- en: '[80] S. S. Liew, M. Khalil-Hani, and R. Bakhteri, “Bounded activation functions
    for enhanced training stability of deep neural networks on visual pattern recognition
    problems,” Neurocomputing, vol. 216, pp. 718–734, 2016.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] S. S. Liew、M. Khalil-Hani 和 R. Bakhteri，“用于提高视觉模式识别问题中深度神经网络训练稳定性的有界激活函数”，《神经计算》，卷号：216，页码：718–734，2016年。'
- en: '[81] C. Mao, Z. Zhong, J. Yang, C. Vondrick, and B. Ray, “Metric learning for
    adversarial robustness,” Advances in Neural Information Processing Systems, vol. 32,
    2019.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] C. Mao、Z. Zhong、J. Yang、C. Vondrick 和 B. Ray，“对抗鲁棒性的度量学习”，神经信息处理系统进展，卷号：32，2019年。'
- en: '[82] N. Kumari, M. Singh, A. Sinha, H. Machiraju, B. Krishnamurthy, and V. N.
    Balasubramanian, “Harnessing the vulnerability of latent layers in adversarially
    trained models,” in Proceedings of the 28th International Joint Conference on
    Artificial Intelligence, pp. 2779–2785, 2019.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] N. Kumari、M. Singh、A. Sinha、H. Machiraju、B. Krishnamurthy 和 V. N. Balasubramanian，“利用对抗训练模型中潜在层的脆弱性”，发表于第28届国际人工智能联合会议论文集，页码：2779–2785，2019年。'
- en: '[83] S.-M. Moosavi-Dezfooli, A. Fawzi, J. Uesato, and P. Frossard, “Robustness
    via curvature regularization, and vice versa,” in Proceedings of the IEEE/CVF
    Conference on CVPR, pp. 9078–9086, 2019.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] S.-M. Moosavi-Dezfooli、A. Fawzi、J. Uesato 和 P. Frossard，“通过曲率正则化实现鲁棒性，反之亦然”，发表于
    IEEE/CVF 计算机视觉与模式识别会议，pp. 9078–9086，2019年。'
- en: '[84] J.-B. Alayrac, J. Uesato, P.-S. Huang, A. Fawzi, R. Stanforth, and P. Kohli,
    “Are labels required for improving adversarial robustness?,” Advances in Neural
    Information Processing Systems, vol. 32, 2019.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] J.-B. Alayrac、J. Uesato、P.-S. Huang、A. Fawzi、R. Stanforth 和 P. Kohli，“改善对抗鲁棒性是否需要标签？”，神经信息处理系统进展，卷
    32，2019年。'
- en: '[85] Y. Carmon, A. Raghunathan, L. Schmidt, J. C. Duchi, and P. S. Liang, “Unlabeled
    data improves adversarial robustness,” Advances in neural information processing
    systems, vol. 32, 2019.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Y. Carmon、A. Raghunathan、L. Schmidt、J. C. Duchi 和 P. S. Liang，“未标记数据提高对抗鲁棒性”，神经信息处理系统进展，卷
    32，2019年。'
- en: '[86] H. Scudder, “Probability of error of some adaptive pattern-recognition
    machines,” IEEE Transactions on Information Theory, vol. 11, no. 3, pp. 363–371,
    1965.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] H. Scudder，“某些自适应模式识别机器的错误概率”，IEEE 信息理论汇刊，卷 11，第 3 期，pp. 363–371，1965年。'
- en: '[87] J. Cohen, E. Rosenfeld, and Z. Kolter, “Certified adversarial robustness
    via randomized smoothing,” in international conference on machine learning, pp. 1310–1320,
    PMLR, 2019.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] J. Cohen、E. Rosenfeld 和 Z. Kolter，“通过随机平滑实现认证对抗鲁棒性”，发表于国际机器学习会议，pp. 1310–1320，PMLR，2019年。'
- en: '[88] X. Gao, R. K. Saha, M. R. Prasad, and A. Roychoudhury, “Fuzz testing based
    data augmentation to improve robustness of deep neural networks,” 2020 IEEE/ACM
    42nd ICSE, pp. 1147–1158, 2020.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] X. Gao、R. K. Saha、M. R. Prasad 和 A. Roychoudhury，“基于模糊测试的数据增强以提高深度神经网络的鲁棒性”，2020
    IEEE/ACM 第42届 ICSE，pp. 1147–1158，2020年。'
- en: '[89] S. Addepalli, S. VivekB., A. Baburaj, G. Sriramanan, and R. V. Babu, “Towards
    achieving adversarial robustness by enforcing feature consistency across bit planes,”
    2020 IEEE/CVF CVPR, pp. 1017–1026, 2020.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] S. Addepalli、S. VivekB.、A. Baburaj、G. Sriramanan 和 R. V. Babu，“通过在比特平面间强制特征一致性来实现对抗鲁棒性”，2020
    IEEE/CVF CVPR，pp. 1017–1026，2020年。'
- en: '[90] D. Wu, S.-T. Xia, and Y. Wang, “Adversarial weight perturbation helps
    robust generalization,” Advances in Neural Information Processing Systems, vol. 33,
    pp. 2958–2969, 2020.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] D. Wu、S.-T. Xia 和 Y. Wang，“对抗权重扰动有助于鲁棒泛化”，神经信息处理系统进展，卷 33，pp. 2958–2969，2020年。'
- en: '[91] L. Huang, C. Zhang, and H. Zhang, “Self-adaptive training: beyond empirical
    risk minimization,” Advances in neural information processing systems, vol. 33,
    pp. 19365–19376, 2020.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] L. Huang、C. Zhang 和 H. Zhang，“自适应训练：超越经验风险最小化”，神经信息处理系统进展，卷 33，pp. 19365–19376，2020年。'
- en: '[92] V. Sehwag, S. Wang, P. Mittal, and S. Jana, “Hydra: Pruning adversarially
    robust neural networks,” Advances in Neural Information Processing Systems, vol. 33,
    pp. 19655–19666, 2020.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] V. Sehwag、S. Wang、P. Mittal 和 S. Jana，“Hydra：修剪对抗鲁棒的神经网络”，神经信息处理系统进展，卷
    33，pp. 19655–19666，2020年。'
- en: '[93] B. Zi, S. Zhao, X. Ma, and Y.-G. Jiang, “Revisiting adversarial robustness
    distillation: Robust soft labels make student better,” in Proceedings of the IEEE/CVF
    ICCV, pp. 16443–16452, 2021.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] B. Zi、S. Zhao、X. Ma 和 Y.-G. Jiang，“重新审视对抗鲁棒性蒸馏：鲁棒软标签使学生更好”，发表于 IEEE/CVF
    ICCV，pp. 16443–16452，2021年。'
- en: '[94] C. Zhang, A. Liu, X. Liu, Y. Xu, H. Yu, Y. Ma, and T. Li, “Interpreting
    and improving adversarial robustness of deep neural networks with neuron sensitivity,”
    IEEE Transactions on Image Processing, vol. 30, pp. 1291–1304, 2021.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] C. Zhang、A. Liu、X. Liu、Y. Xu、H. Yu、Y. Ma 和 T. Li，“通过神经元敏感性解释和改进深度神经网络的对抗鲁棒性”，IEEE
    图像处理汇刊，卷 30，pp. 1291–1304，2021年。'
- en: '[95] S. Kundu, M. Nazemi, P. A. Beerel, and M. Pedram, “Dnr: A tunable robust
    pruning framework through dynamic network rewiring of dnns,” in Proceedings of
    the 26th Asia and South Pacific Design Automation Conference, pp. 344–350, 2021.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] S. Kundu、M. Nazemi、P. A. Beerel 和 M. Pedram，“Dnr：通过动态网络重连的可调鲁棒剪枝框架”，发表于第26届亚太设计自动化会议，pp.
    344–350，2021年。'
- en: '[96] C. Jin and M. Rinard, “Manifold regularization for locally stable deep
    neural networks,” arXiv preprint arXiv:2003.04286, 2020.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] C. Jin 和 M. Rinard，“局部稳定深度神经网络的流形正则化”，arXiv 预印本 arXiv:2003.04286，2020年。'
- en: '[97] J. Cui, S. Liu, L. Wang, and J. Jia, “Learnable boundary guided adversarial
    training,” in Proceedings of the IEEE/CVF ICCV, pp. 15721–15730, 2021.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. Cui、S. Liu、L. Wang 和 J. Jia，“可学习边界引导对抗训练”，发表于 IEEE/CVF ICCV，pp. 15721–15730，2021年。'
- en: '[98] E.-C. Chen and C.-R. Lee, “Ltd: Low temperature distillation for robust
    adversarial training,” arXiv preprint arXiv:2111.02331, 2021.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] E.-C. Chen 和 C.-R. Lee，“Ltd：用于鲁棒对抗训练的低温蒸馏”，arXiv 预印本 arXiv:2111.02331，2021年。'
- en: '[99] H. Yan, J. Du, V. Y. Tan, and J. Feng, “On robustness of neural ordinary
    differential equations,” arXiv preprint arXiv:1910.05513, 2019.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] H. Yan, J. Du, V. Y. Tan, 和 J. Feng，“神经常微分方程的鲁棒性”，arXiv 预印本 arXiv:1910.05513，2019。'
- en: '[100] E. Haber and L. Ruthotto, “Stable architectures for deep neural networks,”
    Inverse problems, vol. 34, no. 1, p. 014004, 2017.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] E. Haber 和 L. Ruthotto，“深度神经网络的稳定架构”，《逆问题》，第34卷，第1期，第014004页，2017。'
- en: '[101] X. Liu, T. Xiao, S. Si, Q. Cao, S. Kumar, and C.-J. Hsieh, “How does
    noise help robustness? explanation and exploration under the neural sde framework,”
    in Proceedings of the IEEE/CVF Conference on CVPR, pp. 282–290, 2020.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] X. Liu, T. Xiao, S. Si, Q. Cao, S. Kumar, 和 C.-J. Hsieh，“噪声如何帮助鲁棒性？在神经SDE框架下的解释与探索”，在IEEE/CVF
    CVPR会议论文集，pp. 282–290，2020。'
- en: '[102] Q. Kang, Y. Song, Q. Ding, and W. P. Tay, “Stable neural ode with lyapunov-stable
    equilibrium points for defending against adversarial attacks,” Advances in Neural
    Information Processing Systems, vol. 34, pp. 14925–14937, 2021.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Q. Kang, Y. Song, Q. Ding, 和 W. P. Tay，“具有利雅普诺夫稳定平衡点的稳定神经常微分方程，用于防御对抗性攻击”，《神经信息处理系统进展》，第34卷，第14925–14937页，2021。'
- en: '[103] T. Pang, M. Lin, X. Yang, J. Zhu, and S. Yan, “Robustness and accuracy
    could be reconcilable by (proper) definition,” in International Conference on
    Machine Learning, pp. 17258–17277, PMLR, 2022.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] T. Pang, M. Lin, X. Yang, J. Zhu, 和 S. Yan，“鲁棒性和准确性可以通过（适当的）定义调和”，在国际机器学习会议，pp. 17258–17277，PMLR，2022。'
- en: '[104] S. Dai, S. Mahloujifar, and P. Mittal, “Parameterizing activation functions
    for adversarial robustness,” in 2022 IEEE SPW, pp. 80–87, IEEE, 2022.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] S. Dai, S. Mahloujifar, 和 P. Mittal，“为对抗鲁棒性参数化激活函数”，在2022 IEEE SPW，pp. 80–87，IEEE，2022。'
- en: '[105] D. Meng and H. Chen, “Magnet: A two-pronged defense against adversarial
    examples,” Proceedings of the 2017 ACM SIGSAC Conference on Computer and Communications
    Security, 2017.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] D. Meng 和 H. Chen，“Magnet：对抗性样本的双重防御”，2017年ACM SIGSAC计算机与通信安全会议论文集，2017。'
- en: '[106] J. H. Metzen, T. Genewein, V. Fischer, and B. Bischoff, “On detecting
    adversarial perturbations,” 2017.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] J. H. Metzen, T. Genewein, V. Fischer, 和 B. Bischoff，“对抗扰动检测”，2017。'
- en: '[107] W. Xu, D. Evans, and Y. Qi, “Feature squeezing: Detecting adversarial
    examples in deep neural networks,” arXiv preprint arXiv:1704.01155, 2017.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] W. Xu, D. Evans, 和 Y. Qi，“特征压缩：在深度神经网络中检测对抗性样本”，arXiv 预印本 arXiv:1704.01155，2017。'
- en: '[108] P. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol, “Extracting
    and composing robust features with denoising autoencoders,” in ICML ’08, 2008.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] P. Vincent, H. Larochelle, Y. Bengio, 和 P.-A. Manzagol，“用去噪自编码器提取和组合鲁棒特征”，在ICML
    ’08，2008。'
- en: '[109] M. Arjovsky, S. Chintala, and L. Bottou, “Wasserstein gan,” ArXiv, vol. abs/1701.07875,
    2017.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M. Arjovsky, S. Chintala, 和 L. Bottou，“Wasserstein GAN”，ArXiv，vol. abs/1701.07875，2017。'
- en: '[110] C. Mao, M. Chiquier, H. Wang, J. Yang, and C. Vondrick, “Adversarial
    attacks are reversible with natural supervision,” in Proceedings of the IEEE/CVF
    ICCV, pp. 661–671, 2021.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] C. Mao, M. Chiquier, H. Wang, J. Yang, 和 C. Vondrick，“对抗攻击在自然监督下是可逆的”，在IEEE/CVF
    ICCV会议论文集，pp. 661–671，2021。'
- en: '[111] Y. Li, M. R. Min, T. Lee, W. Yu, E. Kruus, W. Wang, and C.-J. Hsieh,
    “Towards robustness of deep neural networks via regularization,” in Proceedings
    of the IEEE/CVF ICCV, pp. 7496–7505, October 2021.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Y. Li, M. R. Min, T. Lee, W. Yu, E. Kruus, W. Wang, 和 C.-J. Hsieh，“通过正则化提升深度神经网络的鲁棒性”，在IEEE/CVF
    ICCV会议论文集，pp. 7496–7505，2021年10月。'
- en: '[112] D. Zhou, N. Wang, C. Peng, X. Gao, X. Wang, J. Yu, and T. Liu, “Removing
    adversarial noise in class activation feature space,” in Proceedings of the IEEE/CVF
    ICCV, pp. 7878–7887, 2021.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] D. Zhou, N. Wang, C. Peng, X. Gao, X. Wang, J. Yu, 和 T. Liu，“去除类别激活特征空间中的对抗噪声”，在IEEE/CVF
    ICCV会议论文集，pp. 7878–7887，2021。'
- en: '[113] A. Abusnaina, Y. Wu, S. Arora, Y. Wang, F. Wang, H. Yang, and D. Mohaisen,
    “Adversarial example detection using latent neighborhood graph,” in Proceedings
    of the IEEE/CVF ICCV, pp. 7687–7696, October 2021.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] A. Abusnaina, Y. Wu, S. Arora, Y. Wang, F. Wang, H. Yang, 和 D. Mohaisen，“利用潜在邻域图进行对抗样本检测”，在IEEE/CVF
    ICCV会议论文集，pp. 7687–7696，2021年10月。'
- en: '[114] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini,
    “The graph neural network model,” IEEE Transactions on Neural Networks, vol. 20,
    no. 1, pp. 61–80, 2009.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, 和 G. Monfardini，“图神经网络模型”，《IEEE神经网络汇刊》，第20卷，第1期，第61–80页，2009。'
- en: '[115] Y. Chen, S. Liu, and X. Wang, “Learning continuous image representation
    with local implicit image function,” in Proceedings of the IEEE/CVF conference
    on CVPR, pp. 8628–8638, 2021.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Y. Chen, S. Liu, 和 X. Wang，“用局部隐式图像函数学习连续图像表示”，在IEEE/CVF CVPR会议论文集，pp. 8628–8638，2021。'
- en: '[116] C.-H. Ho and N. Vasconcelos, “Disco: Adversarial defense with local implicit
    functions,” in Advances in Neural Information Processing Systems (S. Koyejo, S. Mohamed,
    A. Agarwal, D. Belgrave, K. Cho, and A. Oh, eds.), vol. 35, pp. 23818–23837, Curran
    Associates, Inc., 2022.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] C.-H. Ho 和 N. Vasconcelos，“Disco: 通过局部隐式函数进行对抗性防御”，在《神经信息处理系统进展》（S. Koyejo,
    S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, 和 A. Oh 主编），第35卷，第23818–23837页，Curran
    Associates, Inc., 2022年。'
- en: '[117] C. Xie, Y. Wu, L. van der Maaten, A. L. Yuille, and K. He, “Feature denoising
    for improving adversarial robustness,” 2019 IEEE/CVF Conference on CVPR, pp. 501–509,
    2019.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] C. Xie, Y. Wu, L. van der Maaten, A. L. Yuille, 和 K. He，“通过特征去噪提高对抗性鲁棒性”，2019
    IEEE/CVF CVPR会议, 第501–509页, 2019年。'
- en: '[118] M. Guo, Y. Yang, R. Xu, and Z. Liu, “When nas meets robustness: In search
    of robust architectures against adversarial attacks,” 2020 IEEE/CVF Conference
    on CVPR, pp. 628–637, 2020.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] M. Guo, Y. Yang, R. Xu, 和 Z. Liu，“当NAS遇上鲁棒性：寻找抵御对抗攻击的鲁棒架构”，2020 IEEE/CVF
    CVPR会议, 第628–637页, 2020年。'
- en: '[119] C. Xie, J. Wang, Z. Zhang, Z. Ren, and A. Yuille, “Mitigating adversarial
    effects through randomization,” in International Conference on Learning Representations,
    2018.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] C. Xie, J. Wang, Z. Zhang, Z. Ren, 和 A. Yuille，“通过随机化减轻对抗性影响”，在国际学习表征会议,
    2018年。'
- en: '[120] M. Atzmon, N. Haim, L. Yariv, O. Israelov, H. Maron, and Y. Lipman, “Controlling
    neural level sets,” Advances in Neural Information Processing Systems, vol. 32,
    2019.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] M. Atzmon, N. Haim, L. Yariv, O. Israelov, H. Maron, 和 Y. Lipman，“控制神经网络水平集”，《神经信息处理系统进展》，第32卷,
    2019年。'
- en: '[121] B. Sun, N.-h. Tsai, F. Liu, R. Yu, and H. Su, “Adversarial defense by
    stratified convolutional sparse coding,” in Proceedings of the IEEE/CVF Conference
    on CVPR, pp. 11447–11456, 2019.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] B. Sun, N.-h. Tsai, F. Liu, R. Yu, 和 H. Su，“通过分层卷积稀疏编码进行对抗性防御”，在IEEE/CVF
    CVPR会议论文集, 第11447–11456页, 2019年。'
- en: '[122] P. Benz, C. Zhang, and I. S. Kweon, “Batch normalization increases adversarial
    vulnerability: Disentangling usefulness and robustness of model features,” ArXiv,
    vol. abs/2010.03316, 2020.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] P. Benz, C. Zhang, 和 I. S. Kweon，“批量归一化增加了对抗性脆弱性：解开模型特征的有用性与鲁棒性”，ArXiv,
    第abs/2010.03316卷, 2020年。'
- en: '[123] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” in International conference on
    machine learning, pp. 448–456, PMLR, 2015.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] S. Ioffe 和 C. Szegedy，“批量归一化：通过减少内部协变量偏移加速深度网络训练”，在国际机器学习会议, 第448–456页,
    PMLR, 2015年。'
- en: '[124] W. F. Good, G. S. Maitz, and D. Gur, “Joint photographic experts group
    (jpeg) compatible data compression of mammograms,” Journal of Digital Imaging,
    vol. 7, no. 3, pp. 123–132, 1994.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] W. F. Good, G. S. Maitz, 和 D. Gur，“兼容JPEG的数据压缩乳腺X光图像”，《数字成像杂志》，第7卷，第3期，第123–132页,
    1994年。'
- en: '[125] G. K. Dziugaite, Z. Ghahramani, and D. M. Roy, “A study of the effect
    of jpg compression on adversarial images,” arXiv preprint arXiv:1608.00853, 2016.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] G. K. Dziugaite, Z. Ghahramani, 和 D. M. Roy，“JPEG压缩对对抗性图像的影响研究”，arXiv预印本
    arXiv:1608.00853, 2016年。'
- en: '[126] X. Huang, M. Kwiatkowska, S. Wang, and M. Wu, “Safety verification of
    deep neural networks,” in CAV, 2017.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] X. Huang, M. Kwiatkowska, S. Wang, 和 M. Wu，“深度神经网络的安全性验证”，在CAV会议, 2017年。'
- en: '[127] K. Pei, Y. Cao, J. Yang, and S. S. Jana, “Deepxplore: Automated whitebox
    testing of deep learning systems,” Proceedings of the 26th Symposium on Operating
    Systems Principles, 2017.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] K. Pei, Y. Cao, J. Yang, 和 S. S. Jana，“Deepxplore: 自动化白盒测试深度学习系统”，第26届操作系统原理研讨会论文集,
    2017年。'
- en: '[128] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,
    L. Li, Y. Liu, J. Zhao, and Y. Wang, “Deepgauge: Multi-granularity testing criteria
    for deep learning systems,” 2018 33rd IEEE/ACM International Conference on ASE,
    pp. 120–131, 2018.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] L. Ma, F. Juefei-Xu, F. Zhang, J. Sun, M. Xue, B. Li, C. Chen, T. Su,
    L. Li, Y. Liu, J. Zhao, 和 Y. Wang，“Deepgauge: 多粒度测试标准用于深度学习系统”，2018年第33届IEEE/ACM国际ASE会议,
    第120–131页, 2018年。'
- en: '[129] J. Kim, R. Feldt, and S. Yoo, “Guiding deep learning system testing using
    surprise adequacy,” 2019 IEEE/ACM 41st ICSE, pp. 1039–1049, 2019.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] J. Kim, R. Feldt, 和 S. Yoo，"使用惊讶度充分性指导深度学习系统测试"，2019 IEEE/ACM 第41届 ICSE,
    第1039–1049页, 2019年。'
- en: '[130] T. DeVries and G. W. Taylor, “Improved regularization of convolutional
    neural networks with cutout,” arXiv preprint arXiv:1708.04552, 2017.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] T. DeVries 和 G. W. Taylor，“通过cutout改进卷积神经网络的正则化”，arXiv预印本 arXiv:1708.04552,
    2017年。'
- en: '[131] H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical
    risk minimization,” in International Conference on Learning Representations, 2018.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] H. Zhang, M. Cisse, Y. N. Dauphin, 和 D. Lopez-Paz，“mixup: 超越经验风险最小化”，在国际学习表征会议,
    2018年。'
- en: '[132] L. Rice, E. Wong, and Z. Kolter, “Overfitting in adversarially robust
    deep learning,” in International Conference on Machine Learning, pp. 8093–8104,
    PMLR, 2020.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] L. Rice, E. Wong, 和 Z. Kolter, “对抗鲁棒深度学习中的过拟合，” 机器学习国际会议论文集，第 8093–8104
    页, PMLR, 2020。'
- en: '[133] T. Pang, X. Yang, Y. Dong, H. Su, and J. Zhu, “Bag of tricks for adversarial
    training,” arXiv preprint arXiv:2010.00467, 2020.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] T. Pang, X. Yang, Y. Dong, H. Su, 和 J. Zhu, “对抗训练的技巧袋，” arXiv 预印本 arXiv:2010.00467,
    2020。'
- en: '[134] S.-A. Rebuffi, S. Gowal, D. A. Calian, F. Stimberg, O. Wiles, and T. Mann,
    “Fixing data augmentation to improve adversarial robustness,” arXiv preprint arXiv:2103.01946,
    2021.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] S.-A. Rebuffi, S. Gowal, D. A. Calian, F. Stimberg, O. Wiles, 和 T. Mann,
    “修正数据增强以提高对抗鲁棒性，” arXiv 预印本 arXiv:2103.01946, 2021。'
- en: '[135] S. Gowal, C. Qin, J. Uesato, T. Mann, and P. Kohli, “Uncovering the limits
    of adversarial training against norm-bounded adversarial examples,” arXiv preprint
    arXiv:2010.03593, 2020.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] S. Gowal, C. Qin, J. Uesato, T. Mann, 和 P. Kohli, “揭示对抗训练在范数约束对抗样本中的极限，”
    arXiv 预印本 arXiv:2010.03593, 2020。'
- en: '[136] S. Elfwing, E. Uchibe, and K. Doya, “Sigmoid-weighted linear units for
    neural network function approximation in reinforcement learning,” Neural Networks,
    vol. 107, pp. 3–11, 2018.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] S. Elfwing, E. Uchibe, 和 K. Doya, “用于强化学习中神经网络函数逼近的Sigmoid加权线性单元，” 神经网络，卷
    107，第 3–11 页, 2018。'
- en: '[137] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    Advances in Neural Information Processing Systems, vol. 33, pp. 6840–6851, 2020.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] J. Ho, A. Jain, 和 P. Abbeel, “去噪扩散概率模型，” 神经信息处理系统进展，卷 33，第 6840–6851
    页, 2020。'
- en: '[138] S. Gowal, S.-A. Rebuffi, O. Wiles, F. Stimberg, D. A. Calian, and T. A.
    Mann, “Improving robustness using generated data,” Advances in Neural Information
    Processing Systems, vol. 34, pp. 4218–4233, 2021.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] S. Gowal, S.-A. Rebuffi, O. Wiles, F. Stimberg, D. A. Calian, 和 T. A.
    Mann, “使用生成数据提升鲁棒性，” 神经信息处理系统进展，卷 34，第 4218–4233 页, 2021。'
- en: '[139] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, and S. Ganguli, “Deep
    unsupervised learning using nonequilibrium thermodynamics,” in International Conference
    on Machine Learning, pp. 2256–2265, PMLR, 2015.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] J. Sohl-Dickstein, E. Weiss, N. Maheswaranathan, 和 S. Ganguli, “使用非平衡热力学的深度无监督学习，”
    机器学习国际会议论文集，第 2256–2265 页, PMLR, 2015。'
- en: '[140] V. Sehwag, S. Mahloujifar, T. Handina, S. Dai, C. Xiang, M. Chiang, and
    P. Mittal, “Robust learning meets generative models: Can proxy distributions improve
    adversarial robustness?,” in International Conference on Learning Representations,
    2022.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] V. Sehwag, S. Mahloujifar, T. Handina, S. Dai, C. Xiang, M. Chiang, 和
    P. Mittal, “鲁棒学习与生成模型相遇：代理分布能否提升对抗鲁棒性？”，在《学习表征国际会议论文集》，2022。'
- en: '[141] C. Shi, C. Holtz, and G. Mishne, “Online adversarial purification based
    on self-supervised learning,” in International Conference on Learning Representations,
    2021.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] C. Shi, C. Holtz, 和 G. Mishne, “基于自监督学习的在线对抗净化，” 学习表征国际会议论文集, 2021。'
- en: '[142] J. Yoon, S. J. Hwang, and J. Lee, “Adversarial purification with score-based
    generative models,” in International Conference on Machine Learning, pp. 12062–12072,
    PMLR, 2021.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] J. Yoon, S. J. Hwang, 和 J. Lee, “基于评分生成模型的对抗净化，” 机器学习国际会议论文集，第 12062–12072
    页, PMLR, 2021。'
- en: '[143] W. Nie, B. Guo, Y. Huang, C. Xiao, A. Vahdat, and A. Anandkumar, “Diffusion
    models for adversarial purification,” in International Conference on Machine Learning,
    pp. 16805–16827, PMLR, 2022.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] W. Nie, B. Guo, Y. Huang, C. Xiao, A. Vahdat, 和 A. Anandkumar, “用于对抗净化的扩散模型，”
    机器学习国际会议论文集，第 16805–16827 页, PMLR, 2022。'
- en: '[144] Q. Wu, H. Ye, and Y. Gu, “Guided diffusion model for adversarial purification
    from random noise,” arXiv e-prints, pp. arXiv–2206, 2022.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Q. Wu, H. Ye, 和 Y. Gu, “针对随机噪声的引导扩散模型进行对抗净化，” arXiv 电子预印本，第 arXiv–2206
    页, 2022。'
- en: '[145] C. Xiao, Z. Chen, K. Jin, J. Wang, W. Nie, M. Liu, A. Anandkumar, B. Li,
    and D. Song, “Densepure: Understanding diffusion models towards adversarial robustness,”
    arXiv preprint arXiv:2211.00322, 2022.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] C. Xiao, Z. Chen, K. Jin, J. Wang, W. Nie, M. Liu, A. Anandkumar, B.
    Li, 和 D. Song, “Densepure: 理解扩散模型对抗鲁棒性，” arXiv 预印本 arXiv:2211.00322, 2022。'
- en: '[146] Z. Wang, T. Pang, C. Du, M. Lin, W. Liu, and S. Yan, “Better diffusion
    models further improve adversarial training,” arXiv preprint arXiv:2302.04638,
    2023.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Z. Wang, T. Pang, C. Du, M. Lin, W. Liu, 和 S. Yan, “更好的扩散模型进一步提升对抗训练，”
    arXiv 预印本 arXiv:2302.04638, 2023。'
- en: '[147] T. Karras, M. Aittala, T. Aila, and S. Laine, “Elucidating the design
    space of diffusion-based generative models,” in Advances in Neural Information
    Processing Systems (A. H. Oh, A. Agarwal, D. Belgrave, and K. Cho, eds.), 2022.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] T. Karras, M. Aittala, T. Aila, 和 S. Laine, “阐明基于扩散的生成模型的设计空间，” 在《神经信息处理系统进展》（A.
    H. Oh, A. Agarwal, D. Belgrave, 和 K. Cho 编辑），2022。'
- en: '[148] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in Advances in Neural Information Processing
    Systems (F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger, eds.),
    vol. 25, Curran Associates, Inc., 2012.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton， “使用深度卷积神经网络的ImageNet分类”，收录于《神经信息处理系统进展》（F.
    Pereira, C. J. C. Burges, L. Bottou, 和 K. Q. Weinberger编），卷25，Curran Associates,
    Inc.，2012年。'
- en: '[149] M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. S. Khan, and M. Yang,
    “Intriguing properties of vision transformers,” ArXiv, vol. abs/2105.10497, 2021.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] M. Naseer, K. Ranasinghe, S. H. Khan, M. Hayat, F. S. Khan, 和 M. Yang，
    “视觉变换器的有趣特性”，ArXiv，卷abs/2105.10497，2021年。'
- en: '[150] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” ArXiv, vol. abs/1512.03385, 2015.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] K. He, X. Zhang, S. Ren, 和 J. Sun， “用于图像识别的深度残差学习”，ArXiv，卷abs/1512.03385，2015年。'
- en: '[151] K. Mahmood, R. Mahmood, and M. van Dijk, “On the robustness of vision
    transformers to adversarial examples,” in Proceedings of the IEEE/CVF ICCV, pp. 7838–7847,
    October 2021.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] K. Mahmood, R. Mahmood, 和 M. van Dijk， “视觉变换器对对抗样本的鲁棒性”，收录于《IEEE/CVF
    ICCV会议论文集》，第7838–7847页，2021年10月。'
- en: '[152] A. Aldahdooh, W. Hamidouche, and O. Déforges, “Reveal of vision transformers
    robustness against adversarial attacks,” ArXiv, vol. abs/2106.03734, 2021.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] A. Aldahdooh, W. Hamidouche, 和 O. Déforges， “揭示视觉变换器对抗攻击的鲁棒性”，ArXiv，卷abs/2106.03734，2021年。'
- en: '[153] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and D. Batra,
    “Grad-cam: Visual explanations from deep networks via gradient-based localization,”
    in Proceedings of the IEEE ICCV, Oct 2017.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, 和 D. Batra，
    “Grad-cam: 通过基于梯度的定位从深度网络中获得视觉解释”，收录于《IEEE ICCV会议录》，2017年10月。'
- en: '[154] Z. Wei, J. Chen, M. Goldblum, Z. Wu, T. Goldstein, and Y. Jiang, “Towards
    transferable adversarial attacks on vision transformers,” ArXiv, vol. abs/2109.04176,
    2021.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Z. Wei, J. Chen, M. Goldblum, Z. Wu, T. Goldstein, 和 Y. Jiang， “面向视觉变换器的可转移对抗攻击”，ArXiv，卷abs/2109.04176，2021年。'
- en: '[155] H. Salman, S. Jain, E. Wong, and A. Madry, “Certified patch robustness
    via smoothed vision transformers,” ArXiv, vol. abs/2110.07719, 2021.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] H. Salman, S. Jain, E. Wong, 和 A. Madry， “通过平滑视觉变换器的认证补丁鲁棒性”，ArXiv，卷abs/2110.07719，2021年。'
- en: '[156] Y. Bai, J. Mei, A. L. Yuille, and C. Xie, “Are transformers more robust
    than cnns?,” Advances in Neural Information Processing Systems, vol. 34, pp. 26831–26843,
    2021.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Y. Bai, J. Mei, A. L. Yuille, 和 C. Xie， “变换器是否比CNN更鲁棒？”，《神经信息处理系统进展》，卷34，第26831–26843页，2021年。'
- en: '[157] Y. Wang, J. Wang, Z. Yin, R. Gong, J. Wang, A. Liu, and X. Liu, “Generating
    transferable adversarial examples against vision transformers,” in Proceedings
    of the 30th ACM International Conference on Multimedia, pp. 5181–5190, 2022.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Y. Wang, J. Wang, Z. Yin, R. Gong, J. Wang, A. Liu, 和 X. Liu， “生成可转移的对抗样本以对抗视觉变换器”，收录于第30届ACM国际多媒体会议论文集，第5181–5190页，2022年。'
- en: '[158] Y. Fu, S. Zhang, S. Wu, C. Wan, and Y. Lin, “Patch-fool: Are vision transformers
    always robust against adversarial perturbations?,” in International Conference
    on Learning Representations, 2022.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Y. Fu, S. Zhang, S. Wu, C. Wan, 和 Y. Lin， “Patch-fool: 视觉变换器是否始终对对抗扰动鲁棒？”，收录于《国际学习表征会议》，2022年。'
- en: '[159] J. Gu, V. Tresp, and Y. Qin, “Are vision transformers robust to patch
    perturbations?,” in Computer Vision – ECCV 2022, pp. 404–421, Springer, 2022.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] J. Gu, V. Tresp, 和 Y. Qin， “视觉变换器对补丁扰动的鲁棒性如何？”，收录于《计算机视觉 – ECCV 2022》，第404–421页，Springer，2022年。'
- en: '[160] Z. Chen, B. Li, J. Xu, S. Wu, S. Ding, and W. Zhang, “Towards practical
    certifiable patch defense with vision transformer,” in Proceedings of the IEEE/CVF
    Conference on CVPR, pp. 15148–15158, 2022.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Z. Chen, B. Li, J. Xu, S. Wu, S. Ding, 和 W. Zhang， “面向实际认证补丁防御的视觉变换器”，收录于《IEEE/CVF
    CVPR会议论文集》，第15148–15158页，2022年。'
- en: '[161] Y. Li, S. Ruan, H. Qin, S. Deng, and M. A. El-Yacoubi, “Transformer based
    defense gan against palm-vein adversarial attacks,” IEEE Transactions on Information
    Forensics and Security, vol. 18, pp. 1509–1523, 2023.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Y. Li, S. Ruan, H. Qin, S. Deng, 和 M. A. El-Yacoubi， “基于变换器的防御GAN对抗掌静脉对抗攻击”，《IEEE信息取证与安全学报》，卷18，第1509–1523页，2023年。'
- en: '[162] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” Proceedings of the IEEE, vol. 86, no. 11, pp. 2278–2324,
    1998.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Y. LeCun, L. Bottou, Y. Bengio, 和 P. Haffner， “基于梯度的学习应用于文档识别”，《IEEE会议录》，卷86，第11期，第2278–2324页，1998年。'
- en: '[163] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image dataset
    for benchmarking machine learning algorithms,” ArXiv, vol. abs/1708.07747, 2017.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] H. Xiao, K. Rasul, 和 R. Vollgraf， “Fashion-mnist: 用于基准测试机器学习算法的新型图像数据集”，ArXiv，卷abs/1708.07747，2017年。'
- en: '[164] A. Krizhevsky, G. Hinton, et al., “Learning multiple layers of features
    from tiny images,” 2009.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] A. Krizhevsky, G. Hinton 等， “从微小图像中学习多层特征，” 2009年。'
- en: '[165] A. Torralba, R. Fergus, and W. T. Freeman, “80 million tiny images: A
    large data set for nonparametric object and scene recognition,” IEEE Transactions
    on Pattern Analysis and Machine Intelligence, vol. 30, no. 11, pp. 1958–1970,
    2008.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] A. Torralba, R. Fergus 和 W. T. Freeman， “8000万微小图像：用于非参数对象和场景识别的大型数据集，”
    《IEEE模式分析与机器智能学报》，第30卷，第11期，pp. 1958–1970，2008年。'
- en: '[166] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, “Reading
    digits in natural images with unsupervised feature learning,” in NIPS Workshop
    on Deep Learning and Unsupervised Feature Learning 2011, 2011.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu 和 A. Y. Ng， “在自然图像中使用无监督特征学习读取数字，”
    发表在NIPS深度学习和无监督特征学习研讨会，2011年。'
- en: '[167] J. Stallkamp, M. Schlipsing, J. Salmen, and C. Igel, “Man vs. computer:
    Benchmarking machine learning algorithms for traffic sign recognition,” Neural
    networks, vol. 32, pp. 323–332, 2012.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. Stallkamp, M. Schlipsing, J. Salmen 和 C. Igel， “人类与计算机：交通标志识别机器学习算法的基准测试，”
    《神经网络》，第32卷，pp. 323–332，2012年。'
- en: '[168] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, et al., “Imagenet large scale visual recognition
    challenge,” IJCV, vol. 115, no. 3, pp. 211–252, 2015.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein 等， “ImageNet大规模视觉识别挑战，” 《国际计算机视觉期刊》，第115卷，第3期，pp.
    211–252，2015年。'
- en: '[169] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt, and D. Song, “Natural
    adversarial examples,” in Proceedings of the IEEE/CVF Conference on CVPR, pp. 15262–15271,
    June 2021.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] D. Hendrycks, K. Zhao, S. Basart, J. Steinhardt 和 D. Song， “自然对抗样本，”
    发表在IEEE/CVF计算机视觉与模式识别会议论文集中，pp. 15262–15271，2021年6月。'
- en: '[170] D. Hendrycks and T. Dietterich, “Benchmarking neural network robustness
    to common corruptions and perturbations,” in International Conference on Learning
    Representations, 2019.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] D. Hendrycks 和 T. Dietterich， “基准测试神经网络对常见损坏和扰动的鲁棒性，” 发表在国际表示学习会议，2019年。'
- en: '[171] K. De and M. Pedersen, “Impact of colour on robustness of deep neural
    networks,” in 2021 IEEE/CVF International Conference on Computer Vision Workshops
    (ICCVW), pp. 21–30, 2021.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] K. De 和 M. Pedersen， “颜色对深度神经网络鲁棒性的影响，” 发表在2021年IEEE/CVF国际计算机视觉大会研讨会（ICCVW），pp.
    21–30，2021年。'
- en: '[172] Y. Le and X. Yang, “Tiny imagenet visual recognition challenge,” CS 231N,
    vol. 7, no. 7, p. 3, 2015.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Y. Le 和 X. Yang， “Tiny ImageNet视觉识别挑战，” CS 231N，第7卷，第7期，第3页，2015年。'
- en: '[173] Z. Huan, Y. Wang, X. Zhang, L. Shang, C. Fu, and J. Zhou, “Data-free
    adversarial perturbations for practical black-box attack,” in Pacific-Asia conference
    on knowledge discovery and data mining, pp. 127–138, Springer, 2020.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Z. Huan, Y. Wang, X. Zhang, L. Shang, C. Fu 和 J. Zhou， “无数据对抗扰动用于实际黑箱攻击，”
    发表在太平洋-亚太知识发现与数据挖掘大会，pp. 127–138，Springer，2020年。'
- en: '[174] D. Hendrycks, K. Lee, and M. Mazeika, “Using pre-training can improve
    model robustness and uncertainty,” in International Conference on Machine Learning,
    pp. 2712–2721, PMLR, 2019.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] D. Hendrycks, K. Lee 和 M. Mazeika， “使用预训练可以提高模型的鲁棒性和不确定性，” 发表在国际机器学习大会，pp.
    2712–2721，PMLR，2019年。'
- en: '[175] S. Zagoruyko and N. Komodakis, “Wide residual networks,” in British Machine
    Vision Conference 2016, British Machine Vision Association, 2016.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] S. Zagoruyko 和 N. Komodakis， “宽残差网络，” 发表在2016年英国机器视觉大会，英国机器视觉协会，2016年。'
- en: '[176] F. Croce, M. Andriushchenko, V. Sehwag, E. Debenedetti, N. Flammarion,
    M. Chiang, P. Mittal, and M. Hein, “Robustbench: a standardized adversarial robustness
    benchmark,” in Thirty-fifth Conference on Neural Information Processing Systems
    Datasets and Benchmarks Track (Round 2), 2021.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] F. Croce, M. Andriushchenko, V. Sehwag, E. Debenedetti, N. Flammarion,
    M. Chiang, P. Mittal 和 M. Hein， “Robustbench：一个标准化的对抗鲁棒性基准测试，” 发表在第三十五届神经信息处理系统会议数据集与基准测试轨道（第二轮），2021年。'
- en: '[177] H. Salman, A. Ilyas, L. Engstrom, A. Kapoor, and A. Madry, “Do adversarially
    robust imagenet models transfer better?,” Advances in Neural Information Processing
    Systems, vol. 33, pp. 3533–3545, 2020.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] H. Salman, A. Ilyas, L. Engstrom, A. Kapoor 和 A. Madry， “对抗鲁棒的ImageNet模型转移是否更好？”，
    《神经信息处理系统进展》，第33卷，pp. 3533–3545，2020年。'
