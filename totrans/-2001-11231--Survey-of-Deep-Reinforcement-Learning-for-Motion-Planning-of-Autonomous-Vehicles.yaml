- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:02:47'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:02:47
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2001.11231] Survey of Deep Reinforcement Learning for Motion Planning of Autonomous
    Vehicles'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2001.11231] 自动驾驶汽车运动规划的深度强化学习调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2001.11231](https://ar5iv.labs.arxiv.org/html/2001.11231)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2001.11231](https://ar5iv.labs.arxiv.org/html/2001.11231)
- en: Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动驾驶汽车运动规划的深度强化学习调查
- en: Szilárd Aradi¹ ¹ Department of Control for Transportation and Vehicle Systems,
    Budapest University of Technology and Economics, Hungary
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Szilárd Aradi¹ ¹ 布达佩斯科技大学交通与车辆系统控制系，匈牙利
- en: 'e-mail: aradi.szilard@mail.bme.hu'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：aradi.szilard@mail.bme.hu
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Academic research in the field of autonomous vehicles has reached high popularity
    in recent years related to several topics as sensor technologies, V2X communications,
    safety, security, decision making, control, and even legal and standardization
    rules. Besides classic control design approaches, Artificial Intelligence and
    Machine Learning methods are present in almost all of these fields. Another part
    of research focuses on different layers of Motion Planning, such as strategic
    decisions, trajectory planning, and control. A wide range of techniques in Machine
    Learning itself have been developed, and this article describes one of these fields,
    Deep Reinforcement Learning (DRL). The paper provides insight into the hierarchical
    motion planning problem and describes the basics of DRL. The main elements of
    designing such a system are the modeling of the environment, the modeling abstractions,
    the description of the state and the perception models, the appropriate rewarding,
    and the realization of the underlying neural network. The paper describes vehicle
    models, simulation possibilities and computational requirements. Strategic decisions
    on different layers and the observation models, e.g., continuous and discrete
    state representations, grid-based, and camera-based solutions are presented. The
    paper surveys the state-of-art solutions systematized by the different tasks and
    levels of autonomous driving, such as car-following, lane-keeping, trajectory
    following, merging, or driving in dense traffic. Finally, open questions and future
    challenges are discussed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，自动驾驶汽车领域的学术研究受到了高度关注，涉及多个主题，如传感器技术、V2X通信、安全、保障、决策、控制，甚至法律和标准化规则。除了经典的控制设计方法，人工智能和机器学习方法几乎在所有这些领域中都有应用。另一部分研究集中于运动规划的不同层次，如战略决策、轨迹规划和控制。机器学习本身已经发展出多种技术，本文描述了其中一个领域——深度强化学习（DRL）。论文提供了对分层运动规划问题的见解，并描述了DRL的基础知识。设计此类系统的主要元素包括环境建模、建模抽象、状态描述和感知模型、适当的奖励以及底层神经网络的实现。论文描述了车辆模型、仿真可能性和计算需求。不同层次的战略决策和观察模型，例如连续和离散状态表示、基于网格和基于摄像头的解决方案被介绍。论文系统化地调查了不同任务和自动驾驶层次的最新解决方案，如车距保持、车道保持、轨迹跟踪、合流或在密集交通中行驶。最后，讨论了开放问题和未来挑战。
- en: 'Index Terms:'
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Machine Learning, Motion Planning, Autonomous Vehicles, Artificial intelligence,
    Reinforcement Learning.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习、运动规划、自动驾驶汽车、人工智能、强化学习。
- en: I Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: Motion planning for autonomous vehicle functions is a vast and long-researched
    area using a wide variety of approaches such as different optimization techniques,
    modern control methods, artificial intelligence, and machine learning. This article
    presents the achievements of the field from recent years focused on Deep Reinforcement
    Learning (DRL) approach. DRL combines the classic reinforcement learning with
    deep neural networks, and gained popularity after the breakthrough article from
    Deepmind [[1](#bib.bib1), [2](#bib.bib2)]. In the number of research papers about
    autonomous vehicles and the DRL has been increased in the last few years (see
    Fig. [1](#S1.F1 "Figure 1 ‣ I Introduction ‣ Survey of Deep Reinforcement Learning
    for Motion Planning of Autonomous Vehicles").), and because of the complexity
    of the different motion planning problems, it is a convenient choice to evaluate
    the applicability of DRL for these problems.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶车辆功能的运动规划是一个广泛且经过长期研究的领域，使用了多种方法，如不同的优化技术、现代控制方法、人工智能和机器学习。本文介绍了近年来在深度强化学习（DRL）方法方面的领域成就。DRL将经典的强化学习与深度神经网络相结合，并在Deepmind的突破性文章[[1](#bib.bib1),
    [2](#bib.bib2)]之后获得了广泛关注。在过去几年中关于自动驾驶车辆和DRL的研究论文数量增加（见图[1](#S1.F1 "图 1 ‣ I 引言
    ‣ 自动驾驶车辆运动规划的深度强化学习调查")），由于不同运动规划问题的复杂性，评估DRL在这些问题上的适用性是一个便捷的选择。
- en: '![Refer to caption](img/8964a6b4d55cc056b3df2738f7601436.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/8964a6b4d55cc056b3df2738f7601436.png)'
- en: 'Figure 1: Web of Science topic search for ”Deep Reinforcement Learning” and
    ”Autonomous Vehicles (2020.01.17.)”'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：Web of Science 对“深度强化学习”和“自动驾驶车辆（2020.01.17.）”的主题搜索
- en: I-A The Hierarchical Classification of Motion Planning for Autonomous Driving
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-A 自动驾驶运动规划的层次分类
- en: 'Using deep neural networks for self-driving cars gives the possibility to develop
    ”end-to-end” solutions where the system operates like a human driver: its inputs
    are the travel destination, the knowledge about the road network and various sensor
    information, and the output is the direct vehicle control commands, e.g., steering,
    torque, and brake. However, on the one hand, realizing such a scheme is quite
    complicated, since it needs to handle all layers of the driving task, on the other
    hand, the system itself behaves like a black box, which raises design and validation
    problems. By examining the recent advantages in the field, it can be said that
    most researches focus on solving some sub-tasks of the hierarchical motion planning
    problem. This decision-making system of autonomous driving can be decomposed into
    at least four layers, as stated in [[3](#bib.bib3)] (see Fig.[2](#S1.F2 "Figure
    2 ‣ I-A The Hierarchical Classification of Motion Planning for Autonomous Driving
    ‣ I Introduction ‣ Survey of Deep Reinforcement Learning for Motion Planning of
    Autonomous Vehicles").). Route planning, as the highest level, defines the way-points
    of the journey based on the map of the road network, with the possibility of using
    real-time traffic data. Though optimal route choice has a high interest among
    the research community, papers dealing with this level do not employ reinforcement
    learning. A comprehensive study on the subject can be found in [[4](#bib.bib4)].'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度神经网络来开发自驾车提供了“端到端”解决方案的可能性，该系统的工作方式类似于人类驾驶员：其输入包括旅行目的地、道路网络知识和各种传感器信息，输出则是直接的车辆控制指令，例如转向、扭矩和刹车。然而，一方面，实现这样的方案相当复杂，因为需要处理驾驶任务的所有层级；另一方面，系统本身表现得像一个黑箱，这带来了设计和验证问题。通过检查该领域的最新优势，可以说大多数研究集中于解决层次运动规划问题的一些子任务。正如[[3](#bib.bib3)]所述，这种自动驾驶的决策系统可以分解为至少四个层次（见图[2](#S1.F2
    "图 2 ‣ I-A 自动驾驶运动规划的层次分类 ‣ I 引言 ‣ 自动驾驶车辆运动规划的深度强化学习调查")）。路线规划作为最高层级，根据道路网络地图定义行程的路径点，并可以使用实时交通数据。尽管最优路线选择在研究界引起了高度关注，但处理这一层级的论文并未采用强化学习。有关该主题的全面研究可以参见[[4](#bib.bib4)]。
- en: '![Refer to caption](img/6326e6d3ac5bcf3d469e88b5f5024168.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/6326e6d3ac5bcf3d469e88b5f5024168.png)'
- en: 'Figure 2: Layers of motion planning'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：运动规划的层次
- en: The Behavioral layer is the strategic level of autonomous driving. With the
    given way-points, the agent decides on the short term policy, by taking into consideration
    the local road topology, the traffic rules, and the perceived state of other traffic
    participants. Having a finite set of available actions for the driving context,
    the realization of this layer is usually a finite state-machine having basic strategies
    in its states (i.e., car following, lane changing, etc.) with well-defined transitions
    between them based on the change of the environment. However, even with the full
    knowledge of the current state of the traffic, the future intentions of the surrounding
    drivers are unknown, making the problem partially observable [[5](#bib.bib5)].
    Hence the future state not only depends on the behavior of the ego vehicle but
    also relies on unknown processes; this problem forms a Partially Observable Markov
    Decision Process (POMDP). Different techniques exist to mitigate these effects
    by predicting the possible trajectories of other road users, like in [[6](#bib.bib6)],
    where the authors used gaussian mixture models, or in [[7](#bib.bib7)], where
    support vector machines and artificial neural networks were trained based on recorded
    traffic data. Since finite action POMDPs are the natural way of modeling reinforcement
    learning problems, a high amount of research papers deal with this level, as can
    be seen in the sections of the paper. To carry out the strategy defined by the
    behavioral layer, the motion planning layer needs to design a feasible trajectory
    consisting of the expected speed, yaw, and position states of the vehicle on a
    short horizon. Naturally, on this level, the vehicle dynamics has to be considered,
    hence classic exact solutions of motion planning are impractical since they usually
    assume holonomic dynamics. It has long been known that the numerical complexity
    of solving the motion planning problem with nonholonomic dynamics is Polynomial-Space
    Algorithm (PSPACE) [[8](#bib.bib8)], meaning it is hard to elaborate an overall
    solution by solving the nonlinear programming problem in real-time [[9](#bib.bib9)].
    On the other hand, the output representation of the layer makes it hard to directly
    handle it with ”pure” reinforcement learning, only a few papers deal solely with
    this layer, and they usually use DRL to define splines as a result of the training
    [[10](#bib.bib10), [11](#bib.bib11)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 行为层是自动驾驶的战略层。根据给定的路径点，代理决定短期策略，考虑到局部道路拓扑、交通规则以及对其他交通参与者状态的感知。在驾驶环境中有一有限的动作集，这一层的实现通常是一个有限状态机，在其状态中具有基本策略（即，跟车、变道等），并根据环境变化在这些状态之间进行明确定义的转换。然而，即使完全了解当前交通状态，周围司机的未来意图仍然未知，这使得问题成为部分可观察的[[5](#bib.bib5)]。因此，未来状态不仅依赖于自车的行为，还依赖于未知的过程；这个问题形成了部分可观察马尔可夫决策过程（POMDP）。为减轻这些影响，存在不同的技术，通过预测其他道路使用者的可能轨迹，例如在[[6](#bib.bib6)]中，作者使用了高斯混合模型，或在[[7](#bib.bib7)]中，使用了基于记录的交通数据训练的支持向量机和人工神经网络。由于有限动作POMDPs是建模强化学习问题的自然方式，大量研究论文涉及这一层，如论文的各部分所示。为了执行行为层定义的策略，运动规划层需要设计一个可行的轨迹，该轨迹包括车辆在短期内的预期速度、偏航角和位置状态。自然地，在这一层级，必须考虑车辆动力学，因此经典的运动规划精确解是不切实际的，因为它们通常假设全动力学。长期以来，已知使用非全动力学求解运动规划问题的数值复杂性是多项式空间算法（PSPACE）[[8](#bib.bib8)]，这意味着实时求解非线性规划问题的整体解决方案很难制定[[9](#bib.bib9)]。另一方面，该层的输出表示使得用“纯”强化学习直接处理变得困难，只有少数论文仅涉及这一层，且通常使用深度强化学习（DRL）定义样条作为训练结果[[10](#bib.bib10),
    [11](#bib.bib11)]。
- en: At the lowest level, the local feedback control is responsible for minimizing
    the deviation from the prescribed path or trajectory. A significant amount of
    papers reviewed in this article deals with the aspects of this task, where lane-keeping,
    trajectory following, or car following is the higher-level strategy. Though at
    this level, the action space becomes continuous, and classical approaches of RL
    can not handle this. Hence discretization of the control outputs is needed, or
    - as in some papers - continuous variants of DRL are used.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在最低层级，本地反馈控制负责最小化与规定路径或轨迹的偏差。本文回顾的大量论文涉及这一任务的各个方面，其中车道保持、轨迹跟随或跟车是更高层次的策略。尽管在这一层级，动作空间变为连续，经典的强化学习方法无法处理这一点。因此，需要对控制输出进行离散化，或者
    - 如某些论文中所示 - 使用深度强化学习的连续变体。
- en: I-B Reinforcement Learning
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: I-B 强化学习
- en: As an area of Artificial Intelligence and Machine Learning, Reinforcement learning
    (RL) deals with the problem of a learning agent placed in an environment to achieve
    a goal. Contrary to supervised learning, where the learner structure gets examples
    of good and bad behavior, the RL agent must discover by trial and error how to
    behave to get the most reward [[12](#bib.bib12)]. For this task, the agent must
    percept the state of the environment at some level and based on this information,
    and it needs to take actions that result in a new state. As a result of its action,
    the agent receives a reward, which aids in the development of future behavior.
    To ultimately formulate the problem, modeling the state transitions of the environment,
    based on the actions of the agent is also a necessity. This leads to the formulation
    of a POMDP defined by the functions of $(\mathcal{S},\mathcal{A},T,R,\Omega,O)$,
    where $\mathcal{S}$ is the set of environment states, $\mathcal{A}$ is the set
    of possible actions in that particular state, $T$ is the transition function between
    the states based on the actions, $R$ is the reward for the given $(\mathcal{S},\mathcal{A})$
    pair, while $\Omega$ is the set of observations, and $O$ is the sensor model.
    The agent in this context can be formulated by any inference model whose parameters
    can be modified in response to the experience gained. In the context of Deep Reinforcement
    Learning, this model is implemented by neural networks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人工智能和机器学习的一个领域，强化学习（RL）处理的是将学习代理放置在一个环境中以实现目标的问题。与监督学习不同，在监督学习中，学习者结构会得到好行为和坏行为的示例，而RL代理必须通过试错法发现如何行为才能获得最大的奖励[[12](#bib.bib12)]。在这个任务中，代理必须在某种程度上感知环境的状态，并基于这些信息采取导致新状态的行动。作为其行动的结果，代理会获得奖励，这有助于未来行为的发展。为了**最终**制定问题，建模环境状态的转换，基于代理的行动也是必要的。这导致了一个由$(\mathcal{S},\mathcal{A},T,R,\Omega,O)$函数定义的POMDP模型，其中$\mathcal{S}$是环境状态的集合，$\mathcal{A}$是该特定状态下可能的行动集合，$T$是基于行动的状态之间的转换函数，$R$是给定$(\mathcal{S},\mathcal{A})$对的奖励，而$\Omega$是观察集合，$O$是传感器模型。在这种情况下，代理可以通过任何推理模型进行制定，该模型的参数可以根据获得的经验进行修改。在深度强化学习的背景下，这一模型由神经网络实现。
- en: '![Refer to caption](img/8b423a2c15a1ab8801ac5083c9837b14.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/8b423a2c15a1ab8801ac5083c9837b14.png)'
- en: 'Figure 3: The POMDP model for Deep Reinforcement Learning based autonomous
    driving'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：用于深度强化学习的POMDP模型在自动驾驶中的应用
- en: 'The problem in the POMDP scenario is that the current actions affect the future
    states, therefore the future rewards, meaning that for optimizing the behavior
    for the cumulative reward throughout the entire episode, the agent needs to have
    information about the future consequences of its actions. RL has two main approaches
    for determining the optimal behavior: value-based and policy-based methods.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: POMDP场景中的问题是当前的行动会影响未来的状态，因此未来的奖励，这意味着为了优化整个过程中的累积奖励，代理需要了解其行动的未来后果。RL有两种主要方法来确定最佳行为：基于价值的方法和基于策略的方法。
- en: The original concept using a value-based method is the Deep-Q Learning Network
    (DQN) introduced in [[1](#bib.bib1)]. Described briefly, the agent predicts a
    so-called Q value for each state-action pair, which formulate the expected immediate
    and future reward. From this set, the agent can choose the action with the highest
    value as an optimal policy or can use the values for exploration during the training
    process. The main goal is to learn the optimal Q function, represented by a neural
    network in this case. This can be done by conducting experiments, calculating
    the discounted rewards of the future states for each action, and updating the
    network by using the Bellman-equation [[13](#bib.bib13)] as a target. Using the
    same network for value evaluation and action selection results in unstable behavior
    and slow learning in noisy environments. Meta-heuristics, such as experience replay,
    can handle this problem, while other variants of the original DQN exist, such
    as Double DQN [[14](#bib.bib14)] or Dueling DQN [[15](#bib.bib15)], separating
    the action and the value prediction streams, leading to faster and more stable
    learning.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基于价值的方法的原始概念是[[1](#bib.bib1)]中介绍的深度Q学习网络（DQN）。简要描述，代理为每个状态-动作对预测一个所谓的Q值，表示期望的即时和未来奖励。从这个集合中，代理可以选择具有最高值的动作作为最优策略，或在训练过程中使用这些值进行探索。主要目标是学习最优Q函数，在这种情况下由神经网络表示。这可以通过进行实验、计算每个动作未来状态的折扣奖励，并使用Bellman方程[[13](#bib.bib13)]作为目标来更新网络来完成。使用相同的网络进行价值评估和动作选择会导致在噪声环境中行为不稳定和学习缓慢。元启发式方法，例如经验回放，可以解决这个问题，而原始DQN的其他变体，如Double
    DQN[[14](#bib.bib14)]或Dueling DQN[[15](#bib.bib15)]，分离动作和价值预测流，从而实现更快和更稳定的学习。
- en: Policy-based methods target at choosing the optimal behavior directly, where
    the policy $\pi_{\Theta}$ is a function of $(\mathcal{S},\mathcal{A})$. Represented
    by a neural network, with a softmax head, the agent generally predicts a normalized
    probability of the expected goodness of the actions. In the most natural implementation,
    this output integrates the exploration property of the RL process. In advanced
    variants, such as the actor-critic, the agent uses different predictions for the
    value and the action [[16](#bib.bib16)]. Initially, RL algorithms use finite action
    space, though, for many control problems, they are not suitable. To overcome this
    issue in [[17](#bib.bib17)] introduced the Deep Deterministic Policy Gradients
    (DDPG) agent, where the actor directly maps states to continuous actions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 基于策略的方法直接选择最佳行为，其中策略$\pi_{\Theta}$是$(\mathcal{S},\mathcal{A})$的一个函数。由神经网络表示，带有softmax头，代理通常预测动作的归一化概率。在最自然的实现中，这个输出整合了RL过程的探索特性。在高级变体中，例如演员-评论家，代理使用不同的预测来评估价值和动作[[16](#bib.bib16)]。最初，RL算法使用有限的动作空间，但对于许多控制问题，它们并不适用。为解决这个问题，在[[17](#bib.bib17)]中引入了深度确定性策略梯度（DDPG）代理，其中演员直接将状态映射到连续动作。
- en: 'For complex problems, the learning process can still be long or even unsuccessful.
    It can be soluted in many ways:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂问题，学习过程仍可能很长或甚至失败。可以通过多种方式解决：
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Curriculum learning describes a type of learning in which the training starts
    with only easy examples of a task and then gradually increase difficulty. This
    approach is used in [[18](#bib.bib18), [19](#bib.bib19), [20](#bib.bib20)].
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 课程学习描述了一种学习类型，其中训练从任务的简单示例开始，然后逐渐增加难度。这种方法在[[18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)]中被使用。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Adversarial learning aims to fool models through malicious input. Papers using
    variants of this technique are: [[21](#bib.bib21), [22](#bib.bib22)]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗学习旨在通过恶意输入欺骗模型。使用这种技术变体的论文有：[[21](#bib.bib21), [22](#bib.bib22)]
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Model-based action choice, such as the MCTS based solution of Alpha-Go, can
    reduce the effect of the problem of distant rewarding.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于模型的动作选择，例如Alpha-Go的MCTS解决方案，可以减少远程奖励问题的影响。
- en: 'Since reinforcement learning models the problem as a POMDP, a discrete-time
    stochastic control process, the solutions need to provide a mathematical framework
    for this decision making in situations where outcomes are partly random and partly
    under the control of a decision-maker, while the states are also partly observable
    [[23](#bib.bib23)]. In the case of motion planning for autonomous or highly automated
    vehicles, the tuple $(\mathcal{S},\mathcal{A},T,R,\Omega,O)$ of the POMDP is illustrated
    in Fig. [3](#S1.F3 "Figure 3 ‣ I-B Reinforcement Learning ‣ I Introduction ‣ Survey
    of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles") and
    can be interpreted as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强化学习将问题建模为POMDP，一个离散时间随机控制过程，解决方案需要为这种决策提供一个数学框架，以应对结果既部分随机又部分由决策者控制的情况，同时状态也部分可观察[[23](#bib.bib23)]。在自动或高度自动化车辆的运动规划情况下，POMDP的元组$(\mathcal{S},\mathcal{A},T,R,\Omega,O)$在图[3](#S1.F3
    "Figure 3 ‣ I-B Reinforcement Learning ‣ I Introduction ‣ Survey of Deep Reinforcement
    Learning for Motion Planning of Autonomous Vehicles")中进行了说明，可以解释如下：
- en: $\mathcal{S},\mathcal{A},T,$ and $R$ describe the MDP, the modeling environment
    of the learning process. It can vary depending on the goals, though in our case
    it needs to model the dynamics of the vehicle, the surrounding static and dynamic
    objects, such as other participants of the traffic, the road topology, lane markings,
    signs, traffic rules, etc. $\mathcal{S}$ holds the current actual state of the
    simulation. $A$ is the possible set of actions of the agent driving the ego-car,
    while $T$, the so-called state-transition function updates the vehicle state and
    also the states of the traffic participants depending on the action of the vehicle.
    The different levels of abstraction are described in section [II-A](#S2.SS1 "II-A
    Vehicle modeling ‣ II Modeling for Reinforcement Learning ‣ Survey of Deep Reinforcement
    Learning for Motion Planning of Autonomous Vehicles"). Many research papers use
    different software platforms for modeling the environment. A brief collection
    of the used frameworks are presented in section [II-B](#S2.SS2 "II-B Simulators
    ‣ II Modeling for Reinforcement Learning ‣ Survey of Deep Reinforcement Learning
    for Motion Planning of Autonomous Vehicles"). $R$ is the reward function of the
    MDP, section [II-D](#S2.SS4 "II-D Rewarding ‣ II Modeling for Reinforcement Learning
    ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles")
    gives a summary on this topic.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{S},\mathcal{A},T,$ 和 $R$ 描述了MDP，即学习过程的建模环境。根据目标的不同，它可能有所变化，但在我们的案例中，它需要建模车辆的动态，周围的静态和动态物体，如交通参与者、道路拓扑、车道标记、标志、交通规则等。$\mathcal{S}$
    保存了仿真的当前实际状态。$A$ 是代理驾驶自车的可能动作集合，而 $T$，即所谓的状态转移函数，根据车辆的动作更新车辆状态以及交通参与者的状态。不同的抽象层次在[II-A](#S2.SS1
    "II-A Vehicle modeling ‣ II Modeling for Reinforcement Learning ‣ Survey of Deep
    Reinforcement Learning for Motion Planning of Autonomous Vehicles")部分中进行了描述。许多研究论文使用不同的软件平台来建模环境。所使用的框架的简要汇总见[II-B](#S2.SS2
    "II-B Simulators ‣ II Modeling for Reinforcement Learning ‣ Survey of Deep Reinforcement
    Learning for Motion Planning of Autonomous Vehicles")部分。$R$ 是MDP的奖励函数，[II-D](#S2.SS4
    "II-D Rewarding ‣ II Modeling for Reinforcement Learning ‣ Survey of Deep Reinforcement
    Learning for Motion Planning of Autonomous Vehicles")部分对此进行了总结。
- en: $\Omega$ is the set of observations the agent can experience in the world, while
    $O$ is the observation function giving a possibility distribution over the possible
    observations. In more uncomplicated cases, the studies assume full observability
    and formulate the problem as an MDP, though in many cases, the vehicle does not
    possess all information. Another interesting topic is the representation of the
    state observation, which is a crucial factor for the architecture choice and performance
    of Deep RL agents. The observation models used in the literature are summarized
    in section [II-E](#S2.SS5 "II-E Observation Space ‣ II Modeling for Reinforcement
    Learning ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous
    Vehicles").
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: $\Omega$ 是代理可以在世界中体验到的观察集合，而 $O$ 是给出可能观察的分布的观察函数。在更简单的情况下，研究假设完全可观察，并将问题表述为MDP，尽管在许多情况下，车辆并不拥有所有信息。另一个有趣的话题是状态观察的表示，这是Deep
    RL代理架构选择和性能的关键因素。文献中使用的观察模型在[II-E](#S2.SS5 "II-E Observation Space ‣ II Modeling
    for Reinforcement Learning ‣ Survey of Deep Reinforcement Learning for Motion
    Planning of Autonomous Vehicles")部分中进行了总结。
- en: II Modeling for Reinforcement Learning
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 强化学习的建模
- en: II-A Vehicle modeling
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 车辆建模
- en: Modeling the movement of the ego-vehicle is a crucial part of the training process
    since it raises the trade-off problem between model accuracy and computational
    resource. Since RL techniques use a massive number of episodes for determining
    optimal policy, the step time of the environment, which highly depends on the
    evaluation time of the vehicle dynamics model, profoundly affects training time.
    Therefore during environment design, one needs to choose from the simplest kinematic
    model to more sophisticated dynamics models ranging from 2 Degree of Freedom (2DoF)
    lateral model to the more and more complex models with a higher number of parameters
    and complicated tire models.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对自车运动的建模是训练过程中的关键部分，因为它引发了模型准确性与计算资源之间的权衡问题。由于强化学习（RL）技术使用大量的实验来确定最佳策略，环境的步进时间高度依赖于车辆动态模型的评估时间，这对训练时间有着深远的影响。因此，在环境设计过程中，需要从最简单的运动学模型选择到更复杂的动态模型，范围从2自由度（2DoF）侧向模型到参数更多、更复杂的轮胎模型。
- en: At rigid kinematic single-track vehicle models, which neglect tire slip and
    skip, lateral motion is only affected by the geometric parameters. Therefore,
    they are usually limited to low-speed applications. More details about the model
    can be found in [[24](#bib.bib24)]. The simplest dynamic models with longitudinal
    and lateral movements are based on the 3 Degrees of Freedom (3DoF) dynamic bicycle
    model, usually with a linear tire model. They consider $(V_{x},V_{y},\dot{\Psi})$
    as independent variables, namely longitudinal and lateral speed, and yaw rate.
    A more complex model is the four-tire 9 Degrees of Freedom (9DoF) vehicle model,
    where amongst the parameters of the 3DoF, body roll and pitch $(\dot{\Theta},\dot{\Phi})$
    and the angular velocities of the four wheels $({\omega_{fl},\omega_{fr},\omega_{rl},\omega_{rr}})$
    are also considered, to calculate tire forces more precisely. Hence the model
    takes into account both the coupling of longitudinal and lateral slips and the
    load transfer between tires.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在刚性运动学单轨车辆模型中，这些模型忽略了轮胎滑移和跳跃，侧向运动仅受几何参数的影响。因此，它们通常仅限于低速应用。有关模型的更多细节可以在[[24](#bib.bib24)]中找到。最简单的动态模型包括纵向和侧向运动，基于3自由度（3DoF）动态自行车模型，通常使用线性轮胎模型。它们将$(V_{x},V_{y},\dot{\Psi})$视为独立变量，即纵向和侧向速度，以及偏航率。一个更复杂的模型是四轮9自由度（9DoF）车辆模型，其中除了3DoF模型的参数，还考虑了车身的滚动和俯仰$(\dot{\Theta},\dot{\Phi})$以及四个轮子的角速度$({\omega_{fl},\omega_{fr},\omega_{rl},\omega_{rr}})$，以更精确地计算轮胎力。因此，该模型考虑了纵向和侧向滑移的耦合以及轮胎之间的负荷转移。
- en: Though the kinematic model seems quite simplified, and as stated in [[25](#bib.bib25)],
    such a model can behave significantly different from an actual vehicle, though
    for the many control situations, the accuracy is suitable [[24](#bib.bib24)].
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管运动学模型看起来非常简化，并且正如[[25](#bib.bib25)]中所述，这种模型的行为可能与实际车辆有显著不同，但对于许多控制情况，其准确性是合适的[[24](#bib.bib24)]。
- en: According to [[25](#bib.bib25)], using a kinematic bicycle model with a limitation
    on the lateral acceleration at around $0.5g$ or less provides appropriate results,
    but only with the assumption of dry road. Above this limit, the model is unable
    to handle dynamics. Hence a more accurate vehicle model should be used when dealing
    with higher accelerations to push the vehicle’s dynamics near its handling limits.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据[[25](#bib.bib25)]，使用一个具有大约$0.5g$或更低侧向加速度限制的运动学自行车模型可以提供合适的结果，但前提是道路是干燥的。超过此限制，模型将无法处理动态问题。因此，在处理更高加速度时，应使用更准确的车辆模型，以将车辆的动态推向其操控极限。
- en: Regarding calculation time, based on the kinematic model, the calculation of
    the 3DoF model can be $10\dots 50$ times higher, and the precise calculation of
    a 9DoF model with nonlinear tire model can be $100\dots 300$ times higher, which
    is the main reason for the RL community to use a low level of abstraction.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 关于计算时间，基于运动学模型的3DoF模型的计算可能是$10\dots 50$倍，而具有非线性轮胎模型的9DoF模型的精确计算可能是$100\dots
    300$倍，这也是RL社区使用低抽象级别的主要原因。
- en: Modeling traffic and surrounding vehicles is often performed by using unique
    simulators, as described in section [II-B](#S2.SS2 "II-B Simulators ‣ II Modeling
    for Reinforcement Learning ‣ Survey of Deep Reinforcement Learning for Motion
    Planning of Autonomous Vehicles"). Some authors develop their environments, using
    cellular automata models [[26](#bib.bib26)]. Some use MOBIL, which is a general
    model (minimizing overall braking induced by lane change) to derive lane-changing
    rules for discretionary and mandatory lane changes for a broad class of car-following
    models [[27](#bib.bib27)]; the Intelligent Driving Model (IDM), a continuous microscopic
    single-lane model [[28](#bib.bib28)].
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 交通和周围车辆的建模通常使用独特的模拟器，如[II-B](#S2.SS2 "II-B Simulators ‣ II Modeling for Reinforcement
    Learning ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous
    Vehicles")节中所述。一些作者使用细胞自动机模型 [[26](#bib.bib26)] 开发他们的环境。一些使用 MOBIL，这是一个通用模型（最小化车道变换引起的整体制动）来推导适用于广泛的汽车跟随模型的车道变换规则
    [[27](#bib.bib27)]；还有智能驾驶模型（IDM），这是一个连续的微观单车道模型 [[28](#bib.bib28)]。
- en: II-B Simulators
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 模拟器
- en: Some authors create self-made environments to achieve full control over the
    model, though there are commercial and Open-source environments that can provide
    this feature. This section briefly identifies some of them used in recent researches
    in motion planning with RL.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一些作者创建自制环境以实现对模型的完全控制，尽管也有商业和开源环境可以提供此功能。本节简要介绍了一些在使用 RL 进行运动规划的近期研究中使用的环境。
- en: In modeling the traffic environment, the most popular choice is SUMO (Simulation
    of Urban MObility), which is a microscopic, inter- and multi-modal, space-continuous
    and time-discrete traffic flow simulation platform [[29](#bib.bib29)]. It can
    convert networks from other traffic simulators such as VISUM, Vissim, or MATSim
    and also reads other standard digital road network formats, such as OpenStreetMap
    or OpenDRIVE. It also provides interfaces to several environments, such as python,
    Matlab, .Net, C++, etc. Though the abstraction level, in this case, is microscopic,
    and vehicle behavior is limited, its ease of use and high speed makes it an excellent
    choice for training agents to handle traffic, though it does not provide any sensor
    model besides the ground truth state of the vehicles.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模交通环境时，最受欢迎的选择是 SUMO（城市移动仿真），这是一个微观的、跨模式和多模式、空间连续且时间离散的交通流量模拟平台 [[29](#bib.bib29)]。它可以从其他交通模拟器（如
    VISUM、Vissim 或 MATSim）转换网络，并且还读取其他标准数字道路网络格式，如 OpenStreetMap 或 OpenDRIVE。它还提供了与多个环境的接口，如
    python、Matlab、.Net、C++ 等。尽管这种情况下的抽象层次是微观的，车辆行为有限，但它的易用性和高速度使其成为训练代理处理交通的绝佳选择，尽管它不提供除了车辆真实状态之外的任何传感器模型。
- en: Another popular microscopic simulator that has been used commercially and for
    research also is VISSIM [[30](#bib.bib30)]. In [[31](#bib.bib31)] it is used for
    developing car-following behavior and lane changing decisions.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种广泛用于商业和研究的微观模拟器是 VISSIM [[30](#bib.bib30)]。在 [[31](#bib.bib31)] 中，它被用于开发汽车跟随行为和车道变换决策。
- en: Considering only vehicle dynamics, the most popular choice is TORCS (The Open
    Racing Car Simulator), which is a modern, modular, highly portable multi-player,
    multi-agent car simulator. Its high degree of modularity and portability render
    it ideal for artificial intelligence research [[32](#bib.bib32)]. Interfacing
    with python, the most popular AI research environment is comfortable and runs
    at an acceptable speed. TORCS also comes with different tracks, competing robots,
    and several sensor models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 仅考虑车辆动力学时，最受欢迎的选择是 TORCS（开放赛车模拟器），这是一个现代的、模块化的、高度便携的多玩家、多智能体汽车模拟器。它的高度模块化和便携性使其成为人工智能研究的理想选择
    [[32](#bib.bib32)]。与 python 的接口使得最受欢迎的 AI 研究环境舒适且运行速度可接受。TORCS 还配有不同的赛道、竞争机器人和多个传感器模型。
- en: It is assumed that for vehicle dynamics, the best choices would be the professional
    tools, such as CarSIM [[33](#bib.bib33)] or CarMaker [[34](#bib.bib34)], though
    the utilization of these softwares can not be found in the reinforcement learning
    literature. This may be caused by the fact that these are expensive commercial
    platforms, though more importantly, their lack of python interfaces and high precision,
    but resource-intensive models prevent them from running several episodes within
    a reasonable time.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 假设在车辆动态方面，最佳选择是专业工具，如CarSIM[[33](#bib.bib33)]或CarMaker[[34](#bib.bib34)]，尽管这些软件在强化学习文献中并未出现。这可能是由于这些软件是昂贵的商业平台，更重要的是，它们缺乏Python接口和高精度，但资源密集型模型使得它们无法在合理时间内运行多个回合。
- en: 'For more detailed sensor models or traffic, the authors usually use Airsim,
    Udacity Gazebo/ROS, and CARLA:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更详细的传感器模型或交通，作者通常使用Airsim、Udacity Gazebo/ROS和CARLA：
- en: AirSim, used by a recent research in [[35](#bib.bib35)], is a simulator initially
    developed for drones built on Unreal Engine now has a vehicle extension with different
    weather conditions and scenarios.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: AirSim，在最近的研究中[[35](#bib.bib35)]使用，是一个最初为无人机开发的模拟器，基于Unreal Engine构建，现在具有不同天气条件和场景的车辆扩展。
- en: Udacity, used in [[36](#bib.bib36)], is a simulator that was built for Udacity’s
    Self-Driving Car Nanodegree [[37](#bib.bib37)] provides various sensors, such
    as high quality rendered camera image LIDAR and Infrared information, and also
    has capabilities to model other traffic participants.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Udacity，在[[36](#bib.bib36)]中使用，是一个为Udacity的自动驾驶汽车纳米学位[[37](#bib.bib37)]构建的模拟器，提供了各种传感器，如高质量渲染的摄像头图像LIDAR和红外信息，并且能够模拟其他交通参与者。
- en: Another notable mention is CARLA, an open-source simulator for autonomous driving
    research. CARLA has been developed from the ground up to support the development,
    training, and validation of autonomous urban driving systems. In addition to open-source
    code and protocols, CARLA provides open digital assets (urban layouts, buildings,
    vehicles) that were created for this purpose and can be used freely. The simulation
    platform supports flexible specification of sensor suites and environmental conditions
    [[38](#bib.bib38)].
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个值得提及的是CARLA，这是一个用于自动驾驶研究的开源模拟器。CARLA从头开始开发，支持自动化城市驾驶系统的开发、训练和验证。除了开源代码和协议外，CARLA还提供了为此目的创建并可以自由使用的开放数字资产（城市布局、建筑物、车辆）。该模拟平台支持传感器套件和环境条件的灵活指定[[38](#bib.bib38)]。
- en: Though this section provides only a brief description of the simulators, a more
    systematic review of the topic can be found in [[39](#bib.bib39)].
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这一部分仅提供了模拟器的简要描述，但可以在[[39](#bib.bib39)]中找到对该主题的更系统的回顾。
- en: II-C Action Space
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-C 行动空间
- en: 'The choice of action space highly depends on the vehicle model and task designed
    for the reinforcement learning problem in each previous research. Though two main
    levels of control can be found: one is the direct control of the vehicle by steering
    braking and accelerating commands, and the other acts on the behavioral layer
    and defines choices on strategic levels, such as lane change, lane keeping, setting
    ACC reference point, etc. At this level, the agent gives a command to low-level
    controllers, which calculate the actual trajectory. Only a few papers deal with
    the motion planning layer, where the task defines the endpoints $(x,y,\theta)$,
    and the agent defines the knots of the trajectory to follow represented as a spline,
    as can be seen in [[11](#bib.bib11)]. Also, few papers deviate from vehicle motion
    restrictions and generate actions by stepping in a grid, like in classic cellular
    automata microscopic models [[40](#bib.bib40)].'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 行动空间的选择高度依赖于车辆模型和为每个前期研究设计的强化学习问题任务。虽然可以找到两个主要的控制层级：一个是通过转向、制动和加速命令直接控制车辆，另一个作用于行为层，定义战略层次上的选择，如换车道、保持车道、设置ACC参考点等。在这个层级，代理给低层控制器发出命令，后者计算实际轨迹。只有少数几篇论文处理运动规划层，其中任务定义了端点$(x,y,\theta)$，代理定义了要跟随的轨迹的节点，这些节点表示为样条曲线，如[[11](#bib.bib11)]中所示。此外，很少有论文偏离车辆运动限制，并通过在网格中步进生成动作，如经典的细胞自动机微观模型[[40](#bib.bib40)]。
- en: Some papers combine the control and behavioral layers by separating longitudinal
    and lateral tasks, where longitudinal acceleration is a direct command, while
    lane changing is a strategic decision like in [[41](#bib.bib41)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一些论文通过分离纵向和横向任务来结合控制层和行为层，其中纵向加速度是直接命令，而换车道是战略决策，如[[41](#bib.bib41)]中所示。
- en: The behavioral layer usually holds a few distinct choices, from which the underlying
    neural network needs to choose, making it a classic reinforcement learning task
    with finite actions.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 行为层通常包含几个不同的选择，底层神经网络需要从中选择，这使其成为一个经典的有限动作的强化学习任务。
- en: Though on the level of control, the actuation of vehicles, i.e., steering, throttle,
    and braking, are continuous parameters and many reinforcement learning techniques
    like DQN and PG can not handle this since they need finite action set, while some,
    like DDPG, works with continuous action space. To adapt to the finite action requirements
    of the RL technique used, most papers discretizes the steering and acceleration
    commands to 3 to 9 possibilities per channel. The low number of possible choices
    pushes the solution farther from reality, which could raise vehicle dynamics issues
    with uncontrollable slips, massive jerk, and yaw-rate, though the utilization
    of kinematic models sometimes covers this in the papers. A large number of discrete
    choices, however, ends up in an exponential growth in the possible outcomes in
    the POMDP approach, which slows down the learning process.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在控制层面上，车辆的执行，即转向、油门和刹车，是连续参数，许多强化学习技术如DQN和PG不能处理这种情况，因为它们需要有限的动作集，而一些技术，如DDPG，则适用于连续动作空间。为了适应使用的强化学习技术的有限动作要求，大多数论文将转向和加速命令离散化为每个通道3到9种可能性。选择的可能性较少将解决方案推得离现实更远，这可能会引发车辆动态问题，如无法控制的打滑、大幅度的突然加速和偏航率，尽管利用运动学模型有时可以在论文中覆盖这些问题。然而，大量离散选择最终会导致POMDP方法中可能结果的指数增长，这会减缓学习过程。
- en: II-D Rewarding
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-D 奖励
- en: 'During training, the agent tries to fulfill a task, generally consisting of
    more than one step. This task is called an episode. An episode ends if one of
    the following conditions are met:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，代理尝试完成一个任务，这个任务通常由多个步骤组成。这个任务被称为一个回合。回合在以下条件之一满足时结束：
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The agent successfully fulfills the task;
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 代理成功完成任务；
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The episode reaches a previously defined steps
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 回合达到预先定义的步骤。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A terminating condition rises.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 出现终止条件。
- en: 'The first two cases are trivial and depend on the design of the actual problem.
    Terminal conditions are typically situations where the agent reaches a state from
    which the actual task is impossible to fulfill, or the agent makes a mistake that
    is not acceptable. Vehicle motion planning agents usually use terminating conditions,
    such as: collision with other participants or obstacles or leaving the track or
    lane, since these two inevitably end the episode. There are lighter approaches,
    where the episode terminates with failure before the accident occurred, with examples
    of having a too high tangent angle to the track or reaching too close to other
    participants. These ”before accident” terminations speed up the training by bringing
    the information of failure forward in time, though their design needs caution
    [[42](#bib.bib42)].'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种情况是微不足道的，取决于实际问题的设计。终止条件通常是代理到达一个状态，从该状态实际任务无法完成，或者代理犯了一个不可接受的错误。车辆运动规划代理通常使用终止条件，例如：与其他参与者或障碍物碰撞或离开赛道或车道，因为这两者不可避免地结束回合。还有一些较轻的做法，其中回合在事故发生之前以失败结束，例如：与赛道的切线角度过高或与其他参与者过于接近。这些“事故前”终止通过将失败的信息提前到达，从而加快了训练，尽管它们的设计需要小心[[42](#bib.bib42)]。
- en: 'Rewarding plays the role of evaluating the goodness of the choices the agent
    made during the episode giving feedback to improve the policy. The first important
    aspect is the timing of the reward, where the designer of the reinforcement learning
    solution needs to choose a mixture of the following strategies all having their
    pros and cons:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励在评估代理在回合期间所做选择的好坏方面发挥作用，提供反馈以改进策略。第一个重要的方面是奖励的时机，强化学习解决方案的设计者需要选择以下策略的混合，这些策略都有其优缺点：
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Giving reward only at the end of the episode and discounting it back to the
    previous $(\mathcal{S},\mathcal{A})$ pairs, which could result in a slower learning
    process, though minimizes the human-driven shaping of the policy.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 仅在回合结束时给予奖励并将其折现回先前的$(\mathcal{S},\mathcal{A})$对，这可能会导致学习过程较慢，但最小化了人为对策略的塑造。
- en: •
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Giving immediate reward at each step by evaluating the current state, naturally
    discount also appears in this solution, which results in significantly faster
    learning, though the choice of the immediate reward highly affects the established
    strategy, which sometimes prevents the agent from developing better overall solutions
    than the one that gave the intention of the designed reward.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过评估当前状态在每一步给予即时奖励，当然，折扣也会出现在这种解决方案中，这导致学习速度显著提高，尽管即时奖励的选择高度影响建立的策略，有时这会阻止智能体发展比设计奖励意图更好的整体解决方案。
- en: •
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An intermediate solution can be to give a reward in predefined periods or travel
    distance [[43](#bib.bib43)], or when a good or bad decision occurs.
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一种中间解决方案是通过在预定义的时间段或行驶距离内给予奖励[[43](#bib.bib43)]，或在做出好或坏决定时给予奖励。
- en: 'In the area of motion planning, the end episode rewards are calculated from
    the fulfillment or failure of the driving task. The overall performance factors
    are generally: time of finishing the task, keeping the desired speed or achieving
    as high average speed as possible, yaw or distance from lane middle or the desired
    trajectory, overtaking more vehicles, achieve as few lane changes as possible
    [[44](#bib.bib44)], keeping right [[45](#bib.bib45), [46](#bib.bib46)] etc. Rewarding
    systems also can represent passenger comfort, where the smoothness of the vehicle
    dynamics is enforced. The most used quantitative measures are the longitudinal
    acceleration [[47](#bib.bib47)], lateral acceleration [[48](#bib.bib48), [49](#bib.bib49)]
    and jerk [[50](#bib.bib50), [10](#bib.bib10)].'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在运动规划领域，最终回合奖励是根据驾驶任务的完成或失败来计算的。总体性能因素通常包括：完成任务的时间、保持期望速度或实现尽可能高的平均速度、航向或与车道中线的距离或期望轨迹、超越更多车辆、尽量减少车道变换[[44](#bib.bib44)]、保持右侧[[45](#bib.bib45),
    [46](#bib.bib46)]等。奖励系统也可以体现乘客舒适度，其中强调车辆动态的平稳性。最常用的定量指标包括纵向加速度[[47](#bib.bib47)]、横向加速度[[48](#bib.bib48),
    [49](#bib.bib49)]和颠簸[[50](#bib.bib50), [10](#bib.bib10)]。
- en: In some researches, the reward is based on the deviation from a dataset [[51](#bib.bib51)],
    or calculated as a deviation from a reference model like in [[52](#bib.bib52)].
    These approaches can provide favorable results, though a bit tends from the original
    philosophy of reinforcement learning since a previously known strategy could guide
    the learning.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些研究中，奖励基于与数据集的偏差[[51](#bib.bib51)]，或计算为与参考模型的偏差，如[[52](#bib.bib52)]中所示。这些方法可以提供有利的结果，但有些偏离了强化学习的原始理念，因为之前已知的策略可能会指导学习。
- en: II-E Observation Space
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-E 观察空间
- en: 'The observation space describes the world to the agent. It needs to give sufficient
    information for choosing the appropriate action, hence - depending on the task
    - it contains the following knowledge:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 观察空间向智能体描述了世界。它需要提供足够的信息来选择合适的动作，因此——根据任务的不同——它包含以下知识：
- en: •
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The state of the vehicle in the world, e.g., position, speed, yaw, etc.
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 车辆在世界中的状态，例如，位置、速度、航向等。
- en: •
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Topological information like lanes, signs, rules, etc.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 顶点信息，如车道、标志、规则等。
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Other participants: surrounding vehicles, obstacles, etc.'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其他参与者：周围的车辆、障碍物等。
- en: The reference frame of the observation can be absolute and fixed to the coordinate
    system of the world, though as the decision process focuses on the ego-vehicle,
    it is more straightforward to choose an ego-centric reference frame pinned to
    the vehicle’s coordinate system, or the vehicle’s position in the world, and the
    orientation of the road. It allows concentrating the distribution of visited states
    around the origin in both position, heading, and velocity space, as other vehicles
    are often close to the ego-vehicle and with similar speed and heading, reducing
    the region of state-space in which the policy must perform. [[53](#bib.bib53)]
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 观察的参考系可以是绝对的，并固定在世界的坐标系统中，但由于决策过程集中在自车上，因此更直接的选择是以自车的坐标系统或车辆在世界中的位置以及道路的方向作为自车中心的参考系。这允许将访问状态的分布集中在位置、航向和速度空间的原点周围，因为其他车辆通常靠近自车，并具有相似的速度和航向，从而减少了策略必须执行的状态空间区域。[[53](#bib.bib53)]
- en: II-E1 Vehicle state observation
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-E1 车辆状态观察
- en: For lane keeping, navigation, simple racing, overtaking, or maneuvering tasks,
    the most commonly used and also the simplest observation for the ego vehicle consists
    of the continuous variables of $(|e|,v,\theta_{e})$ describing the lateral position
    from the center-line of the lane, vehicle speed, and yaw angle respectively. (see
    Fig. [4](#S2.F4 "Figure 4 ‣ II-E1 Vehicle state observation ‣ II-E Observation
    Space ‣ II Modeling for Reinforcement Learning ‣ Survey of Deep Reinforcement
    Learning for Motion Planning of Autonomous Vehicles")). This information is the
    absolute minimum for guiding car-like vehicles, and only eligible for the control
    of the classical kinematic car-like models, where the system implies the motion
    without skidding assumption. Though in many cases in the literature, this can
    be sufficient, since the vehicles remain deep in the dynamically stable region.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 对于车道保持、导航、简单赛车、超车或机动任务，最常用且最简单的自车观察包括连续变量$(|e|,v,\theta_{e})$，分别描述车道中心线的横向位置、车速和横摆角。（见图
    [4](#S2.F4 "图 4 ‣ II-E1 车辆状态观察 ‣ II-E 观察空间 ‣ II 建模强化学习 ‣ 自动驾驶车辆运动规划的深度强化学习调查")）。这些信息是引导车类车辆的绝对最低要求，仅适用于经典运动学车类模型的控制，其中系统假设运动没有打滑。尽管在许多文献中，这可能已经足够，因为车辆仍然深处于动态稳定区域。
- en: '![Refer to caption](img/a24094f1f2f8ccf90d4c7ed677de4391.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/a24094f1f2f8ccf90d4c7ed677de4391.png)'
- en: 'Figure 4: Observation for basic vehicle state (source: [[3](#bib.bib3)])'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：基础车辆状态的观察（来源：[[3](#bib.bib3)]）
- en: For tasks, where more complex vehicle dynamics is inevitable, such as racing
    situations, or where the stability of the vehicle is essential, this set of observable
    state would not be enough, and it should be extended with yaw, pitch, roll, tire
    dynamics, and slip.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些复杂的车辆动态不可避免的任务，如赛车情况，或车辆稳定性至关重要的情况，这组可观察状态将不足以满足需求，应该扩展为包含横摆角、俯仰角、滚转角、轮胎动态和滑移。
- en: II-E2 Environment observation
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-E2 环境观察
- en: 'Getting information about the surroundings of the vehicle and representing
    it to the learning agent shows high diversity in the literature. Different levels
    of sensor abstractions can be observed:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 获取车辆周围环境的信息并将其表示给学习代理在文献中表现出高度的多样性。可以观察到不同水平的传感器抽象：
- en: •
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: sensor level, where camera images, lidar or radar information is passed to the
    agent;
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传感器水平，将相机图像、激光雷达或雷达信息传递给代理；
- en: •
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: intermediate level, where idealized sensor information is provided;
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 中间水平，提供理想化的传感器信息；
- en: •
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: ground truth level, where all detectable and non-detectable information is given.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 地面真实水平，提供所有可检测和不可检测的信息。
- en: The structure of the sensor model also affects the neural network structure
    of the Deep RL agent since image like, or array-like inputs infer 2D or 1D CNN
    structures, while the simple set of scalar information results in a simple dense
    network. There are cases where these two kinds of inputs are mixed. Hence the
    network needs to have two different types of input layers.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器模型的结构也影响深度强化学习代理的神经网络结构，因为图像或数组输入推断2D或1D CNN结构，而简单的标量信息集合会导致简单的全连接网络。有时这两种输入会混合，因此网络需要有两种不同类型的输入层。
- en: Image-based solutions usually use front-facing camera images extracted from
    3D simulators to represent the observation space. The data is structured in a
    ($C$ x $W$ x $H$) sized matrix, where $C$ is the number of channels, usually one
    for intensity images and three for RGB, while $W$ and $H$ are the width and height
    resolution of the image. In some cases, for the detection of movement, multiple
    images are fed to the network in parallel. Sometimes it is convenient to down-sample
    the images - like ($1$x$48$x$27$) in [[54](#bib.bib54)] or ($3$x$84$x$84$) in
    [[55](#bib.bib55), [56](#bib.bib56)] - for data and network compression purposes.
    Since images hold the information in an unstructured manner, i.e., the state information,
    such as object positions, or lane information are deeply encoded in the data,
    deep neural networks, such as CNN, usually need large samples and time to converge
    [[57](#bib.bib57)]. This problem escalates, with the high number of steps that
    the RL process requires, resulting in a lengthy learning process, like $1.5M$
    steps in [[54](#bib.bib54)] or $100M$ steps in [[55](#bib.bib55)].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图像的解决方案通常使用从3D模拟器提取的前视摄像头图像来表示观察空间。这些数据结构化为（$C$ x $W$ x $H$）大小的矩阵，其中$C$是通道数，通常一个用于强度图像，三个用于RGB，而$W$和$H$是图像的宽度和高度分辨率。在某些情况下，为了检测运动，将多个图像并行输入到网络中。有时，为了数据和网络压缩的目的，图像会被降采样——如[[54](#bib.bib54)]中的（$1$x$48$x$27$）或[[55](#bib.bib55),
    [56](#bib.bib56)]中的（$3$x$84$x$84$）。由于图像以非结构化的方式存储信息，即状态信息，如物体位置或车道信息，深度神经网络（如CNN）通常需要大量样本和时间才能收敛[[57](#bib.bib57)]。由于RL过程需要的步骤数量很高，这个问题会加剧，导致学习过程非常漫长，比如[[54](#bib.bib54)]中的$1.5M$步或[[55](#bib.bib55)]中的$100M$步。
- en: Many image-based solutions propose some kind of preprocessing of the data to
    overcome this issue. In [[57](#bib.bib57)], the authors propose a framework for
    vision-based lateral control, which combines DL and RL methods. To improve the
    perception accuracy, an MTL (Multitask learning) CNN model is proposed to learn
    the critical track features, which are used to locate the vehicle in the track
    coordinate, and trains a policy gradient RL controller to solve the continuous
    sequential decision-making problem. Naturally, this approach can also be viewed
    as an RL solution with structured features, though the combined approach has its
    place in the image-based solutions also.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 许多基于图像的解决方案提出了一些数据预处理方法来克服这个问题。在[[57](#bib.bib57)]中，作者提出了一种基于视觉的横向控制框架，该框架结合了DL和RL方法。为了提高感知准确性，提出了一种MTL（多任务学习）CNN模型来学习关键轨道特征，这些特征用于在轨道坐标中定位车辆，并训练一个策略梯度RL控制器来解决连续序列决策问题。自然地，这种方法也可以视为一种具有结构化特征的RL解决方案，尽管这种结合的方法在基于图像的解决方案中也占有一席之地。
- en: 'Another approach could be the simplification of the unstructured data. In [[58](#bib.bib58)]
    Kotyan et al. uses the difference image as the background subtraction between
    the two consecutive frames as an input, assuming this image contains the motion
    of the foreground and the underlying neural network would focus more on the features
    of the foreground than the background. By using the same training algorithm, their
    results showed that the including difference image instead of the original unprocessed
    input needs approximately 10 times less training steps to achieve the same performance.
    The second possibility is, instead of using the original image as an input, it
    can be driven through an image semantic segmentation network as proposed in [[59](#bib.bib59)].
    As the authors state: ”Semantic image contains less information compared to the
    original image, but includes most information needed by the agent to take actions.
    In other words, semantic image neglects useless information in the original image.”
    Another advantage of this approach is that the trained agent can use the segmented
    output of images obtained from real-world scenarios, since on this level, the
    difference is much smaller between the simulated and real-world data than in the
    case of the simulated and real-world images. Fig. [5](#S2.F5 "Figure 5 ‣ II-E2
    Environment observation ‣ II-E Observation Space ‣ II Modeling for Reinforcement
    Learning ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous
    Vehicles") shows the $640x400$ resolution inputs used in this research.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法可能是简化非结构化数据。在[[58](#bib.bib58)]中，Kotyan等人使用差异图像作为输入，这种图像是两个连续帧之间的背景减法，假设该图像包含前景的运动，底层神经网络会更关注前景的特征而非背景。通过使用相同的训练算法，他们的结果显示，包括差异图像而不是原始未经处理的输入，所需的训练步骤大约少10倍以达到相同的性能。第二种可能性是，将原始图像作为输入，可以通过图像语义分割网络进行处理，如[[59](#bib.bib59)]中提出的那样。正如作者所说：“语义图像相比原始图像包含的信息更少，但包括了代理采取行动所需的大部分信息。换句话说，语义图像忽略了原始图像中的无用信息。”这种方法的另一个优点是，训练后的代理可以使用来自实际场景的分割输出，因为在这一层面，模拟数据与实际数据之间的差异要比模拟图像与实际图像之间的差异小得多。图[5](#S2.F5
    "Figure 5 ‣ II-E2 Environment observation ‣ II-E Observation Space ‣ II Modeling
    for Reinforcement Learning ‣ Survey of Deep Reinforcement Learning for Motion
    Planning of Autonomous Vehicles")展示了本研究中使用的$640x400$分辨率输入。
- en: '![Refer to caption](img/409ea263894f1b31442880c9b81edd1c.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/409ea263894f1b31442880c9b81edd1c.png)'
- en: 'Figure 5: Real images from the driving data and their semantic segmentations
    (source:[[59](#bib.bib59)])'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：来自驾驶数据的真实图像及其语义分割（来源:[[59](#bib.bib59)]）
- en: 2D or 3D Lidar like sensor models are not common among the recent studies, though
    they could provide excellent depth-map like information about the environment.
    Though the same problem arises as with the camera images, that the provided data
    - let them be a vector for 2D, and a matrix for 3D Lidars - is unstructured. The
    usage of this type of input only can be found in [[60](#bib.bib60)], where the
    observation emulates a 2D Lidar that provides the distance from obstacles in $31$
    directions within the field-of-view of $150^{\circ}$, and agent uses sensor data
    as its state. A similar input structure, though not modeling a Lidar, since there
    is no reflection, which is provided by TORCS and used in [[20](#bib.bib20)], is
    to represent the lane markings with imagined beam sensors. The agent in the cited
    example uses readings from 19 sensors with a 200m range, presenting at every $10^{\circ}$
    on the front half of the car returning distance to the track edge.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究中，像2D或3D激光雷达这样的传感器模型并不常见，尽管它们可以提供关于环境的优质深度图信息。不过，与相机图像一样，数据的问题依然存在——无论是2D的向量，还是3D激光雷达的矩阵——这些数据都是非结构化的。此类输入的使用仅能在[[60](#bib.bib60)]中找到，其中观察模拟了一个2D激光雷达，提供了$31$个方向的障碍物距离，视场角为$150^{\circ}$，而代理使用传感器数据作为状态。类似的输入结构，尽管没有模拟激光雷达，因为没有反射，由TORCS提供并用于[[20](#bib.bib20)]，是通过想象的光束传感器表示车道标记。在引用的例子中，代理使用来自19个传感器的读数，范围为200米，在车前半部分每$10^{\circ}$处显示到赛道边缘的距离。
- en: 'Grid-based path planning methods, like the A* or various SLAM (Simultaneous
    Localization and Mapping) algorithms exist and are used widespread in the area
    of mobile robot navigation, where the environment is represented as a spatial
    map [[61](#bib.bib61)], usually formulated as a 2D matrix assigning to each 2D
    location in a surface grid one of three possible values: Occupied, free, and unknown
    [[62](#bib.bib62)]. This approach can also be used representing probabilistic
    maneuvers of surrounding vehicles [[63](#bib.bib63)], or by generating spatiotemporal
    map from a predicted sequence of movements, motion planning in a dynamic environment
    can also be achieved [[64](#bib.bib64)]. Though the previously cited examples
    didn’t use RL techniques, they prove that grid representation holds high potential
    in this field. Navigation in a static environment by using a grid map as the observation,
    together with position and yaw of the vehicle with an RL agent, is presented in
    [[65](#bib.bib65)] (See Fig.[6](#S2.F6 "Figure 6 ‣ II-E2 Environment observation
    ‣ II-E Observation Space ‣ II Modeling for Reinforcement Learning ‣ Survey of
    Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles")). Grid
    maps are also unstructured data, and their complexity is similar to the semantically
    segmented images, since the cells store class information in both cases, and hence
    their optimal handling is using the CNN architecture.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基于网格的路径规划方法，比如 A* 或各种SLAM（同时定位和映射）算法存在并且广泛应用于移动机器人导航领域，其中环境被表示为一个空间地图[[61](#bib.bib61)]，通常被表述为分配给表面网格中每个2D位置的2D矩阵的三种可能值之一：占用、空闲和未知[[62](#bib.bib62)]。这种方法也可以用来表示周围车辆的概率操纵[[63](#bib.bib63)]，或通过从预测的运动序列生成时空地图，实现动态环境中的运动规划[[64](#bib.bib64)]。虽然先前引用的例子没有使用RL技术，但它们证明了网格表示在这一领域具有很高的潜力。在[[65](#bib.bib65)]中呈现了通过使用网格地图作为观察、以及与RL代理一起使用车辆的位置和偏航角来进行静态环境中的导航（见图[6](#S2.F6
    "图 6 ‣ II-E2 环境观察 ‣ II-E 观察空间 ‣ II 自动驾驶强化学习建模 ‣ 自动驾驶汽车运动规划的深度强化学习调查")）。网格地图也是非结构化数据，以及它们的复杂性类似于语义分割图像，因为单元在两种情况下都存储类信息，因此它们的最佳处理方法是使用CNN架构。
- en: '![Refer to caption](img/8d81b8a981d60ffe1e6329633f9deb35.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/8d81b8a981d60ffe1e6329633f9deb35.png)'
- en: (a) Sensors
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 传感器
- en: '![Refer to caption](img/ce2f2e19c8cd3a8c7e7385a8e9048811.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/ce2f2e19c8cd3a8c7e7385a8e9048811.png)'
- en: (b) Target state $z^{t}$
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 目标状态 $z^{t}$
- en: '![Refer to caption](img/b18aa432c411ca891396f706c74ad83a.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/b18aa432c411ca891396f706c74ad83a.png)'
- en: (c) Perception Ø
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 感知 Ø
- en: 'Figure 6: The surrounding from the perspective of the vehicle can be described
    by a coarse perception map where the target is represented by a red dot (c) (source:
    [[65](#bib.bib65)])'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：从车辆角度描述的周围可以通过粗略的感知地图来表示，其中目标由一个红点表示（c）（来源：[[65](#bib.bib65)]）
- en: Representing moving objects, i.e. surrounding vehicles in a grid needs not only
    occupancy, but other information hence the spatial grid’s cell need to hold additional
    information. In [[44](#bib.bib44)] the authors used equidistant grid, where the
    ego-vehicle is placed in the center, and the cells occupied by other vehicles
    represented the longitudinal velocity of the corresponding car (See Fig. [7](#S2.F7
    "Figure 7 ‣ II-E2 Environment observation ‣ II-E Observation Space ‣ II Modeling
    for Reinforcement Learning ‣ Survey of Deep Reinforcement Learning for Motion
    Planning of Autonomous Vehicles")). The same approach can also be found in [[49](#bib.bib49)].
    Naturally this simple representation can not provide information about the lateral
    movement of the other traffic participants, though they give significantly more
    than the simple occupancy based ones.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 代表移动物体，即网格中的周围车辆，不仅需要占用情况，还需要其他信息，因此空间网格的单元需要保存额外的信息。在[[44](#bib.bib44)]中，作者使用等距网格，其中自动驾驶汽车放置在中心，其他车辆占用的单元表示相应车辆的纵向速度（见图[7](#S2.F7
    "图 7 ‣ II-E2 环境观察 ‣ II-E 观察空间 ‣ II 自动驾驶强化学习建模 ‣ 自动驾驶汽车运动规划的深度强化学习调查")）。相同的方法也可以在[[49](#bib.bib49)]中找到。显然，这种简单的表示无法提供有关其他交通参与者的横向移动的信息，尽管它们比简单的占用基础表示提供了更多的信息。
- en: '![Refer to caption](img/04a0c189530181f944bde9d2628a21f4.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/04a0c189530181f944bde9d2628a21f4.png)'
- en: (a) Mathematical model for the traffic
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 交通的数学模型
- en: '![Refer to caption](img/2cb54680021a0df4abe8756d0a847860.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2cb54680021a0df4abe8756d0a847860.png)'
- en: (b) Visualization of the Hyper Grid Matrix
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 超网格矩阵的可视化
- en: 'Figure 7: The visualization of the HDM mapping process (source:[[44](#bib.bib44)])'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：HDM映射过程的可视化（来源：[44](#bib.bib44)）
- en: Equidistant grids are a logical choice for generic environments, where the moving
    directions of the mobile robot are free, though, in the case of road vehicles,
    the vehicle mainly follows the direction of the traffic flow. In this case, the
    spatial representation could be chosen fixed to the road topology, namely the
    lanes of the road, regardless of its curvature or width. In these lane-based grid
    solutions, the grid representing the highway has as many rows as the actual lane
    count, and the lanes are discretized longitudinally. The simplest utilization
    of this approach can be found in [[26](#bib.bib26)], where the length of the cells
    is equivalent to the unit vehicle length, and also, the behavior of the traffic
    acts similar to the classic cellular automata-based microscopic models [[66](#bib.bib66)].
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 等距网格对于通用环境而言是一个合乎逻辑的选择，在这种环境中，移动机器人的运动方向是自由的。然而，在道路车辆的情况下，车辆主要遵循交通流向。在这种情况下，空间表示可以选择固定为道路拓扑，即道路车道，而不管其曲率或宽度。在这些基于车道的网格解决方案中，表示高速公路的网格行数与实际车道数相同，车道在纵向上被离散化。这种方法的最简单应用可以在[26](#bib.bib26)中找到，其中单元格的长度等于单个车辆长度，而且交通行为类似于经典的基于细胞自动机的微观模型[66](#bib.bib66)。
- en: This representation, similarly to the equidistant ones, can be used for occupancy,
    though they still do not hold any information on vehicle dynamics. [[67](#bib.bib67)]
    is to fed multiple consecutive traffic snapshots into the underlying CNN structure,
    which inherently extracts the velocity of the moving objects. Representing speed
    in grid cells is also possible in this setup, for that example can be found in
    [[36](#bib.bib36)], where the authors converted the traffic extracted from the
    Udacity simulator to the lane-based grid.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示方式，类似于等距网格，也可以用于道路占用，尽管它们仍然不包含任何车辆动态信息。[67](#bib.bib67)旨在将多个连续的交通快照输入到底层CNN结构中，这本质上提取了移动物体的速度。在这种设置中，也可以在网格单元中表示速度，相关示例见[36](#bib.bib36)，其中作者将从Udacity模拟器中提取的交通数据转换为基于车道的网格。
- en: Besides the position and the longitudinal speed of the surrounding vehicles
    are essential from the aspect of the decision making, other features (such as
    heading, acceleration, lateral speed) should be considered. Multi-layer grid maps
    could be used for each vital parameter to overcome this issue. In [[10](#bib.bib10)]
    the authors processed the simulator state to calculate an observation tensor of
    size 4 x 3 x (2 x FoV + 1), where Fov stands for Field of View and represents
    the maximum distance of the observation in cell count. There is one channel (first
    dimension) each for on-road occupancy, relative velocities of vehicles, relative
    lateral displacements, and relative headings to the ego-vehicle. Fig.[8](#S2.F8
    "Figure 8 ‣ II-E2 Environment observation ‣ II-E Observation Space ‣ II Modeling
    for Reinforcement Learning ‣ Survey of Deep Reinforcement Learning for Motion
    Planning of Autonomous Vehicles") shows an example of the simulator state and
    corresponding input observation used for their network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除了周围车辆的位置和纵向速度在决策制定方面至关重要之外，其他特征（如航向、加速度、横向速度）也应考虑。可以为每个重要参数使用多层网格地图来解决此问题。在[10](#bib.bib10)中，作者处理了模拟器状态，以计算一个大小为4
    x 3 x (2 x FoV + 1)的观察张量，其中Fov代表视场，表示观察的最大距离（以单元格计数）。每个通道（第一个维度）分别用于道路占用、车辆的相对速度、相对横向位移和相对航向到自车。图[8](#S2.F8
    "图8 ‣ II-E2 环境观察 ‣ II-E 观察空间 ‣ II 强化学习建模 ‣ 自主驾驶车辆运动规划的深度强化学习调查")展示了模拟器状态和用于其网络的相应输入观察示例。
- en: '![Refer to caption](img/20a70183bb097cc979845e5fd80d4d2f.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/20a70183bb097cc979845e5fd80d4d2f.png)'
- en: 'Figure 8: The simulator state (top, zoomed in) gets converted to a 4 x 3 x
    (2 x FoV + 1) input observation tensor (bottom) (source:[[10](#bib.bib10)])'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：模拟器状态（顶部，放大）被转换为4 x 3 x (2 x FoV + 1)的输入观察张量（底部）（来源：[10](#bib.bib10)）
- en: 'The previous observation models (image, lidar, or grid-based) all have some
    common properties: All of them are unstructured datasets, need a CNN architecture
    to process, which hardens the learning process since the agent simultaneously
    needs to extract the exciting features and form the policy for action. It would
    be obvious to pre-process the unstructured data and feed structured information
    to the agents’ network. Structured data refers to any data that resides in a fixed
    field within a record or file. As an example, for navigating in traffic, based
    on the task, the parameters of the surrounding vehicles are represented on the
    same element of the input. In the simplest scenario of car following, the agent
    only focuses on the leading vehicle, and the input beside the state of the ego
    vehicle consists of $(d,v)$ as in [[51](#bib.bib51)] or $(d,v,a)$ as in [[68](#bib.bib68)],
    where these parameters are the headway distance, speed, and acceleration of the
    leading vehicle. Contrary to the unstructured data, these approaches significantly
    reduce the amount of the input and can be handled with simple DNN structures,
    which profoundly affects the convergence of the agent’s performance. For navigating
    in traffic, i.e., performing merging or lane changing maneuvers, not only the
    leading vehicle’s, but the other surrounding vehicles’ states also need to be
    considered. In a merging scenario, the most crucial information is the relative
    longitudinal position and speed $2$x$(dx,dv)$ of the two vehicles bounding the
    target gap, as used by [[69](#bib.bib69)]. Naturally, this is the absolute minimal
    representation of such a problem, but in the future, more sophisticated representations
    would be developed. In highway maneuvering situations, both ego-lane, and neighboring
    lane vehicles need to be considered, in [[41](#bib.bib41)] the authors used the
    above mentioned $6$x$(dx,dv)$ scalar vector is used for the front and rear vehicles
    in the three interesting lanes. While in [[70](#bib.bib70)] the authors extended
    this information with the occupancy of the neighboring lanes right at the side
    of the ego-vehicle (See Fig. [9](#S2.F9 "Figure 9 ‣ II-E2 Environment observation
    ‣ II-E Observation Space ‣ II Modeling for Reinforcement Learning ‣ Survey of
    Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles")). The
    same approach can be seen in [[42](#bib.bib42)], though extending the number of
    traced objects to nine. These researches lack lateral information, though, in
    [[41](#bib.bib41)], the lateral positions and speeds are also involved in the
    input vector resulting in a $6$x$(dx,dy,dvx,dvy)$ structure, logically representing
    longitudinal and lateral distance, and speed differences to the ego, respectively.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的观测模型（图像、激光雷达或基于网格的）都有一些共同的特点：它们都是非结构化数据集，需要使用 CNN 架构来处理，这使得学习过程变得困难，因为代理需要同时提取重要特征并形成行动策略。预处理非结构化数据并将结构化信息输入到代理的网络中是显而易见的。结构化数据是指任何在记录或文件中的固定字段内存在的数据。例如，在交通导航中，根据任务，周围车辆的参数在输入的同一元素上表示。在最简单的跟车场景中，代理只关注前方车辆，除了自车状态，输入由
    $(d,v)$ 组成，如 [[51](#bib.bib51)] 或 $(d,v,a)$ 如 [[68](#bib.bib68)]，其中这些参数是前车的车距、速度和加速度。与非结构化数据相反，这些方法显著减少了输入量，并可以使用简单的
    DNN 结构处理，这对代理的性能收敛产生了深远影响。对于交通导航，即执行合并或车道变换操作，不仅需要考虑前车，还需要考虑其他周围车辆的状态。在合并场景中，最关键的信息是界定目标间隙的两辆车的相对纵向位置和速度
    $2$x$(dx,dv)$，如 [[69](#bib.bib69)] 使用的。自然，这是此类问题的绝对最小表示，但未来将会开发出更复杂的表示。在高速公路操控情况下，需要考虑自车车道和邻近车道车辆，在
    [[41](#bib.bib41)] 中，作者使用了上述提到的 $6$x$(dx,dv)$ 标量向量，用于三个有趣车道的前车和后车。而在 [[70](#bib.bib70)]
    中，作者通过扩展邻近车道的占用信息来扩展这些信息，正好位于自车旁边（见图 [9](#S2.F9 "图 9 ‣ II-E2 环境观察 ‣ II-E 观察空间
    ‣ II 强化学习建模 ‣ 自动驾驶车辆运动规划深度强化学习综述")）。在 [[42](#bib.bib42)] 中也可以看到类似的方法，尽管将追踪的对象数量扩展到了九个。这些研究缺乏横向信息，但在
    [[41](#bib.bib41)] 中，横向位置和速度也被包含在输入向量中，形成了 $6$x$(dx,dy,dvx,dvy)$ 结构，逻辑上表示自车的纵向和横向距离及速度差异。
- en: '![Refer to caption](img/238c105f162792b94278d33271d39757.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/238c105f162792b94278d33271d39757.png)'
- en: 'Figure 9: Environment state on the highway [[70](#bib.bib70)]'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：高速公路上的环境状态 [[70](#bib.bib70)]
- en: In a special case of handling unsignalized intersection [[71](#bib.bib71)] the
    authors also used this formulation scheme where the other vehicle’s Cartesian
    coordinates, speed and heading were considered.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理无信号交叉口的特殊情况下 [[71](#bib.bib71)]，作者也使用了这种表示方案，其中考虑了其他车辆的笛卡尔坐标、速度和航向。
- en: III Scenario-based Classification of the Approaches
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 基于场景的方法分类
- en: Though this survey focuses on Deep Reinforcement Learning based motion planning
    research, it is essential to mention that some papers try to solve some subtasks
    of automated driving through classic reinforcement techniques. One problem of
    these classic methods, that they can not handle unstructured data, such as images,
    mid-level radar, or lidar sensing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项调查重点关注基于深度强化学习的运动规划研究，但必须提到一些论文尝试通过经典强化技术解决自动驾驶的某些子任务。这些经典方法的问题在于它们无法处理非结构化数据，如图像、中级雷达或激光雷达传感器数据。
- en: The other problem comes from the need of maintaining the Q-table for all $(\mathcal{S},\mathcal{A})$
    state-action pairs. This results in space complexity explosion, since the size
    of the table equals the product of the size of all classes both in state and action.
    As an example, the Q-learning made in [[72](#bib.bib72)] is presented. The authors
    trained an agent in TORCS, which tries to achieve a policy for the best overtaking
    maneuver, by taking advantage of the aerodynamic drag. There are only two participants
    in the scenario, the overtaking vehicle, and the vehicle in front on a long straight
    track. The state representation contains the longitudinal and lateral distance
    of the two vehicles and also the the lateral position of the ego-vehicle and the
    speed difference of the two.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是需要维护所有 $(\mathcal{S},\mathcal{A})$ 状态-动作对的 Q 表。这会导致空间复杂度的爆炸，因为表的大小等于状态和动作中所有类别大小的乘积。例如，[[72](#bib.bib72)]
    中展示了 Q 学习的一个实例。作者在 TORCS 中训练了一个智能体，尝试通过利用空气动力学阻力来实现最佳超车策略。在场景中只有两个参与者，即超车车辆和长直道上的前车。状态表示包括两辆车的纵向和横向距离，以及自车的横向位置和两者之间的速度差。
- en: 'TABLE I: State representation discretization in [[72](#bib.bib72)]'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 状态表示离散化见 [[72](#bib.bib72)]'
- en: '| Name | Size | Class bounds |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 尺寸 | 类别范围 |'
- en: '| --- | --- | --- |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $dist_{y}[m]$ | 6 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| $dist_{y}[m]$ | 6 |'
- en: '&#124; {0, 10, 20 ,30, 50, 100, 200} &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; {0, 10, 20 ,30, 50, 100, 200} &#124;'
- en: '|'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $dist_{x}[m]$ | 10 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| $dist_{x}[m]$ | 10 |'
- en: '&#124; {-25, -15, -5, -3 , -1, 0, 1, 3, 5, 15, 25} &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; {-25, -15, -5, -3 , -1, 0, 1, 3, 5, 15, 25} &#124;'
- en: '|'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $pos[m]$ | 8 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| $pos[m]$ | 8 |'
- en: '&#124; {-10, -5, -2, -1, 0, 1, 2, 5, 10} &#124;'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; {-10, -5, -2, -1, 0, 1, 2, 5, 10} &#124;'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| $\Delta speed[km/h]$ | 9 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| $\Delta speed[km/h]$ | 9 |'
- en: '&#124; {-300, 0, 30, 60, 90, 120, &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; {-300, 0, 30, 60, 90, 120, &#124;'
- en: '&#124; 150, 200, 250, 300} &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 150, 200, 250, 300} &#124;'
- en: '|'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The authors discretized this state space to classes of size $(6,10,8,9)$ respectfully
    (see table [I](#S3.T1 "TABLE I ‣ III Scenario-based Classification of the Approaches
    ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles"));
    and used the minimal lateral action set size of 3, where the actions are sweeping
    $1m$ to the left or right and maintaining lateral position. Together, this problem
    generates a Q-table with $6*10*8*9*3=12960$ elements. Though a table of this size
    can be easily handled nowadays, it is easy to imagine that with more complex problems
    with more vehicles, more sensors, complex dynamics, denser state and action representation,
    the table can grow to enormous size. A possible reduction is the utilization of
    the Multiple-Goal Reinforcement Learning Method and dividing the overall problem
    to sub-tasks, as can be seen in [[73](#bib.bib73)] for overtaking maneuver. In
    a latter research, the authors widened the problem and separated the driving problem
    to the tasks of collision avoidance, target seeking, lane following, Lane choice,
    speed keeping, and steady steering [[74](#bib.bib74)]. To reduce problem size,
    the authors of [[75](#bib.bib75)] used strategic-level decisions to set movement
    targets for the vehicles concerning the surrounding ones, and left the low-level
    control to classic solutions, which significantly reduced the action space.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 作者将这个状态空间离散化为 $(6,10,8,9)$ 这样大小的类别（见表 [I](#S3.T1 "TABLE I ‣ III Scenario-based
    Classification of the Approaches ‣ Survey of Deep Reinforcement Learning for Motion
    Planning of Autonomous Vehicles)")）；并使用了最小的横向动作集合大小为 3，其中动作是向左或向右移动 $1m$ 和保持横向位置。这样，这个问题生成了一个
    $6*10*8*9*3=12960$ 元素的 Q 表。虽然如今这样大小的表可以很容易处理，但很容易想象，随着问题的复杂性增加，比如更多的车辆、更多的传感器、复杂的动态、密集的状态和动作表示，表的规模可能会增长到巨大的程度。一种可能的减少方法是利用多目标强化学习方法，将总体问题分解为子任务，如
    [[73](#bib.bib73)] 中的超车机动所示。在后续研究中，作者扩大了问题范围，将驾驶问题分解为碰撞避免、目标寻找、车道跟随、车道选择、速度保持和稳定转向等任务
    [[74](#bib.bib74)]。为了减少问题规模，[[75](#bib.bib75)] 的作者使用了战略级决策为车辆设置移动目标，相对于周围的车辆，将低级控制留给经典解决方案，从而显著减少了动作空间。
- en: An other interesting example of classic Q-learning is described in [[76](#bib.bib76)]
    where the authors designed an agent for the path planning problem of a ground
    vehicle considering obstacles with Ackermann steering by using $(v,x,y,\theta)$
    (speed, positions and heading) as state representation, and used reinforcement
    learning as an optimizer (See Fig. [10](#S3.F10 "Figure 10 ‣ III Scenario-based
    Classification of the Approaches ‣ Survey of Deep Reinforcement Learning for Motion
    Planning of Autonomous Vehicles")).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[76](#bib.bib76)] 中描述了另一个有趣的经典 Q-learning 示例，作者设计了一个用于路径规划问题的代理，考虑了具有 Ackermann
    转向的地面车辆障碍物，使用 $(v,x,y,\theta)$（速度、位置和航向）作为状态表示，并使用强化学习作为优化器（见图 [10](#S3.F10 "Figure
    10 ‣ III Scenario-based Classification of the Approaches ‣ Survey of Deep Reinforcement
    Learning for Motion Planning of Autonomous Vehicles)")）。
- en: '![Refer to caption](img/e317ddc5a5c485c8a4a06c1a3538fa6b.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e317ddc5a5c485c8a4a06c1a3538fa6b.png)'
- en: 'Figure 10: Path planning results from [[76](#bib.bib76)]'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：路径规划结果来自于 [[76](#bib.bib76)]
- en: Though one would expect that machine learning could give an overall end-to-end
    solution to automated driving, the study of the recent literature shows that Reinforcement
    Learning research can give answers to certain sub-tasks of this problem. The papers
    in recent years can be organized around these problems, where a well-dedicated
    situation or scenario is chosen and examined whether a self-learning agent can
    solve it. These problem statements vary in complexity. As mentioned earlier, the
    complexity of reinforcement learning, and thus training time, is greatly influenced
    by the complexity of the problem chosen, the nature of the action space, and the
    timeliness and proper formulation of rewards. The simplest problems, such as lane-keeping
    or vehicle following, can generally be traced back to simple convex optimization
    or control problems. However, in these cases, the formulation of secondary control
    goals, such as passenger comfort, is more comfortable to articulate. At the other
    end of the imagined complexity scale, there are problems, like in the case of
    maneuvering in dense traffic, the efficient fulfillment of the task is hard to
    formulate, and the agent needs predictive ”thinking” to achieve its goals. In
    the following, these approaches are presented.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人们通常期望机器学习能够为自动驾驶提供一个整体的端到端解决方案，但近期文献的研究表明，强化学习研究可以为这一问题的某些子任务提供答案。近年来的论文可以围绕这些问题进行组织，其中选择一个专门的情境或场景，并检查自学习代理是否能够解决它。这些问题陈述的复杂性各不相同。如前所述，强化学习的复杂性以及训练时间在很大程度上受到所选问题的复杂性、行动空间的性质以及奖励的时效性和正确制定的影响。最简单的问题，如车道保持或车辆跟随，通常可以追溯到简单的凸优化或控制问题。然而，在这些情况下，诸如乘客舒适度等次级控制目标的制定更加容易。在设想的复杂性尺度的另一端，例如在密集交通中机动的问题，高效完成任务的定义很难制定，代理需要具有预测性的“思维”以实现其目标。以下将介绍这些方法。
- en: III-A Car following
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 车辆跟随
- en: 'Car following is the simplest task in this survey, where the problem is formulated
    as follows: There are two participants of the simulation, a leading and the following
    vehicle, both keeping their lateral positions in a lane, and the following vehicle
    adjusts its longitudinal speed to keep a safe following distance. The observation
    space consists of the $(v,dv,ds)$ tuple, representing agent speed, speed difference
    to the lead, and headway distance. The action is the acceleration command. Reward
    systems use the collision of the two vehicles as a failure naturally, while the
    performance of the agent is based on the jerk, TTC (time to collision) [[50](#bib.bib50)],
    or passenger comfort [[77](#bib.bib77)]. Another approach is shown in [[51](#bib.bib51)],
    where the performance of the car following agent is evaluated against real-world
    measurement to achieve human-like behavior.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 车跟随是本调查中最简单的任务，问题的制定如下：模拟中有两个参与者，一个是领先车辆，一个是跟随车辆，两者都保持在车道中的横向位置，跟随车辆调整其纵向速度以保持安全的跟随距离。观察空间由$(v,dv,ds)$元组组成，表示代理速度、与前车的速度差异以及车头距离。动作是加速命令。奖励系统自然地将两辆车的碰撞作为失败，而代理的表现则基于突变、TTC（碰撞时间）[[50](#bib.bib50)]或乘客舒适度[[77](#bib.bib77)]。另一种方法展示在[[51](#bib.bib51)]中，其中对车跟随代理的表现进行了与现实世界测量的比较，以实现类人行为。
- en: III-B Lane keeping
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 车道保持
- en: 'Lane-keeping or trajectory following is still a simple control task, but contrary
    to car following, this problem focuses on lateral control. The observation space
    in these studies us two different approaches: One is the ”ground truth” lateral
    position and angle of the vehicle in lane [[78](#bib.bib78), [60](#bib.bib60),
    [22](#bib.bib22)], while the second is the image of a front-facing camera [[54](#bib.bib54),
    [59](#bib.bib59), [57](#bib.bib57)]. Naturally, for image-based control, the agents
    use external simulators, TORCS, and GAZEBO/ROS in these cases. Reward systems
    almost always consider the distance from the center-line of the lane as an immediate
    reward. It is important to mention that these agents hardly consider vehicle dynamics,
    and surprisingly does not focus on joined longitudinal control.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 保持车道或跟随轨迹仍然是一个简单的控制任务，但与跟车不同的是，这个问题集中于横向控制。这些研究中的观察空间使用了两种不同的方法：一种是车辆在车道中的“真实位置”和角度[[78](#bib.bib78),
    [60](#bib.bib60), [22](#bib.bib22)]，另一种是前置摄像头的图像[[54](#bib.bib54), [59](#bib.bib59),
    [57](#bib.bib57)]。自然地，对于基于图像的控制，代理使用外部模拟器，如TORCS和GAZEBO/ROS。在这些情况下，奖励系统几乎总是考虑到距离车道中心线的距离作为即时奖励。值得提到的是，这些代理几乎不考虑车辆动力学，令人惊讶的是，它们也没有集中于联合纵向控制。
- en: III-C Merging
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 合并
- en: The ramp merge problem deals with the on-ramp highway scenario (see Fig. [11](#S3.F11
    "Figure 11 ‣ III-C Merging ‣ III Scenario-based Classification of the Approaches
    ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles")),
    where the ego vehicle needs to find the acceptable gap between two vehicles to
    get on the highway. In the simplest approach, it is eligible to learn the longitudinal
    control, where the agent reaches this position, as can be seen in [[79](#bib.bib79),
    [45](#bib.bib45), [19](#bib.bib19)]. Other papers, like [[69](#bib.bib69)] use
    full steering and acceleration control. In [[45](#bib.bib45)], the actions control
    the longitudinal movement of the vehicle accelerate and decelerate, and while
    executing these actions, the ego vehicle keeps its lane. Actions ”lane change
    left” as well as ”lane change right” imply lateral movement. Only a single action
    is executed at a time, and actions are executed in their entirety, the vehicle
    is not able to prematurely abort an action.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 坡道合并问题涉及到上坡高速公路场景（见图 [11](#S3.F11 "图 11 ‣ III-C 合并 ‣ III 基于场景的分类方法 ‣ 深度强化学习在自主车辆运动规划中的应用调查")），在这个场景中，自车需要找到两个车辆之间的合适间隙才能上高速公路。在最简单的方法中，可以学习纵向控制，即代理车辆到达该位置，如[[79](#bib.bib79),
    [45](#bib.bib45), [19](#bib.bib19)]中所示。其他论文，如[[69](#bib.bib69)]，则使用了全面的转向和加速控制。在[[45](#bib.bib45)]中，动作控制车辆的纵向运动，加速和减速，同时执行这些动作时，自车保持车道。动作“左侧变道”以及“右侧变道”意味着横向移动。每次只执行一个动作，且动作必须完全执行，车辆无法提前中止动作。
- en: '![Refer to caption](img/ad322f8a0143825bdf8e55e3ee894c95.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ad322f8a0143825bdf8e55e3ee894c95.png)'
- en: 'Figure 11: Ramp merge: (a) simulated scenario and (b) real-world location (source:
    [[69](#bib.bib69)])'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：坡道合并：（a）模拟场景和（b）现实世界位置（来源：[[69](#bib.bib69)]）
- en: 'An exciting addition can be examined in [[19](#bib.bib19)], where the surrounding
    vehicles act differently, as there are cooperative and non-cooperative drivers
    among them. They trained their agents with the knowledge about cooperative behavior,
    and also compared the results with three differently built MTCS planners. Full
    information MCTS naturally outperforms RL, though they are computationally expensive.
    The authors used a curriculum learning approach to train the agent by gradually
    increasing traffic density. As they stated: ”When training an RL agent in dense
    traffic directly, the policy converged to a suboptimal solution which is to stay
    still in the merge lane and does not leverage the cooperativeness of other drivers.
    Such a policy avoids collisions but fails at achieving the maneuver.”'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[19](#bib.bib19)]中可以看到一个有趣的补充，其中周围的车辆行为各异，因为其中有合作型和非合作型驾驶员。他们用关于合作行为的知识训练了他们的代理，并且将结果与三种不同构建的MTCS规划器进行了比较。尽管全信息MCTS自然优于强化学习，但其计算成本较高。作者使用了课程学习方法，通过逐步增加交通密度来训练代理。正如他们所述：“当在密集交通中直接训练RL代理时，策略会收敛到一个次优解，即在合并车道中保持静止，并没有利用其他驾驶员的合作性。这种策略避免了碰撞，但未能实现操控。”
- en: The most detailed description for this problem is given by [[69](#bib.bib69)],
    where ”the driving environment is trained as an LSTM architecture to incorporate
    the influence of historical and interactive driving behaviors on the action selection.
    The Deep Q-learning process takes the internal state from LSTM as the input to
    the Q-function approximator, using it for the action selection based on more past
    information. The Q-network parameters are updated with an experience replay, and
    a second target Q-network is used to relieve the problems of local optima and
    instability.” With this approach, the researchers try to mix the possibilities
    from behavior prediction and learning, simultaneously achieving better performance.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题的最详细描述见[[69](#bib.bib69)]，其中“驾驶环境被训练为一个LSTM架构，以结合历史和互动驾驶行为对行动选择的影响。深度Q学习过程将LSTM的内部状态作为输入，传递给Q函数近似器，根据更多的过去信息来进行行动选择。Q网络参数通过经验回放进行更新，并使用第二个目标Q网络来缓解局部最优和不稳定性的问题。”通过这种方法，研究人员尝试混合行为预测和学习的可能性，从而实现更好的性能。
- en: III-D Driving in traffic
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 交通中的驾驶
- en: The most complicated scenario examined in the recent papers are those where
    the autonomous agent drives in traffic. Naturally, this task is also scalable
    by the topology of the network, the amount and behavior of the surrounding vehicles,
    the application of traffic rules, and many other properties. Therefore almost
    all of the current solutions deal with highway driving, where the scenario lacks
    intersections, pedestrians, and the traffic flow in one direction in all lanes.
    Sub-tasks of this scenario were examined in the previous sections, such as lane-keeping,
    or car following. In the following, two types of highway driving will be presented.
    First, the hierarchical approaches are outlined, where the agents act on the behavioral
    layer, making decisions about lane changing or overtaking and performs these actions
    with an underlying controller using classic control approaches. Secondly, end-to-end
    solutions are presented, where the agents directly control the vehicle by steering
    and acceleration. As the problem gets more complicated, it is important to mention
    that the agents trained this would only be able to solve the type of situations
    that it is exposed to in the simulations. It is, therefore, crucial that the design
    of the simulated traffic environment covers the intended case [[52](#bib.bib52)].
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的文献中研究的最复杂场景是自主代理在交通中驾驶的情况。自然，这个任务也受到网络拓扑、周围车辆的数量和行为、交通规则的应用以及许多其他属性的影响。因此，几乎所有当前的解决方案都处理高速公路驾驶，在这种场景下缺乏交叉路口、行人，并且所有车道的交通流向单一。该场景的子任务在前面的部分已经讨论过，例如车道保持或跟车。接下来，将介绍两种类型的高速公路驾驶。首先，概述了层次化方法，其中代理在行为层上行动，做出车道变换或超车的决策，并通过经典控制方法使用底层控制器执行这些动作。其次，介绍了端到端的解决方案，其中代理通过转向和加速直接控制车辆。随着问题的复杂性增加，重要的是要提到，这些训练过的代理只能解决其在模拟中暴露的情况。因此，模拟交通环境的设计必须覆盖预期的情况[[52](#bib.bib52)]。
- en: 'Making decisions on the behavioral layer consists of at least three discrete
    actions: Keeping current lane, Change to the left, and Change to the right, as
    can be seen in [[42](#bib.bib42)]. In this paper, the authors used the ground
    truth information about the ego vehicle’s speed and lane position, and the relative
    position and speed of the eight surrounding vehicles as the observation space.
    They trained and tested the agents in three categories of observation noises:
    noise-free, mid-level noise (%5), and high-level noise (%15), and showed that
    the training environments with higher noises resulted in more robust and reliable
    performance, also outperforming the rule-based MOBIL model, by using DQN with
    a DNN of ${64,128,128,64}$ hidden layers with $tanh$ activation. In a quite similar
    environment and observation space, [[52](#bib.bib52)] used a widened set of actions
    to perform the lane changing with previous accelerations or target gap approaching,
    resulting in six different actions as can be seen in table [II](#S3.T2 "TABLE
    II ‣ III-D Driving in traffic ‣ III Scenario-based Classification of the Approaches
    ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous Vehicles").
    They also achieved the result that the DQN agent - using two convolutional and
    one dense layer - performed on par with, or better than, a reference model based
    on the IDM [[28](#bib.bib28)]. and MOBIL [[27](#bib.bib27)] model. In the other
    publication from the same author [[80](#bib.bib80)], the action space is changed
    slightly by changing the acceleration commands to increasing and decreasing the
    ACC set-points and let the underlying controller perform these actions.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在行为层做决策包括至少三个离散动作：保持当前车道、变道至左侧和变道至右侧，如在[[42](#bib.bib42)]中所示。本文中，作者使用了关于自车速度和车道位置以及八辆周围车辆的相对位置和速度的实际数据作为观察空间。他们在三种观察噪声类别中训练和测试了智能体：无噪声、中等噪声（%5）和高噪声（%15），并展示了噪声较高的训练环境在性能上更具鲁棒性和可靠性，同时使用DQN与
    ${64,128,128,64}$ 隐藏层和 $tanh$ 激活函数，超越了基于规则的MOBIL模型。在类似的环境和观察空间中，[[52](#bib.bib52)]使用了扩展的动作集来执行变道操作，包括先前的加速或目标间隙接近，形成六种不同的动作，如表[II](#S3.T2
    "TABLE II ‣ III-D Driving in traffic ‣ III Scenario-based Classification of the
    Approaches ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous
    Vehicles")所示。他们还取得了DQN智能体—使用两个卷积层和一个全连接层—的表现与基于IDM [[28](#bib.bib28)] 和 MOBIL
    [[27](#bib.bib27)] 模型相当或更好。在同一作者的另一篇出版物[[80](#bib.bib80)]中，动作空间通过将加速命令改为增加和减少ACC设定点，并让底层控制器执行这些动作，略有变化。
- en: 'TABLE II: Action space in [[52](#bib.bib52)]'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '表 II: [[52](#bib.bib52)]中的动作空间'
- en: '| $a_{1}$ | Stay in current lane, keep current speed |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| $a_{1}$ | 保持当前车道，保持当前速度 |'
- en: '| $a_{2}$ | Stay in current lane, accelerate with $-2m/s^{2}$ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| $a_{2}$ | 保持当前车道，加速至 $-2m/s^{2}$ |'
- en: '| $a_{3}$ | Stay in current lane, accelerate with $-9m/s^{2}$ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| $a_{3}$ | 保持当前车道，加速至 $-9m/s^{2}$ |'
- en: '| $a_{4}$ | Stay in current lane, accelerate with $2m/s^{2}$ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| $a_{4}$ | 保持当前车道，加速至 $2m/s^{2}$ |'
- en: '| $a_{5}$ | Change lanes to the left, keep current speed |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| $a_{5}$ | 变道至左侧，保持当前速度 |'
- en: '| $a_{6}$ | Change lanes to the right, keep current speed |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| $a_{6}$ | 变道至右侧，保持当前速度 |'
- en: In [[68](#bib.bib68)], a two-lane scenario is considered to distribute the hierarchical
    decisions further. First, a DQN makes a binary decision about ”to or not to change
    lane”, and afterward, the other Q network is responsible for the longitudinal
    acceleration, based on the previous decision. Hence the second layer, integrated
    with classic control modules (e.g., Pure Pursuit Control), outputs appropriate
    control actions for adjusting its position. In [[47](#bib.bib47)], the above mentioned
    two-lane scenario is considered, though the authors used an actor-critic like
    learning agent.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[68](#bib.bib68)]中，考虑了一个双车道场景以进一步分配层次决策。首先，DQN 对“是否变道”做出二元决策，然后，另一个Q网络负责纵向加速，基于之前的决策。因此，第二层与经典控制模块（如纯追踪控制）集成，输出适当的控制动作以调整其位置。在[[47](#bib.bib47)]中，考虑了上述双车道场景，尽管作者使用了类似演员-评论员的学习智能体。
- en: An interesting question in automated driving is the cooperative behavior of
    the trained agent. In [[67](#bib.bib67)] the authors considered a three-lane highway
    with a lane-based grid representation as observation space and a simple tuple
    of four for action space left, right, speedup, none, and used the reward function
    to achieve cooperative and non-cooperative behaviors. Not only the classic performance
    indicators of the ego vehicle is considered in the reward function, but also the
    speed of the surrounding traffic, which is naturally affected by the behavior
    of the agent. The underlying network uses two convolutional layers with 16 filters
    of patch size (2,2) and RELU activation, and two dense layers with 500 neurons
    each. To evaluate the effects of the cooperative behavior, the authors collected
    traffic data by virtual loops in the simulation and visualized the performance
    of the resulting traffic in the classic flow-density diagram (see Fig. [12](#S3.F12
    "Figure 12 ‣ III-D Driving in traffic ‣ III Scenario-based Classification of the
    Approaches ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous
    Vehicles").) It is shown that the cooperative behavior results in higher traffic
    flow, hence better highway capacity and lower overall travel time.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶中的一个有趣问题是训练代理的合作行为。在[[67](#bib.bib67)]中，作者考虑了一个三车道高速公路，使用基于车道的网格表示作为观察空间，简单的四元组用于行动空间：左、右、加速、无，并使用奖励函数实现合作和非合作行为。奖励函数不仅考虑了自车的经典性能指标，还考虑了周围交通的速度，这自然受到代理行为的影响。基础网络使用了两个卷积层，每个层有16个滤波器，卷积核大小为（2,2），激活函数为RELU，以及两个各有500个神经元的全连接层。为了评估合作行为的效果，作者通过虚拟环路在模拟中收集了交通数据，并在经典的流量-密度图中可视化了结果交通的表现（见图[12](#S3.F12
    "Figure 12 ‣ III-D Driving in traffic ‣ III Scenario-based Classification of the
    Approaches ‣ Survey of Deep Reinforcement Learning for Motion Planning of Autonomous
    Vehicles")）。结果显示，合作行为导致了更高的交通流量，从而提高了高速公路的通行能力，减少了总体旅行时间。
- en: '![Refer to caption](img/5b50596e55506704b851c69303f5f7d1.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5b50596e55506704b851c69303f5f7d1.png)'
- en: 'Figure 12: Flow-density relations detected by the virtual loops under different
    strategies (source:[[67](#bib.bib67)])'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：不同策略下虚拟环路检测到的流量-密度关系（来源:[[67](#bib.bib67)]）
- en: The realism of the models could still differentiate end-to-end solutions. For
    example, in [[44](#bib.bib44)], instead of using the nonholonomic Ackermann steering
    geometry, the authors use a holonomic robot model for the action space, which
    highly reduces the complexity of the control problem. Their actions are Acceleration,
    Deceleration, Change lane to the left, Change lane to the right, and Take no action,
    where the first two apply maximal acceleration and deceleration, while the two
    lane-changing actions simply use constant speed lateral movements. They use Dueling
    DQN and prioritized experience replay with a grid-based observation model. A similar
    control method and nonholonomic kinematics is used in [[41](#bib.bib41)]. The
    importance of this research is that it considers safety aspects during the learning
    process. By using an MPC like safety check, the agent avoids actions that lead
    to a collision, which makes the training faster and more robust.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的现实性仍然可以区分端到端的解决方案。例如，在[[44](#bib.bib44)]中，作者使用了一个全约束机器人模型作为行动空间，而不是使用非完整的Ackermann转向几何，这大大减少了控制问题的复杂性。他们的行动包括加速、减速、向左换道、向右换道和不采取行动，其中前两者应用最大加速和减速，而两个换道动作则简单地使用恒速横向移动。他们使用了对抗DQN和优先经验回放与基于网格的观察模型。在[[41](#bib.bib41)]中使用了类似的控制方法和非完整运动学。这项研究的重要性在于它在学习过程中考虑了安全性方面。通过使用类似MPC的安全检查，代理避免了导致碰撞的动作，从而使训练更快、更稳健。
- en: Using nonholonomic kinematics needs acceleration and steering commands. In [[70](#bib.bib70),
    [46](#bib.bib46)], the authors used a continuous observation space of the structured
    information of the surrounding vehicles and Policy-gradient RL structure to achieve
    end-to-end driving. Since the utilized method has discrete action-space, the steering
    and acceleration command needed to be quantized. The complexity of driving in
    traffic with an end-to-end solution can be well examined by the number of training
    episodes needed by the agent. While in simple lane-keeping scenarios, the agents
    finished the task in few hundreds of episodes, the agent used for these problems
    needed 300’000.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非完整运动学需要加速和转向命令。在[[70](#bib.bib70)]和[[46](#bib.bib46)]中，作者使用了周围车辆结构化信息的连续观测空间和策略梯度RL结构来实现端到端驾驶。由于所使用的方法具有离散的动作空间，因此需要对转向和加速命令进行量化。在交通中使用端到端解决方案的驾驶复杂性可以通过代理所需的训练回合数来很好地检验。虽然在简单的车道保持场景中，代理只需数百回合即可完成任务，但用于这些问题的代理需要300,000回合。
- en: IV Future Challenges
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 未来挑战
- en: The recent achievements on the field showed that different deep reinforcement
    learning techniques could be effectively used for different levels of autonomous
    vehicles’ motion planning problems, though many questions remain unanswered. The
    main advantage of these methods is that they can handle unstructured data such
    as raw or slightly pre-processed radar or camera-based image information.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最近该领域的成就表明，不同的深度强化学习技术可以有效地用于不同级别的自动驾驶车辆运动规划问题，尽管许多问题仍未解答。这些方法的主要优势在于它们能够处理诸如原始或稍微预处理的雷达或基于相机的图像信息等非结构化数据。
- en: Though using neural networks and deep learning techniques as universal function-approximators
    in automotive systems poses several questions. As stated in [[81](#bib.bib81)],
    function development for automotive applications realized in electronic control
    units (ECUs) is subject to proprietary OEM norms and several international standards,
    such as Automotive SPICE (Software Process Improvement and Capability Determination)
    [[82](#bib.bib82)] and ISO 26262 [[83](#bib.bib83)]. However, these standards
    are still far from addressing deep learning with dedicated statements, since verification
    and validation is not a solved issue in this domain. Some papers deal with these
    issues by using an underlying safety layer, which verifies the safety of a planned
    trajectory before the vehicle control system executes it. However, full functional
    safety coverage can not be guaranteed in complex scenarios this way.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在汽车系统中使用神经网络和深度学习技术作为通用函数逼近器提出了几个问题。如[[81](#bib.bib81)]所述，用于汽车应用的函数开发在电子控制单元（ECUs）中实现，受制于专有的OEM规范以及多个国际标准，例如汽车SPICE（软件过程改进与能力确定）[[82](#bib.bib82)]和ISO
    26262 [[83](#bib.bib83)]。然而，这些标准仍远未对深度学习提供专门的说明，因为在这一领域，验证和确认仍未解决。一些论文通过使用基础安全层来处理这些问题，该层在车辆控制系统执行之前验证计划轨迹的安全性。然而，这种方法无法在复杂场景中保证全面的功能安全覆盖。
- en: One of the main benefits of using deep neural networks trained by a reinforcement
    learning agent in motion planning is the relatively low computational requirements
    of the trained network. Though this property needs a vast amount of trials in
    the learning phase to gain enough experience, as mentioned before, for simple
    convex optimization problems, the convergence of the process is fast. However,
    for complex scenarios, the training can quickly reach millions of steps, meaning
    that one setup of hyper-parameters or reward hypothesis can last hours or even
    days. Since complicated reinforcement learning tasks need continuous iteration
    on the environment design, network structure, reward scheme, or even the used
    algorithm itself, designing such a system is a time-consuming project. Besides
    the appropriate result analysis and inference, the evaluation time highly depends
    on the computational capacities assigned. On this basis, it is not a surprise
    that most papers nowadays deal with minor subtasks of the motion planning problem,
    and the most complex scenarios, such as navigating in urban traffic, can not be
    found in the literature.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用由强化学习代理训练的深度神经网络在运动规划中的主要好处之一是训练后的网络具有相对较低的计算要求。尽管如前所述，这一特性在学习阶段需要大量的试验以获得足够的经验，但对于简单的凸优化问题，过程的收敛速度较快。然而，对于复杂的场景，训练可能迅速达到数百万步，这意味着一组超参数或奖励假设可能需要数小时甚至数天才能完成。由于复杂的强化学习任务需要对环境设计、网络结构、奖励机制或甚至所使用的算法本身进行持续的迭代，设计这样一个系统是一个耗时的项目。除了适当的结果分析和推断外，评估时间在很大程度上依赖于分配的计算能力。在这一基础上，现在大多数论文处理的是运动规划问题的次要子任务，而像在城市交通中导航这样复杂的场景在文献中并不常见。
- en: By examining the observation element of the recent articles, it can be stated
    that most researches ignore complex sensor models. Some papers use ”ground truth”
    environment representations or ”ideal” sensor models, and only a few articles
    utilize sensor noise. On the one hand, transferring the knowledge acquired from
    ideal observations to real-world application poses several feasibility questions
    [[84](#bib.bib84)], on the other hand, using noisy or erroneous models could lead
    to actually more robust agents, as stated in [[42](#bib.bib42)].
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 通过审查近期文章的观察元素，可以指出大多数研究忽略了复杂的传感器模型。一些论文使用“真实情况”环境表示或“理想”传感器模型，只有少数文章利用了传感器噪声。一方面，将从理想观察中获得的知识转移到现实世界应用中提出了若干可行性问题[[84](#bib.bib84)]，另一方面，使用带噪声或错误的模型可能会导致实际更稳健的代理，如[[42](#bib.bib42)]所述。
- en: The same applies to the environment, which can be examined best amongst the
    group of highway learners, where the road topology is almost always fixed, and
    the surrounding vehicles’ behavior is limited. Validation of these agents is usually
    made in the same environment setup, which contradicts the basic techniques of
    machine learning, where the training and validation scenarios should differ in
    some aspects. As a reinforcement learning agent can generally act well in the
    situations that are close to those it has experience with, it is crucial to focus
    on developing more realistic and diverse environments, including the modeling
    level of any interacting traffic participant to achieve such agents that are easily
    transferable to real-world applications. This applies to vehicle dynamics, where
    more diverse and more realistic modeling would be needed. Naturally, these improvements
    increase the numerical complexity of the environment model, which is one of the
    main issues in these applications.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 环境同样适用这一点，在高速公路学习者群体中表现最好，因为道路拓扑几乎总是固定的，并且周围车辆的行为是有限的。这些代理的验证通常是在相同的环境设置中进行的，这与机器学习的基本技术相矛盾，机器学习中训练和验证场景应该在某些方面有所不同。由于强化学习代理通常能够在接近其经验的情境下表现良好，因此专注于开发更现实和多样化的环境，包括对任何互动交通参与者的建模，以实现易于迁移到实际应用的代理是至关重要的。这同样适用于车辆动态，需要更多的多样化和现实的建模。自然，这些改进增加了环境模型的数值复杂性，这是这些应用中的主要问题之一。
- en: Tending towards mixed or hierarchical system design would be a solution to this
    problem in the future, by mixing classic control approaches and deep RL. Also,
    the use of extended learning techniques, such as curriculum learning, transfer
    learning, or Alpha-Go like planning agents, would profoundly affect the efficiency
    of these projects.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 向混合或层次系统设计的倾斜将是未来解决这一问题的一个途径，通过将经典控制方法与深度强化学习结合使用。此外，使用扩展学习技术，如课程学习、迁移学习或类似
    Alpha-Go 的规划代理，将会深刻影响这些项目的效率。
- en: Overall it can be said that many problems need to be solved in this field, such
    as the detail of the environment and sensor modeling, the computational requirements,
    the transferability to real applications, robustness, and validation of the agents.
    Because of these issues, it is hard to predict whether reinforcement learning
    is an appropriate tool for automotive applications.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，可以说这个领域还有许多问题需要解决，比如环境和传感器建模的细节、计算需求、在实际应用中的可转移性、鲁棒性以及代理的验证。由于这些问题，很难预测强化学习是否适合汽车应用。
- en: Acknowledgment
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: The research reported in this paper was supported by the Higher Education Excellence
    Program of the Ministry of Human Capacities in the frame of Artificial Intelligence
    research area of Budapest University of Technology and Economics (BME FIKPMI/FM).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 本文所报告的研究得到了人力资源部高等教育卓越计划的资助，作为布达佩斯科技经济大学（BME FIKPMI/FM）人工智能研究领域的一部分。
- en: References
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    and M. Riedmiller, “Playing Atari with Deep Reinforcement Learning,” 12 2013.
    [Online]. Available: http://arxiv.org/abs/1312.5602'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] V. Mnih, K. Kavukcuoglu, D. Silver, A. Graves, I. Antonoglou, D. Wierstra,
    和 M. Riedmiller，“使用深度强化学习玩 Atari 游戏，” 2013年12月。[在线]. 可用: http://arxiv.org/abs/1312.5602'
- en: '[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, and D. Hassabis,
    “Human-level control through deep reinforcement learning,” *Nature*, vol. 518,
    no. 7540, pp. 529–533, 2 2015. [Online]. Available: http://www.nature.com/articles/nature14236'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] V. Mnih, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G. Bellemare,
    A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, S. Petersen, C. Beattie,
    A. Sadik, I. Antonoglou, H. King, D. Kumaran, D. Wierstra, S. Legg, 和 D. Hassabis，“通过深度强化学习实现人类水平的控制，”*自然*，第518卷，第7540期，第529–533页，2015年2月。[在线].
    可用: http://www.nature.com/articles/nature14236'
- en: '[3] B. Paden, M. Cap, S. Z. Yong, D. Yershov, and E. Frazzoli, “A Survey of
    Motion Planning and Control Techniques for Self-driving Urban Vehicles,” *CoRR*,
    pp. 1–27, 2016\. [Online]. Available: https://ieeexplore.ieee.org/abstract/document/7490340'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] B. Paden, M. Cap, S. Z. Yong, D. Yershov, 和 E. Frazzoli，“自动驾驶城市车辆的运动规划和控制技术综述，”*CoRR*，第1–27页，2016年。[在线].
    可用: https://ieeexplore.ieee.org/abstract/document/7490340'
- en: '[4] H. Bast, D. Delling, A. Goldberg, M. Müller-Hannemann, T. Pajor, P. Sanders,
    D. Wagner, and R. F. Werneck, “Route Planning in Transportation Networks,” in
    *Lecture Notes in Computer Science*.   Springer, Cham, 2016, pp. 19–80.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] H. Bast, D. Delling, A. Goldberg, M. Müller-Hannemann, T. Pajor, P. Sanders,
    D. Wagner, 和 R. F. Werneck，“运输网络中的路径规划，” 收录于*计算机科学讲义集*。Springer，Cham，2016年，第19–80页。'
- en: '[5] S. Brechtel, T. Gindele, and R. Dillmann, “Probabilistic decision-making
    under uncertainty for autonomous driving using continuous POMDPs,” in *17th International
    IEEE Conference on Intelligent Transportation Systems (ITSC)*.   IEEE, 10 2014,
    pp. 392–399\. [Online]. Available: http://ieeexplore.ieee.org/document/6957722/'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] S. Brechtel, T. Gindele, 和 R. Dillmann，“在不确定条件下进行自主驾驶的概率决策，使用连续 POMDPs，”
    收录于*第17届国际 IEEE 智能交通系统会议（ITSC）*。IEEE，2014年10月，第392–399页。[在线]. 可用: http://ieeexplore.ieee.org/document/6957722/'
- en: '[6] J. Wiest, M. Hoffken, U. Kresel, and K. Dietmayer, “Probabilistic trajectory
    prediction with Gaussian mixture models,” in *2012 IEEE Intelligent Vehicles Symposium*.   IEEE,
    6 2012, pp. 141–146\. [Online]. Available: http://ieeexplore.ieee.org/document/6232277/'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] J. Wiest, M. Hoffken, U. Kresel, 和 K. Dietmayer，“使用高斯混合模型进行概率轨迹预测，” 收录于*2012
    IEEE 智能车辆研讨会*。IEEE，2012年6月，第141–146页。[在线]. 可用: http://ieeexplore.ieee.org/document/6232277/'
- en: '[7] Y. Dou, F. Yan, and D. Feng, “Lane changing prediction at highway lane
    drops using support vector machine and artificial neural network classifiers,”
    in *2016 IEEE International Conference on Advanced Intelligent Mechatronics (AIM)*.   IEEE,
    7 2016, pp. 901–906. [Online]. Available: http://ieeexplore.ieee.org/document/7576883/'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. H. Reif, “Complexity of the mover’s problem and generalizations,” in
    *20th Annual Symposium on Foundations of Computer Science (sfcs 1979)*.   IEEE,
    10 1979, pp. 421–427. [Online]. Available: http://ieeexplore.ieee.org/document/4568037/'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] F. Hegedus, T. Becsi, S. Aradi, and G. Galdi, “Hybrid Trajectory Planning
    for Autonomous Vehicles using Neural Networks,” in *2018 IEEE 18th International
    Symposium on Computational Intelligence and Informatics (CINTI)*.   IEEE, 11 2018,
    pp. 000 025–000 030\. [Online]. Available: https://ieeexplore.ieee.org/document/8928220/'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] D. M. Saxena, S. Bae, A. Nakhaei, K. Fujimura, and M. Likhachev, “Driving
    in Dense Traffic with Model-Free Reinforcement Learning,” 9 2019\. [Online]. Available:
    http://arxiv.org/abs/1909.06710'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] r. Fehér, S. Aradi, F. Hegedűs, T. Bécsi, and P. Gáspár, “Hybrid DDPG
    Approach for Vehicle Motion Planning,” in *Proceedings of the 16th International
    Conference on Informatics in Control, Automation and Robotics*.   SCITEPRESS -
    Science and Technology Publications, 2019, pp. 422–429.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] R. S. Sutton and A. G. Barto, *Reinforcement Learning: An Introduction*,
    second edi ed.   The MIT Press, 2017.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] R. Bellman, *Dynamic Programming*.   Princeton University Press, Princeton,
    NJ, 1957.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] H. Van Hasselt, A. Guez, and D. Silver, “Deep reinforcement learning with
    double q-learning,” in *Thirtieth AAAI conference on artificial intelligence*,
    2016.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Z. Wang, T. Schaul, M. Hessel, H. van Hasselt, M. Lanctot, and N. de Freitas,
    “Dueling Network Architectures for Deep Reinforcement Learning,” 11 2015. [Online].
    Available: http://arxiv.org/abs/1511.06581'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] D. Silver, G. Lever, N. Heess, T. Degris, D. Wierstra, and M. Riedmiller,
    “Deterministic policy gradient algorithms,” in *Proceedings of the 31st International
    Conference on Machine Learning (ICML 2014)*, 2014, pp. 387–395.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] T. P. Lillicrap, J. J. Hunt, A. Pritzel, N. Heess, T. Erez, Y. Tassa,
    D. Silver, and D. Wierstra, “Continuous control with deep reinforcement learning,”
    9 2015\. [Online]. Available: http://arxiv.org/abs/1509.02971'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Z. Qiao, K. Muelling, J. M. Dolan, P. Palanisamy, and P. Mudalige, “Automatically
    Generated Curriculum based Reinforcement Learning for Autonomous Vehicles in Urban
    Environment,” in *2018 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 6 2018,
    pp. 1233–1238\. [Online]. Available: https://ieeexplore.ieee.org/document/8500603/'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] M. Bouton, A. Nakhaei, K. Fujimura, and M. J. Kochenderfer, “Cooperation-Aware
    Reinforcement Learning for Merging in Dense Traffic,” 6 2019\. [Online]. Available:
    http://arxiv.org/abs/1906.11021'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] M. Kaushik, V. Prasad, K. M. Krishna, and B. Ravindran, “Overtaking Maneuvers
    in Simulated Highway Driving using Deep Reinforcement Learning,” in *2018 IEEE
    Intelligent Vehicles Symposium (IV)*.   IEEE, 6 2018, pp. 1885–1890\. [Online].
    Available: https://ieeexplore.ieee.org/document/8500718/'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Ferdowsi, U. Challita, W. Saad, and N. B. Mandayam, “Robust Deep Reinforcement
    Learning for Security and Safety in Autonomous Vehicle Systems,” in *IEEE Conference
    on Intelligent Transportation Systems, Proceedings, ITSC*, vol. 2018-Novem.   Institute
    of Electrical and Electronics Engineers Inc., 12 2018, pp. 307–312.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] X. Ma, K. Driggs-Campbell, and M. J. Kochenderfer, “Improved Robustness
    and Safety for Autonomous Vehicle Control with Adversarial Reinforcement Learning,”
    in *IEEE Intelligent Vehicles Symposium, Proceedings*, vol. 2018-June.   Institute
    of Electrical and Electronics Engineers Inc., 10 2018, pp. 1665–1671.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] L. P. Kaelbling, M. L. Littman, and A. R. Cassandra, “Planning and acting
    in partially observable stochastic domains,” *Artificial Intelligence*, vol. 101,
    no. 1-2, pp. 99–134, 5 1998\. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S000437029800023X'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] J. Kong, M. Pfeiffer, G. Schildbach, and F. Borrelli, “Kinematic and dynamic
    vehicle models for autonomous driving control design,” in *2015 IEEE Intelligent
    Vehicles Symposium (IV)*.   IEEE, 6 2015, pp. 1094–1099\. [Online]. Available:
    http://ieeexplore.ieee.org/document/7225830/'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] P. Polack, F. Altche, B. D’Andrea-Novel, and A. de La Fortelle, “The kinematic
    bicycle model: A consistent model for planning feasible trajectories for autonomous
    vehicles?” in *2017 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 6 2017,
    pp. 812–818\. [Online]. Available: http://ieeexplore.ieee.org/document/7995816/'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] C. You, J. Lu, D. Filev, and P. Tsiotras, “Advanced planning for autonomous
    vehicles using reinforcement learning and deep inverse reinforcement learning,”
    *Robotics and Autonomous Systems*, vol. 114, pp. 1–18, 4 2019\. [Online]. Available:
    https://linkinghub.elsevier.com/retrieve/pii/S0921889018302021'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Kesting, M. Treiber, and D. Helbing, “General Lane-Changing Model MOBIL
    for Car-Following Models,” *Transportation Research Record: Journal of the Transportation
    Research Board*, vol. 1999, no. 1, pp. 86–94, 1 2007. [Online]. Available: http://journals.sagepub.com/doi/10.3141/1999-10'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] M. Treiber, A. Hennecke, and D. Helbing, “Congested traffic states in
    empirical observations and microscopic simulations,” *Physical Review E*, vol. 62,
    no. 2, pp. 1805–1824, 8 2000\. [Online]. Available: https://link.aps.org/doi/10.1103/PhysRevE.62.1805'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] D. Krajzewicz, J. Erdmann, M. Behrisch, and L. Bieker, “Recent Development
    and Applications of SUMO - Simulation of Urban MObility,” *International Journal
    On Advances in Systems and Measurements*, vol. 5, no. 3, pp. 128–138, 2012.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] M. Fellendorf and P. Vortisch, “Microscopic Traffic Flow Simulator VISSIM,”
    in *Fundamentals of Traffic Simulation. International Series in Operations Research
    & Management Science*, J. Barceló, Ed.   Springer, 2010, pp. 63–93.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Ye, X. Zhang, and J. Sun, “Automated Vehicle’s behavior decision making
    using deep reinforcement learning and high-fidelity simulation environment,” *Transportation
    Research Part C*, vol. 107, no. May, pp. 155–170, 2019\. [Online]. Available:
    https://doi.org/10.1016/j.trc.2019.08.011'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] B. Wymann, E. Espié, C. Guionneau, C. Dimitrakakis, R. Coulom, and A. Sumner,
    “TORCS: The Open Racing Car Simulator,” 2014\. [Online]. Available: http://www.torcs.org'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] “CarSIM, Mechanical Simulation Corporation.” [Online]. Available: https://www.carsim.com/'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] “CarMaker, IPG Automotive.” [Online]. Available: https://ipg-automotive.com/products-services/simulation-software/carmaker/'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] H. An and J.-i. Jung, “Decision-Making System for Lane Change Using Deep
    Reinforcement Learning in Connected and Automated Driving,” *Electronics*, vol. 8,
    no. 5, p. 543, 5 2019\. [Online]. Available: https://www.mdpi.com/2079-9292/8/5/543'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] J. Wang, Q. Zhang, D. Zhao, and Y. Chen, “Lane Change Decision-making
    through Deep Reinforcement Learning with Rule-based Constraints,” in *2019 International
    Joint Conference on Neural Networks (IJCNN)*.   IEEE, 7 2019, pp. 1–6\. [Online].
    Available: http://arxiv.org/abs/1904.00231 https://ieeexplore.ieee.org/document/8852110/'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] “Welcome to Udacity’s Self-Driving Car Simulator.” [Online]. Available:
    https://github.com/udacity/self-driving-car-sim'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] A. Dosovitskiy, G. Ros, F. Codevilla, A. Lopez, and V. Koltun, “CARLA:
    An Open Urban Driving Simulator,” in *Proceedings of the 1st Annual Conference
    on Robot Learning*, 2017, pp. 1–17.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] F. Rosique, P. J. Navarro, C. Fernández, and A. Padilla, “A Systematic
    Review of Perception System and Simulators for Autonomous Vehicles Research,”
    *Sensors*, vol. 19, no. 3, p. 648, 2 2019\. [Online]. Available: http://www.mdpi.com/1424-8220/19/3/648'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] K. Kashihara, “Deep Q learning for traffic simulation in autonomous driving
    at a highway junction,” in *2017 IEEE International Conference on Systems, Man,
    and Cybernetics (SMC)*.   IEEE, 10 2017, pp. 984–988\. [Online]. Available: http://ieeexplore.ieee.org/document/8122738/'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] S. Nageshrao, H. E. Tseng, and D. Filev, “Autonomous Highway Driving using
    Deep Reinforcement Learning,” in *2019 IEEE International Conference on Systems,
    Man and Cybernetics (SMC)*.   IEEE, 10 2019, pp. 2326–2331\. [Online]. Available:
    http://arxiv.org/abs/1904.00035 https://ieeexplore.ieee.org/document/8914621/'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Alizadeh, M. Moghadam, Y. Bicer, N. K. Ure, U. Yavas, and C. Kurtulus,
    “Automated Lane Change Decision Making using Deep Reinforcement Learning in Dynamic
    and Uncertain Highway Environment,” in *2019 IEEE Intelligent Transportation Systems
    Conference (ITSC)*, no. August.   IEEE, 10 2019, pp. 1399–1404\. [Online]. Available:
    https://ieeexplore.ieee.org/document/8917192/'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. Feher, S. Aradi, and T. Becsi, “Q-learning based Reinforcement Learning
    Approach for Lane Keeping,” in *2018 IEEE 18th International Symposium on Computational
    Intelligence and Informatics (CINTI)*.   Budapest: IEEE, 11 2018, pp. 000 031–000 036\.
    [Online]. Available: https://ieeexplore.ieee.org/document/8928230/'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Z. Bai, W. Shangguan, B. Cai, and L. Chai, “Deep Reinforcement Learning
    Based High-level Driving Behavior Decision-making Model in Heterogeneous Traffic,”
    in *2019 Chinese Control Conference (CCC)*, no. February.   IEEE, 7 2019, pp.
    8600–8605\. [Online]. Available: http://arxiv.org/abs/1902.05772 https://ieeexplore.ieee.org/document/8866005/'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] P. Wolf, K. Kurzer, T. Wingert, F. Kuhnt, and J. M. Zollner, “Adaptive
    Behavior Generation for Autonomous Driving using Deep Reinforcement Learning with
    Compact Semantic States,” in *2018 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE,
    6 2018, pp. 993–1000\. [Online]. Available: https://ieeexplore.ieee.org/document/8500427/'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Aradi, T. Becsi, and P. Gaspar, “Policy Gradient Based Reinforcement
    Learning Approach for Autonomous Highway Driving,” in *2018 IEEE Conference on
    Control Technology and Applications (CCTA)*.   IEEE, 8 2018, pp. 670–675\. [Online].
    Available: https://ieeexplore.ieee.org/document/8511514/'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] X. Xu, L. Zuo, X. Li, L. Qian, J. Ren, and Z. Sun, “A Reinforcement Learning
    Approach to Autonomous Decision Making of Intelligent Vehicles on Highways,” *IEEE
    Transactions on Systems, Man, and Cybernetics: Systems*, no. December, 2018.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] P. Wang, C. Y. Chan, and A. De La Fortelle, “A Reinforcement Learning
    Based Approach for Automated Lane Change Maneuvers,” in *IEEE Intelligent Vehicles
    Symposium, Proceedings*, vol. 2018-June.   Institute of Electrical and Electronics
    Engineers Inc., 10 2018, pp. 1379–1384.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. P. Ronecker and Y. Zhu, “Deep Q-Network Based Decision Making for Autonomous
    Driving,” in *2019 3rd International Conference on Robotics and Automation Sciences
    (ICRAS)*.   IEEE, 6 2019, pp. 154–160\. [Online]. Available: https://ieeexplore.ieee.org/document/8808950/'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] M. Zhu, Y. Wang, J. Hu, X. Wang, and R. Ke, “Safe, Efficient, and Comfortable
    Velocity Control based on Reinforcement Learning for Autonomous Driving,” 1 2019\.
    [Online]. Available: http://arxiv.org/abs/1902.00089'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] M. Zhu, X. Wang, and Y. Wang, “Human-like autonomous car-following model
    with deep reinforcement learning,” *Transportation Research Part C: Emerging Technologies*,
    vol. 97, pp. 348–368, 12 2018.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] C. J. Hoel, K. Wolff, and L. Laine, “Automated Speed and Lane Change Decision
    Making using Deep Reinforcement Learning,” in *IEEE Conference on Intelligent
    Transportation Systems, Proceedings, ITSC*, vol. 2018-Novem.   Institute of Electrical
    and Electronics Engineers Inc., 12 2018, pp. 2148–2155.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] C. J. Hoel, K. Wolff, 和 L. Laine, “使用深度强化学习进行自动化速度和车道变更决策，” *IEEE智能交通系统会议论文集（ITSC）*，第2018年11月卷。
    电气和电子工程师学会，2018年12月，第2148–2155页。'
- en: '[53] E. Leurent, E. Leurent, A. Survey, S.-a. Representations, and A. Driving,
    “A Survey of State-Action Representations for Autonomous Driving To cite this
    version : HAL Id : hal-01908175,” 2018.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] E. Leurent, E. Leurent, A. Survey, S.-a. Representations, 和 A. Driving,
    “自动驾驶状态-动作表示的调查 引用此版本：HAL Id : hal-01908175，” 2018。'
- en: '[54] P. Wolf, C. Hubschneider, M. Weber, A. Bauer, J. Hartl, F. Durr, and J. M.
    Zollner, “Learning how to drive in a real world simulation with deep Q-Networks,”
    in *2017 IEEE Intelligent Vehicles Symposium (IV)*.   IEEE, 6 2017, pp. 244–250\.
    [Online]. Available: http://ieeexplore.ieee.org/document/7995727/'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] P. Wolf, C. Hubschneider, M. Weber, A. Bauer, J. Hartl, F. Durr, 和 J.
    M. Zollner, “通过深度Q网络在现实世界仿真中学习驾驶，” *2017 IEEE智能车辆研讨会（IV）*。 IEEE, 2017年6月，第244–250页。
    [在线]. 可用： http://ieeexplore.ieee.org/document/7995727/'
- en: '[55] M. Jaritz, R. De Charette, M. Toromanoff, E. Perot, and F. Nashashibi,
    “End-to-End Race Driving with Deep Reinforcement Learning,” in *Proceedings -
    IEEE International Conference on Robotics and Automation*.   Institute of Electrical
    and Electronics Engineers Inc., 9 2018, pp. 2070–2075.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] M. Jaritz, R. De Charette, M. Toromanoff, E. Perot, 和 F. Nashashibi, “使用深度强化学习进行端到端赛车驾驶，”
    *IEEE国际机器人与自动化会议论文集*。 电气和电子工程师学会，2018年9月，第2070–2075页。'
- en: '[56] E. Perot, M. Jaritz, M. Toromanoff, and R. d. Charette, “End-to-End Driving
    in a Realistic Racing Game with Deep Reinforcement Learning,” in *2017 IEEE Conference
    on Computer Vision and Pattern Recognition Workshops (CVPRW)*.   IEEE, 7 2017,
    pp. 474–475. [Online]. Available: http://ieeexplore.ieee.org/document/8014798/'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] E. Perot, M. Jaritz, M. Toromanoff, 和 R. d. Charette, “在现实赛车游戏中使用深度强化学习进行端到端驾驶，”
    *2017 IEEE计算机视觉与模式识别会议研讨会（CVPRW）*。 IEEE, 2017年7月，第474–475页。 [在线]. 可用： http://ieeexplore.ieee.org/document/8014798/'
- en: '[57] D. Li, D. Zhao, Q. Zhang, and Y. Chen, “Reinforcement Learning and Deep
    Learning Based Lateral Control for Autonomous Driving [Application Notes],” *IEEE
    Computational Intelligence Magazine*, vol. 14, no. 2, pp. 83–98, 5 2019.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] D. Li, D. Zhao, Q. Zhang, 和 Y. Chen, “基于强化学习和深度学习的自动驾驶横向控制 [应用说明]，” *IEEE计算智能杂志*，第14卷，第2期，第83–98页，2019年5月。'
- en: '[58] S. Kotyan, D. V. Vargas, and U. Venkanna, “Self Training Autonomous Driving
    Agent,” in *2019 58th Annual Conference of the Society of Instrument and Control
    Engineers of Japan (SICE)*.   IEEE, 9 2019, pp. 1456–1461\. [Online]. Available:
    http://arxiv.org/abs/1904.12738 https://ieeexplore.ieee.org/document/8859883/'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] S. Kotyan, D. V. Vargas, 和 U. Venkanna, “自我训练的自动驾驶代理，” *2019年第58届日本仪器与控制工程师学会年会（SICE）*。
    IEEE, 2019年9月，第1456–1461页。 [在线]. 可用： http://arxiv.org/abs/1904.12738 https://ieeexplore.ieee.org/document/8859883/'
- en: '[59] N. Xu, B. Tan, and B. Kong, “Autonomous Driving in Reality with Reinforcement
    Learning and Image Translation,” 1 2018\. [Online]. Available: http://arxiv.org/abs/1801.05299'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] N. Xu, B. Tan, 和 B. Kong, “在现实中使用强化学习和图像翻译的自动驾驶，” 2018年1月。 [在线]. 可用： http://arxiv.org/abs/1801.05299'
- en: '[60] J. Lee, T. Kim, and H. J. Kim, “Autonomous lane keeping based on approximate
    Q-learning,” in *2017 14th International Conference on Ubiquitous Robots and Ambient
    Intelligence (URAI)*.   IEEE, 6 2017, pp. 402–405\. [Online]. Available: http://ieeexplore.ieee.org/document/7992762/'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] J. Lee, T. Kim, 和 H. J. Kim, “基于近似Q学习的自动车道保持，” *2017年第14届普及机器人和环境智能国际会议（URAI）*。
    IEEE, 2017年6月，第402–405页。 [在线]. 可用： http://ieeexplore.ieee.org/document/7992762/'
- en: '[61] A. Elfes, “Using occupancy grids for mobile robot perception and navigation,”
    *Computer*, vol. 22, no. 6, pp. 46–57, 6 1989\. [Online]. Available: http://ieeexplore.ieee.org/document/30720/'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A. Elfes, “使用占用网格进行移动机器人感知和导航，” *计算机*，第22卷，第6期，第46–57页，1989年6月。 [在线].
    可用： http://ieeexplore.ieee.org/document/30720/'
- en: '[62] S. Thrun, M. Montemerlo, H. Dahlkamp, D. Stavens, A. Aron, J. Diebel,
    P. Fong, J. Gale, M. Halpenny, G. Hoffmann, K. Lau, C. Oakley, M. Palatucci, V. Pratt,
    P. Stang, S. Strohband, C. Dupont, L.-E. Jendrossek, C. Koelen, C. Markey, C. Rummel,
    J. van Niekerk, E. Jensen, P. Alessandrini, G. Bradski, B. Davies, S. Ettinger,
    A. Kaehler, A. Nefian, and P. Mahoney, “Stanley: The robot that won the DARPA
    Grand Challenge,” *Journal of Field Robotics*, vol. 23, no. 9, pp. 661–692, 9
    2006\. [Online]. Available: http://doi.wiley.com/10.1002/rob.20147'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] N. Deo and M. M. Trivedi, “Multi-Modal Trajectory Prediction of Surrounding
    Vehicles with Maneuver based LSTMs,” in *2018 IEEE Intelligent Vehicles Symposium
    (IV)*.   IEEE, 6 2018, pp. 1179–1184\. [Online]. Available: https://ieeexplore.ieee.org/document/8500493/'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] T. Hegedűs, B. Németh, and P. Gáspár, “Graph-based Multi-Vehicle Overtaking
    Strategy for Autonomous Vehicles,” *IFAC-PapersOnLine*, vol. 52, no. 5, pp. 372–377,
    2019\. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S2405896319306822'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] A. Folkers, M. Rick, and C. Buskens, “Controlling an Autonomous Vehicle
    with Deep Reinforcement Learning,” in *2019 IEEE Intelligent Vehicles Symposium
    (IV)*.   IEEE, 6 2019, pp. 2025–2031\. [Online]. Available: https://ieeexplore.ieee.org/document/8814124/'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Esser and M. Schreckenberg, “Microscopic Simulation of Urban Traffic
    Based on Cellular Automata,” *International Journal of Modern Physics C*, vol. 08,
    no. 05, pp. 1025–1036, 10 1997\. [Online]. Available: https://www.worldscientific.com/doi/abs/10.1142/S0129183197000904'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] G. Wang, J. Hu, Z. Li, and L. Li, “Cooperative Lane Changing via Deep
    Reinforcement Learning,” 2019\. [Online]. Available: http://arxiv.org/abs/1906.08662'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] T. Shi, P. Wang, X. Cheng, and C.-Y. Chan, “Driving Decision and Control
    for Autonomous Lane Change based on Deep Reinforcement Learning,” in *2019 IEEE
    Intelligent Transportation Systems Conference (ITSC)*.   Auckland, New Zealand:
    IEEE, 2019\. [Online]. Available: http://arxiv.org/abs/1904.10171'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] P. Wang and C.-Y. Chan, “Formulation of deep reinforcement learning architecture
    toward autonomous driving for on-ramp merge,” in *2017 IEEE 20th International
    Conference on Intelligent Transportation Systems (ITSC)*.   IEEE, 10 2017, pp.
    1–6. [Online]. Available: http://ieeexplore.ieee.org/document/8317735/'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] T. Bécsi, S. Aradi, r. Fehér, J. Szalay, and P. Gáspár, “Highway Environment
    Model for Reinforcement Learning,” *IFAC-PapersOnLine*, vol. 51, no. 22, pp. 429–434,
    2018\. [Online]. Available: https://linkinghub.elsevier.com/retrieve/pii/S2405896318333032'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. Bouton, J. Karlsson, A. Nakhaei, K. Fujimura, M. J. Kochenderfer, and
    J. Tumova, “Reinforcement Learning with Probabilistic Guarantees for Autonomous
    Driving,” 2019\. [Online]. Available: http://arxiv.org/abs/1904.07189'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] D. Loiacono, A. Prete, P. L. Lanzi, and L. Cardamone, “Learning to overtake
    in TORCS using simple reinforcement learning,” in *2010 IEEE World Congress on
    Computational Intelligence, WCCI 2010 - 2010 IEEE Congress on Evolutionary Computation,
    CEC 2010*, 2010.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] D. C. K. Ngai and N. H. C. Yung, “Automated Vehicle Overtaking based on
    a Multiple-Goal Reinforcement Learning Framework,” in *2007 IEEE Intelligent Transportation
    Systems Conference*.   IEEE, 9 2007, pp. 818–823\. [Online]. Available: http://ieeexplore.ieee.org/document/4357682/'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] ——, “A multiple-goal reinforcement learning method for complex vehicle
    overtaking maneuvers,” *IEEE Transactions on Intelligent Transportation Systems*,
    vol. 12, no. 2, pp. 509–522, 2011.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] C. Desjardins and B. Chaib-draa, “Cooperative Adaptive Cruise Control:
    A Reinforcement Learning Approach,” *IEEE Transactions on Intelligent Transportation
    Systems*, vol. 12, no. 4, pp. 1248–1260, 12 2011\. [Online]. Available: http://ieeexplore.ieee.org/document/5876320/'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] M. Gómez, R. V. González, T. Martínez-Marín, D. Meziat, and S. Sánchez,
    “Optimal motion planning by reinforcement learning in autonomous mobile vehicles,”
    *Robotica*, vol. 30, no. 2, pp. 159–170, 3 2012.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Y. Ye, X. Zhang, and J. Sun, “Automated Vehicle’s behavior decision making
    using deep reinforcement learning and high-fidelity simulation environment,” Tech.
    Rep.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] A. E. Sallab, M. Abdou, E. Perot, and S. Yogamani, “End-to-End Deep Reinforcement
    Learning for Lane Keeping Assist,” 12 2016\. [Online]. Available: http://arxiv.org/abs/1612.04340'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] P. Wang and C.-Y. Chan, “Autonomous Ramp Merge Maneuver Based on Reinforcement
    Learning with Continuous Action Space,” 3 2018\. [Online]. Available: http://arxiv.org/abs/1803.09203'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] C.-J. Hoel, K. Driggs-Campbell, K. Wolff, L. Laine, and M. J. Kochenderfer,
    “Combining Planning and Deep Reinforcement Learning in Tactical Decision Making
    for Autonomous Driving,” pp. 1–12, 5 2019\. [Online]. Available: http://arxiv.org/abs/1905.02680'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] F. Falcini, G. Lami, and A. M. Costanza, “Deep Learning in Automotive
    Software,” *IEEE Software*, vol. 34, no. 3, pp. 56–63, 5 2017. [Online]. Available:
    https://ieeexplore.ieee.org/document/7927925/'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] “Automotive SPICE process assessment/reference model,” 2015.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] “ISO 26262, ”Road Vehicles—Functional Safety—Part 1: Vocabulary”,” 2011.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Z. Szalay, T. Tettamanti, D. Esztergár-Kiss, I. Varga, and C. Bartolini,
    “Development of a Test Track for Driverless Cars: Vehicle Design, Track Configuration,
    and Liability Considerations,” *Periodica Polytechnica Transportation Engineering*,
    vol. 46, no. 1, p. 29, 3 2018\. [Online]. Available: https://pp.bme.hu/tr/article/view/10753'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
