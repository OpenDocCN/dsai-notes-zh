- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '类别: 未分类'
- en: 'date: 2024-09-06 19:49:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '日期: 2024-09-06 19:49:13'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2112.02719] A Survey on Deep learning based Document Image Enhancement'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2112.02719] 基于深度学习的文档图像增强调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.02719](https://ar5iv.labs.arxiv.org/html/2112.02719)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2112.02719](https://ar5iv.labs.arxiv.org/html/2112.02719)
- en: A Survey on Deep learning based Document Image Enhancement
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的文档图像增强调查
- en: Zahra Anvari
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zahra Anvari
- en: Department of Computer Science and Engineering
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学与工程系
- en: University of Texas Arlington
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯大学阿灵顿分校
- en: Arlington, TX
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 阿灵顿，德克萨斯州
- en: zahra.anvari@mavs.uta.edu
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: zahra.anvari@mavs.uta.edu
- en: '&Vassilis Athitsos'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '&Vassilis Athitsos'
- en: Department of Computer Science and Engineering
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学与工程系
- en: University of Texas Arlington
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 德克萨斯大学阿灵顿分校
- en: Arlington, TX
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 阿灵顿，德克萨斯州
- en: athitsos@uta.edu
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: athitsos@uta.edu
- en: Abstract
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Digitized documents such as scientific articles, tax forms, invoices, contract
    papers, historic texts are widely used nowadays. These document images could be
    degraded or damaged due to various reasons including poor lighting conditions,
    shadow, distortions like noise and blur, aging, ink stain, bleed-through, watermark,
    stamp, *etc*. Document image enhancement plays a crucial role as a pre-processing
    step in many automated document analysis and recognition tasks such as character
    recognition. With recent advances in deep learning, many methods are proposed
    to enhance the quality of these document images. In this paper, we review deep
    learning-based methods, datasets, and metrics for six main document image enhancement
    tasks, including binarization, debluring, denoising, defading, watermark removal,
    and shadow removal. We summarize the recent works for each task and discuss their
    features, challenges, and limitations. We introduce multiple document image enhancement
    tasks that have received little to no attention, including over and under exposure
    correction, super resolution, and bleed-through removal. We identify several promising
    research directions and opportunities for future research.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数字化文档如科学文章、税务表格、发票、合同文件、历史文本等被广泛使用。这些文档图像可能由于各种原因而受到损坏或退化，包括光照条件差、阴影、噪声和模糊等失真、老化、墨水污迹、渗透、
    水印、印章，*等*。文档图像增强作为许多自动化文档分析和识别任务（如字符识别）的预处理步骤，起着至关重要的作用。随着深度学习的最新进展，提出了许多方法来增强这些文档图像的质量。本文回顾了基于深度学习的方法、数据集和六种主要文档图像增强任务的度量指标，包括二值化、去模糊、去噪、去褪色、水印去除和阴影去除。我们总结了每个任务的最新研究成果，并讨论了它们的特点、挑战和局限性。我们介绍了多个鲜有关注的文档图像增强任务，包括过度曝光和曝光不足修正、超分辨率和渗透去除。我们确定了若干有前景的研究方向和未来研究机会。
- en: '*K*eywords Document Image Enhancement, Image Enhancement, Document Image Analysis
    and recognition, Deep Learning'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*K*eywords 文档图像增强、图像增强、文档图像分析与识别、深度学习'
- en: 1 Introduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Digitized documents such as scientific articles, tax forms, invoices, contract
    papers, personnel records, legal documents, historic texts, *etc.* are ubiquitous
    and widely used nowadays. These documents can be damaged due to watermark, stamps,
    aging, ink stains, bleed-through, etc., or can be degraded during the digitization
    process due to poor lighting conditions, shadow, camera distortion like noise
    and blur, etc. [[22](#bib.bib22), [63](#bib.bib63), [41](#bib.bib41), [7](#bib.bib7),
    [36](#bib.bib36), [29](#bib.bib29)].
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数字化文档如科学文章、税务表格、发票、合同文件、人员记录、法律文件、历史文本，*等*在今天无处不在，广泛使用。这些文档可能因水印、印章、老化、墨水污迹、渗透等而受损，或者在数字化过程中由于光照条件差、阴影、相机失真如噪声和模糊等而退化。[[22](#bib.bib22)、[63](#bib.bib63)、[41](#bib.bib41)、[7](#bib.bib7)、[36](#bib.bib36)、[29](#bib.bib29)]。
- en: Degraded document images have low visual quality and legibility. They could
    contain handwritten or machine printed text, or a mixture of both. In addition,
    they could contain multiple handwriting styles with different languages. Further
    complicating matter, the machine used to print the document could have used various
    technologies with variable quality ( e.g., documents printed in low DPI), thus
    affecting the quality of the image captured. Moreover, old documents could be
    degraded over time due to different reasons, such as humidity, being washed out,
    poor storage, low quality medium, etc. Therefore, there are many factors that
    affect the quality and legibility of the digitized document images.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 退化的文档图像具有较低的视觉质量和可读性。它们可能包含手写或机器打印的文字，或两者的混合。此外，它们可能包含多种书写风格和不同语言。更复杂的是，用于打印文档的机器可能使用了不同技术，其质量也可能不同（例如，低DPI打印的文档），从而影响捕获图像的质量。此外，旧文档可能因湿度、褪色、储存不善、低质量介质等原因而退化。因此，有许多因素会影响数字化文档图像的质量和可读性。
- en: The degraded document images make automated document analysis tasks such as
    character recognition (OCR) very challenging and such tasks perform poorly on
    these images. On the other hand, it is impractical and sometimes infeasible to
    manually enhance such images, especially in large scale, thus it is essential
    to develop methods that can automatically enhance the visual quality and the legibility
    of these images and restore the corrupted parts.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 退化的文档图像使得诸如字符识别（OCR）等自动文档分析任务非常具有挑战性，这些任务在这些图像上的表现很差。另一方面，手动增强这些图像在大规模时是不切实际且有时不可行的，因此开发能够自动增强这些图像的视觉质量和可读性并恢复损坏部分的方法至关重要。
- en: 'Document image enhancement problem consists of several tasks that are studied
    in the literature. In this survey, we focus on six main tasks that are illustrated
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep learning based
    Document Image Enhancement"), and we explain each task in details in Section [2](#S2
    "2 Document Image Enhancement Tasks ‣ A Survey on Deep learning based Document
    Image Enhancement"). Here we summarize these tasks:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 文档图像增强问题包括文献中研究的多个任务。在这项调查中，我们关注六个主要任务，如图[1](#S1.F1 "图1 ‣ 1 引言 ‣ 关于基于深度学习的文档图像增强的调查")所示，并在第[2](#S2
    "2 文档图像增强任务 ‣ 关于基于深度学习的文档图像增强的调查")节中详细解释每个任务。我们在此总结这些任务：
- en: '![Refer to caption](img/5a3422a1caa25d7bddbeaf9564466403.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/5a3422a1caa25d7bddbeaf9564466403.png)'
- en: 'Figure 1: Document image enhancement problems.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：文档图像增强问题。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Binarization: It aims at separating the background from the foreground (i.e.,
    text) in order to remove noise, ink stain, bleed-through, wrinkles, *etc*. The
    output of this task is a binary image with two classes: foreground and background.'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 二值化：它旨在将背景与前景（即文字）分离，以去除噪声、墨水污点、渗透、皱纹，*等等*。此任务的输出是一个具有两个类别的二值图像：前景和背景。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Deblur: This task aims at removing various blur types, e.g., Gaussian, motion,
    de-focus, *etc.*, from the document images.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去模糊：此任务旨在去除各种模糊类型，例如高斯模糊、运动模糊、对焦模糊，*等等*，从文档图像中。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Denoise: Denoising aims at removing various noise types, e.g., salt and pepper,
    wrinkles, dog-eared, background, and stain, etc. from the document images.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去噪：去噪旨在去除各种噪声类型，例如盐和胡椒噪声、皱纹、翻角、背景和污点，*等等*，从文档图像中。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Defade: It aims at improving the faded document images. A document could be
    faded due to againg, overexposure, or being washed out, etc.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 去褪色：它旨在改善褪色的文档图像。文档可能因老化、过度曝光或褪色等原因而褪色。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Watermark removal: Some documents, e.g., financial forms, can contain watermarks,
    and the text underneath a watermark might not be recognizable. This task aims
    at removing such watermarks.'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 水印去除：某些文档，例如财务表格，可能包含水印，水印下的文字可能无法识别。此任务旨在去除这些水印。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Shadow removal: Blocking the source of light while capturing an image (usually
    by a phone) could leave shadows on the captured document image. This task aims
    at estimating the show and removing it.'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 阴影去除：在拍摄图像（通常是用手机）时阻挡光源可能会在捕获的文档图像上留下阴影。此任务旨在估计阴影并去除它。
- en: With recent advances in deep learning, deep learning-based approaches have been
    proposed and applied to different computer vision and image processing tasks,
    such as object detection [[37](#bib.bib37), [56](#bib.bib56)], semantic segmentation [[38](#bib.bib38)],
    face detection and dataset creation [[2](#bib.bib2), [35](#bib.bib35), [59](#bib.bib59)],
    and image enhancement [[3](#bib.bib3), [17](#bib.bib17), [16](#bib.bib16)], etc.
    It has been shown that such deep learning-based methods achieve promising results
    and surpass the traditional methods. Similarly, deep learning-based methods for
    document image enhancement problems have received a great deal of attention over
    the past few years. The goal of this survey is to review these methods and discuss
    their features, advantages, disadvantages, challenges, and limitations, and identify
    opportunities for future research.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习的最新进展，基于深度学习的方法已经被提出并应用于不同的计算机视觉和图像处理任务，如物体检测 [[37](#bib.bib37), [56](#bib.bib56)]，语义分割 [[38](#bib.bib38)]，人脸检测和数据集创建 [[2](#bib.bib2),
    [35](#bib.bib35), [59](#bib.bib59)]，以及图像增强 [[3](#bib.bib3), [17](#bib.bib17),
    [16](#bib.bib16)]等。已显示出这些基于深度学习的方法取得了令人满意的结果，并超过了传统方法。同样，基于深度学习的文档图像增强问题的方法在过去几年中也受到了极大的关注。本次调查的目标是回顾这些方法并讨论它们的特点、优缺点、挑战和局限性，并识别未来研究的机会。
- en: 'To the best of our knowledge, this survey is the first survey of the recent
    advances in deep learning-based document image enhancement methods. We have several
    key contributions in this paper:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，本调查是关于深度学习基础的文档图像增强方法的最新进展的第一次调查。本文有几个关键贡献：
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review recent advances, mostly from the past five years, on deep learning-based
    methods for document image enhancement, to help readers and researchers to better
    understand this area of research.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了近年来，主要是过去五年中，基于深度学习的方法在文档图像增强方面的最新进展，以帮助读者和研究人员更好地理解这一研究领域。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We provide an overview of six main document image enhancement problems, including
    binarization, deblure, denoise, defade, watermark removal, and shadow removal.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们提供了六个主要文档图像增强问题的概述，包括二值化、去模糊、去噪声、去褪色、水印去除和阴影去除。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We review the state-of-the-art methods, and discuss their features, advantages,
    and disadvantages to help researchers and investigators to select suitable methods
    for their need.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们回顾了最先进的方法，并讨论了它们的特点、优缺点，以帮助研究人员和调查者选择适合他们需求的方法。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We introduce several important document image enhancement tasks that have received
    little to no attention, such as bleed through removal.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们介绍了一些重要的文档图像增强任务，这些任务尚未得到足够的关注，例如去除渗透。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We identify several open problems and promising research directions and opportunities
    for future research.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们识别了几个未解决的问题和有前景的研究方向，并指出了未来研究的机会。
- en: 2 Document Image Enhancement Tasks
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 文档图像增强任务
- en: In this section, we describe six main document image enhancement tasks, including
    binarization, debluring, denoising, defading, watermark removal, and shadow removal.
    Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Debluring ‣ 2 Document Image Enhancement Tasks
    ‣ A Survey on Deep learning based Document Image Enhancement") shows some image
    examples for each of these tasks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们描述了六个主要的文档图像增强任务，包括二值化、去模糊、去噪声、去褪色、水印去除和阴影去除。图 [3](#S2.F3 "Figure 3
    ‣ 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey on Deep learning
    based Document Image Enhancement") 显示了这些任务的图像示例。
- en: 2.1 Binarization
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 二值化
- en: Document image binarization refers to the process of segmenting a gray scale
    or color image to a black-and-white or binary image with only text and background.
    During this process any existing degradations such as bleed-through, noise, stamp,
    ink stains, faded characters, artifacts, etc. are removed. Formally, it seeks
    a decision function $f_{binarize}(·)$ for a document image $D_{orig}$ of width
    $W$ and height $H$, such that the resulting image $D_{binarized}$ of the same
    size only contains binary values while the overall document legibility is at least
    maintained if not enhanced.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 文档图像二值化指的是将灰度或彩色图像分割为仅包含文本和背景的黑白或二值图像的过程。在此过程中，任何现有的退化现象，如渗透、噪声、印章、墨迹、字符褪色、伪影等，都会被去除。正式地说，它寻求一个决策函数
    $f_{binarize}(·)$ 用于宽度为 $W$、高度为 $H$ 的文档图像 $D_{orig}$，使得得到的相同大小的图像 $D_{binarized}$
    仅包含二值值，同时整体文档的可读性至少得到保持，如果不是增强的话。
- en: '|  | $D_{binarized}=f_{binarize}(D_{orig})$ |  | (1) |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  | $D_{binarized}=f_{binarize}(D_{orig})$ |  | (1) |'
- en: Figure [2(a)](#S2.F2.sf1 "In 2.2 Debluring ‣ 2 Document Image Enhancement Tasks
    ‣ A Survey on Deep learning based Document Image Enhancement") shows an example
    of an image along with its binarized one.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [2(a)](#S2.F2.sf1 "在 2.2 去模糊 ‣ 2 文档图像增强任务 ‣ 基于深度学习的文档图像增强综述") 展示了一个图像及其二值化图像的例子。
- en: 2.2 Debluring
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 去模糊
- en: Nowadays, smartphones are widely used to digitize documents. This might propose
    various issues. The most prevalent one is the blur that might be introduced during
    capturing process. For instance, movement of the document, camera being out of
    focus, and camera shakes can add blur to the captured image. Figure [2(b)](#S2.F2.sf2
    "In 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey on Deep learning
    based Document Image Enhancement") shows an example of a blurry document image
    along with its corresponding clean one.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，智能手机被广泛用于数字化文档。这可能会带来各种问题。其中最常见的问题是拍摄过程中可能产生的模糊。例如，文档的移动、相机失焦以及相机抖动都可能使捕获的图像变得模糊。图 [2(b)](#S2.F2.sf2
    "在 2.2 去模糊 ‣ 2 文档图像增强任务 ‣ 基于深度学习的文档图像增强综述") 展示了一个模糊文档图像的例子及其对应的清晰图像。
- en: The goal of deblurring methods is to recover the clean or deblurred version
    of the blurry document image. These methods could be prior-based or learning based.
    The former ones attempt to estimate the blur kernel and the corresponding parameters
    to detect blur and use these parameters to remove it, thus recover the clean images.
    The learning-based methods which are also called data-driven methods are widely
    used in the past decade. These methods take advantage of the deep neural networks
    and large amount of data to propose a deblurring model that can recover the clean
    image without requiring any priors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 去模糊方法的目标是恢复模糊文档图像的清晰或去模糊版本。这些方法可以是基于先验的，也可以是基于学习的。前者尝试估计模糊核和相应的参数来检测模糊，并利用这些参数去除模糊，从而恢复清晰图像。基于学习的方法，也称为数据驱动的方法，在过去十年里得到了广泛应用。这些方法利用深度神经网络和大量数据来提出一个去模糊模型，能够在不需要任何先验信息的情况下恢复清晰图像。
- en: Document image deblurring is an ill-posed problem and it is a more challenging
    problem compared to natural/non-document image deblurring. One of the main reasons
    is that the performance of the OCR engines directly depend on the quality of the
    document images that are input to them. If the legibility and the quality of these
    document images were low, the performance of the OCR output will be affected accordingly.
    Therefore, the enhanced document images not only need to be visually improved
    they also need to become more legible.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 文档图像去模糊是一个病态问题，相比于自然/非文档图像去模糊，更具挑战性。主要原因之一是OCR引擎的性能直接依赖于输入文档图像的质量。如果这些文档图像的可读性和质量较低，则OCR输出的性能也会受到影响。因此，增强后的文档图像不仅需要在视觉上得到改善，还需要变得更具可读性。
- en: '![Refer to caption](img/13a914982ee2f0a6c1f7e95eb539f815.png)  ![Refer to caption](img/8a8d6201914411c6530e4d4b16f6343c.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/13a914982ee2f0a6c1f7e95eb539f815.png)  ![参见说明](img/8a8d6201914411c6530e4d4b16f6343c.png)'
- en: (a) Binarization task [[55](#bib.bib55)].
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 二值化任务 [[55](#bib.bib55)]。
- en: '![Refer to caption](img/b67a34a082582a243f41fd0c1b27f321.png)  ![Refer to caption](img/ee06a7741657b1880b7d38b12723038d.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b67a34a082582a243f41fd0c1b27f321.png)  ![参见说明](img/ee06a7741657b1880b7d38b12723038d.png)'
- en: (b) Deblur task. [[23](#bib.bib23)]
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 去模糊任务。[[23](#bib.bib23)]
- en: '![Refer to caption](img/2f001d65954264684f9a98784ee7e250.png)  ![Refer to caption](img/0e34197e8a7f173707b8fe701cf1981a.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2f001d65954264684f9a98784ee7e250.png)  ![参见说明](img/0e34197e8a7f173707b8fe701cf1981a.png)'
- en: (c) Defade task.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 去褪色任务。
- en: '![Refer to caption](img/9acf15a1a860b40a363393c8ff5f0e2a.png)  ![Refer to caption](img/b0ac0a9bde02ba8c0efe289b36c293de.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9acf15a1a860b40a363393c8ff5f0e2a.png)  ![参见说明](img/b0ac0a9bde02ba8c0efe289b36c293de.png)'
- en: (d) Denoise task. [[32](#bib.bib32)]
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 去噪任务。[[32](#bib.bib32)]
- en: '![Refer to caption](img/42b6554003362008ce7299580b9fda68.png)  ![Refer to caption](img/5fd5138fb773cf19bda77d1a01dc76f3.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/42b6554003362008ce7299580b9fda68.png)  ![参见说明](img/5fd5138fb773cf19bda77d1a01dc76f3.png)'
- en: (a) Shadow removal task. [[36](#bib.bib36)]
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 阴影去除任务。[[36](#bib.bib36)]
- en: '![Refer to caption](img/a94ba4bd450bfac76d39f8433641c300.png)  ![Refer to caption](img/2edd1b0e5983c849dba173e68358dc06.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a94ba4bd450bfac76d39f8433641c300.png)  ![参见说明](img/2edd1b0e5983c849dba173e68358dc06.png)'
- en: (b) Watermark removal task [[61](#bib.bib61)]
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 水印去除任务 [[61](#bib.bib61)]
- en: 'Figure 3: Sample images for different document image enhancement tasks. The
    image on the left is the input, and the right image is the output of each task.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：不同文档图像增强任务的示例图像。左侧的图像是输入，右侧的图像是每个任务的输出。
- en: 2.3 Defading
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 去褪色
- en: Defading is the process of recovering documents’ text that have become faded/faint.
    Documents’ content can be faded due to different factors. For instance, the ink
    can wear off over time, which is more prevalent in old documents. Sun-light or
    overexposure while digitizing the document can also make the document content
    lightened and hard to read. In addition, the handwriting or the printed text can
    be faint in the first place and deteriorate over time. This type of degradation
    poses issues such as low visual quality, poor legibility, and poor OCR performance.
    Defading methods mainly attempt to increase the visibility and recover a more
    legible version of the document image. Figure [2(c)](#S2.F2.sf3 "In 2.2 Debluring
    ‣ 2 Document Image Enhancement Tasks ‣ A Survey on Deep learning based Document
    Image Enhancement") shows an example of a defaded document image and its corresponding
    ground truth.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 去褪色是恢复已褪色或变暗的文档文本的过程。文档内容可能因各种因素而褪色。例如，随着时间的推移，墨水可能会磨损，这在旧文档中更为常见。阳光或在数字化文档时的过度曝光也可能使文档内容变淡，难以阅读。此外，手写或打印文本本身可能已经很淡，并且随着时间的推移变得更糟。这种退化会导致视觉质量低、可读性差以及OCR性能差等问题。去褪色方法主要尝试提高可见性，恢复更易读的文档图像版本。图
    [2(c)](#S2.F2.sf3 "In 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey
    on Deep learning based Document Image Enhancement") 显示了一个去褪色的文档图像及其对应的真实情况。
- en: 2.4 Denoising
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 去噪
- en: Some documents may contain artifacts such as salt and pepper noise, stamps,
    annotations, ink or coffee stains, wrinkles, *etc.* The image recovery is even
    harder when certain types of these artifacts cover the text specially in cases
    where the artifacts color is similar to or darker than the document text color.
    To improve the visual quality of these document images alongside the legibility,
    approaches that recover the clean version of the degraded documents are proposed.
    The methods that attempt to remove these artifacts include document image denoising,
    cleanup and binarization methods. Figure [2(d)](#S2.F2.sf4 "In 2.2 Debluring ‣
    2 Document Image Enhancement Tasks ‣ A Survey on Deep learning based Document
    Image Enhancement") illustrates an example of a noisy document image and its ground
    truth.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文档可能包含如盐和胡椒噪声、印章、注释、墨迹或咖啡渍、皱纹，*等*的伪影。当这些伪影覆盖文本时，尤其是伪影颜色与文档文本颜色相似或更暗的情况，图像恢复变得更困难。为了提高这些文档图像的视觉质量和可读性，提出了恢复退化文档干净版本的方法。这些去除伪影的方法包括文档图像去噪、清理和二值化方法。图
    [2(d)](#S2.F2.sf4 "In 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey
    on Deep learning based Document Image Enhancement") 说明了一个有噪声的文档图像及其真实情况。
- en: 2.5 Shadow Removal
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 阴影去除
- en: Documents can be digitized using scanners or mobile phone cameras. In the past,
    scanners were commonly used for digitizing documents with high quality, but with
    the prevalence of mobile phones more people tend to use their phones cameras in
    place of scanners to capture digital copies of their documents.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 文档可以使用扫描仪或手机摄像头进行数字化。在过去，扫描仪通常用于高质量数字化文档，但随着手机的普及，更多人倾向于使用手机摄像头代替扫描仪来捕获文档的数字副本。
- en: The document images captured using mobile phones are vulnerable to shadows mainly
    because the light sources are often blocked by the camera or even the person’s
    hand. Furthermore, even in the absence of objects that could be a source of occlusion,
    the lighting is often uneven when the document image is being captured in the
    real life. Therefore, document images digitized by mobile phone cameras in particular
    can suffer from shadows blocking a portion or all of the document and also uneven
    lighting and shading. These result in poor visual quality and legibility. Shadow
    removal methods focus on estimating the shadow casted on the document image and
    attempt to remove that in order to recover a clean, evenly lit document image
    which is more legible than the shadowed version. Figure [3(a)](#S2.F3.sf1 "In
    Figure 3 ‣ 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey on Deep
    learning based Document Image Enhancement") presents a sample of a document image
    with shadow and its ground truth.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用手机拍摄的文档图像容易出现阴影，主要是因为光源常常被相机或甚至是人的手挡住。此外，即使在没有可能遮挡物体的情况下，现实生活中拍摄文档图像时光线通常也不均匀。因此，尤其是手机相机拍摄的文档图像可能会受到阴影遮挡部分或全部文档，以及光照和阴影不均的问题。这些会导致视觉质量和可读性差。去阴影方法集中在估计文档图像上的阴影，并尝试去除这些阴影，以恢复干净、光照均匀的文档图像，使其比有阴影的版本更易读。图[3(a)](#S2.F3.sf1
    "In Figure 3 ‣ 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey on
    Deep learning based Document Image Enhancement")展示了一个带阴影的文档图像及其真实情况的样本。
- en: 2.6 Watermark Removal
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.6 水印去除
- en: Some documents, *e.g.,* financial forms, may contain one or multiple watermarks
    which occlude the document texts or makes it hard to read. Similar to denoising,
    the document image recovery is even harder in cases where the watermark color
    is the same or darker than the document text color or the watermark is thick and
    dense. Hence, we need approaches that recover the clean version of the degraded
    documents. Watermark removal methods focus on removing watermarks in order to
    increase the visual quality and legibility of the document images. Figure [3(b)](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey on
    Deep learning based Document Image Enhancement") shows an image sample along with
    its ground truth for this task.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 一些文档，如*财务表格*，可能包含一个或多个水印，这些水印遮挡了文档文本或使其难以阅读。类似于去噪，在水印颜色与文档文本颜色相同或更深，或者水印厚重且密集的情况下，文档图像的恢复更为困难。因此，我们需要能够恢复退化文档干净版本的方法。水印去除方法集中在去除水印，以提高文档图像的视觉质量和可读性。图[3(b)](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey on
    Deep learning based Document Image Enhancement")展示了一个图像样本及其真实情况。
- en: 3 Datasets
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 数据集
- en: In this section, we describe datasets that are used in the literature for different
    document image enhancement tasks. Table [1](#S3.T1 "Table 1 ‣ 3 Datasets ‣ A Survey
    on Deep learning based Document Image Enhancement") provides the specifications
    of these datasets and we describe them in more details in below. In addition,
    Figure [5](#S3.F5 "Figure 5 ‣ 3 Datasets ‣ A Survey on Deep learning based Document
    Image Enhancement") shows image samples from these datasets.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了文献中用于不同文档图像增强任务的数据集。表[1](#S3.T1 "Table 1 ‣ 3 Datasets ‣ A Survey on
    Deep learning based Document Image Enhancement")提供了这些数据集的规格，下面我们将详细描述它们。此外，图[5](#S3.F5
    "Figure 5 ‣ 3 Datasets ‣ A Survey on Deep learning based Document Image Enhancement")展示了这些数据集中的图像样本。
- en: '| Dataset | Task | No. of images | Resolution(Pixels) | Real vs. synthetic
    |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 任务 | 图像数量 | 分辨率（像素） | 真实 vs. 合成 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Bishop Bickley diary [[9](#bib.bib9)] | Binarization | 7 | 1050 x 1350 |
    Real |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| Bishop Bickley diary [[9](#bib.bib9)] | 二值化 | 7 | 1050 x 1350 | 真实 |'
- en: '| NoisyOffice [[12](#bib.bib12)] | Denoising | 288 | Variable | Real/Synthetic
    |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| NoisyOffice [[12](#bib.bib12)] | 去噪 | 288 | 可变 | 真实/合成 |'
- en: '| S-MS [[21](#bib.bib21)] | Multiple | 240 | 1001 x 330 | Synthetic |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| S-MS [[21](#bib.bib21)] | 多样本 | 240 | 1001 x 330 | 合成 |'
- en: '| Tobacco 800 [[32](#bib.bib32)] | Denoising | 1290 | (1200x1600) - (2500x3200)
    | Real |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| Tobacco 800 [[32](#bib.bib32)] | 去噪 | 1290 | (1200x1600) - (2500x3200) |
    真实 |'
- en: '| DIBCO’17 | Binarization | 10 | (1050x608) - (2092x951) | Real |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| DIBCO’17 | 二值化 | 10 | (1050x608) - (2092x951) | 真实 |'
- en: '| H-DIBCO’17 | Binarization | 10 | (351x292) - (2439x1229) | Real |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| H-DIBCO’17 | 二值化 | 10 | (351x292) - (2439x1229) | 真实 |'
- en: '| SmartDoc-QA [[44](#bib.bib44)] | Deblurring | 4260 | - | Real |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| SmartDoc-QA [[44](#bib.bib44)] | 去模糊 | 4260 | - | 真实 |'
- en: '| Blurry document images [[23](#bib.bib23)] | Deblurring | 3M train/35K validation
    | 300 x 300 | Synthetic |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 模糊文档图像[[23](#bib.bib23)] | 去模糊 | 3M训练/35K验证 | 300 x 300 | 合成 |'
- en: 'Table 1: Specifications of the datasets used for different document image enhancement
    tasks.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：用于不同文档图像增强任务的数据集规格。
- en: 'Bickley diary [[9](#bib.bib9)]: The images of Bickley diary dataset are taken
    from a photocopy of a diary that is written about 100 years ago. These images
    suffer from different kinds of degradation, such as water stains, ink bleed-through,
    and significant foreground text intensity. This dataset contains 7 document images/pages
    along with the binarized/clean ground truth images.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Bickley日记[[9](#bib.bib9)]：Bickley日记数据集的图像来自大约100年前的一本日记的复印件。这些图像受到各种退化影响，如水渍、墨水渗透和显著的前景文本强度。该数据集包含7张文档图像/页面以及二值化/清洁的地面真实图像。
- en: 'NoisyOffice [[12](#bib.bib12)]: This dataset contains two sets of images: 1)
    Real Noisy Office: it contains 72 grayscale images of scanned noisy images, 2)
    Simulated Noisy Office: it contains 72 grayscale images of scanned simulated noisy
    images for training, validation and test. The images in this dataset contain various
    styles of text, to which synthetic noise has been added to simulate real-world,
    messy artifacts.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: NoisyOffice[[12](#bib.bib12)]：此数据集包含两组图像：1）真实噪声办公室：包含72张扫描的噪声图像的灰度图像，2）模拟噪声办公室：包含72张扫描的模拟噪声图像，用于训练、验证和测试。该数据集中的图像包含各种样式的文本，合成噪声已添加以模拟现实世界中的混乱伪影。
- en: 'S-MS (Synchromedia MultiSpectral Ancient document) [[21](#bib.bib21)]: Multi-spectral
    imaging (MSI) represents an innovative and non-destructive technique for the analysis
    of materials such as ancient documents. They collected a multispectral image database
    of ancient handwritten letters. This database consists of multispectral images
    of 30 real historical handwritten letters. These extremely old documents were
    all written by iron gall ink and date from the 17th to the 20th century. Original
    documents were borrowed from Quebec’s national library and have been imaged using
    a CROMA CX MSI camera. Through this process, they produced 8 images for each document
    resulting in total of 240 images of real documents.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: S-MS（Synchromedia MultiSpectral Ancient document）[[21](#bib.bib21)]：多光谱成像（MSI）是一种创新的非破坏性技术，用于分析如古代文献等材料。他们收集了古代手写信件的多光谱图像数据库。该数据库包含30封真实历史手写信件的多光谱图像。这些极其古老的文献均使用铁墨水书写，时间跨度从17世纪到20世纪。这些原始文献借自魁北克国家图书馆，并使用CROMA
    CX MSI相机进行成像。通过此过程，他们为每个文档生成了8张图像，总共得到240张真实文档图像。
- en: '![Refer to caption](img/8486a869ff8c05cf5fa7263aabf12e73.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/8486a869ff8c05cf5fa7263aabf12e73.png)'
- en: (a) Sample image from Bickley Diary Dataset [[9](#bib.bib9)]
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 来自Bickley Diary数据集的示例图像[[9](#bib.bib9)]
- en: '![Refer to caption](img/457777d54db1ac3762b348ae7ae292f1.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/457777d54db1ac3762b348ae7ae292f1.png)'
- en: (b) Sample image the dataset introduced in [[23](#bib.bib23)]
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 来自[[23](#bib.bib23)]介绍的数据集的示例图像
- en: '![Refer to caption](img/c41d77fe1a37435d9ba96a8729ef7551.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c41d77fe1a37435d9ba96a8729ef7551.png)'
- en: (c) Sample image from DIBCO Dataset [[55](#bib.bib55)]
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 来自DIBCO数据集的示例图像[[55](#bib.bib55)]
- en: '![Refer to caption](img/b248f87b5e329779573491c0d19145ab.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/b248f87b5e329779573491c0d19145ab.png)'
- en: (d) Sample image from SmartDoc-QA Dataset [[44](#bib.bib44)]
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 来自SmartDoc-QA数据集的示例图像[[44](#bib.bib44)]
- en: '![Refer to caption](img/bdaa491c7aa1e83bd8dfadcb52b95a4c.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bdaa491c7aa1e83bd8dfadcb52b95a4c.png)'
- en: (e) Sample image from PHIDB dataset [[43](#bib.bib43)]
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: (e) 来自PHIDB数据集的示例图像[[43](#bib.bib43)]
- en: '![Refer to caption](img/062d5c15cbce412ee77450f94ff60fb4.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/062d5c15cbce412ee77450f94ff60fb4.png)'
- en: (f) Sample image from S-MS Dataset [[21](#bib.bib21)]
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: (f) 来自S-MS数据集的示例图像[[21](#bib.bib21)]
- en: 'Figure 4: Sample images from datasets for document image enhancements tasks.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：用于文档图像增强任务的数据集示例图像。
- en: '![Refer to caption](img/ee45259a0c827b55c34c33dff282be71.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ee45259a0c827b55c34c33dff282be71.png)'
- en: (a) Sample image from Tobacco Dataset [[32](#bib.bib32)]
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 来自Tobacco数据集的示例图像[[32](#bib.bib32)]
- en: '![Refer to caption](img/c71f97d91ccc890bcddc9816b17babb2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c71f97d91ccc890bcddc9816b17babb2.png)'
- en: (b) Sample image from Noisy Office Dataset [[32](#bib.bib32)]
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 来自Noisy Office数据集的示例图像[[32](#bib.bib32)]
- en: '![Refer to caption](img/64cb1e7187bb9c0a2813ddd18ea38489.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/64cb1e7187bb9c0a2813ddd18ea38489.png)'
- en: (c) Sample image from MCS dataset [[20](#bib.bib20)]
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 来自MCS数据集的示例图像[[20](#bib.bib20)]
- en: '![Refer to caption](img/ae8aa67cf1c412f2a896f4b6e2ee2260.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ae8aa67cf1c412f2a896f4b6e2ee2260.png)'
- en: (d) Sample image from H-DIBCO Dataset [[51](#bib.bib51)]
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: (d) 来自H-DIBCO数据集的示例图像[[51](#bib.bib51)]
- en: 'Figure 5: Sample images from datasets for document image enhancements tasks.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：用于文档图像增强任务的数据集样本图像。
- en: 'Tobacco 800 [[32](#bib.bib32)]: This is a publicly available subset of 42 million
    pages of documents that are scanned with various equipment. It contains real-world
    documents with different types of noise and artifact, such as stamps, handwritten
    texts, and ruling lines, on the signatures. Resolutions of documents in Tobacco800
    vary significantly from 150 to 300 DPI and the resolution of the document images
    vary from 1200x1600 to 2500x3200 pixels.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Tobacco 800 [[32](#bib.bib32)]：这是一个公开的子集，包含42百万页文档，使用各种设备扫描。这些文档包含不同类型的噪声和伪影，如印章、手写文本和签名上的划线。Tobacco
    800 中的文档分辨率差异显著，从150到300 DPI不等，文档图像的分辨率从1200x1600到2500x3200像素不等。
- en: 'DIBCO and H-DIBCO: These datasets were introduced for the Document Image Binarization
    Contest since 2009\. There are DIBCO 2009 [[15](#bib.bib15)], H-DIBCO 2010 [[49](#bib.bib49)],
    DIBCO 2011 [[50](#bib.bib50)], H-DIBCO 2012 [[51](#bib.bib51)], DIBCO 2013 [[52](#bib.bib52)],
    H-DIBCO 2014 [[46](#bib.bib46)], H-DIBCO 2016 [[54](#bib.bib54)], DIBCO 2017 [[55](#bib.bib55)],
    H-DIBCO 2014 [[46](#bib.bib46)], H-DIBCO 2018 [[53](#bib.bib53)]. DIBCO datasets
    contain both printed and handwritten document images mainly for the binarization
    task.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: DIBCO 和 H-DIBCO：这些数据集自2009年起用于文档图像二值化竞赛。包括 DIBCO 2009 [[15](#bib.bib15)]、H-DIBCO
    2010 [[49](#bib.bib49)]、DIBCO 2011 [[50](#bib.bib50)]、H-DIBCO 2012 [[51](#bib.bib51)]、DIBCO
    2013 [[52](#bib.bib52)]、H-DIBCO 2014 [[46](#bib.bib46)]、H-DIBCO 2016 [[54](#bib.bib54)]、DIBCO
    2017 [[55](#bib.bib55)]、H-DIBCO 2014 [[46](#bib.bib46)]、H-DIBCO 2018 [[53](#bib.bib53)]。DIBCO
    数据集包含用于二值化任务的打印和手写文档图像。
- en: 'SmartDoc-QA [[44](#bib.bib44)]: This is a dataset for quality assessment of
    smartphone captured document images containing both single and multiple distortions.
    This dataset is created using smartphone’s camera captured document images, under
    varying capture conditions such as light, shadow, different types of blur and
    perspective angles. SmartDoc-QA is categorized in three subsets of documents:
    contemporary documents, old administrative documents and shop’s receipts.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: SmartDoc-QA [[44](#bib.bib44)]：这是一个用于评估智能手机拍摄文档图像质量的数据集，包含单一和多重失真。该数据集使用智能手机相机拍摄的文档图像创建，拍摄条件如光线、阴影、不同类型的模糊和透视角度各异。SmartDoc-QA
    被分类为三个子集：现代文档、旧行政文档和商店收据。
- en: 'Blurry document images (BMVC) [[23](#bib.bib23)]: The training data contains
    3M train and 35k validation 300x300 image patches. Each patch is extracted from
    a different document page and each blur kernel used is unique.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Blurry document images (BMVC) [[23](#bib.bib23)]：训练数据包含3M训练和35k验证的300x300图像补丁。每个补丁均从不同的文档页面中提取，每个模糊内核都是唯一的。
- en: 'Monk Cuper Set (MSC) [[20](#bib.bib20)]: This dataset contains 25 pages sampled
    from real historical documents which are collected from the Cuper book collection
    of the Monk system [[68](#bib.bib68)]. MSC documents suffer from heavy bleed-through
    degradations and textural background.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Monk Cuper Set (MSC) [[20](#bib.bib20)]：该数据集包含从Monk系统的Cuper书籍收藏中收集的真实历史文档中采样的25页。MSC文档受到严重的透视降解和纹理背景影响。
- en: 'Persian heritage image binarization dataset (PHIDB) [[43](#bib.bib43)]: The
    PHIBD 2012 dataset contains 15 historical document images with their corresponding
    ground truth binary images. The historical images in this dataset suffer from
    various types of degradation. In particular two types of foreground text degradation
    are nebulous, and weak strokes/sub-strokes and the background degradation types
    are global bleed-through, local bleed-through, unwanted lines/patterns, and alien
    ink.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Persian heritage image binarization dataset (PHIDB) [[43](#bib.bib43)]：PHIBD
    2012数据集包含15张历史文档图像及其对应的真实二值图像。该数据集中的历史图像遭受各种类型的降解。特别是前景文本降解类型包括模糊、弱笔画/副笔画，背景降解类型包括全球透视、局部透视、非期望的线条/图案和外来墨水。
- en: 4 Metrics
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 个指标
- en: In this section, we describe the evaluations metrics that are used in the literature
    for different document image enhancement tasks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了文献中用于不同文档图像增强任务的评估指标。
- en: •
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Peak signal-to-noise ratio (PSNR): PSNR is a referenced-based metric. It provides
    a pixel-wise evaluation and is capable of indicating the effectiveness of document
    enhancement methods in terms of visual quality. PSNR measures the ratio between
    the maximum possible value of a signal and the power of distorting noise that
    affects the quality. In other words, it measures the closeness of two images.
    The higher the value of PSNR, the higher the similarity of the two images. MAX
    is the maximum possible pixel value of the image. When the pixels are represented
    using 8 bits per sample, MAX is 255\. Given two MxN images, this metric would
    be formulated as follows:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 峰值信噪比（PSNR）：PSNR 是一种基于参考的度量。它提供了逐像素的评估，并能够指示文档增强方法在视觉质量方面的有效性。PSNR 测量信号的最大可能值与影响质量的失真噪声的功率之间的比率。换句话说，它衡量两幅图像的接近程度。PSNR
    值越高，两个图像的相似度越高。MAX 是图像的最大可能像素值。当像素使用每个样本 8 位表示时，MAX 为 255。给定两个 MxN 图像，此度量公式如下：
- en: '|  | $PSNR=10\log(\frac{MAX^{2}}{MSE})$ |  | (2) |'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $PSNR=10\log(\frac{MAX^{2}}{MSE})$ |  | (2) |'
- en: where
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $MSE=\frac{\sum_{x=1}^{M}\sum_{y=1}^{N}(I(x,y)-I^{{}^{\prime}}(x,y))^{2}}{MN}$
    |  | (3) |'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $MSE=\frac{\sum_{x=1}^{M}\sum_{y=1}^{N}(I(x,y)-I^{{}^{\prime}}(x,y))^{2}}{MN}$
    |  | (3) |'
- en: •
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Structural Similarity Index (SSIM) [[72](#bib.bib72)]: SSIM is a reference-based
    metric designed to measure the structural similarity between two images and quantifies
    image quality degradation. SSIM computation requires two images from the same
    image, a reference image and a processed image. It actually measures the perceptual
    difference between two similar images. This metric extracts three key features
    from an image: luminance, contrast, and structure. The comparison between the
    two images is performed on the basis of these three features.'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结构相似性指数（SSIM）[[72](#bib.bib72)]：SSIM 是一种基于参考的度量，旨在衡量两幅图像之间的结构相似性，并量化图像质量退化。SSIM
    计算需要来自相同图像的两幅图像，即参考图像和处理图像。它实际上测量两幅相似图像之间的感知差异。此度量从图像中提取三个关键特征：亮度、对比度和结构。两幅图像的比较基于这三个特征进行。
- en: •
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Character Error Rate (CER): Character Error Rate is computed based on the Levenshtein
    distance. It is the minimum number of character-level operations required to transform
    the ground truth or reference text into the OCR output text. CER is formulated
    as follows:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 字符错误率（CER）：字符错误率基于 Levenshtein 距离计算。它是将地面真相或参考文本转换为 OCR 输出文本所需的最小字符级操作数。CER
    的公式如下：
- en: '|  | $CER=\frac{S+D+I}{N}$ |  | (4) |'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $CER=\frac{S+D+I}{N}$ |  | (4) |'
- en: where $S$ is the number of Substitutions, $D$ is the number of Deletions, $I$
    is the number of Insertions, and $N$ is the number of characters in reference
    or ground truth text.
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 $S$ 是替换的数量，$D$ 是删除的数量，$I$ 是插入的数量，$N$ 是参考或地面真相文本中的字符数量。
- en: 'CER represents the percentage of characters in the reference text that was
    incorrectly predicted or mis-recognized in the OCR output. The lower the CER value
    the better the performance of the OCR model. CER can be normalized to ensure that
    it will not fall out of the 0-100 range due to many insertions. In normalized
    CER, $C$ is the number of correct recognition. Normalized CER is formulated as
    follows:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CER 表示参考文本中在 OCR 输出中被错误预测或误识别的字符百分比。CER 值越低，OCR 模型的性能越好。CER 可以被归一化，以确保它不会因许多插入而超出
    0-100 范围。在归一化的 CER 中，$C$ 是正确识别的数量。归一化 CER 的公式如下：
- en: '|  | $CER_{normalized}=\frac{S+D+I}{S+D+I+C}$ |  | (5) |'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $CER_{normalized}=\frac{S+D+I}{S+D+I+C}$ |  | (5) |'
- en: •
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Word Error Rate (WER): Word Error Rate can be more used for evaluating the
    OCR performance on paragraphs and sentences. WER is formulated in below:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 单词错误率（WER）：单词错误率更适用于评估 OCR 在段落和句子上的性能。WER 的公式如下：
- en: '|  | $WER=\frac{S_{w}+D_{w}+I_{w}}{N}$ |  | (6) |'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $WER=\frac{S_{w}+D_{w}+I_{w}}{N}$ |  | (6) |'
- en: WER is computed similar to CER, but WER operates at word level. It represents
    the number of word substitutions, deletions, or insertions needed to transform
    one sentence into another.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: WER 的计算类似于 CER，但 WER 在词级别操作。它表示将一个句子转换成另一个句子所需的单词替换、删除或插入的数量。
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'F-measure [[52](#bib.bib52)]: The F-measure score is the harmonic mean of the
    precision and recall. Precision is the positive predictive value, and recall aka
    sensitivity is used in binary classification. F-measure is formulated as follows:'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: F-度量[[52](#bib.bib52)]：F-度量分数是精确度和召回率的调和均值。精确度是正预测值，召回率也称为灵敏度，用于二元分类。F-度量的公式如下：
- en: '|  | $FM=\frac{2\times Recall\times Precision}{Recall+Precision}$ |  | (7)
    |'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $FM=\frac{2\times Recall\times Precision}{Recall+Precision}$ |  | (7)
    |'
- en: where
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中
- en: '|  | $Recall=\frac{TP}{TP+FN}$ |  | (8) |'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $Recall=\frac{TP}{TP+FN}$ |  | (8) |'
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (9) |'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (9) |'
- en: TP, FP, FN denote the True Positive, False Positive and False Negative values,
    respectively.
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TP、FP、FN 分别表示真正例、假正例和假负例值。
- en: •
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Pseudo-FMeasure ($F_{ps}$) [[52](#bib.bib52)]: $F_{ps}$ is introduced in [[45](#bib.bib45)]
    and it utilizes pseudo-recall Rps and pseudo-precision $P_{ps}$. It follows the
    same formula as F-Measure explained above and is particularly used for the binarization
    task.'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '伪-F测量 ($F_{ps}$) [[52](#bib.bib52)]: $F_{ps}$ 在 [[45](#bib.bib45)] 中引入，它利用伪召回率
    Rps 和伪精确度 $P_{ps}$。它遵循与上述 F-测量相同的公式，特别用于二值化任务。'
- en: In the case of pseudo-recall, the weights of the ground truth(GT) foreground
    are normalized according to the local stroke width. Generally, those weights are
    between [0,1]. In the case of pseudo-precision, the weights are constrained within
    an area that expands to the GT background taking into account the stroke width
    of the nearest $GT$ component. Inside this area, the weights are greater than
    one (generally between (1,2]) while outside this area they are equal to one.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在伪召回率的情况下，地面真实（GT）前景的权重根据局部笔画宽度进行归一化。通常，这些权重在 [0,1] 之间。对于伪精确度，权重限制在一个扩展到 GT
    背景的区域内，考虑到最近的 $GT$ 组件的笔画宽度。在这个区域内，权重大于一（通常在 (1,2] 之间），而在这个区域之外则等于一。
- en: •
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Distance Reciprocal Distortion Metric (DRD) [[52](#bib.bib52)]: DRD metric
    is used to measure the visual distortion in binary document images [[39](#bib.bib39)].
    It correlates with the human visual perception and it measures the distortion
    for all pixels as follows:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '距离倒数失真度量 (DRD) [[52](#bib.bib52)]: DRD 度量用于测量二值文档图像中的视觉失真 [[39](#bib.bib39)]。它与人类视觉感知相关，并对所有像素的失真度进行如下测量：'
- en: '|  | $DRD=\frac{\sum_{k=1}^{S}DRD_{k}}{NUBN}$ |  | (10) |'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $DRD=\frac{\sum_{k=1}^{S}DRD_{k}}{NUBN}$ |  | (10) |'
- en: where NUBN is the number of the non-uniform 8x8 blocks in the GT image, and
    $DRD_{k}$ is the distortion of the kth flipped pixel that is calculated using
    a 5x5 normalized weight matrix $W_{Nm}$ as defined in [[39](#bib.bib39)]. $DRD_{k}$
    equals to the weighted sum of the pixels in the 5x5 block of the GT that differ
    from the centered kth flipped pixel at $(x,y)$ in the binarization result image
    (equation [11](#S4.E11 "In 7th item ‣ 4 Metrics ‣ A Survey on Deep learning based
    Document Image Enhancement")).
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 NUBN 是 GT 图像中非均匀 8x8 块的数量，而 $DRD_{k}$ 是第 k 个翻转像素的失真度，该失真度使用一个 5x5 的归一化权重矩阵
    $W_{Nm}$ 计算，如 [[39](#bib.bib39)] 所定义的。$DRD_{k}$ 等于 GT 图像中与中心第 k 个翻转像素 $(x,y)$
    在二值化结果图像中不同的 5x5 块像素的加权和（方程 [11](#S4.E11 "在第 7 项 ‣ 4 指标 ‣ 基于深度学习的文档图像增强综述")）。
- en: '|  | $DRD_{k}=\sum_{i=-2}^{2}\sum_{j=-2}^{2}\left&#124;{GT_{k}(i,j)-B_{k}(x,y)}\right&#124;\times
    W_{Nm}(i,j)$ |  | (11) |'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '|  | $DRD_{k}=\sum_{i=-2}^{2}\sum_{j=-2}^{2}\left&#124;{GT_{k}(i,j)-B_{k}(x,y)}\right&#124;\times
    W_{Nm}(i,j)$ |  | (11) |'
- en: 5 Document Image Enhancement Methods
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 种文档图像增强方法
- en: In this section, we describe the main deep learning based methods for document
    image enhancement and discuss their features, challenges, and limitations. Most
    of these works focused on multiple tasks, therefore in this section we discuss
    the document enhancement methods chronologically. Table [3](#S5.T3 "Table 3 ‣
    5 Document Image Enhancement Methods ‣ A Survey on Deep learning based Document
    Image Enhancement") summarizes the advantages, disadvantages, and results of these
    methods. Below, we describe these methods in more details.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了基于深度学习的文档图像增强的主要方法，并讨论它们的特点、挑战和局限性。这些工作大多集中于多个任务，因此在本节中我们按时间顺序讨论文档增强方法。表
    [3](#S5.T3 "表 3 ‣ 5 种文档图像增强方法 ‣ 基于深度学习的文档图像增强综述") 总结了这些方法的优缺点和结果。以下是这些方法的详细描述。
- en: '| Methods | Document Image Enhancement Tasks | Document Type |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 文档图像增强任务 | 文档类型 |'
- en: '| Binarization | Deblur | Denoise | Defade | Watermark Removal | Shadow Removal
    | Handwritten | Printed |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 二值化 | 去模糊 | 去噪声 | 去褪色 | 水印去除 | 阴影去除 | 手写 | 印刷 |'
- en: '| Gangeh et al. [[13](#bib.bib13)] | - | ✓ | ✓ | ✓ | ✓ | - | - | - |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Gangeh 等 [[13](#bib.bib13)] | - | ✓ | ✓ | ✓ | ✓ | - | - | - |'
- en: '| Zhao et al. [[74](#bib.bib74)] | - | ✓ | ✓ | - | - | - | - | - |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| Zhao 等 [[74](#bib.bib74)] | - | ✓ | ✓ | - | - | - | - | - |'
- en: '| Sharma et al. [[60](#bib.bib60)] | - | ✓ | - | ✓ | ✓ | - | - | ✓ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Sharma 等 [[60](#bib.bib60)] | - | ✓ | - | ✓ | ✓ | - | - | ✓ |'
- en: '| Lin et al. [[36](#bib.bib36)] | - | - | - | - | - | ✓ | ✓ | - |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| Lin 等 [[36](#bib.bib36)] | - | - | - | - | - | ✓ | ✓ | - |'
- en: '| Souibgui et al. [[61](#bib.bib61)] | - | ✓ | - | - | ✓ | - | ✓ | ✓ |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| Souibgui et al. [[61](#bib.bib61)] | - | ✓ | - | - | ✓ | - | ✓ | ✓ |'
- en: '| Gangeh et al. [[14](#bib.bib14)] | - | ✓ | - | - | ✓ | - | - | ✓ |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| Gangeh et al. [[14](#bib.bib14)] | - | ✓ | - | - | ✓ | - | - | ✓ |'
- en: '| Hradiš et al. [[23](#bib.bib23)] | - | ✓ | - | - | - | - | - | - |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Hradiš et al. [[23](#bib.bib23)] | - | ✓ | - | - | - | - | - | - |'
- en: '| Jemni et al. [[25](#bib.bib25)] | ✓ | - | - | - | - | - | ✓ | - |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| Jemni et al. [[25](#bib.bib25)] | ✓ | - | - | - | - | - | ✓ | - |'
- en: '| Xu et al. [[73](#bib.bib73)] | ✓ | - | - | - | - | - | - | ✓ |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. [[73](#bib.bib73)] | ✓ | - | - | - | - | - | - | ✓ |'
- en: '| Souibgui et al. [[62](#bib.bib62)] | - | ✓ | - | - | - | ✓ | - | ✓ |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| Souibgui et al. [[62](#bib.bib62)] | - | ✓ | - | - | - | ✓ | - | ✓ |'
- en: '| Calvo-Zaragoza et al. [[6](#bib.bib6)] | ✓ | - | - | - | - | - | ✓ | ✓ |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| Calvo-Zaragoza et al. [[6](#bib.bib6)] | ✓ | - | - | - | - | - | ✓ | ✓ |'
- en: '| Dey et al. [[10](#bib.bib10)] | ✓ | - | ✓ | - | - | - | - | ✓ |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| Dey et al. [[10](#bib.bib10)] | ✓ | - | ✓ | - | - | - | - | ✓ |'
- en: '| Li et al. [[33](#bib.bib33)] | ✓ | - | - | - | - | - | ✓ | ✓ |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[33](#bib.bib33)] | ✓ | - | - | - | - | - | ✓ | ✓ |'
- en: (a) Tasks and document types handled by the of main methods reviewed in this
    paper
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 本文回顾的主要方法处理的任务和文档类型
- en: '| Methods | GAN | CNN | Paired vs. unpaired supervision |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | GAN | CNN | 配对与未配对监督 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Gangeh et al. [[13](#bib.bib13)] | ✓ | - | Unpaired |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| Gangeh et al. [[13](#bib.bib13)] | ✓ | - | 未配对 |'
- en: '| Zhao et al. [[74](#bib.bib74)] | - | ✓ | Paired |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. [[74](#bib.bib74)] | - | ✓ | 配对 |'
- en: '| Sharma et al. [[60](#bib.bib60)] | ✓ | - | Unpaired |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| Sharma et al. [[60](#bib.bib60)] | ✓ | - | 未配对 |'
- en: '| Lin et al. [[36](#bib.bib36)] | ✓ | - | Paired |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| Lin et al. [[36](#bib.bib36)] | ✓ | - | 配对 |'
- en: '| Souibgui et al. [[61](#bib.bib61)] | ✓ | - | Paired |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| Souibgui et al. [[61](#bib.bib61)] | ✓ | - | 配对 |'
- en: '| Gangeh et al. [[14](#bib.bib14)] | - | ✓ | Paired |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| Gangeh et al. [[14](#bib.bib14)] | - | ✓ | 配对 |'
- en: '| Hradiš et al. [[23](#bib.bib23)] | - | ✓ | Paired |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| Hradiš et al. [[23](#bib.bib23)] | - | ✓ | 配对 |'
- en: '| Jemni et al. [[25](#bib.bib25)] | ✓ | - | Paired |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| Jemni et al. [[25](#bib.bib25)] | ✓ | - | 配对 |'
- en: '| Xu et al. [[73](#bib.bib73)] | ✓ | - | Paired |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| Xu et al. [[73](#bib.bib73)] | ✓ | - | 配对 |'
- en: '| Souibgui et al. [[62](#bib.bib62)] | ✓ | - | Paired |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| Souibgui et al. [[62](#bib.bib62)] | ✓ | - | 配对 |'
- en: '| Calvo-Zaragoza et al. [[6](#bib.bib6)] | - | ✓ | Paired |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| Calvo-Zaragoza et al. [[6](#bib.bib6)] | - | ✓ | 配对 |'
- en: '| Dey et al. [[10](#bib.bib10)] | - | ✓ | Paired |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| Dey et al. [[10](#bib.bib10)] | - | ✓ | 配对 |'
- en: '| Li et al. [[33](#bib.bib33)] | - | ✓ | Paired |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| Li et al. [[33](#bib.bib33)] | - | ✓ | 配对 |'
- en: (b) Methodologies used in the reviewed methods.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 回顾方法中使用的方法论。
- en: 'Table 2: Description of main methods reviewed in this paper.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：本文回顾的主要方法描述。
- en: The method introduced in [[23](#bib.bib23)] is proposed for document image deblurring
    problem. The authors proposed a small and computationally efficient convolutional
    neural network model to deblur images without assuming any priors. In particular
    the authors focused on a combination of realistic de-focus blur and camera shake
    blur. They demonstrated that the proposed network significantly outperform existing
    blind deconvolution methods both in terms of image quality, PSNR, and OCR accuracy,
    CER. The proposed model can also be used on mobile devices as well.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 引入的方法 [[23](#bib.bib23)] 用于文档图像去模糊问题。作者提出了一个小型且计算效率高的卷积神经网络模型，以去模糊图像而不假设任何先验条件。特别是，作者专注于现实的去焦模糊和相机抖动模糊的组合。他们展示了所提出的网络在图像质量、PSNR和OCR准确性、CER方面显著超越了现有的盲解卷积方法。该模型也可以在移动设备上使用。
- en: '| Methods | Advantages | Disadvantages | Results |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 优势 | 劣势 | 结果 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Gangeh et al. [[13](#bib.bib13)] | - Handles multiple noises including salt
    and pepper noise, faded, blurred, and watermarked documents in an end-to-end manner.
    - It does not rely on paired document images. | - Computationally complex. | -
    Method has best results in terms of PSNR and OCR as compared to previous three
    methods. |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| Gangeh et al. [[13](#bib.bib13)] | - 处理多种噪声，包括盐和胡椒噪声、褪色、模糊和水印文档，且以端到端的方式进行。
    - 不依赖于配对的文档图像。 | - 计算复杂。 | - 相比于前三种方法，该方法在PSNR和OCR方面表现最好。 |'
- en: '| Zhao et al. [[74](#bib.bib74)] | - Method is fast and easy to implement.
    | - Inadequate qualitative and qualitative results. | - Marginal PSNR improvement.
    |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| Zhao et al. [[74](#bib.bib74)] | - 方法快速且易于实现。 | - 定性和定量结果不足。 | - PSNR改进有限。
    |'
- en: '| Sharma et al. [[60](#bib.bib60)] | - Adaptable for both paired and unpaired
    supervision scenarios. | - | - Marginal improvement in terms of PSNR. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| Sharma et al. [[60](#bib.bib60)] | - 适用于配对和未配对监督场景。 | - | - PSNR方面的边际改进。
    |'
- en: '| Lin et al. [[36](#bib.bib36)] | - First deep learning-based approach for
    shadow removal. - It works on both gray-scale and RGB images. | - Computationally
    complex. - It does not work well on images with complex background and layouts.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '| Lin 等人 [[36](#bib.bib36)] | - 首个基于深度学习的阴影去除方法。 - 适用于灰度图像和 RGB 图像。 | - 计算复杂。
    - 对背景和布局复杂的图像效果不佳。'
- en: '- It works well on partially shadowed documents only. | - It achieves the best
    results in terms of PSNR/SSIM compared to four previous work when evaluated on
    five different datasets. - It also generalizes relatively well on real-world images.
    |'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '- 仅在部分阴影文档上表现良好。 | - 在 PSNR/SSIM 方面相比于四项之前的工作取得了最佳结果，在五个不同数据集上的评估表现优异。 - 在真实世界图像上的泛化能力也相对较好。
    |'
- en: '| Souibgui et al. [[61](#bib.bib61)] | - Flexible architecture could be used
    for other document degradation problems. - First work on dense watermark and stamp
    removal problems.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '| Souibgui 等人 [[61](#bib.bib61)] | - 灵活的架构可用于其他文档退化问题。 - 首个研究密集水印和印章去除问题的工作。'
- en: '- Generalize well on real-world images.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '- 在真实世界图像上的泛化能力良好。'
- en: '- Pre-trained models are publicly available. | - Computationally complex. -
    It needs a threshold to be pre-determined and needs to be tuned per image which
    makes this method less practical. | Binarization: Achieves best results in terms
    of PSNR, $F_{measure}$, $F_{ps}$ and DRD compared to top five competitors. Watermark:
    Achieves best results in terms of PSNR/SSIM compared to three previous work.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '- 公开可用的预训练模型。 | - 计算复杂。 - 需要预先确定阈值，并且需要根据每张图像进行调节，这使得该方法不够实用。 | 二值化：在 PSNR、$F_{measure}$、$F_{ps}$
    和 DRD 方面表现最佳，相比于前五名竞争者。 水印：在 PSNR/SSIM 方面表现最佳，相比于之前的三项工作。'
- en: 'Deblur: Achieves best results in terms of PSNR compared to two previous work.
    |'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 去模糊：在 PSNR 方面相比于之前的两项工作取得了最佳结果。 |
- en: '| Gangeh et al. [[14](#bib.bib14)] | - Works on both gray-scale and RGB watermarks.
    - Works on blurry images with various intensity. | - Inadequate quantitative evaluation
    and comparison with previous work. | - Effectively removes watermark and blur.
    - Improved OCR on a small test set of nine images. |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| Gangeh 等人 [[14](#bib.bib14)] | - 适用于灰度和 RGB 水印。 - 适用于各种强度的模糊图像。 | - 定量评估和与之前工作的比较不足。
    | - 有效去除水印和模糊。 - 在九张图像的小测试集上提高了 OCR 性能。 |'
- en: '| Hradiš et al. [[23](#bib.bib23)] | - Small and computationally efficient
    network. - Can be used on mobile devices. | - Adds ringing artifacts in some situations.
    - Does not work well on uncommon words when the image is severely blurred. | -
    Outperforms other methods in terms of PSNR and Character Error Rate compared to
    previous four work. |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| Hradiš 等人 [[23](#bib.bib23)] | - 小型且计算高效的网络。 - 可用于移动设备。 | - 在某些情况下添加了响铃伪影。
    - 当图像严重模糊时，对不常见的词汇效果不佳。 | - 在 PSNR 和字符错误率方面优于之前的四项工作。 |'
- en: '| Xu et al. [[73](#bib.bib73)] | - Computationally efficient network. - It
    deblurs and super-resolves simultaneously. | - Does not generalize well for generic
    images. - OCR performance evaluation is ignored and only visual quality of the
    documents are evaluated. | - Performs favorably against previous work on both
    synthetic and real-world datasets. |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| Xu 等人 [[73](#bib.bib73)] | - 计算高效的网络。 - 同时进行去模糊和超分辨率处理。 | - 对通用图像的泛化能力较差。
    - 忽略了 OCR 性能评估，仅评估了文档的视觉质量。 | - 在合成和真实世界数据集上表现优于之前的工作。 |'
- en: '| Souibgui et al. [[62](#bib.bib62)] | - It handles multiple camera distortions.
    - It incorporates a text recognizer for generating more legible images. | - Model
    only processed and trained on single lines and can not handle full pages. | -
    Achieves best results in terms of Character Error Rate and second best in terms
    of PSNR/SSIM compared to previous three work. |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| Souibgui 等人 [[62](#bib.bib62)] | - 处理多种相机失真。 - 包含一个文本识别器以生成更清晰的图像。 | - 仅处理和训练了单行文本，无法处理整页内容。
    | - 在字符错误率方面表现最佳，在 PSNR/SSIM 方面排名第二，相比于之前的三项工作。 |'
- en: 'Table 3: Comparison of document image enhancement methods.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：文档图像增强方法的比较。
- en: In another document image deblurring work [[73](#bib.bib73)], the authors proposed
    an algorithm to directly restore a high-resolution de-blurred image from a blurry
    low-resolution input. Other deblurring methods such as Hradis et al. [[23](#bib.bib23)]
    cannot be easily extended for joint super-resolution and deblurring tasks. This
    work focuses on blurry face and blurry document images distributions and a multi-class
    GAN model was developed to learn a category-specific prior and process multi-class
    image restoration tasks, using a single generator network. The authors employed
    a deep CNN architecture proposed by Hradis et al. [[23](#bib.bib23)] in an adversarial
    setting. Unlike Hradis et al., in this work the generator network contains upsampling
    layers, which are fractionally-strided convolutional layers aka deconvolution
    layers. The generator first upsamples low-resolution blurry images, and then performs
    convolutions to generate clear images thus the output would be both super-resolved
    and deblurred. Since their model has a discriminator network in addition to the
    generator network, it is more complex and has more parameters compared to model
    proposed in [[23](#bib.bib23)].
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项文档图像去模糊的工作[[73](#bib.bib73)]中，作者提出了一种算法，直接从模糊的低分辨率输入中恢复高分辨率去模糊图像。其他去模糊方法，如
    Hradis 等[[23](#bib.bib23)]，不能轻易扩展用于联合超分辨率和去模糊任务。这项工作专注于模糊的人脸和模糊的文档图像分布，并开发了一个多类
    GAN 模型来学习类别特定的先验并处理多类图像恢复任务，使用单个生成器网络。作者在对抗设置中使用了 Hradis 等[[23](#bib.bib23)] 提出的深度
    CNN 架构。与 Hradis 等不同的是，这项工作中的生成器网络包含上采样层，即分数步长卷积层，也称为反卷积层。生成器首先对低分辨率模糊图像进行上采样，然后执行卷积以生成清晰图像，从而输出图像将同时具有超分辨率和去模糊效果。由于他们的模型除了生成器网络外还包含一个判别器网络，因此相较于[[23](#bib.bib23)]中提出的模型更复杂，参数更多。
- en: The visual quality of the generated images were evaluated in terms of PSNR and
    SSIM but the deblurred document images were not evaluated in terms of OCR performance
    and no Character Error Rate or Word Error Rate which are OCR performance evaluation
    metrics are reported. In terms of PSNR/SSIM, this work performs favorably against
    previous work on both synthetic and real-world datasets.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图像的视觉质量通过 PSNR 和 SSIM 进行了评估，但去模糊的文档图像在 OCR 性能方面没有进行评估，也没有报告 OCR 性能评估指标，如字符错误率或词错误率。在
    PSNR/SSIM 方面，这项工作在合成数据集和真实数据集上都优于以前的工作。
- en: One limitation of this work is that since the model is trained on multi-class
    images, it is essentially designed to approximate the mixture distribution of
    these two classes of images and when this mixture distribution becomes too complex,
    it is difficult to learn a unified model to cover the diversity of all image classes.
    Therefore, this method is less effective for generic images.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作的一个局限性是，由于模型是在多类图像上训练的，它本质上是为了近似这两类图像的混合分布。当这种混合分布变得过于复杂时，很难学习一个统一的模型来覆盖所有图像类别的多样性。因此，这种方法对于通用图像的效果较差。
- en: Authors in  [[66](#bib.bib66)] focused on the degraded historical manuscript
    images binarization, and formulated binarization task as a pixel classification
    learning task. They developed a Fully Convolutional Network (FCN) architecture
    that operates at multiple image scales, including full resolution. The authors
    claimed that the proposed binarization technique can also be applied to different
    domains such as Palm Leaf Manuscripts with good performance.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 论文[[66](#bib.bib66)]的作者专注于退化历史手稿图像的二值化，并将二值化任务制定为像素分类学习任务。他们开发了一个在多个图像尺度（包括全分辨率）上运行的全卷积网络（FCN）架构。作者声称，所提出的二值化技术也可以应用于不同领域，如棕榈叶手稿，并且效果良好。
- en: Zhao et. al. [[74](#bib.bib74)] investigated the denoising and deblurring problems
    and proposed a method for document image restoration called Skip-Connected Deep
    Convolutional Autoencoder (SCDCA) which is based on residual learning. They employed
    two types of skip connections, identity mapping between convolution layers inspired
    by residual blocks, and another is defined to connect the input to the output
    directly. These connections assist the network to learn the residual content between
    the noisy and clean images instead of learning an ordinary transformation function.
    The proposed network was inspired by [[23](#bib.bib23)] which is a 15-layer CNN.
    Compared to method in  [[23](#bib.bib23)], the authors added batch normalization [[24](#bib.bib24)]
    and skip-connections [[19](#bib.bib19)] to accelerate the model convergence of
    the model and boost the performance.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人[[74](#bib.bib74)] 研究了去噪和去模糊问题，并提出了一种基于残差学习的文档图像恢复方法，称为 Skip-Connected
    Deep Convolutional Autoencoder (SCDCA)。他们采用了两种类型的跳跃连接，一种是基于残差块的卷积层之间的恒等映射，另一种是直接将输入与输出连接。这些连接帮助网络学习噪声图像与清晰图像之间的残差内容，而不是学习普通的变换函数。该网络的灵感来源于[[23](#bib.bib23)]，这是一个15层的卷积神经网络。与[[23](#bib.bib23)]中的方法相比，作者添加了批量归一化[[24](#bib.bib24)]和跳跃连接[[19](#bib.bib19)]，以加速模型收敛并提升性能。
- en: In [[60](#bib.bib60)], the authors cast the image restoration problem as an
    image-to-image translation task i.e, translating a document from noisy domain
    (*i.e.,* background noise, blurred, faded, watermarked) to a target clean document
    using a GAN approach. To do so, they employed CycleGAN model which is an unpaired
    image-to-image translation network, for cleaning the noisy documents. They also
    synthetically created a document dataset for watermark removal and defading problems
    by inserting logos as watermarks and applying fading techniques on Google News
    dataset [[67](#bib.bib67)] of documents.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[60](#bib.bib60)]中，作者将图像恢复问题视为图像到图像的转换任务，即将文档从噪声域（*例如* 背景噪声、模糊、褪色、水印）转换为目标清晰文档，使用
    GAN 方法。为此，他们采用了 CycleGAN 模型，这是一个未配对的图像到图像转换网络，用于清理噪声文档。他们还通过将徽标作为水印插入以及在 Google
    新闻数据集[[67](#bib.bib67)]上应用褪色技术，合成了用于去水印和去褪色问题的文档数据集。
- en: Authors in [[14](#bib.bib14)] proposed an end-to-end document enhancement pipeline
    which takes in blurry and watermarked document images and produces clean documents.
    They trained an auto-encoder model that works on different noise levels of documents.
    They adopted the neural network architecture described in [[40](#bib.bib40)] called
    REDNET and designed a REDNET with 15 convolutional layers and 15 deconvolutional
    layers, including 8 symmetric skip connections between alternate convolutional
    layers and the mirrored deconvolutional layers. The advantage of this method compared
    to fully convolutional network is that pooling and un-pooling, which tend to eliminate
    image details, is avoided for low-level image tasks such as image restoration.
    This results in higher resolution outputs. The key differences of this work from [[74](#bib.bib74)]
    is the use of larger dataset and training a blind model.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '[[14](#bib.bib14)]的作者提出了一种端到端的文档增强管道，该管道接收模糊和加水印的文档图像并生成干净的文档。他们训练了一个自编码器模型，能够处理不同噪声水平的文档。他们采用了[[40](#bib.bib40)]中描述的神经网络架构，称为
    REDNET，并设计了一个具有15层卷积层和15层反卷积层的 REDNET，包括8个对称的跳跃连接，连接交替的卷积层和镜像的反卷积层。与完全卷积网络相比，该方法的优势在于避免了池化和反池化，这些操作通常会消除图像细节，适用于图像恢复等低级图像任务，从而实现更高分辨率的输出。这项工作的关键区别在于使用了更大的数据集并训练了一个盲模型。'
- en: In [[67](#bib.bib67)] authors developed convolutional auto-encoders to learn
    an end-to-end map from an input image to its selectional output, in which the
    activations indicate the likelihood of pixels to be either foreground or background.
    Once trained, this model can be applied to documents to be binarized and then
    a global threshold will be applied. This approach has proven to outperform existing
    binarization strategies in a number of document types.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[67](#bib.bib67)]中，作者开发了卷积自编码器，以学习从输入图像到其选择性输出的端到端映射，其中激活值表示像素是前景还是背景的可能性。训练完成后，该模型可以应用于文档进行二值化，然后应用全局阈值。这种方法已被证明在多种文档类型中优于现有的二值化策略。
- en: In DE-GAN [[61](#bib.bib61)], the authors proposed an end-to-end framework called
    Document Enhancement Generative Adversarial Networks. This network is based on
    conditional GANs and cGANs, a network to restore severely degraded document images.
    The tasks that are studied in this paper are document clean up, binarization,
    deblurring and watermark removal. Due to unavailability of a dataset for the watermark
    removal task, the authors synthetically created a watermark dataset including
    the watermarked images and their clean ground truth.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在DE-GAN中[[61](#bib.bib61)]，作者提出了一种名为文档增强生成对抗网络的端到端框架。该网络基于条件GAN和cGAN，用于恢复严重退化的文档图像。本文研究的任务包括文档清理、二值化、去模糊和水印去除。由于缺乏用于水印去除任务的数据集，作者合成创建了一个水印数据集，包括带水印的图像及其干净的真实图像。
- en: Authors in [[36](#bib.bib36)] proposed the Background Estimation Document Shadow
    Removal Network (BEDSR-Net) which is the first deep network designed for document
    image shadow removal. They designed a background estimation module for extracting
    the global background color of the document. During the process of estimating
    the background color, this module learns information about the spatial distribution
    of background and also the non-background pixels. They created an attention map
    through encoding this information. Having estimated the global background color
    and the attention map, the shadow removal network can now effectively recover
    the shadow-free document image. BEDSR-Net can fail in some situations including
    when there is no single dominant color, such as a paper entirely with a color
    gradient and another case is when the document is entirely shadowed, or multiple
    shadows were formed by multiple light sources.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[36](#bib.bib36)]中，作者提出了背景估计文档阴影去除网络（BEDSR-Net），这是第一个针对文档图像阴影去除设计的深度网络。他们设计了一个背景估计模块，用于提取文档的全局背景颜色。在估计背景颜色的过程中，该模块学习背景的空间分布信息以及非背景像素的信息。他们通过编码这些信息创建了一个注意力图。通过估计全局背景颜色和注意力图，阴影去除网络现在可以有效地恢复无阴影的文档图像。BEDSR-Net在某些情况下可能会失败，包括当没有单一主导颜色时，例如整个纸张有颜色渐变，以及当文档完全被阴影覆盖，或多个光源形成了多个阴影的情况。
- en: In another work [[62](#bib.bib62)] the authors focused on documents that are
    digitized using smart phone’s cameras. They stated that these types of digitized
    documents are highly vulnerable to capturing various distortions including but
    not limited to perspective angle, shadow, blur, warping, etc. The authors proposed
    a conditional generative adversarial network that maps the distorted images from
    its domain into a readable domain. This model integrates a recognizer in the discriminator
    part for better distinguishing the generated document images.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项研究中[[62](#bib.bib62)]，作者关注使用智能手机相机数字化的文档。他们指出，这些类型的数字化文档极易受到各种失真的影响，包括但不限于透视角度、阴影、模糊、变形等。作者提出了一种条件生成对抗网络，将失真的图像从其领域映射到可读的领域。该模型在鉴别器部分集成了一个识别器，以更好地区分生成的文档图像。
- en: In another study [[13](#bib.bib13)], an end-to-end unsupervised deep learning
    model to remove multiple types of noise, including salt & pepper noise, blurred
    and/or faded text, and watermarks from documents was proposed. In particular they
    proposed a unified architecture by integrating deep mixture of experts [[70](#bib.bib70)]
    with a cycle-consistent GAN as the base network for document image blind denoising
    problem.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项研究中[[13](#bib.bib13)]，提出了一种端到端的无监督深度学习模型，用于去除多种类型的噪声，包括椒盐噪声、模糊和/或褪色的文本以及文档上的水印。特别是，他们通过将深度专家混合[[70](#bib.bib70)]与循环一致GAN集成，提出了一种统一架构作为文档图像盲目去噪问题的基础网络。
- en: In [[10](#bib.bib10)], authors target document image cleanup problem on embedded
    applications such as smartphone apps, which usually have memory, energy, and latency
    limitations. They proposed a light-weight encoder-decoder CNN architecture, incorporated
    with perceptual loss. They proved that in terms of the number of parameters and
    product-sum operations, their models are 65-1030 and 3-27 times, respectively,
    smaller than existing SOTA document enhancement models.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[10](#bib.bib10)]中，作者针对嵌入式应用（如智能手机应用）中的文档图像清理问题，这些应用通常存在内存、能源和延迟限制。他们提出了一种轻量级的编码器-解码器CNN架构，并结合了感知损失。他们证明了在参数数量和乘积和操作方面，他们的模型比现有的SOTA文档增强模型分别小65-1030倍和3-27倍。
- en: In another work [[25](#bib.bib25)], authors focused on enhancing handwritten
    documents and proposed an end-to-end GAN-based architecture to recover the degraded
    documents. Unlike most document binarization methods, which only attempt to improve
    the visual quality of the degraded document, the proposed architecture integrates
    a handwritten text recognizer that promotes the generated document image to be
    also more legible. This approach is the first work to use the text information
    while binarizing handwritten documents. They performed experiments on degraded
    Arabic and Latin handwritten documents and showed that their model improves both
    the visual quality and the legibility of the degraded document images.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项工作中 [[25](#bib.bib25)]，作者专注于增强手写文档，并提出了一种基于生成对抗网络（GAN）的端到端架构来恢复退化的文档。与大多数文档二值化方法不同，这些方法仅尝试改善退化文档的视觉质量，所提出的架构整合了手写文本识别器，使得生成的文档图像更加清晰易读。这种方法是首个在对手写文档进行二值化时利用文本信息的工作。他们对退化的阿拉伯文和拉丁文手写文档进行了实验，并展示了他们的模型在改善退化文档图像的视觉质量和可读性方面的效果。
- en: In [[33](#bib.bib33)], authors proposed a document binarization method called
    SauvolaNet. They investigated the classic Sauvola [[58](#bib.bib58)] document
    binarization method from the deep learning perspective and proposed a multi-window
    Sauvola model. They also introduced an attention mechanism to automatically estimate
    the required Sauvola window sizes for each pixel location therefore could effectively
    estimate the Sauvola threshold. The proposed network has three modules, Multi-Window
    Sauvola, Pixelwise Window Attention, and Adaptive Sauolva Threshold. The Multi-Window
    Sauvola module reflects the classic Sauvola but with trainable parameters and
    multi-window settings. The next module which is Pixelwise Window Attention that
    is in charge of estimating the preferred window sizes for each pixel. The other
    module, Adaptive Sauolva Threshold, combines the outputs from the other two modules
    and predicts the final adaptive threshold for each pixel. The SauvolaNet model
    significantly reduces the number of required network parameters and achieves SOTA
    performance for document binarization task.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [[33](#bib.bib33)]中，作者提出了一种名为 SauvolaNet 的文档二值化方法。他们从深度学习的角度研究了经典的 Sauvola [[58](#bib.bib58)]
    文档二值化方法，并提出了一个多窗口 Sauvola 模型。他们还引入了一种注意力机制，以自动估计每个像素位置所需的 Sauvola 窗口大小，从而有效地估计
    Sauvola 阈值。所提出的网络包含三个模块：多窗口 Sauvola、逐像素窗口注意力和自适应 Sauvola 阈值。多窗口 Sauvola 模块反映了经典的
    Sauvola，但具有可训练的参数和多窗口设置。下一个模块是逐像素窗口注意力，负责估计每个像素的首选窗口大小。另一个模块，自适应 Sauvola 阈值，结合了其他两个模块的输出，并预测每个像素的最终自适应阈值。SauvolaNet
    模型显著减少了所需的网络参数数量，并在文档二值化任务中实现了 SOTA 性能。
- en: 6 Open Problems and Future Directions
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 个开放问题及未来方向
- en: In this section, we present open problems in this area and provide several directions
    for the future work. Document image enhancement tasks are far from solved and
    even some tasks are either not studied or studied in a very limited fashion. We
    discuss these problems and future work below.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提出了该领域中的开放问题，并提供了未来工作的几个方向。文档图像增强任务仍未解决，甚至有些任务尚未研究或仅以非常有限的方式进行研究。我们在下面讨论了这些问题和未来的工作方向。
- en: 6.1 Overexposure and underexposure correction tasks
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 过度曝光和曝光不足修正任务
- en: Overexposure problem occurs when too much light is captured while digitizing
    the document, mostly when the capturing device is a mobile phone and camera flash
    adds too much reflection or glare to the image (Figure [6(a)](#S6.F6.sf1 "In Figure
    6 ‣ 6.1 Overexposure and underexposure correction tasks ‣ 6 Open Problems and
    Future Directions ‣ A Survey on Deep learning based Document Image Enhancement")).
    This problem has received limited attention even in the image and photo enhancement
    domain [[1](#bib.bib1), [5](#bib.bib5)], and to the best of our knowledge no study
    has tried to address this problem for the document images. To address this issue
    with a deep learning based approach, training and testing datasets are required
    to be collected, as no public datasets are available that can be leveraged for
    this problem.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 曝光过度问题发生在数字化文档时捕获了过多光线，主要是当捕获设备为手机且相机闪光灯给图像带来了过多的反射或眩光（图 [6(a)](#S6.F6.sf1 "在图6
    ‣ 6.1 曝光过度和曝光不足校正任务 ‣ 6 开放问题和未来方向 ‣ 基于深度学习的文档图像增强调研")）。即使在图像和照片增强领域，这个问题也得到了有限的关注 [[1](#bib.bib1),
    [5](#bib.bib5)]，据我们所知，尚未有研究尝试解决文档图像中的这个问题。要用深度学习方法解决这个问题，需要收集训练和测试数据集，因为目前没有可以用于这个问题的公开数据集。
- en: '![Refer to caption](img/34fe89aa30fc1e32d8c067121dee8685.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/34fe89aa30fc1e32d8c067121dee8685.png)'
- en: (a) Over-exposure problem. Image obtained from [[4](#bib.bib4)].
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 曝光过度问题。图像来源于 [[4](#bib.bib4)]。
- en: '![Refer to caption](img/e3f2853de1a482d068468f882d106cb8.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e3f2853de1a482d068468f882d106cb8.png)'
- en: (b) Under-exposure problem. Image obtained from [[42](#bib.bib42)].
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 曝光不足问题。图像来源于 [[42](#bib.bib42)]。
- en: 'Figure 6: Open problems: over-exposure and under-exposure correction.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：开放问题：曝光过度和曝光不足校正。
- en: '![Refer to caption](img/147e58f485d8cb54be3ff0451d085e8f.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/147e58f485d8cb54be3ff0451d085e8f.png)'
- en: (a) Sample slightly faded image studied in the literature [[13](#bib.bib13)].
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 文献中研究的略微褪色图像 [[13](#bib.bib13)]。
- en: '![Refer to caption](img/a5cdc6b75014822034b461eae0c353a4.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a5cdc6b75014822034b461eae0c353a4.png)'
- en: (b) Sample real-world image with severe and non-homogeneous fade. These types
    of fade is not studied in the literature.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 严重和非均匀褪色的真实世界图像。这类褪色在文献中未被研究。
- en: 'Figure 7: Open problem in defading task: Severely and/or non-homogeneously
    faded images.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7：褪色任务中的开放问题：严重和/或非均匀褪色的图像。
- en: On the other hand, underexposure happens when the lighting condition is poor
    while digitizing the document and as a result the captured image becomes dark
    (Figure [6(b)](#S6.F6.sf2 "In Figure 6 ‣ 6.1 Overexposure and underexposure correction
    tasks ‣ 6 Open Problems and Future Directions ‣ A Survey on Deep learning based
    Document Image Enhancement")). This problem is different from shadow removal,
    as shadowed document images can be partly/non-uniformly dark [[36](#bib.bib36)].
    While low-light image enhancement problem received a lot of attention for photos [[26](#bib.bib26),
    [69](#bib.bib69), [18](#bib.bib18), [57](#bib.bib57)], it has not received much
    attention in document image enhancement [[34](#bib.bib34)]. One possible future
    work could be to evaluate the practicality of these methods over document images.
    Similar to overexposure correction task, developing deep learning based methods
    for this problem needs training/testing datasets, but such datasets are unavailable.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**曝光不足**发生在数字化文档时光线条件较差，导致捕获的图像变暗（图 [6(b)](#S6.F6.sf2 "在图6 ‣ 6.1 曝光过度和曝光不足校正任务
    ‣ 6 开放问题和未来方向 ‣ 基于深度学习的文档图像增强调研")）。这个问题不同于阴影去除，因为阴影文档图像可能会部分/不均匀地变暗 [[36](#bib.bib36)]。尽管低光照图像增强问题在照片处理中得到了大量关注 [[26](#bib.bib26),
    [69](#bib.bib69), [18](#bib.bib18), [57](#bib.bib57)]，但在文档图像增强方面关注较少 [[34](#bib.bib34)]。未来一个可能的工作方向是评估这些方法在文档图像上的实际应用。类似于曝光过度校正任务，开发针对该问题的深度学习方法需要训练/测试数据集，但此类数据集尚不可用。
- en: 6.2 Defading task
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 褪色任务
- en: Fading could occur due to exposure to light, againg, being washed out, *etc*.
    This task is yet another ill-posed and under-studied task. Current work [[13](#bib.bib13)]
    makes two assumptions that may not be practical. They assume that the documents
    are uniformly faded, and the documents are very lightly faded (Fig. [7(a)](#S6.F7.sf1
    "In Figure 7 ‣ 6.1 Overexposure and underexposure correction tasks ‣ 6 Open Problems
    and Future Directions ‣ A Survey on Deep learning based Document Image Enhancement")),
    while in real-world scenarios the documents could be severely and/or non-homogeneously
    faded, e.g., aged or washed out documents (Fig. [7(b)](#S6.F7.sf2 "In Figure 7
    ‣ 6.1 Overexposure and underexposure correction tasks ‣ 6 Open Problems and Future
    Directions ‣ A Survey on Deep learning based Document Image Enhancement")).
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 褪色可能由于光照、老化、清洗等因素发生。这项任务依然是一个不良定posed和研究不足的任务。目前的工作[[13](#bib.bib13)]做出了两个可能不切实际的假设。它们假设文档的褪色是均匀的，并且文档褪色非常轻微（图[7(a)](#S6.F7.sf1
    "图7 ‣ 6.1 过度曝光和曝光不足修正任务 ‣ 6 开放问题和未来方向 ‣ 基于深度学习的文档图像增强调查")），而在现实场景中，文档可能严重褪色和/或不均匀褪色，例如，老化或清洗过的文档（图[7(b)](#S6.F7.sf2
    "图7 ‣ 6.1 过度曝光和曝光不足修正任务 ‣ 6 开放问题和未来方向 ‣ 基于深度学习的文档图像增强调查")）。
- en: Heavily and/or non-homogeneously faded documents are hard to read and very challenging
    for OCR and could considerably affect the performance of OCR, while lightly faded
    documents are usually still legible and recognizable to OCR. Therefore, to address
    these challenges we need to develop solutions that would take into account both
    severely and non-homogeneously faded documents. In addition, to train deep learning
    models (for both lightly and severely faded documents) training datasets are required,
    but similar to previous task discussed above no such datasets are publicly available.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 严重和/或不均匀褪色的文档很难阅读，对OCR非常具有挑战性，并可能显著影响OCR的性能，而轻微褪色的文档通常仍然可读且可被OCR识别。因此，为了应对这些挑战，我们需要开发考虑到严重和不均匀褪色文档的解决方案。此外，训练深度学习模型（用于轻微和严重褪色文档）需要训练数据集，但与上述讨论的任务类似，公开的数据集并不可用。
- en: 6.3 Super-resolution task
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 超分辨率任务
- en: Low-resolution documents are often hard to read and also very challenging to
    character recognition methods. Super-resolving low resolution document images
    can enhance the visual quality, readability of the text, and more importantly
    improve the OCR accuracy. Document image super-resolving is an ill-posed and challenging
    problem, especially when there are artifacts and noises present in the documents.
    Developing a model that super-resolves the document images in particular low-quality
    document images is even harder and more challenging.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 低分辨率文档通常难以阅读，对字符识别方法也非常具有挑战性。对低分辨率文档图像进行超分辨率处理可以增强视觉质量、文本的可读性，更重要的是提高OCR的准确性。文档图像超分辨率是一个不良定posed且具有挑战性的问题，尤其是在文档中存在伪影和噪声时。开发一个可以超分辨率处理文档图像，特别是低质量文档图像的模型更加困难且具有挑战性。
- en: One way to tackle this issue is to use Bicubic interpolation but such basic
    methods can introduce noise or exacerbate the noise/artifacts that the document
    in particular low-quality ones have. To increase the resolution of the document
    images and recover as much details as possible we need super resolution methods.
    Through super-resolving these document images, characters become more legible
    and it could boost the OCR performance as well.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 解决此问题的一种方法是使用双三次插值，但这种基本方法可能会引入噪声或加剧文档中存在的噪声/伪影，特别是在低质量文档中。为了提高文档图像的分辨率并尽可能多地恢复细节，我们需要超分辨率方法。通过超分辨率处理这些文档图像，字符变得更加清晰，也有助于提高OCR的性能。
- en: While image/photo super-resolution problem has received a great deal of attention [[64](#bib.bib64),
    [71](#bib.bib71), [27](#bib.bib27), [30](#bib.bib30), [31](#bib.bib31), [11](#bib.bib11),
    [8](#bib.bib8), [40](#bib.bib40), [65](#bib.bib65), [28](#bib.bib28)], this task
    has received little attention for the document images [[47](#bib.bib47), [48](#bib.bib48)].
    As a future work, we need to develop effective super-resolution methods specifically
    designed for documents image with low-quality in order to improve the legibility
    and OCR performance.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图像/照片超分辨率问题已受到广泛关注[[64](#bib.bib64), [71](#bib.bib71), [27](#bib.bib27), [30](#bib.bib30),
    [31](#bib.bib31), [11](#bib.bib11), [8](#bib.bib8), [40](#bib.bib40), [65](#bib.bib65),
    [28](#bib.bib28)]，但文档图像的这一任务却鲜有关注[[47](#bib.bib47), [48](#bib.bib48)]。作为未来的工作，我们需要开发专门针对低质量文档图像的有效超分辨率方法，以提高可读性和OCR性能。
- en: '![Refer to caption](img/4fcd6276accf924ace7ca62d8fb3dc57.png)  ![Refer to caption](img/9e59d10909eecc018dfb7fac511d18f1.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4fcd6276accf924ace7ca62d8fb3dc57.png)  ![参见说明](img/9e59d10909eecc018dfb7fac511d18f1.png)'
- en: (a) Low contrast problem. Low contrast text is not recovered.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 低对比度问题。低对比度的文字无法恢复。
- en: '![Refer to caption](img/dca717607d76f7ba6f6a06dca65a7c3e.png)  ![Refer to caption](img/ae2c60138d98d711b7358fe93687cb79.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/dca717607d76f7ba6f6a06dca65a7c3e.png)  ![参见说明](img/ae2c60138d98d711b7358fe93687cb79.png)'
- en: (b) Bleed-through problem. Bleed through text is not effectively removed.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 渗透问题。渗透的文字未能有效去除。
- en: '![Refer to caption](img/07fcb46e2ba3086f0c1bd44f9f6f173a.png)  ![Refer to caption](img/c5ade3d79f1eaf3f8e9800b477c0f2f8.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/07fcb46e2ba3086f0c1bd44f9f6f173a.png)  ![参见说明](img/c5ade3d79f1eaf3f8e9800b477c0f2f8.png)'
- en: (c) RGB document with various ink intensities. Text in lighter color i.e. orange,
    is not effectively recovered.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 各种墨水浓度的RGB文档。较浅颜色的文字（即橙色）未能有效恢复。
- en: 'Figure 8: Open problems in binarization task. Left side images are the original
    images, and the right side images are the binarized images using a recent state-of-the-art
    method.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：二值化任务中的开放问题。左侧图像为原始图像，右侧图像为使用最新的先进方法进行二值化后的图像。
- en: 6.4 Binarization task
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 二值化任务
- en: While the binarization task has received a great deal of attention, there are
    still multiple scenarios that current binarization methods do not perform well
    on them. Specifically, when the image has low contrast or when ghosting and bleed
    through are present in the document, or when the image is RGB with various ink
    color and intensities. These scenarios are challenging for binarization methods
    to handle.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管二值化任务已受到广泛关注，但当前的二值化方法在多个场景下表现仍不理想。具体而言，当图像对比度低、文档中存在残影和渗透现象，或图像为RGB格式且墨水颜色和浓度各异时，这些场景对于二值化方法来说都具有挑战性。
- en: Ghosting in documents occurs when the ink or text from the other side of the
    page can be seen, but ink does not completely come through to the other side.
    Bleed-through on the other hand, happens when the ink seeps in to the other side
    and interferes with the text on the front page. Both issues make character recognition
    very challenging, specially bleed-through.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 文档中的残影现象发生在页面的另一侧可以看到墨水或文字，但墨水并未完全透过到另一侧。而渗透现象则发生在墨水渗透到另一侧并干扰到正面页面上的文字。这两种问题都使字符识别变得非常困难，特别是渗透问题。
- en: Figure [8(a)](#S6.F8.sf1 "In Figure 8 ‣ 6.3 Super-resolution task ‣ 6 Open Problems
    and Future Directions ‣ A Survey on Deep learning based Document Image Enhancement")
    shows a low contrast image and its binarized one. The current binarization methods
    are not able to recover the text properly when text has low contrast. Figure [8(b)](#S6.F8.sf2
    "In Figure 8 ‣ 6.3 Super-resolution task ‣ 6 Open Problems and Future Directions
    ‣ A Survey on Deep learning based Document Image Enhancement") presents another
    example with bleed though present in the image. As one can see, the method was
    not able to remove the bleed through completely. Figure [8(c)](#S6.F8.sf3 "In
    Figure 8 ‣ 6.3 Super-resolution task ‣ 6 Open Problems and Future Directions ‣
    A Survey on Deep learning based Document Image Enhancement") shows an example
    of an RGB image and its binarized one. As you can see, the method is not performing
    well over texts with orange color. Thus to address these issues we need to develop
    a method that would take them into account.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [8(a)](#S6.F8.sf1 "图 8 ‣ 6.3 超分辨率任务 ‣ 6 开放问题与未来方向 ‣ 基于深度学习的文档图像增强调查") 显示了一个低对比度图像及其二值化后的图像。当前的二值化方法在文字对比度较低时无法有效恢复文字。图 [8(b)](#S6.F8.sf2
    "图 8 ‣ 6.3 超分辨率任务 ‣ 6 开放问题与未来方向 ‣ 基于深度学习的文档图像增强调查") 展示了另一个带有渗透现象的图像示例。如图所示，该方法无法完全去除渗透现象。图 [8(c)](#S6.F8.sf3
    "图 8 ‣ 6.3 超分辨率任务 ‣ 6 开放问题与未来方向 ‣ 基于深度学习的文档图像增强调查") 展示了一个RGB图像及其二值化后的图像。可以看到，该方法在处理橙色文字时效果不佳。因此，为了解决这些问题，我们需要开发一个能够考虑到这些因素的方法。
- en: 6.5 OCR performance evaluation
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 OCR性能评估
- en: One of the main purpose of document image enhancement is to enhance character
    recognition methods or OCR to facilitate automated document analysis. Currently
    there is no document image test dataset with the extracted ground truth text so
    that could be utilized to evaluate document images enhancement methods in terms
    of OCR improvement. Current methods either ignore to evaluate their methods in
    terms of OCR, or show the OCR improvement only on a few images which is not sufficient
    to prove the practicality of their methods in the wild. This calls for a separate
    study to collect such dataset and benchmark current methods against this test
    dataset.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 文档图像增强的主要目的是提升字符识别方法或OCR，以促进自动化文档分析。目前没有具有提取的真实文本的文档图像测试数据集，可以用来评估文档图像增强方法在OCR改进方面的效果。目前的方法要么忽视了在OCR方面评估其方法，要么仅在少数图像上展示OCR改进，这不足以证明其方法在实际应用中的可行性。这需要一个单独的研究来收集这样的数据集，并将当前的方法与该测试数据集进行基准测试。
- en: 7 Conclusion
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this paper we reviewed deep learning based methods for six different document
    image enhancement tasks, including binarization, debluring, denoising, defading,
    watermark removal, and shadow removal. We also summarized datasets used for these
    tasks along with the metrics used to evaluate the performance of these methods.
    We discussed the features, challenges, advantages and disadvantages of the deep
    learning based document image enhancement methods.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了基于深度学习的六种不同文档图像增强任务，包括二值化、去模糊、去噪、去褪色、水印去除和阴影去除。我们还总结了这些任务使用的数据集以及用于评估这些方法性能的指标。我们讨论了基于深度学习的文档图像增强方法的特性、挑战、优点和缺点。
- en: We also discussed open problems in this area and identified multiple important
    tasks that have received little to no attention. These tasks are over-exposure/under-exposure
    correction, defading, and super-resolution. Over-exposure problem usually occurs
    when the imaging device captures too much light or glare due to reflection, and
    under-exposure occurs when the lighting condition is poor and the captured image
    becomes dark and hard to read. Fading could happen due to sunlight, aging, and
    being washed out, *etc*. Low-resolution document images need to be super-resolved
    to enhance their visual quality and more importantly make small text more legible.
    Enhancing the document image resolution is more challenging when noise and artifacts
    are present in the document image. Such images are often hard to read and the
    low legibility affects the performance of character recognition techniques. The
    above-explained tasks have received little attention and they are far from solved.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了这一领域的未解问题，并识别出多个重要任务，这些任务几乎没有受到关注。这些任务包括过度曝光/曝光不足校正、去褪色和超分辨率。过度曝光问题通常发生在成像设备捕捉到过多的光线或因反射产生的眩光时，而曝光不足发生在照明条件差、捕捉到的图像变暗且难以阅读时。褪色可能由于阳光、老化和褪色*等*原因发生。低分辨率的文档图像需要进行超分辨率处理，以提高其视觉质量，更重要的是使小文字更加清晰。当文档图像中存在噪声和伪影时，提升图像分辨率更具挑战性。这些图像通常难以阅读，低可读性影响了字符识别技术的性能。上述任务受到了很少关注，距离解决还有很长的路要走。
- en: Binarization task has received a great deal of attention over the past years,
    however, these methods underperform in multiple scenarios. For example, when the
    image has low contrast or multiple artifacts *e.g.,* stamp, signature, ghosting
    or bleed-through are present. Ghosting and bleed-through occur when the text from
    the other side of the document can be seen or ink seeps in to the other side of
    the document. These artifacts are challenging to remove and effective methods
    are needed to address and resolve these problems properly.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 二值化任务在过去几年受到了极大的关注，但这些方法在多个场景下表现不佳。例如，当图像对比度低或存在多个伪影*例如*邮票、签名、重影或渗透时。重影和渗透发生在文档的另一侧文字可见或墨水渗透到文档另一侧时。这些伪影很难去除，需要有效的方法来妥善解决这些问题。
- en: Current document image enhancement methods mainly focus on improving the visual
    quality of the images. While this is an important aspect, the performance of these
    methods for automatic document analysis problems, *e.g.,* character recognition,
    is largely ignored. Thus there is an emerging need to develop methods that can
    jointly enhance the visual quality and OCR performance. The OCR performance needs
    to be evaluated over a larger test dataset, and not just over a few samples as
    was done in the literature.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 当前文档图像增强方法主要集中在提高图像的视觉质量上。虽然这是一个重要方面，但这些方法在自动文档分析问题中的表现，如字符识别，基本被忽视。因此，迫切需要开发能够同时提升视觉质量和OCR性能的方法。OCR性能需要在更大的测试数据集上评估，而不仅仅是在文献中处理的少数样本上。
- en: All that said, current methods target only one problem, *e.g.,* debluring, at
    a time, but in reality a document image can have multiple issues at the same time.
    For example, a document image could be blurry, faded and noisy. To the best of
    our knowledge, currently these is no method that can tackle multiple issues in
    a single image at the same time.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，目前的方法仅针对一个问题，*例如*，去模糊，但实际上文档图像可能同时存在多个问题。例如，文档图像可能模糊、褪色且有噪声。据我们所知，目前没有一种方法能够同时处理图像中的多个问题。
- en: References
  id: totrans-269
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Afifi, M., Derpanis, K. G., Ommer, B., and Brown, M. S. Learning to correct
    overexposed and underexposed photos. arXiv preprint arXiv:2003.11596 13 (2020).'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Afifi, M., Derpanis, K. G., Ommer, B., and Brown, M. S. 学习修正过度曝光和曝光不足的照片。arXiv
    预印本 arXiv:2003.11596 13 (2020)。'
- en: '[2] Anvari, Z., and Athitsos, V. A pipeline for automated face dataset creation
    from unlabeled images. In Proceedings of the 12th ACM International Conference
    on PErvasive Technologies Related to Assistive Environments (2019), pp. 227–235.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Anvari, Z., and Athitsos, V. 从未标记图像自动创建面部数据集的流程。在第12届ACM国际助理环境相关技术会议论文集中（2019），第227–235页。'
- en: '[3] Anvari, Z., and Athitsos, V. Enhanced cyclegan dehazing network. In VISIGRAPP
    (4: VISAPP) (2021), pp. 193–202.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Anvari, Z., and Athitsos, V. 增强的CycleGAN去雾网络。在VISIGRAPP（4: VISAPP）（2021），第193–202页。'
- en: '[4] Arlazarov, V. V., Bulatov, K. B., Chernov, T. S., and Arlazarov, V. L.
    Midv-500: a dataset for identity document analysis and recognition on mobile devices
    in video stream.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Arlazarov, V. V., Bulatov, K. B., Chernov, T. S., and Arlazarov, V. L.
    Midv-500: 一个用于移动设备视频流中的身份文档分析和识别的数据集。'
- en: '[5] Cai, J., Gu, S., and Zhang, L. Learning a deep single image contrast enhancer
    from multi-exposure images. IEEE Transactions on Image Processing 27, 4 (2018),
    2049–2062.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Cai, J., Gu, S., and Zhang, L. 从多曝光图像中学习深度单图像对比度增强器。IEEE图像处理学报 27, 4 (2018)，2049–2062页。'
- en: '[6] Calvo-Zaragoza, J., and Gallego, A.-J. A selectional auto-encoder approach
    for document image binarization. Pattern Recognition 86 (2019), 37–47.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Calvo-Zaragoza, J., and Gallego, A.-J. 一种用于文档图像二值化的选择性自编码器方法。模式识别 86 (2019),
    37–47。'
- en: '[7] Chen, X., He, X., Yang, J., and Wu, Q. An effective document image deblurring
    algorithm. In CVPR 2011 (2011), IEEE, pp. 369–376.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Chen, X., He, X., Yang, J., and Wu, Q. 一种有效的文档图像去模糊算法。在CVPR 2011（2011），IEEE，第369–376页。'
- en: '[8] Chu, M., Xie, Y., Leal-Taixé, L., and Thuerey, N. Temporally coherent gans
    for video super-resolution (tecogan). arXiv preprint arXiv:1811.09393 1, 2 (2018),
    3.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Chu, M., Xie, Y., Leal-Taixé, L., and Thuerey, N. 时序一致的GAN用于视频超分辨率（tecogan）。arXiv
    预印本 arXiv:1811.09393 1, 2 (2018)，第3页。'
- en: '[9] Deng, F., Wu, Z., Lu, Z., and Brown, M. S. Binarizationshop: a user-assisted
    software suite for converting old documents to black-and-white. In Proceedings
    of the 10th annual joint conference on Digital libraries (2010), pp. 255–258.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Deng, F., Wu, Z., Lu, Z., and Brown, M. S. Binarizationshop: 一个用户辅助的软件套件，用于将旧文档转换为黑白。见于第十届年度联合数字图书馆会议论文集（2010），第255–258页。'
- en: '[10] Dey, S., and Jawanpuria, P. Light-weight document image cleanup using
    perceptual loss. arXiv preprint arXiv:2105.09076 (2021).'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Dey, S., and Jawanpuria, P. 采用感知损失的轻量级文档图像清理。arXiv 预印本 arXiv:2105.09076
    (2021)。'
- en: '[11] Dong, C., Loy, C. C., He, K., and Tang, X. Image super-resolution using
    deep convolutional networks. IEEE transactions on pattern analysis and machine
    intelligence 38, 2 (2015), 295–307.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Dong, C., Loy, C. C., He, K., and Tang, X. 使用深度卷积网络进行图像超分辨率。IEEE模式分析与机器智能学报
    38, 2 (2015)，295–307页。'
- en: '[12] Dua, D., and Graff, C. UCI machine learning repository, 2017.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Dua, D., and Graff, C. UCI机器学习数据集库，2017年。'
- en: '[13] Gangeh, M. J., Plata, M., Motahari, H., and Duffy, N. P. End-to-end unsupervised
    document image blind denoising. arXiv preprint arXiv:2105.09437 (2021).'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] Gangeh, M. J., Plata, M., Motahari, H., and Duffy, N. P. 端到端的无监督文档图像盲去噪。arXiv
    预印本 arXiv:2105.09437 (2021)。'
- en: '[14] Gangeh, M. J., Tiyyagura, S. R., Dasaratha, S. V., Motahari, H., and Duffy,
    N. P. Document enhancement system using auto-encoders. In Workshop on Document
    Intelligence at NeurIPS 2019 (2019).'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Gangeh, M. J., Tiyyagura, S. R., Dasaratha, S. V., Motahari, H., 和 Duffy,
    N. P. 使用自动编码器的文档增强系统。在NeurIPS 2019文档智能研讨会（2019）。'
- en: '[15] Gatos, B., Ntirogiannis, K., and Pratikakis, I. Icdar 2009 document image
    binarization contest (dibco 2009). In 2009 10th International conference on document
    analysis and recognition (2009), IEEE, pp. 1375–1382.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Gatos, B., Ntirogiannis, K., 和 Pratikakis, I. Icdar 2009文档图像二值化竞赛（dibco
    2009）。在2009年第10届国际文档分析与识别会议（2009），IEEE，页码1375–1382。'
- en: '[16] Gu, S., Zuo, W., Guo, S., Chen, Y., Chen, C., and Zhang, L. Learning dynamic
    guidance for depth image enhancement. In Proceedings of the IEEE conference on
    computer vision and pattern recognition (2017), pp. 3769–3778.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Gu, S., Zuo, W., Guo, S., Chen, Y., Chen, C., 和 Zhang, L. 学习动态引导以增强深度图像。在2017年IEEE计算机视觉与模式识别会议论文集，页码3769–3778。'
- en: '[17] Guo, C., Li, C., Guo, J., Loy, C. C., Hou, J., Kwong, S., and Cong, R.
    Zero-reference deep curve estimation for low-light image enhancement. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020),
    pp. 1780–1789.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] Guo, C., Li, C., Guo, J., Loy, C. C., Hou, J., Kwong, S., 和 Cong, R. 无参考深度曲线估计用于低光照图像增强。在IEEE/CVF计算机视觉与模式识别会议论文集（2020），页码1780–1789。'
- en: '[18] Guo, X., Li, Y., and Ling, H. Lime: Low-light image enhancement via illumination
    map estimation. IEEE Transactions on image processing 26, 2 (2016), 982–993.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Guo, X., Li, Y., 和 Ling, H. Lime: 通过光照图估计进行低光照图像增强。IEEE图像处理汇刊26, 2（2016），982–993。'
- en: '[19] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image
    recognition. In Proceedings of the IEEE conference on computer vision and pattern
    recognition (2016), pp. 770–778.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] He, K., Zhang, X., Ren, S., 和 Sun, J. 用于图像识别的深度残差学习。在2016年IEEE计算机视觉与模式识别会议论文集，页码770–778。'
- en: '[20] He, S., and Schomaker, L. Deepotsu: Document enhancement and binarization
    using iterative deep learning. Pattern recognition 91 (2019), 379–390.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] He, S., 和 Schomaker, L. Deepotsu: 使用迭代深度学习进行文档增强和二值化。模式识别91（2019），379–390。'
- en: '[21] Hedjam, R., Nafchi, H. Z., Moghaddam, R. F., Kalacska, M., and Cheriet,
    M. Icdar 2015 contest on multispectral text extraction (ms-tex 2015). In 2015
    13th International Conference on Document Analysis and Recognition (ICDAR) (2015),
    IEEE, pp. 1181–1185.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Hedjam, R., Nafchi, H. Z., Moghaddam, R. F., Kalacska, M., 和 Cheriet,
    M. Icdar 2015多光谱文本提取竞赛（ms-tex 2015）。在2015年第13届国际文档分析与识别会议（ICDAR）（2015），IEEE，页码1181–1185。'
- en: '[22] Howe, N. R. Document binarization with automatic parameter tuning. International
    journal on document analysis and recognition (ijdar) 16, 3 (2013), 247–258.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Howe, N. R. 自动参数调优的文档二值化。国际文档分析与识别期刊（ijdar）16, 3（2013），247–258。'
- en: '[23] Hradiš, M., Kotera, J., Zemcık, P., and Šroubek, F. Convolutional neural
    networks for direct text deblurring. In Proceedings of BMVC (2015), vol. 10.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Hradiš, M., Kotera, J., Zemcík, P., 和 Šroubek, F. 用于直接文本去模糊的卷积神经网络。在BMVC会议论文集（2015），第10卷。'
- en: '[24] Ioffe, S., and Szegedy, C. Batch normalization: Accelerating deep network
    training by reducing internal covariate shift. In International conference on
    machine learning (2015), PMLR, pp. 448–456.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Ioffe, S., 和 Szegedy, C. 批量归一化：通过减少内部协变量偏移加速深度网络训练。在国际机器学习会议（2015），PMLR，页码448–456。'
- en: '[25] Jemni, S. K., Souibgui, M. A., Kessentini, Y., and Fornés, A. Enhance
    to read better: An improved generative adversarial network for handwritten document
    image enhancement. arXiv preprint arXiv:2105.12710 (2021).'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Jemni, S. K., Souibgui, M. A., Kessentini, Y., 和 Fornés, A. 增强以更好地阅读：一种改进的生成对抗网络用于手写文档图像增强。arXiv预印本
    arXiv:2105.12710（2021）。'
- en: '[26] Jiang, Y., Gong, X., Liu, D., Cheng, Y., Fang, C., Shen, X., Yang, J.,
    Zhou, P., and Wang, Z. Enlightengan: Deep light enhancement without paired supervision.
    IEEE Transactions on Image Processing 30 (2021), 2340–2349.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Jiang, Y., Gong, X., Liu, D., Cheng, Y., Fang, C., Shen, X., Yang, J.,
    Zhou, P., 和 Wang, Z. Enlightengan: 无配对监督的深度光照增强。IEEE图像处理汇刊30（2021），2340–2349。'
- en: '[27] Jo, Y., Oh, S. W., Kang, J., and Kim, S. J. Deep video super-resolution
    network using dynamic upsampling filters without explicit motion compensation.
    In Proceedings of the IEEE conference on computer vision and pattern recognition
    (2018), pp. 3224–3232.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] Jo, Y., Oh, S. W., Kang, J., 和 Kim, S. J. 使用动态上采样滤波器的深度视频超分辨网络，无需显式运动补偿。在2018年IEEE计算机视觉与模式识别会议论文集，页码3224–3232。'
- en: '[28] Kim, J., Lee, J. K., and Lee, K. M. Deeply-recursive convolutional network
    for image super-resolution. In Proceedings of the IEEE conference on computer
    vision and pattern recognition (2016), pp. 1637–1645.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Kim, J., Lee, J. K., 和 Lee, K. M. 用于图像超分辨率的深度递归卷积网络。发表于 IEEE 计算机视觉与模式识别会议
    (2016), 页码 1637–1645.'
- en: '[29] Kligler, N., Katz, S., and Tal, A. Document enhancement using visibility
    detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (2018), pp. 2374–2382.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Kligler, N., Katz, S., 和 Tal, A. 使用可见性检测进行文档增强。发表于 IEEE 计算机视觉与模式识别会议 (2018),
    页码 2374–2382.'
- en: '[30] Lai, W.-S., Huang, J.-B., Ahuja, N., and Yang, M.-H. Deep laplacian pyramid
    networks for fast and accurate super-resolution. In Proceedings of the IEEE conference
    on computer vision and pattern recognition (2017), pp. 624–632.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Lai, W.-S., Huang, J.-B., Ahuja, N., 和 Yang, M.-H. 用于快速准确超分辨率的深度拉普拉斯金字塔网络。发表于
    IEEE 计算机视觉与模式识别会议 (2017), 页码 624–632.'
- en: '[31] Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta,
    A., Aitken, A., Tejani, A., Totz, J., Wang, Z., et al. Photo-realistic single
    image super-resolution using a generative adversarial network. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (2017), pp. 4681–4690.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta,
    A., Aitken, A., Tejani, A., Totz, J., Wang, Z., 等. 使用生成对抗网络进行照片级逼真单图像超分辨率。发表于
    IEEE 计算机视觉与模式识别会议 (2017), 页码 4681–4690.'
- en: '[32] Lewis, D., Agam, G., Argamon, S., Frieder, O., Grossman, D., and Heard,
    J. Building a test collection for complex document information processing. In
    Proceedings of the 29th annual international ACM SIGIR conference on Research
    and development in information retrieval (2006), pp. 665–666.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Lewis, D., Agam, G., Argamon, S., Frieder, O., Grossman, D., 和 Heard,
    J. 构建复杂文档信息处理的测试集合。发表于 第29届国际 ACM SIGIR 研究与开发信息检索会议 (2006), 页码 665–666.'
- en: '[33] Li, D., Wu, Y., and Zhou, Y. Sauvolanet: Learning adaptive sauvola network
    for degraded document binarization. arXiv preprint arXiv:2105.05521 (2021).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] Li, D., Wu, Y., 和 Zhou, Y. Sauvolanet: 学习自适应 Sauvola 网络进行退化文档二值化。arXiv
    预印本 arXiv:2105.05521 (2021).'
- en: '[34] Li, X., Zhang, B., Liao, J., and Sander, P. V. Document rectification
    and illumination correction using a patch-based cnn. ACM Transactions on Graphics
    (TOG) 38, 6 (2019), 1–11.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Li, X., Zhang, B., Liao, J., 和 Sander, P. V. 使用基于补丁的 cnn 进行文档矫正和光照修正。ACM
    Transactions on Graphics (TOG) 38, 6 (2019), 1–11.'
- en: '[35] Lin, W.-A., Chen, J.-C., Castillo, C. D., and Chellappa, R. Deep density
    clustering of unconstrained faces. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (2018), pp. 8128–8137.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Lin, W.-A., Chen, J.-C., Castillo, C. D., 和 Chellappa, R. 无约束人脸的深度密度聚类。发表于
    IEEE 计算机视觉与模式识别会议 (2018), 页码 8128–8137.'
- en: '[36] Lin, Y.-H., Chen, W.-C., and Chuang, Y.-Y. Bedsr-net: A deep shadow removal
    network from a single document image. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (2020), pp. 12905–12914.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Lin, Y.-H., Chen, W.-C., 和 Chuang, Y.-Y. Bedsr-net: 从单张文档图像中去除阴影的深度网络。发表于
    IEEE/CVF 计算机视觉与模式识别会议 (2020), 页码 12905–12914.'
- en: '[37] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., and
    Berg, A. C. Ssd: Single shot multibox detector. In European conference on computer
    vision (2016), Springer, pp. 21–37.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., 和
    Berg, A. C. SSD: 单次多框检测器。发表于 欧洲计算机视觉会议 (2016), Springer, 页码 21–37.'
- en: '[38] Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks
    for semantic segmentation. In Proceedings of the IEEE conference on computer vision
    and pattern recognition (2015), pp. 3431–3440.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Long, J., Shelhamer, E., 和 Darrell, T. 用于语义分割的全卷积网络。发表于 IEEE 计算机视觉与模式识别会议
    (2015), 页码 3431–3440.'
- en: '[39] Lu, H., Kot, A. C., and Shi, Y. Q. Distance-reciprocal distortion measure
    for binary document images. IEEE Signal Processing Letters 11, 2 (2004), 228–231.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Lu, H., Kot, A. C., 和 Shi, Y. Q. 二值文档图像的距离倒数失真度量。IEEE 信号处理通讯 11, 2 (2004),
    228–231.'
- en: '[40] Mao, X., Shen, C., and Yang, Y.-B. Image restoration using very deep convolutional
    encoder-decoder networks with symmetric skip connections. Advances in neural information
    processing systems 29 (2016), 2802–2810.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Mao, X., Shen, C., 和 Yang, Y.-B. 使用具有对称跳跃连接的非常深卷积编码器-解码器网络进行图像恢复。神经信息处理系统进展
    29 (2016), 2802–2810.'
- en: '[41] Mesquita, R. G., Mello, C. A., and Almeida, L. A new thresholding algorithm
    for document images based on the perception of objects by distance. Integrated
    Computer-Aided Engineering 21, 2 (2014), 133–146.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Mesquita, R. G., Mello, C. A., 和 Almeida, L. 一种基于距离的文档图像阈值算法。集成计算机辅助工程
    21, 2 (2014), 133–146。'
- en: '[42] Michalak, H., and Okarma, K. Robust combined binarization method of non-uniformly
    illuminated document images for alphanumerical character recognition. Sensors
    20, 10 (2020), 2914.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Michalak, H., 和 Okarma, K. 结合方法对非均匀照明文档图像的稳健二值化用于字母数字字符识别。传感器 20, 10 (2020),
    2914。'
- en: '[43] Nafchi, H. Z., Ayatollahi, S. M., Moghaddam, R. F., and Cheriet, M. An
    efficient ground truthing tool for binarization of historical manuscripts. In
    2013 12th International Conference on Document Analysis and Recognition (2013),
    IEEE, pp. 807–811.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Nafchi, H. Z., Ayatollahi, S. M., Moghaddam, R. F., 和 Cheriet, M. 一种高效的历史手稿二值化地面真实工具。在
    2013 第12届国际文档分析与识别会议 (2013), IEEE, pp. 807–811。'
- en: '[44] Nayef, N., Luqman, M. M., Prum, S., Eskenazi, S., Chazalon, J., and Ogier,
    J.-M. Smartdoc-qa: A dataset for quality assessment of smartphone captured document
    images-single and multiple distortions. In 2015 13th International Conference
    on Document Analysis and Recognition (ICDAR) (2015), IEEE, pp. 1231–1235.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Nayef, N., Luqman, M. M., Prum, S., Eskenazi, S., Chazalon, J., 和 Ogier,
    J.-M. Smartdoc-qa: 用于智能手机拍摄文档图像质量评估的数据集——单一和多重失真。 在 2015 第13届国际文档分析与识别会议 (ICDAR)
    (2015), IEEE, pp. 1231–1235。'
- en: '[45] Ntirogiannis, K., Gatos, B., and Pratikakis, I. Performance evaluation
    methodology for historical document image binarization. IEEE Transactions on Image
    Processing 22, 2 (2012), 595–609.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Ntirogiannis, K., Gatos, B., 和 Pratikakis, I. 历史文档图像二值化的性能评估方法论。IEEE 图像处理汇刊
    22, 2 (2012), 595–609。'
- en: '[46] Ntirogiannis, K., Gatos, B., and Pratikakis, I. Icfhr2014 competition
    on handwritten document image binarization (h-dibco 2014). In 2014 14th International
    conference on frontiers in handwriting recognition (2014), IEEE, pp. 809–813.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Ntirogiannis, K., Gatos, B., 和 Pratikakis, I. Icfhr2014 手写文档图像二值化竞赛（h-dibco
    2014）。在 2014 第14届手写识别前沿国际会议 (2014), IEEE, pp. 809–813。'
- en: '[47] Pandey, R. K., and Ramakrishnan, A. Language independent single document
    image super-resolution using cnn for improved recognition. arXiv preprint arXiv:1701.08835
    (2017).'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Pandey, R. K., 和 Ramakrishnan, A. 语言无关的单文档图像超分辨率使用 CNN 以提高识别率。arXiv 预印本
    arXiv:1701.08835 (2017)。'
- en: '[48] Peng, X., and Wang, C. Building super-resolution image generator for ocr
    accuracy improvement. In International Workshop on Document Analysis Systems (2020),
    Springer, pp. 145–160.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Peng, X., 和 Wang, C. 构建超分辨率图像生成器以提高 OCR 精度。在国际文档分析系统研讨会 (2020), Springer,
    pp. 145–160。'
- en: '[49] Pratikakis, I., Gatos, B., and Ntirogiannis, K. H-dibco 2010-handwritten
    document image binarization competition. In 2010 12th International Conference
    on Frontiers in Handwriting Recognition (2010), IEEE, pp. 727–732.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Pratikakis, I., Gatos, B., 和 Ntirogiannis, K. H-dibco 2010-手写文档图像二值化竞赛。在
    2010 第12届手写识别前沿国际会议 (2010), IEEE, pp. 727–732。'
- en: '[50] Pratikakis, I., Gatos, B., and Ntirogiannis, K. Icdar 2011 document image
    binarization contest (dibco 2011). In 2011 International Conference on Document
    Analysis and Recognition (2011), pp. 1506–1510.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Pratikakis, I., Gatos, B., 和 Ntirogiannis, K. Icdar 2011 文档图像二值化竞赛（dibco
    2011）。在 2011 国际文档分析与识别会议 (2011), pp. 1506–1510。'
- en: '[51] Pratikakis, I., Gatos, B., and Ntirogiannis, K. Icfhr 2012 competition
    on handwritten document image binarization (h-dibco 2012). In 2012 international
    conference on frontiers in handwriting recognition (2012), IEEE, pp. 817–822.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Pratikakis, I., Gatos, B., 和 Ntirogiannis, K. Icfhr 2012 手写文档图像二值化竞赛（h-dibco
    2012）。在 2012 第一次手写识别前沿国际会议 (2012), IEEE, pp. 817–822。'
- en: '[52] Pratikakis, I., Gatos, B., and Ntirogiannis, K. Icdar 2013 document image
    binarization contest (dibco 2013). In 2013 12th International Conference on Document
    Analysis and Recognition (2013), IEEE, pp. 1471–1476.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Pratikakis, I., Gatos, B., 和 Ntirogiannis, K. Icdar 2013 文档图像二值化竞赛（dibco
    2013）。在 2013 第12届国际文档分析与识别会议 (2013), IEEE, pp. 1471–1476。'
- en: '[53] Pratikakis, I., Zagori, K., Kaddas, P., and Gatos, B. Icfhr 2018 competition
    on handwritten document image binarization (h-dibco 2018). In 2018 16th International
    Conference on Frontiers in Handwriting Recognition (ICFHR) (2018), pp. 489–493.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Pratikakis, I., Zagori, K., Kaddas, P., 和 Gatos, B. Icfhr 2018 手写文档图像二值化竞赛（h-dibco
    2018）。在 2018 第16届手写识别前沿国际会议 (ICFHR) (2018), pp. 489–493。'
- en: '[54] Pratikakis, I., Zagoris, K., Barlas, G., and Gatos, B. Icfhr2016 handwritten
    document image binarization contest (h-dibco 2016). In 2016 15th International
    Conference on Frontiers in Handwriting Recognition (ICFHR) (2016), IEEE, pp. 619–623.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Pratikakis, I., Zagoris, K., Barlas, G., 和 Gatos, B. ICFHR2016 手写文档图像二值化竞赛（H-DIBCO
    2016）。在2016年第15届国际手写识别前沿会议（ICFHR）（2016），IEEE，第619–623页。'
- en: '[55] Pratikakis, I., Zagoris, K., Barlas, G., and Gatos, B. Icdar2017 competition
    on document image binarization (dibco 2017). In 2017 14th IAPR International Conference
    on Document Analysis and Recognition (ICDAR) (2017), vol. 1, IEEE, pp. 1395–1403.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Pratikakis, I., Zagoris, K., Barlas, G., 和 Gatos, B. ICDAR2017 文档图像二值化竞赛（DIBCO
    2017）。在2017年第14届IAPR国际文档分析与识别会议（ICDAR）（2017），第1卷，IEEE，第1395–1403页。'
- en: '[56] Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. You only look once:
    Unified, real-time object detection. In Proceedings of the IEEE conference on
    computer vision and pattern recognition (2016), pp. 779–788.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Redmon, J., Divvala, S., Girshick, R., 和 Farhadi, A. 你只需看一次：统一的实时物体检测。在《IEEE计算机视觉与模式识别会议论文集》（2016），第779–788页。'
- en: '[57] Ren, W., Liu, S., Ma, L., Xu, Q., Xu, X., Cao, X., Du, J., and Yang, M.-H.
    Low-light image enhancement via a deep hybrid network. IEEE Transactions on Image
    Processing 28, 9 (2019), 4364–4375.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Ren, W., Liu, S., Ma, L., Xu, Q., Xu, X., Cao, X., Du, J., 和 Yang, M.-H.
    通过深度混合网络进行低光照图像增强。《IEEE图像处理汇刊》28，9（2019），4364–4375。'
- en: '[58] Sauvola, J., and Pietikäinen, M. Adaptive document image binarization.
    Pattern recognition 33, 2 (2000), 225–236.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Sauvola, J., 和 Pietikäinen, M. 自适应文档图像二值化。《模式识别》33，2（2000），225–236。'
- en: '[59] Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A unified embedding
    for face recognition and clustering. In Proceedings of the IEEE conference on
    computer vision and pattern recognition (2015), pp. 815–823.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Schroff, F., Kalenichenko, D., 和 Philbin, J. Facenet：用于面部识别和聚类的统一嵌入。在《IEEE计算机视觉与模式识别会议论文集》（2015），第815–823页。'
- en: '[60] Sharma, M., Verma, A., and Vig, L. Learning to clean: A gan perspective.
    In Asian Conference on Computer Vision (2018), Springer, pp. 174–185.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Sharma, M., Verma, A., 和 Vig, L. 学习清理：一个生成对抗网络的视角。在《亚洲计算机视觉会议》（2018），Springer，第174–185页。'
- en: '[61] Souibgui, M. A., and Kessentini, Y. De-gan: A conditional generative adversarial
    network for document enhancement. IEEE Transactions on Pattern Analysis and Machine
    Intelligence (2020).'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] Souibgui, M. A., 和 Kessentini, Y. De-gan：用于文档增强的条件生成对抗网络。《IEEE模式分析与机器智能汇刊》（2020）。'
- en: '[62] Souibgui, M. A., Kessentini, Y., and Fornés, A. A conditional gan based
    approach for distorted camera captured documents recovery. Pattern Recognition
    and Artificial Intelligence 1322 (2021), 215.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Souibgui, M. A., Kessentini, Y., 和 Fornés, A. 基于条件生成对抗网络的方法用于恢复畸变的摄像机捕获文档。《模式识别与人工智能》1322（2021），215。'
- en: '[63] Su, B., Lu, S., and Tan, C. L. Robust document image binarization technique
    for degraded document images. IEEE transactions on image processing 22, 4 (2012),
    1408–1417.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Su, B., Lu, S., 和 Tan, C. L. 针对退化文档图像的鲁棒文档图像二值化技术。《IEEE图像处理汇刊》22，4（2012），1408–1417。'
- en: '[64] Tai, Y., Yang, J., and Liu, X. Image super-resolution via deep recursive
    residual network. In Proceedings of the IEEE conference on computer vision and
    pattern recognition (2017), pp. 3147–3155.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Tai, Y., Yang, J., 和 Liu, X. 通过深度递归残差网络的图像超分辨率。在《IEEE计算机视觉与模式识别会议论文集》（2017），第3147–3155页。'
- en: '[65] Tao, X., Gao, H., Liao, R., Wang, J., and Jia, J. Detail-revealing deep
    video super-resolution. In Proceedings of the IEEE International Conference on
    Computer Vision (2017), pp. 4472–4480.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Tao, X., Gao, H., Liao, R., Wang, J., 和 Jia, J. 细节揭示的深度视频超分辨率。在《IEEE国际计算机视觉会议论文集》（2017），第4472–4480页。'
- en: '[66] Tensmeyer, C., and Martinez, T. Document image binarization with fully
    convolutional neural networks. In 2017 14th IAPR international conference on document
    analysis and recognition (ICDAR) (2017), vol. 1, IEEE, pp. 99–104.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Tensmeyer, C., 和 Martinez, T. 使用全卷积神经网络的文档图像二值化。在2017年第14届IAPR国际文档分析与识别会议（ICDAR）（2017），第1卷，IEEE，第99–104页。'
- en: '[67] Translation, S. M. Sixth workshop on statistical machine translation.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Translation, S. M. 第六届统计机器翻译研讨会。'
- en: '[68] Van der Zant, T., Schomaker, L., and Haak, K. Handwritten-word spotting
    using biologically inspired features. Ieee transactions on pattern analysis and
    machine intelligence 30, 11 (2008), 1945–1957.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Van der Zant, T., Schomaker, L., 和 Haak, K. 使用生物启发特征的手写词检索。《IEEE模式分析与机器智能汇刊》30，11（2008），1945–1957。'
- en: '[69] Wang, R., Zhang, Q., Fu, C.-W., Shen, X., Zheng, W.-S., and Jia, J. Underexposed
    photo enhancement using deep illumination estimation. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (2019), pp. 6849–6857.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Wang, R., Zhang, Q., Fu, C.-W., Shen, X., Zheng, W.-S., 和 Jia, J. 使用深度照明估计进行欠曝光照片增强。收录于IEEE/CVF计算机视觉与模式识别会议论文集（2019年），第6849–6857页。'
- en: '[70] Wang, X., Yu, F., Dunlap, L., Ma, Y.-A., Wang, R., Mirhoseini, A., Darrell,
    T., and Gonzalez, J. E. Deep mixture of experts via shallow embedding. In Uncertainty
    in Artificial Intelligence (2020), PMLR, pp. 552–562.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Wang, X., Yu, F., Dunlap, L., Ma, Y.-A., Wang, R., Mirhoseini, A., Darrell,
    T., 和 Gonzalez, J. E. 通过浅层嵌入的深度专家混合。收录于人工智能中的不确定性（2020年），PMLR，第552–562页。'
- en: '[71] Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y., Dong, C., Qiao, Y., and Change Loy,
    C. Esrgan: Enhanced super-resolution generative adversarial networks. In Proceedings
    of the European conference on computer vision (ECCV) workshops (2018), pp. 0–0.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y., Dong, C., Qiao, Y., 和 Change
    Loy, C. Esrgan：增强型超分辨率生成对抗网络。收录于欧洲计算机视觉会议（ECCV）研讨会论文集（2018年），第0–0页。'
- en: '[72] Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image quality
    assessment: from error visibility to structural similarity. IEEE transactions
    on image processing 13, 4 (2004), 600–612.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Wang, Z., Bovik, A. C., Sheikh, H. R., 和 Simoncelli, E. P. 图像质量评估：从错误可见性到结构相似性。IEEE图像处理学报
    13, 4（2004年），第600–612页。'
- en: '[73] Xu, X., Sun, D., Pan, J., Zhang, Y., Pfister, H., and Yang, M.-H. Learning
    to super-resolve blurry face and text images. In Proceedings of the IEEE international
    conference on computer vision (2017), pp. 251–260.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Xu, X., Sun, D., Pan, J., Zhang, Y., Pfister, H., 和 Yang, M.-H. 学习超分辨率以解决模糊人脸和文本图像问题。收录于IEEE国际计算机视觉会议论文集（2017年），第251–260页。'
- en: '[74] Zhao, G., Liu, J., Jiang, J., Guan, H., and Wen, J.-R. Skip-connected
    deep convolutional autoencoder for restoration of document images. In 2018 24th
    International Conference on Pattern Recognition (ICPR) (2018), IEEE, pp. 2935–2940.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Zhao, G., Liu, J., Jiang, J., Guan, H., 和 Wen, J.-R. 跳跃连接深度卷积自编码器用于文档图像恢复。收录于2018年第24届国际模式识别会议（ICPR）论文集（2018年），IEEE，第2935–2940页。'
