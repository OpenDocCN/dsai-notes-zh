- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:49:13'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2112.02719] A Survey on Deep learning based Document Image Enhancement'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2112.02719](https://ar5iv.labs.arxiv.org/html/2112.02719)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Deep learning based Document Image Enhancement
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zahra Anvari
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science and Engineering
  prefs: []
  type: TYPE_NORMAL
- en: University of Texas Arlington
  prefs: []
  type: TYPE_NORMAL
- en: Arlington, TX
  prefs: []
  type: TYPE_NORMAL
- en: zahra.anvari@mavs.uta.edu
  prefs: []
  type: TYPE_NORMAL
- en: '&Vassilis Athitsos'
  prefs: []
  type: TYPE_NORMAL
- en: Department of Computer Science and Engineering
  prefs: []
  type: TYPE_NORMAL
- en: University of Texas Arlington
  prefs: []
  type: TYPE_NORMAL
- en: Arlington, TX
  prefs: []
  type: TYPE_NORMAL
- en: athitsos@uta.edu
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Digitized documents such as scientific articles, tax forms, invoices, contract
    papers, historic texts are widely used nowadays. These document images could be
    degraded or damaged due to various reasons including poor lighting conditions,
    shadow, distortions like noise and blur, aging, ink stain, bleed-through, watermark,
    stamp, *etc*. Document image enhancement plays a crucial role as a pre-processing
    step in many automated document analysis and recognition tasks such as character
    recognition. With recent advances in deep learning, many methods are proposed
    to enhance the quality of these document images. In this paper, we review deep
    learning-based methods, datasets, and metrics for six main document image enhancement
    tasks, including binarization, debluring, denoising, defading, watermark removal,
    and shadow removal. We summarize the recent works for each task and discuss their
    features, challenges, and limitations. We introduce multiple document image enhancement
    tasks that have received little to no attention, including over and under exposure
    correction, super resolution, and bleed-through removal. We identify several promising
    research directions and opportunities for future research.
  prefs: []
  type: TYPE_NORMAL
- en: '*K*eywords Document Image Enhancement, Image Enhancement, Document Image Analysis
    and recognition, Deep Learning'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Digitized documents such as scientific articles, tax forms, invoices, contract
    papers, personnel records, legal documents, historic texts, *etc.* are ubiquitous
    and widely used nowadays. These documents can be damaged due to watermark, stamps,
    aging, ink stains, bleed-through, etc., or can be degraded during the digitization
    process due to poor lighting conditions, shadow, camera distortion like noise
    and blur, etc. [[22](#bib.bib22), [63](#bib.bib63), [41](#bib.bib41), [7](#bib.bib7),
    [36](#bib.bib36), [29](#bib.bib29)].
  prefs: []
  type: TYPE_NORMAL
- en: Degraded document images have low visual quality and legibility. They could
    contain handwritten or machine printed text, or a mixture of both. In addition,
    they could contain multiple handwriting styles with different languages. Further
    complicating matter, the machine used to print the document could have used various
    technologies with variable quality ( e.g., documents printed in low DPI), thus
    affecting the quality of the image captured. Moreover, old documents could be
    degraded over time due to different reasons, such as humidity, being washed out,
    poor storage, low quality medium, etc. Therefore, there are many factors that
    affect the quality and legibility of the digitized document images.
  prefs: []
  type: TYPE_NORMAL
- en: The degraded document images make automated document analysis tasks such as
    character recognition (OCR) very challenging and such tasks perform poorly on
    these images. On the other hand, it is impractical and sometimes infeasible to
    manually enhance such images, especially in large scale, thus it is essential
    to develop methods that can automatically enhance the visual quality and the legibility
    of these images and restore the corrupted parts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Document image enhancement problem consists of several tasks that are studied
    in the literature. In this survey, we focus on six main tasks that are illustrated
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep learning based
    Document Image Enhancement"), and we explain each task in details in Section [2](#S2
    "2 Document Image Enhancement Tasks ‣ A Survey on Deep learning based Document
    Image Enhancement"). Here we summarize these tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a3422a1caa25d7bddbeaf9564466403.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Document image enhancement problems.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Binarization: It aims at separating the background from the foreground (i.e.,
    text) in order to remove noise, ink stain, bleed-through, wrinkles, *etc*. The
    output of this task is a binary image with two classes: foreground and background.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deblur: This task aims at removing various blur types, e.g., Gaussian, motion,
    de-focus, *etc.*, from the document images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Denoise: Denoising aims at removing various noise types, e.g., salt and pepper,
    wrinkles, dog-eared, background, and stain, etc. from the document images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Defade: It aims at improving the faded document images. A document could be
    faded due to againg, overexposure, or being washed out, etc.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Watermark removal: Some documents, e.g., financial forms, can contain watermarks,
    and the text underneath a watermark might not be recognizable. This task aims
    at removing such watermarks.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shadow removal: Blocking the source of light while capturing an image (usually
    by a phone) could leave shadows on the captured document image. This task aims
    at estimating the show and removing it.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With recent advances in deep learning, deep learning-based approaches have been
    proposed and applied to different computer vision and image processing tasks,
    such as object detection [[37](#bib.bib37), [56](#bib.bib56)], semantic segmentation [[38](#bib.bib38)],
    face detection and dataset creation [[2](#bib.bib2), [35](#bib.bib35), [59](#bib.bib59)],
    and image enhancement [[3](#bib.bib3), [17](#bib.bib17), [16](#bib.bib16)], etc.
    It has been shown that such deep learning-based methods achieve promising results
    and surpass the traditional methods. Similarly, deep learning-based methods for
    document image enhancement problems have received a great deal of attention over
    the past few years. The goal of this survey is to review these methods and discuss
    their features, advantages, disadvantages, challenges, and limitations, and identify
    opportunities for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 'To the best of our knowledge, this survey is the first survey of the recent
    advances in deep learning-based document image enhancement methods. We have several
    key contributions in this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We review recent advances, mostly from the past five years, on deep learning-based
    methods for document image enhancement, to help readers and researchers to better
    understand this area of research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide an overview of six main document image enhancement problems, including
    binarization, deblure, denoise, defade, watermark removal, and shadow removal.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We review the state-of-the-art methods, and discuss their features, advantages,
    and disadvantages to help researchers and investigators to select suitable methods
    for their need.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce several important document image enhancement tasks that have received
    little to no attention, such as bleed through removal.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We identify several open problems and promising research directions and opportunities
    for future research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2 Document Image Enhancement Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe six main document image enhancement tasks, including
    binarization, debluring, denoising, defading, watermark removal, and shadow removal.
    Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Debluring ‣ 2 Document Image Enhancement Tasks
    ‣ A Survey on Deep learning based Document Image Enhancement") shows some image
    examples for each of these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Binarization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Document image binarization refers to the process of segmenting a gray scale
    or color image to a black-and-white or binary image with only text and background.
    During this process any existing degradations such as bleed-through, noise, stamp,
    ink stains, faded characters, artifacts, etc. are removed. Formally, it seeks
    a decision function $f_{binarize}(·)$ for a document image $D_{orig}$ of width
    $W$ and height $H$, such that the resulting image $D_{binarized}$ of the same
    size only contains binary values while the overall document legibility is at least
    maintained if not enhanced.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D_{binarized}=f_{binarize}(D_{orig})$ |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: Figure [2(a)](#S2.F2.sf1 "In 2.2 Debluring ‣ 2 Document Image Enhancement Tasks
    ‣ A Survey on Deep learning based Document Image Enhancement") shows an example
    of an image along with its binarized one.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Debluring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nowadays, smartphones are widely used to digitize documents. This might propose
    various issues. The most prevalent one is the blur that might be introduced during
    capturing process. For instance, movement of the document, camera being out of
    focus, and camera shakes can add blur to the captured image. Figure [2(b)](#S2.F2.sf2
    "In 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey on Deep learning
    based Document Image Enhancement") shows an example of a blurry document image
    along with its corresponding clean one.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of deblurring methods is to recover the clean or deblurred version
    of the blurry document image. These methods could be prior-based or learning based.
    The former ones attempt to estimate the blur kernel and the corresponding parameters
    to detect blur and use these parameters to remove it, thus recover the clean images.
    The learning-based methods which are also called data-driven methods are widely
    used in the past decade. These methods take advantage of the deep neural networks
    and large amount of data to propose a deblurring model that can recover the clean
    image without requiring any priors.
  prefs: []
  type: TYPE_NORMAL
- en: Document image deblurring is an ill-posed problem and it is a more challenging
    problem compared to natural/non-document image deblurring. One of the main reasons
    is that the performance of the OCR engines directly depend on the quality of the
    document images that are input to them. If the legibility and the quality of these
    document images were low, the performance of the OCR output will be affected accordingly.
    Therefore, the enhanced document images not only need to be visually improved
    they also need to become more legible.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13a914982ee2f0a6c1f7e95eb539f815.png)  ![Refer to caption](img/8a8d6201914411c6530e4d4b16f6343c.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Binarization task [[55](#bib.bib55)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b67a34a082582a243f41fd0c1b27f321.png)  ![Refer to caption](img/ee06a7741657b1880b7d38b12723038d.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Deblur task. [[23](#bib.bib23)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2f001d65954264684f9a98784ee7e250.png)  ![Refer to caption](img/0e34197e8a7f173707b8fe701cf1981a.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Defade task.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9acf15a1a860b40a363393c8ff5f0e2a.png)  ![Refer to caption](img/b0ac0a9bde02ba8c0efe289b36c293de.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Denoise task. [[32](#bib.bib32)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/42b6554003362008ce7299580b9fda68.png)  ![Refer to caption](img/5fd5138fb773cf19bda77d1a01dc76f3.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Shadow removal task. [[36](#bib.bib36)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a94ba4bd450bfac76d39f8433641c300.png)  ![Refer to caption](img/2edd1b0e5983c849dba173e68358dc06.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Watermark removal task [[61](#bib.bib61)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: Sample images for different document image enhancement tasks. The
    image on the left is the input, and the right image is the output of each task.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Defading
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Defading is the process of recovering documents’ text that have become faded/faint.
    Documents’ content can be faded due to different factors. For instance, the ink
    can wear off over time, which is more prevalent in old documents. Sun-light or
    overexposure while digitizing the document can also make the document content
    lightened and hard to read. In addition, the handwriting or the printed text can
    be faint in the first place and deteriorate over time. This type of degradation
    poses issues such as low visual quality, poor legibility, and poor OCR performance.
    Defading methods mainly attempt to increase the visibility and recover a more
    legible version of the document image. Figure [2(c)](#S2.F2.sf3 "In 2.2 Debluring
    ‣ 2 Document Image Enhancement Tasks ‣ A Survey on Deep learning based Document
    Image Enhancement") shows an example of a defaded document image and its corresponding
    ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Denoising
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some documents may contain artifacts such as salt and pepper noise, stamps,
    annotations, ink or coffee stains, wrinkles, *etc.* The image recovery is even
    harder when certain types of these artifacts cover the text specially in cases
    where the artifacts color is similar to or darker than the document text color.
    To improve the visual quality of these document images alongside the legibility,
    approaches that recover the clean version of the degraded documents are proposed.
    The methods that attempt to remove these artifacts include document image denoising,
    cleanup and binarization methods. Figure [2(d)](#S2.F2.sf4 "In 2.2 Debluring ‣
    2 Document Image Enhancement Tasks ‣ A Survey on Deep learning based Document
    Image Enhancement") illustrates an example of a noisy document image and its ground
    truth.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Shadow Removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Documents can be digitized using scanners or mobile phone cameras. In the past,
    scanners were commonly used for digitizing documents with high quality, but with
    the prevalence of mobile phones more people tend to use their phones cameras in
    place of scanners to capture digital copies of their documents.
  prefs: []
  type: TYPE_NORMAL
- en: The document images captured using mobile phones are vulnerable to shadows mainly
    because the light sources are often blocked by the camera or even the person’s
    hand. Furthermore, even in the absence of objects that could be a source of occlusion,
    the lighting is often uneven when the document image is being captured in the
    real life. Therefore, document images digitized by mobile phone cameras in particular
    can suffer from shadows blocking a portion or all of the document and also uneven
    lighting and shading. These result in poor visual quality and legibility. Shadow
    removal methods focus on estimating the shadow casted on the document image and
    attempt to remove that in order to recover a clean, evenly lit document image
    which is more legible than the shadowed version. Figure [3(a)](#S2.F3.sf1 "In
    Figure 3 ‣ 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey on Deep
    learning based Document Image Enhancement") presents a sample of a document image
    with shadow and its ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: 2.6 Watermark Removal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Some documents, *e.g.,* financial forms, may contain one or multiple watermarks
    which occlude the document texts or makes it hard to read. Similar to denoising,
    the document image recovery is even harder in cases where the watermark color
    is the same or darker than the document text color or the watermark is thick and
    dense. Hence, we need approaches that recover the clean version of the degraded
    documents. Watermark removal methods focus on removing watermarks in order to
    increase the visual quality and legibility of the document images. Figure [3(b)](#S2.F3.sf2
    "In Figure 3 ‣ 2.2 Debluring ‣ 2 Document Image Enhancement Tasks ‣ A Survey on
    Deep learning based Document Image Enhancement") shows an image sample along with
    its ground truth for this task.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe datasets that are used in the literature for different
    document image enhancement tasks. Table [1](#S3.T1 "Table 1 ‣ 3 Datasets ‣ A Survey
    on Deep learning based Document Image Enhancement") provides the specifications
    of these datasets and we describe them in more details in below. In addition,
    Figure [5](#S3.F5 "Figure 5 ‣ 3 Datasets ‣ A Survey on Deep learning based Document
    Image Enhancement") shows image samples from these datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Task | No. of images | Resolution(Pixels) | Real vs. synthetic
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Bishop Bickley diary [[9](#bib.bib9)] | Binarization | 7 | 1050 x 1350 |
    Real |'
  prefs: []
  type: TYPE_TB
- en: '| NoisyOffice [[12](#bib.bib12)] | Denoising | 288 | Variable | Real/Synthetic
    |'
  prefs: []
  type: TYPE_TB
- en: '| S-MS [[21](#bib.bib21)] | Multiple | 240 | 1001 x 330 | Synthetic |'
  prefs: []
  type: TYPE_TB
- en: '| Tobacco 800 [[32](#bib.bib32)] | Denoising | 1290 | (1200x1600) - (2500x3200)
    | Real |'
  prefs: []
  type: TYPE_TB
- en: '| DIBCO’17 | Binarization | 10 | (1050x608) - (2092x951) | Real |'
  prefs: []
  type: TYPE_TB
- en: '| H-DIBCO’17 | Binarization | 10 | (351x292) - (2439x1229) | Real |'
  prefs: []
  type: TYPE_TB
- en: '| SmartDoc-QA [[44](#bib.bib44)] | Deblurring | 4260 | - | Real |'
  prefs: []
  type: TYPE_TB
- en: '| Blurry document images [[23](#bib.bib23)] | Deblurring | 3M train/35K validation
    | 300 x 300 | Synthetic |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Specifications of the datasets used for different document image enhancement
    tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bickley diary [[9](#bib.bib9)]: The images of Bickley diary dataset are taken
    from a photocopy of a diary that is written about 100 years ago. These images
    suffer from different kinds of degradation, such as water stains, ink bleed-through,
    and significant foreground text intensity. This dataset contains 7 document images/pages
    along with the binarized/clean ground truth images.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NoisyOffice [[12](#bib.bib12)]: This dataset contains two sets of images: 1)
    Real Noisy Office: it contains 72 grayscale images of scanned noisy images, 2)
    Simulated Noisy Office: it contains 72 grayscale images of scanned simulated noisy
    images for training, validation and test. The images in this dataset contain various
    styles of text, to which synthetic noise has been added to simulate real-world,
    messy artifacts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'S-MS (Synchromedia MultiSpectral Ancient document) [[21](#bib.bib21)]: Multi-spectral
    imaging (MSI) represents an innovative and non-destructive technique for the analysis
    of materials such as ancient documents. They collected a multispectral image database
    of ancient handwritten letters. This database consists of multispectral images
    of 30 real historical handwritten letters. These extremely old documents were
    all written by iron gall ink and date from the 17th to the 20th century. Original
    documents were borrowed from Quebec’s national library and have been imaged using
    a CROMA CX MSI camera. Through this process, they produced 8 images for each document
    resulting in total of 240 images of real documents.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8486a869ff8c05cf5fa7263aabf12e73.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Sample image from Bickley Diary Dataset [[9](#bib.bib9)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/457777d54db1ac3762b348ae7ae292f1.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Sample image the dataset introduced in [[23](#bib.bib23)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c41d77fe1a37435d9ba96a8729ef7551.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Sample image from DIBCO Dataset [[55](#bib.bib55)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b248f87b5e329779573491c0d19145ab.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Sample image from SmartDoc-QA Dataset [[44](#bib.bib44)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bdaa491c7aa1e83bd8dfadcb52b95a4c.png)'
  prefs: []
  type: TYPE_IMG
- en: (e) Sample image from PHIDB dataset [[43](#bib.bib43)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/062d5c15cbce412ee77450f94ff60fb4.png)'
  prefs: []
  type: TYPE_IMG
- en: (f) Sample image from S-MS Dataset [[21](#bib.bib21)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 4: Sample images from datasets for document image enhancements tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ee45259a0c827b55c34c33dff282be71.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Sample image from Tobacco Dataset [[32](#bib.bib32)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c71f97d91ccc890bcddc9816b17babb2.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Sample image from Noisy Office Dataset [[32](#bib.bib32)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/64cb1e7187bb9c0a2813ddd18ea38489.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Sample image from MCS dataset [[20](#bib.bib20)]
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae8aa67cf1c412f2a896f4b6e2ee2260.png)'
  prefs: []
  type: TYPE_IMG
- en: (d) Sample image from H-DIBCO Dataset [[51](#bib.bib51)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Sample images from datasets for document image enhancements tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Tobacco 800 [[32](#bib.bib32)]: This is a publicly available subset of 42 million
    pages of documents that are scanned with various equipment. It contains real-world
    documents with different types of noise and artifact, such as stamps, handwritten
    texts, and ruling lines, on the signatures. Resolutions of documents in Tobacco800
    vary significantly from 150 to 300 DPI and the resolution of the document images
    vary from 1200x1600 to 2500x3200 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'DIBCO and H-DIBCO: These datasets were introduced for the Document Image Binarization
    Contest since 2009\. There are DIBCO 2009 [[15](#bib.bib15)], H-DIBCO 2010 [[49](#bib.bib49)],
    DIBCO 2011 [[50](#bib.bib50)], H-DIBCO 2012 [[51](#bib.bib51)], DIBCO 2013 [[52](#bib.bib52)],
    H-DIBCO 2014 [[46](#bib.bib46)], H-DIBCO 2016 [[54](#bib.bib54)], DIBCO 2017 [[55](#bib.bib55)],
    H-DIBCO 2014 [[46](#bib.bib46)], H-DIBCO 2018 [[53](#bib.bib53)]. DIBCO datasets
    contain both printed and handwritten document images mainly for the binarization
    task.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SmartDoc-QA [[44](#bib.bib44)]: This is a dataset for quality assessment of
    smartphone captured document images containing both single and multiple distortions.
    This dataset is created using smartphone’s camera captured document images, under
    varying capture conditions such as light, shadow, different types of blur and
    perspective angles. SmartDoc-QA is categorized in three subsets of documents:
    contemporary documents, old administrative documents and shop’s receipts.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Blurry document images (BMVC) [[23](#bib.bib23)]: The training data contains
    3M train and 35k validation 300x300 image patches. Each patch is extracted from
    a different document page and each blur kernel used is unique.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Monk Cuper Set (MSC) [[20](#bib.bib20)]: This dataset contains 25 pages sampled
    from real historical documents which are collected from the Cuper book collection
    of the Monk system [[68](#bib.bib68)]. MSC documents suffer from heavy bleed-through
    degradations and textural background.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Persian heritage image binarization dataset (PHIDB) [[43](#bib.bib43)]: The
    PHIBD 2012 dataset contains 15 historical document images with their corresponding
    ground truth binary images. The historical images in this dataset suffer from
    various types of degradation. In particular two types of foreground text degradation
    are nebulous, and weak strokes/sub-strokes and the background degradation types
    are global bleed-through, local bleed-through, unwanted lines/patterns, and alien
    ink.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the evaluations metrics that are used in the literature
    for different document image enhancement tasks.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peak signal-to-noise ratio (PSNR): PSNR is a referenced-based metric. It provides
    a pixel-wise evaluation and is capable of indicating the effectiveness of document
    enhancement methods in terms of visual quality. PSNR measures the ratio between
    the maximum possible value of a signal and the power of distorting noise that
    affects the quality. In other words, it measures the closeness of two images.
    The higher the value of PSNR, the higher the similarity of the two images. MAX
    is the maximum possible pixel value of the image. When the pixels are represented
    using 8 bits per sample, MAX is 255\. Given two MxN images, this metric would
    be formulated as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $PSNR=10\log(\frac{MAX^{2}}{MSE})$ |  | (2) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $MSE=\frac{\sum_{x=1}^{M}\sum_{y=1}^{N}(I(x,y)-I^{{}^{\prime}}(x,y))^{2}}{MN}$
    |  | (3) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Structural Similarity Index (SSIM) [[72](#bib.bib72)]: SSIM is a reference-based
    metric designed to measure the structural similarity between two images and quantifies
    image quality degradation. SSIM computation requires two images from the same
    image, a reference image and a processed image. It actually measures the perceptual
    difference between two similar images. This metric extracts three key features
    from an image: luminance, contrast, and structure. The comparison between the
    two images is performed on the basis of these three features.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Character Error Rate (CER): Character Error Rate is computed based on the Levenshtein
    distance. It is the minimum number of character-level operations required to transform
    the ground truth or reference text into the OCR output text. CER is formulated
    as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $CER=\frac{S+D+I}{N}$ |  | (4) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where $S$ is the number of Substitutions, $D$ is the number of Deletions, $I$
    is the number of Insertions, and $N$ is the number of characters in reference
    or ground truth text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'CER represents the percentage of characters in the reference text that was
    incorrectly predicted or mis-recognized in the OCR output. The lower the CER value
    the better the performance of the OCR model. CER can be normalized to ensure that
    it will not fall out of the 0-100 range due to many insertions. In normalized
    CER, $C$ is the number of correct recognition. Normalized CER is formulated as
    follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $CER_{normalized}=\frac{S+D+I}{S+D+I+C}$ |  | (5) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Word Error Rate (WER): Word Error Rate can be more used for evaluating the
    OCR performance on paragraphs and sentences. WER is formulated in below:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $WER=\frac{S_{w}+D_{w}+I_{w}}{N}$ |  | (6) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: WER is computed similar to CER, but WER operates at word level. It represents
    the number of word substitutions, deletions, or insertions needed to transform
    one sentence into another.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'F-measure [[52](#bib.bib52)]: The F-measure score is the harmonic mean of the
    precision and recall. Precision is the positive predictive value, and recall aka
    sensitivity is used in binary classification. F-measure is formulated as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $FM=\frac{2\times Recall\times Precision}{Recall+Precision}$ |  | (7)
    |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $Recall=\frac{TP}{TP+FN}$ |  | (8) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: '|  | $Precision=\frac{TP}{TP+FP}$ |  | (9) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: TP, FP, FN denote the True Positive, False Positive and False Negative values,
    respectively.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pseudo-FMeasure ($F_{ps}$) [[52](#bib.bib52)]: $F_{ps}$ is introduced in [[45](#bib.bib45)]
    and it utilizes pseudo-recall Rps and pseudo-precision $P_{ps}$. It follows the
    same formula as F-Measure explained above and is particularly used for the binarization
    task.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the case of pseudo-recall, the weights of the ground truth(GT) foreground
    are normalized according to the local stroke width. Generally, those weights are
    between [0,1]. In the case of pseudo-precision, the weights are constrained within
    an area that expands to the GT background taking into account the stroke width
    of the nearest $GT$ component. Inside this area, the weights are greater than
    one (generally between (1,2]) while outside this area they are equal to one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distance Reciprocal Distortion Metric (DRD) [[52](#bib.bib52)]: DRD metric
    is used to measure the visual distortion in binary document images [[39](#bib.bib39)].
    It correlates with the human visual perception and it measures the distortion
    for all pixels as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $DRD=\frac{\sum_{k=1}^{S}DRD_{k}}{NUBN}$ |  | (10) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: where NUBN is the number of the non-uniform 8x8 blocks in the GT image, and
    $DRD_{k}$ is the distortion of the kth flipped pixel that is calculated using
    a 5x5 normalized weight matrix $W_{Nm}$ as defined in [[39](#bib.bib39)]. $DRD_{k}$
    equals to the weighted sum of the pixels in the 5x5 block of the GT that differ
    from the centered kth flipped pixel at $(x,y)$ in the binarization result image
    (equation [11](#S4.E11 "In 7th item ‣ 4 Metrics ‣ A Survey on Deep learning based
    Document Image Enhancement")).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '|  | $DRD_{k}=\sum_{i=-2}^{2}\sum_{j=-2}^{2}\left&#124;{GT_{k}(i,j)-B_{k}(x,y)}\right&#124;\times
    W_{Nm}(i,j)$ |  | (11) |'
  prefs:
  - PREF_IND
  type: TYPE_TB
- en: 5 Document Image Enhancement Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we describe the main deep learning based methods for document
    image enhancement and discuss their features, challenges, and limitations. Most
    of these works focused on multiple tasks, therefore in this section we discuss
    the document enhancement methods chronologically. Table [3](#S5.T3 "Table 3 ‣
    5 Document Image Enhancement Methods ‣ A Survey on Deep learning based Document
    Image Enhancement") summarizes the advantages, disadvantages, and results of these
    methods. Below, we describe these methods in more details.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Document Image Enhancement Tasks | Document Type |'
  prefs: []
  type: TYPE_TB
- en: '| Binarization | Deblur | Denoise | Defade | Watermark Removal | Shadow Removal
    | Handwritten | Printed |'
  prefs: []
  type: TYPE_TB
- en: '| Gangeh et al. [[13](#bib.bib13)] | - | ✓ | ✓ | ✓ | ✓ | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[74](#bib.bib74)] | - | ✓ | ✓ | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Sharma et al. [[60](#bib.bib60)] | - | ✓ | - | ✓ | ✓ | - | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[36](#bib.bib36)] | - | - | - | - | - | ✓ | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| Souibgui et al. [[61](#bib.bib61)] | - | ✓ | - | - | ✓ | - | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Gangeh et al. [[14](#bib.bib14)] | - | ✓ | - | - | ✓ | - | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Hradiš et al. [[23](#bib.bib23)] | - | ✓ | - | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Jemni et al. [[25](#bib.bib25)] | ✓ | - | - | - | - | - | ✓ | - |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[73](#bib.bib73)] | ✓ | - | - | - | - | - | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Souibgui et al. [[62](#bib.bib62)] | - | ✓ | - | - | - | ✓ | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Calvo-Zaragoza et al. [[6](#bib.bib6)] | ✓ | - | - | - | - | - | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Dey et al. [[10](#bib.bib10)] | ✓ | - | ✓ | - | - | - | - | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[33](#bib.bib33)] | ✓ | - | - | - | - | - | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: (a) Tasks and document types handled by the of main methods reviewed in this
    paper
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | GAN | CNN | Paired vs. unpaired supervision |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Gangeh et al. [[13](#bib.bib13)] | ✓ | - | Unpaired |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[74](#bib.bib74)] | - | ✓ | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Sharma et al. [[60](#bib.bib60)] | ✓ | - | Unpaired |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[36](#bib.bib36)] | ✓ | - | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Souibgui et al. [[61](#bib.bib61)] | ✓ | - | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Gangeh et al. [[14](#bib.bib14)] | - | ✓ | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Hradiš et al. [[23](#bib.bib23)] | - | ✓ | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Jemni et al. [[25](#bib.bib25)] | ✓ | - | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[73](#bib.bib73)] | ✓ | - | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Souibgui et al. [[62](#bib.bib62)] | ✓ | - | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Calvo-Zaragoza et al. [[6](#bib.bib6)] | - | ✓ | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Dey et al. [[10](#bib.bib10)] | - | ✓ | Paired |'
  prefs: []
  type: TYPE_TB
- en: '| Li et al. [[33](#bib.bib33)] | - | ✓ | Paired |'
  prefs: []
  type: TYPE_TB
- en: (b) Methodologies used in the reviewed methods.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Description of main methods reviewed in this paper.'
  prefs: []
  type: TYPE_NORMAL
- en: The method introduced in [[23](#bib.bib23)] is proposed for document image deblurring
    problem. The authors proposed a small and computationally efficient convolutional
    neural network model to deblur images without assuming any priors. In particular
    the authors focused on a combination of realistic de-focus blur and camera shake
    blur. They demonstrated that the proposed network significantly outperform existing
    blind deconvolution methods both in terms of image quality, PSNR, and OCR accuracy,
    CER. The proposed model can also be used on mobile devices as well.
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | Advantages | Disadvantages | Results |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Gangeh et al. [[13](#bib.bib13)] | - Handles multiple noises including salt
    and pepper noise, faded, blurred, and watermarked documents in an end-to-end manner.
    - It does not rely on paired document images. | - Computationally complex. | -
    Method has best results in terms of PSNR and OCR as compared to previous three
    methods. |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[74](#bib.bib74)] | - Method is fast and easy to implement.
    | - Inadequate qualitative and qualitative results. | - Marginal PSNR improvement.
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sharma et al. [[60](#bib.bib60)] | - Adaptable for both paired and unpaired
    supervision scenarios. | - | - Marginal improvement in terms of PSNR. |'
  prefs: []
  type: TYPE_TB
- en: '| Lin et al. [[36](#bib.bib36)] | - First deep learning-based approach for
    shadow removal. - It works on both gray-scale and RGB images. | - Computationally
    complex. - It does not work well on images with complex background and layouts.'
  prefs: []
  type: TYPE_NORMAL
- en: '- It works well on partially shadowed documents only. | - It achieves the best
    results in terms of PSNR/SSIM compared to four previous work when evaluated on
    five different datasets. - It also generalizes relatively well on real-world images.
    |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Souibgui et al. [[61](#bib.bib61)] | - Flexible architecture could be used
    for other document degradation problems. - First work on dense watermark and stamp
    removal problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Generalize well on real-world images.'
  prefs: []
  type: TYPE_NORMAL
- en: '- Pre-trained models are publicly available. | - Computationally complex. -
    It needs a threshold to be pre-determined and needs to be tuned per image which
    makes this method less practical. | Binarization: Achieves best results in terms
    of PSNR, $F_{measure}$, $F_{ps}$ and DRD compared to top five competitors. Watermark:
    Achieves best results in terms of PSNR/SSIM compared to three previous work.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Deblur: Achieves best results in terms of PSNR compared to two previous work.
    |'
  prefs: []
  type: TYPE_NORMAL
- en: '| Gangeh et al. [[14](#bib.bib14)] | - Works on both gray-scale and RGB watermarks.
    - Works on blurry images with various intensity. | - Inadequate quantitative evaluation
    and comparison with previous work. | - Effectively removes watermark and blur.
    - Improved OCR on a small test set of nine images. |'
  prefs: []
  type: TYPE_TB
- en: '| Hradiš et al. [[23](#bib.bib23)] | - Small and computationally efficient
    network. - Can be used on mobile devices. | - Adds ringing artifacts in some situations.
    - Does not work well on uncommon words when the image is severely blurred. | -
    Outperforms other methods in terms of PSNR and Character Error Rate compared to
    previous four work. |'
  prefs: []
  type: TYPE_TB
- en: '| Xu et al. [[73](#bib.bib73)] | - Computationally efficient network. - It
    deblurs and super-resolves simultaneously. | - Does not generalize well for generic
    images. - OCR performance evaluation is ignored and only visual quality of the
    documents are evaluated. | - Performs favorably against previous work on both
    synthetic and real-world datasets. |'
  prefs: []
  type: TYPE_TB
- en: '| Souibgui et al. [[62](#bib.bib62)] | - It handles multiple camera distortions.
    - It incorporates a text recognizer for generating more legible images. | - Model
    only processed and trained on single lines and can not handle full pages. | -
    Achieves best results in terms of Character Error Rate and second best in terms
    of PSNR/SSIM compared to previous three work. |'
  prefs: []
  type: TYPE_TB
- en: 'Table 3: Comparison of document image enhancement methods.'
  prefs: []
  type: TYPE_NORMAL
- en: In another document image deblurring work [[73](#bib.bib73)], the authors proposed
    an algorithm to directly restore a high-resolution de-blurred image from a blurry
    low-resolution input. Other deblurring methods such as Hradis et al. [[23](#bib.bib23)]
    cannot be easily extended for joint super-resolution and deblurring tasks. This
    work focuses on blurry face and blurry document images distributions and a multi-class
    GAN model was developed to learn a category-specific prior and process multi-class
    image restoration tasks, using a single generator network. The authors employed
    a deep CNN architecture proposed by Hradis et al. [[23](#bib.bib23)] in an adversarial
    setting. Unlike Hradis et al., in this work the generator network contains upsampling
    layers, which are fractionally-strided convolutional layers aka deconvolution
    layers. The generator first upsamples low-resolution blurry images, and then performs
    convolutions to generate clear images thus the output would be both super-resolved
    and deblurred. Since their model has a discriminator network in addition to the
    generator network, it is more complex and has more parameters compared to model
    proposed in [[23](#bib.bib23)].
  prefs: []
  type: TYPE_NORMAL
- en: The visual quality of the generated images were evaluated in terms of PSNR and
    SSIM but the deblurred document images were not evaluated in terms of OCR performance
    and no Character Error Rate or Word Error Rate which are OCR performance evaluation
    metrics are reported. In terms of PSNR/SSIM, this work performs favorably against
    previous work on both synthetic and real-world datasets.
  prefs: []
  type: TYPE_NORMAL
- en: One limitation of this work is that since the model is trained on multi-class
    images, it is essentially designed to approximate the mixture distribution of
    these two classes of images and when this mixture distribution becomes too complex,
    it is difficult to learn a unified model to cover the diversity of all image classes.
    Therefore, this method is less effective for generic images.
  prefs: []
  type: TYPE_NORMAL
- en: Authors in  [[66](#bib.bib66)] focused on the degraded historical manuscript
    images binarization, and formulated binarization task as a pixel classification
    learning task. They developed a Fully Convolutional Network (FCN) architecture
    that operates at multiple image scales, including full resolution. The authors
    claimed that the proposed binarization technique can also be applied to different
    domains such as Palm Leaf Manuscripts with good performance.
  prefs: []
  type: TYPE_NORMAL
- en: Zhao et. al. [[74](#bib.bib74)] investigated the denoising and deblurring problems
    and proposed a method for document image restoration called Skip-Connected Deep
    Convolutional Autoencoder (SCDCA) which is based on residual learning. They employed
    two types of skip connections, identity mapping between convolution layers inspired
    by residual blocks, and another is defined to connect the input to the output
    directly. These connections assist the network to learn the residual content between
    the noisy and clean images instead of learning an ordinary transformation function.
    The proposed network was inspired by [[23](#bib.bib23)] which is a 15-layer CNN.
    Compared to method in  [[23](#bib.bib23)], the authors added batch normalization [[24](#bib.bib24)]
    and skip-connections [[19](#bib.bib19)] to accelerate the model convergence of
    the model and boost the performance.
  prefs: []
  type: TYPE_NORMAL
- en: In [[60](#bib.bib60)], the authors cast the image restoration problem as an
    image-to-image translation task i.e, translating a document from noisy domain
    (*i.e.,* background noise, blurred, faded, watermarked) to a target clean document
    using a GAN approach. To do so, they employed CycleGAN model which is an unpaired
    image-to-image translation network, for cleaning the noisy documents. They also
    synthetically created a document dataset for watermark removal and defading problems
    by inserting logos as watermarks and applying fading techniques on Google News
    dataset [[67](#bib.bib67)] of documents.
  prefs: []
  type: TYPE_NORMAL
- en: Authors in [[14](#bib.bib14)] proposed an end-to-end document enhancement pipeline
    which takes in blurry and watermarked document images and produces clean documents.
    They trained an auto-encoder model that works on different noise levels of documents.
    They adopted the neural network architecture described in [[40](#bib.bib40)] called
    REDNET and designed a REDNET with 15 convolutional layers and 15 deconvolutional
    layers, including 8 symmetric skip connections between alternate convolutional
    layers and the mirrored deconvolutional layers. The advantage of this method compared
    to fully convolutional network is that pooling and un-pooling, which tend to eliminate
    image details, is avoided for low-level image tasks such as image restoration.
    This results in higher resolution outputs. The key differences of this work from [[74](#bib.bib74)]
    is the use of larger dataset and training a blind model.
  prefs: []
  type: TYPE_NORMAL
- en: In [[67](#bib.bib67)] authors developed convolutional auto-encoders to learn
    an end-to-end map from an input image to its selectional output, in which the
    activations indicate the likelihood of pixels to be either foreground or background.
    Once trained, this model can be applied to documents to be binarized and then
    a global threshold will be applied. This approach has proven to outperform existing
    binarization strategies in a number of document types.
  prefs: []
  type: TYPE_NORMAL
- en: In DE-GAN [[61](#bib.bib61)], the authors proposed an end-to-end framework called
    Document Enhancement Generative Adversarial Networks. This network is based on
    conditional GANs and cGANs, a network to restore severely degraded document images.
    The tasks that are studied in this paper are document clean up, binarization,
    deblurring and watermark removal. Due to unavailability of a dataset for the watermark
    removal task, the authors synthetically created a watermark dataset including
    the watermarked images and their clean ground truth.
  prefs: []
  type: TYPE_NORMAL
- en: Authors in [[36](#bib.bib36)] proposed the Background Estimation Document Shadow
    Removal Network (BEDSR-Net) which is the first deep network designed for document
    image shadow removal. They designed a background estimation module for extracting
    the global background color of the document. During the process of estimating
    the background color, this module learns information about the spatial distribution
    of background and also the non-background pixels. They created an attention map
    through encoding this information. Having estimated the global background color
    and the attention map, the shadow removal network can now effectively recover
    the shadow-free document image. BEDSR-Net can fail in some situations including
    when there is no single dominant color, such as a paper entirely with a color
    gradient and another case is when the document is entirely shadowed, or multiple
    shadows were formed by multiple light sources.
  prefs: []
  type: TYPE_NORMAL
- en: In another work [[62](#bib.bib62)] the authors focused on documents that are
    digitized using smart phone’s cameras. They stated that these types of digitized
    documents are highly vulnerable to capturing various distortions including but
    not limited to perspective angle, shadow, blur, warping, etc. The authors proposed
    a conditional generative adversarial network that maps the distorted images from
    its domain into a readable domain. This model integrates a recognizer in the discriminator
    part for better distinguishing the generated document images.
  prefs: []
  type: TYPE_NORMAL
- en: In another study [[13](#bib.bib13)], an end-to-end unsupervised deep learning
    model to remove multiple types of noise, including salt & pepper noise, blurred
    and/or faded text, and watermarks from documents was proposed. In particular they
    proposed a unified architecture by integrating deep mixture of experts [[70](#bib.bib70)]
    with a cycle-consistent GAN as the base network for document image blind denoising
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: In [[10](#bib.bib10)], authors target document image cleanup problem on embedded
    applications such as smartphone apps, which usually have memory, energy, and latency
    limitations. They proposed a light-weight encoder-decoder CNN architecture, incorporated
    with perceptual loss. They proved that in terms of the number of parameters and
    product-sum operations, their models are 65-1030 and 3-27 times, respectively,
    smaller than existing SOTA document enhancement models.
  prefs: []
  type: TYPE_NORMAL
- en: In another work [[25](#bib.bib25)], authors focused on enhancing handwritten
    documents and proposed an end-to-end GAN-based architecture to recover the degraded
    documents. Unlike most document binarization methods, which only attempt to improve
    the visual quality of the degraded document, the proposed architecture integrates
    a handwritten text recognizer that promotes the generated document image to be
    also more legible. This approach is the first work to use the text information
    while binarizing handwritten documents. They performed experiments on degraded
    Arabic and Latin handwritten documents and showed that their model improves both
    the visual quality and the legibility of the degraded document images.
  prefs: []
  type: TYPE_NORMAL
- en: In [[33](#bib.bib33)], authors proposed a document binarization method called
    SauvolaNet. They investigated the classic Sauvola [[58](#bib.bib58)] document
    binarization method from the deep learning perspective and proposed a multi-window
    Sauvola model. They also introduced an attention mechanism to automatically estimate
    the required Sauvola window sizes for each pixel location therefore could effectively
    estimate the Sauvola threshold. The proposed network has three modules, Multi-Window
    Sauvola, Pixelwise Window Attention, and Adaptive Sauolva Threshold. The Multi-Window
    Sauvola module reflects the classic Sauvola but with trainable parameters and
    multi-window settings. The next module which is Pixelwise Window Attention that
    is in charge of estimating the preferred window sizes for each pixel. The other
    module, Adaptive Sauolva Threshold, combines the outputs from the other two modules
    and predicts the final adaptive threshold for each pixel. The SauvolaNet model
    significantly reduces the number of required network parameters and achieves SOTA
    performance for document binarization task.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Open Problems and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present open problems in this area and provide several directions
    for the future work. Document image enhancement tasks are far from solved and
    even some tasks are either not studied or studied in a very limited fashion. We
    discuss these problems and future work below.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Overexposure and underexposure correction tasks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overexposure problem occurs when too much light is captured while digitizing
    the document, mostly when the capturing device is a mobile phone and camera flash
    adds too much reflection or glare to the image (Figure [6(a)](#S6.F6.sf1 "In Figure
    6 ‣ 6.1 Overexposure and underexposure correction tasks ‣ 6 Open Problems and
    Future Directions ‣ A Survey on Deep learning based Document Image Enhancement")).
    This problem has received limited attention even in the image and photo enhancement
    domain [[1](#bib.bib1), [5](#bib.bib5)], and to the best of our knowledge no study
    has tried to address this problem for the document images. To address this issue
    with a deep learning based approach, training and testing datasets are required
    to be collected, as no public datasets are available that can be leveraged for
    this problem.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/34fe89aa30fc1e32d8c067121dee8685.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Over-exposure problem. Image obtained from [[4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e3f2853de1a482d068468f882d106cb8.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Under-exposure problem. Image obtained from [[42](#bib.bib42)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6: Open problems: over-exposure and under-exposure correction.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/147e58f485d8cb54be3ff0451d085e8f.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Sample slightly faded image studied in the literature [[13](#bib.bib13)].
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a5cdc6b75014822034b461eae0c353a4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Sample real-world image with severe and non-homogeneous fade. These types
    of fade is not studied in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 7: Open problem in defading task: Severely and/or non-homogeneously
    faded images.'
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, underexposure happens when the lighting condition is poor
    while digitizing the document and as a result the captured image becomes dark
    (Figure [6(b)](#S6.F6.sf2 "In Figure 6 ‣ 6.1 Overexposure and underexposure correction
    tasks ‣ 6 Open Problems and Future Directions ‣ A Survey on Deep learning based
    Document Image Enhancement")). This problem is different from shadow removal,
    as shadowed document images can be partly/non-uniformly dark [[36](#bib.bib36)].
    While low-light image enhancement problem received a lot of attention for photos [[26](#bib.bib26),
    [69](#bib.bib69), [18](#bib.bib18), [57](#bib.bib57)], it has not received much
    attention in document image enhancement [[34](#bib.bib34)]. One possible future
    work could be to evaluate the practicality of these methods over document images.
    Similar to overexposure correction task, developing deep learning based methods
    for this problem needs training/testing datasets, but such datasets are unavailable.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Defading task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fading could occur due to exposure to light, againg, being washed out, *etc*.
    This task is yet another ill-posed and under-studied task. Current work [[13](#bib.bib13)]
    makes two assumptions that may not be practical. They assume that the documents
    are uniformly faded, and the documents are very lightly faded (Fig. [7(a)](#S6.F7.sf1
    "In Figure 7 ‣ 6.1 Overexposure and underexposure correction tasks ‣ 6 Open Problems
    and Future Directions ‣ A Survey on Deep learning based Document Image Enhancement")),
    while in real-world scenarios the documents could be severely and/or non-homogeneously
    faded, e.g., aged or washed out documents (Fig. [7(b)](#S6.F7.sf2 "In Figure 7
    ‣ 6.1 Overexposure and underexposure correction tasks ‣ 6 Open Problems and Future
    Directions ‣ A Survey on Deep learning based Document Image Enhancement")).
  prefs: []
  type: TYPE_NORMAL
- en: Heavily and/or non-homogeneously faded documents are hard to read and very challenging
    for OCR and could considerably affect the performance of OCR, while lightly faded
    documents are usually still legible and recognizable to OCR. Therefore, to address
    these challenges we need to develop solutions that would take into account both
    severely and non-homogeneously faded documents. In addition, to train deep learning
    models (for both lightly and severely faded documents) training datasets are required,
    but similar to previous task discussed above no such datasets are publicly available.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Super-resolution task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Low-resolution documents are often hard to read and also very challenging to
    character recognition methods. Super-resolving low resolution document images
    can enhance the visual quality, readability of the text, and more importantly
    improve the OCR accuracy. Document image super-resolving is an ill-posed and challenging
    problem, especially when there are artifacts and noises present in the documents.
    Developing a model that super-resolves the document images in particular low-quality
    document images is even harder and more challenging.
  prefs: []
  type: TYPE_NORMAL
- en: One way to tackle this issue is to use Bicubic interpolation but such basic
    methods can introduce noise or exacerbate the noise/artifacts that the document
    in particular low-quality ones have. To increase the resolution of the document
    images and recover as much details as possible we need super resolution methods.
    Through super-resolving these document images, characters become more legible
    and it could boost the OCR performance as well.
  prefs: []
  type: TYPE_NORMAL
- en: While image/photo super-resolution problem has received a great deal of attention [[64](#bib.bib64),
    [71](#bib.bib71), [27](#bib.bib27), [30](#bib.bib30), [31](#bib.bib31), [11](#bib.bib11),
    [8](#bib.bib8), [40](#bib.bib40), [65](#bib.bib65), [28](#bib.bib28)], this task
    has received little attention for the document images [[47](#bib.bib47), [48](#bib.bib48)].
    As a future work, we need to develop effective super-resolution methods specifically
    designed for documents image with low-quality in order to improve the legibility
    and OCR performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4fcd6276accf924ace7ca62d8fb3dc57.png)  ![Refer to caption](img/9e59d10909eecc018dfb7fac511d18f1.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Low contrast problem. Low contrast text is not recovered.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dca717607d76f7ba6f6a06dca65a7c3e.png)  ![Refer to caption](img/ae2c60138d98d711b7358fe93687cb79.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Bleed-through problem. Bleed through text is not effectively removed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/07fcb46e2ba3086f0c1bd44f9f6f173a.png)  ![Refer to caption](img/c5ade3d79f1eaf3f8e9800b477c0f2f8.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) RGB document with various ink intensities. Text in lighter color i.e. orange,
    is not effectively recovered.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 8: Open problems in binarization task. Left side images are the original
    images, and the right side images are the binarized images using a recent state-of-the-art
    method.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Binarization task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While the binarization task has received a great deal of attention, there are
    still multiple scenarios that current binarization methods do not perform well
    on them. Specifically, when the image has low contrast or when ghosting and bleed
    through are present in the document, or when the image is RGB with various ink
    color and intensities. These scenarios are challenging for binarization methods
    to handle.
  prefs: []
  type: TYPE_NORMAL
- en: Ghosting in documents occurs when the ink or text from the other side of the
    page can be seen, but ink does not completely come through to the other side.
    Bleed-through on the other hand, happens when the ink seeps in to the other side
    and interferes with the text on the front page. Both issues make character recognition
    very challenging, specially bleed-through.
  prefs: []
  type: TYPE_NORMAL
- en: Figure [8(a)](#S6.F8.sf1 "In Figure 8 ‣ 6.3 Super-resolution task ‣ 6 Open Problems
    and Future Directions ‣ A Survey on Deep learning based Document Image Enhancement")
    shows a low contrast image and its binarized one. The current binarization methods
    are not able to recover the text properly when text has low contrast. Figure [8(b)](#S6.F8.sf2
    "In Figure 8 ‣ 6.3 Super-resolution task ‣ 6 Open Problems and Future Directions
    ‣ A Survey on Deep learning based Document Image Enhancement") presents another
    example with bleed though present in the image. As one can see, the method was
    not able to remove the bleed through completely. Figure [8(c)](#S6.F8.sf3 "In
    Figure 8 ‣ 6.3 Super-resolution task ‣ 6 Open Problems and Future Directions ‣
    A Survey on Deep learning based Document Image Enhancement") shows an example
    of an RGB image and its binarized one. As you can see, the method is not performing
    well over texts with orange color. Thus to address these issues we need to develop
    a method that would take them into account.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5 OCR performance evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the main purpose of document image enhancement is to enhance character
    recognition methods or OCR to facilitate automated document analysis. Currently
    there is no document image test dataset with the extracted ground truth text so
    that could be utilized to evaluate document images enhancement methods in terms
    of OCR improvement. Current methods either ignore to evaluate their methods in
    terms of OCR, or show the OCR improvement only on a few images which is not sufficient
    to prove the practicality of their methods in the wild. This calls for a separate
    study to collect such dataset and benchmark current methods against this test
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper we reviewed deep learning based methods for six different document
    image enhancement tasks, including binarization, debluring, denoising, defading,
    watermark removal, and shadow removal. We also summarized datasets used for these
    tasks along with the metrics used to evaluate the performance of these methods.
    We discussed the features, challenges, advantages and disadvantages of the deep
    learning based document image enhancement methods.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed open problems in this area and identified multiple important
    tasks that have received little to no attention. These tasks are over-exposure/under-exposure
    correction, defading, and super-resolution. Over-exposure problem usually occurs
    when the imaging device captures too much light or glare due to reflection, and
    under-exposure occurs when the lighting condition is poor and the captured image
    becomes dark and hard to read. Fading could happen due to sunlight, aging, and
    being washed out, *etc*. Low-resolution document images need to be super-resolved
    to enhance their visual quality and more importantly make small text more legible.
    Enhancing the document image resolution is more challenging when noise and artifacts
    are present in the document image. Such images are often hard to read and the
    low legibility affects the performance of character recognition techniques. The
    above-explained tasks have received little attention and they are far from solved.
  prefs: []
  type: TYPE_NORMAL
- en: Binarization task has received a great deal of attention over the past years,
    however, these methods underperform in multiple scenarios. For example, when the
    image has low contrast or multiple artifacts *e.g.,* stamp, signature, ghosting
    or bleed-through are present. Ghosting and bleed-through occur when the text from
    the other side of the document can be seen or ink seeps in to the other side of
    the document. These artifacts are challenging to remove and effective methods
    are needed to address and resolve these problems properly.
  prefs: []
  type: TYPE_NORMAL
- en: Current document image enhancement methods mainly focus on improving the visual
    quality of the images. While this is an important aspect, the performance of these
    methods for automatic document analysis problems, *e.g.,* character recognition,
    is largely ignored. Thus there is an emerging need to develop methods that can
    jointly enhance the visual quality and OCR performance. The OCR performance needs
    to be evaluated over a larger test dataset, and not just over a few samples as
    was done in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: All that said, current methods target only one problem, *e.g.,* debluring, at
    a time, but in reality a document image can have multiple issues at the same time.
    For example, a document image could be blurry, faded and noisy. To the best of
    our knowledge, currently these is no method that can tackle multiple issues in
    a single image at the same time.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Afifi, M., Derpanis, K. G., Ommer, B., and Brown, M. S. Learning to correct
    overexposed and underexposed photos. arXiv preprint arXiv:2003.11596 13 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Anvari, Z., and Athitsos, V. A pipeline for automated face dataset creation
    from unlabeled images. In Proceedings of the 12th ACM International Conference
    on PErvasive Technologies Related to Assistive Environments (2019), pp. 227–235.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Anvari, Z., and Athitsos, V. Enhanced cyclegan dehazing network. In VISIGRAPP
    (4: VISAPP) (2021), pp. 193–202.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Arlazarov, V. V., Bulatov, K. B., Chernov, T. S., and Arlazarov, V. L.
    Midv-500: a dataset for identity document analysis and recognition on mobile devices
    in video stream.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Cai, J., Gu, S., and Zhang, L. Learning a deep single image contrast enhancer
    from multi-exposure images. IEEE Transactions on Image Processing 27, 4 (2018),
    2049–2062.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Calvo-Zaragoza, J., and Gallego, A.-J. A selectional auto-encoder approach
    for document image binarization. Pattern Recognition 86 (2019), 37–47.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Chen, X., He, X., Yang, J., and Wu, Q. An effective document image deblurring
    algorithm. In CVPR 2011 (2011), IEEE, pp. 369–376.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Chu, M., Xie, Y., Leal-Taixé, L., and Thuerey, N. Temporally coherent gans
    for video super-resolution (tecogan). arXiv preprint arXiv:1811.09393 1, 2 (2018),
    3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Deng, F., Wu, Z., Lu, Z., and Brown, M. S. Binarizationshop: a user-assisted
    software suite for converting old documents to black-and-white. In Proceedings
    of the 10th annual joint conference on Digital libraries (2010), pp. 255–258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Dey, S., and Jawanpuria, P. Light-weight document image cleanup using
    perceptual loss. arXiv preprint arXiv:2105.09076 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Dong, C., Loy, C. C., He, K., and Tang, X. Image super-resolution using
    deep convolutional networks. IEEE transactions on pattern analysis and machine
    intelligence 38, 2 (2015), 295–307.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Dua, D., and Graff, C. UCI machine learning repository, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Gangeh, M. J., Plata, M., Motahari, H., and Duffy, N. P. End-to-end unsupervised
    document image blind denoising. arXiv preprint arXiv:2105.09437 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Gangeh, M. J., Tiyyagura, S. R., Dasaratha, S. V., Motahari, H., and Duffy,
    N. P. Document enhancement system using auto-encoders. In Workshop on Document
    Intelligence at NeurIPS 2019 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Gatos, B., Ntirogiannis, K., and Pratikakis, I. Icdar 2009 document image
    binarization contest (dibco 2009). In 2009 10th International conference on document
    analysis and recognition (2009), IEEE, pp. 1375–1382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Gu, S., Zuo, W., Guo, S., Chen, Y., Chen, C., and Zhang, L. Learning dynamic
    guidance for depth image enhancement. In Proceedings of the IEEE conference on
    computer vision and pattern recognition (2017), pp. 3769–3778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Guo, C., Li, C., Guo, J., Loy, C. C., Hou, J., Kwong, S., and Cong, R.
    Zero-reference deep curve estimation for low-light image enhancement. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (2020),
    pp. 1780–1789.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Guo, X., Li, Y., and Ling, H. Lime: Low-light image enhancement via illumination
    map estimation. IEEE Transactions on image processing 26, 2 (2016), 982–993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learning for image
    recognition. In Proceedings of the IEEE conference on computer vision and pattern
    recognition (2016), pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] He, S., and Schomaker, L. Deepotsu: Document enhancement and binarization
    using iterative deep learning. Pattern recognition 91 (2019), 379–390.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Hedjam, R., Nafchi, H. Z., Moghaddam, R. F., Kalacska, M., and Cheriet,
    M. Icdar 2015 contest on multispectral text extraction (ms-tex 2015). In 2015
    13th International Conference on Document Analysis and Recognition (ICDAR) (2015),
    IEEE, pp. 1181–1185.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Howe, N. R. Document binarization with automatic parameter tuning. International
    journal on document analysis and recognition (ijdar) 16, 3 (2013), 247–258.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Hradiš, M., Kotera, J., Zemcık, P., and Šroubek, F. Convolutional neural
    networks for direct text deblurring. In Proceedings of BMVC (2015), vol. 10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Ioffe, S., and Szegedy, C. Batch normalization: Accelerating deep network
    training by reducing internal covariate shift. In International conference on
    machine learning (2015), PMLR, pp. 448–456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Jemni, S. K., Souibgui, M. A., Kessentini, Y., and Fornés, A. Enhance
    to read better: An improved generative adversarial network for handwritten document
    image enhancement. arXiv preprint arXiv:2105.12710 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Jiang, Y., Gong, X., Liu, D., Cheng, Y., Fang, C., Shen, X., Yang, J.,
    Zhou, P., and Wang, Z. Enlightengan: Deep light enhancement without paired supervision.
    IEEE Transactions on Image Processing 30 (2021), 2340–2349.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Jo, Y., Oh, S. W., Kang, J., and Kim, S. J. Deep video super-resolution
    network using dynamic upsampling filters without explicit motion compensation.
    In Proceedings of the IEEE conference on computer vision and pattern recognition
    (2018), pp. 3224–3232.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Kim, J., Lee, J. K., and Lee, K. M. Deeply-recursive convolutional network
    for image super-resolution. In Proceedings of the IEEE conference on computer
    vision and pattern recognition (2016), pp. 1637–1645.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Kligler, N., Katz, S., and Tal, A. Document enhancement using visibility
    detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (2018), pp. 2374–2382.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Lai, W.-S., Huang, J.-B., Ahuja, N., and Yang, M.-H. Deep laplacian pyramid
    networks for fast and accurate super-resolution. In Proceedings of the IEEE conference
    on computer vision and pattern recognition (2017), pp. 624–632.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Ledig, C., Theis, L., Huszár, F., Caballero, J., Cunningham, A., Acosta,
    A., Aitken, A., Tejani, A., Totz, J., Wang, Z., et al. Photo-realistic single
    image super-resolution using a generative adversarial network. In Proceedings
    of the IEEE conference on computer vision and pattern recognition (2017), pp. 4681–4690.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Lewis, D., Agam, G., Argamon, S., Frieder, O., Grossman, D., and Heard,
    J. Building a test collection for complex document information processing. In
    Proceedings of the 29th annual international ACM SIGIR conference on Research
    and development in information retrieval (2006), pp. 665–666.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Li, D., Wu, Y., and Zhou, Y. Sauvolanet: Learning adaptive sauvola network
    for degraded document binarization. arXiv preprint arXiv:2105.05521 (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Li, X., Zhang, B., Liao, J., and Sander, P. V. Document rectification
    and illumination correction using a patch-based cnn. ACM Transactions on Graphics
    (TOG) 38, 6 (2019), 1–11.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Lin, W.-A., Chen, J.-C., Castillo, C. D., and Chellappa, R. Deep density
    clustering of unconstrained faces. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (2018), pp. 8128–8137.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Lin, Y.-H., Chen, W.-C., and Chuang, Y.-Y. Bedsr-net: A deep shadow removal
    network from a single document image. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition (2020), pp. 12905–12914.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., and
    Berg, A. C. Ssd: Single shot multibox detector. In European conference on computer
    vision (2016), Springer, pp. 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Long, J., Shelhamer, E., and Darrell, T. Fully convolutional networks
    for semantic segmentation. In Proceedings of the IEEE conference on computer vision
    and pattern recognition (2015), pp. 3431–3440.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Lu, H., Kot, A. C., and Shi, Y. Q. Distance-reciprocal distortion measure
    for binary document images. IEEE Signal Processing Letters 11, 2 (2004), 228–231.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Mao, X., Shen, C., and Yang, Y.-B. Image restoration using very deep convolutional
    encoder-decoder networks with symmetric skip connections. Advances in neural information
    processing systems 29 (2016), 2802–2810.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Mesquita, R. G., Mello, C. A., and Almeida, L. A new thresholding algorithm
    for document images based on the perception of objects by distance. Integrated
    Computer-Aided Engineering 21, 2 (2014), 133–146.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Michalak, H., and Okarma, K. Robust combined binarization method of non-uniformly
    illuminated document images for alphanumerical character recognition. Sensors
    20, 10 (2020), 2914.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Nafchi, H. Z., Ayatollahi, S. M., Moghaddam, R. F., and Cheriet, M. An
    efficient ground truthing tool for binarization of historical manuscripts. In
    2013 12th International Conference on Document Analysis and Recognition (2013),
    IEEE, pp. 807–811.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Nayef, N., Luqman, M. M., Prum, S., Eskenazi, S., Chazalon, J., and Ogier,
    J.-M. Smartdoc-qa: A dataset for quality assessment of smartphone captured document
    images-single and multiple distortions. In 2015 13th International Conference
    on Document Analysis and Recognition (ICDAR) (2015), IEEE, pp. 1231–1235.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Ntirogiannis, K., Gatos, B., and Pratikakis, I. Performance evaluation
    methodology for historical document image binarization. IEEE Transactions on Image
    Processing 22, 2 (2012), 595–609.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Ntirogiannis, K., Gatos, B., and Pratikakis, I. Icfhr2014 competition
    on handwritten document image binarization (h-dibco 2014). In 2014 14th International
    conference on frontiers in handwriting recognition (2014), IEEE, pp. 809–813.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Pandey, R. K., and Ramakrishnan, A. Language independent single document
    image super-resolution using cnn for improved recognition. arXiv preprint arXiv:1701.08835
    (2017).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Peng, X., and Wang, C. Building super-resolution image generator for ocr
    accuracy improvement. In International Workshop on Document Analysis Systems (2020),
    Springer, pp. 145–160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Pratikakis, I., Gatos, B., and Ntirogiannis, K. H-dibco 2010-handwritten
    document image binarization competition. In 2010 12th International Conference
    on Frontiers in Handwriting Recognition (2010), IEEE, pp. 727–732.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Pratikakis, I., Gatos, B., and Ntirogiannis, K. Icdar 2011 document image
    binarization contest (dibco 2011). In 2011 International Conference on Document
    Analysis and Recognition (2011), pp. 1506–1510.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Pratikakis, I., Gatos, B., and Ntirogiannis, K. Icfhr 2012 competition
    on handwritten document image binarization (h-dibco 2012). In 2012 international
    conference on frontiers in handwriting recognition (2012), IEEE, pp. 817–822.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Pratikakis, I., Gatos, B., and Ntirogiannis, K. Icdar 2013 document image
    binarization contest (dibco 2013). In 2013 12th International Conference on Document
    Analysis and Recognition (2013), IEEE, pp. 1471–1476.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Pratikakis, I., Zagori, K., Kaddas, P., and Gatos, B. Icfhr 2018 competition
    on handwritten document image binarization (h-dibco 2018). In 2018 16th International
    Conference on Frontiers in Handwriting Recognition (ICFHR) (2018), pp. 489–493.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Pratikakis, I., Zagoris, K., Barlas, G., and Gatos, B. Icfhr2016 handwritten
    document image binarization contest (h-dibco 2016). In 2016 15th International
    Conference on Frontiers in Handwriting Recognition (ICFHR) (2016), IEEE, pp. 619–623.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Pratikakis, I., Zagoris, K., Barlas, G., and Gatos, B. Icdar2017 competition
    on document image binarization (dibco 2017). In 2017 14th IAPR International Conference
    on Document Analysis and Recognition (ICDAR) (2017), vol. 1, IEEE, pp. 1395–1403.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Redmon, J., Divvala, S., Girshick, R., and Farhadi, A. You only look once:
    Unified, real-time object detection. In Proceedings of the IEEE conference on
    computer vision and pattern recognition (2016), pp. 779–788.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Ren, W., Liu, S., Ma, L., Xu, Q., Xu, X., Cao, X., Du, J., and Yang, M.-H.
    Low-light image enhancement via a deep hybrid network. IEEE Transactions on Image
    Processing 28, 9 (2019), 4364–4375.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Sauvola, J., and Pietikäinen, M. Adaptive document image binarization.
    Pattern recognition 33, 2 (2000), 225–236.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Schroff, F., Kalenichenko, D., and Philbin, J. Facenet: A unified embedding
    for face recognition and clustering. In Proceedings of the IEEE conference on
    computer vision and pattern recognition (2015), pp. 815–823.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Sharma, M., Verma, A., and Vig, L. Learning to clean: A gan perspective.
    In Asian Conference on Computer Vision (2018), Springer, pp. 174–185.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Souibgui, M. A., and Kessentini, Y. De-gan: A conditional generative adversarial
    network for document enhancement. IEEE Transactions on Pattern Analysis and Machine
    Intelligence (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Souibgui, M. A., Kessentini, Y., and Fornés, A. A conditional gan based
    approach for distorted camera captured documents recovery. Pattern Recognition
    and Artificial Intelligence 1322 (2021), 215.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Su, B., Lu, S., and Tan, C. L. Robust document image binarization technique
    for degraded document images. IEEE transactions on image processing 22, 4 (2012),
    1408–1417.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Tai, Y., Yang, J., and Liu, X. Image super-resolution via deep recursive
    residual network. In Proceedings of the IEEE conference on computer vision and
    pattern recognition (2017), pp. 3147–3155.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Tao, X., Gao, H., Liao, R., Wang, J., and Jia, J. Detail-revealing deep
    video super-resolution. In Proceedings of the IEEE International Conference on
    Computer Vision (2017), pp. 4472–4480.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Tensmeyer, C., and Martinez, T. Document image binarization with fully
    convolutional neural networks. In 2017 14th IAPR international conference on document
    analysis and recognition (ICDAR) (2017), vol. 1, IEEE, pp. 99–104.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Translation, S. M. Sixth workshop on statistical machine translation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Van der Zant, T., Schomaker, L., and Haak, K. Handwritten-word spotting
    using biologically inspired features. Ieee transactions on pattern analysis and
    machine intelligence 30, 11 (2008), 1945–1957.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Wang, R., Zhang, Q., Fu, C.-W., Shen, X., Zheng, W.-S., and Jia, J. Underexposed
    photo enhancement using deep illumination estimation. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition (2019), pp. 6849–6857.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Wang, X., Yu, F., Dunlap, L., Ma, Y.-A., Wang, R., Mirhoseini, A., Darrell,
    T., and Gonzalez, J. E. Deep mixture of experts via shallow embedding. In Uncertainty
    in Artificial Intelligence (2020), PMLR, pp. 552–562.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Wang, X., Yu, K., Wu, S., Gu, J., Liu, Y., Dong, C., Qiao, Y., and Change Loy,
    C. Esrgan: Enhanced super-resolution generative adversarial networks. In Proceedings
    of the European conference on computer vision (ECCV) workshops (2018), pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Wang, Z., Bovik, A. C., Sheikh, H. R., and Simoncelli, E. P. Image quality
    assessment: from error visibility to structural similarity. IEEE transactions
    on image processing 13, 4 (2004), 600–612.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Xu, X., Sun, D., Pan, J., Zhang, Y., Pfister, H., and Yang, M.-H. Learning
    to super-resolve blurry face and text images. In Proceedings of the IEEE international
    conference on computer vision (2017), pp. 251–260.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Zhao, G., Liu, J., Jiang, J., Guan, H., and Wen, J.-R. Skip-connected
    deep convolutional autoencoder for restoration of document images. In 2018 24th
    International Conference on Pattern Recognition (ICPR) (2018), IEEE, pp. 2935–2940.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
