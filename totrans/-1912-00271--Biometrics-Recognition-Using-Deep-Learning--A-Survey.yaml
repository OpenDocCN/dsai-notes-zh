- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:04:01'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1912.00271] Biometrics Recognition Using Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1912.00271](https://ar5iv.labs.arxiv.org/html/1912.00271)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: ∎
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹institutetext: Shervin Minaee ²²institutetext: Snapchat, Machine Learning
    R&D ³³institutetext: Amirali Abdolrashidi ⁴⁴institutetext: University of California,
    Riverside ⁵⁵institutetext: Hang Su ⁶⁶institutetext: Facebook Research ⁷⁷institutetext:
    Mohammed Bennamoun ⁸⁸institutetext: The University of Western Australia ⁹⁹institutetext:
    David Zhang ^(10)^(10)institutetext: Chinese University of Hong Kong'
  prefs: []
  type: TYPE_NORMAL
- en: 'Biometrics Recognition Using Deep Learning: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Shervin Minaee    Amirali Abdolrashidi    Hang Su    Mohammed Bennamoun    David
    Zhang
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning-based models have been very successful in achieving state-of-the-art
    results in many of the computer vision, speech recognition, and natural language
    processing tasks in the last few years. These models seem a natural fit for handling
    the ever-increasing scale of biometric recognition problems, from cellphone authentication
    to airport security systems. Deep learning-based models have increasingly been
    leveraged to improve the accuracy of different biometric recognition systems in
    recent years. In this work, we provide a comprehensive survey of more than 120
    promising works on biometric recognition (including face, fingerprint, iris, palmprint,
    ear, voice, signature, and gait recognition), which deploy deep learning models,
    and show their strengths and potentials in different applications. For each biometric,
    we first introduce the available datasets that are widely used in the literature
    and their characteristics. We will then talk about several promising deep learning
    works developed for that biometric, and show their performance on popular public
    benchmarks. We will also discuss some of the main challenges while using these
    models for biometric recognition, and possible future directions to which research
    in this area is headed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Biometric Recognition, Deep Learning, Face Recognition, Fingerprint Recognition,
    Iris Recognition, Palmprint Recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Biometric features¹¹1In this paper, we commonly refer to a biometric characteristic
    as biometric for short. hold a unique place when it comes to recognition, authentication,
    and security applications [jain2000biometric](#bib.bib1) , [zhang2013automated](#bib.bib2)
    . They cannot get lost, unlike token-based features such as keys and ID cards,
    and they cannot be forgotten, unlike knowledge-based features, such as passwords
    or answers to security questions [zhang2018advanced](#bib.bib3) . In addition,
    they are almost impossible to perfectly imitate or duplicate. Even though there
    have been recent attempts to generate and forge various biometric features [galbally2008fake](#bib.bib4)
    , [eskimez2018generating](#bib.bib5) , there have also been methods proposed to
    distinguish fake biometric features from authentic ones [deepfake](#bib.bib6)
    , [mo2018fake](#bib.bib7) , [li2018exposing](#bib.bib8) . Changes over time for
    many biometric features are also extremely little. For these reasons, they have
    been utilized in many applications, including cellphone authentication, airport
    security, and forensic science. Biometric features can be physiological, which
    are features possessed by any person, such as fingerprints [jain2004introduction](#bib.bib9)
    , palmprints [lu2003palmprint](#bib.bib10) , [zhang1999two](#bib.bib11) , facial
    features [face_Chellappa](#bib.bib12) , ears [mu2004shape](#bib.bib13) , irises
    [daugman2009iris](#bib.bib14) , [bowyer2016handbook](#bib.bib15) , and retinas
    [borgen_retina](#bib.bib16) , or behavioral, which are apparent in a person’s
    interaction with the environment, such as signatures [elhoseny2018hybrid](#bib.bib17)
    , gaits [gait_review](#bib.bib18) , and keystroke [monrose2000keystroke](#bib.bib19)
    . Voice/Speech contains both behavioral features, such as accent, and physiological
    features, such as voice pitch [speech_rec](#bib.bib20) .
  prefs: []
  type: TYPE_NORMAL
- en: 'Face and fingerprint are arguably the most commonly used physiological biometric
    feature. Fingerprint is the oldest, dating back to 1893 when it was used to convict
    a murder suspect in Argentina [hawthorne2017fingerprints](#bib.bib21) . Face has
    many discriminative features which can be used for recognition tasks [jain2011handbook](#bib.bib22)
    . However, its susceptibility to change due to factors such as expression or aging,
    may present a challenge [guo2016ei3d](#bib.bib23) , [park2010age](#bib.bib24)
    . Fingerprint consists of ridges and valleys, which form unique patterns. Minutiae
    are major local portions of the fingerprint which can be used to determine the
    uniqueness of the fingerprint, two of the most important ones of which being ridge
    endings and ridge bifurcations [jain1997line](#bib.bib25) . Palmprint is another
    alternative used for authentication purposes. In addition to minutiae features,
    palmprints also consist of geometry-based features, delta points, principal lines,
    and wrinkles [multispectral_palm](#bib.bib26) . Iris and retina are the two most
    popular biometrics that are present in the eye, and can be used for recognition
    through the texture of the iris or the pattern of blood vessels in the retina.
    One interesting point worth noting is that even the two eyes in the same person
    have different patterns [iris_survey_old](#bib.bib27) . Ears can also be used
    as a biometric through the shape of their lobes, and helix, and unlike most biometric
    features, do not need the person’s direct interaction. The right and left ears
    are symmetrical in a person in most cases. However, their sizes are subject to
    change over time [emervsivc2017ear](#bib.bib28) . Among the behavioral features,
    signatures are arguably the most widely used today. The strokes in the signature
    can be examined for the pressure of the pen throughout the signature as well as
    the speed, which is a factor in the thickness of the stroke [galbally2015line](#bib.bib29)
    . Gait refers to the manner of walking, which has been gaining more attention
    in the recent years. Due to the involvement of many joints and body parts in the
    process of walking, gait can also be used to uniquely identify a person from a
    distance [wang2004fusion](#bib.bib30) . Samples of various biometrics are shown
    in Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Biometrics Recognition Using
    Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9aac9a2366e11d48f6caf2ccfbdeaab6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Sample images for various biometrics. The images in the first to
    sixth rows denote samples from face, fingerprint, iris, palmprint, ear, and gait
    respectively [casia_gaitA](#bib.bib31) ; [yaleb](#bib.bib32) ; [polyU_finger](#bib.bib33)
    ; [polyu_palm](#bib.bib34) ; [kumar2010comparison](#bib.bib35) ; [iit_ear](#bib.bib36)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditionally, the biometric recognition process involved several key steps.
    Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Biometrics Recognition Using Deep
    Learning: A Survey") shows the block-diagram of traditional biometric recognition
    systems. Firstly, the image data are acquired via (various) camera or optical
    sensors, and are then pre-processed so as to make the algorithm work on as much
    useful data as possible. Then, features are extracted from each image. Classical
    biometric recognition works were mostly based on hand-crafted features (designed
    by computer vision experts) to work with a certain type of data [ahonen2004face](#bib.bib37)
    , [2d_3d_face](#bib.bib38) , [zhang2009advanced](#bib.bib39) . Many of the hand-crafted
    features were based on the distribution of edges (SIFT [lowe2004distinctive](#bib.bib40)
    , HOG [dalal2005histograms](#bib.bib41) ), or where derived from transform domain,
    such as Gabor [kong2003palmprint](#bib.bib42) , Fourier [lai2001face](#bib.bib43)
    , and wavelet [jin2004efficient](#bib.bib44) . Principal component analysis is
    also used in many works to reduce the dimensionality of the features [abdi2010principal](#bib.bib45)
    , [yang2004two](#bib.bib46) . Once the features are extracted, they are fed into
    a classifier to perform recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/630cc1dfd6e6432f5292266fa934c6d2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: The block-diagram of most of classical biometric recognition algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Many challenges arise in a traditional biometric recognition task. For example,
    the hand-crafted features that are suitable for one biometric, will not necessarily
    perform well on others. Therefore, it would take a great number of experiments
    to find and choose the most efficient set of hand-crafted features for a certain
    biometric. Also many of the classical models were based on multi-class SVM trained
    in an one-vs-one fashion, which will not scale well when the number of classes
    is large.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, a paradigm shift started to occur in 2012, when a deep learning-based
    model, AlexNet [alexnet](#bib.bib47) , won the ImageNet competition by a large
    margin. Since then, deep learning models have been applied to a wide range of
    problems in computer vision and Natural Language Processing (NLP), and achieved
    promising results. Not surprisingly, biometric recognition methods were not an
    exception, and were taken over by deep learning models (with a few years delay).
    Deep learning based models provide an end-to-end learning framework, which can
    jointly learn the feature representation while performing classification/regression.
    This is achieved through a multi-layer neural networks, also known as Deep Neural
    Networks (DNNs), to learn multiple levels of representations that correspond to
    different levels of abstraction, which is better suited to uncover underlying
    patterns of the data (as shown in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction
    ‣ Biometrics Recognition Using Deep Learning: A Survey")). The idea of a multi-layer
    neural network dates back to the 1960s [ivakhnenko1966cybernetic](#bib.bib48)
    ; [schmidhuber2015deep](#bib.bib49) . However, their feasible implementation was
    a challenge in itself, as the training time would be too large (due to lack of
    powerful computers at that time). The progresses made in processor technology,
    and especially the development of General-Purpose GPUs (GPGPUs), as well as development
    of new techniques (such as Dropout) for training neural networks with a lower
    chance of over-fitting, enabled scientists to train very deep neural networks
    much faster [lecun2015deep](#bib.bib50) . The main idea of a neural network is
    to pass the (raw) data through a series of interconnected neurons or nodes, each
    of which emulates a linear or non-linear function based on its own weights and
    biases. These weights and biases would change during the training through back-propagation
    of the gradients from the output [backprop](#bib.bib51) , usually resulted from
    the differences between the expected output and the actual current output, aimed
    to minimized a loss function or cost function (difference between the predicted
    and actual outputs according to some metric) [bottou1991stochastic](#bib.bib52)
    . We will talk about different deep architectures in more details in Section 2.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using deep models for biometric recognition, one can learn a hierarchy of concepts
    as we go deeper in the network. Looking at face recognition for example, as shown
    in Figure [3](#S1.F3 "Figure 3 ‣ 1 Introduction ‣ Biometrics Recognition Using
    Deep Learning: A Survey"), starting from the first few layers of the deep neural
    network, we can observe learned patterns similar to the Gabor feature (oriented
    edges with different scales). The next few layers can learn more complex texture
    features and part of the face. The following layers are able to catch more complex
    pattern, such as high-bridged nose and big eyes. Finally the last few layers can
    learn very abstract concepts and certain facial attribute (such as smile, roar,
    and even eye color faces).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/12a26b83f39076df1ce517e79418b6c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Illustration of the hierarchical concepts learned by a deep learning
    models trained for face recognition. Courtesy of [deep_face_survey](#bib.bib53)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we present a comprehensive review of the recent advances in biometric
    recognition using deep learning frameworks. For each work, we provide an overview
    of the the key contributions, network architecture, and loss functions, developed
    to push state-of-the-art performance in biometric recognition. We have gathered
    more than 150 papers, which appeared between 2014 and 2019, in leading computer
    vision, biometric recognition, and machine learning conferences and journals.
    For each biometric, we provide some of the most popular datasets used by the computer
    vision community, and the most promising state-of-the-art deep learning works
    utilized in the area of biometric recognition. We then provide a quantitative
    analysis of well-known models for each biometric. Finally, we explore the challenges
    associated with deep learning-based methods in biometric recognition and research
    opportunities for the future.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of this survey is to help new researchers in this field to navigate
    through the progress of deep learning-based biometric recognition models, particularly
    with the growing interest of multi-modal biometrics systems [multimodal_challenge](#bib.bib54)
    . Compared to the existing literature, the main contributions of this paper are
    as follow:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, this is the only review paper which provides an
    overview of eight popular biometrics proposed before and in 2019, including face,
    fingerprint, iris, palmprint, ear, voice, signature, and gait.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We cover the contemporary literature with respect to this area. We present a
    comprehensive review of more than 150 methods, which have appeared since 2014.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a comprehensive review and an insightful analysis of different aspects
    of biometric recognition using deep learning, including the training data, the
    choice of network architectures, training strategies, and their key contributions.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a comparative summary of the properties and performance of the reviewed
    methods for biometric recognition.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide seven challenges and potential future direction for deep learning-based
    biometric recognition models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The structure of the rest of this paper is as follows. In Section 2, we provide
    an overview of popular deep neural networks architectures, which serve as the
    backbone of many biometric recognition algorithms, including convolutional neural
    networks, recurrent neural networks, auto-encoders, and generative adversarial
    networks. Then in Section 3, we provide an introduction to each of the eight biometrics
    (Face, Fingerprint, Iris, Palmprint, Ear, Voice, Signature, and Gait), some of
    the popular datasets for each of them, as well as the promising deep learning
    based works developed for them. The quantitative results and experimental performance
    of these models for all biometrics are provided in Section 4\. Finally in Section
    5, we explore the challenges and future directions for deep learning-based biometric
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Deep Neural Network Overview
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we provide an overview of some of the most promising deep learning
    architectures used by the computer vision community, including convolutional neural
    networks (CNN) [CNN](#bib.bib55) , recurrent neural networks (RNN) and one of
    their specific version called long short term memory (LSTM) [lstm](#bib.bib56)
    , auto-encoders, and generative adversarial networks (GANs) [gan](#bib.bib57)
    . It is noteworthy that with the popularity of deep learning in recent years,
    there are several other deep neural architectures proposed (such as Transformers,
    Capsule Network, GRU, and spatial transformer networks), which we will not cover
    in this work.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Convolutional Neural Networks (CNN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Convolutional Neural Networks (CNN) (inspired by the mammalian visual cortex)
    are one of the most successful and widely used architectures in deep learning
    community (specially for computer vision tasks). CNN was initially proposed by
    Fukushima in a seminal paper, called ”Neocognitron” [neocog](#bib.bib58) , based
    on the model of human visual system proposed by Nobel laureates Hubel and Wiesel.
    Later on Yann Lecun and colleagues developed an optimization framework (based
    on back-propagation) to efficiently learn the model weights for a CNN architecture
    [CNN](#bib.bib55) . The block-diagram of one of the first CNN models developed
    by Lecun et al. is shown in Figure [4](#S2.F4 "Figure 4 ‣ 2.1 Convolutional Neural
    Networks (CNN) ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition Using
    Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs mainly consist of three type of layers: convolutional layers, where a
    sliding kernel is applied to the image (as in image convolution operation) in
    order to extract features; nonlinear layers (usually applied in an element-wise
    fashion), which apply an activation function on the features in order to enable
    the modeling of non-linear functions by the network; and pooling layers, which
    takes a small neighborhood of the feature map and replaces it with some statistical
    information (mean, max, etc.) of the neighborhood. Nodes in the CNN layers are
    locally connected; that is, each unit in a layer receives input from a small neighborhood
    of the previous layer (known as the receptive field). The main advantage of CNN
    is the weight sharing mechanism through the use of the sliding kernel, which goes
    through the images, and aggregates the local information to extract the features.
    Since the kernel weights are shared across the entire image, CNNs have a significantly
    smaller number of parameters than a similar fully connected neural network. Also
    by stacking multiple convolution layers, the higher-level layers learn features
    from increasingly wider receptive fields.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/125bacd1df96012853184de2e7a77b04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Architecture of a Convolutional Neural Network (CNN), showing the
    main two operations of convolution and pooling. Courtesy of Yann LeCun.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CNNs have been applied to various computer vision tasks such as: semantic segmentation
    [seman_seg](#bib.bib59) , medical image segmentation [med_seg](#bib.bib60) , object
    detection [faster_rcnn](#bib.bib61) , super-resolution [sisr](#bib.bib62) , image
    enhancement [enhance](#bib.bib63) , caption generation for image and videos [caption](#bib.bib64)
    , and many more. Some of the most well-known CNN architectures include AlexNet
    [alexnet](#bib.bib47) , ZFNet [zfnet](#bib.bib65) , VGGNet [vggnet](#bib.bib66)
    , ResNet [resnet](#bib.bib67) , GoogLenet [googlenet](#bib.bib68) , MobileNet
    [mobilenet](#bib.bib69) , and DenseNet [densenet](#bib.bib70) .'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Recurrent Neural Networks and LSTM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Recurrent Neural Networks (RNNs) [RNN](#bib.bib71) are widely used for processing
    sequential data like speech, text, video, and time-series (such as stock prices),
    where data at any given time/position depends on the previously encountered data.
    A high-level architecture of a simple RNN is shown in Figure [5](#S2.F5 "Figure
    5 ‣ 2.2 Recurrent Neural Networks and LSTM ‣ 2 Deep Neural Network Overview ‣
    Biometrics Recognition Using Deep Learning: A Survey"). As we can see at each
    time-stamp, the model gets the input from the current time $X_{i}$ and the hidden
    state from the previous step $h_{i-1}$ and outputs the hidden state (and possibly
    an output value). The hidden state from the very last time-stamp (or a weighted
    average of all hidden states) can then be used to perform a task.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2444bbe48c1611815e150bbd586e7d85.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Architecture of a Recurrent Neural Network (RNN).'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs usually suffer when dealing with long sequences, as they cannot capture
    the long-term dependencies of many real application (although in theory there
    is nothing limiting them from doing so). However, there is a variation of RNNs,
    called LSTM, which is designed to better capture long-term dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Long Short Term Memory (LSTM): LSTM is a popular recurrent neural network architecture
    for modeling sequential data, which is designed to have a better ability to capture
    long term dependencies than the vanilla RNN model [lstm](#bib.bib56) . As mentioned
    above, the vanilla RNN often suffers from the gradient vanishing or exploding
    problems, and LSTM network tries to overcome this issue by introducing some internal
    gates. In the LSTM architecture, there are three gates (input gate, output gate,
    forget gate) and a memory cell. The cell remembers values over arbitrary time
    intervals and the other three gates regulate the flow of information into and
    out of the cell. Figure [6](#S2.F6 "Figure 6 ‣ 2.2 Recurrent Neural Networks and
    LSTM ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition Using Deep Learning:
    A Survey") illustrates the inner architecture of a single LSTM module.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cc19c8962b628e4565c8da701e33f195.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: The architecture of a standard LSTM module, courtesy of Andrej Karpathy
    [lstm_cell](#bib.bib72) .'
  prefs: []
  type: TYPE_NORMAL
- en: 'The relationship between input, hidden states, and different gates is shown
    in Equation [1](#S2.E1 "In 2.2 Recurrent Neural Networks and LSTM ‣ 2 Deep Neural
    Network Overview ‣ Biometrics Recognition Using Deep Learning: A Survey"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle f_{t}$ | $\displaystyle=\sigma(\textbf{W}^{(f)}x_{t}+\textbf{U}^{(f)}h_{t-1}+b^{(f)})$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle i_{t}$ | $\displaystyle=\sigma(\textbf{W}^{(i)}x_{t}+\textbf{U}^{(i)}h_{t-1}+b^{(i)})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle o_{t}$ | $\displaystyle=\sigma(\textbf{W}^{(o)}x_{t}+\textbf{U}^{(o)}h_{t-1}+b^{(o)})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle c_{t}$ | $\displaystyle=f_{t}\odot c_{t-1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+i_{t}\odot\text{tanh}(\textbf{W}^{(c)}x_{t}+\textbf{U}^{(c)}h_{t-1}+b^{(c)})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle h_{t}$ | $\displaystyle=o_{t}\odot\text{tanh}(c_{t})$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $x_{t}\in R^{d}$ is the input at time-step t, and $d$ denotes the feature
    dimension for each word, $\sigma$ denotes the element-wise sigmoid function (to
    squash/map the values within $[0,1]$), $\odot$ denotes the element-wise product.
    $c_{t}$ denotes the memory cell designed to lower the risk of vanishing/exploding
    gradient, and therefore enabling the learning of dependencies over larger periods
    of time, which is infeasible with traditional recurrent networks. The forget gate,
    $f_{t}$ is to reset the memory cell. $i_{t}$ and $o_{t}$ denote the input and
    output gates, and essentially control the input and output of the memory cell.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Auto-Encoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Auto-encoders are a family of neural network models used to learn efficient
    data encoding in an unsupervised manner. They achieve this by compressing the
    input data into a latent-space representation, and then reconstructing the output
    (which is usually the same as the input) from this representation. Auto-encoders
    are composed of two parts: Encoder: This is the part of the network that compresses
    the input into a latent-space representation. It can be represented by an encoding
    function $z=f(x)$. Decoder: This part aims to reconstruct the input from the latent
    space representation. It can be represented by a decoding function $y=g(z)$.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of a simple auto-encoder model is demonstrated in Figure [7](#S2.F7
    "Figure 7 ‣ 2.3 Auto-Encoders ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition
    Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c0b5336fb93d2eed2585347326937af1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Architecture of a standard Auto-Encoder Model.'
  prefs: []
  type: TYPE_NORMAL
- en: Auto-encoders are usually trained by minimizing the reconstruction error, $L(x,\hat{x})$
    (unsupervised, i.e. no need for labeled data), which measures the differences
    between our original input $x$, and the consequent reconstruction $\hat{x}$. Mean
    square error, and mean absolute deviation are popular choices for the reconstruction
    loss in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: There are several variations of auto-encoders where were proposed in the past.
    One of the popular ones is the stacked denoising auto-encoder (SDAE) which stacks
    several auto-encoders and uses them for image denoising [stac_ae](#bib.bib73)
    . Another popular variation of autoencoders is ”variational auto-encoder (VAE)”
    which imposes a prior distribution on the latent representation [vr_ae](#bib.bib74)
    . Variational auto-encoders are able to generate realistic samples from a data
    distribution. Another variation of auto-encoders is the adversarial auto-encoders,
    which introduces an adversarial loss on the latent representation to encourage
    them to be close to a prior distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Generative Adversarial Networks (GAN)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Generative Adversarial Networks (GANs) are a newer family of deep learning
    models, which consists of two networks, one generator, and one discriminator [gan](#bib.bib57)
    . On a high level, the generator’s job is to generate samples from a distribution
    which are close enough to real samples with the objective to fool the discriminator,
    while the discriminator’s job is to distinguish the generated samples (fakes)
    from the authentic ones. The general architecture of a vanilla GAN model is demonstrated
    in Figure [8](#S2.F8 "Figure 8 ‣ 2.4 Generative Adversarial Networks (GAN) ‣ 2
    Deep Neural Network Overview ‣ Biometrics Recognition Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/acc8370fd09080415685544b15e82d96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: The architecture of generative adversarial network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The generator network in vanilla GAN learns a mapping from noise $z$ (with
    a prior distribution, such as Gaussian) to a target distribution $y$, $G=z\rightarrow
    y$, which look similar to the real samples, while the discriminator network, $D$,
    tries to distinguish the samples generated by the generator models from the real
    ones. The loss function of GAN can be written as Equation [2](#S2.E2 "In 2.4 Generative
    Adversarial Networks (GAN) ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition
    Using Deep Learning: A Survey"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathcal{L}_{GAN}$ | $\displaystyle=\mathbb{E}_{x\sim p_{data}(x)}[\text{log}D(x)]$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+\mathbb{E}_{z\sim p_{z}(z)}[\text{log}(1-D(G(z)))]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'in which we can think of GAN, as a minimax game between $D$ and $G$, where
    $D$ is trying to minimize its classification error in detecting fake samples from
    the real ones (maximize the above loss function), and $G$ is trying to maximize
    the discriminator network’s error (minimize the above loss function). After training
    this model, the trained generator model would be:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle G^{*}=\text{arg}\ \text{min}_{G}\text{max}_{D}\ \mathcal{L}_{GAN}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'In practice, the loss function in Equation [3](#S2.E3 "In 2.4 Generative Adversarial
    Networks (GAN) ‣ 2 Deep Neural Network Overview ‣ Biometrics Recognition Using
    Deep Learning: A Survey") may not provide enough gradient for $G$ to get trained
    well, specially at the beginning where $D$ can easily detect fake samples from
    the real ones. One solution is to maximize $\mathbb{E}_{z\sim p_{z}(z)}[\text{log}(D(G(z)))]$.'
  prefs: []
  type: TYPE_NORMAL
- en: Since the invention of GAN, there have been several works trying to improve/modify
    GAN in different aspects. For a detailed list of works relevant to GAN, please
    refer to [GanZoo](#bib.bib75) .
  prefs: []
  type: TYPE_NORMAL
- en: 2.5 Transfer Learning Approach
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now that we talked about some of the popular deep learning architectures, let
    us briefly talk about how these models are applied to new applications. Of course,
    these models can always be trained from scratch on new applications, assuming
    they are provided with sufficient labeled data. But depending on the depth of
    the model (i.e. how large if the number of parameters), it may not be very straightforward
    to make the model converge to a good local minimum. Also, for many applications,
    there may not be enough labeled data available to train a deep model from scratch.
    For these situations, transfer learning approach can be used to better handle
    labeled data limitations and the local-minimum problem.
  prefs: []
  type: TYPE_NORMAL
- en: In transfer learning, a model trained on one task is re-purposed on another
    related task, usually by some adaptation toward the new task. For example, one
    can imagine using an image classification model trained on ImageNet, to be used
    for a different task such as texture classification, or iris recognition. There
    are two main ways in which the pre-trained model is used for a different task.
    In one approach, the pre-trained model, e.g. a language model, is treated as a
    feature extractor, and a classifier is trained on top of it to perform classification
    (e.g. sentiment classification). Here the internal weights of the pre-trained
    model are not adapted to the new task. In the other approach, the whole network,
    or a subset of it, is fine-tuned on the new task. Therefore the pre-trained model
    weights are treated as the initial values for the new task, and are updated during
    the training stage.
  prefs: []
  type: TYPE_NORMAL
- en: Many of the deep learning-based models for biometric recognition are based on
    transfer learning (except for voice because of the difference in the nature of
    the data, and face because of the availability of large-scale datasets), which
    we are going to explain in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Deep Learning Based Works on Biometric Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we provide an overview of some of the most promising deep learning
    works for various biometric recognition works. Within each subsection, we also
    provide a summary of some of the most popular datasets for each biometric.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Face Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Face is perhaps one of the most popular biometrics (and the most researched
    one during the last few years). It has a wide range of applications, from security
    cameras in airports and government offices, to daily usage for cellphone authentication
    (such as in FaceID in iPhones). Various hand-crafted features were used for recognition
    in the past, such as the LBP, Gabor Wavelet, SIFT, HoG, and also sparsity-based
    representations [deng2013defense](#bib.bib76) , [cao2013similarity](#bib.bib77)
    , [face_sparse](#bib.bib78) , [yang2012regularized](#bib.bib79) , [yi2013towards](#bib.bib80)
    . Both 2D and 3D versions of faces are used for recognition [mian2007efficient](#bib.bib81)
    , but most people have focused on 2D face recognition so far. One of the main
    challenges for facial recognition is the face’s susceptibility to change over
    time due to aging or external factors, such as scars, or medical conditions [park2010age](#bib.bib24)
    . We will introduce some of the most widely used face recognition datasets in
    the next section, and then talk about the promising deep learning-based face recognition
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1 Face Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Due to the wide application of face recognition in the industry, a large number
    of datasets are proposed for that purpose. We will introduce some of the most
    popular ones here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Yale and Yale Face Database B: Yale face dataset is perhaps one of the earliest
    face recognition datasets [yale](#bib.bib82) . It Contains 165 grayscale images
    of 15 individuals. There are 11 images per subject, one per different facial expression
    or configuration (center-light, w/glasses, happy, left-light, w/no glasses, normal,
    right-light, sad, sleepy, surprised, and wink).'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is extended version, Yale Face Database B [yaleb](#bib.bib32) , contains
    5760 single light source images of 10 subjects each seen under 576 viewing conditions
    (9 poses x 64 illumination conditions). For every subject in a particular pose,
    an image with ambient (background) illumination was also captured. Ten example
    images from Yale face B dataset are shown in Figure [9](#S3.F9 "Figure 9 ‣ 3.1.1
    Face Datasets ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric
    Recognition ‣ Biometrics Recognition Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ea1d6bf59370fa442524649b792965ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Ten example images from Yale Face B Dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CMU Multi-PIE: The CMU Multi-PIE face database contains more than 750,000 images
    of 337 people [cmu](#bib.bib83) , [gross2010multi](#bib.bib84) . Subjects were
    imaged under 15 view points and 19 illumination conditions while displaying a
    range of facial expressions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Labeled Face in The Wild (LFW): Labeled Faces in the Wild is a database of
    face images designed for studying unconstrained face recognition. The database
    contains more than 13,000 images of faces collected from the web. Each face has
    been labeled with the name of the person pictured [lfw](#bib.bib85) . 1680 of
    the people pictured have two or more distinct photos in the database. The only
    constraint on these faces is that they were detected by the Viola-Jones face detector.
    For more details on this dataset, we refer the readers to the database web-page.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PolyU NIR Face Database: The Biometric Research Centre at The Hong Kong Polytechnic
    University developed a NIR face capture device and used it to construct a large-scale
    NIR face database [polyu_face](#bib.bib86) . By using the self-designed data acquisition
    device, they collected NIR face images from 335 subjects. In each recording, 100
    images from each subject is captured, and in total about 34,000 images were collected
    in the PolyU-NIRFD database.'
  prefs: []
  type: TYPE_NORMAL
- en: 'YouTube Faces: This data set contains 3,425 videos of 1,595 different people.
    All videos were downloaded from YouTube. An average of 2.15 videos are available
    for each subject. The goal of this dataset was to produce a large scale collection
    of videos along with labels indicating the identities of a person appearing in
    each video [youtube](#bib.bib87) . In addition, they published benchmark tests,
    intended to measure the performance of video pair-matching techniques on these
    videos.'
  prefs: []
  type: TYPE_NORMAL
- en: 'VGGFace2: VGGFace2 is a large-scale face recognition dataset [vggface2](#bib.bib88)
    . Images are downloaded from Google Image Search and have a large variations in
    pose, age, illumination, ethnicity and profession. It contains 3.31 million images
    of 9131 subjects (identities), with an average of 362.6 images for each subject.
    Face distribution for different identities is varied, from 87 to 843.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CASIA-WebFace: CASIA WebFace Facial dataset of 453,453 images over 10,575 identities
    after face detection [casiawebface](#bib.bib89) . This is one of the largest publicly
    available face datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MS-Celeb: Microsoft Celeb is a dataset of 10 million face images harvested
    from the Internet for the purpose of developing face recognition technologies,
    from nearly 100,000 individuals [msceleb1m](#bib.bib90) .'
  prefs: []
  type: TYPE_NORMAL
- en: 'CelebA: CelebFaces Attributes Dataset (CelebA) is a large-scale face attributes
    dataset with more than 200K celebrity images [celeba](#bib.bib91) . CelebA has
    a large diversity, large quantities, and rich annotations, including more than
    10,000 identities, more than 202,599 face images, 5 landmark locations, 40 binary
    attributes annotations per image. The dataset can be employed as the training
    and test sets for the following computer vision tasks: face attribute recognition,
    face detection, landmark (or facial part) localization, and face editing & synthesis.'
  prefs: []
  type: TYPE_NORMAL
- en: 'IJB-C: The IJB-C dataset [ijbc](#bib.bib92) contains about 3,500 identities
    with a total of 31,334 still facial images and 117,542 unconstrained video frames.
    The entire IJB-C testing protocols are designed to test detection, identification,
    verification and clustering of faces. In the 1:1 verification protocol, there
    are 19,557 positive matches and 15,638,932 negative matches.'
  prefs: []
  type: TYPE_NORMAL
- en: 'MegaFace: MegaFace Challenge [megaface](#bib.bib93) is a publicly available
    benchmark, which is widely used to test the performance of facial recognition
    algorithms (for both identification and verification). The gallery set of MegaFace
    contains over 1 million images from 690K identities collected from Flickr [flicker](#bib.bib94)
    . The probe sets are two existing databases: FaceScrub and FGNet. The FaceScrub
    dataset contains 106,863 face images of 530 celebrities. The FGNet dataset is
    mainly used for testing age invariant face recognition, with 1002 face images
    from 82 persons.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Other Datasets: It is worth mentioning that there are several other datasets
    which we skipped the details due to being private or less popularity, such as
    DeepFace (Facebook private dataset of 4.4M photos of 4k subjects), NTechLab (a
    private dataset of 18.4M photos of 200k subjects), FaceNet (Google private dataset
    of more than 500M photos of more than 10M subjects), WebFaces (a dataset of 80M
    photos crawled from web) [megaface](#bib.bib93) , and Disguised Faces in the Wild
    (DFW) [DFW](#bib.bib95) which contains over 11,000 images of 1,000 identities
    with variations across different types of disguise accessories.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2 Deep Learning Works on Face Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are countless number of works using deep learning for face recognition.
    In this survey, we provide an overview of some of the most promising works developed
    for face verification and/or identification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2014, Taigman and colleagues proposed one of the earliest deep learning
    work for face recognition in a paper called DeepFace [DeepFace](#bib.bib96) ,
    and achieved the state-of-the-art accuracy on the LFW benchmark [lfw](#bib.bib85)
    , approaching human performance on the unconstrained condition for the first time
    ever (DeepFace: 97.35% vs. Human: 97.53%). DeepFace was trained on 4 million facial
    images. This work was a milestone on face recognition, and after that several
    researchers started using deep learning for face recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: In another promising work in the same year, Sun et al. proposed DeepID (Deep
    hidden IDentity features) [DeepID](#bib.bib97) , for face verification. DeepID
    features were taken from the last hidden layer of a deep convolutional network,
    which is trained to recognize about 10,000 face identities in the training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a follow up work, Sun et al. extended DeepID for joint face identification
    and verification called DeepID2 [DeepID2](#bib.bib98) . By training the model
    for joint identification and verification, they showed that the face identification
    task increases the inter-personal variations by drawing DeepID2 features extracted
    from different identities apart, while the face verification task reduces the
    intra-personal variations by pulling DeepID2 features extracted from the same
    identity together. For identification, cross-entropy is used as the loss function
    (as defined in the Equation [4](#S3.E4 "In 3.1.2 Deep Learning Works on Face Recognition
    ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric Recognition
    ‣ Biometrics Recognition Using Deep Learning: A Survey")), while for verification
    they proposed to use the loss function of Equation [5](#S3.E5 "In 3.1.2 Deep Learning
    Works on Face Recognition ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works
    on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning: A Survey")
    to reduce the intra-class distances on the features and increase the inter-class
    distances.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{L}_{Ident}(f,t,\theta_{id})=-\sum_{i}p_{i}\log\hat{p}_{i}$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | <math   alttext="\begin{split}\mathbf{L}_{Verif}&amp;(f_{i},f_{j},y_{ij},\theta_{vr})=\\
    &amp;\begin{cases}\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},&amp;\text{if }y_{ij}=1\\'
  prefs: []
  type: TYPE_NORMAL
- en: \frac{1}{2}max(1-\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},0),&amp;\text{otherwise}\end{cases}\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt"  ><mtr 
    ><mtd  columnalign="right"
     ><msub 
    ><mi  >𝐋</mi><mrow
     ><mi 
    >V</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi  >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >r</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >i</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi
     >f</mi></mrow></msub></mtd><mtd
     columnalign="left"  ><mrow
     ><mrow 
    ><mo stretchy="false"  >(</mo><msub
     ><mi 
    >f</mi><mi  >i</mi></msub><mo
     >,</mo><msub 
    ><mi  >f</mi><mi
     >j</mi></msub><mo
     >,</mo><msub 
    ><mi  >y</mi><mrow
     ><mi 
    >i</mi><mo lspace="0em" rspace="0em" 
    >​</mo><mi 
    >j</mi></mrow></msub><mo 
    >,</mo><msub 
    ><mi  >θ</mi><mrow
     ><mi
     >v</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >r</mi></mrow></msub><mo
    stretchy="false"  >)</mo></mrow><mo
     >=</mo></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow 
    ><mo  >{</mo><mtable
    columnspacing="5pt" displaystyle="true" rowspacing="0pt" 
    ><mtr  ><mtd
     columnalign="left"  ><mrow
     ><mrow
     ><mstyle
    displaystyle="false"  ><mfrac
     ><mn
     >1</mn><mn
     >2</mn></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><msubsup
     ><mrow
     ><mo
    stretchy="false"  >‖</mo><mrow
     ><msub
     ><mi
     >f</mi><mi
     >i</mi></msub><mo
     >−</mo><msub
     ><mi
     >f</mi><mi
     >j</mi></msub></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn
     >2</mn><mn
     >2</mn></msubsup></mrow><mo
     >,</mo></mrow></mtd><mtd
     columnalign="left"  ><mrow
     ><mrow
     ><mtext
     >if </mtext><mo
    lspace="0em" rspace="0em"  >​</mo><msub
     ><mi
     >y</mi><mrow
     ><mi
     >i</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >j</mi></mrow></msub></mrow><mo
     >=</mo><mn
     >1</mn></mrow></mtd></mtr><mtr
     ><mtd 
    columnalign="left"  ><mrow
     ><mrow
     ><mstyle
    displaystyle="false"  ><mfrac
     ><mn
     >1</mn><mn
     >2</mn></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >m</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >a</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi
     >x</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mrow
     ><mo
    stretchy="false"  >(</mo><mrow
     ><mn
     >1</mn><mo
     >−</mo><mrow
     ><mstyle
    displaystyle="false"  ><mfrac
     ><mn
     >1</mn><mn
     >2</mn></mfrac></mstyle><mo
    lspace="0em" rspace="0em"  >​</mo><msubsup
     ><mrow
     ><mo
    stretchy="false"  >‖</mo><mrow
     ><msub
     ><mi
     >f</mi><mi
     >i</mi></msub><mo
     >−</mo><msub
     ><mi
     >f</mi><mi
     >j</mi></msub></mrow><mo
    stretchy="false"  >‖</mo></mrow><mn
     >2</mn><mn
     >2</mn></msubsup></mrow></mrow><mo
     >,</mo><mn
     >0</mn><mo
    stretchy="false"  >)</mo></mrow></mrow><mo
     >,</mo></mrow></mtd><mtd
     columnalign="left"  ><mtext
     >otherwise</mtext></mtd></mtr></mtable></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply 
    ><apply  ><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝐋</ci><apply 
    ><ci  >𝑉</ci><ci
     >𝑒</ci><ci 
    >𝑟</ci><ci  >𝑖</ci><ci
     >𝑓</ci></apply></apply><vector
     ><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑓</ci><ci  >𝑖</ci></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝑓</ci><ci 
    >𝑗</ci></apply><apply 
    ><csymbol cd="ambiguous" 
    >subscript</csymbol><ci 
    >𝑦</ci><apply 
    ><ci  >𝑖</ci><ci
     >𝑗</ci></apply></apply><apply
     ><csymbol cd="ambiguous"
     >subscript</csymbol><ci
     >𝜃</ci><apply
     ><ci
     >𝑣</ci><ci
     >𝑟</ci></apply></apply></vector></apply><apply
     ><csymbol cd="latexml"
     >cases</csymbol><apply
     ><apply
     ><cn
    type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑓</ci><ci
     >𝑖</ci></apply><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑓</ci><ci
     >𝑗</ci></apply></apply></apply><cn
    type="integer"  >2</cn></apply><cn
    type="integer"  >2</cn></apply></apply><apply
     ><apply
     ><ci
     ><mtext
     >if </mtext></ci><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci
     >𝑦</ci><apply
     ><ci
     >𝑖</ci><ci
     >𝑗</ci></apply></apply></apply><cn
    type="integer"  >1</cn></apply><apply
     ><apply
     ><cn
    type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><ci
     >𝑚</ci><ci
     >𝑎</ci><ci
     >𝑥</ci><interval
    closure="open"  ><apply
     ><cn
    type="integer"  >1</cn><apply
     ><apply
     ><cn
    type="integer"  >1</cn><cn
    type="integer"  >2</cn></apply><apply
     ><csymbol
    cd="ambiguous"  >superscript</csymbol><apply
     ><csymbol
    cd="ambiguous"  >subscript</csymbol><apply
     ><csymbol
    cd="latexml"  >norm</csymbol><apply
     ><apply
     ><csymbol
    cd="ambiguous" 
    >subscript</csymbol><ci
     >𝑓</ci><ci
     >𝑖</ci></apply><apply
     ><csymbol
    cd="ambiguous" 
    >subscript</csymbol><ci
     >𝑓</ci><ci
     >𝑗</ci></apply></apply></apply><cn
    type="integer"  >2</cn></apply><cn
    type="integer"  >2</cn></apply></apply></apply><cn
    type="integer"  >0</cn></interval></apply><ci
     ><mtext
     >otherwise</mtext></ci></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathbf{L}_{Verif}&(f_{i},f_{j},y_{ij},\theta_{vr})=\\
    &\begin{cases}\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},&\text{if }y_{ij}=1\\
    \frac{1}{2}max(1-\frac{1}{2}\&#124;f_{i}-f_{j}\&#124;_{2}^{2},0),&\text{otherwise}\end{cases}\end{split}</annotation></semantics></math>
    |  | (5) |
  prefs: []
  type: TYPE_NORMAL
- en: As an extension of DeepID2, in DeepID3 [DeepID3](#bib.bib99) Sun et al proposed
    a new model which has higher dimensional hidden representation, and deploys VGGNet
    and GoogleNet as the main architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'In 2015, FaceNet [FaceNet](#bib.bib100) trained a GoogLeNet model on a large
    private dataset. This work tried to learn a mapping from face images to a compact
    Euclidean space where distances directly corresponds to a measure of face similarity.
    It adopted a triplet loss function based on triplets of roughly aligned matching/non-matching
    face patches generated by a novel online triplet mining method and achieved good
    performance on LFW dataset (99.63%). Given features for a given sample $f(x_{i}^{a})$,
    a positive sample $f(x_{i}^{p})$ (matching $x_{i}^{a}$), and a negative sample
    $f(x_{i}^{n})$, the triplet loss for a given margin $\alpha$ is defined as Equation
    [6](#S3.E6 "In 3.1.2 Deep Learning Works on Face Recognition ‣ 3.1 Face Recognition
    ‣ 3 Deep Learning Based Works on Biometric Recognition ‣ Biometrics Recognition
    Using Deep Learning: A Survey"):'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{L}_{triplet}=\sum\bigg{[}\&#124;f(x_{i}^{a})-f(x_{i}^{p})\&#124;_{2}^{2}-\&#124;f(x_{i}^{a})-f(x_{i}^{n})\&#124;_{2}^{2}+\alpha\bigg{]}_{+}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: In the same year, Parkhi et al. proposed a model called VGGface [VGGface_model](#bib.bib101)
    (trained on a large-scale dataset collected from the Internet). It trained the
    VGGNet on this dataset and fine-tuned the networks via a triplet loss function,
    Similar to FaceNet. VGGface obtained a very high accuracy rate of 98.95%.
  prefs: []
  type: TYPE_NORMAL
- en: In 2016, Liu and colleagues developed a ”Large-Margin Softmax Loss” for CNNs
    [Largesoftmax](#bib.bib102) , and showed its promise on multiple computer vision
    datasets, including LFW. They claimed that, cross-entropy does not explicitly
    encourage discriminative learning of features, and proposed a generalized large-margin
    softmax loss, which explicitly encourages intra-class compactness and inter-class
    separability between learned features.
  prefs: []
  type: TYPE_NORMAL
- en: In the same year, Wen et al. proposed a new supervision signal, called ”center
    loss”, for face recognition task [wen2016discriminative](#bib.bib103) . The center
    loss simultaneously learns a center for deep features of each class and penalizes
    the distances between the deep features and their corresponding class centers.
    With the joint supervision of softmax loss and center loss, they trained a CNN
    to obtain the deep features with the two key learning objectives, inter-class
    dispension and intra-class compactness as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: In another work in 2016, Sun et al. proposed a face recognition model using
    a convolutional network with sparse neural connections [sun2016sparsifying](#bib.bib104)
    . This sparse ConvNet is learned in an iterative fashion, where each time one
    additional layer is sparsified and the entire model is re-trained given the initial
    weights learned in previous iterations (they found out training the sparse ConvNet
    from scratch usually fails to find good solutions for face recognition).
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, in [RangeLoss](#bib.bib105) , Zhang and colleagues developed a range
    loss to reduce the overall intra-personal variations while increasing inter-personal
    differences simultaneously. In the same year, Ranjan and colleagues developed
    an ”L2-constraint softmax loss function” and used it for face verification [L2softmax](#bib.bib106)
    . This loss function restricts the feature descriptors to lie on a hyper-sphere
    of a fixed radius. This work achieved state-of-the-art performance on LFW dataset
    with an accuracy of 99.78% at the time. In [cocoloss](#bib.bib107) , Liu and colleagues
    developed a face recognition model based on the intuition that the cosine distance
    of face features in high-dimensional space should be close enough within one class
    and far away across categories. They proposed the congenerous cosine (COCO) algorithm
    to simultaneously optimize the cosine similarity among data.
  prefs: []
  type: TYPE_NORMAL
- en: In the same year, Liu et al. developed SphereFace [Sphereface](#bib.bib108)
    , a deep hypersphere embedding for face recognition. They proposed an angular
    softmax (A-Softmax) loss function that enables CNNs to learn angular discriminative
    features. Geometrically, A-Softmax loss can be viewed as imposing discriminative
    constraints on a hypersphere manifold, which intrinsically matches the prior that
    faces also lie on a manifold. They showed promising face recognition accuracy
    on LFW, MegaFace, and Youtube Face databases.
  prefs: []
  type: TYPE_NORMAL
- en: In 2018, in [amsloss](#bib.bib109) Wang et al. developed a simple and geometrically
    interpretable objective function, called additive margin Softmax (AM-Softmax),
    for deep face verification. This work is heavily inspired by two previous works,
    Large-margin Softmax [Largesoftmax](#bib.bib102) , and Angular Softmax in [Sphereface](#bib.bib108)
    .
  prefs: []
  type: TYPE_NORMAL
- en: CosFace [CosFace](#bib.bib110) and ArcFace [Arcface](#bib.bib111) are two other
    promising face recognition works developed in 2018. In [CosFace](#bib.bib110)
    , Wang et al. proposed a novel loss function, namely large margin cosine loss
    (LM-CL). More specifically, they reformulate the softmax loss as a cosine loss
    by L2 normalizing both features and weight vectors to remove radial variations,
    based on which a cosine margin term is introduced to further maximize the decision
    margin in the angular space. As a result, minimum intra-class variance and maximum
    inter-class variance are achieved by virtue of normalization and cosine decision
    margin maximization.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ring-Loss [RingLoss](#bib.bib112) is another work focused on designing a new
    loss function, which applies soft normalization, where it gradually learns to
    constrain the norm to the scaled unit circle while preserving convexity leading
    to more robust features. The comparison of learned features by regular softmax
    and the Ring-loss function is shown in Figure [10](#S3.F10 "Figure 10 ‣ 3.1.2
    Deep Learning Works on Face Recognition ‣ 3.1 Face Recognition ‣ 3 Deep Learning
    Based Works on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3203cb722b1e7df3cf4fa17e0d23905e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Sample MNIST features trained using (a) Softmax and (b) Ring loss
    on top of Softmax. Courtesy of [RingLoss](#bib.bib112) .'
  prefs: []
  type: TYPE_NORMAL
- en: AdaCos [adacos](#bib.bib113) , P2SGrad [p2sgrad](#bib.bib114) , UniformFace
    [uniface](#bib.bib115) , and AdaptiveFace [adapface](#bib.bib116) are among the
    most promising works proposed in 2019. In AdaCos [adacos](#bib.bib113) , Zhang
    et al. proposed a novel cosine-based softmax loss, AdaCos, which is hyperparameter-free
    and leverages an adaptive scale parameter to automatically strengthen the training
    supervisions during the training process. In [p2sgrad](#bib.bib114) , Zhang et
    al. claimed that cosine based losses always include sensitive hyper-parameters
    which can make training process unstable, and it is very tricky to set suitable
    hyperparameters for a specific dataset. They addressed this challenge by directly
    designing the gradients for training in an adaptive manner. P2SGrad was able to
    achieves state-of-the-art performance on all three face recognition benchmarks,
    LFW, MegaFace, and IJB-C. There are several other works proposed for face recognition.
    For more detailed overview of deep learning-based face recognition, we refer the
    readers to [deep_face_survey](#bib.bib53) .
  prefs: []
  type: TYPE_NORMAL
- en: 'There have also been several works on using generative models for face image
    generation. To show the results of one promising model, in Progressive-GAN [prog_gan](#bib.bib117)
    , Karras et al developed a framework to grow both the generator and discriminator
    of GAN progressively, which can learn to generate high-resolution realistic images.
    Figure [11](#S3.F11 "Figure 11 ‣ 3.1.2 Deep Learning Works on Face Recognition
    ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric Recognition
    ‣ Biometrics Recognition Using Deep Learning: A Survey") shows 8 sample face images
    generated by this Progressive-GAN model trained on CELEB-A dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2ccfe3a0b1a88e3bb2cbcda907ec49da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: 8 sample images (of 1024 x 1024) generated by progressive GAN, using
    the CELEBA-HQ dataset. Courtesy of [prog_gan](#bib.bib117) .'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [12](#S3.F12 "Figure 12 ‣ 3.1.2 Deep Learning Works on Face Recognition
    ‣ 3.1 Face Recognition ‣ 3 Deep Learning Based Works on Biometric Recognition
    ‣ Biometrics Recognition Using Deep Learning: A Survey") illustrates the timeline
    of popular face recognition models since 2012. The listed models after 2014 are
    all deep learning based models. DeepFace and DeepID mark the beginning of deep
    learning based face recognition. As we can see many of the models after 2017 have
    focused on developing new loss functions for more discriminative feature learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1bf96d31ba95ba0e3b0406022b171be2.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: A timeline of face recognition methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Fingerprint Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fingerprint is arguably the most commonly used physiological biometric feature.
    It consists of ridges and valleys, which form unique patterns. Minutiae are major
    local portions of the fingerprint which can be used to determine the uniqueness
    of the fingerprint [jain1997line](#bib.bib25) . Important features exist in a
    fingerprint include ridge endings, bifurcations, islands, bridges, crossovers,
    and dots. [hrechak1990automated](#bib.bib118) .
  prefs: []
  type: TYPE_NORMAL
- en: A fingerprint needs to be captured by a special device in its close proximity.
    This makes making a dataset of fingerprints more time-consuming than some other
    biometrics, such as faces and ears. Nevertheless, there are quite a few remarkable
    fingerprint datasets that are being used around the world. Fingerprint recognition
    has always been a very active area with wide applications in industry, such as
    smartphone authentication, border security, and forensic science. As one of the
    classical works, Lee et al [lee1999fingerprint](#bib.bib119) used Gabor filtering
    on partitioned fingerprint images to extract features, followed by a k-NN classifier
    for the recognition, achieving 97.2% recognition rate. In addition, using the
    magnitude of the filter output with eight orientations added a degree of shift-invariance
    to the recognition scheme. Tico et al [tico2001wavelet](#bib.bib120) extracted
    wavelet features from the fingerprint to use in a k-NN classifier.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Fingerprint Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are several datasets developed for fingerprint recognition. Some of the
    most popular ones include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'FVC Fingerprint Database: Fingerprint Verification Competition (FVC) is widely
    used for fingerprint evaluation [fvc_finger](#bib.bib121) . FVC 2002 consists
    of three fingerprint datasets (DB1, DB2, and DB3) collected using different sensors.
    Each of these datasets consists of two sets: (i) Set A with 100 subjects and 8
    impressions per subject, (ii) Set B with 10 subjects and 8 impressions per subject.
    FVC 2004 adds another dataset (DB4) and contains more deliberate noise, e.g. skin
    distortions, skin moisture, and rotation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'PolyU High-resolution Fingerprint Database: This dataset contains two high
    resolution fingerprint image databases (denoted as DBI and DBII), provided by
    the Hong Kong Polytechnic University [polyU_finger](#bib.bib33) . It contains
    1480 images of 148 fingers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CASIA Fingerprint Dataset: CASIA Fingerprint Image Database V5 contains 20,000
    fingerprint images of 500 subjects [casia_finger](#bib.bib122) . Each volunteer
    contributed 40 fingerprint images of his eight fingers (left and right thumb,
    second, third, fourth finger), i.e., 5 images per finger. The volunteers were
    asked to rotate their fingers with various levels of pressure to generate significant
    intra-class variations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'NIST Fingerprint Dataset: NIST SD27 consists of 258 latent fingerprints and
    corresponding reference fingerprints [nist_finger](#bib.bib123) .'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Deep Learning Works on Fingerprint Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There have been numerous works on using deep learning for fingerprint recognition.
    Here we provide a summary of some of the prominent works in this area.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [finger_1](#bib.bib124) , Darlow et al. proposed a fingerprint minutiae
    extraction algorithm based on deep learning models, called MENet, and achieve
    promising results on fingerprint images from FVC datasets. In [finger_2](#bib.bib125)
    , Tang and colleagues proposed another deep learning-based model for fingerprint
    minutiae extraction, called FingerNet. This model jointly performs feature extraction,
    orientation estimation, segmentation, and uses them to estimate the minutiae maps.
    The block-diagram of this model is shown in Figure [13](#S3.F13 "Figure 13 ‣ 3.2.2
    Deep Learning Works on Fingerprint Recognition ‣ 3.2 Fingerprint Recognition ‣
    3 Deep Learning Based Works on Biometric Recognition ‣ Biometrics Recognition
    Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d2fa993c6465c25cf0cd6d815f5a6f2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The block-diagram of the proposed FingerNet model for minutiae extraction.
    Courtesy of [finger_2](#bib.bib125) .'
  prefs: []
  type: TYPE_NORMAL
- en: .
  prefs: []
  type: TYPE_NORMAL
- en: In another work [finger_3](#bib.bib126) , Lin and Kumar proposed a multi-view
    deep representation (based on CNNs) for contact-less and partial 3D fingerprint
    recognition. The proposed model includes one fully convolutional network for fingerprint
    segmentation and three Siamese networks to learn multi-view 3D fingerprint feature
    representation. They show promising results on several 3D fingerprint databases.
    In [finger_4](#bib.bib127) , the authors develop a fingerprint texture learning
    using a deep learning framework. They evaluate their models on several benchmarks,
    and achieve verification accuracies of 100, 98.65, 100 and 98% on the four databases
    of PolyU2D, IITD, CASIA-BLU and CASIA-WHT, respectively. In [finger_5](#bib.bib128)
    , Minaee et al. proposed a deep transfer learning approach to perform fingerprint
    recognition with a very high accuracy. They fine-tuned a pre-trained ResNet model
    on a popular fingerprint dataset, and are able to achieve very high recognition
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: In [finger_8](#bib.bib129) , Lin and Kumar proposed a multi-Siamese network
    to accurately match contactless to contact-based fingerprint images. In addition
    to the fingerprint images, hand-crafted fingerprint features, e.g., minutiae and
    core point, are also incorporated into the proposed architecture. This multi-Siamese
    CNN is trained using the fingerprint images and extracted features.
  prefs: []
  type: TYPE_NORMAL
- en: There are also some works using deep learning models for fingerprint segmentation.
    In [finger_6](#bib.bib130) , Stojanovic and colleagues proposed a fingerprint
    ROI segmentation algorithm based on convolutional neural networks. In another
    work [finger_7](#bib.bib131) , Zhu et al. proposed a new latent fingerprint segmentation
    method based on convolutional neural networks (”ConvNets”). The latent fingerprint
    segmentation problem is formulated as a classification system, in which a set
    of ConvNets are trained to classify each patch as either fingerprint or background.
    Then, a score map is calculated based on the classification results to evaluate
    the possibility of a pixel belonging to the fingerprint foreground. Finally, a
    segmentation mask is generated by thresholding the score map and used to delineate
    the latent fingerprint boundary.
  prefs: []
  type: TYPE_NORMAL
- en: There have also been some works for fake fingerprint detection. In [finger_9](#bib.bib132)
    , Kim et al. proposed a fingerprint liveliness detection based on statistical
    features learned from deep belief network (DBN). This method achieves good accuracy
    on various sensor datasets of the LivDet2013 test. In [finger_10](#bib.bib133)
    , Nogueira and colleagues proposed a model to detect fingerprint liveliness (where
    they are real or fake) using a convolutional neural network, which achieved an
    accuracy of 95.5% on fingerprint liveness detection competition 2015.
  prefs: []
  type: TYPE_NORMAL
- en: There have also been some works on using generative models for fingerprint image
    generation. In [finger_11](#bib.bib134) , Minaee et al proposed an algorithm for
    fingerprint image generation based on an extension of GAN, called ”Connectivity
    Imposed GAN”. This model adds total variation of the generated image to the GAN
    loss function, to promote the connectivity of generated fingerprint images. In
    [anil_finger](#bib.bib135) , Tabassi et al. developed a framework to synthesize
    altered fingerprints whose characteristics are similar to true altered fingerprints,
    and used them to train a classifier to detect ”Fingerprint alteration/obfuscation
    presentation attack” (i.e. intentional tamper or damage to the real friction ridge
    patterns to avoid identification).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Iris Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Iris images contain a rich set of features embedded in their texture and patterns
    which do not change over time, such as rings, corona, ciliary processes, freckles,
    and the striated trabecular meshwork of chromatophore and fibroblast cells, which
    is the most prevailing under visible light [daugman1993high](#bib.bib136) . Iris
    recognition has gained a lot of attention in recent years in different security-related
    fields.
  prefs: []
  type: TYPE_NORMAL
- en: 'John Daugman developed one of the first modern iris recognition frameworks
    using 2D Gabor wavelet transform [kumar2010comparison](#bib.bib35) . Iris recognition
    started to rise in popularity in the 1990s. In 1994, Wildes et al [wildes1994system](#bib.bib137)
    introduced a device using iris recognition for personnel authentication. After
    that, many researchers started looking at iris recognition problem. Early works
    have used a variety of methods to extract hand-crafted features from the iris.
    Williams et al [williams1996iris](#bib.bib138) converted all iris entries to an
    “IrisCode” and used Hamming’s distance of an input iris image’s IrisCode from
    those of the irises in the database as a metric for recognition. In [iris_0](#bib.bib139)
    , the authors proposed an iris recognition system based on ”deep scattering convolutional
    features”, which achieved a significantly high accuracy rate on IIT Delhi dataset.
    This work is not exactly using deep learning, but is using a deep scattering convolutional
    network, to extract hierarchical features from the image. The output images at
    different nodes of scattering network denote the transformed image along different
    orientation and scales. The transformed images of the first and second layers
    of scattering transform for a sample iris image are shown in Figures [14](#S3.F14
    "Figure 14 ‣ 3.3 Iris Recognition ‣ 3 Deep Learning Based Works on Biometric Recognition
    ‣ Biometrics Recognition Using Deep Learning: A Survey"). These images are derived
    by applying bank of filters of 5 different scales and 6 orientations.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/229e2b3ee2f78aa8687446cfebc345af.png)![Refer to caption](img/ddaf8ba82d1fcd7ddf3467cb6149c6b3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: The images from the first (on the left) and second (on the right)
    layers of the scattering transform.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth mentioning that many of the classical iris recognition models perform
    several pre-processing steps such as iris detection, normalization, and enhancement,
    as shown in Figure [15](#S3.F15 "Figure 15 ‣ 3.3 Iris Recognition ‣ 3 Deep Learning
    Based Works on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning:
    A Survey"). They then extract features from the normalized or enhanced image.
    Many of the modern works on iris recognition skip normalization and enhancement,
    and yet, they are still able to achieve very high recognition accuracy. One reason
    is the ability of deep models to capture high-level semantic a features from original
    iris images, which are discriminative enough to perform well for iris recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/84cef33b9ed0dffea02bbe5435836013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Illustration of some of the key pre-processing steps for iris recognition,
    courtesy of [iris_12](#bib.bib140) .'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Iris Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Various datasets have been proposed for iris recognition in the past. Some
    of the most popular ones include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CASIA-Iris-1000 Database: CASIA-Iris-1000 contains 20,000 iris images from
    1,000 subjects, which were collected using an IKEMB-100 camera. The main sources
    of intra-class variations in CASIA-Iris-1000 are eyeglasses and specular reflections
    [casia_iris](#bib.bib142) .'
  prefs: []
  type: TYPE_NORMAL
- en: 'UBIRIS Dataset: The UBIRIS database has two distinct versions, UBIRIS.v1 and
    UBIRIS.v2. The first version of this database is composed of 1877 images collected
    from 241 eyes in two distinct sessions. It simulates less constrained imaging
    conditions [ubiris_iris](#bib.bib143) . The second version of the UBIRIS database
    has over 11000 images (and continuously growing) and more realistic noise factors.'
  prefs: []
  type: TYPE_NORMAL
- en: 'IIT Delhi Iris Dataset: IIT Delhi iris database contains 2240 iris images captured
    from 224 different people. The resolution of these images is 320x240 pixels [iit_iris](#bib.bib144)
    . Iris images in this dataset have variable color distribution, and different
    (iris) sizes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'ND Datasets: ND-CrossSensor-Iris-2013 consists of two iris databases, taken
    with two iris sensors: LG2200 and LG4000\. The LG2200 dataset consists of 116,564
    iris images, and LG4000 consists of 29,986 iris images of 676 subjects [lg_iris](#bib.bib145)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'MICHE Dataset: Mobile Iris Challenge Evaluation (MICHE) consists of iris images
    acquired under unconstrained conditions using smartphones. It consists of more
    than 3,732 images acquired from 92 subjects using three different smartphones
    [miche_iris](#bib.bib146) .'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Deep Learning Works on Iris Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Compared to face recognition, deep learning models made their ways to iris recognition
    with a few years delay.
  prefs: []
  type: TYPE_NORMAL
- en: As one of the first works using deep learning for iris recognition, in [iris_1](#bib.bib147)
    Minaee et al. showed that features extracted from a pre-trained CNN model trained
    on ImageNet are able to achieve a reasonably high accuracy rate for iris recognition.
    In this work, they used features derived from different layers of VGGNet [vggnet](#bib.bib66)
    , and trained a multi-class SVM on top of it, and showed that the trained model
    can achieve state-of-the-art accuracy on two iris recognition benchmarks, CASIA-1000
    and IIT Delhi databases. They also showed that features extracted from the mid-layers
    of VGGNet achieve slightly higher accuracy from the the very last layers. In another
    work [iris_1_2](#bib.bib148) , Gangwar and Joshi proposed an iris recognition
    network based on convolutional neural network, which provides robust, discriminative,
    compact resulting in very high accuracy rate, and can work pretty well in cross-sensor
    recognition of iris images.
  prefs: []
  type: TYPE_NORMAL
- en: In [iris_1_3](#bib.bib149) , Baqar and colleagues proposed an iris recognition
    framework based on deep belief networks, as well as contour information of iris
    images. Contour based feature vector has been used to discriminate samples belonging
    to different classes i.e., difference of sclera-iris and iris-pupil contours,
    and is named as “Unique Signature”. Once the features extracted, deep belief network
    (DBN) with modified back-propagation algorithm based feed-forward neural network
    (RVLR-NN) has been used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [iris_12](#bib.bib140) , Zhao and Kumar proposed an iris recognition model
    based on ”Deeply Learned Spatially Corresponding Features”. The proposed framework
    is based on a fully convolutional network (FCN), which outputs spatially corresponding
    iris feature descriptors. They also introduce a specially designed ”Extended Triplet
    Loss (ETL)” function to incorporate the bit-shifting and non-iris masking. The
    triplet network is illustrated in Figure [16](#S3.F16 "Figure 16 ‣ 3.3.2 Deep
    Learning Works on Iris Recognition ‣ 3.3 Iris Recognition ‣ 3 Deep Learning Based
    Works on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning: A
    Survey"). They also developed a sub-network to provide appropriate information
    for identifying meaningful iris regions, which serves as essential input for the
    newly developed ETL. They were able to outperform several classic and state-of-the-art
    iris recognition approaches on a few iris databases.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/557c4fbb7894aa90bcb8ffcf04c0cb66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: The block-diagram of triplet network used for iris recognition,
    courtesy of [iris_12](#bib.bib140) .'
  prefs: []
  type: TYPE_NORMAL
- en: In another work [iris_2](#bib.bib150) , Alaslani et al. developed an iris recognition
    system, based on deep features extracted from AlexNet, followed by a multi-class
    classification, and achieved high accuracy rates on CASIA-Iris-V1, CASIA-Iris-1000
    and, CASIA-Iris-V3 Interval databases. In [menon2018iris](#bib.bib151) , Menon
    shows the applications of convolutional features from a fine-tuned pre-trained
    model for both identification and verification problems. In [iris_4](#bib.bib152)
    , Hofbauer and colleagues proposed a CNN based algorithm for segmentation of iris
    images, which can results in higher accuracies than previous models. In another
    work [iris_5](#bib.bib153) , Ahmad and Fuller developed an iris recognition model
    based on triplet network, call ThirdEye. Their work directly uses the segmented,
    un-normalized iris images, and is shown to achieve equal error rates of 1.32%,
    9.20%, and 0.59% on the ND-0405, UbirisV2, and IITD datasets respectively. In
    a more recent work [iris_8](#bib.bib154) , Minaee and colleagues proposed an algorithm
    for iris recognition based using a deep transfer learning approach. They trained
    a CNN model (by fine-tuning a pre-trained ResNet model) on an iris dataset, and
    achieved very accurate recognition on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the rise of deep generative models, there have been works that apply them
    to iris recognition. In [iris_6](#bib.bib155) , Minaee et al proposed an algorithm
    for iris image generation based on convolutional GAN, which can generate realistic
    iris images. These images can be used for augmenting the training set, resulting
    in better feature representation and higher accuracy. Four sample iris images
    generated by this work (over different training epochs) are shown in Figure [17](#S3.F17
    "Figure 17 ‣ 3.3.2 Deep Learning Works on Iris Recognition ‣ 3.3 Iris Recognition
    ‣ 3 Deep Learning Based Works on Biometric Recognition ‣ Biometrics Recognition
    Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fa8a042282b4bedf3562177a3daa9ecf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 17: The generated iris images for 4 input latent vectors, over 140 epochs
    (on every 10 epochs), using the trained model on IIT Delhi Iris database. Courtesy
    of [iris_6](#bib.bib155) .'
  prefs: []
  type: TYPE_NORMAL
- en: In [iris_7](#bib.bib156) , Lee and colleagues proposed a data augmentation technique
    based on GAN to augment the training data for iris recognition, resulting in a
    higher accuracy rate. They claim that historical data augmentation techniques
    such as geometric transformations and brightness adjustment result in samples
    with very high correlation with the original ones, but using augmentation based
    on a conditional generative adversarial network can result in higher test accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Palmprint Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Palmprint is another biometric which is gaining more attention recently. In
    addition to minutiae features, palmprints also consist of geometry-based features,
    delta points, principal lines, and wrinkles [zhang1999two](#bib.bib11) ; [zhang2012comparative](#bib.bib157)
    . Each part of a palmprint has different features, including texture, ridges,
    lines and creases. An advantage of palmprints is that the creases in palmprint
    virtually do not change over time and are easy to extract [chen2001palmprint](#bib.bib158)
    . However, sampling palmprints requires special devices, making their collection
    not as easy as other biometrics such as fingerprint, iris and face. Classical
    works on palmprint recognition have explored a wide range of hand-carfted features
    such as as PCA and ICA [connie2003palmprint](#bib.bib159) , Fourier transform
    [li2002palmprint](#bib.bib160) , wavelet transform [wu2002wavelet](#bib.bib161)
    , line feature matching [shu1998palmprint](#bib.bib162) , and deep scaterring
    features [palm_2](#bib.bib163) .
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.1 Palmprint Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Several datasets have been proposed for palmprint recognition dataset. Some
    of the most widely used datasets include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'PolyU Multispectral Palmprint Dataset: The images from PolyU dataset were collected
    from 250 volunteers, including 195 males and 55 females. In total, the database
    contains 6,000 images from 500 different palms for one illumination [polyu_palm](#bib.bib34)
    . Samples are collected in two separate sessions. In each session, the subject
    was asked to provide 6 images for each palm. Therefore, 24 images of each illumination
    from 2 palms were collected from each subject.'
  prefs: []
  type: TYPE_NORMAL
- en: 'CASIA Palmprint Database: CASIA Palmprint Image Database contains of 5,502
    palmprint images captured from 312 subjects. For each subject, they collect palmprint
    images from both left and right palms [casia_palm](#bib.bib164) . All palmprint
    images are 8-bit gray-level JPEG files by their self-developed palmprint recognition
    device.'
  prefs: []
  type: TYPE_NORMAL
- en: 'IIT Delhi Touchless Palmprint Database: The IIT Delhi palmprint image database
    consists of the hand images collected from the students and staff at IIT Delhi,
    New Delhi, India [iit_palm](#bib.bib165) . This database has been acquired using
    a simple and touchless imaging setup. The currently available database is from
    235 users. Seven images from each subject, from each of the left and right hand,
    are acquired in varying hand pose variations. Each image has a size of 800x600
    pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.4.2 Deep Learning Works on Palmprint Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [palm_1](#bib.bib166) , Xin et al. proposed one of the early works on palmprint
    recognition using a deep learning framework. The authors built a deep belief net
    by top-to-down unsupervised training, and tuned the model parameters toward a
    robust accuracy on the validation set. Their experimental analysis showed a performance
    gain over classical models that are based on LBP, and PCA, and other other hand-crafted
    features.
  prefs: []
  type: TYPE_NORMAL
- en: In another work, Samai et al. proposed a deep learning-based model for 2D and
    3D palmprint recognition [palm_3](#bib.bib167) . They proposed an efficient biometric
    identification system combining 2D and 3D palmprint by fusing them at matching
    score level. To exploit the 3D palmprint data, they converted them to grayscale
    images by using the Mean Curvature (MC) and the Gauss Curvature (GC). They then
    extracted features from images using Discrete Cosine Transform Net (DCT Net).
  prefs: []
  type: TYPE_NORMAL
- en: Zhong et al. proposed a palmprint recognition algorithm using Siamese network
    [palm_4](#bib.bib168) . Two VGG-16 networks (with shared parameters) were employed
    to extract features for two input palmprint images, and another network is used
    on top of them to directly obtain the similarity of two input palmprints according
    to their convolutional features. This method achieved an Equal Error Rate (EER)
    of 0.2819% on on PolyU dataset. In [palm_5](#bib.bib169) , Izadpanahkakhk et al.
    proposed a transfer learning approach towards palmprint verification, which jointly
    extracts regions of interests and features from the images. They use a pre-trained
    convolutional network, along with SVM to make prediction. They achieved an IoU
    score of 93% and EER of 0.0125 on Hong Kong Polytechnic University Palmprint (HKPU)
    database.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [palm_6](#bib.bib170) , Shao and Zhong proposed a few-shot palmprint recognition
    model using a graph neural network. In this work, the palmprint features extracted
    by a convolutional neural network are processed into nodes in the GNN. The edges
    in the GNN are used to represent similarities between image nodes. In a more recent
    work [palm_7](#bib.bib171) , Shao and colleagues proposed a deep palmprint recognition
    approach by combining hash coding and knowledge distillation. Deep hashing network
    are used to convert palmprint images to binary codes to save storage space and
    speed up the matching process. The architecture of the proposed deep hashing network
    is shown in Figure [18](#S3.F18 "Figure 18 ‣ 3.4.2 Deep Learning Works on Palmprint
    Recognition ‣ 3.4 Palmprint Recognition ‣ 3 Deep Learning Based Works on Biometric
    Recognition ‣ Biometrics Recognition Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9d8434caa80baea2e62fcd7382bb7d08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 18: The block-diagram of the proposed deep hashing network, courtesy
    of [palm_7](#bib.bib171) .'
  prefs: []
  type: TYPE_NORMAL
- en: They also proposed a database for unconstrained palmprint recognition, which
    consists of more than 30,000 images collected by 5 different mobile phones, and
    achieved promising results on that dataset. In [palm_8](#bib.bib172) , Shao et
    al. proposed a cross-domain palmprint recognition based on transfer convolutional
    autoencoder. Convolutional autoencoders were firstly used to extract low-dimensional
    features. A discriminator was then introduced to reduce the gap of two domains.
    The auto-encoders and discriminator were alternately trained, and finally the
    features with the same distribution were extracted.
  prefs: []
  type: TYPE_NORMAL
- en: In [palm_9](#bib.bib173) , Zhao and colleagues proposed a joint deep convolutional
    feature representation for hyperspectral palmprint recognition. A CNN stack is
    constructed to extract its features from the entire spectral bands and generate
    a joint convolutional feature. They evaluated their model on a hyperspectral palmprint
    dataset consisting of 53 spectral bands with 110,770 images. They achieved an
    EER of 0.01%. In [palm_10](#bib.bib174) , Xie et al. proposed a gender classification
    framework using convolutional neural network on plamprint images. They fine-tuned
    the pre-trained VGGNet on a palmprint dataset and showed that the proposed structure
    could achieve a good performance for gender classification.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Ear Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Ear recognition is a more recent problem that scientists are exploring, and
    the volume of biometric recognition works involving ears is expected to increase
    in the coming years. One of the more prominent aspects of ear recognition is the
    fact that the subject can be photographed from either side of their head and the
    ears are almost identical (suitable when subject is not cooperating, or hiding
    his/her face). Also, since there is no need for the subject’s proximity, images
    may be taken from the ear more easily. However, ears of the subject may still
    be occluded by factors such as hair, hat, and jewelry, making it difficult to
    detect and use the ear image [cintas2019automatic](#bib.bib175) . There are multiple
    classical methods to perform ear recognition: geometric methods, which try to
    extract the shape of the ear; holistic methods, which extract the features from
    the ear image as a whole; local methods, which specifically use a portion of the
    image; and hybrid methods, which use a combination of the others [emervsivc2017training](#bib.bib176)
    , [naseem2008sparse](#bib.bib177) .'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.1 Ear Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The datasets below are some of the popular 2D ear recognition datasets, which
    are used by researchers.
  prefs: []
  type: TYPE_NORMAL
- en: 'IIT Ear Database: The IIT Delhi ear image database contains 471 images, acquired
    from the 121 different subjects and each subject has at least three ear images
    [iit_ear](#bib.bib36) . All the subjects in the database are in the age group
    14-58 years. The resolution of these images is 272x204 pixels and all these images
    are available in jpeg format.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AWE Ear Dataset: This database contains 1,000 images of 100 persons. Images
    were collected from the web using a semi-automatic procedure, and contain the
    following annotations: gender, ethnicity, accessories, occlusions, head pitch,
    head roll, head yaw, head side, and central tragus point [awe_ear](#bib.bib178)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-PIE Ear Dataset: This dataset was created in 2017 [eyiokur2017domain](#bib.bib179)
    based on the Multi-PIE face dataset [gross2010multi](#bib.bib84) . There are 17,000
    ear images extracted from the profile and near-profile images of 205 subjects
    present in the face dataset. The ears in the images are in different illuminations,
    angles, and conditions, making it a decent dataset for a more generalized ear
    recognition approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'USTB Ear Database: This dataset contains ear images of 60 volunteers captured
    in 2002 [ustb_ear](#bib.bib180) . Every volunteer is photographed three different
    images. They are normal frontal image, frontal image with trivial angle rotation
    and image under different lighting condition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'UERC Ear Dataset: The ear images in this dataset [emervsivc2017unconstrained](#bib.bib181)
    are collected from the Internet in unconstrained conditions, i.e., from the wild.
    There is a total of 11,804 images from 3,706 subjects, of which 2,304 images from
    166 subjects are for training, and the rest are for testing.'
  prefs: []
  type: TYPE_NORMAL
- en: 'AMI Ear Dataset: This dataset [gonzalez2008biometria](#bib.bib182) contains
    700 images of size 492 x 702 from 100 subjects in the age range of 19 to 65 years
    old. The images are all in the same lighting condition and distance, and from
    both sides of the subject’s head. The images, however, differ in focal lengths,
    and the direction the subject is looking (up, down, left, right).'
  prefs: []
  type: TYPE_NORMAL
- en: 'CP Ear Dataset: One of the older datasets in this area, the Carreira-Perpinan
    dataset [perpinan1995compression](#bib.bib183) contains 102 left ear images taken
    from 17 subjects in the same conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'WPUT Ear Dataset: The West Pomeranian University of Technology (WPUT) dataset
    [frejlichowski2010west](#bib.bib184) contains 2,071 images from 501 subjects (247
    male and 254 female subjects), from different age groups and ethnicities. The
    images are taken in different lighting conditions, from various distances and
    two angles, and include ears with and without accessories, including earrings,
    glasses, scarves, and hearing aids.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.5.2 Deep Learning Works on Ear Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ear recognition is not as popular as face, iris, and fingerprint recognition
    yet. Therefore, datasets used for this procedure are still limited in size. Based
    on this, Zhang et al [zhang2019few](#bib.bib185) proposed few-shot learning methods,
    where the network use the limited training and quickly learn to recognize the
    images. Dodge et al [dodge2018unconstrained](#bib.bib186) , who proposed using
    transfer learning with deep networks for unconstrained ear recognition Emersic
    and colleagues [emervsivc2017unconstrained](#bib.bib181) , also proposed a deep
    learning-based averaging system to mitigate the overfitting caused by the small
    size of the datasets. In [emersic2017training](#bib.bib187) , the authors proposed
    the first publicly available CNN-based ear recognition method. They explored different
    strategies, such as different architectures, selective learning on pre-trained
    data and aggressive data augmentation to find the best configurations for their
    work.
  prefs: []
  type: TYPE_NORMAL
- en: In [emervsivc2018towards](#bib.bib188) , the authors showed how ear accessories
    can disrupt the recognition process and even be used for spoofing, especially
    in a CNN-based method, e.g., VGG-16, against a traditional method, e.g., local
    binary patterns (LBP), and proposed methods to remove such accessories and improve
    the performance, such as ”inprinting” and area coloring. Sinha et al [sinha2019convolutional](#bib.bib189)
    proposed a framework which localizes the outer ear image using HOG and SVMs, and
    then uses CNNs to perform ear recognition. It aims to resolve the issues usually
    associated with feature extraction appearance-based techniques, namely the conditions
    in which the image was taken, such as illumination, angle, contrast, and scale,
    which are also present in other biometric recognition systems, e.g. for face.
    Omara et al [omara2018learning](#bib.bib190) proposed extracting hierarchical
    deep features from ear images, fusing the features using discriminant correlation
    analysis (DCA) Haghighat et al [haghighat2016discriminant](#bib.bib191) to reduce
    their dimensions, and due to the lack of ear images per person, creating pairwise
    samples and using pairwise SVM [brunner2012pairwise](#bib.bib192) to perform the
    matching (since regular SVM would not perform well due to the small size of the
    datasets). Hansley et al [hansley2018employing](#bib.bib193) used a fusion of
    CNNs and handcrafted features for ear recognition which outperformed other state-of-the-art
    CNN-based works, reaching to the conclusion that handcrafted features can complement
    deep learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 Voice Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Voice Recognition (also known as speaker recognition) is the task of determining
    a person’s ID using the characteristics of one’s voice. In a way, speaker recognition
    includes both behavioral and physiological features, such as accent and pitch
    respectively. Using automatic ways to perform speaker recognition dates back to
    1960s when Bell Laboratories were approached by law enforcement agencies about
    the possibility of identifying callers who had made verbal bomb threats over the
    telephone [shaver2016brief](#bib.bib194) . Over the years, researchers have developed
    many models that can perform this task effectively, especially with the help of
    deep learning. In addition to security applications, it is also being used in
    virtual personal assistants, such as Google Assistant, so they can recognize and
    distinguish the phone owner’s voice from the others [yoffie2018voice](#bib.bib195)
    .
  prefs: []
  type: TYPE_NORMAL
- en: Speaker recognition can be classified into speaker identification and speaker
    verification. speaker identification is the process of determining a person’s
    ID from a set of registered voice using a given utterance [naseem2010sparse](#bib.bib196)
    , whereas speaker verification is the process of accepting or rejecting a proposed
    identity claimed for a speaker [Furui2008](#bib.bib197) . Since these two tasks
    usually share the same evaluation process under commonly-used metrics, the terms
    are sometimes used interchangeably in referenced papers. Speaker recognition is
    also closely related to speaker diarization, where an input audio stream is partitioned
    into homogeneous segments according to the speaker identity [garcia2017speaker](#bib.bib198)
    .
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.1 Voice Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some of the popular datasets on voice/speaker recognition are:'
  prefs: []
  type: TYPE_NORMAL
- en: 'NIST SRE: Starting in 1996, the National Institute of Standards and Technology
    (NIST) has organized a series of evaluations for speaker recognition research
    [martin2001nist](#bib.bib199) . The Speaker Recognition Evaluation (SRE) datasets
    compiled by NIST have thus become the most widely used datasets for evaluation
    of speaker recognition systems. These datasets are collected in an evolving fashion,
    and each evaluation plan has a slightly different focus. These evaluation datasets
    differ in audio lengths [nist2010sre](#bib.bib200) , recording devices (telephone,
    handsets, and video) [nist2018sre](#bib.bib201) , data origination (in North America
    or outside) [nist2016sre](#bib.bib202) , and match/mismatch scenarios. In recent
    years, SRE 2016 and SRE 2018 are the most popular datasets in this area.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SITW: The Speakers in the Wild (SITW) dataset was acquired across unconstrained
    conditions [mclaren2016speakers](#bib.bib203) . Unlike the SRE datasets, this
    data was not collected under controlled conditions and thus contains real noise
    and reverberation. The database consists of recordings of 299 speakers, with an
    average of eight different sessions per person.'
  prefs: []
  type: TYPE_NORMAL
- en: 'VoxCeleb: The VoxCeleb dataset [nagrani2017voxceleb](#bib.bib204) and VoxCeleb2
    dataset [chung2018voxceleb2](#bib.bib205) are public datasets compiled from interview
    videos uploaded to YouTube to emphasize the lack of large scale unconstrained
    data for speaker recognition. These data are collected using a fully automated
    pipeline. A two-stream synchronization CNN is used to estimate the correlation
    between the audio track and the mouth motion of the video, and then CNN-based
    facial recognition techniques are used to identify speakers for speech annotation.
    VoxCeleb1 contains over 100,000 utterances for 1,251 celebrities, and VoxCeleb2
    contains over a million utterances for 6,112 identities.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from datasets designed purely for speaker recognition tasks, many datasets
    collected for automatic speech recognition can also be used for training or evaluation
    of speaker recognition systems. For example, the Switchboard dataset [godfrey1997switchboard](#bib.bib206)
    and the Fisher Corpus [cieri2004fisher](#bib.bib207) , which were originally collected
    for speech recognition tasks, are also used for model training in NIST Speaker
    Recognition Evaluations. On the other hand, researchers may utilize existing speech
    recognition datasets to prepare their own speaker recognition evaluation dataset
    to prove the effectiveness of their research. For example, Librispeech dataset
    [panayotov2015librispeech](#bib.bib208) and the TIMIT dataset [zue1990speech](#bib.bib209)
    are pre-processed by the author in [ravanelli2018learning](#bib.bib210) to serve
    as evaluation set for speaker recognition task.
  prefs: []
  type: TYPE_NORMAL
- en: 3.6.2 Deep Learning Works on Voice Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before the era of deep learning, most state-of-the-art speaker recognition systems
    are built with the i-vectors approach [dehak2010front](#bib.bib211) , which uses
    factor analysis to define a low-dimensional space that models both speaker and
    channel variabilities. In recent years, it has become more and more popular to
    explore deep learning approaches for speaker recognition. One of the first approach
    among these efforts is to incorporate DNN-based acoustic models into the i-vector
    framework [lei2014novel](#bib.bib212) . This method uses an DNN acoustic model
    trained for Automatic Speech Recognition (ASR) to gather speaker statistics for
    i-vector model training. It has been shown that this improvement leads to a 30%
    relative reduction in equal error rate.
  prefs: []
  type: TYPE_NORMAL
- en: Around the same time, d-vector was proposed in [variani2014deep](#bib.bib213)
    to tackle text-dependent speaker recognition using neural network. In this approach,
    a DNN is trained to classify speakers at the frame-level. During enrollment and
    testing, the trained DNN is used to extract speaker specific features from the
    last hidden layer. “d-vectors” are then computed by averaging these features and
    used as speaker embeddings for recognition. This method shows 14% and 25% relative
    improvement over an i-vector system under clean and noisy conditions, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: In [snyder2018x](#bib.bib214) , a time-delay neural network is trained to extract
    segment level “x-vectors” for text-independent speech recognition. This network
    takes in features of speech segments and passes them through a few non-linear
    layers followed by a pooling layer to classify speakers at segment-level. X-vectors
    are then extracted from the pooling layer for enrollment and testing. It is shown
    that an x-vector system can achieve a better speaker recognition performance compared
    to the traditional i-vector approach, with the help of data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end approaches based on neural networks are also explored in various
    papers. In [heigold2016end](#bib.bib215) and [zhang2016end](#bib.bib216) , neural
    networks are designed to take in pairs of speech segments, and are trained to
    classify match/mismatch targets. A specially designed triplet loss function is
    proposed in [zhang2017end](#bib.bib217) to substitute a binary classification
    loss function. Generalized end-to-end (GE2E) loss, which is similar to triplet
    loss, is proposed in [wan2018generalized](#bib.bib218) for text-dependent speaker
    recognition on an in-house dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In [le2018robust](#bib.bib219) , a complementary optimizing goal called intra-class
    loss is proposed to improve deep speaker embeddings learned with triplet loss.
    It is shown in the paper that models trained using intra-class loss can yield
    a significant relative reduction of 30% in equal error rate (EER) compared to
    the original triplet loss. The effectiveness is evaluated on both VoxCeleb and
    VoxForge datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In [ravanelli2018learning](#bib.bib210) , the authors proposed a method for
    learning speaker embeddings from raw waveform by maximizing the mutual information.
    This approach uses an encoder-discriminator architecture similar to that of Generative
    Adversarial Networks (GANs) to optimize mutual information implicitly. The authors
    show that this approach effectively learns useful speaker representations, leading
    to a superior performance on the VoxCeleb corpus when compared with i-vector baseline
    and CNN-based triples loss systems.
  prefs: []
  type: TYPE_NORMAL
- en: In [bhattacharya2019deep](#bib.bib220) , the authors combine a deep convolutional
    feature extractor, self-attentive pooling and large-margin loss functions into
    their end-to-end deep speaker recognizers. The individual and ensemble models
    from this approach achieved state-of-the-art performance on VoxCeleb with a relative
    improvement of 70% and 82%, respectively, over the best reported results. The
    authors also proposed to use a neural network to subsitute PLDA classifier, which
    enables them to get the state-of-the-art results on NIST-SRE 2016 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Signature Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Signature is considered a behavioral biometric. It is widely used in traditional
    and digital formats to verify the user’s identity for the purposes of security,
    transactions, agreements, etc. Therefore, being able to distinguish an authentic
    signature from a forged one is of utmost importance. Signature forgery can be
    performed as either a random forgery, where no attempt is made to make an authentic
    signature (e.g., merely writing the name [radhika2011approach](#bib.bib221) ),
    or a skilled forgery, where the signature is made to look like the original and
    is performed with the genuine signature in mind [soleimani2016deep](#bib.bib222)
    .
  prefs: []
  type: TYPE_NORMAL
- en: In order to distinguish an authentic signature from a forged one, one may either
    store merely signature samples to compare against (offline verification), or also
    the features of the written signature such as the thickness of a stroke and the
    speed of the pen during the signing [impedovo2008automatic](#bib.bib223) . For
    verification, there are writer-dependent (WD) and writer-independent (WI) methods.
    In WD methods, a classifier is trained for each signature owner, whereas, in WI
    methods, one is trained for all owners [srihari2004learning](#bib.bib224) .
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.1 Signature Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some of the popular signature verification datasets include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'ICDAR 2009 SVC: ICDAR 2009 Signature Verification Competition contains simultaneously
    acquired online and offline signature samples [icdar_svc](#bib.bib225) . The online
    dataset is called ”NFI-online” and was processed and segmented by Louis Vuurpijl.
    The offline dataset is called ”NFI-offline” and was scanned by Vivian Blankers
    from the NFI. The collection contains: authentic signatures from 100 writers,
    and forged signatures from 33 writers. The NLDCC-online signature collection contains
    in total 1953 online and 1953 offline signature files.'
  prefs: []
  type: TYPE_NORMAL
- en: 'SVC 2004: Signature Verification Competition 2004 consists of two datasets
    for two verification tasks: one for pen-based input devices like PDAs and another
    one for digitizing tablets [SVC2004](#bib.bib226) . Each dataset consists of 100
    sets of signatures with each set containing 20 genuine signatures and 20 skilled
    forgeries.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Offline GPDS-960 Corpus: This offline signature dataset [vargas2007off](#bib.bib227)
    includes signatures from 960 subjects. There are 24 authentic signatures for each
    person, and 30 forgeries performed by other people not in the original 960 (1920
    forgers in total). Some works have used a subset of this public dataset, usually
    the images for the first 160 or 300 subjects, dubbing them GPDS-160 and GPDS-300
    respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.7.2 Deep Learning Works on Signature Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Before the rise of deep learning to its current popularity, there were a few
    works seeking to use it. For example, Ribeiro et al [ribeiro2011deep](#bib.bib228)
    proposed a deep learning-based method to both identify a signature’s owner and
    distinguish an authentic signature from a fake, making use of the Restricted Boltzmann
    Machine (RBM) [ackley1985learning](#bib.bib229) . With more powerful computer
    and massively parallel architectures making deep learning mainstream, the number
    of deep learning-based works increased dramatically, including those involving
    signature recognition. Rantzsch et al [rantzsch2016signature](#bib.bib230) proposed
    an embedding-based WI offline signature verification, in which the input signatures
    are embedded in a high-dimensional space using a specific training pattern, and
    the Euclidean distance between the input and the embedded signatures will determine
    the outcome. Soleimani et al [soleimani2016deep](#bib.bib222) proposed Deep Multitask
    Metric Learning (DMML), a deep neural network used for offline signature verification,
    mixing WD methods, WI methods, and transfer learning. Zhang et al [zhang2016multi](#bib.bib231)
    proposed a hybrid WD-WI classifier in conjuction with a DC-GAN network in order
    to learn to extract the signature features in an unsupervised manner. With signature
    being a behavioral biometric, it is imperative to learn the best features to distinguish
    an authentic signature from a forged one. Hafemann et al [hafemann2017learning](#bib.bib232)
    proposed a WI CNN-based system to learn features of forgeries from multiple datasets,
    which greatly reduced the error equal rate compared to that of the state-of-the-art.
    Wang et al [wang2019signature](#bib.bib233) proposed signature identification
    using a special GAN network (SIGAN) in which the loss value from the discriminator
    network is utilized as the threshold for the identification process. Tolosana
    et al [tolosana2018exploring](#bib.bib234) proposed an online writer-independent
    signature verification method using Siamese recurrent neural networks (RNNs),
    including long short term memory (LSTM) and gated recurrent units (GRUs).
  prefs: []
  type: TYPE_NORMAL
- en: 3.8 Gait Recognition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Gait recognition is a popular pattern recognition problem and attracts a lot
    of researchers from different communities such as computer vision, machine learning,
    biomedical, forensic studying and robotics. This problem has also great potential
    in industries such as visual surveillance, since gait can be observed from a distance
    without the need for the subject’s cooperation. Similar to other behavioral biometrics,
    it is difficult, however possible, to try to imitate someone else’s gait [zhang2016siamese](#bib.bib235)
    . It is also possible for the gait to change due to factors such as the carried
    load, injuries, clothing, walking speed, viewing angle, and weather conditions,
    [alotaibi2017improved](#bib.bib236) , [wolf2016multi](#bib.bib237) . It is also
    a challenge to recognize a person among a group of walking people [chen2017multi](#bib.bib238)
    . Gait recognition can be model-based, in which the the structure of the subject’s
    body is extracted (meaning more compute demand), or appearance-based, in which
    features are extracted from the person’s movement in the images [wolf2016multi](#bib.bib237)
    , [zhang2016siamese](#bib.bib235) .
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.1 Gait Datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Some of the widely used gait recognition datasets include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'CASIA Gait Database: This CASIA Gait Recognition Dataset contains 4 subsets:
    Dataset A (standard dataset) [casia_gaitA](#bib.bib31) , Dataset B (multi-view
    gait dataset), Dataset C (infrared gait dataset), and Dataset D (gait and its
    corresponding footprint dataset) [CASIA_gait](#bib.bib239) . Here we give details
    of CASIA B dataset, which is very popular. Dataset B is a large multi-view gait
    database, which is created in 2005\. There are 124 subjects, and the gait data
    was captured from 11 views. Three variations, namely view angle, clothing and
    carrying condition changes, are separately considered. Besides the video files,
    they also provide human silhouettes extracted from video files. The reader is
    referred to [casia_gaitB](#bib.bib240) for more detailed information about Dataset
    B.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Osaka Treadmill Dataset: This dataset has been collected in March 2007 at the
    Institute of Scientific and Industrial Research (ISIR), Osaka University (OU)
    [osaka_url](#bib.bib241) . The dataset consists of 4,007 persons walking on a
    treadmill surrounded by the 25 cameras at 60 fps, 640 by 480 pixels. The datasets
    are basically distributed in a form of silhouette sequences registered and size-normalized
    to 88x128 pixels size. They have four subsets of this dataset, dataset A: Speed
    variation, dataset B: Clothes variation, dataset C: view variations, and dataset
    D: Gait fluctuation. The dataset B is composed of gait silhouette sequences of
    68 subjects from the side view with clothes variations of up to 32 combinations.
    Detailed descriptions about all these datasets can be found in this technical
    note [osaka_gait](#bib.bib242) .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Osaka University Large Population (OULP) Dataset: This dataset [iwama2012isir](#bib.bib243)
    includes images from 4,016 subjects from different ages (up to 94 years old) taken
    from two surrounding cameras and 4 observation angles. The images are normalized
    to 88x128 pixels.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.8.2 Deep Learning Works on Gait Recognition
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Research on gait recognition based on deep learning has only taken off in the
    past few years. In one of the older works, Wolf et al [wolf2016multi](#bib.bib237)
    proposed a gait recognition system using 3D convolutional neural networks which
    learns the gait from multiple viewing angles. This model consists of multiple
    layers of 3D convolutions, max pooling and ReLUs, followed by fully-connected
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhang et al [zhang2016siamese](#bib.bib235) proposed a Siamese neural network
    for gait recognition, in which the sequences of images are converted into gait
    energy images (GEI) [han2005individual](#bib.bib244) . Next, they are fed to the
    twin CNN networks and their contrastive losses are also calculated. This allows
    the system to minimize the loss for similar inputs and maximize it for different
    ones. The network for this work is shown in Figure [19](#S3.F19 "Figure 19 ‣ 3.8.2
    Deep Learning Works on Gait Recognition ‣ 3.8 Gait Recognition ‣ 3 Deep Learning
    Based Works on Biometric Recognition ‣ Biometrics Recognition Using Deep Learning:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e29d9c494ed29a9e006d15fbf58ee435.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 19: Siamese network for gait recognition, courtesy of [zhang2016siamese](#bib.bib235)
    .'
  prefs: []
  type: TYPE_NORMAL
- en: Battistone et al [battistone2019tglstm](#bib.bib245) proposed gait recognition
    through a time-based graph LSTM network, which uses alternating recursive LSTM
    layers and dense layers to extract skeletons from the person’s images and learn
    their joint features. Zou et al [zou2018deep](#bib.bib246) proposed a hybrid CNN-RNN
    network which uses the data from smartphone sensors for gait recognition, particularly
    from the accelerometer and the gyroscope, and the subjects are not restricted
    in their walking in any way.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Performance of Different Models on Different Datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we are going to present the performance of different biometric
    recognition models developed over the past few years. We are going to present
    the results of each biometric recognition model separately, by providing the performance
    of several promising works on one or two widely used dataset of that biometric.
    Before getting into the quantitative analysis, we are going to first briefly introduce
    some of the popular metrics that are used for evaluating biometric recognition
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Popular Metrics For Evaluating Biometrics Recognition Systems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Various metrics are designed to evaluate the performance a biometric recognition
    systems. Here we provide an overview of some of the popular metrics for evaluation
    verification and identification algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Biometric verification is relevant to the problem of re-identification, where
    we want to see if a given data matches a registered sample. In many cases the
    performance is measured in terms of verification accuracy, specially when a test
    dataset is provided. Equal error rate (EER) is another popular metric, which is
    the rate of error decided by a threshold that yields equal false negative rate
    and false positive rate. Receiver operating characteristic (ROC) is also another
    classical metric used for verification performance. ROC essentially measures the
    true positive rate (TPR), which is the fraction of genuine comparisons that correctly
    exceeds the threshold, and the false positive rate (FPR), which is the fraction
    of impostor comparisons that incorrectly exceeds the threshold, at different thresholds.
    ACC (classification accuracy) is another metric used by LFW, which is simply the
    percentage of correct classifications. Many works also use TPR for a certain FPR.
    For example IJB-A focuses TPR@FAR=$10^{-3}$, while Megaface uses TPR@FPR= $10^{-6}$.
  prefs: []
  type: TYPE_NORMAL
- en: Closed-set identification can be measured in terms of closed-set identification
    accuracy, as well as rank-N detection and identification rate. Rank-N measures
    the percentage of probe searches return the samples from probe’s gallery within
    the top N rank-ordered results (e.g. IJB-A/B/C focuses on the rank-1 and rank-5
    recognition rates). The cumulative match characteristic (CMC) is another popular
    metric, which measures the percentage of probes identified within a given rank.
    Confusion matrix is also a popular metric for smaller datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Open-set identification deals with the cases where the recognition system should
    reject unknown/unseen subjects (probes which are not present in gallery) at the
    test time. At present, there are very few databases covering the task of open-set
    biometric recognition. Open-set identification accuracy is a popular metrics for
    this task. Some benchmarks also suggested to use the decision error trade-off
    (DET) curve to characterize the FNIR (false-negative identification rate) as a
    function of FPIR (false-positive identification rate).
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance of Models for Face Recognition: For face recognition, various metrics
    are used for verification and identification. For face verification, EER is one
    of the most popular metrics. For identification, various metrics are used such
    as close-set identification accuracy, open-set identification accuracy. For open-set
    performance, many works used detection and identification accuracy at a certain
    false-alarm rate (mostly 1%).'
  prefs: []
  type: TYPE_NORMAL
- en: Due to the popularity of face recognition, there are a large number of algorithms
    and datasets available. Here, we are going to provide the performance of some
    of the most promising deep learning-based face recognition models, and their comparison
    with some of the promising classical face recognition models on three popular
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned earlier, LFW is one of the most widely used for face recognition.
    The performance of some of the most prominent deep learning-based face verification
    models on this dataset is provided in Table [1](#S4.T1 "Table 1 ‣ 4.1 Popular
    Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different
    Models on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey").
    We have also included the results of two very well-known classical face verification
    works. As we can see, models based on deep learning algorithms achieve superior
    performance over classical techniques with a large margin. In fact, many deep
    learning approaches have surpassed human performance and are already close to
    100% (For verification task, not identification).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Accuracy of different face recognition models for face verification
    on LFW dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Architecture | Used Dataset | Accuracy on LFW |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Joint Bayesian [jointbayes](#bib.bib247) | Classical | - | 92.4 |'
  prefs: []
  type: TYPE_TB
- en: '| Tom-vs-Pete [Tom-vs-Pete](#bib.bib248) | Classical | - | 93.3 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepFace [DeepFace](#bib.bib96) | AlexNet | Facebook (4.4M,4K) | 97.35 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepID2 [DeepID2](#bib.bib98) | AlexNet | CelebFaces+ (0.2M,10K) | 99.15
    |'
  prefs: []
  type: TYPE_TB
- en: '| VGGface [VGGface_model](#bib.bib101) | VGGNet-16 | VGGface (2.6M,2.6K) |
    98.95 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepID3 [DeepID3](#bib.bib99) | VGGNet-10 | CelebFaces+ (0.2M,10K) | 99.53
    |'
  prefs: []
  type: TYPE_TB
- en: '| FaceNet [FaceNet](#bib.bib100) | GoogleNet-24 | Google (500M,10M) | 99.63
    |'
  prefs: []
  type: TYPE_TB
- en: '| Range Loss [RangeLoss](#bib.bib105) | VGGNet-16 | MS-Celeb-1M, CASIA-WebFace
    | 99.52 |'
  prefs: []
  type: TYPE_TB
- en: '| L2-softmax [L2softmax](#bib.bib106) | ResNet-101 | MS-Celeb-1M (3.7M,58K)
    | 99.87 |'
  prefs: []
  type: TYPE_TB
- en: '| Marginal Loss [marginalloss](#bib.bib249) | ResNet-27 | MS-Celeb-1M (4M,80K)
    | 99.48 |'
  prefs: []
  type: TYPE_TB
- en: '| SphereFace [Sphereface](#bib.bib108) | ResNet-64 | CASIA-WebFace (0.49M,10K)
    | 99.42 |'
  prefs: []
  type: TYPE_TB
- en: '| AMS loss [amsloss](#bib.bib109) | ResNet-20 | CASIA-WebFace (0.49M,10K) |
    99.12 |'
  prefs: []
  type: TYPE_TB
- en: '| Cos Face [CosFace](#bib.bib110) | ResNet-64 | CASIA-WebFace (0.49M,10K) |
    99.33 |'
  prefs: []
  type: TYPE_TB
- en: '| Ring loss [RingLoss](#bib.bib112) | ResNet-64 | CelebFaces+ (0.2M,10K) |
    99.50 |'
  prefs: []
  type: TYPE_TB
- en: '| Arcface [Arcface](#bib.bib111) | ResNet-100 | MS-Celeb-1M (3.8M,85K) | 99.45
    |'
  prefs: []
  type: TYPE_TB
- en: '| AdaCos [adacos](#bib.bib113) | ResNet-50 | WebFace | 99.71 |'
  prefs: []
  type: TYPE_TB
- en: '| P2SGrad [p2sgrad](#bib.bib114) | ResNet-50 | CASIAWebFace | 99.82 |'
  prefs: []
  type: TYPE_TB
- en: 'As mentioned earlier, closed-set identification is another popular face recognition
    task. Table [2](#S4.T2 "Table 2 ‣ 4.1 Popular Metrics For Evaluating Biometrics
    Recognition Systems ‣ 4 Performance of Different Models on Different Datasets
    ‣ Biometrics Recognition Using Deep Learning: A Survey"), provides the summary
    of the performance of some of the recent state-of-the-art deep learning-based
    works on the MegaFace challenge 1 (for both identification and verification tasks).
    MegaFace challenge evaluates rank1 recognition rate as a function of an increasing
    number of gallery distractors (going from 10 to 1 million) for identification
    accuracy. For verification, they report TPR at FAR= $10^{-6}$. Some of these reported
    accuracies are taken from [adapface](#bib.bib116) , where they implemented the
    Softmax, A-Softmax, CosFace, ArcFace and the AdaptiveFace models with the same
    50-layer CNN, for fair comparison. As we can see the deep learning-based models
    in recent years achieve very high Rank-1 identification accuracy even in the case
    where 1 million distractors are included in the gallery.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Face identification and verification evaluation on MegaFace Challenge
    1.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Protocol | Rank1 Identification Accuracy | (TPR@$10^{-6}$FPR) Verification
    Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Beijing FaceAll Norm 1600, from [adapface](#bib.bib116) | Large | 64.8 |
    67.11 |'
  prefs: []
  type: TYPE_TB
- en: '| Softmax [adapface](#bib.bib116) | Large | 71.36 | 73.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Google - FaceNet v8 [FaceNet](#bib.bib100) | Large | 70.49 | 86.47 |'
  prefs: []
  type: TYPE_TB
- en: '| YouTu Lab, from [adapface](#bib.bib116) | Large | 83.29 | 91.34 |'
  prefs: []
  type: TYPE_TB
- en: '| DeepSense V2, from [adapface](#bib.bib116) | Large | 81.29 | 95.99 |'
  prefs: []
  type: TYPE_TB
- en: '| Cos Face (Single-patch) [CosFace](#bib.bib110) | Large | 82.72 | 96.65 |'
  prefs: []
  type: TYPE_TB
- en: '| Cos Face (3-patch ensemble) [CosFace](#bib.bib110) | Large | 84.26 | 97.96
    |'
  prefs: []
  type: TYPE_TB
- en: '| SphereFace [Sphereface](#bib.bib108) | Large | 92.241 | 93.423 |'
  prefs: []
  type: TYPE_TB
- en: '| Arcface [Arcface](#bib.bib111) | Large | 94.637 | 94.850 |'
  prefs: []
  type: TYPE_TB
- en: '| AdaptiveFace [adapface](#bib.bib116) | Large | 95.023 | 95.608 |'
  prefs: []
  type: TYPE_TB
- en: Deep learning-based models have achieved great performance on other facial analysis
    tasks too, such as facial landmark detection, facial expression recognition, face
    tracking, age prediction from face, face aging, part of face tracking, and many
    more. As this paper is mostly focused on biometric recognition, we skip the details
    of models developed for those works here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance of Models for Fingerprint Recognition: It is common for fingerprint
    recognition models to report their results using either the accuracy or equal
    error rate (EER). Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating Biometrics
    Recognition Systems ‣ 4 Performance of Different Models on Different Datasets
    ‣ Biometrics Recognition Using Deep Learning: A Survey") provides the accuracy
    of some of the recent fingerprint recognition works on PolyU, FVC, and CASIA databases.
    As we can see, deep learning-based models achieve very high accuracy rate on these
    benchmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Accuracy of several fingerprint recognition algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: \csvreader
  prefs: []
  type: TYPE_NORMAL
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  prefs: []
  type: TYPE_NORMAL
- en: ']csv/fingerprint.csv\csvlinetotablerow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance of Models for Iris Recognition: Many of the recent iris recognition
    works have reported their accuracy rates on different iris databases, making it
    hard to compare all of them on a single benchmark. The performance of deep learning-based
    iris recognition algorithms, and their comparison with some of the promising classical
    iris recognition models are provided in Table [4](#S4.T4 "Table 4 ‣ 4.1 Popular
    Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different
    Models on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey").
    As we can see models based on deep learning algorithms achieve superior performance
    over classical techniques. Some of these numbers are taken from [iris_survey](#bib.bib251)
    and [iris_2019](#bib.bib252) .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: The performance of iris recognition models on some of the most popular
    datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Dataset | Model/Feature | Performance |'
  prefs: []
  type: TYPE_TB
- en: '| Elastic Graph Matching [elastic](#bib.bib253) | IITD | - | Acc= 98% |'
  prefs: []
  type: TYPE_TB
- en: '| SIFT Based Model [sift_iris](#bib.bib254) | CASIA, MMU, UBIRIS | SIFT features
    | Acc= 99.05% EER=3.5% |'
  prefs: []
  type: TYPE_TB
- en: '| Deep CNN [menon2018iris](#bib.bib151) | IITD | - | Acc=99.8% |'
  prefs: []
  type: TYPE_TB
- en: '| Deep CNN [menon2018iris](#bib.bib151) | UBIRIS v2 | - | Acc=95.36% |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Scattering [iris_0](#bib.bib139) | IITD | ScatNet3+Texture features
    | Acc= 99.2% |'
  prefs: []
  type: TYPE_TB
- en: '| Deep Features [iris_1](#bib.bib147) | IITD | VGG-16 | Acc= 99.4% |'
  prefs: []
  type: TYPE_TB
- en: '| SCNN [iris_scnn](#bib.bib255) | CASIA-v4, FRGC FOCS | Semantics-assisted
    convolutional networks | R1-ACC= 98.4 (CASIA-v4) |'
  prefs: []
  type: TYPE_TB
- en: 'Performance of Models for Palmprint Recognition: It is common for palmprint
    recognition papers to compare their work against others using the accuracy rate
    or equal error rate (EER). Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating
    Biometrics Recognition Systems ‣ 4 Performance of Different Models on Different
    Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey") displays the
    accuracy of some of the palmprint recognition works. As we can see, deep learning-based
    models achieve very high accuracy rate on PolyU palmprint dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 5: Accuracy of various palmprint recognition systems.'
  prefs: []
  type: TYPE_NORMAL
- en: \csvreader
  prefs: []
  type: TYPE_NORMAL
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  prefs: []
  type: TYPE_NORMAL
- en: ']csv/palmprint.csv\csvlinetotablerow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance of Models for Ear Recognition: The results of some of the recent
    ear recognition models are provided in Table [4.1](#S4.SS1 "4.1 Popular Metrics
    For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different Models
    on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey").
    Besides recognition accuracy, some of the works have also reported their rank-5
    accuracy, i.e. if one of the first 5 outputs of the algorithm is correct, the
    algorithm has succeeded. Different deep learning-based models for ear recognition
    report their accuracy on different benchmarks. Therefore, we list some of the
    promising works, along with the respective datasets that they are evaluated on,
    in Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating Biometrics Recognition
    Systems ‣ 4 Performance of Different Models on Different Datasets ‣ Biometrics
    Recognition Using Deep Learning: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Accuracy of select ear recognition algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: \csvreader
  prefs: []
  type: TYPE_NORMAL
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  prefs: []
  type: TYPE_NORMAL
- en: ']csv/ear.csv\csvlinetotablerow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance of Models for Voice Recognition: The most widely used metric for
    evaluation of speaker recognition systems is Equal Error Rate (EER). Apart from
    EER, other metrics are also used for system evaluation. For example, detection
    error trade-off curve (DET curve) is used in SRE performance evaluations to compare
    different systems. A DET curve is created by plotting the false negative rate
    versus false positive rate, with logarithmic scale on the x- and y-axes. (EER
    corresponds to the point on a DET curve where false negative rate and false positive
    rate are equal.) Minimum detection cost is another metric that is frequently used
    in speaker recognition tasks [van2007introduction](#bib.bib261) . This cost is
    defined as a weighted average of two normalized error rates. Not all of these
    metrics are reported in every research papers, but EER is the most important metric
    to compare different systems.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [7](#S4.T7 "Table 7 ‣ 4.1 Popular Metrics For Evaluating Biometrics Recognition
    Systems ‣ 4 Performance of Different Models on Different Datasets ‣ Biometrics
    Recognition Using Deep Learning: A Survey") records the performance of some of
    the best deep leaning based spearker recognition systems on VoxCeleb1 dataset.
    As is shown in the table, the progress made by researchers over the last two years
    are prominent. All these systems shown in Table [7](#S4.T7 "Table 7 ‣ 4.1 Popular
    Metrics For Evaluating Biometrics Recognition Systems ‣ 4 Performance of Different
    Models on Different Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey")
    are single systems, which means the performance can be boosted further with system
    combination or ensembles.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 7: Accuracy of different speaker recognition systems on VoxCeleb1 dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Loss | Training set |   EER |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| i-vector + PLDA [shon2018frame](#bib.bib262) | - | VoxCeleb1 | 5.39 |'
  prefs: []
  type: TYPE_TB
- en: '| SincNet+LIM (raw audio) [ravanelli2018learning](#bib.bib210) | - | VoxCeleb1
    | 5.80 |'
  prefs: []
  type: TYPE_TB
- en: '| x-vector* [shon2018frame](#bib.bib262) | Softmax | VoxCeleb1 | 6.00 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-34 [cai2018exploring](#bib.bib263) | A-Softmax + GNLL | VoxCeleb1
    | 4.46 |'
  prefs: []
  type: TYPE_TB
- en: '| x-vector [okabe2018attentive](#bib.bib264) | Softmax | VoxCeleb1 | 3.85 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-20 [hajibabaei2018unified](#bib.bib265) | AM-Softmax | VoxCeleb1 |
    4.30 |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-50 [chung2018voxceleb2](#bib.bib205) | Softmax + Contrastive | VoxCeleb2
    | 3.95 |'
  prefs: []
  type: TYPE_TB
- en: '| Thin ResNet-34 [xie2019utterance](#bib.bib266) | Softmax | VoxCeleb2 | 3.22
    |'
  prefs: []
  type: TYPE_TB
- en: '| ResNet-28 [bhattacharya2019deep](#bib.bib220) | AAM | VoxCeleb1 | 0.95 |'
  prefs: []
  type: TYPE_TB
- en: For SRE datasets, due to the large number of its series and complexity of different
    evaluation conditions, it is hard to compile all results into one table. Also
    different papers may present results on different sets or conditions, making it
    hard to compare the performance across different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: The deep learning-based approaches discussed above have also been applied to
    other related areas, e.g. speech diarization, replay attack detection and language
    identification. Since this paper focuses on biometric recognition, we skip the
    details for these tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance of Models for Signature Recognition: Most signature recognition
    works use EER as the performance metric, but sometimes, they also report accuracy.
    Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating Biometrics Recognition
    Systems ‣ 4 Performance of Different Models on Different Datasets ‣ Biometrics
    Recognition Using Deep Learning: A Survey") summarizes the EER of several signature
    verification methods on GPDS dataset, where there are 12 authentic signature samples
    used for each person (except in [soleimani2016deep](#bib.bib222) where it is 10
    samples). In addition, Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating
    Biometrics Recognition Systems ‣ 4 Performance of Different Models on Different
    Datasets ‣ Biometrics Recognition Using Deep Learning: A Survey") provides the
    reported accuracy of a few other works on other datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Reported EER of selected signature recognition models on GPDS dataset
    (using 10-12 genuine samples).'
  prefs: []
  type: TYPE_NORMAL
- en: \csvreader
  prefs: []
  type: TYPE_NORMAL
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  prefs: []
  type: TYPE_NORMAL
- en: ']csv/signature_eer.csv\csvlinetotablerow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 9: Accuracy reported by some signature recognition models.'
  prefs: []
  type: TYPE_NORMAL
- en: \csvreader
  prefs: []
  type: TYPE_NORMAL
- en: '[tabular=—c—c—c—, no head,column count=3, table head=, late after line='
  prefs: []
  type: TYPE_NORMAL
- en: ']csv/signature.csv\csvlinetotablerow'
  prefs: []
  type: TYPE_NORMAL
- en: 'Performance of Models for Gait Recognition: Likely due to the different configurations
    of the existing gait datasets, it is difficult to compare the deep learning-based
    gait recognition works. The results are reported in the form of accuracies and
    EER across different gallery view angles and cross-view settings. For Gait recognition,
    it is common to compare rank-5 statistics as well as the normal rank-1 ones. We
    have gathered some of the averaged accuracy results reported in [sundararajan2018deep](#bib.bib270)
    in Table [4.1](#S4.SS1 "4.1 Popular Metrics For Evaluating Biometrics Recognition
    Systems ‣ 4 Performance of Different Models on Different Datasets ‣ Biometrics
    Recognition Using Deep Learning: A Survey"). Note that results using CASIA-B are
    collected in various scene and viewing conditions, while, using OU-ISIR, they
    are for cross-view conditions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 10: Accuracy of select gait recognition models.'
  prefs: []
  type: TYPE_NORMAL
- en: \csvreader
  prefs: []
  type: TYPE_NORMAL
- en: '[tabular=—c—c—c—c—, no head,column count=4, table head=, late after line='
  prefs: []
  type: TYPE_NORMAL
- en: ']csv/gait.csv\csvlinetotablerow'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Challenges and Opportunities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Biometric recognition systems have undergone great progress with the help of
    deep learning-based models, in the past few years, but there are still several
    challenges ahead which may be tackled in the few years and decades.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 More Challenging Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although some of the current biometric recognition datasets (such as MegaFace,
    MS-Celeb-1M) contain a very large number of candidates, they are still far from
    representing all the real-world scenarios. Although state-of-the-art algorithms
    can achieve accuracy rates of over 99.9% on LFW and Megaface databases, fundamental
    challenges such as matching faces/biometrics across ages, different poses, partial-data,
    different sensor types still remain challenging. Also the number of subjects/people
    in real-world scenarios should be in the order of tens of millions. Therefore
    biometrics dataset which contain a much larger number of classes (10M-100M), as
    well as a lot more intra-class variations, would be another big step towards supporting
    all real-world conditions.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Interpretable Deep Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is true that deep learning-based models achieved an astonishing performance
    on many of the challenging benchmarks, but there are still several open questions
    about these models. For example, what exactly are deep learning models learning?
    Why are these models easily fooled by adversarial examples (while human can detect
    many of those examples easily)? What is a minimal neural architecture which can
    achieve a certain recognition accuracy on a given dataset?
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Few Shot Learning, and Self-Supervised Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many of the successful models developed for biometric recognition are trained
    on large datasets with enough samples for each class. One of the interesting future
    trend will be to develop recognition models which can learn a powerful models
    from very few shots (zero/one shot in extreme case). This would enable training
    discriminative models without the need to provide several samples for each person/identity.
    Self-supervised learning [selfsuper](#bib.bib279) is also another recent popular
    topic in deep learning, which has not been explored enough for biometrics recognition.
    One way to use it would be to learn discriminative biometric feature from local
    patches of an image, and then aggregating those features and used for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Biometric Fusion
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Single biometric recognition by itself is far from sufficient to solve all biometric/forensic
    tasks. For example distinguishing identical twins may not be possible from face
    only, or matching an identity from face with disguise, or after surgery may not
    be that easy. Fusing the information from multiple biometrics can provide a more
    reliable solution/system in many of these cases (for example using voice+face
    or voice+gait can potentially solve the identical twin detection) [ross2004multimodal](#bib.bib280)
    ; [ross2003information](#bib.bib281) . A good neural architecture which can jointly
    encode and aggregate different biometrics would be an interesting problem (information
    fusion can happen at the data level, feature level, score level, or decision level).
    Image set classification could also be useful in this direction [imageset](#bib.bib282)
    . There have been some works on biometric fusion, but most of them are far from
    the real-world scale of biometric recognition, and are mostly in their infancy.
    For some of the challenges of multi-modal machine learning, we refer the reader
    to [multimodal_challenge](#bib.bib54) .
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Realtime Models for Various Applications
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In many applications, accuracy is the most important factor; however, there
    are many applications in which it is also crucial to have a near real-time biometric
    recognition model. This could be very useful for on-device solutions, such as
    the one for cellphone and tablet authentication. Some of the current deep models
    for biometrics recognition are far from this speed requirement, and developing
    near real-time models yet accurate models would be very valuable.
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 Memory Efficient Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many of the deep learning-based models require a significant amount of memory
    even during inference. So far, most of the effort has focused on improving the
    accuracy of these models, but in order to fit these models in devices, the networks
    must be simplified. This can be done either by using a simpler model, using model
    compression techniques, or training a complex model and then using knowledge distillation
    techniques to compress that into a smaller network mimicking the initial complex
    model. Having a memory-efficient model opens up the door for these models to be
    used even on consumer devices.
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Security and Privacy Issues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Security is of great importance in biometric recognition systems. Presentation
    attack, template attack, and adversarial attack threaten the security of deep
    biometric recognition systems, and challenge the current anti-spoofing methods.
    Although some attempts have been done for adversarial example detection, there
    is still a long way to robust/reliable anti-spoofing capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: With the leakage of biological/biometrics data nowadays, privacy concerns are
    rising. Some information about the user identity/age/gender can be decoded from
    the neural feature representation of their images. Research on visual cryptography,
    to protect users’ privacy on stored biometrics templates are essential for addressing
    public concern on privacy.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we provided a summary of the recent deep learning-based models
    (till 2019) for biometric recognition. As opposed to the other surveys, it provides
    an overview of most used biometrics. Deep neural models have shown promising improvement
    over classical models for various biometrics. Some biometrics have attracted a
    lot more attention (such as face) due to the wider industrial applications, and
    availability of large-scale datasets, but other biometrics seem to be following
    the same trend. Although deep learning research in biometrics has achieved promising
    results, there is still a great room for improvement in different directions,
    such as creating larger and more challenging datasets, addressing model interpretation,
    fusing multiple biometrics, and addressing security and privacy issues.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgements.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: We would like to thank Prof. Rama Chellappa, and Dr. Nalini Ratha for reviewing
    this work, and providing very helpful comments and suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Anil Jain, Lin Hong, and Sharath Pankanti. Biometric identification. Communications
    of the ACM, 43(2):90–98, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] David Zhang. Automated biometrics: Technologies and systems, volume 7.
    Springer Science & Business Media, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] David Zhang, Guangming Lu, and Lei Zhang. Advanced biometrics. Springer,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Javier Galbally, Raffaele Cappelli, Alessandra Lumini, Davide Maltoni,
    and Julian Fierrez. Fake fingertip generation from a minutiae template. In International
    Conference on Pattern Recognition. IEEE, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Sefik Eskimez, Ross K Maddox, Chenliang Xu, and Zhiyao Duan. Generating
    talking face landmarks from speech. In Conference on Latent Variable Analysis
    and Signal Separation. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] https://deepfakedetectionchallenge.ai/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Huaxiao Mo, Bolin Chen, and Weiqi Luo. Fake faces identification via convolutional
    neural network. In Information Hiding and Multimedia Security. ACM, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Yuezun Li and Siwei Lyu. Exposing deepfake videos by detecting face warping
    artifacts. arXiv preprint arXiv:1811.00656, 2, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Anil K Jain, Arun Ross, Salil Prabhakar, et al. An introduction to biometric
    recognition. IEEE Transactions on circuits and systems for video technology, 14(1),
    2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Guangming Lu, David Zhang, and Kuanquan Wang. Palmprint recognition using
    eigenpalms features. Pattern Recognition Letters, 24(9-10):1463–1467, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Dapeng Zhang and Wei Shu. Two novel characteristics in palmprint verification:
    datum point invariance and line feature matching. Pattern recognition, 32(4):691–702,
    1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Wenyi Zhao, Rama Chellappa, P Jonathon Phillips, and Azriel Rosenfeld.
    Face recognition: A literature survey. ACM computing surveys (CSUR), 35(4):399–458,
    2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Zhichun Mu, Li Yuan, Zhengguang Xu, Dechun Xi, and Shuai Qi. Shape and
    structural feature based ear recognition. In Chinese Conference on Biometric Recognition,
    pages 663–670. Springer, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] John Daugman. How iris recognition works. In The essential guide to image
    processing, pages 715–739. Elsevier, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Kevin W Bowyer and Mark J Burge. Handbook of iris recognition. Springer,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Halvor Borgen, Patrick Bours, and Stephen D Wolthusen. Visible-spectrum
    biometric retina recognition. In Conference on Intelligent Information Hiding
    and Multimedia Signal Processing. IEEE, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Mohamed Elhoseny, Amir Nabil, Aboul Hassanien, and Diego Oliva. Hybrid
    rough neural network model for signature recognition. In Advances in Soft Computing
    and Machine Learning in Image Processing, pages 295–318\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Jin Wang, Mary She, Saeid Nahavandi, and Abbas Kouzani. A review of vision-based
    gait recognition methods for human identification. In 2010 international conference
    on digital image computing: techniques and applications, pages 320–327\. IEEE,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Fabian Monrose and Aviel D Rubin. Keystroke dynamics as a biometric for
    authentication. Future Generation computer systems, 16(4):351–359, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Joseph P Campbell. Speaker recognition: A tutorial. Proceedings of the
    IEEE, 85(9):1437–1462, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Mark Hawthorne. Fingerprints: analysis and understanding. CRC Press, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Anil K Jain and Stan Z Li. Handbook of face recognition. Springer, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Yulan Guo, Yinjie Lei, Li Liu, Yan Wang, Mohammed Bennamoun, and Ferdous
    Sohel. Ei3d: Expression-invariant 3d face recognition based on feature and shape
    matching. Pattern Recognition Letters, 83, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Unsang Park, Yiying Tong, and Anil K Jain. Age-invariant face recognition.
    IEEE transactions on pattern analysis and machine intelligence, 32(5), 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Anil Jain, Lin Hong, and Ruud Bolle. On-line fingerprint verification.
    IEEE transactions on pattern analysis and machine intelligence, 19(4):302–314,
    1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] David Zhang, Zhenhua Guo, and Yazhuo Gong. Multispectral Biometrics: Systems
    and Applications. Springer, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Maria De Marsico, Alfredo Petrosino, and Stefano Ricciardi. Iris recognition
    through machine learning techniques: A survey. Pattern Recognition Letters, 82:106–115,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Žiga Emeršič, Vitomir Štruc, and Peter Peer. Ear recognition: More than
    a survey. Neurocomputing, 255:26–39, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Javier Galbally, Moises Diaz-Cabrera, Miguel A Ferrer, Marta Gomez-Barrero,
    Aythami Morales, and Julian Fierrez. On-line signature recognition through the
    combination of real dynamic data and synthetically generated static data. Pattern
    Recognition, 48(9), 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Liang Wang, Huazhong Ning, Tieniu Tan, and Weiming Hu. Fusion of static
    and dynamic body biometrics for gait recognition. IEEE Transactions on circuits
    and systems for video technology, 14(2):149–158, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Liang Wang, Tieniu Tan, Huazhong Ning, and Weiming Hu. Silhouette analysis-based
    gait recognition for human identification. IEEE transactions on pattern analysis
    and machine intelligence, 25(12):1505–1518, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Extended yale face database b (b+). http://vision.ucsd.edu/content/extended-yale-face-database-b-b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Polyu fingerprint dataset. http://www4.comp.polyu.edu.hk/~biometrics/HRF/HRF_old.htm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Polyu palmprint dataset. https://www4.comp.polyu.edu.hk/~biometrics/MultispectralPalmprint/MSP.htm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Ajay Kumar and Arun Passi. Comparison and combination of iris matchers
    for reliable personal authentication. Pattern recognition, 43(3):1016–1026, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Ajay Kumar and Chenye Wu. Automated human identification using ear imaging.
    Pattern Recognition, 45(3):956–968, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Timo Ahonen, Abdenour Hadid, and Matti Pietikäinen. Face recognition with
    local binary patterns. In European conference on computer vision, pages 469–481.
    Springer, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Andrea F Abate, Michele Nappi, Daniel Riccio, and Gabriele Sabatino. 2d
    and 3d face recognition: A survey. Pattern recognition letters, 28(14):1885–1906,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] David Zhang, Fengxi Song, Yong Xu, and Zhizhen Liang. Advanced pattern
    recognition technologies with applications to biometrics. IGI Global Hershey,
    PA, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] David G Lowe. Distinctive image features from scale-invariant keypoints.
    International journal of computer vision, 60(2):91–110, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Navneet Dalal and Bill Triggs. Histograms of oriented gradients for human
    detection. 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Wai Kin Kong, David Zhang, and Wenxin Li. Palmprint feature extraction
    using 2-d gabor filters. Pattern recognition, 36(10):2339–2347, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Jian Huang Lai, Pong C Yuen, and Guo Can Feng. Face recognition using
    holistic fourier invariant features. Pattern Recognition, 34(1):95–109, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Andrew Teoh Beng Jin, David Ngo Chek Ling, and Ong Thian Song. An efficient
    fingerprint verification system using integrated wavelet and fourier–mellin invariant
    transform. Image and Vision Computing, 22(6):503–513, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Hervé Abdi and Lynne J Williams. Principal component analysis. Wiley interdisciplinary
    reviews: computational statistics, 2(4):433–459, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Jian Yang, David Zhang, Alejandro F Frangi, and Jing-yu Yang. Two-dimensional
    pca: a new approach to appearance-based face representation and recognition. IEEE
    transactions on pattern analysis and machine intelligence, 26(1):131–137, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification
    with deep convolutional neural networks. In Advances in neural information processing
    systems, pages 1097–1105, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Aleksei Grigorevich Ivakhnenko and Valentin Grigorevich Lapa. Cybernetic
    predicting devices. Technical report, Purdue Univ, School of Electrical Engineering,
    1966.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Jürgen Schmidhuber. Deep learning in neural networks: An overview. Neural
    networks, 61:85–117, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning
    representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Léon Bottou. Stochastic gradient learning in neural networks. Proceedings
    of Neuro-Nımes, 91(8):12, 1991.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Mei Wang and Weihong Deng. Deep face recognition: A survey. arXiv preprint
    arXiv:1804.06655, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Tadas Baltrusaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal
    machine learning: A survey and taxonomy. IEEE Transactions on Pattern Analysis
    and Machine Intelligence, 41(2), 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Yann LeCun, Léon Bottou, Yoshua Bengio, Patrick Haffner, et al. Gradient-based
    learning applied to document recognition. Proceedings of the IEEE, 86(11):2278–2324,
    1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural
    computation, 9(8):1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Advances in neural information processing systems, pages 2672–2680, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Kunihiko Fukushima. Neocognitron: A self-organizing neural network model
    for a mechanism of pattern recognition unaffected by shift in position. Biological
    cybernetics, 36(4):193–202, 1980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional
    networks for biomedical image segmentation. In International Conference on Medical
    image computing and computer-assisted intervention, pages 234–241\. Springer,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards
    real-time object detection with region proposal networks. In Advances in neural
    information processing systems, pages 91–99, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou Tang. Learning a deep
    convolutional network for image super-resolution. In European conference on computer
    vision, pages 184–199. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Kin Gwn Lore, Adedotun Akintayo, and Soumik Sarkar. Llnet: A deep autoencoder
    approach to natural low-light image enhancement. Pattern Recognition, 61:650–662,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Quanzeng You, Hailin Jin, Zhaowen Wang, Chen Fang, and Jiebo Luo. Image
    captioning with semantic attention. In IEEE conference on computer vision and
    pattern recognition, pages 4651–4659, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional
    networks. In European conference on computer vision. Springer, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks
    for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 770–778, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,
    Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going
    deeper with convolutions. In IEEE conference on computer vision and pattern recognition,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Andrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional
    neural networks for mobile vision applications. arXiv preprint arXiv:1704.04861,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger.
    Densely connected convolutional networks. In IEEE conference on computer vision
    and pattern recognition, pages 4700–4708, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] David E Rumelhart, Geoffrey E Hinton, Ronald J Williams, et al. Learning
    representations by back-propagating errors. Cognitive modeling, 5(3):1, 1988.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] http://colah.github.io/posts/2015-08-Understanding-LSTMs/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, and Pierre
    Manzagol. Stacked denoising autoencoders: Learning useful representations in a
    deep network with a local denoising criterion. Journal of machine learning research,
    11(Dec):3371–3408, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv
    preprint arXiv:1312.6114, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] https://github.com/hindupuravinash/the-gan-zoo.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Weihong Deng, Jiani Hu, and Jun Guo. In defense of sparsity based face
    recognition. In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pages 399–406, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Qiong Cao, Yiming Ying, and Peng Li. Similarity metric learning for face
    recognition. In Proceedings of the IEEE International Conference on Computer Vision,
    pages 2408–2415, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] John Wright, Allen Y Yang, Arvind Ganesh, S Shankar Sastry, and Yi Ma.
    Robust face recognition via sparse representation. IEEE transactions on pattern
    analysis and machine intelligence, 31(2):210–227, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Meng Yang, Lei Zhang, Jian Yang, and David Zhang. Regularized robust coding
    for face recognition. IEEE transactions on image processing, 22(5), 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Dong Yi, Zhen Lei, and Stan Z Li. Towards pose robust face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 3539–3545, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Ajmal Mian, Mohammed Bennamoun, and Robyn Owens. An efficient multimodal
    2d-3d hybrid approach to automatic face recognition. IEEE transactions on pattern
    analysis and machine intelligence, 29(11), 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Yale face database. http://vision.ucsd.edu/content/yale-face-database.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] The cmu multi-pie face database. http://www.cs.cmu.edu/afs/cs/project/PIE/MultiPie/Multi-Pie/Home.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Ralph Gross, Iain Matthews, Jeffrey Cohn, Takeo Kanade, and Simon Baker.
    Multi-pie. Image and Vision Computing, 28(5):807–813, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Labeled faces in the wild. http://vis-www.cs.umass.edu/lfw/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Polyu nir face database. http://www4.comp.polyu.edu.hk/~biometrics/polyudb_face.htm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Youtube faces db. http://www.cs.tau.ac.il/~wolf/ytfaces/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Vggface2. http://www.robots.ox.ac.uk/~vgg/data/vgg_face2/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Dong Yi, Zhen Lei, Shengcai Liao, and Stan Z Li. Learning face representation
    from scratch. arXiv preprint arXiv:1411.7923, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m:
    A dataset and benchmark for large-scale face recognition. In European Conference
    on Computer Vision, pages 87–102. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face
    attributes in the wild. In Proceedings of the IEEE international conference on
    computer vision, pages 3730–3738, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Brianna Maze, Jocelyn Adams, James A Duncan, Nathan Kalka, Tim Miller,
    Charles Otto, Anil K Jain, W Tyler Niggel, Janet Anderson, Jordan Cheney, et al.
    Iarpa janus benchmark-c: Face dataset and protocol. In 2018 International Conference
    on Biometrics (ICB), pages 158–165\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Ira Kemelmacher-Shlizerman, Steven Seitz, D Miller, and Evan Brossard.
    The megaface benchmark: 1 million faces for recognition at scale. In IEEE Conference
    on Computer Vision and Pattern Recognition, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl
    Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia
    research. arXiv preprint arXiv:1503.01817, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Vineet Kushwaha, Maneet Singh, Richa Singh, Mayank Vatsa, Nalini Ratha,
    and Rama Chellappa. Disguised faces in the wild. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition Workshops, pages 1–9, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Yaniv Taigman, Ming Yang, Marc’Aurelio Ranzato, and Lior Wolf. Deepface:
    Closing the gap to human-level performance in face verification. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 1701–1708,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Deep learning face representation
    from predicting 10,000 classes. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 1891–1898, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Yi Sun, Yuheng Chen, Xiaogang Wang, and Xiaoou Tang. Deep learning face
    representation by joint identification-verification. In Advances in neural information
    processing systems, pages 1988–1996, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Yi Sun, Ding Liang, Xiaogang Wang, and Xiaoou Tang. Deepid3: Face recognition
    with very deep neural networks. arXiv preprint arXiv:1502.00873, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Florian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified
    embedding for face recognition and clustering. In IEEE conference on computer
    vision and pattern recognition, pages 815–823, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Omkar M Parkhi, Andrea Vedaldi, Andrew Zisserman, et al. Deep face recognition.
    In bmvc, volume 1, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax
    loss for convolutional neural networks. In ICML, volume 2, page 7, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative
    feature learning approach for deep face recognition. In European conference on
    computer vision, pages 499–515. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Yi Sun, Xiaogang Wang, and Xiaoou Tang. Sparsifying neural network connections
    for face recognition. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 4856–4864, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Xiao Zhang, Zhiyuan Fang, Yandong Wen, Zhifeng Li, and Yu Qiao. Range
    loss for deep face recognition with long-tailed training data. In IEEE International
    Conference on Computer Vision, pages 5409–5418, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Rajeev Ranjan, Carlos D Castillo, and Rama Chellappa. L2-constrained
    softmax loss for discriminative face verification. arXiv preprint arXiv:1703.09507,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Yu Liu, Hongyang Li, and Xiaogang Wang. Rethinking feature discrimination
    and polymerization for large-scale recognition. preprint, arXiv:1710.00870, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Weiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song.
    Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of
    the IEEE conference on computer vision and pattern recognition, pages 212–220,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. Additive margin softmax
    for face verification. IEEE Signal Processing Letters, 25(7):926–930, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou,
    Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 5265–5274, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface:
    Additive angular margin loss for deep face recognition. In IEEE Conference on
    Computer Vision and Pattern Recognition, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Yutong Zheng, Dipan K Pal, and Marios Savvides. Ring loss: Convex feature
    normalization for face recognition. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 5089–5097, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Xiao Zhang, Rui Zhao, Yu Qiao, Xiaogang Wang, and Hongsheng Li. Adacos:
    Adaptively scaling cosine logits for effectively learning deep face representations.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 10823–10832, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Xiao Zhang, Rui Zhao, Junjie Yan, Mengya Gao, Yu Qiao, Xiaogang Wang,
    and Hongsheng Li. P2sgrad: Refined gradients for optimizing deep face models.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 9906–9914, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Yueqi Duan, Jiwen Lu, and Jie Zhou. Uniformface: Learning deep equidistributed
    representation for face recognition. In IEEE Conference on Computer Vision and
    Pattern Recognition, pages 3415–3424, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Hao Liu, Xiangyu Zhu, Zhen Lei, and Stan Z Li. Adaptiveface: Adaptive
    margin and sampling for face recognition. In IEEE Conference on Computer Vision
    and Pattern Recognition, pages 11947–11956, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive
    growing of gans for improved quality, stability, and variation. arXiv preprint
    arXiv:1710.10196, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Andrew K Hrechak and James A McHugh. Automated fingerprint recognition
    using structural matching. Pattern Recognition, 23(8):893–904, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Chih-Jen Lee and Sheng-De Wang. Fingerprint feature extraction using
    gabor filters. Electronics Letters, 35(4):288–290, 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Marius Tico, Pauli Kuosmanen, and Jukka Saarinen. Wavelet domain features
    for fingerprint recognition. Electronics Letters, 37(1):21–22, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Fvc fingerprint dataset. http://bias.csr.unibo.it/fvc2002/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Casia fingerprint dataset. http://biometrics.idealtest.org/dbDetailForUser.do?id=7.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Michael D Garris and R Michael McCabe. Fingerprint minutiae from latent
    and matching tenprint images. In Tenprint Images”, National Institute of Standards
    and Technology. Citeseer, 2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Luke Nicholas Darlow and Benjamin Rosman. Fingerprint minutiae extraction
    using deep learning. In 2017 IEEE International Joint Conference on Biometrics
    (IJCB), pages 22–30\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Yao Tang, Fei Gao, Jufu Feng, and Yuhang Liu. Fingernet: An unified deep
    network for fingerprint minutiae extraction. In 2017 IEEE International Joint
    Conference on Biometrics (IJCB), pages 108–116\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Chenhao Lin and Ajay Kumar. Contactless and partial 3d fingerprint recognition
    using multi-view deep representation. Pattern Recognition, 83:314–327, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Raid Omar, Tingting Han, Saadoon AM Al-Sumaidaee, and Taolue Chen. Deep
    finger texture learning for verifying people. IET Biometrics, 8(1):40–48, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Shervin Minaee, Elham Azimi, and Amirali Abdolrashidi. Fingernet: Pushing
    the limits of fingerprint recognition using convolutional neural network. arXiv
    preprint arXiv:1907.12956, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Chenhao Lin and Ajay Kumar. Multi-siamese networks to accurately match
    contactless to contact-based fingerprint images. In International Joint Conference
    on Biometrics (IJCB), pages 277–285\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Branka Stojanović, Oge Marques, Aleksandar Nešković, and Snežana Puzović.
    Fingerprint roi segmentation based on deep learning. In 2016 24th Telecommunications
    Forum (TELFOR), pages 1–4. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Yanming Zhu, Xuefei Yin, Xiuping Jia, and Jiankun Hu. Latent fingerprint
    segmentation based on convolutional neural networks. In Workshop on Information
    Forensics and Security (WIFS), pages 1–6\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Soowoong Kim, Bogun Park, Bong Seop Song, and Seungjoon Yang. Deep belief
    network based statistical feature learning for fingerprint liveness detection.
    Pattern Recognition Letters, 77:58–65, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Rodrigo Frassetto Nogueira, Roberto de Alencar Lotufo, and Rubens Campos
    Machado. Fingerprint liveness detection using convolutional neural networks. IEEE
    transactions on information forensics and security, 11(6):1206–1213, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Shervin Minaee and Amirali Abdolrashidi. Finger-gan: Generating realistic
    fingerprint images using connectivity imposed gan. preprint, arXiv:1812.10482,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Elham Tabassi, Tarang Chugh, Debayan Deb, and Anil K Jain. Altered fingerprints:
    Detection and localization. In 2018 IEEE 9th International Conference on Biometrics
    Theory, Applications and Systems (BTAS), pages 1–9\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] John G Daugman. High confidence visual recognition of persons by a test
    of statistical independence. IEEE transactions on pattern analysis and machine
    intelligence, 15(11):1148–1161, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Richard Wildes, Jane Asmuth, Gilbert Green, Stephen Hsu, Raymond Kolczynski,
    James Matey, and Sterling McBride. A system for automated iris recognition. In
    Workshop on Applications of Computer Vision, pages 121–128. IEEE, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Gerald O Williams. Iris recognition technology. In 1996 30th Annual International
    Carnahan Conference on Security Technology, pages 46–59\. IEEE, 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Shervin Minaee, AmirAli Abdolrashidi, and Yao Wang. Iris recognition
    using scattering transform and textural features. In signal processing and signal
    processing education workshop, pages 37–42\. IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Zijing Zhao and Ajay Kumar. Towards more accurate iris recognition using
    deeply learned spatially corresponding features. In IEEE International Conference
    on Computer Vision, pages 3809–3818, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] http://biometrics.idealtest.org/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Casia iris dataset. http://biometrics.idealtest.org/findTotalDbByMode.do?mode=Iris.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Ubiris iris dataset. http://iris.di.ubi.pt/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Iit iris dataset. https://www4.comp.polyu.edu.hk/~csajaykr/IITD/Database_Iris.htm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Lg iris. https://cvrl.nd.edu/projects/data/.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Maria De Marsico, Michele Nappi, Daniel Riccio, and Harry Wechsler. Mobile
    iris challenge evaluation (miche)-i, biometric iris dataset and protocols. Pattern
    Recognition Letters, 57:17–23, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Shervin Minaee, Amirali Abdolrashidiy, and Yao Wang. An experimental
    study of deep convolutional features for iris recognition. In signal processing
    in medicine and biology symposium, pages 1–6\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Abhishek Gangwar and Akanksha Joshi. Deepirisnet: Deep iris representation
    with applications in iris recognition and cross-sensor iris recognition. In 2016
    IEEE International Conference on Image Processing (ICIP), pages 2301–2305\. IEEE,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Mohtashim Baqar, Azfar Ghani, Azeem Aftab, Saira Arbab, and Sajid Yasin.
    Deep belief networks for iris recognition based on contour detection. In 2016
    International Conference on Open Source Systems & Technologies (ICOSST), pages
    72–77\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Maram Alaslani and Lamiaa Elrefaei. Convolutional neural network-based
    feature extraction for iris recognition. Int. J. Comp. Sci. Info. Tech., 10:65–78,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Hrishikesh Menon and Anirban Mukherjee. Iris biometrics using deep convolutional
    networks. In 2018 IEEE International Instrumentation and Measurement Technology
    Conference (I2MTC), pages 1–5\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Heinz Hofbauer, Ehsaneddin Jalilian, and Andreas Uhl. Exploiting superior
    cnn-based iris segmentation for better recognition accuracy. Pattern Recognition
    Letters, 120:17–23, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Sohaib Ahmad and Benjamin Fuller. Thirdeye: Triplet based iris recognition
    without normalization. arXiv preprint arXiv:1907.06147, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Shervin Minaee and Amirali Abdolrashidi. Deepiris: Iris recognition using
    a deep learning approach. arXiv preprint arXiv:1907.09380, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Shervin Minaee and Amirali Abdolrashidi. Iris-gan: Learning to generate
    realistic iris images using convolutional gan. arXiv preprint arXiv:1812.04822,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Min Beom Lee, Yu Hwan Kim, and Kang Ryoung Park. Conditional generative
    adversarial network-based data augmentation for enhancement of iris recognition
    accuracy. IEEE Access, 7:122134–122152, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] David Zhang, Wangmeng Zuo, and Feng Yue. A comparative study of palmprint
    recognition algorithms. ACM computing surveys (CSUR), 44(1):2, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Jun Chen, Changshui Zhang, and Gang Rong. Palmprint recognition using
    crease. In Proceedings 2001 International Conference on Image Processing (Cat.
    No. 01CH37205), volume 3, pages 234–237\. IEEE, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Tee Connie, Andrew Teoh, Michael Goh, and David Ngo. Palmprint recognition
    with pca and ica. In Proc. Image and Vision Computing, New Zealand, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Wenxin Li, David Zhang, and Zhuoqun Xu. Palmprint identification by fourier
    transform. International Journal of Pattern Recognition and Artificial Intelligence,
    16(04):417–432, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Xiang-Qian Wu, Kuan-Quan Wang, and David Zhang. Wavelet based palm print
    recognition. In Proceedings. International Conference on Machine Learning and
    Cybernetics, volume 3, pages 1253–1257\. IEEE, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Wei Shu and David Zhang. Palmprint verification: an implementation of
    biometric technology. In Fourteenth International Conference on Pattern Recognition,
    volume 1, pages 219–221\. IEEE, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Shervin Minaee and Yao Wang. Palmprint recognition using deep scattering
    network. In International Symposium on Circuits and Systems (ISCAS). IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Casia palmprint dataset. http://www.cbsr.ia.ac.cn/english/Palmprint\%20Databases.asp.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Iit palmprint dataset. https://www4.comp.polyu.edu.hk/~csajaykr/IITD/Database_Palm.htm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Zhao Dandan Pan Xin, Pan Xin, Luo Xiaoling, and Gao Xiaojing. Palmprint
    recognition based on deep learning. 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Djamel Samai, Khaled Bensid, Abdallah Meraoumia, Abdelmalik Taleb-Ahmed,
    and Mouldi Bedda. 2d and 3d palmprint recognition using deep learning method.
    In IInternational Conference on Pattern Analysis and Intelligent Systems, pages
    1–6\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Dexing Zhong, Yuan Yang, and Xuefeng Du. Palmprint recognition using
    siamese network. In Chinese Conference on Biometric Recognition, pages 48–55.
    Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Mahdieh Izadpanahkakhk, Seyyed Razavi, Mehran Gorjikolaie, Seyyed Zahiri,
    and Aurelio Uncini. Deep region of interest and feature extraction models for
    palmprint verification using convolutional neural networks transfer learning.
    Applied Sciences, 8(7), 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Huikai Shao and Dexing Zhong. Few-shot palmprint recognition via graph
    neural networks. Electronics Letters, 55(16):890–892, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Huikai Shao, Dexing Zhong, and Xuefeng Du. Efficient deep palmprint recognition
    via distilled hashing coding. In IEEE Conference on Computer Vision and Pattern
    Recognition Workshops, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Huikai Shao, Dexing Zhong, and Xuefeng Du. Cross-domain palmprint recognition
    based on transfer convolutional autoencoder. In International Conference on Image
    Processing, pages 1153–1157\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Shuping Zhao, Bob Zhang, and CL Philip Chen. Joint deep convolutional
    feature representation for hyperspectral palmprint recognition. Information Sciences,
    489:167–181, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Zhihuai Xie, Zhenhua Guo, and Chengshan Qian. Palmprint gender classification
    by convolutional neural network. IET Computer Vision, 12(4):476–483, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Celia Cintas, Claudio Delrieux, Pablo Navarro, Mirsha Quinto-Sanchez,
    Bruno Pazos, and Rolando Gonzalez-Jose. Automatic ear detection and segmentation
    over partially occluded profile face images. Journal of Computer Science & Technology,
    19, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Žiga Emeršič, Dejan Štepec, Vitomir Štruc, and Peter Peer. Training convolutional
    neural networks with limited training data for ear recognition in the wild. arXiv
    preprint arXiv:1711.09952, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Imran Naseem, Roberto Togneri, and Mohammed Bennamoun. Sparse representation
    for ear biometrics. In International Symposium on Visual Computing, pages 336–345.
    Springer, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Awe ear dataset. http://awe.fri.uni-lj.si/home.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Fevziye Irem Eyiokur, Dogucan Yaman, and Hazım Kemal Ekenel. Domain adaptation
    for ear recognition using deep convolutional neural networks. iet Biometrics,
    7(3):199–206, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Ustb ear dataset. http://www1.ustb.edu.cn/resb/en/visit/visit.htm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Ziga Emersic, Dejan Stepec, Vitomir Struc, Peter Peer, Anjith George,
    Adii Ahmad, Elshibani Omar, Terranee E Boult, Reza Safdaii, Yuxiang Zhou, et al.
    The unconstrained ear recognition challenge. In 2017 IEEE International Joint
    Conference on Biometrics (IJCB), pages 715–724\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] E Gonzalez-Sanchez. Biometria de la oreja. PhD thesis, Ph. D. thesis,
    Universidad de Las Palmas de Gran Canaria, Spain, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Carreira Perpinan. Compression neural networks for feature extraction:
    Application to human recognition from ear images. Master’s thesis, Faculty of
    Informatics, Technical University of Madrid, Spain, 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Dariusz Frejlichowski and Natalia Tyszkiewicz. The west pomeranian university
    of technology ear database–a tool for testing biometric algorithms. In International
    Conference Image Analysis and Recognition, pages 227–234\. Springer, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Jie Zhang, Wen Yu, Xudong Yang, and Fang Deng. Few-shot learning for
    ear recognition. In Proceedings of the 2019 International Conference on Image,
    Video and Signal Processing, pages 50–54\. ACM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Samuel Dodge, Jinane Mounsef, and Lina Karam. Unconstrained ear recognition
    using deep neural networks. IET Biometrics, 7(3):207–214, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Ziga Emersic, Dejan Stepec, Vitomir Struc, and Peter Peer. Training convolutional
    neural networks with limited training data for ear recognition in the wild. In
    International Conference on Automatic Face & Gesture Recognition, pages 987–994\.
    IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Žiga Emeršič, Nil Oleart Playà, Vitomir Štruc, and Peter Peer. Towards
    accessories-aware ear recognition. In 2018 IEEE International Work Conference
    on Bioinspired Intelligence (IWOBI), pages 1–8\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Harsh Sinha, Raunak Manekar, Yash Sinha, and Pawan K Ajmera. Convolutional
    neural network-based human identification using outer ear images. In Soft Computing
    for Problem Solving, pages 707–719. Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Ibrahim Omara, Xiaohe Wu, Hongzhi Zhang, Yong Du, and Wangmeng Zuo. Learning
    pairwise svm on hierarchical deep features for ear recognition. IET Biometrics,
    7(6):557–566, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Mohammad Haghighat, Mohamed Abdel-Mottaleb, and Wadee Alhalabi. Discriminant
    correlation analysis: Real-time feature level fusion for multimodal biometric
    recognition. IEEE Transactions on Information Forensics and Security, 11(9):1984–1996,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Carl Brunner, Andreas Fischer, Klaus Luig, and Thorsten Thies. Pairwise
    support vector machines and their application to large scale problems. Journal
    of Machine Learning Research, 13(Aug):2279–2292, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Earnest E Hansley, Maurício Pamplona Segundo, and Sudeep Sarkar. Employing
    fusion of learned and handcrafted features for unconstrained ear recognition.
    IET Biometrics, 7(3):215–223, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Clark D Shaver and John M Acken. A brief review of speaker recognition
    technology. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] David B Yoffie, Liang Wu, Jodie Sweitzer, Denzil Eden, and Karan Ahuja.
    Voice war: Hey google vs. alexa vs. siri. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Imran Naseem, Roberto Togneri, and Mohammed Bennamoun. Sparse representation
    for speaker identification. In 2010 20th International Conference on Pattern Recognition,
    pages 4460–4463\. IEEE, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] S. Furui. Speaker recognition. Scholarpedia, 3(4):3715, 2008. revision
    #64889.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Daniel Garcia-Romero, David Snyder, Gregory Sell, Daniel Povey, and Alan
    McCree. Speaker diarization using deep neural network embeddings. In International
    Conference on Acoustics, Speech and Signal Processing, pages 4930–4934\. IEEE,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Alvin F Martin and Mark A Przybocki. The nist speaker recognition evaluations:
    1996-2001. In 2001: A Speaker Odyssey-The Speaker Recognition Workshop, 2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] The 2010 nist speaker recognition evaluation. 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] The 2018 nist speaker recognition evaluation. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] The 2016 nist speaker recognition evaluation. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Mitchell McLaren, Luciana Ferrer, Diego Castan, and Aaron Lawson. The
    speakers in the wild (sitw) speaker recognition database. In Interspeech, pages
    818–822, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Voxceleb: a large-scale
    speaker identification dataset. arXiv preprint arXiv:1706.08612, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Joon Son Chung, Arsha Nagrani, and Andrew Zisserman. Voxceleb2: Deep
    speaker recognition. arXiv preprint arXiv:1806.05622, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] J Godfrey and E Holliman. Switchboard-1 release 2: Linguistic data consortium.
    SWITCHBOARD: A User’s Manual, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Christopher Cieri, David Miller, and Kevin Walker. The fisher corpus:
    a resource for the next generations of speech-to-text. In LREC, volume 4, pages
    69–71, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. Librispeech:
    an asr corpus based on public domain audio books. In 2015 IEEE International Conference
    on Acoustics, Speech and Signal Processing (ICASSP), pages 5206–5210\. IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Victor Zue, Stephanie Seneff, and James Glass. Speech database development
    at mit: Timit and beyond. Speech communication, 9(4):351–356, 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Mirco Ravanelli and Yoshua Bengio. Learning speaker representations with
    mutual information. arXiv preprint arXiv:1812.00271, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Najim Dehak, Patrick J Kenny, Réda Dehak, Pierre Dumouchel, and Pierre
    Ouellet. Front-end factor analysis for speaker verification. IEEE Transactions
    on Audio, Speech, and Language Processing, 19(4):788–798, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Yun Lei, Nicolas Scheffer, Luciana Ferrer, and Mitchell McLaren. A novel
    scheme for speaker recognition using a phonetically-aware deep neural network.
    In International Conference on Acoustics, Speech and Signal Processing, pages
    1695–1699\. IEEE, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Ehsan Variani, Xin Lei, Erik McDermott, Ignacio Lopez Moreno, and Javier
    Gonzalez-Dominguez. Deep neural networks for small footprint text-dependent speaker
    verification. In International Conference on Acoustics, Speech and Signal Processing
    (ICASSP). IEEE, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] David Snyder, Daniel Garcia-Romero, Gregory Sell, Daniel Povey, and Sanjeev
    Khudanpur. X-vectors: Robust dnn embeddings for speaker recognition. In International
    Conference on Acoustics, Speech and Signal Processing. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Georg Heigold, Ignacio Moreno, Samy Bengio, and Noam Shazeer. End-to-end
    text-dependent speaker verification. In International Conference on Acoustics,
    Speech and Signal Processing (ICASSP). IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Shi-Xiong Zhang, Zhuo Chen, Yong Zhao, Jinyu Li, and Yifan Gong. End-to-end
    attention based text-dependent speaker verification. In Spoken Language Technology
    Workshop (SLT), pages 171–178. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Chunlei Zhang and Kazuhito Koishida. End-to-end text-independent speaker
    verification with triplet loss on short utterances. In Interspeech, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Li Wan, Quan Wang, Alan Papir, and Ignacio Lopez Moreno. Generalized
    end-to-end loss for speaker verification. In International Conference on Acoustics,
    Speech and Signal Processing. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Nam Le and Jean-Marc Odobez. Robust and discriminative speaker embedding
    via intra-class distance variance regularization. In Interspeech, pages 2257–2261,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Gautam Bhattacharya, Jahangir Alam, and Patrick Kenny. Deep speaker recognition:
    Modular or monolithic? Proc. Interspeech 2019, pages 1143–1147, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] KR Radhika, MK Venkatesha, and GN Sekhar. An approach for on-line signature
    authentication using zernike moments. Pattern Recognition Letters, 32(5), 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Amir Soleimani, Babak N Araabi, and Kazim Fouladi. Deep multitask metric
    learning for offline signature verification. Pattern Recognition Letters, 80:84–90,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Donato Impedovo and Giuseppe Pirlo. Automatic signature verification:
    The state of the art. IEEE Transactions on Systems, Man, and Cybernetics, Part
    C (Applications and Reviews), 38(5):609–635, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Sargur Srihari, Aihua Xu, and Meenakshi Kalera. Learning strategies and
    classification methods for off-line signature verification. In Workshop on Frontiers
    in Handwriting Recognition, pages 161–166\. IEEE, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] Icdar svc 2009. http://tc11.cvc.uab.es/datasets/SigComp2009_1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Dit-Yan Yeung, Hong Chang, Yimin Xiong, Susan George, Ramanujan Kashi,
    Takashi Matsumoto, and Gerhard Rigoll. Svc2004: First international signature
    verification competition. In International conference on biometric authentication,
    pages 16–22\. Springer, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Francisco Vargas, M Ferrer, Carlos Travieso, and J Alonso. Off-line handwritten
    signature gpds-960 corpus. In International Conference on Document Analysis and
    Recognition, volume 2, pages 764–768\. IEEE, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Bernardete Ribeiro, Ivo Gonçalves, Sérgio Santos, and Alexander Kovacec.
    Deep learning networks for off-line handwritten signature recognition. In Iberoamerican
    Congress on Pattern Recognition. Springer, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] David H Ackley, Geoffrey E Hinton, and Terrence J Sejnowski. A learning
    algorithm for boltzmann machines. Cognitive science, 9(1):147–169, 1985.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Hannes Rantzsch, Haojin Yang, and Christoph Meinel. Signature embedding:
    Writer independent offline signature verification with deep metric learning. In
    International symposium on visual computing, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Zehua Zhang, Xiangqian Liu, and Yan Cui. Multi-phase offline signature
    verification system using deep convolutional generative adversarial networks.
    In 2016 9th international Symposium on Computational Intelligence and Design,
    volume 2, pages 103–107\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Luiz G Hafemann, Robert Sabourin, and Luiz S Oliveira. Learning features
    for offline handwritten signature verification using deep convolutional neural
    networks. Pattern Recognition, 70:163–176, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Siyue Wang and Shijie Jia. Signature handwriting identification based
    on generative adversarial networks. In Journal of Physics: Conference Series,
    number 4, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Ruben Tolosana, Ruben Vera-Rodriguez, Julian Fierrez, and Javier Ortega-Garcia.
    Exploring recurrent neural networks for on-line handwritten signature biometrics.
    IEEE Access, 6:5128–5138, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Cheng Zhang, Wu Liu, Huadong Ma, and Huiyuan Fu. Siamese neural network
    based gait recognition for human identification. In International Conference on
    Acoustics, Speech and Signal Processing. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Munif Alotaibi and Ausif Mahmood. Improved gait recognition based on
    specialized deep convolutional neural network. Computer Vision and Image Understanding,
    164:103–110, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Thomas Wolf, Mohammadreza Babaee, and Gerhard Rigoll. Multi-view gait
    recognition using 3d convolutional neural networks. In International Conference
    on Image Processing, pages 4165–4169\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Xin Chen, Jian Weng, Wei Lu, and Jiaming Xu. Multi-gait recognition based
    on attribute discovery. IEEE transactions on pattern analysis and machine intelligence,
    40(7):1697–1710, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Casia gait database. http://www.cbsr.ia.ac.cn/users/szheng/?page_id=71.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Shuai Zheng, Junge Zhang, Kaiqi Huang, Ran He, and Tieniu Tan. Robust
    view transformation model for gait recognition. In International Conference on
    Image Processing. IEEE, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Osaka gait database. http://www.am.sanken.osaka-u.ac.jp/BiometricDB/GaitTM.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Y. Makihara, H. Mannami, A. Tsuji, M.A. Hossain, K. Sugiura, A. Mori,
    and Y. Yagi. The ou-isir gait database comprising the treadmill dataset. IPSJ
    Trans. on Computer Vision and Applications, 4:53–62, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Haruyuki Iwama, Mayu Okumura, Yasushi Makihara, and Yasushi Yagi. The
    ou-isir gait database comprising the large population dataset and performance
    evaluation of gait recognition. IEEE Transactions on Information Forensics and
    Security, 7(5):1511–1521, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Ju Han and Bir Bhanu. Individual recognition using gait energy image.
    IEEE transactions on pattern analysis and machine intelligence, 28(2):316–322,
    2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Francesco Battistone and Alfredo Petrosino. Tglstm: A time based graph
    deep learning approach to gait recognition. Pattern Recognition Letters, 126:132–138,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] Qin Zou, Yanling Wang, Yi Zhao, Qian Wang, Chao Shen, and Qingquan Li.
    Deep learning based gait recognition using smartphones in the wild. arXiv preprint
    arXiv:1811.00338, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] Dong Chen, Xudong Cao, Liwei Wang, Fang Wen, and Jian Sun. Bayesian face
    revisited: A joint formulation. In European conference on computer vision, pages
    566–579. Springer, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] Thomas Berg and Peter N Belhumeur. Tom-vs-pete classifiers and identity-preserving
    alignment for face verification. In BMVC, volume 2, page 7\. Citeseer, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] Jiankang Deng, Yuxiang Zhou, and Stefanos Zafeiriou. Marginal loss for
    deep face recognition. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition Workshops, pages 60–68, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] Bhavesh Pandya, Georgina Cosma, Ali A Alani, Aboozar Taherkhani, Vinayak
    Bharadi, and TM McGinnity. Fingerprint classification using a deep convolutional
    neural network. In 2018 4th International Conference on Information Management
    (ICIM), pages 86–91\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] J Jenkin Winston and D Jude Hemanth. A comprehensive review on iris image-based
    biometric system. Soft Computing, 23(19):9361–9384, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] Punam Kumari and KR Seeja. Periocular biometrics: A survey. Journal of
    King Saud University-Computer and Information Sciences, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] RM Farouk. Iris recognition based on elastic graph matching and gabor
    wavelets. Computer Vision and Image Understanding, 115(8):1239–1244, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] Yuniol Alvarez-Betancourt and Miguel Garcia-Silvente. A keypoints-based
    feature extraction method for iris recognition under variable image quality conditions.
    Knowledge-Based Systems, 92:169–182, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] Zijing Zhao and Ajay Kumar. Accurate periocular recognition under less
    constrained environment using semantics-assisted convolutional neural network.
    IEEE Transactions on Information Forensics and Security, 12(5):1017–1030, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] Imad Rida, Romain Herault, Gian Luca Marcialis, and Gilles Gasso. Palmprint
    recognition with an efficient data driven ensemble classifier. Pattern Recognition
    Letters, 126:21–30, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Xiangyu Xu, Nuoya Xu, Huijie Li, and Qi Zhu. Multi-spectral palmprint
    recognition with deep multi-view representation learning. In International Conference
    on Machine Learning and Intelligent Communications, pages 748–758\. Springer,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] Aurelia Michele, Vincent Colin, and Diaz D Santika. Mobilenet convolutional
    neural networks and support vector machines for palmprint recognition. Procedia
    Computer Science, 157:110–117, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Amin Jalali, Rommohan Mallipeddi, and Minho Lee. Deformation invariant
    and contactless palmprint recognition using convolutional neural network. In Proceedings
    of the 3rd International Conference on Human-Agent Interaction, pages 209–212\.
    ACM, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] Liang Tian and Zhichun Mu. Ear recognition based on deep convolutional
    network. In International Congress on Image and Signal Processing, BioMedical
    Engineering and Informatics, pages 437–441\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] David A Van Leeuwen and Niko Brümmer. An introduction to application-independent
    evaluation of speaker recognition systems. In Speaker classification I, pages
    330–353\. Springer, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] Suwon Shon, Hao Tang, and James Glass. Frame-level speaker embeddings
    for text-independent speaker recognition and analysis of end-to-end model. In
    Spoken Language Technology Workshop (SLT). IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] Weicheng Cai, Jinkun Chen, and Ming Li. Exploring the encoding layer
    and loss function in end-to-end speaker and language recognition system. arXiv
    preprint arXiv:1804.05160, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Koji Okabe, Takafumi Koshinaka, and Koichi Shinoda. Attentive statistics
    pooling for deep speaker embedding. arXiv preprint arXiv:1803.10963, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[265] Mahdi Hajibabaei and Dengxin Dai. Unified hypersphere embedding for speaker
    recognition. arXiv preprint arXiv:1807.08312, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[266] Weidi Xie, Arsha Nagrani, Joon Son Chung, and Andrew Zisserman. Utterance-level
    aggregation for speaker recognition in the wild. In ICASSP 2019-2019 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 5791–5795\.
    IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[267] Luiz G Hafemann, Robert Sabourin, and Luiz S Oliveira. Writer-independent
    feature learning for offline signature verification using deep convolutional neural
    networks. In International Joint Conference on Neural Networks, pages 2576–2583\.
    IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[268] Mustafa Berkay Yılmaz and Berrin Yanıkoğlu. Score level fusion of classifiers
    in off-line signature verification. Information Fusion, 32:109–119, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[269] Victor LF Souza, Adriano LI Oliveira, and Robert Sabourin. A writer-independent
    approach for offline signature verification using deep convolutional neural networks
    features. In Brazilian Conference on Intelligent Systems (BRACIS), pages 212–217\.
    IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[270] Kalaivani Sundararajan and Damon L Woodard. Deep learning for biometrics:
    a survey. ACM Computing Surveys (CSUR), 51(3):65, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[271] Worapan Kusakunniran. Recognizing gaits on spatio-temporal feature domain.
    IEEE Transactions on Information Forensics and Security, 9(9):1416–1423, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[272] Worapan Kusakunniran, Qiang Wu, Jian Zhang, Hongdong Li, and Liang Wang.
    Recognizing gaits across views through correlated motion co-clustering. IEEE Transactions
    on Image Processing, 23(2):696–709, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[273] Daigo Muramatsu, Yasushi Makihara, and Yasushi Yagi. View transformation
    model incorporating quality measures for cross-view gait recognition. IEEE transactions
    on cybernetics, 46(7):1602–1615, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[274] Daigo Muramatsu, Yasushi Makihara, and Yasushi Yagi. Cross-view gait
    recognition by fusion of multiple transformation consistency measures. IET Biometrics,
    4(2):62–73, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[275] Chao Yan, Bailing Zhang, and Frans Coenen. Multi-attributes gait identification
    by convolutional neural networks. In International Congress on Image and Signal
    Processing (CISP), pages 642–647\. IEEE, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[276] Zifeng Wu, Yongzhen Huang, Liang Wang, Xiaogang Wang, and Tieniu Tan.
    A comprehensive study on cross-view gait based human identification with deep
    cnns. IEEE transactions on pattern analysis and machine intelligence, 39(2):209–226,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[277] Chao Li, Xin Min, Shouqian Sun, Wenqian Lin, and Zhichuan Tang. Deepgait:
    a learning deep convolutional representation for view-invariant gait recognition
    using joint bayesian. Applied Sciences, 7(3):210, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[278] Kohei Shiraga, Yasushi Makihara, Daigo Muramatsu, Tomio Echigo, and Yasushi
    Yagi. Geinet: View-invariant gait recognition using a convolutional neural network.
    In 2016 international conference on biometrics (ICB), pages 1–8\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[279] Longlong Jing and Yingli Tian. Self-supervised visual feature learning
    with deep neural networks: A survey. arXiv preprint arXiv:1902.06162, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[280] Arun Ross and Anil K Jain. Multimodal biometrics: an overview. In 2004
    12th European Signal Processing Conference, pages 1221–1224\. IEEE, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[281] Arun Ross and Anil Jain. Information fusion in biometrics. Pattern recognition
    letters, 24(13):2115–2125, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[282] Munawar Hayat, Mohammed Bennamoun, and Senjian An. Deep reconstruction
    models for image set classification. IEEE transactions on pattern analysis and
    machine intelligence, 37(4):713–727, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
