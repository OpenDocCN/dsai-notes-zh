- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:07:45'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:07:45
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1807.10854] A Survey of the Usages of Deep Learning for Natural Language Processing'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1807.10854] 深度学习在自然语言处理中的应用概述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1807.10854](https://ar5iv.labs.arxiv.org/html/1807.10854)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1807.10854](https://ar5iv.labs.arxiv.org/html/1807.10854)
- en: A Survey of the Usages of Deep Learning for Natural Language Processing
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习在自然语言处理中的应用概述
- en: 'Daniel W. Otter, Julian R. Medina, and Jugal K. Kalita Manuscript received
    MONTH DD, YYYY; revised MONTH DD, YYYY. Authors are with the University of Colorado
    at Colorado Springs, 1420 Austin Bluffs Pkwy. Colorado Springs, Colorado 80918
    USA. Corresponding author: Jugal K. Kalita (email: jkalita@uccs.edu). This survey
    was supported in part by National Science Foundation grant numbers IIS-1359275
    and IIS-1659788\. Any opinions, findings, conclusions, and recommendations expressed
    in this material are those of the authors and do not necessarily reflect the views
    of the National Science Foundation.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Daniel W. Otter, Julian R. Medina, 和 Jugal K. Kalita 手稿接收日期：MONTH DD, YYYY；修订日期：MONTH
    DD, YYYY。作者所在机构：科罗拉多大学科罗拉多 Springs分校，1420 Austin Bluffs Pkwy，科罗拉多 Springs，科罗拉多州80918，美国。通讯作者：Jugal
    K. Kalita（电子邮件：jkalita@uccs.edu）。本调查部分由国家科学基金会资助，资助编号为IIS-1359275和IIS-1659788。本文所表达的任何观点、发现、结论和建议均为作者个人观点，不一定反映国家科学基金会的意见。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Over the last several years, the field of natural language processing has been
    propelled forward by an explosion in the use of deep learning models. This survey
    provides a brief introduction to the field and a quick overview of deep learning
    architectures and methods. It then sifts through the plethora of recent studies
    and summarizes a large assortment of relevant contributions. Analyzed research
    areas include several core linguistic processing issues in addition to a number
    of applications of computational linguistics. A discussion of the current state
    of the art is then provided along with recommendations for future research in
    the field.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几年里，自然语言处理领域由于深度学习模型的广泛应用而取得了巨大的进展。本调查简要介绍了该领域，并快速概述了深度学习架构和方法。然后，深入探讨了大量近期研究，并总结了大量相关贡献。分析的研究领域包括几个核心语言处理问题以及计算语言学的一些应用。接着提供了当前最先进技术的讨论，并对未来的研究方向提出了建议。
- en: 'Index Terms:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: deep learning, neural networks, natural language processing, computational linguistics,
    machine learning
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，神经网络，自然语言处理，计算语言学，机器学习
- en: I Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: I 引言
- en: The field of natural language processing (NLP) encompasses a variety of topics
    which involve the computational processing and understanding of human languages.
    Since the 1980s, the field has increasingly relied on data-driven computation
    involving statistics, probability, and machine learning [[1](#bib.bib1), [2](#bib.bib2)].
    Recent increases in computational power and parallelization, harnessed by Graphical
    Processing Units (GPUs) [[3](#bib.bib3), [4](#bib.bib4)], now allow for “deep
    learning”, which utilizes artificial neural networks (ANNs), sometimes with billions
    of trainable parameters [[5](#bib.bib5)]. Additionally, the contemporary availability
    of large datasets, facilitated by sophisticated data collection processes, enables
    the training of such deep architectures [[6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)].
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）领域涵盖了涉及人类语言的计算处理和理解的各种主题。自1980年代以来，该领域越来越依赖数据驱动的计算，包括统计、概率和机器学习 [[1](#bib.bib1),
    [2](#bib.bib2)]。近年来，计算能力和并行化的提升，通过图形处理单元（GPU） [[3](#bib.bib3), [4](#bib.bib4)]，使得“深度学习”成为可能，它利用人工神经网络（ANNs），有时具有数十亿个可训练的参数
    [[5](#bib.bib5)]。此外，现代大数据集的可用性，通过复杂的数据收集过程，支持了这种深度结构的训练 [[6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8)]。
- en: In recent years, researchers and practitioners in NLP have leveraged the power
    of modern ANNs with many propitious results, beginning in large part with the
    pioneering work of Collobert et al. [[9](#bib.bib9)]. In the very recent past,
    the use of deep learning has upsurged considerably [[10](#bib.bib10), [11](#bib.bib11)].
    This has led to significant advances both in core areas of NLP and in areas in
    which it is directly applied to achieve practical and useful objectives. This
    survey provides a brief introduction to both natural language processing and deep
    neural networks, and then presents an extensive discussion on how deep learning
    is being used to solve current problems in NLP. While several other papers and
    books on the topic have been published [[12](#bib.bib12), [10](#bib.bib10)], none
    have extensively covered the state-of-the-art in as many areas within it. Furthermore,
    no other survey has examined not only the applications of deep learning to computational
    linguistics, but also the underlying theory and traditional NLP tasks. In addition
    to the discussion of recent revolutionary developments in the field, this survey
    will be useful to readers who want to familiarize themselves quickly with the
    current state of the art before embarking upon further advanced research and practice.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，NLP（自然语言处理）领域的研究人员和从业者充分利用了现代人工神经网络（ANNs）的强大功能，取得了许多有利的成果，这在很大程度上始于Collobert等人的开创性工作[[9](#bib.bib9)]。在最近，深度学习的使用显著增加了[[10](#bib.bib10),
    [11](#bib.bib11)]。这导致了NLP核心领域及其直接应用于实现实际有用目标的领域的重大进展。本文综述了自然语言处理和深度神经网络的简要介绍，并对深度学习如何用于解决当前NLP问题进行了广泛的讨论。虽然已有几篇其他相关论文和书籍[[12](#bib.bib12),
    [10](#bib.bib10)]，但没有一篇在如此多领域中广泛覆盖最先进的技术。此外，没有其他综述既探讨深度学习在计算语言学中的应用，也涉及其理论基础和传统NLP任务。除了讨论该领域最近的革命性进展外，这篇综述对于那些希望在进行进一步的高级研究和实践之前快速了解当前技术状态的读者将非常有用。
- en: The topics of NLP and AI, including deep learning, are introduced in Section
    [II](#S2 "II Overview of Natural Language Processing and Deep Learning ‣ A Survey
    of the Usages of Deep Learning for Natural Language Processing"). The ways in
    which deep learning has been used to solve problems in core areas of NLP are presented
    in Section [III](#S3 "III Deep Learning in Core Areas of Natural Language Processing
    ‣ A Survey of the Usages of Deep Learning for Natural Language Processing"). The
    section is broken down into several subsections, namely natural language modeling
    ([III-A](#S3.SS1 "III-A Language Modeling and Word Embeddings ‣ III Deep Learning
    in Core Areas of Natural Language Processing ‣ A Survey of the Usages of Deep
    Learning for Natural Language Processing")), morphology ([III-B](#S3.SS2 "III-B
    Morphology ‣ III Deep Learning in Core Areas of Natural Language Processing ‣
    A Survey of the Usages of Deep Learning for Natural Language Processing")), parsing
    ([III-C](#S3.SS3 "III-C Parsing ‣ III Deep Learning in Core Areas of Natural Language
    Processing ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")),
    and semantics ([III-D](#S3.SS4 "III-D Semantics ‣ III Deep Learning in Core Areas
    of Natural Language Processing ‣ A Survey of the Usages of Deep Learning for Natural
    Language Processing")). Applications of deep learning to more practical areas
    are discussed in Section [IV](#S4 "IV Applications of Natural Language Processing
    Using Deep Learning ‣ A Survey of the Usages of Deep Learning for Natural Language
    Processing"). Specifically discussed are information retrieval ([IV-A](#S4.SS1
    "IV-A Information Retrieval ‣ IV Applications of Natural Language Processing Using
    Deep Learning ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")),
    information extraction ([IV-B](#S4.SS2 "IV-B Information Extraction ‣ IV Applications
    of Natural Language Processing Using Deep Learning ‣ A Survey of the Usages of
    Deep Learning for Natural Language Processing")), text classification ([IV-C](#S4.SS3
    "IV-C Text Classification ‣ IV Applications of Natural Language Processing Using
    Deep Learning ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")),
    text generation ([IV-D](#S4.SS4 "IV-D Text Generation ‣ IV Applications of Natural
    Language Processing Using Deep Learning ‣ A Survey of the Usages of Deep Learning
    for Natural Language Processing")), summarization ([IV-E](#S4.SS5 "IV-E Summarization
    ‣ IV Applications of Natural Language Processing Using Deep Learning ‣ A Survey
    of the Usages of Deep Learning for Natural Language Processing")), question answering
    ([IV-F](#S4.SS6 "IV-F Question Answering ‣ IV Applications of Natural Language
    Processing Using Deep Learning ‣ A Survey of the Usages of Deep Learning for Natural
    Language Processing")), and machine translation ([IV-G](#S4.SS7 "IV-G Machine
    Translation ‣ IV Applications of Natural Language Processing Using Deep Learning
    ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")).
    Conclusions are then drawn in Section [V](#S5 "V Conclusions ‣ A Survey of the
    Usages of Deep Learning for Natural Language Processing") with a brief summary
    of the state of the art as well as predictions, suggestions, and other thoughts
    on the future of this dynamically evolving area.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理和人工智能的主题，包括深度学习，在第[II](#S2 "II 自然语言处理和深度学习概述 ‣ 深度学习在自然语言处理中的应用调研")节中介绍。第[III](#S3
    "III 深度学习在自然语言处理核心领域的应用 ‣ 深度学习在自然语言处理中的应用调研")节介绍了深度学习如何被用于解决自然语言处理核心领域中的问题。该节被分为几个小节，即自然语言建模（[III-A](#S3.SS1
    "III-A 语言建模和词嵌入 ‣ III 深度学习在自然语言处理核心领域的应用 ‣ 深度学习在自然语言处理中的应用调研")）、形态学（[III-B](#S3.SS2
    "III-B 形态学 ‣ III 深度学习在自然语言处理核心领域的应用 ‣ 深度学习在自然语言处理中的应用调研")）、句法分析（[III-C](#S3.SS3
    "III-C 句法分析 ‣ III 深度学习在自然语言处理核心领域的应用 ‣ 深度学习在自然语言处理中的应用调研")）和语义学（[III-D](#S3.SS4
    "III-D 语义学 ‣ III 深度学习在自然语言处理核心领域的应用 ‣ 深度学习在自然语言处理中的应用调研")）。第[IV](#S4 "IV 使用深度学习的自然语言处理应用
    ‣ 深度学习在自然语言处理中的应用调研")节讨论了深度学习在更实际领域中的应用。具体讨论了信息检索（[IV-A](#S4.SS1 "IV-A 信息检索 ‣
    IV 使用深度学习的自然语言处理应用 ‣ 深度学习在自然语言处理中的应用调研")）、信息抽取（[IV-B](#S4.SS2 "IV-B 信息抽取 ‣ IV
    使用深度学习的自然语言处理应用 ‣ 深度学习在自然语言处理中的应用调研")）、文本分类（[IV-C](#S4.SS3 "IV-C 文本分类 ‣ IV 使用深度学习的自然语言处理应用
    ‣ 深度学习在自然语言处理中的应用调研")）、文本生成（[IV-D](#S4.SS4 "IV-D 文本生成 ‣ IV 使用深度学习的自然语言处理应用 ‣ 深度学习在自然语言处理中的应用调研")）、摘要生成（[IV-E](#S4.SS5
    "IV-E 摘要生成 ‣ IV 使用深度学习的自然语言处理应用 ‣ 深度学习在自然语言处理中的应用调研")）、问答系统（[IV-F](#S4.SS6 "IV-F
    问答系统 ‣ IV 使用深度学习的自然语言处理应用 ‣ 深度学习在自然语言处理中的应用调研")）和机器翻译（[IV-G](#S4.SS7 "IV-G 机器翻译
    ‣ IV 使用深度学习的自然语言处理应用 ‣ 深度学习在自然语言处理中的应用调研")）。最后，在第[V](#S5 "V 结论 ‣ 深度学习在自然语言处理中的应用调研")节中总结了现状，并对这一动态发展的领域做出预测、建议以及其他相关想法。
- en: II Overview of Natural Language Processing and Deep Learning
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: II 自然语言处理与深度学习概述
- en: In this section, significant issues that draw attention of researchers and practitioners
    are introduced, followed by a brisk explanation of the deep learning architectures
    commonly used in the field.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了引起研究人员和从业者关注的重要问题，并简要解释了在该领域常用的深度学习架构。
- en: (
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: (
- en: (a)
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: (
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: (
- en: (b)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: a) ![Refer to caption](img/0a88cc7ea826945e549a8d6e50381f6d.png) b) ![Refer
    to caption](img/2ff7a1b3f73b15f70eb569d45c82b079.png)
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: a) ![参见标题](img/0a88cc7ea826945e549a8d6e50381f6d.png) b) ![参见标题](img/2ff7a1b3f73b15f70eb569d45c82b079.png)
- en: 'Figure 1: Encoder–Decoder Architectures. While there are multiple options of
    encoders and decoders available, RNN variants are a common choice for each, particularly
    the latter. Such a network is shown in (a). Attention mechanisms, such as that
    present in (b), allow the decoder to determine which portions of the encoding
    are most relevant at each output step.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：编码器-解码器架构。尽管有多种编码器和解码器的选项可用，但RNN变体是每种选项的常见选择，尤其是后者。这样的网络在(a)中显示。注意机制，例如(b)中存在的机制，使解码器能够确定在每个输出步骤中哪些编码部分最相关。
- en: II-A Natural Language Processing
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-A 自然语言处理
- en: 'The field of natural language processing, also known as computational linguistics,
    involves the engineering of computational models and processes to solve practical
    problems in understanding human languages. These solutions are used to build useful
    software. Work in NLP can be divided into two broad sub-areas: core areas and
    applications, although it is sometimes difficult to distinguish clearly to which
    areas issues belong. The core areas address fundamental problems such as language
    modeling, which underscores quantifying associations among naturally occurring
    words; morphological processing, dealing with segmentation of meaningful components
    of words and identifying the true parts of speech of words as used; syntactic
    processing, or parsing, which builds sentence diagrams as possible precursors
    to semantic processing; and semantic processing, which attempts to distill meaning
    of words, phrases, and higher level components in text. The application areas
    involve topics such as extraction of useful information (e.g. named entities and
    relations), translation of text between and among languages, summarization of
    written works, automatic answering of questions by inferring answers, and classification
    and clustering of documents. Often one needs to handle one or more of the core
    issues successfully and apply those ideas and procedures to solve practical problems.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）领域，也称为计算语言学，涉及计算模型和过程的工程，以解决理解人类语言中的实际问题。这些解决方案用于构建有用的软件。NLP的工作可以分为两个广泛的子领域：核心领域和应用领域，尽管有时很难明确区分问题属于哪个领域。核心领域解决基础问题，例如语言建模，它强调量化自然出现的单词之间的关联；形态处理，处理单词有意义组成部分的分割以及确定单词的真实词性；句法处理或解析，构建句子图作为语义处理的可能前奏；以及语义处理，试图提炼文本中单词、短语和更高层次组件的含义。应用领域涉及诸如提取有用信息（例如命名实体和关系）、文本翻译、书面作品总结、通过推断答案自动回答问题以及文档分类和聚类等主题。通常需要成功处理一个或多个核心问题，并将这些想法和程序应用于解决实际问题。
- en: Currently, NLP is primarily a data-driven field using statistical and probabilistic
    computations along with machine learning. In the past, machine learning approaches
    such as naïve Bayes, $k$-nearest neighbors, hidden Markov models, conditional
    random fields, decision trees, random forests, and support vector machines were
    widely used. However, during the past several years, there has been a wholesale
    transformation, and these approaches have been entirely replaced, or at least
    enhanced, by neural models, discussed next.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，自然语言处理（NLP）主要是一个数据驱动的领域，使用统计和概率计算以及机器学习。过去，诸如朴素贝叶斯、$k$-最近邻、隐马尔可夫模型、条件随机场、决策树、随机森林和支持向量机等机器学习方法被广泛使用。然而，在过去几年中，这些方法经历了彻底的转变，已经完全被神经模型取代，或者至少得到了增强，接下来将讨论这些模型。
- en: II-B Neural Networks and Deep Learning
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: II-B 神经网络与深度学习
- en: (
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: (
- en: (a)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: (a)
- en: (
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (
- en: (b)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: (b)
- en: (
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (
- en: (c)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (c)
- en: a) ![Refer to caption](img/6990365df83e748f9fd064187360059a.png) b) ![Refer
    to caption](img/4c8efb077ce38a811a97ec66d9ea8128.png) c) ![Refer to caption](img/d04268d21b9b7ffd5055362e964af689.png)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: a) ![参见标题](img/6990365df83e748f9fd064187360059a.png) b) ![参见标题](img/4c8efb077ce38a811a97ec66d9ea8128.png)
    c) ![参见标题](img/d04268d21b9b7ffd5055362e964af689.png)
- en: 'Figure 2: Transformer Model. (a) shows a transformer with four ”encoders” followed
    by four ”decoders”, all following a ”positional encoder”. (b) shows the inner
    workings of each ”encoder”, which contains a self-attention layer followed by
    a feed forward layer. (c) shows the inner workings of each ”decoder”, which contains
    a self-attention layer followed by an attentional encoder-decoder layer and then
    a feed forward layer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：变换器模型。(a) 显示了一个带有四个“编码器”的变换器，后面跟着四个“解码器”，所有这些都跟随一个“位置编码器”。(b) 显示了每个“编码器”的内部工作原理，其中包含一个自注意力层，后面跟着一个前馈层。(c)
    显示了每个“解码器”的内部工作原理，其中包含一个自注意力层，后面跟着一个注意力编码器-解码器层，然后是一个前馈层。
- en: Neural networks are composed of interconnected nodes, or neurons, each receiving
    some number of inputs and supplying an output. Each of the nodes in the output
    layers perform weighted sum computation on the values they receive from the input
    nodes and then generate outputs using simple nonlinear transformation functions
    on these summations. Corrections to the weights are made in response to individual
    errors or losses the networks exhibit at the output nodes. Such corrections are
    usually made in modern networks using stochastic gradient descent, considering
    the derivatives of errors at the nodes, an approach called back-propagation [[13](#bib.bib13)].
    The main factors that distinguish different types of networks from each other
    are how the nodes are connected and the number of layers. Basic networks in which
    all nodes can be organized into sequential layers, with every node receiving inputs
    only from nodes in earlier layers, are known as feedforward neural networks (FFNNs).
    While there is no clear consensus on exactly what defines a deep neural network
    (DNN), generally networks with multiple hidden layers are considered deep and
    those with many layers are considered very deep [[7](#bib.bib7)].
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络由互连的节点或神经元组成，每个节点接收一定数量的输入并提供输出。输出层中的每个节点对从输入节点接收到的值执行加权和计算，然后使用简单的非线性变换函数对这些求和结果生成输出。根据网络在输出节点处表现出的个别错误或损失，权重会进行修正。现代网络通常使用随机梯度下降来进行这些修正，考虑节点处的误差导数，这种方法称为反向传播[[13](#bib.bib13)]。不同类型网络的主要区别在于节点的连接方式和层数。所有节点可以组织成顺序层，每个节点仅从早期层的节点接收输入的基本网络称为前馈神经网络（FFNNs）。虽然对深度神经网络（DNN）的定义没有明确的共识，但一般认为具有多个隐藏层的网络是深度的，层数多的网络被认为是非常深的[[7](#bib.bib7)]。
- en: II-B1 Convolutional Neural Networks
  id: totrans-37
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B1 卷积神经网络
- en: Convolutional neural networks (CNNs) [[14](#bib.bib14), [15](#bib.bib15)], built
    upon Fukashima’s neocognitron [[16](#bib.bib16), [17](#bib.bib17)], derive the
    name from the convolution operation in mathematics and signal processing. CNNs
    use functions, known as filters, allowing for simultaneous analysis of different
    features in the data [[18](#bib.bib18), [19](#bib.bib19)]. CNNs are used extensively
    in image and video processing, as well as speech and NLP [[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)]. Often, it is not important precisely where
    certain features occur, but rather whether or not they appear in particular localities.
    Therefore, pooling operations, can be used to minimize the size of feature maps
    (the outputs of the convolutional filters). The sizes of such pools are generally
    small in order to prevent the loss of too much precision.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）[[14](#bib.bib14), [15](#bib.bib15)]，建立在福岛的神经认知网络[[16](#bib.bib16),
    [17](#bib.bib17)]的基础上，得名于数学和信号处理中的卷积操作。CNNs 使用被称为滤波器的函数，从而允许同时分析数据中的不同特征[[18](#bib.bib18),
    [19](#bib.bib19)]。CNNs 被广泛应用于图像和视频处理、以及语音和自然语言处理[[20](#bib.bib20), [21](#bib.bib21),
    [22](#bib.bib22), [23](#bib.bib23)]。通常，某些特征出现的位置并不重要，而是这些特征是否出现在特定的局部区域。因此，池化操作可以用来最小化特征图的大小（卷积滤波器的输出）。这些池的大小通常较小，以防止过多精度的丧失。
- en: II-B2 Recursive Neural Networks
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B2 递归神经网络
- en: Much like CNNs, recursive networks [[24](#bib.bib24), [25](#bib.bib25)] use
    a form of weight sharing to minimize training. However, whereas CNNs share weights
    horizontally (within a layer), recursive nets share weights vertically (between
    layers). This is particularly appealing, as it allows for easy modeling of structures
    such as parse trees. In recursive networks, a single tensor (or a generalized
    matrix) of weights can be used at a low level in the tree, and then used recursively
    at successively higher levels [[26](#bib.bib26)].
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 CNNs，递归网络 [[24](#bib.bib24), [25](#bib.bib25)] 使用一种权重共享的形式来最小化训练。然而，尽管 CNNs
    在同一层内水平共享权重，递归网络在不同层之间垂直共享权重。这一点特别吸引人，因为它允许对诸如解析树之类的结构进行简单建模。在递归网络中，可以在树的低层使用单一的张量（或广义矩阵）作为权重，然后在逐渐更高的层次上递归使用
    [[26](#bib.bib26)]。
- en: II-B3 Recurrent Neural Networks and Long Short-Term Memory Networks
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B3 递归神经网络和长短期记忆网络
- en: A type of recursive neural network that has been used heavily is the recurrent
    neural network (RNN) [[27](#bib.bib27), [28](#bib.bib28)]. Since much of NLP is
    dependent on the order of words or other elements such as phonemes or sentences,
    it is useful to have memory of the previous elements when processing new ones
    [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]. Sometimes, backwards dependencies
    exist, i.e., correct processing of some words may depend on words that follow.
    Thus, it is beneficial to look at sentences in both directions, forwards and backwards,
    using two RNN layers, and combine their outputs. This arrangement of RNNs is called
    a bidirectional RNN. It may also lead to a better final representation if there
    is a sequence of RNN layers. This may allow the effect of an input to linger longer
    than a single RNN layer, allowing for longer-term effects. This setup of sequential
    RNN cells is called an RNN stack [[32](#bib.bib32), [33](#bib.bib33)].
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一种被广泛使用的递归神经网络是递归神经网络（RNN） [[27](#bib.bib27), [28](#bib.bib28)]。由于自然语言处理的很多任务依赖于单词或其他元素如音素或句子的顺序，因此在处理新元素时能够记住之前的元素是有用的
    [[29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]。有时，存在反向依赖，即某些单词的正确处理可能依赖于后续的单词。因此，使用两个
    RNN 层来双向查看句子，并结合它们的输出是有益的。这种 RNN 的排列称为双向 RNN。如果有一系列的 RNN 层，这也可能导致更好的最终表示。这种设置允许输入的效果持续比单一的
    RNN 层更长，从而允许长期效应。这种顺序的 RNN 单元设置称为 RNN 堆栈 [[32](#bib.bib32), [33](#bib.bib33)]。
- en: One highly engineered RNN is the long short-term memory (LSTM) network [[34](#bib.bib34),
    [35](#bib.bib35)]. In LSTMs, the recursive nodes are composed of several individual
    neurons connected in a manner designed to retain, forget, or expose specific information.
    Whereas generic RNNs with single neurons feeding back to themselves technically
    have some memory of long passed results, these results are diluted with each successive
    iteration. Oftentimes, it is important to remember information from the distant
    past, while at the same time, other very recent information may not be important.
    By using LSTM blocks, this important information can be retained much longer while
    irrelevant information can be forgotten. A slightly simpler variant of the LSTM,
    called the Gated Recurrent Unit (GRU), has been shown to perform as well as or
    better than standard LSTMs in many tasks [[36](#bib.bib36), [37](#bib.bib37)].
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一种高度工程化的 RNN 是长短期记忆（LSTM）网络 [[34](#bib.bib34), [35](#bib.bib35)]。在 LSTM 中，递归节点由若干个单独的神经元组成，这些神经元以设计的方式连接，以保留、遗忘或暴露特定的信息。虽然通用的
    RNN 具有反馈给自身的单个神经元在技术上对长时间过去的结果有一些记忆，但这些结果在每次迭代中都被稀释。通常，记住远古的过去信息是重要的，同时，其他非常近期的信息可能并不重要。通过使用
    LSTM 块，这些重要的信息可以保留得更久，而无关的信息则可以被遗忘。一种稍微简单的 LSTM 变体，称为门控递归单元（GRU），在许多任务中表现得与标准
    LSTM 一样好甚至更好 [[36](#bib.bib36), [37](#bib.bib37)]。
- en: II-B4 Attention Mechanisms and Transformer
  id: totrans-44
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B4 注意力机制和 Transformer
- en: For tasks such as machine translation, text summarization, or captioning, the
    output is in textual form. Typically, this is done through the use of encoder–decoder
    pairs. An encoding ANN is used to produce a vector of a particular length and
    a decoding ANN is used to return variable length text based on this vector. The
    problem with this scheme, which is shown in Figure [1](#S2.F1 "Figure 1 ‣ II Overview
    of Natural Language Processing and Deep Learning ‣ A Survey of the Usages of Deep
    Learning for Natural Language Processing")(a), is that the RNN is forced to encode
    an entire sequence to a finite length vector, without regards to whether or not
    any of the inputs are more important than others.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于机器翻译、文本摘要或图像描述等任务，输出是以文本形式呈现的。通常，这通过使用编码器-解码器对来完成。一个编码ANN用于生成一个特定长度的向量，而一个解码ANN则用于基于这个向量返回可变长度的文本。该方案的问题如图
    [1](#S2.F1 "图 1 ‣ II 自然语言处理和深度学习概述 ‣ 深度学习在自然语言处理中的应用调查") (a) 所示，是RNN被迫将整个序列编码为一个有限长度的向量，而不考虑任何输入是否比其他输入更重要。
- en: A robust solution to this is that of attention. The first noted use of an attention
    mechanism [[38](#bib.bib38)] used a dense layer for annotated weighting of an
    RNN’s hidden state, allowing the network to learn what to pay attention to in
    accordance with the current hidden state and annotation. Such a mechanism is present
    in Fig. [1](#S2.F1 "Figure 1 ‣ II Overview of Natural Language Processing and
    Deep Learning ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")(b).
    Variants of the mechanism have been introduced, popular ones including convolutional
    [[39](#bib.bib39)], intra-temporal [[40](#bib.bib40)], gated [[41](#bib.bib41)],
    and self-attention [[42](#bib.bib42)]. Self-attention involves providing attention
    to words in the same sentence. For example, during encoding a word in an input
    sentence, it is beneficial to project variable amounts of attention to other words
    in the sentence. During decoding to produce a resulting sentence, it makes sense
    to provide appropriate attention to words that have already been produced. Self-attention
    in particular has become widely used in a state-of-the-art encoder-decoder model
    called Transformer [[42](#bib.bib42)]. The Transformer model, shown in Fig. [2](#S2.F2
    "Figure 2 ‣ II-B Neural Networks and Deep Learning ‣ II Overview of Natural Language
    Processing and Deep Learning ‣ A Survey of the Usages of Deep Learning for Natural
    Language Processing"), has a number of encoders and decoders stacked on top of
    each other, self-attention in each of the encoder and decoder units, and cross-attention
    between the encoders and decoders. It uses multiple instances of attention in
    parallel and eschews the use of recurrences and convolutions. The Transformer
    has become a quintessential component in most state-of-the-art neural networks
    for natural language processing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对此的一个稳健解决方案是注意力机制。注意力机制的首次使用[[38](#bib.bib38)]采用了一个密集层来对RNN的隐藏状态进行加权注释，使得网络能够根据当前隐藏状态和注释学习该关注什么。这样的机制存在于图
    [1](#S2.F1 "图 1 ‣ II 自然语言处理和深度学习概述 ‣ 深度学习在自然语言处理中的应用调查") (b) 中。该机制的变体已经被提出，其中流行的包括卷积
    [[39](#bib.bib39)]、时间内 [[40](#bib.bib40)]、门控 [[41](#bib.bib41)] 和自注意力 [[42](#bib.bib42)]。自注意力涉及对同一句子中的词进行注意。例如，在对输入句子中的一个词进行编码时，将注意力投射到句子中的其他词上是有益的。在解码以生成结果句子时，给予已经产生的词适当的注意力是有意义的。特别是，自注意力已经在一种名为
    Transformer [[42](#bib.bib42)] 的最先进的编码器-解码器模型中得到了广泛使用。Transformer 模型如图 [2](#S2.F2
    "图 2 ‣ II-B 神经网络和深度学习 ‣ II 自然语言处理和深度学习概述 ‣ 深度学习在自然语言处理中的应用调查") 所示，包含了许多堆叠在一起的编码器和解码器单元，每个编码器和解码器单元中都有自注意力，并且编码器和解码器之间有交叉注意力。它并行使用多个注意力实例，并摒弃了递归和卷积的使用。Transformer
    已成为大多数最先进的自然语言处理神经网络中的典型组件。
- en: II-B5 Residual Connections and Dropout
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: II-B5 残差连接和丢弃法
- en: In deep networks, trained via backpropagation [[13](#bib.bib13)], the gradients
    used to correct for error often vanish or explode [[43](#bib.bib43)]. This can
    be mitigated by choosing activation functions, such as the Rectified Linear Unit
    (ReLU) [[44](#bib.bib44)], which do not exhibit regions that are arêtically steep
    or have bosonically small gradients. Also in response to this issue, as well as
    others [[45](#bib.bib45)], residual connections are often used. Such connections
    are simply those that skip layers (usually one). If used in every alternating
    layer, this cuts in half the number of layers through which the gradient must
    backpropagate. Such a network is known as a residual network (ResNet). A number
    of variants exist, including Highway Networks [[46](#bib.bib46)] and DenseNets
    [[47](#bib.bib47)].
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过反向传播训练的深度网络中[[13](#bib.bib13)]，用于纠正误差的梯度经常会消失或爆炸[[43](#bib.bib43)]。这可以通过选择激活函数来缓解，例如修正线性单元（ReLU）[[44](#bib.bib44)]，它们没有陡峭的区域或梯度非常小的区域。此外，为了应对这一问题及其他问题[[45](#bib.bib45)]，通常会使用残差连接。这些连接简单来说就是跳过层（通常是一个）。如果在每个交替层中使用，这将减少梯度必须反向传播的层数一半。这样的网络被称为残差网络（ResNet）。存在许多变体，包括高速公路网络[[46](#bib.bib46)]和密集网络[[47](#bib.bib47)]。
- en: Another important method used in training ANNs is dropout. In dropout, some
    connections and maybe even nodes are deactivated, usually randomly, for each training
    batch (small set of examples), varying which nodes are deactivated each batch.
    This forces the network to distribute its memory across multiple paths, helping
    with generalization and lessening the likelihood of overfitting to the training
    data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个在训练人工神经网络（ANNs）中使用的重要方法是 dropout。在 dropout 中，一些连接甚至可能连同节点被停用，通常是随机的，每个训练批次（小的样本集）都会有所不同，每个批次停用的节点也会有所不同。这迫使网络在多个路径上分配其记忆，有助于提高泛化能力，并减少对训练数据的过拟合可能性。
- en: III Deep Learning in Core Areas of Natural Language Processing
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: III 深度学习在自然语言处理核心领域
- en: '![Refer to caption](img/84d2069069cfe1a5a86cac895e62bda2.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![参考图例](img/84d2069069cfe1a5a86cac895e62bda2.png)'
- en: 'Figure 3: Publication Volume for Core Areas of NLP. The number of publications,
    indexed by Google Scholar, relating to each topic over the last decade is shown.
    While all areas have experienced growth, language modeling has grown the most.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：自然语言处理核心领域的出版量。过去十年中，Google Scholar 索引的与每个主题相关的出版物数量如图所示。尽管所有领域都有增长，但语言建模的增长最多。
- en: 'The core issues are those that are inherently present in any computational
    linguistic system. To perform translation, text summarization, image captioning,
    or any other linguistic task, there must be some understanding of the underlying
    language. This understanding can be broken down into at least four main areas:
    language modeling, morphology, parsing, and semantics. The number of scholarly
    works in each area over the last decade is shown in Figure [3](#S3.F3 "Figure
    3 ‣ III Deep Learning in Core Areas of Natural Language Processing ‣ A Survey
    of the Usages of Deep Learning for Natural Language Processing").'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 核心问题是任何计算语言学系统中固有存在的。为了执行翻译、文本摘要、图像描述或任何其他语言任务，必须对基础语言有一定的理解。这种理解可以分解为至少四个主要领域：语言建模、形态学、句法分析和语义学。过去十年中每个领域的学术工作数量如图[3](#S3.F3
    "图 3 ‣ III 深度学习在自然语言处理核心领域 ‣ 深度学习在自然语言处理中的应用调查")所示。
- en: Language modeling can be viewed in two ways. First, it determines which words
    follow which. By extension, however, this can be viewed as determining what words
    mean, as individual words are only weakly meaningful, deriving their full value
    only from their interactions with other words. Morphology is the study of how
    words themselves are formed. It considers the roots of words and the use of prefixes
    and suffixes, compounds, and other intraword devices, to display tense, gender,
    plurality, and a other linguistic constructs. Parsing considers which words modify
    others, forming constituents, leading to a sentential structure. The area of semantics
    is the study of what words mean. It takes into account the meanings of the individual
    words and how they relate to and modify others, as well as the context these words
    appear in and some degree of world knowledge, i.e., “common sense”.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模可以从两个方面来观察。首先，它确定哪些词跟随哪些词。然而，从延伸来看，这也可以视为确定词的含义，因为单个词只有在与其他词互动时才具有完整的意义。形态学研究词汇本身是如何形成的。它考虑了词的词根以及前缀和后缀、复合词和其他词内部的设备，以展示时态、性别、复数以及其他语言结构。句法分析考虑了哪些词修饰其他词，形成成分，从而导致句子的结构。语义学领域研究词汇的含义。它考虑了单个词的含义以及它们如何与其他词相关联和修改，同时还考虑了这些词出现的上下文以及一定程度的世界知识，即“常识”。
- en: There is a significant amount of overlap between each of these areas. Therefore,
    many models analyzed can be classified as belonging in multiple sections. As such,
    they are discussed in the most relevant sections with logical connections to those
    other places where they also interact.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些领域之间有显著的重叠。因此，许多分析过的模型可以归类为多个部分。作为这样，它们在最相关的部分中进行讨论，并与其他地方的逻辑联系进行讨论。
- en: III-A Language Modeling and Word Embeddings
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-A 语言建模与词嵌入
- en: Arguably, the most important task in NLP is that of language modeling. Language
    modeling (LM) is an essential piece of almost any application of NLP. Language
    modeling is the process of creating a model to predict words or simple linguistic
    components given previous words or components[[48](#bib.bib48)]. This is useful
    for applications in which a user types input, to provide predictive ability for
    fast text entry. However, its power and versatility emanate from the fact that
    it can implicitly capture syntactic and semantic relationships among words or
    components in a linear neighborhood, making it useful for tasks such as machine
    translation or text summarization. Using prediction, such programs are able to
    generate more relevant, human-sounding sentences.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 可以说，NLP中最重要的任务是语言建模。语言建模（LM）几乎是任何NLP应用中的关键部分。语言建模是创建一个模型，以便在给定前一个词或组件的情况下预测词汇或简单的语言成分[[48](#bib.bib48)]。这对用户输入时提供快速文本输入的预测能力非常有用。然而，它的力量和多功能性来源于它能够隐式捕捉词汇或组件之间的句法和语义关系，使其在机器翻译或文本摘要等任务中变得非常有用。通过预测，这些程序能够生成更相关、更像人类语言的句子。
- en: III-A1 Neural Language Modeling
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A1 神经语言建模
- en: A problem with statistical language models was the inability to deal well with
    synonyms or out-of-vocabulary (OOV) words that were not present in the training
    corpus. Progress was made in solving the problems with the introduction of the
    neural language model [[49](#bib.bib49)]. While much of NLP took another decade
    to begin to use ANNs heavily, the LM community immediately took advantage of them,
    and continued to develop sophisticated models, many of which were summarized by
    DeMulder et al. [[50](#bib.bib50)].
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 统计语言模型的一个问题是无法很好地处理同义词或训练语料库中不存在的词汇（OOV）。通过引入神经语言模型[[49](#bib.bib49)]，解决了这些问题。虽然NLP领域花了十年时间才开始广泛使用ANN，但LM社区立即利用了这些技术，并继续开发复杂的模型，其中许多模型由DeMulder等人总结[[50](#bib.bib50)]。
- en: III-A2 Evaluation of Language Models
  id: totrans-60
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A2 语言模型评估
- en: While neural networks have made breakthroughs in the LM field, it is hard to
    quantify improvements. It is desirable to evaluate language models independently
    of the applications in which they appear. A number of metrics have been proposed,
    but no perfect solution has yet been found. [[51](#bib.bib51), [52](#bib.bib52),
    [53](#bib.bib53)] The most commonly used metric is perplexity, which is the inverse
    probability of a test set normalized by the number of words. Perplexity is a reasonable
    measurement for LMs trained on the same datasets, but when they are trained on
    different vocabularies, the metric becomes less meaningful. Luckily, there are
    several benchmark datasets that are used in the field, allowing for comparison.
    Two such datasets are the Penn Treebank (PTB) [[54](#bib.bib54)], and the Billion
    Word Benchmark [[55](#bib.bib55)].
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管神经网络在语言模型（LM）领域取得了突破，但很难量化改进。理想情况下，应该独立于语言模型出现的应用来评估它们。已经提出了许多评估指标，但尚未找到完美的解决方案
    [[51](#bib.bib51), [52](#bib.bib52), [53](#bib.bib53)]。最常用的指标是困惑度，它是测试集的逆概率，并由单词数量进行归一化。困惑度是对在相同数据集上训练的
    LM 的合理测量，但当它们在不同的词汇上训练时，这个指标就变得不那么有意义了。幸运的是，该领域有几个基准数据集可以进行比较。其中两个数据集是 Penn Treebank
    (PTB) [[54](#bib.bib54)] 和 Billion Word Benchmark [[55](#bib.bib55)]。
- en: III-A3 Memory Networks and Attention Mechanisms in Language Modeling
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A3 记忆网络和语言建模中的注意力机制
- en: 'Daniluk et al. [[56](#bib.bib56)] tested several networks using variations
    of attention mechanisms. The first network had a simple attention mechanism, which
    was not fully connected, having a window length of five. They hypothesized that
    using a single value to predict the next token, to encode information for the
    attentional unit, and to decode the information in the attentional unit hinders
    a network, as it is difficult to train a single parameter to perform three distinct
    tasks simultaneously. Therefore, in the second network, they designed each node
    to have two outputs: one to encode and decode the information in the attentional
    unit, and another to predict the next tokens explicitly. In the third network,
    they further separated the outputs, using separate values to encode the information
    entering the attentional unit and decode the information being retrieved from
    it. Tests on a Wikipedia corpus showed that the attention mechanism improved perplexity
    compared to the baseline, and that successively adding the second and third parameters
    led to further increases. It was also noted that only the previous five or so
    tokens carried much value (hence the selection of the window size of five). Therefore,
    they tested a fourth network which simply used residual connections from each
    of the previous five units. It was found that this network also provided results
    comparable to many larger RNNs and LSTMs, suggesting that reasonable results can
    be achieved using simpler networks.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Daniluk 等人 [[56](#bib.bib56)] 使用了几种注意力机制的变体来测试多个网络。第一个网络采用了一个简单的注意力机制，这个机制并不完全连接，窗口长度为五。他们假设，使用单一值来预测下一个标记、编码注意力单元中的信息以及解码注意力单元中的信息会阻碍网络，因为很难训练一个参数同时执行三个不同的任务。因此，在第二个网络中，他们设计了每个节点具有两个输出：一个用于编码和解码注意力单元中的信息，另一个用于显式预测下一个标记。在第三个网络中，他们进一步分离了输出，使用独立的值来编码进入注意力单元的信息和解码从中检索的信息。对维基百科语料库的测试表明，与基准相比，注意力机制提高了困惑度，而连续添加第二和第三个参数进一步提高了结果。还注意到，只有之前的五个标记携带了较大的价值（因此选择了五的窗口大小）。因此，他们测试了一个第四个网络，该网络仅使用了来自之前五个单元的残差连接。结果发现，这个网络也提供了与许多更大
    RNN 和 LSTM 相当的结果，这表明使用更简单的网络也能获得合理的结果。
- en: Another recent study was done on the usage of residual memory networks (RMNs)
    for LM [[57](#bib.bib57)]. The authors found that residual connections skipping
    two layers were most effective, followed closely by those skipping a single layer.
    In particular, a residual connection was present between the first layer and the
    fourth, as was between the fifth layer and the eighth, and between the ninth and
    the twelfth. It was found that increasing network depth improved results, but
    that when using large batch sizes, memory constraints were encountered. Network
    width was not found to be of particular importance for performance, however, wide
    networks were found to be harder to train. It was found that RMNs are capable
    of outperforming LSTMs of similar size.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的另一项研究探讨了在语言模型中使用残差记忆网络（RMNs）[[57](#bib.bib57)]。作者发现，跳过两个层的残差连接最为有效，其次是跳过单层的残差连接。特别是，第一层和第四层之间存在残差连接，第五层和第八层之间也有，此外，第九层和第十二层之间也有。研究发现，增加网络深度能够改善结果，但在使用大批量时会遇到内存限制。网络宽度对性能的影响不大，但宽网络的训练更为困难。研究还发现，RMNs能够超越类似规模的LSTMs。
- en: III-A4 Convolutional Neural Networks in Language Modeling
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A4 卷积神经网络在语言建模中的应用
- en: 'A CNN used recently in LM replaced the pooling layers with fully-connected
    layers [[58](#bib.bib58)]. These layers allowed the feature maps to be reduced
    to lower dimensional spaces just like the pooling layers. However, whereas any
    references to location of such features are lost in pooling layers, fully-connected
    layers somewhat retain this information. Three different architectures were implemented:
    a multilayer perceptron CNN (MLPConv) in which the filters were not simply linear,
    but instead small MLPs [[59](#bib.bib59)]; a multilayer CNN (ML-CNN) in which
    multiple convolutional layers were stacked on top of each other; and a combination
    of these networks called COM, in which kernel sizes for filters varied (in this
    case they were three and five). The results showed that stacking convolutional
    layers was detrimental in LM, but that both MLPConv and COM reduced perplexity.
    Combining MLPConv with the varying kernel sizes of COM provided even better results.
    Analysis showed that the networks learned specific patterns of words, such as,
    “as . . . as”. Lastly, this study showed that CNNs can be used to capture long
    term dependencies in sentences. Closer words were found to be of greatest importance,
    but words located farther away were of some significance as well.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在语言模型中使用的CNN将池化层替换为全连接层[[58](#bib.bib58)]。这些层允许特征图被缩减到较低维度的空间，就像池化层一样。然而，虽然池化层会丢失特征位置的任何引用，全连接层在某种程度上保留了这些信息。实现了三种不同的架构：多层感知器CNN（MLPConv），其中滤波器不仅是线性的，而是小型的MLPs[[59](#bib.bib59)]；多层CNN（ML-CNN），其中多个卷积层堆叠在一起；以及这些网络的组合称为COM，其中滤波器的卷积核大小有所变化（在这种情况下为三和五）。结果显示，堆叠卷积层在语言模型中是有害的，但MLPConv和COM都降低了困惑度。将MLPConv与COM的不同卷积核大小结合起来取得了更好的结果。分析显示，这些网络学习了特定的词语模式，例如，“as
    . . . as”。最后，这项研究表明，CNN可以用于捕捉句子中的长期依赖关系。较近的词语被发现最为重要，但较远的词语也有一定意义。
- en: III-A5 Character Aware Neural Language Models
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A5 特征感知神经语言模型
- en: While most CNNs used in NLP receive word embeddings (Section [III-A6](#S3.SS1.SSS6
    "III-A6 Development of Word Embeddings ‣ III-A Language Modeling and Word Embeddings
    ‣ III Deep Learning in Core Areas of Natural Language Processing ‣ A Survey of
    the Usages of Deep Learning for Natural Language Processing")) as input, recent
    networks have analyzed character level input instead. For example, Kim et al.
    [[60](#bib.bib60)], unlike previous networks [[61](#bib.bib61)], accepted only
    character level input, rather than combining it with word embeddings. A CNN was
    used to process the character level input to provide representations of the words.
    In a similar manner as word embeddings usually are, these representations were
    then fed into an encoder–decoder pair composed of a highway network (a gated network
    resembling an LSTM) [[46](#bib.bib46)] and an LSTM. They trained the network on
    the English Penn Treebank, as well as on datasets for Czech, German, Spanish,
    French, Russian, and Arabic. For every non-English language except Russian, the
    network outperformed previously published results [[61](#bib.bib61)] in both the
    large and small datasets. On the Penn Treebank, results were produced on par with
    the existing state of the art [[62](#bib.bib62)]. However, the network had only
    19 million trainable parameters, which is considerably lower than others. Since
    the network focused on morphological similarities produced by character level
    analysis, it was more capable than previous models of handling rare words. Analysis
    showed that without the use of highway layers, many words had nearest neighbors
    that were orthographically similar, but not necessarily semantically similar.
    Additionally, the network was capable of recognizing misspelled words or words
    not spelled in the standard way (e.g. looooook instead of look) and of recognizing
    out of vocabulary words. The analysis also showed that the network was capable
    of identifying prefixes, roots, and suffixes, as well as understanding hyphenated
    words, making it a robust model.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数用于自然语言处理的 CNN 接受词嵌入（第 [III-A6](#S3.SS1.SSS6 "III-A6 词嵌入的发展 ‣ III-A 语言建模和词嵌入
    ‣ III 自然语言处理核心领域的深度学习 ‣ 深度学习在自然语言处理中的应用概述") 节）作为输入，最近的网络分析则改为字符级输入。例如，Kim 等人 [[60](#bib.bib60)]
    不像之前的网络 [[61](#bib.bib61)]，只接受字符级输入，而不是将其与词嵌入结合。使用 CNN 处理字符级输入，以提供词的表示。类似于词嵌入，这些表示随后被输入到由高速公路网络（类似于
    LSTM 的门控网络） [[46](#bib.bib46)] 和 LSTM 组成的编码器-解码器对中。他们在英语 Penn Treebank 以及捷克语、德语、西班牙语、法语、俄语和阿拉伯语的数据集上训练了该网络。除俄语外的每种非英语语言中，该网络在大数据集和小数据集上的表现都优于先前公布的结果
    [[61](#bib.bib61)]。在 Penn Treebank 上，结果与现有的最先进技术相当 [[62](#bib.bib62)]。然而，该网络仅有
    1900 万个可训练参数，明显低于其他网络。由于网络专注于字符级分析产生的形态相似性，它比以前的模型更能够处理稀有词。分析表明，如果没有使用高速公路层，许多词的最近邻在形态上相似，但不一定在语义上相似。此外，网络能够识别拼写错误的词或不标准拼写的词（例如，looooook
    代替 look）以及识别词汇外的词。分析还显示，网络能够识别前缀、词根和后缀，并理解带连字符的词，使其成为一个强大的模型。
- en: Jozefowicz et al. [[63](#bib.bib63)] tested a number of architectures producing
    character level outputs [[64](#bib.bib64), [55](#bib.bib55), [65](#bib.bib65),
    [66](#bib.bib66)]. Whereas many of these models had only been tested on small
    scale language modeling, this study tested them on a large scale, testing them
    with the Billion Word Benchmark. The most effective model, achieving a state-of-the-art
    (for single models) perplexity of 30.0 with 1.04 billion trainable parameters
    (compared to a previous best by a single model of 51.3 with 20 billion parameters
    [[55](#bib.bib55)]), was a large LSTM using a character level CNN as an input
    network. The best performance, however, was achieved using an ensemble of ten
    LSTMs. This ensemble, with a perplexity of 23.7, far surpassed the previous state-of-the-art
    ensemble [[65](#bib.bib65)], which had a perplexity of 41.0.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Jozefowicz 等人 [[63](#bib.bib63)] 测试了多种产生字符级输出的架构 [[64](#bib.bib64), [55](#bib.bib55),
    [65](#bib.bib65), [66](#bib.bib66)]。尽管这些模型中的许多只在小规模语言建模上进行了测试，本研究却在大规模的环境中进行了测试，使用了十亿词基准。最有效的模型是一个大型
    LSTM，使用字符级 CNN 作为输入网络，它在 10.4 亿可训练参数下达到了 30.0 的最先进的困惑度（相比于先前单个模型的最佳结果 51.3，参数为
    200 亿 [[55](#bib.bib55)]）。然而，最佳性能是在使用十个 LSTM 的集成模型下取得的。这个集成模型的困惑度为 23.7，远超之前最先进的集成模型
    [[65](#bib.bib65)]，其困惑度为 41.0。
- en: III-A6 Development of Word Embeddings
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A6 词嵌入的发展
- en: Not only do neural language models allow for the prediction of unseen synonymous
    words, they also allow for modeling the relationships between words [[67](#bib.bib67),
    [68](#bib.bib68)]. Vectors with numeric components, representing individual words,
    obtained by LM techniques are called embeddings. This is usually done either by
    use of Principle Component Analysis or by capturing internal states in a neural
    language model. (Note that these are not standard LMs, but rather are LMs constructed
    specifically for this purpose.) Typically, word embeddings have between 50 and
    300 dimensions. An overused example is that of the distributed representations
    of the words king, queen, man, and woman. If one takes the embedding vectors for
    each of these words, computation can be performed to obtain highly sensible results.
    If the vectors representing these words are respectively represented as $\vec{k}$,
    $\vec{q}$, $\vec{m}$, and $\vec{w}$, it can be observed that $\vec{k}-\vec{q}\approx\vec{m}-\vec{w}$,
    which is extremely intuitive to human reasoning. In recent years, word embeddings
    have been the standard form of input to NLP systems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 神经语言模型不仅允许预测未见过的同义词，还能够建模词与词之间的关系 [[67](#bib.bib67), [68](#bib.bib68)]。由语言模型技术获得的、表示单个词的数值分量的向量称为嵌入。通常，这通过主成分分析（Principle
    Component Analysis）或通过捕捉神经语言模型中的内部状态来实现。（注意，这些不是标准的语言模型，而是专门为此目的构建的语言模型。）通常，词嵌入的维度在
    50 到 300 之间。一个被过度使用的例子是单词“king”、“queen”、“man”和“woman”的分布式表示。如果对这些单词的嵌入向量进行计算，可以得到非常合理的结果。如果这些单词的向量分别表示为
    $\vec{k}$、$\vec{q}$、$\vec{m}$ 和 $\vec{w}$，可以观察到 $\vec{k}-\vec{q}\approx\vec{m}-\vec{w}$，这对人类推理来说是非常直观的。近年来，词嵌入已经成为自然语言处理系统的标准输入形式。
- en: III-A7 Recent Advances and Challenges
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-A7 最近的进展和挑战
- en: Language modeling has been evolving on a weekly basis, beginning with the works
    of Radford et al. [[69](#bib.bib69)] and Peters et al. [[70](#bib.bib70)]. Radford
    et al. introduced Generative Pre-Training (GPT) which pretrained a language model
    based on the Transformer model [[42](#bib.bib42)] (Section [IV-G](#S4.SS7 "IV-G
    Machine Translation ‣ IV Applications of Natural Language Processing Using Deep
    Learning ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")),
    learning dependencies of words in sentences and longer segments of text, rather
    than just the immediately surrounding words. Peters et al. incorporated bi-directionalism
    to capture backwards context in addition to the forward context, in their Embeddings
    from Language Models (ELMo). Additionally, they captured the vectorizations at
    multiple levels, rather than just the final layer. This allowed for multiple encodings
    of the same information to be captured, which was empirically shown to boost the
    performance significantly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模正在每周不断演变，从 Radford 等人的工作 [[69](#bib.bib69)] 和 Peters 等人的工作 [[70](#bib.bib70)]
    开始。Radford 等人介绍了生成预训练（Generative Pre-Training, GPT），它基于 Transformer 模型 [[42](#bib.bib42)]
    预训练了一个语言模型（见第 [IV-G](#S4.SS7 "IV-G 机器翻译 ‣ IV 自然语言处理中的深度学习应用 ‣ 深度学习在自然语言处理中的应用概述")
    节），学习句子和更长文本片段中词的依赖关系，而不仅仅是立即周围的词。Peters 等人在他们的语言模型嵌入（Embeddings from Language
    Models, ELMo）中引入了双向性，以捕捉向前和向后的上下文。此外，他们在多个层次上捕捉了向量化，而不仅仅是最终层。这允许捕捉相同信息的多重编码，这在经验上显示出显著提升了性能。
- en: Devlin et al. [[71](#bib.bib71)], added an additional unsupervised training
    tasks of random masked neighbor word prediction, and next-sentence-prediction
    (NSP), in which, given a sentence (or other continuous segment of text), another
    sentence was predicted to either be the next sentence or not. These Bidirectional
    Encoder Representations from Transformers (BERT) were further built upon by Liu
    et al. [[72](#bib.bib72)] to create Multi-Task Deep Neural Network (MT-DNN) representations,
    which are the current state of the art in LM. The model used a stochastic answer
    network (SAN) [[73](#bib.bib73), [74](#bib.bib74)] ontop of a BERT-like model.
    After pretraining, the model was trained on a number of different tasks before
    being fine-tuned to the task at hand. Using MT-DNN as the LM, they achieved state-of-the-art
    results on ten out of eleven of the attempted tasks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Devlin 等人 [[71](#bib.bib71)] 增加了一项额外的无监督训练任务，即随机遮蔽邻近词预测和下一个句子预测（NSP），其中给定一个句子（或其他连续文本片段），预测另一个句子是否为下一个句子。这些来自双向编码器表示的变换器（BERT）进一步由
    Liu 等人 [[72](#bib.bib72)] 构建，创造了多任务深度神经网络（MT-DNN）表示，这在语言模型中代表了当前的最先进技术。该模型在类似
    BERT 的模型之上使用了随机答案网络（SAN）[[73](#bib.bib73), [74](#bib.bib74)]。在预训练之后，该模型在多个不同的任务上进行训练，然后根据当前任务进行微调。使用
    MT-DNN 作为语言模型，他们在十个尝试的任务中取得了最先进的结果。
- en: While these pretrained models have made excellent headway in “understanding”
    language, as is required for some tasks such as entailment inference, it has been
    hypothesized by some that these models are learning templates or syntactic patterns
    present within the datasets, unrelated to logic or inference. When new datasets
    are created removing such patterns carefully, the models do not perform well [[75](#bib.bib75)].
    Additionally, while there has been recent work on cross-language modeling and
    universal language modeling, the amount and level of work needs to pick up to
    address low-resource languages.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些预训练模型在“理解”语言方面取得了很好的进展，例如在蕴涵推理等任务中，但一些人假设这些模型学习的是数据集中的模板或句法模式，与逻辑或推理无关。当新数据集被创建以仔细移除这些模式时，模型的表现不佳
    [[75](#bib.bib75)]。此外，尽管最近在跨语言建模和通用语言建模方面有一些工作，但还需要增加工作量和水平，以应对低资源语言的挑战。
- en: III-B Morphology
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-B 形态学
- en: Morphology is concerned with finding segments within single words, including
    roots and stems, prefixes, suffixes, and—in some languages—infixes. Affixes (prefixes,
    suffixes, or infixes) are used to overtly modify stems for gender, number, person,
    et cetera.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 形态学关注的是在单词内部找到词缀，包括词根、词干、前缀、后缀以及—在某些语言中—插入成分。词缀（前缀、后缀或插入成分）用于显著修改词干的性别、数量、人称等。
- en: Luong et al. [[76](#bib.bib76)] constructed a morphologically aware LM. An RvNN
    was used to model the morphological structure. A neural language model was then
    placed on top of the RvNN. The model was trained on the WordSim-353 dataset [[77](#bib.bib77)]
    and segmentation was performed using Morfessor [[78](#bib.bib78)]. Two models
    were constructed—one using context and one not. It was found that the model that
    was insensitive to context over-accounted for certain morphological structures.
    In particular, words with the same stem were clustered together, even if they
    were antonyms. The context-sensitive model performed better, noting the relationships
    between the stems, but also accounting for other features such as the prefix “un”.
    The model was also tested on several other popular datasets [[79](#bib.bib79),
    [80](#bib.bib80), [81](#bib.bib81)], significantly outperforming previous embedding
    models on all.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Luong 等人 [[76](#bib.bib76)] 构建了一个具有形态学意识的语言模型。使用 RvNN 来建模形态学结构，然后在 RvNN 之上建立了神经语言模型。该模型在
    WordSim-353 数据集 [[77](#bib.bib77)] 上进行训练，分词使用了 Morfessor [[78](#bib.bib78)]。构建了两个模型—一个使用上下文，一个不使用。发现不敏感于上下文的模型过度考虑了某些形态学结构。特别是，相同词干的词被聚类在一起，即使它们是反义词。对上下文敏感的模型表现更好，考虑了词干之间的关系，还考虑了其他特征，如前缀“un”。该模型还在几个其他流行的数据集
    [[79](#bib.bib79), [80](#bib.bib80), [81](#bib.bib81)] 上进行了测试，显著超越了之前的嵌入模型。
- en: A good morphological analyzer is often important for many NLP tasks. As such,
    one recent study by Belinkov et al. [[82](#bib.bib82)] examined the extent to
    which morphology was learned and used by a variety of neural machine translation
    models. A number of translation models were constructed, all translating from
    English to French, German, Czech, Arabic, or Hebrew. Encoders and decoders were
    LSTM-based models (some with attention mechanisms) or character aware CNNs, and
    the models were trained on the WIT³ corpus [[83](#bib.bib83), [84](#bib.bib84)].
    The decoders were then replaced with part-of-speech (POS) taggers and morphological
    taggers, fixing the weights of the encoders to preserve the internal representations.
    The effects of the encoders were examined as were the effects of the decoders
    attached during training. The study concluded that the use of attention mechanisms
    decreases the performance of encoders, but increases the performance of decoders.
    Furthermore, it was found that character-aware models are superior to others for
    learning morphology and that the output language affects the performance of the
    encoders. Specifically, the more morphologically rich the output language, the
    worse the representations created by the encoders.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的形态分析器对于许多自然语言处理任务通常是重要的。因此，Belinkov等人进行了一项最近的研究[[82](#bib.bib82)]，研究了形态学在各种神经机器翻译模型中的学习和使用程度。构建了多个翻译模型，所有模型都从英语翻译成法语、德语、捷克语、阿拉伯语或希伯来语。编码器和解码器是基于LSTM的模型（有些带有注意力机制）或字符感知的CNN，这些模型在WIT³语料库[[83](#bib.bib83),
    [84](#bib.bib84)]上进行了训练。然后将解码器替换为词性（POS）标注器和形态标注器，固定编码器的权重以保留内部表示。研究检查了编码器的效果以及训练过程中附加的解码器的效果。研究得出结论，注意力机制的使用会降低编码器的性能，但会提高解码器的性能。此外，发现字符感知模型在学习形态学方面优于其他模型，并且输出语言会影响编码器的性能。具体而言，输出语言的形态学越丰富，编码器创建的表示效果越差。
- en: Morita et al. [[85](#bib.bib85)] analyzed a new morphological language model
    for unsegmented languages such as Japanese. They constructed an RNN-based model
    with a beam search decoder and trained it on an automatically labeled [[86](#bib.bib86)]
    corpus and on a manually labeled corpus. The model performed a number of tasks
    jointly, including morphological analysis, POS tagging, and lemmatization. The
    model was then tested on the Kyoto Text Corpus [[87](#bib.bib87)] and the Kyoto
    University Web Document Leads Corpus [[88](#bib.bib88)], outperforming all baselines
    on all tasks.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Morita等人[[85](#bib.bib85)]分析了一种用于未分词语言（如日语）的新型形态学语言模型。他们构建了一个基于RNN的模型，配有束搜索解码器，并在自动标注[[86](#bib.bib86)]的语料库和手动标注的语料库上进行了训练。该模型共同执行了多个任务，包括形态分析、POS标注和词形还原。该模型随后在京都文本语料库[[87](#bib.bib87)]和京都大学网页文档领先语料库[[88](#bib.bib88)]上进行了测试，在所有任务上都超越了所有基线模型。
- en: A recent line of work in morphology is universal morphology. This task considers
    the relationships between the morphologies of different languages and how they
    relate to each other, aiming towards the ultimate goal of a single morphological
    analyzer. However, to the authors’ knowledge, there has been only a single study
    applying deep learning to this area [[89](#bib.bib89)], and even then, only as
    a supporting task to universal parsing (Section [III-C4](#S3.SS3.SSS4 "III-C4
    Universal Parsing ‣ III-C Parsing ‣ III Deep Learning in Core Areas of Natural
    Language Processing ‣ A Survey of the Usages of Deep Learning for Natural Language
    Processing")). For those wishing to apply deep learning to this task, several
    datasets are already available, including one from a CoNLL shared task [[90](#bib.bib90)].
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 形态学的一个近期研究方向是普遍形态学。这项任务考虑了不同语言形态学之间的关系及其相互关系，旨在实现最终目标——单一形态分析器。然而，据作者所知，只有一项研究将深度学习应用于这一领域[[89](#bib.bib89)]，即使如此，也仅作为普遍解析的辅助任务（第[III-C4节](#S3.SS3.SSS4
    "III-C4 Universal Parsing ‣ III-C Parsing ‣ III Deep Learning in Core Areas of
    Natural Language Processing ‣ A Survey of the Usages of Deep Learning for Natural
    Language Processing")）。对于那些希望将深度学习应用于这一任务的人，已经有几个数据集可用，包括来自CoNLL共享任务的一个数据集[[90](#bib.bib90)]。
- en: In addition to universal morphology, the development of morphological embeddings,
    that take into account the structures of words, could aid in multi-language processing.
    They could possibly be used across cognate languages, which would be valuable
    when some languages are more resourced than others. In addition, morphological
    structures may be important in handling specialized languages such as those used
    in the biomedical literature. Since deep learning has become quite entrenched
    in NLP, better handling of morphological components is likely to improve performance
    of overall models.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通用形态学外，考虑到单词结构的形态嵌入的发展可能有助于多语言处理。它们可能在同源语言间使用，这在某些语言资源更丰富时特别有价值。此外，形态结构在处理如生物医学文献中使用的专业语言时可能也很重要。由于深度学习在自然语言处理（NLP）中已相当根深蒂固，更好地处理形态成分可能会提升整体模型的性能。
- en: III-C Parsing
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-C 解析
- en: 'Parsing examines how different words and phrases relate to each other within
    a sentence. There are at least two distinct forms of parsing: constituency parsing
    and dependency parsing [[48](#bib.bib48)]. In constituency parsing, phrasal constituents
    are extracted from a sentence in a hierarchical fashion. Dependency parsing looks
    at the relationships between pairs of individual words.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 解析检查句子中不同单词和短语之间的关系。解析至少有两种不同形式：短语结构解析和依赖解析 [[48](#bib.bib48)]。在短语结构解析中，短语成分以分层的方式从句子中提取出来。依赖解析则查看单个词对之间的关系。
- en: Most recent uses of deep learning in parsing have been in dependency parsing,
    within which there exists another major divide in types of solutions. Graph-based
    parsing constructs a number of parse trees that are then searched to find the
    correct one. Most graph-based approaches are generative models, in which a formal
    grammar, based on the natural language, is used to construct the trees [[48](#bib.bib48)].
    More popular in recent years than graph-based approaches have been transition-based
    approaches that usually construct only one parse tree. While a number of modifications
    have been proposed, the standard method of transition-based dependency parsing
    is to create a buffer containing all of the words in the sentence and stack containing
    only the ROOT label. Words are then pushed onto the stack, where connections,
    known as arcs, are made between the top two items. Once dependencies have been
    determined, words are popped off the stack. The process continues until the buffer
    is empty and only the ROOT label remains on the stack. Three major approaches
    are used to regulate the conditions in which each of the previously described
    actions takes place. In the arc-standard approach [[91](#bib.bib91), [92](#bib.bib92)],
    all dependents are connected to a word before the word is connected to its parent.
    In the arc-eager approach [[91](#bib.bib91), [92](#bib.bib92)], words are connected
    to their parents as soon as possible, regardless of whether or not their children
    are all connected to them. Finally, in the swap-lazy approach [[93](#bib.bib93)],
    the arc-standard approach is modified to allow swapping of positions on the stack.
    This makes the graphing of non-projective edges possible.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在解析中使用深度学习大多集中于依赖解析，其中存在另一种主要的解决方案类型划分。基于图的解析构建多个解析树，然后进行搜索以找到正确的解析树。大多数基于图的方法是生成模型，其中使用基于自然语言的形式语法来构建树
    [[48](#bib.bib48)]。近年来，比起基于图的方法，基于转换的方法更为流行，这些方法通常只构建一个解析树。尽管提出了许多修改方案，但基于转换的依赖解析的标准方法是创建一个包含句子中所有单词的缓冲区和一个仅包含ROOT标签的栈。然后，将单词推入栈中，顶部的两个项目之间会建立连接，称为弧。一旦依赖关系被确定，单词就从栈中弹出。这个过程持续进行，直到缓冲区为空且栈上只剩下ROOT标签。三种主要方法用于调节先前描述的每个操作的条件。在arc-standard方法
    [[91](#bib.bib91), [92](#bib.bib92)] 中，所有的依赖项在连接到其父词之前都先连接到一个词。在arc-eager方法 [[91](#bib.bib91),
    [92](#bib.bib92)] 中，无论其子词是否全部连接到该词，词都会尽可能快地与其父词连接。最后，在swap-lazy方法 [[93](#bib.bib93)]
    中，arc-standard方法被修改以允许在栈上交换位置，这使得绘制非投影边成为可能。
- en: III-C1 Early Neural Parsing
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C1 早期神经解析
- en: One early application of deep learning to NLP, that of Socher et al. [[94](#bib.bib94),
    [95](#bib.bib95)], included the use of RNNs with probabilistic context-free grammars
    (PCFGs) [[96](#bib.bib96), [97](#bib.bib97)]. As far as the authors are aware,
    the first neural model to achieve state-of-the-art performance in parsing was
    that of Le and Zuidema [[98](#bib.bib98)]. Such performance was achieved on the
    Penn Treebank for both labeled attachment score (LAS) and unlabeled attachment
    score (UAS) by using an Inside-Out Recursive Neural Network, which used two vector
    representations (an inner and an outer) to allow both top-down and bottom-up flows
    of data. Vinyals et al. [[99](#bib.bib99)] created an LSTM with an attention mechanism
    in a syntactic constituency parser, which they tested on data from domains different
    from those of the test data (the English Web Treebank [[100](#bib.bib100)] and
    the Question Treebank [[101](#bib.bib101)] as opposed to the Wall Street Journal
    portion of the Penn Treebank [[54](#bib.bib54)]), showing that neural models can
    generalize between domains. Embeddings were first used in dependency parsing by
    Stenetorp [[102](#bib.bib102)]. This approach used an RNN to create a directed
    acyclic graph. While this model did produce results within 2% of the state of
    the art (on the Wall Street Journal portion of the CoNLL 2008 Shared Task dataset
    [[103](#bib.bib103)]), by the time it reached the end of a sentence, it seemed
    to have difficulty remembering phrases from early in the sentence.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在NLP中的早期应用之一是Socher等人[[94](#bib.bib94), [95](#bib.bib95)]提出的，其中包括使用带有概率上下文无关文法（PCFGs）[[96](#bib.bib96),
    [97](#bib.bib97)]的RNN。根据作者了解，首个在解析中实现技术前沿性能的神经模型是Le和Zuidema[[98](#bib.bib98)]提出的。该性能在宾州树库上通过使用Inside-Out递归神经网络实现，该网络使用两个向量表示（一个内层和一个外层），允许数据的自上而下和自下而上的流动。Vinyals等人[[99](#bib.bib99)]在句法成分解析器中创建了一个带有注意力机制的LSTM，并在来自不同领域的数据（如英语网页树库[[100](#bib.bib100)]和问答树库[[101](#bib.bib101)]，而非宾州树库的华尔街日报部分[[54](#bib.bib54)]）上进行了测试，显示神经模型能够在领域之间进行泛化。Stenetorp[[102](#bib.bib102)]首次在依赖解析中使用了嵌入。这种方法使用RNN创建了一个有向无环图。尽管该模型在CoNLL
    2008共享任务数据集的华尔街日报部分[[103](#bib.bib103)]上产生了接近技术前沿的结果，但在句子结束时，它似乎难以记住句子开头的短语。
- en: III-C2 Transition-Based Dependency Parsing
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C2 基于过渡的依赖解析
- en: Chen and Manning [[104](#bib.bib104)] pushed the state of the art in both UAS
    and LAS on both English and Chinese datasets on the English Penn Treebank. They
    accomplished this by using a simple feedforward neural network as the decision
    maker in a transition-based parser. By doing so they were able to subvert the
    problem of sparsity persistent in the statistical models.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 陈和曼宁[[104](#bib.bib104)]在英语宾州树库的英语和中文数据集上推动了UAS和LAS的技术前沿。他们通过在过渡基础解析器中使用简单的前馈神经网络作为决策者来实现这一点。这样，他们能够规避统计模型中普遍存在的稀疏性问题。
- en: Chen and Manning used a simple greedy search, which was replaced by Zhou et
    al. [[105](#bib.bib105)] with a beam search, achieving a significant improvement.
    Weiss et al. [[106](#bib.bib106)] improved upon Chen and Manning’s work by using
    a deeper neural network with residual connections and a perceptron layer placed
    after the softmax layer. They were able to train on significantly more examples
    than typical by using tri-training [[107](#bib.bib107)], a process in which potential
    data samples are fed to two other parsers, and those samples upon which both of
    the parsers agree are used for training the primary parser.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 陈和曼宁使用了简单的贪婪搜索，这一方法被周等人[[105](#bib.bib105)]的束搜索所取代，实现了显著的改进。魏斯等人[[106](#bib.bib106)]在陈和曼宁的工作基础上进行了改进，使用了具有残差连接的更深层神经网络，并在softmax层之后放置了一个感知机层。他们通过使用三重训练[[107](#bib.bib107)]，在训练时使用显著更多的示例，这一过程涉及将潜在的数据样本提供给另外两个解析器，并利用两个解析器都同意的样本来训练主要解析器。
- en: Another model was produced using an LSTM instead of a feedforward network [[108](#bib.bib108)].
    Unlike previous models, this model was given knowledge of the entire buffer and
    the entire stack and had knowledge of the entire history of transition decisions.
    This allowed for better predictions, generating state-of-the-art on the Stanford
    Dependency Treebank [[109](#bib.bib109)], as well as state-of-the-art results
    on the CTB5 Chinese dataset [[110](#bib.bib110)]. Lastly, Andor et al. [[111](#bib.bib111)]
    used a feedforward network with global normalization on a number of tasks including
    part-of-speech tagging, sentence compression, and dependency parsing. State-of-the-art
    results were obtained on all tasks on the Wall Street Journal dataset. Notably,
    their model required significantly less computation than comparable models.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个模型使用了 LSTM 而不是前馈网络 [[108](#bib.bib108)]。与之前的模型不同，该模型能够获取整个缓冲区和整个栈的信息，并了解所有过渡决策的历史。这使得模型能够做出更好的预测，在斯坦福依赖树库
    [[109](#bib.bib109)] 上生成了最先进的结果，同时在 CTB5 中文数据集 [[110](#bib.bib110)] 上也取得了最先进的结果。最后，Andor
    等人 [[111](#bib.bib111)] 使用了具有全局归一化的前馈网络，处理了包括词性标注、句子压缩和依赖解析在内的多个任务。在华尔街日报数据集上，所有任务均取得了最先进的结果。值得注意的是，他们的模型所需的计算量显著低于可比模型。
- en: 'Much like Stenentorp [[102](#bib.bib102)], Wang et al. [[112](#bib.bib112)]
    used an alternative algorithm to produce directed acyclic graphs, for a task called
    semantic parsing, where deeper relationships between the words are found. The
    task seeks to identify what types of actions are taking place and how words modify
    each other. In addition to the typical stack and buffer used in transition-based
    parsing, the algorithm employed a deque. This allowed for the representation of
    multi-parented words, which although rare in English, are common in many natural
    languages. Furthermore, it allowed for multiple children of the ROOT label. In
    addition to producing said graphs, this work is novel in its use of two new LSTM-based
    techniques: Bi-LSTM Subtraction and Incremental Tree-LSTM. Bi-LSTM Subtraction
    built on previous work [[41](#bib.bib41), [113](#bib.bib113)] to represent the
    buffer as a subtraction of the vectors from the head and tail of the LSTM, in
    addition to using an additional LSTM to represent the deque. Incremental Tree-LSTM
    is an extension of Tree-LSTM [[114](#bib.bib114)], modified for directed acyclic
    graphs, by connecting children to parents incrementally, rather than connecting
    all children to a parent simultaneously. The model achieved the best published
    scores at the time for fourteen of the sixteen evaluation metrics used on SemEval-2015
    Task 18 (English) [[115](#bib.bib115)] and SemEval-2016 Task 9 (Chinese) [[116](#bib.bib116)].
    While deep learning had been applied to semantic parsing in particular domains,
    such as Question Answering [[117](#bib.bib117), [118](#bib.bib118)], to the authors’
    knowledge, this was the first time it was applied in large scale to semantic parsing
    as a whole.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 Stenentorp [[102](#bib.bib102)]，Wang 等人 [[112](#bib.bib112)] 使用了一种替代算法来生成有向无环图，进行一种称为语义解析的任务，在该任务中找到单词之间的更深层次的关系。该任务旨在识别正在发生的动作类型以及单词如何互相修饰。除了在基于转换的解析中使用的典型栈和缓冲区，该算法还使用了一个双端队列（deque）。这使得能够表示多父节点单词，虽然在英语中这种情况较少见，但在许多自然语言中很常见。此外，它还允许多个
    ROOT 标签的子节点。除了生成上述图形外，这项工作还创新性地使用了两种新的基于 LSTM 的技术：Bi-LSTM 减法和增量 Tree-LSTM。Bi-LSTM
    减法基于之前的工作 [[41](#bib.bib41), [113](#bib.bib113)]，将缓冲区表示为来自 LSTM 头部和尾部的向量的减法，并使用额外的
    LSTM 来表示双端队列。增量 Tree-LSTM 是 Tree-LSTM [[114](#bib.bib114)] 的扩展，针对有向无环图进行了修改，通过逐步连接子节点到父节点，而不是同时将所有子节点连接到一个父节点。该模型在
    SemEval-2015 任务 18（英语） [[115](#bib.bib115)] 和 SemEval-2016 任务 9（中文） [[116](#bib.bib116)]
    的十六项评估指标中取得了当时发布的最佳成绩。虽然深度学习已在特定领域（如问答系统 [[117](#bib.bib117), [118](#bib.bib118)]）应用于语义解析，但据作者所知，这是首次大规模应用于整体语义解析。
- en: III-C3 Generative Dependency and Constituent Parsing
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C3 生成依赖和成分解析
- en: Dyer et al. [[119](#bib.bib119)] proposed a model that used recurrent neural
    network grammars for parsing and language modeling. Whereas most approaches take
    a bottom-up approach to parsing, this took a top-down approach, taking as input
    the full sentence in addition to the current parse tree. This allowed the sentence
    to be viewed as a whole, rather than simply allowing local phrases within it to
    be considered. This model achieved the then best results in English generative
    parsing as well as in single sentence language modeling. It also attained results
    close to the best in Chinese generative parsing.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: Dyer 等人 [[119](#bib.bib119)] 提出了一个使用递归神经网络语法进行解析和语言建模的模型。尽管大多数方法采取自下而上的解析方法，这个模型采用了自上而下的方法，除了当前的解析树，还将整个句子作为输入。这使得句子可以整体视角看待，而不仅仅是考虑局部短语。这个模型在英语生成解析和单句语言建模中取得了当时最好的结果，也在中文生成解析中达到了接近最佳的结果。
- en: Choe and Charniak [[120](#bib.bib120)] treated parsing as a language modeling
    problem, and used an LSTM to assign probabilities to the parse trees, achieving
    state-of-the art. Fried et al. [[121](#bib.bib121)] wanted to determine whether
    the power of the models came from the reranking process or simply from the combined
    power of two models. They found that while using one parser for producing candidate
    trees and another for ranking them was superior to a single parser approach, combining
    two parsers explicitly was preferable. They used two parsers to both select the
    candidates and rerank them, achieving state-of-the-art results. They extended
    this model to use three parsers, achieving even better results. Finally, an ensemble
    of eight such models (using two parsers) was constructed and achieved the best
    results on Penn Treebank at the time.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: Choe 和 Charniak [[120](#bib.bib120)] 将解析视为语言建模问题，并使用 LSTM 为解析树分配概率，取得了最先进的成果。Fried
    等人 [[121](#bib.bib121)] 想要确定模型的力量是否来自重新排序过程，还是仅仅来自两个模型的组合力量。他们发现，使用一个解析器生成候选树，另一个解析器进行排名优于单一解析器方法，但显式地组合两个解析器是更好的选择。他们使用两个解析器来选择候选者并重新排序，取得了最先进的结果。他们将该模型扩展到使用三个解析器，取得了更好的结果。最后，构建了一个由八个此类模型（使用两个解析器）组成的集成模型，并在当时的
    Penn Treebank 上取得了最佳结果。
- en: A model created by Dozat and Manning [[122](#bib.bib122)] used a graph-based
    approach with a self-attentive network. Similarly, Tan et al. [[123](#bib.bib123)]
    used a self-attentional model for semantic role labeling, a subtask of semantic
    parsing, achieving excellent results. They experimented with recurrent and convolutional
    replacements to the feed-forward portions of the self-attention mechanism, finding
    that the feed forward variant had the best performance. Another novel approach
    is that of Duong et al. [[124](#bib.bib124)], who used active learning. While
    not perfect, this is a possible solution to one of the biggest problems in semantic
    parsing—the availability of data.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Dozat 和 Manning [[122](#bib.bib122)] 创建的模型使用了基于图的自注意网络方法。同样，Tan 等人 [[123](#bib.bib123)]
    采用了自注意模型进行语义角色标注，这是语义解析的一个子任务，取得了优秀的结果。他们尝试了递归和卷积替代自注意机制的前馈部分，发现前馈变体表现最好。另一种新颖的方法是
    Duong 等人 [[124](#bib.bib124)] 使用主动学习。虽然不是完美的，这仍然是解决语义解析中最大问题之一——数据可用性——的一个可能解决方案。
- en: III-C4 Universal Parsing
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C4 通用解析
- en: Much like universal morphology, universal dependency parsing, or universal parsing,
    is the relatively new task of parsing language using a standardized set of tags
    and relationships across all languages. While current parsing varies drastically
    from language to language, this attempts to make it uniform between them, in order
    to allow for easier processing between and among them. Nivre [[125](#bib.bib125)]
    discussed the recent development of universal grammar and presented the challenges
    that lie ahead, mainly the development of tree banks in more languages and the
    consistency of labeling between tree banks in different (and even the same) languages.
    This task has gained traction in large part due to the fact that it has been a
    CoNLL shared task for the past two years. [[126](#bib.bib126)] A number of approaches
    from the 2018 task included using deep transition parsing [[127](#bib.bib127)],
    graph-based neural parsing [[128](#bib.bib128)], and a competitive model which
    used only a single neural model, rather than an ensemble [[129](#bib.bib129)].
    The task has begun to be examined outside of CoNLL, with Liu et al. [[130](#bib.bib130)]
    applying universal dependencies to the parsing of tweets, using an ensemble of
    bidirectional LSTM.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 像通用形态学、通用依赖解析或通用解析一样，通用依赖解析是一个相对较新的任务，它使用标准化的一组标签和关系来解析所有语言。虽然当前的解析在语言之间差异很大，但这个任务试图使其在语言之间统一，以便更容易进行处理。Nivre
    [[125](#bib.bib125)] 讨论了通用语法的最新发展，并介绍了面临的挑战，主要是开发更多语言的树库以及不同（甚至相同）语言的树库之间标签的一致性。由于过去两年它一直是
    CoNLL 共享任务，这个任务得到了很大关注 [[126](#bib.bib126)]。2018 年任务中的一些方法包括使用深度转换解析 [[127](#bib.bib127)]、基于图的神经解析
    [[128](#bib.bib128)] 和使用单一神经模型而非集成模型的竞争模型 [[129](#bib.bib129)]。该任务已经开始在 CoNLL
    之外进行研究，Liu 等人 [[130](#bib.bib130)] 将通用依赖应用于推文解析，使用双向 LSTM 的集成模型。
- en: III-C5 Remaining Challenges
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-C5 剩余挑战
- en: Outside of universal parsing, a parsing challenge that needs to be further investigated
    is the building of syntactic structures without the use of treebanks for training.
    Attempts have been made using attention scores and Tree-LSTMs, as well as “outside-inside”
    auto-encoders. If such approaches are successful, they have potential use in many
    environments, including in the context of low-resource languages and out-of-domain
    scenarios. While a number of other challenges remain, these are the largest and
    are expected to receive the most focus.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了通用解析之外，需要进一步研究的解析挑战是构建句法结构而不使用树库进行训练。已经尝试使用注意力评分和树LSTM，以及“外部-内部”自编码器。如果这些方法成功，它们在许多环境中，包括低资源语言和领域外场景中，具有潜在的应用价值。虽然还有许多其他挑战存在，但这些是最大的，预计会得到最多关注。
- en: III-D Semantics
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-D 语义学
- en: 'Semantic processing involves understanding the meaning of words, phrases, sentences,
    or documents at some level. Word embeddings, such as Word2Vec [[67](#bib.bib67),
    [68](#bib.bib68)] and GloVe [[131](#bib.bib131)], claim to capture meanings of
    words, following the Distributional Hypothesis of Meaning [[132](#bib.bib132)].
    As a corollary, when vectors corresponding to phrases, sentences, or other components
    of text are processed using a neural network, a representation that can be loosely
    thought to be semantically representative is computed compositionally. In this
    section, neural semantic processing research is separated into two distinct areas:
    Work on comparing the semantic similarity of two portions of text, and work on
    capturing and transferring meaning in high level constituents, particularly sentences.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 语义处理涉及在某种程度上理解单词、短语、句子或文档的意义。词嵌入，如 Word2Vec [[67](#bib.bib67), [68](#bib.bib68)]
    和 GloVe [[131](#bib.bib131)]，声称捕捉了单词的意义，遵循了意义的分布假设 [[132](#bib.bib132)]。作为推论，当处理与短语、句子或其他文本组成部分相对应的向量时，使用神经网络计算出一个可以大致认为是语义代表的表示是组合性的。在本节中，神经语义处理研究分为两个不同领域：比较两段文本的语义相似性的工作，以及捕捉和转移高层次成分（特别是句子）中的意义的工作。
- en: III-D1 Semantic Comparison
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D1 语义比较
- en: One way to test the efficacy of an approach to computing semantics is to see
    if two similar phrases, sentences or documents, judged by humans to have similar
    meaning also are judged similarly by a program.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 测试计算语义方法有效性的一种方式是观察两个被人类评判为具有相似意义的短语、句子或文档，是否也会被程序评判为相似。
- en: Hu et al. [[133](#bib.bib133)] proposed two CNNs to perform a semantic comparison
    task. The first model, ARC-I, inspired by Bordes et al. [[134](#bib.bib134)],
    used a Siamese network, in which two CNNs sharing weights evaluated two sentences
    in parallel. In the second network, connections were placed between the two, allowing
    for sharing before the final states of the CNNs. The approach outperformed a number
    of existing models in tasks in English and Chinese.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 胡等人[[133](#bib.bib133)]提出了两个CNN模型来执行语义比较任务。第一个模型ARC-I，受到Bordes等人[[134](#bib.bib134)]的启发，使用了一个Siamese网络，其中两个共享权重的CNN并行评估两个句子。在第二个网络中，两个网络之间放置了连接，在CNN的最终状态之前允许共享。这种方法在英文和中文任务中优于许多现有模型。
- en: Building on prior work [[26](#bib.bib26), [21](#bib.bib21), [133](#bib.bib133)],
    Yin and Schütze [[135](#bib.bib135)] proposed a Bi-CNN-MI (MI for multigranular
    interaction features), consisting of a pretrained CNN sentence model, a CNN interaction
    model, and a logistic regressor. They modifiied a Siamese network using Dynamic
    CNNs [[21](#bib.bib21)] (Section [III-D2](#S3.SS4.SSS2 "III-D2 Sentence Modeling
    ‣ III-D Semantics ‣ III Deep Learning in Core Areas of Natural Language Processing
    ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")).
    Additionally, the feature maps from each level were used in the comparison, rather
    than simply the top-level feature maps. They achieved state-of-the-art results
    on the Microsoft Research Paraphrase Corpus (MSRP) [[136](#bib.bib136)].
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前工作的基础上[[26](#bib.bib26), [21](#bib.bib21), [133](#bib.bib133)]，Yin和Schütze[[135](#bib.bib135)]提出了一个Bi-CNN-MI（MI为多粒度交互特征），包括一个预训练的CNN句子模型、一个CNN交互模型和一个逻辑回归器。他们修改了使用Dynamic
    CNNs[[21](#bib.bib21)]的Siamese网络（第[III-D2](#S3.SS4.SSS2 "III-D2 Sentence Modeling
    ‣ III-D Semantics ‣ III Deep Learning in Core Areas of Natural Language Processing
    ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")节）。此外，比较时使用了每一层的特征图，而不仅仅是顶层特征图。他们在微软研究释义语料库（MSRP）[[136](#bib.bib136)]上取得了最先进的结果。
- en: 'He et al. [[137](#bib.bib137)] constructed feature maps, which were then compared
    using a “similarity measurement layer” followed by a fully-connected layer and
    then a log-softmax output layer within a CNN. The windows used in the convolutional
    layers ranged in length from one to four. The network was trained and evaluated
    on three datasets: MSRP, the Sentences Involving Compositional Knowledge (SICK)
    dataset [[138](#bib.bib138)], and the Microsoft Video Paraphrase Corpus (MSRVID)
    [[139](#bib.bib139)]. State-of-the-art results were achieved on the first and
    the third.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: He等人[[137](#bib.bib137)]构建了特征图，然后使用“相似性测量层”进行比较，随后是全连接层和CNN中的log-softmax输出层。卷积层中使用的窗口长度从一到四不等。该网络在三个数据集上进行了训练和评估：MSRP、涉及组合知识的句子（SICK）数据集[[138](#bib.bib138)]和微软视频释义语料库（MSRVID）[[139](#bib.bib139)]。在第一个和第三个数据集上取得了最先进的结果。
- en: Tai et al. concocted a model using an RvNN with LSTM-like nodes [[114](#bib.bib114)]
    called a Tree-LSTM. Two variations were examined (constituency- and dependency-based)
    and tested on both the SICK dataset and Stanford Sentiment Treebank [[94](#bib.bib94)].
    The constituency-based model achieved state-of-the-art results on the Stanford
    Sentiment Treebank and the dependency-based one achieved state-of-the-art results
    on SICK.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Tai等人设计了一个使用类似LSTM节点的RvNN模型[[114](#bib.bib114)]，称为Tree-LSTM。研究了两种变体（基于成分和基于依赖）并在SICK数据集和斯坦福情感树库[[94](#bib.bib94)]上进行了测试。基于成分的模型在斯坦福情感树库上达到了最先进的结果，而基于依赖的模型在SICK上达到了最先进的结果。
- en: He et al. presented another model [[140](#bib.bib140)], which outperformed that
    of Tai et al. on SICK. The model formed a matrix of the two sentences before applying
    a “similarity focus layer” and then a nineteen-layer CNN followed by dense layers
    with a softmax output. The similarity focus layer matched semantically similar
    pairs of words from the input sentences and applied weights to the matrix locations
    representing the relations between the words in each pair. They also obtained
    state-of-the-art resuults on MSRVID, SemEval 2014 Task 10 [[141](#bib.bib141)],
    WikiQA [[142](#bib.bib142)], and TreeQA [[143](#bib.bib143)] datasets.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: He等人提出了另一个模型[[140](#bib.bib140)]，在SICK上优于Tai等人的模型。该模型在应用“相似性关注层”之前，首先形成两个句子的矩阵，然后是一个十九层的CNN，接着是密集层和softmax输出。相似性关注层匹配了输入句子中语义相似的词对，并对表示词对之间关系的矩阵位置应用权重。他们还在MSRVID、SemEval
    2014任务10[[141](#bib.bib141)]、WikiQA[[142](#bib.bib142)]和TreeQA[[143](#bib.bib143)]数据集上获得了最先进的结果。
- en: III-D2 Sentence Modeling
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D2 句子建模
- en: Extending from neural language modeling, sentence modeling attempts to capture
    the meaning of sentences in vectors. Taking this a step further are models, such
    as that of Le and Mikolov [[144](#bib.bib144)], which attempt to model paragraphs
    or larger bodies of text in this way.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从神经语言建模扩展而来的句子建模试图将句子的意义捕捉为向量。进一步发展的是像Le和Mikolov [[144](#bib.bib144)]的模型，这些模型试图以这种方式对段落或更大文本块进行建模。
- en: Kalchbrenner et al. [[21](#bib.bib21)] generated representations of sentences
    using a dynamic convolutional neural network (DCNN), which used a number of filters
    and dynamic $k$-max pooling layers. Due to dynamic pooling, features of different
    types and lengths could be identified in sentences with varying structures without
    padding of the input. This allowed not only short-range dependencies, but also
    long-range dependencies to be identified. The DCNN was tested in applied tasks
    that require semantic understanding. It outperformed all comparison models in
    predicting sentiment of movie reviews in the Stanford Sentiment Treebank [[95](#bib.bib95)]
    and in identification of sentiment in tweets [[145](#bib.bib145)]. It was also
    one of the top performers in classifying types of questions using the TREC database
    [[146](#bib.bib146)].
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Kalchbrenner等人 [[21](#bib.bib21)] 使用动态卷积神经网络（DCNN）生成句子的表示，这种网络使用了多个滤波器和动态$k$-max池化层。由于动态池化，能够在具有不同结构的句子中识别不同类型和长度的特征，而无需对输入进行填充。这不仅允许识别短程依赖关系，还允许识别长程依赖关系。DCNN在需要语义理解的应用任务中进行了测试。在斯坦福情感树库
    [[95](#bib.bib95)] 中预测电影评论的情感以及在推文 [[145](#bib.bib145)] 中识别情感时，其表现优于所有比较模型。它在使用TREC数据库
    [[146](#bib.bib146)] 分类问题类型时也表现为顶尖。
- en: 'Between their requirement for such understanding and their ease of examination
    due to the typical encoder–decoder structure they use, neural machine translation
    (NMT) systems (Section [IV-G](#S4.SS7 "IV-G Machine Translation ‣ IV Applications
    of Natural Language Processing Using Deep Learning ‣ A Survey of the Usages of
    Deep Learning for Natural Language Processing")) are splendid testbeds for researching
    internal semantic representations. Poliak et al. [[147](#bib.bib147)] trained
    encoders on four different language pairs: English and Arabic, English and Spanish,
    English and Chinese, and English and German. The decoding classifiers were trained
    on four distinct datasets: Multi-NLI [[148](#bib.bib148)], which is an expanded
    version of SNLI [[149](#bib.bib149)], as well as three recast datasets from the
    JHU Decompositional Semantics Initiative [[150](#bib.bib150)] (FrameNet Plus or
    FN+ [[151](#bib.bib151)], Definite Pronoun Resolution or DPR [[152](#bib.bib152)],
    and Semantic Proto-Roles or SPR [[153](#bib.bib153)]). None of the results were
    particularly strong, although they were strongest in SPR. This led to the conclusion
    that NMT models do a poor job of capturing paraphrased information and fail to
    capture inferences that help in anaphora resolution. (e.g. resolving gender).
    They did, however, find that the models learn about proto-roles (e.g. who or what
    is the recipient of an action). A concurrent work [[154](#bib.bib154)] analyzed
    the quality of many datasets used for natural language inference.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对这种理解的要求以及它们由于通常的编码器-解码器结构而易于检查，神经机器翻译（NMT）系统（第[IV-G](#S4.SS7 "IV-G Machine
    Translation ‣ IV Applications of Natural Language Processing Using Deep Learning
    ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")节）是研究内部语义表示的绝佳测试平台。Poliak等人
    [[147](#bib.bib147)] 对四种不同语言对的编码器进行了训练：英语和阿拉伯语、英语和西班牙语、英语和中文、以及英语和德语。解码分类器在四个不同的数据集上进行了训练：Multi-NLI
    [[148](#bib.bib148)]，这是SNLI [[149](#bib.bib149)] 的扩展版本，以及来自JHU Decompositional
    Semantics Initiative [[150](#bib.bib150)] 的三个重构数据集（FrameNet Plus 或 FN+ [[151](#bib.bib151)]，Definite
    Pronoun Resolution 或 DPR [[152](#bib.bib152)]，以及Semantic Proto-Roles 或 SPR [[153](#bib.bib153)]）。虽然结果没有特别强，但在SPR中表现最好。这导致了NMT模型在捕捉同义信息方面表现不佳，并且未能捕捉有助于指代解析的推理（例如，解决性别问题）。然而，他们发现模型能够学习关于原型角色的信息（例如，谁或什么是动作的接受者）。一项相关工作
    [[154](#bib.bib154)] 分析了用于自然语言推理的许多数据集的质量。
- en: 'Herzig and Berant [[155](#bib.bib155)] found that training semantic parsers
    on a single domain, as is often done, is less effective than training across many
    domains. This conclusion was drawn after testing three LSTM-based models. The
    first model was a one-to-one model, in which a single encoder and single decoder
    were used, requiring the network itself to determine the domain of the input.
    In the second model, a many-to-many model, a decoder was used for each domain,
    as were two encoders: the domain specific encoder and a multidomain encoder. The
    third model was a one-to-many model, using a single encoder, but separate decoders
    for each domain. Each model was trained on the “OVERNIGHT” dataset [[156](#bib.bib156)].
    Exceptional results were achieved for all models, with a state-of-the-art performance
    exhibited by the one-to-one model.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Herzig 和 Berant [[155](#bib.bib155)] 发现，在单一领域上训练语义解析器，如常做的那样，效果不如在多个领域上训练。这个结论是在测试了三个基于
    LSTM 的模型后得出的。第一个模型是一个一对一模型，其中使用了一个编码器和一个解码器，要求网络自行确定输入的领域。在第二个模型中，使用了一个多对多模型，每个领域使用一个解码器，并且有两个编码器：领域特定编码器和多领域编码器。第三个模型是一个一对多模型，使用一个编码器，但每个领域使用单独的解码器。每个模型都在“OVERNIGHT”数据集[[156](#bib.bib156)]
    上进行训练。所有模型均取得了卓越的结果，其中一对一模型展示了最先进的性能。
- en: Similar conclusions were drawn by Brunner et al. [[157](#bib.bib157)]. who created
    several LSTM-based encoder–decoder networks, and analyzed the embedding vectors
    produced. A single encoder accepting English sentences as input was used, as were
    four different decoders. The first such decoder was a replicating decoder, which
    reproduced the original English input. The second and third decoders translated
    the text into German and French. Finally, the fourth decoder was a POS tagger.
    Different combinations of decoders were used; one model had only the replicating
    decoder while others had two, three, or all four. Sentences of fourteen different
    structures from the EuroParl dataset [[158](#bib.bib158)] were used to train the
    networks. A set of test sentences were then fed to the encoders and their output
    analyzed. In all cases, fourteen clusters were formed, each corresponding to one
    of the sentence structures. Analysis showed that adding more decoders led to more
    correct and more definitive clusters. In particular, using all four of the decoders
    led to zero error. Furthermore, the researchers confirmed a hypothesis that just
    as logical arithmetic can be performed on word embeddings, so can it be performed
    on sentence embeddings.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Brunner 等人[[157](#bib.bib157)] 得出了类似的结论。他们创建了几个基于 LSTM 的编码器–解码器网络，并分析了产生的嵌入向量。使用了一个接受英语句子作为输入的编码器，以及四个不同的解码器。第一个解码器是一个复制解码器，它复现了原始的英语输入。第二个和第三个解码器将文本翻译成德语和法语。最后，第四个解码器是一个
    POS 标注器。使用了不同组合的解码器；一个模型只有复制解码器，而其他模型则有两个、三个或全部四个解码器。使用了来自 EuroParl 数据集[[158](#bib.bib158)]
    的十四种不同结构的句子来训练网络。然后，将一组测试句子输入到编码器中，并分析其输出。在所有情况下，形成了十四个簇，每个簇对应一种句子结构。分析显示，添加更多解码器会导致更多正确且更明确的簇。特别是，使用所有四个解码器可以达到零错误。此外，研究人员确认了一个假设，即正如可以在词嵌入上进行逻辑算术一样，也可以在句子嵌入上进行。
- en: III-D3 Semantic Challenges
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: III-D3 语义挑战
- en: In addition to the challenges already mentioned, researchers believe that being
    able to solve tasks well does not indicate actual understanding. Integrating deep
    networks with general word-graphs (e.g. WordNet [[159](#bib.bib159)]) or knowledge-graphs
    (e.g. DBPedia [[160](#bib.bib160)]) may be able to endow a sense of understanding.
    Graph-embedding is an active area of research [[161](#bib.bib161)], and work on
    integrating language-based models and graph models has only recently begun to
    take off, giving hope for better machine understanding.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 除了已提到的挑战，研究人员认为，能够很好地解决任务并不代表实际理解。将深度网络与通用词图（如 WordNet [[159](#bib.bib159)]）或知识图谱（如
    DBPedia [[160](#bib.bib160)]）结合起来，可能能够赋予理解感。图嵌入是一个活跃的研究领域[[161](#bib.bib161)]，关于语言模型与图模型的结合的工作最近才开始起步，这给更好的机器理解带来了希望。
- en: III-E Summary of Core Issues
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: III-E 核心问题总结
- en: Deep learning has generally performed very well, surpassing existing states
    of the art in many individual core NLP tasks, and has thus created the foundation
    on which useful natural language applications can and are being built. However,
    it is clear from examining the research reviewed here that natural language is
    an enigmatically complex topic, with myriad core or basic tasks, of which deep
    learning has only grazed the surface. It is also not clear how architectures for
    ably executing individual core tasks can be synthesized to build a common edifice,
    possibly a much more complex distributed neural architecture, to show competence
    in multiple or “all” core tasks. More fundamentally, it is also not clear, how
    mastering of basic tasks, may lead to superior performance in applied tasks, which
    are the ultimate engineering goals, especially in the context of building effective
    and efficient deep learning models. Many, if not most, successful deep learning
    architectures for applied tasks, discussed in the next section, seem to forgo
    explicit architectural components for core tasks, and learn such tasks implicitly.
    Thus, some researchers argue that the relevance of the large amount of work on
    core issues is not fully justified, while others argue that further extensive
    research in such areas is necessary to better understand and develop systems which
    more perfectly perform these tasks, whether explicitly or implicitly.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习一般表现非常出色，超越了许多核心NLP任务的现有技术水平，因此为有用的自然语言应用打下了基础。然而，从审视这里回顾的研究可以看出，自然语言是一个极其复杂的主题，具有无数核心或基本任务，而深度学习仅仅触及了表面。如何将有效执行个别核心任务的架构综合起来，以构建一个可能更复杂的分布式神经架构，从而在多个或“所有”核心任务中表现出能力，目前也不清楚。从更根本的角度来看，掌握基本任务如何可能导致在应用任务中表现优异，这些应用任务才是终极工程目标，特别是在构建有效和高效的深度学习模型的背景下，也不清楚。许多，甚至大多数，成功的深度学习架构对于应用任务，在下一节中讨论的，似乎放弃了核心任务的明确架构组件，而是隐式地学习这些任务。因此，一些研究人员认为，大量关于核心问题的工作相关性并不完全充分，而其他人则认为，在这些领域进行进一步的广泛研究是必要的，以更好地理解和开发那些更完美地执行这些任务的系统，无论是显式的还是隐式的。
- en: IV Applications of Natural Language Processing Using Deep Learning
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IV 深度学习应用于自然语言处理
- en: '![Refer to caption](img/0e10df8007585f98c2cd92885e2b5afd.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0e10df8007585f98c2cd92885e2b5afd.png)'
- en: 'Figure 4: Publication Volume for Applied Areas of NLP. All areas of applied
    natural language processing discussed have witnessed growth in recent years, with
    the largest growth occurring in the last two to three years.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：应用NLP领域的出版量。所有讨论的自然语言处理应用领域近年来都见证了增长，其中最大增长发生在过去两到三年。
- en: While the study of core areas of NLP is important to understanding how neural
    models work, it is meaningless in and of itself from an engineering perspective,
    which values applications that benefit humanity, not pure philosophical and scientific
    inquiry. Current approaches to solving several immediately useful NLP tasks are
    summarized here. Note that the issues included here are only those involving the
    processing of text, not the processing of verbal speech. Because speech processing
    [[162](#bib.bib162), [163](#bib.bib163)] requires expertise on several other topics
    including acoustic processing, it is generally considered another field of its
    own, sharing many commonalities with the field of NLP. The number of studies in
    each discussed area over the last decade is shown in Figure [4](#S4.F4 "Figure
    4 ‣ IV Applications of Natural Language Processing Using Deep Learning ‣ A Survey
    of the Usages of Deep Learning for Natural Language Processing")
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管研究NLP的核心领域对于理解神经模型的工作原理非常重要，但从工程学的角度来看，这本身并没有意义，工程学更看重那些对人类有益的应用，而非纯粹的哲学和科学探究。这里总结了几项当前对解决一些立即有用的NLP任务的现有方法。请注意，这里讨论的问题仅涉及文本处理，而不包括语音处理。因为语音处理[[162](#bib.bib162),
    [163](#bib.bib163)]需要涉及其他多个领域的专业知识，包括声学处理，所以通常被视为一个独立的领域，与NLP领域有许多共性。过去十年中讨论的每个领域的研究数量见图[4](#S4.F4
    "Figure 4 ‣ IV Applications of Natural Language Processing Using Deep Learning
    ‣ A Survey of the Usages of Deep Learning for Natural Language Processing")。
- en: IV-A Information Retrieval
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-A 信息检索
- en: The purpose of Information Retrieval (IR) systems is to help people find the
    right (most useful) information in the right (most convenient) format at the right
    time (when they need it) [[164](#bib.bib164)]. Among many issues in IR, a primary
    problem that needs addressing pertains to ranking documents with respect to a
    query string in terms of relevance scores for ad-hoc retrieval tasks, similar
    to what happens in a search engine.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 信息检索（IR）系统的目的是帮助人们在正确的时间（当他们需要时）以正确的格式（最方便的格式）找到正确（最有用）的信息[[164](#bib.bib164)]。在IR的众多问题中，一个主要的问题是需要解决如何根据查询字符串对文档进行排名，以便进行临时检索任务，这类似于搜索引擎的工作方式。
- en: Deep learning models for ad-hoc retrieval match texts of queries to texts of
    documents to obtain relevance scores. Thus, such models have to focus on producing
    representations of the interactions among individual words in the query and the
    documents. Some representation-focused approaches build deep learning models to
    produce good representations for the texts and then match the representations
    straightforwardly [[165](#bib.bib165), [133](#bib.bib133), [166](#bib.bib166)],
    whereas interaction-focused approaches first build local interactions directly,
    and then use deep neural networks to learn how the two pieces of text match based
    on word interactions [[133](#bib.bib133), [167](#bib.bib167), [168](#bib.bib168)].
    When matching a long document to a short query, the relevant portion can potentially
    occur anywhere in the long document and may also be distributed, thus, finding
    how each word in the query relates to portions of the document is helpful.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型用于临时检索，通过匹配查询文本和文档文本来获得相关性评分。因此，这些模型必须专注于生成查询与文档中单个词之间交互的表示。一些关注表示的方法构建深度学习模型以生成文本的良好表示，然后直接匹配这些表示[[165](#bib.bib165),
    [133](#bib.bib133), [166](#bib.bib166)]，而关注交互的方法则首先直接构建局部交互，然后使用深度神经网络学习基于词语交互的文本匹配[[133](#bib.bib133),
    [167](#bib.bib167), [168](#bib.bib168)]。在将长文档与短查询进行匹配时，相关部分可能出现在长文档的任何位置，并且可能是分布式的，因此，找到查询中的每个词如何与文档的各个部分相关是很有帮助的。
- en: Mindful of the specific needs for IR, Guo et al. [[169](#bib.bib169)] built
    a neural architecture called DRMM, enhancing an interaction-focused model that
    feeds quantized histograms of the local interaction intensities to an MLP for
    matching. In parallel, the query terms go through a small sub-network on their
    own to establish term importance and term dependencies. The outputs of the two
    parallel networks are mixed at the top so that the relevance of the document to
    the query can be better learned. DRMM achieved state-of-the-art performance for
    its time.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于信息检索（IR）的特定需求，郭等人[[169](#bib.bib169)] 构建了一种名为DRMM的神经网络架构，该架构增强了一种以交互为重点的模型，该模型将局部交互强度的量化直方图输入到多层感知机（MLP）进行匹配。同时，查询词会通过一个小的子网络单独处理，以确定词语的重要性和词语之间的依赖关系。两个并行网络的输出在顶部进行混合，从而可以更好地学习文档与查询的相关性。DRMM在其时达到了最先进的性能。
- en: Most current neural IR models are not end-to-end relevance rankers, but are
    re-rankers for documents a first-stage efficient traditional ranker has deemed
    relevant to a query. The representations the neural re-rankers learn are dense
    for both documents and queries, i.e., most documents in a collection seem to be
    relevant to a query, making it impossible to use such ANNs for ranking an entire
    collection of documents. In contrast, Zamani et al. [[170](#bib.bib170)] presented
    a standalone neural ranking model called SNRM_PRF, that learned sparse representations
    for both queries and documents, mimicking what traditional approaches do. Since
    queries are much shorter than documents and queries contain much less information
    than documents, it makes sense for query representations to be denser. This was
    achieved by using, during training, a sparsity objective combined with hinge loss.
    In particular, an $n$-gram representation for queries and documents was used.
    It passed the embedding of each word separately through an individual MLP and
    performed average pooling on top. During training, the approach used pseudo-relevant
    documents obtained by retrieving documents using existing models like TF-IDF and
    BM25, because of the lack of enough correctly labeled documents to train large
    ANN models. The approach created a 20,000 bit long inverted index for each document
    using the trained network, just like a traditional end-to-end approach. For retrieval,
    a dot product was computed between query and document representations to obtain
    the retrieval relevance score. The SNRM_PRF system obtained the best metrics (measured
    by MAP, P@20, nDCG@20, and Recall) across the board for two large datasets, Robust
    and ClueWeb.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当前大多数神经 IR 模型不是端到端的相关性排序器，而是对第一阶段高效传统排序器认为与查询相关的文档进行重新排序。神经重新排序器学习的表示对于文档和查询都是稠密的，即大多数文档似乎与查询相关，导致无法使用此类
    ANNs 对整个文档集合进行排序。相比之下，Zamani 等人 [[170](#bib.bib170)] 提出了一个独立的神经排序模型 SNRM_PRF，它为查询和文档学习了稀疏表示，模拟了传统方法的做法。由于查询比文档短得多且包含的信息比文档少，因此查询表示更稠密是合理的。这是通过在训练过程中使用结合了铰链损失的稀疏目标来实现的。特别是，为查询和文档使用了
    $n$-gram 表示。它将每个单词的嵌入单独通过一个 MLP，并在顶部进行平均池化。在训练过程中，该方法使用通过现有模型如 TF-IDF 和 BM25 检索的伪相关文档，因为缺乏足够的正确标注文档来训练大型
    ANN 模型。该方法使用训练网络为每个文档创建了 20,000 位长的倒排索引，就像传统的端到端方法一样。为了检索，计算查询和文档表示之间的点积以获得检索相关性评分。SNRM_PRF
    系统在两个大型数据集 Robust 和 ClueWeb 上获得了最好的指标（通过 MAP、P@20、nDCG@20 和 Recall 测量）。
- en: MacAveney et al. [[171](#bib.bib171)] extracted query term representations from
    two pre-trained contextualized language models, ELMo [[70](#bib.bib70)] and BERT
    [[71](#bib.bib71)], and used the representations to augment three existing competitive
    neural ranking architectures for ad-hoc document ranking, one of them being DRMM
    [[169](#bib.bib169)]. They also presented a joint model that combined BERT’s classification
    vector with these architectures to get benefits from both approaches. MacAveney’s
    system called CEDR (Contextualized Embeddings for Document Ranking) improved performance
    of all three prior models, and produced state-of-the-art results using BERT’s
    token representations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: MacAveney 等人 [[171](#bib.bib171)] 从两个预训练的上下文语言模型 ELMo [[70](#bib.bib70)] 和 BERT
    [[71](#bib.bib71)] 提取了查询词的表示，并利用这些表示来增强三种现有的竞争性神经排序架构用于即时文档排序，其中之一是 DRMM [[169](#bib.bib169)]。他们还提出了一个联合模型，将
    BERT 的分类向量与这些架构结合，以获得两种方法的优势。MacAveney 的系统 CEDR（上下文化嵌入文档排序）提升了所有三种之前模型的性能，并利用
    BERT 的 token 表现取得了最先进的结果。
- en: IV-B Information Extraction
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-B 信息提取
- en: Information extraction extracts explicit or implicit information from text.
    The outputs of systems vary, but often the extracted data and the relationships
    within it are saved in relational databases [[172](#bib.bib172)]. Commonly extracted
    information includes named entities and relations, events and their participants,
    temporal information, and tuples of facts.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 信息提取从文本中提取明确或隐含的信息。系统的输出有所不同，但提取的数据及其关系通常保存在关系型数据库中 [[172](#bib.bib172)]。常提取的信息包括命名实体及其关系、事件及其参与者、时间信息和事实元组。
- en: IV-B1 Named Entity Recognition
  id: totrans-132
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B1 命名实体识别
- en: Named entity recognition (NER) refers to the identification of proper nouns
    as well as information such as dates, times, prices, and product IDs. The multi-task
    approach of Collobert et al. [[9](#bib.bib9)] included the task, although no results
    were reported. In their approach, a simple feedforward network was used, having
    a context with a fixed sized window around each word. Presumably, this made it
    difficult to capture long-distance relations between words.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 命名实体识别（NER）指的是识别专有名词以及诸如日期、时间、价格和产品 ID 等信息。Collobert 等人的多任务方法[[9](#bib.bib9)]包括了这一任务，尽管没有报告结果。在他们的方法中，使用了一个简单的前馈网络，该网络具有一个固定大小窗口的上下文环绕每个单词。可以推测，这使得捕捉单词之间的远程关系变得困难。
- en: 'LSTMs were first used for NER by Hammerton [[173](#bib.bib173)]. The model,
    which was ahead of its time, had a small network due to the lack of available
    computing power at the time. Additionally, sophisticated numeric vector models
    for words were not yet available. Results were slightly better than the baseline
    for English and much better than the baseline for German. Dos Santos et al. [[174](#bib.bib174)]
    used a deep neural network architecture, known as CharWNN, which jointly used
    word-level and character-level inputs to perform sequential classification. In
    this study, a number of experiments were performed using the HAREM I annotated
    Portuguese corpus [[175](#bib.bib175)], and the SPA CoNLL2002 annotated Spanish
    corpus [[176](#bib.bib176)]. For the Portuguese corpus, CharWNN outperformed the
    previous state-of-the-art system across ten named entity classes. It also achieved
    state-of-the-art performance in Spanish. The authors noted that when used alone,
    neither word embeddings nor character level embeddings worked. This revalidated
    a fact long-known: Joint use of word-level and character-level features is important
    to effective NER performance.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 首次由 Hammerton [[173](#bib.bib173)] 用于 NER。该模型由于当时计算能力不足，网络规模较小。此外，复杂的数值向量模型尚未出现。结果在英语中略好于基线，在德语中远好于基线。Dos
    Santos 等人[[174](#bib.bib174)] 使用了一种深度神经网络架构，称为 CharWNN，该架构联合使用了词级和字符级输入来执行序列分类。在这项研究中，使用了
    HAREM I 注释的葡萄牙语语料库[[175](#bib.bib175)]和 SPA CoNLL2002 注释的西班牙语语料库[[176](#bib.bib176)]进行了一些实验。对于葡萄牙语语料库，CharWNN
    在十个命名实体类别中超越了之前的最先进系统。在西班牙语中也取得了最先进的性能。作者指出，当单独使用时，词嵌入和字符级嵌入都不起作用。这验证了一个早已知晓的事实：联合使用词级和字符级特征对有效的
    NER 性能至关重要。
- en: Chiu and Nichols [[177](#bib.bib177)] used a bidirectional LSTM with a character-level
    CNN resembling those used by dos Santos et al. [[174](#bib.bib174)]. Without using
    any private lexicons, detailed information about linked entities, or produce state-of-the-art
    results on the CoNLL-2003 [[178](#bib.bib178)] and OntoNotes [[179](#bib.bib179),
    [180](#bib.bib180)] datasets.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Chiu 和 Nichols [[177](#bib.bib177)] 使用了一个双向 LSTM 和一个字符级 CNN，类似于 dos Santos 等人使用的那些[[174](#bib.bib174)]。在没有使用任何私有词典、详细信息链接实体或在
    CoNLL-2003 [[178](#bib.bib178)] 和 OntoNotes [[179](#bib.bib179), [180](#bib.bib180)]
    数据集上产生最先进结果的情况下，进行了一些研究。
- en: Lample et al. [[181](#bib.bib181)] developed an architecture based on bidirectional
    LSTMs and conditional random fields (CRFs). The model used both character-level
    inputs and word embeddings. The inputs were combined and then fed to a bidirectional
    LSTM, whose outputs were in turn fed to a layer that performed CRF computations
    [[182](#bib.bib182)]. The model, when trained using dropout, obtained state-of-the-art
    performance in both German and Spanish. The LSTM-CRF model was also very close
    in both English and Dutch. The claim of this study was that state-of-the-art results
    were achieved without the use of any hand-engineered features or gazetteers.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Lample 等人[[181](#bib.bib181)]开发了一种基于双向 LSTM 和条件随机场（CRF）的架构。该模型同时使用了字符级输入和词嵌入。这些输入被结合起来，然后传递给一个双向
    LSTM，其输出再传递到一个执行 CRF 计算的层[[182](#bib.bib182)]。该模型在使用 dropout 进行训练时，在德语和西班牙语中取得了最先进的性能。LSTM-CRF
    模型在英语和荷兰语中的表现也非常接近。这项研究声称，在没有使用任何手工设计特征或词典的情况下取得了最先进的结果。
- en: Akbik et al. [[183](#bib.bib183)] achieved state-of-the-art performance in German
    and English NER using a pre-trained bidirectional character language model. They
    retrieved for each word a contextual embedding that they passed into a BiLSTM-CRF
    sequence labeler to perform NER.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: Akbik 等人[[183](#bib.bib183)] 使用预训练的双向字符语言模型，在德语和英语 NER 中取得了最先进的性能。他们为每个单词检索了一个上下文嵌入，并将其传递给
    BiLSTM-CRF 序列标注器以执行 NER。
- en: IV-B2 Event Extraction
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B2 事件抽取
- en: 'Event extraction is concerned with identifying words or phrases that refer
    to the occurrence of events, along with participants such as agents, objects,
    recipients, and times of occurrence. Event extraction usually deals with four
    sub-tasks: identifying event mentions, or phrases that describe events; identifying
    event triggers, which are the main words—usually verbs or gerunds—that specify
    the occurrence of the events; identifying arguments of the events; and identifying
    arguments’ roles in the events.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 事件提取关注于识别提及事件发生的词或短语，以及参与者如施事者、对象、接收者和发生时间。事件提取通常涉及四个子任务：识别事件提及，即描述事件的短语；识别事件触发词，这些是主要词汇——通常是动词或动名词——指定事件的发生；识别事件的论元；以及识别论元在事件中的角色。
- en: Chen et al. [[184](#bib.bib184)] argued that CNNs that use max-pooling are likely
    to capture only the most important information in a sentence, and as a result,
    might miss valuable facts when considering sentences that refer to several events.
    To address this drawback, they divided the feature map into three parts, and instead
    of using one maximum value, kept the maximum value of each part. In the first
    stage, they classified each word as either being a trigger word or non-trigger
    word. If triggers were found, the second stage aligned the roles of arguments.
    Results showed that this approach significantly outperformed other state-of-the-art
    methods of the time. The following year, Nguyen et al. [[185](#bib.bib185)] used
    an RNN-based encoder–decoder pair to identify event triggers and roles, exceeding
    earlier results. Liu et al. [[186](#bib.bib186)] presented a latent variable neural
    model to induce event schemas and extract open domain events, achieving best results
    on a dataset they created and released.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Chen等人[[184](#bib.bib184)]认为，使用最大池化的CNN可能只会捕捉句子中最重要的信息，因此，在考虑提到多个事件的句子时，可能会遗漏有价值的事实。为了应对这一缺陷，他们将特征图分为三部分，并且不是使用一个最大值，而是保留每部分的最大值。在第一阶段，他们将每个词分类为触发词或非触发词。如果发现了触发词，第二阶段将对论元的角色进行对齐。结果表明，这种方法显著优于当时的其他最先进方法。次年，Nguyen等人[[185](#bib.bib185)]使用基于RNN的编码器-解码器对来识别事件触发词和角色，超越了早期的结果。Liu等人[[186](#bib.bib186)]提出了一种潜变量神经模型，用于诱导事件模式和提取开放领域事件，在他们创建并发布的数据集上取得了最佳结果。
- en: IV-B3 Relationship Extraction
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-B3 关系提取
- en: Another important type of information extracted from text is that of relationships.
    These may be possessive, antonymous or synonymous relationships, or more natural,
    familial or geographic, relationships. The first deep learning approach was that
    of Zeng et al. [[23](#bib.bib23)], who used a simple CNN to classify a number
    of relationships between elements in sentences. Using only two layers, a window
    size of three, and word embeddings with only fifty dimensions they attained better
    results than any prior approach. Further work, by Zheng et al. [[187](#bib.bib187)],
    used a bidirectional LSTM and a CNN for relationship classification as well as
    entity recognition. More recently, Sun et al. [[188](#bib.bib188)] used an attention-based
    GRU model with a copy mechanism. This network was novel in its use of a data structure
    known as a coverage mechanism [[189](#bib.bib189)], which helped ensure that all
    important information was extracted the correct number of times. Lin et al. [[190](#bib.bib190)]
    achieve state-of-the-art performance in clinical temporal relation extraction
    using the pre-trained BERT [[71](#bib.bib71)] model with supervised training on
    a biomedical dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 从文本中提取的另一类重要信息是关系。这些关系可能是所有格的、反义的或同义的，或更自然的、家庭的或地理的关系。最早的深度学习方法是由Zeng等人提出的[[23](#bib.bib23)]，他们使用了一个简单的CNN来分类句子中元素之间的关系。仅使用了两层、窗口大小为三以及只有五十维的词嵌入，他们取得了比任何之前的方法更好的结果。之后，Zheng等人[[187](#bib.bib187)]使用了双向LSTM和CNN进行关系分类以及实体识别。最近，Sun等人[[188](#bib.bib188)]使用了基于注意力的GRU模型，并配备了复制机制。该网络在使用一种称为覆盖机制的数据结构[[189](#bib.bib189)]上具有新颖性，这有助于确保所有重要信息被正确提取。Lin等人[[190](#bib.bib190)]在使用预训练BERT[[71](#bib.bib71)]模型对生物医学数据集进行监督训练时，取得了临床时间关系提取的最先进性能。
- en: IV-C Text Classification
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-C 文本分类
- en: Another classic application for NLP is text classification, or the assignment
    of free-text documents to predefined classes. Document classification has numerous
    applications.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的另一个经典应用是文本分类，即将自由文本文档分配到预定义的类别中。文档分类有众多应用。
- en: Kim [[20](#bib.bib20)] was the first to use pretrained word vectors in a CNN
    for sentence-level classification. Kim’s work was motivating, and showed that
    simple CNNs, with one convolutional layer followed by a dense layer with dropout
    and softmax output, could achieve excellent results on multiple benchmarks using
    little hyperparameter tuning. The CNN models proposed were able to improve upon
    the state of the art on 4 out of 7 different tasks cast as sentence classification,
    including sentiment analysis and question classification. Conneau et al. [[191](#bib.bib191)]
    later showed that networks that employ a large number of convolutional layers
    work well for document classification.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Kim [[20](#bib.bib20)] 是第一个在 CNN 中使用预训练词向量进行句子级分类的人。Kim 的工作很有启发性，显示了简单的 CNN，采用一个卷积层加上一个带有
    dropout 和 softmax 输出的密集层，能够在多个基准测试中取得优秀的结果，且只需少量超参数调优。提出的 CNN 模型在 7 个不同的任务中有 4
    个任务（包括情感分析和问题分类）的表现优于现有技术。后来，Conneau 等人 [[191](#bib.bib191)] 显示，使用大量卷积层的网络在文档分类中表现良好。
- en: Jiang [[192](#bib.bib192)] used a hybrid architecture combining a deep belief
    network [[193](#bib.bib193)] and softmax regression [[194](#bib.bib194)]. (A deep
    belief network is a feedforward network where pairs of hidden layers are designed
    to resemble restricted Boltzmann machines [[195](#bib.bib195)], which are trained
    using unsupervised learning and are designed to increase or decrease dimensionality
    of data.) This was achieved by making passes over the data using forward and backward
    propagation many times until a minimum engery-based loss was found. This process
    was independent of the labeled or classification portion of the task, and was
    therefore initially trained without the softmax regression output layer. Once
    both sections of the architecture were pretrained, they were combined and trained
    like a regular deep neural net with backpropagation and quasi-Newton methods [[196](#bib.bib196)].
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: Jiang [[192](#bib.bib192)] 使用了一种混合架构，结合了深度置信网络 [[193](#bib.bib193)] 和 softmax
    回归 [[194](#bib.bib194)]。 （深度置信网络是一种前馈网络，其中一对隐藏层设计成类似于受限玻尔兹曼机 [[195](#bib.bib195)]，这些机器通过无监督学习进行训练，旨在增加或减少数据的维度。）这是通过多次进行前向和后向传播的过程来实现的，直到找到最小能量损失。这一过程与任务的标记或分类部分无关，因此最初在没有
    softmax 回归输出层的情况下进行训练。一旦两个架构部分都预训练完成，它们被结合并像常规深度神经网络一样使用反向传播和拟牛顿方法 [[196](#bib.bib196)]
    进行训练。
- en: Adhikari et al. [[197](#bib.bib197)] used BERT [[71](#bib.bib71)] to obtain
    state-of-the-art classification results on four document datasets.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Adhikari 等人 [[197](#bib.bib197)] 使用 BERT [[71](#bib.bib71)] 在四个文档数据集上获得了最先进的分类结果。
- en: While deep learning is promising for many areas of NLP, including text classification,
    it is not necessarily the end-all-be-all, and many hurdles are still present.
    Worsham and Kalita [[198](#bib.bib198)] found that for the task of classifying
    long full-length books by genre, gradient boosting trees are superior to neural
    networks, including both CNNs and LSTMs.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在包括文本分类在内的许多 NLP 领域中表现出色，但它并非万无一失，仍然存在许多障碍。Worsham 和 Kalita [[198](#bib.bib198)]
    发现，在按类别分类长篇书籍的任务中，梯度提升树优于神经网络，包括 CNN 和 LSTM。
- en: IV-D Text Generation
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-D 文本生成
- en: Many NLP tasks require the generation of human-like language. Summarization
    and machine translation convert one text to another in a sequence-to-sequence
    (seq2seq) fashion. Other tasks, such as image and video captioning and automatic
    weather and sports reporting, convert non-textual data to text. Some tasks, however,
    produce text without any input data to convert (or with only small amounts used
    as a topic or guide). These tasks include poetry generation, joke generation,
    and story generation.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 许多 NLP 任务需要生成类似人类的语言。摘要和机器翻译以序列到序列（seq2seq）的方式将一种文本转换成另一种文本。其他任务，如图像和视频标注以及自动天气和体育报道，则将非文本数据转换为文本。然而，一些任务在没有任何输入数据可转换（或仅使用少量数据作为主题或指南）的情况下生成文本。这些任务包括诗歌生成、笑话生成和故事生成。
- en: IV-D1 Poetry Generation
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D1 诗歌生成
- en: Poetry generation is arguably the hardest of the generation subtasks, as in
    addition to producing creative content, the content must be delivered in an aesthetic
    manner, usually following a specific structure. As with most tasks requiring textual
    output, recurrent models are the standard. However, while recurrent networks are
    great at learning internal language models, they do a poor job of producing structured
    output or adhering to any single style. Wei et al. [[199](#bib.bib199)] addressed
    the style issue by training using particular poets and controlling for style in
    Chinese poetry. They found that with enough training data, adequate results could
    be achieved. The structure problem was addressed by Hopkins and Kiela [[200](#bib.bib200)],
    who generated rhythmic poetry by training the network on only a single type of
    poem to ensure produced poems adhered to a single rhythmic structure. Human evaluators
    judged poems produced to be of lower quality than, but indistinguishable from,
    human produced poems.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 诗歌生成可以说是所有生成子任务中最困难的，因为除了要创作富有创意的内容外，内容还必须以美学的方式呈现，通常遵循特定的结构。与大多数需要文本输出的任务一样，递归模型是标准选择。然而，虽然递归网络在学习内部语言模型方面表现出色，但它们在生成结构化输出或遵循单一风格方面表现不佳。Wei
    等人 [[199](#bib.bib199)] 通过训练特定诗人并控制风格解决了风格问题。他们发现，只要训练数据足够，能够取得满意的结果。结构问题则由 Hopkins
    和 Kiela [[200](#bib.bib200)] 解决，他们通过仅训练网络于单一类型的诗歌以确保生成的诗歌遵循单一的韵律结构。人类评估者判断生成的诗歌质量低于但无法与人工诗歌区分。
- en: 'Another approach to poetry generation, beginning this year, has been to use
    pretrained language models. Specifically, Radford et al.’s GPT-2 model [[201](#bib.bib201)],
    the successor of the GPT model (Section [III-A7](#S3.SS1.SSS7 "III-A7 Recent Advances
    and Challenges ‣ III-A Language Modeling and Word Embeddings ‣ III Deep Learning
    in Core Areas of Natural Language Processing ‣ A Survey of the Usages of Deep
    Learning for Natural Language Processing")) has been used. Radford et al. hypothesised
    that alongside sequence-to-sequence learning and attention, language models can
    inherently start to learn text generation while training over a vast dataset.
    As of late 2019, these pre-trained GPT-2 models are arguably the most effective
    and prolific neural natural language generators. Bena and Kalita [[202](#bib.bib202)]
    used the 774 million parameter GPT-2 model to generate high-quality poems in English,
    demonstrating and eliciting emotional response in readers. (Two other models are
    available: 355 million parameters, and as of Novemeber 2019, 1.5 billion parameters.)
    Tucker and Kalita [[203](#bib.bib203)] generated poems in several languages—English,
    Spanish, Ukrainian, Hindi, Bengali, and Assamese—using the 774 M model as well.
    This study provided astonishing results in the fact that GPT-2 was pre-trained
    on a large English corpus, yet with further training on only a few hundred poems
    in another language, it turns into a believable generator in that language, even
    for poetry.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 今年开始，另一种诗歌生成的方法是使用预训练语言模型。具体来说，Radford 等人的 GPT-2 模型 [[201](#bib.bib201)]，作为
    GPT 模型的继任者（详见 [III-A7](#S3.SS1.SSS7 "III-A7 Recent Advances and Challenges ‣ III-A
    Language Modeling and Word Embeddings ‣ III Deep Learning in Core Areas of Natural
    Language Processing ‣ A Survey of the Usages of Deep Learning for Natural Language
    Processing")）已被使用。Radford 等人假设，除了序列到序列学习和注意力机制，语言模型在对大量数据集进行训练时，能够本质上开始学习文本生成。到
    2019 年末，这些预训练的 GPT-2 模型可以说是最有效和最具生产力的神经自然语言生成器。Bena 和 Kalita [[202](#bib.bib202)]
    使用了 7.74 亿参数的 GPT-2 模型来生成高质量的英文诗歌，展示并引发了读者的情感反应。（还有两种其他模型：3.55 亿参数，以及截至 2019 年
    11 月，15 亿参数。）Tucker 和 Kalita [[203](#bib.bib203)] 也使用 7.74 亿模型生成了多种语言的诗歌——包括英文、西班牙文、乌克兰文、印地文、孟加拉文和阿萨姆文。这项研究提供了惊人的结果，因为
    GPT-2 是在一个大型英文语料库上预训练的，但经过在另一种语言中的几百首诗歌的进一步训练后，它竟然能在该语言中生成可信的诗歌。
- en: IV-D2 Joke and Pun Generation
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D2 笑话和双关语生成
- en: Another area which has received little attention is the use of deep learning
    for joke and pun generation. Yu et al. [[204](#bib.bib204)] generated homographic
    puns (puns which use multiple meanings of the same written word) using a small
    LSTM. The network produced sentences in which ambiguities were introduced by words
    with multiple meanings, although it did a poor job of making the puns humorous.
    The generated puns were classified by human evaluators as machine generated a
    majority of the time. The authors noted that training on pun data alone is not
    sufficient for generating good puns. Ren and Yang [[205](#bib.bib205)] used an
    LSTM to generate jokes, training on two datasets, one of which was a collection
    of short jokes from Conan O’Brien. Since many of these jokes pertain to current
    events, the network was also trained on a set of news articles. This gave context
    to the example jokes. Chippada and Saha [[206](#bib.bib206)] generated jokes,
    quotes, and tweets using the same neural network, using an additional input to
    specify which should be produced. It was found that providing more general knowledge
    of other types of language, and examples of non-jokes, increased the quality of
    the jokes produced.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个鲜有关注的领域是使用深度学习生成笑话和双关语。Yu等人[[204](#bib.bib204)]使用一个小型LSTM生成同形双关语（使用相同书写单词的多个含义的双关语）。网络生成的句子通过具有多个含义的词引入了歧义，尽管它在制造幽默方面表现较差。生成的双关语大多数时间被人工评估者分类为机器生成。作者指出，仅仅在双关语数据上训练是不够的，不能生成好的双关语。Ren和Yang[[205](#bib.bib205)]使用LSTM生成笑话，在两个数据集上进行训练，其中一个数据集包含了Conan
    O’Brien的短笑话。由于这些笑话中许多涉及当前事件，因此网络还在一组新闻文章上进行了训练。这为示例笑话提供了背景。Chippada和Saha[[206](#bib.bib206)]使用相同的神经网络生成笑话、名言和推文，使用额外的输入来指定应生成哪种类型。发现提供更多关于其他类型语言的一般知识和非笑话的示例，提升了生成笑话的质量。
- en: IV-D3 Story Generation
  id: totrans-156
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D3 故事生成
- en: While poetry and especially humor generation have not gained much traction,
    story generation has seen a recent rise in interest. Jain et al. [[207](#bib.bib207)]
    used RNN variants with attention to produce short stories from “one-liner” story
    descriptions. Another recent study of interest is that by Peng et al. [[208](#bib.bib208)],
    who used LSTMs to generate stories, providing an input to specify whether the
    story should have a happy or sad ending. Their model successfully did so while
    at the same time providing better coherence than non-controlled stories. More
    recent attempts at the task have used special mechanisms focusing on the “events”
    (or actions) in the stories [[209](#bib.bib209)] or on the entities (characters
    and important objects) [[210](#bib.bib210)]. Even with such constraints, generated
    stories generally become incoherent or lose direction rather shortly. Xu et al.
    [[211](#bib.bib211)] addressed this by using a “skeleton” based model to build
    general sentences and fill in important information. This did a great job of capturing
    only the most important information, but still provided only modest end results
    in human evaluation. Drissi et al. [[212](#bib.bib212)] followed a similar approach.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管诗歌，尤其是幽默生成尚未受到广泛关注，但故事生成最近引起了兴趣。Jain等人[[207](#bib.bib207)]使用带有注意力机制的RNN变体从“一句话”故事描述生成短篇故事。另一个值得关注的最近研究是Peng等人[[208](#bib.bib208)]的研究，他们使用LSTM生成故事，并提供输入以指定故事应有快乐或悲伤的结局。他们的模型成功实现了这一点，同时比非控制故事提供了更好的连贯性。最近在这个任务上的尝试使用了专门的机制，关注故事中的“事件”（或行动）[[209](#bib.bib209)]或实体（角色和重要对象）[[210](#bib.bib210)]。即使有这样的约束，生成的故事通常还是会很快变得不连贯或失去方向。Xu等人[[211](#bib.bib211)]通过使用基于“骨架”的模型来构建一般句子并填补重要信息来解决这一问题。这很好地捕捉了最重要的信息，但在人类评估中仍然只提供了适度的最终结果。Drissi等人[[212](#bib.bib212)]采用了类似的方法。
- en: The strongest models to date focus on creating high level overviews of stories
    before breaking them down into smaller components to convert to text. Huang et
    al. [[213](#bib.bib213)] generated short stories from images using a two-tiered
    network. The first constructed a conceptual overview while the second converted
    the overview into words. Fan et al. [[214](#bib.bib214)] used a hierarchical approach,
    based on CNNs, which beat out the non-hierarchical approach in blind comparison
    by human evaluators. Additionally, they found that self attention leads to better
    perplexity. They also developed a fusion model with a pretrained language model,
    leading to greater improvements. These results concur with those of an older study
    by Li et al. [[215](#bib.bib215)] who read documents in a hierarchical fashion
    and reproduced them in hierarchical fashion, achieving great results.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止最强大的模型集中在创建故事的高层次概述，然后将其分解成更小的部分以转换为文本。黄等人[[213](#bib.bib213)] 使用了一个双层网络从图像生成短篇故事。第一个层次构建了概念概述，而第二个层次则将概述转换为文字。范等人[[214](#bib.bib214)]
    使用了基于卷积神经网络（CNNs）的分层方法，这在盲目比较中优于非分层方法，并且他们发现自注意力机制能带来更好的困惑度。他们还开发了一个与预训练语言模型融合的模型，带来了更大的改进。这些结果与李等人[[215](#bib.bib215)]
    的早期研究结果一致，该研究以分层方式阅读和再现文档，取得了优异的成果。
- en: IV-D4 Text Generation with GANs
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D4 使用GANs生成文本
- en: In order to make stories seem more human-like, He et al. [[216](#bib.bib216)]
    used GANs (generative adversarial networks) to measure human-likeness of generated
    text, forcing the network toward more natural reading output. Generative Adversarial
    Networks are based on the concept of a minimax two-player game, in which a generative
    network and a discriminative network are designed to work against each other with
    the discriminator attempting to determine if examples are from the generative
    network or the training set, and the generator trying to maximize the number of
    mistakes made by the discriminator. RankGAN, the GAN used in the study, measured
    differences in embedding space, rather than in output tokens. This meant the story
    content was evaluated more directly, without respect to the specific words and
    grammars used to tell it. Rather than simply using standard metrics and minimizing
    loss, Tambwekar et al. [[217](#bib.bib217)] used reinforcement learning to train
    a text generation model. This taught the model to not only attempt to optimize
    metrics, but to generate stories that humans evaluated to be meaningful. Zhange
    et al. [[218](#bib.bib218)] used another modified GAN, referred to as textGAN,
    for text generation, employing an LSTM generator and a CNN discriminator, achieving
    a promising BLEU score and a high tendency to reproduce realistic-looking sentences.
    Generative adversarial networks have seen increasing use in text generation recently
    [[219](#bib.bib219), [220](#bib.bib220)].
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使故事看起来更具人类特征，何等人[[216](#bib.bib216)] 使用了生成对抗网络（GANs）来衡量生成文本的类人程度，推动网络生成更自然的阅读输出。生成对抗网络基于一种极小极大双人游戏的概念，其中生成网络和判别网络设计成相互对抗，判别器尝试确定样本是否来自生成网络或训练集，而生成器则试图最大化判别器犯错的次数。研究中使用的RankGAN在嵌入空间中测量差异，而非在输出标记中。这意味着故事内容被更直接地评估，而不考虑讲述故事时使用的具体单词和语法。Tambwekar等人[[217](#bib.bib217)]
    并非仅使用标准指标和最小化损失，而是使用强化学习训练文本生成模型。这使模型不仅优化指标，还生成被人类评估为有意义的故事。张等人[[218](#bib.bib218)]
    使用了另一种修改过的GAN，即textGAN，用于文本生成，采用了LSTM生成器和CNN判别器，取得了有希望的BLEU分数和较高的再现现实句子的倾向。生成对抗网络在文本生成中的使用最近逐渐增加[[219](#bib.bib219),
    [220](#bib.bib220)]。
- en: IV-D5 Text Generation with VAEs
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D5 使用VAEs生成文本
- en: Another interesting type of network is the variational autoencoder (VAE) [[221](#bib.bib221)].
    While GANs attempt to produce output indistinguishable (at least to the model’s
    discriminator) from actual samples, VAEs attempt to create output similar to samples
    in the training set [[222](#bib.bib222)]. Several recent studies have used VAEs
    for text generation [[223](#bib.bib223), [224](#bib.bib224)], including Wang et
    al. [[225](#bib.bib225)], who adapted it by adding a module for learning a guiding
    topic for sequence generation, producing good results.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种有趣的网络类型是变分自编码器（VAE）[[221](#bib.bib221)]。尽管GANs试图生成与实际样本不可区分的输出（至少对模型的判别器而言），VAE则试图创建与训练集样本相似的输出[[222](#bib.bib222)]。最近的一些研究使用了VAE进行文本生成[[223](#bib.bib223),
    [224](#bib.bib224)]，包括Wang等人[[225](#bib.bib225)]，他们通过添加一个学习指导主题的模块来改进VAE，取得了良好的结果。
- en: IV-D6 Summary of Text Generation
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: IV-D6 文本生成的总结
- en: Humor and poetry generation are still understudied topics. As machine generated
    texts improve, the desire for more character, personality, and color in the texts
    will almost certainly emerge. Hence, it can be expected that research in these
    areas will increase.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 幽默和诗歌生成仍然是研究不足的话题。随着机器生成文本的改进，对文本中更多个性、特点和色彩的渴望几乎肯定会出现。因此，可以预期这些领域的研究将会增加。
- en: While story generation is improving, coherence is still a major problem, especially
    for longer stories. This has been addressed in part, by Haltzman et al., who [[226](#bib.bib226)]
    have proposed “nucleus sampling” to help counteract this problem, performing their
    experiments using the GPT-2 model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管故事生成在不断改进，但连贯性仍然是一个主要问题，特别是对于较长的故事。Haltzman等人[[226](#bib.bib226)] 部分解决了这个问题，他们提出了“核采样”来帮助应对这个问题，并使用GPT-2模型进行了实验。
- en: In addition to issues with lack of creativity and coherence, creating metrics
    to measure any sort of creative task is difficult, and therefore, human evaluations
    are the norm, often utilizing Amazon’s Mechanical Turk. However, recent works
    have proposed metrics that make a large step toward reliable automatic evaluation
    of generated text [[227](#bib.bib227), [228](#bib.bib228)]. In addition to the
    more creative tasks surveyed here, a number of others were previously discussed
    by Gatt and Krahmer [[229](#bib.bib229)]. The use of deep learning for image captioning
    has been surveyed very recently [[230](#bib.bib230), [231](#bib.bib231)], and
    tasks that generate text given textual inputs are discussed in the following subsections.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 除了缺乏创造力和连贯性的问题外，创建衡量任何创造性任务的度量标准也是困难的，因此人类评估是常规做法，通常利用亚马逊的Mechanical Turk。然而，最近的工作提出了一些度量标准，大大迈向了对生成文本的可靠自动评估[[227](#bib.bib227),
    [228](#bib.bib228)]。除了这里调查的更具创造性的任务外，Gatt和Krahmer[[229](#bib.bib229)] 还讨论了许多其他任务。最近对图像标题生成的深度学习应用进行了调查[[230](#bib.bib230),
    [231](#bib.bib231)]，以下小节将讨论根据文本输入生成文本的任务。
- en: IV-E Summarization
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-E 摘要生成
- en: 'Summarization finds elements of interest in documents in order to produce an
    encapsulation of the most important content. There are two primary types of summarization:
    extractive and abstractive. The first focuses on sentence extraction, simplification,
    reordering, and concatenation to relay the important information in documents
    using text taken directly from the documents. Abstractive summaries rely on expressing
    documents’ contents through generation-style abstraction, possibly using words
    never seen in the documents [[48](#bib.bib48)].'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要生成通过识别文档中的兴趣元素来产生最重要内容的概述。摘要生成主要有两种类型：抽取式和生成式。抽取式摘要侧重于句子的提取、简化、重新排序和拼接，利用直接从文档中提取的文本传递重要信息。生成式摘要则通过生成风格的抽象来表达文档内容，可能使用文档中未出现过的词汇[[48](#bib.bib48)]。
- en: Rush et al. [[39](#bib.bib39)] introduced deep learning to summarization, using
    a feedforward neural networrk. The language model used an encoder and a generative
    beam search decoder. The initial input was given directly to both the language
    model and the convolutional attention-based encoder, which determined contextual
    importance surrounding the summary sentences and phrases. The performance of the
    model was comparable to other state-of-the-art models of the time.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Rush 等人[[39](#bib.bib39)] 将深度学习引入摘要生成中，使用了前馈神经网络。语言模型使用了编码器和生成式束搜索解码器。初始输入直接提供给语言模型和卷积注意力编码器，后者确定了总结句子和短语的上下文重要性。该模型的性能与当时的其他最先进模型相当。
- en: As in other areas, attention mechanisms have improved performance of encoder–decoder
    models. Krantz and Kalita [[232](#bib.bib232)] compared various attention models
    for abstractive summarization. A state-of-the-art approach developed by Paulus
    et al. [[40](#bib.bib40)] used a multiple intra-temporal attention encoder mechanism
    that considered not only the input text tokens, but also the output tokens used
    by the decoder for previously generated words. They also used similar hybrid cross-entropy
    loss functions to those proposed by Ranzato et al. [[233](#bib.bib233)], which
    led to decreases in training and execution by orders of magnitude. Finally, they
    recommended using strategies seen in reinforcement learning to modify gradients
    and reduce exposure bias, which has been noted in models trained exclusively via
    supervised learning. The use of attention also boosted accuracy in the fully convolutional
    model proposed by Gehring et al. [[234](#bib.bib234)], who implemented an attention
    mechanism for each layer.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如同其他领域一样，注意力机制提升了编码器–解码器模型的性能。Krantz 和 Kalita [[232](#bib.bib232)] 比较了用于抽象总结的各种注意力模型。Paulus
    等人 [[40](#bib.bib40)] 开发了一种最先进的方法，采用了多重时间内注意力编码机制，该机制不仅考虑输入文本标记，还考虑解码器用于以前生成词的输出标记。他们还使用了类似于
    Ranzato 等人 [[233](#bib.bib233)] 提出的混合交叉熵损失函数，这使得训练和执行时间缩短了几个数量级。最后，他们建议使用在强化学习中看到的策略来修改梯度并减少暴露偏差，这在仅通过监督学习训练的模型中已被注意到。注意力的使用也提升了
    Gehring 等人 [[234](#bib.bib234)] 提出的全卷积模型的准确性，他们为每一层实现了注意力机制。
- en: Zhang et al. [[235](#bib.bib235)] proposed an encoder-decoder framework, which
    generated an output sequence based on an input sequence in a two-stage manner.
    They encoded the input sequence using BERT [[71](#bib.bib71)]. The decoder had
    two stages. In the first stage, a Transformer-based decoder generated a draft
    output sequence. In the second stage, they masked each word of the draft sequence
    and fed it to BERT, and then by combining the input sequence and the draft representation
    generated by BERT, they used a Transformer-based decoder to predict the refined
    word for each masked position. Their model achieved state-of-the-art performance
    on the CNN/Daily Mail and New York Times datasets.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人 [[235](#bib.bib235)] 提出了一个编码器-解码器框架，该框架以两阶段的方式生成基于输入序列的输出序列。他们使用 BERT
    [[71](#bib.bib71)] 对输入序列进行编码。解码器有两个阶段。在第一阶段，基于 Transformer 的解码器生成了一个草稿输出序列。在第二阶段，他们对草稿序列的每个词进行了掩码，并将其输入
    BERT，然后结合输入序列和 BERT 生成的草稿表示，使用基于 Transformer 的解码器预测每个掩码位置的精炼词。他们的模型在 CNN/Daily
    Mail 和纽约时报数据集上达到了最先进的性能。
- en: IV-F Question Answering
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-F 问答
- en: Similar to summarization and information extraction, question answering (QA)
    gathers relevant words, phrases, or sentences from a document. QA returns this
    information in a coherent fashion in response to a request. Current methods resemble
    those of summarization.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于总结和信息提取，问答（QA）从文档中收集相关的词语、短语或句子。QA 以连贯的方式响应请求返回这些信息。当前的方法类似于总结方法。
- en: Wang et al. [[41](#bib.bib41)] used a gated attention-based recurrent network
    to match the question with an answer-containing passage. A self-matching attention
    mechanism was used to refine the machine representation by mapping the entire
    passage. Pointer networks were used to predict the location and boundary of an
    answer. These networks used attention-pooling vector representations of passages,
    as well as the words being analyzed, to model the critical tokens or phrases necessary.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[41](#bib.bib41)] 使用了基于门控注意力的递归网络来将问题与包含答案的段落匹配。使用了自匹配注意力机制，通过映射整个段落来细化机器表示。指针网络被用来预测答案的位置和边界。这些网络使用了段落的注意力池化向量表示，以及正在分析的词汇，以建模必要的关键标记或短语。
- en: Multicolumn CNNs were used by Dong et al. [[236](#bib.bib236)] to automatically
    analyze questions from multiple viewpoints. Parallel networks were used to extract
    pertinent information from input questions. Separate networks were used to find
    context information and relationships and to determine which forms of answers
    should be returned. The output of these networks was combined and used to rank
    possible answers.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Dong 等人 [[236](#bib.bib236)] 使用了多列 CNN 来自动从多个视角分析问题。并行网络用于从输入问题中提取相关信息。独立的网络用于寻找上下文信息和关系，并确定应返回哪些形式的答案。这些网络的输出被合并并用于对可能的答案进行排序。
- en: Santoro et al. [[237](#bib.bib237)] used relational networks (RNs) for summarization.
    First proposed by Raposo et al. [[238](#bib.bib238)], RNs are built upon an MLP
    architecture, with focus on relational reasoning, i.e. defining relationships
    among entities in the data. These feedforward networks implement a similar function
    among all pairs of objects in order to aggregate correlations among them. For
    input, the RNs took final LSTM representations of document sentences. These inputs
    were further paired with a representation of the information request given [[237](#bib.bib237)].
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Santoro 等人 [[237](#bib.bib237)] 使用了关系网络（RNs）进行摘要生成。关系网络最初由 Raposo 等人 [[238](#bib.bib238)]
    提出，建立在多层感知机（MLP）架构上，重点关注关系推理，即定义数据中实体之间的关系。这些前馈网络在所有对象对之间实现类似功能，以便聚合它们之间的相关性。作为输入，RNs
    采用了文档句子的最终 LSTM 表示。这些输入进一步与给定的信息请求的表示配对 [[237](#bib.bib237)]。
- en: BERT [[71](#bib.bib71)] achieved state of theart in QA experiments on SQuAD
    1.1 and SQuAD 2.0 datasets. Yang et al. [[239](#bib.bib239)] demonstrate an end-to-end
    question answering system that integrates BERT with the open-source Anserini information
    retrieval toolkit. This system is able to identify answers from a large corpus
    of Wikipedia articles in an end-to-end fashion, obtaining best results on a standard
    benchmark test collection.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: BERT [[71](#bib.bib71)] 在 SQuAD 1.1 和 SQuAD 2.0 数据集的 QA 实验中取得了最先进的成果。Yang 等人
    [[239](#bib.bib239)] 展示了一个端到端的问题回答系统，将 BERT 与开源的 Anserini 信息检索工具包集成。该系统能够从大量的维基百科文章中识别答案，并在标准基准测试集上取得了最佳结果。
- en: IV-G Machine Translation
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-G 机器翻译
- en: Machine translation (MT) is the quintessential application of NLP. It involves
    the use of mathematical and algorithmic techniques to translate documents in one
    language to another. Performing effective translation is intrinsically onerous
    even for humans, requiring proficiency in areas such as morphology, syntax, and
    semantics, as well as an adept understanding and discernment of cultural sensitivities,
    for both of the languages (and associated societies) under consideration [[48](#bib.bib48)].
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 机器翻译（MT）是自然语言处理（NLP）的典型应用。它涉及使用数学和算法技术将一种语言的文档翻译成另一种语言。即使是对人类来说，进行有效翻译也是一项繁重的任务，需要掌握形态学、句法和语义等领域的知识，并且对两种语言（及其相关社会）的文化敏感性有充分的理解和辨别能力
    [[48](#bib.bib48)]。
- en: The first attempt at neural machine translation (NMT) was that by Schwenk [[240](#bib.bib240)],
    although neural models had previously been used for the similar task of transliteration,
    converting certain parts of text, such as proper nouns, into different languages
    [[241](#bib.bib241)]. Schwenk used a feed-forward network with seven-word inputs
    and outputs, padding and trimming when necessary. The ability to translate from
    a sentence of one length to a sentence of another length came about with the introduction
    of encoder-decoder models.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 神经机器翻译（NMT）的首次尝试是由 Schwenk [[240](#bib.bib240)] 进行的，尽管之前已经使用神经模型处理类似的任务，即将文本中的某些部分（如专有名词）转换为不同的语言
    [[241](#bib.bib241)]。Schwenk 使用了一个前馈网络，处理七个词的输入和输出，必要时进行填充和修剪。通过引入编码器-解码器模型，实现了从一个长度的句子翻译到另一个长度的句子的能力。
- en: The first use of such a model, by Kalchbrenner and Blumson [[242](#bib.bib242)],
    stemmed from the success of continuous recurrent representations in capturing
    syntax, semantics, and morphology [[243](#bib.bib243)] in addition to the ability
    of RNNs to build robust language models [[29](#bib.bib29)]. This original NMT
    encoder-decoder model used a combination of generative convolutional and recurrent
    layers to encode and optimize a source language model and cast this into a target
    language. The model was quickly reworked and further studied by Cho et al. [[244](#bib.bib244)]
    and numerous novel and effective advances to this model have since been made [[38](#bib.bib38),
    [245](#bib.bib245)]. Encoder-decoder models have continuously defined the state
    of the art, being expanded to contain dozens of layers, with residual connections,
    attention mechanisms, and even residual attention mechanisms allowing the final
    decoding layer to attend to the first encoding layer [[246](#bib.bib246)]. State-of-the-art
    results have also been achieved by using numerous convolutional layers in both
    the encoder and decoder, allowing information to be viewed in several hierarchical
    layers rather than a multitude of recurrent steps [[234](#bib.bib234)]. Such derived
    models are continually improving, finding answers to the shortcomings of their
    predecessors and overcoming any need for hand engineering [[247](#bib.bib247)].
    Recent progress includes effective initialization of decoder hidden states, use
    of conditional gated attentional cells, removal of bias in embedding layers, use
    of alternative decoding phases, factorization of embeddings, and test time use
    of the beam search algorithm [[248](#bib.bib248), [249](#bib.bib249)].
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 这种模型的首次应用，由 Kalchbrenner 和 Blumson [[242](#bib.bib242)] 提出，源于连续递归表示在捕捉句法、语义和形态学方面的成功
    [[243](#bib.bib243)]，以及 RNNs 在构建强大语言模型方面的能力 [[29](#bib.bib29)]。这个最初的 NMT 编码器-解码器模型使用了生成卷积层和递归层的组合来编码和优化源语言模型，并将其转化为目标语言。该模型很快被
    Cho 等人 [[244](#bib.bib244)] 重新设计并进一步研究，自那时起，对该模型进行了众多新颖而有效的改进 [[38](#bib.bib38),
    [245](#bib.bib245)]。编码器-解码器模型不断定义了技术的前沿，被扩展为包含数十层，具有残差连接、注意机制，甚至残差注意机制，使得最终的解码层能够关注到第一个编码层
    [[246](#bib.bib246)]。通过在编码器和解码器中使用大量卷积层，也达到了最先进的结果，使得信息可以在几个层次中查看，而不是在多个递归步骤中
    [[234](#bib.bib234)]。这种派生模型持续改进，解决了其前身的不足，并克服了对手工工程的需求 [[247](#bib.bib247)]。最近的进展包括有效的解码器隐藏状态初始化、条件门控注意单元的使用、嵌入层中的偏差移除、替代解码阶段的使用、嵌入的分解以及测试时使用束搜索算法
    [[248](#bib.bib248), [249](#bib.bib249)]。
- en: 'The standard initialization for the decoder state is that proposed by Bahdanau
    et al. [[38](#bib.bib38)], using the last backward encoder state. However, as
    noted by Britz et al. [[247](#bib.bib247)], using the average of the embedding
    or annotation layer seems to lead to the best translations. Gated recurrent cells
    have been the gold standard for sequence-to-sequence tasks, a variation of which
    is a conditional GRU (cGRU) [[248](#bib.bib248)], most effectively utilized with
    an attention mechanism. A cGRU cell consists of three key components: two GRU
    transition blocks and an attention mechanism between them. These three blocks
    combine the previous hidden state, along with the attention context window to
    generate the next hidden state. Altering the decoding process [[38](#bib.bib38)]
    from Look at input, Generate output token, Update hidden representation to a process
    of Look, Update, Generate can simplify the final decoding. Adding further source
    attributes such as morphological segmentation labels, POS tags, and syntactic
    dependency labels improves models, and concatenating or factorizing these with
    embeddings increases robustness further [[250](#bib.bib250), [248](#bib.bib248)].
    For remembering long-term dependencies, vertically stacked recurrent units have
    been the standard, with the optimum number of layers having been determined to
    be roughly between two and sixteen [[247](#bib.bib247)], depending on the desired
    input length as well as the presence and density of residual connections. At test
    time, a beam search algorithm can be used beside the final softmax layer for considering
    multiple target predictions in a greedy fashion, allowing the best predictions
    to be found without looking through the entire hypothesis space [[249](#bib.bib249)].'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器状态的标准初始化方法是Bahdanau等人提出的[[38](#bib.bib38)]，使用最后的反向编码器状态。然而，正如Britz等人所指出的[[247](#bib.bib247)]，使用嵌入层或注释层的平均值似乎能得到最佳的翻译。门控递归单元一直是序列到序列任务的金标准，其中一种变体是条件GRU（cGRU）[[248](#bib.bib248)]，最有效地与注意力机制结合使用。cGRU单元由三个关键组件组成：两个GRU转换块和它们之间的注意力机制。这三个块结合了先前的隐藏状态以及注意力上下文窗口来生成下一个隐藏状态。将解码过程从“查看输入，生成输出令牌，更新隐藏表示”改为“查看，更新，生成”可以简化最终的解码。添加额外的源属性如形态分割标签、POS标签和句法依赖标签可以改进模型，而将这些与嵌入进行连接或分解进一步提高了鲁棒性[[250](#bib.bib250),
    [248](#bib.bib248)]。为了记住长期依赖关系，垂直堆叠的递归单元一直是标准，其中最佳的层数被确定在大约两到十六层之间[[247](#bib.bib247)]，这取决于所需的输入长度以及残差连接的存在和密度。在测试时，可以在最终的softmax层旁边使用束搜索算法来考虑多个目标预测，以贪婪的方式找到最佳预测，而无需遍历整个假设空间[[249](#bib.bib249)]。
- en: 'In a direction diverging from previous work, Vaswani et al. [[42](#bib.bib42),
    [251](#bib.bib251)] proposed discarding the large number of recurrent and convolutional
    layers and instead focusing exclusively on attention mechanisms to encode a language
    globally from input to output. Preferring such ”self-attention” mechanisms over
    traditional layers is motivated by the following three principles: reducing the
    complexity of computations required per layer, minimizing sequential training
    steps, and lastly, abating the path length from input to output and its handicap
    on the learning of the long-range dependencies which are necessary in many sequencing
    tasks [[252](#bib.bib252)]. Apart from increased accuracy across translation tasks,
    self-attention models allow more parallelization throughout architectures, decreasing
    the training times and minimizing necessary sequential steps. At time of writing,
    the state-of-the-art model generating the best results for English to German and
    English to French on the IWSLT (International Workshop on Spoken Language Translation)
    2014 test corpus [[253](#bib.bib253)] is that of Medina and Kalita [[254](#bib.bib254)],
    which modified the model proposed by Vaswani to use parallel self-attention mechanisms,
    rather than stacking them as was done in the original model. In addition to improving
    BLEU (Bilingual Evaluation Understudy) scores [[255](#bib.bib255)], this also
    reduced training times. Ghazvininejad et al. [[256](#bib.bib256)] recently applied
    BERT to the machine translation task, using constant-time models. They were able
    to achieve relatively competitive performance in a fraction of the time. Lample
    et al. [[257](#bib.bib257)] attained state-of-the-art results, performing unsupervised
    machine translation using multiple languages in their language model pretraining.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在与之前工作的方向有所不同的情况下，Vaswani 等人[[42](#bib.bib42), [251](#bib.bib251)] 提出了丢弃大量的递归和卷积层，转而专注于仅使用注意力机制来从输入到输出全局地编码语言。选择这种“自注意力”机制而非传统层的原因有三个：减少每层所需计算的复杂性，最小化顺序训练步骤，以及减少从输入到输出的路径长度及其对学习长距离依赖的影响，这在许多序列任务中是必要的[[252](#bib.bib252)]。除了在翻译任务中提高准确率，自注意力模型还允许在架构中进行更多的并行化，减少训练时间并最小化所需的顺序步骤。在撰写时，为英语到德语和英语到法语的IWSLT（国际口语语言翻译研讨会）2014测试语料库[[253](#bib.bib253)]
    生成最佳结果的最先进模型是Medina 和 Kalita[[254](#bib.bib254)]的模型，该模型对Vaswani提出的模型进行了修改，使用并行自注意力机制，而不是像原始模型那样堆叠它们。除了提高BLEU（双语评价替代）分数[[255](#bib.bib255)]外，这也减少了训练时间。Ghazvininejad
    等人[[256](#bib.bib256)] 最近将BERT应用于机器翻译任务，使用了常数时间模型。他们能够在极短的时间内实现相对竞争力的性能。Lample
    等人[[257](#bib.bib257)] 达到了最先进的结果，在他们的语言模型预训练中执行了多语言的无监督机器翻译。
- en: 'Several of the recent state-of-the-art models were examined by Chen et al.
    [[258](#bib.bib258)]. The models were picked apart to determine which features
    were truly responsible for their strength and to provide a fair comparison. Hybrid
    models were then created using this knowledge, and incorporating the best parts
    of each previous model, outperforming the previous models. In addition to creating
    two models with both a self-attentive component and a recurrent component (in
    one model they were stacked, in the other parallel), they determined four techniques
    which they believe should always be employed, as they are crucial to some models,
    at best, and neutral to all models examined, at worst. These are label smoothing,
    multi-head attention, layer normalization, and synchronous training. Another study,
    by Denkowski et al. [[259](#bib.bib259)], examined a number of other techniques,
    recommending three: using Adam optimization, restarting multiple times, with learning
    rate annealing; performing subword translation; and using an ensemble of decoders.
    Furthermore, they tested a number of common techniques on models that were strong
    to begin, and determined that three of the four provided no additional benefits
    to, or actually hurt, the model, those three being lexicon-bias (priming the outputs
    with directly translated words), pre-translation (using translations from another
    model, usually of lower quality, as additional input), and dropout. They did find,
    however, that data-bootstrapping (using phrases that are parts of training examples
    as additional independent smaller samples) was advantageous even to models that
    are already high-performing. They recommended that future developments be tested
    on top performing models in order to determine their realm of effectiveness.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人对最近的几种最先进的模型进行了检验[[258](#bib.bib258)]。他们对这些模型进行了详细分析，以确定哪些特性真正负责它们的优势，并进行公平比较。随后，利用这些知识创建了混合模型，将每个先前模型的最佳部分整合在一起，从而超越了之前的模型。除了创建两个同时具有自注意力组件和递归组件的模型（一个模型中它们是堆叠的，另一个模型中是并行的），他们还确定了四种技术，这些技术在他们看来应始终使用，因为它们对一些模型至关重要，最糟情况下对所有研究过的模型都是中立的。这些技术是标签平滑、多头注意力、层归一化和同步训练。另一项由Denkowski等人进行的研究[[259](#bib.bib259)]检验了其他一些技术，推荐了三种：使用Adam优化、进行多次重启并进行学习率退火；执行子词翻译；以及使用解码器集成。此外，他们对本来就强大的模型测试了多种常见技术，发现其中三种技术对模型没有额外好处，甚至有害，这三种技术是词汇偏置（用直接翻译的词填充输出）、预翻译（使用另一个模型的翻译，通常质量较低，作为额外输入）和dropout。然而，他们发现数据自举（使用训练样本的短语作为额外独立的小样本）即使对已经高性能的模型也是有利的。他们建议未来的开发应在顶级性能模型上进行测试，以确定其有效性范围。
- en: In addition to studies presenting recommendations, one study has listed a number
    of challenges facing the field [[260](#bib.bib260)]. While neural machine translation
    models are superior to other forms of statistical machine translation models (as
    well as rule-based models), they require significantly more data, perform poorly
    outside of the domain in which they are trained, fail to handle rare words adequately,
    and do not do well with long sentences (more than about sixty words). Furthermore,
    attention mechanisms do not perform as well as their statistical counterparts
    for aligning words, and beam searches used for decoding only work when the search
    space is small. Surely these six drawbacks will be, or in some cases, will continue
    to be, the focus of much research in the coming years. Additionally, as mentioned
    in Section [III-D2](#S3.SS4.SSS2 "III-D2 Sentence Modeling ‣ III-D Semantics ‣
    III Deep Learning in Core Areas of Natural Language Processing ‣ A Survey of the
    Usages of Deep Learning for Natural Language Processing"), NMT models still struggle
    with some semantic concepts, which will also be a likely area of focus in years
    to come. While examining some of these failings of NMT can help, predicting the
    future of research and development in the field is nearly impossible.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 除了提出建议的研究之外，还有一项研究列出了该领域面临的若干挑战[[260](#bib.bib260)]。尽管神经机器翻译模型优于其他形式的统计机器翻译模型（以及基于规则的模型），但它们需要显著更多的数据，表现不佳于训练领域之外，对稀有词汇处理不足，且长句子（超过大约六十个词）处理效果差。此外，注意机制在对齐单词方面的表现不如其统计学对手，且用于解码的束搜索仅在搜索空间较小时有效。这六个缺点无疑将成为未来几年研究的重点。此外，如[III-D2](#S3.SS4.SSS2
    "III-D2 Sentence Modeling ‣ III-D Semantics ‣ III Deep Learning in Core Areas
    of Natural Language Processing ‣ A Survey of the Usages of Deep Learning for Natural
    Language Processing")部分所提到，NMT模型仍然在某些语义概念上存在困难，这也将是未来几年的研究重点。虽然检查NMT的一些失败之处可以提供帮助，但预测该领域未来的研究和发展几乎是不可能的。
- en: 'New models and methods are being reported on a daily basis with far too many
    advancements to survey, and state-of-the-art practices becoming outdated in a
    matter of months. Notable recent advancements include using caching to provide
    networks with greater context than simply the individual sentences being translated
    [[261](#bib.bib261)], the ability to better handle rare words [[262](#bib.bib262),
    [263](#bib.bib263)], and the ability to translate to and from understudied languages,
    such as those that are polysynthetic [[264](#bib.bib264)]. Additionally work has
    been conducted on the selection, sensitivity, and tuning of hyperparameters [[265](#bib.bib265)],
    denoising of data [[266](#bib.bib266)], and a number of other important topics
    surrounding neural machine translation. Finally, a new branch of machine translation
    has been opened up by groundbreaking research: multilingual translation.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 每天都有新模型和方法被报道，进展迅速以至于难以全面调查，最先进的实践在几个月内就会过时。近期值得注意的进展包括利用缓存提供比单个句子更丰富的上下文[[261](#bib.bib261)]，更好地处理稀有词汇[[262](#bib.bib262),
    [263](#bib.bib263)]，以及能够翻译到和从未充分研究的语言（如合成语语言）[[264](#bib.bib264)]。此外，还对超参数的选择、敏感性和调优[[265](#bib.bib265)]、数据去噪[[266](#bib.bib266)]及其他神经机器翻译相关的重要主题进行了研究。最后，突破性的研究开辟了机器翻译的新分支：多语种翻译。
- en: A fairly recent study [[267](#bib.bib267)] showed that a single, simple (but
    large) neural network could be trained to convert a number (up to at least twelve)
    of different languages to each other, automatically recognizing the source language
    and simply needing an input token to identify the output language. Furthermore,
    the model was found to be capable of understanding, at least somewhat, multilingual
    input, and of producing mixed outputs when multiple language tokens are given,
    sometimes even in languages related to, but not actually, those selected. This
    suggests that deep neural networks may be capable of learning universal representations
    for information, independent of language, and even more, that they might possibly
    be capable of learning some etymology and relationships between and among families
    of different languages.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的一项研究[[267](#bib.bib267)]表明，一个简单（但规模较大）的神经网络可以被训练来相互转换多种语言（至少达到十二种），自动识别源语言，仅需一个输入标记来识别输出语言。此外，研究发现该模型至少在一定程度上能够理解多语言输入，并在给定多个语言标记时生成混合输出，有时甚至是与选定语言相关但不完全相同的语言。这表明，深度神经网络可能能够学习信息的普遍表示，而不依赖于语言，甚至可能能够学习一些词源学和不同语言家族之间的关系。
- en: IV-H Summary of Deep Learning NLP Applications
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IV-H 深度学习自然语言处理应用总结
- en: Numerous other applications of natural language processing exist including grammar
    correction, as seen in word processors, and author mimicking, which, given sufficient
    data, generates text replicating the style of a particular writer. Many of these
    applications are infrequently used, understudied, or not yet exposed to deep learning.
    However, the area of sentiment analysis should be noted, as it is becoming increasingly
    popular and utilizing deep learning. In large part a semantic task, it is the
    extraction of a writer’s sentiment—their positive, negative, or neutral inclination
    towards some subject or idea [[268](#bib.bib268)]. Applications are varied, including
    product research, futures prediction, social media analysis, and classification
    of spam [[269](#bib.bib269), [270](#bib.bib270)]. The current state of the art
    uses an ensemble including both LSTMs and CNNs [[271](#bib.bib271)].
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理还有许多其他应用，包括语法纠正（如在文字处理软件中）和作者模仿。给定足够的数据，这些应用能够生成复制特定作家风格的文本。许多这些应用很少使用、研究不足，或者尚未接触深度学习。然而，情感分析领域值得注意，因为它正变得越来越受欢迎，并利用深度学习。情感分析主要是一项语义任务，即提取作家的情感——他们对某个主题或思想的积极、消极或中立倾向[[268](#bib.bib268)]。应用包括产品研究、期货预测、社交媒体分析和垃圾邮件分类[[269](#bib.bib269),
    [270](#bib.bib270)]。当前的最先进技术使用包括LSTM和CNN的集成方法[[271](#bib.bib271)]。
- en: This section has provided a number of select examples of the applied usages
    of deep learning in natural language processing. Countless studies have been conducted
    in these and similar areas, chronicling the ways in which deep learning has facilitated
    the successful use of natural language in a wide variety of applications. Only
    a minuscule fraction of such work has been referred to in this survey.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 本节提供了一些深度学习在自然语言处理中的应用示例。在这些及类似领域中，已经进行了无数研究，记录了深度学习如何促进自然语言在各种应用中的成功使用。本调查中仅提及了这些工作的极小一部分。
- en: While more specific recommendations for practitioners have been discussed in
    some individual subsections, the current trend in state-of-the-art models in all
    application areas is to use pre-trained stacks of Transformer units in some configuration,
    whether in encoder-decoder configurations or just as encoders. Thus, self-attention
    which is the mainstay of Transformer has become the norm, along with cross-attention
    between encoder and decoder units, if decoders are present. In fact, in many recent
    papers, if not most, Transformers have begun to replace LSTM units that were preponderant
    just a few months ago. Pre-training of these large Transformer models has also
    become the accepted way to endow a model with generalized knowledge of language.
    Models such as BERT, which have been trained on corpora of billions of words,
    are available for download, thus providing a practitioner with a model that possesses
    a great amount of general knowledge of language already. A practitioner can further
    train it with one’s own general corpora, if desired, but such training is not
    always necessary, considering the enormous sizes of the pre-training that downloaded
    models have received. To train a model to perform a certain task well, the last
    step a practitioner must go through is to use available downloadable task-specific
    corpora, or build one’s own task-specific corpus. This last training step is usually
    supervised. It is also recommended that if several tasks are to be performed,
    multi-task training be used wherever possible.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一些具体的建议在某些子章节中已有讨论，但当前所有应用领域的最先进模型趋势是使用预训练的Transformer单元堆栈，无论是在编码器-解码器配置中还是仅作为编码器。因此，作为Transformer支柱的自注意力已经成为常态，加上编码器和解码器单元之间的交叉注意力（如果存在解码器）。实际上，在许多近期论文中（如果不是大多数），Transformer已经开始取代几个月前占主导地位的LSTM单元。对这些大型Transformer模型的预训练也已成为赋予模型语言通用知识的公认方式。像BERT这样的模型，已经在数十亿单词的语料库上进行训练，可以下载，从而为从业者提供了一个已经具有大量语言通用知识的模型。若需要，从业者可以进一步用自己的一般语料库进行训练，但鉴于下载模型已接受的大量预训练，这种训练并非总是必要的。为了使模型在执行某项特定任务时表现良好，从业者必须经历的最后一步是使用可下载的任务特定语料库，或建立自己的任务特定语料库。这最后的训练步骤通常是有监督的。如果需要执行多个任务，还建议尽可能使用多任务训练。
- en: V Conclusions
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: V 结论
- en: Early applications of natural language processing included a well-acclaimed
    but simpleminded algebra word problem solver program called STUDENT [[272](#bib.bib272)],
    as well as interesting but severely constrained conversational systems such as
    Eliza, which acted as a “psycho-therapist” [[273](#bib.bib273)]), and another
    that conversed about manipulating blocks in a microworld [[274](#bib.bib274)].
    Nowadays, highly advanced applications of NLP are ubiquitous. These include Google’s
    and Microsoft’s machine translators, which translate more or less competently
    from a language to scores of other languages, as well as a number of devices which
    process voice commands and respond in like. The emergence of these sophisticated
    applications, particularly in deployed settings, acts as a testament to the impressive
    accomplishments that have been made in this domain over the last sixty or so years.
    Without a doubt, incredible progress has taken place, particularly in the last
    several years.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理的早期应用包括一个受到广泛好评但相对简单的代数文字问题解决程序，称为STUDENT[[272](#bib.bib272)]，以及一些有趣但极为受限的对话系统，如Eliza，它充当了一个“心理治疗师”[[273](#bib.bib273)]，还有另一个在微观世界中讨论操作块的系统[[274](#bib.bib274)]。如今，高度先进的NLP应用无处不在。这些应用包括Google和微软的机器翻译器，它们能从一种语言翻译成多个其他语言，基本上都能胜任，以及许多处理语音命令并作出回应的设备。这些复杂应用的出现，特别是在实际部署中，证明了在过去六十多年里该领域取得了令人印象深刻的成就。毫无疑问，特别是在最近几年，取得了令人难以置信的进步。
- en: As has been shown, this recent progress has a clear causal relationship with
    the remarkable advances in Artificial Neural Networks. Considered an “old” technology
    just a decade ago, these machine learning constructs have ushered in progress
    at an unprecedented rate, breaking performance records in myriad tasks in miscellaneous
    fields. In particular, deep neural architectures, have instilled models with higher
    performance in natural language tasks, in terms of “imperfect” metrics. Consolidating
    the analysis of all the models surveyed, a few general trends can be surmised.
    Both convolutional and recurrent specimen had contributed to the state of the
    art in the recent past, however, of very late, stacks of attention-powered Transformer
    units as encoders and often decoders, have consistently produced superior results
    across the rich and varying terrain of the NLP field. These models are generally
    heavily pre-trained on general language knowledge in an unsupervised or supervised
    manner, and somewhat lightly trained on specific tasks in a supervised fashion.
    Second, attention mechanisms alone, without recurrences or convolutions, seem
    to provide the best connections between encoders and decoders. Third, forcing
    networks to examine different features (by performing multiple tasks) usually
    improves results. Finally, while highly engineering networks usually optimizes
    results, there is no substitute for cultivating networks with large quantities
    of high quality data, although pre-training on large generic corpora seems to
    help immensely. Following from this final observation, it may be useful to direct
    more research effort toward pre-training methodologies, rather than developing
    highly-specialized components to squeeze the last drops of performance from complex
    models.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 正如所展示的，这一近期进展与人工神经网络的显著进步有着明显的因果关系。仅仅十年前被认为是“旧”技术的这些机器学习构造，已经以空前的速度带来了进展，在各种领域的多种任务中打破了性能记录。特别是，深度神经架构在自然语言任务中提供了更高的性能，就“缺陷”指标而言。对所有调查过的模型进行分析，可以总结出一些普遍趋势。卷积和递归模型在近期的最前沿中都有贡献，但最近，堆叠的基于注意力的Transformer单元作为编码器和通常作为解码器，在NLP领域的丰富多变的环境中
    consistently produced superior results。这些模型通常在无监督或监督的方式下对一般语言知识进行大量预训练，在特定任务上则进行相对较少的监督训练。其次，单独的注意力机制（没有递归或卷积）似乎提供了编码器和解码器之间最佳的连接。第三，迫使网络检查不同特征（通过执行多个任务）通常会改善结果。最后，尽管高度工程化的网络通常能优化结果，但没有什么可以替代用大量高质量数据来培养网络，尽管在大规模通用语料上的预训练似乎有极大的帮助。根据这一最终观察，可能更有用的是将更多的研究精力投入到预训练方法上，而不是开发高度专业化的组件以从复杂模型中榨取最后一滴性能。
- en: While the numerous stellar architectures being proposed each month are highly
    competitive, muddling the process of identifying a winning architecture, the methods
    of evaluation used add just as much complexity to the problem. Datasets used to
    evaluate new models are often generated specifically for those models and are
    then used only several more times, if at all, although consolidated datasets encompassing
    several tasks such as GLUE [[275](#bib.bib275)] have started to emerge. As the
    features and sizes of these datasets are highly variable, this makes comparison
    difficult. Most subfields of NLP, as well as the field as a whole, would benefit
    from extensive, large-scale discussions regarding the necessary contents of such
    datasets, followed by the compilation of such sets. In addition to high variability
    in evaluation data, there are numerous metrics used to evaluate performance on
    each task. Oftentimes, comparing similar models is difficult due to the fact that
    different metrics are reported for each. Agreement on particular sets of metrics
    would go a long way toward ensuring clear comparisons in the field.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每个月提出的众多卓越架构竞争激烈，使得识别获胜架构的过程变得混乱，但用于评估的方法同样增加了问题的复杂性。用于评估新模型的数据集通常是专门为这些模型生成的，之后仅被使用几次，即使有，也多是这样，尽管涵盖多个任务的整合数据集如GLUE
    [[275](#bib.bib275)] 已经开始出现。由于这些数据集的特征和规模差异极大，这使得比较变得困难。自然语言处理（NLP）的大多数子领域，以及整个领域，都将从对这些数据集必要内容的广泛、大规模讨论中受益，接着编制这样的数据集。除了评估数据的高度变异性，还有众多用于评估每个任务表现的指标。由于不同模型报告的指标不同，比较类似模型往往很困难。对特定指标集的达成一致将大大有助于确保领域内的清晰比较。
- en: Furthermore, metrics are usually only reported for the best case, with few mentions
    of average cases and variability, or of worst cases. While it is important to
    understand the possible performance of new models, it is just as important to
    understand the standard performance. If models produce highly variable results,
    they may take many attempts to train to the cutting-edge levels reported. In most
    cases, this is undesirable, and models that can be consistently trained to relatively
    high levels of performance are preferable. While increasingly large numbers of
    randomized parameters do reduce variation in performance, some variance will always
    exist, necessitating the reporting of more than just best-case metrics.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，度量通常仅针对最佳情况报告，且很少提及平均情况和变异性，或者最坏情况。虽然了解新模型可能的性能很重要，但了解标准性能同样重要。如果模型产生高度可变的结果，它们可能需要多次尝试才能训练到报告的前沿水平。在大多数情况下，这种情况是不理想的，能够一致地训练到相对较高性能水平的模型更为可取。虽然越来越多的随机参数确实减少了性能的变异，但总会存在一些变异，这需要报告的不仅仅是最佳情况度量。
- en: One final recommendation for future work is that it be directed toward a wider
    variety of languages than it is at present. Currently, the vast majority of research
    in NLP is conducted on the English language, with another sizeable portion using
    Mandarin Chinese. In translation tasks, English is almost always either the input
    or output language, with the other end usually being one of a dozen major European
    or Eastern Asian languages. This neglects entire families of languages, as well
    as the people who speak them. Many linguistic intricacies may not be expressed
    in any of the languages used, and therefore are not captured in current NLP software.
    Furthermore, there are thousands of languages spoken throughout the world, with
    at least eighty spoken by more than 10 million people, meaning that current research
    excludes an immense segment of humankind. Collection and validation of data in
    under-analyzed languages, as well as testing NLP models using such data, will
    be a tremendous contribution to not only the field of natural language processing,
    but to human society as a whole.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对未来工作的最后一个建议是，将其方向扩展到比现在更多样化的语言。目前，大多数自然语言处理的研究都集中在英语上，另一部分则使用普通话。在翻译任务中，英语几乎总是作为输入或输出语言，另一端通常是十几种主要的欧洲或东亚语言之一。这忽略了整个语言家族，以及讲这些语言的人。许多语言的细微差别可能不会在任何使用的语言中表达出来，因此也没有被当前的自然语言处理软件捕捉到。此外，世界上有数千种语言，其中至少有八十种语言的使用者超过1000万人，这意味着当前的研究排除了大量人群。在未充分分析的语言中收集和验证数据，以及使用这些数据测试自然语言处理模型，将对自然语言处理领域及整个人类社会作出巨大的贡献。
- en: Due to the small amounts of data available in many languages, the authors do
    not foresee the complete usurpation of traditional NLP models by deep learning
    any time in the near future. Deep learning models (and even shallow ANNs) are
    extremely data-hungry. Contrastingly, many traditional models require only relatively
    small amounts of training data. However, looking further forward, it can be anticipated
    that deep learning models will become the norm in computational linguistics, with
    pre-training and transfer learning playing highly impactful roles. Collobert et
    al. [[9](#bib.bib9)] sparked the deep learning revolution in NLP, although one
    of the key contributions of their work—that of a single unified model—was not
    realized widely. Instead, neural networks were introduced into traditional NLP
    tasks, and are only now reconnecting. In the field of parsing, for example, most
    models continue to implement non-neural structures, simply using ANNs on the side
    to make the decisions that were previously done using rules and probability models.
    While more versatile and general architectures are obviously becoming more and
    more of a reality, understanding the abstract concepts handled by such networks
    is important to understanding how to build and train better networks. Furthermore,
    as abstraction is a hallmark of human intelligence, understanding of the abstractions
    that take place inside an ANN may aid in the understanding of human intelligence
    and the processes that underlie it. Just as human linguistic ability is only a
    piece of our sentience, so is linguistic processing just a small piece of artificial
    intelligence. Understanding how such components are interrelated is important
    in constructing more complete AI systems, and creating a unified NLP architecture
    is another step toward making such a system a reality.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多语言的数据量较少，作者不预见深度学习会在不久的将来完全取代传统的自然语言处理模型。深度学习模型（甚至是浅层人工神经网络）对数据的需求极大。相比之下，许多传统模型只需要相对较少的训练数据。然而，从长远来看，可以预期深度学习模型将成为计算语言学中的常态，预训练和迁移学习将发挥重要作用。Collobert等人[[9](#bib.bib9)]引发了自然语言处理中的深度学习革命，尽管他们工作的一个关键贡献——统一模型——并未广泛实现。相反，神经网络被引入到传统的自然语言处理任务中，现在才开始重新连接。例如，在解析领域，大多数模型仍继续实现非神经结构，只是将人工神经网络作为辅助来做出以前由规则和概率模型做出的决策。虽然更通用和多功能的架构显然越来越成为现实，但理解这些网络处理的抽象概念对于了解如何构建和训练更好的网络非常重要。此外，由于抽象是人类智能的标志，理解人工神经网络内部发生的抽象过程可能有助于理解人类智能及其底层过程。正如人类语言能力只是我们知觉的一部分，语言处理也是人工智能的一小部分。理解这些组件如何相互关联对于构建更完整的人工智能系统至关重要，而创建一个统一的自然语言处理架构则是实现这一系统的又一步。
- en: 'This goal will also be aided by further advances in computational equipment.
    While GPUs have significantly improved the ability to train deep networks, they
    are only a step in the right direction [[276](#bib.bib276)]. The next step is
    the wider availability of chips designed specifically for this purpose, such as
    Google’s Tensor Processing Unit (TPU), Microsoft’s Catapult, and Intel’s Lake
    Crest [[277](#bib.bib277)]. Ultimately, artificial neural networks implemented
    in traditional von Neumann style computers may not be able to reach their full
    potential. Luckily, another old line of work in computer science and engineering
    has seen a resurgance in recent years: neuromorphic computing. With neuromorphic
    chips, which implement neural structures at the hardware level, expected much
    more widely in coming years [[278](#bib.bib278)], the continuation of deep learning
    and the longevity of its success can be highly anticipated, ensuring the opportunity
    for sustained progress in natural language processing.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个目标还将通过计算设备的进一步进展得到帮助。虽然GPU显著提高了训练深度网络的能力，但这仅仅是朝着正确方向迈出的第一步[[276](#bib.bib276)]。下一步是更广泛地使用专门为此目的设计的芯片，如谷歌的张量处理单元（TPU）、微软的Catapult和英特尔的Lake
    Crest[[277](#bib.bib277)]。**最终**，以传统冯·诺依曼风格计算机实现的人工神经网络可能无法发挥其全部潜力。幸运的是，计算机科学和工程领域的另一条旧路线近年来又出现了复兴：神经形态计算。随着神经形态芯片（在硬件层面实现神经结构）的预期广泛应用[[278](#bib.bib278)]，深度学习的延续和成功的持久性可以被高度期待，这确保了自然语言处理领域持续进步的机会。
- en: References
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] K. S. Jones, “Natural language processing: a historical review,” in *Current
    Issues in Computational Linguistics: in Honour of Don Walker*.   Springer, 1994,
    pp. 3–16.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] K. S. Jones，《自然语言处理：历史回顾》，见于*计算语言学的当前问题：献给唐·沃克*。Springer，1994，第3–16页。'
- en: '[2] E. D. Liddy, “Natural language processing,” 2001.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] E. D. Liddy，《自然语言处理》，2001年。'
- en: '[3] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, and N. Andrew, “Deep
    learning with cots hpc systems,” in *ICML*, 2013, pp. 1337–1345.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Coates, B. Huval, T. Wang, D. Wu, B. Catanzaro, 和 N. Andrew，《使用商用HPC系统的深度学习》，见于*ICML*，2013年，第1337–1345页。'
- en: '[4] R. Raina, A. Madhavan, and A. Y. Ng, “Large-scale deep unsupervised learning
    using graphics processors,” in *ICML*, 2009, pp. 873–880.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] R. Raina, A. Madhavan, 和 A. Y. Ng，《使用图形处理器的大规模深度无监督学习》，见于*ICML*，2009年，第873–880页。'
- en: '[5] I. Goodfellow, Y. Bengio, A. Courville, and Y. Bengio, *Deep learning*.   MIT
    Press, Cambridge, 2016, vol. 1.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] I. Goodfellow, Y. Bengio, A. Courville, 和 Y. Bengio，*深度学习*。MIT Press，剑桥，2016年，第1卷。'
- en: '[6] Y. LeCun, Y. Bengio, and G. Hinton, “Deep learning,” *Nature*, vol. 521,
    no. 7553, pp. 436–444, 2015.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Y. LeCun, Y. Bengio, 和 G. Hinton，《深度学习》，*自然*，第521卷，第7553期，第436–444页，2015年。'
- en: '[7] J. Schmidhuber, “Deep learning in neural networks: An overview,” *Neural
    networks*, vol. 61, pp. 85–117, 2015.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] J. Schmidhuber，《神经网络中的深度学习：概述》，*神经网络*，第61卷，第85–117页，2015年。'
- en: '[8] D. Ciresan, U. Meier, J. Masci, L. Maria Gambardella, and J. Schmidhuber,
    “Flexible, high performance convolutional neural networks for image classification,”
    in *IJCAI*, vol. 22, no. 1, 2011, p. 1237.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] D. Ciresan, U. Meier, J. Masci, L. Maria Gambardella, 和 J. Schmidhuber，《用于图像分类的灵活、高性能卷积神经网络》，见于*IJCAI*，第22卷，第1期，2011年，第1237页。'
- en: '[9] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, and P. Kuksa,
    “Natural language processing (almost) from scratch,” *Journal of Machine Learning
    Research*, vol. 12, pp. 2493–2537, 2011.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] R. Collobert, J. Weston, L. Bottou, M. Karlen, K. Kavukcuoglu, 和 P. Kuksa，《从头开始的自然语言处理》，*机器学习研究期刊*，第12卷，第2493–2537页，2011年。'
- en: '[10] Y. Goldberg, “Neural network methods for natural language processing,”
    *Synthesis Lectures on Human Language Technologies*, vol. 10, no. 1, pp. 1–309,
    2017.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Y. Goldberg，《自然语言处理的神经网络方法》，*人类语言技术讲座综合*，第10卷，第1期，第1–309页，2017年。'
- en: '[11] Y. Liu and M. Zhang, “Neural network methods for natural language processing,”
    2018.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Y. Liu 和 M. Zhang，《自然语言处理的神经网络方法》，2018年。'
- en: '[12] T. Young, D. Hazarika, S. Poria, and E. Cambria, “Recent trends in deep
    learning based natural language processing,” *ieee Computational intelligenCe
    magazine*, vol. 13, no. 3, pp. 55–75, 2018.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] T. Young, D. Hazarika, S. Poria, 和 E. Cambria，《基于深度学习的自然语言处理的最新趋势》，*IEEE计算智能杂志*，第13卷，第3期，第55–75页，2018年。'
- en: '[13] D. Rumelhart, G. Hinton, and R. Williams, “Learning internal representations
    by error propagation,” UCSD, Tech. Rep., 1985.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] D. Rumelhart, G. Hinton, 和 R. Williams，《通过误差传播学习内部表征》，UCSD，技术报告，1985年。'
- en: '[14] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    and L. D. Jackel, “Backpropagation applied to handwritten zip code recognition,”
    *Neural Computation*, vol. 1, no. 4, 1989.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard,
    和 L. D. Jackel，《反向传播应用于手写邮政编码识别》，*神经计算*，第1卷，第4期，1989年。'
- en: '[15] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based learning
    applied to document recognition,” *Proc of the IEEE*, vol. 86, no. 11, pp. 2278–2324,
    1998.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] Y. LeCun, L. Bottou, Y. Bengio, 和 P. Haffner，《基于梯度的学习应用于文档识别》，*IEEE汇刊*，第86卷，第11期，第2278–2324页，1998年。'
- en: '[16] K. Fukushima, “Neocognitron: A self-organizing neural network model for
    a mechanism of pattern recognition unaffected by shift in position,” *Bioological
    Cybernetics*, vol. 36, pp. 193–202, 1980.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] K. Fukushima，《Neocognitron：一种自组织神经网络模型，机制不受位置偏移影响的模式识别》，*生物控制论*，第36卷，第193–202页，1980年。'
- en: '[17] K. Fukushima and S. Miyake, “Neocognitron: A new algorithm for pattern
    recognition tolerant of deformations and shifts in position,” *Pattern Recognition*,
    vol. 15, no. 6, pp. 455–469, 1982.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] K. Fukushima 和 S. Miyake，《Neocognitron：一种对变形和位置偏移具有容忍性的模式识别新算法》，*模式识别*，第15卷，第6期，第455–469页，1982年。'
- en: '[18] Y. LeCun, Y. Bengio *et al.*, “Convolutional networks for images, speech,
    and time series,” *The Handbook of Brain Theory and Neural Networks*, vol. 3361,
    no. 10, 1995.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] Y. LeCun, Y. Bengio *等*，《用于图像、语音和时间序列的卷积网络》，*大脑理论与神经网络手册*，第3361卷，第10期，1995年。'
- en: '[19] A. Krizhevsky, “One weird trick for parallelizing convolutional neural
    networks,” *arXiv preprint arXiv:1404.5997*, 2014.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. Krizhevsky，《并行化卷积神经网络的一个奇怪技巧》，*arXiv预印本 arXiv:1404.5997*，2014年。'
- en: '[20] Y. Kim, “Convolutional neural networks for sentence classification,” *arXiv
    preprint arXiv:1408.5882*, 2014.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] Y. Kim，“用于句子分类的卷积神经网络，” *arXiv 预印本 arXiv:1408.5882*，2014年。'
- en: '[21] N. Kalchbrenner, E. Grefenstette, and P. Blunsom, “A convolutional neural
    network for modelling sentences,” *arXiv preprint arXiv:1404.2188*, 2014.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] N. Kalchbrenner, E. Grefenstette, 和 P. Blunsom，“用于建模句子的卷积神经网络，” *arXiv
    预印本 arXiv:1404.2188*，2014年。'
- en: '[22] C. N. Dos Santos and M. Gatti, “Deep convolutional neural networks for
    sentiment analysis of short texts.” in *COLING*, 2014, pp. 69–78.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] C. N. Dos Santos 和 M. Gatti，“用于短文本情感分析的深度卷积神经网络。” 见 *COLING*，2014年，页69–78。'
- en: '[23] D. Zeng, K. Liu, S. Lai, G. Zhou, J. Zhao *et al.*, “Relation classification
    via convolutional deep neural network.” in *COLING*, 2014.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] D. Zeng, K. Liu, S. Lai, G. Zhou, J. Zhao *等*，“通过卷积深度神经网络进行关系分类。” 见 *COLING*，2014年。'
- en: '[24] M. Kawato, K. Furukawa, and R. Suzuki, “A hierarchical neural-network
    model for control and learning of voluntary movement,” *Biological Cybernetics*,
    vol. 57, no. 3, pp. 169–185, 1987.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] M. Kawato, K. Furukawa, 和 R. Suzuki，“一个用于控制和学习自愿运动的层次神经网络模型，” *生物网络学*，第57卷，第3期，页169–185，1987年。'
- en: '[25] C. Goller and A. Kuchler, “Learning task-dependent distributed representations
    by backpropagation through structure,” in *IEEE International Conf on Neural Networks*,
    vol. 1, 1996, pp. 347–352.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] C. Goller 和 A. Kuchler，“通过结构反向传播学习任务依赖的分布式表示，” 见 *IEEE 国际神经网络大会*，第1卷，1996年，页347–352。'
- en: '[26] R. Socher, E. Huang, J. Pennin, C. Manning, and A. Ng, “Dynamic pooling
    and unfolding recursive autoencoders for paraphrase detection,” in *NIPS*, 2011,
    pp. 801–809.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] R. Socher, E. Huang, J. Pennin, C. Manning, 和 A. Ng，“动态池化和展开递归自编码器用于释义检测，”
    见 *NIPS*，2011年，页801–809。'
- en: '[27] J. L. Elman, “Finding structure in time,” *Cognitive Science*, vol. 14,
    no. 2, pp. 179–211, 1990.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] J. L. Elman，“时间中的结构发现，” *认知科学*，第14卷，第2期，页179–211，1990年。'
- en: '[28] L. Fausett, *Fundamentals of neural networks: architectures, algorithms,
    and applications*.   Prentice-Hall, Inc., 1994.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] L. Fausett, *神经网络基础：架构、算法和应用*。 Prentice-Hall, Inc.，1994年。'
- en: '[29] T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, and S. Khudanpur, “Recurrent
    neural network based language model,” in *Annual Conf of the Intnl Speech Communication
    Assoc*, vol. 2, 2010, p. 3.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] T. Mikolov, M. Karafiát, L. Burget, J. Černockỳ, 和 S. Khudanpur，“基于递归神经网络的语言模型，”
    见 *国际语音通信协会年会*，第2卷，2010年，页3。'
- en: '[30] T. Mikolov, S. Kombrink, L. Burget, J. Černockỳ, and S. Khudanpur, “Extensions
    of recurrent neural network language model,” in *IEEE ICASSP*, 2011, pp. 5528–5531.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] T. Mikolov, S. Kombrink, L. Burget, J. Černockỳ, 和 S. Khudanpur，“递归神经网络语言模型的扩展，”
    见 *IEEE ICASSP*，2011年，页5528–5531。'
- en: '[31] T. Mikolov, A. Deoras, D. Povey, L. Burget, and J. Černockỳ, “Strategies
    for training large scale neural network language models,” in *IEEE Workshop on
    Automatic Speech Recognition and Understanding*, 2011.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] T. Mikolov, A. Deoras, D. Povey, L. Burget, 和 J. Černockỳ，“训练大规模神经网络语言模型的策略，”
    见 *IEEE 自动语音识别与理解研讨会*，2011年。'
- en: '[32] J. Schmidhuber, “Learning complex, extended sequences using the principle
    of history compression,” *Neural Computation*, vol. 4, no. 2, pp. 234–242, 1992.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] J. Schmidhuber，“使用历史压缩原则学习复杂的扩展序列，” *神经计算*，第4卷，第2期，页234–242，1992年。'
- en: '[33] S. El Hihi and Y. Bengio, “Hierarchical recurrent neural networks for
    long-term dependencies,” in *NIPS*, 1996, pp. 493–499.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] S. El Hihi 和 Y. Bengio，“用于长期依赖的层次递归神经网络，” 见 *NIPS*，1996年，页493–499。'
- en: '[34] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Hochreiter 和 J. Schmidhuber，“长短期记忆，” *神经计算*，第9卷，第8期，页1735–1780，1997年。'
- en: '[35] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, and J. Schmidhuber,
    “Lstm: A search space odyssey,” *IEEE Transactions on Neural Networks and Learning
    Systems*, vol. 28, no. 10, 2017.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] K. Greff, R. K. Srivastava, J. Koutník, B. R. Steunebrink, 和 J. Schmidhuber，“LSTM：搜索空间的奥德赛，”
    *IEEE 神经网络与学习系统汇刊*，第28卷，第10期，2017年。'
- en: '[36] K. Cho, B. Van Merriënboer, D. Bahdanau, and Y. Bengio, “On the properties
    of neural machine translation: Encoder-decoder approaches,” *arXiv preprint arXiv:1409.1259*,
    2014.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] K. Cho, B. Van Merriënboer, D. Bahdanau, 和 Y. Bengio，“神经机器翻译的属性：编码器-解码器方法，”
    *arXiv 预印本 arXiv:1409.1259*，2014年。'
- en: '[37] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” *arXiv preprint arXiv:1412.3555*,
    2014.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] J. Chung, C. Gulcehre, K. Cho, 和 Y. Bengio，“门控递归神经网络在序列建模中的实证评估，” *arXiv
    预印本 arXiv:1412.3555*，2014年。'
- en: '[38] D. Bahdanau, K. Cho, and Y. Bengio, “Neural machine translation by jointly
    learning to align and translate,” *arXiv preprint arXiv:1409.0473*, 2014.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] D. Bahdanau, K. Cho 和 Y. Bengio，“通过联合学习对齐和翻译的神经机器翻译”，*arXiv 预印本 arXiv:1409.0473*，2014年。'
- en: '[39] A. Rush, S. Chopra, and J. Weston, “A neural attention model for abstractive
    sentence summarization,” *arXiv preprint arXiv:1509.00685*, 2015.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] A. Rush, S. Chopra 和 J. Weston，“用于抽象句子总结的神经注意力模型”，*arXiv 预印本 arXiv:1509.00685*，2015年。'
- en: '[40] R. Paulus, C. Xiong, and R. Socher, “A deep reinforced model for abstractive
    summarization,” *arXiv preprint arXiv:1705.04304*, 2017.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] R. Paulus, C. Xiong 和 R. Socher，“用于抽象总结的深度强化模型”，*arXiv 预印本 arXiv:1705.04304*，2017年。'
- en: '[41] W. Wang, N. Yang, F. Wei, B. Chang, and M. Zhou, “Gated self-matching
    networks for reading comprehension and question answering,” in *ACL*, vol. 1,
    2017, pp. 189–198.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] W. Wang, N. Yang, F. Wei, B. Chang 和 M. Zhou，“用于阅读理解和问题回答的门控自匹配网络”，在*ACL*，第1卷，2017年，页189–198。'
- en: '[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” in *NIPS*, 2017, pp.
    6000–6010.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser 和 I. Polosukhin，“注意力机制才是你所需要的一切”，在*NIPS*，2017年，页6000–6010。'
- en: '[43] Y. Bengio, P. Simard, and P. Frasconi, “Learning long-term dependencies
    with gradient descent is difficult,” *IEEE Transactions on Neural Networks*, vol. 5,
    no. 2, 1994.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Y. Bengio, P. Simard 和 P. Frasconi，“用梯度下降学习长期依赖关系是困难的”，*IEEE 神经网络汇刊*，第5卷，第2期，1994年。'
- en: '[44] V. Nair and G. E. Hinton, “Rectified linear units improve restricted boltzmann
    machines,” in *ICML*, 2010, pp. 807–814.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] V. Nair 和 G. E. Hinton，“修正线性单元改善限制玻尔兹曼机”，在*ICML*，2010年，页807–814。'
- en: '[45] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *IEEE CVPR*, 2016, pp. 770–778.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] K. He, X. Zhang, S. Ren 和 J. Sun，“用于图像识别的深度残差学习”，在*IEEE CVPR*，2016年，页770–778。'
- en: '[46] R. K. Srivastava, K. Greff, and J. Schmidhuber, “Highway networks,” *arXiv
    preprint arXiv:1505.00387*, 2015.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] R. K. Srivastava, K. Greff 和 J. Schmidhuber，“高速公路网络”，*arXiv 预印本 arXiv:1505.00387*，2015年。'
- en: '[47] G. Huang, Z. Liu, K. Q. Weinberger, and L. van der Maaten, “Densely connected
    convolutional networks,” in *IEEE CVPR*, vol. 1, no. 2, 2017.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] G. Huang, Z. Liu, K. Q. Weinberger 和 L. van der Maaten，“密集连接卷积网络”，在*IEEE
    CVPR*，第1卷，第2期，2017年。'
- en: '[48] D. Jurafsky and J. Martin, *Speech & language processing*.   Pearson Education,
    2000.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] D. Jurafsky 和 J. Martin，*语音与语言处理*。Pearson 教育，2000年。'
- en: '[49] Y. Bengio, R. Ducharme, P. Vincent, and C. Jauvin, “A neural probabilistic
    language model,” *J. of Machine Learning Research*, vol. 3, 2003.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Y. Bengio, R. Ducharme, P. Vincent 和 C. Jauvin，“一个神经概率语言模型”，*机器学习研究期刊*，第3卷，2003年。'
- en: '[50] W. De Mulder, S. Bethard, and M.-F. Moens, “A survey on the application
    of recurrent neural networks to statistical language modeling,” *Computer Speech
    & Language*, vol. 30, no. 1, pp. 61–98, 2015.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] W. De Mulder, S. Bethard 和 M.-F. Moens，“递归神经网络在统计语言建模中的应用调查”，*计算机语音与语言*，第30卷，第1期，页61–98，2015年。'
- en: '[51] R. Iyer, M. Ostendorf, and M. Meteer, “Analyzing and predicting language
    model improvements,” in *IEEE Workshop on Automatic Speech Recognition and Understanding*,
    1997, pp. 254–261.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] R. Iyer, M. Ostendorf 和 M. Meteer，“分析和预测语言模型的改进”，在*IEEE 自动语音识别与理解研讨会*，1997年，页254–261。'
- en: '[52] S. F. Chen, D. Beeferman, and R. Rosenfeld, “Evaluation metrics for language
    models,” 1998.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] S. F. Chen, D. Beeferman 和 R. Rosenfeld，“语言模型的评估指标”，1998年。'
- en: '[53] P. Clarkson and T. Robinson, “Improved language modelling through better
    language model evaluation measures,” *Computer Speech & Language*, vol. 15, no. 1,
    pp. 39–53, 2001.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] P. Clarkson 和 T. Robinson，“通过更好的语言模型评估措施改进语言建模”，*计算机语音与语言*，第15卷，第1期，页39–53，2001年。'
- en: '[54] M. Marcus, M. A. Marcinkiewicz, and B. Santorini, “Building a large annotated
    corpus of english: The penn treebank,” *Computational Linguistics*, vol. 19, no. 2,
    pp. 313–330, 1993.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] M. Marcus, M. A. Marcinkiewicz 和 B. Santorini，“建立大规模的英语注释语料库：Penn Treebank”，*计算语言学*，第19卷，第2期，页313–330，1993年。'
- en: '[55] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn, and T. Robinson,
    “One billion word benchmark for measuring progress in statistical language modeling,”
    *arXiv preprint arXiv:1312.3005*, 2013.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] C. Chelba, T. Mikolov, M. Schuster, Q. Ge, T. Brants, P. Koehn 和 T. Robinson，“用于测量统计语言建模进展的十亿字基准”，*arXiv
    预印本 arXiv:1312.3005*，2013年。'
- en: '[56] M. Daniluk, T. Rocktäschel, J. Welbl, and S. Riedel, “Frustratingly short
    attention spans in neural language modeling,” *arXiv preprint arXiv:1702.04521*,
    2017.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] M. Daniluk, T. Rocktäschel, J. Welbl 和 S. Riedel，“神经语言建模中的短暂注意力跨度”，*arXiv
    预印本 arXiv:1702.04521*，2017年。'
- en: '[57] K. Beneš, M. K. Baskar, and L. Burget, “Residual memory networks in language
    modeling: Improving the reputation of feed-forward networks,” *Interspeech 2017*,
    pp. 284–288, 2017.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] K. Beneš, M. K. Baskar, 和 L. Burget，“语言建模中的残差记忆网络：改善前馈网络的声誉”，*Interspeech
    2017*，第284–288页，2017年。'
- en: '[58] N.-Q. Pham, G. Kruszewski, and G. Boleda, “Convolutional neural network
    language models,” in *EMNLP*, 2016, pp. 1153–1162.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] N.-Q. Pham, G. Kruszewski, 和 G. Boleda，“卷积神经网络语言模型”，在*EMNLP*，2016年，第1153–1162页。'
- en: '[59] M. Lin, Q. Chen, and S. Yan, “Network in network,” *arXiv preprint arXiv:1312.4400*,
    2013.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. Lin, Q. Chen, 和 S. Yan，“网络中的网络”，*arXiv preprint arXiv:1312.4400*，2013年。'
- en: '[60] Y. Kim, Y. Jernite, D. Sontag, and A. M. Rush, “Character-aware neural
    language models.” in *AAAI*, 2016, pp. 2741–2749.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Y. Kim, Y. Jernite, D. Sontag, 和 A. M. Rush，“字符感知神经语言模型”，在*AAAI*，2016年，第2741–2749页。'
- en: '[61] J. Botha and P. Blunsom, “Compositional morphology for word representations
    and language modelling,” in *ICML*, 2014, pp. 1899–1907.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. Botha 和 P. Blunsom，“用于词表示和语言建模的组合形态学”，在*ICML*，2014年，第1899–1907页。'
- en: '[62] W. Zaremba, I. Sutskever, and O. Vinyals, “Recurrent neural network regularization,”
    *arXiv preprint arXiv:1409.2329*, 2014.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] W. Zaremba, I. Sutskever, 和 O. Vinyals，“递归神经网络正则化”，*arXiv preprint arXiv:1409.2329*，2014年。'
- en: '[63] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, and Y. Wu, “Exploring
    the limits of language modeling,” 2016.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] R. Jozefowicz, O. Vinyals, M. Schuster, N. Shazeer, 和 Y. Wu，“探索语言建模的极限”，2016年。'
- en: '[64] Y. Ji, T. Cohn, L. Kong, C. Dyer, and J. Eisenstein, “Document context
    language models,” *arXiv preprint arXiv:1511.03962*, 2015.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Y. Ji, T. Cohn, L. Kong, C. Dyer, 和 J. Eisenstein，“文档上下文语言模型”，*arXiv preprint
    arXiv:1511.03962*，2015年。'
- en: '[65] N. Shazeer, J. Pelemans, and C. Chelba, “Sparse non-negative matrix language
    modeling for skip-grams,” in *Interspeech 2015*, 2015.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] N. Shazeer, J. Pelemans, 和 C. Chelba，“用于跳字的稀疏非负矩阵语言建模”，在*Interspeech 2015*，2015年。'
- en: '[66] W. Williams, N. Prasad, D. Mrva, T. Ash, and T. Robinson, “Scaling recurrent
    neural network language models,” in *IEEE ICASSP*, 2015.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] W. Williams, N. Prasad, D. Mrva, T. Ash, 和 T. Robinson，“扩展递归神经网络语言模型”，在*IEEE
    ICASSP*，2015年。'
- en: '[67] T. Mikolov, K. Chen, G. Corrado, and J. Dean, “Efficient estimation of
    word representations in vector space,” *arXiv preprint arXiv:1301.3781*, 2013.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] T. Mikolov, K. Chen, G. Corrado, 和 J. Dean，“在向量空间中高效估计词表示”，*arXiv preprint
    arXiv:1301.3781*，2013年。'
- en: '[68] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean, “Distributed
    representations of words and phrases and their compositionality,” in *NIPS*, 2013,
    pp. 3111–3119.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, 和 J. Dean，“词汇和短语的分布式表示及其组合性”，在*NIPS*，2013年，第3111–3119页。'
- en: '[69] A. Radford, K. Narasimhan, T. Salimans, and I. Sutskever, “Improving language
    understanding by generative pre-training,” *URL https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf*,
    2018.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Radford, K. Narasimhan, T. Salimans, 和 I. Sutskever，“通过生成预训练提高语言理解”，*URL
    https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf*，2018年。'
- en: '[70] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and
    L. Zettlemoyer, “Deep contextualized word representations,” *arXiv preprint arXiv:1802.05365*,
    2018.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, 和 L.
    Zettlemoyer，“深度上下文化词表示”，*arXiv preprint arXiv:1802.05365*，2018年。'
- en: '[71] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training
    of deep bidirectional transformers for language understanding,” *arXiv preprint
    arXiv:1810.04805*, 2018.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova，“Bert：用于语言理解的深度双向变换器预训练”，*arXiv
    preprint arXiv:1810.04805*，2018年。'
- en: '[72] X. Liu, P. He, W. Chen, and J. Gao, “Multi-task deep neural networks for
    natural language understanding,” *arXiv preprint arXiv:1901.11504*, 2019.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] X. Liu, P. He, W. Chen, 和 J. Gao，“用于自然语言理解的多任务深度神经网络”，*arXiv preprint
    arXiv:1901.11504*，2019年。'
- en: '[73] X. Liu, Y. Shen, K. Duh, and J. Gao, “Stochastic answer networks for machine
    reading comprehension,” *arXiv preprint arXiv:1712.03556*, 2017.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] X. Liu, Y. Shen, K. Duh, 和 J. Gao，“用于机器阅读理解的随机答案网络”，*arXiv preprint arXiv:1712.03556*，2017年。'
- en: '[74] X. Liu, K. Duh, and J. Gao, “Stochastic answer networks for natural language
    inference,” *arXiv preprint arXiv:1804.07888*, 2018.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] X. Liu, K. Duh, 和 J. Gao，“用于自然语言推理的随机答案网络”，*arXiv preprint arXiv:1804.07888*，2018年。'
- en: '[75] R. T. McCoy, E. Pavlick, and T. Linzen, “Right for the wrong reasons:
    Diagnosing syntactic heuristics in natural language inference,” *ACL*, 2019.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] R. T. McCoy, E. Pavlick, 和 T. Linzen，“错误的原因对的：诊断自然语言推理中的句法启发式”，*ACL*，2019年。'
- en: '[76] T. Luong, R. Socher, and C. Manning, “Better word representations with
    recursive neural networks for morphology,” in *CoNLL*, 2013.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] T. Luong, R. Socher 和 C. Manning, “利用递归神经网络改进词表示以适应形态学，” 见于 *CoNLL*, 2013年。'
- en: '[77] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman,
    and E. Ruppin, “Placing search in context: The concept revisited,” in *Intnl Conf
    on World Wide Web*, 2001, pp. 406–414.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] L. Finkelstein, E. Gabrilovich, Y. Matias, E. Rivlin, Z. Solan, G. Wolfman
    和 E. Ruppin, “将搜索置于上下文中：重新审视概念，” 见于 *Intnl Conf on World Wide Web*, 2001年, 第406–414页。'
- en: '[78] M. Creutz and K. Lagus, “Unsupervised models for morpheme segmentation
    and morphology learning,” *ACM TSLP*, vol. 4, no. 1, 2007.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] M. Creutz 和 K. Lagus, “用于词素分割和形态学习的无监督模型，” *ACM TSLP*, 第4卷，第1期, 2007年。'
- en: '[79] G. A. Miller and W. G. Charles, “Contextual correlates of semantic similarity,”
    *Language and Cognitive Processes*, vol. 6, no. 1, 1991.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] G. A. Miller 和 W. G. Charles, “语义相似性的上下文相关性，” *Language and Cognitive
    Processes*, 第6卷，第1期, 1991年。'
- en: '[80] H. Rubenstein and J. B. Goodenough, “Contextual correlates of synonymy,”
    *CACM*, vol. 8, no. 10, pp. 627–633, 1965.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] H. Rubenstein 和 J. B. Goodenough, “同义词的上下文相关性，” *CACM*, 第8卷，第10期, 第627–633页,
    1965年。'
- en: '[81] R. Huang, Eric Hand Socher, C. Manning, and A. Ng, “Improving word representations
    via global context and multiple word prototypes,” in *ACL: Vol 1*, 2012, pp. 873–882.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] R. Huang, Eric Hand Socher, C. Manning 和 A. Ng, “通过全局上下文和多个词汇原型改进词表示，”
    见于 *ACL: Vol 1*, 2012年, 第873–882页。'
- en: '[82] Y. Belinkov, N. Durrani, F. Dalvi, H. Sajjad, and J. Glass, “What do neural
    machine translation models learn about morphology?” *arXiv preprint arXiv:1704.03471*,
    2017.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Y. Belinkov, N. Durrani, F. Dalvi, H. Sajjad 和 J. Glass, “神经机器翻译模型对形态学的学习？”
    *arXiv preprint arXiv:1704.03471*, 2017年。'
- en: '[83] M. Cettolo, C. Girardi, and M. Federico, “Wit3: Web inventory of transcribed
    and translated talks,” in *Conf of European Assoc. for Machine Translation*, 2012,
    pp. 261–268.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] M. Cettolo, C. Girardi 和 M. Federico, “Wit3: 网络转录和翻译演讲的库存，” 见于 *Conf of
    European Assoc. for Machine Translation*, 2012年, 第261–268页。'
- en: '[84] M. Cettolo, “An arabic-hebrew parallel corpus of ted talks,” *arXiv preprint
    arXiv:1610.00572*, 2016.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] M. Cettolo, “阿拉伯语-希伯来语平行语料库的TED演讲，” *arXiv preprint arXiv:1610.00572*,
    2016年。'
- en: '[85] H. Morita, D. Kawahara, and S. Kurohashi, “Morphological analysis for
    unsegmented languages using recurrent neural network language model,” in *EMNLP*,
    2015, pp. 2292–2297.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] H. Morita, D. Kawahara 和 S. Kurohashi, “使用递归神经网络语言模型进行未分割语言的形态分析，” 见于
    *EMNLP*, 2015年, 第2292–2297页。'
- en: '[86] D. Kawahara and S. Kurohashi, “Case frame compilation from the web using
    high-performance computing,” in *LREC*, 2006, pp. 1344–1347.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] D. Kawahara 和 S. Kurohashi, “利用高性能计算从网络编译案例框架，” 见于 *LREC*, 2006年, 第1344–1347页。'
- en: '[87] D. Kawahara, S. Kurohashi, and K. Hasida, “Construction of a japanese
    relevance-tagged corpus.” in *LREC*, 2002.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] D. Kawahara, S. Kurohashi 和 K. Hasida, “构建一个带有相关性标记的日语语料库，” 见于 *LREC*,
    2002年。'
- en: '[88] M. Hangyo, D. Kawahara, and S. Kurohashi, “Building a diverse document
    leads corpus annotated with semantic relations,” in *Pacific-Asia Conf on Language,
    Information, & Computation*, 2012, pp. 535–544.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] M. Hangyo, D. Kawahara 和 S. Kurohashi, “构建一个带有语义关系注释的多样化文档语料库，” 见于 *Pacific-Asia
    Conf on Language, Information, & Computation*, 2012年, 第535–544页。'
- en: '[89] M. Dehouck and P. Denis, “A framework for understanding the role of morphology
    in universal dependency parsing,” 2018.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] M. Dehouck 和 P. Denis, “理解形态学在通用依赖解析中的作用的框架，” 2018年。'
- en: '[90] A. More, Ö. Çetinoğlu, Ç. Çöltekin, N. Habash, B. Sagot, D. Seddah, D. Taji,
    and R. Tsarfaty, “Conll-ul: Universal morphological lattices for universal dependency
    parsing,” in *Proceedings of the Eleventh International Conference on Language
    Resources and Evaluation*, 2018.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. More, Ö. Çetinoğlu, Ç. Çöltekin, N. Habash, B. Sagot, D. Seddah, D.
    Taji 和 R. Tsarfaty, “Conll-ul: 适用于通用依赖解析的通用形态学格，” 见于 *Proceedings of the Eleventh
    International Conference on Language Resources and Evaluation*, 2018年。'
- en: '[91] J. Nivre, “An efficient algorithm for projective dependency parsing,”
    in *Intnl Workshop on Parsing Technologies*, 2003.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] J. Nivre, “一个高效的投影依赖解析算法，” 见于 *Intnl Workshop on Parsing Technologies*,
    2003年。'
- en: '[92] ——, “Incrementality in deterministic dependency parsing,” in *Workshop
    on Incremental Parsing: Bringing Engineering and Cognition Together*, 2004, pp.
    50–57.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] ——, “确定性依赖解析中的增量性，” 见于 *Workshop on Incremental Parsing: Bringing Engineering
    and Cognition Together*, 2004年, 第50–57页。'
- en: '[93] J. Nivre, M. Kuhlmann, and J. Hall, “An improved oracle for dependency
    parsing with online reordering,” in *Intnl Conf on Parsing Technologies*, 2009,
    pp. 73–76.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Nivre, M. Kuhlmann 和 J. Hall, “一个改进的在线重排序依赖解析oracle，” 见于 *Intnl Conf
    on Parsing Technologies*, 2009年, 第73–76页。'
- en: '[94] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, and C. Potts,
    “Recursive deep models for semantic compositionality over a sentiment treebank,”
    in *EMNLP*, 2013, pp. 1631–1642.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] R. Socher, A. Perelygin, J. Wu, J. Chuang, C. D. Manning, A. Ng, 和 C.
    Potts, “在情感树库上进行语义组合性的递归深度模型”，在 *EMNLP*，2013年，第1631–1642页。'
- en: '[95] R. Socher, J. Bauer, C. Manning *et al.*, “Parsing with compositional
    vector grammars,” in *ACL*, vol. 1, 2013, pp. 455–465.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] R. Socher, J. Bauer, C. Manning *等*，“使用组合向量语法的解析”，在 *ACL*，第1卷，2013年，第455–465页。'
- en: '[96] T. Fujisaki, F. Jelinek, J. Cocke, E. Black, and T. Nishino, “A probabilistic
    parsing method for sentence disambiguation,” in *Current issues in Parsing Technology*,
    1991, pp. 139–152.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] T. Fujisaki, F. Jelinek, J. Cocke, E. Black, 和 T. Nishino, “用于句子消歧的概率解析方法”，在
    *当前解析技术问题*，1991年，第139–152页。'
- en: '[97] F. Jelinek, J. Lafferty, and R. Mercer, “Basic methods of probabilistic
    context free grammars,” in *Speech Recognition and Understanding*.   Springer,
    1992, pp. 345–360.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] F. Jelinek, J. Lafferty, 和 R. Mercer, “概率上下文无关文法的基本方法”，在 *语音识别与理解*。Springer，1992年，第345–360页。'
- en: '[98] P. Le and W. Zuidema, “The inside-outside recursive neural network model
    for dependency parsing,” in *EMNLP*, 2014, pp. 729–739.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] P. Le 和 W. Zuidema, “用于依赖解析的内外递归神经网络模型”，在 *EMNLP*，2014年，第729–739页。'
- en: '[99] O. Vinyals, Ł. Kaiser, T. Koo, S. Petrov, I. Sutskever, and G. Hinton,
    “Grammar as a foreign language,” in *NIPS*, 2015, pp. 2773–2781.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] O. Vinyals, Ł. Kaiser, T. Koo, S. Petrov, I. Sutskever, 和 G. Hinton, “语法作为外语”，在
    *NIPS*，2015年，第2773–2781页。'
- en: '[100] S. Petrov and R. McDonald, “Overview of the 2012 shared task on parsing
    the web,” in *Notes of the 1st Workshop on Syntactic Analysis of Non-canonical
    Language*, vol. 59, 2012.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Petrov 和 R. McDonald, “2012年网页解析共享任务概述”，在 *第1届非规范语言句法分析研讨会笔记*，第59卷，2012年。'
- en: '[101] J. Judge, A. Cahill, and J. Van Genabith, “Questionbank: Creating a corpus
    of parse-annotated questions,” in *COLING*, 2006, pp. 497–504.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] J. Judge, A. Cahill, 和 J. Van Genabith, “Questionbank：创建一个解析注释的问题语料库”，在
    *COLING*，2006年，第497–504页。'
- en: '[102] P. Stenetorp, “Transition-based dependency parsing using recursive neural
    networks,” in *NIPS Workshop on Deep Learning*, 2013.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] P. Stenetorp, “使用递归神经网络的过渡依赖解析”，在 *NIPS 深度学习研讨会*，2013年。'
- en: '[103] M. Surdeanu, R. Johansson, A. Meyers, L. Màrquez, and J. Nivre, “The
    conll-2008 shared task on joint parsing of syntactic and semantic dependencies,”
    in *CONLL*, 2008, pp. 159–177.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. Surdeanu, R. Johansson, A. Meyers, L. Màrquez, 和 J. Nivre, “CONLL-2008
    共享任务：句法和语义依赖的联合解析”，在 *CONLL*，2008年，第159–177页。'
- en: '[104] D. Chen and C. Manning, “A fast and accurate dependency parser using
    neural networks,” in *EMNLP*, 2014, pp. 740–750.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] D. Chen 和 C. Manning, “使用神经网络的快速而准确的依赖解析器”，在 *EMNLP*，2014年，第740–750页。'
- en: '[105] H. Zhou, Y. Zhang, S. Huang, and J. Chen, “A neural probabilistic structured-prediction
    model for transition-based dependency parsing,” in *ACL and IJCNLP*, vol. 1, 2015,
    pp. 1213–1222.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] H. Zhou, Y. Zhang, S. Huang, 和 J. Chen, “用于过渡依赖解析的神经概率结构预测模型”，在 *ACL
    和 IJCNLP*，第1卷，2015年，第1213–1222页。'
- en: '[106] D. Weiss, C. Alberti, M. Collins, and S. Petrov, “Structured training
    for neural network transition-based parsing,” *arXiv preprint arXiv:1506.06158*,
    2015.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] D. Weiss, C. Alberti, M. Collins, 和 S. Petrov, “神经网络过渡解析的结构化训练”，*arXiv
    预印本 arXiv:1506.06158*，2015年。'
- en: '[107] Z. Li, M. Zhang, and W. Chen, “Ambiguity-aware ensemble training for
    semi-supervised dependency parsing,” in *ACL*, vol. 1, 2014.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Z. Li, M. Zhang, 和 W. Chen, “面向半监督依赖解析的模糊性感知集成训练”，在 *ACL*，第1卷，2014年。'
- en: '[108] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, and N. A. Smith, “Transition-based
    dependency parsing with stack long short-term memory,” *arXiv preprint arXiv:1505.08075*,
    2015.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] C. Dyer, M. Ballesteros, W. Ling, A. Matthews, 和 N. A. Smith, “基于堆栈长短期记忆的过渡依赖解析”，*arXiv
    预印本 arXiv:1505.08075*，2015年。'
- en: '[109] M.-C. De Marneffe and C. D. Manning, “The stanford typed dependencies
    representation,” in *COLING Workshop on Cross-framework and Cross-domain Parser
    Evaluation*, 2008, pp. 1–8.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] M.-C. De Marneffe 和 C. D. Manning, “斯坦福类型依赖表示”，在 *COLING 跨框架和跨领域解析器评估研讨会*，2008年，第1–8页。'
- en: '[110] N. Xue, F. Xia, F.-D. Chiou, and M. Palmer, “The penn chinese treebank:
    Phrase structure annotation of a large corpus,” *Natural Language Engineering*,
    vol. 11, no. 2, pp. 207–238, 2005.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] N. Xue, F. Xia, F.-D. Chiou, 和 M. Palmer, “Penn Chinese Treebank：大型语料库的短语结构注释”，*自然语言工程*，第11卷，第2期，第207–238页，2005年。'
- en: '[111] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev, S. Petrov,
    and M. Collins, “Globally normalized transition-based neural networks,” *arXiv
    preprint arXiv:1603.06042*, 2016.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] D. Andor, C. Alberti, D. Weiss, A. Severyn, A. Presta, K. Ganchev, S.
    Petrov, 和 M. Collins, “全局归一化的过渡神经网络”，*arXiv 预印本 arXiv:1603.06042*，2016年。'
- en: '[112] Y. Wang, W. Che, J. Guo, and T. Liu, “A neural transition-based approach
    for semantic dependency graph parsing,” 2018.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Y. Wang, W. Che, J. Guo, 和 T. Liu, “一种基于神经过渡的方法用于语义依赖图解析，” 2018年。'
- en: '[113] J. Cross and L. Huang, “Incremental parsing with minimal features using
    bi-directional lstm,” *arXiv preprint arXiv:1606.06406*, 2016.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] J. Cross 和 L. Huang, “使用双向LSTM进行增量解析，特征最小化，” *arXiv预印本 arXiv:1606.06406*，2016年。'
- en: '[114] K. S. Tai, R. Socher, and C. D. Manning, “Improved semantic representations
    from tree-structured long short-term memory networks,” *arXiv preprint arXiv:1503.00075*,
    2015.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] K. S. Tai, R. Socher, 和 C. D. Manning, “来自树结构长短期记忆网络的改进语义表示，” *arXiv预印本
    arXiv:1503.00075*，2015年。'
- en: '[115] S. Oepen, M. Kuhlmann, Y. Miyao, D. Zeman, S. Cinková, D. Flickinger,
    J. Hajic, and Z. Uresova, “Semeval 2015 task 18: Broad-coverage semantic dependency
    parsing,” in *Intnl Workshop on Semantic Evaluation*, 2015, pp. 915–926.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] S. Oepen, M. Kuhlmann, Y. Miyao, D. Zeman, S. Cinková, D. Flickinger,
    J. Hajic, 和 Z. Uresova, “Semeval 2015 任务18：广覆盖语义依赖解析，” 在 *国际语义评估研讨会*，2015年，第915–926页。'
- en: '[116] W. Che, M. Zhang, Y. Shao, and T. Liu, “Semeval-2012 task 9: Chinese
    semantic dependency parsing,” in *Conference on Lexical and Computational Semantics*,
    2012, pp. 378–384.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] W. Che, M. Zhang, Y. Shao, 和 T. Liu, “Semeval-2012 任务9：中文语义依赖解析，” 在 *词汇与计算语义会议*，2012年，第378–384页。'
- en: '[117] W.-t. Yih, X. He, and C. Meek, “Semantic parsing for single-relation
    question answering,” in *Proceedings of the 52nd Annual Meeting of the ACL (Volume
    2: Short Papers)*, 2014, pp. 643–648.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] W.-t. Yih, X. He, 和 C. Meek, “单一关系问答的语义解析，” 在 *第52届ACL年会论文集（第2卷：短文集）*，2014年，第643–648页。'
- en: '[118] J. Krishnamurthy, P. Dasigi, and M. Gardner, “Neural semantic parsing
    with type constraints for semi-structured tables,” in *Proceedings of the 2017
    Conference on Empirical Methods in NLP*, 2017, pp. 1516–1526.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] J. Krishnamurthy, P. Dasigi, 和 M. Gardner, “带有类型约束的神经语义解析用于半结构化表格，” 在
    *2017年NLP实证方法会议论文集*，2017年，第1516–1526页。'
- en: '[119] C. Dyer, A. Kuncoro, M. Ballesteros, and N. A. Smith, “Recurrent neural
    network grammars,” *arXiv preprint arXiv:1602.07776*, 2016.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] C. Dyer, A. Kuncoro, M. Ballesteros, 和 N. A. Smith, “递归神经网络语法，” *arXiv预印本
    arXiv:1602.07776*，2016年。'
- en: '[120] D. K. Choe and E. Charniak, “Parsing as language modeling,” in *EMNLP*,
    2016, pp. 2331–2336.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] D. K. Choe 和 E. Charniak, “将解析作为语言建模，” 在 *EMNLP*，2016年，第2331–2336页。'
- en: '[121] D. Fried, M. Stern, and D. Klein, “Improving neural parsing by disentangling
    model combination and reranking effects,” *arXiv preprint arXiv:1707.03058*, 2017.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] D. Fried, M. Stern, 和 D. Klein, “通过解开模型组合和重排名效应来改进神经解析，” *arXiv预印本 arXiv:1707.03058*，2017年。'
- en: '[122] T. Dozat and C. D. Manning, “Simpler but more accurate semantic dependency
    parsing,” *arXiv preprint arXiv:1807.01396*, 2018.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] T. Dozat 和 C. D. Manning, “更简单但更准确的语义依赖解析，” *arXiv预印本 arXiv:1807.01396*，2018年。'
- en: '[123] Z. Tan, M. Wang, J. Xie, Y. Chen, and X. Shi, “Deep semantic role labeling
    with self-attention,” in *Thirty-Second AAAI Conference on Artificial Intelligence*,
    2018.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Z. Tan, M. Wang, J. Xie, Y. Chen, 和 X. Shi, “自注意力的深度语义角色标注，” 在 *第三十二届AAAI人工智能大会*，2018年。'
- en: '[124] L. Duong, H. Afshar, D. Estival, G. Pink, P. Cohen, and M. Johnson, “Active
    learning for deep semantic parsing,” in *Proceedings of the 56th Annual Meeting
    of the ACL (Vol. 2: Short Papers)*, 2018, pp. 43–48.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] L. Duong, H. Afshar, D. Estival, G. Pink, P. Cohen, 和 M. Johnson, “深度语义解析的主动学习，”
    在 *第56届ACL年会论文集（第2卷：短文集）*，2018年，第43–48页。'
- en: '[125] J. Nivre, “Towards a universal grammar for natural language processing,”
    in *International Conference on Intelligent Text Processing and Computational
    Linguistics*.   Springer, 2015, pp. 3–16.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] J. Nivre, “朝着自然语言处理的通用语法迈进，” 在 *国际智能文本处理与计算语言学会议*。 Springer，2015年，第3–16页。'
- en: '[126] D. Zeman, J. Hajič, M. Popel, M. Potthast, M. Straka, F. Ginter, J. Nivre,
    and S. Petrov, “Conll 2018 shared task: multilingual parsing from raw text to
    universal dependencies,” 2018.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] D. Zeman, J. Hajič, M. Popel, M. Potthast, M. Straka, F. Ginter, J. Nivre,
    和 S. Petrov, “Conll 2018 共享任务：从原始文本到通用依赖的多语言解析，” 2018年。'
- en: '[127] D. Hershcovich, O. Abend, and A. Rappoport, “Universal dependency parsing
    with a general transition-based dag parser,” *arXiv preprint arXiv:1808.09354*,
    2018.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] D. Hershcovich, O. Abend, 和 A. Rappoport, “使用通用过渡基DAG解析器的通用依赖解析，” *arXiv预印本
    arXiv:1808.09354*，2018年。'
- en: '[128] T. Ji, Y. Liu, Y. Wang, Y. Wu, and M. Lan, “Antnlp at conll 2018 shared
    task: A graph-based parser for universal dependency parsing,” in *Proceedings
    of the CoNLL 2018 Shared Task: Multilingual Parsing from Raw Text to Universal
    Dependencies*, 2018, pp. 248–255.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] T. Ji, Y. Liu, Y. Wang, Y. Wu 和 M. Lan，“Antnlp 在 CoNLL 2018 共享任务中的表现：一种基于图的通用依赖解析器”，发表于
    *CoNLL 2018 共享任务论文集：从原始文本到通用依赖的多语言解析*，2018年，第248–255页。'
- en: '[129] P. Qi, T. Dozat, Y. Zhang, and C. D. Manning, “Universal dependency parsing
    from scratch,” *arXiv preprint arXiv:1901.10457*, 2019.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] P. Qi, T. Dozat, Y. Zhang 和 C. D. Manning，“从头开始的通用依赖解析”，*arXiv 预印本 arXiv:1901.10457*，2019年。'
- en: '[130] Y. Liu, Y. Zhu, W. Che, B. Qin, N. Schneider, and N. A. Smith, “Parsing
    tweets into universal dependencies,” *arXiv preprint arXiv:1804.08228*, 2018.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Y. Liu, Y. Zhu, W. Che, B. Qin, N. Schneider 和 N. A. Smith，“将推文解析为通用依赖”，*arXiv
    预印本 arXiv:1804.08228*，2018年。'
- en: '[131] J. Pennington, R. Socher, and C. Manning, “Glove: Global vectors for
    word representation,” in *EMNLP*, 2014, pp. 1532–1543.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] J. Pennington, R. Socher 和 C. Manning，“GloVe：全球词向量表示”，发表于 *EMNLP*，2014年，第1532–1543页。'
- en: '[132] Z. S. Harris, “Distributional structure,” *Word*, vol. 10, no. 2-3, pp.
    146–162, 1954.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Z. S. Harris，“分布结构”，*Word*，第10卷，第2-3期，第146–162页，1954年。'
- en: '[133] B. Hu, Z. Lu, H. Li, and Q. Chen, “Convolutional neural network architectures
    for matching natural language sentences,” in *NIPS*, 2014, pp. 2042–2050.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] B. Hu, Z. Lu, H. Li 和 Q. Chen，“用于匹配自然语言句子的卷积神经网络架构”，发表于 *NIPS*，2014年，第2042–2050页。'
- en: '[134] A. Bordes, X. Glorot, J. Weston, and Y. Bengio, “A semantic matching
    energy function for learning with multi-relational data,” *Machine Learning*,
    vol. 94, no. 2, pp. 233–259, 2014.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] A. Bordes, X. Glorot, J. Weston 和 Y. Bengio，“用于多关系数据学习的语义匹配能量函数”，*机器学习*，第94卷，第2期，第233–259页，2014年。'
- en: '[135] W. Yin and H. Schütze, “Convolutional neural network for paraphrase identification,”
    in *NAACL: HLT*, 2015, pp. 901–911.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] W. Yin 和 H. Schütze，“用于同义句识别的卷积神经网络”，发表于 *NAACL: HLT*，2015年，第901–911页。'
- en: '[136] B. Dolan, C. Quirk, and C. Brockett, “Unsupervised construction of large
    paraphrase corpora: Exploiting massively parallel news sources,” in *COLING*,
    2004, p. 350.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] B. Dolan, C. Quirk 和 C. Brockett，“无监督构建大规模同义句语料库：利用大规模并行新闻源”，发表于 *COLING*，2004年，第350页。'
- en: '[137] H. He, K. Gimpel, and J. Lin, “Multi-perspective sentence similarity
    modeling with convolutional neural networks,” in *EMNLP*, 2015.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] H. He, K. Gimpel 和 J. Lin，“使用卷积神经网络的多角度句子相似性建模”，发表于 *EMNLP*，2015年。'
- en: '[138] M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi, S. Menini, and R. Zamparelli,
    “Semeval-2014 task 1: Evaluation of compositional distributional semantic models
    on full sentences through semantic relatedness and textual entailment,” in *Intnl
    Workshop on Semantic Evaluation*, 2014, pp. 1–8.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] M. Marelli, L. Bentivogli, M. Baroni, R. Bernardi, S. Menini 和 R. Zamparelli，“Semeval-2014
    任务 1：通过语义相关性和文本蕴含评估句子的组合分布语义模型”，发表于 *国际语义评估研讨会*，2014年，第1–8页。'
- en: '[139] E. Agirre, M. Diab, D. Cer, and A. Gonzalez-Agirre, “Semeval-2012 task
    6: A pilot on semantic textual similarity,” in *Joint Conf on Lexical and Computational
    Semantics-Vol 1*, 2012, pp. 385–393.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] E. Agirre, M. Diab, D. Cer 和 A. Gonzalez-Agirre，“Semeval-2012 任务 6：语义文本相似性的初步研究”，发表于
    *Lexical and Computational Semantics-Vol 1 联合会议*，2012年，第385–393页。'
- en: '[140] H. He and J. Lin, “Pairwise word interaction modeling with deep neural
    networks for semantic similarity measurement,” in *NAACL: HLT*, 2016, pp. 937–948.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] H. He 和 J. Lin，“使用深度神经网络建模词对交互以进行语义相似性测量”，发表于 *NAACL: HLT*，2016年，第937–948页。'
- en: '[141] E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre,
    W. Guo, R. Mihalcea, G. Rigau, and J. Wiebe, “Semeval-2014 task 10: Multilingual
    semantic textual similarity,” in *Intnl Workshop on Semantic Evaluation*, 2014,
    pp. 81–91.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] E. Agirre, C. Banea, C. Cardie, D. Cer, M. Diab, A. Gonzalez-Agirre,
    W. Guo, R. Mihalcea, G. Rigau 和 J. Wiebe，“Semeval-2014 任务 10：多语言语义文本相似性”，发表于 *国际语义评估研讨会*，2014年，第81–91页。'
- en: '[142] Y. Yang, W.-t. Yih, and C. Meek, “Wikiqa: A challenge dataset for open-domain
    question answering,” in *EMNLP*, 2015, pp. 2013–2018.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Y. Yang, W.-t. Yih 和 C. Meek，“Wikiqa：一个开放领域问答的挑战数据集”，发表于 *EMNLP*，2015年，第2013–2018页。'
- en: '[143] M. Wang, N. A. Smith, and T. Mitamura, “What is the jeopardy model? a
    quasi-synchronous grammar for qa,” in *Joint EMNLP and CoNLL*, 2007.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] M. Wang, N. A. Smith 和 T. Mitamura，“什么是危险模式？一种准同步语法用于问答”，发表于 *EMNLP 和
    CoNLL 联合会议*，2007年。'
- en: '[144] Q. Le and T. Mikolov, “Distributed representations of sentences and documents,”
    in *ICML*, 2014, pp. 1188–1196.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Q. Le 和 T. Mikolov，“句子和文档的分布式表示”，发表于 *ICML*，2014年，第1188–1196页。'
- en: '[145] A. Go, R. Bhayani, and L. Huang, “Twitter sentiment classification using
    distant supervision,” *CS224N Project Report, Stanford*, vol. 1, no. 12, 2009.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] A. Go, R. Bhayani, 和 L. Huang, “使用远程监督的 Twitter 情感分类，” *CS224N 项目报告，斯坦福大学*，第
    1 卷，第 12 期，2009年。'
- en: '[146] X. Li and D. Roth, “Learning question classifiers,” in *COLING-Vol 1*,
    2002, pp. 1–7.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] X. Li 和 D. Roth, “学习问题分类器，” 见 *COLING-第 1 卷*，2002年，页 1–7。'
- en: '[147] A. Poliak, Y. Belinkov, J. Glass, and B. Van Durme, “On the evaluation
    of semantic phenomena in neural machine translation using natural language inference,”
    *arXiv preprint arXiv:1804.09779*, 2018.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] A. Poliak, Y. Belinkov, J. Glass, 和 B. Van Durme, “使用自然语言推理评估神经机器翻译中的语义现象，”
    *arXiv 预印本 arXiv:1804.09779*，2018年。'
- en: '[148] A. Williams, N. Nangia, and S. Bowman, “A broad-coverage challenge corpus
    for sentence understanding through inference,” *arXiv preprint arXiv:1704.05426*,
    2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] A. Williams, N. Nangia, 和 S. Bowman, “通过推理进行句子理解的广泛覆盖挑战语料库，” *arXiv 预印本
    arXiv:1704.05426*，2017年。'
- en: '[149] N. Nangia, A. Williams, A. Lazaridou, and S. R. Bowman, “The repeval
    2017 shared task: Multi-genre natural language inference with sentence representations,”
    *arXiv preprint arXiv:1707.08172*, 2017.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] N. Nangia, A. Williams, A. Lazaridou, 和 S. R. Bowman, “Repeval 2017 共享任务：多体裁自然语言推理与句子表示，”
    *arXiv 预印本 arXiv:1707.08172*，2017年。'
- en: '[150] A. S. White, P. Rastogi, K. Duh, and B. Van Durme, “Inference is everything:
    Recasting semantic resources into a unified evaluation framework,” in *IJCNLP*,
    vol. 1, 2017, pp. 996–1005.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] A. S. White, P. Rastogi, K. Duh, 和 B. Van Durme, “推理即一切：将语义资源重新构建为统一的评估框架，”
    见 *IJCNLP*，第 1 卷，2017年，页 996–1005。'
- en: '[151] E. Pavlick, T. Wolfe, P. Rastogi, C. Callison-Burch, M. Dredze, and B. Van Durme,
    “Framenet+: Fast paraphrastic tripling of framenet,” in *ACL*, vol. 2, 2015, pp.
    408–413.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] E. Pavlick, T. Wolfe, P. Rastogi, C. Callison-Burch, M. Dredze, 和 B.
    Van Durme, “Framenet+: 快速生成 Framenet 的同义短语，” 见 *ACL*，第 2 卷，2015年，页 408–413。'
- en: '[152] A. Rahman and V. Ng, “Resolving complex cases of definite pronouns: the
    winograd schema challenge,” in *Joint EMNLP and CoNLL*, 2012.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] A. Rahman 和 V. Ng, “解决定指代复杂案例：Winograd 语料库挑战，” 见 *联合 EMNLP 和 CoNLL*，2012年。'
- en: '[153] D. Reisinger, R. Rudinger, F. Ferraro, C. Harman, K. Rawlins, and B. Van Durme,
    “Semantic proto-roles,” *Transactions of the ACL*, vol. 3, pp. 475–488, 2015.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] D. Reisinger, R. Rudinger, F. Ferraro, C. Harman, K. Rawlins, 和 B. Van
    Durme, “语义原型角色，” *ACL 交易*，第 3 卷，页 475–488，2015年。'
- en: '[154] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, and B. Van Durme, “Hypothesis
    only baselines in natural language inference,” *arXiv preprint arXiv:1805.01042*,
    2018.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] A. Poliak, J. Naradowsky, A. Haldar, R. Rudinger, 和 B. Van Durme, “自然语言推理中的假设基线，”
    *arXiv 预印本 arXiv:1805.01042*，2018年。'
- en: '[155] J. Herzig and J. Berant, “Neural semantic parsing over multiple knowledge-bases,”
    *arXiv preprint arXiv:1702.01569*, 2017.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] J. Herzig 和 J. Berant, “多知识库上的神经语义解析，” *arXiv 预印本 arXiv:1702.01569*，2017年。'
- en: '[156] Y. Wang, J. Berant, and P. Liang, “Building a semantic parser overnight,”
    in *ACL and IJCNLP*, vol. 1, 2015, pp. 1332–1342.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Y. Wang, J. Berant, 和 P. Liang, “一夜之间构建语义解析器，” 见 *ACL 和 IJCNLP*，第 1 卷，2015年，页
    1332–1342。'
- en: '[157] G. Brunner, Y. Wang, R. Wattenhofer, and M. Weigelt, “Natural language
    multitasking: Analyzing and improving syntactic saliency of hidden representations,”
    *arXiv preprint arXiv:1801.06024*, 2018.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] G. Brunner, Y. Wang, R. Wattenhofer, 和 M. Weigelt, “自然语言多任务：分析和改进隐藏表示的句法显著性，”
    *arXiv 预印本 arXiv:1801.06024*，2018年。'
- en: '[158] P. Koehn, “Europarl: A parallel corpus for statistical machine translation,”
    in *MT summit*, vol. 5, 2005, pp. 79–86.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] P. Koehn, “Europarl: 用于统计机器翻译的平行语料库，” 见 *MT 峰会*，第 5 卷，2005年，页 79–86。'
- en: '[159] G. A. Miller, “Wordnet: a lexical database for english,” *Communications
    of the ACM*, vol. 38, no. 11, pp. 39–41, 1995.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] G. A. Miller, “Wordnet: 英语词汇数据库，” *ACM 通讯*，第 38 卷，第 11 期，页 39–41，1995年。'
- en: '[160] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, and Z. Ives,
    “Dbpedia: A nucleus for a web of open data,” in *The Semantic Web*.   Springer,
    2007, pp. 722–735.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] S. Auer, C. Bizer, G. Kobilarov, J. Lehmann, R. Cyganiak, 和 Z. Ives,
    “DBpedia: 开放数据网络的核心，” 见 *语义网*。 Springer，2007年，页 722–735。'
- en: '[161] Q. Wang, Z. Mao, B. Wang, and L. Guo, “Knowledge graph embedding: A survey
    of approaches and applications,” *IEEE Transactions on Knowledge & Data Engineering*,
    vol. 29, no. 12, pp. 2724–2743, 2017.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Q. Wang, Z. Mao, B. Wang, 和 L. Guo, “知识图谱嵌入：方法与应用综述，” *IEEE 知识与数据工程汇刊*，第
    29 卷，第 12 期，页 2724–2743，2017年。'
- en: '[162] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*, “Deep neural networks for acoustic
    modeling in speech recognition: The shared views of four research groups,” *IEEE
    Signal Processing Magazine*, vol. 29, no. 6, pp. 82–97, 2012.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior,
    V. Vanhoucke, P. Nguyen, T. N. Sainath *et al.*，“用于语音识别的深度神经网络：四个研究小组的共享观点，”*IEEE
    Signal Processing Magazine*，第 29 卷，第 6 期，第 82–97 页，2012。'
- en: '[163] A. Graves, A.-r. Mohamed, and G. Hinton, “Speech recognition with deep
    recurrent neural networks,” in *IEEE International Conf on Acoustics, Speech and
    Signal Processing*, 2013, pp. 6645–6649.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] A. Graves, A.-r. Mohamed, 和 G. Hinton，“使用深度递归神经网络的语音识别，”在 *IEEE International
    Conf on Acoustics, Speech and Signal Processing*，2013，第 6645–6649 页。'
- en: '[164] T. Kenter, A. Borisov, C. Van Gysel, M. Dehghani, M. de Rijke, and B. Mitra,
    “Neural networks for information retrieval,” in *Proceedings of the 40th International
    ACM SIGIR Conference on Research and Development in Information Retrieval*.   ACM,
    2017, pp. 1403–1406.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] T. Kenter, A. Borisov, C. Van Gysel, M. Dehghani, M. de Rijke, 和 B. Mitra，“信息检索中的神经网络，”在
    *Proceedings of the 40th International ACM SIGIR Conference on Research and Development
    in Information Retrieval*。ACM，2017，第 1403–1406 页。'
- en: '[165] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, and L. Heck, “Learning
    deep structured semantic models for web search using clickthrough data,” in *ACM
    CIKM*, 2013, pp. 2333–2338.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] P.-S. Huang, X. He, J. Gao, L. Deng, A. Acero, 和 L. Heck，“使用点击数据学习深度结构化语义模型进行网页搜索，”在
    *ACM CIKM*，2013，第 2333–2338 页。'
- en: '[166] Y. Shen, X. He, J. Gao, L. Deng, and G. Mesnil, “Learning semantic representations
    using convolutional neural networks for web search,” in *Intnl Conf on World Wide
    Web*, 2014, pp. 373–374.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Y. Shen, X. He, J. Gao, L. Deng, 和 G. Mesnil，“使用卷积神经网络进行网页搜索的语义表示学习，”在
    *Intnl Conf on World Wide Web*，2014，第 373–374 页。'
- en: '[167] Z. Lu and H. Li, “A deep architecture for matching short texts,” in *Advances
    in neural information processing systems*, 2013.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Z. Lu 和 H. Li，“用于匹配短文本的深度架构，”在 *Advances in neural information processing
    systems*，2013。'
- en: '[168] L. Pang, Y. Lan, J. Guo, J. Xu, S. Wan, and X. Cheng, “Text matching
    as image recognition,” in *Thirtieth AAAI Conference on Artificial Intelligence*,
    2016.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] L. Pang, Y. Lan, J. Guo, J. Xu, S. Wan, 和 X. Cheng，“将文本匹配作为图像识别，”在 *Thirtieth
    AAAI Conference on Artificial Intelligence*，2016。'
- en: '[169] J. Guo, Y. Fan, Q. Ai, and W. B. Croft, “A deep relevance matching model
    for ad-hoc retrieval,” in *Proceedings of the 25th ACM International on Conference
    on Information and Knowledge Management*.   ACM, 2016, pp. 55–64.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] J. Guo, Y. Fan, Q. Ai, 和 W. B. Croft，“用于临时检索的深度相关匹配模型，”在 *Proceedings
    of the 25th ACM International on Conference on Information and Knowledge Management*。ACM，2016，第
    55–64 页。'
- en: '[170] H. Zamani, M. Dehghani, W. B. Croft, E. Learned-Miller, and J. Kamps,
    “From neural re-ranking to neural ranking: Learning a sparse representation for
    inverted indexing,” in *27th ACM International Conference on Information and Knowledge
    Management*.   ACM, 2018.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] H. Zamani, M. Dehghani, W. B. Croft, E. Learned-Miller, 和 J. Kamps，“从神经重排序到神经排序：学习稀疏表示以进行倒排索引，”在
    *27th ACM International Conference on Information and Knowledge Management*。ACM，2018。'
- en: '[171] S. MacAvaney, A. Yates, A. Cohan, and N. Goharian, “Cedr: Contextualized
    embeddings for document ranking,” *CoRR*, 2019.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. MacAvaney, A. Yates, A. Cohan, 和 N. Goharian，“CEDR：用于文档排序的上下文化嵌入，”*CoRR*，2019。'
- en: '[172] J. Cowie and W. Lehnert, “Information extraction,” *CACM*, vol. 39, no. 1,
    pp. 80–91, 1996.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] J. Cowie 和 W. Lehnert，“信息提取，”*CACM*，第 39 卷，第 1 期，第 80–91 页，1996。'
- en: '[173] J. Hammerton, “Named entity recognition with long short-term memory,”
    in *HLT-NAACL 2003-Volume 4*, 2003, pp. 172–175.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] J. Hammerton，“使用长短期记忆的命名实体识别，”在 *HLT-NAACL 2003-Volume 4*，2003，第 172–175
    页。'
- en: '[174] C. N. d. Santos and V. Guimaraes, “Boosting named entity recognition
    with neural character embeddings,” *arXiv preprint arXiv:1505.05008*, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] C. N. d. Santos 和 V. Guimaraes，“通过神经字符嵌入提升命名实体识别，”*arXiv 预印本 arXiv:1505.05008*，2015。'
- en: '[175] D. Santos, N. Seco, N. Cardoso, and R. Vilela, “Harem: An advanced ner
    evaluation contest for portuguese,” in *LREC, Genoa*, 2006.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] D. Santos, N. Seco, N. Cardoso, 和 R. Vilela，“Harem：一种先进的葡萄牙语 NER 评估竞赛，”在
    *LREC, Genoa*，2006。'
- en: '[176] X. Carreras, L. Marquez, and L. Padró, “Named entity extraction using
    adaboost,” in *CoNLL*, 2002, pp. 1–4.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] X. Carreras, L. Marquez, 和 L. Padró，“使用 Adaboost 的命名实体提取，”在 *CoNLL*，2002，第
    1–4 页。'
- en: '[177] J. Chiu and E. Nichols, “Named entity recognition with bidirectional
    lstm-cnns,” *arXiv preprint arXiv:1511.08308*, 2015.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] J. Chiu 和 E. Nichols，“使用双向 LSTM-CNNs 的命名实体识别，”*arXiv 预印本 arXiv:1511.08308*，2015。'
- en: '[178] E. F. Tjong Kim Sang and F. De Meulder, “Introduction to the conll-2003
    shared task: Language-independent named entity recognition,” in *HLT-NAACL-Volume
    4*, 2003, pp. 142–147.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] E. F. Tjong Kim Sang 和 F. De Meulder，“CONLL-2003共享任务简介：语言独立的命名实体识别，”发表在*HLT-NAACL-第4卷*，2003年，页码
    142–147。'
- en: '[179] E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R. Weischedel, “Ontonotes:
    the 90% solution,” in *Human Language Technology Conf, Companion Volume*, 2006,
    pp. 57–60.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] E. Hovy, M. Marcus, M. Palmer, L. Ramshaw, 和 R. Weischedel，“Ontonotes：90%解决方案，”发表在*人类语言技术会议，补充卷*，2006年，页码
    57–60。'
- en: '[180] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Björkelund, O. Uryupina,
    Y. Zhang, and Z. Zhong, “Towards robust linguistic analysis using ontonotes,”
    in *CoNLL*, 2013, pp. 143–152.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] S. Pradhan, A. Moschitti, N. Xue, H. T. Ng, A. Björkelund, O. Uryupina,
    Y. Zhang, 和 Z. Zhong，“通过Ontonotes实现稳健的语言分析，”发表在*CoNLL*，2013年，页码 143–152。'
- en: '[181] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, and C. Dyer,
    “Neural architectures for named entity recognition,” *arXiv preprint arXiv:1603.01360*,
    2016.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] G. Lample, M. Ballesteros, S. Subramanian, K. Kawakami, 和 C. Dyer，“用于命名实体识别的神经架构，”*arXiv预印本
    arXiv:1603.01360*，2016年。'
- en: '[182] J. Lafferty, A. McCallum, and F. C. Pereira, “Conditional random fields:
    Probabilistic models for segmenting and labeling sequence data,” 2001.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] J. Lafferty, A. McCallum, 和 F. C. Pereira，“条件随机场：用于分割和标注序列数据的概率模型，”2001年。'
- en: '[183] A. Akbik, D. Blythe, and R. Vollgraf, “Contextual string embeddings for
    sequence labeling,” in *COLING*, 2018, pp. 1638–1649.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] A. Akbik, D. Blythe, 和 R. Vollgraf，“用于序列标注的上下文字符串嵌入，”发表在*COLING*，2018年，页码
    1638–1649。'
- en: '[184] Y. Chen, L. Xu, K. Liu, D. Zeng, and J. Zhao, “Event extraction via dynamic
    multi-pooling convolutional neural networks,” in *ACL*, vol. 1, 2015, pp. 167–176.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Y. Chen, L. Xu, K. Liu, D. Zeng, 和 J. Zhao，“通过动态多池化卷积神经网络进行事件提取，”发表在*ACL*，第1卷，2015年，页码
    167–176。'
- en: '[185] T. H. Nguyen, K. Cho, and R. Grishman, “Joint event extraction via recurrent
    neural networks,” in *Conf of the North American Chapter of ACL: Human Language
    Technologies*, 2016, pp. 300–309.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] T. H. Nguyen, K. Cho, 和 R. Grishman，“通过递归神经网络进行联合事件提取，”发表在*北美ACL会议：人类语言技术*，2016年，页码
    300–309。'
- en: '[186] X. Liu, H. Huang, and Y. Zhang, “Open domain event extraction using neural
    latent variable models,” *arXiv preprint arXiv:1906.06947*, 2019.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] X. Liu, H. Huang, 和 Y. Zhang，“使用神经潜变量模型的开放域事件提取，”*arXiv预印本 arXiv:1906.06947*，2019年。'
- en: '[187] S. Zheng, Y. Hao, D. Lu, H. Bao, J. Xu, H. Hao, and B. Xu, “Joint entity
    and relation extraction based on a hybrid neural network,” *Neurocomputing*, vol.
    257, pp. 59–66, 2017.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] S. Zheng, Y. Hao, D. Lu, H. Bao, J. Xu, H. Hao, 和 B. Xu，“基于混合神经网络的联合实体和关系提取，”*神经计算*，第257卷，页码
    59–66，2017年。'
- en: '[188] M. Sun, X. Li, X. Wang, M. Fan, Y. Feng, and P. Li, “Logician: A unified
    end-to-end neural approach for open-domain information extraction,” in *ACM Intnl
    Conf on Web Search and Data Mining*, 2018.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] M. Sun, X. Li, X. Wang, M. Fan, Y. Feng, 和 P. Li，“Logician: 一种统一的端到端神经方法用于开放域信息提取，”发表在*ACM国际网络搜索与数据挖掘会议*，2018年。'
- en: '[189] Z. Tu, Z. Lu, Y. Liu, X. Liu, and H. Li, “Modeling coverage for neural
    machine translation,” *arXiv preprint arXiv:1601.04811*, 2016.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Z. Tu, Z. Lu, Y. Liu, X. Liu, 和 H. Li，“神经机器翻译的覆盖建模，”*arXiv预印本 arXiv:1601.04811*，2016年。'
- en: '[190] C. Lin, T. Miller, D. Dligach, S. Bethard, and G. Savova, “A bert-based
    universal model for both within-and cross-sentence clinical temporal relation
    extraction,” in *Clinical NLP Workshop*, 2019, pp. 65–71.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] C. Lin, T. Miller, D. Dligach, S. Bethard, 和 G. Savova，“基于BERT的通用模型用于句内和跨句临床时间关系提取，”发表在*临床自然语言处理工作坊*，2019年，页码
    65–71。'
- en: '[191] A. Conneau, H. Schwenk, L. Barrault, and Y. Lecun, “Very deep convolutional
    networks for text classification,” in *European ACL*, vol. 1, 2017, pp. 1107–1116.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] A. Conneau, H. Schwenk, L. Barrault, 和 Y. Lecun，“用于文本分类的非常深的卷积网络，”发表在*欧洲ACL*，第1卷，2017年，页码
    1107–1116。'
- en: '[192] M. Jiang, Y. Liang, X. Feng, X. Fan, Z. Pei, Y. Xue, and R. Guan, “Text
    classification based on deep belief network and softmax regression,” *Neural Computing
    and Applications*, vol. 29, no. 1, pp. 61–70, 2018.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] M. Jiang, Y. Liang, X. Feng, X. Fan, Z. Pei, Y. Xue, 和 R. Guan，“基于深度信念网络和Softmax回归的文本分类，”*神经计算与应用*，第29卷，第1期，页码
    61–70，2018年。'
- en: '[193] G. Hinton, S. Osindero, and Y.-W. Teh, “A fast learning algorithm for
    deep belief nets,” *Neural computation*, vol. 18, no. 7, 2006.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] G. Hinton, S. Osindero, 和 Y.-W. Teh，“深度信念网络的快速学习算法，”*神经计算*，第18卷，第7期，2006年。'
- en: '[194] R. S. Sutton and A. G. Barto, *Reinforcement learning: An introduction*.   MIT
    Press Cambridge, 1998, vol. 1, no. 1.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] R. S. Sutton 和 A. G. Barto，*强化学习：导论*。MIT出版社剑桥，1998年，第1卷，第1期。'
- en: '[195] P. Smolensky, “Information processing in dynamical systems: Foundations
    of harmony theory,” Tech. Rep., 1986.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] P. Smolensky，“动态系统中的信息处理：和谐理论的基础，”技术报告，1986年。'
- en: '[196] R. Fletcher, *Practical methods of optimization*.   Wiley & Sons, 2013.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] R. Fletcher，*优化的实用方法*。Wiley & Sons，2013。'
- en: '[197] A. Adhikari, A. Ram, R. Tang, and J. Lin, “Docbert: Bert for document
    classification,” *arXiv preprint arXiv:1904.08398*, 2019.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] A. Adhikari, A. Ram, R. Tang, 和 J. Lin，“Docbert：用于文档分类的Bert”，*arXiv 预印本
    arXiv:1904.08398*，2019。'
- en: '[198] J. Worsham and J. Kalita, “Genre identification and the compositional
    effect of genre in literature,” in *COLING*, 2018, pp. 1963–1973.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] J. Worsham 和 J. Kalita，“文学中的体裁识别及体裁的组合效应”，在*COLING*，2018，第1963–1973页。'
- en: '[199] J. Wei, Q. Zhou, and Y. Cai, “Poet-based poetry generation: Controlling
    personal style with recurrent neural networks,” in *2018 International Conference
    on Computing, Networking and Communications (ICNC)*.   IEEE, 2018, pp. 156–160.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] J. Wei, Q. Zhou, 和 Y. Cai，“基于诗人的诗歌生成：用递归神经网络控制个人风格”，在*2018年国际计算、网络和通信会议（ICNC）*。IEEE，2018，第156–160页。'
- en: '[200] J. Hopkins and D. Kiela, “Automatically generating rhythmic verse with
    neural networks,” in *Proceedings of the 55th Annual Meeting of the ACL (Volume
    1: Long Papers)*, 2017, pp. 168–178.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] J. Hopkins 和 D. Kiela，“使用神经网络自动生成节奏诗歌”，在*第55届ACL年会会议记录（第1卷：长篇论文）*，2017，第168–178页。'
- en: '[201] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, “Language
    models are unsupervised multitask learners,” *OpenAI Blog*, vol. 1, no. 8, 2019.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, 和 I. Sutskever，“语言模型是无监督的多任务学习者”，*OpenAI博客*，第1卷，第8期，2019。'
- en: '[202] B. Bena and J. Kalita, “Introducing aspects of creativity in automatic
    poetry generation,” in *Intnl Conf on NLP*, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] B. Bena 和 J. Kalita，“在自动诗歌生成中引入创造力的方面”，在*国际自然语言处理会议*，2019。'
- en: '[203] S. Tucker and J. Kalita, “Genrating believable poetry in multiple languages
    using gpt-2,” in *Technical Report, University of Colorado, Colorado Springs*,
    2019.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] S. Tucker 和 J. Kalita，“使用gpt-2生成多语言的可信诗歌”，在*科罗拉多大学科罗拉多斯普林斯分校技术报告*，2019。'
- en: '[204] Z. Yu, J. Tan, and X. Wan, “A neural approach to pun generation,” in
    *Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics
    (Vol. 1: Long Papers)*, 2018, pp. 1650–1660.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Z. Yu, J. Tan, 和 X. Wan，“一种神经网络方法生成双关语”，在*第56届计算语言学协会年会（第1卷：长篇论文）*，2018，第1650–1660页。'
- en: '[205] H. Ren and Q. Yang, “Neural joke generation,” *Final Project Reports
    of Course CS224n*, 2017.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] H. Ren 和 Q. Yang，“神经网络笑话生成”，*CS224n课程最终项目报告*，2017。'
- en: '[206] B. Chippada and S. Saha, “Knowledge amalgam: Generating jokes and quotes
    together,” *arXiv preprint arXiv:1806.04387*, 2018.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] B. Chippada 和 S. Saha，“知识融合：生成笑话和名言”，*arXiv 预印本 arXiv:1806.04387*，2018。'
- en: '[207] P. Jain, P. Agrawal, A. Mishra, M. Sukhwani, A. Laha, and K. Sankaranarayanan,
    “Story generation from sequence of independent short descriptions,” *arXiv preprint
    arXiv:1707.05501*, 2017.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] P. Jain, P. Agrawal, A. Mishra, M. Sukhwani, A. Laha, 和 K. Sankaranarayanan，“从独立的简短描述序列生成故事”，*arXiv
    预印本 arXiv:1707.05501*，2017。'
- en: '[208] N. Peng, M. Ghazvininejad, J. May, and K. Knight, “Towards controllable
    story generation,” in *Proceedings of the First Workshop on Storytelling*, 2018,
    pp. 43–49.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] N. Peng, M. Ghazvininejad, J. May, 和 K. Knight，“迈向可控的故事生成”，在*首次讲故事工作坊会议记录*，2018，第43–49页。'
- en: '[209] L. J. Martin, P. Ammanabrolu, X. Wang, W. Hancock, S. Singh, B. Harrison,
    and M. O. Riedl, “Event representations for automated story generation with deep
    neural nets,” in *Thirty-Second AAAI Conference on Artificial Intelligence*, 2018.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] L. J. Martin, P. Ammanabrolu, X. Wang, W. Hancock, S. Singh, B. Harrison,
    和 M. O. Riedl，“用于自动化故事生成的事件表示与深度神经网络”，在*第32届AAAI人工智能大会*，2018。'
- en: '[210] E. Clark, Y. Ji, and N. A. Smith, “Neural text generation in stories
    using entity representations as context,” in *Proceedings of the 2018 Conference
    of the North American Chapter of the ACL: Human Language Technologies, Vol. 1
    (Long Papers)*, 2018, pp. 2250–2260.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] E. Clark, Y. Ji, 和 N. A. Smith，“在故事中使用实体表示作为上下文的神经文本生成”，在*2018年北美ACL会议会议记录：人类语言技术，第1卷（长篇论文）*，2018，第2250–2260页。'
- en: '[211] J. Xu, Y. Zhang, Q. Zeng, X. Ren, X. Cai, and X. Sun, “A skeleton-based
    model for promoting coherence among sentences in narrative story generation,”
    *arXiv preprint arXiv:1808.06945*, 2018.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] J. Xu, Y. Zhang, Q. Zeng, X. Ren, X. Cai, 和 X. Sun，“一种基于骨架的模型以促进叙事故事生成中的句子一致性”，*arXiv
    预印本 arXiv:1808.06945*，2018。'
- en: '[212] M. Drissi, O. Watkins, and J. Kalita, “Hierarchical text generation using
    an outline,” *Intl Conf on NLP*, 2018.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] M. Drissi, O. Watkins, 和 J. Kalita，“使用大纲的层次文本生成”，*国际自然语言处理会议*，2018。'
- en: '[213] Q. Huang, Z. Gan, A. Celikyilmaz, D. Wu, J. Wang, and X. He, “Hierarchically
    structured reinforcement learning for topically coherent visual story generation,”
    *arXiv preprint arXiv:1805.08191*, 2018.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Q. Huang, Z. Gan, A. Celikyilmaz, D. Wu, J. Wang, 和 X. He，“用于主题一致视觉故事生成的层次结构强化学习，”
    *arXiv预印本 arXiv:1805.08191*，2018。'
- en: '[214] A. Fan, M. Lewis, and Y. Dauphin, “Hierarchical neural story generation,”
    *arXiv preprint arXiv:1805.04833*, 2018.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] A. Fan, M. Lewis, 和 Y. Dauphin，“层次神经故事生成，” *arXiv预印本 arXiv:1805.04833*，2018。'
- en: '[215] J. Li, M.-T. Luong, and D. Jurafsky, “A hierarchical neural autoencoder
    for paragraphs and documents,” *arXiv preprint arXiv:1506.01057*, 2015.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] J. Li, M.-T. Luong, 和 D. Jurafsky，“用于段落和文档的层次神经自编码器，” *arXiv预印本 arXiv:1506.01057*，2015。'
- en: '[216] K. Lin, D. Li, X. He, Z. Zhang, and M.-T. Sun, “Adversarial ranking for
    language generation,” in *Advances in Neural Information Processing Systems*,
    2017, pp. 3155–3165.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] K. Lin, D. Li, X. He, Z. Zhang, 和 M.-T. Sun，“用于语言生成的对抗排序，” 见于 *神经信息处理系统进展*，2017,
    第3155–3165页。'
- en: '[217] P. Tambwekar, M. Dhuliawala, A. Mehta, L. J. Martin, B. Harrison, and
    M. O. Riedl, “Controllable neural story generation via reward shaping,” *arXiv
    preprint arXiv:1809.10736*, 2018.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] P. Tambwekar, M. Dhuliawala, A. Mehta, L. J. Martin, B. Harrison, 和 M.
    O. Riedl，“通过奖励塑造进行可控神经故事生成，” *arXiv预印本 arXiv:1809.10736*，2018。'
- en: '[218] Y. Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao, D. Shen, and L. Carin, “Adversarial
    feature matching for text generation,” in *34th International Conference on Machine
    Learning-Vol 70*.   JMLR, 2017.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Y. Zhang, Z. Gan, K. Fan, Z. Chen, R. Henao, D. Shen, 和 L. Carin，“用于文本生成的对抗特征匹配，”
    见于 *第34届国际机器学习会议-第70卷*。 JMLR, 2017。'
- en: '[219] L. Chen, S. Dai, C. Tao, H. Zhang, Z. Gan, D. Shen, Y. Zhang, G. Wang,
    R. Zhang, and L. Carin, “Adversarial text generation via feature-mover’s distance,”
    in *Advances in Neural Information Processing Systems*, 2018, pp. 4666–4677.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] L. Chen, S. Dai, C. Tao, H. Zhang, Z. Gan, D. Shen, Y. Zhang, G. Wang,
    R. Zhang, 和 L. Carin，“通过特征移动者距离进行对抗文本生成，” 见于 *神经信息处理系统进展*，2018, 第4666–4677页。'
- en: '[220] J. Guo, S. Lu, H. Cai, W. Zhang, Y. Yu, and J. Wang, “Long text generation
    via adversarial training with leaked information,” in *Thirty-Second AAAI Conference
    on Artificial Intelligence*, 2018.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] J. Guo, S. Lu, H. Cai, W. Zhang, Y. Yu, 和 J. Wang，“通过对抗训练和泄露信息进行长文本生成，”
    见于 *第三十二届AAAI人工智能会议*，2018。'
- en: '[221] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” *arXiv
    preprint arXiv:1312.6114*, 2013.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] D. P. Kingma 和 M. Welling，“自动编码变分贝叶斯，” *arXiv预印本 arXiv:1312.6114*，2013。'
- en: '[222] C. Doersch, “Tutorial on variational autoencoders,” *arXiv preprint arXiv:1606.05908*,
    2016.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] C. Doersch，“变分自编码器教程，” *arXiv预印本 arXiv:1606.05908*，2016。'
- en: '[223] I. V. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau, A. Courville,
    and Y. Bengio, “A hierarchical latent variable encoder-decoder model for generating
    dialogues,” in *Thirty-First AAAI Conference on Artificial Intelligence*, 2017.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] I. V. Serban, A. Sordoni, R. Lowe, L. Charlin, J. Pineau, A. Courville,
    和 Y. Bengio，“用于生成对话的层次潜变量编码解码模型，” 见于 *第三十一届AAAI人工智能会议*，2017。'
- en: '[224] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, and E. P. Xing, “Toward controlled
    generation of text,” in *Proceedings of the 34th International Conference on Machine
    Learning-Volume 70*.   JMLR. org, 2017, pp. 1587–1596.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Z. Hu, Z. Yang, X. Liang, R. Salakhutdinov, 和 E. P. Xing，“朝向受控文本生成，”
    见于 *第34届国际机器学习会议-第70卷*。 JMLR. org, 2017, 第1587–1596页。'
- en: '[225] W. Wang, Z. Gan, H. Xu, R. Zhang, G. Wang, D. Shen, C. Chen, and L. Carin,
    “Topic-guided variational autoencoders for text generation,” *arXiv preprint arXiv:1903.07137*,
    2019.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] W. Wang, Z. Gan, H. Xu, R. Zhang, G. Wang, D. Shen, C. Chen, 和 L. Carin，“基于主题引导的变分自编码器用于文本生成，”
    *arXiv预印本 arXiv:1903.07137*，2019。'
- en: '[226] A. Holtzman, J. Buys, M. Forbes, and Y. Choi, “The curious case of neural
    text degeneration,” *arXiv preprint arXiv:1904.09751*, 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] A. Holtzman, J. Buys, M. Forbes, 和 Y. Choi，“神经文本退化的奇怪案例，” *arXiv预印本 arXiv:1904.09751*，2019。'
- en: '[227] E. Clark, A. Celikyilmaz, and N. A. Smith, “Sentence mover, similarity:
    Automatic evaluation for multi-sentence texts,” in *Proceedings of the 57th Annual
    Meeting of the Association for Computational Linguistics*, 2019, pp. 2748–2760.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] E. Clark, A. Celikyilmaz, 和 N. A. Smith，“句子移动者，相似性：多句文本的自动评估，” 见于 *第57届计算语言学协会年会论文集*，2019,
    第2748–2760页。'
- en: '[228] T. B. Hashimoto, H. Zhang, and P. Liang, “Unifying human and statistical
    evaluation for natural language generation,” *arXiv preprint arXiv:1904.02792*,
    2019.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] T. B. Hashimoto, H. Zhang, 和 P. Liang，“统一人类和统计评估自然语言生成，” *arXiv预印本 arXiv:1904.02792*，2019。'
- en: '[229] A. Gatt and E. Krahmer, “Survey of the state of the art in natural language
    generation: Core tasks, applications and evaluation,” *Journal of Artificial Intelligence
    Research*, vol. 61, pp. 65–170, 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] A. Gatt 和 E. Krahmer，"自然语言生成的最新进展调查：核心任务、应用和评估"，*人工智能研究期刊*，第61卷，第65–170页，2018年。'
- en: '[230] M. Hossain, F. Sohel, M. F. Shiratuddin, and H. Laga, “A comprehensive
    survey of deep learning for image captioning,” *ACM Computing Surveys (CSUR)*,
    vol. 51, no. 6, p. 118, 2019.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] M. Hossain, F. Sohel, M. F. Shiratuddin 和 H. Laga，"深度学习在图像字幕生成中的综合调查"，*ACM计算机调查（CSUR）*，第51卷，第6期，第118页，2019年。'
- en: '[231] X. Liu, Q. Xu, and N. Wang, “A survey on deep neural network-based image
    captioning,” *The Visual Computer*, vol. 35, no. 3, 2019.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] X. Liu, Q. Xu 和 N. Wang，"基于深度神经网络的图像字幕生成调查"，*视觉计算*，第35卷，第3期，2019年。'
- en: '[232] J. Krantz and J. Kalita, “Abstractive summarization using attentive neural
    techniques,” *Intl Conf on NLP*, 2018.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] J. Krantz 和 J. Kalita，"使用注意力神经技术的抽象摘要"，*国际自然语言处理会议*，2018年。'
- en: '[233] M. Ranzato, S. Chopra, M. Auli, and W. Zaremba, “Sequence level training
    with recurrent neural networks,” *arXiv preprint arXiv:1511.06732*, 2015.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] M. Ranzato, S. Chopra, M. Auli 和 W. Zaremba，"使用递归神经网络的序列级训练"，*arXiv 预印本
    arXiv:1511.06732*，2015年。'
- en: '[234] J. Gehring, M. Auli, D. Grangier, D. Yarats, and Y. N. Dauphin, “Convolutional
    sequence to sequence learning,” *arXiv preprint arXiv:1705.03122*, 2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] J. Gehring, M. Auli, D. Grangier, D. Yarats 和 Y. N. Dauphin，"卷积序列到序列学习"，*arXiv
    预印本 arXiv:1705.03122*，2017年。'
- en: '[235] H. Zhang, Y. Gong, Y. Yan, N. Duan, J. Xu, J. Wang, M. Gong, and M. Zhou,
    “Pretraining-based natural language generation for text summarization,” *arXiv
    preprint arXiv:1902.09243*, 2019.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] H. Zhang, Y. Gong, Y. Yan, N. Duan, J. Xu, J. Wang, M. Gong 和 M. Zhou，"基于预训练的自然语言生成用于文本摘要"，*arXiv
    预印本 arXiv:1902.09243*，2019年。'
- en: '[236] L. Dong, F. Wei, M. Zhou, and K. Xu, “Question answering over freebase
    with multi-column convolutional neural networks,” in *ACL and International Joint
    Conf on NLP*, vol. 1, 2015, pp. 260–269.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] L. Dong, F. Wei, M. Zhou 和 K. Xu，"使用多列卷积神经网络在Freebase上进行问答"，在*ACL和国际联合自然语言处理会议*，第1卷，2015年，第260–269页。'
- en: '[237] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia,
    and T. Lillicrap, “A simple neural network module for relational reasoning,” in
    *NIPS*, 2017, pp. 4974–4983.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] A. Santoro, D. Raposo, D. G. Barrett, M. Malinowski, R. Pascanu, P. Battaglia
    和 T. Lillicrap，"用于关系推理的简单神经网络模块"，在*NIPS*，2017年，第4974–4983页。'
- en: '[238] D. Raposo, A. Santoro, D. Barrett, R. Pascanu, T. Lillicrap, and P. Battaglia,
    “Discovering objects and their relations from entangled scene representations,”
    *arXiv preprint arXiv:1702.05068*, 2017.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] D. Raposo, A. Santoro, D. Barrett, R. Pascanu, T. Lillicrap 和 P. Battaglia，"从纠缠的场景表示中发现对象及其关系"，*arXiv
    预印本 arXiv:1702.05068*，2017年。'
- en: '[239] W. Yang, Y. Xie, A. Lin, X. Li, L. Tan, K. Xiong, M. Li, and J. Lin,
    “End-to-end open-domain question answering with bertserini,” *arXiv preprint arXiv:1902.01718*,
    2019.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] W. Yang, Y. Xie, A. Lin, X. Li, L. Tan, K. Xiong, M. Li 和 J. Lin，"基于bertserini的端到端开放域问答"，*arXiv
    预印本 arXiv:1902.01718*，2019年。'
- en: '[240] H. Schwenk, “Continuous space translation models for phrase-based statistical
    machine translation,” *COLING*, pp. 1071–1080, 2012.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] H. Schwenk，"基于短语的统计机器翻译中的连续空间翻译模型"，*COLING*，第1071–1080页，2012年。'
- en: '[241] T. Deselaers, S. Hasan, O. Bender, and H. Ney, “A deep learning approach
    to machine transliteration,” in *Workshop on Statistical Machine Translation*.   ACL,
    2009, pp. 233–241.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] T. Deselaers, S. Hasan, O. Bender 和 H. Ney，"深度学习方法用于机器音译"，在*统计机器翻译研讨会*，ACL，2009年，第233–241页。'
- en: '[242] N. Kalchbrenner and P. Blunsom, “Recurrent continuous translation models,”
    in *EMNLP*, 2013, pp. 1700–1709.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] N. Kalchbrenner 和 P. Blunsom，"递归连续翻译模型"，在*EMNLP*，2013年，第1700–1709页。'
- en: '[243] R. Collobert and J. Weston, “A unified architecture for natural language
    processing: Deep neural networks with multitask learning,” in *ICML*, 2008, pp.
    160–167.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] R. Collobert 和 J. Weston，"自然语言处理的统一架构：具有多任务学习的深度神经网络"，在*ICML*，2008年，第160–167页。'
- en: '[244] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H. Schwenk,
    and Y. Bengio, “Learning phrase representations using rnn encoder-decoder for
    statistical machine translation,” *arXiv preprint arXiv:1406.1078*, 2014.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] K. Cho, B. Van Merriënboer, C. Gulcehre, D. Bahdanau, F. Bougares, H.
    Schwenk 和 Y. Bengio，"使用RNN编码器-解码器学习短语表示，用于统计机器翻译"，*arXiv 预印本 arXiv:1406.1078*，2014年。'
- en: '[245] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *NIPS*, 2014, pp. 3104–3112.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] I. Sutskever, O. Vinyals 和 Q. V. Le，"使用神经网络的序列到序列学习"，在*NIPS*，2014年，第3104–3112页。'
- en: '[246] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun,
    Y. Cao, Q. Gao, K. Macherey *et al.*, “Google’s neural machine translation system:
    Bridging the gap between human and machine translation,” *arXiv preprint arXiv:1609.08144*,
    2016.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] Y. Wu, M. Schuster, Z. Chen, Q. V. Le, M. Norouzi, W. Macherey, M. Krikun,
    Y. Cao, Q. Gao, K. Macherey *等*，“谷歌的神经机器翻译系统：弥合人类翻译和机器翻译之间的差距，” *arXiv 预印本 arXiv:1609.08144*，2016年。'
- en: '[247] D. Britz, A. Goldie, T. Luong, and Q. Le, “Massive exploration of neural
    machine translation architectures,” *arXiv preprint arXiv:1703.03906*, 2017.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] D. Britz, A. Goldie, T. Luong 和 Q. Le，“神经机器翻译架构的大规模探索，” *arXiv 预印本 arXiv:1703.03906*，2017年。'
- en: '[248] R. Sennrich, O. Firat, K. Cho, A. Birch, B. Haddow, J. Hitschler, M. Junczys-Dowmunt,
    S. Läubli, A. V. M. Barone, J. Mokry *et al.*, “Nematus: a toolkit for neural
    machine translation,” *arXiv preprint arXiv:1703.04357*, 2017.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] R. Sennrich, O. Firat, K. Cho, A. Birch, B. Haddow, J. Hitschler, M.
    Junczys-Dowmunt, S. Läubli, A. V. M. Barone, J. Mokry *等*，“Nematus：神经机器翻译工具包，”
    *arXiv 预印本 arXiv:1703.04357*，2017年。'
- en: '[249] G. Klein, Y. Kim, Y. Deng, J. Senellart, and A. M. Rush, “Opennmt: Open-source
    toolkit for neural machine translation,” *arXiv preprint arXiv:1701.02810*, 2017.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] G. Klein, Y. Kim, Y. Deng, J. Senellart 和 A. M. Rush，“Opennmt：神经机器翻译的开源工具包，”
    *arXiv 预印本 arXiv:1701.02810*，2017年。'
- en: '[250] R. Sennrich and B. Haddow, “Linguistic input features improve neural
    machine translation,” *arXiv preprint arXiv:1606.02892*, 2016.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] R. Sennrich 和 B. Haddow，“语言输入特征改善神经机器翻译，” *arXiv 预印本 arXiv:1606.02892*，2016年。'
- en: '[251] K. Ahmed, N. S. Keskar, and R. Socher, “Weighted transformer network
    for machine translation,” *arXiv preprint arXiv:1711.02132*, 2017.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] K. Ahmed, N. S. Keskar 和 R. Socher，“用于机器翻译的加权变换器网络，” *arXiv 预印本 arXiv:1711.02132*，2017年。'
- en: '[252] S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber *et al.*, “Gradient
    flow in recurrent nets: the difficulty of learning long-term dependencies,” 2001.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber *等*，“递归网络中的梯度流：学习长期依赖的难度，”2001年。'
- en: '[253] M. Cettolo, J. Niehues, S. Stüker, L. Bentivogli, and M. Federico, “Report
    on the 11th iwslt evaluation campaign, iwslt 2014,” in *International Workshop
    on Spoken Language Translation, Hanoi*, 2014.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] M. Cettolo, J. Niehues, S. Stüker, L. Bentivogli 和 M. Federico，“第11届iwslt评估活动报告，iwslt
    2014，”在 *国际口语语言翻译研讨会，河内*，2014年。'
- en: '[254] J. R. Medina and J. Kalita, “Parallel attention mechanisms in neural
    machine translation,” *arXiv preprint arXiv:1810.12427*, 2018.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] J. R. Medina 和 J. Kalita，“神经机器翻译中的并行注意机制，” *arXiv 预印本 arXiv:1810.12427*，2018年。'
- en: '[255] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, “Bleu: a method for automatic
    evaluation of machine translation,” in *ACL*, 2002, pp. 311–318.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] K. Papineni, S. Roukos, T. Ward 和 W.-J. Zhu，“Bleu：一种自动评估机器翻译的方法，”在 *ACL*，2002年，页码311–318。'
- en: '[256] M. Ghazvininejad, O. Levy, Y. Liu, and L. Zettlemoyer, “Constant-time
    machine translation with conditional masked language models,” *arXiv preprint
    arXiv:1904.09324*, 2019.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] M. Ghazvininejad, O. Levy, Y. Liu 和 L. Zettlemoyer，“使用条件掩码语言模型的常数时间机器翻译，”
    *arXiv 预印本 arXiv:1904.09324*，2019年。'
- en: '[257] G. Lample and A. Conneau, “Cross-lingual language model pretraining,”
    *arXiv preprint arXiv:1901.07291*, 2019.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] G. Lample 和 A. Conneau，“跨语言语言模型预训练，” *arXiv 预印本 arXiv:1901.07291*，2019年。'
- en: '[258] M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. Foster, L. Jones,
    N. Parmar, M. Schuster, Z. Chen *et al.*, “The best of both worlds: Combining
    recent advances in neural machine translation,” *arXiv preprint arXiv:1804.09849*,
    2018.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] M. X. Chen, O. Firat, A. Bapna, M. Johnson, W. Macherey, G. Foster, L.
    Jones, N. Parmar, M. Schuster, Z. Chen *等*，“两全其美：结合神经机器翻译的最新进展，” *arXiv 预印本 arXiv:1804.09849*，2018年。'
- en: '[259] M. Denkowski and G. Neubig, “Stronger baselines for trustable results
    in neural machine translation,” *arXiv preprint arXiv:1706.09733*, 2017.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] M. Denkowski 和 G. Neubig，“神经机器翻译中的强基线以获得可靠结果，” *arXiv 预印本 arXiv:1706.09733*，2017年。'
- en: '[260] P. Koehn and R. Knowles, “Six challenges for neural machine translation,”
    *arXiv preprint arXiv:1706.03872*, 2017.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] P. Koehn 和 R. Knowles，“神经机器翻译的六大挑战，” *arXiv 预印本 arXiv:1706.03872*，2017年。'
- en: '[261] S. Kuang, D. Xiong, W. Luo, and G. Zhou, “Modeling coherence for neural
    machine translation with dynamic and topic caches,” in *COLING*, 2018, pp. 596–606.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] S. Kuang, D. Xiong, W. Luo 和 G. Zhou，“通过动态和主题缓存建模神经机器翻译的连贯性，”在 *COLING*，2018年，页码596–606。'
- en: '[262] M.-T. Luong, I. Sutskever, Q. V. Le, O. Vinyals, and W. Zaremba, “Addressing
    the rare word problem in neural machine translation,” *arXiv preprint arXiv:1410.8206*,
    2014.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] M.-T. Luong, I. Sutskever, Q. V. Le, O. Vinyals 和 W. Zaremba，“解决神经机器翻译中的稀有词问题，”
    *arXiv 预印本 arXiv:1410.8206*，2014年。'
- en: '[263] R. Sennrich, B. Haddow, and A. Birch, “Neural machine translation of
    rare words with subword units,” *arXiv preprint arXiv:1508.07909*, 2015.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] R. Sennrich, B. Haddow, 和 A. Birch，“使用子词单元的稀有词神经机器翻译，” *arXiv 预印本 arXiv:1508.07909*，2015年。'
- en: '[264] M. Mager, E. Mager, A. Medina-Urrea, I. Meza, and K. Kann, “Lost in translation:
    Analysis of information loss during machine translation between polysynthetic
    and fusional languages,” *arXiv preprint arXiv:1807.00286*, 2018.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] M. Mager, E. Mager, A. Medina-Urrea, I. Meza, 和 K. Kann，“翻译中的信息丢失：多合成语言与融合语言之间的机器翻译分析，”
    *arXiv 预印本 arXiv:1807.00286*，2018年。'
- en: '[265] M. Ott, M. Auli, D. Granger, and M. Ranzato, “Analyzing uncertainty in
    neural machine translation,” *arXiv preprint arXiv:1803.00047*, 2018.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[265] M. Ott, M. Auli, D. Granger, 和 M. Ranzato，“分析神经机器翻译中的不确定性，” *arXiv 预印本
    arXiv:1803.00047*，2018年。'
- en: '[266] W. Wang, T. Watanabe, M. Hughes, T. Nakagawa, and C. Chelba, “Denoising
    neural machine translation training with trusted data and online data selection,”
    *arXiv preprint arXiv:1809.00068*, 2018.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[266] W. Wang, T. Watanabe, M. Hughes, T. Nakagawa, 和 C. Chelba，“使用可信数据和在线数据选择进行神经机器翻译训练去噪，”
    *arXiv 预印本 arXiv:1809.00068*，2018年。'
- en: '[267] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat,
    F. Viégas, M. Wattenberg, G. Corrado *et al.*, “Google’s multilingual neural machine
    translation system: enabling zero-shot translation,” *arXiv preprint arXiv:1611.04558*,
    2016.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[267] M. Johnson, M. Schuster, Q. V. Le, M. Krikun, Y. Wu, Z. Chen, N. Thorat,
    F. Viégas, M. Wattenberg, G. Corrado *等*，“谷歌的多语言神经机器翻译系统：实现零样本翻译，” *arXiv 预印本
    arXiv:1611.04558*，2016年。'
- en: '[268] D. Jurafsky and J. Martin, *Speech & language processing (3rd. Edition
    Draft)*.   Pearson Education, 2017.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[268] D. Jurafsky 和 J. Martin，*语音与语言处理（第3版草稿）*。Pearson Education，2017年。'
- en: '[269] L. Zheng, H. Wang, and S. Gao, “Sentimental feature selection for sentiment
    analysis of chinese online reviews,” *Intnl J. of Machine Learning and Cybernetics*,
    vol. 9, no. 1, pp. 75–84, 2018.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[269] L. Zheng, H. Wang, 和 S. Gao，“中文在线评论的情感特征选择，” *国际机器学习与网络学报*，第9卷，第1期，页75–84，2018年。'
- en: '[270] M. Etter, E. Colleoni, L. Illia, K. Meggiorin, and A. D’Eugenio, “Measuring
    organizational legitimacy in social media: Assessing citizens’ judgments with
    sentiment analysis,” *Business & Society*, vol. 57, no. 1, pp. 60–97, 2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[270] M. Etter, E. Colleoni, L. Illia, K. Meggiorin, 和 A. D’Eugenio，“在社交媒体中衡量组织合法性：通过情感分析评估公民的判断，”
    *商业与社会*，第57卷，第1期，页60–97，2018年。'
- en: '[271] M. Cliche, “Bb_twtr at semeval-2017 task 4: Twitter sentiment analysis
    with cnns and lstms,” *arXiv preprint arXiv:1704.06125*, 2017.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[271] M. Cliche，“Bb_twtr 在 semeval-2017 任务 4 中：使用 CNN 和 LSTM 的推特情感分析，” *arXiv
    预印本 arXiv:1704.06125*，2017年。'
- en: '[272] D. G. Bobrow, “Natural language input for a computer problem solving
    system,” 1964.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[272] D. G. Bobrow，“计算机问题解决系统的自然语言输入，” 1964年。'
- en: '[273] J. Weizenbaum, “Eliza, a computer program for the study of natural language
    communication between man and machine,” *CACM*, vol. 9, no. 1, pp. 36–45, 1966.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[273] J. Weizenbaum，“Eliza：一个用于研究人机自然语言交流的计算机程序，” *CACM*，第9卷，第1期，页36–45，1966年。'
- en: '[274] T. Winograd, “Procedures as a representation for data in a computer program
    for understanding natural language,” MIT, Tech. Rep., 1971.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[274] T. Winograd，“作为数据表示的程序：用于理解自然语言的计算机程序，” MIT，技术报告，1971年。'
- en: '[275] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, and S. R. Bowman, “Glue:
    A multi-task benchmark and analysis platform for natural language understanding,”
    *arXiv preprint arXiv:1804.07461*, 2018.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[275] A. Wang, A. Singh, J. Michael, F. Hill, O. Levy, 和 S. R. Bowman，“Glue：用于自然语言理解的多任务基准和分析平台，”
    *arXiv 预印本 arXiv:1804.07461*，2018年。'
- en: '[276] C. D. Schuman, T. E. Potok, R. M. Patton, J. D. Birdwell, M. E. Dean,
    G. S. Rose, and J. S. Plank, “A survey of neuromorphic computing and neural networks
    in hardware,” *arXiv preprint arXiv:1705.06963*, 2017.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[276] C. D. Schuman, T. E. Potok, R. M. Patton, J. D. Birdwell, M. E. Dean,
    G. S. Rose, 和 J. S. Plank，“神经形态计算和硬件神经网络的调查，” *arXiv 预印本 arXiv:1705.06963*，2017年。'
- en: '[277] J. Hennessy and D. Patterson, *Computer architecture: a quantitative
    approach*.   Elsevier, 2017.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[277] J. Hennessy 和 D. Patterson，*计算机体系结构：定量方法*。Elsevier，2017年。'
- en: '[278] D. Monroe, “Neuromorphic computing gets ready for the (really) big time,”
    *CACM*, vol. 57, no. 6, pp. 13–15, 2014.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[278] D. Monroe，“神经形态计算准备迎接（真正的）大时代，” *CACM*，第57卷，第6期，页13–15，2014年。'
