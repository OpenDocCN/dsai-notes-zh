["```py\n**class** **Char3Model**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n\n        *# The 'green arrow' from our diagram*\n        self.l_in = nn.Linear(n_fac, n_hidden)\n\n        *# The 'orange arrow' from our diagram*\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n\n        *# The 'blue arrow' from our diagram*\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n\n    **def** forward(self, c1, c2, c3):\n        in1 = F.relu(self.l_in(self.e(c1)))\n        in2 = F.relu(self.l_in(self.e(c2)))\n        in3 = F.relu(self.l_in(self.e(c3)))\n\n        h = V(torch.zeros(in1.size()).cuda())\n        h = F.tanh(self.l_hidden(h+in1))\n        h = F.tanh(self.l_hidden(h+in2))\n        h = F.tanh(self.l_hidden(h+in3))\n\n        **return** F.log_softmax(self.l_out(h))\n```", "```py\n**class** **CharLoopModel**(nn.Module):\n    *# This is an RNN!*\n    **def** __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.l_in = nn.Linear(n_fac, n_hidden)\n        self.l_hidden = nn.Linear(n_hidden, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n\n    **def** forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(bs, n_hidden).cuda())\n        **for** c **in** cs:\n            inp = F.relu(self.l_in(self.e(c)))\n            h = F.tanh(self.l_hidden(h+inp))\n\n        **return** F.log_softmax(self.l_out(h), dim=-1)\n```", "```py\n**class** **CharRnn**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac):\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n\n    **def** forward(self, *cs):\n        bs = cs[0].size(0)\n        h = V(torch.zeros(1, bs, n_hidden))\n        inp = self.e(torch.stack(cs))\n        outp,h = self.rnn(inp, h)\n\n        **return** F.log_softmax(self.l_out(outp[-1]), dim=-1)\n```", "```py\n**class** **CharSeqStatefulRnn**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac, bs):\n        self.vocab_size = vocab_size\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        **self.init_hidden(bs)**\n\n    **def** forward(self, cs):\n        bs = cs[0].size(0)\n        **if** self.h.size(1) != bs: self.init_hidden(bs)\n        outp,h = self.rnn(self.e(cs), self.h)\n        **self.h = repackage_var(h)**\n        **return** F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n\n    **def** init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))\n```", "```py\ndef repackage_var(h):return Variable(h.data) if type(h) == Variable else tuple(repackage_var(v) for v in h)\n```", "```py\n**from** **torchtext** **import** vocab, data **from** **fastai.nlp** **import** * \n**from** **fastai.lm_rnn** **import** * PATH='data/nietzsche/' TRN_PATH = 'trn/' \nVAL_PATH = 'val/' \nTRN = f'**{PATH}{TRN_PATH}**' \nVAL = f'**{PATH}{VAL_PATH}**'%ls {PATH}\n*models/  nietzsche.txt  trn/  val/*%ls {PATH}trn\n*trn.txt*\n```", "```py\nTEXT = data.Field(lower=**True**, tokenize=list)\nbs=64; bptt=8; n_fac=42; n_hidden=256\n\nFILES = dict(train=TRN_PATH, validation=VAL_PATH, test=VAL_PATH)\nmd = LanguageModelData.from_text_files(PATH, TEXT, **FILES, bs=bs, bptt=bptt, min_freq=3)\n\nlen(md.trn_dl), md.nt, len(md.trn_ds), len(md.trn_ds[0].text)\n*(963, 56, 1, 493747)*\n```", "```py\n**class** **CharSeqStatefulRnn**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac, bs):\n        self.vocab_size = vocab_size\n        super().__init__()\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.RNN(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n\n    **def** forward(self, cs):\n        bs = cs[0].size(0)\n        **if self.h.size(1) != bs: self.init_hidden(bs)**\n        outp,h = self.rnn(self.e(cs), self.h)\n        self.h = repackage_var(h)\n        **return** **F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)**\n\n    **def** init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))\n```", "```py\nm = CharSeqStatefulRnn(md.nt, n_fac, 512).cuda() \nopt = optim.Adam(m.parameters(), 1e-3)fit(m, md, 4, opt, F.nll_loss)\n```", "```py\n**def** RNNCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    **return** F.tanh(F.linear(input, w_ih, b_ih) + F.linear(hidden, w_hh, b_hh))\n```", "```py\n**class** **CharSeqStatefulRnn2**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac, bs):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = **nn.RNNCell**(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n\n    **def** forward(self, cs):\n        bs = cs[0].size(0)\n        **if** self.h.size(1) != bs: self.init_hidden(bs)\n        outp = []\n        o = self.h\n        **for** c **in** cs: \n            o = self.rnn(self.e(c), o)\n            outp.append(o)\n        outp = self.l_out(torch.stack(outp))\n        self.h = repackage_var(o)\n        **return** F.log_softmax(outp, dim=-1).view(-1, self.vocab_size)\n\n    **def** init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))\n```", "```py\n**def** GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh):\n    gi = F.linear(input, w_ih, b_ih)\n    gh = F.linear(hidden, w_hh, b_hh)\n    i_r, i_i, i_n = gi.chunk(3, 1)\n    h_r, h_i, h_n = gh.chunk(3, 1)\n\n    resetgate = F.sigmoid(i_r + h_r)\n    inputgate = F.sigmoid(i_i + h_i)\n    newgate = F.tanh(i_n + resetgate * h_n)\n    **return** newgate + inputgate * (hidden - newgate)\n```", "```py\n**class** **CharSeqStatefulGRU**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac, bs):\n        super().__init__()\n        self.vocab_size = vocab_size\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.GRU(n_fac, n_hidden)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n\n    **def** forward(self, cs):\n        bs = cs[0].size(0)\n        **if** self.h.size(1) != bs: self.init_hidden(bs)\n        outp,h = self.rnn(self.e(cs), self.h)\n        self.h = repackage_var(h)\n        **return** F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n\n    **def** init_hidden(self, bs): self.h = V(torch.zeros(1, bs, n_hidden))\n```", "```py\n**from** **fastai** **import** sgdr\n\nn_hidden=512**class** **CharSeqStatefulLSTM**(nn.Module):\n    **def** __init__(self, vocab_size, n_fac, bs, nl):\n        super().__init__()\n        self.vocab_size,self.nl = vocab_size,nl\n        self.e = nn.Embedding(vocab_size, n_fac)\n        self.rnn = nn.LSTM(n_fac, n_hidden, nl, **dropout**=0.5)\n        self.l_out = nn.Linear(n_hidden, vocab_size)\n        self.init_hidden(bs)\n\n    **def** forward(self, cs):\n        bs = cs[0].size(0)\n        **if** self.h[0].size(1) != bs: self.init_hidden(bs)\n        outp,h = self.rnn(self.e(cs), self.h)\n        self.h = repackage_var(h)\n        **return** F.log_softmax(self.l_out(outp), dim=-1).view(-1, self.vocab_size)\n\n    **def** init_hidden(self, bs):\n **self.h = (V(torch.zeros(self.nl, bs, n_hidden)),\n                  V(torch.zeros(self.nl, bs, n_hidden)))**\n```", "```py\nm = CharSeqStatefulLSTM(md.nt, n_fac, 512, 2).cuda()\nlo = LayerOptimizer(optim.Adam, m, 1e-2, 1e-5)\n```", "```py\non_end = **lambda** sched, cycle: save_model(m, f'**{PATH}**models/cyc_**{cycle}**')cb = [CosAnneal(lo, len(md.trn_dl), cycle_mult=2, on_cycle_end=on_end)]fit(m, md, 2**4-1, lo.opt, F.nll_loss, callbacks=cb)\n```", "```py\n**def** get_next(inp):\n    idxs = TEXT.numericalize(inp)\n    p = m(VV(idxs.transpose(0,1)))\n    r = **torch.multinomial(p[-1].exp(), 1)**\n    **return** TEXT.vocab.itos[to_np(r)[0]]**def** get_next_n(inp, n):\n    res = inp\n    **for** i **in** range(n):\n        c = get_next(inp)\n        res += c\n        inp = inp[1:]+c\n    **return** resprint(get_next_n('for thos', 400))*for those the skemps), or imaginates, though they deceives. it should so each ourselvess and new present, step absolutely for the science.\" the contradity and measuring,  the whole!* *293\\. perhaps, that every life a values of blood of intercourse when it senses there is unscrupulus, his very rights, and still impulse, love? just after that thereby how made with the way anything, and set for harmless philos*\n```", "```py\n**from** **fastai.conv_learner** **import** *\nPATH = \"data/cifar10/\"\nos.makedirs(PATH,exist_ok=**True**)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\nstats = (np.array([ 0.4914 ,  0.48216,  0.44653]), np.array([ 0.24703,  0.24349,  0.26159]))**def** get_data(sz,bs):\n     tfms = **tfms_from_stats**(stats, sz, aug_tfms=[RandomFlipXY()], pad=sz//8)\n     **return** ImageClassifierData.from_paths(PATH, val_name='test', tfms=tfms, bs=bs)bs=256\n```", "```py\ndata = get_data(32,bs)\n\nlr=1e-2\n```", "```py\n**class** **SimpleNet**(nn.Module):\n    **def** __init__(self, layers):\n        super().__init__()\n        self.layers = **nn.ModuleList**([\n            nn.Linear(layers[i], layers[i + 1]) **for** i **in** range(len(layers) - 1)])\n\n    **def** forward(self, x):\n        x = x.view(x.size(0), -1)\n        **for** l **in** self.layers:\n            l_x = l(x)\n            x = F.relu(l_x)\n        **return** F.log_softmax(l_x, dim=-1)\n```", "```py\nlearn = ConvLearner.from_model_data(SimpleNet([32*32*3, 40,10]), data)\n```", "```py\nlearn, [o.numel() **for** o **in** learn.model.parameters()]*(SimpleNet(\n   (layers): ModuleList(\n     (0): Linear(in_features=3072, out_features=40)\n     (1): Linear(in_features=40, out_features=10)\n   )\n ), [122880, 40, 400, 10])*learn.summary()*OrderedDict([('Linear-1',\n              OrderedDict([('input_shape', [-1, 3072]),\n                           ('output_shape', [-1, 40]),\n                           ('trainable', True),\n                           ('nb_params', 122920)])),\n             ('Linear-2',\n              OrderedDict([('input_shape', [-1, 40]),\n                           ('output_shape', [-1, 10]),\n                           ('trainable', True),\n                           ('nb_params', 410)]))])*learn.lr_find()learn.sched.plot()\n```", "```py\n%time learn.fit(lr, 2)A Jupyter Widget[ 0\\.       1.7658   1.64148  0.42129]                       \n[ 1\\.       1.68074  1.57897  0.44131]                       \n\nCPU times: user 1min 11s, sys: 32.3 s, total: 1min 44s\nWall time: 55.1 s%time learn.fit(lr, 2, cycle_len=1)A Jupyter Widget[ 0\\.       1.60857  1.51711  0.46631]                       \n[ 1\\.       1.59361  1.50341  0.46924]                       \n\nCPU times: user 1min 12s, sys: 31.8 s, total: 1min 44s\nWall time: 55.3 s\n```", "```py\n**class** **ConvNet**(nn.Module):\n    **def** __init__(self, layers, c):\n        super().__init__()\n        self.layers = nn.ModuleList([\n            **nn.Conv2d(layers[i], layers[i + 1], kernel_size=3, stride=2)**\n            **for** i **in** range(len(layers) - 1)])\n        self.pool = nn.AdaptiveMaxPool2d(1)\n        self.out = nn.Linear(layers[-1], c)\n\n    **def** forward(self, x):\n        **for** l **in** self.layers: x = F.relu(l(x))\n        x = self.pool(x)\n        x = x.view(x.size(0), -1)\n        **return** F.log_softmax(self.out(x), dim=-1)\n```", "```py\nlearn = ConvLearner.from_model_data(ConvNet([3, 20, 40, 80], 10), data)learn.summary()*OrderedDict([('Conv2d-1',\n              OrderedDict([('input_shape', [-1, 3, 32, 32]),\n                           ('output_shape', [-1, 20, 15, 15]),\n                           ('trainable', True),\n                           ('nb_params', 560)])),\n             ('Conv2d-2',\n              OrderedDict([('input_shape', [-1, 20, 15, 15]),\n                           ('output_shape', [-1, 40, 7, 7]),\n                           ('trainable', True),\n                           ('nb_params', 7240)])),\n             ('Conv2d-3',\n              OrderedDict([('input_shape', [-1, 40, 7, 7]),\n                           ('output_shape', [-1, 80, 3, 3]),\n                           ('trainable', True),\n                           ('nb_params', 28880)])),\n             ('AdaptiveMaxPool2d-4',\n              OrderedDict([('input_shape', [-1, 80, 3, 3]),\n                           ('output_shape', [-1, 80, 1, 1]),\n                           ('nb_params', 0)])),\n             ('Linear-5',\n              OrderedDict([('input_shape', [-1, 80]),\n                           ('output_shape', [-1, 10]),\n                           ('trainable', True),\n                           ('nb_params', 810)]))])*\n```", "```py\nlearn.lr_find(**end_lr=100**)\nlearn.sched.plot()\n```", "```py\n%time learn.fit(1e-1, 2)*A Jupyter Widget**[ 0\\.       1.72594  1.63399  0.41338]                       \n[ 1\\.       1.51599  1.49687  0.45723]                       \n\nCPU times: user 1min 14s, sys: 32.3 s, total: 1min 46s\nWall time: 56.5 s*%time learn.fit(1e-1, 4, cycle_len=1)*A Jupyter Widget**[ 0\\.       1.36734  1.28901  0.53418]                       \n[ 1\\.       1.28854  1.21991  0.56143]                       \n[ 2\\.       1.22854  1.15514  0.58398]                       \n[ 3\\.       1.17904  1.12523  0.59922]                       \n\nCPU times: user 2min 21s, sys: 1min 3s, total: 3min 24s\nWall time: 1min 46s*\n```", "```py\n**class** **ConvLayer**(nn.Module):\n    **def** __init__(self, ni, nf):\n        super().__init__()\n        self.conv = nn.Conv2d(ni, nf, kernel_size=3, stride=2, padding=1)\n\n    **def** forward(self, x): **return** F.relu(self.conv(x))\n```", "```py\n**class** **ConvNet2**(nn.Module):\n    **def** __init__(self, layers, c):\n        super().__init__()\n        self.layers = nn.ModuleList([ConvLayer(layers[i], layers[i + 1])\n            **for** i **in** range(len(layers) - 1)])\n        self.out = nn.Linear(layers[-1], c)\n\n    **def** forward(self, x):\n        **for** l **in** self.layers: x = l(x)\n        x = **F.adaptive_max_pool2d(x, 1)**\n        x = x.view(x.size(0), -1)\n        **return** F.log_softmax(self.out(x), dim=-1)\n```", "```py\n**class** **BnLayer**(nn.Module):\n    **def** __init__(self, ni, nf, stride=2, kernel_size=3):\n        super().__init__()\n        self.conv = nn.Conv2d(ni, nf, kernel_size=kernel_size, \n                              stride=stride, bias=**False**, padding=1)\n        self.a = nn.Parameter(torch.zeros(nf,1,1))\n        self.m = nn.Parameter(torch.ones(nf,1,1))\n\n    **def** forward(self, x):\n        x = F.relu(self.conv(x))\n        x_chan = x.transpose(0,1).contiguous().view(x.size(1), -1)\n        **if** self.training:\n            **self.means = x_chan.mean(1)[:,None,None]**\n           ** self.stds  = x_chan.std (1)[:,None,None]**\n        **return** **(x-self.means) / self.stds *self.m + self.a**\n```", "```py\n**class** **ConvBnNet**(nn.Module):\n    **def** __init__(self, layers, c):\n        super().__init__()\n        **self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)**\n        self.layers = nn.ModuleList([**BnLayer**(layers[i], layers[i + 1])\n            **for** i **in** range(len(layers) - 1)])\n        self.out = nn.Linear(layers[-1], c)\n\n    **def** forward(self, x):\n        x = self.conv1(x)\n        **for** l **in** self.layers: x = l(x)\n        x = F.adaptive_max_pool2d(x, 1)\n        x = x.view(x.size(0), -1)\n        **return** F.log_softmax(self.out(x), dim=-1)\n```", "```py\n**class** **ConvBnNet2**(nn.Module):\n    **def** __init__(self, layers, c):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)\n        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n            **for** i **in** range(len(layers) - 1)])\n        self.layers2 = nn.ModuleList([BnLayer(layers[i+1], layers[i + 1], 1)\n            **for** i **in** range(len(layers) - 1)])\n        self.out = nn.Linear(layers[-1], c)\n\n    **def** forward(self, x):\n        x = self.conv1(x)\n        **for** l,l2 **in** zip(self.layers, self.layers2):\n            x = l(x)\n            x = l2(x)\n        x = F.adaptive_max_pool2d(x, 1)\n        x = x.view(x.size(0), -1)\n        **return** F.log_softmax(self.out(x), dim=-1)learn = ConvLearner.from_model_data((ConvBnNet2([10, 20, 40, 80, 160], 10), data)%time learn.fit(1e-2, 2)*A Jupyter Widget**[ 0\\.       1.53499  1.43782  0.47588]                       \n[ 1\\.       1.28867  1.22616  0.55537]                       \n\nCPU times: user 1min 22s, sys: 34.5 s, total: 1min 56s\nWall time: 58.2 s*%time learn.fit(1e-2, 2, cycle_len=1)*A Jupyter Widget**[ 0\\.       1.10933  1.06439  0.61582]                       \n[ 1\\.       1.04663  0.98608  0.64609]                       \n\nCPU times: user 1min 21s, sys: 32.9 s, total: 1min 54s\nWall time: 57.6 s*\n```", "```py\n**class** **ResnetLayer**(BnLayer):\n    **def** forward(self, x): **return** **x + super().forward(x)****class** **Resnet**(nn.Module):\n    **def** __init__(self, layers, c):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 10, kernel_size=5, stride=1, padding=2)\n        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n            **for** i **in** range(len(layers) - 1)])\n        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n            **for** i **in** range(len(layers) - 1)])\n        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n            **for** i **in** range(len(layers) - 1)])\n        self.out = nn.Linear(layers[-1], c)\n\n    **def** forward(self, x):\n        x = self.conv1(x)\n        **for** l,l2,l3 **in** zip(self.layers, self.layers2, self.layers3):\n            x = l3(l2(l(x)))\n        x = F.adaptive_max_pool2d(x, 1)\n        x = x.view(x.size(0), -1)\n        **return** F.log_softmax(self.out(x), dim=-1)\n```", "```py\nlearn = ConvLearner.from_model_data(Resnet([10, 20, 40, 80, 160], 10), data)wd=1e-5%time learn.fit(1e-2, 2, wds=wd)*A Jupyter Widget**[ 0\\.       1.58191  1.40258  0.49131]                       \n[ 1\\.       1.33134  1.21739  0.55625]                       \n\nCPU times: user 1min 27s, sys: 34.3 s, total: 2min 1s\nWall time: 1min 3s*%time learn.fit(1e-2, 3, cycle_len=1, cycle_mult=2, wds=wd)*A Jupyter Widget**[ 0\\.       1.11534  1.05117  0.62549]                       \n[ 1\\.       1.06272  0.97874  0.65185]                       \n[ 2\\.       0.92913  0.90472  0.68154]                        \n[ 3\\.       0.97932  0.94404  0.67227]                        \n[ 4\\.       0.88057  0.84372  0.70654]                        \n[ 5\\.       0.77817  0.77815  0.73018]                        \n[ 6\\.       0.73235  0.76302  0.73633]                        \n\nCPU times: user 5min 2s, sys: 1min 59s, total: 7min 1s\nWall time: 3min 39s*%time learn.fit(1e-2, 8, cycle_len=4, wds=wd)*A Jupyter Widget**[ 0\\.       0.8307   0.83635  0.7126 ]                        \n[ 1\\.       0.74295  0.73682  0.74189]                        \n[ 2\\.       0.66492  0.69554  0.75996]                        \n[ 3\\.       0.62392  0.67166  0.7625 ]                        \n[ 4\\.       0.73479  0.80425  0.72861]                        \n[ 5\\.       0.65423  0.68876  0.76318]                        \n[ 6\\.       0.58608  0.64105  0.77783]                        \n[ 7\\.       0.55738  0.62641  0.78721]                        \n[ 8\\.       0.66163  0.74154  0.7501 ]                        \n[ 9\\.       0.59444  0.64253  0.78106]                        \n[ 10\\.        0.53      0.61772   0.79385]                    \n[ 11\\.        0.49747   0.65968   0.77832]                    \n[ 12\\.        0.59463   0.67915   0.77422]                    \n[ 13\\.        0.55023   0.65815   0.78106]                    \n[ 14\\.        0.48959   0.59035   0.80273]                    \n[ 15\\.        0.4459    0.61823   0.79336]                    \n[ 16\\.        0.55848   0.64115   0.78018]                    \n[ 17\\.        0.50268   0.61795   0.79541]                    \n[ 18\\.        0.45084   0.57577   0.80654]                    \n[ 19\\.        0.40726   0.5708    0.80947]                    \n[ 20\\.        0.51177   0.66771   0.78232]                    \n[ 21\\.        0.46516   0.6116    0.79932]                    \n[ 22\\.        0.40966   0.56865   0.81172]                    \n[ 23\\.        0.3852    0.58161   0.80967]                    \n[ 24\\.        0.48268   0.59944   0.79551]                    \n[ 25\\.        0.43282   0.56429   0.81182]                    \n[ 26\\.        0.37634   0.54724   0.81797]                    \n[ 27\\.        0.34953   0.54169   0.82129]                    \n[ 28\\.        0.46053   0.58128   0.80342]                    \n[ 29\\.        0.4041    0.55185   0.82295]                    \n[ 30\\.        0.3599    0.53953   0.82861]                    \n[ 31\\.        0.32937   0.55605   0.82227]                    \n\nCPU times: user 22min 52s, sys: 8min 58s, total: 31min 51s\nWall time: 16min 38s*\n```", "```py\n**class** **Resnet2**(nn.Module):\n    **def** __init__(self, layers, c, p=0.5):\n        super().__init__()\n        self.conv1 = BnLayer(3, 16, stride=1, kernel_size=7)\n        self.layers = nn.ModuleList([BnLayer(layers[i], layers[i+1])\n            **for** i **in** range(len(layers) - 1)])\n        self.layers2 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n            **for** i **in** range(len(layers) - 1)])\n        self.layers3 = nn.ModuleList([ResnetLayer(layers[i+1], layers[i + 1], 1)\n            **for** i **in** range(len(layers) - 1)])\n        self.out = nn.Linear(layers[-1], c)\n        self.drop = nn.Dropout(p)\n\n    **def** forward(self, x):\n        x = self.conv1(x)\n        **for** l,l2,l3 **in** zip(self.layers, self.layers2, self.layers3):\n            x = l3(l2(l(x)))\n        x = F.adaptive_max_pool2d(x, 1)\n        x = x.view(x.size(0), -1)\n        x = self.drop(x)\n        **return** F.log_softmax(self.out(x), dim=-1)learn = ConvLearner.from_model_data(Resnet2([**16, 32, 64, 128, 256**], 10, 0.2), data)wd=1e-6%time learn.fit(1e-2, 2, wds=wd)\n%time learn.fit(1e-2, 3, cycle_len=1, cycle_mult=2, wds=wd)\n%time learn.fit(1e-2, 8, cycle_len=4, wds=wd)log_preds,y = learn.TTA()\npreds = np.mean(np.exp(log_preds),0)metrics.log_loss(y,preds), accuracy(preds,y)\n*(0.44507397166057938, 0.84909999999999997)*\n```", "```py\nPATH = \"data/dogscats/\"\nsz = 224\narch = resnet34  # <-- Name of the function \nbs = 64m = arch(pretrained=True) # Get a model w/ pre-trained weight loaded\nm*ResNet(\n  (conv1): Conv2d (3, 64,* ***kernel_size=(7, 7)****, stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n  (relu): ReLU(inplace)\n  (maxpool): MaxPool2d(kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n  (****layer1****): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d (64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n    )\n  )\n  (****layer2****): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d (64, 128, kernel_size=(3, 3),* ***stride=(2, 2)****, padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n      (downsample): Sequential(\n        (0): Conv2d (64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d (128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n    )\n  )* *...* *(avgpool): AvgPool2d(kernel_size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n  (fc): Linear(in_features=512, out_features=1000)\n)*\n```", "```py\nm = nn.Sequential(*children(m)[:-2], \n                  nn.Conv2d(512, 2, 3, padding=1), \n                  nn.AdaptiveAvgPool2d(1), Flatten(), \n                  nn.LogSoftmax())\n```", "```py\ntfms = tfms_from_model(arch, sz, aug_tfms=transforms_side_on, max_zoom=1.1)\ndata = ImageClassifierData.from_paths(PATH, tfms=tfms, bs=bs)learn = **ConvLearner.from_model_data**(m, data)learn.freeze_to(-4)learn.fit(0.01, 1)\nlearn.fit(0.01, 1, cycle_len=1)\n```", "```py\nf2=np.dot(np.rollaxis(**feat**,0,3), **py**)\nf2-=f2.min()\nf2/=f2.max()\nf2\n```", "```py\nsf = SaveFeatures(m[-4])\npy = m(Variable(x.cuda()))\nsf.remove()\n\npy = np.exp(to_np(py)[0]); py*array([ 1.,  0.], dtype=float32)*feat = np.maximum(0, sf.features[0])\nfeat.shape\n```", "```py\n**class** **SaveFeatures**():\n    features=**None**\n    **def** __init__(self, m): \n        self.hook = m.register_forward_hook(self.hook_fn)\n    **def** hook_fn(self, module, input, output): \n        self.features = to_np(output)\n    **def** remove(self): self.hook.remove()\n```"]