- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:53:25'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:53:25
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2107.01272] Physics-Guided Deep Learning for Dynamical Systems: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2107.01272] 物理引导的深度学习用于动力系统：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2107.01272](https://ar5iv.labs.arxiv.org/html/2107.01272)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2107.01272](https://ar5iv.labs.arxiv.org/html/2107.01272)
- en: 'Physics-Guided Deep Learning for Dynamical Systems: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物理引导的深度学习用于动力系统：综述
- en: Rui (Ray) Wang
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Rui (Ray) Wang
- en: Computer Science and Engineering
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学与工程
- en: University of California, San Diego
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学圣迭戈分校
- en: ruw020@ucsd.edu &Rose Yu
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: ruw020@ucsd.edu & Rose Yu
- en: Computer Science and Engineering
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机科学与工程
- en: University of California, San Diego
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 加州大学圣迭戈分校
- en: roseyu@ucsd.edu
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: roseyu@ucsd.edu
- en: Abstract
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Modeling complex physical dynamics is a fundamental task in science and engineering.
    Traditional physics-based models are sample efficient, and interpretable but often
    rely on rigid assumptions. Furthermore, direct numerical approximation is usually
    computationally intensive, requiring significant computational resources and expertise,
    and many real-world systems do not have fully-known governing laws. While deep
    learning (DL) provides novel alternatives for efficiently recognizing complex
    patterns and emulating nonlinear dynamics, its predictions do not necessarily
    obey the governing laws of physical systems, nor do they generalize well across
    different systems. Thus, the study of physics-guided DL emerged and has gained
    great progress. Physics-guided DL aims to take the best from both physics-based
    modeling and state-of-the-art DL models to better solve scientific problems. In
    this paper, we provide a structured overview of existing methodologies of integrating
    prior physical knowledge or physics-based modeling into DL, with a special emphasis
    on learning dynamical systems. We also discuss the fundamental challenges and
    emerging opportunities in the area.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 建模复杂的物理动态是科学和工程中的基本任务。传统的基于物理的模型样本效率高且可解释，但往往依赖于僵化的假设。此外，直接的数值近似通常计算量大，需要显著的计算资源和专业知识，许多现实世界的系统没有完全已知的控制规律。虽然深度学习（DL）为高效识别复杂模式和模拟非线性动态提供了新的选择，但其预测不一定遵循物理系统的控制规律，也不一定在不同系统之间具有良好的泛化能力。因此，物理引导的深度学习应运而生，并取得了很大进展。物理引导的深度学习旨在结合物理建模和最先进的深度学习模型的优点，更好地解决科学问题。本文提供了将先验物理知识或基于物理的建模整合到深度学习中的现有方法的结构化概述，特别强调了学习动力系统的方面。我们还讨论了该领域中的基本挑战和新兴机会。
- en: 1 Introduction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Modeling complex physical dynamics over a wide range of spatial and temporal
    scales is a fundamental task in a wide range of fields including, for example,
    fluid dynamics [[147](#bib.bib147)], cosmology [[156](#bib.bib156)], economics[[36](#bib.bib36)],
    and neuroscience [[69](#bib.bib69)].
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在广泛的空间和时间尺度上建模复杂的物理动态是许多领域中的基本任务，例如流体动力学[[147](#bib.bib147)]、宇宙学[[156](#bib.bib156)]、经济学[[36](#bib.bib36)]和神经科学[[69](#bib.bib69)]。
- en: Dynamical systems are mathematical objects that are used to describe the evolution
    of phenomena over time and space occurring in nature. Dynamical systems are commonly
    described with differential equations which are equations related to one or more
    unknown functions and their derivatives.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 动态系统是用于描述自然界现象在时间和空间中演变的数学对象。动态系统通常用微分方程描述，这些方程与一个或多个未知函数及其导数相关。
- en: Definition 1.
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 1.
- en: Fix an integer $k\geq 1$ and let $U$ denote an open subset of $\mathbb{R}^{n}$.
    Let $u:U\mapsto\mathbb{R}^{m}$ and we write $\bm{u}=(u^{1},...,u^{m})$, where
    $x\in U$. Then an expression of the form
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 设定一个整数$k\geq 1$，令$U$表示$\mathbb{R}^{n}$的一个开子集。令$u:U\mapsto\mathbb{R}^{m}$，并写作$\bm{u}=(u^{1},...,u^{m})$，其中$x\in
    U$。然后，形式为
- en: '|  | $\mathcal{F}(D^{k}\bm{u}(x),D^{k-1}\bm{u}(x),...,D\bm{u}(x),\bm{u}(x),x)=0$
    |  | (1) |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{F}(D^{k}\bm{u}(x),D^{k-1}\bm{u}(x),...,D\bm{u}(x),\bm{u}(x),x)=0$
    |  | (1) |'
- en: is called a $k^{\text{-th}}$-order system of partial differential equation (or
    ordinary differential equation when $n=1$), where $\mathcal{F}:\mathbb{R}^{mn^{k}}\times\mathbb{R}^{mn^{k-1}}\times...\times\mathbb{R}^{mn}\times\mathbb{R}^{m}\times
    U\mapsto\mathbb{R}^{m}$.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 的表达式被称为$k^{\text{-th}}$阶偏微分方程系统（当$n=1$时为常微分方程），其中$\mathcal{F}:\mathbb{R}^{mn^{k}}\times\mathbb{R}^{mn^{k-1}}\times...\times\mathbb{R}^{mn}\times\mathbb{R}^{m}\times
    U\mapsto\mathbb{R}^{m}$。
- en: '$\mathcal{F}$ models the dynamics of a $n$-dimensional state $x\in\mathbb{R}^{n}$
    and it can be either a linear or non-linear operator. Since most dynamics evolve
    over time, one of the variables of $u$ is usually the time dimension. In general,
    one must specify appropriate boundary and initial conditions of Equ.[1](#S1.E1
    "In Definition 1\. ‣ 1 Introduction ‣ Physics-Guided Deep Learning for Dynamical
    Systems: A Survey") to ensure the existence of a solution. Learning dynamical
    systems is to search for a model $\mathcal{F}$ that can accurately describe the
    behavior of the physical process insofar as we are interested.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{F}$ 模型描述了 $n$ 维状态 $x\in\mathbb{R}^{n}$ 的动态，它可以是线性或非线性算子。由于大多数动态随时间演变，$u$
    的一个变量通常是时间维度。一般来说，必须指定适当的边界和初始条件以确保方程[1](#S1.E1 "在定义1\. ‣ 1 引言 ‣ 物理指导的深度学习用于动态系统：综述")的解的存在。学习动态系统就是寻找一个模型
    $\mathcal{F}$，它可以准确描述我们感兴趣的物理过程的行为。
- en: 'Physics as a discipline has a long tradition of using first principles to describe
    spatiotemporal dynamics. The laws of physics have greatly improved our understanding
    of the physical world. Many physics laws are described by systems of highly nonlinear
    differential equations that have direct implications for understanding and predicting
    physical dynamics. However, these equations are usually too complicated to be
    solvable. The current paradigm of numerical methods for solution approximation
    is purely physics-based: known physical laws encoded in systems of coupled differential
    equations are solved over space and time via numerical differentiation and integration
    schemes [[66](#bib.bib66), [67](#bib.bib67), [100](#bib.bib100), [70](#bib.bib70),
    [112](#bib.bib112), [131](#bib.bib131)]. However, these methods are tremendously
    computationally intensive, requiring significant computational resources and expertise.
    An alternative is seeking simplified models that are based on certain assumptions
    and roughly can describe the dynamics, such as Reynolds-averaged Navier-stokes
    equations for turbulent flows and Euler equations for gas dynamics [[25](#bib.bib25),
    [88](#bib.bib88), [153](#bib.bib153)]. But it is highly nontrivial to obtain a
    simplified model that can describe a phenomenon with satisfactory accuracy. More
    importantly, for many complex real-world phenomena, only partial knowledge of
    their dynamics is known. The equations may not fully represent the true system
    states.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个学科，物理学有着利用第一性原理描述时空动态的悠久传统。物理定律极大地提高了我们对物理世界的理解。许多物理定律由高度非线性的微分方程系统描述，这些方程直接影响了对物理动态的理解和预测。然而，这些方程通常过于复杂而无法解出。目前的数值方法求解近似的范式纯粹基于物理学：已知的物理定律通过耦合微分方程系统编码，并通过数值微分和积分方案在空间和时间上进行求解[[66](#bib.bib66),
    [67](#bib.bib67), [100](#bib.bib100), [70](#bib.bib70), [112](#bib.bib112), [131](#bib.bib131)]。然而，这些方法计算量巨大，需要大量的计算资源和专业知识。另一种方法是寻求基于某些假设的简化模型，这些模型大致可以描述动态，如湍流流动的雷诺平均纳维-斯托克斯方程和气体动力学的欧拉方程[[25](#bib.bib25),
    [88](#bib.bib88), [153](#bib.bib153)]。但获得能够以令人满意的精度描述现象的简化模型是非常困难的。更重要的是，对于许多复杂的现实世界现象，仅对其动态有部分了解。方程可能无法完全代表真实的系统状态。
- en: Deep Learning (DL) provides efficient alternatives to learn high-dimensional
    spatiotemporal dynamics from massive datasets. It achieves so by directly predicting
    the input-output mapping and bypassing numerical integration. Recent works have
    shown that DL can generate realistic predictions and significantly accelerate
    the simulation of physical dynamics relative to numerical solvers, from turbulence
    modeling to weather prediction [[159](#bib.bib159), [81](#bib.bib81), [77](#bib.bib77),
    [76](#bib.bib76), [79](#bib.bib79)]. This opens up new opportunities at the intersection
    of DL and physical sciences, such as molecular dynamics[[142](#bib.bib142), [144](#bib.bib144)],
    epidemiology[[170](#bib.bib170)], cardiology[[99](#bib.bib99), [158](#bib.bib158)]
    and material science [[101](#bib.bib101), [22](#bib.bib22)].
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）提供了从海量数据集中学习高维时空动态的有效替代方案。它通过直接预测输入-输出映射并绕过数值积分来实现这一点。最近的研究表明，DL可以生成现实的预测，并且相较于数值求解器，显著加速物理动态的模拟，从湍流建模到天气预测[[159](#bib.bib159),
    [81](#bib.bib81), [77](#bib.bib77), [76](#bib.bib76), [79](#bib.bib79)]。这为DL与物理科学的交汇处开辟了新的机会，如分子动力学[[142](#bib.bib142),
    [144](#bib.bib144)]、流行病学[[170](#bib.bib170)]、心脏病学[[99](#bib.bib99), [158](#bib.bib158)]以及材料科学[[101](#bib.bib101),
    [22](#bib.bib22)]。
- en: 'Despite the tremendous progress, DL is purely data-driven by nature, which
    has many limitations. DL models still adhere to the fundamental rules of statistical
    inference. The nonlinear and chaotic nature of real-world dynamics poses significant
    challenges to existing DL frameworks. Without explicit constraints, DL models
    are prone to make physically implausible forecasts, violating the governing laws
    of physical systems. Additionally, DL models often struggle with generalization:
    models trained on one dataset cannot adapt properly to unseen scenarios with different
    distributions, known as distribution shift. For dynamics learning, the distribution
    shift occurs not only because the dynamics are non-stationary and nonlinear, but
    also due to the changes in system parameters, such as external forces and boundary
    conditions. In a word, the current limitation of DL models for learning complex
    dynamics is their lack of ability to understand the system solely from data and
    cope with the distributional shifts that naturally occur.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了巨大进展，DL本质上仍然是数据驱动的，这具有许多局限性。DL模型仍然遵循统计推断的基本规则。真实世界动态的非线性和混沌性质对现有的DL框架提出了重大挑战。没有明确的约束，DL模型容易做出物理上不可信的预测，违反物理系统的基本定律。此外，DL模型往往在泛化方面存在困难：在一个数据集上训练的模型无法有效适应具有不同分布的未见场景，这称为分布转移。对于动态学习，分布转移的发生不仅由于动态是非平稳和非线性的，还由于系统参数的变化，例如外部力量和边界条件。总之，目前DL模型在学习复杂动态方面的限制在于其无法仅通过数据理解系统，并应对自然发生的分布转移。
- en: Neither DL alone nor purely physics-based approaches can be considered sufficient
    for learning complex dynamical systems in scientific domains. Therefore, there
    is a growing need for integrating traditional physics-based approaches with DL
    models so that we can make the best of both types of approaches. There is already
    a vast amount of work about physics-guided DL [[168](#bib.bib168), [46](#bib.bib46),
    [17](#bib.bib17), [86](#bib.bib86), [76](#bib.bib76), [127](#bib.bib127), [18](#bib.bib18)],
    but the focus on deep learning for dynamical systems is still nascent. Physics-guided
    DL offers a set of tools to blend these physical concepts such as differential
    equations and symmetry with deep neural networks. On one hand, these DL models
    offer great computational benefits over traditional numerical solvers. On the
    other hand, the physical constraints impose appropriate inductive biases on the
    DL models, leading to accurate simulation, scientifically valid predictions, reduced
    sample complexity, and guaranteed improvement in generalization to unknown environments.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 仅依赖DL或纯物理方法都不足以学习科学领域中的复杂动态系统。因此，结合传统的物理基础方法与DL模型的需求日益增加，以便我们能够充分利用这两种方法。关于物理引导的DL已经有大量的工作[[168](#bib.bib168),
    [46](#bib.bib46), [17](#bib.bib17), [86](#bib.bib86), [76](#bib.bib76), [127](#bib.bib127),
    [18](#bib.bib18)]，但对动态系统的深度学习的关注仍处于初期阶段。物理引导的DL提供了一套工具，将微分方程和对称性等物理概念与深度神经网络融合在一起。一方面，这些DL模型在计算上相较于传统数值求解器具有显著的优势。另一方面，物理约束对DL模型施加了适当的归纳偏置，从而实现准确的模拟、科学有效的预测、降低样本复杂性，并保证在未知环境中的泛化能力。
- en: This survey paper aims to provide a structured overview of existing methodologies
    of incorporating prior physical knowledge into DL models for learning dynamical
    systems. The paper is organized as below.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查论文旨在提供将先验物理知识融入深度学习（DL）模型以学习动态系统的现有方法的结构化概述。论文的组织结构如下。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [2](#S2 "2 Significance of Physics-Guided Deep Learning ‣ Physics-Guided
    Deep Learning for Dynamical Systems: A Survey") describes the significance of
    physics-guided DL.'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[第2节](#S2 "2 Significance of Physics-Guided Deep Learning ‣ Physics-Guided
    Deep Learning for Dynamical Systems: A Survey") 描述了物理引导DL的意义。'
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [3](#S3 "3 Problem Formulation ‣ Physics-Guided Deep Learning for Dynamical
    Systems: A Survey") formulates the four main learning problems of physics-guided
    DL, including solving differential equations, dynamics forecasting, learning dynamics
    residuals, and equation discovery.'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[第3节](#S3 "3 Problem Formulation ‣ Physics-Guided Deep Learning for Dynamical
    Systems: A Survey") 构造了物理引导的DL的四个主要学习问题，包括求解微分方程、动态预测、学习动态残差和方程发现。'
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [4](#S4 "4 Physics-Guided Loss Functions and Regularization ‣ Physics-Guided
    Deep Learning for Dynamical Systems: A Survey")$\sim$[7](#S7 "7 Invariant and
    Equivariant DL Models ‣ Physics-Guided Deep Learning for Dynamical Systems: A
    Survey") categorizes existing physics-guided DL approaches into four groups based
    on the way how physics and DL are combined. Each lead with a detailed review of
    recent work as a case study and further categorized based on objectives or model
    architecture.'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[4](#S4 "4 Physics-Guided Loss Functions and Regularization ‣ Physics-Guided
    Deep Learning for Dynamical Systems: A Survey")$\sim$[7](#S7 "7 Invariant and
    Equivariant DL Models ‣ Physics-Guided Deep Learning for Dynamical Systems: A
    Survey")节将现有的物理指导深度学习方法根据物理与深度学习的结合方式分为四类。每一类都详细回顾了近期的工作作为案例研究，并根据目标或模型架构进一步分类。'
- en: –
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [4](#S4 "4 Physics-Guided Loss Functions and Regularization ‣ Physics-Guided
    Deep Learning for Dynamical Systems: A Survey"): Physics-guided loss function:
    prior physics knowledge is imposed as additional soft constraints in the loss
    function.'
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[4](#S4 "4 Physics-Guided Loss Functions and Regularization ‣ Physics-Guided
    Deep Learning for Dynamical Systems: A Survey")节：物理指导的损失函数：先验物理知识作为额外的软约束施加在损失函数中。'
- en: –
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [5](#S5 "5 Physics-Guided Design of Architecture ‣ Physics-Guided Deep
    Learning for Dynamical Systems: A Survey"): Physics-guided architecture design:
    prior physics knowledge is strictly incorporated into the design of neural network
    modules.'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[5](#S5 "5 Physics-Guided Design of Architecture ‣ Physics-Guided Deep Learning
    for Dynamical Systems: A Survey")节：物理指导的架构设计：先验物理知识严格融入神经网络模块的设计中。'
- en: –
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [6](#S6 "6 Hybrid Physics-DL Model ‣ Physics-Guided Deep Learning for
    Dynamical Systems: A Survey"): Hybrid physics-DL models: complete physics-based
    approaches are directly combined with DL models.'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[6](#S6 "6 Hybrid Physics-DL Model ‣ Physics-Guided Deep Learning for Dynamical
    Systems: A Survey")节：混合物理-深度学习模型：完整的基于物理的方法直接与深度学习模型结合。'
- en: –
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: 'Section [7](#S7 "7 Invariant and Equivariant DL Models ‣ Physics-Guided Deep
    Learning for Dynamical Systems: A Survey"): Invariant and equivariant DL models:
    DL models are designed to respect the symmetries of a given physical system.'
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[7](#S7 "7 Invariant and Equivariant DL Models ‣ Physics-Guided Deep Learning
    for Dynamical Systems: A Survey")节：不变性和协变性深度学习模型：深度学习模型被设计为尊重给定物理系统的对称性。'
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Section [8](#S8 "8 Discussion ‣ Physics-Guided Deep Learning for Dynamical
    Systems: A Survey") summarizes the challenges in this field and discusses the
    emerging opportunities for future research.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '第[8](#S8 "8 Discussion ‣ Physics-Guided Deep Learning for Dynamical Systems:
    A Survey")节总结了该领域的挑战，并讨论了未来研究的新机会。'
- en: 2 Significance of Physics-Guided Deep Learning
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 物理指导深度学习的重要性
- en: This subsection provides an overview of the motivations and significance of
    physics-guided DL for learning dynamical systems. By incorporating physical principles,
    governing equations, mathematical modeling, and domain knowledge into DL models,
    the rapidly growing field of physics-guided DL can potentially (1) accelerate
    data simulation (2) build physically scientifically valid models (3) improve the
    generalizability of DL models (4) discover governing equations.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节概述了物理指导深度学习在学习动态系统中的动机和意义。通过将物理原理、控制方程、数学建模和领域知识融入深度学习模型，迅速发展的物理指导深度学习领域有可能（1）加速数据模拟（2）构建物理上科学有效的模型（3）提高深度学习模型的泛化能力（4）发现控制方程。
- en: 2.1 Accelerate Data Simulation.
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 加速数据模拟。
- en: 'Simulation is an important method of analyzing, optimizing, and designing real-world
    processes, which are easily verified, communicated, and understood. It serves
    as the surrogate modeling and digital twin and provides valuable insights into
    complex physical systems. Traditional physics-based simulations often rely on
    running numerical methods: known physical laws encoded in systems of coupled differential
    equations are solved over space and time via numerical differentiation and integration
    schemes [[66](#bib.bib66), [100](#bib.bib100), [70](#bib.bib70), [112](#bib.bib112),
    [131](#bib.bib131)]. Although the governing equations of many physical systems
    are known, finding approximate solutions using numerical algorithms and computers
    is still prohibitively expensive. Because the discretization step size is usually
    confined to be very small due to stability constraints when the dynamics are complex.
    Moreover, the performance of numerical methods can highly depend on the initial
    guesses of unknown parameters [[68](#bib.bib68)]. Recently, DL has demonstrated
    great success in the automation, acceleration, and streamlining of highly compute-intensive
    workflows for science [[127](#bib.bib127), [153](#bib.bib153), [81](#bib.bib81)].'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟是分析、优化和设计现实世界过程的重要方法，这些过程易于验证、沟通和理解。它作为替代建模和数字双胞胎，并为复杂物理系统提供有价值的见解。传统的基于物理的模拟通常依赖于运行数值方法：已知的物理定律通过数值微分和积分方案在空间和时间上求解耦合微分方程[[66](#bib.bib66),
    [100](#bib.bib100), [70](#bib.bib70), [112](#bib.bib112), [131](#bib.bib131)]。虽然许多物理系统的控制方程是已知的，但使用数值算法和计算机找到近似解仍然非常昂贵。这是因为当动态复杂时，离散化步长通常需要非常小，以满足稳定性约束。此外，数值方法的性能可能高度依赖于未知参数的初始猜测[[68](#bib.bib68)]。最近，深度学习在科学的自动化、加速和精简计算密集型工作流方面展示了巨大的成功[[127](#bib.bib127),
    [153](#bib.bib153), [81](#bib.bib81)]。
- en: Deep dynamics models can directly approximate high-dimensional spatiotemporal
    dynamics by directly forecasting the future states and bypassing numerical integration
    [[161](#bib.bib161), [38](#bib.bib38), [135](#bib.bib135), [119](#bib.bib119),
    [135](#bib.bib135), [162](#bib.bib162), [117](#bib.bib117), [96](#bib.bib96)].
    These models are trained to make forward predictions given the historic frames
    as input with one or more steps of supervision and can roll out up to hundreds
    of steps during inference. DL models are usually faster than classic numerical
    solvers by orders of magnitude since DL is able to take much larger space or time
    steps than classical solvers [[119](#bib.bib119)].
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 深度动态模型可以通过直接预测未来状态并绕过数值积分，直接逼近高维时空动态[[161](#bib.bib161), [38](#bib.bib38), [135](#bib.bib135),
    [119](#bib.bib119), [135](#bib.bib135), [162](#bib.bib162), [117](#bib.bib117),
    [96](#bib.bib96)]。这些模型通过将历史帧作为输入进行训练，进行前向预测，并且在推理过程中可以展开数百步。由于深度学习能够处理比经典求解器更大的空间或时间步长，因此通常比经典数值求解器快几个数量级[[119](#bib.bib119)]。
- en: Another common approach is that deep neural networks can directly approximate
    the solution of complex coupled differential equations via gradient-based optimization,
    which is the so-called physics-informed neural networks (PINNs). This approach
    has shown success in approximating a variety of PDEs [[124](#bib.bib124), [123](#bib.bib123),
    [24](#bib.bib24), [60](#bib.bib60)]. Additionally, deep generative models, such
    as diffusion models and score-based generative models, have been shown effective
    in accurate molecule graph generation [[57](#bib.bib57), [65](#bib.bib65)]. The
    computer graphics community has also investigated using DL to speed up numerical
    simulations for generating realistic animations of fluids such as water and smoke
    [[77](#bib.bib77), [153](#bib.bib153), [167](#bib.bib167), [172](#bib.bib172)].
    However, the community focuses more on the visual realism of the simulation rather
    than the physical characteristics.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的方法是深度神经网络可以通过基于梯度的优化直接逼近复杂耦合微分方程的解，这就是所谓的物理信息神经网络（PINNs）。这种方法在逼近各种 PDE（偏微分方程）方面已显示出成功[[124](#bib.bib124),
    [123](#bib.bib123), [24](#bib.bib24), [60](#bib.bib60)]。此外，深度生成模型，如扩散模型和基于评分的生成模型，已被证明在准确的分子图生成方面有效[[57](#bib.bib57),
    [65](#bib.bib65)]。计算机图形学界也研究了利用深度学习加速数值模拟，以生成逼真的流体动画，如水和烟雾[[77](#bib.bib77), [153](#bib.bib153),
    [167](#bib.bib167), [172](#bib.bib172)]。然而，该领域更注重模拟的视觉真实感而非物理特性。
- en: 2.2 Build Scientifically Valid Models.
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 构建科学有效的模型。
- en: Despite the tremendous progress of DL for science, e.g., atmospheric science
    [[127](#bib.bib127)], computational biology [[1](#bib.bib1)], material science
    [[22](#bib.bib22)], quantum chemistry [[141](#bib.bib141)], it remains a grand
    challenge to incorporate physical principles in a systematic manner to the design,
    training, and inference of such models. DL models are essentially statistical
    models that learn patterns from the data they are trained on. Without explicit
    constraints, DL models, when trained solely on data, are prone to make scientifically
    implausible predictions, violating the governing laws of physical systems. In
    many scientific applications, it is important that the predictions made by DL
    models are consistent with the known physical laws and constraints. For example,
    in fluid dynamics, a model that predicts the velocity field of a fluid must satisfy
    the conservation of mass and momentum. In materials science, a model that predicts
    the properties of a material must obey the laws of thermodynamics and the principles
    of quantum mechanics.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在科学领域取得了巨大进展，例如，大气科学[[127](#bib.bib127)]、计算生物学[[1](#bib.bib1)]、材料科学[[22](#bib.bib22)]、量子化学[[141](#bib.bib141)]，但在系统地将物理原则融入这些模型的设计、训练和推断过程中仍然是一个重大挑战。DL模型本质上是统计模型，从其训练数据中学习模式。没有显式约束的情况下，当DL模型仅依靠数据进行训练时，容易做出科学上不合理的预测，违背物理系统的基本规律。在许多科学应用中，DL模型的预测结果需要与已知的物理定律和约束一致。例如，在流体动力学中，预测流体速度场的模型必须满足质量和动量守恒。在材料科学中，预测材料性质的模型必须遵循热力学定律和量子力学原理。
- en: Thus, to build trustworthy predictive models for science and engineering, we
    need to leverage known physical principles to guide DL models to learn the correct
    underlying dynamics instead of simply fitting the observed data. For instance,
    [[75](#bib.bib75), [71](#bib.bib71), [171](#bib.bib171), [10](#bib.bib10), [58](#bib.bib58),
    [33](#bib.bib33)] improve the physical and statistical consistency of DL models
    by explicitly regularising the loss function with physical constraints. Hybrid
    DL models, e.g., [[104](#bib.bib104), [4](#bib.bib4), [26](#bib.bib26)] integrate
    differential equations in DL for temporal dynamics forecasting and achieve promising
    performance. [[98](#bib.bib98)] and [[49](#bib.bib49)] studied tensor invariant
    neural networks that can learn the Reynolds stress tensor while preserving Galilean
    invariance. [[159](#bib.bib159)] presented a hybrid model that combines the numerical
    RANS-LES coupling method with a custom-designed U-net. The model uses the temporal
    and spatial filters in the RANS-LES coupling method to guide the U-net in learning
    both large and small eddies. This approach improves the both accuracy and physical
    consistency of the model, making it more effective at representing the complex
    flow phenomena observed in many fluid dynamics applications.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了建立值得信赖的科学和工程预测模型，我们需要利用已知的物理原则来引导深度学习（DL）模型学习正确的基本动态，而不仅仅是拟合观察到的数据。例如，[[75](#bib.bib75)、[71](#bib.bib71)、[171](#bib.bib171)、[10](#bib.bib10)、[58](#bib.bib58)、[33](#bib.bib33)]
    通过显式地使用物理约束来正则化损失函数，从而提高了DL模型的物理和统计一致性。混合DL模型，例如，[[104](#bib.bib104)、[4](#bib.bib4)、[26](#bib.bib26)]
    将微分方程集成到DL中以进行时间动态预测，并取得了良好的性能。[[98](#bib.bib98)] 和 [[49](#bib.bib49)] 研究了张量不变神经网络，这些网络能够在保持伽利略不变性的同时学习雷诺应力张量。[[159](#bib.bib159)]
    提出了一个混合模型，该模型将数值RANS-LES耦合方法与定制设计的U-net结合。该模型利用RANS-LES耦合方法中的时间和空间滤波器来引导U-net学习大尺度和小尺度的涡流。这种方法提高了模型的准确性和物理一致性，使其在许多流体动力学应用中更有效地表示复杂的流动现象。
- en: 2.3 Improve the generalizability of DL models
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 提高DL模型的泛化能力
- en: 'DL models often struggle with generalization: models trained on one dataset
    cannot adapt properly to unseen scenarios with distributional shifts that may
    naturally occur in dynamical systems [[84](#bib.bib84), [115](#bib.bib115), [103](#bib.bib103),
    [2](#bib.bib2), [160](#bib.bib160)]. Because they learn to represent the statistical
    patterns in the training data, rather than the underlying causal relationships.
    In addition, most current approaches are still trained to model a specific system
    and multiple systems with close distributions, making it challenging to meet the
    needs of the scientific domain with heterogeneous environments. Thus, it is imperative
    to develop generalizable DL models that can learn and generalize well across systems
    with various parameter domains.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型经常面临泛化问题：在一个数据集上训练的模型无法适应在动态系统中可能自然发生的分布偏移的未知场景[[84](#bib.bib84), [115](#bib.bib115),
    [103](#bib.bib103), [2](#bib.bib2), [160](#bib.bib160)]。这是因为它们学习的是训练数据中的统计模式，而不是潜在的因果关系。此外，大多数当前的方法仍然被训练来建模特定系统和具有相近分布的多个系统，这使得满足异质环境科学领域的需求变得具有挑战性。因此，开发能够在不同参数域的系统之间学习和泛化良好的深度学习模型是至关重要的。
- en: Prior physical knowledge can be considered as an inductive bias that can place
    a prior distribution on the model class and shrink the model parameter search
    space. With the guide of inductive bias, DL models can better capture the underlying
    dynamics from the data that are consistent with physical laws. Across different
    data domains and systems, the laws of physics stay constant. Hence, integrating
    physical laws in DL enables the models to generalize outside of the training domain
    and even to different systems.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 先验物理知识可以视为一种归纳偏置，它可以在模型类别上放置先验分布并缩小模型参数的搜索空间。在归纳偏置的指导下，深度学习模型能够更好地捕捉与物理法则一致的底层动态。在不同的数据域和系统中，物理法则保持不变。因此，将物理法则集成到深度学习中可以使模型在训练域之外甚至在不同系统中进行泛化。
- en: 'Embedding symmetries into DL models is one way to improve the generalization,
    which we will discuss in detail in subsection [7](#S7 "7 Invariant and Equivariant
    DL Models ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey"). For
    example, [[161](#bib.bib161)] designed deep equivariant dynamics models that respect
    the rotation, scaling, and uniform motion symmetries in fluid dynamics. The models
    are both theoretically and experimentally robust to distributional shifts by symmetry
    group transformations and enjoy favorable sample complexity compared with data
    augmentation. There are many other ways to improve the generalization of DL models
    by incorporating other physical knowledge. [[162](#bib.bib162)] proposed a meta-learning
    framework to forecast systems with different parameters. It leverages prior physics
    knowledge to distinguish different systems. Specifically, it uses an encoder to
    infer the physical parameters of different systems and a prediction network to
    adapt and forecast giving the inferred system. Moreover, [[48](#bib.bib48)] encodes
    Lyapunov stability into an autoencoder model for predicting fluid flow and sea
    surface temperature. They show improved generalizability and reduced prediction
    uncertainty for neural nets that preserve Lyapunov stability. [[143](#bib.bib143)]
    shows adding spectral normalization to DNN to regularize its Lipschitz continuity
    can greatly improve the generalization to new input domains on the task of drone
    landing control.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '将对称性嵌入深度学习模型中是一种提高泛化能力的方法，我们将在子节[7](#S7 "7 Invariant and Equivariant DL Models
    ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey")中详细讨论。例如，[[161](#bib.bib161)]设计了尊重流体动力学中的旋转、缩放和均匀运动对称性的深度等变动态模型。这些模型在通过对称群变换应对分布偏移时，在理论和实验上都表现出强大的鲁棒性，并且与数据增强相比，样本复杂性较低。还有许多其他方法可以通过结合其他物理知识来提高深度学习模型的泛化能力。[[162](#bib.bib162)]提出了一个元学习框架，用于预测具有不同参数的系统。它利用先验物理知识来区分不同的系统。具体来说，它使用编码器来推断不同系统的物理参数，并使用预测网络来适应和预测推断出的系统。此外，[[48](#bib.bib48)]将Lyapunov稳定性编码到自编码器模型中，以预测流体流动和海面温度。他们展示了在保留Lyapunov稳定性的神经网络中，泛化能力的提高和预测不确定性的减少。[[143](#bib.bib143)]显示，将谱归一化添加到DNN中以正则化其Lipschitz连续性，可以显著提高无人机着陆控制任务对新输入域的泛化能力。'
- en: 2.4 Discover Governing Equations
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 发现主导方程
- en: One of the main objectives of science is to discover fundamental laws that can
    solve practical problems [[46](#bib.bib46), [44](#bib.bib44)]. The discovery of
    governing equations is crucial as it enables us to comprehend the underlying physical
    laws that regulate complex systems. By identifying the mathematical models that
    describe the behavior of a system, we can make accurate predictions and gain insights
    into how the system will behave under different conditions. This knowledge can
    be applied to optimize the performance of engineering systems, improve the precision
    of weather forecasts, and understand the mechanisms behind biological processes,
    among other applications [[45](#bib.bib45), [15](#bib.bib15)]. However, discovering
    governing equations is a challenging task for various reasons. Firstly, real-world
    systems are frequently complex and involve many interdependent variables, making
    it difficult to identify the relevant variables and their relationships. Secondly,
    many systems are nonlinear and involve interactions between variables that are
    hard to model using linear equations. Thirdly, the available data may be noisy
    or incomplete, making it challenging to extract meaningful patterns and relationships.
    Despite these challenges, recent advances in machine learning have made it possible
    to automate the process of governing equations discovery and identify complex,
    nonlinear models from data. These approaches may lead to new discoveries and insights
    into the behavior of complex systems for a wide range of applications.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 科学的主要目标之一是发现能够解决实际问题的基本规律[[46](#bib.bib46), [44](#bib.bib44)]。发现支配方程至关重要，因为它使我们能够理解调节复杂系统的基本物理规律。通过识别描述系统行为的数学模型，我们可以做出准确的预测，并洞察系统在不同条件下的表现。这些知识可以应用于优化工程系统的性能、提高天气预报的精度，以及理解生物过程背后的机制等多个领域[[45](#bib.bib45),
    [15](#bib.bib15)]。然而，发现支配方程是一项具有挑战性的任务，原因有多方面。首先，现实世界中的系统通常复杂，并涉及许多相互依赖的变量，使得识别相关变量及其关系变得困难。其次，许多系统是非线性的，涉及变量之间的交互，这些交互难以用线性方程建模。第三，现有数据可能存在噪声或不完整，使得提取有意义的模式和关系具有挑战性。尽管面临这些挑战，近年来机器学习的进步使得自动化支配方程发现的过程成为可能，并从数据中识别复杂的非线性模型。这些方法可能会导致新的发现，并为广泛应用的复杂系统行为提供深入的洞察。
- en: Discovering governing equations from data is often accomplished by defining
    a large set of possible mathematical basis functions and learning the coefficients.
    [[19](#bib.bib19), [20](#bib.bib20), [73](#bib.bib73), [15](#bib.bib15), [138](#bib.bib138)]
    proposed to find ordinary differential equations by creating a dictionary of possible
    basis functions and discovering sparse, low-dimensional, and nonlinear models
    from data using the sparse identification. More recent work, such as [[89](#bib.bib89),
    [128](#bib.bib128)], incorporated neural networks to further augment the dictionary
    to model more complex dynamics. [[23](#bib.bib23)] contributed to this trend by
    introducing an efficient first-order conditional gradient algorithm for solving
    the optimization problem of finding the best sparse fit to observational data
    in a large library of potential nonlinear models. Alternatively, [[110](#bib.bib110),
    [132](#bib.bib132)] presented a shallow neural network approach, EQL to identify
    concise equations from data. They replaced the activation functions with predefined
    basis functions, including identity and trigonometry functions, and used specially
    designed division units to model division relationships in the potential governing
    equations. Similarly, [[105](#bib.bib105), [106](#bib.bib106)] designed PDE-Nets
    that use convolution to approximate differential operators and symbolic neural
    networks to approximate and recover multivariate functions. These models could
    learn various functional relations, with and without divisions, from noisy data
    in a confined domain. However, scalability, overfitting, and over-reliance on
    high-quality measurement data remain critical concerns in this research area [[126](#bib.bib126)].
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中发现控制方程通常通过定义一大组可能的数学基函数并学习系数来实现。[[19](#bib.bib19), [20](#bib.bib20), [73](#bib.bib73),
    [15](#bib.bib15), [138](#bib.bib138)] 提出了通过创建可能的基函数字典并使用稀疏识别从数据中发现稀疏、低维和非线性模型来寻找常微分方程。更近期的工作，如
    [[89](#bib.bib89), [128](#bib.bib128)]，结合了神经网络以进一步扩展字典，以模拟更复杂的动态系统。[[23](#bib.bib23)]
    通过引入一种高效的一阶条件梯度算法，解决了在大量潜在非线性模型的库中找到最佳稀疏拟合观测数据的优化问题，从而推动了这一趋势。另一种方法，[[110](#bib.bib110),
    [132](#bib.bib132)]，提出了一种浅层神经网络方法EQL，从数据中识别简洁方程。他们用预定义的基函数（包括恒等和三角函数）替换了激活函数，并使用专门设计的除法单元来模拟潜在控制方程中的除法关系。类似地，[[105](#bib.bib105),
    [106](#bib.bib106)] 设计了使用卷积来逼近微分算子的 PDE-Nets 和使用符号神经网络来逼近和恢复多变量函数的模型。这些模型可以从有噪声的数据中学习各种功能关系，包括和不包括除法。然而，可扩展性、过拟合和对高质量测量数据的过度依赖仍然是这一研究领域的关键问题
    [[126](#bib.bib126)]。
- en: 3 Problem Formulation
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 问题定义
- en: In light of the motivation and significance of physics-guided deep learning
    we discuss in the previous section, the primary research efforts in this field
    have been aimed at tackling the following four fundamental problems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前一节中讨论的物理指导深度学习的动机和意义，本领域的主要研究工作旨在解决以下四个基本问题。
- en: 3.1 Solving Differential Equations
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 解常微分方程
- en: 'When $\mathcal{F}$ in Eq. [1](#S1.E1 "In Definition 1\. ‣ 1 Introduction ‣
    Physics-Guided Deep Learning for Dynamical Systems: A Survey") is known but Eq.
    [1](#S1.E1 "In Definition 1\. ‣ 1 Introduction ‣ Physics-Guided Deep Learning
    for Dynamical Systems: A Survey") is too complicated to be solvable, researchers
    tend to directly solve the differential Eq.ations by approximating solution of
    $\bm{u}(x)$ with a deep neural network, and enforcing the governing equations
    as a soft constraint on the output of the neural nets during training at the same
    time[[125](#bib.bib125), [124](#bib.bib124), [85](#bib.bib85)]. This approach
    can be formulated as the following optimization problem,'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '当方程 [1](#S1.E1 "In Definition 1\. ‣ 1 Introduction ‣ Physics-Guided Deep Learning
    for Dynamical Systems: A Survey") 中的 $\mathcal{F}$ 已知但方程 [1](#S1.E1 "In Definition
    1\. ‣ 1 Introduction ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey")
    过于复杂而无法求解时，研究人员通常通过用深度神经网络近似 $\bm{u}(x)$ 的解，直接解决微分方程，同时在训练期间将控制方程作为对神经网络输出的软约束来执行[[125](#bib.bib125),
    [124](#bib.bib124), [85](#bib.bib85)]。这种方法可以表述为以下优化问题，'
- en: '|  | $\text{min}_{\theta}\;\mathcal{L}(\bm{u})+\lambda_{\mathcal{F}}\mathcal{L}_{\mathcal{F}}(\bm{u})$
    |  | (2) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\text{min}_{\theta}\;\mathcal{L}(\bm{u})+\lambda_{\mathcal{F}}\mathcal{L}_{\mathcal{F}}(\bm{u})$
    |  | (2) |'
- en: '$\mathcal{L}(\bm{u})$ denotes the misfit of neural net predictions and the
    training data points. $\theta$ denotes the neural net parameters. $\mathcal{L}_{\mathcal{F}}(\bm{u})$
    is a constraint on the residual of the differential equation system under consideration
    and $\lambda_{\mathcal{F}}$ is a regularization parameter that controls the emphasis
    on this residual. The goal is then to train the neural nets to minimize the loss
    function in Eq. [2](#S3.E2 "In 3.1 Solving Differential Equations ‣ 3 Problem
    Formulation ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{L}(\bm{u})$ 表示神经网络预测与训练数据点之间的误差。$\theta$ 表示神经网络参数。$\mathcal{L}_{\mathcal{F}}(\bm{u})$
    是对所考虑的微分方程系统残差的约束，而 $\lambda_{\mathcal{F}}$ 是一个正则化参数，用于控制对该残差的重视程度。目标是训练神经网络以最小化公式
    [2](#S3.E2 "在 3.1 解微分方程 ‣ 3 问题定义 ‣ 面向物理的深度学习动态系统：综述") 中的损失函数。
- en: 3.2 Learning Dynamics Residuals
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 学习动态残差
- en: 'When $\mathcal{F}$ in Eq. [1](#S1.E1 "In Definition 1\. ‣ 1 Introduction ‣
    Physics-Guided Deep Learning for Dynamical Systems: A Survey") is partially know,
    we can use neural nets to learn the errors or residuals made by physics-based
    models [[37](#bib.bib37), [176](#bib.bib176), [74](#bib.bib74)]. The key is to
    learn the bias of physics-based models and correct it with the help of deep learning.
    The final prediction of the state is composed of the simulation from the physics-based
    models and the residual prediction from neural nets as below,'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当公式 [1](#S1.E1 "在定义 1\. ‣ 1 引言 ‣ 面向物理的深度学习动态系统：综述") 中的 $\mathcal{F}$ 部分已知时，我们可以使用神经网络来学习物理模型的误差或残差
    [[37](#bib.bib37), [176](#bib.bib176), [74](#bib.bib74)]。关键是学习物理模型的偏差，并借助深度学习对其进行修正。状态的最终预测由物理模型的模拟和神经网络的残差预测组成，如下所示，
- en: '|  | $\hat{\bm{u}}=\hat{\bm{u}}_{\mathcal{F}}+\hat{\bm{u}}_{\text{NN}}.$ |  |
    (3) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{\bm{u}}=\hat{\bm{u}}_{\mathcal{F}}+\hat{\bm{u}}_{\text{NN}}.$ |  |
    (3) |'
- en: where $\hat{u}_{\mathcal{F}}$ is the prediction obtained by numerically solving
    $\mathcal{F}$, $\hat{u}_{\text{NN}}$is the prediction from neural networks and
    $\hat{u}$ is the final prediction made by hybrid physics-DL models.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\hat{u}_{\mathcal{F}}$ 是通过数值解法得到的预测值，$\hat{u}_{\text{NN}}$ 是神经网络的预测值，而 $\hat{u}$
    是混合物理-DL 模型做出的最终预测。
- en: 'This learning problem generally involves two training strategies: 1) joint
    training: optimizing the parameters in the differential equations and the neural
    networks at the same time by minimizing the prediction errors of the system states.
    2) two-stage training: we first fit differential equations on the training data
    and obtain the residuals, then directly optimize the neural nets on predicting
    the residuals.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这个学习问题通常涉及两种训练策略：1) 联合训练：通过最小化系统状态的预测误差来同时优化微分方程和神经网络中的参数。2) 两阶段训练：我们首先在训练数据上拟合微分方程并获得残差，然后直接优化神经网络以预测这些残差。
- en: 3.3 Dynamics Forecasting
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 动态预测
- en: 'When $\mathcal{F}$ in Eq. [1](#S1.E1 "In Definition 1\. ‣ 1 Introduction ‣
    Physics-Guided Deep Learning for Dynamical Systems: A Survey") is unknown or numerically
    solving Eq. [1](#S1.E1 "In Definition 1\. ‣ 1 Introduction ‣ Physics-Guided Deep
    Learning for Dynamical Systems: A Survey") requires too much computation, many
    works studied learning high-dimensional spatiotemporal dynamics by directly predicting
    the input-output system state mapping and bypassing numerical discretization and
    integration [[38](#bib.bib38), [76](#bib.bib76), [142](#bib.bib142), [161](#bib.bib161)].
    If we assume the first dimension $x_{1}$ of $\bm{u}$ in Eq. [1](#S1.E1 "In Definition
    1\. ‣ 1 Introduction ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey")
    is the time dimension $t$, then the problem of dynamics forecasting can be defined
    as learning a map $f:\mathbb{R}^{n\times k}\mapsto\mathbb{R}^{n\times q}$ that
    maps a sequence of historic states to future states of the dynamical system,'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当公式 [1](#S1.E1 "在定义 1\. ‣ 1 引言 ‣ 面向物理的深度学习动态系统：综述") 中的 $\mathcal{F}$ 未知或数值解法需要过多计算时，许多研究通过直接预测输入-输出系统状态映射并绕过数值离散化和积分来研究高维时空动态
    [[38](#bib.bib38), [76](#bib.bib76), [142](#bib.bib142), [161](#bib.bib161)]。如果我们假设公式
    [1](#S1.E1 "在定义 1\. ‣ 1 引言 ‣ 面向物理的深度学习动态系统：综述") 中的 $\bm{u}$ 的第一个维度 $x_{1}$ 是时间维度
    $t$，那么动态预测问题可以定义为学习一个映射 $f:\mathbb{R}^{n\times k}\mapsto\mathbb{R}^{n\times q}$，该映射将一系列历史状态映射到动态系统的未来状态，
- en: '|  | $f(\bm{u}\text{\footnotesize$(t-k+1,\cdot)$},...,\bm{u}\text{\footnotesize$(t,\cdot)$})=\bm{u}\text{\footnotesize$(t+1,\cdot)$},...,\bm{u}\text{\footnotesize$(t+q,\cdot)$}$
    |  | (4) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(\bm{u}\text{\footnotesize$(t-k+1,\cdot)$},...,\bm{u}\text{\footnotesize$(t,\cdot)$})=\bm{u}\text{\footnotesize$(t+1,\cdot)$},...,\bm{u}\text{\footnotesize$(t+q,\cdot)$}$
    |  | (4) |'
- en: where $k$ is the input length and $q$ is the output length. $f$ is commonly
    approximated with purely data-driven or physics-guided neural nets and the neural
    nets are optimized by minimizing the prediction errors of the state $\mathcal{L}(\bm{u})$.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $k$ 是输入长度，$q$ 是输出长度。$f$ 通常通过纯数据驱动或物理指导的神经网络来逼近，神经网络通过最小化状态 $\mathcal{L}(\bm{u})$
    的预测误差来进行优化。
- en: 3.4 Search for Governing Equations
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 查找控制方程
- en: 'When $\mathcal{F}$ in Eq. [1](#S1.E1 "In Definition 1\. ‣ 1 Introduction ‣
    Physics-Guided Deep Learning for Dynamical Systems: A Survey") is unknown and
    it is necessary to determine the precise governing equations to solve practical
    problems, numerous efforts have been made to discover the exact mathematical formulation
    of $\mathcal{F}$. The most common approach is to select from a wide range of possible
    candidate functions and choose the model that minimizes fitting errors on observation
    data.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当方程 $\mathcal{F}$ 在公式[1](#S1.E1 "在定义 1\. ‣ 1 引言 ‣ 基于物理的深度学习用于动态系统的综述") 中未知，并且需要确定精确的控制方程以解决实际问题时，已经付出了大量努力以发现
    $\mathcal{F}$ 的确切数学形式。最常见的方法是从广泛的候选函数中选择，并选择在观测数据上最小化拟合误差的模型。
- en: 'More specifically, based on Definition [1](#Thmdfn1 "Definition 1\. ‣ 1 Introduction
    ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey"), the goal of
    discovering governing equations is to find an approximate function $\mathcal{\hat{F}}=\Phi(\bm{u}(x),x)\bm{\theta}\approx\mathcal{F}$,
    where $\Phi(\bm{u}(x),x)=[\phi_{1}(\bm{u}(x),x),\phi_{2}(\bm{u},x),\ldots,\phi_{p}(\bm{u},x)]$
    is a library of candidate functions, such as polynomials and trigonometric functions,
    and $\bm{\theta}\in\mathbb{R}^{p}$ is a sparse vector indicating which candidate
    functions are active in the dynamics. This problem can be formulated as an optimization
    problem, where we aim to minimize the following cost function over a set of observed
    data $\{\bm{y}_{i}\}_{i=1}^{n}$ of $\bm{u}$:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，根据定义[1](#Thmdfn1 "定义 1\. ‣ 1 引言 ‣ 基于物理的深度学习用于动态系统的综述")，发现控制方程的目标是找到一个近似函数
    $\mathcal{\hat{F}}=\Phi(\bm{u}(x),x)\bm{\theta}\approx\mathcal{F}$，其中 $\Phi(\bm{u}(x),x)=[\phi_{1}(\bm{u}(x),x),\phi_{2}(\bm{u},x),\ldots,\phi_{p}(\bm{u},x)]$
    是候选函数的库，例如多项式和三角函数，而 $\bm{\theta}\in\mathbb{R}^{p}$ 是一个稀疏向量，指示哪些候选函数在动态中是活跃的。这个问题可以被表述为一个优化问题，我们的目标是最小化以下成本函数，数据集为
    $\{\bm{y}_{i}\}_{i=1}^{n}$ 的观测数据：
- en: '|  | $\mathcal{L}(\bm{\theta})=\sum_{i=1}^{n}&#124;&#124;\Phi(\bm{y}_{i},x)\bm{\theta}&#124;&#124;^{2}$
    |  | (5) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}(\bm{\theta})=\sum_{i=1}^{n}&#124;&#124;\Phi(\bm{y}_{i},x)\bm{\theta}&#124;&#124;^{2}$
    |  | (5) |'
- en: 4 Physics-Guided Loss Functions and Regularization
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 物理指导的损失函数和正则化
- en: Complex physical dynamics occur over a wide range of spatial and temporal scales.
    Standard DL models may simply fit the observed data while failing to learn the
    correct underlying dynamics, thus leading to low physical consistency and poor
    generalizability. One of the simplest and most widely used approaches to incorporate
    physical constraints is via designing loss functions (regularization). Physics-guided
    loss functions (regularization) can assist DL models to capture correct and generalizable
    dynamic patterns that are consistent with physical laws. Furthermore, the loss
    functions constrained by physics laws can reduce the possible search space of
    parameters. This approach is sometimes referred to as imposing differentiable
    “soft” constraints, which will be contrasted with imposing “hard” constraints
    (physics-guided architecture) in the next section. In this chapter, we will start
    with a case study of physics-guided loss functions, and then categorize these
    types of methods based on their objectives, including solving differential equations,
    improving prediction, and accelerating data generation.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的物理动态在广泛的空间和时间尺度上发生。标准的深度学习（DL）模型可能仅仅拟合观测数据，而未能学习到正确的基本动态，从而导致物理一致性差和泛化能力差。最简单且最广泛使用的物理约束方法之一是通过设计损失函数（正则化）。物理引导的损失函数（正则化）可以帮助深度学习模型捕捉与物理定律一致的正确和可泛化的动态模式。此外，由物理定律约束的损失函数可以减少参数的可能搜索空间。这种方法有时被称为施加可微分的“软”约束，接下来的章节将与施加“硬”约束（物理引导的架构）进行对比。在本章中，我们将以物理引导损失函数的案例研究开始，然后根据其目标对这些方法进行分类，包括求解微分方程、改进预测和加速数据生成。
- en: '4.1 Case Study: Physics-informed Neural Networks'
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 案例研究：物理引导神经网络
- en: 'The Physics-informed Neural Networks (PINNs) approach [[123](#bib.bib123),
    [125](#bib.bib125), [34](#bib.bib34), [124](#bib.bib124), [21](#bib.bib21)] is
    a prime example of incorporating physics knowledge into the design of loss functions.
    PINNs have shown efficiency and accuracy in learning simple differential equations.
    Using fully connected neural networks, PINNs directly approximate the solution
    of differential equations with space coordinates and time stamps as inputs. These
    networks are trained by minimizing both the loss on measurements and the residual
    function error through the partial differential equation. More specifically, based
    on the Def. [1](#Thmdfn1 "Definition 1\. ‣ 1 Introduction ‣ Physics-Guided Deep
    Learning for Dynamical Systems: A Survey"), a fully connected neural network is
    employed to model solution $\bm{\hat{u}}(x,t|\bm{\theta}_{\text{PINN}})$, where
    $\bm{\theta}_{\text{PINN}}$ denotes the weights of the PINN and be optimized by
    minimizing the following loss function.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 物理引导神经网络（PINNs）方法 [[123](#bib.bib123), [125](#bib.bib125), [34](#bib.bib34),
    [124](#bib.bib124), [21](#bib.bib21)] 是将物理知识融入损失函数设计的典型例子。PINNs 在学习简单微分方程方面表现出了高效性和准确性。使用全连接神经网络，PINNs
    直接通过空间坐标和时间戳作为输入来近似微分方程的解。这些网络通过最小化测量损失和通过偏微分方程的残差函数误差来训练。更具体地说，根据 Def. [1](#Thmdfn1
    "定义 1\. ‣ 1 引言 ‣ 物理引导深度学习在动态系统中的应用：综述")，使用全连接神经网络来建模解 $\bm{\hat{u}}(x,t|\bm{\theta}_{\text{PINN}})$，其中
    $\bm{\theta}_{\text{PINN}}$ 表示 PINN 的权重，并通过最小化以下损失函数来优化。
- en: '|  | $\mathcal{L}_{\text{PINN}}=\mathcal{L}(\bm{u})+\lambda_{\mathcal{F}}\mathcal{L}_{\mathcal{F}}(\bm{u})$
    |  | (6) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{PINN}}=\mathcal{L}(\bm{u})+\lambda_{\mathcal{F}}\mathcal{L}_{\mathcal{F}}(\bm{u})$
    |  | (6) |'
- en: $\mathcal{L}(\bm{u})=\|\bm{\hat{u}}-\bm{y}\|_{\Gamma}$ is the error between
    the $\bm{\hat{u}}(x,t|\bm{\theta}_{\text{PINN}})$ and the set of boundary conditions
    and measured data on the set of points $\Gamma$ where the boundary conditions
    and data are defined. $\mathcal{L}_{\mathcal{F}}=\|\mathcal{F}(\bm{\hat{u}}(x,t|\bm{\theta}_{\text{PINN}}),x,t)\|_{\Gamma}$
    is the mean-squared error of the residual function to enforce the predictions
    generated by PINNs satisfy the desired differential equations.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{L}(\bm{u})=\|\bm{\hat{u}}-\bm{y}\|_{\Gamma}$ 是 $\bm{\hat{u}}(x,t|\bm{\theta}_{\text{PINN}})$
    与边界条件集合和点集 $\Gamma$ 上的测量数据之间的误差，其中边界条件和数据被定义。$\mathcal{L}_{\mathcal{F}}=\|\mathcal{F}(\bm{\hat{u}}(x,t|\bm{\theta}_{\text{PINN}}),x,t)\|_{\Gamma}$
    是残差函数的均方误差，用于强制 PINNs 生成的预测满足期望的微分方程。
- en: However, while PINNs have shown some success in capturing the underlying physical
    phenomena, [[85](#bib.bib85)] has pointed out that they often struggle to learn
    complex physical systems due to the difficulties posed by PDE regularizations
    in the optimization problem. Furthermore, the effectiveness of PINNs is highly
    dependent on the quality of the input data, and performance may suffer in the
    presence of noise or limited data [[28](#bib.bib28), [6](#bib.bib6), [174](#bib.bib174)].
    Moreover, limited by poor generalizability of neural networks, PINNs have trouble
    generalizing to the space and time domain that is not covered in the training
    set [[84](#bib.bib84), [115](#bib.bib115), [2](#bib.bib2)]. These limitations
    present significant challenges for the development and application of PINNs in
    real-world applications. Nonetheless, continued research into PINNs may help to
    overcome these challenges and improve their ability to capture and predict complex
    physical phenomena.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然PINNs在捕捉基础物理现象方面表现出了一些成功，但[[85](#bib.bib85)] 指出，由于PDE正则化在优化问题中带来的困难，它们往往难以学习复杂的物理系统。此外，PINNs的效果高度依赖于输入数据的质量，在噪声或数据有限的情况下，性能可能会受到影响
    [[28](#bib.bib28), [6](#bib.bib6), [174](#bib.bib174)]。此外，由于神经网络的泛化能力有限，PINNs在训练集未覆盖的时空领域中难以泛化
    [[84](#bib.bib84), [115](#bib.bib115), [2](#bib.bib2)]。这些限制给PINNs在实际应用中的发展和应用带来了重大挑战。然而，对PINNs的持续研究可能有助于克服这些挑战，并提高其捕捉和预测复杂物理现象的能力。
- en: 4.2 Solving Differential Equations
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 求解微分方程
- en: Continuing from the previous discussion of PINNs in the previous section, to
    overcome the optimization difficulties of PINNs, [[85](#bib.bib85)] proposed two
    ways to alleviate this optimization problem. One is to start by training the PINN
    on a small constraint coefficient and then gradually increase the coefficient
    instead of using a big coefficient right away. The other one is training the PINN
    to predict the solution one time step at a time instead of the entire space-time
    at once. Furthermore, [[28](#bib.bib28)] found that PINNs can overfit and propagate
    errors on domain boundaries, even when using physics-inspired regularizers. To
    address this, they introduced Gaussian Process-based smoothing on boundary conditions
    to recover PINNs’ performance against noise and errors in measurements. Moreover,
    [[174](#bib.bib174)] proposed a Bayesian framework that combines PINNs with a
    Bayesian network. Compared to PINNs, the hybrid model can provide uncertainty
    quantification and more accurate predictions in scenarios with large noise because
    it can avoid overfitting. Apart from PINNs, [[184](#bib.bib184)] proposed to use
    flow-based generative models to learn the solutions of probabilistic PDEs while
    the PDE constraints are enforced in the loss function. [[159](#bib.bib159)] investigated
    using neural nets to learn the evolution of the velocity fields of incompressible
    turbulent flow, the divergence of which is always zero. It found that constraining
    the model with a divergence-free regularizer can reduce the divergence of prediction
    and improve prediction accuracy.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 继续上一节关于PINNs的讨论，为了克服PINNs的优化困难，[[85](#bib.bib85)] 提出了两种缓解该优化问题的方法。一种方法是从用较小的约束系数开始训练PINN，然后逐渐增加系数，而不是立即使用较大的系数。另一种方法是训练PINN逐步预测每一个时间步的解，而不是一次性预测整个时空。此外，[[28](#bib.bib28)]
    发现即使使用了物理启发的正则化器，PINNs也可能在领域边界上过拟合并传播误差。为了解决这个问题，他们引入了基于高斯过程的边界条件平滑技术，以恢复PINNs在噪声和测量误差下的性能。此外，[[174](#bib.bib174)]
    提出了一个将PINNs与贝叶斯网络结合的贝叶斯框架。与PINNs相比，混合模型能够提供不确定性量化，并在噪声较大的情况下提供更准确的预测，因为它可以避免过拟合。除了PINNs，[[184](#bib.bib184)]
    提议使用基于流的生成模型来学习概率偏微分方程的解，同时在损失函数中强制执行PDE约束。[[159](#bib.bib159)] 研究了使用神经网络来学习不可压缩湍流的速度场演化，其散度始终为零。研究发现，通过用无散度正则化器来约束模型，可以减少预测的散度并提高预测准确性。
- en: 4.3 Improving Prediction Performance
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 改善预测性能
- en: Physics-guided loss functions or regularization have shown great success in
    improving prediction performance, especially the physical consistency of DL models.
    [[75](#bib.bib75)] used neural nets to model lake temperature at different times
    and different depths. They ensure that the predictions are physically meaningful
    by regularizing that the denser water predictions are at lower depths than predictions
    of less dense water. [[71](#bib.bib71)] further introduced a loss term that ensures
    thermal energy conservation between incoming and outgoing heat fluxes for modeling
    lake temperature. [[11](#bib.bib11)] designed conservation layers to strictly
    enforce conservation laws in their NN emulator of atmospheric convection. [[9](#bib.bib9)]
    introduced a more systematic way of enforcing nonlinear analytic constraints in
    neural networks via constraints in the loss function. [[178](#bib.bib178)] incorporated
    the loss of atomic force and atomic energy into neural nets for improved accuracy
    of simulating molecular dynamics. [[101](#bib.bib101)] proposed a novel multi-fidelity
    physics-constrained neural network for material modeling, in which the neural
    net was constrained by the losses caused by the violations of the model, initial
    conditions, and boundary conditions. [[42](#bib.bib42)] proposed a novel paradigm
    for spatiotemporal dynamics forecasting that performs spatiotemporal disentanglement
    using the functional variable separation. The specific-designed time invariance
    and regression loss functions ensure the separation of spatial and temporal information.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 物理指导的损失函数或正则化在提高预测性能，尤其是深度学习模型的物理一致性方面，取得了巨大成功。[[75](#bib.bib75)] 使用神经网络对不同时间和不同深度的湖泊温度进行建模。他们通过正则化使得较密集水体的预测温度低于较少密集水体的预测温度，从而确保预测结果在物理上有意义。[[71](#bib.bib71)]
    进一步引入了一个损失项，确保在湖泊温度建模中，传入和传出的热流之间的热能守恒。[[11](#bib.bib11)] 设计了保守层，以严格执行其大气对流神经网络模拟器中的守恒定律。[[9](#bib.bib9)]
    通过在损失函数中引入约束，提出了一种更系统化的方式来强制执行神经网络中的非线性解析约束。[[178](#bib.bib178)] 将原子力和原子能量的损失纳入神经网络中，以提高分子动力学模拟的准确性。[[101](#bib.bib101)]
    提出了一个新颖的多保真度物理约束神经网络用于材料建模，其中神经网络受到了由模型、初始条件和边界条件的违反造成的损失的约束。[[42](#bib.bib42)]
    提出了一个新颖的时空动态预测范式，通过功能变量分离进行时空解耦。特定设计的时间不变性和回归损失函数确保了空间和时间信息的分离。
- en: Hamiltonian mechanics is a mathematical framework that describes the dynamics
    of a system in terms of the total energy of the system, which is the sum of the
    kinetic and potential energy. [[58](#bib.bib58)] proposed Hamiltonian Neural Nets
    (HNN) that parameterizes a Hamiltonian with a neural network and then learn it
    directly from data. The conservation of desired quantities is constrained in the
    loss function during training. The proposed HNN has shown success in predicting
    mass-spring and pendulum systems. Lagrangian mechanics describes the dynamics
    of a system in terms of the difference between the kinetic energy and the potential
    energy of the system. [[33](#bib.bib33)] proposed Lagrangian Neural Nets (LNN)
    used a neural network to parameterize the Lagrangian function that is the kinetic
    energy minus the potential energy. They trained the neural network with the Euler-Lagrange
    constraint loss functions such that it can learn to approximately conserve the
    total energy of the system. [[52](#bib.bib52)] further simplify the HNN and LNN
    via explicit constraints. [[91](#bib.bib91)] further introduced a meta-learning
    approach in HNN to find the structure of the Hamiltonian that can be adapted quickly
    to a new instance of a physical system. [[182](#bib.bib182)] benchmark recent
    energy-conserving neural network models based on Lagrangian/Hamiltonian dynamics
    on four different physical systems.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 哈密顿力学是一个数学框架，用于描述系统的动态，基于系统的总能量，即动能和势能的总和。[[58](#bib.bib58)] 提出了哈密顿神经网络（HNN），通过神经网络对哈密顿量进行参数化，然后直接从数据中学习。在训练过程中，通过损失函数对期望量的守恒进行约束。提出的HNN在预测质量弹簧和摆系统方面取得了成功。拉格朗日力学描述了系统的动态，基于动能和势能之间的差值。[[33](#bib.bib33)]
    提出的拉格朗日神经网络（LNN）使用神经网络对拉格朗日函数进行参数化，该函数是动能减去势能。他们使用欧拉-拉格朗日约束损失函数训练神经网络，使其能够近似地守恒系统的总能量。[[52](#bib.bib52)]
    进一步通过显式约束简化了HNN和LNN。[[91](#bib.bib91)] 在HNN中引入了元学习方法，以快速适应物理系统的新实例并找到哈密顿量的结构。[[182](#bib.bib182)]
    基准测试了基于拉格朗日/哈密顿动力学的最新能量守恒神经网络模型在四种不同物理系统上的表现。
- en: 4.4 Data Generation
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 数据生成
- en: Simulation is an important method of analyzing, optimizing and designing real-world
    processes. Current numerical methods require significant computational resources
    when solving chaotic and complex differential equations. Because numerical discretization
    step size is confined to be very small due to stability constraints [[68](#bib.bib68)].
    Also, the estimation of unknown parameters by fitting equations to the observed
    data requires much manual engineering in each application since the optimization
    of the unknown parameters in the system highly depends on the initial guesses.
    Thus, there is an increasing interest in utilizing deep generative models for
    simulating complex physical dynamics. Many works also imposed physical constraints
    in the loss function for better physical consistency.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 模拟是分析、优化和设计现实世界过程的重要方法。目前的数值方法在解决混沌和复杂的微分方程时需要大量计算资源。这是因为数值离散化步长由于稳定性约束被限制为非常小[[68](#bib.bib68)]。此外，通过将方程拟合到观测数据来估计未知参数时，每个应用程序中都需要大量的人工工程，因为系统中未知参数的优化高度依赖于初始猜测。因此，利用深度生成模型模拟复杂物理动态的兴趣日益增加。许多研究还在损失函数中施加物理约束，以获得更好的物理一致性。
- en: For instance, [[171](#bib.bib171)] enforced the constraints of covariance into
    standard Generative Adversarial Networks (GAN) via statistical regularization,
    which leads to faster training and better physical consistency compared with standard
    GAN. [[172](#bib.bib172)] proposed tempoGAN for super-resolution fluid flow, in
    which an advection difference loss is used to enforce the temporal coherence of
    fluid simulation. [[166](#bib.bib166)] modified ESRGAN, which is a conditional
    GAN designed for super-resolution, by replacing the adversarial loss with a loss
    that penalizes errors in the energy spectrum between the generated images and
    the ground truth data. Conditional GAN is also applied to emulating numeric hydroclimate
    models in [[108](#bib.bib108)]. The simulation performance is further improved
    by penalizing the snow water equivalent via the loss function. [[77](#bib.bib77)]
    proposed a generative model to simulate fluid flows, in which a novel stream function-based
    loss function is designed to ensure divergence-free motion for incompressible
    flows. [[55](#bib.bib55)] proposed a physics-informed convolutional model for
    flow super-resolution, in which the physical consistency of the generated high-resolution
    flow fields is improved by minimizing the residuals of Navier-Stokes equations.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[[171](#bib.bib171)]通过统计正则化将协方差的约束强加到标准生成对抗网络（GAN）中，这导致训练更快，物理一致性更好，与标准GAN相比。[[172](#bib.bib172)]提出了tempoGAN用于超分辨率流体流动，其中使用了一个对流差异损失函数来强制流体模拟的时间一致性。[[166](#bib.bib166)]修改了ESRGAN，这是一种用于超分辨率的条件GAN，通过用一个惩罚生成图像与真实数据之间能量谱误差的损失函数来替换对抗损失。条件GAN也被应用于[[108](#bib.bib108)]中模拟数值水文气候模型。通过损失函数惩罚雪水当量，进一步提高了模拟性能。[[77](#bib.bib77)]提出了一种生成模型来模拟流体流动，其中设计了一个新颖的基于流函数的损失函数，以确保不可压缩流体的无散运动。[[55](#bib.bib55)]提出了一种物理信息卷积模型用于流动超分辨率，其中通过最小化Navier-Stokes方程的残差来提高生成的高分辨率流场的物理一致性。
- en: 4.5 Pros and Cons
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 优缺点
- en: While physics-guided loss functions are easy to design and use, and can improve
    prediction accuracy and physical consistency, they do have several limitations.
    Firstly, the physical constraints incorporated in the loss functions are usually
    considered soft constraints, and may not be strictly enforced. This means that
    the desired physical properties may not be guaranteed when the models are applied
    to new datasets. Secondly, PDE regularization can make loss landscapes more complex
    and cause optimization issues that are difficult to address, as noted in [[85](#bib.bib85)].
    Finally, there may be a trade-off between prediction errors and physics-guided
    regularizers. For example, [[159](#bib.bib159)] investigated incompressible turbulent
    flow prediction using neural nets and found that while constraining the model
    with a divergence-free regularizer can reduce the divergence of predictions, too
    much regularization may smooth out small eddies in the turbulence, resulting in
    a larger prediction error.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管物理指导的损失函数易于设计和使用，并且可以提高预测准确性和物理一致性，但它们确实存在一些限制。首先，纳入损失函数的物理约束通常被视为软约束，可能不会被严格执行。这意味着当模型应用于新数据集时，所期望的物理属性可能无法得到保证。其次，PDE正则化可能使损失函数的形状更复杂，并导致优化问题，这些问题难以解决，如[[85](#bib.bib85)]所述。最后，预测误差与物理指导正则化器之间可能存在权衡。例如，[[159](#bib.bib159)]研究了使用神经网络预测不可压湍流流动，发现虽然用无散度正则化器约束模型可以减少预测的散度，但过多的正则化可能会平滑湍流中的小涡旋，从而导致更大的预测误差。
- en: 5 Physics-Guided Design of Architecture
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 物理指导的架构设计
- en: While incorporating physical constraints as regularizers in the loss function
    can improve performance, DL is still used as a black box model in most cases.
    The modularity of neural networks offers opportunities for the design of novel
    neurons, layers, or blocks that encode specific physical properties. The advantage
    of physics-guided NN architectures is that they can impose “hard” constraints
    that are strictly enforced, compared to the “soft” constraints described in the
    previous section. The “soft” constraints are much easier to design than hard constraints,
    yet not required to be strictly satisfied. DL models with physics-guided architectures
    have theoretically guaranteed properties, and hence are more interpretable and
    generalizable. In this chapter, we will start with a case study of Turbulent-Flow
    Net (TF-Net) that unifies a popular Computational Fluid Dynamics (CFD) technique
    and a custom-designed U-net. We further categorize other related methods based
    on their architectural design.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然将物理约束作为正则化器纳入损失函数可以提高性能，但在大多数情况下，深度学习仍被用作黑箱模型。神经网络的模块化设计为设计具有特定物理属性的新型神经元、层或模块提供了机会。物理指导的神经网络架构的优点在于，它们可以施加“硬”约束，这些约束会被严格执行，而与前述的“软”约束相比，物理指导神经网络架构更具解释性和泛化能力。在本章中，我们将从湍流流场网络（TF-Net）的案例研究开始，该网络将一种流行的计算流体动力学（CFD）技术与自定义设计的U-net结合起来。我们进一步根据架构设计对其他相关方法进行分类。
- en: '![Refer to caption](img/1beae1a49aa1fba54a7dd149f9126d0e.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1beae1a49aa1fba54a7dd149f9126d0e.png)'
- en: 'Figure 1: Turbulent Flow Net: three identical encoders to learn the transformations
    of the three components of different scales, and one shared decoder that learns
    the interactions among these three components to generate the predicted 2D velocity
    field at the next instant. Each encoder-decoder pair can be viewed as a U-net
    and the aggregation is a weighted summation.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：湍流流场网络：三个相同的编码器学习不同尺度的三种成分的转换，以及一个共享解码器学习这三种成分之间的交互，以生成下一时刻的预测二维速度场。每对编码器-解码器可以视为一个U-net，聚合过程为加权求和。
- en: '5.1 Case Study: Turbulent-Flow Net'
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 案例研究：湍流流场网络
- en: 'TF-Net [[159](#bib.bib159)] is a physics-guided DL model for turbulent flow
    prediction. As shown in Figure [1](#S5.F1 "Figure 1 ‣ 5 Physics-Guided Design
    of Architecture ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey"),
    it applies scale separation to model different ranges of scales of the turbulent
    flow individually. Computational fluid dynamics (CFD) techniques are at the core
    of present-day turbulence simulation. Direct Numerical simulations (DNS) are accurate
    but not computationally feasible for practical applications. Great emphasis was
    placed on the alternative approaches including Large Eddy Simulation (LES) and
    Reynolds-averaged Navier-Stokes (RANS). Both resort to resolving large scales
    while modeling small scales, using various averaging techniques and/or low-pass
    filtering of the governing equations [[112](#bib.bib112), [131](#bib.bib131)].'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'TF-Net [[159](#bib.bib159)] 是一种物理引导的深度学习模型，用于湍流流动预测。如图 [1](#S5.F1 "Figure 1
    ‣ 5 Physics-Guided Design of Architecture ‣ Physics-Guided Deep Learning for Dynamical
    Systems: A Survey") 所示，它应用尺度分离方法分别建模不同范围的湍流流动。计算流体力学（CFD）技术是当前湍流模拟的核心。直接数值模拟（DNS）准确但在实际应用中不可行。对包括大涡模拟（LES）和
    Reynolds 平均 Navier-Stokes（RANS）等替代方法给予了高度重视。两者都利用解析大尺度，同时建模小尺度，使用各种平均技术和/或低通滤波的控制方程
    [[112](#bib.bib112), [131](#bib.bib131)]。'
- en: One of the widely used CFD techniques, the RANS-LES coupling approach [[47](#bib.bib47)],
    combines both Reynolds-averaged Equations (RANs) and Large Eddy Simulation (LES)
    approaches in order to take advantage of both methods. Inspired by RANS-LES coupling,
    TF-Net replaces a priori spectral filters with trainable convolutional layers.
    The turbulent flow is decomposed into three components, each of which is approximated
    by a specialized U-net to preserve the multiscale properties of the flow. A shared
    decoder learns the interactions among these three components and generates the
    final prediction. The motivation for this design is to explicitly guide the ML
    model to learn the nonlinear dynamics of large-scale and Subgrid-Scale Modeling
    motions as relevant to the task of spatiotemporal prediction. In other words,
    we need to force the model to learn not only the large eddies but also the small
    ones. When we train a predictive model directly on the data with MSE loss, the
    model may overlook the small eddies and only focus on large eddies to achieve
    reasonably good accuracy.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一种广泛使用的 CFD 技术，RANS-LES 结合方法 [[47](#bib.bib47)]，结合了 Reynolds 平均方程（RANs）和大涡模拟（LES）方法，以利用这两种方法的优势。受到
    RANS-LES 结合方法的启发，TF-Net 用可训练的卷积层替代了先验的频谱滤波器。湍流被分解为三个分量，每个分量由一个专门的 U-net 近似，以保持流动的多尺度特性。一个共享解码器学习这三个分量之间的交互，并生成最终预测。这个设计的动机是明确指导机器学习模型学习与时空预测任务相关的大尺度和子网格尺度建模运动的非线性动态。换句话说，我们需要迫使模型不仅学习大涡，还要学习小涡。当我们直接在数据上使用
    MSE 损失训练预测模型时，模型可能会忽视小涡，只关注大涡以实现合理的准确度。
- en: 'Besides RMSE, physically relevant metrics including divergence and energy spectrum
    are used to evaluate the performance of the model’s prediction. Figure [2](#S5.F2
    "Figure 2 ‣ 5.1 Case Study: Turbulent-Flow Net ‣ 5 Physics-Guided Design of Architecture
    ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey") shows TF-Net
    consistently outperforms all baselines on physically relevant metrics (Divergence
    and Energy Spectrum). Constraining it with the divergence-free regularizer that
    we described in the previous section can further reduce the Divergence. Figure
    [3](#S5.F3 "Figure 3 ‣ 5.1 Case Study: Turbulent-Flow Net ‣ 5 Physics-Guided Design
    of Architecture ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey")
    shows the ground truth and predicted velocity along $x$ direction by TF-Net and
    three best baselines. We see that the predictions by our TF-Net model are the
    closest to the target based on the shape and frequency of the motions.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '除了 RMSE 外，物理相关的指标包括发散度和能量谱也用于评估模型预测的性能。图 [2](#S5.F2 "Figure 2 ‣ 5.1 Case Study:
    Turbulent-Flow Net ‣ 5 Physics-Guided Design of Architecture ‣ Physics-Guided
    Deep Learning for Dynamical Systems: A Survey") 显示了 TF-Net 在物理相关指标（发散度和能量谱）上始终优于所有基准模型。使用我们在前一节描述的无发散度正则化器对其进行约束可以进一步减少发散度。图
    [3](#S5.F3 "Figure 3 ‣ 5.1 Case Study: Turbulent-Flow Net ‣ 5 Physics-Guided Design
    of Architecture ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey")
    显示了 TF-Net 和三种最佳基准模型在 $x$ 方向上的实际值和预测速度。我们看到，基于运动的形状和频率，TF-Net 模型的预测最接近目标。'
- en: 'We also perform an ablation study to understand each component of TF-Net and
    investigate whether the model has actually learned the flow with different scales.
    Figure [2](#S5.F2 "Figure 2 ‣ 5.1 Case Study: Turbulent-Flow Net ‣ 5 Physics-Guided
    Design of Architecture ‣ Physics-Guided Deep Learning for Dynamical Systems: A
    Survey") right includes predictions and the outputs of each small U-net while
    the other two encoders are zeroed out. We observe that the outputs of each small
    U-net are the flow with different scales, which demonstrates that can learn multi-scale
    behaviors. In summary, TF-Net is able to generate both accurate and physically
    meaningful predictions of the velocity fields that preserve critical quantities
    of relevance.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还进行了消融研究，以理解TF-Net的每个组件，并调查模型是否确实学习了不同尺度的流动。图[2](#S5.F2 "图 2 ‣ 5.1 案例研究：湍流网
    ‣ 5 物理引导的架构设计 ‣ 物理引导的动态系统深度学习：综述")右侧包括了每个小U-net的预测和输出，而其他两个编码器被置为零。我们观察到每个小U-net的输出是不同尺度的流动，这表明模型能够学习多尺度行为。总之，TF-Net能够生成既准确又具有物理意义的速度场预测，保持了相关的关键量。
- en: '![Refer to caption](img/3780eac2a4cb8268d3f373a399cd1635.png)![Refer to caption](img/2fe9e8509ab9d742a27bf3b3339023a9.png)![Refer
    to caption](img/896a7ae030dbbd0b8928a4da9e245709.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/3780eac2a4cb8268d3f373a399cd1635.png)![参考标题](img/2fe9e8509ab9d742a27bf3b3339023a9.png)![参考标题](img/896a7ae030dbbd0b8928a4da9e245709.png)'
- en: 'Figure 2: From left to right: Mean absolute divergence of different models’
    predictions at varying forecasting horizon; The Energy Spectrum of the target,
    TF-Net, U-net and ResNet on the leftmost square sub-region; Ablation Study: Ground
    truth, prediction from TF-Net and the outputs of each small U-net while the other
    two encoders are zeroed out.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：从左到右：不同模型预测在不同预测时间范围的平均绝对散度；左最方块子区域中的目标、TF-Net、U-net和ResNet的能量谱；消融研究：真实值、TF-Net的预测以及每个小U-net的输出，而其他两个编码器被置为零。
- en: '![Refer to caption](img/7474b2703880ca76dd9fc9d9606087f5.png)![Refer to caption](img/dec080b92a512057907bd4adac2a50ed.png)![Refer
    to caption](img/6e412da89009989a5039a2cfb7f4914d.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7474b2703880ca76dd9fc9d9606087f5.png)![参考标题](img/dec080b92a512057907bd4adac2a50ed.png)![参考标题](img/6e412da89009989a5039a2cfb7f4914d.png)'
- en: 'Figure 3: Ground truth and predicted velocity $u$ by TF-Net and three best
    baselines (U-Net, ResNet, and GAN) at time $T+10$, $T+30$ to $T+60$ (suppose $T$
    is the time step of the last input frame).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：TF-Net和三个最佳基线（U-Net、ResNet和GAN）在时间$T+10$、$T+30$到$T+60$（假设$T$是最后输入帧的时间步长）处的真实值和预测速度$u$。
- en: 5.2 Convolutional architecture
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 卷积架构
- en: Convolutional architecture remains dominant in most tasks of computer vision,
    such as objection, image classification, and video prediction. Thanks to their
    efficiency and desired inductive biases, such as locality and translation equivariance,
    convolution neural nets have been widely applied to emulate and predict complex
    spatiotemporal physical dynamics. Researchers have proposed various ways to bake
    desired physical properties into the design of convolutional models.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积架构在大多数计算机视觉任务中仍然占据主导地位，例如目标检测、图像分类和视频预测。由于其高效性和期望的归纳偏置，如局部性和平移等变性，卷积神经网络已被广泛应用于模拟和预测复杂的时空物理动态。研究人员已提出了多种方法将期望的物理属性融入卷积模型的设计中。
- en: For example, [[72](#bib.bib72)] proposed to enforce hard linear spatial PDE
    constraints within CNNs using the Fast Fourier Transform algorithm. [[35](#bib.bib35)]
    modified the LSTM units to introduce an intermediate variable to preserve monotonicity
    in a convolutional auto-encoder model for lake temperature. [[113](#bib.bib113)]
    proposed a physics-guided convolutional model, PhyDNN, which uses physics-guided
    structural priors and physics-guided aggregate supervision for modeling the drag
    forces acting on each particle in a computational fluid dynamics-discrete element
    Method. [[104](#bib.bib104)] designed HybridNet for dynamics predictions that
    combine ConvLSTM for predicting external forces with model-driven computation
    with CeNN for system dynamics. HybridNet achieves higher accuracy on the tasks
    of forecasting heat convection-diffusion and fluid dynamics. [[64](#bib.bib64)]
    proposed to combine deep learning and a differentiable PDE solver for understanding
    and controlling complex nonlinear physical systems over a long time horizon. [[141](#bib.bib141)]
    proposed continuous-filter convolutional layers for modeling quantum interactions.
    The convolutional kernel is parametrized by neural nets that take relative positions
    between any two points as input. They obtained a joint model for the total energy
    and interatomic forces that follow fundamental quantum-chemical principles.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[[72](#bib.bib72)] 提出了在 CNN 中通过快速傅里叶变换算法强制施加硬线性空间 PDE 约束。[[35](#bib.bib35)]
    修改了 LSTM 单元，引入了一个中间变量，以在湖泊温度的卷积自编码模型中保持单调性。[[113](#bib.bib113)] 提出了一个物理引导的卷积模型
    PhyDNN，该模型使用物理引导的结构先验和物理引导的汇总监督来建模计算流体动力学-离散元方法中作用于每个粒子的阻力。[[104](#bib.bib104)]
    设计了 HybridNet，用于结合 ConvLSTM 预测外部力和 CeNN 进行系统动态的模型驱动计算的动态预测。HybridNet 在热对流-扩散和流体动力学预测任务中实现了更高的准确性。[[64](#bib.bib64)]
    提出了将深度学习与可微分 PDE 求解器结合，用于理解和控制复杂的非线性物理系统，并对其进行长期预测。[[141](#bib.bib141)] 提出了用于建模量子相互作用的连续滤波卷积层。卷积核由神经网络参数化，神经网络以任意两点之间的相对位置作为输入。他们获得了一个联合模型，用于总能量和原子间力，遵循基本的量子化学原理。
- en: In addition, convolution layers have the potential to uncover governing equations.
    For instance, [[105](#bib.bib105), [106](#bib.bib106)] developed PDE-Net, which
    utilizes convolution to approximate differential operators over spatial domains
    of different orders. It also includes a symbolic neural network based on EQL [[110](#bib.bib110),
    [132](#bib.bib132)] to approximate and recover multivariate functions. The authors
    demonstrated that PDE-Net is more compact than SINDy dictionaries [[20](#bib.bib20)]
    and numerical experiments suggest that it can uncover the hidden PDE of various
    observed dynamics.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，卷积层有可能揭示控制方程。例如，[[105](#bib.bib105), [106](#bib.bib106)] 开发了 PDE-Net，该网络利用卷积来近似不同阶数的空间域上的微分算子。它还包括一个基于
    EQL [[110](#bib.bib110), [132](#bib.bib132)] 的符号神经网络，用于近似和恢复多变量函数。作者展示了 PDE-Net
    比 SINDy 字典 [[20](#bib.bib20)] 更加紧凑，并且数值实验表明它可以揭示各种观测动态的隐藏 PDE。
- en: 5.3 Graph Neural Networks
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 图神经网络
- en: Standard convolutional neural nets only operate on regular or uniform mesh such
    as images. Graph neural networks move beyond data on the regular grid towards
    modeling objects with arbitrary positions. For instance, graph neural networks
    can model the fluid dynamics on irregular meshes that CNNs cannot. [[135](#bib.bib135)]
    designed a deep encoder-processor-decoder graphic architecture for simulating
    fluid dynamics under Lagrangian description. The rich physical states are represented
    by graphs of interacting particles, and complex interactions are approximated
    by learned message-passing among nodes.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 标准卷积神经网络仅在规则或均匀网格（例如图像）上运行。图神经网络超越了规则网格的数据，朝着建模具有任意位置的对象的方向发展。例如，图神经网络可以在不规则网格上建模流体动力学，这是
    CNN 无法做到的。[[135](#bib.bib135)] 设计了一种深度编码器-处理器-解码器图形架构，用于在拉格朗日描述下模拟流体动力学。丰富的物理状态由相互作用的粒子图表示，复杂的相互作用通过节点间的学习消息传递来近似。
- en: '[[119](#bib.bib119)] utilized the same architecture to learn mesh-based simulation.
    The authors directly construct graphs on the irregular meshes constructed in the
    numerical simulation methods. In addition, they proposed an adaptive re-meshing
    algorithm that allows the model to accurately predict dynamics at both large and
    small scales. [[14](#bib.bib14)] further proposed two tricks to address the instability
    and error accumulation issues of training graph neural nets for solving PDEs.
    One is perturbing the input by a certain noise and only backpropagating errors
    on the last unroll step, and the other one is predicting multiple steps simultaneously
    in time. Both tricks make the model faster and more stable.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '[[119](#bib.bib119)] 利用相同的架构来学习基于网格的仿真。作者直接在数值仿真方法中构造不规则网格上的图。此外，他们提出了一种自适应重新网格算法，使模型能够准确预测大尺度和小尺度上的动态。[[14](#bib.bib14)]
    进一步提出了两个技巧来解决训练图神经网络解决 PDE 的不稳定性和误差积累问题。一种是通过一定噪声扰动输入并仅在最后一次展开步骤上反向传播误差，另一种是同时预测多个时间步骤。这两个技巧使模型更快且更稳定。'
- en: '[[95](#bib.bib95)] proposed a Neural Operator approach that learns the mapping
    between function spaces, and is invariant to different approximations and grids.
    More specifically, it used the message-passing graph network to learn Green’s
    function from the data, and then the learned Green’s function can be used to compute
    the final solution of PDEs. [[96](#bib.bib96)] further extended it to Fourier
    Neural Operator by replacing the kernel integral operator with a convolution operator
    defined in Fourier space, which is much more efficient than Neural Operator. In
    [[134](#bib.bib134)], graph networks were also used to represent, learn, and infer
    robotics systems, bodies, and joints. [[94](#bib.bib94)] proposed to learn compositional
    Koopman operators, using graph neural networks to encode the state into object-centric
    embeddings and using a block-wise linear transition matrix to regularize the shared
    structure across objects. Another important line of work is incorporating symmetries
    to design equivariant graph neural nets for modeling molecular dynamics, which
    will discuss in detail in Section [7](#S7 "7 Invariant and Equivariant DL Models
    ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey").'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[[95](#bib.bib95)] 提出了一个神经算子方法，该方法学习函数空间之间的映射，对不同的近似和网格具有不变性。更具体地说，它使用消息传递图网络从数据中学习格林函数，然后将学习到的格林函数用于计算
    PDE 的最终解。[[96](#bib.bib96)] 进一步将其扩展为傅里叶神经算子，通过用定义在傅里叶空间中的卷积算子替换核积分算子，这比神经算子高效得多。在
    [[134](#bib.bib134)] 中，图网络也被用于表示、学习和推断机器人系统、身体和关节。[[94](#bib.bib94)] 提出了学习组合性库普曼算子，使用图神经网络将状态编码为以对象为中心的嵌入，并使用块状线性过渡矩阵来正则化对象之间的共享结构。另一个重要的研究方向是将对称性融入设计等变图神经网络以建模分子动力学，详细内容将在第
    [7](#S7 "7 Invariant and Equivariant DL Models ‣ Physics-Guided Deep Learning
    for Dynamical Systems: A Survey") 节中讨论。'
- en: 5.4 Multilayer Perceptron
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 多层感知器
- en: One of the main applications of Multilayer perceptron(MLP) in physics-guided
    architecture design is finding the linear Koopman operator. Koopman theory [[83](#bib.bib83)]
    provides a way to represent a nonlinear dynamical system using an infinite-dimensional
    linear Koopman operator that acts on a Hilbert space of measurement functions
    of the system state. However, finding the appropriate measurement functions that
    map the dynamics to the function space, as well as an approximate and finite-dimensional
    Koopman operator, is highly nontrivial. One way to obtain an approximation of
    the Koopman operator is through the Dynamic Mode Decomposition algorithm [[139](#bib.bib139)],
    but this requires manually preparing nonlinear observables, which is not always
    feasible as prior knowledge about them may be lacking.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器（MLP）在物理引导架构设计中的主要应用之一是寻找线性库普曼算子。库普曼理论 [[83](#bib.bib83)] 提供了一种使用无限维线性库普曼算子表示非线性动态系统的方法，该算子作用于系统状态的希尔伯特空间中的测量函数。然而，找到适当的测量函数将动态映射到函数空间，以及一个近似和有限维的库普曼算子，是非常困难的。获得库普曼算子近似的一种方法是通过动态模态分解算法
    [[139](#bib.bib139)]，但这需要手动准备非线性可观察量，这在缺乏先验知识的情况下并不总是可行的。
- en: To address this challenge, recent research has explored using neural networks
    to learn the Koopman operator. One popular approach hypothesizes that there exists
    a data transformation that can be learned by neural networks, which yields an
    approximate finite-dimensional Koopman operator. For example, [[175](#bib.bib175)]
    and [[148](#bib.bib148)] have proposed using fully connected neural networks to
    directly map the observed dynamics to a dictionary of nonlinear observables that
    span a Koopman invariant subspace. This mapping is represented through an autoencoder
    network, which embeds the observed dynamics onto a low-dimensional latent space
    where the Koopman operator is approximated by a linear layer. [[107](#bib.bib107)]
    have further generalized this approach to enable learning the Koopman operator
    for systems with continuous spectra. [[5](#bib.bib5)] have also designed a similar
    autoencoder architecture for forecasting physical processes. But in the latent
    space, the consistency of both the forward and backward systems is ensured, while
    other models only consider the forward system. This approach performs well on
    systems that have both forward and backward dynamics, enabling long time prediction.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这一挑战，近期的研究探索了使用神经网络来学习Koopman算子。一种流行的方法假设存在一种可以通过神经网络学习的数据变换，从而得到一个近似的有限维Koopman算子。例如，[[175](#bib.bib175)]和[[148](#bib.bib148)]
    提出了使用全连接神经网络直接将观察到的动态映射到一个涵盖Koopman不变子空间的非线性可观测字典。这种映射通过一个自动编码器网络表示，该网络将观察到的动态嵌入到一个低维潜在空间，在这个空间中，Koopman算子通过一个线性层来近似。[[107](#bib.bib107)]
    进一步推广了这种方法，以便为具有连续谱的系统学习Koopman算子。[[5](#bib.bib5)] 还为物理过程的预测设计了类似的自动编码器架构。但是在潜在空间中，确保了正向系统和反向系统的一致性，而其他模型仅考虑正向系统。这种方法在具有正向和反向动态的系统上表现良好，能够进行长期预测。
- en: Koopman theory can also be used to model real-world dynamics without known governing
    laws. [[163](#bib.bib163)] have developed a novel approach, Koopman Neural Forecaster
    (KNF), to forecast highly non-stationary time series in an interpretable and robust
    manner. This approach uses Koopman theory to simplify non-linear real-world dynamics
    into linear systems, which then can be easily manipulated by modifying the Koopman
    matrix. It employs predefined measurement functions to impose appropriate inductive
    biases and uses a Koopman operator that varies over time to capture the underlying
    changing distribution. The model outperforms the state-of-the-art on highly non-stationary
    time series datasets, including M4, cryptocurrency return forecasting, and sports
    player trajectory prediction.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Koopman理论也可以用来建模没有已知控制规律的现实世界动态。[[163](#bib.bib163)] 开发了一种新颖的方法，Koopman神经预报器（KNF），以一种可解释且稳健的方式预测高度非平稳的时间序列。这种方法利用Koopman理论将非线性现实世界动态简化为线性系统，然后通过修改Koopman矩阵进行操作。它使用预定义的测量函数来施加适当的归纳偏差，并使用随时间变化的Koopman算子来捕捉潜在的变化分布。该模型在高度非平稳的时间序列数据集上优于现有技术，包括M4、加密货币回报预测和运动员轨迹预测。
- en: 5.5 Pros and Cons
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 优点与缺点
- en: Embedding physics into the design of the model architecture can enable physical
    principles strictly enforced and theoretically guaranteed. That leads to more
    interpretable and generalizable deep learning models. However, it is not trivial
    to design physics-guided architectures that perform and generalize well without
    hurting the representation power of neural nets. Hard inductive biases can greatly
    improve the sample efficiency of learning, but could potentially become restrictive
    when the size of the dataset is big enough for models to learn all the necessary
    inductive biases from the data or when the inductive biases are not strict.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 将物理学嵌入到模型架构的设计中，可以严格执行物理原理并理论上得到保证。这将导致更具解释性和更具泛化性的深度学习模型。然而，设计物理引导的架构，使其在不损害神经网络表示能力的情况下表现良好和具有良好的泛化能力，并非易事。强归纳偏差可以极大地提高学习的样本效率，但在数据集足够大以使模型从数据中学习所有必要的归纳偏差，或当归纳偏差不严格时，可能会变得具有限制性。
- en: 6 Hybrid Physics-DL Model
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 混合物理-深度学习模型
- en: The papers discussed in the previous two sections focus on incorporating the
    known properties of physical systems into the design of loss functions or neural
    network modules. In this section, we talk about works that directly combine pure
    physics-based models, such as numerical methods, with DL models.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 前两节讨论的论文侧重于将物理系统的已知属性融入损失函数或神经网络模块的设计中。在本节中，我们讨论那些直接将纯物理模型（如数值方法）与深度学习模型相结合的工作。
- en: '6.1 Case Study: Neural Differential Equations'
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 案例研究：神经微分方程
- en: Neural Ordinary Differential Equations (Neural ODEs) [[27](#bib.bib27)] generalize
    traditional RNNs that process data sequentially in discrete time steps by modeling
    data as continuous functions that change over time, allowing for a more flexible
    way to capture complex dynamics. They changed the traditionally discretized neuron
    layer depths into continuous equivalents such that the derivative of the hidden
    state can be parameterized using a neural network. The output of the network is
    then computed using a black box differential equation solver, making Neural ODEs
    an efficient combination of neural nets and numerical solvers.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 神经常微分方程（Neural ODEs）[[27](#bib.bib27)] 将传统的按离散时间步处理数据的递归神经网络（RNNs）推广为通过将数据建模为随时间变化的连续函数，从而捕捉更复杂的动态。它们将传统的离散化神经元层深度转换为连续的等效形式，使得隐藏状态的导数可以通过神经网络进行参数化。然后，网络的输出通过黑箱微分方程求解器计算，这使得神经ODEs成为神经网络和数值求解器的高效结合。
- en: 'More specifically, they parametrize the velocity $\bm{\hat{z}}$ of a hidden
    state $\bm{z}$ with the help of a neural network $\bm{\hat{z}}=f_{\theta}(\bm{z},t)$.
    Given the initial time $t_{0}$ and target time $t_{T}$, Neural ODEs predict the
    target state $\bm{\hat{y}}_{T}$ by performing the following encoding, integration,
    and decoding operations:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，它们通过神经网络 $\bm{\hat{z}}=f_{\theta}(\bm{z},t)$ 对隐藏状态 $\bm{z}$ 的速度 $\bm{\hat{z}}$
    进行参数化。给定初始时间 $t_{0}$ 和目标时间 $t_{T}$，神经ODEs 通过执行以下编码、积分和解码操作来预测目标状态 $\bm{\hat{y}}_{T}$：
- en: '|  | $\bm{z}(t_{0})=\phi_{\text{enc}}(\bm{y}_{0}),\;\;\;\;\;\;\bm{z}(t_{T})=\bm{z}(t_{0})+\int_{t_{0}}^{t_{T}}f_{\bm{\theta}}(\bm{z},t)dt,\;\;\;\;\;\;\bm{\hat{y}}_{T}=\psi_{\text{dec}}(\bm{z}(t_{T}))$
    |  | (7) |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{z}(t_{0})=\phi_{\text{enc}}(\bm{y}_{0}),\;\;\;\;\;\;\bm{z}(t_{T})=\bm{z}(t_{0})+\int_{t_{0}}^{t_{T}}f_{\bm{\theta}}(\bm{z},t)dt,\;\;\;\;\;\;\bm{\hat{y}}_{T}=\psi_{\text{dec}}(\bm{z}(t_{T}))$
    |  | (7) |'
- en: 'where the encoder $\phi_{\text{enc}}$ and the decoder $\psi_{\text{dec}}$ can
    be neural networks. Solving an ODE numerically is commonly done by discretization
    and integration, such as the simple Euler method and higher-order variants of
    the Runge-Kutta method. However, all of these are computationally intensive since
    they require backpropagating through the operations of the solvers and store any
    intermediate quantities of the forward pass, incurring a high memory cost. Thus,
    the adjoint method [[121](#bib.bib121)] is used to efficiently compute gradients
    during backpropagation. To compute the gradients of a loss function $L$ with respect
    to the initial state $\bm{z}(t_{0})$ and the parameters $\bm{\theta}$, the key
    idea of the adjoint method is to introduce an adjoint state $\bm{p}(t)$, $\bm{p}(t)=\frac{\partial
    L}{\partial\bm{z}(t)}$, which satisfies the following differential equation:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 其中编码器 $\phi_{\text{enc}}$ 和解码器 $\psi_{\text{dec}}$ 可以是神经网络。数值求解ODE通常通过离散化和积分来完成，如简单的欧拉法和高阶的龙格-库塔法。然而，这些方法计算量大，因为它们需要在求解器的操作过程中反向传播并存储前向传递的任何中间量，导致高内存开销。因此，使用对偶方法
    [[121](#bib.bib121)] 来高效计算反向传播过程中的梯度。要计算损失函数 $L$ 相对于初始状态 $\bm{z}(t_{0})$ 和参数 $\bm{\theta}$
    的梯度，对偶方法的关键思想是引入对偶状态 $\bm{p}(t)$，$\bm{p}(t)=\frac{\partial L}{\partial\bm{z}(t)}$，它满足以下微分方程：
- en: '|  | $\frac{d\bm{p}(t)}{dt}=-\bm{p}(t)^{T}\frac{\partial f_{\bm{\theta}}(\bm{z}(t),t)}{\partial\bm{z}}$
    |  | (8) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{d\bm{p}(t)}{dt}=-\bm{p}(t)^{T}\frac{\partial f_{\bm{\theta}}(\bm{z}(t),t)}{\partial\bm{z}}$
    |  | (8) |'
- en: 'The adjoint state is used to compute the gradients of the loss function with
    respect to the initial state and the parameters using the following formulas:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对偶状态用于计算损失函数相对于初始状态和参数的梯度，使用以下公式：
- en: '|  | $\frac{\partial L}{\partial\bm{\theta}}=-\int_{t_{T}}^{t_{0}}\bm{p}(t)^{T}\frac{\partial
    f_{\bm{\theta}}(\bm{z}(t),t)}{\partial\bm{\theta}}dt;$ |  | (9) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\frac{\partial L}{\partial\bm{\theta}}=-\int_{t_{T}}^{t_{0}}\bm{p}(t)^{T}\frac{\partial
    f_{\bm{\theta}}(\bm{z}(t),t)}{\partial\bm{\theta}}dt;$ |  | (9) |'
- en: 'In a word, these formulas can be computed efficiently by solving the ODE for
    $\bm{p}(t)$ using the same numerical method used to solve the forward ODE. During
    the forward pass, the ODE solver computes the solution of the differential equation
    $\bm{z}(t)$ using the initial state $\bm{z}(t_{0})$ and the function $f_{\bm{\theta}}(\bm{z}(t),t)$.
    During the backward pass, the adjoint state $\bm{p}(t)$ is computed by solving
    Eqn. [8](#S6.E8 "In 6.1 Case Study: Neural Differential Equations ‣ 6 Hybrid Physics-DL
    Model ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey") that starts
    from the final time $t_{T}$ and backpropagates through time. This adjoint state
    is then used to compute the gradients of the loss function with respect to the
    initial state and the parameters of the ODE function in Eqn. [9](#S6.E9 "In 6.1
    Case Study: Neural Differential Equations ‣ 6 Hybrid Physics-DL Model ‣ Physics-Guided
    Deep Learning for Dynamical Systems: A Survey"), which can then be used to update
    the model parameters through gradient descent.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '总之，这些公式可以通过使用与求解正向ODE相同的数值方法来高效计算。正向传播过程中，ODE求解器使用初始状态$\bm{z}(t_{0})$和函数$f_{\bm{\theta}}(\bm{z}(t),t)$计算微分方程$\bm{z}(t)$的解。反向传播过程中，伴随状态$\bm{p}(t)$通过求解从最终时间$t_{T}$开始并通过时间反向传播的方程[8](#S6.E8
    "In 6.1 Case Study: Neural Differential Equations ‣ 6 Hybrid Physics-DL Model
    ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey")进行计算。然后使用这个伴随状态计算损失函数相对于初始状态和方程[9](#S6.E9
    "In 6.1 Case Study: Neural Differential Equations ‣ 6 Hybrid Physics-DL Model
    ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey")中ODE函数的参数的梯度，这些梯度可以通过梯度下降更新模型参数。'
- en: Neural ODEs have broad potential applications, particularly in domains that
    require continuous and dynamic models. They offer a useful tool for building continuous-time
    time series models, which can easily handle data coming at irregular intervals.
    They also allow for building normalizing flow, which makes it easy to track the
    change in density, even for unrestricted neural architecture. There have been
    several follow-up works that further extended the idea of continuous neural nets.
    For instance, [[43](#bib.bib43)] introduced Augmented Neural ODE that is more
    expressive, empirically more stable, and lower computationally efficient than
    Neural ODEs. More importantly, it can learn the functions that have continuous
    trajectories mappings intersecting each other, which Neural ODEs cannot represent.
    [[120](#bib.bib120)] further extended this idea of continuous neural nets to graph
    convolutions, and proposed Graph Neural ODE. [[102](#bib.bib102)] proposed Neural
    Stochastic Differential Equation (Neural SDE), which models stochastic noise injection
    by stochastic differential equations. They demonstrated that incorporating the
    noise injection regularization mechanism into the continuous neural network can
    reduce overfitting and achieve lower generalization errors. [[99](#bib.bib99)]
    proposed a Neural ODE-based generative time-series model that uses the known differential
    equation instead of treating it as hidden unit dynamics so that they can integrate
    mechanistic knowledge into the Neural ODE. [[122](#bib.bib122)] utilized neural
    networks to directly approximate the unknown terms in the differential equations.
    By using the adjoint method, the proposed model can efficiently compute gradients
    with respect to all parameters in the model, including the initial conditions,
    the parameters of the ODE, and the boundary conditions.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 神经ODE具有广泛的潜在应用，特别是在需要连续和动态模型的领域。它们为构建连续时间序列模型提供了有用的工具，这些模型可以轻松处理不规则间隔的数据。它们还允许构建归一化流，这使得跟踪密度变化变得容易，即使对于不受限制的神经架构也是如此。已有几个后续工作进一步扩展了连续神经网络的概念。例如，[[43](#bib.bib43)]
    引入了增强型神经ODE，这种方法比神经ODE更具表现力、经验上更稳定，并且计算效率更低。更重要的是，它可以学习具有互相交叉的连续轨迹映射的函数，而神经ODE无法表示。[[120](#bib.bib120)]
    进一步将这种连续神经网络的思想扩展到图卷积，并提出了图神经ODE。[[102](#bib.bib102)] 提出了神经随机微分方程（Neural SDE），它通过随机微分方程建模随机噪声注入。他们证明，将噪声注入正则化机制融入连续神经网络可以减少过拟合，并实现更低的泛化误差。[[99](#bib.bib99)]
    提出了基于神经ODE的生成时间序列模型，该模型使用已知的微分方程，而不是将其视为隐藏单元动态，从而将机制知识整合到神经ODE中。[[122](#bib.bib122)]
    利用神经网络直接近似微分方程中的未知项。通过使用伴随方法，提出的模型可以高效计算相对于模型中所有参数的梯度，包括初始条件、ODE参数和边界条件。
- en: 6.2 Residual Modeling
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 残差建模
- en: Perhaps the simplest form of hybrid modeling is residual learning, where DL
    learns to predict the errors or residuals made by physics-based models. The key
    is to learn the bias of physics-based models and correct it with the help of DL
    models [[54](#bib.bib54), [152](#bib.bib152)]. A representative example is DeepGLEAM
    [[170](#bib.bib170)] for forecasting COVID-19 mortality that combines a mechanistic
    epidemic simulation model GLEAM with DL. It uses a Diffusion Convolutional RNN
    [[93](#bib.bib93)] (DCRNN) to learn the correction terms from GLEAM, which leads
    to improved performance over either purely mechanistic models or purely DL models
    on the task of one-week ahead COVID-19 death count predictions
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的混合建模形式之一是残差学习，其中DL学习预测物理模型产生的误差或残差。关键是学习物理模型的偏差，并借助DL模型进行修正[[54](#bib.bib54),
    [152](#bib.bib152)]。一个代表性例子是DeepGLEAM [[170](#bib.bib170)]，它结合了机制性流行病模拟模型GLEAM和DL，用于预测COVID-19的死亡率。它使用扩散卷积RNN
    [[93](#bib.bib93)] (DCRNN) 从GLEAM中学习修正项，从而在一周内的COVID-19死亡人数预测任务上，相比纯粹的机制模型或纯DL模型，性能得到了提升。
- en: Similarly, [[37](#bib.bib37)] combines graph neural nets with a CFD simulator
    run on a coarse mesh to generate high-resolution fluid flow prediction. CNNs are
    used to correct the velocity field from the numerical solver on a coarse grid
    in [[81](#bib.bib81)]. [[111](#bib.bib111)] utilized neural networks for subgrid
    modeling of the LES simulation of two-dimensional turbulence. In [[133](#bib.bib133)],
    a neural network model is implemented in the reduced order modeling framework
    to compensate for the errors from the model reduction. [[74](#bib.bib74)] proposed
    DR-RNN that is trained to find the residual minimizer of numerically discretized
    ODEs or PDEs. They showed that DR-RNN can greatly reduce both computational cost
    and time discretization error of the reduced order modeling framework. [[176](#bib.bib176)]
    introduced the APHYNITY framework that can efficiently augment approximate physical
    models with deep data-driven networks. A key feature of their method is being
    able to decompose the problem in such a way that the data-driven model only models
    what cannot be captured by the physical model.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，[[37](#bib.bib37)] 将图神经网络与在粗网格上运行的CFD模拟器结合，用于生成高分辨率的流体流动预测。在[[81](#bib.bib81)]中，CNNs被用来修正粗网格上数值解算器的速度场。[[111](#bib.bib111)]
    利用神经网络对二维湍流的LES模拟进行亚网格建模。在[[133](#bib.bib133)]中，将神经网络模型实现于降阶建模框架中，以补偿模型降阶的误差。[[74](#bib.bib74)]
    提出了DR-RNN，通过训练来寻找数值离散化ODEs或PDEs的残差最小化器。他们展示了DR-RNN能大大减少降阶建模框架的计算成本和时间离散化误差。[[176](#bib.bib176)]
    引入了APHYNITY框架，能够有效地用深度数据驱动网络增强近似物理模型。其方法的一个关键特点是能够将问题分解，使数据驱动模型仅建模物理模型无法捕捉的部分。
- en: 6.3 Intermediate Variable Modeling
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 中间变量建模
- en: DL models can be used to replace one or more components of physics-based models
    that are difficult to compute or unknown. For example, [[153](#bib.bib153)] replaced
    the numerical solver for solving Poisson’s equations with convolution networks
    in the procedure of Eulerian fluid simulation, and the obtained results are realistic
    and showed good generalization properties. [[116](#bib.bib116)] proposed to use
    neural nets to reconstruct the model corrections in terms of variables that appear
    in the closure model. [[39](#bib.bib39)] applied a U-net to estimate the velocity
    field given the historical temperature frames, then used the estimated velocity
    to forecast the sea surface temperature based on the closed-form solution of the
    advection-diffusion equation. [[109](#bib.bib109)] combined the high-dimensional
    model representation that is represented as a sum of mode terms each of which
    is a sum of component functions with NNs to build multidimensional potential,
    in which NNs are used to represent the component functions that minimize the error
    mode term by mode term.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: DL模型可以用来替代计算困难或未知的物理模型中的一个或多个组件。例如，[[153](#bib.bib153)]在欧拉流体模拟过程中，用卷积网络替代了解算泊松方程的数值解算器，得到的结果既真实又表现出良好的泛化特性。[[116](#bib.bib116)]
    提出了使用神经网络来重建闭合模型中变量的模型修正。[[39](#bib.bib39)] 应用U-net来根据历史温度帧估计速度场，然后使用估计的速度基于平流-扩散方程的封闭形式解决方案预测海面温度。[[109](#bib.bib109)]
    结合了高维模型表示，将其表示为每一项都是组件函数之和的模式项，利用NNs来建立多维潜力，其中NNs用于表示组件函数，从而逐项减少误差模式项。
- en: 6.4 Pros and Cons
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 优势与劣势
- en: Combining physics-based and deep learning models can enable leveraging both
    the flexibility of neural nets for modeling unknown parts of the dynamics and
    the interpretability and generalizability of physics-based models. However, one
    potential downside of hybrid physics-DL models worth mentioning is that all or
    most of the dynamics could be captured by neural nets and the physics-based models
    contribute little to the learning. That would hurt the interpretability and the
    generalizability of the model. We must ensure an optimal balance between the physics-based
    and DL models. We need neural nets to only model the information that cannot be
    represented by the physical prior.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 结合基于物理的模型和深度学习模型可以同时利用神经网络在建模动态未知部分上的灵活性以及基于物理的模型在可解释性和泛化能力上的优势。然而，值得提及的一个潜在缺点是，混合物理-深度学习模型可能会使得所有或大多数动态被神经网络捕获，而基于物理的模型对学习贡献甚微。这将损害模型的可解释性和泛化能力。我们必须确保基于物理的模型和深度学习模型之间的最佳平衡。我们需要神经网络仅用于建模那些物理先验无法表示的信息。
- en: 7 Invariant and Equivariant DL Models
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 不变与等变深度学习模型
- en: Symmetry has long been implicitly used in DL to design networks with known invariances
    and equivariances. Convolutional neural networks enabled breakthroughs in computer
    vision by leveraging translation equivariance [[179](#bib.bib179), [90](#bib.bib90),
    [180](#bib.bib180)]. Similarly, recurrent neural networks [[129](#bib.bib129),
    [63](#bib.bib63)], graph neural networks [[137](#bib.bib137), [80](#bib.bib80)],
    and capsule networks [[130](#bib.bib130), [62](#bib.bib62)] all impose symmetries.
    While the equivariant DL models have achieved remarkable success in image and
    text data [[32](#bib.bib32), [164](#bib.bib164), [30](#bib.bib30), [29](#bib.bib29),
    [92](#bib.bib92), [82](#bib.bib82), [7](#bib.bib7), [169](#bib.bib169), [31](#bib.bib31),
    [51](#bib.bib51), [165](#bib.bib165), [41](#bib.bib41), [56](#bib.bib56), [146](#bib.bib146)],
    the study of equivariant nets in learning dynamical systems has become increasingly
    popular recently [[97](#bib.bib97), [161](#bib.bib161), [145](#bib.bib145), [65](#bib.bib65),
    [155](#bib.bib155), [144](#bib.bib144)]. Since the symmetries can be integrated
    into neural nets through not only loss functions but also the design of neural
    net layers and there has been a large volume of works about equivariant and invariant
    DL models for physical dynamics, we discuss this topic separately in this section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对称性长期以来被隐式地用于深度学习中，以设计具有已知不变性和等变性的网络。卷积神经网络通过利用平移等变性在计算机视觉领域取得了突破 [[179](#bib.bib179),
    [90](#bib.bib90), [180](#bib.bib180)]。类似地，递归神经网络 [[129](#bib.bib129), [63](#bib.bib63)]、图神经网络
    [[137](#bib.bib137), [80](#bib.bib80)] 和胶囊网络 [[130](#bib.bib130), [62](#bib.bib62)]
    都施加了对称性。虽然等变深度学习模型在图像和文本数据 [[32](#bib.bib32), [164](#bib.bib164), [30](#bib.bib30),
    [29](#bib.bib29), [92](#bib.bib92), [82](#bib.bib82), [7](#bib.bib7), [169](#bib.bib169),
    [31](#bib.bib31), [51](#bib.bib51), [165](#bib.bib165), [41](#bib.bib41), [56](#bib.bib56),
    [146](#bib.bib146)] 中取得了显著成功，但最近在学习动态系统中的等变网络研究也越来越受到关注 [[97](#bib.bib97), [161](#bib.bib161),
    [145](#bib.bib145), [65](#bib.bib65), [155](#bib.bib155), [144](#bib.bib144)]。由于对称性不仅可以通过损失函数集成到神经网络中，还可以通过神经网络层的设计进行集成，且关于物理动态的等变和不变深度学习模型的研究已经有大量的工作，我们将在本节中单独讨论这个主题。
- en: '![Refer to caption](img/f80871402dcb845b43107d3f63c36457.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/f80871402dcb845b43107d3f63c36457.png)'
- en: 'Figure 4: Illustration of equivariance: $f(x)=2x$ w.r.t $T=\mathrm{rot}(\pi/4)$'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：等变性的示意图：$f(x)=2x$ 对 $T=\mathrm{rot}(\pi/4)$ 的情况
- en: In physics, there is a deep connection between symmetries and physics. Noether’s
    law gives a correspondence between conserved quantities and groups of symmetries.
    For instance, translation symmetry corresponds to the conservation of energy and
    rotation symmetry corresponds to the conservation of angular momentum. By building
    a neural network that inherently respects a given symmetry, we thus make conservation
    of the associated quantity more likely and consequently the model’s prediction
    more physically accurate. Furthermore, by designing a model that is inherently
    equivariant to transformations of its inputs, we can guarantee that our model
    generalizes automatically across these transformations, making it robust to distributional
    shifts.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在物理学中，对称性与物理学之间有着深刻的联系。诺特定律提供了守恒量与对称群之间的对应关系。例如，平移对称性对应于能量守恒，而旋转对称性对应于角动量守恒。通过构建一个本质上尊重给定对称性的神经网络，我们使得相关量的守恒更有可能，从而使模型的预测更符合物理实际。此外，通过设计一个本质上对输入变换保持等变的模型，我们可以保证模型能够自动在这些变换上进行泛化，使其对分布变化具有鲁棒性。
- en: A group of symmetries or simply group consists of a set $G$ together with an
    associative composition map $\circ\colon G\times G\to G$. The composition map
    has an identity $1\in G$ and composition with any element of $G$ is required to
    be invertible. A group $G$ has an action on a set $S$ if there is an action map
    $\cdot\colon G\times S\to S$ which is compatible with the composition law. We
    say further that $\rho:G\mapsto GL(V)$ is a $G$-representation if the set $V$
    is a vector space and each group element $g\in G$ is represented by a linear map
    (matrix) $\rho(g)$ that acts on $V$. Formally, a function $f\colon X\to Y$ may
    be described as respecting the symmetry coming from a group $G$ using the notion
    of equivariance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一组对称性，或简称为群，由一个集合 $G$ 及其结合运算映射 $\circ\colon G\times G\to G$ 组成。该运算映射具有一个单位元
    $1\in G$，并且与 $G$ 中的任何元素进行组合时都需要可逆。如果一个群 $G$ 在集合 $S$ 上有一个作用，则存在一个作用映射 $\cdot\colon
    G\times S\to S$，它与运算规律兼容。进一步地，我们说 $\rho:G\mapsto GL(V)$ 是一个 $G$-表示，如果集合 $V$ 是一个向量空间，并且每个群元素
    $g\in G$ 都通过一个线性映射（矩阵）$\rho(g)$ 表示，该映射作用于 $V$。形式上，函数 $f\colon X\to Y$ 可以用等变的概念来描述尊重来自群
    $G$ 的对称性。
- en: Definition 2.
  id: totrans-149
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 定义 2
- en: Assume a group representation $\rho_{\text{in}}$ of $G$ acts on $X$ and $\rho_{\text{out}}$
    acts on $Y$. We say a function $f$ is $G$-equivariant if
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 假设群表示 $\rho_{\text{in}}$ 作用于 $X$，而 $\rho_{\text{out}}$ 作用于 $Y$。我们说函数 $f$ 是 $G$-等变的，如果
- en: '|  | $f(\rho_{\text{in}}(g)(x))=\rho_{\text{out}}(g)f(x)$ |  | (10) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $f(\rho_{\text{in}}(g)(x))=\rho_{\text{out}}(g)f(x)$ |  | (10) |'
- en: 'for all $x\in X$ and $g\in G$. The function $f$ is $G$-invariant if $f(\rho_{\text{in}}(g)(x))=f(x)$
    for all $x\in X$ and $g\in G$. This is a special case of equivariance for the
    case $\rho_{\mathrm{out}}(g)=1$. See Figure [4](#S7.F4 "Figure 4 ‣ 7 Invariant
    and Equivariant DL Models ‣ Physics-Guided Deep Learning for Dynamical Systems:
    A Survey") for an illustration of a rotation equivariant function.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '对于所有 $x\in X$ 和 $g\in G$。如果 $f$ 是 $G$-不变的，当且仅当对所有 $x\in X$ 和 $g\in G$，都有 $f(\rho_{\text{in}}(g)(x))=f(x)$。这是等变性的一个特殊情况，其中
    $\rho_{\mathrm{out}}(g)=1$。有关旋转等变函数的示意图，请参见图 [4](#S7.F4 "Figure 4 ‣ 7 Invariant
    and Equivariant DL Models ‣ Physics-Guided Deep Learning for Dynamical Systems:
    A Survey")。'
- en: '7.1 Case Study: Equivariant Deep Dynamics Models'
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 案例研究：等变深度动力学模型
- en: '[[161](#bib.bib161)] exploited the symmetries of fluid dynamics to design equivariant
    networks. The Navier-Stokes equations are invariant under the following five different
    transformations. Individually, each of these types of transformations generates
    a group of symmetries in the system.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '[[161](#bib.bib161)] 利用流体动力学的对称性设计了等变网络。Navier-Stokes 方程在以下五种不同变换下是不变的。每种变换单独生成系统中的一组对称性。'
- en: •
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Space translation:   $T_{\bm{c}}^{\mathrm{sp}}\bm{w}(\bm{x},t)=\bm{w}(\bm{x-c},t)$,
      $\bm{c}\in\mathbb{R}^{2}$,'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 空间平移：$T_{\bm{c}}^{\mathrm{sp}}\bm{w}(\bm{x},t)=\bm{w}(\bm{x-c},t)$，$\bm{c}\in\mathbb{R}^{2}$，
- en: •
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Time translation:  $T_{\tau}^{\mathrm{time}}\bm{w}(\bm{x},t)=\bm{w}(\bm{x},t-\tau)$,
      $\tau\in\mathbb{R}$,'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 时间平移：$T_{\tau}^{\mathrm{time}}\bm{w}(\bm{x},t)=\bm{w}(\bm{x},t-\tau)$，$\tau\in\mathbb{R}$，
- en: •
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Galilean transformation:  $T_{\bm{c}}^{\mathrm{gal}}\bm{w}(\bm{x},t)=\bm{w}(\bm{x}-\bm{c}t,t)+\bm{c}$,
      $\bm{c}\in\mathbb{R}^{2}$,'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 加利略变换：$T_{\bm{c}}^{\mathrm{gal}}\bm{w}(\bm{x},t)=\bm{w}(\bm{x}-\bm{c}t,t)+\bm{c}$，$\bm{c}\in\mathbb{R}^{2}$，
- en: •
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Rotation/Reflection:   $T_{R}^{\mathrm{rot}}\bm{w}(\bm{x},t)=R\bm{w}(R^{-1}\bm{x},t),\;R\in
    O(2)$,'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 旋转/反射：$T_{R}^{\mathrm{rot}}\bm{w}(\bm{x},t)=R\bm{w}(R^{-1}\bm{x},t),\;R\in O(2)$，
- en: •
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Scaling:   $T_{\lambda}^{sc}\bm{w}(\bm{x},t)=\lambda\bm{w}(\lambda\bm{x},\lambda^{2}t)$,
      $\lambda\in\mathbb{R}_{>0}$.'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 缩放：$T_{\lambda}^{sc}\bm{w}(\bm{x},t)=\lambda\bm{w}(\lambda\bm{x},\lambda^{2}t)$，$\lambda\in\mathbb{R}_{>0}$。
- en: Consider a system of differential operators $\mathcal{D}$ acting on $\hat{\mathcal{F}}_{V}$.
    Denote the set of solutions $\mathrm{Sol}(\mathcal{D})\subseteq\hat{\mathcal{F}}_{V}.$
    We say $G$ is a symmetry group of $\mathcal{D}$ if $G$ preserves $\mathrm{Sol}(\mathcal{D})$.
    That is, if $\varphi$ is a solution of $\mathcal{D}$, then for all $g\in G$, $g(\varphi)$
    is also. In order to forecast the evolution of a system $\mathcal{D}$, we model
    the forward prediction function $f$. Let $\bm{w}\in\mathrm{Sol}(\mathcal{D})$.
    The input to $f$ is a collection of $k$ snapshots at times $t-k,\ldots,t-1$ denoted
    $\bm{w}_{t-i}\in\mathcal{F}_{d}$. The prediction function $f\colon\mathcal{F}_{d}^{k}\to\mathcal{F}_{d}$
    is defined $f(\bm{w}_{t-k},\ldots,\bm{w}_{t-1})=\bm{w}_{t}$. It predicts the solution
    at a time $t$ based on the solution in the past. Let $G$ be a symmetry group of
    $\mathcal{D}$. Then for $g\in G$, $g(\bm{w})$ is also a solution of $\mathcal{D}$.
    Thus $f(g\bm{w}_{t-k},\ldots,g\bm{w}_{t-1})=g\bm{w}_{t}$. Consequently, $f$ is
    $G$-equivariant.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑作用在$\hat{\mathcal{F}}_{V}$上的微分算子系统$\mathcal{D}$。设解集为$\mathrm{Sol}(\mathcal{D})\subseteq\hat{\mathcal{F}}_{V}$。我们说$G$是$\mathcal{D}$的对称群，如果$G$保持$\mathrm{Sol}(\mathcal{D})$不变。也就是说，如果$\varphi$是$\mathcal{D}$的一个解，那么对于所有$g\in
    G$，$g(\varphi)$也是解。为了预测系统$\mathcal{D}$的演变，我们对前向预测函数$f$建模。令$\bm{w}\in\mathrm{Sol}(\mathcal{D})$。$f$的输入是一组在时间$t-k,\ldots,t-1$的$k$个快照，记作$\bm{w}_{t-i}\in\mathcal{F}_{d}$。预测函数$f\colon\mathcal{F}_{d}^{k}\to\mathcal{F}_{d}$定义为$f(\bm{w}_{t-k},\ldots,\bm{w}_{t-1})=\bm{w}_{t}$。它基于过去的解预测时间$t$的解。设$G$为$\mathcal{D}$的对称群。那么对于$g\in
    G$，$g(\bm{w})$也是$\mathcal{D}$的一个解。因此$f(g\bm{w}_{t-k},\ldots,g\bm{w}_{t-1})=g\bm{w}_{t}$。因此，$f$是$G$-等变的。
- en: They tailored different methods for incorporating each symmetry into CNNs for
    spatiotemporal dynamics forecasting. CNNs are time translation-equivariant when
    used in an autoregressive manner. Convolutions are also naturally space translation
    equivariant. Scale equivariance in dynamics is unique as the physical law dictates
    the scaling of magnitude, space and time simultaneously. To achieve this, they
    replaced the standard convolution layers with group correlation layers over the
    group $G=(\mathbb{R}_{>0},\cdot)\ltimes(\mathbb{R}^{2},+)$ of both scaling and
    translations. The $G$-correlation upgrades this operation by both translating
    and scaling the kernel relative to the input,
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 他们为将每种对称性融入CNN用于时空动态预测而定制了不同的方法。当CNN以自回归方式使用时，它们是时间平移-等变的。卷积也自然是空间平移等变的。动态中的尺度等变性是独特的，因为物理法则同时决定了幅度、空间和时间的缩放。为了实现这一点，他们用群$G=(\mathbb{R}_{>0},\cdot)\ltimes(\mathbb{R}^{2},+)$的群相关层替代了标准卷积层，该群包括缩放和翻译。$G$-相关通过相对于输入同时进行卷积核的平移和缩放来升级这一操作。
- en: '|  | $\bm{v}(\bm{p},s,\mu)=\sum_{\lambda\in\mathbb{R}_{>0},t\in\mathbb{R},\bm{q}\in\mathbb{Z}^{2}}\mu\bm{w}(\bm{p}+\mu\bm{q},\mu^{2}t,\lambda)K(\bm{q},s,t,\lambda),$
    |  | (11) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $\bm{v}(\bm{p},s,\mu)=\sum_{\lambda\in\mathbb{R}_{>0},t\in\mathbb{R},\bm{q}\in\mathbb{Z}^{2}}\mu\bm{w}(\bm{p}+\mu\bm{q},\mu^{2}t,\lambda)K(\bm{q},s,t,\lambda),$
    |  | (11) |'
- en: where $s$ and $t$ denote the indices of output and input channels. They add
    an axis to the tensors corresponding to the scale factor $\mu$. In addition, the
    rotational symmetry was modeled using $\mathrm{SO}(2)$-equivariant convolutions
    and activations within the E(2)-CNN framework [[164](#bib.bib164)].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$s$和$t$表示输出和输入通道的索引。它们向张量中添加了与尺度因子$\mu$相关的一个轴。此外，旋转对称性是通过在E(2)-CNN框架内使用$\mathrm{SO}(2)$-等变卷积和激活来建模的[[164](#bib.bib164)]。
- en: To make CNNs equivariant to Galilean transformation, since they are already
    translation-equivariant, it is only necessary to make them equivariant to uniform
    motion transformation, which is adding a constant vector field to the vector field.
    This is part of Galilean invariance and relevant to all non-relativistic physics
    modeling. And the uniform motion equivariance is enforced by conjugating the model
    with shifted input distribution. Basically, for each sliding local block in each
    convolutional layer, they shift the mean of the input tensor to zero and shift
    the output back after convolution and activation function per sample. In other
    words, if the input is $\bm{\mathcal{P}}_{b\times d_{in}\times s\times s}$ and
    the output is $\bm{\mathcal{Q}}_{b\times d_{out}}=\sigma(\bm{\mathcal{P}}\cdot
    K)$ for one sliding local block, where $b$ is batch size, $d$ is number of channels,
    $s$ is the kernel size, and $K$ is the kernel, then
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使卷积神经网络（CNNs）对伽利略变换保持等变性，由于它们已经对平移保持等变性，因此只需使它们对均匀运动变换保持等变性，即向向量场中添加一个常量向量场。这是伽利略不变性的一部分，与所有非相对论物理建模相关。均匀运动等变性是通过与平移输入分布的模型共轭来强制执行的。基本上，对于每个卷积层中的滑动局部块，它们将输入张量的均值移到零，并在每个样本的卷积和激活函数之后将输出移回。换句话说，如果输入是$\bm{\mathcal{P}}_{b\times
    d_{in}\times s\times s}$，输出是$\bm{\mathcal{Q}}_{b\times d_{out}}=\sigma(\bm{\mathcal{P}}\cdot
    K)$对于一个滑动局部块，其中$b$是批量大小，$d$是通道数，$s$是卷积核大小，$K$是卷积核，那么
- en: '|  | $\displaystyle\bm{\mu}_{i}=\mathrm{Mean}_{jkl}\left(\bm{\mathcal{P}}_{ijkl}\right);\quad\bm{\mathcal{P}}_{ijkl}\mapsto\bm{\mathcal{P}}_{ijkl}-\bm{\mu}_{i};\quad\bm{\mathcal{Q}}_{ij}\mapsto\bm{\mathcal{Q}}_{ij}+\bm{\mu}_{i}.$
    |  | (12) |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\bm{\mu}_{i}=\mathrm{Mean}_{jkl}\left(\bm{\mathcal{P}}_{ijkl}\right);\quad\bm{\mathcal{P}}_{ijkl}\mapsto\bm{\mathcal{P}}_{ijkl}-\bm{\mu}_{i};\quad\bm{\mathcal{Q}}_{ij}\mapsto\bm{\mathcal{Q}}_{ij}+\bm{\mu}_{i}.$
    |  | (12) |'
- en: This will allow the convolution layer to be equivariant with respect to uniform
    motion. If the input is a vector field, this operation is applied to each element.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使卷积层在均匀运动方面保持等变性。如果输入是一个向量场，这个操作将应用于每个元素。
- en: 'The DL models used are ResNet and U-Net, and their equivariant counterparts.
    Spatiotemporal prediction is done autoregressively. Standard RMSE and an RMSE
    computed on the energy spectra are used to measure performance. The models are
    tested on Rayleigh-Bénard convection (RBC) and reanalysis ocean current velocity
    data. For RBC, the test sets have random transformations from the relevant symmetry
    groups applied to each sample. This mimics real-world data in which each sample
    has an unknown reference frame. For ocean data, tests are also performed on different
    time ranges and different domains from the training set, representing distributional
    shifts. Figure [5](#S7.F5 "Figure 5 ‣ 7.1 Case Study: Equivariant Deep Dynamics
    Models ‣ 7 Invariant and Equivariant DL Models ‣ Physics-Guided Deep Learning
    for Dynamical Systems: A Survey") shows the equivariant models perform significantly
    better than their non-equivariant counterparts on both simulated RBC data and
    real-world reanalysis ocean currents. They also show equivariant models also achieve
    much lower energy spectrum errors and enjoy favorable sample complexity compared
    with data augmentation.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '使用的深度学习模型是ResNet和U-Net及其等变对应模型。时空预测是自回归的。标准的均方根误差（RMSE）和在能量谱上计算的RMSE用于测量性能。模型在Rayleigh-Bénard对流（RBC）和再分析海洋流速数据上进行了测试。对于RBC，测试集对每个样本应用了来自相关对称群的随机变换。这模拟了每个样本具有未知参考框架的真实数据。对于海洋数据，测试也在训练集之外的不同时间范围和不同领域上进行，表示分布变化。图[5](#S7.F5
    "Figure 5 ‣ 7.1 Case Study: Equivariant Deep Dynamics Models ‣ 7 Invariant and
    Equivariant DL Models ‣ Physics-Guided Deep Learning for Dynamical Systems: A
    Survey")显示，等变模型在模拟的RBC数据和真实的再分析海洋流量数据上表现显著优于非等变模型。它们还显示等变模型在能量谱误差上也显著较低，并且相比于数据增强具有更有利的样本复杂度。'
- en: '![Refer to caption](img/866e79fefb016129188e1353a7fff07c.png)![Refer to caption](img/870d72312df8745bbba2ad390f0d7ba1.png)![Refer
    to caption](img/724622a5781d258d28b705d4b4c928a3.png)![Refer to caption](img/3d358ad0b6844b3952fe8c6939c6abcc.png)![Refer
    to caption](img/ff954353389ad46446e1d91b4328d16b.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/866e79fefb016129188e1353a7fff07c.png)![参考标题](img/870d72312df8745bbba2ad390f0d7ba1.png)![参考标题](img/724622a5781d258d28b705d4b4c928a3.png)![参考标题](img/3d358ad0b6844b3952fe8c6939c6abcc.png)![参考标题](img/ff954353389ad46446e1d91b4328d16b.png)'
- en: 'Figure 5: Top: The ground truth and the predicted velocity norm fields $\|\bm{w}\|_{2}$
    of RBC at time step $1$, $5$ and $10$ by the ResNet and four Equ-ResNets on four
    test samples applied with random uniform motion, magnitude, rotation, and scaling
    transformations respectively. The first column is the target, the second is ResNet
    predictions, and the third is predictions by Equ-ResNets. Bottom: The ground truth
    and predicted velocity norm fields of ocean currents by ResNet (Unet) and four
    Equ-ResNets (Equ-Unets) on the test set.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：顶部：ResNet 和四个 Equ-ResNets 对四个测试样本应用随机均匀运动、幅度、旋转和缩放变换后，RBC 在时间步 $1$、$5$ 和
    $10$ 的真实值和预测速度范数场 $\|\bm{w}\|_{2}$。第一列是目标，第二列是 ResNet 预测，第三列是 Equ-ResNets 预测。底部：ResNet（Unet）和四个
    Equ-ResNets（Equ-Unets）在测试集上的海洋流速的真实值和预测速度范数场。
- en: 7.2 Equivariant Convolutional Neural Networks
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 等变卷积神经网络
- en: However, real-world dynamical data rarely conform to strict mathematical symmetries,
    due to noise and missing values or symmetry-breaking features in the underlying
    dynamical system. [[149](#bib.bib149)] further explored approximately equivariant
    convolutional networks that are biased towards preserving symmetry but are not
    strictly constrained to do so. The key idea is relaxing the weight-sharing schemes
    by introducing additional trainable weights that can vary across group elements
    to break the strict equivariance constraints. The proposed approximate equivariant
    networks can always learn the correct amount of symmetry from the data, and thus
    consistently perform well on real-world turbulence data with no symmetry, approximate
    symmetry, and perfect symmetry. When we incorporate prior knowledge into neural
    nets, we usually need to choose between strictly enforcing it in the design of
    the model or softly constraining it via regularizers. But this approach allows
    the model to decide whether and how to use prior knowledge (symmetry) based on
    the specific task. Moreover, [[162](#bib.bib162)] built a meta-learning framework,
    DyAd, to forecast systems with different parameters. Specifically, it utilized
    an encoder capable of extracting the time-invariant and translation-invariant
    parts of a dynamical system and a prediction network to adapt and forecast giving
    the inferred system. Time invariance is achieved by using 3D convolution and time-shift
    invariant loss. On challenging turbulent flow prediction and real-world ocean
    temperature and currents forecasting tasks, this is the first framework that can
    generalize and predict dynamics across a wide range of heterogeneous domains.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，现实世界中的动态数据很少符合严格的数学对称性，因为噪声、缺失值或潜在动态系统中的对称性破坏特征都会影响对称性。[[149](#bib.bib149)]
    进一步探讨了近似等变卷积网络，这些网络偏向于保持对称性，但并不严格受限于此。关键思想是通过引入可以在组元素之间变化的附加可训练权重来放宽权重共享方案，从而打破严格的等变约束。所提出的近似等变网络始终可以从数据中学习到正确数量的对称性，因此在没有对称性、近似对称性和完美对称性的现实世界湍流数据上表现稳定良好。当我们将先验知识纳入神经网络时，通常需要在严格执行模型设计中的先验知识或通过正则化器进行软约束之间进行选择。但这种方法允许模型根据具体任务决定是否以及如何使用先验知识（对称性）。此外，[[162](#bib.bib162)]
    构建了一个元学习框架 DyAd，用于预测具有不同参数的系统。具体而言，它利用一个能够提取动态系统中时间不变和平移不变部分的编码器和一个预测网络来适应和预测给定的推断系统。时间不变性是通过使用
    3D 卷积和时间偏移不变损失来实现的。在具有挑战性的湍流流动预测和现实世界的海洋温度及洋流预测任务中，这是第一个可以在广泛的异质领域中进行泛化和预测动态的框架。
- en: Apart from incorporating symmetries into regular convolution, there has been
    a surge of interest in designing equivariant continuous convolution models. This
    is due to the fact that continuous convolution allows for convolutional operations
    to be performed on a continuous input domain. For instance, [[141](#bib.bib141)]
    proposed SchNet, which is a continuous convolution framework that generalizes
    the CNN approach to continuous convolutions to model particles at arbitrary positions.
    Continuous convolution kernels are generated by dense neural networks that operate
    on the interatomic distances, which ensures rotational and translation invariance
    of the energy. In a traffic forecasting application, [[157](#bib.bib157)] proposed
    a novel model, Equivariant Continuous COnvolution (ECCO) that uses rotationally
    equivariant continuous convolutions to embed the symmetries of the system for
    improved trajectory prediction. The rotational equivariance is achieved by a weight-sharing
    scheme within kernels in polar coordinates. ECCO achieves superior performance
    to baselines on two real-world trajectory prediction datasets, Argoverse and TrajNet++.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将对称性融入常规卷积之外，人们对设计等变连续卷积模型也产生了浓厚的兴趣。这是因为连续卷积允许在连续输入域上执行卷积操作。例如，[[141](#bib.bib141)]
    提出了 SchNet，这是一个连续卷积框架，将 CNN 方法推广到连续卷积，以建模处于任意位置的粒子。连续卷积核由对原子间距离进行操作的密集神经网络生成，这确保了能量的旋转和翻译不变性。在交通预测应用中，[[157](#bib.bib157)]
    提出了一个新模型，等变连续卷积（ECCO），它使用旋转等变的连续卷积将系统的对称性嵌入以改进轨迹预测。通过在极坐标内的权重共享方案实现了旋转等变性。ECCO
    在两个实际轨迹预测数据集 Argoverse 和 TrajNet++ 上表现优于基线模型。
- en: 7.3 Equivariant Graph Neural Networks
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 等变图神经网络
- en: In addition to the equivariant convolution, numerous equivariant graph neural
    nets have also been developed, particularly for modeling atomic systems and molecular
    dynamics. This is due to the pervasive presence of symmetry in molecular physics,
    as evidenced by roto-translation equivariance in molecular conformations and coordinates.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 除了等变卷积之外，还开发了许多等变图神经网络，特别是用于建模原子系统和分子动力学。这是因为分子物理中普遍存在对称性，例如分子构型和坐标中的旋转-平移等变性。
- en: '[[136](#bib.bib136)] designed E(n)-equivariant graph neural network for predicting
    molecular properties. It updates edge features with the Euclidean distance between
    nodes and updates the coordinates of particles with the weighted sum of relative
    differences of all neighbors. [[142](#bib.bib142)] proposed to use a score-based
    generative model for generating molecular conformation. The authors used equivariant
    graph neural networks to estimate the score function, which is the gradient fields
    of the log density of atomic coordinates because it is roto-translation equivariant.
    [[3](#bib.bib3)] designed Cormorant, a rotationally covariant neural network architecture
    for learning the behavior and properties of complex many-body physical systems.
    Cormorant achieves promising results in learning molecular potential energy surfaces
    on the MD-17 dataset and learning the geometric, energetic, electronic, and thermodynamic
    properties of molecules on the GDB-9 dataset. [[144](#bib.bib144)] proposed a
    model for autoregressive generation of 3D molecular structures with reinforcement
    learning (RL). The method uses equivariant state representations for autoregressive
    generation, built largely from Cormorant, and integrates such representations
    within an existing actor-critic RL generation framework. [[154](#bib.bib154)]
    further designed a series SE(3)-equivariant operations and building blocks for
    DL architectures operating on geometric point cloud data, which was used to construct
    PhiSNet, a novel architecture capable of accurately predicting wavefunctions and
    electronic densities.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[[136](#bib.bib136)] 设计了一个 E(n)-等变图神经网络用于预测分子性质。它通过节点之间的欧几里得距离更新边特征，并通过所有邻居的相对差异的加权和更新粒子坐标。
    [[142](#bib.bib142)] 提出了使用基于评分的生成模型来生成分子构象。作者使用等变图神经网络来估计评分函数，因为它是旋转-平移等变的，评分函数是原子坐标对数密度的梯度场。
    [[3](#bib.bib3)] 设计了 Cormorant，这是一种旋转协变的神经网络架构，用于学习复杂多体物理系统的行为和性质。Cormorant 在
    MD-17 数据集上学习分子势能面和在 GDB-9 数据集上学习分子的几何、能量、电子和热力学性质方面取得了令人满意的结果。 [[144](#bib.bib144)]
    提出了一个用于 3D 分子结构自回归生成的模型，结合了强化学习 (RL)。该方法使用等变状态表示进行自回归生成，主要基于 Cormorant，并将这些表示集成到现有的演员-评论家
    RL 生成框架中。 [[154](#bib.bib154)] 进一步设计了一系列 SE(3)-等变操作和用于几何点云数据的 DL 架构构建块，用于构建 PhiSNet，这是一种能够准确预测波函数和电子密度的新型架构。'
- en: Additionally, permutation invariance also exists in molecular dynamics. For
    instance, quantum mechanical energies are invariant if we exchange the labels
    of identical atoms. However, [[151](#bib.bib151)] stated that enforcing equivariance
    to all permutations in graph neural nets can be very restrictive when modeling
    molecules. Thus, they proposed to decompose a graph into a collection of local
    graphs that are isomorphic to a pre-selected template graph so that the sub-graphs
    can always be canonicalized to template graphs before convolution is applied.
    By doing this, the graph neural nets can not only be much more expressive but
    also locally equivariant. [[155](#bib.bib155)] proposed to build equivariant neural
    networks based on the idea that nonlinear $O(d)$-equivariant functions can be
    universally expressed in terms of a lightweight collection of scalars, which are
    simpler to build. They demonstrated the efficiency and scalability of their proposed
    approach to two classical physics problems, calculating the total mechanical energy
    of particles and the total electromagnetic force, that obeys all translation,
    rotation, reflection, and permutation symmetries. Moreover, since the design of
    equivariant layers is a difficult task, [[13](#bib.bib13)] proposed a lie point
    symmetry data augmentation method for training graph neural PDE solvers and this
    method enables these neural solvers to preserve multiple symmetries.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，排列不变性在分子动力学中也存在。例如，如果我们交换相同原子的标签，量子力学能量是不变的。然而，[[151](#bib.bib151)] 指出，在图神经网络中强制所有排列的等变性在建模分子时可能会非常限制。因此，他们提出将图分解为一组与预选模板图同构的局部图，使得子图在卷积应用之前可以始终标准化为模板图。通过这样做，图神经网络不仅可以变得更具表现力，还可以局部等变。[[155](#bib.bib155)]
    提出了基于非线性 $O(d)$-等变函数可以通过轻量级标量集合通用表示的想法来构建等变神经网络，这些标量集合更容易构建。他们展示了所提出方法在两个经典物理问题中的效率和可扩展性，这两个问题分别是计算粒子的总机械能和总电磁力，这些都遵循所有平移、旋转、反射和排列对称性。此外，由于设计等变层是一项困难的任务，[[13](#bib.bib13)]
    提出了一个李点对称数据增强方法，用于训练图神经PDE求解器，这种方法使得这些神经求解器能够保持多种对称性。
- en: 7.4 Symmetry Discovery
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 对称性发现
- en: There has been also an emerging area that is symmetry discovery, the key idea
    of which is to find the weight-sharing patterns in neural networks that have been
    trained on data with symmetries. For instance, [[183](#bib.bib183)] factorized
    the weight matrix in a fully connected layer into a symmetry (i.e. weight-sharing)
    matrix and a vector of filter parameters. The two parts are learned separately
    in the inner and outer loop training with the Model-Agnostic Meta-Learning algorithm
    (MAML) [[50](#bib.bib50)], which is an optimization-based meta-learning method
    so that the symmetry matrix can learn the weight-sharing pattern from the data.
    Furthermore, [[40](#bib.bib40)] proposed Lie Algebra Convolutional Network (L-conv),
    a novel architecture that can learn the Lie algebra basis and automatically discover
    symmetries from data. It can be considered as an infinitesimal version of group
    convolution. [[173](#bib.bib173)] further leveraged L-conv to construct the LieGAN,
    to automatically discover equivariances from a dataset using a paradigm akin to
    generative adversarial training. It represents symmetry as an interpretable Lie
    algebra basis and can discover various symmetries. Specifically, a generator learns
    a group of transformations applied to the data, which preserves the original distribution
    and fools the discriminator.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个新兴领域是对称性发现，其关键思想是找到在具有对称性的训练数据上训练的神经网络中的权重共享模式。例如，[[183](#bib.bib183)] 将全连接层中的权重矩阵分解为对称（即权重共享）矩阵和滤波器参数向量。这两个部分分别在内循环和外循环训练中使用模型无关元学习算法（MAML）[[50](#bib.bib50)]进行学习，这是一种基于优化的元学习方法，使得对称矩阵可以从数据中学习权重共享模式。此外，[[40](#bib.bib40)]
    提出了李代数卷积网络（L-conv），这是一种新颖的架构，可以学习李代数基，并自动从数据中发现对称性。它可以被视为群卷积的一个无穷小版本。[[173](#bib.bib173)]
    进一步利用L-conv构建了LieGAN，通过类似于生成对抗训练的范式自动从数据集中发现等变性。它将对称性表示为可解释的李代数基，并可以发现各种对称性。具体来说，生成器学习一组应用于数据的变换，这些变换保持原始分布并欺骗判别器。
- en: 7.5 Pros and Cons
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 优缺点
- en: By designing a model that is intrinsically equivariant or invariant to input
    transformations, we can ensure that our model generalizes automatically across
    these transformations, making it resilient to distributional shifts. In contrast,
    data augmentation techniques cannot provide equivariance guarantees when the models
    are applied to new datasets. Empirically and theoretically, it has been shown
    that equivariant and invariant neural nets offer superior data and parameter efficiency
    compared to data augmentation techniques. Furthermore, incorporating symmetries
    enhances the physical consistency of neural nets because of Noether’s Law.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设计一个在输入变换下本质上等变或不变的模型，我们可以确保模型在这些变换下自动泛化，使其对分布变化具有弹性。相比之下，当模型应用于新数据集时，数据增强技术无法提供等变性保证。从经验和理论上看，等变和不变的神经网络在数据和参数效率方面优于数据增强技术。此外，结合对称性由于诺特定律增强了神经网络的物理一致性。
- en: 'However, incorporating too many symmetries may overly constrain the representation
    power of neural nets and slow down both training and inference. In addition, many
    real-world dynamics do not have perfect symmetries. A perfectly equivariant model
    that respects a given symmetry may have trouble learning partial or approximated
    symmetries in real-world data. Thus, an ideal model for real-world dynamics should
    be approximately equivariant and automatically learn the correct amount of symmetry
    in the data, such as the paper we discussed in Section [7.2](#S7.SS2 "7.2 Equivariant
    Convolutional Neural Networks ‣ 7 Invariant and Equivariant DL Models ‣ Physics-Guided
    Deep Learning for Dynamical Systems: A Survey"). There are a few other works that
    explore the same idea. For instance, [[53](#bib.bib53)] proposed the soft equivariant
    layer by directly summing up a flexible layer with one that has strong equivariance
    inductive biases to model the soft equivariance.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，过多的对称性可能会过度限制神经网络的表示能力，并减缓训练和推理过程。此外，许多现实世界的动态系统并不具有完美的对称性。一个完全等变的模型可能在学习现实世界数据中的部分或近似对称性时遇到困难。因此，理想的现实世界动态模型应该是近似等变的，并且能够自动学习数据中的正确对称性，例如我们在第[7.2](#S7.SS2
    "7.2 Equivariant Convolutional Neural Networks ‣ 7 Invariant and Equivariant DL
    Models ‣ Physics-Guided Deep Learning for Dynamical Systems: A Survey")节中讨论的论文。还有一些其他工作探索了相同的思路。例如，[[53](#bib.bib53)]提出了软等变层，通过将一个灵活的层与一个具有强等变归纳偏置的层直接相加，以建模软等变性。'
- en: 8 Discussion
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 讨论
- en: In this paper, we systematically review the recent progress in physics-guided
    DL for learning dynamical systems. We discussed multiple ways to inject first-principle
    and physical constraints into DL including (1) physics-informed loss regularizers
    (2) physics-guided design, (3) hybrid models, and (4) symmetry. By integrating
    physical principles, the DL models can achieve better physical consistency, higher
    accuracy, increased data efficiency, improved generalization, and greater interpretability.
    Despite the great promise and exciting progress in the field, physics-guided AI
    is still at its infant stage. Below we review the emerging challenges and opportunities
    of learning physical dynamics with deep learning for future studies.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们系统地回顾了物理引导的深度学习在学习动态系统方面的最新进展。我们讨论了将第一性原理和物理约束注入深度学习的多种方法，包括（1）物理信息损失正则化器，（2）物理引导设计，（3）混合模型，以及（4）对称性。通过整合物理原理，深度学习模型可以实现更好的物理一致性、更高的准确性、更高的数据效率、改进的泛化能力和更强的可解释性。尽管该领域展现了巨大的潜力和令人兴奋的进展，物理引导的人工智能仍处于初级阶段。以下是我们对未来研究中物理动态学习的挑战和机遇的回顾。
- en: 8.1 Improving Generalization
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1 提升泛化能力
- en: Generalization is a central problem in machine learning. One current limitation
    of deep learning models for learning complex dynamics is their inability to understand
    the system solely from data and handle distributional shifts that naturally occur.
    Most deep learning models for dynamics modeling are trained to model a specific
    system and still struggle with generalization. For example, in turbulence modeling,
    deep learning models trained with fixed boundaries and initial conditions often
    fail to generalize to fluid flows with different characteristics. To overcome
    this limitation, one approach is to build physics-guided deep learning models,
    where the physics part plays a dominant role while the neural networks focus on
    learning the unknown process [[122](#bib.bib122)]. Another promising direction
    is meta-learning. For instance, [[162](#bib.bib162)] proposed a model-based meta-learning
    method called DyAd that can generalize across heterogeneous domains of fluid dynamics.
    However, this model can only generalize well on the dynamics with interpolated
    physical parameters and cannot extrapolate beyond the range of the physical parameters
    in the training set. Another idea is to transform data into a canonical distribution
    that neural networks can learn from and then restore the original data after predictions
    are made [[78](#bib.bib78)]. Since neural networks struggle with multiple distributions,
    this approach aims to find a single distribution that can represent the dynamics
    effectively. A trustworthy and reliable model for learning physical dynamics should
    be able to extrapolate to systems with various parameters, external forces, or
    boundary conditions while maintaining high accuracy. Therefore, further research
    into generalizable physics-guided deep learning is crucial.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 泛化是机器学习中的核心问题。当前深度学习模型在学习复杂动态时的一个限制是它们无法仅凭数据理解系统，并处理自然发生的分布变化。大多数用于动态建模的深度学习模型是针对特定系统进行训练的，仍然在泛化上存在困难。例如，在湍流建模中，使用固定边界和初始条件训练的深度学习模型常常无法对具有不同特征的流体流动进行泛化。为了解决这一限制，一种方法是构建物理指导的深度学习模型，其中物理部分发挥主导作用，而神经网络则专注于学习未知过程[[122](#bib.bib122)]。另一种有前景的方向是元学习。例如，[[162](#bib.bib162)]提出了一种基于模型的元学习方法，称为DyAd，它可以在流体动力学的异质领域中进行泛化。然而，该模型只能在插值物理参数的动态中进行良好的泛化，无法超出训练集中的物理参数范围。另一种思路是将数据转换为神经网络可以学习的标准分布，然后在进行预测后恢复原始数据[[78](#bib.bib78)]。由于神经网络在处理多个分布时存在困难，这种方法旨在找到一个能够有效代表动态的单一分布。一个值得信赖且可靠的物理动态学习模型应该能够在保持高准确度的同时，推广到具有各种参数、外部力量或边界条件的系统。因此，进一步研究具有良好泛化性的物理指导深度学习是至关重要的。
- en: 8.2 Improving Robustness of Long-term Forecasting
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2 提升长期预测的鲁棒性
- en: Long-term forecasting of physical dynamics is a challenging task as it is prone
    to error accumulation and instability to perturbations in the input, which significantly
    affect the accuracy of neural networks over a long forecasting horizon. To address
    these issues, several training techniques have been proposed in recent years.
    One such technique involves adding noise to the input, which makes the models
    less sensitive to perturbations [[14](#bib.bib14)]. It also suggests when making
    predictions in an autoregressive manner, the neural nets should be trained to
    make multiple steps of predictions in each autoregressive call instead of just
    one step. Additionally, [[181](#bib.bib181)] proposed a time-based Lyapunov regularizer
    to the loss function of deep forecasters to avoid training error propagation and
    improve the trained long-term prediction. Moreover, [[119](#bib.bib119), [8](#bib.bib8)]
    utilized online normalization that is normalizing the current training sample
    using a running mean and standard deviation, which also increases the time horizon
    that the model can predict. These models are trained on a large amount of simulation
    data. However, for real-world problems, obtaining real-world data such as experimental
    data of jet flow can be expensive, which presents a significant challenge for
    improving the robustness of predictions on limited training data. In such cases,
    developing robust prediction models that can generalize well on limited training
    data is of great importance.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 长期预测物理动态是一项具有挑战性的任务，因为它容易出现误差累积和对输入扰动的不稳定，这显著影响神经网络在长期预测中的准确性。为了解决这些问题，近年来提出了几种训练技术。其中一种技术涉及向输入中添加噪声，这使得模型对扰动的敏感性降低[[14](#bib.bib14)]。它还建议在以自回归方式进行预测时，神经网络应被训练在每次自回归调用中进行多步预测，而不仅仅是一步。此外，[[181](#bib.bib181)]
    提出了一个基于时间的李雅普诺夫正则化器，加入到深度预测器的损失函数中，以避免训练误差传播，并提高长期预测的准确性。此外，[[119](#bib.bib119),
    [8](#bib.bib8)] 采用了在线归一化，即使用运行均值和标准差对当前训练样本进行归一化，这也增加了模型的预测时间范围。这些模型是在大量仿真数据上训练的。然而，对于现实世界的问题，获取如喷流实验数据等实际数据可能代价高昂，这对在有限训练数据上提高预测鲁棒性提出了重大挑战。在这种情况下，开发能够在有限训练数据上很好泛化的鲁棒预测模型至关重要。
- en: 8.3 Learning Dynamics in Non-Euclidean Spaces
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3 在非欧几里得空间中的学习动态
- en: Spatiotemporal phenomena, from global ocean currents to the spread of infectious
    diseases, are examples of dynamics in non-Euclidean spaces, which means they cannot
    be easily represented using traditional Euclidean geometry. To address this issue,
    the field of geometric deep learning [[16](#bib.bib16)] has emerged. Geometric
    deep learning aims to generalize neural network models to non-Euclidean domains
    such as graphs and manifolds. However, most of the existing work in this field
    has been limited to static graph data. Thus, learning dynamics in non-Euclidean
    Ssaces is a promising direction, and geometric concepts, such as rent notions
    of distance, curvature, and parallel transport, must be taken into account when
    designing models. For example, when modeling the ocean dynamics on the earth,
    which is a sphere, we need to encode the gauge equivariance [[32](#bib.bib32)]
    in the design of neural nets since there is no canonical coordinate system on
    a sphere.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 时空现象，如全球洋流和传染病传播，是非欧几里得空间中动态的例子，这意味着它们不能轻易地用传统的欧几里得几何表示。为了解决这个问题，几何深度学习[[16](#bib.bib16)]领域应运而生。几何深度学习旨在将神经网络模型推广到图和流形等非欧几里得领域。然而，目前在这一领域的大多数工作都局限于静态图数据。因此，在非欧几里得空间中学习动态是一条有前途的方向，设计模型时必须考虑几何概念，如距离、曲率和平行传输等。例如，在建模地球上的海洋动态时，因为地球是一个球体，我们需要在神经网络的设计中编码规范等变性[[32](#bib.bib32)]，因为球面上没有规范的坐标系统。
- en: 8.4 Theoretical Analysis
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4 理论分析
- en: The majority of literature on learning dynamics with DL focuses on the methodological
    and practical aspects. Research into the theoretical analysis of generalization
    is lacking. Current statistical learning theory is based on the typical assumption
    that training and test data are identically and independently distributed (i.i.d.)
    samples from some unknown distribution [[177](#bib.bib177), [114](#bib.bib114)].
    However, this assumption does not hold for most dynamical systems, where observations
    at different times and locations may be highly correlated. [[87](#bib.bib87)]
    provided the discrepancy-based generalization guarantees for time series forecasting.
    On the basis of this, [[150](#bib.bib150)] took the first step to derive generalization
    bounds for equivariant models and data augmentation in the dynamics forecasting
    setting. The derived upper bounds are expressed in terms of measures of distributional
    shifts and group transformations, as well as the Rademacher complexity. But these
    bounds are sometimes not very informative since many of the inequalities used
    can be loose. However, to better understand the performance of DL on learning
    dynamics, we need to derive generalization bounds expressed in terms of the characteristics
    of the dynamics, such as the order and Lyapunov exponents. Deriving lower generalization
    bounds are also necessary since they reveal the best performance scenarios. Theoretical
    studies can also inspire research into model design and algorithm development
    for learning dynamics.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 关于深度学习（DL）学习动态的文献大多数集中于方法论和实际应用方面。对泛化的理论分析的研究仍然不足。当前的统计学习理论基于一个典型的假设，即训练数据和测试数据是来自某个未知分布的相同且独立分布（i.i.d.）样本[[177](#bib.bib177),
    [114](#bib.bib114)]。然而，这个假设在大多数动态系统中并不成立，因为在不同时间和地点的观察值可能高度相关。[[87](#bib.bib87)]
    提供了基于差异的时间序列预测泛化保证。在此基础上，[[150](#bib.bib150)] 迈出了第一步，推导了在动态预测设置中，等变模型和数据增强的泛化界限。推导的上界以分布偏移和群体变换的度量以及
    Rademacher 复杂性为表达形式。然而，这些界限有时并不十分具有信息性，因为许多使用的不等式可能比较宽松。然而，为了更好地理解深度学习在学习动态中的表现，我们需要推导以动态特征（如阶数和
    Lyapunov 指数）为表达形式的泛化界限。推导下界也是必要的，因为它们揭示了最佳性能场景。理论研究还可以激发对学习动态的模型设计和算法开发的研究。
- en: 8.5 Causal Inference in Dynamical Systems
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5 动态系统中的因果推断
- en: A fundamental pursuit in science is to identify causal relationships. In the
    context of dynamical systems, we may ask which variables directly or indirectly
    influence other variables through intermediates. While traditional approaches
    to the discovery of causation involve conducting controlled real experiments [[118](#bib.bib118),
    [12](#bib.bib12)], data-driven approaches have been proposed to identify causal
    relations from observational data in the past few decades [[61](#bib.bib61), [59](#bib.bib59)].
    However, most data-driven approaches do not directly address the challenge of
    learning causality with big data. Many questions remain open, such as using causality
    to improve deep learning models, disentangling complex and multiple treatments,
    and designing the environment to control the given dynamics. Additionally, we
    are also interested in understanding the system’s response under interventions.
    For example, when using deep learning to model climate dynamics, we need to make
    accurate predictions under different climate policies, such as carbon pricing
    policies and the development of clean energy, to enable better decisions by governments
    for controlling climate change.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 科学中的一个基本追求是识别因果关系。在动态系统的背景下，我们可能会问哪些变量通过中介直接或间接影响其他变量。虽然传统的因果发现方法涉及进行受控的真实实验[[118](#bib.bib118),
    [12](#bib.bib12)]，但近年来提出了从观察数据中识别因果关系的数据驱动方法[[61](#bib.bib61), [59](#bib.bib59)]。然而，大多数数据驱动方法并没有直接解决大数据因果学习的挑战。许多问题仍未解决，例如如何利用因果关系改进深度学习模型、解开复杂和多重处理的关系，以及设计环境以控制给定的动态。此外，我们还希望了解系统在干预下的响应。例如，在使用深度学习建模气候动态时，我们需要在不同的气候政策下做出准确预测，如碳定价政策和清洁能源的发展，以帮助政府更好地控制气候变化。
- en: 8.6 Search for Physical Laws
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6 物理定律的探索
- en: Another promising direction is to seek physics laws with the help of DL. The
    search for fundamental laws of practical problems is the main theme of science.
    Once the governing equations of dynamical systems are found, they allow for accurate
    mathematical modeling, increased interpretability, and robust forecasting. However,
    current methods are limited to selecting from a large dictionary of possible mathematical
    terms [[126](#bib.bib126), [140](#bib.bib140), [19](#bib.bib19), [89](#bib.bib89),
    [128](#bib.bib128)]. The extremely large search space, limited high-quality experimental
    data, and overfitting issues have been critical concerns. Another line of work
    is to discover symmetry from the observed data instead of the entire dynamics
    with the help of DL [[40](#bib.bib40), [50](#bib.bib50)]. But these works can
    only work well on synthetic data and discover known symmetries. Still, research
    on data-driven methods based on DL for discovering physics laws is quite preliminary.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有前途的方向是借助深度学习寻找物理定律。寻找实际问题的基本定律是科学的主要主题。一旦发现了动态系统的控制方程，它们可以实现准确的数学建模、提高解释性和增强预测能力。然而，目前的方法仅限于从大量可能的数学术语中选择[[126](#bib.bib126),
    [140](#bib.bib140), [19](#bib.bib19), [89](#bib.bib89), [128](#bib.bib128)]。极大的搜索空间、有限的高质量实验数据和过拟合问题是关键关注点。另一项工作是从观察到的数据中发现对称性，而不是依赖于整个动态系统，这可以借助深度学习
    [[40](#bib.bib40), [50](#bib.bib50)]。但这些研究仅在合成数据上表现良好，并且只能发现已知的对称性。尽管如此，基于深度学习的物理定律发现数据驱动方法的研究仍处于初步阶段。
- en: 8.7 Efficient Computation
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.7 高效计算
- en: Given the rapid growth in high-performance computation, we need to improve automation
    and accelerate streamlining of highly compute-intensive workflows for science.
    We should focus on how to efficiently train, test, and deploy complex physics-guided
    DL models on large datasets and high-performance computing systems, such that
    these models can be quickly utilized to solve real-world scientific problems.
    To really revolutionize the field, these DL tools need to become more scalable
    and transferable and converge into a complete pipeline for the simulation and
    analysis of dynamical systems. A simple example is that we can integrate machine
    learning tools into the existing numerical simulation platforms so that we do
    not need to move data between systems every time and we can easily use either
    or both types of methods for analyzing data.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于高性能计算的快速增长，我们需要改善自动化并加速高计算密集型工作流的简化，以服务科学。我们应专注于如何高效地训练、测试和部署复杂的物理指导深度学习（DL）模型在大规模数据集和高性能计算系统上，以便这些模型能够迅速用于解决现实世界的科学问题。为了真正革新这一领域，这些深度学习工具需要变得更加可扩展和可转移，并汇聚成一个完整的动态系统仿真和分析管道。一个简单的例子是，我们可以将机器学习工具集成到现有的数值模拟平台中，以便每次不需要在系统之间移动数据，并且我们可以轻松地使用这两种方法之一或两者来分析数据。
- en: In conclusion, given the availability of abundant data and rapid growth in computation,
    we envision that the integration of physics and DL will play an increasingly essential
    role in advancing scientific discovery and addressing important dynamics modeling
    problems.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，鉴于丰富的数据和计算的快速增长，我们预见物理学和深度学习的结合将在推进科学发现和解决重要的动态建模问题中发挥越来越重要的作用。
- en: Acknowledgement
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: 'This work was supported in part by U.S. Department Of Energy, Office of Science
    under grant DESC0022331, U. S. Army Research Office under Grant W911NF-20-1-0334,
    Facebook Data Science Award, Google Faculty Award, and NSF Grant #2037745.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分得到美国能源部科学办公室（Grant DESC0022331）、美国陆军研究办公室（Grant W911NF-20-1-0334）、Facebook
    数据科学奖、Google 教授奖以及 NSF 资助（#2037745）的支持。
- en: References
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Alipanahi et al. [2015] B. Alipanahi, Andrew Delong, Matthew T. Weirauch, and
    B. Frey. Predicting the sequence specificities of dna- and rna-binding proteins
    by deep learning. *Nature Biotechnology*, 33:831–838, 2015.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alipanahi 等 [2015] B. Alipanahi、Andrew Delong、Matthew T. Weirauch 和 B. Frey。通过深度学习预测
    DNA 和 RNA 结合蛋白的序列特异性。*自然生物技术*，33:831–838，2015。
- en: Amodei et al. [2019] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano,
    John Schulman, and Dan Mané. Concrete problems in ai safety. *arXiv preprint arXiv:1606.06565*,
    2019.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amodei 等 [2019] Dario Amodei、Chris Olah、Jacob Steinhardt、Paul Christiano、John
    Schulman 和 Dan Mané。人工智能安全中的具体问题。*arXiv 预印本 arXiv:1606.06565*，2019。
- en: 'Anderson et al. [2019] Brandon Anderson, Truong-Son Hy, and Risi Kondor. Cormorant:
    Covariant molecular neural networks. In *Advances in neural information processing
    systems (NeurIPS)*, 2019.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anderson 等 [2019] Brandon Anderson、Truong-Son Hy 和 Risi Kondor。Cormorant：协变分子神经网络。在
    *神经信息处理系统进展（NeurIPS）*，2019。
- en: Ayed et al. [2019] Ibrahim Ayed, Emmanuel De Bézenac, Arthur Pajot, and Patrick
    Gallinari. Learning partially observed PDE dynamics with neural networks, 2019.
    URL [https://openreview.net/forum?id=HyefgnCqFm](https://openreview.net/forum?id=HyefgnCqFm).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ayed et al. [2019] Ibrahim Ayed, Emmanuel De Bézenac, Arthur Pajot, 和 Patrick
    Gallinari。使用神经网络学习部分观测的 PDE 动力学，2019。网址 [https://openreview.net/forum?id=HyefgnCqFm](https://openreview.net/forum?id=HyefgnCqFm)。
- en: Azencot et al. [2020] Omri Azencot, N. Erichson, Vanessa Lin, and Michael W.
    Mahoney. Forecasting sequential data using consistent koopman autoencoders. *arXiv
    Preprint arXiv:2003.02236*, 2020.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Azencot et al. [2020] Omri Azencot, N. Erichson, Vanessa Lin, 和 Michael W. Mahoney。使用一致的
    Koopman 自编码器预测序列数据。*arXiv 预印本 arXiv:2003.02236*，2020。
- en: Bajaj et al. [2021] Chandrajit Bajaj, Luke McLennan, Timothy Andeen, and Avik
    Roy. Robust learning of physics informed neural networks. *arXiv preprint arXiv:2110.13330*,
    2021.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bajaj et al. [2021] Chandrajit Bajaj, Luke McLennan, Timothy Andeen, 和 Avik
    Roy。物理信息神经网络的鲁棒学习。*arXiv 预印本 arXiv:2110.13330*，2021。
- en: Bao and Song [2019] Erkao Bao and Linqi Song. Equivariant neural networks and
    equivarification. *arXiv preprint arXiv:1906.07172*, 2019.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bao and Song [2019] Erkao Bao 和 Linqi Song。等变神经网络和等变化。*arXiv 预印本 arXiv:1906.07172*，2019。
- en: Bartoldson et al. [2021] Brian R. Bartoldson, Rui Wang, Brenda M. Ng, Phuoc
    Chanh N Nguyen, Jose E. Cadena Pico, Phan Nguyen, David P. Widemann, and USDOE
    National Nuclear Security Administration. Meshgraphnets, 2021. URL [https://www.osti.gov//servlets/purl/1834708](https://www.osti.gov//servlets/purl/1834708).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bartoldson et al. [2021] Brian R. Bartoldson, Rui Wang, Brenda M. Ng, Phuoc
    Chanh N Nguyen, Jose E. Cadena Pico, Phan Nguyen, David P. Widemann, 和 USDOE 国家核安全管理局。Meshgraphnets，2021。网址
    [https://www.osti.gov//servlets/purl/1834708](https://www.osti.gov//servlets/purl/1834708)。
- en: 'Beucler et al. [2019a] Tom Beucler, M. Pritchard, S. Rasp, J. Ott, P. Baldi,
    and P. Gentine. Enforcing analytic constraints in neural-networks emulating physical
    systems. *arXiv: Computational Physics*, 2019a.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Beucler et al. [2019a] Tom Beucler, M. Pritchard, S. Rasp, J. Ott, P. Baldi,
    和 P. Gentine。强制在模仿物理系统的神经网络中应用解析约束。*arXiv: 计算物理*，2019a。'
- en: Beucler et al. [2019b] Tom Beucler, Michael Pritchard, Stephan Rasp, Pierre
    Gentine, Jordan Ott, and Pierre Baldi. Enforcing analytic constraints in neural-networks
    emulating physical systems. *arXiv preprint arXiv:1909.00912*, 2019b.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beucler et al. [2019b] Tom Beucler, Michael Pritchard, Stephan Rasp, Pierre
    Gentine, Jordan Ott, 和 Pierre Baldi。强制在模仿物理系统的神经网络中应用解析约束。*arXiv 预印本 arXiv:1909.00912*，2019b。
- en: Beucler et al. [2019c] Tom Beucler, S. Rasp, M. Pritchard, and P. Gentine. Achieving
    conservation of energy in neural network emulators for climate modeling. *ArXiv*,
    abs/1906.06622, 2019c.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beucler et al. [2019c] Tom Beucler, S. Rasp, M. Pritchard, 和 P. Gentine。在气候建模的神经网络模拟器中实现能量守恒。*ArXiv*，abs/1906.06622，2019c。
- en: 'Bollt et al. [2018] E. Bollt, J. Sun, and J. Runge. Introduction to focus issue:
    Causation inference and information flow in dynamical systems: Theory and applications.
    *Chaos*, 2018.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bollt et al. [2018] E. Bollt, J. Sun, 和 J. Runge。焦点问题介绍：动态系统中的因果推断和信息流：理论与应用。*混沌*，2018。
- en: Brandstetter et al. [2022a] Johannes Brandstetter, Max Welling, and Daniel E
    Worrall. Lie point symmetry data augmentation for neural pde solvers. In *International
    Conference on Machine Learning*, pages 2241–2256\. PMLR, 2022a.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandstetter et al. [2022a] Johannes Brandstetter, Max Welling, 和 Daniel E Worrall。用于神经
    PDE 求解器的李点对称数据增强。在 *国际机器学习大会*，页码 2241–2256。PMLR，2022a。
- en: Brandstetter et al. [2022b] Johannes Brandstetter, Daniel E. Worrall, and Max
    Welling. Message passing neural PDE solvers. In *International Conference on Learning
    Representations*, 2022b. URL [https://openreview.net/forum?id=vSix3HPYKSU](https://openreview.net/forum?id=vSix3HPYKSU).
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brandstetter et al. [2022b] Johannes Brandstetter, Daniel E. Worrall, 和 Max
    Welling。信息传递神经 PDE 求解器。在 *国际学习表征大会*，2022b。网址 [https://openreview.net/forum?id=vSix3HPYKSU](https://openreview.net/forum?id=vSix3HPYKSU)。
- en: Brence et al. [2021] Jure Brence, Ljupčo Todorovski, and Sašo Džeroski. Probabilistic
    grammars for equation discovery. *Knowledge-Based Systems*, 224:107077, 2021.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brence et al. [2021] Jure Brence, Ljupčo Todorovski, 和 Sašo Džeroski。方程发现的概率语法。*知识基础系统*，224:107077，2021。
- en: 'Bronstein et al. [2021] Michael M Bronstein, Joan Bruna, Taco Cohen, and Petar
    Veličković. Geometric deep learning: Grids, groups, graphs, geodesics, and gauges.
    *arXiv preprint arXiv:2104.13478*, 2021.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bronstein et al. [2021] Michael M Bronstein, Joan Bruna, Taco Cohen, 和 Petar
    Veličković。几何深度学习：网格、群体、图形、测地线和度规。*arXiv 预印本 arXiv:2104.13478*，2021。
- en: Brunton et al. [2019] S. Brunton, B. R. Noack, and P. Koumoutsakos. Machine
    learning for fluid mechanics. *ArXiv*, abs/1905.11075, 2019.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brunton et al. [2019] S. Brunton, B. R. Noack, 和 P. Koumoutsakos。流体力学的机器学习。*ArXiv*，abs/1905.11075，2019。
- en: Brunton [2021] Steven L. Brunton. Applying machine learning to study fluid mechanics.
    *arXiv preprint arXiv:2110.02083*, 2021.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brunton [2021] Steven L. Brunton。应用机器学习研究流体力学。*arXiv 预印本 arXiv:2110.02083*，2021年。
- en: 'Brunton et al. [2015] Steven L. Brunton, Joshua L. Proctor, and J. Nathan Kutz.
    Discovering governing equations from data: Sparse identification of nonlinear
    dynamical systems. *arXiv preprint arXiv:1509.03580*, 2015.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brunton 等人 [2015] Steven L. Brunton, Joshua L. Proctor, 和 J. Nathan Kutz。 从数据中发现主方程：非线性动力系统的稀疏识别。*arXiv
    预印本 arXiv:1509.03580*，2015年。
- en: Brunton et al. [2016] Steven L Brunton, Joshua L Proctor, and J Nathan Kutz.
    Discovering governing equations from data by sparse identification of nonlinear
    dynamical systems. *Proceedings of the national academy of sciences*, 113(15):3932–3937,
    2016.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brunton 等人 [2016] Steven L Brunton, Joshua L Proctor, 和 J Nathan Kutz。通过稀疏识别非线性动力系统从数据中发现主方程。*Proceedings
    of the National Academy of Sciences*，113(15):3932–3937，2016年。
- en: 'Cai et al. [2021] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, and
    George Em Karniadakis. Physics-informed neural networks (pinns) for fluid mechanics:
    A review. *Acta Mechanica Sinica*, 37(12):1727–1738, 2021.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cai 等人 [2021] Shengze Cai, Zhiping Mao, Zhicheng Wang, Minglang Yin, 和 George
    Em Karniadakis。用于流体力学的物理信息神经网络（PINNs）：综述。*Acta Mechanica Sinica*，37(12):1727–1738，2021年。
- en: 'Cang et al. [2017] Ruijin Cang, Hechao Li, Hope Yao, Yang Jiao, and Yi Ren.
    Improving direct physical properties prediction of heterogeneous materials from
    imaging data via convolutional neural network and a morphology-aware generative
    model. *arXiv: Computational Physics*, 2017.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cang 等人 [2017] Ruijin Cang, Hechao Li, Hope Yao, Yang Jiao, 和 Yi Ren。通过卷积神经网络和形态感知生成模型改善异质材料的直接物理属性预测。*arXiv:
    Computational Physics*，2017年。'
- en: 'Carderera et al. [2021] Alejandro Carderera, Sebastian Pokutta, Christof Schütte,
    and Martin Weiser. Cindy: Conditional gradient-based identification of non-linear
    dynamics – noise-robust recovery. *arXiv preprint arXiv:2101.02630*, 2021.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carderera 等人 [2021] Alejandro Carderera, Sebastian Pokutta, Christof Schütte,
    和 Martin Weiser。Cindy：基于条件梯度的非线性动力学识别——噪声鲁棒恢复。*arXiv 预印本 arXiv:2101.02630*，2021年。
- en: Carleo and Troyer [2017] G. Carleo and M. Troyer. Solving the quantum many-body
    problem with artificial neural networks. *Science*, 355:602 – 606, 2017.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carleo 和 Troyer [2017] G. Carleo 和 M. Troyer。用人工神经网络解决量子多体问题。*Science*，355:602
    – 606，2017年。
- en: 'Chaoua [2017] Bruno Chaoua. The state of the art of hybrid rans/les modeling
    for the simulation of turbulent flows. *Springer Netherlands*, 99:279–327, 2017.
    doi: https://doi.org/10.1007/s10494-017-9828-8.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chaoua [2017] Bruno Chaoua。混合 RANS/LES 模型在湍流流动模拟中的前沿技术。*Springer Netherlands*，99:279–327，2017年。doi:
    https://doi.org/10.1007/s10494-017-9828-8。'
- en: Chen et al. [2018a] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, and
    David Duvenaud. Neural ordinary differential equations. In S. Bengio, H. Wallach,
    H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, *Advances
    in Neural Information Processing Systems 31*, pages 6571–6583\. Curran Associates,
    Inc., 2018a. URL [http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf](http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2018a] Ricky T. Q. Chen, Yulia Rubanova, Jesse Bettencourt, 和 David
    Duvenaud。神经普通微分方程。在 S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi,
    和 R. Garnett 主编的 *Advances in Neural Information Processing Systems 31* 中，第6571–6583页。Curran
    Associates, Inc.，2018a。网址 [http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf](http://papers.nips.cc/paper/7892-neural-ordinary-differential-equations.pdf)。
- en: Chen et al. [2018b] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, and David K
    Duvenaud. Neural ordinary differential equations. In *Advances in neural information
    processing systems*, pages 6571–6583, 2018b.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2018b] Tian Qi Chen, Yulia Rubanova, Jesse Bettencourt, 和 David K Duvenaud。神经普通微分方程。在
    *Advances in Neural Information Processing Systems* 中，第6571–6583页，2018b年。
- en: Chen et al. [2021] Zhao Chen, Yang Liu, and Hao Sun. Physics-informed learning
    of governing equations from scarce data. *Nature communications*, 12(1):6136,
    2021.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2021] Zhao Chen, Yang Liu, 和 Hao Sun。 从稀缺数据中学习主方程的物理信息。*Nature communications*，12(1):6136，2021年。
- en: Chidester et al. [2018] Benjamin Chidester, Minh N. Do, and Jian Ma. Rotation
    equivariance and invariance in convolutional neural networks. *arXiv preprint
    arXiv:1805.12301*, 2018.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chidester 等人 [2018] Benjamin Chidester, Minh N. Do, 和 Jian Ma。卷积神经网络中的旋转等变性和不变性。*arXiv
    预印本 arXiv:1805.12301*，2018年。
- en: Cohen and Welling [2016a] Taco S. Cohen and Max Welling. Group equivariant convolutional
    networks. In *International conference on machine learning (ICML)*, pages 2990–2999,
    2016a.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen 和 Welling [2016a] Taco S. Cohen 和 Max Welling。群体等变卷积网络。在 *国际机器学习会议（ICML）*，第2990–2999页，2016a年。
- en: Cohen and Welling [2016b] Taco S. Cohen and Max Welling. Steerable CNNs. *arXiv
    preprint arXiv:1612.08498*, 2016b.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen 和 Welling [2016b] Taco S. Cohen 和 Max Welling。可调 steerable CNNs。*arXiv
    预印本 arXiv:1612.08498*，2016b。
- en: Cohen et al. [2019] Taco S. Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max
    Welling. Gauge equivariant convolutional networks and the icosahedral CNN. In
    *Proceedings of the 36th International Conference on Machine Learning (ICML)*,
    volume 97, pages 1321–1330, 2019.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cohen等人 [2019] Taco S. Cohen、Maurice Weiler、Berkay Kicanaoglu 和 Max Welling。规范等变卷积网络和二十面体CNN。发表于*第36届国际机器学习会议
    (ICML)*，第97卷，第1321–1330页，2019年。
- en: Cranmer et al. [2020] M. Cranmer, S. Greydanus, S. Hoyer, P. Battaglia, D. Spergel,
    and S. Ho. Lagrangian neural networks. *ArXiv*, abs/2003.04630, 2020.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cranmer等人 [2020] M. Cranmer、S. Greydanus、S. Hoyer、P. Battaglia、D. Spergel 和
    S. Ho。拉格朗日神经网络。*ArXiv*，abs/2003.04630，2020年。
- en: 'Cuomo et al. [2022] Salvatore Cuomo, Vincenzo Schiano Di Cola, Fabio Giampaolo,
    Gianluigi Rozza, Maziar Raissi, and Francesco Piccialli. Scientific machine learning
    through physics–informed neural networks: where we are and what’s next. *Journal
    of Scientific Computing*, 92(3):88, 2022.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cuomo等人 [2022] Salvatore Cuomo、Vincenzo Schiano Di Cola、Fabio Giampaolo、Gianluigi
    Rozza、Maziar Raissi 和 Francesco Piccialli。通过物理信息神经网络的科学机器学习：我们现在的位置以及未来的发展。*科学计算期刊*，92(3):88，2022年。
- en: Daw et al. [2020] Arka Daw, R. Thomas, C. Carey, J. Read, A. Appling, and A. Karpatne.
    Physics-guided architecture (pga) of neural networks for quantifying uncertainty
    in lake temperature modeling. *ArXiv*, abs/1911.02682, 2020.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Daw等人 [2020] Arka Daw、R. Thomas、C. Carey、J. Read、A. Appling 和 A. Karpatne。物理引导的神经网络架构
    (PGA) 用于量化湖泊温度建模中的不确定性。*ArXiv*，abs/1911.02682，2020年。
- en: 'Day [1994] Richard H. Day. Complex economic dynamics-vol. 1: An introduction
    to dynamical systems and market mechanisms. *MIT Press Books*, 1, 1994.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Day [1994] Richard H. Day。复杂经济动态-vol. 1：动态系统和市场机制导论。*MIT出版社*，1，1994年。
- en: de Avila Belbute-Peres et al. [2020] Filipe de Avila Belbute-Peres, Thomas D.
    Economon, and J. Z. Kolter. Combining differentiable pde solvers and graph neural
    networks for fluid flow prediction. *ArXiv*, abs/2007.04439, 2020.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Avila Belbute-Peres等人 [2020] Filipe de Avila Belbute-Peres、Thomas D. Economon
    和 J. Z. Kolter。结合可微分PDE求解器和图神经网络用于流体流动预测。*ArXiv*，abs/2007.04439，2020年。
- en: 'de Bezenac et al. [2018a] Emmanuel de Bezenac, Arthur Pajot, and Patrick Gallinari.
    Deep learning for physical processes: Incorporating prior scientific knowledge.
    In *International Conference on Learning Representations*, 2018a. URL [https://openreview.net/forum?id=By4HsfWAZ](https://openreview.net/forum?id=By4HsfWAZ).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Bezenac等人 [2018a] Emmanuel de Bezenac、Arthur Pajot 和 Patrick Gallinari。物理过程的深度学习：融入先验科学知识。发表于*国际学习表示会议*，2018a。网址
    [https://openreview.net/forum?id=By4HsfWAZ](https://openreview.net/forum?id=By4HsfWAZ)。
- en: 'de Bezenac et al. [2018b] Emmanuel de Bezenac, Arthur Pajot, and Patrick Gallinari.
    Deep learning for physical processes: Incorporating prior scientific knowledge.
    In *International Conference on Learning Representations*, 2018b. URL [https://openreview.net/forum?id=By4HsfWAZ](https://openreview.net/forum?id=By4HsfWAZ).'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Bezenac等人 [2018b] Emmanuel de Bezenac、Arthur Pajot 和 Patrick Gallinari。物理过程的深度学习：融入先验科学知识。发表于*国际学习表示会议*，2018b。网址
    [https://openreview.net/forum?id=By4HsfWAZ](https://openreview.net/forum?id=By4HsfWAZ)。
- en: Dehmamy et al. [2021] Nima Dehmamy, Robin Walters, Yanchen Liu, Dashun Wang,
    and Rose Yu. Automatic symmetry discovery with lie algebra convolutional network.
    *Advances in Neural Information Processing Systems*, 34, 2021.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dehmamy等人 [2021] Nima Dehmamy、Robin Walters、Yanchen Liu、Dashun Wang 和 Rose Yu。利用李代数卷积网络的自动对称发现。*神经信息处理系统进展*，34，2021年。
- en: Dieleman et al. [2016] Sander Dieleman, Jeffrey De Fauw, and Koray Kavukcuoglu.
    Exploiting cyclic symmetry in convolutional neural networks. In *International
    Conference on Machine Learning (ICML)*, 2016.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dieleman等人 [2016] Sander Dieleman、Jeffrey De Fauw 和 Koray Kavukcuoglu。利用卷积神经网络中的周期对称性。发表于*国际机器学习会议
    (ICML)*，2016年。
- en: Donà et al. [2021] Jérémie Donà, Jean-Yves Franceschi, sylvain lamprier, and
    patrick gallinari. {PDE}-driven spatiotemporal disentanglement. In *International
    Conference on Learning Representations*, 2021. URL [https://openreview.net/forum?id=vLaHRtHvfFp](https://openreview.net/forum?id=vLaHRtHvfFp).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Donà等人 [2021] Jérémie Donà、Jean-Yves Franceschi、Sylvain Lamprier 和 Patrick Gallinari。{PDE}-驱动的时空解耦。发表于*国际学习表示会议*，2021年。网址
    [https://openreview.net/forum?id=vLaHRtHvfFp](https://openreview.net/forum?id=vLaHRtHvfFp)。
- en: Dupont et al. [2019] E. Dupont, A. Doucet, and Y. Teh. Augmented neural odes.
    In *NeurIPS*, 2019.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dupont等人 [2019] E. Dupont、A. Doucet 和 Y. Teh。增强神经常微分方程。发表于*NeurIPS*，2019年。
- en: 'Dzeroski and Todorovski [2007] Saso Dzeroski and Ljupco Todorovski. *Computational
    discovery of scientific knowledge: introduction, techniques, and applications
    in environmental and life sciences*, volume 4660. Springer, 2007.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dzeroski 和 Todorovski [2007] Saso Dzeroski 和 Ljupco Todorovski. *计算科学知识的发现：介绍、技术及在环境和生命科学中的应用*，第4660卷。Springer，2007年。
- en: Džeroski et al. [2007] Sašo Džeroski, Pat Langley, and Ljupčo Todorovski. *Computational
    discovery of scientific knowledge*. Springer, 2007.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Džeroski et al. [2007] Sašo Džeroski, Pat Langley, 和 Ljupčo Todorovski. *计算科学知识的发现*。Springer，2007年。
- en: E et al. [2019] Weinan E, Jiequn Han, and Linfeng Zhang. Integrating machine
    learning with physics-based modeling. *arXiv:2006.02619*, 2019.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: E et al. [2019] Weinan E, Jiequn Han, 和 Linfeng Zhang. 将机器学习与基于物理的建模相结合。*arXiv:2006.02619*，2019年。
- en: E.Labourasse and P.Sagaut [2002] E.Labourasse and P.Sagaut. Reconstruction of
    turbulent fluctuations using a hybrid rans-les approach. *Journal of Computational
    Physics*, 182:301–336, 2002.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: E.Labourasse 和 P.Sagaut [2002] E.Labourasse 和 P.Sagaut. 使用混合 rans-les 方法重建湍流波动。*Journal
    of Computational Physics*，182:301–336，2002年。
- en: Erichson et al. [2019] N. Erichson, Michael Muehlebach, and Michael W. Mahoney.
    Physics-informed autoencoders for lyapunov-stable fluid flow prediction. *arXiv
    preprint arXiv:1905.10866*, 2019.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Erichson et al. [2019] N. Erichson, Michael Muehlebach, 和 Michael W. Mahoney.
    物理信息自编码器用于Lyapunov稳定流体流预测。*arXiv preprint arXiv:1905.10866*，2019年。
- en: Fang et al. [2018] Rui Fang, David Sondak, Pavlos Protopapas, and Sauro Succi.
    Deep learning for turbulent channel flow. *arXiv preprint arXiv:1812.02241*, 2018.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fang et al. [2018] Rui Fang, David Sondak, Pavlos Protopapas, 和 Sauro Succi.
    用于湍流通道流的深度学习。*arXiv preprint arXiv:1812.02241*，2018年。
- en: Finn et al. [2017] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *International conference
    on machine learning*, pages 1126–1135\. PMLR, 2017.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn et al. [2017] Chelsea Finn, Pieter Abbeel, 和 Sergey Levine. 模型无关的元学习用于深度网络的快速适应。*国际机器学习会议*，第1126–1135页。PMLR，2017年。
- en: Finzi et al. [2020a] Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon
    Wilson. Generalizing convolutional neural networks for equivariance to lie groups
    on arbitrary continuous data. *arXiv preprint arXiv:2002.12880*, 2020a.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finzi et al. [2020a] Marc Finzi, Samuel Stanton, Pavel Izmailov, 和 Andrew Gordon
    Wilson. 为任意连续数据上的 Lie 群进行等变性的一般化卷积神经网络。*arXiv preprint arXiv:2002.12880*，2020年。
- en: Finzi et al. [2020b] Marc Finzi, K. Wang, and A. Wilson. Simplifying hamiltonian
    and lagrangian neural networks via explicit constraints. *ArXiv*, abs/2010.13581,
    2020b.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finzi et al. [2020b] Marc Finzi, K. Wang, 和 A. Wilson. 通过显式约束简化哈密顿和拉格朗日神经网络。*ArXiv*，abs/2010.13581，2020年。
- en: Finzi et al. [2021] Marc Finzi, Gregory Benton, and Andrew G Wilson. Residual
    pathway priors for soft equivariance constraints. *Advances in Neural Information
    Processing Systems*, 34, 2021.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finzi et al. [2021] Marc Finzi, Gregory Benton, 和 Andrew G Wilson. 用于软等变约束的残差路径先验。*Advances
    in Neural Information Processing Systems*，34，2021年。
- en: 'Forssell and Lindskog [1997] U. Forssell and P. Lindskog. Combining semi-physical
    and neural network modeling: An example of its usefulness u. forssell and p. lindskog.
    1997.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Forssell 和 Lindskog [1997] U. Forssell 和 P. Lindskog. 结合半物理和神经网络建模：其有效性的一个例子。u.
    forssell 和 p. lindskog. 1997年。
- en: 'Gao et al. [2020] Han Gao, Luning Sun, and J. Wang. Super-resolution and denoising
    of fluid flow using physics-informed convolutional neural networks without high-resolution
    labels. *arXiv: Fluid Dynamics*, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Gao et al. [2020] Han Gao, Luning Sun, 和 J. Wang. 使用物理信息卷积神经网络对流体流动进行超分辨率和去噪，而无需高分辨率标签。*arXiv:
    Fluid Dynamics*，2020年。'
- en: Ghosh and Gupta [2019] Rohan Ghosh and Anupam K. Gupta. Scale steerable filters
    for locally scale-invariant convolutional neural networks. *arXiv preprint arXiv:1906.03861*,
    2019.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ghosh 和 Gupta [2019] Rohan Ghosh 和 Anupam K. Gupta. 局部尺度不变卷积神经网络的尺度可操控滤波器。*arXiv
    preprint arXiv:1906.03861*，2019年。
- en: Gnaneshwar et al. [2022] Dwaraknath Gnaneshwar, Bharath Ramsundar, Dhairya Gandhi,
    Rachel Kurchin, and Venkatasubramanian Viswanathan. Score-based generative models
    for molecule generation. In *International Conference on Machine Learning*, 2022.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gnaneshwar et al. [2022] Dwaraknath Gnaneshwar, Bharath Ramsundar, Dhairya Gandhi,
    Rachel Kurchin, 和 Venkatasubramanian Viswanathan. 基于分数的生成模型用于分子生成。*国际机器学习会议*，2022年。
- en: Greydanus et al. [2019] S. Greydanus, Misko Dzamba, and J. Yosinski. Hamiltonian
    neural networks. *ArXiv*, abs/1906.01563, 2019.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Greydanus et al. [2019] S. Greydanus, Misko Dzamba, 和 J. Yosinski. 哈密顿神经网络。*ArXiv*，abs/1906.01563，2019年。
- en: Guo et al. [2020] Ruocheng Guo, Lu Cheng, Jundong Li, P. R. Hahn, and Huan Liu.
    A survey of learning causality with data. *ACM Computing Surveys (CSUR)*, 53:1
    – 37, 2020.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo et al. [2020] Ruocheng Guo, Lu Cheng, Jundong Li, P. R. Hahn, 和 Huan Liu.
    数据驱动的因果学习调查。*ACM Computing Surveys (CSUR)*，53:1 – 37，2020年。
- en: Han et al. [2019] Jiequn Han, Linfeng Zhang, and E. Weinan. Solving many-electron
    schrödinger equation using deep neural networks. *J. Comput. Phys.*, 399, 2019.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Han et al. [2019] Jiequn Han, Linfeng Zhang, 和 E. Weinan. 使用深度神经网络求解多电子薛定谔方程。*计算物理学杂志*，399，2019年。
- en: Harradon et al. [2018] Michael Harradon, Jeff Druce, and Brian E. Ruttenberg.
    Causal learning and explanation of deep neural networks via autoencoded activations.
    *ArXiv*, abs/1802.00541, 2018.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Harradon et al. [2018] Michael Harradon, Jeff Druce, 和 Brian E. Ruttenberg.
    通过自编码激活进行深度神经网络的因果学习和解释。*ArXiv*，abs/1802.00541，2018年。
- en: Hinton et al. [2011] Geoffrey E Hinton, Alex Krizhevsky, and Sida D Wang. Transforming
    auto-encoders. In *International conference on artificial neural networks*, pages
    44–51\. Springer, 2011.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton et al. [2011] Geoffrey E Hinton, Alex Krizhevsky, 和 Sida D Wang. 转换自编码器。发表于*国际人工神经网络会议*，第44–51页。施普林格出版社，2011年。
- en: Hochreiter and Schmidhuber [1997] Sepp Hochreiter and Jürgen Schmidhuber. Long
    short-term memory. *Neural computation*, 9(8):1735–1780, 1997.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hochreiter 和 Schmidhuber [1997] Sepp Hochreiter 和 Jürgen Schmidhuber. 长短期记忆。*神经计算*，9(8):1735–1780，1997年。
- en: Holl et al. [2020] Philipp Holl, Nils Thuerey, and Vladlen Koltun. Learning
    to control pdes with differentiable physics. In *International Conference on Learning
    Representations*, 2020. URL [https://openreview.net/forum?id=HyeSin4FPB](https://openreview.net/forum?id=HyeSin4FPB).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Holl et al. [2020] Philipp Holl, Nils Thuerey, 和 Vladlen Koltun. 学习用可微分物理控制pde。发表于*国际学习表征会议*，2020年。URL
    [https://openreview.net/forum?id=HyeSin4FPB](https://openreview.net/forum?id=HyeSin4FPB)。
- en: Hoogeboom et al. [2022] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac,
    and Max Welling. Equivariant diffusion for molecule generation in 3d. In *International
    Conference on Machine Learning*, pages 8867–8887\. PMLR, 2022.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoogeboom et al. [2022] Emiel Hoogeboom, Victor Garcia Satorras, Clement Vignac,
    和 Max Welling. 3D分子生成的对称扩散。发表于*国际机器学习会议*，第8867–8887页。PMLR，2022年。
- en: Houska et al. [2012] B. Houska, F. Logist, M. Diehl, and J. Van Impe. A tutorial
    on numerical methods for state and parameter estimation in nonlinear dynamic systems.
    In D. Alberer, H. Hjalmarsson, and L. Del Re, editors, *Identification for Automotive
    Systems, Volume 418, Lecture Notes in Control and Information Sciences*, page
    67–88\. Springer, 2012.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houska et al. [2012] B. Houska, F. Logist, M. Diehl, 和 J. Van Impe. 非线性动态系统中状态和参数估计的数值方法教程。发表于D.
    Alberer, H. Hjalmarsson, 和 L. Del Re主编的*汽车系统识别，第418卷，控制与信息科学讲义*，第67–88页。施普林格出版社，2012年。
- en: 'Hughes [2012] Thomas JR Hughes. *The finite element method: linear static and
    dynamic finite element analysis*. Courier Corporation, 2012.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hughes [2012] Thomas JR Hughes. *有限元法：线性静态和动态有限元分析*。库里尔公司，2012年。
- en: Iserles [2009] Arieh Iserles. *A first course in the numerical analysis of differential
    equations*. Number 44\. Cambridge university press, 2009.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Iserles [2009] Arieh Iserles. *微分方程的数值分析入门*。第44号。剑桥大学出版社，2009年。
- en: Izhikevich [2007] Eugene M. Izhikevich. *Dynamical systems in neuroscience*.
    MIT press, 2007.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Izhikevich [2007] Eugene M. Izhikevich. *神经科学中的动态系统*。麻省理工学院出版社，2007年。
- en: J.C.Butcher [1996] J.C.Butcher. *Applied Numerical Mathematics*, volume 20.
    Elsevier B.V., 1996.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J.C.Butcher [1996] J.C.Butcher. *应用数值数学*，第20卷。爱思唯尔B.V.，1996年。
- en: 'Jia et al. [2019] Xiaowei Jia, Jared Willard, Anuj Karpatne, Jordan Read, Jacob
    Zwart, Michael Steinbach, and Vipin Kumar. Physics guided rnns for modeling dynamical
    systems: A case study in simulating lake temperature profiles. In *Proceedings
    of the 2019 SIAM International Conference on Data Mining*, pages 558–566\. SIAM,
    2019.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jia et al. [2019] Xiaowei Jia, Jared Willard, Anuj Karpatne, Jordan Read, Jacob
    Zwart, Michael Steinbach, 和 Vipin Kumar. 物理引导的rnn用于建模动态系统：湖泊温度分布模拟案例研究。发表于*2019年SIAM国际数据挖掘会议论文集*，第558–566页。SIAM，2019年。
- en: Jiang et al. [2020] Chiyu ”Max” Jiang, Karthik Kashinath, Prabhat, and Philip
    Marcus. Enforcing physical constraints in {cnn}s through differentiable {pde}
    layer. In *ICLR 2020 Workshop on Integration of Deep Neural Models and Differential
    Equations*, 2020. URL [https://openreview.net/forum?id=q2noHUqMkK](https://openreview.net/forum?id=q2noHUqMkK).
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jiang et al. [2020] Chiyu "Max" Jiang, Karthik Kashinath, Prabhat, 和 Philip
    Marcus. 通过可微分{pde}层强制{cnn}中的物理约束。发表于*ICLR 2020深度神经模型与微分方程集成研讨会*，2020年。URL [https://openreview.net/forum?id=q2noHUqMkK](https://openreview.net/forum?id=q2noHUqMkK)。
- en: Kaiser et al. [2018] Eurika Kaiser, J Nathan Kutz, and Steven L Brunton. Sparse
    identification of nonlinear dynamics for model predictive control in the low-data
    limit. *Proceedings of the Royal Society A*, 474(2219):20180335, 2018.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kaiser et al. [2018] Eurika Kaiser, J Nathan Kutz, 和 Steven L Brunton. 在低数据限制下的模型预测控制的非线性动态稀疏识别。*皇家学会A卷会议录*，474(2219):20180335，2018年。
- en: 'Kani and Elsheikh [2017] J. Kani and A. H. Elsheikh. Dr-rnn: A deep residual
    recurrent neural network for model reduction. *ArXiv*, abs/1709.00939, 2017.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kani 和 Elsheikh [2017] J. Kani 和 A. H. Elsheikh. DR-RNN：一种用于模型降维的深度残差递归神经网络。*ArXiv*，abs/1709.00939，2017年。
- en: 'Karpatne et al. [2017] Anuj Karpatne, William Watkins, Jordan Read, and Vipin
    Kumar. Physics-guided neural networks (pgnn): An application in lake temperature
    modeling. *arXiv Preprint arXiv:1710.11431*, 2017.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karpatne 等 [2017] Anuj Karpatne, William Watkins, Jordan Read 和 Vipin Kumar.
    物理引导神经网络（pgnn）：在湖泊温度建模中的应用。*arXiv 预印本 arXiv:1710.11431*，2017年。
- en: 'Kashinath et al. [2020] K. Kashinath, M. Mustafa, J-L. Wu A. Albert, C. Jiang,
    K. Azizzadenesheli S. Esmaeilzadeh, R. Wang, A. Chattopadhyay, A. Singh, A. Manepalli,
    D. Chirila, R. Yu, R. Walters, B. White, H. Xiao, H. A. Tchelepi, P. Marcus, A. Anandkumar,
    and P. Hassanzadeh. Physics-informed machine learning: case studies for weather
    and climate modelling. *Journal of Philosophical Transactions of the Royal Society
    A*, 2020.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kashinath 等 [2020] K. Kashinath, M. Mustafa, J-L. Wu A. Albert, C. Jiang, K.
    Azizzadenesheli S. Esmaeilzadeh, R. Wang, A. Chattopadhyay, A. Singh, A. Manepalli,
    D. Chirila, R. Yu, R. Walters, B. White, H. Xiao, H. A. Tchelepi, P. Marcus, A.
    Anandkumar 和 P. Hassanzadeh. 物理信息机器学习：天气和气候建模的案例研究。*皇家学会A卷哲学交易期刊*，2020年。
- en: 'Kim et al. [2019] B. Kim, V. C. Azevedo, N. Thürey, Theodore Kim, M. Gross,
    and B. Solenthaler. Deep fluids: A generative network for parameterized fluid
    simulations. *Computer Graphics Forum*, 38, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 [2019] B. Kim, V. C. Azevedo, N. Thürey, Theodore Kim, M. Gross 和 B. Solenthaler.
    深度流体：一种用于参数化流体模拟的生成网络。*计算机图形论坛*，38，2019年。
- en: Kim et al. [2022] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho
    Choi, and Jaegul Choo. Reversible instance normalization for accurate time-series
    forecasting against distribution shift. In *International Conference on Learning
    Representations*, 2022. URL [https://openreview.net/forum?id=cGDAkQo1C0p](https://openreview.net/forum?id=cGDAkQo1C0p).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kim 等 [2022] Taesung Kim, Jinhee Kim, Yunwon Tae, Cheonbok Park, Jang-Ho Choi
    和 Jaegul Choo. 可逆实例归一化以准确预测时间序列中的分布变化。在 *国际学习表示会议*，2022年。网址 [https://openreview.net/forum?id=cGDAkQo1C0p](https://openreview.net/forum?id=cGDAkQo1C0p)。
- en: Kima and Lee [2019] Junhyuk Kima and Changhoon Lee. Deep unsupervised learning
    of turbulence for inflow generation at various reynolds numbers. *arXiv:1908.10515v1*,
    2019.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kima 和 Lee [2019] Junhyuk Kima 和 Changhoon Lee. 用于不同雷诺数的流入生成的深度无监督湍流学习。*arXiv:1908.10515v1*，2019年。
- en: Kipf and Welling [2016] Thomas N Kipf and Max Welling. Semi-supervised classification
    with graph convolutional networks. *arXiv preprint arXiv:1609.02907*, 2016.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf 和 Welling [2016] Thomas N Kipf 和 Max Welling. 使用图卷积网络的半监督分类。*arXiv 预印本
    arXiv:1609.02907*，2016年。
- en: Kochkov et al. [2021] D. Kochkov, J. A. Smith, A. Alieva, Qifeng Wang, M. Brenner,
    and Stephan Hoyer. Machine learning accelerated computational fluid dynamics.
    *ArXiv*, abs/2102.01010, 2021.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kochkov 等 [2021] D. Kochkov, J. A. Smith, A. Alieva, Qifeng Wang, M. Brenner
    和 Stephan Hoyer. 机器学习加速的计算流体动力学。*ArXiv*，abs/2102.01010，2021年。
- en: Kondor and Trivedi [2018] Risi Kondor and Shubhendu Trivedi. On the generalization
    of equivariance and convolution in neural networks to the action of compact groups.
    In *Proceedings of the 35th International Conference on Machine Learning (ICML)*,
    volume 80, pages 2747–2755, 2018.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kondor 和 Trivedi [2018] Risi Kondor 和 Shubhendu Trivedi. 神经网络中等变性和卷积的推广到紧群的作用。在
    *第35届国际机器学习会议（ICML）* 论文集中，卷80，页2747–2755，2018年。
- en: Koopman [1931] B. O. Koopman. Hamiltonian systems and transformation in hilbert
    space. *Proceedings of the National Academy of Sciences of the United States of
    America*, 17 5:315–8, 1931.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koopman [1931] B. O. Koopman. 哈密顿系统和希尔伯特空间中的变换。*美国国家科学院院刊*，17 5:315–8，1931年。
- en: Kouw and Loog [2018] Wouter M. Kouw and Marco Loog. An introduction to domain
    adaptation and transfer learning. *arXiv preprint arXiv:1812.11806*, 2018.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kouw 和 Loog [2018] Wouter M. Kouw 和 Marco Loog. 域适应和迁移学习简介。*arXiv 预印本 arXiv:1812.11806*，2018年。
- en: Krishnapriyan et al. [2021] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe,
    Robert Kirby, and Michael W Mahoney. Characterizing possible failure modes in
    physics-informed neural networks. *Advances in Neural Information Processing Systems*,
    34, 2021.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krishnapriyan 等 [2021] Aditi Krishnapriyan, Amir Gholami, Shandian Zhe, Robert
    Kirby 和 Michael W Mahoney. 物理信息神经网络中的可能失败模式特征。*神经信息处理系统进展*，34，2021年。
- en: Kutz [2017] J. Kutz. Deep learning in fluid dynamics. *Journal of Fluid Mechanics*,
    814:1–4, 2017.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kutz [2017] J. Kutz. 流体动力学中的深度学习。*流体力学杂志*，814:1–4，2017年。
- en: Kuznetsov and Mohri [2020] Vitaly Kuznetsov and Mehryar Mohri. Discrepancy-based
    theory and algorithms for forecasting non-stationary time series. *Annals of Mathematics
    and Artificial Intelligence*, 88(4):367–399, 2020.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kuznetsov 和 Mohri [2020] Vitaly Kuznetsov 和 Mehryar Mohri. 基于差异的理论和算法用于预测非平稳时间序列。*数学与人工智能年鉴*，88(4):367–399，2020。
- en: Labourasse and Sagaut [2004] E. Labourasse and P. Sagaut. Advance in rans-les
    coupling, a review and an insight on the nlde approach. *Archives of Computational
    Methods in Engineering*, 11:199–256, 2004.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Labourasse 和 Sagaut [2004] E. Labourasse 和 P. Sagaut. RANS-LES耦合的进展，综述及对Nlde方法的见解。*计算工程方法档案*，11:199–256，2004。
- en: Lagergren et al. [2020] John H. Lagergren, John T. Nardini, G. Michael Lavigne,
    E. Rutter, and K. Flores. Learning partial differential equations for biological
    transport models from noisy spatio-temporal data. *Proceedings of the Royal Society
    A*, 476, 2020.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lagergren 等人 [2020] John H. Lagergren, John T. Nardini, G. Michael Lavigne,
    E. Rutter, 和 K. Flores. 从嘈杂的时空数据中学习生物运输模型的偏微分方程。*皇家学会A辑会议录*，476，2020。
- en: LeCun et al. [1989] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson,
    Richard E Howard, Wayne Hubbard, and Lawrence D Jackel. Backpropagation applied
    to handwritten zip code recognition. *Neural computation*, 1(4):541–551, 1989.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 [1989] Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson,
    Richard E Howard, Wayne Hubbard, 和 Lawrence D Jackel. 反向传播应用于手写邮政编码识别。*神经计算*，1(4):541–551，1989。
- en: Lee et al. [2021] Seungjun Lee, Haesang Yang, and Woojae Seong. Identifying
    physical law of hamiltonian systems via meta-learning. In *International Conference
    on Learning Representations*, 2021. URL [https://openreview.net/forum?id=45NZvF1UHam](https://openreview.net/forum?id=45NZvF1UHam).
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee 等人 [2021] Seungjun Lee, Haesang Yang, 和 Woojae Seong. 通过元学习识别哈密顿系统的物理法则。发表于
    *国际学习表征会议*，2021。网址 [https://openreview.net/forum?id=45NZvF1UHam](https://openreview.net/forum?id=45NZvF1UHam)。
- en: Lenc and Vedaldi [2015] Karel Lenc and Andrea Vedaldi. Understanding image representations
    by measuring their equivariance and equivalence. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*, pages 991–999, 2015.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lenc 和 Vedaldi [2015] Karel Lenc 和 Andrea Vedaldi. 通过测量图像表示的等变性和等效性来理解图像表示。发表于
    *IEEE计算机视觉与模式识别会议录*，第991–999页，2015。
- en: 'Li et al. [2018] Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. Diffusion
    convolutional recurrent neural network: Data-driven traffic forecasting. In *International
    Conference on Learning Representations (ICLR ’18)*, 2018.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2018] Yaguang Li, Rose Yu, Cyrus Shahabi, 和 Yan Liu. 扩散卷积递归神经网络：数据驱动的交通预测。发表于
    *国际学习表征会议 (ICLR ’18)*，2018。
- en: Li et al. [2020a] Yunzhu Li, Hao He, Jiajun Wu, D. Katabi, and A. Torralba.
    Learning compositional koopman operators for model-based control. *arXiv Preprint
    arXiv:1910.08264*, 2020a.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2020a] Yunzhu Li, Hao He, Jiajun Wu, D. Katabi, 和 A. Torralba. 学习组合的Koopman操作符用于基于模型的控制。*arXiv预印本
    arXiv:1910.08264*，2020a。
- en: 'Li et al. [2020b] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede
    Liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Neural operator:
    Graph kernel network for partial differential equations. *arXiv preprint arXiv:2003.03485*,
    2020b.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2020b] Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu,
    Kaushik Bhattacharya, Andrew Stuart, 和 Anima Anandkumar. 神经操作符：用于偏微分方程的图核网络。*arXiv预印本
    arXiv:2003.03485*，2020b。
- en: Li et al. [2021] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli,
    Burigede liu, Kaushik Bhattacharya, Andrew Stuart, and Anima Anandkumar. Fourier
    neural operator for parametric partial differential equations. In *International
    Conference on Learning Representations*, 2021. URL [https://openreview.net/forum?id=c8P9NQVtmnO](https://openreview.net/forum?id=c8P9NQVtmnO).
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人 [2021] Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli,
    Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, 和 Anima Anandkumar. 用于参数化偏微分方程的傅里叶神经操作符。发表于
    *国际学习表征会议*，2021。网址 [https://openreview.net/forum?id=c8P9NQVtmnO](https://openreview.net/forum?id=c8P9NQVtmnO)。
- en: Ling et al. [2016] Julia Ling, Andrew Kurzawski, and Jeremy Templeton. Reynolds
    averaged turbulence modeling using deep neural networks with embedded invariance.
    *Journal of Fluid Mechanics*, 807:155–166, 2016.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling 等人 [2016] Julia Ling, Andrew Kurzawski, 和 Jeremy Templeton. 使用嵌入不变性的深度神经网络进行雷诺平均湍流建模。*流体力学期刊*，807:155–166，2016。
- en: Ling et al. [2017] Julia Ling, Andrew Kurzawskim, and Jeremy Templeton. Reynolds
    averaged turbulence modeling using deep neural networks with embedded invariance.
    *Journal of Fluid Mechanics*, 2017.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ling 等人 [2017] Julia Ling, Andrew Kurzawski, 和 Jeremy Templeton. 使用嵌入不变性的深度神经网络进行雷诺平均湍流建模。*流体力学期刊*，2017。
- en: Linial et al. [2021] Ori Linial, D. Eytan, and U. Shalit. Generative ode modeling
    with known unknowns. *Proceedings of the Conference on Health, Inference, and
    Learning*, 2021.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Linial et al. [2021] Ori Linial, D. Eytan, 和 U. Shalit. 具有已知未知数的生成 ODE 建模. *健康、推理和学习会议论文集*，2021。
- en: Lisitsa et al. [2012] Vadim Lisitsa, Galina Reshetova, and Vladimir Tcheverda.
    Finite-difference algorithm with local time-space grid refinement for simulation
    of waves. *Computational geosciences*, 16(1):39–54, 2012.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lisitsa et al. [2012] Vadim Lisitsa, Galina Reshetova, 和 Vladimir Tcheverda.
    带局部时间-空间网格细化的有限差分算法用于波浪模拟. *计算地球科学*，16(1):39–54，2012。
- en: Liu and Wang [2019] Dehao Liu and Yan Wang. Multi-fidelity physics-constrained
    neural network and its application in materials modeling. *Journal of Mechanical
    Design*, 141, 2019.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 和 Wang [2019] Dehao Liu 和 Yan Wang. 多保真物理约束神经网络及其在材料建模中的应用. *机械设计杂志*，141，2019。
- en: 'Liu et al. [2019] Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, and
    Cho-Jui Hsieh. Neural sde: Stabilizing neural ode networks with stochastic noise.
    *ArXiv*, abs/1906.02355, 2019.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Liu et al. [2019] Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, 和
    Cho-Jui Hsieh. 神经 SDE: 用随机噪声稳定神经 ODE 网络. *ArXiv*，abs/1906.02355，2019。'
- en: Long et al. [2015] Mingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan.
    Learning transferable features with deep adaptation networks. In *International
    conference on machine learning*, pages 97–105\. PMLR, 2015.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. [2015] Mingsheng Long, Yue Cao, Jianmin Wang, 和 Michael Jordan.
    通过深度适应网络学习可迁移特征. 见于 *国际机器学习会议*，页码 97–105\. PMLR，2015。
- en: 'Long et al. [2019a] Yun Long, Xueyuan She, and Saibal Mukhopadhyay. Hybridnet:
    Integrating model-based and data-driven learning to predict evolution of dynamical
    systems. *ArXiv Preprint arXiv:1806.07439*, 2019a.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Long et al. [2019a] Yun Long, Xueyuan She, 和 Saibal Mukhopadhyay. Hybridnet:
    集成基于模型和数据驱动的学习以预测动态系统的演化. *ArXiv 预印本 arXiv:1806.07439*，2019a。'
- en: 'Long et al. [2018] Zichao Long, Yiping Lu, Xianzhong Ma, and Bin Dong. Pde-net:
    Learning pdes from data. In *International Conference on Machine Learning*, pages
    3214–3222, 2018.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Long et al. [2018] Zichao Long, Yiping Lu, Xianzhong Ma, 和 Bin Dong. Pde-net:
    从数据中学习 PDEs. 见于 *国际机器学习会议*，页码 3214–3222，2018。'
- en: 'Long et al. [2019b] Zichao Long, Yiping Lu, and Bin Dong. Pde-net 2.0: Learning
    pdes from data with a numeric-symbolic hybrid deep network. *Journal of Computational
    Physics*, page 108925, 2019b.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Long et al. [2019b] Zichao Long, Yiping Lu, 和 Bin Dong. Pde-net 2.0: 使用数值-符号混合深度网络从数据中学习
    PDEs. *计算物理学杂志*，页码 108925，2019b。'
- en: Lusch et al. [2018] Bethany Lusch, J. N. Kutz, and S. Brunton. Deep learning
    for universal linear embeddings of nonlinear dynamics. *Nature Communications*,
    9, 2018.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lusch et al. [2018] Bethany Lusch, J. N. Kutz, 和 S. Brunton. 用于非线性动态的通用线性嵌入的深度学习.
    *自然通讯*，9，2018。
- en: Manepalli and Albert [2019] A. Manepalli and A. Albert. Emulating numeric hydroclimate
    models with physics-informed cgans. 2019.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manepalli 和 Albert [2019] A. Manepalli 和 A. Albert. 使用物理信息 CGANs 模拟数值气候模型. 2019。
- en: Manzhos and Carrington [2006] S. Manzhos and T. Carrington. A random-sampling
    high dimensional model representation neural network for building potential energy
    surfaces. *The Journal of chemical physics*, 125 8:084109, 2006.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Manzhos 和 Carrington [2006] S. Manzhos 和 T. Carrington. 用于建立势能面的随机采样高维模型表示神经网络.
    *化学物理学杂志*，125 8:084109，2006。
- en: Martius and Lampert [2016] Georg Martius and Christoph H Lampert. Extrapolation
    and learning equations. *arXiv preprint arXiv:1610.02995*, 2016.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Martius 和 Lampert [2016] Georg Martius 和 Christoph H Lampert. 外推和学习方程. *arXiv
    预印本 arXiv:1610.02995*，2016。
- en: Maulik et al. [2019] R. Maulik, O. San, A. Rasheed, and P. Vedula. Subgrid modelling
    for two-dimensional turbulence using neural networks. *Journal of Fluid Mechanics*,
    858:122–144, 2019.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Maulik et al. [2019] R. Maulik, O. San, A. Rasheed, 和 P. Vedula. 使用神经网络的二维湍流亚格网建模.
    *流体力学杂志*，858:122–144，2019。
- en: McDonough [2007] J. M. McDonough. *Introductory Lectures on Turbulence*. Mechanical
    Engineering Textbook Gallery, 2007. URL [https://uknowledge.uky.edu/me_textbooks/2](https://uknowledge.uky.edu/me_textbooks/2).
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: McDonough [2007] J. M. McDonough. *湍流导论讲座*. 机械工程教科书系列，2007。网址 [https://uknowledge.uky.edu/me_textbooks/2](https://uknowledge.uky.edu/me_textbooks/2)。
- en: 'Muralidhar et al. [2020] Nikhil Muralidhar, Jie Bu, Z. Cao, Long He, N. Ramakrishnan,
    D. Tafti, and A. Karpatne. Phynet: Physics guided neural networks for particle
    drag force prediction in assembly. In *SDM*, 2020.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Muralidhar et al. [2020] Nikhil Muralidhar, Jie Bu, Z. Cao, Long He, N. Ramakrishnan,
    D. Tafti, 和 A. Karpatne. Phynet: 用于组装中的粒子阻力预测的物理引导神经网络. 见于 *SDM*，2020。'
- en: Neyshabur et al. [2017] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester,
    and Nati Srebro. Exploring generalization in deep learning. *Advances in neural
    information processing systems*, 30, 2017.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neyshabur 等人 [2017] Behnam Neyshabur, Srinadh Bhojanapalli, David McAllester
    和 Nati Srebro. 探索深度学习中的泛化能力。*Advances in Neural Information Processing Systems*,
    30, 2017。
- en: Pan and Yang [2010] Sinno Jialin Pan and Qiang Yang. A survey on transfer learning.
    *IEEE Transactions on knowledge and data engineering*, 22(10):1345–1359, 2010.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan 和 Yang [2010] Sinno Jialin Pan 和 Qiang Yang. 迁移学习调查。*IEEE Transactions on
    Knowledge and Data Engineering*, 22(10):1345–1359, 2010。
- en: Parish and Duraisamy [2016] Eric J. Parish and K. Duraisamy. A paradigm for
    data-driven predictive modeling using field inversion and machine learning. *J.
    Comput. Phys.*, 305:758–774, 2016.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parish 和 Duraisamy [2016] Eric J. Parish 和 K. Duraisamy. 使用场反演和机器学习的数据驱动预测建模范式。*J.
    Comput. Phys.*, 305:758–774, 2016。
- en: 'Pathak et al. [2022] Jaideep Pathak, Shashank Subramanian, Peter Harrington,
    Sanjeev Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall,
    Zongyi Li, Kamyar Azizzadenesheli, et al. Fourcastnet: A global data-driven high-resolution
    weather model using adaptive fourier neural operators. *arXiv preprint arXiv:2202.11214*,
    2022.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pathak 等人 [2022] Jaideep Pathak, Shashank Subramanian, Peter Harrington, Sanjeev
    Raja, Ashesh Chattopadhyay, Morteza Mardani, Thorsten Kurth, David Hall, Zongyi
    Li, Kamyar Azizzadenesheli 等人. Fourcastnet: 一个使用自适应傅里叶神经算子的全球数据驱动高分辨率天气模型。*arXiv
    预印本 arXiv:2202.11214*, 2022。'
- en: 'Pearl [2009] J. Pearl. Causal inference in statistics: An overview. *Statistics
    Surveys*, 3:96–146, 2009.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pearl [2009] J. Pearl. 统计学中的因果推断：概述。*Statistics Surveys*, 3:96–146, 2009。
- en: Pfaff et al. [2021] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez,
    and Peter Battaglia. Learning mesh-based simulation with graph networks. In *International
    Conference on Learning Representations*, 2021. URL [https://openreview.net/forum?id=roNqYL0_XP](https://openreview.net/forum?id=roNqYL0_XP).
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pfaff 等人 [2021] Tobias Pfaff, Meire Fortunato, Alvaro Sanchez-Gonzalez 和 Peter
    Battaglia. 使用图网络学习基于网格的模拟。发表于 *International Conference on Learning Representations*,
    2021。网址 [https://openreview.net/forum?id=roNqYL0_XP](https://openreview.net/forum?id=roNqYL0_XP)。
- en: Poli et al. [2019] Michael Poli, Stefano Massaroli, Junyoung Park, A. Yamashita,
    H. Asama, and Jinkyoo Park. Graph neural ordinary differential equations. *ArXiv*,
    abs/1911.07532, 2019.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poli 等人 [2019] Michael Poli, Stefano Massaroli, Junyoung Park, A. Yamashita,
    H. Asama 和 Jinkyoo Park. 图神经普通微分方程。*ArXiv*, abs/1911.07532, 2019。
- en: Pontryagin [1987] Lev Semenovich Pontryagin. *Mathematical theory of optimal
    processes*. CRC press, 1987.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pontryagin [1987] Lev Semenovich Pontryagin. *最优过程的数学理论*。CRC press, 1987。
- en: Rackauckas et al. [2020] Christopher Rackauckas, Yingbo Ma, Julius Martensen,
    Collin Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan, and
    Alan Edelman. Universal differential equations for scientific machine learning.
    *arXiv preprint arXiv:2001.04385*, 2020.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rackauckas 等人 [2020] Christopher Rackauckas, Yingbo Ma, Julius Martensen, Collin
    Warner, Kirill Zubov, Rohit Supekar, Dominic Skinner, Ali Ramadhan 和 Alan Edelman.
    用于科学机器学习的通用微分方程。*arXiv 预印本 arXiv:2001.04385*, 2020。
- en: 'Raissi and Karniadakis [2018] Maziar Raissi and George Em Karniadakis. Hidden
    physics models: Machine learning of nonlinear partial differential equations.
    *Journal of Computational Physics*, 357:125–141, 2018.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raissi 和 Karniadakis [2018] Maziar Raissi 和 George Em Karniadakis. 隐含物理模型：非线性偏微分方程的机器学习。*Journal
    of Computational Physics*, 357:125–141, 2018。
- en: 'Raissi et al. [2017] Maziar Raissi, Paris Perdikaris, and George Em Karniadakis.
    Physics informed deep learning (part i): Data-driven solutions of nonlinear partial
    differential equations. *arXiv preprint arXiv:1711.10561*, 2017.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raissi 等人 [2017] Maziar Raissi, Paris Perdikaris 和 George Em Karniadakis. 物理信息深度学习（第
    I 部分）：非线性偏微分方程的数据驱动解决方案。*arXiv 预印本 arXiv:1711.10561*, 2017。
- en: 'Raissi et al. [2019] Maziar Raissi, Paris Perdikaris, and George E Karniadakis.
    Physics-informed neural networks: A deep learning framework for solving forward
    and inverse problems involving nonlinear partial differential equations. *Journal
    of Computational Physics*, 378:686–707, 2019.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Raissi 等人 [2019] Maziar Raissi, Paris Perdikaris 和 George E Karniadakis. 物理信息神经网络：解决涉及非线性偏微分方程的前向和反向问题的深度学习框架。*Journal
    of Computational Physics*, 378:686–707, 2019。
- en: Rao et al. [2022] Chengping Rao, Pu Ren, Yang Liu, and Hao Sun. Discovering
    nonlinear PDEs from scarce data with physics-encoded learning. In *International
    Conference on Learning Representations*, 2022.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rao 等人 [2022] Chengping Rao, Pu Ren, Yang Liu 和 Hao Sun. 从稀缺数据中发现非线性 PDE，通过物理编码学习。发表于
    *International Conference on Learning Representations*, 2022。
- en: 'Reichstein et al. [2019] Markus Reichstein, Gustau Camps-Valls, Bjorn Stevens,
    Martin Jung, Joachim Denzler, Nuno Carvalhais, and Prabhat. Deep learning and
    process understanding for data-driven earth system science. *Nature*, 566(7743):195–204,
    2019. doi: 10.1038/s41586-019-0912-1. URL [https://doi.org/10.1038/s41586-019-0912-1](https://doi.org/10.1038/s41586-019-0912-1).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Reichstein et al. [2019] 马库斯·赖希斯坦、古斯陶·坎普斯-瓦尔斯、比约恩·史蒂文斯、马丁·容、约阿希姆·登茨勒、努诺·卡瓦尔哈伊斯和普拉巴特。深度学习与过程理解在数据驱动地球系统科学中的应用。*自然*，566(7743):195–204,
    2019。doi: 10.1038/s41586-019-0912-1。网址 [https://doi.org/10.1038/s41586-019-0912-1](https://doi.org/10.1038/s41586-019-0912-1)。'
- en: Rudy et al. [2016] Samuel H. Rudy, Steven L. Brunton, Joshua L. Proctor, and
    J. Nathan Kutz. Data-driven discovery of partial differential equations. *arXiv
    preprint arXiv:1609.06401*, 2016.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rudy et al. [2016] 塞缪尔·H·鲁迪、史蒂文·L·布伦顿、乔舒亚·L·普罗克特和J·内森·库茨。基于数据的偏微分方程发现。*arXiv预印本
    arXiv:1609.06401*，2016。
- en: Rumelhart et al. [1986] David E Rumelhart, Geoffrey E Hinton, and Ronald J Williams.
    Learning representations by back-propagating errors. *nature*, 323(6088):533–536,
    1986.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rumelhart et al. [1986] 大卫·E·鲁梅尔哈特、杰弗里·E·辛顿和罗纳德·J·威廉姆斯。通过反向传播误差学习表示。*自然*，323(6088):533–536,
    1986。
- en: Sabour et al. [2017] Sara Sabour, Nicholas Frosst, and Geoffrey E Hinton. Dynamic
    routing between capsules. *arXiv preprint arXiv:1710.09829*, 2017.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabour et al. [2017] 萨拉·萨布尔、尼古拉斯·弗罗斯特和杰弗里·E·辛顿。胶囊之间的动态路由。*arXiv预印本 arXiv:1710.09829*，2017。
- en: Sagaut et al. [2006] Pierre Sagaut, Sebastien Deck, and Marc Terracol. *Multiscale
    and Multiresolution Approaches in Turbulence*. Imperial College Press, 2006.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sagaut et al. [2006] 皮埃尔·萨戈、塞巴斯蒂安·德克和马克·特里科。*湍流中的多尺度和多分辨率方法*。帝国理工学院出版社，2006。
- en: Sahoo et al. [2018] Subham Sahoo, Christoph Lampert, and Georg Martius. Learning
    equations for extrapolation and control. In *International Conference on Machine
    Learning*, pages 4442–4450\. PMLR, 2018.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sahoo et al. [2018] 苏巴姆·萨霍、克里斯托夫·兰佩特和乔治·马尔修斯。学习方程用于外推和控制。在*国际机器学习大会*，第4442–4450页。PMLR，2018。
- en: San and Maulik [2018] O. San and R. Maulik. Neural network closures for nonlinear
    model order reduction. *Advances in Computational Mathematics*, 44:1717–1750,
    2018.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: San and Maulik [2018] O·桑和R·毛利克。用于非线性模型降阶的神经网络闭包。*计算数学进展*，44:1717–1750, 2018。
- en: Sanchez-Gonzalez et al. [2018] Alvaro Sanchez-Gonzalez, N. Heess, Jost Tobias
    Springenberg, J. Merel, Martin A. Riedmiller, Raia Hadsell, and P. Battaglia.
    Graph networks as learnable physics engines for inference and control. *ArXiv*,
    abs/1806.01242, 2018.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanchez-Gonzalez et al. [2018] 阿尔瓦罗·桑切斯-冈萨雷斯、N·赫斯、约斯特·托比亚斯·斯普林根贝格、J·梅雷尔、马丁·A·里德米勒、拉伊亚·哈德塞尔和P·巴塔利亚。图网络作为可学习的物理引擎用于推理和控制。*ArXiv*，abs/1806.01242,
    2018。
- en: Sanchez-Gonzalez et al. [2020] Alvaro Sanchez-Gonzalez, J. Godwin, T. Pfaff,
    Rex Ying, J. Leskovec, and P. Battaglia. Learning to simulate complex physics
    with graph networks. *ArXiv*, abs/2002.09405, 2020.
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanchez-Gonzalez et al. [2020] 阿尔瓦罗·桑切斯-冈萨雷斯、J·戈德温、T·普法夫、雷克斯·英、J·莱斯科维奇和P·巴塔利亚。通过图网络学习模拟复杂物理。*ArXiv*，abs/2002.09405,
    2020。
- en: Satorras et al. [2021] Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling.
    E (n) equivariant graph neural networks. In *International Conference on Machine
    Learning*, pages 9323–9332\. PMLR, 2021.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Satorras et al. [2021] 维克多·加西亚·萨托拉斯、埃米尔·霍赫博姆和马克斯·韦林。E (n) 等变图神经网络。在*国际机器学习大会*，第9323–9332页。PMLR，2021。
- en: Scarselli et al. [2008] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus
    Hagenbuchner, and Gabriele Monfardini. The graph neural network model. *IEEE transactions
    on neural networks*, 20(1):61–80, 2008.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scarselli et al. [2008] 弗朗科·斯卡尔塞利、马尔科·戈里、阿·钟·蔡、马库斯·哈根布赫和加布里埃莱·蒙法尔迪尼。图神经网络模型。*IEEE神经网络交易*，20(1):61–80,
    2008。
- en: 'Schaeffer [2017] Hayden Schaeffer. Learning partial differential equations
    via data discovery and sparse optimization. *Proceedings of the Royal Society
    A: Mathematical, Physical and Engineering Sciences*, 473(2197):20160446, 2017.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schaeffer [2017] 海登·谢弗。通过数据发现和稀疏优化学习偏微分方程。*皇家学会A辑：数学、物理与工程科学*, 473(2197):20160446,
    2017。
- en: Schmid [2008] P. Schmid. Dynamic mode decomposition of numerical and experimental
    data. *Journal of Fluid Mechanics*, 656:5–28, 2008.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmid [2008] P·施密德。数值和实验数据的动态模式分解。*流体力学杂志*，656:5–28, 2008。
- en: Schmidt and Lipson [2009] Michael D. Schmidt and Hod Lipson. Distilling free-form
    natural laws from experimental data. *Science*, 324:81 – 85, 2009.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schmidt and Lipson [2009] 迈克尔·D·施密特和霍德·利普森。从实验数据中提炼自由形式自然法则。*科学*，324:81 – 85,
    2009。
- en: 'Schütt et al. [2017] Kristof Schütt, P. Kindermans, Huziel Enoc Sauceda Felix,
    Stefan Chmiela, A. Tkatchenko, and K. Müller. Schnet: A continuous-filter convolutional
    neural network for modeling quantum interactions. In *NIPS*, 2017.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Schütt et al. [2017] Kristof Schütt、P. Kindermans、Huziel Enoc Sauceda Felix、Stefan
    Chmiela、A. Tkatchenko 和 K. Müller。Schnet: 一种用于建模量子相互作用的连续滤波卷积神经网络。在 *NIPS*，2017年。'
- en: Shi et al. [2021] Chence Shi, Shitong Luo, and Minkai Xu1. Learning gradient
    fields for molecular conformation generation. *International Conference on Machine
    Learning*, 2021.
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shi et al. [2021] Chence Shi、Shitong Luo 和 Minkai Xu1。学习梯度场用于分子构象生成。*国际机器学习大会*，2021年。
- en: 'Shi et al. [2019] Guanya Shi, Xichen Shi, Michael O’Connell, Rose Yu, Kamyar
    Azizzadenesheli, Animashree Anandkumar, Yisong Yue, and Soon-Jo Chung. Neural
    lander: Stable drone landing control using learned dynamics. In *2019 International
    Conference on Robotics and Automation (ICRA)*, pages 9784–9790, 2019. doi: 10.1109/ICRA.2019.8794351.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Shi et al. [2019] Guanya Shi、Xichen Shi、Michael O’Connell、Rose Yu、Kamyar Azizzadenesheli、Animashree
    Anandkumar、Yisong Yue 和 Soon-Jo Chung。Neural lander: 稳定的无人机降落控制使用学习的动力学。在 *2019
    国际机器人与自动化大会（ICRA）*，第9784–9790页，2019年。doi: 10.1109/ICRA.2019.8794351。'
- en: Simm et al. [2021] Gregor N. C. Simm, Robert Pinsler, Gábor Csányi, and José Miguel
    Hernández-Lobato. Symmetry-aware actor-critic for 3d molecular design. In *International
    Conference on Learning Representations*, 2021. URL [https://openreview.net/forum?id=jEYKjPE1xYN](https://openreview.net/forum?id=jEYKjPE1xYN).
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simm et al. [2021] Gregor N. C. Simm、Robert Pinsler、Gábor Csányi 和 José Miguel
    Hernández-Lobato。面向对称的演员-评论员用于3D分子设计。在 *国际学习表示大会*，2021年。网址 [https://openreview.net/forum?id=jEYKjPE1xYN](https://openreview.net/forum?id=jEYKjPE1xYN)。
- en: Smidt [2021] Tess E Smidt. Euclidean symmetry and equivariance in machine learning.
    *Trends in Chemistry*, 3(2):82–85, 2021.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smidt [2021] Tess E Smidt。机器学习中的欧几里得对称性和等变性。*化学趋势*，3(2):82–85，2021年。
- en: Sosnovik et al. [2020] Ivan Sosnovik, Michał Szmaja, and Arnold Smeulders. Scale-equivariant
    steerable networks. In *International Conference on Learning Representations*,
    2020. URL [https://openreview.net/forum?id=HJgpugrKPS](https://openreview.net/forum?id=HJgpugrKPS).
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sosnovik et al. [2020] Ivan Sosnovik、Michał Szmaja 和 Arnold Smeulders。尺度等变可调网络。在
    *国际学习表示大会*，2020年。网址 [https://openreview.net/forum?id=HJgpugrKPS](https://openreview.net/forum?id=HJgpugrKPS)。
- en: 'Strogatz [2018] Steven H. Strogatz. *Nonlinear dynamics and chaos: with applications
    to physics, biology, chemistry, and engineering*. CRC press, 2018.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Strogatz [2018] Steven H. Strogatz。*非线性动力学与混沌：应用于物理学、生物学、化学和工程学*。CRC出版社，2018年。
- en: Takeishi et al. [2017] Naoya Takeishi, Y. Kawahara, and T. Yairi. Learning koopman
    invariant subspaces for dynamic mode decomposition. In *NIPS*, 2017.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Takeishi et al. [2017] Naoya Takeishi、Y. Kawahara 和 T. Yairi。学习Koopman不变子空间以进行动态模式分解。在
    *NIPS*，2017年。
- en: Rui Wang et al. [2022a] Rui Wang, Robin Walters, and Rose Yu. Approximately
    equivariant networks for imperfectly symmetric dynamics. *International Conference
    on Machine Learning (ICML)*, 2022a.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rui Wang et al. [2022a] Rui Wang、Robin Walters 和 Rose Yu。近似等变网络用于不完全对称的动力学。*国际机器学习大会（ICML）*，2022a年。
- en: 'Rui Wang et al. [2022b] Rui Wang, Robin Walters, and Rose Yu. Data augmentation
    vs. equivariant networks: A theoretical study of generalizability on dynamics
    forecasting. *International Conference on Machine Learning, Principles of Distribution
    Shift Workshop*, 2022b.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rui Wang et al. [2022b] Rui Wang、Robin Walters 和 Rose Yu。数据增强与等变网络：关于动态预测可推广性的理论研究。*国际机器学习大会，分布偏移原则研讨会*，2022b年。
- en: 'Thiede et al. [2021] Erik Thiede, Wenda Zhou, and Risi Kondor. Autobahn: Automorphism-based
    graph neural nets. *Advances in Neural Information Processing Systems*, 34, 2021.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thiede et al. [2021] Erik Thiede、Wenda Zhou 和 Risi Kondor。Autobahn: 基于自同构的图神经网络。*神经信息处理系统进展*，34，2021年。'
- en: Thompson and Kramer [1994] Michael L. Thompson and M. Kramer. Modeling chemical
    processes using prior knowledge and neural networks. *Aiche Journal*, 40:1328–1340,
    1994.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Thompson and Kramer [1994] Michael L. Thompson 和 M. Kramer。利用先验知识和神经网络建模化学过程。*化学工程学杂志*，40:1328–1340，1994年。
- en: Tompson et al. [2017] Jonathan Tompson, Kristofer Schlachter, Pablo Sprechmann,
    and Ken Perlin. Accelerating eulerian fluid simulation with convolutional networks.
    In *Proceedings of the 34th International Conference on Machine Learning-Volume
    70*, pages 3424–3433\. JMLR. org, 2017.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tompson et al. [2017] Jonathan Tompson、Kristofer Schlachter、Pablo Sprechmann
    和 Ken Perlin。利用卷积网络加速欧拉流体模拟。在 *第34届国际机器学习大会论文集-第70卷*，第3424–3433页。JMLR.org，2017年。
- en: Unke et al. [2021] Oliver Unke, Mihail Bogojeski, Michael Gastegger, Mario Geiger,
    Tess Smidt, and Klaus-Robert Müller. Se (3)-equivariant prediction of molecular
    wavefunctions and electronic densities. *Advances in Neural Information Processing
    Systems*, 34:14434–14447, 2021.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Unke 等人 [2021] Oliver Unke, Mihail Bogojeski, Michael Gastegger, Mario Geiger,
    Tess Smidt, 和 Klaus-Robert Müller。Se (3)-等变分子波函数和电子密度的预测。*神经信息处理系统进展*，34:14434–14447，2021年。
- en: 'Villar et al. [2021] Soledad Villar, David W Hogg, Kate Storey-Fisher, Weichi
    Yao, and Ben Blum-Smith. Scalars are universal: Equivariant machine learning,
    structured like classical physics. In *Advances in Neural Information Processing
    Systems*, 2021.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Villar 等人 [2021] Soledad Villar, David W Hogg, Kate Storey-Fisher, Weichi Yao,
    和 Ben Blum-Smith。标量是通用的：等变机器学习，结构如经典物理学。发表于*神经信息处理系统进展*，2021年。
- en: Wainwright [2005] John Wainwright. *Dynamical systems in cosmology*. Cambridge
    University Press, 2005.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wainwright [2005] John Wainwright。*宇宙学中的动力系统*。剑桥大学出版社，2005年。
- en: Walters et al. [2021] Robin Walters, Jinxi Li, and Rose Yu. Trajectory prediction
    using equivariant continuous convolution. In *International Conference on Learning
    Representations*, 2021. URL [https://openreview.net/forum?id=J8_GttYLFgr](https://openreview.net/forum?id=J8_GttYLFgr).
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Walters 等人 [2021] Robin Walters, Jinxi Li, 和 Rose Yu。使用等变连续卷积进行轨迹预测。发表于*国际学习表征会议*，2021年。网址
    [https://openreview.net/forum?id=J8_GttYLFgr](https://openreview.net/forum?id=J8_GttYLFgr)。
- en: Wang et al. [2020a] Rui Wang, E. Huang, Uma Chandrasekaran, and Rose Yu. Aortic
    pressure forecasting with deep learning. *2020 Computing in Cardiology*, pages
    1–4, 2020a.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2020a] Rui Wang, E. Huang, Uma Chandrasekaran, 和 Rose Yu。使用深度学习进行主动脉压力预测。*2020年计算心脏病学会议*，第1–4页，2020年。
- en: Wang et al. [2020b] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert,
    and Rose Yu. Towards physics-informed deep learning for turbulent flow prediction.
    *Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery
    and Data Mining*, 2020b.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2020b] Rui Wang, Karthik Kashinath, Mustafa Mustafa, Adrian Albert,
    和 Rose Yu。向物理知识驱动的深度学习迈进，以预测湍流流动。*第26届 ACM SIGKDD 国际知识发现与数据挖掘会议论文集*，2020年。
- en: Wang et al. [2020c] Rui Wang, Danielle Maddix, Christos Faloutsos, Yuyang Wang,
    and Rose Yu. Bridging physics-based and data-driven modeling for learning dynamical
    systems. *arXiv Preprint arXiv:2011.10616*, 2020c.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2020c] Rui Wang, Danielle Maddix, Christos Faloutsos, Yuyang Wang,
    和 Rose Yu。弥合基于物理和数据驱动的建模以学习动力系统。*arXiv 预印本 arXiv:2011.10616*，2020年。
- en: Wang et al. [2021a] Rui Wang, Robin Walters, and Rose Yu. Incorporating symmetry
    into deep dynamics models for improved generalization. 2021a. URL [https://openreview.net/forum?id=wta_8Hx2KD](https://openreview.net/forum?id=wta_8Hx2KD).
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2021a] Rui Wang, Robin Walters, 和 Rose Yu。将对称性纳入深度动力学模型以提高泛化能力。2021年。网址
    [https://openreview.net/forum?id=wta_8Hx2KD](https://openreview.net/forum?id=wta_8Hx2KD)。
- en: Wang et al. [2021b] Rui Wang, Robin Walters, and Rose Yu. Bridging physics-based
    and data-driven modeling for learning dynamical systems. *arXiv preprint arXiv:2102.10271*,
    2021b.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2021b] Rui Wang, Robin Walters, 和 Rose Yu。弥合基于物理和数据驱动的建模以学习动力系统。*arXiv
    预印本 arXiv:2102.10271*，2021年。
- en: Wang et al. [2023] Rui Wang, Yihe Dong, Sercan O Arik, and Rose Yu. Koopman
    neural operator forecaster for time-series with temporal distributional shifts.
    In *International Conference on Learning Representations*, 2023. URL [https://openreview.net/forum?id=kUmdmHxK5N](https://openreview.net/forum?id=kUmdmHxK5N).
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2023] Rui Wang, Yihe Dong, Sercan O Arik, 和 Rose Yu。Koopman 神经算子预测时间序列中的时间分布偏移。发表于*国际学习表征会议*，2023年。网址
    [https://openreview.net/forum?id=kUmdmHxK5N](https://openreview.net/forum?id=kUmdmHxK5N)。
- en: Weiler and Cesa [2019] Maurice Weiler and Gabriele Cesa. General E(2)-equivariant
    steerable CNNs. In *Advances in Neural Information Processing Systems (NeurIPS)*,
    pages 14334–14345, 2019.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weiler 和 Cesa [2019] Maurice Weiler 和 Gabriele Cesa。通用 E(2)-等变可旋转 CNNs。发表于*神经信息处理系统进展（NeurIPS）*，第14334–14345页，2019年。
- en: Weiler et al. [2018] Maurice Weiler, Fred A. Hamprecht, and Martin Storath.
    Learning steerable filters for rotation equivariant CNNs. *Computer Vision and
    Pattern Recognition (CVPR)*, 2018.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Weiler 等人 [2018] Maurice Weiler, Fred A. Hamprecht, 和 Martin Storath。学习可旋转滤波器以实现旋转等变
    CNNs。*计算机视觉与模式识别（CVPR）*，2018年。
- en: White et al. [2019] B. White, A. Singh, and A. Albert. Downscaling numerical
    weather models with gans. 2019.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: White 等人 [2019] B. White, A. Singh, 和 A. Albert。利用 GANs 进行数值天气模型的降尺度。2019年。
- en: 'Wiewel et al. [2019] Steffen Wiewel, Moritz Becher, and Nils Thuerey. Latent
    space physics: Towards learning the temporal evolution of fluid flow. In *Computer
    Graphics Forum*, volume 38, pages 71–82, 2019.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wiewel 等 [2019] Steffen Wiewel, Moritz Becher, 和 Nils Thuerey。潜在空间物理：朝着学习流体流动的时间演变前进。发表于
    *计算机图形学论坛*，第 38 卷，页码 71–82，2019。
- en: 'Willard et al. [2020] Jared Willard, Xiaowei Jia, Shaoming Xu, Michael S. Steinbach,
    and Vipin Kumar. Integrating physics-based modeling with machine learning: A survey.
    *CoRR*, abs/2003.04919, 2020. URL [https://arxiv.org/abs/2003.04919](https://arxiv.org/abs/2003.04919).'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Willard 等 [2020] Jared Willard, Xiaowei Jia, Shaoming Xu, Michael S. Steinbach,
    和 Vipin Kumar。将基于物理的建模与机器学习相结合：综述。*CoRR*，abs/2003.04919，2020。网址 [https://arxiv.org/abs/2003.04919](https://arxiv.org/abs/2003.04919)。
- en: 'Worrall et al. [2017] Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov,
    and Gabriel J Brostow. Harmonic networks: Deep translation and rotation equivariance.
    In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*,
    pages 5028–5037, 2017.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Worrall 等 [2017] Daniel E Worrall, Stephan J Garbin, Daniyar Turmukhambetov,
    和 Gabriel J Brostow。和谐网络：深度翻译和旋转等变性。发表于 *IEEE 计算机视觉与模式识别会议论文集*，页码 5028–5037，2017。
- en: 'Wu et al. [2021] Dongxian Wu, Liyao Gao, X. Xiong, Matteo Chinazzi, Alessandro
    Vespignani, Y. Ma, and Rose Yu. Deepgleam: a hybrid mechanistic and deep learning
    model for covid-19 forecasting. *ArXiv*, abs/2102.06684, 2021.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2021] Dongxian Wu, Liyao Gao, X. Xiong, Matteo Chinazzi, Alessandro Vespignani,
    Y. Ma, 和 Rose Yu。Deepgleam：一种混合机械模型和深度学习模型用于 COVID-19 预测。*ArXiv*，abs/2102.06684，2021。
- en: Wu et al. [2019] Jin-Long Wu, Karthik Kashinath, Adrian Albert, Dragos Chirila,
    Prabhat, and Heng Xiao. Enforcing Statistical Constraints in Generative Adversarial
    Networks for Modeling Chaotic Dynamical Systems. *arXiv e-prints*, May 2019.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等 [2019] Jin-Long Wu, Karthik Kashinath, Adrian Albert, Dragos Chirila, Prabhat,
    和 Heng Xiao。在生成对抗网络中强制执行统计约束以建模混沌动态系统。*arXiv e-prints*，2019 年 5 月。
- en: 'Xie et al. [2018] You Xie, Erik Franz, Mengyu Chu, and Nils Thuerey. tempogan:
    A temporally coherent, volumetric gan for super-resolution fluid flow. *ACM Transactions
    on Graphics (TOG)*, 37(4):95, 2018.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie 等 [2018] You Xie, Erik Franz, Mengyu Chu, 和 Nils Thuerey。tempogan：一种时间一致的体积
    GAN 用于超分辨率流体流动。*ACM 计算机图形学学报 (TOG)*，37(4):95，2018。
- en: Yang et al. [2023] Jianke Yang, Robin Walters, Nima Dehmamy, and Rose Yu. Generative
    adversarial symmetry discovery. *arXiv preprint arXiv:2302.00236*, 2023.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 [2023] Jianke Yang, Robin Walters, Nima Dehmamy, 和 Rose Yu。生成对抗对称性发现。*arXiv
    预印本 arXiv:2302.00236*，2023。
- en: 'Yang et al. [2021] Liu Yang, Xuhui Meng, and George Em Karniadakis. B-pinns:
    Bayesian physics-informed neural networks for forward and inverse pde problems
    with noisy data. *Journal of Computational Physics*, 425:109913, 2021.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等 [2021] Liu Yang, Xuhui Meng, 和 George Em Karniadakis。B-pinns：用于带噪声数据的前向和逆向
    PDE 问题的贝叶斯物理信息神经网络。*计算物理学杂志*，425:109913，2021。
- en: Yeung et al. [2019] E. Yeung, Soumya Kundu, and Nathan Oken Hodas. Learning
    deep neural network representations for koopman operators of nonlinear dynamical
    systems. *2019 American Control Conference (ACC)*, pages 4832–4839, 2019.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeung 等 [2019] E. Yeung, Soumya Kundu, 和 Nathan Oken Hodas。为非线性动态系统的 Koopman
    算子学习深度神经网络表征。*2019 年美国控制会议 (ACC)*，页码 4832–4839，2019。
- en: Yin et al. [2021] Yuan Yin, Vincent LE GUEN, Jérémie DONA, Emmanuel de Bezenac,
    Ibrahim Ayed, Nicolas THOME, and patrick gallinari. Augmenting physical models
    with deep networks for complex dynamics forecasting. In *International Conference
    on Learning Representations*, 2021. URL [https://openreview.net/forum?id=kmG8vRXTFv](https://openreview.net/forum?id=kmG8vRXTFv).
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yin 等 [2021] Yuan Yin, Vincent LE GUEN, Jérémie DONA, Emmanuel de Bezenac, Ibrahim
    Ayed, Nicolas THOME, 和 Patrick Gallinari。通过深度网络增强物理模型以预测复杂动态。发表于 *国际学习表征会议*，2021。网址
    [https://openreview.net/forum?id=kmG8vRXTFv](https://openreview.net/forum?id=kmG8vRXTFv)。
- en: Zhang et al. [2017] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht,
    and Oriol Vinyals. Understanding deep learning requires rethinking generalization.
    In *International Conference on Learning Representations*, 2017. URL [https://openreview.net/forum?id=Sy8gdB9xx](https://openreview.net/forum?id=Sy8gdB9xx).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2017] Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, 和 Oriol
    Vinyals。理解深度学习需要重新思考泛化问题。发表于 *国际学习表征会议*，2017。网址 [https://openreview.net/forum?id=Sy8gdB9xx](https://openreview.net/forum?id=Sy8gdB9xx)。
- en: 'Zhang et al. [2018] Linfeng Zhang, Jiequn Han, Han Wang, R. Car, and E. Weinan.
    Deep potential molecular dynamics: a scalable model with the accuracy of quantum
    mechanics. *Physical review letters*, page 143001, 2018.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等 [2018] Linfeng Zhang, Jiequn Han, Han Wang, R. Car, 和 E. Weinan。深度势能分子动力学：一种具有量子力学精度的可扩展模型。*物理评论快报*，页码
    143001，2018。
- en: Zhang [1988] Wei Zhang. Shift-invariant pattern recognition neural network and
    its optical architecture. In *Proceedings of annual conference of the Japan Society
    of Applied Physics*, 1988.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张 [1988] 魏·张。位移不变模式识别神经网络及其光学架构。在*日本应用物理学会年会论文集*，1988年。
- en: Zhang et al. [1990] Wei Zhang, Kazuyoshi Itoh, Jun Tanida, and Yoshiki Ichioka.
    Parallel distributed processing model with local space-invariant interconnections
    and its optical architecture. *Applied optics*, 29(32):4790–4797, 1990.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 张等人 [1990] 魏·张、伊藤和义、田田君和市冈义树。具有局部空间不变互连的并行分布处理模型及其光学架构。*应用光学*，29(32):4790–4797，1990年。
- en: Zheng et al. [2022] Rong Zheng, Sicun Gao, and Rose Yu. Lyapunov regularized
    forecaster. *Machine Learning and the Physical Sciences workshop, NeurIPS 2022.*,
    2022.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 郑等人 [2022] 荣·郑、司存·高和玫瑰·余。Lyapunov 正则化预报器。*机器学习与物理科学研讨会，NeurIPS 2022*，2022年。
- en: Zhong et al. [2020] Yaofeng Zhong, Biswadip Dey, and Amit Chakraborty. Benchmarking
    energy-conserving neural networks for learning dynamics from data. *arXiv Preprint
    arXiv:2012.02334*, 2020.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 钟等人 [2020] 姚峰·钟、比斯瓦迪普·德伊和阿米特·查克拉博提。评估用于从数据中学习动态的能量守恒神经网络。*arXiv 预印本 arXiv:2012.02334*，2020年。
- en: Zhou et al. [2020] Allan Zhou, Tom Knowles, and Chelsea Finn. Meta-learning
    symmetries by reparameterization. *arXiv preprint arXiv:2007.02933*, 2020.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周等人 [2020] 艾伦·周、汤姆·诺尔斯和切尔西·芬。通过重参数化元学习对称性。*arXiv 预印本 arXiv:2007.02933*，2020年。
- en: Zhu et al. [2019] Yinhao Zhu, Nicholas Zabaras, Phaedon-Stelios Koutsourelakis,
    and Paris Perdikaris. Physics-constrained deep learning for high-dimensional surrogate
    modeling and uncertainty quantification without labeled data. *Journal of Computational
    Physics*, 394:56–81, 2019.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朱等人 [2019] 虞浩·朱、尼古拉斯·扎巴拉斯、法伊登-斯特利奥斯·库楚雷基斯和巴黎斯·佩尔迪卡里斯。受物理约束的深度学习用于高维替代建模和无标签数据的不确定性量化。*计算物理学杂志*，394:56–81，2019年。
