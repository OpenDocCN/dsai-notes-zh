- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:31:08'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2407.17030] Applications of Multi-Agent Deep Reinforcement Learning with Communication
    in Network Management: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2407.17030](https://ar5iv.labs.arxiv.org/html/2407.17030)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Applications of Multi-Agent Deep
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement Learning with Communication
  prefs: []
  type: TYPE_NORMAL
- en: 'in Network Management: A Survey'
  prefs: []
  type: TYPE_NORMAL
- en: Yue Pi23, Wang Zhang2, Yong Zhang4, Hairong Huang4, Baoquan Rao4,
  prefs: []
  type: TYPE_NORMAL
- en: '¹¹1Corresponding author.¹Yulong Ding2, ²²footnotemark: 2¹Shuanghua Yang25 2Shenzhen
    Key Laboratory of Safety and Security for Next Generation of Industrial Internet
    and Department of Computer Science and Engineering, Southern University of Science
    and Technology, Shenzhen, China 3Peng Cheng Laboratory, Shenzhen, China 4Huawei
    Technologies Co., Ltd. 5Department of Computer Science, University of Reading,
    UK'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: With the advancement of artificial intelligence technology, the automation of
    network management, also known as Autonomous Driving Networks (ADN), is gaining
    widespread attention. The network management has shifted from traditional homogeneity
    and centralization to heterogeneity and decentralization. Multi-agent deep reinforcement
    learning (MADRL) allows agents to make decisions based on local observations independently.
    This approach is in line with the needs of automation and has garnered significant
    attention from academia and industry. In a distributed environment, information
    interaction between agents can effectively address the non-stationarity problem
    of multiple agents and promote cooperation. Therefore, in this survey, we first
    examined the application of MADRL in network management, including specific application
    fields such as traffic engineering, wireless network access, power control, and
    network security. Then, we conducted a detailed analysis of communication behavior
    between agents, including communication schemes, communication content construction,
    communication object selection, message processing, and communication constraints.
    Finally, we discussed the open issues and future research directions of agent
    communication in MADRL for future network management and ADN applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multi-agent deep reinforcement learning, Agent communication, Emergent communication,
    Autonomous Driving Networks.
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The improvement of general computing capabilities has led to the continuous
    expansion of various production networks. The traditional centralized network
    management mode has gradually become unable to meet the demand. Therefore, deploying
    distributed and decentralized network engineering technologies to improve network
    performance under different network resource and service demand conditions is
    an essential trend in future network management. Network management involves monitoring
    and controlling network resources to ensure effective network operation. This
    comprises several technologies catering to different scenarios and requirements.
    Traditional network management technologies usually rely on heuristic methods,
    but with the increase of new devices and services, the network is becoming more
    extensive and more complex, and it is difficult to model and predict network devices
    accurately. At the same time, traditional methods are hard to meet the demand
    for automation and intelligent management for future network management.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, due to the development of artificial intelligence technology,
    technologies such as Machine Learning, Deep Learning (DL), and Reinforcement Learning
    (RL) have started to be implemented in network management. Multi-Agent Deep Reinforcement
    Learning (MADRL) based on DL and RL is considered an effective technique to provide
    AI network solutions for critical problems in the future Internet[[94](#biba.bibx24)].
    In MADRL, each network entity is regarded as an agent, gathers dynamic and uncertain
    environmental information, and independently takes action. However, in some partially
    observable distributed multi-agent systems, due to the mutual influence of adaptation
    strategies, the system is vulnerable to non-stationary problems [[106](#biba.bibx36)].
    Sharing observations, intentions, or experiences helps agents understand the environment
    and achieve stable learning. Meanwhile, effective communication between agents
    can be crucial in promoting cooperation among them [[130](#biba.bibx60)].
  prefs: []
  type: TYPE_NORMAL
- en: The current communication protocols between agents are usually predefined, resulting
    in a considerable increase in the total communication overhead as the number of
    agents increases. An alternative method called emergent communication enables
    agents to autonomously learn communication protocols and select the communication
    objects through interaction. However, it also faces several challenges, such as
    unreadable emergent messages by humans and requiring massive training before deployment.
    Moreover, its current application in network management is relatively limited;
    therefore, the reliability and feasibility of emergent communication need further
    validation.
  prefs: []
  type: TYPE_NORMAL
- en: This survey summarizes the communication behavior of multi-agent systems in
    network management. We first introduce and summarize the MARL system with agent
    communication currently applied in the four research directions of network management.
    Then we conduct a detailed analysis of the communication schemes of the system,
    the type of communication messages, the selection of communication objects, the
    message processing method of communication, and the constraints in agents’ communication.
    Finally, we propose existing issues and suggest future research directions for
    the communication of agents in network management.
  prefs: []
  type: TYPE_NORMAL
- en: TABLE I
  prefs: []
  type: TYPE_NORMAL
- en: Existing Surveys on MADRL Communication or Network Management
  prefs: []
  type: TYPE_NORMAL
- en: '| Work | MADRL | Scope |'
  prefs: []
  type: TYPE_TB
- en: '| Network Management | Communication |'
  prefs: []
  type: TYPE_TB
- en: '| [[94](#biba.bibx24)] | ✓ |  | Future Internet |'
  prefs: []
  type: TYPE_TB
- en: '| [[78](#biba.bibx8)] | ✓ |  | B5G/6G wireless network |'
  prefs: []
  type: TYPE_TB
- en: '| [[123](#biba.bibx53)] | ✓ |  | Traffic Engineering |'
  prefs: []
  type: TYPE_TB
- en: '| [[72](#biba.bibx2)] | ✓ |  | Vehicular Networks |'
  prefs: []
  type: TYPE_TB
- en: '| [[140](#biba.bibx70)] |  | ✓ | Agent Communication |'
  prefs: []
  type: TYPE_TB
- en: '| [[73](#biba.bibx3)] |  | ✓ | Emergent Communication |'
  prefs: []
  type: TYPE_TB
- en: '| Our Survey | ✓ | ✓ | Network Management + Agent Communication |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/bde68aa84a83278a8dd3bbe4f9018d87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: A classification of the applications of MADRL in network management
    with communication.'
  prefs: []
  type: TYPE_NORMAL
- en: I-A Related Existing Survey and Our Scope
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Since MADRL has been widely used to solve critical problems in network management,
    many surveys have summarized and classified relevant literature from different
    aspects, as shown in Table I. For example, The authors in [[94](#biba.bibx24)]
    conducted a detailed analysis of the application of MARL in five fields in the
    future network: network access, edge computing, routing, unmanned aerial vehicle
    (UAV), and network security. In [[78](#biba.bibx8)], the authors focus on the
    MADRL application in the future 6G network management. Furthermore, some works
    focus on specific scenarios, such as the authors in [[123](#biba.bibx53)] detailing
    DRL and MADRL’s application in Traffic Engineering. In [[72](#biba.bibx2)], the
    authors analyzed some applications of MARL in vehicular networks. In addition
    to network management, we also pay attention to the reviews of communication between
    multiple agents. For example, in [[140](#biba.bibx70)], the authors analyzed the
    characteristics of communication between agents in MADRL from nine dimensions
    in detail. The authors in [[73](#biba.bibx3)] discuss the potential application
    of Emergent Communication in future wireless networks.'
  prefs: []
  type: TYPE_NORMAL
- en: However, to the best of our knowledge, there is currently no comprehensive review
    of communication in MADRL systems applied to network management. Therefore, this
    survey focuses on reviewing, analyzing, and comparing existing MADRL works with
    communication among agents applied to network management, as shown in Figure 1\.
    We classify and summarize six key characteristics of agent communication and conduct
    a separate analysis of each of them. Meanwhile, we introduce several MADRL emergent
    communication algorithms that have the potential for application to network management
    in the future.
  prefs: []
  type: TYPE_NORMAL
- en: I-B Contributions and Survey Organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our contributions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We introduce the application of MADRL in network management and classify the
    properties of the agent communication in the MADRL system. We summarize the settings
    for agent communication that could be selected in different application scenarios
    and show the potential of using these algorithms to solve networking issues in
    future network management.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: $\bullet$
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We summarize several uncommon characteristics in MADRL related to communication
    in network management and discuss the unresolved issues and future research directions
    of using MADRL systems with agent communication in future network management.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of this paper is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Section II summarizes the recent works for the MADRL with communication in the
    four research directions of network management, including traffic engineering,
    network spectrum access, transmit power control, and network security. Section
    III classifies these works by their communication schemes. In Section IV, we introduce
    the classification of the message content of agent communication. Sections V and
    VI introduce the selection scheme of communication objects and message processing
    methods of MADRL’s work with communication. Section VII provides a summary of
    the communication constraints of agent communication. In Section VIII, we discuss
    the open issues and future research directions of agent communication of the applications
    of MARL for network management. The conclusions are given in Section IX.
  prefs: []
  type: TYPE_NORMAL
- en: The summary of the classification of MADRL communication in network management
    is shown in Table II.
  prefs: []
  type: TYPE_NORMAL
- en: TABLE II
  prefs: []
  type: TYPE_NORMAL
- en: Summary of MADRL Communication in Network Management
  prefs: []
  type: TYPE_NORMAL
- en: '| Work | Application Background | Communication Schemes | Communication Content
    | Communication object | Message Processing | Communication Constraints |'
  prefs: []
  type: TYPE_TB
- en: '| [31] | Traffic Engineering | Fully Distributed | State | Neighbor | Concatenation
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| [34] | CTDE | State | Neighbor | Concatenation | Limited Bandwidth |'
  prefs: []
  type: TYPE_TB
- en: '| [55] | Fully Distributed | Reward | All | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [56] | Fully Distributed | State | All | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [57] | Fully Distributed | State | All | - | Noise |'
  prefs: []
  type: TYPE_TB
- en: '| [35] | Fully Distributed | Reward | Neighbor | - | Noise |'
  prefs: []
  type: TYPE_TB
- en: '| [32] | Fully Distributed | M&OP | Neighbor | Neural Network | - |'
  prefs: []
  type: TYPE_TB
- en: '| [36] | Fully Distributed | Reward | All | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [58] | Fully Distributed | Reward | All | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [37] | CTDE | State | Neighbor | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [33] | CTDE | State | Neighbor | Neural Network | - |'
  prefs: []
  type: TYPE_TB
- en: '| [40] | Network Spectrum Access | CTDE | State | All | Concatenation | Noise
    |'
  prefs: []
  type: TYPE_TB
- en: '| [39] | Fully Centralized | M&OP | Central | Neural Network | Noise |'
  prefs: []
  type: TYPE_TB
- en: '| [54] | Fully Centralized | M&OP | Central | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [42] | Fully Distributed | State | Neighbor | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [38] | Fully Distributed | M&OP | All | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [43] | Fully Distributed | M&OP | All | Average | - |'
  prefs: []
  type: TYPE_TB
- en: '| [41] | CTDE | State | Neighbor | Neural Network | Noise |'
  prefs: []
  type: TYPE_TB
- en: '| [44] | Transmit Power Control | CTDE | State | All | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [49] | Fully Distributed | State | Neighbor | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [45] | CTDE | State | Neighbor | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| [46] | Fully Distributed | State | Neighbor | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [50] | Fully Distributed | State | Neighbor | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [47] | Fully Distributed | State | All | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [48] | Fully Distributed | Reward | All | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [51] | Network Security | CTDE | Reward | All | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [52] | CTDE | Reward | All | Concatenation | - |'
  prefs: []
  type: TYPE_TB
- en: '| [53] | Fully Centralized | M&OP | Central | - | Limited Bandwidth |'
  prefs: []
  type: TYPE_TB
- en: II MADRL Communication with Application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The information interaction between agents can help each agent in the system
    better understand the variation of the global environment and the impact of other
    agents on the environment, thereby improving the system’s overall performance.
    Moreover, agents can effectively achieve better cooperation by sharing information
    such as observations, actions, or reward values, thereby achieving better team
    performance [[78](#biba.bibx8)]. In wireless communication environments, where
    communication could be unreliable or costly, some works assume that agents don’t
    communicate with each other. For example, wireless heterogeneous networks [[96](#biba.bibx26),
    [137](#biba.bibx67), [133](#biba.bibx63), [100](#biba.bibx30), [82](#biba.bibx12)],
    cognitive radio (CR) networks [[88](#biba.bibx18), [126](#biba.bibx56)], wireless
    cellular networks [[131](#biba.bibx61), [77](#biba.bibx7), [97](#biba.bibx27),
    [127](#biba.bibx57), [93](#biba.bibx23)], UAV networks[[134](#biba.bibx64), [135](#biba.bibx65),
    [108](#biba.bibx38), [114](#biba.bibx44), [75](#biba.bibx5)], vehicular networks
    [[98](#biba.bibx28), [104](#biba.bibx34)], and IoT [[115](#biba.bibx45), [111](#biba.bibx41)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In this survey, we only focus on the existing applications of MADRL in network
    management with agent communication. This section categorizes works with agent
    communication by their application: traffic engineering, network spectrum access,
    transmit power control, and network security.'
  prefs: []
  type: TYPE_NORMAL
- en: II-A Traffic Engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Traffic engineering refers to optimizing routing paths for traffic flows based
    on their characteristics and balancing the load between different switches, routers,
    and links in the network. This requires the system to achieve dynamic and real-time
    monitoring, analysis, control, and prediction of traffic status on the network
    [[123](#biba.bibx53)]. Traditional traffic engineering methods or routing protocols
    are defined by fixed rules [[112](#biba.bibx42)], which make it challenging to
    achieve the autonomous monitoring and dynamic network management requirements
    in the network. Nevertheless, MADRL methods enable each network entity, as an
    agent, to independently learn an adaptive routing strategy and dynamically change
    its routing rules. Furthermore, the MADRL system with agent communication enables
    agents to make dynamic decisions that consider the current network status and
    the impact of other agent routing policies. Therefore, MADRL, especially with
    agents’ communication, might be an effective solution for traffic engineering
    in future network management.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of [[138](#biba.bibx68)] analyze and verify the improvement of agents’
    communication on Autonomous Systems (ASs) throughput. They propose a MADRL routing
    method for ASs, where each AS on the Internet acts as an agent. ASs select the
    best next-hop AS for different flows to maximize system throughput, according
    to the observation of oneself and adjacent agents. The authors test the system
    performance when the agent communicates with neighbor agents within different
    ranges. The simulation results show that the system with agent communication performs
    better than the system where agents only use their local observations to make
    discussions. Moreover, improving the communication ranges results in an increase
    in the system’s average throughput. The author points out that interacting with
    more agents can help agents better understand the network state and reduce the
    impact of non-stationary issues in the distributed MADRL system.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[71](#biba.bibx1)], the authors design two communication mechanisms for
    the packet routing problem: value sharing and model sharing. Agents establish
    communication connections through signaling and sharing their Deep Neural Networks
    (DNN) models (model sharing) or exchange estimates of end-to-end delay (value
    sharing) with neighboring agents. Regarding packet loss ratio and average end-to-end
    packet delay, model sharing is always better than value sharing when replay-memory
    sizes are large enough.'
  prefs: []
  type: TYPE_NORMAL
- en: In [[87](#biba.bibx17)], graph convolution reinforcement learning is used to
    adapt to the dynamics of the underlying network graph in multi-agent environments.
    Unlike the traditional MADRL approach, it regards each data packet in the network
    rather than the router as an agent. The packet as the agent is a node in the graph,
    and the local observation encoding of the agent is a node feature. Agents formulate
    cooperation policies by using convolutional neural networks (CNNs) to learn the
    feature information of nodes from the communication between agents. The experiment
    shows that compared to the routing algorithm using the Deep Q network, the routing
    algorithm that learns to encode and communicate messages through Graph Neural
    Networks (GNN) performs better in system throughput and latency.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, several works allow information exchange among agents but do not
    discuss the impact of communication between agents on the system. For example,
    In [[129](#biba.bibx59)], the authors propose Deep Q-routing, based on the Deep
    Q network, to reduce the average transmission time of packets in Autonomous Systems.
    The network model is represented as a directed graph. The routers, which are the
    nodes on the network graph, act as agents. Routers choose one of the neighboring
    agents as the next hop node for the data packet based on their local observations
    and neighbors’ information.
  prefs: []
  type: TYPE_NORMAL
- en: The study presented in [[74](#biba.bibx4)] has introduced a traffic allocation
    plan for the Crowd-sourced Licast services (CLS) system. The authors use the Augmented
    Graph Model to model a large-scale CLS system as a multi-hop routing problem.
    As an agent, the CLS system’s network node determines the flow path in the network
    depending on observation and the other agents’ local rewards.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of [[101](#biba.bibx31)] found that designing a single reward function
    can lead to agents becoming lazy or selfish. Thus, the authors designed a delay-tolerant
    global and local reward function. By capturing information from neighboring agents
    during training and implicitly sharing packet TTL during execution, a higher level
    of cooperation between agents is achieved in the SDN environment.
  prefs: []
  type: TYPE_NORMAL
- en: Work [[132](#biba.bibx62)] proposes a MADRL routing algorithm in a mixed-distributed
    IoT system. Agents collect and exchange self-observation information containing
    queue length and remaining energy with other agents to maximize the total amount
    of data transmission for long-term goals while reducing device energy consumption.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Network Spectrum Access
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The network spectrum access method aims to improve spectrum efficiency by allocating
    spectrum resources reasonably and avoiding quality of service (QoS) reduction
    caused by competition among network entities for spectrum resources. The algorithm
    based on MARL can enable network entities to adaptively select spectrum resources
    based on environmental changes, and is therefore widely used. Agents in the MARL
    system typically make decisions to improve system spectrum utilization and throughput
    by collecting and sharing information such as channel state information (CSI),
    QoS, etc. The information exchange between agents can help them perceive the network
    status and make better channel allocation policies.
  prefs: []
  type: TYPE_NORMAL
- en: 'The authors of [[76](#biba.bibx6)] argue that the different types of agent
    information exchange can impact the MADRL dynamic spectrum access system. They
    explore a MADRL algorithm that maximizes the information rate of secondary users
    while ensuring that the primary user’s spectrum usage is not affected. Moreover,
    the performance of three agent communication models is tested: (1) agents only
    exchanging reward information, (2) agents exchanging reward and state, and (3)
    agents exchanging reward, state, and action. The result of the experiment shows
    that as the types of message parameters exchanged between agents increase, the
    system can adapt more quickly in the spectrum environment and avoid more channel
    conflicts. Moreover, more information exchange can also improve the sum information
    rate of the secondary users.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[119](#biba.bibx49)] propose a communication information pre-processing
    method for agent communication. In [[119](#biba.bibx49)], each vehicle in the
    vehicle networking system acts as an agent, transmitting its local CSI to the
    base station, which is the central controller. The central controller integrates
    the messages and sends local agents the vehicle-to-vehicle (V2V) link channel
    allocation policies to maximize the system’s throughput. To avoid the significant
    signaling overhead that may arise from the instantaneous global CSI collected
    [[94](#biba.bibx24)], each local agent learns to compress the CSI with an independent
    DNN.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[81](#biba.bibx11)] design a MADRL-based method to maximize
    vehicle networks’ packet reception rate (PRR) in cellular-free scenarios. Each
    vehicle agent independently selects spectrum resources based on its state and
    messages exchanged with other agents through V2V links.
  prefs: []
  type: TYPE_NORMAL
- en: Different for [[81](#biba.bibx11)], [[122](#biba.bibx52)] proposes a decentralized
    spectrum access algorithm for cellular vehicle-to-everything (C-V2X) networks
    to maximize the total throughput of vehicle-to-infrastructure (V2I) users. Agents
    in [[122](#biba.bibx52)] learn communication through dedicated channels using
    a DNN-based message generator module (MGM) independent of action selection.
  prefs: []
  type: TYPE_NORMAL
- en: '[[139](#biba.bibx69)] proposed a joint optimization scheme for mode selection
    and channel allocation in Device-to-Device (D2D) Heterogeneous Cellular Networks
    based on MADRL for millimeter wave and cellular frequency bands. The authors studied
    the interference problem caused by spectrum sharing between users of different
    cellular networks. To ensure the quality of service (QoS) of each D2D user, the
    agents exchange their satisfaction state information on QoS constraints to maximize
    user satisfaction.'
  prefs: []
  type: TYPE_NORMAL
- en: The MADRL system in [[107](#biba.bibx37)] implemented a two-level unlicensed
    spectrum access framework consisting of a feedback cycle and an execution cycle.
    The base stations, as agents, transmit data to unlicensed spectrums to alleviate
    the pressure on cellular networks. The MADRL system is designed as a multi-agent
    game model, where agents share their state and reward information through broadcast
    to achieve the system’s Nash equilibrium and maximize the network’s total throughput.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Transmit Power Control
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In wireless networks, network entities typically need to control transmission
    power to reduce interference with other network entities. In the MADRL system,
    agents can adaptively select transmit power based on the observation of the environmental
    changes, which is considered an effective tool for solving power control problems.
    Moreover, through communication, agents can obtain the interference they cause
    to other agents or the power allocation of other agents to find a balance between
    improving transmit power and reducing interference.
  prefs: []
  type: TYPE_NORMAL
- en: '[[90](#biba.bibx20)] proposed a power adaptive allocation algorithm based on
    reinforcement learning in multi-user cellular networks. The agent is the base
    station (BS), and the state of each BS is the local CSI and the power allocation
    of the previous time step. BSs exchange their power allocation information to
    improve the network throughput.'
  prefs: []
  type: TYPE_NORMAL
- en: '[[136](#biba.bibx66)] designs a power control algorithm for a cellular vehicle
    network based on MADRL. The system consists of a BS and multiple vehicle user
    equipment (VUEs) covered by that BS, where the agent is active V2V link. Agents
    reduce channel interference by sharing their channel selection from the previous
    time step with neighbors, maximizing the total capacity of the V2I link.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors of [[102](#biba.bibx32)] proposed a radio resource management algorithm
    based on MADRL, where each Access Point (AP) is an agent, and each AP connects
    multiple user equipment devices (UEs). The data is adaptively transmitted to the
    associated UEs based on SNR and weight. APs exchange observations with neighboring
    agents, which are the weights and SNR of UEs. In addition to direct communication
    between APs, agents also receive feedback reports from UEs to understand the information
    of other APs, which has an unavoidable delay.
  prefs: []
  type: TYPE_NORMAL
- en: In multi-user downlink small cell networks, traditional cooperative resource
    allocation (RA) requires collecting global CSI to calculate SNR, which is difficult
    to achieve in network environments. To address the difficulty of collecting global
    CSI in practical networks with limited direct link capacity, [[85](#biba.bibx15)]
    proposes a power control algorithm based on MADRL for small cell clusters. Small
    cell BSs as agents only use local CSI at the transmitters and exchange in-cell
    sum rate through direct links to maximize the system sum rate.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome the cross-layer signal interference problem between small and macro
    cells, the authors of [[124](#biba.bibx54)] designed a penalty-based Q-learning
    algorithm. By introducing regularization terms into the loss function, agents
    are encouraged to choose experiential actions with high global rewards to promote
    cooperation between agents. Agents share local observations through wired and
    wireless backhaul links to achieve balanced power allocation policies.
  prefs: []
  type: TYPE_NORMAL
- en: In [[103](#biba.bibx33)], the authors introduce an interference sorting technique
    to handle interference information in dynamic power allocation in wireless networks.
    In HetNet with multiple APs and users, the interference sources of the transmitter
    are classified based on the received power of the corresponding interference sources
    at the receiver. Each transmitter (agent) not only collects CSI and QoS information
    from neighboring agents but also exchanges status information, such as transmit
    power interference through communication, and adjusts its transmit power accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: '[[80](#biba.bibx10)] implementing collaborative power control and resource
    management in a multi-user downlink small cell network using MADRL. The optimal
    power allocation strategy for joint sub-carriers in IoT systems is learned through
    a dual deep Q network algorithm without complete instantaneous CSI. The adjacent
    agents share information regarding spectral efficiency, channel gain, and received
    power.'
  prefs: []
  type: TYPE_NORMAL
- en: II-D Network Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Network security is a technology that protects network entities and information
    from malicious attacks. Common challenges include jamming attacks, distributed
    denial of service (DDoS) attacks, etc. The jamming attack refers to attackers
    interfering with legitimate communication channels by sending interference signals.
    DDoS is a distributed cyber attack aimed at depleting the network resources of
    the target system. Recently, the joint network attack defense method based on
    MADRL for multiple network entities has been extensively studied.
  prefs: []
  type: TYPE_NORMAL
- en: '[[128](#biba.bibx58)] uses a multi-agent Q-learning algorithm to learn distributed
    anti-jamming strategies for each agent. A jammer in the system initiates jamming
    attacks on one of the channels each time. Meanwhile, agents choose the same channel,
    causing co-channel interference. The agent is the legitimate user, each agent
    exchanges Q-values and sums them up to select the joint action that can maximize
    the sum of Q-values.'
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[125](#biba.bibx55)] propose a collaborative anti-jamming algorithm
    based on multi-agent Q-learning in UAV communication networks. As agents, the
    UAV group users are usually in a competitive relationship without communication.
    When the agent perceives that the energy of the co-channel interference signal
    exceeds the threshold, the agent determines that they have been affected by the
    co-channel jamming, and the agents in the system switch to cooperative mode. In
    the cooperative mode, agents share their Q table to output joint action to counteract
    jamming attacks and maximize the user utility.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[121](#biba.bibx51)] use a centralized MADRL system based on
    a hierarchical communication mechanism to defend against DDoS attacks. Each router,
    as a local agent, sends its traffic reading to the central router, which then
    decides the throttling rate for each router. To reduce the huge communication
    costs caused by the frequent exchange of information with the central agent, each
    local agent adds a deep deterministic policy gradient network to determine whether
    to send local information to the central agent.
  prefs: []
  type: TYPE_NORMAL
- en: III Communication Schemes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The communication schemes refer to the learning and execution schemes of the
    MADRL system, which can be classified into fully centralized learning and execution,
    centralized training and distributed execution, and fully distributed execution.
    Different learning and execution schemes will be selected according to the needs
    of different application scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: The summary is shown in Table III.
  prefs: []
  type: TYPE_NORMAL
- en: TABLE III
  prefs: []
  type: TYPE_NORMAL
- en: The Category of Communication Schemes
  prefs: []
  type: TYPE_NORMAL
- en: '| Types | Traffic Engineering | Network Access | Power Control | Network Security
    |'
  prefs: []
  type: TYPE_TB
- en: '| Fully Centralized | - | [[119](#biba.bibx49)]  [[113](#biba.bibx43)] | -
    | [[121](#biba.bibx51)] |'
  prefs: []
  type: TYPE_TB
- en: '| CTDE | [[129](#biba.bibx59)]  [[132](#biba.bibx62)]  [[87](#biba.bibx17)]
    | [[81](#biba.bibx11)]  [[122](#biba.bibx52)] | [[90](#biba.bibx20)]  [[136](#biba.bibx66)]
    | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
  prefs: []
  type: TYPE_TB
- en: '| Fully Distributed | [[138](#biba.bibx68)]  [[89](#biba.bibx19)]  [[95](#biba.bibx25)]  [[92](#biba.bibx22)]  [[74](#biba.bibx4)]  [[71](#biba.bibx1)]  [[101](#biba.bibx31)]  [[109](#biba.bibx39)]
    | [[139](#biba.bibx69)]  [[76](#biba.bibx6)]  [[107](#biba.bibx37)] | [[103](#biba.bibx33)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[85](#biba.bibx15)]  [[124](#biba.bibx54)]
    | - |'
  prefs: []
  type: TYPE_TB
- en: III-A Fully Centralized
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the fully centralized scheme, local agents report their observations to a
    central agent, which decides what local agents should execute. The central agent
    can effectively output the globally optimal policy by utilizing instantaneous
    global state information.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the traditional dynamic spectrum access algorithm usually relies
    on global CSI [[120](#biba.bibx50), [91](#biba.bibx21)], while the MADRL dynamic
    spectrum access algorithm, such as [[119](#biba.bibx49)], evolved from traditional
    methods, adopts a fully centralized structure to enable the central agent to achieve
    adaptive channel adjustment based on global CSI received from the local agents.
  prefs: []
  type: TYPE_NORMAL
- en: By collecting global instantaneous state information, a fully centralized model
    can effectively overcome non-stationary problems. However, frequent interaction
    between all local agents and the central agent may lead to high communication
    costs. In addition, communication delays may result in the central control being
    unable to collect complete instantaneous global states, thereby affecting agent
    decision-making.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Centralized Training and Distributed Execution
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Centralized Training and Distributed Execution (CTDE) allows agents to use non-immediate
    global information for centralized learning, and make their own decisions independently
    during the execution phase. Compared with the fully centralized system, the MADRL
    system with the CTDE framework usually has less communication cost during execution
    and has been widely used in network management. Additionally, compared with distributed
    learning, agents learning a shared policy can reduce training parameters and accelerate
    convergence speed [[83](#biba.bibx13)]. Therefore, the CTDE framework has been
    widely used in network management. However, centralized training may lead to poor
    scalability. Changes in network topology may require all agents to be retrained
    in a centralized training system.
  prefs: []
  type: TYPE_NORMAL
- en: III-C Fully Distributed
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the fully distributed scheme, each agent is trained with an independent network.
    Similar to CTDE, agents can overcome non-stationarity during the execution phase
    by exchanging messages with other agents. Compared to centrally trained models,
    distributed training models have better flexibility and scalability, thereby have
    been used in networks built on mobile network entities, such as [[89](#biba.bibx19)]
    [[95](#biba.bibx25)] [[92](#biba.bibx22)] [[101](#biba.bibx31)] [[109](#biba.bibx39)]
    [[103](#biba.bibx33)], or heterogeneous networks composed of different network
    entities, such as [[139](#biba.bibx69)] [[124](#biba.bibx54)].
  prefs: []
  type: TYPE_NORMAL
- en: The fully distributed learning and execution schemes align with the trend of
    future network management toward distribution and decentralization. However, each
    agent must be trained through an independent network, which can result in higher
    training costs.
  prefs: []
  type: TYPE_NORMAL
- en: IV Communication Content
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The communication content refers to what information is encoded in the communication
    messages. The communication messages may generally be the agent’s state, action,
    reward, or strategy. This information may be used to assist agents in completing
    their perception of environmental changes, learning policies from other agents,
    or promoting collaboration between agents. Agents in different systems interact
    with various types of information based on diverse application scenarios. The
    summary is shown in Table IV.
  prefs: []
  type: TYPE_NORMAL
- en: TABLE IV
  prefs: []
  type: TYPE_NORMAL
- en: The Category of Communication Content
  prefs: []
  type: TYPE_NORMAL
- en: '| Types | Traffic Engineering | Network Access | Power Control | Network Security
    |'
  prefs: []
  type: TYPE_TB
- en: '| State | [[138](#biba.bibx68)]  [[129](#biba.bibx59)]  [[95](#biba.bibx25)]  [[92](#biba.bibx22)]  [[132](#biba.bibx62)]  [[87](#biba.bibx17)]
    | [[81](#biba.bibx11)]  [[139](#biba.bibx69)]  [[122](#biba.bibx52)] | [[90](#biba.bibx20)]  [[103](#biba.bibx33)]  [[136](#biba.bibx66)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[85](#biba.bibx15)]
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| Reward | [[74](#biba.bibx4)]  [[89](#biba.bibx19)]  [[101](#biba.bibx31)]  [[109](#biba.bibx39)]
    | - | [[124](#biba.bibx54)] | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
  prefs: []
  type: TYPE_TB
- en: '| Mixed & Other | [[71](#biba.bibx1)] | [[119](#biba.bibx49)]  [[113](#biba.bibx43)]  [[76](#biba.bibx6)]  [[107](#biba.bibx37)]
    | - | [[121](#biba.bibx51)] |'
  prefs: []
  type: TYPE_TB
- en: IV-A State or observation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Agents can compensate for their partial knowledge of the environment by exchanging
    observation (partial state) or complete state information, and its specific parameters
    are usually closely related to the application scenario and optimization goal
    of the MADRL system.
  prefs: []
  type: TYPE_NORMAL
- en: For example, in research on traffic engineering, agents learn to select the
    next hop for data packets or flows, in order to improve system throughput, reduce
    transmission delay, and avoid congestion. Therefore, to minimize the average transmission
    time of each data packet, in [[129](#biba.bibx59)] and [[132](#biba.bibx62)],
    agents exchange queue length with neighbors.
  prefs: []
  type: TYPE_NORMAL
- en: In [[138](#biba.bibx68)], the objective of the research is to enhance the average
    throughput performance of the system that comprises ASs acting as agents. The
    agents collaborate by sharing their observations including the current flow in
    the agent, the maximum number of flows, the number of neighboring agents, and
    the throughput of the flow.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, traffic engineering in wireless communication would consider the
    link stability or data validation. For example, the research on unmanned space
    self-organizing network [[92](#biba.bibx22)] shares the effective data payload
    with the next hop agent. In research on underwater wireless sensor networks [[95](#biba.bibx25)],
    communication messages are designed to reflect agent energy and link stability
    to reduce sensor energy use and extend the network life cycle.
  prefs: []
  type: TYPE_NORMAL
- en: '[[87](#biba.bibx17)] propose Graph Convolutional Reinforcement Learning in
    a graph representation multi-agent system to achieve adaptive routing. The agent
    is a data packet, and the characteristics of the nodes on the graph serve as the
    state information of the agent, including the packet’s current location, destination,
    data size, link load, and the number of adjacent data packets. Agents use GNN
    to learn the encoding of state messages and share them with neighboring agents.'
  prefs: []
  type: TYPE_NORMAL
- en: Research on transmit power control in wireless communication networks usually
    aims to improve the transmission rate while reducing the impact of interference
    between network entities on the system. At the same time, adjusting the power
    allocation ratio improves the overall network throughput or reduces energy consumption.
    Therefore, the state information interacted by agents in related research includes
    power allocation ratio [[90](#biba.bibx20)], interference from other agents [[103](#biba.bibx33)],
    SNR [[102](#biba.bibx32)], channel gain [[103](#biba.bibx33), [80](#biba.bibx10)],
    received power [[80](#biba.bibx10)], and CSI [[124](#biba.bibx54)].
  prefs: []
  type: TYPE_NORMAL
- en: The network spectrum access method needs to avoid excessive competition for
    spectrum resources among network entities and choose a balanced spectrum resource
    allocation policy. Therefore, agents inform other agents of channel occupancy
    information [[139](#biba.bibx69)] or interference [[122](#biba.bibx52)] through
    communication. Moreover, the MADRL system in [[139](#biba.bibx69)] aims to maximize
    user satisfaction, thereby, the agents also exchange their satisfaction with QoS
    constraints with each other.
  prefs: []
  type: TYPE_NORMAL
- en: Besides, in [[122](#biba.bibx52)], the agent does not directly transmit observation
    information but encodes the observation through a message communication network
    that is opposed to the action decision network. Message communication network
    of agents in [[122](#biba.bibx52)] applying a Discretization/Regularization Unit
    (DRU) to regularize the output during the training phase and discretize it during
    the execution phase so that the parameters of the communication network can be
    updated using gradient backpropagation.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the location information of mobile devices as agents is also considered
    as shared status information in the wireless communication MADRL system, such
    as [[81](#biba.bibx11)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-B Reward
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The reward function and its parameters intuitively reflect the impact of the
    agent’s decisions on the environment. Agents can better perceive their impact
    on the environment by exchanging rewards. For example, [[74](#biba.bibx4)] models
    the CLS system as a flow routing problem to achieve the minor consumption of system
    joint resources. The reward function used for sharing considers the number of
    flows in the current agent, the number of agents that each flow needs to pass
    through, and the number of virtual links connected to the agent. Similar work
    include [[101](#biba.bibx31)] and [[109](#biba.bibx39)].
  prefs: []
  type: TYPE_NORMAL
- en: '[[124](#biba.bibx54)] investigates maximizing the network overall rate by using
    power MADRL power control, with each cell’s in-cell sum rate (ICSR) as the agent’s
    reward. The simulation results indicate that sharing ICSR between agents can enhance
    cooperation and improve the overall rate. In addition, in wireless communication,
    agents can confirm the arrival of packets through feedback ACK messages, which
    is part of the reward function, such as in [[89](#biba.bibx19)].'
  prefs: []
  type: TYPE_NORMAL
- en: In Q-learning, the Q-value is the maximum reward that an agent can obtain in
    a particular state and specific action [[110](#biba.bibx40)]. Agents can achieve
    the global optimal joint action by sharing Q-values. In scenarios with jamming
    attacks, in [[128](#biba.bibx58)], each legitimate user, regarded as an agent,
    learns the best joint co-channel anti-interference strategy by sharing Q-values.
    Similarly, in the research anti-jamming attacks, the agents in [[125](#biba.bibx55)]
    share Q-tables instead of Q-values. Simulation results depict that by sharing
    Q-tables, agents can avoid co-channel jamming effectively, but the Q-table is
    about 1024 bytes, while each Q-value is only 1 bytes. Thus, agents only communicate
    with each other when the system is subjected to external co-channel jamming in
    [[125](#biba.bibx55)].
  prefs: []
  type: TYPE_NORMAL
- en: IV-C Mixed & Other Parameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The communication messages between agents may contain various parameters, not
    just sharing one of the states, actions, or rewards.
  prefs: []
  type: TYPE_NORMAL
- en: In the fully centralized MADRL model, such as [[119](#biba.bibx49)], [[113](#biba.bibx43)],
    and [[121](#biba.bibx51)], local agents submit observations to the central agent
    and receive action decisions information. Furthermore, [[119](#biba.bibx49)] uses
    DNN to compress the observations of each local agent in the vehicular network
    system and further enhances it through a quantization layer to reduce network
    signaling costs.
  prefs: []
  type: TYPE_NORMAL
- en: The authors of [[71](#biba.bibx1)] design two communication modes for agents,
    model sharing and value sharing. In model sharing, agents would share a copy of
    their machine learning model parameters, and agents would share their observations
    during value sharing mode. The system performance using model sharing is superior
    to value sharing in all aspects, but the size of each value sharing information
    is 8 Bytes, while the size of each target model update packet is 512 Bytes.
  prefs: []
  type: TYPE_NORMAL
- en: The authors in [[76](#biba.bibx6)] compared the effect of agents transmitting
    states, actions, and rewards concurrently in communication, as well as only exchanging
    partial information, on the overall spectrum utilization of the system. According
    to the simulation results, the system exhibited better performance with an increase
    in the number of types of communication parameters.
  prefs: []
  type: TYPE_NORMAL
- en: V Communication Object
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The communication object refers to whom the agents in the system determine to
    send messages to. Depending on the range of communication objects that can be
    reached, they are categorized as neighbor agents, all other agents in the system,
    or a central agent.
  prefs: []
  type: TYPE_NORMAL
- en: The summary is shown in Table V.
  prefs: []
  type: TYPE_NORMAL
- en: TABLE V
  prefs: []
  type: TYPE_NORMAL
- en: The Category of Communication Object
  prefs: []
  type: TYPE_NORMAL
- en: '| Types | Traffic Engineering | Network Access | Power Control | Network Security
    |'
  prefs: []
  type: TYPE_TB
- en: '| Neighbor | [[138](#biba.bibx68)]  [[129](#biba.bibx59)]  [[74](#biba.bibx4)]  [[71](#biba.bibx1)]  [[132](#biba.bibx62)]  [[87](#biba.bibx17)]
    | [[139](#biba.bibx69)]  [[122](#biba.bibx52)] | [[103](#biba.bibx33)]  [[136](#biba.bibx66)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[124](#biba.bibx54)]
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| All | [[89](#biba.bibx19)]  [[95](#biba.bibx25)]  [[92](#biba.bibx22)]  [[101](#biba.bibx31)]  [[109](#biba.bibx39)]
    | [[81](#biba.bibx11)]  [[76](#biba.bibx6)]  [[107](#biba.bibx37)] | [[90](#biba.bibx20)]  [[85](#biba.bibx15)]
    | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
  prefs: []
  type: TYPE_TB
- en: '| Central | - | [[119](#biba.bibx49)]  [[113](#biba.bibx43)] | - | [[121](#biba.bibx51)]
    |'
  prefs: []
  type: TYPE_TB
- en: V-A Neighbor Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In a wired network, network entities such as routers or AS typically have established
    upstream and downstream relationships in the network topology. In this environment
    with relatively fixed network topology, agents usually choose to share information
    with neighboring entities, considering their greater impact than other agents
    in the network, such as [[138](#biba.bibx68), [129](#biba.bibx59), [74](#biba.bibx4),
    [71](#biba.bibx1), [132](#biba.bibx62), [87](#biba.bibx17)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, in the wireless network, to ensure communication quality, agents
    are set to communicate with other agents that can establish stable communication
    channels within a specific range, such as [[139](#biba.bibx69), [103](#biba.bibx33),
    [136](#biba.bibx66), [102](#biba.bibx32), [80](#biba.bibx10), [124](#biba.bibx54),
    [122](#biba.bibx52)].
  prefs: []
  type: TYPE_NORMAL
- en: V-B All Agents
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Network entities can naturally communicate with each other on the Internet,
    and agents applied on the Internet can communicate with all other agents directly,
    such as in [[101](#biba.bibx31)], [[128](#biba.bibx58)] and [[125](#biba.bibx55)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition, in wireless communication environments, agents, such as UAVs or
    vehicles, broadcast their information to all other agents in the system due to
    device mobility or to simplify the model. For example, [[89](#biba.bibx19), [95](#biba.bibx25),
    [92](#biba.bibx22), [109](#biba.bibx39), [81](#biba.bibx11), [76](#biba.bibx6),
    [107](#biba.bibx37), [90](#biba.bibx20), [85](#biba.bibx15)].
  prefs: []
  type: TYPE_NORMAL
- en: Besides, through deep learning or gating mechanisms, agents can selectively
    transmit messages to all other agents within the system, such as [[117](#biba.bibx47),
    [84](#biba.bibx14), [118](#biba.bibx48)], to reduce communication costs.
  prefs: []
  type: TYPE_NORMAL
- en: V-C Central Agent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the fully centralized training and execution system, all agents only communicate
    with a central agent. The agents transmit their local information to the central
    agent and receive decision information from the center without communication with
    other local agents [[119](#biba.bibx49), [113](#biba.bibx43), [121](#biba.bibx51)].
    The central agent maintains real-time communication with all local agents, inevitably
    resulting in high communication costs. Therefore, in order to reduce communication
    costs, each agent in [[121](#biba.bibx51)] adds a Deep Deterministic Policy Gradient
    network to decide whether to send messages to the central agent. The central agent
    aggregates traffic information and makes joint action decisions that are allocated
    to several specific agents for execution.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, in order to enable each agent to observe the global environment
    in a distributed execution system, the agent communication algorithm using a central
    communication agent has been proposed [[105](#biba.bibx35), [86](#biba.bibx16),
    [99](#biba.bibx29), [116](#biba.bibx46)]. The central communication agent aggregates
    and encodes other agents’ messages as feedback, without outputting any action
    decisions. However, to our knowledge, this algorithm has not yet been applied
    to network management instances.
  prefs: []
  type: TYPE_NORMAL
- en: VI Communication Message Processing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Communication message processing refers to the process of an agent integrating
    messages that are received from other agents. Deep Neural Networks typically require
    a fixed-dimension state representation as input for iteration. Therefore, incoming
    messages from other agents must be integrated to match input dimensions. Several
    methods for aggregating messages include concatenation, averaging, attention mechanisms,
    and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, messages may be utilized as parameters for the reward function
    rather than inputs for neural networks. For example, in the wireless network,
    exchanging messages is to confirm whether the action is completed, such as in
    [[89](#biba.bibx19), [95](#biba.bibx25), [92](#biba.bibx22)]. Moreover, the message
    may be used as the parameters for calculating the reward function [[101](#biba.bibx31),
    [109](#biba.bibx39), [103](#biba.bibx33)]. Additionally, in the centralized system,
    the local agents receive action decisions from the central agent, which information
    would not need to be integrated [[119](#biba.bibx49), [113](#biba.bibx43), [121](#biba.bibx51)].
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we only focus on the message integration methods in agent communication
    and classify recent works into three categories, as shown in Table VI.
  prefs: []
  type: TYPE_NORMAL
- en: TABLE VI
  prefs: []
  type: TYPE_NORMAL
- en: The Category of Communication Message Processing
  prefs: []
  type: TYPE_NORMAL
- en: '| Types | Traffic Engineering | Network Access | Power Control | Network Security
    |'
  prefs: []
  type: TYPE_TB
- en: '| Concatenation | [[138](#biba.bibx68)]  [[129](#biba.bibx59)]  [[132](#biba.bibx62)]
    | [[139](#biba.bibx69)]  [[76](#biba.bibx6)] | [[90](#biba.bibx20)]  [[102](#biba.bibx32)]  [[80](#biba.bibx10)]  [[85](#biba.bibx15)]  [[124](#biba.bibx54)]
    | [[128](#biba.bibx58)]  [[125](#biba.bibx55)] |'
  prefs: []
  type: TYPE_TB
- en: '| Average | - | [[107](#biba.bibx37)] | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Neural Network | [[71](#biba.bibx1)]  [[87](#biba.bibx17)] | [[119](#biba.bibx49)]  [[122](#biba.bibx52)]
    | - | - |'
  prefs: []
  type: TYPE_TB
- en: VI-A Concatenation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Concatenation is a common message integration method, ensuring that messages
    are not lost during the integration process. However, the concatenation method
    does not consider the message weights of different agents, and it reduces the
    system’s scalability. The recent work that has used the concatenation method is
    presented in Table IV.
  prefs: []
  type: TYPE_NORMAL
- en: VI-B Average
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The average calculation is a simple information integration method for agents’
    communication, such as in [[107](#biba.bibx37)]. It reduces system complexity,
    but it is obvious that this approach overlooks the impact of the number of agents
    and their interdependence. Thus, it has not been widely used.
  prefs: []
  type: TYPE_NORMAL
- en: VI-C Neural Network
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Simple neural networks or deep neural networks are used to learn to integrate
    messages from other agents, such as [[71](#biba.bibx1), [119](#biba.bibx49), [122](#biba.bibx52)].
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, the agents in [[87](#biba.bibx17)] measure the weight of information
    with attention mechanism and use GCN to integrate the feature vectors in neighbor
    messages. Then, generate latent feature vectors as the interactive messages.
  prefs: []
  type: TYPE_NORMAL
- en: VII Communication Constraints
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a natural environment, various constraints, such as noise or limited bandwidth,
    can affect the communication performance between agents. However, currently, there
    is still relatively little research on how to address communication constraints
    between agents.
  prefs: []
  type: TYPE_NORMAL
- en: VII-A Limited Bandwidth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bandwidth is frequently restricted in the natural surroundings. When the available
    bandwidth is being fully utilized, the messages sent by the agents may not be
    transmitted on time, leading to a delay in the agents’ decision-making process.
    The network management literature studies the effect of limited system bandwidth
    on communication between multiple agents, including [[129](#biba.bibx59)] and
    [[121](#biba.bibx51)]. The experiment proves that the two works have better system
    performance in a limited Bandwidth environment compared to their baseline.
  prefs: []
  type: TYPE_NORMAL
- en: VII-B Noise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In communication, the influence of noise is inevitable. The network management
    literature which has considered the effects of noise on communication between
    agents includes [[92](#biba.bibx22), [74](#biba.bibx4), [81](#biba.bibx11), [119](#biba.bibx49),
    [122](#biba.bibx52)]. These works have been experimentally proven to have good
    robustness in systems under the influence of noise, but no denoising method has
    been proposed or applied.
  prefs: []
  type: TYPE_NORMAL
- en: VIII Open Issues and Future Research Direction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section discusses the issues and challenges of multi-agent communication
    in current network management. And briefly review future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-A Denoise
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Noise can corrupt data and reduce the reliability of communication between agents
    [[79](#biba.bibx9)]. However, existing research on multi-agent systems in network
    management has not given much attention to the impact of communication noise between
    agents. Moreover, there is a lack of research on denoising in multi-agent communication
    processes. Hence, the problem of noise and interference in communication among
    agents is a pressing issue that requires immediate attention and resolution.
  prefs: []
  type: TYPE_NORMAL
- en: Recent research has continuously explored the application of denoising with
    deep learning. Deep learning, especially CNN-based image-denoising methods, has
    been widely studied. Since many MARL environments can be characterized as graphs,
    the impact of noise on agent communication might also be optimized using image-denoising
    algorithms in the future.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-B Scalability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In scenarios where the number of agents varies over time, the dimension of the
    system state representation changes dynamically. However, Neural networks typically
    require fixed-dimension state representation, posing scalability issues for MADRL
    systems. Many existing studies use mean calculation, attention, or deep neural
    networks to aggregate messages and reduce them to a fixed size. However, this
    approach may not be applicable in all scenarios. When new agents are added to
    a system, it can increase the number of messages and require changes to the action
    space of other agents, which can pose a significant challenge to the system’s
    scalability.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-C Synchronization and Information Delay
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In distributed or decentralized multi-agent networks, ensuring synchronization
    of agent states during training is a common challenge. Agents in a system rely
    on communication with each other to make decisions and cooperate effectively.
    However, if messages are delayed or agents are out of sync, it can lead to decision
    delays and decreased overall system performance. This is because agents may use
    outdated information for training or make decisions. Therefore, it is important
    to ensure that agents can communicate effectively and in a timely manner to optimize
    system performance.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-D Limited Bandwidth
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the natural environment, bandwidth cannot be unlimited. Excessive messages
    from agents can result in delays in transmitting information, which further reduces
    the system’s performance. It is important to ensure that messages are transmitted
    efficiently in limited bandwidth. Emergent communication research seems to be
    an effective solution by allowing agents to self-learn to determine communication
    objects, content, or timing. However, the use of emergent communication in managing
    multi-agent networks is still relatively limited.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-E Readability of Emergent Communication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The messages of communication can emerge from agents through deep learning,
    such as in [[87](#biba.bibx17), [119](#biba.bibx49), [122](#biba.bibx52)], but
    the emergent messages may be unreadable to humans. This makes it difficult to
    evaluate communication effectiveness and formulate relevant specifications for
    industrial applications. Therefore, emergent message interpretation is one of
    the future research directions of agent communication.
  prefs: []
  type: TYPE_NORMAL
- en: VIII-F Communication Security
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the application scenario of network management, agents may contain privacy-sensitive
    information, such as user location, traffic, device power, etc., through sharing
    state or observation. If the agent containing privacy-sensitive information is
    left unprocessed, hackers may gain access to the entire system by attacking and
    controlling some agents. In this case, the information security issues in agent
    communication are worth studying.
  prefs: []
  type: TYPE_NORMAL
- en: IX Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In conclusion, we conducted a comprehensive survey of the application of MADRL
    in network management, especially a detailed analysis of research involving communication
    between multiple agents. First, we introduced the research contributions in current
    network management that involve communication between numerous agents. Then, we
    compared and classified agent communication work in detail from the aspects of
    learning and training schemes, communication objects, communication content, and
    communication constraints. Finally, we analyzed the challenges of Multi-agent
    communication applied in current network management and the future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This research is supported in part by the Huawei Technologies Co., Ltd., in
    part by the National Natural Science Foundation of China (Grant No. 92067109,
    61873119, 62211530106), and in part by Shenzhen Science and Technology Program
    (Grant No. ZDSYS20210623092007023, GJHZ20210705141808024).
  prefs: []
  type: TYPE_NORMAL
- en: '{refcontext}'
  prefs: []
  type: TYPE_NORMAL
- en: '[sorting = none]'
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Tianxu Li et al. “Applications of multi-agent reinforcement learning in
    future internet: A comprehensive survey” In *IEEE Communications Surveys & Tutorials*
    24.2 IEEE, 2022, pp. 1240–1279'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Georgios Papoudakis, Filippos Christianos, Arrasy Rahman and Stefano V
    Albrecht “Dealing with non-stationarity in multi-agent deep reinforcement learning”
    In *arXiv preprint arXiv:1906.04737*, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Mohamed Salah Zaiem and Etienne Bennequin “Learning to communicate in multi-agent
    reinforcement learning: A review” In *arXiv preprint arXiv:1911.05438*, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Amal Feriani and Ekram Hossain “Single and multi-agent deep reinforcement
    learning for AI-enabled wireless networks: A tutorial” In *IEEE Communications
    Surveys & Tutorials* 23.2 IEEE, 2021, pp. 1226–1252'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Yang Xiao, Jun Liu, Jiawei Wu and Nirwan Ansari “Leveraging deep reinforcement
    learning for traffic engineering: A survey” In *IEEE Communications Surveys &
    Tutorials* 23.4 IEEE, 2021, pp. 2064–2097'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Ibrahim Althamary, Chih-Wei Huang and Phone Lin “A survey on multi-agent
    reinforcement learning methods for vehicular networks” In *2019 15th International
    Wireless Communications & Mobile Computing Conference (IWCMC)*, 2019, pp. 1154–1159
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Changxi Zhu, Mehdi Dastani and Shihan Wang “A survey of multi-agent reinforcement
    learning with communication” In *arXiv preprint arXiv:2203.08975*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Marwa Chafii et al. “Emergent Communication in Multi-Agent Reinforcement
    Learning for Future Wireless Networks” In *IEEE Internet of Things Magazine* 6.4
    IEEE, 2023, pp. 18–24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Zheng Li and Caili Guo “Multi-agent deep reinforcement learning based spectrum
    allocation for D2D underlay communications” In *IEEE Transactions on Vehicular
    Technology* 69.2 IEEE, 2019, pp. 1828–1840'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Nan Zhao et al. “Deep reinforcement learning for user association and
    resource allocation in heterogeneous cellular networks” In *IEEE Transactions
    on Wireless Communications* 18.11 IEEE, 2019, pp. 5141–5152'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Xu Zhang et al. “Deep multi-agent reinforcement learning for resource
    allocation in D2D communication underlaying cellular networks” In *2020 21st Asia-Pacific
    Network Operations and Management Symposium (APNOMS)*, 2020, pp. 55–60 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Fan Meng, Peng Chen, Lenan Wu and Julian Cheng “Power allocation in multi-user
    cellular networks: Deep reinforcement learning approaches” In *IEEE Transactions
    on Wireless Communications* 19.10 IEEE, 2020, pp. 6255–6267'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Delin Guo, Lan Tang, Xinggan Zhang and Ying-Chang Liang “Joint optimization
    of handover control and power allocation based on multi-agent deep reinforcement
    learning” In *IEEE Transactions on Vehicular Technology* 69.11 IEEE, 2020, pp.
    13124–13138'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Amandeep Kaur and Krishan Kumar “Energy-efficient resource allocation
    in cognitive radio networks under cooperative multi-agent model-free reinforcement
    learning schemes” In *IEEE Transactions on Network and Service Management* 17.3
    IEEE, 2020, pp. 1337–1348'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Ning Yang, Haijun Zhang and Randall Berry “Partially observable multi-agent
    deep reinforcement learning for cognitive resource management” In *GLOBECOM 2020-2020
    IEEE Global Communications Conference*, 2020, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Lin Zhang and Ying-Chang Liang “Deep reinforcement learning for multi-agent
    power control in heterogeneous networks” In *IEEE Transactions on Wireless Communications*
    20.4 IEEE, 2020, pp. 2551–2564'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Akash Doshi et al. “A deep reinforcement learning framework for contention-based
    spectrum sharing” In *IEEE Journal on Selected Areas in Communications* 39.8 IEEE,
    2021, pp. 2526–2540'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Zheng Li, Caili Guo and Yidi Xuan “A multi-agent deep reinforcement learning
    based spectrum allocation framework for D2D communications” In *2019 IEEE Global
    Communications Conference (GLOBECOM)*, 2019, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Yi Yang et al. “Dynamic power allocation in cellular network based on
    multi-agent double deep reinforcement learning” In *Computer Networks* 217 Elsevier,
    2022, pp. 109342'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Fenglei Li, Zhixin Liu, Xinzhe Zhang and Yi Yang “Dynamic power allocation
    in IIoT based on multi-agent deep reinforcement learning” In *Neurocomputing*
    505 Elsevier, 2022, pp. 10–18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Yu Zhang et al. “Multi-agent deep reinforcement learning for secure UAV
    communications” In *2020 IEEE Wireless Communications and Networking Conference
    (WCNC)*, 2020, pp. 1–5 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Yu Zhang et al. “UAV-enabled secure communications by multi-agent deep
    reinforcement learning” In *IEEE Transactions on Vehicular Technology* 69.10 IEEE,
    2020, pp. 11599–11611'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Xiulin Qiu et al. “A data-driven packet routing algorithm for an unmanned
    aerial vehicle swarm: A multi-agent reinforcement learning approach” In *IEEE
    Wireless Communications Letters* 11.10 IEEE, 2022, pp. 2160–2164'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Alireza Shamsoshoara et al. “Distributed cooperative spectrum sharing
    in uav networks using multi-agent reinforcement learning” In *2019 16th IEEE Annual
    Consumer Communications & Networking Conference (CCNC)*, 2019, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Jingjing Cui, Yuanwei Liu and Arumugam Nallanathan “Multi-agent reinforcement
    learning-based resource allocation for UAV networks” In *IEEE Transactions on
    Wireless Communications* 19.2 IEEE, 2019, pp. 729–743'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Le Liang, Hao Ye and Geoffrey Ye Li “Spectrum sharing in vehicular networks
    based on multi-agent reinforcement learning” In *IEEE Journal on Selected Areas
    in Communications* 37.10 IEEE, 2019, pp. 2282–2292'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Khoi Khac Nguyen et al. “Distributed deep deterministic policy gradient
    for power allocation control in D2D-based V2V communications” In *IEEE Access*
    7 IEEE, 2019, pp. 164533–164543'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Mohit K Sharma, Alessio Zappone, Mérouane Debbah and Mohamad Assaad “Multi-agent
    deep reinforcement learning based power control for large energy harvesting networks”
    In *2019 International Symposium on Modeling and Optimization in Mobile, Ad Hoc,
    and Wireless Networks (WiOPT)*, 2019, pp. 1–7 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Adeb Salh et al. “Intelligent Resource Management Using Multiagent Double
    Deep Q-Networks to Guarantee Strict Reliability and Low Latency in IoT Network”
    In *IEEE Open Journal of the Communications Society* 3 IEEE, 2022, pp. 2245–2257'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Zawar Shah et al. “Routing protocols for mobile Internet of things (IoT):
    A survey on challenges and solutions” In *Electronics* 10.19 MDPI, 2021, pp. 2320'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Xiaoyang Zhao, Chuan Wu and Franck Le “Improving inter-domain routing
    through multi-agent reinforcement learning” In *IEEE INFOCOM 2020-IEEE Conference
    on Computer Communications Workshops (INFOCOM WKSHPS)*, 2020, pp. 1129–1134 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Redha A Alliche, Tiago Silva Barros, Ramon Aparicio-Pardo and Lucile Sassatelli
    “Impact evaluation of control signalling onto distributed learning-based packet
    routing” In *34th Intl. Teletraffic Congress, ITC 2022*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Jiechuan Jiang, Chen Dun, Tiejun Huang and Zongqing Lu “Graph convolutional
    reinforcement learning” In *arXiv preprint arXiv:1810.09202*, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Xinyu You et al. “Toward packet routing with fully distributed multiagent
    deep reinforcement learning” In *IEEE Transactions on Systems, Man, and Cybernetics:
    Systems* 52.2 IEEE, 2020, pp. 855–868'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Xingyan Chen et al. “A universal transcoding and transmission method for
    livecast with networked multi-agent reinforcement learning” In *IEEE INFOCOM 2021-IEEE
    Conference on Computer Communications*, 2021, pp. 1–10 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Aniket Modi et al. “Multi-Agent Packet Routing (MAPR): Co-Operative Packet
    Routing Algorithm with Multi-Agent Reinforcement Learning” In *2023 15th International
    Conference on COMmunication Systems & NETworkS (COMSNETS)*, 2023, pp. 722–730
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Wen Zhang et al. “Sac: A novel multi-hop routing policy in hybrid distributed
    iot system based on multi-agent reinforcement learning” In *2021 22nd International
    Symposium on Quality Electronic Design (ISQED)*, 2021, pp. 129–134 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Liang Dong, Yuchen Qian and Yuan Xing “Dynamic spectrum access and sharing
    through actor-critic deep reinforcement learning” In *EURASIP Journal on Wireless
    Communications and Networking* 2022.1 Springer, 2022, pp. 48'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Liang Wang, Hao Ye, Le Liang and Geoffrey Ye Li “Learn to compress CSI
    and allocate resources in vehicular networks” In *IEEE Transactions on Communications*
    68.6 IEEE, 2020, pp. 3640–3653'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Alperen Gündoğan, H Murat Gürsu, Volker Pauli and Wolfgang Kellerer “Distributed
    resource allocation with multi-agent deep reinforcement learning for 5G-V2V communication”
    In *Proceedings of the Twenty-First International Symposium on Theory, Algorithmic
    Foundations, and Protocol Design for Mobile Networks and Mobile Computing*, 2020,
    pp. 357–362'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Ping Xiang et al. “Multi-Agent Reinforcement Learning-Based Decentralized
    Spectrum Access in Vehicular Networks With Emergent Communication” In *IEEE Communications
    Letters* 27.1 IEEE, 2022, pp. 195–199'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Yuan Zhi et al. “Deep reinforcement learning-based resource allocation
    for D2D communications in heterogeneous cellular networks” In *Digital Communications
    and Networks* 8.5 Elsevier, 2022, pp. 834–842'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Errong Pei et al. “Intelligent Access to Unlicensed Spectrum: A Mean Field
    Based Deep Reinforcement Learning Approach” In *IEEE Transactions on Wireless
    Communications* 22.4 IEEE, 2022, pp. 2325–2337'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Ahmad Ali Khan and Raviraj S Adve “Centralized and distributed deep reinforcement
    learning methods for downlink sum-rate optimization” In *IEEE Transactions on
    Wireless Communications* 19.12 IEEE, 2020, pp. 8410–8426'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Di Zhao et al. “A reinforcement learning method for joint mode selection
    and power adaptation in the V2V communication network in 5G” In *IEEE Transactions
    on Cognitive Communications and Networking* 6.2 IEEE, 2020, pp. 452–463'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Navid Naderializadeh, Jaroslaw J Sydir, Meryem Simsek and Hosein Nikopour
    “Resource management in wireless networks via multi-agent deep reinforcement learning”
    In *IEEE Transactions on Wireless Communications* 20.6 IEEE, 2021, pp. 3507–3523'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Jonggyu Jang and Hyun Jong Yang “Deep reinforcement learning-based resource
    allocation and power control in small cells with limited information exchange”
    In *IEEE transactions on vehicular technology* 69.11 IEEE, 2020, pp. 13768–13783'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Kaidi Xu, Nguyen Van Huynh and Geoffrey Ye Li “Distributed-Training-and-Execution
    Multi-Agent Reinforcement Learning for Power Control in HetNet” In *IEEE Transactions
    on Communications* IEEE, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Yasar Sinan Nasir and Dongning Guo “Multi-agent deep reinforcement learning
    for dynamic power allocation in wireless networks” In *IEEE Journal on Selected
    Areas in Communications* 37.10 IEEE, 2019, pp. 2239–2250'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Bo Gu, Xu Zhang, Ziqi Lin and Mamoun Alazab “Deep multiagent reinforcement-learning-based
    resource allocation for internet of controllable things” In *IEEE Internet of
    Things Journal* 8.5 IEEE, 2020, pp. 3066–3074'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Fuqiang Yao and Luliang Jia “A collaborative multi-agent reinforcement
    learning anti-jamming algorithm in wireless networks” In *IEEE wireless communications
    letters* 8.4 IEEE, 2019, pp. 1024–1027'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Yifan Xu et al. “Interference-aware cooperative anti-jamming distributed
    channel selection in UAV communication networks” In *Applied Sciences* 8.10 MDPI,
    2018, pp. 1911'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Shi-Ming Xia et al. “A new smart router-throttling method to mitigate
    DDoS attacks” In *IEEE Access* 7 IEEE, 2019, pp. 107952–107963'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Alireza Shamsoshoara et al. “An autonomous spectrum management scheme
    for unmanned aerial vehicle networks in disaster relief operations” In *IEEE Access*
    8 IEEE, 2020, pp. 58064–58079'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Saeed Kaviani et al. “Robust and scalable routing with multi-agent deep
    reinforcement learning for MANETs” In *arXiv preprint arXiv:2101.03273*, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Xinge Li, Xiaoya Hu, Rongqing Zhang and Liuqing Yang “Routing protocol
    design for underwater optical wireless sensor networks: A multiagent reinforcement
    learning approach” In *IEEE Internet of Things Journal* 7.10 IEEE, 2020, pp. 9805–9818'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Chao Li and Jing Liu “A modified multi-agent reinforcement learning protocol
    based on prediction for UAANETs” In *2020 IEEE 92nd Vehicular Technology Conference
    (VTC2020-Fall)*, 2020, pp. 1–5 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Xiulin Qiu et al. “QLGR: A Q-learning-based Geographic FANET Routing Algorithm
    Based on Multiagent Reinforcement Learning.” In *KSII Transactions on Internet
    & Information Systems* 15.11, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Di Wu and Nirwan Ansari “High capacity spectrum allocation for multiple
    D2D users reusing downlink spectrum in LTE” In *2018 IEEE International Conference
    on Communications (ICC)*, 2018, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Zhufang Kuang, Gang Liu, Gongqiang Li and Xiaoheng Deng “Energy efficient
    resource allocation algorithm in energy harvesting-based D2D heterogeneous networks”
    In *IEEE Internet of Things Journal* 6.1 IEEE, 2018, pp. 557–567'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Jayesh K Gupta, Maxim Egorov and Mykel Kochenderfer “Cooperative multi-agent
    control using deep reinforcement learning” In *Autonomous Agents and Multiagent
    Systems: AAMAS 2017 Workshops, Best Papers, São Paulo, Brazil, May 8-12, 2017,
    Revised Selected Papers 16*, 2017, pp. 66–83 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Martin Riedmiller “Neural fitted Q iteration–first experiences with a
    data efficient neural reinforcement learning method” In *Machine Learning: ECML
    2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7,
    2005\. Proceedings 16*, 2005, pp. 317–328 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Amanpreet Singh, Tushar Jain and Sainbayar Sukhbaatar “Learning when to
    communicate at scale in multiagent cooperative and competitive tasks” In *arXiv
    preprint arXiv:1812.09755*, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Guangzheng Hu et al. “Event-triggered multi-agent reinforcement learning
    with communication under limited-bandwidth constraint” In *arXiv preprint arXiv:2010.04978*,
    2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Qingshuang Sun et al. “Learning controlled and targeted communication
    with the centralized critic for the multi-agent system” In *Applied Intelligence*
    53.12 Springer, 2023, pp. 14819–14837'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Yaru Niu, Rohan R Paleja and Matthew C Gombolay “Multi-Agent Graph-Attention
    Communication and Teaming.” In *AAMAS*, 2021, pp. 964–973'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Jiechuan Jiang and Zongqing Lu “Learning attentional communication for
    multi-agent cooperation” In *Advances in neural information processing systems*
    31, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Hangyu Mao et al. “Learning agent communication under limited bandwidth
    by message pruning” In *Proceedings of the AAAI Conference on Artificial Intelligence*
    34.04, 2020, pp. 5142–5149'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Junjie Sheng et al. “Learning structured communication for multi-agent
    reinforcement learning” In *Autonomous Agents and Multi-Agent Systems* 36.2 Springer,
    2022, pp. 50'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas and Shimon
    Whiteson “Learning to communicate with deep multi-agent reinforcement learning”
    In *Advances in neural information processing systems* 29, 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[71] Redha A Alliche, Tiago Silva Barros, Ramon Aparicio-Pardo and Lucile Sassatelli
    “Impact evaluation of control signalling onto distributed learning-based packet
    routing” In *34th Intl. Teletraffic Congress, ITC 2022*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Ibrahim Althamary, Chih-Wei Huang and Phone Lin “A survey on multi-agent
    reinforcement learning methods for vehicular networks” In *2019 15th International
    Wireless Communications & Mobile Computing Conference (IWCMC)*, 2019, pp. 1154–1159
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Marwa Chafii et al. “Emergent Communication in Multi-Agent Reinforcement
    Learning for Future Wireless Networks” In *IEEE Internet of Things Magazine* 6.4
    IEEE, 2023, pp. 18–24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Xingyan Chen et al. “A universal transcoding and transmission method for
    livecast with networked multi-agent reinforcement learning” In *IEEE INFOCOM 2021-IEEE
    Conference on Computer Communications*, 2021, pp. 1–10 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Jingjing Cui, Yuanwei Liu and Arumugam Nallanathan “Multi-agent reinforcement
    learning-based resource allocation for UAV networks” In *IEEE Transactions on
    Wireless Communications* 19.2 IEEE, 2019, pp. 729–743'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Liang Dong, Yuchen Qian and Yuan Xing “Dynamic spectrum access and sharing
    through actor-critic deep reinforcement learning” In *EURASIP Journal on Wireless
    Communications and Networking* 2022.1 Springer, 2022, pp. 48'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Akash Doshi et al. “A deep reinforcement learning framework for contention-based
    spectrum sharing” In *IEEE Journal on Selected Areas in Communications* 39.8 IEEE,
    2021, pp. 2526–2540'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Amal Feriani and Ekram Hossain “Single and multi-agent deep reinforcement
    learning for AI-enabled wireless networks: A tutorial” In *IEEE Communications
    Surveys & Tutorials* 23.2 IEEE, 2021, pp. 1226–1252'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas and Shimon
    Whiteson “Learning to communicate with deep multi-agent reinforcement learning”
    In *Advances in neural information processing systems* 29, 2016'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Bo Gu, Xu Zhang, Ziqi Lin and Mamoun Alazab “Deep multiagent reinforcement-learning-based
    resource allocation for internet of controllable things” In *IEEE Internet of
    Things Journal* 8.5 IEEE, 2020, pp. 3066–3074'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Alperen Gündoğan, H Murat Gürsu, Volker Pauli and Wolfgang Kellerer “Distributed
    resource allocation with multi-agent deep reinforcement learning for 5G-V2V communication”
    In *Proceedings of the Twenty-First International Symposium on Theory, Algorithmic
    Foundations, and Protocol Design for Mobile Networks and Mobile Computing*, 2020,
    pp. 357–362'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Delin Guo, Lan Tang, Xinggan Zhang and Ying-Chang Liang “Joint optimization
    of handover control and power allocation based on multi-agent deep reinforcement
    learning” In *IEEE Transactions on Vehicular Technology* 69.11 IEEE, 2020, pp.
    13124–13138'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Jayesh K Gupta, Maxim Egorov and Mykel Kochenderfer “Cooperative multi-agent
    control using deep reinforcement learning” In *Autonomous Agents and Multiagent
    Systems: AAMAS 2017 Workshops, Best Papers, São Paulo, Brazil, May 8-12, 2017,
    Revised Selected Papers 16*, 2017, pp. 66–83 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Guangzheng Hu et al. “Event-triggered multi-agent reinforcement learning
    with communication under limited-bandwidth constraint” In *arXiv preprint arXiv:2010.04978*,
    2020'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Jonggyu Jang and Hyun Jong Yang “Deep reinforcement learning-based resource
    allocation and power control in small cells with limited information exchange”
    In *IEEE transactions on vehicular technology* 69.11 IEEE, 2020, pp. 13768–13783'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Jiechuan Jiang and Zongqing Lu “Learning attentional communication for
    multi-agent cooperation” In *Advances in neural information processing systems*
    31, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Jiechuan Jiang, Chen Dun, Tiejun Huang and Zongqing Lu “Graph convolutional
    reinforcement learning” In *arXiv preprint arXiv:1810.09202*, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Amandeep Kaur and Krishan Kumar “Energy-efficient resource allocation
    in cognitive radio networks under cooperative multi-agent model-free reinforcement
    learning schemes” In *IEEE Transactions on Network and Service Management* 17.3
    IEEE, 2020, pp. 1337–1348'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Saeed Kaviani et al. “Robust and scalable routing with multi-agent deep
    reinforcement learning for MANETs” In *arXiv preprint arXiv:2101.03273*, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Ahmad Ali Khan and Raviraj S Adve “Centralized and distributed deep reinforcement
    learning methods for downlink sum-rate optimization” In *IEEE Transactions on
    Wireless Communications* 19.12 IEEE, 2020, pp. 8410–8426'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Zhufang Kuang, Gang Liu, Gongqiang Li and Xiaoheng Deng “Energy efficient
    resource allocation algorithm in energy harvesting-based D2D heterogeneous networks”
    In *IEEE Internet of Things Journal* 6.1 IEEE, 2018, pp. 557–567'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Chao Li and Jing Liu “A modified multi-agent reinforcement learning protocol
    based on prediction for UAANETs” In *2020 IEEE 92nd Vehicular Technology Conference
    (VTC2020-Fall)*, 2020, pp. 1–5 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Fenglei Li, Zhixin Liu, Xinzhe Zhang and Yi Yang “Dynamic power allocation
    in IIoT based on multi-agent deep reinforcement learning” In *Neurocomputing*
    505 Elsevier, 2022, pp. 10–18'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Tianxu Li et al. “Applications of multi-agent reinforcement learning in
    future internet: A comprehensive survey” In *IEEE Communications Surveys & Tutorials*
    24.2 IEEE, 2022, pp. 1240–1279'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Xinge Li, Xiaoya Hu, Rongqing Zhang and Liuqing Yang “Routing protocol
    design for underwater optical wireless sensor networks: A multiagent reinforcement
    learning approach” In *IEEE Internet of Things Journal* 7.10 IEEE, 2020, pp. 9805–9818'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Zheng Li and Caili Guo “Multi-agent deep reinforcement learning based
    spectrum allocation for D2D underlay communications” In *IEEE Transactions on
    Vehicular Technology* 69.2 IEEE, 2019, pp. 1828–1840'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Zheng Li, Caili Guo and Yidi Xuan “A multi-agent deep reinforcement learning
    based spectrum allocation framework for D2D communications” In *2019 IEEE Global
    Communications Conference (GLOBECOM)*, 2019, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Le Liang, Hao Ye and Geoffrey Ye Li “Spectrum sharing in vehicular networks
    based on multi-agent reinforcement learning” In *IEEE Journal on Selected Areas
    in Communications* 37.10 IEEE, 2019, pp. 2282–2292'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Hangyu Mao et al. “Learning agent communication under limited bandwidth
    by message pruning” In *Proceedings of the AAAI Conference on Artificial Intelligence*
    34.04, 2020, pp. 5142–5149'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Fan Meng, Peng Chen, Lenan Wu and Julian Cheng “Power allocation in multi-user
    cellular networks: Deep reinforcement learning approaches” In *IEEE Transactions
    on Wireless Communications* 19.10 IEEE, 2020, pp. 6255–6267'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Aniket Modi et al. “Multi-Agent Packet Routing (MAPR): Co-Operative Packet
    Routing Algorithm with Multi-Agent Reinforcement Learning” In *2023 15th International
    Conference on COMmunication Systems & NETworkS (COMSNETS)*, 2023, pp. 722–730
    IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Navid Naderializadeh, Jaroslaw J Sydir, Meryem Simsek and Hosein Nikopour
    “Resource management in wireless networks via multi-agent deep reinforcement learning”
    In *IEEE Transactions on Wireless Communications* 20.6 IEEE, 2021, pp. 3507–3523'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Yasar Sinan Nasir and Dongning Guo “Multi-agent deep reinforcement learning
    for dynamic power allocation in wireless networks” In *IEEE Journal on Selected
    Areas in Communications* 37.10 IEEE, 2019, pp. 2239–2250'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Khoi Khac Nguyen et al. “Distributed deep deterministic policy gradient
    for power allocation control in D2D-based V2V communications” In *IEEE Access*
    7 IEEE, 2019, pp. 164533–164543'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Yaru Niu, Rohan R Paleja and Matthew C Gombolay “Multi-Agent Graph-Attention
    Communication and Teaming.” In *AAMAS*, 2021, pp. 964–973'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Georgios Papoudakis, Filippos Christianos, Arrasy Rahman and Stefano
    V Albrecht “Dealing with non-stationarity in multi-agent deep reinforcement learning”
    In *arXiv preprint arXiv:1906.04737*, 2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Errong Pei et al. “Intelligent Access to Unlicensed Spectrum: A Mean
    Field Based Deep Reinforcement Learning Approach” In *IEEE Transactions on Wireless
    Communications* 22.4 IEEE, 2022, pp. 2325–2337'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Xiulin Qiu et al. “A data-driven packet routing algorithm for an unmanned
    aerial vehicle swarm: A multi-agent reinforcement learning approach” In *IEEE
    Wireless Communications Letters* 11.10 IEEE, 2022, pp. 2160–2164'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Xiulin Qiu et al. “QLGR: A Q-learning-based Geographic FANET Routing
    Algorithm Based on Multiagent Reinforcement Learning.” In *KSII Transactions on
    Internet & Information Systems* 15.11, 2021'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Martin Riedmiller “Neural fitted Q iteration–first experiences with a
    data efficient neural reinforcement learning method” In *Machine Learning: ECML
    2005: 16th European Conference on Machine Learning, Porto, Portugal, October 3-7,
    2005\. Proceedings 16*, 2005, pp. 317–328 Springer'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Adeb Salh et al. “Intelligent Resource Management Using Multiagent Double
    Deep Q-Networks to Guarantee Strict Reliability and Low Latency in IoT Network”
    In *IEEE Open Journal of the Communications Society* 3 IEEE, 2022, pp. 2245–2257'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Zawar Shah et al. “Routing protocols for mobile Internet of things (IoT):
    A survey on challenges and solutions” In *Electronics* 10.19 MDPI, 2021, pp. 2320'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Alireza Shamsoshoara et al. “An autonomous spectrum management scheme
    for unmanned aerial vehicle networks in disaster relief operations” In *IEEE Access*
    8 IEEE, 2020, pp. 58064–58079'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Alireza Shamsoshoara et al. “Distributed cooperative spectrum sharing
    in uav networks using multi-agent reinforcement learning” In *2019 16th IEEE Annual
    Consumer Communications & Networking Conference (CCNC)*, 2019, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Mohit K Sharma, Alessio Zappone, Mérouane Debbah and Mohamad Assaad “Multi-agent
    deep reinforcement learning based power control for large energy harvesting networks”
    In *2019 International Symposium on Modeling and Optimization in Mobile, Ad Hoc,
    and Wireless Networks (WiOPT)*, 2019, pp. 1–7 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Junjie Sheng et al. “Learning structured communication for multi-agent
    reinforcement learning” In *Autonomous Agents and Multi-Agent Systems* 36.2 Springer,
    2022, pp. 50'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Amanpreet Singh, Tushar Jain and Sainbayar Sukhbaatar “Learning when
    to communicate at scale in multiagent cooperative and competitive tasks” In *arXiv
    preprint arXiv:1812.09755*, 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Qingshuang Sun et al. “Learning controlled and targeted communication
    with the centralized critic for the multi-agent system” In *Applied Intelligence*
    53.12 Springer, 2023, pp. 14819–14837'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Liang Wang, Hao Ye, Le Liang and Geoffrey Ye Li “Learn to compress CSI
    and allocate resources in vehicular networks” In *IEEE Transactions on Communications*
    68.6 IEEE, 2020, pp. 3640–3653'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Di Wu and Nirwan Ansari “High capacity spectrum allocation for multiple
    D2D users reusing downlink spectrum in LTE” In *2018 IEEE International Conference
    on Communications (ICC)*, 2018, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Shi-Ming Xia et al. “A new smart router-throttling method to mitigate
    DDoS attacks” In *IEEE Access* 7 IEEE, 2019, pp. 107952–107963'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Ping Xiang et al. “Multi-Agent Reinforcement Learning-Based Decentralized
    Spectrum Access in Vehicular Networks With Emergent Communication” In *IEEE Communications
    Letters* 27.1 IEEE, 2022, pp. 195–199'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Yang Xiao, Jun Liu, Jiawei Wu and Nirwan Ansari “Leveraging deep reinforcement
    learning for traffic engineering: A survey” In *IEEE Communications Surveys &
    Tutorials* 23.4 IEEE, 2021, pp. 2064–2097'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Kaidi Xu, Nguyen Van Huynh and Geoffrey Ye Li “Distributed-Training-and-Execution
    Multi-Agent Reinforcement Learning for Power Control in HetNet” In *IEEE Transactions
    on Communications* IEEE, 2023'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Yifan Xu et al. “Interference-aware cooperative anti-jamming distributed
    channel selection in UAV communication networks” In *Applied Sciences* 8.10 MDPI,
    2018, pp. 1911'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Ning Yang, Haijun Zhang and Randall Berry “Partially observable multi-agent
    deep reinforcement learning for cognitive resource management” In *GLOBECOM 2020-2020
    IEEE Global Communications Conference*, 2020, pp. 1–6 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Yi Yang et al. “Dynamic power allocation in cellular network based on
    multi-agent double deep reinforcement learning” In *Computer Networks* 217 Elsevier,
    2022, pp. 109342'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Fuqiang Yao and Luliang Jia “A collaborative multi-agent reinforcement
    learning anti-jamming algorithm in wireless networks” In *IEEE wireless communications
    letters* 8.4 IEEE, 2019, pp. 1024–1027'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Xinyu You et al. “Toward packet routing with fully distributed multiagent
    deep reinforcement learning” In *IEEE Transactions on Systems, Man, and Cybernetics:
    Systems* 52.2 IEEE, 2020, pp. 855–868'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Mohamed Salah Zaiem and Etienne Bennequin “Learning to communicate in
    multi-agent reinforcement learning: A review” In *arXiv preprint arXiv:1911.05438*,
    2019'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Lin Zhang and Ying-Chang Liang “Deep reinforcement learning for multi-agent
    power control in heterogeneous networks” In *IEEE Transactions on Wireless Communications*
    20.4 IEEE, 2020, pp. 2551–2564'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Wen Zhang et al. “Sac: A novel multi-hop routing policy in hybrid distributed
    iot system based on multi-agent reinforcement learning” In *2021 22nd International
    Symposium on Quality Electronic Design (ISQED)*, 2021, pp. 129–134 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Xu Zhang et al. “Deep multi-agent reinforcement learning for resource
    allocation in D2D communication underlaying cellular networks” In *2020 21st Asia-Pacific
    Network Operations and Management Symposium (APNOMS)*, 2020, pp. 55–60 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Yu Zhang et al. “Multi-agent deep reinforcement learning for secure UAV
    communications” In *2020 IEEE Wireless Communications and Networking Conference
    (WCNC)*, 2020, pp. 1–5 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Yu Zhang et al. “UAV-enabled secure communications by multi-agent deep
    reinforcement learning” In *IEEE Transactions on Vehicular Technology* 69.10 IEEE,
    2020, pp. 11599–11611'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Di Zhao et al. “A reinforcement learning method for joint mode selection
    and power adaptation in the V2V communication network in 5G” In *IEEE Transactions
    on Cognitive Communications and Networking* 6.2 IEEE, 2020, pp. 452–463'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Nan Zhao et al. “Deep reinforcement learning for user association and
    resource allocation in heterogeneous cellular networks” In *IEEE Transactions
    on Wireless Communications* 18.11 IEEE, 2019, pp. 5141–5152'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Xiaoyang Zhao, Chuan Wu and Franck Le “Improving inter-domain routing
    through multi-agent reinforcement learning” In *IEEE INFOCOM 2020-IEEE Conference
    on Computer Communications Workshops (INFOCOM WKSHPS)*, 2020, pp. 1129–1134 IEEE'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Yuan Zhi et al. “Deep reinforcement learning-based resource allocation
    for D2D communications in heterogeneous cellular networks” In *Digital Communications
    and Networks* 8.5 Elsevier, 2022, pp. 834–842'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Changxi Zhu, Mehdi Dastani and Shihan Wang “A survey of multi-agent reinforcement
    learning with communication” In *arXiv preprint arXiv:2203.08975*, 2022'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
