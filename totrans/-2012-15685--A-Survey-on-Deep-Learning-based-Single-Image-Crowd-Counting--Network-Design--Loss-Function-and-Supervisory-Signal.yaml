- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:57:42'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:57:42
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2012.15685] A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2012.15685] 基于深度学习的单图像人群计数调查：网络设计、损失函数和监督信号'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2012.15685](https://ar5iv.labs.arxiv.org/html/2012.15685)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2012.15685](https://ar5iv.labs.arxiv.org/html/2012.15685)
- en: '[type=editor, orcid=0000-0001-8139-0431 ]'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '[type=editor, orcid=0000-0001-8139-0431 ]'
- en: 1]organization=The Hong Kong University of Science and Technology, city=Hong
    Kong
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 1]组织=香港科技大学，城市=香港
- en: 2]organization=The Chinese University of Hong Kong, city=Hong Kong
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2]组织=香港中文大学，城市=香港
- en: 'A Survey on Deep Learning-based Single Image Crowd Counting: Network Design,
    Loss Function and Supervisory Signal'
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的单图像人群计数调查：网络设计、损失函数和监督信号
- en: Haoyue Bai [    Jiageng Mao [    S.-H. Gary Chan
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 白浩悦 [    毛佳耕 [    陈思豪
- en: Abstract
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Single image crowd counting is a challenging computer vision problem with wide
    applications in public safety, city planning, traffic management, etc. With the
    recent development of deep learning techniques, crowd counting has aroused much
    attention and achieved great success in recent years. This survey is to provide
    a comprehensive summary of recent advances on deep learning-based crowd counting
    techniques via density map estimation by systematically reviewing and summarizing
    more than 200 works in the area since 2015. Our goals are to provide an up-to-date
    review of recent approaches, and educate new researchers in this field the design
    principles and trade-offs. After presenting publicly available datasets and evaluation
    metrics, we review the recent advances with detailed comparisons on three major
    design modules for crowd counting: deep neural network designs, loss functions,
    and supervisory signals. We study and compare the approaches using the public
    datasets and evaluation metrics. We conclude the survey with some future directions.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 单图像人群计数是一个具有挑战性的计算机视觉问题，在公共安全、城市规划、交通管理等领域有广泛应用。随着深度学习技术的最新发展，人群计数引起了广泛关注，并在近年来取得了巨大成功。本调查旨在通过系统回顾和总结自2015年以来该领域200多项工作的进展，提供基于深度学习的人群计数技术在密度图估计方面的全面总结。我们的目标是提供最新的方法综述，并教育该领域的新研究人员设计原则和权衡。在介绍公开数据集和评估指标之后，我们详细比较了人群计数的三大设计模块：深度神经网络设计、损失函数和监督信号。我们使用公开数据集和评估指标对这些方法进行了研究和比较。最后，我们总结了未来的研究方向。
- en: 'keywords:'
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Crowd Counting \sepNetwork Design \sepLoss Function \sepSupervisory Signal
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 人群计数 \sep 网络设计 \sep 损失函数 \sep 监督信号
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Single image crowd counting is to estimate the number of objects (people, cars,
    cells, etc.) in an image of an unconstrained scene, i.e., an image without any
    restriction on the scene. Crowd counting has attracted much attention in recent
    years due to its important applications in public safety, traffic management,
    consumer behavior, cell counting, etc. [[131](#bib.bib131), [73](#bib.bib73),
    [12](#bib.bib12)]. In this survey, we mainly focus on people as the crowd, though
    the techniques discussed may be extended to other domains.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 单图像人群计数是指在无约束场景的图像中估计物体（人、车、细胞等）的数量，即在没有任何场景限制的图像中进行计数。由于其在公共安全、交通管理、消费者行为、细胞计数等方面的重要应用，人群计数近年来引起了广泛关注[[131](#bib.bib131)、[73](#bib.bib73)、[12](#bib.bib12)]。在这项调查中，我们主要关注将人视为人群，尽管所讨论的技术可能扩展到其他领域。
- en: Due to the importance of crowd counting, extensive research have been done in
    the area, especially with the use of deep learning, which has demonstrated superior
    performances on various applications, such as computer vision [[50](#bib.bib50),
    [117](#bib.bib117), [118](#bib.bib118)], image classification [[69](#bib.bib69)],
    and multi-dimensional time series [[5](#bib.bib5)]. Deep learning achieves success
    for single image crowd counting with large-scale publicly available benchmarks [[60](#bib.bib60),
    [185](#bib.bib185)] in recent years. This may be due to its data-driven properties [[228](#bib.bib228),
    [80](#bib.bib80)] and capability of self-learning from raw data [[103](#bib.bib103),
    [148](#bib.bib148)] for deep learning-based methods. In this work, we mainly discuss
    recent advanced deep learning-based single image crowd counting approaches due
    to its superiority in comparison to machine learning models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人群计数的重要性，该领域已经进行了广泛的研究，特别是深度学习的应用显示出在计算机视觉[[50](#bib.bib50)、[117](#bib.bib117)、[118](#bib.bib118)]、图像分类[[69](#bib.bib69)]和多维时间序列[[5](#bib.bib5)]等各种应用中表现优越。近年来，深度学习在大规模公开基准数据集[[60](#bib.bib60)、[185](#bib.bib185)]上成功实现了单幅图像人群计数。这可能归因于其数据驱动特性[[228](#bib.bib228)、[80](#bib.bib80)]和从原始数据中自我学习的能力[[103](#bib.bib103)、[148](#bib.bib148)]。在本研究中，我们主要讨论最近先进的深度学习基于单幅图像的人群计数方法，因为它相对于机器学习模型具有优越性。
- en: Early approaches to count people are based on detection-based computer vision
    techniques, which are to detect individual objects, heads, or body parts and then
    count the total number in the image [[135](#bib.bib135), [86](#bib.bib86), [76](#bib.bib76)].
    However, its accuracy deteriorates quickly for crowded scenes where objects have
    severe occlusions. To overcome it, the regression-based approach has been recently
    proposed, which directly estimates the count by relating it with the image. While
    achieving higher accuracy than the detection-based approach for crowded scenes,
    it lacks adequate spatial information of the people and is less interpretable [[14](#bib.bib14),
    [177](#bib.bib177), [13](#bib.bib13)], hindering its extension to localization
    study.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 早期的人群计数方法基于基于检测的计算机视觉技术，即检测个体物体、头部或身体部位，然后统计图像中的总人数[[135](#bib.bib135)、[86](#bib.bib86)、[76](#bib.bib76)]。然而，这种方法在物体严重遮挡的拥挤场景中精度迅速下降。为了解决这一问题，最近提出了基于回归的方法，该方法通过将计数与图像相关联直接估计计数。尽管在拥挤场景中比基于检测的方法具有更高的准确性，但它缺乏足够的空间信息，且不易解释[[14](#bib.bib14)、[177](#bib.bib177)、[13](#bib.bib13)]，这限制了它在定位研究中的扩展。
- en: '![Refer to caption](img/f667b8b9e27c145f7a27b859a3ff957f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f667b8b9e27c145f7a27b859a3ff957f.png)'
- en: 'Figure 1: The structure of this survey. First, we overview the four main categories
    of deep learning-based crowd counting methods. Second, we present publicly available
    counting datasets and evaluation metrics. Then, we review recent advances on crowd
    counting schemes, which is mainly pertain to deep neural network design, loss
    function and supervisory signal. We conclude the survey with future directions.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：本调查的结构。首先，我们概述了基于深度学习的人群计数方法的四个主要类别。其次，我们介绍了公开可用的计数数据集和评估指标。然后，我们回顾了人群计数方案的最新进展，主要涉及深度神经网络设计、损失函数和监督信号。最后，我们总结了未来的研究方向。
- en: Most recently, crowd counting via density map estimation has emerged as a promising
    approach with encouraging results, where the input image is processed to a crowd
    density map, which is simply integrated to obtain the number of people in a pixel
    of the image  [[73](#bib.bib73), [133](#bib.bib133), [7](#bib.bib7), [11](#bib.bib11),
    [228](#bib.bib228), [97](#bib.bib97), [80](#bib.bib80), [58](#bib.bib58)]. Such
    approaches achieve high accuracy for crowded scenes and preserve spatial information
    of people distribution. Besides, there are some emerging approaches such as S-DCNet [[186](#bib.bib186)]
    which classifies the features into a predefined count range for crowd estimation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，通过密度图估计进行的人群计数作为一种有前景的方法出现，并取得了令人鼓舞的结果，在这种方法中，输入图像被处理为人群密度图，通过简单的集成来获得图像中每个像素的人员数量[[73](#bib.bib73)、[133](#bib.bib133)、[7](#bib.bib7)、[11](#bib.bib11)、[228](#bib.bib228)、[97](#bib.bib97)、[80](#bib.bib80)、[58](#bib.bib58)]。这种方法在拥挤场景中实现了高精度，并保持了人员分布的空间信息。此外，还有一些新兴的方法，如S-DCNet[[186](#bib.bib186)]，它将特征分类到预定义的计数范围内以进行人群估计。
- en: 'We summarize by comparing the four major crowd counting approaches in Table [1](#S1.T1
    "Table 1 ‣ 1 Introduction ‣ A Survey on Deep Learning-based Single Image Crowd
    Counting: Network Design, Loss Function and Supervisory Signal"). All of them
    require image annotation through labeling in the training step. For detection-based
    approach, each object has to be fully identified and outlined, which incurs the
    highest labeling cost. On the other hand, regression-based approach does not need
    to annotate individual objects but the total object count, and hence its annotation
    cost is the lowest. Density estimation has an intermediate labeling cost between
    the two because only the heads of the people need to be indicated.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '我们通过比较表 [1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")
    中的四种主要人群计数方法进行总结。所有方法都需要通过标注进行图像注释。在基于检测的方法中，每个对象必须被完全识别和勾勒，这会产生最高的标注成本。另一方面，基于回归的方法不需要标注单个对象，而只需标注总对象数，因此其标注成本最低。密度估计的标注成本介于两者之间，因为只需指示出人头。'
- en: 'We focus in this survey on crowd counting via density map estimation. With
    the development of deep learning approaches in the field of computer vision, counting
    accuracy has been greatly improved with the use of deep learning-based models
    as compared with approaches based on handcrafted features. We overview in Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning-based Single Image Crowd
    Counting: Network Design, Loss Function and Supervisory Signal") (a) the major
    design components for CNN-based crowd counting via density map estimation. An
    input image of a crowd scene is fed into a deep neural network which estimates
    the density map of the image (the upper branch). Here the critical issue is the
    network design so that the sum of the density value in all the pixels closely
    matches with the crowd count in the input image. For training (the lower branch),
    an image is first annotated with supervisory signal, which may range from fully
    to pseudo labeled, to generate the ground truth (given by the number of people
    per pixel in the image). The ground truth is used to adjust the node parameters
    of the deep neural network through minimizing a loss function between the network-generated
    density map and the ground truth.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '在这项调查中，我们专注于通过密度图估计进行人群计数。随着深度学习方法在计算机视觉领域的发展，与基于手工特征的方法相比，使用基于深度学习的模型的计数准确性得到了极大的提升。我们在图 [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning-based Single Image Crowd
    Counting: Network Design, Loss Function and Supervisory Signal") (a) 中概述了基于 CNN
    的人群计数通过密度图估计的主要设计组件。将人群场景的输入图像输入到深度神经网络中，该网络估计图像的密度图（上支路）。这里的关键问题是网络设计，使得所有像素的密度值之和与输入图像中的人群计数紧密匹配。对于训练（下支路），首先对图像进行标注，以生成地面真实值（由图像中每像素的人数给出），标注可以是完全标记的或伪标记的。地面真实值用于通过最小化网络生成的密度图与地面真实值之间的损失函数来调整深度神经网络的节点参数。'
- en: We present recent advances on deep learning-based crowd counting. Our goals
    are to educate the new researchers state-of-the-arts and equip them with insights,
    tools, and principles to design novel networks. We survey and compare the available
    datasets, performance metrics, network design, loss function, and supervisory
    signal. Our survey is timely and unique.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了基于深度学习的人群计数的最新进展。我们的目标是向新研究人员介绍最先进的技术，并为他们提供设计新型网络的见解、工具和原则。我们调查并比较了现有的数据集、性能指标、网络设计、损失函数和监督信号。我们的调查是及时且独特的。
- en: We discuss related work as follows. Teixeira et al. [[167](#bib.bib167)] is
    an early survey on human sensing. However, it has not focused on crowd scene analysis.
    Li et al. [[77](#bib.bib77)] reviews crowd scene analysis in terms of crowd behavior,
    activity analysis, and anomaly detection, with crowd counting playing a small
    role. Ahuja et al. [[2](#bib.bib2)] covers different crowd estimation methods.
    Though Zitouni et al. [[239](#bib.bib239)] evaluate different crowd analysis methods,
    is not mainly for CNN-based approach via density map estimation, which has become
    the mainstream for crowd counting in recent years. Chrysler et al. [[26](#bib.bib26)]
    discusses the methods to tackle the challenges of the lack of training data, perspective
    distortion faced by the crowd counting system. The work [[158](#bib.bib158)] surveys
    on CNN-based approach for a single image, but it only roughly discussed the recent
    advances on CNN-based methods. It has not discussed the advanced convolutional
    operations and attention-based model, loss function, and supervisory signal, and
    only up to the year 2017.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来讨论相关工作。Teixeira 等人 [[167](#bib.bib167)] 是关于人类感知的早期调查。然而，它并未专注于拥挤场景分析。Li
    等人 [[77](#bib.bib77)] 综述了拥挤场景分析，包括拥挤行为、活动分析和异常检测，其中拥挤计数的作用较小。Ahuja 等人 [[2](#bib.bib2)]
    涵盖了不同的拥挤估计方法。尽管 Zitouni 等人 [[239](#bib.bib239)] 评估了不同的拥挤分析方法，但并非主要针对基于密度图估计的 CNN
    方法，而该方法近年来已成为拥挤计数的主流。Chrysler 等人 [[26](#bib.bib26)] 讨论了应对缺乏训练数据和视角失真的方法，这些都是拥挤计数系统面临的挑战。工作
    [[158](#bib.bib158)] 综述了针对单张图像的 CNN 方法，但仅粗略讨论了 CNN 方法的最新进展，未涉及先进的卷积操作、基于注意力的模型、损失函数和监督信号，仅到2017年。
- en: 'Table 1: Summary of crowd counting approaches on four major categories: detection-based,
    regression-based, density map estimation, and emerging approaches.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 四大类拥挤计数方法的总结：基于检测、基于回归、密度图估计和新兴方法。'
- en: '| Category | Principles |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 原则 |'
- en: '&#124; Crowd &#124;'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 拥挤 &#124;'
- en: '&#124; Counting &#124;'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计数 &#124;'
- en: '&#124; Accuracy &#124;'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确度 &#124;'
- en: '|'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Location &#124;'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 位置 &#124;'
- en: '&#124; Accuracy &#124;'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 准确度 &#124;'
- en: '|'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Annotation &#124;'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注释 &#124;'
- en: '&#124; Complexity &#124;'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 复杂度 &#124;'
- en: '| Limitations | Examples |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 局限性 | 示例 |'
- en: '|'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Detection &#124;'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '&#124; based &#124;'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于 &#124;'
- en: '|'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Detect &#124;'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测 &#124;'
- en: '&#124; then count; &#124;'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 然后计数; &#124;'
- en: '&#124; early approach &#124;'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 早期方法 &#124;'
- en: '| Low | High |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 低 | 高 |'
- en: '&#124; High &#124;'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高 &#124;'
- en: '&#124; (object framing) &#124;'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (对象框定) &#124;'
- en: '|'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Low accuracy &#124;'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低准确度 &#124;'
- en: '&#124; for highly &#124;'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对于高 &#124;'
- en: '&#124; crowded scenes &#124;'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 拥挤场景 &#124;'
- en: '|'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[135](#bib.bib135)], [[86](#bib.bib86)], &#124;'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[135](#bib.bib135)], [[86](#bib.bib86)], &#124;'
- en: '&#124; [[76](#bib.bib76)] &#124;'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[76](#bib.bib76)] &#124;'
- en: '|'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Regression &#124;'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回归 &#124;'
- en: '&#124; based &#124;'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于 &#124;'
- en: '|'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Directly learn &#124;'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 直接学习 &#124;'
- en: '&#124; to regress &#124;'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 回归 &#124;'
- en: '&#124; the count &#124;'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计数 &#124;'
- en: '| Medium | N/A |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 中等 | 不适用 |'
- en: '&#124; Low &#124;'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低 &#124;'
- en: '&#124; (image-level) &#124;'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (图像级) &#124;'
- en: '|'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Less interpretable; &#124;'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 较少可解释; &#124;'
- en: '&#124; lacks location &#124;'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 缺少位置 &#124;'
- en: '&#124; information &#124;'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息 &#124;'
- en: '|'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[14](#bib.bib14)], [[177](#bib.bib177)], &#124;'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[14](#bib.bib14)], [[177](#bib.bib177)], &#124;'
- en: '&#124; [[13](#bib.bib13)] &#124;'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[13](#bib.bib13)] &#124;'
- en: '|'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Density map &#124;'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 密度图 &#124;'
- en: '&#124; estimation &#124;'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 估计 &#124;'
- en: '|'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Compute number &#124;'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算数量 &#124;'
- en: '&#124; of people &#124;'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人数 &#124;'
- en: '&#124; per pixel &#124;'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 每像素 &#124;'
- en: '| High | Medium |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 高 | 中等 |'
- en: '&#124; Medium &#124;'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 中等 &#124;'
- en: '&#124; (head indication) &#124;'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (头部指示) &#124;'
- en: '|'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Low accuracy &#124;'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低准确度 &#124;'
- en: '&#124; in low &#124;'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在低 &#124;'
- en: '&#124; crowd scenes &#124;'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 拥挤场景 &#124;'
- en: '|'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[73](#bib.bib73)], [[133](#bib.bib133)], &#124;'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[73](#bib.bib73)], [[133](#bib.bib133)], &#124;'
- en: '&#124; [[11](#bib.bib11)], [[228](#bib.bib228)], &#124;'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[11](#bib.bib11)], [[228](#bib.bib228)], &#124;'
- en: '&#124; [[97](#bib.bib97)], [[80](#bib.bib80)] &#124;'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; [[97](#bib.bib97)], [[80](#bib.bib80)] &#124;'
- en: '|'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Emerging &#124;'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 新兴 &#124;'
- en: '&#124; approaches &#124;'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法 &#124;'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Classify the &#124;'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 分类 &#124;'
- en: '&#124; features into &#124;'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征到 &#124;'
- en: '&#124; a predefined &#124;'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 预定义的 &#124;'
- en: '&#124; count range &#124;'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计数范围 &#124;'
- en: '| High | Low |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 高 | 低 |'
- en: '&#124; Medium &#124;'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 中等 &#124;'
- en: '&#124; (head indication) &#124;'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; (头部指示) &#124;'
- en: '|'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Not flexible &#124;'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不灵活 &#124;'
- en: '&#124; to wide &#124;'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 到宽 &#124;'
- en: '&#124; count range &#124;'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计数范围 &#124;'
- en: '|'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124;  [[205](#bib.bib205)] &#124;'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124;  [[205](#bib.bib205)] &#124;'
- en: '|'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '![Refer to caption](img/e4151c7f9fcc61695198b19570423e15.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/e4151c7f9fcc61695198b19570423e15.png)'
- en: 'Figure 2: Overview of deep learning-based single image crowd counting methods
    via density map estimation. Figure (a) shows the major components for deep learning-based
    crowd counting via density estimation. Figure (b) presents visualization of original
    image, labor-intensive dense annotation, ground truth density maps, and image-level
    weak annotation. The annotation paradigms are from [[159](#bib.bib159)].'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：基于深度学习的单张图像人群计数方法概述，通过密度图估计。图（a）展示了基于深度学习的人群计数的主要组成部分。图（b）展示了原始图像、劳动密集型密集注释、真实密度图和图像级弱注释的可视化。注释范式来源于
    [[159](#bib.bib159)]。
- en: 'Table 2: A comprehensive analysis of other counting related survey papers.
    Compared with previous related works, our work is of current interest and value,
    because it is timely, more comprehensive and provide an in-depth analysis of the
    representative approaches in this active area.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：对其他计数相关综述论文的全面分析。与之前相关工作相比，我们的工作具有当前的兴趣和价值，因为它是及时的、更全面，并对这一活跃领域的代表性方法进行了深入分析。
- en: '| Paper | Year | Venue | Comparison of Other Crowd Counting Surveys |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 论文 | 年份 | 发表场所 | 与其他人群计数调查的比较 |'
- en: '|'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Approaches on Crowd Counting and &#124;'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人群计数方法和 &#124;'
- en: '&#124; Density Estimation: A Review [[74](#bib.bib74)] &#124;'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 密度估计：综述 [[74](#bib.bib74)] &#124;'
- en: '| 2021 | PAA |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | PAA |'
- en: '&#124; This paper focus on elaborating deep &#124;'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文重点阐述深度 &#124;'
- en: '&#124; learning-based counting methods, which &#124;'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于学习的计数方法，这 &#124;'
- en: '&#124; is board-based and mainly focus on the network &#124;'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于广泛的内容，主要关注网络 &#124;'
- en: '&#124; design considerations without discussing loss &#124;'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设计考虑而未讨论损失 &#124;'
- en: '&#124; functions and supervisory signals. &#124;'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 功能和监督信号。 &#124;'
- en: '|'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A Literature Review of &#124;'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 文献综述 &#124;'
- en: '&#124; Crowd-counting System on &#124;'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人群计数系统 &#124;'
- en: '&#124; Convolutional Neural Network [[26](#bib.bib26)] &#124;'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积神经网络 [[26](#bib.bib26)] &#124;'
- en: '| 2021 | IOPCS |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 2021 | IOPCS |'
- en: '&#124; This survey discusses the challenges faced by crowd &#124;'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本次调查讨论了人群面临的挑战 &#124;'
- en: '&#124; counting systems and focuses on developing a more robust &#124;'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计数系统，并重点发展更为健壮的 &#124;'
- en: '&#124; crowd counting methodology. However, this survey is &#124;'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人群计数方法。然而，这项调查 &#124;'
- en: '&#124; a short paper. The network design discussion misses &#124;'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 短篇论文。网络设计讨论缺少 &#124;'
- en: '&#124; some important recent approaches such as &#124;'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 一些重要的近期方法，如 &#124;'
- en: '&#124; DM-Count, SASNet. It also lacks unsupervised &#124;'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; DM-Count，SASNet。它还缺少无监督 &#124;'
- en: '&#124; learning counting approaches. &#124;'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习计数方法。 &#124;'
- en: '|'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A Survey of Recent Advances &#124;'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 近期进展综述 &#124;'
- en: '&#124; in Crowd Density Estimation &#124;'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在人群密度估计中的应用 &#124;'
- en: '&#124; using Image Processing [[2](#bib.bib2)] &#124;'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 使用图像处理 [[2](#bib.bib2)] &#124;'
- en: '| 2019 | ICCES |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 2019 | ICCES |'
- en: '&#124; This is a short paper, which mainly discusses &#124;'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文是一篇简短的论文，主要讨论 &#124;'
- en: '&#124; the traditional approaches with &#124;'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传统方法与 &#124;'
- en: '&#124; hand-crafted features. Deep learning-based &#124;'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 手工特征。基于深度学习的 &#124;'
- en: '&#124; approaches only play a small part. &#124;'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法只占很小一部分。 &#124;'
- en: '|'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A Survey of Techniques &#124;'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 技术综述 &#124;'
- en: '&#124; for Automatically Sensing &#124;'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动感知 &#124;'
- en: '&#124; the Behavior of a Crowd [[33](#bib.bib33)] &#124;'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人群行为 [[33](#bib.bib33)] &#124;'
- en: '| 2018 | ACMCS |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | ACMCS |'
- en: '&#124; This paper surveys practical solutions for sensing &#124;'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文调查了感知 &#124;'
- en: '&#124; pedestrian behavior, and also combining privacy, transparency, &#124;'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 行为，并结合隐私、透明度， &#124;'
- en: '&#124; scalability, and ease of deployment. However, this paper is for &#124;'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可扩展性和部署的便利性。然而，这篇论文是针对 &#124;'
- en: '&#124; traditional methods with hand-crafted features. &#124;'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 传统方法与手工特征。 &#124;'
- en: '|'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A Survey of Recent Advances &#124;'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 近期进展综述 &#124;'
- en: '&#124; in CNN-based Single Image &#124;'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于CNN的单张图像 &#124;'
- en: '&#124; Crowd Counting and &#124;'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人群计数和 &#124;'
- en: '&#124; Density Estimation [[158](#bib.bib158)] &#124;'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 密度估计 [[158](#bib.bib158)] &#124;'
- en: '| 2018 | PRL |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 2018 | PRL |'
- en: '&#124; This paper surveys CNN-based crowd counting approaches &#124;'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文调查了基于CNN的人群计数方法 &#124;'
- en: '&#124; for a single image, but it only roughly discussed the recent &#124;'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 针对单张图像，但仅大致讨论了近期的 &#124;'
- en: '&#124; advances on CNN-based methods. It has not discussed the &#124;'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 在基于CNN的方法上的进展。它未讨论 &#124;'
- en: '&#124; advanced convolutional operations and attention-based &#124;'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高级卷积操作和基于注意力的 &#124;'
- en: '&#124; model, loss function and supervisory signal, and &#124;'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型、损失函数和监督信号，并 &#124;'
- en: '&#124; only up to the year 2017. &#124;'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仅截至2017年。 &#124;'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Crowded Scene Analysis: &#124;'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 拥挤场景分析： &#124;'
- en: '&#124; A Survey [[77](#bib.bib77)] &#124;'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 调查 [[77](#bib.bib77)] &#124;'
- en: '| 2014 | T-CSVT |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | T-CSVT |'
- en: '&#124; This paper reviews crowd scene analysis in terms of &#124;'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文回顾了人群场景分析的 &#124;'
- en: '&#124; crowd behavior, activity analysis, and anomaly detection, &#124;'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人群行为、活动分析和异常检测， &#124;'
- en: '&#124; with crowd counting playing a small role. &#124;'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人群计数占据了较小的角色。 &#124;'
- en: '|'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Advances and Trends in Visual &#124;'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉中的进展和趋势 &#124;'
- en: '&#124; Crowd Analysis: A Systematic &#124;'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人群分析：系统性 &#124;'
- en: '&#124; Survey and Evaluation of Crowd &#124;'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人群调查与评估 &#124;'
- en: '&#124; Modelling Techniques [[239](#bib.bib239)] &#124;'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 建模技术 [[239](#bib.bib239)] &#124;'
- en: '| 2016 | NC |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 2016 | NC |'
- en: '&#124; This paper evaluates different crowd analysis &#124;'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 本文评估了不同的人群分析 &#124;'
- en: '&#124; methods, is not mainly for CNN-based approach &#124;'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法，并非主要针对基于CNN的方法 &#124;'
- en: '&#124; via density map estimation, which has become the &#124;'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过密度图估计，这已成为 &#124;'
- en: '&#124; mainstream for crowd counting in recent years. &#124;'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 近年来人群计数的主流。 &#124;'
- en: '|'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; A Survey of Human-Sensing: Methods &#124;'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 人体感知调查：方法 &#124;'
- en: '&#124; for Detecting, Presence, Count, &#124;'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 检测、存在、计数， &#124;'
- en: '&#124; Location, Track, and Identity [[167](#bib.bib167)] &#124;'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 位置、跟踪和身份 [[167](#bib.bib167)] &#124;'
- en: '| 2010 | CS |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| 2010 | CS |'
- en: '&#124; An early survey on human sensing. However, it has not &#124;'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 早期的人体感知调查。然而，它尚未 &#124;'
- en: '&#124; focused on crowd scene analysis but on the study of presence, &#124;'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 专注于人群场景分析而不是存在的研究， &#124;'
- en: '&#124; count, location, track, and identification. &#124;'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计数、位置、跟踪和识别。 &#124;'
- en: '|'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: In contrast with previous papers, our work comprehensively summarizes more than
    two hundred deep learning-based crowd counting algorithms in the recent five years.
    Our work is of current interest and value, because it is more comprehensive, summarizing
    the more recent, popular, and critical design components of this active field
    and provide an in-depth illustration of the representative schemes in the area.
    Through this survey, we expect to offer an up-to-date summary of recent advances
    in this field and educate new researchers on the design principles and trade-offs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于以往的论文，我们的工作全面总结了过去五年中200多种基于深度学习的人群计数算法。我们的工作具有当前的兴趣和价值，因为它更全面，总结了这个活跃领域中更近、流行和关键的设计组件，并深入阐述了该领域的代表性方案。通过这项调查，我们希望提供该领域最新进展的总结，并教育新研究人员有关设计原则和权衡。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")
    shows the main design components for crowd counting we will discuss in this paper.
    For network design, we describe the basic principles of major techniques such
    as fully convolutional network, encoder-decoder architecture, multi-column, and
    pyramid network, etc. For loss function, we discuss the widely used Euclidean
    loss and the recently advanced schemes such as SSIM loss, and multi-task learning.
    For supervisory signal, we introduce different ground truth generation methods
    for fully supervised setting and compare it with weakly supervised and semi-supervised
    learning, and self-supervised learning, and automatic labeling through synthetic
    data. Typical representative schemes are summarized and compared in each section.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning-based Single
    Image Crowd Counting: Network Design, Loss Function and Supervisory Signal") 展示了我们在本文中讨论的人群计数的主要设计组件。对于网络设计，我们描述了全卷积网络、编码器-解码器架构、多列网络和金字塔网络等主要技术的基本原理。对于损失函数，我们讨论了广泛使用的欧几里得损失以及最近提出的方案，如SSIM损失和多任务学习。对于监督信号，我们介绍了完全监督设置的不同真实值生成方法，并与弱监督和半监督学习、自监督学习以及通过合成数据进行自动标注进行比较。每个部分总结并比较了典型的代表性方案。'
- en: 'The rest of the paper is organized as follows. In Section [2](#S2 "2 Datasets
    and Performance Evaluation ‣ A Survey on Deep Learning-based Single Image Crowd
    Counting: Network Design, Loss Function and Supervisory Signal"), we summarize
    the publicly available crowd counting datasets, evaluation metrics, and design
    considerations. We present in Section [3](#S3 "3 Deep Neural Network Design ‣
    A Survey on Deep Learning-based Single Image Crowd Counting: Network Design, Loss
    Function and Supervisory Signal") the details of deep neural network design. Section [4](#S4
    "4 Loss Function ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal") discusses the loss functions,
    and Section [5](#S5 "5 Supervisory Signal ‣ A Survey on Deep Learning-based Single
    Image Crowd Counting: Network Design, Loss Function and Supervisory Signal") reviews
    supervisory signal to train crowd counting network. We conclude with future directions
    in Section [6](#S6 "6 Conclusion and Future Directions ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal").'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的其余部分组织如下。在第[2节](#S2 "2 数据集与性能评估 ‣ 基于深度学习的单幅图像人群计数调查：网络设计、损失函数和监督信号")中，我们总结了公开可用的人群计数数据集、评估指标和设计考虑因素。我们在第[3节](#S3
    "3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数调查：网络设计、损失函数和监督信号")中介绍了深度神经网络设计的细节。第[4节](#S4 "4 损失函数
    ‣ 基于深度学习的单幅图像人群计数调查：网络设计、损失函数和监督信号")讨论了损失函数，第[5节](#S5 "5 监督信号 ‣ 基于深度学习的单幅图像人群计数调查：网络设计、损失函数和监督信号")回顾了训练人群计数网络的监督信号。我们在第[6节](#S6
    "6 结论与未来方向 ‣ 基于深度学习的单幅图像人群计数调查：网络设计、损失函数和监督信号")中总结了未来的研究方向。
- en: 2 Datasets and Performance Evaluation
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据集与性能评估
- en: 'In this section, we first summarize the most widely used crowd counting datasets
    in Section [2.1](#S2.SS1 "2.1 Datasets ‣ 2 Datasets and Performance Evaluation
    ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network Design,
    Loss Function and Supervisory Signal"). Then we discuss the design considerations
    and performance metrics to study crowd counting in Section [2.2](#S2.SS2 "2.2
    Performance Evaluation and Metrics ‣ 2 Datasets and Performance Evaluation ‣ A
    Survey on Deep Learning-based Single Image Crowd Counting: Network Design, Loss
    Function and Supervisory Signal").'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先总结了在第[2.1节](#S2.SS1 "2.1 数据集 ‣ 2 数据集与性能评估 ‣ 基于深度学习的单幅图像人群计数调查：网络设计、损失函数和监督信号")中最广泛使用的人群计数数据集。然后，我们讨论了第[2.2节](#S2.SS2
    "2.2 性能评估与指标 ‣ 2 数据集与性能评估 ‣ 基于深度学习的单幅图像人群计数调查：网络设计、损失函数和监督信号")中的设计考虑因素和性能指标。
- en: 2.1 Datasets
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 数据集
- en: 'Public datasets are used as benchmarks to evaluate crowd counting models. In
    choosing a dataset, the following metrics are often considered:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 公共数据集被用作评估人群计数模型的基准。在选择数据集时，通常考虑以下指标：
- en: •
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Image resolution: Datasets with high resolutions usually show better visual
    quality. Furthermore, due to their higher pixel density, they often achieve higher
    count accuracy.'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像分辨率：高分辨率的数据集通常显示出更好的视觉质量。此外，由于其较高的像素密度，这些数据集通常能实现更高的计数准确性。
- en: •
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Number of images: Datasets with a large number of images often cover more diverse
    scenes, backgrounds, view angles, and lighting conditions. Large and diverse datasets
    are beneficial to optimize deep learning-based models and mitigate over-fitting
    problems.'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像数量：具有大量图像的数据集通常覆盖更多样化的场景、背景、视角和光照条件。大型且多样的数据集有助于优化基于深度学习的模型，并减轻过拟合问题。
- en: •
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Object count: The number of objects in a dataset is an important consideration
    for crowd analysis. The minimum, maximum, and average counts shed light on crowd
    density in the dataset. Datasets with a large crowd density level coverage and
    the number of objects is usually more challenging for crowd counting.'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对象数量：数据集中的对象数量是人群分析的重要考虑因素。最小值、最大值和平均值揭示了数据集中人群的密度。具有较高人群密度覆盖水平的数据集通常对人群计数更具挑战性。
- en: 'We identify some common datasets used in the research community including pedestrian
    counting and object datasets, and extract and present some typical images from
    the datasets in Fig. [3](#S2.F3 "Figure 3 ‣ 2.1 Datasets ‣ 2 Datasets and Performance
    Evaluation ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal"). There are also some other works
    focus on counting from remote scenes [[237](#bib.bib237), [34](#bib.bib34), [230](#bib.bib230),
    [38](#bib.bib38), [121](#bib.bib121)] and indoor crowd counting [[87](#bib.bib87)].
    We also compare these datasets in Table [3](#S2.T3 "Table 3 ‣ 11st item ‣ 2.1
    Datasets ‣ 2 Datasets and Performance Evaluation ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal").
    These datasets are elaborated below:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们识别出一些研究界常用的数据集，包括行人计数和物体数据集，并从这些数据集中提取并展示了一些典型图像，如图 [3](#S2.F3 "图 3 ‣ 2.1
    数据集 ‣ 2 数据集与性能评估 ‣ 关于基于深度学习的单图像人群计数的调查：网络设计、损失函数和监督信号")。还有一些其他工作关注于远程场景的计数 [[237](#bib.bib237),
    [34](#bib.bib34), [230](#bib.bib230), [38](#bib.bib38), [121](#bib.bib121)] 和室内人群计数 [[87](#bib.bib87)]。我们还在表 [3](#S2.T3
    "表 3 ‣ 第11项 ‣ 2.1 数据集 ‣ 2 数据集与性能评估 ‣ 关于基于深度学习的单图像人群计数的调查：网络设计、损失函数和监督信号") 中比较了这些数据集。以下是对这些数据集的详细介绍：
- en: '![Refer to caption](img/834317c4e5555388a128d121a6250a7f.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/834317c4e5555388a128d121a6250a7f.png)'
- en: 'Figure 3: Some typical crowd scene images of publicly available datasets. Different
    columns represents different crowd counting datasets and we visualize four typical
    images for each dataset. The NWPU [[185](#bib.bib185)], UCF-QNRF [[60](#bib.bib60)],
    ShanghaiTech A & B [[228](#bib.bib228)], WorldExpo’10 [[221](#bib.bib221)], and
    UCF_CC_50 [[59](#bib.bib59)] are image-based datasets. The FDST [[37](#bib.bib37)],
    Mall [[17](#bib.bib17)], and UCSD [[12](#bib.bib12)] are video-based datasets.
    The GCC [[148](#bib.bib148)] is a diverse synthetic crowd dataset.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：公开可用数据集中一些典型的人群场景图像。不同的列表示不同的人群计数数据集，我们为每个数据集可视化了四张典型图像。NWPU [[185](#bib.bib185)]、UCF-QNRF [[60](#bib.bib60)]、上海科技
    A & B [[228](#bib.bib228)]、WorldExpo’10 [[221](#bib.bib221)] 和 UCF_CC_50 [[59](#bib.bib59)]
    是基于图像的数据集。FDST [[37](#bib.bib37)]、Mall [[17](#bib.bib17)] 和 UCSD [[12](#bib.bib12)]
    是基于视频的数据集。GCC [[148](#bib.bib148)] 是一个多样化的合成人群数据集。
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'RGBT-CC contains RGB-thermal data captured in different scenarios in urban
    scenes with various densities, e.g., malls, streets, playgrounds, train stations,
    etc. $1,013$ pairs are in light and $1,017$ pairs are in darkness. RGBT-CC is
    randomly divided into three sets: 1030 pairs for training, 200 pairs for validation,
    800 pairs for testing.'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RGBT-CC 包含在不同城市场景下拍摄的 RGB-热成像数据，场景包括商场、街道、游乐场、火车站等，具有不同的密度。$1,013$ 对图像在光线下拍摄，$1,017$
    对图像在黑暗中拍摄。RGBT-CC 随机分为三个数据集：1030 对用于训练，200 对用于验证，800 对用于测试。
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: NWPU-Crowd [[185](#bib.bib185)] consists of $5,109$ images and $2,133,375$ annotated
    instances with points and boxes. Compared with other real-world crowd counting
    datasets, the NWPU-Crowd dataset has the largest density range of the annotated
    objects from 0 to $20,033$ per image. The average resolution of this dataset is
    $2191\times 3209$, which is generally larger than other widely used 2D single
    image crowd counting datasets.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: NWPU-Crowd [[185](#bib.bib185)] 包含 $5,109$ 张图像和 $2,133,375$ 个标注实例，包括点和框。与其他实际的人群计数数据集相比，NWPU-Crowd
    数据集的标注对象密度范围从 0 到 $20,033$ 每张图像，密度范围最大。该数据集的平均分辨率为 $2191\times 3209$，通常大于其他广泛使用的
    2D 单图像人群计数数据集。
- en: •
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'JHU-Crowd [[162](#bib.bib162)] is collected under diverse scenarios, environmental,
    and weather conditions include images with weather-based degradations and illumination
    variations. This dataset contains a rich set of labels: blur-level, occlusion-level,
    size-level, and other image-level annotations.'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: JHU-Crowd [[162](#bib.bib162)] 是在不同场景、环境和天气条件下收集的，包括具有天气衰减和光照变化的图像。该数据集包含丰富的标签：模糊程度、遮挡程度、尺寸等级以及其他图像级注释。
- en: •
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: CrowdSurveillance [[210](#bib.bib210)] is a large scale crowd counting dataset
    with high-resolution images captured under challenging scenarios. The dataset
    is built by both online crawling and real-world surveillance video which covers
    more challenging scenarios with complicated backgrounds and varying crowd counts.
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: CrowdSurveillance [[210](#bib.bib210)] 是一个大规模的人群计数数据集，包含在具有挑战性场景下拍摄的高分辨率图像。该数据集通过在线抓取和实际监控视频构建，涵盖了更多具有复杂背景和不同人群数量的挑战性场景。
- en: •
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: DroneCrowd [[198](#bib.bib198)] is formed by 112 video clips with 33,600 high
    resolution frames with large variations in scale, viewpoint, and background clutters,
    which captured under 70 different scenarios across 4 cities. The video clips are
    recorded at 25 frames per seconds with $1920\times 1080$ resolution. This dataset
    also provides 20,800 people trajectories with head annotations and several video-level
    attributes in sequences, i.e., illumination, altitude, and density. DroneCrowd
    is divided into training and testing sets, with 82 and 30 video sequences respectively.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: DroneCrowd [[198](#bib.bib198)] 由 112 个视频片段组成，共 33,600 张高分辨率的帧，具有大幅度的尺度、视角和背景杂乱变化，这些视频是在
    4 个城市的 70 个不同场景下拍摄的。视频片段的录制速度为每秒 25 帧，分辨率为 $1920\times 1080$。该数据集还提供了 20,800 条带头部标注的人物轨迹以及几个视频级别的序列属性，例如，照明、海拔和密度。DroneCrowd
    被分为训练集和测试集，分别包含 82 和 30 个视频序列。
- en: •
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: UCF-QNRF [[60](#bib.bib60)] contains 1,535 challenging images and a total of
    1,251,642 annotations. The minimum and the maximum number of objects within an
    image are 49 and 12,865\. The training and testing sets are selected by sorting
    the images according to the counts and picking every 5th image as the test set
    (1201 images for training and 334 images for testing). Besides, this large-scale
    dataset covers different locations, viewpoints, perspective effects, and different
    times of the day.
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UCF-QNRF [[60](#bib.bib60)] 包含 1,535 张具有挑战性的图片和总计 1,251,642 个标注。每张图片中的对象最小数和最大数分别为
    49 和 12,865。训练集和测试集是通过根据计数对图片进行排序，并每隔 5 张图片选择一张作为测试集（训练集 1201 张图片，测试集 334 张图片）来选择的。此外，这个大规模数据集涵盖了不同的地点、视角、透视效果以及不同的时间段。
- en: •
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: GCC [[148](#bib.bib148)] [[187](#bib.bib187)] is a large-scale diverse synthetic
    crowd dataset, which was generated based on a computer game, Grand Theft Auto
    V. GTA V Crowd Counting (GCC) dataset consists of $15,212$ images, with a resolution
    of $1080\times 1920$, containing more than $7,625,843$ people annotation. GCC
    is more diverse than other real-world datasets. It captures 400 different crowd
    scenes in the GTA C game, which includes multiple types of locations.
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: GCC [[148](#bib.bib148)] [[187](#bib.bib187)] 是一个大规模多样化的合成人群数据集，基于电脑游戏《侠盗猎车手
    V》生成。GTA V 人群计数 (GCC) 数据集包含 $15,212$ 张图片，分辨率为 $1080\times 1920$，包含超过 $7,625,843$
    个标注头部。GCC 比其他现实世界的数据集更加多样化。它捕捉了 GTA C 游戏中的 400 个不同的人群场景，涵盖了多种类型的位置。
- en: •
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fudan-ShanghaiTech [[37](#bib.bib37)] contains 100 videos captured from 13 different
    scenes. FDST includes 150,000 frames and 394,081 annotated heads, which is larger
    than previous video crowd counting datasets in terms of frames. The training set
    of the FDST dataset consists of 60 videos, 9000 frames, and the testing set contains
    the remaining 40 videos, 6000 frames. The number of frames per second (FPS) for
    FDST is 30.
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复旦-上海科技大学 [[37](#bib.bib37)] 包含从 13 个不同场景拍摄的 100 个视频。FDST 包括 150,000 帧和 394,081
    个标注头部，这在帧数上大于之前的视频人群计数数据集。FDST 数据集的训练集由 60 个视频、9000 帧组成，测试集包含剩余的 40 个视频和 6000
    帧。FDST 的每秒帧数（FPS）为 30。
- en: •
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'ShanghaiTech A $\&amp;$ B [[228](#bib.bib228)] consists of two parts: Part
    A and Part B, which contains 482 images (300 images for training, 182 images for
    testing), and 716 images (400 images for training, 316 images for testing), respectively.
    Part A includes high-density crowds that are collected from the Internet. Part
    B is captured from the busy streets of urban areas in Shanghai, which are less
    crowded than the scenes from Part A.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上海科技大学 A $\&amp;$ B [[228](#bib.bib228)] 包含两个部分：A 部分和 B 部分，分别包含 482 张图片（300
    张用于训练，182 张用于测试），以及 716 张图片（400 张用于训练，316 张用于测试）。A 部分包括从互联网收集的高密度人群。B 部分则是在上海市区的繁忙街道拍摄的，人群密度低于
    A 部分的场景。
- en: •
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: WorldExpo’10 [[221](#bib.bib221)] focus on cross-scene counting. It consists
    of 1132 video sequences captured by 108 surveillance cameras during the Shanghai
    2010 WorldExpo. WorldExpo’10 dataset is randomly selected from the video sequences,
    which has 3,980 frames with 199,923 head annotations. The training set of WorldExpo’10
    contains 3,380 frames from 103 scenes, and the remaining 600 frames are sampled
    from five other different scenes with each scene being 120 frames for testing.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 世界博览会’10 [[221](#bib.bib221)] 关注跨场景计数。它由 108 个监控摄像头在 2010 年上海世界博览会期间拍摄的 1132
    个视频序列组成。WorldExpo’10 数据集是从视频序列中随机选择的，包含 3,980 帧和 199,923 个头部标注。WorldExpo’10 的训练集包含来自
    103 个场景的 3,380 帧，其余的 600 帧从另外五个不同场景中采样，每个场景 120 帧用于测试。
- en: •
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: UCF_CC_50 [[59](#bib.bib59)] has 50 black and white crowd images and 63974 annotations,
    with the object counts ranging from 94 to 4543 and an average of 1280\. The original
    average resolution of the dataset is $2101\times 2888$. This challenging dataset
    is crawled from the Internet. For experiments, UCF_CC_50 were divided into 5 subsets
    and performed five-fold cross-validation. The maximum resolution was reduced to
    1024 for efficient computation.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UCF_CC_50 [[59](#bib.bib59)] 包含 50 张黑白人群图像和 63974 个注释，对象计数从 94 到 4543 不等，平均为
    1280。数据集的原始平均分辨率为 $2101\times 2888$。该具有挑战性的数据集是从互联网抓取的。为进行实验，UCF_CC_50 被分为 5 个子集，并进行了五折交叉验证。最大分辨率被降低到
    1024 以提高计算效率。
- en: 'Table 3: An overview of datasets statistics for crowd counting. Image Number
    is the number of images; Total is total number of labeled objects; Min Count is
    the minimal crowd count; Max Count is the maximum crowd count; Ave Count is the
    average crowd count.'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 表 3：人群计数数据集统计概览。图像数量是图像的总数；总计是标记对象的总数；最小计数是最小的人群计数；最大计数是最大的人群计数；平均计数是平均人群计数。
- en: '| Category | Dataset | Year |'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 分类 | 数据集 | 年份 |'
- en: '&#124; Average &#124;'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 平均 &#124;'
- en: '&#124; Resolution &#124;'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 分辨率 &#124;'
- en: '|'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Image &#124;'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 图像 &#124;'
- en: '&#124; Number &#124;'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 数量 &#124;'
- en: '| Total |'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 总计 |'
- en: '&#124; Min &#124;'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 最小 &#124;'
- en: '&#124; Count &#124;'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 计数 &#124;'
- en: '|'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Max &#124;'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 最大 &#124;'
- en: '&#124; Count &#124;'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 计数 &#124;'
- en: '|'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Avg &#124;'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 平均 &#124;'
- en: '&#124; Count &#124;'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 计数 &#124;'
- en: '|'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '| Pedestrian Counting | RGBT [[92](#bib.bib92)] | 2021 | 640$\times$480 | 2,030
    | 138,389 | - | - | 68 |'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 行人计数 | RGBT [[92](#bib.bib92)] | 2021 | 640$\times$480 | 2,030 | 138,389
    | - | - | 68 |'
- en: '| NWPU-Crowd [[185](#bib.bib185)] | 2020 | 2191$\times$3209 | 5,109 | 2,133,375
    | 0 | 20,033 | 418 |'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| NWPU-Crowd [[185](#bib.bib185)] | 2020 | 2191$\times$3209 | 5,109 | 2,133,375
    | 0 | 20,033 | 418 |'
- en: '| JHU-Crowd [[162](#bib.bib162)] | 2019 | 1450$\times$900 | 4250 | 1,114,785
    | 0 | 7286 | 262 |'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| JHU-Crowd [[162](#bib.bib162)] | 2019 | 1450$\times$900 | 4250 | 1,114,785
    | 0 | 7286 | 262 |'
- en: '|'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Crowd Surveillance [[210](#bib.bib210)] &#124;'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 人群监控 [[210](#bib.bib210)] &#124;'
- en: '| 2019 | 1342$\times$840 | 13,945 | 386,513 | - | - | 35 |'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2019 | 1342$\times$840 | 13,945 | 386,513 | - | - | 35 |'
- en: '| DroneCrowd [[198](#bib.bib198)] | 2019 | 1920$\times$1080 | 33,600 | 4,864,280
    | 25 | 455 | 145 |'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| DroneCrowd [[198](#bib.bib198)] | 2019 | 1920$\times$1080 | 33,600 | 4,864,280
    | 25 | 455 | 145 |'
- en: '| UCF-QNRF [[60](#bib.bib60)] | 2019 | 2013$\times$2902 | 1,535 | 1,251,642
    | 49 | 12,865 | 815 |'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| UCF-QNRF [[60](#bib.bib60)] | 2019 | 2013$\times$2902 | 1,535 | 1,251,642
    | 49 | 12,865 | 815 |'
- en: '| GCC [[148](#bib.bib148)] | 2019 | 1080$\times$1920 | 15,212 | 7,625,843 |
    0 | 3,995 | 501 |'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| GCC [[148](#bib.bib148)] | 2019 | 1080$\times$1920 | 15,212 | 7,625,843 |
    0 | 3,995 | 501 |'
- en: '|'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fudan-ST [[37](#bib.bib37)] &#124;'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; 复旦-ST [[37](#bib.bib37)] &#124;'
- en: '| 2019 | 1080$\times$1920 | 15,000 | 394,081 | 9 | 57 | 27 |'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2019 | 1080$\times$1920 | 15,000 | 394,081 | 9 | 57 | 27 |'
- en: '|'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ST Part A [[228](#bib.bib228)] &#124;'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; ST 部分 A [[228](#bib.bib228)] &#124;'
- en: '| 2016 | 589$\times$868 | 482 | 241,677 | 33 | 3,139 | 501 |'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2016 | 589$\times$868 | 482 | 241,677 | 33 | 3,139 | 501 |'
- en: '|'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ST Part B [[228](#bib.bib228)] &#124;'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '&#124; ST 部分 B [[228](#bib.bib228)] &#124;'
- en: '| 2016 | 768$\times$1024 | 716 | 88,488 | 9 | 578 | 124 |'
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 2016 | 768$\times$1024 | 716 | 88,488 | 9 | 578 | 124 |'
- en: '| WorldExpo’10 [[221](#bib.bib221)] | 2015 | 576 $\times$ 720 | 3,980 | 199,923
    | 1 | 253 | 50 |'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| WorldExpo’10 [[221](#bib.bib221)] | 2015 | 576 $\times$ 720 | 3,980 | 199,923
    | 1 | 253 | 50 |'
- en: '| UCF_CC_50 [[59](#bib.bib59)] | 2013 | 2101$\times$2888 | 50 | 63,974 | 94
    | 4,543 | 1,280 |'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| UCF_CC_50 [[59](#bib.bib59)] | 2013 | 2101$\times$2888 | 50 | 63,974 | 94
    | 4,543 | 1,280 |'
- en: '| Mall [[17](#bib.bib17)] | 2012 | 240$\times$320 | 2,000 | 62,325 | 13 | 53
    | 31 |'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Mall [[17](#bib.bib17)] | 2012 | 240$\times$320 | 2,000 | 62,325 | 13 | 53
    | 31 |'
- en: '| UCSD [[12](#bib.bib12)] | 2008 | 158$\times$238 | 2,000 | 49,885 | 11 | 46
    | 25 |'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| UCSD [[12](#bib.bib12)] | 2008 | 158$\times$238 | 2,000 | 49,885 | 11 | 46
    | 25 |'
- en: '| Object Counting | VisDrone Vehicle [[238](#bib.bib238)] | 2019 | 991$\times$1511
    | 5303 | 198,984 | 10 | 349 | 38 |'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| 对象计数 | VisDrone 车辆 [[238](#bib.bib238)] | 2019 | 991$\times$1511 | 5303 |
    198,984 | 10 | 349 | 38 |'
- en: '| Penguin [[4](#bib.bib4)] | 2016 | 700$\times$700 | 8200 | 72160 | - | 5 |
    8.8 |'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| Penguin [[4](#bib.bib4)] | 2016 | 700$\times$700 | 8200 | 72160 | - | 5 |
    8.8 |'
- en: '| TRANCOS [[45](#bib.bib45)] | 2015 | 640$\times$480 | 1244 | 46796 | - | -
    | 38 |'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| TRANCOS [[45](#bib.bib45)] | 2015 | 640$\times$480 | 1244 | 46796 | - | -
    | 38 |'
- en: •
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Mall [[17](#bib.bib17)] was captured by a public surveillance camera in a shopping
    mall, which contains more challenging lighting conditions and more severe perspective
    distortion than the UCSD dataset [[12](#bib.bib12)]. The Mall dataset consists
    of 2000 video frames with fixed resolution ($320\times 240$) and 62,325 total
    pedestrian instances. The first 800 frames were used for training and the remaining
    1200 frames for testing.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Mall [[17](#bib.bib17)] 是由购物中心的公共监控摄像机拍摄的，该数据集在光照条件和透视失真方面比 UCSD 数据集 [[12](#bib.bib12)]
    更具挑战性。Mall 数据集包含 2000 个视频帧，固定分辨率为 ($320\times 240$)，总共 62,325 个行人实例。前 800 帧用于训练，其余
    1200 帧用于测试。
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: UCSD [[12](#bib.bib12)] consists of an hour of video with 2000 annotated frames
    and in a total of 49,885 pedestrian instances, which was captured from a pedestrian
    walkway of the UCSD campus by a stationary camera. The original video was recorded
    at 30fps with a frame size of $480\times 740$ and later downsampled to 10fps with
    dimension $158\times 238$. The 601-1400 frames were used for training and the
    remaining 1200 frames for testing. The ROI of the walkway and the traveling direction
    are also provided.
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UCSD [[12](#bib.bib12)] 包含一个小时的视频，具有 2000 张标注帧，总共有 49,885 个行人实例，该视频由固定摄像机在 UCSD
    校园的人行道上拍摄。原始视频以 30fps 录制，帧大小为 $480\times 740$，后来降采样到 10fps，尺寸为 $158\times 238$。601-1400
    帧用于训练，其余 1200 帧用于测试。还提供了人行道的 ROI 和行进方向。
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: VisDrone Vehicle [[7](#bib.bib7)] is modified from the original VisDrone2019
    detection dataset [[238](#bib.bib238)] with bounding boxes of targets to crowd
    counting annotations. The new vehicle annotation location is the center point
    of the original bounding box. This dataset consists of 3953 training samples,
    364 validation samples, and 986 test samples.
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: VisDrone Vehicle [[7](#bib.bib7)] 是对原始 VisDrone2019 检测数据集 [[238](#bib.bib238)]
    的修改，使用目标的边界框转换为人群计数注释。新的车辆注释位置是原始边界框的中心点。该数据集包含 3953 个训练样本、364 个验证样本和 986 个测试样本。
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Penguin [[4](#bib.bib4)] is a large and challenging dataset of penguins in the
    wild with high-degree of object occlusion and scale variation. The collected images
    are compounded by many factors, e.g., adversarial weather conditions, variability
    of vantage points of the cameras, extreme crowding, and inter-occlusion between
    penguins. The Penguin dataset is divided into two subsets for $70\%$ and $30\%$
    of the total images respectively.
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Penguin [[4](#bib.bib4)] 是一个大型且具有挑战性的野生企鹅数据集，具有高程度的对象遮挡和尺度变化。收集的图像受到多种因素的影响，例如恶劣的天气条件、相机视角的变化、极度拥挤以及企鹅之间的相互遮挡。Penguin
    数据集被分为两个子集，分别占总图像的 $70\%$ 和 $30\%$。
- en: •
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'TRANCOS [[45](#bib.bib45)] is a vehicle crowd counting dataset which is to
    estimate the number of vehicles in an image of a traffic congestion situation.
    TRANCOS consists of 1244 traffic jam images with 46796 annotated vehicles. All
    the collected images contain traffic congestions with a variety of different scenes
    and viewpoints, covering different lighting conditions, different levels of overlap,
    and crowdedness. This dataset is divided into three parts: 403 images for training,
    420 images for validation, and 421 images for testing.'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: TRANCOS [[45](#bib.bib45)] 是一个车辆人群计数数据集，用于估计交通拥堵情况图像中的车辆数量。TRANCOS 包含 1244 张交通拥堵图像，标注了
    46796 辪车辆。所有收集到的图像都包含交通拥堵情况，涵盖了不同的场景和视角，包括不同的光照条件、重叠程度和拥挤程度。该数据集分为三部分：403 张用于训练，420
    张用于验证，421 张用于测试。
- en: 2.2 Performance Evaluation and Metrics
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 性能评估和指标
- en: 'In evaluating crowd counting networks, the following performance metrics are
    often used:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估人群计数网络时，通常使用以下性能指标：
- en: •
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Accuracy: Accuracy refers to counting accuracy and location accuracy.'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确性：准确性指的是计数准确性和位置准确性。
- en: –
  id: totrans-293
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: –
- en: Counting accuracy is affected by scale variation and isolated clusters of objects [[7](#bib.bib7)].
    Scale variation means the same object would appear as a different size in an image
    due to its perspective and distance from the camera. Besides, an image may have
    isolated object clusters, and models properly capturing such contextual information
    usually perform better than others. To quantitatively evaluate counting accuracy,
    Mean Absolute Error (MAE), Mean Squared Error (MSE) and mean Normalized Absolute
    Error (NAE) are commonly used, defined respectively as:$MAE=\frac{1}{N}\sum_{i=1}^{N}|C_{i}-\hat{C}_{i}|$,
    $MSE=\sqrt{\frac{1}{N}\sum_{i=1}^{N}|C_{i}-\hat{C}_{i}|^{2}}$, $NAE=\frac{1}{N}\sum_{i=1}^{N}\frac{|C_{i}-\hat{C}_{i}|}{C_{i}}$,
    where $N$ is total number of test images, $C_{i}$ the ground truth of the $i$-th
    image, and $\hat{C}_{i}$ the estimated count.
  id: totrans-294
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计数准确性受到尺度变化和孤立物体簇的影响[[7](#bib.bib7)]。尺度变化意味着相同的物体在图像中由于视角和距离不同而显现为不同的大小。此外，图像中可能存在孤立的物体簇，能够正确捕捉这些上下文信息的模型通常表现更好。为了定量评估计数准确性，常用均绝对误差（MAE）、均方误差（MSE）和均归一化绝对误差（NAE），分别定义为：$MAE=\frac{1}{N}\sum_{i=1}^{N}|C_{i}-\hat{C}_{i}|$，$MSE=\sqrt{\frac{1}{N}\sum_{i=1}^{N}|C_{i}-\hat{C}_{i}|^{2}}$，$NAE=\frac{1}{N}\sum_{i=1}^{N}\frac{|C_{i}-\hat{C}_{i}|}{C_{i}}$，其中$N$为测试图像的总数，$C_{i}$为第$i$张图像的真实值，$\hat{C}_{i}$为估计计数。
- en: –
  id: totrans-295
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: —
- en: Location accuracy is related to the spatial information preserved in the density
    map. Models with higher quality density map generated usually contains more spatial
    information for localization tasks.
  id: totrans-296
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 位置准确性与密度图中保留的空间信息有关。生成的高质量密度图通常包含更多用于定位任务的空间信息。
- en: •
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Quality of density map: Density map can be evaluated in terms of resolution
    and visual quality.'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 密度图质量：密度图可以从分辨率和视觉质量方面进行评估。
- en: –
  id: totrans-299
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: —
- en: High-resolution density maps usually show better location accuracy and preserve
    more spatial information for localization tasks (e.g., detection and tracking).
  id: totrans-300
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 高分辨率密度图通常表现出更好的位置准确性，并保留更多用于定位任务的空间信息（例如检测和跟踪）。
- en: –
  id: totrans-301
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: —
- en: To quantitatively evaluate the visual quality of the generated density maps,
    Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity in Images (SSIM) [[194](#bib.bib194)].
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了定量评估生成的密度图的视觉质量，可以使用峰值信噪比（PSNR）和图像结构相似性（SSIM）[[194](#bib.bib194)]。
- en: •
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Complexity: Complexity consists of computational complexity and annotation
    complexity.'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复杂性：复杂性包括计算复杂性和注释复杂性。
- en: –
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: —
- en: Computational complexity is evaluated based on measures such as the number of
    model parameters, floating-point operations (FLOPs), and inference time.
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 计算复杂性通过模型参数数量、浮点运算（FLOPs）和推理时间等指标来评估。
- en: –
  id: totrans-307
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: —
- en: 'Annotation complexity, as shown in Table [1](#S1.T1 "Table 1 ‣ 1 Introduction
    ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network Design,
    Loss Function and Supervisory Signal"), refers to data labeling cost. In general,
    object-level annotation as conducted in the detection-based approach has high
    complexity. Density map estimation requires point-level (head) annotation, which
    is relatively less costly. If unlabeled or synthetic data are used, the complexity
    can be further reduced.'
  id: totrans-308
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '注释复杂性，如表格[1](#S1.T1 "Table 1 ‣ 1 Introduction ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")所示，指的是数据标注成本。一般来说，基于检测的方法进行的对象级注释具有较高的复杂性。密度图估计需要点级（头部）注释，相对成本较低。如果使用未标注或合成数据，复杂性可以进一步降低。'
- en: •
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Flexibility and robustness:'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 灵活性和鲁棒性：
- en: –
  id: totrans-311
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: —
- en: The flexibility of models is evaluated based on the sensitivity of processing
    images with arbitrary sizes and the ability to model different kinds of objects
    (e.g., non-rigid objects).
  id: totrans-312
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型的灵活性根据处理任意尺寸图像的敏感性和建模不同类型对象（如非刚性对象）的能力来评估。
- en: –
  id: totrans-313
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: —
- en: Robustness refers to distribution shift robustness. It is evaluated in terms
    of out-of-distribution accuracy, where the test data come from another distribution
    (w.r.t. the training one).
  id: totrans-314
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鲁棒性指的是对分布变化的鲁棒性。它通过超出分布准确度来评估，其中测试数据来自与训练数据不同的分布。
- en: 3 Deep Neural Network Design
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 深度神经网络设计
- en: 'Network design is one of the most important parts for density map estimation.
    In this section, we present the major deep networks for crowd counting: fully
    convolutional networks (Section [3.1](#S3.SS1 "3.1 Fully Convolutional Network
    ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based Single Image
    Crowd Counting: Network Design, Loss Function and Supervisory Signal")), encoder-decoder
    architecture (Section [3.2](#S3.SS2 "3.2 Encoder-Decoder Architecture ‣ 3 Deep
    Neural Network Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal")), multi-column network
    (Section [3.3](#S3.SS3 "3.3 Multi-Column Network ‣ 3 Deep Neural Network Design
    ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network Design,
    Loss Function and Supervisory Signal")), pyramid structure (Section [3.4](#S3.SS4
    "3.4 Pyramid Structure ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")),
    advanced operations (Section [3.5](#S3.SS5 "3.5 Advanced Convolution Operations
    ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based Single Image
    Crowd Counting: Network Design, Loss Function and Supervisory Signal")), attention-based
    model (Section [3.6](#S3.SS6 "3.6 Attention-based Model ‣ 3 Deep Neural Network
    Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal")), vision transformer (Section [3.7](#S3.SS7
    "3.7 Vision Transformer ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")),
    and neural architecture search (Section [3.8](#S3.SS8 "3.8 Neural Architecture
    Search ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based Single
    Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")).
    We compare these approaches in Section [3.9](#S3.SS9 "3.9 Comparisons ‣ 3 Deep
    Neural Network Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal"), and remark on some other
    emerging approaches in Section [3.10](#S3.SS10 "3.10 Others ‣ 3 Deep Neural Network
    Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal").'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 网络设计是密度图估计的最重要部分之一。在本节中，我们介绍了主要用于人群计数的深度网络：完全卷积网络（第[3.1](#S3.SS1 "3.1 完全卷积网络
    ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节），编码器-解码器架构（第[3.2](#S3.SS2
    "3.2 编码器-解码器架构 ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节），多列网络（第[3.3](#S3.SS3
    "3.3 多列网络 ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节），金字塔结构（第[3.4](#S3.SS4
    "3.4 金字塔结构 ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节），高级操作（第[3.5](#S3.SS5
    "3.5 高级卷积操作 ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节），注意力模型（第[3.6](#S3.SS6
    "3.6 注意力模型 ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节），视觉变换器（第[3.7](#S3.SS7
    "3.7 视觉变换器 ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节）和神经架构搜索（第[3.8](#S3.SS8
    "3.8 神经架构搜索 ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节）。我们在第[3.9](#S3.SS9
    "3.9 比较 ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节中比较了这些方法，并在第[3.10](#S3.SS10
    "3.10 其他 ‣ 3 深度神经网络设计 ‣ 基于深度学习的单幅图像人群计数：网络设计、损失函数和监督信号的调查")节中对其他新兴方法进行了说明。
- en: 3.1 Fully Convolutional Network
  id: totrans-317
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 完全卷积网络
- en: 'An early CNN-based density map estimation approach is based on a fully convolutional
    network (FCN) [[119](#bib.bib119)], which is modified from the existing CNN architecture
    (VGG16) and replaces all the fully-connected layers with fully convolutional layers
    in order to analyze images of arbitrary sizes. As shown in Fig. [4](#S3.F4 "Figure
    4 ‣ 3.1 Fully Convolutional Network ‣ 3 Deep Neural Network Design ‣ A Survey
    on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function
    and Supervisory Signal") (a), FCN learns an end-to-end mapping from an input image
    to the corresponding density map and produces a proportionally sized density map
    output gave the input image. The FCN structure is simple but accurate, which has
    been widely used.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '早期基于 CNN 的密度图估计方法基于全卷积网络（FCN）[[119](#bib.bib119)]，该网络从现有的 CNN 架构（VGG16）修改而来，将所有全连接层替换为全卷积层，以分析任意大小的图像。如图
    [4](#S3.F4 "Figure 4 ‣ 3.1 Fully Convolutional Network ‣ 3 Deep Neural Network
    Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal") (a) 所示，FCN 学习了从输入图像到相应密度图的端到端映射，并根据输入图像生成按比例大小的密度图输出。FCN
    结构简单但准确，已被广泛使用。'
- en: '![Refer to caption](img/45d709200edd71f440974adc73378dda.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/45d709200edd71f440974adc73378dda.png)'
- en: 'Figure 4: A summary of the diverse range of network architectures used for
    deep learning-based single image crowd counting: (a) fully connected network [[119](#bib.bib119)];
    (b) encoder-decoder architecture [[11](#bib.bib11)]; (c) multi-column network [[228](#bib.bib228)];
    (d) pyramid structure [[66](#bib.bib66)]; (e) attention-based model [[42](#bib.bib42)]
    ; (f) graph neural network [[110](#bib.bib110)]. The order of the networks according
    their presentation in this paper. (Better viewed in the zoom-in mode)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '图 4: 深度学习基础上的单图像人群计数所使用的多种网络架构的总结：(a) 全连接网络 [[119](#bib.bib119)]; (b) 编码器-解码器架构
    [[11](#bib.bib11)]; (c) 多列网络 [[228](#bib.bib228)]; (d) 金字塔结构 [[66](#bib.bib66)];
    (e) 基于注意力的模型 [[42](#bib.bib42)]; (f) 图神经网络 [[110](#bib.bib110)]。这些网络的顺序按照论文中的展示排列。（在放大模式下查看效果更佳）'
- en: However, the FCN crowd counting method has some limitations. The resolution
    of the generated density map is only $1/4$ of the input width and $1/4$ of the
    input height due to the max pooling operations (extract high-level features but
    reduce resolutions) in FCN, which lacks fine details and spatial information for
    localization tasks, compared with high-resolution density maps. Besides, the FCN
    crowd counting model is susceptible to scale variation problems in crowd scene
    images, which limits its applicability in the general environment.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，FCN 人群计数方法存在一些局限性。由于 FCN 中的最大池化操作（提取高级特征但降低分辨率），生成的密度图的分辨率仅为输入宽度的 $1/4$ 和输入高度的
    $1/4$，这与高分辨率密度图相比，缺乏细节和空间信息，适用于定位任务。此外，FCN 人群计数模型在人群场景图像中容易受到尺度变化问题的影响，这限制了其在一般环境中的适用性。
- en: 3.2 Encoder-Decoder Architecture
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 编码器-解码器架构
- en: 'The Encoder-decoder model is proposed to align the resolution of the produced
    density map with the input image. As shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1
    Fully Convolutional Network ‣ 3 Deep Neural Network Design ‣ A Survey on Deep
    Learning-based Single Image Crowd Counting: Network Design, Loss Function and
    Supervisory Signal") (b), the encoder-decoder network consists an encoder and
    a decoder: an encoder network takes the input image and output high-level features,
    which hold the information and represents the input; a decoder network takes the
    features from the encoder and generate high-resolution density map. The encoder
    gradually downsamples the image resolution with convolutional or pooling layers,
    and the decoder progressively upsamples the feature maps from the encoder with
    deconvolutional layers or interpolations. The skip connections are applied on
    the feature maps from the encoder and decoder respectively.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '编码器-解码器模型旨在使生成的密度图的分辨率与输入图像对齐。如图 [4](#S3.F4 "Figure 4 ‣ 3.1 Fully Convolutional
    Network ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based Single
    Image Crowd Counting: Network Design, Loss Function and Supervisory Signal") (b)
    所示，编码器-解码器网络由一个编码器和一个解码器组成：编码器网络接收输入图像并输出高级特征，这些特征包含信息并表示输入；解码器网络从编码器获取特征并生成高分辨率密度图。编码器通过卷积或池化层逐步下采样图像分辨率，而解码器通过反卷积层或插值逐步上采样编码器的特征图。跳跃连接分别应用于编码器和解码器的特征图。'
- en: Some of the deep learning-based crowd counting approaches are following the
    encoder-decoder structure in recent years (see, for examples, [[220](#bib.bib220),
    [62](#bib.bib62), [11](#bib.bib11), [96](#bib.bib96), [164](#bib.bib164), [20](#bib.bib20),
    [168](#bib.bib168), [29](#bib.bib29)]). SANet [[11](#bib.bib11)] proposed a novel
    encoder-decoder network, called scale aggregation Network, which achieves accurate
    and efficient crowd estimation. The decoder generates high-resolution density
    maps with a set of transposed convolutions.Furthermore, encoder-decoder based
    architecture can significantly reduce the number of parameters compared with other
    architectures due to the downsample operations in the encoder. However, such architecture
    has not addressed the scale variation problem and has not considered the local
    and global contextual information.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，基于深度学习的人群计数方法采用了编码器-解码器结构（例如，参见[[220](#bib.bib220)、[62](#bib.bib62)、[11](#bib.bib11)、[96](#bib.bib96)、[164](#bib.bib164)、[20](#bib.bib20)、[168](#bib.bib168)、[29](#bib.bib29)]）。SANet [[11](#bib.bib11)]
    提出了一个新型的编码器-解码器网络，称为尺度聚合网络，它实现了准确且高效的人群估计。解码器通过一组反卷积生成高分辨率密度图。此外，由于编码器中的下采样操作，基于编码器-解码器的架构相比其他架构可以显著减少参数数量。然而，这种架构尚未解决尺度变化问题，也没有考虑局部和全局上下文信息。
- en: 3.3 Multi-Column Network
  id: totrans-325
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 多列网络
- en: Multi-column and pyramid network is the most prominent models in recent crowd
    counting algorithms to extract the multi-scale features and tackle the scale variation
    problem  [[75](#bib.bib75), [201](#bib.bib201), [202](#bib.bib202), [99](#bib.bib99),
    [212](#bib.bib212), [28](#bib.bib28), [215](#bib.bib215), [181](#bib.bib181)].
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 多列和金字塔网络是最近人群计数算法中最突出的模型之一，用于提取多尺度特征并解决尺度变化问题[[75](#bib.bib75)、[201](#bib.bib201)、[202](#bib.bib202)、[99](#bib.bib99)、[212](#bib.bib212)、[28](#bib.bib28)、[215](#bib.bib215)、[181](#bib.bib181)]。
- en: 'The multi-column architecture incorporates multi-column architecture with different
    kernel sizes to extract different scale features in order to achieve accurate
    counting accuracy such as MCNN [[228](#bib.bib228)] and McML [[25](#bib.bib25)].
    As shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Fully Convolutional Network ‣ 3 Deep
    Neural Network Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal") (c), multi-column neural
    network (MCNN) consists of multiple branches with different kernel sizes (e.g.,
    $5\times 5$, $7\times 7$ and $9\times 9$). The different branches accommodate
    different receptive fields, thus sensitive to multi-scale features. Finally, the
    features extracted by different columns are fused together to generate density
    maps. However, the accommodated scale diversity is restricted by the number of
    columns.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '多列架构结合了不同核大小的多列架构，以提取不同尺度的特征，从而实现准确的计数精度，如MCNN [[228](#bib.bib228)] 和 McML [[25](#bib.bib25)]。如图[4](#S3.F4
    "Figure 4 ‣ 3.1 Fully Convolutional Network ‣ 3 Deep Neural Network Design ‣ A
    Survey on Deep Learning-based Single Image Crowd Counting: Network Design, Loss
    Function and Supervisory Signal") (c)所示，多列神经网络（MCNN）由多个具有不同核大小的分支组成（例如，$5\times
    5$、$7\times 7$ 和 $9\times 9$）。不同的分支适应不同的感受野，因此对多尺度特征敏感。最后，通过不同列提取的特征被融合在一起生成密度图。然而，适应的尺度多样性受到列数的限制。'
- en: 3.4 Pyramid Structure
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 金字塔结构
- en: 'Image pyramid and feature pyramid architectures are yet another approach to
    address scale variations (e.g., AFP [[66](#bib.bib66)], CP-CNN [[157](#bib.bib157)],
     [[3](#bib.bib3)] and [[206](#bib.bib206)]), which mainly consists of two subgroups,
    image pyramid, and feature pyramid pooling. For the image pyramid-based model,
    as Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Fully Convolutional Network ‣ 3 Deep Neural
    Network Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal") (d) shows, different scale
    of the image pyramid (scale 1, …, scale S) is feed into an FCN to predict the
    density map of that scale. Then, the final estimation is produced by adaptive
    fusing the prediction from different scales. However, this kind of architecture
    remains a high computational complexity.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '图像金字塔和特征金字塔架构是解决尺度变化的另一种方法（例如，AFP [[66](#bib.bib66)]、CP-CNN [[157](#bib.bib157)]、[[3](#bib.bib3)]
    和 [[206](#bib.bib206)]），主要包括两个子组，即图像金字塔和特征金字塔池化。对于基于图像金字塔的模型，如图[4](#S3.F4 "Figure
    4 ‣ 3.1 Fully Convolutional Network ‣ 3 Deep Neural Network Design ‣ A Survey
    on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function
    and Supervisory Signal") (d)所示，不同尺度的图像金字塔（尺度1，…，尺度S）被输入到FCN中，以预测该尺度的密度图。然后，通过自适应融合来自不同尺度的预测生成最终估计。然而，这种架构仍然具有较高的计算复杂性。'
- en: Besides, some relevant techniques are usually used together with the multi-column
    and pyramid networks to enhance the multi-scale feature extraction process such
    as skip-connections [[162](#bib.bib162), [160](#bib.bib160), [195](#bib.bib195),
    [30](#bib.bib30), [120](#bib.bib120), [108](#bib.bib108)] and dense blocks [[126](#bib.bib126),
    [134](#bib.bib134), [111](#bib.bib111), [63](#bib.bib63), [60](#bib.bib60), [27](#bib.bib27)].
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些相关技术通常与多列和金字塔网络一起使用，以增强多尺度特征提取过程，例如跳跃连接 [[162](#bib.bib162), [160](#bib.bib160),
    [195](#bib.bib195), [30](#bib.bib30), [120](#bib.bib120), [108](#bib.bib108)]
    和密集块 [[126](#bib.bib126), [134](#bib.bib134), [111](#bib.bib111), [63](#bib.bib63),
    [60](#bib.bib60), [27](#bib.bib27)]。
- en: 3.5 Advanced Convolution Operations
  id: totrans-331
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 高级卷积操作
- en: There is a trend to leverage advanced convolutional operations to facilitate
    accurate crowd counting models and better CNN feature learning [[233](#bib.bib233),
    [207](#bib.bib207), [56](#bib.bib56)]. The deep learning-based single image crowd
    counting model benefits a lot from the advanced convolution such as dilated and
    deformable convolution, adaptive dilated convolution, and perspective-guided convolution.
    This can replace the traditional convolutional operations in the counting models.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 趋势是利用高级卷积操作来促进准确的人群计数模型和更好的 CNN 特征学习 [[233](#bib.bib233), [207](#bib.bib207),
    [56](#bib.bib56)]。基于深度学习的单图像人群计数模型从诸如膨胀卷积、可变形卷积、适应性膨胀卷积和视角引导卷积等高级卷积中受益匪浅。这些可以替代计数模型中的传统卷积操作。
- en: 'There are four important advanced operations:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 有四种重要的高级操作：
- en: •
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Dilated convolution introduces the dilated rate to the convolutional layers,
    which defines a spacing between the weights of the kernel. Traditional convolutional
    operation is more focused on extracting local features. For the dilated convolution,
    three subfigures represent dilated operations with the same kernel size ($3\times
    3$) but different dilated rates (Dilation = $1$, Dilation = $2$, and Dilation
    = $3$), which enlarges the receptive field without increasing the computational
    cost and also preserves the resolution of the feature maps. Dilated convolution
    facilitate real-time applications and is popular in many recent crowd counting
    models: Dynamic Region Division (DRD) [[49](#bib.bib49)], Scale Pyramid Network
    (SPN) [[19](#bib.bib19)], Atrous convolutions spatial pyramid network (ACSPNet) [[111](#bib.bib111)],
    DENet [[93](#bib.bib93)], Dilated Convolutional Neural Networks (CSRNet) [[80](#bib.bib80)]
    and An Aggregated Multicolumn Dilated Convolution Network (AMDCNet) [[28](#bib.bib28)].
    But this kind of operations not consider the multi-scale features and cannot fully
    capture the non-rigid objects.'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 膨胀卷积将膨胀率引入卷积层，定义了卷积核权重之间的间隔。传统的卷积操作更侧重于提取局部特征。对于膨胀卷积，三个子图表示了相同卷积核大小（$3\times
    3$）但不同膨胀率（膨胀 = $1$，膨胀 = $2$，和膨胀 = $3$）的膨胀操作，这在不增加计算成本的情况下扩大了感受野，同时保持了特征图的分辨率。膨胀卷积有助于实时应用，并且在许多近期的人群计数模型中非常受欢迎：动态区域划分
    (DRD) [[49](#bib.bib49)]，尺度金字塔网络 (SPN) [[19](#bib.bib19)]，Atrous 卷积空间金字塔网络 (ACSPNet) [[111](#bib.bib111)]，DENet [[93](#bib.bib93)]，膨胀卷积神经网络
    (CSRNet) [[80](#bib.bib80)] 和聚合多列膨胀卷积网络 (AMDCNet) [[28](#bib.bib28)]。但这种操作没有考虑多尺度特征，也无法完全捕捉非刚性物体。
- en: •
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Deformable convolution is a kind of spatial sampling location augmenting schemes
    in the modules with additional offsets and learning the offsets from the target
    tasks, without additional supervision. This can model non-rigid objects with additional
    learnable offsets. Some recent literatures replace the traditional convolutions
    with the deformable convolutions and achieves superiors performance: Dilated-Attention-Deformable
    ConvNet (DADNet) [[46](#bib.bib46)], An Attention-injective Deformable Convolutional
    Network (ADCrowdNet) [[97](#bib.bib97)].However, the deformable convolutional
    operations require high computational complexity.'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可变形卷积是一种在模块中通过额外的偏移量增强空间采样位置的方案，通过学习目标任务的偏移量而不需要额外的监督。这可以通过额外的可学习偏移量来建模非刚性物体。一些最新文献用可变形卷积替代了传统卷积，取得了更优的性能：Dilated-Attention-Deformable
    ConvNet (DADNet) [[46](#bib.bib46)]，An Attention-injective Deformable Convolutional
    Network (ADCrowdNet) [[97](#bib.bib97)]。然而，可变形卷积操作需要较高的计算复杂度。
- en: •
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adaptive dilated convolution is formed to predicts a continuous value of dilation
    rate for each location in order to effectively match the scale variation at different
    locations, which is better than fixed and discrete dilate rates. ADSCNet [[8](#bib.bib8)]
    is formulated based on adaptive dilated convolution, which is also able to preserve
    the strong consistency between the density and feature of each location.
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自适应膨胀卷积用于预测每个位置的膨胀率连续值，以有效匹配不同位置的尺度变化，这比固定的离散膨胀率更优。ADSCNet [[8](#bib.bib8)]基于自适应膨胀卷积构建，能够保持每个位置的密度和特征之间的强一致性。
- en: •
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Perspective-guided convolution aims to tackle the continuous scale variation
    issue with perspective information. The perspective information contains instance
    information between camera and a scene, which is a reasonable prior for people
    scale estimation. Concretely, the perspective information functions are leveraged
    to guide the spatially variant smoothing of feature maps before feeding to the
    successive convolutions. PGCNet [[210](#bib.bib210)] is built by stacking multiple
    Perspective-guided convolutions (PGC) blocks based on a CNN backbone, which is
    a single-column CNN target to tackle the scale variation issues with a moderate
    increase in computation.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 透视引导卷积旨在利用透视信息解决连续尺度变化问题。透视信息包含摄像头与场景之间的实例信息，这是估计人物尺度的合理先验。具体来说，透视信息功能被用于引导特征图的空间变化平滑，然后再输入到后续卷积中。PGCNet [[210](#bib.bib210)]通过在CNN骨干网络上堆叠多个透视引导卷积（PGC）块构建，属于单列CNN，旨在以适度的计算增加解决尺度变化问题。
- en: 'Table 4: Comparisons of network design considerations for crowd counting. Computational
    complexity is evaluated based on the number of model parameters. The representative
    schemes of each network design category are analyzed thoroughly in terms of advantages
    and limitations.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '表4: 人群计数网络设计考虑因素的比较。计算复杂度基于模型参数数量评估。对每种网络设计类别的代表性方案进行了深入分析，包括其优势和局限性。'
- en: '| Category |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 类别 |'
- en: '&#124; Representative &#124;'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 代表性 &#124;'
- en: '&#124; Scheme &#124;'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方案 &#124;'
- en: '| Advantages |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| 优势 |'
- en: '&#124; Computational &#124;'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算 &#124;'
- en: '&#124; Complexity &#124;'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 复杂度 &#124;'
- en: '| Limitations |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 局限性 |'
- en: '|'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Fully convolution &#124;'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 完全卷积 &#124;'
- en: '&#124; neural networks &#124;'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 神经网络 &#124;'
- en: '| FCN [[119](#bib.bib119)] |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| FCN [[119](#bib.bib119)] |'
- en: '&#124; Can analyze &#124;'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 能分析 &#124;'
- en: '&#124; images of &#124;'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 图像的 &#124;'
- en: '&#124; arbitrary size &#124;'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 任意大小 &#124;'
- en: '| Low |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 低 |'
- en: '&#124; Low-resolution &#124;'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低分辨率 &#124;'
- en: '&#124; density maps &#124;'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 密度图 &#124;'
- en: '|'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Encoder-decoder &#124;'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 编码-解码 &#124;'
- en: '&#124; architecture &#124;'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 架构 &#124;'
- en: '| SANet [[11](#bib.bib11)] |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[11](#bib.bib11)] |'
- en: '&#124; Able to generate &#124;'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 能生成 &#124;'
- en: '&#124; high-resolution &#124;'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高分辨率 &#124;'
- en: '&#124; density maps &#124;'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 密度图 &#124;'
- en: '|'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Low(0.9M) &#124;'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低(0.9M) &#124;'
- en: '|'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Not consider &#124;'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不考虑 &#124;'
- en: '&#124; scale variation &#124;'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 规模变化 &#124;'
- en: '|'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Multi-column &#124;'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多列 &#124;'
- en: '&#124; architecture &#124;'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 架构 &#124;'
- en: '| MCNN [[228](#bib.bib228)] |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| MCNN [[228](#bib.bib228)] |'
- en: '&#124; Extract multi-scale &#124;'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提取多尺度 &#124;'
- en: '&#124; features with &#124;'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征与 &#124;'
- en: '&#124; multi-column &#124;'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多列 &#124;'
- en: '&#124; architecture &#124;'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 架构 &#124;'
- en: '|'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Low(0.1M) &#124;'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低(0.1M) &#124;'
- en: '|'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; The scale diversity &#124;'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 规模多样性 &#124;'
- en: '&#124; is restricted by &#124;'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 受限于 &#124;'
- en: '&#124; the number &#124;'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数量 &#124;'
- en: '&#124; of columns &#124;'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 列的 &#124;'
- en: '|'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Pyramid &#124;'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 金字塔 &#124;'
- en: '&#124; architecture &#124;'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 架构 &#124;'
- en: '| CP-CNN [[157](#bib.bib157)] |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| CP-CNN [[157](#bib.bib157)] |'
- en: '&#124; Extract multi-scale &#124;'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提取多尺度 &#124;'
- en: '&#124; features with &#124;'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特征与 &#124;'
- en: '&#124; pyramid architecture &#124;'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 金字塔架构 &#124;'
- en: '|'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; High(68.4M) &#124;'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高(68.4M) &#124;'
- en: '|'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; High computational &#124;'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高计算 &#124;'
- en: '&#124; complexity &#124;'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 复杂性 &#124;'
- en: '|'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Advanced convolution operations |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '| 高级卷积操作 |'
- en: '&#124; Dilated &#124;'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 膨胀 &#124;'
- en: '&#124; convolution &#124;'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积 &#124;'
- en: '&#124; operations &#124;'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 操作 &#124;'
- en: '| CSRNet [[80](#bib.bib80)] |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| CSRNet [[80](#bib.bib80)] |'
- en: '&#124; Enlarge receptive &#124;'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 扩大感受 &#124;'
- en: '&#124; field without &#124;'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无 &#124;'
- en: '&#124; increase the &#124;'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提高 &#124;'
- en: '&#124; computational cost &#124;'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算成本 &#124;'
- en: '|'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Medium(16.3M) &#124;'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 中等(16.3M) &#124;'
- en: '|'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Not consider the &#124;'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不考虑 &#124;'
- en: '&#124; non-rigid objects &#124;'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 非刚性物体 &#124;'
- en: '|'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Deformable &#124;'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可变形的 &#124;'
- en: '&#124; convolution &#124;'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积 &#124;'
- en: '&#124; operations &#124;'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 操作 &#124;'
- en: '| ADCrowdNet [[97](#bib.bib97)] |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '| ADCrowdNet [[97](#bib.bib97)] |'
- en: '&#124; Learnable additional &#124;'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 可学习的附加 &#124;'
- en: '&#124; offsets for &#124;'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 偏移量 &#124;'
- en: '&#124; better modeling &#124;'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 更好的建模 &#124;'
- en: '&#124; non-rigid objects &#124;'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 非刚性物体 &#124;'
- en: '| High |'
  id: totrans-427
  prefs: []
  type: TYPE_TB
  zh: '| 高 |'
- en: '&#124; High computational &#124;'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高计算 &#124;'
- en: '&#124; complexity &#124;'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 复杂性 &#124;'
- en: '|'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Adaptive &#124;'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自适应 &#124;'
- en: '&#124; dilated &#124;'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 膨胀的 &#124;'
- en: '&#124; convolution &#124;'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积 &#124;'
- en: '| ADSCNet [[8](#bib.bib8)] |'
  id: totrans-435
  prefs: []
  type: TYPE_TB
  zh: '| ADSCNet [[8](#bib.bib8)] |'
- en: '&#124; Learn continuous &#124;'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习连续的 &#124;'
- en: '&#124; dilation rate &#124;'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 膨胀率 &#124;'
- en: '| Medium |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 中等 |'
- en: '&#124; Not flexible &#124;'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不灵活 &#124;'
- en: '&#124; to non-rigid &#124;'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对非刚性 &#124;'
- en: '&#124; objects &#124;'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 物体 &#124;'
- en: '|'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Perspective- &#124;'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视角- &#124;'
- en: '&#124; guided &#124;'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 引导的 &#124;'
- en: '&#124; convolution &#124;'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 卷积 &#124;'
- en: '| PGCNet [[210](#bib.bib210)] |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| PGCNet [[210](#bib.bib210)] |'
- en: '&#124; Perspective &#124;'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视角 &#124;'
- en: '&#124; information &#124;'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息 &#124;'
- en: '&#124; facilitate people &#124;'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 便利人们 &#124;'
- en: '&#124; scale estimation &#124;'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺度估计 &#124;'
- en: '| Medium |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 中等 |'
- en: '&#124; Requires additional &#124;'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 需要额外的 &#124;'
- en: '&#124; perspective &#124;'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视角 &#124;'
- en: '&#124; information &#124;'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息 &#124;'
- en: '|'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Attention-based &#124;'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 基于注意力的 &#124;'
- en: '&#124; Model &#124;'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 模型 &#124;'
- en: '| SCAR [[42](#bib.bib42)] |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| SCAR [[42](#bib.bib42)] |'
- en: '&#124; Capture local &#124;'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 捕捉局部 &#124;'
- en: '&#124; and global &#124;'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 和全局的 &#124;'
- en: '&#124; contextual &#124;'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 上下文 &#124;'
- en: '&#124; information &#124;'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息 &#124;'
- en: '| Medium |'
  id: totrans-465
  prefs: []
  type: TYPE_TB
  zh: '| 中等 |'
- en: '&#124; Rely on pixel-wise &#124;'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 依赖于像素级 &#124;'
- en: '&#124; loss function &#124;'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 损失函数 &#124;'
- en: '|'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Vision Transformer &#124;'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 视觉变换器 &#124;'
- en: '| TransCrowd [[82](#bib.bib82)] |'
  id: totrans-471
  prefs: []
  type: TYPE_TB
  zh: '| TransCrowd [[82](#bib.bib82)] |'
- en: '&#124; Able to model &#124;'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 能够建模 &#124;'
- en: '&#124; long-range context &#124;'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 长程上下文 &#124;'
- en: '&#124; information &#124;'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息 &#124;'
- en: '| Medium |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 中等 |'
- en: '&#124; Computational &#124;'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算 &#124;'
- en: '&#124; expensive &#124;'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 昂贵 &#124;'
- en: '|'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Neural Architecture &#124;'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 神经架构 &#124;'
- en: '&#124; Search &#124;'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 搜索 &#124;'
- en: '| NAS-Count [[55](#bib.bib55)] |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| NAS-Count [[55](#bib.bib55)] |'
- en: '&#124; Automate crowd &#124;'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动化人群 &#124;'
- en: '&#124; counting model &#124;'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计数模型 &#124;'
- en: '&#124; design &#124;'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 设计 &#124;'
- en: '| Medium |'
  id: totrans-486
  prefs: []
  type: TYPE_TB
  zh: '| 中等 |'
- en: '&#124; Computational &#124;'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算 &#124;'
- en: '&#124; expensive &#124;'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 昂贵 &#124;'
- en: '|'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 3.6 Attention-based Model
  id: totrans-490
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 基于注意力的模型
- en: 'Attention mechanisms can be roughly divided into two subgroups: hard attention
    and soft attention [[85](#bib.bib85), [178](#bib.bib178), [184](#bib.bib184),
    [203](#bib.bib203), [145](#bib.bib145), [64](#bib.bib64), [35](#bib.bib35), [54](#bib.bib54),
    [16](#bib.bib16), [18](#bib.bib18)]. Such mechanisms have been explicitly explored
    in recent years, and we summarize several recent algorithms applied with the attention
    mechanism: AFPNet [[66](#bib.bib66)], MRA-CNN [[227](#bib.bib227)], SAAN [[52](#bib.bib52)],
    DADNet [[46](#bib.bib46)], Relational Attention Network [[219](#bib.bib219)],
    Hierarchical Scale Recalibration Network [[241](#bib.bib241)], ACM-CNN [[240](#bib.bib240)],
    HA-CNN [[159](#bib.bib159)], Shallow Feature-based Dense Attention Network [[123](#bib.bib123)]
    and Multi-supervised Parallel Network [[196](#bib.bib196)].'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 注意机制大致可以分为两个子组：硬注意力和软注意力 [[85](#bib.bib85), [178](#bib.bib178), [184](#bib.bib184),
    [203](#bib.bib203), [145](#bib.bib145), [64](#bib.bib64), [35](#bib.bib35), [54](#bib.bib54),
    [16](#bib.bib16), [18](#bib.bib18)]。这些机制近年来已被明确探索，我们总结了几种最近应用了注意力机制的算法：AFPNet [[66](#bib.bib66)]，MRA-CNN [[227](#bib.bib227)]，SAAN [[52](#bib.bib52)]，DADNet [[46](#bib.bib46)]，关系注意力网络 [[219](#bib.bib219)]，层级尺度重新校准网络 [[241](#bib.bib241)]，ACM-CNN [[240](#bib.bib240)]，HA-CNN [[159](#bib.bib159)]，浅层特征密集注意力网络 [[123](#bib.bib123)]和多监督并行网络 [[196](#bib.bib196)]。
- en: 'SCAR [[42](#bib.bib42)] is one of the typical models to make use of attention
    schemes. SCAR proposes a spatial- /channel-wise attention regression module for
    crowd counting. As shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Fully Convolutional
    Network ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based Single
    Image Crowd Counting: Network Design, Loss Function and Supervisory Signal") (e),
    the top half branch (spatial-wise attention) captures large-range contextual information
    and the change of density distribution, which the output feature map is weighted
    sum of attention map and original local feature map. The bottom half branch shows
    the channel-wise attention, which leverages both local and global contextual information
    for crowd counting. The features extracted by these two branches are late fused
    by concatenation and upsample post-processing to generate density maps. However,
    most of the methods discussed above are relying on pixel-wise loss functions for
    optimizing the model. We will discussadvanced loss functions to better capture
    spatial correlations between pixels and to generate high-quality density maps.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 'SCAR [[42](#bib.bib42)] 是利用注意力机制的典型模型之一。SCAR 提出了一个空间/通道注意力回归模块用于 crowd counting。如图 [4](#S3.F4
    "Figure 4 ‣ 3.1 Fully Convolutional Network ‣ 3 Deep Neural Network Design ‣ A
    Survey on Deep Learning-based Single Image Crowd Counting: Network Design, Loss
    Function and Supervisory Signal") (e) 所示，上半部分分支（空间注意力）捕捉大范围的上下文信息和密度分布的变化，输出特征图是注意力图和原始局部特征图的加权和。下半部分分支展示了通道注意力，它利用局部和全局上下文信息进行
    crowd counting。这两个分支提取的特征通过拼接和上采样后处理进行融合，以生成密度图。然而，大多数上述方法依赖于逐像素损失函数来优化模型。我们将讨论高级损失函数，以更好地捕捉像素之间的空间相关性，并生成高质量的密度图。'
- en: 'Table 5: Quantitative comparisons of different network design considerations
    on widely used crowd counting datasets. The counting accuracy is evaluated based
    on MAE and MSE. The visual quality of the generated density maps is evaluated
    based on PSNR and SSIM. ST PartA and ST partB denotes ShanghaiTech A and ShanghaiTech
    B dataset [[228](#bib.bib228)], respectively.'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5：不同网络设计考虑因素在广泛使用的 crowd counting 数据集上的定量比较。计数精度基于 MAE 和 MSE 进行评估。生成的密度图的视觉质量基于
    PSNR 和 SSIM 进行评估。ST PartA 和 ST partB 分别表示 ShanghaiTech A 和 ShanghaiTech B 数据集 [[228](#bib.bib228)]。
- en: '| Representative Schemes | ST PartA | ST PartB | UCF_CC_50 |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| 代表方案 | ST PartA | ST PartB | UCF_CC_50 |'
- en: '|'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Methods &#124;'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 方法 &#124;'
- en: '| Year | Column | MAE | MSE | PSNR | SSIM | MAE | MSE | MAE | MSE |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 列 | MAE | MSE | PSNR | SSIM | MAE | MSE | MAE | MSE |'
- en: '| FCN [[119](#bib.bib119)] | 2016 | Single | 126.5 | 173.5 | - | - | 23.8 |
    33.1 | 338.6 | 424.5 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| FCN [[119](#bib.bib119)] | 2016 | 单通道 | 126.5 | 173.5 | - | - | 23.8 | 33.1
    | 338.6 | 424.5 |'
- en: '| MCNN [[228](#bib.bib228)] | 2016 | Multi | 110.2 | 173.2 | 21.40 | 0.52 |
    26.4 | 41.3 | 377.6 | 509.1 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| MCNN [[228](#bib.bib228)] | 2016 | 多通道 | 110.2 | 173.2 | 21.40 | 0.52 | 26.4
    | 41.3 | 377.6 | 509.1 |'
- en: '| CP-CNN [[157](#bib.bib157)] | 2017 | Multi | 73.6 | 106.4 | 21.72 | 0.72
    | 20.1 | 30.1 | 295.8 | 320.9 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| CP-CNN [[157](#bib.bib157)] | 2017 | 多通道 | 73.6 | 106.4 | 21.72 | 0.72 |
    20.1 | 30.1 | 295.8 | 320.9 |'
- en: '| SANet [[11](#bib.bib11)] | 2018 | Single | 67.0 | 104.5 | - | - | 8.4 | 13.6
    | 258.4 | 334.9 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| SANet [[11](#bib.bib11)] | 2018 | 单通道 | 67.0 | 104.5 | - | - | 8.4 | 13.6
    | 258.4 | 334.9 |'
- en: '| CSRNet [[80](#bib.bib80)] | 2018 | Single | 68.2 | 115.0 | 23.79 | 0.76 |
    10.6 | 16.0 | 266.1 | 397.5 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| CSRNet [[80](#bib.bib80)] | 2018 | 单通道 | 68.2 | 115.0 | 23.79 | 0.76 | 10.6
    | 16.0 | 266.1 | 397.5 |'
- en: '| ADCrowd [[97](#bib.bib97)] | 2019 | Single | 63.2 | 98.9 | 24.48 | 0.88 |
    8.2 | 15.7 | 266.4 | 358.0 |'
  id: totrans-503
  prefs: []
  type: TYPE_TB
  zh: '| ADCrowd [[97](#bib.bib97)] | 2019 | 单通道 | 63.2 | 98.9 | 24.48 | 0.88 | 8.2
    | 15.7 | 266.4 | 358.0 |'
- en: '| PGCNet [[210](#bib.bib210)] | 2019 | Single | 57.0 | 86.0 | - | - | 8.8 |
    13.7 | 244.6 | 361.2 |'
  id: totrans-504
  prefs: []
  type: TYPE_TB
  zh: '| PGCNet [[210](#bib.bib210)] | 2019 | 单通道 | 57.0 | 86.0 | - | - | 8.8 | 13.7
    | 244.6 | 361.2 |'
- en: '| SCAR [[42](#bib.bib42)] | 2019 | Double | 66.3 | 114.1 | 23.93 | 0.81 | 9.5
    | 15.2 | 259.0 | 374.0 |'
  id: totrans-505
  prefs: []
  type: TYPE_TB
  zh: '| SCAR [[42](#bib.bib42)] | 2019 | 双通道 | 66.3 | 114.1 | 23.93 | 0.81 | 9.5
    | 15.2 | 259.0 | 374.0 |'
- en: '| ADSCNet [[8](#bib.bib8)] | 2020 | Single | 60.7 | 100.6 | - | - | 6.4 | 11.3
    | 198.4 | 267.3 |'
  id: totrans-506
  prefs: []
  type: TYPE_TB
  zh: '| ADSCNet [[8](#bib.bib8)] | 2020 | 单通道 | 60.7 | 100.6 | - | - | 6.4 | 11.3
    | 198.4 | 267.3 |'
- en: '| MS-GAN [[236](#bib.bib236)] | 2020 | Single | - | - | - | - | 18.7 | 30.5
    | 345.7 | 418.3 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| MS-GAN [[236](#bib.bib236)] | 2020 | 单通道 | - | - | - | - | 18.7 | 30.5 |
    345.7 | 418.3 |'
- en: '| HyGnn [[110](#bib.bib110)] | 2020 | Double | 60.2 | 94.5 | - | - | 7.5 |
    12.7 | 184.4 | 270.1 |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| HyGnn [[110](#bib.bib110)] | 2020 | 双通道 | 60.2 | 94.5 | - | - | 7.5 | 12.7
    | 184.4 | 270.1 |'
- en: '| TransCrowd [[82](#bib.bib82)] | 2021 | Single | 66.1 | 105.1 | - | - | 9.3
    | 16.1 | - | - |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| TransCrowd [[82](#bib.bib82)] | 2021 | 单通道 | 66.1 | 105.1 | - | - | 9.3 |
    16.1 | - | - |'
- en: '| NAS-Count [[55](#bib.bib55)] | 2021 | Single | 56.7 | 93.4 | - | - | 6.7
    | 10.2 | 208.4 | 297.3 |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| NAS-Count [[55](#bib.bib55)] | 2021 | 单通道 | 56.7 | 93.4 | - | - | 6.7 | 10.2
    | 208.4 | 297.3 |'
- en: '| STNet [[180](#bib.bib180)] | 2022 | Single | 52.9 | 83.6 | - | - | 6.3 |
    10.3 | 162.0 | 230.4 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| STNet [[180](#bib.bib180)] | 2022 | 单一 | 52.9 | 83.6 | - | - | 6.3 | 10.3
    | 162.0 | 230.4 |'
- en: 3.7 Vision Transformer
  id: totrans-512
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 视觉变压器
- en: The mainstream crowd estimation approaches usually leverage the convolution
    neural network to extract features and significant progress has been achieved
    by incorporating larger context information into CNNs, which indicates that long-range
    context is essential. The self-attention mechanisms of transformers, which explicitly
    model all pairwise interactions between elements in a sequence, which is particularly
    suitable to extract the semantic crowd information.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 主流的人群估计方法通常利用卷积神经网络提取特征，通过将更大上下文信息纳入CNN，已经取得了显著进展，这表明长距离上下文是至关重要的。变压器的自注意力机制明确建模序列中元素之间的所有配对交互，特别适合提取语义人群信息。
- en: 'TransCrowd [[82](#bib.bib82)] proposes two different kinds of approaches for
    single image crowd counting: TransCrowd-Token and TransCrowd-GAP, which can generate
    reasonable attention weight and achieve high counting performance.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: TransCrowd [[82](#bib.bib82)] 提出了两种不同的单图像人群计数方法：TransCrowd-Token 和 TransCrowd-GAP，这些方法能够生成合理的注意力权重并实现高计数性能。
- en: 3.8 Neural Architecture Search
  id: totrans-515
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8 神经架构搜索
- en: Most of the recent advances in counting network design are based on hand-designed
    neural networks, which require large design efforts and strong domain knowledge.
    To extract multi-level features, convolutions with various receptive fields are
    designed by hand. Recently, automatic and lightweight network design has drawn
    much attention. Automated Machine Learning and Neural Architecture Search (NAS)
    techniques can be used to automatically design effective and efficient crowd counting
    architectures [[193](#bib.bib193)]. And the NAS-based approach is able to automatically
    discover the task-specific multi-scale crowd estimation models.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 最近在计数网络设计方面的大多数进展基于手工设计的神经网络，这需要大量设计工作和强大的领域知识。为了提取多层次特征，手工设计了具有各种接收场的卷积。近年来，自动化和轻量级网络设计受到广泛关注。自动机器学习和神经架构搜索（NAS）技术可以用于自动设计有效且高效的人群计数架构 [[193](#bib.bib193)]。基于NAS的方法能够自动发现任务特定的多尺度人群估计模型。
- en: NAS-Count [[55](#bib.bib55)] automates the design of crowd counting models with
    NAS and proposes an end-to-end searched encoder-decoder architecture, where multi-scale
    features can be leveraged to tackle the scale variation problem. The first attempt
    in NAS needs hundreds of GPUs to run. However, NAS-Count leverage a differential
    one-shot search strategy to achieve fast search speed, where network parameters
    and architecture parameters are jointly optimized via gradient descent. In addition,
    NAS-Count is enabled by the compositional nature of CNN and is guided by task-specific
    search space and strategies. The architectures searched by the counting-oriented
    NAS framework achieve superior performance without demanding expert-involvement.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: NAS-Count [[55](#bib.bib55)] 自动化设计了人群计数模型，并提出了一个端到端搜索的编码器-解码器架构，其中多尺度特征可以被利用来解决尺度变化问题。NAS的首次尝试需要数百个GPU进行运行。然而，NAS-Count利用了差分一击搜索策略来实现快速搜索速度，通过梯度下降联合优化网络参数和架构参数。此外，NAS-Count
    依赖于CNN的组合性质，并由任务特定的搜索空间和策略指导。由计数导向的NAS框架搜索出的架构在没有专家参与的情况下实现了卓越的性能。
- en: 3.9 Comparisons
  id: totrans-518
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.9 比较
- en: 'We compare the different networks discussed above in Table [4](#S3.T4 "Table
    4 ‣ 3.5 Advanced Convolution Operations ‣ 3 Deep Neural Network Design ‣ A Survey
    on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function
    and Supervisory Signal"), and present their performance on three challenging crowd
    counting datasets in Table [5](#S3.T5 "Table 5 ‣ 3.6 Attention-based Model ‣ 3
    Deep Neural Network Design ‣ A Survey on Deep Learning-based Single Image Crowd
    Counting: Network Design, Loss Function and Supervisory Signal"). We also provide
    a comprehensive performance analysis of state-of-the-art crowd counting approaches
    in Table [10](#S6.T10 "Table 10 ‣ 6 Conclusion and Future Directions ‣ A Survey
    on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function
    and Supervisory Signal"). By analyzing the data, we find some intriguing observations.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表 [4](#S3.T4 "Table 4 ‣ 3.5 Advanced Convolution Operations ‣ 3 Deep Neural
    Network Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal") 中比较了上述不同网络，并在表 [5](#S3.T5
    "Table 5 ‣ 3.6 Attention-based Model ‣ 3 Deep Neural Network Design ‣ A Survey
    on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function
    and Supervisory Signal") 中展示了它们在三个具有挑战性的 crowd counting 数据集上的表现。我们还在表 [10](#S6.T10
    "Table 10 ‣ 6 Conclusion and Future Directions ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")
    中提供了对最先进 crowd counting 方法的全面性能分析。通过分析数据，我们发现了一些有趣的观察结果。'
- en: 'As Tables [4](#S3.T4 "Table 4 ‣ 3.5 Advanced Convolution Operations ‣ 3 Deep
    Neural Network Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal") and [5](#S3.T5 "Table 5
    ‣ 3.6 Attention-based Model ‣ 3 Deep Neural Network Design ‣ A Survey on Deep
    Learning-based Single Image Crowd Counting: Network Design, Loss Function and
    Supervisory Signal") show, SANet achieves better counting performance on datasets
    with different crowd levels, compared with FCN. The generated density maps of
    FCN are only $1/4\times 1/4$ of the original input image, which SANet is able
    to generate high-resolution density maps. The computational complexity for both
    the FCN and SANet is low (e.g., 0.91M for SANet), which indicates that the encoder-decoder
    architecture is lightweight.'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [4](#S3.T4 "Table 4 ‣ 3.5 Advanced Convolution Operations ‣ 3 Deep Neural
    Network Design ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal") 和 [5](#S3.T5 "Table 5 ‣
    3.6 Attention-based Model ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")
    所示，SANet 在不同 crowd levels 的数据集上相较于 FCN 实现了更好的计数性能。FCN 生成的密度图仅为原始输入图像的 $1/4\times
    1/4$，而 SANet 能够生成高分辨率的密度图。FCN 和 SANet 的计算复杂度都较低（例如，SANet 为 0.91M），这表明编码器-解码器架构较轻量。'
- en: MCNN and CP-CNN consider scale variation problem, which is able to capture multi-scale
    features. MCNN extracts multi-scale features with multi-column architecture and
    CP-CNN extracts multi-scale features with pyramid architecture. CP-CNN achieves
    better counting accuracy and visual quality than MCNN, while for the computational
    complexity, the number of parameters for CP-CNN (68.4M) is much larger than MCNN
    (0.13M). This further demonstrates the effectiveness of multi-column architecture
    and pyramid architecture, while image pyramid architecture (e.g., CP-CNN) is of
    high computational complexity.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: MCNN 和 CP-CNN 考虑了尺度变化问题，能够捕捉多尺度特征。MCNN 通过多列结构提取多尺度特征，而 CP-CNN 通过金字塔结构提取多尺度特征。CP-CNN
    在计数准确性和视觉质量方面优于 MCNN，但在计算复杂度方面，CP-CNN 的参数数量（68.4M）远大于 MCNN（0.13M）。这进一步证明了多列结构和金字塔结构的有效性，而图像金字塔结构（例如，CP-CNN）具有较高的计算复杂度。
- en: CSRNet and ADCrowdNet achieve better counting accuracy and visual quality than
    MCNN and CP-CNN on most of the datasets. CSRNet relies on dilated convolutional
    operations, which enlarge the receptive field without increase the computational
    cost. ADCrowdNet incorporates deformable convolutional operations, which are based
    on learnable additional offsets for better modeling non-rigid objects such as
    people. In addition, ADCrowdNet achieves better counting accuracy and visual quality
    than CSRNet but requires higher computational complexity.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: CSRNet 和 ADCrowdNet 在大多数数据集上实现了比 MCNN 和 CP-CNN 更好的计数准确性和视觉质量。CSRNet 依赖于扩张卷积操作，这可以在不增加计算成本的情况下扩大感受野。ADCrowdNet
    结合了可变形卷积操作，基于可学习的附加偏移量，更好地建模诸如人员等非刚性物体。此外，ADCrowdNet 在计数准确性和视觉质量方面优于 CSRNet，但需要更高的计算复杂度。
- en: SCAR shows better counting accuracy and visual quality than MCNN and CP-CNN,
    which is able to capture local and global contextual information based on spatial-wise
    attention and channel-wise attention schemes. The experimental results confirm
    the effectiveness of attention mechanism variations for crowd counting. HyGnn
    shows good counting performance on different crowd counting datasets, which demonstrates
    the effectiveness of graph-based models to distill rich relations among multi-scale
    features for crowd counting.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: SCAR显示出比MCNN和CP-CNN更好的计数精度和视觉质量，能够基于空间注意力和通道注意力方案捕捉局部和全局的上下文信息。实验结果确认了注意力机制变体在群体计数中的有效性。HyGnn在不同的群体计数数据集上表现出良好的计数性能，这证明了基于图的模型在提取多尺度特征之间的丰富关系方面的有效性。
- en: The multi-path encoder-decoder network searched by NAS-Count demonstrates better
    performance than tedious hand-designing crowd counting models on four challenging
    datasets, which achieves a multi-scale model automatically without strong domain
    knowledge. This clearly demonstrates the potential to automatically design effective
    and efficient crowd counting architectures.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: NAS-Count搜索出的多路径编码器-解码器网络在四个具有挑战性的数据集上展示了比繁琐的手工设计的群体计数模型更好的性能，实现了一个自动的多尺度模型，无需强的领域知识。这清楚地展示了自动设计有效且高效的群体计数架构的潜力。
- en: 3.10 Others
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.10 其他
- en: 'There are also some other emerging network designs for crowd counting, discussed
    below:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他新兴的网络设计用于群体计数，具体讨论如下：
- en: •
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Generative Adversarial Networks Generative Adversarial Networks (GAN) has been
    applied to a wide range of tasks in computer vision, and also have been adopted
    to crowd counting tasks such as GAN-MTR [[128](#bib.bib128)], MS-GAN [[213](#bib.bib213)], [[236](#bib.bib236)],
    ACSCP [[111](#bib.bib111)] and CODA [[79](#bib.bib79)]. Generative adversarial
    networks can be used to improve the visual quality of the generated density maps,
    but usually degrades counting accuracy. For example, MS-GAN [[213](#bib.bib213),
    [236](#bib.bib236)] proposed multi-scale GAN, which incorporates the inception
    module in the generation part. This paper investigated GAN as an effective solution
    to the crowd counting problem, to generate high-quality crowd density maps of
    arbitrary crowd density scenes. Besides, Adversarial Cross-Scale Consistency Pursuit
    (ACSCP) [[111](#bib.bib111)] designed a novel scale-consistency regularizer that
    enforces that the sum up of the crowd counts from local patches. The authors further
    boosted density estimation performance by further exploring the collaboration
    between both objectives.
  id: totrans-528
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 生成对抗网络（GAN）已经被应用于计算机视觉的广泛任务中，也被用于群体计数任务，如GAN-MTR [[128](#bib.bib128)]、MS-GAN [[213](#bib.bib213)]、[[236](#bib.bib236)]、ACSCP [[111](#bib.bib111)]和CODA [[79](#bib.bib79)]。生成对抗网络可以用来提高生成的密度图的视觉质量，但通常会降低计数准确性。例如，MS-GAN [[213](#bib.bib213),
    [236](#bib.bib236)]提出了多尺度GAN，将inception模块融入生成部分。本文探讨了GAN作为解决群体计数问题的有效方案，以生成高质量的任意密度场景的群体密度图。此外，Adversarial
    Cross-Scale Consistency Pursuit (ACSCP) [[111](#bib.bib111)]设计了一种新的尺度一致性正则化器，强制本地补丁的群体计数之和相等。作者通过进一步探索两个目标之间的协作，进一步提升了密度估计性能。
- en: •
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Graph neural networks based method distills rich relations among multi-scale
    features for crowd counting. As shown in Fig. [4](#S3.F4 "Figure 4 ‣ 3.1 Fully
    Convolutional Network ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")
    (f), HyGnn [[110](#bib.bib110)] exploits useful information from the auxiliary
    task (localization branch). The HyGnn module in the red box jointly represents
    the task-specific feature maps of different scales as nodes, multi-scale relations
    as edges, counting, and localization relations as edges, which distilled rich
    relations between the nodes to obtain more powerful representations, leading to
    robust and accurate results.'
  id: totrans-530
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '基于图神经网络的方法提取多尺度特征之间的丰富关系用于群体计数。如图[4](#S3.F4 "Figure 4 ‣ 3.1 Fully Convolutional
    Network ‣ 3 Deep Neural Network Design ‣ A Survey on Deep Learning-based Single
    Image Crowd Counting: Network Design, Loss Function and Supervisory Signal") (f)所示，HyGnn [[110](#bib.bib110)]利用来自辅助任务（定位分支）的有用信息。红框中的HyGnn模块将不同尺度的任务特定特征图联合表示为节点，多尺度关系表示为边缘，计数和定位关系也表示为边缘，这些关系提取了节点之间的丰富关系，以获得更强大的表示，从而得出鲁棒且准确的结果。'
- en: •
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Recurrent neural networks based Deep Recurrent Spatial-Aware Network (DSRNet) [[96](#bib.bib96)]
    utilize a learnable spatial transform module with a region-wise refinement process
    to adaptively enlarge the varied scales coverage. Researchers in [[150](#bib.bib150)]
    decoded the features into local counts using an LSTM decoder, finally predicts
    the image global count. The local counts and global count are all learning targets.
  id: totrans-532
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于循环神经网络的深度循环空间感知网络（DSRNet）[[96](#bib.bib96)]使用可学习的空间变换模块和区域增强过程来自适应地扩大各种尺度的覆盖范围。[[150](#bib.bib150)]中的研究人员使用LSTM解码器将特征解码为局部计数，最终预测图像的全局计数。局部计数和全局计数都是学习目标。
- en: •
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Prior-guided modules help enhancing counting performance, as discussed in recent
    literature [[81](#bib.bib81), [141](#bib.bib141), [211](#bib.bib211), [216](#bib.bib216),
    [143](#bib.bib143), [132](#bib.bib132), [124](#bib.bib124), [229](#bib.bib229),
    [61](#bib.bib61), [179](#bib.bib179)]. Multi-stage density map regression network
    is a scale-aware convolutional neural network (MMNet) [[31](#bib.bib31)], which
    not only captures multi-scale features generated by various sizes of filters but
    also integrates multi-scale features generated by different stages to handle scale
    variation problems.
  id: totrans-534
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 先验引导模块有助于提高计数性能，正如最近的文献[[81](#bib.bib81), [141](#bib.bib141), [211](#bib.bib211),
    [216](#bib.bib216), [143](#bib.bib143), [132](#bib.bib132), [124](#bib.bib124),
    [229](#bib.bib229), [61](#bib.bib61), [179](#bib.bib179)]中讨论的那样。多阶段密度图回归网络是一个尺度感知的卷积神经网络（MMNet）[[31](#bib.bib31)]，它不仅捕捉由各种大小的滤波器生成的多尺度特征，还整合了由不同阶段生成的多尺度特征来处理尺度变化问题。
- en: •
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Local counting network proposes an adaptive mixture regression framework [[104](#bib.bib104)]
    in a coarse-to-fine manner to improve counting accuracy, which fully utilizes
    the context and multi-scale information from different convolutional features.
    Besides, local counting networks perform more precise counting regression on local
    patches of images.
  id: totrans-536
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本地计数网络提出了一种自适应混合回归框架[[104](#bib.bib104)]，以粗粒度到细粒度的方式来提高计数准确性，充分利用了不同卷积特征的上下文和多尺度信息。此外，本地计数网络对图像的局部补丁进行更精确的计数回归。
- en: •
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multi-model fusion is another class of techniques for crowd counting [[142](#bib.bib142),
    [92](#bib.bib92), [166](#bib.bib166)]. Recently, most of the current works for
    crowd counting with state-of-the-art performance are density-map estimation-based
    approaches. Some researchers tried to improve the existing framework with both
    point and box annotation such as LCFCN [[71](#bib.bib71)], PSDDN [[106](#bib.bib106)],
    BSAD [[57](#bib.bib57)], DecideNet [[90](#bib.bib90)] and DRD [[49](#bib.bib49)].
    DecideNet [[90](#bib.bib90)] is one of the typical methods, which proposed a separate
    decide subnet to combine detection and density estimation. Combining detection
    with density map estimation usually utilizes detection for the low crowd and density
    estimation for the high crowd. However, these kinds of methods require high computational
    complexity and high annotation complexity.
  id: totrans-538
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多模型融合是另一类技术用于人群计数[[142](#bib.bib142), [92](#bib.bib92), [166](#bib.bib166)]。
    最近，大多数具有最先进性能的人群计数方法都是基于密度图估计的方法。一些研究人员尝试通过同时使用点和框注释（例如LCFCN [[71](#bib.bib71)],
    PSDDN [[106](#bib.bib106)], BSAD [[57](#bib.bib57)], DecideNet [[90](#bib.bib90)]
    and DRD [[49](#bib.bib49)]）对现有框架进行改进。 DecideNet [[90](#bib.bib90)] 是一种典型的方法，它提出了一个单独的决策子网络来结合检测和密度估计。将检测与密度图估计相结合通常利用检测来处理低密度的人群，而利用密度估计处理高密度的人群。然而，这些方法需要高计算复杂性和高注释复杂度。
- en: 4 Loss Function
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 损失函数
- en: 'The loss functions are used to optimize the model. Early works usually adopt
    the pixel-wise Euclidean loss (Section [4.1](#S4.SS1 "4.1 Euclidean Loss ‣ 4 Loss
    Function ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal")), later different advanced loss
    functions are utilized for better density estimation. In this section, we discuss
    some recent advances on loss functions for crowd counting: SSIM loss (Section [4.2](#S4.SS2
    "4.2 SSIM Loss ‣ 4 Loss Function ‣ A Survey on Deep Learning-based Single Image
    Crowd Counting: Network Design, Loss Function and Supervisory Signal")), and multi-task
    learning (Section [4.3](#S4.SS3 "4.3 Multi-task Learning ‣ 4 Loss Function ‣ A
    Survey on Deep Learning-based Single Image Crowd Counting: Network Design, Loss
    Function and Supervisory Signal")). We compare them in Section [4.4](#S4.SS4 "4.4
    Comparisons ‣ 4 Loss Function ‣ A Survey on Deep Learning-based Single Image Crowd
    Counting: Network Design, Loss Function and Supervisory Signal") and present some
    other emerging considerations in Section [4.5](#S4.SS5 "4.5 Others ‣ 4 Loss Function
    ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network Design,
    Loss Function and Supervisory Signal").'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: '损失函数用于优化模型。早期工作通常采用像素级欧几里得损失（第[4.1](#S4.SS1 "4.1 Euclidean Loss ‣ 4 Loss Function
    ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network Design,
    Loss Function and Supervisory Signal")节），随后使用了不同的先进损失函数以获得更好的密度估计。在本节中，我们讨论了一些关于人群计数的损失函数的最新进展：SSIM损失（第[4.2](#S4.SS2
    "4.2 SSIM Loss ‣ 4 Loss Function ‣ A Survey on Deep Learning-based Single Image
    Crowd Counting: Network Design, Loss Function and Supervisory Signal")节）和多任务学习（第[4.3](#S4.SS3
    "4.3 Multi-task Learning ‣ 4 Loss Function ‣ A Survey on Deep Learning-based Single
    Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")节）。我们在第[4.4](#S4.SS4
    "4.4 Comparisons ‣ 4 Loss Function ‣ A Survey on Deep Learning-based Single Image
    Crowd Counting: Network Design, Loss Function and Supervisory Signal")节对它们进行了比较，并在第[4.5](#S4.SS5
    "4.5 Others ‣ 4 Loss Function ‣ A Survey on Deep Learning-based Single Image Crowd
    Counting: Network Design, Loss Function and Supervisory Signal")节提出了一些其他的新兴考虑因素。'
- en: 4.1 Euclidean Loss
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 欧几里得损失
- en: 'Most of the early crowd counting approaches use Euclidean loss to optimize
    the models. The Euclidean loss is a pixel-wise estimation error:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数早期的人群计数方法使用欧几里得损失来优化模型。欧几里得损失是一种像素级估计误差：
- en: '|  | $L_{E}=\frac{1}{N}\left&#124;&#124;F(x_{i};\theta)-y_{i}\right&#124;&#124;_{2}^{2},$
    |  | (1) |'
  id: totrans-543
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{E}=\frac{1}{N}\left&#124;&#124;F(x_{i};\theta)-y_{i}\right&#124;&#124;_{2}^{2},$
    |  | (1) |'
- en: where $\theta$ indicates the model parameters, N means the number of pixels,
    $x_{i}$ denotes the input image, and $y_{i}$ is ground truth and $F(x_{i};\theta)$
    is the generated density map. The total crowd counting result can be summarized
    over the estimated crowd density map. The pixel-wise L2 loss is a flexible and
    widely used loss function for crowd counting. However, this pixel-wise loss does
    not take local and global contextual information as well as the visual quality
    of the generated density maps into account. Thus, this kind of loss function cannot
    produce satisfactory high-quality density maps and highly accurate crowd estimation.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\theta$ 表示模型参数，N 是像素数量，$x_{i}$ 代表输入图像，$y_{i}$ 是真实值，$F(x_{i};\theta)$ 是生成的密度图。总的人群计数结果可以在估计的人群密度图上汇总。像素级L2损失是一种灵活且广泛使用的人群计数损失函数。然而，这种像素级损失没有考虑局部和全局上下文信息以及生成密度图的视觉质量。因此，这种损失函数无法产生令人满意的高质量密度图和高精度的人群估计。
- en: 4.2 SSIM Loss
  id: totrans-545
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 SSIM损失
- en: 'Some variants of structure similarity (SSIM) loss are proposed for crowd counting
    to force the network to learn the local correlation within regions of various
    sizes, thereby producing locally consistent estimation results such as SSIM loss [[11](#bib.bib11)],
    multi-scale SSIM loss [[134](#bib.bib134)], DMS-SSIM loss [[95](#bib.bib95)] and
    DMSSIM loss [[75](#bib.bib75)]. Then the local pattern consistency can be formulated
    as:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 针对人群计数提出了一些结构相似性（SSIM）损失的变体，以迫使网络学习不同尺寸区域内的局部相关性，从而生成局部一致的估计结果，如SSIM损失 [[11](#bib.bib11)]、多尺度SSIM损失 [[134](#bib.bib134)]、DMS-SSIM损失 [[95](#bib.bib95)]和DMSSIM损失 [[75](#bib.bib75)]。局部模式一致性可以表示为：
- en: '|  | $L_{s}=1-\frac{1}{N}\sum_{x}SSIM(x).$ |  | (2) |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{s}=1-\frac{1}{N}\sum_{x}SSIM(x).$ |  | (2) |'
- en: The pixel-wise Euclidean loss usually assumes that adjacent pixels are independent
    and ignores the local correlation in the density maps, the Euclidean loss can
    be fused with the SSIM loss to leverage local correlations among pixels for generating
    high-quality density maps and accurate crowd estimation.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 像素级欧几里得损失通常假设相邻像素是独立的，并忽略了密度图中的局部相关性，欧几里得损失可以与SSIM损失融合，以利用像素之间的局部相关性，从而生成高质量的密度图和准确的人群估计。
- en: For example, the Cross-Level Parallel Network [[75](#bib.bib75)] fused the difference
    of mean structural similarity index (DMSSIM) with the MSE loss to optimize the
    module. Besides, Multi-View Scale Aggregation Networks [[134](#bib.bib134)] proposed
    a multi-scale SSIM for multi-view crowd counting. However, SSIM loss is hard to
    learn local correlations with a large spectrum of varied scales.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Cross-Level Parallel Network [[75](#bib.bib75)] 融合了均值结构相似性指数（DMSSIM）与均方误差损失来优化模块。此外，Multi-View
    Scale Aggregation Networks [[134](#bib.bib134)] 提出了多尺度 SSIM 用于多视角人群计数。然而，SSIM
    损失难以学习具有广泛尺度的局部相关性。
- en: 4.3 Multi-task Learning
  id: totrans-550
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 多任务学习
- en: 'The main task of crowd counting is the total counting accuracy, thus the direct
    global count constraints may benefit the counting accuracy. The headcount loss
    can be defined as:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 人群计数的主要任务是总体计数准确性，因此直接的全局计数约束可能有助于提高计数准确性。头部计数损失可以定义为：
- en: '|  | $L_{c}=\frac{1}{N}\sum_{i=1}^{N}&#124;&#124;\frac{F_{c}(x_{i};\theta)-y_{i}}{y_{i}+1}&#124;&#124;,$
    |  | (3) |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{c}=\frac{1}{N}\sum_{i=1}^{N}&#124;&#124;\frac{F_{c}(x_{i};\theta)-y_{i}}{y_{i}+1}&#124;&#124;,$
    |  | (3) |'
- en: 'where $F_{c}(x_{i};\theta)$ is the estimated head count, and $y_{i}$ is the
    ground truth head count. Then the total loss function is formulated as follow:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $F_{c}(x_{i};\theta)$ 是估计的头部数量，而 $y_{i}$ 是实际的头部数量。总损失函数的公式如下：
- en: '|  | $L_{total}=L_{E}+\alpha L_{c},$ |  | (4) |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{total}=L_{E}+\alpha L_{c},$ |  | (4) |'
- en: where $\alpha$ is the weight to balance the pixel-wise Euclidean loss and the
    total head counting loss. BL [[114](#bib.bib114)] stated that the original GT
    density map is imperfect due to occlusions, perspective effects, variations in
    object shapes and proposed Bayesian loss to constructs a density contribution
    probability model from the point annotations and addressed the above issues. The
    proposed Bayesian loss adopted more reliable supervision on the count expectation
    at each annotated point.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\alpha$ 是用来平衡像素级欧几里得损失和总头部计数损失的权重。BL [[114](#bib.bib114)] 说明了由于遮挡、透视效应、物体形状变化，原始
    GT 密度图是不完美的，并提出了贝叶斯损失，从点标注中构建了一个密度贡献概率模型，并解决了上述问题。提出的贝叶斯损失在每个标注点的计数期望上采用了更可靠的监督。
- en: SaCNN [[222](#bib.bib222)] proposed to combine density map loss with the relative
    count loss. The relative count loss helps to reduce the variance of the prediction
    errors and improve the network generalization on very sparse crowd scenes. CFF [[153](#bib.bib153)]
    fused segmentation map loss, density map loss and global density loss. Plug-and-Play
    Rescaling [[144](#bib.bib144)] combined regression loss with classification loss.
    Shallow Feature-based Dense Attention Network [[123](#bib.bib123)] proposed to
    use MSE loss with counting loss and stated that counting loss not only accelerates
    the convergence but also improves the counting accuracy. Multi-supervised Parallel
    Network [[196](#bib.bib196)] combined MSE loss, cross-entropy loss, and L1 loss.
    Besides, there is also some paper to use a kind of combination loss to enforce
    similarities in local coherence and spatial correlation between maps [[62](#bib.bib62)], [[136](#bib.bib136)] [[60](#bib.bib60)].
    Multi-task learning based framework is widely used in recent papers [[165](#bib.bib165)], [[88](#bib.bib88)], [[48](#bib.bib48)], [[70](#bib.bib70)], [[41](#bib.bib41)], [[227](#bib.bib227)], [[156](#bib.bib156)].
    However, this kind of framework is sensitive to hyper-parameters.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: SaCNN [[222](#bib.bib222)] 提出了将密度图损失与相对计数损失结合起来。相对计数损失有助于减少预测误差的方差，并提高网络在非常稀疏人群场景中的泛化能力。CFF
    [[153](#bib.bib153)] 融合了分割图损失、密度图损失和全局密度损失。Plug-and-Play Rescaling [[144](#bib.bib144)]
    将回归损失与分类损失结合。Shallow Feature-based Dense Attention Network [[123](#bib.bib123)]
    提出了使用均方误差损失与计数损失，并指出计数损失不仅加速了收敛，还提高了计数准确性。Multi-supervised Parallel Network [[196](#bib.bib196)]
    结合了均方误差损失、交叉熵损失和 L1 损失。此外，还有一些论文使用组合损失来强制局部一致性和空间相关性之间的相似性 [[62](#bib.bib62)],
    [[136](#bib.bib136)] [[60](#bib.bib60)]。基于多任务学习的框架在最近的论文中被广泛使用 [[165](#bib.bib165)],
    [[88](#bib.bib88)], [[48](#bib.bib48)], [[70](#bib.bib70)], [[41](#bib.bib41)],
    [[227](#bib.bib227)], [[156](#bib.bib156)]。然而，这种框架对超参数非常敏感。
- en: 4.4 Comparisons
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 比较
- en: 'We summarize the advantages and limitations of the above loss functions in
    Table [6](#S4.T6 "Table 6 ‣ 4.4 Comparisons ‣ 4 Loss Function ‣ A Survey on Deep
    Learning-based Single Image Crowd Counting: Network Design, Loss Function and
    Supervisory Signal"). We compare in Table [7](#S4.T7 "Table 7 ‣ 4.4 Comparisons
    ‣ 4 Loss Function ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal") the performance of several
    state-of-the-arts with different loss functions.'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表[6](#S4.T6 "Table 6 ‣ 4.4 Comparisons ‣ 4 Loss Function ‣ A Survey on Deep
    Learning-based Single Image Crowd Counting: Network Design, Loss Function and
    Supervisory Signal")中总结了上述损失函数的优缺点。我们在表[7](#S4.T7 "Table 7 ‣ 4.4 Comparisons ‣
    4 Loss Function ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal")中比较了几种最先进的方法在不同损失函数下的表现。'
- en: 'Table 6: Comparisons of recent advanced loss functions for crowd counting.
    The property of representative schemes for each loss functions category are summarized
    based on advantages and limitations.'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6：人群计数的近期先进损失函数比较。每种损失函数类别的代表方案属性基于优点和局限性进行总结。
- en: '| Category | Representative Scheme | Advantages | Limitations |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 代表方案 | 优点 | 局限性 |'
- en: '| Euclidean loss | CSRNet [[80](#bib.bib80)] |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| 欧几里得损失 | CSRNet [[80](#bib.bib80)] |'
- en: '&#124; Flexible; widely used &#124;'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 灵活；广泛使用 &#124;'
- en: '|'
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Not consider context &#124;'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不考虑上下文 &#124;'
- en: '&#124; information and visual quality &#124;'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 信息和视觉质量 &#124;'
- en: '|'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| SSIM loss | SANet [[11](#bib.bib11)] |'
  id: totrans-567
  prefs: []
  type: TYPE_TB
  zh: '| SSIM损失 | SANet [[11](#bib.bib11)] |'
- en: '&#124; Variants of structural similarity &#124;'
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 结构相似性的变体 &#124;'
- en: '&#124; loss to learn local correlation &#124;'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 学习局部相关性的损失 &#124;'
- en: '|'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Hard to learn the local &#124;'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 难以学习局部 &#124;'
- en: '&#124; correlation with various scales &#124;'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 与各种尺度的相关性 &#124;'
- en: '|'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Multi-task learning | MSPNet [[196](#bib.bib196)] |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| 多任务学习 | MSPNet [[196](#bib.bib196)] |'
- en: '&#124; Varied and flexible &#124;'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多样且灵活 &#124;'
- en: '&#124; to fuse different constrains &#124;'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 融合不同约束 &#124;'
- en: '|'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Sensitive to hyper-parameters &#124;'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对超参数敏感 &#124;'
- en: '|'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Others | S-DCNet [[205](#bib.bib205)] |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| 其他 | S-DCNet [[205](#bib.bib205)] |'
- en: '&#124; Efficient divide &#124;'
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 高效分割 &#124;'
- en: '&#124; and conquer manner &#124;'
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 征服方式 &#124;'
- en: '|'
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Computational expensive &#124;'
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 计算开销大 &#124;'
- en: '|'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'Table 7: Comparisons of state-of-the-art crowd counting approaches with different
    loss functions. Multi scale is the multi-scale design considerations; Dilated
    is dilated convolutions; Deform is the deformable convolutions; Atten represents
    the attention-based scheme. ST-A denotes ShanghaiTech A dataset [[228](#bib.bib228)]
    and ST-B denotes ShanghaiTech B dataset [[228](#bib.bib228)].'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7：不同损失函数的最先进人群计数方法比较。多尺度是多尺度设计考虑；膨胀是膨胀卷积；形变是可变形卷积；注意力表示基于注意力的方案。ST-A表示上海科技大学
    A 数据集 [[228](#bib.bib228)]，ST-B表示上海科技大学 B 数据集 [[228](#bib.bib228)]。
- en: '| Scheme |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '| 方案 |'
- en: '&#124; Multi &#124;'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 多种 &#124;'
- en: '&#124; scale &#124;'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 尺度 &#124;'
- en: '| Dilated | Deform | Atten |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '| 膨胀 | 形变 | 注意力 |'
- en: '&#124; Loss function &#124;'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 损失函数 &#124;'
- en: '|'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ST-A &#124;'
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ST-A &#124;'
- en: '&#124; MAE &#124;'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MAE &#124;'
- en: '|'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ST-A &#124;'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ST-A &#124;'
- en: '&#124; MSE &#124;'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSE &#124;'
- en: '|'
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ST-B &#124;'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ST-B &#124;'
- en: '&#124; MAE &#124;'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MAE &#124;'
- en: '|'
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; ST-B &#124;'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; ST-B &#124;'
- en: '&#124; MSE &#124;'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSE &#124;'
- en: '|'
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| CSRNet [[80](#bib.bib80)] |  | $\surd$ |  |  |'
  id: totrans-605
  prefs: []
  type: TYPE_TB
  zh: '| CSRNet [[80](#bib.bib80)] |  | $\surd$ |  |  |'
- en: '&#124; Euclidean loss &#124;'
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 欧几里得损失 &#124;'
- en: '| 68.2 | 115.0 | 10.6 | 16.0 |'
  id: totrans-607
  prefs: []
  type: TYPE_TB
  zh: '| 68.2 | 115.0 | 10.6 | 16.0 |'
- en: '| ADCrowd [[97](#bib.bib97)] |  |  | $\surd$ | $\surd$ |'
  id: totrans-608
  prefs: []
  type: TYPE_TB
  zh: '| ADCrowd [[97](#bib.bib97)] |  |  | $\surd$ | $\surd$ |'
- en: '&#124; Euclidean loss &#124;'
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 欧几里得损失 &#124;'
- en: '| 63.2 | 98.9 | 8.2 | 15.7 |'
  id: totrans-610
  prefs: []
  type: TYPE_TB
  zh: '| 63.2 | 98.9 | 8.2 | 15.7 |'
- en: '| DSSINet [[95](#bib.bib95)] | $\surd$ | $\surd$ |  |  | SSIM Loss | 60.63
    | 96.04 | 6.8 | 10.3 |'
  id: totrans-611
  prefs: []
  type: TYPE_TB
  zh: '| DSSINet [[95](#bib.bib95)] | $\surd$ | $\surd$ |  |  | SSIM Loss | 60.63
    | 96.04 | 6.8 | 10.3 |'
- en: '| S-DCNet [[205](#bib.bib205)] | $\surd$ |  |  |  | Divide-conquer | 58.3 |
    95.0 | 6.7 | 10.7 |'
  id: totrans-612
  prefs: []
  type: TYPE_TB
  zh: '| S-DCNet [[205](#bib.bib205)] | $\surd$ |  |  |  | 分而治之 | 58.3 | 95.0 | 6.7
    | 10.7 |'
- en: '| HA-CCN [[159](#bib.bib159)] |  |  |  | $\surd$ |'
  id: totrans-613
  prefs: []
  type: TYPE_TB
  zh: '| HA-CCN [[159](#bib.bib159)] |  |  |  | $\surd$ |'
- en: '&#124; MSE loss with &#124;'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; MSE损失与 &#124;'
- en: '&#124; global counting &#124;'
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 全球计数 &#124;'
- en: '| 62.9 | 94.9 | 8.1 | 13.4 |'
  id: totrans-616
  prefs: []
  type: TYPE_TB
  zh: '| 62.9 | 94.9 | 8.1 | 13.4 |'
- en: '| GLoss [[173](#bib.bib173)] |  |  |  |  |'
  id: totrans-617
  prefs: []
  type: TYPE_TB
  zh: '| GLoss [[173](#bib.bib173)] |  |  |  |  |'
- en: '&#124; Unbalanced optimal &#124;'
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不平衡的最优 &#124;'
- en: '&#124; transport loss &#124;'
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 运输损失 &#124;'
- en: '| 61.3 | 95.4 | 7.3 | 11.7 |'
  id: totrans-620
  prefs: []
  type: TYPE_TB
  zh: '| 61.3 | 95.4 | 7.3 | 11.7 |'
- en: CSRNet and ADCrowdNet are based on the same Euclidean loss but with different
    deep neural network designs and show different counting accuracy, which shows
    that the Euclidean loss is flexible and widely used in the early approaches. However,
    the Euclidean loss lacks contextual information and ignores the local correlation
    among pixels in the density maps.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: CSRNet 和 ADCrowdNet 基于相同的欧几里得损失，但具有不同的深度神经网络设计，并展示了不同的计数准确性，这表明欧几里得损失在早期方法中具有灵活性和广泛应用。然而，欧几里得损失缺乏上下文信息，并忽略了密度图像素之间的局部相关性。
- en: The DSSINet achieves better performance than CSRNet and ADCrowdNet on different
    crowd counting datasets. These variants of structural similarity loss show counting
    improvements based on utilizing local correlation. However, these kinds of methods
    suffer in the situation of a large spectrum of various scales.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: DSSINet 在不同的众包计数数据集上表现优于 CSRNet 和 ADCrowdNet。这些结构相似性损失的变体展示了基于利用局部相关性的计数改进。然而，这些方法在不同尺度的大范围情况下表现不佳。
- en: 'As Table [7](#S4.T7 "Table 7 ‣ 4.4 Comparisons ‣ 4 Loss Function ‣ A Survey
    on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function
    and Supervisory Signal") shows, DSSINet (SSIM loss) achieves better counting accuracy
    than ACSCP (Adversarial loss) with similar network design considerations (i.e.,
    multi-scale scheme and dilated convolutional operations). The poor performance
    of ACSCP on ShanghaiTech A & B may probably be due to the adversarial loss. This
    further demonstrates that adversarial loss can help to generate high-quality density
    maps but may sacrifice counting accuracy.'
  id: totrans-623
  prefs: []
  type: TYPE_NORMAL
  zh: '如表 [7](#S4.T7 "Table 7 ‣ 4.4 Comparisons ‣ 4 Loss Function ‣ A Survey on Deep
    Learning-based Single Image Crowd Counting: Network Design, Loss Function and
    Supervisory Signal") 所示，DSSINet（SSIM 损失）在类似网络设计考虑（即多尺度方案和膨胀卷积操作）下比 ACSCP（对抗性损失）实现了更好的计数准确性。ACSCP
    在 ShanghaiTech A & B 上的表现差可能是由于对抗性损失。这进一步证明了对抗性损失可以帮助生成高质量的密度图，但可能牺牲计数准确性。'
- en: HA-CNN shows better performance than ADCrowdNet even without deformable convolutional
    operations on two different crowd counting datasets. This demonstrates that multi-task
    learning with global counting constrain can work well in highly crowded scenes
    even without some advanced network operations. S-DCNet also achieves satisfactory
    counting accuracy on different crowd counting datasets, which confirms the effectiveness
    of the divide and conquer manner but is computationally expensive.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有变形卷积操作，HA-CNN 在两个不同的众包计数数据集上也表现优于 ADCrowdNet。这表明，即使没有一些先进的网络操作，带有全局计数约束的多任务学习在高密度场景中也能很好地工作。S-DCNet
    在不同的众包计数数据集上也取得了令人满意的计数准确性，这确认了分而治之的有效性，但计算开销较大。
- en: 4.5 Others
  id: totrans-625
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 其他
- en: 'There are some other loss optimization strategies to enhance crowd counting
    tasks [[94](#bib.bib94), [176](#bib.bib176), [190](#bib.bib190), [65](#bib.bib65),
    [149](#bib.bib149)]. CNN-Boosting [[169](#bib.bib169)] employed CNNs and incorporate
    two significant improvements: layered boosting and selective sampling. DAL-SVR [[197](#bib.bib197)]
    boosted deep attribute learning via support vector regression for fast-moving
    crowd counting. The paper learned superpixel segmentation-fast moving segmentation-feature
    extraction-motion features/appearance features/sift feature-features aggregation
    by PCA-regression learning SVR-data fusion and deeply learning cumulative attribute.
    D-ConvNet [[154](#bib.bib154)] used seep negative correlation learning, which
    is a successful ensemble learning technique for crowd counting. The authors extended
    D-ConvNet in [[223](#bib.bib223)], which proposed to regress via an efficient
    divide and conquer manner. D-ConvNet has been shown to work well for non-deep
    regression problems. Without extra parameters, the method controls the bias-variance-covariance
    trade-off systematically and usually yields a deep regression ensemble where each
    base model is both accurate and diversified. However, the whole framework is computationally
    expensive.'
  id: totrans-626
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的损失优化策略来增强人群计数任务 [[94](#bib.bib94), [176](#bib.bib176), [190](#bib.bib190),
    [65](#bib.bib65), [149](#bib.bib149)]。CNN-Boosting [[169](#bib.bib169)] 使用了卷积神经网络，并结合了两个重要的改进：分层提升和选择性采样。DAL-SVR [[197](#bib.bib197)]
    通过支持向量回归提升了深度属性学习，以实现快速移动的人群计数。该论文学习了超像素分割-快速移动分割-特征提取-运动特征/外观特征/SIFT特征-特征聚合（通过
    PCA）-回归学习 SVR-数据融合和深度学习累积属性。D-ConvNet [[154](#bib.bib154)] 使用了深度负相关学习，这是一种成功的人群计数集成学习技术。作者在 [[223](#bib.bib223)]
    中扩展了 D-ConvNet，提出通过高效的分割与征服方式进行回归。D-ConvNet 已被证明在非深度回归问题上表现良好。在没有额外参数的情况下，该方法系统地控制了偏差-方差-协方差的权衡，并通常产生一个深度回归集成，其中每个基础模型既准确又多样。然而，整个框架计算开销较大。
- en: S-DCNet [[205](#bib.bib205)] designed a multi-stage spatial divide and conquer
    network. The collected images and labeled count values are limited in reality
    for crowd counting, which means that only a small closed set is observed. A dense
    region can always be divide until sub-region counts are within the previously
    observed closed set. S-DCNet only learns from a closed set but can generalize
    well to open-set scenarios. And avoid repeatedly computing sub-region convolutional
    features, this method is also efficient.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: S-DCNet [[205](#bib.bib205)] 设计了一个多阶段空间分割与征服网络。实际中用于人群计数的图像和标注计数值是有限的，这意味着只观察到一个小的封闭集合。密集区域可以不断被分割，直到子区域计数值落在先前观察到的封闭集合内。S-DCNet
    仅从封闭集合中学习，但能够很好地泛化到开放集场景。为了避免重复计算子区域的卷积特征，这种方法也是高效的。
- en: 5 Supervisory Signal
  id: totrans-628
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 监督信号
- en: 'In this section, we discuss different supervisory signals for crowd counting:
    fully supervised learning (Section [5.1](#S5.SS1 "5.1 Fully Supervised Learning
    ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal")), weakly supervised and
    semi-supervised learning (Section [5.2](#S5.SS2 "5.2 Weakly Supervised and Semi-supervised
    Learning ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based Single Image
    Crowd Counting: Network Design, Loss Function and Supervisory Signal")), unsupervised
    and self-supervised learning (Section [5.3](#S5.SS3 "5.3 Unsupervised and Self-supervised
    Learning ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based Single Image
    Crowd Counting: Network Design, Loss Function and Supervisory Signal")), and automatic
    labeling through synthetic data (Section [5.4](#S5.SS4 "5.4 Automatic Labeling
    through Synthetic Data ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")).
    We evaluate and compare them in Section [5.6](#S5.SS6 "5.6 Comparisons ‣ 5 Supervisory
    Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal").'
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: '在本节中，我们讨论了人群计数的不同监督信号：完全监督学习（第[5.1](#S5.SS1 "5.1 Fully Supervised Learning
    ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal")节），弱监督和半监督学习（第[5.2](#S5.SS2
    "5.2 Weakly Supervised and Semi-supervised Learning ‣ 5 Supervisory Signal ‣ A
    Survey on Deep Learning-based Single Image Crowd Counting: Network Design, Loss
    Function and Supervisory Signal")节），无监督和自监督学习（第[5.3](#S5.SS3 "5.3 Unsupervised
    and Self-supervised Learning ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")节），以及通过合成数据进行自动标注（第[5.4](#S5.SS4
    "5.4 Automatic Labeling through Synthetic Data ‣ 5 Supervisory Signal ‣ A Survey
    on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function
    and Supervisory Signal")节）。我们在第[5.6](#S5.SS6 "5.6 Comparisons ‣ 5 Supervisory
    Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal")节对它们进行了评估和比较。'
- en: 5.1 Fully Supervised Learning
  id: totrans-630
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 完全监督学习
- en: 'In the fully supervised crowd counting paradigm, the model is hard to optimize
    if we utilize the original discrete point-wise annotation maps as ground truth [[83](#bib.bib83),
    [39](#bib.bib39), [218](#bib.bib218), [109](#bib.bib109), [89](#bib.bib89), [1](#bib.bib1),
    [192](#bib.bib192), [234](#bib.bib234), [172](#bib.bib172), [147](#bib.bib147)].
    There are also some recent works study the problem of counting from scalar representations [[182](#bib.bib182),
    [163](#bib.bib163), [84](#bib.bib84), [116](#bib.bib116)]. The continuous ground
    truth density map is usually generated from the original point-wise annotations
    via different ground truth generation methods such as applying an adaptive Gaussian
    kernel for each head annotation, which is important for accurate crowd estimation [[174](#bib.bib174)].
    The fixed kernel or adaptive Gaussian kernel are widely used approaches to prepossess
    the original annotation and get the ground truth for density estimation and crowd
    counting [[80](#bib.bib80)]. The geometry-adaptive kernel is defined as follows:'
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: 在完全监督的人群计数范式中，如果我们使用原始的离散点注释图作为地面真实值，模型很难进行优化 [[83](#bib.bib83)，[39](#bib.bib39)，[218](#bib.bib218)，[109](#bib.bib109)，[89](#bib.bib89)，[1](#bib.bib1)，[192](#bib.bib192)，[234](#bib.bib234)，[172](#bib.bib172)，[147](#bib.bib147)]。一些最近的工作也研究了从标量表示中进行计数的问题 [[182](#bib.bib182)，[163](#bib.bib163)，[84](#bib.bib84)，[116](#bib.bib116)]。连续的地面真实值密度图通常是通过不同的地面真实值生成方法从原始点注释生成的，例如对每个头部注释应用自适应高斯核，这对准确的人群估计非常重要 [[174](#bib.bib174)]。固定核或自适应高斯核是预处理原始注释并获得密度估计和人群计数地面真实值的广泛使用的方法 [[80](#bib.bib80)]。几何自适应核定义如下：
- en: '|  | $F(x)=\sum_{i=1}^{N}\delta(x-x_{i})\times G_{\sigma_{i}}(x),with\;\sigma_{i}=\beta\bar{d_{i}},$
    |  | (5) |'
  id: totrans-632
  prefs: []
  type: TYPE_TB
  zh: '|  | $F(x)=\sum_{i=1}^{N}\delta(x-x_{i})\times G_{\sigma_{i}}(x),with\;\sigma_{i}=\beta\bar{d_{i}},$
    |  | (5) |'
- en: where x denotes the pixel position in an image. For each target object, $x_{i}$
    in the ground truth, which is presented with a delta function $\delta(x-x_{i})$.
    The ground truth density map $F(x)$ is generated by convolving $\delta(x-x_{i})$
    with a normalized Gaussian kernel based on parameter $\sigma_{i}$. And $\bar{d_{i}}$
    shows the average distance of the k nearest neighbors.
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 x 表示图像中的像素位置。对于每个目标物体，地面真实值中的 $x_{i}$ 由一个 delta 函数 $\delta(x-x_{i})$ 表示。地面真实值密度图
    $F(x)$ 是通过将 $\delta(x-x_{i})$ 与基于参数 $\sigma_{i}$ 的归一化高斯核进行卷积生成的。而 $\bar{d_{i}}$
    显示了 k 个最近邻的平均距离。
- en: '![Refer to caption](img/1e0440f85177552ab633e5e841bc5658.png)'
  id: totrans-634
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/1e0440f85177552ab633e5e841bc5658.png)'
- en: 'Figure 5: The workflow of the original semi-supervised learning for classification
    problem (Figure a) and semi-supervised learning for single image crowd counting
    (Figure b) [[127](#bib.bib127)].'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：原始半监督学习的工作流程，用于分类问题（图 a）和用于单图像人群计数的半监督学习（图 b）[[127](#bib.bib127)]。
- en: 'GP [[9](#bib.bib9)] devises a Bayesian model that places a Gaussian process
    before a latent function whose square is the count density. Compared to different
    annotation methods concerning their difficulty for the annotator: dots or bounding
    box in all objects, GP is better in terms of accuracy and labeling effort. Besides,
    there are some recent advances to use a learned kernel to improve the prepossessing
    step and proposed an adaptive density map generator [[170](#bib.bib170)].'
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: GP [[9](#bib.bib9)] 设计了一种贝叶斯模型，该模型在潜在函数之前放置了一个高斯过程，其平方即为计数密度。与不同标注方法在标注者的难度方面进行比较：在所有对象中的点或边界框，GP
    在准确性和标注工作量方面更具优势。此外，最近有一些进展使用学习到的核来改进预处理步骤，并提出了一种自适应密度图生成器 [[170](#bib.bib170)]。
- en: DM-Count [[175](#bib.bib175)] optimizes the network directly on the dot map,
    which can be considered as a special type of density map with $1\times 1$ Gaussian
    blur. Most existing methods need to use an adaptive or fixed Gaussian to smooth
    each annotated dot or to estimate the likelihood of every pixel given the annotated
    point. DM-Count directly optimizes the original annotation and shows its generation
    error bound is tighter than that of Gaussian smoothed methods.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: DM-Count [[175](#bib.bib175)] 直接在点图上优化网络，可以视为一种特殊类型的密度图，具有 $1\times 1$ 的高斯模糊。大多数现有方法需要使用自适应或固定的高斯平滑每个标注点，或者估计每个像素在给定标注点时的可能性。DM-Count
    直接优化原始标注，并且其生成误差界限比高斯平滑方法更紧凑。
- en: 5.2 Weakly Supervised and Semi-supervised Learning
  id: totrans-638
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 **弱监督和半监督学习**
- en: 'Recently, a number of works have emerged to make use of weakly labeled data
    for crowd counting [[122](#bib.bib122), [191](#bib.bib191), [67](#bib.bib67),
    [161](#bib.bib161), [105](#bib.bib105), [231](#bib.bib231), [217](#bib.bib217),
    [68](#bib.bib68), [235](#bib.bib235)] and the problem of learning from noisy annotations [[171](#bib.bib171),
    [78](#bib.bib78)]. The original annotation process for crowd counting via density
    map estimation is point-level annotation, which is labor-intensive, HA-CCN [[159](#bib.bib159)]
    proposed a weakly supervised learning setup and leveraged the image-level labels
    instead of the densely point-wise annotation process to reduce label effort. As
    shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")
    (b), the first column is the original image, the second column is the labor-intensive
    dense (head) annotation, the third column is the ground truth density maps, and
    the last column is the image-level weak annotation, which is used in the weakly
    supervised learning setting. This clearly shows that leveraging weakly labeled
    data (the last column) can largely reduce the annotation complexity compared with
    fully point-wise annotation (the second column). Besides, Scale-Recursive Network
    (SRN) with point supervision [[32](#bib.bib32)] is also a kind of weakly supervised
    framework based on SRN structure.'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，出现了许多工作利用弱标注数据进行人群计数 [[122](#bib.bib122), [191](#bib.bib191), [67](#bib.bib67),
    [161](#bib.bib161), [105](#bib.bib105), [231](#bib.bib231), [217](#bib.bib217),
    [68](#bib.bib68), [235](#bib.bib235)]，以及从噪声标注中学习的问题 [[171](#bib.bib171), [78](#bib.bib78)]。通过密度图估计进行人群计数的原始标注过程是点级标注，这是一项劳动密集型的工作。HA-CCN [[159](#bib.bib159)]
    提出了一个弱监督学习设置，并利用图像级别标签代替密集的点级标注过程，以减少标注工作量。如图 [2](#S1.F2 "图 2 ‣ 1 引言 ‣ 深度学习单图像人群计数的综述：网络设计、损失函数和监督信号")
    (b) 所示，第一列是原始图像，第二列是劳动密集型的密集（头部）标注，第三列是真实密度图，最后一列是图像级弱标注，后者用于弱监督学习设置。这清楚地表明，利用弱标注数据（最后一列）可以大大减少标注复杂性，相比于完全点级的标注（第二列）。此外，基于
    SRN 结构的 Scale-Recursive Network (SRN) 具有点监督 [[32](#bib.bib32)]，也是一种基于 SRN 结构的弱监督框架。
- en: 'Typical semi-supervised GANs are unable to function in the regression regime
    due to biases introduced when using a single prediction goal. DG-GAN [[127](#bib.bib127)]
    generalized semi-supervised generative adversarial network (GANs) from classification
    problems to regression for use in dense crowd counting, refer to Fig. [5](#S5.F5
    "Figure 5 ‣ 5.1 Fully Supervised Learning ‣ 5 Supervisory Signal ‣ A Survey on
    Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function
    and Supervisory Signal"). This work allows the dual-goal GAN to benefit from unlabeled
    data in the training process. And [[130](#bib.bib130)] is an extension of DG-GAN,
    which proposed a novel loss function for feature contrasting and resulted in a
    discriminator that can distinguish between fake and real data based on feature
    statistics. However, weakly supervised crowd counting still requires annotations.
    Besides, it also requires task-specific knowledge to design effective neural networks
    and loss functions for leveraging weakly labeled data.'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: '典型的半监督 GAN 在回归模式下无法正常工作，因为使用单一预测目标会引入偏差。DG-GAN [[127](#bib.bib127)] 将半监督生成对抗网络
    (GANs) 从分类问题推广到回归，用于密集人群计数，参见图 [5](#S5.F5 "Figure 5 ‣ 5.1 Fully Supervised Learning
    ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal")。这项工作使得双目标 GAN 能够在训练过程中利用未标注数据。而
    [[130](#bib.bib130)] 是 DG-GAN 的扩展，提出了一种新颖的特征对比损失函数，结果生成了一个能够基于特征统计区分虚假和真实数据的鉴别器。然而，弱监督人群计数仍需要标注。此外，它还需要任务特定的知识来设计有效的神经网络和损失函数，以利用弱标注数据。'
- en: '![Refer to caption](img/ce63e790f755b70bfe184a54e6adfff1.png)'
  id: totrans-641
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ce63e790f755b70bfe184a54e6adfff1.png)'
- en: 'Figure 6: The workflow of self-supervised learning and almost unsupervised
    learning for crowd counting. (a) The architecture of L2R: a self-supervised learning
    setup [[103](#bib.bib103)]. (b) The framework of GWTA-CCNN: an almost unsupervised
    learning method [[148](#bib.bib148)].'
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：人群计数的自监督学习和几乎无监督学习的工作流程。 (a) L2R 的架构：一个自监督学习设置 [[103](#bib.bib103)]。 (b)
    GWTA-CCNN 的框架：一种几乎无监督学习方法 [[148](#bib.bib148)]。
- en: 5.3 Unsupervised and Self-supervised Learning
  id: totrans-643
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 无监督和自监督学习
- en: 'Deep learning-based approaches are highly data-driven, i.e., they require a
    large amount of diverse labeled data in the training process. The labeling process
    for crowd counting is expensive, but the unlabeled data are cheap and widely available [[146](#bib.bib146),
    [107](#bib.bib107)]. L2R [[103](#bib.bib103)] leveraged abundantly available unlabeled
    crowd images in learning to rank framework, refer to Fig. [6](#S5.F6 "Figure 6
    ‣ 5.2 Weakly Supervised and Semi-supervised Learning ‣ 5 Supervisory Signal ‣
    A Survey on Deep Learning-based Single Image Crowd Counting: Network Design, Loss
    Function and Supervisory Signal") (a), which is based on the observation that
    any sub-image of a crowded scene image is guaranteed to contain the same number
    or fewer persons than the super-image. The pixel-wise regression loss is fused
    with the ranking regularization to learn better representation for crowd counting
    tasks on unlabeled data.'
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: '基于深度学习的方法高度依赖数据，即在训练过程中需要大量多样化的标注数据。人群计数的标注过程昂贵，但未标注的数据便宜且广泛可用 [[146](#bib.bib146),
    [107](#bib.bib107)]。L2R [[103](#bib.bib103)] 在学习排序框架中利用了大量可用的未标注人群图像，参见图 [6](#S5.F6
    "Figure 6 ‣ 5.2 Weakly Supervised and Semi-supervised Learning ‣ 5 Supervisory
    Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal") (a)，这基于观察到拥挤场景图像的任何子图像保证包含与超级图像相同数量或更少的人。像素级回归损失与排序正则化相融合，以便在未标注数据上学习更好的表示，从而用于人群计数任务。'
- en: 'There is another potential direction to make use of unlabeled data such as
    the convolutional Winner-Take-All models, whose most parameters are obtained by
    unsupervised learning. GWTA-CCNN [[148](#bib.bib148)] utilized a Grid Winner-Take-All
    (GWTA) autoencoder to learn several layers of useful filters from unlabeled crowd
    images, refer to Fig. [6](#S5.F6 "Figure 6 ‣ 5.2 Weakly Supervised and Semi-supervised
    Learning ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based Single Image
    Crowd Counting: Network Design, Loss Function and Supervisory Signal") (b). A
    small patch cropped from the original image is fed into the model. Most of the
    parameters are trained layer by layer based on the reconstruction loss. GWTA divides
    a convolution layer spatially into a grid of cells. Within each cell, only the
    maximumly activated neuron is allowed to update the filter. almost 99.9$\%$ of
    the parameters of the proposed model are trained without any labeled data, which
    the rest 0.1$\%$ are tuned with supervision. However, these kinds of self-supervised
    learning and almost unsupervised crowd counting approaches need a large amount
    of data to show effectiveness, which requires more training time and computational
    resources.'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: '另一个潜在方向是利用未标注数据，如卷积赢家通吃模型，其中大多数参数通过无监督学习获得。GWTA-CCNN [[148](#bib.bib148)] 利用网格赢家通吃
    (GWTA) 自编码器从未标注的群体图像中学习几个层次的有用滤波器，参见图 [6](#S5.F6 "Figure 6 ‣ 5.2 Weakly Supervised
    and Semi-supervised Learning ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")
    (b)。将从原始图像裁剪的小块输入模型。大多数参数基于重建损失逐层训练。GWTA 将卷积层在空间上划分为网格单元。在每个单元内，只有激活最大神经元被允许更新滤波器。提议模型的几乎
    99.9$\%$ 参数在没有任何标注数据的情况下训练，其余的 0.1$\%$ 在监督下调整。然而，这些自监督学习和几乎无监督的群体计数方法需要大量数据才能显现有效性，这需要更多的训练时间和计算资源。'
- en: Lei et al. [[72](#bib.bib72)] proposed a weakly supervised crowd counting method
    to train the model from a small number of dot-map annotations and a large number
    of count-level annotations, with is used to reducing the annotation cost for crowd
    counting. The key idea is to enforce the consistency between density maps and
    total object count on weakly labeled images as regularization terms. The work
    of complete self-supervision  [[146](#bib.bib146)] introduce a new training paradigm
    that does not need labeled data. This work reveals the power law nature for the
    distribution of crowds and adopt this signal for backpropagation in the optimal
    transport framework. This work achieves efficient crowd estimation.
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: Lei 等人 [[72](#bib.bib72)] 提出了一种弱监督群体计数方法，利用少量点图注释和大量计数级注释来训练模型，从而降低群体计数的注释成本。关键思想是在弱标注图像上强制密度图和总体物体计数的一致性作为正则化项。完整自监督
    [[146](#bib.bib146)] 介绍了一种不需要标注数据的新训练范式。该工作揭示了人群分布的幂律特性，并采用该信号进行最优传输框架中的反向传播。这项工作实现了高效的人群估计。
- en: 'Table 8: Comparisons of different supervisory signals for crowd counting. The
    representative schemes of different supervisory signals are analyzed based on
    their advantages and limitations.'
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: '表 8: 群体计数不同监督信号的比较。对不同监督信号的代表性方案进行了基于其优势和局限性的分析。'
- en: '| Category | Schemes | Advantages | Limitations |'
  id: totrans-648
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 方案 | 优势 | 局限性 |'
- en: '| Fully Supervised learning | CP-CNN [[157](#bib.bib157)] |'
  id: totrans-649
  prefs: []
  type: TYPE_TB
  zh: '| 完全监督学习 | CP-CNN [[157](#bib.bib157)] |'
- en: '&#124; Adaptive Gaussian kernel &#124;'
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自适应高斯核 &#124;'
- en: '&#124; to accommodate &#124;'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 适应 &#124;'
- en: '&#124; different scales &#124;'
  id: totrans-652
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 不同尺度 &#124;'
- en: '|'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Not flexible to &#124;'
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 对 &#124;'
- en: '&#124; non-rigid object &#124;'
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 非刚性物体中 &#124;'
- en: '|'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| BL [[114](#bib.bib114)] |'
  id: totrans-657
  prefs: []
  type: TYPE_TB
  zh: '| BL [[114](#bib.bib114)] |'
- en: '&#124; Bayesian loss &#124;'
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 贝叶斯损失 &#124;'
- en: '&#124; to model &#124;'
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 建模 &#124;'
- en: '&#124; non-rigid objects &#124;'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 非刚性物体 &#124;'
- en: '|'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; More reliable supervision &#124;'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 更可靠的监督 &#124;'
- en: '&#124; but suffers in &#124;'
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 但在 &#124;'
- en: '&#124; large varied scales &#124;'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大的多变尺度 &#124;'
- en: '|'
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Weakly supervised and &#124;'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 弱监督和 &#124;'
- en: '&#124; semi-supervised learning &#124;'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 半监督学习 &#124;'
- en: '| HA-CCN [[159](#bib.bib159)] |'
  id: totrans-669
  prefs: []
  type: TYPE_TB
  zh: '| HA-CCN [[159](#bib.bib159)] |'
- en: '&#124; Low annotation &#124;'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低注释 &#124;'
- en: '&#124; complexity &#124;'
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 复杂性 &#124;'
- en: '|'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Still requires weakly &#124;'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 仍然需要弱标注 &#124;'
- en: '&#124; annotations and task &#124;'
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 注释和任务 &#124;'
- en: '&#124; specific knowledge &#124;'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 特定知识 &#124;'
- en: '|'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Unsupervised and &#124;'
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 无监督和 &#124;'
- en: '&#124; self-supervised learning &#124;'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自监督学习 &#124;'
- en: '| L2R [[103](#bib.bib103)] |'
  id: totrans-680
  prefs: []
  type: TYPE_TB
  zh: '| L2R [[103](#bib.bib103)] |'
- en: '&#124; Low annotation cost; &#124;'
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 低标注成本; &#124;'
- en: '&#124; abundantly available &#124;'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 丰富可用 &#124;'
- en: '|'
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Large amount of &#124;'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大量的 &#124;'
- en: '&#124; data requires more &#124;'
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 数据需要更多 &#124;'
- en: '&#124; training time &#124;'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 训练时间 &#124;'
- en: '|'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '|'
  id: totrans-688
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Automatic labeling &#124;'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 自动标注 &#124;'
- en: '&#124; through synthetic data &#124;'
  id: totrans-690
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 通过合成数据 &#124;'
- en: '| CCWld [[186](#bib.bib186)] |'
  id: totrans-691
  prefs: []
  type: TYPE_TB
  zh: '| CCWld [[186](#bib.bib186)] |'
- en: '&#124; Reduce labeling effort; &#124;'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 减少标注工作量; &#124;'
- en: '&#124; enahnce accuracy; &#124;'
  id: totrans-693
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提高准确性; &#124;'
- en: '&#124; improve robustness &#124;'
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 提升鲁棒性 &#124;'
- en: '|'
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; Large domain gap &#124;'
  id: totrans-696
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 大领域差距 &#124;'
- en: '&#124; from synthetic &#124;'
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 来自合成数据 &#124;'
- en: '&#124; to real data &#124;'
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; 真实数据 &#124;'
- en: '|'
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 5.4 Automatic Labeling through Synthetic Data
  id: totrans-700
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 通过合成数据自动标注
- en: There are more challenges for crowd counting in the wild due to the changeable
    environment, large-range number of people cause the current methods can not work
    well. Due to scarce data, many methods suffer from over-fitting to a different
    extent. Some researchers attempt to tackle this problem through synthetic data [[53](#bib.bib53),
    [188](#bib.bib188)]. CCWld [[186](#bib.bib186)] built a large-scale, diverse synthetic
    dataset, pretrain a crowd counter on the synthetic data, finetune on real data,
    propose a counting method via domain adaptation based cycle GAN, free humans from
    heavy data annotations. The authors in [[48](#bib.bib48)] based on the GCC dataset,
    designed a better domain adaptation scheme for reducing the counting noise in
    the background area. This paper pays more attention to the semantic consistency
    of the crowd and then could narrow the gap using a large-scale human detection
    dataset to train a crowd, semantic model. This method reduces the labeling effort,
    enhances accuracy, and improves robustness by making use of synthetic data. However,
    the synthetic data are still witnessed a larger domain gap compared with real
    data.
  id: totrans-701
  prefs: []
  type: TYPE_NORMAL
  zh: 由于环境变化无常，人群计数在野外面临更多挑战，大范围的人数使得当前方法无法有效工作。由于数据稀缺，许多方法在不同程度上受到过拟合困扰。一些研究者尝试通过合成数据
    [[53](#bib.bib53), [188](#bib.bib188)] 来解决这个问题。CCWld [[186](#bib.bib186)] 构建了一个大规模、多样化的合成数据集，在合成数据上预训练了一个人群计数器，并在真实数据上进行微调，提出了一种通过领域适应的循环
    GAN 的计数方法，减少了数据标注的工作量。[[48](#bib.bib48)] 的作者基于 GCC 数据集，设计了一种更好的领域适应方案，以减少背景区域的计数噪声。本文更多关注人群的语义一致性，并利用大规模的人体检测数据集训练人群语义模型，从而缩小了差距。这种方法减少了标注工作量，提高了准确性，并通过利用合成数据提升了鲁棒性。然而，合成数据与真实数据相比仍然存在较大的领域差距。
- en: 5.5 Domain Adaptive Crowd Counting
  id: totrans-702
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 领域自适应人群计数
- en: Most of the existing crowd counting methods are designed in a specific domain.
    Thus, designing crowd counting models that can achieve high counting performance
    in any domain is a challenging but meaningful problem. There is some robust crowd
    counting approaches against domain shifts proposed in recent years [[226](#bib.bib226),
    [51](#bib.bib51), [98](#bib.bib98), [214](#bib.bib214), [232](#bib.bib232), [138](#bib.bib138)].
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 现有的大多数人群计数方法都是针对特定领域设计的。因此，设计能够在任何领域实现高计数性能的人群计数模型是一个具有挑战但意义重大的问题。近年来提出了一些针对领域迁移的鲁棒人群计数方法
    [[226](#bib.bib226), [51](#bib.bib51), [98](#bib.bib98), [214](#bib.bib214), [232](#bib.bib232),
    [138](#bib.bib138)]。
- en: CVCS [[226](#bib.bib226)] proposes a cross-view cross-scene multi-view crowd
    counting paradigm, where the training and test set are from different scenes with
    arbitrary camera locations. CVCS are able to attentively selects and fuses multiple
    views using camera layout geometry, and a noise view regularization method to
    handle non-correspondence errors. CDCC [[189](#bib.bib189)] proposes a neural
    linear transformation method, which exploits domain factor and biases weights
    to learn the domain shift. AdaCrowd [[139](#bib.bib139)] makes use of a crowd
    counting network and a guiding network, which predicts some parameters in the
    counting network based on the unlabeled data from a particular scene and adapt
    to the new scene.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: CVCS [[226](#bib.bib226)] 提出了一个跨视角跨场景的多视角人群计数范式，其中训练集和测试集来自具有任意摄像头位置的不同场景。CVCS
    能够通过摄像头布局几何来精确选择和融合多个视角，并采用噪声视角正则化方法来处理不对应错误。CDCC [[189](#bib.bib189)] 提出了神经线性变换方法，该方法利用领域因子并偏置权重来学习领域迁移。AdaCrowd
    [[139](#bib.bib139)] 利用人群计数网络和引导网络，根据特定场景的未标记数据预测计数网络中的一些参数，并适应新场景。
- en: The work of [[43](#bib.bib43)] introduces a domain-adaptation-style crowd counting
    method by using multilevel feature-aware adaptation and structured density map
    alignment module, which is trained on generated data with ground-truth to the
    specific real-world scenes. The work [[40](#bib.bib40)] proposes to learn from
    synthetic crowd data and transferring knowledge to real data without ground truth.
    This DACC frame work adopt a high-quality image translation and density map reconstruction
    to enhance cross domain crowd counting quality. The work [[10](#bib.bib10)] propose
    a two-step approach that captures the intra-domain knowledge to facilitate unsupervised
    cross-domain crowd counting via synthetic datasets.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: '[[43](#bib.bib43)] 的工作通过使用多级特征感知适应和结构化密度图对齐模块，引入了一种领域适应式的人群计数方法，该方法在与地面真实数据相符的生成数据上进行训练。[[40](#bib.bib40)]
    的工作提出了从合成的人群数据中学习并将知识转移到没有真实数据的真实数据中。这个 DACC 框架采用高质量的图像翻译和密度图重建，以提高跨领域人群计数质量。[[10](#bib.bib10)]
    的工作提出了一种两步法，通过合成数据集捕捉领域内知识，以促进无监督的跨领域人群计数。'
- en: The scale or density gap among datasets is another type of domain gap for domain
    adaptive crowd counting [[113](#bib.bib113), [209](#bib.bib209), [199](#bib.bib199),
    [44](#bib.bib44)]. For example, The work of [[113](#bib.bib113)] proposes a universal
    crowd counting model that can be applied across scenes and datasets via a scale
    alignment module. DCANet [[209](#bib.bib209)] introduces a domain-guided channel
    attention network to guide the extraction of domain-specific feature representation
    for multi-domain crowd counting. DKPNet [[15](#bib.bib15)] designs a domain-specific
    knowledge propagating network for extracking knowledge from multiple domains for
    improving crowd counting performance.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集之间的尺度或密度差异是领域自适应人群计数的另一种领域差距 [[113](#bib.bib113), [209](#bib.bib209), [199](#bib.bib199),
    [44](#bib.bib44)]。例如，[[113](#bib.bib113)] 的工作提出了一种通用的人群计数模型，可以通过尺度对齐模块应用于各种场景和数据集。DCANet [[209](#bib.bib209)]
    引入了一种领域引导的通道注意力网络，用于引导提取领域特定的特征表示，以实现多领域人群计数。DKPNet [[15](#bib.bib15)] 设计了一种领域特定的知识传播网络，用于从多个领域提取知识，以提高人群计数性能。
- en: 'Table 9: Quantitative comparisons of state-of-the-art crowd counting approaches
    with different supervisory signals. Column shows the type and number of columns
    for counting model. Multi is the Multi-column network; Double represents two columns;
    Single is the single column network. ST PartA and ST PartB denotes ShanghaiTech
    A & B dataset [[228](#bib.bib228)], respectively. The evaluation metrics for counting
    accuracy is MAE and MSE.'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9：不同监督信号下最先进的人群计数方法的定量比较。列显示了计数模型的类型和列数。Multi 是多列网络；Double 表示两列；Single 是单列网络。ST
    PartA 和 ST PartB 分别表示上海科技大学 A 和 B 数据集 [[228](#bib.bib228)]。计数精度的评价指标是 MAE 和 MSE。
- en: '| Typical Schemes | ST PartA | ST PartB | UCF_CC_50 | UCF-QNRF |'
  id: totrans-708
  prefs: []
  type: TYPE_TB
  zh: '| 典型方案 | ST PartA | ST PartB | UCF_CC_50 | UCF-QNRF |'
- en: '| Methods | Column | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 列数 | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE |'
- en: '| CP-CNN [[157](#bib.bib157)] | Multi | 73.6 | 106.4 | 20.1 | 30.1 | 295.8
    | 320.9 | - | - |'
  id: totrans-710
  prefs: []
  type: TYPE_TB
  zh: '| CP-CNN [[157](#bib.bib157)] | Multi | 73.6 | 106.4 | 20.1 | 30.1 | 295.8
    | 320.9 | - | - |'
- en: '|'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; L2R [[103](#bib.bib103)] (Query by example) &#124;'
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; L2R [[103](#bib.bib103)]（按示例查询）&#124;'
- en: '| Double | 72.0 | 106.6 | 14.4 | 23.8 | 291.5 | 397.6 | - | - |'
  id: totrans-713
  prefs: []
  type: TYPE_TB
  zh: '| Double | 72.0 | 106.6 | 14.4 | 23.8 | 291.5 | 397.6 | - | - |'
- en: '|'
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '&#124; L2R [[103](#bib.bib103)] (Query by keyword) &#124;'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '&#124; L2R [[103](#bib.bib103)]（按关键词查询）&#124;'
- en: '| Double | 73.6 | 112.0 | 13.7 | 21.4 | 279.6 | 388.9 | - | - |'
  id: totrans-716
  prefs: []
  type: TYPE_TB
  zh: '| Double | 73.6 | 112.0 | 13.7 | 21.4 | 279.6 | 388.9 | - | - |'
- en: '| BL [[114](#bib.bib114)] | Single | 62.8 | 101.8 | 7.7 | 12.7 | 229.3 | 308.2
    | 88.7 | 154.8 |'
  id: totrans-717
  prefs: []
  type: TYPE_TB
  zh: '| BL [[114](#bib.bib114)] | Single | 62.8 | 101.8 | 7.7 | 12.7 | 229.3 | 308.2
    | 88.7 | 154.8 |'
- en: '| CCWld [[186](#bib.bib186)] | Single | 64.8 | 107.5 | 7.6 | 13.0 | - | - |
    102.0 | 171.4 |'
  id: totrans-718
  prefs: []
  type: TYPE_TB
  zh: '| CCWld [[186](#bib.bib186)] | Single | 64.8 | 107.5 | 7.6 | 13.0 | - | - |
    102.0 | 171.4 |'
- en: '| URC [[208](#bib.bib208)] | Single | 72.8 | 111.6 | 12.0 | 18.7 | 294.0 |
    443.1 | 128.1 | 218.1 |'
  id: totrans-719
  prefs: []
  type: TYPE_TB
  zh: '| URC [[208](#bib.bib208)] | Single | 72.8 | 111.6 | 12.0 | 18.7 | 294.0 |
    443.1 | 128.1 | 218.1 |'
- en: '| HA-CCN [[159](#bib.bib159)] | Single | 58.3 | 95.0 | 6.7 | 10.7 | 256.2 |
    348.4 | 118.1 | 180.4 |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '| HA-CCN [[159](#bib.bib159)] | Single | 58.3 | 95.0 | 6.7 | 10.7 | 256.2 |
    348.4 | 118.1 | 180.4 |'
- en: 5.6 Comparisons
  id: totrans-721
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 比较
- en: 'We summarize different supervisory signals for crowd counting with their representative
    schemes in Table [8](#S5.T8 "Table 8 ‣ 5.3 Unsupervised and Self-supervised Learning
    ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal"). We compare them in Table [9](#S5.T9
    "Table 9 ‣ 5.5 Domain Adaptive Crowd Counting ‣ 5 Supervisory Signal ‣ A Survey
    on Deep Learning-based Single Image Crowd Counting: Network Design, Loss Function
    and Supervisory Signal").'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在表格 [8](#S5.T8 "Table 8 ‣ 5.3 Unsupervised and Self-supervised Learning ‣
    5 Supervisory Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal") 中总结了用于人群计数的不同监督信号及其代表性方案。我们在表格
    [9](#S5.T9 "Table 9 ‣ 5.5 Domain Adaptive Crowd Counting ‣ 5 Supervisory Signal
    ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network Design,
    Loss Function and Supervisory Signal") 中对它们进行了比较。'
- en: BL achieves better performance on four different crowd counting datasets compared
    with CP-CNN, with a similar number of parameters for the backbone. The good performance
    of BL may be due to the Bayesian loss used to better model the non-rigid objects
    (e.g., people). The adaptive Gaussian kernel is widely used in crowd counting
    approaches, while the experimental results demonstrate the effectiveness of Bayesian
    loss, which is more reliable supervision.
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: BL 在四个不同的人群计数数据集上相较于 CP-CNN 实现了更好的性能，且骨干网络参数数量相似。BL 的良好性能可能归因于用于更好地建模非刚性物体（例如，人）的贝叶斯损失。自适应高斯核在人群计数方法中被广泛使用，而实验结果证明了贝叶斯损失的有效性，这是一种更可靠的监督方式。
- en: 'CCWld shows much better accuracy than MCNN in Table [9](#S5.T9 "Table 9 ‣ 5.5
    Domain Adaptive Crowd Counting ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based
    Single Image Crowd Counting: Network Design, Loss Function and Supervisory Signal")
    on various datasets with different backgrounds. We observe CCWld enhances the
    performance of counting accuracy and also improves the robustness, which is suitable
    for many real-world applications with diverse scenes, different view angles, and
    lighting conditions.'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 'CCWld 在各种背景的不同数据集上表现出比 MCNN 更好的准确性，如表格 [9](#S5.T9 "Table 9 ‣ 5.5 Domain Adaptive
    Crowd Counting ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based Single
    Image Crowd Counting: Network Design, Loss Function and Supervisory Signal") 所示。我们观察到，CCWld
    提高了计数准确性，并且增强了鲁棒性，这使其适用于许多具有多样场景、不同视角和光照条件的实际应用。'
- en: 'As shown in Table [9](#S5.T9 "Table 9 ‣ 5.5 Domain Adaptive Crowd Counting
    ‣ 5 Supervisory Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting:
    Network Design, Loss Function and Supervisory Signal"), the performance of HA-CNN
    is much better than other state-of-the-arts. After carefully designing the deep
    neural networks and loss functions, weakly supervised crowd counting achieves
    much better accuracy with relatively low annotation complexity.'
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: '如表格 [9](#S5.T9 "Table 9 ‣ 5.5 Domain Adaptive Crowd Counting ‣ 5 Supervisory
    Signal ‣ A Survey on Deep Learning-based Single Image Crowd Counting: Network
    Design, Loss Function and Supervisory Signal") 所示，HA-CNN 的性能远远优于其他最先进的方法。在精心设计深度神经网络和损失函数后，弱监督的人群计数在相对低的标注复杂度下实现了更好的准确性。'
- en: The MAE and MSE of L2R (query by example) and L2R (query by keyword) is lower
    than CP-CNN. This confirms that leveraging the abundantly available unlabeled
    data improves counting performance. The experimental results further demonstrate
    that making use of unlabeled data is a promising direction for crowd counting.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: L2R（按示例查询）和 L2R（按关键词查询）的 MAE 和 MSE 都低于 CP-CNN。这证实了利用大量未标记数据可以提高计数性能。实验结果进一步表明，利用未标记数据是人群计数的一个有前途的方向。
- en: 5.7 Others
  id: totrans-727
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 其他
- en: There are some other learning paradigms for crowd counting.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些其他的人群计数学习范式。
- en: There is a typical training paradigm that is count from scalar representation.
    Some recent works achieve excellent results compared with density map regression
    method or learning from point map representation. TransCrowd [[82](#bib.bib82)]
    proposes to formulate crowd counting as a sequence-to-count paradigm based on
    transformers and achieves satisfactory performance. CrowdMLP [[182](#bib.bib182)]
    presents a multi-granularity MLP regressor for capturing global information and
    enchance crowd counting quality.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种典型的训练范式是从标量表示进行计数。与密度图回归方法或从点图表示学习相比，一些近期工作取得了优秀的结果。TransCrowd [[82](#bib.bib82)]
    提出了将人群计数公式化为基于变换器的序列计数范式，并取得了令人满意的性能。CrowdMLP [[182](#bib.bib182)] 提出了一个多粒度的 MLP
    回归器，用于捕捉全局信息并提高人群计数质量。
- en: Recent research shows that the crowd localization can enhance the counting performance.
    FIDT [[83](#bib.bib83)] introduces a focal inverse distance transform map for
    crowd counting and crowd localization, which simultaneously conduct counting and
    crowd localization based on the FIDT map. IIM [[39](#bib.bib39)] presents an independent
    instance map segmentation for crowd localization by segmenting people crowds into
    non-overlapped independent components.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的研究表明，人群定位可以提高计数性能。FIDT [[83](#bib.bib83)] 引入了一种焦点逆距离变换图用于人群计数和人群定位，该方法基于
    FIDT 图同时进行计数和人群定位。IIM [[39](#bib.bib39)] 提出了通过将人群分割成不重叠的独立组件来进行人群定位的独立实例图分割方法。
- en: There is another series of counting works that achieve crowd counting from remote
    sensing data. The work [[237](#bib.bib237)] introduces a crowd counting benchmark
    from remote sensing perspective. The work [[38](#bib.bib38)] proposes a large-scale
    dense objects counting dataset based on remote sensing images. The work [[230](#bib.bib230)]
    proposes a flow-based Bi-path Network for remote sensing video sequences. IS-Count [[121](#bib.bib121)]
    presents a convariate-based importance sampling method for counting from remote
    sensing images. Compared with counting from normal perspective, the remote sensing
    images suffers more from small object recognition issues in designing the counting
    networks but the problem of scale variation for counting from normal perspective
    is more serious.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 另一系列工作实现了基于遥感数据的人群计数。工作 [[237](#bib.bib237)] 从遥感角度介绍了人群计数基准。工作 [[38](#bib.bib38)]
    提出了一个基于遥感图像的大规模密集物体计数数据集。工作 [[230](#bib.bib230)] 提出了一个流式双路径网络用于遥感视频序列。IS-Count [[121](#bib.bib121)]
    提出了一个基于协方差的重要性采样方法用于从遥感图像中计数。与常规视角计数相比，遥感图像在设计计数网络时更容易遇到小物体识别问题，但常规视角计数中的尺度变化问题更加严重。
- en: 6 Conclusion and Future Directions
  id: totrans-732
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论与未来方向
- en: 'Crowd counting is an important and challenging problem in computer vision.
    This survey paper covers the design considerations and recent advances with respect
    to single image crowd counting problem, and summarizes more than 200 crowd counting
    schemes using deep learning approaches proposed since 2015. We have discussed
    the major datasets, performance metrics, design considerations, techniques, and
    representative schemes to tackle the problem. We provide a comprehensive overview
    and comparison of three major design modules for deep learning in crowd counting,
    deep neural network design, loss function, and supervisory signal. The research
    field of crowd counting is rich and still evolving. We discuss some future trends
    and possible research directions below:'
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 人群计数是计算机视觉中的一个重要且具有挑战性的问题。这篇综述文章涵盖了单图像人群计数问题的设计考虑和近期进展，并总结了自 2015 年以来提出的 200
    多种基于深度学习的人群计数方案。我们讨论了主要的数据集、性能指标、设计考虑、技术和代表性方案来解决这一问题。我们提供了对人群计数中深度学习的三大设计模块——深度神经网络设计、损失函数和监督信号——的全面概述和比较。人群计数的研究领域丰富且仍在发展中。我们在下文中讨论了一些未来趋势和可能的研究方向：
- en: •
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Automatic and lightweight network designing has drawn much attention in recent
    years [[91](#bib.bib91), [152](#bib.bib152), [183](#bib.bib183), [200](#bib.bib200)].
    Currently, designing CNN-based crowd counting models still requires a manual network
    and feature selection with strong domain knowledge. Automated Machine Learning
    has been applied to image classification and object detection, which has the potential
    to automatically design efficient crowd counting architectures. Besides, CNN-based
    crowd counting models have increased in-depth with millions of parameters, which
    requires massive computation. Thus, there is also a need for model compression
    and acceleration techniques to deploy lightweight model.
  id: totrans-735
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动化和轻量化的网络设计近年来引起了广泛关注 [[91](#bib.bib91), [152](#bib.bib152), [183](#bib.bib183),
    [200](#bib.bib200)]。目前，基于 CNN 的人群计数模型设计仍需要手动进行网络和特征选择，并需要强大的领域知识。自动化机器学习已被应用于图像分类和目标检测，这具有自动设计高效人群计数架构的潜力。此外，基于
    CNN 的人群计数模型参数量增加到了数百万，这需要大量的计算。因此，还需要模型压缩和加速技术来部署轻量化模型。
- en: •
  id: totrans-736
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Weakly supervised and unsupervised crowd counting is able to reduce the labeling
    effort. With the performance saturation for some supervised learning scenarios,
    researchers devote efforts to make use of unlabeled and weakly labeled images
    for crowd counting Most of the state-of-the-art algorithms are based on fully
    supervised learning and trained with point-wise annotations, which has several
    limitations such as labor-intensive labeling process, easily over-fitting, and
    not salable in the absence of densely labeled crowd images. Weakly-supervised
    and unsupervised learning has attracted much attention in vision applications,
    which has value for crowd counting tasks to reduce labeling effort, enhance counting
    accuracy and improve robustness.
  id: totrans-737
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 弱监督和无监督的人群计数能够减少标注工作量。随着一些监督学习场景的性能饱和，研究人员致力于利用未标注和弱标注的图像进行人群计数。大多数最先进的算法基于完全监督学习，并用点状标注进行训练，这存在一些局限性，例如劳动密集的标注过程、容易过拟合，并且在缺乏密集标注的人群图像的情况下不可扩展。弱监督和无监督学习在视觉应用中引起了很多关注，这对减少标注工作量、提高计数准确性和增强鲁棒性的人群计数任务具有价值。
- en: •
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Crowd counting in videos is becoming an active research direction. A straightforward
    approach is to consider the video frames independently by making use of the crowd
    counting techniques proposed for still images. This is not satisfactory because
    it ignores the continuity or temporal correlation between frames, i.e., the motion
    information. Bidirectional ConvLSTM [[204](#bib.bib204)] is a recent attempt to
    leverage spatial-temporal information in video. There are some recent attempts
    to exploit the correlation in video data  [[242](#bib.bib242), [36](#bib.bib36),
    [47](#bib.bib47), [100](#bib.bib100), [140](#bib.bib140), [112](#bib.bib112),
    [101](#bib.bib101), [43](#bib.bib43)]. However, LSTM-based framework is not easy
    to train or to be extended to a general scenario. The 3D kernel is not effective
    in extracting the long-range contextual information. Effectively making use of
    the temporal correlation for accurate and efficient near real-time crowd counting
    systems is also a potential research direction.
  id: totrans-739
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 视频中的人群计数正在成为一个活跃的研究方向。一种直接的方法是将视频帧独立考虑，通过利用为静态图像提出的人群计数技术。这种方法并不令人满意，因为它忽略了帧之间的连续性或时间相关性，即运动信息。双向
    ConvLSTM [[204](#bib.bib204)] 是最近尝试利用视频中的时空信息的方法之一。最近也有一些尝试利用视频数据中的相关性 [[242](#bib.bib242),
    [36](#bib.bib36), [47](#bib.bib47), [100](#bib.bib100), [140](#bib.bib140), [112](#bib.bib112),
    [101](#bib.bib101), [43](#bib.bib43)]。然而，基于 LSTM 的框架不易训练或扩展到通用场景。3D 核在提取长范围上下文信息方面效果不佳。有效利用时间相关性以实现准确且高效的近实时人群计数系统也是一个潜在的研究方向。
- en: •
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Multi-view fusion for crowd counting is important as a single camera cannot
    capture large and wide areas (e.g., parks, public squares). Multiple cameras with
    overlapping view are required to solve the wide-area counting task. There are
    some recent multi-view fusion approaches for crowd counting [[224](#bib.bib224)],
    which proposes a multi-camera fusion method to predict a ground-plane density
    map of the 3D world. There is also another approach based on a 2D-to-3D projection
    with 3D density map estimation and a 3D-to-2D projection consistency measure method [[225](#bib.bib225)].
    Multi-view fusion for crowd counting provides a vivid visualization for the scenes,
    as well as the potentials for other applications like observing the scene in arbitrary
    view angles, which may contribute to better scene understanding. Therefore, crowd
    counting with multi-view fusion represents important research value.
  id: totrans-741
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多视角融合在人群计数中非常重要，因为单台摄像机无法捕捉到大而宽的区域（例如，公园、公共广场）。需要多个视角重叠的摄像机来解决广域计数任务。最近有一些多视角融合方法用于人群计数
    [[224](#bib.bib224)]，其中提出了一种多摄像机融合方法来预测 3D 世界的地面密度图。还有另一种基于 2D 到 3D 投影的方法，通过 3D
    密度图估计和 3D 到 2D 投影一致性度量方法 [[225](#bib.bib225)]。多视角融合提供了生动的场景可视化，以及其他应用的潜力，例如观察任意视角的场景，这可能有助于更好的场景理解。因此，基于多视角融合的人群计数具有重要的研究价值。
- en: 'Table 10: A comprehensive performance analysis of various categories of crowd
    counting methods across different datasets. Red denotes the best performance and
    blue denotes the third best performance. ST PartA is the ShanghaiTech A dataset [[228](#bib.bib228)].
    The evaluation metrics for the counting performance is MAE and MSE.'
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
- en: '| Typical Schemes | ST PartA | UCF_CC_50 | UCF-QNRF | NWPU |'
  id: totrans-743
  prefs: []
  type: TYPE_TB
- en: '|'
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
- en: '&#124; Methods &#124;'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Column | MAE | MSE | MAE | MSE | MAE | MSE | MAE | MSE |'
  id: totrans-746
  prefs: []
  type: TYPE_TB
- en: '| CSRNet [[80](#bib.bib80)] | 2018 | Single | 68.2 | 115.0 | 266.1 | 397.5
    | - | - | 104.8 | 433.4 |'
  id: totrans-747
  prefs: []
  type: TYPE_TB
- en: '| SaCNN [[222](#bib.bib222)] | 2018 | Single | 86.8 | 139.2 | 314.9 | 424.8
    | - | - | - | - |'
  id: totrans-748
  prefs: []
  type: TYPE_TB
- en: '| DADNet [[46](#bib.bib46)] | 2019 | Single | 64.2 | 99.9 | 285.5 | 389.7 |
    113.2 | 189.4 | - | - |'
  id: totrans-749
  prefs: []
  type: TYPE_TB
- en: '| MRNet [[165](#bib.bib165)] | 2019 | Single | 63.3 | 97.8 | 232.3 | 314.8
    | 111.1 | 182.8 | - | - |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
- en: '| ADCNet [[97](#bib.bib97)] | 2019 | Single | 70.9 | 115.2 | 273.6 | 362.0
    | - | - | - | - |'
  id: totrans-751
  prefs: []
  type: TYPE_TB
- en: '| HA-CNN [[159](#bib.bib159)] | 2019 | Single | 62.9 | 94.9 | 256.2 | 348.4
    | 118.1 | 180.4 | - | - |'
  id: totrans-752
  prefs: []
  type: TYPE_TB
- en: '| PGCNet [[210](#bib.bib210)] | 2019 | Single | 57.0 | 86.0 | 244.6 | 361.2
    | - | - | - | - |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
- en: '| SDANet [[123](#bib.bib123)] | 2020 | Single | 63.6 | 101.8 | 227.6 | 316.4
    | - | - | - | - |'
  id: totrans-754
  prefs: []
  type: TYPE_TB
- en: '| CTN [[137](#bib.bib137)] | 2020 | Single | 61.5 | 103.4 | 210.0 | 305.4 |
    86.0 | 146.0 | 78.0 | 448.0 |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
- en: '| DM-Count [[175](#bib.bib175)] | 2020 | Single | 59.7 | 95.7 | 211.0 | 291.5
    | 85.6 | 148.3 | 70.5 | 357.6 |'
  id: totrans-756
  prefs: []
  type: TYPE_TB
- en: '| NAS-Count [[55](#bib.bib55)] | 2020 | Single | 56.7 | 93.4 | 208.4 | 297.3
    | 101.8 | 163.2 | - | - |'
  id: totrans-757
  prefs: []
  type: TYPE_TB
- en: '| SRF-Net [[20](#bib.bib20)] | 2020 | Single | 60.4 | 97.2 | 197.3 | 271.8
    | 98.0 | 170.0 | - | - |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
- en: '| ADSCNet [[8](#bib.bib8)] | 2020 | Single | 55.4 | 97.7 | 198.4 | 267.3 |
    71.3 | 132.5 | - | - |'
  id: totrans-759
  prefs: []
  type: TYPE_TB
- en: '| UEPNet [[176](#bib.bib176)] | 2021 | Single | 54.6 | 91.2 | 165.2 | 275.9
    | 81.1 | 131.7 | - | - |'
  id: totrans-760
  prefs: []
  type: TYPE_TB
- en: '| S3 [[84](#bib.bib84)] | 2021 | Single | 57.0 | 96.0 | - | - | 80.6 | 139.8
    | 83.5 | 346.9 |'
  id: totrans-761
  prefs: []
  type: TYPE_TB
- en: '| NDConv [[218](#bib.bib218)] | 2022 | Single | 61.4 | 104.2 | 167.2 | 240.6
    | 95.9 | 182.4 | - | - |'
  id: totrans-762
  prefs: []
  type: TYPE_TB
- en: '| TransCrowd [[82](#bib.bib82)] | 2022 | Single | 66.1 | 105.1 | 272.2 | 395.3
    | 97.2 | 168.5 | 88.4 | 400.5 |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
- en: '| MAN [[85](#bib.bib85)] | 2022 | Single | 56.8 | 90.3 | - | - | 77.3 | 131.5
    | 76.5 | 323.0 |'
  id: totrans-764
  prefs: []
  type: TYPE_TB
- en: '| CMTL [[156](#bib.bib156)] | 2017 | Double | 101.3 | 152.4 | 322.8 | 397.9
    | - | - | - | - |'
  id: totrans-765
  prefs: []
  type: TYPE_TB
- en: '| ACSCP [[151](#bib.bib151)] | 2018 | Double | 75.7 | 102.7 | 291.0 | 404.6
    | - | - | - | - |'
  id: totrans-766
  prefs: []
  type: TYPE_TB
- en: '| SDNet [[113](#bib.bib113)] | 2021 | Double | 55.0 | 92.7 | 197.5 | 264.1
    | 80.7 | 146.3 | - | - |'
  id: totrans-767
  prefs: []
  type: TYPE_TB
- en: '| BM-Count [[89](#bib.bib89)] | 2021 | Double | 57.3 | 90.7 | - | - | 81.2
    | 138.6 | 83.4 | 358.4 |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
- en: '| BSCC [[125](#bib.bib125)] | 2021 | Double | 58.3 | 100.1 | - | - | 86.3 |
    153.1 | - | - |'
  id: totrans-769
  prefs: []
  type: TYPE_TB
- en: '| P2PNet [[163](#bib.bib163)] | 2021 | Double | 52.7 | 85.1 | 172.7 | 256.2
    | 85.3 | 154.5 | 77.4 | 362.0 |'
  id: totrans-770
  prefs: []
  type: TYPE_TB
- en: '| GauNet [[23](#bib.bib23)] | 2022 | Double | 54.8 | 89.1 | 186.3 | 256.5 |
    81.6 | 153.7 | - | - |'
  id: totrans-771
  prefs: []
  type: TYPE_TB
- en: '| RAN [[21](#bib.bib21)] | 2022 | Double | 57.9 | 99.2 | 155.0 | 219.5 | 83.4
    | 141.8 | 65.3 | 432.9 |'
  id: totrans-772
  prefs: []
  type: TYPE_TB
- en: '| MCNN [[228](#bib.bib228)] | 2016 | Multi | 110.2 | 173.2 | 377.6 | 509.1
    | 277 | 426 | 218.5 | 700.6 |'
  id: totrans-773
  prefs: []
  type: TYPE_TB
- en: '| CP-CNN [[157](#bib.bib157)] | 2017 | Multi | 73.6 | 106.4 | 295.8 | 320.9
    | - | - | - | - |'
  id: totrans-774
  prefs: []
  type: TYPE_TB
- en: '| Switching [[6](#bib.bib6)] | 2017 | Multi | 90.4 | 135.0 | 318.1 | 439.2
    | - | - | - | - |'
  id: totrans-775
  prefs: []
  type: TYPE_TB
- en: '| SANet [[11](#bib.bib11)] | 2018 | Multi | 67.0 | 104.5 | 258.4 | 334.9 |
    - | - | - | - |'
  id: totrans-776
  prefs: []
  type: TYPE_TB
- en: '| DSSINet [[95](#bib.bib95)] | 2019 | Multi | 60.6 | 96.0 | 216.9 | 302.4 |
    99.1 | 159.2 | - | - |'
  id: totrans-777
  prefs: []
  type: TYPE_TB
- en: '| CFF [[153](#bib.bib153)] | 2019 | Multi | 65.2 | 109.4 | - | - | 93.8 | 146.5
    | - | - |'
  id: totrans-778
  prefs: []
  type: TYPE_TB
- en: '| S-DCNet [[205](#bib.bib205)] | 2019 | Multi | 58.3 | 95.0 | 204.2 | 301.3
    | 104.4 | 176.1 | - | - |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
- en: '| CAN [[99](#bib.bib99)] | 2019 | Multi | 62.3 | 100.0 | 212.2 | 243.7 | 107.0
    | 183.0 | - | - |'
  id: totrans-780
  prefs: []
  type: TYPE_TB
- en: '| SPANet [[24](#bib.bib24)] | 2019 | Multi | 59.4 | 92.5 | 232.6 | 311.7 |
    - | - | - | - |'
  id: totrans-781
  prefs: []
  type: TYPE_TB
- en: '| DPN [[115](#bib.bib115)] | 2020 | Multi | 58.1 | 91.7 | 183.2 | 284.5 | 84.7
    | 147.2 | - | - |'
  id: totrans-782
  prefs: []
  type: TYPE_TB
- en: '| AMRNet [[104](#bib.bib104)] | 2020 | Multi | 61.6 | 98.4 | 184.0 | 265.8
    | 86.6 | 152.2 | - | - |'
  id: totrans-783
  prefs: []
  type: TYPE_TB
- en: '| ASNet [[64](#bib.bib64)] | 2020 | Multi | 57.8 | 90.1 | 174.8 | 251.6 | 91.6
    | 159.7 | - | - |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
- en: '| DeepCount [[22](#bib.bib22)] | 2020 | Multi | 65.2 | 112.5 | - | - | 95.7
    | 167.1 | - | - |'
  id: totrans-785
  prefs: []
  type: TYPE_TB
- en: '| ikNN [[129](#bib.bib129)] | 2020 | Multi | 68.0 | 117.7 | 237.8 | 305.7 |
    104.0 | 172.0 | - | - |'
  id: totrans-786
  prefs: []
  type: TYPE_TB
- en: '| M-SFANet [[168](#bib.bib168)] | 2020 | Multi | 57.6 | 94.5 | 167.5 | 256.3
    | 87.6 | 147.8 | - | - |'
  id: totrans-787
  prefs: []
  type: TYPE_TB
- en: '| EPA [[215](#bib.bib215)] | 2021 | Multi | 60.9 | 91.6 | 250.1 | 352.1 | -
    | - | - | - |'
  id: totrans-788
  prefs: []
  type: TYPE_TB
- en: '| DKPNet [[15](#bib.bib15)] | 2021 | Multi | 55.6 | 91.0 | - | - | 81.4 | 147.2
    | 61.8 | 438.7 |'
  id: totrans-789
  prefs: []
  type: TYPE_TB
- en: '| SASNet [[164](#bib.bib164)] | 2021 | Multi | 53.6 | 88.4 | 161.4 | 234.5
    | 85.2 | 147.3 | - | - |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
- en: '| MFDC [[102](#bib.bib102)] | 2021 | Multi | 55.4 | 91.3 | - | - | 76.2 | 121.5
    | 74.7 | 267.9 |'
  id: totrans-791
  prefs: []
  type: TYPE_TB
- en: '| MPS [[218](#bib.bib218)] | 2022 | Multi | 71.4 | 110.7 | - | - | - | - |
    - | - |'
  id: totrans-792
  prefs: []
  type: TYPE_TB
- en: '| MNA [[171](#bib.bib171)] | 2020 | N/A | 61.9 | 99.6 | - | - | 85.8 | 150.6
    | 96.9 | 534.2 |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
- en: '| BL [[114](#bib.bib114)] | 2019 | N/A | 62.8 | 101.8 | 229.3 | 308.2 | 88.7
    | 154.8 | - | - |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
- en: '| UOT [[116](#bib.bib116)] | 2021 | N/A | 58.1 | 95.9 | - | - | 83.3 | 142.3
    | 87.8 | 387.5 |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
- en: '| BinLoss [[155](#bib.bib155)] | 2021 | N/A | 61.3 | 88.7 | - | - | 85.9 |
    120.6 | 71.7 | 376.4 |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
- en: References
  id: totrans-797
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Abousamra, S., Hoai, M., Samaras, D., Chen, C.: Localization in the crowd
    with topological constraints. In: AAAI (2021)'
  id: totrans-798
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Ahuja, K.R., Charniya, N.N.: A survey of recent advances in crowd density
    estimation using image processing. In: ICCES (2019)'
  id: totrans-799
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Amirgholipour, S., He, X., Jia, W., Wang, D., Liu, L.: Pdanet: Pyramid
    density-aware attention net for accurate crowd counting. NeuroComputing (2020)'
  id: totrans-800
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Arteta, C., Lempitsky, V., Zisserman, A.: Counting in the wild. In: ECCV
    (2016)'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Aydın, S.: Deep learning classification of neuro-emotional phase domain
    complexity levels induced by affective video film clips. IEEE Journal of Biomedical
    and Health Informatics (2019)'
  id: totrans-802
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 艾丁：通过情感视频片段引发的神经情感相位域复杂性水平的深度学习分类。IEEE生物医学与健康信息学杂志（2019）'
- en: '[6] Babu Sam, D., Surya, S., Venkatesh Babu, R.: Switching convolutional neural
    network for crowd counting. In: CVPR (2017)'
  id: totrans-803
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 巴布·萨姆，苏利亚，文凯斯·巴布：用于人群计数的切换卷积神经网络。在：CVPR（2017）'
- en: '[7] Bai, H., Wen, S., Gary Chan, S.H.: Crowd counting on images with scale
    variation and isolated clusters. In: ICCV Workshops (2019)'
  id: totrans-804
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 白辉，温曦，Gary Chan：在尺度变化和孤立簇的图像上进行人群计数。在：ICCV研讨会（2019）'
- en: '[8] Bai, S., He, Z., Qiao, Y., Hu, H., Wu, W., Yan, J.: Adaptive dilated network
    with self-correction supervision for counting. In: CVPR (2020)'
  id: totrans-805
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 白胜，何志，乔阳，胡辉，吴文，颜杰：具有自我纠正监督的自适应膨胀网络进行计数。在：CVPR（2020）'
- en: '[9] von Borstel, M., Kandemir, M., Schmidt, P., Rao, M.K., Rajamani, K., Hamprecht,
    F.A.: Gaussian process density counting from weak supervision. In: ECCV (2016)'
  id: totrans-806
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 冯博斯特尔，坎德米尔，施密特，拉奥，拉贾马尼，汉普雷希特：从弱监督中进行高斯过程密度计数。在：ECCV（2016）'
- en: '[10] Cai, Y., Chen, L., Ma, Z., Lu, C., Wang, C., He, G.: Leveraging intra-domain
    knowledge to strengthen cross-domain crowd counting. In: ICME (2021)'
  id: totrans-807
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 蔡义，陈亮，马志，卢冲，王超，贺国：利用领域内知识增强跨领域人群计数。在：ICME（2021）'
- en: '[11] Cao, X., Wang, Z., Zhao, Y., Su, F.: Scale aggregation network for accurate
    and efficient crowd counting. In: ECCV (2018)'
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 曹旭，王志，赵怡，苏芳：用于精确和高效人群计数的尺度聚合网络。在：ECCV（2018）'
- en: '[12] Chan, A.B., Liang, Z.S.J., Vasconcelos, N.: Privacy preserving crowd monitoring:
    Counting people without people models or tracking. In: CVPR (2008)'
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 陈安博，梁志杰，瓦斯孔塞洛斯：隐私保护的人群监测：在没有人群模型或跟踪的情况下计数人群。在：CVPR（2008）'
- en: '[13] Chan, A.B., Vasconcelos, N.: Bayesian poisson regression for crowd counting.
    In: ICCV (2009)'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 陈安博，瓦斯孔塞洛斯：用于人群计数的贝叶斯泊松回归。在：ICCV（2009）'
- en: '[14] Chan, A.B., Vasconcelos, N.: Counting people with low-level features and
    bayesian regression. TIP (2012)'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 陈安博，瓦斯孔塞洛斯：利用低级特征和贝叶斯回归计数人群。TIP（2012）'
- en: '[15] Chen, B., Yan, Z., Li, K., Li, P., Wang, B., Zuo, W., Zhang, L.: Variational
    attention: Propagating domain-specific knowledge for multi-domain learning in
    crowd counting. In: ICCV (2021)'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 陈斌，闫振，李凯，李鹏，王博，左伟，张琳：变分注意力：传播领域特定知识以进行人群计数的多领域学习。在：ICCV（2021）'
- en: '[16] Chen, J., Su, W., Wang, Z.: Crowd counting with crowd attention convolutional
    neural network. Neurocomputing (2020)'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 陈杰，苏伟，王志：带有人群注意卷积神经网络的人群计数。神经计算（2020）'
- en: '[17] Chen, K., Loy, C.C., Gong, S., Xiang, T.: Feature mining for localised
    crowd counting. In: BMVC (2012)'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 陈凯，赖成城，龚森，项腾：局部人群计数的特征挖掘。在：BMVC（2012）'
- en: '[18] Chen, X., Bin, Y., Gao, C., Sang, N., Tang, H.: Relevant region prediction
    for crowd counting. Neurocomputing (2020)'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 陈晓宇，边艳，高超，桑宁，唐浩：人群计数的相关区域预测。神经计算（2020）'
- en: '[19] Chen, X., Bin, Y., Sang, N., Gao, C.: Scale pyramid network for crowd
    counting. In: WACV (2019)'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 陈晓宇，边艳，桑宁，高超：用于人群计数的尺度金字塔网络。在：WACV（2019）'
- en: '[20] Chen, Y., Gao, C., Su, Z., He, X., Liu, N.: Scale-aware rolling fusion
    network for crowd counting. In: ICME (2020)'
  id: totrans-817
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 陈彦，高超，苏志，贺晓，刘宁：尺度感知滚动融合网络用于人群计数。在：ICME（2020）'
- en: '[21] Chen, Y., Yang, J., Zhang, D., Zhang, K., Chen, B., Du, S.: Region-aware
    network: Model human’s top-down visual perception mechanism for crowd counting.
    Neural Networks (2022)'
  id: totrans-818
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 陈彦，杨军，张丹，张凯，陈斌，杜胜：区域感知网络：模拟人类的自上而下视觉感知机制进行人群计数。神经网络（2022）'
- en: '[22] Chen, Z., Cheng, J., Yuan, Y., Liao, D., Li, Y., Lv, J.: Deep density-aware
    count regressor. ECAI (2019)'
  id: totrans-819
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 陈哲，程静，袁岩，廖东，李洋，吕杰：深度密度感知计数回归器。ECAI（2019）'
- en: '[23] Cheng, Z.Q., Dai, Q., Li, H., Song, J., Wu, X., Hauptmann, A.G.: Rethinking
    spatial invariance of convolutional networks for object counting. In: CVPR (2022)'
  id: totrans-820
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 程志强，戴庆，李辉，宋佳，吴旭，霍普特曼：重新思考卷积网络的空间不变性用于物体计数。在：CVPR（2022）'
- en: '[24] Cheng, Z.Q., Li, J.X., Dai, Q., Wu, X., Hauptmann, A.G.: Learning spatial
    awareness to improve crowd counting. In: ICCV (2019)'
  id: totrans-821
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 程志强，李俊熙，戴庆，吴旭，霍普特曼：学习空间感知以改善人群计数。在：ICCV（2019）'
- en: '[25] Cheng, Z.Q., Li, J.X., Dai, Q., Wu, X., He, J.Y., Hauptmann, A.G.: Improving
    the learning of multi-column convolutional neural network for crowd counting.
    In: ACM Multimedia (2019)'
  id: totrans-822
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 程志强，李俊熙，戴庆，吴旭，贺金岩，霍普特曼：改善多列卷积神经网络的人群计数学习。在：ACM Multimedia（2019）'
- en: '[26] Chrysler, A., Gunarso, R., Puteri, T., Warnars, H.: A literature review
    of crowd-counting system on convolutional neural network. In: IOP Conference Series:
    Earth and Environmental Science (2021)'
  id: totrans-823
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Dai, F., Liu, H., Ma, Y., Zhang, X., Zhao, Q.: Dense scale network for
    crowd counting. In: International Conference on Multimedia Retrieval (2021)'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Deb, D., Ventura, J.: An aggregated multicolumn dilated convolution network
    for perspective-free counting. In: CVPR Workshops (2018)'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Ding, X., He, F., Lin, Z., Wang, Y., Guo, H., Huang, Y.: Crowd density
    estimation using fusion of multi-layer features. IEEE Transactions on Intelligent
    Transportation Systems (2020)'
  id: totrans-826
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Ding, X., Lin, Z., He, F., Wang, Y., Huang, Y.: A deeply-recursive convolutional
    network for crowd counting. In: ICASSP (2018)'
  id: totrans-827
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Dong, L., Zhang, H., Ji, Y., Ding, Y.: Crowd counting by using multi-level
    density-based spatial information: A multi-scale cnn framework. Information Sciences
    (2020)'
  id: totrans-828
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Dong, Z., Zhang, R., Shao, X., Li, Y.: Scale-recursive network with point
    supervision for crowd scene analysis. Neurocomputing (2020)'
  id: totrans-829
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Draghici, A., Steen, M.V.: A survey of techniques for automatically sensing
    the behavior of a crowd. ACM Computing Surveys (2018)'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Du, D., Wen, L., Zhu, P., Fan, H., Hu, Q., Ling, H., Shah, M., Pan, J.,
    Al-Ali, A., Mohamed, A., et al.: Visdrone-cc2020: The vision meets drone crowd
    counting challenge results. In: ECCV (2020)'
  id: totrans-831
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Duan, H., Wang, S., Guan, Y.: Sofa-net: Second-order and first-order attention
    network for crowd counting. BMVC (2020)'
  id: totrans-832
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Fang, Y., Gao, S., Li, J., Luo, W., He, L., Hu, B.: Multi-level feature
    fusion based locality-constrained spatial transformer network for video crowd
    counting. Neurocomputing (2020)'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Fang, Y., Zhan, B., Cai, W., Gao, S., Hu, B.: Locality-constrained spatial
    transformer network for video crowd counting. In: ICME (2019)'
  id: totrans-834
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Gao, G., Liu, Q., Wang, Y.: Counting dense objects in remote sensing images.
    In: ICASSP (2020)'
  id: totrans-835
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Gao, J., Han, T., Yuan, Y., Wang, Q.: Learning independent instance maps
    for crowd localization. arXiv preprint arXiv:2012.04164 (2020)'
  id: totrans-836
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Gao, J., Han, T., Yuan, Y., Wang, Q.: Domain-adaptive crowd counting via
    high-quality image translation and density reconstruction. TNNLS (2021)'
  id: totrans-837
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Gao, J., Wang, Q., Li, X.: Pcc net: Perspective crowd counting via spatial
    convolutional network. TCSVT (2019)'
  id: totrans-838
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Gao, J., Wang, Q., Yuan, Y.: Scar: Spatial-/channel-wise attention regression
    networks for crowd counting. Neurocomputing (2019)'
  id: totrans-839
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Gao, J., Yuan, Y., Wang, Q.: Feature-aware adaptation and density alignment
    for crowd counting in video surveillance. IEEE transactions on cybernetics (2020)'
  id: totrans-840
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Gong, S., Zhang, S., Yang, J., Dai, D., Schiele, B.: Bi-level alignment
    for cross-domain crowd counting. In: CVPR (2022)'
  id: totrans-841
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Guerrero-Gómez-Olmedo, R., Torre-Jiménez, B., López-Sastre, R., Maldonado-Bascón,
    S., Onoro-Rubio, D.: Extremely overlapping vehicle counting. In: Iberian Conference
    on Pattern Recognition and Image Analysis (2015)'
  id: totrans-842
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Guo, D., Li, K., Zha, Z.J., Wang, M.: Dadnet: Dilated-attention-deformable
    convnet for crowd counting. In: ACM Multimedia (2019)'
  id: totrans-843
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Han, T., Bai, L., Gao, J., Wang, Q., Ouyang, W.: Dr. vic: Decomposition
    and reasoning for video individual counting. In: CVPR (2022)'
  id: totrans-844
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Han, T., Gao, J., Yuan, Y., Wang, Q.: Focus on semantic consistency for
    cross-domain crowd understanding. In: ICASSP (2020)'
  id: totrans-845
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] He, G., Ma, Z., Huang, B., Sheng, B., Yuan, Y.: Dynamic region division
    for adaptive learning pedestrian counting. In: ICME (2019)'
  id: totrans-846
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] He, K., Zhang, X., Ren, S., Sun, J.: Deep residual learning for image
    recognition. In: CVPR (2016)'
  id: totrans-847
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] He, Y., Ma, Z., Wei, X., Hong, X., Ke, W., Gong, Y.: Error-aware density
    isomorphism reconstruction for unsupervised cross-domain crowd counting. In: AAAI
    (2021)'
  id: totrans-848
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Hossain, M., Hosseinzadeh, M., Chanda, O., Wang, Y.: Crowd counting using
    scale-aware attention networks. In: WACV (2019)'
  id: totrans-849
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Hou, Y., Li, C., Lu, Y., Zhu, L., Li, Y., Jia, H., Xie, X.: Enhancing
    and dissecting crowd counting by synthetic data. In: ICASSP (2022)'
  id: totrans-850
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Hou, Y., Li, C., Yang, F., Ma, C., Zhu, L., Li, Y., Jia, H., Xie, X.:
    Bba-net: A bi-branch attention network for crowd counting. In: ICASSP (2020)'
  id: totrans-851
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Hu, Y., Jiang, X., Liu, X., Zhang, B., Han, J., Cao, X., Doermann, D.:
    Nas-count: Counting-by-density with neural architecture search. In: ECCV (2020)'
  id: totrans-852
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Huang, S., Li, X., Cheng, Z.Q., Zhang, Z., Hauptmann, A.: Stacked pooling
    for boosting scale invariance of crowd counting. In: ICASSP (2020)'
  id: totrans-853
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Huang, S., Li, X., Zhang, Z., Wu, F., Gao, S., Ji, R., Han, J.: Body structure
    aware deep crowd counting. TIP (2017)'
  id: totrans-854
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Huberman-Spiegelglas, I., Fattal, R.: Single image object counting and
    localizing using active-learning. In: WACV (2022)'
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Idrees, H., Saleemi, I., Seibert, C., Shah, M.: Multi-source multi-scale
    counting in extremely dense crowd images. In: CVPR (2013)'
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Idrees, H., Tayyab, M., Athrey, K., Zhang, D., Al-Maadeed, S., Rajpoot,
    N., Shah, M.: Composition loss for counting, density map estimation and localization
    in dense crowds. In: ECCV (2018)'
  id: totrans-857
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Jiang, S., Lu, X., Lei, Y., Liu, L.: Mask-aware networks for crowd counting.
    TCSVT (2019)'
  id: totrans-858
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Jiang, X., Xiao, Z., Zhang, B., Zhen, X., Cao, X., Doermann, D., Shao,
    L.: Crowd counting and density estimation by trellis encoder-decoder networks.
    In: CVPR (2019)'
  id: totrans-859
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Jiang, X., Zhang, L., Lv, P., Guo, Y., Zhu, R., Li, Y., Pang, Y., Li,
    X., Zhou, B., Xu, M.: Learning multi-level density maps for crowd counting. TNNLS
    (2019)'
  id: totrans-860
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Jiang, X., Zhang, L., Xu, M., Zhang, T., Lv, P., Zhou, B., Yang, X., Pang,
    Y.: Attention scaling for crowd counting. In: CVPR (2020)'
  id: totrans-861
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Jiang, X., Zhang, L., Zhang, T., Lv, P., Zhou, B., Pang, Y., Xu, M., Xu,
    C.: Density-aware multi-task learning for crowd counting. IEEE Transactions on
    Multimedia (2020)'
  id: totrans-862
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Kang, D., Chan, A.: Crowd counting by adaptively fusing predictions from
    an image pyramid. BMVC (2018)'
  id: totrans-863
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Khaki, S., Pham, H., Han, Y., Kuhl, A., Kent, W., Wang, L.: Deepcorn:
    A semi-supervised deep learning method for high-throughput image-based corn kernel
    counting and yield estimation. Knowledge-Based Systems (2021)'
  id: totrans-864
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Kong, X., Zhao, M., Zhou, H., Zhang, C.: Weakly supervised crowd-wise
    attention for robust crowd counting. In: ICASSP (2020)'
  id: totrans-865
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Krizhevsky, A., Sutskever, I., Hinton, G.E.: Imagenet classification with
    deep convolutional neural networks. NIPS (2012)'
  id: totrans-866
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Kumar, A., Jain, N., Tripathi, S., Singh, C., Krishna, K.: Mtcnet: Multi-task
    learning paradigm for crowd count estimation. IEEE AVSS (2019)'
  id: totrans-867
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Laradji, I.H., Rostamzadeh, N., Pinheiro, P.O., Vazquez, D., Schmidt,
    M.: Where are the blobs: Counting by localization with point supervision. In:
    ECCV (2018)'
  id: totrans-868
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Lei, Y., Liu, Y., Zhang, P., Liu, L.: Towards using count-level weak supervision
    for crowd counting. Pattern Recognition (2021)'
  id: totrans-869
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Lempitsky, V., Zisserman, A.: Learning to count objects in images. In:
    NIPS (2010)'
  id: totrans-870
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Li, B., Huang, H., Zhang, A., Liu, P., Liu, C.: Approaches on crowd counting
    and density estimation: a review. Pattern Analysis and Applications (2021)'
  id: totrans-871
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Li, J., Xue, Y., Wang, W., Ouyang, G.: Cross-level parallel network for
    crowd counting. IEEE Transactions on Industrial Informatics (2019)'
  id: totrans-872
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Li, M., Zhang, Z., Huang, K., Tan, T.: Estimating the number of people
    in crowded scenes by mid based foreground segmentation and head-shoulder detection.
    In: ICPR (2008)'
  id: totrans-873
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Li, T., Chang, H., Wang, M., Ni, B., Hong, R., Yan, S.: Crowded scene
    analysis: A survey. TCSVT (2014)'
  id: totrans-874
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Li, W., Cao, Z., Wang, Q., Chen, S., Feng, R.: Learning error-driven curriculum
    for crowd counting. In: ICPR (2021)'
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Li, W., Yongbo, L., Xiangyang, X.: Coda: Counting objects via scale-aware
    adversarial density adaption. In: ICME (2019)'
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Li, Y., Zhang, X., Chen, D.: Csrnet: Dilated convolutional neural networks
    for understanding the highly congested scenes. In: CVPR (2018)'
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Lian, D., Chen, X., Li, J., Luo, W., Gao, S.: Locating and counting heads
    in crowds with a depth prior. TPAMI (2021)'
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Liang, D., Chen, X., Xu, W., Zhou, Y., Bai, X.: Transcrowd: weakly-supervised
    crowd counting with transformers. Science China Information Sciences (2022)'
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Liang, D., Xu, W., Zhu, Y., Zhou, Y.: Focal inverse distance transform
    maps for crowd localization and counting in dense crowd. arXiv preprint arXiv:2102.07925
    (2021)'
  id: totrans-880
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Lin, H., Hong, X., Ma, Z., Wei, X., Qiu, Y., Wang, Y., Gong, Y.: Direct
    measure matching for crowd counting. IJCAI (2021)'
  id: totrans-881
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Lin, H., Ma, Z., Ji, R., Wang, Y., Hong, X.: Boosting crowd counting via
    multifaceted attention. In: CVPR (2022)'
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Lin, Z., Davis, L.S.: Shape-based human detection and segmentation via
    hierarchical part-template matching. TPAMI (2010)'
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Lin, Z., Davis, L.S.：通过分层部件模板匹配进行基于形状的人体检测和分割。TPAMI (2010)'
- en: '[87] Ling, M., Geng, X.: Indoor crowd counting by mixture of gaussians label
    distribution learning. TIP (2019)'
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Ling, M., Geng, X.：室内人群计数通过高斯混合标签分布学习。TIP (2019)'
- en: '[88] Liu, C., Weng, X., Mu, Y.: Recurrent attentive zooming for joint crowd
    counting and precise localization. In: CVPR (2019)'
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Liu, C., Weng, X., Mu, Y.：递归注意力缩放用于联合人群计数和精确定位。会议：CVPR (2019)'
- en: '[89] Liu, H., Zhao, Q., Ma, Y., Dai, F.: Bipartite matching for crowd counting
    with point supervision. In: IJCAI (2021)'
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Liu, H., Zhao, Q., Ma, Y., Dai, F.：基于点监督的人群计数的二分匹配。会议：IJCAI (2021)'
- en: '[90] Liu, J., Gao, C., Meng, D., Hauptmann, A.G.: Decidenet: Counting varying
    density crowds through attention guided detection and density estimation. In:
    CVPR (2018)'
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Liu, J., Gao, C., Meng, D., Hauptmann, A.G.：Decidenet：通过注意力引导检测和密度估计计数不同密度的人群。会议：CVPR
    (2018)'
- en: '[91] Liu, L., Chen, J., Wu, H., Chen, T., Li, G., Lin, L.: Efficient crowd
    counting via structured knowledge transfer. In: ACM Multimedia (2020)'
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Liu, L., Chen, J., Wu, H., Chen, T., Li, G., Lin, L.：通过结构化知识转移进行高效的人群计数。会议：ACM
    Multimedia (2020)'
- en: '[92] Liu, L., Chen, J., Wu, H., Li, G., Li, C., Lin, L.: Cross-modal collaborative
    representation learning and a large-scale rgbt benchmark for crowd counting. In:
    CVPR (2021)'
  id: totrans-889
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Liu, L., Chen, J., Wu, H., Li, G., Li, C., Lin, L.：跨模态协同表示学习及大规模RGBT基准测试用于人群计数。会议：CVPR
    (2021)'
- en: '[93] Liu, L., Jia, W., Jiang, J., Amirgholipour, S., Wang, Y., Zeibots, M.,
    He, X.: Denet: A universal network for counting crowd with varying densities and
    scales. IEEE Transactions on Multimedia (2020)'
  id: totrans-890
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Liu, L., Jia, W., Jiang, J., Amirgholipour, S., Wang, Y., Zeibots, M.,
    He, X.：Denet：一种通用网络用于计数具有不同密度和尺度的人群。IEEE 多媒体学报 (2020)'
- en: '[94] Liu, L., Lu, H., Zou, H., Xiong, H., Cao, Z., Shen, C.: Weighing counts:
    Sequential crowd counting by reinforcement learning. In: ECCV (2020)'
  id: totrans-891
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Liu, L., Lu, H., Zou, H., Xiong, H., Cao, Z., Shen, C.：计数加权：通过强化学习进行顺序人群计数。会议：ECCV
    (2020)'
- en: '[95] Liu, L., Qiu, Z., Li, G., Liu, S., Ouyang, W., Lin, L.: Crowd counting
    with deep structured scale integration network. In: ICCV (2019)'
  id: totrans-892
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Liu, L., Qiu, Z., Li, G., Liu, S., Ouyang, W., Lin, L.：通过深度结构化尺度融合网络进行人群计数。会议：ICCV
    (2019)'
- en: '[96] Liu, L., Wang, H., Li, G., Ouyang, W., Lin, L.: Crowd counting using deep
    recurrent spatial-aware network. IJCAI (2018)'
  id: totrans-893
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Liu, L., Wang, H., Li, G., Ouyang, W., Lin, L.：使用深度递归空间感知网络进行人群计数。IJCAI
    (2018)'
- en: '[97] Liu, N., Long, Y., Zou, C., Niu, Q., Pan, L., Wu, H.: Adcrowdnet: An attention-injective
    deformable convolutional network for crowd understanding. In: CVPR (2019)'
  id: totrans-894
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Liu, N., Long, Y., Zou, C., Niu, Q., Pan, L., Wu, H.：Adcrowdnet：一种用于人群理解的注意力注入可变形卷积网络。会议：CVPR
    (2019)'
- en: '[98] Liu, W., Durasov, N., Fua, P.: Leveraging self-supervision for cross-domain
    crowd counting. In: CVPR (2022)'
  id: totrans-895
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Liu, W., Durasov, N., Fua, P.：利用自监督进行跨域人群计数。会议：CVPR (2022)'
- en: '[99] Liu, W., Salzmann, M., Fua, P.: Context-aware crowd counting. In: CVPR
    (2019)'
  id: totrans-896
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Liu, W., Salzmann, M., Fua, P.：上下文感知的人群计数。会议：CVPR (2019)'
- en: '[100] Liu, W., Salzmann, M., Fua, P.: Counting people by estimating people
    flows. TPAMI (2020)'
  id: totrans-897
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Liu, W., Salzmann, M., Fua, P.：通过估计人员流动来计数人员。TPAMI (2020)'
- en: '[101] Liu, W., Salzmann, M., Fua, P.: Estimating people flows to better count
    them in crowded scenes. In: ECCV (2020)'
  id: totrans-898
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Liu, W., Salzmann, M., Fua, P.：估计人员流动以更好地计数拥挤场景中的人群。会议：ECCV (2020)'
- en: '[102] Liu, X., Li, G., Han, Z., Zhang, W., Yang, Y., Huang, Q., Sebe, N.: Exploiting
    sample correlation for crowd counting with multi-expert network. In: ICCV (2021)'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Liu, X., Li, G., Han, Z., Zhang, W., Yang, Y., Huang, Q., Sebe, N.：利用样本相关性通过多专家网络进行人群计数。会议：ICCV
    (2021)'
- en: '[103] Liu, X., Van De Weijer, J., Bagdanov, A.D.: Leveraging unlabeled data
    for crowd counting by learning to rank. In: CVPR (2018)'
  id: totrans-900
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Liu, X., Van De Weijer, J., Bagdanov, A.D.：通过学习排序利用未标记数据进行人群计数。会议：CVPR
    (2018)'
- en: '[104] Liu, X., Yang, J., Ding, W., Wang, T., Wang, Z., Xiong, J.: Adaptive
    mixture regression network with local counting map for crowd counting. In: ECCV
    (2020)'
  id: totrans-901
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Liu, X., Yang, J., Ding, W., Wang, T., Wang, Z., Xiong, J.：具有局部计数图的自适应混合回归网络用于人群计数。会议：ECCV
    (2020)'
- en: '[105] Liu, Y., Liu, L., Wang, P., Zhang, P., Lei, Y.: Semi-supervised crowd
    counting via self-training on surrogate tasks. In: ECCV (2020)'
  id: totrans-902
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Liu, Y., Liu, L., Wang, P., Zhang, P., Lei, Y.：通过在替代任务上的自我训练进行半监督人群计数。会议：ECCV
    (2020)'
- en: '[106] Liu, Y., Shi, M., Zhao, Q., Wang, X.: Point in, box out: Beyond counting
    persons in crowds. In: CVPR (2019)'
  id: totrans-903
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Liu, Y., Shi, M., Zhao, Q., Wang, X.：点在，框出：超越人群中的人员计数。会议：CVPR (2019)'
- en: '[107] Liu, Y., Wang, Z., Shi, M., Satoh, S., Zhao, Q., Yang, H.: Towards unsupervised
    crowd counting via regression-detection bi-knowledge transfer. In: ACM Multimedia
    (2020)'
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Liu, Y., Wen, Q., Chen, H., Liu, W., Qin, J., Han, G., He, S.: Crowd
    counting via cross-stage refinement networks. IEEE Transactions on Image Processing
    (2020)'
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Louëdec, J.L., Cielniak, G.: Gaussian map predictions for 3d surface
    feature localisation and counting. BMVC (2021)'
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Luo, A., Yang, F., Li, X., Nie, D., Jiao, Z., Zhou, S., Cheng, H.: Hybrid
    graph neural networks for crowd counting. In: AAAI (2020)'
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Ma, J., Dai, Y., Tan, Y.P.: Atrous convolutions spatial pyramid network
    for crowd counting and density estimation. Neurocomputing (2019)'
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Ma, Y.J., Shuai, H.H., Cheng, W.H.: Spatiotemporal dilated convolution
    with uncertain matching for video-based crowd estimation. IEEE Transactions on
    Multimedia (2021)'
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Ma, Z., Hong, X., Wei, X., Qiu, Y., Gong, Y.: Towards a universal model
    for cross-dataset crowd counting. In: ICCV (2021)'
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Ma, Z., Wei, X., Hong, X., Gong, Y.: Bayesian loss for crowd count estimation
    with point supervision. In: ICCV (2019)'
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Ma, Z., Wei, X., Hong, X., Gong, Y.: Learning scales from points: A scale-aware
    probabilistic model for crowd counting. In: ACM Multimedia (2020)'
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Ma, Z., Wei, X., Hong, X., Lin, H., Qiu, Y., Gong, Y.: Learning to count
    via unbalanced optimal transport. In: AAAI (2021)'
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Mao, J., Niu, M., Bai, H., Liang, X., Xu, H., Xu, C.: Pyramid r-cnn:
    Towards better performance and adaptability for 3d object detection. In: ICCV
    (2021)'
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Mao, J., Niu, M., Jiang, C., Liang, H., Chen, J., Liang, X., Li, Y.,
    Ye, C., Zhang, W., Li, Z., et al.: One million scenes for autonomous driving:
    Once dataset. arXiv preprint arXiv:2106.11037 (2021)'
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Marsden, M., McGuinness, K., Little, S., O’Connor, N.E.: Fully convolutional
    crowd counting on highly congested scenes. VISAPP (2016)'
  id: totrans-916
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Marsden, M., McGuinness, K., Little, S., O’Connor, N.E.: Resnetcrowd:
    A residual deep learning architecture for crowd counting, violent behaviour detection
    and crowd density level classification. In: AVSS (2017)'
  id: totrans-917
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Meng, C., Liu, E., Neiswanger, W., Song, J., Burke, M., Lobell, D., Ermon,
    S.: Is-count: Large-scale object counting from satellite images with covariate-based
    importance sampling. AAAI (2021)'
  id: totrans-918
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Meng, Y., Zhang, H., Zhao, Y., Yang, X., Qian, X., Huang, X., Zheng,
    Y.: Spatial uncertainty-aware semi-supervised crowd counting. In: ICCV (2021)'
  id: totrans-919
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Miao, Y., Lin, Z., Ding, G., Han, J.: Shallow feature based dense attention
    network for crowd counting. In: AAAI (2020)'
  id: totrans-920
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Mo, H., Ren, W., Xiong, Y., Pan, X., Zhou, Z., Cao, X., Wu, W.: Background
    noise filtering and distribution dividing for crowd counting. IEEE Transactions
    on Image Processing (2020)'
  id: totrans-921
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Modolo, D., Shuai, B., Varior, R.R., Tighe, J.: Understanding the impact
    of mistakes on background regions in crowd counting. In: WACV (2021)'
  id: totrans-922
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Oh, M.h., Olsen, P.A., Ramamurthy, K.N.: Crowd counting with decomposed
    uncertainty. In: AAAI (2020)'
  id: totrans-923
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Olmschenk, G., Chen, J., Tang, H., Zhu, Z.: Dense crowd counting convolutional
    neural networks with minimal data using semi-supervised dual-goal generative adversarial
    networks. In: CVPR Workshops (2019)'
  id: totrans-924
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Olmschenk, G., Tang, H., Zhu, Z.: Crowd counting with minimal data using
    generative adversarial networks for multiple target regression. In: WACV (2018)'
  id: totrans-925
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Olmschenk, G., Tang, H., Zhu, Z.: Improving dense crowd counting convolutional
    neural networks using inverse k-nearest neighbor maps and multiscale upsampling.
    VISAPP (2019)'
  id: totrans-926
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Olmschenk, G., Zhu, Z., Tang, H.: Generalizing semi-supervised generative
    adversarial networks to regression using feature contrasting. CVIU (2019)'
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Onoro-Rubio, D., López-Sastre, R.J.: Towards perspective-free object
    counting with deep learning. In: ECCV (2016)'
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Pan, X., Mo, H., Zhou, Z., Wu, W.: Attention guided region division for
    crowd counting. In: ICASSP (2020)'
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Pham, V.Q., Kozakaya, T., Yamaguchi, O., Okada, R.: Count forest: Co-voting
    uncertain number of targets using random forest for crowd density estimation.
    In: ICCV (2015)'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Qiu, Z., Liu, L., Li, G., Wang, Q., Xiao, N., Lin, L.: Crowd counting
    via multi-view scale aggregation networks. In: ICME (2019)'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Rabaud, V., Belongie, S.: Counting crowded moving objects. In: CVPR (2006)'
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Ranjan, V., Le, H., Hoai, M.: Iterative crowd counting. In: ECCV (2018)'
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Ranjan, V., Wang, B., Shah, M., Hoai, M.: Uncertainty estimation and
    sample selection for crowd counting. In: ACCV (2020)'
  id: totrans-934
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Reddy, M.K.K., Hossain, M., Rochan, M., Wang, Y.: Few-shot scene adaptive
    crowd counting using meta-learning. In: WACV (2020)'
  id: totrans-935
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Reddy, M.K.K., Rochan, M., Lu, Y., Wang, Y.: Adacrowd: unlabeled scene
    adaptation for crowd counting. IEEE Transactions on Multimedia (2021)'
  id: totrans-936
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Ren, W., Wang, X., Tian, J., Tang, Y., Chan, A.B.: Tracking-by-counting:
    Using network flows on crowd density maps for tracking multiple targets. IEEE
    Transactions on Image Processing (2020)'
  id: totrans-937
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Rong, L., Li, C.: Coarse-and fine-grained attention network with background-aware
    loss for crowd density map estimation. In: WACV (2021)'
  id: totrans-938
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Sajid, U., Chen, X., Sajid, H., Kim, T., Wang, G.: Audio-visual transformer
    based crowd counting. In: ICCV (2021)'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Sajid, U., Ma, W., Wang, G.: Multi-resolution fusion and multi-scale
    input priors based crowd counting. In: ICPR (2021)'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Sajid, U., Wang, G.: Plug-and-play rescaling based crowd counting in
    static images. In: WACV (2020)'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Sajid, U., Wang, G.: Towards more effective prm-based crowd counting
    via a multi-resolution fusion and attention network. Neurocomputing (2022)'
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Sam, D.B., Agarwalla, A., Joseph, J., Sindagi, V.A., Babu, R.V., Patel,
    V.M.: Completely self-supervised crowd counting via distribution matching. arXiv
    preprint arXiv:2009.06420 (2020)'
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Sam, D.B., Peri, S.V., Sundararaman, M.N., Kamath, A., Babu, R.V.: Locate,
    size, and count: accurately resolving people in dense crowds via detection. TPAMI
    (2020)'
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Sam, D.B., Sajjan, N.N., Maurya, H., Babu, R.V.: Almost unsupervised
    learning for dense crowd counting. In: AAAI (2019)'
  id: totrans-945
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Servadei, L., Sun, H., Ott, J., Stephan, M., Hazra, S., Stadelmayer,
    T., Lopera, D.S., Wille, R., Santra, A.: Label-aware ranked loss for robust people
    counting using automotive in-cabin radar. In: ICASSP (2022)'
  id: totrans-946
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Shang, C., Ai, H., Bai, B.: End-to-end crowd counting via joint learning
    local and global count. In: ICIP (2016)'
  id: totrans-947
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Shen, Z., Xu, Y., Ni, B., Wang, M., Hu, J., Yang, X.: Crowd counting
    via adversarial cross-scale consistency pursuit. In: CVPR (2018)'
  id: totrans-948
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Shi, X., Li, X., Wu, C., Kong, S., Yang, J., He, L.: A real-time deep
    network for crowd counting. In: ICASSP (2020)'
  id: totrans-949
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Shi, Z., Mettes, P., Snoek, C.G.: Counting with focus for free. In: ICCV
    (2019)'
  id: totrans-950
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Shi, Z., Zhang, L., Liu, Y., Cao, X., Ye, Y., Cheng, M.M., Zheng, G.:
    Crowd counting with deep negative correlation learning. In: CVPR (2018)'
  id: totrans-951
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Shivapuja, S.V., Khamkar, M.P., Bajaj, D., Ramakrishnan, G., Sarvadevabhatla,
    R.K.: Wisdom of (binned) crowds: A bayesian stratification paradigm for crowd
    counting. In: ACM Multimedia (2021)'
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Sindagi, V.A., Patel, V.M.: Cnn-based cascaded multi-task learning of
    high-level prior and density estimation for crowd counting. In: AVSS (2017)'
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Sindagi, V.A., Patel, V.M.: Generating high-quality crowd density maps
    using contextual pyramid cnns. In: ICCV (2017)'
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Sindagi, V.A., Patel, V.M.: A survey of recent advances in cnn-based
    single image crowd counting and density estimation. Pattern Recognition Letters
    (2018)'
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Sindagi, V.A., Patel, V.M.: Ha-ccn: Hierarchical attention-based crowd
    counting network. TIP (2019)'
  id: totrans-956
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Sindagi, V.A., Patel, V.M.: Multi-level bottom-top and top-bottom feature
    fusion for crowd counting. In: ICCV (2019)'
  id: totrans-957
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Sindagi, V.A., Yasarla, R., Babu, D.S., Babu, R.V., Patel, V.M.: Learning
    to count in the crowd from limited labeled data. In: ECCV (2020)'
  id: totrans-958
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Sindagi, V.A., Yasarla, R., Patel, V.M.: Pushing the frontiers of unconstrained
    crowd counting: New dataset and benchmark method. In: ICCV (2019)'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Song, Q., Wang, C., Jiang, Z., Wang, Y., Tai, Y., Wang, C., Li, J., Huang,
    F., Wu, Y.: Rethinking counting and localization in crowds: A purely point-based
    framework. In: ICCV (2021)'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Song, Q., Wang, C., Wang, Y., Tai, Y., Wang, C., Li, J., Wu, J., Ma,
    J.: To choose or to fuse? scale selection for crowd counting. In: AAAI (2021)'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Tan, X., Tao, C., Ren, T., Tang, J., Wu, G.: Crowd counting via multi-layer
    regression. In: ACM Multimedia (2019)'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Tang, H., Wang, Y., Chau, L.P.: Tafnet: A three-stream adaptive fusion
    network for rgb-t crowd counting. ISCAS (2022)'
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Teixeira, T., Dublon, G., Savvides, A.: A survey of human-sensing: Methods
    for detecting presence, count, location, track, and identity. ACM Computing Surveys
    (2010)'
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Thanasutives, P., Fukui, K.i., Numao, M., Kijsirikul, B.: Encoder-decoder
    based convolutional neural networks with multi-scale-aware modules for crowd counting.
    In: ICPR (2021)'
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Walach, E., Wolf, L.: Learning to count with cnn boosting. In: ECCV (2016)'
  id: totrans-966
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Wan, J., Chan, A.: Adaptive density map generation for crowd counting.
    In: ICCV (2019)'
  id: totrans-967
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Wan, J., Chan, A.: Modeling noisy annotations for crowd counting. NeurIPS
    (2020)'
  id: totrans-968
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Wan, J., Kumar, N.S., Chan, A.B.: Fine-grained crowd counting. IEEE transactions
    on image processing (2021)'
  id: totrans-969
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Wan, J., Liu, Z., Chan, A.B.: A generalized loss function for crowd counting
    and localization. In: CVPR (2021)'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Wan, J., Wang, Q., Chan, A.B.: Kernel-based density map generation for
    dense object counting. TPAMI (2020)'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Wang, B., Liu, H., Samaras, D., Nguyen, M.H.: Distribution matching for
    crowd counting. NeurIPS (2020)'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Wang, C., Song, Q., Zhang, B., Wang, Y., Tai, Y., Hu, X., Wang, C., Li,
    J., Ma, J., Wu, Y.: Uniformity in heterogeneity: Diving deep into count interval
    partition for crowd counting. In: ICCV (2021)'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Wang, C., Zhang, H., Yang, L., Liu, S., Cao, X.: Deep people counting
    in extremely dense crowds. In: ACM Multimedia (2015)'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Wang, F., Sang, J., Wu, Z., Liu, Q., Sang, N.: Hybrid attention network
    based on progressive embedding scale-context for crowd counting. Information Sciences
    (2022)'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Wang, L., Yin, B., Tang, X., Li, Y.: Removing background interference
    for crowd counting via de-background detail convolutional network. Neurocomputing
    (2019)'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Wang, M., Cai, H., Han, X., Zhou, J., Gong, M.: Stnet: Scale tree network
    with multi-level auxiliator for crowd counting. IEEE Transactions on Multimedia
    (2022)'
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Wang, M., Cai, H., Zhou, J., Gong, M.: Interlayer and intralayer scale
    aggregation for scale-invariant crowd counting. Neurocomputing (2021)'
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Wang, M., Zhou, J., Cai, H., Gong, M.: Crowdmlp: Weakly-supervised crowd
    counting via multi-granularity mlp. arXiv preprint arXiv:2203.08219 (2022)'
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Wang, P., Gao, C., Wang, Y., Li, H., Gao, Y.: Mobilecount: An efficient
    encoder-decoder framework for real-time crowd counting. Neurocomputing (2020)'
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Wang, Q., Breckon, T.P.: Crowd counting via segmentation guided attention
    networks and curriculum loss. IEEE Transactions on Intelligent Transportation
    Systems (2022)'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Wang, Q., Gao, J., Lin, W., Li, X.: Nwpu-crowd: A large-scale benchmark
    for crowd counting and localization. TPAMI (2020)'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Wang, Q., Gao, J., Lin, W., Yuan, Y.: Learning from synthetic data for
    crowd counting in the wild. In: CVPR (2019)'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Wang, Q., Gao, J., Lin, W., Yuan, Y.: Pixel-wise crowd understanding
    via synthetic data. IJCV (2020)'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Wang, Q., Gao, J., Lin, W., Yuan, Y.: Pixel-wise crowd understanding
    via synthetic data. IJCV (2021)'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Wang, Q., Han, T., Gao, J., Yuan, Y.: Neuron linear transformation: Modeling
    the domain shift for crowd counting. TNNLS (2021)'
  id: totrans-986
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Wang, Q., Lin, W., Gao, J., Li, X.: Density-aware curriculum learning
    for crowd counting. IEEE Transactions on Cybernetics (2020)'
  id: totrans-987
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Wang, Y., Hou, J., Hou, X., Chau, L.P.: A self-training approach for
    point-supervised object detection and counting in crowds. IEEE Transactions on
    Image Processing (2021)'
  id: totrans-988
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Wang, Y., Hou, X., Chau, L.P.: Dense point prediction: A simple baseline
    for crowd counting and localization. In: ICMEW (2021)'
  id: totrans-989
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Wang, Y., Ma, Z., Wei, X., Zheng, S., Wang, Y., Hong, X.: Eccnas: Efficient
    crowd counting neural architecture search. TOMM (2022)'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P.: Image quality
    assessment: from error visibility to structural similarity. TIP (2004)'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Wang, Z., Xiao, Z., Xie, K., Qiu, Q., Zhen, X., Cao, X.: In defense of
    single-column networks for crowd counting. BMVC (2018)'
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Wei, B., Yuan, Y., Wang, Q.: Mspnet: Multi-supervised parallel network
    for crowd counting. In: ICASSP (2020)'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Wei, X., Du, J., Liang, M., Ye, L.: Boosting deep attribute learning
    via support vector regression for fast moving crowd counting. Pattern Recognition
    Letters (2019)'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] Wen, L., Du, D., Zhu, P., Hu, Q., Wang, Q., Bo, L., Lyu, S.: Detection,
    tracking, and counting meets drones in crowds: A benchmark. In: CVPR (2021)'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Wu, Q., Wan, J., Chan, A.B.: Dynamic momentum adaptation for zero-shot
    cross-domain crowd counting. In: ACM Multimedia (2021)'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Wu, X., Xu, B., Zheng, Y., Ye, H., Yang, J., He, L.: Fast video crowd
    counting with a temporal aware network. Neurocomputing (2020)'
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Wu, X., Zheng, Y., Ye, H., Hu, W., Ma, T., Yang, J., He, L.: Counting
    crowds with varying densities via adaptive scenario discovery framework. Neurocomputing
    (2020)'
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Wu, X., Zheng, Y., Ye, H., Hu, W., Yang, J., He, L.: Adaptive scenario
    discovery for crowd counting. In: ICASSP (2019)'
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Wu, Z., Sang, J., Shi, Y., Liu, Q., Sang, N., Liu, X.: Cranet: Cascade
    residual attention network for crowd counting. In: ICME (2021)'
  id: totrans-1000
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Xiong, F., Shi, X., Yeung, D.Y.: Spatiotemporal modeling for crowd counting
    in videos. In: ICCV (2017)'
  id: totrans-1001
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Xiong, H., Lu, H., Liu, C., Liu, L., Cao, Z., Shen, C.: From open set
    to closed set: Counting objects by spatial divide-and-conquer. In: ICCV (2019)'
  id: totrans-1002
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Xu, C., Liang, D., Xu, Y., Bai, S., Zhan, W., Bai, X., Tomizuka, M.:
    Autoscale: Learning to scale for crowd counting. IJCV (2022)'
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Xu, W., Liang, D., Zheng, Y., Xie, J., Ma, Z.: Dilated-scale-aware category-attention
    convnet for multi-class object counting. IEEE Signal Processing Letters (2021)'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Xu, Y., Zhong, Z., Lian, D., Li, J., Li, Z., Xu, X., Gao, S.: Crowd counting
    with partial annotations in an image. In: ICCV (2021)'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Yan, Z., Li, P., Wang, B., Ren, D., Zuo, W.: Towards learning multi-domain
    crowd counting. IEEE Trans. Circuits Syst. Video Technol (2021)'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Yan, Z., Yuan, Y., Zuo, W., Tan, X., Wang, Y., Wen, S., Ding, E.: Perspective-guided
    convolution networks for crowd counting. In: ICCV (2019)'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Yan, Z., Zhang, R., Zhang, H., Zhang, Q., Zuo, W.: Crowd counting via
    perspective-guided fractional-dilation convolution. IEEE Transactions on Multimedia
    (2021)'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Yang, B., Zhan, W., Wang, N., Liu, X., Lv, J.: Counting crowds using
    a scale-distribution-aware network and adaptive human-shaped kernel. Neurocomputing
    (2020)'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Yang, J., Zhou, Y., Kung, S.Y.: Multi-scale generative adversarial networks
    for crowd counting. In: ICPR (2018)'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Yang, S.D., Su, H.T., Hsu, W.H., Chen, W.C.: Class-agnostic few-shot
    object counting. In: WACV (2021)'
  id: totrans-1011
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Yang, Y., Li, G., Du, D., Huang, Q., Sebe, N.: Embedding perspective
    analysis into multi-column convolutional neural network for crowd counting. IEEE
    Transactions on Image Processing (2020)'
  id: totrans-1012
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Yang, Y., Li, G., Wu, Z., Su, L., Huang, Q., Sebe, N.: Reverse perspective
    network for perspective-aware object counting. In: CVPR (2020)'
  id: totrans-1013
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Yang, Y., Li, G., Wu, Z., Su, L., Huang, Q., Sebe, N.: Weakly-supervised
    crowd counting learns from sorting rather than locations. In: ECCV (2020)'
  id: totrans-1014
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Zand, M., Damirchi, H., Farley, A., Molahasani, M., Greenspan, M., Etemad,
    A.: Multiscale crowd counting and localization by multitask point supervision.
    In: ICASSP (2022)'
  id: totrans-1015
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Zhang, A., Shen, J., Xiao, Z., Zhu, F., Zhen, X., Cao, X., Shao, L.:
    Relational attention network for crowd counting. In: ICCV (2019)'
  id: totrans-1016
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Zhang, A., Yue, L., Shen, J., Zhu, F., Zhen, X., Cao, X., Shao, L.: Attentional
    neural fields for crowd counting. In: ICCV (2019)'
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Zhang, C., Li, H., Wang, X., Yang, X.: Cross-scene crowd counting via
    deep convolutional neural networks. In: CVPR (2015)'
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Zhang, L., Shi, M., Chen, Q.: Crowd counting via scale-adaptive convolutional
    neural network. In: WACV (2018)'
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Zhang, L., Shi, Z., Cheng, M.M., Liu, Y., Bian, J.W., Zhou, J.T., Zheng,
    G., Zeng, Z.: Nonlinear regression via deep negative correlation learning. TPAMI
    (2019)'
  id: totrans-1020
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Zhang, Q., Chan, A.B.: Wide-area crowd counting via ground-plane density
    maps and multi-view fusion cnns. In: CVPR (2019)'
  id: totrans-1021
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] Zhang, Q., Chan, A.B.: 3d crowd counting via multi-view fusion with 3d
    gaussian kernels. In: AAAI (2020)'
  id: totrans-1022
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Zhang, Q., Lin, W., Chan, A.B.: Cross-view cross-scene multi-view crowd
    counting. In: CVPR (2021)'
  id: totrans-1023
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Zhang, Y., Zhou, C., Chang, F., Kot, A.C.: Multi-resolution attention
    convolutional neural network for crowd counting. Neurocomputing (2019)'
  id: totrans-1024
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Zhang, Y., Zhou, D., Chen, S., Gao, S., Ma, Y.: Single-image crowd counting
    via multi-column convolutional neural network. In: CVPR (2016)'
  id: totrans-1025
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Zhao, M., Zhang, C., Zhang, J., Porikli, F., Ni, B., Zhang, W.: Scale-aware
    crowd counting via depth-embedded convolutional neural networks. TCSVT (2019)'
  id: totrans-1026
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Zhao, Z., Han, T., Gao, J., Wang, Q., Li, X.: A flow base bi-path network
    for cross-scene video crowd understanding in aerial view. In: ECCV (2020)'
  id: totrans-1027
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Zhao, Z., Shi, M., Zhao, X., Li, L.: Active crowd counting with limited
    supervision. In: ECCV (2020)'
  id: totrans-1028
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Zheng, L., Li, Y., Mu, Y.: Learning factorized cross-view fusion for
    multi-view crowd counting. In: ICME (2021)'
  id: totrans-1029
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Zhong, X., Yan, Z., Qin, J., Zuo, W., Lu, W.: An improved normed-deformable
    convolution for crowd counting. SPL (2022)'
  id: totrans-1030
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Zhou, J.T., Zhang, L., Du, J., Peng, X., Fang, Z., Xiao, Z., Zhu, H.:
    Locality-aware crowd counting. TPAMI (2021)'
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Zhou, Q., Zhang, J., Che, L., Shan, H., Wang, J.Z.: Crowd counting with
    limited labeling through submodular frame selection. IEEE Transactions on Intelligent
    Transportation Systems (2018)'
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Zhou, Y., Yang, J., Li, H., Cao, T., Kung, S.Y.: Adversarial learning
    for multiscale crowd counting under complex scenes. IEEE transactions on cybernetics
    (2020)'
  id: totrans-1033
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Zhu, P., Peng, T., Du, D., Yu, H., Zhang, L., Hu, Q.: Graph regularized
    flow attention network for video animal counting from drones. IEEE Transactions
    on Image Processing (2021)'
  id: totrans-1034
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Zhu, P., Wen, L., Du, D., Bian, X., Ling, H., Hu, Q., Wu, H., Nie, Q.,
    Cheng, H., Liu, C., et al.: Visdrone-vdt2018: The vision meets drone video detection
    and tracking challenge results. In: ECCV Workshops (2018)'
  id: totrans-1035
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Zitouni, M.S., Bhaskar, H., Dias, J., Al-Mualla, M.E.: Advances and trends
    in visual crowd analysis: A systematic survey and evaluation of crowd modelling
    techniques. Neurocomputing (2016)'
  id: totrans-1036
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Zou, Z., Cheng, Y., Qu, X., Ji, S., Guo, X., Zhou, P.: Attend to count:
    Crowd counting with adaptive capacity multi-scale cnns. Neurocomputing (2019)'
  id: totrans-1037
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Zou, Z., Liu, Y., Xu, S., Wei, W., Wen, S., Zhou, P.: Crowd counting
    via hierarchical scale recalibration network. ECAI (2020)'
  id: totrans-1038
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Zou, Z., Shao, H., Qu, X., Wei, W., Zhou, P.: Enhanced 3d convolutional
    networks for crowd counting. BMVC (2019)'
  id: totrans-1039
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
