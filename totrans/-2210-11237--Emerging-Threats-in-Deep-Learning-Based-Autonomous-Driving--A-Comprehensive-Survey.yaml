- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:43:44'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2210.11237] Emerging Threats in Deep Learning-Based Autonomous Driving: A
    Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2210.11237](https://ar5iv.labs.arxiv.org/html/2210.11237)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: \WarningFilter
  prefs: []
  type: TYPE_NORMAL
- en: Fancyhdris too small
  prefs: []
  type: TYPE_NORMAL
- en: \tnotetext
  prefs: []
  type: TYPE_NORMAL
- en: '[1]This document is the results of the research project funded by the National
    Natural Science Foundation of China: Research on secure data management mechanism
    of new college entrance examination comprehensive quality evaluation: A security
    enhancement of block chain empowerment(No. 72204077)'
  prefs: []
  type: TYPE_NORMAL
- en: \tnotetext
  prefs: []
  type: TYPE_NORMAL
- en: '[2]This document is the results of the research project funded bt the Hubei
    Natural Science Foundation project:Research on the data security management mechanism
    of new College Entrance Examination comprehensive quality evaluation based on
    blockchain.(No. 2021CFB470)'
  prefs: []
  type: TYPE_NORMAL
- en: '[style=chinese]'
  prefs: []
  type: TYPE_NORMAL
- en: url]https://scholar.google.com/citations?user=1XoXUTYAAAAJ&hl=en
  prefs: []
  type: TYPE_NORMAL
- en: '[style=chinese]'
  prefs: []
  type: TYPE_NORMAL
- en: '[style=chinese]'
  prefs: []
  type: TYPE_NORMAL
- en: '[style=chinese]'
  prefs: []
  type: TYPE_NORMAL
- en: '[style=chinese] \cormark[1]'
  prefs: []
  type: TYPE_NORMAL
- en: \cortext
  prefs: []
  type: TYPE_NORMAL
- en: '[cor1]Corresponding author \cortext[cor2]First two author equal contribution'
  prefs: []
  type: TYPE_NORMAL
- en: 'Emerging Threats in Deep Learning-Based Autonomous Driving: A Comprehensive
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Cao Hui cao-hui@whu.edu.cn[    Zou Wenlong    Wang Yinkun    Song Ting    Liu
    Mengjun lmj_whu@163.com School of Education,Hubei University,Youyi Road No.368,Wuhan,430062,Hubei,P.R.China
    Institute of Information Engineering,Chinese Academy of Sciences, Beijing,100093,P.R.China
    School of Foreign Languages,Hubei University,Youyi Road,No.368, Wuhan,430062,Hubei,P.R.China
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Since the 2004 DARPA Grand Challenge, the autonomous driving technology has
    witnessed nearly two decades of rapid development. Particularly, in recent years,
    with the application of new sensors and deep learning technologies extending to
    the autonomous field, the development of autonomous driving technology has continued
    to make breakthroughs. Thus, many carmakers and high-tech giants dedicated to
    research and system development of autonomous driving. However, as the foundation
    of autonomous driving, the deep learning technology faces many new security risks.
    The academic community has proposed deep learning countermeasures against the
    adversarial examples and AI backdoor, and has introduced them into the autonomous
    driving field for verification. Deep learning security matters to autonomous driving
    system security, and then matters to personal safety, which is an issue that deserves
    attention and research.This paper provides an summary of the concepts, developments
    and recent research in deep learning security technologies in autonomous driving.
    Firstly, we briefly introduce the deep learning framework and pipeline in the
    autonomous driving system, which mainly include the deep learning technologies
    and algorithms commonly used in this field. Moreover, we focus on the potential
    security threats of the deep learning based autonomous driving system in each
    functional layer in turn. We reviews the development of deep learning attack technologies
    to autonomous driving, investigates the State-of-the-Art algorithms, and reveals
    the potential risks. At last, we provides an outlook on deep learning security
    in the autonomous driving field and proposes recommendations for building a safe
    and trustworthy autonomous driving system.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Trustworthy AI
  prefs: []
  type: TYPE_NORMAL
- en: Deep Learning
  prefs: []
  type: TYPE_NORMAL
- en: Artificial Intelligence
  prefs: []
  type: TYPE_NORMAL
- en: Autonomous Driving
  prefs: []
  type: TYPE_NORMAL
- en: Cyber Security
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial Examples
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Research about Autonomous Land Vehicles (ALVs) began as early as 1980s with
    funding from the US Department of Defense(DoD). In the 21st century, DARPA conducted
    the Grand Challenge that launched a new generation of autonomous driving. The
    development of artificial intelligence(AI) technology is driving the rapid progress
    of autonomous vehicles with an increasing expectation from the public. Currently,
    many traditional carmakers, such as universal Motors, Toyota, Volvo, BMW and Audi
    have carried out researches into the autonomous driving system. On another hand,
    not to be outdone, most of high-tech giants, Google Waymo, Tesla, Baidu and Huawei,
    devoted themselves to autonomous driving technology. Along with artificial intelligence
    technology, autonomous driving has seen rapid development and is expected to enter
    the practical stage.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, security is a major concern in the application of the autonomous driving
    system, because there are new types of security risks associated with autonomous
    driving system that depends heavily on deep learning. On the one hand, from the
    perspective of technical threat on AI security and privacy protection, new countermeasures
    have been proposed successively, including adversarial examples [[1](#bib.bib1),
    [2](#bib.bib2)], data poisoning and AI Backdoor[[3](#bib.bib3)], model extraction[[4](#bib.bib4)],
    model inversion[[5](#bib.bib5)], and membership privacy inference[[6](#bib.bib6)].
    On the another hand, from the social trust perspective of AI, issues about fairness,
    AI abuse, environment, compliance, and ethic, have also received attention and
    research. Currently, there is some literature[[7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]
    summarized AI security threats in the general environment. Different from that,
    this paper focuses on the environment of autonomous driving system, it reveals
    the new security risks posed by AI technologies bringing new security challenges
    to autonomous driving. Unlike other applications of deep learning, the autonomous
    driving system is a more complex AI architecture consisting of dozens of functional
    modules, and different environment modules with different characteristics, raising
    different requirements for AI security attack and mitigation techniques, including:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical world requirements. AI threats of autonomous driving system should
    be able to take effect in the real physical world, and not only in the digital
    world and computer simulation systems. Techniques specific to adversarial examples
    attacks in the physical world are the focus of this paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robustness requirements. The environment is uncertain and often varies to a
    large extent in autonomous driving. On the one hand, the images collected under
    different weather, light and other natural conditions can vary; on the other hand,
    changes in a long distance and large angle range also make image acquisition highly
    variable due to the high-speed movement of vehicles. Therefore, AI threats need
    to be able to take effect continuously and stably under a variety of conditions,
    which raises very high demands on the robustness of attacks, such as adversarial
    examples and AI backdoor. This paper focuses on robustness enhancement methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fusion environment requirements. Autonomous driving system often employs multi-modal
    fusion sensing techniques that combine different types of information from multiple
    RGB cameras, LiDAR, RaDAR, etc., to sense the fused images. The autonomous driving
    environment requires that adversarial examples countermeasures and other related
    threat technologies can be stabilized to remain in effect in the fused environment.
    Artificial intelligence threats in multi-modal fusion environments are also the
    focus of this paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Due to the above concerns and requirements, AI safety technologies in the field
    of autonomous driving have continued to develop, and some research results and
    breakthroughs have been achieved. This paper introduces the latest research progress
    relevant to unique technologies and reveals the AI security risks in autonomous
    driving systems. This paper faces the above challenges of autonomous driving systems,
    rather than in the general environment. Section 1 briefly introduces the infrastructure
    and key technologies of AI in autonomous driving; section 2 offers a glimpse of
    the AI risks in the sensor layer; section 3 comprehensively reviewed the AI risk
    in the perception layer, introduced the idea and detail of important algorithms;
    section 4 provides the potential deep leaning risk and attack technology in decision
    layer in autonomous driving; section 5 focus on new threat of V2X that based on
    federation learning in the future; section 6 gives a summary and outlook.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Basic Concepts of Autonomous Driving
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Essentially, autonomous driving is making driving decisions through artificial
    intelligence techniques or other automated decision-making methods. According
    to the Society of Automotive Engineers (SAE) standard J3016[[14](#bib.bib14)],
    autonomous driving can be categorized into the following classes.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L0 – No Driving Automation: driving is carried out entirely by a person, but
    warnings and system assistance are available during the journey.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L1 – Driver Assistance: based on the perception of the driving environment,
    only a single aspect of automation, which system operates the steering wheel or
    acceleration and deceleration assists the driver with ADAS, while other driving
    operations are performed by the human driver.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L2 – Partial Driving Automation: based on the perception of the driving environment,
    the system operates both the steering wheel and acceleration or deceleration.
    However, it requires a human driver to remain constantly alert and ready to take
    full control with little or no warning.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L3 – Conditional Driving Automation: based on the perception of the driving
    environment, autonomous driving system can perform all driving operations under
    the supervision of a human driver.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L4 – High Driving Automation: under certain environmental conditions, autonomous
    driving system can perform all driving operations unsupervised.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'L5 – Full Driving Automation: the autonomous driving system can perform all
    driving operations unsupervised in all environmental conditions.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For autonomous driving system, there are different views and concepts, as well
    as different development and evolutionary routes. One is focus on intelligentization
    and cyberization of vehicle components, mainly researching on sensors, in-vehicle
    communication, vehicle-to-everything (V2X), and et al, which main participant
    by traditional car-makers. The other is focus on autonomous diving decisions,
    mainly researching artificial intelligence and autonomous driving, and the main
    participants include: UC Berkeley, Google WayMo, Baidu, Apollo, Intel Carla, NVIDIA
    and other artificial intelligence companies. However, whether it starts from the
    vehicle moving towards AI or the other way round, automated driving decision is
    the core mission in autonomous driving, and safety based on AI driving decisions
    making is a necessary prerequisite for the safety of autonomous driving system.
    The higher the level of autonomous driving, the higher the reliance on AI technology
    represented by deep learning, which lead to higher the requirements for the safety
    and robustness of deep learning itself.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Architecture of Autonomous Driving System
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In terms of the autonomous driving architecture and machine learning technologies
    based, autonomous driving systems can be divided into end-to-end (E2E) and modular
    architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8aa4d0566f64fe4afda7d74cb6aa71a8.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Modular Autonomous Driving Framework
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/cda4813ff83678103cf71f1797445fd7.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) E2E Autonomous Driving Framework
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Autonomous Driving Framework'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The modular autonomous driving system divides an individual set of autonomous
    driving functions into several parts, each of which is completed by one or a group
    of artificial intelligence models, usually including: positioning and projecting,
    target recognition, trajectory prediction, road planning & driving decision making,
    vehicle control, and other functions. These functional modules contain the sensing
    layer, the perception layer, the decision layer and the vehicle networking layer.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The End-to-End autonomous driving system often consists of a large number of
    complex judgment functions in driving decisions performed by one or a group of
    artificial intelligence models that make the final driving decision based on the
    environment and cloud inputs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.3 Sensing Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The sensing layer includes a variety of sensors that collect information about
    the environment for the autonomous driving system. Common sensors used in autonomous
    driving vehicles compromise RGB cameras, LiDAR (Light Detection and Ranging),
    RaDAR (Radio Waves to Determine the Distance), GPS, and ultrasonic sensors. Here
    are the characteristics of different sensors:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The advantages of RGB cameras are: 1) lower cost, and 2) relatively mature
    recognition technology; their limitation is that the distance is dependent on
    estimation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantage of LiDAR is that it is accurate; its limitation is that it is
    susceptible to interference from the weather.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The advantage of radar is that it is relatively immune to weather interference;
    its limitation is that it has insufficient imaging capability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are a number of existing works that provide a detailed comparison of sensors
    for autonomous driving vehicles, which will not be the emphasis of this paper.
    There are some survey papers related to the sensing layer[[15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17)].
  prefs: []
  type: TYPE_NORMAL
- en: Most companies have chosen autonomous driving technology solutions that multi-modal
    fusion, while some have chosen solutions that rely primarily on RGB cameras. However,
    it needs to be emphasized that, regardless of the choice of sensor configuration
    solution, the various advanced sensors only fulfill the function of raw information
    collection and do not replace the key role played by artificial intelligence in
    the perception and decision-making of autonomous driving system, and are equally
    unable to avoid the new safety risks posed by AI.
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Perception Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The perception layer perceives and identifies things like object perceiving
    and identification, segmentation, depth estimation and localization, which are
    based on the vehicle’s state and road information collected by the sensors in
    the sensor layer. The commonly used techniques are given as follows, which include
    2D object recognition, 3D object recognition, multi-modal fusion, trajectory prediction,
    and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f586ee39c77a5c7424f1b827406a697f.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Sensor in Autonomous Driving'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '2D Objection Recognition is based on a flat image to identify the presence
    or absence of a specific target in the image and locate it. technologically, 2D
    object recognition can be divided into two classifications: two-stage objection
    recognition algorithms and one-stage objection recognition algorithms. The two-stage
    algorithms first find a series of region proposals, and then classify the objects
    in the proposals by Convolutional Neural Networks(CNN). Commonly used two-stage
    algorithms include FasterRCNN[[18](#bib.bib18)] and MaskRCNN[[19](#bib.bib19)]
    characterized by relatively high accuracy and high consumption. One-stage algorithms
    do not generate a separate region proposal but return to the predicted class and
    location of the target directly. Commonly used one-stage algorithms include: SSD[[20](#bib.bib20)]and
    Yolo v3[[21](#bib.bib21)]. In 2017, Lin et al.[[22](#bib.bib22)] proposed a new
    loss function - "Focal Loss", which can significantly improve the accuracy of
    dense target recognition, and this technique was first applied to the field of
    face recognition. It is now applied to many target recognition fields, among which,
    in 2021, Yosuke Shinya et al.[[23](#bib.bib23)] proposed UniverseNet, a target
    detection algorithm that applies Focal Loss, which can achieve better results
    in dense target and small target scenarios. A detailed comparison of current mainstream
    2D target recognition techniques can be found in references[[15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17)]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multi-Modal Fusion. A single type of sensor cannot capture all of the environmental
    information needed to support autonomous driving, while autonomous driving systems
    require information from several types and a large number of sensors to make integrated
    decisions, which leads us to make multi-modal fusion. Depending on occurred times[[24](#bib.bib24)],
    the fusion can be divided into three modes: pre-fusion, post-fusion, and deep
    fusion. Pre-fusion combines the data collected by all types of sensors and then
    makes a comprehensive decision. Post-fusion to make decisions on the data collected
    by different sensors and then aggregate the sub-decisions. Deep fusion constitute
    by the fusion of data, features and decision integration, and can be subdivided
    into five types: data in data out, data in feature out, feature in feature out,
    feature in decision out, and decision in decision out[[25](#bib.bib25), [26](#bib.bib26)].
    An in-depth analysis and comparison of the various integration methods can be
    found in the literature.[[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28),
    [26](#bib.bib26)]'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ae86a2cf5ce8b735bbabdd81e19deb7f.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (a) Pre-fusion
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59df5e78e80e8902394b6adfaf29392a.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (b) Post-fusion
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af86df09c3344896b35a0087373c3451.png)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: (c) Deep fusion
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 3: Fusion of Autonomous Driving'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '3D Objection Detection and Segmentation. Because 2D images have no depth information
    that is needed in autonomous driving, such as path planning and collision avoidance
    in autonomous driving, therefore 3D objection detection plays a key role. Classified
    by the detected information, 3D target detection has 3 bases: 2D image, 3D point
    cloud map and multi-modal fusion image. Among them, 3D target detection based
    on 2D images often uses 3D target matching and depth estimation to estimate the
    3D target bounding box for targets in 2D images using algorithms like Mono3D[[29](#bib.bib29)],
    3DVP[[30](#bib.bib30)], Deepmanta[[31](#bib.bib31)], and SVGA-Net[[32](#bib.bib32)].
    3D target recognition based on 3D point cloud maps is the recognition of targets
    in the images with 3D information and marks the target outline. Commonly used
    algorithms include: VeloFCN[[33](#bib.bib33)] , BirdNet[[34](#bib.bib34)], 3DFCN[[35](#bib.bib35)]
    , PointNet++[[36](#bib.bib36)] and VoxelNet[[37](#bib.bib37)]. 3D target detection
    based on multi-modal integration images is to use different integration modes
    to identify 3D targets. Commonly used algorithms include: MV3D[[38](#bib.bib38)],
    AVOD[[39](#bib.bib39)], and F-PointNet[[40](#bib.bib40)]. A comparison and in-depth
    study of various 3D target detection algorithms can be found in the literature[[41](#bib.bib41),
    [42](#bib.bib42)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Other deep learning research directions in the perception layer include Pedestrian
    Detection, Lane Detection, Traffic Sign Recognition, Pedestrian Attribute Recognition,
    Fast Vehicle Detection, Pedestrian Density Estimation, Plate Recognition, etc.
    There is detail on the leaderboard[[43](#bib.bib43)].
  prefs: []
  type: TYPE_NORMAL
- en: 1.5 Decision-Making Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Driving decision-making is the core of autonomous driving, and machine learning
    methods are often used, with two technical routes available: Imitation Learning
    and Reinforcement Learning.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Imitation Learning. Imitation learning refers to the learning behavior of agents
    who acquire the ability to perform a specific task by observing and imitating
    the behavior of human experts[[44](#bib.bib44)]. Imitation learning has been successful
    in the field of autonomous driving[[45](#bib.bib45)] Imitation learning tends
    to collect a large amount of environmental state $S_{i}$ (environmental data collected
    by various sensors, including 3D point cloud maps, RGB images, etc.) as features
    and record the actions performed by the human experts at the same time. $A_{i}$
    is used as a label to form a training data set $D:{(s1,a1),(s2,a2),(s3,a3),...}$.
    Using specific imitation learning algorithms, artificial intelligence models are
    trained and used to make future driving decisions. The famous imitation learning
    methods include the E2E autonomous driving algorithm based on conditional imitation
    learning [[46](#bib.bib46)], and the ChauffeurNet[[47](#bib.bib47)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep Reinforcement Learning. Deep reinforcement learning simulates the self-learning
    model of organisms in nature. To be concrete, an agent monitors its own behavior
    and the resulting environmental changes, sets the reward value for different changes,
    and then continuously optimizes the model and its own behavior based on this.
    In 2013, Mnih et al.[[48](#bib.bib48)] combined deep learning with reinforcement
    learning and proposed the Deep Q Learning(DQN) method. DQN is based on a set of
    Q values in a reward table. The system’s driving status $S_{i}$ and the driving
    operation $a_{i}$ to obtain the corresponding reward value $r_{i}$, which automatically
    generates training data $D:{((s1,a1),r1),((s2,a2),r2),((s3,a3),r3),...}$. The
    reinforcement learning model is then trained by specific algorithms, while reinforcement
    learning is supplemented with current operational data to continuously optimize
    the model. Nowadays, deep reinforcement learning has been rapidly developed and
    widely used, with subsequently emerged Deep Recurrent Q Networks (DRQNs)[[49](#bib.bib49)],
    attention mechanism deep recurrent Q networks[[50](#bib.bib50)], asynchronous/synchronous
    dominant actor-critic (A3C/A2C)[[51](#bib.bib51)], and reinforcement learning
    for unsupervised and unassisted tasks[[52](#bib.bib52)], which are widely used
    in e-Sports, health & medicine, recommendation system and other fields. There
    are some surveys of deep reinforcement learning[[53](#bib.bib53), [54](#bib.bib54)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A variety of deep reinforcement learning frameworks and algorithms are widely
    used in the field of autonomous driving vehicles. For example, Feng et al.[[55](#bib.bib55)],
    Alizadeh et al.[[56](#bib.bib56)], Mirchevska et al.[[57](#bib.bib57)], and Quek
    et al.[[58](#bib.bib58)] apply deep reinforcement learning techniques to driving
    decisions; Holen et al.[[59](#bib.bib59)] use deep reinforcement learning for
    autonomous driving roadway recognition; Feng et al.[[60](#bib.bib60)] utilize
    deep reinforcement learning techniques for traffic light optimization control.
    Some researchers have also proposed an autonomous driving solution with the fusion
    of imitation learning and reinforcement learning[[61](#bib.bib61), [62](#bib.bib62)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.6 Vehicle Networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the development of communications and AI technology, vehicle networks are
    increasingly playing an important role in autonomous driving, especially the vehicle
    networks construction, which supports a distributed AI model and provides a novel
    type of AI technology in autonomous driving, while also bringing new security
    risks.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vehicle-to-Everything (V2X). V2X is a multi-layered network system designed
    to enhance collaboration between pedestrians, vehicles and transport infrastructure.
    It is universally composed of Vehicle-to-Vehicle (V2V) networks, Vehicle-to-Infrastructure
    (V2I) networks, Vehicle-to-Pedestrian (V2P) networks and Vehicle-to-Road side
    units (V2R) networks[[63](#bib.bib63)]. The communication technologies used in
    the vehicular internet of things can be broadly classified into two categories,
    Dedicated Short Range Communication (DSRC) and Long-Term Evolution (LTE) cellular
    communication, called cellular-V2X or C-V2X for short[[64](#bib.bib64)].
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Federated Learning. The vehicular internet of things provides the network foundation
    for distributed artificial intelligence. Federated Learning is a distributed AI
    framework that replaces sensitive data interactions with model interactions, enabling
    more efficient and better privacy for knowledge sharing and transition. Based
    on the V2X, the federated learning can provide distributed and interactive AI
    services[[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67)] for autonomous
    driving system. This paper focuses on the novel security risks posed by Federated
    Learning in the vehicular internet of things, and reviews related security technology
    developments.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 1.7 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We concluded the major AI application used in autonomous driving in Table1
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Major Deep Learning-based Tasks in Autonomous Driving'
  prefs: []
  type: TYPE_NORMAL
- en: '| Layer | Task | Major typical deep learning algorithm |'
  prefs: []
  type: TYPE_TB
- en: '| Sensor | 3D PointCloud Registration | 3DFeat-Net[[68](#bib.bib68)], FCGF[[69](#bib.bib69)],
    D3Feat-pred[[70](#bib.bib70)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pre-Fusion | Multi-Frame Fusion[[71](#bib.bib71)], MTF4VT[[72](#bib.bib72)],TransFuser[[73](#bib.bib73)],DeepFusion[[74](#bib.bib74)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Perception | 2D Object Detection | Fast-RCNN[[75](#bib.bib75)], Faster R-CNN[[75](#bib.bib75)],
    Mask R-CNN[[19](#bib.bib19)], D-RFCN[[76](#bib.bib76)], Yolov4[[77](#bib.bib77)],
    YOLOv7[[78](#bib.bib78)],FD-SwinV2[[79](#bib.bib79)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | 3D Object Detection | PointRCNN[[80](#bib.bib80)], PV-RCNN[[81](#bib.bib81)],Se-SSD[[82](#bib.bib82)],
    GLENet-VR[[83](#bib.bib83)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Lane Detection | SCNN[[84](#bib.bib84)] LaneATT[[85](#bib.bib85)] CLRNet[[86](#bib.bib86)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Traffic Sign Recognition | CNN with 3 Spatial Transformer[[87](#bib.bib87)],
    Mask R-CNN with adaptations and augmentations[[19](#bib.bib19)], TSR-SA[[88](#bib.bib88)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fast Vehicle Detection | YOLOv3-tiny[[89](#bib.bib89)], LittleYolo-SPP[[90](#bib.bib90)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Pedestrian detection | SA-FastRCNN[[91](#bib.bib91)],RPN+BF[[92](#bib.bib92)],Pedestron[[93](#bib.bib93)],
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Semantic Segmentation | FCN[[94](#bib.bib94)], PSPNet[[95](#bib.bib95)],
    DRAN[[96](#bib.bib96)],Swin trasformer[[97](#bib.bib97)], ViT-Adapter[[98](#bib.bib98)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Object Tracking | M2-Track[[99](#bib.bib99)],BAT[[100](#bib.bib100)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Multiple Object Tracking | QDTrack[[98](#bib.bib98)], RetinaTrack[Lu2020retinatrack]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Decision | Trajectory Prediction | NSP-SFM[[101](#bib.bib101)], Y-Net[[102](#bib.bib102)],Trajectron++[[103](#bib.bib103)],Social
    GAN[[104](#bib.bib104)],SoPhie[[105](#bib.bib105)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Motion Forecasting | VI LaneIter[[106](#bib.bib106)], Wayformer[[107](#bib.bib107)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Deep Reinforcement Learning | Deep Q-Learning[[108](#bib.bib108)]),Deep
    recurrent q-learning[[49](#bib.bib49)],Deep attention recurrent Q-network[[50](#bib.bib50)],Double
    Q-learning[[109](#bib.bib109)],A3C/A2C[[51](#bib.bib51)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Imitation Learning | Generative adversarial imitation learning[[110](#bib.bib110)],
    Conditional Imitation Learning[[46](#bib.bib46), [111](#bib.bib111)], Self-Imitation
    Learning[[112](#bib.bib112)], Chauffeurnet[[47](#bib.bib47)] |'
  prefs: []
  type: TYPE_TB
- en: '| V2X | Federated Learning | FedAvg[[113](#bib.bib113)] |'
  prefs: []
  type: TYPE_TB
- en: 2 Emerging Threats in Sensors
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Sensors are foundational part for the autonomous driving system, which provide
    raw environmental information for autonomous driving decision-making. The security
    of sensors directly affects the safety of autonomous driving system. We classify
    attacks against sensors into two categories, where attacks that aim to compromise
    the usability of the sensing are classified as Jamming Attacks and attacks that
    aim to compromise the integrity of the information collected by the sensors are
    classified as Spoofing Attacks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Jamming Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Jamming Attack means that attackers take some actions to reduce the quality
    of data collected by the sensor, even making sensor unavailable. In 2015, Petit
    et al.[[114](#bib.bib114)] attempted a jamming attack on autonomous driving sensors
    by artificially setting up bright light interference that could "blind" the camera.
    In 2016, Yan et al.[[115](#bib.bib115)] experimented with blind attacks on ultrasonic
    sensors. Similarly, a variety of in-vehicle sensors such as RGB cameras, LiDAR,
    RaDAR, gyroscopic sensors and GPS sensors could be subject to jamming attacks[[116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Spoofing Attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Spoofing Attacks means that attackers injecting fake signals to affect the
    normal behaviour of the autonomous driving system. In 2015, Petit et al.[[114](#bib.bib114)]
    attempted to send specific spoofed laser signals, causing the LiDAR systems to
    be misled. Later, Park et al.[[120](#bib.bib120)] conducted similar experiments
    on in-vehicle IR sensors. Yan et al.[[115](#bib.bib115)] worked on gyroscopic
    sensors and RaDAR. Nassi et al.[[121](#bib.bib121)] conducted combined experiments
    on RGB cameras, LiDAR and RaDAR. Psiaki et al.[[122](#bib.bib122)], Meng et al.[[123](#bib.bib123)]
    conducted spoofing experiments on GPS for multiple environments.
  prefs: []
  type: TYPE_NORMAL
- en: Currently, most attacks against sensors of vehicle are trend towards physical
    attack rather than attack on deep learning. In this paper, we only give a general
    overview, there are more details in the surveys[[17](#bib.bib17), [15](#bib.bib15)].
  prefs: []
  type: TYPE_NORMAL
- en: 3 Emerging Threats in Perceptual Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Based on the information captured by various types of sensors in the sensor
    layer, the perception layer performs recognition and perception. These tasks,
    such as objective recognition, segmentation, and depth estimation, are often difficult
    to accomplish through simple computing based on some certain rules. Artificial
    intelligence is also subject to new types of security threats. For example, attackers
    could use Adversarial Examples or AI Backdoor attacks, which can mislead to wrong
    predictions that be controlled by attackers, which leads to dangerous driving
    decisions. Attackers may also use Model Extraction to obtain the parameters or
    Hyper-parameters of the AI model, resulting in model leakage and loss of intellectual
    property. And then attackers would use Model Inversion or Membership Privacy attacks
    leading to sensitive training data leakage and privacy risks.
  prefs: []
  type: TYPE_NORMAL
- en: Different from some existing surveys that introduce general adversarial examples
    or AI backdoors in cyberspace, this paper focuses on advanced research in the
    physical world. Attacking in the physical world has faced higher demands, especially,
    on attack constancy, high success rate, and robustness on environmental uncertainties.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Physical World Adversarial Examples for Static Objective Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In 2014, researchers discovered that adding a small amount of specific interference,
    which is imperceptible to human beings, may still cause machine learning to be
    misled by attackers. This could cause serious security even safety problem, if
    machine learning be applied to a critical domain. Such an attack is known as adversarial
    examples attack and can be formalized as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $argmin_{x^{\prime}}\left\&#124;x^{\prime}-x\right\&#124;_{p}s.t.f(x^{\prime})=\hat{y}$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $f$ denotes a machine learning model, $x$ denotes a test example, and
    $x^{\prime}$ denotes an adversarial example generated based on the addition of
    a small amount of interference, with $c$ for the prediction result of the model
    for a normal example, and $c^{\prime}$ for the prediction result of the model
    on the adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'After the first work, adversarial examples technique has been widely studied
    and has seen rapid development. FGSM algorithm proposed by Goodfellow et al.[[2](#bib.bib2)]
    FGSM works by calculating the gradient of the loss function between the input
    and target classification and creating a small perturbation in terms of the sign
    vector coefficient of this gradient as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{Adv}=x+\alpha sign(\triangledown_{x}J(x,y))$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $x_{adv}$ denotes the corresponding adversarial example of $x$, $\alpha$
    is a specific constant, $sign()$ is a sign function, $y_{true}$ is the the corresponding
    true label of $x$, $J()$ denotes the loss function used to train the model, and
    $\triangledown_{x}$ denotes the the gradient of $x$. The algorithm can be used
    to add a certain amount of adversarial noise to an image that is normally predicted
    as a panda, so that the machine learning model can predicted it as a gibbon with
    a high confidence. This algorithm could achieve both untargeted attacks and targeted
    attacks. There are more concerned adversarial examples algorithms be proposed
    in following. In 2016, Papernot et al.[[124](#bib.bib124)] proposed a black-box
    attack using an alternative model approach. In 2017, Moosavi-Dezfooli et al.[[125](#bib.bib125)]
    proposed an universal adversarial example attack, where a particular adversarial
    interference is able to influence the classification of multiple or even all examples
    in machine learning. In the same year, Carlini et al.[[126](#bib.bib126)] proposed
    an optimization-based C&W algorithm to improve the adversarial examples attack.
    In 2018, Zhao et al.[[127](#bib.bib127)] found that not need to be filled with
    artificial interference, but found different distribution in nature and can cause
    misclassification of machine learning models, which is called Natural Adversarial
    Examples.
  prefs: []
  type: TYPE_NORMAL
- en: In the real physical world, the environment of autonomous driving is more complex,
    so there are more challenges for attacker to generate adversarial examples in
    the physical-world[[128](#bib.bib128)]. The requirements of adversarial examples
    in the physical world are as follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical generatible. In the physical world, it is insufficient that only adding
    perturbations via cyber space. Such perturbations must be capable of being physically
    generated by printing, 3D printing or spraying, etc.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local generatible. The local nature of the adversarial example. In an adversarial
    example of the digital world, the attacker can add perturbations to any pixel
    within the range of the image; however, in a physical world attack, there are
    often only local areas of the target that are available, and in many cases, the
    background areas of the image are difficult to use to generate a physical world
    adversarial example.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Robustness. In the physical world, especially in the field of autonomous driving,
    it is often required that the adversarial examples can continuously produce misleading
    effects on machine learning models during multiple angle changes. At the same
    time, the adversarial examples need to be continuously effective against mainstream
    target recognition algorithms under certain distance and angle ranges, multiple
    natural environments, and multiple resolution sensor devices. This puts forward
    higher requirements on the persistence and universality of adversarial examples,
    which greatly increases the complexity of the generation of adversarial examples.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Therefore, more adversarial example robustness enhancement measures are often
    required to realize the physical world adversarial example attacks; and the specific
    measures applied to vary according to different scenarios and attack targets.
    In accordance with the scenarios and objects for which countermeasures are set,
    this paper divides the recognition targets in the autonomous driving system into
    three categories: vehicles including various motor vehicles, pedestrians such
    as walkers and cyclists, and static targets like road facilities, traffic signs,
    markings, roadside advertising signs and other static objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Thus, at present, the physical world in the autonomous driving field universally
    has 3 types of targets for adversarial example techniques: static objective recognition,
    pedestrian recognition, and vehicle recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical world adversarial examples for static targets. Such attack targets
    include a variety of static target recognition systems such as traffic signs,
    traffic signals, and traffic markings, which are characterized by the requirement
    that the adversarial examples can continuously and steadily interfere with the
    judgments of machine learning models over a large range of angles and distances.
    In 2017, Lu et al.[[129](#bib.bib129)] successfully performed adversarial example
    generation in the physical world for the popular objective recognition algorithm
    FasterRCNN. To achieve better distance and angle range adaptation, in 2018, Eykholt
    et al.[[130](#bib.bib130)] proposed the Robust Physical Perturbations (RP2) algorithm.
    In the same year, Chen et al.[[131](#bib.bib131)] adopted the Expectation over
    Transformation (EoT) method[[132](#bib.bib132)] to improve the generation of adversarial
    examples for traffic signs, resulting in improved adaptability of the adversarial
    examples to distance, angle, light and other environments. In 2019, Zhao et al.[[133](#bib.bib133)]
    proposed the feature-interference reinforcement (FIR) algorithm and the realistic
    constraints generation (ERG) algorithm to enhance the robustness of the adversarial
    examples. At the same time, they proposed the nested-AE algorithm to improve the
    adaptability of the adversarial examples to long and short distances. Finally,
    the composite scheme is able to success attack against popular objective recognition
    algorithms, such as YOLO v3 and Faster-RCNN, within $\pm 60^{\circ}$ angle and
    $[1m,25m]$ distance range. Based on the above methods, hiding attacks and appearing
    attacks can be carried out. The hiding attack is to paste an adversarial example
    on a normal traffic sign to make the target recognition system fail to recognize
    the traffic sign. The appearing attack to paste an adversarial example on other
    objects, causing the target recognition system to recognize the object as a characteristic
    traffic sign or make a false recognition. In 2020, Kong et al.[[134](#bib.bib134)]
    proposed PhysGAN, a physical world adversarial examples attack method based on
    adversarial example generative networks, and the generated adversarial examples
    of advertising signs have better robustness and invisibility.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A physical world adversarial example for pedestrian recognition. In autonomous
    driving system, a missed detection of pedestrians by the objective recognition
    system can have serious consequences. In 2020, Wu[[135](#bib.bib135)] proposed
    the "invisibility cloak" algorithm, where pedestrians are not normally detected
    by Yolo v2 and Yolo v3 objective detection models when wearing sprayed adversarial
    example clothing. In the same year, Wang et al.[[136](#bib.bib136)] conducted
    a similar study.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Physical world adversarial examples for vehicle recognition. These examples
    are often pasted or painted on the vehicle body with a specific pattern, so that
    the vehicle detection system can not identify the vehicle or mistakenly identify
    the vehicle as other objects. Such physical world adversarial examples try to
    maintain attack effectiveness under high speed movement, various light and other
    external conditions, especially in the 360-degree view and within the detection
    range of vehicle identification system, which puts forward higher requirements
    for the robustness of adversarial samples. At the same time, at different angles,
    the camera may only be able to acquire part of the images of he adversarial example
    in the vehicle body, which in turn places a local requirement on the adversarial
    example, i.e., part of the adversarial example can also achieve the attack. In
    2019, Zhang et al.[[137](#bib.bib137)] proposed a vehicle body painting method
    of black box adversarial example based on transition models so that the example
    vehicle could not be identified by autonomous driving vehicle detection system.
    In 2020, Wu et al.[[135](#bib.bib135)] proposed the discrete searching algorithm
    to efficiently generate adversarial patches, and then proposed the Enlarge-and-Repeat
    (ER) algorithm to extend the adversarial patches to the whole body using body
    images collected from all angles. Both are with pretty good adversarial results.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The following highlights the key algorithms in the physical world adversarial
    example enhancement described above.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm1\. Expectation over Transformation (EoT) [[132](#bib.bib132)]
  prefs: []
  type: TYPE_NORMAL
- en: 'The core idea of the EoT algorithm is to add a certain random perturbation
    to each iteration of the adversarial example generation process, so that the final
    generated adversarial example has better robustness, with specific transformations
    including: projecting, rotation, and scaling. In the formula, the operation $M_{t}(x_{b},x_{o})$
    is defined to project the target image $x_{o}$ onto the background image $x_{b}$
    through some transformation $t$, and then the EoT is optimized as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}\hat{p}=&amp;\arg\min_{x^{\prime}\in\mathbb{R}^{h\times
    w\times 3}}\mathbb{E}_{x\sim X,t\sim T}[L(F(M_{t}(x,tanh(x^{\prime}))]\\ &amp;+c\cdot\left\&#124;tanh(x^{\prime})-x\right\&#124;\end{split}$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $X$ denote the training set of background images, and $F$ denote the target
    network.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm2\. Adversarial Patch [[138](#bib.bib138)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the EoT transform, the attacker could generate an adversarial patch
    $\hat{p}$ which the image with the this adversarial patch attacked by adversarial
    examples. The adversarial patch can be any shape, transformed by EoT methods such
    as random projecting, rotation, and scaling, and then generated by optimization
    methods such as gradient descent. An adversarial patch generation task $A(p,x,l,t)$
    can be formally described as: for any particular $x\in\mathbb{R}^{w\times\ h\times
    c}$ to generate adversarial patch $p$ through the EoT transformation $t$ at position
    $l$, then the adversarial path $p$ is continuously optimized by the following
    optimization algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\hat{p}=\arg\max_{x\in\mathbb{R}^{h\times w\times c}}\mathbb{E}_{x\sim
    X,t\sim T,l\sim L}[\log Pr(\hat{y}&#124;A(p,x,l,t))]$ |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: where $X$ denotes the training set of background images, $T$ denotes the distribution
    of the EoT transform used by the patch, and $L$ denotes the distribution of the
    locations of the adversarial patches.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm3\. Feature-inference Reinforcement[[133](#bib.bib133)]
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial example generation algorithms often require an objective function,
    or called it as loss function, designed to minimize the difference between the
    predicted and expected values of a deep learning model. The neural network extracts
    features of the objects in the image and makes classification predictions based
    on these extracted features. The researchers found that generating adversarial
    examples with perturbations further forward in the hidden layer in the neural
    network would make the adversarial examples more robust. The core idea of the
    FIR algorithm is therefore to minimize the difference between the feature images
    of the layers of the adversarial examples and the normal examples, except for
    the use of the adversarial examples to mislead the prediction results of the neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: First, attacker obtain the feature images $Q_{n}$ and $Q^{\prime}_{n}$ generated
    by each hidden layer of the neural network for corresponding categories $y$ and
    $y^{\prime}$. Next, generate the feature vectors $v$ with $v^{\prime}$ from the
    feature images $Q_{n}$ and $Q^{\prime}_{n}$. Finally, attacker optimize with the
    a composited loss function until convergence, then return the adversarial example.
    Such a vector loss can be defined as $Loss_{f}=\sum\left|v-v^{\prime}\right|$.
    The FIR algorithm can be described formally as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: normal example $x$, target objective detection model $f$Output: adversarial
    example $x^{\prime}$12while *until convergence*  do3      4      foreach *$i$th
    hidden layer in $f$*  do5             $q_{i}\leftarrow f(x)$ //get feature map
    $q_{i}$ of normal examples $x$ with prediction $y$ ;6            7            $q^{\prime}_{i}\leftarrow
    f(x^{\prime})$ // get feature map $q^{\prime}_{i}$ of adversarial examples $x^{\prime}$
    with prediction $y^{\prime}$ ;8            9       end foreach10      $v\leftarrow
    Q:\left\{{q_{1},q_{2},...,q_{n}}\right\}$ // generate normal feature vector from
    feature maps11      $v^{\prime}\leftarrow Q^{\prime}:\left\{{q^{\prime}_{1},q^{\prime}_{2},...,q^{\prime}_{n}}\right\}$
    // generate adversarial feature vector from feature maps12      Optimize $x^{\prime}$
    to minimize the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\alpha C_{N}^{box}+\beta p_{N}(y^{N}&#124;S)+c(loss_{f})^{-1}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 13 end while14
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 1 3\. Feature-inference Reinforcement
  prefs: []
  type: TYPE_NORMAL
- en: where $C_{N}^{box}$ denotes the confidence level of the target detection system
    for the target area; $y^{N}$ denotes the confidence level that example $x$ is
    judged by the neural network to be class $N$; $S$ denotes the distribution space
    of confidence levels; $loss_{f}$ denotes the vector loss; $\alpha$, $\beta$ and
    $c$ are three constants representing the weights respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm4\. Nested adversarial examples (Nested-AE)[[133](#bib.bib133)]
  prefs: []
  type: TYPE_NORMAL
- en: Most objective detectors were designed to use multi-scales, each works better
    at different distances individually. It means that at different distances, different
    scales play different roles. In order to make the generated adversarial examples
    achieve adversarial effects over a large distance range and multiple angles, the
    Nested-AE algorithm considers many scales for different distances and angles to
    generate many adversarial patches. Then Nested-AE obtained the adversarial patches
    to synthesize an adversarial example. Thus nested adversarial examples are more
    adapted to the environment of vehicle movement in autonomous driving, which can
    achieve the purpose of a continuous adversary effect on the objective detection
    system. The nested adversarial examples could be formally described as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X^{adv}_{i+1}=Clip\begin{Bmatrix}X_{i}+\varepsilon sign(J(X_{i})),&amp;S_{P}\leq
    S_{thres}\\ X_{i}+\varepsilon M_{center}sign(J(X_{i})),&amp;S_{P}>S_{thres}\end{Bmatrix}$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $X_{i}$ denotes the original example, $X^{adv}_{i+1}$ denotes the example
    with adversarial perturbation, $J()$ represents the gradient of input $X_{i}$
    and $Clip()$ refers to regularize the input to the $[0,255]$ interval.
  prefs: []
  type: TYPE_NORMAL
- en: If the adversarial example size $S_{p}$ is less than or equal to the threshold
    $S_{thres}$ , then it is considered a long-range adversarial attack at that point
    and the whole will be perturbed ; otherwise it is a close-range attack and only
    the central region will be perturbed. This decomposes the adversarial attack at
    different distances into two sub-tasks, which are perturbed and optimized separately.
  prefs: []
  type: TYPE_NORMAL
- en: The objective detection systems usually divides each video frame into a grid
    that consisted of $m\times n$ boxes. Base on the prediction of each box, attacker
    could find the decisive box for each scales, and then add the adversarial perturbation
    in this box. As the predicted output is usually tensors, there are only need the
    tensor of the index where the adversarial example is located. Therefore, it can
    calculate the index by the size of the example and the position of the centre
    region, so that we can get the tensor representing the example region, which is
    denoted as $N_{p}$. Then $N_{p}$ needs to be calculated in each video frame. The
    nested adversarial example algorithm can be optimized using the following loss
    function.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $N_{p}=f(p_{size},P_{position}),1-C^{box}_{N_{p}}+\beta\sum\left&#124;p_{N_{p}},j-y_{j}\right&#124;^{2}$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 3.2 Physical World Adversarial Example for Pedestrian Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Pedestrian detection often places high demands on the universality, portability,
    robustness and feasibility of the adversarial examples. To due this problem, some
    algorithms was proposed. As a typical example, Invisibility Cloak algorithms[[135](#bib.bib135)]
    can generate adversarial examples in the physical world, so that pedestrians wearing
    clothes painted with specific adversarial examples cannot be recognized properly.
    If the similar method be used in pedestrian, serious consequences of autonomous
    driving system will be lead.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 5\. Invisibility Cloak[[135](#bib.bib135)]
  prefs: []
  type: TYPE_NORMAL
- en: The policy of the invisibility cloak algorithm is to use a large number of images
    containing people to train against patches; in each iteration, a random batch
    of images is selected and sent to the objective detection system to obtain the
    bounding box of people. The critical idea is that place a randomly transformed
    patch on each detected person so that the score that feature images are detected
    to have people’s presence is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: The patch $P\in\mathbb{R}^{w\times h\times 3}$ is projected to the target image
    $I$ by transformation function $R_{\theta}$ which performs data augmentation for
    lighting, contrast changes, distortion, with $\theta$ as the parameter, in addition
    to scaling the patch to fit the size of image $I$.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, two effective advanced methods for physical world attack were
    proposed as auxiliary. One was called Total-Variation (TV) loss. Add a TV penalty
    function to the patch to make adversarial example smoother to improve robustness
    of physical world adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'with the final loss function as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{obj}(P)=loss+\gamma\cdot TV(P)$ |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: Another is Ensemble training. In the black-box case, the attacker cannot obtain
    the gradient of the target detection model under attack, so a possible solution
    is to collect multiple similar white-box models for ensemble training. The loss
    function is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{ens}(P)=\mathbb{\theta,I}\sum_{i,j}\left\{S_{i}^{(j)}(R_{\theta}(I,P))+1,0\right\}$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $S_{(j)}$ represents the objective detection model of target $j$. It is
    formally described as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: normal example $x$, target objective detection model $f$Output: adversarial
    example $x^{\prime}$12$I=P(x)$ // generate projection3 ;45$R_{\theta}\leftarrow\theta$
    // define a transfer function $R_{\theta}$6 ;78while *until convergence*  do9      10      Optimize
    $x^{\prime}$ to minimize the loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{obj}(P)=R_{\theta}(I,P)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '11 end while1213or Auxiliary1: TV Loss'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{obj}(P)=R_{\theta}(I,P)+\gamma\cdot TV(P)$ |  |'
  prefs: []
  type: TYPE_TB
- en: '14or Auxiliary2: Ensemble training'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{obj}(P)=\mathbb{E}_{\theta,I}\sum_{i}\max\left\{{S_{i}(R_{\theta}(I,P))+1,0}\right\}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Algorithm 2 5\. Invisibility Cloak
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Physical World Adversarial Example for Vehicle Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The aim of the adversarial examples for vehicle detection is evade or mislead
    other vehicle detector by adversarial spraying. Different from other scenarios,
    the challenge is that the spraying need works for the detector at all angles.
    Discrete Searching is a typical and effect algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 6\. Discrete Searching[[139](#bib.bib139)]
  prefs: []
  type: TYPE_NORMAL
- en: The discrete searching algorithm is essentially a black-box adversarial example
    generation algorithm based on a genetic algorithm that continuously optimizes
    the adversarial examples through mutation and selection. The discrete searching
    algorithm that mutation-based search method defines two mutation strategies. One
    is random mutation, where a point within the circle with $\epsilon$ as the radius,
    is randomly selected as the direction and advanced a step length as the new mutation
    point. If the current mutation optimization fails to outperform the original one,
    the choice will be made to continue with the random mutation. The other is directed
    mutation, in which the candidate’s best mutation point is selected within a particular
    angular expansion of the current direction, advancing a random step length. If
    the current variation outperforms the original one, the choice will be made to
    continue with the directed mutation.
  prefs: []
  type: TYPE_NORMAL
- en: '12Input: normal example $x$, target objective detection model $f$Output: adversarial
    example $x^{\prime}$12while *until convergence*  do3       foreach *$i\in\left\{0,...,N_{a}-1\right\}$*
     do4            5            $C_{i}^{j_{w}}$, $C_{i}^{j_{w}}=Clip(C_{i}+Random(H,W,3)\cdot\epsilon_{1}\cdot\delta)$
    //generate candidate point6            select $\hat{C}_{i}$ in $C_{i}^{j_{w}}$7            if *$\hat{C}_{i}$
    is better than $C_{i}$* then8                   $C_{i}=\hat{C}_{i}$9            else10                  
    $C_{i}$ remains unchanged11             end if12            13       end foreach14      15
    end while'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 3 6\. Discrete Searching
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Adversarial Examples on LiDAR and RaDAR
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Adversarial examples are related to the characteristics of deep learning itself,
    and both RGB image-based and LiDAR- or RaDAR-based objective recognition systems
    are likely to suffer from adversarial examples attacks. In 2019, Cao et al.[[140](#bib.bib140)]
    proposed an adversarial examples attack method for LiDAR target recognition. The
    attacker emits a small amount of perturbation laser at the LiDAR system, which
    led to a small perturbation in the imaging of the LiDAR system, and such perturbation
    made the LiDAR-based 3D objective recognition erroneous. Then, an adversarial
    object attack method LiDAR-Adv was proposed for LiDAR objective recognition, in
    which the attacker could construct a certain special shape of objects, causing
    the LiDAR objective recognition system to mispredict special objects. Such an
    attack can be targeted, i.e., the real object is recognized as the specified by
    the attacker, so this is more easier to exploit for the attacker. As the figure
    shows, some specially shaped objects can be misidentified as "pedestrians" by
    the 3D objective recognition system, while others are not recognized properly
    by the 3D objective recognition system, posing a security risk. The above method
    was successfully tested on Baidu’s autonomous driving system Apollo. In 2020,
    after research and improvement, SUN et al.[[141](#bib.bib141)] implemented a black-box
    attack of the above method, which was successfully tested on Intel’s autonomous
    driving simulation system Carla. In real autonomous driving environments, the
    detection of dynamic targets often takes a multi-modal fusion of RGB images, LiDAR,
    and RaDAR for target recognition, which improves the robustness of the system
    to some extent.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c8f28ae95660a4cee5405f31cb02f1c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Adversarial Examples on LiDAR[[140](#bib.bib140)]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Higher demands are placed on the adversarial example for multi-modal environments,
    mainly in terms of:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial perturbation needs to be able to be physically generated in both
    the RGB image and the LiDAR system environment. Traditional RGB adversarial examples
    usually change the RGB values of some pixels in the image, however, this method
    cannot works on the 3D cloud point map generated from the LiDAR. On another hand,
    emit a specific adversarial laser at LiDAR can interfere with the LiDAR system,
    but it is also difficult to effectively influence the RGB objective recognition
    system.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The adversarial examples need to be able to physically and continuously work
    on both the RGB-based system and the LiDAR-based system. In a real vehicle environment,
    the RGB system and the LiDAR system need to attack successfully and continuously
    at a long distance and at different angles.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial examples need to be able to adapt to different data preprocessing
    between RGB-based systems and LiDAR-based systems. RGB image acquisition system
    and LiDAR data acquisition system both have certain data preprocessing, which
    will have impact on adversarial examples. The algorithm of adversarial examples
    generation needs to have strong robustness to different preprocessing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Meanwhile, after optimized by some specific algorithms , the threat to the fused
    objective recognition system by adversarial examples is still exists. In 2021,
    Cao et al.[[142](#bib.bib142)] proposed the MSF-ADV method with LiDAR and RGB
    image fusion environment as an example, and successfully implemented the physical
    world adversarial examples.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 7\. MSF-ADV Algorithm[[142](#bib.bib142)]
  prefs: []
  type: TYPE_NORMAL
- en: To accommodate the above challenges, MSF-ADV first generates 3D objects of different
    shapes so that they can simultaneously affect the LiDAR-based 3D point cloud imaging
    and also the RGB colour values of the pixels in the RGB image. Secondly, MSF-ADV
    uses an optimization algorithm to generate the 3D shapes with the best adversarial
    effect. Finally, MSF-ADV uses a 3D printer for physical generation. The loss function
    of the optimization algorithm can be described as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $min_{S^{a}}\mathbb{E}_{t\sim T}[\mathcal{L}_{a}(t(S_{a});\mathcal{R}^{l},\mathcal{R}^{c},\mathcal{P},\mathcal{M})+\lambda\cdot\mathcal{L}_{r}(S^{a},S)]$
    |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $S$ denotes the original examples the $S_{a}$ denotes the adversarial
    examples, $\mathcal{M}$s denotes the fusion algorithm, and $\mathcal{R}^{c}$ is
    the derivative projection function used to represent RGB image based prediction.
    $\mathcal{R}^{l}$ is the derivative projection function used to represent the
    LiDAR prediction. $\mathcal{P}$ is the ultimate output of the objective recognition
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Through thus optimization, the trade-off between RGB colour and LiDAR shape
    was found.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Adversarial Examples on Object Tracking & Trajectory Prediction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Usually, autonomous driving system relies on object tracking and trajectory
    prediction, to determine and predict target states, and to support driving decisions.
    Object trajectory tracking can be divided into Single-Object Tracking (SOT) and
    Multi-Object Tracking (MOT). With the application of object tracking in critical
    cyber systems, adversarial examples attacks on it are also rising. Among them,
    the main purpose of the adversarial examples attack on SOT is to achieve objective
    evasion. In 2020, Chen et al.[[143](#bib.bib143)] proposed the one-shot adversarial
    attack, which only adds a weak perturbation to the initial frame in the video,
    and the tracked object may not be able to track the trajectory in subsequent frames.
    In the same year, Yan et al.[[144](#bib.bib144)] proposed the cooling-shrinking
    attack, which perturbs the object search area by adding specific adversarial noises,
    so that the tracker cannot identify the object and interrupted the trajectory
    tracking. In the next year, Jia et al.[[145](#bib.bib145)] proposed the IoU Attack,
    the idea of which is to reduce the fractional difference between the normal object
    border and the adversarial object border in object tracking, thus enabling the
    trajectory offset using SOT system.
  prefs: []
  type: TYPE_NORMAL
- en: The autonomous driving system more often uses MOT systems. An adversarial example
    attack on MOT may achieve both evasion and object obfuscation. In 2020, Jia et
    al.[[146](#bib.bib146)] generated an adversarial example on an autonomous driving
    object tracking system that minutely deviation the normal target identification
    bounding box of the attacked target in a specific direction, causing the tracker
    to assign the wrong velocity to the attacked trajectory, resulting in the target
    tracking system not being able to associate with the target properly, thus achieving
    an escape attack. In 2021, Lin et al.[[147](#bib.bib147)] proposed a new adversarial
    example scheme that mainly uses the "PullPush Loss" algorithm and "Center Leaping"
    algorithm. The scheme leads to object tracking system confuses, when objects cross
    each other.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 8\. Push-Pull Loss [[147](#bib.bib147)]
  prefs: []
  type: TYPE_NORMAL
- en: A video $V$ consists of a series of frames, which can be marked as $V=\left\{{I_{1},I_{2},...,I_{N}}\right\}$,
    The trajectories of target$i$ and $j$ are respectively $T_{i}=\left\{{O_{s_{i}}^{i}},...,O_{t}^{i},...,O_{E_{i}^{i}}\right\}$
    and $T_{j}=\left\{{O_{s_{j}}^{j}},...,O_{t}^{j},...,O_{E_{j}^{j}}\right\}$
  prefs: []
  type: TYPE_NORMAL
- en: The attacker’s target generates a series of adversarial frames $\hat{V}$, as
    $\hat{V}=\left\{{I_{1},...,I_{t-1},\hat{I}_{t},...,\hat{I}_{t+n-1},I_{t+n},...,I_{N}}\right\}$,
    such that from the moment of time $t$, an adversarial misdirection of the trajectory
    of $i$ and $j$ occurs. Then there is the formula, $\hat{T}_{i}=\left\{{O_{s_{i}}^{i},...,O_{t-1}^{i},O_{t}^{i},...,O_{t+n-1}^{j},O_{t+n}^{j},...,O_{e_{j}}^{j}}\right\}$
    where $O_{t}^{i}$ indicates the target identified as $i$ at the moment of time
    $t$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the PushPull loss function to optimize and realize:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}L_{pullpush}(a_{t-1}^{i},a_{t-1}^{j},feat_{t}^{i},feat_{t}^{j})\\
    =\sum_{k\in\left\{i,j\right\}}d_{feat}(a_{t-1}^{k},feat_{t}^{\widetilde{k}})-d_{feat}(a_{t-1}^{k},feat_{t}^{k})\end{split}$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $d_{feat}()$ denotes the cosine distance and $a_{t-1}^{i}$ and $a_{t-1}^{j}$
    represent the trajectory features of object $i$ and $j$ while $feat_{t}^{i}$ and
    denotes the features of object $i$ and object $j$. After continuous optimization,
    it make the adversarial feature $feat_{t}^{j}$ instead of $feat_{t}^{j}$ be classified
    as trajectory $k$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8d86380cf2a4742b6189ac1ecafc1a17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Push-pull Loss Function [[147](#bib.bib147)]'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 9\. Center Leaping [[147](#bib.bib147)]
  prefs: []
  type: TYPE_NORMAL
- en: With the PushPull loss function optimization described above, it is able to
    make the object tracking misled against attacks, which is still difficult to succeed
    when the difference between the two objects is large. The idea of the Center Leaping
    algorithm is to first mislead the objective recognition link so that the objective
    candidate box identified by the target recognition system is shifted towards the
    target to be misled, thus achieving a better attack success rate when there is
    a large deviation in the distance and size difference between the two tracked
    objects. The loss function is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_math_unparsed" alttext="\begin{split}L&amp;=min\sum_{k\in\left\{i,j\right\}}d_{box}(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &amp;=min\sum_{k\in\left\{i,j\right\}}d(cent(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;+min\sum_{k\in\left\{i,j\right\}}d(size(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;+min\sum_{k\in\left\{i,j\right\}}d(off(K(m_{t-1}^{\widetilde{k}},box_{t}^{k})))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd
    class="ltx_align_right" columnalign="right" ><mi >L</mi></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    >=</mo><mrow ><mi
    >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >n</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><munder ><mo
    movablelimits="false" >∑</mo><mrow ><mi
    >k</mi><mo >∈</mo><mrow
    ><mo >{</mo><mi
    >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><mrow
    ><msub ><mi
    >d</mi><mrow ><mi >b</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >x</mi></mrow></msub><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><mi >K</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><msubsup
    ><mi >m</mi><mrow
    ><mi >t</mi><mo
    >−</mo><mn >1</mn></mrow><mover
    accent="true" ><mi >k</mi><mo
    >~</mo></mover></msubsup><mo >,</mo><mrow
    ><mi >b</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em" >​</mo><msubsup
    ><mi >x</mi><mi
    >t</mi><mi >k</mi></msubsup></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left" ><mrow
    ><mo >=</mo><mi >m</mi><mi
    >i</mi><mi >n</mi><munder
    ><mo movablelimits="false" >∑</mo><mrow
    ><mi >k</mi><mo
    >∈</mo><mrow ><mo
    >{</mo><mi >i</mi><mo
    >,</mo><mi >j</mi><mo
    >}</mo></mrow></mrow></munder><mi >d</mi><mrow
    ><mo stretchy="false" >(</mo><mi
    >c</mi><mi >e</mi><mi
    >n</mi><mi >t</mi><mrow
    ><mo stretchy="false" >(</mo><mi
    >K</mi><mrow ><mo
    stretchy="false" >(</mo><msubsup ><mi
    >m</mi><mrow ><mi
    >t</mi><mo >−</mo><mn
    >1</mn></mrow><mover accent="true" ><mi
    >k</mi><mo >~</mo></mover></msubsup><mo
    >,</mo><mi >b</mi><mi
    >o</mi><msubsup ><mi
    >x</mi><mi >t</mi><mi
    >k</mi></msubsup><mo stretchy="false" >)</mo></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left" ><mrow
    ><mo >+</mo><mi >m</mi><mi
    >i</mi><mi >n</mi><munder
    ><mo movablelimits="false" >∑</mo><mrow
    ><mi >k</mi><mo
    >∈</mo><mrow ><mo
    >{</mo><mi >i</mi><mo
    >,</mo><mi >j</mi><mo
    >}</mo></mrow></mrow></munder><mi >d</mi><mrow
    ><mo stretchy="false" >(</mo><mi
    >s</mi><mi >i</mi><mi
    >z</mi><mi >e</mi><mrow
    ><mo stretchy="false" >(</mo><mi
    >K</mi><mrow ><mo
    stretchy="false" >(</mo><msubsup ><mi
    >m</mi><mrow ><mi
    >t</mi><mo >−</mo><mn
    >1</mn></mrow><mover accent="true" ><mi
    >k</mi><mo >~</mo></mover></msubsup><mo
    >,</mo><mi >b</mi><mi
    >o</mi><msubsup ><mi
    >x</mi><mi >t</mi><mi
    >k</mi></msubsup><mo stretchy="false" >)</mo></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left" ><mrow
    ><mo >+</mo><mrow
    ><mi >m</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >n</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><munder ><mo
    movablelimits="false" >∑</mo><mrow ><mi
    >k</mi><mo >∈</mo><mrow
    ><mo >{</mo><mi
    >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><mrow
    ><mi >d</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><mi >o</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi
    >f</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >f</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false" >(</mo><mrow
    ><mi >K</mi><mo
    lspace="0em" rspace="0em" >​</mo><mrow
    ><mo stretchy="false"
    >(</mo><msubsup ><mi
    >m</mi><mrow ><mi
    >t</mi><mo >−</mo><mn
    >1</mn></mrow><mover accent="true" ><mi
    >k</mi><mo >~</mo></mover></msubsup><mo
    >,</mo><mrow ><mi
    >b</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em" >​</mo><msubsup
    ><mi >x</mi><mi
    >t</mi><mi >k</mi></msubsup></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo stretchy="false"
    >)</mo></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{split}L&=min\sum_{k\in\left\{i,j\right\}}d_{box}(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &=min\sum_{k\in\left\{i,j\right\}}d(cent(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &+min\sum_{k\in\left\{i,j\right\}}d(size(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &+min\sum_{k\in\left\{i,j\right\}}d(off(K(m_{t-1}^{\widetilde{k}},box_{t}^{k})))\end{split}</annotation></semantics></math>
    |  | (11) |'
  prefs: []
  type: TYPE_NORMAL
- en: Of which $m_{t}^{k}$ and $box_{t}^{k}$ respectively represent the trajectory
    state and the candidate frame of target $k$ at time $t$; $cent()$, $size()$, and
    $off()$ respectively represent the centre point position, the size, and the offset
    of the candidate box; and $d()$ represents distance $L_{1}$.
  prefs: []
  type: TYPE_NORMAL
- en: The central leaping algorithm can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}L_{cl}=\sum_{k\in\left\{i,j\right\}}(&amp;\sum_{(x,y)\in
    B_{c->\widetilde{k}}}(1-M_{x,y}^{\gamma}log(M_{x,y})+\\ &amp;\sum_{(x,y)\in B_{c->k}}(M_{x,y}^{\gamma}log(1-M_{P}{x,y})))\end{split}$
    |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: $M(x,y)$ denotes the heat value of $(x,y)$, $c_{k}$ denotes the center of the
    object, $c\rightarrow\widetilde{k}$ denotes the direction from $c_{k}$ to $cent(K(m^{\widetilde{k}}_{t-1}))$.
    During the optimization process, the center point will move to the adjacent grid
    along this direction. In the object recognition and tracking system, the heat
    value of the original target center will drop, and the heat value in the direction
    close to the object will rise, so as to achieve the goal of the candidate frame
    approaching the object.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, it is able to integrate the size loss function and the offset loss
    function into a A novel composite loss function.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | <math  class="ltx_math_unparsed" alttext="\begin{split}L_{reg}&amp;=L_{size}+L_{off}\\
    &amp;=\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(size(K(m_{t-1}^{\widetilde{k}}),size(box_{t}^{k}))\\'
  prefs: []
  type: TYPE_NORMAL
- en: '&amp;+\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(off(K(m_{t-1}^{\widetilde{k}}),off(box_{t}^{k}))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true"
    rowspacing="0pt" ><mtr ><mtd class="ltx_align_right"
    columnalign="right" ><msub ><mi >L</mi><mrow
    ><mi >r</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >e</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >g</mi></mrow></msub></mtd><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    >=</mo><mrow ><msub ><mi
    >L</mi><mrow ><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >z</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >e</mi></mrow></msub><mo
    >+</mo><msub ><mi >L</mi><mrow
    ><mi >o</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >f</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >f</mi></mrow></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd class="ltx_align_left" columnalign="left" ><mrow
    ><mo rspace="0.111em" >=</mo><munder
    ><mo movablelimits="false" >∑</mo><mrow
    ><mi >k</mi><mo
    >∈</mo><mrow ><mo
    >{</mo><mi >i</mi><mo
    >,</mo><mi >j</mi><mo
    >}</mo></mrow></mrow></munder><msubsup ><mi
    >L</mi><mn >1</mn><mrow
    ><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >m</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >h</mi></mrow></msubsup><mrow
    ><mo stretchy="false" >(</mo><mi
    >s</mi><mi >i</mi><mi
    >z</mi><mi >e</mi><mrow
    ><mo stretchy="false" >(</mo><mi
    >K</mi><mrow ><mo
    stretchy="false" >(</mo><msubsup ><mi
    >m</mi><mrow ><mi
    >t</mi><mo >−</mo><mn
    >1</mn></mrow><mover accent="true" ><mi
    >k</mi><mo >~</mo></mover></msubsup><mo
    stretchy="false" >)</mo></mrow><mo >,</mo><mi
    >s</mi><mi >i</mi><mi
    >z</mi><mi >e</mi><mrow
    ><mo stretchy="false" >(</mo><mi
    >b</mi><mi >o</mi><msubsup
    ><mi >x</mi><mi
    >t</mi><mi >k</mi></msubsup><mo
    stretchy="false" >)</mo></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd
    class="ltx_align_left" columnalign="left" ><mrow ><mo
    rspace="0.055em" >+</mo><munder ><mo
    movablelimits="false" >∑</mo><mrow ><mi
    >k</mi><mo >∈</mo><mrow
    ><mo >{</mo><mi
    >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><msubsup
    ><mi >L</mi><mn >1</mn><mrow
    ><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >m</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >t</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >h</mi></mrow></msubsup><mrow
    ><mo stretchy="false" >(</mo><mi
    >o</mi><mi >f</mi><mi
    >f</mi><mrow ><mo
    stretchy="false" >(</mo><mi >K</mi><mrow
    ><mo stretchy="false" >(</mo><msubsup
    ><mi >m</mi><mrow
    ><mi >t</mi><mo
    >−</mo><mn >1</mn></mrow><mover
    accent="true" ><mi >k</mi><mo
    >~</mo></mover></msubsup><mo stretchy="false"
    >)</mo></mrow><mo >,</mo><mi
    >o</mi><mi >f</mi><mi
    >f</mi><mrow ><mo
    stretchy="false" >(</mo><mi >b</mi><mi
    >o</mi><msubsup ><mi
    >x</mi><mi >t</mi><mi
    >k</mi></msubsup><mo stretchy="false" >)</mo></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{split}L_{reg}&=L_{size}+L_{off}\\
    &=\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(size(K(m_{t-1}^{\widetilde{k}}),size(box_{t}^{k}))\\
    &+\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(off(K(m_{t-1}^{\widetilde{k}}),off(box_{t}^{k}))\end{split}</annotation></semantics></math>
    |  | (13) |'
  prefs: []
  type: TYPE_NORMAL
- en: 'where $L_{1}^{smooth}$ is smooth loss function based on $L_{1}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L_{1}^{smooth}(a,b)=\left\{\begin{matrix}0.5\cdot(a-b)^{2}&amp;if&#124;a-b&#124;<1\\
    &#124;a-b&#124;-0.5&amp;else\end{matrix}\right.$ |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/2d834c11121af354f403debd3f2c49c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Centre Leaping Principle[[147](#bib.bib147)]'
  prefs: []
  type: TYPE_NORMAL
- en: 3.6 AI Backdoor & Poisoning on ADS
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Artificial intelligence models are often generated from a certain amount of
    training data. Some scholars have found that if the training data is not trustworthy,
    it may lead to the generation of models with "backdoor", which can be manipulated
    by attackers in the subsequent use of the models, causing serious security risks.
    Currently, the concepts of "AI backdoor", "AI model poisoning" and "AI Trojan
    horse" have some similarities, but are not expressed in the same way in different
    literature. One type of attack is called Training-only attacks, or Poisoning Attacks
    are usually defined as attacker contaminating part of the training data or modifying
    the labels of the training data. On the contrary, another type of attack is called
    Backdoor Attacks or AI Trojans[[13](#bib.bib13)], in which attackers must participate
    in both training and testing. Both poisoning attacks and AI backdoor attacks can
    cause serious security threats to autonomous driving system, and this paper refers
    to these two types of attacks as "AI backdoor attacks", where a specific malicious
    modification is made to a target model in a specific way, causing the model to
    make harmful judgments about a specifically predicted example. There are similarities
    and differences between backdoor attacks and adversarial example attacks. Adversarial
    examples usually do not change the model itself and will not damage the integrity
    of the AI model, but mainly interfere with the test examples and affect the availability
    and correctness of the machine learning model. On another hand, AI backdoor takes
    the form of modification of the AI model, poison of the training data, aggregation
    of the backdoor model, etc., causing tiny changes to the AI model, affecting both
    the integrity of the AI model. AI backdoor attacks tend to be more hidden, highly
    universal, and more damaging. So far, there are two major methods to implement
    AI backdoor, one is Data Poisoning Attack, and the other is Model Poisoning Attack.
    Data Poisoning Attack means attacker adds a small amount of poisoning data into
    training dataset, so that the resulting AI model has a backdoor, and the AI model
    may make a specific judgement when the predicted example contains a "trigger".
    Model Poisoning Attack means the attacker directly modifies the model or indirectly
    fuses the target model with a harmful model by using model integration, federated
    learning, and transition learning, causing the model to make a directed and erroneous
    judgment on a specific prediction example.
  prefs: []
  type: TYPE_NORMAL
- en: It has been argued that AI backdoor attacks already exist in traditional machine
    learning. In 2008, Nelson et al. [[148](#bib.bib148), [149](#bib.bib149)] proposed
    the backdoor attack on Bayesian networks. In 2012 Biggio et al.[[150](#bib.bib150)]
    proposed backdoor attack on SVMs. In 2016, Alfeld et al.[[151](#bib.bib151)] proposed
    backdoor on auto-regressive prediction models. In 2017, Gu et al.[[3](#bib.bib3)]
    first proposed backdoor attack on deep learning, then AI backdoor became a promising
    research topic. The BadNet algorithm adds a small number of training data with
    the pre-designed pattern into the training data and labels such training examples
    with a specific target, then the trained model is likely to predict examples with
    "Trigger" according to the attacker. In the same year, Muñoz-González et al.[[152](#bib.bib152)]
    proposed a gradient-based algorithm for AI data poisoning. However, for autonomous
    driving system, the basic AI backdoor algorithms described above have two limitations.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Control right of training data by attacker.As it requires the attacker to be
    able to contaminate a certain amount of training data, this requires the attacker
    to have some control over the training data; at the very least the attacker needs
    to have background knowledge of the target model’s structure, parameters, etc.,
    which places certain requirements on the attacker.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concealment of attack. It relies on contaminating part of the training data
    by adding a ’pattern’ or changing the label of the data. Although the pattern
    may be relatively insidious, forcing patterns into normal examples may cause a
    certain amount of unnaturalness that may be detected by humans, or possibly by
    automated detection through some anomaly identification method. It may also lead
    to human feel a sense of inharmonious if the attacker modifies excessive label
    of the training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In response to these limitations, researchers have made a number of subsequent
    improvements. On the one hand, attackers have improved the concealment of poisoning
    attacks by enhancing the concealment of the patterns in the example or minimizing
    the impact on the integrity of the label. One of the research directions is "clean
    label", which aims to keep the label of poisoned example semantically correct
    while realizing data poisoning. In 2018, Shafahi et al.[[153](#bib.bib153)] proposed
    the Poison Frogs algorithm, which was the first to implement the Clean Label attack
    for deep learning. In the same year, Truner et al.[[154](#bib.bib154)] proposed
    two methods of data generation based on adversarial network and adversarial example
    to achieve a label-consistent "clean example" attack. Another research direction
    is "Hidden Trigger", also known as "Invisible Trigger", which aims to optimize
    the trigger pattern to make it as invisible as possible to escape detection by
    humans and machines. In 2018, Suciu et al.[[155](#bib.bib155)] , and in 2019 Saha
    et al.[[156](#bib.bib156)] put forward "hidden trigger" which can generate trigger
    patterns that humans are unable to directly perceive through the senses. In 2020,
    Wallace et al.[[157](#bib.bib157)] devised a "hidden trigger" poisoning attack
    in the field of natural language processing. In the same year, Li et al.[[158](#bib.bib158)]
    used information hiding and regularization methods to improve the bad net algorithm
    to improve the invisibility of the trigger pattern.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, the attacker reduces the proportion of contaminated training
    data as much as possible, or even implements a black-box attack that does not
    require contaminating data, thus reducing the background knowledge required for
    the attack and lowering the threshold for implementing the attack. In 2017, Liu
    et al. [[159](#bib.bib159)] implemented a black-box approach to generate backdoor
    by exploiting the migratory nature of the attack, but such backdoor mainly exists
    in the fully connected layer at the end of the AI model, which can easily fail
    once the model is fine-tuned; in the same year, Chen et al.[[38](#bib.bib38)]
    proposed a machine learning-based approach to generate AI backdoor, which eliminates
    the need for attackers to understand the structure of the target system and other
    information, and reduces the background knowledge requirement; in 2019, Yao et
    al.[[160](#bib.bib160)] proposed "latent triggers", which are first generated
    in the "teacher model" and then migrated to the "student model" through transition
    learning. The backdoor is not only found in the last fully connected layer of
    the student model, but also in all of its layers and thus the difficulty of detecting
    the "backdoor" through analysis is increased. In the same year, Zhu et al.[[161](#bib.bib161)]
    investigated the migratory nature of clean label attacks and used knowledge transition
    to realize a black-box clean label attack.
  prefs: []
  type: TYPE_NORMAL
- en: In the field of autonomous driving, in 2018, Liu et al.[[162](#bib.bib162)]
    realized AI backdoor attacks in a variety of environments, including simulated
    autonomous driving platforms. In 2019, Rehman et al.[[163](#bib.bib163)] implemented
    an AI backdoor attack on traffic signs in the physical world; Barni et al.[[164](#bib.bib164)]
    conducted a clean label poisoning attack on traffic signs; Ding et al. from Nanjing
    University[[165](#bib.bib165)] designed a "natural trigger" for autonomous driving
    system to trigger AI model backdoor in special weather like a rainy day, to make
    red lights incorrectly identified as green lights and numbers incorrectly identified
    in a specific way; Yao et al.[[160](#bib.bib160)] from the University of Chicago
    used their proposed "latent trigger" method to generate backdoor traffic signs
    for a variety of models, generating human-imperceptible triggers on physical traffic
    signs. In 2021, Tian et al.[[166](#bib.bib166)] achieved a clean label attack
    on 3D cloud point map. In 2022, Udeshi et al.[[167](#bib.bib167)] proposed an
    anti-backdoor attack method that can be used in traffic sign recognition scenarios,
    which achieved avoiding AI backdoor attacks by filtering the triggers in the captured
    images and correcting the prediction examples.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 9\. Feature Collisions
  prefs: []
  type: TYPE_NORMAL
- en: Feature collision is the more common method of AI backdoor generation, where
    the attacker first selects a target instance from the test set. To achieve poisoning,
    the attacker chooses a base class instance from the base class and makes imperceptible
    changes to it, thereby generating a poisoned instance that is injected into the
    training data later; then, during the training phase, the model is trained using
    a poisoned data set consisting of a clean data set plus poisoned instance; in
    the reasoning phase, this causes the target instance to be mistaken by the misclassification
    model for being in the base class during testing. It is described formally as
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: $f(x)$ represents the neural network’s prediction on input example $x$. Example
    $x$ colliding with the target is found in the feature space and then computed
    to be close to the base instance $b$. The target function is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $p=arg\min\left\&#124;f(x)-f(t)\right\&#124;^{2}_{2}+\beta\left\&#124;x-b\right\&#124;^{2}_{2}$
    |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: $p$ is the poisoning instance, which will be misdirected as the attack target.
  prefs: []
  type: TYPE_NORMAL
- en: 3.7 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The risk of adversarial example and AI backdoor is brought by the characteristics
    of deep learning itself. Whether an autonomous driving system uses RGB cameras,
    LiDAR, RaDAR or other sensors as the source of information collection, it often
    dependent on deep learning for perception and driving decisions. Going with it,
    there are new safety risks associated with artificial intelligence. At the same
    time, the autonomous driving system is a huge system, and in the perception layer
    alone, they consist of many links that rely on deep learning technologies, such
    as target recognition, image segmentation, depth estimation and target tracking,
    which constitute a complex decision-making process, and each link is also subject
    to different types of AI security threats. It is necessary to ensure the safety
    of each link to constitute a set of safe autonomous driving system.
  prefs: []
  type: TYPE_NORMAL
- en: Generated adversarial examples are hard to consistently fool neural network
    classifiers in the physical world. In this chapter, we introduced emphatically
    method of physical world adversarial examples enhancement, summarized it in Table
    2.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Physical World Adversarial Examples Enhance Methods'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Contributions | Scenarios in Autonomous Driving |'
  prefs: []
  type: TYPE_TB
- en: '| EoT[[132](#bib.bib132)], | EoT generate adversarial examples over a chosen
    distribution of transformations.EoT is the first algorithm that produces robust
    adversarial examples, which single adversarial examples to an entire distribution
    of transformations | Object detection |'
  prefs: []
  type: TYPE_TB
- en: '| Adversarial Patch[[138](#bib.bib138)] | This attack generates an image-independent
    patch that can then be placed anywhere within the field of view of the classifier,
    and causes the classifier to output a targeted class. | Object detection |'
  prefs: []
  type: TYPE_TB
- en: '| FIR[[168](#bib.bib168)] | FIR generated adversarial examples to impact both
    hidden layers and the final layer. Therefore, the misclassification for adversarial
    examples depends more on the prior layers in the neural-network, which lead to
    be more robust in physical scenarios. | Traffic sign detection |'
  prefs: []
  type: TYPE_TB
- en: '| Nested-AE[[168](#bib.bib168)] | Nested-AE contains two or more Adversarial
    examples inside that for different distances or angles. It significantly improve
    the robustness of adversarial attack at the various position. | Traffic sign detection
    |'
  prefs: []
  type: TYPE_TB
- en: '| Randomly Transformed Patch[[135](#bib.bib135)] | These transforms are a composition
    of brightness, contrast, rotation, translation, and sheering transforms that help
    make patches robust to variations caused by lighting and viewing angle that occur
    in the real world. | Pedestrian detection |'
  prefs: []
  type: TYPE_TB
- en: '| TV Loss[[169](#bib.bib169), [170](#bib.bib170), [135](#bib.bib135)] | TV
    Loss ensures a more smooth patch in which all pixels in the patch get optimized.
    | Object detection |'
  prefs: []
  type: TYPE_TB
- en: '| Ensemble training[[135](#bib.bib135)] | Ensemble training fool an ensemble
    of detectors that were not used for training. | Object detection |'
  prefs: []
  type: TYPE_TB
- en: '| UPC [[171](#bib.bib171)] | UPC optimization constraint to make generated
    patterns look natural to human observers. | Object detection |'
  prefs: []
  type: TYPE_TB
- en: '| NPS [[169](#bib.bib169)] | NPS deal with the difference in digital RGB-values
    and the ability of real printers to reproduce these values. | Object detection
    |'
  prefs: []
  type: TYPE_TB
- en: '| Discrete Search[[139](#bib.bib139)] | Discrete Search improve the black-box
    attack by iteratively refining the camouflage using a mutation-based search method.
    | physical-world Black-box attack |'
  prefs: []
  type: TYPE_TB
- en: '| MSF-Adv[[142](#bib.bib142)] | MSF-Adv generate adversarial examples in Lidar,
    RaDar, and fusion. | Vehicle detection |'
  prefs: []
  type: TYPE_TB
- en: '| Spatial Transformer Layer (STL) to project[[170](#bib.bib170)] | Many kinds
    of Projects imitate the form changes for rectangula adversarial patches after
    placing it in physical world. | Object detection |'
  prefs: []
  type: TYPE_TB
- en: '| Sticker Projection[[170](#bib.bib170)] | Project the obtained adversarial
    examples with small perturbations in the projection parameters to make the attack
    more robust. | Object detection |'
  prefs: []
  type: TYPE_TB
- en: As the foundation, some major general adversarial examples algorithms list in
    Appendix A.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Emerging Threats of Decision-Making Layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The major function of the decision layer is to make the correct driving decision
    based on sensing and perception. In common autonomous driving architectures, the
    trajectory of dynamic objects, such as vehicles or pedestrians, must predicted.
    If the prediction process is maliciously interfered with by an attacker, vehicle
    may under security threat.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Emerging Threat on Prediction-oriented attack techniques
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In general, autonomous driving systems need to predict the short-term or long-term
    spatial coordinates of various road agents such as cars, buses, pedestrians, rickshaws,
    and animals, etc. Predicting usually base on Recurrent Neural Network (RNN) techniques,
    the algorithms of which include LSTM, and Sequence to Sequence. Researchers have
    proposed attack methods for recurrent neural network algorithms. In 2016, Papernot
    et al.[[124](#bib.bib124)] proposed an RNN-oriented adversarial example attack,
    and many subsequent researchers have continued to improve the attack method and
    enhance the attack effect[[172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174),
    [175](#bib.bib175), [176](#bib.bib176)].
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Emerging Threat in Imitation Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Imitation Learning and Reinforcement Learning are the two main approaches to
    driving decision making. Imitation Learning is a data-driven approach that imitates
    expert driver policies to make decisions[[177](#bib.bib177)] and some end-to-end
    autonomous driving systems use an imitation learning framework[[178](#bib.bib178)].
    Reinforcement learning, on the other hand, uses deep reinforcement learning algorithms
    to optimize the model and make the best decisions. Whether a autonomous driving
    system adopts an imitation learning or a reinforcement learning architecture,
    an attacker could interfere with the AI model, thereby affecting normal driving
    decisions to pose a risk to the autonomous driving system.
  prefs: []
  type: TYPE_NORMAL
- en: Imitation learning can be described as a process[[44](#bib.bib44)], and human
    expert experience can be described as a tuple such as $(s,a,r,{s}^{\prime})$,
    where s is the state of driving, $a$ is the behavior of the human expert, $r$
    is the reward created by the behaviour $a$, and $s^{\prime}$ is the resulting
    new state. Imitation learning generates policy $\pi$ through machine learning,
    based on the captured set of behaviour of the human experts $D=(x_{i},y_{i})$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $u(t)=\pi(x(t),t,\alpha)$ |  |'
  prefs: []
  type: TYPE_TB
- en: $u$ is the predicted behavior given by the machine, $x$ is the feature vector
    of the state of the environment $s$, $t$ is the time, and $\alpha$ is the set
    of parameters for a set of policies.
  prefs: []
  type: TYPE_NORMAL
- en: Imitation learning is still a branch of deep learning and based on deep neural
    networks (DNNs), also it is equally threatened by adversarial examples, AI backdoor
    and other forms of attack. In the autonomous driving field in 2020, Boloor et
    al.[[179](#bib.bib179)] proposed an adversarial example generation algorithm based
    on Bayesian optimization that can attack end-to-end (E2E) autonomous driving trajectory
    prediction system, which was successfully experimented on Intel’s Carla simulation
    platform. This approach sprayed special adversarial patterns generated with the
    algorithm on the road, interfering with the autonomous driving system’s prediction
    of its own vehicle, to induce the autonomous driving system to make a wrong driving
    decision. In the same year, Yang et al.[[180](#bib.bib180)] proposed two adversarial
    attack algorithms for vehicle trajectory prediction, which improved the above
    method, reducing the number of optimization rounds required for adversarial example
    generation and improving efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 10\. Bayesian Optimization (BO) algorithm
  prefs: []
  type: TYPE_NORMAL
- en: The objective of the BO algorithm is to generate adversarial examples suitable
    for end-to-end autonomous driving system and find the best adversarial perturbation
    through optimization $\delta$, with an loss function of
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\delta^{*}=\arg\max_{\delta}f(\delta)$ |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: where $\delta^{*}\in\mathcal{R}^{d}$, assuming that the autonomous driving model
    $f$ prediction conforms to a Gaussian process, it can be written as $GP(f,\mu(\delta),k(\delta,\delta^{\prime}))$,
    and let the mathematical expectation be 0, then $\mu(\delta)=0$, with the variance
    is the Mattern covariance function $K$.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $k(\delta,{\delta}^{\prime})=(1+\frac{\sqrt{5}r}{l}+\frac{5r^{2}}{3l^{2}})exp(-\frac{\sqrt{5}r}{l})$
    |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: where $r$ denotes the Euclidean distance and $l$ is a factor coefficient. In
    such a manner, we can consider $\mathcal{l}$ is the adversarial perturbation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/948a0ed4bc04f82839ef155fde28ff6e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: BO Algorithm [[179](#bib.bib179)]'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Emerging Threat on Reinforcement Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Reinforcement learning (deep reinforcement learning) is widely used in fields
    such as autonomous decision-making, electronic combat and competition. Combined
    with a number of other techniques such as deep search trees, deep reinforcement
    learning has enabled AlphaGo to explore some of the blind spots of human cognition.
    Adopting reinforcement learning technique for autonomous driving decisions is
    one of the major technology routes in academia and industry. Reinforcement learning
    security has recently received extensive attention and research.
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning can be described as a Markov Decision Process(MDP)[[54](#bib.bib54),
    [44](#bib.bib44)]. A finite state decision making process consists of the tuple$(S,A,T,R)$
    where $S$ is the set of finite states, $A$ is the possible behaviour of the system,
    and $T$ is the State Transition Probabilities consisting of a set of probabilities
    $P_{s,a}$. It indicates that when behaviour $a$ is taken, the probability of reaching
    state $s$ is achieved, and reward function $R$ can return the reward value $Y$
    which can be obtained by the reward policy $R(s_{k},a_{k},s_{k+1})$ where the
    reward value $Y$ denotes changing the state to state $s_{k+1}$ when taking behaviour
    $a_{k}$ at the state $s_{k}$. Reinforcement learning is the process of starting
    with a random policy, receiving a reward based on the execution of that policy,
    and then continuously optimizing the policy by maximizing the reward.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e9841500457b76defb0699492b70d1fc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Adversarial Attacks in Reinforcement Learning [[181](#bib.bib181)]'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning is a process of self-optimization, where models evolve
    and improve, but the process is also threatened by AI security risks[[181](#bib.bib181),
    [182](#bib.bib182)]. In the field of reinforcement learning, it is difficult to
    distinguish between the concepts of adversarial examples and AI backdoor, which
    are collectively referred to as "adversarial attacks". Based on attack paths,
    Kiourti et al.[[183](#bib.bib183)] categorize the attacks against reinforcement
    learning into environmental adversarial attacks, reward adversarial attacks, and
    adversarial policy attacks.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Environmental Adversarial Attacks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Environment-based adversarial attacks are those that add perturbations to the
    environment perceived in reinforcement learning, thereby affecting the system’s
    perception of state $s$, which in turn incorrectly matches the attacker’s specified
    policy ${s}^{\prime}$ to finally manipulate the decisions of the reinforcement
    learning system in a given state. In 2017, Huang et al.[[184](#bib.bib184)] implemented
    an environmental adversarial attack on a reinforcement learning system by adding
    adversarial perturbations to the external environment image frames in reinforcement
    learning based on the white-box FGSM algorithm; in the same year, Lin et al.[[185](#bib.bib185)]
    proposed an optimized environmental adversarial attack method targeting at the
    best behavior in a specific state; Behzadan et al.[[186](#bib.bib186)] verified
    the transition of adversarial attacks among reinforcement learning models and
    thus proposed a transition-based black-box attack. In 2019, Xiao et al.[[187](#bib.bib187)]
    proposed a method for estimating model gradients based on frame consistency information,
    thus enabling the first adversarial black-box attack in reinforcement learning.
    In 2020, Kiourti et al.[[183](#bib.bib183)] proposed TrojDRL, which describes
    a reward-based adversarial attack as one that is performed by adding minute specific
    perturbations to the environmental state to enables $\hat{s}=s+\delta$ that eventually
    makes the behaviour given by the reinforcement learning model misunderstood, i.e.$\hat{A}(s,m,\delta)\neq
    A(s,m,\delta)$. In the area of autonomous driving, Behzadan et al.[[188](#bib.bib188)]
    in 2019 verified that in the environment of autonomous driving, using reward adversarial
    attacks, an attacker could cause a direct collision or malicious manipulation
    of the trajectory of the autonomous driving vehicles.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reward Adversarial Attacks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: If an attacker is able to maliciously tamper with some of the rewards, it may
    lead to the policy of the reinforcement learning system being manipulated by the
    attacker, thus posing a severe threat to the system. Kiourti et al.[[183](#bib.bib183)]
    validated a reward adversarial attack where an attacker would set the corresponding
    reward to 1 when its target state $s$ is reached, and otherwise set the reward
    to -1, to create a strong attack scenario. In 2022, Islam et al.[[189](#bib.bib189)]
    proposed a reward adversarial attack applicable to the UAV environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adversarial Policy Attacks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Unlike the reward adversarial attack approach, an adversarial policy attack
    does not need to tamper with the victim’s reward or policy; instead, in an adversarial
    environment, the attacker quickly finds a policy to defeat the victim by analyzing
    the victim’s policy or behavior, finding its vulnerabilities and exploiting them.
    Tretschk et al.[[190](#bib.bib190)] proposed the adversarial policy of Adversarial
    Transformer Networks (ATN). In 2020, Gleave et al.[[191](#bib.bib191)] proposed
    Adversarial Policies, in which an attacker generates targeted adversarial policies
    based on the behavior of the victim, producing seemingly random and uncoordinated
    behavior to defeat or disrupt the victim. Such policies are more successful in
    high-dimensional environments and have been validated in real eSports environments.
    In 2021, Wang et al.[[192](#bib.bib192)] synthesized reward adversarial attacks
    with adversarial policy attacks and proposed BackDooRK, which significantly improved
    the success rate of attackers in defeating their victims.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Algorithm 11\. Adversarial Policies
  prefs: []
  type: TYPE_NORMAL
- en: For multiple (in the case of two) participants in a reinforcement learning environment,
    it is assumed that the victim’s policy $\pi_{v}$ has been determined, and here
    the victim’s policy determines its behaviour $a_{v}\sim\pi_{v}(\cdot\mid s)$.
    And the attacker continuously optimizes its own policy $\pi_{\alpha}$ based on
    the victim’s policy and behaviour. It then be described as in this Markov decision
    process $M_{a}=(S,A_{\alpha},T_{\alpha},{R}^{\prime}_{\alpha})$, considered within
    state transition probabilities $T_{\alpha}$ and rewards $R_{\alpha}$, the integration
    of the victim policy $\pi_{v}$ yields
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $T_{\alpha}(s,a_{\alpha})=T(s,a_{\alpha,a_{v}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: and
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${R}^{\prime}_{\alpha}(s,a_{\alpha,{s}^{\prime}})=R_{\alpha}(s,a_{\alpha,a_{v},{s}^{\prime}})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The attacker finds an adversarial policy against the victim by optimizing the
    following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\arg\max\sum_{t=0}^{\infty}\gamma^{t}R_{\alpha}(s_{t},a_{\alpha}^{t},s^{t+1})$
    |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: where it is subject to $s^{t+1}\sim T_{\alpha}(s^{t},a_{\alpha}^{t})$ and $a_{\alpha}\sim\pi(\cdot\mid
    s^{t})$
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This chapter introduces some technologies that may pose a security risk to the
    autonomous driving AI decision-making layer and briefly describes the technical
    principles. The main function of the sensor layer is to recognize the raw information
    collected by sensors, while the main function of the decision layer is to make
    driving decisions based on the perceived state of the environment. Each layer
    has its own autonomous driving function, and a threat to any of these layers could
    affect the overall safety of the autonomous driving system.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Emerging Threats in Federated Learning-based Vehicular Internet of Things
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the rapid development of smart vehicles, the vehicle is no longer an isolated
    single point, but increasingly a terminal worker in the pan-vehicle network. In
    many countries and regions around the world, the vehicular internet of things
    is already under rapid construction and its security based on traditional cyber
    security technologies has been widely studied[[15](#bib.bib15)]. However, with
    the development of emerging technologies such as arithmetic networks and privacy
    computing, new generation technologies such as deep learning technologies, edge
    computing and federated learning will be further integrated into the environment
    of the vehicular internet of things, giving rise to new business forms. Still,
    new technologies and new business forms also bring new security risks, and this
    chapter focuses on the new risks that they may bring.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is well known that current artificial intelligence technology is a data-driven
    approach[[193](#bib.bib193), [194](#bib.bib194), [195](#bib.bib195)]. In AI applications,
    a large amount of information needs to be collected in advance as the training
    data. Traditional methods of data collection and information interaction face
    a number of limitations: First, traditional data collection and transmission are
    often inefficient. Secondly, traditional data collection often leads to invasion
    of user privacy. Therefore, Federated Learning (FL)[[196](#bib.bib196), [197](#bib.bib197),
    [198](#bib.bib198)] is a new distributed learning framework that does not require
    data collection by a central worker, providing a relatively more efficient and
    private way of interaction. In federated learning, each worker is trained with
    local data sets to obtain local gradients or weights through machine learning
    algorithms, and then uploads local gradients instead of local sensitive data,
    enabling knowledge interaction instead of data interaction. Federated learning
    has been widely used in the Internet, mobile terminals and other fields. At the
    same time, federated learning has also been introduced into the field of the vehicular
    internet of things [[199](#bib.bib199), [67](#bib.bib67), [200](#bib.bib200),
    [66](#bib.bib66)] and has become a future trend.'
  prefs: []
  type: TYPE_NORMAL
- en: The federated learning environment offers more new attack methods[[201](#bib.bib201),
    [202](#bib.bib202), [203](#bib.bib203)]. With more workers participating in federated
    learning and the trustworthiness of each worker with the cloud difficult to guarantee,
    malicious workers in the vehicular IoT may attack the federated learning system
    in a variety of ways, while the privacy of the workers may also be at risk.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Byzantine Attack on Federated Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Byzantine Attack in Federated Learning refers to workers attacked by a malicious
    Byzantine worker to construct harmful gradients which after aggregation will make
    the global model difficult to aggregate, thus making the system unusable or generating
    a global model with a malicious backdoor. Distinguished from traditional Data
    Posioning attacks, the above attacks are also known as Model Posioning attacks.
    In 2017, Blanchard et al.[[204](#bib.bib204)] first proposed the Byzantine attack
    in a machine learning environment. The principle is that in round $t$, a non-Byzantine
    worker $p$ in federated learning will locally compute the unbiased estimate $V_{p}^{t}$
    of its gradient $\bigtriangledown Q(x_{t})$ and send it to the aggregation worker
    which according to some aggregation rule $F$, aggregates the received gradient
    estimates, then in round $t+1$, the weight of the global model is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{t+1}=x_{t}-\gamma_{t}\cdot F(V_{1}^{t},...,V_{n}^{t})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\gamma_{t}$ is the learning rate. While the malicious Byzantine worker
    cleverly constructs destructive local gradient estimates:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V_{n}=\frac{1}{\lambda_{n}}\cdot U-\sum^{n-1}_{i=1}\frac{\lambda_{i}}{\lambda_{n}}V_{i}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\gamma_{t}$ is the weight the gradient estimate of worker $n$ at the
    time of aggregation. This will cause the global gradient to become any harmful
    gradient $U$ provided by the Byzantine worker after the final aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d57d46cd366832914257c6f68cef28bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Schematic of Byzantine attack [[204](#bib.bib204)]. The black dashed
    line represents the gradient estimate for non-Byzantine workers, the blue solid
    line represents the global gradient after normal aggregation, and the red dashed
    line represents the gradient estimate submitted by the Byzantine worker.'
  prefs: []
  type: TYPE_NORMAL
- en: The above describes a Byzantine worker that poisons in only some round of aggregation
    $t$, which is called a "single-shot attack". Generally, its limitations are as
    follows.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attack’s Capability Is Limited. Attack just is a single worker or a single aggregation
    that has limited influence on its attacking power.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poor Concealment. A single worker attack or a single-shot attack on an aggregator
    usually makes the state of the poisoned worker significantly different from a
    normal worker, and that leads to easily detected.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prone to Recession. After multiple aggregations, the effect of poisoning a single
    worker during an aggregation round tends to fade, even does not continue to be
    effective.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Blanchard et al.[[204](#bib.bib204)] also proposed the concept of Byzantine
    tolerance for measuring the robustness of federated learning models against Byzantine
    attacks. To improve Byzantine robustness, some scholars have proposed novel aggregation
    algorithms that can be used for federated learning[[205](#bib.bib205), [206](#bib.bib206)].
  prefs: []
  type: TYPE_NORMAL
- en: To overcome these limitations, an attacker can adopt to Repeated Attacks or
    Collusion Attacks. Repeated Attacks mean that the attacker can perform poisoning
    in multiple rounds of aggregation; Collusion Attacks are joint poisoning of multiple
    Byzantine workers. Experiments show that repeated attacks and collusion attacks
    can enhance the capability of Byzantine attacks and can significantly improve
    the concealment and recession resistance of the attacks[[207](#bib.bib207)]. Xie
    et al.[[208](#bib.bib208)] proposed a distributed backdoor attack that can be
    performed in a federated learning environment.
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 12\. Distributed Backdoor Attack (DBA)[[208](#bib.bib208)]
  prefs: []
  type: TYPE_NORMAL
- en: Distributed backdoor attacks provide a efficient way for multiple malicious
    workers in federated learning to conspire to an attack. The DBA algorithm takes
    advantage of the local data opacity in federated learning, with multiple malicious
    workers each adding more minute malicious perturbations in multiple rounds to
    improve concealment. The malicious permissions of each worker are optimally generated
    in the following manner.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\begin{split}w^{*}_{i}=&amp;\arg\max_{w_{i}}(\sum_{j\in s_{poi}^{i}}P[G^{t+1}(R(X_{i}^{j},\phi^{*}_{i}))=\tau;\gamma;I]\\
    &amp;+\sum_{j\in S^{i}_{cln}}P[G^{t+1}(x_{j}^{i})=y_{j}^{i}]),\forall i\in[M]\end{split}$
    |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: where $\phi^{*}_{i}=\left\{\phi,O(i)\right\}$ represents the local poisoning
    policy of attacker $m_{i}$, and $\forall$ is the global trigger.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c92652123f5763adad46742a6b65d6d9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Schematic of Byzantine Attack [[201](#bib.bib201)]. The black dashed
    line denotes the gradient estimate for non-Byzantine workers, the blue solid line
    denotes the global gradient after normal aggregation, and the red dashed line
    denotes the gradient estimate submitted by the Byzantine worker. [[201](#bib.bib201)]'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Privacy Inference on Federated Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The interior and exterior images of an autonomous driving vehicle may include
    sensitive information such as faces and license plate numbers. So local user data
    may reflect the user’s location and trajectory, in-vehicle behavior and driving
    habits, which are also often considered sensitive, and therefore direct user data
    capture may lead to user privacy violations. One of the aims of Federated Learning
    is to avoid the direct leakage of sensitive user data, thereby achieving user
    privacy protection. The effects of user privacy in federated learning have received
    extensive attention and researches[[209](#bib.bib209), [113](#bib.bib113), [210](#bib.bib210),
    [211](#bib.bib211), [201](#bib.bib201), [212](#bib.bib212), [213](#bib.bib213),
    [214](#bib.bib214)] . However, it shows that local gradients are highly correlated
    with the user’s data and that data may still be inferred when local gradients
    are obtained. Techniques such as Model Inversion and Membership Privacy Inference
    may help infer local user data.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model Inversion The model inversion method replaces the pixels in the initial
    random images one by one, then classifies the constructed images with the help
    of some model, and iteratively optimizes the constructed image based on the results
    of the classification, resulting in a constructed image that is highly similar
    to the target image. To speed up aggregation, model inversion algorithms mostly
    utilize the distribution of the target image as prior knowledge to participate
    in the optimization process. In 2015, Fredrikson et al.[[5](#bib.bib5)] first
    proposed a model inversion attack; in 2016, Wu et al.[[215](#bib.bib215)] proposed
    a black-box model inversion attack algorithm; in 2017, Hitaj et al.[[216](#bib.bib216)]
    proposed a model inversion algorithm based on adversarial generative network techniques,
    which achieved better reconstruction results, also known as "Adversarial Generative
    Network Reconstruction Attacks" (GAN Reconstruction Attacks). Mai et al.[[217](#bib.bib217)]
    extended model inversion to the field of face recognition to recover from face
    recognition features for face image data, proposing the Face Recovery Attack,
    and Razzhigaev et al.[[218](#bib.bib218), [219](#bib.bib219)] continuously optimized
    the black box face recovery attack.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Membership Privacy Inference Member privacy inference means that an attacker
    exploits the special feedback of the overfitting phenomenon and tries to infer
    whether the target data is in the training set or not. Typically, attackers first
    construct a Shadow Model similar to the target model; then uses the shadow model
    to generate training data; furthermore, used the data to train an Attack Model;
    last, the attackers use the Attack Model to construct the complete attack process.
    In 2017, Shokri et al.[[6](#bib.bib6)] at Cornell University first proposed the
    member privacy inference attack. Once proposed, the member privacy inference method
    has been continuously researched and further optimized and improved[[214](#bib.bib214),
    [213](#bib.bib213), [220](#bib.bib220)]. In 2018, Yeom et al.[[221](#bib.bib221)]
    analyzed in depth the relationship between overfitting and the risk of member
    privacy leakage. In 2019, Salem et al.[[222](#bib.bib222)] incorporated a data
    transition attack method that reduces the attacker’s reliance on background knowledge
    and makes the "shadow model" less necessary. Sablayrolles et al.[[223](#bib.bib223)]
    improved the membership privacy inference attack using a Bayesian optimization
    policy. Zhang et al.[[224](#bib.bib224)] extended the member privacy inference
    attack to the recommender system domain. Hui et al.[[225](#bib.bib225)] proposed
    the BlINDMI algorithm, which first generates a certain amount of non-membership
    data, and then iteratively generates a comparison between non-membership data
    and membership data to improve the accuracy of membership privacy inference.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In a federated learning, the data distribution of workers is broadly similar.
    That providing more contextual knowledge and creating better conditions for malicious
    workers and the cloud to conduct member privacy inference attacks. In 2019, Nasr
    et al.[[226](#bib.bib226)] validated the membership privacy inference attack risk
    in federated learning. In 2020, Chen et al.[[227](#bib.bib227)] further improved
    the success rate of member privacy inference attacks in federated learning using
    adversarial generative networks for data augmentation. In 2021, Hu et al.[[228](#bib.bib228)]
    proposed Source Inference Attack for federated learning, which uses Bayesian methods
    to infer the training data of federated learning workers and in the same year,
    Gupta et al.[[229](#bib.bib229)] extended the traditional membership privacy inference
    attack from classification tasks to regression tasks, and also verified that their
    method is equally applicable to regression tasks in a federated learning environment.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Algorithm 13\. General Model Inversion[[5](#bib.bib5)]
  prefs: []
  type: TYPE_NORMAL
- en: Model inversion used be recover the training dataset that probability includes
    sensitive data. The general model inversion algorithm first computes each possible
    target feature vector $v$ for feature $x_{1}$ and then evaluates its probability
    of being correct. Also, since the class distribution of deep learning models tends
    to obey a Gaussian Distribution, so adapt a Gaussian function as a penalty function
    can accelerate the convergence. The algorithm can be described as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input: Target model $f$Output: An example of training data $x^{\prime}$12initialized
    $x_{0}$ ;3foreach *feature $x_{i}\in$ the feature vector $X$* do4      5      foreach *the
    possible value $v\in x_{i}$*  do6            7            $x^{\prime}={v,x_{2},x_{3},...,x_{n}}$8            $r_{v}\leftarrow
    err(y,f(x^{\prime}))\cdot\Pi_{i}p_{i}(x_{i})$9       end foreach10      11 end
    foreach'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 4 13\. Model Inversion
  prefs: []
  type: TYPE_NORMAL
- en: Algorithm 14\. Membership Privacy Inference[[6](#bib.bib6)]
  prefs: []
  type: TYPE_NORMAL
- en: 'Implementing a member privacy inference attack requires several processes,
    starting with the generation of data for training the shadow model. If the attacker
    has some background knowledge and possesses some homogeneous distribution data,
    it can be used directly for shadow model training. If the attacker does not have
    the appropriate background knowledge, data with a high confidence level can be
    selected as integrated training data by querying the target model. Each classification
    category $c$ will be initially recorded as $x$ randomly and iterated as follows:
    sequentially select the record data classified by the target model as $c$ with
    the maximum confidence level $y_{c}$ which is ensured to be greater than a certain
    threshold on $f$ , into the integrated data set. Once a record $x$ is selected
    into the integrated data set, randomly change the features as many as $k$ based
    on $x$ to generate a new record $X^{*}$. This is iterated until a certain amount
    of training data is generated. The second process is to generate a shadow model.
    Based on the generated training data set$D^{train}_{shadow_{i}}$, after training,
    shadow models $shadow_{i}$ will be generated. The third step is to train the attack
    model. Query the prediction vector of the records $(x,y)\in D_{shadow}^{train}$
    in the training data set of shadow models $shadow_{i}$, then record $(y,\hat{y},in)$
    can be generated. And calculate the prediction vector$\hat{y}=f_{shadow}^{i}(x)$
    in the shadow model test data set $\forall(x,y)\in D_{shadow_{i}}^{test}$ then
    vector $\hat{y}=f_{shadow}^{i}(x)$ can be obtained. Next the two corresponding
    sets of vectors of each category $c$ are aggregated into the training data $D_{attack}^{train}$
    of the attack model. On this basis, a classifier is trained to determine whether
    the data is included in the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the fusion and development of technologies, such as federated learning
    , edge computing and etc., the intelligent vehicular internet of things is gradually
    growing and becoming a future trend[[230](#bib.bib230)]. However, the security
    and user privacy risks associated with federated learning and other technologies
    are also a concern. In the vehicular IoTs, the data distribution of each end is
    similar, providing attackers with certain background knowledge. Once a end is
    controlled by an attacker, the whole IoT networks may be subject to Byzantine
    attacks, and the risk of user privacy analysis is greatly increased. While enjoying
    the convenience offered by vehicular IoT and AI, we should not ignore the associated
    risks, but rather conduct relevant research and security protection.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Autonomous driving is a complex system based on artificial intelligence technology.
    A number of artificial intelligence applications, such as objective detection,
    segmentation, speech recognition and driving decision-making, play an important
    role in autonomous driving. Safety is a key concern in autonomous driving systems.
    AI security is crucial and directly affects autonomous driving system security,
    and leads to personal safety, which is far beyond traditional network security
    and basic software security. The novel technologies, such as AI, bring emerging
    risks.
  prefs: []
  type: TYPE_NORMAL
- en: 'This paper briefly introduces the AI technology route and AI functional modules
    in the autonomous driving system, and analyses the origins, development, and current
    appropriate AI security technologies for autonomous driving. Similar to general
    AI, autonomous driving is under threats of adversarial examples attacks, including
    AI backdoor attacks, model inversion and member privacy. Despite there are some
    defense methods that can be useful against these threats, ensuring safety for
    complex autonomous driving systems requires not only single-point defense techniques
    against certain threats but also building a complete trusted AI system[[231](#bib.bib231),
    [232](#bib.bib232)], as following:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Trustworthy AI evaluation system. The safety of AI in autonomous driving system
    requires a complete evaluation and validation system, which covering data preparation,
    model training, model deployment, system application and other parts of the AI
    model life cycle. Especially, there are some noteworthy issues, including: AI
    adversarial robustness assessment, cross-domain data robustness assessment, model
    safety validation, training data safety validation, and data adversarial example
    detection.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trustworthy AI Architecture. Further more, we need to improve autonomous driving
    security from just reducing some specific threats to building Trustworthy AI Architecture.
    There is something beyond adversarial detecting need to do to build an AI architecture
    with human agency and oversight, robustness and safety, privacy and data governance,
    transparency, diversity, non-discrimination and fairness, societal and environmental
    well-being, and accountability.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To summarize, this paper appeals for attention and focus on emerging technologies
    such as artificial intelligence that bring new safety risks to autonomous driving
    systems. It is necessary that construct a safer and trusted autonomous driving
    system through the establishment of a trusted artificial intelligence technology
    system.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Appendix A. Summary table
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here we present a table containing a summary of the adversarial examples algorithms
    as the foundation in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Foundational Adversarial Examples Algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | method | Major algorithms |'
  prefs: []
  type: TYPE_TB
- en: '| White-Box | Gradient sign-based | FGSM[[2](#bib.bib2)], BIM[Kurakin2018adversarial],
    MIM[[233](#bib.bib233)],PGD[[234](#bib.bib234)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Optimization-based | CW[[126](#bib.bib126)], |'
  prefs: []
  type: TYPE_TB
- en: '|  | Others | … |'
  prefs: []
  type: TYPE_TB
- en: '| Black-Box | Transfer-Based | DIM[[235](#bib.bib235)], TI[[236](#bib.bib236)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Approximate Gradient | BPDA[[237](#bib.bib237)],EoT[[132](#bib.bib132)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Score-based | ZOO[[238](#bib.bib238)], NES[[239](#bib.bib239)], SPSA[[240](#bib.bib240)],
    $N$Attack[[241](#bib.bib241)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Decision-based | Boundary Attack[[242](#bib.bib242)], Evolutionary Attack[[243](#bib.bib243)]
    |'
  prefs: []
  type: TYPE_TB
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
    Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks.
    arXiv preprint arXiv:1312.6199, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and
    harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying
    vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
    Stealing machine learning models via prediction $\{$APIs$\}$. In 25th USENIX security
    symposium (USENIX Security 16), pages 601–618, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks
    that exploit confidence information and basic countermeasures. In Proceedings
    of the 22nd ACM SIGSAC conference on computer and communications security, pages
    1322–1333, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership
    inference attacks against machine learning models. In 2017 IEEE symposium on security
    and privacy (SP), pages 3–18\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, and Jinwen He. Towards security
    threats of deep learning systems: A survey. IEEE Transactions on Software Engineering,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Shouling Ji, tianyu Du, Jinfeng Li, Chao Shen, and Bo Li. Security and
    privacy of machine learning models: A survey. Journal of Software, 32(1), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Han Xu, Yaxin Li, Wei Jin, and Jiliang Tang. Adversarial attacks and defenses:
    frontiers, advances and practice. In Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining, pages 3541–3542, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Jiao Li, Yang Liu, Tao Chen, Zhen Xiao, Zhenjiang Li, and Jianping Wang.
    Adversarial attacks and defenses on cyber–physical systems: A survey. IEEE Internet
    of Things Journal, 7(6):5103–5115, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Jiliang Zhang and Chen Li. Adversarial examples: Opportunities and challenges.
    IEEE transactions on neural networks and learning systems, 31(7):2578–2593, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples:
    Attacks and defenses for deep learning. IEEE transactions on neural networks and
    learning systems, 30(9):2805–2824, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild,
    Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. Data security for machine
    learning: data poisoning, backdoor attacks, and defenses. arXiv e-prints, pages
    arXiv–2012, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] SAE On-Road Automated Vehicle Standards Committee et al. Taxonomy and
    definitions for terms related to on-road motor vehicle automated driving systems.
    SAE Standard J, 3016:1–16, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Kui Ren, Qian Wang, Cong Wang, Zhan Qin, and Xiaodong Lin. The security
    of autonomous driving: Threats, defenses, and future directions. Proceedings of
    the IEEE, 108(2):357–372, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Yu Huang and Yue Chen. Survey of state-of-art autonomous driving technologies
    with deep learning. In 2020 IEEE 20th International Conference on Software Quality,
    Reliability and Security Companion (QRS-C), pages 221–228\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, and Qing-Long
    Han. Deep learning-based autonomous driving systems: a survey of attacks and defenses.
    IEEE Transactions on Industrial Informatics, 17(12):7897–7912, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards
    real-time object detection with region proposal networks. Advances in neural information
    processing systems, 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn.
    In Proceedings of the IEEE international conference on computer vision, pages
    2961–2969, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European
    conference on computer vision, pages 21–37. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv
    preprint arXiv:1804.02767, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
    Focal loss for dense object detection. In Proceedings of the IEEE international
    conference on computer vision, pages 2980–2988, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Yosuke Shinya. Usb: Universal-scale object detection benchmark. arXiv
    preprint arXiv:2103.14027, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Di Feng, Christian Haase-Schütz, Lars Rosenbaum, Heinz Hertlein, Claudius
    Glaeser, Fabian Timm, Werner Wiesbeck, and Klaus Dietmayer. Deep multi-modal object
    detection and semantic segmentation for autonomous driving: Datasets, methods,
    and challenges. IEEE Transactions on Intelligent Transportation Systems, 22(3):1341–1360,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Belur V Dasarathy. Sensor fusion potential exploitation-innovative architectures
    and illustrative applications. Proceedings of the IEEE, 85(1):24–38, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Jamil Fayyad, Mohammad A Jaradat, Dominique Gruyer, and Homayoun Najjaran.
    Deep learning sensor fusion for autonomous vehicle perception and localization:
    A review. Sensors, 20(15):4220, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Yingjie Wang, Qiuyu Mao, Hanqi Zhu, Yu Zhang, Jianmin Ji, and Yanyong
    Zhang. Multi-modal 3d object detection in autonomous driving: a survey. arXiv
    preprint arXiv:2106.12735, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] De Jong Yeong, Gustavo Velasco-Hernandez, John Barry, Joseph Walsh, et al.
    Sensor and sensor fusion technology in autonomous vehicles: A review. Sensors,
    21(6):2140, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and
    Raquel Urtasun. Monocular 3d object detection for autonomous driving. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 2147–2156,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese. Data-driven
    3d voxel patterns for object category recognition. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 1903–1911, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, Céline Teuliere, and
    Thierry Chateau. Deep manta: A coarse-to-fine many-task network for joint 2d and
    3d vehicle analysis from monocular image. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 2040–2049, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Qingdong He, Zhengning Wang, Hao Zeng, Yi Zeng, Shuaicheng Liu, and Bing
    Zeng. Svga-net: Sparse voxel-graph attention network for 3d object detection from
    point clouds. arXiv preprint arXiv:2006.04043, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Bo Li, Tianlei Zhang, and Tian Xia. Vehicle detection from 3d lidar using
    fully convolutional network. arXiv preprint arXiv:1608.07916, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Jorge Beltrán, Carlos Guindel, Francisco Miguel Moreno, Daniel Cruzado,
    Fernando Garcia, and Arturo De La Escalera. Birdnet: a 3d object detection framework
    from lidar information. In 2018 21st International Conference on Intelligent Transportation
    Systems (ITSC), pages 3517–3523\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Bo Li. 3d fully convolutional network for vehicle detection in point cloud.
    In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
    pages 1513–1518\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++:
    Deep hierarchical feature learning on point sets in a metric space. Advances in
    neural information processing systems, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud
    based 3d object detection. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 4490–4499, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object
    detection network for autonomous driving. In Proceedings of the IEEE conference
    on Computer Vision and Pattern Recognition, pages 1907–1915, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L Waslander.
    Joint 3d proposal generation and object detection from view aggregation. In 2018
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages
    1–8\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum
    pointnets for 3d object detection from rgb-d data. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 918–927, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Duarte Fernandes, António Silva, Rafael Névoa, Claudia Simoes, Dibet Gonzalez,
    Miguel Guevara, Paulo Novais, Joao Monteiro, and Pedro Melo-Pinto. Point-cloud
    based 3d object detection and classification methods for self-driving applications:
    A survey and taxonomy. Information Fusion, 68:161–191, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Rui Qian, Xin Lai, and Xirong Li. 3d object detection for autonomous driving:
    a survey. Pattern Recognition, page 108796, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Browse state-of-the-art. [https://paperswithcode.com/area/computer-vision/autonomous-vehicles/](https://paperswithcode.com/area/computer-vision/autonomous-vehicles/).
    Accessed May 18, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation
    learning: A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1–35,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos A
    Theodorou, and Byron Boots. Imitation learning for agile autonomous driving. The
    International Journal of Robotics Research, 39(2-3):286–302, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, and
    Alexey Dosovitskiy. End-to-end driving via conditional imitation learning. In
    2018 IEEE international conference on robotics and automation (ICRA), pages 4693–4700\.
    IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learning
    to drive by imitating the best and synthesizing the worst. arXiv preprint arXiv:1812.03079,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
    Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,
    et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially
    observable mdps. In 2015 aaai fall symposium series, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, and
    Anastasiia Ignateva. Deep attention recurrent q-network. arXiv preprint arXiv:1512.01693,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In International conference on machine learning,
    pages 1928–1937\. PMLR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul,
    Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with
    unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony
    Bharath. Deep reinforcement learning: A brief survey. IEEE Signal Processing Magazine,
    34(6):26–38, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Xidong Feng, Jianming Hu, Yusen Huo, and Yi Zhang. Autonomous lane change
    decision making using different deep reinforcement learning methods. In CICTP
    2019, pages 5563–5575\. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Ali Alizadeh, Majid Moghadam, Yunus Bicer, Nazim Kemal Ure, Ugur Yavas,
    and Can Kurtulus. Automated lane change decision making using deep reinforcement
    learning in dynamic and uncertain highway environment. In 2019 IEEE Intelligent
    Transportation Systems Conference (ITSC), pages 1399–1404\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Branka Mirchevska, Christian Pek, Moritz Werling, Matthias Althoff, and
    Joschka Boedecker. High-level decision making for safe and reasonable autonomous
    lane changing using reinforcement learning. In 2018 21st International Conference
    on Intelligent Transportation Systems (ITSC), pages 2156–2162\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Yang Thee Quek, Li Ling Koh, Ngiap Tiam Koh, Wai Ann Tso, and Wai Lok
    Woo. Deep q-network implementation for simulated autonomous vehicle control. IET
    Intelligent Transport Systems, 15(7):875–885, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Martin Holen, Rupsa Saha, Morten Goodwin, Christian W Omlin, and Knut Eivind
    Sandsmark. Road detection for reinforcement learning based autonomous car. In
    Proceedings of the 2020 the 3rd International Conference on Information Science
    and System, pages 67–71, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Yanming Feng and Yongrong Wu. Environmental adaptive urban traffic signal
    control based on reinforcement learning algorithm. In Journal of Physics: Conference
    Series, volume 1650, page 032097\. IOP Publishing, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A
    Al Sallab, Senthil Yogamani, and Patrick Pérez. Deep reinforcement learning for
    autonomous driving: A survey. IEEE Transactions on Intelligent Transportation
    Systems, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Szilárd Aradi. Survey of deep reinforcement learning for motion planning
    of autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Maurice Pope. Etsi. universal mobile telecommunications system (umts);
    lte; architecture enhancements for v2x services (3gpp ts 23.285 version 14.2\.
    0 release 14), 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Apostolos Papathanassiou and Alexey Khoryaev. Cellular v2x as the essential
    enabler of superior global connected transportation services. IEEE 5G Tech Focus,
    1(2):1–2, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Xiaobo Ma, Jiahao Peng, Lei Xue, and Xiaohong Guan. ntegrated security
    of cyber-physical vehicle networked systems in the age of 5g (in chinese). Sci
    Sin Inform, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-Lim Alvin Yau, Yusheng
    Ji, and Jie Li. Federated learning for vehicular internet of things: Recent advances
    and open issues. IEEE Open Journal of the Computer Society, 1:45–61, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Jason Posner, Lewis Tseng, Moayad Aloqaily, and Yaser Jararweh. Federated
    learning in vehicular networks: opportunities and solutions. IEEE Network, 35(2):152–159,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Zi Jian Yew and Gim Hee Lee. 3dfeat-net: Weakly supervised local 3d features
    for point cloud registration. In Proceedings of the European conference on computer
    vision (ECCV), pages 607–623, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully convolutional
    geometric features. In Proceedings of the IEEE/CVF International Conference on
    Computer Vision, pages 8958–8966, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, and Chiew-Lan Tai. End-to-end learning
    local multi-view descriptors for 3d point clouds. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 1919–1928, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang, Erik B Sudderth,
    and Jan Kautz. A fusion approach for multi-frame optical flow estimation. In 2019
    IEEE Winter Conference on Applications of Computer Vision (WACV), pages 2077–2086\.
    IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, and Yunhe
    Wang. Multimodal token fusion for vision transformers. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 12186–12195, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz,
    and Andreas Geiger. Transfuser: Imitation with transformer-based sensor fusion
    for autonomous driving. arXiv preprint arXiv:2205.15997, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi
    Peng, Junyang Shen, Yifeng Lu, Denny Zhou, Quoc V Le, et al. Deepfusion: Lidar-camera
    deep fusion for multi-modal 3d object detection. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 17182–17191, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference
    on computer vision, pages 1440–1448, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Diganta Misra. Mish: A self regularized non-monotonic neural activation
    function. arXiv preprint arXiv:1908.08681, 4(2):10–48550, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects
    with recursive feature pyramid and switchable atrous convolution. In Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pages 10213–10224,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable
    bag-of-freebies sets new state-of-the-art for real-time object detectors. arXiv
    preprint arXiv:2207.02696, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong
    Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning
    via feature distillation. arXiv preprint arXiv:2205.14141, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal
    generation and detection from point cloud. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 770–779, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang
    Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object
    detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pages 10529–10538, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Wu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu. Se-ssd: Self-ensembling
    single-stage object detector from point cloud. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 14494–14503, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Yifan Zhang, Qijian Zhang, Zhiyu Zhu, Junhui Hou, and Yixuan Yuan. Glenet:
    Boosting 3d object detectors with generative label uncertainty estimation. arXiv
    preprint arXiv:2207.02466, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan
    Venkatesan, Brucek Khailany, Joel Emer, Stephen W Keckler, and William J Dally.
    Scnn: An accelerator for compressed-sparse convolutional neural networks. ACM
    SIGARCH computer architecture news, 45(2):27–40, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Lucas Tabelini, Rodrigo Berriel, Thiago M Paixao, Claudine Badue, Alberto F
    De Souza, and Thiago Oliveira-Santos. Keep your eyes on the lane: Real-time attention-guided
    lane detection. In Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition, pages 294–302, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Tu Zheng, Yifei Huang, Yang Liu, Wenjian Tang, Zheng Yang, Deng Cai, and
    Xiaofei He. Clrnet: Cross layer refinement network for lane detection. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 898–907,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Álvaro Arcos-García, Juan A Alvarez-Garcia, and Luis M Soria-Morillo.
    Deep neural network for traffic sign recognition systems: An analysis of spatial
    transformers and stochastic optimisation methods. Neural Networks, 99:158–165,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Junzhou Chen, Kunkun Jia, Wenquan Chen, Zhihan Lv, and Ronghui Zhang.
    A real-time and high-precision method for small traffic-signs recognition. Neural
    Computing and Applications, 34(3):2233–2245, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Esther Rani et al. Littleyolo-spp: A delicate real-time vehicle detection
    algorithm. Optik, 225:165818, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Pranav Adarsh, Pratibha Rathi, and Manoj Kumar. Yolo v3-tiny: Object detection
    and recognition using one stage improved model. In 2020 6th International Conference
    on Advanced Computing and Communication Systems (ICACCS), pages 687–694\. IEEE,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Jiashi Feng, and Shuicheng
    Yan. Scale-aware fast r-cnn for pedestrian detection. IEEE transactions on Multimedia,
    20(4):985–996, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Liliang Zhang, Liang Lin, Xiaodan Liang, and Kaiming He. Is faster r-cnn
    doing well for pedestrian detection? In European conference on computer vision,
    pages 443–457. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Irtiza Hasan, Shengcai Liao, Jinpeng Li, Saad Ullah Akram, and Ling Shao.
    Generalizable pedestrian detection: The elephant in the room. In Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11328–11337,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.
    Pyramid scene parsing network. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2881–2890, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Jun Fu, Jing Liu, Jie Jiang, Yong Li, Yongjun Bao, and Hanqing Lu. Scene
    segmentation with dual relation-aware attention network. IEEE Transactions on
    Neural Networks and Learning Systems, 32(6):2547–2560, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen
    Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using
    shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pages 10012–10022, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and
    Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Chaoda Zheng, Xu Yan, Haiming Zhang, Baoyuan Wang, Shenghui Cheng, Shuguang
    Cui, and Zhen Li. Beyond 3d siamese tracking: A motion-centric paradigm for 3d
    single object tracking in point clouds. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 8111–8120, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Chaoda Zheng, Xu Yan, Jiantao Gao, Weibing Zhao, Wei Zhang, Zhen Li,
    and Shuguang Cui. Box-aware feature enhancement for single object tracking on
    point clouds. In Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pages 13199–13208, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Jiangbei Yue, Dinesh Manocha, and He Wang. Human trajectory prediction
    via neural social physics. arXiv preprint arXiv:2207.10435, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Karttikeya Mangalam, Yang An, Harshayu Girase, and Jitendra Malik. From
    goals, waypoints & paths to long term human trajectory forecasting. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 15233–15242,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone.
    Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data.
    In European Conference on Computer Vision, pages 683–700. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre
    Alahi. Social gan: Socially acceptable trajectories with generative adversarial
    networks. In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pages 2255–2264, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid
    Rezatofighi, and Silvio Savarese. Sophie: An attentive gan for predicting paths
    compliant to social and physical constraints. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 1349–1358, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Motion transformer
    with global intention localization and local movement refinement. arXiv preprint
    arXiv:2209.13508, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S
    Refaat, and Benjamin Sapp. Wayformer: Motion forecasting via simple & efficient
    attention networks. arXiv preprint arXiv:2207.05844, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
    Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement
    learning. arXiv preprint arXiv:1312.5602, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Hado Hasselt. Double q-learning. Advances in neural information processing
    systems, 23, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning.
    Advances in neural information processing systems, 29, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Jeffrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma, Daniele
    Reda, Nikolay Nikolov, Przemysław Mazur, Sean Micklethwaite, Nicolas Griffiths,
    Amar Shah, et al. Urban driving with conditional imitation learning. In 2020 IEEE
    International Conference on Robotics and Automation (ICRA), pages 251–257\. IEEE,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation
    learning. In International Conference on Machine Learning, pages 3878–3887\. PMLR,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated
    learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine,
    37(3):50–60, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Jonathan Petit, Bas Stottelaar, Michael Feiri, and Frank Kargl. Remote
    attacks on automated vehicles sensors: Experiments on camera and lidar. Black
    Hat Europe, 11(2015):995, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Chen Yan, Wenyuan Xu, and Jianhao Liu. Can you trust autonomous vehicles:
    Contactless attacks against sensors of self-driving vehicle. Def Con, 24(8):109,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and
    Wenyuan Xu. Dolphinattack: Inaudible voice commands. In Proceedings of the 2017
    ACM SIGSAC Conference on Computer and Communications Security, pages 103–117,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Bing Shun Lim, Sye Loong Keoh, and Vrizlynn LL Thing. Autonomous vehicle
    ultrasonic sensor vulnerability and impact assessment. In 2018 IEEE 4th World
    Forum on Internet of Things (WF-IoT), pages 231–236\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Yunmok Son, Hocheol Shin, Dongkwan Kim, Youngseok Park, Juhwan Noh, Kibum
    Choi, Jungwoo Choi, and Yongdae Kim. Rocking drones with intentional sound noise
    on gyroscopic sensors. In 24th USENIX Security Symposium (USENIX Security 15),
    pages 881–896, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Gorkem Kar, Hossen Mustafa, Yan Wang, Yingying Chen, Wenyuan Xu, Marco
    Gruteser, and Tam Vu. Detection of on-road vehicles emanating gps interference.
    In Proceedings of the 2014 ACM SIGSAC conference on computer and communications
    security, pages 621–632, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Youngseok Park, Yunmok Son, Hocheol Shin, Dohyun Kim, and Yongdae Kim.
    This ain’t your dose: Sensor spoofing attack on medical infusion pump. In 10th
    USENIX workshop on offensive technologies (WOOT 16), 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Dudi Nassi, Raz Ben-Netanel, Yuval Elovici, and Ben Nassi. Mobilbye:
    attacking adas with camera spoofing. arXiv preprint arXiv:1906.09765, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Mark L Psiaki and Todd E Humphreys. Protecting gps from spoofers is critical
    to the future of navigation. IEEE spectrum, 10, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Qian Meng, Li-Ta Hsu, Bing Xu, Xiapu Luo, and Ahmed El-Mowafy. A gps
    spoofing generator using an open sourced vector tracking-based receiver. Sensors,
    19(18):3993, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
    Celik, and Ananthram Swami. Practical black-box attacks against machine learning.
    In Proceedings of the 2017 ACM on Asia conference on computer and communications
    security, pages 506–519, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
    Frossard. Universal adversarial perturbations. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 1765–1773, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Nicholas Carlini and David Wagner. Towards evaluating the robustness
    of neural networks. In 2017 ieee symposium on security and privacy (sp), pages
    39–57\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial
    examples. In International Conference on Learning Representations, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples
    in the physical world, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Jiajun Lu, Hussein Sibai, and Evan Fabry. Adversarial examples that fool
    detectors. arXiv preprint arXiv:1712.02494, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati,
    Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world
    attacks on deep learning visual classification. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 1625–1634, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Polo Chau.
    Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector.
    In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
    pages 52–68\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing
    robust adversarial examples. In International conference on machine learning,
    pages 284–293\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection
    with deep learning: A review. IEEE transactions on neural networks and learning
    systems, 30(11):3212–3232, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Zelun Kong, Junfeng Guo, Ang Li, and Cong Liu. Physgan: Generating physical-world-resilient
    adversarial examples for autonomous driving. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 14254–14263, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] Zuxuan Wu, Ser-Nam Lim, Larry S Davis, and Tom Goldstein. Making an invisibility
    cloak: Real world adversarial attacks on object detectors. In European Conference
    on Computer Vision, pages 1–17. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] Yi Wang, Jingyang Zhou, Tianlong Chen, Sijia Liu, Shiyu Chang, Chandrajit
    Bajaj, and Zhangyang Wang. Can 3d adversarial logos cloak humans? arXiv preprint
    arXiv:2006.14655, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Yang Zhang, PD Hassan Foroosh, and Boqing Gong. Camou: Learning a vehicle
    camouflage for physical adversarial attack on object detections in the wild. ICLR,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer.
    Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, and Yu Wang.
    Physical adversarial attack on vehicle detector in the carla simulator. arXiv
    preprint arXiv:2007.16118, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi,
    Qi Alfred Chen, Kevin Fu, and Z Morley Mao. Adversarial sensor attack on lidar-based
    perception in autonomous driving. In Proceedings of the 2019 ACM SIGSAC conference
    on computer and communications security, pages 2267–2281, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Qi Sun, Xufeng Yao, Arjun Ashok Rao, Bei Yu, and Shiyan Hu. Counteracting
    adversarial attacks in autonomous driving. IEEE Transactions on Computer-Aided
    Design of Integrated Circuits and Systems, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] Yulong Cao, Ningfei Wang, Chaowei Xiao, Dawei Yang, Jin Fang, Ruigang
    Yang, Qi Alfred Chen, Mingyan Liu, and Bo Li. Invisible for both camera and lidar:
    Security of multi-sensor fusion based perception in autonomous driving under physical-world
    attacks. In 2021 IEEE Symposium on Security and Privacy (SP), pages 176–194\.
    IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Xuesong Chen, Xiyu Yan, Feng Zheng, Yong Jiang, Shu-Tao Xia, Yong Zhao,
    and Rongrong Ji. One-shot adversarial attacks on visual tracking with dual attention.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 10176–10185, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Bin Yan, Dong Wang, Huchuan Lu, and Xiaoyun Yang. Cooling-shrinking attack:
    Blinding the tracker with imperceptible noises. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 990–999, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] Shuai Jia, Yibing Song, Chao Ma, and Xiaokang Yang. Iou attack: Towards
    temporally coherent black-box adversarial attack for visual object tracking. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 6709–6718, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Yunhan Jia Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Hao Chen, Zhenyu
    Zhong, and Tao Wei Wei. Fooling detection alone is not enough: Adversarial attack
    against multiple object tracking. In International Conference on Learning Representations
    (ICLR’20), 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Delv Lin, Qi Chen, Chengyu Zhou, and Kun He. Trasw: Tracklet-switch adversarial
    attacks against multi-object tracking. arXiv preprint arXiv:2111.08954, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP
    Rubinstein, Udam Saini, Charles Sutton, J Doug Tygar, and Kai Xia. Exploiting
    machine learning to subvert your spam filter. LEET, 8(1-9):16–17, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The
    security of machine learning. Machine Learning, 81(2):121–148, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against
    support vector machines. arXiv preprint arXiv:1206.6389, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against
    autoregressive models. In Proceedings of the AAAI Conference on Artificial Intelligence,
    volume 30, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice,
    Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning
    algorithms with back-gradient optimization. In Proceedings of the 10th ACM workshop
    on artificial intelligence and security, pages 27–38, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph
    Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label
    poisoning attacks on neural networks. Advances in neural information processing
    systems, 31, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label
    backdoor attacks. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor
    Dumitras. When does machine learning $\{$FAIL$\}$? generalized transferability
    for evasion and poisoning attacks. In 27th USENIX Security Symposium (USENIX Security
    18), pages 1299–1316, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden
    trigger backdoor attacks. In Proceedings of the AAAI conference on artificial
    intelligence, volume 34, pages 11957–11965, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh.
    Universal adversarial triggers for attacking and analyzing nlp. arXiv preprint
    arXiv:1908.07125, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng
    Zhang. Invisible backdoor attacks on deep neural networks via steganography and
    regularization. IEEE Transactions on Dependable and Secure Computing, 18(5):2088–2105,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable
    adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor
    attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference
    on Computer and Communications Security, pages 2041–2055, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Chen Zhu, W Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer,
    and Tom Goldstein. Transferable clean-label poisoning attacks on deep neural nets.
    In International Conference on Machine Learning, pages 7614–7623\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang
    Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] Huma Rehman, Andreas Ekelhart, and Rudolf Mayer. Backdoor attacks in
    neural networks–a systematic evaluation on multiple traffic sign datasets. In
    International Cross-Domain Conference for Machine Learning and Knowledge Extraction,
    pages 285–300\. Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack
    in cnns by training set corruption without label poisoning. In 2019 IEEE International
    Conference on Image Processing (ICIP), pages 101–105\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] Shaohua Ding, Yulong Tian, Fengyuan Xu, Qun Li, and Sheng Zhong. Trojan
    attack on deep generative models in autonomous driving. In International Conference
    on Security and Privacy in Communication Systems, pages 299–318\. Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] Guiyu Tian, Wenhao Jiang, Wei Liu, and Yadong Mu. Poisoning morphnet
    for clean-label backdoor attack to point clouds. arXiv preprint arXiv:2105.04839,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan,
    and Sudipta Chattopadhyay. Model agnostic defence against backdoor attacks in
    machine learning. IEEE Transactions on Reliability, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, and Kai
    Chen. Seeing isn’t believing: Towards more robust adversarial attack against real
    world object detectors. In Proceedings of the 2019 ACM SIGSAC Conference on Computer
    and Communications Security, pages 1989–2004, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter.
    Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition.
    In Proceedings of the 2016 acm sigsac conference on computer and communications
    security, pages 1528–1540, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Stepan Komkov and Aleksandr Petiushko. Advhat: Real-world adversarial
    attack on arcface face id system. In 2020 25th International Conference on Pattern
    Recognition (ICPR), pages 819–826\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] Lifeng Huang, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan L Yuille, Changqing
    Zou, and Ning Liu. Universal physical camouflage attacks on object detectors.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 720–729, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] Weiwei Hu and Ying Tan. Black-box attacks against rnn based malware detection
    algorithms. In Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] Kuei-Huan Chang, Po-Hao Huang, Honggang Yu, Yier Jin, and Ting-Chi Wang.
    Audio adversarial examples generation with recurrent neural networks. In 2020
    25th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 488–493\.
    IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and Cho-Jui Hsieh.
    Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial
    examples. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
    pages 3601–3608, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted
    attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW),
    pages 1–7. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] Felix Kreuk, Yossi Adi, Moustapha Cisse, and Joseph Keshet. Fooling end-to-end
    speaker verification with adversarial examples. In 2018 IEEE international conference
    on acoustics, speech and signal processing (ICASSP), pages 1962–1966\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] Xiaolin Song, Xin Sheng, Haotian Cao, Mingjun Li, Binlin Yi, and Zhi
    Huang. Lane-change behavior decision-making of intelligent vehicle based on imitation
    learning and reinforcement learning(in chinese). Automotive Engineering, 43(1):59–67,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan,
    Evangelos Theodorou, and Byron Boots. Agile autonomous driving using end-to-end
    deep imitation learning. arXiv preprint arXiv:1709.07174, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy Vorobeychik,
    and Xuan Zhang. Attacking vision-based perception in end-to-end autonomous driving
    models. Journal of Systems Architecture, 110:101766, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] Jinghan Yang, Adith Boloor, Ayan Chakrabarti, Xuan Zhang, and Yevgeniy
    Vorobeychik. Finding physical adversarial examples for autonomous driving with
    fast and differentiable image compositing. arXiv preprint arXiv:2010.08844, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala
    Al-Fuqaha, Dinh Thai Huang, and Dusit Niyato. Challenges and countermeasures for
    adversarial attacks on deep reinforcement learning. IEEE Transactions on Artificial
    Intelligence, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Jinyin Chen, Yan Zhang, Wang Xueke, Cai Hongbin, Jue Wang, and Shouling
    Ji. A survey of attack, defense and related security analysis for deep reinforcement
    learning(in chinese). Acta Automatica Sinica, 48(1):21–39, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl:
    evaluation of backdoor attacks on deep reinforcement learning. In 2020 57th ACM/IEEE
    Design Automation Conference (DAC), pages 1–6\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel.
    Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu,
    and Min Sun. Tactics of adversarial attack on deep reinforcement learning agents.
    arXiv preprint arXiv:1703.06748, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement
    learning to policy induction attacks. In International Conference on Machine Learning
    and Data Mining in Pattern Recognition, pages 262–275\. Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun, Jinfeng
    Yi, Mingyan Liu, Bo Li, and Dawn Song. Characterizing attacks on deep reinforcement
    learning. arXiv preprint arXiv:1907.09470, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] Vahid Behzadan and Arslan Munir. Adversarial reinforcement learning framework
    for benchmarking collision avoidance mechanisms in autonomous vehicles. IEEE Intelligent
    Transportation Systems Magazine, 13(2):236–241, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Shafkat Islam, Shahriar Badsha, Ibrahim Khalil, Mohammed Atiquzzaman,
    and Charalambos Konstantinou. A triggerless backdoor attack and defense mechanism
    for intelligent task offloading in multi-uav systems. IEEE Internet of Things
    Journal, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] Edgar Tretschk, Seong Joon Oh, and Mario Fritz. Sequential attacks on
    agents for long-term adversarial goals. arXiv preprint arXiv:1805.12487, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and
    Stuart Russell. Adversarial policies: Attacking deep reinforcement learning. arXiv
    preprint arXiv:1905.10615, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song.
    Backdoorl: Backdoor attack against competitive reinforcement learning. arXiv preprint
    arXiv:2105.00579, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436–444, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] Yoshua Bengio, Yann Lecun, and Geoffrey Hinton. Deep learning for ai.
    Communications of the ACM, 64(7):58–65, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Bo Zhang, Jun Zhu, and Hang Su. Toward the third generation of artificial
    intelligence (in chinese). Sci Sin Inform, 50(9):1281–1302, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] Jakub Konečnỳ, H Brendan McMahan, Daniel Ramage, and Peter Richtárik.
    Federated optimization: Distributed machine learning for on-device intelligence.
    arXiv preprint arXiv:1610.02527, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha
    Suresh, and Dave Bacon. Federated learning: Strategies for improving communication
    efficiency. arXiv preprint arXiv:1610.05492, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] H Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas.
    Federated learning of deep networks using model averaging.(2016). arXiv preprint
    arXiv:1602.05629, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] Ahmet M Elbir, Burak Soner, and Sinem Coleri. Federated learning in vehicular
    networks. arXiv preprint arXiv:2006.01412, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] Hongyi Zhang, Jan Bosch, and Helena Holmström Olsson. Real-time end-to-end
    federated learning: An automotive case study. In 2021 IEEE 45th Annual Computers,
    Software, and Applications Conference (COMPSAC), pages 459–468\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] Lingjuan Lyu, Han Yu, and Qiang Yang. Threats to federated learning:
    A survey. arXiv preprint arXiv:2003.02133, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali
    Dehghantanha, and Gautam Srivastava. A survey on security and privacy of federated
    learning. Future Generation Computer Systems, 115:619–640, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Xuefei Yin, Yanming Zhu, and Jiankun Hu. A comprehensive survey of privacy-preserving
    federated learning: A taxonomy, review, and future directions. ACM Computing Surveys
    (CSUR), 54(6):1–36, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.
    Machine learning with adversaries: Byzantine tolerant gradient descent. Advances
    in Neural Information Processing Systems, 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Rachid Guerraoui, Sébastien Rouault, et al. The hidden vulnerability
    of distributed learning in byzantium. In International Conference on Machine Learning,
    pages 3521–3530\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust
    distributed learning: Towards optimal statistical rates. In International Conference
    on Machine Learning, pages 5650–5659\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
    Shmatikov. How to backdoor federated learning. In International Conference on
    Artificial Intelligence and Statistics, pages 2938–2948\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor
    attacks against federated learning. In International Conference on Learning Representations,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
    Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
    Rachel Cummings, et al. Advances and open problems in federated learning. Foundations
    and Trends® in Machine Learning, 14(1–2):1–210, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller.
    Inverting gradients-how easy is it to break privacy in federated learning? Advances
    in Neural Information Processing Systems, 33:16937–16947, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] Mohammad Al-Rubaie and J Morris Chang. Privacy-preserving machine learning:
    Threats and solutions. IEEE Security & Privacy, 17(2):49–58, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu,
    and Bingsheng He. A survey on federated learning systems: vision, hype and reality
    for data privacy and protection. IEEE Transactions on Knowledge and Data Engineering,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] Chunyi Zhou, Dawei Chen, Shan Wang, Anmin Fu, and Yansong Gao. Research
    and challenge of distributed deep learning privacy and security attack(in chinese).
    Journal of Computer Research and Development, 58(5):927, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] Junxu Liu and Xiaofeng Meng. Survey on privacy-preserving machine learning(in
    chinese). Journal of Computer Research and Development, 57(2):346, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F Naughton. A methodology
    for formalizing model-inversion attacks. In 2016 IEEE 29th Computer Security Foundations
    Symposium (CSF), pages 355–370\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models
    under the gan: information leakage from collaborative deep learning. In Proceedings
    of the 2017 ACM SIGSAC conference on computer and communications security, pages
    603–618, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] Guangcan Mai, Kai Cao, Pong C Yuen, and Anil K Jain. On the reconstruction
    of face images from deep face templates. IEEE transactions on pattern analysis
    and machine intelligence, 41(5):1188–1202, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] Anton Razzhigaev, Klim Kireev, Edgar Kaziakhmedov, Nurislam Tursynbek,
    and Aleksandr Petiushko. Black-box face recovery from identity features. In European
    Conference on Computer Vision, pages 462–475. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] Anton Razzhigaev, Klim Kireev, Igor Udovichenko, and Aleksandr Petiushko.
    Darker than black-box: Face reconstruction from similarity queries. arXiv preprint
    arXiv:2106.14290, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu,
    and Xuyun Zhang. Membership inference attacks on machine learning: A survey. ACM
    Computing Surveys (CSUR), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy
    risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE
    31st computer security foundations symposium (CSF), pages 268–282\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael Backes.
    Ml-leaks: Model and data independent membership inference attacks and defenses
    on machine learning models. In Network and Distributed Systems Security Symposium
    2019. Internet Society, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier,
    and Hervé Jégou. White-box vs black-box: Bayes optimal strategies for membership
    inference. In International Conference on Machine Learning, pages 5558–5567\.
    PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Minxing Zhang, Zhaochun Ren, Zihan Wang, Pengjie Ren, Zhunmin Chen, Pengfei
    Hu, and Yang Zhang. Membership inference attacks against recommender systems.
    In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications
    Security, pages 864–879, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] Bo Hui, Yuchen Yang, Haolin Yuan, Philippe Burlina, Neil Zhenqiang Gong,
    and Yinzhi Cao. Practical blind membership inference attack via differential comparisons.
    In ISOC Network and Distributed System Security Symposium (NDSS), 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis
    of deep learning: Passive and active white-box inference attacks against centralized
    and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages
    739–753\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] Jiale Chen, Jiale Zhang, Yanchao Zhao, Hao Han, Kun Zhu, and Bing Chen.
    Beyond model-level membership privacy leakage: an adversarial approach in federated
    learning. In 2020 29th International Conference on Computer Communications and
    Networks (ICCCN), pages 1–9\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, and Xuyun Zhang.
    Source inference attacks in federated learning. In 2021 IEEE International Conference
    on Data Mining (ICDM), pages 1102–1107\. IEEE, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] Umang Gupta, Dimitris Stripelis, Pradeep K Lam, Paul Thompson, José Luis
    Ambite, and Greg Ver Steeg. Membership inference attacks on deep regression models
    for neuroimaging. In Medical Imaging with Deep Learning, pages 228–251\. PMLR,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] Anh Nguyen, Tuong Do, Minh Tran, Binh X Nguyen, Chien Duong, Tu Phan,
    Erman Tjiputra, and Quang D Tran. Deep federated learning for autonomous driving.
    arXiv preprint arXiv:2110.05754, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Davinder Kaur, Suleyman Uslu, Kaley J Rittichier, and Arjan Durresi.
    Trustworthy artificial intelligence: A review. ACM Computing Surveys (CSUR), 55(2):1–38,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] Luciano Floridi. Establishing the rules for building trustworthy ai.
    Nature Machine Intelligence, 1(6):261–262, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu,
    and Jianguo Li. Boosting adversarial attacks with momentum. In Proceedings of
    the IEEE conference on computer vision and pattern recognition, pages 9185–9193,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,
    and Adrian Vladu. Towards deep learning models resistant to adversarial attacks.
    arXiv preprint arXiv:1706.06083, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren,
    and Alan L Yuille. Improving transferability of adversarial examples with input
    diversity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pages 2730–2739, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to
    transferable adversarial examples by translation-invariant attacks. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4312–4321,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients
    give a false sense of security: Circumventing defenses to adversarial examples.
    In International conference on machine learning, pages 274–283\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
    Zoo: Zeroth order optimization based black-box attacks to deep neural networks
    without training substitute models. In Proceedings of the 10th ACM workshop on
    artificial intelligence and security, pages 15–26, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box
    adversarial attacks with limited queries and information. In International Conference
    on Machine Learning, pages 2137–2146\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Jonathan Uesato, Brendan O’donoghue, Pushmeet Kohli, and Aaron Oord.
    Adversarial risk and the dangers of evaluating against weak attacks. In International
    Conference on Machine Learning, pages 5025–5034\. PMLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing Gong. Nattack:
    Learning the distributions of adversarial examples for an improved black-box attack
    on deep neural networks. In International Conference on Machine Learning, pages
    3866–3876\. PMLR, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial
    attacks: Reliable attacks against black-box machine learning models. arXiv preprint
    arXiv:1712.04248, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, and
    Jun Zhu. Efficient decision-based black-box adversarial attacks on face recognition.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 7714–7722, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
