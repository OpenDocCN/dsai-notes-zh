- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:43:44'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:43:44
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2210.11237] Emerging Threats in Deep Learning-Based Autonomous Driving: A
    Comprehensive Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2210.11237] 基于深度学习的自动驾驶中的新兴威胁：综合调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2210.11237](https://ar5iv.labs.arxiv.org/html/2210.11237)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2210.11237](https://ar5iv.labs.arxiv.org/html/2210.11237)
- en: \WarningFilter
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: \WarningFilter
- en: Fancyhdris too small
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Fancyhdr过小。
- en: \tnotetext
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \tnotetext
- en: '[1]This document is the results of the research project funded by the National
    Natural Science Foundation of China: Research on secure data management mechanism
    of new college entrance examination comprehensive quality evaluation: A security
    enhancement of block chain empowerment(No. 72204077)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[1]本文件是中国国家自然科学基金资助的研究项目成果：新高考综合素质评价的安全数据管理机制研究：区块链赋能的安全增强（编号：72204077）。'
- en: \tnotetext
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \tnotetext
- en: '[2]This document is the results of the research project funded bt the Hubei
    Natural Science Foundation project:Research on the data security management mechanism
    of new College Entrance Examination comprehensive quality evaluation based on
    blockchain.(No. 2021CFB470)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[2]本文件是湖北省自然科学基金项目资助的研究成果：基于区块链的新高考综合素质评价数据安全管理机制研究。（编号：2021CFB470）'
- en: '[style=chinese]'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[style=chinese]'
- en: url]https://scholar.google.com/citations?user=1XoXUTYAAAAJ&hl=en
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: url]https://scholar.google.com/citations?user=1XoXUTYAAAAJ&hl=en
- en: '[style=chinese]'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[style=chinese]'
- en: '[style=chinese]'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '[style=chinese]'
- en: '[style=chinese]'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '[style=chinese]'
- en: '[style=chinese] \cormark[1]'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '[style=chinese] \cormark[1]'
- en: \cortext
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: \cortext
- en: '[cor1]Corresponding author \cortext[cor2]First two author equal contribution'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[cor1]通讯作者 \cortext[cor2]前两位作者等同贡献。'
- en: 'Emerging Threats in Deep Learning-Based Autonomous Driving: A Comprehensive
    Survey'
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于深度学习的自动驾驶中的新兴威胁：综合调查
- en: Cao Hui cao-hui@whu.edu.cn[    Zou Wenlong    Wang Yinkun    Song Ting    Liu
    Mengjun lmj_whu@163.com School of Education,Hubei University,Youyi Road No.368,Wuhan,430062,Hubei,P.R.China
    Institute of Information Engineering,Chinese Academy of Sciences, Beijing,100093,P.R.China
    School of Foreign Languages,Hubei University,Youyi Road,No.368, Wuhan,430062,Hubei,P.R.China
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 曹晖 cao-hui@whu.edu.cn[    邹文龙    王银坤    宋婷    刘孟君 lmj_whu@163.com 武汉大学教育学院，友谊路368号，武汉，430062，湖北，中国
    中国科学院信息工程研究所，北京，100093，中国 湖北大学外语学院，友谊路368号，武汉，430062，湖北，中国
- en: Abstract
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Since the 2004 DARPA Grand Challenge, the autonomous driving technology has
    witnessed nearly two decades of rapid development. Particularly, in recent years,
    with the application of new sensors and deep learning technologies extending to
    the autonomous field, the development of autonomous driving technology has continued
    to make breakthroughs. Thus, many carmakers and high-tech giants dedicated to
    research and system development of autonomous driving. However, as the foundation
    of autonomous driving, the deep learning technology faces many new security risks.
    The academic community has proposed deep learning countermeasures against the
    adversarial examples and AI backdoor, and has introduced them into the autonomous
    driving field for verification. Deep learning security matters to autonomous driving
    system security, and then matters to personal safety, which is an issue that deserves
    attention and research.This paper provides an summary of the concepts, developments
    and recent research in deep learning security technologies in autonomous driving.
    Firstly, we briefly introduce the deep learning framework and pipeline in the
    autonomous driving system, which mainly include the deep learning technologies
    and algorithms commonly used in this field. Moreover, we focus on the potential
    security threats of the deep learning based autonomous driving system in each
    functional layer in turn. We reviews the development of deep learning attack technologies
    to autonomous driving, investigates the State-of-the-Art algorithms, and reveals
    the potential risks. At last, we provides an outlook on deep learning security
    in the autonomous driving field and proposes recommendations for building a safe
    and trustworthy autonomous driving system.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 自2004年DARPA“大挑战”以来，自动驾驶技术经历了近二十年的快速发展。特别是近年来，随着新传感器和深度学习技术在自动驾驶领域的应用，这一技术的发展不断突破。因此，许多汽车制造商和高科技巨头致力于自动驾驶的研究和系统开发。然而，作为自动驾驶基础的深度学习技术面临许多新的安全风险。学术界提出了针对对抗样本和AI后门的深度学习对策，并将其引入自动驾驶领域进行验证。深度学习的安全性关系到自动驾驶系统的安全，进而关系到个人安全，这是一项值得关注和研究的问题。本文总结了自动驾驶领域深度学习安全技术的概念、发展及最新研究。首先，我们简要介绍了自动驾驶系统中的深度学习框架和流程，这主要包括该领域常用的深度学习技术和算法。此外，我们依次关注基于深度学习的自动驾驶系统在每个功能层的潜在安全威胁。我们回顾了深度学习攻击技术的发展，调查了最先进的算法，并揭示了潜在风险。最后，我们对自动驾驶领域的深度学习安全进行了展望，并提出了构建安全可信的自动驾驶系统的建议。
- en: 'keywords:'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: Trustworthy AI
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 可信赖的人工智能
- en: Deep Learning
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习
- en: Artificial Intelligence
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能
- en: Autonomous Driving
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶
- en: Cyber Security
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 网络安全
- en: Adversarial Examples
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本
- en: 1 Introduction
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Research about Autonomous Land Vehicles (ALVs) began as early as 1980s with
    funding from the US Department of Defense(DoD). In the 21st century, DARPA conducted
    the Grand Challenge that launched a new generation of autonomous driving. The
    development of artificial intelligence(AI) technology is driving the rapid progress
    of autonomous vehicles with an increasing expectation from the public. Currently,
    many traditional carmakers, such as universal Motors, Toyota, Volvo, BMW and Audi
    have carried out researches into the autonomous driving system. On another hand,
    not to be outdone, most of high-tech giants, Google Waymo, Tesla, Baidu and Huawei,
    devoted themselves to autonomous driving technology. Along with artificial intelligence
    technology, autonomous driving has seen rapid development and is expected to enter
    the practical stage.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 关于自动驾驶陆地车辆（ALVs）的研究早在1980年代就已开始，得到美国国防部（DoD）的资助。在21世纪，DARPA组织了“大挑战”比赛，推动了新一代自动驾驶的诞生。人工智能（AI）技术的发展推动了自动驾驶技术的迅速进步，公众对其期望也日益增加。目前，许多传统汽车制造商，如通用汽车、丰田、沃尔沃、宝马和奥迪，都在研究自动驾驶系统。另一方面，科技巨头如谷歌Waymo、特斯拉、百度和华为也积极投入自动驾驶技术的研究。随着人工智能技术的发展，自动驾驶取得了迅速的进展，预计将进入实际应用阶段。
- en: 'However, security is a major concern in the application of the autonomous driving
    system, because there are new types of security risks associated with autonomous
    driving system that depends heavily on deep learning. On the one hand, from the
    perspective of technical threat on AI security and privacy protection, new countermeasures
    have been proposed successively, including adversarial examples [[1](#bib.bib1),
    [2](#bib.bib2)], data poisoning and AI Backdoor[[3](#bib.bib3)], model extraction[[4](#bib.bib4)],
    model inversion[[5](#bib.bib5)], and membership privacy inference[[6](#bib.bib6)].
    On the another hand, from the social trust perspective of AI, issues about fairness,
    AI abuse, environment, compliance, and ethic, have also received attention and
    research. Currently, there is some literature[[7](#bib.bib7), [8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11), [12](#bib.bib12), [13](#bib.bib13)]
    summarized AI security threats in the general environment. Different from that,
    this paper focuses on the environment of autonomous driving system, it reveals
    the new security risks posed by AI technologies bringing new security challenges
    to autonomous driving. Unlike other applications of deep learning, the autonomous
    driving system is a more complex AI architecture consisting of dozens of functional
    modules, and different environment modules with different characteristics, raising
    different requirements for AI security attack and mitigation techniques, including:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，安全性是自主驾驶系统应用中的一个主要问题，因为依赖深度学习的自主驾驶系统伴随着新的安全风险。一方面，从 AI 安全和隐私保护的技术威胁角度来看，已经提出了一些新对策，包括对抗样本
    [[1](#bib.bib1), [2](#bib.bib2)]、数据中毒和 AI 后门 [[3](#bib.bib3)]、模型提取 [[4](#bib.bib4)]、模型反演
    [[5](#bib.bib5)] 和成员隐私推断 [[6](#bib.bib6)]。另一方面，从 AI 的社会信任角度来看，关于公平性、AI 滥用、环境、合规和伦理的问题也受到关注和研究。目前，有一些文献
    [[7](#bib.bib7), [8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11),
    [12](#bib.bib12), [13](#bib.bib13)] 总结了 AI 安全威胁在一般环境中的情况。与此不同的是，本文关注于自主驾驶系统的环境，揭示了
    AI 技术带来的新安全风险，为自主驾驶提出了新的安全挑战。与深度学习的其他应用不同，自主驾驶系统是一个更复杂的 AI 架构，由数十个功能模块和具有不同特性的环境模块组成，对
    AI 安全攻击和缓解技术提出了不同的要求，包括：
- en: •
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Physical world requirements. AI threats of autonomous driving system should
    be able to take effect in the real physical world, and not only in the digital
    world and computer simulation systems. Techniques specific to adversarial examples
    attacks in the physical world are the focus of this paper.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物理世界要求。自主驾驶系统的 AI 威胁应能够在真实的物理世界中生效，而不仅仅是在数字世界和计算机模拟系统中。针对物理世界中对抗样本攻击的技术是本文的重点。
- en: •
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Robustness requirements. The environment is uncertain and often varies to a
    large extent in autonomous driving. On the one hand, the images collected under
    different weather, light and other natural conditions can vary; on the other hand,
    changes in a long distance and large angle range also make image acquisition highly
    variable due to the high-speed movement of vehicles. Therefore, AI threats need
    to be able to take effect continuously and stably under a variety of conditions,
    which raises very high demands on the robustness of attacks, such as adversarial
    examples and AI backdoor. This paper focuses on robustness enhancement methods.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鲁棒性要求。自主驾驶中的环境是不确定的，并且通常变化很大。一方面，在不同天气、光线和其他自然条件下采集的图像可能会有所不同；另一方面，由于车辆的高速运动，远距离和大角度范围的变化也使得图像采集高度可变。因此，AI
    威胁需要能够在各种条件下持续稳定地生效，这对攻击的鲁棒性提出了很高的要求，例如对抗样本和 AI 后门。本文关注于鲁棒性增强方法。
- en: •
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fusion environment requirements. Autonomous driving system often employs multi-modal
    fusion sensing techniques that combine different types of information from multiple
    RGB cameras, LiDAR, RaDAR, etc., to sense the fused images. The autonomous driving
    environment requires that adversarial examples countermeasures and other related
    threat technologies can be stabilized to remain in effect in the fused environment.
    Artificial intelligence threats in multi-modal fusion environments are also the
    focus of this paper.
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 融合环境要求。自主驾驶系统通常采用多模态融合传感技术，结合来自多个 RGB 摄像头、激光雷达、雷达等不同类型的信息，以感知融合图像。自主驾驶环境要求对抗样本对策和其他相关威胁技术能够在融合环境中保持稳定生效。多模态融合环境中的人工智能威胁也是本文的重点。
- en: Due to the above concerns and requirements, AI safety technologies in the field
    of autonomous driving have continued to develop, and some research results and
    breakthroughs have been achieved. This paper introduces the latest research progress
    relevant to unique technologies and reveals the AI security risks in autonomous
    driving systems. This paper faces the above challenges of autonomous driving systems,
    rather than in the general environment. Section 1 briefly introduces the infrastructure
    and key technologies of AI in autonomous driving; section 2 offers a glimpse of
    the AI risks in the sensor layer; section 3 comprehensively reviewed the AI risk
    in the perception layer, introduced the idea and detail of important algorithms;
    section 4 provides the potential deep leaning risk and attack technology in decision
    layer in autonomous driving; section 5 focus on new threat of V2X that based on
    federation learning in the future; section 6 gives a summary and outlook.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 由于上述问题和要求，自动驾驶领域的AI安全技术不断发展，取得了一些研究成果和突破。本文介绍了与独特技术相关的最新研究进展，并揭示了自动驾驶系统中的AI安全风险。本文面对的是自动驾驶系统的挑战，而非一般环境。第1节简要介绍了自动驾驶中AI的基础设施和关键技术；第2节概述了传感器层中的AI风险；第3节全面回顾了感知层中的AI风险，介绍了重要算法的思想和细节；第4节提供了决策层中潜在的深度学习风险和攻击技术；第5节关注基于联邦学习的未来V2X的新威胁；第6节给出了总结和展望。
- en: 1.1 Basic Concepts of Autonomous Driving
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 自动驾驶的基本概念
- en: Essentially, autonomous driving is making driving decisions through artificial
    intelligence techniques or other automated decision-making methods. According
    to the Society of Automotive Engineers (SAE) standard J3016[[14](#bib.bib14)],
    autonomous driving can be categorized into the following classes.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，自动驾驶是通过人工智能技术或其他自动化决策方法来做出驾驶决策。根据美国汽车工程师学会（SAE）标准J3016[[14](#bib.bib14)]，自动驾驶可以分为以下几类。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'L0 – No Driving Automation: driving is carried out entirely by a person, but
    warnings and system assistance are available during the journey.'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L0 – 无驾驶自动化：驾驶完全由人工完成，但在行程中提供警告和系统辅助。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'L1 – Driver Assistance: based on the perception of the driving environment,
    only a single aspect of automation, which system operates the steering wheel or
    acceleration and deceleration assists the driver with ADAS, while other driving
    operations are performed by the human driver.'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L1 – 驾驶辅助：基于对驾驶环境的感知，仅涉及单一的自动化方面，即系统操作方向盘或加速和减速，辅助驾驶员进行ADAS操作，而其他驾驶操作由人工驾驶员完成。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'L2 – Partial Driving Automation: based on the perception of the driving environment,
    the system operates both the steering wheel and acceleration or deceleration.
    However, it requires a human driver to remain constantly alert and ready to take
    full control with little or no warning.'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L2 – 部分驾驶自动化：基于对驾驶环境的感知，系统操作方向盘和加速或减速。然而，它要求人工驾驶员保持持续警觉，并在几乎没有预警的情况下准备全面接管控制。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'L3 – Conditional Driving Automation: based on the perception of the driving
    environment, autonomous driving system can perform all driving operations under
    the supervision of a human driver.'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L3 – 有条件驾驶自动化：基于对驾驶环境的感知，自动驾驶系统可以在人工驾驶员监督下执行所有驾驶操作。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'L4 – High Driving Automation: under certain environmental conditions, autonomous
    driving system can perform all driving operations unsupervised.'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L4 – 高级驾驶自动化：在某些环境条件下，自动驾驶系统可以在无人监督的情况下执行所有驾驶操作。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'L5 – Full Driving Automation: the autonomous driving system can perform all
    driving operations unsupervised in all environmental conditions.'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: L5 – 完全驾驶自动化：在所有环境条件下，自动驾驶系统可以在无人监督的情况下执行所有驾驶操作。
- en: 'For autonomous driving system, there are different views and concepts, as well
    as different development and evolutionary routes. One is focus on intelligentization
    and cyberization of vehicle components, mainly researching on sensors, in-vehicle
    communication, vehicle-to-everything (V2X), and et al, which main participant
    by traditional car-makers. The other is focus on autonomous diving decisions,
    mainly researching artificial intelligence and autonomous driving, and the main
    participants include: UC Berkeley, Google WayMo, Baidu, Apollo, Intel Carla, NVIDIA
    and other artificial intelligence companies. However, whether it starts from the
    vehicle moving towards AI or the other way round, automated driving decision is
    the core mission in autonomous driving, and safety based on AI driving decisions
    making is a necessary prerequisite for the safety of autonomous driving system.
    The higher the level of autonomous driving, the higher the reliance on AI technology
    represented by deep learning, which lead to higher the requirements for the safety
    and robustness of deep learning itself.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于自动驾驶系统，有不同的视角和概念，以及不同的发展和演变路线。一种是专注于车辆组件的智能化和网络化，主要研究传感器、车载通信、车联网（V2X）等，主要由传统汽车制造商参与。另一种是专注于自动驾驶决策，主要研究人工智能和自动驾驶，主要参与者包括：加州大学伯克利分校、谷歌
    WayMo、百度、Apollo、Intel Carla、NVIDIA 以及其他人工智能公司。然而，无论是从车辆向 AI 发展还是反向发展，自动驾驶决策都是自动驾驶的核心任务，基于
    AI 驾驶决策的安全性是自动驾驶系统安全的必要前提。自动驾驶级别越高，对深度学习等 AI 技术的依赖越大，从而对深度学习本身的安全性和鲁棒性的要求也更高。
- en: 1.2 Architecture of Autonomous Driving System
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 自动驾驶系统架构
- en: In terms of the autonomous driving architecture and machine learning technologies
    based, autonomous driving systems can be divided into end-to-end (E2E) and modular
    architectures.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从自动驾驶架构和基于机器学习技术的角度来看，自动驾驶系统可以分为端到端（E2E）和模块化架构。
- en: '![Refer to caption](img/8aa4d0566f64fe4afda7d74cb6aa71a8.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8aa4d0566f64fe4afda7d74cb6aa71a8.png)'
- en: (a) Modular Autonomous Driving Framework
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 模块化自动驾驶框架
- en: '![Refer to caption](img/cda4813ff83678103cf71f1797445fd7.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/cda4813ff83678103cf71f1797445fd7.png)'
- en: (b) E2E Autonomous Driving Framework
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 端到端自动驾驶框架
- en: 'Figure 1: Autonomous Driving Framework'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：自动驾驶框架
- en: •
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The modular autonomous driving system divides an individual set of autonomous
    driving functions into several parts, each of which is completed by one or a group
    of artificial intelligence models, usually including: positioning and projecting,
    target recognition, trajectory prediction, road planning & driving decision making,
    vehicle control, and other functions. These functional modules contain the sensing
    layer, the perception layer, the decision layer and the vehicle networking layer.'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模块化自动驾驶系统将单独的自动驾驶功能集分成多个部分，每部分由一个或一组人工智能模型完成，通常包括：定位与投影、目标识别、轨迹预测、道路规划与驾驶决策、车辆控制和其他功能。这些功能模块包含感知层、感知层、决策层和车辆网络层。
- en: •
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The End-to-End autonomous driving system often consists of a large number of
    complex judgment functions in driving decisions performed by one or a group of
    artificial intelligence models that make the final driving decision based on the
    environment and cloud inputs.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 端到端自动驾驶系统通常由大量复杂的判断功能组成，这些功能由一个或一组人工智能模型执行，模型根据环境和云端输入做出最终驾驶决策。
- en: 1.3 Sensing Layer
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 感知层
- en: 'The sensing layer includes a variety of sensors that collect information about
    the environment for the autonomous driving system. Common sensors used in autonomous
    driving vehicles compromise RGB cameras, LiDAR (Light Detection and Ranging),
    RaDAR (Radio Waves to Determine the Distance), GPS, and ultrasonic sensors. Here
    are the characteristics of different sensors:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 感知层包括各种传感器，用于收集关于环境的信息，以支持自动驾驶系统。常见的自动驾驶车辆传感器包括 RGB 摄像头、激光雷达（LiDAR）、雷达（RaDAR）、GPS
    和超声波传感器。以下是不同传感器的特征：
- en: •
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The advantages of RGB cameras are: 1) lower cost, and 2) relatively mature
    recognition technology; their limitation is that the distance is dependent on
    estimation.'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: RGB 摄像头的优点是：1）成本较低，2）识别技术相对成熟；它们的局限性在于距离依赖于估算。
- en: •
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The advantage of LiDAR is that it is accurate; its limitation is that it is
    susceptible to interference from the weather.
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 激光雷达的优点是其精确；其局限性是容易受到天气干扰。
- en: •
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The advantage of radar is that it is relatively immune to weather interference;
    its limitation is that it has insufficient imaging capability.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 雷达的优点是相对免疫于天气干扰；其局限性是成像能力不足。
- en: There are a number of existing works that provide a detailed comparison of sensors
    for autonomous driving vehicles, which will not be the emphasis of this paper.
    There are some survey papers related to the sensing layer[[15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17)].
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 已有一些现有的研究详细比较了自动驾驶车辆的传感器，这不是本文的重点。相关的感知层调查论文有[[15](#bib.bib15), [16](#bib.bib16),
    [17](#bib.bib17)]。
- en: Most companies have chosen autonomous driving technology solutions that multi-modal
    fusion, while some have chosen solutions that rely primarily on RGB cameras. However,
    it needs to be emphasized that, regardless of the choice of sensor configuration
    solution, the various advanced sensors only fulfill the function of raw information
    collection and do not replace the key role played by artificial intelligence in
    the perception and decision-making of autonomous driving system, and are equally
    unable to avoid the new safety risks posed by AI.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数公司选择了多模态融合的自动驾驶技术解决方案，而一些公司则选择了主要依赖RGB摄像头的解决方案。然而，需要强调的是，无论选择何种传感器配置方案，各种先进的传感器只是实现原始信息收集的功能，并不能替代人工智能在自动驾驶系统中的感知和决策中的关键作用，也无法避免人工智能带来的新安全风险。
- en: 1.4 Perception Layer
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.4 感知层
- en: The perception layer perceives and identifies things like object perceiving
    and identification, segmentation, depth estimation and localization, which are
    based on the vehicle’s state and road information collected by the sensors in
    the sensor layer. The commonly used techniques are given as follows, which include
    2D object recognition, 3D object recognition, multi-modal fusion, trajectory prediction,
    and so on.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 感知层通过传感器层收集的车辆状态和道路信息来感知和识别物体，例如对象识别、分割、深度估计和定位。常用的技术包括2D物体识别、3D物体识别、多模态融合、轨迹预测等。
- en: '![Refer to caption](img/f586ee39c77a5c7424f1b827406a697f.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f586ee39c77a5c7424f1b827406a697f.png)'
- en: 'Figure 2: Sensor in Autonomous Driving'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：自动驾驶中的传感器
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '2D Objection Recognition is based on a flat image to identify the presence
    or absence of a specific target in the image and locate it. technologically, 2D
    object recognition can be divided into two classifications: two-stage objection
    recognition algorithms and one-stage objection recognition algorithms. The two-stage
    algorithms first find a series of region proposals, and then classify the objects
    in the proposals by Convolutional Neural Networks(CNN). Commonly used two-stage
    algorithms include FasterRCNN[[18](#bib.bib18)] and MaskRCNN[[19](#bib.bib19)]
    characterized by relatively high accuracy and high consumption. One-stage algorithms
    do not generate a separate region proposal but return to the predicted class and
    location of the target directly. Commonly used one-stage algorithms include: SSD[[20](#bib.bib20)]and
    Yolo v3[[21](#bib.bib21)]. In 2017, Lin et al.[[22](#bib.bib22)] proposed a new
    loss function - "Focal Loss", which can significantly improve the accuracy of
    dense target recognition, and this technique was first applied to the field of
    face recognition. It is now applied to many target recognition fields, among which,
    in 2021, Yosuke Shinya et al.[[23](#bib.bib23)] proposed UniverseNet, a target
    detection algorithm that applies Focal Loss, which can achieve better results
    in dense target and small target scenarios. A detailed comparison of current mainstream
    2D target recognition techniques can be found in references[[15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17)]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 2D目标识别基于平面图像来识别图像中是否存在特定目标并进行定位。从技术上讲，2D目标识别可以分为两类：两阶段目标识别算法和单阶段目标识别算法。两阶段算法首先找到一系列区域提议，然后通过卷积神经网络（CNN）对提议中的对象进行分类。常用的两阶段算法包括FasterRCNN[[18](#bib.bib18)]和MaskRCNN[[19](#bib.bib19)]，其特点是准确度较高但消耗较大。单阶段算法不会生成单独的区域提议，而是直接返回目标的预测类别和位置。常用的单阶段算法包括：SSD[[20](#bib.bib20)]和Yolo
    v3[[21](#bib.bib21)]。2017年，Lin等人[[22](#bib.bib22)]提出了一种新的损失函数——“Focal Loss”，该技术可以显著提高密集目标识别的准确性，最初应用于人脸识别领域。现在，它已应用于许多目标识别领域，其中，2021年，Yosuke
    Shinya等人[[23](#bib.bib23)]提出了UniverseNet，这是一种应用Focal Loss的目标检测算法，可以在密集目标和小目标场景中取得更好的结果。关于当前主流2D目标识别技术的详细比较可以在参考文献[[15](#bib.bib15),
    [16](#bib.bib16), [17](#bib.bib17)]中找到。
- en: •
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Multi-Modal Fusion. A single type of sensor cannot capture all of the environmental
    information needed to support autonomous driving, while autonomous driving systems
    require information from several types and a large number of sensors to make integrated
    decisions, which leads us to make multi-modal fusion. Depending on occurred times[[24](#bib.bib24)],
    the fusion can be divided into three modes: pre-fusion, post-fusion, and deep
    fusion. Pre-fusion combines the data collected by all types of sensors and then
    makes a comprehensive decision. Post-fusion to make decisions on the data collected
    by different sensors and then aggregate the sub-decisions. Deep fusion constitute
    by the fusion of data, features and decision integration, and can be subdivided
    into five types: data in data out, data in feature out, feature in feature out,
    feature in decision out, and decision in decision out[[25](#bib.bib25), [26](#bib.bib26)].
    An in-depth analysis and comparison of the various integration methods can be
    found in the literature.[[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28),
    [26](#bib.bib26)]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 多模态融合。单一类型的传感器无法捕捉支持自动驾驶所需的所有环境信息，而自动驾驶系统需要来自多种类型和大量传感器的信息来做出综合决策，这就引出了多模态融合。根据发生的时间[[24](#bib.bib24)]，融合可以分为三种模式：预融合、后融合和深度融合。预融合将所有类型传感器收集的数据结合起来，然后做出综合决策。后融合对不同传感器收集的数据进行决策，然后聚合子决策。深度融合由数据、特征和决策集成的融合组成，可以进一步细分为五种类型：数据输入数据输出、数据输入特征输出、特征输入特征输出、特征输入决策输出和决策输入决策输出[[25](#bib.bib25),
    [26](#bib.bib26)]。有关各种融合方法的深入分析和比较可以在文献[[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28),
    [26](#bib.bib26)]中找到。
- en: '![Refer to caption](img/ae86a2cf5ce8b735bbabdd81e19deb7f.png)'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/ae86a2cf5ce8b735bbabdd81e19deb7f.png)'
- en: (a) Pre-fusion
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (a) 预融合
- en: '![Refer to caption](img/59df5e78e80e8902394b6adfaf29392a.png)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/59df5e78e80e8902394b6adfaf29392a.png)'
- en: (b) Post-fusion
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (b) 融合后
- en: '![Refer to caption](img/af86df09c3344896b35a0087373c3451.png)'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![参见说明](img/af86df09c3344896b35a0087373c3451.png)'
- en: (c) Deep fusion
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: (c) 深度融合
- en: 'Figure 3: Fusion of Autonomous Driving'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3：自动驾驶的融合
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '3D Objection Detection and Segmentation. Because 2D images have no depth information
    that is needed in autonomous driving, such as path planning and collision avoidance
    in autonomous driving, therefore 3D objection detection plays a key role. Classified
    by the detected information, 3D target detection has 3 bases: 2D image, 3D point
    cloud map and multi-modal fusion image. Among them, 3D target detection based
    on 2D images often uses 3D target matching and depth estimation to estimate the
    3D target bounding box for targets in 2D images using algorithms like Mono3D[[29](#bib.bib29)],
    3DVP[[30](#bib.bib30)], Deepmanta[[31](#bib.bib31)], and SVGA-Net[[32](#bib.bib32)].
    3D target recognition based on 3D point cloud maps is the recognition of targets
    in the images with 3D information and marks the target outline. Commonly used
    algorithms include: VeloFCN[[33](#bib.bib33)] , BirdNet[[34](#bib.bib34)], 3DFCN[[35](#bib.bib35)]
    , PointNet++[[36](#bib.bib36)] and VoxelNet[[37](#bib.bib37)]. 3D target detection
    based on multi-modal integration images is to use different integration modes
    to identify 3D targets. Commonly used algorithms include: MV3D[[38](#bib.bib38)],
    AVOD[[39](#bib.bib39)], and F-PointNet[[40](#bib.bib40)]. A comparison and in-depth
    study of various 3D target detection algorithms can be found in the literature[[41](#bib.bib41),
    [42](#bib.bib42)].'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 3D 目标检测与分割。由于 2D 图像缺乏自动驾驶所需的深度信息，例如路径规划和碰撞避免，因此 3D 目标检测发挥了关键作用。根据检测到的信息，3D 目标检测有
    3 个基础：2D 图像、3D 点云地图和多模态融合图像。其中，基于 2D 图像的 3D 目标检测通常使用 3D 目标匹配和深度估计来估计 2D 图像中目标的
    3D 边界框，算法如 Mono3D[[29](#bib.bib29)]、3DVP[[30](#bib.bib30)]、Deepmanta[[31](#bib.bib31)]
    和 SVGA-Net[[32](#bib.bib32)]。基于 3D 点云地图的 3D 目标识别是识别图像中具有 3D 信息的目标并标记目标轮廓。常用的算法包括：VeloFCN[[33](#bib.bib33)]、BirdNet[[34](#bib.bib34)]、3DFCN[[35](#bib.bib35)]、PointNet++[[36](#bib.bib36)]
    和 VoxelNet[[37](#bib.bib37)]。基于多模态融合图像的 3D 目标检测是使用不同的融合模式来识别 3D 目标。常用的算法包括：MV3D[[38](#bib.bib38)]、AVOD[[39](#bib.bib39)]
    和 F-PointNet[[40](#bib.bib40)]。各种 3D 目标检测算法的比较和深入研究可以参考文献[[41](#bib.bib41), [42](#bib.bib42)]。
- en: Other deep learning research directions in the perception layer include Pedestrian
    Detection, Lane Detection, Traffic Sign Recognition, Pedestrian Attribute Recognition,
    Fast Vehicle Detection, Pedestrian Density Estimation, Plate Recognition, etc.
    There is detail on the leaderboard[[43](#bib.bib43)].
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 感知层中其他深度学习研究方向包括行人检测、车道检测、交通标志识别、行人属性识别、快速车辆检测、行人密度估计、车牌识别等。详细内容可见排行榜[[43](#bib.bib43)]。
- en: 1.5 Decision-Making Layer
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.5 决策层
- en: 'Driving decision-making is the core of autonomous driving, and machine learning
    methods are often used, with two technical routes available: Imitation Learning
    and Reinforcement Learning.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动决策是自动驾驶的核心，通常使用机器学习方法，主要有两种技术路线：模仿学习和强化学习。
- en: •
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Imitation Learning. Imitation learning refers to the learning behavior of agents
    who acquire the ability to perform a specific task by observing and imitating
    the behavior of human experts[[44](#bib.bib44)]. Imitation learning has been successful
    in the field of autonomous driving[[45](#bib.bib45)] Imitation learning tends
    to collect a large amount of environmental state $S_{i}$ (environmental data collected
    by various sensors, including 3D point cloud maps, RGB images, etc.) as features
    and record the actions performed by the human experts at the same time. $A_{i}$
    is used as a label to form a training data set $D:{(s1,a1),(s2,a2),(s3,a3),...}$.
    Using specific imitation learning algorithms, artificial intelligence models are
    trained and used to make future driving decisions. The famous imitation learning
    methods include the E2E autonomous driving algorithm based on conditional imitation
    learning [[46](#bib.bib46)], and the ChauffeurNet[[47](#bib.bib47)].
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模仿学习。模仿学习指的是通过观察和模仿人类专家的行为来获取执行特定任务能力的学习行为[[44](#bib.bib44)]。模仿学习在自动驾驶领域取得了成功[[45](#bib.bib45)]。模仿学习通常会收集大量的环境状态
    $S_{i}$（由各种传感器收集的环境数据，包括 3D 点云地图、RGB 图像等）作为特征，并同时记录人类专家所执行的动作。$A_{i}$ 被用作标签，形成训练数据集
    $D:{(s1,a1),(s2,a2),(s3,a3),...}$。使用特定的模仿学习算法来训练人工智能模型，并用于做出未来的驾驶决策。著名的模仿学习方法包括基于条件模仿学习的
    E2E 自动驾驶算法 [[46](#bib.bib46)]，以及 ChauffeurNet[[47](#bib.bib47)]。
- en: •
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Deep Reinforcement Learning. Deep reinforcement learning simulates the self-learning
    model of organisms in nature. To be concrete, an agent monitors its own behavior
    and the resulting environmental changes, sets the reward value for different changes,
    and then continuously optimizes the model and its own behavior based on this.
    In 2013, Mnih et al.[[48](#bib.bib48)] combined deep learning with reinforcement
    learning and proposed the Deep Q Learning(DQN) method. DQN is based on a set of
    Q values in a reward table. The system’s driving status $S_{i}$ and the driving
    operation $a_{i}$ to obtain the corresponding reward value $r_{i}$, which automatically
    generates training data $D:{((s1,a1),r1),((s2,a2),r2),((s3,a3),r3),...}$. The
    reinforcement learning model is then trained by specific algorithms, while reinforcement
    learning is supplemented with current operational data to continuously optimize
    the model. Nowadays, deep reinforcement learning has been rapidly developed and
    widely used, with subsequently emerged Deep Recurrent Q Networks (DRQNs)[[49](#bib.bib49)],
    attention mechanism deep recurrent Q networks[[50](#bib.bib50)], asynchronous/synchronous
    dominant actor-critic (A3C/A2C)[[51](#bib.bib51)], and reinforcement learning
    for unsupervised and unassisted tasks[[52](#bib.bib52)], which are widely used
    in e-Sports, health & medicine, recommendation system and other fields. There
    are some surveys of deep reinforcement learning[[53](#bib.bib53), [54](#bib.bib54)].
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度强化学习。深度强化学习模拟了自然界中生物的自学习模型。具体而言，代理监测自身行为及其导致的环境变化，为不同变化设定奖励值，然后基于此不断优化模型和自身行为。2013年，Mnih等人[[48](#bib.bib48)]将深度学习与强化学习结合，提出了深度Q学习（DQN）方法。DQN基于奖励表中的一组Q值。系统的驾驶状态
    $S_{i}$ 和驾驶操作 $a_{i}$ 以获得相应的奖励值 $r_{i}$，从而自动生成训练数据 $D:{((s1,a1),r1),((s2,a2),r2),((s3,a3),r3),...}$。然后，通过特定算法训练强化学习模型，同时用当前操作数据补充强化学习，以不断优化模型。如今，深度强化学习已迅速发展并得到广泛应用，随后出现了深度递归Q网络（DRQNs）[[49](#bib.bib49)]、注意力机制深度递归Q网络[[50](#bib.bib50)]、异步/同步主导演员-评论家（A3C/A2C）[[51](#bib.bib51)]，以及用于无监督和无辅助任务的强化学习[[52](#bib.bib52)]，这些技术广泛应用于电子竞技、健康与医学、推荐系统等领域。有一些关于深度强化学习的调查[[53](#bib.bib53),
    [54](#bib.bib54)]。
- en: A variety of deep reinforcement learning frameworks and algorithms are widely
    used in the field of autonomous driving vehicles. For example, Feng et al.[[55](#bib.bib55)],
    Alizadeh et al.[[56](#bib.bib56)], Mirchevska et al.[[57](#bib.bib57)], and Quek
    et al.[[58](#bib.bib58)] apply deep reinforcement learning techniques to driving
    decisions; Holen et al.[[59](#bib.bib59)] use deep reinforcement learning for
    autonomous driving roadway recognition; Feng et al.[[60](#bib.bib60)] utilize
    deep reinforcement learning techniques for traffic light optimization control.
    Some researchers have also proposed an autonomous driving solution with the fusion
    of imitation learning and reinforcement learning[[61](#bib.bib61), [62](#bib.bib62)].
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 各种深度强化学习框架和算法在自动驾驶领域得到了广泛应用。例如，Feng等人[[55](#bib.bib55)]、Alizadeh等人[[56](#bib.bib56)]、Mirchevska等人[[57](#bib.bib57)]和Quek等人[[58](#bib.bib58)]将深度强化学习技术应用于驾驶决策；Holen等人[[59](#bib.bib59)]利用深度强化学习进行自动驾驶道路识别；Feng等人[[60](#bib.bib60)]则将深度强化学习技术用于交通信号灯优化控制。一些研究人员还提出了一种结合模仿学习和强化学习的自动驾驶解决方案[[61](#bib.bib61),
    [62](#bib.bib62)]。
- en: 1.6 Vehicle Networks
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.6 车辆网络
- en: With the development of communications and AI technology, vehicle networks are
    increasingly playing an important role in autonomous driving, especially the vehicle
    networks construction, which supports a distributed AI model and provides a novel
    type of AI technology in autonomous driving, while also bringing new security
    risks.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 随着通信和人工智能技术的发展，车辆网络在自动驾驶中扮演着越来越重要的角色，尤其是车辆网络建设，它支持分布式AI模型，并为自动驾驶提供了一种新型AI技术，同时也带来了新的安全风险。
- en: •
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Vehicle-to-Everything (V2X). V2X is a multi-layered network system designed
    to enhance collaboration between pedestrians, vehicles and transport infrastructure.
    It is universally composed of Vehicle-to-Vehicle (V2V) networks, Vehicle-to-Infrastructure
    (V2I) networks, Vehicle-to-Pedestrian (V2P) networks and Vehicle-to-Road side
    units (V2R) networks[[63](#bib.bib63)]. The communication technologies used in
    the vehicular internet of things can be broadly classified into two categories,
    Dedicated Short Range Communication (DSRC) and Long-Term Evolution (LTE) cellular
    communication, called cellular-V2X or C-V2X for short[[64](#bib.bib64)].
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 车对一切（V2X）。V2X是一个多层次的网络系统，旨在增强行人、车辆和交通基础设施之间的协作。它通常由车对车（V2V）网络、车对基础设施（V2I）网络、车对行人（V2P）网络和车对路边单元（V2R）网络组成[[63](#bib.bib63)]。车联网中使用的通信技术可以大致分为两类，专用短程通信（DSRC）和长期演进（LTE）蜂窝通信，简称为蜂窝-V2X或C-V2X[[64](#bib.bib64)]。
- en: •
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Federated Learning. The vehicular internet of things provides the network foundation
    for distributed artificial intelligence. Federated Learning is a distributed AI
    framework that replaces sensitive data interactions with model interactions, enabling
    more efficient and better privacy for knowledge sharing and transition. Based
    on the V2X, the federated learning can provide distributed and interactive AI
    services[[65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67)] for autonomous
    driving system. This paper focuses on the novel security risks posed by Federated
    Learning in the vehicular internet of things, and reviews related security technology
    developments.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 联邦学习。车联网为分布式人工智能提供了网络基础。联邦学习是一个分布式的AI框架，它用模型交互替代了敏感数据的交互，从而实现了更高效和更好的隐私保护的知识共享与转移。基于V2X，联邦学习可以为自动驾驶系统提供分布式和交互式AI服务[[65](#bib.bib65),
    [66](#bib.bib66), [67](#bib.bib67)]。本文重点讨论了联邦学习在车联网中带来的新型安全风险，并回顾了相关的安全技术发展。
- en: 1.7 Summary
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.7 总结
- en: We concluded the major AI application used in autonomous driving in Table1
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在表1中总结了自动驾驶中使用的主要AI应用
- en: 'Table 1: Major Deep Learning-based Tasks in Autonomous Driving'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '表1: 自动驾驶中的主要深度学习任务'
- en: '| Layer | Task | Major typical deep learning algorithm |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 层 | 任务 | 主要典型深度学习算法 |'
- en: '| Sensor | 3D PointCloud Registration | 3DFeat-Net[[68](#bib.bib68)], FCGF[[69](#bib.bib69)],
    D3Feat-pred[[70](#bib.bib70)] |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 传感器 | 3D点云配准 | 3DFeat-Net[[68](#bib.bib68)], FCGF[[69](#bib.bib69)], D3Feat-pred[[70](#bib.bib70)]
    |'
- en: '|  | Pre-Fusion | Multi-Frame Fusion[[71](#bib.bib71)], MTF4VT[[72](#bib.bib72)],TransFuser[[73](#bib.bib73)],DeepFusion[[74](#bib.bib74)]
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '|  | 预融合 | Multi-Frame Fusion[[71](#bib.bib71)], MTF4VT[[72](#bib.bib72)],
    TransFuser[[73](#bib.bib73)], DeepFusion[[74](#bib.bib74)] |'
- en: '| Perception | 2D Object Detection | Fast-RCNN[[75](#bib.bib75)], Faster R-CNN[[75](#bib.bib75)],
    Mask R-CNN[[19](#bib.bib19)], D-RFCN[[76](#bib.bib76)], Yolov4[[77](#bib.bib77)],
    YOLOv7[[78](#bib.bib78)],FD-SwinV2[[79](#bib.bib79)] |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 感知 | 2D物体检测 | Fast-RCNN[[75](#bib.bib75)], Faster R-CNN[[75](#bib.bib75)],
    Mask R-CNN[[19](#bib.bib19)], D-RFCN[[76](#bib.bib76)], Yolov4[[77](#bib.bib77)],
    YOLOv7[[78](#bib.bib78)], FD-SwinV2[[79](#bib.bib79)] |'
- en: '|  | 3D Object Detection | PointRCNN[[80](#bib.bib80)], PV-RCNN[[81](#bib.bib81)],Se-SSD[[82](#bib.bib82)],
    GLENet-VR[[83](#bib.bib83)] |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '|  | 3D物体检测 | PointRCNN[[80](#bib.bib80)], PV-RCNN[[81](#bib.bib81)], Se-SSD[[82](#bib.bib82)],
    GLENet-VR[[83](#bib.bib83)] |'
- en: '|  | Lane Detection | SCNN[[84](#bib.bib84)] LaneATT[[85](#bib.bib85)] CLRNet[[86](#bib.bib86)]
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | 车道检测 | SCNN[[84](#bib.bib84)], LaneATT[[85](#bib.bib85)], CLRNet[[86](#bib.bib86)]
    |'
- en: '|  | Traffic Sign Recognition | CNN with 3 Spatial Transformer[[87](#bib.bib87)],
    Mask R-CNN with adaptations and augmentations[[19](#bib.bib19)], TSR-SA[[88](#bib.bib88)]
    |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '|  | 交通标志识别 | CNN with 3 Spatial Transformer[[87](#bib.bib87)], Mask R-CNN
    with adaptations and augmentations[[19](#bib.bib19)], TSR-SA[[88](#bib.bib88)]
    |'
- en: '|  | Fast Vehicle Detection | YOLOv3-tiny[[89](#bib.bib89)], LittleYolo-SPP[[90](#bib.bib90)]
    |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | 快速车辆检测 | YOLOv3-tiny[[89](#bib.bib89)], LittleYolo-SPP[[90](#bib.bib90)]
    |'
- en: '|  | Pedestrian detection | SA-FastRCNN[[91](#bib.bib91)],RPN+BF[[92](#bib.bib92)],Pedestron[[93](#bib.bib93)],
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | 行人检测 | SA-FastRCNN[[91](#bib.bib91)], RPN+BF[[92](#bib.bib92)], Pedestron[[93](#bib.bib93)]
    |'
- en: '|  | Semantic Segmentation | FCN[[94](#bib.bib94)], PSPNet[[95](#bib.bib95)],
    DRAN[[96](#bib.bib96)],Swin trasformer[[97](#bib.bib97)], ViT-Adapter[[98](#bib.bib98)]
    |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | 语义分割 | FCN[[94](#bib.bib94)], PSPNet[[95](#bib.bib95)], DRAN[[96](#bib.bib96)],
    Swin transformer[[97](#bib.bib97)], ViT-Adapter[[98](#bib.bib98)] |'
- en: '|  | Object Tracking | M2-Track[[99](#bib.bib99)],BAT[[100](#bib.bib100)] |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | 物体跟踪 | M2-Track[[99](#bib.bib99)], BAT[[100](#bib.bib100)] |'
- en: '|  | Multiple Object Tracking | QDTrack[[98](#bib.bib98)], RetinaTrack[Lu2020retinatrack]
    |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  | 多目标跟踪 | QDTrack[[98](#bib.bib98)], RetinaTrack[Lu2020retinatrack] |'
- en: '| Decision | Trajectory Prediction | NSP-SFM[[101](#bib.bib101)], Y-Net[[102](#bib.bib102)],Trajectron++[[103](#bib.bib103)],Social
    GAN[[104](#bib.bib104)],SoPhie[[105](#bib.bib105)] |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 决策 | 轨迹预测 | NSP-SFM[[101](#bib.bib101)], Y-Net[[102](#bib.bib102)], Trajectron++[[103](#bib.bib103)],
    Social GAN[[104](#bib.bib104)], SoPhie[[105](#bib.bib105)] |'
- en: '|  | Motion Forecasting | VI LaneIter[[106](#bib.bib106)], Wayformer[[107](#bib.bib107)]
    |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | 运动预测 | VI LaneIter[[106](#bib.bib106)], Wayformer[[107](#bib.bib107)]
    |'
- en: '|  | Deep Reinforcement Learning | Deep Q-Learning[[108](#bib.bib108)]),Deep
    recurrent q-learning[[49](#bib.bib49)],Deep attention recurrent Q-network[[50](#bib.bib50)],Double
    Q-learning[[109](#bib.bib109)],A3C/A2C[[51](#bib.bib51)] |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | 深度强化学习 | 深度Q学习[[108](#bib.bib108)]), 深度递归Q学习[[49](#bib.bib49)], 深度注意力递归Q网络[[50](#bib.bib50)],
    双重Q学习[[109](#bib.bib109)], A3C/A2C[[51](#bib.bib51)] |'
- en: '|  | Imitation Learning | Generative adversarial imitation learning[[110](#bib.bib110)],
    Conditional Imitation Learning[[46](#bib.bib46), [111](#bib.bib111)], Self-Imitation
    Learning[[112](#bib.bib112)], Chauffeurnet[[47](#bib.bib47)] |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | 模仿学习 | 生成对抗模仿学习[[110](#bib.bib110)], 条件模仿学习[[46](#bib.bib46), [111](#bib.bib111)],
    自我模仿学习[[112](#bib.bib112)], Chauffeurnet[[47](#bib.bib47)] |'
- en: '| V2X | Federated Learning | FedAvg[[113](#bib.bib113)] |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| V2X | 联邦学习 | FedAvg[[113](#bib.bib113)] |'
- en: 2 Emerging Threats in Sensors
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 传感器中的新兴威胁
- en: Sensors are foundational part for the autonomous driving system, which provide
    raw environmental information for autonomous driving decision-making. The security
    of sensors directly affects the safety of autonomous driving system. We classify
    attacks against sensors into two categories, where attacks that aim to compromise
    the usability of the sensing are classified as Jamming Attacks and attacks that
    aim to compromise the integrity of the information collected by the sensors are
    classified as Spoofing Attacks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器是自动驾驶系统的基础部分，为自动驾驶决策提供原始环境信息。传感器的安全性直接影响自动驾驶系统的安全。我们将针对传感器的攻击分为两类，其中旨在破坏传感器可用性的攻击被归类为**干扰攻击**，旨在破坏传感器收集信息完整性的攻击被归类为**伪造攻击**。
- en: 2.1 Jamming Attacks
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 干扰攻击
- en: The Jamming Attack means that attackers take some actions to reduce the quality
    of data collected by the sensor, even making sensor unavailable. In 2015, Petit
    et al.[[114](#bib.bib114)] attempted a jamming attack on autonomous driving sensors
    by artificially setting up bright light interference that could "blind" the camera.
    In 2016, Yan et al.[[115](#bib.bib115)] experimented with blind attacks on ultrasonic
    sensors. Similarly, a variety of in-vehicle sensors such as RGB cameras, LiDAR,
    RaDAR, gyroscopic sensors and GPS sensors could be subject to jamming attacks[[116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119)].
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**干扰攻击**是指攻击者采取某些行动以降低传感器收集数据的质量，甚至使传感器无法使用。2015年，Petit等人[[114](#bib.bib114)]尝试通过人工设置强光干扰来对自动驾驶传感器进行干扰攻击，可能“使相机失明”。2016年，Yan等人[[115](#bib.bib115)]对超声波传感器进行了盲目攻击实验。同样，各种车载传感器，如RGB相机、激光雷达、雷达、陀螺仪传感器和GPS传感器，都可能受到干扰攻击[[116](#bib.bib116),
    [117](#bib.bib117), [118](#bib.bib118), [119](#bib.bib119)]。'
- en: 2.2 Spoofing Attacks
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 伪造攻击
- en: The Spoofing Attacks means that attackers injecting fake signals to affect the
    normal behaviour of the autonomous driving system. In 2015, Petit et al.[[114](#bib.bib114)]
    attempted to send specific spoofed laser signals, causing the LiDAR systems to
    be misled. Later, Park et al.[[120](#bib.bib120)] conducted similar experiments
    on in-vehicle IR sensors. Yan et al.[[115](#bib.bib115)] worked on gyroscopic
    sensors and RaDAR. Nassi et al.[[121](#bib.bib121)] conducted combined experiments
    on RGB cameras, LiDAR and RaDAR. Psiaki et al.[[122](#bib.bib122)], Meng et al.[[123](#bib.bib123)]
    conducted spoofing experiments on GPS for multiple environments.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**伪造攻击**是指攻击者注入虚假信号，以影响自动驾驶系统的正常行为。2015年，Petit等人[[114](#bib.bib114)]尝试发送特定伪造激光信号，导致激光雷达系统被误导。之后，Park等人[[120](#bib.bib120)]对车载红外传感器进行了类似实验。Yan等人[[115](#bib.bib115)]研究了陀螺仪传感器和雷达。Nassi等人[[121](#bib.bib121)]对RGB相机、激光雷达和雷达进行了综合实验。Psiaki等人[[122](#bib.bib122)],
    Meng等人[[123](#bib.bib123)]则在多种环境下对GPS进行了伪造实验。'
- en: Currently, most attacks against sensors of vehicle are trend towards physical
    attack rather than attack on deep learning. In this paper, we only give a general
    overview, there are more details in the surveys[[17](#bib.bib17), [15](#bib.bib15)].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，大多数针对车辆传感器的攻击趋向于物理攻击，而非深度学习攻击。本文仅提供一般概述，更多细节请参见相关调查[[17](#bib.bib17), [15](#bib.bib15)]。
- en: 3 Emerging Threats in Perceptual Layer
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 感知层中的新兴威胁
- en: Based on the information captured by various types of sensors in the sensor
    layer, the perception layer performs recognition and perception. These tasks,
    such as objective recognition, segmentation, and depth estimation, are often difficult
    to accomplish through simple computing based on some certain rules. Artificial
    intelligence is also subject to new types of security threats. For example, attackers
    could use Adversarial Examples or AI Backdoor attacks, which can mislead to wrong
    predictions that be controlled by attackers, which leads to dangerous driving
    decisions. Attackers may also use Model Extraction to obtain the parameters or
    Hyper-parameters of the AI model, resulting in model leakage and loss of intellectual
    property. And then attackers would use Model Inversion or Membership Privacy attacks
    leading to sensitive training data leakage and privacy risks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于传感器层捕获的各种类型的传感器信息，感知层执行识别和感知。这些任务，如目标识别、分割和深度估计，通常难以通过基于某些规则的简单计算完成。人工智能也面临新的安全威胁。例如，攻击者可能使用对抗样本或AI后门攻击，这可能误导模型产生攻击者控制的错误预测，进而导致危险的驾驶决策。攻击者还可能使用模型提取技术来获取AI模型的参数或超参数，导致模型泄漏和知识产权损失。随后，攻击者可能会使用模型反演或成员隐私攻击，导致敏感训练数据泄漏和隐私风险。
- en: Different from some existing surveys that introduce general adversarial examples
    or AI backdoors in cyberspace, this paper focuses on advanced research in the
    physical world. Attacking in the physical world has faced higher demands, especially,
    on attack constancy, high success rate, and robustness on environmental uncertainties.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 与一些现有的调查不同，这些调查介绍了网络空间中的一般对抗样本或AI后门，本文专注于物理世界中的高级研究。物理世界中的攻击面临更高的要求，特别是在攻击一致性、高成功率和对环境不确定性的鲁棒性方面。
- en: 3.1 Physical World Adversarial Examples for Static Objective Detection
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 静态目标检测中的物理世界对抗样本
- en: In 2014, researchers discovered that adding a small amount of specific interference,
    which is imperceptible to human beings, may still cause machine learning to be
    misled by attackers. This could cause serious security even safety problem, if
    machine learning be applied to a critical domain. Such an attack is known as adversarial
    examples attack and can be formalized as
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，研究人员发现，添加少量对人类不可察觉的特定干扰，仍然可能导致机器学习被攻击者误导。这可能在机器学习应用于关键领域时造成严重的安全甚至安全问题。这种攻击被称为对抗样本攻击，可以形式化为
- en: '|  | $argmin_{x^{\prime}}\left\&#124;x^{\prime}-x\right\&#124;_{p}s.t.f(x^{\prime})=\hat{y}$
    |  | (1) |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|  | $argmin_{x^{\prime}}\left\|x^{\prime}-x\right\|_{p} \text{s.t.} f(x^{\prime})=\hat{y}$
    |  | (1) |'
- en: where $f$ denotes a machine learning model, $x$ denotes a test example, and
    $x^{\prime}$ denotes an adversarial example generated based on the addition of
    a small amount of interference, with $c$ for the prediction result of the model
    for a normal example, and $c^{\prime}$ for the prediction result of the model
    on the adversarial examples.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 表示一个机器学习模型，$x$ 表示一个测试样本，$x^{\prime}$ 表示基于添加少量干扰生成的对抗样本，$c$ 为模型对正常样本的预测结果，$c^{\prime}$
    为模型对对抗样本的预测结果。
- en: 'After the first work, adversarial examples technique has been widely studied
    and has seen rapid development. FGSM algorithm proposed by Goodfellow et al.[[2](#bib.bib2)]
    FGSM works by calculating the gradient of the loss function between the input
    and target classification and creating a small perturbation in terms of the sign
    vector coefficient of this gradient as:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在首次工作之后，对抗样本技术已被广泛研究并快速发展。Goodfellow等人提出的FGSM算法[[2](#bib.bib2)] FGSM通过计算输入和目标分类之间损失函数的梯度，并在该梯度的符号向量系数上创建小的扰动，具体如下：
- en: '|  | $x_{Adv}=x+\alpha sign(\triangledown_{x}J(x,y))$ |  | (2) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{Adv}=x+\alpha \text{sign}(\nabla_{x}J(x,y))$ |  | (2) |'
- en: where $x_{adv}$ denotes the corresponding adversarial example of $x$, $\alpha$
    is a specific constant, $sign()$ is a sign function, $y_{true}$ is the the corresponding
    true label of $x$, $J()$ denotes the loss function used to train the model, and
    $\triangledown_{x}$ denotes the the gradient of $x$. The algorithm can be used
    to add a certain amount of adversarial noise to an image that is normally predicted
    as a panda, so that the machine learning model can predicted it as a gibbon with
    a high confidence. This algorithm could achieve both untargeted attacks and targeted
    attacks. There are more concerned adversarial examples algorithms be proposed
    in following. In 2016, Papernot et al.[[124](#bib.bib124)] proposed a black-box
    attack using an alternative model approach. In 2017, Moosavi-Dezfooli et al.[[125](#bib.bib125)]
    proposed an universal adversarial example attack, where a particular adversarial
    interference is able to influence the classification of multiple or even all examples
    in machine learning. In the same year, Carlini et al.[[126](#bib.bib126)] proposed
    an optimization-based C&W algorithm to improve the adversarial examples attack.
    In 2018, Zhao et al.[[127](#bib.bib127)] found that not need to be filled with
    artificial interference, but found different distribution in nature and can cause
    misclassification of machine learning models, which is called Natural Adversarial
    Examples.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{adv}$ 表示 $x$ 的对应对抗样本，$\alpha$ 是一个特定常数，$sign()$ 是符号函数，$y_{true}$ 是 $x$
    的真实标签，$J()$ 表示用于训练模型的损失函数，$\triangledown_{x}$ 表示 $x$ 的梯度。该算法可以在正常预测为熊猫的图像中添加一定量的对抗噪声，使得机器学习模型可以以较高的置信度将其预测为长臂猿。该算法可以实现无目标攻击和有目标攻击。接下来会提出更多关注的对抗样本算法。2016
    年，Papernot 等人[[124](#bib.bib124)] 提出了使用替代模型方法的黑箱攻击。2017 年，Moosavi-Dezfooli 等人[[125](#bib.bib125)]
    提出了通用对抗样本攻击，其中一种特定的对抗干扰能够影响机器学习中多个甚至所有样本的分类。同年，Carlini 等人[[126](#bib.bib126)]
    提出了基于优化的 C&W 算法以改进对抗样本攻击。2018 年，Zhao 等人[[127](#bib.bib127)] 发现不需要填充人工干扰，而是发现自然界中不同的分布可以导致机器学习模型的误分类，这被称为自然对抗样本。
- en: In the real physical world, the environment of autonomous driving is more complex,
    so there are more challenges for attacker to generate adversarial examples in
    the physical-world[[128](#bib.bib128)]. The requirements of adversarial examples
    in the physical world are as follows.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在真实物理世界中，自动驾驶的环境更加复杂，因此攻击者在物理世界中生成对抗样本面临更多挑战[[128](#bib.bib128)]。物理世界中对抗样本的要求如下。
- en: •
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Physical generatible. In the physical world, it is insufficient that only adding
    perturbations via cyber space. Such perturbations must be capable of being physically
    generated by printing, 3D printing or spraying, etc.
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物理可生成。在物理世界中，仅通过网络空间添加扰动是不够的。这些扰动必须能够通过打印、3D 打印或喷涂等方式物理生成。
- en: •
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Local generatible. The local nature of the adversarial example. In an adversarial
    example of the digital world, the attacker can add perturbations to any pixel
    within the range of the image; however, in a physical world attack, there are
    often only local areas of the target that are available, and in many cases, the
    background areas of the image are difficult to use to generate a physical world
    adversarial example.
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 局部可生成。对抗样本的局部特性。在数字世界的对抗样本中，攻击者可以在图像范围内的任何像素添加扰动；然而，在物理世界攻击中，通常只有目标的局部区域是可用的，且在许多情况下，图像的背景区域很难用来生成物理世界的对抗样本。
- en: •
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Robustness. In the physical world, especially in the field of autonomous driving,
    it is often required that the adversarial examples can continuously produce misleading
    effects on machine learning models during multiple angle changes. At the same
    time, the adversarial examples need to be continuously effective against mainstream
    target recognition algorithms under certain distance and angle ranges, multiple
    natural environments, and multiple resolution sensor devices. This puts forward
    higher requirements on the persistence and universality of adversarial examples,
    which greatly increases the complexity of the generation of adversarial examples.
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 鲁棒性。在物理世界中，特别是在自动驾驶领域，通常要求对抗样本能够在多个角度变化中持续对机器学习模型产生误导性效果。同时，对抗样本需要在一定距离和角度范围、多个自然环境以及多个分辨率传感器设备下对主流目标识别算法持续有效。这对对抗样本的持久性和普遍性提出了更高的要求，极大地增加了对抗样本生成的复杂性。
- en: 'Therefore, more adversarial example robustness enhancement measures are often
    required to realize the physical world adversarial example attacks; and the specific
    measures applied to vary according to different scenarios and attack targets.
    In accordance with the scenarios and objects for which countermeasures are set,
    this paper divides the recognition targets in the autonomous driving system into
    three categories: vehicles including various motor vehicles, pedestrians such
    as walkers and cyclists, and static targets like road facilities, traffic signs,
    markings, roadside advertising signs and other static objects.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为实现物理世界的对抗样本攻击，通常需要更多的对抗样本鲁棒性增强措施；具体采取的措施根据不同的场景和攻击目标而有所不同。根据设定对策的场景和对象，本文将自动驾驶系统中的识别目标分为三类：包括各种机动车辆的车辆、步行者和骑行者等行人，以及道路设施、交通标志、标线、路边广告牌等静态目标。
- en: 'Thus, at present, the physical world in the autonomous driving field universally
    has 3 types of targets for adversarial example techniques: static objective recognition,
    pedestrian recognition, and vehicle recognition.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目前在自动驾驶领域，物理世界普遍存在3种对抗样本技术的目标：静态目标识别、行人识别和车辆识别。
- en: •
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Physical world adversarial examples for static targets. Such attack targets
    include a variety of static target recognition systems such as traffic signs,
    traffic signals, and traffic markings, which are characterized by the requirement
    that the adversarial examples can continuously and steadily interfere with the
    judgments of machine learning models over a large range of angles and distances.
    In 2017, Lu et al.[[129](#bib.bib129)] successfully performed adversarial example
    generation in the physical world for the popular objective recognition algorithm
    FasterRCNN. To achieve better distance and angle range adaptation, in 2018, Eykholt
    et al.[[130](#bib.bib130)] proposed the Robust Physical Perturbations (RP2) algorithm.
    In the same year, Chen et al.[[131](#bib.bib131)] adopted the Expectation over
    Transformation (EoT) method[[132](#bib.bib132)] to improve the generation of adversarial
    examples for traffic signs, resulting in improved adaptability of the adversarial
    examples to distance, angle, light and other environments. In 2019, Zhao et al.[[133](#bib.bib133)]
    proposed the feature-interference reinforcement (FIR) algorithm and the realistic
    constraints generation (ERG) algorithm to enhance the robustness of the adversarial
    examples. At the same time, they proposed the nested-AE algorithm to improve the
    adaptability of the adversarial examples to long and short distances. Finally,
    the composite scheme is able to success attack against popular objective recognition
    algorithms, such as YOLO v3 and Faster-RCNN, within $\pm 60^{\circ}$ angle and
    $[1m,25m]$ distance range. Based on the above methods, hiding attacks and appearing
    attacks can be carried out. The hiding attack is to paste an adversarial example
    on a normal traffic sign to make the target recognition system fail to recognize
    the traffic sign. The appearing attack to paste an adversarial example on other
    objects, causing the target recognition system to recognize the object as a characteristic
    traffic sign or make a false recognition. In 2020, Kong et al.[[134](#bib.bib134)]
    proposed PhysGAN, a physical world adversarial examples attack method based on
    adversarial example generative networks, and the generated adversarial examples
    of advertising signs have better robustness and invisibility.
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 针对静态目标的物理世界对抗样本。这类攻击目标包括各种静态目标识别系统，如交通标志、交通信号和交通标线，其特点是对抗样本可以在大范围的角度和距离上持续稳定地干扰机器学习模型的判断。2017年，陆等人[[129](#bib.bib129)]
    成功地在物理世界中生成了流行的目标识别算法FasterRCNN的对抗样本。为了实现更好的距离和角度范围适应，2018年，Eykholt等人[[130](#bib.bib130)]
    提出了鲁棒物理扰动（RP2）算法。在同一年，陈等人[[131](#bib.bib131)] 采用了期望变换（EoT）方法[[132](#bib.bib132)]
    来改进交通标志对抗样本的生成，从而提高了对抗样本对距离、角度、光线等环境的适应性。2019年，赵等人[[133](#bib.bib133)] 提出了特征干扰增强（FIR）算法和真实约束生成（ERG）算法，以增强对抗样本的鲁棒性。同时，他们提出了嵌套对抗样本（nested-AE）算法，以提高对抗样本对长短距离的适应性。最后，该复合方案能够在
    $\pm 60^{\circ}$ 的角度范围和 $[1m,25m]$ 的距离范围内成功攻击流行的目标识别算法，如YOLO v3和Faster-RCNN。基于上述方法，可以实施隐藏攻击和显现攻击。隐藏攻击是在普通交通标志上粘贴对抗样本，使目标识别系统无法识别该交通标志。显现攻击是在其他物体上粘贴对抗样本，导致目标识别系统将物体识别为特征交通标志或发生错误识别。2020年，孔等人[[134](#bib.bib134)]
    提出了基于对抗样本生成网络的物理世界对抗样本攻击方法PhysGAN，生成的广告标志对抗样本具有更好的鲁棒性和隐蔽性。
- en: •
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A physical world adversarial example for pedestrian recognition. In autonomous
    driving system, a missed detection of pedestrians by the objective recognition
    system can have serious consequences. In 2020, Wu[[135](#bib.bib135)] proposed
    the "invisibility cloak" algorithm, where pedestrians are not normally detected
    by Yolo v2 and Yolo v3 objective detection models when wearing sprayed adversarial
    example clothing. In the same year, Wang et al.[[136](#bib.bib136)] conducted
    a similar study.
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 物理世界中的行人识别对抗样本。在自动驾驶系统中，目标识别系统未能检测到行人可能会导致严重后果。2020年，吴[[135](#bib.bib135)] 提出了“隐形斗篷”算法，当行人穿着喷涂的对抗样本服装时，Yolo
    v2和Yolo v3目标检测模型通常无法检测到他们。在同一年，王等人[[136](#bib.bib136)] 进行了类似的研究。
- en: •
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Physical world adversarial examples for vehicle recognition. These examples
    are often pasted or painted on the vehicle body with a specific pattern, so that
    the vehicle detection system can not identify the vehicle or mistakenly identify
    the vehicle as other objects. Such physical world adversarial examples try to
    maintain attack effectiveness under high speed movement, various light and other
    external conditions, especially in the 360-degree view and within the detection
    range of vehicle identification system, which puts forward higher requirements
    for the robustness of adversarial samples. At the same time, at different angles,
    the camera may only be able to acquire part of the images of he adversarial example
    in the vehicle body, which in turn places a local requirement on the adversarial
    example, i.e., part of the adversarial example can also achieve the attack. In
    2019, Zhang et al.[[137](#bib.bib137)] proposed a vehicle body painting method
    of black box adversarial example based on transition models so that the example
    vehicle could not be identified by autonomous driving vehicle detection system.
    In 2020, Wu et al.[[135](#bib.bib135)] proposed the discrete searching algorithm
    to efficiently generate adversarial patches, and then proposed the Enlarge-and-Repeat
    (ER) algorithm to extend the adversarial patches to the whole body using body
    images collected from all angles. Both are with pretty good adversarial results.
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 车辆识别的物理世界对抗样本。这些样本通常以特定的图案贴在或涂在车身上，使得车辆检测系统无法识别该车辆或错误地将车辆识别为其他物体。这种物理世界对抗样本试图在高速移动、各种光照和其他外部条件下保持攻击效果，尤其是在360度视角和车辆识别系统的检测范围内，这对对抗样本的鲁棒性提出了更高要求。同时，在不同角度下，相机可能只能获取车身对抗样本的部分图像，从而对对抗样本提出了局部要求，即对抗样本的部分也能实现攻击。2019年，张等人[[137](#bib.bib137)]提出了一种基于转移模型的黑盒对抗样本车身涂装方法，使得样例车辆无法被自动驾驶车辆检测系统识别。2020年，吴等人[[135](#bib.bib135)]提出了离散搜索算法来高效生成对抗贴片，并提出了放大-重复（ER）算法，将对抗贴片扩展到整个车身，使用从各个角度收集的车身图像。这两者都具有相当好的对抗效果。
- en: The following highlights the key algorithms in the physical world adversarial
    example enhancement described above.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以下重点介绍了上述物理世界对抗样本增强的关键算法。
- en: Algorithm1\. Expectation over Transformation (EoT) [[132](#bib.bib132)]
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 算法1\. 变换期望（EoT）[[132](#bib.bib132)]
- en: 'The core idea of the EoT algorithm is to add a certain random perturbation
    to each iteration of the adversarial example generation process, so that the final
    generated adversarial example has better robustness, with specific transformations
    including: projecting, rotation, and scaling. In the formula, the operation $M_{t}(x_{b},x_{o})$
    is defined to project the target image $x_{o}$ onto the background image $x_{b}$
    through some transformation $t$, and then the EoT is optimized as follows.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: EoT算法的核心思想是对每次对抗样本生成过程中的迭代添加一定的随机扰动，以便最终生成的对抗样本具有更好的鲁棒性，具体变换包括：投影、旋转和缩放。在公式中，操作$M_{t}(x_{b},x_{o})$定义为通过某种变换$t$将目标图像$x_{o}$投影到背景图像$x_{b}$上，然后对EoT进行如下优化。
- en: '|  | $\begin{split}\hat{p}=&amp;\arg\min_{x^{\prime}\in\mathbb{R}^{h\times
    w\times 3}}\mathbb{E}_{x\sim X,t\sim T}[L(F(M_{t}(x,tanh(x^{\prime}))]\\ &amp;+c\cdot\left\&#124;tanh(x^{\prime})-x\right\&#124;\end{split}$
    |  | (3) |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}\hat{p}=&amp;\arg\min_{x^{\prime}\in\mathbb{R}^{h\times
    w\times 3}}\mathbb{E}_{x\sim X,t\sim T}[L(F(M_{t}(x,tanh(x^{\prime}))]\\ &amp;+c\cdot\left\&#124;tanh(x^{\prime})-x\right\&#124;\end{split}$
    |  | (3) |'
- en: where $X$ denote the training set of background images, and $F$ denote the target
    network.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$X$表示背景图像的训练集，$F$表示目标网络。
- en: Algorithm2\. Adversarial Patch [[138](#bib.bib138)]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 算法2\. 对抗性贴片 [[138](#bib.bib138)]
- en: 'Based on the EoT transform, the attacker could generate an adversarial patch
    $\hat{p}$ which the image with the this adversarial patch attacked by adversarial
    examples. The adversarial patch can be any shape, transformed by EoT methods such
    as random projecting, rotation, and scaling, and then generated by optimization
    methods such as gradient descent. An adversarial patch generation task $A(p,x,l,t)$
    can be formally described as: for any particular $x\in\mathbb{R}^{w\times\ h\times
    c}$ to generate adversarial patch $p$ through the EoT transformation $t$ at position
    $l$, then the adversarial path $p$ is continuously optimized by the following
    optimization algorithm:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 基于EoT变换，攻击者可以生成一个对抗贴片$\hat{p}$，该图像包含被对抗样本攻击的对抗贴片。对抗贴片可以是任何形状，通过EoT方法（如随机投影、旋转和缩放）进行变换，然后通过优化方法（如梯度下降）生成。对抗贴片生成任务$A(p,x,l,t)$可以正式描述为：对于任意特定的$x\in\mathbb{R}^{w\times\
    h\times c}$，通过EoT变换$t$在位置$l$生成对抗贴片$p$，然后对抗贴片$p$通过以下优化算法持续优化：
- en: '|  | $\hat{p}=\arg\max_{x\in\mathbb{R}^{h\times w\times c}}\mathbb{E}_{x\sim
    X,t\sim T,l\sim L}[\log Pr(\hat{y}&#124;A(p,x,l,t))]$ |  | (4) |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{p}=\arg\max_{x\in\mathbb{R}^{h\times w\times c}}\mathbb{E}_{x\sim
    X,t\sim T,l\sim L}[\log Pr(\hat{y}&#124;A(p,x,l,t))]$ |  | (4) |'
- en: where $X$ denotes the training set of background images, $T$ denotes the distribution
    of the EoT transform used by the patch, and $L$ denotes the distribution of the
    locations of the adversarial patches.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$X$表示背景图像的训练集，$T$表示贴片使用的EoT变换的分布，$L$表示对抗贴片的位置分布。
- en: Algorithm3\. Feature-inference Reinforcement[[133](#bib.bib133)]
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Algorithm3\. Feature-inference Reinforcement[[133](#bib.bib133)]
- en: Adversarial example generation algorithms often require an objective function,
    or called it as loss function, designed to minimize the difference between the
    predicted and expected values of a deep learning model. The neural network extracts
    features of the objects in the image and makes classification predictions based
    on these extracted features. The researchers found that generating adversarial
    examples with perturbations further forward in the hidden layer in the neural
    network would make the adversarial examples more robust. The core idea of the
    FIR algorithm is therefore to minimize the difference between the feature images
    of the layers of the adversarial examples and the normal examples, except for
    the use of the adversarial examples to mislead the prediction results of the neural
    network.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本生成算法通常需要一个目标函数，也称为损失函数，旨在最小化深度学习模型的预测值和期望值之间的差异。神经网络提取图像中物体的特征，并基于这些提取的特征进行分类预测。研究人员发现，通过在神经网络的隐藏层中进一步引入扰动生成对抗样本，可以使对抗样本更具鲁棒性。因此，FIR算法的核心思想是最小化对抗样本与正常样本在特征图像层之间的差异，而不是利用对抗样本误导神经网络的预测结果。
- en: First, attacker obtain the feature images $Q_{n}$ and $Q^{\prime}_{n}$ generated
    by each hidden layer of the neural network for corresponding categories $y$ and
    $y^{\prime}$. Next, generate the feature vectors $v$ with $v^{\prime}$ from the
    feature images $Q_{n}$ and $Q^{\prime}_{n}$. Finally, attacker optimize with the
    a composited loss function until convergence, then return the adversarial example.
    Such a vector loss can be defined as $Loss_{f}=\sum\left|v-v^{\prime}\right|$.
    The FIR algorithm can be described formally as follows.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，攻击者获取神经网络的每个隐藏层生成的特征图像$Q_{n}$和$Q^{\prime}_{n}$，对应于类别$y$和$y^{\prime}$。接下来，从特征图像$Q_{n}$和$Q^{\prime}_{n}$生成特征向量$v$和$v^{\prime}$。最后，攻击者使用组合损失函数进行优化，直到收敛，然后返回对抗样本。这样的向量损失可以定义为$Loss_{f}=\sum\left|v-v^{\prime}\right|$。FIR算法可以形式化描述如下。
- en: 'Input: normal example $x$, target objective detection model $f$Output: adversarial
    example $x^{\prime}$12while *until convergence*  do3      4      foreach *$i$th
    hidden layer in $f$*  do5             $q_{i}\leftarrow f(x)$ //get feature map
    $q_{i}$ of normal examples $x$ with prediction $y$ ;6            7            $q^{\prime}_{i}\leftarrow
    f(x^{\prime})$ // get feature map $q^{\prime}_{i}$ of adversarial examples $x^{\prime}$
    with prediction $y^{\prime}$ ;8            9       end foreach10      $v\leftarrow
    Q:\left\{{q_{1},q_{2},...,q_{n}}\right\}$ // generate normal feature vector from
    feature maps11      $v^{\prime}\leftarrow Q^{\prime}:\left\{{q^{\prime}_{1},q^{\prime}_{2},...,q^{\prime}_{n}}\right\}$
    // generate adversarial feature vector from feature maps12      Optimize $x^{\prime}$
    to minimize the loss function:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：正常样本 $x$，目标检测模型 $f$ 输出：对抗样本 $x^{\prime}$12当 *直到收敛* 时 进行3      4      对每个
    *$i$ 层隐藏层在 $f$ 中* 进行5             $q_{i}\leftarrow f(x)$ // 获取正常样本 $x$ 的特征图 $q_{i}$
    和预测 $y$ ;6            7            $q^{\prime}_{i}\leftarrow f(x^{\prime})$ //
    获取对抗样本 $x^{\prime}$ 的特征图 $q^{\prime}_{i}$ 和预测 $y^{\prime}$ ;8            9      
    结束对每个10      $v\leftarrow Q:\left\{{q_{1},q_{2},...,q_{n}}\right\}$ // 从特征图生成正常特征向量11      $v^{\prime}\leftarrow
    Q^{\prime}:\left\{{q^{\prime}_{1},q^{\prime}_{2},...,q^{\prime}_{n}}\right\}$
    // 从特征图生成对抗特征向量12      优化 $x^{\prime}$ 以最小化损失函数：
- en: '|  | $\alpha C_{N}^{box}+\beta p_{N}(y^{N}&#124;S)+c(loss_{f})^{-1}$ |  |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $\alpha C_{N}^{box}+\beta p_{N}(y^{N}&#124;S)+c(loss_{f})^{-1}$ |  |'
- en: 13 end while14
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 13 结束
- en: Algorithm 1 3\. Feature-inference Reinforcement
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 3\. 特征推断强化
- en: where $C_{N}^{box}$ denotes the confidence level of the target detection system
    for the target area; $y^{N}$ denotes the confidence level that example $x$ is
    judged by the neural network to be class $N$; $S$ denotes the distribution space
    of confidence levels; $loss_{f}$ denotes the vector loss; $\alpha$, $\beta$ and
    $c$ are three constants representing the weights respectively.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $C_{N}^{box}$ 表示目标检测系统对目标区域的置信度水平；$y^{N}$ 表示神经网络判断样本 $x$ 为类别 $N$ 的置信度水平；$S$
    表示置信度水平的分布空间；$loss_{f}$ 表示向量损失；$\alpha$、$\beta$ 和 $c$ 是分别表示权重的三个常数。
- en: Algorithm4\. Nested adversarial examples (Nested-AE)[[133](#bib.bib133)]
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 算法4\. 嵌套对抗样本 (Nested-AE)[[133](#bib.bib133)]
- en: Most objective detectors were designed to use multi-scales, each works better
    at different distances individually. It means that at different distances, different
    scales play different roles. In order to make the generated adversarial examples
    achieve adversarial effects over a large distance range and multiple angles, the
    Nested-AE algorithm considers many scales for different distances and angles to
    generate many adversarial patches. Then Nested-AE obtained the adversarial patches
    to synthesize an adversarial example. Thus nested adversarial examples are more
    adapted to the environment of vehicle movement in autonomous driving, which can
    achieve the purpose of a continuous adversary effect on the objective detection
    system. The nested adversarial examples could be formally described as
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数目标检测器被设计为使用多尺度，每个尺度在不同距离上效果最佳。这意味着在不同距离上，不同尺度发挥不同的作用。为了使生成的对抗样本在较大的距离范围和多个角度上实现对抗效果，Nested-AE算法考虑了多个尺度以应对不同的距离和角度，从而生成许多对抗补丁。然后，Nested-AE将这些对抗补丁合成一个对抗样本。因此，嵌套对抗样本更适应于自动驾驶中的车辆运动环境，可以实现对目标检测系统的持续对抗效果。嵌套对抗样本可以正式描述为
- en: '|  | $X^{adv}_{i+1}=Clip\begin{Bmatrix}X_{i}+\varepsilon sign(J(X_{i})),&amp;S_{P}\leq
    S_{thres}\\ X_{i}+\varepsilon M_{center}sign(J(X_{i})),&amp;S_{P}>S_{thres}\end{Bmatrix}$
    |  | (5) |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '|  | $X^{adv}_{i+1}=Clip\begin{Bmatrix}X_{i}+\varepsilon sign(J(X_{i})),&amp;S_{P}\leq
    S_{thres}\\ X_{i}+\varepsilon M_{center}sign(J(X_{i})),&amp;S_{P}>S_{thres}\end{Bmatrix}$
    |  | (5) |'
- en: where $X_{i}$ denotes the original example, $X^{adv}_{i+1}$ denotes the example
    with adversarial perturbation, $J()$ represents the gradient of input $X_{i}$
    and $Clip()$ refers to regularize the input to the $[0,255]$ interval.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $X_{i}$ 表示原始样本，$X^{adv}_{i+1}$ 表示具有对抗扰动的样本，$J()$ 表示输入 $X_{i}$ 的梯度，$Clip()$
    指将输入规范化到 $[0,255]$ 区间。
- en: If the adversarial example size $S_{p}$ is less than or equal to the threshold
    $S_{thres}$ , then it is considered a long-range adversarial attack at that point
    and the whole will be perturbed ; otherwise it is a close-range attack and only
    the central region will be perturbed. This decomposes the adversarial attack at
    different distances into two sub-tasks, which are perturbed and optimized separately.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果对抗样本的大小 $S_{p}$ 小于或等于阈值 $S_{thres}$，则被视为长距离对抗攻击，此时整个图像将受到扰动；否则，它被视为近距离攻击，仅中心区域会受到扰动。这将不同距离的对抗攻击分解为两个子任务，分别进行扰动和优化。
- en: The objective detection systems usually divides each video frame into a grid
    that consisted of $m\times n$ boxes. Base on the prediction of each box, attacker
    could find the decisive box for each scales, and then add the adversarial perturbation
    in this box. As the predicted output is usually tensors, there are only need the
    tensor of the index where the adversarial example is located. Therefore, it can
    calculate the index by the size of the example and the position of the centre
    region, so that we can get the tensor representing the example region, which is
    denoted as $N_{p}$. Then $N_{p}$ needs to be calculated in each video frame. The
    nested adversarial example algorithm can be optimized using the following loss
    function.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测系统通常将每个视频帧分成由 $m\times n$ 个框组成的网格。根据每个框的预测，攻击者可以找到每个尺度的决定性框，然后在这个框中添加对抗扰动。由于预测的输出通常是张量，因此只需要对抗样本所在位置的张量。因此，可以通过样本的大小和中心区域的位置来计算索引，从而获得表示样本区域的张量，记作
    $N_{p}$。然后需要在每个视频帧中计算 $N_{p}$。嵌套对抗样本算法可以使用以下损失函数进行优化。
- en: '|  | $N_{p}=f(p_{size},P_{position}),1-C^{box}_{N_{p}}+\beta\sum\left&#124;p_{N_{p}},j-y_{j}\right&#124;^{2}$
    |  | (6) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  | $N_{p}=f(p_{size},P_{position}),1-C^{box}_{N_{p}}+\beta\sum\left&#124;p_{N_{p}},j-y_{j}\right&#124;^{2}$
    |  | (6) |'
- en: 3.2 Physical World Adversarial Example for Pedestrian Detection
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 行人检测中的物理世界对抗样本
- en: Pedestrian detection often places high demands on the universality, portability,
    robustness and feasibility of the adversarial examples. To due this problem, some
    algorithms was proposed. As a typical example, Invisibility Cloak algorithms[[135](#bib.bib135)]
    can generate adversarial examples in the physical world, so that pedestrians wearing
    clothes painted with specific adversarial examples cannot be recognized properly.
    If the similar method be used in pedestrian, serious consequences of autonomous
    driving system will be lead.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 行人检测通常对对抗样本的通用性、便携性、鲁棒性和可行性提出较高要求。为了解决这个问题，提出了一些算法。作为一个典型的例子，隐形斗篷算法[[135](#bib.bib135)]
    可以在物理世界中生成对抗样本，从而使穿着特定对抗样本图案衣物的行人无法被正确识别。如果在行人检测中使用类似的方法，将对自动驾驶系统产生严重后果。
- en: Algorithm 5\. Invisibility Cloak[[135](#bib.bib135)]
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 5. 隐形斗篷[[135](#bib.bib135)]
- en: The policy of the invisibility cloak algorithm is to use a large number of images
    containing people to train against patches; in each iteration, a random batch
    of images is selected and sent to the objective detection system to obtain the
    bounding box of people. The critical idea is that place a randomly transformed
    patch on each detected person so that the score that feature images are detected
    to have people’s presence is minimized.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 隐形斗篷算法的策略是使用大量包含人员的图像进行训练以对抗补丁；在每次迭代中，随机选择一批图像并将其发送到目标检测系统，以获得人员的边界框。关键思想是将一个随机变换的补丁放置在每个检测到的人员身上，以最小化检测到的特征图像中人员存在的得分。
- en: The patch $P\in\mathbb{R}^{w\times h\times 3}$ is projected to the target image
    $I$ by transformation function $R_{\theta}$ which performs data augmentation for
    lighting, contrast changes, distortion, with $\theta$ as the parameter, in addition
    to scaling the patch to fit the size of image $I$.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 补丁 $P\in\mathbb{R}^{w\times h\times 3}$ 通过变换函数 $R_{\theta}$ 投射到目标图像 $I$ 上，该函数对光照、对比度变化、畸变进行数据增强，$\theta$
    为参数，此外，还会将补丁缩放以适应图像 $I$ 的大小。
- en: Additionally, two effective advanced methods for physical world attack were
    proposed as auxiliary. One was called Total-Variation (TV) loss. Add a TV penalty
    function to the patch to make adversarial example smoother to improve robustness
    of physical world adversarial examples.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，提出了两种有效的物理世界攻击高级方法作为辅助。其一是所谓的总变差 (TV) 损失。向补丁添加一个 TV 惩罚函数，使对抗样本更平滑，从而提高物理世界对抗样本的鲁棒性。
- en: 'with the final loss function as:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的损失函数为：
- en: '|  | $L_{obj}(P)=loss+\gamma\cdot TV(P)$ |  | (7) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{obj}(P)=loss+\gamma\cdot TV(P)$ |  | (7) |'
- en: Another is Ensemble training. In the black-box case, the attacker cannot obtain
    the gradient of the target detection model under attack, so a possible solution
    is to collect multiple similar white-box models for ensemble training. The loss
    function is
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个是集成训练。在黑箱情况下，攻击者无法获得被攻击目标检测模型的梯度，因此一个可能的解决方案是收集多个相似的白箱模型进行集成训练。损失函数为
- en: '|  | $L_{ens}(P)=\mathbb{\theta,I}\sum_{i,j}\left\{S_{i}^{(j)}(R_{\theta}(I,P))+1,0\right\}$
    |  | (8) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{ens}(P)=\mathbb{\theta,I}\sum_{i,j}\left\{S_{i}^{(j)}(R_{\theta}(I,P))+1,0\right\}$
    |  | (8) |'
- en: where $S_{(j)}$ represents the objective detection model of target $j$. It is
    formally described as follows.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $S_{(j)}$ 表示目标 $j$ 的目标检测模型。其形式描述如下。
- en: 'Input: normal example $x$, target objective detection model $f$Output: adversarial
    example $x^{\prime}$12$I=P(x)$ // generate projection3 ;45$R_{\theta}\leftarrow\theta$
    // define a transfer function $R_{\theta}$6 ;78while *until convergence*  do9      10      Optimize
    $x^{\prime}$ to minimize the loss function:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：正常样本 $x$，目标目标检测模型 $f$输出：对抗样本 $x^{\prime}$12$I=P(x)$ // 生成投影3 ;45$R_{\theta}\leftarrow\theta$
    // 定义转移函数 $R_{\theta}$6 ;78*直到收敛* 进行中9      10      优化 $x^{\prime}$ 以最小化损失函数：
- en: '|  | $L_{obj}(P)=R_{\theta}(I,P)$ |  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{obj}(P)=R_{\theta}(I,P)$ |  |'
- en: '11 end while1213or Auxiliary1: TV Loss'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 11 结束时1213或附加1：TV 损失
- en: '|  | $L_{obj}(P)=R_{\theta}(I,P)+\gamma\cdot TV(P)$ |  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{obj}(P)=R_{\theta}(I,P)+\gamma\cdot TV(P)$ |  |'
- en: '14or Auxiliary2: Ensemble training'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 14或附加2：集成训练
- en: '|  | $L_{obj}(P)=\mathbb{E}_{\theta,I}\sum_{i}\max\left\{{S_{i}(R_{\theta}(I,P))+1,0}\right\}^{2}$
    |  |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{obj}(P)=\mathbb{E}_{\theta,I}\sum_{i}\max\left\{{S_{i}(R_{\theta}(I,P))+1,0}\right\}^{2}$
    |  |'
- en: Algorithm 2 5\. Invisibility Cloak
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 2 5\. 隐形斗篷
- en: 3.3 Physical World Adversarial Example for Vehicle Detection
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 物理世界对抗样本用于车辆检测
- en: The aim of the adversarial examples for vehicle detection is evade or mislead
    other vehicle detector by adversarial spraying. Different from other scenarios,
    the challenge is that the spraying need works for the detector at all angles.
    Discrete Searching is a typical and effect algorithm.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 对于车辆检测的对抗样本，其目标是通过对抗喷涂避开或误导其他车辆检测器。与其他场景不同，挑战在于喷涂需要适用于所有角度的检测器。离散搜索是一种典型且有效的算法。
- en: Algorithm 6\. Discrete Searching[[139](#bib.bib139)]
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 6\. 离散搜索[[139](#bib.bib139)]
- en: The discrete searching algorithm is essentially a black-box adversarial example
    generation algorithm based on a genetic algorithm that continuously optimizes
    the adversarial examples through mutation and selection. The discrete searching
    algorithm that mutation-based search method defines two mutation strategies. One
    is random mutation, where a point within the circle with $\epsilon$ as the radius,
    is randomly selected as the direction and advanced a step length as the new mutation
    point. If the current mutation optimization fails to outperform the original one,
    the choice will be made to continue with the random mutation. The other is directed
    mutation, in which the candidate’s best mutation point is selected within a particular
    angular expansion of the current direction, advancing a random step length. If
    the current variation outperforms the original one, the choice will be made to
    continue with the directed mutation.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 离散搜索算法本质上是一种基于遗传算法的黑箱对抗样本生成算法，通过变异和选择不断优化对抗样本。离散搜索算法中的变异方法定义了两种变异策略。一种是随机变异，即在半径为$\epsilon$的圆内随机选择一个点作为方向，并前进一步作为新的变异点。如果当前变异优化未能优于原始样本，则选择继续进行随机变异。另一种是定向变异，在当前方向的特定角度范围内选择候选的最佳变异点，并随机前进一步。如果当前变异优于原始样本，则选择继续进行定向变异。
- en: '12Input: normal example $x$, target objective detection model $f$Output: adversarial
    example $x^{\prime}$12while *until convergence*  do3       foreach *$i\in\left\{0,...,N_{a}-1\right\}$*
     do4            5            $C_{i}^{j_{w}}$, $C_{i}^{j_{w}}=Clip(C_{i}+Random(H,W,3)\cdot\epsilon_{1}\cdot\delta)$
    //generate candidate point6            select $\hat{C}_{i}$ in $C_{i}^{j_{w}}$7            if *$\hat{C}_{i}$
    is better than $C_{i}$* then8                   $C_{i}=\hat{C}_{i}$9            else10                  
    $C_{i}$ remains unchanged11             end if12            13       end foreach14      15
    end while'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 12输入：正常样本 $x$，目标目标检测模型 $f$ 输出：对抗样本 $x^{\prime}$12当 *直到收敛* 时3       对于 *$i\in\left\{0,...,N_{a}-1\right\}$*
    进行4            5            $C_{i}^{j_{w}}$，$C_{i}^{j_{w}}=Clip(C_{i}+Random(H,W,3)\cdot\epsilon_{1}\cdot\delta)$
    //生成候选点6            在 $C_{i}^{j_{w}}$ 中选择 $\hat{C}_{i}$7            如果 *$\hat{C}_{i}$
    比 $C_{i}$ 更好* 则8                   $C_{i}=\hat{C}_{i}$9            否则10                  
    $C_{i}$ 保持不变11             结束如果12            13       结束对每个14      15 结束当
- en: Algorithm 3 6\. Discrete Searching
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 3 6\. 离散搜索
- en: 3.4 Adversarial Examples on LiDAR and RaDAR
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 LiDAR 和 RaDAR 上的对抗样本
- en: Adversarial examples are related to the characteristics of deep learning itself,
    and both RGB image-based and LiDAR- or RaDAR-based objective recognition systems
    are likely to suffer from adversarial examples attacks. In 2019, Cao et al.[[140](#bib.bib140)]
    proposed an adversarial examples attack method for LiDAR target recognition. The
    attacker emits a small amount of perturbation laser at the LiDAR system, which
    led to a small perturbation in the imaging of the LiDAR system, and such perturbation
    made the LiDAR-based 3D objective recognition erroneous. Then, an adversarial
    object attack method LiDAR-Adv was proposed for LiDAR objective recognition, in
    which the attacker could construct a certain special shape of objects, causing
    the LiDAR objective recognition system to mispredict special objects. Such an
    attack can be targeted, i.e., the real object is recognized as the specified by
    the attacker, so this is more easier to exploit for the attacker. As the figure
    shows, some specially shaped objects can be misidentified as "pedestrians" by
    the 3D objective recognition system, while others are not recognized properly
    by the 3D objective recognition system, posing a security risk. The above method
    was successfully tested on Baidu’s autonomous driving system Apollo. In 2020,
    after research and improvement, SUN et al.[[141](#bib.bib141)] implemented a black-box
    attack of the above method, which was successfully tested on Intel’s autonomous
    driving simulation system Carla. In real autonomous driving environments, the
    detection of dynamic targets often takes a multi-modal fusion of RGB images, LiDAR,
    and RaDAR for target recognition, which improves the robustness of the system
    to some extent.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本与深度学习本身的特性有关，无论是基于 RGB 图像还是基于 LiDAR 或 RaDAR 的目标识别系统，都容易受到对抗样本攻击。在 2019 年，Cao
    等人[[140](#bib.bib140)] 提出了一个针对 LiDAR 目标识别的对抗样本攻击方法。攻击者向 LiDAR 系统发射少量扰动激光，这导致 LiDAR
    系统成像出现小的扰动，从而使基于 LiDAR 的 3D 目标识别出现错误。随后，提出了一种针对 LiDAR 目标识别的对抗物体攻击方法 LiDAR-Adv，其中攻击者可以构造具有某种特殊形状的物体，导致
    LiDAR 目标识别系统错误预测特殊物体。这种攻击是有针对性的，即真实物体被识别为攻击者指定的物体，因此攻击者更容易利用。正如图所示，一些特殊形状的物体可能会被
    3D 目标识别系统误识别为“行人”，而其他物体则未被正确识别，带来了安全隐患。上述方法在百度的自动驾驶系统 Apollo 上成功测试。在 2020 年，经过研究和改进，SUN
    等人[[141](#bib.bib141)] 实现了上述方法的黑箱攻击，并在英特尔的自动驾驶模拟系统 Carla 上成功测试。在实际的自动驾驶环境中，动态目标的检测通常采用
    RGB 图像、LiDAR 和 RaDAR 的多模态融合进行目标识别，这在一定程度上提高了系统的鲁棒性。
- en: '![Refer to caption](img/7c8f28ae95660a4cee5405f31cb02f1c.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7c8f28ae95660a4cee5405f31cb02f1c.png)'
- en: 'Figure 4: Adversarial Examples on LiDAR[[140](#bib.bib140)]'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：LiDAR 上的对抗样本[[140](#bib.bib140)]
- en: 'Higher demands are placed on the adversarial example for multi-modal environments,
    mainly in terms of:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多模态环境中的对抗样本要求更高，主要体现在以下方面：
- en: •
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial perturbation needs to be able to be physically generated in both
    the RGB image and the LiDAR system environment. Traditional RGB adversarial examples
    usually change the RGB values of some pixels in the image, however, this method
    cannot works on the 3D cloud point map generated from the LiDAR. On another hand,
    emit a specific adversarial laser at LiDAR can interfere with the LiDAR system,
    but it is also difficult to effectively influence the RGB objective recognition
    system.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性扰动需要能够在 RGB 图像和 LiDAR 系统环境中物理生成。传统的 RGB 对抗性样本通常会改变图像中某些像素的 RGB 值，但这种方法无法作用于从
    LiDAR 生成的 3D 点云图。另一方面，在 LiDAR 上发射特定的对抗性激光可以干扰 LiDAR 系统，但也很难有效影响 RGB 目标识别系统。
- en: •
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The adversarial examples need to be able to physically and continuously work
    on both the RGB-based system and the LiDAR-based system. In a real vehicle environment,
    the RGB system and the LiDAR system need to attack successfully and continuously
    at a long distance and at different angles.
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性样本需要能够在 RGB 系统和 LiDAR 系统中物理且持续地工作。在实际车辆环境中，RGB 系统和 LiDAR 系统需要在远距离和不同角度下成功并持续地进行攻击。
- en: •
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial examples need to be able to adapt to different data preprocessing
    between RGB-based systems and LiDAR-based systems. RGB image acquisition system
    and LiDAR data acquisition system both have certain data preprocessing, which
    will have impact on adversarial examples. The algorithm of adversarial examples
    generation needs to have strong robustness to different preprocessing.
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性样本需要能够适应 RGB 系统和 LiDAR 系统之间的不同数据预处理。RGB 图像采集系统和 LiDAR 数据采集系统都有一定的数据预处理，这会影响对抗性样本。对抗性样本生成算法需要对不同的预处理具有强鲁棒性。
- en: Meanwhile, after optimized by some specific algorithms , the threat to the fused
    objective recognition system by adversarial examples is still exists. In 2021,
    Cao et al.[[142](#bib.bib142)] proposed the MSF-ADV method with LiDAR and RGB
    image fusion environment as an example, and successfully implemented the physical
    world adversarial examples.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，经过一些特定算法的优化后，融合目标识别系统仍然受到对抗性样本的威胁。2021年，Cao 等人[[142](#bib.bib142)]提出了以 LiDAR
    和 RGB 图像融合环境为例的 MSF-ADV 方法，并成功实现了物理世界中的对抗性样本。
- en: Algorithm 7\. MSF-ADV Algorithm[[142](#bib.bib142)]
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 7\. MSF-ADV 算法[[142](#bib.bib142)]
- en: To accommodate the above challenges, MSF-ADV first generates 3D objects of different
    shapes so that they can simultaneously affect the LiDAR-based 3D point cloud imaging
    and also the RGB colour values of the pixels in the RGB image. Secondly, MSF-ADV
    uses an optimization algorithm to generate the 3D shapes with the best adversarial
    effect. Finally, MSF-ADV uses a 3D printer for physical generation. The loss function
    of the optimization algorithm can be described as
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对上述挑战，MSF-ADV 首先生成不同形状的 3D 对象，以便它们能够同时影响基于 LiDAR 的 3D 点云成像和 RGB 图像中像素的 RGB
    颜色值。其次，MSF-ADV 使用优化算法生成具有最佳对抗效果的 3D 形状。最后，MSF-ADV 使用 3D 打印机进行物理生成。优化算法的损失函数可以描述为
- en: '|  | $min_{S^{a}}\mathbb{E}_{t\sim T}[\mathcal{L}_{a}(t(S_{a});\mathcal{R}^{l},\mathcal{R}^{c},\mathcal{P},\mathcal{M})+\lambda\cdot\mathcal{L}_{r}(S^{a},S)]$
    |  | (9) |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '|  | $min_{S^{a}}\mathbb{E}_{t\sim T}[\mathcal{L}_{a}(t(S_{a});\mathcal{R}^{l},\mathcal{R}^{c},\mathcal{P},\mathcal{M})+\lambda\cdot\mathcal{L}_{r}(S^{a},S)]$
    |  | (9) |'
- en: where $S$ denotes the original examples the $S_{a}$ denotes the adversarial
    examples, $\mathcal{M}$s denotes the fusion algorithm, and $\mathcal{R}^{c}$ is
    the derivative projection function used to represent RGB image based prediction.
    $\mathcal{R}^{l}$ is the derivative projection function used to represent the
    LiDAR prediction. $\mathcal{P}$ is the ultimate output of the objective recognition
    system.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$S$ 表示原始样本，$S_{a}$ 表示对抗性样本，$\mathcal{M}$s 表示融合算法，$\mathcal{R}^{c}$ 是用于表示
    RGB 图像预测的导数投影函数。$\mathcal{R}^{l}$ 是用于表示 LiDAR 预测的导数投影函数。$\mathcal{P}$ 是目标识别系统的最终输出。
- en: Through thus optimization, the trade-off between RGB colour and LiDAR shape
    was found.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种优化，找到了 RGB 颜色和 LiDAR 形状之间的权衡。
- en: 3.5 Adversarial Examples on Object Tracking & Trajectory Prediction
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 对象跟踪与轨迹预测中的对抗性样本
- en: Usually, autonomous driving system relies on object tracking and trajectory
    prediction, to determine and predict target states, and to support driving decisions.
    Object trajectory tracking can be divided into Single-Object Tracking (SOT) and
    Multi-Object Tracking (MOT). With the application of object tracking in critical
    cyber systems, adversarial examples attacks on it are also rising. Among them,
    the main purpose of the adversarial examples attack on SOT is to achieve objective
    evasion. In 2020, Chen et al.[[143](#bib.bib143)] proposed the one-shot adversarial
    attack, which only adds a weak perturbation to the initial frame in the video,
    and the tracked object may not be able to track the trajectory in subsequent frames.
    In the same year, Yan et al.[[144](#bib.bib144)] proposed the cooling-shrinking
    attack, which perturbs the object search area by adding specific adversarial noises,
    so that the tracker cannot identify the object and interrupted the trajectory
    tracking. In the next year, Jia et al.[[145](#bib.bib145)] proposed the IoU Attack,
    the idea of which is to reduce the fractional difference between the normal object
    border and the adversarial object border in object tracking, thus enabling the
    trajectory offset using SOT system.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，自动驾驶系统依赖于物体跟踪和轨迹预测，以确定和预测目标状态，并支持驾驶决策。物体轨迹跟踪可以分为单目标跟踪（SOT）和多目标跟踪（MOT）。随着物体跟踪在关键网络系统中的应用，对其的对抗样本攻击也在上升。其中，对SOT的对抗样本攻击的主要目的是实现目标逃避。2020年，Chen等人[[143](#bib.bib143)]提出了一次性对抗攻击，这种攻击只对视频中的初始帧添加了微弱的扰动，导致被跟踪物体在随后的帧中可能无法跟踪轨迹。同年，Yan等人[[144](#bib.bib144)]提出了冷却收缩攻击，通过添加特定的对抗噪声来扰动物体搜索区域，从而使跟踪器无法识别物体，并中断轨迹跟踪。次年，Jia等人[[145](#bib.bib145)]提出了IoU攻击，其思想是在物体跟踪中减少正常物体边界与对抗物体边界之间的分数差异，从而使得使用SOT系统的轨迹偏移。
- en: The autonomous driving system more often uses MOT systems. An adversarial example
    attack on MOT may achieve both evasion and object obfuscation. In 2020, Jia et
    al.[[146](#bib.bib146)] generated an adversarial example on an autonomous driving
    object tracking system that minutely deviation the normal target identification
    bounding box of the attacked target in a specific direction, causing the tracker
    to assign the wrong velocity to the attacked trajectory, resulting in the target
    tracking system not being able to associate with the target properly, thus achieving
    an escape attack. In 2021, Lin et al.[[147](#bib.bib147)] proposed a new adversarial
    example scheme that mainly uses the "PullPush Loss" algorithm and "Center Leaping"
    algorithm. The scheme leads to object tracking system confuses, when objects cross
    each other.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶系统更常使用MOT系统。对MOT的对抗样本攻击可能同时实现逃避和物体混淆。2020年，Jia等人[[146](#bib.bib146)]在自动驾驶物体跟踪系统上生成了一个对抗样本，该样本在特定方向上细微偏离了正常目标识别边界框，导致跟踪器给攻击的轨迹分配错误的速度，导致目标跟踪系统无法正确关联目标，从而实现了逃脱攻击。2021年，Lin等人[[147](#bib.bib147)]提出了一种新的对抗样本方案，该方案主要使用“PushPull
    Loss”算法和“Center Leaping”算法。当物体彼此交叉时，该方案会使物体跟踪系统混淆。
- en: Algorithm 8\. Push-Pull Loss [[147](#bib.bib147)]
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 算法8\. Push-Pull Loss [[147](#bib.bib147)]
- en: A video $V$ consists of a series of frames, which can be marked as $V=\left\{{I_{1},I_{2},...,I_{N}}\right\}$,
    The trajectories of target$i$ and $j$ are respectively $T_{i}=\left\{{O_{s_{i}}^{i}},...,O_{t}^{i},...,O_{E_{i}^{i}}\right\}$
    and $T_{j}=\left\{{O_{s_{j}}^{j}},...,O_{t}^{j},...,O_{E_{j}^{j}}\right\}$
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 一个视频$V$由一系列帧组成，可以标记为$V=\left\{{I_{1},I_{2},...,I_{N}}\right\}$，目标$i$和$j$的轨迹分别是$T_{i}=\left\{{O_{s_{i}}^{i}},...,O_{t}^{i},...,O_{E_{i}^{i}}\right\}$和$T_{j}=\left\{{O_{s_{j}}^{j}},...,O_{t}^{j},...,O_{E_{j}^{j}}\right\}$。
- en: The attacker’s target generates a series of adversarial frames $\hat{V}$, as
    $\hat{V}=\left\{{I_{1},...,I_{t-1},\hat{I}_{t},...,\hat{I}_{t+n-1},I_{t+n},...,I_{N}}\right\}$,
    such that from the moment of time $t$, an adversarial misdirection of the trajectory
    of $i$ and $j$ occurs. Then there is the formula, $\hat{T}_{i}=\left\{{O_{s_{i}}^{i},...,O_{t-1}^{i},O_{t}^{i},...,O_{t+n-1}^{j},O_{t+n}^{j},...,O_{e_{j}}^{j}}\right\}$
    where $O_{t}^{i}$ indicates the target identified as $i$ at the moment of time
    $t$.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者的目标生成了一系列对抗帧$\hat{V}$，如$\hat{V}=\left\{{I_{1},...,I_{t-1},\hat{I}_{t},...,\hat{I}_{t+n-1},I_{t+n},...,I_{N}}\right\}$，使得从时间$t$开始，目标$i$和$j$的轨迹发生对抗性误导。然后有公式$\hat{T}_{i}=\left\{{O_{s_{i}}^{i},...,O_{t-1}^{i},O_{t}^{i},...,O_{t+n-1}^{j},O_{t+n}^{j},...,O_{e_{j}}^{j}}\right\}$，其中$O_{t}^{i}$表示在时间$t$时被识别为$i$的目标。
- en: 'Use the PushPull loss function to optimize and realize:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用推拉损失函数进行优化并实现：
- en: '|  | $\begin{split}L_{pullpush}(a_{t-1}^{i},a_{t-1}^{j},feat_{t}^{i},feat_{t}^{j})\\
    =\sum_{k\in\left\{i,j\right\}}d_{feat}(a_{t-1}^{k},feat_{t}^{\widetilde{k}})-d_{feat}(a_{t-1}^{k},feat_{t}^{k})\end{split}$
    |  | (10) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}L_{pullpush}(a_{t-1}^{i},a_{t-1}^{j},feat_{t}^{i},feat_{t}^{j})\\
    =\sum_{k\in\left\{i,j\right\}}d_{feat}(a_{t-1}^{k},feat_{t}^{\widetilde{k}})-d_{feat}(a_{t-1}^{k},feat_{t}^{k})\end{split}$
    |  | (10) |'
- en: where $d_{feat}()$ denotes the cosine distance and $a_{t-1}^{i}$ and $a_{t-1}^{j}$
    represent the trajectory features of object $i$ and $j$ while $feat_{t}^{i}$ and
    denotes the features of object $i$ and object $j$. After continuous optimization,
    it make the adversarial feature $feat_{t}^{j}$ instead of $feat_{t}^{j}$ be classified
    as trajectory $k$.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $d_{feat}()$ 表示余弦距离，$a_{t-1}^{i}$ 和 $a_{t-1}^{j}$ 代表对象 $i$ 和 $j$ 的轨迹特征，而
    $feat_{t}^{i}$ 和 $feat_{t}^{j}$ 表示对象 $i$ 和对象 $j$ 的特征。经过持续优化，它使对抗特征 $feat_{t}^{j}$
    而不是 $feat_{t}^{j}$ 被归类为轨迹 $k$。
- en: '![Refer to caption](img/8d86380cf2a4742b6189ac1ecafc1a17.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/8d86380cf2a4742b6189ac1ecafc1a17.png)'
- en: 'Figure 5: Push-pull Loss Function [[147](#bib.bib147)]'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '图5: 推拉损失函数 [[147](#bib.bib147)]'
- en: Algorithm 9\. Center Leaping [[147](#bib.bib147)]
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 算法9\. 中心跳跃 [[147](#bib.bib147)]
- en: With the PushPull loss function optimization described above, it is able to
    make the object tracking misled against attacks, which is still difficult to succeed
    when the difference between the two objects is large. The idea of the Center Leaping
    algorithm is to first mislead the objective recognition link so that the objective
    candidate box identified by the target recognition system is shifted towards the
    target to be misled, thus achieving a better attack success rate when there is
    a large deviation in the distance and size difference between the two tracked
    objects. The loss function is
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述推拉损失函数优化，它能够使目标跟踪在面对攻击时误导系统，即使当两个对象之间的差异很大时也仍然难以成功。中心跳跃算法的思想是首先误导目标识别链，使得目标识别系统识别的目标候选框向需要误导的目标偏移，从而在两个被跟踪对象之间的距离和尺寸差异较大时实现更好的攻击成功率。损失函数为
- en: '|  | <math   alttext="\begin{split}L&amp;=min\sum_{k\in\left\{i,j\right\}}d_{box}(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &amp;=min\sum_{k\in\left\{i,j\right\}}d(cent(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}L&amp;=min\sum_{k\in\left\{i,j\right\}}d_{box}(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &amp;=min\sum_{k\in\left\{i,j\right\}}d(cent(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\'
- en: '&amp;+min\sum_{k\in\left\{i,j\right\}}d(size(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;+min\sum_{k\in\left\{i,j\right\}}d(size(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\'
- en: '&amp;+min\sum_{k\in\left\{i,j\right\}}d(off(K(m_{t-1}^{\widetilde{k}},box_{t}^{k})))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mi >L</mi></mtd><mtd columnalign="left" ><mrow
    ><mo >=</mo><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >n</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><munder ><mo movablelimits="false" >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow
    ><mo >{</mo><mi >i</mi><mo >,</mo><mi >j</mi><mo >}</mo></mrow></mrow></munder><mrow
    ><msub ><mi >d</mi><mrow ><mi >b</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >x</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi >K</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >m</mi><mrow
    ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover accent="true" ><mi >k</mi><mo
    >~</mo></mover></msubsup><mo >,</mo><mrow ><mi >b</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><msubsup ><mi >x</mi><mi
    >t</mi><mi >k</mi></msubsup></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mo >=</mo><mi >m</mi><mi >i</mi><mi >n</mi><munder ><mo movablelimits="false"
    >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow ><mo >{</mo><mi >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><mi >d</mi><mrow ><mo stretchy="false"
    >(</mo><mi >c</mi><mi >e</mi><mi >n</mi><mi >t</mi><mrow ><mo stretchy="false"
    >(</mo><mi >K</mi><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >m</mi><mrow
    ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover accent="true" ><mi >k</mi><mo
    >~</mo></mover></msubsup><mo >,</mo><mi >b</mi><mi >o</mi><msubsup ><mi >x</mi><mi
    >t</mi><mi >k</mi></msubsup><mo stretchy="false" >)</mo></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow
    ><mo >+</mo><mi >m</mi><mi >i</mi><mi >n</mi><munder ><mo movablelimits="false"
    >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow ><mo >{</mo><mi >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><mi >d</mi><mrow ><mo stretchy="false"
    >(</mo><mi >s</mi><mi >i</mi><mi >z</mi><mi >e</mi><mrow ><mo stretchy="false"
    >(</mo><mi >K</mi><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >m</mi><mrow
    ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover accent="true" ><mi >k</mi><mo
    >~</mo></mover></msubsup><mo >,</mo><mi >b</mi><mi >o</mi><msubsup ><mi >x</mi><mi
    >t</mi><mi >k</mi></msubsup><mo stretchy="false" >)</mo></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow
    ><mo >+</mo><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >n</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><munder ><mo movablelimits="false" >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow
    ><mo >{</mo><mi >i</mi><mo >,</mo><mi >j</mi><mo >}</mo></mrow></mrow></munder><mrow
    ><mi >d</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow
    ><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >f</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >f</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mi >K</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >m</mi><mrow ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover
    accent="true" ><mi >k</mi><mo >~</mo></mover></msubsup><mo >,</mo><mrow ><mi >b</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><msubsup
    ><mi >x</mi><mi >t</mi><mi >k</mi></msubsup></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{split}L&=min\sum_{k\in\left\{i,j\right\}}d_{box}(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &=min\sum_{k\in\left\{i,j\right\}}d(cent(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &+min\sum_{k\in\left\{i,j\right\}}d(size(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &+min\sum_{k\in\left\{i,j\right\}}d(off(K(m_{t-1}^{\widetilde{k}},box_{t}^{k})))\end{split}</annotation></semantics></math>
    |  | (11) |'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;+min\sum_{k\in\left\{i,j\right\}}d(off(K(m_{t-1}^{\widetilde{k}},box_{t}^{k})))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><mi >L</mi></mtd><mtd columnalign="left" ><mrow
    ><mo >=</mo><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >n</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><munder ><mo movablelimits="false" >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow
    ><mo >{</mo><mi >i</mi><mo >,</mo><mi >j</mi><mo >}</mo></mrow></mrow></munder><mrow
    ><msub ><mi >d</mi><mrow ><mi >b</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >x</mi></mrow></msub><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow ><mi >K</mi><mo lspace="0em"
    rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >m</mi><mrow
    ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover accent="true" ><mi >k</mi><mo
    >~</mo></mover></msubsup><mo >,</mo><mrow ><mi >b</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><msubsup ><mi >x</mi><mi
    >t</mi><mi >k</mi></msubsup></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left"
    ><mrow ><mo >=</mo><mi >m</mi><mi >i</mi><mi >n</mi><munder ><mo movablelimits="false"
    >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow ><mo >{</mo><mi >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><mi >d</mi><mrow ><mo stretchy="false"
    >(</mo><mi >c</mi><mi >e</mi><mi >n</mi><mi >t</mi><mrow ><mo stretchy="false"
    >(</mo><mi >K</mi><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >m</mi><mrow
    ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover accent="true" ><mi >k</mi><mo
    >~</mo></mover></msubsup><mo >,</mo><mi >b</mi><mi >o</mi><msubsup ><mi >x</mi><mi
    >t</mi><mi >k</mi></msubsup><mo stretchy="false" >)</mo></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow
    ><mo >+</mo><mi >m</mi><mi >i</mi><mi >n</mi><munder ><mo movablelimits="false"
    >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow ><mo >{</mo><mi >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><mi >d</mi><mrow ><mo stretchy="false"
    >(</mo><mi >s</mi><mi >i</mi><mi >z</mi><mi >e</mi><mrow ><mo stretchy="false"
    >(</mo><mi >K</mi><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >m</mi><mrow
    ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover accent="true" ><mi >k</mi><mo
    >~</mo></mover></msubsup><mo >,</mo><mi >b</mi><mi >o</mi><msubsup ><mi >x</mi><mi
    >t</mi><mi >k</mi></msubsup><mo stretchy="false" >)</mo></mrow><mo stretchy="false"
    >)</mo></mrow></mrow></mrow></mtd></mtr><mtr ><mtd  columnalign="left" ><mrow
    ><mo >+</mo><mrow ><mi >m</mi><mo lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >n</mi><mo lspace="0em" rspace="0em" >​</mo><mrow
    ><munder ><mo movablelimits="false" >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow
    ><mo >{</mo><mi >i</mi><mo >,</mo><mi >j</mi><mo >}</mo></mrow></mrow></munder><mrow
    ><mi >d</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false" >(</mo><mrow
    ><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >f</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >f</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><mrow ><mi >K</mi><mo lspace="0em" rspace="0em" >​</mo><mrow ><mo stretchy="false"
    >(</mo><msubsup ><mi >m</mi><mrow ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover
    accent="true" ><mi >k</mi><mo >~</mo></mover></msubsup><mo >,</mo><mrow ><mi >b</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><msubsup
    ><mi >x</mi><mi >t</mi><mi >k</mi></msubsup></mrow><mo stretchy="false" >)</mo></mrow></mrow><mo
    stretchy="false" >)</mo></mrow></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mrow></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{split}L&=min\sum_{k\in\left\{i,j\right\}}d_{box}(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &=min\sum_{k\in\left\{i,j\right\}}d(cent(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &+min\sum_{k\in\left\{i,j\right\}}d(size(K(m_{t-1}^{\widetilde{k}},box_{t}^{k}))\\
    &+min\sum_{k\in\left\{i,j\right\}}d(off(K(m_{t-1}^{\widetilde{k}},box_{t}^{k})))\end{split}</annotation></semantics></math>
    |  | (11) |'
- en: Of which $m_{t}^{k}$ and $box_{t}^{k}$ respectively represent the trajectory
    state and the candidate frame of target $k$ at time $t$; $cent()$, $size()$, and
    $off()$ respectively represent the centre point position, the size, and the offset
    of the candidate box; and $d()$ represents distance $L_{1}$.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$m_{t}^{k}$和$box_{t}^{k}$分别表示时间$t$时目标$k$的轨迹状态和候选帧；cent()，size()和off()分别表示候选框的中心点位置，大小和偏移；d()表示距离$L_{1}$。
- en: The central leaping algorithm can be expressed as
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 中心跃迁算法可以表示为。
- en: '|  | $\begin{split}L_{cl}=\sum_{k\in\left\{i,j\right\}}(&amp;\sum_{(x,y)\in
    B_{c->\widetilde{k}}}(1-M_{x,y}^{\gamma}log(M_{x,y})+\\ &amp;\sum_{(x,y)\in B_{c->k}}(M_{x,y}^{\gamma}log(1-M_{P}{x,y})))\end{split}$
    |  | (12) |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}L_{cl}=\sum_{k\in\left\{i,j\right\}}(&amp;\sum_{(x,y)\in
    B_{c->\widetilde{k}}}(1-M_{x,y}^{\gamma}log(M_{x,y})+\\ &amp;\sum_{(x,y)\in B_{c->k}}(M_{x,y}^{\gamma}log(1-M_{P}{x,y})))\end{split}$
    |  | (12) |'
- en: $M(x,y)$ denotes the heat value of $(x,y)$, $c_{k}$ denotes the center of the
    object, $c\rightarrow\widetilde{k}$ denotes the direction from $c_{k}$ to $cent(K(m^{\widetilde{k}}_{t-1}))$.
    During the optimization process, the center point will move to the adjacent grid
    along this direction. In the object recognition and tracking system, the heat
    value of the original target center will drop, and the heat value in the direction
    close to the object will rise, so as to achieve the goal of the candidate frame
    approaching the object.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: $M(x,y)$表示$(x,y)$的热值，$c_{k}$表示物体的中心，$c\rightarrow\widetilde{k}$表示从$c_{k}$到$cent(K(m^{\widetilde{k}}_{t-1}))$的方向。在优化过程中，中心点将沿着这个方向移动到相邻的网格。在物体识别和跟踪系统中，原始目标中心的热值会下降，而接近物体的方向的热值会上升，从而实现候选帧向物体靠近的目标。
- en: Similarly, it is able to integrate the size loss function and the offset loss
    function into a A novel composite loss function.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，它能够将大小损失函数和偏移损失函数整合成为一种新的复合损失函数。
- en: '|  | <math   alttext="\begin{split}L_{reg}&amp;=L_{size}+L_{off}\\ &amp;=\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(size(K(m_{t-1}^{\widetilde{k}}),size(box_{t}^{k}))\\'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}L_{reg}&amp;=L_{size}+L_{off}\\ &amp;=\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(size(K(m_{t-1}^{\widetilde{k}}),size(box_{t}^{k}))\\'
- en: '&amp;+\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(off(K(m_{t-1}^{\widetilde{k}}),off(box_{t}^{k}))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><msub ><mi >L</mi><mrow ><mi >r</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >e</mi><mo lspace="0em" rspace="0em" >​</mo><mi >g</mi></mrow></msub></mtd><mtd
    columnalign="left" ><mrow ><mo >=</mo><mrow ><msub ><mi >L</mi><mrow ><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >z</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi></mrow></msub><mo >+</mo><msub
    ><mi >L</mi><mrow ><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >f</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >f</mi></mrow></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mo rspace="0.111em" >=</mo><munder ><mo movablelimits="false"
    >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow ><mo >{</mo><mi >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><msubsup ><mi >L</mi><mn >1</mn><mrow
    ><mi >s</mi><mo lspace="0em" rspace="0em" >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi >h</mi></mrow></msubsup><mrow
    ><mo stretchy="false" >(</mo><mi >s</mi><mi >i</mi><mi >z</mi><mi >e</mi><mrow
    ><mo stretchy="false" >(</mo><mi >K</mi><mrow ><mo stretchy="false" >(</mo><msubsup
    ><mi >m</mi><mrow ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover accent="true"
    ><mi >k</mi><mo >~</mo></mover></msubsup><mo stretchy="false" >)</mo></mrow><mo
    >,</mo><mi >s</mi><mi >i</mi><mi >z</mi><mi >e</mi><mrow ><mo stretchy="false"
    >(</mo><mi >b</mi><mi >o</mi><msubsup ><mi >x</mi><mi >t</mi><mi >k</mi></msubsup><mo
    stretchy="false" >)</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left" ><mrow ><mo rspace="0.055em" >+</mo><munder ><mo movablelimits="false"
    >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow ><mo >{</mo><mi >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><msubsup ><mi >L</mi><mn >1</mn><mrow
    ><mi >s</mi><mo lspace="0em" rspace="0em" >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi >h</mi></mrow></msubsup><mrow
    ><mo stretchy="false" >(</mo><mi >o</mi><mi >f</mi><mi >f</mi><mrow ><mo stretchy="false"
    >(</mo><mi >K</mi><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >m</mi><mrow
    ><mi >t</mi><mo >−</mo><mn >1</mn></mrow><mover accent="true" ><mi >k</mi><mo
    >~</mo></mover></msubsup><mo stretchy="false" >)</mo></mrow><mo >,</mo><mi >o</mi><mi
    >f</mi><mi >f</mi><mrow ><mo stretchy="false" >(</mo><mi >b</mi><mi >o</mi><msubsup
    ><mi >x</mi><mi >t</mi><mi >k</mi></msubsup><mo stretchy="false" >)</mo></mrow><mo
    stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{split}L_{reg}&=L_{size}+L_{off}\\ &=\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(size(K(m_{t-1}^{\widetilde{k}}),size(box_{t}^{k}))\\
    &+\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(off(K(m_{t-1}^{\widetilde{k}}),off(box_{t}^{k}))\end{split}</annotation></semantics></math>
    |  | (13) |'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '&amp;+\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(size(K(m_{t-1}^{\widetilde{k}}),size(box_{t}^{k}))\end{split}"
    display="block"><semantics ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt"
    ><mtr ><mtd columnalign="right" ><msub ><mi >L</mi><mrow ><mi >r</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >e</mi><mo lspace="0em" rspace="0em" >​</mo><mi >g</mi></mrow></msub></mtd><mtd
    columnalign="left" ><mrow ><mo >=</mo><mrow ><msub ><mi >L</mi><mrow ><mi >s</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >i</mi><mo lspace="0em" rspace="0em" >​</mo><mi
    >z</mi><mo lspace="0em" rspace="0em" >​</mo><mi >e</mi></mrow></msub><mo >+</mo><msub
    ><mi >L</mi><mrow ><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >f</mi><mo
    lspace="0em" rspace="0em" >​</mo><mi >f</mi></mrow></msub></mrow></mrow></mtd></mtr><mtr
    ><mtd  columnalign="left" ><mrow ><mo rspace="0.111em" >=<mo><munder ><mo movablelimits="false"
    >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow ><mo >{</mo><mi >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><msubsup ><mi >L</mi><mn >1</mn><mrow
    ><mi >s</mi><mo lspace="0em" rspace="0em" >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi >h</mi></mrow></msubsup><mrow
    ><mo stretchy="false" >(</mo><mi >size</mi><mrow ><mo stretchy="false" >(</mo><mi
    >K</mi><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >m</mi><mrow ><mi >t</mi><mo
    >−</mo><mn >1</mn></mrow><mover accent="true" ><mi >k</mi><mo >~</mo></mover></msubsup><mo
    stretchy="false" >)</mo></mrow><mo >,</mo><mi >size</mi><mrow ><mo stretchy="false"
    >(</mo><mi >b</mi><mi >o</mi><msubsup ><mi >x</mi><mi >t</mi><mi >k</mi></msubsup><mo
    stretchy="false" >)</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr><mtr
    ><mtd columnalign="left" ><mrow ><mo rspace="0.055em" >+</mo><munder ><mo movablelimits="false"
    >∑</mo><mrow ><mi >k</mi><mo >∈</mo><mrow ><mo >{</mo><mi >i</mi><mo >,</mo><mi
    >j</mi><mo >}</mo></mrow></mrow></munder><msubsup ><mi >L</mi><mn >1</mn><mrow
    ><mi >s</mi><mo lspace="0em" rspace="0em" >​</mo><mi >m</mi><mo lspace="0em" rspace="0em"
    >​</mo><mi >o</mi><mo lspace="0em" rspace="0em" >​</mo><mi >o</mi><mo lspace="0em"
    rspace="0em" >​</mo><mi >t</mi><mo lspace="0em" rspace="0em" >​</mo><mi >h</mi></mrow></msubsup><mrow
    ><mo stretchy="false" >(</mo><mi >off</mi><mrow ><mo stretchy="false" >(</mo><mi
    >K</mi><mrow ><mo stretchy="false" >(</mo><msubsup ><mi >m</mi><mrow ><mi >t</mi><mo
    >−</mo><mn >1</mn></mrow><mover accent="true" ><mi >k</mi><mo >~</mo></mover></msubsup><mo
    stretchy="false" >)</mo></mrow><mo >,</mo><mi >off</mi><mrow ><mo stretchy="false"
    >(</mo><mi >b</mi><mi >o</mi><msubsup ><mi >x</mi><mi >t</mi><mi >k</mi></msubsup><mo
    stretchy="false" >)</mo></mrow><mo stretchy="false" >)</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation
    encoding="application/x-tex" >\begin{split}L_{reg}&=L_{size}+L_{off}\\ &=\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(size(K(m_{t-1}^{\widetilde{k}}),size(box_{t}^{k}))\\
    &+\sum_{k\in\left\{i,j\right\}}L_{1}^{smooth}(off(K(m_{t-1}^{\widetilde{k}}),off(box_{t}^{k}))\end{split}</annotation></semantics></math>
    |  | (13)'
- en: 'where $L_{1}^{smooth}$ is smooth loss function based on $L_{1}$:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L_{1}^{smooth}$ 是基于 $L_{1}$ 的平滑损失函数：
- en: '|  | $L_{1}^{smooth}(a,b)=\left\{\begin{matrix}0.5\cdot(a-b)^{2}&amp;if&#124;a-b&#124;<1\\
    &#124;a-b&#124;-0.5&amp;else\end{matrix}\right.$ |  | (14) |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | $L_{1}^{smooth}(a,b)=\left\{\begin{matrix}0.5\cdot(a-b)^{2}&amp; \text{如果}
    \; |a-b| < 1\\ |a-b| - 0.5 & \text{否则}\end{matrix}\right.$ |  | (14) |'
- en: '![Refer to caption](img/2d834c11121af354f403debd3f2c49c7.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/2d834c11121af354f403debd3f2c49c7.png)'
- en: 'Figure 6: Centre Leaping Principle[[147](#bib.bib147)]'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：中心跃迁原理[[147](#bib.bib147)]
- en: 3.6 AI Backdoor & Poisoning on ADS
  id: totrans-252
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6 AI 后门与毒化攻击在自动驾驶系统中的应用
- en: Artificial intelligence models are often generated from a certain amount of
    training data. Some scholars have found that if the training data is not trustworthy,
    it may lead to the generation of models with "backdoor", which can be manipulated
    by attackers in the subsequent use of the models, causing serious security risks.
    Currently, the concepts of "AI backdoor", "AI model poisoning" and "AI Trojan
    horse" have some similarities, but are not expressed in the same way in different
    literature. One type of attack is called Training-only attacks, or Poisoning Attacks
    are usually defined as attacker contaminating part of the training data or modifying
    the labels of the training data. On the contrary, another type of attack is called
    Backdoor Attacks or AI Trojans[[13](#bib.bib13)], in which attackers must participate
    in both training and testing. Both poisoning attacks and AI backdoor attacks can
    cause serious security threats to autonomous driving system, and this paper refers
    to these two types of attacks as "AI backdoor attacks", where a specific malicious
    modification is made to a target model in a specific way, causing the model to
    make harmful judgments about a specifically predicted example. There are similarities
    and differences between backdoor attacks and adversarial example attacks. Adversarial
    examples usually do not change the model itself and will not damage the integrity
    of the AI model, but mainly interfere with the test examples and affect the availability
    and correctness of the machine learning model. On another hand, AI backdoor takes
    the form of modification of the AI model, poison of the training data, aggregation
    of the backdoor model, etc., causing tiny changes to the AI model, affecting both
    the integrity of the AI model. AI backdoor attacks tend to be more hidden, highly
    universal, and more damaging. So far, there are two major methods to implement
    AI backdoor, one is Data Poisoning Attack, and the other is Model Poisoning Attack.
    Data Poisoning Attack means attacker adds a small amount of poisoning data into
    training dataset, so that the resulting AI model has a backdoor, and the AI model
    may make a specific judgement when the predicted example contains a "trigger".
    Model Poisoning Attack means the attacker directly modifies the model or indirectly
    fuses the target model with a harmful model by using model integration, federated
    learning, and transition learning, causing the model to make a directed and erroneous
    judgment on a specific prediction example.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能模型通常是从一定量的训练数据中生成的。一些学者发现，如果训练数据不可信，可能会导致生成带有“后门”的模型，这些模型在后续使用中可能被攻击者操控，从而引发严重的安全风险。目前，“AI
    后门”、“AI 模型毒化”和“AI 木马”的概念有一些相似之处，但在不同的文献中表述方式不同。一种攻击被称为仅训练攻击，或称为毒化攻击，通常定义为攻击者污染部分训练数据或修改训练数据的标签。相对而言，另一种攻击称为后门攻击或
    AI 木马[[13](#bib.bib13)]，攻击者必须同时参与训练和测试。毒化攻击和 AI 后门攻击都可能对自动驾驶系统造成严重的安全威胁，本文将这两种攻击称为“AI
    后门攻击”，即对目标模型进行特定的恶意修改，导致模型对特定预测样本做出有害的判断。后门攻击与对抗样本攻击之间存在相似性和差异性。对抗样本通常不会改变模型本身，也不会损害
    AI 模型的完整性，但主要干扰测试样本，影响机器学习模型的可用性和正确性。另一方面，AI 后门采取的形式包括对 AI 模型的修改、对训练数据的毒化、对后门模型的聚合等，导致
    AI 模型发生微小变化，从而影响 AI 模型的完整性。AI 后门攻击往往更加隐蔽、普遍性更强且破坏性更大。目前，实现 AI 后门的主要方法有两种，一种是数据毒化攻击，另一种是模型毒化攻击。数据毒化攻击是指攻击者将少量毒化数据添加到训练数据集中，使得生成的
    AI 模型具有后门，当预测样本中包含“触发器”时，AI 模型可能会做出特定的判断。模型毒化攻击是指攻击者直接修改模型或通过模型集成、联邦学习和迁移学习等方式间接将目标模型与有害模型融合，导致模型对特定预测样本做出有导向性的错误判断。
- en: It has been argued that AI backdoor attacks already exist in traditional machine
    learning. In 2008, Nelson et al. [[148](#bib.bib148), [149](#bib.bib149)] proposed
    the backdoor attack on Bayesian networks. In 2012 Biggio et al.[[150](#bib.bib150)]
    proposed backdoor attack on SVMs. In 2016, Alfeld et al.[[151](#bib.bib151)] proposed
    backdoor on auto-regressive prediction models. In 2017, Gu et al.[[3](#bib.bib3)]
    first proposed backdoor attack on deep learning, then AI backdoor became a promising
    research topic. The BadNet algorithm adds a small number of training data with
    the pre-designed pattern into the training data and labels such training examples
    with a specific target, then the trained model is likely to predict examples with
    "Trigger" according to the attacker. In the same year, Muñoz-González et al.[[152](#bib.bib152)]
    proposed a gradient-based algorithm for AI data poisoning. However, for autonomous
    driving system, the basic AI backdoor algorithms described above have two limitations.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为，AI 后门攻击已经存在于传统机器学习中。2008年，Nelson 等人[[148](#bib.bib148), [149](#bib.bib149)]
    提出了对贝叶斯网络的后门攻击。2012年，Biggio 等人[[150](#bib.bib150)] 提出了对支持向量机的后门攻击。2016年，Alfeld
    等人[[151](#bib.bib151)] 提出了对自回归预测模型的后门攻击。2017年，Gu 等人[[3](#bib.bib3)] 首次提出了对深度学习的后门攻击，从而使AI后门成为一个有前途的研究主题。BadNet
    算法将少量带有预设计模式的训练数据添加到训练数据中，并将这些训练样本标记为特定目标，然后训练出的模型可能根据攻击者的要求预测带有“触发器”的示例。同年，Muñoz-González
    等人[[152](#bib.bib152)] 提出了基于梯度的AI数据中毒算法。然而，对于自动驾驶系统，上述基本AI后门算法有两个限制。
- en: •
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Control right of training data by attacker.As it requires the attacker to be
    able to contaminate a certain amount of training data, this requires the attacker
    to have some control over the training data; at the very least the attacker needs
    to have background knowledge of the target model’s structure, parameters, etc.,
    which places certain requirements on the attacker.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 攻击者控制训练数据的权利。由于这要求攻击者能够污染一定量的训练数据，这就需要攻击者对训练数据有一定控制权；至少攻击者需要对目标模型的结构、参数等有一定的背景知识，这对攻击者提出了一定要求。
- en: •
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Concealment of attack. It relies on contaminating part of the training data
    by adding a ’pattern’ or changing the label of the data. Although the pattern
    may be relatively insidious, forcing patterns into normal examples may cause a
    certain amount of unnaturalness that may be detected by humans, or possibly by
    automated detection through some anomaly identification method. It may also lead
    to human feel a sense of inharmonious if the attacker modifies excessive label
    of the training data.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 攻击的隐蔽性。它依赖于通过添加“模式”或更改数据标签来污染部分训练数据。尽管模式可能相对隐蔽，但将模式强行加入正常示例中可能会导致一定程度的不自然，这可能被人类或通过某些异常检测方法自动检测到。如果攻击者过度修改训练数据的标签，也可能使人类感到不协调。
- en: In response to these limitations, researchers have made a number of subsequent
    improvements. On the one hand, attackers have improved the concealment of poisoning
    attacks by enhancing the concealment of the patterns in the example or minimizing
    the impact on the integrity of the label. One of the research directions is "clean
    label", which aims to keep the label of poisoned example semantically correct
    while realizing data poisoning. In 2018, Shafahi et al.[[153](#bib.bib153)] proposed
    the Poison Frogs algorithm, which was the first to implement the Clean Label attack
    for deep learning. In the same year, Truner et al.[[154](#bib.bib154)] proposed
    two methods of data generation based on adversarial network and adversarial example
    to achieve a label-consistent "clean example" attack. Another research direction
    is "Hidden Trigger", also known as "Invisible Trigger", which aims to optimize
    the trigger pattern to make it as invisible as possible to escape detection by
    humans and machines. In 2018, Suciu et al.[[155](#bib.bib155)] , and in 2019 Saha
    et al.[[156](#bib.bib156)] put forward "hidden trigger" which can generate trigger
    patterns that humans are unable to directly perceive through the senses. In 2020,
    Wallace et al.[[157](#bib.bib157)] devised a "hidden trigger" poisoning attack
    in the field of natural language processing. In the same year, Li et al.[[158](#bib.bib158)]
    used information hiding and regularization methods to improve the bad net algorithm
    to improve the invisibility of the trigger pattern.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 针对这些限制，研究人员进行了多项后续改进。一方面，攻击者通过增强示例中模式的隐蔽性或最小化对标签完整性的影响来改进毒害攻击的隐蔽性。其中一个研究方向是**“干净标签”**，其目标是在实现数据毒害的同时保持被污染示例的标签语义正确。2018年，Shafahi等人[[153](#bib.bib153)]提出了**Poison
    Frogs**算法，这是第一个实现深度学习**干净标签攻击**的方法。同年，Truner等人[[154](#bib.bib154)]提出了基于对抗网络和对抗示例的两种数据生成方法，以实现标签一致的**“干净示例”**攻击。另一个研究方向是**“隐藏触发器”**，也称为**“隐形触发器”**，其目标是优化触发模式，使其尽可能不被人类和机器检测到。2018年，Suciu等人[[155](#bib.bib155)]，以及2019年，Saha等人[[156](#bib.bib156)]提出了**“隐藏触发器”**，能够生成人类无法通过感官直接感知的触发模式。2020年，Wallace等人[[157](#bib.bib157)]在自然语言处理领域设计了一种**“隐藏触发器”**毒害攻击。同年，Li等人[[158](#bib.bib158)]利用信息隐藏和正则化方法改进了**bad
    net**算法，以提高触发模式的隐蔽性。
- en: On the other hand, the attacker reduces the proportion of contaminated training
    data as much as possible, or even implements a black-box attack that does not
    require contaminating data, thus reducing the background knowledge required for
    the attack and lowering the threshold for implementing the attack. In 2017, Liu
    et al. [[159](#bib.bib159)] implemented a black-box approach to generate backdoor
    by exploiting the migratory nature of the attack, but such backdoor mainly exists
    in the fully connected layer at the end of the AI model, which can easily fail
    once the model is fine-tuned; in the same year, Chen et al.[[38](#bib.bib38)]
    proposed a machine learning-based approach to generate AI backdoor, which eliminates
    the need for attackers to understand the structure of the target system and other
    information, and reduces the background knowledge requirement; in 2019, Yao et
    al.[[160](#bib.bib160)] proposed "latent triggers", which are first generated
    in the "teacher model" and then migrated to the "student model" through transition
    learning. The backdoor is not only found in the last fully connected layer of
    the student model, but also in all of its layers and thus the difficulty of detecting
    the "backdoor" through analysis is increased. In the same year, Zhu et al.[[161](#bib.bib161)]
    investigated the migratory nature of clean label attacks and used knowledge transition
    to realize a black-box clean label attack.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，攻击者尽可能减少被污染训练数据的比例，甚至实现无需污染数据的黑箱攻击，从而减少实施攻击所需的背景知识，降低实施攻击的门槛。2017年，Liu等人[[159](#bib.bib159)]实施了一种黑箱方法，通过利用攻击的迁移性质生成后门，但这种后门主要存在于AI模型的全连接层末端，一旦模型进行微调就容易失效；同年，Chen等人[[38](#bib.bib38)]提出了一种基于机器学习的方法生成AI后门，消除了攻击者对目标系统结构及其他信息的理解需求，减少了背景知识要求；2019年，Yao等人[[160](#bib.bib160)]提出了**“潜在触发器”**，这些触发器首先在**“教师模型”**中生成，然后通过迁移学习迁移到**“学生模型”**中。后门不仅存在于学生模型的最后一个全连接层中，还存在于其所有层中，从而增加了通过分析检测**“后门”**的难度。同年，Zhu等人[[161](#bib.bib161)]研究了干净标签攻击的迁移性质，并利用知识迁移实现了黑箱干净标签攻击。
- en: In the field of autonomous driving, in 2018, Liu et al.[[162](#bib.bib162)]
    realized AI backdoor attacks in a variety of environments, including simulated
    autonomous driving platforms. In 2019, Rehman et al.[[163](#bib.bib163)] implemented
    an AI backdoor attack on traffic signs in the physical world; Barni et al.[[164](#bib.bib164)]
    conducted a clean label poisoning attack on traffic signs; Ding et al. from Nanjing
    University[[165](#bib.bib165)] designed a "natural trigger" for autonomous driving
    system to trigger AI model backdoor in special weather like a rainy day, to make
    red lights incorrectly identified as green lights and numbers incorrectly identified
    in a specific way; Yao et al.[[160](#bib.bib160)] from the University of Chicago
    used their proposed "latent trigger" method to generate backdoor traffic signs
    for a variety of models, generating human-imperceptible triggers on physical traffic
    signs. In 2021, Tian et al.[[166](#bib.bib166)] achieved a clean label attack
    on 3D cloud point map. In 2022, Udeshi et al.[[167](#bib.bib167)] proposed an
    anti-backdoor attack method that can be used in traffic sign recognition scenarios,
    which achieved avoiding AI backdoor attacks by filtering the triggers in the captured
    images and correcting the prediction examples.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在自动驾驶领域，2018年，Liu 等人[[162](#bib.bib162)] 实现了在各种环境下的AI后门攻击，包括模拟自动驾驶平台。2019年，Rehman
    等人[[163](#bib.bib163)] 在现实世界中的交通标志上实施了AI后门攻击；Barni 等人[[164](#bib.bib164)] 对交通标志进行了干净标签中毒攻击；南京大学的
    Ding 等人[[165](#bib.bib165)] 设计了一个“自然触发器”以在特殊天气（如雨天）下触发AI模型后门，使红灯错误地被识别为绿灯，数字以特定方式被错误识别；芝加哥大学的
    Yao 等人[[160](#bib.bib160)] 使用他们提出的“潜在触发器”方法生成了适用于各种模型的后门交通标志，在实物交通标志上生成了人眼无法察觉的触发器。2021年，Tian
    等人[[166](#bib.bib166)] 实现了对3D点云地图的干净标签攻击。2022年，Udeshi 等人[[167](#bib.bib167)] 提出了一个反后门攻击的方法，可用于交通标志识别场景，通过过滤捕获图像中的触发器和校正预测示例来避免AI后门攻击。
- en: Algorithm 9\. Feature Collisions
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 9\. 特征碰撞
- en: Feature collision is the more common method of AI backdoor generation, where
    the attacker first selects a target instance from the test set. To achieve poisoning,
    the attacker chooses a base class instance from the base class and makes imperceptible
    changes to it, thereby generating a poisoned instance that is injected into the
    training data later; then, during the training phase, the model is trained using
    a poisoned data set consisting of a clean data set plus poisoned instance; in
    the reasoning phase, this causes the target instance to be mistaken by the misclassification
    model for being in the base class during testing. It is described formally as
    follows.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 特征碰撞是更常见的AI后门生成方法，攻击者首先从测试集中选择一个目标实例。为了实现中毒，攻击者选择一个基类实例并对其进行微妙的更改，从而生成一个被污染的实例，随后将其注入训练数据中；然后，在训练阶段，模型使用由干净数据集和被污染实例组成的污染数据集进行训练；在推理阶段，这导致目标实例在测试时被误分类模型错误地认为属于基类。其形式描述如下。
- en: $f(x)$ represents the neural network’s prediction on input example $x$. Example
    $x$ colliding with the target is found in the feature space and then computed
    to be close to the base instance $b$. The target function is
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: $f(x)$ 表示神经网络对输入示例 $x$ 的预测。示例 $x$ 在特征空间中与目标发生碰撞，然后计算其接近基实例 $b$。目标函数是
- en: '|  | $p=arg\min\left\&#124;f(x)-f(t)\right\&#124;^{2}_{2}+\beta\left\&#124;x-b\right\&#124;^{2}_{2}$
    |  | (15) |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  | $p=arg\min\left\&#124;f(x)-f(t)\right\&#124;^{2}_{2}+\beta\left\&#124;x-b\right\&#124;^{2}_{2}$
    |  | (15) |'
- en: $p$ is the poisoning instance, which will be misdirected as the attack target.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: $p$ 是中毒实例，将被误导为攻击目标。
- en: 3.7 Summary
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7 总结
- en: The risk of adversarial example and AI backdoor is brought by the characteristics
    of deep learning itself. Whether an autonomous driving system uses RGB cameras,
    LiDAR, RaDAR or other sensors as the source of information collection, it often
    dependent on deep learning for perception and driving decisions. Going with it,
    there are new safety risks associated with artificial intelligence. At the same
    time, the autonomous driving system is a huge system, and in the perception layer
    alone, they consist of many links that rely on deep learning technologies, such
    as target recognition, image segmentation, depth estimation and target tracking,
    which constitute a complex decision-making process, and each link is also subject
    to different types of AI security threats. It is necessary to ensure the safety
    of each link to constitute a set of safe autonomous driving system.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本和人工智能后门的风险来源于深度学习本身的特性。无论自动驾驶系统使用 RGB 摄像头、LiDAR、RaDAR 还是其他传感器作为信息采集的来源，它通常依赖深度学习进行感知和驾驶决策。随之而来的是人工智能相关的新安全风险。同时，自动驾驶系统是一个庞大的系统，在感知层
    alone，包含许多依赖深度学习技术的环节，如目标识别、图像分割、深度估计和目标跟踪，这构成了一个复杂的决策过程，每个环节也受到不同类型的 AI 安全威胁。因此，有必要确保每个环节的安全，以构建一套安全的自动驾驶系统。
- en: Generated adversarial examples are hard to consistently fool neural network
    classifiers in the physical world. In this chapter, we introduced emphatically
    method of physical world adversarial examples enhancement, summarized it in Table
    2.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的对抗样本在物理世界中难以持续欺骗神经网络分类器。在本章中，我们重点介绍了物理世界对抗样本增强的方法，并在表 2 中进行了总结。
- en: 'Table 2: Physical World Adversarial Examples Enhance Methods'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：物理世界对抗样本增强方法
- en: '| Method | Contributions | Scenarios in Autonomous Driving |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 贡献 | 自动驾驶中的场景 |'
- en: '| EoT[[132](#bib.bib132)], | EoT generate adversarial examples over a chosen
    distribution of transformations.EoT is the first algorithm that produces robust
    adversarial examples, which single adversarial examples to an entire distribution
    of transformations | Object detection |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| EoT[[132](#bib.bib132)] | EoT 在选择的变换分布上生成对抗样本。EoT 是第一个能够产生鲁棒对抗样本的算法，它将单个对抗样本扩展到整个变换分布上。
    | 物体检测 |'
- en: '| Adversarial Patch[[138](#bib.bib138)] | This attack generates an image-independent
    patch that can then be placed anywhere within the field of view of the classifier,
    and causes the classifier to output a targeted class. | Object detection |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 对抗补丁[[138](#bib.bib138)] | 该攻击生成一个与图像无关的补丁，可以放置在分类器视场内的任何位置，并使分类器输出一个目标类别。
    | 物体检测 |'
- en: '| FIR[[168](#bib.bib168)] | FIR generated adversarial examples to impact both
    hidden layers and the final layer. Therefore, the misclassification for adversarial
    examples depends more on the prior layers in the neural-network, which lead to
    be more robust in physical scenarios. | Traffic sign detection |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| FIR[[168](#bib.bib168)] | FIR 生成的对抗样本影响隐藏层和最终层。因此，对抗样本的错误分类更多地依赖于神经网络中的前层，这使得在实际场景中更加稳健。
    | 交通标志检测 |'
- en: '| Nested-AE[[168](#bib.bib168)] | Nested-AE contains two or more Adversarial
    examples inside that for different distances or angles. It significantly improve
    the robustness of adversarial attack at the various position. | Traffic sign detection
    |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 嵌套对抗样本[[168](#bib.bib168)] | 嵌套对抗样本包含两个或更多的对抗样本，针对不同的距离或角度。这显著提高了对抗攻击在各种位置的鲁棒性。
    | 交通标志检测 |'
- en: '| Randomly Transformed Patch[[135](#bib.bib135)] | These transforms are a composition
    of brightness, contrast, rotation, translation, and sheering transforms that help
    make patches robust to variations caused by lighting and viewing angle that occur
    in the real world. | Pedestrian detection |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 随机变换补丁[[135](#bib.bib135)] | 这些变换包括亮度、对比度、旋转、平移和剪切变换，有助于使补丁对真实世界中的光照和视角变化具有鲁棒性。
    | 行人检测 |'
- en: '| TV Loss[[169](#bib.bib169), [170](#bib.bib170), [135](#bib.bib135)] | TV
    Loss ensures a more smooth patch in which all pixels in the patch get optimized.
    | Object detection |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| TV 损失[[169](#bib.bib169), [170](#bib.bib170), [135](#bib.bib135)] | TV 损失确保补丁的平滑度，使得补丁中的所有像素都得到优化。
    | 物体检测 |'
- en: '| Ensemble training[[135](#bib.bib135)] | Ensemble training fool an ensemble
    of detectors that were not used for training. | Object detection |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 集成训练[[135](#bib.bib135)] | 集成训练使得一个未用于训练的检测器集合变得无效。 | 物体检测 |'
- en: '| UPC [[171](#bib.bib171)] | UPC optimization constraint to make generated
    patterns look natural to human observers. | Object detection |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| UPC [[171](#bib.bib171)] | UPC优化约束使生成的图案对人类观察者看起来自然。 | 目标检测 |'
- en: '| NPS [[169](#bib.bib169)] | NPS deal with the difference in digital RGB-values
    and the ability of real printers to reproduce these values. | Object detection
    |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| NPS [[169](#bib.bib169)] | NPS处理数字RGB值与实际打印机再现这些值的能力之间的差异。 | 目标检测 |'
- en: '| Discrete Search[[139](#bib.bib139)] | Discrete Search improve the black-box
    attack by iteratively refining the camouflage using a mutation-based search method.
    | physical-world Black-box attack |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 离散搜索[[139](#bib.bib139)] | 离散搜索通过使用基于突变的搜索方法迭代地改进伪装，从而提升黑箱攻击的效果。 | 物理世界黑箱攻击
    |'
- en: '| MSF-Adv[[142](#bib.bib142)] | MSF-Adv generate adversarial examples in Lidar,
    RaDar, and fusion. | Vehicle detection |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| MSF-Adv[[142](#bib.bib142)] | MSF-Adv在激光雷达、雷达和融合中生成对抗样本。 | 车辆检测 |'
- en: '| Spatial Transformer Layer (STL) to project[[170](#bib.bib170)] | Many kinds
    of Projects imitate the form changes for rectangula adversarial patches after
    placing it in physical world. | Object detection |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 空间变换层 (STL) 投影[[170](#bib.bib170)] | 许多种投影模拟了在物理世界中放置矩形对抗补丁后的形态变化。 | 目标检测
    |'
- en: '| Sticker Projection[[170](#bib.bib170)] | Project the obtained adversarial
    examples with small perturbations in the projection parameters to make the attack
    more robust. | Object detection |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 贴纸投影[[170](#bib.bib170)] | 将获得的对抗样本通过在投影参数上进行小的扰动以增强攻击的鲁棒性。 | 目标检测 |'
- en: As the foundation, some major general adversarial examples algorithms list in
    Appendix A.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基础，一些主要的对抗样本算法列在附录A中。
- en: 4 Emerging Threats of Decision-Making Layer
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 决策层的新兴威胁
- en: The major function of the decision layer is to make the correct driving decision
    based on sensing and perception. In common autonomous driving architectures, the
    trajectory of dynamic objects, such as vehicles or pedestrians, must predicted.
    If the prediction process is maliciously interfered with by an attacker, vehicle
    may under security threat.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 决策层的主要功能是根据感知和传感做出正确的驾驶决策。在常见的自动驾驶架构中，必须预测动态物体（如车辆或行人）的轨迹。如果预测过程被攻击者恶意干扰，车辆可能会面临安全威胁。
- en: 4.1 Emerging Threat on Prediction-oriented attack techniques
  id: totrans-288
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 预测导向攻击技术的新兴威胁
- en: In general, autonomous driving systems need to predict the short-term or long-term
    spatial coordinates of various road agents such as cars, buses, pedestrians, rickshaws,
    and animals, etc. Predicting usually base on Recurrent Neural Network (RNN) techniques,
    the algorithms of which include LSTM, and Sequence to Sequence. Researchers have
    proposed attack methods for recurrent neural network algorithms. In 2016, Papernot
    et al.[[124](#bib.bib124)] proposed an RNN-oriented adversarial example attack,
    and many subsequent researchers have continued to improve the attack method and
    enhance the attack effect[[172](#bib.bib172), [173](#bib.bib173), [174](#bib.bib174),
    [175](#bib.bib175), [176](#bib.bib176)].
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，自动驾驶系统需要预测各种道路代理（如汽车、公共汽车、行人、人力车和动物等）的短期或长期空间坐标。预测通常基于递归神经网络（RNN）技术，其算法包括LSTM和序列到序列。研究人员已经提出了针对递归神经网络算法的攻击方法。2016年，Papernot等人[[124](#bib.bib124)]提出了一种面向RNN的对抗样本攻击，许多后续研究人员继续改进攻击方法并增强攻击效果[[172](#bib.bib172),
    [173](#bib.bib173), [174](#bib.bib174), [175](#bib.bib175), [176](#bib.bib176)]。
- en: 4.2 Emerging Threat in Imitation Learning
  id: totrans-290
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 模仿学习中的新兴威胁
- en: Imitation Learning and Reinforcement Learning are the two main approaches to
    driving decision making. Imitation Learning is a data-driven approach that imitates
    expert driver policies to make decisions[[177](#bib.bib177)] and some end-to-end
    autonomous driving systems use an imitation learning framework[[178](#bib.bib178)].
    Reinforcement learning, on the other hand, uses deep reinforcement learning algorithms
    to optimize the model and make the best decisions. Whether a autonomous driving
    system adopts an imitation learning or a reinforcement learning architecture,
    an attacker could interfere with the AI model, thereby affecting normal driving
    decisions to pose a risk to the autonomous driving system.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习和强化学习是驾驶决策的两种主要方法。模仿学习是一种数据驱动的方法，通过模仿专家驾驶策略来做出决策[[177](#bib.bib177)]，一些端到端自主驾驶系统使用模仿学习框架[[178](#bib.bib178)]。而强化学习则使用深度强化学习算法来优化模型并做出最佳决策。无论自主驾驶系统采用模仿学习还是强化学习架构，攻击者都可能干扰
    AI 模型，从而影响正常的驾驶决策，给自主驾驶系统带来风险。
- en: Imitation learning can be described as a process[[44](#bib.bib44)], and human
    expert experience can be described as a tuple such as $(s,a,r,{s}^{\prime})$,
    where s is the state of driving, $a$ is the behavior of the human expert, $r$
    is the reward created by the behaviour $a$, and $s^{\prime}$ is the resulting
    new state. Imitation learning generates policy $\pi$ through machine learning,
    based on the captured set of behaviour of the human experts $D=(x_{i},y_{i})$.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习可以被描述为一个过程[[44](#bib.bib44)]，而人类专家经验可以被描述为一个元组，例如 $(s,a,r,{s}^{\prime})$，其中
    s 是驾驶状态，$a$ 是人类专家的行为，$r$ 是由行为 $a$ 产生的奖励，$s^{\prime}$ 是结果新状态。模仿学习通过机器学习生成策略 $\pi$，基于捕获的人类专家行为集合
    $D=(x_{i},y_{i})$。
- en: '|  | $u(t)=\pi(x(t),t,\alpha)$ |  |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '|  | $u(t)=\pi(x(t),t,\alpha)$ |  |'
- en: $u$ is the predicted behavior given by the machine, $x$ is the feature vector
    of the state of the environment $s$, $t$ is the time, and $\alpha$ is the set
    of parameters for a set of policies.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: $u$ 是机器给出的预测行为，$x$ 是环境状态 $s$ 的特征向量，$t$ 是时间，$\alpha$ 是一组策略的参数集合。
- en: Imitation learning is still a branch of deep learning and based on deep neural
    networks (DNNs), also it is equally threatened by adversarial examples, AI backdoor
    and other forms of attack. In the autonomous driving field in 2020, Boloor et
    al.[[179](#bib.bib179)] proposed an adversarial example generation algorithm based
    on Bayesian optimization that can attack end-to-end (E2E) autonomous driving trajectory
    prediction system, which was successfully experimented on Intel’s Carla simulation
    platform. This approach sprayed special adversarial patterns generated with the
    algorithm on the road, interfering with the autonomous driving system’s prediction
    of its own vehicle, to induce the autonomous driving system to make a wrong driving
    decision. In the same year, Yang et al.[[180](#bib.bib180)] proposed two adversarial
    attack algorithms for vehicle trajectory prediction, which improved the above
    method, reducing the number of optimization rounds required for adversarial example
    generation and improving efficiency.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 模仿学习仍然是深度学习的一个分支，基于深度神经网络（DNN），同样也受到对抗样本、AI 后门和其他形式攻击的威胁。在2020年自主驾驶领域，Boloor
    等人[[179](#bib.bib179)]提出了一种基于贝叶斯优化的对抗样本生成算法，该算法可以攻击端到端（E2E）自主驾驶轨迹预测系统，并在英特尔的 Carla
    仿真平台上成功实验。这种方法将使用算法生成的特殊对抗模式喷洒在道路上，干扰自主驾驶系统对自身车辆的预测，从而诱使自主驾驶系统做出错误的驾驶决策。同年，杨等人[[180](#bib.bib180)]提出了两种针对车辆轨迹预测的对抗攻击算法，改进了上述方法，减少了对抗样本生成所需的优化轮次，提高了效率。
- en: Algorithm 10\. Bayesian Optimization (BO) algorithm
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 10\. 贝叶斯优化（BO）算法
- en: The objective of the BO algorithm is to generate adversarial examples suitable
    for end-to-end autonomous driving system and find the best adversarial perturbation
    through optimization $\delta$, with an loss function of
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: BO 算法的目标是生成适用于端到端自主驾驶系统的对抗样本，并通过优化 $\delta$ 寻找最佳对抗扰动，损失函数为
- en: '|  | $\delta^{*}=\arg\max_{\delta}f(\delta)$ |  | (16) |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '|  | $\delta^{*}=\arg\max_{\delta}f(\delta)$ |  | (16) |'
- en: where $\delta^{*}\in\mathcal{R}^{d}$, assuming that the autonomous driving model
    $f$ prediction conforms to a Gaussian process, it can be written as $GP(f,\mu(\delta),k(\delta,\delta^{\prime}))$,
    and let the mathematical expectation be 0, then $\mu(\delta)=0$, with the variance
    is the Mattern covariance function $K$.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\delta^{*}\in\mathcal{R}^{d}$，假设自主驾驶模型$f$的预测符合高斯过程，可以写作$GP(f,\mu(\delta),k(\delta,\delta^{\prime}))$，并设数学期望为0，则$\mu(\delta)=0$，方差为Mattern协方差函数$K$。
- en: '|  | $k(\delta,{\delta}^{\prime})=(1+\frac{\sqrt{5}r}{l}+\frac{5r^{2}}{3l^{2}})exp(-\frac{\sqrt{5}r}{l})$
    |  | (17) |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|  | $k(\delta,{\delta}^{\prime})=(1+\frac{\sqrt{5}r}{l}+\frac{5r^{2}}{3l^{2}})exp(-\frac{\sqrt{5}r}{l})$
    |  | (17) |'
- en: where $r$ denotes the Euclidean distance and $l$ is a factor coefficient. In
    such a manner, we can consider $\mathcal{l}$ is the adversarial perturbation.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$r$表示欧几里得距离，$l$是因子系数。这样，我们可以认为$\mathcal{l}$是对抗扰动。
- en: '![Refer to caption](img/948a0ed4bc04f82839ef155fde28ff6e.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/948a0ed4bc04f82839ef155fde28ff6e.png)'
- en: 'Figure 7: BO Algorithm [[179](#bib.bib179)]'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：BO算法 [[179](#bib.bib179)]
- en: 4.3 Emerging Threat on Reinforcement Learning
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 强化学习中的新兴威胁
- en: Reinforcement learning (deep reinforcement learning) is widely used in fields
    such as autonomous decision-making, electronic combat and competition. Combined
    with a number of other techniques such as deep search trees, deep reinforcement
    learning has enabled AlphaGo to explore some of the blind spots of human cognition.
    Adopting reinforcement learning technique for autonomous driving decisions is
    one of the major technology routes in academia and industry. Reinforcement learning
    security has recently received extensive attention and research.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（深度强化学习）在自主决策、电子战和竞争等领域得到了广泛应用。结合深度搜索树等多种其他技术，深度强化学习使AlphaGo能够探索人类认知的一些盲点。采用强化学习技术进行自主驾驶决策是学术界和工业界的主要技术路线之一。近年来，强化学习安全引起了广泛关注和研究。
- en: Reinforcement learning can be described as a Markov Decision Process(MDP)[[54](#bib.bib54),
    [44](#bib.bib44)]. A finite state decision making process consists of the tuple$(S,A,T,R)$
    where $S$ is the set of finite states, $A$ is the possible behaviour of the system,
    and $T$ is the State Transition Probabilities consisting of a set of probabilities
    $P_{s,a}$. It indicates that when behaviour $a$ is taken, the probability of reaching
    state $s$ is achieved, and reward function $R$ can return the reward value $Y$
    which can be obtained by the reward policy $R(s_{k},a_{k},s_{k+1})$ where the
    reward value $Y$ denotes changing the state to state $s_{k+1}$ when taking behaviour
    $a_{k}$ at the state $s_{k}$. Reinforcement learning is the process of starting
    with a random policy, receiving a reward based on the execution of that policy,
    and then continuously optimizing the policy by maximizing the reward.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习可以被描述为马尔可夫决策过程（MDP）[[54](#bib.bib54), [44](#bib.bib44)]。有限状态决策过程由元组$(S,A,T,R)$组成，其中$S$是有限状态的集合，$A$是系统可能的行为，$T$是状态转移概率，由概率集合$P_{s,a}$组成。它表示当采取行为$a$时，达到状态$s$的概率，并且奖励函数$R$可以返回奖励值$Y$，该值可以通过奖励策略$R(s_{k},a_{k},s_{k+1})$获得，其中奖励值$Y$表示在状态$s_{k}$下采取行为$a_{k}$时，状态变为$s_{k+1}$。强化学习是从一个随机策略开始，根据该策略的执行获得奖励，然后通过最大化奖励不断优化策略的过程。
- en: '![Refer to caption](img/e9841500457b76defb0699492b70d1fc.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e9841500457b76defb0699492b70d1fc.png)'
- en: 'Figure 8: Adversarial Attacks in Reinforcement Learning [[181](#bib.bib181)]'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：强化学习中的对抗攻击 [[181](#bib.bib181)]
- en: Reinforcement learning is a process of self-optimization, where models evolve
    and improve, but the process is also threatened by AI security risks[[181](#bib.bib181),
    [182](#bib.bib182)]. In the field of reinforcement learning, it is difficult to
    distinguish between the concepts of adversarial examples and AI backdoor, which
    are collectively referred to as "adversarial attacks". Based on attack paths,
    Kiourti et al.[[183](#bib.bib183)] categorize the attacks against reinforcement
    learning into environmental adversarial attacks, reward adversarial attacks, and
    adversarial policy attacks.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一个自我优化的过程，其中模型不断进化和改进，但该过程也面临AI安全风险的威胁[[181](#bib.bib181), [182](#bib.bib182)]。在强化学习领域，对抗样本和AI后门的概念难以区分，这些都统称为“对抗攻击”。根据攻击路径，Kiourti等[[183](#bib.bib183)]将对强化学习的攻击分类为环境对抗攻击、奖励对抗攻击和对抗策略攻击。
- en: •
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Environmental Adversarial Attacks
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 环境对抗攻击
- en: Environment-based adversarial attacks are those that add perturbations to the
    environment perceived in reinforcement learning, thereby affecting the system’s
    perception of state $s$, which in turn incorrectly matches the attacker’s specified
    policy ${s}^{\prime}$ to finally manipulate the decisions of the reinforcement
    learning system in a given state. In 2017, Huang et al.[[184](#bib.bib184)] implemented
    an environmental adversarial attack on a reinforcement learning system by adding
    adversarial perturbations to the external environment image frames in reinforcement
    learning based on the white-box FGSM algorithm; in the same year, Lin et al.[[185](#bib.bib185)]
    proposed an optimized environmental adversarial attack method targeting at the
    best behavior in a specific state; Behzadan et al.[[186](#bib.bib186)] verified
    the transition of adversarial attacks among reinforcement learning models and
    thus proposed a transition-based black-box attack. In 2019, Xiao et al.[[187](#bib.bib187)]
    proposed a method for estimating model gradients based on frame consistency information,
    thus enabling the first adversarial black-box attack in reinforcement learning.
    In 2020, Kiourti et al.[[183](#bib.bib183)] proposed TrojDRL, which describes
    a reward-based adversarial attack as one that is performed by adding minute specific
    perturbations to the environmental state to enables $\hat{s}=s+\delta$ that eventually
    makes the behaviour given by the reinforcement learning model misunderstood, i.e.$\hat{A}(s,m,\delta)\neq
    A(s,m,\delta)$. In the area of autonomous driving, Behzadan et al.[[188](#bib.bib188)]
    in 2019 verified that in the environment of autonomous driving, using reward adversarial
    attacks, an attacker could cause a direct collision or malicious manipulation
    of the trajectory of the autonomous driving vehicles.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 基于环境的对抗攻击是指对强化学习中感知到的环境添加扰动，从而影响系统对状态 $s$ 的感知，进而错误地将攻击者指定的策略 ${s}^{\prime}$
    匹配到最终操控强化学习系统在给定状态下的决策。2017 年，Huang 等人[[184](#bib.bib184)] 通过将对抗扰动添加到基于白盒 FGSM
    算法的强化学习外部环境图像帧中，实现了对抗环境攻击；同年，Lin 等人[[185](#bib.bib185)] 提出了针对特定状态下最佳行为的优化环境对抗攻击方法；Behzadan
    等人[[186](#bib.bib186)] 验证了对抗攻击在强化学习模型中的转移，进而提出了一种基于转移的黑盒攻击。2019 年，Xiao 等人[[187](#bib.bib187)]
    提出了基于帧一致性信息的模型梯度估计方法，从而实现了强化学习中的首个对抗黑盒攻击。2020 年，Kiourti 等人[[183](#bib.bib183)]
    提出了 TrojDRL，描述了一种奖励对抗攻击，即通过对环境状态添加细微特定的扰动来实现 $\hat{s}=s+\delta$，最终使得强化学习模型给出的行为被误解，即
    $\hat{A}(s,m,\delta)\neq A(s,m,\delta)$。在自动驾驶领域，Behzadan 等人[[188](#bib.bib188)]
    在 2019 年验证了在自动驾驶环境中，使用奖励对抗攻击，攻击者可以直接导致碰撞或恶意操控自动驾驶车辆的轨迹。
- en: •
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Reward Adversarial Attacks
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 奖励对抗攻击
- en: If an attacker is able to maliciously tamper with some of the rewards, it may
    lead to the policy of the reinforcement learning system being manipulated by the
    attacker, thus posing a severe threat to the system. Kiourti et al.[[183](#bib.bib183)]
    validated a reward adversarial attack where an attacker would set the corresponding
    reward to 1 when its target state $s$ is reached, and otherwise set the reward
    to -1, to create a strong attack scenario. In 2022, Islam et al.[[189](#bib.bib189)]
    proposed a reward adversarial attack applicable to the UAV environment.
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果攻击者能够恶意篡改一些奖励，这可能导致强化学习系统的策略被攻击者操控，从而对系统构成严重威胁。Kiourti 等人[[183](#bib.bib183)]
    验证了一种奖励对抗攻击，其中攻击者会在其目标状态 $s$ 被达到时将相应的奖励设置为 1，否则将奖励设置为 -1，以创建一个强大的攻击场景。在 2022 年，Islam
    等人[[189](#bib.bib189)] 提出了适用于无人机环境的奖励对抗攻击。
- en: •
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial Policy Attacks
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗策略攻击
- en: Unlike the reward adversarial attack approach, an adversarial policy attack
    does not need to tamper with the victim’s reward or policy; instead, in an adversarial
    environment, the attacker quickly finds a policy to defeat the victim by analyzing
    the victim’s policy or behavior, finding its vulnerabilities and exploiting them.
    Tretschk et al.[[190](#bib.bib190)] proposed the adversarial policy of Adversarial
    Transformer Networks (ATN). In 2020, Gleave et al.[[191](#bib.bib191)] proposed
    Adversarial Policies, in which an attacker generates targeted adversarial policies
    based on the behavior of the victim, producing seemingly random and uncoordinated
    behavior to defeat or disrupt the victim. Such policies are more successful in
    high-dimensional environments and have been validated in real eSports environments.
    In 2021, Wang et al.[[192](#bib.bib192)] synthesized reward adversarial attacks
    with adversarial policy attacks and proposed BackDooRK, which significantly improved
    the success rate of attackers in defeating their victims.
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与奖励对抗攻击方法不同，对抗策略攻击不需要篡改受害者的奖励或策略；相反，在对抗环境中，攻击者通过分析受害者的策略或行为，快速找到击败受害者的策略，发现其漏洞并加以利用。Tretschk等人[[190](#bib.bib190)]
    提出了对抗变换器网络（ATN）的对抗策略。2020年，Gleave等人[[191](#bib.bib191)] 提出了对抗策略，其中攻击者基于受害者的行为生成有针对性的对抗策略，产生看似随机且不协调的行为以击败或干扰受害者。这种策略在高维环境中更为成功，并已在真实电子竞技环境中得到验证。2021年，Wang等人[[192](#bib.bib192)]
    将奖励对抗攻击与对抗策略攻击综合，提出了BackDooRK，显著提高了攻击者击败受害者的成功率。
- en: Algorithm 11\. Adversarial Policies
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 11\. 对抗策略
- en: For multiple (in the case of two) participants in a reinforcement learning environment,
    it is assumed that the victim’s policy $\pi_{v}$ has been determined, and here
    the victim’s policy determines its behaviour $a_{v}\sim\pi_{v}(\cdot\mid s)$.
    And the attacker continuously optimizes its own policy $\pi_{\alpha}$ based on
    the victim’s policy and behaviour. It then be described as in this Markov decision
    process $M_{a}=(S,A_{\alpha},T_{\alpha},{R}^{\prime}_{\alpha})$, considered within
    state transition probabilities $T_{\alpha}$ and rewards $R_{\alpha}$, the integration
    of the victim policy $\pi_{v}$ yields
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于强化学习环境中的多个（以两个为例）参与者，假设受害者的策略$\pi_{v}$已经确定，并且这里受害者的策略决定其行为$a_{v}\sim\pi_{v}(\cdot\mid
    s)$。攻击者则不断根据受害者的策略和行为优化自己的策略$\pi_{\alpha}$。在这种马尔可夫决策过程中$M_{a}=(S,A_{\alpha},T_{\alpha},{R}^{\prime}_{\alpha})$中，考虑状态转移概率$T_{\alpha}$和奖励$R_{\alpha}$，受害者策略$\pi_{v}$的整合为
- en: '|  | $T_{\alpha}(s,a_{\alpha})=T(s,a_{\alpha,a_{v}})$ |  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|  | $T_{\alpha}(s,a_{\alpha})=T(s,a_{\alpha,a_{v}})$ |  |'
- en: and
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 和
- en: '|  | ${R}^{\prime}_{\alpha}(s,a_{\alpha,{s}^{\prime}})=R_{\alpha}(s,a_{\alpha,a_{v},{s}^{\prime}})$
    |  |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
  zh: '|  | ${R}^{\prime}_{\alpha}(s,a_{\alpha,{s}^{\prime}})=R_{\alpha}(s,a_{\alpha,a_{v},{s}^{\prime}})$
    |  |'
- en: 'The attacker finds an adversarial policy against the victim by optimizing the
    following loss function:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 攻击者通过优化以下损失函数来找到针对受害者的对抗策略：
- en: '|  | $\arg\max\sum_{t=0}^{\infty}\gamma^{t}R_{\alpha}(s_{t},a_{\alpha}^{t},s^{t+1})$
    |  | (18) |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
  zh: '|  | $\arg\max\sum_{t=0}^{\infty}\gamma^{t}R_{\alpha}(s_{t},a_{\alpha}^{t},s^{t+1})$
    |  | (18) |'
- en: where it is subject to $s^{t+1}\sim T_{\alpha}(s^{t},a_{\alpha}^{t})$ and $a_{\alpha}\sim\pi(\cdot\mid
    s^{t})$
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 它受制于$s^{t+1}\sim T_{\alpha}(s^{t},a_{\alpha}^{t})$ 和 $a_{\alpha}\sim\pi(\cdot\mid
    s^{t})$
- en: 4.4 Summary
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 总结
- en: This chapter introduces some technologies that may pose a security risk to the
    autonomous driving AI decision-making layer and briefly describes the technical
    principles. The main function of the sensor layer is to recognize the raw information
    collected by sensors, while the main function of the decision layer is to make
    driving decisions based on the perceived state of the environment. Each layer
    has its own autonomous driving function, and a threat to any of these layers could
    affect the overall safety of the autonomous driving system.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了一些可能对自动驾驶AI决策层构成安全风险的技术，并简要描述了技术原理。传感器层的主要功能是识别传感器收集的原始信息，而决策层的主要功能是根据感知到的环境状态做出驾驶决策。每一层都有其自身的自动驾驶功能，任何一层的威胁都可能影响自动驾驶系统的整体安全。
- en: 5 Emerging Threats in Federated Learning-based Vehicular Internet of Things
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 基于联邦学习的车载物联网中的新兴威胁
- en: With the rapid development of smart vehicles, the vehicle is no longer an isolated
    single point, but increasingly a terminal worker in the pan-vehicle network. In
    many countries and regions around the world, the vehicular internet of things
    is already under rapid construction and its security based on traditional cyber
    security technologies has been widely studied[[15](#bib.bib15)]. However, with
    the development of emerging technologies such as arithmetic networks and privacy
    computing, new generation technologies such as deep learning technologies, edge
    computing and federated learning will be further integrated into the environment
    of the vehicular internet of things, giving rise to new business forms. Still,
    new technologies and new business forms also bring new security risks, and this
    chapter focuses on the new risks that they may bring.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 随着智能车辆的快速发展，车辆不再是孤立的单点，而越来越成为泛车联网中的终端参与者。在世界许多国家和地区，车载物联网已经在快速建设中，其基于传统网络安全技术的安全性也得到了广泛研究[[15](#bib.bib15)]。然而，随着算力网络和隐私计算等新兴技术的发展，深度学习技术、边缘计算和联邦学习等新一代技术将进一步融入车载物联网环境，催生新的商业形式。然而，新技术和新商业形式也带来了新的安全风险，本章重点讨论了这些新风险。
- en: 'It is well known that current artificial intelligence technology is a data-driven
    approach[[193](#bib.bib193), [194](#bib.bib194), [195](#bib.bib195)]. In AI applications,
    a large amount of information needs to be collected in advance as the training
    data. Traditional methods of data collection and information interaction face
    a number of limitations: First, traditional data collection and transmission are
    often inefficient. Secondly, traditional data collection often leads to invasion
    of user privacy. Therefore, Federated Learning (FL)[[196](#bib.bib196), [197](#bib.bib197),
    [198](#bib.bib198)] is a new distributed learning framework that does not require
    data collection by a central worker, providing a relatively more efficient and
    private way of interaction. In federated learning, each worker is trained with
    local data sets to obtain local gradients or weights through machine learning
    algorithms, and then uploads local gradients instead of local sensitive data,
    enabling knowledge interaction instead of data interaction. Federated learning
    has been widely used in the Internet, mobile terminals and other fields. At the
    same time, federated learning has also been introduced into the field of the vehicular
    internet of things [[199](#bib.bib199), [67](#bib.bib67), [200](#bib.bib200),
    [66](#bib.bib66)] and has become a future trend.'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，目前的人工智能技术是一种数据驱动的方法[[193](#bib.bib193), [194](#bib.bib194), [195](#bib.bib195)]。在人工智能应用中，需要事先收集大量信息作为训练数据。传统的数据收集和信息交互方法面临许多限制：首先，传统的数据收集和传输通常效率低下。其次，传统的数据收集常常导致用户隐私的侵害。因此，联邦学习（FL）[[196](#bib.bib196),
    [197](#bib.bib197), [198](#bib.bib198)]是一种新的分布式学习框架，不需要中心参与者进行数据收集，提供了一种相对更高效和私密的交互方式。在联邦学习中，每个参与者使用本地数据集进行训练，通过机器学习算法获得本地梯度或权重，然后上传本地梯度而不是本地敏感数据，从而实现知识交互而不是数据交互。联邦学习已广泛应用于互联网、移动终端等领域。同时，联邦学习也已引入车载物联网领域[[199](#bib.bib199),
    [67](#bib.bib67), [200](#bib.bib200), [66](#bib.bib66)]，并成为未来的趋势。
- en: The federated learning environment offers more new attack methods[[201](#bib.bib201),
    [202](#bib.bib202), [203](#bib.bib203)]. With more workers participating in federated
    learning and the trustworthiness of each worker with the cloud difficult to guarantee,
    malicious workers in the vehicular IoT may attack the federated learning system
    in a variety of ways, while the privacy of the workers may also be at risk.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习环境带来了更多新的攻击方法[[201](#bib.bib201), [202](#bib.bib202), [203](#bib.bib203)]。随着更多参与者加入联邦学习，并且每个参与者对云端的可信度难以保证，车载物联网中的恶意参与者可能会以各种方式攻击联邦学习系统，同时参与者的隐私也可能面临风险。
- en: 5.1 Byzantine Attack on Federated Learning
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 联邦学习中的拜占庭攻击
- en: The Byzantine Attack in Federated Learning refers to workers attacked by a malicious
    Byzantine worker to construct harmful gradients which after aggregation will make
    the global model difficult to aggregate, thus making the system unusable or generating
    a global model with a malicious backdoor. Distinguished from traditional Data
    Posioning attacks, the above attacks are also known as Model Posioning attacks.
    In 2017, Blanchard et al.[[204](#bib.bib204)] first proposed the Byzantine attack
    in a machine learning environment. The principle is that in round $t$, a non-Byzantine
    worker $p$ in federated learning will locally compute the unbiased estimate $V_{p}^{t}$
    of its gradient $\bigtriangledown Q(x_{t})$ and send it to the aggregation worker
    which according to some aggregation rule $F$, aggregates the received gradient
    estimates, then in round $t+1$, the weight of the global model is
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习中的拜占庭攻击指的是工作者被恶意的拜占庭工作者攻击，以构造有害的梯度，这些梯度在聚合后会使全局模型难以聚合，从而使系统无法使用或生成一个带有恶意后门的全局模型。不同于传统的数据污染攻击，上述攻击也被称为模型污染攻击。在
    2017 年，Blanchard 等人[[204](#bib.bib204)] 首次提出了在机器学习环境中的拜占庭攻击。其原理是在第 $t$ 轮中，联邦学习中的一个非拜占庭工作者
    $p$ 将本地计算的其梯度 $\bigtriangledown Q(x_{t})$ 的无偏估计 $V_{p}^{t}$ 发送到聚合工作者，后者根据某些聚合规则
    $F$ 聚合接收到的梯度估计，然后在第 $t+1$ 轮中，全局模型的权重为
- en: '|  | $x_{t+1}=x_{t}-\gamma_{t}\cdot F(V_{1}^{t},...,V_{n}^{t})$ |  |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t+1}=x_{t}-\gamma_{t}\cdot F(V_{1}^{t},...,V_{n}^{t})$ |  |'
- en: 'where $\gamma_{t}$ is the learning rate. While the malicious Byzantine worker
    cleverly constructs destructive local gradient estimates:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma_{t}$ 是学习率。与此同时，恶意拜占庭工作者巧妙地构造破坏性的本地梯度估计：
- en: '|  | $V_{n}=\frac{1}{\lambda_{n}}\cdot U-\sum^{n-1}_{i=1}\frac{\lambda_{i}}{\lambda_{n}}V_{i}$
    |  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|  | $V_{n}=\frac{1}{\lambda_{n}}\cdot U-\sum^{n-1}_{i=1}\frac{\lambda_{i}}{\lambda_{n}}V_{i}$
    |  |'
- en: where $\gamma_{t}$ is the weight the gradient estimate of worker $n$ at the
    time of aggregation. This will cause the global gradient to become any harmful
    gradient $U$ provided by the Byzantine worker after the final aggregation.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\gamma_{t}$ 是在聚合时工作者 $n$ 的梯度估计权重。这会导致全局梯度在最终聚合后成为任何由拜占庭工作者提供的有害梯度 $U$。
- en: '![Refer to caption](img/d57d46cd366832914257c6f68cef28bb.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/d57d46cd366832914257c6f68cef28bb.png)'
- en: 'Figure 9: Schematic of Byzantine attack [[204](#bib.bib204)]. The black dashed
    line represents the gradient estimate for non-Byzantine workers, the blue solid
    line represents the global gradient after normal aggregation, and the red dashed
    line represents the gradient estimate submitted by the Byzantine worker.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：拜占庭攻击示意图 [[204](#bib.bib204)]。黑色虚线表示非拜占庭工作者的梯度估计，蓝色实线表示正常聚合后的全局梯度，红色虚线表示拜占庭工作者提交的梯度估计。
- en: The above describes a Byzantine worker that poisons in only some round of aggregation
    $t$, which is called a "single-shot attack". Generally, its limitations are as
    follows.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述了一种拜占庭工作者仅在某一轮聚合 $t$ 中进行污染的情况，这称为“单次攻击”。一般来说，其限制如下。
- en: •
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Attack’s Capability Is Limited. Attack just is a single worker or a single aggregation
    that has limited influence on its attacking power.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 攻击能力有限。攻击仅是单一工作者或单次聚合，对其攻击能力的影响有限。
- en: •
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Poor Concealment. A single worker attack or a single-shot attack on an aggregator
    usually makes the state of the poisoned worker significantly different from a
    normal worker, and that leads to easily detected.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 隐蔽性差。单一工作者攻击或对聚合器的单次攻击通常会使受污染工作者的状态与正常工作者显著不同，这容易被检测出来。
- en: •
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Prone to Recession. After multiple aggregations, the effect of poisoning a single
    worker during an aggregation round tends to fade, even does not continue to be
    effective.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 易于衰退。经过多次聚合后，单次聚合轮次中污染单一工作者的效果往往会减弱，甚至不再有效。
- en: Blanchard et al.[[204](#bib.bib204)] also proposed the concept of Byzantine
    tolerance for measuring the robustness of federated learning models against Byzantine
    attacks. To improve Byzantine robustness, some scholars have proposed novel aggregation
    algorithms that can be used for federated learning[[205](#bib.bib205), [206](#bib.bib206)].
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: Blanchard 等人[[204](#bib.bib204)] 还提出了拜占庭容忍度的概念，用于衡量联邦学习模型对拜占庭攻击的鲁棒性。为了提高拜占庭鲁棒性，一些学者提出了可用于联邦学习的新型聚合算法[[205](#bib.bib205),
    [206](#bib.bib206)]。
- en: To overcome these limitations, an attacker can adopt to Repeated Attacks or
    Collusion Attacks. Repeated Attacks mean that the attacker can perform poisoning
    in multiple rounds of aggregation; Collusion Attacks are joint poisoning of multiple
    Byzantine workers. Experiments show that repeated attacks and collusion attacks
    can enhance the capability of Byzantine attacks and can significantly improve
    the concealment and recession resistance of the attacks[[207](#bib.bib207)]. Xie
    et al.[[208](#bib.bib208)] proposed a distributed backdoor attack that can be
    performed in a federated learning environment.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些限制，攻击者可以采用重复攻击或串通攻击。重复攻击指攻击者可以在多轮聚合中进行投毒；串通攻击是指多个拜占庭工作者联合进行投毒。实验表明，重复攻击和串通攻击可以增强拜占庭攻击的能力，并且显著提高攻击的隐蔽性和抗回退能力[[207](#bib.bib207)]。谢等人[[208](#bib.bib208)]
    提出了在联邦学习环境中可以执行的分布式后门攻击。
- en: Algorithm 12\. Distributed Backdoor Attack (DBA)[[208](#bib.bib208)]
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 12\. 分布式后门攻击（DBA）[[208](#bib.bib208)]
- en: Distributed backdoor attacks provide a efficient way for multiple malicious
    workers in federated learning to conspire to an attack. The DBA algorithm takes
    advantage of the local data opacity in federated learning, with multiple malicious
    workers each adding more minute malicious perturbations in multiple rounds to
    improve concealment. The malicious permissions of each worker are optimally generated
    in the following manner.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式后门攻击为多个恶意工作者在联邦学习中联合发起攻击提供了一种高效的方法。DBA 算法利用了联邦学习中的本地数据不透明性，多个恶意工作者每轮增加更微小的恶意扰动以提高隐蔽性。每个工作者的恶意权限以以下方式进行最优生成。
- en: '|  | $\begin{split}w^{*}_{i}=&amp;\arg\max_{w_{i}}(\sum_{j\in s_{poi}^{i}}P[G^{t+1}(R(X_{i}^{j},\phi^{*}_{i}))=\tau;\gamma;I]\\
    &amp;+\sum_{j\in S^{i}_{cln}}P[G^{t+1}(x_{j}^{i})=y_{j}^{i}]),\forall i\in[M]\end{split}$
    |  | (19) |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '|  | $\begin{split}w^{*}_{i}=&amp;\arg\max_{w_{i}}(\sum_{j\in s_{poi}^{i}}P[G^{t+1}(R(X_{i}^{j},\phi^{*}_{i}))=\tau;\gamma;I]\\
    &amp;+\sum_{j\in S^{i}_{cln}}P[G^{t+1}(x_{j}^{i})=y_{j}^{i}]),\forall i\in[M]\end{split}$
    |  | (19) |'
- en: where $\phi^{*}_{i}=\left\{\phi,O(i)\right\}$ represents the local poisoning
    policy of attacker $m_{i}$, and $\forall$ is the global trigger.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi^{*}_{i}=\left\{\phi,O(i)\right\}$ 表示攻击者 $m_{i}$ 的本地投毒策略，$\forall$ 是全局触发器。
- en: '![Refer to caption](img/c92652123f5763adad46742a6b65d6d9.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c92652123f5763adad46742a6b65d6d9.png)'
- en: 'Figure 10: Schematic of Byzantine Attack [[201](#bib.bib201)]. The black dashed
    line denotes the gradient estimate for non-Byzantine workers, the blue solid line
    denotes the global gradient after normal aggregation, and the red dashed line
    denotes the gradient estimate submitted by the Byzantine worker. [[201](#bib.bib201)]'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '图 10: 拜占庭攻击示意图 [[201](#bib.bib201)]。黑色虚线表示非拜占庭工作者的梯度估计，蓝色实线表示正常聚合后的全局梯度，红色虚线表示由拜占庭工作者提交的梯度估计。
    [[201](#bib.bib201)]'
- en: 5.2 Privacy Inference on Federated Learning
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 联邦学习中的隐私推断
- en: The interior and exterior images of an autonomous driving vehicle may include
    sensitive information such as faces and license plate numbers. So local user data
    may reflect the user’s location and trajectory, in-vehicle behavior and driving
    habits, which are also often considered sensitive, and therefore direct user data
    capture may lead to user privacy violations. One of the aims of Federated Learning
    is to avoid the direct leakage of sensitive user data, thereby achieving user
    privacy protection. The effects of user privacy in federated learning have received
    extensive attention and researches[[209](#bib.bib209), [113](#bib.bib113), [210](#bib.bib210),
    [211](#bib.bib211), [201](#bib.bib201), [212](#bib.bib212), [213](#bib.bib213),
    [214](#bib.bib214)] . However, it shows that local gradients are highly correlated
    with the user’s data and that data may still be inferred when local gradients
    are obtained. Techniques such as Model Inversion and Membership Privacy Inference
    may help infer local user data.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶车辆的内部和外部图像可能包含诸如面孔和车牌号码等敏感信息。因此，本地用户数据可能反映用户的位置和轨迹、车内行为和驾驶习惯，这些也常常被视为敏感信息，因此直接捕获用户数据可能导致用户隐私泄露。联邦学习的一个目标是避免敏感用户数据的直接泄露，从而实现用户隐私保护。联邦学习中用户隐私的影响已受到广泛关注和研究[[209](#bib.bib209),
    [113](#bib.bib113), [210](#bib.bib210), [211](#bib.bib211), [201](#bib.bib201),
    [212](#bib.bib212), [213](#bib.bib213), [214](#bib.bib214)]。然而，它显示出本地梯度与用户数据高度相关，当获取本地梯度时，数据仍然可能被推断。模型反演和成员隐私推断等技术可能有助于推断本地用户数据。
- en: •
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Model Inversion The model inversion method replaces the pixels in the initial
    random images one by one, then classifies the constructed images with the help
    of some model, and iteratively optimizes the constructed image based on the results
    of the classification, resulting in a constructed image that is highly similar
    to the target image. To speed up aggregation, model inversion algorithms mostly
    utilize the distribution of the target image as prior knowledge to participate
    in the optimization process. In 2015, Fredrikson et al.[[5](#bib.bib5)] first
    proposed a model inversion attack; in 2016, Wu et al.[[215](#bib.bib215)] proposed
    a black-box model inversion attack algorithm; in 2017, Hitaj et al.[[216](#bib.bib216)]
    proposed a model inversion algorithm based on adversarial generative network techniques,
    which achieved better reconstruction results, also known as "Adversarial Generative
    Network Reconstruction Attacks" (GAN Reconstruction Attacks). Mai et al.[[217](#bib.bib217)]
    extended model inversion to the field of face recognition to recover from face
    recognition features for face image data, proposing the Face Recovery Attack,
    and Razzhigaev et al.[[218](#bib.bib218), [219](#bib.bib219)] continuously optimized
    the black box face recovery attack.
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 模型反演 方法通过逐一替换初始随机图像中的像素，然后利用某些模型对构建的图像进行分类，并根据分类结果迭代优化构建的图像，从而得到与目标图像高度相似的构建图像。为了加快聚合速度，模型反演算法大多利用目标图像的分布作为先验知识参与优化过程。2015年，Fredrikson等人[[5](#bib.bib5)]首次提出了模型反演攻击；2016年，Wu等人[[215](#bib.bib215)]提出了黑箱模型反演攻击算法；2017年，Hitaj等人[[216](#bib.bib216)]提出了基于对抗生成网络技术的模型反演算法，该算法取得了更好的重建结果，也被称为“对抗生成网络重建攻击”（GAN重建攻击）。Mai等人[[217](#bib.bib217)]将模型反演扩展到人脸识别领域，以从人脸识别特征中恢复人脸图像数据，提出了人脸恢复攻击；Razzhigaev等人[[218](#bib.bib218),
    [219](#bib.bib219)]不断优化黑箱人脸恢复攻击。
- en: •
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Membership Privacy Inference Member privacy inference means that an attacker
    exploits the special feedback of the overfitting phenomenon and tries to infer
    whether the target data is in the training set or not. Typically, attackers first
    construct a Shadow Model similar to the target model; then uses the shadow model
    to generate training data; furthermore, used the data to train an Attack Model;
    last, the attackers use the Attack Model to construct the complete attack process.
    In 2017, Shokri et al.[[6](#bib.bib6)] at Cornell University first proposed the
    member privacy inference attack. Once proposed, the member privacy inference method
    has been continuously researched and further optimized and improved[[214](#bib.bib214),
    [213](#bib.bib213), [220](#bib.bib220)]. In 2018, Yeom et al.[[221](#bib.bib221)]
    analyzed in depth the relationship between overfitting and the risk of member
    privacy leakage. In 2019, Salem et al.[[222](#bib.bib222)] incorporated a data
    transition attack method that reduces the attacker’s reliance on background knowledge
    and makes the "shadow model" less necessary. Sablayrolles et al.[[223](#bib.bib223)]
    improved the membership privacy inference attack using a Bayesian optimization
    policy. Zhang et al.[[224](#bib.bib224)] extended the member privacy inference
    attack to the recommender system domain. Hui et al.[[225](#bib.bib225)] proposed
    the BlINDMI algorithm, which first generates a certain amount of non-membership
    data, and then iteratively generates a comparison between non-membership data
    and membership data to improve the accuracy of membership privacy inference.
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 成员隐私推断 成员隐私推断是指攻击者利用过拟合现象的特殊反馈，尝试推测目标数据是否在训练集中。通常，攻击者首先构建一个与目标模型类似的影子模型；然后使用影子模型生成训练数据；进一步，用这些数据训练攻击模型；最后，攻击者使用攻击模型构建完整的攻击过程。2017年，Shokri等人[[6](#bib.bib6)]在康奈尔大学首次提出了成员隐私推断攻击。提出后，成员隐私推断方法持续受到研究并进一步优化和改进[[214](#bib.bib214),
    [213](#bib.bib213), [220](#bib.bib220)]。2018年，Yeom等人[[221](#bib.bib221)]深入分析了过拟合与成员隐私泄露风险之间的关系。2019年，Salem等人[[222](#bib.bib222)]引入了一种数据迁移攻击方法，该方法减少了攻击者对背景知识的依赖，使“影子模型”变得不那么必要。Sablayrolles等人[[223](#bib.bib223)]利用贝叶斯优化策略改进了成员隐私推断攻击。Zhang等人[[224](#bib.bib224)]将成员隐私推断攻击扩展到推荐系统领域。Hui等人[[225](#bib.bib225)]提出了BlINDMI算法，该算法首先生成一定数量的非成员数据，然后迭代生成非成员数据与成员数据的比较，以提高成员隐私推断的准确性。
- en: In a federated learning, the data distribution of workers is broadly similar.
    That providing more contextual knowledge and creating better conditions for malicious
    workers and the cloud to conduct member privacy inference attacks. In 2019, Nasr
    et al.[[226](#bib.bib226)] validated the membership privacy inference attack risk
    in federated learning. In 2020, Chen et al.[[227](#bib.bib227)] further improved
    the success rate of member privacy inference attacks in federated learning using
    adversarial generative networks for data augmentation. In 2021, Hu et al.[[228](#bib.bib228)]
    proposed Source Inference Attack for federated learning, which uses Bayesian methods
    to infer the training data of federated learning workers and in the same year,
    Gupta et al.[[229](#bib.bib229)] extended the traditional membership privacy inference
    attack from classification tasks to regression tasks, and also verified that their
    method is equally applicable to regression tasks in a federated learning environment.
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在联邦学习中，工作者的数据分布大致相似。这提供了更多的背景知识，也为恶意工作者和云端进行成员隐私推断攻击创造了更好的条件。2019年，Nasr等人[[226](#bib.bib226)]验证了联邦学习中的成员隐私推断攻击风险。2020年，Chen等人[[227](#bib.bib227)]进一步提高了使用对抗生成网络进行数据增强的联邦学习中的成员隐私推断攻击成功率。2021年，Hu等人[[228](#bib.bib228)]提出了用于联邦学习的源推断攻击，利用贝叶斯方法推断联邦学习工作者的训练数据，同年，Gupta等人[[229](#bib.bib229)]将传统的成员隐私推断攻击从分类任务扩展到回归任务，并验证了他们的方法同样适用于联邦学习环境中的回归任务。
- en: Algorithm 13\. General Model Inversion[[5](#bib.bib5)]
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 13\. 一般模型反演[[5](#bib.bib5)]
- en: Model inversion used be recover the training dataset that probability includes
    sensitive data. The general model inversion algorithm first computes each possible
    target feature vector $v$ for feature $x_{1}$ and then evaluates its probability
    of being correct. Also, since the class distribution of deep learning models tends
    to obey a Gaussian Distribution, so adapt a Gaussian function as a penalty function
    can accelerate the convergence. The algorithm can be described as follows.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 模型反演用于恢复可能包含敏感数据的训练数据集。一般模型反演算法首先计算每个可能的目标特征向量$v$对于特征$x_{1}$的概率，然后评估其正确性。由于深度学习模型的类别分布往往遵循高斯分布，因此采用高斯函数作为惩罚函数可以加速收敛。算法描述如下。
- en: 'Input: Target model $f$Output: An example of training data $x^{\prime}$12initialized
    $x_{0}$ ;3foreach *feature $x_{i}\in$ the feature vector $X$* do4      5      foreach *the
    possible value $v\in x_{i}$*  do6            7            $x^{\prime}={v,x_{2},x_{3},...,x_{n}}$8            $r_{v}\leftarrow
    err(y,f(x^{\prime}))\cdot\Pi_{i}p_{i}(x_{i})$9       end foreach10      11 end
    foreach'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：目标模型$f$ 输出：训练数据示例$x^{\prime}$ 12 初始化$x_{0}$；3 对于*特征$x_{i}\in$特征向量$X$* 进行操作4
    5 对于*可能的值$v\in x_{i}$* 进行操作6 7 $x^{\prime}={v,x_{2},x_{3},...,x_{n}}$ 8 $r_{v}\leftarrow
    err(y,f(x^{\prime}))\cdot\Pi_{i}p_{i}(x_{i})$ 9 结束 对于 10 11 结束 对于
- en: Algorithm 4 13\. Model Inversion
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 4 13\. 模型反演
- en: Algorithm 14\. Membership Privacy Inference[[6](#bib.bib6)]
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 14\. 成员隐私推断[[6](#bib.bib6)]
- en: 'Implementing a member privacy inference attack requires several processes,
    starting with the generation of data for training the shadow model. If the attacker
    has some background knowledge and possesses some homogeneous distribution data,
    it can be used directly for shadow model training. If the attacker does not have
    the appropriate background knowledge, data with a high confidence level can be
    selected as integrated training data by querying the target model. Each classification
    category $c$ will be initially recorded as $x$ randomly and iterated as follows:
    sequentially select the record data classified by the target model as $c$ with
    the maximum confidence level $y_{c}$ which is ensured to be greater than a certain
    threshold on $f$ , into the integrated data set. Once a record $x$ is selected
    into the integrated data set, randomly change the features as many as $k$ based
    on $x$ to generate a new record $X^{*}$. This is iterated until a certain amount
    of training data is generated. The second process is to generate a shadow model.
    Based on the generated training data set$D^{train}_{shadow_{i}}$, after training,
    shadow models $shadow_{i}$ will be generated. The third step is to train the attack
    model. Query the prediction vector of the records $(x,y)\in D_{shadow}^{train}$
    in the training data set of shadow models $shadow_{i}$, then record $(y,\hat{y},in)$
    can be generated. And calculate the prediction vector$\hat{y}=f_{shadow}^{i}(x)$
    in the shadow model test data set $\forall(x,y)\in D_{shadow_{i}}^{test}$ then
    vector $\hat{y}=f_{shadow}^{i}(x)$ can be obtained. Next the two corresponding
    sets of vectors of each category $c$ are aggregated into the training data $D_{attack}^{train}$
    of the attack model. On this basis, a classifier is trained to determine whether
    the data is included in the training data.'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 实施成员隐私推断攻击需要几个过程，首先是生成用于训练影子模型的数据。如果攻击者拥有一些背景知识并拥有一些同质分布数据，则可以直接用于影子模型训练。如果攻击者没有适当的背景知识，则可以通过查询目标模型选择高置信度的数据作为综合训练数据。每个分类类别$c$将最初记录为$x$，并按以下步骤迭代：依次选择由目标模型分类为$c$的记录数据，并确保其最大置信度水平$y_{c}$大于$f$上的某个阈值，进入综合数据集。一旦记录$x$被选择进入综合数据集，则基于$x$随机改变最多$k$个特征以生成新的记录$X^{*}$。这个过程会重复，直到生成一定数量的训练数据。第二个过程是生成影子模型。基于生成的训练数据集$D^{train}_{shadow_{i}}$，经过训练后将生成影子模型$shadow_{i}$。第三步是训练攻击模型。查询影子模型$shadow_{i}$的训练数据集中记录$(x,y)\in
    D_{shadow}^{train}$的预测向量，然后生成记录$(y,\hat{y},in)$。并计算影子模型测试数据集$\forall(x,y)\in D_{shadow_{i}}^{test}$中的预测向量$\hat{y}=f_{shadow}^{i}(x)$，然后得到向量$\hat{y}=f_{shadow}^{i}(x)$。接下来，将每个类别$c$的两个对应的向量集聚合到攻击模型的训练数据$D_{attack}^{train}$中。在此基础上，训练一个分类器来判断数据是否包含在训练数据中。
- en: 5.3 Summary
  id: totrans-369
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 总结
- en: With the fusion and development of technologies, such as federated learning
    , edge computing and etc., the intelligent vehicular internet of things is gradually
    growing and becoming a future trend[[230](#bib.bib230)]. However, the security
    and user privacy risks associated with federated learning and other technologies
    are also a concern. In the vehicular IoTs, the data distribution of each end is
    similar, providing attackers with certain background knowledge. Once a end is
    controlled by an attacker, the whole IoT networks may be subject to Byzantine
    attacks, and the risk of user privacy analysis is greatly increased. While enjoying
    the convenience offered by vehicular IoT and AI, we should not ignore the associated
    risks, but rather conduct relevant research and security protection.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 随着联邦学习、边缘计算等技术的融合和发展，智能车联网逐渐成长并成为未来的趋势[[230](#bib.bib230)]。然而，与联邦学习和其他技术相关的安全性和用户隐私风险也令人担忧。在车联网中，每个终端的数据分布相似，这为攻击者提供了一定的背景知识。一旦某个终端被攻击者控制，整个物联网网络可能会遭受拜占庭攻击，用户隐私分析的风险大大增加。在享受车联网和人工智能带来的便利时，我们不应忽视相关风险，而应进行相关研究和安全保护。
- en: 6 Conclusion
  id: totrans-371
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: Autonomous driving is a complex system based on artificial intelligence technology.
    A number of artificial intelligence applications, such as objective detection,
    segmentation, speech recognition and driving decision-making, play an important
    role in autonomous driving. Safety is a key concern in autonomous driving systems.
    AI security is crucial and directly affects autonomous driving system security,
    and leads to personal safety, which is far beyond traditional network security
    and basic software security. The novel technologies, such as AI, bring emerging
    risks.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶是一个基于人工智能技术的复杂系统。许多人工智能应用，如目标检测、分割、语音识别和驾驶决策，在自动驾驶中发挥着重要作用。安全性是自动驾驶系统中的一个关键问题。AI
    安全至关重要，并直接影响到自动驾驶系统的安全性，进而关乎个人安全，这远远超出了传统网络安全和基础软件安全的范畴。新兴技术，如 AI，带来了新的风险。
- en: 'This paper briefly introduces the AI technology route and AI functional modules
    in the autonomous driving system, and analyses the origins, development, and current
    appropriate AI security technologies for autonomous driving. Similar to general
    AI, autonomous driving is under threats of adversarial examples attacks, including
    AI backdoor attacks, model inversion and member privacy. Despite there are some
    defense methods that can be useful against these threats, ensuring safety for
    complex autonomous driving systems requires not only single-point defense techniques
    against certain threats but also building a complete trusted AI system[[231](#bib.bib231),
    [232](#bib.bib232)], as following:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 本文简要介绍了自动驾驶系统中的 AI 技术路线和 AI 功能模块，并分析了自动驾驶的起源、发展及当前适用的 AI 安全技术。类似于一般 AI，自动驾驶也面临对抗样本攻击的威胁，包括
    AI 后门攻击、模型反演和成员隐私。尽管有一些防御方法可以应对这些威胁，但确保复杂的自动驾驶系统安全不仅需要对某些威胁的单点防御技术，还需要构建一个完整的可信
    AI 系统[[231](#bib.bib231), [232](#bib.bib232)]，如以下所示：
- en: •
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Trustworthy AI evaluation system. The safety of AI in autonomous driving system
    requires a complete evaluation and validation system, which covering data preparation,
    model training, model deployment, system application and other parts of the AI
    model life cycle. Especially, there are some noteworthy issues, including: AI
    adversarial robustness assessment, cross-domain data robustness assessment, model
    safety validation, training data safety validation, and data adversarial example
    detection.'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可信赖的 AI 评估系统。自动驾驶系统中 AI 的安全性需要一个完整的评估和验证系统，这包括数据准备、模型训练、模型部署、系统应用以及 AI 模型生命周期的其他部分。特别地，有一些值得注意的问题，包括：AI
    对抗鲁棒性评估、跨领域数据鲁棒性评估、模型安全验证、训练数据安全验证和数据对抗样本检测。
- en: •
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Trustworthy AI Architecture. Further more, we need to improve autonomous driving
    security from just reducing some specific threats to building Trustworthy AI Architecture.
    There is something beyond adversarial detecting need to do to build an AI architecture
    with human agency and oversight, robustness and safety, privacy and data governance,
    transparency, diversity, non-discrimination and fairness, societal and environmental
    well-being, and accountability.
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 可信赖的 AI 架构。进一步来说，我们需要从仅仅减少一些特定威胁提升到构建可信赖的 AI 架构。除了对抗检测，还需要构建一个具有人工代理和监督、鲁棒性和安全性、隐私和数据治理、透明度、多样性、不歧视和公平性、社会和环境福祉，以及问责制的
    AI 架构。
- en: To summarize, this paper appeals for attention and focus on emerging technologies
    such as artificial intelligence that bring new safety risks to autonomous driving
    systems. It is necessary that construct a safer and trusted autonomous driving
    system through the establishment of a trusted artificial intelligence technology
    system.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本文呼吁关注并集中精力于如人工智能等带来新安全风险的技术。必须通过建立一个可信赖的人工智能技术系统来构建一个更安全的自动驾驶系统。
- en: 7 Appendix A. Summary table
  id: totrans-379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 附录 A. 摘要表
- en: Here we present a table containing a summary of the adversarial examples algorithms
    as the foundation in this paper.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们呈现了一个包含本文中基础对抗样本算法摘要的表格。
- en: 'Table 3: Foundational Adversarial Examples Algorithms'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：基础对抗样本算法
- en: '| Method | method | Major algorithms |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 方法 | 主要算法 |'
- en: '| White-Box | Gradient sign-based | FGSM[[2](#bib.bib2)], BIM[Kurakin2018adversarial],
    MIM[[233](#bib.bib233)],PGD[[234](#bib.bib234)] |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 白盒 | 基于梯度符号 | FGSM[[2](#bib.bib2)], BIM[Kurakin2018adversarial], MIM[[233](#bib.bib233)],
    PGD[[234](#bib.bib234)] |'
- en: '|  | Optimization-based | CW[[126](#bib.bib126)], |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于优化 | CW[[126](#bib.bib126)], |'
- en: '|  | Others | … |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '|  | 其他 | … |'
- en: '| Black-Box | Transfer-Based | DIM[[235](#bib.bib235)], TI[[236](#bib.bib236)]
    |'
  id: totrans-386
  prefs: []
  type: TYPE_TB
  zh: '| 黑箱 | 基于迁移 | DIM[[235](#bib.bib235)], TI[[236](#bib.bib236)] |'
- en: '|  | Approximate Gradient | BPDA[[237](#bib.bib237)],EoT[[132](#bib.bib132)]
    |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '|  | 近似梯度 | BPDA[[237](#bib.bib237)], EoT[[132](#bib.bib132)] |'
- en: '|  | Score-based | ZOO[[238](#bib.bib238)], NES[[239](#bib.bib239)], SPSA[[240](#bib.bib240)],
    $N$Attack[[241](#bib.bib241)] |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于得分 | ZOO[[238](#bib.bib238)], NES[[239](#bib.bib239)], SPSA[[240](#bib.bib240)],
    $N$Attack[[241](#bib.bib241)] |'
- en: '|  | Decision-based | Boundary Attack[[242](#bib.bib242)], Evolutionary Attack[[243](#bib.bib243)]
    |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '|  | 决策基 | 边界攻击[[242](#bib.bib242)], 演化攻击[[243](#bib.bib243)] |'
- en: References
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
    Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks.
    arXiv preprint arXiv:1312.6199, 2013.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru
    Erhan, Ian Goodfellow, 和 Rob Fergus. 神经网络的有趣特性。arXiv预印本 arXiv:1312.6199，2013年。'
- en: '[2] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and
    harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] Ian J Goodfellow, Jonathon Shlens, 和 Christian Szegedy. 解释和利用对抗样本。arXiv预印本
    arXiv:1412.6572，2014年。'
- en: '[3] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying
    vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733,
    2017.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] Tianyu Gu, Brendan Dolan-Gavitt, 和 Siddharth Garg. Badnets：识别机器学习模型供应链中的漏洞。arXiv预印本
    arXiv:1708.06733，2017年。'
- en: '[4] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, and Thomas Ristenpart.
    Stealing machine learning models via prediction $\{$APIs$\}$. In 25th USENIX security
    symposium (USENIX Security 16), pages 601–618, 2016.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] Florian Tramèr, Fan Zhang, Ari Juels, Michael K Reiter, 和 Thomas Ristenpart.
    通过预测$\{$APIs$\}$窃取机器学习模型。见第25届USENIX安全研讨会（USENIX Security 16）论文集，页601–618，2016年。'
- en: '[5] Matt Fredrikson, Somesh Jha, and Thomas Ristenpart. Model inversion attacks
    that exploit confidence information and basic countermeasures. In Proceedings
    of the 22nd ACM SIGSAC conference on computer and communications security, pages
    1322–1333, 2015.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] Matt Fredrikson, Somesh Jha, 和 Thomas Ristenpart. 利用置信信息的模型反演攻击及基本对策。见第22届ACM
    SIGSAC计算机与通信安全会议论文集，页1322–1333，2015年。'
- en: '[6] Reza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership
    inference attacks against machine learning models. In 2017 IEEE symposium on security
    and privacy (SP), pages 3–18\. IEEE, 2017.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Reza Shokri, Marco Stronati, Congzheng Song, 和 Vitaly Shmatikov. 针对机器学习模型的成员推断攻击。见2017年IEEE安全与隐私研讨会（SP）论文集，页3–18，IEEE，2017年。'
- en: '[7] Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, and Jinwen He. Towards security
    threats of deep learning systems: A survey. IEEE Transactions on Software Engineering,
    2020.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] Yingzhe He, Guozhu Meng, Kai Chen, Xingbo Hu, 和 Jinwen He. 深度学习系统的安全威胁：一项调查。IEEE软件工程学报，2020年。'
- en: '[8] Shouling Ji, tianyu Du, Jinfeng Li, Chao Shen, and Bo Li. Security and
    privacy of machine learning models: A survey. Journal of Software, 32(1), 2021.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Shouling Ji, Tianyu Du, Jinfeng Li, Chao Shen, 和 Bo Li. 机器学习模型的安全与隐私：一项调查。软件学报，32(1)，2021年。'
- en: '[9] Han Xu, Yaxin Li, Wei Jin, and Jiliang Tang. Adversarial attacks and defenses:
    frontiers, advances and practice. In Proceedings of the 26th ACM SIGKDD International
    Conference on Knowledge Discovery & Data Mining, pages 3541–3542, 2020.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] Han Xu, Yaxin Li, Wei Jin, 和 Jiliang Tang. 对抗攻击与防御：前沿、进展与实践。见第26届ACM SIGKDD国际知识发现与数据挖掘大会论文集，页3541–3542，2020年。'
- en: '[10] Jiao Li, Yang Liu, Tao Chen, Zhen Xiao, Zhenjiang Li, and Jianping Wang.
    Adversarial attacks and defenses on cyber–physical systems: A survey. IEEE Internet
    of Things Journal, 7(6):5103–5115, 2020.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] Jiao Li, Yang Liu, Tao Chen, Zhen Xiao, Zhenjiang Li, 和 Jianping Wang.
    针对网络物理系统的对抗攻击与防御：一项调查。IEEE物联网学报，7(6):5103–5115，2020年。'
- en: '[11] Jiliang Zhang and Chen Li. Adversarial examples: Opportunities and challenges.
    IEEE transactions on neural networks and learning systems, 31(7):2578–2593, 2019.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Jiliang Zhang 和 Chen Li. 对抗样本：机遇与挑战。IEEE神经网络与学习系统学报，31(7):2578–2593，2019年。'
- en: '[12] Xiaoyong Yuan, Pan He, Qile Zhu, and Xiaolin Li. Adversarial examples:
    Attacks and defenses for deep learning. IEEE transactions on neural networks and
    learning systems, 30(9):2805–2824, 2019.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] Xiaoyong Yuan, Pan He, Qile Zhu, 和 Xiaolin Li. 对抗样本：深度学习的攻击与防御。IEEE神经网络与学习系统学报，30(9):2805–2824，2019年。'
- en: '[13] Micah Goldblum, Dimitris Tsipras, Chulin Xie, Xinyun Chen, Avi Schwarzschild,
    Dawn Song, Aleksander Madry, Bo Li, and Tom Goldstein. Data security for machine
    learning: data poisoning, backdoor attacks, and defenses. arXiv e-prints, pages
    arXiv–2012, 2020.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 米卡·戈德布鲁姆、迪米特里斯·齐普拉斯、徐春林、陈心云、阿维·施瓦茨柴尔德、道恩·宋、亚历克斯·马德里、博·李和汤姆·戈德斯坦。机器学习的数据安全：数据中毒、后门攻击和防御。arXiv电子印刷本，第arXiv–2012页，2020年。'
- en: '[14] SAE On-Road Automated Vehicle Standards Committee et al. Taxonomy and
    definitions for terms related to on-road motor vehicle automated driving systems.
    SAE Standard J, 3016:1–16, 2014.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] SAE公路自动驾驶车辆标准委员会等。与公路机动车辆自动驾驶系统相关的术语的分类法和定义。SAE标准J，3016:1–16，2014年。'
- en: '[15] Kui Ren, Qian Wang, Cong Wang, Zhan Qin, and Xiaodong Lin. The security
    of autonomous driving: Threats, defenses, and future directions. Proceedings of
    the IEEE, 108(2):357–372, 2019.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 任奎、王倩、王聪、秦湛和林晓东。自动驾驶的安全性：威胁、防御与未来方向。IEEE会议论文集，108(2):357–372，2019年。'
- en: '[16] Yu Huang and Yue Chen. Survey of state-of-art autonomous driving technologies
    with deep learning. In 2020 IEEE 20th International Conference on Software Quality,
    Reliability and Security Companion (QRS-C), pages 221–228\. IEEE, 2020.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 余黄和岳晨。基于深度学习的前沿自动驾驶技术综述。收录于2020 IEEE第20届软件质量、可靠性和安全性会议（QRS-C），第221–228页。IEEE，2020年。'
- en: '[17] Yao Deng, Tiehua Zhang, Guannan Lou, Xi Zheng, Jiong Jin, and Qing-Long
    Han. Deep learning-based autonomous driving systems: a survey of attacks and defenses.
    IEEE Transactions on Industrial Informatics, 17(12):7897–7912, 2021.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 邓耀、张铁华、娄关南、郑曦、金炯和韩青龙。基于深度学习的自动驾驶系统：攻击与防御的综述。IEEE工业信息学杂志，17(12):7897–7912，2021年。'
- en: '[18] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards
    real-time object detection with region proposal networks. Advances in neural information
    processing systems, 28, 2015.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 任少卿、何凯明、罗斯·吉尔什克和简孙。Faster r-cnn: 面向实时物体检测的区域提议网络。神经信息处理系统进展，28，2015年。'
- en: '[19] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Girshick. Mask r-cnn.
    In Proceedings of the IEEE international conference on computer vision, pages
    2961–2969, 2017.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 何凯明、乔治亚·格基奥萨里、皮奥特·多拉尔和罗斯·吉尔什克。Mask r-cnn。收录于《IEEE国际计算机视觉会议论文集》，第2961–2969页，2017年。'
- en: '[20] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European
    conference on computer vision, pages 21–37. Springer, 2016.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 魏刘、德拉戈米尔·安格洛夫、杜米特鲁·厄尔汉、克里斯蒂安·谢格迪、斯科特·里德、程阳福和亚历山大·C·伯格。Ssd: 单次检测多框检测器。收录于《欧洲计算机视觉会议》，第21–37页。Springer，2016年。'
- en: '[21] Joseph Redmon and Ali Farhadi. Yolov3: An incremental improvement. arXiv
    preprint arXiv:1804.02767, 2018.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] 约瑟夫·雷德蒙和阿里·法尔哈迪。Yolov3: 增量改进。arXiv预印本 arXiv:1804.02767，2018年。'
- en: '[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollár.
    Focal loss for dense object detection. In Proceedings of the IEEE international
    conference on computer vision, pages 2980–2988, 2017.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] 林宗毅、普里亚·戈亚尔、罗斯·吉尔什克、何凯明和皮奥特·多拉尔。用于密集物体检测的焦点损失。收录于《IEEE国际计算机视觉会议论文集》，第2980–2988页，2017年。'
- en: '[23] Yosuke Shinya. Usb: Universal-scale object detection benchmark. arXiv
    preprint arXiv:2103.14027, 2021.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] 深谷佳之。Usb: 通用尺度物体检测基准。arXiv预印本 arXiv:2103.14027，2021年。'
- en: '[24] Di Feng, Christian Haase-Schütz, Lars Rosenbaum, Heinz Hertlein, Claudius
    Glaeser, Fabian Timm, Werner Wiesbeck, and Klaus Dietmayer. Deep multi-modal object
    detection and semantic segmentation for autonomous driving: Datasets, methods,
    and challenges. IEEE Transactions on Intelligent Transportation Systems, 22(3):1341–1360,
    2020.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] 丁峰、克里斯蒂安·哈斯-舒茨、拉尔斯·罗森鲍姆、海因茨·赫特林、克劳迪乌斯·格莱瑟、法比安·蒂姆、维尔纳·维斯贝克和克劳斯·迪特迈耶。深度多模态物体检测与语义分割用于自动驾驶：数据集、方法和挑战。IEEE智能交通系统学报，22(3):1341–1360，2020年。'
- en: '[25] Belur V Dasarathy. Sensor fusion potential exploitation-innovative architectures
    and illustrative applications. Proceedings of the IEEE, 85(1):24–38, 1997.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] 贝卢尔·V·达萨拉西。传感器融合潜力的开发——创新架构和说明性应用。IEEE会议论文集，85(1):24–38，1997年。'
- en: '[26] Jamil Fayyad, Mohammad A Jaradat, Dominique Gruyer, and Homayoun Najjaran.
    Deep learning sensor fusion for autonomous vehicle perception and localization:
    A review. Sensors, 20(15):4220, 2020.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] 贾米尔·法亚德、穆罕默德·A·贾拉达特、多米尼克·格鲁耶和霍马勇·纳贾兰。深度学习传感器融合用于自动驾驶车辆感知与定位：综述。传感器，20(15):4220，2020年。'
- en: '[27] Yingjie Wang, Qiuyu Mao, Hanqi Zhu, Yu Zhang, Jianmin Ji, and Yanyong
    Zhang. Multi-modal 3d object detection in autonomous driving: a survey. arXiv
    preprint arXiv:2106.12735, 2021.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] 王英杰、毛秋雨、朱汉奇、张宇、季建民和张彦永。自动驾驶中的多模态3D物体检测：综述。arXiv预印本 arXiv:2106.12735，2021年。'
- en: '[28] De Jong Yeong, Gustavo Velasco-Hernandez, John Barry, Joseph Walsh, et al.
    Sensor and sensor fusion technology in autonomous vehicles: A review. Sensors,
    21(6):2140, 2021.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] 戴宗英、古斯塔沃·维拉斯科-埃尔南德斯、约翰·巴里、约瑟夫·沃尔什等。自动驾驶中的传感器及传感器融合技术：综述。《传感器》，21(6)：2140，2021年。'
- en: '[29] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and
    Raquel Urtasun. Monocular 3d object detection for autonomous driving. In Proceedings
    of the IEEE conference on computer vision and pattern recognition, pages 2147–2156,
    2016.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] 陈小志、考斯塔夫·昆杜、张子瑜、马慧敏、桑贾·费德勒和拉奎尔·乌尔塔孙。单目3D物体检测用于自动驾驶。见于《IEEE计算机视觉与模式识别会议论文集》，第2147–2156页，2016年。'
- en: '[30] Yu Xiang, Wongun Choi, Yuanqing Lin, and Silvio Savarese. Data-driven
    3d voxel patterns for object category recognition. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 1903–1911, 2015.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] 谷翔、黄君、林元青和西尔维奥·萨瓦雷斯。数据驱动的3D体素模式用于物体类别识别。见于《IEEE计算机视觉与模式识别会议论文集》，第1903–1911页，2015年。'
- en: '[31] Florian Chabot, Mohamed Chaouch, Jaonary Rabarisoa, Céline Teuliere, and
    Thierry Chateau. Deep manta: A coarse-to-fine many-task network for joint 2d and
    3d vehicle analysis from monocular image. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 2040–2049, 2017.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] 弗洛里安·沙博、穆罕默德·查乌赫、贾奥纳里·拉巴里索阿、塞琳·特雷利埃和蒂耶里·沙托。Deep Manta：一种从单目图像中进行联合2D和3D车辆分析的粗到细多任务网络。见于《IEEE计算机视觉与模式识别会议论文集》，第2040–2049页，2017年。'
- en: '[32] Qingdong He, Zhengning Wang, Hao Zeng, Yi Zeng, Shuaicheng Liu, and Bing
    Zeng. Svga-net: Sparse voxel-graph attention network for 3d object detection from
    point clouds. arXiv preprint arXiv:2006.04043, 2020.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] 贺青东、郑宁旺、曾浩、曾毅、刘帅诚和曾冰。Svga-net：用于点云中3D物体检测的稀疏体素图注意力网络。arXiv预印本 arXiv:2006.04043，2020年。'
- en: '[33] Bo Li, Tianlei Zhang, and Tian Xia. Vehicle detection from 3d lidar using
    fully convolutional network. arXiv preprint arXiv:1608.07916, 2016.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] 李波、张天磊和夏天。从3D激光雷达中使用全卷积网络进行车辆检测。arXiv预印本 arXiv:1608.07916，2016年。'
- en: '[34] Jorge Beltrán, Carlos Guindel, Francisco Miguel Moreno, Daniel Cruzado,
    Fernando Garcia, and Arturo De La Escalera. Birdnet: a 3d object detection framework
    from lidar information. In 2018 21st International Conference on Intelligent Transportation
    Systems (ITSC), pages 3517–3523\. IEEE, 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] 豪尔赫·贝尔特朗、卡洛斯·吉恩德尔、弗朗西斯科·米格尔·莫雷诺、丹尼尔·克鲁萨多、费尔南多·加西亚和阿图罗·德·拉·埃斯卡莱拉。Birdnet：基于激光雷达信息的3D物体检测框架。见于2018年第21届国际智能交通系统会议（ITSC），第3517–3523页。IEEE，2018年。'
- en: '[35] Bo Li. 3d fully convolutional network for vehicle detection in point cloud.
    In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS),
    pages 1513–1518\. IEEE, 2017.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] 李波。用于点云中的车辆检测的3D全卷积网络。见于2017 IEEE/RSJ国际智能机器人与系统会议（IROS），第1513–1518页。IEEE，2017年。'
- en: '[36] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++:
    Deep hierarchical feature learning on point sets in a metric space. Advances in
    neural information processing systems, 30, 2017.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] 查尔斯·瑞中台·齐、李毅、苏浩和利奥尼达斯·J·吉比斯。Pointnet++：在度量空间中对点集进行深度层次特征学习。《神经信息处理系统进展》，30，2017年。'
- en: '[37] Yin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud
    based 3d object detection. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 4490–4499, 2018.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] 周银和翁切尔·图泽尔。Voxelnet：基于点云的3D物体检测的端到端学习。见于《IEEE计算机视觉与模式识别会议论文集》，第4490–4499页，2018年。'
- en: '[38] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3d object
    detection network for autonomous driving. In Proceedings of the IEEE conference
    on Computer Vision and Pattern Recognition, pages 1907–1915, 2017.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] 陈小志、马慧敏、万博、李波和夏天。用于自动驾驶的多视角3D物体检测网络。见于《IEEE计算机视觉与模式识别会议论文集》，第1907–1915页，2017年。'
- en: '[39] Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L Waslander.
    Joint 3d proposal generation and object detection from view aggregation. In 2018
    IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages
    1–8\. IEEE, 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] 杰森·库、梅丽莎·莫齐菲安、李钟旭、阿里·哈拉克和史蒂文·L·瓦斯兰德。从视图聚合中生成联合3D提案和物体检测。见于2018 IEEE/RSJ国际智能机器人与系统会议（IROS），第1–8页。IEEE，2018年。'
- en: '[40] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum
    pointnets for 3d object detection from rgb-d data. In Proceedings of the IEEE
    conference on computer vision and pattern recognition, pages 918–927, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, 和 Leonidas J Guibas. 基于RGB-D数据的3D物体检测的Frustum
    Pointnets。在IEEE计算机视觉与模式识别会议论文集，页码918–927，2018年。'
- en: '[41] Duarte Fernandes, António Silva, Rafael Névoa, Claudia Simoes, Dibet Gonzalez,
    Miguel Guevara, Paulo Novais, Joao Monteiro, and Pedro Melo-Pinto. Point-cloud
    based 3d object detection and classification methods for self-driving applications:
    A survey and taxonomy. Information Fusion, 68:161–191, 2021.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] Duarte Fernandes, António Silva, Rafael Névoa, Claudia Simoes, Dibet Gonzalez,
    Miguel Guevara, Paulo Novais, Joao Monteiro, 和 Pedro Melo-Pinto. 基于点云的3D物体检测和分类方法用于自动驾驶应用：综述与分类。
    《信息融合》，68:161–191，2021年。'
- en: '[42] Rui Qian, Xin Lai, and Xirong Li. 3d object detection for autonomous driving:
    a survey. Pattern Recognition, page 108796, 2022.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Rui Qian, Xin Lai, 和 Xirong Li. 用于自动驾驶的3D物体检测：综述。《模式识别》，第108796页，2022年。'
- en: '[43] Browse state-of-the-art. [https://paperswithcode.com/area/computer-vision/autonomous-vehicles/](https://paperswithcode.com/area/computer-vision/autonomous-vehicles/).
    Accessed May 18, 2022.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] 浏览最先进技术。 [https://paperswithcode.com/area/computer-vision/autonomous-vehicles/](https://paperswithcode.com/area/computer-vision/autonomous-vehicles/)。访问日期：2022年5月18日。'
- en: '[44] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation
    learning: A survey of learning methods. ACM Computing Surveys (CSUR), 50(2):1–35,
    2017.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, 和 Chrisina Jayne. 模仿学习：学习方法综述。《ACM计算调查》（CSUR），50(2):1–35，2017年。'
- en: '[45] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos A
    Theodorou, and Byron Boots. Imitation learning for agile autonomous driving. The
    International Journal of Robotics Research, 39(2-3):286–302, 2020.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan, Evangelos
    A Theodorou, 和 Byron Boots. 为敏捷自动驾驶进行模仿学习。《国际机器人研究杂志》，39(2-3):286–302，2020年。'
- en: '[46] Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, and
    Alexey Dosovitskiy. End-to-end driving via conditional imitation learning. In
    2018 IEEE international conference on robotics and automation (ICRA), pages 4693–4700\.
    IEEE, 2018.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Felipe Codevilla, Matthias Müller, Antonio López, Vladlen Koltun, 和 Alexey
    Dosovitskiy. 通过条件模仿学习实现端到端驾驶。在2018年IEEE国际机器人与自动化会议（ICRA）上，页码4693–4700。IEEE，2018年。'
- en: '[47] Mayank Bansal, Alex Krizhevsky, and Abhijit Ogale. Chauffeurnet: Learning
    to drive by imitating the best and synthesizing the worst. arXiv preprint arXiv:1812.03079,
    2018.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] Mayank Bansal, Alex Krizhevsky, 和 Abhijit Ogale. Chauffeurnet：通过模仿最佳驾驶和合成最差驾驶来学习驾驶。arXiv预印本
    arXiv:1812.03079，2018年。'
- en: '[48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
    Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,
    et al. Human-level control through deep reinforcement learning. nature, 518(7540):529–533,
    2015.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness,
    Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski,
    等。通过深度强化学习实现人类水平控制。《自然》，518(7540):529–533，2015年。'
- en: '[49] Matthew Hausknecht and Peter Stone. Deep recurrent q-learning for partially
    observable mdps. In 2015 aaai fall symposium series, 2015.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] Matthew Hausknecht 和 Peter Stone. 深度递归Q学习用于部分可观测MDP。在2015年AAAI秋季研讨会系列，2015年。'
- en: '[50] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, and
    Anastasiia Ignateva. Deep attention recurrent q-network. arXiv preprint arXiv:1512.01693,
    2015.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] Ivan Sorokin, Alexey Seleznev, Mikhail Pavlov, Aleksandr Fedorov, 和 Anastasiia
    Ignateva. 深度注意力递归Q网络。arXiv预印本 arXiv:1512.01693，2015年。'
- en: '[51] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods
    for deep reinforcement learning. In International conference on machine learning,
    pages 1928–1937\. PMLR, 2016.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy
    Lillicrap, Tim Harley, David Silver, 和 Koray Kavukcuoglu. 深度强化学习的异步方法。在国际机器学习会议上，页码1928–1937。PMLR，2016年。'
- en: '[52] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul,
    Joel Z Leibo, David Silver, and Koray Kavukcuoglu. Reinforcement learning with
    unsupervised auxiliary tasks. arXiv preprint arXiv:1611.05397, 2016.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul,
    Joel Z Leibo, David Silver, 和 Koray Kavukcuoglu. 通过无监督辅助任务进行强化学习。arXiv预印本 arXiv:1611.05397，2016年。'
- en: '[53] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction.
    MIT press, 2018.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] Richard S Sutton 和 Andrew G Barto. 强化学习：导论。MIT出版社，2018年。'
- en: '[54] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony
    Bharath. Deep reinforcement learning: A brief survey. IEEE Signal Processing Magazine,
    34(6):26–38, 2017.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, 和 Anil Anthony
    Bharath. 深度强化学习：简要概述。IEEE信号处理杂志，34(6)：26–38，2017年。'
- en: '[55] Xidong Feng, Jianming Hu, Yusen Huo, and Yi Zhang. Autonomous lane change
    decision making using different deep reinforcement learning methods. In CICTP
    2019, pages 5563–5575\. 2019.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] Xidong Feng, Jianming Hu, Yusen Huo, 和 Yi Zhang. 使用不同深度强化学习方法进行自主车道变换决策。在CICTP
    2019，第5563–5575页。2019年。'
- en: '[56] Ali Alizadeh, Majid Moghadam, Yunus Bicer, Nazim Kemal Ure, Ugur Yavas,
    and Can Kurtulus. Automated lane change decision making using deep reinforcement
    learning in dynamic and uncertain highway environment. In 2019 IEEE Intelligent
    Transportation Systems Conference (ITSC), pages 1399–1404\. IEEE, 2019.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] Ali Alizadeh, Majid Moghadam, Yunus Bicer, Nazim Kemal Ure, Ugur Yavas,
    和 Can Kurtulus. 在动态和不确定的高速公路环境中使用深度强化学习进行自动车道变换决策。2019 IEEE智能交通系统会议（ITSC），第1399–1404页。IEEE，2019年。'
- en: '[57] Branka Mirchevska, Christian Pek, Moritz Werling, Matthias Althoff, and
    Joschka Boedecker. High-level decision making for safe and reasonable autonomous
    lane changing using reinforcement learning. In 2018 21st International Conference
    on Intelligent Transportation Systems (ITSC), pages 2156–2162\. IEEE, 2018.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] Branka Mirchevska, Christian Pek, Moritz Werling, Matthias Althoff, 和
    Joschka Boedecker. 使用强化学习进行安全且合理的自主车道变换的高层决策。在2018年第21届智能交通系统国际会议（ITSC）上，第2156–2162页。IEEE，2018年。'
- en: '[58] Yang Thee Quek, Li Ling Koh, Ngiap Tiam Koh, Wai Ann Tso, and Wai Lok
    Woo. Deep q-network implementation for simulated autonomous vehicle control. IET
    Intelligent Transport Systems, 15(7):875–885, 2021.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] Yang Thee Quek, Li Ling Koh, Ngiap Tiam Koh, Wai Ann Tso, 和 Wai Lok Woo.
    用于模拟自主车辆控制的深度Q网络实现。IET智能交通系统，15(7)：875–885，2021年。'
- en: '[59] Martin Holen, Rupsa Saha, Morten Goodwin, Christian W Omlin, and Knut Eivind
    Sandsmark. Road detection for reinforcement learning based autonomous car. In
    Proceedings of the 2020 the 3rd International Conference on Information Science
    and System, pages 67–71, 2020.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] Martin Holen, Rupsa Saha, Morten Goodwin, Christian W Omlin, 和 Knut Eivind
    Sandsmark. 基于强化学习的自主车辆道路检测。在2020年第三届国际信息科学与系统会议论文集中，第67–71页，2020年。'
- en: '[60] Yanming Feng and Yongrong Wu. Environmental adaptive urban traffic signal
    control based on reinforcement learning algorithm. In Journal of Physics: Conference
    Series, volume 1650, page 032097\. IOP Publishing, 2020.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Yanming Feng 和 Yongrong Wu. 基于强化学习算法的环境自适应城市交通信号控制。在《物理学期刊：会议系列》，第1650卷，第032097页。IOP出版，2020年。'
- en: '[61] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A
    Al Sallab, Senthil Yogamani, and Patrick Pérez. Deep reinforcement learning for
    autonomous driving: A survey. IEEE Transactions on Intelligent Transportation
    Systems, 2021.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] B Ravi Kiran, Ibrahim Sobh, Victor Talpaert, Patrick Mannion, Ahmad A
    Al Sallab, Senthil Yogamani, 和 Patrick Pérez. 自主驾驶的深度强化学习：综述。IEEE智能交通系统汇刊，2021年。'
- en: '[62] Szilárd Aradi. Survey of deep reinforcement learning for motion planning
    of autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems,
    2020.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] Szilárd Aradi. 自主车辆运动规划的深度强化学习概述。IEEE智能交通系统汇刊，2020年。'
- en: '[63] Maurice Pope. Etsi. universal mobile telecommunications system (umts);
    lte; architecture enhancements for v2x services (3gpp ts 23.285 version 14.2\.
    0 release 14), 2016.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] Maurice Pope. Etsi. 通用移动通信系统（umts）；lte；v2x服务的架构增强（3gpp ts 23.285版本14.2\.
    0发布14），2016年。'
- en: '[64] Apostolos Papathanassiou and Alexey Khoryaev. Cellular v2x as the essential
    enabler of superior global connected transportation services. IEEE 5G Tech Focus,
    1(2):1–2, 2017.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] Apostolos Papathanassiou 和 Alexey Khoryaev. 作为全球连接运输服务优越支持者的蜂窝V2X。IEEE
    5G技术焦点，1(2)：1–2，2017年。'
- en: '[65] Xiaobo Ma, Jiahao Peng, Lei Xue, and Xiaohong Guan. ntegrated security
    of cyber-physical vehicle networked systems in the age of 5g (in chinese). Sci
    Sin Inform, 2019.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] Xiaobo Ma, Jiahao Peng, Lei Xue, 和 Xiaohong Guan. 5G时代网络化车辆系统的集成安全（中文）。科学通报，2019年。'
- en: '[66] Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-Lim Alvin Yau, Yusheng
    Ji, and Jie Li. Federated learning for vehicular internet of things: Recent advances
    and open issues. IEEE Open Journal of the Computer Society, 1:45–61, 2020.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] Zhaoyang Du, Celimuge Wu, Tsutomu Yoshinaga, Kok-Lim Alvin Yau, Yusheng
    Ji, 和 Jie Li. 面向车联网的联邦学习：近期进展与开放问题。IEEE计算机学会开放期刊，1：45–61，2020年。'
- en: '[67] Jason Posner, Lewis Tseng, Moayad Aloqaily, and Yaser Jararweh. Federated
    learning in vehicular networks: opportunities and solutions. IEEE Network, 35(2):152–159,
    2021.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] Jason Posner, Lewis Tseng, Moayad Aloqaily, 和 Yaser Jararweh. 车载网络中的联邦学习：机遇与解决方案。IEEE网络，35(2):152–159，2021年。'
- en: '[68] Zi Jian Yew and Gim Hee Lee. 3dfeat-net: Weakly supervised local 3d features
    for point cloud registration. In Proceedings of the European conference on computer
    vision (ECCV), pages 607–623, 2018.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] Zi Jian Yew 和 Gim Hee Lee. 3dfeat-net: 用于点云配准的弱监督局部3d特征。发表在《欧洲计算机视觉会议论文集（ECCV）》第607–623页，2018年。'
- en: '[69] Christopher Choy, Jaesik Park, and Vladlen Koltun. Fully convolutional
    geometric features. In Proceedings of the IEEE/CVF International Conference on
    Computer Vision, pages 8958–8966, 2019.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Christopher Choy, Jaesik Park, 和 Vladlen Koltun. 完全卷积几何特征。发表在《IEEE/CVF国际计算机视觉会议论文集》，第8958–8966页，2019年。'
- en: '[70] Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, and Chiew-Lan Tai. End-to-end learning
    local multi-view descriptors for 3d point clouds. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 1919–1928, 2020.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] Lei Li, Siyu Zhu, Hongbo Fu, Ping Tan, 和 Chiew-Lan Tai. 端到端学习局部多视角描述符用于3d点云。发表在《IEEE/CVF计算机视觉与模式识别会议论文集》，第1919–1928页，2020年。'
- en: '[71] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang, Erik B Sudderth,
    and Jan Kautz. A fusion approach for multi-frame optical flow estimation. In 2019
    IEEE Winter Conference on Applications of Computer Vision (WACV), pages 2077–2086\.
    IEEE, 2019.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Zhile Ren, Orazio Gallo, Deqing Sun, Ming-Hsuan Yang, Erik B Sudderth,
    和 Jan Kautz. 一种多帧光流估计的融合方法。发表在2019 IEEE冬季计算机视觉应用会议（WACV），第2077–2086页。IEEE，2019年。'
- en: '[72] Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, and Yunhe
    Wang. Multimodal token fusion for vision transformers. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 12186–12195, 2022.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Yikai Wang, Xinghao Chen, Lele Cao, Wenbing Huang, Fuchun Sun, 和 Yunhe
    Wang. 视觉变压器的多模态标记融合。发表在《IEEE/CVF计算机视觉与模式识别会议论文集》，第12186–12195页，2022年。'
- en: '[73] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz,
    and Andreas Geiger. Transfuser: Imitation with transformer-based sensor fusion
    for autonomous driving. arXiv preprint arXiv:2205.15997, 2022.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz,
    和 Andreas Geiger. Transfuser: 基于变压器的传感器融合模仿用于自主驾驶。arXiv预印本 arXiv:2205.15997，2022年。'
- en: '[74] Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi
    Peng, Junyang Shen, Yifeng Lu, Denny Zhou, Quoc V Le, et al. Deepfusion: Lidar-camera
    deep fusion for multi-modal 3d object detection. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 17182–17191, 2022.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Jiquan Ngiam, Daiyi
    Peng, Junyang Shen, Yifeng Lu, Denny Zhou, Quoc V Le 等. Deepfusion: 激光雷达-相机深度融合用于多模态3d物体检测。发表在《IEEE/CVF计算机视觉与模式识别会议论文集》，第17182–17191页，2022年。'
- en: '[75] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference
    on computer vision, pages 1440–1448, 2015.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Ross Girshick. Fast r-cnn. 发表在《IEEE国际计算机视觉会议论文集》，第1440–1448页，2015年。'
- en: '[76] Diganta Misra. Mish: A self regularized non-monotonic neural activation
    function. arXiv preprint arXiv:1908.08681, 4(2):10–48550, 2019.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Diganta Misra. Mish: 一种自我正则化的非单调神经激活函数。arXiv预印本 arXiv:1908.08681，4(2):10–48550，2019年。'
- en: '[77] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects
    with recursive feature pyramid and switchable atrous convolution. In Proceedings
    of the IEEE/CVF conference on computer vision and pattern recognition, pages 10213–10224,
    2021.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Siyuan Qiao, Liang-Chieh Chen, 和 Alan Yuille. 检测器: 通过递归特征金字塔和可切换的空洞卷积检测物体。发表在《IEEE/CVF计算机视觉与模式识别会议论文集》，第10213–10224页，2021年。'
- en: '[78] Chien-Yao Wang, Alexey Bochkovskiy, and Hong-Yuan Mark Liao. Yolov7: Trainable
    bag-of-freebies sets new state-of-the-art for real-time object detectors. arXiv
    preprint arXiv:2207.02696, 2022.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Chien-Yao Wang, Alexey Bochkovskiy, 和 Hong-Yuan Mark Liao. Yolov7: 可训练的免费物品包设置了实时物体检测的新最先进水平。arXiv预印本
    arXiv:2207.02696，2022年。'
- en: '[79] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong
    Chen, and Baining Guo. Contrastive learning rivals masked image modeling in fine-tuning
    via feature distillation. arXiv preprint arXiv:2205.14141, 2022.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao, Jianmin Bao, Dong
    Chen, 和 Baining Guo. 对比学习在通过特征蒸馏进行微调时与掩码图像建模相媲美。arXiv预印本 arXiv:2205.14141，2022年。'
- en: '[80] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal
    generation and detection from point cloud. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 770–779, 2019.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Shaoshuai Shi, Xiaogang Wang 和 Hongsheng Li. Pointrcnn: 从点云中生成和检测 3D 目标提案.
    在 IEEE/CVF 计算机视觉与模式识别会议论文集, 页码 770–779, 2019.'
- en: '[81] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang
    Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object
    detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pages 10529–10538, 2020.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang
    Wang 和 Hongsheng Li. Pv-rcnn: 用于 3D 目标检测的点-体素特征集抽象. 在 IEEE/CVF 计算机视觉与模式识别会议论文集,
    页码 10529–10538, 2020.'
- en: '[82] Wu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu. Se-ssd: Self-ensembling
    single-stage object detector from point cloud. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 14494–14503, 2021.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Wu Zheng, Weiliang Tang, Li Jiang 和 Chi-Wing Fu. Se-ssd: 从点云中自我集成的单阶段目标检测器.
    在 IEEE/CVF 计算机视觉与模式识别会议论文集, 页码 14494–14503, 2021.'
- en: '[83] Yifan Zhang, Qijian Zhang, Zhiyu Zhu, Junhui Hou, and Yixuan Yuan. Glenet:
    Boosting 3d object detectors with generative label uncertainty estimation. arXiv
    preprint arXiv:2207.02466, 2022.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Yifan Zhang, Qijian Zhang, Zhiyu Zhu, Junhui Hou 和 Yixuan Yuan. Glenet:
    通过生成标签不确定性估计提升 3D 目标检测器. arXiv 预印本 arXiv:2207.02466, 2022.'
- en: '[84] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan
    Venkatesan, Brucek Khailany, Joel Emer, Stephen W Keckler, and William J Dally.
    Scnn: An accelerator for compressed-sparse convolutional neural networks. ACM
    SIGARCH computer architecture news, 45(2):27–40, 2017.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Angshuman Parashar, Minsoo Rhu, Anurag Mukkara, Antonio Puglielli, Rangharajan
    Venkatesan, Brucek Khailany, Joel Emer, Stephen W Keckler 和 William J Dally. Scnn:
    一种压缩稀疏卷积神经网络的加速器. ACM SIGARCH 计算机架构新闻, 45(2):27–40, 2017.'
- en: '[85] Lucas Tabelini, Rodrigo Berriel, Thiago M Paixao, Claudine Badue, Alberto F
    De Souza, and Thiago Oliveira-Santos. Keep your eyes on the lane: Real-time attention-guided
    lane detection. In Proceedings of the IEEE/CVF conference on computer vision and
    pattern recognition, pages 294–302, 2021.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Lucas Tabelini, Rodrigo Berriel, Thiago M Paixao, Claudine Badue, Alberto
    F De Souza 和 Thiago Oliveira-Santos. 盯住车道: 实时注意力引导的车道检测. 在 IEEE/CVF 计算机视觉与模式识别会议论文集,
    页码 294–302, 2021.'
- en: '[86] Tu Zheng, Yifei Huang, Yang Liu, Wenjian Tang, Zheng Yang, Deng Cai, and
    Xiaofei He. Clrnet: Cross layer refinement network for lane detection. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 898–907,
    2022.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Tu Zheng, Yifei Huang, Yang Liu, Wenjian Tang, Zheng Yang, Deng Cai 和
    Xiaofei He. Clrnet: 用于车道检测的跨层细化网络. 在 IEEE/CVF 计算机视觉与模式识别会议论文集, 页码 898–907, 2022.'
- en: '[87] Álvaro Arcos-García, Juan A Alvarez-Garcia, and Luis M Soria-Morillo.
    Deep neural network for traffic sign recognition systems: An analysis of spatial
    transformers and stochastic optimisation methods. Neural Networks, 99:158–165,
    2018.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Álvaro Arcos-García, Juan A Alvarez-Garcia 和 Luis M Soria-Morillo. 用于交通标志识别系统的深度神经网络:
    空间变换器和随机优化方法的分析. 神经网络, 99:158–165, 2018.'
- en: '[88] Junzhou Chen, Kunkun Jia, Wenquan Chen, Zhihan Lv, and Ronghui Zhang.
    A real-time and high-precision method for small traffic-signs recognition. Neural
    Computing and Applications, 34(3):2233–2245, 2022.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Junzhou Chen, Kunkun Jia, Wenquan Chen, Zhihan Lv 和 Ronghui Zhang. 一种实时高精度的小型交通标志识别方法.
    神经计算与应用, 34(3):2233–2245, 2022.'
- en: '[89] Esther Rani et al. Littleyolo-spp: A delicate real-time vehicle detection
    algorithm. Optik, 225:165818, 2021.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Esther Rani 等人. Littleyolo-spp: 一种精致的实时车辆检测算法. Optik, 225:165818, 2021.'
- en: '[90] Pranav Adarsh, Pratibha Rathi, and Manoj Kumar. Yolo v3-tiny: Object detection
    and recognition using one stage improved model. In 2020 6th International Conference
    on Advanced Computing and Communication Systems (ICACCS), pages 687–694\. IEEE,
    2020.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Pranav Adarsh, Pratibha Rathi 和 Manoj Kumar. Yolo v3-tiny: 使用改进的单阶段模型进行目标检测和识别.
    在 2020 第六届国际先进计算与通信系统会议 (ICACCS), 页码 687–694. IEEE, 2020.'
- en: '[91] Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Jiashi Feng, and Shuicheng
    Yan. Scale-aware fast r-cnn for pedestrian detection. IEEE transactions on Multimedia,
    20(4):985–996, 2017.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Jianan Li, Xiaodan Liang, ShengMei Shen, Tingfa Xu, Jiashi Feng 和 Shuicheng
    Yan. 规模感知快速 R-CNN 用于行人检测. IEEE 多媒体事务, 20(4):985–996, 2017.'
- en: '[92] Liliang Zhang, Liang Lin, Xiaodan Liang, and Kaiming He. Is faster r-cnn
    doing well for pedestrian detection? In European conference on computer vision,
    pages 443–457. Springer, 2016.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Liliang Zhang, Liang Lin, Xiaodan Liang 和 Kaiming He. Faster R-CNN 在行人检测中表现如何?
    在欧洲计算机视觉会议论文集, 页码 443–457. Springer, 2016.'
- en: '[93] Irtiza Hasan, Shengcai Liao, Jinpeng Li, Saad Ullah Akram, and Ling Shao.
    Generalizable pedestrian detection: The elephant in the room. In Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11328–11337,
    2021.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Irtiza Hasan, Shengcai Liao, Jinpeng Li, Saad Ullah Akram 和 Ling Shao.
    可泛化的行人检测：房间里的大象。载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第11328–11337页，2021年。'
- en: '[94] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional
    networks for semantic segmentation. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 3431–3440, 2015.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Jonathan Long, Evan Shelhamer 和 Trevor Darrell. 用于语义分割的全卷积网络。载于《IEEE计算机视觉与模式识别会议论文集》，第3431–3440页，2015年。'
- en: '[95] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia.
    Pyramid scene parsing network. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pages 2881–2890, 2017.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang 和 Jiaya Jia.
    金字塔场景解析网络。载于《IEEE计算机视觉与模式识别会议论文集》，第2881–2890页，2017年。'
- en: '[96] Jun Fu, Jing Liu, Jie Jiang, Yong Li, Yongjun Bao, and Hanqing Lu. Scene
    segmentation with dual relation-aware attention network. IEEE Transactions on
    Neural Networks and Learning Systems, 32(6):2547–2560, 2020.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Jun Fu, Jing Liu, Jie Jiang, Yong Li, Yongjun Bao 和 Hanqing Lu. 具有双重关系感知注意力网络的场景分割。《IEEE神经网络与学习系统汇刊》，32(6):2547–2560，2020年。'
- en: '[97] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen
    Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using
    shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pages 10012–10022, 2021.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen
    Lin 和 Baining Guo. Swin变换器：使用移动窗口的分层视觉变换器。载于《IEEE/CVF国际计算机视觉会议论文集》，第10012–10022页，2021年。'
- en: '[98] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and
    Yu Qiao. Vision transformer adapter for dense predictions. arXiv preprint arXiv:2205.08534,
    2022.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai 和 Yu
    Qiao. 用于密集预测的视觉变换器适配器。arXiv预印本 arXiv:2205.08534，2022年。'
- en: '[99] Chaoda Zheng, Xu Yan, Haiming Zhang, Baoyuan Wang, Shenghui Cheng, Shuguang
    Cui, and Zhen Li. Beyond 3d siamese tracking: A motion-centric paradigm for 3d
    single object tracking in point clouds. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 8111–8120, 2022.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Chaoda Zheng, Xu Yan, Haiming Zhang, Baoyuan Wang, Shenghui Cheng, Shuguang
    Cui 和 Zhen Li. 超越3D Siamese跟踪：一种以运动为中心的3D单目标跟踪点云范式。载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第8111–8120页，2022年。'
- en: '[100] Chaoda Zheng, Xu Yan, Jiantao Gao, Weibing Zhao, Wei Zhang, Zhen Li,
    and Shuguang Cui. Box-aware feature enhancement for single object tracking on
    point clouds. In Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pages 13199–13208, 2021.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Chaoda Zheng, Xu Yan, Jiantao Gao, Weibing Zhao, Wei Zhang, Zhen Li 和
    Shuguang Cui. 针对点云单目标跟踪的盒子感知特征增强。载于《IEEE/CVF国际计算机视觉会议论文集》，第13199–13208页，2021年。'
- en: '[101] Jiangbei Yue, Dinesh Manocha, and He Wang. Human trajectory prediction
    via neural social physics. arXiv preprint arXiv:2207.10435, 2022.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] Jiangbei Yue, Dinesh Manocha 和 He Wang. 通过神经社会物理进行人类轨迹预测。arXiv预印本 arXiv:2207.10435，2022年。'
- en: '[102] Karttikeya Mangalam, Yang An, Harshayu Girase, and Jitendra Malik. From
    goals, waypoints & paths to long term human trajectory forecasting. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision, pages 15233–15242,
    2021.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] Karttikeya Mangalam, Yang An, Harshayu Girase 和 Jitendra Malik. 从目标、航点和路径到长期人类轨迹预测。载于《IEEE/CVF国际计算机视觉会议论文集》，第15233–15242页，2021年。'
- en: '[103] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone.
    Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data.
    In European Conference on Computer Vision, pages 683–700. Springer, 2020.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] Tim Salzmann, Boris Ivanovic, Punarjay Chakravarty 和 Marco Pavone. Trajectron++：使用异质数据进行动态可行的轨迹预测。载于《欧洲计算机视觉会议论文集》，第683–700页。Springer，2020年。'
- en: '[104] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre
    Alahi. Social gan: Socially acceptable trajectories with generative adversarial
    networks. In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pages 2255–2264, 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese 和 Alexandre
    Alahi. Social gan：使用生成对抗网络生成社会可接受的轨迹。载于《IEEE计算机视觉与模式识别会议论文集》，第2255–2264页，2018年。'
- en: '[105] Amir Sadeghian, Vineet Kosaraju, Ali Sadeghian, Noriaki Hirose, Hamid
    Rezatofighi, and Silvio Savarese. Sophie: An attentive gan for predicting paths
    compliant to social and physical constraints. In Proceedings of the IEEE/CVF conference
    on computer vision and pattern recognition, pages 1349–1358, 2019.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Amir Sadeghian、Vineet Kosaraju、Ali Sadeghian、Noriaki Hirose、Hamid Rezatofighi
    和 Silvio Savarese。Sophie：一种用于预测符合社会和物理约束的路径的注意力GAN。发表于《IEEE/CVF计算机视觉与模式识别会议论文集》，页面1349–1358，2019年。'
- en: '[106] Shaoshuai Shi, Li Jiang, Dengxin Dai, and Bernt Schiele. Motion transformer
    with global intention localization and local movement refinement. arXiv preprint
    arXiv:2209.13508, 2022.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Shaoshuai Shi、Li Jiang、Dengxin Dai 和 Bernt Schiele。具有全局意图定位和局部运动细化的运动变换器。arXiv预印本
    arXiv:2209.13508，2022年。'
- en: '[107] Nigamaa Nayakanti, Rami Al-Rfou, Aurick Zhou, Kratarth Goel, Khaled S
    Refaat, and Benjamin Sapp. Wayformer: Motion forecasting via simple & efficient
    attention networks. arXiv preprint arXiv:2207.05844, 2022.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] Nigamaa Nayakanti、Rami Al-Rfou、Aurick Zhou、Kratarth Goel、Khaled S Refaat
    和 Benjamin Sapp。Wayformer：通过简单高效的注意力网络进行运动预测。arXiv预印本 arXiv:2207.05844，2022年。'
- en: '[108] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis
    Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement
    learning. arXiv preprint arXiv:1312.5602, 2013.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] Volodymyr Mnih、Koray Kavukcuoglu、David Silver、Alex Graves、Ioannis Antonoglou、Daan
    Wierstra 和 Martin Riedmiller。使用深度强化学习玩Atari游戏。arXiv预印本 arXiv:1312.5602，2013年。'
- en: '[109] Hado Hasselt. Double q-learning. Advances in neural information processing
    systems, 23, 2010.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] Hado Hasselt。双重Q学习。《神经信息处理系统进展》，23，2010年。'
- en: '[110] Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning.
    Advances in neural information processing systems, 29, 2016.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Jonathan Ho 和 Stefano Ermon。生成对抗模仿学习。《神经信息处理系统进展》，29，2016年。'
- en: '[111] Jeffrey Hawke, Richard Shen, Corina Gurau, Siddharth Sharma, Daniele
    Reda, Nikolay Nikolov, Przemysław Mazur, Sean Micklethwaite, Nicolas Griffiths,
    Amar Shah, et al. Urban driving with conditional imitation learning. In 2020 IEEE
    International Conference on Robotics and Automation (ICRA), pages 251–257\. IEEE,
    2020.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] Jeffrey Hawke、Richard Shen、Corina Gurau、Siddharth Sharma、Daniele Reda、Nikolay
    Nikolov、Przemysław Mazur、Sean Micklethwaite、Nicolas Griffiths、Amar Shah 等。使用条件模仿学习进行城市驾驶。发表于2020
    IEEE国际机器人与自动化会议（ICRA），页面251–257，IEEE，2020年。'
- en: '[112] Junhyuk Oh, Yijie Guo, Satinder Singh, and Honglak Lee. Self-imitation
    learning. In International Conference on Machine Learning, pages 3878–3887\. PMLR,
    2018.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] Junhyuk Oh、Yijie Guo、Satinder Singh 和 Honglak Lee。自我模仿学习。发表于国际机器学习会议，页面3878–3887，PMLR，2018年。'
- en: '[113] Tian Li, Anit Kumar Sahu, Ameet Talwalkar, and Virginia Smith. Federated
    learning: Challenges, methods, and future directions. IEEE Signal Processing Magazine,
    37(3):50–60, 2020.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Tian Li、Anit Kumar Sahu、Ameet Talwalkar 和 Virginia Smith。联邦学习：挑战、方法及未来方向。《IEEE信号处理杂志》，37(3)：50–60，2020年。'
- en: '[114] Jonathan Petit, Bas Stottelaar, Michael Feiri, and Frank Kargl. Remote
    attacks on automated vehicles sensors: Experiments on camera and lidar. Black
    Hat Europe, 11(2015):995, 2015.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Jonathan Petit、Bas Stottelaar、Michael Feiri 和 Frank Kargl。对自动驾驶车辆传感器的远程攻击：相机和激光雷达的实验。黑帽欧洲，11(2015)：995，2015年。'
- en: '[115] Chen Yan, Wenyuan Xu, and Jianhao Liu. Can you trust autonomous vehicles:
    Contactless attacks against sensors of self-driving vehicle. Def Con, 24(8):109,
    2016.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Chen Yan、Wenyuan Xu 和 Jianhao Liu。你能相信自动驾驶车辆吗：针对自动驾驶车辆传感器的非接触攻击。Def Con，24(8)：109，2016年。'
- en: '[116] Guoming Zhang, Chen Yan, Xiaoyu Ji, Tianchen Zhang, Taimin Zhang, and
    Wenyuan Xu. Dolphinattack: Inaudible voice commands. In Proceedings of the 2017
    ACM SIGSAC Conference on Computer and Communications Security, pages 103–117,
    2017.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] Guoming Zhang、Chen Yan、Xiaoyu Ji、Tianchen Zhang、Taimin Zhang 和 Wenyuan
    Xu。Dolphinattack：不可听见的语音命令。发表于2017年ACM SIGSAC计算机与通信安全会议，页面103–117，2017年。'
- en: '[117] Bing Shun Lim, Sye Loong Keoh, and Vrizlynn LL Thing. Autonomous vehicle
    ultrasonic sensor vulnerability and impact assessment. In 2018 IEEE 4th World
    Forum on Internet of Things (WF-IoT), pages 231–236\. IEEE, 2018.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] Bing Shun Lim、Sye Loong Keoh 和 Vrizlynn LL Thing。自动驾驶车辆超声波传感器的脆弱性和影响评估。发表于2018
    IEEE第四届物联网世界论坛（WF-IoT），页面231–236，IEEE，2018年。'
- en: '[118] Yunmok Son, Hocheol Shin, Dongkwan Kim, Youngseok Park, Juhwan Noh, Kibum
    Choi, Jungwoo Choi, and Yongdae Kim. Rocking drones with intentional sound noise
    on gyroscopic sensors. In 24th USENIX Security Symposium (USENIX Security 15),
    pages 881–896, 2015.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] Yunmok Son、Hocheol Shin、Dongkwan Kim、Youngseok Park、Juhwan Noh、Kibum
    Choi、Jungwoo Choi 和 Yongdae Kim。通过故意的声音噪声在陀螺传感器上摇摆无人机。发表于第24届USENIX安全研讨会（USENIX
    Security 15），页面881–896，2015年。'
- en: '[119] Gorkem Kar, Hossen Mustafa, Yan Wang, Yingying Chen, Wenyuan Xu, Marco
    Gruteser, and Tam Vu. Detection of on-road vehicles emanating gps interference.
    In Proceedings of the 2014 ACM SIGSAC conference on computer and communications
    security, pages 621–632, 2014.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] Gorkem Kar、Hossen Mustafa、Yan Wang、Yingying Chen、Wenyuan Xu、Marco Gruteser
    和 Tam Vu. 检测道路上车辆发出的 GPS 干扰。收录于 2014 年 ACM SIGSAC 计算机与通信安全会议论文集，页码 621–632，2014
    年。'
- en: '[120] Youngseok Park, Yunmok Son, Hocheol Shin, Dohyun Kim, and Yongdae Kim.
    This ain’t your dose: Sensor spoofing attack on medical infusion pump. In 10th
    USENIX workshop on offensive technologies (WOOT 16), 2016.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] Youngseok Park、Yunmok Son、Hocheol Shin、Dohyun Kim 和 Yongdae Kim. 这不是你的剂量：医疗输液泵的传感器伪造攻击。收录于第
    10 届 USENIX 进攻技术研讨会（WOOT 16），2016 年。'
- en: '[121] Dudi Nassi, Raz Ben-Netanel, Yuval Elovici, and Ben Nassi. Mobilbye:
    attacking adas with camera spoofing. arXiv preprint arXiv:1906.09765, 2019.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] Dudi Nassi、Raz Ben-Netanel、Yuval Elovici 和 Ben Nassi. Mobilbye：通过摄像头伪造攻击高级驾驶辅助系统（ADAS）。arXiv
    预印本 arXiv:1906.09765，2019 年。'
- en: '[122] Mark L Psiaki and Todd E Humphreys. Protecting gps from spoofers is critical
    to the future of navigation. IEEE spectrum, 10, 2016.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Mark L Psiaki 和 Todd E Humphreys. 保护 GPS 免受伪造者的攻击对未来导航至关重要。IEEE Spectrum，第
    10 期，2016 年。'
- en: '[123] Qian Meng, Li-Ta Hsu, Bing Xu, Xiapu Luo, and Ahmed El-Mowafy. A gps
    spoofing generator using an open sourced vector tracking-based receiver. Sensors,
    19(18):3993, 2019.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] Qian Meng、Li-Ta Hsu、Bing Xu、Xiapu Luo 和 Ahmed El-Mowafy. 使用开源矢量跟踪接收机的
    GPS 伪造生成器。传感器，19(18)：3993，2019 年。'
- en: '[124] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay
    Celik, and Ananthram Swami. Practical black-box attacks against machine learning.
    In Proceedings of the 2017 ACM on Asia conference on computer and communications
    security, pages 506–519, 2017.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] Nicolas Papernot、Patrick McDaniel、Ian Goodfellow、Somesh Jha、Z Berkay
    Celik 和 Ananthram Swami. 针对机器学习的实用黑箱攻击。收录于 2017 年 ACM 亚洲计算机与通信安全会议论文集，页码 506–519，2017
    年。'
- en: '[125] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal
    Frossard. Universal adversarial perturbations. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 1765–1773, 2017.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] Seyed-Mohsen Moosavi-Dezfooli、Alhussein Fawzi、Omar Fawzi 和 Pascal Frossard.
    通用对抗扰动。收录于 IEEE 计算机视觉与模式识别会议论文集，页码 1765–1773，2017 年。'
- en: '[126] Nicholas Carlini and David Wagner. Towards evaluating the robustness
    of neural networks. In 2017 ieee symposium on security and privacy (sp), pages
    39–57\. IEEE, 2017.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Nicholas Carlini 和 David Wagner. 评估神经网络鲁棒性的进展。收录于 2017 年 IEEE 安全与隐私研讨会（SP），页码
    39–57。IEEE，2017 年。'
- en: '[127] Zhengli Zhao, Dheeru Dua, and Sameer Singh. Generating natural adversarial
    examples. In International Conference on Learning Representations, 2018.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] Zhengli Zhao、Dheeru Dua 和 Sameer Singh. 生成自然对抗示例。收录于国际学习表征会议，2018 年。'
- en: '[128] Alexey Kurakin, Ian Goodfellow, Samy Bengio, et al. Adversarial examples
    in the physical world, 2016.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] Alexey Kurakin、Ian Goodfellow、Samy Bengio 等. 物理世界中的对抗示例，2016 年。'
- en: '[129] Jiajun Lu, Hussein Sibai, and Evan Fabry. Adversarial examples that fool
    detectors. arXiv preprint arXiv:1712.02494, 2017.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] Jiajun Lu、Hussein Sibai 和 Evan Fabry. 误导检测器的对抗示例。arXiv 预印本 arXiv:1712.02494，2017
    年。'
- en: '[130] Kevin Eykholt, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati,
    Chaowei Xiao, Atul Prakash, Tadayoshi Kohno, and Dawn Song. Robust physical-world
    attacks on deep learning visual classification. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 1625–1634, 2018.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] Kevin Eykholt、Ivan Evtimov、Earlence Fernandes、Bo Li、Amir Rahmati、Chaowei
    Xiao、Atul Prakash、Tadayoshi Kohno 和 Dawn Song. 对深度学习视觉分类的鲁棒物理世界攻击。收录于 IEEE 计算机视觉与模式识别会议论文集，页码
    1625–1634，2018 年。'
- en: '[131] Shang-Tse Chen, Cory Cornelius, Jason Martin, and Duen Horng Polo Chau.
    Shapeshifter: Robust physical adversarial attack on faster r-cnn object detector.
    In Joint European Conference on Machine Learning and Knowledge Discovery in Databases,
    pages 52–68\. Springer, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] Shang-Tse Chen、Cory Cornelius、Jason Martin 和 Duen Horng Polo Chau. Shapeshifter：对
    Faster R-CNN 目标检测器的鲁棒物理对抗攻击。收录于欧盟联合机器学习与知识发现数据库会议，页码 52–68。施普林格，2018 年。'
- en: '[132] Anish Athalye, Logan Engstrom, Andrew Ilyas, and Kevin Kwok. Synthesizing
    robust adversarial examples. In International conference on machine learning,
    pages 284–293\. PMLR, 2018.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] Anish Athalye、Logan Engstrom、Andrew Ilyas 和 Kevin Kwok. 合成鲁棒对抗示例。收录于国际机器学习会议，页码
    284–293。PMLR，2018 年。'
- en: '[133] Zhong-Qiu Zhao, Peng Zheng, Shou-tao Xu, and Xindong Wu. Object detection
    with deep learning: A review. IEEE transactions on neural networks and learning
    systems, 30(11):3212–3232, 2019.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] Zhong-Qiu Zhao、Peng Zheng、Shou-tao Xu 和 Xindong Wu. 深度学习的目标检测：综述。IEEE
    神经网络与学习系统汇刊，30(11)：3212–3232，2019 年。'
- en: '[134] Zelun Kong, Junfeng Guo, Ang Li, and Cong Liu. Physgan: Generating physical-world-resilient
    adversarial examples for autonomous driving. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 14254–14263, 2020.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] Zelun Kong, Junfeng Guo, Ang Li, 和 Cong Liu。Physgan：为自动驾驶生成物理世界抗干扰的对抗样本。在《IEEE/CVF计算机视觉与模式识别会议论文集》，第14254–14263页，2020年。'
- en: '[135] Zuxuan Wu, Ser-Nam Lim, Larry S Davis, and Tom Goldstein. Making an invisibility
    cloak: Real world adversarial attacks on object detectors. In European Conference
    on Computer Vision, pages 1–17. Springer, 2020.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] Zuxuan Wu, Ser-Nam Lim, Larry S Davis, 和 Tom Goldstein。制造隐身斗篷：对物体检测器的现实世界对抗攻击。在《欧洲计算机视觉会议》，第1–17页。施普林格，2020年。'
- en: '[136] Yi Wang, Jingyang Zhou, Tianlong Chen, Sijia Liu, Shiyu Chang, Chandrajit
    Bajaj, and Zhangyang Wang. Can 3d adversarial logos cloak humans? arXiv preprint
    arXiv:2006.14655, 2020.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] Yi Wang, Jingyang Zhou, Tianlong Chen, Sijia Liu, Shiyu Chang, Chandrajit
    Bajaj, 和 Zhangyang Wang。3D对抗性标志能遮蔽人类吗？arXiv预印本 arXiv:2006.14655，2020年。'
- en: '[137] Yang Zhang, PD Hassan Foroosh, and Boqing Gong. Camou: Learning a vehicle
    camouflage for physical adversarial attack on object detections in the wild. ICLR,
    2019.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] Yang Zhang, PD Hassan Foroosh, 和 Boqing Gong。Camou：为物体检测的物理对抗攻击学习车辆伪装。ICLR，2019年。'
- en: '[138] Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, and Justin Gilmer.
    Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] Tom B Brown, Dandelion Mané, Aurko Roy, Martín Abadi, 和 Justin Gilmer。对抗性补丁。arXiv预印本
    arXiv:1712.09665，2017年。'
- en: '[139] Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, and Yu Wang.
    Physical adversarial attack on vehicle detector in the carla simulator. arXiv
    preprint arXiv:2007.16118, 2020.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] Tong Wu, Xuefei Ning, Wenshuo Li, Ranran Huang, Huazhong Yang, 和 Yu Wang。在CARLA模拟器中对车辆检测器的物理对抗攻击。arXiv预印本
    arXiv:2007.16118，2020年。'
- en: '[140] Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi,
    Qi Alfred Chen, Kevin Fu, and Z Morley Mao. Adversarial sensor attack on lidar-based
    perception in autonomous driving. In Proceedings of the 2019 ACM SIGSAC conference
    on computer and communications security, pages 2267–2281, 2019.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] Yulong Cao, Chaowei Xiao, Benjamin Cyr, Yimeng Zhou, Won Park, Sara Rampazzi,
    Qi Alfred Chen, Kevin Fu, 和 Z Morley Mao。对基于激光雷达的自动驾驶感知的对抗传感器攻击。在2019年ACM SIGSAC计算机与通信安全会议论文集中，第2267–2281页，2019年。'
- en: '[141] Qi Sun, Xufeng Yao, Arjun Ashok Rao, Bei Yu, and Shiyan Hu. Counteracting
    adversarial attacks in autonomous driving. IEEE Transactions on Computer-Aided
    Design of Integrated Circuits and Systems, 2022.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Qi Sun, Xufeng Yao, Arjun Ashok Rao, Bei Yu, 和 Shiyan Hu。在自动驾驶中对抗对抗攻击。IEEE《集成电路与系统计算机辅助设计汇刊》，2022年。'
- en: '[142] Yulong Cao, Ningfei Wang, Chaowei Xiao, Dawei Yang, Jin Fang, Ruigang
    Yang, Qi Alfred Chen, Mingyan Liu, and Bo Li. Invisible for both camera and lidar:
    Security of multi-sensor fusion based perception in autonomous driving under physical-world
    attacks. In 2021 IEEE Symposium on Security and Privacy (SP), pages 176–194\.
    IEEE, 2021.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] Yulong Cao, Ningfei Wang, Chaowei Xiao, Dawei Yang, Jin Fang, Ruigang
    Yang, Qi Alfred Chen, Mingyan Liu, 和 Bo Li。对摄像头和激光雷达均隐形：物理世界攻击下基于多传感器融合的自动驾驶感知的安全性。在2021年IEEE安全与隐私研讨会（SP），第176–194页。IEEE，2021年。'
- en: '[143] Xuesong Chen, Xiyu Yan, Feng Zheng, Yong Jiang, Shu-Tao Xia, Yong Zhao,
    and Rongrong Ji. One-shot adversarial attacks on visual tracking with dual attention.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 10176–10185, 2020.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] Xuesong Chen, Xiyu Yan, Feng Zheng, Yong Jiang, Shu-Tao Xia, Yong Zhao,
    和 Rongrong Ji。单次对抗攻击视觉跟踪与双重注意力。在《IEEE/CVF计算机视觉与模式识别会议论文集》，第10176–10185页，2020年。'
- en: '[144] Bin Yan, Dong Wang, Huchuan Lu, and Xiaoyun Yang. Cooling-shrinking attack:
    Blinding the tracker with imperceptible noises. In Proceedings of the IEEE/CVF
    Conference on Computer Vision and Pattern Recognition, pages 990–999, 2020.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Bin Yan, Dong Wang, Huchuan Lu, 和 Xiaoyun Yang。冷却收缩攻击：用难以察觉的噪声使跟踪器失效。在《IEEE/CVF计算机视觉与模式识别会议论文集》，第990–999页，2020年。'
- en: '[145] Shuai Jia, Yibing Song, Chao Ma, and Xiaokang Yang. Iou attack: Towards
    temporally coherent black-box adversarial attack for visual object tracking. In
    Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 6709–6718, 2021.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] Shuai Jia, Yibing Song, Chao Ma, 和 Xiaokang Yang。IoU攻击：面向时间一致的黑箱对抗攻击的视觉目标跟踪。在《IEEE/CVF计算机视觉与模式识别会议论文集》，第6709–6718页，2021年。'
- en: '[146] Yunhan Jia Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Hao Chen, Zhenyu
    Zhong, and Tao Wei Wei. Fooling detection alone is not enough: Adversarial attack
    against multiple object tracking. In International Conference on Learning Representations
    (ICLR’20), 2020.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Yunhan Jia Jia, Yantao Lu, Junjie Shen, Qi Alfred Chen, Hao Chen, Zhenyu
    Zhong, 和 Tao Wei Wei. 单独欺骗检测是不够的：对多个目标跟踪的对抗攻击。国际学习表征会议（ICLR’20），2020年。'
- en: '[147] Delv Lin, Qi Chen, Chengyu Zhou, and Kun He. Trasw: Tracklet-switch adversarial
    attacks against multi-object tracking. arXiv preprint arXiv:2111.08954, 2021.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] Delv Lin, Qi Chen, Chengyu Zhou, 和 Kun He. Trasw: 对多目标跟踪的轨迹切换对抗攻击。arXiv预印本
    arXiv:2111.08954, 2021年。'
- en: '[148] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin IP
    Rubinstein, Udam Saini, Charles Sutton, J Doug Tygar, and Kai Xia. Exploiting
    machine learning to subvert your spam filter. LEET, 8(1-9):16–17, 2008.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] Blaine Nelson, Marco Barreno, Fuching Jack Chi, Anthony D Joseph, Benjamin
    IP Rubinstein, Udam Saini, Charles Sutton, J Doug Tygar, 和 Kai Xia. 利用机器学习破坏你的垃圾邮件过滤器。LEET,
    8(1-9):16–17, 2008年。'
- en: '[149] Marco Barreno, Blaine Nelson, Anthony D Joseph, and J Doug Tygar. The
    security of machine learning. Machine Learning, 81(2):121–148, 2010.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] Marco Barreno, Blaine Nelson, Anthony D Joseph, 和 J Doug Tygar. 机器学习的安全性。机器学习,
    81(2):121–148, 2010年。'
- en: '[150] Battista Biggio, Blaine Nelson, and Pavel Laskov. Poisoning attacks against
    support vector machines. arXiv preprint arXiv:1206.6389, 2012.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] Battista Biggio, Blaine Nelson, 和 Pavel Laskov. 针对支持向量机的中毒攻击。arXiv预印本
    arXiv:1206.6389, 2012年。'
- en: '[151] Scott Alfeld, Xiaojin Zhu, and Paul Barford. Data poisoning attacks against
    autoregressive models. In Proceedings of the AAAI Conference on Artificial Intelligence,
    volume 30, 2016.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] Scott Alfeld, Xiaojin Zhu, 和 Paul Barford. 针对自回归模型的数据中毒攻击。发表于《AAAI人工智能会议论文集》，第30卷，2016年。'
- en: '[152] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice,
    Vasin Wongrassamee, Emil C Lupu, and Fabio Roli. Towards poisoning of deep learning
    algorithms with back-gradient optimization. In Proceedings of the 10th ACM workshop
    on artificial intelligence and security, pages 27–38, 2017.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] Luis Muñoz-González, Battista Biggio, Ambra Demontis, Andrea Paudice,
    Vasin Wongrassamee, Emil C Lupu, 和 Fabio Roli. 通过反向梯度优化来中毒深度学习算法。发表于第10届ACM人工智能与安全研讨会，第27–38页，2017年。'
- en: '[153] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph
    Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label
    poisoning attacks on neural networks. Advances in neural information processing
    systems, 31, 2018.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] Ali Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph
    Studer, Tudor Dumitras, 和 Tom Goldstein. **毒蛙**！对神经网络的有针对性干净标签中毒攻击。神经信息处理系统进展，31，2018年。'
- en: '[154] Alexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label
    backdoor attacks. 2018.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] Alexander Turner, Dimitris Tsipras, 和 Aleksander Madry. 干净标签后门攻击。2018年。'
- en: '[155] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, and Tudor
    Dumitras. When does machine learning $\{$FAIL$\}$? generalized transferability
    for evasion and poisoning attacks. In 27th USENIX Security Symposium (USENIX Security
    18), pages 1299–1316, 2018.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Octavian Suciu, Radu Marginean, Yigitcan Kaya, Hal Daume III, 和 Tudor
    Dumitras. 机器学习何时会**失败**？逃避和中毒攻击的广义可转移性。发表于第27届USENIX安全研讨会（USENIX Security 18），第1299–1316页，2018年。'
- en: '[156] Aniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden
    trigger backdoor attacks. In Proceedings of the AAAI conference on artificial
    intelligence, volume 34, pages 11957–11965, 2020.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Aniruddha Saha, Akshayvarun Subramanya, 和 Hamed Pirsiavash. 隐藏触发后门攻击。发表于《AAAI人工智能会议论文集》，第34卷，第11957–11965页，2020年。'
- en: '[157] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh.
    Universal adversarial triggers for attacking and analyzing nlp. arXiv preprint
    arXiv:1908.07125, 2019.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, 和 Sameer Singh.
    攻击和分析自然语言处理的通用对抗触发器。arXiv预印本 arXiv:1908.07125, 2019年。'
- en: '[158] Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, and Xinpeng
    Zhang. Invisible backdoor attacks on deep neural networks via steganography and
    regularization. IEEE Transactions on Dependable and Secure Computing, 18(5):2088–2105,
    2020.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] Shaofeng Li, Minhui Xue, Benjamin Zi Hao Zhao, Haojin Zhu, 和 Xinpeng
    Zhang. 通过隐写术和正则化对深度神经网络进行**隐形后门攻击**。IEEE《可靠和安全计算学报》，18(5):2088–2105, 2020年。'
- en: '[159] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable
    adversarial examples and black-box attacks. arXiv preprint arXiv:1611.02770, 2016.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] Yanpei Liu, Xinyun Chen, Chang Liu, 和 Dawn Song. 深入探讨可转移对抗样本和黑盒攻击。arXiv预印本
    arXiv:1611.02770, 2016年。'
- en: '[160] Yuanshun Yao, Huiying Li, Haitao Zheng, and Ben Y Zhao. Latent backdoor
    attacks on deep neural networks. In Proceedings of the 2019 ACM SIGSAC Conference
    on Computer and Communications Security, pages 2041–2055, 2019.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] Yuanshun Yao、Huiying Li、Haitao Zheng 和 Ben Y Zhao. 对深度神经网络的潜在后门攻击。见于
    2019 年 ACM SIGSAC 计算机与通信安全会议论文集，第 2041–2055 页，2019 年。'
- en: '[161] Chen Zhu, W Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer,
    and Tom Goldstein. Transferable clean-label poisoning attacks on deep neural nets.
    In International Conference on Machine Learning, pages 7614–7623\. PMLR, 2019.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] Chen Zhu、W Ronny Huang、Hengduo Li、Gavin Taylor、Christoph Studer 和 Tom
    Goldstein. 对深度神经网络的可转移清洁标签中毒攻击。见于国际机器学习大会，第 7614–7623 页。PMLR，2019 年。'
- en: '[162] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang
    Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] Yingqi Liu、Shiqing Ma、Yousra Aafer、Wen-Chuan Lee、Juan Zhai、Weihang Wang
    和 Xiangyu Zhang. 对神经网络的特洛伊木马攻击。2017 年。'
- en: '[163] Huma Rehman, Andreas Ekelhart, and Rudolf Mayer. Backdoor attacks in
    neural networks–a systematic evaluation on multiple traffic sign datasets. In
    International Cross-Domain Conference for Machine Learning and Knowledge Extraction,
    pages 285–300\. Springer, 2019.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] Huma Rehman、Andreas Ekelhart 和 Rudolf Mayer. 神经网络中的后门攻击——在多个交通标志数据集上的系统评估。见于国际跨领域机器学习与知识提取大会，第
    285–300 页。Springer，2019 年。'
- en: '[164] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack
    in cnns by training set corruption without label poisoning. In 2019 IEEE International
    Conference on Image Processing (ICIP), pages 101–105\. IEEE, 2019.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] Mauro Barni、Kassem Kallas 和 Benedetta Tondi. 一种通过训练集污染而非标签中毒的 CNN 新后门攻击。见于
    2019 年 IEEE 国际图像处理大会 (ICIP)，第 101–105 页。IEEE，2019 年。'
- en: '[165] Shaohua Ding, Yulong Tian, Fengyuan Xu, Qun Li, and Sheng Zhong. Trojan
    attack on deep generative models in autonomous driving. In International Conference
    on Security and Privacy in Communication Systems, pages 299–318\. Springer, 2019.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] Shaohua Ding、Yulong Tian、Fengyuan Xu、Qun Li 和 Sheng Zhong. 对深度生成模型的特洛伊木马攻击在自动驾驶中的应用。见于安全与隐私通信系统国际会议，第
    299–318 页。Springer，2019 年。'
- en: '[166] Guiyu Tian, Wenhao Jiang, Wei Liu, and Yadong Mu. Poisoning morphnet
    for clean-label backdoor attack to point clouds. arXiv preprint arXiv:2105.04839,
    2021.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] Guiyu Tian、Wenhao Jiang、Wei Liu 和 Yadong Mu. 通过中毒 MorphNet 对点云进行清洁标签后门攻击。arXiv
    预印本 arXiv:2105.04839，2021 年。'
- en: '[167] Sakshi Udeshi, Shanshan Peng, Gerald Woo, Lionell Loh, Louth Rawshan,
    and Sudipta Chattopadhyay. Model agnostic defence against backdoor attacks in
    machine learning. IEEE Transactions on Reliability, 2022.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] Sakshi Udeshi、Shanshan Peng、Gerald Woo、Lionell Loh、Louth Rawshan 和 Sudipta
    Chattopadhyay. 针对机器学习中后门攻击的模型无关防御。IEEE 可靠性学报，2022 年。'
- en: '[168] Yue Zhao, Hong Zhu, Ruigang Liang, Qintao Shen, Shengzhi Zhang, and Kai
    Chen. Seeing isn’t believing: Towards more robust adversarial attack against real
    world object detectors. In Proceedings of the 2019 ACM SIGSAC Conference on Computer
    and Communications Security, pages 1989–2004, 2019.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] Yue Zhao、Hong Zhu、Ruigang Liang、Qintao Shen、Shengzhi Zhang 和 Kai Chen.
    眼见未必为实：朝着更强对抗性攻击现实世界物体检测器的方向前进。见于 2019 年 ACM SIGSAC 计算机与通信安全会议论文集，第 1989–2004
    页，2019 年。'
- en: '[169] Mahmood Sharif, Sruti Bhagavatula, Lujo Bauer, and Michael K Reiter.
    Accessorize to a crime: Real and stealthy attacks on state-of-the-art face recognition.
    In Proceedings of the 2016 acm sigsac conference on computer and communications
    security, pages 1528–1540, 2016.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] Mahmood Sharif、Sruti Bhagavatula、Lujo Bauer 和 Michael K Reiter. 罪恶的装饰：对最先进面部识别技术的真实而隐蔽的攻击。见于
    2016 年 ACM SIGSAC 计算机与通信安全会议论文集，第 1528–1540 页，2016 年。'
- en: '[170] Stepan Komkov and Aleksandr Petiushko. Advhat: Real-world adversarial
    attack on arcface face id system. In 2020 25th International Conference on Pattern
    Recognition (ICPR), pages 819–826\. IEEE, 2021.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] Stepan Komkov 和 Aleksandr Petiushko. Advhat: 对 ArcFace 面部识别系统的现实世界对抗性攻击。见于
    2020 年第 25 届国际模式识别大会 (ICPR)，第 819–826 页。IEEE，2021 年。'
- en: '[171] Lifeng Huang, Chengying Gao, Yuyin Zhou, Cihang Xie, Alan L Yuille, Changqing
    Zou, and Ning Liu. Universal physical camouflage attacks on object detectors.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 720–729, 2020.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] Lifeng Huang、Chengying Gao、Yuyin Zhou、Cihang Xie、Alan L Yuille、Changqing
    Zou 和 Ning Liu. 对物体检测器的通用物理伪装攻击。见于 IEEE/CVF 计算机视觉与模式识别大会论文集，第 720–729 页，2020 年。'
- en: '[172] Weiwei Hu and Ying Tan. Black-box attacks against rnn based malware detection
    algorithms. In Workshops at the Thirty-Second AAAI Conference on Artificial Intelligence,
    2018.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] Weiwei Hu 和 Ying Tan. 针对基于 RNN 的恶意软件检测算法的黑盒攻击。见于第三十二届 AAAI 人工智能会议研讨会，2018
    年。'
- en: '[173] Kuei-Huan Chang, Po-Hao Huang, Honggang Yu, Yier Jin, and Ting-Chi Wang.
    Audio adversarial examples generation with recurrent neural networks. In 2020
    25th Asia and South Pacific Design Automation Conference (ASP-DAC), pages 488–493\.
    IEEE, 2020.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] Kuei-Huan Chang, Po-Hao Huang, Honggang Yu, Yier Jin 和 Ting-Chi Wang.
    使用递归神经网络生成音频对抗样本。发表于2020年第25届亚洲和南太平洋设计自动化会议（ASP-DAC），第488–493页。IEEE，2020年。'
- en: '[174] Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang, and Cho-Jui Hsieh.
    Seq2sick: Evaluating the robustness of sequence-to-sequence models with adversarial
    examples. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34,
    pages 3601–3608, 2020.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] Minhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang 和 Cho-Jui Hsieh. Seq2sick：评估序列到序列模型在对抗样本中的鲁棒性。发表于AAAI人工智能会议论文集，卷34，第3601–3608页，2020年。'
- en: '[175] Nicholas Carlini and David Wagner. Audio adversarial examples: Targeted
    attacks on speech-to-text. In 2018 IEEE Security and Privacy Workshops (SPW),
    pages 1–7. IEEE, 2018.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] Nicholas Carlini 和 David Wagner. 音频对抗样本：针对语音识别的定向攻击。发表于2018年IEEE安全与隐私研讨会（SPW），第1–7页。IEEE，2018年。'
- en: '[176] Felix Kreuk, Yossi Adi, Moustapha Cisse, and Joseph Keshet. Fooling end-to-end
    speaker verification with adversarial examples. In 2018 IEEE international conference
    on acoustics, speech and signal processing (ICASSP), pages 1962–1966\. IEEE, 2018.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] Felix Kreuk, Yossi Adi, Moustapha Cisse 和 Joseph Keshet. 通过对抗样本欺骗端到端的说话人验证。发表于2018年IEEE国际声学、语音和信号处理会议（ICASSP），第1962–1966页。IEEE，2018年。'
- en: '[177] Xiaolin Song, Xin Sheng, Haotian Cao, Mingjun Li, Binlin Yi, and Zhi
    Huang. Lane-change behavior decision-making of intelligent vehicle based on imitation
    learning and reinforcement learning(in chinese). Automotive Engineering, 43(1):59–67,
    2021.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] Xiaolin Song, Xin Sheng, Haotian Cao, Mingjun Li, Binlin Yi 和 Zhi Huang.
    基于模仿学习和强化学习的智能车辆换道行为决策（中文）。《汽车工程》，43(1):59–67，2021年。'
- en: '[178] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan,
    Evangelos Theodorou, and Byron Boots. Agile autonomous driving using end-to-end
    deep imitation learning. arXiv preprint arXiv:1709.07174, 2017.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] Yunpeng Pan, Ching-An Cheng, Kamil Saigol, Keuntaek Lee, Xinyan Yan,
    Evangelos Theodorou 和 Byron Boots. 使用端到端深度模仿学习的灵活自主驾驶。arXiv预印本 arXiv:1709.07174，2017年。'
- en: '[179] Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy Vorobeychik,
    and Xuan Zhang. Attacking vision-based perception in end-to-end autonomous driving
    models. Journal of Systems Architecture, 110:101766, 2020.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] Adith Boloor, Karthik Garimella, Xin He, Christopher Gill, Yevgeniy Vorobeychik
    和 Xuan Zhang. 攻击端到端自主驾驶模型中的基于视觉的感知。系统架构杂志，110:101766，2020年。'
- en: '[180] Jinghan Yang, Adith Boloor, Ayan Chakrabarti, Xuan Zhang, and Yevgeniy
    Vorobeychik. Finding physical adversarial examples for autonomous driving with
    fast and differentiable image compositing. arXiv preprint arXiv:2010.08844, 2020.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] Jinghan Yang, Adith Boloor, Ayan Chakrabarti, Xuan Zhang 和 Yevgeniy Vorobeychik.
    通过快速且可微分的图像合成寻找自主驾驶的物理对抗样本。arXiv预印本 arXiv:2010.08844，2020年。'
- en: '[181] Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala
    Al-Fuqaha, Dinh Thai Huang, and Dusit Niyato. Challenges and countermeasures for
    adversarial attacks on deep reinforcement learning. IEEE Transactions on Artificial
    Intelligence, 2021.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] Inaam Ilahi, Muhammad Usama, Junaid Qadir, Muhammad Umar Janjua, Ala
    Al-Fuqaha, Dinh Thai Huang 和 Dusit Niyato. 对深度强化学习中的对抗攻击的挑战和对策。IEEE人工智能学报，2021年。'
- en: '[182] Jinyin Chen, Yan Zhang, Wang Xueke, Cai Hongbin, Jue Wang, and Shouling
    Ji. A survey of attack, defense and related security analysis for deep reinforcement
    learning(in chinese). Acta Automatica Sinica, 48(1):21–39, 2022.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] Jinyin Chen, Yan Zhang, Wang Xueke, Cai Hongbin, Jue Wang 和 Shouling
    Ji. 深度强化学习的攻击、防御及相关安全分析综述（中文）。《自动化学报》，48(1):21–39，2022年。'
- en: '[183] Panagiota Kiourti, Kacper Wardega, Susmit Jha, and Wenchao Li. Trojdrl:
    evaluation of backdoor attacks on deep reinforcement learning. In 2020 57th ACM/IEEE
    Design Automation Conference (DAC), pages 1–6\. IEEE, 2020.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] Panagiota Kiourti, Kacper Wardega, Susmit Jha 和 Wenchao Li. Trojdrl：对深度强化学习中后门攻击的评估。发表于2020年第57届ACM/IEEE设计自动化会议（DAC），第1–6页。IEEE，2020年。'
- en: '[184] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan, and Pieter Abbeel.
    Adversarial attacks on neural network policies. arXiv preprint arXiv:1702.02284,
    2017.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] Sandy Huang, Nicolas Papernot, Ian Goodfellow, Yan Duan 和 Pieter Abbeel.
    对神经网络策略的对抗攻击。arXiv预印本 arXiv:1702.02284，2017年。'
- en: '[185] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu,
    and Min Sun. Tactics of adversarial attack on deep reinforcement learning agents.
    arXiv preprint arXiv:1703.06748, 2017.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] Yen-Chen Lin, Zhang-Wei Hong, Yuan-Hong Liao, Meng-Li Shih, Ming-Yu Liu
    和 Min Sun. 深度强化学习代理的对抗攻击策略。arXiv预印本 arXiv:1703.06748，2017年。'
- en: '[186] Vahid Behzadan and Arslan Munir. Vulnerability of deep reinforcement
    learning to policy induction attacks. In International Conference on Machine Learning
    and Data Mining in Pattern Recognition, pages 262–275\. Springer, 2017.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] Vahid Behzadan 和 Arslan Munir。深度强化学习对策略诱导攻击的脆弱性。在国际机器学习与模式识别数据挖掘会议，第262–275页。Springer，2017年。'
- en: '[187] Chaowei Xiao, Xinlei Pan, Warren He, Jian Peng, Mingjie Sun, Jinfeng
    Yi, Mingyan Liu, Bo Li, and Dawn Song. Characterizing attacks on deep reinforcement
    learning. arXiv preprint arXiv:1907.09470, 2019.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] Chaowei Xiao、Xinlei Pan、Warren He、Jian Peng、Mingjie Sun、Jinfeng Yi、Mingyan
    Liu、Bo Li 和 Dawn Song。深度强化学习中的攻击特征。arXiv预印本arXiv:1907.09470，2019年。'
- en: '[188] Vahid Behzadan and Arslan Munir. Adversarial reinforcement learning framework
    for benchmarking collision avoidance mechanisms in autonomous vehicles. IEEE Intelligent
    Transportation Systems Magazine, 13(2):236–241, 2019.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] Vahid Behzadan 和 Arslan Munir。用于基准测试自主车辆碰撞避免机制的对抗性强化学习框架。IEEE智能交通系统杂志，13(2)：236–241，2019年。'
- en: '[189] Shafkat Islam, Shahriar Badsha, Ibrahim Khalil, Mohammed Atiquzzaman,
    and Charalambos Konstantinou. A triggerless backdoor attack and defense mechanism
    for intelligent task offloading in multi-uav systems. IEEE Internet of Things
    Journal, 2022.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] Shafkat Islam、Shahriar Badsha、Ibrahim Khalil、Mohammed Atiquzzaman 和 Charalambos
    Konstantinou。用于多无人机系统的无触发后门攻击与防御机制。IEEE物联网杂志，2022年。'
- en: '[190] Edgar Tretschk, Seong Joon Oh, and Mario Fritz. Sequential attacks on
    agents for long-term adversarial goals. arXiv preprint arXiv:1805.12487, 2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] Edgar Tretschk、Seong Joon Oh 和 Mario Fritz。针对长期对抗性目标的序列攻击。arXiv预印本arXiv:1805.12487，2018年。'
- en: '[191] Adam Gleave, Michael Dennis, Cody Wild, Neel Kant, Sergey Levine, and
    Stuart Russell. Adversarial policies: Attacking deep reinforcement learning. arXiv
    preprint arXiv:1905.10615, 2019.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] Adam Gleave、Michael Dennis、Cody Wild、Neel Kant、Sergey Levine 和 Stuart
    Russell。对抗性策略：攻击深度强化学习。arXiv预印本arXiv:1905.10615，2019年。'
- en: '[192] Lun Wang, Zaynah Javed, Xian Wu, Wenbo Guo, Xinyu Xing, and Dawn Song.
    Backdoorl: Backdoor attack against competitive reinforcement learning. arXiv preprint
    arXiv:2105.00579, 2021.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] Lun Wang、Zaynah Javed、Xian Wu、Wenbo Guo、Xinyu Xing 和 Dawn Song。Backdoorl：针对竞争性强化学习的后门攻击。arXiv预印本arXiv:2105.00579，2021年。'
- en: '[193] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436–444, 2015.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] Yann LeCun、Yoshua Bengio 和 Geoffrey Hinton。深度学习。自然，521(7553)：436–444，2015年。'
- en: '[194] Yoshua Bengio, Yann Lecun, and Geoffrey Hinton. Deep learning for ai.
    Communications of the ACM, 64(7):58–65, 2021.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] Yoshua Bengio、Yann Lecun 和 Geoffrey Hinton。人工智能的深度学习。ACM通讯，64(7)：58–65，2021年。'
- en: '[195] Bo Zhang, Jun Zhu, and Hang Su. Toward the third generation of artificial
    intelligence (in chinese). Sci Sin Inform, 50(9):1281–1302, 2020.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] Bo Zhang、Jun Zhu 和 Hang Su。迈向第三代人工智能（中文）。科学通报信息，50(9)：1281–1302，2020年。'
- en: '[196] Jakub Konečnỳ, H Brendan McMahan, Daniel Ramage, and Peter Richtárik.
    Federated optimization: Distributed machine learning for on-device intelligence.
    arXiv preprint arXiv:1610.02527, 2016.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] Jakub Konečnỳ、H Brendan McMahan、Daniel Ramage 和 Peter Richtárik。联邦优化：用于设备智能的分布式机器学习。arXiv预印本arXiv:1610.02527，2016年。'
- en: '[197] Jakub Konečnỳ, H Brendan McMahan, Felix X Yu, Peter Richtárik, Ananda Theertha
    Suresh, and Dave Bacon. Federated learning: Strategies for improving communication
    efficiency. arXiv preprint arXiv:1610.05492, 2016.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] Jakub Konečnỳ、H Brendan McMahan、Felix X Yu、Peter Richtárik、Ananda Theertha
    Suresh 和 Dave Bacon。联邦学习：提高通信效率的策略。arXiv预印本arXiv:1610.05492，2016年。'
- en: '[198] H Brendan McMahan, Eider Moore, Daniel Ramage, and Blaise Agüera y Arcas.
    Federated learning of deep networks using model averaging.(2016). arXiv preprint
    arXiv:1602.05629, 2016.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] H Brendan McMahan、Eider Moore、Daniel Ramage 和 Blaise Agüera y Arcas。使用模型平均的深度网络联邦学习（2016年）。arXiv预印本arXiv:1602.05629，2016年。'
- en: '[199] Ahmet M Elbir, Burak Soner, and Sinem Coleri. Federated learning in vehicular
    networks. arXiv preprint arXiv:2006.01412, 2020.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] Ahmet M Elbir、Burak Soner 和 Sinem Coleri。车载网络中的联邦学习。arXiv预印本arXiv:2006.01412，2020年。'
- en: '[200] Hongyi Zhang, Jan Bosch, and Helena Holmström Olsson. Real-time end-to-end
    federated learning: An automotive case study. In 2021 IEEE 45th Annual Computers,
    Software, and Applications Conference (COMPSAC), pages 459–468\. IEEE, 2021.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] 张宏义、Jan Bosch 和 Helena Holmström Olsson。实时端到端联邦学习：一个汽车案例研究。在2021年IEEE第45届年度计算机、软件和应用会议（COMPSAC），第459–468页。IEEE，2021年。'
- en: '[201] Lingjuan Lyu, Han Yu, and Qiang Yang. Threats to federated learning:
    A survey. arXiv preprint arXiv:2003.02133, 2020.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] Lingjuan Lyu、Han Yu 和 Qiang Yang。联邦学习的威胁：综述。arXiv预印本arXiv:2003.02133，2020年。'
- en: '[202] Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali
    Dehghantanha, and Gautam Srivastava. A survey on security and privacy of federated
    learning. Future Generation Computer Systems, 115:619–640, 2021.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] Viraaji Mothukuri, Reza M Parizi, Seyedamin Pouriyeh, Yan Huang, Ali
    Dehghantanha 和 Gautam Srivastava. 关于联邦学习的安全性与隐私的调查。未来计算机系统，115:619–640，2021。'
- en: '[203] Xuefei Yin, Yanming Zhu, and Jiankun Hu. A comprehensive survey of privacy-preserving
    federated learning: A taxonomy, review, and future directions. ACM Computing Surveys
    (CSUR), 54(6):1–36, 2021.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Xuefei Yin, Yanming Zhu 和 Jiankun Hu. 隐私保护联邦学习的综合调查：分类、评述与未来方向。ACM 计算调查
    (CSUR)，54(6):1–36，2021。'
- en: '[204] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui, and Julien Stainer.
    Machine learning with adversaries: Byzantine tolerant gradient descent. Advances
    in Neural Information Processing Systems, 30, 2017.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Peva Blanchard, El Mahdi El Mhamdi, Rachid Guerraoui 和 Julien Stainer.
    与对手的机器学习：拜占庭容忍的梯度下降。神经信息处理系统进展，30，2017。'
- en: '[205] Rachid Guerraoui, Sébastien Rouault, et al. The hidden vulnerability
    of distributed learning in byzantium. In International Conference on Machine Learning,
    pages 3521–3530\. PMLR, 2018.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Rachid Guerraoui, Sébastien Rouault 等。拜占庭中的分布式学习的隐蔽漏洞。在国际机器学习大会，页面 3521–3530。PMLR，2018。'
- en: '[206] Dong Yin, Yudong Chen, Ramchandran Kannan, and Peter Bartlett. Byzantine-robust
    distributed learning: Towards optimal statistical rates. In International Conference
    on Machine Learning, pages 5650–5659\. PMLR, 2018.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] Dong Yin, Yudong Chen, Ramchandran Kannan 和 Peter Bartlett. 拜占庭鲁棒的分布式学习：迈向最优统计率。在国际机器学习大会，页面
    5650–5659。PMLR，2018。'
- en: '[207] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly
    Shmatikov. How to backdoor federated learning. In International Conference on
    Artificial Intelligence and Statistics, pages 2938–2948\. PMLR, 2020.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin 和 Vitaly
    Shmatikov. 如何在联邦学习中植入后门。在人工智能与统计国际会议，页面 2938–2948。PMLR，2020。'
- en: '[208] Chulin Xie, Keli Huang, Pin-Yu Chen, and Bo Li. Dba: Distributed backdoor
    attacks against federated learning. In International Conference on Learning Representations,
    2019.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Chulin Xie, Keli Huang, Pin-Yu Chen 和 Bo Li. DBA：针对联邦学习的分布式后门攻击。在国际学习表示大会，2019。'
- en: '[209] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
    Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
    Rachel Cummings, et al. Advances and open problems in federated learning. Foundations
    and Trends® in Machine Learning, 14(1–2):1–210, 2021.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
    Bennis, Arjun Nitin Bhagoji, Kallista Bonawitz, Zachary Charles, Graham Cormode,
    Rachel Cummings 等。联邦学习的进展与未解决的问题。机器学习基础与趋势®，14(1–2):1–210，2021。'
- en: '[210] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge, and Michael Moeller.
    Inverting gradients-how easy is it to break privacy in federated learning? Advances
    in Neural Information Processing Systems, 33:16937–16947, 2020.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] Jonas Geiping, Hartmut Bauermeister, Hannah Dröge 和 Michael Moeller.
    反转梯度——在联邦学习中破坏隐私有多简单？神经信息处理系统进展，33:16937–16947，2020。'
- en: '[211] Mohammad Al-Rubaie and J Morris Chang. Privacy-preserving machine learning:
    Threats and solutions. IEEE Security & Privacy, 17(2):49–58, 2019.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] Mohammad Al-Rubaie 和 J Morris Chang. 隐私保护机器学习：威胁与解决方案。IEEE 安全与隐私，17(2):49–58，2019。'
- en: '[212] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu,
    and Bingsheng He. A survey on federated learning systems: vision, hype and reality
    for data privacy and protection. IEEE Transactions on Knowledge and Data Engineering,
    2021.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] Qinbin Li, Zeyi Wen, Zhaomin Wu, Sixu Hu, Naibo Wang, Yuan Li, Xu Liu
    和 Bingsheng He. 关于联邦学习系统的调查：数据隐私和保护的愿景、炒作与现实。IEEE 知识与数据工程汇刊，2021。'
- en: '[213] Chunyi Zhou, Dawei Chen, Shan Wang, Anmin Fu, and Yansong Gao. Research
    and challenge of distributed deep learning privacy and security attack(in chinese).
    Journal of Computer Research and Development, 58(5):927, 2021.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] Chunyi Zhou, Dawei Chen, Shan Wang, Anmin Fu 和 Yansong Gao. 分布式深度学习隐私和安全攻击的研究与挑战（中文）。计算机研究与发展杂志，58(5):927，2021。'
- en: '[214] Junxu Liu and Xiaofeng Meng. Survey on privacy-preserving machine learning(in
    chinese). Journal of Computer Research and Development, 57(2):346, 2020.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] Junxu Liu 和 Xiaofeng Meng. 隐私保护机器学习调查（中文）。计算机研究与发展杂志，57(2):346，2020。'
- en: '[215] Xi Wu, Matthew Fredrikson, Somesh Jha, and Jeffrey F Naughton. A methodology
    for formalizing model-inversion attacks. In 2016 IEEE 29th Computer Security Foundations
    Symposium (CSF), pages 355–370\. IEEE, 2016.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] Xi Wu, Matthew Fredrikson, Somesh Jha 和 Jeffrey F Naughton. 形式化模型反演攻击的方法论。在2016
    IEEE第29届计算机安全基础研讨会 (CSF)，页面 355–370。IEEE，2016。'
- en: '[216] Briland Hitaj, Giuseppe Ateniese, and Fernando Perez-Cruz. Deep models
    under the gan: information leakage from collaborative deep learning. In Proceedings
    of the 2017 ACM SIGSAC conference on computer and communications security, pages
    603–618, 2017.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] Briland Hitaj, Giuseppe Ateniese, 和 Fernando Perez-Cruz。深度模型下的GAN：来自协作深度学习的信息泄露。发表于2017年ACM
    SIGSAC计算机与通信安全会议论文集，第603–618页，2017年。'
- en: '[217] Guangcan Mai, Kai Cao, Pong C Yuen, and Anil K Jain. On the reconstruction
    of face images from deep face templates. IEEE transactions on pattern analysis
    and machine intelligence, 41(5):1188–1202, 2018.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] Guangcan Mai, Kai Cao, Pong C Yuen, 和 Anil K Jain。基于深度人脸模板的面部图像重建。IEEE模式分析与机器智能学报，41(5)：1188–1202，2018年。'
- en: '[218] Anton Razzhigaev, Klim Kireev, Edgar Kaziakhmedov, Nurislam Tursynbek,
    and Aleksandr Petiushko. Black-box face recovery from identity features. In European
    Conference on Computer Vision, pages 462–475. Springer, 2020.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] Anton Razzhigaev, Klim Kireev, Edgar Kaziakhmedov, Nurislam Tursynbek,
    和 Aleksandr Petiushko。基于身份特征的黑盒面部恢复。发表于欧洲计算机视觉会议论文集，第462–475页。Springer，2020年。'
- en: '[219] Anton Razzhigaev, Klim Kireev, Igor Udovichenko, and Aleksandr Petiushko.
    Darker than black-box: Face reconstruction from similarity queries. arXiv preprint
    arXiv:2106.14290, 2021.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] Anton Razzhigaev, Klim Kireev, Igor Udovichenko, 和 Aleksandr Petiushko。比黑盒更黑暗：从相似性查询中重建面部。arXiv预印本arXiv:2106.14290，2021年。'
- en: '[220] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu,
    and Xuyun Zhang. Membership inference attacks on machine learning: A survey. ACM
    Computing Surveys (CSUR), 2021.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, Philip S Yu,
    和 Xuyun Zhang。机器学习中的成员推断攻击：综述。ACM计算机调查（CSUR），2021年。'
- en: '[221] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, and Somesh Jha. Privacy
    risk in machine learning: Analyzing the connection to overfitting. In 2018 IEEE
    31st computer security foundations symposium (CSF), pages 268–282\. IEEE, 2018.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] Samuel Yeom, Irene Giacomelli, Matt Fredrikson, 和 Somesh Jha。机器学习中的隐私风险：分析与过拟合的关联。发表于2018年IEEE第31届计算机安全基础研讨会（CSF），第268–282页。IEEE，2018年。'
- en: '[222] Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, and Michael Backes.
    Ml-leaks: Model and data independent membership inference attacks and defenses
    on machine learning models. In Network and Distributed Systems Security Symposium
    2019. Internet Society, 2019.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] Ahmed Salem, Yang Zhang, Mathias Humbert, Mario Fritz, 和 Michael Backes。ML-leaks：模型和数据独立的成员推断攻击与防御。发表于2019年网络与分布式系统安全研讨会。互联网协会，2019年。'
- en: '[223] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier,
    and Hervé Jégou. White-box vs black-box: Bayes optimal strategies for membership
    inference. In International Conference on Machine Learning, pages 5558–5567\.
    PMLR, 2019.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Alexandre Sablayrolles, Matthijs Douze, Cordelia Schmid, Yann Ollivier,
    和 Hervé Jégou。白盒与黑盒：成员推断的贝叶斯最优策略。发表于国际机器学习会议论文集，第5558–5567页。PMLR，2019年。'
- en: '[224] Minxing Zhang, Zhaochun Ren, Zihan Wang, Pengjie Ren, Zhunmin Chen, Pengfei
    Hu, and Yang Zhang. Membership inference attacks against recommender systems.
    In Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications
    Security, pages 864–879, 2021.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Minxing Zhang, Zhaochun Ren, Zihan Wang, Pengjie Ren, Zhunmin Chen, Pengfei
    Hu, 和 Yang Zhang。针对推荐系统的成员推断攻击。发表于2021年ACM SIGSAC计算机与通信安全会议论文集，第864–879页，2021年。'
- en: '[225] Bo Hui, Yuchen Yang, Haolin Yuan, Philippe Burlina, Neil Zhenqiang Gong,
    and Yinzhi Cao. Practical blind membership inference attack via differential comparisons.
    In ISOC Network and Distributed System Security Symposium (NDSS), 2021.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] Bo Hui, Yuchen Yang, Haolin Yuan, Philippe Burlina, Neil Zhenqiang Gong,
    和 Yinzhi Cao。通过差分比较进行实用的盲成员推断攻击。发表于ISOC网络与分布式系统安全研讨会（NDSS），2021年。'
- en: '[226] Milad Nasr, Reza Shokri, and Amir Houmansadr. Comprehensive privacy analysis
    of deep learning: Passive and active white-box inference attacks against centralized
    and federated learning. In 2019 IEEE symposium on security and privacy (SP), pages
    739–753\. IEEE, 2019.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] Milad Nasr, Reza Shokri, 和 Amir Houmansadr。深度学习的全面隐私分析：对集中式和联邦学习的被动和主动白盒推断攻击。发表于2019年IEEE安全与隐私研讨会（SP），第739–753页。IEEE，2019年。'
- en: '[227] Jiale Chen, Jiale Zhang, Yanchao Zhao, Hao Han, Kun Zhu, and Bing Chen.
    Beyond model-level membership privacy leakage: an adversarial approach in federated
    learning. In 2020 29th International Conference on Computer Communications and
    Networks (ICCCN), pages 1–9\. IEEE, 2020.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] Jiale Chen, Jiale Zhang, Yanchao Zhao, Hao Han, Kun Zhu, 和 Bing Chen。超越模型级成员隐私泄露：联邦学习中的对抗性方法。发表于2020年第29届国际计算机通信与网络会议（ICCCN），第1–9页。IEEE，2020年。'
- en: '[228] Hongsheng Hu, Zoran Salcic, Lichao Sun, Gillian Dobbie, and Xuyun Zhang.
    Source inference attacks in federated learning. In 2021 IEEE International Conference
    on Data Mining (ICDM), pages 1102–1107\. IEEE, 2021.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] 洪生胡、佐兰·萨尔齐奇、李超·孙、吉莲·多比和徐云张。联邦学习中的源推断攻击。载于2021年IEEE数据挖掘国际会议（ICDM），第1102–1107页。IEEE,
    2021年。'
- en: '[229] Umang Gupta, Dimitris Stripelis, Pradeep K Lam, Paul Thompson, José Luis
    Ambite, and Greg Ver Steeg. Membership inference attacks on deep regression models
    for neuroimaging. In Medical Imaging with Deep Learning, pages 228–251\. PMLR,
    2021.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] 乌芒·古普塔、迪米特里斯·斯特里佩利斯、普拉迪普·K·拉姆、保罗·汤普森、何塞·路易斯·安比特和格雷格·弗尔斯蒂格。对神经影像深度回归模型的成员推断攻击。载于《深度学习医学影像》，第228–251页。PMLR,
    2021年。'
- en: '[230] Anh Nguyen, Tuong Do, Minh Tran, Binh X Nguyen, Chien Duong, Tu Phan,
    Erman Tjiputra, and Quang D Tran. Deep federated learning for autonomous driving.
    arXiv preprint arXiv:2110.05754, 2021.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] 安·阮、杜翁·多、敏·陈、宾·X·阮、前·杜翁、图·潘、厄尔曼·吉普特拉和光·D·陈。深度联邦学习用于自动驾驶。arXiv预印本 arXiv:2110.05754，2021年。'
- en: '[231] Davinder Kaur, Suleyman Uslu, Kaley J Rittichier, and Arjan Durresi.
    Trustworthy artificial intelligence: A review. ACM Computing Surveys (CSUR), 55(2):1–38,
    2022.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] 达文德·考尔、苏莱曼·乌斯卢、凯莉·J·里蒂歇尔和阿尔詹·杜雷西。值得信赖的人工智能：综述。《ACM计算机调查（CSUR）》, 55(2):1–38,
    2022年。'
- en: '[232] Luciano Floridi. Establishing the rules for building trustworthy ai.
    Nature Machine Intelligence, 1(6):261–262, 2019.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] 卢西亚诺·弗洛里迪。建立构建值得信赖的人工智能的规则。《自然机器智能》，1(6):261–262, 2019年。'
- en: '[233] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu,
    and Jianguo Li. Boosting adversarial attacks with momentum. In Proceedings of
    the IEEE conference on computer vision and pattern recognition, pages 9185–9193,
    2018.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] 尹鹏董、方舟廖、天宇庞、杭苏、俊朱、肖林胡和建国李。通过动量提升对抗攻击。载于《IEEE计算机视觉与模式识别会议论文集》，第9185–9193页，2018年。'
- en: '[234] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras,
    and Adrian Vladu. Towards deep learning models resistant to adversarial attacks.
    arXiv preprint arXiv:1706.06083, 2017.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] 亚历山大·马德里、亚历山大·马凯洛夫、路德维希·施密特、迪米特里斯·齐普拉斯和阿德里安·弗拉杜。朝着对抗攻击抗性的深度学习模型迈进。arXiv预印本
    arXiv:1706.06083，2017年。'
- en: '[235] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren,
    and Alan L Yuille. Improving transferability of adversarial examples with input
    diversity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
    Recognition, pages 2730–2739, 2019.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] 谷城谢、志帅张、宇音周、宋白、建宇王、周仁和艾伦·L·尤伊尔。通过输入多样性提高对抗样本的迁移性。载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第2730–2739页，2019年。'
- en: '[236] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to
    transferable adversarial examples by translation-invariant attacks. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4312–4321,
    2019.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] 尹鹏董、天宇庞、杭苏和俊朱。通过平移不变攻击规避对可转移对抗样本的防御。载于《IEEE/CVF计算机视觉与模式识别会议论文集》，第4312–4321页，2019年。'
- en: '[237] Anish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients
    give a false sense of security: Circumventing defenses to adversarial examples.
    In International conference on machine learning, pages 274–283\. PMLR, 2018.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] 阿尼什·阿塔利、尼古拉斯·卡尔尼和大卫·瓦格纳。模糊梯度带来虚假的安全感：规避对抗样本的防御。载于国际机器学习会议论文集，第274–283页。PMLR,
    2018年。'
- en: '[238] Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, and Cho-Jui Hsieh.
    Zoo: Zeroth order optimization based black-box attacks to deep neural networks
    without training substitute models. In Proceedings of the 10th ACM workshop on
    artificial intelligence and security, pages 15–26, 2017.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] 钟玉陈、环张、雅什·夏尔马、金锋·易和卓睿·谢。动物园：基于零阶优化的黑盒攻击，无需训练替代模型的深度神经网络。载于第10届ACM人工智能与安全研讨会论文集，第15–26页，2017年。'
- en: '[239] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box
    adversarial attacks with limited queries and information. In International Conference
    on Machine Learning, pages 2137–2146\. PMLR, 2018.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] 安德鲁·伊利亚斯、洛根·恩格斯特罗姆、阿尼什·阿塔利和杰西·林。具有有限查询和信息的黑盒对抗攻击。载于国际机器学习会议论文集，第2137–2146页。PMLR,
    2018年。'
- en: '[240] Jonathan Uesato, Brendan O’donoghue, Pushmeet Kohli, and Aaron Oord.
    Adversarial risk and the dangers of evaluating against weak attacks. In International
    Conference on Machine Learning, pages 5025–5034\. PMLR, 2018.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] 乔纳森·乌萨托、布伦丹·奥多诺霍、普什米特·科利和亚伦·奥尔德。对抗风险及评估弱攻击的危险。载于国际机器学习会议论文集，第5025–5034页。PMLR,
    2018年。'
- en: '[241] Yandong Li, Lijun Li, Liqiang Wang, Tong Zhang, and Boqing Gong. Nattack:
    Learning the distributions of adversarial examples for an improved black-box attack
    on deep neural networks. In International Conference on Machine Learning, pages
    3866–3876\. PMLR, 2019.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] 李延东、李立军、王力强、张同、龚博青。《Nattack：学习对抗样本的分布以改进黑箱攻击深度神经网络》。在国际机器学习大会，页3866–3876。PMLR，2019年。'
- en: '[242] Wieland Brendel, Jonas Rauber, and Matthias Bethge. Decision-based adversarial
    attacks: Reliable attacks against black-box machine learning models. arXiv preprint
    arXiv:1712.04248, 2017.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] Wieland Brendel、Jonas Rauber和Matthias Bethge。《基于决策的对抗攻击：针对黑箱机器学习模型的可靠攻击》。arXiv预印本arXiv:1712.04248，2017年。'
- en: '[243] Yinpeng Dong, Hang Su, Baoyuan Wu, Zhifeng Li, Wei Liu, Tong Zhang, and
    Jun Zhu. Efficient decision-based black-box adversarial attacks on face recognition.
    In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
    pages 7714–7722, 2019.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] 董寅鹏、苏航、吴宝源、李志峰、刘伟、张同和朱俊。《高效的基于决策的黑箱对抗攻击在人脸识别中的应用》。在IEEE/CVF计算机视觉与模式识别大会论文集，页7714–7722，2019年。'
