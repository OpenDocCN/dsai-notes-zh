- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:42:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:42:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2301.09362] A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2301.09362] 《深度学习时代的心音分析全面调查》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2301.09362](https://ar5iv.labs.arxiv.org/html/2301.09362)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2301.09362](https://ar5iv.labs.arxiv.org/html/2301.09362)
- en: A Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深度学习时代的心音分析全面调查》
- en: 'Zhao Ren    \IEEEmembershipMember, IEEE    Yi Chang    Thanh Tam Nguyen   
    Yang Tan    Kun Qian    \IEEEmembershipSenior Member, IEEE    and Björn W. Schuller
       \IEEEmembershipFellow, IEEE This research was funded in part by the Federal
    Ministry of Education and Research (BMBF), Germany under the project LeibnizKILabor
    with grant No. 01DD20003, the Ministry of Science and Technology of the People’s
    Republic of China with the STI2030-Major Projects (No. 2021ZD0201900), and the
    National Natural Science Foundation of China (No. 62272044), and the Teli Young
    Fellow Program from the Beijing Institute of Technology, China. (*Corresponding
    authors*: Zhao Ren and Kun Qian.)Z. Ren is with the L3S Research Center, Leibniz
    University Hannover, Germany (zren@l3s.de).Y. Chang and B. W. Schuller are with
    the GLAM – the Group on Language, Audio, & Music, Imperial College London, United
    Kingdom (y.chang20@imperial.ac.uk, schuller@ieee.org).T. T. Nguyen is with the
    Griffith University, Australia (t.nguyen19@griffith.edu.au).Y. Tan and K. Qian
    are with the School of Medical Technology, Beijing Institute of Technology, China
    (qian@bit.edu.cn).B. W. Schuller is also with the Chair of Embedded Intelligence
    for Health Care and Wellbeing, University of Augsburg, Germany.This work has been
    submitted to the IEEE for possible publication. Copyright may be transferred without
    notice, after which this version may no longer be accessible.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao Ren    \IEEEmembershipMember, IEEE    Yi Chang    Thanh Tam Nguyen    Yang
    Tan    Kun Qian    \IEEEmembershipSenior Member, IEEE    和 Björn W. Schuller   
    \IEEEmembershipFellow, IEEE 本研究部分由德国联邦教育和研究部（BMBF）资助，项目名称为 LeibnizKILabor，资助编号为01DD20003；中华人民共和国科学技术部资助了
    STI2030-重大项目（编号：2021ZD0201900）；中国国家自然科学基金（编号：62272044）；以及北京理工大学的 Teli Young Fellow
    Program。(*通讯作者*：Zhao Ren 和 Kun Qian.) Z. Ren 现为德国汉诺威大学 L3S 研究中心成员（zren@l3s.de）。Y.
    Chang 和 B. W. Schuller 现为英国帝国理工学院 GLAM 语言、音频与音乐组成员（y.chang20@imperial.ac.uk, schuller@ieee.org）。T.
    T. Nguyen 现为澳大利亚格里菲斯大学成员（t.nguyen19@griffith.edu.au）。Y. Tan 和 K. Qian 现为中国北京理工大学医药技术学院成员（qian@bit.edu.cn）。B.
    W. Schuller 还担任德国奥格斯堡大学健康与福祉嵌入式智能主席。本工作已提交给 IEEE 以待可能的出版。版权可能在未经通知的情况下转让，转让后此版本可能不再可访问。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Heart sound auscultation has been demonstrated to be beneficial in clinical
    usage for early screening of cardiovascular diseases. Due to the high requirement
    of well-trained professionals for auscultation, automatic auscultation benefiting
    from signal processing and machine learning can help auxiliary diagnosis and reduce
    the burdens of training professional clinicians. Nevertheless, classic machine
    learning is limited to performance improvement in the era of big data. Deep learning
    has achieved better performance than classic machine learning in many research
    fields, as it employs more complex model architectures with stronger capability
    of extracting effective representations. Deep learning has been successfully applied
    to heart sound analysis in the past years. As most review works about heart sound
    analysis were given before 2017, the present survey is the first to work on a
    comprehensive overview to summarise papers on heart sound analysis with deep learning
    in the past six years 2017–2022\. We introduce both classic machine learning and
    deep learning for comparison, and further offer insights about the advances and
    future research directions in deep learning for heart sound analysis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 心音听诊已被证明对早期筛查心血管疾病具有临床价值。由于听诊对专业人员的要求很高，依托信号处理和机器学习的自动听诊可以辅助诊断并减少培训专业医生的负担。然而，经典机器学习在大数据时代的性能提升有限。深度学习在许多研究领域中已表现出比经典机器学习更好的性能，因为它采用了更复杂的模型架构，具有更强的有效表征提取能力。近年来，深度学习已成功应用于心音分析。由于大多数关于心音分析的综述工作在2017年之前完成，本综述首次全面总结了2017年至2022年间深度学习在心音分析中的论文。我们介绍了经典机器学习和深度学习的对比，并进一步提供了深度学习在心音分析中的进展和未来研究方向的见解。
- en: '{IEEEkeywords}'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{IEEEkeywords}'
- en: Automated auscultation, computer audition, deep learning, heart sounds, digital
    health
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自动听诊、计算机听音、深度学习、心音、数字健康
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: \IEEEPARstart
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \IEEEPARstart
- en: Cardiac auscultation, i. e., listening and interpreting the heart sound, is
    an indispensable and critical part for the clinical examination of the patient [[1](#bib.bib1)].
    As a low-cost and non-invasive examination, cardiac auscultation is invaluable
    for the presence of a heart disease and providing an estimate of its severity,
    evolution, and prognosis [[2](#bib.bib2)]. Accurate cardiac auscultation may determine
    whether more expensive examination should be planed [[2](#bib.bib2)]. Nevertheless,
    due to difficulties in diagnosing diastolic murmurs, the overall sensitivity of
    cardiac auscultation is poor (i. e., ranging from 0.21 to 1.00) [[1](#bib.bib1)].
    Moreover, cardiac auscultation depends on the physician’s skills which have been
    shown decreased over time [[3](#bib.bib3)]. Poor cardiac auscultation skills may
    miss significant pathology, causing worsening condition, and may over diagnose
    pathology, leading to inappropriate referral for expensive echocardiography [[1](#bib.bib1)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 心脏听诊，即听取和解读心音，是临床检查患者的重要且不可或缺的部分 [[1](#bib.bib1)]。作为一种低成本且非侵入性的检查方法，心脏听诊在发现心脏疾病以及提供其严重程度、发展和预后的估计方面具有重要价值
    [[2](#bib.bib2)]。准确的心脏听诊可能决定是否需要计划更昂贵的检查 [[2](#bib.bib2)]。然而，由于诊断舒张期杂音的困难，心脏听诊的整体敏感性较差（即，范围从
    0.21 到 1.00） [[1](#bib.bib1)]。此外，心脏听诊依赖于医生的技能，而这些技能随着时间的推移已经显示出下降 [[3](#bib.bib3)]。差的心脏听诊技能可能会遗漏重要的病理信息，导致病情恶化，或过度诊断病理，从而导致不必要的昂贵心脏超声检查
    [[1](#bib.bib1)]。
- en: To solve the above problem of cardiac auscultation, classic machine learning
    (ML) has been widely used for automated heart sound analysis, including denoising,
    segmentation, and classification. For instance, noisy audio clips were detected
    by support vector machines (SVMs) [[4](#bib.bib4)], hidden markov models (HMMs)
    were used for heart sound segmentation [[5](#bib.bib5)], and classifiers like
    SVMs and decision trees were applied to heart sound classification [[6](#bib.bib6),
    [7](#bib.bib7)]. Classic machine learning often takes acoustic features as the
    input, while selecting useful features needs lots of human efforts. Additionally,
    classic machine learning usually performs very well on small-scale data, nevertheless,
    model performance on big data has been a bottleneck of classic machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述心脏听诊问题，经典机器学习（ML）已被广泛用于自动化心音分析，包括去噪、分割和分类。例如，支持向量机（SVMs）用于检测噪声音频片段 [[4](#bib.bib4)]，隐马尔可夫模型（HMMs）用于心音分割
    [[5](#bib.bib5)]，分类器如 SVMs 和决策树用于心音分类 [[6](#bib.bib6), [7](#bib.bib7)]。经典机器学习通常将声学特征作为输入，而选择有用特征需要大量人力。此外，经典机器学习通常在小规模数据上表现良好，但在大数据上的模型性能一直是经典机器学习的瓶颈。
- en: More recently, deep learning (DL) has demonstrated its more powerful capability
    of analysing heart sounds than classic machine learning [[8](#bib.bib8)]. DL models
    are usually fed with either raw audio signals or time-frequency representations
    extracted from heart sounds as the inputs [[9](#bib.bib9), [10](#bib.bib10)],
    therefore improving the efficiency by skipping selecting hand-crafted acoustic
    features. More complex model structures in deep learning models also enhance the
    models’ capability of learning abstract representations from large-scale datasets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习（DL）已展示出比经典机器学习更强大的心音分析能力 [[8](#bib.bib8)]。DL 模型通常以原始音频信号或从心音中提取的时间-频率表示作为输入
    [[9](#bib.bib9), [10](#bib.bib10)]，因此通过跳过手工选择声学特征来提高效率。深度学习模型中的更复杂模型结构也增强了模型从大规模数据集中学习抽象表示的能力。
- en: 'Table 1: A comparison between existing surveys on heart sound analysis.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：现有心音分析调查的比较。
- en: '| Surveys | Deep Learning | Heart sound denoising | Heart sound segmentation
    | Heart sound classification | Heart sound interpretation |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 调查 | 深度学习 | 心音去噪 | 心音分割 | 心音分类 | 心音解释 |'
- en: '| Bhoi et al. [[11](#bib.bib11)] | ✗ | mentioned only | ✓ | ✓ | ✗ |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| Bhoi 等人 [[11](#bib.bib11)] | ✗ | 仅提及 | ✓ | ✓ | ✗ |'
- en: '| Chakrabarti et al. [[12](#bib.bib12)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Chakrabarti 等人 [[12](#bib.bib12)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| Nabih-Ali et al. [[13](#bib.bib13)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Nabih-Ali 等人 [[13](#bib.bib13)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| Clifford et al. [[14](#bib.bib14)] | ✓ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Clifford 等人 [[14](#bib.bib14)] | ✓ | ✓ | ✓ | ✓ | ✗ |'
- en: '| Ghosh et al. [[15](#bib.bib15)] | mentioned only | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
- en: '| Majhi et al. [[16](#bib.bib16)] | – | – | – | ✓ | – |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
- en: '| Dwivedi et al. [[17](#bib.bib17)] | ✗ | ✗ | ✓ | ✓ | ✗ |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
- en: '| This survey | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
- en: 1.1 Differences between this survey and the former ones
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several review research studies on heart sound analysis. Feature extraction
    and basic ML models (e. g., SVMs) were introduced in [[11](#bib.bib11)]. Waveform
    features of heart sounds and shallow artificial neural networks were overviewed
    in [[12](#bib.bib12)]. Heart sound denoising and segmentation were discussed in [[13](#bib.bib13)].
    In [[14](#bib.bib14)], approaches for heart sound classification submitted to
    the PhysioNet challenge 2016 were summarised. The study in [[15](#bib.bib15)]
    summarised approaches used for heart sound analysis, from heart sound segmentation
    and feature extraction to heart sound classification. Approaches for heart sound
    classification were summarised and analysed in [[16](#bib.bib16)]. Different from
    these above studies, we will review all DL technologies on heart sound segmentation
    and classification. Furthermore, the state-of-the-art approaches about interpretability
    of DL models will be summarised and discussed (see [Table 1](#S1.T1 "Table 1 ‣
    1 Introduction ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era")). We will also discuss the potential research problems and future research
    directions in this survey, helping promote research studies in heart sound analysis.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Challenges in Heart Sound Analysis
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many machine learning and deep learning methods have been applied to heart sound
    analysis. However, this research field is still facing many technical challenges.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The first challenge is denoising, which aims to remove the noise from heart
    sounds. As the recording environments can be noisy with environmental noise and
    speech, denoising is an essential pre-processing procedure to improve the audio
    quality for better performance of segmentation and classification. Other pre-processing
    procedures such as separation (from lung sounds) [[18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)] are not introduced here, as they were mostly dealing with signal
    processing methods.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: The second challenge is segmentation that targets on splitting a heart sound
    signal into multiple parts, i. e., cardiac cycles or smaller segments (S1, systole,
    S2, and diastole). Heart sound segmentation is often a pre-processing process
    of heart sound classification.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: The third is classification which predicts the severe level of cardiovascular
    diseases or abnormality of a heart from heart sounds. Heart sound classification
    is helpful for early screening of heart diseases in primary care.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: The final one is explaining DL models for heart sound analysis. DL models are
    black boxes for human due to their complex structures, although they are promising
    in performance improvement for heart sound analysis. As digital health is a sensitive
    domain, explainable DL models are crucial for clinicians to give in-time and suitable
    therapies for patients. Correspondingly, trust from clinicians and patients can
    promote applications of explainable DL models in real life.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一项是解释心音分析中的深度学习模型。由于其复杂的结构，深度学习模型对人类来说是“黑箱”，尽管它们在心音分析中的性能提升方面很有前景。由于数字健康是一个敏感领域，可解释的深度学习模型对于临床医生提供及时和适当的治疗至关重要。因此，临床医生和患者的信任可以促进可解释深度学习模型在现实生活中的应用。
- en: 1.3 Contributions of this survey
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 本调查的贡献
- en: The survey is expected to have the following contributions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查预计将有以下贡献。
- en: '*The first comprehensive survey in heart sound analysis with deep learning.*
    Apart from summarising machine learning techniques for heart sound denoising,
    segmentation, and classification, we review the state-of-the-art deep learning
    topologies for heart sound analysis, especially segmentation and classification.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习在心音分析中的第一次全面调查。* 除了总结心音去噪、分割和分类的机器学习技术外，我们还回顾了心音分析的最先进深度学习拓扑，特别是分割和分类。'
- en: '*Summarisation of resources.* We summarise publicly available datasets for
    heart sound analysis, particularly classification. We also provide the collection
    of open-sourced deep learning algorithms for heart sound classification, and discuss
    possible evaluation metrics.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*资源总结。* 我们总结了用于心音分析的公开数据集，特别是分类数据集。我们还提供了开源深度学习算法的汇总，并讨论了可能的评估指标。'
- en: '*Future research directions.* We discuss the limitation of current deep learning
    methods for heart sound classification, and point out potential future research
    topics in this area. We also discuss the importance of explainable DL models for
    heart sound classification, current advances, and future directions in explainable
    AI.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*未来研究方向。* 我们讨论了当前心音分类深度学习方法的局限性，并指出了该领域潜在的未来研究课题。我们还讨论了可解释的深度学习模型在心音分类中的重要性、当前进展以及未来方向。'
- en: 2 Background
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: '![Refer to caption](img/986c97cc8039efeb6b3b45b70aea4450.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/986c97cc8039efeb6b3b45b70aea4450.png)'
- en: 'Figure 1: The framework of heart sound analysis. After denoising and segmentation,
    a classifier is trained to produce the prediction and the corresponding interpretation
    for users (clinicians and patients). The ✓ is a normal prediction, and the ✗ means
    the prediction is abnormal. The dashes ‘- - -’ denote optional procedures.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：心音分析框架。经过去噪和分割后，训练一个分类器来生成预测结果及相应的解释（供临床医生和患者使用）。✓ 表示正常预测，而 ✗ 表示预测异常。破折号‘- - -’表示可选程序。
- en: 2.1 Heart Sounds
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 心音
- en: 'In a human’s cardiac system, a normal cardiac cycle contains two heart sounds:
    the first heart sound S1 and the second heart sound S2\. Additional sounds indicate
    diseases: a presented third heart sound S3 could be a sign of heart failure; a
    murmur could indicate defective valves or an orifice in the septal wall [[21](#bib.bib21)].
    The frequency range of S1 and S2 is 20–200 Hz, while the frequency of S3 and S4
    ranges in 15–65 Hz [[22](#bib.bib22)].'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在人类心脏系统中，一个正常的心动周期包含两个心音：第一心音 S1 和第二心音 S2。额外的声音表示疾病：出现的第三心音 S3 可能是心力衰竭的迹象；杂音可能表明瓣膜缺陷或隔膜上的孔洞[[21](#bib.bib21)]。S1
    和 S2 的频率范围为 20–200 Hz，而 S3 和 S4 的频率范围为 15–65 Hz[[22](#bib.bib22)]。
- en: 'As an example, murmurs, caused by the turbulent blood flow in the heart system,
    are identified as abnormal sounds. They are very important for verifying the timing
    and pitch for diagnosing cardiovascular diseases [[3](#bib.bib3)]. Murmurs often
    constitute the only basis for diagnosing valvular heart disease [[23](#bib.bib23)].
    Clinically, murmurs consists of two types: systolic murmurs and diastolic murmurs.
    Aortic stenosis, mitral regurgitation, and tricuspid regurgitation happen during
    systole; mitral stenosis and tricuspid stenosis occur during diastole [[3](#bib.bib3)].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由心脏系统中的湍流血流引起的杂音被识别为异常声音。它们对于验证心血管疾病的诊断时机和音调非常重要[[3](#bib.bib3)]。杂音往往是诊断瓣膜心脏病的唯一依据[[23](#bib.bib23)]。临床上，杂音包括两种类型：收缩期杂音和舒张期杂音。主动脉狭窄、二尖瓣返流和三尖瓣返流发生在收缩期；二尖瓣狭窄和三尖瓣狭窄发生在舒张期[[3](#bib.bib3)]。
- en: 2.2 Diagnosis of Cardiovascular Diseases
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 心血管疾病的诊断
- en: Nowadays, there are several non-invasive diagnostic tools for cardiovascular
    diseases. Electrocardiogram (ECG), sensing the P-QRS-T wave depicting the electrical
    activity of the heart [[24](#bib.bib24)], is an inexpensive and commonly-used
    tool for the screening of heart diseases. Yet, it has difficulty in detecting
    structural abnormalities in heart valves and defects characterised by heart murmurs [[25](#bib.bib25)].
    Additionally, several medical imaging tools are able to visualise the cardiovascular
    system. For instance, the echocardiogram (echo) is an ultra sound scan to create
    a moving picture of the heart. It can provide information about the heart’s size,
    shape, structure, and function [[26](#bib.bib26)]. Cardiac computed tomography
    (CT) uses x-rays to create detailed pictures of the heart and its blood vessels [[26](#bib.bib26)].
    For assessment of the cardiovascular system’s function and structure, cardiac
    magnetic resonance imaging (CMRI) creates both still and moving pictures of the
    heart and major blood vessels [[26](#bib.bib26)]. However, these above imaging
    instruments are expensive and require medical professionals for operation, thereby
    limiting their application in clinics, and small and medium hospitals.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有几种非侵入性心血管疾病诊断工具。心电图（ECG）通过检测P-QRS-T波来描绘心脏的电活动 [[24](#bib.bib24)]，是一种便宜且常用的心脏疾病筛查工具。然而，它在检测心脏瓣膜结构异常和以心杂音为特征的缺陷方面存在困难 [[25](#bib.bib25)]。此外，一些医学影像工具能够可视化心血管系统。例如，超声心动图（echo）是一种超声扫描，可以创建心脏的动态图像。它可以提供有关心脏的大小、形状、结构和功能的信息 [[26](#bib.bib26)]。心脏计算机断层扫描（CT）使用X射线创建心脏及其血管的详细图像 [[26](#bib.bib26)]。心脏磁共振成像（CMRI）通过创建静态和动态图像来评估心脏和主要血管的功能和结构 [[26](#bib.bib26)]。然而，上述影像仪器价格昂贵且需要医疗专业人员操作，从而限制了其在诊所以及中小型医院中的应用。
- en: Compared to the above diagnostic instruments, cardiac auscultation is low-cost
    and essential in preliminary physical examinations. Phonocardiogram (PCG) signals
    recorded with a phonocardiograph have proven to be important in pediatric cardiology,
    cardiology, and internal diseases [[27](#bib.bib27)]. Recent advances of electronic
    stethoscopes facilitated computer-aided auscultation by integrating sensor design,
    signal processing, and machine learning techniques [[27](#bib.bib27)]. The low-cost
    and portable advantage of electronic stethoscopes make it possible to apply computer-aided
    auscultation to primary care and remote/home health care. Fig.[1](#S2.F1 "Figure
    1 ‣ 2 Background ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep
    Learning Era") depicts a pipeline of heart sound analysis. Heart sounds are processed
    by de-noising, segmentation, and classification, and then clinicians and patients
    receive the predictions and interpretations in primary care. In real-life, patients
    with heart sounds predicted as abnormal will be suggested further professional
    medical examinations for accurate diagnosis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于上述诊断仪器，心脏听诊成本低且在初步体检中至关重要。用心音图（PCG）记录的心音信号已被证明在儿科心脏病学、心脏病学和内科疾病中非常重要 [[27](#bib.bib27)]。电子听诊器的最新进展通过整合传感器设计、信号处理和机器学习技术，促进了计算机辅助听诊 [[27](#bib.bib27)]。电子听诊器的低成本和便携优势使得将计算机辅助听诊应用于基层医疗和远程/居家医疗成为可能。图[1](#S2.F1
    "Figure 1 ‣ 2 Background ‣ A Comprehensive Survey on Heart Sound Analysis in the
    Deep Learning Era") 描绘了心音分析的流程。心音经过去噪、分割和分类处理后，临床医生和患者在基层医疗中接收预测和解释。在现实生活中，心音被预测为异常的患者将建议进行进一步的专业医学检查以获得准确诊断。
- en: 3 Heart Sound Analysis Tasks
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 心音分析任务
- en: '![Refer to caption](img/aa30dba5df380eeba063f5712995e62c.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/aa30dba5df380eeba063f5712995e62c.png)'
- en: 'Figure 2: Categorisation of methods for heart sound analysis. Bold texts are
    deep learning approaches.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：心音分析方法的分类。粗体文本为深度学习方法。
- en: The goal of this section is to describe the tasks and summarise classic machine
    learning techniques for each problem. Specifically, denoising and segmentation
    are two pre-processing steps for heart sound classification in many research studies.
    In Fig.[2](#S3.F2 "Figure 2 ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey
    on Heart Sound Analysis in the Deep Learning Era"), we summarise approaches used
    for the four tasks during 2017–2022. We can see that, classic machine learning
    was still used in the past years, and deep learning was mainly applied to segmentation
    and classification, as well as interpretation methods. For comparison of machine
    learning and deep learning used for heart sound analysis, this section introduce
    the tasks and classic machine learning methods, and deep learning will be discussed
    in [section 4](#S4 "4 State-of-the-art Studies ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era"). As there are only a few works for interpretation,
    we will discuss heart sound interpretation in the discussion part of this survey (see [section 6](#S6
    "6 Future Research Directions and Open Issues ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era")). Therefore, we describe the three tasks
    (de-noising, segmentation, and classification) as independent sections in the
    following for detailed discussion.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是描述任务并总结每个问题的经典机器学习技术。具体而言，去噪和分割是许多研究中用于心音分类的两个预处理步骤。在 图。[2](#S3.F2 "Figure
    2 ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")中，我们总结了2017–2022年期间用于四个任务的方法。我们可以看到，经典机器学习在过去几年仍被使用，而深度学习主要应用于分割和分类，以及解释方法。为了比较心音分析中使用的机器学习和深度学习，本节介绍了任务和经典机器学习方法，深度学习将在[第4节](#S4
    "4 State-of-the-art Studies ‣ A Comprehensive Survey on Heart Sound Analysis in
    the Deep Learning Era")中讨论。由于解释方面的工作较少，我们将在本调查的讨论部分讨论心音解释（参见[第6节](#S6 "6 Future
    Research Directions and Open Issues ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")）。因此，我们将以下三个任务（去噪、分割和分类）描述为独立的部分，以便详细讨论。
- en: 3.1 Denoising
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 去噪
- en: Generally, recorded heart sounds consist of many kinds of noises [[28](#bib.bib28)],
    including white noise and other sounds presented in the recording environments,
    e. g., humans’ speech. Noise may degrade the segmentation and classification performance
    of heart sounds[[28](#bib.bib28)]. In this regard, many studies have investigated
    denoising methods for better performance on the task of heart sound segmentation
    and classification.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，记录的心音包含许多种噪声[[28](#bib.bib28)]，包括白噪声和录音环境中的其他声音，例如人类的语音。噪声可能会降低心音的分割和分类性能[[28](#bib.bib28)]。在这方面，许多研究已经调查了去噪方法，以便在心音分割和分类任务中获得更好的性能。
- en: Filters. As a preprocessing procedure of heart sound classification, many denoising
    approaches employ signal filters for removing noise from noisy heart sounds [[9](#bib.bib9),
    [29](#bib.bib29), [30](#bib.bib30)]. Highpass filters [[31](#bib.bib31), [32](#bib.bib32)]
    have been used to eliminate low-frequency noise. With the capability of mitigating
    both high- and low-frequency noises, bandpass filters are more often used for
    heart sounds denoising [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)].
    A Butterworth bandpass filter has been successfully employed in many studies [[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]. The cutoff frequency of a Butterworth bandpass
    filter is set with a low frequency for filtering out noise with very low frequencies
    and a high frequency for filtering out high-frequency noises. A range of Butterworth
    bandpass filters with various orders have been applied with different cutoff frequency
    settings. For instance, a 4-th order Butterworth filter was set with a cutoff
    frequency of 25-400 Hz in [[44](#bib.bib44)], and a 5-th order Butterworth filter
    was designed to have a cutoff frequency of 25-500 Hz in [[45](#bib.bib45)] and
    25-250 Hz in [[46](#bib.bib46)]. A 6-th order Butterworth filter was designed
    with a cutoff frequency of 50–950 Hz [[47](#bib.bib47), [48](#bib.bib48)] and
    30-900 Hz in [[49](#bib.bib49)]. Additionally, several other filters were also
    used for denoising heart sound, such as a Savitzky–Golay filter [[50](#bib.bib50),
    [51](#bib.bib51)], Chebyshev low-pass filter [[52](#bib.bib52), [53](#bib.bib53)],
    and Notch filter [[54](#bib.bib54)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**滤波器**。作为心音分类的预处理程序，许多去噪方法采用信号滤波器来去除嘈杂心音中的噪声[[9](#bib.bib9), [29](#bib.bib29),
    [30](#bib.bib30)]。高通滤波器[[31](#bib.bib31), [32](#bib.bib32)] 被用来消除低频噪声。带通滤波器具有缓解高频和低频噪声的能力，因此更常用于心音去噪[[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)]。Butterworth带通滤波器在许多研究中取得了成功应用[[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]。Butterworth带通滤波器的截止频率设置为低频，以滤除非常低频的噪声，以及高频，以滤除高频噪声。一系列具有不同阶数的Butterworth带通滤波器已应用于不同的截止频率设置。例如，4阶Butterworth滤波器在[[44](#bib.bib44)]中设置为25-400
    Hz的截止频率，而5阶Butterworth滤波器在[[45](#bib.bib45)]中设计为25-500 Hz的截止频率，在[[46](#bib.bib46)]中设计为25-250
    Hz的截止频率。6阶Butterworth滤波器在[[47](#bib.bib47), [48](#bib.bib48)]中设计为50–950 Hz的截止频率，在[[49](#bib.bib49)]中设计为30-900
    Hz。此外，还使用了其他几种滤波器来去噪心音，如Savitzky–Golay滤波器[[50](#bib.bib50), [51](#bib.bib51)]、Chebyshev低通滤波器[[52](#bib.bib52),
    [53](#bib.bib53)]和Notch滤波器[[54](#bib.bib54)]。'
- en: Spectrum-based denoising. To remove noise, the spectrogram was simply selected
    with a threshold of -30, -45, -60, or -75 dB in [[55](#bib.bib55)]. However, it
    is time-consuming to search a suitable threshold among different heart sounds.
    A more flexible method, spectral subtraction [[56](#bib.bib56)], was used to estimate
    the noise and remove it from the heart sounds [[43](#bib.bib43)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于频谱的去噪**。为了去除噪声，频谱图通常选择-30、-45、-60或-75 dB的阈值[[55](#bib.bib55)]。然而，在不同的心音中寻找合适的阈值是非常耗时的。一种更灵活的方法是频谱减法[[56](#bib.bib56)]，该方法用于估计噪声并从心音中去除它[[43](#bib.bib43)]。'
- en: Spike removal. Frictional spike is a redundant part of the amplitude of a heart
    sound. In several studies [[44](#bib.bib44), [38](#bib.bib38)], frictional spikes
    were detected and removed (i. e., replaced by zeros) during pre-processing of
    heart sounds.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**尖峰去除**。摩擦尖峰是心音幅度中的冗余部分。在几项研究中[[44](#bib.bib44), [38](#bib.bib38)]，在心音预处理过程中检测并去除了摩擦尖峰（即，用零替换）。'
- en: Selection of noise-free segments. Apart from removing noise from heart sounds,
    the usage of noise-free heart sound segments has been shown helpful for heart
    sound analysis. Wavelet entropy was used to evaluate noise in heart sound segments [[7](#bib.bib7)],
    as clean heart sounds have relatively higher wavelet entropy than noisy heart
    sounds. Empirical wavelet transform was used to separate heart sounds, murmurs,
    low-frequency artefacts, and high-frequency noises in another study [[57](#bib.bib57)].
    Additionally, classic machine learning was also used for detecting noise-free
    heart sound segments. In [[4](#bib.bib4)], SVMs were applied to classify the quality
    of heart sound signals into binary classes (i. e., ‘unacceptable’ and ‘acceptable’)
    or three classes (i. e., ‘unacceptable’, ‘good’, ‘excellent’) without segmentation
    based on ten types of multi-domain features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 选择无噪声段。除了从心音中去除噪声外，使用无噪声的心音段被证明对心音分析很有帮助。*小波熵*被用于评估心音段中的噪声[[7](#bib.bib7)]，因为干净的心音具有相对较高的小波熵，而噪声心音的熵较低。在另一项研究中，*经验小波变换*被用于分离心音、杂音、低频伪影和高频噪声[[57](#bib.bib57)]。此外，经典的*机器学习*也被用于检测无噪声的心音段。在[[4](#bib.bib4)]中，应用了支持向量机（SVM）将心音信号的质量分类为二类（即‘不可接受’和‘可接受’）或三类（即‘不可接受’、‘良好’、‘优秀’），而不进行基于十种多领域特征的分割。
- en: 3.2 Segmentation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 分割
- en: '![Refer to caption](img/4bb5979f0042467d42d79ce700c1c783.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4bb5979f0042467d42d79ce700c1c783.png)'
- en: 'Figure 3: The PCG recording of a *normal* heart sound recording from the PhysioNet/CinC
    Database [[58](#bib.bib58)]. Frames in the middle with four states labelled (i. e., S1,
    systole, S2, and diastole) are depicted.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：来自PhysioNet/CinC数据库的*正常*心音录音[[58](#bib.bib58)]。图中中间的帧标有四种状态（即S1、收缩期、S2和舒张期）。
- en: Heart sound segmentation was used to split an audio sample into a set of smaller
    audio segments, which could be equal to or shorter than a complete cardiac cycle [[39](#bib.bib39),
    [59](#bib.bib59), [60](#bib.bib60)]. The segments shorter than a cardiac cycle
    could include S1, systole, S2, and diastole, as indicated in Fig. [3](#S3.F3 "Figure
    3 ‣ 3.2 Segmentation ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on
    Heart Sound Analysis in the Deep Learning Era").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 心音分割用于将音频样本分割成一组较小的音频段，这些段可以等于或短于完整的心动周期[[39](#bib.bib39), [59](#bib.bib59),
    [60](#bib.bib60)]。短于心动周期的段可能包括S1、收缩期、S2和舒张期，如图[3](#S3.F3 "Figure 3 ‣ 3.2 Segmentation
    ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")所示。
- en: Energy-based segmentation. As heart sounds at different states have various
    energies, signal energy has been used for localising S1 and S2 peaks [[61](#bib.bib61),
    [29](#bib.bib29), [30](#bib.bib30), [62](#bib.bib62)]. Based on frequency information
    (e. g., Wavelet Transform (WT)) of heart sounds, energy peaks of wavelet coefficients
    were detected for localising S1 and S2 in [[63](#bib.bib63)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的分割。由于心音在不同状态下具有不同的能量，因此信号能量被用于定位S1和S2峰值[[61](#bib.bib61), [29](#bib.bib29),
    [30](#bib.bib30), [62](#bib.bib62)]。根据心音的频率信息（例如，*小波变换（WT）*），在[[63](#bib.bib63)]中检测到的小波系数的能量峰值用于定位S1和S2。
- en: Envelope-based segmentation. Apart from energy, heart sound segmentation can
    be achieved based on envelopes [[64](#bib.bib64), [46](#bib.bib46)]. For instance,
    heart sound segmentation was implemented based on Shannon energy envelope and
    zero crossings of heart sounds in [[9](#bib.bib9)]. In another two studies [[47](#bib.bib47),
    [48](#bib.bib48)], S1 of the first heart cycle was detected based on Shannon energy
    envelopes, and the next S1 heart sounds were detected by a sliding heart cycle
    window.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于包络的分割。除了能量外，心音分割还可以基于包络[[64](#bib.bib64), [46](#bib.bib46)]。例如，在[[9](#bib.bib9)]中，基于香农能量包络和心音的零交叉实现了心音分割。在另外两项研究中[[47](#bib.bib47),
    [48](#bib.bib48)]，基于香农能量包络检测了第一个心动周期的S1，并通过滑动心动周期窗口检测了下一个S1心音。
- en: 'Loudness-based segmentation. Loudness has proven its potential to segment heart
    sounds [[53](#bib.bib53), [65](#bib.bib65)]. Specifically, spectrograms extracted
    from heart sounds are firstly converted into the Bark scale and smoothed with
    a Hanning window. At each time frame, the sensation of loudness is then calculated
    by the mean of the amplitudes at all frequency bands: $L(t)=\frac{\sum_{t=1}^{T}A(t)}{T}$,
    where $A(t)$ is the amplitude at the $t$-th time frame, and $T$ is the total number
    of time frames. Furthermore, the derivation of the loudness function is computed
    for obtaining peaks. Therefore, systoles and diastoles can be localised as they
    have different time lengths.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于响度的分割。响度已经证明其在分割心音方面的潜力 [[53](#bib.bib53), [65](#bib.bib65)]。具体来说，从心音中提取的声谱图首先被转换为
    Bark 量表，并用 Hanning 窗口进行平滑。在每个时间帧中，通过所有频带幅度的均值来计算响度感受：$L(t)=\frac{\sum_{t=1}^{T}A(t)}{T}$，其中
    $A(t)$ 是第 $t$ 时刻的幅度，$T$ 是时间帧的总数。此外，为了获得峰值，还计算了响度函数的导数。因此，收缩期和舒张期可以被定位，因为它们具有不同的时间长度。
- en: Classic Machine Learning for Segmentation. ML models have been proposed for
    more precise heart sound segmentation than the above mentioned rule-based segmentation
    methods. ML models for segmentation are mainly trained in supervised learning
    frameworks. There are also a few works using unsupervised learning. Both unsupervised
    and supervised learning approaches are introduced in the following.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分割的经典机器学习。已有机器学习模型被提出，用于比上述规则基础的分割方法更精确的心音分割。用于分割的机器学习模型主要在监督学习框架中进行训练。也有一些工作采用无监督学习。以下介绍了无监督学习和监督学习的方法。
- en: '*Unsupervised Learning* Considering the limited availability of the heart sound
    datasets, the authors of [[66](#bib.bib66)] adopted an unsupervised spectral clustering
    technique based on Gaussian kernel similarity to get the frame labels (e. g., S1
    and S2), which are further utilised to segment the heart sound.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习* 鉴于心音数据集的有限可用性，[[66](#bib.bib66)] 的作者采用了基于高斯核相似性的无监督谱聚类技术来获得帧标签（例如，S1
    和 S2），这些标签进一步用于分割心音。'
- en: '*Supervised Learning* Particularly, a hidden Markov model (HMM) has been widely
    used for this purpose [[5](#bib.bib5)]. Let us assume the heart states as $S=\{s_{1},s_{2},s_{3},s_{4}\}=\{S1,\mbox{systole},S2,\mbox{diastole}\}$,
    and the observations $O=\{o_{1},o_{2},...,o_{T}\}$ as the raw heart sounds or
    acoustic features. A transmission matrix $A=\{a_{ij}\}$ denotes the probablity
    of a state $s_{i}$ at the $t$-th time frame moving to $s_{j}$ at the $(t+1)$-th
    time frame. The probability density distribution of an observation $o_{t}$ to
    be generated by a state $s_{i}$ is $B={b_{i}(o_{t})}=P[o_{t}|s_{i}]$, where $P$
    means probability. The initial state distribution is $\pi=\{\pi_{i}\}$, representing
    the probability of state $s_{i}$ at the start time frame. Given $A$, $B$, $\pi$,
    and $O$, an HMM model aims to optimise the state sequence. The Viterbi algorithm
    is often used for this purpose [[67](#bib.bib67)] and more details can be found
    in [[5](#bib.bib5)].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*监督学习* 特别是，隐马尔可夫模型（HMM）已经广泛用于这一目的 [[5](#bib.bib5)]。我们假设心脏状态为 $S=\{s_{1},s_{2},s_{3},s_{4}\}=\{S1,\mbox{收缩期},S2,\mbox{舒张期}\}$，观察值
    $O=\{o_{1},o_{2},...,o_{T}\}$ 为原始心音或声学特征。传输矩阵 $A=\{a_{ij}\}$ 表示状态 $s_{i}$ 在第 $t$
    时刻转移到状态 $s_{j}$ 在第 $(t+1)$ 时刻的概率。观察值 $o_{t}$ 由状态 $s_{i}$ 生成的概率密度分布为 $B={b_{i}(o_{t})}=P[o_{t}|s_{i}]$，其中
    $P$ 代表概率。初始状态分布为 $\pi=\{\pi_{i}\}$，表示在开始时刻状态 $s_{i}$ 的概率。给定 $A$、$B$、$\pi$ 和 $O$，HMM
    模型旨在优化状态序列。维特比算法通常用于此目的 [[67](#bib.bib67)]，更多详细信息可以在 [[5](#bib.bib5)] 中找到。'
- en: In order to better capture the abrupt changes in the PCG signal, the envelope
    of the signal was obtained in [[68](#bib.bib68)], and the kurtosis of the envelope
    was computed to get the impulse-like characteristics, which were further passed
    through a zero frequency filter for pure impulse information. Along with the heart
    sound labels, the extracted features were fed into a hidden semi-Markov model
    (HSMM). To better adapt the variability of the heart cycle duration (HCD) in the
    PCG recordings, a multi-centroid-duration-based HSMM was introduced in [[69](#bib.bib69)],
    where HCDs were estimated at various instances of a PCG to get maximum possible
    duration values and those nearest values were clubbed into clusters to refer each
    centroid. With more accurate state duration information, the HSMM achieved better
    segmentation performance. Similarly, considering the inter-patient variability,
    the emission probability distributions to each patient were estimated through
    a Gaussian mixture model (GMM) in an unsupervised and adaptive way in the improved
    HSMM in another study [[70](#bib.bib70)]. Moreover, the expectation maximisation
    algorithm developed in [[71](#bib.bib71)] searched for sojourn time distribution
    parameters of an HSMM for each subject. Many studies [[6](#bib.bib6), [72](#bib.bib72),
    [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [40](#bib.bib40), [41](#bib.bib41),
    [76](#bib.bib76), [43](#bib.bib43), [31](#bib.bib31), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79)] have employed logistic regression-based HSMM (LR-HSMM) proposed
    in [[80](#bib.bib80)] for heart sound segmentation. LR was incorporated to predict
    the probability of $P[s_{j}|o_{t}]$, and $B$ is then computed with the Bayes rule.
    There are also other improved HMM methods, such as the *duration-dependent HMM* [[67](#bib.bib67),
    [81](#bib.bib81)], which considers the probability density function of the duration
    at each state. Another study [[38](#bib.bib38)] proposed a Markov-switching model
    for heart sound segmentation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地捕捉PCG信号中的突变，信号的包络线在[[68](#bib.bib68)]中被提取出来，并计算了包络线的峭度以获取冲击般的特征，然后将其通过零频率滤波器以获取纯冲击信息。结合心音标签，提取的特征被输入到隐藏的半马尔可夫模型（HSMM）中。为了更好地适应PCG记录中心周期持续时间（HCD）的变化，在[[69](#bib.bib69)]中引入了基于多中心持续时间的HSMM，其中在PCG的不同实例中估计HCD，以获取最大可能的持续时间值，并将这些最接近的值聚集成簇以指代每个中心。通过更准确的状态持续时间信息，HSMM实现了更好的分割性能。类似地，考虑到患者之间的变异性，通过改进的HSMM中的高斯混合模型（GMM）以无监督和自适应的方式估计了每个患者的发射概率分布[[70](#bib.bib70)]。此外，[[71](#bib.bib71)]中开发的期望最大化算法为每个受试者搜索了HSMM的停留时间分布参数。许多研究[[6](#bib.bib6),
    [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [40](#bib.bib40),
    [41](#bib.bib41), [76](#bib.bib76), [43](#bib.bib43), [31](#bib.bib31), [77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79)]使用了[[80](#bib.bib80)]中提出的基于逻辑回归的HSMM（LR-HSMM）进行心音分割。LR被用来预测$P[s_{j}|o_{t}]$的概率，然后用贝叶斯规则计算$B$。还有其他改进的HMM方法，如*依赖于持续时间的HMM*[[67](#bib.bib67),
    [81](#bib.bib81)]，它考虑了每个状态下持续时间的概率密度函数。另一项研究[[38](#bib.bib38)]提出了一种用于心音分割的马尔可夫切换模型。
- en: 3.3 Classification
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 分类
- en: The target task of automated auscultation is heart sound classification, including
    i) abnormal heart sound detection (e. g., murmurs, mitral stenosis, etc.) and
    ii) severity of cardiovascular diseases (normal/mild/moderate).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自动听诊的目标任务是心音分类，包括i）异常心音检测（如，杂音，二尖瓣狭窄等）和ii）心血管疾病的严重程度（正常/轻度/中度）。
- en: Feature Engineering. After denoising and segmentation, feature extraction is
    often performed before training a classifier. Low-level descriptors (LLDs) and
    functionals are often extracted as acoustic features. LLDs are segmental features
    on short-time segment analysis (see Table [2](#S3.T2 "Table 2 ‣ 3.3 Classification
    ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")), and functionals are supra-segmental feature vectors
    projected from LLDs. Functionals are generally statistical features, such as mean,
    max, standard deviation, and many others.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程。在去噪和分割之后，通常会进行特征提取以训练分类器。低级描述符（LLDs）和函数特征通常作为声学特征提取。LLDs是在短时间段分析上的分段特征（见表[2](#S3.T2
    "Table 2 ‣ 3.3 Classification ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era")），而函数特征是从LLDs投影得到的超段特征向量。函数特征通常是统计特征，如均值、最大值、标准差等。
- en: We mainly list the LLDs used for heart sound classification in [Table 2](#S3.T2
    "Table 2 ‣ 3.3 Classification ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era"). Apart from time-domain
    LLDs, LLDs in the frequency-domain are widely used for heart sound classification.
    Specifically, the frequency-domain features consist of spectral features based
    on Fourier Transform, Mel-scaled spectral features, and wavelet features. There
    are also pre-existing feature sets used for heart sound classification, including
    the ComParE feature set [[82](#bib.bib82)] and the eGeMAPS feature set [[83](#bib.bib83)].
    Both feature sets can be extracted with openSMILE which is an open-source toolkit [[84](#bib.bib84)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the aforementioned hand-crafted features, deep representations
    have been studied more recently (see [Table 2](#S3.T2 "Table 2 ‣ 3.3 Classification
    ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")). Due to the strong capability of deep learning models
    for extracting abstract features, deep representations bear potential to improve
    the performance of hand-crafted features.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Hand-crafted features and deep representations for heart sound classification.
    We present various low-level descriptors (LLDs) in hand-crafted features.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '| Group | Features | Description | Reference |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
- en: '| Time-domain | Envolope | Envelope of a signal | [[85](#bib.bib85), [57](#bib.bib57)]
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
- en: '|  | Amplitude | Amplitude of a signal | [[63](#bib.bib63)] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
- en: '|  | Energy | Energy of a signal | [[46](#bib.bib46), [32](#bib.bib32), [62](#bib.bib62)]
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
- en: '|  | Entropy | Signal entropy | [[86](#bib.bib86), [32](#bib.bib32)] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
- en: '|  | Loudness | Perception of sound magnitude | [[53](#bib.bib53)] |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
- en: '|  | Peak amplitude | Amplitude of peaks | [[40](#bib.bib40)] |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
- en: '| Spectral | Spectral amplitude | Fourier transform | [[7](#bib.bib7), [87](#bib.bib87),
    [46](#bib.bib46), [88](#bib.bib88), [86](#bib.bib86), [33](#bib.bib33), [32](#bib.bib32),
    [89](#bib.bib89), [38](#bib.bib38)] |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
- en: '|  | Dominant frequency value | Frequency which leads to the maximum spectrum
    | [[87](#bib.bib87), [88](#bib.bib88)] |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
- en: '|  | Dominant frequency ratio | Ratio of the maximun energy to the total energy
    | [[87](#bib.bib87), [88](#bib.bib88)] |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
- en: '|  | Energy | Spectral energy | [[38](#bib.bib38)] |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
- en: '|  | Spectral roll-off | Frequency below a specific percentage of the total
    spectral energy | [[32](#bib.bib32)] |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
- en: '|  | Spectral centroid | Average of magnitude spectrogram at each frame | [[32](#bib.bib32),
    [89](#bib.bib89)] |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
- en: '|  | Specrtal flux | Changing speed of the power spectrum | [[32](#bib.bib32)]
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
- en: '|  | Power spectral density (PSD) | Distribution of power in spectral components
    | [[46](#bib.bib46), [85](#bib.bib85), [89](#bib.bib89)] |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
- en: '|  | Spectral entropy | Shannon entropy of PSD | [[87](#bib.bib87), [88](#bib.bib88),
    [86](#bib.bib86), [54](#bib.bib54)] |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
- en: '|  | Instantaneous frequency | Frequency for non-stationary signals | [[90](#bib.bib90)]
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 瞬时频率 | 非平稳信号的频率 | [[90](#bib.bib90)] |'
- en: '|  | Fractional Fourier transform entropy | Spectral entropy of the fractional
    Fourier transform | [[91](#bib.bib91)] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 分数傅里叶变换熵 | 分数傅里叶变换的谱熵 | [[91](#bib.bib91)] |'
- en: '|  | Spectrogram | Short-Time Fourier Transform (STFT) | [[47](#bib.bib47),
    [48](#bib.bib48)] |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | 谱图 | 短时傅里叶变换（STFT） | [[47](#bib.bib47), [48](#bib.bib48)] |'
- en: '|  | Cepstrum | Inverse Fourier transform on the logarithm of the signal spectrum
    | [[33](#bib.bib33)] |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 倒谱 | 对信号谱的对数进行傅里叶逆变换 | [[33](#bib.bib33)] |'
- en: '| Mel frequency | Mel-frequency | Mel-scaled frequency | [[92](#bib.bib92),
    [73](#bib.bib73)] |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Mel频率 | Mel频率 | Mel尺度频率 | [[92](#bib.bib92), [73](#bib.bib73)] |'
- en: '|  | Mel-Frequency Cepstral Coefficients (MFCCs) | Discrete cosine transform
    of Mel-scaled spectrogram | [[38](#bib.bib38), [93](#bib.bib93), [87](#bib.bib87),
    [86](#bib.bib86), [79](#bib.bib79), [94](#bib.bib94), [75](#bib.bib75), [95](#bib.bib95)]
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | Mel频率倒谱系数（MFCCs） | Mel尺度谱图的离散余弦变换 | [[38](#bib.bib38), [93](#bib.bib93),
    [87](#bib.bib87), [86](#bib.bib86), [79](#bib.bib79), [94](#bib.bib94), [75](#bib.bib75),
    [95](#bib.bib95)] |'
- en: '|  | Fractional Fourier transform-based Mel-frequency | Mel-frequency from
    the fractional Fourier transform | [[43](#bib.bib43)] |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于分数傅里叶变换的Mel频率 | 从分数傅里叶变换得到的Mel频率 | [[43](#bib.bib43)] |'
- en: '| Wavelet | Wavelet transform | Frequency analysis of a signal at various scales
    | [[85](#bib.bib85), [59](#bib.bib59), [89](#bib.bib89)] |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 小波 | 小波变换 | 在不同尺度下的信号频率分析 | [[85](#bib.bib85), [59](#bib.bib59), [89](#bib.bib89)]
    |'
- en: '|  | Wavelet scattering transform | “Wavelet convolution with nonlinear modulus
    and averaging scaling function”¹¹1https://de.mathworks.com/help/wavelet/ug/wavelet-scattering.html
    (translation invariance and elastic deformation stability [[96](#bib.bib96)])
    | [[97](#bib.bib97), [96](#bib.bib96)] |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 小波散射变换 | “具有非线性模量和平均缩放函数的小波卷积”¹¹1https://de.mathworks.com/help/wavelet/ug/wavelet-scattering.html（平移不变性和弹性变形稳定性[[96](#bib.bib96)]）
    | [[97](#bib.bib97), [96](#bib.bib96)] |'
- en: '|  | Wavelet synchrosqueezing transform | Reassignment of wavelet coefficients
    | [[35](#bib.bib35)] |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | 小波同步压缩变换 | 小波系数的重新分配 | [[35](#bib.bib35)] |'
- en: '|  | Tunable quality wavelet transform | “Wavelet multiresolution analysis
    with a user-specified Q-factor, which is the ratio of the centre frequency to
    the bandwidth of the filters”²²2https://de.mathworks.com/help/wavelet/ug/tunable-q-factor-wavelet-transform.html
    | [[98](#bib.bib98), [52](#bib.bib52)] |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 可调质量小波变换 | “具有用户指定Q因子的多尺度小波分析，Q因子是滤波器中心频率与带宽的比率”²²2https://de.mathworks.com/help/wavelet/ug/tunable-q-factor-wavelet-transform.html
    | [[98](#bib.bib98), [52](#bib.bib52)] |'
- en: '|  | Wavelet entropy | Temporal energy distribution based on wavelet coefficients
    | [[7](#bib.bib7)] |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 小波熵 | 基于小波系数的时间能量分布 | [[7](#bib.bib7)] |'
- en: '| Feature set | ComParE | Computational Paralinguistics ChallengE feature set
    | [[92](#bib.bib92)] |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 特征集 | ComParE | 计算语言学挑战特征集 | [[92](#bib.bib92)] |'
- en: '|  | eGeMAPS | The extended Geneva Minimalistic Acoustic Parameter Set | [[92](#bib.bib92)]
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | eGeMAPS | 扩展的日内瓦最简声学参数集 | [[92](#bib.bib92)] |'
- en: '| Deep representation | Graph-based features | Petersen graph pattern | [[99](#bib.bib99)]
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 深度表示 | 基于图的特征 | Petersen图模式 | [[99](#bib.bib99)] |'
- en: '|  | Sparse coefficient | Result of sparse coding | [[6](#bib.bib6)] |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 稀疏系数 | 稀疏编码的结果 | [[6](#bib.bib6)] |'
- en: '|  | Autoencoder-based features | Features extracted by an autoencoder from
    hand-crafted features | [[100](#bib.bib100), [55](#bib.bib55)] |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于自编码器的特征 | 从手工设计特征中提取的自编码器特征 | [[100](#bib.bib100), [55](#bib.bib55)]
    |'
- en: Classic Machine Learning for Classification.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习用于分类。
- en: '![Refer to caption](img/7e32d7c8630835d1474b4becf9ce2037.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7e32d7c8630835d1474b4becf9ce2037.png)'
- en: 'Figure 4: Statistics of the literature using discriminative machine learning
    models for heart sound classification during 2017–2022\. FNN: feed-forward neural
    network; KNN: $k$-nearest neighbour; LDC: linear discriminant classifier.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：2017–2022年使用辨别式机器学习模型进行心音分类的文献统计。FNN：前馈神经网络；KNN：$k$-最近邻；LDC：线性判别分类器。
- en: Rule-based classification was proposed for heart sound classification in [[57](#bib.bib57),
    [101](#bib.bib101)]. For better performance, ML was used for heart sound classification
    in most research studies. We briefly introduce the classifiers in the two groups
    of generative models and discriminative models in the following.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative models* aim to generate the joint probability distribution $P(X,y)$,
    given the features $X$ and the labels $y$. The posterior probability $P(y|X)$
    is computed via the Bayes rule $P(y|X)=\frac{P(X,y)}{P(X)}=\frac{P(X|y)P(y)}{P(X)}$,
    where $P(X|y)$ is the likelihood probability distribution. The Naïve Bayes Classifier [[102](#bib.bib102),
    [91](#bib.bib91), [41](#bib.bib41), [103](#bib.bib103)] was widely used for heart
    sound classification due to its advantage of being easy-to-use. Gaussian Mixture
    Models (GMMs) [[95](#bib.bib95), [53](#bib.bib53)] were used to estimate the data
    distribution by optimising the weights of Gaussian mixture components and mean
    and variance in each component. A Gaussian mixture-based HMM [[38](#bib.bib38)]
    was employed for heart sound classification considering the four sequential heart
    states, i. e., S1, systole, S2, and diastole.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '*Discriminative models* are designed to directly predict the posterior probability
    $P(y|X)$ given $X$. Fig. [4](#S3.F4 "Figure 4 ‣ 3.3 Classification ‣ 3 Heart Sound
    Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era") presents a statistic of recent works from 2017 to 2022 that employ classic
    machine learning models for heart sound classification. SVMs have been very widely
    used for heart sound classification by learning a supporting hyperplane between
    classes [[6](#bib.bib6), [47](#bib.bib47), [48](#bib.bib48), [91](#bib.bib91),
    [93](#bib.bib93), [97](#bib.bib97), [46](#bib.bib46), [99](#bib.bib99), [40](#bib.bib40),
    [75](#bib.bib75), [41](#bib.bib41), [95](#bib.bib95), [60](#bib.bib60), [43](#bib.bib43),
    [92](#bib.bib92), [100](#bib.bib100), [55](#bib.bib55), [103](#bib.bib103), [33](#bib.bib33),
    [79](#bib.bib79), [64](#bib.bib64), [85](#bib.bib85), [96](#bib.bib96)]. Apart
    from linear projection between data samples and labels, SVMs can learn separating
    hyperplanes on non-linear data via non-linear kernels, such as radial basis function.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, $k$-nearest neighbours (KNNs) has shown good performance on heart
    sound classification with the idea of classifying a data sample according to the
    classes of its $k$-nearest neighbours [[91](#bib.bib91), [93](#bib.bib93), [46](#bib.bib46),
    [40](#bib.bib40), [75](#bib.bib75), [85](#bib.bib85), [90](#bib.bib90), [41](#bib.bib41),
    [60](#bib.bib60), [43](#bib.bib43), [103](#bib.bib103), [104](#bib.bib104)]. Also,
    decision trees were successfully used for heart sound classification [[7](#bib.bib7),
    [99](#bib.bib99), [40](#bib.bib40), [88](#bib.bib88), [75](#bib.bib75), [41](#bib.bib41),
    [103](#bib.bib103)]. One reason is that limiting the number of decision nodes
    can help avoid overfitting [[7](#bib.bib7)], and another reason is that the structure
    of a decision tree can show the internal logic for classification. Bagged trees [[99](#bib.bib99),
    [40](#bib.bib40)] assemble multiple decision trees for more complex model architectures,
    therefore it is possible to achieve better performance. Random forests [[93](#bib.bib93),
    [88](#bib.bib88), [90](#bib.bib90), [103](#bib.bib103), [79](#bib.bib79), [35](#bib.bib35)]
    further improve bagged trees with less features to be used when splitting each
    node.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，$k$-最近邻（KNNs）在心音分类中表现良好，其思想是根据数据样本的$k$-最近邻的类别进行分类[[91](#bib.bib91), [93](#bib.bib93),
    [46](#bib.bib46), [40](#bib.bib40), [75](#bib.bib75), [85](#bib.bib85), [90](#bib.bib90),
    [41](#bib.bib41), [60](#bib.bib60), [43](#bib.bib43), [103](#bib.bib103), [104](#bib.bib104)]。此外，决策树也被成功应用于心音分类[[7](#bib.bib7),
    [99](#bib.bib99), [40](#bib.bib40), [88](#bib.bib88), [75](#bib.bib75), [41](#bib.bib41),
    [103](#bib.bib103)]。一个原因是限制决策节点的数量可以帮助避免过拟合[[7](#bib.bib7)]，另一个原因是决策树的结构可以展示分类的内部逻辑。袋装树[[99](#bib.bib99),
    [40](#bib.bib40)]将多个决策树组合成更复杂的模型架构，因此有可能实现更好的性能。随机森林[[93](#bib.bib93), [88](#bib.bib88),
    [90](#bib.bib90), [103](#bib.bib103), [79](#bib.bib79), [35](#bib.bib35)]通过在分裂每个节点时使用更少的特征来进一步改进袋装树。
- en: In recent years, feed-forward Neural Networks (FNNs) have started to be applied
    to heart sound classification [[63](#bib.bib63), [52](#bib.bib52), [87](#bib.bib87),
    [75](#bib.bib75), [55](#bib.bib55), [32](#bib.bib32), [94](#bib.bib94), [30](#bib.bib30),
    [89](#bib.bib89)]. FNNs can automatically learn a non-linear projection between
    acoustic features and labels. Although FNNs lack of explainability compared to
    other classifiers such as SVMs and decision trees, they are promising to produce
    good performance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，前馈神经网络（FNNs）已开始应用于心音分类[[63](#bib.bib63), [52](#bib.bib52), [87](#bib.bib87),
    [75](#bib.bib75), [55](#bib.bib55), [32](#bib.bib32), [94](#bib.bib94), [30](#bib.bib30),
    [89](#bib.bib89)]。FNNs能够自动学习声学特征与标签之间的非线性映射。尽管与其他分类器如支持向量机（SVMs）和决策树相比，FNNs缺乏可解释性，但它们在产生良好性能方面具有潜力。
- en: There are also several other machine learning models, such as, linear discriminant
    classifiers [[99](#bib.bib99), [55](#bib.bib55), [103](#bib.bib103)], logistic
    regression [[103](#bib.bib103)], quadratic discriminant analysis [[46](#bib.bib46)],
    boosting methods [[88](#bib.bib88), [59](#bib.bib59), [79](#bib.bib79), [98](#bib.bib98)],
    and others [[86](#bib.bib86), [54](#bib.bib54)]. Finally, multiple classifiers
    can be further assembled for better performance compared to that of a single model [[93](#bib.bib93),
    [75](#bib.bib75), [59](#bib.bib59), [43](#bib.bib43), [85](#bib.bib85)].
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他几种机器学习模型，如线性判别分类器[[99](#bib.bib99), [55](#bib.bib55), [103](#bib.bib103)]，逻辑回归[[103](#bib.bib103)]，二次判别分析[[46](#bib.bib46)]，提升方法[[88](#bib.bib88),
    [59](#bib.bib59), [79](#bib.bib79), [98](#bib.bib98)]，以及其他[[86](#bib.bib86), [54](#bib.bib54)]。最后，与单一模型相比，多个分类器可以进一步组合以提高性能[[93](#bib.bib93),
    [75](#bib.bib75), [59](#bib.bib59), [43](#bib.bib43), [85](#bib.bib85)]。
- en: 4 State-of-the-art Studies
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 先进的研究
- en: Due to the strong capability of extracting effective representations, deep learning
    has been successfully applied to a range of acoustic tasks, e. g., speech emotion
    recognition [[105](#bib.bib105)], respiratory sound classification [[106](#bib.bib106)],
    snore sound classification [[107](#bib.bib107)], and many more. In recent advances,
    deep learning has been also employed in processing heart sound signals and achieved
    good performance [[8](#bib.bib8)]. In this regard, the applications of deep learning
    methods for heart sound analysis tasks are introduced and discussed in the following.
    Notably, as there are few works focusing on denoising in this context with deep
    learning, we introduce deep learning topologies for segmentation and classification
    in this section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强大的有效表示提取能力，深度学习已成功应用于各种声学任务，例如，*语音情感识别*[[105](#bib.bib105)]、*呼吸声分类*[[106](#bib.bib106)]、*打鼾声分类*[[107](#bib.bib107)]等。在近期的进展中，深度学习也被用于处理心音信号，并取得了良好的性能[[8](#bib.bib8)]。在这方面，本文介绍和讨论了深度学习方法在心音分析任务中的应用。值得注意的是，由于在这一背景下针对去噪的深度学习工作较少，我们在本节中介绍了用于分割和分类的深度学习拓扑结构。
- en: 4.1 Deep Learning for Segmentation
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 深度学习在分割中的应用
- en: Various DL models have been proposed for heart sound segmentation. We group
    them into CNNs and RNNs which extract spatial and sequential representations,
    respectively.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 各种深度学习（DL）模型已经被提出用于心音分割。我们将它们分为卷积神经网络（CNNs）和递归神经网络（RNNs），分别用于提取空间和序列表示。
- en: Convolutional neural networks. Inspired by successful applications of deep convolutional
    neural networks (CNNs) in image segmentation, deep CNNs have been applied to heart
    sound segmentation in recent studies [[108](#bib.bib108)]. For instance, several
    CNN-based segmentation algorithms were proposed and compared in [[108](#bib.bib108)],
    including CNNs with sequential max temporal modelling, CNNs with HMMs or HSMMs
    to model the probability density distribution of observations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络。受到深度卷积神经网络（CNNs）在图像分割中成功应用的启发，近年来深度CNNs已被应用于心音分割的研究[[108](#bib.bib108)]。例如，[[108](#bib.bib108)]中提出并比较了几种基于CNN的分割算法，包括具有序列最大时间建模的CNNs，以及用于建模观察值概率密度分布的具有HMM或HSMM的CNNs。
- en: Recurrent neural networks. Recurrent neural networks (RNNs) have demonstrated
    their ability to exploit temporal information in sequential data. Therefore, RNNs
    can also help in locating the states of heart sounds. In [[109](#bib.bib109)],
    the authors regarded segmentation as an event detection task and developed bi-directional
    Gated Recurrent Unit (GRU)-RNNs based on spectrogram and envelop features. Since
    envelope features fail to effectively model the intrinsic duration information
    of the heart cycles, a duration-LSTM was proposed in [[110](#bib.bib110)] to integrate
    the duration vector into the standard LSTM cells with envelope features to obtain
    better segmentation performance. Specifically, duration parameters consist of
    heart cycle duration and systole duration estimated from the envelope autocorrelation.
    Without envelopes and time-frequency based features, the authors of [[111](#bib.bib111)]
    utilised bi-directional GRU-RNNs to segment the heart sound directly. Considering
    the possible noisy and irregular sequences in heart sound signals, an attention-based
    RNN framework was introduced in [[112](#bib.bib112)]. Specifically, before the
    final classification layer, with the hidden representation returned by bi-directional
    LSTMs, a single linear layer was applied to learn the weight score of each hidden
    state. Such weight score values are multiplied with the hidden representation
    for the final classification.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络。递归神经网络（RNNs）已证明了其在序列数据中利用时间信息的能力。因此，RNNs 也可以帮助定位心音的状态。在[[109](#bib.bib109)]中，作者将分割视为事件检测任务，并基于频谱图和包络特征开发了双向门控递归单元（GRU）-RNNs。由于包络特征无法有效建模心周期的内在持续时间信息，[[110](#bib.bib110)]提出了一种持续时间LSTM，将持续时间向量集成到标准LSTM单元中，以包络特征获得更好的分割性能。具体来说，持续时间参数包括心周期持续时间和从包络自相关估计的收缩期持续时间。在没有包络和时频特征的情况下，[[111](#bib.bib111)]的作者利用双向GRU-RNNs直接对心音进行分割。考虑到心音信号中的噪声和不规则序列，[[112](#bib.bib112)]引入了一种基于注意力的RNN框架。具体来说，在最终分类层之前，利用双向LSTM返回的隐藏表示，应用了一个线性层来学习每个隐藏状态的权重分数。这些权重分数值与隐藏表示相乘，用于最终分类。
- en: CNNs + RNNs. An end-to-end model was proposed in [[113](#bib.bib113)], where
    CNNs and LSTM recurrent neural networks (RNNs) are integrated to learn rich and
    efficient features from the audio directly. The gate structures of each LSTM unit
    are optimised in [[114](#bib.bib114)] for efficiency.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs + RNNs。在[[113](#bib.bib113)]中提出了一种端到端的模型，其中CNNs和LSTM递归神经网络（RNNs）被集成以直接从音频中学习丰富且高效的特征。每个LSTM单元的门控结构在[[114](#bib.bib114)]中进行了效率优化。
- en: 4.2 Deep Learning for Classification
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 深度学习在分类中的应用
- en: '![Refer to caption](img/c58f3a7a394e25d1527ffcb1838fdbd6.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c58f3a7a394e25d1527ffcb1838fdbd6.png)'
- en: 'Figure 5: Pipeline of DL models working on heart sounds. “1” indicates transfer
    learning; “2” means deep learning on the time-frequency representation; “3” depicts
    end-to-end learning. The three branches can be in parallel or assembled at the
    feature-/decision-level. DL can be also utilised for processing features apart
    from raw audio signals and time-frequency representations. The processing of DL
    on other features can be found in [subsection 4.2](#S4.SS2 "4.2 Deep Learning
    for Classification ‣ 4 State-of-the-art Studies ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era").'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：心音的DL模型流程图。 “1”表示迁移学习；“2”表示对时频表示的深度学习；“3”表示端到端学习。这三条分支可以并行运行，也可以在特征/决策级别进行组合。DL也可以用于处理原始音频信号和时频表示之外的特征。关于DL在其他特征上的处理，可以参考[第4.2节](#S4.SS2
    "4.2 深度学习在分类中的应用 ‣ 4 先进研究 ‣ 深度学习时代心音分析的全面调查")。
- en: Different from classic machine learning, DL learns effective representations
    from either raw heart sounds or simple time-frequency representations using models
    with more parameters. DL often performs very well on many acoustic tasks benefiting
    from its strong capability. Apart from the pipeline shown in Fig. [5](#S4.F5 "Figure
    5 ‣ 4.2 Deep Learning for Classification ‣ 4 State-of-the-art Studies ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era"), we summarise the advances
    of deep learning methods for heart sound classification in the following.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与经典机器学习不同，DL通过具有更多参数的模型从原始心音或简单时频表示中学习有效表示。DL通常在许多声学任务中表现非常好，这得益于其强大的能力。除了图[5](#S4.F5
    "图5 ‣ 4.2 深度学习在分类中的应用 ‣ 4 先进研究 ‣ 深度学习时代心音分析的全面调查")中所示的流程，我们总结了深度学习方法在心音分类中的进展。
- en: Deep learning on time-frequency representations. As it is challenging to extract
    effective representations from raw heart sound signals, 2D time-frequency representations
    have been widely used as the input of 2D CNNs for heart sound classification,
    respectively [[72](#bib.bib72), [73](#bib.bib73), [50](#bib.bib50), [39](#bib.bib39),
    [115](#bib.bib115), [81](#bib.bib81), [76](#bib.bib76), [116](#bib.bib116), [117](#bib.bib117),
    [49](#bib.bib49), [77](#bib.bib77), [42](#bib.bib42), [118](#bib.bib118), [61](#bib.bib61),
    [29](#bib.bib29), [119](#bib.bib119)]. Spectrograms, extracted by STFT from heart
    sounds, were fed into ResNet for abormal heart sounds detection in [[120](#bib.bib120)].
    Multi-domain features were considered to be more comprehensive in reflecting the
    characteristics of all heart sound classes. In [[50](#bib.bib50)], spectrograms,
    Mel spectrograms and MFCCs were extracted as the inputs of VGG models, and the
    final predictions were obtained by ensemble learning.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对时频表示的深度学习。由于从原始心音信号中提取有效表示具有挑战性，2D时频表示已被广泛用作2D CNNs的输入以进行心音分类[[72](#bib.bib72),
    [73](#bib.bib73), [50](#bib.bib50), [39](#bib.bib39), [115](#bib.bib115), [81](#bib.bib81),
    [76](#bib.bib76), [116](#bib.bib116), [117](#bib.bib117), [49](#bib.bib49), [77](#bib.bib77),
    [42](#bib.bib42), [118](#bib.bib118), [61](#bib.bib61), [29](#bib.bib29), [119](#bib.bib119)]。通过STFT从心音中提取的频谱图被输入到ResNet中用于异常心音检测[[120](#bib.bib120)]。多域特征被认为在反映所有心音类别特征方面更为全面。在[[50](#bib.bib50)]中，频谱图、Mel频谱图和MFCCs被提取作为VGG模型的输入，最终预测通过集成学习获得。
- en: Apart from CNNs that are good at extracting spatial features, RNNs are more
    capable of extracting temporal features from sequential signals. LSTM nets were
    applied to process discrete wavelet transforms and MFCCs for heart sound classification [[36](#bib.bib36),
    [75](#bib.bib75)]. Deng et al. employed convolutional recurrent neural networks
    which combined the CNNs and RNNs [[121](#bib.bib121)].
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了擅长提取空间特征的CNNs外，RNNs更能够从序列信号中提取时间特征。LSTM网络被应用于处理离散小波变换和MFCCs用于心音分类[[36](#bib.bib36),
    [75](#bib.bib75)]。Deng等人使用了卷积递归神经网络，将CNNs和RNNs相结合[[121](#bib.bib121)]。
- en: Additionally, there are also other classifiers used for heart sound classification,
    such as a stacked sparse autoencoder deep neural network [[122](#bib.bib122)]
    and a semi-non-negative matrix factorisation classifier [[78](#bib.bib78)]. More
    details can be found in the listed references.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning on other features. Apart from the above excellent works about
    DL on time-frequency representations, other features extracted from heart sounds
    were also used as the input of DL models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: i) *Time-domain features*. Similar to features in classic machine learning,
    1D time-domain features can be also fed into DL models for heart sound classification.
    For instance, instant energy of the heart sound was extracted and used as the
    input of stacked auto-encoder networks [[62](#bib.bib62)]. Multiple statistical
    features (such as mean, median, variance, and many more) were extracted from all
    75 ms segments in each complete heart sound clip, and fed into a bidirectional
    LSTM (BiLSTM) net for classification in [[123](#bib.bib123)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: ii) *1D frequency-based features*. Either 1D CNNs or feed-forward DNN models
    can be used to process 1D features. General frequency features and Mel domain
    features were fed into 1D CNNs and then the multiple CNNs were assembled for the
    final prediction in [[119](#bib.bib119)]. Moreover, Mel spectrograms and MFCC
    were used to further extract features as the input of a 5-layer feed-forward DNN
    model in [[9](#bib.bib9)].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: iii) *2D frequency-based features*. Different from the above time-frequency
    representations, herein, we list 2D frequency-based features to include (a) multiple
    1D frequency features directly extracted from audio segments rather than window
    functions in the STFT domain and (b) features computed from time-frequency features.
    Qian et al. utilised wavelets to calculate wavelet energy features from a set
    of short acoustic segments and further used GRU-RNNs as the classifier [[124](#bib.bib124)].
    Dong et al. extracted log Mel features and corresponding functionals from heart
    sound segments and implemented classification LSTM-RNNs and GRU-RNNs [[92](#bib.bib92)].
    In their experiments, log Mel features performed better than MFCCs and other LLDs [[92](#bib.bib92)].
    Zhang et al. extracted temporal quasi-periodic features computed by an average
    magnitude difference function from spectrograms and applied LSTM-RNNs for exploring
    the dependency relation within the features [[125](#bib.bib125)]. A denoising
    auto-encoder was employed to extract deep representations from spectrograms as
    the input of the classifier of 1D CNNs in [[126](#bib.bib126)].
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end learning. In recent years, as time-frequency representations and
    other features still need human efforts to select features, it has been becoming
    increasingly popular to use end-to-end networks to learn representations from
    heart sounds. Based on raw heart sound signals, various 1D CNN architectures have
    been proposed and applied to the task of heart sound classification [[10](#bib.bib10),
    [39](#bib.bib39), [51](#bib.bib51), [34](#bib.bib34), [127](#bib.bib127)]. Furthermore,
    Liu et al. introduced a temporal convolutional network (TCN) that performed a
    high sensitivity for heart sound classification [[74](#bib.bib74)], as a TCN benefiting
    from dilated and casual convolutions is more suitable for sequential data than
    typical CNNs are. A 1D CNN model consisting of residual blocks was developed for
    classifying heart sounds [[128](#bib.bib128)]. Apart from CNNs, GRU-RNNs were
    also used to process raw heart sound signals for the screening of heart failure [[129](#bib.bib129)].
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Several studies also mentioned the capability of CNNs and RNNs for learning
    frequency-domain and time-domain characteristics of heart sounds. For instance,
    Shuvo et al. proposed a CardioXNet model that employed representation learning
    followed by sequence residual learning without any preprocessing [[130](#bib.bib130)].
    In the representation learning phase, three parallel 1D CNN pathways were constructed
    to extract time-invariant features from heart sound signals; in the sequence residual
    learning phase, BiLSTM nets were employed to learn sequential representation.
    The study in [[45](#bib.bib45)] attempted to automatically learn time-frequency
    features, i. e., frequency-domain features were extracted by 1D CNNs and the time-domain
    characteristics were extracted by GRU-RNNs. A self-attention mechanism was further
    used to fuse the two types of features for the final classification. In [[131](#bib.bib131),
    [132](#bib.bib132)], time-convolution (tConv) layers were implemented at the front
    end of the network for learning finite impulse response filters.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning. As data collection in the healthcare domain has to be executed
    according to extremely strict regulations, heart sound datasets are usually not
    as large as datasets in other areas of Computer Audition. Transfer learning attempts
    to employ pre-trained DL models which are optimised on large-scale datasets. In
    recent studies, pre-trained models are mainly learnt on either an image dataset
    (i. e., ImageNet [[133](#bib.bib133)]) and an audio dataset (i. e., AudioSet [[134](#bib.bib134)]).
    Although heart sounds are presented as audio signals which are a different data
    type from ImageNet, DL models trained on ImageNet have shown good performance
    on time-frequency representations extracted from heart sounds for heart sound
    classification. Typical DL models on ImageNet, such as AlexNet [[135](#bib.bib135)]
    and VGG [[136](#bib.bib136)], have been successfully used for heart sound classification [[44](#bib.bib44),
    [31](#bib.bib31), [137](#bib.bib137), [8](#bib.bib8)]. Compared to ImageNet, AudioSet
    includes multiple types of acoustic signals and therefore is more close to heart
    sounds with the consideration of data type. In [[138](#bib.bib138)], pre-trained
    Audio Neural Networks (PANNs) trained on AudioSet were used for classifying heart
    sounds with inputs of time-frequency representations. PANNs outperformed ImageNet-based
    models [[138](#bib.bib138)], including VGG, MobileNet V2 [[139](#bib.bib139)],
    ResNet [[140](#bib.bib140)], and ResNeXt [[141](#bib.bib141)].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: After extracting representations by pre-trained models, transfer learning uses
    various types of classifiers for classification, mainly including classic machine
    learning classifiers and feed-forward neural networks. For instance, SVMs were
    applied to representations extracted by AlexNet, VGG16, and VGG19 in [[44](#bib.bib44),
    [137](#bib.bib137), [31](#bib.bib31)]. Other classifiers such as KNNs were used
    in [[44](#bib.bib44)]. Pre-trained VGG model was frozen and followed by fully
    connected layers in [[8](#bib.bib8)].
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, fine-tuning pre-trained models in transfer learning has also shown
    good performance for heart sound classification. A pre-trained AlexNet after fine-tuning
    provided effective representations for heart sound classification [[44](#bib.bib44),
    [31](#bib.bib31)]. Similarly, PANNs was also fine-tuned in [[138](#bib.bib138)].
    Fine-tuned models have even outperformed pre-trained models as they adapted to
    the data distribution of heart sound datasets. In [[8](#bib.bib8)], fine-tuned
    VGG performed better than pre-trained VGG on heart sound classification when SVMs
    were the classifiers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: 5 Published Resources on Heart Sound Analysis
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Published Datasets
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the past years, several heart sound databases have been collected. Herein,
    we briefly introduce the following access-available databases listed in [Table 3](#S5.T3
    "Table 3 ‣ 5.1 Published Datasets ‣ 5 Published Resources on Heart Sound Analysis
    ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era").
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: 'The PASCAL challenge Database [[142](#bib.bib142)] was split into two sets
    A and B. In the dataset A, 176 heart sounds (0.393 hour) were recorded with the
    iStethoscope Pro iPhone app and annotated into S1 and S2 sounds for heart sound
    segmentation. Each heart sound in the dataset A was also labelled into one of
    the four classes: *normal*, *murmur*, *extra heart sound*, and *artifact*. The
    dataset B with 656 recordings (1.194 hours) was annotated into three classes:
    *normal*, *murmur*, and *extrasystole*.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'The PhysioNet/CinC Database [[58](#bib.bib58)], used in the PhysioNet/CinC
    Challenge 2016 [[143](#bib.bib143)], consists of multiple databases that were
    recorded from different data collectors. Especially, the publicly available training
    set has five databases collected from both healthy individuals and patients. The
    training set consists of 3,240 recordings (20.216 hours in total) from more than
    764 subjects. The task was set to classifying each data sample into three classes:
    *normal* and *abnormal*, and *noisy*.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: 'The Heart Sounds Shenzhen (HSS) corpus [[92](#bib.bib92)], employed in the
    INTERSPEECH Computaional Paralinguistic challengE (ComParE) 2018, was collected
    by the Shenzhen University General Hospital. In total, 170 subjects (f: 55, m:
    115) participated in the data collection with an electronic stethoscope, leading
    to 845 heart sound recordings of 7.047 hours. Each audio sample was annotated
    into one of the three classes: *normal*, *mild*, *moderate/severe*.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: 'An open heart sound database on GitHub [[144](#bib.bib144)] was collected for
    1,000 audio files (0.679 hour in total). The audio recordings are balanced in
    five classes: *normal*, *aortic stenosis*, *mitral regurgitation*, *mitral stenosis*,
    *mitral valve prolapse*.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: The Michigan Heart sound database³³3https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library⁴⁴4https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'primer_heartsound.html provides heart sound samples recorded from different
    areas and poses: the apex area when a subject is supine, the apex area for left
    decubitus, the aortic area when sitting, and the pulmonic area for supine. In
    total, 23 heart sounds were recorded and the total duration is 0.413 hour. The
    heart sounds were annotated into *normal* and multiple *pathological* states.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'The CirCor DigiScope Database [[145](#bib.bib145)], used in the George B. Moody
    PhysioNet Challenge 2022 [[146](#bib.bib146)], was collected from a pediatric
    population at or under 21 years old. The heart sounds were recorded from one or
    multiple locations: pulmonary valve, aortic valve, mitral valve, tricuspid valve,
    and others. The publicly available training set consist of 3,163 audio samples
    with 20.094 hours in total from 942 participants. Two classification tasks were
    targeted: i) *normal* and *abnormal*, and ii) *presence of murmurs*, *absence
    of murmurs*, and *unclear cases of murmurs*.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Published datasets for heart sound classification. AS: aortic stenosis,
    MR: mitral regurgitation, MS: mitral stenosis, MVP: mitral valve prolapse. Notably,
    the statistics in this table considered access available data sets only.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Challenge | #Samples | Duration (h) | #Subjects | Task |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
- en: '| PASCAL Database [[142](#bib.bib142)] | PASCAL Challenge [[142](#bib.bib142)]
    | 176 | 0.393 | unknown | Dataset A: Normal, Murmur, Extra Heart Sound, Artifact
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
- en: '| 656 | 1.194 | unknown | Dataset B: Normal, Murmur, Extrasystole |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
- en: '| PhysioNet/CinC Database [[58](#bib.bib58)] | PhysioNet/CinC Challenge 2016 [[143](#bib.bib143)]
    | 3,240 | 20.216 | 764+ | Normal, Abnormal, Too noisy or ambiguous |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
- en: '| HSS [[92](#bib.bib92)] | ComParE Challenge 2018 [[82](#bib.bib82)] | 845
    | 7.047 | 170 | Normal, Mild, Moderate/Severe |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
- en: '| Data on GitHub[[144](#bib.bib144)] | – | 1,000 | 0.679 | unknown | Normal,
    AS, MR, MS, MVP |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
- en: '| Michigan Heart sound database⁵⁵5[https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library](https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library)[https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/primer_heartsound.html](https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/primer_heartsound.html)
    | – | 23 | 0.413 | unknown | Normal, Pathological |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
- en: '| CirCor DigiScope Database [[145](#bib.bib145)] | George B. Moody PhysioNet
    Challenge 2022 [[146](#bib.bib146)] | 3,163 | 20.094 | 942 | Normal, abnormal;
    presence, absence, or unclear cases of murmurs |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Evaluation metrics. $k$ is the $k$-th class of all $K$ classes; $N_{k}$,
    $\hat{N_{k}}$, and $\tilde{N_{k}}$ are the total number of samples, the total
    number of the predicted samples, and the number of correctly predicted samples
    for the $k$-th class, respectively. $N$ is the total number of all samples. TPR:
    true positive rate, FPR: false positive rate, AUC: area under curve, ROC: receiver
    operating characteristic, PR: precision-recall.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation Metric | Description | Formula |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
- en: '| Sensitivity | Proportion of the actual positive cases which are correctly
    classified, i. e., true positive rate (TPR) | $\frac{\mbox{True Positives}}{\mbox{True
    Positives}+\mbox{False Negatives}}$ |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: '| Specificity | Proportion of the actual negative cases which are correctly
    classified, i. e., true negative rate (TNR) | $\frac{\mbox{True Negatives}}{\mbox{True
    Negatives}+\mbox{False Positives}}$ |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
- en: '| Mean Accuracy | Arithmetic mean of sensitivity and specificity | $\frac{\mbox{sensitivity}+\mbox{specificity}}{2}$
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: '| Youden’s index | A measure of the ability to balance sensitivity and specificity
    | $\mbox{sensitivity}+\mbox{specificity}-1$ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
- en: '| Precision | Proportion of correctly classified cases based on all cases predicted
    into $k$ | $\frac{\tilde{N_{k}}}{\hat{N_{k}}}$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
- en: '| Recall | Proportion of correctly classified cases based on all cases in the
    class $k$. | $\frac{\tilde{N_{k}}}{N_{k}}$ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
- en: '| UAR | Unweighted average recall | $\frac{\sum_{k=1}^{K}\mbox{recall}_{k}}{K}$
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
- en: '| WAR | Weighted average recall, i. e., accuracy | $\sum_{k=1}^{K}\frac{N_{k}}{N}\mbox{recall}_{k}$
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
- en: '| F1-Score | Harmonic mean of precision and recall | $2\times\frac{\mbox{precision}\times\mbox{recall}}{\mbox{precision}+\mbox{recall}}$
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
- en: '| AUC-ROC | The area under the probability curve plotting TPR against FPR at
    various probability thresholds | —— |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
- en: '| AUC-PR | The area under the probability curve plotting precision against
    recall at various probability thresholds | —— |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: '| Average precision | Summarisation of a precision-recall curve as the weighted
    mean of precisions at each threshold | $\sum_{n}(\mbox{recall}_{n}-\mbox{recall}_{n-1})\times\mbox{precision}_{n}$
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
- en: 5.2 Evaluation Metrics
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 4](#S5.T4 "Table 4 ‣ 5.1 Published Datasets ‣ 5 Published Resources
    on Heart Sound Analysis ‣ A Comprehensive Survey on Heart Sound Analysis in the
    Deep Learning Era") summarises evaluation metrics that can be used for evaluating
    model performance on heart sound classification.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Class-wise evaluation metrics. In binary classification tasks, sensitivity and
    specificity focus on calculating the proportion of correctly classified positive
    samples and negative samples, respectively. In order to balance sensitivity and
    specificity, mean accuracy and Youden’s index are utilised. As for evaluating
    a model’s performance on a specific class in a multi-class classification task,
    precision and recall are frequently used. For a specific class, precision calculates
    the proportion of correctly classified cases based on all cases predicted to this
    class, whereas recall quantifies the proportion of correctly classified cases
    based on all cases in the class. Usually, there is an inverse relation between
    precision and recall. Therefore, their combinations (e. g., F1-Score, average
    precision) are referred to to better evaluate the performance of a model. Notably,
    in multi-class classification, recall is an extension of sensitivity.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class evaluation metrics. Besides the class-wise evaluation, there are
    other metrics across all classes, such as weighted average recall (WAR) and unweighted
    average recall (UAR). WAR, also known as accuracy and as the name implies, tends
    to overestimate a model if it performs better on the majority classes. Therefore,
    with an imbalanced dataset, UAR is preferred. UAR is the mean recall across classes
    w/o instance-based weighting. With various probability decision thresholds, we
    can also plot AUC-ROC and precision-recall curves. In addition, we would like
    to mention the cost-based metric devised in the PhysioNet Challenge 2022 [[146](#bib.bib146)],
    where cost calculation involves model pre-screening, expert screening, treatment,
    and diagnostic errors. In this way, more clinically practical models can be developed,
    especially in resource-constrained scenarios.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Published Algorithms
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We discuss publicly available codes in the papers discussed in Section [4.2](#S4.SS2
    "4.2 Deep Learning for Classification ‣ 4 State-of-the-art Studies ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era") as well as other papers
    on open-access repositories during 2017–2022\. There are only a few publicly available
    codes in the past years, while there are many more released codes in 2016 benefiting
    from the PhysioNet challenge 2016 [[58](#bib.bib58)]. Notably, codes in 2016 are
    not listed, as we want to list the most state-of-the-art studies in the past six
    years.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: In [[144](#bib.bib144)], the authors released a Matlab code⁶⁶6https://github.com/yaseen21khan/Classification-of-Heart-Sound-Signal-Using-Multiple-Features-
    of training machine learning deep neural networks with inputs of multiple features,
    including MFCCs and discrete wavelets transform. In [[131](#bib.bib131)], a CNN
    model with time-convolutional units which simulate finite impulse response filters
    was implemented with Python in the code⁷⁷7https://github.com/mhealthbuet/heartnet.
    ResNets on linear and logarithmic spectrogram-image features were implemented
    in the Python code⁸⁸8https://github.com/mHealthBuet/CepsNET of the study [[147](#bib.bib147)].
    A study [[148](#bib.bib148)] released a Matlab code⁹⁹9https://github.com/uit-hdl/heart-sound-classification
    for detecting valvular heart disease from heart sounds and echocardiograms.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future Research Directions and Open Issues
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Findings
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In classification tasks which mainly focus on the screening of heart diseases,
    segmentation was often considered as a preprocessing procedure before classifying
    heart sounds. It is still an open question whether segmentation is helpful for
    classification or not.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation + Classification. Many studies applied segmentation techniques
    or given segmentation information before the classification procedure. For instance,
    segmented cardiac cycles were used as the input of DL models in [[131](#bib.bib131)].
    Clips starting from the S1 heart sound with a fixed 1.6 s length were used for
    classification in [[129](#bib.bib129)]. In [[149](#bib.bib149)], the importance
    of segmentation was demonstrated for abnormal heart sound detection. Compared
    to models without segmentation information, segmentation did not significantly
    improve the model performance in the experiments [[149](#bib.bib149)]. The reason
    can be that models have already been powerful and robust, therefore, segmentation
    can be automatically done by the intermediate models layers. The authors in [[149](#bib.bib149)]
    proved this by explaining models with SHapley Additive exPlanations (SHAP) [[150](#bib.bib150)].
    The contribution of S1 and S2 sounds was found to be larger than other clips in
    a heart sound segment. Therefore, segmentation is required either as an additional
    procedure for classifiers that are not very powerful, or as an internal procedure
    in robust classifiers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: No-segment. There are also many approaches which proposed to use non-segmented
    heart sounds, therefore, the complexity of automated auscultation can be reduced [[10](#bib.bib10),
    [52](#bib.bib52), [44](#bib.bib44), [90](#bib.bib90), [115](#bib.bib115)]. Apart
    from feeding a complete heart sound sample into neural networks, heart sounds
    are segmented into shorter clips with an equal length for model training [[50](#bib.bib50)].
    For instance, the fist 5 s of each audio sample were selected as the model input
    in [[7](#bib.bib7), [85](#bib.bib85), [93](#bib.bib93), [37](#bib.bib37)], and
    segmented 5 s clips were also used in [[91](#bib.bib91), [88](#bib.bib88), [116](#bib.bib116)].
    Most studies employ audio clips with a time length from 2 s to 6 s [[45](#bib.bib45),
    [34](#bib.bib34), [49](#bib.bib49), [51](#bib.bib51)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Limitations and Outlook
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hardware development. In clinics, echocardiography obtains ultrasound scan with
    a small probe which emits high-frequency sound waves, therefore, physicians can
    diagnose via observing the heart and blood vessels, as well as blood flows^(10)^(10)10https://www.nhs.uk/conditions/echocardiogram/.
    However, echocardiography requires well-trained skills for professionals, limiting
    its usage in primary care. Classic acoustic stethoscopes used in primary care
    need physicians and nurses to be trained as well. To this end, there is a high
    demand of electronic stethoscopes in primary care. In recent years, electronic
    stethoscopes have been developed and produced to record heart sounds and transmit
    heart sounds to computers or mobile phones for further analysis [[26](#bib.bib26)].
    Most electronic stethoscopes can only achieve basic functions such as amplifying
    and visualising heart sounds without diagnosis. There are a few studies and hardware
    working on automated diagnosis more recently. For instance, a field-programmable
    gate array (FPGA) was designed to classify heart sounds via an LSTM-RNN model
    in [[151](#bib.bib151)]. “HD Steth with ECG”^(11)^(11)11https://www.stethoscope.com/hd-steth-with-ecg/
    embedded artificial intelligence (AI) into the electronic stethoscope to detect
    multiple cardiac abnormalities. As outlined throughout this overview, AI is promising
    to diagnose heart sound abnormalities, therefore reducing the requirement of well-trained
    professionals. Devices which diagnose cardiac diseases with high accuracies will
    be very helpful for promoting the early screening of cardiac diseases to primary
    care and home care.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Performance improvement. Although automated auscultation is ideally expected
    to replace human analysis, model performance can be a bottleneck for applying
    automated auscultation to clinical usages. False negative predictions can result
    in delayed or missed therapies and aggravated condition. In future efforts, i)
    automated auscultation will be required to achieve very good performance and consider
    the difference between individuals in the context of personalised healthcare.
    The current research studies are mostly based on heart sounds only, while many
    types of individual information such as age can affect model performance [[152](#bib.bib152)].
    Therefore, how to integrate individual information will be a question for performance
    improvement. ii) In terms of ML and DL, we are currently witnessing the advent
    and adoption of foundation models [[153](#bib.bib153)] (pre-)trained on large
    to big data. We have already seen and listed above several approaches using pre-trained
    often self-supervised learnt models. However, one can expect even bigger models
    to appear with the potential emergence of abilities directly related to heart
    sound analysis as a ‘downstream’ task. On the other hand, the upcoming era of
    foundation models is expected to be marked by homogenisation, and it remains to
    be seen if the diversity of heart sound analysis approaches reported herein will
    indeed converge to a few large data trained models over the next years [[153](#bib.bib153)].
    iii) Furthermore, human-machine collaboration will be very promising to improve
    the system performance and provide more accurate diagnosis and in-time treatments
    for patients. Human-machine classification can be a solution to combine both machines’
    predictions and human’s (crowd workers’ and experts’) predictions for more precise
    diagnosis [[154](#bib.bib154)]. In [[154](#bib.bib154)], data samples predicted
    with high uncertainties were sent to crowd workers for majority voting; similarly,
    samples will be sent to an expert according to a certainty threshold of predictions
    from the crowd.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Interpretable, Dependable, and Actionable Deep Heart Sound Analysis
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Explainable deep learning has been emerging as one of the key topics in many
    applications, especially in the healthcare area. Building explainable deep learning
    models can help physicians and patients to trust the predictions. In a past study [[155](#bib.bib155)],
    an attention mechanism was used to visualise the contribution of each feature
    unit to the final predictions of heart sounds. Similarly, SHAP was also used to
    compute the contribution of each feature unit in [[150](#bib.bib150)]. The above
    explanation methods are both local explanations that interpret DL models case-by-case,
    lacking of a global explanation that can reveal the hidden classification rule
    in the models or summarise the characteristics of each heart sound class. Explainable
    DL models, such as deep neural decision trees [[156](#bib.bib156)], are promising
    to explain the models themselves from the perspectives of structure. Learnt or
    searched data samples of prototypes, criticisms, and counterfactuals [[106](#bib.bib106),
    [157](#bib.bib157)] can present the typical characteristics of each class, therefore,
    physicians can compare new samples with these heart sounds for better understanding
    and analysis. Specifically, by analysing the patterns of criticisms, physicians
    can potentially decrease the number of false negatives, which is crucial in the
    healthcare field. More recently, sonification was proposed to explain DL models
    for better human-computer interaction in [[158](#bib.bib158)]. Apart from visualisation,
    sonification can provide a new perspective to explain models by listening.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: In addition, given the health implications, it appears crucial that heart sound
    analysis by AI is utmost dependable [[159](#bib.bib159)]. There are mechanisms
    available, but more adaptation to the field of application if not novel algorithms
    will need to be designed. Ultimately, dependability will be a major driving factor
    for the trustability of according heart monitoring solutions – applied in everyday
    situations, trustability is a key factor to win users.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, to enable DL models being actionable in real-life, data privacy has
    been another emerging research topic in protecting users’ data from leakage or
    external attacks. Machine unlearning [[160](#bib.bib160)] and federated learning
    methods [[161](#bib.bib161)] can help healthcare institutions better organise
    patients’ private data in a secure way without losing diagnosis accuracy. Further,
    AI attacks on AI for heart sound analysis could be thought off, such as by adversarial
    attacks, and needs to dealt with. In summary, DL models are promising to guide
    healthcare providers’ actions in their daily practices for providing better care
    for patients. It will be essential to improve DL models from not only the performance,
    but also the human-centred perspectives in future.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we summarised both classic machine learning and deep learning
    technologies applied to heart sound analysis during 2017–2022, including denoising,
    segmentation, and classification. Databases available for these above tasks were
    introduced with evaluation metrics in our study. We also listed publicly released
    codes for implementation of heart sound classification. Additionally, several
    findings and limitations of heart sound classification were analysed and possible
    future works were discussed. Finally, we discussed the importance of heart sound
    interpretation in the context of deep learning. This work is expected to present
    a summary of the advances of heart sound analysis, provide helpful discussions,
    and point out promising research directions that are helpful for the community.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] U. Alam, O. Asghar, S. Q. Khan, S. Hayat, and R. A. Malik, “Cardiac auscultation:
    an essential clinical skill in decline,” British Journal of Cardiology, vol. 17,
    no. 1, p. 8, 2010.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] I. R. Hanna and M. E. Silverman, “A history of cardiac auscultation and
    some of its contributors,” The American journal of cardiology, vol. 90, no. 3,
    pp. 259–267, 2002.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. M. Noor and M. F. Shadi, “The heart auscultation. from sound to graphical,”
    Journal of Engineering and Technology, vol. 4, no. 2, pp. 73–84, 2013.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] H. Tang, M. Wang, Y. Hu, B. Guo, and T. Li, “Automated signal quality assessment
    for heart sound signal by novel features and evaluation in open public datasets,”
    BioMed Research International, vol. 30, pp. 1–15, 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] L. R. Rabiner, “A tutorial on hidden markov models and selected applications
    in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2, pp. 257–286,
    1989.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] B. M. Whitaker, P. B. Suresha, C. Liu, G. D. Clifford, and D. V. Anderson,
    “Combining sparse coding and time-domain features for heart sound classification,”
    Physiological measurement, vol. 38, no. 8, p. 1701, 2017.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] P. Langley and A. Murray, “Heart sound classification from unsegmented
    phonocardiograms,” Physiological measurement, vol. 38, no. 8, p. 1658, 2017.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Z. Ren, N. Cummins, V. Pandit, J. Han, K. Qian, and B. Schuller, “Learning
    image-based representations for heart sound classification,” in Proc. DH, (Lyon,
    France), pp. 143–147, 2018.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] T. H. Chowdhury, K. N. Poudel, and Y. Hu, “Time-frequency analysis, denoising,
    compression, segmentation, and classification of pcg signals,” IEEE Access, vol. 8,
    pp. 160882–160890, 2020.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] B. Xiao, Y. Xu, X. Bi, J. Zhang, and X. Ma, “Heart sounds classification
    using a novel 1-D convolutional neural network with extremely low parameter consumption,”
    Neurocomputing, vol. 392, pp. 153–159, 2020.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. K. Bhoi, K. S. Sherpa, and B. Khandelwal, “Multidimensional analytical
    study of heart sounds: A review,” International Journal Bioautomation, vol. 19,
    no. 3, pp. 351–376, 2015.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] T. Chakrabarti, S. Saha, S. Roy, and I. Chel, “Phonocardiogram signal
    analysis-practices, trends and challenges: A critical review,” in Proc. IEMCON,
    (Vancouver, Canada), pp. 1–4, 2015.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Nabih-Ali, E.-S. A. El-Dahshan, and A. S. Yahia, “A review of intelligent
    systems for heart sound signal analysis,” Journal of medical engineering & technology,
    vol. 41, no. 7, pp. 553–563, 2017.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] G. D. Clifford et al., “Recent advances in heart sound analysis,” Physiological
    measurement, vol. 38, pp. E10–E25, 2017.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. K. Ghosh, P. R. Nagarajan, and R. K. Tripathy, Heart sound data acquisition
    and preprocessing techniques: A review. IGI Global, 2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] B. Majhi and A. Kashyap, “Application of soft computing techniques to
    heart sound classification: A review of the decade,” Soft Computing Applications
    and Techniques in Healthcare, pp. 113–138, 2020.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. K. Dwivedi, S. A. Imtiaz, and E. Rodriguez-Villegas, “Algorithms for
    automatic analysis and classification of heart sounds–a systematic review,” IEEE
    Access, vol. 7, pp. 8316–8345, 2018.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. J. Muñoz-Montoro, D. Suarez-Dou, R. Cortina, F. J. Cañadas-Quesada,
    and E. F. Combarro, “Parallel source separation system for heart and lung sounds,”
    The Journal of Supercomputing, vol. 77, no. 8, pp. 8135–8150, 2021.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] K.-H. Tsai, W.-C. Wang, C.-H. Cheng, C.-Y. Tsai, J.-K. Wang, T.-H. Lin,
    S.-H. Fang, L.-C. Chen, and Y. Tsao, “Blind monaural source separation on heart
    and lung sounds based on periodic-coded deep autoencoder,” IEEE Journal of Biomedical
    and Health Informatics, vol. 24, no. 11, pp. 3203–3214, 2020.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] E. Grooby, J. He, D. Fattahi, L. Zhou, A. King, A. Ramanathan, A. Malhotra,
    G. A. Dumont, and F. Marzbanrad, “A new non-negative matrix co-factorisation approach
    for noisy neonatal chest sound separation,” in 2021 43rd Annual International
    Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 5668–5673,
    2021.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] C. Ahlström, Processing of the Phonocardiographic Signal: methods for
    the intelligent stethoscope. PhD thesis, Institutionen för medicinsk teknik, 2006.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Naseri and M. Homaeinezhad, “Detection and boundary identification
    of phonocardiogram sounds using an expert frequency-energy based metric,” Annals
    of biomedical engineering, vol. 41, no. 2, pp. 279–292, 2013.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] D. S. Gerbarg, A. Taranta, M. Spagnuolo, and J. J. Hofler, “Computer analysis
    of phonocardiograms,” Progress in Cardiovascular Diseases, vol. 5, pp. 393–405,
    1963.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R. J. Martis, U. R. Acharya, and H. Adeli, “Current methods in electrocardiogram
    characterization,” Computers in biology and medicine, vol. 48, pp. 133–149, 2014.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] P. S. Molcer, I. Kecskes, V. Delić, E. Domijan, and M. Domijan, “Examination
    of formant frequencies for further classification of heart murmurs,” in Proc. SISY,
    (Subotica, Serbia), pp. 575–578, 2010.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Leng, R. San Tan, K. T. C. Chai, C. Wang, D. Ghista, and L. Zhong,
    “The electronic stethoscope,” Biomedical engineering online, vol. 14, no. 1, pp. 1–37,
    2015.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] E. Delgado-Trejos, A. Quiceno-Manrique, J. Godino-Llorente, M. Blanco-Velasco,
    and G. Castellanos-Dominguez, “Digital auscultation analysis for heart murmur
    detection,” Annals of biomedical engineering, vol. 37, no. 2, pp. 337–353, 2009.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Tsao, T.-H. Lin, F. Chen, Y.-F. Chang, C.-H. Cheng, and K.-H. Tsai,
    “Robust s1 and s2 heart sound recognition based on spectral restoration and multi-style
    training,” Biomedical Signal Processing and Control, vol. 49, pp. 173–180, 2019.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] O. Deperlioglu, “Classification of segmented phonocardiograms by convolutional
    neural networks,” BRAIN. Broad Research in Artificial Intelligence and Neuroscience,
    vol. 10, no. 2, pp. 5–13, 2019.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Ö. DEPERLİĞLU, “Classification of segmented heart sounds with artificial
    neural networks,” International Journal of Applied Mathematics Electronics and
    Computers, vol. 6, no. 4, pp. 39–44, 2018.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] H. Alaskar, N. Alzhrani, A. Hussain, and F. Almarshed, “The implementation
    of pretrained AlexNet on PCG classification,” in Proc. ICIC, (San Sebastian, Basque
    Country), pp. 784–794, 2019.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] N. M. Khan, M. S. Khan, and G. M. Khan, “Automated heart sound classification
    from unsegmented phonocardiogram signals using time frequency features,” International
    Journal of Computer and Information Engineering, vol. 12, no. 8, pp. 598–603,
    2018.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. Yadav, M. K. Dutta, C. M. Travieso, and J. B. Alonso, “Automatic classification
    of normal and abnormal PCG recording heart sound recording using Fourier transform,”
    in Proc. IWOBI, (Alajuela Province, Costa Rica), pp. 1–9, 2018.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Q. Hu, J. Hu, X. Yu, and Y. Liu, “Automatic heart sound classification
    using one dimension deep neural network,” in International Conference on Security,
    Privacy and Anonymity in Computation, Communication and Storage, pp. 200–208,
    2020.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] S. K. Ghosh, R. K. Tripathy, R. Ponnalagu, and R. B. Pachori, “Automated
    detection of heart valve disorders from the PCG signal using time-frequency magnitude
    and phase features,” IEEE Sensors Letters, vol. 3, no. 12, pp. 1–4, 2019.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] B. Ahmad, F. A. Khan, K. N. Khan, and M. S. Khan, “Automatic classification
    of heart sounds using long short-term memory,” in Proc. ICOSST, (Virtual Event),
    pp. 1–6, 2021.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. A. Singh, T. G. Meitei, and S. Majumder, “Short pcg classification
    based on deep learning,” in Deep Learning Techniques for Biomedical and Health
    Informatics, pp. 141–164, Elsevier, 2020.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] F. Noman, S.-H. Salleh, C.-M. Ting, S. B. Samdin, H. Ombao, and H. Hussain,
    “A markov-switching model approach to heart sound segmentation and classification,”
    IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 3, pp. 705–716,
    2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] F. Noman, C.-M. Ting, S.-H. Salleh, and H. Ombao, “Short-segment heart
    sound classification using an ensemble of deep convolutional neural networks,”
    in Proc. ICASSP, (Brighton, UK), pp. 1318–1322, 2019.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Dastagir, F. A. Khan, M. S. Khan, and K. N. Khan, “Computer-aided phonocardiogram
    classification using multidomain time and frequency features,” in Proc. ICAI,
    (Islamabad, Pakistan), pp. 50–55, 2021.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] A. Bourouhou, A. Jilbab, C. Nacir, and A. Hammouch, “Heart sound signals
    segmentation and multiclass classification,” International Journal of Online and
    Biomedical Engineering, vol. 16, no. 15, pp. 64–79, 2020.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Meintjes, A. Lowe, and M. Legget, “Fundamental heart sound classification
    using the continuous wavelet transform and convolutional neural networks,” in
    Proc. EMBC, (Honolulu, Hawaii), pp. 409–412, 2018.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Z. Abduh, E. A. Nehary, M. A. Wahed, and Y. M. Kadah, “Classification
    of heart sounds using fractional fourier transform based mel-frequency spectral
    coefficients and traditional classifiers,” Biomedical Signal Processing and Control,
    vol. 57, p. 101788, 2020.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. A. Singh, S. Majumder, and M. Mishra, “Classification of short unsegmented
    heart sound based on deep learning,” in Proc. I2MTC, (Auckland, New Zealand),
    pp. 1–6, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] S. Li, F. Li, S. Tang, and F. Luo, “Heart sounds classification based
    on feature fusion using lightweight neural networks,” IEEE Transactions on Instrumentation
    and Measurement, vol. 70, pp. 1–9, 2021.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] N. Ibrahim, N. Jamal, M. N. A.-H. Sha’abani, and L. F. Mahadi, “A comparative
    study of heart sound signal classification based on temporal, spectral and geometric
    features,” in Proc. IECBES, (Langkawi, Malaysia), pp. 24–29, 2020.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] W. Zhang, J. Han, and S. Deng, “Heart sound classification based on scaled
    spectrogram and tensor decomposition,” Expert Systems with Applications, vol. 84,
    pp. 220–231, 2017.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] W. Zhang, J. Han, and S. Deng, “Heart sound classification based on scaled
    spectrogram and partial least squares regression,” Biomedical Signal Processing
    and Control, vol. 32, pp. 20–28, 2017.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Banerjee and S. Majhi, “Multi-class heart sounds classification using
    2D-convolutional neural network,” in Proc. ICCCS, (Guangzhou, China), pp. 1–6,
    2020.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. M.-T. Wu, M.-H. Tsai, Y. Z. Huang, S. H. Islam, M. M. Hassan, A. Alelaiwi,
    and G. Fortino, “Applying an ensemble convolutional neural network with Savitzky–Golay
    filter to construct a phonocardiogram prediction model,” Applied Soft Computing,
    vol. 78, pp. 29–40, 2019.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] P. T. Krishnan, P. Balasubramanian, and S. Umapathy, “Automated heart
    sound classification system from unsegmented phonocardiogram (pcg) using deep
    neural network,” Physical and Engineering Sciences in Medicine, vol. 43, no. 2,
    pp. 505–515, 2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] W. Zeng, J. Yuan, C. Yuan, Q. Wang, F. Liu, and Y. Wang, “A new approach
    for the detection of abnormal heart sound signals using TQWT, VMD and neural networks,”
    Artificial Intelligence Review, vol. 54, no. 3, pp. 1613–1647, 2021.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. V. Shervegar and G. V. Bhat, “Heart sound classification using gaussian
    mixture model,” Porto Biomedical Journal, vol. 3, no. 1, pp. 1–7, 2018.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] B. Al-Naami, H. Fraihat, N. Y. Gharaibeh, and A.-R. M. Al-Hinnawi, “A
    framework classification of heart sound signals in physionet challenge 2016 using
    high order statistics and adaptive neuro-fuzzy inference system,” IEEE Access,
    vol. 8, pp. 224852–224859, 2020.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. I. Humayun, M. Khan, S. Ghaffarzadegan, Z. Feng, T. Hasan, et al.,
    “An ensemble of transfer, semi-supervised and supervised learning methods for
    pathological heart sound classification,” in Proc. INTERSPEECH, (Hyderabad, India),
    pp. 127–131, 2018.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Boll, “Suppression of acoustic noise in speech using spectral subtraction,”
    IEEE Transactions on acoustics, speech, and signal processing, vol. 27, no. 2,
    pp. 113–120, 1979.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] V. N. Varghees and K. Ramachandran, “Effective heart sound segmentation
    and murmur classification using empirical wavelet transform and instantaneous
    phase for electronic stethoscope,” IEEE Sensors Journal, vol. 17, no. 12, pp. 3861–3872,
    2017.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] C. Liu, D. Springer, et al., “An open access database for the evaluation
    of heart sound algorithms,” Physiological Measurement, vol. 37, no. 12, p. 2181,
    2016.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] M. Baydoun, L. Safatly, H. Ghaziri, and A. El Hajj, “Analysis of heart
    sound anomalies using ensemble learning,” Biomedical Signal Processing and Control,
    vol. 62, p. 102019, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] P. Upretee and M. E. Yüksel, “Accurate classification of heart sounds
    for disease diagnosis by a single time-varying spectral feature: Preliminary results,”
    in Proc. EBBT, (Istanbul, Turkey), pp. 1–4, 2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] J. X. Low and K. W. Choo, “Classification of heart sounds using softmax
    regression and convolutional neural network,” in Proc. ICCET, (New York, NY),
    pp. 18–21, 2018.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] O. Deperlioglu, “Heart sound classification with signal instant energy
    and stacked autoencoder network,” Biomedical Signal Processing and Control, vol. 64,
    p. 102211, 2021.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] G. Eslamizadeh and R. Barati, “Heart murmur detection based on wavelet
    transformation and a synergy between artificial neural network and modified neighbor
    annealing methods,” Artificial intelligence in medicine, vol. 78, pp. 23–40, 2017.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] M. U. Akram, A. Shaukat, F. Hussain, S. G. Khawaja, W. H. Butt, et al.,
    “Analysis of PCG signals using quality assessment and homomorphic filters for
    localization and classification of heart sounds,” Computer methods and programs
    in biomedicine, vol. 164, pp. 143–157, 2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] M. V. Shervegar and G. V. Bhat, “Automatic segmentation of phonocardiogram
    using the occurrence of the cardiac events,” Informatics in Medicine Unlocked,
    vol. 9, pp. 6–10, 2017.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. Das, S. Pal, and M. Mitra, “Acoustic feature based unsupervised approach
    of heart sound event detection,” Computers in Biology and Medicine, vol. 126,
    p. 103990, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] S. E. Schmidt, C. Holst-Hansen, C. Graff, E. Toft, and J. J. Struijk,
    “Segmentation of heart sound recordings by a duration-dependent hidden markov
    model,” Physiological measurement, vol. 31, no. 4, p. 513, 2010.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] S. Shukla, S. K. Singh, and D. Mitra, “An efficient heart sound segmentation
    approach using kurtosis and zero frequency filter features,” Biomedical Signal
    Processing and Control, vol. 57, p. 101762, 2020.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. P. Kamson, L. Sharma, and S. Dandapat, “Multi-centroid diastolic duration
    distribution based HSMM for heart sound segmentation,” Biomedical Signal Processing
    and Control, vol. 48, pp. 265–272, 2019.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] J. Oliveira, F. Renna, and M. Coimbra, “A subject-driven unsupervised
    hidden semi-Markov model and Gaussian mixture model for heart sound segmentation,”
    IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 2, pp. 323–331,
    2019.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J. Oliveira, F. Renna, T. Mantadelis, and M. Coimbra, “Adaptive sojourn
    time HSMM for heart sound segmentation,” IEEE Journal of Biomedical and Health
    Informatics, vol. 23, no. 2, pp. 642–649, 2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] J. Rubin, R. Abreu, A. Ganguli, S. Nelaturi, I. Matei, and K. Sricharan,
    “Recognizing abnormal heart sounds using deep learning,” arXiv preprint arXiv:1707.04642,
    2017.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] V. Maknickas and A. Maknickas, “Recognition of normal–abnormal phonocardiographic
    signals using deep convolutional neural networks and mel-frequency spectral coefficients,”
    Physiological measurement, vol. 38, no. 8, p. 1671, 2017.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] K. Liu, L. Yuan, C. Huang, W. Wu, Q. Wang, and G. Wu, “Abnormal heart
    sound detection by using temporal convolutional network,” in Proc. IPEC, (Potsdam,
    Germany), pp. 1026–1029, 2022.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] F. A. Khan, A. Abid, and M. S. Khan, “Automatic heart sound classification
    from segmented/unsegmented phonocardiogram signals using time and frequency features,”
    Physiological measurement, vol. 41, no. 5, p. 055006, 2020.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Y. Chen, S. Wei, and Y. Zhang, “Classification of heart sounds based on
    the combination of the modified frequency wavelet transform and convolutional
    neural network,” Medical & Biological Engineering & Computing, vol. 58, no. 9,
    pp. 2039–2047, 2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] W. Han, Z. Yang, J. Lu, and S. Xie, “Supervised threshold-based heart
    sound classification algorithm,” Physiological Measurement, vol. 39, no. 11, p. 115011,
    2018.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] W. Han, S. Xie, Z. Yang, S. Zhou, and H. Huang, “Heart sound classification
    using the SNMFNet classifier,” Physiological measurement, vol. 40, no. 10, p. 105003,
    2019.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] J. F. Chen and X. Dang, “Heart sound analysis based on extended features
    and related factors,” in Proc. SSCI, (Xiamen, China), pp. 2189–2194, 2019.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] D. B. Springer, L. Tarassenko, and G. D. Clifford, “Logistic regression-hsmm-based
    heart sound segmentation,” IEEE transactions on biomedical engineering, vol. 63,
    no. 4, pp. 822–832, 2015.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] A. Duggento, A. Conti, M. Guerrisi, and N. Toschi, “Classification of
    real-world pathological phonocardiograms through multi-instance learning,” in
    Proc. EMBC, (Virtual Event), pp. 771–774, 2021.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] B. Schuller et al., “The INTERSPEECH 2018 computational paralinguistics
    challenge: Atypical & self-assessed affect, crying & heart beats,” in Proc. INTERSPEECH,
    (Hyderbad, India), pp. 122–126, 2018.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] F. Eyben, K. R. Scherer, B. W. Schuller, et al., “The geneva minimalistic
    acoustic parameter set (gemaps) for voice research and affective computing,” IEEE
    transactions on affective computing, vol. 7, no. 2, pp. 190–202, 2015.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] F. Eyben, M. Wöllmer, and B. Schuller, “Opensmile: The munich versatile
    and fast open-source audio feature extractor,” in Proc. ACM Multimedia, (Firenze,
    Italy), pp. 1459–1462, 2010.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. A. Singh and S. Majumder, “Classification of unsegmented heart sound
    recording using KNN classifier,” Journal of Mechanics in Medicine and Biology,
    vol. 19, no. 04, p. 1950025, 2019.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] E. Soares, P. Angelov, and X. Gu, “Autonomous learning multiple-model
    zero-order classifier for heart sound classification,” Applied Soft Computing,
    vol. 94, p. 106449, 2020.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] S. Khaled, M. Fakhry, H. Esmail, A. Ezzat, and E. Hamad, “Analysis of
    training optimisation algorithms in the NARX neural network for classification
    of heart sound signals,” International Journal of Scientific & Engineering Research,
    vol. 13, no. 2, pp. 382–390, 2022.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] V. Arora, R. Leekha, R. Singh, and I. Chana, “Heart sound classification
    using machine learning and phonocardiogram,” Modern Physics Letters B, vol. 33,
    no. 26, p. 1950321, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] M. Sotaquirá, D. Alvear, and M. Mondragón, “Phonocardiogram classification
    using deep neural networks and weighted probability comparisons,” Journal of medical
    engineering & technology, vol. 42, no. 7, pp. 510–517, 2018.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. M. Alqudah, “Towards classifying non-segmented heart sound records
    using instantaneous frequency based features,” Journal of medical engineering
    & technology, vol. 43, no. 7, pp. 418–430, 2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Tan, Z. Wang, K. Qian, B. Hu, S. Zhao, B. W. Schuller, and Y. Yamamoto,
    “Heart sound classification based on fractional fourier transformation entropy,”
    in Proc. LifeTech, (Osaka, Japan), pp. 588–589, 2022.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] F. Dong, K. Qian, Z. Ren, A. Baird, X. Li, Z. Dai, B. Dong, F. Metze,
    Y. Yamamoto, and B. Schuller, “Machine listening for heart status monitoring:
    Introducing and benchmarking HSS – the heart sounds shenzhen corpus,” IEEE Journal
    of Biomedical and Health Informatics, vol. 24, pp. 2082–2092, Nov. 2019.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Chen, X. Dang, and M. Li, “Heart sound classification method based
    on ensemble learning,” in Proc. ICSP, (Beijing, China), pp. 8–13, 2022.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] M. Rahmandani, H. A. Nugroho, and N. A. Setiawan, “Cardiac sound classification
    using Mel-frequency cepstral coefficients (MFCC) and artificial neural network
    (ANN),” in Proc. ICITISEE, (Yogyakarta, Indonesia), pp. 22–26, 2018.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] M. Adiban, B. BabaAli, and S. Shehnepoor, “Statistical feature embedding
    for heart sound classification,” Journal of Electrical Engineering, vol. 70, no. 4,
    pp. 259–272, 2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Li, L. Ke, Q. Du, X. Ding, X. Chen, and D. Wang, “Heart sound signal
    classification algorithm: A combination of wavelet scattering transform and twin
    support vector machine,” IEEE Access, vol. 7, pp. 179339–179348, 2019.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] N. Mei, H. Wang, Y. Zhang, F. Liu, X. Jiang, and S. Wei, “Classification
    of heart sounds based on quality assessment and wavelet scattering transform,”
    Computers in Biology and Medicine, vol. 137, p. 104814, 2021.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] N. K. Sawant, S. Patidar, N. Nesaragi, and U. R. Acharya, “Automated detection
    of abnormal heart sound signals using Fano-factor constrained tunable quality
    wavelet transform,” Biocybernetics and Biomedical Engineering, vol. 41, no. 1,
    pp. 111–126, 2021.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] T. Tuncer, S. Dogan, R.-S. Tan, and U. R. Acharya, “Application of Petersen
    graph pattern technique for automated detection of heart valve diseases with PCG
    signals,” Information Sciences, vol. 565, pp. 91–104, 2021.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Amiriparian, M. Schmitt, N. Cummins, K. Qian, F. Dong, and B. Schuller,
    “Deep unsupervised representation learning for abnormal heart sound classification,”
    in Pro. EMBC, (Honolulu, Hawaii), pp. 4776–4779, 2018.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] M. E. Karar, S. H. El-Khafif, and M. A. El-Brawany, “Automated diagnosis
    of heart sounds using rule-based classification tree,” Journal of medical systems,
    vol. 41, no. 4, pp. 1–7, 2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] F. Plesinger, I. Viscor, J. Halamek, J. Jurco, and P. Jurak, “Heart sounds
    analysis using probability assessment,” Physiological measurement, vol. 38, no. 8,
    p. 1685, 2017.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] R. F. Ibarra-Hernández, N. Bertin, M. A. Alonso-Arévalo, and H. A. Guillén-Ramírez,
    “A benchmark of heart sound classification systems based on sparse decompositions,”
    in Proc. SIPAIM, vol. 10975, (Mazatlan, Mexico), pp. 26–38, 2018.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] A. Sofwan, I. Santoso, H. Pradipta, M. Arfan, et al., “Normal and murmur
    heart sound classification using linear predictive coding and k-nearest neighbor
    methods,” in Proc. ICICoS, (Semarang, Japan), pp. 1–5, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Z. Ren, J. Han, N. Cummins, and B. Schuller, “Enhancing transferability
    of black-box adversarial attacks via lifelong learning for speech emotion recognition
    models,” in Proc. INTERSPEECH, (Shanghai, China), pp. 496–500, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Z. Ren, T. T. Nguyen, and W. Nejdl, “Prototype learning for interpretable
    respiratory sound analysis,” in Proc. ICASSP, (Singapore), pp. 9087–9091, 2022.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] K. Qian, C. Janott, M. Schmitt, Z. Zhang, C. Heiser, W. Hemmert, Y. Yamamoto,
    and B. W. Schuller, “Can machine learning assist locating the excitation of snore
    sound? A review,” IEEE Journal of Biomedical and Health Informatics, vol. 25,
    no. 4, pp. 1233–1246, 2020.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] F. Renna, J. Oliveira, and M. T. Coimbra, “Deep convolutional neural
    networks for heart sound segmentation,” IEEE Journal of Biomedical and Health
    Informatics, vol. 23, no. 6, pp. 2435–2445, 2019.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] E. Messner, M. Zöhrer, and F. Pernkopf, “Heart sound segmentation—an
    event detection approach using deep recurrent neural networks,” IEEE Transactions
    on Biomedical Engineering, vol. 65, no. 9, pp. 1964–1974, 2018.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Y. Chen, J. Lv, Y. Sun, and B. Jia, “Heart sound segmentation via duration
    long–short term memory neural network,” Applied Soft Computing, vol. 95, p. 106540,
    2020.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] T. Fan, J. Zhu, Y. Cheng, Q. Li, D. Xue, and R. Munnoch, “A new Direct
    heart sound segmentation approach using bi-directional GRU,” in Proc. ICAC, (Newcastle,
    UK), pp. 1–5, 2018.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] T. Fernando, H. Ghaemmaghami, S. Denman, S. Sridharan, N. Hussain, and
    C. Fookes, “Heart sound segmentation using bidirectional LSTMs with attention,”
    IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 6, pp. 1601–1609,
    2020.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Y. Chen, Y. Sun, J. Lv, B. Jia, and X. Huang, “End-to-end heart sound
    segmentation using deep convolutional recurrent network,” Complex Intell. Syst.,
    vol. 7, p. 2103–2117, 2021.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] C. Xu, J. Zhou, L. Li, J. Wang, D. Ying, and Q. Li, “Heart sound segmentation
    based on SMGU-RNN,” in Proc. BIBE, (Hangzhou, China), pp. 126–132, 2019.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] S. Takezaki and K. Kishida, “Construction of cnns for abnormal heart
    sound detection using data augmentation,” in Proc. IMECS, (Hong Kong), pp. 1–6,
    2021.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] X. Cheng, J. Huang, Y. Li, and G. Gui, “Design and application of a laconic
    heart sound neural network,” IEEE Access, vol. 7, pp. 124417–124425, 2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. M. Alqudah, H. Alquran, and I. A. Qasmieh, “Classification of heart
    sound short records using bispectrum analysis approach images and deep learning,”
    Network Modeling Analysis in Health Informatics and Bioinformatics, vol. 9, no. 66,
    pp. 1–16, 2020.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] M. S. Wibawa, I. M. D. Maysanjaya, N. K. D. P. Novianti, and P. N. Crisnapati,
    “Abnormal heart rhythm detection based on spectrogram of heart sound using convolutional
    neural network,” in Proc. CITSM, (Parapat Nort Sumatera, Indonesia), pp. 1–4,
    2018.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] K. Ranipa, W.-P. Zhu, and M. Swamy, “Multimodal cnn fusion architecture
    with multi-features for heart sound classification,” in Proc. ISCAS, (Suseong-gu,
    Daegu), pp. 1–5, 2021.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] A. Balamurugan, S. G. Teo, J. Yang, Z. Peng, Y. Xulei, and Z. Zeng, “ResHNet:
    spectrograms based efficient heart sounds classification using stacked residual
    networks,” in Proc. BHI, (Chicago, IL), pp. 1–4, 2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. Deng, T. Meng, J. Cao, S. Wang, J. Zhang, and H. Fan, “Heart sound
    classification based on improved MFCC features and convolutional recurrent neural
    networks,” Neural Networks, vol. 130, pp. 22–32, 2020.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Z. Abduh, E. A. Nehary, M. A. Wahed, and Y. M. Kadah, “Classification
    of heart sounds using fractional fourier transform based mel-frequency spectral
    coefficients and stacked autoencoder deep neural network,” Journal of Medical
    Imaging and Health Informatics, vol. 9, no. 1, pp. 1–8, 2019.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] M. Fakhry and A. F. Brery, “A comparison study on training optimization
    algorithms in the bilstm neural network for classification of pcg signals,” in
    Proc. IRASET, (Meknes, Morocco), pp. 1–6, 2022.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] K. Qian, Z. Ren, F. Dong, W.-H. Lai, B. Schuller, and Y. Yoshiharu, “Deep
    wavelets for heart sound classification,” in Proc. ISPACS, (Taipei, Taiwan), 2019.
    2 pages.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] W. Zhang, J. Han, and S. Deng, “Abnormal heart sound detection using
    temporal quasi-periodic features and long short-term memory without segmentation,”
    Biomedical Signal Processing and Control, vol. 53, p. 101560, 2019.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] F. Li, M. Liu, Y. Zhao, L. Kong, L. Dong, X. Liu, and M. Hui, “Feature
    extraction and classification of heart sound using 1d convolutional neural networks,”
    EURASIP Journal on Advances in Signal Processing, vol. 2019, no. 1, pp. 1–11,
    2019.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] R. Avanzato and F. Beritelli, “Heart sound multiclass analysis based
    on raw data and convolutional neural network,” IEEE Sensors Letters, vol. 4, no. 12,
    pp. 1–4, 2020.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] S. L. Oh, V. Jahmunah, C. P. Ooi, R.-S. Tan, E. J. Ciaccio, T. Yamakawa,
    M. Tanabe, M. Kobayashi, and U. R. Acharya, “Classification of heart sound signals
    using a novel deep WaveNet model,” Computer Methods and Programs in Biomedicine,
    vol. 196, pp. 1–9, 2020.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] S. Gao, Y. Zheng, and X. Guo, “Gated recurrent unit-based heart sound
    analysis for heart failure screening,” Biomedical engineering online, vol. 19,
    no. 1, pp. 1–17, 2020.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. B. Shuvo, S. N. Ali, S. I. Swapnil, M. S. Al-Rakhami, and A. Gumaei,
    “CardioXNet: A novel lightweight deep learning framework for cardiovascular disease
    classification using heart sound recordings,” IEEE Access, vol. 9, pp. 36955–36967,
    2021.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] A. I. Humayun, S. Ghaffarzadegan, M. I. Ansari, Z. Feng, and T. Hasan,
    “Towards domain invariant heart sound abnormality detection using learnable filterbanks,”
    IEEE journal of biomedical and health informatics, vol. 24, no. 8, pp. 2189–2198,
    2020.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] A. I. Humayun, S. Ghaffarzadegan, Z. Feng, and T. Hasan, “Learning front-end
    filter-bank parameters using convolutional neural networks for abnormal heart
    sound detection,” in Proc. EMBC, (Honolulu, Hawaii), pp. 1408–1411, 2018.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:
    A large-scale hierarchical image database,” in Proc. CVPR, (Miami, FL), pp. 248–255,
    2009.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.
    Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset
    for audio events,” in Proc. ICASSP, (New Orleans), pp. 776–780, 2017.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” Communications of the ACM, vol. 60,
    no. 6, pp. 84–90, 2017.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” (San Diego, CA), pp. 1–14, 2015.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] F. Demir, A. Şengür, V. Bajaj, and K. Polat, “Towards the classification
    of heart sounds based on convolutional deep neural network,” Health information
    science and systems, vol. 7, no. 1, pp. 1–9, 2019.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] T. Koike, K. Qian, Q. Kong, M. D. Plumbley, B. W. Schuller, and Y. Yamamoto,
    “Audio for audio is better? An investigation on transfer learning models for heart
    sound classification,” in Proc. EMBC, (Virtual Event), pp. 74–77, 2020.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “MobileNets: Efficient convolutional neural networks for mobile vision
    applications,” arXiv preprint arXiv:1704.04861, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in Proc. CVPR, (Las Vegas, NV), pp. 770–778, 2016.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in Proc. CVPR, (Honolulu, Hawaii),
    pp. 1492–1500, 2017.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] P. Bentley, G. Nordehn, M. Coimbra, and S. Mannor, “The PASCAL Classifying
    Heart Sounds Challenge 2011 (CHSC2011) Results.” http://www.peterjbentley.com/heartchallenge/index.html.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Goldberger, L. Amaral, L. Glass, J. Hausdorff, P. Ivanov, R. Mark,
    J. Mietus, G. Moody, C. Peng, and H. Stanley, “PhysioBank, PhysioToolkit, and
    PhysioNet: Components of a new research resource for complex physiologic signals,”
    2000.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] G.-Y. Son and S. Kwon, “Classification of heart sound signal using multiple
    features,” Applied Sciences, vol. 8, no. 12, p. 2344, 2018.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] J. Oliveira, F. Renna, P. D. Costa, M. Nogueira, C. Oliveira, C. Ferreira,
    A. Jorge, S. Mattos, T. Hatem, T. Tavares, et al., “The CirCor DigiScope dataset:
    From murmur detection to murmur classification,” IEEE journal of biomedical and
    health informatics, vol. 26, no. 6, pp. 2524–2535, 2021.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] M. A. Reyna et al., “Heart murmur detection from phonocardiogram recordings:
    The George B. Moody PhysioNet Challenge 2022,” medRxiv, 2022.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] F. B. Azam, M. Ansari, I. Mclane, T. Hasan, et al., “Heart sound classification
    considering additive noise and convolutional distortion,” arXiv preprint arXiv:2106.01865,
    2021.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] P. N. Waaler, H. Melbye, et al., “Algorithm for predicting valvular heart
    disease from heart sounds in an unselected cohort,” medRxiv, 2022.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] T. Dissanayake, T. Fernando, S. Denman, S. Sridharan, H. Ghaemmaghami,
    and C. Fookes, “A robust interpretable deep learning classifier for heart anomaly
    detection without segmentation,” IEEE Journal of Biomedical and Health Informatics,
    vol. 25, no. 6, pp. 2162–2171, 2020.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model
    predictions,” in Proc. NIPS, (Long Beach, CA), p. 4768–4777, 2017.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] W.-S. Jhong, S.-I. Chu, Y.-J. Huang, T.-Y. Hsu, W.-C. Lin, P. Huang,
    and J.-J. Wang, “Deep learning hardware/software co-design for heart sound classification,”
    in Proc. ISOCC, (Yeosu, Korea), pp. 27–28, 2020.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] W. R. Thompson, A. J. Reinisch, M. J. Unterberger, and A. J. Schriefl,
    “Artificial intelligence-assisted auscultation of heart murmurs: validation by
    virtual clinical trial,” Pediatric cardiology, vol. 40, no. 3, pp. 623–629, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., “On the opportunities
    and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] W. Callaghan, J. Goh, M. Mohareb, A. Lim, and E. Law, “Mechanicalheart:
    A human-machine framework for the classification of phonocardiograms,” Proceedings
    of the ACM on Human-Computer Interaction, vol. 2, no. CSCW, pp. 1–17, 2018.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Z. Ren, K. Qian, F. Dong, Z. Dai, W. Nejdl, Y. Yamamoto, and B. W. Schuller,
    “Deep attention-based neural networks for explainable heart sound classification,”
    Machine Learning with Applications, p. 100322, 2022.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Y. Yang, I. G. Morillo, and T. M. Hospedales, “Deep neural decision trees,”
    in Proc. ICML WHI, (Stockholm, Sweden), pp. 34–40, 2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Y. Chang, Z. Ren, T. T. Nguyen, W. Nejdl, and B. Schuller, “Example-based
    explanations with adversarial attacks for respiratory sound analysis,” in Proc. INTERPSEECH,
    (Incheon, Korea), pp. 4003–4007, 2022.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] B. W. Schuller, T. Virtanen, M. Riveiro, G. Rizos, J. Han, A. Mesaros,
    and K. Drossos, “Towards sonification in multimodal and user-friendly explainable
    artificial intelligence,” in Proc. ICMI, pp. 788–792, 2021.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] M. Pelillo and T. Scantamburlo, Machines We Trust: Perspectives on Dependable
    AI. MIT Press, 2021.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] V. Gupta, C. Jung, S. Neel, A. Roth, S. Sharifi-Malvajerdi, and C. Waites,
    “Adaptive machine unlearning,” in Advances in Neural Information Processing Systems,
    vol. 34, pp. 16319–16330, 2021.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] W. Qiu et al., “A federated learning paradigm for heart sound classification,”
    in Proc. EMBC, (Glasgow, UK), pp. 1045–1048, 2022.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] C. T. Williams, “A lecture on laënnec and the evolution of the stethoscope,”
    British medical journal, vol. 2, no. 2427, pp. 6–8, 1907.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] B. Silverman and M. Balk, “Digital stethoscope—improved auscultation
    at the bedside,” The American journal of cardiology, vol. 123, no. 6, pp. 984–985,
    2019.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] M. E. Tavel, “Cardiac auscultation: a glorious past—and it does have
    a future!,” Circulation, vol. 113, no. 9, pp. 1255–1259, 2006.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] M. Abella, J. Formolo, and D. G. Penney, “Comparison of the acoustic
    properties of six popular stethoscopes,” The Journal of the Acoustical Society
    of America, vol. 91, no. 4, pp. 2224–2228, 1992.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] N. K. Bakshi and A. K. Acharya, “Wireless electronic stethoscope,” INTERNATIONAL
    JOURNAL OF ENGINEERING RESEARCH & TECHNOLOGY, vol. 03, pp. 459–462, 2014.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. E. Schenthal, J. W. Sweeney, and J. Nettleton, Wilson, “Clinical application
    of large-scale electronic data processing apparatus: I. new concepts in clinical
    use of the electronic digital computer,” Journal of the American Medical Association,
    vol. 173, pp. 6–11, 05 1960.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] H. R. Warner, A. F. Toronto, L. G. Veasey, and R. Stephenson, “A mathematical
    approach to medical diagnosis: Application to congenital heart disease,” JAMA,
    vol. 177, pp. 177–183, 07 1961.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] L. Taback, E. Marden, H. L. Mason, and H. V. Pipberger, “Digital recording
    of electrocardiographic data for analysis by a digital computer,” IRE Transactions
    on Medical Electronics, vol. ME-6, no. 3, pp. 167–171, 1959.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] C. A. CACERES, “Electrocardiographic analysis by a computer system,”
    Archives of Internal Medicine, vol. 111, pp. 196–202, 02 1963.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] H. B. Sprague, “History and present status of phonocardiography,” IRE
    Transactions on Medical Electronics, pp. 2–3, 1957.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] W. Evans, “The use of the phonocardiograph in clinical cardiology,” British
    heart journal, vol. 10, no. 2, pp. 92–98, 1948.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] A. Quiceno-Manrique, J. Godino-Llorente, M. Blanco-Velasco, and G. Castellanos-Dominguez,
    “Selection of dynamic features based on time–frequency representations for heart
    murmur detection from phonocardiographic signals,” Annals of biomedical engineering,
    vol. 38, no. 1, pp. 118–137, 2010.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] L. J. Nowak and K. M. Nowak, “Sound differences between electronic and
    acoustic stethoscopes,” Biomedical engineering online, vol. 17, no. 104, pp. 1–11,
    2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] J. Y. Shin, S. L’Yi, D. H. Jo, J. H. Bae, and T. S. Lee, “Development
    of smartphone-based stethoscope system,” in Proc. ICCAS, (Busan, South Korea),
    pp. 1288–1291, 2013.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] S. Hadiyoso, D. R. Mardiyah, D. N. Ramadan, and A. Ibrahim, “Implementation
    of electronic stethoscope for online remote monitoring with mobile application,”
    Bulletin of Electrical Engineering and Informatics, vol. 9, no. 4, pp. 1595–1603,
    2020.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] C. Yang, W. Zhang, Z. Pang, J. Zhang, D. Zou, X. Zhang, S. Guo, J. Wan,
    K. Wang, and W. Pang, “A aow-Cost, ear-contactless electronic stethoscope powered
    by Raspberry Pi for auscultation of patients with COVID-19: Prototype development
    and feasibility study,” JMIR Medical Informatics, vol. 9, no. 1, p. e22753, 2021.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \appendices
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 8 History
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before the 19th century, immediate auscultation conducted by a physician putting
    his/her ear directly to the patient’s chest was disliked because of its implications
    from a social perspective and in technique [[2](#bib.bib2)]. Immediate auscultation
    was even partially unacceptable due to the patient’s age and gender [[2](#bib.bib2)].
    Therefore, physical examination was mostly limited to inspection and palpation,
    which were not available in some cases, e. g., the great degree of body fat [[2](#bib.bib2)].
    Until 1816, the invention of the earliest stethoscope (Greek: stethos = chest,
    skopein = to view or to see) by a clinician, René Théoophile Hyacinthe Laennec,
    started a new era of mediate auscultation [[2](#bib.bib2)]. After Laennec’s cylinder-like
    stethoscope, cardiac auscultation was widely applied in physical examination and
    the structure of stethoscope was also improved. Charles J. B. Williams designed
    a flexible monaural stethoscope with a trumpet-shaped head [[2](#bib.bib2)]. Arthur
    Leared claimed the invention of a binaural stethoscope for more flexibility in
    1851 [[162](#bib.bib162)], and George Cammann further invented a binaural stethoscope
    for commercial production in 1852 [[163](#bib.bib163)]. Afterwards, the stethoscope
    was successfully and widely used as a critical diagnostic tool. However, there
    are several difficulties to employ such traditional acoustic stethoscopes. Firstly,
    acoustic stethoscopes cannot record, play back, and process the heart sounds,
    thus limiting the teaching of auscultation [[164](#bib.bib164)]. Secondly, acoustic
    stethoscopes demand physicians’ substantial clinical experience, but auscultation
    is a difficult skill that take years to acquire and refine [[27](#bib.bib27),
    [26](#bib.bib26)]. Thirdly, the performance of the human ear is limited to its
    physical limitations [[27](#bib.bib27)]. Although the apparent sound amplification
    has been seen by many acoustic stethoscopes, the increasing insensitivity of the
    human ear at frequencies below 100 Hz is still a problem for auscultation [[165](#bib.bib165)].
    Finally, the sound level of acoustic stethoscopes is very low, so they are not
    very suitable in noisy environment [[166](#bib.bib166)]. To this end, developing
    equipment and technologies for computer-aided auscultation is necessary to overcome
    the limitations of acoustic stethoscopes.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
- en: As early as in the late 1950s and 1960s, the computational capability of digital
    electronic computers promoted their applications in heart disease diagnosis [[167](#bib.bib167),
    [23](#bib.bib23)]. One direction was to deal with clinical data, such as diagnosis
    of congenital heart disease based on the incidence of clinical signs, symptoms,
    and electrocardiographic findings [[168](#bib.bib168)]. Compared to processing
    human ‘predigested’ clinical data, analysing raw physiologic records (e. g., electrocardiogram)
    is a more automated direction to save the time and efforts of physicians [[23](#bib.bib23),
    [169](#bib.bib169), [170](#bib.bib170)]. Additionally, another automated way similar
    to physicians’ cardiac auscultation is to analyse magnetic tape recordings of
    heart sounds and murmurs [[23](#bib.bib23)]. Automatically analysing recorded
    heart sounds and murmurs is promising to solve the aforementioned problems of
    acoustic stethoscopes.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'Phonocardiograph equipments were developed in the 1930s and 1940s [[171](#bib.bib171)].
    The recorded PCG signals can be objectively analysed and explained. An early study [[171](#bib.bib171)]
    reported the phonocardiograph is useful in timing and explaining whether an individual’s
    heart sounds contain abnormal sounds: atrial sounds, opening snaps, and gallop
    rhythms. A phonocardiograph was also described to be valuable in studying murmurs,
    particularly in mitral disease and congenital heart disease [[172](#bib.bib172),
    [171](#bib.bib171)]. In the past decades, due to the development of digital technologies,
    computer-aided auscultation from PCG signals played an important role in pediatric
    cardiology, internal diseases, and evaluating congenital cardiac defects [[173](#bib.bib173)].
    Particularly, the electronic stethoscope, as a light phonocardiograph equipment,
    gains an edge over acoustic stethoscopes for automatically percepting, processing,
    and analysing heart sounds. Heart sounds are electronically amplified in electronic
    stethoscopes to overcome the low sound levels of acoustic stethoscopes [[174](#bib.bib174)].
    Generally, the recorded PCG signals with an electronic stethoscope are transferred
    to a computer for visualisation and further analysis [[26](#bib.bib26)]. More
    recently, portable electronic stethoscopes were developed to be either connected
    to other mobile devices, e. g., mobile phones, or to wirelessly transmit to a
    remote processing unit via a Bluetooth/wireless interface [[26](#bib.bib26)].
    Consequently, electronic stethoscopes are able to be applied to several applications,
    such as real-time/remote monitoring and diagnosis [[175](#bib.bib175), [176](#bib.bib176)].
    In 2021, an ear-contactless electronic stethoscope was designed for auscultation
    of patients with coronavirus disease 2019 [[177](#bib.bib177)].'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
