- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:42:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:42:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2301.09362] A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2301.09362] 《深度学习时代的心音分析全面调查》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2301.09362](https://ar5iv.labs.arxiv.org/html/2301.09362)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2301.09362](https://ar5iv.labs.arxiv.org/html/2301.09362)
- en: A Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深度学习时代的心音分析全面调查》
- en: 'Zhao Ren    \IEEEmembershipMember, IEEE    Yi Chang    Thanh Tam Nguyen   
    Yang Tan    Kun Qian    \IEEEmembershipSenior Member, IEEE    and Björn W. Schuller
       \IEEEmembershipFellow, IEEE This research was funded in part by the Federal
    Ministry of Education and Research (BMBF), Germany under the project LeibnizKILabor
    with grant No. 01DD20003, the Ministry of Science and Technology of the People’s
    Republic of China with the STI2030-Major Projects (No. 2021ZD0201900), and the
    National Natural Science Foundation of China (No. 62272044), and the Teli Young
    Fellow Program from the Beijing Institute of Technology, China. (*Corresponding
    authors*: Zhao Ren and Kun Qian.)Z. Ren is with the L3S Research Center, Leibniz
    University Hannover, Germany (zren@l3s.de).Y. Chang and B. W. Schuller are with
    the GLAM – the Group on Language, Audio, & Music, Imperial College London, United
    Kingdom (y.chang20@imperial.ac.uk, schuller@ieee.org).T. T. Nguyen is with the
    Griffith University, Australia (t.nguyen19@griffith.edu.au).Y. Tan and K. Qian
    are with the School of Medical Technology, Beijing Institute of Technology, China
    (qian@bit.edu.cn).B. W. Schuller is also with the Chair of Embedded Intelligence
    for Health Care and Wellbeing, University of Augsburg, Germany.This work has been
    submitted to the IEEE for possible publication. Copyright may be transferred without
    notice, after which this version may no longer be accessible.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao Ren    \IEEEmembershipMember, IEEE    Yi Chang    Thanh Tam Nguyen    Yang
    Tan    Kun Qian    \IEEEmembershipSenior Member, IEEE    和 Björn W. Schuller   
    \IEEEmembershipFellow, IEEE 本研究部分由德国联邦教育和研究部（BMBF）资助，项目名称为 LeibnizKILabor，资助编号为01DD20003；中华人民共和国科学技术部资助了
    STI2030-重大项目（编号：2021ZD0201900）；中国国家自然科学基金（编号：62272044）；以及北京理工大学的 Teli Young Fellow
    Program。(*通讯作者*：Zhao Ren 和 Kun Qian.) Z. Ren 现为德国汉诺威大学 L3S 研究中心成员（zren@l3s.de）。Y.
    Chang 和 B. W. Schuller 现为英国帝国理工学院 GLAM 语言、音频与音乐组成员（y.chang20@imperial.ac.uk, schuller@ieee.org）。T.
    T. Nguyen 现为澳大利亚格里菲斯大学成员（t.nguyen19@griffith.edu.au）。Y. Tan 和 K. Qian 现为中国北京理工大学医药技术学院成员（qian@bit.edu.cn）。B.
    W. Schuller 还担任德国奥格斯堡大学健康与福祉嵌入式智能主席。本工作已提交给 IEEE 以待可能的出版。版权可能在未经通知的情况下转让，转让后此版本可能不再可访问。
- en: Abstract
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Heart sound auscultation has been demonstrated to be beneficial in clinical
    usage for early screening of cardiovascular diseases. Due to the high requirement
    of well-trained professionals for auscultation, automatic auscultation benefiting
    from signal processing and machine learning can help auxiliary diagnosis and reduce
    the burdens of training professional clinicians. Nevertheless, classic machine
    learning is limited to performance improvement in the era of big data. Deep learning
    has achieved better performance than classic machine learning in many research
    fields, as it employs more complex model architectures with stronger capability
    of extracting effective representations. Deep learning has been successfully applied
    to heart sound analysis in the past years. As most review works about heart sound
    analysis were given before 2017, the present survey is the first to work on a
    comprehensive overview to summarise papers on heart sound analysis with deep learning
    in the past six years 2017–2022\. We introduce both classic machine learning and
    deep learning for comparison, and further offer insights about the advances and
    future research directions in deep learning for heart sound analysis.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 心音听诊已被证明对早期筛查心血管疾病具有临床价值。由于听诊对专业人员的要求很高，依托信号处理和机器学习的自动听诊可以辅助诊断并减少培训专业医生的负担。然而，经典机器学习在大数据时代的性能提升有限。深度学习在许多研究领域中已表现出比经典机器学习更好的性能，因为它采用了更复杂的模型架构，具有更强的有效表征提取能力。近年来，深度学习已成功应用于心音分析。由于大多数关于心音分析的综述工作在2017年之前完成，本综述首次全面总结了2017年至2022年间深度学习在心音分析中的论文。我们介绍了经典机器学习和深度学习的对比，并进一步提供了深度学习在心音分析中的进展和未来研究方向的见解。
- en: '{IEEEkeywords}'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '{IEEEkeywords}'
- en: Automated auscultation, computer audition, deep learning, heart sounds, digital
    health
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自动听诊、计算机听音、深度学习、心音、数字健康
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: \IEEEPARstart
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: \IEEEPARstart
- en: Cardiac auscultation, i. e., listening and interpreting the heart sound, is
    an indispensable and critical part for the clinical examination of the patient [[1](#bib.bib1)].
    As a low-cost and non-invasive examination, cardiac auscultation is invaluable
    for the presence of a heart disease and providing an estimate of its severity,
    evolution, and prognosis [[2](#bib.bib2)]. Accurate cardiac auscultation may determine
    whether more expensive examination should be planed [[2](#bib.bib2)]. Nevertheless,
    due to difficulties in diagnosing diastolic murmurs, the overall sensitivity of
    cardiac auscultation is poor (i. e., ranging from 0.21 to 1.00) [[1](#bib.bib1)].
    Moreover, cardiac auscultation depends on the physician’s skills which have been
    shown decreased over time [[3](#bib.bib3)]. Poor cardiac auscultation skills may
    miss significant pathology, causing worsening condition, and may over diagnose
    pathology, leading to inappropriate referral for expensive echocardiography [[1](#bib.bib1)].
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 心脏听诊，即听取和解读心音，是临床检查患者的重要且不可或缺的部分 [[1](#bib.bib1)]。作为一种低成本且非侵入性的检查方法，心脏听诊在发现心脏疾病以及提供其严重程度、发展和预后的估计方面具有重要价值
    [[2](#bib.bib2)]。准确的心脏听诊可能决定是否需要计划更昂贵的检查 [[2](#bib.bib2)]。然而，由于诊断舒张期杂音的困难，心脏听诊的整体敏感性较差（即，范围从
    0.21 到 1.00） [[1](#bib.bib1)]。此外，心脏听诊依赖于医生的技能，而这些技能随着时间的推移已经显示出下降 [[3](#bib.bib3)]。差的心脏听诊技能可能会遗漏重要的病理信息，导致病情恶化，或过度诊断病理，从而导致不必要的昂贵心脏超声检查
    [[1](#bib.bib1)]。
- en: To solve the above problem of cardiac auscultation, classic machine learning
    (ML) has been widely used for automated heart sound analysis, including denoising,
    segmentation, and classification. For instance, noisy audio clips were detected
    by support vector machines (SVMs) [[4](#bib.bib4)], hidden markov models (HMMs)
    were used for heart sound segmentation [[5](#bib.bib5)], and classifiers like
    SVMs and decision trees were applied to heart sound classification [[6](#bib.bib6),
    [7](#bib.bib7)]. Classic machine learning often takes acoustic features as the
    input, while selecting useful features needs lots of human efforts. Additionally,
    classic machine learning usually performs very well on small-scale data, nevertheless,
    model performance on big data has been a bottleneck of classic machine learning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述心脏听诊问题，经典机器学习（ML）已被广泛用于自动化心音分析，包括去噪、分割和分类。例如，支持向量机（SVMs）用于检测噪声音频片段 [[4](#bib.bib4)]，隐马尔可夫模型（HMMs）用于心音分割
    [[5](#bib.bib5)]，分类器如 SVMs 和决策树用于心音分类 [[6](#bib.bib6), [7](#bib.bib7)]。经典机器学习通常将声学特征作为输入，而选择有用特征需要大量人力。此外，经典机器学习通常在小规模数据上表现良好，但在大数据上的模型性能一直是经典机器学习的瓶颈。
- en: More recently, deep learning (DL) has demonstrated its more powerful capability
    of analysing heart sounds than classic machine learning [[8](#bib.bib8)]. DL models
    are usually fed with either raw audio signals or time-frequency representations
    extracted from heart sounds as the inputs [[9](#bib.bib9), [10](#bib.bib10)],
    therefore improving the efficiency by skipping selecting hand-crafted acoustic
    features. More complex model structures in deep learning models also enhance the
    models’ capability of learning abstract representations from large-scale datasets.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，深度学习（DL）已展示出比经典机器学习更强大的心音分析能力 [[8](#bib.bib8)]。DL 模型通常以原始音频信号或从心音中提取的时间-频率表示作为输入
    [[9](#bib.bib9), [10](#bib.bib10)]，因此通过跳过手工选择声学特征来提高效率。深度学习模型中的更复杂模型结构也增强了模型从大规模数据集中学习抽象表示的能力。
- en: 'Table 1: A comparison between existing surveys on heart sound analysis.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：现有心音分析调查的比较。
- en: '| Surveys | Deep Learning | Heart sound denoising | Heart sound segmentation
    | Heart sound classification | Heart sound interpretation |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 调查 | 深度学习 | 心音去噪 | 心音分割 | 心音分类 | 心音解释 |'
- en: '| Bhoi et al. [[11](#bib.bib11)] | ✗ | mentioned only | ✓ | ✓ | ✗ |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| Bhoi 等人 [[11](#bib.bib11)] | ✗ | 仅提及 | ✓ | ✓ | ✗ |'
- en: '| Chakrabarti et al. [[12](#bib.bib12)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| Chakrabarti 等人 [[12](#bib.bib12)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| Nabih-Ali et al. [[13](#bib.bib13)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| Nabih-Ali 等人 [[13](#bib.bib13)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
- en: '| Clifford et al. [[14](#bib.bib14)] | ✓ | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| Clifford 等人 [[14](#bib.bib14)] | ✓ | ✓ | ✓ | ✓ | ✗ |'
- en: '| Ghosh et al. [[15](#bib.bib15)] | mentioned only | ✓ | ✓ | ✓ | ✗ |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| Ghosh等人[[15](#bib.bib15)] | 仅提到 | ✓ | ✓ | ✓ | ✗ |'
- en: '| Majhi et al. [[16](#bib.bib16)] | – | – | – | ✓ | – |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| Majhi等人[[16](#bib.bib16)] | – | – | – | ✓ | – |'
- en: '| Dwivedi et al. [[17](#bib.bib17)] | ✗ | ✗ | ✓ | ✓ | ✗ |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Dwivedi等人[[17](#bib.bib17)] | ✗ | ✗ | ✓ | ✓ | ✗ |'
- en: '| This survey | ✓ | ✓ | ✓ | ✓ | ✓ |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 本调查 | ✓ | ✓ | ✓ | ✓ | ✓ |'
- en: 1.1 Differences between this survey and the former ones
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1 本调查与以往的差异
- en: There are several review research studies on heart sound analysis. Feature extraction
    and basic ML models (e. g., SVMs) were introduced in [[11](#bib.bib11)]. Waveform
    features of heart sounds and shallow artificial neural networks were overviewed
    in [[12](#bib.bib12)]. Heart sound denoising and segmentation were discussed in [[13](#bib.bib13)].
    In [[14](#bib.bib14)], approaches for heart sound classification submitted to
    the PhysioNet challenge 2016 were summarised. The study in [[15](#bib.bib15)]
    summarised approaches used for heart sound analysis, from heart sound segmentation
    and feature extraction to heart sound classification. Approaches for heart sound
    classification were summarised and analysed in [[16](#bib.bib16)]. Different from
    these above studies, we will review all DL technologies on heart sound segmentation
    and classification. Furthermore, the state-of-the-art approaches about interpretability
    of DL models will be summarised and discussed (see [Table 1](#S1.T1 "Table 1 ‣
    1 Introduction ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era")). We will also discuss the potential research problems and future research
    directions in this survey, helping promote research studies in heart sound analysis.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 有几篇关于心脏声音分析的综述研究。特征提取和基本的机器学习模型（例如SVMs）在[[11](#bib.bib11)]中介绍。心脏声音的波形特征和浅层人工神经网络在[[12](#bib.bib12)]中概述。心脏声音去噪和分割在[[13](#bib.bib13)]中讨论。在[[14](#bib.bib14)]中，总结了提交到PhysioNet
    2016挑战的心脏声音分类方法。[[15](#bib.bib15)]的研究总结了用于心脏声音分析的方法，从心脏声音分割和特征提取到心脏声音分类。[[16](#bib.bib16)]总结并分析了心脏声音分类的方法。与以上这些研究不同，我们将回顾所有关于心脏声音分割和分类的深度学习技术。此外，还会总结和讨论有关深度学习模型可解释性的最新方法（见[表1](#S1.T1
    "表1 ‣ 1 介绍 ‣ 关于深度学习时代心脏声音分析的全面调查")）。我们还将讨论潜在的研究问题和未来的研究方向，促进心脏声音分析研究的发展。
- en: 1.2 Challenges in Heart Sound Analysis
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2 心脏声音分析中的挑战
- en: Many machine learning and deep learning methods have been applied to heart sound
    analysis. However, this research field is still facing many technical challenges.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习和深度学习方法已应用于心脏声音分析。然而，这个研究领域仍面临许多技术挑战。
- en: The first challenge is denoising, which aims to remove the noise from heart
    sounds. As the recording environments can be noisy with environmental noise and
    speech, denoising is an essential pre-processing procedure to improve the audio
    quality for better performance of segmentation and classification. Other pre-processing
    procedures such as separation (from lung sounds) [[18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)] are not introduced here, as they were mostly dealing with signal
    processing methods.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个挑战是去噪，旨在去除心脏声音中的噪音。由于录制环境可能存在环境噪音和语音干扰，去噪是一个必不可少的预处理步骤，以改善音频质量，提高分割和分类的性能。其他的预处理步骤，如分离（来自肺音）[[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20)]，这里不做介绍，因为它们主要处理信号处理方法。
- en: The second challenge is segmentation that targets on splitting a heart sound
    signal into multiple parts, i. e., cardiac cycles or smaller segments (S1, systole,
    S2, and diastole). Heart sound segmentation is often a pre-processing process
    of heart sound classification.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个挑战是分割，它的目标是将心脏声音信号分割成多个部分，即心脏周期或更小的分段（S1，收缩，S2和舒张）。心脏声音分割通常是心脏声音分类的预处理过程。
- en: The third is classification which predicts the severe level of cardiovascular
    diseases or abnormality of a heart from heart sounds. Heart sound classification
    is helpful for early screening of heart diseases in primary care.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个挑战是分类，它预测心脏声音的严重程度或心脏异常，可以帮助在初级保健中对心脏疾病进行早期筛查。
- en: The final one is explaining DL models for heart sound analysis. DL models are
    black boxes for human due to their complex structures, although they are promising
    in performance improvement for heart sound analysis. As digital health is a sensitive
    domain, explainable DL models are crucial for clinicians to give in-time and suitable
    therapies for patients. Correspondingly, trust from clinicians and patients can
    promote applications of explainable DL models in real life.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一项是解释心音分析中的深度学习模型。由于其复杂的结构，深度学习模型对人类来说是“黑箱”，尽管它们在心音分析中的性能提升方面很有前景。由于数字健康是一个敏感领域，可解释的深度学习模型对于临床医生提供及时和适当的治疗至关重要。因此，临床医生和患者的信任可以促进可解释深度学习模型在现实生活中的应用。
- en: 1.3 Contributions of this survey
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3 本调查的贡献
- en: The survey is expected to have the following contributions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查预计将有以下贡献。
- en: '*The first comprehensive survey in heart sound analysis with deep learning.*
    Apart from summarising machine learning techniques for heart sound denoising,
    segmentation, and classification, we review the state-of-the-art deep learning
    topologies for heart sound analysis, especially segmentation and classification.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习在心音分析中的第一次全面调查。* 除了总结心音去噪、分割和分类的机器学习技术外，我们还回顾了心音分析的最先进深度学习拓扑，特别是分割和分类。'
- en: '*Summarisation of resources.* We summarise publicly available datasets for
    heart sound analysis, particularly classification. We also provide the collection
    of open-sourced deep learning algorithms for heart sound classification, and discuss
    possible evaluation metrics.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*资源总结。* 我们总结了用于心音分析的公开数据集，特别是分类数据集。我们还提供了开源深度学习算法的汇总，并讨论了可能的评估指标。'
- en: '*Future research directions.* We discuss the limitation of current deep learning
    methods for heart sound classification, and point out potential future research
    topics in this area. We also discuss the importance of explainable DL models for
    heart sound classification, current advances, and future directions in explainable
    AI.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*未来研究方向。* 我们讨论了当前心音分类深度学习方法的局限性，并指出了该领域潜在的未来研究课题。我们还讨论了可解释的深度学习模型在心音分类中的重要性、当前进展以及未来方向。'
- en: 2 Background
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 背景
- en: '![Refer to caption](img/986c97cc8039efeb6b3b45b70aea4450.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/986c97cc8039efeb6b3b45b70aea4450.png)'
- en: 'Figure 1: The framework of heart sound analysis. After denoising and segmentation,
    a classifier is trained to produce the prediction and the corresponding interpretation
    for users (clinicians and patients). The ✓ is a normal prediction, and the ✗ means
    the prediction is abnormal. The dashes ‘- - -’ denote optional procedures.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：心音分析框架。经过去噪和分割后，训练一个分类器来生成预测结果及相应的解释（供临床医生和患者使用）。✓ 表示正常预测，而 ✗ 表示预测异常。破折号‘- - -’表示可选程序。
- en: 2.1 Heart Sounds
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 心音
- en: 'In a human’s cardiac system, a normal cardiac cycle contains two heart sounds:
    the first heart sound S1 and the second heart sound S2\. Additional sounds indicate
    diseases: a presented third heart sound S3 could be a sign of heart failure; a
    murmur could indicate defective valves or an orifice in the septal wall [[21](#bib.bib21)].
    The frequency range of S1 and S2 is 20–200 Hz, while the frequency of S3 and S4
    ranges in 15–65 Hz [[22](#bib.bib22)].'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在人类心脏系统中，一个正常的心动周期包含两个心音：第一心音 S1 和第二心音 S2。额外的声音表示疾病：出现的第三心音 S3 可能是心力衰竭的迹象；杂音可能表明瓣膜缺陷或隔膜上的孔洞[[21](#bib.bib21)]。S1
    和 S2 的频率范围为 20–200 Hz，而 S3 和 S4 的频率范围为 15–65 Hz[[22](#bib.bib22)]。
- en: 'As an example, murmurs, caused by the turbulent blood flow in the heart system,
    are identified as abnormal sounds. They are very important for verifying the timing
    and pitch for diagnosing cardiovascular diseases [[3](#bib.bib3)]. Murmurs often
    constitute the only basis for diagnosing valvular heart disease [[23](#bib.bib23)].
    Clinically, murmurs consists of two types: systolic murmurs and diastolic murmurs.
    Aortic stenosis, mitral regurgitation, and tricuspid regurgitation happen during
    systole; mitral stenosis and tricuspid stenosis occur during diastole [[3](#bib.bib3)].'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，由心脏系统中的湍流血流引起的杂音被识别为异常声音。它们对于验证心血管疾病的诊断时机和音调非常重要[[3](#bib.bib3)]。杂音往往是诊断瓣膜心脏病的唯一依据[[23](#bib.bib23)]。临床上，杂音包括两种类型：收缩期杂音和舒张期杂音。主动脉狭窄、二尖瓣返流和三尖瓣返流发生在收缩期；二尖瓣狭窄和三尖瓣狭窄发生在舒张期[[3](#bib.bib3)]。
- en: 2.2 Diagnosis of Cardiovascular Diseases
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 心血管疾病的诊断
- en: Nowadays, there are several non-invasive diagnostic tools for cardiovascular
    diseases. Electrocardiogram (ECG), sensing the P-QRS-T wave depicting the electrical
    activity of the heart [[24](#bib.bib24)], is an inexpensive and commonly-used
    tool for the screening of heart diseases. Yet, it has difficulty in detecting
    structural abnormalities in heart valves and defects characterised by heart murmurs [[25](#bib.bib25)].
    Additionally, several medical imaging tools are able to visualise the cardiovascular
    system. For instance, the echocardiogram (echo) is an ultra sound scan to create
    a moving picture of the heart. It can provide information about the heart’s size,
    shape, structure, and function [[26](#bib.bib26)]. Cardiac computed tomography
    (CT) uses x-rays to create detailed pictures of the heart and its blood vessels [[26](#bib.bib26)].
    For assessment of the cardiovascular system’s function and structure, cardiac
    magnetic resonance imaging (CMRI) creates both still and moving pictures of the
    heart and major blood vessels [[26](#bib.bib26)]. However, these above imaging
    instruments are expensive and require medical professionals for operation, thereby
    limiting their application in clinics, and small and medium hospitals.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，有几种非侵入性心血管疾病诊断工具。心电图（ECG）通过检测P-QRS-T波来描绘心脏的电活动 [[24](#bib.bib24)]，是一种便宜且常用的心脏疾病筛查工具。然而，它在检测心脏瓣膜结构异常和以心杂音为特征的缺陷方面存在困难 [[25](#bib.bib25)]。此外，一些医学影像工具能够可视化心血管系统。例如，超声心动图（echo）是一种超声扫描，可以创建心脏的动态图像。它可以提供有关心脏的大小、形状、结构和功能的信息 [[26](#bib.bib26)]。心脏计算机断层扫描（CT）使用X射线创建心脏及其血管的详细图像 [[26](#bib.bib26)]。心脏磁共振成像（CMRI）通过创建静态和动态图像来评估心脏和主要血管的功能和结构 [[26](#bib.bib26)]。然而，上述影像仪器价格昂贵且需要医疗专业人员操作，从而限制了其在诊所以及中小型医院中的应用。
- en: Compared to the above diagnostic instruments, cardiac auscultation is low-cost
    and essential in preliminary physical examinations. Phonocardiogram (PCG) signals
    recorded with a phonocardiograph have proven to be important in pediatric cardiology,
    cardiology, and internal diseases [[27](#bib.bib27)]. Recent advances of electronic
    stethoscopes facilitated computer-aided auscultation by integrating sensor design,
    signal processing, and machine learning techniques [[27](#bib.bib27)]. The low-cost
    and portable advantage of electronic stethoscopes make it possible to apply computer-aided
    auscultation to primary care and remote/home health care. Fig.[1](#S2.F1 "Figure
    1 ‣ 2 Background ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep
    Learning Era") depicts a pipeline of heart sound analysis. Heart sounds are processed
    by de-noising, segmentation, and classification, and then clinicians and patients
    receive the predictions and interpretations in primary care. In real-life, patients
    with heart sounds predicted as abnormal will be suggested further professional
    medical examinations for accurate diagnosis.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于上述诊断仪器，心脏听诊成本低且在初步体检中至关重要。用心音图（PCG）记录的心音信号已被证明在儿科心脏病学、心脏病学和内科疾病中非常重要 [[27](#bib.bib27)]。电子听诊器的最新进展通过整合传感器设计、信号处理和机器学习技术，促进了计算机辅助听诊 [[27](#bib.bib27)]。电子听诊器的低成本和便携优势使得将计算机辅助听诊应用于基层医疗和远程/居家医疗成为可能。图[1](#S2.F1
    "Figure 1 ‣ 2 Background ‣ A Comprehensive Survey on Heart Sound Analysis in the
    Deep Learning Era") 描绘了心音分析的流程。心音经过去噪、分割和分类处理后，临床医生和患者在基层医疗中接收预测和解释。在现实生活中，心音被预测为异常的患者将建议进行进一步的专业医学检查以获得准确诊断。
- en: 3 Heart Sound Analysis Tasks
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 心音分析任务
- en: '![Refer to caption](img/aa30dba5df380eeba063f5712995e62c.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/aa30dba5df380eeba063f5712995e62c.png)'
- en: 'Figure 2: Categorisation of methods for heart sound analysis. Bold texts are
    deep learning approaches.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：心音分析方法的分类。粗体文本为深度学习方法。
- en: The goal of this section is to describe the tasks and summarise classic machine
    learning techniques for each problem. Specifically, denoising and segmentation
    are two pre-processing steps for heart sound classification in many research studies.
    In Fig.[2](#S3.F2 "Figure 2 ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey
    on Heart Sound Analysis in the Deep Learning Era"), we summarise approaches used
    for the four tasks during 2017–2022. We can see that, classic machine learning
    was still used in the past years, and deep learning was mainly applied to segmentation
    and classification, as well as interpretation methods. For comparison of machine
    learning and deep learning used for heart sound analysis, this section introduce
    the tasks and classic machine learning methods, and deep learning will be discussed
    in [section 4](#S4 "4 State-of-the-art Studies ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era"). As there are only a few works for interpretation,
    we will discuss heart sound interpretation in the discussion part of this survey (see [section 6](#S6
    "6 Future Research Directions and Open Issues ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era")). Therefore, we describe the three tasks
    (de-noising, segmentation, and classification) as independent sections in the
    following for detailed discussion.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是描述任务并总结每个问题的经典机器学习技术。具体而言，去噪和分割是许多研究中用于心音分类的两个预处理步骤。在 图。[2](#S3.F2 "Figure
    2 ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")中，我们总结了2017–2022年期间用于四个任务的方法。我们可以看到，经典机器学习在过去几年仍被使用，而深度学习主要应用于分割和分类，以及解释方法。为了比较心音分析中使用的机器学习和深度学习，本节介绍了任务和经典机器学习方法，深度学习将在[第4节](#S4
    "4 State-of-the-art Studies ‣ A Comprehensive Survey on Heart Sound Analysis in
    the Deep Learning Era")中讨论。由于解释方面的工作较少，我们将在本调查的讨论部分讨论心音解释（参见[第6节](#S6 "6 Future
    Research Directions and Open Issues ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")）。因此，我们将以下三个任务（去噪、分割和分类）描述为独立的部分，以便详细讨论。
- en: 3.1 Denoising
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 去噪
- en: Generally, recorded heart sounds consist of many kinds of noises [[28](#bib.bib28)],
    including white noise and other sounds presented in the recording environments,
    e. g., humans’ speech. Noise may degrade the segmentation and classification performance
    of heart sounds[[28](#bib.bib28)]. In this regard, many studies have investigated
    denoising methods for better performance on the task of heart sound segmentation
    and classification.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，记录的心音包含许多种噪声[[28](#bib.bib28)]，包括白噪声和录音环境中的其他声音，例如人类的语音。噪声可能会降低心音的分割和分类性能[[28](#bib.bib28)]。在这方面，许多研究已经调查了去噪方法，以便在心音分割和分类任务中获得更好的性能。
- en: Filters. As a preprocessing procedure of heart sound classification, many denoising
    approaches employ signal filters for removing noise from noisy heart sounds [[9](#bib.bib9),
    [29](#bib.bib29), [30](#bib.bib30)]. Highpass filters [[31](#bib.bib31), [32](#bib.bib32)]
    have been used to eliminate low-frequency noise. With the capability of mitigating
    both high- and low-frequency noises, bandpass filters are more often used for
    heart sounds denoising [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)].
    A Butterworth bandpass filter has been successfully employed in many studies [[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]. The cutoff frequency of a Butterworth bandpass
    filter is set with a low frequency for filtering out noise with very low frequencies
    and a high frequency for filtering out high-frequency noises. A range of Butterworth
    bandpass filters with various orders have been applied with different cutoff frequency
    settings. For instance, a 4-th order Butterworth filter was set with a cutoff
    frequency of 25-400 Hz in [[44](#bib.bib44)], and a 5-th order Butterworth filter
    was designed to have a cutoff frequency of 25-500 Hz in [[45](#bib.bib45)] and
    25-250 Hz in [[46](#bib.bib46)]. A 6-th order Butterworth filter was designed
    with a cutoff frequency of 50–950 Hz [[47](#bib.bib47), [48](#bib.bib48)] and
    30-900 Hz in [[49](#bib.bib49)]. Additionally, several other filters were also
    used for denoising heart sound, such as a Savitzky–Golay filter [[50](#bib.bib50),
    [51](#bib.bib51)], Chebyshev low-pass filter [[52](#bib.bib52), [53](#bib.bib53)],
    and Notch filter [[54](#bib.bib54)].
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**滤波器**。作为心音分类的预处理程序，许多去噪方法采用信号滤波器来去除嘈杂心音中的噪声[[9](#bib.bib9), [29](#bib.bib29),
    [30](#bib.bib30)]。高通滤波器[[31](#bib.bib31), [32](#bib.bib32)] 被用来消除低频噪声。带通滤波器具有缓解高频和低频噪声的能力，因此更常用于心音去噪[[33](#bib.bib33),
    [34](#bib.bib34), [35](#bib.bib35)]。Butterworth带通滤波器在许多研究中取得了成功应用[[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]。Butterworth带通滤波器的截止频率设置为低频，以滤除非常低频的噪声，以及高频，以滤除高频噪声。一系列具有不同阶数的Butterworth带通滤波器已应用于不同的截止频率设置。例如，4阶Butterworth滤波器在[[44](#bib.bib44)]中设置为25-400
    Hz的截止频率，而5阶Butterworth滤波器在[[45](#bib.bib45)]中设计为25-500 Hz的截止频率，在[[46](#bib.bib46)]中设计为25-250
    Hz的截止频率。6阶Butterworth滤波器在[[47](#bib.bib47), [48](#bib.bib48)]中设计为50–950 Hz的截止频率，在[[49](#bib.bib49)]中设计为30-900
    Hz。此外，还使用了其他几种滤波器来去噪心音，如Savitzky–Golay滤波器[[50](#bib.bib50), [51](#bib.bib51)]、Chebyshev低通滤波器[[52](#bib.bib52),
    [53](#bib.bib53)]和Notch滤波器[[54](#bib.bib54)]。'
- en: Spectrum-based denoising. To remove noise, the spectrogram was simply selected
    with a threshold of -30, -45, -60, or -75 dB in [[55](#bib.bib55)]. However, it
    is time-consuming to search a suitable threshold among different heart sounds.
    A more flexible method, spectral subtraction [[56](#bib.bib56)], was used to estimate
    the noise and remove it from the heart sounds [[43](#bib.bib43)].
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于频谱的去噪**。为了去除噪声，频谱图通常选择-30、-45、-60或-75 dB的阈值[[55](#bib.bib55)]。然而，在不同的心音中寻找合适的阈值是非常耗时的。一种更灵活的方法是频谱减法[[56](#bib.bib56)]，该方法用于估计噪声并从心音中去除它[[43](#bib.bib43)]。'
- en: Spike removal. Frictional spike is a redundant part of the amplitude of a heart
    sound. In several studies [[44](#bib.bib44), [38](#bib.bib38)], frictional spikes
    were detected and removed (i. e., replaced by zeros) during pre-processing of
    heart sounds.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**尖峰去除**。摩擦尖峰是心音幅度中的冗余部分。在几项研究中[[44](#bib.bib44), [38](#bib.bib38)]，在心音预处理过程中检测并去除了摩擦尖峰（即，用零替换）。'
- en: Selection of noise-free segments. Apart from removing noise from heart sounds,
    the usage of noise-free heart sound segments has been shown helpful for heart
    sound analysis. Wavelet entropy was used to evaluate noise in heart sound segments [[7](#bib.bib7)],
    as clean heart sounds have relatively higher wavelet entropy than noisy heart
    sounds. Empirical wavelet transform was used to separate heart sounds, murmurs,
    low-frequency artefacts, and high-frequency noises in another study [[57](#bib.bib57)].
    Additionally, classic machine learning was also used for detecting noise-free
    heart sound segments. In [[4](#bib.bib4)], SVMs were applied to classify the quality
    of heart sound signals into binary classes (i. e., ‘unacceptable’ and ‘acceptable’)
    or three classes (i. e., ‘unacceptable’, ‘good’, ‘excellent’) without segmentation
    based on ten types of multi-domain features.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 选择无噪声段。除了从心音中去除噪声外，使用无噪声的心音段被证明对心音分析很有帮助。*小波熵*被用于评估心音段中的噪声[[7](#bib.bib7)]，因为干净的心音具有相对较高的小波熵，而噪声心音的熵较低。在另一项研究中，*经验小波变换*被用于分离心音、杂音、低频伪影和高频噪声[[57](#bib.bib57)]。此外，经典的*机器学习*也被用于检测无噪声的心音段。在[[4](#bib.bib4)]中，应用了支持向量机（SVM）将心音信号的质量分类为二类（即‘不可接受’和‘可接受’）或三类（即‘不可接受’、‘良好’、‘优秀’），而不进行基于十种多领域特征的分割。
- en: 3.2 Segmentation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 分割
- en: '![Refer to caption](img/4bb5979f0042467d42d79ce700c1c783.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/4bb5979f0042467d42d79ce700c1c783.png)'
- en: 'Figure 3: The PCG recording of a *normal* heart sound recording from the PhysioNet/CinC
    Database [[58](#bib.bib58)]. Frames in the middle with four states labelled (i. e., S1,
    systole, S2, and diastole) are depicted.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：来自PhysioNet/CinC数据库的*正常*心音录音[[58](#bib.bib58)]。图中中间的帧标有四种状态（即S1、收缩期、S2和舒张期）。
- en: Heart sound segmentation was used to split an audio sample into a set of smaller
    audio segments, which could be equal to or shorter than a complete cardiac cycle [[39](#bib.bib39),
    [59](#bib.bib59), [60](#bib.bib60)]. The segments shorter than a cardiac cycle
    could include S1, systole, S2, and diastole, as indicated in Fig. [3](#S3.F3 "Figure
    3 ‣ 3.2 Segmentation ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on
    Heart Sound Analysis in the Deep Learning Era").
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 心音分割用于将音频样本分割成一组较小的音频段，这些段可以等于或短于完整的心动周期[[39](#bib.bib39), [59](#bib.bib59),
    [60](#bib.bib60)]。短于心动周期的段可能包括S1、收缩期、S2和舒张期，如图[3](#S3.F3 "Figure 3 ‣ 3.2 Segmentation
    ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")所示。
- en: Energy-based segmentation. As heart sounds at different states have various
    energies, signal energy has been used for localising S1 and S2 peaks [[61](#bib.bib61),
    [29](#bib.bib29), [30](#bib.bib30), [62](#bib.bib62)]. Based on frequency information
    (e. g., Wavelet Transform (WT)) of heart sounds, energy peaks of wavelet coefficients
    were detected for localising S1 and S2 in [[63](#bib.bib63)].
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 基于能量的分割。由于心音在不同状态下具有不同的能量，因此信号能量被用于定位S1和S2峰值[[61](#bib.bib61), [29](#bib.bib29),
    [30](#bib.bib30), [62](#bib.bib62)]。根据心音的频率信息（例如，*小波变换（WT）*），在[[63](#bib.bib63)]中检测到的小波系数的能量峰值用于定位S1和S2。
- en: Envelope-based segmentation. Apart from energy, heart sound segmentation can
    be achieved based on envelopes [[64](#bib.bib64), [46](#bib.bib46)]. For instance,
    heart sound segmentation was implemented based on Shannon energy envelope and
    zero crossings of heart sounds in [[9](#bib.bib9)]. In another two studies [[47](#bib.bib47),
    [48](#bib.bib48)], S1 of the first heart cycle was detected based on Shannon energy
    envelopes, and the next S1 heart sounds were detected by a sliding heart cycle
    window.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 基于包络的分割。除了能量外，心音分割还可以基于包络[[64](#bib.bib64), [46](#bib.bib46)]。例如，在[[9](#bib.bib9)]中，基于香农能量包络和心音的零交叉实现了心音分割。在另外两项研究中[[47](#bib.bib47),
    [48](#bib.bib48)]，基于香农能量包络检测了第一个心动周期的S1，并通过滑动心动周期窗口检测了下一个S1心音。
- en: 'Loudness-based segmentation. Loudness has proven its potential to segment heart
    sounds [[53](#bib.bib53), [65](#bib.bib65)]. Specifically, spectrograms extracted
    from heart sounds are firstly converted into the Bark scale and smoothed with
    a Hanning window. At each time frame, the sensation of loudness is then calculated
    by the mean of the amplitudes at all frequency bands: $L(t)=\frac{\sum_{t=1}^{T}A(t)}{T}$,
    where $A(t)$ is the amplitude at the $t$-th time frame, and $T$ is the total number
    of time frames. Furthermore, the derivation of the loudness function is computed
    for obtaining peaks. Therefore, systoles and diastoles can be localised as they
    have different time lengths.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于响度的分割。响度已经证明其在分割心音方面的潜力 [[53](#bib.bib53), [65](#bib.bib65)]。具体来说，从心音中提取的声谱图首先被转换为
    Bark 量表，并用 Hanning 窗口进行平滑。在每个时间帧中，通过所有频带幅度的均值来计算响度感受：$L(t)=\frac{\sum_{t=1}^{T}A(t)}{T}$，其中
    $A(t)$ 是第 $t$ 时刻的幅度，$T$ 是时间帧的总数。此外，为了获得峰值，还计算了响度函数的导数。因此，收缩期和舒张期可以被定位，因为它们具有不同的时间长度。
- en: Classic Machine Learning for Segmentation. ML models have been proposed for
    more precise heart sound segmentation than the above mentioned rule-based segmentation
    methods. ML models for segmentation are mainly trained in supervised learning
    frameworks. There are also a few works using unsupervised learning. Both unsupervised
    and supervised learning approaches are introduced in the following.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分割的经典机器学习。已有机器学习模型被提出，用于比上述规则基础的分割方法更精确的心音分割。用于分割的机器学习模型主要在监督学习框架中进行训练。也有一些工作采用无监督学习。以下介绍了无监督学习和监督学习的方法。
- en: '*Unsupervised Learning* Considering the limited availability of the heart sound
    datasets, the authors of [[66](#bib.bib66)] adopted an unsupervised spectral clustering
    technique based on Gaussian kernel similarity to get the frame labels (e. g., S1
    and S2), which are further utilised to segment the heart sound.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '*无监督学习* 鉴于心音数据集的有限可用性，[[66](#bib.bib66)] 的作者采用了基于高斯核相似性的无监督谱聚类技术来获得帧标签（例如，S1
    和 S2），这些标签进一步用于分割心音。'
- en: '*Supervised Learning* Particularly, a hidden Markov model (HMM) has been widely
    used for this purpose [[5](#bib.bib5)]. Let us assume the heart states as $S=\{s_{1},s_{2},s_{3},s_{4}\}=\{S1,\mbox{systole},S2,\mbox{diastole}\}$,
    and the observations $O=\{o_{1},o_{2},...,o_{T}\}$ as the raw heart sounds or
    acoustic features. A transmission matrix $A=\{a_{ij}\}$ denotes the probablity
    of a state $s_{i}$ at the $t$-th time frame moving to $s_{j}$ at the $(t+1)$-th
    time frame. The probability density distribution of an observation $o_{t}$ to
    be generated by a state $s_{i}$ is $B={b_{i}(o_{t})}=P[o_{t}|s_{i}]$, where $P$
    means probability. The initial state distribution is $\pi=\{\pi_{i}\}$, representing
    the probability of state $s_{i}$ at the start time frame. Given $A$, $B$, $\pi$,
    and $O$, an HMM model aims to optimise the state sequence. The Viterbi algorithm
    is often used for this purpose [[67](#bib.bib67)] and more details can be found
    in [[5](#bib.bib5)].'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*监督学习* 特别是，隐马尔可夫模型（HMM）已经广泛用于这一目的 [[5](#bib.bib5)]。我们假设心脏状态为 $S=\{s_{1},s_{2},s_{3},s_{4}\}=\{S1,\mbox{收缩期},S2,\mbox{舒张期}\}$，观察值
    $O=\{o_{1},o_{2},...,o_{T}\}$ 为原始心音或声学特征。传输矩阵 $A=\{a_{ij}\}$ 表示状态 $s_{i}$ 在第 $t$
    时刻转移到状态 $s_{j}$ 在第 $(t+1)$ 时刻的概率。观察值 $o_{t}$ 由状态 $s_{i}$ 生成的概率密度分布为 $B={b_{i}(o_{t})}=P[o_{t}|s_{i}]$，其中
    $P$ 代表概率。初始状态分布为 $\pi=\{\pi_{i}\}$，表示在开始时刻状态 $s_{i}$ 的概率。给定 $A$、$B$、$\pi$ 和 $O$，HMM
    模型旨在优化状态序列。维特比算法通常用于此目的 [[67](#bib.bib67)]，更多详细信息可以在 [[5](#bib.bib5)] 中找到。'
- en: In order to better capture the abrupt changes in the PCG signal, the envelope
    of the signal was obtained in [[68](#bib.bib68)], and the kurtosis of the envelope
    was computed to get the impulse-like characteristics, which were further passed
    through a zero frequency filter for pure impulse information. Along with the heart
    sound labels, the extracted features were fed into a hidden semi-Markov model
    (HSMM). To better adapt the variability of the heart cycle duration (HCD) in the
    PCG recordings, a multi-centroid-duration-based HSMM was introduced in [[69](#bib.bib69)],
    where HCDs were estimated at various instances of a PCG to get maximum possible
    duration values and those nearest values were clubbed into clusters to refer each
    centroid. With more accurate state duration information, the HSMM achieved better
    segmentation performance. Similarly, considering the inter-patient variability,
    the emission probability distributions to each patient were estimated through
    a Gaussian mixture model (GMM) in an unsupervised and adaptive way in the improved
    HSMM in another study [[70](#bib.bib70)]. Moreover, the expectation maximisation
    algorithm developed in [[71](#bib.bib71)] searched for sojourn time distribution
    parameters of an HSMM for each subject. Many studies [[6](#bib.bib6), [72](#bib.bib72),
    [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [40](#bib.bib40), [41](#bib.bib41),
    [76](#bib.bib76), [43](#bib.bib43), [31](#bib.bib31), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79)] have employed logistic regression-based HSMM (LR-HSMM) proposed
    in [[80](#bib.bib80)] for heart sound segmentation. LR was incorporated to predict
    the probability of $P[s_{j}|o_{t}]$, and $B$ is then computed with the Bayes rule.
    There are also other improved HMM methods, such as the *duration-dependent HMM* [[67](#bib.bib67),
    [81](#bib.bib81)], which considers the probability density function of the duration
    at each state. Another study [[38](#bib.bib38)] proposed a Markov-switching model
    for heart sound segmentation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地捕捉PCG信号中的突变，信号的包络线在[[68](#bib.bib68)]中被提取出来，并计算了包络线的峭度以获取冲击般的特征，然后将其通过零频率滤波器以获取纯冲击信息。结合心音标签，提取的特征被输入到隐藏的半马尔可夫模型（HSMM）中。为了更好地适应PCG记录中心周期持续时间（HCD）的变化，在[[69](#bib.bib69)]中引入了基于多中心持续时间的HSMM，其中在PCG的不同实例中估计HCD，以获取最大可能的持续时间值，并将这些最接近的值聚集成簇以指代每个中心。通过更准确的状态持续时间信息，HSMM实现了更好的分割性能。类似地，考虑到患者之间的变异性，通过改进的HSMM中的高斯混合模型（GMM）以无监督和自适应的方式估计了每个患者的发射概率分布[[70](#bib.bib70)]。此外，[[71](#bib.bib71)]中开发的期望最大化算法为每个受试者搜索了HSMM的停留时间分布参数。许多研究[[6](#bib.bib6),
    [72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [40](#bib.bib40),
    [41](#bib.bib41), [76](#bib.bib76), [43](#bib.bib43), [31](#bib.bib31), [77](#bib.bib77),
    [78](#bib.bib78), [79](#bib.bib79)]使用了[[80](#bib.bib80)]中提出的基于逻辑回归的HSMM（LR-HSMM）进行心音分割。LR被用来预测$P[s_{j}|o_{t}]$的概率，然后用贝叶斯规则计算$B$。还有其他改进的HMM方法，如*依赖于持续时间的HMM*[[67](#bib.bib67),
    [81](#bib.bib81)]，它考虑了每个状态下持续时间的概率密度函数。另一项研究[[38](#bib.bib38)]提出了一种用于心音分割的马尔可夫切换模型。
- en: 3.3 Classification
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 分类
- en: The target task of automated auscultation is heart sound classification, including
    i) abnormal heart sound detection (e. g., murmurs, mitral stenosis, etc.) and
    ii) severity of cardiovascular diseases (normal/mild/moderate).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自动听诊的目标任务是心音分类，包括i）异常心音检测（如，杂音，二尖瓣狭窄等）和ii）心血管疾病的严重程度（正常/轻度/中度）。
- en: Feature Engineering. After denoising and segmentation, feature extraction is
    often performed before training a classifier. Low-level descriptors (LLDs) and
    functionals are often extracted as acoustic features. LLDs are segmental features
    on short-time segment analysis (see Table [2](#S3.T2 "Table 2 ‣ 3.3 Classification
    ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")), and functionals are supra-segmental feature vectors
    projected from LLDs. Functionals are generally statistical features, such as mean,
    max, standard deviation, and many others.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程。在去噪和分割之后，通常会进行特征提取以训练分类器。低级描述符（LLDs）和函数特征通常作为声学特征提取。LLDs是在短时间段分析上的分段特征（见表[2](#S3.T2
    "Table 2 ‣ 3.3 Classification ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era")），而函数特征是从LLDs投影得到的超段特征向量。函数特征通常是统计特征，如均值、最大值、标准差等。
- en: We mainly list the LLDs used for heart sound classification in [Table 2](#S3.T2
    "Table 2 ‣ 3.3 Classification ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era"). Apart from time-domain
    LLDs, LLDs in the frequency-domain are widely used for heart sound classification.
    Specifically, the frequency-domain features consist of spectral features based
    on Fourier Transform, Mel-scaled spectral features, and wavelet features. There
    are also pre-existing feature sets used for heart sound classification, including
    the ComParE feature set [[82](#bib.bib82)] and the eGeMAPS feature set [[83](#bib.bib83)].
    Both feature sets can be extracted with openSMILE which is an open-source toolkit [[84](#bib.bib84)].
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要列出了 [表 2](#S3.T2 "Table 2 ‣ 3.3 Classification ‣ 3 Heart Sound Analysis
    Tasks ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era")
    中用于心音分类的低级特征。除了时域低级特征外，频域低级特征也广泛用于心音分类。具体而言，频域特征包括基于傅里叶变换的光谱特征、Mel标度的光谱特征和小波特征。还有一些预先存在的特征集用于心音分类，包括
    ComParE 特征集 [[82](#bib.bib82)] 和 eGeMAPS 特征集 [[83](#bib.bib83)]。这两个特征集可以使用开源工具包
    openSMILE 提取 [[84](#bib.bib84)]。
- en: In addition to the aforementioned hand-crafted features, deep representations
    have been studied more recently (see [Table 2](#S3.T2 "Table 2 ‣ 3.3 Classification
    ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")). Due to the strong capability of deep learning models
    for extracting abstract features, deep representations bear potential to improve
    the performance of hand-crafted features.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前述的手工特征，深度表示最近也得到了研究（见 [表 2](#S3.T2 "Table 2 ‣ 3.3 Classification ‣ 3 Heart
    Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep
    Learning Era")）。由于深度学习模型在提取抽象特征方面具有强大的能力，深度表示有潜力提高手工特征的性能。
- en: 'Table 2: Hand-crafted features and deep representations for heart sound classification.
    We present various low-level descriptors (LLDs) in hand-crafted features.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：用于心音分类的手工特征和深度表示。我们展示了手工特征中的各种低级描述符（LLDs）。
- en: '| Group | Features | Description | Reference |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 组 | 特征 | 描述 | 参考文献 |'
- en: '| Time-domain | Envolope | Envelope of a signal | [[85](#bib.bib85), [57](#bib.bib57)]
    |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 时域 | 包络 | 信号的包络 | [[85](#bib.bib85), [57](#bib.bib57)] |'
- en: '|  | Amplitude | Amplitude of a signal | [[63](#bib.bib63)] |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | 幅度 | 信号的幅度 | [[63](#bib.bib63)] |'
- en: '|  | Energy | Energy of a signal | [[46](#bib.bib46), [32](#bib.bib32), [62](#bib.bib62)]
    |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | 能量 | 信号的能量 | [[46](#bib.bib46), [32](#bib.bib32), [62](#bib.bib62)] |'
- en: '|  | Entropy | Signal entropy | [[86](#bib.bib86), [32](#bib.bib32)] |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 熵 | 信号熵 | [[86](#bib.bib86), [32](#bib.bib32)] |'
- en: '|  | Loudness | Perception of sound magnitude | [[53](#bib.bib53)] |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  | 响度 | 声音强度的感知 | [[53](#bib.bib53)] |'
- en: '|  | Peak amplitude | Amplitude of peaks | [[40](#bib.bib40)] |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | 峰值幅度 | 峰值的幅度 | [[40](#bib.bib40)] |'
- en: '| Spectral | Spectral amplitude | Fourier transform | [[7](#bib.bib7), [87](#bib.bib87),
    [46](#bib.bib46), [88](#bib.bib88), [86](#bib.bib86), [33](#bib.bib33), [32](#bib.bib32),
    [89](#bib.bib89), [38](#bib.bib38)] |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 频谱 | 频谱幅度 | 傅里叶变换 | [[7](#bib.bib7), [87](#bib.bib87), [46](#bib.bib46),
    [88](#bib.bib88), [86](#bib.bib86), [33](#bib.bib33), [32](#bib.bib32), [89](#bib.bib89),
    [38](#bib.bib38)] |'
- en: '|  | Dominant frequency value | Frequency which leads to the maximum spectrum
    | [[87](#bib.bib87), [88](#bib.bib88)] |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | 主频值 | 导致最大光谱的频率 | [[87](#bib.bib87), [88](#bib.bib88)] |'
- en: '|  | Dominant frequency ratio | Ratio of the maximun energy to the total energy
    | [[87](#bib.bib87), [88](#bib.bib88)] |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | 主频比 | 最大能量与总能量的比率 | [[87](#bib.bib87), [88](#bib.bib88)] |'
- en: '|  | Energy | Spectral energy | [[38](#bib.bib38)] |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | 能量 | 频谱能量 | [[38](#bib.bib38)] |'
- en: '|  | Spectral roll-off | Frequency below a specific percentage of the total
    spectral energy | [[32](#bib.bib32)] |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | 频谱滚降 | 低于总频谱能量特定百分比的频率 | [[32](#bib.bib32)] |'
- en: '|  | Spectral centroid | Average of magnitude spectrogram at each frame | [[32](#bib.bib32),
    [89](#bib.bib89)] |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '|  | 频谱质心 | 每帧幅度光谱图的平均值 | [[32](#bib.bib32), [89](#bib.bib89)] |'
- en: '|  | Specrtal flux | Changing speed of the power spectrum | [[32](#bib.bib32)]
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | 频谱流量 | 功率谱的变化速度 | [[32](#bib.bib32)] |'
- en: '|  | Power spectral density (PSD) | Distribution of power in spectral components
    | [[46](#bib.bib46), [85](#bib.bib85), [89](#bib.bib89)] |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | 功率谱密度 (PSD) | 频谱成分中的功率分布 | [[46](#bib.bib46), [85](#bib.bib85), [89](#bib.bib89)]
    |'
- en: '|  | Spectral entropy | Shannon entropy of PSD | [[87](#bib.bib87), [88](#bib.bib88),
    [86](#bib.bib86), [54](#bib.bib54)] |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  | 频谱熵 | PSD 的香农熵 | [[87](#bib.bib87), [88](#bib.bib88), [86](#bib.bib86),
    [54](#bib.bib54)] |'
- en: '|  | Instantaneous frequency | Frequency for non-stationary signals | [[90](#bib.bib90)]
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | 瞬时频率 | 非平稳信号的频率 | [[90](#bib.bib90)] |'
- en: '|  | Fractional Fourier transform entropy | Spectral entropy of the fractional
    Fourier transform | [[91](#bib.bib91)] |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | 分数傅里叶变换熵 | 分数傅里叶变换的谱熵 | [[91](#bib.bib91)] |'
- en: '|  | Spectrogram | Short-Time Fourier Transform (STFT) | [[47](#bib.bib47),
    [48](#bib.bib48)] |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | 谱图 | 短时傅里叶变换（STFT） | [[47](#bib.bib47), [48](#bib.bib48)] |'
- en: '|  | Cepstrum | Inverse Fourier transform on the logarithm of the signal spectrum
    | [[33](#bib.bib33)] |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  | 倒谱 | 对信号谱的对数进行傅里叶逆变换 | [[33](#bib.bib33)] |'
- en: '| Mel frequency | Mel-frequency | Mel-scaled frequency | [[92](#bib.bib92),
    [73](#bib.bib73)] |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| Mel频率 | Mel频率 | Mel尺度频率 | [[92](#bib.bib92), [73](#bib.bib73)] |'
- en: '|  | Mel-Frequency Cepstral Coefficients (MFCCs) | Discrete cosine transform
    of Mel-scaled spectrogram | [[38](#bib.bib38), [93](#bib.bib93), [87](#bib.bib87),
    [86](#bib.bib86), [79](#bib.bib79), [94](#bib.bib94), [75](#bib.bib75), [95](#bib.bib95)]
    |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | Mel频率倒谱系数（MFCCs） | Mel尺度谱图的离散余弦变换 | [[38](#bib.bib38), [93](#bib.bib93),
    [87](#bib.bib87), [86](#bib.bib86), [79](#bib.bib79), [94](#bib.bib94), [75](#bib.bib75),
    [95](#bib.bib95)] |'
- en: '|  | Fractional Fourier transform-based Mel-frequency | Mel-frequency from
    the fractional Fourier transform | [[43](#bib.bib43)] |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于分数傅里叶变换的Mel频率 | 从分数傅里叶变换得到的Mel频率 | [[43](#bib.bib43)] |'
- en: '| Wavelet | Wavelet transform | Frequency analysis of a signal at various scales
    | [[85](#bib.bib85), [59](#bib.bib59), [89](#bib.bib89)] |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 小波 | 小波变换 | 在不同尺度下的信号频率分析 | [[85](#bib.bib85), [59](#bib.bib59), [89](#bib.bib89)]
    |'
- en: '|  | Wavelet scattering transform | “Wavelet convolution with nonlinear modulus
    and averaging scaling function”¹¹1https://de.mathworks.com/help/wavelet/ug/wavelet-scattering.html
    (translation invariance and elastic deformation stability [[96](#bib.bib96)])
    | [[97](#bib.bib97), [96](#bib.bib96)] |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 小波散射变换 | “具有非线性模量和平均缩放函数的小波卷积”¹¹1https://de.mathworks.com/help/wavelet/ug/wavelet-scattering.html（平移不变性和弹性变形稳定性[[96](#bib.bib96)]）
    | [[97](#bib.bib97), [96](#bib.bib96)] |'
- en: '|  | Wavelet synchrosqueezing transform | Reassignment of wavelet coefficients
    | [[35](#bib.bib35)] |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | 小波同步压缩变换 | 小波系数的重新分配 | [[35](#bib.bib35)] |'
- en: '|  | Tunable quality wavelet transform | “Wavelet multiresolution analysis
    with a user-specified Q-factor, which is the ratio of the centre frequency to
    the bandwidth of the filters”²²2https://de.mathworks.com/help/wavelet/ug/tunable-q-factor-wavelet-transform.html
    | [[98](#bib.bib98), [52](#bib.bib52)] |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | 可调质量小波变换 | “具有用户指定Q因子的多尺度小波分析，Q因子是滤波器中心频率与带宽的比率”²²2https://de.mathworks.com/help/wavelet/ug/tunable-q-factor-wavelet-transform.html
    | [[98](#bib.bib98), [52](#bib.bib52)] |'
- en: '|  | Wavelet entropy | Temporal energy distribution based on wavelet coefficients
    | [[7](#bib.bib7)] |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | 小波熵 | 基于小波系数的时间能量分布 | [[7](#bib.bib7)] |'
- en: '| Feature set | ComParE | Computational Paralinguistics ChallengE feature set
    | [[92](#bib.bib92)] |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 特征集 | ComParE | 计算语言学挑战特征集 | [[92](#bib.bib92)] |'
- en: '|  | eGeMAPS | The extended Geneva Minimalistic Acoustic Parameter Set | [[92](#bib.bib92)]
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | eGeMAPS | 扩展的日内瓦最简声学参数集 | [[92](#bib.bib92)] |'
- en: '| Deep representation | Graph-based features | Petersen graph pattern | [[99](#bib.bib99)]
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 深度表示 | 基于图的特征 | Petersen图模式 | [[99](#bib.bib99)] |'
- en: '|  | Sparse coefficient | Result of sparse coding | [[6](#bib.bib6)] |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  | 稀疏系数 | 稀疏编码的结果 | [[6](#bib.bib6)] |'
- en: '|  | Autoencoder-based features | Features extracted by an autoencoder from
    hand-crafted features | [[100](#bib.bib100), [55](#bib.bib55)] |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | 基于自编码器的特征 | 从手工设计特征中提取的自编码器特征 | [[100](#bib.bib100), [55](#bib.bib55)]
    |'
- en: Classic Machine Learning for Classification.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 经典机器学习用于分类。
- en: '![Refer to caption](img/7e32d7c8630835d1474b4becf9ce2037.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/7e32d7c8630835d1474b4becf9ce2037.png)'
- en: 'Figure 4: Statistics of the literature using discriminative machine learning
    models for heart sound classification during 2017–2022\. FNN: feed-forward neural
    network; KNN: $k$-nearest neighbour; LDC: linear discriminant classifier.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：2017–2022年使用辨别式机器学习模型进行心音分类的文献统计。FNN：前馈神经网络；KNN：$k$-最近邻；LDC：线性判别分类器。
- en: Rule-based classification was proposed for heart sound classification in [[57](#bib.bib57),
    [101](#bib.bib101)]. For better performance, ML was used for heart sound classification
    in most research studies. We briefly introduce the classifiers in the two groups
    of generative models and discriminative models in the following.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基于规则的分类方法在 [[57](#bib.bib57), [101](#bib.bib101)] 中被提出用于心音分类。为了更好的性能，大多数研究采用了机器学习（ML）进行心音分类。我们将在下文中简要介绍生成模型和判别模型这两组分类器。
- en: '*Generative models* aim to generate the joint probability distribution $P(X,y)$,
    given the features $X$ and the labels $y$. The posterior probability $P(y|X)$
    is computed via the Bayes rule $P(y|X)=\frac{P(X,y)}{P(X)}=\frac{P(X|y)P(y)}{P(X)}$,
    where $P(X|y)$ is the likelihood probability distribution. The Naïve Bayes Classifier [[102](#bib.bib102),
    [91](#bib.bib91), [41](#bib.bib41), [103](#bib.bib103)] was widely used for heart
    sound classification due to its advantage of being easy-to-use. Gaussian Mixture
    Models (GMMs) [[95](#bib.bib95), [53](#bib.bib53)] were used to estimate the data
    distribution by optimising the weights of Gaussian mixture components and mean
    and variance in each component. A Gaussian mixture-based HMM [[38](#bib.bib38)]
    was employed for heart sound classification considering the four sequential heart
    states, i. e., S1, systole, S2, and diastole.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成模型*旨在生成联合概率分布 $P(X,y)$，给定特征 $X$ 和标签 $y$。后验概率 $P(y|X)$ 通过贝叶斯规则 $P(y|X)=\frac{P(X,y)}{P(X)}=\frac{P(X|y)P(y)}{P(X)}$
    计算，其中 $P(X|y)$ 是似然概率分布。朴素贝叶斯分类器 [[102](#bib.bib102), [91](#bib.bib91), [41](#bib.bib41),
    [103](#bib.bib103)] 由于其易于使用的优势而广泛用于心音分类。高斯混合模型（GMMs） [[95](#bib.bib95), [53](#bib.bib53)]
    用于通过优化高斯混合成分的权重以及每个成分的均值和方差来估计数据分布。基于高斯混合的HMM [[38](#bib.bib38)] 被用于心音分类，考虑了四种顺序心脏状态，即
    S1、收缩期、S2 和舒张期。'
- en: '*Discriminative models* are designed to directly predict the posterior probability
    $P(y|X)$ given $X$. Fig. [4](#S3.F4 "Figure 4 ‣ 3.3 Classification ‣ 3 Heart Sound
    Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era") presents a statistic of recent works from 2017 to 2022 that employ classic
    machine learning models for heart sound classification. SVMs have been very widely
    used for heart sound classification by learning a supporting hyperplane between
    classes [[6](#bib.bib6), [47](#bib.bib47), [48](#bib.bib48), [91](#bib.bib91),
    [93](#bib.bib93), [97](#bib.bib97), [46](#bib.bib46), [99](#bib.bib99), [40](#bib.bib40),
    [75](#bib.bib75), [41](#bib.bib41), [95](#bib.bib95), [60](#bib.bib60), [43](#bib.bib43),
    [92](#bib.bib92), [100](#bib.bib100), [55](#bib.bib55), [103](#bib.bib103), [33](#bib.bib33),
    [79](#bib.bib79), [64](#bib.bib64), [85](#bib.bib85), [96](#bib.bib96)]. Apart
    from linear projection between data samples and labels, SVMs can learn separating
    hyperplanes on non-linear data via non-linear kernels, such as radial basis function.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*判别模型* 旨在直接预测后验概率 $P(y|X)$ 给定 $X$。图 [4](#S3.F4 "Figure 4 ‣ 3.3 Classification
    ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era") 展示了2017年到2022年使用经典机器学习模型进行心音分类的近期工作统计。支持向量机（SVMs）由于通过学习类间的支持超平面而被广泛用于心音分类
    [[6](#bib.bib6), [47](#bib.bib47), [48](#bib.bib48), [91](#bib.bib91), [93](#bib.bib93),
    [97](#bib.bib97), [46](#bib.bib46), [99](#bib.bib99), [40](#bib.bib40), [75](#bib.bib75),
    [41](#bib.bib41), [95](#bib.bib95), [60](#bib.bib60), [43](#bib.bib43), [92](#bib.bib92),
    [100](#bib.bib100), [55](#bib.bib55), [103](#bib.bib103), [33](#bib.bib33), [79](#bib.bib79),
    [64](#bib.bib64), [85](#bib.bib85), [96](#bib.bib96)]。除了在数据样本和标签之间进行线性投影外，SVMs
    还可以通过非线性核（如径向基函数）在非线性数据上学习分隔超平面。'
- en: Furthermore, $k$-nearest neighbours (KNNs) has shown good performance on heart
    sound classification with the idea of classifying a data sample according to the
    classes of its $k$-nearest neighbours [[91](#bib.bib91), [93](#bib.bib93), [46](#bib.bib46),
    [40](#bib.bib40), [75](#bib.bib75), [85](#bib.bib85), [90](#bib.bib90), [41](#bib.bib41),
    [60](#bib.bib60), [43](#bib.bib43), [103](#bib.bib103), [104](#bib.bib104)]. Also,
    decision trees were successfully used for heart sound classification [[7](#bib.bib7),
    [99](#bib.bib99), [40](#bib.bib40), [88](#bib.bib88), [75](#bib.bib75), [41](#bib.bib41),
    [103](#bib.bib103)]. One reason is that limiting the number of decision nodes
    can help avoid overfitting [[7](#bib.bib7)], and another reason is that the structure
    of a decision tree can show the internal logic for classification. Bagged trees [[99](#bib.bib99),
    [40](#bib.bib40)] assemble multiple decision trees for more complex model architectures,
    therefore it is possible to achieve better performance. Random forests [[93](#bib.bib93),
    [88](#bib.bib88), [90](#bib.bib90), [103](#bib.bib103), [79](#bib.bib79), [35](#bib.bib35)]
    further improve bagged trees with less features to be used when splitting each
    node.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，$k$-最近邻（KNNs）在心音分类中表现良好，其思想是根据数据样本的$k$-最近邻的类别进行分类[[91](#bib.bib91), [93](#bib.bib93),
    [46](#bib.bib46), [40](#bib.bib40), [75](#bib.bib75), [85](#bib.bib85), [90](#bib.bib90),
    [41](#bib.bib41), [60](#bib.bib60), [43](#bib.bib43), [103](#bib.bib103), [104](#bib.bib104)]。此外，决策树也被成功应用于心音分类[[7](#bib.bib7),
    [99](#bib.bib99), [40](#bib.bib40), [88](#bib.bib88), [75](#bib.bib75), [41](#bib.bib41),
    [103](#bib.bib103)]。一个原因是限制决策节点的数量可以帮助避免过拟合[[7](#bib.bib7)]，另一个原因是决策树的结构可以展示分类的内部逻辑。袋装树[[99](#bib.bib99),
    [40](#bib.bib40)]将多个决策树组合成更复杂的模型架构，因此有可能实现更好的性能。随机森林[[93](#bib.bib93), [88](#bib.bib88),
    [90](#bib.bib90), [103](#bib.bib103), [79](#bib.bib79), [35](#bib.bib35)]通过在分裂每个节点时使用更少的特征来进一步改进袋装树。
- en: In recent years, feed-forward Neural Networks (FNNs) have started to be applied
    to heart sound classification [[63](#bib.bib63), [52](#bib.bib52), [87](#bib.bib87),
    [75](#bib.bib75), [55](#bib.bib55), [32](#bib.bib32), [94](#bib.bib94), [30](#bib.bib30),
    [89](#bib.bib89)]. FNNs can automatically learn a non-linear projection between
    acoustic features and labels. Although FNNs lack of explainability compared to
    other classifiers such as SVMs and decision trees, they are promising to produce
    good performance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，前馈神经网络（FNNs）已开始应用于心音分类[[63](#bib.bib63), [52](#bib.bib52), [87](#bib.bib87),
    [75](#bib.bib75), [55](#bib.bib55), [32](#bib.bib32), [94](#bib.bib94), [30](#bib.bib30),
    [89](#bib.bib89)]。FNNs能够自动学习声学特征与标签之间的非线性映射。尽管与其他分类器如支持向量机（SVMs）和决策树相比，FNNs缺乏可解释性，但它们在产生良好性能方面具有潜力。
- en: There are also several other machine learning models, such as, linear discriminant
    classifiers [[99](#bib.bib99), [55](#bib.bib55), [103](#bib.bib103)], logistic
    regression [[103](#bib.bib103)], quadratic discriminant analysis [[46](#bib.bib46)],
    boosting methods [[88](#bib.bib88), [59](#bib.bib59), [79](#bib.bib79), [98](#bib.bib98)],
    and others [[86](#bib.bib86), [54](#bib.bib54)]. Finally, multiple classifiers
    can be further assembled for better performance compared to that of a single model [[93](#bib.bib93),
    [75](#bib.bib75), [59](#bib.bib59), [43](#bib.bib43), [85](#bib.bib85)].
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他几种机器学习模型，如线性判别分类器[[99](#bib.bib99), [55](#bib.bib55), [103](#bib.bib103)]，逻辑回归[[103](#bib.bib103)]，二次判别分析[[46](#bib.bib46)]，提升方法[[88](#bib.bib88),
    [59](#bib.bib59), [79](#bib.bib79), [98](#bib.bib98)]，以及其他[[86](#bib.bib86), [54](#bib.bib54)]。最后，与单一模型相比，多个分类器可以进一步组合以提高性能[[93](#bib.bib93),
    [75](#bib.bib75), [59](#bib.bib59), [43](#bib.bib43), [85](#bib.bib85)]。
- en: 4 State-of-the-art Studies
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 先进的研究
- en: Due to the strong capability of extracting effective representations, deep learning
    has been successfully applied to a range of acoustic tasks, e. g., speech emotion
    recognition [[105](#bib.bib105)], respiratory sound classification [[106](#bib.bib106)],
    snore sound classification [[107](#bib.bib107)], and many more. In recent advances,
    deep learning has been also employed in processing heart sound signals and achieved
    good performance [[8](#bib.bib8)]. In this regard, the applications of deep learning
    methods for heart sound analysis tasks are introduced and discussed in the following.
    Notably, as there are few works focusing on denoising in this context with deep
    learning, we introduce deep learning topologies for segmentation and classification
    in this section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于强大的有效表示提取能力，深度学习已成功应用于各种声学任务，例如，*语音情感识别*[[105](#bib.bib105)]、*呼吸声分类*[[106](#bib.bib106)]、*打鼾声分类*[[107](#bib.bib107)]等。在近期的进展中，深度学习也被用于处理心音信号，并取得了良好的性能[[8](#bib.bib8)]。在这方面，本文介绍和讨论了深度学习方法在心音分析任务中的应用。值得注意的是，由于在这一背景下针对去噪的深度学习工作较少，我们在本节中介绍了用于分割和分类的深度学习拓扑结构。
- en: 4.1 Deep Learning for Segmentation
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 深度学习在分割中的应用
- en: Various DL models have been proposed for heart sound segmentation. We group
    them into CNNs and RNNs which extract spatial and sequential representations,
    respectively.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 各种深度学习（DL）模型已经被提出用于心音分割。我们将它们分为卷积神经网络（CNNs）和递归神经网络（RNNs），分别用于提取空间和序列表示。
- en: Convolutional neural networks. Inspired by successful applications of deep convolutional
    neural networks (CNNs) in image segmentation, deep CNNs have been applied to heart
    sound segmentation in recent studies [[108](#bib.bib108)]. For instance, several
    CNN-based segmentation algorithms were proposed and compared in [[108](#bib.bib108)],
    including CNNs with sequential max temporal modelling, CNNs with HMMs or HSMMs
    to model the probability density distribution of observations.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络。受到深度卷积神经网络（CNNs）在图像分割中成功应用的启发，近年来深度CNNs已被应用于心音分割的研究[[108](#bib.bib108)]。例如，[[108](#bib.bib108)]中提出并比较了几种基于CNN的分割算法，包括具有序列最大时间建模的CNNs，以及用于建模观察值概率密度分布的具有HMM或HSMM的CNNs。
- en: Recurrent neural networks. Recurrent neural networks (RNNs) have demonstrated
    their ability to exploit temporal information in sequential data. Therefore, RNNs
    can also help in locating the states of heart sounds. In [[109](#bib.bib109)],
    the authors regarded segmentation as an event detection task and developed bi-directional
    Gated Recurrent Unit (GRU)-RNNs based on spectrogram and envelop features. Since
    envelope features fail to effectively model the intrinsic duration information
    of the heart cycles, a duration-LSTM was proposed in [[110](#bib.bib110)] to integrate
    the duration vector into the standard LSTM cells with envelope features to obtain
    better segmentation performance. Specifically, duration parameters consist of
    heart cycle duration and systole duration estimated from the envelope autocorrelation.
    Without envelopes and time-frequency based features, the authors of [[111](#bib.bib111)]
    utilised bi-directional GRU-RNNs to segment the heart sound directly. Considering
    the possible noisy and irregular sequences in heart sound signals, an attention-based
    RNN framework was introduced in [[112](#bib.bib112)]. Specifically, before the
    final classification layer, with the hidden representation returned by bi-directional
    LSTMs, a single linear layer was applied to learn the weight score of each hidden
    state. Such weight score values are multiplied with the hidden representation
    for the final classification.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 递归神经网络。递归神经网络（RNNs）已证明了其在序列数据中利用时间信息的能力。因此，RNNs 也可以帮助定位心音的状态。在[[109](#bib.bib109)]中，作者将分割视为事件检测任务，并基于频谱图和包络特征开发了双向门控递归单元（GRU）-RNNs。由于包络特征无法有效建模心周期的内在持续时间信息，[[110](#bib.bib110)]提出了一种持续时间LSTM，将持续时间向量集成到标准LSTM单元中，以包络特征获得更好的分割性能。具体来说，持续时间参数包括心周期持续时间和从包络自相关估计的收缩期持续时间。在没有包络和时频特征的情况下，[[111](#bib.bib111)]的作者利用双向GRU-RNNs直接对心音进行分割。考虑到心音信号中的噪声和不规则序列，[[112](#bib.bib112)]引入了一种基于注意力的RNN框架。具体来说，在最终分类层之前，利用双向LSTM返回的隐藏表示，应用了一个线性层来学习每个隐藏状态的权重分数。这些权重分数值与隐藏表示相乘，用于最终分类。
- en: CNNs + RNNs. An end-to-end model was proposed in [[113](#bib.bib113)], where
    CNNs and LSTM recurrent neural networks (RNNs) are integrated to learn rich and
    efficient features from the audio directly. The gate structures of each LSTM unit
    are optimised in [[114](#bib.bib114)] for efficiency.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs + RNNs。在[[113](#bib.bib113)]中提出了一种端到端的模型，其中CNNs和LSTM递归神经网络（RNNs）被集成以直接从音频中学习丰富且高效的特征。每个LSTM单元的门控结构在[[114](#bib.bib114)]中进行了效率优化。
- en: 4.2 Deep Learning for Classification
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 深度学习在分类中的应用
- en: '![Refer to caption](img/c58f3a7a394e25d1527ffcb1838fdbd6.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/c58f3a7a394e25d1527ffcb1838fdbd6.png)'
- en: 'Figure 5: Pipeline of DL models working on heart sounds. “1” indicates transfer
    learning; “2” means deep learning on the time-frequency representation; “3” depicts
    end-to-end learning. The three branches can be in parallel or assembled at the
    feature-/decision-level. DL can be also utilised for processing features apart
    from raw audio signals and time-frequency representations. The processing of DL
    on other features can be found in [subsection 4.2](#S4.SS2 "4.2 Deep Learning
    for Classification ‣ 4 State-of-the-art Studies ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era").'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：心音的DL模型流程图。 “1”表示迁移学习；“2”表示对时频表示的深度学习；“3”表示端到端学习。这三条分支可以并行运行，也可以在特征/决策级别进行组合。DL也可以用于处理原始音频信号和时频表示之外的特征。关于DL在其他特征上的处理，可以参考[第4.2节](#S4.SS2
    "4.2 深度学习在分类中的应用 ‣ 4 先进研究 ‣ 深度学习时代心音分析的全面调查")。
- en: Different from classic machine learning, DL learns effective representations
    from either raw heart sounds or simple time-frequency representations using models
    with more parameters. DL often performs very well on many acoustic tasks benefiting
    from its strong capability. Apart from the pipeline shown in Fig. [5](#S4.F5 "Figure
    5 ‣ 4.2 Deep Learning for Classification ‣ 4 State-of-the-art Studies ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era"), we summarise the advances
    of deep learning methods for heart sound classification in the following.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 与经典机器学习不同，DL通过具有更多参数的模型从原始心音或简单时频表示中学习有效表示。DL通常在许多声学任务中表现非常好，这得益于其强大的能力。除了图[5](#S4.F5
    "图5 ‣ 4.2 深度学习在分类中的应用 ‣ 4 先进研究 ‣ 深度学习时代心音分析的全面调查")中所示的流程，我们总结了深度学习方法在心音分类中的进展。
- en: Deep learning on time-frequency representations. As it is challenging to extract
    effective representations from raw heart sound signals, 2D time-frequency representations
    have been widely used as the input of 2D CNNs for heart sound classification,
    respectively [[72](#bib.bib72), [73](#bib.bib73), [50](#bib.bib50), [39](#bib.bib39),
    [115](#bib.bib115), [81](#bib.bib81), [76](#bib.bib76), [116](#bib.bib116), [117](#bib.bib117),
    [49](#bib.bib49), [77](#bib.bib77), [42](#bib.bib42), [118](#bib.bib118), [61](#bib.bib61),
    [29](#bib.bib29), [119](#bib.bib119)]. Spectrograms, extracted by STFT from heart
    sounds, were fed into ResNet for abormal heart sounds detection in [[120](#bib.bib120)].
    Multi-domain features were considered to be more comprehensive in reflecting the
    characteristics of all heart sound classes. In [[50](#bib.bib50)], spectrograms,
    Mel spectrograms and MFCCs were extracted as the inputs of VGG models, and the
    final predictions were obtained by ensemble learning.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 对时频表示的深度学习。由于从原始心音信号中提取有效表示具有挑战性，2D时频表示已被广泛用作2D CNNs的输入以进行心音分类[[72](#bib.bib72),
    [73](#bib.bib73), [50](#bib.bib50), [39](#bib.bib39), [115](#bib.bib115), [81](#bib.bib81),
    [76](#bib.bib76), [116](#bib.bib116), [117](#bib.bib117), [49](#bib.bib49), [77](#bib.bib77),
    [42](#bib.bib42), [118](#bib.bib118), [61](#bib.bib61), [29](#bib.bib29), [119](#bib.bib119)]。通过STFT从心音中提取的频谱图被输入到ResNet中用于异常心音检测[[120](#bib.bib120)]。多域特征被认为在反映所有心音类别特征方面更为全面。在[[50](#bib.bib50)]中，频谱图、Mel频谱图和MFCCs被提取作为VGG模型的输入，最终预测通过集成学习获得。
- en: Apart from CNNs that are good at extracting spatial features, RNNs are more
    capable of extracting temporal features from sequential signals. LSTM nets were
    applied to process discrete wavelet transforms and MFCCs for heart sound classification [[36](#bib.bib36),
    [75](#bib.bib75)]. Deng et al. employed convolutional recurrent neural networks
    which combined the CNNs and RNNs [[121](#bib.bib121)].
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了擅长提取空间特征的CNNs外，RNNs更能够从序列信号中提取时间特征。LSTM网络被应用于处理离散小波变换和MFCCs用于心音分类[[36](#bib.bib36),
    [75](#bib.bib75)]。Deng等人使用了卷积递归神经网络，将CNNs和RNNs相结合[[121](#bib.bib121)]。
- en: Additionally, there are also other classifiers used for heart sound classification,
    such as a stacked sparse autoencoder deep neural network [[122](#bib.bib122)]
    and a semi-non-negative matrix factorisation classifier [[78](#bib.bib78)]. More
    details can be found in the listed references.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有其他用于心音分类的分类器，如堆叠稀疏自编码器深度神经网络[[122](#bib.bib122)]和半非负矩阵分解分类器[[78](#bib.bib78)]。更多细节可以在列出的参考文献中找到。
- en: Deep learning on other features. Apart from the above excellent works about
    DL on time-frequency representations, other features extracted from heart sounds
    were also used as the input of DL models.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 深入学习其他特征。除了上述关于时间频率表示的深度学习优秀工作外，从心音中提取的其他特征也被用作深度学习模型的输入。
- en: i) *Time-domain features*. Similar to features in classic machine learning,
    1D time-domain features can be also fed into DL models for heart sound classification.
    For instance, instant energy of the heart sound was extracted and used as the
    input of stacked auto-encoder networks [[62](#bib.bib62)]. Multiple statistical
    features (such as mean, median, variance, and many more) were extracted from all
    75 ms segments in each complete heart sound clip, and fed into a bidirectional
    LSTM (BiLSTM) net for classification in [[123](#bib.bib123)].
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: i) *时域特征*。类似于经典机器学习中的特征，1D 时域特征也可以输入到深度学习模型中进行心音分类。例如，提取了心音的瞬时能量，并用作堆叠自编码器网络的输入[[62](#bib.bib62)]。从每个完整心音片段中的所有
    75 ms 段中提取了多种统计特征（如均值、中位数、方差等），并输入到双向 LSTM (BiLSTM) 网络中进行分类[[123](#bib.bib123)]。
- en: ii) *1D frequency-based features*. Either 1D CNNs or feed-forward DNN models
    can be used to process 1D features. General frequency features and Mel domain
    features were fed into 1D CNNs and then the multiple CNNs were assembled for the
    final prediction in [[119](#bib.bib119)]. Moreover, Mel spectrograms and MFCC
    were used to further extract features as the input of a 5-layer feed-forward DNN
    model in [[9](#bib.bib9)].
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ii) *1D 频率特征*。1D CNNs 或前馈 DNN 模型均可用于处理 1D 特征。将一般频率特征和 Mel 域特征输入到 1D CNNs 中，然后将多个
    CNNs 组装进行最终预测[[119](#bib.bib119)]。此外，Mel 光谱图和 MFCC 被用于进一步提取特征作为 5 层前馈 DNN 模型的输入[[9](#bib.bib9)]。
- en: iii) *2D frequency-based features*. Different from the above time-frequency
    representations, herein, we list 2D frequency-based features to include (a) multiple
    1D frequency features directly extracted from audio segments rather than window
    functions in the STFT domain and (b) features computed from time-frequency features.
    Qian et al. utilised wavelets to calculate wavelet energy features from a set
    of short acoustic segments and further used GRU-RNNs as the classifier [[124](#bib.bib124)].
    Dong et al. extracted log Mel features and corresponding functionals from heart
    sound segments and implemented classification LSTM-RNNs and GRU-RNNs [[92](#bib.bib92)].
    In their experiments, log Mel features performed better than MFCCs and other LLDs [[92](#bib.bib92)].
    Zhang et al. extracted temporal quasi-periodic features computed by an average
    magnitude difference function from spectrograms and applied LSTM-RNNs for exploring
    the dependency relation within the features [[125](#bib.bib125)]. A denoising
    auto-encoder was employed to extract deep representations from spectrograms as
    the input of the classifier of 1D CNNs in [[126](#bib.bib126)].
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: iii) *2D 频率特征*。与上述时间频率表示不同，这里列出了 2D 频率特征，包括 (a) 从音频片段直接提取的多个 1D 频率特征，而不是 STFT
    领域中的窗口函数，以及 (b) 从时间频率特征计算出的特征。钱等人利用小波计算了一组短声学片段的波小波能量特征，并进一步使用 GRU-RNNs 作为分类器[[124](#bib.bib124)]。董等人提取了心音片段的对数
    Mel 特征及相应的函数，并实现了分类 LSTM-RNNs 和 GRU-RNNs[[92](#bib.bib92)]。在他们的实验中，对数 Mel 特征的表现优于
    MFCCs 和其他 LLDs[[92](#bib.bib92)]。张等人提取了通过平均幅度差函数从光谱图中计算的时间准周期特征，并应用 LSTM-RNNs
    探索特征之间的依赖关系[[125](#bib.bib125)]。在[[126](#bib.bib126)]中，使用去噪自编码器从光谱图中提取深层表示作为 1D
    CNN 分类器的输入。
- en: End-to-end learning. In recent years, as time-frequency representations and
    other features still need human efforts to select features, it has been becoming
    increasingly popular to use end-to-end networks to learn representations from
    heart sounds. Based on raw heart sound signals, various 1D CNN architectures have
    been proposed and applied to the task of heart sound classification [[10](#bib.bib10),
    [39](#bib.bib39), [51](#bib.bib51), [34](#bib.bib34), [127](#bib.bib127)]. Furthermore,
    Liu et al. introduced a temporal convolutional network (TCN) that performed a
    high sensitivity for heart sound classification [[74](#bib.bib74)], as a TCN benefiting
    from dilated and casual convolutions is more suitable for sequential data than
    typical CNNs are. A 1D CNN model consisting of residual blocks was developed for
    classifying heart sounds [[128](#bib.bib128)]. Apart from CNNs, GRU-RNNs were
    also used to process raw heart sound signals for the screening of heart failure [[129](#bib.bib129)].
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端学习。近年来，由于时间频率表示和其他特征仍然需要人工选择特征，使用端到端网络从心音中学习表示变得越来越流行。基于原始心音信号，已经提出并应用了各种1D
    CNN架构用于心音分类任务 [[10](#bib.bib10), [39](#bib.bib39), [51](#bib.bib51), [34](#bib.bib34),
    [127](#bib.bib127)]。此外，刘等人引入了一种时间卷积网络（TCN），它在心音分类中表现出了高敏感度 [[74](#bib.bib74)]，因为TCN通过扩张卷积和因果卷积在处理序列数据时比典型的CNN更为合适。一个由残差块组成的1D
    CNN模型也被开发用于心音分类 [[128](#bib.bib128)]。除了CNN，GRU-RNN也被用于处理原始心音信号，以筛查心力衰竭 [[129](#bib.bib129)]。
- en: Several studies also mentioned the capability of CNNs and RNNs for learning
    frequency-domain and time-domain characteristics of heart sounds. For instance,
    Shuvo et al. proposed a CardioXNet model that employed representation learning
    followed by sequence residual learning without any preprocessing [[130](#bib.bib130)].
    In the representation learning phase, three parallel 1D CNN pathways were constructed
    to extract time-invariant features from heart sound signals; in the sequence residual
    learning phase, BiLSTM nets were employed to learn sequential representation.
    The study in [[45](#bib.bib45)] attempted to automatically learn time-frequency
    features, i. e., frequency-domain features were extracted by 1D CNNs and the time-domain
    characteristics were extracted by GRU-RNNs. A self-attention mechanism was further
    used to fuse the two types of features for the final classification. In [[131](#bib.bib131),
    [132](#bib.bib132)], time-convolution (tConv) layers were implemented at the front
    end of the network for learning finite impulse response filters.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究还提到了CNN和RNN在学习心音的频域和时域特征方面的能力。例如，Shuvo等人提出了一种CardioXNet模型，该模型采用了表示学习后跟序列残差学习，而无需任何预处理 [[130](#bib.bib130)]。在表示学习阶段，构建了三条并行的1D
    CNN路径，从心音信号中提取时间不变的特征；在序列残差学习阶段，采用了BiLSTM网络以学习序列表示。研究[[45](#bib.bib45)]试图自动学习时间频率特征，即通过1D
    CNN提取频域特征，通过GRU-RNN提取时域特征。进一步使用自注意力机制融合这两种特征以进行最终分类。在[[131](#bib.bib131), [132](#bib.bib132)]中，时间卷积（tConv）层被实现于网络的前端，以学习有限脉冲响应滤波器。
- en: Transfer learning. As data collection in the healthcare domain has to be executed
    according to extremely strict regulations, heart sound datasets are usually not
    as large as datasets in other areas of Computer Audition. Transfer learning attempts
    to employ pre-trained DL models which are optimised on large-scale datasets. In
    recent studies, pre-trained models are mainly learnt on either an image dataset
    (i. e., ImageNet [[133](#bib.bib133)]) and an audio dataset (i. e., AudioSet [[134](#bib.bib134)]).
    Although heart sounds are presented as audio signals which are a different data
    type from ImageNet, DL models trained on ImageNet have shown good performance
    on time-frequency representations extracted from heart sounds for heart sound
    classification. Typical DL models on ImageNet, such as AlexNet [[135](#bib.bib135)]
    and VGG [[136](#bib.bib136)], have been successfully used for heart sound classification [[44](#bib.bib44),
    [31](#bib.bib31), [137](#bib.bib137), [8](#bib.bib8)]. Compared to ImageNet, AudioSet
    includes multiple types of acoustic signals and therefore is more close to heart
    sounds with the consideration of data type. In [[138](#bib.bib138)], pre-trained
    Audio Neural Networks (PANNs) trained on AudioSet were used for classifying heart
    sounds with inputs of time-frequency representations. PANNs outperformed ImageNet-based
    models [[138](#bib.bib138)], including VGG, MobileNet V2 [[139](#bib.bib139)],
    ResNet [[140](#bib.bib140)], and ResNeXt [[141](#bib.bib141)].
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习。由于医疗领域的数据收集必须遵循极其严格的规定，心音数据集通常没有其他计算机听觉领域的数据集那么大。迁移学习尝试利用在大规模数据集上优化的预训练深度学习（DL）模型。在最近的研究中，预训练模型主要在图像数据集（例如，ImageNet [[133](#bib.bib133)]）和音频数据集（例如，AudioSet [[134](#bib.bib134)]）上进行学习。尽管心音作为音频信号呈现，与ImageNet的数据类型不同，但在ImageNet上训练的深度学习模型在从心音中提取的时频表示的心音分类中表现良好。ImageNet上的典型深度学习模型，如AlexNet [[135](#bib.bib135)]和VGG [[136](#bib.bib136)]，已成功用于心音分类 [[44](#bib.bib44),
    [31](#bib.bib31), [137](#bib.bib137), [8](#bib.bib8)]。与ImageNet相比，AudioSet包含多种类型的声学信号，因此在数据类型方面更接近心音。在 [[138](#bib.bib138)]中，使用了在AudioSet上训练的预训练音频神经网络（PANNs）来分类心音，输入的是时频表示。PANNs的表现优于基于ImageNet的模型 [[138](#bib.bib138)]，包括VGG、MobileNet
    V2 [[139](#bib.bib139)]、ResNet [[140](#bib.bib140)]和ResNeXt [[141](#bib.bib141)]。
- en: After extracting representations by pre-trained models, transfer learning uses
    various types of classifiers for classification, mainly including classic machine
    learning classifiers and feed-forward neural networks. For instance, SVMs were
    applied to representations extracted by AlexNet, VGG16, and VGG19 in [[44](#bib.bib44),
    [137](#bib.bib137), [31](#bib.bib31)]. Other classifiers such as KNNs were used
    in [[44](#bib.bib44)]. Pre-trained VGG model was frozen and followed by fully
    connected layers in [[8](#bib.bib8)].
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在通过预训练模型提取表示后，迁移学习使用各种类型的分类器进行分类，主要包括经典的机器学习分类器和前馈神经网络。例如，SVM被应用于 [[44](#bib.bib44),
    [137](#bib.bib137), [31](#bib.bib31)]中由AlexNet、VGG16和VGG19提取的表示。其他分类器如KNN也在 [[44](#bib.bib44)]中使用。预训练的VGG模型在 [[8](#bib.bib8)]中被冻结，并在其后添加了全连接层。
- en: Additionally, fine-tuning pre-trained models in transfer learning has also shown
    good performance for heart sound classification. A pre-trained AlexNet after fine-tuning
    provided effective representations for heart sound classification [[44](#bib.bib44),
    [31](#bib.bib31)]. Similarly, PANNs was also fine-tuned in [[138](#bib.bib138)].
    Fine-tuned models have even outperformed pre-trained models as they adapted to
    the data distribution of heart sound datasets. In [[8](#bib.bib8)], fine-tuned
    VGG performed better than pre-trained VGG on heart sound classification when SVMs
    were the classifiers.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在迁移学习中，微调预训练模型也表现出了良好的心音分类效果。经过微调的预训练AlexNet为心音分类提供了有效的表示 [[44](#bib.bib44),
    [31](#bib.bib31)]。类似地，PANNs也在 [[138](#bib.bib138)]中进行了微调。微调模型甚至超越了预训练模型，因为它们适应了心音数据集的数据分布。在 [[8](#bib.bib8)]中，当使用支持向量机（SVM）作为分类器时，微调后的VGG在心音分类中的表现优于预训练的VGG。
- en: 5 Published Resources on Heart Sound Analysis
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 发表的心音分析资源
- en: 5.1 Published Datasets
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 发表的数据集
- en: In the past years, several heart sound databases have been collected. Herein,
    we briefly introduce the following access-available databases listed in [Table 3](#S5.T3
    "Table 3 ‣ 5.1 Published Datasets ‣ 5 Published Resources on Heart Sound Analysis
    ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era").
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，已收集了多个心脏声音数据库。这里简要介绍了以下可访问的数据库，详见[表3](#S5.T3 "Table 3 ‣ 5.1 Published Datasets
    ‣ 5 Published Resources on Heart Sound Analysis ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era")。
- en: 'The PASCAL challenge Database [[142](#bib.bib142)] was split into two sets
    A and B. In the dataset A, 176 heart sounds (0.393 hour) were recorded with the
    iStethoscope Pro iPhone app and annotated into S1 and S2 sounds for heart sound
    segmentation. Each heart sound in the dataset A was also labelled into one of
    the four classes: *normal*, *murmur*, *extra heart sound*, and *artifact*. The
    dataset B with 656 recordings (1.194 hours) was annotated into three classes:
    *normal*, *murmur*, and *extrasystole*.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: PASCAL挑战数据库[[142](#bib.bib142)]被分为两个集合A和B。在数据集A中，使用iStethoscope Pro iPhone应用程序记录了176个心脏声音（0.393小时），并标注为S1和S2声音以进行心脏声音分割。数据集A中的每个心脏声音还被标注为四类之一：*正常*、*杂音*、*额外心音*和*伪影*。数据集B包含656个录音（1.194小时），被标注为三类：*正常*、*杂音*和*额外收缩*。
- en: 'The PhysioNet/CinC Database [[58](#bib.bib58)], used in the PhysioNet/CinC
    Challenge 2016 [[143](#bib.bib143)], consists of multiple databases that were
    recorded from different data collectors. Especially, the publicly available training
    set has five databases collected from both healthy individuals and patients. The
    training set consists of 3,240 recordings (20.216 hours in total) from more than
    764 subjects. The task was set to classifying each data sample into three classes:
    *normal* and *abnormal*, and *noisy*.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: PhysioNet/CinC数据库[[58](#bib.bib58)]，用于PhysioNet/CinC挑战赛2016[[143](#bib.bib143)]，由多个数据库组成，这些数据库由不同的数据收集者记录。特别是，公开的训练集包括从健康个体和患者那里收集的五个数据库。训练集包括3,240个录音（总时长20.216小时），来自764名以上的受试者。任务是将每个数据样本分类为三类：*正常*、*异常*和*噪声*。
- en: 'The Heart Sounds Shenzhen (HSS) corpus [[92](#bib.bib92)], employed in the
    INTERSPEECH Computaional Paralinguistic challengE (ComParE) 2018, was collected
    by the Shenzhen University General Hospital. In total, 170 subjects (f: 55, m:
    115) participated in the data collection with an electronic stethoscope, leading
    to 845 heart sound recordings of 7.047 hours. Each audio sample was annotated
    into one of the three classes: *normal*, *mild*, *moderate/severe*.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 心脏声音深圳（HSS）语料库[[92](#bib.bib92)]，在INTERSPEECH计算语言学挑战赛（ComParE）2018中使用，由深圳大学附属医院收集。共有170名受试者（女：55，男：115）参与了数据收集，使用电子听诊器，共记录了845个心脏声音样本，总时长为7.047小时。每个音频样本被标注为以下三类之一：*正常*、*轻度*、*中度/重度*。
- en: 'An open heart sound database on GitHub [[144](#bib.bib144)] was collected for
    1,000 audio files (0.679 hour in total). The audio recordings are balanced in
    five classes: *normal*, *aortic stenosis*, *mitral regurgitation*, *mitral stenosis*,
    *mitral valve prolapse*.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上的一个开放心脏声音数据库[[144](#bib.bib144)]包含了1,000个音频文件（总时长0.679小时）。这些音频录音被平衡分为五类：*正常*、*主动脉狭窄*、*二尖瓣返流*、*二尖瓣狭窄*、*二尖瓣脱垂*。
- en: The Michigan Heart sound database³³3https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library⁴⁴4https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 密歇根心脏声音数据库³³3https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library⁴⁴4https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/
- en: 'primer_heartsound.html provides heart sound samples recorded from different
    areas and poses: the apex area when a subject is supine, the apex area for left
    decubitus, the aortic area when sitting, and the pulmonic area for supine. In
    total, 23 heart sounds were recorded and the total duration is 0.413 hour. The
    heart sounds were annotated into *normal* and multiple *pathological* states.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: primer_heartsound.html提供了来自不同区域和姿势的心脏声音样本：受试者仰卧时的心尖区、左侧卧时的心尖区、坐位时的主动脉区，以及仰卧时的肺动脉区。总共记录了23个心脏声音，时长为0.413小时。这些心脏声音被标注为*正常*和多种*病理*状态。
- en: 'The CirCor DigiScope Database [[145](#bib.bib145)], used in the George B. Moody
    PhysioNet Challenge 2022 [[146](#bib.bib146)], was collected from a pediatric
    population at or under 21 years old. The heart sounds were recorded from one or
    multiple locations: pulmonary valve, aortic valve, mitral valve, tricuspid valve,
    and others. The publicly available training set consist of 3,163 audio samples
    with 20.094 hours in total from 942 participants. Two classification tasks were
    targeted: i) *normal* and *abnormal*, and ii) *presence of murmurs*, *absence
    of murmurs*, and *unclear cases of murmurs*.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: CirCor DigiScope 数据库 [[145](#bib.bib145)]，用于 2022 年 George B. Moody PhysioNet
    挑战 [[146](#bib.bib146)]，是从 21 岁及以下的儿科人群中收集的。心音从一个或多个位置记录：肺动脉瓣，主动脉瓣，二尖瓣，三尖瓣等。公开可用的训练集包括
    3,163 个音频样本，总时长 20.094 小时，来自 942 名参与者。目标有两个分类任务：i) *正常* 和 *异常*，ii) *存在杂音*、*不存在杂音*
    和 *杂音情况不明确*。
- en: 'Table 3: Published datasets for heart sound classification. AS: aortic stenosis,
    MR: mitral regurgitation, MS: mitral stenosis, MVP: mitral valve prolapse. Notably,
    the statistics in this table considered access available data sets only.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '表 3: 已发布的心音分类数据集。AS: 主动脉狭窄，MR: 二尖瓣反流，MS: 二尖瓣狭窄，MVP: 二尖瓣脱垂。特别注意，本表中的统计数据仅考虑了可访问的数据集。'
- en: '| Dataset | Challenge | #Samples | Duration (h) | #Subjects | Task |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 挑战 | 样本数量 | 持续时间 (小时) | 受试者数量 | 任务 |'
- en: '| PASCAL Database [[142](#bib.bib142)] | PASCAL Challenge [[142](#bib.bib142)]
    | 176 | 0.393 | unknown | Dataset A: Normal, Murmur, Extra Heart Sound, Artifact
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| PASCAL 数据库 [[142](#bib.bib142)] | PASCAL 挑战 [[142](#bib.bib142)] | 176 |
    0.393 | unknown | 数据集 A: 正常，杂音，额外心音，伪影 |'
- en: '| 656 | 1.194 | unknown | Dataset B: Normal, Murmur, Extrasystole |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 656 | 1.194 | unknown | 数据集 B: 正常，杂音，早搏 |'
- en: '| PhysioNet/CinC Database [[58](#bib.bib58)] | PhysioNet/CinC Challenge 2016 [[143](#bib.bib143)]
    | 3,240 | 20.216 | 764+ | Normal, Abnormal, Too noisy or ambiguous |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| PhysioNet/CinC 数据库 [[58](#bib.bib58)] | PhysioNet/CinC 挑战 2016 [[143](#bib.bib143)]
    | 3,240 | 20.216 | 764+ | 正常，异常，过于嘈杂或模糊'
- en: '| HSS [[92](#bib.bib92)] | ComParE Challenge 2018 [[82](#bib.bib82)] | 845
    | 7.047 | 170 | Normal, Mild, Moderate/Severe |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| HSS [[92](#bib.bib92)] | ComParE 挑战 2018 [[82](#bib.bib82)] | 845 | 7.047
    | 170 | 正常，轻度，中度/重度 |'
- en: '| Data on GitHub[[144](#bib.bib144)] | – | 1,000 | 0.679 | unknown | Normal,
    AS, MR, MS, MVP |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| GitHub 上的数据[[144](#bib.bib144)] | – | 1,000 | 0.679 | unknown | 正常，AS，MR，MS，MVP
    |'
- en: '| Michigan Heart sound database⁵⁵5[https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library](https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library)[https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/primer_heartsound.html](https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/primer_heartsound.html)
    | – | 23 | 0.413 | unknown | Normal, Pathological |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 密歇根心音数据库⁵⁵5[https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library](https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library)[https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/primer_heartsound.html](https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/primer_heartsound.html)
    | – | 23 | 0.413 | unknown | 正常，病理 |'
- en: '| CirCor DigiScope Database [[145](#bib.bib145)] | George B. Moody PhysioNet
    Challenge 2022 [[146](#bib.bib146)] | 3,163 | 20.094 | 942 | Normal, abnormal;
    presence, absence, or unclear cases of murmurs |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| CirCor DigiScope 数据库 [[145](#bib.bib145)] | George B. Moody PhysioNet 挑战
    2022 [[146](#bib.bib146)] | 3,163 | 20.094 | 942 | 正常，异常；存在、不存在或不明确的杂音 |'
- en: 'Table 4: Evaluation metrics. $k$ is the $k$-th class of all $K$ classes; $N_{k}$,
    $\hat{N_{k}}$, and $\tilde{N_{k}}$ are the total number of samples, the total
    number of the predicted samples, and the number of correctly predicted samples
    for the $k$-th class, respectively. $N$ is the total number of all samples. TPR:
    true positive rate, FPR: false positive rate, AUC: area under curve, ROC: receiver
    operating characteristic, PR: precision-recall.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '表 4: 评估指标。 $k$ 是所有 $K$ 类中的第 $k$ 类；$N_{k}$、$\hat{N_{k}}$ 和 $\tilde{N_{k}}$ 分别是第
    $k$ 类的样本总数、预测样本总数以及正确预测的样本数量。 $N$ 是所有样本的总数。TPR: 真阳性率，FPR: 假阳性率，AUC: 曲线下面积，ROC:
    接收者操作特征，PR: 精确度-召回率。'
- en: '| Evaluation Metric | Description | Formula |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 评估指标 | 描述 | 公式 |'
- en: '| Sensitivity | Proportion of the actual positive cases which are correctly
    classified, i. e., true positive rate (TPR) | $\frac{\mbox{True Positives}}{\mbox{True
    Positives}+\mbox{False Negatives}}$ |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 灵敏度 | 实际阳性病例中被正确分类的比例，即真阳性率 (TPR) | $\frac{\mbox{真阳性}}{\mbox{真阳性}+\mbox{假阴性}}$
    |'
- en: '| Specificity | Proportion of the actual negative cases which are correctly
    classified, i. e., true negative rate (TNR) | $\frac{\mbox{True Negatives}}{\mbox{True
    Negatives}+\mbox{False Positives}}$ |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 特异性 | 正确分类的实际负样本比例，即真正负率（TNR） | $\frac{\mbox{真正负样本}}{\mbox{真正负样本}+\mbox{假阳性样本}}$
    |'
- en: '| Mean Accuracy | Arithmetic mean of sensitivity and specificity | $\frac{\mbox{sensitivity}+\mbox{specificity}}{2}$
    |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 平均准确率 | 敏感性和特异性的算术平均值 | $\frac{\mbox{敏感性}+\mbox{特异性}}{2}$ |'
- en: '| Youden’s index | A measure of the ability to balance sensitivity and specificity
    | $\mbox{sensitivity}+\mbox{specificity}-1$ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| Youden指数 | 衡量平衡敏感性和特异性的能力的指标 | $\mbox{敏感性}+\mbox{特异性}-1$ |'
- en: '| Precision | Proportion of correctly classified cases based on all cases predicted
    into $k$ | $\frac{\tilde{N_{k}}}{\hat{N_{k}}}$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 精确率 | 基于所有预测为$k$的案例的正确分类案例比例 | $\frac{\tilde{N_{k}}}{\hat{N_{k}}}$ |'
- en: '| Recall | Proportion of correctly classified cases based on all cases in the
    class $k$. | $\frac{\tilde{N_{k}}}{N_{k}}$ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 基于类别$k$中所有案例的正确分类案例比例。 | $\frac{\tilde{N_{k}}}{N_{k}}$ |'
- en: '| UAR | Unweighted average recall | $\frac{\sum_{k=1}^{K}\mbox{recall}_{k}}{K}$
    |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| UAR | 无权重平均召回率 | $\frac{\sum_{k=1}^{K}\mbox{召回率}_{k}}{K}$ |'
- en: '| WAR | Weighted average recall, i. e., accuracy | $\sum_{k=1}^{K}\frac{N_{k}}{N}\mbox{recall}_{k}$
    |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| WAR | 加权平均召回率，即准确率 | $\sum_{k=1}^{K}\frac{N_{k}}{N}\mbox{召回率}_{k}$ |'
- en: '| F1-Score | Harmonic mean of precision and recall | $2\times\frac{\mbox{precision}\times\mbox{recall}}{\mbox{precision}+\mbox{recall}}$
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| F1-Score | 精确率和召回率的调和均值 | $2\times\frac{\mbox{精确率}\times\mbox{召回率}}{\mbox{精确率}+\mbox{召回率}}$
    |'
- en: '| AUC-ROC | The area under the probability curve plotting TPR against FPR at
    various probability thresholds | —— |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| AUC-ROC | 在各种概率阈值下绘制TPR对FPR的概率曲线下的面积 | —— |'
- en: '| AUC-PR | The area under the probability curve plotting precision against
    recall at various probability thresholds | —— |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| AUC-PR | 在各种概率阈值下绘制精确率对召回率的概率曲线下的面积 | —— |'
- en: '| Average precision | Summarisation of a precision-recall curve as the weighted
    mean of precisions at each threshold | $\sum_{n}(\mbox{recall}_{n}-\mbox{recall}_{n-1})\times\mbox{precision}_{n}$
    |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 平均精确率 | 将精确率-召回率曲线总结为每个阈值下精确率的加权平均值 | $\sum_{n}(\mbox{召回率}_{n}-\mbox{召回率}_{n-1})\times\mbox{精确率}_{n}$
    |'
- en: 5.2 Evaluation Metrics
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 评估指标
- en: '[Table 4](#S5.T4 "Table 4 ‣ 5.1 Published Datasets ‣ 5 Published Resources
    on Heart Sound Analysis ‣ A Comprehensive Survey on Heart Sound Analysis in the
    Deep Learning Era") summarises evaluation metrics that can be used for evaluating
    model performance on heart sound classification.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '[表4](#S5.T4 "表 4 ‣ 5.1 发布的数据集 ‣ 5 关于心音分析的发布资源 ‣ 深度学习时代心音分析的综合调查") 总结了可用于评估心音分类模型性能的评价指标。'
- en: Class-wise evaluation metrics. In binary classification tasks, sensitivity and
    specificity focus on calculating the proportion of correctly classified positive
    samples and negative samples, respectively. In order to balance sensitivity and
    specificity, mean accuracy and Youden’s index are utilised. As for evaluating
    a model’s performance on a specific class in a multi-class classification task,
    precision and recall are frequently used. For a specific class, precision calculates
    the proportion of correctly classified cases based on all cases predicted to this
    class, whereas recall quantifies the proportion of correctly classified cases
    based on all cases in the class. Usually, there is an inverse relation between
    precision and recall. Therefore, their combinations (e. g., F1-Score, average
    precision) are referred to to better evaluate the performance of a model. Notably,
    in multi-class classification, recall is an extension of sensitivity.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 类别-wise 评估指标。在二分类任务中，敏感性和特异性关注于计算正确分类的正样本和负样本的比例。为了平衡敏感性和特异性，使用了平均准确率和Youden指数。对于多类别分类任务中评估模型在特定类别上的表现，通常使用精确率和召回率。对于特定类别，精确率计算基于所有预测为该类别的案例中的正确分类案例的比例，而召回率量化基于该类别所有案例中的正确分类案例的比例。通常，精确率和召回率之间存在逆相关关系。因此，结合它们（例如，F1分数，平均精确率）来更好地评估模型性能。值得注意的是，在多类别分类中，召回率是敏感性的扩展。
- en: Multi-class evaluation metrics. Besides the class-wise evaluation, there are
    other metrics across all classes, such as weighted average recall (WAR) and unweighted
    average recall (UAR). WAR, also known as accuracy and as the name implies, tends
    to overestimate a model if it performs better on the majority classes. Therefore,
    with an imbalanced dataset, UAR is preferred. UAR is the mean recall across classes
    w/o instance-based weighting. With various probability decision thresholds, we
    can also plot AUC-ROC and precision-recall curves. In addition, we would like
    to mention the cost-based metric devised in the PhysioNet Challenge 2022 [[146](#bib.bib146)],
    where cost calculation involves model pre-screening, expert screening, treatment,
    and diagnostic errors. In this way, more clinically practical models can be developed,
    especially in resource-constrained scenarios.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别评估指标。除了类别逐一评估外，还有其他跨类别的指标，例如加权平均召回率（WAR）和未加权平均召回率（UAR）。WAR，也被称为准确率，如其名称所示，往往会高估模型，特别是当模型在主要类别上表现更好时。因此，在数据集不平衡的情况下，UAR更为适用。UAR是跨类别的平均召回率，不包括基于实例的加权。通过各种概率决策阈值，我们还可以绘制AUC-ROC和精准度-召回率曲线。此外，我们还希望提到在PhysioNet
    Challenge 2022中设计的基于成本的指标[[146](#bib.bib146)]，其中成本计算涉及模型预筛选、专家筛选、治疗和诊断错误。这样，可以开发出更多在临床上实用的模型，特别是在资源有限的情况下。
- en: 5.3 Published Algorithms
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 发布的算法
- en: We discuss publicly available codes in the papers discussed in Section [4.2](#S4.SS2
    "4.2 Deep Learning for Classification ‣ 4 State-of-the-art Studies ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era") as well as other papers
    on open-access repositories during 2017–2022\. There are only a few publicly available
    codes in the past years, while there are many more released codes in 2016 benefiting
    from the PhysioNet challenge 2016 [[58](#bib.bib58)]. Notably, codes in 2016 are
    not listed, as we want to list the most state-of-the-art studies in the past six
    years.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了在第[4.2节](#S4.SS2 "4.2 Deep Learning for Classification ‣ 4 State-of-the-art
    Studies ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era")中提到的论文以及2017-2022年期间在开放获取存储库中发布的其他论文中的公开代码。近年来公开可用的代码较少，而2016年发布的代码数量较多，这得益于PhysioNet挑战赛2016[[58](#bib.bib58)]。特别地，我们未列出2016年的代码，因为我们希望列出过去六年中最前沿的研究。
- en: In [[144](#bib.bib144)], the authors released a Matlab code⁶⁶6https://github.com/yaseen21khan/Classification-of-Heart-Sound-Signal-Using-Multiple-Features-
    of training machine learning deep neural networks with inputs of multiple features,
    including MFCCs and discrete wavelets transform. In [[131](#bib.bib131)], a CNN
    model with time-convolutional units which simulate finite impulse response filters
    was implemented with Python in the code⁷⁷7https://github.com/mhealthbuet/heartnet.
    ResNets on linear and logarithmic spectrogram-image features were implemented
    in the Python code⁸⁸8https://github.com/mHealthBuet/CepsNET of the study [[147](#bib.bib147)].
    A study [[148](#bib.bib148)] released a Matlab code⁹⁹9https://github.com/uit-hdl/heart-sound-classification
    for detecting valvular heart disease from heart sounds and echocardiograms.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在[[144](#bib.bib144)]中，作者发布了一段Matlab代码⁶⁶6https://github.com/yaseen21khan/Classification-of-Heart-Sound-Signal-Using-Multiple-Features-，用于训练具有多个特征输入（包括MFCCs和离散小波变换）的机器学习深度神经网络。在[[131](#bib.bib131)]中，使用Python实现了一个包含时间卷积单元的CNN模型，这些单元模拟有限脉冲响应滤波器，代码链接为⁷⁷7https://github.com/mhealthbuet/heartnet。ResNets在线性和对数谱图像特征上的实现见Python代码⁸⁸8https://github.com/mHealthBuet/CepsNET，研究出处为[[147](#bib.bib147)]。[[148](#bib.bib148)]中发布了一段Matlab代码⁹⁹9https://github.com/uit-hdl/heart-sound-classification，用于从心音和超声心动图中检测瓣膜性心脏病。
- en: 6 Future Research Directions and Open Issues
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 未来研究方向和开放问题
- en: 6.1 Findings
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 发现
- en: In classification tasks which mainly focus on the screening of heart diseases,
    segmentation was often considered as a preprocessing procedure before classifying
    heart sounds. It is still an open question whether segmentation is helpful for
    classification or not.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在主要关注心脏病筛查的分类任务中，分割通常被视为在分类心音之前的预处理过程。分割是否对分类有帮助仍然是一个未解的问题。
- en: Segmentation + Classification. Many studies applied segmentation techniques
    or given segmentation information before the classification procedure. For instance,
    segmented cardiac cycles were used as the input of DL models in [[131](#bib.bib131)].
    Clips starting from the S1 heart sound with a fixed 1.6 s length were used for
    classification in [[129](#bib.bib129)]. In [[149](#bib.bib149)], the importance
    of segmentation was demonstrated for abnormal heart sound detection. Compared
    to models without segmentation information, segmentation did not significantly
    improve the model performance in the experiments [[149](#bib.bib149)]. The reason
    can be that models have already been powerful and robust, therefore, segmentation
    can be automatically done by the intermediate models layers. The authors in [[149](#bib.bib149)]
    proved this by explaining models with SHapley Additive exPlanations (SHAP) [[150](#bib.bib150)].
    The contribution of S1 and S2 sounds was found to be larger than other clips in
    a heart sound segment. Therefore, segmentation is required either as an additional
    procedure for classifiers that are not very powerful, or as an internal procedure
    in robust classifiers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 分段 + 分类。许多研究在分类过程中应用了分段技术或提供了分段信息。例如，分段的心动周期被用作DL模型的输入[[131](#bib.bib131)]。在[[129](#bib.bib129)]中，使用了从S1心音开始、固定长度为1.6秒的片段进行分类。在[[149](#bib.bib149)]中，证明了分段在异常心音检测中的重要性。与没有分段信息的模型相比，分段在实验中并没有显著提高模型性能[[149](#bib.bib149)]。原因可能是模型已经足够强大和稳健，因此，分段可以由中间模型层自动完成。[[149](#bib.bib149)]中的作者通过使用SHapley
    Additive exPlanations (SHAP)解释模型来证明这一点[[150](#bib.bib150)]。发现S1和S2声音在心音片段中的贡献大于其他片段。因此，分段要么作为不够强大的分类器的额外程序，或作为强大分类器的内部程序。
- en: No-segment. There are also many approaches which proposed to use non-segmented
    heart sounds, therefore, the complexity of automated auscultation can be reduced [[10](#bib.bib10),
    [52](#bib.bib52), [44](#bib.bib44), [90](#bib.bib90), [115](#bib.bib115)]. Apart
    from feeding a complete heart sound sample into neural networks, heart sounds
    are segmented into shorter clips with an equal length for model training [[50](#bib.bib50)].
    For instance, the fist 5 s of each audio sample were selected as the model input
    in [[7](#bib.bib7), [85](#bib.bib85), [93](#bib.bib93), [37](#bib.bib37)], and
    segmented 5 s clips were also used in [[91](#bib.bib91), [88](#bib.bib88), [116](#bib.bib116)].
    Most studies employ audio clips with a time length from 2 s to 6 s [[45](#bib.bib45),
    [34](#bib.bib34), [49](#bib.bib49), [51](#bib.bib51)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 无分段。还有许多方法提议使用未分段的心音，因此可以降低自动听诊的复杂性[[10](#bib.bib10), [52](#bib.bib52), [44](#bib.bib44),
    [90](#bib.bib90), [115](#bib.bib115)]。除了将完整的心音样本输入神经网络外，心音还被分段成长度相等的短片段用于模型训练[[50](#bib.bib50)]。例如，在[[7](#bib.bib7),
    [85](#bib.bib85), [93](#bib.bib93), [37](#bib.bib37)]中，选择了每个音频样本的前5秒作为模型输入，分段的5秒片段也被用于[[91](#bib.bib91),
    [88](#bib.bib88), [116](#bib.bib116)]。大多数研究使用的音频片段时长为2秒到6秒[[45](#bib.bib45), [34](#bib.bib34),
    [49](#bib.bib49), [51](#bib.bib51)]。
- en: 6.2 Limitations and Outlook
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 限制与展望
- en: Hardware development. In clinics, echocardiography obtains ultrasound scan with
    a small probe which emits high-frequency sound waves, therefore, physicians can
    diagnose via observing the heart and blood vessels, as well as blood flows^(10)^(10)10https://www.nhs.uk/conditions/echocardiogram/.
    However, echocardiography requires well-trained skills for professionals, limiting
    its usage in primary care. Classic acoustic stethoscopes used in primary care
    need physicians and nurses to be trained as well. To this end, there is a high
    demand of electronic stethoscopes in primary care. In recent years, electronic
    stethoscopes have been developed and produced to record heart sounds and transmit
    heart sounds to computers or mobile phones for further analysis [[26](#bib.bib26)].
    Most electronic stethoscopes can only achieve basic functions such as amplifying
    and visualising heart sounds without diagnosis. There are a few studies and hardware
    working on automated diagnosis more recently. For instance, a field-programmable
    gate array (FPGA) was designed to classify heart sounds via an LSTM-RNN model
    in [[151](#bib.bib151)]. “HD Steth with ECG”^(11)^(11)11https://www.stethoscope.com/hd-steth-with-ecg/
    embedded artificial intelligence (AI) into the electronic stethoscope to detect
    multiple cardiac abnormalities. As outlined throughout this overview, AI is promising
    to diagnose heart sound abnormalities, therefore reducing the requirement of well-trained
    professionals. Devices which diagnose cardiac diseases with high accuracies will
    be very helpful for promoting the early screening of cardiac diseases to primary
    care and home care.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 硬件开发。在诊所中，超声心动图通过发射高频声波的小探头获取超声扫描，因此，医生可以通过观察心脏和血管以及血流来进行诊断^(10)^(10)10https://www.nhs.uk/conditions/echocardiogram/。然而，超声心动图要求专业人员具备良好的技能，这限制了它在初级医疗中的使用。传统的听诊器也需要医生和护士经过培训。为此，初级医疗中对电子听诊器的需求很高。近年来，电子听诊器被开发和生产出来，以记录心音并将其传输到计算机或手机上进行进一步分析[[26](#bib.bib26)]。大多数电子听诊器只能实现基本功能，如放大和可视化心音，而无法进行诊断。最近有一些研究和硬件致力于自动化诊断。例如，一种现场可编程门阵列（FPGA）被设计用于通过LSTM-RNN模型对心音进行分类[[151](#bib.bib151)]。“HD
    Steth with ECG”^(11)^(11)11https://www.stethoscope.com/hd-steth-with-ecg/ 将人工智能（AI）嵌入电子听诊器中，以检测多种心脏异常。如本概述所述，人工智能在诊断心音异常方面具有很大潜力，从而减少对高技能专业人员的需求。能够高精度诊断心脏疾病的设备将对推动初级医疗和家庭护理的早期筛查非常有帮助。
- en: Performance improvement. Although automated auscultation is ideally expected
    to replace human analysis, model performance can be a bottleneck for applying
    automated auscultation to clinical usages. False negative predictions can result
    in delayed or missed therapies and aggravated condition. In future efforts, i)
    automated auscultation will be required to achieve very good performance and consider
    the difference between individuals in the context of personalised healthcare.
    The current research studies are mostly based on heart sounds only, while many
    types of individual information such as age can affect model performance [[152](#bib.bib152)].
    Therefore, how to integrate individual information will be a question for performance
    improvement. ii) In terms of ML and DL, we are currently witnessing the advent
    and adoption of foundation models [[153](#bib.bib153)] (pre-)trained on large
    to big data. We have already seen and listed above several approaches using pre-trained
    often self-supervised learnt models. However, one can expect even bigger models
    to appear with the potential emergence of abilities directly related to heart
    sound analysis as a ‘downstream’ task. On the other hand, the upcoming era of
    foundation models is expected to be marked by homogenisation, and it remains to
    be seen if the diversity of heart sound analysis approaches reported herein will
    indeed converge to a few large data trained models over the next years [[153](#bib.bib153)].
    iii) Furthermore, human-machine collaboration will be very promising to improve
    the system performance and provide more accurate diagnosis and in-time treatments
    for patients. Human-machine classification can be a solution to combine both machines’
    predictions and human’s (crowd workers’ and experts’) predictions for more precise
    diagnosis [[154](#bib.bib154)]. In [[154](#bib.bib154)], data samples predicted
    with high uncertainties were sent to crowd workers for majority voting; similarly,
    samples will be sent to an expert according to a certainty threshold of predictions
    from the crowd.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 性能提升。尽管自动听诊理想上应替代人工分析，但模型性能可能成为将自动听诊应用于临床的瓶颈。假阴性预测可能导致治疗延迟或遗漏，病情加重。在未来的工作中，i)
    自动听诊需要实现非常好的性能，并考虑个体差异以适应个性化医疗。当前的研究大多基于心音，而许多类型的个体信息，如年龄，可能会影响模型性能[[152](#bib.bib152)]。因此，如何整合个体信息将是性能提升的一个问题。ii)
    在机器学习和深度学习方面，我们目前见证了基础模型的出现和应用[[153](#bib.bib153)]（在大数据上进行的预训练）。我们已经看到并列出了几种使用预训练的自监督学习模型的方法。然而，预计会出现更大的模型，这些模型有可能出现与心音分析直接相关的能力作为“下游”任务。另一方面，预计基础模型的新时代将以同质化为特征，未来几年内，心音分析方法的多样性是否会收敛到少数几个大数据训练模型中仍有待观察[[153](#bib.bib153)]。iii)
    此外，人机协作将非常有前景，以提高系统性能，为患者提供更准确的诊断和及时的治疗。人机分类可以是一种解决方案，结合机器的预测和人类（众包工作者和专家）的预测，以实现更精确的诊断[[154](#bib.bib154)]。在[[154](#bib.bib154)]中，高不确定性预测的数据样本被送往众包工作者进行多数投票；类似地，根据众包预测的确定性阈值，样本将被送给专家。
- en: 6.3 Interpretable, Dependable, and Actionable Deep Heart Sound Analysis
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 可解释的、可靠的、可操作的深度心音分析
- en: Explainable deep learning has been emerging as one of the key topics in many
    applications, especially in the healthcare area. Building explainable deep learning
    models can help physicians and patients to trust the predictions. In a past study [[155](#bib.bib155)],
    an attention mechanism was used to visualise the contribution of each feature
    unit to the final predictions of heart sounds. Similarly, SHAP was also used to
    compute the contribution of each feature unit in [[150](#bib.bib150)]. The above
    explanation methods are both local explanations that interpret DL models case-by-case,
    lacking of a global explanation that can reveal the hidden classification rule
    in the models or summarise the characteristics of each heart sound class. Explainable
    DL models, such as deep neural decision trees [[156](#bib.bib156)], are promising
    to explain the models themselves from the perspectives of structure. Learnt or
    searched data samples of prototypes, criticisms, and counterfactuals [[106](#bib.bib106),
    [157](#bib.bib157)] can present the typical characteristics of each class, therefore,
    physicians can compare new samples with these heart sounds for better understanding
    and analysis. Specifically, by analysing the patterns of criticisms, physicians
    can potentially decrease the number of false negatives, which is crucial in the
    healthcare field. More recently, sonification was proposed to explain DL models
    for better human-computer interaction in [[158](#bib.bib158)]. Apart from visualisation,
    sonification can provide a new perspective to explain models by listening.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释深度学习已成为许多应用中的关键话题，特别是在医疗领域。构建可解释的深度学习模型可以帮助医生和患者信任预测结果。在过去的一项研究中[[155](#bib.bib155)]，使用了注意力机制来可视化每个特征单元对心音最终预测的贡献。类似地，SHAP也被用来计算每个特征单元的贡献[[150](#bib.bib150)]。上述解释方法都是局部解释，它们逐个案例解释深度学习模型，缺乏能够揭示模型隐藏分类规则或总结每个心音类别特征的全局解释。可解释的深度学习模型，如深度神经决策树[[156](#bib.bib156)]，有望从结构的角度解释模型本身。学习或搜索的原型、批评和反事实数据样本[[106](#bib.bib106),
    [157](#bib.bib157)]可以呈现每个类别的典型特征，因此医生可以将新样本与这些心音进行比较，以便更好地理解和分析。具体而言，通过分析批评的模式，医生可以潜在地减少假阴性的数量，这在医疗领域至关重要。最近，音频化被提出用于解释深度学习模型，以改善人机交互[[158](#bib.bib158)]。除了可视化，音频化可以通过听觉提供解释模型的新视角。
- en: In addition, given the health implications, it appears crucial that heart sound
    analysis by AI is utmost dependable [[159](#bib.bib159)]. There are mechanisms
    available, but more adaptation to the field of application if not novel algorithms
    will need to be designed. Ultimately, dependability will be a major driving factor
    for the trustability of according heart monitoring solutions – applied in everyday
    situations, trustability is a key factor to win users.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，考虑到健康影响，AI对心音分析的可靠性显得尤为重要[[159](#bib.bib159)]。虽然已有机制，但仍需要更多的应用领域适配或新算法的设计。最终，可靠性将是心脏监测解决方案可信度的主要推动因素——在日常使用中，可信度是赢得用户的关键因素。
- en: Moreover, to enable DL models being actionable in real-life, data privacy has
    been another emerging research topic in protecting users’ data from leakage or
    external attacks. Machine unlearning [[160](#bib.bib160)] and federated learning
    methods [[161](#bib.bib161)] can help healthcare institutions better organise
    patients’ private data in a secure way without losing diagnosis accuracy. Further,
    AI attacks on AI for heart sound analysis could be thought off, such as by adversarial
    attacks, and needs to dealt with. In summary, DL models are promising to guide
    healthcare providers’ actions in their daily practices for providing better care
    for patients. It will be essential to improve DL models from not only the performance,
    but also the human-centred perspectives in future.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了使深度学习模型在实际生活中具有可操作性，数据隐私已成为保护用户数据免受泄露或外部攻击的另一个新兴研究话题。机器遗忘[[160](#bib.bib160)]和联邦学习方法[[161](#bib.bib161)]可以帮助医疗机构以安全的方式更好地组织患者的私人数据，而不会丧失诊断准确性。此外，对心音分析的AI攻击也可能出现，例如对抗性攻击，需要加以应对。总之，深度学习模型有望指导医疗提供者在日常实践中采取行动，以为患者提供更好的护理。未来，不仅需要从性能方面改进深度学习模型，还需要从以人为本的角度进行改进。
- en: 7 Conclusion
  id: totrans-193
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论
- en: In this work, we summarised both classic machine learning and deep learning
    technologies applied to heart sound analysis during 2017–2022, including denoising,
    segmentation, and classification. Databases available for these above tasks were
    introduced with evaluation metrics in our study. We also listed publicly released
    codes for implementation of heart sound classification. Additionally, several
    findings and limitations of heart sound classification were analysed and possible
    future works were discussed. Finally, we discussed the importance of heart sound
    interpretation in the context of deep learning. This work is expected to present
    a summary of the advances of heart sound analysis, provide helpful discussions,
    and point out promising research directions that are helpful for the community.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项工作中，我们总结了2017–2022年间应用于心音分析的经典机器学习和深度学习技术，包括去噪、分割和分类。我们介绍了这些任务所用的数据库及其评估指标，并列出了公开发布的心音分类实现代码。此外，我们分析了心音分类的一些发现和局限性，并讨论了可能的未来研究工作。最后，我们讨论了在深度学习背景下心音解读的重要性。此项工作预计将呈现心音分析的进展总结，提供有益的讨论，并指出对社区有帮助的有前景的研究方向。
- en: References
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] U. Alam, O. Asghar, S. Q. Khan, S. Hayat, and R. A. Malik, “Cardiac auscultation:
    an essential clinical skill in decline,” British Journal of Cardiology, vol. 17,
    no. 1, p. 8, 2010.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] U. Alam, O. Asghar, S. Q. Khan, S. Hayat 和 R. A. Malik，“心脏听诊：一种日益下降的重要临床技能，”《英国心脏病学杂志》，第17卷，第1期，第8页，2010年。'
- en: '[2] I. R. Hanna and M. E. Silverman, “A history of cardiac auscultation and
    some of its contributors,” The American journal of cardiology, vol. 90, no. 3,
    pp. 259–267, 2002.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] I. R. Hanna 和 M. E. Silverman，“心脏听诊的历史及其一些贡献者，”《美国心脏病学杂志》，第90卷，第3期，第259–267页，2002年。'
- en: '[3] A. M. Noor and M. F. Shadi, “The heart auscultation. from sound to graphical,”
    Journal of Engineering and Technology, vol. 4, no. 2, pp. 73–84, 2013.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. M. Noor 和 M. F. Shadi，“心脏听诊：从声音到图形，”《工程与技术杂志》，第4卷，第2期，第73–84页，2013年。'
- en: '[4] H. Tang, M. Wang, Y. Hu, B. Guo, and T. Li, “Automated signal quality assessment
    for heart sound signal by novel features and evaluation in open public datasets,”
    BioMed Research International, vol. 30, pp. 1–15, 2021.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] H. Tang, M. Wang, Y. Hu, B. Guo 和 T. Li，“通过新特征和在开放公共数据集中的评估进行心音信号的自动信号质量评估，”《生物医学研究国际》，第30卷，第1–15页，2021年。'
- en: '[5] L. R. Rabiner, “A tutorial on hidden markov models and selected applications
    in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2, pp. 257–286,
    1989.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] L. R. Rabiner，“隐马尔可夫模型及其在语音识别中的选择性应用教程，”《IEEE会议录》，第77卷，第2期，第257–286页，1989年。'
- en: '[6] B. M. Whitaker, P. B. Suresha, C. Liu, G. D. Clifford, and D. V. Anderson,
    “Combining sparse coding and time-domain features for heart sound classification,”
    Physiological measurement, vol. 38, no. 8, p. 1701, 2017.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] B. M. Whitaker, P. B. Suresha, C. Liu, G. D. Clifford 和 D. V. Anderson，“结合稀疏编码和时域特征进行心音分类，”《生理测量》，第38卷，第8期，第1701页，2017年。'
- en: '[7] P. Langley and A. Murray, “Heart sound classification from unsegmented
    phonocardiograms,” Physiological measurement, vol. 38, no. 8, p. 1658, 2017.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] P. Langley 和 A. Murray，“从未分段的心音图中分类心音，”《生理测量》，第38卷，第8期，第1658页，2017年。'
- en: '[8] Z. Ren, N. Cummins, V. Pandit, J. Han, K. Qian, and B. Schuller, “Learning
    image-based representations for heart sound classification,” in Proc. DH, (Lyon,
    France), pp. 143–147, 2018.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] Z. Ren, N. Cummins, V. Pandit, J. Han, K. Qian 和 B. Schuller，“基于图像的心音分类学习，”在DH会议录，（法国里昂），第143–147页，2018年。'
- en: '[9] T. H. Chowdhury, K. N. Poudel, and Y. Hu, “Time-frequency analysis, denoising,
    compression, segmentation, and classification of pcg signals,” IEEE Access, vol. 8,
    pp. 160882–160890, 2020.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] T. H. Chowdhury, K. N. Poudel 和 Y. Hu，“心音信号的时频分析、去噪、压缩、分割和分类，”《IEEE Access》，第8卷，第160882–160890页，2020年。'
- en: '[10] B. Xiao, Y. Xu, X. Bi, J. Zhang, and X. Ma, “Heart sounds classification
    using a novel 1-D convolutional neural network with extremely low parameter consumption,”
    Neurocomputing, vol. 392, pp. 153–159, 2020.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] B. Xiao, Y. Xu, X. Bi, J. Zhang 和 X. Ma，“使用一种新型的1-D卷积神经网络进行心音分类，具有极低的参数消耗，”《神经计算》，第392卷，第153–159页，2020年。'
- en: '[11] A. K. Bhoi, K. S. Sherpa, and B. Khandelwal, “Multidimensional analytical
    study of heart sounds: A review,” International Journal Bioautomation, vol. 19,
    no. 3, pp. 351–376, 2015.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. K. Bhoi, K. S. Sherpa 和 B. Khandelwal，“心音的多维分析研究：综述，”《国际生物自动化杂志》，第19卷，第3期，第351–376页，2015年。'
- en: '[12] T. Chakrabarti, S. Saha, S. Roy, and I. Chel, “Phonocardiogram signal
    analysis-practices, trends and challenges: A critical review,” in Proc. IEMCON,
    (Vancouver, Canada), pp. 1–4, 2015.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] T. Chakrabarti, S. Saha, S. Roy, 和 I. Chel, “心音图信号分析：实践、趋势与挑战：一项关键评审”，发表于Proc.
    IEMCON，（加拿大温哥华），第1–4页，2015年。'
- en: '[13] M. Nabih-Ali, E.-S. A. El-Dahshan, and A. S. Yahia, “A review of intelligent
    systems for heart sound signal analysis,” Journal of medical engineering & technology,
    vol. 41, no. 7, pp. 553–563, 2017.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] M. Nabih-Ali, E.-S. A. El-Dahshan, 和 A. S. Yahia, “智能系统在心音信号分析中的应用回顾”，《医学工程与技术杂志》，第41卷，第7期，第553–563页，2017年。'
- en: '[14] G. D. Clifford et al., “Recent advances in heart sound analysis,” Physiological
    measurement, vol. 38, pp. E10–E25, 2017.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] G. D. Clifford 等, “心音分析的最新进展”，《生理测量》，第38卷，第E10–E25页，2017年。'
- en: '[15] S. K. Ghosh, P. R. Nagarajan, and R. K. Tripathy, Heart sound data acquisition
    and preprocessing techniques: A review. IGI Global, 2020.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] S. K. Ghosh, P. R. Nagarajan, 和 R. K. Tripathy, 《心音数据采集与预处理技术：一项回顾》。IGI
    Global，2020年。'
- en: '[16] B. Majhi and A. Kashyap, “Application of soft computing techniques to
    heart sound classification: A review of the decade,” Soft Computing Applications
    and Techniques in Healthcare, pp. 113–138, 2020.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] B. Majhi 和 A. Kashyap, “软计算技术在心音分类中的应用：十年的回顾”，《医疗保健中的软计算应用与技术》，第113–138页，2020年。'
- en: '[17] A. K. Dwivedi, S. A. Imtiaz, and E. Rodriguez-Villegas, “Algorithms for
    automatic analysis and classification of heart sounds–a systematic review,” IEEE
    Access, vol. 7, pp. 8316–8345, 2018.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] A. K. Dwivedi, S. A. Imtiaz, 和 E. Rodriguez-Villegas, “自动分析与分类心音的算法–系统回顾”，《IEEE
    Access》，第7卷，第8316–8345页，2018年。'
- en: '[18] A. J. Muñoz-Montoro, D. Suarez-Dou, R. Cortina, F. J. Cañadas-Quesada,
    and E. F. Combarro, “Parallel source separation system for heart and lung sounds,”
    The Journal of Supercomputing, vol. 77, no. 8, pp. 8135–8150, 2021.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] A. J. Muñoz-Montoro, D. Suarez-Dou, R. Cortina, F. J. Cañadas-Quesada,
    和 E. F. Combarro, “心肺声音的并行源分离系统”，《超级计算期刊》，第77卷，第8期，第8135–8150页，2021年。'
- en: '[19] K.-H. Tsai, W.-C. Wang, C.-H. Cheng, C.-Y. Tsai, J.-K. Wang, T.-H. Lin,
    S.-H. Fang, L.-C. Chen, and Y. Tsao, “Blind monaural source separation on heart
    and lung sounds based on periodic-coded deep autoencoder,” IEEE Journal of Biomedical
    and Health Informatics, vol. 24, no. 11, pp. 3203–3214, 2020.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] K.-H. Tsai, W.-C. Wang, C.-H. Cheng, C.-Y. Tsai, J.-K. Wang, T.-H. Lin,
    S.-H. Fang, L.-C. Chen, 和 Y. Tsao, “基于周期编码深度自编码器的心肺声音盲单耳源分离”，《IEEE生物医学与健康信息学期刊》，第24卷，第11期，第3203–3214页，2020年。'
- en: '[20] E. Grooby, J. He, D. Fattahi, L. Zhou, A. King, A. Ramanathan, A. Malhotra,
    G. A. Dumont, and F. Marzbanrad, “A new non-negative matrix co-factorisation approach
    for noisy neonatal chest sound separation,” in 2021 43rd Annual International
    Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 5668–5673,
    2021.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] E. Grooby, J. He, D. Fattahi, L. Zhou, A. King, A. Ramanathan, A. Malhotra,
    G. A. Dumont, 和 F. Marzbanrad, “一种新的非负矩阵共因子分解方法用于噪声新生儿胸部声音分离”，发表于2021年第43届IEEE医学与生物学学会年会（EMBC），第5668–5673页，2021年。'
- en: '[21] C. Ahlström, Processing of the Phonocardiographic Signal: methods for
    the intelligent stethoscope. PhD thesis, Institutionen för medicinsk teknik, 2006.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] C. Ahlström, 《心音图信号处理：智能听诊器的方法》。博士论文，医学技术系，2006年。'
- en: '[22] H. Naseri and M. Homaeinezhad, “Detection and boundary identification
    of phonocardiogram sounds using an expert frequency-energy based metric,” Annals
    of biomedical engineering, vol. 41, no. 2, pp. 279–292, 2013.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] H. Naseri 和 M. Homaeinezhad, “基于专家频率-能量度量的心音检测与边界识别”，《生物医学工程年鉴》，第41卷，第2期，第279–292页，2013年。'
- en: '[23] D. S. Gerbarg, A. Taranta, M. Spagnuolo, and J. J. Hofler, “Computer analysis
    of phonocardiograms,” Progress in Cardiovascular Diseases, vol. 5, pp. 393–405,
    1963.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] D. S. Gerbarg, A. Taranta, M. Spagnuolo, 和 J. J. Hofler, “计算机分析心音图”，《心血管疾病进展》，第5卷，第393–405页，1963年。'
- en: '[24] R. J. Martis, U. R. Acharya, and H. Adeli, “Current methods in electrocardiogram
    characterization,” Computers in biology and medicine, vol. 48, pp. 133–149, 2014.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] R. J. Martis, U. R. Acharya, 和 H. Adeli, “心电图特征化的现有方法”，《生物医学与医学中的计算机》，第48卷，第133–149页，2014年。'
- en: '[25] P. S. Molcer, I. Kecskes, V. Delić, E. Domijan, and M. Domijan, “Examination
    of formant frequencies for further classification of heart murmurs,” in Proc. SISY,
    (Subotica, Serbia), pp. 575–578, 2010.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] P. S. Molcer, I. Kecskes, V. Delić, E. Domijan, 和 M. Domijan, “心脏杂音的形式频率进一步分类的研究”，发表于Proc.
    SISY，（塞尔维亚苏博蒂察），第575–578页，2010年。'
- en: '[26] S. Leng, R. San Tan, K. T. C. Chai, C. Wang, D. Ghista, and L. Zhong,
    “The electronic stethoscope,” Biomedical engineering online, vol. 14, no. 1, pp. 1–37,
    2015.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] S. Leng, R. San Tan, K. T. C. Chai, C. Wang, D. Ghista, 和 L. Zhong, “电子听诊器，”《生物医学工程在线》，第14卷，第1期，第1–37页，2015年。'
- en: '[27] E. Delgado-Trejos, A. Quiceno-Manrique, J. Godino-Llorente, M. Blanco-Velasco,
    and G. Castellanos-Dominguez, “Digital auscultation analysis for heart murmur
    detection,” Annals of biomedical engineering, vol. 37, no. 2, pp. 337–353, 2009.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] E. Delgado-Trejos, A. Quiceno-Manrique, J. Godino-Llorente, M. Blanco-Velasco,
    和 G. Castellanos-Dominguez, “数字听诊分析用于心脏杂音检测，”《生物医学工程年鉴》，第37卷，第2期，第337–353页，2009年。'
- en: '[28] Y. Tsao, T.-H. Lin, F. Chen, Y.-F. Chang, C.-H. Cheng, and K.-H. Tsai,
    “Robust s1 and s2 heart sound recognition based on spectral restoration and multi-style
    training,” Biomedical Signal Processing and Control, vol. 49, pp. 173–180, 2019.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Y. Tsao, T.-H. Lin, F. Chen, Y.-F. Chang, C.-H. Cheng, 和 K.-H. Tsai, “基于光谱恢复和多风格训练的稳健s1和s2心音识别，”《生物医学信号处理与控制》，第49卷，第173–180页，2019年。'
- en: '[29] O. Deperlioglu, “Classification of segmented phonocardiograms by convolutional
    neural networks,” BRAIN. Broad Research in Artificial Intelligence and Neuroscience,
    vol. 10, no. 2, pp. 5–13, 2019.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] O. Deperlioglu, “利用卷积神经网络对分段心音图进行分类，”《脑——人工智能与神经科学广泛研究》，第10卷，第2期，第5–13页，2019年。'
- en: '[30] Ö. DEPERLİĞLU, “Classification of segmented heart sounds with artificial
    neural networks,” International Journal of Applied Mathematics Electronics and
    Computers, vol. 6, no. 4, pp. 39–44, 2018.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Ö. DEPERLİĞLU, “利用人工神经网络对分段心音进行分类，”《应用数学电子与计算机国际期刊》，第6卷，第4期，第39–44页，2018年。'
- en: '[31] H. Alaskar, N. Alzhrani, A. Hussain, and F. Almarshed, “The implementation
    of pretrained AlexNet on PCG classification,” in Proc. ICIC, (San Sebastian, Basque
    Country), pp. 784–794, 2019.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] H. Alaskar, N. Alzhrani, A. Hussain, 和 F. Almarshed, “在PCG分类中应用预训练的AlexNet，”在ICIC会议上，（西班牙圣塞巴斯蒂安），第784–794页，2019年。'
- en: '[32] N. M. Khan, M. S. Khan, and G. M. Khan, “Automated heart sound classification
    from unsegmented phonocardiogram signals using time frequency features,” International
    Journal of Computer and Information Engineering, vol. 12, no. 8, pp. 598–603,
    2018.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] N. M. Khan, M. S. Khan, 和 G. M. Khan, “利用时间频率特征从未分段的心音图信号中自动分类心音，”《计算机与信息工程国际期刊》，第12卷，第8期，第598–603页，2018年。'
- en: '[33] A. Yadav, M. K. Dutta, C. M. Travieso, and J. B. Alonso, “Automatic classification
    of normal and abnormal PCG recording heart sound recording using Fourier transform,”
    in Proc. IWOBI, (Alajuela Province, Costa Rica), pp. 1–9, 2018.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] A. Yadav, M. K. Dutta, C. M. Travieso, 和 J. B. Alonso, “利用傅里叶变换自动分类正常与异常PCG录音的心音，”在IWOBI会议上，（哥斯达黎加阿拉胡埃拉省），第1–9页，2018年。'
- en: '[34] Q. Hu, J. Hu, X. Yu, and Y. Liu, “Automatic heart sound classification
    using one dimension deep neural network,” in International Conference on Security,
    Privacy and Anonymity in Computation, Communication and Storage, pp. 200–208,
    2020.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Q. Hu, J. Hu, X. Yu, 和 Y. Liu, “使用一维深度神经网络进行自动心音分类，”在《计算、通信和存储中的安全、隐私和匿名国际会议》上，第200–208页，2020年。'
- en: '[35] S. K. Ghosh, R. K. Tripathy, R. Ponnalagu, and R. B. Pachori, “Automated
    detection of heart valve disorders from the PCG signal using time-frequency magnitude
    and phase features,” IEEE Sensors Letters, vol. 3, no. 12, pp. 1–4, 2019.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] S. K. Ghosh, R. K. Tripathy, R. Ponnalagu, 和 R. B. Pachori, “利用时间-频率幅值和相位特征从PCG信号中自动检测心脏瓣膜疾病，”《IEEE传感器快报》，第3卷，第12期，第1–4页，2019年。'
- en: '[36] B. Ahmad, F. A. Khan, K. N. Khan, and M. S. Khan, “Automatic classification
    of heart sounds using long short-term memory,” in Proc. ICOSST, (Virtual Event),
    pp. 1–6, 2021.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] B. Ahmad, F. A. Khan, K. N. Khan, 和 M. S. Khan, “使用长短期记忆网络自动分类心音，”在ICOSST会议上，（虚拟活动），第1–6页，2021年。'
- en: '[37] S. A. Singh, T. G. Meitei, and S. Majumder, “Short pcg classification
    based on deep learning,” in Deep Learning Techniques for Biomedical and Health
    Informatics, pp. 141–164, Elsevier, 2020.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. A. Singh, T. G. Meitei, 和 S. Majumder, “基于深度学习的短期PCG分类，”在《生物医学与健康信息学的深度学习技术》一书中，第141–164页，Elsevier出版社，2020年。'
- en: '[38] F. Noman, S.-H. Salleh, C.-M. Ting, S. B. Samdin, H. Ombao, and H. Hussain,
    “A markov-switching model approach to heart sound segmentation and classification,”
    IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 3, pp. 705–716,
    2019.'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] F. Noman, S.-H. Salleh, C.-M. Ting, S. B. Samdin, H. Ombao, 和 H. Hussain,
    “基于马尔可夫切换模型的心音分段与分类方法，”《IEEE生物医学与健康信息学期刊》，第24卷，第3期，第705–716页，2019年。'
- en: '[39] F. Noman, C.-M. Ting, S.-H. Salleh, and H. Ombao, “Short-segment heart
    sound classification using an ensemble of deep convolutional neural networks,”
    in Proc. ICASSP, (Brighton, UK), pp. 1318–1322, 2019.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] F. Noman, C.-M. Ting, S.-H. Salleh 和 H. Ombao，“使用深度卷积神经网络集成进行短段心音分类”，发表于
    Proc. ICASSP，（英国布莱顿），第 1318–1322 页，2019 年。'
- en: '[40] J. Dastagir, F. A. Khan, M. S. Khan, and K. N. Khan, “Computer-aided phonocardiogram
    classification using multidomain time and frequency features,” in Proc. ICAI,
    (Islamabad, Pakistan), pp. 50–55, 2021.'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] J. Dastagir, F. A. Khan, M. S. Khan 和 K. N. Khan，“使用多域时间和频率特征的计算机辅助心音图分类”，发表于
    Proc. ICAI，（巴基斯坦伊斯兰堡），第 50–55 页，2021 年。'
- en: '[41] A. Bourouhou, A. Jilbab, C. Nacir, and A. Hammouch, “Heart sound signals
    segmentation and multiclass classification,” International Journal of Online and
    Biomedical Engineering, vol. 16, no. 15, pp. 64–79, 2020.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] A. Bourouhou, A. Jilbab, C. Nacir 和 A. Hammouch，“心音信号分段和多类别分类”，《国际在线与生物医学工程期刊》，第
    16 卷，第 15 期，第 64–79 页，2020 年。'
- en: '[42] A. Meintjes, A. Lowe, and M. Legget, “Fundamental heart sound classification
    using the continuous wavelet transform and convolutional neural networks,” in
    Proc. EMBC, (Honolulu, Hawaii), pp. 409–412, 2018.'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] A. Meintjes, A. Lowe 和 M. Legget，“使用连续小波变换和卷积神经网络的基本心音分类”，发表于 Proc. EMBC，（夏威夷檀香山），第
    409–412 页，2018 年。'
- en: '[43] Z. Abduh, E. A. Nehary, M. A. Wahed, and Y. M. Kadah, “Classification
    of heart sounds using fractional fourier transform based mel-frequency spectral
    coefficients and traditional classifiers,” Biomedical Signal Processing and Control,
    vol. 57, p. 101788, 2020.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Z. Abduh, E. A. Nehary, M. A. Wahed 和 Y. M. Kadah，“基于分数傅里叶变换的梅尔频率谱系数及传统分类器的心音分类”，《生物医学信号处理与控制》，第
    57 卷，第 101788 页，2020 年。'
- en: '[44] S. A. Singh, S. Majumder, and M. Mishra, “Classification of short unsegmented
    heart sound based on deep learning,” in Proc. I2MTC, (Auckland, New Zealand),
    pp. 1–6, 2019.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] S. A. Singh, S. Majumder 和 M. Mishra，“基于深度学习的短暂未分段心音分类”，发表于 Proc. I2MTC，（新西兰奥克兰），第
    1–6 页，2019 年。'
- en: '[45] S. Li, F. Li, S. Tang, and F. Luo, “Heart sounds classification based
    on feature fusion using lightweight neural networks,” IEEE Transactions on Instrumentation
    and Measurement, vol. 70, pp. 1–9, 2021.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] S. Li, F. Li, S. Tang 和 F. Luo，“基于轻量级神经网络的心音分类特征融合”，《IEEE 仪器与测量学报》，第 70
    卷，第 1–9 页，2021 年。'
- en: '[46] N. Ibrahim, N. Jamal, M. N. A.-H. Sha’abani, and L. F. Mahadi, “A comparative
    study of heart sound signal classification based on temporal, spectral and geometric
    features,” in Proc. IECBES, (Langkawi, Malaysia), pp. 24–29, 2020.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] N. Ibrahim, N. Jamal, M. N. A.-H. Sha’abani 和 L. F. Mahadi，“基于时间、谱和几何特征的心音信号分类比较研究”，发表于
    Proc. IECBES，（马来西亚兰卡威），第 24–29 页，2020 年。'
- en: '[47] W. Zhang, J. Han, and S. Deng, “Heart sound classification based on scaled
    spectrogram and tensor decomposition,” Expert Systems with Applications, vol. 84,
    pp. 220–231, 2017.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] W. Zhang, J. Han 和 S. Deng，“基于缩放谱图和张量分解的心音分类”，《应用系统与应用》，第 84 卷，第 220–231
    页，2017 年。'
- en: '[48] W. Zhang, J. Han, and S. Deng, “Heart sound classification based on scaled
    spectrogram and partial least squares regression,” Biomedical Signal Processing
    and Control, vol. 32, pp. 20–28, 2017.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] W. Zhang, J. Han 和 S. Deng，“基于缩放谱图和偏最小二乘回归的心音分类”，《生物医学信号处理与控制》，第 32 卷，第
    20–28 页，2017 年。'
- en: '[49] M. Banerjee and S. Majhi, “Multi-class heart sounds classification using
    2D-convolutional neural network,” in Proc. ICCCS, (Guangzhou, China), pp. 1–6,
    2020.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] M. Banerjee 和 S. Majhi，“使用 2D 卷积神经网络进行多类别心音分类”，发表于 Proc. ICCCS，（中国广州），第
    1–6 页，2020 年。'
- en: '[50] J. M.-T. Wu, M.-H. Tsai, Y. Z. Huang, S. H. Islam, M. M. Hassan, A. Alelaiwi,
    and G. Fortino, “Applying an ensemble convolutional neural network with Savitzky–Golay
    filter to construct a phonocardiogram prediction model,” Applied Soft Computing,
    vol. 78, pp. 29–40, 2019.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] J. M.-T. Wu, M.-H. Tsai, Y. Z. Huang, S. H. Islam, M. M. Hassan, A. Alelaiwi
    和 G. Fortino，“应用 Savitzky–Golay 滤波器的卷积神经网络集成构建心音图预测模型”，《应用软计算》，第 78 卷，第 29–40
    页，2019 年。'
- en: '[51] P. T. Krishnan, P. Balasubramanian, and S. Umapathy, “Automated heart
    sound classification system from unsegmented phonocardiogram (pcg) using deep
    neural network,” Physical and Engineering Sciences in Medicine, vol. 43, no. 2,
    pp. 505–515, 2020.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] P. T. Krishnan, P. Balasubramanian 和 S. Umapathy，“基于深度神经网络的未分段心音分类系统”，《物理与工程医学科学》，第
    43 卷，第 2 期，第 505–515 页，2020 年。'
- en: '[52] W. Zeng, J. Yuan, C. Yuan, Q. Wang, F. Liu, and Y. Wang, “A new approach
    for the detection of abnormal heart sound signals using TQWT, VMD and neural networks,”
    Artificial Intelligence Review, vol. 54, no. 3, pp. 1613–1647, 2021.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] W. Zeng, J. Yuan, C. Yuan, Q. Wang, F. Liu, 和 Y. Wang, “一种使用 TQWT、VMD
    和神经网络检测异常心音信号的新方法，” *人工智能评论*，第 54 卷，第 3 期，页码 1613–1647，2021 年。'
- en: '[53] M. V. Shervegar and G. V. Bhat, “Heart sound classification using gaussian
    mixture model,” Porto Biomedical Journal, vol. 3, no. 1, pp. 1–7, 2018.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] M. V. Shervegar 和 G. V. Bhat, “使用高斯混合模型的心音分类，” *波尔图生物医学期刊*，第 3 卷，第 1 期，页码
    1–7，2018 年。'
- en: '[54] B. Al-Naami, H. Fraihat, N. Y. Gharaibeh, and A.-R. M. Al-Hinnawi, “A
    framework classification of heart sound signals in physionet challenge 2016 using
    high order statistics and adaptive neuro-fuzzy inference system,” IEEE Access,
    vol. 8, pp. 224852–224859, 2020.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] B. Al-Naami, H. Fraihat, N. Y. Gharaibeh, 和 A.-R. M. Al-Hinnawi, “使用高阶统计和自适应神经模糊推理系统对
    PhysioNet 挑战 2016 中的心音信号进行分类的框架，” *IEEE 访问*，第 8 卷，页码 224852–224859，2020 年。'
- en: '[55] A. I. Humayun, M. Khan, S. Ghaffarzadegan, Z. Feng, T. Hasan, et al.,
    “An ensemble of transfer, semi-supervised and supervised learning methods for
    pathological heart sound classification,” in Proc. INTERSPEECH, (Hyderabad, India),
    pp. 127–131, 2018.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] A. I. Humayun, M. Khan, S. Ghaffarzadegan, Z. Feng, T. Hasan, 等， “用于病理心音分类的转移学习、半监督学习和监督学习方法的集成，”
    在 *INTERSPEECH 会议论文集*，(印度海得拉巴)，页码 127–131，2018 年。'
- en: '[56] S. Boll, “Suppression of acoustic noise in speech using spectral subtraction,”
    IEEE Transactions on acoustics, speech, and signal processing, vol. 27, no. 2,
    pp. 113–120, 1979.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] S. Boll, “使用谱减法抑制语音中的噪声，” *IEEE 声学、语音与信号处理学报*，第 27 卷，第 2 期，页码 113–120，1979
    年。'
- en: '[57] V. N. Varghees and K. Ramachandran, “Effective heart sound segmentation
    and murmur classification using empirical wavelet transform and instantaneous
    phase for electronic stethoscope,” IEEE Sensors Journal, vol. 17, no. 12, pp. 3861–3872,
    2017.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] V. N. Varghees 和 K. Ramachandran, “使用经验小波变换和瞬时相位进行有效的心音分割和杂音分类，以用于电子听诊器，”
    *IEEE 传感器期刊*，第 17 卷，第 12 期，页码 3861–3872，2017 年。'
- en: '[58] C. Liu, D. Springer, et al., “An open access database for the evaluation
    of heart sound algorithms,” Physiological Measurement, vol. 37, no. 12, p. 2181,
    2016.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] C. Liu, D. Springer, 等， “用于评估心音算法的开放访问数据库，” *生理测量*，第 37 卷，第 12 期，页码 2181，2016
    年。'
- en: '[59] M. Baydoun, L. Safatly, H. Ghaziri, and A. El Hajj, “Analysis of heart
    sound anomalies using ensemble learning,” Biomedical Signal Processing and Control,
    vol. 62, p. 102019, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] M. Baydoun, L. Safatly, H. Ghaziri, 和 A. El Hajj, “使用集成学习分析心音异常，” *生物医学信号处理与控制*，第
    62 卷，页码 102019，2020 年。'
- en: '[60] P. Upretee and M. E. Yüksel, “Accurate classification of heart sounds
    for disease diagnosis by a single time-varying spectral feature: Preliminary results,”
    in Proc. EBBT, (Istanbul, Turkey), pp. 1–4, 2019.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] P. Upretee 和 M. E. Yüksel, “通过单一时间变化光谱特征进行疾病诊断的准确心音分类：初步结果，” 在 *EBBT 会议论文集*，(土耳其伊斯坦布尔)，页码
    1–4，2019 年。'
- en: '[61] J. X. Low and K. W. Choo, “Classification of heart sounds using softmax
    regression and convolutional neural network,” in Proc. ICCET, (New York, NY),
    pp. 18–21, 2018.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] J. X. Low 和 K. W. Choo, “使用 softmax 回归和卷积神经网络进行心音分类，” 在 *ICCET 会议论文集*，(纽约,
    NY)，页码 18–21，2018 年。'
- en: '[62] O. Deperlioglu, “Heart sound classification with signal instant energy
    and stacked autoencoder network,” Biomedical Signal Processing and Control, vol. 64,
    p. 102211, 2021.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] O. Deperlioglu, “基于信号瞬时能量和堆叠自编码器网络的心音分类，” *生物医学信号处理与控制*，第 64 卷，页码 102211，2021
    年。'
- en: '[63] G. Eslamizadeh and R. Barati, “Heart murmur detection based on wavelet
    transformation and a synergy between artificial neural network and modified neighbor
    annealing methods,” Artificial intelligence in medicine, vol. 78, pp. 23–40, 2017.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] G. Eslamizadeh 和 R. Barati, “基于小波变换和人工神经网络与改进邻域退火方法的协同作用的心脏杂音检测，” *医学中的人工智能*，第
    78 卷，页码 23–40，2017 年。'
- en: '[64] M. U. Akram, A. Shaukat, F. Hussain, S. G. Khawaja, W. H. Butt, et al.,
    “Analysis of PCG signals using quality assessment and homomorphic filters for
    localization and classification of heart sounds,” Computer methods and programs
    in biomedicine, vol. 164, pp. 143–157, 2018.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] M. U. Akram, A. Shaukat, F. Hussain, S. G. Khawaja, W. H. Butt, 等， “利用质量评估和同态滤波器分析
    PCG 信号，以实现心音的定位和分类，” *计算机方法与生物医学程序*，第 164 卷，页码 143–157，2018 年。'
- en: '[65] M. V. Shervegar and G. V. Bhat, “Automatic segmentation of phonocardiogram
    using the occurrence of the cardiac events,” Informatics in Medicine Unlocked,
    vol. 9, pp. 6–10, 2017.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] M. V. Shervegar 和 G. V. Bhat, “利用心脏事件的发生进行心音图的自动分割，” *医学信息解锁*，第 9 卷，页码
    6–10，2017 年。'
- en: '[66] S. Das, S. Pal, and M. Mitra, “Acoustic feature based unsupervised approach
    of heart sound event detection,” Computers in Biology and Medicine, vol. 126,
    p. 103990, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] S. Das, S. Pal, 和 M. Mitra，"基于声学特征的心音事件检测的无监督方法"，《计算生物医学》，第126卷，页码103990，2020年。'
- en: '[67] S. E. Schmidt, C. Holst-Hansen, C. Graff, E. Toft, and J. J. Struijk,
    “Segmentation of heart sound recordings by a duration-dependent hidden markov
    model,” Physiological measurement, vol. 31, no. 4, p. 513, 2010.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] S. E. Schmidt, C. Holst-Hansen, C. Graff, E. Toft, 和 J. J. Struijk，"基于持续时间依赖的隐马尔可夫模型的心音录音分割"，《生理测量》，第31卷，第4期，页码513，2010年。'
- en: '[68] S. Shukla, S. K. Singh, and D. Mitra, “An efficient heart sound segmentation
    approach using kurtosis and zero frequency filter features,” Biomedical Signal
    Processing and Control, vol. 57, p. 101762, 2020.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] S. Shukla, S. K. Singh, 和 D. Mitra，"一种高效的心音分割方法，使用峭度和零频滤波器特征"，《生物医学信号处理与控制》，第57卷，页码101762，2020年。'
- en: '[69] A. P. Kamson, L. Sharma, and S. Dandapat, “Multi-centroid diastolic duration
    distribution based HSMM for heart sound segmentation,” Biomedical Signal Processing
    and Control, vol. 48, pp. 265–272, 2019.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. P. Kamson, L. Sharma, 和 S. Dandapat，"基于多中心舒张期持续时间分布的HSMM用于心音分割"，《生物医学信号处理与控制》，第48卷，页码265–272，2019年。'
- en: '[70] J. Oliveira, F. Renna, and M. Coimbra, “A subject-driven unsupervised
    hidden semi-Markov model and Gaussian mixture model for heart sound segmentation,”
    IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 2, pp. 323–331,
    2019.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] J. Oliveira, F. Renna, 和 M. Coimbra，"一种以主体为驱动的无监督隐半马尔可夫模型和高斯混合模型用于心音分割"，《IEEE选择主题信号处理期刊》，第13卷，第2期，页码323–331，2019年。'
- en: '[71] J. Oliveira, F. Renna, T. Mantadelis, and M. Coimbra, “Adaptive sojourn
    time HSMM for heart sound segmentation,” IEEE Journal of Biomedical and Health
    Informatics, vol. 23, no. 2, pp. 642–649, 2019.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] J. Oliveira, F. Renna, T. Mantadelis, 和 M. Coimbra，"用于心音分割的自适应停留时间HSMM"，《IEEE生物医学与健康信息学期刊》，第23卷，第2期，页码642–649，2019年。'
- en: '[72] J. Rubin, R. Abreu, A. Ganguli, S. Nelaturi, I. Matei, and K. Sricharan,
    “Recognizing abnormal heart sounds using deep learning,” arXiv preprint arXiv:1707.04642,
    2017.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] J. Rubin, R. Abreu, A. Ganguli, S. Nelaturi, I. Matei, 和 K. Sricharan，"使用深度学习识别异常心音"，arXiv预印本arXiv:1707.04642，2017年。'
- en: '[73] V. Maknickas and A. Maknickas, “Recognition of normal–abnormal phonocardiographic
    signals using deep convolutional neural networks and mel-frequency spectral coefficients,”
    Physiological measurement, vol. 38, no. 8, p. 1671, 2017.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] V. Maknickas 和 A. Maknickas，"使用深度卷积神经网络和梅尔频率谱系数识别正常与异常的心音信号"，《生理测量》，第38卷，第8期，页码1671，2017年。'
- en: '[74] K. Liu, L. Yuan, C. Huang, W. Wu, Q. Wang, and G. Wu, “Abnormal heart
    sound detection by using temporal convolutional network,” in Proc. IPEC, (Potsdam,
    Germany), pp. 1026–1029, 2022.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] K. Liu, L. Yuan, C. Huang, W. Wu, Q. Wang, 和 G. Wu，"使用时间卷积网络检测异常心音"，在《IPEC会议录》上，（德国波茨坦），页码1026–1029，2022年。'
- en: '[75] F. A. Khan, A. Abid, and M. S. Khan, “Automatic heart sound classification
    from segmented/unsegmented phonocardiogram signals using time and frequency features,”
    Physiological measurement, vol. 41, no. 5, p. 055006, 2020.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] F. A. Khan, A. Abid, 和 M. S. Khan，"基于时间和频率特征的自动心音分类，针对分割/未分割的心音图信号"，《生理测量》，第41卷，第5期，页码055006，2020年。'
- en: '[76] Y. Chen, S. Wei, and Y. Zhang, “Classification of heart sounds based on
    the combination of the modified frequency wavelet transform and convolutional
    neural network,” Medical & Biological Engineering & Computing, vol. 58, no. 9,
    pp. 2039–2047, 2020.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Y. Chen, S. Wei, 和 Y. Zhang，"基于改进频率小波变换和卷积神经网络的心音分类"，《医学与生物工程与计算》，第58卷，第9期，页码2039–2047，2020年。'
- en: '[77] W. Han, Z. Yang, J. Lu, and S. Xie, “Supervised threshold-based heart
    sound classification algorithm,” Physiological Measurement, vol. 39, no. 11, p. 115011,
    2018.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] W. Han, Z. Yang, J. Lu, 和 S. Xie，"基于监督阈值的心音分类算法"，《生理测量》，第39卷，第11期，页码115011，2018年。'
- en: '[78] W. Han, S. Xie, Z. Yang, S. Zhou, and H. Huang, “Heart sound classification
    using the SNMFNet classifier,” Physiological measurement, vol. 40, no. 10, p. 105003,
    2019.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] W. Han, S. Xie, Z. Yang, S. Zhou, 和 H. Huang，"使用SNMFNet分类器进行心音分类"，《生理测量》，第40卷，第10期，页码105003，2019年。'
- en: '[79] J. F. Chen and X. Dang, “Heart sound analysis based on extended features
    and related factors,” in Proc. SSCI, (Xiamen, China), pp. 2189–2194, 2019.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] J. F. Chen 和 X. Dang，"基于扩展特征和相关因素的心音分析"，在《SSCI会议录》上，（中国厦门），页码2189–2194，2019年。'
- en: '[80] D. B. Springer, L. Tarassenko, and G. D. Clifford, “Logistic regression-hsmm-based
    heart sound segmentation,” IEEE transactions on biomedical engineering, vol. 63,
    no. 4, pp. 822–832, 2015.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] D. B. Springer, L. Tarassenko, 和 G. D. Clifford，“基于逻辑回归-HSMM的心音分割，” IEEE生物医学工程学报，卷63，第4期，页822–832，2015年。'
- en: '[81] A. Duggento, A. Conti, M. Guerrisi, and N. Toschi, “Classification of
    real-world pathological phonocardiograms through multi-instance learning,” in
    Proc. EMBC, (Virtual Event), pp. 771–774, 2021.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] A. Duggento, A. Conti, M. Guerrisi, 和 N. Toschi，“通过多实例学习对真实世界病理心音图进行分类，”
    在EMBC会议论文集中，（虚拟活动），页771–774，2021年。'
- en: '[82] B. Schuller et al., “The INTERSPEECH 2018 computational paralinguistics
    challenge: Atypical & self-assessed affect, crying & heart beats,” in Proc. INTERSPEECH,
    (Hyderbad, India), pp. 122–126, 2018.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] B. Schuller 等，“INTERSPEECH 2018计算言语学挑战：非典型和自我评估情感、哭泣与心跳，” 在INTERSPEECH会议论文集中，（印度海德拉巴），页122–126，2018年。'
- en: '[83] F. Eyben, K. R. Scherer, B. W. Schuller, et al., “The geneva minimalistic
    acoustic parameter set (gemaps) for voice research and affective computing,” IEEE
    transactions on affective computing, vol. 7, no. 2, pp. 190–202, 2015.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] F. Eyben, K. R. Scherer, B. W. Schuller 等，“日内瓦简约声学参数集（gemaps）用于语音研究和情感计算，”
    IEEE情感计算学报，卷7，第2期，页190–202，2015年。'
- en: '[84] F. Eyben, M. Wöllmer, and B. Schuller, “Opensmile: The munich versatile
    and fast open-source audio feature extractor,” in Proc. ACM Multimedia, (Firenze,
    Italy), pp. 1459–1462, 2010.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] F. Eyben, M. Wöllmer, 和 B. Schuller，“Opensmile：慕尼黑通用快速开源音频特征提取器，” 在ACM多媒体会议论文集中，（意大利佛罗伦萨），页1459–1462，2010年。'
- en: '[85] S. A. Singh and S. Majumder, “Classification of unsegmented heart sound
    recording using KNN classifier,” Journal of Mechanics in Medicine and Biology,
    vol. 19, no. 04, p. 1950025, 2019.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] S. A. Singh 和 S. Majumder，“使用KNN分类器对未分段心音记录进行分类，” 机械医学与生物学期刊，卷19，第04期，页1950025，2019年。'
- en: '[86] E. Soares, P. Angelov, and X. Gu, “Autonomous learning multiple-model
    zero-order classifier for heart sound classification,” Applied Soft Computing,
    vol. 94, p. 106449, 2020.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] E. Soares, P. Angelov, 和 X. Gu，“用于心音分类的自主学习多模型零阶分类器，” 应用软计算，卷94，页106449，2020年。'
- en: '[87] S. Khaled, M. Fakhry, H. Esmail, A. Ezzat, and E. Hamad, “Analysis of
    training optimisation algorithms in the NARX neural network for classification
    of heart sound signals,” International Journal of Scientific & Engineering Research,
    vol. 13, no. 2, pp. 382–390, 2022.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] S. Khaled, M. Fakhry, H. Esmail, A. Ezzat, 和 E. Hamad， “NARX神经网络中训练优化算法的分析用于心音信号分类，”
    国际科学与工程研究期刊，卷13，第2期，页382–390，2022年。'
- en: '[88] V. Arora, R. Leekha, R. Singh, and I. Chana, “Heart sound classification
    using machine learning and phonocardiogram,” Modern Physics Letters B, vol. 33,
    no. 26, p. 1950321, 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] V. Arora, R. Leekha, R. Singh, 和 I. Chana，“利用机器学习和心音图进行心音分类，” 现代物理学快报B，卷33，第26期，页1950321，2019年。'
- en: '[89] M. Sotaquirá, D. Alvear, and M. Mondragón, “Phonocardiogram classification
    using deep neural networks and weighted probability comparisons,” Journal of medical
    engineering & technology, vol. 42, no. 7, pp. 510–517, 2018.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] M. Sotaquirá, D. Alvear, 和 M. Mondragón，“利用深度神经网络和加权概率比较进行心音图分类，” 医学工程与技术期刊，卷42，第7期，页510–517，2018年。'
- en: '[90] A. M. Alqudah, “Towards classifying non-segmented heart sound records
    using instantaneous frequency based features,” Journal of medical engineering
    & technology, vol. 43, no. 7, pp. 418–430, 2019.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. M. Alqudah，“基于瞬时频率特征对非分段心音记录进行分类，” 医学工程与技术期刊，卷43，第7期，页418–430，2019年。'
- en: '[91] Y. Tan, Z. Wang, K. Qian, B. Hu, S. Zhao, B. W. Schuller, and Y. Yamamoto,
    “Heart sound classification based on fractional fourier transformation entropy,”
    in Proc. LifeTech, (Osaka, Japan), pp. 588–589, 2022.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Y. Tan, Z. Wang, K. Qian, B. Hu, S. Zhao, B. W. Schuller, 和 Y. Yamamoto，“基于分数傅里叶变换熵的心音分类，”
    在LifeTech会议论文集中，（日本大阪），页588–589，2022年。'
- en: '[92] F. Dong, K. Qian, Z. Ren, A. Baird, X. Li, Z. Dai, B. Dong, F. Metze,
    Y. Yamamoto, and B. Schuller, “Machine listening for heart status monitoring:
    Introducing and benchmarking HSS – the heart sounds shenzhen corpus,” IEEE Journal
    of Biomedical and Health Informatics, vol. 24, pp. 2082–2092, Nov. 2019.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] F. Dong, K. Qian, Z. Ren, A. Baird, X. Li, Z. Dai, B. Dong, F. Metze,
    Y. Yamamoto, 和 B. Schuller，“心脏状态监测的机器听觉：介绍和基准测试HSS——心音深圳语料库，” IEEE生物医学与健康信息学学报，卷24，页2082–2092，2019年11月。'
- en: '[93] J. Chen, X. Dang, and M. Li, “Heart sound classification method based
    on ensemble learning,” in Proc. ICSP, (Beijing, China), pp. 8–13, 2022.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] J. Chen, X. Dang, 和 M. Li，“基于集成学习的心音分类方法，” 在ICSP会议论文集中，（中国北京），页8–13，2022年。'
- en: '[94] M. Rahmandani, H. A. Nugroho, and N. A. Setiawan, “Cardiac sound classification
    using Mel-frequency cepstral coefficients (MFCC) and artificial neural network
    (ANN),” in Proc. ICITISEE, (Yogyakarta, Indonesia), pp. 22–26, 2018.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] M. Rahmandani, H. A. Nugroho, 和 N. A. Setiawan，“使用Mel频率倒谱系数（MFCC）和人工神经网络（ANN）进行心音分类，”发表于
    ICITISEE 会议论文集，（印度尼西亚，日惹），第22–26页，2018年。'
- en: '[95] M. Adiban, B. BabaAli, and S. Shehnepoor, “Statistical feature embedding
    for heart sound classification,” Journal of Electrical Engineering, vol. 70, no. 4,
    pp. 259–272, 2019.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] M. Adiban, B. BabaAli, 和 S. Shehnepoor，“心音分类的统计特征嵌入，”《电气工程学报》，第70卷，第4期，第259–272页，2019年。'
- en: '[96] J. Li, L. Ke, Q. Du, X. Ding, X. Chen, and D. Wang, “Heart sound signal
    classification algorithm: A combination of wavelet scattering transform and twin
    support vector machine,” IEEE Access, vol. 7, pp. 179339–179348, 2019.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] J. Li, L. Ke, Q. Du, X. Ding, X. Chen, 和 D. Wang，“心音信号分类算法：小波散射变换与双支持向量机的结合，”《IEEE
    Access》，第7卷，第179339–179348页，2019年。'
- en: '[97] N. Mei, H. Wang, Y. Zhang, F. Liu, X. Jiang, and S. Wei, “Classification
    of heart sounds based on quality assessment and wavelet scattering transform,”
    Computers in Biology and Medicine, vol. 137, p. 104814, 2021.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] N. Mei, H. Wang, Y. Zhang, F. Liu, X. Jiang, 和 S. Wei，“基于质量评估和小波散射变换的心音分类，”《生物医学计算机》，第137卷，第104814页，2021年。'
- en: '[98] N. K. Sawant, S. Patidar, N. Nesaragi, and U. R. Acharya, “Automated detection
    of abnormal heart sound signals using Fano-factor constrained tunable quality
    wavelet transform,” Biocybernetics and Biomedical Engineering, vol. 41, no. 1,
    pp. 111–126, 2021.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] N. K. Sawant, S. Patidar, N. Nesaragi, 和 U. R. Acharya，“使用Fano因子约束可调质量小波变换进行异常心音信号的自动检测，”《生物网络与生物医学工程》，第41卷，第1期，第111–126页，2021年。'
- en: '[99] T. Tuncer, S. Dogan, R.-S. Tan, and U. R. Acharya, “Application of Petersen
    graph pattern technique for automated detection of heart valve diseases with PCG
    signals,” Information Sciences, vol. 565, pp. 91–104, 2021.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] T. Tuncer, S. Dogan, R.-S. Tan, 和 U. R. Acharya，“使用Petersen图模式技术进行基于PCG信号的心脏瓣膜疾病自动检测，”《信息科学》，第565卷，第91–104页，2021年。'
- en: '[100] S. Amiriparian, M. Schmitt, N. Cummins, K. Qian, F. Dong, and B. Schuller,
    “Deep unsupervised representation learning for abnormal heart sound classification,”
    in Pro. EMBC, (Honolulu, Hawaii), pp. 4776–4779, 2018.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] S. Amiriparian, M. Schmitt, N. Cummins, K. Qian, F. Dong, 和 B. Schuller，“用于异常心音分类的深度无监督表示学习，”发表于
    EMBC 会议论文集，（美国，夏威夷），第4776–4779页，2018年。'
- en: '[101] M. E. Karar, S. H. El-Khafif, and M. A. El-Brawany, “Automated diagnosis
    of heart sounds using rule-based classification tree,” Journal of medical systems,
    vol. 41, no. 4, pp. 1–7, 2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] M. E. Karar, S. H. El-Khafif, 和 M. A. El-Brawany，“使用基于规则的分类树进行自动心音诊断，”《医学系统杂志》，第41卷，第4期，第1–7页，2017年。'
- en: '[102] F. Plesinger, I. Viscor, J. Halamek, J. Jurco, and P. Jurak, “Heart sounds
    analysis using probability assessment,” Physiological measurement, vol. 38, no. 8,
    p. 1685, 2017.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] F. Plesinger, I. Viscor, J. Halamek, J. Jurco, 和 P. Jurak，“使用概率评估的心音分析，”《生理测量》，第38卷，第8期，第1685页，2017年。'
- en: '[103] R. F. Ibarra-Hernández, N. Bertin, M. A. Alonso-Arévalo, and H. A. Guillén-Ramírez,
    “A benchmark of heart sound classification systems based on sparse decompositions,”
    in Proc. SIPAIM, vol. 10975, (Mazatlan, Mexico), pp. 26–38, 2018.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] R. F. Ibarra-Hernández, N. Bertin, M. A. Alonso-Arévalo, 和 H. A. Guillén-Ramírez，“基于稀疏分解的心音分类系统基准测试，”发表于
    SIPAIM 会议论文集，第10975卷，（墨西哥，Mazatlan），第26–38页，2018年。'
- en: '[104] A. Sofwan, I. Santoso, H. Pradipta, M. Arfan, et al., “Normal and murmur
    heart sound classification using linear predictive coding and k-nearest neighbor
    methods,” in Proc. ICICoS, (Semarang, Japan), pp. 1–5, 2019.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] A. Sofwan, I. Santoso, H. Pradipta, M. Arfan 等，“使用线性预测编码和k-最近邻方法进行正常和杂音心音分类，”发表于
    ICICoS 会议论文集，（印尼，Semarang），第1–5页，2019年。'
- en: '[105] Z. Ren, J. Han, N. Cummins, and B. Schuller, “Enhancing transferability
    of black-box adversarial attacks via lifelong learning for speech emotion recognition
    models,” in Proc. INTERSPEECH, (Shanghai, China), pp. 496–500, 2020.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] Z. Ren, J. Han, N. Cummins, 和 B. Schuller，“通过终身学习增强黑箱对抗攻击的可迁移性，用于语音情感识别模型，”发表于
    INTERSPEECH 会议论文集，（中国，上海），第496–500页，2020年。'
- en: '[106] Z. Ren, T. T. Nguyen, and W. Nejdl, “Prototype learning for interpretable
    respiratory sound analysis,” in Proc. ICASSP, (Singapore), pp. 9087–9091, 2022.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] Z. Ren, T. T. Nguyen, 和 W. Nejdl，“用于可解释的呼吸音分析的原型学习，”发表于 ICASSP 会议论文集，（新加坡），第9087–9091页，2022年。'
- en: '[107] K. Qian, C. Janott, M. Schmitt, Z. Zhang, C. Heiser, W. Hemmert, Y. Yamamoto,
    and B. W. Schuller, “Can machine learning assist locating the excitation of snore
    sound? A review,” IEEE Journal of Biomedical and Health Informatics, vol. 25,
    no. 4, pp. 1233–1246, 2020.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] K. Qian, C. Janott, M. Schmitt, Z. Zhang, C. Heiser, W. Hemmert, Y. Yamamoto,
    和 B. W. Schuller，“机器学习能否帮助定位打鼾声音的激发点？一项综述”，IEEE生物医学与健康信息学杂志，卷25，第4期，第1233–1246页，2020年。'
- en: '[108] F. Renna, J. Oliveira, and M. T. Coimbra, “Deep convolutional neural
    networks for heart sound segmentation,” IEEE Journal of Biomedical and Health
    Informatics, vol. 23, no. 6, pp. 2435–2445, 2019.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] F. Renna, J. Oliveira, 和 M. T. Coimbra，“用于心音分割的深度卷积神经网络”，IEEE生物医学与健康信息学杂志，卷23，第6期，第2435–2445页，2019年。'
- en: '[109] E. Messner, M. Zöhrer, and F. Pernkopf, “Heart sound segmentation—an
    event detection approach using deep recurrent neural networks,” IEEE Transactions
    on Biomedical Engineering, vol. 65, no. 9, pp. 1964–1974, 2018.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] E. Messner, M. Zöhrer, 和 F. Pernkopf，“心音分割—基于深度递归神经网络的事件检测方法”，IEEE生物医学工程学报，卷65，第9期，第1964–1974页，2018年。'
- en: '[110] Y. Chen, J. Lv, Y. Sun, and B. Jia, “Heart sound segmentation via duration
    long–short term memory neural network,” Applied Soft Computing, vol. 95, p. 106540,
    2020.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] Y. Chen, J. Lv, Y. Sun, 和 B. Jia，“通过长短期记忆神经网络进行心音分割”，应用软计算，卷95，第106540页，2020年。'
- en: '[111] T. Fan, J. Zhu, Y. Cheng, Q. Li, D. Xue, and R. Munnoch, “A new Direct
    heart sound segmentation approach using bi-directional GRU,” in Proc. ICAC, (Newcastle,
    UK), pp. 1–5, 2018.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] T. Fan, J. Zhu, Y. Cheng, Q. Li, D. Xue, 和 R. Munnoch，“一种新的双向GRU心音分割方法”，发表于ICAC会议，（英国纽卡斯尔），第1–5页，2018年。'
- en: '[112] T. Fernando, H. Ghaemmaghami, S. Denman, S. Sridharan, N. Hussain, and
    C. Fookes, “Heart sound segmentation using bidirectional LSTMs with attention,”
    IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 6, pp. 1601–1609,
    2020.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] T. Fernando, H. Ghaemmaghami, S. Denman, S. Sridharan, N. Hussain, 和
    C. Fookes，“使用双向LSTM与注意力机制的心音分割”，IEEE生物医学与健康信息学杂志，卷24，第6期，第1601–1609页，2020年。'
- en: '[113] Y. Chen, Y. Sun, J. Lv, B. Jia, and X. Huang, “End-to-end heart sound
    segmentation using deep convolutional recurrent network,” Complex Intell. Syst.,
    vol. 7, p. 2103–2117, 2021.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Y. Chen, Y. Sun, J. Lv, B. Jia, 和 X. Huang，“使用深度卷积递归网络的端到端心音分割”，复杂智能系统，卷7，第2103–2117页，2021年。'
- en: '[114] C. Xu, J. Zhou, L. Li, J. Wang, D. Ying, and Q. Li, “Heart sound segmentation
    based on SMGU-RNN,” in Proc. BIBE, (Hangzhou, China), pp. 126–132, 2019.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] C. Xu, J. Zhou, L. Li, J. Wang, D. Ying, 和 Q. Li，“基于SMGU-RNN的心音分割”，发表于BIBE会议，（中国杭州），第126–132页，2019年。'
- en: '[115] S. Takezaki and K. Kishida, “Construction of cnns for abnormal heart
    sound detection using data augmentation,” in Proc. IMECS, (Hong Kong), pp. 1–6,
    2021.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] S. Takezaki 和 K. Kishida，“使用数据增强构建用于异常心音检测的CNNs”，发表于IMECS会议，（香港），第1–6页，2021年。'
- en: '[116] X. Cheng, J. Huang, Y. Li, and G. Gui, “Design and application of a laconic
    heart sound neural network,” IEEE Access, vol. 7, pp. 124417–124425, 2019.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] X. Cheng, J. Huang, Y. Li, 和 G. Gui，“简洁心音神经网络的设计与应用”，IEEE Access，卷7，第124417–124425页，2019年。'
- en: '[117] A. M. Alqudah, H. Alquran, and I. A. Qasmieh, “Classification of heart
    sound short records using bispectrum analysis approach images and deep learning,”
    Network Modeling Analysis in Health Informatics and Bioinformatics, vol. 9, no. 66,
    pp. 1–16, 2020.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] A. M. Alqudah, H. Alquran, 和 I. A. Qasmieh，“使用双谱分析方法图像和深度学习进行短时间心音记录分类”，健康信息学与生物信息学网络建模分析，卷9，第66期，第1–16页，2020年。'
- en: '[118] M. S. Wibawa, I. M. D. Maysanjaya, N. K. D. P. Novianti, and P. N. Crisnapati,
    “Abnormal heart rhythm detection based on spectrogram of heart sound using convolutional
    neural network,” in Proc. CITSM, (Parapat Nort Sumatera, Indonesia), pp. 1–4,
    2018.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] M. S. Wibawa, I. M. D. Maysanjaya, N. K. D. P. Novianti, 和 P. N. Crisnapati，“基于心音光谱图的异常心律检测使用卷积神经网络”，发表于CITSM会议，（印度尼西亚北苏门答腊省Parapat），第1–4页，2018年。'
- en: '[119] K. Ranipa, W.-P. Zhu, and M. Swamy, “Multimodal cnn fusion architecture
    with multi-features for heart sound classification,” in Proc. ISCAS, (Suseong-gu,
    Daegu), pp. 1–5, 2021.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] K. Ranipa, W.-P. Zhu, 和 M. Swamy，“具有多特征的多模态CNN融合架构用于心音分类”，发表于ISCAS会议，（大邱寿城区），第1–5页，2021年。'
- en: '[120] A. Balamurugan, S. G. Teo, J. Yang, Z. Peng, Y. Xulei, and Z. Zeng, “ResHNet:
    spectrograms based efficient heart sounds classification using stacked residual
    networks,” in Proc. BHI, (Chicago, IL), pp. 1–4, 2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] A. Balamurugan, S. G. Teo, J. Yang, Z. Peng, Y. Xulei, 和 Z. Zeng，“ResHNet：基于光谱图的高效心音分类使用堆叠残差网络”，发表于BHI会议，（美国伊利诺伊州芝加哥），第1–4页，2019年。'
- en: '[121] M. Deng, T. Meng, J. Cao, S. Wang, J. Zhang, and H. Fan, “Heart sound
    classification based on improved MFCC features and convolutional recurrent neural
    networks,” Neural Networks, vol. 130, pp. 22–32, 2020.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] M. Deng, T. Meng, J. Cao, S. Wang, J. Zhang, 和 H. Fan，“基于改进的MFCC特征和卷积递归神经网络的心音分类”，《神经网络》，第130卷，第22–32页，2020年。'
- en: '[122] Z. Abduh, E. A. Nehary, M. A. Wahed, and Y. M. Kadah, “Classification
    of heart sounds using fractional fourier transform based mel-frequency spectral
    coefficients and stacked autoencoder deep neural network,” Journal of Medical
    Imaging and Health Informatics, vol. 9, no. 1, pp. 1–8, 2019.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] Z. Abduh, E. A. Nehary, M. A. Wahed, 和 Y. M. Kadah，“使用基于分数傅里叶变换的梅尔频率谱系数和堆叠自编码器深度神经网络进行心音分类”，《医学成像与健康信息学杂志》，第9卷，第1期，第1–8页，2019年。'
- en: '[123] M. Fakhry and A. F. Brery, “A comparison study on training optimization
    algorithms in the bilstm neural network for classification of pcg signals,” in
    Proc. IRASET, (Meknes, Morocco), pp. 1–6, 2022.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] M. Fakhry 和 A. F. Brery，“关于bilstm神经网络中训练优化算法的比较研究，用于pcg信号分类”，见于 Proc.
    IRASET，（梅克内斯，摩洛哥），第1–6页，2022年。'
- en: '[124] K. Qian, Z. Ren, F. Dong, W.-H. Lai, B. Schuller, and Y. Yoshiharu, “Deep
    wavelets for heart sound classification,” in Proc. ISPACS, (Taipei, Taiwan), 2019.
    2 pages.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] K. Qian, Z. Ren, F. Dong, W.-H. Lai, B. Schuller, 和 Y. Yoshiharu，“用于心音分类的深度小波变换”，见于
    Proc. ISPACS，（台北，台湾），2019年。2页。'
- en: '[125] W. Zhang, J. Han, and S. Deng, “Abnormal heart sound detection using
    temporal quasi-periodic features and long short-term memory without segmentation,”
    Biomedical Signal Processing and Control, vol. 53, p. 101560, 2019.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] W. Zhang, J. Han, 和 S. Deng，“利用时间准周期特征和长短期记忆网络进行异常心音检测，无需分割”，《生物医学信号处理与控制》，第53卷，第101560页，2019年。'
- en: '[126] F. Li, M. Liu, Y. Zhao, L. Kong, L. Dong, X. Liu, and M. Hui, “Feature
    extraction and classification of heart sound using 1d convolutional neural networks,”
    EURASIP Journal on Advances in Signal Processing, vol. 2019, no. 1, pp. 1–11,
    2019.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] F. Li, M. Liu, Y. Zhao, L. Kong, L. Dong, X. Liu, 和 M. Hui，“使用1d卷积神经网络进行心音特征提取和分类”，《EURASIP信号处理进展杂志》，第2019卷，第1期，第1–11页，2019年。'
- en: '[127] R. Avanzato and F. Beritelli, “Heart sound multiclass analysis based
    on raw data and convolutional neural network,” IEEE Sensors Letters, vol. 4, no. 12,
    pp. 1–4, 2020.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] R. Avanzato 和 F. Beritelli，“基于原始数据和卷积神经网络的心音多类分析”，《IEEE传感器快报》，第4卷，第12期，第1–4页，2020年。'
- en: '[128] S. L. Oh, V. Jahmunah, C. P. Ooi, R.-S. Tan, E. J. Ciaccio, T. Yamakawa,
    M. Tanabe, M. Kobayashi, and U. R. Acharya, “Classification of heart sound signals
    using a novel deep WaveNet model,” Computer Methods and Programs in Biomedicine,
    vol. 196, pp. 1–9, 2020.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] S. L. Oh, V. Jahmunah, C. P. Ooi, R.-S. Tan, E. J. Ciaccio, T. Yamakawa,
    M. Tanabe, M. Kobayashi, 和 U. R. Acharya，“使用新型深度WaveNet模型进行心音信号分类”，《计算机方法与生物医学程序》，第196卷，第1–9页，2020年。'
- en: '[129] S. Gao, Y. Zheng, and X. Guo, “Gated recurrent unit-based heart sound
    analysis for heart failure screening,” Biomedical engineering online, vol. 19,
    no. 1, pp. 1–17, 2020.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] S. Gao, Y. Zheng, 和 X. Guo，“基于门控递归单元的心音分析用于心力衰竭筛查”，《生物医学工程在线》，第19卷，第1期，第1–17页，2020年。'
- en: '[130] S. B. Shuvo, S. N. Ali, S. I. Swapnil, M. S. Al-Rakhami, and A. Gumaei,
    “CardioXNet: A novel lightweight deep learning framework for cardiovascular disease
    classification using heart sound recordings,” IEEE Access, vol. 9, pp. 36955–36967,
    2021.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] S. B. Shuvo, S. N. Ali, S. I. Swapnil, M. S. Al-Rakhami, 和 A. Gumaei，“CardioXNet：一种新型轻量级深度学习框架，用于使用心音录音进行心血管疾病分类”，《IEEE
    Access》，第9卷，第36955–36967页，2021年。'
- en: '[131] A. I. Humayun, S. Ghaffarzadegan, M. I. Ansari, Z. Feng, and T. Hasan,
    “Towards domain invariant heart sound abnormality detection using learnable filterbanks,”
    IEEE journal of biomedical and health informatics, vol. 24, no. 8, pp. 2189–2198,
    2020.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] A. I. Humayun, S. Ghaffarzadegan, M. I. Ansari, Z. Feng, 和 T. Hasan，“使用可学习滤波器组进行领域不变的心音异常检测”，《IEEE生物医学与健康信息学期刊》，第24卷，第8期，第2189–2198页，2020年。'
- en: '[132] A. I. Humayun, S. Ghaffarzadegan, Z. Feng, and T. Hasan, “Learning front-end
    filter-bank parameters using convolutional neural networks for abnormal heart
    sound detection,” in Proc. EMBC, (Honolulu, Hawaii), pp. 1408–1411, 2018.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] A. I. Humayun, S. Ghaffarzadegan, Z. Feng, 和 T. Hasan，“利用卷积神经网络学习前端滤波器组参数用于异常心音检测”，见于
    Proc. EMBC，（檀香山，夏威夷），第1408–1411页，2018年。'
- en: '[133] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:
    A large-scale hierarchical image database,” in Proc. CVPR, (Miami, FL), pp. 248–255,
    2009.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, 和 L. Fei-Fei，“ImageNet：一个大规模层次图像数据库”，见于
    Proc. CVPR，（迈阿密，FL），第248–255页，2009年。'
- en: '[134] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.
    Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset
    for audio events,” in Proc. ICASSP, (New Orleans), pp. 776–780, 2017.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.
    Moore, M. Plakal, 和 M. Ritter, “音频集：一个用于音频事件的本体和人工标注数据集”，在 Proc. ICASSP，（新奥尔良），第776–780页，2017年。'
- en: '[135] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” Communications of the ACM, vol. 60,
    no. 6, pp. 84–90, 2017.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. Krizhevsky, I. Sutskever, 和 G. E. Hinton, “使用深度卷积神经网络进行 ImageNet 分类”，《ACM
    通讯》，第60卷，第6期，第84–90页，2017年。'
- en: '[136] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” (San Diego, CA), pp. 1–14, 2015.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] K. Simonyan 和 A. Zisserman, “用于大规模图像识别的非常深的卷积网络”，（圣地亚哥，加州），第1–14页，2015年。'
- en: '[137] F. Demir, A. Şengür, V. Bajaj, and K. Polat, “Towards the classification
    of heart sounds based on convolutional deep neural network,” Health information
    science and systems, vol. 7, no. 1, pp. 1–9, 2019.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] F. Demir, A. Şengür, V. Bajaj, 和 K. Polat, “基于卷积深度神经网络的心音分类研究”，《健康信息科学与系统》，第7卷，第1期，第1–9页，2019年。'
- en: '[138] T. Koike, K. Qian, Q. Kong, M. D. Plumbley, B. W. Schuller, and Y. Yamamoto,
    “Audio for audio is better? An investigation on transfer learning models for heart
    sound classification,” in Proc. EMBC, (Virtual Event), pp. 74–77, 2020.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] T. Koike, K. Qian, Q. Kong, M. D. Plumbley, B. W. Schuller, 和 Y. Yamamoto,
    “音频对音频更好？关于用于心音分类的迁移学习模型的研究”，在 Proc. EMBC，（虚拟活动），第74–77页，2020年。'
- en: '[139] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “MobileNets: Efficient convolutional neural networks for mobile vision
    applications,” arXiv preprint arXiv:1704.04861, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M.
    Andreetto, 和 H. Adam, “MobileNets：用于移动视觉应用的高效卷积神经网络”，arXiv 预印本 arXiv:1704.04861，2017年。'
- en: '[140] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in Proc. CVPR, (Las Vegas, NV), pp. 770–778, 2016.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] K. He, X. Zhang, S. Ren, 和 J. Sun, “用于图像识别的深度残差学习”，在 Proc. CVPR，（拉斯维加斯，内华达州），第770–778页，2016年。'
- en: '[141] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in Proc. CVPR, (Honolulu, Hawaii),
    pp. 1492–1500, 2017.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] S. Xie, R. Girshick, P. Dollár, Z. Tu, 和 K. He, “深度神经网络的聚合残差变换”，在 Proc.
    CVPR，（檀香山，夏威夷），第1492–1500页，2017年。'
- en: '[142] P. Bentley, G. Nordehn, M. Coimbra, and S. Mannor, “The PASCAL Classifying
    Heart Sounds Challenge 2011 (CHSC2011) Results.” http://www.peterjbentley.com/heartchallenge/index.html.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] P. Bentley, G. Nordehn, M. Coimbra, 和 S. Mannor, “PASCAL 心音分类挑战 2011（CHSC2011）结果。”
    [http://www.peterjbentley.com/heartchallenge/index.html](http://www.peterjbentley.com/heartchallenge/index.html)'
- en: '[143] A. Goldberger, L. Amaral, L. Glass, J. Hausdorff, P. Ivanov, R. Mark,
    J. Mietus, G. Moody, C. Peng, and H. Stanley, “PhysioBank, PhysioToolkit, and
    PhysioNet: Components of a new research resource for complex physiologic signals,”
    2000.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] A. Goldberger, L. Amaral, L. Glass, J. Hausdorff, P. Ivanov, R. Mark,
    J. Mietus, G. Moody, C. Peng, 和 H. Stanley, “PhysioBank, PhysioToolkit, 和 PhysioNet：新的复杂生理信号研究资源的组成部分”，2000年。'
- en: '[144] G.-Y. Son and S. Kwon, “Classification of heart sound signal using multiple
    features,” Applied Sciences, vol. 8, no. 12, p. 2344, 2018.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] G.-Y. Son 和 S. Kwon, “使用多个特征的心音信号分类”，《应用科学》，第8卷，第12期，第2344页，2018年。'
- en: '[145] J. Oliveira, F. Renna, P. D. Costa, M. Nogueira, C. Oliveira, C. Ferreira,
    A. Jorge, S. Mattos, T. Hatem, T. Tavares, et al., “The CirCor DigiScope dataset:
    From murmur detection to murmur classification,” IEEE journal of biomedical and
    health informatics, vol. 26, no. 6, pp. 2524–2535, 2021.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] J. Oliveira, F. Renna, P. D. Costa, M. Nogueira, C. Oliveira, C. Ferreira,
    A. Jorge, S. Mattos, T. Hatem, T. Tavares 等, “CirCor DigiScope 数据集：从心杂音检测到心杂音分类”，《IEEE
    生物医学与健康信息学期刊》，第26卷，第6期，第2524–2535页，2021年。'
- en: '[146] M. A. Reyna et al., “Heart murmur detection from phonocardiogram recordings:
    The George B. Moody PhysioNet Challenge 2022,” medRxiv, 2022.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] M. A. Reyna 等, “从心音图录音中检测心脏杂音：乔治·B·穆迪 PhysioNet 挑战 2022”，medRxiv，2022年。'
- en: '[147] F. B. Azam, M. Ansari, I. Mclane, T. Hasan, et al., “Heart sound classification
    considering additive noise and convolutional distortion,” arXiv preprint arXiv:2106.01865,
    2021.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] F. B. Azam, M. Ansari, I. Mclane, T. Hasan 等, “考虑加性噪声和卷积失真的心音分类”，arXiv
    预印本 arXiv:2106.01865，2021年。'
- en: '[148] P. N. Waaler, H. Melbye, et al., “Algorithm for predicting valvular heart
    disease from heart sounds in an unselected cohort,” medRxiv, 2022.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] P. N. Waaler, H. Melbye 等, “从心音预测瓣膜性心脏病的算法，在一个未选定的队列中”，medRxiv，2022年。'
- en: '[149] T. Dissanayake, T. Fernando, S. Denman, S. Sridharan, H. Ghaemmaghami,
    and C. Fookes, “A robust interpretable deep learning classifier for heart anomaly
    detection without segmentation,” IEEE Journal of Biomedical and Health Informatics,
    vol. 25, no. 6, pp. 2162–2171, 2020.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] T. Dissanayake, T. Fernando, S. Denman, S. Sridharan, H. Ghaemmaghami,
    和 C. Fookes，“一种鲁棒的可解释深度学习分类器用于无分割的心脏异常检测，”IEEE 生物医学与健康信息学期刊，第 25 卷，第 6 期，第 2162–2171
    页，2020 年。'
- en: '[150] S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model
    predictions,” in Proc. NIPS, (Long Beach, CA), p. 4768–4777, 2017.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] S. M. Lundberg 和 S.-I. Lee，“统一解释模型预测的方法，”在 Proc. NIPS，（美国，加州长滩），第 4768–4777
    页，2017 年。'
- en: '[151] W.-S. Jhong, S.-I. Chu, Y.-J. Huang, T.-Y. Hsu, W.-C. Lin, P. Huang,
    and J.-J. Wang, “Deep learning hardware/software co-design for heart sound classification,”
    in Proc. ISOCC, (Yeosu, Korea), pp. 27–28, 2020.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] W.-S. Jhong, S.-I. Chu, Y.-J. Huang, T.-Y. Hsu, W.-C. Lin, P. Huang,
    和 J.-J. Wang，“心音分类的深度学习硬件/软件协同设计，”在 Proc. ISOCC，（韩国，丽水），第 27–28 页，2020 年。'
- en: '[152] W. R. Thompson, A. J. Reinisch, M. J. Unterberger, and A. J. Schriefl,
    “Artificial intelligence-assisted auscultation of heart murmurs: validation by
    virtual clinical trial,” Pediatric cardiology, vol. 40, no. 3, pp. 623–629, 2019.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] W. R. Thompson, A. J. Reinisch, M. J. Unterberger, 和 A. J. Schriefl，“人工智能辅助的心脏杂音听诊：通过虚拟临床试验验证，”《儿科心脏病学》，第
    40 卷，第 3 期，第 623–629 页，2019 年。'
- en: '[153] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., “On the opportunities
    and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill 等，“关于基础模型的机会与风险，”arXiv 预印本
    arXiv:2108.07258，2021 年。'
- en: '[154] W. Callaghan, J. Goh, M. Mohareb, A. Lim, and E. Law, “Mechanicalheart:
    A human-machine framework for the classification of phonocardiograms,” Proceedings
    of the ACM on Human-Computer Interaction, vol. 2, no. CSCW, pp. 1–17, 2018.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] W. Callaghan, J. Goh, M. Mohareb, A. Lim, 和 E. Law，“Mechanicalheart:
    一个人机框架用于心音图的分类，”《ACM 人机交互会议论文集》，第 2 卷，第 CSCW 期，第 1–17 页，2018 年。'
- en: '[155] Z. Ren, K. Qian, F. Dong, Z. Dai, W. Nejdl, Y. Yamamoto, and B. W. Schuller,
    “Deep attention-based neural networks for explainable heart sound classification,”
    Machine Learning with Applications, p. 100322, 2022.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] Z. Ren, K. Qian, F. Dong, Z. Dai, W. Nejdl, Y. Yamamoto, 和 B. W. Schuller，“基于深度注意力的神经网络用于可解释的心音分类，”《应用机器学习》，第
    100322 页，2022 年。'
- en: '[156] Y. Yang, I. G. Morillo, and T. M. Hospedales, “Deep neural decision trees,”
    in Proc. ICML WHI, (Stockholm, Sweden), pp. 34–40, 2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] Y. Yang, I. G. Morillo, 和 T. M. Hospedales，“深度神经决策树，”在 Proc. ICML WHI，（瑞典，斯德哥尔摩），第
    34–40 页，2018 年。'
- en: '[157] Y. Chang, Z. Ren, T. T. Nguyen, W. Nejdl, and B. Schuller, “Example-based
    explanations with adversarial attacks for respiratory sound analysis,” in Proc. INTERPSEECH,
    (Incheon, Korea), pp. 4003–4007, 2022.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] Y. Chang, Z. Ren, T. T. Nguyen, W. Nejdl, 和 B. Schuller，“基于示例的解释与对呼吸音分析的对抗攻击，”在
    Proc. INTERPSEECH，（韩国，仁川），第 4003–4007 页，2022 年。'
- en: '[158] B. W. Schuller, T. Virtanen, M. Riveiro, G. Rizos, J. Han, A. Mesaros,
    and K. Drossos, “Towards sonification in multimodal and user-friendly explainable
    artificial intelligence,” in Proc. ICMI, pp. 788–792, 2021.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] B. W. Schuller, T. Virtanen, M. Riveiro, G. Rizos, J. Han, A. Mesaros,
    和 K. Drossos，“朝向多模态和用户友好的可解释人工智能的声化，”在 Proc. ICMI，第 788–792 页，2021 年。'
- en: '[159] M. Pelillo and T. Scantamburlo, Machines We Trust: Perspectives on Dependable
    AI. MIT Press, 2021.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] M. Pelillo 和 T. Scantamburlo，《我们信任的机器：关于可靠人工智能的观点》。MIT 出版社，2021 年。'
- en: '[160] V. Gupta, C. Jung, S. Neel, A. Roth, S. Sharifi-Malvajerdi, and C. Waites,
    “Adaptive machine unlearning,” in Advances in Neural Information Processing Systems,
    vol. 34, pp. 16319–16330, 2021.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] V. Gupta, C. Jung, S. Neel, A. Roth, S. Sharifi-Malvajerdi, 和 C. Waites，“自适应机器遗忘，”在
    Advances in Neural Information Processing Systems，第 34 卷，第 16319–16330 页，2021
    年。'
- en: '[161] W. Qiu et al., “A federated learning paradigm for heart sound classification,”
    in Proc. EMBC, (Glasgow, UK), pp. 1045–1048, 2022.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] W. Qiu 等，“一种用于心音分类的联邦学习范式，”在 Proc. EMBC，（英国，格拉斯哥），第 1045–1048 页，2022
    年。'
- en: '[162] C. T. Williams, “A lecture on laënnec and the evolution of the stethoscope,”
    British medical journal, vol. 2, no. 2427, pp. 6–8, 1907.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] C. T. Williams，“关于拉恩内克和听诊器发展的讲座，”《英国医学杂志》，第 2 卷，第 2427 期，第 6–8 页，1907
    年。'
- en: '[163] B. Silverman and M. Balk, “Digital stethoscope—improved auscultation
    at the bedside,” The American journal of cardiology, vol. 123, no. 6, pp. 984–985,
    2019.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] B. Silverman 和 M. Balk，“数字听诊器——床边听诊的改进，”《美国心脏病学杂志》，第 123 卷，第 6 期，第 984–985
    页，2019 年。'
- en: '[164] M. E. Tavel, “Cardiac auscultation: a glorious past—and it does have
    a future!,” Circulation, vol. 113, no. 9, pp. 1255–1259, 2006.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] M. E. Tavel, “心脏听诊：辉煌的过去——它确实有未来！，” 《循环》，第113卷，第9期，第1255–1259页，2006年。'
- en: '[165] M. Abella, J. Formolo, and D. G. Penney, “Comparison of the acoustic
    properties of six popular stethoscopes,” The Journal of the Acoustical Society
    of America, vol. 91, no. 4, pp. 2224–2228, 1992.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] M. Abella, J. Formolo, 和 D. G. Penney, “六种常见听诊器的声学特性比较，” 《美国声学学会杂志》，第91卷，第4期，第2224–2228页，1992年。'
- en: '[166] N. K. Bakshi and A. K. Acharya, “Wireless electronic stethoscope,” INTERNATIONAL
    JOURNAL OF ENGINEERING RESEARCH & TECHNOLOGY, vol. 03, pp. 459–462, 2014.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] N. K. Bakshi 和 A. K. Acharya, “无线电子听诊器，” 《国际工程研究与技术期刊》，第03卷，第459–462页，2014年。'
- en: '[167] J. E. Schenthal, J. W. Sweeney, and J. Nettleton, Wilson, “Clinical application
    of large-scale electronic data processing apparatus: I. new concepts in clinical
    use of the electronic digital computer,” Journal of the American Medical Association,
    vol. 173, pp. 6–11, 05 1960.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] J. E. Schenthal, J. W. Sweeney, 和 J. Nettleton, Wilson, “大规模电子数据处理设备的临床应用：I.
    电子数字计算机在临床使用中的新概念，” 《美国医学会杂志》，第173卷，第6–11页，1960年5月。'
- en: '[168] H. R. Warner, A. F. Toronto, L. G. Veasey, and R. Stephenson, “A mathematical
    approach to medical diagnosis: Application to congenital heart disease,” JAMA,
    vol. 177, pp. 177–183, 07 1961.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] H. R. Warner, A. F. Toronto, L. G. Veasey, 和 R. Stephenson, “医学诊断的数学方法：应用于先天性心脏病，”
    《JAMA》，第177卷，第177–183页，1961年7月。'
- en: '[169] L. Taback, E. Marden, H. L. Mason, and H. V. Pipberger, “Digital recording
    of electrocardiographic data for analysis by a digital computer,” IRE Transactions
    on Medical Electronics, vol. ME-6, no. 3, pp. 167–171, 1959.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] L. Taback, E. Marden, H. L. Mason, 和 H. V. Pipberger, “心电图数据的数字录制，以供数字计算机分析，”
    《IRE医学电子学交易》，第ME-6卷，第3期，第167–171页，1959年。'
- en: '[170] C. A. CACERES, “Electrocardiographic analysis by a computer system,”
    Archives of Internal Medicine, vol. 111, pp. 196–202, 02 1963.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] C. A. CACERES, “计算机系统下的心电图分析，” 《内科学档案》，第111卷，第196–202页，1963年2月。'
- en: '[171] H. B. Sprague, “History and present status of phonocardiography,” IRE
    Transactions on Medical Electronics, pp. 2–3, 1957.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] H. B. Sprague, “心音图的历史与现状，” 《IRE医学电子学交易》，第2–3页，1957年。'
- en: '[172] W. Evans, “The use of the phonocardiograph in clinical cardiology,” British
    heart journal, vol. 10, no. 2, pp. 92–98, 1948.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] W. Evans, “心脏听音器在临床心脏病学中的使用，” 《英国心脏杂志》，第10卷，第2期，第92–98页，1948年。'
- en: '[173] A. Quiceno-Manrique, J. Godino-Llorente, M. Blanco-Velasco, and G. Castellanos-Dominguez,
    “Selection of dynamic features based on time–frequency representations for heart
    murmur detection from phonocardiographic signals,” Annals of biomedical engineering,
    vol. 38, no. 1, pp. 118–137, 2010.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] A. Quiceno-Manrique, J. Godino-Llorente, M. Blanco-Velasco, 和 G. Castellanos-Dominguez,
    “基于时频表示的动态特征选择，用于从心音图信号中检测心脏杂音，” 《生物医学工程年鉴》，第38卷，第1期，第118–137页，2010年。'
- en: '[174] L. J. Nowak and K. M. Nowak, “Sound differences between electronic and
    acoustic stethoscopes,” Biomedical engineering online, vol. 17, no. 104, pp. 1–11,
    2018.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] L. J. Nowak 和 K. M. Nowak, “电子听诊器与传统听诊器的声音差异，” 《生物医学工程在线》，第17卷，第104期，第1–11页，2018年。'
- en: '[175] J. Y. Shin, S. L’Yi, D. H. Jo, J. H. Bae, and T. S. Lee, “Development
    of smartphone-based stethoscope system,” in Proc. ICCAS, (Busan, South Korea),
    pp. 1288–1291, 2013.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] J. Y. Shin, S. L’Yi, D. H. Jo, J. H. Bae, 和 T. S. Lee, “基于智能手机的听诊器系统开发，”
    见于《ICCAS会议录》，（韩国釜山），第1288–1291页，2013年。'
- en: '[176] S. Hadiyoso, D. R. Mardiyah, D. N. Ramadan, and A. Ibrahim, “Implementation
    of electronic stethoscope for online remote monitoring with mobile application,”
    Bulletin of Electrical Engineering and Informatics, vol. 9, no. 4, pp. 1595–1603,
    2020.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] S. Hadiyoso, D. R. Mardiyah, D. N. Ramadan, 和 A. Ibrahim, “电子听诊器的在线远程监测实施，配合移动应用，”
    《电气工程与信息学公告》，第9卷，第4期，第1595–1603页，2020年。'
- en: '[177] C. Yang, W. Zhang, Z. Pang, J. Zhang, D. Zou, X. Zhang, S. Guo, J. Wan,
    K. Wang, and W. Pang, “A aow-Cost, ear-contactless electronic stethoscope powered
    by Raspberry Pi for auscultation of patients with COVID-19: Prototype development
    and feasibility study,” JMIR Medical Informatics, vol. 9, no. 1, p. e22753, 2021.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] C. Yang, W. Zhang, Z. Pang, J. Zhang, D. Zou, X. Zhang, S. Guo, J. Wan,
    K. Wang, 和 W. Pang, “一种低成本的耳接触式电子听诊器，由 Raspberry Pi 驱动，用于 COVID-19 患者的听诊：原型开发与可行性研究，”
    《JMIR医学信息学》，第9卷，第1期，第e22753页，2021年。'
- en: \appendices
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: \附录
- en: 8 History
  id: totrans-374
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 历史
- en: 'Before the 19th century, immediate auscultation conducted by a physician putting
    his/her ear directly to the patient’s chest was disliked because of its implications
    from a social perspective and in technique [[2](#bib.bib2)]. Immediate auscultation
    was even partially unacceptable due to the patient’s age and gender [[2](#bib.bib2)].
    Therefore, physical examination was mostly limited to inspection and palpation,
    which were not available in some cases, e. g., the great degree of body fat [[2](#bib.bib2)].
    Until 1816, the invention of the earliest stethoscope (Greek: stethos = chest,
    skopein = to view or to see) by a clinician, René Théoophile Hyacinthe Laennec,
    started a new era of mediate auscultation [[2](#bib.bib2)]. After Laennec’s cylinder-like
    stethoscope, cardiac auscultation was widely applied in physical examination and
    the structure of stethoscope was also improved. Charles J. B. Williams designed
    a flexible monaural stethoscope with a trumpet-shaped head [[2](#bib.bib2)]. Arthur
    Leared claimed the invention of a binaural stethoscope for more flexibility in
    1851 [[162](#bib.bib162)], and George Cammann further invented a binaural stethoscope
    for commercial production in 1852 [[163](#bib.bib163)]. Afterwards, the stethoscope
    was successfully and widely used as a critical diagnostic tool. However, there
    are several difficulties to employ such traditional acoustic stethoscopes. Firstly,
    acoustic stethoscopes cannot record, play back, and process the heart sounds,
    thus limiting the teaching of auscultation [[164](#bib.bib164)]. Secondly, acoustic
    stethoscopes demand physicians’ substantial clinical experience, but auscultation
    is a difficult skill that take years to acquire and refine [[27](#bib.bib27),
    [26](#bib.bib26)]. Thirdly, the performance of the human ear is limited to its
    physical limitations [[27](#bib.bib27)]. Although the apparent sound amplification
    has been seen by many acoustic stethoscopes, the increasing insensitivity of the
    human ear at frequencies below 100 Hz is still a problem for auscultation [[165](#bib.bib165)].
    Finally, the sound level of acoustic stethoscopes is very low, so they are not
    very suitable in noisy environment [[166](#bib.bib166)]. To this end, developing
    equipment and technologies for computer-aided auscultation is necessary to overcome
    the limitations of acoustic stethoscopes.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在19世纪之前，医生将耳朵直接贴在病人胸部进行即刻听诊的做法因其社会和技术方面的含义而不被喜欢[[2](#bib.bib2)]。即刻听诊甚至因病人的年龄和性别而在一定程度上不可接受[[2](#bib.bib2)]。因此，体格检查通常仅限于视诊和触诊，而在某些情况下如体脂过多时，触诊也不适用[[2](#bib.bib2)]。直到1816年，临床医生雷内·西奥菲勒·海森·拉内克发明了最早的听诊器（希腊语：stethos
    = 胸部，skopein = 观察或查看），这开启了间接听诊的新纪元[[2](#bib.bib2)]。在拉内克的圆柱形听诊器之后，心脏听诊在体格检查中被广泛应用，听诊器的结构也得到了改进。查尔斯·J·B·威廉姆斯设计了一种带喇叭状头部的柔性单耳听诊器[[2](#bib.bib2)]。亚瑟·利尔德于1851年宣称发明了一种双耳听诊器以提供更多灵活性[[162](#bib.bib162)]，而乔治·卡门于1852年进一步发明了一种用于商业生产的双耳听诊器[[163](#bib.bib163)]。此后，听诊器作为一种关键的诊断工具被成功并广泛使用。然而，使用这种传统的声学听诊器存在一些困难。首先，声学听诊器无法记录、回放和处理心音，因此限制了听诊教学[[164](#bib.bib164)]。其次，声学听诊器需要医生具备大量的临床经验，但听诊是一项需要多年才能掌握和提高的困难技能[[27](#bib.bib27),
    [26](#bib.bib26)]。第三，人耳的性能受到其物理限制的制约[[27](#bib.bib27)]。尽管许多声学听诊器能明显放大声音，但人耳在100
    Hz以下频率的灵敏度逐渐下降仍然是听诊中的一个问题[[165](#bib.bib165)]。最后，声学听诊器的声音水平非常低，因此在嘈杂环境中不太适用[[166](#bib.bib166)]。为此，开发计算机辅助听诊的设备和技术是克服声学听诊器局限性的必要手段。
- en: As early as in the late 1950s and 1960s, the computational capability of digital
    electronic computers promoted their applications in heart disease diagnosis [[167](#bib.bib167),
    [23](#bib.bib23)]. One direction was to deal with clinical data, such as diagnosis
    of congenital heart disease based on the incidence of clinical signs, symptoms,
    and electrocardiographic findings [[168](#bib.bib168)]. Compared to processing
    human ‘predigested’ clinical data, analysing raw physiologic records (e. g., electrocardiogram)
    is a more automated direction to save the time and efforts of physicians [[23](#bib.bib23),
    [169](#bib.bib169), [170](#bib.bib170)]. Additionally, another automated way similar
    to physicians’ cardiac auscultation is to analyse magnetic tape recordings of
    heart sounds and murmurs [[23](#bib.bib23)]. Automatically analysing recorded
    heart sounds and murmurs is promising to solve the aforementioned problems of
    acoustic stethoscopes.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 早在1950年代末和1960年代，数字电子计算机的计算能力促进了其在心脏病诊断中的应用[[167](#bib.bib167), [23](#bib.bib23)]。一种方向是处理临床数据，如基于临床征象、症状和心电图发现的先天性心脏病诊断[[168](#bib.bib168)]。与处理‘消化’后的临床数据相比，分析原始生理记录（如心电图）是一种更自动化的方向，可以节省医生的时间和精力[[23](#bib.bib23),
    [169](#bib.bib169), [170](#bib.bib170)]。此外，另一种类似于医生心脏听诊的自动化方法是分析心音和杂音的磁带录音[[23](#bib.bib23)]。自动分析记录的心音和杂音有望解决声学听诊器的上述问题。
- en: 'Phonocardiograph equipments were developed in the 1930s and 1940s [[171](#bib.bib171)].
    The recorded PCG signals can be objectively analysed and explained. An early study [[171](#bib.bib171)]
    reported the phonocardiograph is useful in timing and explaining whether an individual’s
    heart sounds contain abnormal sounds: atrial sounds, opening snaps, and gallop
    rhythms. A phonocardiograph was also described to be valuable in studying murmurs,
    particularly in mitral disease and congenital heart disease [[172](#bib.bib172),
    [171](#bib.bib171)]. In the past decades, due to the development of digital technologies,
    computer-aided auscultation from PCG signals played an important role in pediatric
    cardiology, internal diseases, and evaluating congenital cardiac defects [[173](#bib.bib173)].
    Particularly, the electronic stethoscope, as a light phonocardiograph equipment,
    gains an edge over acoustic stethoscopes for automatically percepting, processing,
    and analysing heart sounds. Heart sounds are electronically amplified in electronic
    stethoscopes to overcome the low sound levels of acoustic stethoscopes [[174](#bib.bib174)].
    Generally, the recorded PCG signals with an electronic stethoscope are transferred
    to a computer for visualisation and further analysis [[26](#bib.bib26)]. More
    recently, portable electronic stethoscopes were developed to be either connected
    to other mobile devices, e. g., mobile phones, or to wirelessly transmit to a
    remote processing unit via a Bluetooth/wireless interface [[26](#bib.bib26)].
    Consequently, electronic stethoscopes are able to be applied to several applications,
    such as real-time/remote monitoring and diagnosis [[175](#bib.bib175), [176](#bib.bib176)].
    In 2021, an ear-contactless electronic stethoscope was designed for auscultation
    of patients with coronavirus disease 2019 [[177](#bib.bib177)].'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 心音图设备在1930年代和1940年代被开发出来[[171](#bib.bib171)]。记录的PCG信号可以被客观地分析和解释。早期的研究[[171](#bib.bib171)]报告称，心音图在判断和解释个体的心音是否包含异常声音，如心房声音、开瓣音和奔马律方面非常有用。心音图也被描述为在研究杂音方面有价值，特别是在二尖瓣病和先天性心脏病中[[172](#bib.bib172),
    [171](#bib.bib171)]。在过去几十年中，由于数字技术的发展，计算机辅助的PCG信号听诊在儿科心脏病学、内科疾病和评估先天性心脏缺陷中发挥了重要作用[[173](#bib.bib173)]。特别是，作为一种轻型心音图设备的电子听诊器在自动感知、处理和分析心音方面优于声学听诊器。电子听诊器中的心音会被电子放大，以克服声学听诊器的低声音水平[[174](#bib.bib174)]。一般来说，电子听诊器记录的PCG信号会被传输到计算机上进行可视化和进一步分析[[26](#bib.bib26)]。最近，便携式电子听诊器被开发为可以连接到其他移动设备，例如手机，或通过蓝牙/无线接口无线传输到远程处理单元[[26](#bib.bib26)]。因此，电子听诊器能够应用于多种场景，如实时/远程监测和诊断[[175](#bib.bib175),
    [176](#bib.bib176)]。在2021年，设计了一种耳部无接触的电子听诊器用于听诊2019冠状病毒病患者[[177](#bib.bib177)]。
