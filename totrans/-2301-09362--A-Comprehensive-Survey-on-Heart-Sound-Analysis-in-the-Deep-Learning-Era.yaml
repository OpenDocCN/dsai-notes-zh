- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:42:13'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2301.09362] A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2301.09362](https://ar5iv.labs.arxiv.org/html/2301.09362)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Zhao Ren    \IEEEmembershipMember, IEEE    Yi Chang    Thanh Tam Nguyen   
    Yang Tan    Kun Qian    \IEEEmembershipSenior Member, IEEE    and Björn W. Schuller
       \IEEEmembershipFellow, IEEE This research was funded in part by the Federal
    Ministry of Education and Research (BMBF), Germany under the project LeibnizKILabor
    with grant No. 01DD20003, the Ministry of Science and Technology of the People’s
    Republic of China with the STI2030-Major Projects (No. 2021ZD0201900), and the
    National Natural Science Foundation of China (No. 62272044), and the Teli Young
    Fellow Program from the Beijing Institute of Technology, China. (*Corresponding
    authors*: Zhao Ren and Kun Qian.)Z. Ren is with the L3S Research Center, Leibniz
    University Hannover, Germany (zren@l3s.de).Y. Chang and B. W. Schuller are with
    the GLAM – the Group on Language, Audio, & Music, Imperial College London, United
    Kingdom (y.chang20@imperial.ac.uk, schuller@ieee.org).T. T. Nguyen is with the
    Griffith University, Australia (t.nguyen19@griffith.edu.au).Y. Tan and K. Qian
    are with the School of Medical Technology, Beijing Institute of Technology, China
    (qian@bit.edu.cn).B. W. Schuller is also with the Chair of Embedded Intelligence
    for Health Care and Wellbeing, University of Augsburg, Germany.This work has been
    submitted to the IEEE for possible publication. Copyright may be transferred without
    notice, after which this version may no longer be accessible.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Heart sound auscultation has been demonstrated to be beneficial in clinical
    usage for early screening of cardiovascular diseases. Due to the high requirement
    of well-trained professionals for auscultation, automatic auscultation benefiting
    from signal processing and machine learning can help auxiliary diagnosis and reduce
    the burdens of training professional clinicians. Nevertheless, classic machine
    learning is limited to performance improvement in the era of big data. Deep learning
    has achieved better performance than classic machine learning in many research
    fields, as it employs more complex model architectures with stronger capability
    of extracting effective representations. Deep learning has been successfully applied
    to heart sound analysis in the past years. As most review works about heart sound
    analysis were given before 2017, the present survey is the first to work on a
    comprehensive overview to summarise papers on heart sound analysis with deep learning
    in the past six years 2017–2022\. We introduce both classic machine learning and
    deep learning for comparison, and further offer insights about the advances and
    future research directions in deep learning for heart sound analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '{IEEEkeywords}'
  prefs: []
  type: TYPE_NORMAL
- en: Automated auscultation, computer audition, deep learning, heart sounds, digital
    health
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \IEEEPARstart
  prefs: []
  type: TYPE_NORMAL
- en: Cardiac auscultation, i. e., listening and interpreting the heart sound, is
    an indispensable and critical part for the clinical examination of the patient [[1](#bib.bib1)].
    As a low-cost and non-invasive examination, cardiac auscultation is invaluable
    for the presence of a heart disease and providing an estimate of its severity,
    evolution, and prognosis [[2](#bib.bib2)]. Accurate cardiac auscultation may determine
    whether more expensive examination should be planed [[2](#bib.bib2)]. Nevertheless,
    due to difficulties in diagnosing diastolic murmurs, the overall sensitivity of
    cardiac auscultation is poor (i. e., ranging from 0.21 to 1.00) [[1](#bib.bib1)].
    Moreover, cardiac auscultation depends on the physician’s skills which have been
    shown decreased over time [[3](#bib.bib3)]. Poor cardiac auscultation skills may
    miss significant pathology, causing worsening condition, and may over diagnose
    pathology, leading to inappropriate referral for expensive echocardiography [[1](#bib.bib1)].
  prefs: []
  type: TYPE_NORMAL
- en: To solve the above problem of cardiac auscultation, classic machine learning
    (ML) has been widely used for automated heart sound analysis, including denoising,
    segmentation, and classification. For instance, noisy audio clips were detected
    by support vector machines (SVMs) [[4](#bib.bib4)], hidden markov models (HMMs)
    were used for heart sound segmentation [[5](#bib.bib5)], and classifiers like
    SVMs and decision trees were applied to heart sound classification [[6](#bib.bib6),
    [7](#bib.bib7)]. Classic machine learning often takes acoustic features as the
    input, while selecting useful features needs lots of human efforts. Additionally,
    classic machine learning usually performs very well on small-scale data, nevertheless,
    model performance on big data has been a bottleneck of classic machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, deep learning (DL) has demonstrated its more powerful capability
    of analysing heart sounds than classic machine learning [[8](#bib.bib8)]. DL models
    are usually fed with either raw audio signals or time-frequency representations
    extracted from heart sounds as the inputs [[9](#bib.bib9), [10](#bib.bib10)],
    therefore improving the efficiency by skipping selecting hand-crafted acoustic
    features. More complex model structures in deep learning models also enhance the
    models’ capability of learning abstract representations from large-scale datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: A comparison between existing surveys on heart sound analysis.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Surveys | Deep Learning | Heart sound denoising | Heart sound segmentation
    | Heart sound classification | Heart sound interpretation |'
  prefs: []
  type: TYPE_TB
- en: '| Bhoi et al. [[11](#bib.bib11)] | ✗ | mentioned only | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Chakrabarti et al. [[12](#bib.bib12)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Nabih-Ali et al. [[13](#bib.bib13)] | ✗ | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Clifford et al. [[14](#bib.bib14)] | ✓ | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Ghosh et al. [[15](#bib.bib15)] | mentioned only | ✓ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Majhi et al. [[16](#bib.bib16)] | – | – | – | ✓ | – |'
  prefs: []
  type: TYPE_TB
- en: '| Dwivedi et al. [[17](#bib.bib17)] | ✗ | ✗ | ✓ | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| This survey | ✓ | ✓ | ✓ | ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: 1.1 Differences between this survey and the former ones
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several review research studies on heart sound analysis. Feature extraction
    and basic ML models (e. g., SVMs) were introduced in [[11](#bib.bib11)]. Waveform
    features of heart sounds and shallow artificial neural networks were overviewed
    in [[12](#bib.bib12)]. Heart sound denoising and segmentation were discussed in [[13](#bib.bib13)].
    In [[14](#bib.bib14)], approaches for heart sound classification submitted to
    the PhysioNet challenge 2016 were summarised. The study in [[15](#bib.bib15)]
    summarised approaches used for heart sound analysis, from heart sound segmentation
    and feature extraction to heart sound classification. Approaches for heart sound
    classification were summarised and analysed in [[16](#bib.bib16)]. Different from
    these above studies, we will review all DL technologies on heart sound segmentation
    and classification. Furthermore, the state-of-the-art approaches about interpretability
    of DL models will be summarised and discussed (see [Table 1](#S1.T1 "Table 1 ‣
    1 Introduction ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era")). We will also discuss the potential research problems and future research
    directions in this survey, helping promote research studies in heart sound analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Challenges in Heart Sound Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many machine learning and deep learning methods have been applied to heart sound
    analysis. However, this research field is still facing many technical challenges.
  prefs: []
  type: TYPE_NORMAL
- en: The first challenge is denoising, which aims to remove the noise from heart
    sounds. As the recording environments can be noisy with environmental noise and
    speech, denoising is an essential pre-processing procedure to improve the audio
    quality for better performance of segmentation and classification. Other pre-processing
    procedures such as separation (from lung sounds) [[18](#bib.bib18), [19](#bib.bib19),
    [20](#bib.bib20)] are not introduced here, as they were mostly dealing with signal
    processing methods.
  prefs: []
  type: TYPE_NORMAL
- en: The second challenge is segmentation that targets on splitting a heart sound
    signal into multiple parts, i. e., cardiac cycles or smaller segments (S1, systole,
    S2, and diastole). Heart sound segmentation is often a pre-processing process
    of heart sound classification.
  prefs: []
  type: TYPE_NORMAL
- en: The third is classification which predicts the severe level of cardiovascular
    diseases or abnormality of a heart from heart sounds. Heart sound classification
    is helpful for early screening of heart diseases in primary care.
  prefs: []
  type: TYPE_NORMAL
- en: The final one is explaining DL models for heart sound analysis. DL models are
    black boxes for human due to their complex structures, although they are promising
    in performance improvement for heart sound analysis. As digital health is a sensitive
    domain, explainable DL models are crucial for clinicians to give in-time and suitable
    therapies for patients. Correspondingly, trust from clinicians and patients can
    promote applications of explainable DL models in real life.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Contributions of this survey
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The survey is expected to have the following contributions.
  prefs: []
  type: TYPE_NORMAL
- en: '*The first comprehensive survey in heart sound analysis with deep learning.*
    Apart from summarising machine learning techniques for heart sound denoising,
    segmentation, and classification, we review the state-of-the-art deep learning
    topologies for heart sound analysis, especially segmentation and classification.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Summarisation of resources.* We summarise publicly available datasets for
    heart sound analysis, particularly classification. We also provide the collection
    of open-sourced deep learning algorithms for heart sound classification, and discuss
    possible evaluation metrics.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Future research directions.* We discuss the limitation of current deep learning
    methods for heart sound classification, and point out potential future research
    topics in this area. We also discuss the importance of explainable DL models for
    heart sound classification, current advances, and future directions in explainable
    AI.'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Background
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/986c97cc8039efeb6b3b45b70aea4450.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The framework of heart sound analysis. After denoising and segmentation,
    a classifier is trained to produce the prediction and the corresponding interpretation
    for users (clinicians and patients). The ✓ is a normal prediction, and the ✗ means
    the prediction is abnormal. The dashes ‘- - -’ denote optional procedures.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Heart Sounds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In a human’s cardiac system, a normal cardiac cycle contains two heart sounds:
    the first heart sound S1 and the second heart sound S2\. Additional sounds indicate
    diseases: a presented third heart sound S3 could be a sign of heart failure; a
    murmur could indicate defective valves or an orifice in the septal wall [[21](#bib.bib21)].
    The frequency range of S1 and S2 is 20–200 Hz, while the frequency of S3 and S4
    ranges in 15–65 Hz [[22](#bib.bib22)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, murmurs, caused by the turbulent blood flow in the heart system,
    are identified as abnormal sounds. They are very important for verifying the timing
    and pitch for diagnosing cardiovascular diseases [[3](#bib.bib3)]. Murmurs often
    constitute the only basis for diagnosing valvular heart disease [[23](#bib.bib23)].
    Clinically, murmurs consists of two types: systolic murmurs and diastolic murmurs.
    Aortic stenosis, mitral regurgitation, and tricuspid regurgitation happen during
    systole; mitral stenosis and tricuspid stenosis occur during diastole [[3](#bib.bib3)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Diagnosis of Cardiovascular Diseases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Nowadays, there are several non-invasive diagnostic tools for cardiovascular
    diseases. Electrocardiogram (ECG), sensing the P-QRS-T wave depicting the electrical
    activity of the heart [[24](#bib.bib24)], is an inexpensive and commonly-used
    tool for the screening of heart diseases. Yet, it has difficulty in detecting
    structural abnormalities in heart valves and defects characterised by heart murmurs [[25](#bib.bib25)].
    Additionally, several medical imaging tools are able to visualise the cardiovascular
    system. For instance, the echocardiogram (echo) is an ultra sound scan to create
    a moving picture of the heart. It can provide information about the heart’s size,
    shape, structure, and function [[26](#bib.bib26)]. Cardiac computed tomography
    (CT) uses x-rays to create detailed pictures of the heart and its blood vessels [[26](#bib.bib26)].
    For assessment of the cardiovascular system’s function and structure, cardiac
    magnetic resonance imaging (CMRI) creates both still and moving pictures of the
    heart and major blood vessels [[26](#bib.bib26)]. However, these above imaging
    instruments are expensive and require medical professionals for operation, thereby
    limiting their application in clinics, and small and medium hospitals.
  prefs: []
  type: TYPE_NORMAL
- en: Compared to the above diagnostic instruments, cardiac auscultation is low-cost
    and essential in preliminary physical examinations. Phonocardiogram (PCG) signals
    recorded with a phonocardiograph have proven to be important in pediatric cardiology,
    cardiology, and internal diseases [[27](#bib.bib27)]. Recent advances of electronic
    stethoscopes facilitated computer-aided auscultation by integrating sensor design,
    signal processing, and machine learning techniques [[27](#bib.bib27)]. The low-cost
    and portable advantage of electronic stethoscopes make it possible to apply computer-aided
    auscultation to primary care and remote/home health care. Fig.[1](#S2.F1 "Figure
    1 ‣ 2 Background ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep
    Learning Era") depicts a pipeline of heart sound analysis. Heart sounds are processed
    by de-noising, segmentation, and classification, and then clinicians and patients
    receive the predictions and interpretations in primary care. In real-life, patients
    with heart sounds predicted as abnormal will be suggested further professional
    medical examinations for accurate diagnosis.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Heart Sound Analysis Tasks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/aa30dba5df380eeba063f5712995e62c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Categorisation of methods for heart sound analysis. Bold texts are
    deep learning approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this section is to describe the tasks and summarise classic machine
    learning techniques for each problem. Specifically, denoising and segmentation
    are two pre-processing steps for heart sound classification in many research studies.
    In Fig.[2](#S3.F2 "Figure 2 ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey
    on Heart Sound Analysis in the Deep Learning Era"), we summarise approaches used
    for the four tasks during 2017–2022. We can see that, classic machine learning
    was still used in the past years, and deep learning was mainly applied to segmentation
    and classification, as well as interpretation methods. For comparison of machine
    learning and deep learning used for heart sound analysis, this section introduce
    the tasks and classic machine learning methods, and deep learning will be discussed
    in [section 4](#S4 "4 State-of-the-art Studies ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era"). As there are only a few works for interpretation,
    we will discuss heart sound interpretation in the discussion part of this survey (see [section 6](#S6
    "6 Future Research Directions and Open Issues ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era")). Therefore, we describe the three tasks
    (de-noising, segmentation, and classification) as independent sections in the
    following for detailed discussion.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Denoising
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Generally, recorded heart sounds consist of many kinds of noises [[28](#bib.bib28)],
    including white noise and other sounds presented in the recording environments,
    e. g., humans’ speech. Noise may degrade the segmentation and classification performance
    of heart sounds[[28](#bib.bib28)]. In this regard, many studies have investigated
    denoising methods for better performance on the task of heart sound segmentation
    and classification.
  prefs: []
  type: TYPE_NORMAL
- en: Filters. As a preprocessing procedure of heart sound classification, many denoising
    approaches employ signal filters for removing noise from noisy heart sounds [[9](#bib.bib9),
    [29](#bib.bib29), [30](#bib.bib30)]. Highpass filters [[31](#bib.bib31), [32](#bib.bib32)]
    have been used to eliminate low-frequency noise. With the capability of mitigating
    both high- and low-frequency noises, bandpass filters are more often used for
    heart sounds denoising [[33](#bib.bib33), [34](#bib.bib34), [35](#bib.bib35)].
    A Butterworth bandpass filter has been successfully employed in many studies [[36](#bib.bib36),
    [37](#bib.bib37), [38](#bib.bib38), [39](#bib.bib39), [40](#bib.bib40), [41](#bib.bib41),
    [42](#bib.bib42), [43](#bib.bib43)]. The cutoff frequency of a Butterworth bandpass
    filter is set with a low frequency for filtering out noise with very low frequencies
    and a high frequency for filtering out high-frequency noises. A range of Butterworth
    bandpass filters with various orders have been applied with different cutoff frequency
    settings. For instance, a 4-th order Butterworth filter was set with a cutoff
    frequency of 25-400 Hz in [[44](#bib.bib44)], and a 5-th order Butterworth filter
    was designed to have a cutoff frequency of 25-500 Hz in [[45](#bib.bib45)] and
    25-250 Hz in [[46](#bib.bib46)]. A 6-th order Butterworth filter was designed
    with a cutoff frequency of 50–950 Hz [[47](#bib.bib47), [48](#bib.bib48)] and
    30-900 Hz in [[49](#bib.bib49)]. Additionally, several other filters were also
    used for denoising heart sound, such as a Savitzky–Golay filter [[50](#bib.bib50),
    [51](#bib.bib51)], Chebyshev low-pass filter [[52](#bib.bib52), [53](#bib.bib53)],
    and Notch filter [[54](#bib.bib54)].
  prefs: []
  type: TYPE_NORMAL
- en: Spectrum-based denoising. To remove noise, the spectrogram was simply selected
    with a threshold of -30, -45, -60, or -75 dB in [[55](#bib.bib55)]. However, it
    is time-consuming to search a suitable threshold among different heart sounds.
    A more flexible method, spectral subtraction [[56](#bib.bib56)], was used to estimate
    the noise and remove it from the heart sounds [[43](#bib.bib43)].
  prefs: []
  type: TYPE_NORMAL
- en: Spike removal. Frictional spike is a redundant part of the amplitude of a heart
    sound. In several studies [[44](#bib.bib44), [38](#bib.bib38)], frictional spikes
    were detected and removed (i. e., replaced by zeros) during pre-processing of
    heart sounds.
  prefs: []
  type: TYPE_NORMAL
- en: Selection of noise-free segments. Apart from removing noise from heart sounds,
    the usage of noise-free heart sound segments has been shown helpful for heart
    sound analysis. Wavelet entropy was used to evaluate noise in heart sound segments [[7](#bib.bib7)],
    as clean heart sounds have relatively higher wavelet entropy than noisy heart
    sounds. Empirical wavelet transform was used to separate heart sounds, murmurs,
    low-frequency artefacts, and high-frequency noises in another study [[57](#bib.bib57)].
    Additionally, classic machine learning was also used for detecting noise-free
    heart sound segments. In [[4](#bib.bib4)], SVMs were applied to classify the quality
    of heart sound signals into binary classes (i. e., ‘unacceptable’ and ‘acceptable’)
    or three classes (i. e., ‘unacceptable’, ‘good’, ‘excellent’) without segmentation
    based on ten types of multi-domain features.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4bb5979f0042467d42d79ce700c1c783.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The PCG recording of a *normal* heart sound recording from the PhysioNet/CinC
    Database [[58](#bib.bib58)]. Frames in the middle with four states labelled (i. e., S1,
    systole, S2, and diastole) are depicted.'
  prefs: []
  type: TYPE_NORMAL
- en: Heart sound segmentation was used to split an audio sample into a set of smaller
    audio segments, which could be equal to or shorter than a complete cardiac cycle [[39](#bib.bib39),
    [59](#bib.bib59), [60](#bib.bib60)]. The segments shorter than a cardiac cycle
    could include S1, systole, S2, and diastole, as indicated in Fig. [3](#S3.F3 "Figure
    3 ‣ 3.2 Segmentation ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on
    Heart Sound Analysis in the Deep Learning Era").
  prefs: []
  type: TYPE_NORMAL
- en: Energy-based segmentation. As heart sounds at different states have various
    energies, signal energy has been used for localising S1 and S2 peaks [[61](#bib.bib61),
    [29](#bib.bib29), [30](#bib.bib30), [62](#bib.bib62)]. Based on frequency information
    (e. g., Wavelet Transform (WT)) of heart sounds, energy peaks of wavelet coefficients
    were detected for localising S1 and S2 in [[63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: Envelope-based segmentation. Apart from energy, heart sound segmentation can
    be achieved based on envelopes [[64](#bib.bib64), [46](#bib.bib46)]. For instance,
    heart sound segmentation was implemented based on Shannon energy envelope and
    zero crossings of heart sounds in [[9](#bib.bib9)]. In another two studies [[47](#bib.bib47),
    [48](#bib.bib48)], S1 of the first heart cycle was detected based on Shannon energy
    envelopes, and the next S1 heart sounds were detected by a sliding heart cycle
    window.
  prefs: []
  type: TYPE_NORMAL
- en: 'Loudness-based segmentation. Loudness has proven its potential to segment heart
    sounds [[53](#bib.bib53), [65](#bib.bib65)]. Specifically, spectrograms extracted
    from heart sounds are firstly converted into the Bark scale and smoothed with
    a Hanning window. At each time frame, the sensation of loudness is then calculated
    by the mean of the amplitudes at all frequency bands: $L(t)=\frac{\sum_{t=1}^{T}A(t)}{T}$,
    where $A(t)$ is the amplitude at the $t$-th time frame, and $T$ is the total number
    of time frames. Furthermore, the derivation of the loudness function is computed
    for obtaining peaks. Therefore, systoles and diastoles can be localised as they
    have different time lengths.'
  prefs: []
  type: TYPE_NORMAL
- en: Classic Machine Learning for Segmentation. ML models have been proposed for
    more precise heart sound segmentation than the above mentioned rule-based segmentation
    methods. ML models for segmentation are mainly trained in supervised learning
    frameworks. There are also a few works using unsupervised learning. Both unsupervised
    and supervised learning approaches are introduced in the following.
  prefs: []
  type: TYPE_NORMAL
- en: '*Unsupervised Learning* Considering the limited availability of the heart sound
    datasets, the authors of [[66](#bib.bib66)] adopted an unsupervised spectral clustering
    technique based on Gaussian kernel similarity to get the frame labels (e. g., S1
    and S2), which are further utilised to segment the heart sound.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Supervised Learning* Particularly, a hidden Markov model (HMM) has been widely
    used for this purpose [[5](#bib.bib5)]. Let us assume the heart states as $S=\{s_{1},s_{2},s_{3},s_{4}\}=\{S1,\mbox{systole},S2,\mbox{diastole}\}$,
    and the observations $O=\{o_{1},o_{2},...,o_{T}\}$ as the raw heart sounds or
    acoustic features. A transmission matrix $A=\{a_{ij}\}$ denotes the probablity
    of a state $s_{i}$ at the $t$-th time frame moving to $s_{j}$ at the $(t+1)$-th
    time frame. The probability density distribution of an observation $o_{t}$ to
    be generated by a state $s_{i}$ is $B={b_{i}(o_{t})}=P[o_{t}|s_{i}]$, where $P$
    means probability. The initial state distribution is $\pi=\{\pi_{i}\}$, representing
    the probability of state $s_{i}$ at the start time frame. Given $A$, $B$, $\pi$,
    and $O$, an HMM model aims to optimise the state sequence. The Viterbi algorithm
    is often used for this purpose [[67](#bib.bib67)] and more details can be found
    in [[5](#bib.bib5)].'
  prefs: []
  type: TYPE_NORMAL
- en: In order to better capture the abrupt changes in the PCG signal, the envelope
    of the signal was obtained in [[68](#bib.bib68)], and the kurtosis of the envelope
    was computed to get the impulse-like characteristics, which were further passed
    through a zero frequency filter for pure impulse information. Along with the heart
    sound labels, the extracted features were fed into a hidden semi-Markov model
    (HSMM). To better adapt the variability of the heart cycle duration (HCD) in the
    PCG recordings, a multi-centroid-duration-based HSMM was introduced in [[69](#bib.bib69)],
    where HCDs were estimated at various instances of a PCG to get maximum possible
    duration values and those nearest values were clubbed into clusters to refer each
    centroid. With more accurate state duration information, the HSMM achieved better
    segmentation performance. Similarly, considering the inter-patient variability,
    the emission probability distributions to each patient were estimated through
    a Gaussian mixture model (GMM) in an unsupervised and adaptive way in the improved
    HSMM in another study [[70](#bib.bib70)]. Moreover, the expectation maximisation
    algorithm developed in [[71](#bib.bib71)] searched for sojourn time distribution
    parameters of an HSMM for each subject. Many studies [[6](#bib.bib6), [72](#bib.bib72),
    [73](#bib.bib73), [74](#bib.bib74), [75](#bib.bib75), [40](#bib.bib40), [41](#bib.bib41),
    [76](#bib.bib76), [43](#bib.bib43), [31](#bib.bib31), [77](#bib.bib77), [78](#bib.bib78),
    [79](#bib.bib79)] have employed logistic regression-based HSMM (LR-HSMM) proposed
    in [[80](#bib.bib80)] for heart sound segmentation. LR was incorporated to predict
    the probability of $P[s_{j}|o_{t}]$, and $B$ is then computed with the Bayes rule.
    There are also other improved HMM methods, such as the *duration-dependent HMM* [[67](#bib.bib67),
    [81](#bib.bib81)], which considers the probability density function of the duration
    at each state. Another study [[38](#bib.bib38)] proposed a Markov-switching model
    for heart sound segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The target task of automated auscultation is heart sound classification, including
    i) abnormal heart sound detection (e. g., murmurs, mitral stenosis, etc.) and
    ii) severity of cardiovascular diseases (normal/mild/moderate).
  prefs: []
  type: TYPE_NORMAL
- en: Feature Engineering. After denoising and segmentation, feature extraction is
    often performed before training a classifier. Low-level descriptors (LLDs) and
    functionals are often extracted as acoustic features. LLDs are segmental features
    on short-time segment analysis (see Table [2](#S3.T2 "Table 2 ‣ 3.3 Classification
    ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")), and functionals are supra-segmental feature vectors
    projected from LLDs. Functionals are generally statistical features, such as mean,
    max, standard deviation, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: We mainly list the LLDs used for heart sound classification in [Table 2](#S3.T2
    "Table 2 ‣ 3.3 Classification ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era"). Apart from time-domain
    LLDs, LLDs in the frequency-domain are widely used for heart sound classification.
    Specifically, the frequency-domain features consist of spectral features based
    on Fourier Transform, Mel-scaled spectral features, and wavelet features. There
    are also pre-existing feature sets used for heart sound classification, including
    the ComParE feature set [[82](#bib.bib82)] and the eGeMAPS feature set [[83](#bib.bib83)].
    Both feature sets can be extracted with openSMILE which is an open-source toolkit [[84](#bib.bib84)].
  prefs: []
  type: TYPE_NORMAL
- en: In addition to the aforementioned hand-crafted features, deep representations
    have been studied more recently (see [Table 2](#S3.T2 "Table 2 ‣ 3.3 Classification
    ‣ 3 Heart Sound Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis
    in the Deep Learning Era")). Due to the strong capability of deep learning models
    for extracting abstract features, deep representations bear potential to improve
    the performance of hand-crafted features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Hand-crafted features and deep representations for heart sound classification.
    We present various low-level descriptors (LLDs) in hand-crafted features.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Group | Features | Description | Reference |'
  prefs: []
  type: TYPE_TB
- en: '| Time-domain | Envolope | Envelope of a signal | [[85](#bib.bib85), [57](#bib.bib57)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Amplitude | Amplitude of a signal | [[63](#bib.bib63)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Energy | Energy of a signal | [[46](#bib.bib46), [32](#bib.bib32), [62](#bib.bib62)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Entropy | Signal entropy | [[86](#bib.bib86), [32](#bib.bib32)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Loudness | Perception of sound magnitude | [[53](#bib.bib53)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Peak amplitude | Amplitude of peaks | [[40](#bib.bib40)] |'
  prefs: []
  type: TYPE_TB
- en: '| Spectral | Spectral amplitude | Fourier transform | [[7](#bib.bib7), [87](#bib.bib87),
    [46](#bib.bib46), [88](#bib.bib88), [86](#bib.bib86), [33](#bib.bib33), [32](#bib.bib32),
    [89](#bib.bib89), [38](#bib.bib38)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dominant frequency value | Frequency which leads to the maximum spectrum
    | [[87](#bib.bib87), [88](#bib.bib88)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Dominant frequency ratio | Ratio of the maximun energy to the total energy
    | [[87](#bib.bib87), [88](#bib.bib88)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Energy | Spectral energy | [[38](#bib.bib38)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Spectral roll-off | Frequency below a specific percentage of the total
    spectral energy | [[32](#bib.bib32)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Spectral centroid | Average of magnitude spectrogram at each frame | [[32](#bib.bib32),
    [89](#bib.bib89)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Specrtal flux | Changing speed of the power spectrum | [[32](#bib.bib32)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Power spectral density (PSD) | Distribution of power in spectral components
    | [[46](#bib.bib46), [85](#bib.bib85), [89](#bib.bib89)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Spectral entropy | Shannon entropy of PSD | [[87](#bib.bib87), [88](#bib.bib88),
    [86](#bib.bib86), [54](#bib.bib54)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Instantaneous frequency | Frequency for non-stationary signals | [[90](#bib.bib90)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fractional Fourier transform entropy | Spectral entropy of the fractional
    Fourier transform | [[91](#bib.bib91)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Spectrogram | Short-Time Fourier Transform (STFT) | [[47](#bib.bib47),
    [48](#bib.bib48)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Cepstrum | Inverse Fourier transform on the logarithm of the signal spectrum
    | [[33](#bib.bib33)] |'
  prefs: []
  type: TYPE_TB
- en: '| Mel frequency | Mel-frequency | Mel-scaled frequency | [[92](#bib.bib92),
    [73](#bib.bib73)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Mel-Frequency Cepstral Coefficients (MFCCs) | Discrete cosine transform
    of Mel-scaled spectrogram | [[38](#bib.bib38), [93](#bib.bib93), [87](#bib.bib87),
    [86](#bib.bib86), [79](#bib.bib79), [94](#bib.bib94), [75](#bib.bib75), [95](#bib.bib95)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Fractional Fourier transform-based Mel-frequency | Mel-frequency from
    the fractional Fourier transform | [[43](#bib.bib43)] |'
  prefs: []
  type: TYPE_TB
- en: '| Wavelet | Wavelet transform | Frequency analysis of a signal at various scales
    | [[85](#bib.bib85), [59](#bib.bib59), [89](#bib.bib89)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Wavelet scattering transform | “Wavelet convolution with nonlinear modulus
    and averaging scaling function”¹¹1https://de.mathworks.com/help/wavelet/ug/wavelet-scattering.html
    (translation invariance and elastic deformation stability [[96](#bib.bib96)])
    | [[97](#bib.bib97), [96](#bib.bib96)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Wavelet synchrosqueezing transform | Reassignment of wavelet coefficients
    | [[35](#bib.bib35)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Tunable quality wavelet transform | “Wavelet multiresolution analysis
    with a user-specified Q-factor, which is the ratio of the centre frequency to
    the bandwidth of the filters”²²2https://de.mathworks.com/help/wavelet/ug/tunable-q-factor-wavelet-transform.html
    | [[98](#bib.bib98), [52](#bib.bib52)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Wavelet entropy | Temporal energy distribution based on wavelet coefficients
    | [[7](#bib.bib7)] |'
  prefs: []
  type: TYPE_TB
- en: '| Feature set | ComParE | Computational Paralinguistics ChallengE feature set
    | [[92](#bib.bib92)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | eGeMAPS | The extended Geneva Minimalistic Acoustic Parameter Set | [[92](#bib.bib92)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| Deep representation | Graph-based features | Petersen graph pattern | [[99](#bib.bib99)]
    |'
  prefs: []
  type: TYPE_TB
- en: '|  | Sparse coefficient | Result of sparse coding | [[6](#bib.bib6)] |'
  prefs: []
  type: TYPE_TB
- en: '|  | Autoencoder-based features | Features extracted by an autoencoder from
    hand-crafted features | [[100](#bib.bib100), [55](#bib.bib55)] |'
  prefs: []
  type: TYPE_TB
- en: Classic Machine Learning for Classification.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7e32d7c8630835d1474b4becf9ce2037.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Statistics of the literature using discriminative machine learning
    models for heart sound classification during 2017–2022\. FNN: feed-forward neural
    network; KNN: $k$-nearest neighbour; LDC: linear discriminant classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based classification was proposed for heart sound classification in [[57](#bib.bib57),
    [101](#bib.bib101)]. For better performance, ML was used for heart sound classification
    in most research studies. We briefly introduce the classifiers in the two groups
    of generative models and discriminative models in the following.
  prefs: []
  type: TYPE_NORMAL
- en: '*Generative models* aim to generate the joint probability distribution $P(X,y)$,
    given the features $X$ and the labels $y$. The posterior probability $P(y|X)$
    is computed via the Bayes rule $P(y|X)=\frac{P(X,y)}{P(X)}=\frac{P(X|y)P(y)}{P(X)}$,
    where $P(X|y)$ is the likelihood probability distribution. The Naïve Bayes Classifier [[102](#bib.bib102),
    [91](#bib.bib91), [41](#bib.bib41), [103](#bib.bib103)] was widely used for heart
    sound classification due to its advantage of being easy-to-use. Gaussian Mixture
    Models (GMMs) [[95](#bib.bib95), [53](#bib.bib53)] were used to estimate the data
    distribution by optimising the weights of Gaussian mixture components and mean
    and variance in each component. A Gaussian mixture-based HMM [[38](#bib.bib38)]
    was employed for heart sound classification considering the four sequential heart
    states, i. e., S1, systole, S2, and diastole.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Discriminative models* are designed to directly predict the posterior probability
    $P(y|X)$ given $X$. Fig. [4](#S3.F4 "Figure 4 ‣ 3.3 Classification ‣ 3 Heart Sound
    Analysis Tasks ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning
    Era") presents a statistic of recent works from 2017 to 2022 that employ classic
    machine learning models for heart sound classification. SVMs have been very widely
    used for heart sound classification by learning a supporting hyperplane between
    classes [[6](#bib.bib6), [47](#bib.bib47), [48](#bib.bib48), [91](#bib.bib91),
    [93](#bib.bib93), [97](#bib.bib97), [46](#bib.bib46), [99](#bib.bib99), [40](#bib.bib40),
    [75](#bib.bib75), [41](#bib.bib41), [95](#bib.bib95), [60](#bib.bib60), [43](#bib.bib43),
    [92](#bib.bib92), [100](#bib.bib100), [55](#bib.bib55), [103](#bib.bib103), [33](#bib.bib33),
    [79](#bib.bib79), [64](#bib.bib64), [85](#bib.bib85), [96](#bib.bib96)]. Apart
    from linear projection between data samples and labels, SVMs can learn separating
    hyperplanes on non-linear data via non-linear kernels, such as radial basis function.'
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, $k$-nearest neighbours (KNNs) has shown good performance on heart
    sound classification with the idea of classifying a data sample according to the
    classes of its $k$-nearest neighbours [[91](#bib.bib91), [93](#bib.bib93), [46](#bib.bib46),
    [40](#bib.bib40), [75](#bib.bib75), [85](#bib.bib85), [90](#bib.bib90), [41](#bib.bib41),
    [60](#bib.bib60), [43](#bib.bib43), [103](#bib.bib103), [104](#bib.bib104)]. Also,
    decision trees were successfully used for heart sound classification [[7](#bib.bib7),
    [99](#bib.bib99), [40](#bib.bib40), [88](#bib.bib88), [75](#bib.bib75), [41](#bib.bib41),
    [103](#bib.bib103)]. One reason is that limiting the number of decision nodes
    can help avoid overfitting [[7](#bib.bib7)], and another reason is that the structure
    of a decision tree can show the internal logic for classification. Bagged trees [[99](#bib.bib99),
    [40](#bib.bib40)] assemble multiple decision trees for more complex model architectures,
    therefore it is possible to achieve better performance. Random forests [[93](#bib.bib93),
    [88](#bib.bib88), [90](#bib.bib90), [103](#bib.bib103), [79](#bib.bib79), [35](#bib.bib35)]
    further improve bagged trees with less features to be used when splitting each
    node.
  prefs: []
  type: TYPE_NORMAL
- en: In recent years, feed-forward Neural Networks (FNNs) have started to be applied
    to heart sound classification [[63](#bib.bib63), [52](#bib.bib52), [87](#bib.bib87),
    [75](#bib.bib75), [55](#bib.bib55), [32](#bib.bib32), [94](#bib.bib94), [30](#bib.bib30),
    [89](#bib.bib89)]. FNNs can automatically learn a non-linear projection between
    acoustic features and labels. Although FNNs lack of explainability compared to
    other classifiers such as SVMs and decision trees, they are promising to produce
    good performance.
  prefs: []
  type: TYPE_NORMAL
- en: There are also several other machine learning models, such as, linear discriminant
    classifiers [[99](#bib.bib99), [55](#bib.bib55), [103](#bib.bib103)], logistic
    regression [[103](#bib.bib103)], quadratic discriminant analysis [[46](#bib.bib46)],
    boosting methods [[88](#bib.bib88), [59](#bib.bib59), [79](#bib.bib79), [98](#bib.bib98)],
    and others [[86](#bib.bib86), [54](#bib.bib54)]. Finally, multiple classifiers
    can be further assembled for better performance compared to that of a single model [[93](#bib.bib93),
    [75](#bib.bib75), [59](#bib.bib59), [43](#bib.bib43), [85](#bib.bib85)].
  prefs: []
  type: TYPE_NORMAL
- en: 4 State-of-the-art Studies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Due to the strong capability of extracting effective representations, deep learning
    has been successfully applied to a range of acoustic tasks, e. g., speech emotion
    recognition [[105](#bib.bib105)], respiratory sound classification [[106](#bib.bib106)],
    snore sound classification [[107](#bib.bib107)], and many more. In recent advances,
    deep learning has been also employed in processing heart sound signals and achieved
    good performance [[8](#bib.bib8)]. In this regard, the applications of deep learning
    methods for heart sound analysis tasks are introduced and discussed in the following.
    Notably, as there are few works focusing on denoising in this context with deep
    learning, we introduce deep learning topologies for segmentation and classification
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Deep Learning for Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Various DL models have been proposed for heart sound segmentation. We group
    them into CNNs and RNNs which extract spatial and sequential representations,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural networks. Inspired by successful applications of deep convolutional
    neural networks (CNNs) in image segmentation, deep CNNs have been applied to heart
    sound segmentation in recent studies [[108](#bib.bib108)]. For instance, several
    CNN-based segmentation algorithms were proposed and compared in [[108](#bib.bib108)],
    including CNNs with sequential max temporal modelling, CNNs with HMMs or HSMMs
    to model the probability density distribution of observations.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent neural networks. Recurrent neural networks (RNNs) have demonstrated
    their ability to exploit temporal information in sequential data. Therefore, RNNs
    can also help in locating the states of heart sounds. In [[109](#bib.bib109)],
    the authors regarded segmentation as an event detection task and developed bi-directional
    Gated Recurrent Unit (GRU)-RNNs based on spectrogram and envelop features. Since
    envelope features fail to effectively model the intrinsic duration information
    of the heart cycles, a duration-LSTM was proposed in [[110](#bib.bib110)] to integrate
    the duration vector into the standard LSTM cells with envelope features to obtain
    better segmentation performance. Specifically, duration parameters consist of
    heart cycle duration and systole duration estimated from the envelope autocorrelation.
    Without envelopes and time-frequency based features, the authors of [[111](#bib.bib111)]
    utilised bi-directional GRU-RNNs to segment the heart sound directly. Considering
    the possible noisy and irregular sequences in heart sound signals, an attention-based
    RNN framework was introduced in [[112](#bib.bib112)]. Specifically, before the
    final classification layer, with the hidden representation returned by bi-directional
    LSTMs, a single linear layer was applied to learn the weight score of each hidden
    state. Such weight score values are multiplied with the hidden representation
    for the final classification.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs + RNNs. An end-to-end model was proposed in [[113](#bib.bib113)], where
    CNNs and LSTM recurrent neural networks (RNNs) are integrated to learn rich and
    efficient features from the audio directly. The gate structures of each LSTM unit
    are optimised in [[114](#bib.bib114)] for efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Deep Learning for Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c58f3a7a394e25d1527ffcb1838fdbd6.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Pipeline of DL models working on heart sounds. “1” indicates transfer
    learning; “2” means deep learning on the time-frequency representation; “3” depicts
    end-to-end learning. The three branches can be in parallel or assembled at the
    feature-/decision-level. DL can be also utilised for processing features apart
    from raw audio signals and time-frequency representations. The processing of DL
    on other features can be found in [subsection 4.2](#S4.SS2 "4.2 Deep Learning
    for Classification ‣ 4 State-of-the-art Studies ‣ A Comprehensive Survey on Heart
    Sound Analysis in the Deep Learning Era").'
  prefs: []
  type: TYPE_NORMAL
- en: Different from classic machine learning, DL learns effective representations
    from either raw heart sounds or simple time-frequency representations using models
    with more parameters. DL often performs very well on many acoustic tasks benefiting
    from its strong capability. Apart from the pipeline shown in Fig. [5](#S4.F5 "Figure
    5 ‣ 4.2 Deep Learning for Classification ‣ 4 State-of-the-art Studies ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era"), we summarise the advances
    of deep learning methods for heart sound classification in the following.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning on time-frequency representations. As it is challenging to extract
    effective representations from raw heart sound signals, 2D time-frequency representations
    have been widely used as the input of 2D CNNs for heart sound classification,
    respectively [[72](#bib.bib72), [73](#bib.bib73), [50](#bib.bib50), [39](#bib.bib39),
    [115](#bib.bib115), [81](#bib.bib81), [76](#bib.bib76), [116](#bib.bib116), [117](#bib.bib117),
    [49](#bib.bib49), [77](#bib.bib77), [42](#bib.bib42), [118](#bib.bib118), [61](#bib.bib61),
    [29](#bib.bib29), [119](#bib.bib119)]. Spectrograms, extracted by STFT from heart
    sounds, were fed into ResNet for abormal heart sounds detection in [[120](#bib.bib120)].
    Multi-domain features were considered to be more comprehensive in reflecting the
    characteristics of all heart sound classes. In [[50](#bib.bib50)], spectrograms,
    Mel spectrograms and MFCCs were extracted as the inputs of VGG models, and the
    final predictions were obtained by ensemble learning.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from CNNs that are good at extracting spatial features, RNNs are more
    capable of extracting temporal features from sequential signals. LSTM nets were
    applied to process discrete wavelet transforms and MFCCs for heart sound classification [[36](#bib.bib36),
    [75](#bib.bib75)]. Deng et al. employed convolutional recurrent neural networks
    which combined the CNNs and RNNs [[121](#bib.bib121)].
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, there are also other classifiers used for heart sound classification,
    such as a stacked sparse autoencoder deep neural network [[122](#bib.bib122)]
    and a semi-non-negative matrix factorisation classifier [[78](#bib.bib78)]. More
    details can be found in the listed references.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning on other features. Apart from the above excellent works about
    DL on time-frequency representations, other features extracted from heart sounds
    were also used as the input of DL models.
  prefs: []
  type: TYPE_NORMAL
- en: i) *Time-domain features*. Similar to features in classic machine learning,
    1D time-domain features can be also fed into DL models for heart sound classification.
    For instance, instant energy of the heart sound was extracted and used as the
    input of stacked auto-encoder networks [[62](#bib.bib62)]. Multiple statistical
    features (such as mean, median, variance, and many more) were extracted from all
    75 ms segments in each complete heart sound clip, and fed into a bidirectional
    LSTM (BiLSTM) net for classification in [[123](#bib.bib123)].
  prefs: []
  type: TYPE_NORMAL
- en: ii) *1D frequency-based features*. Either 1D CNNs or feed-forward DNN models
    can be used to process 1D features. General frequency features and Mel domain
    features were fed into 1D CNNs and then the multiple CNNs were assembled for the
    final prediction in [[119](#bib.bib119)]. Moreover, Mel spectrograms and MFCC
    were used to further extract features as the input of a 5-layer feed-forward DNN
    model in [[9](#bib.bib9)].
  prefs: []
  type: TYPE_NORMAL
- en: iii) *2D frequency-based features*. Different from the above time-frequency
    representations, herein, we list 2D frequency-based features to include (a) multiple
    1D frequency features directly extracted from audio segments rather than window
    functions in the STFT domain and (b) features computed from time-frequency features.
    Qian et al. utilised wavelets to calculate wavelet energy features from a set
    of short acoustic segments and further used GRU-RNNs as the classifier [[124](#bib.bib124)].
    Dong et al. extracted log Mel features and corresponding functionals from heart
    sound segments and implemented classification LSTM-RNNs and GRU-RNNs [[92](#bib.bib92)].
    In their experiments, log Mel features performed better than MFCCs and other LLDs [[92](#bib.bib92)].
    Zhang et al. extracted temporal quasi-periodic features computed by an average
    magnitude difference function from spectrograms and applied LSTM-RNNs for exploring
    the dependency relation within the features [[125](#bib.bib125)]. A denoising
    auto-encoder was employed to extract deep representations from spectrograms as
    the input of the classifier of 1D CNNs in [[126](#bib.bib126)].
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end learning. In recent years, as time-frequency representations and
    other features still need human efforts to select features, it has been becoming
    increasingly popular to use end-to-end networks to learn representations from
    heart sounds. Based on raw heart sound signals, various 1D CNN architectures have
    been proposed and applied to the task of heart sound classification [[10](#bib.bib10),
    [39](#bib.bib39), [51](#bib.bib51), [34](#bib.bib34), [127](#bib.bib127)]. Furthermore,
    Liu et al. introduced a temporal convolutional network (TCN) that performed a
    high sensitivity for heart sound classification [[74](#bib.bib74)], as a TCN benefiting
    from dilated and casual convolutions is more suitable for sequential data than
    typical CNNs are. A 1D CNN model consisting of residual blocks was developed for
    classifying heart sounds [[128](#bib.bib128)]. Apart from CNNs, GRU-RNNs were
    also used to process raw heart sound signals for the screening of heart failure [[129](#bib.bib129)].
  prefs: []
  type: TYPE_NORMAL
- en: Several studies also mentioned the capability of CNNs and RNNs for learning
    frequency-domain and time-domain characteristics of heart sounds. For instance,
    Shuvo et al. proposed a CardioXNet model that employed representation learning
    followed by sequence residual learning without any preprocessing [[130](#bib.bib130)].
    In the representation learning phase, three parallel 1D CNN pathways were constructed
    to extract time-invariant features from heart sound signals; in the sequence residual
    learning phase, BiLSTM nets were employed to learn sequential representation.
    The study in [[45](#bib.bib45)] attempted to automatically learn time-frequency
    features, i. e., frequency-domain features were extracted by 1D CNNs and the time-domain
    characteristics were extracted by GRU-RNNs. A self-attention mechanism was further
    used to fuse the two types of features for the final classification. In [[131](#bib.bib131),
    [132](#bib.bib132)], time-convolution (tConv) layers were implemented at the front
    end of the network for learning finite impulse response filters.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning. As data collection in the healthcare domain has to be executed
    according to extremely strict regulations, heart sound datasets are usually not
    as large as datasets in other areas of Computer Audition. Transfer learning attempts
    to employ pre-trained DL models which are optimised on large-scale datasets. In
    recent studies, pre-trained models are mainly learnt on either an image dataset
    (i. e., ImageNet [[133](#bib.bib133)]) and an audio dataset (i. e., AudioSet [[134](#bib.bib134)]).
    Although heart sounds are presented as audio signals which are a different data
    type from ImageNet, DL models trained on ImageNet have shown good performance
    on time-frequency representations extracted from heart sounds for heart sound
    classification. Typical DL models on ImageNet, such as AlexNet [[135](#bib.bib135)]
    and VGG [[136](#bib.bib136)], have been successfully used for heart sound classification [[44](#bib.bib44),
    [31](#bib.bib31), [137](#bib.bib137), [8](#bib.bib8)]. Compared to ImageNet, AudioSet
    includes multiple types of acoustic signals and therefore is more close to heart
    sounds with the consideration of data type. In [[138](#bib.bib138)], pre-trained
    Audio Neural Networks (PANNs) trained on AudioSet were used for classifying heart
    sounds with inputs of time-frequency representations. PANNs outperformed ImageNet-based
    models [[138](#bib.bib138)], including VGG, MobileNet V2 [[139](#bib.bib139)],
    ResNet [[140](#bib.bib140)], and ResNeXt [[141](#bib.bib141)].
  prefs: []
  type: TYPE_NORMAL
- en: After extracting representations by pre-trained models, transfer learning uses
    various types of classifiers for classification, mainly including classic machine
    learning classifiers and feed-forward neural networks. For instance, SVMs were
    applied to representations extracted by AlexNet, VGG16, and VGG19 in [[44](#bib.bib44),
    [137](#bib.bib137), [31](#bib.bib31)]. Other classifiers such as KNNs were used
    in [[44](#bib.bib44)]. Pre-trained VGG model was frozen and followed by fully
    connected layers in [[8](#bib.bib8)].
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, fine-tuning pre-trained models in transfer learning has also shown
    good performance for heart sound classification. A pre-trained AlexNet after fine-tuning
    provided effective representations for heart sound classification [[44](#bib.bib44),
    [31](#bib.bib31)]. Similarly, PANNs was also fine-tuned in [[138](#bib.bib138)].
    Fine-tuned models have even outperformed pre-trained models as they adapted to
    the data distribution of heart sound datasets. In [[8](#bib.bib8)], fine-tuned
    VGG performed better than pre-trained VGG on heart sound classification when SVMs
    were the classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Published Resources on Heart Sound Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Published Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the past years, several heart sound databases have been collected. Herein,
    we briefly introduce the following access-available databases listed in [Table 3](#S5.T3
    "Table 3 ‣ 5.1 Published Datasets ‣ 5 Published Resources on Heart Sound Analysis
    ‣ A Comprehensive Survey on Heart Sound Analysis in the Deep Learning Era").
  prefs: []
  type: TYPE_NORMAL
- en: 'The PASCAL challenge Database [[142](#bib.bib142)] was split into two sets
    A and B. In the dataset A, 176 heart sounds (0.393 hour) were recorded with the
    iStethoscope Pro iPhone app and annotated into S1 and S2 sounds for heart sound
    segmentation. Each heart sound in the dataset A was also labelled into one of
    the four classes: *normal*, *murmur*, *extra heart sound*, and *artifact*. The
    dataset B with 656 recordings (1.194 hours) was annotated into three classes:
    *normal*, *murmur*, and *extrasystole*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The PhysioNet/CinC Database [[58](#bib.bib58)], used in the PhysioNet/CinC
    Challenge 2016 [[143](#bib.bib143)], consists of multiple databases that were
    recorded from different data collectors. Especially, the publicly available training
    set has five databases collected from both healthy individuals and patients. The
    training set consists of 3,240 recordings (20.216 hours in total) from more than
    764 subjects. The task was set to classifying each data sample into three classes:
    *normal* and *abnormal*, and *noisy*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Heart Sounds Shenzhen (HSS) corpus [[92](#bib.bib92)], employed in the
    INTERSPEECH Computaional Paralinguistic challengE (ComParE) 2018, was collected
    by the Shenzhen University General Hospital. In total, 170 subjects (f: 55, m:
    115) participated in the data collection with an electronic stethoscope, leading
    to 845 heart sound recordings of 7.047 hours. Each audio sample was annotated
    into one of the three classes: *normal*, *mild*, *moderate/severe*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An open heart sound database on GitHub [[144](#bib.bib144)] was collected for
    1,000 audio files (0.679 hour in total). The audio recordings are balanced in
    five classes: *normal*, *aortic stenosis*, *mitral regurgitation*, *mitral stenosis*,
    *mitral valve prolapse*.'
  prefs: []
  type: TYPE_NORMAL
- en: The Michigan Heart sound database³³3https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library⁴⁴4https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/
  prefs: []
  type: TYPE_NORMAL
- en: 'primer_heartsound.html provides heart sound samples recorded from different
    areas and poses: the apex area when a subject is supine, the apex area for left
    decubitus, the aortic area when sitting, and the pulmonic area for supine. In
    total, 23 heart sounds were recorded and the total duration is 0.413 hour. The
    heart sounds were annotated into *normal* and multiple *pathological* states.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The CirCor DigiScope Database [[145](#bib.bib145)], used in the George B. Moody
    PhysioNet Challenge 2022 [[146](#bib.bib146)], was collected from a pediatric
    population at or under 21 years old. The heart sounds were recorded from one or
    multiple locations: pulmonary valve, aortic valve, mitral valve, tricuspid valve,
    and others. The publicly available training set consist of 3,163 audio samples
    with 20.094 hours in total from 942 participants. Two classification tasks were
    targeted: i) *normal* and *abnormal*, and ii) *presence of murmurs*, *absence
    of murmurs*, and *unclear cases of murmurs*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Published datasets for heart sound classification. AS: aortic stenosis,
    MR: mitral regurgitation, MS: mitral stenosis, MVP: mitral valve prolapse. Notably,
    the statistics in this table considered access available data sets only.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Challenge | #Samples | Duration (h) | #Subjects | Task |'
  prefs: []
  type: TYPE_TB
- en: '| PASCAL Database [[142](#bib.bib142)] | PASCAL Challenge [[142](#bib.bib142)]
    | 176 | 0.393 | unknown | Dataset A: Normal, Murmur, Extra Heart Sound, Artifact
    |'
  prefs: []
  type: TYPE_TB
- en: '| 656 | 1.194 | unknown | Dataset B: Normal, Murmur, Extrasystole |'
  prefs: []
  type: TYPE_TB
- en: '| PhysioNet/CinC Database [[58](#bib.bib58)] | PhysioNet/CinC Challenge 2016 [[143](#bib.bib143)]
    | 3,240 | 20.216 | 764+ | Normal, Abnormal, Too noisy or ambiguous |'
  prefs: []
  type: TYPE_TB
- en: '| HSS [[92](#bib.bib92)] | ComParE Challenge 2018 [[82](#bib.bib82)] | 845
    | 7.047 | 170 | Normal, Mild, Moderate/Severe |'
  prefs: []
  type: TYPE_TB
- en: '| Data on GitHub[[144](#bib.bib144)] | – | 1,000 | 0.679 | unknown | Normal,
    AS, MR, MS, MVP |'
  prefs: []
  type: TYPE_TB
- en: '| Michigan Heart sound database⁵⁵5[https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library](https://open.umich.edu/find/open-educational-resources/medical/heart-sound-murmur-library)[https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/primer_heartsound.html](https://www.med.umich.edu/lrc/psb_open/html/repo/primer_heartsound/primer_heartsound.html)
    | – | 23 | 0.413 | unknown | Normal, Pathological |'
  prefs: []
  type: TYPE_TB
- en: '| CirCor DigiScope Database [[145](#bib.bib145)] | George B. Moody PhysioNet
    Challenge 2022 [[146](#bib.bib146)] | 3,163 | 20.094 | 942 | Normal, abnormal;
    presence, absence, or unclear cases of murmurs |'
  prefs: []
  type: TYPE_TB
- en: 'Table 4: Evaluation metrics. $k$ is the $k$-th class of all $K$ classes; $N_{k}$,
    $\hat{N_{k}}$, and $\tilde{N_{k}}$ are the total number of samples, the total
    number of the predicted samples, and the number of correctly predicted samples
    for the $k$-th class, respectively. $N$ is the total number of all samples. TPR:
    true positive rate, FPR: false positive rate, AUC: area under curve, ROC: receiver
    operating characteristic, PR: precision-recall.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Evaluation Metric | Description | Formula |'
  prefs: []
  type: TYPE_TB
- en: '| Sensitivity | Proportion of the actual positive cases which are correctly
    classified, i. e., true positive rate (TPR) | $\frac{\mbox{True Positives}}{\mbox{True
    Positives}+\mbox{False Negatives}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Specificity | Proportion of the actual negative cases which are correctly
    classified, i. e., true negative rate (TNR) | $\frac{\mbox{True Negatives}}{\mbox{True
    Negatives}+\mbox{False Positives}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Mean Accuracy | Arithmetic mean of sensitivity and specificity | $\frac{\mbox{sensitivity}+\mbox{specificity}}{2}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Youden’s index | A measure of the ability to balance sensitivity and specificity
    | $\mbox{sensitivity}+\mbox{specificity}-1$ |'
  prefs: []
  type: TYPE_TB
- en: '| Precision | Proportion of correctly classified cases based on all cases predicted
    into $k$ | $\frac{\tilde{N_{k}}}{\hat{N_{k}}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| Recall | Proportion of correctly classified cases based on all cases in the
    class $k$. | $\frac{\tilde{N_{k}}}{N_{k}}$ |'
  prefs: []
  type: TYPE_TB
- en: '| UAR | Unweighted average recall | $\frac{\sum_{k=1}^{K}\mbox{recall}_{k}}{K}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| WAR | Weighted average recall, i. e., accuracy | $\sum_{k=1}^{K}\frac{N_{k}}{N}\mbox{recall}_{k}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| F1-Score | Harmonic mean of precision and recall | $2\times\frac{\mbox{precision}\times\mbox{recall}}{\mbox{precision}+\mbox{recall}}$
    |'
  prefs: []
  type: TYPE_TB
- en: '| AUC-ROC | The area under the probability curve plotting TPR against FPR at
    various probability thresholds | —— |'
  prefs: []
  type: TYPE_TB
- en: '| AUC-PR | The area under the probability curve plotting precision against
    recall at various probability thresholds | —— |'
  prefs: []
  type: TYPE_TB
- en: '| Average precision | Summarisation of a precision-recall curve as the weighted
    mean of precisions at each threshold | $\sum_{n}(\mbox{recall}_{n}-\mbox{recall}_{n-1})\times\mbox{precision}_{n}$
    |'
  prefs: []
  type: TYPE_TB
- en: 5.2 Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '[Table 4](#S5.T4 "Table 4 ‣ 5.1 Published Datasets ‣ 5 Published Resources
    on Heart Sound Analysis ‣ A Comprehensive Survey on Heart Sound Analysis in the
    Deep Learning Era") summarises evaluation metrics that can be used for evaluating
    model performance on heart sound classification.'
  prefs: []
  type: TYPE_NORMAL
- en: Class-wise evaluation metrics. In binary classification tasks, sensitivity and
    specificity focus on calculating the proportion of correctly classified positive
    samples and negative samples, respectively. In order to balance sensitivity and
    specificity, mean accuracy and Youden’s index are utilised. As for evaluating
    a model’s performance on a specific class in a multi-class classification task,
    precision and recall are frequently used. For a specific class, precision calculates
    the proportion of correctly classified cases based on all cases predicted to this
    class, whereas recall quantifies the proportion of correctly classified cases
    based on all cases in the class. Usually, there is an inverse relation between
    precision and recall. Therefore, their combinations (e. g., F1-Score, average
    precision) are referred to to better evaluate the performance of a model. Notably,
    in multi-class classification, recall is an extension of sensitivity.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class evaluation metrics. Besides the class-wise evaluation, there are
    other metrics across all classes, such as weighted average recall (WAR) and unweighted
    average recall (UAR). WAR, also known as accuracy and as the name implies, tends
    to overestimate a model if it performs better on the majority classes. Therefore,
    with an imbalanced dataset, UAR is preferred. UAR is the mean recall across classes
    w/o instance-based weighting. With various probability decision thresholds, we
    can also plot AUC-ROC and precision-recall curves. In addition, we would like
    to mention the cost-based metric devised in the PhysioNet Challenge 2022 [[146](#bib.bib146)],
    where cost calculation involves model pre-screening, expert screening, treatment,
    and diagnostic errors. In this way, more clinically practical models can be developed,
    especially in resource-constrained scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Published Algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We discuss publicly available codes in the papers discussed in Section [4.2](#S4.SS2
    "4.2 Deep Learning for Classification ‣ 4 State-of-the-art Studies ‣ A Comprehensive
    Survey on Heart Sound Analysis in the Deep Learning Era") as well as other papers
    on open-access repositories during 2017–2022\. There are only a few publicly available
    codes in the past years, while there are many more released codes in 2016 benefiting
    from the PhysioNet challenge 2016 [[58](#bib.bib58)]. Notably, codes in 2016 are
    not listed, as we want to list the most state-of-the-art studies in the past six
    years.
  prefs: []
  type: TYPE_NORMAL
- en: In [[144](#bib.bib144)], the authors released a Matlab code⁶⁶6https://github.com/yaseen21khan/Classification-of-Heart-Sound-Signal-Using-Multiple-Features-
    of training machine learning deep neural networks with inputs of multiple features,
    including MFCCs and discrete wavelets transform. In [[131](#bib.bib131)], a CNN
    model with time-convolutional units which simulate finite impulse response filters
    was implemented with Python in the code⁷⁷7https://github.com/mhealthbuet/heartnet.
    ResNets on linear and logarithmic spectrogram-image features were implemented
    in the Python code⁸⁸8https://github.com/mHealthBuet/CepsNET of the study [[147](#bib.bib147)].
    A study [[148](#bib.bib148)] released a Matlab code⁹⁹9https://github.com/uit-hdl/heart-sound-classification
    for detecting valvular heart disease from heart sounds and echocardiograms.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Future Research Directions and Open Issues
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 6.1 Findings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In classification tasks which mainly focus on the screening of heart diseases,
    segmentation was often considered as a preprocessing procedure before classifying
    heart sounds. It is still an open question whether segmentation is helpful for
    classification or not.
  prefs: []
  type: TYPE_NORMAL
- en: Segmentation + Classification. Many studies applied segmentation techniques
    or given segmentation information before the classification procedure. For instance,
    segmented cardiac cycles were used as the input of DL models in [[131](#bib.bib131)].
    Clips starting from the S1 heart sound with a fixed 1.6 s length were used for
    classification in [[129](#bib.bib129)]. In [[149](#bib.bib149)], the importance
    of segmentation was demonstrated for abnormal heart sound detection. Compared
    to models without segmentation information, segmentation did not significantly
    improve the model performance in the experiments [[149](#bib.bib149)]. The reason
    can be that models have already been powerful and robust, therefore, segmentation
    can be automatically done by the intermediate models layers. The authors in [[149](#bib.bib149)]
    proved this by explaining models with SHapley Additive exPlanations (SHAP) [[150](#bib.bib150)].
    The contribution of S1 and S2 sounds was found to be larger than other clips in
    a heart sound segment. Therefore, segmentation is required either as an additional
    procedure for classifiers that are not very powerful, or as an internal procedure
    in robust classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: No-segment. There are also many approaches which proposed to use non-segmented
    heart sounds, therefore, the complexity of automated auscultation can be reduced [[10](#bib.bib10),
    [52](#bib.bib52), [44](#bib.bib44), [90](#bib.bib90), [115](#bib.bib115)]. Apart
    from feeding a complete heart sound sample into neural networks, heart sounds
    are segmented into shorter clips with an equal length for model training [[50](#bib.bib50)].
    For instance, the fist 5 s of each audio sample were selected as the model input
    in [[7](#bib.bib7), [85](#bib.bib85), [93](#bib.bib93), [37](#bib.bib37)], and
    segmented 5 s clips were also used in [[91](#bib.bib91), [88](#bib.bib88), [116](#bib.bib116)].
    Most studies employ audio clips with a time length from 2 s to 6 s [[45](#bib.bib45),
    [34](#bib.bib34), [49](#bib.bib49), [51](#bib.bib51)].
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Limitations and Outlook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Hardware development. In clinics, echocardiography obtains ultrasound scan with
    a small probe which emits high-frequency sound waves, therefore, physicians can
    diagnose via observing the heart and blood vessels, as well as blood flows^(10)^(10)10https://www.nhs.uk/conditions/echocardiogram/.
    However, echocardiography requires well-trained skills for professionals, limiting
    its usage in primary care. Classic acoustic stethoscopes used in primary care
    need physicians and nurses to be trained as well. To this end, there is a high
    demand of electronic stethoscopes in primary care. In recent years, electronic
    stethoscopes have been developed and produced to record heart sounds and transmit
    heart sounds to computers or mobile phones for further analysis [[26](#bib.bib26)].
    Most electronic stethoscopes can only achieve basic functions such as amplifying
    and visualising heart sounds without diagnosis. There are a few studies and hardware
    working on automated diagnosis more recently. For instance, a field-programmable
    gate array (FPGA) was designed to classify heart sounds via an LSTM-RNN model
    in [[151](#bib.bib151)]. “HD Steth with ECG”^(11)^(11)11https://www.stethoscope.com/hd-steth-with-ecg/
    embedded artificial intelligence (AI) into the electronic stethoscope to detect
    multiple cardiac abnormalities. As outlined throughout this overview, AI is promising
    to diagnose heart sound abnormalities, therefore reducing the requirement of well-trained
    professionals. Devices which diagnose cardiac diseases with high accuracies will
    be very helpful for promoting the early screening of cardiac diseases to primary
    care and home care.
  prefs: []
  type: TYPE_NORMAL
- en: Performance improvement. Although automated auscultation is ideally expected
    to replace human analysis, model performance can be a bottleneck for applying
    automated auscultation to clinical usages. False negative predictions can result
    in delayed or missed therapies and aggravated condition. In future efforts, i)
    automated auscultation will be required to achieve very good performance and consider
    the difference between individuals in the context of personalised healthcare.
    The current research studies are mostly based on heart sounds only, while many
    types of individual information such as age can affect model performance [[152](#bib.bib152)].
    Therefore, how to integrate individual information will be a question for performance
    improvement. ii) In terms of ML and DL, we are currently witnessing the advent
    and adoption of foundation models [[153](#bib.bib153)] (pre-)trained on large
    to big data. We have already seen and listed above several approaches using pre-trained
    often self-supervised learnt models. However, one can expect even bigger models
    to appear with the potential emergence of abilities directly related to heart
    sound analysis as a ‘downstream’ task. On the other hand, the upcoming era of
    foundation models is expected to be marked by homogenisation, and it remains to
    be seen if the diversity of heart sound analysis approaches reported herein will
    indeed converge to a few large data trained models over the next years [[153](#bib.bib153)].
    iii) Furthermore, human-machine collaboration will be very promising to improve
    the system performance and provide more accurate diagnosis and in-time treatments
    for patients. Human-machine classification can be a solution to combine both machines’
    predictions and human’s (crowd workers’ and experts’) predictions for more precise
    diagnosis [[154](#bib.bib154)]. In [[154](#bib.bib154)], data samples predicted
    with high uncertainties were sent to crowd workers for majority voting; similarly,
    samples will be sent to an expert according to a certainty threshold of predictions
    from the crowd.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Interpretable, Dependable, and Actionable Deep Heart Sound Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Explainable deep learning has been emerging as one of the key topics in many
    applications, especially in the healthcare area. Building explainable deep learning
    models can help physicians and patients to trust the predictions. In a past study [[155](#bib.bib155)],
    an attention mechanism was used to visualise the contribution of each feature
    unit to the final predictions of heart sounds. Similarly, SHAP was also used to
    compute the contribution of each feature unit in [[150](#bib.bib150)]. The above
    explanation methods are both local explanations that interpret DL models case-by-case,
    lacking of a global explanation that can reveal the hidden classification rule
    in the models or summarise the characteristics of each heart sound class. Explainable
    DL models, such as deep neural decision trees [[156](#bib.bib156)], are promising
    to explain the models themselves from the perspectives of structure. Learnt or
    searched data samples of prototypes, criticisms, and counterfactuals [[106](#bib.bib106),
    [157](#bib.bib157)] can present the typical characteristics of each class, therefore,
    physicians can compare new samples with these heart sounds for better understanding
    and analysis. Specifically, by analysing the patterns of criticisms, physicians
    can potentially decrease the number of false negatives, which is crucial in the
    healthcare field. More recently, sonification was proposed to explain DL models
    for better human-computer interaction in [[158](#bib.bib158)]. Apart from visualisation,
    sonification can provide a new perspective to explain models by listening.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, given the health implications, it appears crucial that heart sound
    analysis by AI is utmost dependable [[159](#bib.bib159)]. There are mechanisms
    available, but more adaptation to the field of application if not novel algorithms
    will need to be designed. Ultimately, dependability will be a major driving factor
    for the trustability of according heart monitoring solutions – applied in everyday
    situations, trustability is a key factor to win users.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, to enable DL models being actionable in real-life, data privacy has
    been another emerging research topic in protecting users’ data from leakage or
    external attacks. Machine unlearning [[160](#bib.bib160)] and federated learning
    methods [[161](#bib.bib161)] can help healthcare institutions better organise
    patients’ private data in a secure way without losing diagnosis accuracy. Further,
    AI attacks on AI for heart sound analysis could be thought off, such as by adversarial
    attacks, and needs to dealt with. In summary, DL models are promising to guide
    healthcare providers’ actions in their daily practices for providing better care
    for patients. It will be essential to improve DL models from not only the performance,
    but also the human-centred perspectives in future.
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this work, we summarised both classic machine learning and deep learning
    technologies applied to heart sound analysis during 2017–2022, including denoising,
    segmentation, and classification. Databases available for these above tasks were
    introduced with evaluation metrics in our study. We also listed publicly released
    codes for implementation of heart sound classification. Additionally, several
    findings and limitations of heart sound classification were analysed and possible
    future works were discussed. Finally, we discussed the importance of heart sound
    interpretation in the context of deep learning. This work is expected to present
    a summary of the advances of heart sound analysis, provide helpful discussions,
    and point out promising research directions that are helpful for the community.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] U. Alam, O. Asghar, S. Q. Khan, S. Hayat, and R. A. Malik, “Cardiac auscultation:
    an essential clinical skill in decline,” British Journal of Cardiology, vol. 17,
    no. 1, p. 8, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] I. R. Hanna and M. E. Silverman, “A history of cardiac auscultation and
    some of its contributors,” The American journal of cardiology, vol. 90, no. 3,
    pp. 259–267, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. M. Noor and M. F. Shadi, “The heart auscultation. from sound to graphical,”
    Journal of Engineering and Technology, vol. 4, no. 2, pp. 73–84, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] H. Tang, M. Wang, Y. Hu, B. Guo, and T. Li, “Automated signal quality assessment
    for heart sound signal by novel features and evaluation in open public datasets,”
    BioMed Research International, vol. 30, pp. 1–15, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] L. R. Rabiner, “A tutorial on hidden markov models and selected applications
    in speech recognition,” Proceedings of the IEEE, vol. 77, no. 2, pp. 257–286,
    1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] B. M. Whitaker, P. B. Suresha, C. Liu, G. D. Clifford, and D. V. Anderson,
    “Combining sparse coding and time-domain features for heart sound classification,”
    Physiological measurement, vol. 38, no. 8, p. 1701, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] P. Langley and A. Murray, “Heart sound classification from unsegmented
    phonocardiograms,” Physiological measurement, vol. 38, no. 8, p. 1658, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] Z. Ren, N. Cummins, V. Pandit, J. Han, K. Qian, and B. Schuller, “Learning
    image-based representations for heart sound classification,” in Proc. DH, (Lyon,
    France), pp. 143–147, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] T. H. Chowdhury, K. N. Poudel, and Y. Hu, “Time-frequency analysis, denoising,
    compression, segmentation, and classification of pcg signals,” IEEE Access, vol. 8,
    pp. 160882–160890, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] B. Xiao, Y. Xu, X. Bi, J. Zhang, and X. Ma, “Heart sounds classification
    using a novel 1-D convolutional neural network with extremely low parameter consumption,”
    Neurocomputing, vol. 392, pp. 153–159, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. K. Bhoi, K. S. Sherpa, and B. Khandelwal, “Multidimensional analytical
    study of heart sounds: A review,” International Journal Bioautomation, vol. 19,
    no. 3, pp. 351–376, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] T. Chakrabarti, S. Saha, S. Roy, and I. Chel, “Phonocardiogram signal
    analysis-practices, trends and challenges: A critical review,” in Proc. IEMCON,
    (Vancouver, Canada), pp. 1–4, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. Nabih-Ali, E.-S. A. El-Dahshan, and A. S. Yahia, “A review of intelligent
    systems for heart sound signal analysis,” Journal of medical engineering & technology,
    vol. 41, no. 7, pp. 553–563, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] G. D. Clifford et al., “Recent advances in heart sound analysis,” Physiological
    measurement, vol. 38, pp. E10–E25, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] S. K. Ghosh, P. R. Nagarajan, and R. K. Tripathy, Heart sound data acquisition
    and preprocessing techniques: A review. IGI Global, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] B. Majhi and A. Kashyap, “Application of soft computing techniques to
    heart sound classification: A review of the decade,” Soft Computing Applications
    and Techniques in Healthcare, pp. 113–138, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. K. Dwivedi, S. A. Imtiaz, and E. Rodriguez-Villegas, “Algorithms for
    automatic analysis and classification of heart sounds–a systematic review,” IEEE
    Access, vol. 7, pp. 8316–8345, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. J. Muñoz-Montoro, D. Suarez-Dou, R. Cortina, F. J. Cañadas-Quesada,
    and E. F. Combarro, “Parallel source separation system for heart and lung sounds,”
    The Journal of Supercomputing, vol. 77, no. 8, pp. 8135–8150, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] K.-H. Tsai, W.-C. Wang, C.-H. Cheng, C.-Y. Tsai, J.-K. Wang, T.-H. Lin,
    S.-H. Fang, L.-C. Chen, and Y. Tsao, “Blind monaural source separation on heart
    and lung sounds based on periodic-coded deep autoencoder,” IEEE Journal of Biomedical
    and Health Informatics, vol. 24, no. 11, pp. 3203–3214, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] E. Grooby, J. He, D. Fattahi, L. Zhou, A. King, A. Ramanathan, A. Malhotra,
    G. A. Dumont, and F. Marzbanrad, “A new non-negative matrix co-factorisation approach
    for noisy neonatal chest sound separation,” in 2021 43rd Annual International
    Conference of the IEEE Engineering in Medicine & Biology Society (EMBC), pp. 5668–5673,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] C. Ahlström, Processing of the Phonocardiographic Signal: methods for
    the intelligent stethoscope. PhD thesis, Institutionen för medicinsk teknik, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] H. Naseri and M. Homaeinezhad, “Detection and boundary identification
    of phonocardiogram sounds using an expert frequency-energy based metric,” Annals
    of biomedical engineering, vol. 41, no. 2, pp. 279–292, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] D. S. Gerbarg, A. Taranta, M. Spagnuolo, and J. J. Hofler, “Computer analysis
    of phonocardiograms,” Progress in Cardiovascular Diseases, vol. 5, pp. 393–405,
    1963.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] R. J. Martis, U. R. Acharya, and H. Adeli, “Current methods in electrocardiogram
    characterization,” Computers in biology and medicine, vol. 48, pp. 133–149, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] P. S. Molcer, I. Kecskes, V. Delić, E. Domijan, and M. Domijan, “Examination
    of formant frequencies for further classification of heart murmurs,” in Proc. SISY,
    (Subotica, Serbia), pp. 575–578, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Leng, R. San Tan, K. T. C. Chai, C. Wang, D. Ghista, and L. Zhong,
    “The electronic stethoscope,” Biomedical engineering online, vol. 14, no. 1, pp. 1–37,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] E. Delgado-Trejos, A. Quiceno-Manrique, J. Godino-Llorente, M. Blanco-Velasco,
    and G. Castellanos-Dominguez, “Digital auscultation analysis for heart murmur
    detection,” Annals of biomedical engineering, vol. 37, no. 2, pp. 337–353, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Tsao, T.-H. Lin, F. Chen, Y.-F. Chang, C.-H. Cheng, and K.-H. Tsai,
    “Robust s1 and s2 heart sound recognition based on spectral restoration and multi-style
    training,” Biomedical Signal Processing and Control, vol. 49, pp. 173–180, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] O. Deperlioglu, “Classification of segmented phonocardiograms by convolutional
    neural networks,” BRAIN. Broad Research in Artificial Intelligence and Neuroscience,
    vol. 10, no. 2, pp. 5–13, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Ö. DEPERLİĞLU, “Classification of segmented heart sounds with artificial
    neural networks,” International Journal of Applied Mathematics Electronics and
    Computers, vol. 6, no. 4, pp. 39–44, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] H. Alaskar, N. Alzhrani, A. Hussain, and F. Almarshed, “The implementation
    of pretrained AlexNet on PCG classification,” in Proc. ICIC, (San Sebastian, Basque
    Country), pp. 784–794, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] N. M. Khan, M. S. Khan, and G. M. Khan, “Automated heart sound classification
    from unsegmented phonocardiogram signals using time frequency features,” International
    Journal of Computer and Information Engineering, vol. 12, no. 8, pp. 598–603,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] A. Yadav, M. K. Dutta, C. M. Travieso, and J. B. Alonso, “Automatic classification
    of normal and abnormal PCG recording heart sound recording using Fourier transform,”
    in Proc. IWOBI, (Alajuela Province, Costa Rica), pp. 1–9, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Q. Hu, J. Hu, X. Yu, and Y. Liu, “Automatic heart sound classification
    using one dimension deep neural network,” in International Conference on Security,
    Privacy and Anonymity in Computation, Communication and Storage, pp. 200–208,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] S. K. Ghosh, R. K. Tripathy, R. Ponnalagu, and R. B. Pachori, “Automated
    detection of heart valve disorders from the PCG signal using time-frequency magnitude
    and phase features,” IEEE Sensors Letters, vol. 3, no. 12, pp. 1–4, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] B. Ahmad, F. A. Khan, K. N. Khan, and M. S. Khan, “Automatic classification
    of heart sounds using long short-term memory,” in Proc. ICOSST, (Virtual Event),
    pp. 1–6, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] S. A. Singh, T. G. Meitei, and S. Majumder, “Short pcg classification
    based on deep learning,” in Deep Learning Techniques for Biomedical and Health
    Informatics, pp. 141–164, Elsevier, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] F. Noman, S.-H. Salleh, C.-M. Ting, S. B. Samdin, H. Ombao, and H. Hussain,
    “A markov-switching model approach to heart sound segmentation and classification,”
    IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 3, pp. 705–716,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] F. Noman, C.-M. Ting, S.-H. Salleh, and H. Ombao, “Short-segment heart
    sound classification using an ensemble of deep convolutional neural networks,”
    in Proc. ICASSP, (Brighton, UK), pp. 1318–1322, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] J. Dastagir, F. A. Khan, M. S. Khan, and K. N. Khan, “Computer-aided phonocardiogram
    classification using multidomain time and frequency features,” in Proc. ICAI,
    (Islamabad, Pakistan), pp. 50–55, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] A. Bourouhou, A. Jilbab, C. Nacir, and A. Hammouch, “Heart sound signals
    segmentation and multiclass classification,” International Journal of Online and
    Biomedical Engineering, vol. 16, no. 15, pp. 64–79, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Meintjes, A. Lowe, and M. Legget, “Fundamental heart sound classification
    using the continuous wavelet transform and convolutional neural networks,” in
    Proc. EMBC, (Honolulu, Hawaii), pp. 409–412, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Z. Abduh, E. A. Nehary, M. A. Wahed, and Y. M. Kadah, “Classification
    of heart sounds using fractional fourier transform based mel-frequency spectral
    coefficients and traditional classifiers,” Biomedical Signal Processing and Control,
    vol. 57, p. 101788, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] S. A. Singh, S. Majumder, and M. Mishra, “Classification of short unsegmented
    heart sound based on deep learning,” in Proc. I2MTC, (Auckland, New Zealand),
    pp. 1–6, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] S. Li, F. Li, S. Tang, and F. Luo, “Heart sounds classification based
    on feature fusion using lightweight neural networks,” IEEE Transactions on Instrumentation
    and Measurement, vol. 70, pp. 1–9, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] N. Ibrahim, N. Jamal, M. N. A.-H. Sha’abani, and L. F. Mahadi, “A comparative
    study of heart sound signal classification based on temporal, spectral and geometric
    features,” in Proc. IECBES, (Langkawi, Malaysia), pp. 24–29, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] W. Zhang, J. Han, and S. Deng, “Heart sound classification based on scaled
    spectrogram and tensor decomposition,” Expert Systems with Applications, vol. 84,
    pp. 220–231, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] W. Zhang, J. Han, and S. Deng, “Heart sound classification based on scaled
    spectrogram and partial least squares regression,” Biomedical Signal Processing
    and Control, vol. 32, pp. 20–28, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Banerjee and S. Majhi, “Multi-class heart sounds classification using
    2D-convolutional neural network,” in Proc. ICCCS, (Guangzhou, China), pp. 1–6,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] J. M.-T. Wu, M.-H. Tsai, Y. Z. Huang, S. H. Islam, M. M. Hassan, A. Alelaiwi,
    and G. Fortino, “Applying an ensemble convolutional neural network with Savitzky–Golay
    filter to construct a phonocardiogram prediction model,” Applied Soft Computing,
    vol. 78, pp. 29–40, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] P. T. Krishnan, P. Balasubramanian, and S. Umapathy, “Automated heart
    sound classification system from unsegmented phonocardiogram (pcg) using deep
    neural network,” Physical and Engineering Sciences in Medicine, vol. 43, no. 2,
    pp. 505–515, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] W. Zeng, J. Yuan, C. Yuan, Q. Wang, F. Liu, and Y. Wang, “A new approach
    for the detection of abnormal heart sound signals using TQWT, VMD and neural networks,”
    Artificial Intelligence Review, vol. 54, no. 3, pp. 1613–1647, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] M. V. Shervegar and G. V. Bhat, “Heart sound classification using gaussian
    mixture model,” Porto Biomedical Journal, vol. 3, no. 1, pp. 1–7, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] B. Al-Naami, H. Fraihat, N. Y. Gharaibeh, and A.-R. M. Al-Hinnawi, “A
    framework classification of heart sound signals in physionet challenge 2016 using
    high order statistics and adaptive neuro-fuzzy inference system,” IEEE Access,
    vol. 8, pp. 224852–224859, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. I. Humayun, M. Khan, S. Ghaffarzadegan, Z. Feng, T. Hasan, et al.,
    “An ensemble of transfer, semi-supervised and supervised learning methods for
    pathological heart sound classification,” in Proc. INTERSPEECH, (Hyderabad, India),
    pp. 127–131, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] S. Boll, “Suppression of acoustic noise in speech using spectral subtraction,”
    IEEE Transactions on acoustics, speech, and signal processing, vol. 27, no. 2,
    pp. 113–120, 1979.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] V. N. Varghees and K. Ramachandran, “Effective heart sound segmentation
    and murmur classification using empirical wavelet transform and instantaneous
    phase for electronic stethoscope,” IEEE Sensors Journal, vol. 17, no. 12, pp. 3861–3872,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] C. Liu, D. Springer, et al., “An open access database for the evaluation
    of heart sound algorithms,” Physiological Measurement, vol. 37, no. 12, p. 2181,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] M. Baydoun, L. Safatly, H. Ghaziri, and A. El Hajj, “Analysis of heart
    sound anomalies using ensemble learning,” Biomedical Signal Processing and Control,
    vol. 62, p. 102019, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] P. Upretee and M. E. Yüksel, “Accurate classification of heart sounds
    for disease diagnosis by a single time-varying spectral feature: Preliminary results,”
    in Proc. EBBT, (Istanbul, Turkey), pp. 1–4, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] J. X. Low and K. W. Choo, “Classification of heart sounds using softmax
    regression and convolutional neural network,” in Proc. ICCET, (New York, NY),
    pp. 18–21, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] O. Deperlioglu, “Heart sound classification with signal instant energy
    and stacked autoencoder network,” Biomedical Signal Processing and Control, vol. 64,
    p. 102211, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] G. Eslamizadeh and R. Barati, “Heart murmur detection based on wavelet
    transformation and a synergy between artificial neural network and modified neighbor
    annealing methods,” Artificial intelligence in medicine, vol. 78, pp. 23–40, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] M. U. Akram, A. Shaukat, F. Hussain, S. G. Khawaja, W. H. Butt, et al.,
    “Analysis of PCG signals using quality assessment and homomorphic filters for
    localization and classification of heart sounds,” Computer methods and programs
    in biomedicine, vol. 164, pp. 143–157, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] M. V. Shervegar and G. V. Bhat, “Automatic segmentation of phonocardiogram
    using the occurrence of the cardiac events,” Informatics in Medicine Unlocked,
    vol. 9, pp. 6–10, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] S. Das, S. Pal, and M. Mitra, “Acoustic feature based unsupervised approach
    of heart sound event detection,” Computers in Biology and Medicine, vol. 126,
    p. 103990, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] S. E. Schmidt, C. Holst-Hansen, C. Graff, E. Toft, and J. J. Struijk,
    “Segmentation of heart sound recordings by a duration-dependent hidden markov
    model,” Physiological measurement, vol. 31, no. 4, p. 513, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] S. Shukla, S. K. Singh, and D. Mitra, “An efficient heart sound segmentation
    approach using kurtosis and zero frequency filter features,” Biomedical Signal
    Processing and Control, vol. 57, p. 101762, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. P. Kamson, L. Sharma, and S. Dandapat, “Multi-centroid diastolic duration
    distribution based HSMM for heart sound segmentation,” Biomedical Signal Processing
    and Control, vol. 48, pp. 265–272, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] J. Oliveira, F. Renna, and M. Coimbra, “A subject-driven unsupervised
    hidden semi-Markov model and Gaussian mixture model for heart sound segmentation,”
    IEEE Journal of Selected Topics in Signal Processing, vol. 13, no. 2, pp. 323–331,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] J. Oliveira, F. Renna, T. Mantadelis, and M. Coimbra, “Adaptive sojourn
    time HSMM for heart sound segmentation,” IEEE Journal of Biomedical and Health
    Informatics, vol. 23, no. 2, pp. 642–649, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] J. Rubin, R. Abreu, A. Ganguli, S. Nelaturi, I. Matei, and K. Sricharan,
    “Recognizing abnormal heart sounds using deep learning,” arXiv preprint arXiv:1707.04642,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] V. Maknickas and A. Maknickas, “Recognition of normal–abnormal phonocardiographic
    signals using deep convolutional neural networks and mel-frequency spectral coefficients,”
    Physiological measurement, vol. 38, no. 8, p. 1671, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] K. Liu, L. Yuan, C. Huang, W. Wu, Q. Wang, and G. Wu, “Abnormal heart
    sound detection by using temporal convolutional network,” in Proc. IPEC, (Potsdam,
    Germany), pp. 1026–1029, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] F. A. Khan, A. Abid, and M. S. Khan, “Automatic heart sound classification
    from segmented/unsegmented phonocardiogram signals using time and frequency features,”
    Physiological measurement, vol. 41, no. 5, p. 055006, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Y. Chen, S. Wei, and Y. Zhang, “Classification of heart sounds based on
    the combination of the modified frequency wavelet transform and convolutional
    neural network,” Medical & Biological Engineering & Computing, vol. 58, no. 9,
    pp. 2039–2047, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] W. Han, Z. Yang, J. Lu, and S. Xie, “Supervised threshold-based heart
    sound classification algorithm,” Physiological Measurement, vol. 39, no. 11, p. 115011,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] W. Han, S. Xie, Z. Yang, S. Zhou, and H. Huang, “Heart sound classification
    using the SNMFNet classifier,” Physiological measurement, vol. 40, no. 10, p. 105003,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] J. F. Chen and X. Dang, “Heart sound analysis based on extended features
    and related factors,” in Proc. SSCI, (Xiamen, China), pp. 2189–2194, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] D. B. Springer, L. Tarassenko, and G. D. Clifford, “Logistic regression-hsmm-based
    heart sound segmentation,” IEEE transactions on biomedical engineering, vol. 63,
    no. 4, pp. 822–832, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] A. Duggento, A. Conti, M. Guerrisi, and N. Toschi, “Classification of
    real-world pathological phonocardiograms through multi-instance learning,” in
    Proc. EMBC, (Virtual Event), pp. 771–774, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] B. Schuller et al., “The INTERSPEECH 2018 computational paralinguistics
    challenge: Atypical & self-assessed affect, crying & heart beats,” in Proc. INTERSPEECH,
    (Hyderbad, India), pp. 122–126, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] F. Eyben, K. R. Scherer, B. W. Schuller, et al., “The geneva minimalistic
    acoustic parameter set (gemaps) for voice research and affective computing,” IEEE
    transactions on affective computing, vol. 7, no. 2, pp. 190–202, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] F. Eyben, M. Wöllmer, and B. Schuller, “Opensmile: The munich versatile
    and fast open-source audio feature extractor,” in Proc. ACM Multimedia, (Firenze,
    Italy), pp. 1459–1462, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] S. A. Singh and S. Majumder, “Classification of unsegmented heart sound
    recording using KNN classifier,” Journal of Mechanics in Medicine and Biology,
    vol. 19, no. 04, p. 1950025, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] E. Soares, P. Angelov, and X. Gu, “Autonomous learning multiple-model
    zero-order classifier for heart sound classification,” Applied Soft Computing,
    vol. 94, p. 106449, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] S. Khaled, M. Fakhry, H. Esmail, A. Ezzat, and E. Hamad, “Analysis of
    training optimisation algorithms in the NARX neural network for classification
    of heart sound signals,” International Journal of Scientific & Engineering Research,
    vol. 13, no. 2, pp. 382–390, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] V. Arora, R. Leekha, R. Singh, and I. Chana, “Heart sound classification
    using machine learning and phonocardiogram,” Modern Physics Letters B, vol. 33,
    no. 26, p. 1950321, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] M. Sotaquirá, D. Alvear, and M. Mondragón, “Phonocardiogram classification
    using deep neural networks and weighted probability comparisons,” Journal of medical
    engineering & technology, vol. 42, no. 7, pp. 510–517, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. M. Alqudah, “Towards classifying non-segmented heart sound records
    using instantaneous frequency based features,” Journal of medical engineering
    & technology, vol. 43, no. 7, pp. 418–430, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Y. Tan, Z. Wang, K. Qian, B. Hu, S. Zhao, B. W. Schuller, and Y. Yamamoto,
    “Heart sound classification based on fractional fourier transformation entropy,”
    in Proc. LifeTech, (Osaka, Japan), pp. 588–589, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] F. Dong, K. Qian, Z. Ren, A. Baird, X. Li, Z. Dai, B. Dong, F. Metze,
    Y. Yamamoto, and B. Schuller, “Machine listening for heart status monitoring:
    Introducing and benchmarking HSS – the heart sounds shenzhen corpus,” IEEE Journal
    of Biomedical and Health Informatics, vol. 24, pp. 2082–2092, Nov. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Chen, X. Dang, and M. Li, “Heart sound classification method based
    on ensemble learning,” in Proc. ICSP, (Beijing, China), pp. 8–13, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] M. Rahmandani, H. A. Nugroho, and N. A. Setiawan, “Cardiac sound classification
    using Mel-frequency cepstral coefficients (MFCC) and artificial neural network
    (ANN),” in Proc. ICITISEE, (Yogyakarta, Indonesia), pp. 22–26, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] M. Adiban, B. BabaAli, and S. Shehnepoor, “Statistical feature embedding
    for heart sound classification,” Journal of Electrical Engineering, vol. 70, no. 4,
    pp. 259–272, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Li, L. Ke, Q. Du, X. Ding, X. Chen, and D. Wang, “Heart sound signal
    classification algorithm: A combination of wavelet scattering transform and twin
    support vector machine,” IEEE Access, vol. 7, pp. 179339–179348, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] N. Mei, H. Wang, Y. Zhang, F. Liu, X. Jiang, and S. Wei, “Classification
    of heart sounds based on quality assessment and wavelet scattering transform,”
    Computers in Biology and Medicine, vol. 137, p. 104814, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] N. K. Sawant, S. Patidar, N. Nesaragi, and U. R. Acharya, “Automated detection
    of abnormal heart sound signals using Fano-factor constrained tunable quality
    wavelet transform,” Biocybernetics and Biomedical Engineering, vol. 41, no. 1,
    pp. 111–126, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] T. Tuncer, S. Dogan, R.-S. Tan, and U. R. Acharya, “Application of Petersen
    graph pattern technique for automated detection of heart valve diseases with PCG
    signals,” Information Sciences, vol. 565, pp. 91–104, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] S. Amiriparian, M. Schmitt, N. Cummins, K. Qian, F. Dong, and B. Schuller,
    “Deep unsupervised representation learning for abnormal heart sound classification,”
    in Pro. EMBC, (Honolulu, Hawaii), pp. 4776–4779, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] M. E. Karar, S. H. El-Khafif, and M. A. El-Brawany, “Automated diagnosis
    of heart sounds using rule-based classification tree,” Journal of medical systems,
    vol. 41, no. 4, pp. 1–7, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] F. Plesinger, I. Viscor, J. Halamek, J. Jurco, and P. Jurak, “Heart sounds
    analysis using probability assessment,” Physiological measurement, vol. 38, no. 8,
    p. 1685, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] R. F. Ibarra-Hernández, N. Bertin, M. A. Alonso-Arévalo, and H. A. Guillén-Ramírez,
    “A benchmark of heart sound classification systems based on sparse decompositions,”
    in Proc. SIPAIM, vol. 10975, (Mazatlan, Mexico), pp. 26–38, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] A. Sofwan, I. Santoso, H. Pradipta, M. Arfan, et al., “Normal and murmur
    heart sound classification using linear predictive coding and k-nearest neighbor
    methods,” in Proc. ICICoS, (Semarang, Japan), pp. 1–5, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Z. Ren, J. Han, N. Cummins, and B. Schuller, “Enhancing transferability
    of black-box adversarial attacks via lifelong learning for speech emotion recognition
    models,” in Proc. INTERSPEECH, (Shanghai, China), pp. 496–500, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Z. Ren, T. T. Nguyen, and W. Nejdl, “Prototype learning for interpretable
    respiratory sound analysis,” in Proc. ICASSP, (Singapore), pp. 9087–9091, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] K. Qian, C. Janott, M. Schmitt, Z. Zhang, C. Heiser, W. Hemmert, Y. Yamamoto,
    and B. W. Schuller, “Can machine learning assist locating the excitation of snore
    sound? A review,” IEEE Journal of Biomedical and Health Informatics, vol. 25,
    no. 4, pp. 1233–1246, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] F. Renna, J. Oliveira, and M. T. Coimbra, “Deep convolutional neural
    networks for heart sound segmentation,” IEEE Journal of Biomedical and Health
    Informatics, vol. 23, no. 6, pp. 2435–2445, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] E. Messner, M. Zöhrer, and F. Pernkopf, “Heart sound segmentation—an
    event detection approach using deep recurrent neural networks,” IEEE Transactions
    on Biomedical Engineering, vol. 65, no. 9, pp. 1964–1974, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Y. Chen, J. Lv, Y. Sun, and B. Jia, “Heart sound segmentation via duration
    long–short term memory neural network,” Applied Soft Computing, vol. 95, p. 106540,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] T. Fan, J. Zhu, Y. Cheng, Q. Li, D. Xue, and R. Munnoch, “A new Direct
    heart sound segmentation approach using bi-directional GRU,” in Proc. ICAC, (Newcastle,
    UK), pp. 1–5, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] T. Fernando, H. Ghaemmaghami, S. Denman, S. Sridharan, N. Hussain, and
    C. Fookes, “Heart sound segmentation using bidirectional LSTMs with attention,”
    IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 6, pp. 1601–1609,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Y. Chen, Y. Sun, J. Lv, B. Jia, and X. Huang, “End-to-end heart sound
    segmentation using deep convolutional recurrent network,” Complex Intell. Syst.,
    vol. 7, p. 2103–2117, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] C. Xu, J. Zhou, L. Li, J. Wang, D. Ying, and Q. Li, “Heart sound segmentation
    based on SMGU-RNN,” in Proc. BIBE, (Hangzhou, China), pp. 126–132, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] S. Takezaki and K. Kishida, “Construction of cnns for abnormal heart
    sound detection using data augmentation,” in Proc. IMECS, (Hong Kong), pp. 1–6,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] X. Cheng, J. Huang, Y. Li, and G. Gui, “Design and application of a laconic
    heart sound neural network,” IEEE Access, vol. 7, pp. 124417–124425, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. M. Alqudah, H. Alquran, and I. A. Qasmieh, “Classification of heart
    sound short records using bispectrum analysis approach images and deep learning,”
    Network Modeling Analysis in Health Informatics and Bioinformatics, vol. 9, no. 66,
    pp. 1–16, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] M. S. Wibawa, I. M. D. Maysanjaya, N. K. D. P. Novianti, and P. N. Crisnapati,
    “Abnormal heart rhythm detection based on spectrogram of heart sound using convolutional
    neural network,” in Proc. CITSM, (Parapat Nort Sumatera, Indonesia), pp. 1–4,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] K. Ranipa, W.-P. Zhu, and M. Swamy, “Multimodal cnn fusion architecture
    with multi-features for heart sound classification,” in Proc. ISCAS, (Suseong-gu,
    Daegu), pp. 1–5, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] A. Balamurugan, S. G. Teo, J. Yang, Z. Peng, Y. Xulei, and Z. Zeng, “ResHNet:
    spectrograms based efficient heart sounds classification using stacked residual
    networks,” in Proc. BHI, (Chicago, IL), pp. 1–4, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] M. Deng, T. Meng, J. Cao, S. Wang, J. Zhang, and H. Fan, “Heart sound
    classification based on improved MFCC features and convolutional recurrent neural
    networks,” Neural Networks, vol. 130, pp. 22–32, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Z. Abduh, E. A. Nehary, M. A. Wahed, and Y. M. Kadah, “Classification
    of heart sounds using fractional fourier transform based mel-frequency spectral
    coefficients and stacked autoencoder deep neural network,” Journal of Medical
    Imaging and Health Informatics, vol. 9, no. 1, pp. 1–8, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] M. Fakhry and A. F. Brery, “A comparison study on training optimization
    algorithms in the bilstm neural network for classification of pcg signals,” in
    Proc. IRASET, (Meknes, Morocco), pp. 1–6, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] K. Qian, Z. Ren, F. Dong, W.-H. Lai, B. Schuller, and Y. Yoshiharu, “Deep
    wavelets for heart sound classification,” in Proc. ISPACS, (Taipei, Taiwan), 2019.
    2 pages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] W. Zhang, J. Han, and S. Deng, “Abnormal heart sound detection using
    temporal quasi-periodic features and long short-term memory without segmentation,”
    Biomedical Signal Processing and Control, vol. 53, p. 101560, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] F. Li, M. Liu, Y. Zhao, L. Kong, L. Dong, X. Liu, and M. Hui, “Feature
    extraction and classification of heart sound using 1d convolutional neural networks,”
    EURASIP Journal on Advances in Signal Processing, vol. 2019, no. 1, pp. 1–11,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] R. Avanzato and F. Beritelli, “Heart sound multiclass analysis based
    on raw data and convolutional neural network,” IEEE Sensors Letters, vol. 4, no. 12,
    pp. 1–4, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] S. L. Oh, V. Jahmunah, C. P. Ooi, R.-S. Tan, E. J. Ciaccio, T. Yamakawa,
    M. Tanabe, M. Kobayashi, and U. R. Acharya, “Classification of heart sound signals
    using a novel deep WaveNet model,” Computer Methods and Programs in Biomedicine,
    vol. 196, pp. 1–9, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] S. Gao, Y. Zheng, and X. Guo, “Gated recurrent unit-based heart sound
    analysis for heart failure screening,” Biomedical engineering online, vol. 19,
    no. 1, pp. 1–17, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] S. B. Shuvo, S. N. Ali, S. I. Swapnil, M. S. Al-Rakhami, and A. Gumaei,
    “CardioXNet: A novel lightweight deep learning framework for cardiovascular disease
    classification using heart sound recordings,” IEEE Access, vol. 9, pp. 36955–36967,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] A. I. Humayun, S. Ghaffarzadegan, M. I. Ansari, Z. Feng, and T. Hasan,
    “Towards domain invariant heart sound abnormality detection using learnable filterbanks,”
    IEEE journal of biomedical and health informatics, vol. 24, no. 8, pp. 2189–2198,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] A. I. Humayun, S. Ghaffarzadegan, Z. Feng, and T. Hasan, “Learning front-end
    filter-bank parameters using convolutional neural networks for abnormal heart
    sound detection,” in Proc. EMBC, (Honolulu, Hawaii), pp. 1408–1411, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “ImageNet:
    A large-scale hierarchical image database,” in Proc. CVPR, (Miami, FL), pp. 248–255,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] J. F. Gemmeke, D. P. Ellis, D. Freedman, A. Jansen, W. Lawrence, R. C.
    Moore, M. Plakal, and M. Ritter, “Audio set: An ontology and human-labeled dataset
    for audio events,” in Proc. ICASSP, (New Orleans), pp. 776–780, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” Communications of the ACM, vol. 60,
    no. 6, pp. 84–90, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” (San Diego, CA), pp. 1–14, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] F. Demir, A. Şengür, V. Bajaj, and K. Polat, “Towards the classification
    of heart sounds based on convolutional deep neural network,” Health information
    science and systems, vol. 7, no. 1, pp. 1–9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] T. Koike, K. Qian, Q. Kong, M. D. Plumbley, B. W. Schuller, and Y. Yamamoto,
    “Audio for audio is better? An investigation on transfer learning models for heart
    sound classification,” in Proc. EMBC, (Virtual Event), pp. 74–77, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] A. G. Howard, M. Zhu, B. Chen, D. Kalenichenko, W. Wang, T. Weyand, M. Andreetto,
    and H. Adam, “MobileNets: Efficient convolutional neural networks for mobile vision
    applications,” arXiv preprint arXiv:1704.04861, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in Proc. CVPR, (Las Vegas, NV), pp. 770–778, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] S. Xie, R. Girshick, P. Dollár, Z. Tu, and K. He, “Aggregated residual
    transformations for deep neural networks,” in Proc. CVPR, (Honolulu, Hawaii),
    pp. 1492–1500, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] P. Bentley, G. Nordehn, M. Coimbra, and S. Mannor, “The PASCAL Classifying
    Heart Sounds Challenge 2011 (CHSC2011) Results.” http://www.peterjbentley.com/heartchallenge/index.html.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] A. Goldberger, L. Amaral, L. Glass, J. Hausdorff, P. Ivanov, R. Mark,
    J. Mietus, G. Moody, C. Peng, and H. Stanley, “PhysioBank, PhysioToolkit, and
    PhysioNet: Components of a new research resource for complex physiologic signals,”
    2000.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] G.-Y. Son and S. Kwon, “Classification of heart sound signal using multiple
    features,” Applied Sciences, vol. 8, no. 12, p. 2344, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] J. Oliveira, F. Renna, P. D. Costa, M. Nogueira, C. Oliveira, C. Ferreira,
    A. Jorge, S. Mattos, T. Hatem, T. Tavares, et al., “The CirCor DigiScope dataset:
    From murmur detection to murmur classification,” IEEE journal of biomedical and
    health informatics, vol. 26, no. 6, pp. 2524–2535, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] M. A. Reyna et al., “Heart murmur detection from phonocardiogram recordings:
    The George B. Moody PhysioNet Challenge 2022,” medRxiv, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] F. B. Azam, M. Ansari, I. Mclane, T. Hasan, et al., “Heart sound classification
    considering additive noise and convolutional distortion,” arXiv preprint arXiv:2106.01865,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] P. N. Waaler, H. Melbye, et al., “Algorithm for predicting valvular heart
    disease from heart sounds in an unselected cohort,” medRxiv, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] T. Dissanayake, T. Fernando, S. Denman, S. Sridharan, H. Ghaemmaghami,
    and C. Fookes, “A robust interpretable deep learning classifier for heart anomaly
    detection without segmentation,” IEEE Journal of Biomedical and Health Informatics,
    vol. 25, no. 6, pp. 2162–2171, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. M. Lundberg and S.-I. Lee, “A unified approach to interpreting model
    predictions,” in Proc. NIPS, (Long Beach, CA), p. 4768–4777, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] W.-S. Jhong, S.-I. Chu, Y.-J. Huang, T.-Y. Hsu, W.-C. Lin, P. Huang,
    and J.-J. Wang, “Deep learning hardware/software co-design for heart sound classification,”
    in Proc. ISOCC, (Yeosu, Korea), pp. 27–28, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] W. R. Thompson, A. J. Reinisch, M. J. Unterberger, and A. J. Schriefl,
    “Artificial intelligence-assisted auscultation of heart murmurs: validation by
    virtual clinical trial,” Pediatric cardiology, vol. 40, no. 3, pp. 623–629, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] R. Bommasani, D. A. Hudson, E. Adeli, R. Altman, S. Arora, S. von Arx,
    M. S. Bernstein, J. Bohg, A. Bosselut, E. Brunskill, et al., “On the opportunities
    and risks of foundation models,” arXiv preprint arXiv:2108.07258, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] W. Callaghan, J. Goh, M. Mohareb, A. Lim, and E. Law, “Mechanicalheart:
    A human-machine framework for the classification of phonocardiograms,” Proceedings
    of the ACM on Human-Computer Interaction, vol. 2, no. CSCW, pp. 1–17, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] Z. Ren, K. Qian, F. Dong, Z. Dai, W. Nejdl, Y. Yamamoto, and B. W. Schuller,
    “Deep attention-based neural networks for explainable heart sound classification,”
    Machine Learning with Applications, p. 100322, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] Y. Yang, I. G. Morillo, and T. M. Hospedales, “Deep neural decision trees,”
    in Proc. ICML WHI, (Stockholm, Sweden), pp. 34–40, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Y. Chang, Z. Ren, T. T. Nguyen, W. Nejdl, and B. Schuller, “Example-based
    explanations with adversarial attacks for respiratory sound analysis,” in Proc. INTERPSEECH,
    (Incheon, Korea), pp. 4003–4007, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] B. W. Schuller, T. Virtanen, M. Riveiro, G. Rizos, J. Han, A. Mesaros,
    and K. Drossos, “Towards sonification in multimodal and user-friendly explainable
    artificial intelligence,” in Proc. ICMI, pp. 788–792, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] M. Pelillo and T. Scantamburlo, Machines We Trust: Perspectives on Dependable
    AI. MIT Press, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] V. Gupta, C. Jung, S. Neel, A. Roth, S. Sharifi-Malvajerdi, and C. Waites,
    “Adaptive machine unlearning,” in Advances in Neural Information Processing Systems,
    vol. 34, pp. 16319–16330, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] W. Qiu et al., “A federated learning paradigm for heart sound classification,”
    in Proc. EMBC, (Glasgow, UK), pp. 1045–1048, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] C. T. Williams, “A lecture on laënnec and the evolution of the stethoscope,”
    British medical journal, vol. 2, no. 2427, pp. 6–8, 1907.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] B. Silverman and M. Balk, “Digital stethoscope—improved auscultation
    at the bedside,” The American journal of cardiology, vol. 123, no. 6, pp. 984–985,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] M. E. Tavel, “Cardiac auscultation: a glorious past—and it does have
    a future!,” Circulation, vol. 113, no. 9, pp. 1255–1259, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] M. Abella, J. Formolo, and D. G. Penney, “Comparison of the acoustic
    properties of six popular stethoscopes,” The Journal of the Acoustical Society
    of America, vol. 91, no. 4, pp. 2224–2228, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] N. K. Bakshi and A. K. Acharya, “Wireless electronic stethoscope,” INTERNATIONAL
    JOURNAL OF ENGINEERING RESEARCH & TECHNOLOGY, vol. 03, pp. 459–462, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] J. E. Schenthal, J. W. Sweeney, and J. Nettleton, Wilson, “Clinical application
    of large-scale electronic data processing apparatus: I. new concepts in clinical
    use of the electronic digital computer,” Journal of the American Medical Association,
    vol. 173, pp. 6–11, 05 1960.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] H. R. Warner, A. F. Toronto, L. G. Veasey, and R. Stephenson, “A mathematical
    approach to medical diagnosis: Application to congenital heart disease,” JAMA,
    vol. 177, pp. 177–183, 07 1961.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] L. Taback, E. Marden, H. L. Mason, and H. V. Pipberger, “Digital recording
    of electrocardiographic data for analysis by a digital computer,” IRE Transactions
    on Medical Electronics, vol. ME-6, no. 3, pp. 167–171, 1959.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] C. A. CACERES, “Electrocardiographic analysis by a computer system,”
    Archives of Internal Medicine, vol. 111, pp. 196–202, 02 1963.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] H. B. Sprague, “History and present status of phonocardiography,” IRE
    Transactions on Medical Electronics, pp. 2–3, 1957.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] W. Evans, “The use of the phonocardiograph in clinical cardiology,” British
    heart journal, vol. 10, no. 2, pp. 92–98, 1948.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] A. Quiceno-Manrique, J. Godino-Llorente, M. Blanco-Velasco, and G. Castellanos-Dominguez,
    “Selection of dynamic features based on time–frequency representations for heart
    murmur detection from phonocardiographic signals,” Annals of biomedical engineering,
    vol. 38, no. 1, pp. 118–137, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] L. J. Nowak and K. M. Nowak, “Sound differences between electronic and
    acoustic stethoscopes,” Biomedical engineering online, vol. 17, no. 104, pp. 1–11,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] J. Y. Shin, S. L’Yi, D. H. Jo, J. H. Bae, and T. S. Lee, “Development
    of smartphone-based stethoscope system,” in Proc. ICCAS, (Busan, South Korea),
    pp. 1288–1291, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] S. Hadiyoso, D. R. Mardiyah, D. N. Ramadan, and A. Ibrahim, “Implementation
    of electronic stethoscope for online remote monitoring with mobile application,”
    Bulletin of Electrical Engineering and Informatics, vol. 9, no. 4, pp. 1595–1603,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] C. Yang, W. Zhang, Z. Pang, J. Zhang, D. Zou, X. Zhang, S. Guo, J. Wan,
    K. Wang, and W. Pang, “A aow-Cost, ear-contactless electronic stethoscope powered
    by Raspberry Pi for auscultation of patients with COVID-19: Prototype development
    and feasibility study,” JMIR Medical Informatics, vol. 9, no. 1, p. e22753, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: \appendices
  prefs: []
  type: TYPE_NORMAL
- en: 8 History
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before the 19th century, immediate auscultation conducted by a physician putting
    his/her ear directly to the patient’s chest was disliked because of its implications
    from a social perspective and in technique [[2](#bib.bib2)]. Immediate auscultation
    was even partially unacceptable due to the patient’s age and gender [[2](#bib.bib2)].
    Therefore, physical examination was mostly limited to inspection and palpation,
    which were not available in some cases, e. g., the great degree of body fat [[2](#bib.bib2)].
    Until 1816, the invention of the earliest stethoscope (Greek: stethos = chest,
    skopein = to view or to see) by a clinician, René Théoophile Hyacinthe Laennec,
    started a new era of mediate auscultation [[2](#bib.bib2)]. After Laennec’s cylinder-like
    stethoscope, cardiac auscultation was widely applied in physical examination and
    the structure of stethoscope was also improved. Charles J. B. Williams designed
    a flexible monaural stethoscope with a trumpet-shaped head [[2](#bib.bib2)]. Arthur
    Leared claimed the invention of a binaural stethoscope for more flexibility in
    1851 [[162](#bib.bib162)], and George Cammann further invented a binaural stethoscope
    for commercial production in 1852 [[163](#bib.bib163)]. Afterwards, the stethoscope
    was successfully and widely used as a critical diagnostic tool. However, there
    are several difficulties to employ such traditional acoustic stethoscopes. Firstly,
    acoustic stethoscopes cannot record, play back, and process the heart sounds,
    thus limiting the teaching of auscultation [[164](#bib.bib164)]. Secondly, acoustic
    stethoscopes demand physicians’ substantial clinical experience, but auscultation
    is a difficult skill that take years to acquire and refine [[27](#bib.bib27),
    [26](#bib.bib26)]. Thirdly, the performance of the human ear is limited to its
    physical limitations [[27](#bib.bib27)]. Although the apparent sound amplification
    has been seen by many acoustic stethoscopes, the increasing insensitivity of the
    human ear at frequencies below 100 Hz is still a problem for auscultation [[165](#bib.bib165)].
    Finally, the sound level of acoustic stethoscopes is very low, so they are not
    very suitable in noisy environment [[166](#bib.bib166)]. To this end, developing
    equipment and technologies for computer-aided auscultation is necessary to overcome
    the limitations of acoustic stethoscopes.'
  prefs: []
  type: TYPE_NORMAL
- en: As early as in the late 1950s and 1960s, the computational capability of digital
    electronic computers promoted their applications in heart disease diagnosis [[167](#bib.bib167),
    [23](#bib.bib23)]. One direction was to deal with clinical data, such as diagnosis
    of congenital heart disease based on the incidence of clinical signs, symptoms,
    and electrocardiographic findings [[168](#bib.bib168)]. Compared to processing
    human ‘predigested’ clinical data, analysing raw physiologic records (e. g., electrocardiogram)
    is a more automated direction to save the time and efforts of physicians [[23](#bib.bib23),
    [169](#bib.bib169), [170](#bib.bib170)]. Additionally, another automated way similar
    to physicians’ cardiac auscultation is to analyse magnetic tape recordings of
    heart sounds and murmurs [[23](#bib.bib23)]. Automatically analysing recorded
    heart sounds and murmurs is promising to solve the aforementioned problems of
    acoustic stethoscopes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Phonocardiograph equipments were developed in the 1930s and 1940s [[171](#bib.bib171)].
    The recorded PCG signals can be objectively analysed and explained. An early study [[171](#bib.bib171)]
    reported the phonocardiograph is useful in timing and explaining whether an individual’s
    heart sounds contain abnormal sounds: atrial sounds, opening snaps, and gallop
    rhythms. A phonocardiograph was also described to be valuable in studying murmurs,
    particularly in mitral disease and congenital heart disease [[172](#bib.bib172),
    [171](#bib.bib171)]. In the past decades, due to the development of digital technologies,
    computer-aided auscultation from PCG signals played an important role in pediatric
    cardiology, internal diseases, and evaluating congenital cardiac defects [[173](#bib.bib173)].
    Particularly, the electronic stethoscope, as a light phonocardiograph equipment,
    gains an edge over acoustic stethoscopes for automatically percepting, processing,
    and analysing heart sounds. Heart sounds are electronically amplified in electronic
    stethoscopes to overcome the low sound levels of acoustic stethoscopes [[174](#bib.bib174)].
    Generally, the recorded PCG signals with an electronic stethoscope are transferred
    to a computer for visualisation and further analysis [[26](#bib.bib26)]. More
    recently, portable electronic stethoscopes were developed to be either connected
    to other mobile devices, e. g., mobile phones, or to wirelessly transmit to a
    remote processing unit via a Bluetooth/wireless interface [[26](#bib.bib26)].
    Consequently, electronic stethoscopes are able to be applied to several applications,
    such as real-time/remote monitoring and diagnosis [[175](#bib.bib175), [176](#bib.bib176)].
    In 2021, an ear-contactless electronic stethoscope was designed for auscultation
    of patients with coronavirus disease 2019 [[177](#bib.bib177)].'
  prefs: []
  type: TYPE_NORMAL
