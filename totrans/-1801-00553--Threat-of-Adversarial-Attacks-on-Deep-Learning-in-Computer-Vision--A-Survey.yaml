- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:08:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:08:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1801.00553] Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1801.00553] 《深度学习在计算机视觉中的对抗攻击威胁：综述》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1801.00553](https://ar5iv.labs.arxiv.org/html/1801.00553)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1801.00553](https://ar5iv.labs.arxiv.org/html/1801.00553)
- en: 'Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深度学习在计算机视觉中的对抗攻击威胁：综述》
- en: Naveed Akhtar and Ajmal Mian
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Naveed Akhtar 和 Ajmal Mian
- en: 'ACKNOWLEDGEMENTS: The authors thank Nicholas Carlini (UC Berkeley) and Dimitris
    Tsipras (MIT) for feedback to improve the survey quality. We also acknowledge
    X. Huang (Uni. Liverpool), K. R. Reddy (IISC), E. Valle (UNICAMP), Y. Yoo (CLAIR)
    and others for providing pointers to make the survey more comprehensive. This
    research was supported by ARC grant DP160101458. N. Akhtar and A. Mian are with
    the School of Computer Science and Software Engineering, University of Western
    Australia.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢：作者感谢Nicholas Carlini（加州大学伯克利分校）和Dimitris Tsipras（麻省理工学院）对提升综述质量的反馈。我们还感谢X.
    Huang（利物浦大学）、K. R. Reddy（印度科学研究所）、E. Valle（UNICAMP）、Y. Yoo（CLAIR）及其他人提供的建议，使综述更加全面。本研究得到了ARC资助DP160101458的支持。N.
    Akhtar和A. Mian在西澳大学计算机科学与软件工程学院工作。
- en: 'E-mail: {naveed.akhtar, ajmal.mian}@uwa.edu.au Manuscript received August 2017,
    revised…'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：{naveed.akhtar, ajmal.mian}@uwa.edu.au 手稿接收于2017年8月，修订中…
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning is at the heart of the current rise of artificial intelligence.
    In the field of Computer Vision, it has become the workhorse for applications
    ranging from self-driving cars to surveillance and security. Whereas deep neural
    networks have demonstrated phenomenal success (often beyond human capabilities)
    in solving complex problems, recent studies show that they are vulnerable to adversarial
    attacks in the form of subtle perturbations to inputs that lead a model to predict
    incorrect outputs. For images, such perturbations are often too small to be perceptible,
    yet they completely fool the deep learning models. Adversarial attacks pose a
    serious threat to the success of deep learning in practice. This fact has recently
    lead to a large influx of contributions in this direction. This article presents
    the first comprehensive survey on adversarial attacks on deep learning in Computer
    Vision. We review the works that design adversarial attacks, analyze the existence
    of such attacks and propose defenses against them. To emphasize that adversarial
    attacks are possible in practical conditions, we separately review the contributions
    that evaluate adversarial attacks in the real-world scenarios. Finally, drawing
    on the reviewed literature, we provide a broader outlook of this research direction.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是当前人工智能崛起的核心。在计算机视觉领域，它已成为从自动驾驶汽车到监控和安全等应用的主力。尽管深度神经网络在解决复杂问题方面取得了惊人的成功（常常超越人类能力），但最近的研究显示，它们容易受到对抗攻击，这种攻击通过对输入进行细微扰动，使模型预测出错误的输出。对于图像而言，这种扰动通常过于微小，以至于不可察觉，但却完全欺骗了深度学习模型。对抗攻击对深度学习的实际成功构成了严重威胁。这一事实最近引发了大量相关研究。本文首次全面综述了计算机视觉中深度学习的对抗攻击。我们回顾了设计对抗攻击的研究，分析了这些攻击的存在，并提出了防御措施。为了强调对抗攻击在实际条件下是可能的，我们分别回顾了评估真实世界场景中的对抗攻击的贡献。最后，基于回顾的文献，我们提供了对这一研究方向的更广泛展望。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep Learning, adversarial perturbation, black-box attack, white-box attack,
    adversarial learning, perturbation detection.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，对抗扰动，黑箱攻击，白箱攻击，对抗学习，扰动检测。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep Learning [[1](#bib.bib1)] is providing major breakthroughs in solving the
    problems that have withstood many attempts of machine learning and artificial
    intelligence community in the past. As a result, it is currently being used to
    decipher hard scientific problems at an unprecedented scale, e.g. in reconstruction
    of brain circuits [[2](#bib.bib2)]; analysis of mutations in DNA [[3](#bib.bib3)];
    prediction of structure-activity of potential drug molecules [[4](#bib.bib4)],
    and analyzing the particle accelerator data [[5](#bib.bib5)] [[6](#bib.bib6)].
    Deep neural networks have also become the preferred choice to solve many challenging
    tasks in speech recognition [[7](#bib.bib7)] and natural language understanding [[8](#bib.bib8)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习 [[1](#bib.bib1)] 正在解决许多机器学习和人工智能领域过去难以突破的问题。因此，它目前被用于以空前的规模解读复杂的科学问题，例如 脑电路重建 [[2](#bib.bib2)];
    DNA突变分析 [[3](#bib.bib3)]; 预测潜在药物分子的结构-活性 [[4](#bib.bib4)]，以及分析粒子加速器数据 [[5](#bib.bib5)]
    [[6](#bib.bib6)]。深度神经网络也成为解决语音识别 [[7](#bib.bib7)] 和自然语言理解 [[8](#bib.bib8)] 等许多具有挑战性的任务的首选。
- en: In the field of Computer Vision, deep learning became the center of attention
    after Krizhevsky et al. [[9](#bib.bib9)] demonstrated the impressive performance
    of a Convolutional Neural Network (CNN) [[10](#bib.bib10)] based model on a very
    challenging large-scale visual recognition task [[11](#bib.bib11)] in 2012. A
    significant credit for the current popularity of deep learning can also be attributed
    to this seminal work. Since 2012, the Computer Vision community has made numerous
    valuable contributions to deep learning research, enabling it to provide solutions
    for the problems encountered in medical science [[21](#bib.bib21)] to mobile applications [[181](#bib.bib181)].
    The recent breakthrough in artificial intelligence in the form of tabula-rasa
    learning of AlphaGo Zero [[14](#bib.bib14)] also owes a fair share to deep Residual
    Networks (ResNets) [[147](#bib.bib147)] that were originally proposed for the
    task of image recognition.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉领域，深度学习在2012年引起了广泛关注，此前Krizhevsky等人 [[9](#bib.bib9)] 展示了基于卷积神经网络（CNN） [[10](#bib.bib10)]
    的模型在一个非常具有挑战性的大规模视觉识别任务 [[11](#bib.bib11)] 上的卓越表现。当前深度学习流行度的显著归功于这项开创性工作。自2012年以来，计算机视觉社区为深度学习研究做出了大量宝贵贡献，使其能够为从医学科学 [[21](#bib.bib21)]
    到移动应用 [[181](#bib.bib181)] 等领域的问题提供解决方案。AlphaGo Zero [[14](#bib.bib14)] 在人工智能领域的突破性进展也得益于深度残差网络（ResNets） [[147](#bib.bib147)]，这一网络最初是为图像识别任务提出的。
- en: With the continuous improvements of deep neural network models [[145](#bib.bib145)],
    [[147](#bib.bib147)], [[168](#bib.bib168)]; open access to efficient deep learning
    software libraries [[177](#bib.bib177)], [[178](#bib.bib178)], [[179](#bib.bib179)];
    and easy availability of hardware required to train complex models, deep learning
    is fast achieving the maturity to enter into safety and security critical applications,
    e.g. self driving cars [[12](#bib.bib12)], [[182](#bib.bib182)], surveillance [[13](#bib.bib13)],
    maleware detection [[34](#bib.bib34)],[[107](#bib.bib107)], drones and robotics [[157](#bib.bib157)],[[180](#bib.bib180)],
    and voice command recognition [[7](#bib.bib7)]. With the recent real-world developments
    like facial recognition ATM [[183](#bib.bib183)] and Face ID security on mobile
    phones [[184](#bib.bib184)], it is apparent that deep learning solutions, especially
    those originating from Computer Vision problems are about to play a major role
    in our daily lives.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度神经网络模型的不断改进 [[145](#bib.bib145)], [[147](#bib.bib147)], [[168](#bib.bib168)];
    高效深度学习软件库的开放获取 [[177](#bib.bib177)], [[178](#bib.bib178)], [[179](#bib.bib179)];
    以及训练复杂模型所需硬件的易得性，深度学习正迅速成熟，进入安全和保密关键应用领域，例如 自动驾驶汽车 [[12](#bib.bib12)], [[182](#bib.bib182)],
    监控 [[13](#bib.bib13)], 恶意软件检测 [[34](#bib.bib34)], [[107](#bib.bib107)], 无人机和机器人 [[157](#bib.bib157)],
    [[180](#bib.bib180)], 以及语音命令识别 [[7](#bib.bib7)]。随着近期如面部识别ATM [[183](#bib.bib183)]
    和手机上的Face ID安全性 [[184](#bib.bib184)]等现实世界的发展，深度学习解决方案，特别是那些源于计算机视觉问题的解决方案，显然将在我们的日常生活中发挥重要作用。
- en: '![Refer to caption](img/296782cb8ce405019937c8d23f63544b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/296782cb8ce405019937c8d23f63544b.png)'
- en: 'Figure 1: Example of attacks on deep learning models with ‘universal adversarial
    perturbations’ [[16](#bib.bib16)]: The attacks are shown for the CaffeNet [[9](#bib.bib9)],
    VGG-F network [[17](#bib.bib17)] and GoogLeNet [[18](#bib.bib18)]. All the networks
    recognized the original clean images correctly with high confidence. After small
    perturbations were added to the images, the networks predicted wrong labels with
    similar high confidence. Notice that the perturbations are hardly perceptible
    for human vision system, however their effects on the deep learning models are
    catastrophic.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用“通用对抗扰动”攻击深度学习模型的示例 [[16](#bib.bib16)]：攻击展示了 CaffeNet [[9](#bib.bib9)]、VGG-F
    网络 [[17](#bib.bib17)] 和 GoogLeNet [[18](#bib.bib18)] 的情况。所有网络都以高度信心正确识别了原始干净图像。在对图像添加了小的扰动后，网络以类似的高信心预测了错误的标签。请注意，这些扰动对于人眼几乎不可察觉，但它们对深度学习模型的影响却是灾难性的。
- en: Whereas deep learning performs a wide variety of Computer Vision tasks with
    remarkable accuracies, Szegedy et al. [[22](#bib.bib22)] first discovered an intriguing
    weakness of deep neural networks in the context of image classification. They
    showed that despite their high accuracies, modern deep networks are surprisingly
    susceptible to adversarial attacks in the form of small perturbations to images
    that remain (almost) imperceptible to human vision system. Such attacks can cause
    a neural network classifier to completely change its prediction about the image.
    Even worse, the attacked models report high confidence on the wrong prediction.
    Moreover, the same image perturbation can fool multiple network classifiers. The
    profound implications of these results triggered a wide interest of researchers
    in adversarial attacks and their defenses for deep learning in general.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在执行各种计算机视觉任务时表现出色，但 Szegedy 等人 [[22](#bib.bib22)] 首次发现了深度神经网络在图像分类中的一个有趣弱点。他们表明，尽管现代深度网络具有很高的准确率，但它们对小的扰动攻击（这些扰动对人眼几乎不可察觉）却出奇地敏感。这些攻击可以导致神经网络分类器完全改变对图像的预测。更糟糕的是，被攻击的模型会对错误的预测报告高度自信。此外，相同的图像扰动可以欺骗多个网络分类器。这些结果的深远影响引起了研究人员对深度学习对抗攻击及其防御的广泛关注。
- en: 'Since the findings of Szegedy et al. [[22](#bib.bib22)], several interesting
    results have surfaced regarding adversarial attacks on deep learning in Computer
    Vision. For instance, in addition to the image-specific adversarial perturbations [[22](#bib.bib22)],
    Moosavi-Dezfooli et al. [[16](#bib.bib16)] showed the existence of ‘universal
    perturbations’ that can fool a network classifier on any image (see Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Threat of Adversarial Attacks on Deep Learning in
    Computer Vision: A Survey") for example). Similarly, Athalye et al. [[65](#bib.bib65)]
    demonstrated that it is possible to even 3-D print real-world objects that can
    fool deep neural network classifiers (see Section [4.3](#S4.SS3 "4.3 Generic adversarial
    3D objects ‣ 4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey")). Keeping in view the significance of
    deep learning research in Computer Vision and its potential applications in the
    real life, this article presents the first comprehensive survey on adversarial
    attacks on deep learning in Computer Vision. The article is intended for a wider
    readership than Computer Vision community, hence it assumes only basic knowledge
    of deep learning and image processing. Nevertheless, it also discusses technical
    details of important contributions for the interested readers.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '自从 Szegedy 等人 [[22](#bib.bib22)] 的研究发现以来，计算机视觉领域出现了关于深度学习对抗攻击的若干有趣结果。例如，除了图像特定的对抗扰动
    [[22](#bib.bib22)] 外，Moosavi-Dezfooli 等人 [[16](#bib.bib16)] 还展示了“通用扰动”的存在，这些扰动能够欺骗任何图像上的网络分类器（例如见图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey")）。类似地，Athalye 等人 [[65](#bib.bib65)] 证明了甚至可以
    3-D 打印真实世界中的物体，以欺骗深度神经网络分类器（见第 [4.3](#S4.SS3 "4.3 Generic adversarial 3D objects
    ‣ 4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey") 节）。鉴于深度学习研究在计算机视觉中的重要性及其在现实生活中的潜在应用，本文首次全面综述了计算机视觉中对深度学习的对抗攻击。本文面向比计算机视觉社区更广泛的读者，因此仅假设对深度学习和图像处理有基本知识。然而，本文也讨论了感兴趣读者可能关注的重要贡献的技术细节。'
- en: 'We first describe the common terms related to adversarial attacks in the parlance
    of Computer Vision in Section [2](#S2 "2 Definitions of terms ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"). In Section [3](#S3 "3
    Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey"), we review the adversarial attacks for the task of image classification
    and beyond. A separate section is dedicated to the approaches that deal with adversarial
    attacks in the real-world conditions. Those approaches are reviewed in Section [4](#S4
    "4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey"). In the literature, there are also works that mainly
    focus on analyzing the existence of adversarial attacks. We discuss those contributions
    in Section [5](#S5 "5 Existence of adversarial examples ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"). The approaches that make
    defense against the adversarial attacks as their central topic are discussed in
    Section [6](#S6 "6 Defenses against adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"). In Section [7](#S7 "7
    Outlook of the research direction ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey"), we provide a broader outlook of the research direction
    based on the reviewed literature. Finally, we draw conclusion in Section [8](#S8
    "8 Conclusion ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在第[2](#S2 "2 Definitions of terms ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey")节中描述与对抗性攻击相关的常见术语。在第[3](#S3 "3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")节中，我们回顾了图像分类及其他任务中的对抗性攻击。一个单独的部分专门讨论在现实世界条件下处理对抗性攻击的方法。这些方法在第[4](#S4 "4
    Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning in
    Computer Vision: A Survey")节中进行了回顾。在文献中，也有一些工作主要集中于分析对抗性攻击的存在性。我们在第[5](#S5 "5
    Existence of adversarial examples ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey")节中讨论这些贡献。以防御对抗性攻击为中心的研究方法在第[6](#S6 "6 Defenses against
    adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")节中进行讨论。在第[7](#S7 "7 Outlook of the research direction ‣ Threat
    of Adversarial Attacks on Deep Learning in Computer Vision: A Survey")节中，我们根据回顾的文献提供了研究方向的更广泛展望。最后，我们在第[8](#S8
    "8 Conclusion ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey")节中得出结论。'
- en: 2 Definitions of terms
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 术语定义
- en: In this section, we describe the common technical terms used in the literature
    related to adversarial attacks on deep learning in Computer Vision. The remaining
    article also follows the same definitions of the terms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了与计算机视觉中对抗性攻击相关的文献中使用的常见技术术语。余下的文章也遵循相同的术语定义。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial example/image is a modified version of a clean image that is intentionally
    perturbed (e.g. by adding noise) to confuse/fool a machine learning technique,
    such as deep neural networks.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性示例/图像是干净图像的修改版，通过有意的扰动（例如添加噪声）来混淆/欺骗机器学习技术，例如深度神经网络。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial perturbation is the noise that is added to the clean image to make
    it an adversarial example.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性扰动是加在干净图像上的噪声，使其变成对抗性示例。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial training uses adversarial images besides the clean images to train
    machine learning models.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性训练除了使用干净图像外，还使用对抗性图像来训练机器学习模型。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversary more commonly refers to the agent who creates an adversarial example.
    However, in some cases the example itself is also called adversary.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对手通常指的是创建对抗性示例的代理。然而，在某些情况下，示例本身也被称为对手。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Black-box attacks feed a targeted model with the adversarial examples (during
    testing) that are generated without the knowledge of that model. In some instances,
    it is assumed that the adversary has a limited knowledge of the model (e.g. its
    training procedure and/or its architecture) but definitely does not know about
    the model parameters. In other instances, using any information about the target
    model is referred to as ‘semi-black-box’ attack. We use the former convention
    in this article.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 黑箱攻击向目标模型输入在不了解该模型的情况下生成的对抗性示例（在测试期间）。在某些情况下，假设对手对模型有有限了解（例如其训练过程和/或架构），但绝对不知道模型参数。在其他情况下，使用关于目标模型的任何信息被称为“半黑箱”攻击。我们在本文中使用前者的定义。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Detector is a mechanism to (only) detect if an image is an adversarial example.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检测器是一个机制，用于（仅）检测图像是否为对抗性示例。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fooling ratio/rate indicates the percentage of images on which a trained model
    changes its prediction label after the images are perturbed.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欺骗率/比例表示在对图像进行扰动后，训练模型更改其预测标签的图像百分比。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: One-shot/one-step methods generate an adversarial perturbation by performing
    a single step computation, e.g. computing gradient of model loss once. The opposite
    are iterative methods that perform the same computation multiple times to get
    a single perturbation. The latter are often computationally expensive.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一次性/一步法方法通过执行单步计算来生成对抗扰动，例如，仅计算模型损失的梯度。相反的是迭代方法，它们执行相同的计算多次以获得一个扰动。后者通常计算开销较大。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Quasi-imperceptible perturbations impair images very slightly for human perception.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 几乎不可察觉的扰动对人类感知的图像影响非常轻微。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rectifier modifies an adversarial example to restore the prediction of the targeted
    model to its prediction on the clean version of the same example.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 校正器修改对抗样本，以将目标模型的预测恢复到该样本的干净版本上的预测。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Targeted attacks fool a model into falsely predicting a specific label for the
    adversarial image. They are opposite to the non-targeted attacks in which the
    predicted label of the adversarial image is irrelevant, as long as it is not the
    correct label.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定向攻击欺骗模型错误地预测对抗图像的特定标签。它们与非定向攻击相对，后者只要不是正确标签，对抗图像的预测标签无关紧要。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Threat model refers to the types of potential attacks considered by an approach,
    e.g. black-box attack.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 威胁模型指的是方法考虑的潜在攻击类型，例如黑盒攻击。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transferability refers to the ability of an adversarial example to remain effective
    even for the models other than the one used to generate it.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迁移性指的是对抗样本在生成它的模型之外仍然有效的能力。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Universal perturbation is able to fool a given model on ‘any’ image with high
    probability. Note that, universality refers to the property of a perturbation
    being ‘image-agnostic’ as opposed to having good transferability.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通用扰动能够以高概率欺骗给定模型在“任何”图像上。请注意，通用性指的是扰动具有“图像无关性”的特性，而不是具有良好的迁移性。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: White-box attacks assume the complete knowledge of the targeted model, including
    its parameter values, architecture, training method, and in some cases its training
    data as well.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 白盒攻击假设对目标模型具有完全知识，包括其参数值、架构、训练方法以及在某些情况下的训练数据。
- en: 3 Adversarial attacks
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 对抗攻击
- en: 'In this section, we review the body of literature in Computer Vision that introduces
    methods for adversarial attacks on deep learning. The reviewed literature mainly
    deals with the art of fooling the deep neural networks in ‘laboratory settings’,
    where approaches are developed for the typical Computer Vision tasks, e.g. recognition,
    and their effectiveness is demonstrated using standard datasets, e.g. MNIST [[10](#bib.bib10)].
    The techniques that focus on attacking deep learning in the real-world conditions
    are separately reviewed in Section [4](#S4 "4 Attacks in the real world ‣ Threat
    of Adversarial Attacks on Deep Learning in Computer Vision: A Survey"). However,
    it should be noted that the approaches reviewed in this section form the basis
    of the real-world attacks, and almost each one of them has the potential to significantly
    affect deep learning in practice. Our division is based on the evaluation conditions
    of the attacks in the original contributions.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们回顾了计算机视觉领域的文献，这些文献介绍了对深度学习的对抗攻击方法。回顾的文献主要涉及在“实验室环境”中欺骗深度神经网络的艺术，其中开发了用于典型计算机视觉任务（例如识别）的方法，并使用标准数据集（例如MNIST）展示其有效性[[10](#bib.bib10)]。关注于现实世界条件下攻击深度学习的技术在第[4](#S4
    "4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey")节中单独回顾。然而，需要注意的是，本节回顾的方法构成了现实世界攻击的基础，而且几乎每一种方法都可能在实践中显著影响深度学习。我们的划分基于原始贡献中攻击的评估条件。'
- en: 'The review in this section is mainly organized in chronological order, with
    few exceptions to maintain the flow of discussion. To provide technical understanding
    of the core concepts to the reader, we also go into technical details of the popular
    approaches and some representative techniques of the emerging directions in this
    area. Other methods are discussed briefly. We refer to the original papers for
    the details on those techniques. This section is divided into two parts. In part [3.1](#S3.SS1
    "3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"), we review the methods
    that attack deep neural networks performing the most common task in Computer Vision,
    i.e. classification/recognition. Approaches that are predominantly designed to
    attack deep learning beyond this task are discussed in part [3.2](#S3.SS2 "3.2
    Attacks beyond classification/recognition ‣ 3 Adversarial attacks ‣ Threat of
    Adversarial Attacks on Deep Learning in Computer Vision: A Survey").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的回顾主要按时间顺序组织，少数例外以保持讨论的连贯性。为了向读者提供核心概念的技术理解，我们还深入探讨了流行方法及其代表性技术，以及该领域新兴方向的一些技术。其他方法则简要讨论。有关这些技术的详细信息，请参阅原始论文。本节分为两个部分。在部分[3.1](#S3.SS1
    "3.1 分类攻击 ‣ 3 对抗攻击 ‣ 对抗攻击对计算机视觉深度学习的威胁：综述")中，我们回顾了攻击深度神经网络的那些方法，这些网络执行计算机视觉中最常见的任务，即分类/识别。在部分[3.2](#S3.SS2
    "3.2 超越分类/识别的攻击 ‣ 3 对抗攻击 ‣ 对抗攻击对计算机视觉深度学习的威胁：综述")中，我们讨论了主要设计用于攻击深度学习以外任务的方法。
- en: 3.1 Attacks for classification
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 分类攻击
- en: 3.1.1 Box-constrained L-BFGS
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 有盒约束的 L-BFGS
- en: 'Szegedy et al. [[22](#bib.bib22)] first demonstrated the existence of small
    perturbations to the images, such that the perturbed images could fool deep learning
    models into misclassification. Let ${\bf I}_{c}\in\mathbb{R}^{m}$ denote a vectorized
    clean image - the subscript ‘$c$’ emphasizes that the image is clean. To compute
    an additive perturbation $\boldsymbol{\rho}\in\mathbb{R}^{m}$ that would distort
    the image very slightly to fool the network, Szegedy et al. proposed to solve
    the following problem:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Szegedy 等人 [[22](#bib.bib22)] 首次演示了对图像进行小扰动的存在，以至于这些扰动后的图像可以欺骗深度学习模型进行错误分类。令${\bf
    I}_{c}\in\mathbb{R}^{m}$表示一个向量化的干净图像——下标‘$c$’强调图像是干净的。为了计算一个加性扰动$\boldsymbol{\rho}\in\mathbb{R}^{m}$，该扰动会略微扭曲图像以欺骗网络，Szegedy
    等人提出解决以下问题：
- en: '|  | $\displaystyle\min_{\boldsymbol{\rho}}&#124;&#124;\boldsymbol{\rho}&#124;&#124;_{2}\hskip
    5.69054pt\text{s.t.}~{}\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})=\ell;~{}{\bf
    I}_{c}+\boldsymbol{\rho}\in[0,1]^{m},$ |  | (1) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\boldsymbol{\rho}}&#124;&#124;\boldsymbol{\rho}&#124;&#124;_{2}\hskip
    5.69054pt\text{s.t.}~{}\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})=\ell;~{}{\bf
    I}_{c}+\boldsymbol{\rho}\in[0,1]^{m},$ |  | (1) |'
- en: 'where ‘$\ell$’ denotes the label of the image and $\mathcal{C}(.)$ is the deep
    neural network classifier. The authors proposed to solve ([1](#S3.E1 "In 3.1.1
    Box-constrained L-BFGS ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks
    ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey"))
    for its non-trivial solution where ‘$\ell$’ is different from the original label
    of ${\bf I}_{c}$. In that case, ([1](#S3.E1 "In 3.1.1 Box-constrained L-BFGS ‣
    3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")) becomes a hard problem,
    hence an approximate solution is sought using a box-constrained L-BFGS [[20](#bib.bib20)].
    This is done by finding the minimum $c>0$ for which the minimizer $\boldsymbol{\rho}$
    of the following problem satisfies the condition $\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})=\ell$:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中‘$\ell$’表示图像的标签，而$\mathcal{C}(.)$是深度神经网络分类器。作者提出解决([1](#S3.E1 "在 3.1.1 有盒约束的
    L-BFGS ‣ 3.1 分类攻击 ‣ 3 对抗攻击 ‣ 对抗攻击对计算机视觉深度学习的威胁：综述"))的非平凡解，其中‘$\ell$’与原始标签${\bf
    I}_{c}$不同。在这种情况下，([1](#S3.E1 "在 3.1.1 有盒约束的 L-BFGS ‣ 3.1 分类攻击 ‣ 3 对抗攻击 ‣ 对抗攻击对计算机视觉深度学习的威胁：综述"))变成了一个难题，因此通过使用有盒约束的
    L-BFGS [[20](#bib.bib20)]寻求近似解。这是通过寻找最小的$c>0$来完成的，对于该$c$，以下问题的最小化器$\boldsymbol{\rho}$满足条件$\mathcal{C}({\bf
    I}_{c}+\boldsymbol{\rho})=\ell$：
- en: '|  | $\displaystyle\min_{\boldsymbol{\rho}}~{}~{}c&#124;\boldsymbol{\rho}&#124;+\mathcal{L}({\bf
    I}_{c}+\boldsymbol{\rho},\ell)~{}~{}s.t.~{}{\bf I}_{c}+\boldsymbol{\rho}\in[0,1]^{m},$
    |  | (2) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\boldsymbol{\rho}}~{}~{}c&#124;\boldsymbol{\rho}&#124;+\mathcal{L}({\bf
    I}_{c}+\boldsymbol{\rho},\ell)~{}~{}s.t.~{}{\bf I}_{c}+\boldsymbol{\rho}\in[0,1]^{m},$
    |  | (2) |'
- en: 'where $\mathcal{L}(.,.)$ computes the loss of the classifier. We note that
    ([2](#S3.E2 "In 3.1.1 Box-constrained L-BFGS ‣ 3.1 Attacks for classification
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")) results in the exact solution for a classifier that has a
    convex loss function. However, for deep neural networks, this is generally not
    the case. The computed perturbation is simply added to the image to make it an
    adversarial example.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathcal{L}(.,.)$ 计算分类器的损失。我们注意到 ([2](#S3.E2 "In 3.1.1 Box-constrained
    L-BFGS ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")) 结果对于具有凸损失函数的分类器提供了精确的解决方案。然而，对于深度神经网络，通常情况并非如此。计算出的扰动被简单地添加到图像中，使其成为对抗样本。'
- en: 'As shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3.1.1 Box-constrained L-BFGS ‣ 3.1
    Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks
    on Deep Learning in Computer Vision: A Survey"), the above method is able to compute
    perturbations that when added to clean images fool a neural network, but the adversarial
    images appear similar to the clean images to the human vision system. It was observed
    by Szegedy et al. that the perturbations computed for one neural network were
    also able to fool multiple networks. These astonishing results identified a blind-spot
    in deep learning. At the time of this discovery the Computer Vision community
    was fast adapting to the impression that deep learning features define the space
    where perceptual distances are well approximated by the Euclidean distances. Hence,
    these contradictory results triggered a wide interest of researchers in adversarial
    attacks on deep learning in Computer Vision.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [2](#S3.F2 "Figure 2 ‣ 3.1.1 Box-constrained L-BFGS ‣ 3.1 Attacks for classification
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey") 所示，上述方法能够计算出扰动，当这些扰动添加到干净图像中时，可以欺骗神经网络，但对抗样本在人的视觉系统中与干净图像类似。Szegedy
    等人观察到，对一个神经网络计算出的扰动也能欺骗多个网络。这些惊人的结果揭示了深度学习中的一个盲点。在这一发现时，计算机视觉界正快速适应于深度学习特征定义了感知距离在欧几里得距离下的空间。因此，这些矛盾的结果引发了研究人员对计算机视觉中深度学习的对抗攻击的广泛兴趣。'
- en: '![Refer to caption](img/9b9c87a88fe8aa4426dafef268228d74.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9b9c87a88fe8aa4426dafef268228d74.png)'
- en: 'Figure 2: Illustration of adversarial examples generated using [[22](#bib.bib22)]
    for AlexNet [[9](#bib.bib9)]. The perturbations are magnified 10x for better visualization
    (values shifted by 128 and clamped). The predicted labels of adversarial examples
    are also shown.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用 [[22](#bib.bib22)] 为 AlexNet [[9](#bib.bib9)] 生成的对抗样本示意图。为了更好地可视化，扰动被放大了
    10 倍（值偏移了 128 并被限制）。对抗样本的预测标签也被显示出来。
- en: 3.1.2 Fast Gradient Sign Method (FGSM)
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 快速梯度符号方法（FGSM）
- en: 'It was observed by Szegedy et al. [[22](#bib.bib22)] that the robustness of
    deep neural networks against the adversarial examples could be improved by adversarial
    training. To enable effective adversarial training, Goodfellow et al. [[23](#bib.bib23)]
    developed a method to efficiently compute an adversarial perturbation for a given
    image by solving the following problem:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Szegedy 等人观察到 [[22](#bib.bib22)] 深度神经网络对对抗样本的鲁棒性可以通过对抗训练得到改善。为了实现有效的对抗训练，Goodfellow
    等人 [[23](#bib.bib23)] 开发了一种方法，通过解决以下问题来高效计算给定图像的对抗扰动：
- en: '|  | $\displaystyle\boldsymbol{\rho}=\epsilon~{}\text{sign}\left(\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{c},\ell)\right),$ |  | (3) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\rho}=\epsilon~{}\text{sign}\left(\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{c},\ell)\right),$ |  | (3) |'
- en: 'where $\nabla\mathcal{J}(.,.,.)$ computes the gradient of the cost function
    around the current value of the model parameters $\boldsymbol{\theta}$ w.r.t. ${\bf
    I}_{c}$, sign$(.)$ denotes the sign function and $\epsilon$ is a small scalar
    value that restricts the norm of the perturbation. The method for solving ([3](#S3.E3
    "In 3.1.2 Fast Gradient Sign Method (FGSM) ‣ 3.1 Attacks for classification ‣
    3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")) was termed ‘Fast Gradient Sign Method’ (FGSM) in the original
    work.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\nabla\mathcal{J}(.,.,.)$ 计算损失函数在模型参数 $\boldsymbol{\theta}$ 当前值下对 ${\bf
    I}_{c}$ 的梯度，sign$(.)$ 表示符号函数，$\epsilon$ 是一个限制扰动范数的小标量值。解决该问题的方法在原始工作中被称为“快速梯度符号方法”（FGSM）。
- en: Interestingly, the adversarial examples generated by FGSM exploit the ‘linearity’
    of deep network models in the higher dimensional space whereas such models were
    commonly thought to be highly non-linear at that time. Goodfellow et al. [[23](#bib.bib23)]
    hypothesized that the designs of modern deep neural networks that (intentionally)
    encourage linear behavior for computational gains, also make them susceptible
    to cheap analytical perturbations. In the related literature, this idea is often
    referred to as the ‘linearity hypothesis’, which is substantiated by the FGSM
    approach.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Kurakin et al. [[80](#bib.bib80)] noted that on the popular large-scale image
    recognition data set ImageNet [[11](#bib.bib11)], the top-1 error rate on the
    adversarial examples generated by FGSM is around $63-69\%$ for $\epsilon\in[2,32]$.
    The authors also proposed a ‘one-step target class’ variation of the FGSM where
    instead of using the true label $\ell$ of the image in ([3](#S3.E3 "In 3.1.2 Fast
    Gradient Sign Method (FGSM) ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks
    ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey")),
    they used the label $\ell_{\text{target}}$ of the least likely class predicted
    by the network for ${\bf I}_{c}$. The computed perturbation is then subtracted
    from the original image to make it an adversarial example. For a neural network
    with cross-entropy loss, doing so maximizes the probability that the network predicts
    $\ell_{\text{target}}$ as the label of the adversarial example. It is suggested,
    that a random class can also be used as the target class for fooling the network,
    however it may lead to less interesting fooling, e.g. misclassification of one
    breed of dog as another dog breed. The authors also demonstrated that adversarial
    training improves robustness of deep neural networks against the attacks generated
    by FGSM and its proposed variants.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The FGSM perturbs an image to increase the loss of the classifier on the resulting
    image. The sign function ensures that the magnitude of the loss is maximized,
    while $\epsilon$ essentially restricts the $\ell_{\infty}$-norm of the perturbation.
    Miyato et al. [[103](#bib.bib103)] proposed a closely related method to compute
    the perturbation as follows
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\rho}=\epsilon\frac{\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{c},\ell)}{&#124;&#124;\nabla\mathcal{J}(\boldsymbol{\theta},{\bf I}_{c},\ell)&#124;&#124;_{2}}.$
    |  | (4) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: In the above equation, the computed gradient is normalized with its $\ell_{2}$-norm.
    Kurakin et al. [[80](#bib.bib80)] referred to this technique as ‘Fast Gradient
    L[2]’ method and also proposed an alternative of using the $\ell_{\infty}$-norm
    for normalization, and referred to the resulting technique as ‘Fast Gradient L[∞]’
    method. Broadly speaking, all of these methods are seen as ‘one-step’ or ‘one-shot’
    methods in the literature related to adversarial attacks in Computer Vision.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Basic & Least-Likely-Class Iterative Methods
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 基本与最不可能类别迭代方法
- en: 'The one-step methods perturb images by taking a single large step in the direction
    that increases the loss of the classifier (i.e. one-step gradient ascent). An
    intuitive extension of this idea is to iteratively take multiple small steps while
    adjusting the direction after each step. The Basic Iterative Method (BIM) [[35](#bib.bib35)]
    does exactly that, and iteratively computes the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一步法通过在增加分类器损失的方向上迈出一个大的步伐来扰动图像（即一步梯度上升）。这个想法的直观扩展是迭代地迈出多个小步，同时在每一步后调整方向。基本迭代方法（BIM）
    [[35](#bib.bib35)] 正是这样做的，并迭代地计算以下内容：
- en: '|  | $\displaystyle{\bf I}_{\boldsymbol{\rho}}^{i+1}=\text{Clip}_{\epsilon}\left\{{\bf
    I}_{\boldsymbol{\rho}}^{i}+\alpha~{}\text{sign}(\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{\boldsymbol{\rho}}^{i},\ell)\right\},$ |  | (5) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf I}_{\boldsymbol{\rho}}^{i+1}=\text{Clip}_{\epsilon}\left\{{\bf
    I}_{\boldsymbol{\rho}}^{i}+\alpha~{}\text{sign}(\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{\boldsymbol{\rho}}^{i},\ell)\right\},$ |  | (5) |'
- en: where ${\bf I}_{\boldsymbol{\rho}}^{i}$ denotes the perturbed image at the $i^{\text{th}}$
    iteration, Clip${}_{\epsilon}\{.\}$ clips (the values of the pixels of) the image
    in its argument at $\epsilon$ and $\alpha$ determines the step size (normally,
    $\alpha=1$). The BIM algorithm starts with ${\bf I}_{\boldsymbol{\rho}}^{0}={\bf
    I}_{c}$ and runs for the number of iterations determined by the formula $\lfloor\min(\epsilon+4,1.25\epsilon)\rfloor$.
    Madry et al. [[55](#bib.bib55)] pointed out that BIM is equivalent to (the $\ell_{\infty}$
    version of) Projected Gradient Descent (PGD), a standard convex optimization method.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bf I}_{\boldsymbol{\rho}}^{i}$ 表示第 $i^{\text{th}}$ 次迭代中的扰动图像，Clip${}_{\epsilon}\{.\}$
    将图像在其参数处的像素值限制在 $\epsilon$ 范围内，$\alpha$ 决定步长（通常，$\alpha=1$）。BIM 算法从 ${\bf I}_{\boldsymbol{\rho}}^{0}={\bf
    I}_{c}$ 开始，并运行至由公式 $\lfloor\min(\epsilon+4,1.25\epsilon)\rfloor$ 确定的迭代次数。Madry
    等人 [[55](#bib.bib55)] 指出，BIM 等同于（$\ell_{\infty}$ 版本的）投影梯度下降（PGD），这是一种标准的凸优化方法。
- en: 'Similar to extending the FGSM to its ‘one-step target class’ variation, Kurakin
    et al. [[35](#bib.bib35)] also extended BIM to Iterative Least-likely Class Method
    (ILCM). In that case, the label $\ell$ of the image in ([5](#S3.E5 "In 3.1.3 Basic
    & Least-Likely-Class Iterative Methods ‣ 3.1 Attacks for classification ‣ 3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")) is replaced by the target label $\ell_{\text{target}}$ of the least
    likely class predicted by the classifier. The adversarial examples generated by
    the ILCM method has been shown to seriously affect the classification accuracy
    of a modern deep architecture Inception v3 [[145](#bib.bib145)], even for very
    small values of $\epsilon$, e.g. $<16$.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于将 FGSM 扩展到其“单步目标类别”变体，Kurakin 等人 [[35](#bib.bib35)] 还将 BIM 扩展到了迭代最不可能类别方法（ILCM）。在这种情况下，图像的标签
    $\ell$（参见 [5](#S3.E5 "在 3.1.3 基本与最不可能类别迭代方法 ‣ 3.1 分类攻击 ‣ 3 对深度学习在计算机视觉中的对抗攻击的威胁：综述")）被分类器预测的最不可能类别的目标标签
    $\ell_{\text{target}}$ 替代。使用 ILCM 方法生成的对抗样本已被证明严重影响现代深度架构 Inception v3 [[145](#bib.bib145)]
    的分类准确性，即使对于非常小的 $\epsilon$ 值，例如 $<16$。
- en: 3.1.4 Jacobian-based Saliency Map Attack (JSMA)
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 基于雅可比矩阵的显著性图攻击（JSMA）
- en: In the literature, it is more common to generate adversarial examples by restricting
    $\ell_{\infty}$ or $\ell_{2}$-norms of the perturbations to make them imperceptible
    for humans. However, Papernot et al. [[60](#bib.bib60)] also created an adversarial
    attack by restricting the $\ell_{0}$-norm of the perturbations. Physically, it
    means that the goal is to modify only a few pixels in the image instead of perturbing
    the whole image to fool the classifier. The crux of their algorithm to generate
    the desired adversarial image can be understood as follows. The algorithm modifies
    pixels of the clean image one at a time and monitors the effects of the change
    on the resulting classification. The monitoring is performed by computing a saliency
    map using the gradients of the outputs of the network layers. In this map, a larger
    value indicates a higher likelihood of fooling the network to predict $\ell_{\text{target}}$
    as the label of the modified image instead of the original label $\ell$. Thus,
    the algorithm performs targeted fooling. Once the map has been computed, the algorithm
    chooses the pixel that is most effective to fool the network and alters it. This
    process is repeated until either the maximum number of allowable pixels are altered
    in the adversarial image or the fooling succeeds.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，通过限制$\ell_{\infty}$或$\ell_{2}$-范数的扰动来生成对抗样本，以使其对人类几乎不可察觉是更为常见的。然而，Papernot等人[[60](#bib.bib60)]还通过限制扰动的$\ell_{0}$-范数创建了一种对抗攻击。从物理角度来看，这意味着目标是仅修改图像中的几个像素，而不是扰动整个图像来欺骗分类器。他们生成所需对抗图像的算法核心可以理解如下。该算法一次修改干净图像中的一个像素，并监视变化对分类结果的影响。监视是通过计算网络层输出的梯度来生成显著性图完成的。在此图中，较大的值表示欺骗网络将$\ell_{\text{target}}$预测为修改后图像的标签而非原始标签$\ell$的可能性更高。因此，该算法执行了有针对性的欺骗。一旦计算出图，算法选择对欺骗网络最有效的像素并对其进行更改。此过程重复进行，直到在对抗图像中更改的像素达到最大允许数量或欺骗成功为止。
- en: 3.1.5 One Pixel Attack
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 单像素攻击
- en: 'An extreme case for the adversarial attack is when only one pixel in the image
    is changed to fool the classifier. Interestingly, Su et al. [[68](#bib.bib68)]
    claimed successful fooling of three different network models on $70.97\%$ of the
    tested images by changing just one pixel per image. They also reported that the
    average confidence of the networks on the wrong labels was found to be $97.47\%$.
    We show representative examples of the adversarial images from [[68](#bib.bib68)]
    in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.5 One Pixel Attack ‣ 3.1 Attacks for classification
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey"). Su et al. computed the adversarial examples by using the concept
    of Differential Evolution [[148](#bib.bib148)]. For a clean image ${\bf I}_{c}$,
    they first created a set of $400$ vectors in $\mathbb{R}^{5}$ such that each vector
    contained $xy$-coordinates and RGB values for an arbitrary candidate pixel. Then,
    they randomly modified the elements of the vectors to create children such that
    a child competes with its parent for fitness in the next iteration, while the
    probabilistic predicted label of the network is used as the fitness criterion.
    The last surviving child is used to alter the pixel in the image.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '对于对抗攻击的一个极端情况是，仅改变图像中的一个像素来欺骗分类器。有趣的是，Su等人[[68](#bib.bib68)]声称，通过每张图像仅更改一个像素，成功欺骗了三种不同的网络模型，在$70.97\%$的测试图像上。他们还报告了网络对错误标签的平均置信度为$97.47\%$。我们在图[3](#S3.F3
    "Figure 3 ‣ 3.1.5 One Pixel Attack ‣ 3.1 Attacks for classification ‣ 3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")中展示了来自[[68](#bib.bib68)]的对抗图像的代表性示例。Su等人通过使用差分进化[[148](#bib.bib148)]的概念计算了对抗样本。对于干净图像${\bf
    I}_{c}$，他们首先创建了一组$400$个在$\mathbb{R}^{5}$中的向量，其中每个向量包含了一个任意候选像素的$xy$-坐标和RGB值。然后，他们随机修改这些向量的元素以创建子代，使得子代在下一次迭代中与其父代竞争适应度，同时网络的概率预测标签作为适应度标准。最后幸存的子代用于更改图像中的像素。'
- en: '![Refer to caption](img/42b28391de9aabeb86852ef597413404.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/42b28391de9aabeb86852ef597413404.png)'
- en: 'Figure 3: Illustration of one pixel adversarial attacks [[68](#bib.bib68)]:
    The correct label is mentioned with each image. The corresponding predicted label
    is given in parentheses.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：单像素对抗攻击的示意图[[68](#bib.bib68)]：每张图像都标有正确的标签。相应的预测标签以括号形式给出。
- en: Even with such a simple evolutionary strategy Su et al. [[68](#bib.bib68)] were
    able to show successful fooling of deep networks. Notice that, differential evolution
    enables their approach to generate adversarial examples without having access
    to any information about the network parameter values or their gradients. The
    only input their technique requires is the probabilistic labels predicted by the
    targeted model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6 Carlini and Wagner Attacks (C&W)
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A set of three adversarial attacks were introduced by Carlini and Wagner [[36](#bib.bib36)]
    in the wake of defensive distillation against the adversarial perturbations [[38](#bib.bib38)].
    These attacks make the perturbations quasi-imperceptible by restricting their
    $\ell_{2}$, $\ell_{\infty}$ and $\ell_{0}$ norms, and it is shown that defensive
    distillation for the targeted networks almost completely fails against these attacks.
    Moreover, it is also shown that the adversarial examples generated using the unsecured
    (un-distilled) networks transfer well to the secured (distilled) networks, which
    makes the computed perturbations suitable for black-box attacks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Whereas it is more common to exploit the transferability property of adversarial
    examples to generate black-box attacks, Chen et al. [[41](#bib.bib41)] also proposed
    ‘Zeroth Order Optimization (ZOO)’ based attacks that directly estimate the gradients
    of the targeted model for generating the adversarial examples. These attacks were
    inspired by C&W attacks. We refer to the original papers for further details on
    C&W and ZOO attacks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.7 DeepFool
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Moosavi-Dezfooli et al. [[72](#bib.bib72)] proposed to compute a minimal norm
    adversarial perturbation for a given image in an iterative manner. Their algorithm,
    i.e. DeepFool initializes with the clean image that is assumed to reside in a
    region confined by the decision boundaries of the classifier. This region decides
    the class-label of the image. At each iteration, the algorithm perturbs the image
    by a small vector that is computed to take the resulting image to the boundary
    of the polyhydron that is obtained by linearizing the boundaries of the region
    within which the image resides. The perturbations added to the image in each iteration
    are accumulated to compute the final perturbation once the perturbed image changes
    its label according to the original decision boundaries of the network. The authors
    show that the DeepFool algorithm is able to compute perturbations that are smaller
    than the perturbations computed by FGSM [[23](#bib.bib23)] in terms of their norm,
    while having similar fooling ratios.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.8 Universal Adversarial Perturbations
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Whereas the methods like FGSM [[23](#bib.bib23)], ILCM [[35](#bib.bib35)],
    DeepFool [[72](#bib.bib72)] etc. compute perturbations to fool a network on a
    single image, the ‘universal’ adversarial perturbations computed by Moosavi-Dezfooli
    et al. [[16](#bib.bib16)] are able to fool a network on ‘any’ image with high
    probability. These image-agnostic perturbations also remain quasi-imperceptible
    for the human vision system, as can be observed in Fig. [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey"). To formally define these perturbations, let us assume that clean images
    are sampled from the distribution $\boldsymbol{\Im}_{c}$. A perturbation $\boldsymbol{\rho}$
    is ‘universal’ if it satisfies the following constraint:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '与 FGSM [[23](#bib.bib23)]、ILCM [[35](#bib.bib35)]、DeepFool [[72](#bib.bib72)]
    等方法通过计算扰动来欺骗网络的单张图像不同，Moosavi-Dezfooli 等人 [[16](#bib.bib16)] 计算的‘通用’对抗扰动能够以高概率欺骗网络上的‘任何’图像。这些与图像无关的扰动对于人类视觉系统仍然几乎不可察觉，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey") 所示。为了正式定义这些扰动，假设干净的图像是从分布 $\boldsymbol{\Im}_{c}$
    中采样的。一个扰动 $\boldsymbol{\rho}$ 是‘通用’的，如果它满足以下约束：'
- en: '|  | $\displaystyle\underset{{\bf I}_{c}\sim\boldsymbol{\Im}_{c}}{\mathrm{\text{P}}}\Big{(}\mathcal{C}({\bf
    I}_{c})\neq\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})\Big{)}\geq\delta~{}~{}~{}\text{s.t.}~{}~{}&#124;&#124;\boldsymbol{\rho}&#124;&#124;_{p}\leq\xi,$
    |  | (6) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{{\bf I}_{c}\sim\boldsymbol{\Im}_{c}}{\mathrm{\text{P}}}\Big{(}\mathcal{C}({\bf
    I}_{c})\neq\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})\Big{)}\geq\delta~{}~{}~{}\text{s.t.}~{}~{}&#124;&#124;\boldsymbol{\rho}&#124;&#124;_{p}\leq\xi,$
    |  | (6) |'
- en: 'where P(.) denotes the probability, $\delta\in(0,1]$ is the fooling ratio,
    $||.||_{p}$ denotes the $\ell_{p}$-norm and $\xi$ is a pre-defined constant. The
    smaller the value of $\xi$, the harder it is to perceive the perturbation in the
    image. Strictly speaking, the perturbations that satisfy ([6](#S3.E6 "In 3.1.8
    Universal Adversarial Perturbations ‣ 3.1 Attacks for classification ‣ 3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")) should be referred to as $(\delta,\xi)$-universal because of their strong
    dependence on the mentioned parameters. However, these perturbations are commonly
    referred to as the ‘universal adversarial perturbations’ in the literature.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 P(.) 表示概率，$\delta\in(0,1]$ 是欺骗率，$||.||_{p}$ 表示 $\ell_{p}$-范数，$\xi$ 是预定义常量。$\xi$
    的值越小，图像中扰动的感知难度越大。严格来说，满足 ([6](#S3.E6 "In 3.1.8 Universal Adversarial Perturbations
    ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")) 的扰动应称为 $(\delta,\xi)$-通用，因为它们对这些参数有很强的依赖。然而，这些扰动在文献中通常被称为‘通用对抗扰动’。'
- en: The authors computed the universal perturbations by restricting their $\ell_{2}$-norm
    as well as $\ell_{\infty}$-norm, and showed that the perturbations with their
    norms upper bounded by $4\%$ of the respective image norms already achieved significant
    fooling ratios of around 0.8 or more for state-of-the-art image classifiers. Their
    iterative approach to compute a perturbation is related to the DeepFool strategy [[72](#bib.bib72)]
    of gradually pushing a data point (i.e. an image) to the decision boundary for
    its class. However, in this case, ‘all’ the training data points are sequentially
    pushed to the respective decision boundaries and the perturbations computed over
    all the images are gradually accumulated by back-projecting the accumulator to
    the desired $\ell_{p}$ ball of radius $\xi$ every time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过限制其 $\ell_{2}$-范数以及 $\ell_{\infty}$-范数来计算通用扰动，并展示了其范数上界为图像范数的 $4\%$ 的扰动在最先进的图像分类器中已经实现了约
    0.8 或更高的显著欺骗率。他们的迭代计算扰动的方法与 DeepFool 策略 [[72](#bib.bib72)] 相关，该策略通过逐步推送数据点（即图像）到其类别的决策边界来计算扰动。然而，在这种情况下，‘所有’训练数据点都被顺序推送到各自的决策边界，并且计算的扰动通过每次将累加器反投影到所需的
    $\ell_{p}$ 半径为 $\xi$ 的球体上来逐步累积。
- en: The algorithm proposed by Moosavi-Dezfooli et al. [[16](#bib.bib16)] computes
    perturbations while targeting a single network model, e.g. ResNet [[147](#bib.bib147)].
    However, it is shown that these perturbations also generalize well across different
    networks (especially those having similar architectures). In that sense, the author’s
    claim the perturbations to be, to some extent, ‘doubly universal’. Moreover, it
    is also shown that high fooling ratio (e.g. $\delta\geq 0.5$) is achievable by
    learning a perturbation using only around $2,000$ training images.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
- en: Khrulkov et al. [[190](#bib.bib190)] also proposed a method for constructing
    universal adversarial perturbations as singular vectors of the Jacobian matrices
    of feature maps of the networks, which allowed for achieving relatively high fooling
    rates using only a small number of images. Another method to generate universal
    perturbations is fast-feature-fool by Mopuri et al. [[135](#bib.bib135)]. Their
    method generates the universal perturbations independent of data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.9 UPSET and ANGRI
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Sarkar et al. [[146](#bib.bib146)] proposed two black-box attack algorithms,
    namely UPSET: Universal Perturbations for Steering to Exact Targets, and ANGRI:
    Antagonistic Network for Generating Rogue Images for targeted fooling of deep
    neural networks. For ‘n’ classes, UPSET seeks to produce ‘n’ image-agnostic perturbations
    such that when the perturbation is added to an image that does not belong to a
    targeted class, the classifier will classify the perturbed image as being from
    that class. The power of UPSET comes from a residual generating network R(.),
    that takes the target class ‘t’ as input and produces a perturbation R(t) for
    fooling. The overall method solves the following optimization problem using the
    so-called UPSET network:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle{\bf I}_{\boldsymbol{\rho}}=\max(\min(s\text{R(t)}+{\bf
    I}_{c},1),-1),$ |  | (7) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
- en: where the pixel values in ${\bf I}_{c}$ are normalized to lie in $[-1,1]$, and
    ‘$s$’ is a scalar. To ensure ${\bf I}_{\boldsymbol{\rho}}$ to be a valid image,
    all values outside the interval $[-1,1]$ are clipped. As compared to the image-agnostic
    perturbations of UPSET, ANGRI computes image-specific perturbations in a closely
    related manner, for which we refer to the original work. The perturbations resulting
    from ANGRI are also used for targeted fooling. Both algorithms have been reported
    to achieve high fooling ratios on MNIST [[10](#bib.bib10)] and CIFAR-10 [[152](#bib.bib152)]
    datasets.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.10 Houdini
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cisse et al. [[131](#bib.bib131)] proposed ‘Houdini’- an approach for fooling
    gradient-based learning machines by generating adversarial examples that can be
    tailored to task losses. Typical algorithms that generate adversarial examples
    employ gradients of differentiable loss functions of the networks to compute the
    perturbations. However, task losses are often not amenable to this approach. For
    instance, the task loss of speech recognition is based on word-error-rate, which
    does not allow straightforward exploitation of loss function gradient. Houdini
    is tailored to generate adversarial examples for such tasks. Besides successful
    generation of adversarial images for classification, Houdini has also been shown
    to successfully attack a popular deep Automatic Speech Recognition system [[151](#bib.bib151)].
    The authors have also demonstrated the transferability of attacks in speech recognition
    by fooling Google Voice in a black-box attack scenario. Moreover, successful targeted
    and non-targeted attacks are also demonstrated for a deep learning model for human
    pose estimation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.11 Adversarial Transformation Networks (ATNs)
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Baluja and Fischer [[42](#bib.bib42)] trained feed-forward neural networks to
    generate adversarial examples against other targeted networks or set of networks.
    The trained models were termed Adversarial Transformation Networks (ATNs). The
    adversarial examples generated by these networks are computed by minimizing a
    joint loss function comprising of two parts. The first part restricts the adversarial
    example to have perceptual similarity with the original image, whereas the second
    part aims at altering the prediction of the targeted network on the resulting
    image.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: Along the same direction, Hayex and Danezis [[47](#bib.bib47)] also used an
    attacker neural network to learn adversarial examples for black-box attacks. In
    the presented results, the examples computed by the attacker network remain perceptually
    indistinguishable from the clean images but they are misclassified by the targeted
    networks with overwhelming probabilities - reducing classification accuracy from
    99.4% to 0.77% on MNIST data [[10](#bib.bib10)], and from 91.4% to 6.8% on the
    CIFAR-10 dataset [[152](#bib.bib152)].
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Summary of the attributes of diverse attacking methods: The ‘perturbation
    norm’ indicates the restricted $\ell_{p}$-norm of the perturbations to make them
    imperceptible. The strength (higher for more asterisks) is based on the impression
    from the reviewed literature.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Black/White box | Targeted/Non-targeted | Specific/Universal | Perturbation
    norm | Learning | Strength |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
- en: '| L-BFGS [[22](#bib.bib22)] | White box | Targeted | Image specific | $\ell_{\infty}$
    | One shot | $***$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
- en: '| FGSM [[23](#bib.bib23)] | White box | Targeted | Image specific | $\ell_{\infty}$
    | One shot | $***$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
- en: '| BIM & ILCM [[35](#bib.bib35)] | White box | Non targeted | Image specific
    | $\ell_{\infty}$ | Iterative | $**$$**$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| BIM & ILCM [[35](#bib.bib35)] | 白盒 | 非定向攻击 | 图像特定 | $\ell_{\infty}$ | 迭代式
    | $**$$**$ |'
- en: '| JSMA [[60](#bib.bib60)] | White box | Targeted | Image specific | $\ell_{0}$
    | Iterative | $***$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| JSMA [[60](#bib.bib60)] | 白盒 | 定向攻击 | 图像特定 | $\ell_{0}$ | 迭代式 | $***$ |'
- en: '| One-pixel [[68](#bib.bib68)] | Black box | Non Targeted | Image specific
    | $\ell_{0}$ | Iterative | $**$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| One-pixel [[68](#bib.bib68)] | 黑盒 | 非定向攻击 | 图像特定 | $\ell_{0}$ | 迭代式 | $**$
    |'
- en: '| C&W attacks [[36](#bib.bib36)] | White box | Targeted | Image specific |
    $\ell_{0},\ell_{2},\ell_{\infty}$ | Iterative | $*****$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| C&W 攻击 [[36](#bib.bib36)] | 白盒 | 定向攻击 | 图像特定 | $\ell_{0},\ell_{2},\ell_{\infty}$
    | 迭代式 | $*****$ |'
- en: '| DeepFool [[72](#bib.bib72)] | White box | Non targeted | Image specific |
    $\ell_{2},\ell_{\infty}$ | Iterative | $**$$**$ |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| DeepFool [[72](#bib.bib72)] | 白盒 | 非定向攻击 | 图像特定 | $\ell_{2},\ell_{\infty}$
    | 迭代式 | $**$$**$ |'
- en: '| Uni. perturbations [[16](#bib.bib16)] | White box | Non targeted | Universal
    | $\ell_{2},\ell_{\infty}$ | Iterative | $*****$ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Uni. 扰动 [[16](#bib.bib16)] | 白盒 | 非定向攻击 | 通用 | $\ell_{2},\ell_{\infty}$ |
    迭代式 | $*****$ |'
- en: '| UPSET [[146](#bib.bib146)] | Black box | Targeted | Universal | $\ell_{\infty}$
    | Iterative | $**$$**$ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| UPSET [[146](#bib.bib146)] | 黑盒 | 定向攻击 | 通用 | $\ell_{\infty}$ | 迭代式 | $**$$**$
    |'
- en: '| ANGRI [[146](#bib.bib146)] | Black box | Targeted | Image specific | $\ell_{\infty}$
    | Iterative | $**$$**$ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ANGRI [[146](#bib.bib146)] | 黑盒 | 定向攻击 | 图像特定 | $\ell_{\infty}$ | 迭代式 | $**$$**$
    |'
- en: '| Houdini [[131](#bib.bib131)] | Black box | Targeted | Image specific | $\ell_{2},\ell_{\infty}$
    | Iterative | $**$$**$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Houdini [[131](#bib.bib131)] | 黑盒 | 定向攻击 | 图像特定 | $\ell_{2},\ell_{\infty}$
    | 迭代式 | $**$$**$ |'
- en: '| ATNs [[42](#bib.bib42)] | White box | Targeted | Image specific | $\ell_{\infty}$
    | Iterative | $**$$**$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ATNs [[42](#bib.bib42)] | 白盒 | 定向攻击 | 图像特定 | $\ell_{\infty}$ | 迭代式 | $**$$**$
    |'
- en: 3.1.12 Miscellaneous Attacks
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.12 杂项攻击
- en: 'The adversarial attacks discussed above are either the popular ones in the
    recent literature or they are representative of the research directions that are
    fast becoming popular. A summary of the main attributes of these attacks is also
    provided in Table [I](#S3.T1 "TABLE I ‣ 3.1.11 Adversarial Transformation Networks
    (ATNs) ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"). For a comprehensive study,
    below we provide brief descriptions of further techniques to generate adversarial
    attacks on deep neural networks. We note that this research area is currently
    highly active. Whereas every attempt has been made to review as many approaches
    as possible, we do not claim the review to be exhaustive. Due to high activity
    in this research direction, many more attacks are likely to surface in the near
    future.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '上述讨论的对抗攻击要么是最近文献中的流行攻击，要么是代表性地快速变得流行的研究方向。这些攻击的主要特征的总结也提供在表[I](#S3.T1 "TABLE
    I ‣ 3.1.11 Adversarial Transformation Networks (ATNs) ‣ 3.1 Attacks for classification
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")中。为了全面研究，下面我们提供进一步生成对抗攻击深度神经网络的技术的简要描述。我们注意到，这一研究领域目前非常活跃。尽管尽力回顾了尽可能多的方法，但我们不声称回顾是详尽的。由于这一研究方向的高活动性，未来可能会出现更多攻击。'
- en: Sabour et al. [[26](#bib.bib26)] showed the possibility of generating adversarial
    examples by altering the internal layers of deep neural networks. The authors
    demonstrated that it is possible to make internal network representation of adversarial
    images to resemble representations of images from different classes. Papernot
    et al. [[109](#bib.bib109)] studied transferability of adversarial attacks for
    deep learning as well as other machine learning techniques and introduced further
    transferability attacks. Narodytska and Kasiviswanathan [[54](#bib.bib54)] also
    introduced further black-box attacks that have been found effective in fooling
    the neural networks by changing only few pixel values in the images. Liu et al. [[31](#bib.bib31)]
    introduced ‘epsilon-neighborhood’ attack that have been shown to fool defensively
    distilled networks [[108](#bib.bib108)] with $100\%$ success for white-box attacks.
    Oh et al. [[133](#bib.bib133)] took a ‘Game Theory’ perspective on adversarial
    attacks and derived a strategy to counter the counter-measures taken against adversarial
    attacks on deep neural networks. Mpouri et al. [[135](#bib.bib135)] developed
    a data-independent approach to generate universal adversarial perturbations for
    the deep network models. Hosseini et al. [[98](#bib.bib98)] introduced the notion
    of ‘semantic adversarial examples’ - input images that represent semantically
    same objects for humans but deep neural networks misclassify them. They used negatives
    of the images as semantic adversarial examples. Kanbak et al. [[73](#bib.bib73)]
    introduced ‘ManiFool’ algorithm in the wake of DeepFool method [[72](#bib.bib72)]
    to measure robustness of deep neural networks against geometrically perturbed
    images. Dong et al. [[170](#bib.bib170)] proposed an iterative method to boost
    adversarial attacks for black-box scenarios. Recently, Carlini and Wagner [[59](#bib.bib59)]
    also demonstrated that ten different defenses against perturbations can again
    be defeated by new attacks constructed using new loss functions. Rozsa et al. [[94](#bib.bib94)]
    also proposed a ‘hot/cold’ method for generating multiple possible adversarial
    examples for a single image. Interestingly, adversarial perturbations are not
    only being added to images to reduces the accuracy of deep learning classifiers.
    Yoo et al. [[195](#bib.bib195)] recently proposed an approach to also slightly
    improve the classification performance with the help of subtle perturbation to
    images.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Sabour 等人 [[26](#bib.bib26)] 展示了通过改变深度神经网络的内部层可以生成对抗性示例的可能性。作者们证明了可以使对抗性图像的内部网络表示与来自不同类别的图像的表示相似。Papernot
    等人 [[109](#bib.bib109)] 研究了对深度学习以及其他机器学习技术的对抗性攻击的可转移性，并引入了进一步的可转移攻击。Narodytska
    和 Kasiviswanathan [[54](#bib.bib54)] 也引入了进一步的黑盒攻击方法，这些攻击方法通过仅更改图像中的少数像素值就能成功愚弄神经网络。Liu
    等人 [[31](#bib.bib31)] 提出了‘epsilon-邻域’攻击方法，已被证明可以以$100\%$的成功率愚弄防御性精馏网络 [[108](#bib.bib108)]
    进行白盒攻击。Oh 等人 [[133](#bib.bib133)] 从‘博弈论’的角度对对抗性攻击进行了研究，并推导出了对抗性攻击深度神经网络所采取的反措施的策略。Mpouri
    等人 [[135](#bib.bib135)] 开发了一种数据独立的方法来生成深度网络模型的通用对抗扰动。Hosseini 等人 [[98](#bib.bib98)]
    提出了‘语义对抗性示例’的概念 - 输入图像对于人类来说代表语义相同的对象，但深度神经网络会误分类它们。他们使用图像的负面作为语义对抗性示例。Kanbak
    等人 [[73](#bib.bib73)] 在DeepFool方法 [[72](#bib.bib72)] 后提出了‘ManiFool’算法，用于测量深度神经网络对几何扰动图像的稳健性。Dong
    等人 [[170](#bib.bib170)] 提出了一种迭代方法，以增强黑盒场景下的对抗攻击。最近，Carlini 和 Wagner [[59](#bib.bib59)]
    也证明了十种不同的抵御扰动的方法可以通过使用新的损失函数构造的新攻击再次被打败。Rozsa 等人 [[94](#bib.bib94)] 还提出了一种‘热/冷’方法，用于生成单个图像的多个可能对抗性示例。有趣的是，对抗性扰动不仅仅是为了降低深度学习分类器的准确性。Yoo
    等人 [[195](#bib.bib195)] 最近提出了一种方法，通过对图像进行微小扰动，还能略微提高分类性能。
- en: We note that the authors of many works reviewed in this article have made the
    source code of their implementations publicly available. This is one of the major
    reasons behind the current rise in this research direction. Beside those resources,
    there are also libraries, e.g. Cleverhans [[111](#bib.bib111)], [[112](#bib.bib112)]
    that have started emerging in order to further boost this research direction.
    Adversarial-Playground ([https://github.com/QData/AdversarialDNN-Playground](https://github.com/QData/AdversarialDNN-Playground))
    is another example of a toolbox made public by Norton and Qi [[142](#bib.bib142)]
    to understand adversarial attacks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Attacks beyond classification/recognition
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the exception of Houdini [[131](#bib.bib131)], all the mainstream adversarial
    attacks reviewed in Section [3.1](#S3.SS1 "3.1 Attacks for classification ‣ 3
    Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey") directly focused on the task of classification - typically
    fooling CNN-based [[10](#bib.bib10)] classifiers. However, due to the seriousness
    of adversarial threats, attacks are also being actively investigated beyond the
    classification/recognition task in Computer Vision. Below, we review the works
    that develop approaches to attack deep neural networks beyond classification.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1 Attacks on Autoencoders and Generative Models
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Tabacof et al. [[128](#bib.bib128)] investigated adversarial attacks for autoencoders [[154](#bib.bib154)],
    and proposed a technique to distort input image (to make it adversarial) that
    misleads the autoencoder to reconstruct a completely different image. Their approach
    attacks the internal representation of a neural network such that the representation
    for the adversarial image becomes similar to that of the target image. However,
    it is reported in [[128](#bib.bib128)] that autoencoders seem to be much more
    robust to adversarial attacks than the typical classifier networks. Kos et al. [[121](#bib.bib121)]
    also explored methods for computing adversarial examples for deep generative models,
    e.g. variational autoencoder (VAE) and the VAE-Generative Adversarial Networks
    (VAE-GANs). GANs, such as [[153](#bib.bib153)] are becoming exceedingly popular
    now-a-days in Computer Vision applications due to their ability to learn data
    distributions and generate realistic images using those distributions. The authors
    introduced three different classes of attacks for VAE and VAE-GANs. Owing to the
    success of these attacks it is concluded that the deep generative models are also
    vulnerable to adversaries that can convince them to turn inputs into very different
    outputs. This work adds further support to the hypothesis that “adversarial examples
    are a general phenomenon for current neural network architectures”.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Attack on Recurrent Neural Networks
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Papernot et al. [[110](#bib.bib110)] successfully generated adversarial input
    sequences for Recurrent Neural Networks (RNNs). RNNs are deep learning models
    that are particularly suitable for learning mappings between sequential inputs
    and outputs [[155](#bib.bib155)]. Papernot et al. demonstrated that the algorithms
    proposed to compute adversarial examples for the feed-forward neural networks
    (e.g. FGSM [[23](#bib.bib23)]) can also be adapted for fooling RNNs. In particular,
    the authors demonstrated successful fooling of the popular Long-Short-Term-Memory
    (LSTM) RNN architecture [[156](#bib.bib156)]. It is concluded that the cyclic
    neural network model like RNNs are also not immune to the adversarial perturbations
    that were originally uncovered in the context of acyclic neural networks, i.e. CNNs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.3 Attacks on Deep Reinforcement Learning
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Lin et al. [[134](#bib.bib134)] proposed two different adversarial attacks
    for the agents trained by deep reinforcement learning [[157](#bib.bib157)]. In
    the first attack, called ‘strategically-timed attack’, the adversary minimizes
    the reward of the agent by attacking it at a small subset of time steps in an
    episode. A method is proposed to determine when an adversarial example should
    be crafted and applied, which enables the attack to go undetected. In the second
    attack, referred as ‘enchanting attack’, the adversary lures the agent to a designated
    target state by integrating a generative model and a planning algorithm. The generative
    model is used for predicting the future states of the agent, whereas the planning
    algorithm generates the actions for luring it. The attacks are successfully tested
    against the agents trained by the state-of-the-art deep reinforcement learning
    algorithms [[157](#bib.bib157)], [[158](#bib.bib158)]. Details on this work and
    example videos of the adversarial attacks can be found on the following URL: [http://yclin.me/adversarial_attack_RL/](http://yclin.me/adversarial_attack_RL/).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: In another work, Huang et al. [[62](#bib.bib62)] demonstrated that FGSM [[23](#bib.bib23)]
    can also be used to significantly degrade performance of trained policies in the
    context of deep reinforcement learning. Their threat model considers adversaries
    that are capable of introducing minor perturbations to the raw input of the policy.
    The conducted experiments demonstrate that it is fairly easy to confuse neural
    network policies with adversarial examples, even in black-box scenarios. Videos
    and further details on this work are available on [http://rll.berkeley.edu/adversarial/](http://rll.berkeley.edu/adversarial/).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.4 Attacks on Semantic Segmentation and Object Detection
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Semantic image segmentation and object detection are among the mainstream problems
    in Computer Vision. Inspired by Moosavi-Dezfooli [[16](#bib.bib16)], Metzen et
    al. [[67](#bib.bib67)] showed the existence of image-agnostic quasi-imperceptible
    perturbations that can fool a deep neural network into significantly corrupting
    the predicted segmentation of the images. Moreover, they also showed that it is
    possible to compute noise vectors that can remove a specific class from the segmented
    classes while keeping most of the image segmentation unchanged (e.g. removing
    pedestrians from road scenes). Although it is argued that the “space of the adversarial
    perturbations for the semantic image segmentation is presumably smaller than image
    classification”, the perturbations have been shown to generalize well for unseen
    validation images with high probability. Arnab et al. [[51](#bib.bib51)] also
    evaluated FGSM [[23](#bib.bib23)] based adversarial attacks for semantic segmentation
    and noted that many observations about these attacks for classification do not
    directly transfer to segmentation task.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: 'Xie et al. [[115](#bib.bib115)] computed adversarial examples for semantic
    segmentation and object detection under the observation that these tasks can be
    formulated as classifying multiple targets in an image - the target is a pixel
    or a receptive field in segmentation, and object proposal in detection. Under
    this perspective, their approach, called ‘Dense Adversary Generation’ optimizes
    a loss function over a set of pixels/proposals to generate adversarial examples.
    The generated examples are tested to fool a variety of deep learning based segmentation
    and detection approaches. Their experimental evaluation not only demonstrates
    successful fooling of the targeted networks but also shows that the generated
    perturbations generalize well across different network models. In Fig. [4](#S3.F4
    "Figure 4 ‣ 3.2.4 Attacks on Semantic Segmentation and Object Detection ‣ 3.2
    Attacks beyond classification/recognition ‣ 3 Adversarial attacks ‣ Threat of
    Adversarial Attacks on Deep Learning in Computer Vision: A Survey"), we show a
    representative example of network fooling for segmentation and detection using
    the approach in [[115](#bib.bib115)].'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8daade2dc7ab4ea04f130e8a46e5ba31.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Adversarial example for semantic segmentation and object detection [[115](#bib.bib115)].
    FCN [[159](#bib.bib159)] and Faster-RCNN [[150](#bib.bib150)] are used for segmentation
    and detection, respectively. Left column (top-down): Clean image, normal segmentation
    (purple region is predicted as dog) and detection results. Right column (top-down):
    Perturbation 10x, fooled segmentation (light green region is predicted as train
    and the pink region as person) and detection results.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ffb8e5af69b7ac91459d86c7675fd2db.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Top-row: Example of changing a facial attribute ‘wearing lipstick’
    to ‘not wearing lipstick’ by Fast Flipping Attribute method [[130](#bib.bib130)].
    Bottom row: Changing gender with perturbation generated by [[162](#bib.bib162)].'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 4 Attacks in the real world
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2447e21608fa97547d8d052ce125bb0b.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Example of adversarial attack on mobile phone cameras: A clean image
    (a) was taken and used to generate different adversarial images. The images were
    printed and the TensorFlow Camera Demo app [[181](#bib.bib181)] was used to classify
    them. A clean image (b) is recognized correctly as a ‘washer’ when perceived through
    the camera, whereas adversarial images (c) and (d) are mis-classified. The images
    also show network confidence in the range [0,1] for each image. The value of $\epsilon$
    is given for ([3](#S3.E3 "In 3.1.2 Fast Gradient Sign Method (FGSM) ‣ 3.1 Attacks
    for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on
    Deep Learning in Computer Vision: A Survey")).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: 4.0.1 Attacks on Face Attributes
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Face attributes are among the emerging soft biometrics for modern security systems.
    Although face attribute recognition can also be categorized as a classification
    problem, we separately review some interesting attacks in this direction because
    face recognition itself is treated as a mainstream problem in Computer Vision.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Rozsa et al. [[130](#bib.bib130)], [[160](#bib.bib160)] explored the stability
    of multiple deep learning approaches using the CelebA benchmark [[161](#bib.bib161)]
    by generating adversarial examples to alter the results of facial attribute recognition,
    see top-row in Fig. [5](#S3.F5 "Figure 5 ‣ 3.2.4 Attacks on Semantic Segmentation
    and Object Detection ‣ 3.2 Attacks beyond classification/recognition ‣ 3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey"). By attacking the deep network classifiers with their so-called ‘Fast
    Flipping Attribute’ technique, they found that robustness of deep neural networks
    against the adversarial attacks varies highly between facial attributes. It is
    claimed that adversarial attacks are very effective in changing the label of a
    target attribute to a correlated attribute. Mirjalili and Ross [[162](#bib.bib162)]
    proposed a technique that modifies a face image such that its gender (for a gender
    classifier) is modified, whereas its biometric utility for a face matching system
    remains intact, see bottom-row in Fig. [5](#S3.F5 "Figure 5 ‣ 3.2.4 Attacks on
    Semantic Segmentation and Object Detection ‣ 3.2 Attacks beyond classification/recognition
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey"). Similarly, Shen et al. [[144](#bib.bib144)] proposed two different
    techniques to generate adversarial examples for faces that can have high ‘attractiveness
    scores’ but low ‘subjective scores’ for the face attractiveness evaluation using
    deep neural network. We refer to [[185](#bib.bib185)] for further attacks related
    to the task of face recognition.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: 'The literature reviewed in Section [3](#S3 "3 Adversarial attacks ‣ Threat
    of Adversarial Attacks on Deep Learning in Computer Vision: A Survey") assumes
    settings where adversaries directly feed deep neural networks with perturbed images.
    Moreover, the effectiveness of attacks are also evaluated using standard image
    databases. Whereas those settings have proven sufficient to convince many researchers
    that adversarial attacks are a real concern for deep learning in practice, we
    also come across instances in the literature (e.g. [[48](#bib.bib48)], [[30](#bib.bib30)])
    where this concern is down-played and adversarial examples are implicated to be
    ‘only a matter of curiosity’ with little practical concerns. Therefore, this Section
    is specifically dedicated to the literature that deals with the adversarial attacks
    in practical real-world conditions to help settle the debate.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Cell-phone camera attack
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Kurakin et al. [[35](#bib.bib35)] first demonstrated that threats of adversarial
    attacks also exist in the physical world. To illustrate this, they printed adversarial
    images and took snapshots from a cell-phone camera. These images were fed to TensorFlow
    Camera Demo app [[181](#bib.bib181)] that uses Google’s Inception model [[145](#bib.bib145)]
    for object classification. It was shown that a large fraction of images were misclassified
    even when perceived through the camera. In Fig. [6](#S4.F6 "Figure 6 ‣ 4 Attacks
    in the real world ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey"), an example is shown from the original paper. A video is also
    provided on the following URL [https://youtu.be/zQ_uMenoBCk](https://youtu.be/zQ_uMenoBCk)
    that shows the threat of adversarial attacks with further images. This work studies
    FGSM [[23](#bib.bib23)], BIM and ILCM [[35](#bib.bib35)] methods for attacks in
    the physical world.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/201d9e8eb9f8745a13743cd1f5b32569.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Example of road sign attack [[75](#bib.bib75)]: The success rate
    of fooling LISA-CNN [[75](#bib.bib75)] classifier on all the shown images is $100\%$.
    The distance and angle to the camera are also shown. The classifier is trained
    using LISA dataset for road signs [[176](#bib.bib176)].'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Road sign attack
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Etimov et al. [[75](#bib.bib75)] built on the attacks proposed in [[36](#bib.bib36)]
    and [[88](#bib.bib88)] to design robust perturbations for the physical world.
    They demonstrated the possibility of attacks that are robust to physical conditions,
    such as variation in view angles, distance and resolution. The proposed algorithm,
    termed RP[2] for Robust Physical Perturbations, was used to generate adversarial
    examples for road sign recognition systems that achieved high fooling ratios in
    practical drive-by settings. Two attack classes were introduced in this work for
    the physical road signs, (a) poster-printing: where the attacker prints a perturbed
    road sign poster and places it over the real sign (see Fig. [7](#S4.F7 "Figure
    7 ‣ 4.1 Cell-phone camera attack ‣ 4 Attacks in the real world ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")), (b) sticker perturbation:
    where the printing is done on a paper and the paper is stuck over the real sign.
    For (b) two types of perturbations were studied, (b1) subtle perturbations: that
    occupied the entire sign and (b2) camouflage perturbations: that took the form
    of graffiti sticker on the sign. As such, all these perturbations require access
    to a color printer and no other special hardware. Successful generation of perturbations
    for both (a) and (b) such that the perturbations remained robust to natural variations
    in the physical world demonstrate the threat of adversarial example in the real
    world. We refer to the following URL for further details and videos related to
    this work: [https://iotsecurity.eecs.umich.edu/#roadsigns](https://iotsecurity.eecs.umich.edu/#roadsigns).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'It should be noted that Lu et al. [[30](#bib.bib30)] had previously claimed
    that adversarial examples are not a concern for object detection in Autonomous
    Vehicles because of the changing physical conditions in a moving car. However,
    the attacking methods they employed [[22](#bib.bib22)], [[23](#bib.bib23)], [[35](#bib.bib35)]
    were somewhat primitive. The findings of Etimov et al. [[75](#bib.bib75)] are
    orthogonal to the results in [[66](#bib.bib66)]. However, in a follow-up work
    Lu et al. [[19](#bib.bib19)] showed that the detectors like YOLO 9000 [[149](#bib.bib149)]
    and FasterRCNN [[150](#bib.bib150)] are ‘currently’ not fooled by the attacks
    introduced by Etimov et al. [[75](#bib.bib75)]. Zeng et al. [[87](#bib.bib87)]
    also argue that adversarial perturbations in the image space do not generalize
    well in the physical space of the real-world. However, Athalye et al. [[65](#bib.bib65)]
    showed that we can actually print 3D physical objects for successful adversarial
    attacks in the physical world. We discuss [[65](#bib.bib65)] in Section [4.3](#S4.SS3
    "4.3 Generic adversarial 3D objects ‣ 4 Attacks in the real world ‣ Threat of
    Adversarial Attacks on Deep Learning in Computer Vision: A Survey").'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Gu et al. [[33](#bib.bib33)] also explored an interesting notion of threats
    to outsourced training of the neural networks in the context of fooling neural
    networks on street signs. They showed that it is possible to train a network (a
    *BadNet*) that shows state-of-the-art performance on the user’s training and validation
    samples, but behaves badly on attacker-chosen inputs. They demonstrated this attack
    in a realistic scenario by creating a street sign classifier that identifies stop
    signs as speed limits when a special sticker is added to the stop sign. Moreover,
    it was found that the fooling of the network persisted to a reasonable extent
    even when the network was later fine-tuned with additional training data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Generic adversarial 3D objects
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Athalye et al. [[65](#bib.bib65)] introduced a method for constructing 3D objects
    that can fool neural networks across a wide variety of angles and viewpoints.
    Their ‘Expectation Over Transformation’ (EOT) framework is able to construct examples
    that are adversarial over an entire distribution of image/object transformations.
    Their end-to-end approach is able to print arbitrary adversarial 3D objects. In
    our opinion, results of this work ascertain that adversarial attacks are a real
    concern for deep learning in the physical world. In Fig. [8](#S4.F8 "Figure 8
    ‣ 4.3 Generic adversarial 3D objects ‣ 4 Attacks in the real world ‣ Threat of
    Adversarial Attacks on Deep Learning in Computer Vision: A Survey") we show an
    example of 3D-printed turtle that is modified by EOT framework to be classified
    as rifle. A video demonstrating the fooling by EOT in the physical world is available
    at the following URL: [https://www.youtube.com/watch?v=YXy6oX1iNoA&feature=youtu.be](https://www.youtube.com/watch?v=YXy6oX1iNoA&feature=youtu.be).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f3f402a58f81966529e4d1e80c88f431.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Different random poses of a 3D-printed turtle perturbed by EOT [[65](#bib.bib65)]
    to be classified as a rifle by an ImageNet classifier. The unperturbed version
    (not shown) is classified correctly with $100\%$ probability.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Cyberspace attacks
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Papernot et al. [[39](#bib.bib39)] launched one of the first attacks against
    the deep neural network classifiers in cyberspace in the real-world settings.
    They trained a substitute network for the targeted black-box classifier on synthetic
    data, and instantiated the attack against remotely hosted neural networks by MetaMind,
    Amazon and Google. They were able to show that the respective targeted networks
    misclassified $84.24\%$, $96.19\%$ and $88.94\%$ of the adversarial examples generated
    by their method. Indeed, the only information available to the attacker in their
    threat model was the output label of the targeted network for the input image
    fed by the attacker. In a related work, Liu et al. [[88](#bib.bib88)] developed
    an ensemble based attack and showed its success against Clarifai.com - a commercial
    company providing state-of-the-art image classification services. The authors
    claim that their attacks for both targeted and non-targeted fooling are able to
    achieve high success rates.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Grosse et al. [[61](#bib.bib61)] showed construction of effective adversarial
    attacks for neural networks used as malware classifiers. As compared to image
    recognition, the domain of malware classification introduces additional constraints
    in the adversarial settings, e.g. continuous input domains are replaced by discrete
    inputs, the condition of visual similarity is replaced by requiring equivalent
    functional behavior. However, Grosse et al. [[61](#bib.bib61)] showed that creating
    effective adversarial examples is still possible for maleware classification.
    Further examples of successful adversarial attacks against deep lrearning based
    malware classification can also be found in [[64](#bib.bib64)], [[107](#bib.bib107)],
    [[125](#bib.bib125)].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Robotic Vision & Visual QA Attacks
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Melis et al. [[63](#bib.bib63)] demonstrated the vulnerability of robots to
    the adversarial manipulations of the input images using the techniques in [[22](#bib.bib22)].
    The authors argue that strategies to enforce deep neural networks to learn more
    stable representations are necessary for secure robotics. Xu et al. [[40](#bib.bib40)]
    generated adversarial attacks for the Visual Turing Test, also known as ‘Visual
    Question Answer’ (VQA). The authors show that the commonly used compositional
    and non-compositional VQA architectures that employ deep neural networks are vulnerable
    to adversarial attacks. Moreover, the adversarial examples are transferable between
    the models. They conclude that the “adversarial examples pose real threats to
    not only image classification models, but also more complicated VQA models” [[63](#bib.bib63)].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 5 Existence of adversarial examples
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the literature related to adversarial attacks on deep learning in Computer
    Vision, there are varied views on the existence of adversarial examples. These
    views generally align well with the local empirical observations made by the researchers
    while attacking or defending the deep neural networks. However, they often fall
    short in terms of generalization. For instance, the popular linearity hypothesis
    of Goodfellow et al. [[23](#bib.bib23)] explains the FGSM and related attacks
    very well. However, Tanay and Griffin [[74](#bib.bib74)] demonstrated image classes
    that do not suffer from adversarial examples for linear classifier, which is not
    in-line with the linearity hypothesis. Not to mention, the linearity hypothesis
    itself deviates strongly from the previously prevailing opinion that the adversarial
    examples stem from highly non-linear decision boundaries induced by deep neural
    networks. There are also other examples in the literature where the linearity
    hypothesis is not directly supported [[119](#bib.bib119)].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: 'Flatness of decision boundaries [[69](#bib.bib69)], large local curvature of
    the decision boundaries [[70](#bib.bib70)] and low flexibility of the networks [[71](#bib.bib71)]
    are some more examples of the viewpoints on the existence of adversarial examples
    that do not perfectly align with each other. Whereas it is apparent that adversarial
    examples can be formed by modifying as little as one pixel in an image, current
    literature seems to lack consensus on the reasons for the existence of the adversarial
    examples. This fact also makes analysis of adversarial examples an active research
    direction that is expected to explore and explain the nature of the decision boundaries
    induced by deep neural networks, which are currently more commonly treated as
    black-box models. Below, we review the works that mainly focus on analyzing the
    existence of adversarial perturbations for deep learning. We note that, besides
    the literature reviewed below, works related to adversarial attacks (Section [3](#S3
    "3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")) and defenses (Section [6](#S6 "6 Defenses against adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")) often provide brief analysis of adversarial perturbations while conjecturing
    about the phenomena resulting in the existence of the adversarial examples.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Limits on adversarial robustness
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fawzi et al. [[118](#bib.bib118)] introduced a framework for studying the instability
    of classifiers to adversarial perturbations. They established fundamental limits
    on the robustness of classifiers in terms of a ‘distinguishability measure’ between
    the classes of the dataset, where distinguishability is defined as the distance
    between the means of two classes for linear classifiers and the distance between
    the matrices of second order moments for the studied non-linear classifiers. This
    work shows that adversarial examples also exist for the classifiers beyond deep
    neural networks. The presented analysis traces back the phenomenon of adversarial
    instability to the low flexibility of the classifiers, which is not completely
    orthogonal to the prevailing belief at that time that high-nonlinearity of the
    networks make them susceptible to adversarial examples.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Space of adversarial examples
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tabacof and Eduardo [[25](#bib.bib25)] generated adversarial examples for shallow
    and deep network classifiers on MNIST [[10](#bib.bib10)] and ImageNet [[11](#bib.bib11)]
    datasets and probed the pixel space of adversarial examples by using noise of
    varying distribution and intensity. The authors empirically demonstrated that
    adversarial examples appear in large regions in the pixel space, which is in-line
    with the similar claim in [[23](#bib.bib23)]. However, somewhat in contrast to
    the linearity hypothesis, they argue that a weak, shallow and more linear classifier
    is also as susceptible to adversarial examples as a strong deep classifier.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: Tramer et al. [[132](#bib.bib132)] proposed a method to estimate the dimensionality
    of the space of the adversarial examples. It is claimed that the adversarial examples
    span a contiguous high dimension space (e.g. with dimensionality $\approx 25$).
    Due to high dimensionality, the subspaces of different classifiers can intersect,
    which gives rise to the transferability of the adversarial examples. Interestingly,
    their analysis suggests that it is possible to defend classifiers against transfer-based
    attacks even when they are vulnerable to direct attacks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Boundary tilting perspective
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tanay and Griffin [[74](#bib.bib74)] provided a ‘boundary tilting’ perspective
    on the existence of adversarial examples for deep neural networks. They argued
    that generally a single class data that is sampled to learn and evaluate a classifier
    lives in a sub-manifold of the class, and adversarial examples for that class
    exist when the classification boundary lies close to this sub-manifold. They formalized
    the notion of ‘adversarial strength’ of a classifier and reduced it to the ‘deviation
    angle’ between the boundaries of the considered classifier and the nearest centroid
    classifier. It is then shown that adversarial strength of a classifier can be
    varied by decision ‘boundary tilting’. The authors also argued that adversarial
    stability of the classifier is associated with its regularization. In the opinion
    of Tanay and Griffin, the linearity hypothesis [[23](#bib.bib23)] about the existence
    of adversarial examples is “unconvincing”.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Prediction uncertainty and evolutionary stalling of training cause adversaries
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cubuk et al. [[91](#bib.bib91)] argue that the “origin of adversarial examples
    is primarily due to an inherent uncertainty that neural networks have about their
    predictions”. They empirically compute a functional form of the uncertainty, which
    is shown to be independent of network architecture, training protocol and dataset.
    It is argued that this form only depends on the statistics of the network logit
    differences. This eventually results in fooling ratios caused by adversarial attacks
    to exhibit a universal scaling with respect to the size of perturbation. They
    studied FGSM [[23](#bib.bib23)], ILCM and BIM [[35](#bib.bib35)] based attacks
    to corroborate their claims. It is also claimed that accuracy of a network on
    clean images correlates with its adversarial robustness (see Section [5.5](#S5.SS5
    "5.5 Accuracy-adversarial robustness correlation ‣ 5 Existence of adversarial
    examples ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey") for more arguments in this direction).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Rozsa et al. [[102](#bib.bib102)] hypothesized that the existence of adversarial
    perturbations is a result of evolutionary stalling of decision boundaries on training
    images. In their opinion, individual training samples stop contributing to the
    training loss of the model (i.e. neural network) once they are classified correctly,
    which can eventually leave them close to the decision boundary. Hence, it becomes
    possible to throw those (and similar) samples away to a wrong class region by
    adding minor perturbations. They proposed a Batch Adjusted Network Gradients (BANG)
    algorithm to train a network to mitigate the evolutionary stalling during training.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 5.5 Accuracy-adversarial robustness correlation
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the quest of explaining the existence of adversarial perturbations, Rozsa
    et al. [[97](#bib.bib97)] empirically analyzed the correlation between the accuracy
    of eight deep network classifiers and their robustness to three adversarial attacks
    introduced in [[23](#bib.bib23)],[[94](#bib.bib94)]. The studied classifiers include
    AlexNet [[9](#bib.bib9)], VGG-16 and VGG-19 networks [[163](#bib.bib163)], Berkeley-trained
    version of GoogLeNet and Princeton-GoogLeNet [[18](#bib.bib18)], ResNet-52; ResNet-101;
    and ResNet-152 [[147](#bib.bib147)]. The adversarial examples are generated with
    the help of large-scale ImageNet dataset [[11](#bib.bib11)] using the techniques
    proposed in [[23](#bib.bib23)] and [[94](#bib.bib94)]. Their experiments lead
    to the observation that the networks with higher classification accuracy generally
    also exhibit more robustness against the adversarial examples. They also concluded
    that adversarial examples transfer better between similar network topologies.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: 5.6 More on linearity as the source
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Kortov and Hopfiled [[127](#bib.bib127)] examined the existence of adversarial
    perturbations in the context of Dense Associative Memory (DAM) models [[164](#bib.bib164)].
    As compared to the typical modern deep neural networks, DAM models employ higher
    order (more than quadratic) interactions between the neurons. The authors have
    demonstrated that adversarial examples generated using DAM models with smaller
    interaction power, which is similar to using a deep neural network with ReLU activation
    [[165](#bib.bib165)] for inducing linearity, are unable to fool models having
    higher order interactions. The authors provided empirical evidence on the existence
    of adversarial examples that is independent of the FGSM [[23](#bib.bib23)] attack,
    yet supports the linearity hypothesis of Goodfellow et al. [[23](#bib.bib23)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 5.7 Existence of universal perturbations
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Moosavi-Dezfooli et al. [[16](#bib.bib16)] initially argued that universal adversarial
    perturbations exploit geometric correlations between the decision boundaries induced
    by the classifiers. Their existence partly owes to a subspace containing normals
    to the decision boundaries, such that the normals also surround the natural images.
    In [[70](#bib.bib70)], they built further on their theory and showed the existence
    of common directions (shared across datapoints) along which the decision boundary
    of a classifier can be highly positively curved. They argue that such directions
    play a key role in the existence of universal perturbations. Based on their findings,
    the authors also propose a new geometric method to efficiently compute universal
    adversarial perturbations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: It is worth noting that previously Fawzi et al. [[69](#bib.bib69)] also associated
    the theoretical bounds on the robustness of classifiers to the curvature of decision
    boundaries. Similarly, Tramer et al. [[77](#bib.bib77)] also held the curvature
    of decision boundaries in the vicinity of data points responsible for the vulnerability
    of neural networks to black-box attacks. In another recent work, Mopuri et al. [[193](#bib.bib193)]
    present a GAN-like model to learn the distribution of the universal adversarial
    perturbations for a given target model. The learned distributions are also observed
    to show good transferability across models.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: 6 Defenses against adversarial attacks
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Currently, the defenses against the adversarial attacks are being developed
    along three main directions:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using modified training during learning or modified input during testing.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modifying networks, e.g. by adding more layers/sub-networks, changing loss/activation
    functions etc.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using external models as network add-on when classifying unseen examples.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The approaches along the first direction do not directly deal with the learning
    models. On the other hand, the other two categories are more concerned with the
    neural networks themselves. The techniques under these categories can be further
    divided into two types; namely (a) complete defense and (b) detection only. The
    ‘complete defense’ approaches aim at enabling the targeted network to achieve
    its original goal on the adversarial examples, e.g. a classifier predicting labels
    of adversarial examples with acceptable accuracy. On the other hand, ‘detection
    only’ approaches are meant to raise the red flag on potentially adversarial examples
    to reject them in any further processing. The taxonomy of the described categories
    is also shown in Fig. [9](#S6.F9 "Figure 9 ‣ 6 Defenses against adversarial attacks
    ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey").
    The remaining section is organized according to this taxonomy. In the used taxonomy,
    the difference between ‘modifying’ a network and employing an ‘add-on’ is that
    the former makes changes to the original deep neural network architecture/parameters
    during training. On the other hand, the latter keeps the original model intact
    and appends external model(s) to it during testing.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/13bd7a1a97672006f64ebb23fb11dd17.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Broad categorization of approaches aimed at defending deep neural
    networks against adversarial attacks.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Modified training/input
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.1.1 Brute-force adversarial training
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Since the discovery of adversarial examples for the deep neural networks [[22](#bib.bib22)],
    there has been a general consensus in the related literature that robustness of
    neural networks against these examples improves with adversarial training. Therefore,
    most of the contributions introducing new adversarial attacks, e.g. [[22](#bib.bib22)], [[23](#bib.bib23)],
    [[72](#bib.bib72)] (see Section [3](#S3 "3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")) simultaneously propose
    adversarial training as the first line of defense against those attacks. Although
    adversarial training improves robustness of a network, to be really effective,
    it requires that training is performed using strong attacks and the architecture
    of the network is sufficiently expressive. Since adversarial training necessitates
    increased training/data size, we refer to it as a ‘brute-force’ strategy.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: It is also commonly observed in the literature that brute-force adversarial
    training results in regularizing the network (e.g. see [[23](#bib.bib23)], [[90](#bib.bib90)])
    to reduce over-fitting, which in turn improves robustness of the networks against
    the adversarial attacks. Inspired by this observation, Miyato et al. [[113](#bib.bib113)]
    proposed a ‘Virtual Adversarial Training’ approach to smooth the output distributions
    of the neural networks. A related ‘stability training’ method is also proposed
    by Zheng et al. [[116](#bib.bib116)] to improve the robustness of neural networks
    against small distortions to input images. It is noteworthy that whereas adversarial
    training is known to improve robustness of neural networks, Moosavi-Dezfooli [[16](#bib.bib16)]
    showed that effective adversarial examples can again be computed for already adversarially
    trained networks.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.2 Data compression as defense
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Dziugaite et al. [[123](#bib.bib123)] noted that most of the popular image classification
    datasets comprise JPG images. Motivated by this observation, they studied the
    effects of JPG compression on the perturbations computed by FGSM [[23](#bib.bib23)].
    It is reported that JPG compression can actually reverse the drop in classification
    accuracy to a large extent for the FGSM perturbations. Nevertheless, it is concluded
    that compression alone is far from an effective defense. JPEG compression was
    also studied by Guo et al. [[82](#bib.bib82)] for mitigating the effectiveness
    of adversarial images. Das et al. [[37](#bib.bib37)] also took a similar approach
    and used JPEG compression to remove the high frequency components from images
    and proposed an ensemble-based technique to counter the adversarial attacks generated
    by FGSM [[23](#bib.bib23)] and DeepFool method [[72](#bib.bib72)]. Whereas encouraging
    results are reported in [[37](#bib.bib37)], there is no analysis provided for
    the stronger attacks, e.g. C&W attacks [[36](#bib.bib36)]. Moreover, Shin and
    Song [[186](#bib.bib186)] have demonstrated the existence of adversarial examples
    that can survive JPEG compression. Compression under Discrete Cosine Transform
    (DCT) was also found inadequate as a defense against the universal perturbations [[16](#bib.bib16)]
    in our previous work [[81](#bib.bib81)]. One major limitation of compression based
    defense is that larger compressions also result in loss of classification accuracy
    on clean images, whereas smaller compressions often do not adequately remove the
    adversarial perturbations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: In another related approach, Bhagoji et al. [[169](#bib.bib169)] proposed to
    compress input data using Principal Component Analysis for adversarial robustness.
    However, Xu et al. [[140](#bib.bib140)] noted that this compression also results
    in corrupting the spatial structure of the image, hence often adversely affecting
    the classification performance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.3 Foveation based defense
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Luo et al. [[119](#bib.bib119)] demonstrated that significant robustness against
    the adversarial attacks using L-BFGS [[22](#bib.bib22)] and FGSM [[23](#bib.bib23)]
    is possible with ‘foveation’ mechanism - applying neural network in different
    regions of images. It is hypothesized that CNN-based classifiers trained on large
    datasets, such as ImageNet [[11](#bib.bib11)] are generally robust to scale and
    translation variations of objects in the images. However, this invariance does
    not extend to adversarial patterns in the images. This makes foveation as a viable
    option for reducing the effectiveness of adversarial attacks proposed in [[22](#bib.bib22)],
    [[23](#bib.bib23)]. However, foveation is yet to demonstrate its effectiveness
    against more powerful attacks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: 6.1.4 Data randomization and other methods
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Xie et al. [[115](#bib.bib115)] showed that random resizing of the adversarial
    examples reduces their effectiveness. Moreover, adding random padding to such
    examples also results in reducing the fooling rates of the networks. Wang et al. [[138](#bib.bib138)]
    transformed the input data with a separate data-transformation module to remove
    possible adversarial perturbations in images. In the literature, we also find
    evidence that data augmentation during training (e.g. Gaussian data augmentation [[46](#bib.bib46)])
    also helps in improving robustness of neural networks to adversarial attacks,
    albeit only slightly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Modifying the network
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the approaches that modify the neural networks for defense against the
    adversarial attacks, we first discuss the ‘complete defense’ approaches. The ‘detection
    only’ approaches are separately reviewed in Section [6.2.8](#S6.SS2.SSS8 "6.2.8
    Detection Only approaches ‣ 6.2 Modifying the network ‣ 6 Defenses against adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey").'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1 Deep Contractive Networks
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the early attempts of making deep learning robust to adversarial attacks,
    Gu and Rigazio [[24](#bib.bib24)] introduced Deep Contractive Networks (DCN).
    It was shown that Denoising Auto Encoders [[154](#bib.bib154)] can reduce adversarial
    noise, however stacking them with the original networks can make the resulting
    network even more vulnerable to perturbations. Based on this observation, the
    training procedure of DCNs used a smoothness penalty similar to Contractive Auto
    Encoders [[173](#bib.bib173)]. Whereas reasonable robustness of DCNs was demonstrated
    against the L-BGFS [[22](#bib.bib22)] based attacks, many stronger attacks have
    been introduced since DCNs were initially proposed. A related concept of using
    auto encoders for adversarial robustness of the neural networks can be also found
    in [[141](#bib.bib141)].
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2 Gradient regularization/masking
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ross and Doshi-Velez [[52](#bib.bib52)] studied input gradient regularization [[167](#bib.bib167)]
    as a method for adversarial robustness. Their method trains differentiable models
    (e.g. deep neural networks) while penalizing the degree of variation resulting
    in the output with respect to change in the input. Implying, a small adversarial
    perturbation becomes unlikely to change the output of the trained model drastically.
    It is shown that this method, when combined with brute-force adversarial training,
    can result in very good robustness against attacks like FGSM [[23](#bib.bib23)]
    and JSMA [[60](#bib.bib60)]. However, each of these methods almost double the
    training complexity of a network, which is already prohibitive in many cases.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Previously, Lyu et al. [[28](#bib.bib28)] also used the notion of penalizing
    the gradient of loss function of network models with respect to the inputs to
    incorporate robustness in the networks against L-BFGS [[22](#bib.bib22)] and FGSM [[23](#bib.bib23)]
    based attacks. Similarly, Shaham et al. [[27](#bib.bib27)] attempted to improve
    the local stability of neural networks by minimizing the loss of a model over
    adversarial examples at each parameter update. They minimized the loss of their
    model over worst-case adversarial examples instead of the original data. In a
    related work, Nguyen and Sinha [[44](#bib.bib44)] introduced a masking based defense
    against C&W attack [[36](#bib.bib36)] by adding noise to the logit outputs of
    networks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.3 Defensive distillation
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Papernot et al. [[38](#bib.bib38)] exploited the notion of ‘distillation’ [[166](#bib.bib166)]
    to make deep neural networks robust against adversarial attacks. Distillation
    was introduced by Hinton et al. [[166](#bib.bib166)] as a training procedure to
    transfer knowledge of a more complex network to a smaller network. The variant
    of the procedure introduced by Papernot et al. [[38](#bib.bib38)] essentially
    uses the knowledge of the network to improve its own robustness. The knowledge
    is extracted in the form of class probability vectors of the training data and
    it is fed back to train the original model. It is shown that doing so improves
    resilience of a network to small perturbation in the images. Further empirical
    evidence in this regard is also provided in [[108](#bib.bib108)]. Moreover, in
    a follow-up work, Papernot et al. [[84](#bib.bib84)] also extended the defensive
    distillation method by addressing the numerical instabilities that were encountered
    in [[38](#bib.bib38)]. It is worth noting that the ‘Carlini and Wagner’ (C&W)
    attacks [[36](#bib.bib36)] introduced in Section [3.1](#S3.SS1 "3.1 Attacks for
    classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey") are claimed to be successful against the
    defensive distillation technique. We also note that defensive distillation can
    also be seen as an example of ‘gradient masking’ technique. However, we describe
    it separately keeping in view its popularity in the literature.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.4 Biologically inspired protection
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Nayebi and Ganguli [[124](#bib.bib124)] demonstrated natural robustness of neural
    networks against adversarial attacks with highly non-linear activations (similar
    to nonlinear dendritic computations). It is noteworthy that the Dense Associative
    Memory models of Krotov and Hopfield [[127](#bib.bib127)] also work on a similar
    principle for robustness against the adversarial examples. Considering the linearity
    hypothesis of Goodfellow et al. [[23](#bib.bib23)], [[124](#bib.bib124)] and [[127](#bib.bib127)]
    seem to further the notion of susceptibility of modern neural networks to adversarial
    examples being the effect of linearity of activations. We note that Brendel and
    Bethge [[187](#bib.bib187)] claim that the attacks fail on the biologically inspired
    protection [[124](#bib.bib124)] due to numerical limitations of computations.
    Stabilizing the computations again allow successful attacks on the protected networks.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.5 Parseval Networks
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cisse et al. [[131](#bib.bib131)] proposed ‘Parseval’ networks as a defense
    against the adversarial attacks. These networks employ a layer-wise regularization
    by controlling the global Lipschitz constant of the network. Considering that
    a network can be seen as a composition of functions (at each layer), robustification
    against small input perturbations is possible by maintaining a small Lipschitz
    constant for these functions. Cisse et al. proposed to do so by controlling the
    spectral norm of the weight matrices of the networks by parameterizing them with
    ‘parseval tight frames’ [[172](#bib.bib172)], hence the name ‘Parseval’ networks.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.6 DeepCloak
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Gao et al. [[139](#bib.bib139)] proposed to insert a masking layer immediately
    before the layer handling the classification. The added layer is explicitly trained
    by forward-passing clean and adversarial pair of images, and it encodes the differences
    between the output features of the previous layers for those image pairs. It is
    argued that the most dominant weights in the added layer correspond to the most
    sensitive features of the network (in terms of adversarial manipulation). Therefore,
    while classifying, those features are masked by forcing the dominant weights of
    the added layer to zero.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.7 Miscellaneous approaches
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Among other notable efforts in making neural networks robust to adversarial
    attacks, Zantedeschi et al. [[46](#bib.bib46)] proposed to use bounded ReLU [[174](#bib.bib174)]
    to reduce the effectiveness of adversarial patterns in the images. Jin et al. [[120](#bib.bib120)]
    introduced a feedforward CNN that used additive noise to mitigate the effects
    of adversarial examples. Sun et al. [[56](#bib.bib56)] prposed ‘HyperNetworks’
    that use statistical filtering as a method to make the network robust. Madry et
    al. [[55](#bib.bib55)] studied adversarial defense from the perspective of robust
    optimization. They showed that adversarial training with a PGD adversary can successfully
    defend against a range of other adversaries. Later, Carlini et al. [[59](#bib.bib59)]
    also verified this observation. Na et al. [[85](#bib.bib85)] employed a network
    that is regularized with a unified embedding for classification and low-level
    similarity learning. The network is penalized using the distance between clean
    and the corresponding adversarial embeddings. Strauss et al. [[89](#bib.bib89)]
    studied ensemble of methods to defend a network against the perturbations. Kadran
    et al. [[136](#bib.bib136)] modified the output layer of a neural network to induce
    robustness against the adversarial attacks. Wang et al. [[129](#bib.bib129)],
    [[122](#bib.bib122)] developed adversary resistant neural networks by leveraging
    non-invertible data transformation in the network. Lee et al. [[106](#bib.bib106)]
    developed manifold regularized networks that use a training objective to minimizes
    the difference between multi-layer embedding results of clean and adversarial
    images. Kotler and Wong [[96](#bib.bib96)] proposed to learn ReLU-based classifier
    that show robustness against small adversarial perturbations. They train a neural
    network that provably achieves high accuracy (¿90%) against any adversary in a
    canonical setting ($\epsilon=0.1$ for $\ell_{\infty}$-norm perturbation on MNIST).
    Raghunathan et al. [[189](#bib.bib189)] studied the problem of defense for neural
    networks with one hidden layer. Their approach produces a network and a certificate
    on MNIST dataset such that no attack perturbing image pixels by at most $\epsilon=0.1$
    could results in more than 35% test error. Kolter and Wong [[96](#bib.bib96)]
    and Raghunathan et al. [[189](#bib.bib189)] are among very few provable methods
    in defense against the adversarial attacks. Given that these methods are computationally
    infeasible to apply on larger networks, the only defenses that have been extensively
    evaluated are those of Madry et al. [[55](#bib.bib55)] giving  89% accuracy against
    large epsilon (0.3/1) on MNIST and 45% for moderate epsilon (8/255) on CIFAR.
    Another thread of works that can be seen as adversarial attacks/defenses with
    guarantees is related to verification of deep neural networks, e.g. [[191](#bib.bib191)],
    [[192](#bib.bib192)]. In their approach OrOrbia et al. [[194](#bib.bib194)] show
    that many different proposals of adversarial training are instances of more general
    regularized objective, they termed DataGrad. The proposed DataGrad framework can
    be seen as an extension of layerwise contractive autoencoder penalty.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.8 Detection Only approaches
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SafetyNet: Lu et al. [[66](#bib.bib66)] hypothesized that adversarial examples
    produce different patterns of ReLU activations in (the late stages of) networks
    than what is produced by clean images. Based on this hypothesis, they proposed
    to append a Radial Basis Function SVM classifier to the targeted models such that
    the SVM uses discrete codes computed by the late stage ReLUs of the network. To
    detect perturbation in a test image, its code is compared against those of training
    samples using the SVM. Effective detection of adversarial examples generated by
    [[23](#bib.bib23)], [[35](#bib.bib35)], [[72](#bib.bib72)] is demonstrated by
    their framework, named SafetyNet.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'Detector subnetwork: Metzen et al. [[78](#bib.bib78)] proposed to augment a
    targeted network with a subnetwork that is trained for a binary classification
    task of detecting adversarial perturbations in inputs. It is shown that appending
    such a network to the internal layers of a model and using adversarial training
    can help in detecting perturbations generated using FGSM [[23](#bib.bib23)], BIM [[35](#bib.bib35)]
    and DeepFool [[72](#bib.bib72)] methods. However, Lu et al. [[66](#bib.bib66)]
    later showed that this approach is again vulnerable to counter-counter measures.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploiting convolution filter statistics: Li and Li [[105](#bib.bib105)] used
    statistics of the convolution filters in CNN-based neural networks to classify
    the input images as clean or adversarial. A cascaded classifier is designed that
    uses these statistics, and it is shown to detect more than $85\%$ adversarial
    images generated by the methods in [[22](#bib.bib22)], [[114](#bib.bib114)].'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'Additional class augmentation: Grosse et al. [[57](#bib.bib57)] proposed to
    augment the potentially targeted neural network model with an additional class
    in which the model is trained to classify all the adversarial examples. Hosseini
    et al. [[32](#bib.bib32)] also employed a similar strategy to detect black-box
    attacks.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: 6.3 Network add-ons
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 6.3.1 Defense against universal perturbations
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Akhtar et al. [[81](#bib.bib81)] proposed a defense framework against the adversarial
    attacks generated using universal perturbations [[16](#bib.bib16)]. The framework
    appends extra ‘pre-input’ layers to the targeted network and trains them to rectify
    a perturbed image so that the classifier’s prediction becomes the same as its
    prediction on the clean version of the same image. The pre-input layers are termed
    Perturbation Rectifying Network (PRN), and they are trained without updating the
    parameters of the targeted network. A separate detector is trained by extracting
    features from the input-output differences of PRN for the training images. A test
    image is first passed through the PRN and then its features are used to detect
    perturbations. If adversarial perturbations are detected, the output of PRN is
    used to classify the test image. Fig. [10](#S6.F10 "Figure 10 ‣ 6.3.1 Defense
    against universal perturbations ‣ 6.3 Network add-ons ‣ 6 Defenses against adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey"), illustrates the rectification performed by PRN. The removed patterns
    are separately analyzed for detection.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7518593d135fed29ba48c13855dcf804.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Illustration of defense against universal perturbations [[81](#bib.bib81)]:
    The approach rectifies an image to restore the network prediction. The pattern
    removed by rectification is separately analyzed to detect perturbation.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.2 GAN-based defense
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lee et al. [[101](#bib.bib101)] used the popular framework of Generative Adversarial
    Networks [[153](#bib.bib153)] to train a network that is robust to FGSM [[23](#bib.bib23)]
    like attacks. The authors proposed to directly train the network along a generator
    network that attempts to generate perturbation for that network. During its training,
    the classifier keeps trying to correctly classify both the clean and perturbed
    images. We categorize this technique as an ‘add-on’ approach because the authors
    propose to always train any network in this fashion. In another GAN-based defense,
    Shen et al. [[58](#bib.bib58)] use the generator part of the network to rectify
    a perturbed image.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 6.3.3 Detection Only approaches
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Feature squeezing: Xu et al. [[43](#bib.bib43)] proposed to use feature squeezing
    to detect adversarial perturbation to an image. They added two external models
    to the classifier network, such that these models reduce the color bit depth of
    each pixel in the image, and perform spatial smoothing over the image. The predictions
    of the targeted network over the original image and the squeezed images are compared.
    If a large difference is found between the predictions, the image is considered
    to be an adversarial example. Whereas [[43](#bib.bib43)] demonstrated the effectiveness
    of this approach against more classical attacks [[23](#bib.bib23)], a follow-up
    work [[140](#bib.bib140)] also claims that the method works reasonably well against
    the more powerful C&W attacks [[36](#bib.bib36)]. He et al. [[76](#bib.bib76)]
    also combined feature squeezing with the ensemble method proposed in [[175](#bib.bib175)]
    to show that strength of defenses does not always increase by combining them.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: 'MagNet: Meng and Chen [[45](#bib.bib45)] proposed a framework that uses one
    or more external detectors to classify an input image as adversarial or clean.
    During training, the framework aims at learning the manifold of clean images.
    In the testing phase, the images that are found far from the manifold are treated
    as adversarial and are rejected. The images that are close to the manifold (but
    not exactly on it) are always reformed to lie on the manifold and the classifier
    is fed with the reformed images. The notion of attracting nearby images to the
    manifold of clean images and dropping the far-off images also inspires the name
    of the framework, i.e. MagNet. It is noteworthy that Carlini and Wagner [[188](#bib.bib188)]
    very recently demonstrated that this defense technique can also be defeated with
    slightly larger perturbations.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: 'Miscellaneous methods: Liang et al. [[50](#bib.bib50)] treated perturbations
    to images as noise and used scalar quantization and spatial smoothing filter to
    separately detect such perturbations. In a related approach, Feinman et al. [[86](#bib.bib86)]
    proposed to detect adversarial perturbations by harnessing uncertainty estimates
    (of dropout neural networks) and performing density estimation in the feature
    space of neural networks. Eventually, separate binary classifiers are trained
    as adversarial example detectors using the proposed features. Gebhart and Schrater [[92](#bib.bib92)]
    viewed neural network computation as information flow in graphs and proposed a
    method to detect adversarial perturbations by applying persistent homology to
    the induced graphs.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: 7 Outlook of the research direction
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous sections, we presented a comprehensive review of the recent
    literature in adversarial attacks on deep learning. Whereas several interesting
    facts were reported in those sections along the technical details, below we make
    more general observations regarding this emerging research direction. The discussion
    presents a broader outlook to the readers without in-depth technical knowledge
    of this area. Our arguments are based on the literature reviewed above.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'The threat is real: Whereas few works suggest that adversarial attacks on deep
    learning may not be a serious concern, a large body of the related literature
    indicates otherwise. The literature reviewed in Sections [3](#S3 "3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey") and [4](#S4 "4 Attacks in the real world ‣ Threat of Adversarial Attacks
    on Deep Learning in Computer Vision: A Survey") clearly demonstrate that adversarial
    attacks can severely degrade the performance of deep learning techniques on multiple
    Computer Vision tasks, and beyond. In particular, the literature reviewed in Section [4](#S4
    "4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey") ascertains that deep learning is vulnerable to
    adversarial attacks in the real physical world. Therefore, we can conclusively
    argue that adversarial attacks pose a real threat to deep learning in practice.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversarial vulnerability is a general phenomenon: The reviewed literature
    shows successful fooling of different types of deep neural networks, e.g. MLPs,
    CNNs, RNNs on a variety of tasks in Computer Vision, e.g. recognition, segmentation,
    detection. Although most of the existing works focus on fooling deep learning
    on the task of classification/recognition, based on the surveyed literature we
    can easily observe that deep learning approaches are vulnerable to adversarial
    attacks in general.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: 'Adversarial examples often generalize well: One of the most common properties
    of adversarial examples reported in the literature is that they transfer well
    between different neural networks. This is especially true for the networks that
    have relatively similar architecture. The generalization of adversarial examples
    is often exploited in black-box attacks.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: 'Reasons of adversarial vulnerability need more investigation: There are varied
    view-points in the literature on the reasons behind the vulnerability of deep
    neural networks to subtle adversarial perturbations. Often, these view-points
    are not well-aligned with each other. There is an obvious need for systematic
    investigation in this direction.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Linearity does promote vulnerability: Goodfellow et al. [[23](#bib.bib23)]
    first suggested that the design of modern deep neural networks that forces them
    to behave linearly in high dimensional spaces also makes them vulnerable to adversarial
    attacks. Although popular, this notion has also faced some opposition in the literature.
    Our survey pointed out multiple independent contributions that hold linearity
    of the neural networks accountable for their vulnerability to adversarial attacks.
    Based on this fact, we can argue that linearity does promote vulnerability of
    deep neural networks to the adversarial attacks. However, it does not seem to
    be the only reason behind successful fooling of deep neural networks with cheap
    analytical perturbations.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: 'Counter-counter measures are possible: Whereas multiple defense techniques
    exist to counter adversarial attacks, it is often shown in the literature that
    a defended model can again be successfully attacked by devising counter-counter
    measures, e.g. see [[49](#bib.bib49)]. This observation necessitates that new
    defenses also provide an estimate of their robustness against obvious counter-counter
    measures.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Highly active research direction: The profound implications of vulnerability
    of deep neural networks to adversarial perturbations have made research in adversarial
    attacks and their defenses highly active in recent time. The majority of the literature
    reviewed in this survey surfaced in the last two years, and there is currently
    a continuous influx of contributions in this direction. On one hand, techniques
    are being proposed to defend neural networks against the known attacks, on the
    other; more and mote powerful attacks are being devised. Recently, a Kaggle competition
    was also organized for the defenses against the adversarial attacks ([https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack/](https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack/)).
    It can be hoped that this high research activity will eventually result in making
    deep learning approaches robust enough to be used in safety and security critical
    applications in the real world.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 8 Conclusion
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This article presented the first comprehensive survey in the direction of adversarial
    attacks on deep learning in Computer Vision. Despite the high accuracies of deep
    neural networks on a wide variety of Computer Vision tasks, they have been found
    vulnerable to subtle input perturbations that lead them to completely change their
    outputs. With deep learning at the heart of the current advances in machine learning
    and artificial intelligence, this finding has resulted in numerous recent contributions
    that devise adversarial attacks and their defenses for deep learning. This article
    reviews these contributions, mainly focusing on the most influential and interesting
    works in the literature. From the reviewed literature, it is apparent that adversarial
    attacks are a real threat to deep learning in practice, especially in safety and
    security critical applications. The existing literature demonstrates that currently
    deep learning can not only be effectively attacked in cyberspace but also in the
    physical world. However, owing to the very high activity in this research direction
    it can be hoped that deep learning will be able to show considerable robustness
    against the adversarial attacks in the future.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Y. LeCun, Y. Bengio and G. Hinton, *Deep learning*, Nature, vol. 521, no. 7553,
    pp. 436-444, 2015.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] M. Helmstaedter, K. L. Briggman, S. C. Turaga, V. Jain, H. S. Seung, and
    W. Denk, *Connectomic reconstruction of the inner plexiform layer in the mouse
    retina*. Nature, vol. 500, no. 7461, pp. 168-174, 2013.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] H. Y. Xiong, B. Alipanahi, J. L. Lee, H. Bretschneider, D. Merico, R. K.
    Yuen, and Q. Morris, *The human splicing code reveals new insights into the genetic
    determinants of disease*, Science, vol. 347, no. 6218, 1254806 2015.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] J. Ma, R. P. Sheridan, A. Liaw, G. E. Dahl and V. Svetnik, *Deep neural
    nets as a method for quantitative structure-activity relationships*, Journal of
    chemical information and modeling, vol. 55, no. 2 pp. 263-274, 2015.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] T. Ciodaro, D. Deva, J. de Seixas and D. Damazio, *Online particle detection
    with neural networks based on topological calorimetry information*. Journal of
    physics: conference series. vol. 368, no. 1\. IOP Publishing, 2012.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Kaggle. Higgs boson machine learning challenge. Kaggle [https://www.kaggle.com/c/higgs-boson](https://www.kaggle.com/c/higgs-boson),
    2014.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. R. Mohamed, N. Jaitly, and B.
    Kingsbury, *Deep neural networks for acoustic modeling in speech recognition:
    The shared views of four research groups*, IEEE Signal Processing Magazine, vol.
    29, no. 6, pp. 82-97, 2012.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] I. Sutskever, O. Vinyals, and Q. V. Le, *Sequence to sequence learning
    with neural networks*. In Advances in neural information processing systems, pp.
    3104-3112, 2014.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] A. Krizhevsky, I. Sutskever and G. E. Hinton, *Imagenet classification
    with deep convolutional neural networks*. In Advances in neural information processing
    systems, pp. 1097-1105, 2012.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard
    and L. D. Jackel, *Backpropagation applied to handwritten zip code recognition*.
    Neural computation, vol. 1m no. 4, pp. 541-551, 1989.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-Fei, *Imagenet:
    A large-scale hierarchical image database*. In IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 248-255, 2009.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] E. Ackerman, *How Drive.ai is mastering autonomous driving with deep learning*,
    [https://spectrum.ieee.org/cars-that-think/transportation/self-driving/how-driveai-is-mastering-autonomous-driving-with-deep-learning](https://spectrum.ieee.org/cars-that-think/transportation/self-driving/how-driveai-is-mastering-autonomous-driving-with-deep-learning),
    Accessed December 2017.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. M. Najafabadi, F. Villanustre, T. M. Khoshgoftaar, N. Seliya, R. Wald
    and E. Muharemagic, *Deep learning applications and challenges in big data analytics*,
    Journal of Big Data, vol. 2, no. 1, 2015.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A.
    Guez, Y. Chen, *Mastering the game of go without human knowledge*. Nature, vol.
    550, no. 7676, pp. 354-359, 2017.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] K. He, X. Zhang, S. Ren, J. Sun, *Deep residual learning for image recognition*.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 770-778, 2016.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi and P. Frossard, *Universal
    adversarial perturbations*. In Proceedings of IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), 2017.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] K. Chatfield, K. Simonyan, A. Vedaldi, A. Zisserman, *Return of the devil
    in the details: Delving deep into convolutional nets*, arXiv preprint arXiv:1405.3531,
    2014.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, A. Rabinovich,
    *Going deeper with convolutions*. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pp. 1-9, 2015.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Lu, H. Sibai, E. Fabry, D. Forsyth, *Standard detectors aren’t (currently)
    fooled by physical adversarial stop signs*, arXiv preprint arXiv:1710.03337, 2017.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. Fletcher, *Practical methods of optimization*, John Wiley and Sons,
    2013.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, S.
    Thrun, *Dermatologist-level classification of skin cancer with deep neural networks*,
    Nature, vol. 542, pp. 115 - 118, 2017.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
    R. Fergus, *Intriguing properties of neural networks*, arXiv preprint arXiv:1312.6199,
    2014.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] I. J. Goodfellow, J. Shlens, C. Szegedy, *Explaining and Harnessing Adversarial
    Examples*, arXiv preprint arXiv:1412.6572, 2015.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Gu, L. Rigazio, *Towards Deep Neural Network Architectures Robust to
    Adversarial Examples*, arXiv preprint arXiv:1412.5068, 2015'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] P. Tabacof, E. Valle, *Exploring the Space of Adversarial Images*, In
    IEEE International Joint Conference on Neural Networks, pp. 426-433, 2016.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Sabour, Y. Cao, F. Faghri, and D. J. Fleet, *Adversarial manipulation
    of deep representations*, arXiv preprint arXiv:1511.05122, 2015.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] U. Shaham, Y. Yamada, S. Negahban, *Understanding Adversarial Training:
    Increasing Local Stability of Neural Nets through Robust Optimization*, arXiv
    preprint arXiv:1511.05432, 2016.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] C. Lyu, K. Huang, H. Liang, *A Unified Gradient Regularization Family
    for Adversarial Examples*, In IEEE International Conference on Data Mining, pp.
    301-309, 2015.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A.
    Rahmati, D. Song, *Robust Physical-World Attacks on Deep Learning Models*, arXiv
    preprint arXiv:1707.08945, 2017.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Lu, H. Sibai, E. Fabry, D. Forsyth, *No need to worry about adversarial
    examples in object detection in autonomous vehicles*, arXiv preprint arXiv:1707.03501,
    2017.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Liu, W. Zhang, S. Li, N. Yu, *Enhanced Attacks on Defensively Distilled
    Deep Neural Networks*, arXiv preprint arXiv:1711.05934, 2017.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] H. Hosseini, Y. Chen, S. Kannan, B. Zhang, R. Poovendran, *Blocking transferability
    of adversarial examples in black-box learning systems*, arXiv preprint arXiv:1703.04318,
    2017.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] T. Gu, B. Dolan-Gavitt, S. Garg, *BadNets: Identifying Vulnerabilities
    in the Machine Learning Model Supply Chain.* arXiv preprint arXiv:1708.06733,
    2017.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] N. Papernot, P. McDaniel, A. Sinha, M. Wellman, *Towards the Science of
    Security and Privacy in Machine Learning*, arXiv preprint arXiv:1611.03814, 2016.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Kurakin, I. Goodfellow, S. Bengio, *Adversarial examples in the physical
    world*, arXiv preprint arXiv:1607.02533, 2016.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] N. Carlini, D. Wagner, *Towards Evaluating the Robustness of Neural Networks*,
    arXiv preprint arXiv:1608.04644, 2016.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] N. Das, M. Shanbhogue, S. Chen, F. Hohman, L. Chen, M. E. Kounavis, D.
    H. Chau, *Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with
    JPEG Compression*, arXiv preprint arXiv:1705.02900, 2017.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] N. Papernot, P. McDaniel, X. Wu, S. Jha, A. Swami, *Distillation as a
    Defense to Adversarial Perturbations against Deep Neural Networks*, In IEEE Symposium
    on Security and Privacy (SP), pp. 582-597, 2016.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, A. Swami,
    *Practical Black-Box Attacks against Machine Learning*, In Proceedings of the
    ACM on Asia Conference on Computer and Communications Security, pp. 506-519\.
    ACM, 2017.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] X. Xu, X. Chen, C. Liu, A. Rohrbach, T. Darell, D. Song, *Can you fool
    AI with adversarial examples on a visual Turing test?*, arXiv preprint arXiv:1709.08693,
    2017'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] P. Chen, H. Zhang, Y. Sharma, J. Yi, C. Hsieh, *ZOO: Zeroth Order Optimization
    based Black-box Attacks to Deep Neural Networks without Training Substitute Models*,
    In Proceedings of 10th ACM Workshop on Artificial Intelligence and Security (AISEC),
    2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] S. Baluja, I. Fischer, *Adversarial Transformation Networks: Learning
    to Generate Adversarial Examples*, arXiv preprint arXiv:1703.09387, 2017.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] W. Xu, D. Evans, Y. Qi, *Feature Squeezing: Detecting Adversarial Examples
    in Deep Neural Networks*, arXiv preprint arXiv:1704.01155, 2017.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] L. Nguyen, A. Sinha, *A Learning and Masking Approach to Secure Learning*,
    arXiv preprint arXiv:1709.04447, 2017.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Dongyu Meng, Hao Chen, *MagNet: a Two-Pronged Defense against Adversarial
    Examples*, In Proceedings of ACM Conference on Computer and Communications Security
    (CCS), 2017.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] V. Zantedeschi, M. Nicolae, A. Rawat, *Efficient Defenses Against Adversarial
    Attacks*, arXiv preprint arXiv:1707.06728, 2017.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J. Hayes, G. Danezis, *Machine Learning as an Adversarial Service: Learning
    Black-Box Adversarial Examples*, arXiv preprint arXiv:1708.05207, 2017.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Graese, A. Rozsa, T. E. Boult, *Assessing Threat of Adversarial Examples
    on Deep Neural Networks*, In IEEE International Conference on Machine Learning
    and Applications, pp. 69-74, 2016.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] N. Carlini, D. Wagner, *Adversarial Examples Are Not Easily Detected:
    Bypassing Ten Detection Methods*, arXiv preprint arXiv:1705.07263, 2017.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] B. Liang, H. Li, M. Su, X. Li, W. Shi, X. Wang, *Detecting Adversarial
    Examples in Deep Networks with Adaptive Noise Reduction*, arXiv preprint arXiv:1705.08378,
    2017.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Arnab, O. Miksik, P. H. S. Torr, *On the Robustness of Semantic Segmentation
    Models to Adversarial Attacks*, arXiv preprint arXiv:1711.09856, 2017.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] A. S. Ross, F. Doshi-Velez, *Improving the Adversarial Robustness and
    Interpretability of Deep Neural Networks by Regularizing their Input Gradients*,
    arXiv preprint arXiv:1711.09404, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Song, T. Kim, S. Nowozin, S. Ermon, N. Kushman, *PixelDefend: Leveraging
    Generative Models to Understand and Defend against Adversarial Examples*, arXiv
    preprint arXiv:1710.10766, 2017.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] N. Narodytska, S. P. Kasiviswanathan, *Simple Black-Box Adversarial Perturbations
    for Deep Networks*, arXiv preprint arXiv:1612.06299, 2016.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, A. Vladu, *Towards Deep
    Learning Models Resistant to Adversarial Attacks*, arXiv preprint arXiv:1706.06083,
    2017.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Z. Sun, M. Ozay, T. Okatani, *HyperNetworks with statistical filtering
    for defending adversarial examples*, arXiv preprint arXiv:1711.01791, 2017.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] K. Grosse, P. Manoharan, N. Papernot, M. Backes, P. McDaniel, *On the
    (Statistical) Detection of Adversarial Examples*, arXiv preprint arXiv:1702.06280,
    2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] S. Shen, G. Jin, K. Gao, Y. Zhang, *APE-GAN: Adversarial Perturbation
    Elimination with GAN*, arXiv preprint arXiv:1707.05474, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] N. Carlini, G. Katz, C. Barrett, D. L. Dill, *Ground-Truth Adversarial
    Examples*, arXiv preprint arXiv:1709.10207, 2017.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, A. Swami,
    *The Limitations of Deep Learning in Adversarial Settings*, In Proceedings of
    IEEE European Symposium on Security and Privacy, 2016.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] K. Grosse, N. Papernot, P. Manoharan, M. Backes, P. McDaniel, *Adversarial
    Perturbations Against Deep Neural Networks for Malware Classification*, arXiv
    preprint arXiv:1606.04435, 2016.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, P. Abbeel, *Adversarial
    Attacks on Neural Network Policies*, arXiv preprint arXiv:1702.02284, 2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera, F. Roli, *Is Deep
    Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid*,
    arXiv preprint arXiv:1708.06939, 2017.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] I. Rosenberg, A. Shabtai, L. Rokach, Y. Elovici, *Generic Black-Box End-to-End
    Attack against RNNs and Other API Calls Based Malware Classifiers*, arXiv preprint
    arXiv:1707.05970, 2017.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] A. Athalye, L. Engstrom, A. Ilyas, K. Kwok, *Synthesizing Robust Adversarial
    Examples*, arXiv preprint arXiv:1707.07397, 2017.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] J. Lu, T. Issaranon, D. Forsyth, *SafetyNet: Detecting and Rejecting Adversarial
    Examples Robustly*, arXiv preprint arXiv:1704.00103, 2017.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] J. H. Metzen, M. C. Kumar, T. Brox, V. Fischer *Universal Adversarial
    Perturbations Against Semantic Image Segmentation*, In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 2755-2764, 2017\.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] J. Su, D. V. Vargas, S. Kouichi, *One pixel attack for fooling deep neural
    networks*, arXiv preprint arXiv:1710.08864, 2017.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] A. Fawzi, S. Moosavi-Dezfooli, P. Frossard, *Robustness of classifiers:
    from adversarial to random noise*, In Advances in Neural Information Processing
    Systems, pp. 1632-1640, 2016.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, P. Frossard, S Soatto, *Analysis
    of universal adversarial perturbations*, arXiv preprint arXiv:1705.09554, 2017'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] A. Fawzi, O. Fawzi, P. Frossard, *Analysis of classifiers’ robustness
    to adversarial perturbations*, arXiv preprint arXiv:1502.02590, 2015.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] S. Moosavi-Dezfooli, A. Fawzi, P. Frossard, *DeepFool: a simple and accurate
    method to fool deep neural networks*, In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 2574-2582, 2016.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] C. Kanbak, SS. Moosavi-Dezfooli, P. Frossard, *Geometric robustness of
    deep networks: analysis and improvement*, arXiv preprint arXiv:1711.09115, 2017.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] T. Tanay, L. Griffin, *A Boundary Tilting Persepective on the Phenomenon
    of Adversarial Examples*, arXiv preprint arXiv:1608.07690, 2016.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A.
    Rahmati, D. Song, *Robust Physical-World Attacks on Deep Learning Models*, arXiv
    preprint arXiv:1707.08945, 2017'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] W. He, J. Wei, X. Chen, N. Carlini, D. Song, *Adversarial Example Defenses:
    Ensembles of Weak Defenses are not Strong*, arXiv preprint arXiv:1706.04701, 2017.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] F. Tramer, A. Kurakin, N. Papernot, D. Boneh, P. McDaniel, *Ensemble Adversarial
    Training: Attacks and Defenses*, arXiv preprint arXiv:1705.07204, 2017.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] J. H. Metzen, T. Genewein, V. Fischer, B. Bischoff, *On Detecting Adversarial
    Perturbations*, arXiv preprint arXiv:1702.04267, 2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] C. Xie, J. Wang, Z. Zhang, Z. Ren, A. Yuille, *Mitigating adversarial
    effects through randomization*, arXiv preprint arXiv:1711.01991, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Kurakin, I. Goodfellow, S. Bengio, *Adversarial Machine Learning at
    Scale*, arXiv preprint arXiv:1611.01236, 2017.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] N. Akhtar, J. Liu, A. Mian, *Defense against Universal Adversarial Perturbations*,
    arXiv preprint arXiv:1711.05929, 2017.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C. Guo, M. Rana, M. Cisse, L. Maaten, *Countering Adversarial Images using
    Input Transformations*, arXiv preprint arXiv:1711.00117, 2017.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] A. Galloway, G. W. Taylor, M. Moussa, *Attacking Binarized Neural Networks*,
    arXiv preprint arXiv:1711.00449, 2017.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] N. Papernot, P. McDaniel, *Extending Defensive Distillation*, arXiv preprint
    arXiv:1705.05264, 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] T. Na, J. H. Ko, S. Mukhopadhyay, *Cascade Adversarial Machine Learning
    Regularized with a Unified Embedding*, arXiv preprint arXiv:1708.02582, 2017.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] R. Feinman, R. R. Curtin, S. Shintre, A. B. Gardner, *Detecting Adversarial
    Samples from Artifacts*, arXiv preprint arXiv:1703.00410, 2017.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] X. Zeng, C. Liu, Y. Wang, W. Qiu, L. Xie, Y. Tai, C. K. Tang, A. L. Yuille,
    *Adversarial Attacks Beyond the Image Space*, arXiv preprint arXiv:1711.07183,
    2017.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Liu, X. Chen, C. Liu, D. Song, *Delving into Transferable Adversarial
    Examples and Black-box Attacks*, arXiv preprint arXiv:1611.02770, 2017.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] T. Strauss, M. Hanselmann, A. Junginger, H. Ulmer, *Ensemble Methods as
    a Defense to Adversarial Perturbations Against Deep Neural Networks*, arXiv preprint
    arXiv:1709.03423, 2017.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] S. Sankaranarayanan, A. Jain, R. Chellappa, S. N. Lim, *Regularizing deep
    networks using efficient layerwise adversarial training*, arXiv preprint arXiv:1705.07819,
    2017.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] E. D. Cubuk, B. Zoph, S. S. Schoenholz, Q. V. Le, *Intriguing Properties
    of Adversarial Examples*, arXiv preprint arXiv:1711.02846, 2017.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] T. Gebhart, P. Schrater, *Adversary Detection in Neural Networks via Persistent
    Homology*, arXiv preprint arXiv:1711.10056, 2017.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Bradshaw, A. G. Matthews, Z. Ghahramani, *Adversarial Examples, Uncertainty,
    and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks*, arXiv
    preprint arXiv:1707.02476, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Rozsa, E. M. Rudd, T. E. Boult, *Adversarial Diversity and Hard Positive
    Generation*, arXiv preprint arXiv:1605.01775, 2016.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori, A. Criminisi,
    *Measuring Neural Net Robustness with Constraints*, arXiv preprint arXiv:1605.07262,
    2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Z. Kolter, E. Wong, *Provable defenses against adversarial examples
    via the convex outer adversarial polytope*, arXiv preprint arXiv:1711.00851, 2017.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] A. Rozsa, M. Geunther, T. E. Boult, *Are Accuracy and Robustness Correlated?*,
    In IEEE International Conference on Machine Learning and Applications, pp. 227-232,
    2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] H. Hosseini, B. Xiao, M. Jaiswal, R. Poovendran, *On the Limitation of
    Convolutional Neural Networks in Recognizing Negative Images*, arXiv preprint
    arXiv:1703.06857, 2017.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, N. Usunier, *Parseval Networks:
    Improving Robustness to Adversarial Examples*, arXiv preprint arXiv:1704.08847,
    2017.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] N. Cheney, M. Schrimpf, G. Kreiman, *On the Robustness of Convolutional
    Neural Networks to Internal Architecture and Weight Perturbations*, arXiv preprint
    arXiv:1703.08245, 2017.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] H. Lee, S. Han, J. Lee, *Generative Adversarial Trainer: Defense to Adversarial
    Perturbations with GAN*, arXiv preprint arXiv:1705.03387, 2017.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A. Rozsa, M. Gunther, T. E. Boult, *Towards Robust Deep Neural Networks
    with BANG*, arXiv preprint arXiv:1612.00138, 2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T. Miyato, S. Maeda, M. Koyama, S. Ishii, *Virtual Adversarial Training:
    a Regularization Method for Supervised and Semi-supervised Learning*, arXiv preprint
    1704.03976, 2017.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,
    T. Maharaj, A. Fischer, A. Courville, Y. Bengio, S. Lacoste-Julien, *A Closer
    Look at Memorization in Deep Networks*, arXiv preprint arXiv:1706.05394, 2017.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] X. Li, F. Li, *Adversarial Examples Detection in Deep Networks with Convolutional
    Filter Statistics*, In Proceedings of International Conference on Computer Vision,
    2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] T. Lee, M. Choi, S. Yoon, *Manifold Regularized Deep Neural Networks
    using Adversarial Examples*, arXiv preprint arXiv:1511.06381, 2016.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel, *Adversarial
    examples for malware detection*, In European Symposium on Research in Computer
    Security, pp. 62-79\. 2017.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] N. Papernot, and P. McDaniel, *On the effectiveness of defensive distillation*,
    arXiv preprint arXiv:1607.05113, 2016.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] N. Papernot, Patrick McDaniel, and Ian Goodfellow, *Transferability in
    machine learning: from phenomena to black-box attacks using adversarial samples*,
    arXiv preprint arXiv:1605.07277, 2016.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] N. Papernot, P. McDaniel, A. Swami, and R. Harang, *Crafting adversarial
    input sequences for recurrent neural networks*, In IEEE Military Communications
    Conference, pp. 49-54, 2016.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] N. Papernot, I. Goodfellow, R. Sheatsley, R. Feinman, and P. McDaniel.
    *Cleverhans v1\. 0.0: an adversarial machine learning library*, arXiv preprint
    arXiv:1610.00768, 2016.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] I. Goodfellow, N. Papernot, and P. McDaniel, *cleverhans v0\. 1: an adversarial
    machine learning library*, arXiv preprint arXiv:1610.00768, 2016.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] T. Miyato, A. M. Dai, and Ian Goodfellow, *Adversarial Training Methods
    for Semi-Supervised Text Classification*, arXiv preprint arXiv:1605.07725, 2016.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] A. Nguyen, J. Yosinski, and J. Clune, *Deep neural networks are easily
    fooled: High confidence predictions for unrecognizable images*, In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427-436,
    2015.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] C. Xie, J. Wang, Z. Zhang, Y. Zhou, L. Xie, and A. Yuille, *Adversarial
    Examples for Semantic Segmentation and Object Detection*, arXiv preprint arXiv:1703.08603,
    2017.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] S. Zheng, Y. Song, T. Leung, and I. Goodfellow, *Improving the robustness
    of deep neural networks via stability training*, In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 4480-4488, 2016.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] P. Tabacof, and E. Valle, *Exploring the space of adversarial images*,
    In IEEE International Joint Conference on Neural Networks, pp. 426-433, 2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] A. Fawzi, O. Fawzi, and P. Frossard, *Fundamental limits on adversarial
    robustness*, In Proceedings of ICML, Workshop on Deep Learning. 2015.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Y. Luo, Xavier Boix, Gemma Roig, Tomaso Poggio, and Qi Zhao, *Foveation-based
    mechanisms alleviate adversarial examples*, arXiv preprint arXiv:1511.06292, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Jin, A. Dundar, and E. Culurciello, *Robust convolutional neural networks
    under adversarial noise*, arXiv preprint arXiv:1511.06306, 2015.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. Kos, I. Fischer, and D. Song, *Adversarial examples for generative
    models*, arXiv preprint arXiv:1702.06832, 2017.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Q. Wang, W. Guo, K. Zhang, I. I. Ororbia, G. Alexander, X. Xing, C. L.
    Giles, and X Liu, *Adversary Resistant Deep Neural Networks with an Application
    to Malware Detection*, arXiv preprint arXiv:1610.01239, 2016.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] G. K. Dziugaite, Z. Ghahramani, and D. M. Roy, *A study of the effect
    of JPG compression on adversarial images*, arXiv preprint arXiv:1608.00853, 2016.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] A. Nayebi, and S. Ganguli, *Biologically inspired protection of deep
    networks from adversarial attacks*, arXiv preprint arXiv:1703.09202, 2017.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] W. Hu, and Y. Tan, *Generating Adversarial Malware Examples for Black-Box
    Attacks Based on GAN*, arXiv preprint arXiv:1702.05983, 2017.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, *Towards
    proving the adversarial robustness of deep neural networks*, arXiv preprint arXiv:1709.02802,
    2017.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] D. Krotov, and J. J. Hopfield. *Dense Associative Memory is Robust to
    Adversarial Inputs*, arXiv preprint arXiv:1701.00939, 2017.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] P. Tabacof, T. Julia, E. Valle, *Adversarial images for variational autoencoders*,
    arXiv preprint arXiv:1612.00155, 2016.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Q. Wang, W. Guo, I. I. Ororbia, G. Alexander, X. Xing, L. Lin, C. L.
    Giles, X. Liu, P. Liu, and G. Xiong, *Using non-invertible data transformations
    to build adversary-resistant deep neural networks*, arXiv preprint arXiv:1610.01934,
    2016.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] A. Rozsa, M. Geunther, E. M. Rudd, and T. E. Boult. ”Facial attributes:
    Accuracy and adversarial robustness.” Pattern Recognition Letters (2017).'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] M. Cisse, Y. Adi, N. Neverova, and J. Keshet, *Houdini: Fooling deep
    structured prediction models*, arXiv preprint arXiv:1707.05373, 2017.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] F. Tramer, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel, *The
    Space of Transferable Adversarial Examples*, arXiv preprint arXiv:1704.03453,
    2017.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] S. J. Oh, M. Fritz, and B. Schiele, *Adversarial Image Perturbation for
    Privacy Protection–A Game Theory Perspective*, arXiv preprint arXiv:1703.09471,
    2017.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Lin, Z. Hong, Y. Liao, M. Shih, M. Liu, and M. Sun, *Tactics of Adversarial
    Attack on Deep Reinforcement Learning Agents*, arXiv preprint arXiv:1703.06748,
    2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] K. R. Mopuri, U. Garg, and R. V. Babu, *Fast Feature Fool: A data independent
    approach to universal adversarial perturbations*, arXiv preprint arXiv:1707.05572,
    2017.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] N. Kardan, and K. O. Stanley, *Mitigating fooling with competitive overcomplete
    output layer neural networks*, In International Joint Conference on Neural Networks
    pp. 518-525, 2017.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Y. Dong, H. Su, J. Zhu, and F. Bao, *Towards Interpretable Deep Neural
    Networks by Leveraging Adversarial Examples*, arXiv preprint arXiv:1708.05493,
    2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Q. Wang, W. Guo, K. Zhang, I. I. Ororbia, G. Alexander, X. Xing, C. L.
    Giles, and X. Liu, *Learning Adversary-Resistant Deep Neural Networks*, arXiv
    preprint arXiv:1612.01401, 2016.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Gao, B. Wang, Z. Lin, W. Xu, and Y. Qi, *DeepCloak: Masking Deep Neural
    Network Models for Robustness Against Adversarial Samples*, (2017).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] W. Xu, D. Evans, and Y. Qi, *Feature Squeezing Mitigates and Detects
    Carlini/Wagner Adversarial Examples*, arXiv preprint arXiv:1705.10686, 2017.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] W. Bai, C. Quan, and Z. Luo, *Alleviating adversarial attacks via convolutional
    autoencoder*, In International Conference on Software Engineering, Artificial
    Intelligence, Networking and Parallel/Distributed Computing (SNPD), pp. 53-58,
    2017.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] A. P. Norton, Y. Qi, *Adversarial-Playground: A visualization suite showing
    how adversarial examples fool deep learning*, In IEEE Symposium on Visualization
    for Cyber Security, pp. 1-4, 2017.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Y. Dong, F. Liao, T. Pang, X. Hu, and J. Zhu, *Discovering Adversarial
    Examples with Momentum*, arXiv preprint arXiv:1710.06081, 2017.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] S. Shen, R. Furuta, T. Yamasaki, and K. Aizawa, *Fooling Neural Networks
    in Face Attractiveness Evaluation: Adversarial Examples with High Attractiveness
    Score But Low Subjective Score*, In IEEE Third International Conference on Multimedia
    Big Data, pp. 66-69, 2017.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] C. Szegedy, V. Vincent, S. Ioffe, J. Shlens, and Z. Wojna. *Rethinking
    the inception architecture for computer vision*, In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2818-282, 2016.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] S. Sarkar, A. Bansal, U. Mahbub, and R. Chellappa, *UPSET and ANGRI:
    Breaking High Performance Image Classifiers*, arXiv preprint arXiv:1707.01159,
    2017.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] K. He, X. Zhang, S. Ren, and J. Sun, *Deep residual learning for image
    recognition*, In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 770-778, 2016.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Das, and P. N. Suganthan, *Differential evolution: A survey of the
    state-of-the-art*, IEEE transactions on evolutionary computation vol. 15, no.
    1, pp. 4-31, 2011.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] J. Redmon and A. Farhadi. *Yolo9000: better, faster, stronger*, arXiv
    preprint arXiv:1612.08242, 2016.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Ren, K. He, R. Girshick, and J. Sun, *Faster r-cnn: Towards real-time
    object detection with region proposal networks*, In Advances in neural information
    processing systems, pages 91-99, 2015.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro,
    J. Chen, M. Chrzanowski, A. Coates, G. Diamos. *Deep speech 2: End-to-end speech
    recognition in English and Mandarin*, arXiv preprint arXiv:1512.02595, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] A. Krizhevsky, *Learning multiple layers of features from tiny image*,
    2009.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Diederik P Kingma and Max Welling, *Auto-encoding variational bayes*,
    arXiv preprint arXiv:1312.6114, 2013.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Y. Bengio, *Learning deep architectures for AI*, Foundations and trends
    in Machine Learning vol. 2, no. 1, pp. 1-127, 2009.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, *Learning representations
    by back-propagating errors,* Cognitive modeling, vol. 5, 1988.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. Hochreiter and J. Schmidhuber, *Long short-term memory*, Neural computation,
    vol. 9, no. 8, pp. 1735-1780, 1997.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] M. Volodymyr, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
    Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, *Human-level
    control through deep reinforcement learning*, Nature, vol. 518, no. 7540, pp.
    529-533, 2015.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] M. Volodymyr, A. P. Badia, and M. Mirza, *Asynchronous methods for deep
    reinforcement learning* In International Conference on Machine Learning, 2016.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] J. Long, E. Shelhamer, and T. Darrell, *Fully convolutional networks
    for semantic segmentation*, In Proceedings of IEEE Conference on Computer Vision
    and Pattern Recognition, 2015.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Rozsa,M. Gunther, E. M. Rudd, and T. E. Boult, *Are facial attributes
    adversarially robust?* In International Conference on Pattern Recognition, pp.
    3121-3127, 2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Z. Liu, P. Luo, X. Wang, X. Tang, *Deep learning face attributes in the
    wild*, International Conference on Computer Vision, pp. 3730-3738, 2015.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] V. Mirjalili, and A. Ross, *Soft Biometric Privacy: Retaining Biometric
    Utility of Face Images while Perturbing Gender*, In International Joint Conference
    on Biometrics, 2017.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] K. Simonyan and A. Zisserman, *Very deep convolutional networks for large-scale
    image recognition*, in Proceedings of the International Conference on Learning
    Representations, 2015.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] D. Krotov, and J.J. Hopfield, *Dense Associative Memory for Pattern Recognition*,
    In Advances in Neural Information Processing Systems, 2016.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] R. Hahnloser, R. Sarpeshkar, M. A. Mahowald, R. J. Douglas, H.S. Seung,
    *Digital selection and analogue amplification coexist in a cortex-inspired silicon
    circuit*, Nature, vol. 405\. pp. 947-951.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] G. Hinton, O. Vinyals, and J. Dean, *Distilling the knowledge in a neural
    network*, in Deep Learning and Representation Learning Workshop at NIPS 2014\.
    arXiv preprint arXiv:1503.02531, 2014.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] H. Drucker, Y.Le Cun, *Improving generalization performance using double
    backpropagation*, IEEE Transactions on Neural Networks vol. 3, no. 6, pp. 991-997,
    1992.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] G. Huang, Z. Liu, K. Q. Weinberger, and L. Maaten, *Densely connected
    convolutional networks*, arXiv preprint arXiv:1608.06993, 2016.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] A. N. Bhagoji, D. Cullina, C. Sitawarin, P. Mittal, *Enhancing Robustness
    of Machine Learning Systems via Data Transformations*, arXiv preprint arXiv:1704.02654,
    2017.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Y. Dong, F. Liao, T. Pang, H. Su, X. Hu, J. Li, J. Zhu, *Boosting Adversarial
    Attacks with Momentum*, arXiv preprint arXiv:1710.06081, 2017.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] I. Goodfellow, J. P. Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, *Generative adversarial nets*, In Advances in neural
    information processing systems, pp. 2672-2680, 2014.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] K. Jelena and C. Amina, *An introduction to frames*. Foundations and
    Trends in Signal Processing, 2008.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, *Contractive
    auto-encoders: Explicit invariance during feature extraction*, In Proceedings
    of International Conference on Machine Learning, pp. 833 - 840, 2011.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. S. Liew, M. Khalil-Hani, and R. Bakhteri, *Bounded activation functions
    for enhanced training stability of deep neural networks on visual pattern recognition
    problems*, Neurocomputing, vol. 216, pp. 718-734, 2016.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] M. Abbasi, C. Gagne, *Robustness to adversarial examples through an ensemble
    of specialists*, arXiv preprint arXiv:1702.06856, 2017.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] A. Mogelmose, M. M. Trivedi, and T. B. Moeslund, *Vision-based traffic
    sign detection and analysis for intelligent driver assistance systems: Perspectives
    and survey*, In IEEE Transaction on Intelligent Transportation Systems, vol. 13,
    no. 4, pp. 1484-1497, 2012.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] A. Vedaldi and K. Lenc, *MatConvNet – Convolutional Neural Networks for
    MATLAB*, In Proceeding of the ACM International Conference on Multimedia, 2015.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] J. Yangqing, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
    S. Guadarrama, T. Darrell, *Caffe: Convolutional Architecture for Fast Feature
    Embedding*, arXiv preprint arXiv:1408.5093, 2014.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
    Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G.
    Irving, M. Isard, R. Jozefowicz, Y. Jia, L. Kaiser, M. Kudlur, J. Levenberg, D.
    Mane, M Schuster, R. Monga, S. Moore, D. Murray, C. Olah, J. Shlens, B. Steiner,
    I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O.
    Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, *TensorFlow:
    Large-scale machine learning on heterogeneous systems*, 2015\. Software available
    from tensorflow.org.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] A. Giusti, J. Guzzi, D. C. Ciresan, F. He, J. P. Rodriguez, F. Fontana,
    M. Faessler, *A machine learning approach to visual perception of forest trails
    for mobile robots*, IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 661
    - 667, 2016.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Objects Detection Machine Learning TensorFlow Demo, [https://play.google.com/store/apps/details?id=org.tensorflow.detect&hl=en](https://play.google.com/store/apps/details?id=org.tensorflow.detect&hl=en),
    Accessed December 2017.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Class central, *Deep Learning for Self-Driving Cars*, [https://www.class-central.com/mooc/8132/6-s094-deep-learning-for-self-driving-cars](https://www.class-central.com/mooc/8132/6-s094-deep-learning-for-self-driving-cars),
    Accessed December 2017.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] C. Middlehurst, *China unveils world’s first facial recognition ATM*,
    [http://www.telegraph.co.uk/news/worldnews/asia/china/11643314/China-unveils-worlds-first-facial-recognition-ATM.html](http://www.telegraph.co.uk/news/worldnews/asia/china/11643314/China-unveils-worlds-first-facial-recognition-ATM.html),
    2015.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] About Face ID advanced technology, [https://support.apple.com/en-au/HT208108](https://support.apple.com/en-au/HT208108),
    Accessed December 2017.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, *Accessorize to
    a crime: Real and stealthy attacks on state-of-the-art face recognition*, In Proceedings
    of ACM SIGSAC Conference on Computer and Communications Security, pp. 1528-1540,
    2016.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] R. Shin and D. Song, *JPEG-resistant adversarial images*, In MAchine
    LEarning and Computer Security Workshop, 2017.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] W. Brendel and M. Bethge *Comment on ”Biologically inspired protection
    of deep networks from adversarial attacks”*, arXiv preprint arXiv:1704.01547,
    2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] N. Carlini, D. Wagner, *MagNet and ”Efficient Defenses Against Adversarial
    Attacks” are Not Robust to Adversarial Examples*, arXiv preprint arXiv:1711.08478,
    2017.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] A. Raghunathan, J. Steinhardt, P. Liang, *Certified Defenses against
    Adversarial Examples*, arXiv preprint arXiv:1801.09344\. 2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] V. Khrulkov and I. Oseledets, *Art of singular vectors and universal
    adversarial perturbations*, arXiv preprint arXiv:1709.03582, 2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] X. Huang, M. Kwiatkowska, S. Wang, M. Wu, *Safety Verification of Deep
    Neural Networks*, In 29th International Conference on Computer Aided Verification,
    pages 3-29, 2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] M. Wicker, X. Huang, and M. Kwiatkowska, *Feature-Guided Black-Box Safety
    Testing of Deep Neural Networks*, In 24th International Conference on Tools and
    Algorithms for the Construction and Analysis of Systems, 2018.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] K. R. Mopuri, U. Ojha, U. Garg, V. Babu, *NAG: Network for Adversary
    Generation*, arXiv preprint arXiv:1712.03390, 2017.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] A. G. Ororbia II, C. L. Giles and D Kifer, *Unifying Adversarial Training
    Algorithms with Flexible Deep Data Gradient Regularization*, arXiv preprint arXiv:1601.07213,
    2016.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Y. Yoo, S. Park, J. Choi, S. Yun, N. Kwak, *Butterfly Effect: Bidirectional
    Control of Classification Performance by Small Additive Perturbation*, arXiv preprint
    arXiv:1711.09681, 2017.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/107c4671c54048f865e382e829fc4892.png) | Naveed
    Akhtar received his PhD in Computer Vision from The University of Western Australia
    (UWA) and Master degree in Computer Science from Hochschule Bonn-Rhein-Sieg, Germany
    (HBRS). His research in Computer Vision and Pattern Recognition has been published
    in prestigious venues of the field, including IEEE CVPR and IEEE TPAMI. He has
    also served as a reviewer for these venues. During his PhD, he was recipient of
    multiple scholarships, and runner-up for the Canon Extreme Imaging Competition
    in 2015. Currently, he is a Research Fellow at UWA since July 2017\. Previously,
    he has also served on the same position at the Australian National University
    for one year. His current research interests include adversarial machine learning,
    action recognition and hyperspectral image analysis. |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/26bb29c1831543e1cf9a814f6897655b.png) | Ajmal
    Mian completed his PhD from The University of Western Australia in 2006 with distinction
    and received the Australasian Distinguished Doctoral Dissertation Award from Computing
    Research and Education Association of Australasia. He received the prestigious
    Australian Postdoctoral and Australian Research Fellowships in 2008 and 2011 respectively.
    He received the UWA Outstanding Young Investigator Award in 2011, the West Australian
    Early Career Scientist of the Year award in 2012 and the Vice-Chancellors Mid-Career
    Research Award in 2014\. He has secured seven Australian Research Council grants
    and one National Health and Medical Research Council grant with a total funding
    of over $3 Million. He is currently in the School of Computer Science and Software
    Engineering at The University of Western Australia and is a guest editor of Pattern
    Recognition, Computer Vision and Image Understanding and Image and Vision Computing
    journals. His research interests include computer vision, machine learning, 3D
    shape analysis, hyperspectral image analysis, pattern recognition, and multimodal
    biometrics. |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
