- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:08:23'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:08:23
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1801.00553] Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1801.00553] 《深度学习在计算机视觉中的对抗攻击威胁：综述》'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1801.00553](https://ar5iv.labs.arxiv.org/html/1801.00553)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1801.00553](https://ar5iv.labs.arxiv.org/html/1801.00553)
- en: 'Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 《深度学习在计算机视觉中的对抗攻击威胁：综述》
- en: Naveed Akhtar and Ajmal Mian
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Naveed Akhtar 和 Ajmal Mian
- en: 'ACKNOWLEDGEMENTS: The authors thank Nicholas Carlini (UC Berkeley) and Dimitris
    Tsipras (MIT) for feedback to improve the survey quality. We also acknowledge
    X. Huang (Uni. Liverpool), K. R. Reddy (IISC), E. Valle (UNICAMP), Y. Yoo (CLAIR)
    and others for providing pointers to make the survey more comprehensive. This
    research was supported by ARC grant DP160101458. N. Akhtar and A. Mian are with
    the School of Computer Science and Software Engineering, University of Western
    Australia.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 致谢：作者感谢Nicholas Carlini（加州大学伯克利分校）和Dimitris Tsipras（麻省理工学院）对提升综述质量的反馈。我们还感谢X.
    Huang（利物浦大学）、K. R. Reddy（印度科学研究所）、E. Valle（UNICAMP）、Y. Yoo（CLAIR）及其他人提供的建议，使综述更加全面。本研究得到了ARC资助DP160101458的支持。N.
    Akhtar和A. Mian在西澳大学计算机科学与软件工程学院工作。
- en: 'E-mail: {naveed.akhtar, ajmal.mian}@uwa.edu.au Manuscript received August 2017,
    revised…'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 电子邮件：{naveed.akhtar, ajmal.mian}@uwa.edu.au 手稿接收于2017年8月，修订中…
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning is at the heart of the current rise of artificial intelligence.
    In the field of Computer Vision, it has become the workhorse for applications
    ranging from self-driving cars to surveillance and security. Whereas deep neural
    networks have demonstrated phenomenal success (often beyond human capabilities)
    in solving complex problems, recent studies show that they are vulnerable to adversarial
    attacks in the form of subtle perturbations to inputs that lead a model to predict
    incorrect outputs. For images, such perturbations are often too small to be perceptible,
    yet they completely fool the deep learning models. Adversarial attacks pose a
    serious threat to the success of deep learning in practice. This fact has recently
    lead to a large influx of contributions in this direction. This article presents
    the first comprehensive survey on adversarial attacks on deep learning in Computer
    Vision. We review the works that design adversarial attacks, analyze the existence
    of such attacks and propose defenses against them. To emphasize that adversarial
    attacks are possible in practical conditions, we separately review the contributions
    that evaluate adversarial attacks in the real-world scenarios. Finally, drawing
    on the reviewed literature, we provide a broader outlook of this research direction.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是当前人工智能崛起的核心。在计算机视觉领域，它已成为从自动驾驶汽车到监控和安全等应用的主力。尽管深度神经网络在解决复杂问题方面取得了惊人的成功（常常超越人类能力），但最近的研究显示，它们容易受到对抗攻击，这种攻击通过对输入进行细微扰动，使模型预测出错误的输出。对于图像而言，这种扰动通常过于微小，以至于不可察觉，但却完全欺骗了深度学习模型。对抗攻击对深度学习的实际成功构成了严重威胁。这一事实最近引发了大量相关研究。本文首次全面综述了计算机视觉中深度学习的对抗攻击。我们回顾了设计对抗攻击的研究，分析了这些攻击的存在，并提出了防御措施。为了强调对抗攻击在实际条件下是可能的，我们分别回顾了评估真实世界场景中的对抗攻击的贡献。最后，基于回顾的文献，我们提供了对这一研究方向的更广泛展望。
- en: 'Index Terms:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 索引词：
- en: Deep Learning, adversarial perturbation, black-box attack, white-box attack,
    adversarial learning, perturbation detection.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习，对抗扰动，黑箱攻击，白箱攻击，对抗学习，扰动检测。
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Deep Learning [[1](#bib.bib1)] is providing major breakthroughs in solving the
    problems that have withstood many attempts of machine learning and artificial
    intelligence community in the past. As a result, it is currently being used to
    decipher hard scientific problems at an unprecedented scale, e.g. in reconstruction
    of brain circuits [[2](#bib.bib2)]; analysis of mutations in DNA [[3](#bib.bib3)];
    prediction of structure-activity of potential drug molecules [[4](#bib.bib4)],
    and analyzing the particle accelerator data [[5](#bib.bib5)] [[6](#bib.bib6)].
    Deep neural networks have also become the preferred choice to solve many challenging
    tasks in speech recognition [[7](#bib.bib7)] and natural language understanding [[8](#bib.bib8)].
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习 [[1](#bib.bib1)] 正在解决许多机器学习和人工智能领域过去难以突破的问题。因此，它目前被用于以空前的规模解读复杂的科学问题，例如 脑电路重建 [[2](#bib.bib2)];
    DNA突变分析 [[3](#bib.bib3)]; 预测潜在药物分子的结构-活性 [[4](#bib.bib4)]，以及分析粒子加速器数据 [[5](#bib.bib5)]
    [[6](#bib.bib6)]。深度神经网络也成为解决语音识别 [[7](#bib.bib7)] 和自然语言理解 [[8](#bib.bib8)] 等许多具有挑战性的任务的首选。
- en: In the field of Computer Vision, deep learning became the center of attention
    after Krizhevsky et al. [[9](#bib.bib9)] demonstrated the impressive performance
    of a Convolutional Neural Network (CNN) [[10](#bib.bib10)] based model on a very
    challenging large-scale visual recognition task [[11](#bib.bib11)] in 2012. A
    significant credit for the current popularity of deep learning can also be attributed
    to this seminal work. Since 2012, the Computer Vision community has made numerous
    valuable contributions to deep learning research, enabling it to provide solutions
    for the problems encountered in medical science [[21](#bib.bib21)] to mobile applications [[181](#bib.bib181)].
    The recent breakthrough in artificial intelligence in the form of tabula-rasa
    learning of AlphaGo Zero [[14](#bib.bib14)] also owes a fair share to deep Residual
    Networks (ResNets) [[147](#bib.bib147)] that were originally proposed for the
    task of image recognition.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉领域，深度学习在2012年引起了广泛关注，此前Krizhevsky等人 [[9](#bib.bib9)] 展示了基于卷积神经网络（CNN） [[10](#bib.bib10)]
    的模型在一个非常具有挑战性的大规模视觉识别任务 [[11](#bib.bib11)] 上的卓越表现。当前深度学习流行度的显著归功于这项开创性工作。自2012年以来，计算机视觉社区为深度学习研究做出了大量宝贵贡献，使其能够为从医学科学 [[21](#bib.bib21)]
    到移动应用 [[181](#bib.bib181)] 等领域的问题提供解决方案。AlphaGo Zero [[14](#bib.bib14)] 在人工智能领域的突破性进展也得益于深度残差网络（ResNets） [[147](#bib.bib147)]，这一网络最初是为图像识别任务提出的。
- en: With the continuous improvements of deep neural network models [[145](#bib.bib145)],
    [[147](#bib.bib147)], [[168](#bib.bib168)]; open access to efficient deep learning
    software libraries [[177](#bib.bib177)], [[178](#bib.bib178)], [[179](#bib.bib179)];
    and easy availability of hardware required to train complex models, deep learning
    is fast achieving the maturity to enter into safety and security critical applications,
    e.g. self driving cars [[12](#bib.bib12)], [[182](#bib.bib182)], surveillance [[13](#bib.bib13)],
    maleware detection [[34](#bib.bib34)],[[107](#bib.bib107)], drones and robotics [[157](#bib.bib157)],[[180](#bib.bib180)],
    and voice command recognition [[7](#bib.bib7)]. With the recent real-world developments
    like facial recognition ATM [[183](#bib.bib183)] and Face ID security on mobile
    phones [[184](#bib.bib184)], it is apparent that deep learning solutions, especially
    those originating from Computer Vision problems are about to play a major role
    in our daily lives.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度神经网络模型的不断改进 [[145](#bib.bib145)], [[147](#bib.bib147)], [[168](#bib.bib168)];
    高效深度学习软件库的开放获取 [[177](#bib.bib177)], [[178](#bib.bib178)], [[179](#bib.bib179)];
    以及训练复杂模型所需硬件的易得性，深度学习正迅速成熟，进入安全和保密关键应用领域，例如 自动驾驶汽车 [[12](#bib.bib12)], [[182](#bib.bib182)],
    监控 [[13](#bib.bib13)], 恶意软件检测 [[34](#bib.bib34)], [[107](#bib.bib107)], 无人机和机器人 [[157](#bib.bib157)],
    [[180](#bib.bib180)], 以及语音命令识别 [[7](#bib.bib7)]。随着近期如面部识别ATM [[183](#bib.bib183)]
    和手机上的Face ID安全性 [[184](#bib.bib184)]等现实世界的发展，深度学习解决方案，特别是那些源于计算机视觉问题的解决方案，显然将在我们的日常生活中发挥重要作用。
- en: '![Refer to caption](img/296782cb8ce405019937c8d23f63544b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/296782cb8ce405019937c8d23f63544b.png)'
- en: 'Figure 1: Example of attacks on deep learning models with ‘universal adversarial
    perturbations’ [[16](#bib.bib16)]: The attacks are shown for the CaffeNet [[9](#bib.bib9)],
    VGG-F network [[17](#bib.bib17)] and GoogLeNet [[18](#bib.bib18)]. All the networks
    recognized the original clean images correctly with high confidence. After small
    perturbations were added to the images, the networks predicted wrong labels with
    similar high confidence. Notice that the perturbations are hardly perceptible
    for human vision system, however their effects on the deep learning models are
    catastrophic.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：使用“通用对抗扰动”攻击深度学习模型的示例 [[16](#bib.bib16)]：攻击展示了 CaffeNet [[9](#bib.bib9)]、VGG-F
    网络 [[17](#bib.bib17)] 和 GoogLeNet [[18](#bib.bib18)] 的情况。所有网络都以高度信心正确识别了原始干净图像。在对图像添加了小的扰动后，网络以类似的高信心预测了错误的标签。请注意，这些扰动对于人眼几乎不可察觉，但它们对深度学习模型的影响却是灾难性的。
- en: Whereas deep learning performs a wide variety of Computer Vision tasks with
    remarkable accuracies, Szegedy et al. [[22](#bib.bib22)] first discovered an intriguing
    weakness of deep neural networks in the context of image classification. They
    showed that despite their high accuracies, modern deep networks are surprisingly
    susceptible to adversarial attacks in the form of small perturbations to images
    that remain (almost) imperceptible to human vision system. Such attacks can cause
    a neural network classifier to completely change its prediction about the image.
    Even worse, the attacked models report high confidence on the wrong prediction.
    Moreover, the same image perturbation can fool multiple network classifiers. The
    profound implications of these results triggered a wide interest of researchers
    in adversarial attacks and their defenses for deep learning in general.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习在执行各种计算机视觉任务时表现出色，但 Szegedy 等人 [[22](#bib.bib22)] 首次发现了深度神经网络在图像分类中的一个有趣弱点。他们表明，尽管现代深度网络具有很高的准确率，但它们对小的扰动攻击（这些扰动对人眼几乎不可察觉）却出奇地敏感。这些攻击可以导致神经网络分类器完全改变对图像的预测。更糟糕的是，被攻击的模型会对错误的预测报告高度自信。此外，相同的图像扰动可以欺骗多个网络分类器。这些结果的深远影响引起了研究人员对深度学习对抗攻击及其防御的广泛关注。
- en: 'Since the findings of Szegedy et al. [[22](#bib.bib22)], several interesting
    results have surfaced regarding adversarial attacks on deep learning in Computer
    Vision. For instance, in addition to the image-specific adversarial perturbations [[22](#bib.bib22)],
    Moosavi-Dezfooli et al. [[16](#bib.bib16)] showed the existence of ‘universal
    perturbations’ that can fool a network classifier on any image (see Fig. [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Threat of Adversarial Attacks on Deep Learning in
    Computer Vision: A Survey") for example). Similarly, Athalye et al. [[65](#bib.bib65)]
    demonstrated that it is possible to even 3-D print real-world objects that can
    fool deep neural network classifiers (see Section [4.3](#S4.SS3 "4.3 Generic adversarial
    3D objects ‣ 4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey")). Keeping in view the significance of
    deep learning research in Computer Vision and its potential applications in the
    real life, this article presents the first comprehensive survey on adversarial
    attacks on deep learning in Computer Vision. The article is intended for a wider
    readership than Computer Vision community, hence it assumes only basic knowledge
    of deep learning and image processing. Nevertheless, it also discusses technical
    details of important contributions for the interested readers.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '自从 Szegedy 等人 [[22](#bib.bib22)] 的研究发现以来，计算机视觉领域出现了关于深度学习对抗攻击的若干有趣结果。例如，除了图像特定的对抗扰动
    [[22](#bib.bib22)] 外，Moosavi-Dezfooli 等人 [[16](#bib.bib16)] 还展示了“通用扰动”的存在，这些扰动能够欺骗任何图像上的网络分类器（例如见图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey")）。类似地，Athalye 等人 [[65](#bib.bib65)] 证明了甚至可以
    3-D 打印真实世界中的物体，以欺骗深度神经网络分类器（见第 [4.3](#S4.SS3 "4.3 Generic adversarial 3D objects
    ‣ 4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey") 节）。鉴于深度学习研究在计算机视觉中的重要性及其在现实生活中的潜在应用，本文首次全面综述了计算机视觉中对深度学习的对抗攻击。本文面向比计算机视觉社区更广泛的读者，因此仅假设对深度学习和图像处理有基本知识。然而，本文也讨论了感兴趣读者可能关注的重要贡献的技术细节。'
- en: 'We first describe the common terms related to adversarial attacks in the parlance
    of Computer Vision in Section [2](#S2 "2 Definitions of terms ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"). In Section [3](#S3 "3
    Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey"), we review the adversarial attacks for the task of image classification
    and beyond. A separate section is dedicated to the approaches that deal with adversarial
    attacks in the real-world conditions. Those approaches are reviewed in Section [4](#S4
    "4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey"). In the literature, there are also works that mainly
    focus on analyzing the existence of adversarial attacks. We discuss those contributions
    in Section [5](#S5 "5 Existence of adversarial examples ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"). The approaches that make
    defense against the adversarial attacks as their central topic are discussed in
    Section [6](#S6 "6 Defenses against adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"). In Section [7](#S7 "7
    Outlook of the research direction ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey"), we provide a broader outlook of the research direction
    based on the reviewed literature. Finally, we draw conclusion in Section [8](#S8
    "8 Conclusion ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey").'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '我们首先在第[2](#S2 "2 Definitions of terms ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey")节中描述与对抗性攻击相关的常见术语。在第[3](#S3 "3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")节中，我们回顾了图像分类及其他任务中的对抗性攻击。一个单独的部分专门讨论在现实世界条件下处理对抗性攻击的方法。这些方法在第[4](#S4 "4
    Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning in
    Computer Vision: A Survey")节中进行了回顾。在文献中，也有一些工作主要集中于分析对抗性攻击的存在性。我们在第[5](#S5 "5
    Existence of adversarial examples ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey")节中讨论这些贡献。以防御对抗性攻击为中心的研究方法在第[6](#S6 "6 Defenses against
    adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")节中进行讨论。在第[7](#S7 "7 Outlook of the research direction ‣ Threat
    of Adversarial Attacks on Deep Learning in Computer Vision: A Survey")节中，我们根据回顾的文献提供了研究方向的更广泛展望。最后，我们在第[8](#S8
    "8 Conclusion ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey")节中得出结论。'
- en: 2 Definitions of terms
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 术语定义
- en: In this section, we describe the common technical terms used in the literature
    related to adversarial attacks on deep learning in Computer Vision. The remaining
    article also follows the same definitions of the terms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们描述了与计算机视觉中对抗性攻击相关的文献中使用的常见技术术语。余下的文章也遵循相同的术语定义。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial example/image is a modified version of a clean image that is intentionally
    perturbed (e.g. by adding noise) to confuse/fool a machine learning technique,
    such as deep neural networks.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性示例/图像是干净图像的修改版，通过有意的扰动（例如添加噪声）来混淆/欺骗机器学习技术，例如深度神经网络。
- en: •
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial perturbation is the noise that is added to the clean image to make
    it an adversarial example.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性扰动是加在干净图像上的噪声，使其变成对抗性示例。
- en: •
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversarial training uses adversarial images besides the clean images to train
    machine learning models.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对抗性训练除了使用干净图像外，还使用对抗性图像来训练机器学习模型。
- en: •
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Adversary more commonly refers to the agent who creates an adversarial example.
    However, in some cases the example itself is also called adversary.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对手通常指的是创建对抗性示例的代理。然而，在某些情况下，示例本身也被称为对手。
- en: •
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Black-box attacks feed a targeted model with the adversarial examples (during
    testing) that are generated without the knowledge of that model. In some instances,
    it is assumed that the adversary has a limited knowledge of the model (e.g. its
    training procedure and/or its architecture) but definitely does not know about
    the model parameters. In other instances, using any information about the target
    model is referred to as ‘semi-black-box’ attack. We use the former convention
    in this article.
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 黑箱攻击向目标模型输入在不了解该模型的情况下生成的对抗性示例（在测试期间）。在某些情况下，假设对手对模型有有限了解（例如其训练过程和/或架构），但绝对不知道模型参数。在其他情况下，使用关于目标模型的任何信息被称为“半黑箱”攻击。我们在本文中使用前者的定义。
- en: •
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Detector is a mechanism to (only) detect if an image is an adversarial example.
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 检测器是一个机制，用于（仅）检测图像是否为对抗性示例。
- en: •
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Fooling ratio/rate indicates the percentage of images on which a trained model
    changes its prediction label after the images are perturbed.
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 欺骗率/比例表示在对图像进行扰动后，训练模型更改其预测标签的图像百分比。
- en: •
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: One-shot/one-step methods generate an adversarial perturbation by performing
    a single step computation, e.g. computing gradient of model loss once. The opposite
    are iterative methods that perform the same computation multiple times to get
    a single perturbation. The latter are often computationally expensive.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一次性/一步法方法通过执行单步计算来生成对抗扰动，例如，仅计算模型损失的梯度。相反的是迭代方法，它们执行相同的计算多次以获得一个扰动。后者通常计算开销较大。
- en: •
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Quasi-imperceptible perturbations impair images very slightly for human perception.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 几乎不可察觉的扰动对人类感知的图像影响非常轻微。
- en: •
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Rectifier modifies an adversarial example to restore the prediction of the targeted
    model to its prediction on the clean version of the same example.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 校正器修改对抗样本，以将目标模型的预测恢复到该样本的干净版本上的预测。
- en: •
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Targeted attacks fool a model into falsely predicting a specific label for the
    adversarial image. They are opposite to the non-targeted attacks in which the
    predicted label of the adversarial image is irrelevant, as long as it is not the
    correct label.
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 定向攻击欺骗模型错误地预测对抗图像的特定标签。它们与非定向攻击相对，后者只要不是正确标签，对抗图像的预测标签无关紧要。
- en: •
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Threat model refers to the types of potential attacks considered by an approach,
    e.g. black-box attack.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 威胁模型指的是方法考虑的潜在攻击类型，例如黑盒攻击。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Transferability refers to the ability of an adversarial example to remain effective
    even for the models other than the one used to generate it.
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 迁移性指的是对抗样本在生成它的模型之外仍然有效的能力。
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Universal perturbation is able to fool a given model on ‘any’ image with high
    probability. Note that, universality refers to the property of a perturbation
    being ‘image-agnostic’ as opposed to having good transferability.
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通用扰动能够以高概率欺骗给定模型在“任何”图像上。请注意，通用性指的是扰动具有“图像无关性”的特性，而不是具有良好的迁移性。
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: White-box attacks assume the complete knowledge of the targeted model, including
    its parameter values, architecture, training method, and in some cases its training
    data as well.
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 白盒攻击假设对目标模型具有完全知识，包括其参数值、架构、训练方法以及在某些情况下的训练数据。
- en: 3 Adversarial attacks
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 对抗攻击
- en: 'In this section, we review the body of literature in Computer Vision that introduces
    methods for adversarial attacks on deep learning. The reviewed literature mainly
    deals with the art of fooling the deep neural networks in ‘laboratory settings’,
    where approaches are developed for the typical Computer Vision tasks, e.g. recognition,
    and their effectiveness is demonstrated using standard datasets, e.g. MNIST [[10](#bib.bib10)].
    The techniques that focus on attacking deep learning in the real-world conditions
    are separately reviewed in Section [4](#S4 "4 Attacks in the real world ‣ Threat
    of Adversarial Attacks on Deep Learning in Computer Vision: A Survey"). However,
    it should be noted that the approaches reviewed in this section form the basis
    of the real-world attacks, and almost each one of them has the potential to significantly
    affect deep learning in practice. Our division is based on the evaluation conditions
    of the attacks in the original contributions.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '在这一部分，我们回顾了计算机视觉领域的文献，这些文献介绍了对深度学习的对抗攻击方法。回顾的文献主要涉及在“实验室环境”中欺骗深度神经网络的艺术，其中开发了用于典型计算机视觉任务（例如识别）的方法，并使用标准数据集（例如MNIST）展示其有效性[[10](#bib.bib10)]。关注于现实世界条件下攻击深度学习的技术在第[4](#S4
    "4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey")节中单独回顾。然而，需要注意的是，本节回顾的方法构成了现实世界攻击的基础，而且几乎每一种方法都可能在实践中显著影响深度学习。我们的划分基于原始贡献中攻击的评估条件。'
- en: 'The review in this section is mainly organized in chronological order, with
    few exceptions to maintain the flow of discussion. To provide technical understanding
    of the core concepts to the reader, we also go into technical details of the popular
    approaches and some representative techniques of the emerging directions in this
    area. Other methods are discussed briefly. We refer to the original papers for
    the details on those techniques. This section is divided into two parts. In part [3.1](#S3.SS1
    "3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"), we review the methods
    that attack deep neural networks performing the most common task in Computer Vision,
    i.e. classification/recognition. Approaches that are predominantly designed to
    attack deep learning beyond this task are discussed in part [3.2](#S3.SS2 "3.2
    Attacks beyond classification/recognition ‣ 3 Adversarial attacks ‣ Threat of
    Adversarial Attacks on Deep Learning in Computer Vision: A Survey").'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的回顾主要按时间顺序组织，少数例外以保持讨论的连贯性。为了向读者提供核心概念的技术理解，我们还深入探讨了流行方法及其代表性技术，以及该领域新兴方向的一些技术。其他方法则简要讨论。有关这些技术的详细信息，请参阅原始论文。本节分为两个部分。在部分[3.1](#S3.SS1
    "3.1 分类攻击 ‣ 3 对抗攻击 ‣ 对抗攻击对计算机视觉深度学习的威胁：综述")中，我们回顾了攻击深度神经网络的那些方法，这些网络执行计算机视觉中最常见的任务，即分类/识别。在部分[3.2](#S3.SS2
    "3.2 超越分类/识别的攻击 ‣ 3 对抗攻击 ‣ 对抗攻击对计算机视觉深度学习的威胁：综述")中，我们讨论了主要设计用于攻击深度学习以外任务的方法。
- en: 3.1 Attacks for classification
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 分类攻击
- en: 3.1.1 Box-constrained L-BFGS
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 有盒约束的 L-BFGS
- en: 'Szegedy et al. [[22](#bib.bib22)] first demonstrated the existence of small
    perturbations to the images, such that the perturbed images could fool deep learning
    models into misclassification. Let ${\bf I}_{c}\in\mathbb{R}^{m}$ denote a vectorized
    clean image - the subscript ‘$c$’ emphasizes that the image is clean. To compute
    an additive perturbation $\boldsymbol{\rho}\in\mathbb{R}^{m}$ that would distort
    the image very slightly to fool the network, Szegedy et al. proposed to solve
    the following problem:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Szegedy 等人 [[22](#bib.bib22)] 首次演示了对图像进行小扰动的存在，以至于这些扰动后的图像可以欺骗深度学习模型进行错误分类。令${\bf
    I}_{c}\in\mathbb{R}^{m}$表示一个向量化的干净图像——下标‘$c$’强调图像是干净的。为了计算一个加性扰动$\boldsymbol{\rho}\in\mathbb{R}^{m}$，该扰动会略微扭曲图像以欺骗网络，Szegedy
    等人提出解决以下问题：
- en: '|  | $\displaystyle\min_{\boldsymbol{\rho}}&#124;&#124;\boldsymbol{\rho}&#124;&#124;_{2}\hskip
    5.69054pt\text{s.t.}~{}\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})=\ell;~{}{\bf
    I}_{c}+\boldsymbol{\rho}\in[0,1]^{m},$ |  | (1) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\boldsymbol{\rho}}&#124;&#124;\boldsymbol{\rho}&#124;&#124;_{2}\hskip
    5.69054pt\text{s.t.}~{}\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})=\ell;~{}{\bf
    I}_{c}+\boldsymbol{\rho}\in[0,1]^{m},$ |  | (1) |'
- en: 'where ‘$\ell$’ denotes the label of the image and $\mathcal{C}(.)$ is the deep
    neural network classifier. The authors proposed to solve ([1](#S3.E1 "In 3.1.1
    Box-constrained L-BFGS ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks
    ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey"))
    for its non-trivial solution where ‘$\ell$’ is different from the original label
    of ${\bf I}_{c}$. In that case, ([1](#S3.E1 "In 3.1.1 Box-constrained L-BFGS ‣
    3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")) becomes a hard problem,
    hence an approximate solution is sought using a box-constrained L-BFGS [[20](#bib.bib20)].
    This is done by finding the minimum $c>0$ for which the minimizer $\boldsymbol{\rho}$
    of the following problem satisfies the condition $\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})=\ell$:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 其中‘$\ell$’表示图像的标签，而$\mathcal{C}(.)$是深度神经网络分类器。作者提出解决([1](#S3.E1 "在 3.1.1 有盒约束的
    L-BFGS ‣ 3.1 分类攻击 ‣ 3 对抗攻击 ‣ 对抗攻击对计算机视觉深度学习的威胁：综述"))的非平凡解，其中‘$\ell$’与原始标签${\bf
    I}_{c}$不同。在这种情况下，([1](#S3.E1 "在 3.1.1 有盒约束的 L-BFGS ‣ 3.1 分类攻击 ‣ 3 对抗攻击 ‣ 对抗攻击对计算机视觉深度学习的威胁：综述"))变成了一个难题，因此通过使用有盒约束的
    L-BFGS [[20](#bib.bib20)]寻求近似解。这是通过寻找最小的$c>0$来完成的，对于该$c$，以下问题的最小化器$\boldsymbol{\rho}$满足条件$\mathcal{C}({\bf
    I}_{c}+\boldsymbol{\rho})=\ell$：
- en: '|  | $\displaystyle\min_{\boldsymbol{\rho}}~{}~{}c&#124;\boldsymbol{\rho}&#124;+\mathcal{L}({\bf
    I}_{c}+\boldsymbol{\rho},\ell)~{}~{}s.t.~{}{\bf I}_{c}+\boldsymbol{\rho}\in[0,1]^{m},$
    |  | (2) |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\boldsymbol{\rho}}~{}~{}c&#124;\boldsymbol{\rho}&#124;+\mathcal{L}({\bf
    I}_{c}+\boldsymbol{\rho},\ell)~{}~{}s.t.~{}{\bf I}_{c}+\boldsymbol{\rho}\in[0,1]^{m},$
    |  | (2) |'
- en: 'where $\mathcal{L}(.,.)$ computes the loss of the classifier. We note that
    ([2](#S3.E2 "In 3.1.1 Box-constrained L-BFGS ‣ 3.1 Attacks for classification
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")) results in the exact solution for a classifier that has a
    convex loss function. However, for deep neural networks, this is generally not
    the case. The computed perturbation is simply added to the image to make it an
    adversarial example.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathcal{L}(.,.)$ 计算分类器的损失。我们注意到 ([2](#S3.E2 "In 3.1.1 Box-constrained
    L-BFGS ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")) 结果对于具有凸损失函数的分类器提供了精确的解决方案。然而，对于深度神经网络，通常情况并非如此。计算出的扰动被简单地添加到图像中，使其成为对抗样本。'
- en: 'As shown in Fig. [2](#S3.F2 "Figure 2 ‣ 3.1.1 Box-constrained L-BFGS ‣ 3.1
    Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks
    on Deep Learning in Computer Vision: A Survey"), the above method is able to compute
    perturbations that when added to clean images fool a neural network, but the adversarial
    images appear similar to the clean images to the human vision system. It was observed
    by Szegedy et al. that the perturbations computed for one neural network were
    also able to fool multiple networks. These astonishing results identified a blind-spot
    in deep learning. At the time of this discovery the Computer Vision community
    was fast adapting to the impression that deep learning features define the space
    where perceptual distances are well approximated by the Euclidean distances. Hence,
    these contradictory results triggered a wide interest of researchers in adversarial
    attacks on deep learning in Computer Vision.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [2](#S3.F2 "Figure 2 ‣ 3.1.1 Box-constrained L-BFGS ‣ 3.1 Attacks for classification
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey") 所示，上述方法能够计算出扰动，当这些扰动添加到干净图像中时，可以欺骗神经网络，但对抗样本在人的视觉系统中与干净图像类似。Szegedy
    等人观察到，对一个神经网络计算出的扰动也能欺骗多个网络。这些惊人的结果揭示了深度学习中的一个盲点。在这一发现时，计算机视觉界正快速适应于深度学习特征定义了感知距离在欧几里得距离下的空间。因此，这些矛盾的结果引发了研究人员对计算机视觉中深度学习的对抗攻击的广泛兴趣。'
- en: '![Refer to caption](img/9b9c87a88fe8aa4426dafef268228d74.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/9b9c87a88fe8aa4426dafef268228d74.png)'
- en: 'Figure 2: Illustration of adversarial examples generated using [[22](#bib.bib22)]
    for AlexNet [[9](#bib.bib9)]. The perturbations are magnified 10x for better visualization
    (values shifted by 128 and clamped). The predicted labels of adversarial examples
    are also shown.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用 [[22](#bib.bib22)] 为 AlexNet [[9](#bib.bib9)] 生成的对抗样本示意图。为了更好地可视化，扰动被放大了
    10 倍（值偏移了 128 并被限制）。对抗样本的预测标签也被显示出来。
- en: 3.1.2 Fast Gradient Sign Method (FGSM)
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2 快速梯度符号方法（FGSM）
- en: 'It was observed by Szegedy et al. [[22](#bib.bib22)] that the robustness of
    deep neural networks against the adversarial examples could be improved by adversarial
    training. To enable effective adversarial training, Goodfellow et al. [[23](#bib.bib23)]
    developed a method to efficiently compute an adversarial perturbation for a given
    image by solving the following problem:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: Szegedy 等人观察到 [[22](#bib.bib22)] 深度神经网络对对抗样本的鲁棒性可以通过对抗训练得到改善。为了实现有效的对抗训练，Goodfellow
    等人 [[23](#bib.bib23)] 开发了一种方法，通过解决以下问题来高效计算给定图像的对抗扰动：
- en: '|  | $\displaystyle\boldsymbol{\rho}=\epsilon~{}\text{sign}\left(\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{c},\ell)\right),$ |  | (3) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\boldsymbol{\rho}=\epsilon~{}\text{sign}\left(\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{c},\ell)\right),$ |  | (3) |'
- en: 'where $\nabla\mathcal{J}(.,.,.)$ computes the gradient of the cost function
    around the current value of the model parameters $\boldsymbol{\theta}$ w.r.t. ${\bf
    I}_{c}$, sign$(.)$ denotes the sign function and $\epsilon$ is a small scalar
    value that restricts the norm of the perturbation. The method for solving ([3](#S3.E3
    "In 3.1.2 Fast Gradient Sign Method (FGSM) ‣ 3.1 Attacks for classification ‣
    3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")) was termed ‘Fast Gradient Sign Method’ (FGSM) in the original
    work.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\nabla\mathcal{J}(.,.,.)$ 计算损失函数在模型参数 $\boldsymbol{\theta}$ 当前值下对 ${\bf
    I}_{c}$ 的梯度，sign$(.)$ 表示符号函数，$\epsilon$ 是一个限制扰动范数的小标量值。解决该问题的方法在原始工作中被称为“快速梯度符号方法”（FGSM）。
- en: Interestingly, the adversarial examples generated by FGSM exploit the ‘linearity’
    of deep network models in the higher dimensional space whereas such models were
    commonly thought to be highly non-linear at that time. Goodfellow et al. [[23](#bib.bib23)]
    hypothesized that the designs of modern deep neural networks that (intentionally)
    encourage linear behavior for computational gains, also make them susceptible
    to cheap analytical perturbations. In the related literature, this idea is often
    referred to as the ‘linearity hypothesis’, which is substantiated by the FGSM
    approach.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: 'Kurakin et al. [[80](#bib.bib80)] noted that on the popular large-scale image
    recognition data set ImageNet [[11](#bib.bib11)], the top-1 error rate on the
    adversarial examples generated by FGSM is around $63-69\%$ for $\epsilon\in[2,32]$.
    The authors also proposed a ‘one-step target class’ variation of the FGSM where
    instead of using the true label $\ell$ of the image in ([3](#S3.E3 "In 3.1.2 Fast
    Gradient Sign Method (FGSM) ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks
    ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey")),
    they used the label $\ell_{\text{target}}$ of the least likely class predicted
    by the network for ${\bf I}_{c}$. The computed perturbation is then subtracted
    from the original image to make it an adversarial example. For a neural network
    with cross-entropy loss, doing so maximizes the probability that the network predicts
    $\ell_{\text{target}}$ as the label of the adversarial example. It is suggested,
    that a random class can also be used as the target class for fooling the network,
    however it may lead to less interesting fooling, e.g. misclassification of one
    breed of dog as another dog breed. The authors also demonstrated that adversarial
    training improves robustness of deep neural networks against the attacks generated
    by FGSM and its proposed variants.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: The FGSM perturbs an image to increase the loss of the classifier on the resulting
    image. The sign function ensures that the magnitude of the loss is maximized,
    while $\epsilon$ essentially restricts the $\ell_{\infty}$-norm of the perturbation.
    Miyato et al. [[103](#bib.bib103)] proposed a closely related method to compute
    the perturbation as follows
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\boldsymbol{\rho}=\epsilon\frac{\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{c},\ell)}{&#124;&#124;\nabla\mathcal{J}(\boldsymbol{\theta},{\bf I}_{c},\ell)&#124;&#124;_{2}}.$
    |  | (4) |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
- en: In the above equation, the computed gradient is normalized with its $\ell_{2}$-norm.
    Kurakin et al. [[80](#bib.bib80)] referred to this technique as ‘Fast Gradient
    L[2]’ method and also proposed an alternative of using the $\ell_{\infty}$-norm
    for normalization, and referred to the resulting technique as ‘Fast Gradient L[∞]’
    method. Broadly speaking, all of these methods are seen as ‘one-step’ or ‘one-shot’
    methods in the literature related to adversarial attacks in Computer Vision.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3 Basic & Least-Likely-Class Iterative Methods
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3 基本与最不可能类别迭代方法
- en: 'The one-step methods perturb images by taking a single large step in the direction
    that increases the loss of the classifier (i.e. one-step gradient ascent). An
    intuitive extension of this idea is to iteratively take multiple small steps while
    adjusting the direction after each step. The Basic Iterative Method (BIM) [[35](#bib.bib35)]
    does exactly that, and iteratively computes the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 一步法通过在增加分类器损失的方向上迈出一个大的步伐来扰动图像（即一步梯度上升）。这个想法的直观扩展是迭代地迈出多个小步，同时在每一步后调整方向。基本迭代方法（BIM）
    [[35](#bib.bib35)] 正是这样做的，并迭代地计算以下内容：
- en: '|  | $\displaystyle{\bf I}_{\boldsymbol{\rho}}^{i+1}=\text{Clip}_{\epsilon}\left\{{\bf
    I}_{\boldsymbol{\rho}}^{i}+\alpha~{}\text{sign}(\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{\boldsymbol{\rho}}^{i},\ell)\right\},$ |  | (5) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf I}_{\boldsymbol{\rho}}^{i+1}=\text{Clip}_{\epsilon}\left\{{\bf
    I}_{\boldsymbol{\rho}}^{i}+\alpha~{}\text{sign}(\nabla\mathcal{J}(\boldsymbol{\theta},{\bf
    I}_{\boldsymbol{\rho}}^{i},\ell)\right\},$ |  | (5) |'
- en: where ${\bf I}_{\boldsymbol{\rho}}^{i}$ denotes the perturbed image at the $i^{\text{th}}$
    iteration, Clip${}_{\epsilon}\{.\}$ clips (the values of the pixels of) the image
    in its argument at $\epsilon$ and $\alpha$ determines the step size (normally,
    $\alpha=1$). The BIM algorithm starts with ${\bf I}_{\boldsymbol{\rho}}^{0}={\bf
    I}_{c}$ and runs for the number of iterations determined by the formula $\lfloor\min(\epsilon+4,1.25\epsilon)\rfloor$.
    Madry et al. [[55](#bib.bib55)] pointed out that BIM is equivalent to (the $\ell_{\infty}$
    version of) Projected Gradient Descent (PGD), a standard convex optimization method.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ${\bf I}_{\boldsymbol{\rho}}^{i}$ 表示第 $i^{\text{th}}$ 次迭代中的扰动图像，Clip${}_{\epsilon}\{.\}$
    将图像在其参数处的像素值限制在 $\epsilon$ 范围内，$\alpha$ 决定步长（通常，$\alpha=1$）。BIM 算法从 ${\bf I}_{\boldsymbol{\rho}}^{0}={\bf
    I}_{c}$ 开始，并运行至由公式 $\lfloor\min(\epsilon+4,1.25\epsilon)\rfloor$ 确定的迭代次数。Madry
    等人 [[55](#bib.bib55)] 指出，BIM 等同于（$\ell_{\infty}$ 版本的）投影梯度下降（PGD），这是一种标准的凸优化方法。
- en: 'Similar to extending the FGSM to its ‘one-step target class’ variation, Kurakin
    et al. [[35](#bib.bib35)] also extended BIM to Iterative Least-likely Class Method
    (ILCM). In that case, the label $\ell$ of the image in ([5](#S3.E5 "In 3.1.3 Basic
    & Least-Likely-Class Iterative Methods ‣ 3.1 Attacks for classification ‣ 3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")) is replaced by the target label $\ell_{\text{target}}$ of the least
    likely class predicted by the classifier. The adversarial examples generated by
    the ILCM method has been shown to seriously affect the classification accuracy
    of a modern deep architecture Inception v3 [[145](#bib.bib145)], even for very
    small values of $\epsilon$, e.g. $<16$.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于将 FGSM 扩展到其“单步目标类别”变体，Kurakin 等人 [[35](#bib.bib35)] 还将 BIM 扩展到了迭代最不可能类别方法（ILCM）。在这种情况下，图像的标签
    $\ell$（参见 [5](#S3.E5 "在 3.1.3 基本与最不可能类别迭代方法 ‣ 3.1 分类攻击 ‣ 3 对深度学习在计算机视觉中的对抗攻击的威胁：综述")）被分类器预测的最不可能类别的目标标签
    $\ell_{\text{target}}$ 替代。使用 ILCM 方法生成的对抗样本已被证明严重影响现代深度架构 Inception v3 [[145](#bib.bib145)]
    的分类准确性，即使对于非常小的 $\epsilon$ 值，例如 $<16$。
- en: 3.1.4 Jacobian-based Saliency Map Attack (JSMA)
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4 基于雅可比矩阵的显著性图攻击（JSMA）
- en: In the literature, it is more common to generate adversarial examples by restricting
    $\ell_{\infty}$ or $\ell_{2}$-norms of the perturbations to make them imperceptible
    for humans. However, Papernot et al. [[60](#bib.bib60)] also created an adversarial
    attack by restricting the $\ell_{0}$-norm of the perturbations. Physically, it
    means that the goal is to modify only a few pixels in the image instead of perturbing
    the whole image to fool the classifier. The crux of their algorithm to generate
    the desired adversarial image can be understood as follows. The algorithm modifies
    pixels of the clean image one at a time and monitors the effects of the change
    on the resulting classification. The monitoring is performed by computing a saliency
    map using the gradients of the outputs of the network layers. In this map, a larger
    value indicates a higher likelihood of fooling the network to predict $\ell_{\text{target}}$
    as the label of the modified image instead of the original label $\ell$. Thus,
    the algorithm performs targeted fooling. Once the map has been computed, the algorithm
    chooses the pixel that is most effective to fool the network and alters it. This
    process is repeated until either the maximum number of allowable pixels are altered
    in the adversarial image or the fooling succeeds.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在文献中，通过限制$\ell_{\infty}$或$\ell_{2}$-范数的扰动来生成对抗样本，以使其对人类几乎不可察觉是更为常见的。然而，Papernot等人[[60](#bib.bib60)]还通过限制扰动的$\ell_{0}$-范数创建了一种对抗攻击。从物理角度来看，这意味着目标是仅修改图像中的几个像素，而不是扰动整个图像来欺骗分类器。他们生成所需对抗图像的算法核心可以理解如下。该算法一次修改干净图像中的一个像素，并监视变化对分类结果的影响。监视是通过计算网络层输出的梯度来生成显著性图完成的。在此图中，较大的值表示欺骗网络将$\ell_{\text{target}}$预测为修改后图像的标签而非原始标签$\ell$的可能性更高。因此，该算法执行了有针对性的欺骗。一旦计算出图，算法选择对欺骗网络最有效的像素并对其进行更改。此过程重复进行，直到在对抗图像中更改的像素达到最大允许数量或欺骗成功为止。
- en: 3.1.5 One Pixel Attack
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5 单像素攻击
- en: 'An extreme case for the adversarial attack is when only one pixel in the image
    is changed to fool the classifier. Interestingly, Su et al. [[68](#bib.bib68)]
    claimed successful fooling of three different network models on $70.97\%$ of the
    tested images by changing just one pixel per image. They also reported that the
    average confidence of the networks on the wrong labels was found to be $97.47\%$.
    We show representative examples of the adversarial images from [[68](#bib.bib68)]
    in Fig. [3](#S3.F3 "Figure 3 ‣ 3.1.5 One Pixel Attack ‣ 3.1 Attacks for classification
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey"). Su et al. computed the adversarial examples by using the concept
    of Differential Evolution [[148](#bib.bib148)]. For a clean image ${\bf I}_{c}$,
    they first created a set of $400$ vectors in $\mathbb{R}^{5}$ such that each vector
    contained $xy$-coordinates and RGB values for an arbitrary candidate pixel. Then,
    they randomly modified the elements of the vectors to create children such that
    a child competes with its parent for fitness in the next iteration, while the
    probabilistic predicted label of the network is used as the fitness criterion.
    The last surviving child is used to alter the pixel in the image.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '对于对抗攻击的一个极端情况是，仅改变图像中的一个像素来欺骗分类器。有趣的是，Su等人[[68](#bib.bib68)]声称，通过每张图像仅更改一个像素，成功欺骗了三种不同的网络模型，在$70.97\%$的测试图像上。他们还报告了网络对错误标签的平均置信度为$97.47\%$。我们在图[3](#S3.F3
    "Figure 3 ‣ 3.1.5 One Pixel Attack ‣ 3.1 Attacks for classification ‣ 3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")中展示了来自[[68](#bib.bib68)]的对抗图像的代表性示例。Su等人通过使用差分进化[[148](#bib.bib148)]的概念计算了对抗样本。对于干净图像${\bf
    I}_{c}$，他们首先创建了一组$400$个在$\mathbb{R}^{5}$中的向量，其中每个向量包含了一个任意候选像素的$xy$-坐标和RGB值。然后，他们随机修改这些向量的元素以创建子代，使得子代在下一次迭代中与其父代竞争适应度，同时网络的概率预测标签作为适应度标准。最后幸存的子代用于更改图像中的像素。'
- en: '![Refer to caption](img/42b28391de9aabeb86852ef597413404.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/42b28391de9aabeb86852ef597413404.png)'
- en: 'Figure 3: Illustration of one pixel adversarial attacks [[68](#bib.bib68)]:
    The correct label is mentioned with each image. The corresponding predicted label
    is given in parentheses.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：单像素对抗攻击的示意图[[68](#bib.bib68)]：每张图像都标有正确的标签。相应的预测标签以括号形式给出。
- en: Even with such a simple evolutionary strategy Su et al. [[68](#bib.bib68)] were
    able to show successful fooling of deep networks. Notice that, differential evolution
    enables their approach to generate adversarial examples without having access
    to any information about the network parameter values or their gradients. The
    only input their technique requires is the probabilistic labels predicted by the
    targeted model.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管采用如此简单的进化策略，Su 等人 [[68](#bib.bib68)] 仍然成功展示了对深度网络的欺骗效果。请注意，差分进化使得他们的方法在没有网络参数值或梯度信息的情况下生成对抗样本。他们的方法唯一需要的输入是目标模型预测的概率标签。
- en: 3.1.6 Carlini and Wagner Attacks (C&W)
  id: totrans-89
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.6 Carlini 和 Wagner 攻击（C&W）
- en: A set of three adversarial attacks were introduced by Carlini and Wagner [[36](#bib.bib36)]
    in the wake of defensive distillation against the adversarial perturbations [[38](#bib.bib38)].
    These attacks make the perturbations quasi-imperceptible by restricting their
    $\ell_{2}$, $\ell_{\infty}$ and $\ell_{0}$ norms, and it is shown that defensive
    distillation for the targeted networks almost completely fails against these attacks.
    Moreover, it is also shown that the adversarial examples generated using the unsecured
    (un-distilled) networks transfer well to the secured (distilled) networks, which
    makes the computed perturbations suitable for black-box attacks.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Carlini 和 Wagner [[36](#bib.bib36)] 在防御蒸馏针对对抗扰动 [[38](#bib.bib38)] 的背景下介绍了一组三种对抗攻击。这些攻击通过限制其
    $\ell_{2}$、$\ell_{\infty}$ 和 $\ell_{0}$ 范数，使扰动几乎不可感知，且显示出针对目标网络的防御蒸馏几乎完全失败。此外，还显示了使用未蒸馏（未处理）网络生成的对抗样本在安全（蒸馏）网络上传递良好，这使得计算出的扰动适合用于黑箱攻击。
- en: Whereas it is more common to exploit the transferability property of adversarial
    examples to generate black-box attacks, Chen et al. [[41](#bib.bib41)] also proposed
    ‘Zeroth Order Optimization (ZOO)’ based attacks that directly estimate the gradients
    of the targeted model for generating the adversarial examples. These attacks were
    inspired by C&W attacks. We refer to the original papers for further details on
    C&W and ZOO attacks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管利用对抗样本的迁移性属性来生成黑箱攻击更为常见，但 Chen 等人 [[41](#bib.bib41)] 也提出了基于“零阶优化（ZOO）”的攻击，该方法直接估计目标模型的梯度以生成对抗样本。这些攻击受到
    C&W 攻击的启发。有关 C&W 和 ZOO 攻击的更多细节，请参阅原始论文。
- en: 3.1.7 DeepFool
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.7 DeepFool
- en: Moosavi-Dezfooli et al. [[72](#bib.bib72)] proposed to compute a minimal norm
    adversarial perturbation for a given image in an iterative manner. Their algorithm,
    i.e. DeepFool initializes with the clean image that is assumed to reside in a
    region confined by the decision boundaries of the classifier. This region decides
    the class-label of the image. At each iteration, the algorithm perturbs the image
    by a small vector that is computed to take the resulting image to the boundary
    of the polyhydron that is obtained by linearizing the boundaries of the region
    within which the image resides. The perturbations added to the image in each iteration
    are accumulated to compute the final perturbation once the perturbed image changes
    its label according to the original decision boundaries of the network. The authors
    show that the DeepFool algorithm is able to compute perturbations that are smaller
    than the perturbations computed by FGSM [[23](#bib.bib23)] in terms of their norm,
    while having similar fooling ratios.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Moosavi-Dezfooli 等人 [[72](#bib.bib72)] 提出了以迭代方式计算给定图像的最小范数对抗扰动。他们的算法，即 DeepFool，以被假定位于分类器决策边界所限制区域内的干净图像作为初始化。该区域决定了图像的类别标签。在每次迭代中，算法通过计算得到的小向量来扰动图像，使得结果图像接近由线性化区域边界形成的多面体边界。每次迭代中添加到图像的扰动被累积，以计算最终扰动，直到扰动图像根据网络的原始决策边界改变标签为止。作者展示了
    DeepFool 算法能够计算出比 FGSM [[23](#bib.bib23)] 计算的扰动范数更小的扰动，同时拥有类似的欺骗率。
- en: 3.1.8 Universal Adversarial Perturbations
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.8 通用对抗扰动
- en: 'Whereas the methods like FGSM [[23](#bib.bib23)], ILCM [[35](#bib.bib35)],
    DeepFool [[72](#bib.bib72)] etc. compute perturbations to fool a network on a
    single image, the ‘universal’ adversarial perturbations computed by Moosavi-Dezfooli
    et al. [[16](#bib.bib16)] are able to fool a network on ‘any’ image with high
    probability. These image-agnostic perturbations also remain quasi-imperceptible
    for the human vision system, as can be observed in Fig. [1](#S1.F1 "Figure 1 ‣
    1 Introduction ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey"). To formally define these perturbations, let us assume that clean images
    are sampled from the distribution $\boldsymbol{\Im}_{c}$. A perturbation $\boldsymbol{\rho}$
    is ‘universal’ if it satisfies the following constraint:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '与 FGSM [[23](#bib.bib23)]、ILCM [[35](#bib.bib35)]、DeepFool [[72](#bib.bib72)]
    等方法通过计算扰动来欺骗网络的单张图像不同，Moosavi-Dezfooli 等人 [[16](#bib.bib16)] 计算的‘通用’对抗扰动能够以高概率欺骗网络上的‘任何’图像。这些与图像无关的扰动对于人类视觉系统仍然几乎不可察觉，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey") 所示。为了正式定义这些扰动，假设干净的图像是从分布 $\boldsymbol{\Im}_{c}$
    中采样的。一个扰动 $\boldsymbol{\rho}$ 是‘通用’的，如果它满足以下约束：'
- en: '|  | $\displaystyle\underset{{\bf I}_{c}\sim\boldsymbol{\Im}_{c}}{\mathrm{\text{P}}}\Big{(}\mathcal{C}({\bf
    I}_{c})\neq\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})\Big{)}\geq\delta~{}~{}~{}\text{s.t.}~{}~{}&#124;&#124;\boldsymbol{\rho}&#124;&#124;_{p}\leq\xi,$
    |  | (6) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\underset{{\bf I}_{c}\sim\boldsymbol{\Im}_{c}}{\mathrm{\text{P}}}\Big{(}\mathcal{C}({\bf
    I}_{c})\neq\mathcal{C}({\bf I}_{c}+\boldsymbol{\rho})\Big{)}\geq\delta~{}~{}~{}\text{s.t.}~{}~{}&#124;&#124;\boldsymbol{\rho}&#124;&#124;_{p}\leq\xi,$
    |  | (6) |'
- en: 'where P(.) denotes the probability, $\delta\in(0,1]$ is the fooling ratio,
    $||.||_{p}$ denotes the $\ell_{p}$-norm and $\xi$ is a pre-defined constant. The
    smaller the value of $\xi$, the harder it is to perceive the perturbation in the
    image. Strictly speaking, the perturbations that satisfy ([6](#S3.E6 "In 3.1.8
    Universal Adversarial Perturbations ‣ 3.1 Attacks for classification ‣ 3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")) should be referred to as $(\delta,\xi)$-universal because of their strong
    dependence on the mentioned parameters. However, these perturbations are commonly
    referred to as the ‘universal adversarial perturbations’ in the literature.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 P(.) 表示概率，$\delta\in(0,1]$ 是欺骗率，$||.||_{p}$ 表示 $\ell_{p}$-范数，$\xi$ 是预定义常量。$\xi$
    的值越小，图像中扰动的感知难度越大。严格来说，满足 ([6](#S3.E6 "In 3.1.8 Universal Adversarial Perturbations
    ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")) 的扰动应称为 $(\delta,\xi)$-通用，因为它们对这些参数有很强的依赖。然而，这些扰动在文献中通常被称为‘通用对抗扰动’。'
- en: The authors computed the universal perturbations by restricting their $\ell_{2}$-norm
    as well as $\ell_{\infty}$-norm, and showed that the perturbations with their
    norms upper bounded by $4\%$ of the respective image norms already achieved significant
    fooling ratios of around 0.8 or more for state-of-the-art image classifiers. Their
    iterative approach to compute a perturbation is related to the DeepFool strategy [[72](#bib.bib72)]
    of gradually pushing a data point (i.e. an image) to the decision boundary for
    its class. However, in this case, ‘all’ the training data points are sequentially
    pushed to the respective decision boundaries and the perturbations computed over
    all the images are gradually accumulated by back-projecting the accumulator to
    the desired $\ell_{p}$ ball of radius $\xi$ every time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 作者通过限制其 $\ell_{2}$-范数以及 $\ell_{\infty}$-范数来计算通用扰动，并展示了其范数上界为图像范数的 $4\%$ 的扰动在最先进的图像分类器中已经实现了约
    0.8 或更高的显著欺骗率。他们的迭代计算扰动的方法与 DeepFool 策略 [[72](#bib.bib72)] 相关，该策略通过逐步推送数据点（即图像）到其类别的决策边界来计算扰动。然而，在这种情况下，‘所有’训练数据点都被顺序推送到各自的决策边界，并且计算的扰动通过每次将累加器反投影到所需的
    $\ell_{p}$ 半径为 $\xi$ 的球体上来逐步累积。
- en: The algorithm proposed by Moosavi-Dezfooli et al. [[16](#bib.bib16)] computes
    perturbations while targeting a single network model, e.g. ResNet [[147](#bib.bib147)].
    However, it is shown that these perturbations also generalize well across different
    networks (especially those having similar architectures). In that sense, the author’s
    claim the perturbations to be, to some extent, ‘doubly universal’. Moreover, it
    is also shown that high fooling ratio (e.g. $\delta\geq 0.5$) is achievable by
    learning a perturbation using only around $2,000$ training images.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Moosavi-Dezfooli 等人[[16](#bib.bib16)] 提出的算法在针对单一网络模型（例如 ResNet [[147](#bib.bib147)]）时计算扰动。然而，研究表明这些扰动在不同网络（特别是那些具有相似架构的网络）上也能很好地泛化。从这个意义上讲，作者认为这些扰动在某种程度上是“二重通用的”。此外，还表明通过仅使用约
    $2,000$ 张训练图像可以实现高欺骗比率（例如 $\delta\geq 0.5$）。
- en: Khrulkov et al. [[190](#bib.bib190)] also proposed a method for constructing
    universal adversarial perturbations as singular vectors of the Jacobian matrices
    of feature maps of the networks, which allowed for achieving relatively high fooling
    rates using only a small number of images. Another method to generate universal
    perturbations is fast-feature-fool by Mopuri et al. [[135](#bib.bib135)]. Their
    method generates the universal perturbations independent of data.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: Khrulkov 等人[[190](#bib.bib190)] 还提出了一种方法，通过网络的特征图雅可比矩阵的奇异向量来构建通用对抗扰动，这种方法只需少量图像即可实现相对较高的欺骗率。另一种生成通用扰动的方法是
    Mopuri 等人[[135](#bib.bib135)] 提出的快速特征欺骗（fast-feature-fool）。他们的方法生成的通用扰动与数据无关。
- en: 3.1.9 UPSET and ANGRI
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.9 UPSET 和 ANGRI
- en: 'Sarkar et al. [[146](#bib.bib146)] proposed two black-box attack algorithms,
    namely UPSET: Universal Perturbations for Steering to Exact Targets, and ANGRI:
    Antagonistic Network for Generating Rogue Images for targeted fooling of deep
    neural networks. For ‘n’ classes, UPSET seeks to produce ‘n’ image-agnostic perturbations
    such that when the perturbation is added to an image that does not belong to a
    targeted class, the classifier will classify the perturbed image as being from
    that class. The power of UPSET comes from a residual generating network R(.),
    that takes the target class ‘t’ as input and produces a perturbation R(t) for
    fooling. The overall method solves the following optimization problem using the
    so-called UPSET network:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 'Sarkar 等人[[146](#bib.bib146)] 提出了两种黑箱攻击算法，即 UPSET: Universal Perturbations
    for Steering to Exact Targets（通用扰动引导到精确目标）和 ANGRI: Antagonistic Network for Generating
    Rogue Images（敌对网络生成恶意图像）用于针对深度神经网络的欺骗。对于‘n’个类别，UPSET 旨在生成‘n’种图像无关的扰动，使得当这些扰动添加到不属于目标类别的图像上时，分类器将把扰动图像归为该类别。UPSET
    的威力来自于一个残差生成网络 R(.)，它以目标类别‘t’作为输入，并生成扰动 R(t) 用于欺骗。整体方法使用所谓的 UPSET 网络解决以下优化问题：'
- en: '|  | $\displaystyle{\bf I}_{\boldsymbol{\rho}}=\max(\min(s\text{R(t)}+{\bf
    I}_{c},1),-1),$ |  | (7) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle{\bf I}_{\boldsymbol{\rho}}=\max(\min(s\text{R(t)}+{\bf
    I}_{c},1),-1),$ |  | (7) |'
- en: where the pixel values in ${\bf I}_{c}$ are normalized to lie in $[-1,1]$, and
    ‘$s$’ is a scalar. To ensure ${\bf I}_{\boldsymbol{\rho}}$ to be a valid image,
    all values outside the interval $[-1,1]$ are clipped. As compared to the image-agnostic
    perturbations of UPSET, ANGRI computes image-specific perturbations in a closely
    related manner, for which we refer to the original work. The perturbations resulting
    from ANGRI are also used for targeted fooling. Both algorithms have been reported
    to achieve high fooling ratios on MNIST [[10](#bib.bib10)] and CIFAR-10 [[152](#bib.bib152)]
    datasets.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，${\bf I}_{c}$ 中的像素值被归一化到 $[-1,1]$ 区间，而‘s’是一个标量。为了确保 ${\bf I}_{\boldsymbol{\rho}}$
    是有效图像，所有超出区间 $[-1,1]$ 的值都被剪裁。与 UPSET 的图像无关扰动相比，ANGRI 以类似的方式计算图像特定的扰动，具体方法请参阅原始工作。ANGRI
    产生的扰动也用于有针对性的欺骗。这两种算法在 MNIST [[10](#bib.bib10)] 和 CIFAR-10 [[152](#bib.bib152)]
    数据集上均报告了高欺骗比率。
- en: 3.1.10 Houdini
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.10 Houdini
- en: Cisse et al. [[131](#bib.bib131)] proposed ‘Houdini’- an approach for fooling
    gradient-based learning machines by generating adversarial examples that can be
    tailored to task losses. Typical algorithms that generate adversarial examples
    employ gradients of differentiable loss functions of the networks to compute the
    perturbations. However, task losses are often not amenable to this approach. For
    instance, the task loss of speech recognition is based on word-error-rate, which
    does not allow straightforward exploitation of loss function gradient. Houdini
    is tailored to generate adversarial examples for such tasks. Besides successful
    generation of adversarial images for classification, Houdini has also been shown
    to successfully attack a popular deep Automatic Speech Recognition system [[151](#bib.bib151)].
    The authors have also demonstrated the transferability of attacks in speech recognition
    by fooling Google Voice in a black-box attack scenario. Moreover, successful targeted
    and non-targeted attacks are also demonstrated for a deep learning model for human
    pose estimation.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Cisse 等人 [[131](#bib.bib131)] 提出了 ‘Houdini’——一种通过生成可针对任务损失进行定制的对抗样本来欺骗基于梯度的学习机器的方法。典型的对抗样本生成算法使用网络的可微分损失函数的梯度来计算扰动。然而，任务损失通常不适用于这种方法。例如，语音识别的任务损失基于词错误率，这不允许直接利用损失函数梯度。Houdini
    被定制用于生成此类任务的对抗样本。除了成功生成用于分类的对抗图像外，Houdini 还成功攻击了一个流行的深度自动语音识别系统 [[151](#bib.bib151)]。作者还通过在黑盒攻击场景中欺骗
    Google Voice 展示了攻击在语音识别中的可转移性。此外，还展示了对人类姿态估计的深度学习模型成功的定向和非定向攻击。
- en: 3.1.11 Adversarial Transformation Networks (ATNs)
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.11 对抗变换网络 (ATNs)
- en: Baluja and Fischer [[42](#bib.bib42)] trained feed-forward neural networks to
    generate adversarial examples against other targeted networks or set of networks.
    The trained models were termed Adversarial Transformation Networks (ATNs). The
    adversarial examples generated by these networks are computed by minimizing a
    joint loss function comprising of two parts. The first part restricts the adversarial
    example to have perceptual similarity with the original image, whereas the second
    part aims at altering the prediction of the targeted network on the resulting
    image.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Baluja 和 Fischer [[42](#bib.bib42)] 训练了前馈神经网络，以生成针对其他定向网络或网络组的对抗样本。这些训练后的模型被称为对抗变换网络
    (ATNs)。这些网络生成的对抗样本是通过最小化包含两个部分的联合损失函数来计算的。第一部分限制对抗样本与原始图像在感知上的相似性，而第二部分则旨在改变目标网络对结果图像的预测。
- en: Along the same direction, Hayex and Danezis [[47](#bib.bib47)] also used an
    attacker neural network to learn adversarial examples for black-box attacks. In
    the presented results, the examples computed by the attacker network remain perceptually
    indistinguishable from the clean images but they are misclassified by the targeted
    networks with overwhelming probabilities - reducing classification accuracy from
    99.4% to 0.77% on MNIST data [[10](#bib.bib10)], and from 91.4% to 6.8% on the
    CIFAR-10 dataset [[152](#bib.bib152)].
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在相同方向上，Hayex 和 Danezis [[47](#bib.bib47)] 也使用了攻击者神经网络来生成黑盒攻击的对抗样本。在展示的结果中，攻击者网络计算出的样本在感知上与干净图像无异，但被定向网络以压倒性的概率错误分类——使
    MNIST 数据集上的分类准确率从 99.4% 降至 0.77%，而 CIFAR-10 数据集上的分类准确率从 91.4% 降至 6.8% [[10](#bib.bib10)]，[[152](#bib.bib152)]。
- en: 'TABLE I: Summary of the attributes of diverse attacking methods: The ‘perturbation
    norm’ indicates the restricted $\ell_{p}$-norm of the perturbations to make them
    imperceptible. The strength (higher for more asterisks) is based on the impression
    from the reviewed literature.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '表 I: 各种攻击方法属性的总结：‘扰动范数’指的是为了使扰动不可感知的受限 $\ell_{p}$-范数。强度（更多星号表示更强）基于所审阅文献中的印象。'
- en: '| Method | Black/White box | Targeted/Non-targeted | Specific/Universal | Perturbation
    norm | Learning | Strength |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | 黑盒/白盒 | 定向/非定向 | 特定/通用 | 扰动范数 | 学习 | 强度 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| L-BFGS [[22](#bib.bib22)] | White box | Targeted | Image specific | $\ell_{\infty}$
    | One shot | $***$ |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| L-BFGS [[22](#bib.bib22)] | 白盒 | 定向攻击 | 图像特定 | $\ell_{\infty}$ | 一次性 | $***$
    |'
- en: '| FGSM [[23](#bib.bib23)] | White box | Targeted | Image specific | $\ell_{\infty}$
    | One shot | $***$ |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| FGSM [[23](#bib.bib23)] | 白盒 | 定向攻击 | 图像特定 | $\ell_{\infty}$ | 一次性 | $***$
    |'
- en: '| BIM & ILCM [[35](#bib.bib35)] | White box | Non targeted | Image specific
    | $\ell_{\infty}$ | Iterative | $**$$**$ |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| BIM & ILCM [[35](#bib.bib35)] | 白盒 | 非定向攻击 | 图像特定 | $\ell_{\infty}$ | 迭代式
    | $**$$**$ |'
- en: '| JSMA [[60](#bib.bib60)] | White box | Targeted | Image specific | $\ell_{0}$
    | Iterative | $***$ |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| JSMA [[60](#bib.bib60)] | 白盒 | 定向攻击 | 图像特定 | $\ell_{0}$ | 迭代式 | $***$ |'
- en: '| One-pixel [[68](#bib.bib68)] | Black box | Non Targeted | Image specific
    | $\ell_{0}$ | Iterative | $**$ |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| One-pixel [[68](#bib.bib68)] | 黑盒 | 非定向攻击 | 图像特定 | $\ell_{0}$ | 迭代式 | $**$
    |'
- en: '| C&W attacks [[36](#bib.bib36)] | White box | Targeted | Image specific |
    $\ell_{0},\ell_{2},\ell_{\infty}$ | Iterative | $*****$ |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| C&W 攻击 [[36](#bib.bib36)] | 白盒 | 定向攻击 | 图像特定 | $\ell_{0},\ell_{2},\ell_{\infty}$
    | 迭代式 | $*****$ |'
- en: '| DeepFool [[72](#bib.bib72)] | White box | Non targeted | Image specific |
    $\ell_{2},\ell_{\infty}$ | Iterative | $**$$**$ |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| DeepFool [[72](#bib.bib72)] | 白盒 | 非定向攻击 | 图像特定 | $\ell_{2},\ell_{\infty}$
    | 迭代式 | $**$$**$ |'
- en: '| Uni. perturbations [[16](#bib.bib16)] | White box | Non targeted | Universal
    | $\ell_{2},\ell_{\infty}$ | Iterative | $*****$ |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| Uni. 扰动 [[16](#bib.bib16)] | 白盒 | 非定向攻击 | 通用 | $\ell_{2},\ell_{\infty}$ |
    迭代式 | $*****$ |'
- en: '| UPSET [[146](#bib.bib146)] | Black box | Targeted | Universal | $\ell_{\infty}$
    | Iterative | $**$$**$ |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| UPSET [[146](#bib.bib146)] | 黑盒 | 定向攻击 | 通用 | $\ell_{\infty}$ | 迭代式 | $**$$**$
    |'
- en: '| ANGRI [[146](#bib.bib146)] | Black box | Targeted | Image specific | $\ell_{\infty}$
    | Iterative | $**$$**$ |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| ANGRI [[146](#bib.bib146)] | 黑盒 | 定向攻击 | 图像特定 | $\ell_{\infty}$ | 迭代式 | $**$$**$
    |'
- en: '| Houdini [[131](#bib.bib131)] | Black box | Targeted | Image specific | $\ell_{2},\ell_{\infty}$
    | Iterative | $**$$**$ |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| Houdini [[131](#bib.bib131)] | 黑盒 | 定向攻击 | 图像特定 | $\ell_{2},\ell_{\infty}$
    | 迭代式 | $**$$**$ |'
- en: '| ATNs [[42](#bib.bib42)] | White box | Targeted | Image specific | $\ell_{\infty}$
    | Iterative | $**$$**$ |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| ATNs [[42](#bib.bib42)] | 白盒 | 定向攻击 | 图像特定 | $\ell_{\infty}$ | 迭代式 | $**$$**$
    |'
- en: 3.1.12 Miscellaneous Attacks
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.12 杂项攻击
- en: 'The adversarial attacks discussed above are either the popular ones in the
    recent literature or they are representative of the research directions that are
    fast becoming popular. A summary of the main attributes of these attacks is also
    provided in Table [I](#S3.T1 "TABLE I ‣ 3.1.11 Adversarial Transformation Networks
    (ATNs) ‣ 3.1 Attacks for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey"). For a comprehensive study,
    below we provide brief descriptions of further techniques to generate adversarial
    attacks on deep neural networks. We note that this research area is currently
    highly active. Whereas every attempt has been made to review as many approaches
    as possible, we do not claim the review to be exhaustive. Due to high activity
    in this research direction, many more attacks are likely to surface in the near
    future.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '上述讨论的对抗攻击要么是最近文献中的流行攻击，要么是代表性地快速变得流行的研究方向。这些攻击的主要特征的总结也提供在表[I](#S3.T1 "TABLE
    I ‣ 3.1.11 Adversarial Transformation Networks (ATNs) ‣ 3.1 Attacks for classification
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")中。为了全面研究，下面我们提供进一步生成对抗攻击深度神经网络的技术的简要描述。我们注意到，这一研究领域目前非常活跃。尽管尽力回顾了尽可能多的方法，但我们不声称回顾是详尽的。由于这一研究方向的高活动性，未来可能会出现更多攻击。'
- en: Sabour et al. [[26](#bib.bib26)] showed the possibility of generating adversarial
    examples by altering the internal layers of deep neural networks. The authors
    demonstrated that it is possible to make internal network representation of adversarial
    images to resemble representations of images from different classes. Papernot
    et al. [[109](#bib.bib109)] studied transferability of adversarial attacks for
    deep learning as well as other machine learning techniques and introduced further
    transferability attacks. Narodytska and Kasiviswanathan [[54](#bib.bib54)] also
    introduced further black-box attacks that have been found effective in fooling
    the neural networks by changing only few pixel values in the images. Liu et al. [[31](#bib.bib31)]
    introduced ‘epsilon-neighborhood’ attack that have been shown to fool defensively
    distilled networks [[108](#bib.bib108)] with $100\%$ success for white-box attacks.
    Oh et al. [[133](#bib.bib133)] took a ‘Game Theory’ perspective on adversarial
    attacks and derived a strategy to counter the counter-measures taken against adversarial
    attacks on deep neural networks. Mpouri et al. [[135](#bib.bib135)] developed
    a data-independent approach to generate universal adversarial perturbations for
    the deep network models. Hosseini et al. [[98](#bib.bib98)] introduced the notion
    of ‘semantic adversarial examples’ - input images that represent semantically
    same objects for humans but deep neural networks misclassify them. They used negatives
    of the images as semantic adversarial examples. Kanbak et al. [[73](#bib.bib73)]
    introduced ‘ManiFool’ algorithm in the wake of DeepFool method [[72](#bib.bib72)]
    to measure robustness of deep neural networks against geometrically perturbed
    images. Dong et al. [[170](#bib.bib170)] proposed an iterative method to boost
    adversarial attacks for black-box scenarios. Recently, Carlini and Wagner [[59](#bib.bib59)]
    also demonstrated that ten different defenses against perturbations can again
    be defeated by new attacks constructed using new loss functions. Rozsa et al. [[94](#bib.bib94)]
    also proposed a ‘hot/cold’ method for generating multiple possible adversarial
    examples for a single image. Interestingly, adversarial perturbations are not
    only being added to images to reduces the accuracy of deep learning classifiers.
    Yoo et al. [[195](#bib.bib195)] recently proposed an approach to also slightly
    improve the classification performance with the help of subtle perturbation to
    images.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Sabour 等人 [[26](#bib.bib26)] 展示了通过改变深度神经网络的内部层可以生成对抗性示例的可能性。作者们证明了可以使对抗性图像的内部网络表示与来自不同类别的图像的表示相似。Papernot
    等人 [[109](#bib.bib109)] 研究了对深度学习以及其他机器学习技术的对抗性攻击的可转移性，并引入了进一步的可转移攻击。Narodytska
    和 Kasiviswanathan [[54](#bib.bib54)] 也引入了进一步的黑盒攻击方法，这些攻击方法通过仅更改图像中的少数像素值就能成功愚弄神经网络。Liu
    等人 [[31](#bib.bib31)] 提出了‘epsilon-邻域’攻击方法，已被证明可以以$100\%$的成功率愚弄防御性精馏网络 [[108](#bib.bib108)]
    进行白盒攻击。Oh 等人 [[133](#bib.bib133)] 从‘博弈论’的角度对对抗性攻击进行了研究，并推导出了对抗性攻击深度神经网络所采取的反措施的策略。Mpouri
    等人 [[135](#bib.bib135)] 开发了一种数据独立的方法来生成深度网络模型的通用对抗扰动。Hosseini 等人 [[98](#bib.bib98)]
    提出了‘语义对抗性示例’的概念 - 输入图像对于人类来说代表语义相同的对象，但深度神经网络会误分类它们。他们使用图像的负面作为语义对抗性示例。Kanbak
    等人 [[73](#bib.bib73)] 在DeepFool方法 [[72](#bib.bib72)] 后提出了‘ManiFool’算法，用于测量深度神经网络对几何扰动图像的稳健性。Dong
    等人 [[170](#bib.bib170)] 提出了一种迭代方法，以增强黑盒场景下的对抗攻击。最近，Carlini 和 Wagner [[59](#bib.bib59)]
    也证明了十种不同的抵御扰动的方法可以通过使用新的损失函数构造的新攻击再次被打败。Rozsa 等人 [[94](#bib.bib94)] 还提出了一种‘热/冷’方法，用于生成单个图像的多个可能对抗性示例。有趣的是，对抗性扰动不仅仅是为了降低深度学习分类器的准确性。Yoo
    等人 [[195](#bib.bib195)] 最近提出了一种方法，通过对图像进行微小扰动，还能略微提高分类性能。
- en: We note that the authors of many works reviewed in this article have made the
    source code of their implementations publicly available. This is one of the major
    reasons behind the current rise in this research direction. Beside those resources,
    there are also libraries, e.g. Cleverhans [[111](#bib.bib111)], [[112](#bib.bib112)]
    that have started emerging in order to further boost this research direction.
    Adversarial-Playground ([https://github.com/QData/AdversarialDNN-Playground](https://github.com/QData/AdversarialDNN-Playground))
    is another example of a toolbox made public by Norton and Qi [[142](#bib.bib142)]
    to understand adversarial attacks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们注意到，本文中回顾的许多工作的作者已将其实现的源代码公开。这是当前该研究方向兴起的主要原因之一。除了这些资源，还有一些库，例如 Cleverhans
    [[111](#bib.bib111)], [[112](#bib.bib112)]，也开始出现，以进一步推动这一研究方向。Adversarial-Playground
    ([https://github.com/QData/AdversarialDNN-Playground](https://github.com/QData/AdversarialDNN-Playground))
    是由 Norton 和 Qi [[142](#bib.bib142)] 公开的另一个工具箱示例，用于理解对抗攻击。
- en: 3.2 Attacks beyond classification/recognition
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 分类/识别之外的攻击
- en: 'With the exception of Houdini [[131](#bib.bib131)], all the mainstream adversarial
    attacks reviewed in Section [3.1](#S3.SS1 "3.1 Attacks for classification ‣ 3
    Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey") directly focused on the task of classification - typically
    fooling CNN-based [[10](#bib.bib10)] classifiers. However, due to the seriousness
    of adversarial threats, attacks are also being actively investigated beyond the
    classification/recognition task in Computer Vision. Below, we review the works
    that develop approaches to attack deep neural networks beyond classification.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Houdini [[131](#bib.bib131)] 之外，所有在第 [3.1](#S3.SS1 "3.1 Classification 的攻击
    ‣ 3 对抗攻击 ‣ 深度学习在计算机视觉中的对抗攻击威胁：综述") 节中回顾的主流对抗攻击都直接集中在分类任务上 - 通常是欺骗基于 CNN 的 [[10](#bib.bib10)]
    分类器。然而，由于对抗威胁的严重性，攻击也在积极调查分类/识别任务之外的计算机视觉任务。下面，我们回顾了在分类任务之外攻击深度神经网络的方法。
- en: 3.2.1 Attacks on Autoencoders and Generative Models
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 对自动编码器和生成模型的攻击
- en: Tabacof et al. [[128](#bib.bib128)] investigated adversarial attacks for autoencoders [[154](#bib.bib154)],
    and proposed a technique to distort input image (to make it adversarial) that
    misleads the autoencoder to reconstruct a completely different image. Their approach
    attacks the internal representation of a neural network such that the representation
    for the adversarial image becomes similar to that of the target image. However,
    it is reported in [[128](#bib.bib128)] that autoencoders seem to be much more
    robust to adversarial attacks than the typical classifier networks. Kos et al. [[121](#bib.bib121)]
    also explored methods for computing adversarial examples for deep generative models,
    e.g. variational autoencoder (VAE) and the VAE-Generative Adversarial Networks
    (VAE-GANs). GANs, such as [[153](#bib.bib153)] are becoming exceedingly popular
    now-a-days in Computer Vision applications due to their ability to learn data
    distributions and generate realistic images using those distributions. The authors
    introduced three different classes of attacks for VAE and VAE-GANs. Owing to the
    success of these attacks it is concluded that the deep generative models are also
    vulnerable to adversaries that can convince them to turn inputs into very different
    outputs. This work adds further support to the hypothesis that “adversarial examples
    are a general phenomenon for current neural network architectures”.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Tabacof 等人 [[128](#bib.bib128)] 研究了对自动编码器 [[154](#bib.bib154)] 的对抗攻击，并提出了一种扭曲输入图像（使其具有对抗性）的方法，这会误导自动编码器重建出完全不同的图像。他们的方法攻击神经网络的内部表示，使得对抗图像的表示与目标图像的表示变得相似。然而，[[128](#bib.bib128)]
    中报告称，自动编码器似乎比典型的分类器网络对对抗攻击更加稳健。Kos 等人 [[121](#bib.bib121)] 还探讨了计算深度生成模型（如变分自动编码器（VAE）和
    VAE-生成对抗网络（VAE-GANs））的对抗示例的方法。由于 GANs，如 [[153](#bib.bib153)]，在计算机视觉应用中因其学习数据分布并使用这些分布生成逼真图像的能力而变得极受欢迎。作者介绍了对
    VAE 和 VAE-GANs 的三种不同攻击类别。由于这些攻击的成功，得出的结论是深度生成模型也易受对抗攻击，这些攻击能使输入变成完全不同的输出。这项工作进一步支持了“对抗示例是当前神经网络架构的普遍现象”的假设。
- en: 3.2.2 Attack on Recurrent Neural Networks
  id: totrans-133
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2 对递归神经网络的攻击
- en: Papernot et al. [[110](#bib.bib110)] successfully generated adversarial input
    sequences for Recurrent Neural Networks (RNNs). RNNs are deep learning models
    that are particularly suitable for learning mappings between sequential inputs
    and outputs [[155](#bib.bib155)]. Papernot et al. demonstrated that the algorithms
    proposed to compute adversarial examples for the feed-forward neural networks
    (e.g. FGSM [[23](#bib.bib23)]) can also be adapted for fooling RNNs. In particular,
    the authors demonstrated successful fooling of the popular Long-Short-Term-Memory
    (LSTM) RNN architecture [[156](#bib.bib156)]. It is concluded that the cyclic
    neural network model like RNNs are also not immune to the adversarial perturbations
    that were originally uncovered in the context of acyclic neural networks, i.e. CNNs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Papernot等人[[110](#bib.bib110)]成功生成了针对递归神经网络（RNNs）的对抗输入序列。RNNs是特别适合学习序列输入和输出之间映射的深度学习模型[[155](#bib.bib155)]。Papernot等人展示了计算前馈神经网络（例如FGSM[[23](#bib.bib23)]）的对抗样本的算法也可以被调整用来欺骗RNNs。特别是，作者展示了成功欺骗了流行的长短期记忆（LSTM）RNN架构[[156](#bib.bib156)]。结论是，像RNNs这样的循环神经网络模型也无法抵御最初在非循环神经网络（即CNNs）背景下发现的对抗扰动。
- en: 3.2.3 Attacks on Deep Reinforcement Learning
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3 深度强化学习中的攻击
- en: 'Lin et al. [[134](#bib.bib134)] proposed two different adversarial attacks
    for the agents trained by deep reinforcement learning [[157](#bib.bib157)]. In
    the first attack, called ‘strategically-timed attack’, the adversary minimizes
    the reward of the agent by attacking it at a small subset of time steps in an
    episode. A method is proposed to determine when an adversarial example should
    be crafted and applied, which enables the attack to go undetected. In the second
    attack, referred as ‘enchanting attack’, the adversary lures the agent to a designated
    target state by integrating a generative model and a planning algorithm. The generative
    model is used for predicting the future states of the agent, whereas the planning
    algorithm generates the actions for luring it. The attacks are successfully tested
    against the agents trained by the state-of-the-art deep reinforcement learning
    algorithms [[157](#bib.bib157)], [[158](#bib.bib158)]. Details on this work and
    example videos of the adversarial attacks can be found on the following URL: [http://yclin.me/adversarial_attack_RL/](http://yclin.me/adversarial_attack_RL/).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: Lin等人[[134](#bib.bib134)]提出了两种不同的对抗攻击方法，针对的是通过深度强化学习训练的代理[[157](#bib.bib157)]。第一种攻击称为“策略性定时攻击”，对手通过在一个回合中的小部分时间步骤进行攻击，来最小化代理的奖励。提出了一种方法来确定何时应该制作和应用对抗样本，从而使攻击能够未被发现。第二种攻击，称为“诱导攻击”，对手通过结合生成模型和规划算法，将代理诱导到指定的目标状态。生成模型用于预测代理的未来状态，而规划算法生成诱导其的动作。这些攻击已成功地测试了最新深度强化学习算法训练的代理[[157](#bib.bib157)]，[[158](#bib.bib158)]。有关此工作的详细信息和对抗攻击的示例视频可以在以下网址找到：[http://yclin.me/adversarial_attack_RL/](http://yclin.me/adversarial_attack_RL/)。
- en: In another work, Huang et al. [[62](#bib.bib62)] demonstrated that FGSM [[23](#bib.bib23)]
    can also be used to significantly degrade performance of trained policies in the
    context of deep reinforcement learning. Their threat model considers adversaries
    that are capable of introducing minor perturbations to the raw input of the policy.
    The conducted experiments demonstrate that it is fairly easy to confuse neural
    network policies with adversarial examples, even in black-box scenarios. Videos
    and further details on this work are available on [http://rll.berkeley.edu/adversarial/](http://rll.berkeley.edu/adversarial/).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一项工作中，Huang等人[[62](#bib.bib62)]展示了FGSM[[23](#bib.bib23)]也可以用来显著降低在深度强化学习背景下训练策略的性能。他们的威胁模型考虑了能够对策略的原始输入引入微小扰动的对手。实验表明，即使在黑箱场景中，也很容易用对抗样本混淆神经网络策略。有关此工作的更多视频和详细信息可在[http://rll.berkeley.edu/adversarial/](http://rll.berkeley.edu/adversarial/)找到。
- en: 3.2.4 Attacks on Semantic Segmentation and Object Detection
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4 对语义分割和目标检测的攻击
- en: Semantic image segmentation and object detection are among the mainstream problems
    in Computer Vision. Inspired by Moosavi-Dezfooli [[16](#bib.bib16)], Metzen et
    al. [[67](#bib.bib67)] showed the existence of image-agnostic quasi-imperceptible
    perturbations that can fool a deep neural network into significantly corrupting
    the predicted segmentation of the images. Moreover, they also showed that it is
    possible to compute noise vectors that can remove a specific class from the segmented
    classes while keeping most of the image segmentation unchanged (e.g. removing
    pedestrians from road scenes). Although it is argued that the “space of the adversarial
    perturbations for the semantic image segmentation is presumably smaller than image
    classification”, the perturbations have been shown to generalize well for unseen
    validation images with high probability. Arnab et al. [[51](#bib.bib51)] also
    evaluated FGSM [[23](#bib.bib23)] based adversarial attacks for semantic segmentation
    and noted that many observations about these attacks for classification do not
    directly transfer to segmentation task.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 语义图像分割和目标检测是计算机视觉中的主流问题。受Moosavi-Dezfooli [[16](#bib.bib16)]的启发，Metzen等人[[67](#bib.bib67)]
    展示了存在图像无关的准不可感知的扰动，这些扰动可以欺骗深度神经网络，从而显著破坏图像的预测分割。此外，他们还展示了可以计算噪声向量，从而去除分割类别中的特定类别，同时保持大部分图像分割不变（例如，从道路场景中去除行人）。尽管有观点认为“语义图像分割的对抗扰动空间可能小于图像分类”，但这些扰动已被证明能够在未见过的验证图像中表现出较高的泛化能力。Arnab等人[[51](#bib.bib51)]
    还评估了基于FGSM [[23](#bib.bib23)] 的对抗攻击对于语义分割的效果，并指出许多关于这些攻击的观察结果对于分割任务并不直接适用。
- en: 'Xie et al. [[115](#bib.bib115)] computed adversarial examples for semantic
    segmentation and object detection under the observation that these tasks can be
    formulated as classifying multiple targets in an image - the target is a pixel
    or a receptive field in segmentation, and object proposal in detection. Under
    this perspective, their approach, called ‘Dense Adversary Generation’ optimizes
    a loss function over a set of pixels/proposals to generate adversarial examples.
    The generated examples are tested to fool a variety of deep learning based segmentation
    and detection approaches. Their experimental evaluation not only demonstrates
    successful fooling of the targeted networks but also shows that the generated
    perturbations generalize well across different network models. In Fig. [4](#S3.F4
    "Figure 4 ‣ 3.2.4 Attacks on Semantic Segmentation and Object Detection ‣ 3.2
    Attacks beyond classification/recognition ‣ 3 Adversarial attacks ‣ Threat of
    Adversarial Attacks on Deep Learning in Computer Vision: A Survey"), we show a
    representative example of network fooling for segmentation and detection using
    the approach in [[115](#bib.bib115)].'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xie等人[[115](#bib.bib115)] 在观察到这些任务可以被表述为对图像中多个目标的分类——目标是分割中的像素或感受野，检测中的目标提议——后，计算了用于语义分割和目标检测的对抗样本。在这种视角下，他们的方法被称为“密集对抗生成”，通过对一组像素/提议优化损失函数来生成对抗样本。生成的样本经过测试以欺骗多种基于深度学习的分割和检测方法。他们的实验评估不仅展示了成功欺骗目标网络，还表明生成的扰动在不同网络模型中表现出良好的泛化能力。在图[4](#S3.F4
    "Figure 4 ‣ 3.2.4 Attacks on Semantic Segmentation and Object Detection ‣ 3.2
    Attacks beyond classification/recognition ‣ 3 Adversarial attacks ‣ Threat of
    Adversarial Attacks on Deep Learning in Computer Vision: A Survey")中，我们展示了使用[[115](#bib.bib115)]中的方法对分割和检测进行网络欺骗的一个代表性例子。'
- en: '![Refer to caption](img/8daade2dc7ab4ea04f130e8a46e5ba31.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8daade2dc7ab4ea04f130e8a46e5ba31.png)'
- en: 'Figure 4: Adversarial example for semantic segmentation and object detection [[115](#bib.bib115)].
    FCN [[159](#bib.bib159)] and Faster-RCNN [[150](#bib.bib150)] are used for segmentation
    and detection, respectively. Left column (top-down): Clean image, normal segmentation
    (purple region is predicted as dog) and detection results. Right column (top-down):
    Perturbation 10x, fooled segmentation (light green region is predicted as train
    and the pink region as person) and detection results.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：用于语义分割和目标检测的对抗样本[[115](#bib.bib115)]。FCN [[159](#bib.bib159)] 和 Faster-RCNN
    [[150](#bib.bib150)] 分别用于分割和检测。左列（从上到下）：干净的图像、正常分割（紫色区域被预测为狗）和检测结果。右列（从上到下）：扰动10倍、欺骗性的分割（浅绿色区域被预测为火车，粉色区域被预测为人）和检测结果。
- en: '![Refer to caption](img/ffb8e5af69b7ac91459d86c7675fd2db.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/ffb8e5af69b7ac91459d86c7675fd2db.png)'
- en: 'Figure 5: Top-row: Example of changing a facial attribute ‘wearing lipstick’
    to ‘not wearing lipstick’ by Fast Flipping Attribute method [[130](#bib.bib130)].
    Bottom row: Changing gender with perturbation generated by [[162](#bib.bib162)].'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：顶部行：使用快速翻转属性方法将面部属性“涂口红”更改为“不涂口红”的示例[[130](#bib.bib130)]。底部行：使用[[162](#bib.bib162)]生成的扰动来改变性别的示例。
- en: 4 Attacks in the real world
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 攻击在现实世界中
- en: '![Refer to caption](img/2447e21608fa97547d8d052ce125bb0b.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2447e21608fa97547d8d052ce125bb0b.png)'
- en: 'Figure 6: Example of adversarial attack on mobile phone cameras: A clean image
    (a) was taken and used to generate different adversarial images. The images were
    printed and the TensorFlow Camera Demo app [[181](#bib.bib181)] was used to classify
    them. A clean image (b) is recognized correctly as a ‘washer’ when perceived through
    the camera, whereas adversarial images (c) and (d) are mis-classified. The images
    also show network confidence in the range [0,1] for each image. The value of $\epsilon$
    is given for ([3](#S3.E3 "In 3.1.2 Fast Gradient Sign Method (FGSM) ‣ 3.1 Attacks
    for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on
    Deep Learning in Computer Vision: A Survey")).'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：对移动电话摄像头的对抗攻击示例：拍摄并使用 TensorFlow 相机演示应用程序[[181](#bib.bib181)]对生成的不同对抗图像进行分类。干净图像（a）通过摄像头正确识别为“垫圈”，而对抗图像（c）和（d）被错误分类。图像还显示了每张图像的网络置信度在区间[0,1]内。给出了$\epsilon$的值（[3](#S3.E3
    "在3.1.2快速梯度符号方法（FGSM）中 ‣ 3.1分类攻击 ‣ 3对抗攻击 ‣ 对计算机视觉中深度学习的对抗攻击威胁：一项调查")）。
- en: 4.0.1 Attacks on Face Attributes
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.0.1 对面部属性的攻击
- en: Face attributes are among the emerging soft biometrics for modern security systems.
    Although face attribute recognition can also be categorized as a classification
    problem, we separately review some interesting attacks in this direction because
    face recognition itself is treated as a mainstream problem in Computer Vision.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 面部属性是现代安全系统中新兴的软生物特征之一。尽管面部属性识别也可以归类为分类问题，但我们单独审视这个方向中的一些有趣攻击，因为面部识别本身在计算机视觉中被视为主流问题。
- en: 'Rozsa et al. [[130](#bib.bib130)], [[160](#bib.bib160)] explored the stability
    of multiple deep learning approaches using the CelebA benchmark [[161](#bib.bib161)]
    by generating adversarial examples to alter the results of facial attribute recognition,
    see top-row in Fig. [5](#S3.F5 "Figure 5 ‣ 3.2.4 Attacks on Semantic Segmentation
    and Object Detection ‣ 3.2 Attacks beyond classification/recognition ‣ 3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey"). By attacking the deep network classifiers with their so-called ‘Fast
    Flipping Attribute’ technique, they found that robustness of deep neural networks
    against the adversarial attacks varies highly between facial attributes. It is
    claimed that adversarial attacks are very effective in changing the label of a
    target attribute to a correlated attribute. Mirjalili and Ross [[162](#bib.bib162)]
    proposed a technique that modifies a face image such that its gender (for a gender
    classifier) is modified, whereas its biometric utility for a face matching system
    remains intact, see bottom-row in Fig. [5](#S3.F5 "Figure 5 ‣ 3.2.4 Attacks on
    Semantic Segmentation and Object Detection ‣ 3.2 Attacks beyond classification/recognition
    ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey"). Similarly, Shen et al. [[144](#bib.bib144)] proposed two different
    techniques to generate adversarial examples for faces that can have high ‘attractiveness
    scores’ but low ‘subjective scores’ for the face attractiveness evaluation using
    deep neural network. We refer to [[185](#bib.bib185)] for further attacks related
    to the task of face recognition.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Rozsa等人[[130](#bib.bib130)]、[[160](#bib.bib160)]通过生成对抗性示例来改变面部属性识别结果，探索了使用CelebA基准[[161](#bib.bib161)]的多种深度学习方法的稳定性，参见图[5](#S3.F5
    "图5 ‣ 3.2.4 对语义分割和目标检测的攻击 ‣ 3.2 超越分类/识别的攻击 ‣ 3 对抗性攻击 ‣ 深度学习在计算机视觉中的对抗性攻击威胁：综述")的顶行。通过用他们所谓的‘快速翻转属性’技术攻击深度网络分类器，他们发现深度神经网络对对抗性攻击的鲁棒性在面部属性之间差异很大。声称对抗性攻击在将目标属性的标签更改为相关属性方面非常有效。Mirjalili和Ross[[162](#bib.bib162)]提出了一种技术，修改面部图像，使得其性别（对于性别分类器）被改变，而其在面部匹配系统中的生物识别效用保持不变，参见图[5](#S3.F5
    "图5 ‣ 3.2.4 对语义分割和目标检测的攻击 ‣ 3.2 超越分类/识别的攻击 ‣ 3 对抗性攻击 ‣ 深度学习在计算机视觉中的对抗性攻击威胁：综述")的底行。同样，Shen等人[[144](#bib.bib144)]提出了两种不同的技术来生成面部的对抗性示例，这些示例可以在深度神经网络的面部吸引力评估中具有高‘吸引力分数’但低‘主观分数’。有关面部识别任务的进一步攻击，请参考[[185](#bib.bib185)]。
- en: 'The literature reviewed in Section [3](#S3 "3 Adversarial attacks ‣ Threat
    of Adversarial Attacks on Deep Learning in Computer Vision: A Survey") assumes
    settings where adversaries directly feed deep neural networks with perturbed images.
    Moreover, the effectiveness of attacks are also evaluated using standard image
    databases. Whereas those settings have proven sufficient to convince many researchers
    that adversarial attacks are a real concern for deep learning in practice, we
    also come across instances in the literature (e.g. [[48](#bib.bib48)], [[30](#bib.bib30)])
    where this concern is down-played and adversarial examples are implicated to be
    ‘only a matter of curiosity’ with little practical concerns. Therefore, this Section
    is specifically dedicated to the literature that deals with the adversarial attacks
    in practical real-world conditions to help settle the debate.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 第[3](#S3 "3 对抗性攻击 ‣ 深度学习在计算机视觉中的对抗性攻击威胁：综述")节中审阅的文献假设了对手直接向深度神经网络输入扰动图像的环境。此外，攻击的有效性也通过标准图像数据库进行评估。尽管这些设置已经足以让许多研究人员相信对抗性攻击确实是深度学习在实际应用中的一个真实问题，但我们也发现了一些文献（例如[[48](#bib.bib48)]、[[30](#bib.bib30)]）中对此问题的关注被淡化，并且对抗性示例被认为‘仅仅是好奇心的问题’，与实际关切关系不大。因此，本节特别致力于处理在实际现实条件下的对抗性攻击文献，以帮助解决这一争论。
- en: 4.1 Cell-phone camera attack
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 手机摄像头攻击
- en: 'Kurakin et al. [[35](#bib.bib35)] first demonstrated that threats of adversarial
    attacks also exist in the physical world. To illustrate this, they printed adversarial
    images and took snapshots from a cell-phone camera. These images were fed to TensorFlow
    Camera Demo app [[181](#bib.bib181)] that uses Google’s Inception model [[145](#bib.bib145)]
    for object classification. It was shown that a large fraction of images were misclassified
    even when perceived through the camera. In Fig. [6](#S4.F6 "Figure 6 ‣ 4 Attacks
    in the real world ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey"), an example is shown from the original paper. A video is also
    provided on the following URL [https://youtu.be/zQ_uMenoBCk](https://youtu.be/zQ_uMenoBCk)
    that shows the threat of adversarial attacks with further images. This work studies
    FGSM [[23](#bib.bib23)], BIM and ILCM [[35](#bib.bib35)] methods for attacks in
    the physical world.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kurakin 等人[[35](#bib.bib35)] 首次证明了物理世界中也存在对抗攻击的威胁。为了说明这一点，他们打印了对抗性图像，并用手机相机拍摄了快照。这些图像被输入到使用谷歌的
    Inception 模型[[145](#bib.bib145)] 进行对象分类的 TensorFlow Camera Demo 应用[[181](#bib.bib181)]
    中。结果显示，即使通过相机观察，很多图像也被误分类了。在图 [6](#S4.F6 "Figure 6 ‣ 4 Attacks in the real world
    ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey")
    中展示了原始论文中的一个例子。另有一个视频可在以下网址 [https://youtu.be/zQ_uMenoBCk](https://youtu.be/zQ_uMenoBCk)
    中查看，展示了对抗攻击的威胁及更多图像。这项工作研究了 FGSM[[23](#bib.bib23)]、BIM 和 ILCM[[35](#bib.bib35)]
    方法在物理世界中的攻击。'
- en: '![Refer to caption](img/201d9e8eb9f8745a13743cd1f5b32569.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/201d9e8eb9f8745a13743cd1f5b32569.png)'
- en: 'Figure 7: Example of road sign attack [[75](#bib.bib75)]: The success rate
    of fooling LISA-CNN [[75](#bib.bib75)] classifier on all the shown images is $100\%$.
    The distance and angle to the camera are also shown. The classifier is trained
    using LISA dataset for road signs [[176](#bib.bib176)].'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 路标攻击示例[[75](#bib.bib75)]：在所有显示的图像中欺骗 LISA-CNN[[75](#bib.bib75)] 分类器的成功率为
    $100\%$。图中还展示了距离和角度。分类器使用 LISA 数据集进行训练，该数据集用于路标[[176](#bib.bib176)]。'
- en: 4.2 Road sign attack
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 路标攻击
- en: 'Etimov et al. [[75](#bib.bib75)] built on the attacks proposed in [[36](#bib.bib36)]
    and [[88](#bib.bib88)] to design robust perturbations for the physical world.
    They demonstrated the possibility of attacks that are robust to physical conditions,
    such as variation in view angles, distance and resolution. The proposed algorithm,
    termed RP[2] for Robust Physical Perturbations, was used to generate adversarial
    examples for road sign recognition systems that achieved high fooling ratios in
    practical drive-by settings. Two attack classes were introduced in this work for
    the physical road signs, (a) poster-printing: where the attacker prints a perturbed
    road sign poster and places it over the real sign (see Fig. [7](#S4.F7 "Figure
    7 ‣ 4.1 Cell-phone camera attack ‣ 4 Attacks in the real world ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")), (b) sticker perturbation:
    where the printing is done on a paper and the paper is stuck over the real sign.
    For (b) two types of perturbations were studied, (b1) subtle perturbations: that
    occupied the entire sign and (b2) camouflage perturbations: that took the form
    of graffiti sticker on the sign. As such, all these perturbations require access
    to a color printer and no other special hardware. Successful generation of perturbations
    for both (a) and (b) such that the perturbations remained robust to natural variations
    in the physical world demonstrate the threat of adversarial example in the real
    world. We refer to the following URL for further details and videos related to
    this work: [https://iotsecurity.eecs.umich.edu/#roadsigns](https://iotsecurity.eecs.umich.edu/#roadsigns).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: Etimov 等人 [[75](#bib.bib75)] 基于 [[36](#bib.bib36)] 和 [[88](#bib.bib88)] 提出的攻击，设计了针对物理世界的鲁棒扰动。他们展示了在物理条件下，如视角、距离和分辨率变化下，攻击的鲁棒性。所提出的算法称为
    RP[2]（鲁棒物理扰动），用于生成针对道路标志识别系统的对抗样本，并在实际行驶环境中取得了较高的欺骗率。在这项工作中，介绍了两类针对物理道路标志的攻击：（a）海报打印：攻击者打印一个扰动的道路标志海报并将其贴在真实标志上（见图
    [7](#S4.F7 "图 7 ‣ 4.1 手机相机攻击 ‣ 4 真实世界中的攻击 ‣ 深度学习计算机视觉中的对抗攻击威胁：综述")），（b）贴纸扰动：将打印在纸上的扰动纸贴在真实标志上。对于（b），研究了两种类型的扰动，（b1）细微扰动：占据整个标志，（b2）伪装扰动：呈现为标志上的涂鸦贴纸。因此，所有这些扰动都需要一个彩色打印机，而不需要其他特殊硬件。成功生成针对（a）和（b）的扰动，并且这些扰动对物理世界中的自然变化保持鲁棒，展示了对抗样本在现实世界中的威胁。有关更多细节和相关视频，请参见以下网址：[https://iotsecurity.eecs.umich.edu/#roadsigns](https://iotsecurity.eecs.umich.edu/#roadsigns)。
- en: 'It should be noted that Lu et al. [[30](#bib.bib30)] had previously claimed
    that adversarial examples are not a concern for object detection in Autonomous
    Vehicles because of the changing physical conditions in a moving car. However,
    the attacking methods they employed [[22](#bib.bib22)], [[23](#bib.bib23)], [[35](#bib.bib35)]
    were somewhat primitive. The findings of Etimov et al. [[75](#bib.bib75)] are
    orthogonal to the results in [[66](#bib.bib66)]. However, in a follow-up work
    Lu et al. [[19](#bib.bib19)] showed that the detectors like YOLO 9000 [[149](#bib.bib149)]
    and FasterRCNN [[150](#bib.bib150)] are ‘currently’ not fooled by the attacks
    introduced by Etimov et al. [[75](#bib.bib75)]. Zeng et al. [[87](#bib.bib87)]
    also argue that adversarial perturbations in the image space do not generalize
    well in the physical space of the real-world. However, Athalye et al. [[65](#bib.bib65)]
    showed that we can actually print 3D physical objects for successful adversarial
    attacks in the physical world. We discuss [[65](#bib.bib65)] in Section [4.3](#S4.SS3
    "4.3 Generic adversarial 3D objects ‣ 4 Attacks in the real world ‣ Threat of
    Adversarial Attacks on Deep Learning in Computer Vision: A Survey").'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，Lu 等人 [[30](#bib.bib30)] 之前声称，对抗样本在自主驾驶车辆中的目标检测中不是问题，因为车内物理条件变化。然而，他们使用的攻击方法
    [[22](#bib.bib22)], [[23](#bib.bib23)], [[35](#bib.bib35)] 有些原始。Etimov 等人 [[75](#bib.bib75)]
    的发现与 [[66](#bib.bib66)] 的结果是正交的。然而，在后续工作中，Lu 等人 [[19](#bib.bib19)] 显示像 YOLO 9000
    [[149](#bib.bib149)] 和 FasterRCNN [[150](#bib.bib150)] 这样的检测器“目前”没有被 Etimov 等人
    [[75](#bib.bib75)] 引入的攻击所欺骗。Zeng 等人 [[87](#bib.bib87)] 也认为图像空间中的对抗扰动在现实世界的物理空间中不能很好地泛化。然而，Athalye
    等人 [[65](#bib.bib65)] 显示我们实际上可以打印 3D 物理对象来成功实施现实世界中的对抗攻击。我们在第 [4.3](#S4.SS3 "4.3
    通用对抗 3D 对象 ‣ 4 真实世界中的攻击 ‣ 深度学习计算机视觉中的对抗攻击威胁：综述") 节中讨论了 [[65](#bib.bib65)]。
- en: Gu et al. [[33](#bib.bib33)] also explored an interesting notion of threats
    to outsourced training of the neural networks in the context of fooling neural
    networks on street signs. They showed that it is possible to train a network (a
    *BadNet*) that shows state-of-the-art performance on the user’s training and validation
    samples, but behaves badly on attacker-chosen inputs. They demonstrated this attack
    in a realistic scenario by creating a street sign classifier that identifies stop
    signs as speed limits when a special sticker is added to the stop sign. Moreover,
    it was found that the fooling of the network persisted to a reasonable extent
    even when the network was later fine-tuned with additional training data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Gu 等人 [[33](#bib.bib33)] 还探索了一个有趣的概念，即在欺骗街道标志上的神经网络的背景下，外包训练的威胁。他们展示了可以训练一个网络（*BadNet*），该网络在用户的训练和验证样本上表现出最先进的性能，但在攻击者选择的输入上表现不佳。他们通过创建一个街道标志分类器来演示这种攻击，当在停止标志上添加一个特殊的贴纸时，它会将停止标志识别为限速标志。此外，发现即使在网络后续通过额外的训练数据进行微调时，对抗性欺骗仍能在合理的程度上持续存在。
- en: 4.3 Generic adversarial 3D objects
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 通用对抗性 3D 物体
- en: 'Athalye et al. [[65](#bib.bib65)] introduced a method for constructing 3D objects
    that can fool neural networks across a wide variety of angles and viewpoints.
    Their ‘Expectation Over Transformation’ (EOT) framework is able to construct examples
    that are adversarial over an entire distribution of image/object transformations.
    Their end-to-end approach is able to print arbitrary adversarial 3D objects. In
    our opinion, results of this work ascertain that adversarial attacks are a real
    concern for deep learning in the physical world. In Fig. [8](#S4.F8 "Figure 8
    ‣ 4.3 Generic adversarial 3D objects ‣ 4 Attacks in the real world ‣ Threat of
    Adversarial Attacks on Deep Learning in Computer Vision: A Survey") we show an
    example of 3D-printed turtle that is modified by EOT framework to be classified
    as rifle. A video demonstrating the fooling by EOT in the physical world is available
    at the following URL: [https://www.youtube.com/watch?v=YXy6oX1iNoA&feature=youtu.be](https://www.youtube.com/watch?v=YXy6oX1iNoA&feature=youtu.be).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 'Athalye 等人 [[65](#bib.bib65)] 提出了一种构建 3D 物体的方法，该方法可以在各种角度和视角下欺骗神经网络。他们的“期望变换”
    (EOT) 框架能够构建对整个图像/物体变换分布具有对抗性的示例。他们的端到端方法可以打印任意对抗性 3D 物体。在我们看来，这项工作的结果确认了对深度学习在物理世界中对抗性攻击的真正关注。在图
    Fig. [8](#S4.F8 "Figure 8 ‣ 4.3 Generic adversarial 3D objects ‣ 4 Attacks in
    the real world ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey") 中，我们展示了一个由 EOT 框架修改的 3D 打印乌龟示例，它被分类为步枪。演示 EOT 在物理世界中欺骗的影片可在以下网址查看:
    [https://www.youtube.com/watch?v=YXy6oX1iNoA&feature=youtu.be](https://www.youtube.com/watch?v=YXy6oX1iNoA&feature=youtu.be)。'
- en: '![Refer to caption](img/f3f402a58f81966529e4d1e80c88f431.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f3f402a58f81966529e4d1e80c88f431.png)'
- en: 'Figure 8: Different random poses of a 3D-printed turtle perturbed by EOT [[65](#bib.bib65)]
    to be classified as a rifle by an ImageNet classifier. The unperturbed version
    (not shown) is classified correctly with $100\%$ probability.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：不同随机姿态的 3D 打印乌龟被 EOT [[65](#bib.bib65)] 扰动后被 ImageNet 分类器分类为步枪。未扰动的版本（未显示）以
    $100\%$ 的概率被正确分类。
- en: 4.4 Cyberspace attacks
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 网络空间攻击
- en: Papernot et al. [[39](#bib.bib39)] launched one of the first attacks against
    the deep neural network classifiers in cyberspace in the real-world settings.
    They trained a substitute network for the targeted black-box classifier on synthetic
    data, and instantiated the attack against remotely hosted neural networks by MetaMind,
    Amazon and Google. They were able to show that the respective targeted networks
    misclassified $84.24\%$, $96.19\%$ and $88.94\%$ of the adversarial examples generated
    by their method. Indeed, the only information available to the attacker in their
    threat model was the output label of the targeted network for the input image
    fed by the attacker. In a related work, Liu et al. [[88](#bib.bib88)] developed
    an ensemble based attack and showed its success against Clarifai.com - a commercial
    company providing state-of-the-art image classification services. The authors
    claim that their attacks for both targeted and non-targeted fooling are able to
    achieve high success rates.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Grosse et al. [[61](#bib.bib61)] showed construction of effective adversarial
    attacks for neural networks used as malware classifiers. As compared to image
    recognition, the domain of malware classification introduces additional constraints
    in the adversarial settings, e.g. continuous input domains are replaced by discrete
    inputs, the condition of visual similarity is replaced by requiring equivalent
    functional behavior. However, Grosse et al. [[61](#bib.bib61)] showed that creating
    effective adversarial examples is still possible for maleware classification.
    Further examples of successful adversarial attacks against deep lrearning based
    malware classification can also be found in [[64](#bib.bib64)], [[107](#bib.bib107)],
    [[125](#bib.bib125)].
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Robotic Vision & Visual QA Attacks
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Melis et al. [[63](#bib.bib63)] demonstrated the vulnerability of robots to
    the adversarial manipulations of the input images using the techniques in [[22](#bib.bib22)].
    The authors argue that strategies to enforce deep neural networks to learn more
    stable representations are necessary for secure robotics. Xu et al. [[40](#bib.bib40)]
    generated adversarial attacks for the Visual Turing Test, also known as ‘Visual
    Question Answer’ (VQA). The authors show that the commonly used compositional
    and non-compositional VQA architectures that employ deep neural networks are vulnerable
    to adversarial attacks. Moreover, the adversarial examples are transferable between
    the models. They conclude that the “adversarial examples pose real threats to
    not only image classification models, but also more complicated VQA models” [[63](#bib.bib63)].
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: 5 Existence of adversarial examples
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the literature related to adversarial attacks on deep learning in Computer
    Vision, there are varied views on the existence of adversarial examples. These
    views generally align well with the local empirical observations made by the researchers
    while attacking or defending the deep neural networks. However, they often fall
    short in terms of generalization. For instance, the popular linearity hypothesis
    of Goodfellow et al. [[23](#bib.bib23)] explains the FGSM and related attacks
    very well. However, Tanay and Griffin [[74](#bib.bib74)] demonstrated image classes
    that do not suffer from adversarial examples for linear classifier, which is not
    in-line with the linearity hypothesis. Not to mention, the linearity hypothesis
    itself deviates strongly from the previously prevailing opinion that the adversarial
    examples stem from highly non-linear decision boundaries induced by deep neural
    networks. There are also other examples in the literature where the linearity
    hypothesis is not directly supported [[119](#bib.bib119)].
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在与计算机视觉中的深度学习对抗攻击相关的文献中，对对抗样本存在的看法各不相同。这些观点通常与研究人员在攻击或防御深度神经网络时所做的局部经验观察较为一致。然而，它们在泛化方面往往存在不足。例如，Goodfellow
    等人 [[23](#bib.bib23)] 的流行线性假设很好地解释了 FGSM 和相关攻击。然而，Tanay 和 Griffin [[74](#bib.bib74)]
    展示了线性分类器下不受对抗样本影响的图像类别，这与线性假设不符。更不用说，线性假设本身也与之前的普遍观点，即对抗样本源自深度神经网络引起的高度非线性决策边界，存在很大偏差。文献中还有其他例子表明线性假设并没有直接得到支持
    [[119](#bib.bib119)]。
- en: 'Flatness of decision boundaries [[69](#bib.bib69)], large local curvature of
    the decision boundaries [[70](#bib.bib70)] and low flexibility of the networks [[71](#bib.bib71)]
    are some more examples of the viewpoints on the existence of adversarial examples
    that do not perfectly align with each other. Whereas it is apparent that adversarial
    examples can be formed by modifying as little as one pixel in an image, current
    literature seems to lack consensus on the reasons for the existence of the adversarial
    examples. This fact also makes analysis of adversarial examples an active research
    direction that is expected to explore and explain the nature of the decision boundaries
    induced by deep neural networks, which are currently more commonly treated as
    black-box models. Below, we review the works that mainly focus on analyzing the
    existence of adversarial perturbations for deep learning. We note that, besides
    the literature reviewed below, works related to adversarial attacks (Section [3](#S3
    "3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")) and defenses (Section [6](#S6 "6 Defenses against adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")) often provide brief analysis of adversarial perturbations while conjecturing
    about the phenomena resulting in the existence of the adversarial examples.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '决策边界的平坦性 [[69](#bib.bib69)]、决策边界的大局部曲率 [[70](#bib.bib70)] 和网络的低灵活性 [[71](#bib.bib71)]
    是一些关于对抗样本存在的观点，这些观点彼此之间并不完全一致。尽管显而易见，对抗样本可以通过修改图像中的一个像素来形成，但当前的文献似乎对对抗样本存在的原因缺乏共识。这一事实也使得对抗样本的分析成为一个积极的研究方向，预计将探索并解释由深度神经网络引起的决策边界的本质，这些网络目前更常被视作黑箱模型。以下，我们回顾了主要集中于分析深度学习中的对抗扰动存在性的研究工作。我们注意到，除了下面回顾的文献外，与对抗攻击（第[3](#S3
    "3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")节）和防御（第[6](#S6 "6 Defenses against adversarial attacks ‣ Threat
    of Adversarial Attacks on Deep Learning in Computer Vision: A Survey")节）相关的工作通常会简要分析对抗扰动，同时对导致对抗样本存在的现象进行猜测。'
- en: 5.1 Limits on adversarial robustness
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 对对抗鲁棒性的限制
- en: Fawzi et al. [[118](#bib.bib118)] introduced a framework for studying the instability
    of classifiers to adversarial perturbations. They established fundamental limits
    on the robustness of classifiers in terms of a ‘distinguishability measure’ between
    the classes of the dataset, where distinguishability is defined as the distance
    between the means of two classes for linear classifiers and the distance between
    the matrices of second order moments for the studied non-linear classifiers. This
    work shows that adversarial examples also exist for the classifiers beyond deep
    neural networks. The presented analysis traces back the phenomenon of adversarial
    instability to the low flexibility of the classifiers, which is not completely
    orthogonal to the prevailing belief at that time that high-nonlinearity of the
    networks make them susceptible to adversarial examples.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: Fawzi 等人 [[118](#bib.bib118)] 引入了一个框架来研究分类器对对抗扰动的不稳定性。他们建立了分类器鲁棒性的基本限制，这些限制是通过数据集类别之间的“可区分性度量”来定义的，其中可区分性被定义为线性分类器中两个类别均值之间的距离以及所研究的非线性分类器中二阶矩阵之间的距离。这项工作表明，对抗样本也存在于深度神经网络之外的分类器中。所展示的分析将对抗不稳定性的现象追溯到分类器的低灵活性，这与当时普遍认为网络的高非线性使其容易受到对抗样本影响的观点并不完全正交。
- en: 5.2 Space of adversarial examples
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 对抗样本空间
- en: Tabacof and Eduardo [[25](#bib.bib25)] generated adversarial examples for shallow
    and deep network classifiers on MNIST [[10](#bib.bib10)] and ImageNet [[11](#bib.bib11)]
    datasets and probed the pixel space of adversarial examples by using noise of
    varying distribution and intensity. The authors empirically demonstrated that
    adversarial examples appear in large regions in the pixel space, which is in-line
    with the similar claim in [[23](#bib.bib23)]. However, somewhat in contrast to
    the linearity hypothesis, they argue that a weak, shallow and more linear classifier
    is also as susceptible to adversarial examples as a strong deep classifier.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Tabacof 和 Eduardo [[25](#bib.bib25)] 在 MNIST [[10](#bib.bib10)] 和 ImageNet [[11](#bib.bib11)]
    数据集上为浅层和深层网络分类器生成了对抗样本，并通过使用不同分布和强度的噪声探测了对抗样本的像素空间。作者通过实验证明，对抗样本在像素空间中出现于大范围区域，这与
    [[23](#bib.bib23)] 中的类似主张一致。然而，与线性假设有些相反的是，他们认为弱、浅层且更线性的分类器也和强大的深层分类器一样容易受到对抗样本的影响。
- en: Tramer et al. [[132](#bib.bib132)] proposed a method to estimate the dimensionality
    of the space of the adversarial examples. It is claimed that the adversarial examples
    span a contiguous high dimension space (e.g. with dimensionality $\approx 25$).
    Due to high dimensionality, the subspaces of different classifiers can intersect,
    which gives rise to the transferability of the adversarial examples. Interestingly,
    their analysis suggests that it is possible to defend classifiers against transfer-based
    attacks even when they are vulnerable to direct attacks.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Tramer 等人 [[132](#bib.bib132)] 提出了一个方法来估计对抗样本空间的维度。该方法声称对抗样本在一个连续的高维空间中分布（例如，维度约为
    $\approx 25$）。由于高维度，不同分类器的子空间可能会相交，这导致了对抗样本的可转移性。有趣的是，他们的分析表明，即使分类器对直接攻击脆弱，也可能防御基于转移的攻击。
- en: 5.3 Boundary tilting perspective
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 边界倾斜视角
- en: Tanay and Griffin [[74](#bib.bib74)] provided a ‘boundary tilting’ perspective
    on the existence of adversarial examples for deep neural networks. They argued
    that generally a single class data that is sampled to learn and evaluate a classifier
    lives in a sub-manifold of the class, and adversarial examples for that class
    exist when the classification boundary lies close to this sub-manifold. They formalized
    the notion of ‘adversarial strength’ of a classifier and reduced it to the ‘deviation
    angle’ between the boundaries of the considered classifier and the nearest centroid
    classifier. It is then shown that adversarial strength of a classifier can be
    varied by decision ‘boundary tilting’. The authors also argued that adversarial
    stability of the classifier is associated with its regularization. In the opinion
    of Tanay and Griffin, the linearity hypothesis [[23](#bib.bib23)] about the existence
    of adversarial examples is “unconvincing”.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Tanay 和 Griffin [[74](#bib.bib74)] 提出了一个关于深度神经网络中对抗样本存在的‘边界倾斜’视角。他们认为，通常情况下，为了学习和评估分类器而采样的单类数据存在于该类别的一个子流形中，当分类边界接近该子流形时，对该类别的对抗样本就会存在。他们将分类器的‘对抗强度’的概念形式化，并将其简化为所考虑的分类器与最近质心分类器之间的‘偏差角度’。随后显示，分类器的对抗强度可以通过决策‘边界倾斜’来变化。作者还认为，分类器的对抗稳定性与其正则化有关。在
    Tanay 和 Griffin 看来，关于对抗样本存在的线性假设[[23](#bib.bib23)]是“令人不信服的”。
- en: 5.4 Prediction uncertainty and evolutionary stalling of training cause adversaries
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 预测不确定性和训练的进化停滞导致对抗攻击
- en: 'Cubuk et al. [[91](#bib.bib91)] argue that the “origin of adversarial examples
    is primarily due to an inherent uncertainty that neural networks have about their
    predictions”. They empirically compute a functional form of the uncertainty, which
    is shown to be independent of network architecture, training protocol and dataset.
    It is argued that this form only depends on the statistics of the network logit
    differences. This eventually results in fooling ratios caused by adversarial attacks
    to exhibit a universal scaling with respect to the size of perturbation. They
    studied FGSM [[23](#bib.bib23)], ILCM and BIM [[35](#bib.bib35)] based attacks
    to corroborate their claims. It is also claimed that accuracy of a network on
    clean images correlates with its adversarial robustness (see Section [5.5](#S5.SS5
    "5.5 Accuracy-adversarial robustness correlation ‣ 5 Existence of adversarial
    examples ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision:
    A Survey") for more arguments in this direction).'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 'Cubuk 等人[[91](#bib.bib91)] 认为，“对抗样本的起源主要是由于神经网络对其预测存在固有的不确定性”。他们经验性地计算了一种不确定性的函数形式，并表明这种形式独立于网络架构、训练协议和数据集。有人认为这种形式仅依赖于网络
    logit 差异的统计数据。这最终导致对抗攻击引起的欺骗率表现出相对于扰动大小的普遍缩放。他们研究了 FGSM [[23](#bib.bib23)]、ILCM
    和 BIM [[35](#bib.bib35)] 基于的攻击以证实他们的主张。还声称网络在干净图像上的准确性与其对抗鲁棒性相关（有关此方向更多论证，请参见第
    [5.5 节](#S5.SS5 "5.5 Accuracy-adversarial robustness correlation ‣ 5 Existence
    of adversarial examples ‣ Threat of Adversarial Attacks on Deep Learning in Computer
    Vision: A Survey")）。'
- en: Rozsa et al. [[102](#bib.bib102)] hypothesized that the existence of adversarial
    perturbations is a result of evolutionary stalling of decision boundaries on training
    images. In their opinion, individual training samples stop contributing to the
    training loss of the model (i.e. neural network) once they are classified correctly,
    which can eventually leave them close to the decision boundary. Hence, it becomes
    possible to throw those (and similar) samples away to a wrong class region by
    adding minor perturbations. They proposed a Batch Adjusted Network Gradients (BANG)
    algorithm to train a network to mitigate the evolutionary stalling during training.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Rozsa 等人[[102](#bib.bib102)] 假设，对抗扰动的存在是由于训练图像上的决策边界进化停滞。他们认为，一旦单个训练样本被正确分类，这些样本便停止对模型（即神经网络）训练损失的贡献，这最终可能使它们接近决策边界。因此，通过添加小的扰动，有可能将这些（以及类似）样本抛到错误的类别区域。他们提出了一种批量调整网络梯度（BANG）算法，以训练网络来缓解训练过程中的进化停滞。
- en: 5.5 Accuracy-adversarial robustness correlation
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5 准确性与对抗鲁棒性相关性
- en: In the quest of explaining the existence of adversarial perturbations, Rozsa
    et al. [[97](#bib.bib97)] empirically analyzed the correlation between the accuracy
    of eight deep network classifiers and their robustness to three adversarial attacks
    introduced in [[23](#bib.bib23)],[[94](#bib.bib94)]. The studied classifiers include
    AlexNet [[9](#bib.bib9)], VGG-16 and VGG-19 networks [[163](#bib.bib163)], Berkeley-trained
    version of GoogLeNet and Princeton-GoogLeNet [[18](#bib.bib18)], ResNet-52; ResNet-101;
    and ResNet-152 [[147](#bib.bib147)]. The adversarial examples are generated with
    the help of large-scale ImageNet dataset [[11](#bib.bib11)] using the techniques
    proposed in [[23](#bib.bib23)] and [[94](#bib.bib94)]. Their experiments lead
    to the observation that the networks with higher classification accuracy generally
    also exhibit more robustness against the adversarial examples. They also concluded
    that adversarial examples transfer better between similar network topologies.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释对抗扰动的存在，Rozsa 等人 [[97](#bib.bib97)] 通过实证分析了八种深度网络分类器的准确性与它们对三种在 [[23](#bib.bib23)]
    和 [[94](#bib.bib94)] 中提出的对抗攻击的鲁棒性之间的相关性。研究的分类器包括 AlexNet [[9](#bib.bib9)]、VGG-16
    和 VGG-19 网络 [[163](#bib.bib163)]、伯克利训练版的 GoogLeNet 和普林斯顿 GoogeLeNet [[18](#bib.bib18)]、ResNet-52；ResNet-101；以及
    ResNet-152 [[147](#bib.bib147)]。对抗样本是利用大规模 ImageNet 数据集 [[11](#bib.bib11)] 和 [[23](#bib.bib23)]、[[94](#bib.bib94)]
    中提出的技术生成的。他们的实验结果表明，具有更高分类准确性的网络通常在对抗样本面前表现出更好的鲁棒性。他们还得出结论，对抗样本在类似网络拓扑之间的迁移效果更好。
- en: 5.6 More on linearity as the source
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6 更多关于线性作为来源
- en: Kortov and Hopfiled [[127](#bib.bib127)] examined the existence of adversarial
    perturbations in the context of Dense Associative Memory (DAM) models [[164](#bib.bib164)].
    As compared to the typical modern deep neural networks, DAM models employ higher
    order (more than quadratic) interactions between the neurons. The authors have
    demonstrated that adversarial examples generated using DAM models with smaller
    interaction power, which is similar to using a deep neural network with ReLU activation
    [[165](#bib.bib165)] for inducing linearity, are unable to fool models having
    higher order interactions. The authors provided empirical evidence on the existence
    of adversarial examples that is independent of the FGSM [[23](#bib.bib23)] attack,
    yet supports the linearity hypothesis of Goodfellow et al. [[23](#bib.bib23)].
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Kortov 和 Hopfield [[127](#bib.bib127)] 在 Dense Associative Memory (DAM) 模型 [[164](#bib.bib164)]
    的背景下检查了对抗扰动的存在。与典型的现代深度神经网络相比，DAM 模型采用了神经元之间的高阶（多于二次）交互。作者已经证明，使用 DAM 模型生成的对抗样本，其交互作用力较小，这类似于使用
    ReLU 激活 [[165](#bib.bib165)] 的深度神经网络来诱导线性，无法欺骗具有高阶交互的模型。作者提供了对抗样本存在的实证证据，这与 FGSM
    [[23](#bib.bib23)] 攻击无关，但支持了 Goodfellow 等人 [[23](#bib.bib23)] 的线性假设。
- en: 5.7 Existence of universal perturbations
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.7 通用扰动的存在
- en: Moosavi-Dezfooli et al. [[16](#bib.bib16)] initially argued that universal adversarial
    perturbations exploit geometric correlations between the decision boundaries induced
    by the classifiers. Their existence partly owes to a subspace containing normals
    to the decision boundaries, such that the normals also surround the natural images.
    In [[70](#bib.bib70)], they built further on their theory and showed the existence
    of common directions (shared across datapoints) along which the decision boundary
    of a classifier can be highly positively curved. They argue that such directions
    play a key role in the existence of universal perturbations. Based on their findings,
    the authors also propose a new geometric method to efficiently compute universal
    adversarial perturbations.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Moosavi-Dezfooli 等人 [[16](#bib.bib16)] 最初提出，通用对抗扰动利用了分类器引起的决策边界之间的几何相关性。它们的存在部分归因于包含决策边界法线的子空间，使得这些法线也包围自然图像。在
    [[70](#bib.bib70)] 中，他们进一步发展了他们的理论，展示了存在共同方向（在数据点间共享），沿这些方向，分类器的决策边界可以高度正曲。作者认为这些方向在通用扰动的存在中起着关键作用。根据他们的发现，作者还提出了一种新的几何方法来高效计算通用对抗扰动。
- en: It is worth noting that previously Fawzi et al. [[69](#bib.bib69)] also associated
    the theoretical bounds on the robustness of classifiers to the curvature of decision
    boundaries. Similarly, Tramer et al. [[77](#bib.bib77)] also held the curvature
    of decision boundaries in the vicinity of data points responsible for the vulnerability
    of neural networks to black-box attacks. In another recent work, Mopuri et al. [[193](#bib.bib193)]
    present a GAN-like model to learn the distribution of the universal adversarial
    perturbations for a given target model. The learned distributions are also observed
    to show good transferability across models.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，之前 Fawzi 等人[[69](#bib.bib69)] 也将分类器的鲁棒性理论界限与决策边界的曲率相关联。类似地，Tramer 等人[[77](#bib.bib77)]
    也认为数据点附近的决策边界曲率导致了神经网络对黑箱攻击的脆弱性。在另一项近期研究中，Mopuri 等人[[193](#bib.bib193)] 提出了一个类似于
    GAN 的模型，以学习给定目标模型的通用对抗扰动的分布。所学分布还显示出在模型之间的良好迁移性。
- en: 6 Defenses against adversarial attacks
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗攻击的防御
- en: 'Currently, the defenses against the adversarial attacks are being developed
    along three main directions:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，对抗攻击的防御正沿着三个主要方向进行开发：
- en: '1.'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '1.'
- en: Using modified training during learning or modified input during testing.
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在学习过程中使用修改后的训练或在测试过程中使用修改后的输入。
- en: '2.'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '2.'
- en: Modifying networks, e.g. by adding more layers/sub-networks, changing loss/activation
    functions etc.
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 修改网络，例如通过添加更多的层/子网络、更改损失/激活函数等。
- en: '3.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '3.'
- en: Using external models as network add-on when classifying unseen examples.
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在对未见示例进行分类时，使用外部模型作为网络附加组件。
- en: 'The approaches along the first direction do not directly deal with the learning
    models. On the other hand, the other two categories are more concerned with the
    neural networks themselves. The techniques under these categories can be further
    divided into two types; namely (a) complete defense and (b) detection only. The
    ‘complete defense’ approaches aim at enabling the targeted network to achieve
    its original goal on the adversarial examples, e.g. a classifier predicting labels
    of adversarial examples with acceptable accuracy. On the other hand, ‘detection
    only’ approaches are meant to raise the red flag on potentially adversarial examples
    to reject them in any further processing. The taxonomy of the described categories
    is also shown in Fig. [9](#S6.F9 "Figure 9 ‣ 6 Defenses against adversarial attacks
    ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A Survey").
    The remaining section is organized according to this taxonomy. In the used taxonomy,
    the difference between ‘modifying’ a network and employing an ‘add-on’ is that
    the former makes changes to the original deep neural network architecture/parameters
    during training. On the other hand, the latter keeps the original model intact
    and appends external model(s) to it during testing.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '第一类方法不直接处理学习模型。另一方面，另外两类方法更关注神经网络本身。这些类别下的技术可以进一步分为两种类型；即（a）完全防御和（b）仅检测。‘完全防御’方法旨在使目标网络在对抗示例上实现其原始目标，例如分类器以可接受的准确度预测对抗示例的标签。另一方面，‘仅检测’方法旨在对潜在的对抗示例发出警报，以在进一步处理时拒绝它们。所描述类别的分类法也显示在图[9](#S6.F9
    "Figure 9 ‣ 6 Defenses against adversarial attacks ‣ Threat of Adversarial Attacks
    on Deep Learning in Computer Vision: A Survey")中。剩余部分根据这一分类法进行组织。在使用的分类法中，‘修改’网络和使用‘附加组件’之间的区别在于前者在训练过程中对原始深度神经网络架构/参数进行更改。另一方面，后者在测试过程中保持原始模型不变，并将外部模型附加到它上面。'
- en: '![Refer to caption](img/13bd7a1a97672006f64ebb23fb11dd17.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/13bd7a1a97672006f64ebb23fb11dd17.png)'
- en: 'Figure 9: Broad categorization of approaches aimed at defending deep neural
    networks against adversarial attacks.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9：旨在防御对抗攻击的深度神经网络方法的广泛分类。
- en: 6.1 Modified training/input
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 修改训练/输入
- en: 6.1.1 Brute-force adversarial training
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1 强力对抗训练
- en: 'Since the discovery of adversarial examples for the deep neural networks [[22](#bib.bib22)],
    there has been a general consensus in the related literature that robustness of
    neural networks against these examples improves with adversarial training. Therefore,
    most of the contributions introducing new adversarial attacks, e.g. [[22](#bib.bib22)], [[23](#bib.bib23)],
    [[72](#bib.bib72)] (see Section [3](#S3 "3 Adversarial attacks ‣ Threat of Adversarial
    Attacks on Deep Learning in Computer Vision: A Survey")) simultaneously propose
    adversarial training as the first line of defense against those attacks. Although
    adversarial training improves robustness of a network, to be really effective,
    it requires that training is performed using strong attacks and the architecture
    of the network is sufficiently expressive. Since adversarial training necessitates
    increased training/data size, we refer to it as a ‘brute-force’ strategy.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '自从深度神经网络的对抗样本被发现 [[22](#bib.bib22)] 以来，相关文献普遍认为，通过对抗训练可以提高神经网络对这些样本的鲁棒性。因此，大多数引入新的对抗攻击的研究成果，例如
    [[22](#bib.bib22)], [[23](#bib.bib23)], [[72](#bib.bib72)]（见第 [3](#S3 "3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey") 节），同时提出对抗训练作为防御这些攻击的第一道防线。尽管对抗训练可以提高网络的鲁棒性，但要真正有效，需要使用强攻击进行训练，并且网络架构要足够表达能力。由于对抗训练需要增加训练/数据规模，我们将其称为‘暴力’策略。'
- en: It is also commonly observed in the literature that brute-force adversarial
    training results in regularizing the network (e.g. see [[23](#bib.bib23)], [[90](#bib.bib90)])
    to reduce over-fitting, which in turn improves robustness of the networks against
    the adversarial attacks. Inspired by this observation, Miyato et al. [[113](#bib.bib113)]
    proposed a ‘Virtual Adversarial Training’ approach to smooth the output distributions
    of the neural networks. A related ‘stability training’ method is also proposed
    by Zheng et al. [[116](#bib.bib116)] to improve the robustness of neural networks
    against small distortions to input images. It is noteworthy that whereas adversarial
    training is known to improve robustness of neural networks, Moosavi-Dezfooli [[16](#bib.bib16)]
    showed that effective adversarial examples can again be computed for already adversarially
    trained networks.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中还普遍观察到，暴力对抗训练导致网络正则化（例如见 [[23](#bib.bib23)], [[90](#bib.bib90)]），从而减少过拟合，进而提高网络对抗攻击的鲁棒性。受此观察启发，宫户等人
    [[113](#bib.bib113)] 提出了一种‘虚拟对抗训练’方法，以平滑神经网络的输出分布。郑等人还提出了相关的‘稳定性训练’方法 [[116](#bib.bib116)]，以提高神经网络对输入图像小扭曲的鲁棒性。值得注意的是，尽管对抗训练已知可以提高神经网络的鲁棒性，但穆萨维-德佐福利
    [[16](#bib.bib16)] 表明，对已经进行对抗训练的网络仍然可以计算出有效的对抗样本。
- en: 6.1.2 Data compression as defense
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2 数据压缩作为防御措施
- en: Dziugaite et al. [[123](#bib.bib123)] noted that most of the popular image classification
    datasets comprise JPG images. Motivated by this observation, they studied the
    effects of JPG compression on the perturbations computed by FGSM [[23](#bib.bib23)].
    It is reported that JPG compression can actually reverse the drop in classification
    accuracy to a large extent for the FGSM perturbations. Nevertheless, it is concluded
    that compression alone is far from an effective defense. JPEG compression was
    also studied by Guo et al. [[82](#bib.bib82)] for mitigating the effectiveness
    of adversarial images. Das et al. [[37](#bib.bib37)] also took a similar approach
    and used JPEG compression to remove the high frequency components from images
    and proposed an ensemble-based technique to counter the adversarial attacks generated
    by FGSM [[23](#bib.bib23)] and DeepFool method [[72](#bib.bib72)]. Whereas encouraging
    results are reported in [[37](#bib.bib37)], there is no analysis provided for
    the stronger attacks, e.g. C&W attacks [[36](#bib.bib36)]. Moreover, Shin and
    Song [[186](#bib.bib186)] have demonstrated the existence of adversarial examples
    that can survive JPEG compression. Compression under Discrete Cosine Transform
    (DCT) was also found inadequate as a defense against the universal perturbations [[16](#bib.bib16)]
    in our previous work [[81](#bib.bib81)]. One major limitation of compression based
    defense is that larger compressions also result in loss of classification accuracy
    on clean images, whereas smaller compressions often do not adequately remove the
    adversarial perturbations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Dziugaite 等人 [[123](#bib.bib123)] 指出，大多数流行的图像分类数据集由 JPG 图像组成。基于这一观察，他们研究了 JPG
    压缩对 FGSM [[23](#bib.bib23)] 计算的扰动的影响。据报道，JPG 压缩实际上可以在很大程度上逆转 FGSM 扰动所导致的分类精度下降。然而，得出的结论是，仅凭压缩远不能作为有效的防御手段。Guo
    等人 [[82](#bib.bib82)] 也研究了 JPEG 压缩对减轻对抗图像效果的作用。Das 等人 [[37](#bib.bib37)] 采用了类似的方法，使用
    JPEG 压缩去除图像中的高频分量，并提出了一种基于集成的方法来对抗由 FGSM [[23](#bib.bib23)] 和 DeepFool 方法 [[72](#bib.bib72)]
    生成的对抗攻击。虽然 [[37](#bib.bib37)] 中报告了令人鼓舞的结果，但对于更强攻击（例如 C&W 攻击 [[36](#bib.bib36)]）没有提供分析。此外，Shin
    和 Song [[186](#bib.bib186)] 证明了存在能够在 JPEG 压缩下生存的对抗样本。我们之前的研究 [[81](#bib.bib81)]
    也发现，离散余弦变换 (DCT) 下的压缩对于对抗性扰动 [[16](#bib.bib16)] 作为防御手段也是不够的。基于压缩的防御的一个主要限制是，较大的压缩会导致干净图像的分类精度下降，而较小的压缩通常不能有效去除对抗性扰动。
- en: In another related approach, Bhagoji et al. [[169](#bib.bib169)] proposed to
    compress input data using Principal Component Analysis for adversarial robustness.
    However, Xu et al. [[140](#bib.bib140)] noted that this compression also results
    in corrupting the spatial structure of the image, hence often adversely affecting
    the classification performance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种相关方法中，Bhagoji 等人 [[169](#bib.bib169)] 提出了使用主成分分析对输入数据进行压缩以增强对抗鲁棒性。然而，Xu
    等人 [[140](#bib.bib140)] 指出，这种压缩也会破坏图像的空间结构，从而常常对分类性能产生不利影响。
- en: 6.1.3 Foveation based defense
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3 基于注视的防御
- en: Luo et al. [[119](#bib.bib119)] demonstrated that significant robustness against
    the adversarial attacks using L-BFGS [[22](#bib.bib22)] and FGSM [[23](#bib.bib23)]
    is possible with ‘foveation’ mechanism - applying neural network in different
    regions of images. It is hypothesized that CNN-based classifiers trained on large
    datasets, such as ImageNet [[11](#bib.bib11)] are generally robust to scale and
    translation variations of objects in the images. However, this invariance does
    not extend to adversarial patterns in the images. This makes foveation as a viable
    option for reducing the effectiveness of adversarial attacks proposed in [[22](#bib.bib22)],
    [[23](#bib.bib23)]. However, foveation is yet to demonstrate its effectiveness
    against more powerful attacks.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Luo 等人 [[119](#bib.bib119)] 证明了使用 L-BFGS [[22](#bib.bib22)] 和 FGSM [[23](#bib.bib23)]
    的对抗攻击具有显著的鲁棒性，这是通过“注视”机制实现的——在图像的不同区域应用神经网络。假设基于 CNN 的分类器在大规模数据集（如 ImageNet [[11](#bib.bib11)]）上训练，一般对图像中对象的尺度和位移变化具有鲁棒性。然而，这种不变性并不适用于图像中的对抗模式。这使得注视成为减少对抗攻击有效性的一个可行选择，如
    [[22](#bib.bib22)] 和 [[23](#bib.bib23)] 中所提。然而，注视尚未证明其对更强攻击的有效性。
- en: 6.1.4 Data randomization and other methods
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4 数据随机化和其他方法
- en: Xie et al. [[115](#bib.bib115)] showed that random resizing of the adversarial
    examples reduces their effectiveness. Moreover, adding random padding to such
    examples also results in reducing the fooling rates of the networks. Wang et al. [[138](#bib.bib138)]
    transformed the input data with a separate data-transformation module to remove
    possible adversarial perturbations in images. In the literature, we also find
    evidence that data augmentation during training (e.g. Gaussian data augmentation [[46](#bib.bib46)])
    also helps in improving robustness of neural networks to adversarial attacks,
    albeit only slightly.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Xie 等人 [[115](#bib.bib115)] 证明了随机调整对抗样本的大小可以降低其有效性。此外，对这些样本添加随机填充也会降低网络的欺骗率。Wang
    等人 [[138](#bib.bib138)] 使用一个单独的数据转换模块来转换输入数据，以去除图像中可能存在的对抗扰动。在文献中，我们还发现数据增强（例如高斯数据增强
    [[46](#bib.bib46)]）在训练过程中也有助于提高神经网络对对抗攻击的鲁棒性，尽管效果只是稍微有所改善。
- en: 6.2 Modifying the network
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 修改网络
- en: 'For the approaches that modify the neural networks for defense against the
    adversarial attacks, we first discuss the ‘complete defense’ approaches. The ‘detection
    only’ approaches are separately reviewed in Section [6.2.8](#S6.SS2.SSS8 "6.2.8
    Detection Only approaches ‣ 6.2 Modifying the network ‣ 6 Defenses against adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey").'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '对于那些修改神经网络以防御对抗攻击的方法，我们首先讨论“完全防御”方法。‘仅检测’的方法在第 [6.2.8](#S6.SS2.SSS8 "6.2.8
    Detection Only approaches ‣ 6.2 Modifying the network ‣ 6 Defenses against adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey") 节中单独审查。'
- en: 6.2.1 Deep Contractive Networks
  id: totrans-213
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 深度收缩网络
- en: In the early attempts of making deep learning robust to adversarial attacks,
    Gu and Rigazio [[24](#bib.bib24)] introduced Deep Contractive Networks (DCN).
    It was shown that Denoising Auto Encoders [[154](#bib.bib154)] can reduce adversarial
    noise, however stacking them with the original networks can make the resulting
    network even more vulnerable to perturbations. Based on this observation, the
    training procedure of DCNs used a smoothness penalty similar to Contractive Auto
    Encoders [[173](#bib.bib173)]. Whereas reasonable robustness of DCNs was demonstrated
    against the L-BGFS [[22](#bib.bib22)] based attacks, many stronger attacks have
    been introduced since DCNs were initially proposed. A related concept of using
    auto encoders for adversarial robustness of the neural networks can be also found
    in [[141](#bib.bib141)].
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期尝试使深度学习对抗对抗攻击时，Gu 和 Rigazio [[24](#bib.bib24)] 引入了深度收缩网络（DCN）。研究表明，去噪自编码器
    [[154](#bib.bib154)] 可以减少对抗噪声，但将它们与原始网络堆叠在一起会使得结果网络对扰动更加脆弱。基于这一观察，DCN 的训练过程使用了类似于收缩自编码器
    [[173](#bib.bib173)] 的平滑性惩罚。虽然对 DCN 对抗 L-BGFS [[22](#bib.bib22)] 基于攻击表现出了合理的鲁棒性，但自
    DCN 初次提出以来，已经出现了许多更强的攻击。有关使用自编码器来提高神经网络对对抗鲁棒性的相关概念也可以在 [[141](#bib.bib141)] 中找到。
- en: 6.2.2 Gradient regularization/masking
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 梯度正则化/遮蔽
- en: Ross and Doshi-Velez [[52](#bib.bib52)] studied input gradient regularization [[167](#bib.bib167)]
    as a method for adversarial robustness. Their method trains differentiable models
    (e.g. deep neural networks) while penalizing the degree of variation resulting
    in the output with respect to change in the input. Implying, a small adversarial
    perturbation becomes unlikely to change the output of the trained model drastically.
    It is shown that this method, when combined with brute-force adversarial training,
    can result in very good robustness against attacks like FGSM [[23](#bib.bib23)]
    and JSMA [[60](#bib.bib60)]. However, each of these methods almost double the
    training complexity of a network, which is already prohibitive in many cases.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Ross 和 Doshi-Velez [[52](#bib.bib52)] 研究了输入梯度正则化 [[167](#bib.bib167)] 作为对抗鲁棒性的方法。他们的方法训练可微模型（例如深度神经网络），同时惩罚由于输入变化导致的输出变化程度。这意味着，小的对抗扰动不容易使训练模型的输出发生剧烈变化。研究表明，当这种方法与蛮力对抗训练结合时，可以对像
    FGSM [[23](#bib.bib23)] 和 JSMA [[60](#bib.bib60)] 这样的攻击表现出非常好的鲁棒性。然而，这些方法几乎使网络的训练复杂度增加了一倍，这在很多情况下已经是过于昂贵的。
- en: Previously, Lyu et al. [[28](#bib.bib28)] also used the notion of penalizing
    the gradient of loss function of network models with respect to the inputs to
    incorporate robustness in the networks against L-BFGS [[22](#bib.bib22)] and FGSM [[23](#bib.bib23)]
    based attacks. Similarly, Shaham et al. [[27](#bib.bib27)] attempted to improve
    the local stability of neural networks by minimizing the loss of a model over
    adversarial examples at each parameter update. They minimized the loss of their
    model over worst-case adversarial examples instead of the original data. In a
    related work, Nguyen and Sinha [[44](#bib.bib44)] introduced a masking based defense
    against C&W attack [[36](#bib.bib36)] by adding noise to the logit outputs of
    networks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 此前，Lyu 等人 [[28](#bib.bib28)] 也利用了惩罚网络模型输入的损失函数梯度的概念，以增强网络对 L-BFGS [[22](#bib.bib22)]
    和 FGSM [[23](#bib.bib23)] 基于攻击的鲁棒性。类似地，Shaham 等人 [[27](#bib.bib27)] 通过在每次参数更新时最小化模型在对抗样本上的损失，尝试提高神经网络的局部稳定性。他们最小化了模型在最坏情况下的对抗样本上的损失，而不是原始数据。在相关工作中，Nguyen
    和 Sinha [[44](#bib.bib44)] 通过在网络的 logit 输出中添加噪声，提出了一种针对 C&W 攻击 [[36](#bib.bib36)]
    的掩蔽防御方法。
- en: 6.2.3 Defensive distillation
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3 防御性蒸馏
- en: 'Papernot et al. [[38](#bib.bib38)] exploited the notion of ‘distillation’ [[166](#bib.bib166)]
    to make deep neural networks robust against adversarial attacks. Distillation
    was introduced by Hinton et al. [[166](#bib.bib166)] as a training procedure to
    transfer knowledge of a more complex network to a smaller network. The variant
    of the procedure introduced by Papernot et al. [[38](#bib.bib38)] essentially
    uses the knowledge of the network to improve its own robustness. The knowledge
    is extracted in the form of class probability vectors of the training data and
    it is fed back to train the original model. It is shown that doing so improves
    resilience of a network to small perturbation in the images. Further empirical
    evidence in this regard is also provided in [[108](#bib.bib108)]. Moreover, in
    a follow-up work, Papernot et al. [[84](#bib.bib84)] also extended the defensive
    distillation method by addressing the numerical instabilities that were encountered
    in [[38](#bib.bib38)]. It is worth noting that the ‘Carlini and Wagner’ (C&W)
    attacks [[36](#bib.bib36)] introduced in Section [3.1](#S3.SS1 "3.1 Attacks for
    classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey") are claimed to be successful against the
    defensive distillation technique. We also note that defensive distillation can
    also be seen as an example of ‘gradient masking’ technique. However, we describe
    it separately keeping in view its popularity in the literature.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 'Papernot 等人 [[38](#bib.bib38)] 利用“蒸馏”的概念 [[166](#bib.bib166)] 使深度神经网络在面对对抗攻击时更具鲁棒性。蒸馏由
    Hinton 等人 [[166](#bib.bib166)] 引入，作为一种将复杂网络的知识转移到较小网络的训练程序。Papernot 等人 [[38](#bib.bib38)]
    引入的这一程序变体本质上利用了网络的知识来提升其自身的鲁棒性。这些知识以训练数据的类概率向量的形式提取，并反馈到原始模型的训练中。研究表明，这种做法能提升网络对图像中小扰动的抗扰动能力。在
    [[108](#bib.bib108)] 中还提供了进一步的实证证据。此外，在后续工作中，Papernot 等人 [[84](#bib.bib84)] 还通过解决
    [[38](#bib.bib38)] 中遇到的数值不稳定性，扩展了防御性蒸馏方法。值得注意的是，在第 [3.1](#S3.SS1 "3.1 Attacks
    for classification ‣ 3 Adversarial attacks ‣ Threat of Adversarial Attacks on
    Deep Learning in Computer Vision: A Survey") 节介绍的“Carlini 和 Wagner”（C&W）攻击 [[36](#bib.bib36)]
    被声称能成功攻击防御性蒸馏技术。我们还注意到，防御性蒸馏也可以视为一种“梯度掩蔽”技术的例子。然而，我们单独描述它，以便考虑到其在文献中的受欢迎程度。'
- en: 6.2.4 Biologically inspired protection
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4 生物启发的保护
- en: Nayebi and Ganguli [[124](#bib.bib124)] demonstrated natural robustness of neural
    networks against adversarial attacks with highly non-linear activations (similar
    to nonlinear dendritic computations). It is noteworthy that the Dense Associative
    Memory models of Krotov and Hopfield [[127](#bib.bib127)] also work on a similar
    principle for robustness against the adversarial examples. Considering the linearity
    hypothesis of Goodfellow et al. [[23](#bib.bib23)], [[124](#bib.bib124)] and [[127](#bib.bib127)]
    seem to further the notion of susceptibility of modern neural networks to adversarial
    examples being the effect of linearity of activations. We note that Brendel and
    Bethge [[187](#bib.bib187)] claim that the attacks fail on the biologically inspired
    protection [[124](#bib.bib124)] due to numerical limitations of computations.
    Stabilizing the computations again allow successful attacks on the protected networks.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Nayebi 和 Ganguli [[124](#bib.bib124)] 展示了神经网络在面对高度非线性激活（类似于非线性树突计算）时对抗攻击的自然鲁棒性。值得注意的是，Krotov
    和 Hopfield 的 Dense Associative Memory 模型 [[127](#bib.bib127)] 也采用类似的原则来增强对抗对抗样本的鲁棒性。考虑到
    Goodfellow 等人 [[23](#bib.bib23)] 的线性假设，[[124](#bib.bib124)] 和 [[127](#bib.bib127)]
    似乎进一步推动了现代神经网络对对抗样本的易受攻击性是激活函数线性效应的观念。我们注意到 Brendel 和 Bethge [[187](#bib.bib187)]
    认为，由于计算的数值限制，对生物学启发的保护 [[124](#bib.bib124)] 的攻击会失败。稳定计算再次允许对受保护的网络进行成功攻击。
- en: 6.2.5 Parseval Networks
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.5 Parseval Networks
- en: Cisse et al. [[131](#bib.bib131)] proposed ‘Parseval’ networks as a defense
    against the adversarial attacks. These networks employ a layer-wise regularization
    by controlling the global Lipschitz constant of the network. Considering that
    a network can be seen as a composition of functions (at each layer), robustification
    against small input perturbations is possible by maintaining a small Lipschitz
    constant for these functions. Cisse et al. proposed to do so by controlling the
    spectral norm of the weight matrices of the networks by parameterizing them with
    ‘parseval tight frames’ [[172](#bib.bib172)], hence the name ‘Parseval’ networks.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Cisse 等人 [[131](#bib.bib131)] 提出了‘Parseval’网络作为对抗攻击的防御。这些网络通过控制网络的全局 Lipschitz
    常数来进行逐层正则化。考虑到网络可以看作是各层函数的组合，通过保持这些函数的小 Lipschitz 常数来实现对小输入扰动的鲁棒性。Cisse 等人提议通过将网络的权重矩阵参数化为‘parseval
    tight frames’ [[172](#bib.bib172)] 来实现这一点，因此得名‘Parseval’网络。
- en: 6.2.6 DeepCloak
  id: totrans-224
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.6 DeepCloak
- en: Gao et al. [[139](#bib.bib139)] proposed to insert a masking layer immediately
    before the layer handling the classification. The added layer is explicitly trained
    by forward-passing clean and adversarial pair of images, and it encodes the differences
    between the output features of the previous layers for those image pairs. It is
    argued that the most dominant weights in the added layer correspond to the most
    sensitive features of the network (in terms of adversarial manipulation). Therefore,
    while classifying, those features are masked by forcing the dominant weights of
    the added layer to zero.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Gao 等人 [[139](#bib.bib139)] 提议在处理分类的层之前立即插入一个掩蔽层。新增的层通过正向传递干净和对抗图像对进行显式训练，并对这些图像对的前一层的输出特征之间的差异进行编码。有人认为，新增层中最主要的权重对应于网络中最敏感的特征（就对抗操控而言）。因此，在分类时，通过强制新增层的主要权重为零来对这些特征进行掩蔽。
- en: 6.2.7 Miscellaneous approaches
  id: totrans-226
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.7 其他方法
- en: Among other notable efforts in making neural networks robust to adversarial
    attacks, Zantedeschi et al. [[46](#bib.bib46)] proposed to use bounded ReLU [[174](#bib.bib174)]
    to reduce the effectiveness of adversarial patterns in the images. Jin et al. [[120](#bib.bib120)]
    introduced a feedforward CNN that used additive noise to mitigate the effects
    of adversarial examples. Sun et al. [[56](#bib.bib56)] prposed ‘HyperNetworks’
    that use statistical filtering as a method to make the network robust. Madry et
    al. [[55](#bib.bib55)] studied adversarial defense from the perspective of robust
    optimization. They showed that adversarial training with a PGD adversary can successfully
    defend against a range of other adversaries. Later, Carlini et al. [[59](#bib.bib59)]
    also verified this observation. Na et al. [[85](#bib.bib85)] employed a network
    that is regularized with a unified embedding for classification and low-level
    similarity learning. The network is penalized using the distance between clean
    and the corresponding adversarial embeddings. Strauss et al. [[89](#bib.bib89)]
    studied ensemble of methods to defend a network against the perturbations. Kadran
    et al. [[136](#bib.bib136)] modified the output layer of a neural network to induce
    robustness against the adversarial attacks. Wang et al. [[129](#bib.bib129)],
    [[122](#bib.bib122)] developed adversary resistant neural networks by leveraging
    non-invertible data transformation in the network. Lee et al. [[106](#bib.bib106)]
    developed manifold regularized networks that use a training objective to minimizes
    the difference between multi-layer embedding results of clean and adversarial
    images. Kotler and Wong [[96](#bib.bib96)] proposed to learn ReLU-based classifier
    that show robustness against small adversarial perturbations. They train a neural
    network that provably achieves high accuracy (¿90%) against any adversary in a
    canonical setting ($\epsilon=0.1$ for $\ell_{\infty}$-norm perturbation on MNIST).
    Raghunathan et al. [[189](#bib.bib189)] studied the problem of defense for neural
    networks with one hidden layer. Their approach produces a network and a certificate
    on MNIST dataset such that no attack perturbing image pixels by at most $\epsilon=0.1$
    could results in more than 35% test error. Kolter and Wong [[96](#bib.bib96)]
    and Raghunathan et al. [[189](#bib.bib189)] are among very few provable methods
    in defense against the adversarial attacks. Given that these methods are computationally
    infeasible to apply on larger networks, the only defenses that have been extensively
    evaluated are those of Madry et al. [[55](#bib.bib55)] giving  89% accuracy against
    large epsilon (0.3/1) on MNIST and 45% for moderate epsilon (8/255) on CIFAR.
    Another thread of works that can be seen as adversarial attacks/defenses with
    guarantees is related to verification of deep neural networks, e.g. [[191](#bib.bib191)],
    [[192](#bib.bib192)]. In their approach OrOrbia et al. [[194](#bib.bib194)] show
    that many different proposals of adversarial training are instances of more general
    regularized objective, they termed DataGrad. The proposed DataGrad framework can
    be seen as an extension of layerwise contractive autoencoder penalty.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在使神经网络对对抗攻击具有鲁棒性的其他显著努力中，Zantedeschi 等人[[46](#bib.bib46)] 提出了使用有界 ReLU [[174](#bib.bib174)]
    来减少图像中对抗模式的有效性。Jin 等人[[120](#bib.bib120)] 介绍了一种前馈 CNN，该 CNN 使用附加噪声来减轻对抗样本的影响。Sun
    等人[[56](#bib.bib56)] 提出了“HyperNetworks”，它们使用统计过滤作为一种使网络具有鲁棒性的方法。Madry 等人[[55](#bib.bib55)]
    从鲁棒优化的角度研究了对抗防御。他们展示了使用 PGD 对手的对抗训练可以成功防御一系列其他对手。后来，Carlini 等人[[59](#bib.bib59)]
    也验证了这一观察结果。Na 等人[[85](#bib.bib85)] 使用了一个通过统一嵌入进行分类和低级相似性学习的网络。该网络通过清洁与对应对抗嵌入之间的距离进行惩罚。Strauss
    等人[[89](#bib.bib89)] 研究了防御网络对扰动的集成方法。Kadran 等人[[136](#bib.bib136)] 修改了神经网络的输出层，以提高对对抗攻击的鲁棒性。Wang
    等人[[129](#bib.bib129)], [[122](#bib.bib122)] 通过利用网络中的不可逆数据变换开发了抗对抗攻击的神经网络。Lee
    等人[[106](#bib.bib106)] 开发了流形正则化网络，使用训练目标最小化干净图像和对抗图像的多层嵌入结果之间的差异。Kotler 和 Wong
    [[96](#bib.bib96)] 提出了学习基于 ReLU 的分类器，该分类器表现出对小对抗扰动的鲁棒性。他们训练了一个神经网络，该网络在经典设置中（$\epsilon=0.1$
    对 MNIST 上的 $\ell_{\infty}$-范数扰动）可以证明实现高准确率（¿90%）对抗任何对手。Raghunathan 等人[[189](#bib.bib189)]
    研究了具有一个隐藏层的神经网络的防御问题。他们的方法在 MNIST 数据集上生成一个网络和一个证书，使得任何攻击图像像素扰动最多 $\epsilon=0.1$
    的攻击都不会导致超过 35% 的测试错误。Kolter 和 Wong [[96](#bib.bib96)] 以及 Raghunathan 等人[[189](#bib.bib189)]
    是为数不多的对抗攻击中具有可证明方法的研究之一。鉴于这些方法在更大网络上的计算开销不可行，唯一经过广泛评估的防御方法是 Madry 等人[[55](#bib.bib55)]
    的方法，该方法在 MNIST 上对大 $\epsilon$（0.3/1）提供了 89% 的准确率，在 CIFAR 上对中等 $\epsilon$（8/255）提供了
    45% 的准确率。另一类可以视为具有保证的对抗攻击/防御的工作与深度神经网络的验证相关，例如[[191](#bib.bib191)], [[192](#bib.bib192)]。在他们的方法中，OrOrbia
    等人[[194](#bib.bib194)] 展示了许多不同的对抗训练方案是更一般的正则化目标的实例，他们将其称为 DataGrad。提出的 DataGrad
    框架可以视为逐层收缩自编码器惩罚的扩展。
- en: 6.2.8 Detection Only approaches
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.8 仅检测方法
- en: 'SafetyNet: Lu et al. [[66](#bib.bib66)] hypothesized that adversarial examples
    produce different patterns of ReLU activations in (the late stages of) networks
    than what is produced by clean images. Based on this hypothesis, they proposed
    to append a Radial Basis Function SVM classifier to the targeted models such that
    the SVM uses discrete codes computed by the late stage ReLUs of the network. To
    detect perturbation in a test image, its code is compared against those of training
    samples using the SVM. Effective detection of adversarial examples generated by
    [[23](#bib.bib23)], [[35](#bib.bib35)], [[72](#bib.bib72)] is demonstrated by
    their framework, named SafetyNet.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: SafetyNet：Lu 等人 [[66](#bib.bib66)] 假设对抗样本在网络的（后期）阶段产生的 ReLU 激活模式与干净图像产生的模式不同。基于这一假设，他们建议在目标模型中附加一个径向基函数支持向量机（SVM）分类器，使得
    SVM 使用网络后期 ReLU 计算的离散代码。为了检测测试图像中的扰动，其代码与训练样本的代码通过 SVM 进行比较。他们的框架 SafetyNet 有效检测了由
    [[23](#bib.bib23)]、[[35](#bib.bib35)]、[[72](#bib.bib72)] 生成的对抗样本。
- en: 'Detector subnetwork: Metzen et al. [[78](#bib.bib78)] proposed to augment a
    targeted network with a subnetwork that is trained for a binary classification
    task of detecting adversarial perturbations in inputs. It is shown that appending
    such a network to the internal layers of a model and using adversarial training
    can help in detecting perturbations generated using FGSM [[23](#bib.bib23)], BIM [[35](#bib.bib35)]
    and DeepFool [[72](#bib.bib72)] methods. However, Lu et al. [[66](#bib.bib66)]
    later showed that this approach is again vulnerable to counter-counter measures.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 检测器子网络：Metzen 等人 [[78](#bib.bib78)] 提出了用一个针对检测对抗扰动的二分类任务训练的子网络来增强目标网络。研究表明，将这种网络附加到模型的内部层并使用对抗训练，可以帮助检测使用
    FGSM [[23](#bib.bib23)]、BIM [[35](#bib.bib35)] 和 DeepFool [[72](#bib.bib72)] 方法生成的扰动。然而，Lu
    等人 [[66](#bib.bib66)] 后来展示了这种方法对反对措施仍然脆弱。
- en: 'Exploiting convolution filter statistics: Li and Li [[105](#bib.bib105)] used
    statistics of the convolution filters in CNN-based neural networks to classify
    the input images as clean or adversarial. A cascaded classifier is designed that
    uses these statistics, and it is shown to detect more than $85\%$ adversarial
    images generated by the methods in [[22](#bib.bib22)], [[114](#bib.bib114)].'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 利用卷积滤波器统计信息：Li 和 Li [[105](#bib.bib105)] 使用 CNN 基于神经网络中的卷积滤波器统计信息将输入图像分类为干净或对抗性图像。设计了一个级联分类器，使用这些统计信息，结果表明能够检测到超过
    $85\%$ 由 [[22](#bib.bib22)]、[[114](#bib.bib114)] 方法生成的对抗图像。
- en: 'Additional class augmentation: Grosse et al. [[57](#bib.bib57)] proposed to
    augment the potentially targeted neural network model with an additional class
    in which the model is trained to classify all the adversarial examples. Hosseini
    et al. [[32](#bib.bib32)] also employed a similar strategy to detect black-box
    attacks.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 额外类别增强：Grosse 等人 [[57](#bib.bib57)] 提出了通过额外的类别增强可能的目标神经网络模型，其中模型被训练来分类所有对抗样本。Hosseini
    等人 [[32](#bib.bib32)] 也采用了类似的策略来检测黑箱攻击。
- en: 6.3 Network add-ons
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 网络附加组件
- en: 6.3.1 Defense against universal perturbations
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1 防御通用扰动
- en: 'Akhtar et al. [[81](#bib.bib81)] proposed a defense framework against the adversarial
    attacks generated using universal perturbations [[16](#bib.bib16)]. The framework
    appends extra ‘pre-input’ layers to the targeted network and trains them to rectify
    a perturbed image so that the classifier’s prediction becomes the same as its
    prediction on the clean version of the same image. The pre-input layers are termed
    Perturbation Rectifying Network (PRN), and they are trained without updating the
    parameters of the targeted network. A separate detector is trained by extracting
    features from the input-output differences of PRN for the training images. A test
    image is first passed through the PRN and then its features are used to detect
    perturbations. If adversarial perturbations are detected, the output of PRN is
    used to classify the test image. Fig. [10](#S6.F10 "Figure 10 ‣ 6.3.1 Defense
    against universal perturbations ‣ 6.3 Network add-ons ‣ 6 Defenses against adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey"), illustrates the rectification performed by PRN. The removed patterns
    are separately analyzed for detection.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 'Akhtar 等人[[81](#bib.bib81)] 提出了一个对抗使用通用扰动生成的对抗攻击的防御框架[[16](#bib.bib16)]。该框架在目标网络中添加额外的“前置输入”层，并训练这些层来修正扰动图像，使分类器的预测与同一图像的干净版本的预测一致。前置输入层被称为扰动修正网络（PRN），并且在训练时不更新目标网络的参数。通过提取PRN的输入输出差异特征来训练一个单独的检测器。测试图像首先通过PRN，然后使用其特征来检测扰动。如果检测到对抗扰动，则使用PRN的输出对测试图像进行分类。图[10](#S6.F10
    "Figure 10 ‣ 6.3.1 Defense against universal perturbations ‣ 6.3 Network add-ons
    ‣ 6 Defenses against adversarial attacks ‣ Threat of Adversarial Attacks on Deep
    Learning in Computer Vision: A Survey")展示了PRN执行的修正。移除的模式会单独分析以进行检测。'
- en: '![Refer to caption](img/7518593d135fed29ba48c13855dcf804.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/7518593d135fed29ba48c13855dcf804.png)'
- en: 'Figure 10: Illustration of defense against universal perturbations [[81](#bib.bib81)]:
    The approach rectifies an image to restore the network prediction. The pattern
    removed by rectification is separately analyzed to detect perturbation.'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：对抗通用扰动的防御示意图[[81](#bib.bib81)]：该方法对图像进行修正，以恢复网络预测。通过修正移除的模式进行单独分析，以检测扰动。
- en: 6.3.2 GAN-based defense
  id: totrans-238
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2 基于GAN的防御
- en: Lee et al. [[101](#bib.bib101)] used the popular framework of Generative Adversarial
    Networks [[153](#bib.bib153)] to train a network that is robust to FGSM [[23](#bib.bib23)]
    like attacks. The authors proposed to directly train the network along a generator
    network that attempts to generate perturbation for that network. During its training,
    the classifier keeps trying to correctly classify both the clean and perturbed
    images. We categorize this technique as an ‘add-on’ approach because the authors
    propose to always train any network in this fashion. In another GAN-based defense,
    Shen et al. [[58](#bib.bib58)] use the generator part of the network to rectify
    a perturbed image.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Lee 等人[[101](#bib.bib101)] 使用了流行的生成对抗网络框架[[153](#bib.bib153)]来训练一个对 FGSM[[23](#bib.bib23)]
    类攻击具有鲁棒性的网络。作者提出直接训练一个生成器网络，该网络试图为该网络生成扰动。在训练过程中，分类器不断尝试正确分类干净图像和扰动图像。我们将这种技术归类为“附加”方法，因为作者建议始终以这种方式训练任何网络。在另一种基于GAN的防御中，Shen
    等人[[58](#bib.bib58)] 使用网络的生成器部分来修正扰动图像。
- en: 6.3.3 Detection Only approaches
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3 仅检测方法
- en: 'Feature squeezing: Xu et al. [[43](#bib.bib43)] proposed to use feature squeezing
    to detect adversarial perturbation to an image. They added two external models
    to the classifier network, such that these models reduce the color bit depth of
    each pixel in the image, and perform spatial smoothing over the image. The predictions
    of the targeted network over the original image and the squeezed images are compared.
    If a large difference is found between the predictions, the image is considered
    to be an adversarial example. Whereas [[43](#bib.bib43)] demonstrated the effectiveness
    of this approach against more classical attacks [[23](#bib.bib23)], a follow-up
    work [[140](#bib.bib140)] also claims that the method works reasonably well against
    the more powerful C&W attacks [[36](#bib.bib36)]. He et al. [[76](#bib.bib76)]
    also combined feature squeezing with the ensemble method proposed in [[175](#bib.bib175)]
    to show that strength of defenses does not always increase by combining them.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 特征挤压：Xu 等人 [[43](#bib.bib43)] 提出了使用特征挤压来检测图像的对抗扰动。他们向分类器网络中添加了两个外部模型，这些模型减少了图像中每个像素的颜色位深，并对图像进行空间平滑处理。比较原始图像与挤压图像的预测结果。如果预测结果之间存在较大差异，则该图像被视为对抗样本。虽然
    [[43](#bib.bib43)] 证明了这种方法在对抗更经典攻击 [[23](#bib.bib23)] 中的有效性，但后续工作 [[140](#bib.bib140)]
    也声称该方法在面对更强大的 C&W 攻击 [[36](#bib.bib36)] 时表现良好。He 等人 [[76](#bib.bib76)] 还将特征挤压与
    [[175](#bib.bib175)] 中提出的集成方法结合，显示出防御的强度并不总是通过组合来增加。
- en: 'MagNet: Meng and Chen [[45](#bib.bib45)] proposed a framework that uses one
    or more external detectors to classify an input image as adversarial or clean.
    During training, the framework aims at learning the manifold of clean images.
    In the testing phase, the images that are found far from the manifold are treated
    as adversarial and are rejected. The images that are close to the manifold (but
    not exactly on it) are always reformed to lie on the manifold and the classifier
    is fed with the reformed images. The notion of attracting nearby images to the
    manifold of clean images and dropping the far-off images also inspires the name
    of the framework, i.e. MagNet. It is noteworthy that Carlini and Wagner [[188](#bib.bib188)]
    very recently demonstrated that this defense technique can also be defeated with
    slightly larger perturbations.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: MagNet：Meng 和 Chen [[45](#bib.bib45)] 提出了一个框架，使用一个或多个外部检测器将输入图像分类为对抗样本或干净样本。在训练期间，框架旨在学习干净图像的流形。在测试阶段，发现远离流形的图像被视为对抗样本并被拒绝。接近流形（但不完全在其上）的图像总是被重新调整以位于流形上，分类器则使用这些调整后的图像。将附近图像吸引到干净图像的流形上并丢弃远离图像的概念也激发了该框架的名称，即
    MagNet。值得注意的是，Carlini 和 Wagner [[188](#bib.bib188)] 最近证明，这种防御技术也可以被稍大一点的扰动击败。
- en: 'Miscellaneous methods: Liang et al. [[50](#bib.bib50)] treated perturbations
    to images as noise and used scalar quantization and spatial smoothing filter to
    separately detect such perturbations. In a related approach, Feinman et al. [[86](#bib.bib86)]
    proposed to detect adversarial perturbations by harnessing uncertainty estimates
    (of dropout neural networks) and performing density estimation in the feature
    space of neural networks. Eventually, separate binary classifiers are trained
    as adversarial example detectors using the proposed features. Gebhart and Schrater [[92](#bib.bib92)]
    viewed neural network computation as information flow in graphs and proposed a
    method to detect adversarial perturbations by applying persistent homology to
    the induced graphs.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 杂项方法：Liang 等人 [[50](#bib.bib50)] 将对图像的扰动视为噪声，并使用标量量化和空间平滑滤波器来分别检测这些扰动。在相关方法中，Feinman
    等人 [[86](#bib.bib86)] 提出了通过利用不确定性估计（来自 dropout 神经网络）和在神经网络特征空间中执行密度估计来检测对抗扰动。最终，训练出独立的二分类器作为对抗样本检测器。Gebhart
    和 Schrater [[92](#bib.bib92)] 将神经网络计算视为图中的信息流，并提出了一种通过对诱导图应用持久同调来检测对抗扰动的方法。
- en: 7 Outlook of the research direction
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 研究方向展望
- en: In the previous sections, we presented a comprehensive review of the recent
    literature in adversarial attacks on deep learning. Whereas several interesting
    facts were reported in those sections along the technical details, below we make
    more general observations regarding this emerging research direction. The discussion
    presents a broader outlook to the readers without in-depth technical knowledge
    of this area. Our arguments are based on the literature reviewed above.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们对深度学习中的对抗攻击的最新文献进行了全面回顾。虽然这些章节报告了若干有趣的事实和技术细节，但下面我们将对这一新兴研究方向进行更一般性的观察。讨论向读者展示了一个更广泛的视角，而无需对该领域有深入的技术知识。我们的论点基于上述回顾的文献。
- en: 'The threat is real: Whereas few works suggest that adversarial attacks on deep
    learning may not be a serious concern, a large body of the related literature
    indicates otherwise. The literature reviewed in Sections [3](#S3 "3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey") and [4](#S4 "4 Attacks in the real world ‣ Threat of Adversarial Attacks
    on Deep Learning in Computer Vision: A Survey") clearly demonstrate that adversarial
    attacks can severely degrade the performance of deep learning techniques on multiple
    Computer Vision tasks, and beyond. In particular, the literature reviewed in Section [4](#S4
    "4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey") ascertains that deep learning is vulnerable to
    adversarial attacks in the real physical world. Therefore, we can conclusively
    argue that adversarial attacks pose a real threat to deep learning in practice.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '威胁是真实的：虽然有少数研究表明对深度学习的对抗攻击可能不是一个严重的问题，但大量相关文献却表明情况并非如此。第[3](#S3 "3 Adversarial
    attacks ‣ Threat of Adversarial Attacks on Deep Learning in Computer Vision: A
    Survey")和[4](#S4 "4 Attacks in the real world ‣ Threat of Adversarial Attacks
    on Deep Learning in Computer Vision: A Survey")节中回顾的文献明确表明，对抗攻击可以严重降低深度学习技术在多个计算机视觉任务上的表现，甚至更广泛。特别是，第[4](#S4
    "4 Attacks in the real world ‣ Threat of Adversarial Attacks on Deep Learning
    in Computer Vision: A Survey")节中回顾的文献确认，深度学习在现实物理世界中容易受到对抗攻击。因此，我们可以明确地认为，对抗攻击对实际中的深度学习构成了真实的威胁。'
- en: 'Adversarial vulnerability is a general phenomenon: The reviewed literature
    shows successful fooling of different types of deep neural networks, e.g. MLPs,
    CNNs, RNNs on a variety of tasks in Computer Vision, e.g. recognition, segmentation,
    detection. Although most of the existing works focus on fooling deep learning
    on the task of classification/recognition, based on the surveyed literature we
    can easily observe that deep learning approaches are vulnerable to adversarial
    attacks in general.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗脆弱性是一个普遍现象：回顾的文献展示了在各种计算机视觉任务（如识别、分割、检测）中，成功欺骗了不同类型的深度神经网络，例如MLPs、CNNs、RNNs。尽管大多数现有工作集中在欺骗深度学习的分类/识别任务上，但根据调查的文献，我们可以轻松观察到深度学习方法普遍对对抗攻击存在脆弱性。
- en: 'Adversarial examples often generalize well: One of the most common properties
    of adversarial examples reported in the literature is that they transfer well
    between different neural networks. This is especially true for the networks that
    have relatively similar architecture. The generalization of adversarial examples
    is often exploited in black-box attacks.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗样本通常具有良好的泛化能力：文献中报告的对抗样本的一个最常见特性是它们在不同的神经网络之间能很好地迁移。这在结构相对类似的网络中尤其如此。对抗样本的泛化特性常常在黑盒攻击中被利用。
- en: 'Reasons of adversarial vulnerability need more investigation: There are varied
    view-points in the literature on the reasons behind the vulnerability of deep
    neural networks to subtle adversarial perturbations. Often, these view-points
    are not well-aligned with each other. There is an obvious need for systematic
    investigation in this direction.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗脆弱性的原因需要更多调查：关于深度神经网络对微妙对抗扰动的脆弱性原因，文献中存在各种观点。这些观点通常彼此不太一致。显然，需要在这个方向上进行系统性的调查。
- en: 'Linearity does promote vulnerability: Goodfellow et al. [[23](#bib.bib23)]
    first suggested that the design of modern deep neural networks that forces them
    to behave linearly in high dimensional spaces also makes them vulnerable to adversarial
    attacks. Although popular, this notion has also faced some opposition in the literature.
    Our survey pointed out multiple independent contributions that hold linearity
    of the neural networks accountable for their vulnerability to adversarial attacks.
    Based on this fact, we can argue that linearity does promote vulnerability of
    deep neural networks to the adversarial attacks. However, it does not seem to
    be the only reason behind successful fooling of deep neural networks with cheap
    analytical perturbations.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 线性确实促进了脆弱性：Goodfellow等人[[23](#bib.bib23)]首先建议，现代深度神经网络的设计迫使它们在高维空间中表现出线性行为，这也使它们容易受到敌对攻击。尽管这一观点很流行，但在文献中也遇到了一些反对意见。我们的调查指出了多个独立的贡献，这些贡献将神经网络的线性特性归咎于其对敌对攻击的脆弱性。基于这一事实，我们可以争辩说，线性确实促进了深度神经网络对敌对攻击的脆弱性。然而，这似乎不是深度神经网络受到廉价分析扰动成功欺骗的唯一原因。
- en: 'Counter-counter measures are possible: Whereas multiple defense techniques
    exist to counter adversarial attacks, it is often shown in the literature that
    a defended model can again be successfully attacked by devising counter-counter
    measures, e.g. see [[49](#bib.bib49)]. This observation necessitates that new
    defenses also provide an estimate of their robustness against obvious counter-counter
    measures.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 反制措施是可能的：尽管存在多种防御技术来对抗敌对攻击，但文献中经常显示，被防御的模型仍然可以通过设计反制措施再次成功攻击，例如参见[[49](#bib.bib49)]。这一观察结果要求新的防御措施还需要提供对明显反制措施的鲁棒性评估。
- en: 'Highly active research direction: The profound implications of vulnerability
    of deep neural networks to adversarial perturbations have made research in adversarial
    attacks and their defenses highly active in recent time. The majority of the literature
    reviewed in this survey surfaced in the last two years, and there is currently
    a continuous influx of contributions in this direction. On one hand, techniques
    are being proposed to defend neural networks against the known attacks, on the
    other; more and mote powerful attacks are being devised. Recently, a Kaggle competition
    was also organized for the defenses against the adversarial attacks ([https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack/](https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack/)).
    It can be hoped that this high research activity will eventually result in making
    deep learning approaches robust enough to be used in safety and security critical
    applications in the real world.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 高度活跃的研究方向：深度神经网络对敌对扰动的脆弱性的深远影响使得对敌对攻击及其防御的研究在最近时间内高度活跃。本调查中审阅的大部分文献都出现在过去两年，并且目前在这一方向上不断有新的贡献出现。一方面，提出了保护神经网络免受已知攻击的技术；另一方面，更加强大的攻击手段也在不断被设计。最近，还组织了一场针对敌对攻击的防御的Kaggle竞赛（[https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack/](https://www.kaggle.com/c/nips-2017-defense-against-adversarial-attack/)）。可以期望，这一高水平的研究活动最终将使深度学习方法足够稳健，以用于现实世界中的安全和保密应用。
- en: 8 Conclusion
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: This article presented the first comprehensive survey in the direction of adversarial
    attacks on deep learning in Computer Vision. Despite the high accuracies of deep
    neural networks on a wide variety of Computer Vision tasks, they have been found
    vulnerable to subtle input perturbations that lead them to completely change their
    outputs. With deep learning at the heart of the current advances in machine learning
    and artificial intelligence, this finding has resulted in numerous recent contributions
    that devise adversarial attacks and their defenses for deep learning. This article
    reviews these contributions, mainly focusing on the most influential and interesting
    works in the literature. From the reviewed literature, it is apparent that adversarial
    attacks are a real threat to deep learning in practice, especially in safety and
    security critical applications. The existing literature demonstrates that currently
    deep learning can not only be effectively attacked in cyberspace but also in the
    physical world. However, owing to the very high activity in this research direction
    it can be hoped that deep learning will be able to show considerable robustness
    against the adversarial attacks in the future.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了计算机视觉中对深度学习的对抗攻击的第一个全面调查。尽管深度神经网络在各种计算机视觉任务中具有很高的准确率，但它们在面对微小的输入扰动时容易出现输出完全改变的情况。随着深度学习成为当前机器学习和人工智能进步的核心，这一发现导致了大量针对深度学习的对抗攻击及其防御的最新贡献。本文回顾了这些贡献，主要关注文献中最具影响力和最有趣的工作。从回顾的文献中明显可以看出，对抗攻击对深度学习实践构成了真正的威胁，尤其是在安全和保密关键应用中。现有文献表明，目前深度学习不仅在网络空间中容易受到攻击，而且在物理世界中也会受到威胁。然而，由于该研究方向的活动非常高，可以希望未来深度学习能够展示出对对抗攻击的相当鲁棒性。
- en: References
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Y. LeCun, Y. Bengio and G. Hinton, *Deep learning*, Nature, vol. 521, no. 7553,
    pp. 436-444, 2015.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] Y. LeCun, Y. Bengio 和 G. Hinton，*深度学习*，《自然》，第521卷，第7553期，页436-444，2015年。'
- en: '[2] M. Helmstaedter, K. L. Briggman, S. C. Turaga, V. Jain, H. S. Seung, and
    W. Denk, *Connectomic reconstruction of the inner plexiform layer in the mouse
    retina*. Nature, vol. 500, no. 7461, pp. 168-174, 2013.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] M. Helmstaedter, K. L. Briggman, S. C. Turaga, V. Jain, H. S. Seung 和 W.
    Denk，*小鼠视网膜内层的连接组重建*，《自然》，第500卷，第7461期，页168-174，2013年。'
- en: '[3] H. Y. Xiong, B. Alipanahi, J. L. Lee, H. Bretschneider, D. Merico, R. K.
    Yuen, and Q. Morris, *The human splicing code reveals new insights into the genetic
    determinants of disease*, Science, vol. 347, no. 6218, 1254806 2015.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] H. Y. Xiong, B. Alipanahi, J. L. Lee, H. Bretschneider, D. Merico, R. K.
    Yuen 和 Q. Morris，*人类剪接密码揭示了疾病遗传决定因素的新见解*，《科学》，第347卷，第6218期，1254806 2015年。'
- en: '[4] J. Ma, R. P. Sheridan, A. Liaw, G. E. Dahl and V. Svetnik, *Deep neural
    nets as a method for quantitative structure-activity relationships*, Journal of
    chemical information and modeling, vol. 55, no. 2 pp. 263-274, 2015.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] J. Ma, R. P. Sheridan, A. Liaw, G. E. Dahl 和 V. Svetnik，*深度神经网络作为定量结构-活性关系的方法*，《化学信息与建模杂志》，第55卷，第2期，页263-274，2015年。'
- en: '[5] T. Ciodaro, D. Deva, J. de Seixas and D. Damazio, *Online particle detection
    with neural networks based on topological calorimetry information*. Journal of
    physics: conference series. vol. 368, no. 1\. IOP Publishing, 2012.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] T. Ciodaro, D. Deva, J. de Seixas 和 D. Damazio，*基于拓扑量热学信息的神经网络在线粒子检测*。《物理学杂志：会议系列》。第368卷，第1期。IOP出版公司，2012年。'
- en: '[6] Kaggle. Higgs boson machine learning challenge. Kaggle [https://www.kaggle.com/c/higgs-boson](https://www.kaggle.com/c/higgs-boson),
    2014.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] Kaggle. Higgs boson机器学习挑战。Kaggle [https://www.kaggle.com/c/higgs-boson](https://www.kaggle.com/c/higgs-boson)，2014年。'
- en: '[7] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. R. Mohamed, N. Jaitly, and B.
    Kingsbury, *Deep neural networks for acoustic modeling in speech recognition:
    The shared views of four research groups*, IEEE Signal Processing Magazine, vol.
    29, no. 6, pp. 82-97, 2012.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A. R. Mohamed, N. Jaitly 和 B. Kingsbury，*用于语音识别的声学建模的深度神经网络：四个研究组的共同观点*，《IEEE信号处理杂志》，第29卷，第6期，页82-97，2012年。'
- en: '[8] I. Sutskever, O. Vinyals, and Q. V. Le, *Sequence to sequence learning
    with neural networks*. In Advances in neural information processing systems, pp.
    3104-3112, 2014.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] I. Sutskever, O. Vinyals 和 Q. V. Le，*基于神经网络的序列到序列学习*。在《神经信息处理系统进展》中，页3104-3112，2014年。'
- en: '[9] A. Krizhevsky, I. Sutskever and G. E. Hinton, *Imagenet classification
    with deep convolutional neural networks*. In Advances in neural information processing
    systems, pp. 1097-1105, 2012.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] A. Krizhevsky, I. Sutskever 和 G. E. Hinton，*使用深度卷积神经网络的Imagenet分类*。在《神经信息处理系统进展》中，页1097-1105，2012年。'
- en: '[10] Y. LeCun, B. Boser, J. S. Denker, D. Henderson, R. E. Howard, W. Hubbard
    and L. D. Jackel, *Backpropagation applied to handwritten zip code recognition*.
    Neural computation, vol. 1m no. 4, pp. 541-551, 1989.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, and L. Fei-Fei, *Imagenet:
    A large-scale hierarchical image database*. In IEEE Conference on Computer Vision
    and Pattern Recognition, pp. 248-255, 2009.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] E. Ackerman, *How Drive.ai is mastering autonomous driving with deep learning*,
    [https://spectrum.ieee.org/cars-that-think/transportation/self-driving/how-driveai-is-mastering-autonomous-driving-with-deep-learning](https://spectrum.ieee.org/cars-that-think/transportation/self-driving/how-driveai-is-mastering-autonomous-driving-with-deep-learning),
    Accessed December 2017.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] M. M. Najafabadi, F. Villanustre, T. M. Khoshgoftaar, N. Seliya, R. Wald
    and E. Muharemagic, *Deep learning applications and challenges in big data analytics*,
    Journal of Big Data, vol. 2, no. 1, 2015.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. Silver, J. Schrittwieser, K. Simonyan, I. Antonoglou, A. Huang, A.
    Guez, Y. Chen, *Mastering the game of go without human knowledge*. Nature, vol.
    550, no. 7676, pp. 354-359, 2017.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] K. He, X. Zhang, S. Ren, J. Sun, *Deep residual learning for image recognition*.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pp. 770-778, 2016.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] S. M. Moosavi-Dezfooli, A. Fawzi, O. Fawzi and P. Frossard, *Universal
    adversarial perturbations*. In Proceedings of IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), 2017.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] K. Chatfield, K. Simonyan, A. Vedaldi, A. Zisserman, *Return of the devil
    in the details: Delving deep into convolutional nets*, arXiv preprint arXiv:1405.3531,
    2014.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] C. Szegedy, W. Liu, Y. Jia, P. Sermanet, S. Reed, D. Anguelov, A. Rabinovich,
    *Going deeper with convolutions*. In Proceedings of the IEEE conference on computer
    vision and pattern recognition, pp. 1-9, 2015.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] J. Lu, H. Sibai, E. Fabry, D. Forsyth, *Standard detectors aren’t (currently)
    fooled by physical adversarial stop signs*, arXiv preprint arXiv:1710.03337, 2017.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. Fletcher, *Practical methods of optimization*, John Wiley and Sons,
    2013.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] A. Esteva, B. Kuprel, R. A. Novoa, J. Ko, S. M. Swetter, H. M. Blau, S.
    Thrun, *Dermatologist-level classification of skin cancer with deep neural networks*,
    Nature, vol. 542, pp. 115 - 118, 2017.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] C. Szegedy, W. Zaremba, I. Sutskever, J. Bruna, D. Erhan, I. Goodfellow,
    R. Fergus, *Intriguing properties of neural networks*, arXiv preprint arXiv:1312.6199,
    2014.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] I. J. Goodfellow, J. Shlens, C. Szegedy, *Explaining and Harnessing Adversarial
    Examples*, arXiv preprint arXiv:1412.6572, 2015.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] S. Gu, L. Rigazio, *Towards Deep Neural Network Architectures Robust to
    Adversarial Examples*, arXiv preprint arXiv:1412.5068, 2015'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] P. Tabacof, E. Valle, *Exploring the Space of Adversarial Images*, In
    IEEE International Joint Conference on Neural Networks, pp. 426-433, 2016.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] S. Sabour, Y. Cao, F. Faghri, and D. J. Fleet, *Adversarial manipulation
    of deep representations*, arXiv preprint arXiv:1511.05122, 2015.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] U. Shaham, Y. Yamada, S. Negahban, *Understanding Adversarial Training:
    Increasing Local Stability of Neural Nets through Robust Optimization*, arXiv
    preprint arXiv:1511.05432, 2016.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] C. Lyu, K. Huang, H. Liang, *A Unified Gradient Regularization Family
    for Adversarial Examples*, In IEEE International Conference on Data Mining, pp.
    301-309, 2015.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A.
    Rahmati, D. Song, *Robust Physical-World Attacks on Deep Learning Models*, arXiv
    preprint arXiv:1707.08945, 2017.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] J. Lu, H. Sibai, E. Fabry, D. Forsyth, *No need to worry about adversarial
    examples in object detection in autonomous vehicles*, arXiv preprint arXiv:1707.03501,
    2017.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Y. Liu, W. Zhang, S. Li, N. Yu, *Enhanced Attacks on Defensively Distilled
    Deep Neural Networks*, arXiv preprint arXiv:1711.05934, 2017.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] H. Hosseini, Y. Chen, S. Kannan, B. Zhang, R. Poovendran, *Blocking transferability
    of adversarial examples in black-box learning systems*, arXiv preprint arXiv:1703.04318,
    2017.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] T. Gu, B. Dolan-Gavitt, S. Garg, *BadNets: Identifying Vulnerabilities
    in the Machine Learning Model Supply Chain.* arXiv preprint arXiv:1708.06733,
    2017.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] N. Papernot, P. McDaniel, A. Sinha, M. Wellman, *Towards the Science of
    Security and Privacy in Machine Learning*, arXiv preprint arXiv:1611.03814, 2016.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] A. Kurakin, I. Goodfellow, S. Bengio, *Adversarial examples in the physical
    world*, arXiv preprint arXiv:1607.02533, 2016.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] N. Carlini, D. Wagner, *Towards Evaluating the Robustness of Neural Networks*,
    arXiv preprint arXiv:1608.04644, 2016.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] N. Das, M. Shanbhogue, S. Chen, F. Hohman, L. Chen, M. E. Kounavis, D.
    H. Chau, *Keeping the Bad Guys Out: Protecting and Vaccinating Deep Learning with
    JPEG Compression*, arXiv preprint arXiv:1705.02900, 2017.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] N. Papernot, P. McDaniel, X. Wu, S. Jha, A. Swami, *Distillation as a
    Defense to Adversarial Perturbations against Deep Neural Networks*, In IEEE Symposium
    on Security and Privacy (SP), pp. 582-597, 2016.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] N. Papernot, P. McDaniel, I. Goodfellow, S. Jha, Z. B. Celik, A. Swami,
    *Practical Black-Box Attacks against Machine Learning*, In Proceedings of the
    ACM on Asia Conference on Computer and Communications Security, pp. 506-519\.
    ACM, 2017.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] X. Xu, X. Chen, C. Liu, A. Rohrbach, T. Darell, D. Song, *Can you fool
    AI with adversarial examples on a visual Turing test?*, arXiv preprint arXiv:1709.08693,
    2017'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] P. Chen, H. Zhang, Y. Sharma, J. Yi, C. Hsieh, *ZOO: Zeroth Order Optimization
    based Black-box Attacks to Deep Neural Networks without Training Substitute Models*,
    In Proceedings of 10th ACM Workshop on Artificial Intelligence and Security (AISEC),
    2017.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] S. Baluja, I. Fischer, *Adversarial Transformation Networks: Learning
    to Generate Adversarial Examples*, arXiv preprint arXiv:1703.09387, 2017.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] W. Xu, D. Evans, Y. Qi, *Feature Squeezing: Detecting Adversarial Examples
    in Deep Neural Networks*, arXiv preprint arXiv:1704.01155, 2017.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] L. Nguyen, A. Sinha, *A Learning and Masking Approach to Secure Learning*,
    arXiv preprint arXiv:1709.04447, 2017.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Dongyu Meng, Hao Chen, *MagNet: a Two-Pronged Defense against Adversarial
    Examples*, In Proceedings of ACM Conference on Computer and Communications Security
    (CCS), 2017.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] V. Zantedeschi, M. Nicolae, A. Rawat, *Efficient Defenses Against Adversarial
    Attacks*, arXiv preprint arXiv:1707.06728, 2017.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J. Hayes, G. Danezis, *Machine Learning as an Adversarial Service: Learning
    Black-Box Adversarial Examples*, arXiv preprint arXiv:1708.05207, 2017.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] A. Graese, A. Rozsa, T. E. Boult, *Assessing Threat of Adversarial Examples
    on Deep Neural Networks*, In IEEE International Conference on Machine Learning
    and Applications, pp. 69-74, 2016.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] N. Carlini, D. Wagner, *Adversarial Examples Are Not Easily Detected:
    Bypassing Ten Detection Methods*, arXiv preprint arXiv:1705.07263, 2017.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] B. Liang, H. Li, M. Su, X. Li, W. Shi, X. Wang, *Detecting Adversarial
    Examples in Deep Networks with Adaptive Noise Reduction*, arXiv preprint arXiv:1705.08378,
    2017.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] A. Arnab, O. Miksik, P. H. S. Torr, *On the Robustness of Semantic Segmentation
    Models to Adversarial Attacks*, arXiv preprint arXiv:1711.09856, 2017.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] A. S. Ross, F. Doshi-Velez, *Improving the Adversarial Robustness and
    Interpretability of Deep Neural Networks by Regularizing their Input Gradients*,
    arXiv preprint arXiv:1711.09404, 2017.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Y. Song, T. Kim, S. Nowozin, S. Ermon, N. Kushman, *PixelDefend: Leveraging
    Generative Models to Understand and Defend against Adversarial Examples*, arXiv
    preprint arXiv:1710.10766, 2017.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] N. Narodytska, S. P. Kasiviswanathan, *Simple Black-Box Adversarial Perturbations
    for Deep Networks*, arXiv preprint arXiv:1612.06299, 2016.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, A. Vladu, *Towards Deep
    Learning Models Resistant to Adversarial Attacks*, arXiv preprint arXiv:1706.06083,
    2017.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Z. Sun, M. Ozay, T. Okatani, *HyperNetworks with statistical filtering
    for defending adversarial examples*, arXiv preprint arXiv:1711.01791, 2017.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] K. Grosse, P. Manoharan, N. Papernot, M. Backes, P. McDaniel, *On the
    (Statistical) Detection of Adversarial Examples*, arXiv preprint arXiv:1702.06280,
    2017.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] S. Shen, G. Jin, K. Gao, Y. Zhang, *APE-GAN: Adversarial Perturbation
    Elimination with GAN*, arXiv preprint arXiv:1707.05474, 2017.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] N. Carlini, G. Katz, C. Barrett, D. L. Dill, *Ground-Truth Adversarial
    Examples*, arXiv preprint arXiv:1709.10207, 2017.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, A. Swami,
    *The Limitations of Deep Learning in Adversarial Settings*, In Proceedings of
    IEEE European Symposium on Security and Privacy, 2016.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] N. Papernot, P. McDaniel, S. Jha, M. Fredrikson, Z. B. Celik, A. Swami,
    *深度学习在对抗环境中的局限性*，发表于IEEE欧洲安全与隐私研讨会论文集，2016年。'
- en: '[61] K. Grosse, N. Papernot, P. Manoharan, M. Backes, P. McDaniel, *Adversarial
    Perturbations Against Deep Neural Networks for Malware Classification*, arXiv
    preprint arXiv:1606.04435, 2016.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] K. Grosse, N. Papernot, P. Manoharan, M. Backes, P. McDaniel, *针对深度神经网络的对抗扰动用于恶意软件分类*，arXiv
    预印本 arXiv:1606.04435，2016年。'
- en: '[62] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, P. Abbeel, *Adversarial
    Attacks on Neural Network Policies*, arXiv preprint arXiv:1702.02284, 2017.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] S. Huang, N. Papernot, I. Goodfellow, Y. Duan, P. Abbeel, *对神经网络策略的对抗攻击*，arXiv
    预印本 arXiv:1702.02284，2017年。'
- en: '[63] M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera, F. Roli, *Is Deep
    Learning Safe for Robot Vision? Adversarial Examples against the iCub Humanoid*,
    arXiv preprint arXiv:1708.06939, 2017.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] M. Melis, A. Demontis, B. Biggio, G. Brown, G. Fumera, F. Roli, *深度学习对于机器人视觉安全吗？针对iCub类人机器人对抗样本*，arXiv
    预印本 arXiv:1708.06939，2017年。'
- en: '[64] I. Rosenberg, A. Shabtai, L. Rokach, Y. Elovici, *Generic Black-Box End-to-End
    Attack against RNNs and Other API Calls Based Malware Classifiers*, arXiv preprint
    arXiv:1707.05970, 2017.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] I. Rosenberg, A. Shabtai, L. Rokach, Y. Elovici, *针对RNN和其他API调用的通用黑箱端到端攻击*，arXiv
    预印本 arXiv:1707.05970，2017年。'
- en: '[65] A. Athalye, L. Engstrom, A. Ilyas, K. Kwok, *Synthesizing Robust Adversarial
    Examples*, arXiv preprint arXiv:1707.07397, 2017.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] A. Athalye, L. Engstrom, A. Ilyas, K. Kwok, *合成鲁棒的对抗样本*，arXiv 预印本 arXiv:1707.07397，2017年。'
- en: '[66] J. Lu, T. Issaranon, D. Forsyth, *SafetyNet: Detecting and Rejecting Adversarial
    Examples Robustly*, arXiv preprint arXiv:1704.00103, 2017.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] J. Lu, T. Issaranon, D. Forsyth, *SafetyNet：鲁棒地检测和拒绝对抗样本*，arXiv 预印本 arXiv:1704.00103，2017年。'
- en: '[67] J. H. Metzen, M. C. Kumar, T. Brox, V. Fischer *Universal Adversarial
    Perturbations Against Semantic Image Segmentation*, In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition, pp. 2755-2764, 2017\.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] J. H. Metzen, M. C. Kumar, T. Brox, V. Fischer *对语义图像分割的普遍对抗扰动*，发表于IEEE计算机视觉与模式识别大会论文集，第2755-2764页，2017年。'
- en: '[68] J. Su, D. V. Vargas, S. Kouichi, *One pixel attack for fooling deep neural
    networks*, arXiv preprint arXiv:1710.08864, 2017.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] J. Su, D. V. Vargas, S. Kouichi, *一种像素攻击方法以欺骗深度神经网络*，arXiv 预印本 arXiv:1710.08864，2017年。'
- en: '[69] A. Fawzi, S. Moosavi-Dezfooli, P. Frossard, *Robustness of classifiers:
    from adversarial to random noise*, In Advances in Neural Information Processing
    Systems, pp. 1632-1640, 2016.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] A. Fawzi, S. Moosavi-Dezfooli, P. Frossard, *分类器的鲁棒性：从对抗噪声到随机噪声*，发表于神经信息处理系统进展，第1632-1640页，2016年。'
- en: '[70] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, P. Frossard, S Soatto, *Analysis
    of universal adversarial perturbations*, arXiv preprint arXiv:1705.09554, 2017'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] S. Moosavi-Dezfooli, A. Fawzi, O. Fawzi, P. Frossard, S. Soatto, *对通用对抗扰动的分析*，arXiv
    预印本 arXiv:1705.09554，2017年。'
- en: '[71] A. Fawzi, O. Fawzi, P. Frossard, *Analysis of classifiers’ robustness
    to adversarial perturbations*, arXiv preprint arXiv:1502.02590, 2015.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] A. Fawzi, O. Fawzi, P. Frossard, *分类器对对抗扰动的鲁棒性分析*，arXiv 预印本 arXiv:1502.02590，2015年。'
- en: '[72] S. Moosavi-Dezfooli, A. Fawzi, P. Frossard, *DeepFool: a simple and accurate
    method to fool deep neural networks*, In Proceedings of the IEEE Conference on
    Computer Vision and Pattern Recognition, pp. 2574-2582, 2016.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] S. Moosavi-Dezfooli, A. Fawzi, P. Frossard, *DeepFool：一种简单而准确的欺骗深度神经网络的方法*，发表于IEEE计算机视觉与模式识别大会论文集，第2574-2582页，2016年。'
- en: '[73] C. Kanbak, SS. Moosavi-Dezfooli, P. Frossard, *Geometric robustness of
    deep networks: analysis and improvement*, arXiv preprint arXiv:1711.09115, 2017.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] C. Kanbak, SS. Moosavi-Dezfooli, P. Frossard, *深度网络的几何鲁棒性：分析与改进*，arXiv
    预印本 arXiv:1711.09115，2017年。'
- en: '[74] T. Tanay, L. Griffin, *A Boundary Tilting Persepective on the Phenomenon
    of Adversarial Examples*, arXiv preprint arXiv:1608.07690, 2016.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] T. Tanay, L. Griffin, *对抗样本现象的边界倾斜视角*，arXiv 预印本 arXiv:1608.07690，2016年。'
- en: '[75] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A.
    Rahmati, D. Song, *Robust Physical-World Attacks on Deep Learning Models*, arXiv
    preprint arXiv:1707.08945, 2017'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] I. Evtimov, K. Eykholt, E. Fernandes, T. Kohno, B. Li, A. Prakash, A.
    Rahmati, D. Song, *对深度学习模型的鲁棒物理世界攻击*，arXiv 预印本 arXiv:1707.08945，2017年。'
- en: '[76] W. He, J. Wei, X. Chen, N. Carlini, D. Song, *Adversarial Example Defenses:
    Ensembles of Weak Defenses are not Strong*, arXiv preprint arXiv:1706.04701, 2017.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] W. He, J. Wei, X. Chen, N. Carlini, D. Song, *对抗样本防御：弱防御的集合并不强*，arXiv
    预印本 arXiv:1706.04701，2017年。'
- en: '[77] F. Tramer, A. Kurakin, N. Papernot, D. Boneh, P. McDaniel, *Ensemble Adversarial
    Training: Attacks and Defenses*, arXiv preprint arXiv:1705.07204, 2017.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] J. H. Metzen, T. Genewein, V. Fischer, B. Bischoff, *On Detecting Adversarial
    Perturbations*, arXiv preprint arXiv:1702.04267, 2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] C. Xie, J. Wang, Z. Zhang, Z. Ren, A. Yuille, *Mitigating adversarial
    effects through randomization*, arXiv preprint arXiv:1711.01991, 2017.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] A. Kurakin, I. Goodfellow, S. Bengio, *Adversarial Machine Learning at
    Scale*, arXiv preprint arXiv:1611.01236, 2017.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] N. Akhtar, J. Liu, A. Mian, *Defense against Universal Adversarial Perturbations*,
    arXiv preprint arXiv:1711.05929, 2017.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] C. Guo, M. Rana, M. Cisse, L. Maaten, *Countering Adversarial Images using
    Input Transformations*, arXiv preprint arXiv:1711.00117, 2017.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] A. Galloway, G. W. Taylor, M. Moussa, *Attacking Binarized Neural Networks*,
    arXiv preprint arXiv:1711.00449, 2017.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] N. Papernot, P. McDaniel, *Extending Defensive Distillation*, arXiv preprint
    arXiv:1705.05264, 2017.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] T. Na, J. H. Ko, S. Mukhopadhyay, *Cascade Adversarial Machine Learning
    Regularized with a Unified Embedding*, arXiv preprint arXiv:1708.02582, 2017.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] R. Feinman, R. R. Curtin, S. Shintre, A. B. Gardner, *Detecting Adversarial
    Samples from Artifacts*, arXiv preprint arXiv:1703.00410, 2017.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] X. Zeng, C. Liu, Y. Wang, W. Qiu, L. Xie, Y. Tai, C. K. Tang, A. L. Yuille,
    *Adversarial Attacks Beyond the Image Space*, arXiv preprint arXiv:1711.07183,
    2017.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Y. Liu, X. Chen, C. Liu, D. Song, *Delving into Transferable Adversarial
    Examples and Black-box Attacks*, arXiv preprint arXiv:1611.02770, 2017.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] T. Strauss, M. Hanselmann, A. Junginger, H. Ulmer, *Ensemble Methods as
    a Defense to Adversarial Perturbations Against Deep Neural Networks*, arXiv preprint
    arXiv:1709.03423, 2017.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] S. Sankaranarayanan, A. Jain, R. Chellappa, S. N. Lim, *Regularizing deep
    networks using efficient layerwise adversarial training*, arXiv preprint arXiv:1705.07819,
    2017.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] E. D. Cubuk, B. Zoph, S. S. Schoenholz, Q. V. Le, *Intriguing Properties
    of Adversarial Examples*, arXiv preprint arXiv:1711.02846, 2017.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] T. Gebhart, P. Schrater, *Adversary Detection in Neural Networks via Persistent
    Homology*, arXiv preprint arXiv:1711.10056, 2017.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] J. Bradshaw, A. G. Matthews, Z. Ghahramani, *Adversarial Examples, Uncertainty,
    and Transfer Testing Robustness in Gaussian Process Hybrid Deep Networks*, arXiv
    preprint arXiv:1707.02476, 2017.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Rozsa, E. M. Rudd, T. E. Boult, *Adversarial Diversity and Hard Positive
    Generation*, arXiv preprint arXiv:1605.01775, 2016.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] O. Bastani, Y. Ioannou, L. Lampropoulos, D. Vytiniotis, A. Nori, A. Criminisi,
    *Measuring Neural Net Robustness with Constraints*, arXiv preprint arXiv:1605.07262,
    2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] J. Z. Kolter, E. Wong, *Provable defenses against adversarial examples
    via the convex outer adversarial polytope*, arXiv preprint arXiv:1711.00851, 2017.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] A. Rozsa, M. Geunther, T. E. Boult, *Are Accuracy and Robustness Correlated?*,
    In IEEE International Conference on Machine Learning and Applications, pp. 227-232,
    2016.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] H. Hosseini, B. Xiao, M. Jaiswal, R. Poovendran, *On the Limitation of
    Convolutional Neural Networks in Recognizing Negative Images*, arXiv preprint
    arXiv:1703.06857, 2017.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] M. Cisse, P. Bojanowski, E. Grave, Y. Dauphin, N. Usunier, *Parseval Networks:
    Improving Robustness to Adversarial Examples*, arXiv preprint arXiv:1704.08847,
    2017.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] N. Cheney, M. Schrimpf, G. Kreiman, *On the Robustness of Convolutional
    Neural Networks to Internal Architecture and Weight Perturbations*, arXiv preprint
    arXiv:1703.08245, 2017.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] H. Lee, S. Han, J. Lee, *Generative Adversarial Trainer: Defense to Adversarial
    Perturbations with GAN*, arXiv preprint arXiv:1705.03387, 2017.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] A. Rozsa, M. Gunther, T. E. Boult, *Towards Robust Deep Neural Networks
    with BANG*, arXiv preprint arXiv:1612.00138, 2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] T. Miyato, S. Maeda, M. Koyama, S. Ishii, *Virtual Adversarial Training:
    a Regularization Method for Supervised and Semi-supervised Learning*, arXiv preprint
    1704.03976, 2017.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] D. Arpit, S. Jastrzebski, N. Ballas, D. Krueger, E. Bengio, M. S. Kanwal,
    T. Maharaj, A. Fischer, A. Courville, Y. Bengio, S. Lacoste-Julien, *A Closer
    Look at Memorization in Deep Networks*, arXiv preprint arXiv:1706.05394, 2017.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] X. Li, F. Li, *Adversarial Examples Detection in Deep Networks with Convolutional
    Filter Statistics*, In Proceedings of International Conference on Computer Vision,
    2017.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] T. Lee, M. Choi, S. Yoon, *Manifold Regularized Deep Neural Networks
    using Adversarial Examples*, arXiv preprint arXiv:1511.06381, 2016.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] K. Grosse, N. Papernot, P. Manoharan, M. Backes, and P. McDaniel, *Adversarial
    examples for malware detection*, In European Symposium on Research in Computer
    Security, pp. 62-79\. 2017.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] N. Papernot, and P. McDaniel, *On the effectiveness of defensive distillation*,
    arXiv preprint arXiv:1607.05113, 2016.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] N. Papernot, Patrick McDaniel, and Ian Goodfellow, *Transferability in
    machine learning: from phenomena to black-box attacks using adversarial samples*,
    arXiv preprint arXiv:1605.07277, 2016.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] N. Papernot, P. McDaniel, A. Swami, and R. Harang, *Crafting adversarial
    input sequences for recurrent neural networks*, In IEEE Military Communications
    Conference, pp. 49-54, 2016.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] N. Papernot, I. Goodfellow, R. Sheatsley, R. Feinman, and P. McDaniel.
    *Cleverhans v1\. 0.0: an adversarial machine learning library*, arXiv preprint
    arXiv:1610.00768, 2016.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] I. Goodfellow, N. Papernot, and P. McDaniel, *cleverhans v0\. 1: an adversarial
    machine learning library*, arXiv preprint arXiv:1610.00768, 2016.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] T. Miyato, A. M. Dai, and Ian Goodfellow, *Adversarial Training Methods
    for Semi-Supervised Text Classification*, arXiv preprint arXiv:1605.07725, 2016.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] A. Nguyen, J. Yosinski, and J. Clune, *Deep neural networks are easily
    fooled: High confidence predictions for unrecognizable images*, In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 427-436,
    2015.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] C. Xie, J. Wang, Z. Zhang, Y. Zhou, L. Xie, and A. Yuille, *Adversarial
    Examples for Semantic Segmentation and Object Detection*, arXiv preprint arXiv:1703.08603,
    2017.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] S. Zheng, Y. Song, T. Leung, and I. Goodfellow, *Improving the robustness
    of deep neural networks via stability training*, In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 4480-4488, 2016.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] P. Tabacof, and E. Valle, *Exploring the space of adversarial images*,
    In IEEE International Joint Conference on Neural Networks, pp. 426-433, 2016.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] A. Fawzi, O. Fawzi, and P. Frossard, *Fundamental limits on adversarial
    robustness*, In Proceedings of ICML, Workshop on Deep Learning. 2015.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Y. Luo, Xavier Boix, Gemma Roig, Tomaso Poggio, and Qi Zhao, *Foveation-based
    mechanisms alleviate adversarial examples*, arXiv preprint arXiv:1511.06292, 2015.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Jin, A. Dundar, and E. Culurciello, *Robust convolutional neural networks
    under adversarial noise*, arXiv preprint arXiv:1511.06306, 2015.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] J. Kos, I. Fischer, and D. Song, *Adversarial examples for generative
    models*, arXiv preprint arXiv:1702.06832, 2017.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Q. Wang, W. Guo, K. Zhang, I. I. Ororbia, G. Alexander, X. Xing, C. L.
    Giles, and X Liu, *Adversary Resistant Deep Neural Networks with an Application
    to Malware Detection*, arXiv preprint arXiv:1610.01239, 2016.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] G. K. Dziugaite, Z. Ghahramani, and D. M. Roy, *A study of the effect
    of JPG compression on adversarial images*, arXiv preprint arXiv:1608.00853, 2016.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] A. Nayebi, and S. Ganguli, *Biologically inspired protection of deep
    networks from adversarial attacks*, arXiv preprint arXiv:1703.09202, 2017.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] W. Hu, and Y. Tan, *Generating Adversarial Malware Examples for Black-Box
    Attacks Based on GAN*, arXiv preprint arXiv:1702.05983, 2017.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] G. Katz, C. Barrett, D. L. Dill, K. Julian, and M. J. Kochenderfer, *Towards
    proving the adversarial robustness of deep neural networks*, arXiv preprint arXiv:1709.02802,
    2017.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] D. Krotov, and J. J. Hopfield. *Dense Associative Memory is Robust to
    Adversarial Inputs*, arXiv preprint arXiv:1701.00939, 2017.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] P. Tabacof, T. Julia, E. Valle, *Adversarial images for variational autoencoders*,
    arXiv preprint arXiv:1612.00155, 2016.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Q. Wang, W. Guo, I. I. Ororbia, G. Alexander, X. Xing, L. Lin, C. L.
    Giles, X. Liu, P. Liu, and G. Xiong, *Using non-invertible data transformations
    to build adversary-resistant deep neural networks*, arXiv preprint arXiv:1610.01934,
    2016.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] A. Rozsa, M. Geunther, E. M. Rudd, and T. E. Boult. ”Facial attributes:
    Accuracy and adversarial robustness.” Pattern Recognition Letters (2017).'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] M. Cisse, Y. Adi, N. Neverova, and J. Keshet, *Houdini: Fooling deep
    structured prediction models*, arXiv preprint arXiv:1707.05373, 2017.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] F. Tramer, N. Papernot, I. Goodfellow, D. Boneh, and P. McDaniel, *The
    Space of Transferable Adversarial Examples*, arXiv preprint arXiv:1704.03453,
    2017.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] S. J. Oh, M. Fritz, and B. Schiele, *Adversarial Image Perturbation for
    Privacy Protection–A Game Theory Perspective*, arXiv preprint arXiv:1703.09471,
    2017.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Y. Lin, Z. Hong, Y. Liao, M. Shih, M. Liu, and M. Sun, *Tactics of Adversarial
    Attack on Deep Reinforcement Learning Agents*, arXiv preprint arXiv:1703.06748,
    2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] K. R. Mopuri, U. Garg, and R. V. Babu, *Fast Feature Fool: A data independent
    approach to universal adversarial perturbations*, arXiv preprint arXiv:1707.05572,
    2017.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] N. Kardan, and K. O. Stanley, *Mitigating fooling with competitive overcomplete
    output layer neural networks*, In International Joint Conference on Neural Networks
    pp. 518-525, 2017.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Y. Dong, H. Su, J. Zhu, and F. Bao, *Towards Interpretable Deep Neural
    Networks by Leveraging Adversarial Examples*, arXiv preprint arXiv:1708.05493,
    2017.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] Q. Wang, W. Guo, K. Zhang, I. I. Ororbia, G. Alexander, X. Xing, C. L.
    Giles, and X. Liu, *Learning Adversary-Resistant Deep Neural Networks*, arXiv
    preprint arXiv:1612.01401, 2016.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] J. Gao, B. Wang, Z. Lin, W. Xu, and Y. Qi, *DeepCloak: Masking Deep Neural
    Network Models for Robustness Against Adversarial Samples*, (2017).'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] W. Xu, D. Evans, and Y. Qi, *Feature Squeezing Mitigates and Detects
    Carlini/Wagner Adversarial Examples*, arXiv preprint arXiv:1705.10686, 2017.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] W. Bai, C. Quan, and Z. Luo, *Alleviating adversarial attacks via convolutional
    autoencoder*, In International Conference on Software Engineering, Artificial
    Intelligence, Networking and Parallel/Distributed Computing (SNPD), pp. 53-58,
    2017.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] A. P. Norton, Y. Qi, *Adversarial-Playground: A visualization suite showing
    how adversarial examples fool deep learning*, In IEEE Symposium on Visualization
    for Cyber Security, pp. 1-4, 2017.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] Y. Dong, F. Liao, T. Pang, X. Hu, and J. Zhu, *Discovering Adversarial
    Examples with Momentum*, arXiv preprint arXiv:1710.06081, 2017.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] S. Shen, R. Furuta, T. Yamasaki, and K. Aizawa, *Fooling Neural Networks
    in Face Attractiveness Evaluation: Adversarial Examples with High Attractiveness
    Score But Low Subjective Score*, In IEEE Third International Conference on Multimedia
    Big Data, pp. 66-69, 2017.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] C. Szegedy, V. Vincent, S. Ioffe, J. Shlens, and Z. Wojna. *Rethinking
    the inception architecture for computer vision*, In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pp. 2818-282, 2016.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] S. Sarkar, A. Bansal, U. Mahbub, and R. Chellappa, *UPSET and ANGRI:
    Breaking High Performance Image Classifiers*, arXiv preprint arXiv:1707.01159,
    2017.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] K. He, X. Zhang, S. Ren, and J. Sun, *Deep residual learning for image
    recognition*, In Proceedings of the IEEE conference on computer vision and pattern
    recognition, pp. 770-778, 2016.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] S. Das, and P. N. Suganthan, *Differential evolution: A survey of the
    state-of-the-art*, IEEE transactions on evolutionary computation vol. 15, no.
    1, pp. 4-31, 2011.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] J. Redmon and A. Farhadi. *Yolo9000: better, faster, stronger*, arXiv
    preprint arXiv:1612.08242, 2016.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] S. Ren, K. He, R. Girshick, and J. Sun, *Faster r-cnn: Towards real-time
    object detection with region proposal networks*, In Advances in neural information
    processing systems, pages 91-99, 2015.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] D. Amodei, R. Anubhai, E. Battenberg, C. Case, J. Casper, B. Catanzaro,
    J. Chen, M. Chrzanowski, A. Coates, G. Diamos. *Deep speech 2: End-to-end speech
    recognition in English and Mandarin*, arXiv preprint arXiv:1512.02595, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] A. Krizhevsky, *Learning multiple layers of features from tiny image*,
    2009.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] Diederik P Kingma and Max Welling, *Auto-encoding variational bayes*,
    arXiv preprint arXiv:1312.6114, 2013.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] Y. Bengio, *Learning deep architectures for AI*, Foundations and trends
    in Machine Learning vol. 2, no. 1, pp. 1-127, 2009.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, *Learning representations
    by back-propagating errors,* Cognitive modeling, vol. 5, 1988.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] S. Hochreiter and J. Schmidhuber, *Long short-term memory*, Neural computation,
    vol. 9, no. 8, pp. 1735-1780, 1997.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] M. Volodymyr, K. Kavukcuoglu, D. Silver, A. A. Rusu, J. Veness, M. G.
    Bellemare, A. Graves, M. Riedmiller, A. K. Fidjeland, G. Ostrovski, *Human-level
    control through deep reinforcement learning*, Nature, vol. 518, no. 7540, pp.
    529-533, 2015.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] M. Volodymyr, A. P. Badia, and M. Mirza, *Asynchronous methods for deep
    reinforcement learning* In International Conference on Machine Learning, 2016.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] J. Long, E. Shelhamer, and T. Darrell, *Fully convolutional networks
    for semantic segmentation*, In Proceedings of IEEE Conference on Computer Vision
    and Pattern Recognition, 2015.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Rozsa,M. Gunther, E. M. Rudd, and T. E. Boult, *Are facial attributes
    adversarially robust?* In International Conference on Pattern Recognition, pp.
    3121-3127, 2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] Z. Liu, P. Luo, X. Wang, X. Tang, *Deep learning face attributes in the
    wild*, International Conference on Computer Vision, pp. 3730-3738, 2015.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] V. Mirjalili, and A. Ross, *Soft Biometric Privacy: Retaining Biometric
    Utility of Face Images while Perturbing Gender*, In International Joint Conference
    on Biometrics, 2017.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] K. Simonyan and A. Zisserman, *Very deep convolutional networks for large-scale
    image recognition*, in Proceedings of the International Conference on Learning
    Representations, 2015.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] D. Krotov, and J.J. Hopfield, *Dense Associative Memory for Pattern Recognition*,
    In Advances in Neural Information Processing Systems, 2016.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] R. Hahnloser, R. Sarpeshkar, M. A. Mahowald, R. J. Douglas, H.S. Seung,
    *Digital selection and analogue amplification coexist in a cortex-inspired silicon
    circuit*, Nature, vol. 405\. pp. 947-951.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] G. Hinton, O. Vinyals, and J. Dean, *Distilling the knowledge in a neural
    network*, in Deep Learning and Representation Learning Workshop at NIPS 2014\.
    arXiv preprint arXiv:1503.02531, 2014.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] H. Drucker, Y.Le Cun, *Improving generalization performance using double
    backpropagation*, IEEE Transactions on Neural Networks vol. 3, no. 6, pp. 991-997,
    1992.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] G. Huang, Z. Liu, K. Q. Weinberger, and L. Maaten, *Densely connected
    convolutional networks*, arXiv preprint arXiv:1608.06993, 2016.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] A. N. Bhagoji, D. Cullina, C. Sitawarin, P. Mittal, *Enhancing Robustness
    of Machine Learning Systems via Data Transformations*, arXiv preprint arXiv:1704.02654,
    2017.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] Y. Dong, F. Liao, T. Pang, H. Su, X. Hu, J. Li, J. Zhu, *Boosting Adversarial
    Attacks with Momentum*, arXiv preprint arXiv:1710.06081, 2017.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] I. Goodfellow, J. P. Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, *Generative adversarial nets*, In Advances in neural
    information processing systems, pp. 2672-2680, 2014.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] K. Jelena and C. Amina, *An introduction to frames*. Foundations and
    Trends in Signal Processing, 2008.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] S. Rifai, P. Vincent, X. Muller, X. Glorot, and Y. Bengio, *Contractive
    auto-encoders: Explicit invariance during feature extraction*, In Proceedings
    of International Conference on Machine Learning, pp. 833 - 840, 2011.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. S. Liew, M. Khalil-Hani, and R. Bakhteri, *Bounded activation functions
    for enhanced training stability of deep neural networks on visual pattern recognition
    problems*, Neurocomputing, vol. 216, pp. 718-734, 2016.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] M. Abbasi, C. Gagne, *Robustness to adversarial examples through an ensemble
    of specialists*, arXiv preprint arXiv:1702.06856, 2017.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] A. Mogelmose, M. M. Trivedi, and T. B. Moeslund, *Vision-based traffic
    sign detection and analysis for intelligent driver assistance systems: Perspectives
    and survey*, In IEEE Transaction on Intelligent Transportation Systems, vol. 13,
    no. 4, pp. 1484-1497, 2012.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] A. Vedaldi and K. Lenc, *MatConvNet – Convolutional Neural Networks for
    MATLAB*, In Proceeding of the ACM International Conference on Multimedia, 2015.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] J. Yangqing, E. Shelhamer, J. Donahue, S. Karayev, J. Long, R. Girshick,
    S. Guadarrama, T. Darrell, *Caffe: Convolutional Architecture for Fast Feature
    Embedding*, arXiv preprint arXiv:1408.5093, 2014.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] M. Abadi, A. Agarwal, P. Barham, E. Brevdo, Z. Chen, C. Citro, G. S.
    Corrado, A. Davis, J. Dean, M. Devin, S. Ghemawat, I. Goodfellow, A. Harp, G.
    Irving, M. Isard, R. Jozefowicz, Y. Jia, L. Kaiser, M. Kudlur, J. Levenberg, D.
    Mane, M Schuster, R. Monga, S. Moore, D. Murray, C. Olah, J. Shlens, B. Steiner,
    I. Sutskever, K. Talwar, P. Tucker, V. Vanhoucke, V. Vasudevan, F. Viegas, O.
    Vinyals, P. Warden, M. Wattenberg, M. Wicke, Y. Yu, and X. Zheng, *TensorFlow:
    Large-scale machine learning on heterogeneous systems*, 2015\. Software available
    from tensorflow.org.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] A. Giusti, J. Guzzi, D. C. Ciresan, F. He, J. P. Rodriguez, F. Fontana,
    M. Faessler, *A machine learning approach to visual perception of forest trails
    for mobile robots*, IEEE Robotics and Automation Letters, vol. 1, no. 2, pp. 661
    - 667, 2016.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Objects Detection Machine Learning TensorFlow Demo, [https://play.google.com/store/apps/details?id=org.tensorflow.detect&hl=en](https://play.google.com/store/apps/details?id=org.tensorflow.detect&hl=en),
    Accessed December 2017.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Class central, *Deep Learning for Self-Driving Cars*, [https://www.class-central.com/mooc/8132/6-s094-deep-learning-for-self-driving-cars](https://www.class-central.com/mooc/8132/6-s094-deep-learning-for-self-driving-cars),
    Accessed December 2017.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] C. Middlehurst, *China unveils world’s first facial recognition ATM*,
    [http://www.telegraph.co.uk/news/worldnews/asia/china/11643314/China-unveils-worlds-first-facial-recognition-ATM.html](http://www.telegraph.co.uk/news/worldnews/asia/china/11643314/China-unveils-worlds-first-facial-recognition-ATM.html),
    2015.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] About Face ID advanced technology, [https://support.apple.com/en-au/HT208108](https://support.apple.com/en-au/HT208108),
    Accessed December 2017.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] M. Sharif, S. Bhagavatula, L. Bauer, and M. K. Reiter, *Accessorize to
    a crime: Real and stealthy attacks on state-of-the-art face recognition*, In Proceedings
    of ACM SIGSAC Conference on Computer and Communications Security, pp. 1528-1540,
    2016.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] R. Shin and D. Song, *JPEG-resistant adversarial images*, In MAchine
    LEarning and Computer Security Workshop, 2017.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] W. Brendel and M. Bethge *Comment on ”Biologically inspired protection
    of deep networks from adversarial attacks”*, arXiv preprint arXiv:1704.01547,
    2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] N. Carlini, D. Wagner, *MagNet and ”Efficient Defenses Against Adversarial
    Attacks” are Not Robust to Adversarial Examples*, arXiv preprint arXiv:1711.08478,
    2017.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] A. Raghunathan, J. Steinhardt, P. Liang, *Certified Defenses against
    Adversarial Examples*, arXiv preprint arXiv:1801.09344\. 2018.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] V. Khrulkov and I. Oseledets, *Art of singular vectors and universal
    adversarial perturbations*, arXiv preprint arXiv:1709.03582, 2017.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] X. Huang, M. Kwiatkowska, S. Wang, M. Wu, *Safety Verification of Deep
    Neural Networks*, In 29th International Conference on Computer Aided Verification,
    pages 3-29, 2017.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] M. Wicker, X. Huang, and M. Kwiatkowska, *Feature-Guided Black-Box Safety
    Testing of Deep Neural Networks*, In 24th International Conference on Tools and
    Algorithms for the Construction and Analysis of Systems, 2018.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] K. R. Mopuri, U. Ojha, U. Garg, V. Babu, *NAG: Network for Adversary
    Generation*, arXiv preprint arXiv:1712.03390, 2017.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] A. G. Ororbia II, C. L. Giles and D Kifer, *Unifying Adversarial Training
    Algorithms with Flexible Deep Data Gradient Regularization*, arXiv preprint arXiv:1601.07213,
    2016.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] Y. Yoo, S. Park, J. Choi, S. Yun, N. Kwak, *Butterfly Effect: Bidirectional
    Control of Classification Performance by Small Additive Perturbation*, arXiv preprint
    arXiv:1711.09681, 2017.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/107c4671c54048f865e382e829fc4892.png) | Naveed
    Akhtar received his PhD in Computer Vision from The University of Western Australia
    (UWA) and Master degree in Computer Science from Hochschule Bonn-Rhein-Sieg, Germany
    (HBRS). His research in Computer Vision and Pattern Recognition has been published
    in prestigious venues of the field, including IEEE CVPR and IEEE TPAMI. He has
    also served as a reviewer for these venues. During his PhD, he was recipient of
    multiple scholarships, and runner-up for the Canon Extreme Imaging Competition
    in 2015. Currently, he is a Research Fellow at UWA since July 2017\. Previously,
    he has also served on the same position at the Australian National University
    for one year. His current research interests include adversarial machine learning,
    action recognition and hyperspectral image analysis. |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/26bb29c1831543e1cf9a814f6897655b.png) | Ajmal
    Mian completed his PhD from The University of Western Australia in 2006 with distinction
    and received the Australasian Distinguished Doctoral Dissertation Award from Computing
    Research and Education Association of Australasia. He received the prestigious
    Australian Postdoctoral and Australian Research Fellowships in 2008 and 2011 respectively.
    He received the UWA Outstanding Young Investigator Award in 2011, the West Australian
    Early Career Scientist of the Year award in 2012 and the Vice-Chancellors Mid-Career
    Research Award in 2014\. He has secured seven Australian Research Council grants
    and one National Health and Medical Research Council grant with a total funding
    of over $3 Million. He is currently in the School of Computer Science and Software
    Engineering at The University of Western Australia and is a guest editor of Pattern
    Recognition, Computer Vision and Image Understanding and Image and Vision Computing
    journals. His research interests include computer vision, machine learning, 3D
    shape analysis, hyperspectral image analysis, pattern recognition, and multimodal
    biometrics. |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
