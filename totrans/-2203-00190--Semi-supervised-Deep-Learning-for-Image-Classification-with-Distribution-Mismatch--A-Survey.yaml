- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:47:45'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2203.00190] Semi-supervised Deep Learning for Image Classification with Distribution
    Mismatch: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2203.00190](https://ar5iv.labs.arxiv.org/html/2203.00190)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Saul Calderon-Ramirez, Shengxiang Yang,  and David Elizondo Manuscript received
    07 February 2022.S. Calderon-Ramirez, S. Yang, and D. Elizondo are with the Institute
    of Artificial Intelligence (IAI), De Montfort University, Leicester LE1 9BH, United
    Kingdom (e-mail: sacalderon@itcr.ac.cr, syang@dmu.ac.uk, elizondo@dmu.ac.uk).'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning methodologies have been employed in several different fields,
    with an outstanding success in image recognition applications, such as material
    quality control, medical imaging, autonomous driving, etc. Deep learning models
    rely on the abundance of labelled observations to train a prospective model. These
    models are composed of millions of parameters to estimate, increasing the need
    of more training observations. Frequently it is expensive to gather labelled observations
    of data, making the usage of deep learning models not ideal, as the model might
    over-fit data. In a semi-supervised setting, unlabelled data is used to improve
    the levels of accuracy and generalization of a model with small labelled datasets.
    Nevertheless, in many situations different unlabelled data sources might be available.
    This raises the risk of a significant distribution mismatch between the labelled
    and unlabelled datasets. Such phenomena can cause a considerable performance hit
    to typical semi-supervised deep learning frameworks, which often assume that both
    labelled and unlabelled datasets are drawn from similar distributions. Therefore,
    in this paper we study the latest approaches for semi-supervised deep learning
    for image recognition. Emphasis is made in semi-supervised deep learning models
    designed to deal with a distribution mismatch between the labelled and unlabelled
    datasets. We address open challenges with the aim to encourage the community to
    tackle them, and overcome the high data demand of traditional deep learning pipelines
    under real-world usage settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Impact statement: This paper is a deep review of the state of the art semi-supervised
    deep learning methods, focusing on methods dealing with the distribution mismatch
    setting. Under real world usage scenarios, a distribution mismatch might occur
    between the labelled and unlabelled datasets. Recent research has found an important
    performance degradation of the state of the art semi-supervised deep learning
    (SSDL) methods. Therefore, state of the art methodologies aim to increase the
    robustness of semi-supervised deep learning frameworks to this phenomena. In this
    work, we are the first to our knowledge to systematize and study recent approaches
    to robust SSDL under distribution mismatch scenarios. We think this work can add
    value to the literature around this subject, as it identifies the main tendencies
    surrounding it. Also we consider that our work encourages the community to draw
    the attention on this emerging subject, which we think is an important challenge
    to address in order to decrease the lab-to-real-world gap of deep learning methodologies.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning, image classification, Semi-supervised learning, Distribution
    mismatch
  prefs: []
  type: TYPE_NORMAL
- en: I Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning based approaches continue to provide more accurate results in
    a wide variety of fields, from medicine to biodiversity conservation [[10](#bib.bib10),
    [33](#bib.bib33), [14](#bib.bib14), [69](#bib.bib69), [97](#bib.bib97), [7](#bib.bib7),
    [62](#bib.bib62), [17](#bib.bib17), [12](#bib.bib12)]. Most of deep learning architectures
    rely on the usage of extensively labelled datasets to train models with millions
    of parameters to estimate [[35](#bib.bib35), [16](#bib.bib16), [13](#bib.bib13)].
    Over-fitting is a frequent issue when implementing a deep learning based solution
    trained with a small, or not representative dataset. Such phenomena often causes
    poor generalization performance during its real world usage. In spite of this
    risk, the acquisition of a sufficiently sized and representative sample, through
    rigorous procedures and standards, is a pending challenge, as argued in [[5](#bib.bib5)].
    Moreover, procedures to determine whether a dataset is large and/or representative
    enough is still an open subject in the literature, as discussed in [[58](#bib.bib58)].
  prefs: []
  type: TYPE_NORMAL
- en: Often labels are expensive to generate, especially in fields developed by highly
    trained medical professionals, such as radiologists, pathologists, or psychologists
    [[17](#bib.bib17), [30](#bib.bib30), [10](#bib.bib10), [42](#bib.bib42)]. Examples
    of this include the labelling of hystopathological images, necessary for training
    a deep learning model for its usage in clinical procedures [[17](#bib.bib17)].
    Therefore, there is an increasing interest for dealing with scarce labelled data
    to feed deep learning architectures, stimulated by the success of deep learning
    based models [[63](#bib.bib63)].
  prefs: []
  type: TYPE_NORMAL
- en: Among the most popular and simple approaches to deal with limited labelled observations
    and diminish model over-fitting is data augmentation. Data augmentation adds artificial
    observations to the training dataset, using simple transformations of real data
    samples; namely image rotation, flipping, artificial noise addition [[35](#bib.bib35)].
    A description of simple data augmentation procedures for deep learning architectures
    can be found in [[96](#bib.bib96)]. More complex data augmentation techniques
    make use of generative adversarial networks. Generative models approximate the
    data distribution, which can be sampled to create new observations, as seen in
    [[78](#bib.bib78), [102](#bib.bib102), [32](#bib.bib32)] with different applications.
    Data augmentation is implemented in popular deep learning frameworks, such as
    *Pytorch* and *TensorFlow* [[64](#bib.bib64)].
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is also a common approach for dealing with the lack of enough
    labels. It first trains a model $f$ with an external or source-labelled dataset,
    hopefully from a similar domain. Secondly, parameters are fine-tuned with the
    intended, or target dataset [[88](#bib.bib88)]. Similar to data augmentation,
    *TensorFlow* and *Pytorch* include the weights of widely used deep learning models
    trained in general purpose datasets as ImageNet [[27](#bib.bib27)], making its
    usage widespread. Its implementation yields better results with more similar source
    and target datasets. A detailed review on deep transfer learning can be found
    in [[84](#bib.bib84)].
  prefs: []
  type: TYPE_NORMAL
- en: Another alternative to deal with small labelled datasets is Semi-supervised
    Deep Learning (SSDL) which enables the model to take advantage of unlabelled or
    even noisily-labelled data [[94](#bib.bib94), [47](#bib.bib47)]. As an application
    example, take the problem of training a face based apparent emotion recognition
    model. Unlabelled videos and images of human faces are available on the web, and
    can be fetched with a web crawler. Taking advantage of such unlabelled information
    might yield improved accuracy and generalization for deep learning architectures.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the first works in the literature regarding semi-supervised learning
    is [[76](#bib.bib76)]; where different methods for using unlabelled data were
    proposed. More recently, with the increasing development and usage of deep learning
    architectures, semi-supervised learning methods are attracting more attention.
    An important number of SSDL frameworks are general enough to allow the usage of
    popular deep learning architectures in different application domains [[63](#bib.bib63)].
    Therefore, we argue that it is necessary to review and study the relationship
    between recent deep learning based semi-supervised techniques, in order to spot
    missing gaps and boost research in the field. Some recent semi-supervised learning
    reviews are already available in the literature. These are detailed in Section
    [I-A](#S1.SS1 "I-A Previous work ‣ I Introduction ‣ Semi-supervised Deep Learning
    for Image Classification with Distribution Mismatch: A Survey"). Moreover, we
    argue that it is important to discuss the open challenges of implementing SSDL
    in real-world settings, to narrow the lab-application gap. One of the remaining
    challenges is the frequent distribution mismatch between the labelled and unlabelled
    data, which can hinder the performance of the SSDL framework.'
  prefs: []
  type: TYPE_NORMAL
- en: I-A Previous work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [[101](#bib.bib101)], an extensive review on semi-supervised approaches
    for machine learning was developed. The authors defined the following semi-supervised
    approaches: self-training, co-training, and graph based methods. However, no deep
    learning based concepts were popular by the time of the survey, as auto-encoder
    and generative adversarial networks were less used, given its high computational
    cost and its consequent impractical usage.'
  prefs: []
  type: TYPE_NORMAL
- en: Later, a review of semi-supervised learning methods was developed in [[66](#bib.bib66)].
    In this work, authors enlist self-training, co-training, transductive support
    vector machines, multi-view learning and generative discriminative approaches.
    Still deep learning architectures were not popular by the time. Thus, semi-supervised
    architectures based on more traditional machine learning methods are reviewed
    in such work.
  prefs: []
  type: TYPE_NORMAL
- en: 'A brief survey in semi-supervised learning for image analysis and natural language
    processing was developed in [[67](#bib.bib67)]. The study defines the following
    semi-supervised learning approaches: generative models, self-training, co-training,
    multi-view learning and graph based models. This review, however, does not focus
    on deep semi-supervised learning approaches.'
  prefs: []
  type: TYPE_NORMAL
- en: A more recent survey on semi-supervised learning for medical imaging can be
    found in [[21](#bib.bib21)], with different machine learning based approaches
    listed. Authors distinguished self-training, graph based, co-training, and manifold
    regularization approaches for semi-supervised learning. More medical imaging solutions
    based on transfer learning than semi-supervised learning were found by the authors,
    given its simplicity of implementation.
  prefs: []
  type: TYPE_NORMAL
- en: In [[63](#bib.bib63)], authors experimented with some recent SSDL architectures
    and included a short review. The authors argued that typical testing of semi-supervised
    techniques is not enough to measure its performance in real-world applications.
    For instance, common semi-supervised learning benchmarks do not include unlabelled
    datasets with observations from classes not defined in the labelled data. This
    is referred to as distractor classes or collective outliers [[79](#bib.bib79)].
    The authors also highlight the lack of tests around the interaction of semi-supervised
    learning pipelines with other types of learning, namely transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'More recently, in [[92](#bib.bib92)], the authors extensively review different
    semi-supervised learning frameworks, mostly for deep learning architectures. A
    detailed concept framework around the key assumptions of most SSDL (low density/clustering
    and the manifold assumptions) is developed. The taxonomy proposed for semi-supervised
    methods include two major categories: inductive and transductive based methods.
    Inductive methods build a mathematical model or function that can be used for
    new points in the input space, while transductive methods do not. According to
    the authors, significantly more semi-supervised inductive methods can be found
    in the literature. These methods can be further categorized into: unsupervised
    pre-processing, wrapper based, and intrinsically semi-supervised methods [[92](#bib.bib92)].
    The authors mentioned the distribution mismatch challenge for semi-supervised
    learning introduced in [[63](#bib.bib63)]. However, no focus on techniques around
    this subject was done in their review.'
  prefs: []
  type: TYPE_NORMAL
- en: In [[74](#bib.bib74)], a review of semi-, self- and unsupervised learning methods
    for image classification was developed. The authors focus on the common concepts
    used in these methods (most of them based on deep learning architectures). Concepts
    such as pretext or proxy task learning, data augmentation, contrastive optimization,
    etc., are described as common ideas within the three learning approaches. A useful
    set of tables describing the different semi-supervised learning approaches along
    with the common concepts is included in this work. After reviewing the yielded
    results of recent semi-supervised methods, the authors conclude that few of them
    include benchmarks closer to real-world (high resolution images, with similar
    features for each class). Also, real-world settings, such as class imbalance and
    noisy labels are often missing.
  prefs: []
  type: TYPE_NORMAL
- en: We argue that a detailed survey in SSDL is still missing, as common short reviews
    included in SSDL papers usually focus on its closest related works. The most recent
    semi-supervised learning surveys we have found are outdated and do not focus on
    deep learning based approaches. We argue that recent SSDL approaches add new perspectives
    to the semi-supervised learning framework. However, more importantly, to narrow
    the lab-to-application gap, it is necessary to fully study the state of the art
    in the efforts to address such challenges. In the context of SSDL, we consider
    that increasing the robustness of SSDL methods to the distribution mismatch between
    the labelled and unlabelled datasets is key. Therefore, this review focuses on
    the distribution mismatch problem between the labelled and the unlabelled datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Section [I-B](#S1.SS2 "I-B Semi-supervised learning ‣ I Introduction ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey"),
    a review of the main ideas of semi-supervised learning is carried out. Based on
    the concepts of both previous sections, in Section [II](#S2 "II Semi-supervised
    Deep Learning ‣ Semi-supervised Deep Learning for Image Classification with Distribution
    Mismatch: A Survey") we review the main approaches for SSDL. Later we address
    the different methods developed so far in the literature regarding SSDL when facing
    a distribution mismatch between $S^{(u)}$ and $S^{(l)}$. Finally, we discuss the
    pending challenges of SSDL under distribution mismatch conditions in Section [IV](#S4
    "IV Open challenges ‣ Semi-supervised Deep Learning for Image Classification with
    Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: I-B Semi-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section we describe the key terminology to analyze SSDL in more detail.
    We based our terminology on the semi-supervised learning analytical framework
    developed in [[4](#bib.bib4)]. This framework extends the learning framework proposed
    in [[90](#bib.bib90)], as a machine learning theoretical framework.
  prefs: []
  type: TYPE_NORMAL
- en: A model $f_{\textbf{w}}$ is said to be semi-supervised, if it is trained using
    a set of labelled observations $S^{(l)}$, along a set of unlabelled observations
    $S^{(u)}=\left\{\mathbf{x}_{n_{1}},\mathbf{x}_{n_{2}},\ldots,\mathbf{x}_{n_{u}}\right\},$
    with the total number of observations $n=n_{l}+n_{u}$. Frequently, the number
    of unlabelled observations $n_{u}$ is considerably higher than the number of labelled
    observations. This makes $n_{u}\gg n_{l}$, as labels are expensive to obtain in
    different domains. If the model $f_{\textbf{w}}$ corresponds to a Deep Neural
    Network (DNN), we refer to SSDL. The deep model $f_{\textbf{w}}$ is often referred
    to as a back-bone model. In semi-supervised learning, additional information is
    extracted from an unlabelled dataset $S^{(u)}$. Therefore, training a deep model
    can be extended to $f_{\mathbf{w}}=T\left(S^{(l)},S^{(u)},f_{\mathbf{w}}\right)$.
    The estimated hypothesis should classify test data in $\mathbf{x}\in S^{(t)}$with
    a higher accuracy than just using the labelled data $S^{(l)}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 1: Semi-supervised setting, circles represent the unlabelled observations
    $S^{(u)}$, the filled shapes correspond to labelled observations $S^{(l)}$ of
    $K=2$ classes, and the yellow circles correspond to unlabelled observations or
    members of the distractor class.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ I-B Semi-supervised learning ‣ I Introduction
    ‣ Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey") plots a semi-supervised setting with observations in $d=2$ dimensions.
    The label can also correspond to an array, $\mathbf{y}_{i}\in\mathbb{R}^{k}$,
    in case of using a $1-K$ encoding (one-hot vector) for classifying observations
    in $K$ classes, or $y_{i}\in\mathbb{R}$ for regression. More specifically, observations
    of both the labelled and unlabelled dataset belong to the observation space $\mathbf{x}_{i}\in\mathcal{X}$
    and labels lie within the label space $\mathcal{Y}$. For instance, observation
    for binary images of written digits with $d$ pixels would make up for an observation
    space $\mathcal{X}\in\{0,1\}^{d}$, and its label set is given as $\mathcal{Y}=\left\{0,1,\ldots,9\right\}$.'
  prefs: []
  type: TYPE_NORMAL
- en: The concept class $\mathcal{C}_{k}$ corresponds to all the valid combinations
    of values in the array $\mathbf{x}_{i}\in\mathbb{R}^{d}$ for a specific class
    $k$. For example, for the digit $1$, a subset of all possible of observations
    that belong to class $k$ belong to the concept $\mathcal{C}_{k}$. The concept
    class models all the possible images of the digit $1$. In such case $\mathbf{x}_{i}\in\mathcal{C}_{k=1}$.
    The concept class $\mathcal{C}=\left\{\mathcal{C}_{1},\ldots,\mathcal{C}_{k}\right\}$
    includes all the possible observations which can be drawn for all the existing
    classes in a given problem domain.
  prefs: []
  type: TYPE_NORMAL
- en: From a data distribution perspective, usually the population density function
    of the concept class $p_{\mathbf{x}\sim\mathcal{C}}\left(\mathbf{x}\right)=p\left(\mathbf{x}|y=1,\ldots,K\right)$
    and the density for each concept $p_{\mathbf{x}\sim\mathcal{C}_{k}}\left(\mathbf{x}\right)=p\left(\mathbf{x}|y=k\right)$
    is unknown. Most semi-supervised methods assume that both $S^{(u)}$ and labelled
    data $S^{(l)}$ sample the concept class density, making $p_{\mathbf{x}\sim S^{(l)}}\left(\mathbf{x}\right)$
    and $p_{\mathbf{x}\sim S^{(u)}}\left(\mathbf{x}\right)$ very similar [[92](#bib.bib92)].
    A labelled and an unlabelled dataset, $S^{(l)}$ and $S^{(u)}$, respectively, are
    said to be identically and independently sampled if the density functions $p_{\mathbf{x}\sim
    S^{(u)}}$ and $p_{\mathbf{x}\sim S^{(l)}}$ are identical and are statistically
    independent.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in real-world settings different violations to the Independent and
    Identically Distributed (IID) assumption can be faced. For instance, unlabelled
    data is likely to contain observations which might not belong to any of the $K$
    classes. Potentially, this could lead to a different sampled density function
    from $p_{\mathbf{x}\sim\mathcal{C}}\left(\mathbf{x}\right)$. These observations
    belong to a distractor class dataset $\mathbf{x}\in\mathcal{D}$, and are drawn
    from a theoretical distribution of the distractor class $p_{\mathbf{x}\sim\mathcal{D}}\left(\mathbf{x}\right)$.
    Figure [1](#S1.F1 "Figure 1 ‣ I-B Semi-supervised learning ‣ I Introduction ‣
    Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey") shows distractor observations drawn from a distractor distribution
    $p_{\mathbf{x}\sim\mathcal{D}}\left(\mathbf{x}\right)$ in yellow.'
  prefs: []
  type: TYPE_NORMAL
- en: A subset of unlabelled observations from $S^{(u)}$, referred to as ${S^{(u)}}_{D}$
    are said to belong to a distractor class, if they are drawn from a different distribution
    than the observations that belong to the concept classes. The distractor class
    is frequently semantically different than the concept classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Different causes for a violation to the IID assumption for $S^{(u)}$ and $S^{(l)}$
    might be faced in real-world settings. These are enlisted as follows, and can
    be found with different degrees [[45](#bib.bib45)]:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Prior probability shift: The label distribution in the dataset $S^{(l)}$ might
    differ when compared to $S^{(u)}$. A specific case would be the label imbalance
    of the labelled dataset $S^{(l)}$ and a balanced unlabelled dataset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Covariate shift: A difference in the feature distributions between the datasets
    $S^{(l)}$ with respect to $S^{(u)}$ with the same classes in both, might be sampled,
    leading to a distribution mismatch. In a medical imaging application, for example,
    this can be related to the difference in the distribution of the sampled features
    between $S^{(l)}$ and $S^{(u)}$. This can be caused by the difference of the patients
    sample.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Concept shift: This is associated to a shift in the labels of $S^{(l)}$ with
    respect to $S^{(u)}$ of data with the same features. For example, in the medical
    imaging domain, different practitioners might categorize the same x-ray image
    into different classes. This is very related to the problem of noisy labelling
    [[31](#bib.bib31)].'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unseen classes: The dataset $S^{(u)}$ contains observations of unseen or unrepresented
    classes in the dataset $S^{(l)}$. One or more distractor classes are sampled in
    the unlabelled dataset. Therefore, a mismatch in the number of labels exist, along
    with a prior probability shift and a feature distribution mismatch.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ I-B Semi-supervised learning ‣ I Introduction
    ‣ Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey") illustrates a distribution mismatch setting. The circles correspond
    to unlabelled data and the squares and diamonds to the labelled dataset. The labelled
    and unlabelled data for the two classes are clearly imbalanced and sample different
    feature values. In this case, all the blue unlabelled observations are drawn from
    the concept classes. However, the yellow unlabelled observations, can be considered
    to have different feature value distributions. Many SSDL methods make usage of
    the clustered-data/low-density separation assumption together with the manifold
    hypothesis [[70](#bib.bib70)].'
  prefs: []
  type: TYPE_NORMAL
- en: II Semi-supervised Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section we study recent semi-supervised deep learning architectures.
    They are divided into different categories. Such categorization is meant to ease
    its analysis. However each category is not mutually exclusive with the rest of
    them, as there are several methods that mix concepts of two or more categories.
    This serves as a background to understand current SSDL approaches to deal with
    the distribution mismatch between $S^{(u)}$ and $S^{(l)}$.
  prefs: []
  type: TYPE_NORMAL
- en: II-A Pre-training for semi-supervised deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A basic approach to leverage the information from an unlabelled dataset $S^{(u)}$,
    is to perform as a first step an unsupervised pre-training of the classifier $f_{\textbf{w}}$.
    In this document we refer to it as Pre-trained Semi-Supervised deep learning (PT-SSDL).
    A straightforward way to implement PT-SSDL, is to pre-train the encoding section
    of the model $h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$
    to optimize a *proxy* or *pretext [[89](#bib.bib89)]* task $\delta$. The proxy
    task does not need the specific labels, allowing the usage of unlabelled data.
    This proxy loss is minimized during training, and enables the usage of unlabelled
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{\>L}_{u}^{\left(p\right)}\left(S^{(u)},\textbf{w}_{\textrm{FE}}\right)=\>\sum_{\textbf{x}_{i}\in
    S^{(u)}}\delta\left(r_{i},f_{\textrm{proxy}}\left(f_{\textbf{w}_{\textrm{FE}}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)\right)\right),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where the function $\delta$ compares the proxy label $r_{i}$ with the output
    of the proxy model $f_{\textrm{proxy}}$. The proxy task can be optimized also
    using labelled data. The process of optimizing a proxy task is also known as self-supervision
    [[44](#bib.bib44)]. This can be done in a pre-training step or during training,
    as seen in the models with unsupervised regularization. A simple approach for
    this *proxy* or *auxiliary* loss is to minimize the unsupervised reconstruction
    error. This is similar to the usage of a consistency function $\delta$, where
    the proxy task corresponds to reconstruct the input, making $\delta\left(\textbf{x}_{i},h_{\textbf{w}_{\textrm{DE}}}^{\textrm{(DE)}}\left(h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)\right)\right)$.
    The usage of an auto-encoder based reconstruction means it is usually necessary
    to add a decoder path $h_{\textbf{w}_{\textrm{DE}}}^{\textrm{(DE)}}$, which is
    later discarded at evaluation time. Pre-training can be performed for the whole
    model, or in a per-layer fashion, as initially explored in [[6](#bib.bib6)]. Moreover,
    pre-training can be easily combined with other semi-supervised techniques, as
    seen in [[50](#bib.bib50)].
  prefs: []
  type: TYPE_NORMAL
- en: In [[28](#bib.bib28)], a Convolutional Neural Network (CNN) is pre-trained with
    image patches from unlabelled data, with the proxy task of predicting the position
    of a new second patch. The approach was tested in object detection benchmarks.
    In [[18](#bib.bib18)] an unsupervised pre-training approach was proposed. It implements
    a proxy task optimization followed by a clustering step, both using unlabelled
    data. The proxy task consists of the random rotation of the unlabelled data, and
    the prediction of its rotation. The proposed method was tested against other unsupervised
    pre-training methods, using the PASCAL Visual Object Classes 2007 dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The proxy or auxiliary task is implemented in different manners in SSDL, as
    it is not exclusive to pre-training methods. This can be seen in consistency based
    regularization techniques, later discussed in this work. For instance in [[98](#bib.bib98)],
    an extensive set of proxy tasks are added as an unsupervised regularization term,
    and compared with some popular regularized SSDL methods. The authors used the
    ImageNet Large Scale Visual Recognition Challenge (ILSVRC) for the executed benchmarks.
    The proposed method showed a slight accuracy gain, with no statistical significance,
    against other two unsupervised regularization based methods.
  prefs: []
  type: TYPE_NORMAL
- en: II-B Pseudo-label semi-supervised deep learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Pseudo-label Semi-Supervised deep learning (PLT-SSDL) or also known as self-training,
    self-teaching or bootstrapping, pseudo-labels are estimated for unlabelled data,
    and used for model fine-tuning. A straightforward approach of pseudo-label based
    training consisting in co-training two models can be found in [[4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: In co-training, two or more different sets of input dimensions or views are
    used to train two or more different models. Such views can be just the result
    of splitting the original input array $\textbf{x}_{i}$. For instance, in two-views
    $v_{1}$ and $v_{2}$ co-training [[4](#bib.bib4)], two labelled datasets $S^{\left(l,v_{1}\right)}$
    and $S^{\left(l,v_{2}\right)}$ are used. In an initial iteration $i=1$, two models
    are trained using the labelled dataset, yielding the two view models $\widetilde{\textbf{w}}_{i}^{\left(v_{1}\right)}=T\left(f_{\textbf{w}},S^{\left(l,v_{1}\right)}\right)$
    and $\widetilde{\textbf{w}}_{i}^{\left(v_{1}\right)}=T\left(f_{\textbf{w}},S^{\left(l,v_{2}\right)}\right)$.
    This can be considered as a pre-training step. The resulting models can be referred
    to as an ensemble of models $\textbf{f}_{\widetilde{\textbf{w}}_{1}}=\left[f_{\widetilde{\textbf{w}}_{1}^{\left(v_{2}\right)}},f_{\widetilde{\textbf{w}}_{2}^{\left(v_{2}\right)}}\right]$.
  prefs: []
  type: TYPE_NORMAL
- en: As a second step, the disagreement probability $\mathbf{Pr}_{\textbf{x}_{i}\sim
    S^{(u)}}\left[f_{\widetilde{\textbf{w}}_{1}^{\left(v_{2}\right)}}\left(\textbf{x}_{j}\right)\neq
    f_{\widetilde{\textbf{w}}_{2}^{\left(v_{2}\right)}}\left(\textbf{x}_{j}\right)\right]$
    with $\textbf{x}_{j}\in S^{(u)}$ is used to estimate new labels or pseudo-labels
    $\widehat{y}_{j}^{(i,k)}=f_{\widetilde{\textbf{w}}_{i}^{\left(v_{k}\right)}}\left(\textbf{x}_{j}\right)$.
    The final pseudo-labels for each observation $\textbf{x}_{j}$ can be the result
    of applying a view-wise summarizing operation $\mu$ (like averaging or taking
    the maximum logits) making $\widehat{y}_{j}^{(i)}=\mu\left(\textbf{f}_{\textbf{w}_{i}}\left(\textbf{x}_{j}\right)\right)$.
  prefs: []
  type: TYPE_NORMAL
- en: The set of pseudo labels for the iteration i can be represented as $\widehat{S}_{i}=\mu\left(\textbf{f}_{\textbf{w}_{i}}\left(S^{(u)}\right)\right)$.
    In co-training [[4](#bib.bib4)], the agreed observations for the two models are
    picked in the function $\widetilde{S}_{i}=\varphi\left(\widehat{S}_{i}\right)$,
    as highly confident observations. The pseudo-labelled data with high confidence
    are included in the labelled dataset $S_{i+1}^{\left(r\right)}=S^{(l)}\bigcup\widetilde{S}_{i}$
    as pseudo-labelled observations. Later the model is re-trained for $i=2,..,\vartheta$
    iterations repeating the process of pseudo-labelling, filtering the most confident
    pseudo-labels and re-training the model. In general, we refer to a pseudo-labelling
    to the idea of estimating the hard labels $\widehat{y}_{j}^{(i)}$.
  prefs: []
  type: TYPE_NORMAL
- en: In [[29](#bib.bib29)] the Tri-net semi-supervised deep model (Tri-Net) was proposed.
    Here an ensemble $\textbf{f}_{\textbf{w}}$ of Deep Convolutional Neural Network
    (DCNN) s is trained with $k=1,2,3$ different top models, with also $k=1,2,3$ different
    labelled datasets $S_{i}^{(l,k)}$. The output posterior probability is the result
    of the three models voting. This results in the pseudo-labelled for the whole
    evaluated dataset $\widetilde{S}_{i}$, with $i=1$ for the first iteration. The
    pseudo-label filtering operation $\varphi$ includes the observations where at
    least two of the models agreed are included into the labelled dataset. The process
    is repeated for a fixed number of iterations. Also Tri-Net can be combined with
    any regularized SSDL approach. This combination was tested in [[82](#bib.bib82)],
    and is referred to in this document as Tri-net semi-supervised deep model with
    a Pi-Model regularization (TriNet+Pi). In [[82](#bib.bib82)], a similar ensemble-based
    pseudo-labelling approach is found. In such work, a mammogram image classifier
    was implemented, with an ensemble of classifiers that vote for the unlabelled
    observations. The observations with the highest confidence are added to the dataset,
    in an iterative fashion.
  prefs: []
  type: TYPE_NORMAL
- en: Another recent deep self-training approach can be found in [[23](#bib.bib23)],
    named as Speed as a supervisor for semi-supervised Learning Model (SaaSM). In
    a first step, the pseudo-labels are estimated by measuring the learning speed
    in epochs, optimizing the estimated labels as a probability density function $\widetilde{S}_{1}=f_{\textbf{w}_{1}}\left(S^{(u)}\right)$
    with a stochastic gradient descent approach. The estimated labels are used to
    optimize an unsupervised regularized loss. SaaSM was tested using the Canadian
    Institute for Advanced Research dataset of 10 classes (CIFAR-10) and Street View
    House Numbers dataset (SVHN) datasets. It yielded slightly higher accuracy to
    other consistency regularized methods such as mean teacher, according to the reported
    results. No statistical significance analysis was done.
  prefs: []
  type: TYPE_NORMAL
- en: II-C Regularized semi-supervised learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Regularized Semi-Supervised deep learning (R-SSDL), or co-regularized learning
    as defined in [[101](#bib.bib101)], the loss function of a deep learning model
    $f_{\textbf{w}}$ includes a regularization term using unlabelled data $S^{(u)}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\overset{\underset{\textbf{w}}{\textrm{argmin}}\mathcal{L}\left(S,f_{\textbf{w}}\right)=}{\underset{\textbf{w}}{\textrm{argmin}}\sum_{\left(\textbf{x}_{i},y_{i}\right)\in
    S^{(l)}}\mathcal{L}_{l}\left(f_{\textbf{w}}\left(\textbf{x}_{i}\right),y_{i}\right)+\gamma\sum_{\textbf{x}_{j}\in
    S^{(u)}}\mathcal{L}_{u}\left(f_{\textbf{w}},\textbf{x}_{i}\right)}.$ |  | (2)
    |'
  prefs: []
  type: TYPE_TB
- en: 'The unsupervised loss $\mathcal{L}_{u}$ regularizes the model $f_{\textbf{w}}$
    using the unlabelled observations $\textbf{x}_{j}$. The unsupervised regularization
    coefficient $\gamma$ controls the unsupervised regularization influence during
    the model training. We consider it an SSDL sub-category, as a wide number of approaches
    have been developed inspired by this idea. In the literature, different approaches
    for implementing the unsupervised loss function $\mathcal{L}_{u}$ can be found.
    Sections [II-C1](#S2.SS3.SSS1 "II-C1 Consistency based regularization ‣ II-C Regularized
    semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey"),
    [II-C2](#S2.SS3.SSS2 "II-C2 Adversarial augmentation based regularization ‣ II-C
    Regularized semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    and [II-C3](#S2.SS3.SSS3 "II-C3 Graph based regularization ‣ II-C Regularized
    semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    group the most common approaches for implementing it.'
  prefs: []
  type: TYPE_NORMAL
- en: II-C1 Consistency based regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'A Consistency Regularized Semi-supervised deep learning (RC-SSDL) loss function
    measures how *robust* a model is, when classifying unlabelled observations in
    $S^{(u)}$ with different transformations applied to the unlabelled data. Such
    transformations usually perturb the unlabelled data without changing its semantics
    and class label (label preserving transformations). For instance, in [[4](#bib.bib4)]
    consistency assumption $\chi^{\textrm{(CL)}}$ is enforced for two views in [[4](#bib.bib4)],
    using the Euclidean distance: $\delta\left(\textbf{x}_{j},f_{\textbf{w}}\right)=\left\|f_{\textbf{w}^{\prime}}\left(\Psi^{\eta^{\prime}}\left(\textbf{x}_{j}\right)\right)-f_{\textbf{w}}\left(\Psi^{\eta}\left(\textbf{x}_{j}\right)\right)\right\|.$'
  prefs: []
  type: TYPE_NORMAL
- en: Where $\delta\left(\textbf{x}_{i},f_{\textbf{w}}\right)$ is the consistency
    function. Consistency can also be measured for labelled observations in $\textbf{x}_{j}\in
    S^{(l)}$. A number of SSDL techniques are based on consistency regularization.
    Therefore we refer to this category as Consistency based Regularized Semi-Supervised
    deep learning (CR-SSDL).
  prefs: []
  type: TYPE_NORMAL
- en: A simple interpretation of the consistency regularization term is the increase
    of a model’s robustness to noise, by using the data in $S^{(u)}$. A consistent
    model output for corrupted observations implies a more robust model, with better
    generalization. Consistency can be measured between two deep learning models $f_{\textbf{w}^{\prime}}$
    and $f_{\textbf{w}}$ fed with two different views or random modifications of the
    same observation, $\Psi^{\eta^{\prime}}\left(\textbf{x}_{j}\right)$ and $\Psi^{\eta}\left(\textbf{x}_{j}\right)$.
    For this reason, some authors refer to consistency based SSDL approaches as self-ensemble
    learning models [[55](#bib.bib55)]. The consistency of two or more variations
    of the model is evaluated, measuring the overall model robustness.
  prefs: []
  type: TYPE_NORMAL
- en: Consistency regularized methods are based on the consistency assumption $\chi^{(\textrm{CL})}$.
    Thus, such methods can be related to other previously SSDL approaches that also
    exploit this assumption. For instance, as previously mentioned, the consistency
    assumption is also implemented in PT-SSDL as the proxy task, where the model is
    pre-trained to minimize a proxy function. The consistency function can be thought
    as a particular case of the aforementioned confidence function for self-training
    $\varphi$, using two or more views of the observations, as developed in [[4](#bib.bib4)].
    However, in this case the different corrupted views of the observation $\textbf{x}_{j}$
    are highly correlated, in spite of the original co-training approach developed
    in [[4](#bib.bib4)]. Nevertheless, recent regularized SSDL models [[3](#bib.bib3),
    [86](#bib.bib86), [49](#bib.bib49)] simplify this assumption. They consider as
    a view of an observation $\textbf{x}_{j}$ its corruption with random noise $\eta$,
    making up a corrupted view $\Psi^{\eta}\left(\textbf{x}_{j}\right)$. The independence
    assumption [[4](#bib.bib4)] between the views of co-training, fits better when
    measuring the consistence between different signal sources, as seen in [[56](#bib.bib56)].
    In such work, different data sources are used for semi-supervised human activity
    recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[3](#bib.bib3)] the Pi Model (Pi-M) was proposed. The consistency of the
    deep model with random noise injected to its weights (commonly referred to as
    dropout) is evaluated. The weights $\textbf{w}^{\prime}$ are a corrupted version
    of the parent model with weights w, making up what the authors refer as a pseudo-ensemble.
    The Pi-M model was tested in [[86](#bib.bib86)] using the CIFAR-10 and SVHN datasets.
    Intersected yielded results of Pi-M with the rest of the discussed methods in
    this work can be found in Table [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization
    ‣ II-C Regularized semi-supervised learning ‣ II Semi-supervised Deep Learning
    ‣ Semi-supervised Deep Learning for Image Classification with Distribution Mismatch:
    A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'A consistency evaluation of both unlabelled and labelled datasets can be performed,
    as proposed in [[72](#bib.bib72)], in the Mutual Exclusivity-Transformation Model
    (METM). In such method, an unsupervised loss term for Transformation Supervision
    (TS) was proposed: $\mathcal{L}_{u}^{\left(\textrm{TS}\right)}\left(f_{\textbf{w}},\textbf{x}_{i}\right)=\sum_{j}^{M}\sum_{k}^{M}\left\|f_{\textbf{w}}\left(\Psi_{j}\left(\textbf{x}_{i}\right)\right)-f_{\textbf{w}}\left(\Psi_{k}\left(\textbf{x}_{i}\right)\right)\right\|^{2},$
    where $M$ random transformations $\Psi$ are performed over the observation $\textbf{x}_{i}$.
    This can be used for unsupervised pre-training. Such loss term can be regarded
    as a consistency measurement. Furthermore, a Mutual Exclusivity (ME) based loss
    function is used. It encourages non-overlapping predictions of the model. The
    ME loss term is depicted as $\mathcal{L}_{u}^{\left(\textrm{ME}\right)}=\left\|-\prod_{k}^{K}f\left(\textbf{x}_{i}\right)\prod_{k}^{K}\left(1-f_{\textbf{w}}\left(\textbf{x}_{i}\right)\right)\right\|^{2}$.
    The final unsupervised loss is implemented as $\mathcal{L}_{u}=\lambda_{1}\mathcal{L}_{u}^{\left(\textrm{ME}\right)}+\lambda_{2}\mathcal{L}_{u}^{\left(\textrm{TS}\right)}$,
    with the weighting coefficients $\lambda_{1}$ and $\lambda_{2}$ for each unsupervised
    loss term. METM was tested with the SVHN and CIFAR-10 datasets. Comparable results
    with the rest of reviewed methods in this work are depicted in Table [I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Later, authors in [[49](#bib.bib49)] proposed the Temporal Ensemble Model (TEM),
    which calculates the consistency of the trained model with the the moving weighted
    average of the predictions from different models along each training epoch $\tau$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $f_{\textbf{w}^{\prime}_{\tau}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)=(1-\rho)f_{\textbf{w}_{\tau}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)+\rho
    f_{\textbf{w}_{\tau-1}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right),$ |  |
    (3) |'
  prefs: []
  type: TYPE_TB
- en: 'with $\rho$ the decay parameter, and $\tau$ the current training epoch. The
    temporal ensemble evaluates the output of a temporally averaged model to a noisy
    observation $\Psi^{\eta}\left(\textbf{x}_{i}\right)$. This enforces temporal consistency
    of the model. Table [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C
    Regularized semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey")
    shows the yielded results by the TEM method for the CIFAR-10 dataset. Based on
    this approach, the authors in [[85](#bib.bib85)], developed an SSDL approach based
    on the Kullback-Leibler cross-entropy to measure model consistency. Different
    transformations $\Psi^{\eta}$ are applied to the input observations $\textbf{x}_{j}$.
    These correspond to image flipping, random contrast adjustment, rotation and cropping.
    The method was evaluated in a real world scenario with ultrasound fetal images
    for anatomy classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 'An extension of the temporal ensembling idea was presented by the authors of
    [[86](#bib.bib86)], in the popular Mean Teacher Model (MTM). Instead of averaging
    the predictions of models calculated in past epochs, the authors implemented an
    exponential weight average: $\textbf{w}^{\prime}_{\tau}=\rho\textbf{w}^{\prime}_{\tau-1}+\left(1-\rho\right)\textbf{w}^{\prime}_{\tau}$
    for a training epoch $\tau$, with an exponential weighting coefficient $\rho$.
    Such exponentially averaged model with parameters $\textbf{w}^{\prime}$ is referred
    to by the authors as the teacher model. For comparison purposes, the yielded results
    by MTM using the CIFAR-10 dataset are depicted in Table [I](#S2.T1 "Table I ‣
    II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning ‣
    II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'More recently, authors in [[59](#bib.bib59)], proposed the Virtual Adversarial
    Training Model (VATM). They implemented a generative adversarial network to inject
    adversarial perturbations $\eta$ into the labelled and unlabelled observations.
    Artificially generated observations are compared to the original unlabelled data.
    This results in adversarial noise encouraging a more challenging consistency robustness.
    Furthermore, the authors also added a conditional entropy term, in order to make
    the model more confident when minimizing it. We refer to this variation as Virtual
    Adversarial Training with Entropy Minimization (VATM+EM). Both VATM and VATM+EM
    were tested with the CIFAR-10 dataset, thus we include the comparable results
    with the rest of the reviewed methods in Table [I](#S2.T1 "Table I ‣ II-C3 Graph
    based regularization ‣ II-C Regularized semi-supervised learning ‣ II Semi-supervised
    Deep Learning ‣ Semi-supervised Deep Learning for Image Classification with Distribution
    Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Another variation of the consistency function $\mathcal{L}_{u}$ was developed
    in [[19](#bib.bib19)] in what the authors referred to as a memory loss function.
    We refer to this as the Memory based Model (MeM). This memory loss is based on
    a memory module, consisting of an embedding $m_{i}=\left(\check{\textbf{x}}_{i},\hat{\textbf{y}}_{i}\right)$.
    It is composed of the features extracted $\check{\textbf{x}}_{i}=h_{{}_{\textbf{w}^{\left(\textrm{FE}\right)}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$
    by the deep learning model $f_{\textbf{w}}$, and the corresponding probability
    density function computed $\hat{\textbf{y}}_{i}=f_{\textbf{w}}\left(\textbf{x}_{i}\right)$
    (with $\hat{\textbf{y}}$ the logits output), for a given observation $\textbf{x}_{i}$.
    The memory stores one embedding $k$ per class $m_{k}=\left(\check{\textbf{$\textbf{x}$}}_{k},\hat{\textbf{$\textbf{y}$}}_{k}\right)$,
    corresponding to the average embedding of all the observations within class $k$.
    Previous approaches like the temporal ensemble [[49](#bib.bib49)] needed to store
    the output of past models for each observation. In the memory loss based approach
    of [[19](#bib.bib19)] this is avoided by only storing one average embedding per
    class. In the second step, the memory loss is computed as follows: $\mathcal{L}_{m}=H\left(\textbf{p}_{i}\right)+\max\left(\textbf{p}_{i}\right)\delta_{\textrm{KL}}\left(\textbf{p}_{i},\hat{\textbf{y}}_{i}\right),$
    where $\textbf{p}_{i}$ is they key addressed probability, calculated as the closest
    embedding to $\textbf{x}_{i}$, and $\hat{\textbf{y}}_{i}$ is the model output
    for such observation. The factor $\max\left(\textbf{p}_{i}\right)$ is the highest
    value of the probability distribution $\textbf{p}_{i}$ and $H\left(\textbf{p}_{i}\right)$
    is the entropy of the key addressed output distribution $\textbf{p}_{i}$. The
    factor $\delta_{\textrm{KL}}\left(\textbf{p}_{i},\hat{\textbf{y}}_{i}\right)$
    is the Kullback-Leibler distance of the output for the observation $\textbf{x}_{i}$
    and the recovery key address from the memory mapping. Comparable results to the
    rest of the reviewed methods for the MeM method are shown in Table [I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey") for the CIFAR-10 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'More recently, an SSDL approach was proposed in [[77](#bib.bib77)]. This is
    referred to as the Transductive Model (TransM). The authors implement a transductive
    learning approach. This means that the unknown labels $\widetilde{y}$ are also
    treated as variables, thus optimized along with the model parameters w. Therefore,
    the loss function implements a cross-entropy supervised loss: $\mathcal{L}_{l}\left(f_{\textbf{w}},\textbf{x}_{i},y_{i}\right)=r_{i}H_{\textrm{CE}}\left(f_{\textbf{w}}\left(\textbf{x}_{i}\right),y_{i}\right),$
    with $r_{i}$ an element of the set $R=\left\{r_{i}\right\}_{i=1}^{n_{l}+n_{u}}$,
    which indicates the label estimation confidence level for an observation $\textbf{x}_{i}$.
    Such confidence level coefficient makes the model more robust to outliers in both
    the labelled and unlabelled datasets. The confidence coefficient is calculated
    using a k-nearest neighbors approach from the labelled data, making use of the
    observation density assumption $\chi^{(\textrm{CL})}$. This means that the label
    estimated is of high confidence, if the observations lies in a high density space
    for the labelled data within the feature space. As DCNN s are meant to be used
    by the model, the feature space is learned within the training process, making
    necessary to recalculate $R$ at each training step $\tau$. As for the unlabelled
    regularization term: $\mathcal{L}_{u}\left(f_{\textbf{w}},\textbf{x}_{j}\right)=\lambda_{\textrm{RF}}\mathcal{L}_{\textrm{RF}}\left(f_{\textbf{w}},\textbf{x}_{j}\right)+\lambda_{\textrm{MMF}}\sum_{\textbf{x}_{i}}\mathcal{L}_{\textrm{MMF}}\left(f_{\textbf{w}},\textbf{x}_{j},\textbf{x}_{i}\right),$
    it is composed of a robust feature measurement $\mathcal{L}_{\textrm{RF}}$ and
    a min-max separation term $\mathcal{L}_{\textrm{MMF}}$, where $\lambda_{\textrm{RF}}$
    and $\lambda_{\textrm{MMF}}$ weigh their contribution to the unsupervised signal.
    The first term measures the feature consistency, thus using the output of the
    learned feature extractor of the model $\check{\textbf{x}}_{i}=h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$.
    The consistency of the learned features is measured with the Euclidian distance
    $\mathcal{L}_{\textrm{RF}}\left(\textbf{w},\textbf{x}_{j}\right)=\left\|h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\Psi^{\eta}\left(\textbf{x}_{j}\right)\right)-h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\Psi^{\eta^{\prime}}\left(\textbf{x}_{j}\right)\right)\right\|^{2}$.
    Regarding the second term, referred to as the min-max separation function, it
    is meant to maximize the distance between observations of different classes by
    a minimum margin $\rho$, and to minimize the distance from observations within
    the same class. It is implemented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{\textrm{MMF}}\left(f_{\textbf{w}},\textbf{x}_{i},\textbf{x}_{j}\right)=r_{i}r_{j}\left\&#124;f_{\textbf{w}}\left(\textbf{x}_{i}\right)-f_{\textbf{w}}\left(\textbf{x}_{j}\right)\right\&#124;^{2}\delta\left(\widetilde{y}_{i,}\widetilde{y}_{j}\right)$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $-\textrm{min}\left(\left\&#124;f_{\textbf{w}}\left(\textbf{x}_{i}\right)-f_{\textbf{w}}\left(\textbf{x}_{j}\right)\right\&#124;^{2}-\rho,0\right)\left(1-\delta\left(\widetilde{y}_{i,}\widetilde{y}_{j}\right)\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'With $\delta\left(\widetilde{y}_{i,}\widetilde{y}_{j}\right)=1$ when $\widetilde{y}_{i}=\widetilde{y}_{j}$,
    or cero otherwise. The first term in $\mathcal{L}_{\textrm{MMF}}$ minimizes the
    intra-class distance and the second term maximizes the inter-class observation
    distance. We highlight the theoretical outlier robustness of the method by implementing
    the confidence coefficient $r_{i}$. This coefficient is able to give lower relevance
    to unconfident estimations. However, it is yet to be fully proved, as the experiments
    conducted in [[77](#bib.bib77)] have not tested the model robustness to unlabelled
    single and collective outliers. The approach was also combined and tested against
    other consistency regularization approaches, like the MTM. This is referred to,
    in this work, as Transductive Model with Mean Teacher (TransM+MTM). The comparable
    results with the rest of the reviewed approaches are depicted in Table [I](#S2.T1
    "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'An alternative approach for the consistency function was implemented in [[89](#bib.bib89)],
    with a model named by the authors as Self Supervised network Model (SESEMI). The
    consistency function is fed by what the authors defined as a self-supervised branch.
    This branch aims to learn simple image transformations or pretext tasks, such
    as image rotation. The authors claim that their model is easier to calibrate than
    the MTM, by just using an unsupervised signal weight of $\lambda=1$. The intersected
    results of SESEMI with the rest of the reviewed methods are detailed in Table
    [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[9](#bib.bib9)], the authors proposed a novel SSDL method known as MixMatch.
    This method implements a consistency loss which first calculates a soft pseudo-label
    for each unlabelled observation. Those soft pseudo-labels are the result of averaging
    the model response to a number of transformations of the input $\textbf{x}_{j}$
    $\widehat{\textbf{y}}{}_{j}=\frac{1}{\mathcal{T}}\sum_{\eta=1}^{\mathcal{T}}f_{\textbf{w}}\left(\Psi^{\eta}\left(\textbf{x}_{j}\right)\right)$.
    In such equation, $\mathcal{T}$ refers to the number of transformations of the
    input image (i.e., image rotation, cropping, etc.). The specific image transformation
    is represented in $\Psi^{\eta}$. The authors in [[9](#bib.bib9)] recommend to
    use $\mathcal{T}=2$. Later, the obtained soft pseudo-label is sharpened, in order
    to decrease its entropy and under-confidence of the pseudo-label. For this, a
    parameter $\rho$ is used within the softmax of the output $\widehat{\textbf{y}}{}_{j}$:
    $s\left(\widehat{\textbf{y}},\rho\right)_{i}=\frac{\widehat{y}_{i}^{1/\rho}}{\sum_{j}\widehat{y}_{j}^{1/\rho}}$.
    The dataset $\widetilde{S}_{u}=\left(X_{u},\widetilde{Y}\right)$ contains the
    sharpened soft pseudo-labels, where $\widetilde{Y}=\left\{\widetilde{\textbf{y}}_{1},\widetilde{\textbf{y}}_{2},\ldots,\widetilde{\textbf{y}}_{n_{u}}\right\}$.
    The authors of MixMatch found that data augmentation is very important to improve
    its performance. Taking this into account, the authors implemented the MixUp methodology
    to augment both the labelled and unlabelled datasets [[99](#bib.bib99)]. This
    is represented as follows: $\left(S^{\prime}_{l},\widetilde{S}^{\prime}_{u}\right)=\Psi_{\textrm{MixUp}}\left(S_{l},\widetilde{S}_{u},\alpha\right)$.
    The MixUp generates new observations through a linear interpolation between different
    combinations of both the labelled and unlabelled data. The labels for the new
    observations are also lineally interpolated, using both the labels and the pseudo-labels
    (for the unlabelled data).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mathematically, MixUp takes two pseudo-labelled or labelled data pairs $\left(\textbf{x}_{a},y_{a}\right)$
    and $\left(\textbf{x}_{b},y_{b}\right)$, and generates the augmented datasets
    $\left(S^{\prime}_{l},\widetilde{S}^{\prime}_{u}\right)$. These augmented datasets
    are used by MixMatch, to train neural network with parameters w through the minimization
    of the following loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}\left(S,\textbf{w}\right)=\sum_{\left(\textbf{x}_{i},\textbf{y}_{i}\right)\in
    S^{\prime}_{l}}\mathcal{L}_{l}\left(\textbf{w},\textbf{x}_{i},\textbf{y}_{i}\right)+$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\gamma r(t)\sum_{\left(\textbf{x}_{j},\widetilde{\textbf{y}}_{j}\right)\in\widetilde{S}^{\prime}_{u}}\mathcal{L}_{u}\left(\textbf{w},\textbf{x}_{j},\widetilde{\textbf{y}}_{j}\right)$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'The labelled loss term $\mathcal{L}_{l}$, can be implemented with a cross-entropy
    function, as recommended in [[9](#bib.bib9)]; $\mathcal{L}_{l}\left(\textbf{w},\textbf{x}_{i},\textbf{y}_{i}\right)=H_{\textrm{CE}}\left(\textbf{y}_{i},f_{\textbf{w}}\left(\textbf{x}_{i}\right)\right)$.
    Regarding the unlabelled loss term, an Euclidean distance was tested by the authors
    in [[9](#bib.bib9)] $\mathcal{L}_{u}\left(\textbf{w},\textbf{x}_{j},\widetilde{\textbf{y}}_{j}\right)=\left\|\widetilde{\textbf{y}}_{j}-f_{\textbf{w}}\left(\textbf{x}_{j}\right)\right\|$.
    In the MixMatch loss function, the coefficient $r(t)$ is implemented as a ramp-up
    function which augments the weight of the unlabelled loss term as $t$ increases.
    The parameter $\gamma$ controls the overall influence of the unlabelled loss term.
    In Table [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized
    semi-supervised learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised
    Deep Learning for Image Classification with Distribution Mismatch: A Survey"),
    the results yielded in [[9](#bib.bib9)] are depicted for the  CIFAR-10 dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[8](#bib.bib8)] an extension of the MixMatch algorithm was developed, referred
    to as ReMixMatch. Two main modifications were proposed: a distribution alignment
    procedure and a more extensive use of data augmentation. Distribution alignment
    consists of the normalization of each prediction using both the running average
    prediction of each class (in a set of previous model epochs) and the marginal
    label distribution using the labelled dataset. This way, soft pseudo-label estimation
    accounts for the label distribution and previous label predictions, enforcing
    soft pseudo-label consistency with both distributions. The extension of the previous
    simple data-augmentation step implemented in the original MixMatch algorithm (where
    flips and crops were used) consists of two methods. They are referred to as anchor
    augmentation and CTAugment by the authors. The empirical evidence gathered by
    the authors when implementing stronger data augmenting transformations (i.e. gamma
    and brightness modifications, etc.) in MixMatch showed a performance deterioration.
    This is caused by the larger variation in the model output for each type of strong
    transformation, making the pseudo-label less meaningful. To circumvent this, the
    authors proposed an augmentation anchoring approach. It uses the same pseudo-labels
    estimated when using a weak transformation, for the $\mathcal{T}^{\prime}$ strong
    transformations. Such strong transformations are calculated through a modification
    of the auto-augment algorithm. Auto-augment originally uses reinforcement learning
    to find the best resulting augmentation policy (set of transformations used) for
    the specific target problem [[24](#bib.bib24)]. To simplify its implementation
    for small labelled datasets, the authors in [[8](#bib.bib8)] proposed a modification
    referred to as CTAugment. It estimates the likelihood of generating a correctly
    classified image, in order to generate images that are unlikely to result in wrong
    predictions. The performance reported in the executed benchmarks of ReMixMatch,
    showed an accuracy gain ranging from 1% to 6%, when compared to the original MixMatch
    algorithm. No statistical significance tests were reported. Comparable results
    to other methods reviewed in this work for the CIFAR-10 dataset are shown in Table
    [I](#S2.T1 "Table I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised
    learning ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for
    Image Classification with Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'More recently, an SSDL method referred to as FixMatch was proposed in [[80](#bib.bib80)].
    The authors argue the proposition of a simplified SSDL method compared to other
    techniques. FixMatch is based upon pseudo-labelling and consistency regularized
    SSDL. The loss function uses a cross-entropy labelled loss term, along with weak
    augmentations for the labelled data. For the unlabelled loss term, the cross-entropy
    is also used, but for the strongly unlabelled observations with its corresponding
    pseudo-label. The soft pseudo-label is calculated using weak transformations,
    taking the maximum logit of the model output. Therefore no model output sharpening
    is done, unlike MixMatch. Strong augmentations are tested using both Random Augmentation
    (RA) [[25](#bib.bib25)] and CTAugmentation (CTA) [[8](#bib.bib8)]. For benchmarking
    FixMatch, the authors used the CIFAR-10 (40, 250, 4000 labels), Canadian Institute
    For Advanced Research dataset with 100 classes (CIFAR-100) (400, 2500 and 10000
    labels), SVHN (40, 250, 1000 labels) and Self-Taught Learning 10 classes (STL-10)
    (1000 labels) datasets. For all the methods, variations of the Wide-ResNet CNN
    backbone were used. The average accuracy for each test configuration was similar
    to the results yielded by ReMixMatch, with no statistical significance tests performed.
    Comparable results yielded by FixMatch are depicted in Table [I](#S2.T1 "Table
    I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning
    ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: II-C2 Adversarial augmentation based regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recent advances in deep generative networks for learning data distribution has
    encouraged its usage in SSDL architectures [[36](#bib.bib36)]. Regularized techniques
    usually employ basic data augmentation pipelines, in order to evaluate the consistency
    term $\mathcal{L}_{u}$. However, generative neural networks can be used to learn
    the distribution of labelled and unlabelled data and generate entirely new observations.
    These are categorized as Generative adversarial Network based Consistency Regularized
    Semi-supervised deep learning (GaNC-SSDL). Learning a good approximation of data
    distribution $\mathbf{Pr}_{\textbf{x}\sim\mathcal{C}}\left(\textbf{x}\right)$
    allows the artificial generation of new observations. The observations can be
    added to the unlabelled dataset $S^{(u)}$, or the very same adversarial training
    might lead to a refined set of model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[81](#bib.bib81)], the generative network architecture was extended for
    SSDL, by implementing a discriminator function $f_{\textbf{w}_{d}}^{\left(d\right)}$
    able to estimate not only if an observation $\textbf{x}_{i}$ belongs or not to
    one of the classes to discriminate from, but also to which specific class it belongs
    to. The model was named by the authors as Categorical Generative Adversarial Network
    (CAT-GAN), given the capacity of the discriminator to perform ordinary $1-K$ classification.
    Therefore, $f_{\textbf{w}_{d}}^{\left(d\right)}$ is able to estimate the density
    function of an unlabelled observation $\textbf{x}_{i}\in S^{(u)}$, $\hat{\textbf{y}}_{i}=f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)$.
    The discriminator model implements a semi-supervised loss function:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i}\right)=\mathcal{L}_{l}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i},\textbf{y}_{i}\right)+\mathcal{L}_{u}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i}\right)$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: with $\mathcal{L}_{l}^{\left(d\right)}\left(\textbf{w}_{d},\textbf{x}_{i}\right)=H_{\textrm{CE}}\left(\textbf{y}_{i},f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)\right),$
    where $H_{\textrm{CE}}$ is the cross entropy. The unsupervised discriminator term
    $\mathcal{L}_{u}^{\left(d\right)}$ was designed for maximizing the certainty for
    unlabelled observations and minimizing it for artificial observations. The authors
    also included a term for encouraging imbalance correction for the $K$ classes.
    The proposed method in [[81](#bib.bib81)] was tested using the CIFAR-10 and Modified
    National Institute of Standards and Technology dataset (MNIST) datasets for SSDL.
    It was compared only against the Pi-M, with marginally better average results
    and no statistical significance analysis of the results. However, the CAT-GAN
    served as a foundation for posterior work on using generative deep models for
    SSDL.
  prefs: []
  type: TYPE_NORMAL
- en: 'A breakthrough improvement in training $f_{\textbf{w}_{d}}^{\left(d\right)}$
    and $f_{\textbf{w}_{g}}^{\left(g\right)}$ models was achieved in [[73](#bib.bib73)],
    aiming to overcome the difficulty of training complementary loss functions with
    a stochastic gradient descent algorithm. This problem is known as the Nash equilibrium
    dilemma. The authors yielded such improvement, through a feature matching loss
    for the generator $f_{\textbf{w}_{g}}^{\left(g\right)}$, which seeks to make the
    generated observations match the statistical moments of a real sample from training
    data $S$. The enhanced trainability of the Feature Matching Generative Adversarial
    Network (FM-GAN) was tested in a semi-supervised learning setting. The semi-supervised
    loss function implements an unsupervised term $\mathcal{L}_{u}^{\left(d\right)}\left(f_{\textbf{w}_{d}}^{\left(d\right)},\textbf{x}_{i}\right)$
    which aims to maximize the discriminator success rate in discriminating both unlabelled
    real observations $\textbf{x}_{i}\in S^{(u)}$ and artificially generated ones.
    The discriminator model $f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)$
    outputs the probability of the observation $\textbf{x}_{i}$ belonging to one of
    the real classes. Also in this work, the authors showed how achieving a good semi-supervised
    learning accuracy (thus a good discriminator), often yields a poor generative
    performance. Authors suggested that a bad generator describes better Out of Distribution
    (OOD) data, improving the overall model robustness. Intersected results with the
    rest of the reviewed SSDL methods are depicted in Table [I](#S2.T1 "Table I ‣
    II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning ‣
    II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In [[26](#bib.bib26)], the authors further explored the inverse relationship
    between generator and semi-supervised discriminate performance with the Bad Generative
    Adversarial Network (Bad-GAN). The experiments showed how a *bad* generator $f_{\textbf{w}_{g}}^{\left(g\right)}$,
    created observations out of the distribution of the concept class $\mathbf{Pr}_{\textbf{x}\sim\mathcal{C}}$
    enhancing the performance of the discriminator $f_{\textbf{w}_{d}}^{\left(d\right)}$
    for semi-supervised learning. More specifically, the generator loss $\mathcal{L}^{\left(g\right)}$
    is encouraged to maximize the Kullback-Leibler distance to the sampled data distribution.
    This enforces the boundaries built by the discriminator $f_{\textbf{w}_{d}}^{\left(d\right)}$
    for distractor observations. In [[52](#bib.bib52)] a comparison between the triple
    generative network proposed in [[22](#bib.bib22)] and the bad generator [[26](#bib.bib26)]
    was done. No conclusive results were reached, leading the authors to suggest a
    combination of the approaches to leverage accuracy. For comparison purposes with
    related work, results with the CIFAR-10 are described in Table [I](#S2.T1 "Table
    I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning
    ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Later, in [[68](#bib.bib68)], the authors proposed a co-training adversarial
    regularization approach for the Co-trained Generative Adversarial Network (Co-GAN),
    making use of the consistency assumption of two different models $f_{\textbf{w}_{d_{1}}}^{\left(d_{1}\right)}$
    and $f_{\textbf{w}_{d_{2}}}^{\left(d_{2}\right)}$. Each model is trained with
    a different view from the same observation $\textbf{x}_{i}=\left\langle\textbf{x}_{i}^{\left(v_{1}\right)},\textbf{x}_{i}^{\left(v_{2}\right)}\right\rangle$.
    A general loss function $\mathcal{L}\left(S\right)=\mathcal{L}_{l}\left(S_{l}\right)+\mathcal{L}_{u}\left(S^{(u)}\right)$
    is minimized, with the unsupervised loss function defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{u}\left(f_{\textbf{w}},S^{(u)}\right)=\lambda_{\textrm{cot}}\mathcal{L}_{\textrm{cot}}\left(f_{\textbf{w}},S^{(u)}\right)+\lambda_{\textrm{dif}}\mathcal{L}_{\textrm{dif}}\left(f_{\textbf{w}},\textbf{z}\right).$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: The term $\mathcal{L}_{\textrm{cot}}$ measures the expected consistency of the
    two models using two views from the same observation, through the Jensen-Shannon
    divergence. In $\mathcal{L}_{\textrm{dif}}\left(\textbf{z}\right)$, each model
    artificially generates observations to deceive the other one. This stimulates
    a view difference between the models, to avoid them collapsing into each other.
    The coefficients $\lambda_{\textrm{cot}}$ and $\lambda_{\textrm{dif}}$ weigh the
    contribution of each term. Therefore, for each view, a generator is trained and
    the models $f_{\textbf{w}_{d_{1}}}^{\left(d_{1}\right)}$ and $f_{\textbf{w}_{d_{2}}}^{\left(d_{2}\right)}$
    play the detective role. The proposed method out performed MTM, TEM and the Bad-GAN
    according to [[68](#bib.bib68)]. Experiments were performed with more than two
    observation views, generalizing the model for a multi-view layout $\textbf{x}_{i}=\left\langle\textbf{x}_{i}^{\left(v_{1}\right)},\textbf{x}_{i}^{\left(v_{2}\right)}\right\rangle$.
    The best performing model implemented 8 views, henceforth referred in this document
    as Co-trained Generative Adversarial Network with 8 views (Co-8-GAN). We include
    results of the benchmarks done in [[68](#bib.bib68)] with the SVHN and CIFAR-10
    datasets. The authors did not report any statistical significance analysis of
    the provided results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Triple Generative Adversarial Network (Triple-GAN) [[22](#bib.bib22)] addressed
    the aforementioned inverse relationship between generative and semi-supervised
    classification performance, by training three different models, detailed as follows.
    First, a classifier $f_{\textbf{w}_{c}}^{\left(c\right)}\left(\textbf{x}_{i}\right)$
    which learns the data distribution $\mathbf{Pr}_{\textbf{x}\sim\mathcal{C}}$ and
    outputs pseudo-labels for artificially generated observations. Secondly, a class-conditional
    generator $f_{\textbf{w}_{g}}^{\left(g\right)}$ able to generate observations
    for each individual class. Thirdly, a discriminator $f_{\textbf{w}_{d}}^{\left(d\right)}\left(\textbf{x}_{i}\right)$,
    which rejects observations out of the labelled classes. The architecture uses
    pseudo-labelling, since the discriminator uses the pseudo labels of the classifier
    $\hat{\textbf{y}}_{i}=f_{\textbf{w}_{c}}^{\left(c\right)}\left(\textbf{x}_{i}\right)$.
    Nevertheless, a consistency regularization was implemented in the classifier loss
    $\mathcal{L}^{\left(c\right)}$. The results using CIFAR-10 with the settings also
    tested in the rest of the reviewed works are depicted in tables [I](#S2.T1 "Table
    I ‣ II-C3 Graph based regularization ‣ II-C Regularized semi-supervised learning
    ‣ II Semi-supervised Deep Learning ‣ Semi-supervised Deep Learning for Image Classification
    with Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: II-C3 Graph based regularization
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Graph based Regularized Semi-Supervised Deep Learning (GR-SSDL) is based on
    previous graph based regularization techniques [[34](#bib.bib34)]. The core idea
    of GR-SSDL is to preserve mutual distance from observations in the dataset $S$
    (for both labelled and unlabelled) in a new feature space. An embedding is built
    through a mapping function $\check{\textbf{x}}_{i}=h_{\textbf{w}_{\textrm{FE}}}^{\left(\textrm{FE}\right)}\left(\textbf{x}_{i}\right)$
    which reduces the input dimensionality $d$ to $\check{d}$. The mutual distance
    of the observations in the original input space $\textbf{x}_{i}\in\mathbb{R}^{d}$
    , represented in the matrix $W\in\mathbb{R}^{n\times n}$, with $W_{i,j}=\delta\left(\textbf{x}_{i},\textbf{x}_{j}\right)$
    is meant to be preserved in the new feature space $\check{\textbf{x}}_{i}\in\mathbb{R}^{\check{d}}$.
    The multidimensional scaling algorithm developed in [[48](#bib.bib48)] is one
    of the first approaches to preserve the mutual distance of the embeddings of the
    observations.
  prefs: []
  type: TYPE_NORMAL
- en: 'More recently, in [[55](#bib.bib55)], a graph-based regularization was implemented
    in the Smooth Neighbors on Teacher Graphs Model (SNTGM). The model aims to smooth
    the consistency of the classifier along the observations in a cluster, and not
    only the artificially created observations by the previous consistency-based regularization
    techniques. The proposed approach implements both a consistency based regularization
    $\mathcal{L}_{c}$ with weight $\lambda_{1}$, and a guided embedding $\mathcal{L}_{e}$
    with coefficient $\lambda_{2}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{L}_{u}\left(f_{\textbf{w}},\textbf{x}_{i},\textbf{x}_{j},W_{i,j}\right)=\lambda_{1}\mathcal{L}_{c}\left(f_{\textbf{w}},\textbf{x}_{i}\right)+\lambda_{2}\mathcal{L}_{e}\left(f_{\textbf{w}},\textbf{x}_{i},\textbf{x}_{j},W\right)$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\textbf{x}_{i},\textbf{x}_{j}\in S^{(u)}$. $\mathcal{L}_{c}$ measures
    the prediction consistency, by using previous approaches in consistency based
    regularized techniques. The term $\mathcal{L}_{e}$ implements the observation
    embedding, with a $\gamma$ margin-restricted distance. To build the neighbourhood
    matrix $W$, the authors in [[55](#bib.bib55)] used label information instead of
    computing the distance between the observations. Regarding unlabelled observations
    in $S^{(u)}$, the authors estimated the output of the teacher to be $\hat{y}_{i}=f_{\textbf{w}^{\prime}}\left(\Psi^{\eta}\left(\textbf{x}_{i}\right)\right)$.
    Thus, the neighbourhood matrix is given as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W_{i,j}=\begin{cases}1&amp;\textrm{if }\hat{y}_{i}=\hat{y}_{j}\\ 0&amp;\textrm{if
    }\hat{y}_{i}\neq\hat{y}_{j}\end{cases}.$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: The loss term $\mathcal{L}_{e}$ encourages similar representations for observations
    within the same class and higher difference for representations of different classes.
    The algorithm was combined and tested with a Pi-M and VATM consistency functions,
    henceforth Smooth Neighbors on Teacher Graphs Model with Pi-Model regularization
    (SNTGM+Pi-M) and Smooth Neighbors on Teacher Graphs Model with Virtual Adversarial
    Training (SNTGM+VATM), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '| Model | Category | $n_{l}=2000$ | $n_{l}=4000$ | $n_{l}=5000$ |'
  prefs: []
  type: TYPE_TB
- en: '| Supervised only | Supervised | 33.94$\pm$0.73[[86](#bib.bib86)] | 20.02$\pm$0.6[[86](#bib.bib86)]
    | 18.02$\pm$0.6[[86](#bib.bib86)] |'
  prefs: []
  type: TYPE_TB
- en: '| Pi-M |  | 18.02$\pm$0.6[[86](#bib.bib86)] | 13.2$\pm$0.27[[49](#bib.bib49)]
    | 6.06$\pm$0.11[[49](#bib.bib49)] |'
  prefs: []
  type: TYPE_TB
- en: '| TEM |  | - | 12.16$\pm$0.24[[49](#bib.bib49)] | 5.6$\pm$0.1 [[49](#bib.bib49)]
    |'
  prefs: []
  type: TYPE_TB
- en: '| VATM+EM |  | - | 13.15$\pm$0.21[[59](#bib.bib59)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| VATM |  | - | 14.87$\pm$0.13[[59](#bib.bib59)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| MTM |  | 15.73$\pm$0.31[[86](#bib.bib86)] | 12.31$\pm$0.28[[86](#bib.bib86)]
    | 5.94$\pm$0.15[[68](#bib.bib68), [86](#bib.bib86)] |'
  prefs: []
  type: TYPE_TB
- en: '| SESEMI |  | 14.22$\pm$0.27[[89](#bib.bib89)] | 11.65$\pm$0.13[[89](#bib.bib89)]
    | - |'
  prefs: []
  type: TYPE_TB
- en: '| METM |  | - | 11.29$\pm$0.24[[72](#bib.bib72)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| TransM | RC-SSDL | 14.65$\pm$0.33[[77](#bib.bib77)] | 10.9$\pm$0.23[[77](#bib.bib77)]
    | 5.2$\pm$0.14[[77](#bib.bib77)] |'
  prefs: []
  type: TYPE_TB
- en: '| TransM+MTM |  | 13.54$\pm$0.32[[77](#bib.bib77)] | 9.3$\pm$0.55[[77](#bib.bib77)]
    | 5.19$\pm$0.14[[77](#bib.bib77)] |'
  prefs: []
  type: TYPE_TB
- en: '| MeM |  | - | 11.91$\pm$0.22[[19](#bib.bib19)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| MixMatch |  | - | 6.42$\pm$0.10[[9](#bib.bib9)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| ReMixMatch |  | - | 4.72$\pm$0.13[[8](#bib.bib8)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| FixMatch(RA) |  | - | 4.31$\pm$0.15 | - |'
  prefs: []
  type: TYPE_TB
- en: '| FixMatch(CTA) |  | - | 4.26$\pm$0.05 | - |'
  prefs: []
  type: TYPE_TB
- en: '| SNTGM+VATM | GR-SSDL | - | 12.49$\pm$0.36[[55](#bib.bib55)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| SNTGM+Pi-M |  | - | 13.62$\pm$0.17[[55](#bib.bib55)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| FM-GAN |  | - | 18.63$\pm$2.32[[23](#bib.bib23)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| CAT-GAN |  | - | 19.58$\pm$0.58[[81](#bib.bib81)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| Co-8-GAN | GaNC-SSDL | - | 8.35$\pm$0.06[[68](#bib.bib68)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| Bad-GAN |  | - | 14.41$\pm$0.3[[26](#bib.bib26)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| Triple-GAN |  | - | 16.99$\pm$0.36[[22](#bib.bib22)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| Tri-Net |  | - | 8.45$\pm$0.22[[29](#bib.bib29)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| SaaSM | PLT-SSDL | - | 10.94$\pm$0.07[[23](#bib.bib23)] | - |'
  prefs: []
  type: TYPE_TB
- en: '| TriNet+Pi |  | - | 8.3$\pm$0.15[[29](#bib.bib29)] | - |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE I: SSDL error rates (the lower the better) from literature of state of
    the art methods, using the CIFAR-10 dataset. As number of labels, $n_{l}=2000$,
    $n_{l}=4000$ and $n_{l}=5000$ were the most frequently used in the literature.'
  prefs: []
  type: TYPE_NORMAL
- en: III Dealing with distribution mismatch in SSDL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In [[63](#bib.bib63)] and [[15](#bib.bib15)], extensive evaluation of the distribution
    mismatch setting is developed. The authors agreed upon its decisive impact in
    the performance of SSDL methods and the consequent importance of increase their
    robustness to such phenomena. SSDL methods designed to deal with the distribution
    mismatch between $S^{(u)}$ and $S^{(l)}$ often use ideas and concepts from OOD
    detection techniques. Most methods for SSDL that are robust to distribution mismatch
    calculate a weight or a coefficient referred to as the function $\mathcal{H}\left(\textbf{x}_{j}^{u}\right)$
    in this article, to score how likely the unlabelled observation $\textbf{x}_{j}^{u}$
    is OOD. The score can be used to either completely discard $\textbf{x}_{j}^{u}$
    from the unlabelled training dataset (referred to as hard thresholding in this
    work) or to weigh it (soft thresholding). Thresholding the unlabelled dataset
    can take place as a data pre-processing step or in an online fashion during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, we first review modern approaches for OOD detection using deep learning
    in [Section III-A](#S3.SS1 "III-A OOD Data Detection ‣ III Dealing with distribution
    mismatch in SSDL ‣ Semi-supervised Deep Learning for Image Classification with
    Distribution Mismatch: A Survey"). Later we address state of the art SSDL methods
    that are robust to distribution mismatch in [Section III-B](#S3.SS2 "III-B Semi-supervised
    Deep Learning methods robust to distribution mismatch ‣ III Dealing with distribution
    mismatch in SSDL ‣ Semi-supervised Deep Learning for Image Classification with
    Distribution Mismatch: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: III-A OOD Data Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: OOD data detection is a classic challenge faced in machine learning applications.
    It corresponds to the detection of data observations which are far from the training
    dataset distribution [[38](#bib.bib38)]. Individual and collective outlier detection
    are particular problems of OOD detection [[79](#bib.bib79)]. Other particular
    OOD detection settings have been tackled in the literature such as novel data
    and anomaly detection [[65](#bib.bib65)] and infrequent event detection [[37](#bib.bib37),
    [1](#bib.bib1)]. Well studied and known concepts have been developed within the
    pattern recognition community related to OOD detection. Some of them are kernel
    representations [[87](#bib.bib87)], density estimation [[57](#bib.bib57)], robust
    moment estimation [[71](#bib.bib71)] and prototyping [[57](#bib.bib57)].
  prefs: []
  type: TYPE_NORMAL
- en: 'The more recent developments in the burgeoning field of deep learning for image
    analysis tasks have boosted the interest in developing OOD detection methods for
    deep learning architectures. According to our literature survey, we found that
    OOD detection methods for deep learning architectures can be classified into the
    following categories: DNN output based and DNN feature space based. In the next
    subsections we proceed to describe the most popular methods within each category.'
  prefs: []
  type: TYPE_NORMAL
- en: III-A1 DNN output based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In [[39](#bib.bib39)] the authors proposed a simple method known as Out of DIstribution
    detector for Neural networks (ODIN) to score input observations according to its
    OOD probability. The proposed method by the authors implements a confidence score
    based upon the DNN model’s output which is transformed using a softmax layer.
    The maximum softmax value of all the units is associated with the model’s confidence.
    The authors argued that this scores is able to distinguish in-distribution from
    OOD data.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, in [[53](#bib.bib53)], the authors argued that using the softmax
    output of a DNN model to estimate OOD probability can be often a misleading measure
    in non calibrated models. Therefore, the authors in [[53](#bib.bib53)] proposed
    a DNN calibration method. This method implements a temperature coefficient which
    aims to improve the model’s output discriminatory power between OOD and In-Distribution
    (IOD) data. In [[53](#bib.bib53)] the authors tested ODIN against the softmax
    based OOD score proposed in [[39](#bib.bib39)], with significantly better results
    obtained by ODIN.
  prefs: []
  type: TYPE_NORMAL
- en: An alternative approach for OOD detection using the DNN’s output is the popular
    approach known as Monte Carlo Dropout (MCD) [[54](#bib.bib54), [46](#bib.bib46)].
    This approach uses the distribution of $N$ model forward passes (input evaluation),
    using the same input observation with mild transformations (noise, flips, etc.)
    or injecting noise to the model using a parameter drop-out. The output distribution
    is used to calculate distribution moments (variance usually) or other scalar distribution
    descriptors such as the entropy. This idea has been implemented in OOD detection
    settings, as OOD observations might score higher entropy or variance values [[43](#bib.bib43),
    [75](#bib.bib75)].
  prefs: []
  type: TYPE_NORMAL
- en: III-A2 DNN’s feature space based
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recently, as an alternative approach for OOD detection, different methods use
    the feature or latent space for OOD detection. In [[51](#bib.bib51)] the authors
    propose the usage of the Mahalanobis distance in the feature space between the
    training dataset and the input observation. Therefore the covariance matrix and
    a mean observation is calculated from the training data (in-distribution data).
    By using the Mahalanobis distance, the proposed method by the authors assume a
    Gaussian distribution of the training data. Also, the proposed method was tested
    mixed with the ODIN calibration method previously discussed. The authors reported
    a superior performance of their method over the softmax based score proposed in
    [[39](#bib.bib39)] and ODIN [[53](#bib.bib53), [39](#bib.bib39)]. However no statistical
    significance analysis of the results was carried out.
  prefs: []
  type: TYPE_NORMAL
- en: In [[91](#bib.bib91)] another feature space based was proposed, referred to
    as Deterministic Uncertainty Quantification (DUQ) by the authors. The proposed
    method was tested for both uncertainty estimation and OOD detection. It consists
    in calculating a centroid for each one of the classes within the training dataset
    (IOD dataset). Later, for each new observation where either uncertainty estimation
    or OOD detection is intended to be used, the method calculates the distance to
    each centroid. The shortest distance is used as either uncertainty or OOD score.
    DUQ performance for OOD detection was compared against a variation of the MCD
    approach, with a an ensemble of networks for OOD detection. The authors claimed
    a better performance of DUQ for OOD detection, however no statistical analysis
    of the results was done. The benchmark consisted in using CIFAR-10 as an IOD dataset
    and SVHN as a OOD dataset. Therefore, as usual in OOD detection benchmarks, the
    unseen classes setting for the IID assumption violation was tested.
  prefs: []
  type: TYPE_NORMAL
- en: III-B Semi-supervised Deep Learning methods robust to distribution mismatch
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the literature, there are two most commonly studied causes for the violation
    of the IID assumption. The first one is the prior probability shift (different
    distribution of the labels) between $S^{(u)}$ and $S^{(l)}$. Novel methods proposed
    to deal with this challenge are described in this section. The other cause for
    the IID violation assumption is the unseen class setting, which has been more
    widely studied. State of the art methods are also discussed in this section.
  prefs: []
  type: TYPE_NORMAL
- en: III-B1 Unseen classes as a cause for the distribution mismatch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Most of the SSDL methods designed to deal with distribution mismatch have been
    tested using a labelled dataset with different classes (usually less) from the
    unlabelled dataset. For example, in this setting, for $S^{(l)}$ the SVHN is used,
    and for $S^{(u)}$ a percentage of the sample is drawn from the CIFAR-10 dataset,
    and the rest from the SVHN dataset. In this context, the dataset CIFAR-10 is often
    referred to as the OOD data contamination source. Benchmarks with varying degrees
    of data contamination for SSDL with distribution mismatch can be found in literature.
    In this section we describe the most recent approaches for SSDL under distribution
    mismatch with unseen classes in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: In [[61](#bib.bib61)] an SSDL method for dealing with distribution mismatch
    was developed. The authors refer to this method as RealMix. It was proposed as
    an extension of the MixMatch SSDL method. Therefore, it uses the consistency based
    regularization with augmented observations, and the MixUp data augmentation method
    implemented in MixMatch. For distribution mismatch robustness, RealMix uses the
    softmax of the output from the model as a confidence value, to score each unlabelled
    observation. During training, in the loss function, the unlabelled observations
    are masked out using such confidence score. The $\phi$ percent of unlabelled observations
    with the lowest confidence scores are discarded at each training epoch. To test
    their method, the authors deployed a benchmark based upon CIFAR-10 with a disjoint
    set of classes for $S^{(l)}$ and $S^{(u)}$. The reported results showed a slight
    accuracy gain of the proposed method against other SSDL approaches not designed
    for distribution mismatch robustness. A fixed number of labelled observations
    and CNN backbones were used. No statistical significance tests over the results
    were done. RealMix can be categorized as a DNN output based OOD scoring method.
    The thresholding is done during training, several times, using binary or hard
    thresholding (keep or discard). The testing can be considered limited as the OOD
    contamination source causes a hard distribution mismatch.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, the method known as Uncertainty Aware Self-Distillation (UASD),
    was proposed in [[20](#bib.bib20)] for SSDL distribution mismatch robustness.
    UASD uses an unsupervised regularized loss function. For each unlabelled observation,
    a pseudo-label is estimated as the average label from an ensemble of models. The
    ensemble is composed of past models yielded in previous training epochs. Similar
    to RealMix, UASD uses the output of a DNN model to score each unlabelled observation.
    However, to increase the robustness of such confidence score, UASD uses the ensemble
    of predictions from past models, to estimate the model’s confidence over its prediction
    for each unlabelled observation. The maximum logits of the ensemble prediction
    is used as the confidence score. Therefore we can categorize the UASD method as
    a DNN output based approach. Also in a similar fashion to RealMix, the estimated
    scores are used for hard-thresholding the unlabelled observations. In a resembling
    trend to RealMix, the authors of the UASD method evaluated their approach using
    the CIFAR-10 dataset. $S^{(l)}$ includes 6 classes of animals, and $S^{(u)}$ samples
    other 4 classes from CIFAR-10, with a varying degree of class distribution mismatch.
    Only five runs were performed to approximate the error-rate distribution, and
    no statistical analysis was done for the results. No varying number of labelled
    observations, or different DNN backbones were tested. UASD was compared with SSDL
    methods not designed for distribution mismatch robustness. From the reported results,
    an accuracy gain of up to 6 percent over previous SSDL methods was yielded by
    UASD, when facing heavy distribution mismatch settings.
  prefs: []
  type: TYPE_NORMAL
- en: In [[20](#bib.bib20)], an SSDL approach to deal with distribution mismatch was
    introduced. The authors refer to their proposed approach as Deep Safe Semi-Supervised
    Learning (D3SL). It implements an unsupervised regularization, through the mean
    square loss between the prediction of unlabelled observation and its noisy modification.
    An observation-wise weight for each unlabelled observation is implemented, similar
    to RealMix and UASD. However, the weights for the entire unlabelled dataset are
    calculated using an error gradient optimization approach. Both the model’s parameters
    and the observation-wise weights are estimated in two nested optimization steps.
    Therefore, we can categorize this method as a gradient optimized scoring of the
    unlabelled observations. The weights are continuous or non-binary values, therefore
    we can refer to this method as a softly-thresholded one. According to the authors,
    this increases training time up to $3\times$. The testing benchmark uses the CIFAR-10
    and MNIST datasets. For both of them, 6 classes are used to sample $S^{(l)}$,
    and the remaining for $S^{(u)}$. Only a Wide ResNet-28-10 CNN backbone was used
    with a fixed number of labels. A varying degree of OOD contamination was tested.
    The proposed D3SL method was compared with generic SSDL methods, therefore ignoring
    previous SSDL robust methods to distribution mismatch. From the reported results,
    an averaged accuracy gain of around 2% was yielded by the proposed method under
    the heaviest OOD data contamination settings, with no statistical significance
    reported. Only five runs were done to report such averaged error-rates.
  prefs: []
  type: TYPE_NORMAL
- en: 'A similar gradient optimization based method to D3SL can be found in [[95](#bib.bib95)].
    The proposed method is referred to as a Multi-Task Curriculum Framework (MTCF)
    by the authors. Similar to previous methods, MTCF defines an OOD score for the
    unlabelled observations, as an extension to the MixMatch algorithm [[9](#bib.bib9)].
    Such scores are alternately optimized together with the DNN parameters, as seen
    in D3SL. However, the optimization problem is perhaps more simple than the D3SL,
    as the OOD scores are not optimized in a gradient descent fashion directly. Instead,
    the DNN output is used as the OOD score. The usage of a loss function that includes
    the OOD scores, enforces a new condition to the optimization of the DNN parameters.
    This is referred to as a curriculum multi-task learning framework by the authors
    in [[95](#bib.bib95)]. The proposed method was tested in what the authors defined
    as an Open-set semi-supervised learning setting (Open-Set-SSLS), where different
    OOD data contamination sources were used. Regarding the specific benchmarking
    settings, the authors only tested a Wide ResNet DNN backbone, to compare a baseline
    MixMatch method to their proposed approach. No comparison with other SSDL methods
    was performed. The authors used two IOD datasets: CIFAR-10 and SVHN. Four different
    OOD datasets were used: Uniform, Gaussian noise, Tiny ImageNet (TIN) and Large-scale
    Scene Understanding dataset (LSUN). The average of the last 10 checkpoints of
    the model training, using the same partitions was reported (no different partitions
    were tested). A fixed OOD data contamination degree was tested. The reported accuracy
    gains went from 1% to 10%. The usage of the same data partitions inhibited an
    appropriate statistical analysis of the results.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the same trend, authors in [[100](#bib.bib100)] proposed a gradient optimization
    based method to calculate the observation-wise weights for data in $S^{(u)}$.
    Two different gradient approximation methods were tested: Implicit Differentiation
    (IF) and Meta Approximation (MetaA). Authors argue that finding the weights for
    each unlabelled observation in a large sample $S^{(u)}$ is an intractable problem.
    Therefore, both tested methods aim to reduce the computational cost of optimizing
    such weights. Moreover, to further reduce the number of weights to find, the method
    performs a clustering in the feature space. This reduces the number of weights
    to find, as one weight is assigned per cluster. Another interesting finding reported
    by the authors, is the impact of OOD data in batch normalization. Even if the
    OOD data lies far to the decision boundary, if batch normalization is carried
    out, a degradation of performance is likely. If no batch normalization is performed,
    OOD data far from the decision boundary might not significantly harm performance.
    Therefore, the weights found are also used to perform a weighted mini-batch normalization
    of the data. Regarding the benchmarking of the proposed method, the authors used
    the CIFAR-10 and FashionMNIST datasets, with different degrees of OOD contamination.
    The OOD data was sampled from a set of classes excluded from the IOD dataset.
    A WRN-28-2 (WideResNet) backbone was used. No statistical analysis of the results,
    with the same number of partitions across the tested methods was performed. The
    average accuracy gains show a positive margin for the proposed method ranging
    from 5% to 20%.'
  prefs: []
  type: TYPE_NORMAL
- en: Another approach for robust SSDL to distribution mismatch was proposed in [[93](#bib.bib93)],
    referred to by the authors as Augmented Distribution Alignment (ADA). Similar
    to MixMatch, ADA uses MixMup [[99](#bib.bib99)] for data augmentation. The method
    includes an unsupervised regularization term which measures the distribution divergence
    between the $S^{(u)}$ and $S^{(l)}$ datasets. This divergence is measured in the
    feature space. In order to diminish the empirical distribution mismatch of $S^{(u)}$
    and $S^{(l)}$, the distribution distance of both datasets is minimized to build
    a feature extractor aiming for a latent space where both feature densities are
    aligned. This is done through adversarial loss optimization. We can categorize
    this method as a feature space based method. As for the reported benchmarks, the
    authors did not test different degrees of OOD data contamination, and only compared
    their method to generic SSDL methods, not designed to handle distribution mismatch.
    No statistical significance tests were done to measure the confidence of their
    accuracy gains. In average, the proposed method seems to improve the error-rate
    from 0.5 to 2%, when compared to other SSDL generic methods. These results do
    not ensure a practical accuracy gain, as no statistical analysis was performed.
    From the baseline model with no distribution alignment, an accuracy gain of around
    5% was reported, again with no statistical meaning analysis performed. The authors
    in [[11](#bib.bib11)] also used the feature space to score unlabelled observations.
    The proposed method was tested in the specific application setting of COVID-19
    detection using chest X-ray images.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, in [[40](#bib.bib40)], an SSDL approach to distribution mismatch robustness
    was developed. The method consists of two training steps. The first or warm-up
    step performs a self-training phase, where a pretext task is optimized using the
    DNN backbone. This is implemented as the prediction of the degrees of rotation
    that each image, from a rotationally augmented dataset. This includes observations
    from both data samples $S^{(u)}$ and $S^{(l)}$, (along with the OOD observations).
    In the second step, the model is trained using a consistency based regularization
    approach for the unlabelled data. This consistency regularization also uses the
    rotation consistency loss. In this step, an OOD filtering method is implemented,
    referred by the authors as a cross-modal mechanism. This consists of the prediction
    of a pseudo-label, defined as the softmax of the DNN output. This pseudo-label,
    along its feature embedding, is fed to what the authors refer to as a matching
    head. Such matching head consists of a multi-perceptron model that is trained
    to estimate whether the pseudo-label is accurately matched to its embedding. The
    matching head model is trained with the labelled data, with different matching
    combinations of the labels and the observations. As for the testing benchmark,
    the authors used CIFAR-10, Animals-10 and CIFAR-ID-50 as IOD datasets. For OOD
    data sources, images of Gaussian and Uniform noise, along with the TIN and LSUN
    datasets were used. The average accuracy reported for all the tested methods correspond
    to the last 20 model copies yielded during training. Therefore, no different training
    partitions were tested, preventing an adequate statistical analysis of the results.
    The average results were not significantly better when compared to other generic
    SSDL methods such as FixMatch, with an accuracy gain of around 0.5% to 3%. No
    computational cost figures about the cost of training the additional matching
    head or warm-up training were provided. The authors claim that in their method,
    OOD data is re-used. However other methods like UASD also prevent totally discarding
    OOD data, as dynamic and soft observation-wise weights are calculated every epoch.
    Perhaps, from our point of view, a more appropriate description of the novelty
    of their method could be referred to as the complete usage of OOD data in a pre-training
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: Prior probability shift
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Data imbalance for supervised approaches has been tackled in the literature
    widely. Different approaches have been proposed, ranging from data transformations
    (over-sampling, data augmentation, etc.) to model architecture focused approaches
    (i. e., modification of thee loss function, etc.) [[2](#bib.bib2), [83](#bib.bib83),
    [60](#bib.bib60)]. Nevertheless, related to the problem of label imbalance or
    label balance mismatch between the labelled and unlabelled datasets, more scarce
    is found in the literature. This setting can be interpreted as a particularisation
    of the distribution mismatch problem described in [[63](#bib.bib63)]. A distribution
    mismatch between $S^{(l)}$ and $S^{(u)}$ might arise when the label or class membership
    distribution of the observations in both datasets meaningfully differ.
  prefs: []
  type: TYPE_NORMAL
- en: In [[41](#bib.bib41)], an assessment of how distribution mismatch impacts a
    SSDL model is carried out. The cause of the distribution mismatch between the
    labelled and unlabelled datasets was the label imbalance difference between them.
    An accuracy decrease between 2% and 10% was measured when the SSDL faced such
    data setting. The authors proposed a simple method to recover such performance
    degradation. The method consists on assigning a specific weight for each unlabelled
    observation in the loss term. To choose the weight, the output unit of the model
    with highest score at the current epoch is used as a label prediction. In this
    work, the mean teacher model was tested as a SSDL approach [[86](#bib.bib86)].
    The authors yielded a superior performance of the SSDL model by using the proposed
    method. An extension to the work in [[41](#bib.bib41)] is found in [[16](#bib.bib16)],
    where in this case the more recent MixMatch SSDL method is modified to improve
    the robustness of the model to heavy imbalance conditions in the labelled dataset.
    The approach was extensively tested in the specific application of COVID-19 detection
    using chest X-ray images.
  prefs: []
  type: TYPE_NORMAL
- en: IV Open challenges
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Among the most important challenges faced by SSDL under practical usage situations
    is the distribution mismatch between the labelled and unlabelled datasets. However,
    according to our state of the art review, there is significant work pending, mostly
    related to the implementation of standard benchmarks for novel methods. The benchmarks
    found so far in the literature show a significant bias towards the unseen class
    distribution mismatch setting. No testing of other distribution mismatch causes
    such as covariate shift was found in the literature. Real world usage settings
    might include covariate shift and prior probability distribution shift, which
    violate the frequently used IID assumption. Therefore, we urge the community to
    focus on different distribution mismatch causes.
  prefs: []
  type: TYPE_NORMAL
- en: Studying and developing methods for dealing with distribution mismatch settings,
    shifts the focus upon data-oriented (i.e., data transformation, filtering and
    augmentation) methods instead of more popular model-oriented methods. Recently,
    the renowned researcher Andrew Ng, has drawn the attention towards data-oriented
    methods ¹¹1[https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=68b63b2174f5](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=68b63b2174f5).
    In his view, not enough effort has been carried out by the community in studying
    and developing data-oriented methods to face real-world usage settings.
  prefs: []
  type: TYPE_NORMAL
- en: We agree with Andrew Ng’s opinion, and add that besides establishing and testing
    a set of standard benchmarks where different distribution mismatch settings are
    tested, experimental reproducibility must be enforced. Recent technological advances
    not only allow to share the code and the datasets used, but also the testing environments
    through virtualization and container technology. Finally, we argue that the deep
    learning research community must be mindful of not only comparing average accuracies
    from the different state-of-the art methods. Statistical analysis tools must be
    used to test whether the performance difference between one method over another
    is reproducible and is statistically meaningful. Therefore we suggest that the
    results distribution is shared and not only the means and standard deviations
    of the results, in order to enable further statistical analysis.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman,
    and Dan Mané. Concrete problems in ai safety. arXiv preprint arXiv:1606.06565,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Shin Ando and Chun Yuan Huang. Deep over-sampling framework for classifying
    imbalanced data. In Joint European Conference on Machine Learning and Knowledge
    Discovery in Databases, pages 770–785\. Springer, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles.
    In Advances in Neural Information Processing Systems, pages 3365–3373, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Maria-Florina Balcan and Avrim Blum. 21 an augmented pac model for semi-supervised
    learning. 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Indranil Balki, Afsaneh Amirabadi, Jacob Levman, Anne L Martel, Ziga Emersic,
    Blaz Meden, Angel Garcia-Pedrero, Saul C Ramirez, Dehan Kong, Alan R Moody, et al.
    Sample-size determination methodologies for machine learning in medical imaging
    research: A systematic review. Canadian Association of Radiologists Journal, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Yoshua Bengio, Pascal Lamblin, Dan Popovici, and Hugo Larochelle. Greedy
    layer-wise training of deep networks. In Advances in neural information processing
    systems, pages 153–160, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] Ariana Bermudez, Saul Calderon-Ramirez, Trevor Thang, Pascal Tyrrell, Armaghan
    Moemeni, Shengxiang Yang, and Jordina Torrents-Barrena. Quality assessment of
    dental photostimulable phosphor plates with deep learning. Institute of Electrical
    and Electronics Engineers, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex Kurakin, Kihyuk Sohn,
    Han Zhang, and Colin Raffel. Remixmatch: Semi-supervised learning with distribution
    alignment and augmentation anchoring. arXiv preprint arXiv:1911.09785, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital
    Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning.
    In Advances in Neural Information Processing Systems, pages 5050–5060, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S Calderon, F Fallas, M Zumbado, PN Tyrrell, H Stark, Ziga Emersic, Blaz
    Meden, and M Solis. Assessing the impact of the deceived non local means filter
    as a preprocessing stage in a convolutional neural network based approach for
    age estimation using digital hand x-ray images. In 2018 25th IEEE International
    Conference on Image Processing (ICIP), pages 1752–1756\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Saul Calderon-Ramirez, Raghvendra Giri, Shengxiang Yang, Armaghan Moemeni,
    Mario Umana, David Elizondo, Jordina Torrents-Barrena, and Miguel A Molina-Cabello.
    Dealing with scarce labelled data: Semi-supervised deep learning with mix match
    for covid-19 detection using chest x-ray images. In 2020 25th International Conference
    on Pattern Recognition (ICPR), pages 5294–5301\. IEEE, Jan. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Saul Calderon-Ramirez and Erick Mata-Montero. A first glance into reversing
    senescence on herbarium sample images through conditional generative adversarial
    networks. In High Performance Computing: 6th Latin American Conference, CARLA
    2019, Turrialba, Costa Rica, September 25–27, 2019, Revised Selected Papers, volume
    1087, page 438\. Springer Nature, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Saul Calderon-Ramirez, Diego Murillo-Hernandez, Kevin Rojas-Salazar, Luis-Alexander
    Calvo-Valverde, Shengxiang Yang, Armaghan Moemeni, David Elizondo, Ezequiel Lopez-Rubio,
    and Miguel Molina-Cabello. Improving uncertainty estimations for mammogram classification
    using semi-supervised learning. In Institute of Electrical and Electronics Engineers,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Saul Calderon-Ramirez, Diego Murillo-Hernandez, Kevin Rojas-Salazar, David
    Elizondo, Shengxiang Yang, and Miguel Molina-Cabello. A real use case of semi-supervised
    learning for mammogram classification in a local clinic of costa rica. arXiv preprint
    arXiv:2107.11696, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Saul Calderon-Ramirez, Luis Oala, Jordina Torrents-Barrena, Shengxiang
    Yang, Armaghan Moemeni, Wojciech Samek, and Miguel A Molina-Cabello. Mixmood:
    A systematic approach to class distribution mismatch in semi-supervised learning
    using deep dataset dissimilarity measures. arXiv preprint arXiv:2006.07767, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Saul Calderon-Ramirez, Shengxiang Yang, Armaghan Moemeni, David Elizondo,
    Simon Colreavy-Donnelly, Luis Fernando Chavarría-Estrada, and Miguel A Molina-Cabello.
    Correcting data imbalance for semi-supervised covid-19 detection using x-ray chest
    images. Applied Soft Computing, 111:107692, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] Iván Calvo, Saul Calderon-Ramirez, Jordina Torrents-Barrena, Erick Muñoz,
    and Domenec Puig. Assessing the impact of a preprocessing stage on deep learning
    architectures for breast tumor multi-class classification with histopathological
    images. In Latin American High Performance Computing Conference, pages 262–275\.
    Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Mathilde Caron, Piotr Bojanowski, Julien Mairal, and Armand Joulin. Unsupervised
    pre-training of image features on non-curated data. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pages 2959–2968, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] Yanbei Chen, Xiatian Zhu, and Shaogang Gong. Semi-supervised deep learning
    with memory. In Proceedings of the European Conference on Computer Vision (ECCV),
    pages 268–283, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Yanbei Chen, Xiatian Zhu, Wei Li, and Shaogang Gong. Semi-supervised learning
    under class distribution mismatch. In AAI, pages 3569–3576, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Veronika Cheplygina, Marleen de Bruijne, and Josien PW Pluim. Not-so-supervised:
    a survey of semi-supervised, multi-instance, and transfer learning in medical
    image analysis. Medical image analysis, 54:280–296, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] LI Chongxuan, Taufik Xu, Jun Zhu, and Bo Zhang. Triple generative adversarial
    nets. In Advances in neural information processing systems, pages 4088–4098, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Safa Cicek, Alhussein Fawzi, and Stefano Soatto. Saas: Speed as a supervisor
    for semi-supervised learning. In Proceedings of the European Conference on Computer
    Vision (ECCV), pages 149–163, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V
    Le. Autoaugment: Learning augmentation policies from data. arXiv preprint arXiv:1805.09501,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment:
    Practical automated data augmentation with a reduced search space. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,
    pages 702–703, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and Ruslan R Salakhutdinov.
    Good semi-supervised learning that requires a bad gan. In Advances in neural information
    processing systems, pages 6510–6520, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei.
    Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on
    computer vision and pattern recognition, pages 248–255\. Ieee, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsupervised visual representation
    learning by context prediction. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 1422–1430, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] WeiWang Dong-DongChen and Zhi-HuaZhou WeiGao. Tri-net for semi-supervised
    deep learning. IJCAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] Dominic B Dwyer, Peter Falkai, and Nikolaos Koutsouleris. Machine learning
    approaches for clinical psychology and psychiatry. Annual review of clinical psychology,
    14:91–118, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Benoît Frénay and Michel Verleysen. Classification in the presence of
    label noise: a survey. IEEE transactions on neural networks and learning systems,
    25(5):845–869, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Maayan Frid-Adar, Eyal Klang, Michal Amitai, Jacob Goldberger, and Hayit
    Greenspan. Synthetic data augmentation using gan for improved liver lesion classification.
    In 2018 IEEE 15th international symposium on biomedical imaging (ISBI 2018), pages
    289–293\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Angel Garcia-Pedrero, Ana I García-Cervigón, José M Olano, Miguel García-Hidalgo,
    Mario Lillo-Saavedra, Consuelo Gonzalo-Martín, Cristina Caetano, and Saúl Calderón-Ramírez.
    Convolutional neural networks for segmenting xylem vessels in stained cross-sectional
    images. Neural Computing and Applications, pages 1–13, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Andrew B Goldberg, Xiaojin Zhu, and Stephen Wright. Dissimilarity in graph-based
    semi-supervised classification. In Artificial Intelligence and Statistics, pages
    155–162, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] Ian Goodfellow, Yoshua Bengio, and Aaron Courville. Deep learning. MIT
    press, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
    Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets.
    In Advances in neural information processing systems, pages 2672–2680, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Ryuhei Hamaguchi, Ken Sakurada, and Ryosuke Nakamura. Rare event detection
    using disentangled representation learning. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 9327–9335, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified
    and out-of-distribution examples in neural networks. arXiv preprint arXiv:1610.02136,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified
    and out-of-distribution examples in neural networks. CoRR, abs/1610.02136, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Junkai Huang, Chaowei Fang, Weikai Chen, Zhenhua Chai, Xiaolin Wei, Pengxu
    Wei, Liang Lin, and Guanbin Li. Trash to treasure: Harvesting ood data with cross-modal
    matching for open-set semi-supervised learning. arXiv preprint arXiv:2108.05617,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Minsung Hyun, Jisoo Jeong, and Nojun Kwak. Class-imbalanced semi-supervised
    learning. arXiv preprint arXiv:2002.06815, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Vladimir I Iglovikov, Alexander Rakhlin, Alexandr A Kalinin, and Alexey A
    Shvets. Paediatric bone age assessment using deep convolutional neural networks.
    In Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical
    Decision Support, pages 300–308\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Baihong Jin, Yingshui Tan, Yuxin Chen, and Alberto Sangiovanni-Vincentelli.
    Augmenting monte carlo dropout classification models with unsupervised learning
    tasks for detecting and diagnosing out-of-distribution faults. arXiv preprint
    arXiv:1909.04202, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Longlong Jing and Yingli Tian. Self-supervised visual feature learning
    with deep neural networks: A survey. IEEE transactions on pattern analysis and
    machine intelligence, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Peter Kairouz, H Brendan McMahan, Brendan Avent, Aurélien Bellet, Mehdi
    Bennis, Arjun Nitin Bhagoji, Keith Bonawitz, Zachary Charles, Graham Cormode,
    Rachel Cummings, et al. Advances and open problems in federated learning. arXiv
    preprint arXiv:1912.04977, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian
    deep learning for computer vision? In Proceedings of the 31st International Conference
    on Neural Information Processing Systems, pages 5580–5590, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] Kyeongbo Kong, Junggi Lee, Youngchul Kwak, Minsung Kang, Seong Gyun Kim,
    and Woo-Jin Song. Recycling: Semi-supervised learning with noisy labels in deep
    neural networks. IEEE Access, 7:66998–67005, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Joseph B Kruskal. Multidimensional scaling by optimizing goodness of fit
    to a nonmetric hypothesis. Psychometrika, 29(1):1–27, 1964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning.
    arXiv preprint arXiv:1610.02242, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Hye-Woo Lee, Noo-ri Kim, and Jee-Hyong Lee. Deep neural network self-training
    based on unsupervised learning and dropout. International Journal of Fuzzy Logic
    and Intelligent Systems, 17(1):1–9, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework
    for detecting out-of-distribution samples and adversarial attacks. In Advances
    in Neural Information Processing Systems, pages 7167–7177, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Wenyuan Li, Zichen Wang, Jiayun Li, Jennifer Polson, William Speier, and
    Corey Arnold. Semi-supervised learning based on generative adversarial network:
    a comparison between good gan and bad gan approach. arXiv preprint arXiv:1905.06484,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant. Enhancing the reliability
    of out-of-distribution image detection in neural networks. In 6th International
    Conference on Learning Representations, ICLR 2018, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Antonio Loquercio, Mattia Segu, and Davide Scaramuzza. A general framework
    for uncertainty estimation in deep learning. IEEE Robotics and Automation Letters,
    5(2):3153–3160, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang. Smooth neighbors
    on teacher graphs for semi-supervised learning. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 8896–8905, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Mingqi Lv, Ling Chen, Tieming Chen, and Gencai Chen. Bi-view semi-supervised
    learning based semantic human activity recognition using accelerometers. IEEE
    Transactions on Mobile Computing, 17(9):1991–2001, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Markos Markou and Sameer Singh. Novelty detection: a review—part 1: statistical
    approaches. Signal processing, 83(12):2481–2497, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Mauro Mendez, Saul Calderon-Ramirez, and Pascal Tyrrell. Using cluster
    analysis to assess the impact of dataset heterogeneity on deep convolutional network
    accuracy: A first glance. In Latin America High Performance Computing Conference
    (CARLA) 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Takeru Miyato, Shin-ichi Maeda, Masanori Koyama, and Shin Ishii. Virtual
    adversarial training: a regularization method for supervised and semi-supervised
    learning. IEEE transactions on pattern analysis and machine intelligence, 41(8):1979–1993,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Sankha Subhra Mullick, Shounak Datta, and Swagatam Das. Generative adversarial
    minority oversampling. In Proceedings of the IEEE International Conference on
    Computer Vision, pages 1695–1704, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Varun Nair, Javier Fuentes Alonso, and Tony Beltramelli. Realmix: Towards
    realistic semi-supervised deep learning algorithms. arXiv preprint arXiv:1912.08766,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Luis Oala, Jana Fehr, Luca Gilli, Pradeep Balachandran, Alixandro Werneck
    Leite, Saul Calderon-Ramirez, Danny Xie Li, Gabriel Nobis, Erick Alejandro Muñoz
    Alvarado, Giovanna Jaramillo-Gutierrez, et al. Ml4h auditing: From paper to practice.
    In Machine Learning for Health, pages 280–317\. PMLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Avital Oliver, Augustus Odena, Colin A Raffel, Ekin Dogus Cubuk, and Ian
    Goodfellow. Realistic evaluation of deep semi-supervised learning algorithms.
    In Advances in Neural Information Processing Systems, pages 3235–3246, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang,
    Zachary DeVito, Zeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic
    differentiation in pytorch. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Pramuditha Perera and Vishal M Patel. Deep transfer learning for multiple
    class novelty detection. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition, pages 11544–11552, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Nitin Namdeo Pise and Parag Kulkarni. A survey of semi-supervised learning
    methods. In 2008 International Conference on Computational Intelligence and Security,
    volume 2, pages 30–34\. IEEE, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] V Jothi Prakash and Dr LM Nithya. A survey on semi-supervised learning
    techniques. arXiv preprint arXiv:1402.4645, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and Alan Yuille. Deep
    co-training for semi-supervised image recognition. In Proceedings of the European
    Conference on Computer Vision (ECCV), pages 135–152, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Saúl Calderón Ramírez, Raghvendra Giri, Shengxiang Yang, Armaghan Moemeni,
    Mario Umaña, David A Elizondo, Jordina Torrents-Barrena, and Miguel A Molina-Cabello.
    Dealing with scarce labelled data: Semi-supervised deep learning with mix match
    for covid-19 detection using chest x-ray images. In ICPR, pages 5294–5301, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Salah Rifai, Yann N Dauphin, Pascal Vincent, Yoshua Bengio, and Xavier
    Muller. The manifold tangent classifier. In Advances in neural information processing
    systems, pages 2294–2302, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Peter J. Rousseeuw. Least median of squares regression. Journal of the
    American Statistical Association, 79(388):871–880, 1984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Mehdi Sajjadi, Mehran Javanmardi, and Tolga Tasdizen. Regularization with
    stochastic transformations and perturbations for deep semi-supervised learning.
    In Advances in Neural Information Processing Systems, pages 1163–1171, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford,
    and Xi Chen. Improved techniques for training gans. In Advances in neural information
    processing systems, pages 2234–2242, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Lars Schmarje, Monty Santarossa, Simon-Martin Schröder, and Reinhard Koch.
    A survey on semi-, self-and unsupervised learning for image classification. IEEE
    Access, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Andreas Sedlmeier, Thomas Gabor, Thomy Phan, and Lenz Belzner. Uncertainty-based
    out-of-distribution detection in deep reinforcement learning. Digitale Welt, 4(1):74–78,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Behzad M Shahshahani and David A Landgrebe. The effect of unlabeled samples
    in reducing the small sample size problem and mitigating the hughes phenomenon.
    IEEE Transactions on Geoscience and remote sensing, 32(5):1087–1095, 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng MaXiaoyu Tao, and Nanning
    Zheng. Transductive semi-supervised deep learning using min-max features. In Proceedings
    of the European Conference on Computer Vision (ECCV), pages 299–315, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Hoo-Chang Shin, Neil A Tenenholtz, Jameson K Rogers, Christopher G Schwarz,
    Matthew L Senjem, Jeffrey L Gunter, Katherine P Andriole, and Mark Michalski.
    Medical image synthesis for data augmentation and anonymization using generative
    adversarial networks. In International Workshop on Simulation and Synthesis in
    Medical Imaging, pages 1–11\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Karanjit Singh and Shuchita Upadhyaya. Outlier detection: applications
    and techniques. International Journal of Computer Science Issues (IJCSI), 9(1):307,
    2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang,
    Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch:
    Simplifying semi-supervised learning with consistency and confidence. Advances
    in Neural Information Processing Systems, 33, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] Jost Tobias Springenberg. Unsupervised and semi-supervised learning with
    categorical generative adversarial networks. arXiv preprint arXiv:1511.06390,
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Wenqing Sun, Tzu-Liang Bill Tseng, Jianying Zhang, and Wei Qian. Enhancing
    deep convolutional neural network scheme for breast cancer diagnosis with unlabeled
    data. Computerized Medical Imaging and Graphics, 57:4–9, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Aboozar Taherkhani, Georgina Cosma, and TM McGinnity. Adaboost-cnn: an
    adaptive boosting algorithm for convolutional neural networks to classify multi-class
    imbalanced datasets using transfer learning. Neurocomputing, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Chuanqi Tan, Fuchun Sun, Tao Kong, Wenchang Zhang, Chao Yang, and Chunfang
    Liu. A survey on deep transfer learning. In International Conference on Artificial
    Neural Networks, pages 270–279\. Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Jeremy Tan, Anselm Au, Qingjie Meng, and Bernhard Kainz. Semi-supervised
    learning of fetal anatomy from ultrasound. In Domain Adaptation and Representation
    Transfer and Medical Image Learning with Less Labels and Imperfect Data, pages
    157–164. Springer, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Antti Tarvainen and Harri Valpola. Mean teachers are better role models:
    Weight-averaged consistency targets improve semi-supervised deep learning results.
    In Advances in neural information processing systems, pages 1195–1204, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] David M. J. Tax and Robert P. W. Duin. Support vector data description.
    Mach. Learn., 54(1):45–66, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Lisa Torrey and Jude Shavlik. Transfer learning. In Handbook of research
    on machine learning applications and trends: algorithms, methods, and techniques,
    pages 242–264\. IGI Global, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Phi Vu Tran. Semi-supervised learning with self-supervised networks. arXiv
    preprint arXiv:1906.10343, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Leslie G Valiant. A theory of the learnable. In Proceedings of the sixteenth
    annual ACM symposium on Theory of computing, pages 436–445\. ACM, 1984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Joost van Amersfoort, Lewis Smith, Yee Whye Teh, and Yarin Gal. Simple
    and scalable epistemic uncertainty estimation using a single deep deterministic
    neural network. arXiv e-prints, Jun. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Jesper E Van Engelen and Holger H Hoos. A survey on semi-supervised learning.
    Machine Learning, 109(2):373–440, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Qin Wang, Wen Li, and Luc Van Gool. Semi-supervised learning by augmented
    distribution alignment. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pages 1466–1475, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Tong Xiao, Tian Xia, Yi Yang, Chang Huang, and Xiaogang Wang. Learning
    from massive noisy labeled data for image classification. In Proceedings of the
    IEEE conference on computer vision and pattern recognition, pages 2691–2699, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Qing Yu, Daiki Ikami, Go Irie, and Kiyoharu Aizawa. Multi-task curriculum
    framework for open-set semi-supervised learning. In European Conference on Computer
    Vision, pages 438–454. Springer, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Xingrui Yu, Xiaomin Wu, Chunbo Luo, and Peng Ren. Deep learning in remote
    sensing scene classification: a data augmentation enhanced convolutional neural
    network framework. GIScience & Remote Sensing, 54(5):741–758, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Willard Zamora-Cárdenas, Mauro Mendez, Saul Calderon-Ramirez, Martin Vargas,
    Gerardo Monge, Steve Quiros, David Elizondo, Jordina Torrents-Barrena, and Miguel A
    Molina-Cabello. Enforcing morphological information in fully convolutional networks
    to improve cell instance segmentation in fluorescence microscopy images. In International
    Work-Conference on Artificial Neural Networks, pages 36–46\. Springer, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l:
    Self-supervised semi-supervised learning. In Proceedings of the IEEE international
    conference on computer vision, pages 1476–1485, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup:
    Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Xujiang Zhao, Killamsetty Krishnateja, Rishabh Iyer, and Feng Chen. Robust
    semi-supervised learning with out of distribution data. arXiv preprint arXiv:2010.03658,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Xiaojin Jerry Zhu. Semi-supervised learning literature survey. Technical
    report, University of Wisconsin-Madison Department of Computer Sciences, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] Xinyue Zhu, Yifan Liu, Jiahong Li, Tao Wan, and Zengchang Qin. Emotion
    classification with data augmentation using generative adversarial networks. In
    Pacific-Asia Conference on Knowledge Discovery and Data Mining, pages 349–360\.
    Springer, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
