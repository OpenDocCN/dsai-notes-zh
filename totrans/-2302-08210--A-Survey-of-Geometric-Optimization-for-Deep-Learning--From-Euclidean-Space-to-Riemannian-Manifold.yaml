- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 19:41:53'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 19:41:53'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2302.08210] A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2302.08210] 深度学习的几何优化调查：从欧几里得空间到黎曼流形'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08210](https://ar5iv.labs.arxiv.org/html/2302.08210)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08210](https://ar5iv.labs.arxiv.org/html/2302.08210)
- en: 'A Survey of Geometric Optimization for Deep Learning: From Euclidean Space
    to Riemannian Manifold'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的几何优化调查：从欧几里得空间到黎曼流形
- en: Yanhong Fei, Xian Wei, Yingjie Liu, Zhengyu Li, Mingsong Chen
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 颜洪飞、魏贤、刘英杰、李正宇、陈铭松
- en: Software Engineering Institute, East China Normal University
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 软件工程研究所，华东师范大学
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Although Deep Learning (DL) has achieved success in complex Artificial Intelligence
    (AI) tasks, it suffers from various notorious problems (e.g., feature redundancy,
    and vanishing or exploding gradients), since updating parameters in Euclidean
    space cannot fully exploit the geometric structure of the solution space. As a
    promising alternative solution, Riemannian-based DL uses geometric optimization
    to update parameters on Riemannian manifolds and can leverage the underlying geometric
    information. Accordingly, this article presents a comprehensive survey of applying
    geometric optimization in DL. At first, this article introduces the basic procedure
    of the geometric optimization, including various geometric optimizers and some
    concepts of Riemannian manifold. Subsequently, this article investigates the application
    of geometric optimization in different DL networks in various AI tasks, e.g.,
    convolution neural network, recurrent neural network, transfer learning, and optimal
    transport. Additionally, typical public toolboxes that implement optimization
    on manifold are also discussed. Finally, this article makes a performance comparison
    between different deep geometric optimization methods under image recognition
    scenarios.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管深度学习（DL）在复杂的人工智能（AI）任务中取得了成功，但由于在欧几里得空间中更新参数无法充分利用解空间的几何结构，它仍然面临各种恶名昭彰的问题（例如特征冗余以及梯度消失或爆炸）。作为一种有前景的替代解决方案，基于黎曼几何的深度学习利用几何优化在黎曼流形上更新参数，并能够利用底层几何信息。因此，本文提供了对在深度学习中应用几何优化的全面调查。首先，本文介绍了几何优化的基本过程，包括各种几何优化器和一些黎曼流形的概念。随后，本文调查了几何优化在各种人工智能任务中的不同深度学习网络中的应用，例如卷积神经网络、递归神经网络、迁移学习和最优传输。此外，本文还讨论了实现流形优化的典型公共工具箱。最后，本文在图像识别场景下对不同的深度几何优化方法进行了性能比较。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: With increasing computing power, deep neural networks optimized in Euclidean
    space have achieved remarkable success from computer vision to natural language
    processing (e.g., autonomous driving and protein structure prediction) [[1](#bib.bib1),
    [2](#bib.bib2)]. However, to fully exploit the valuable information hidden in
    the data, most deep learning models tend to increase the capacity of their networks,
    either by widening the existing layers or by adding more layers [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]. For example, models often contain hundreds of
    convolution and pooling layers with various activation functions and multiple
    fully connected layers, producing millions or billions of parameters during training.
    These massive parameters associated with complex model architectures challenge
    the optimization of deep learning networks. As an alternative paradigm, optimization
    on the Riemannian manifold exploits hidden valuable information by utilizing geometric
    properties of parameters, rather than increasing the network capacity. Therefore,
    geometric optimization can alleviate over-parameterization and feature redundancy
    problems. For example, deep learning models trained on the orthogonal manifold
    have less correlated parameters, making features much less redundant [[6](#bib.bib6)].
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 随着计算能力的提升，在欧几里得空间中优化的深度神经网络在从计算机视觉到自然语言处理（例如，自主驾驶和蛋白质结构预测）方面取得了显著成功[[1](#bib.bib1),
    [2](#bib.bib2)]。然而，为了充分利用数据中隐藏的有价值信息，大多数深度学习模型倾向于增加网络的容量，通常是通过扩展现有层或添加更多层[[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]。例如，模型通常包含数百个卷积层和池化层，具有各种激活函数以及多个全连接层，在训练过程中产生数百万或数十亿的参数。这些与复杂模型架构相关的大量参数对深度学习网络的优化提出了挑战。作为一种替代范式，黎曼流形上的优化通过利用参数的几何特性来发掘隐藏的有价值信息，而不是增加网络容量。因此，几何优化可以缓解过度参数化和特征冗余问题。例如，在正交流形上训练的深度学习模型具有较少相关的参数，使得特征冗余大大减少[[6](#bib.bib6)]。
- en: The optimization objective in most deep learning methods can be formulated as
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度学习方法中的优化目标可以被表述为
- en: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in\mathcal{D}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}),\ s.t.\ C(\boldsymbol{\theta}),$ |  | (1)
    |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in\mathcal{D}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}),\ s.t.\ C(\boldsymbol{\theta}),$ |  | (1)
    |'
- en: 'where $\mathcal{D}$ denotes the predefined admissible search space, $f$ denotes
    a real-value optimization function (e.g., loss function) to be minimized by trainable
    parameters $\boldsymbol{\theta}$, and $C(\boldsymbol{\theta})$ represents constraints
    (e.g., orthogonality [[6](#bib.bib6)] and unit row sums [[7](#bib.bib7)]) that
    $\boldsymbol{\theta}$ is subject to. Most deep learning methods define the search
    space $\mathcal{D}$ as the Euclidean space. However, parameters satisfying constraints
    are on the manifold, which is a low dimensional subspace and only occupies a small
    part of Euclidean space. Therefore, to eliminate constraints and reduce parameters,
    geometric optimization [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]
    narrows the search space from Euclidean space to a smooth manifold. Hence, Equation ([1](#S1.E1
    "In 1 Introduction ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) is transformed into a differentiable
    optimization function $f:\mathcal{M}\rightarrow\mathcal{R}$ on a Riemannian manifold,
    i.e.,'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\mathcal{D}$ 表示预定义的可接受搜索空间，$f$ 表示一个需要通过可训练参数 $\boldsymbol{\theta}$ 最小化的实值优化函数（例如，损失函数），$C(\boldsymbol{\theta})$
    表示 $\boldsymbol{\theta}$ 所满足的约束（例如，正交性[[6](#bib.bib6)]和单位行和[[7](#bib.bib7)]）。大多数深度学习方法将搜索空间
    $\mathcal{D}$ 定义为欧几里得空间。然而，满足约束的参数在流形上，流形是一个低维子空间，仅占据欧几里得空间的一小部分。因此，为了消除约束并减少参数，几何优化[[8](#bib.bib8),
    [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)] 将搜索空间从欧几里得空间缩小到一个光滑的流形。因此，方程式（[1](#S1.E1
    "In 1 Introduction ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")）被转化为在黎曼流形上的可微优化函数 $f:\mathcal{M}\rightarrow\mathcal{R}$，即，'
- en: '|  | $\operatorname*{argmin}_{\theta\in\mathcal{M},M=\{\theta&#124;C(\theta)\}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}).$ |  | (2) |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname*{argmin}_{\theta\in\mathcal{M},M=\{\theta\mid C(\theta)\}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}).$ |  | (2) |'
- en: 'As shown in Equation ([2](#S1.E2 "In 1 Introduction ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")),
    selecting a manifold composed of points that meet constraints $C(\boldsymbol{\theta})$
    in Equation ([1](#S1.E1 "In 1 Introduction ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")), a large class
    of constrained deep learning problems in Euclidean space can be optimized as unconstrained
    and convex ones on the Riemannian manifold [[10](#bib.bib10)], which helps ensure
    the convergence. For example, a typical dimension reduction problem can be defined
    as follows'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如方程 ([2](#S1.E2 "在 1 介绍 ‣ 深度学习几何优化的综述：从欧几里得空间到黎曼流形")) 所示，选择一个由满足方程 ([1](#S1.E1
    "在 1 介绍 ‣ 深度学习几何优化的综述：从欧几里得空间到黎曼流形")) 中约束 $C(\boldsymbol{\theta})$ 的点组成的流形，可以将欧几里得空间中大量受限的深度学习问题优化为黎曼流形上的无约束且凸的问题
    [[10](#bib.bib10)]，这有助于确保收敛性。例如，一个典型的降维问题可以定义如下
- en: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in{E}}f_{\boldsymbol{\theta}}(\mathbf{x})=-tr(\boldsymbol{\theta}^{T}x^{T}x\boldsymbol{\theta}),\
    s.t.\ \ \boldsymbol{\theta}^{T}\boldsymbol{\theta}=I,$ |  | (3) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in{E}}f_{\boldsymbol{\theta}}(\mathbf{x})=-tr(\boldsymbol{\theta}^{T}x^{T}x\boldsymbol{\theta}),\
    s.t.\ \ \boldsymbol{\theta}^{T}\boldsymbol{\theta}=I,$ |  | (3) |'
- en: 'where $E$ represents the Euclidean space, $I$ represents the identity matrix
    and parameters $\boldsymbol{\theta}$ are constrained to be orthogonal. Since all
    matrices that satisfy orthogonality compose of the Stiefel manifold , Equation ([3](#S1.E3
    "In 1 Introduction ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) can be treated as an unconstrained problem
    on the Stiefel manifold, which is a kind of Riemannian manifold.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $E$ 代表欧几里得空间，$I$ 代表单位矩阵，参数 $\boldsymbol{\theta}$ 被约束为正交。由于所有满足正交性的矩阵组成 Stiefel
    流形，方程 ([3](#S1.E3 "在 1 介绍 ‣ 深度学习几何优化的综述：从欧几里得空间到黎曼流形")) 可以视作 Stiefel 流形上的无约束问题，这是一种黎曼流形。
- en: '![Refer to caption](img/80fcad216bea951de0934902f9a7c3c0.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/80fcad216bea951de0934902f9a7c3c0.png)'
- en: 'Figure 1: Comparison between geometric and Euclidean optimization path. The
    blue center point is the global optimum. The red curve describes the Riemannian
    optimization path converging upon the global optimal goal, always along a curve
    on manifolds. In contrast, the green dotted line indicates the Euclidean gradient
    descent path towards the optimal goal, taking the risk of moving off the manifold.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：几何优化与欧几里得优化路径的比较。蓝色中心点是全局最优点。红色曲线描述了黎曼优化路径收敛到全局最优目标，总是沿着流形上的曲线。相比之下，绿色虚线表示欧几里得梯度下降路径朝向最优目标，存在偏离流形的风险。
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") depicts the intuitive
    paradigm for optimization processes in arbitrary Euclidean space and on Riemannian
    manifolds. Traditional optimization methods in Euclidean space may ignore the
    advantages of applying geometric optimization strategies. For example, the latter
    can obtain richer geometric information from different unique manifold structures
    and convert constrained optimization problems into unconstrained problems. Moreover,
    geometric optimization can achieve faster convergence speed and mitigate gradient
    explosion and disappearance problems in deep learning, which will be detailed
    in Section [4](#S4 "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold"). Due to the above
    potential, geometric optimization has been applied to various deep neural networks
    in recent years, such as convolution neural network (CNN) [[12](#bib.bib12), [6](#bib.bib6),
    [13](#bib.bib13)], recurrent neural network (RNN) [[14](#bib.bib14)] and vision
    transformer (ViT) [[15](#bib.bib15)]. For instance, orthogonal parameterization
    is used in CNN to reduce filter similarities, make spectra uniform [[16](#bib.bib16)],
    and stabilize the activation distribution in different network layers [[17](#bib.bib17)].
    However, there is a lack of comprehensive surveys focused on deep learning methods
    applying geometric optimization. To explore benefits of geometric optimization,
    this article aims to give an overall review of recent advances on applying geometric
    optimization in deep learning.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") 描绘了任意欧几里得空间和黎曼流形上优化过程的直观范式。传统的欧几里得空间优化方法可能忽视了应用几何优化策略的优势。例如，后者可以从不同的独特流形结构中获得更丰富的几何信息，并将约束优化问题转化为无约束问题。此外，几何优化可以实现更快的收敛速度，减轻深度学习中的梯度爆炸和消失问题，详细内容将在第[4](#S4
    "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")节中介绍。由于上述潜力，近年来几何优化已被应用于各种深度神经网络，如卷积神经网络（CNN）[[12](#bib.bib12),
    [6](#bib.bib6), [13](#bib.bib13)]，递归神经网络（RNN）[[14](#bib.bib14)]和视觉变换器（ViT）[[15](#bib.bib15)]。例如，正交参数化被用于CNN中以减少滤波器相似性，使光谱均匀[[16](#bib.bib16)]，并在不同网络层中稳定激活分布[[17](#bib.bib17)]。然而，缺乏专注于应用几何优化的深度学习方法的综合调查。为了探讨几何优化的好处，本文旨在对近期在深度学习中应用几何优化的进展进行全面回顾。'
- en: '![Refer to caption](img/e1f28b6c67d65b8163e033c412022ca7.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/e1f28b6c67d65b8163e033c412022ca7.png)'
- en: 'Figure 2: An overview of the central idea of this article.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 本文中心思想的概述。'
- en: ¹¹1*Notations* In this work, vectors and matrices are denoted by bold lower
    case letters and upper case ones, respectively. Let $\mathbb{R}$ be the set of
    real numbers, $\mathbb{C}$ be the set of complex numbers and $\nabla f$ denotes
    the Euclidean gradient.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹1*符号* 在这项工作中，向量和矩阵分别用粗体小写字母和大写字母表示。设$\mathbb{R}$为实数集，$\mathbb{C}$为复数集，$\nabla
    f$表示欧几里得梯度。
- en: 'Overview and article organization. In this article, a survey of geometric optimization
    techniques for deep learning is presented, including the theory and applications
    of geometric optimization. Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold") displays an overview of the central idea of this article. Since the
    optimization theory is unified and model-independent, this article illustrates
    the theory first, including various geometric gradient descent optimizers (Section [2](#S2
    "2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")). The motivation and technique
    of applying geometric optimization in classical machine learning is different
    from that of deep learning. Therefore, this article reviews how to apply geometric
    optimization to shallow learning (Section [3](#S3 "3 Applications in Classical
    Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) and deep learning (Section [4](#S4 "4
    Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) separately. In particular, this
    article investigates representative manifold optimization toolboxes (Section [5](#S5
    "5 Toolbox ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")), followed by performance comparisons of different
    geometric deep learning methods on image recognition tasks (Section [6](#S6 "6
    Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")). Finally, we conclude the article
    and highlight future challenges and research trend (Section [7](#S7 "7 Conclusions
    and Future Work ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '概述与文章组织。在本文中，介绍了深度学习几何优化技术的综述，包括几何优化的理论和应用。图[2](#S1.F2 "Figure 2 ‣ 1 Introduction
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")展示了本文的核心思想。由于优化理论是统一的且与模型无关，本文首先阐述了理论，包括各种几何梯度下降优化器（第[2](#S2
    "2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")节）。几何优化在经典机器学习中的动机和技术与在深度学习中的有所不同。因此，本文分别回顾了如何将几何优化应用于浅层学习（第[3](#S3
    "3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")节）和深度学习（第[4](#S4
    "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")节）。特别地，本文调查了具有代表性的流形优化工具箱（第[5](#S5
    "5 Toolbox ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")节），随后比较了不同几何深度学习方法在图像识别任务上的性能（第[6](#S6 "6 Performance
    Evaluation ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")节）。最后，我们总结了本文并突出未来的挑战和研究趋势（第[7](#S7 "7 Conclusions
    and Future Work ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")节）。'
- en: 2 Geometric Optimization Theory
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 几何优化理论
- en: The essence of an optimization problem is to find the maximum or minimum value
    of a cost function. An unconstrained optimization problem can use conventional
    optimization methods (e.g., steepest descent method, conjugate gradient method,
    and Newton method) to find an optimal solution [[18](#bib.bib18)]. However, a
    broad range of optimization problems that occur in computer vision tasks are known
    as constrained optimization problems. In such a case, finding a closed form for
    the cost function is difficult. To use the aforementioned conventional optimization
    techniques, the constrained problem can be transformed into an unconstrained form
    by using the method of Lagrange multipliers or using a barrier penalty function
    [[18](#bib.bib18)]. However, the above methods hardly take advantage of underlying
    manifold structures. They merely treat the constrained problem as a “black box”
    and solve them by using algebraic manipulation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 优化问题的本质是寻找成本函数的最大值或最小值。无约束优化问题可以使用传统的优化方法（例如，最速下降法、共轭梯度法和牛顿法）来找到最优解[[18](#bib.bib18)]。然而，在计算机视觉任务中发生的广泛优化问题被称为有约束优化问题。在这种情况下，找到成本函数的封闭形式是困难的。为了使用上述传统的优化技术，可以通过使用拉格朗日乘子法或障碍惩罚函数将有约束问题转化为无约束形式[[18](#bib.bib18)]。然而，上述方法几乎无法利用潜在的流形结构。它们仅仅将有约束问题视为“黑箱”，并通过代数操作进行求解。
- en: As an alternative solution, geometric optimization methods are developed to
    exploit intrinsic geometric structures of objective function parameters. By utilizing
    the underlying geometry of a cost function, geometric optimization methods can
    narrow the search space of constrained optimization problems from Euclidean space
    to smooth Riemannian manifolds. Riemannian manifold has a differentiable structure
    and is equipped with smooth inner product and Riemannian gradients, which are
    different from Euclidean space and lay the foundation for geometric optimization.
    Based on the Riemannian inner product and Riemannian gradients, a broad spectrum
    of conventional optimization techniques in Euclidean space can have their counterparts
    on smooth manifolds [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [8](#bib.bib8)],
    including the steepest descent method [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)],
    conjugate gradient descent method [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)],
    trust-region method [[8](#bib.bib8), [25](#bib.bib25)] and Newton’s method [[26](#bib.bib26),
    [8](#bib.bib8)]. Therefore, geometric optimization methods can use Riemannian
    optimizers to find an optimal solution for objective functions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种替代方案，几何优化方法被开发出来以利用目标函数参数的内在几何结构。通过利用成本函数的基础几何结构，几何优化方法可以将约束优化问题的搜索空间从欧几里得空间缩小到平滑的黎曼流形。黎曼流形具有可微分的结构，并配备了平滑的内积和黎曼梯度，这些都不同于欧几里得空间，并为几何优化奠定了基础。基于黎曼内积和黎曼梯度，广泛的欧几里得空间中的传统优化技术可以在平滑的流形上找到其对应的技术[[19](#bib.bib19)、[20](#bib.bib20)、[21](#bib.bib21)、[8](#bib.bib8)]，包括最速下降法[[19](#bib.bib19)、[20](#bib.bib20)、[21](#bib.bib21)]、共轭梯度下降法[[22](#bib.bib22)、[23](#bib.bib23)、[24](#bib.bib24)]、信赖域方法[[8](#bib.bib8)、[25](#bib.bib25)]和牛顿法[[26](#bib.bib26)、[8](#bib.bib8)]。因此，几何优化方法可以使用黎曼优化器来寻找目标函数的最优解。
- en: 'In the following subsections, this article first illustrates the model-independent
    optimization process on the Riemannian manifold, covering basic concepts related
    to geometric optimization (Section [2.1](#S2.SS1 "2.1 Geometric Optimization Process
    on Manifolds ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")). Next, this
    article briefly introduces various Riemannian gradient descent optimizers implementing
    geometric optimization, which is a counterpart of optimizers in Euclidean space
    (Section [2.2](#S2.SS2 "2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")). Finally, this article presents a series of manifold
    structures that are commonly used in deep geometric learning methods (Section [2.3](#S2.SS3
    "2.3 Manifold Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '在接下来的各节中，本文首先阐述了黎曼流形上的模型无关优化过程，涵盖与几何优化相关的基本概念（第[2.1节](#S2.SS1 "2.1 Geometric
    Optimization Process on Manifolds ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")）。接着，本文简要介绍了实现几何优化的各种黎曼梯度下降优化器，这些优化器是欧几里得空间中优化器的对应物（第[2.2节](#S2.SS2
    "2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")）。最后，本文介绍了一系列在深度几何学习方法中常用的流形结构（第[2.3节](#S2.SS3 "2.3 Manifold Examples
    ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")）。'
- en: '<svg   height="181.84" overflow="visible" version="1.1" width="300.69"><g transform="translate(0,181.84)
    matrix(1 0 0 -1 0 0) translate(112.31,0) translate(0,50.3)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 18.46 -35.73)" fill="#000000"
    stroke="#000000"><foreignobject width="12.68" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\mathcal{M}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -20.13 8.7)" fill="#000000" stroke="#000000"><foreignobject width="20.42"
    height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j)}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -4.72 -9.19)" fill="#000000" stroke="#000000"><foreignobject
    width="34.24" height="12.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$T_{\bm{\Theta}^{(j)}}\mathcal{M}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.45 58.23)" fill="#000000" stroke="#000000"><foreignobject
    width="8.3" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{H}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -2.82 119.18)" fill="#000000" stroke="#000000"><foreignobject
    width="104.84" height="15.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Update:
    $\bm{\Theta}^{(j)}+\gamma\mathbf{H}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 65.85 98.24)" fill="#000000" stroke="#000000"><foreignobject width="122.53"
    height="13.86" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Retraction:
    $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -78.01 -37.67)" fill="#000000" stroke="#000000"><foreignobject width="56.8"
    height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\operatorname{grad}{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -96.65 60.31)" fill="#000000" stroke="#000000"><foreignobject
    width="44.47" height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\nabla{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 44.48 41.89)" fill="#000000" stroke="#000000"><foreignobject
    width="42.49" height="11.08" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 115.06 31.02)" fill="#000000" stroke="#000000"><foreignobject
    width="30.32" height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j+1)}$</foreignobject></g></g></g></svg>'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '<svg height="181.84" overflow="visible" version="1.1" width="300.69"><g transform="translate(0,181.84)
    matrix(1 0 0 -1 0 0) translate(112.31,0) translate(0,50.3)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 18.46 -35.73)" fill="#000000"
    stroke="#000000"><foreignobject width="12.68" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\mathcal{M}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -20.13 8.7)" fill="#000000" stroke="#000000"><foreignobject width="20.42"
    height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j)}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -4.72 -9.19)" fill="#000000" stroke="#000000"><foreignobject
    width="34.24" height="12.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$T_{\bm{\Theta}^{(j)}}\mathcal{M}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.45 58.23)" fill="#000000" stroke="#000000"><foreignobject
    width="8.3" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{H}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -2.82 119.18)" fill="#000000" stroke="#000000"><foreignobject
    width="104.84" height="15.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Update:
    $\bm{\Theta}^{(j)}+\gamma\mathbf{H}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 65.85 98.24)" fill="#000000" stroke="#000000"><foreignobject width="122.53"
    height="13.86" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Retraction:
    $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -78.01 -37.67)" fill="#000000" stroke="#000000"><foreignobject width="56.8"
    height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\operatorname{grad}{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -96.65 60.31)" fill="#000000" stroke="#000000"><foreignobject
    width="44.47" height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\nabla{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 44.48 41.89)" fill="#000000" stroke="#000000"><foreignobject
    width="42.49" height="11.08" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 115.06 31.02)" fill="#000000" stroke="#000000"><foreignobject
    width="30.32" height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j+1)}$</foreignobject></g></g></g></svg>'
- en: 'Figure 3: The update process in geometric gradient descent algorithm. It shows
    an update from the point $\bm{\Theta}^{(j)}$ to the point $\bm{\Theta}^{(j+1)}$
    in a search direction $\mathbf{H}\in T_{\bm{\Theta}^{(j)}}\mathcal{M}$ along the
    geodesic curve $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$. Moreover, it describes
    how to approximate the geodesic $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$
    by using the retraction $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：几何梯度下降算法中的更新过程。它展示了从点 $\bm{\Theta}^{(j)}$ 到点 $\bm{\Theta}^{(j+1)}$ 的更新过程，在搜索方向
    $\mathbf{H}\in T_{\bm{\Theta}^{(j)}}\mathcal{M}$ 上沿测地线曲线 $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$
    进行。此外，它描述了如何通过使用回缩 $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$ 来近似测地线
    $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$。
- en: 2.1 Geometric Optimization Process on Manifolds
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 流形上的几何优化过程
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2 Geometric Optimization Theory ‣ A Survey of
    Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    depicts the update process in geometric optimization[[27](#bib.bib27)] through
    the gradient descent example. There are two nearby points $\Theta^{(j)}$ and $\Theta^{(j+1)}$
    on a manifold $\mathcal{M}$ together with the tangent space at $\Theta^{(j)}$
    (refer to the green area in Figure [3](#S2.F3 "Figure 3 ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")). Each point $\Theta$ on the manifold has its corresponding
    tangent space $T_{\Theta}\mathcal{M}$, which is a generalization of the tangent
    plane in Euclidean space and consists of all tangent vectors passing through $\Theta$
    [[28](#bib.bib28)]. Each tangent space has an inner product, which is vital for
    vector metrics such as length and angles. Inner product space further helps induce
    the concept of orthogonality, an extension of vertical in higher dimensions. A
    Riemannian gradient $grad\,f(\Theta)$ for geometric optimization is a tangent
    vector on the tangent space $T_{\Theta}\mathcal{M}$ and points to the direction
    where the cost function on the manifold ascends steepest [[28](#bib.bib28)]. Figure [3](#S2.F3
    "Figure 3 ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") shows that gradient
    $\nabla f(\Theta)$ is computed in the ambient Euclidean space. Since the manifold
    is locally homomorphic to the Euclidean space, $grad\,f(\Theta)$ can be achieved
    by projecting Euclidean gradient $\nabla f(\Theta)$ to the appropriate tangent
    space $T_{\Theta}\mathcal{M}$, i.e.,'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [3](#S2.F3 "图 3 ‣ 2 几何优化理论 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形") 通过梯度下降示例描绘了几何优化中的更新过程[[27](#bib.bib27)]。图中展示了流形
    $\mathcal{M}$ 上的两个相邻点 $\Theta^{(j)}$ 和 $\Theta^{(j+1)}` 以及在 $\Theta^{(j)}$ 处的切空间（参见图
    [3](#S2.F3 "图 3 ‣ 2 几何优化理论 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形") 中绿色区域）。流形上的每个点 $\Theta$
    都有其对应的切空间 $T_{\Theta}\mathcal{M}$，它是欧几里得空间中切平面的推广，并且包含了通过 $\Theta$ 的所有切向量 [[28](#bib.bib28)]。每个切空间都有一个内积，这对向量的度量（如长度和角度）至关重要。内积空间进一步帮助引入正交性的概念，这是在更高维度中垂直性的扩展。黎曼梯度
    $grad\,f(\Theta)$ 对于几何优化是切空间 $T_{\Theta}\mathcal{M}$ 上的一个切向量，指向流形上代价函数上升最陡的方向
    [[28](#bib.bib28)]。图 [3](#S2.F3 "图 3 ‣ 2 几何优化理论 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形") 显示了梯度
    $\nabla f(\Theta)$ 是在环境欧几里得空间中计算的。由于流形在局部上同构于欧几里得空间，因此 $grad\,f(\Theta)$ 可以通过将欧几里得梯度
    $\nabla f(\Theta)$ 投影到适当的切空间 $T_{\Theta}\mathcal{M}$ 来实现，即，
- en: '|  | $grad\ f(\Theta)=\Pi_{T_{\Theta}\mathcal{M}}(\nabla f(\Theta)),$ |  |
    (4) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $grad\ f(\Theta)=\Pi_{T_{\Theta}\mathcal{M}}(\nabla f(\Theta)),$ |  |
    (4) |'
- en: where $\Pi$ means the orthogonal projection.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\Pi$ 表示正交投影。
- en: 'As a counterpart of Euclidean straight lines, a geodesic is a locally shortest
    path between two points on the manifold. Therefore, reaching the optimal goal
    along a correct geodesic is shortest. Formally, a geodesic $\Gamma_{\bm{\Theta}}(\gamma\mathbf{H})$
    is a smooth curve on the manifold, proceeding from $\Theta$ in the direction of
    tangent vector $\mathbf{H}\in T_{\Theta}\textit{M}$ with a step size of $\gamma\in\mathbb{R^{+}}$
    [[23](#bib.bib23)]. Since each tangent vector is the direction vector of a specific
    geodesic curve, it can uniquely determine a geodesic curve. In particular, the
    geodesic defined by the negative Riemannian gradient reveals the next point in
    the optimization direction. A point can be mapped from the tangent space to the
    manifold through exponential mapping. In practice, to alleviate the high computational
    cost of exponential mapping, retraction operation $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H})$
    is often used as an approximation [[29](#bib.bib29)]:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 作为欧几里得直线的对应物，测地线是在流形上两个点之间的局部最短路径。因此，沿着正确的测地线达到最优目标是最短的。形式上，测地线 $\Gamma_{\bm{\Theta}}(\gamma\mathbf{H})$
    是流形上的光滑曲线，从 $\Theta$ 沿切向量 $\mathbf{H}\in T_{\Theta}\textit{M}$ 的方向前进，步长为 $\gamma\in\mathbb{R^{+}}$
    [[23](#bib.bib23)]。由于每个切向量是特定测地线曲线的方向向量，它可以唯一确定一条测地线曲线。特别是，由负黎曼梯度定义的测地线揭示了优化方向上的下一点。一个点可以通过指数映射从切空间映射到流形上。在实践中，为了减轻指数映射的高计算成本，通常使用回缩操作
    $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H})$ 作为近似 [[29](#bib.bib29)]：
- en: '|  | $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H}):T_{p}\mathcal{M}\rightarrow\mathcal{M},\
    \gamma\mathbf{H}\rightarrow\Gamma_{\bm{\Theta}}(\gamma\mathbf{H}),$ |  | (5) |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H}):T_{p}\mathcal{M}\rightarrow\mathcal{M},\
    \gamma\mathbf{H}\rightarrow\Gamma_{\bm{\Theta}}(\gamma\mathbf{H}),$ |  | (5) |'
- en: where $\mathbf{H}$ denotes an opposite vector of the Riemannian gradient. Therefore,
    $\mathbf{H}$ points in the direction of the steepest descent of the optimization
    function. As a result, the optimization function will be minimized if parameter
    $\Theta$ is updated along a geodesic curve in the direction of $\mathbf{H}$. In
    summary, with a step size of $\gamma$, the optimizing process from the current
    parameter $\Theta^{(j)}$ to the next parameter $\Theta^{(j+1)}$ can be formulated
    as
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{H}$ 表示黎曼梯度的相反向量。因此，$\mathbf{H}$ 指向优化函数的最陡下降方向。因此，如果参数 $\Theta$ 沿着
    $\mathbf{H}$ 的方向在测地线曲线上更新，优化函数将会最小化。总之，以步长 $\gamma$，从当前参数 $\Theta^{(j)}$ 到下一个参数
    $\Theta^{(j+1)}$ 的优化过程可以表示为
- en: '|  | $\Theta^{(j+1)}=\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})\approx\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})=\mathfrak{R}_{\bm{\Theta}^{(j)}}(-\gamma
    grad\,f(\Theta^{(j)})).$ |  | (6) |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|  | $\Theta^{(j+1)}=\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})\approx\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})=\mathfrak{R}_{\bm{\Theta}^{(j)}}(-\gamma
    grad\,f(\Theta^{(j)})).$ |  | (6) |'
- en: 2.2 Gradient Descent Optimizers
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 梯度下降优化器
- en: Optimization problems defined in Euclidean space can be abstracted as
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在欧几里得空间中定义的优化问题可以抽象化为
- en: '|  | ${\min}\{f_{\boldsymbol{\theta}}(\mathbf{x}):\boldsymbol{\theta}\in\mathbb{E}\},$
    |  | (7) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | ${\min}\{f_{\boldsymbol{\theta}}(\mathbf{x}):\boldsymbol{\theta}\in\mathbb{E}\},$
    |  | (7) |'
- en: 'where $\boldsymbol{\theta}$ are trainable parameters and $E$ means the Euclidean
    space. There are a variety of standard optimizers for Equation ([7](#S2.E7 "In
    2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey of
    Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")).
    The gradient descent method is a most basic optimization strategy. It can be improved
    by stochastic gradient descent (SGD), which can accelerate convergence. The other
    two typical variants of the gradient descent method are stochastic gradient descent-momentum
    (SGD-M) and root mean square prop (RMSProp). To solve valley oscillation and saddle
    point stagnation problems that SGD suffers from, SGD-M is developed to maintain
    the inertia of the previous step. According to empirical judgments of different
    parameters, RMSProp can adaptively determine the learning rate of parameters,
    i.e., parameters with low update frequency can have a larger learning rate, while
    parameters with high update frequency can reduce the step size. Let $\boldsymbol{\theta}^{(k)}$
    represent parameters at iteration $k$ and $\boldsymbol{\theta}^{(k+1)}$ represent
    parameters at iteration $k+1$, this section first explains the above Euclidean
    gradient descent optimizers and then shows how to generalize them to the Riemannian
    manifold for geometric optimization ²²2For simplicity, this paper uses ${\nabla}f$
    to denote $\frac{\partial f_{\boldsymbol{\theta}}(x)}{\partial\boldsymbol{\theta}}$
    in Section [2.2](#S2.SS2 "2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")..'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\boldsymbol{\theta}$ 是可训练参数，$E$ 表示欧几里得空间。对于方程 ([7](#S2.E7 "在 2.2 梯度下降优化器
    ‣ 2 几何优化理论 ‣ 深度学习几何优化综述：从欧几里得空间到黎曼流形")) 有多种标准优化器。梯度下降法是一种最基本的优化策略。它可以通过随机梯度下降（SGD）进行改进，从而加速收敛。梯度下降法的其他两个典型变体是随机梯度下降-动量（SGD-M）和均方根传播（RMSProp）。为了解决
    SGD 遇到的谷底振荡和鞍点停滞问题，SGD-M 被开发出来以保持前一步的惯性。根据不同参数的经验判断，RMSProp 可以自适应地确定参数的学习率，即更新频率低的参数可以具有较大的学习率，而更新频率高的参数则可以减小步长。让
    $\boldsymbol{\theta}^{(k)}$ 代表迭代 $k$ 时的参数，$\boldsymbol{\theta}^{(k+1)}$ 代表迭代 $k+1$
    时的参数，本节首先解释上述欧几里得梯度下降优化器，然后展示如何将其推广到黎曼流形以进行几何优化。²²2为简便起见，本文在 [2.2](#S2.SS2 "2.2
    梯度下降优化器 ‣ 2 几何优化理论 ‣ 深度学习几何优化综述：从欧几里得空间到黎曼流形") 节中使用 ${\nabla}f$ 来表示 $\frac{\partial
    f_{\boldsymbol{\theta}}(x)}{\partial\boldsymbol{\theta}}$。
- en: Gradient Descent. The gradient descent method takes the following form
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法。梯度下降法的形式如下
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-{\lambda}{\nabla}f(\boldsymbol{\theta}^{(k)}),$
    |  | (8) |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-{\lambda}{\nabla}f(\boldsymbol{\theta}^{(k)}),$
    |  | (8) |'
- en: where $\lambda$ is a hyper-parameter representing the step size. The negative
    direction of the gradient ${\nabla}f(\boldsymbol{\theta}^{(k)})$ has a vital property,
    i.e., it is a descent direction of the optimization problem. Therefore, the optimization
    process is to iteratively update trainable parameters along the negative direction
    of gradient until convergence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是表示步长的超参数。梯度 ${\nabla}f(\boldsymbol{\theta}^{(k)})$ 的负方向具有一个重要的属性，即它是优化问题的下降方向。因此，优化过程是沿梯度的负方向迭代更新可训练参数，直到收敛。
- en: Stochastic Gradient Descent (SGD). The main idea behind SGD is to use random
    mini-batches of training data to update parameters of the optimization problem,
    which inherently reduces the calculation workload. Although the parameters may
    not be updated in the direction of the steepest descent every time, the overall
    update is in the steepest descent direction through multiple rounds of updates.
    As a result, SGD can greatly speed up the optimization process.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）。SGD 的主要思想是使用随机的小批量训练数据来更新优化问题的参数，这本质上减少了计算工作量。尽管参数可能不会每次都沿着最陡下降方向更新，但通过多轮更新，总体更新方向仍然是最陡下降方向。因此，SGD
    可以大大加快优化过程。
- en: Stochastic Gradient Descent-Momentum (SGD-M). Inspired by the concept of momentum
    in physics, SGD-M exerts the influence of the last update on the current update
    to damp oscillation and accelerate convergence. Let $m^{(k)}$ denote the update
    imposed on $\boldsymbol{\theta}^{(k-1)}$ and $\nabla f$ denote the gradient at
    time $k$, the update $m^{(k+1)}$ to be imposed on $\boldsymbol{\theta}^{(k)}$
    can be achieved as
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降-动量（SGD-M）。受到物理中动量概念的启发，SGD-M 在当前更新中施加上一次更新的影响，以减缓振荡并加速收敛。令 $m^{(k)}$
    表示施加在 $\boldsymbol{\theta}^{(k-1)}$ 上的更新，$\nabla f$ 表示时间 $k$ 时的梯度，施加在 $\boldsymbol{\theta}^{(k)}$
    上的更新 $m^{(k+1)}$ 可以通过以下方式实现
- en: '|  | $m^{(k+1)}=\lambda_{0}\ m^{(k)}+\lambda_{1}\nabla f,$ |  | (9) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  | $m^{(k+1)}=\lambda_{0}\ m^{(k)}+\lambda_{1}\nabla f,$ |  | (9) |'
- en: where $\lambda_{0}$ and $\lambda_{1}$ are hyper-parameters. Sequentially, the
    parameter $\boldsymbol{\theta}^{(k)}$ is updated to $\boldsymbol{\theta}^{(k+1)}$
    by $m^{(k+1)}$ as follows
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda_{0}$ 和 $\lambda_{1}$ 是超参数。参数 $\boldsymbol{\theta}^{(k)}$ 通过 $m^{(k+1)}$
    被更新为 $\boldsymbol{\theta}^{(k+1)}$，具体如下
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-m^{(k+1)}.$ |  |
    (10) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-m^{(k+1)}.$ |  |
    (10) |'
- en: Root Mean Square Prop (RMSProp). Similar to SGD-M, RMSProp considers the influence
    of the last update when calculating the upcoming update. Let $m^{(k)}$ be the
    update on the previous occasion and $\nabla f$ be the current gradient, RMSProp
    designs upcoming update $m^{(k+1)}$ as follows
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根传播（RMSProp）。类似于 SGD-M，RMSProp 在计算即将到来的更新时考虑上一次更新的影响。令 $m^{(k)}$ 为上一次的更新，$\nabla
    f$ 为当前的梯度，RMSProp 将即将到来的更新 $m^{(k+1)}$ 设计如下
- en: '|  | $m^{(k+1)}=\lambda\ m^{(k)}+(1-\lambda)(\nabla f\odot\nabla f),$ |  |
    (11) |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  | $m^{(k+1)}=\lambda\ m^{(k)}+(1-\lambda)(\nabla f\odot\nabla f),$ |  |
    (11) |'
- en: where $\lambda$ is a hyper-parameter and $\odot$ denotes the $Hadamard$ product
    [[29](#bib.bib29)] which is element-wise. RMSProp updates $\boldsymbol{\theta}^{(k)}$
    to $\boldsymbol{\theta}^{(k+1)}$ in the following way, i.e.,
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\lambda$ 是一个超参数，$\odot$ 表示 $Hadamard$ 乘积 [[29](#bib.bib29)]，即逐元素乘积。RMSProp
    以以下方式将 $\boldsymbol{\theta}^{(k)}$ 更新为 $\boldsymbol{\theta}^{(k+1)}$，即：
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-\eta\frac{\nabla
    f}{\sqrt{m^{(k+1)}+\epsilon}},$ |  | (12) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-\eta\frac{\nabla
    f}{\sqrt{m^{(k+1)}+\epsilon}},$ |  | (12) |'
- en: where $\eta$ is a hyper-parameter and $\epsilon$ is positive to prevent the
    denominator from being zero. Using element-wise square root and division operation,
    RMSProp guarantees that different elements in gradient $\nabla f$ have different
    coefficients, which represent learning rates in deep learning. Therefore, RMSProp
    enables parameters to have different learning rates [[29](#bib.bib29)], which
    makes the optimization process more flexible.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\eta$ 是一个超参数，$\epsilon$ 是正数，以防止分母为零。通过逐元素的平方根和除法操作，RMSProp 确保梯度 $\nabla
    f$ 中的不同元素具有不同的系数，这些系数表示深度学习中的学习率。因此，RMSProp 使得参数能够具有不同的学习率 [[29](#bib.bib29)]，从而使优化过程更加灵活。
- en: 'Based on the aforementioned optimization process on manifolds (Section [2.1](#S2.SS1
    "2.1 Geometric Optimization Process on Manifolds ‣ 2 Geometric Optimization Theory
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")), the Euclidean gradient descent algorithm in Equation ([8](#S2.E8
    "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")) can be transferred to Riemannian manifolds as'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 基于上述流形上的优化过程（第[2.1](#S2.SS1 "2.1 流形上的几何优化过程 ‣ 2 几何优化理论 ‣ 深度学习中的几何优化调查：从欧几里得空间到黎曼流形")节），方程([8](#S2.E8
    "在 2.2 梯度下降优化器 ‣ 2 几何优化理论 ‣ 深度学习中的几何优化调查：从欧几里得空间到黎曼流形")中的欧几里得梯度下降算法可以转移到黎曼流形中，如下所示
- en: '|  | $\theta^{(k+1)}=\mathfrak{R}_{{\theta}^{(k)}}(-\lambda grad\,f(\theta^{(k)})),$
    |  | (13) |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | $\theta^{(k+1)}=\mathfrak{R}_{{\theta}^{(k)}}(-\lambda grad\,f(\theta^{(k)})),$
    |  | (13) |'
- en: where $\mathfrak{R}_{{\theta}^{(k)}}$ means the retraction operation at point
    ${\theta}^{(k)}$ and $grad\,f$ means the Riemannian gradient. For better understanding,
    this article takes constraint SGD-M and constraint RMSProp as an instance to explain
    how to generalize gradient descent optimizers from Euclidean space to manifolds.
    By performing orthogonal projection and retraction, other Euclidean gradient descent
    optimizers can be similarly converted to Riemannian optimizers.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathfrak{R}_{{\theta}^{(k)}}$ 代表在点 ${\theta}^{(k)}$ 的回撤操作，而 $grad\,f$ 代表黎曼梯度。为了更好地理解，本文以约束
    SGD-M 和约束 RMSProp 为例，解释如何将梯度下降优化器从欧几里得空间推广到流形。通过执行正交投影和回撤，其他欧几里得梯度下降优化器也可以类似地转化为黎曼优化器。
- en: Constraint SGD-M [[29](#bib.bib29)]. Constraint SGD-M is a generalization of
    SGD-M optimizer on manifolds. In the $k$-th iteration, $m^{(k)}$ denotes a tangent
    vector on the tangent space $T_{\boldsymbol{\theta}^{(k-1)}}M$ and $m^{(k+1)}$
    denotes another vector on the tangent space $T_{\boldsymbol{\theta}^{(k)}}M$.
    Since $\nabla f$ is in the surrounding Euclidean space, it needs to be orthogonally
    projected to tangent space $T_{\boldsymbol{\theta}^{(k)}}M$, i.e., the current
    Riemannian gradient $grad\,f$ is achieved as follows
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 约束 SGD-M [[29](#bib.bib29)]。约束 SGD-M 是对流形上的 SGD-M 优化器的推广。在第 $k$ 次迭代中，$m^{(k)}$
    表示在切空间 $T_{\boldsymbol{\theta}^{(k-1)}}M$ 上的一个切向量，而 $m^{(k+1)}$ 表示在切空间 $T_{\boldsymbol{\theta}^{(k)}}M$
    上的另一个向量。由于 $\nabla f$ 在周围的欧几里得空间中，它需要正交投影到切空间 $T_{\boldsymbol{\theta}^{(k)}}M$，即当前黎曼梯度
    $grad\,f$ 通过以下方式获得
- en: '|  | $grad\,f=\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla f).$ |  | (14) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | $grad\,f=\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla f).$ |  | (14) |'
- en: 'The transportation from a tangent space associated with point $p$ to another
    tangent space associated with point $q$ is called parallel transportation, i.e.,
    $\Gamma_{p\rightarrow q}:\ T_{p}M\rightarrow T_{q}M$. After projecting the Euclidean
    gradient $\nabla f$ to the tangent space $T_{\boldsymbol{\theta}^{(k)}}M$ and
    transporting $m^{(k)}$ from $T_{\boldsymbol{\theta}^{(k-1)}}M$ to $T_{\boldsymbol{\theta}^{(k)}}M$,
    Equation ([9](#S2.E9 "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")) is transformed to:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '从与点 $p$ 相关的切空间到与点 $q$ 相关的另一个切空间的传输称为平行传输，即 $\Gamma_{p\rightarrow q}:\ T_{p}M\rightarrow
    T_{q}M$。在将欧几里得梯度 $\nabla f$ 投影到切空间 $T_{\boldsymbol{\theta}^{(k)}}M$ 并将 $m^{(k)}$
    从 $T_{\boldsymbol{\theta}^{(k-1)}}M$ 传输到 $T_{\boldsymbol{\theta}^{(k)}}M$ 后，方程 ([9](#S2.E9
    "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")) 被转化为：'
- en: '|  | $m^{(k+1)}=\lambda_{0}\,\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+\lambda_{1}\,\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f).$ |  | (15) |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  | $m^{(k+1)}=\lambda_{0}\,\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+\lambda_{1}\,\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f).$ |  | (15) |'
- en: Based on the retraction operation, the optimization parameter $\boldsymbol{\theta}^{(k+1)}$
    can be updated from $\boldsymbol{\theta}^{(k)}$ by searching along the geodesic
    in the negative direction of $m^{(k+1)}$, i.e., the iterate optimization can be
    expressed as
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于回撤操作，优化参数 $\boldsymbol{\theta}^{(k+1)}$ 可以通过沿着 $m^{(k+1)}$ 的负方向在测地线上搜索来从 $\boldsymbol{\theta}^{(k)}$
    更新，即迭代优化可以表示为
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-m^{(k+1)}).$
    |  | (16) |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-m^{(k+1)}).$
    |  | (16) |'
- en: 'Constraint RMSProp [[29](#bib.bib29)]. Similar to constraint SGD-M, after transporting
    $m^{(k)}$ from tangent space $T_{\boldsymbol{\theta}^{(k-1)}}M$ to $T_{\boldsymbol{\theta}^{(k)}}M$
    and orthogonally projecting $\nabla f\odot\nabla f$ to corresponding tangent space,
    Equation ([11](#S2.E11 "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")) can be transformed into:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '约束 RMSProp [[29](#bib.bib29)]。类似于约束 SGD-M，在将 $m^{(k)}$ 从切空间 $T_{\boldsymbol{\theta}^{(k-1)}}M$
    传送到 $T_{\boldsymbol{\theta}^{(k)}}M$ 并将 $\nabla f\odot\nabla f$ 正交投影到相应的切空间后，方程 ([11](#S2.E11
    "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")) 可以转化为：'
- en: '|  | $m^{(k+1)}=\lambda\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+(1-\lambda)\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f\odot\nabla f).$ |  | (17) |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|  | $m^{(k+1)}=\lambda\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+(1-\lambda)\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f\odot\nabla f).$ |  | (17) |'
- en: The parameter $\boldsymbol{\theta}^{(k+1)}$ of the optimization goal can be
    iteratively searched on the manifold with a determined direction $-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}$, that is,
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 优化目标的参数$\boldsymbol{\theta}^{(k+1)}$可以在流形上以确定的方向$-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}$进行迭代搜索，即，
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}).$ |  | (18) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}).$ |  | (18) |'
- en: 2.3 Manifold Examples
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 流形示例
- en: 'Different kinds of matrix manifolds have different geometry structures and
    satisfy different constraints, bringing different advantages when applying geometric
    optimization to deep learning. For example, the oblique manifold plays a significant
    role in dictionary learning due to its property of unit-norm columns, while the
    Stiefel manifold has a positive effect on optimizing RNNs since matrices on the
    Stiefel manifold have orthogonal and uncorrelated columns, which helps alleviate
    feature abundancy problems in RNNs. Since space is limited, this section only
    presents common manifold structures³³3For more introduction on matrix manifolds,
    we refer interested readers to the website https://www.Pymanopt.org. such as Stiefel
    manifold, oblique manifold, and Graßmann manifold, all of which are widely used
    in existing geometric optimization techniques that are discussed in Section [3](#S3
    "3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") and Section [4](#S4
    "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold").'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '不同种类的矩阵流形具有不同的几何结构和约束条件，在将几何优化应用于深度学习时带来不同的优势。例如，倾斜流形在字典学习中发挥了重要作用，因为它的单位范数列的特性，而Stiefel流形对优化RNN有积极影响，因为Stiefel流形上的矩阵具有正交且不相关的列，这有助于缓解RNN中的特征冗余问题。由于空间有限，本节仅介绍常见的流形结构³³3有关矩阵流形的更多介绍，请感兴趣的读者访问网站
    https://www.Pymanopt.org。例如Stiefel流形、倾斜流形和Grassmann流形，它们都广泛应用于现有的几何优化技术，这些技术在第[3](#S3
    "3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")节和第[4](#S4 "4
    Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")节中讨论。'
- en: Product Manifold and Quotient Manifold. Let $\mathcal{A}$ and $\mathcal{B}$
    be two manifolds of dimension $d_{A}$ and $d_{B}$, for any pair of charts $(U,\phi)$
    and $(V,\varphi)$ of $\mathcal{A}$ and $\mathcal{B}$, the map $\Phi$ is defined
    on $U\times V$ by $\Phi(x,y)=(\phi(x),\varphi(y))$. It specifies a smooth product
    manifold structure on the product space $\mathcal{A}\times\mathcal{B}$. Quotient
    manifold is an abstract space with similar subsets in the same manifold. These
    subsets can be described with equivalence relationship. $\mathcal{A}$ represents
    a manifold equipped with an equivalence relation $\sim$, which satisfies three
    properties, i.e., reflexivity, symmetry and transitivity [[8](#bib.bib8)]. The
    equivalence class of one point $x$ consists of all elements that are equivalent
    to it, i.e.,
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 乘积流形和商流形。设$\mathcal{A}$和$\mathcal{B}$是两个维度分别为$d_{A}$和$d_{B}$的流形，对于$\mathcal{A}$和$\mathcal{B}$的任意一对图$(U,\phi)$和$(V,\varphi)$，映射$\Phi$在$U\times
    V$上定义为$\Phi(x,y)=(\phi(x),\varphi(y))$。它在乘积空间$\mathcal{A}\times\mathcal{B}$上指定了一个光滑的乘积流形结构。商流形是一个抽象空间，具有在同一流形中相似的子集。这些子集可以用等价关系描述。$\mathcal{A}$表示一个配备有等价关系$\sim$的流形，满足三种属性，即自反性、对称性和传递性[[8](#bib.bib8)]。一个点$x$的等价类由所有与之等价的元素组成，即，
- en: '|  | $[x]:=\{y\in\mathcal{A}:y\sim x\},$ |  | (19) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $[x]:=\{y\in\mathcal{A}:y\sim x\},$ |  | (19) |'
- en: where $[x]$ indicates the equivalence class of $x$. The quotient of manifold
    $\mathcal{A}$ by relation $\sim$ is defined as follows
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$[x]$表示$x$的等价类。流形$\mathcal{A}$通过关系$\sim$的商定义如下
- en: '|  | $\mathcal{A}/\sim\ :=\{[x]:x\in\mathcal{A}\},$ |  | (20) |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{A}/\sim\ :=\{[x]:x\in\mathcal{A}\},$ |  | (20) |'
- en: with the projection $\pi:\mathcal{A}\rightarrow\mathcal{A}/\sim$, indicated
    by $x\rightarrow[x]$. When $\pi$ is a submersion projection, and $\mathcal{A}$
    is a smooth manifold [[8](#bib.bib8), [30](#bib.bib30)], $\mathcal{A}/\sim$ admits
    a unique smooth manifold structure $B$, which is the quotient manifold of $\mathcal{A}$.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过投影 $\pi:\mathcal{A}\rightarrow\mathcal{A}/\sim$，用 $x\rightarrow[x]$ 表示。当 $\pi$
    是一个沉浸投影，且 $\mathcal{A}$ 是一个光滑流形 [[8](#bib.bib8), [30](#bib.bib30)] 时，$\mathcal{A}/\sim$
    具有一个唯一的光滑流形结构 $B$，即 $\mathcal{A}$ 的商流形。
- en: Symmetric Positive-Definite Manifold [[29](#bib.bib29)]. It consists of Symmetric
    Positive-Definite (SPD) matrices $M\in\mathbb{R}^{p\times p}$ equipped with the
    Affine Invariant Riemannian Metric (*AIRM*) as follows
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 对称正定流形 [[29](#bib.bib29)]。它由对称正定（SPD）矩阵 $M\in\mathbb{R}^{p\times p}$ 组成，配备有如下的仿射不变黎曼度量
    (*AIRM*)。
- en: '|  | $S_{++}^{p}\triangleq\{M\in\mathbb{R}^{p\times p}:v^{T}Mv>0,\forall v\in\mathbb{R}^{p}-\{0_{p}\}\}.$
    |  | (21) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $S_{++}^{p}\triangleq\{M\in\mathbb{R}^{p\times p}:v^{T}Mv>0,\forall v\in\mathbb{R}^{p}-\{0_{p}\}\}.$
    |  | (21) |'
- en: SPD manifold achieves great success in computer vision due to its powerful statistical
    representations for images and videos. For example, SPD matrices are used to construct
    region covariance matrices for pedestrian detection [[31](#bib.bib31)], joint
    covariance descriptors for action recognition [[32](#bib.bib32)], and image set
    covariance matrices for face recognition [[33](#bib.bib33)].
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: SPD 流形在计算机视觉中取得了巨大成功，因为它对图像和视频具有强大的统计表示。例如，SPD 矩阵用于构建行人检测的区域协方差矩阵 [[31](#bib.bib31)]，动作识别的联合协方差描述符
    [[32](#bib.bib32)]，以及面部识别的图像集协方差矩阵 [[33](#bib.bib33)]。
- en: Stiefel Manifold [[29](#bib.bib29)]. The Stiefel manifold $St(p,n)$ is composed
    of orthogonal matrices $W\in\mathbb{R}^{n\times p}(p\leq n)$ endowed with the
    Frobenius inner product as follows
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Stiefel 流形 [[29](#bib.bib29)]。Stiefel 流形 $St(p,n)$ 由正交矩阵 $W\in\mathbb{R}^{n\times
    p}(p\leq n)$ 组成，配备有如下的 Frobenius 内积。
- en: '|  | $St(p,n)\triangleq\{W\in\mathbb{R}^{n\times p}:W^{T}W=I_{p}\},$ |  | (22)
    |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $St(p,n)\triangleq\{W\in\mathbb{R}^{n\times p}:W^{T}W=I_{p}\},$ |  | (22)
    |'
- en: where $I_{p}$ denotes $\mathbb{R}^{p\times p}$ identity matrix. The optimization
    function over the compact Stiefel manifolds has an upper bound, which allows it
    to achieve an optimal solution.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $I_{p}$ 表示 $\mathbb{R}^{p\times p}$ 单位矩阵。紧致 Stiefel 流形上的优化函数具有上界，这使其能够实现最优解。
- en: Sphere Manifold and Oblique Manifold. The set of unit Frobenius norm matrices
    of size $n\times m$ is denoted by the sphere $\mathbb{S}^{nm-1}$. It can be treated
    as a Riemannian submanifold embedded in Euclidean space $\mathbb{R}^{n\times m}$
    endowed with the usual inner product $\langle H_{1},H_{2}\rangle=\operatorname{trace}(H_{1}^{T}H_{2})$.
    The oblique manifold $\mathcal{OB}(n,m)$ is the set of matrices of size $n\times
    m$ with unit-norm columns. It has the same geometry as that of the product manifold
    of spheres $\prod_{i=0}^{m}\mathbb{S}^{n-1}$.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 球面流形和斜面流形。单位 Frobenius 范数的 $n\times m$ 矩阵集合用球面 $\mathbb{S}^{nm-1}$ 表示。它可以被视为嵌入在欧几里得空间
    $\mathbb{R}^{n\times m}$ 中的黎曼子流形，配备有通常的内积 $\langle H_{1},H_{2}\rangle=\operatorname{trace}(H_{1}^{T}H_{2})$。斜面流形
    $\mathcal{OB}(n,m)$ 是单位范数列的 $n\times m$ 矩阵集合。它具有与球面乘积流形 $\prod_{i=0}^{m}\mathbb{S}^{n-1}$
    相同的几何结构。
- en: Graßmann Manifold [[29](#bib.bib29)]. The Graßmann manifold $\mathcal{G}(n,p)$
    embraces the set of subspaces spanned by the orthogonal matrices $X\in\mathbb{R}^{n\times
    p}(p\leq n)$ as
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Graßmann 流形 [[29](#bib.bib29)]。Graßmann 流形 $\mathcal{G}(n,p)$ 包含由正交矩阵 $X\in\mathbb{R}^{n\times
    p}(p\leq n)$ 张成的子空间集合，如下所示。
- en: '|  | $\mathcal{G}(n,p)\triangleq\{Span(X):X\in\mathbb{R}^{n\times p},X^{T}X=I_{p}\}.$
    |  | (23) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{G}(n,p)\triangleq\{Span(X):X\in\mathbb{R}^{n\times p},X^{T}X=I_{p}\}.$
    |  | (23) |'
- en: Note that a Graßmann manifold is different from a Stiefel manifold, i.e., a
    point on the Stiefel manifold represents a basis for a subspace, whereas a point
    on the Graßmann manifold represents an entire subspace. Moreover, Graßmann manifolds
    are of linear subspaces and can be used to perform a geometry-aware dimension
    reduction.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到 Graßmann 流形与 Stiefel 流形不同，即 Stiefel 流形上的一个点代表一个子空间的基，而 Graßmann 流形上的一个点代表一个整个子空间。此外，Graßmann
    流形是线性子空间的集合，可用于执行几何感知的维度减少。
- en: Unitary Manifold. Unitary matrices are the extension of orthogonal matrices
    to the complex domain, i.e.,
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 单位流形。单位矩阵是正交矩阵在复数域的扩展，即，
- en: '|  | $U(n)\triangleq\{U\in\mathbb{C}^{n\times n}:U^{\ast}U=I_{n}\},$ |  | (24)
    |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  | $U(n)\triangleq\{U\in\mathbb{C}^{n\times n}:U^{\ast}U=I_{n}\},$ |  | (24)
    |'
- en: 'where $U^{\ast}$ denotes the conjugate transpose matrix and $I_{n}$ represents
    the identity matrix of size $n\times n$. Orthogonal or unitary matrices can preserve
    norm of vectors, i.e., $\|Wh\|_{2}$ = $\|h\|_{2}$ when $W$ is an orthogonal or
    unitary matrix. Therefore, exploding and vanishing gradient problems in deep temporary
    networks can be alleviated when parameters are optimized on the orthogonal or
    unitary manifold, which will de detailed in Section [4.2](#S4.SS2 "4.2 Geometric
    RNN ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for
    Deep Learning: From Euclidean Space to Riemannian Manifold").'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $U^{\ast}$ 表示共轭转置矩阵，$I_{n}$ 代表大小为 $n\times n$ 的单位矩阵。正交或单位ary矩阵可以保持向量的范数，即，当
    $W$ 是一个正交或单位ary矩阵时，$\|Wh\|_{2}$ = $\|h\|_{2}$。因此，当参数在正交或单位ary流形上优化时，可以缓解深度临时网络中的梯度爆炸和消失问题，这将在第
    [4.2](#S4.SS2 "4.2 Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold") 节中详细讨论。'
- en: Lie Group [[13](#bib.bib13)]. Lie groups are real or complex manifolds with
    group structure. There are two compact and connected Lie groups, i.e., the special
    orthogonal group formulated as
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 李群 [[13](#bib.bib13)]。李群是具有群结构的实数或复数流形。存在两个紧致且连通的李群，即特殊正交群，公式如下
- en: '|  | $SO(n)=\{B\in\mathbb{R}^{n\times n}&#124;B^{T}B=I,det(B)=1\},$ |  | (25)
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  | $SO(n)=\{B\in\mathbb{R}^{n\times n} \mid B^{T}B=I, \det(B)=1\},$ |  |
    (25) |'
- en: and the unitary group formulated as
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 和单位ary群的公式如下
- en: '|  | $U(n)=\{B\in\mathbb{C}^{n\times n}&#124;B^{\ast}B=I\}.$ |  | (26) |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|  | $U(n)=\{B\in\mathbb{C}^{n\times n} \mid B^{\ast}B=I\}.$ |  | (26) |'
- en: The tangent space at the identity element of the Lie group is called the $Lie\
    algebra$ of it. For the special orthogonal group and the unitary group, their
    Lie algebras are given by
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 李群在单位元素处的切空间被称为它的 $Lie\ algebra$。对于特殊正交群和单位ary群，它们的李代数分别由以下公式给出
- en: '|  | $\displaystyle\mathfrak{so}(n)$ | $\displaystyle=\{A\in\mathbb{R}^{n\times
    n}&#124;A\ +A^{T}=0\},$ |  | (27) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathfrak{so}(n)$ | $\displaystyle=\{A\in\mathbb{R}^{n\times
    n} \mid A + A^{T}=0\},$ |  | (27) |'
- en: '|  | $\displaystyle\mathfrak{u}(n)$ | $\displaystyle=\{A\in\mathbb{C}^{n\times
    n}&#124;A\ +A^{\ast}=0\}.$ |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathfrak{u}(n)$ | $\displaystyle=\{A\in\mathbb{C}^{n\times
    n} \mid A + A^{\ast}=0\}.$ |  |'
- en: $\mathfrak{so}(n)$ is known as skew-symmetric matrix, while $\mathfrak{u}(n)$
    is skew-Hermitian matrix. The Lie exponential map ($exp:\mathfrak{g}\rightarrow
    G$ where $G$ denotes the Lie Group and $\mathfrak{g}$ denotes its Lie algebra)
    on a connected, compact Lie group is surjective. Therefore, the optimization problem
    on a Lie group can be converted to the optimization problem in Euclidean space
    where Euclidean gradient descent optimizers can be directly used.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathfrak{so}(n)$ 被称为反对称矩阵，而 $\mathfrak{u}(n)$ 是反厄米矩阵。对一个连通的、紧致的李群，李指数映射（$exp:\mathfrak{g}\rightarrow
    G$，其中 $G$ 表示李群，$\mathfrak{g}$ 表示其李代数）是满射的。因此，李群上的优化问题可以转换为欧几里得空间中的优化问题，在那里可以直接使用欧几里得梯度下降优化器。
- en: 3 Applications in Classical Machine Learning
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 经典机器学习中的应用
- en: Classical machine learning methods gained achievements in solving artificial
    intelligence problems (e.g., dimension reduction, inverse problem, sparse representation,
    analysis operator learning, and temporal models). Despite the increasing computing
    power of modern computer facilities, it is still difficult to solve a large category
    of constrained classical machine learning problems in Euclidean space. To decrease
    the solving difficulty, geometric optimization focuses on the special structure
    of constrained problems and regards them as unconstrained ones on Riemannian manifolds
    [[10](#bib.bib10)].
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 经典的机器学习方法在解决人工智能问题（例如，维度约简、逆问题、稀疏表示、分析算子学习和时间模型）方面取得了成就。尽管现代计算设施的计算能力不断提高，但在欧几里得空间中解决大量约束的经典机器学习问题仍然困难。为了降低解决难度，几何优化关注约束问题的特殊结构，将其视为黎曼流形上的无约束问题
    [[10](#bib.bib10)]。
- en: 3.1 Dimension Reduction
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 维度约简
- en: 'By using a mapping $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ with $l<m$, dimension
    reduction (DR) aims to find a lower-dimensional representation $y_{i}\in\mathbb{R}^{l}$
    of given data samples $x_{i}\in\mathbb{R}^{m}$. The most popular DR paradigm uses
    a linear projection while others employ a nonlinear transformation to constrain
    locality properties between data. Table [1](#S3.T1 "Table 1 ‣ 3.1 Dimension Reduction
    ‣ 3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") summarizes main
    properties of mainstream DR approaches (e.g., linear discriminant analysis (LDA)
    [[34](#bib.bib34)], principal component analysis (PCA) [[35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)], multi-dimensional scaling (MDS) [[38](#bib.bib38), [39](#bib.bib39)],
    isometric feature mapping (ISOMAP) [[40](#bib.bib40)], local linear embedding
    (LLE) [[41](#bib.bib41)], laplace eigenmaps (LE) [[42](#bib.bib42)], and locality
    preserving projections (LPP) [[43](#bib.bib43)]).'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '通过使用映射 $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ 其中 $l<m$，降维（DR）旨在找到给定数据样本
    $x_{i}\in\mathbb{R}^{m}$ 的低维表示 $y_{i}\in\mathbb{R}^{l}$。最流行的 DR 范式使用线性投影，而其他方法则采用非线性变换以约束数据之间的局部性质。表 [1](#S3.T1
    "Table 1 ‣ 3.1 Dimension Reduction ‣ 3 Applications in Classical Machine Learning
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold") 总结了主流 DR 方法的主要特性（例如，线性判别分析（LDA） [[34](#bib.bib34)]，主成分分析（PCA）
    [[35](#bib.bib35), [36](#bib.bib36), [37](#bib.bib37)]，多维尺度分析（MDS） [[38](#bib.bib38),
    [39](#bib.bib39)]，等距特征映射（ISOMAP） [[40](#bib.bib40)]，局部线性嵌入（LLE） [[41](#bib.bib41)]，拉普拉斯特征映射（LE）
    [[42](#bib.bib42)]，和保持局部性的投影（LPP） [[43](#bib.bib43)])。'
- en: The mapping $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ used in DR methods is
    often restricted to be an orthogonal projection, i.e.,
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DR 方法中使用的映射 $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ 通常被限制为正交投影，即，
- en: '|  | $\mu(\mathbf{x}):=\mathbf{V}^{\top}\mathbf{x},$ |  | (28) |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mu(\mathbf{x}):=\mathbf{V}^{\top}\mathbf{x},$ |  | (28) |'
- en: where the orthogonal matrix $\mathbf{V}\in\mathbb{R}^{m\times l}$ belongs to
    the *Stiefel* manifold ${St}(l,m):=\big{\{}\mathbf{V}\in\mathbb{R}^{m\times l}|\mathbf{V}^{\top}\mathbf{V}=\mathbf{I}_{l}\big{\}}$.
    One generic algorithmic framework to find an optimal $\mathbf{V}\in St(l,m)$ can
    be formulated as a maximization problem, i.e.,
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，正交矩阵 $\mathbf{V}\in\mathbb{R}^{m\times l}$ 属于 *Stiefel* 流形 ${St}(l,m):=\big{\{}\mathbf{V}\in\mathbb{R}^{m\times
    l}|\mathbf{V}^{\top}\mathbf{V}=\mathbf{I}_{l}\big{\}}$。寻找一个最优的 $\mathbf{V}\in
    St(l,m)$ 的一种通用算法框架可以被表述为一个最大化问题，即，
- en: '|  | $\operatorname*{argmax}_{\mathbf{V}\in St(l,m)}\,\frac{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{A}\mathbf{V})}{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{B}\mathbf{V})+\sigma},$
    |  | (29) |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname*{argmax}_{\mathbf{V}\in St(l,m)}\,\frac{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{A}\mathbf{V})}{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{B}\mathbf{V})+\sigma},$
    |  | (29) |'
- en: 'where matrices $A,B\in\mathbb{R}^{m\times m}$ are often symmetric or positive
    definite matrices. Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) is called *trace quotient* or *trace
    ratio*. Note that constant $\sigma>0$ can prevent the denominator from being zero.
    Matrices $A$ and $B$ are constructed to measure the similarity between data points
    according to specific problems. $V$ is not unique and closely related to selected
    eigenvalues. Solutions of Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣
    3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")) are rotation
    invariant, i.e., let $\mathbf{V}^{*}\in St(l,m)$ be a solution of the problem,
    then $\mathbf{V}^{*}\boldsymbol{\Theta}$ for any orthogonal $\boldsymbol{\Theta}\in\mathbb{R}^{l\times
    l}$ is also a solution of Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣
    3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")). In other words,
    the solution set of Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) is the set of all $l$-dimensional
    linear subspaces in $\mathbb{R}^{m}$, which can be represented by Graßmann manifold
    , i.e.,'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中矩阵 $A,B\in\mathbb{R}^{m\times m}$ 通常是对称矩阵或正定矩阵。公式 ([29](#S3.E29 "在 3.1 降维
    ‣ 3 经典机器学习中的应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 被称为*迹商*或*迹比*。注意常数 $\sigma>0$ 可以防止分母为零。矩阵
    $A$ 和 $B$ 被构建用来根据特定问题测量数据点之间的相似性。$V$ 并不是唯一的，且与选择的特征值密切相关。公式 ([29](#S3.E29 "在 3.1
    降维 ‣ 3 经典机器学习中的应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 的解是旋转不变的，即，设 $\mathbf{V}^{*}\in
    St(l,m)$ 为问题的一个解，则对于任何正交矩阵 $\boldsymbol{\Theta}\in\mathbb{R}^{l\times l}$，$\mathbf{V}^{*}\boldsymbol{\Theta}$
    也是公式 ([29](#S3.E29 "在 3.1 降维 ‣ 3 经典机器学习中的应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 的解。换句话说，公式 ([29](#S3.E29
    "在 3.1 降维 ‣ 3 经典机器学习中的应用 ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 的解集是 $\mathbb{R}^{m}$ 中所有
    $l$ 维线性子空间的集合，可以用 Graßmann 流形表示，即，
- en: '|  | $\mathfrak{Gr}(l,m):=\left\{\mathbf{V}\mathbf{V}^{\top}&#124;\mathbf{V}\in
    St(l,m)\right\}.$ |  | (30) |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathfrak{Gr}(l,m):=\left\{\mathbf{V}\mathbf{V}^{\top} \mid \mathbf{V}\in
    St(l,m)\right\}.$ |  | (30) |'
- en: 'As shown above, most linear DR methods begin with solving $tr(V^{T}AV)$ while
    nonlinear DR methods construct a graph by connecting nearby points, which captures
    information on the local neighborhood structure of data and forms a similar optimization
    problem. Taking the non-linear DR method LE as an example, the Laplace matrix
    associated with the neighborhood graph [[44](#bib.bib44)] can be regarded as the
    symmetric matrix $A$ in Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣ 3
    Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如上所示，大多数线性降维方法从解决 $tr(V^{T}AV)$ 开始，而非线性降维方法通过连接邻近点构建图，这捕捉了数据局部邻域结构的信息，并形成类似的优化问题。以非线性降维方法
    LE 为例，与邻域图相关的拉普拉斯矩阵 [[44](#bib.bib44)] 可以视为公式 ([29](#S3.E29 "在 3.1 降维 ‣ 3 经典机器学习中的应用
    ‣ 深度学习几何优化调查：从欧几里得空间到黎曼流形")) 中的对称矩阵 $A$。
- en: '[b]'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[b]'
- en: 'Table 1: Summary of Dimension Reduction Algorithms'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '表 1: 降维算法总结'
- en: '| Methods | $Linear/Non-Linear^{1}$ | $Global/Local^{2}$ | Properties |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 方法 | $线性/非线性^{1}$ | $全局/局部^{2}$ | 属性 |'
- en: '| LDA | Linear | Global | is supervised, uses prior knowledge of categories,
    is limited to Gaussian distribution samples |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| LDA | 线性 | 全局 | 是有监督的，使用类别的先验知识，限制于高斯分布样本 |'
- en: '| PCA | Linear | Global | is unsupervised, uses orthogonal principal components
    to eliminate interactions between each components |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| PCA | 线性 | 全局 | 是无监督的，使用正交主成分消除各成分之间的相互作用 |'
- en: '| MDS | Non-Linear | Global | has simple calculation, preserves the data relationship
    in original space, is visualization-friendly, mistakenly assumes that each dimension has
    a same contribution |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| MDS | 非线性 | 全局 | 计算简单，保留了原始空间中的数据关系，适合可视化，错误地假设每个维度有相同的贡献 |'
- en: '| ISOMAP | Non-Linear | Global | suits low dimensional manifolds with a flat interior
    rather than that with large internal curvature, has high computation cost |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| ISOMAP | 非线性 | 全局 | 适用于具有平坦内部的低维流形，而不是具有大内曲率的流形，计算成本高 |'
- en: '| LLE | Non-Linear | Local | suits non-closed locally linear low dimensional
    manifolds, has small computational complexity, is limited to dense uniform dataset,
    is sensitive to the number of nearest neighbor samples |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| LLE | 非线性 | 局部 | 适用于非封闭的局部线性低维流形，计算复杂度小，限于稠密均匀数据集，对最近邻样本的数量敏感 |'
- en: '| LE | Non-Linear | Local | preserves local features, is less sensitive to
    outliers and noise, has a stable embedding |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| LE | 非线性 | 局部 | 保留局部特征，对离群点和噪声的敏感性较小，嵌入稳定 |'
- en: '| LPP | Linear | Local | is defined at any point in space, i.e., can be generalized
    to the testing set and not limited to the training set |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| LPP | 线性 | 局部 | 在空间中的任何点定义，即可以推广到测试集而不仅限于训练集 |'
- en: '1'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1'
- en: Linear represents linear projection mapping, while non-linear represents non-linear
    projection mapping.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 线性表示线性投影映射，而非线性表示非线性投影映射。
- en: '2'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2'
- en: The global/local represents the geometric relationship of the input data.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 全局/局部代表输入数据的几何关系。
- en: 3.2 Inverse Problem
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 逆问题
- en: 'Aiming to explore internal patterns from phenomena [[45](#bib.bib45)], an inverse
    problem has a significant impact on practical applications. For example, the following
    practical problems can be modeled as inverse problems: i) deducing structural
    information in human body from the X-ray; and ii) infering interior appearance
    of stratigraphy from seismic wave. An inverse problem can be viewed as reconstructing
    inputs from outputs as follows,'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 旨在从现象中探索内部模式 [[45](#bib.bib45)]，逆问题对实际应用具有重要影响。例如，以下实际问题可以被建模为逆问题：i) 从 X 光中推导人体结构信息；ii)
    从地震波推测地层的内部外观。逆问题可以被视为从输出重建输入，如下所示，
- en: '|  | $\mathbf{y}=\bm{{W}}{\mathbf{x}},$ |  | (31) |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathbf{y}=\bm{{W}}{\mathbf{x}},$ |  | (31) |'
- en: 'where $\mathbf{y}\in\mathbb{R}^{l}$ is the given output and $\bm{{W}}$ is a
    matrix that maps input data $\mathbf{x}$ to output data $\mathbf{y}$. The goal
    of the inverse problem in Equation ([31](#S3.E31 "In 3.2 Inverse Problem ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) is to recover $\mathbf{x}$ on the
    premise that $\mathbf{y}$ is a priori. It is challenging to get a precise solution,
    however, an approximate solution can be achieved by confining the parameter matrix
    $\bm{{W}}$ to reside on a smooth Riemannian manifold. Let the sum of elements
    in the same row of matrix $\bm{{W}}$ be exact 1, Equation ([31](#S3.E31 "In 3.2
    Inverse Problem ‣ 3 Applications in Classical Machine Learning ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold"))
    can be solved by optimization on the oblique manifold $\mathcal{M}$ where matrices
    all have unit row sums [[7](#bib.bib7)], i.e.,'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbf{y}\in\mathbb{R}^{l}$ 是给定的输出，而 $\bm{{W}}$ 是一个将输入数据 $\mathbf{x}$ 映射到输出数据
    $\mathbf{y}$ 的矩阵。方程中的逆问题（[31](#S3.E31 "在 3.2 逆问题 ‣ 3 经典机器学习中的应用 ‣ 深度学习的几何优化调查：从欧几里得空间到黎曼流形")）的目标是在
    $\mathbf{y}$ 是先验已知的前提下恢复 $\mathbf{x}$。虽然获取精确解是具有挑战性的，但可以通过将参数矩阵 $\bm{{W}}$ 限制在光滑的黎曼流形上来获得近似解。让矩阵
    $\bm{{W}}$ 中同一行的元素之和恰好为 1，方程（[31](#S3.E31 "在 3.2 逆问题 ‣ 3 经典机器学习中的应用 ‣ 深度学习的几何优化调查：从欧几里得空间到黎曼流形")）可以通过在所有具有单位行和的斜截流形
    $\mathcal{M}$ 上进行优化来解决 [[7](#bib.bib7)]，即，
- en: '|  | $\min_{\bm{{W}}\in\mathcal{M}}\left\&#124;\mathbf{y}-\bm{{W}}\mathbf{x}\right\&#124;_{2}^{2}.$
    |  | (32) |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{\bm{{W}}\in\mathcal{M}}\left\&#124;\mathbf{y}-\bm{{W}}\mathbf{x}\right\&#124;_{2}^{2}.$
    |  | (32) |'
- en: 3.3 Dictionary Learning
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 字典学习
- en: As a specific inverse problem, dictionary learning has been widely used to obtain
    the most essential features of input data [[23](#bib.bib23)]. Let $X\in R_{n\times
    k}$ denote the input sample, in dictionary learning, $X$ is expanded into a linear
    combination as
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个具体的逆问题，字典学习已被广泛用于获取输入数据中最重要的特征 [[23](#bib.bib23)]。设 $X\in R_{n\times k}$
    表示输入样本，在字典学习中，$X$ 被扩展为线性组合如下
- en: '|  | $X=D_{1}\phi_{1}+\cdots+D_{n}\phi_{n},$ |  | (33) |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  | $X=D_{1}\phi_{1}+\cdots+D_{n}\phi_{n},$ |  | (33) |'
- en: 'where $D_{1},\cdots,D_{n}$ represent the most essential features to be learned
    from the input, while $\phi_{1},\cdots,\phi_{n}$ indicate combination coefficients
    of features $D_{1},\cdots,D_{n}$. Let $D\in R_{k\times n}$ indicate the dictionary
    set $\{D_{1},\cdots,D_{n}\}$ and $\Phi\in R_{n\times r}$ indicate the set $\{\phi_{1},\cdots,\phi_{n}\}$,
    Equation ([33](#S3.E33 "In 3.3 Dictionary Learning ‣ 3 Applications in Classical
    Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) can be simplified as follows,'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D_{1},\cdots,D_{n}$ 代表从输入中学习到的最重要特征，而 $\phi_{1},\cdots,\phi_{n}$ 表示特征 $D_{1},\cdots,D_{n}$
    的组合系数。设 $D\in R_{k\times n}$ 表示字典集 $\{D_{1},\cdots,D_{n}\}$，$\Phi\in R_{n\times
    r}$ 表示集合 $\{\phi_{1},\cdots,\phi_{n}\}$，方程 ([33](#S3.E33 "在 3.3 字典学习 ‣ 3 在经典机器学习中的应用
    ‣ 深度学习的几何优化综述：从欧几里得空间到黎曼流形")) 可以简化为如下，
- en: '|  | $X=D\Phi,$ |  | (34) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $X=D\Phi,$ |  | (34) |'
- en: 'where $D$ and $\Phi$ can have various kinds of combinations. Dictionary learning
    aims to learn a $D$ that makes the coefficients $\Phi$ be zero or close to zero,
    i.e., a sparse representation of samples $X$. The dictionary $D$ and the sparse
    coefficients $\Phi$ are calculated alternately. When $\Phi$ is fixed, the dictionary
    learning part is the same as the form of Equation ([31](#S3.E31 "In 3.2 Inverse
    Problem ‣ 3 Applications in Classical Machine Learning ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")),
    which is an inverse problem of reconstructing $D$. Let $\left\|\phi\right\|_{0}$
    denote the number of entries in $\Phi$ that are different from zero, the dictionary
    $D$ is subject to $\left\|D_{1}\right\|=...=\left\|D_{n}\right\|=1$. Therefore,
    the above dictionary learning problem can be transformed to the following minimization
    problem on the oblique manifold:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D$ 和 $\Phi$ 可以有多种组合方式。字典学习旨在学习一个 $D$ 使得系数 $\Phi$ 为零或接近零，即样本 $X$ 的稀疏表示。字典
    $D$ 和稀疏系数 $\Phi$ 是交替计算的。当 $\Phi$ 固定时，字典学习部分与方程 ([31](#S3.E31 "在 3.2 反问题 ‣ 3 在经典机器学习中的应用
    ‣ 深度学习的几何优化综述：从欧几里得空间到黎曼流形")) 的形式相同，这是重建 $D$ 的反问题。设 $\left\|\phi\right\|_{0}$
    表示 $\Phi$ 中非零条目的数量，字典 $D$ 满足 $\left\|D_{1}\right\|=...=\left\|D_{n}\right\|=1$。因此，上述字典学习问题可以转化为在斜流形上的以下最小化问题：
- en: '|  | $\operatorname*{argmin}_{D\in\textit{OB}(k,n)}\left\&#124;X-D\Phi\right\&#124;_{2}^{2}+\lambda\left\&#124;\Phi\right\&#124;_{0}.$
    |  | (35) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\operatorname*{argmin}_{D\in\textit{OB}(k,n)}\left\|X-D\Phi\right\|_{2}^{2}+\lambda\left\|\Phi\right\|_{0}.$
    |  | (35) |'
- en: 3.4 Analysis Operator Learning
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 分析算子学习
- en: Analysis operator learning assumes that a few operators are sufficient to represent
    observed high-dimensional variables [[46](#bib.bib46)]. However, these operators
    are implicit and unobserved, for instance, store environment and service quality
    are latent operators hidden behind the observed variable “price”. The goal of
    analysis operator learning is to find out these invisible operators, since low-dimensional
    operators can simplify original high-dimensional variables.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 分析算子学习假设几个算子足以表示观察到的高维变量[[46](#bib.bib46)]。然而，这些算子是隐含的且不可观察的，例如，存储环境和服务质量是隐藏在观察变量“价格”背后的潜在算子。分析算子学习的目标是找出这些隐形的算子，因为低维算子可以简化原始的高维变量。
- en: Let $X$ be original high-dimensional variables and $F$ be latent operators with
    lower dimensions, the analysis operator learning can be generally formulated as
    follows
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 设 $X$ 为原始高维变量，$F$ 为低维潜在算子，分析算子学习通常可以表述为如下
- en: '|  | $X=AF,$ |  | (36) |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | $X=AF,$ |  | (36) |'
- en: where $A$ denotes the operator loading matrix, in which the element $A_{ij}$
    represents the load of variable $x_{i}$ on factor $f_{j}$. It is proved that the
    parameter $A$ can be positive [[47](#bib.bib47)], the analysis operator learning
    can therefore be converted to an optimization problem on the positive manifold
    $\mathcal{M}$ as follows
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 表示算子载荷矩阵，其中元素 $A_{ij}$ 表示变量 $x_{i}$ 在因子 $f_{j}$ 上的载荷。已经证明参数 $A$ 可以是正的[[47](#bib.bib47)]，因此分析算子学习可以转化为在正流形
    $\mathcal{M}$ 上的优化问题，如下所示
- en: '|  | $\min_{A\in\mathcal{M}}\left\&#124;X-AF\right\&#124;_{2}^{2}.$ |  | (37)
    |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|  | $\min_{A\in\mathcal{M}}\left\|X-AF\right\|_{2}^{2}.$ |  | (37) |'
- en: 3.5 Temporal Model
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5 时间模型
- en: The temporal probability model is composed of a transition model describing
    the state evolution over time and a sensor model describing the observation process
    [[27](#bib.bib27)]. A temporal model is helpful to cope with filtering, prediction
    and smoothing. In the transition model, next state $z_{t+1}$ is transited from
    the current state $z_{t}$, independent from previous states. Given the time-relevant
    transition probability $A(t)$, the transition process of states can be modeled
    as
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 时间概率模型由描述状态随时间演变的转移模型和描述观测过程的传感器模型组成[[27](#bib.bib27)]。时间模型有助于处理滤波、预测和光滑。在转移模型中，下一状态
    $z_{t+1}$ 是从当前状态 $z_{t}$ 转移过来的，与之前的状态无关。给定时间相关的转移概率 $A(t)$，状态的转移过程可以被建模为
- en: '|  | $z_{t+1}=A(t)\cdot z_{t}+\epsilon(t),$ |  | (38) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $z_{t+1}=A(t)\cdot z_{t}+\epsilon(t),$ |  | (38) |'
- en: where the noise $\epsilon(t)$ follows the Gaussian distribution.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 其中噪声 $\epsilon(t)$ 遵循高斯分布。
- en: States are invisible and a hidden state can manifest as a specific observation
    with the help of an emission probability. The current observation $x_{t}$ is only
    defined by the current state $z_{t}$, having nothing to do with previous states
    and observations. Given a time-varying emission probability $C(t)$, the observation
    process can be modeled as
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 状态是不可见的，隐藏状态可以通过发射概率表现为特定的观测。当前观测 $x_{t}$ 仅由当前状态 $z_{t}$ 定义，与之前的状态和观测无关。给定时间变化的发射概率
    $C(t)$，观测过程可以被建模为
- en: '|  | $x_{t}=C(t)\cdot z_{t}+\delta(t),$ |  | (39) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $x_{t}=C(t)\cdot z_{t}+\delta(t),$ |  | (39) |'
- en: where the noise $\delta(t)$ follows the Gaussian distribution.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 其中噪声 $\delta(t)$ 遵循高斯分布。
- en: 'As a mixture of Equation ([38](#S3.E38 "In 3.5 Temporal Model ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) and Equation ([39](#S3.E39 "In
    3.5 Temporal Model ‣ 3 Applications in Classical Machine Learning ‣ A Survey of
    Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")),
    temporal models can be divided into hidden Markov models and linear dynamic systems.
    A hidden Markov model has discrete hidden state variables while the hidden state
    and observed variables of a linear dynamic system obey Gaussian distribution.
    Let $n$ represent the size of the temporal sequence, the expectation of observation
    sequences $E[x_{0},x_{1},x_{2}\cdots]$ can be deduced as:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '作为方程 ([38](#S3.E38 "In 3.5 Temporal Model ‣ 3 Applications in Classical Machine
    Learning ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")) 和方程 ([39](#S3.E39 "In 3.5 Temporal Model ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) 的混合，时间模型可以分为隐马尔可夫模型和线性动态系统。隐马尔可夫模型具有离散的隐藏状态变量，而线性动态系统的隐藏状态和观测变量遵循高斯分布。设
    $n$ 代表时间序列的大小，观测序列 $E[x_{0},x_{1},x_{2}\cdots]$ 的期望可以推导为：'
- en: '|  | $[C(t),C(t)A(t),C(t)A(t)^{2}\cdots C(t)A(t)^{n-1}]\,z_{0},$ |  | (40)
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | $[C(t),C(t)A(t),C(t)A(t)^{2}\cdots C(t)A(t)^{n-1}]\,z_{0},$ |  | (40)
    |'
- en: 'where $z_{0}$ is the initial hidden state. It can be considered as a sequence
    of subspaces spanned by the emission and transition matrix columns at the corresponding
    time [[48](#bib.bib48)]. As is mentioned in Section [2.3](#S2.SS3 "2.3 Manifold
    Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold"), a point on the
    Graßmann manifold is a subspace. Therefore, the temporal model can be mathematically
    optimized on the Graßmann manifold.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $z_{0}$ 是初始隐藏状态。它可以被视为由对应时间的发射和转移矩阵列所张成的子空间序列[[48](#bib.bib48)]。如[2.3](#S2.SS3
    "2.3 Manifold Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")节中提到的，Graßmann
    流形上的一点是一个子空间。因此，时间模型可以在 Graßmann 流形上进行数学优化。'
- en: 4 Applications in Deep Learning
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度学习中的应用
- en: 'With the increasing attention to geometric optimization, more and more deep
    learning methods have developed to combine with it. Geometric optimization techniques
    vary with different deep learning backbones (e.g., CNN, RNN and GNN). Therefore,
    this section classifies applications in deep learning into the following categories,
    i.e., i) geometric CNN; ii) geometric RNN; iii) geometric GNN and iv) geometric
    optimization for other deep learning methods, such as transfer learning and optimal
    transport. Orthogonal manifold is widely employed in geometric CNNs to reduce
    feature redundancy. Examples include utilizing kernel orthogonality in Orthogonal
    CNNs [[6](#bib.bib6)], optimization on Submanifolds of Convolution Kernels in
    CNNs [[49](#bib.bib49)], and regularizing the convolution kernel with orthogonality
    when training deep CNNs [[12](#bib.bib12)]. In addition, geometric CNNs can leverage
    the unique structure of Stiefel manifold [[50](#bib.bib50)] and Graßmann manifold
    [[51](#bib.bib51)]. Geometric RNNs take advantage of the norm-keeping property
    of orthogonal and unitary manifolds to alleviate gradient explosion and vanishing
    problems. Examples include complex unitary matrices in Unitary Evolution Recurrent
    Neural Networks [[14](#bib.bib14)], and the special orthogonal group and unitary
    group in Cheap Orthogonal Constraints: A Simple Parameterization of the Orthogonal
    and Unitary Group [[13](#bib.bib13)]. Geometric GNNs pay much attention to hyperbolic
    manifold and extensively use it for structure capturing. Examples include the
    hyperbolic GNN [[52](#bib.bib52)] and a geometric neural network which incorporates
    Euclidean space with hyperbolic geometry [[53](#bib.bib53)].'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对几何优化的关注日益增加，越来越多的深度学习方法开始与之结合。几何优化技术在不同的深度学习骨干网络（如CNN、RNN和GNN）中有所不同。因此，本节将深度学习中的应用分类为以下几类：即i)
    几何CNN；ii) 几何RNN；iii) 几何GNN以及iv) 针对其他深度学习方法（如迁移学习和最优传输）的几何优化。正交流形在几何CNN中广泛应用以减少特征冗余。示例包括在正交CNN中利用核正交性[[6](#bib.bib6)]，在CNN中对卷积核的子流形进行优化[[49](#bib.bib49)]，以及在训练深度CNN时通过正交性正则化卷积核[[12](#bib.bib12)]。此外，几何CNN可以利用Stiefel流形[[50](#bib.bib50)]和Graßmann流形[[51](#bib.bib51)]的独特结构。几何RNN利用正交和单位流形的范数保持特性，以缓解梯度爆炸和消失问题。示例包括单位进化递归神经网络中的复单位矩阵[[14](#bib.bib14)]，以及在《便宜的正交约束：正交和单位群的简单参数化》中使用的特殊正交群和单位群[[13](#bib.bib13)]。几何GNN特别关注双曲流形，并广泛用于结构捕获。示例包括双曲GNN[[52](#bib.bib52)]和一个将欧几里得空间与双曲几何结合的几何神经网络[[53](#bib.bib53)]。
- en: 4.1 Geometric CNN
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 几何CNN
- en: Deep CNN has achieved great success in various computer vision tasks, such as
    image recognition [[54](#bib.bib54)] and segmentation tasks [[55](#bib.bib55)].
    CNN can automatically learn features from large-scale data by benefiting from
    three essential structures, i.e., convolution, activation, and pooling structures
    [[10](#bib.bib10)]. Although CNNs have worked efficiently, using the entire Euclidean
    space to search optimal solutions cause problems (e.g., training instability and
    feature redundancy) that hinder the further development. To alleviate these problems,
    geometric optimization approaches optimize CNNs on the suitable Riemannian manifold
    via kernel space, geometric regularization, and quasi-CNN architectures with parameters
    on the manifold.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 深度CNN在各种计算机视觉任务中取得了巨大成功，如图像识别[[54](#bib.bib54)]和分割任务[[55](#bib.bib55)]。CNN通过利用卷积、激活和池化这三种基本结构，从大规模数据中自动学习特征[[10](#bib.bib10)]。尽管CNN运作高效，但在整个欧几里得空间中搜索最优解会导致一些问题（例如，训练不稳定和特征冗余），这些问题阻碍了进一步的发展。为了解决这些问题，几何优化方法通过在合适的黎曼流形上进行核空间、几何正则化和带有流形参数的准CNN架构来优化CNN。
- en: Kernel Space. A low-dimensional manifold is often embedded in the high-dimensional
    Euclidean space. Kernel functions can map original features to a higher dimensional
    space. Therefore, with the help of kernel functions, computationally cheap operations
    on manifolds can represent complex operations in Euclidean space. Kernel spaces
    can be utilized and described by topological smooth manifolds. For example, positive-definite
    kernels, which are known as Graßmann kernels on the Graßmann manifold, can be
    used to map the manifold into a Hilbert space [[56](#bib.bib56)]. Zhang et al.
    [[57](#bib.bib57)] designed a new kind of Graßmann kernel based on canonical correlations
    to distinguish one class from others more accurately. Liu et al. [[58](#bib.bib58)]
    designed RBF kernels for linear subspace, covariance matrix, and Gaussian distribution
    to optimize emotion video recognition on the Riemannian manifolds. Hariri et al.
    [[59](#bib.bib59)] defined a kernel based on the SPD covariance matrix to indicate
    the similarity of two face images for face matching.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 核空间。低维流形通常嵌入在高维欧几里得空间中。核函数可以将原始特征映射到更高维的空间。因此，借助核函数，流形上的计算成本低的操作可以表示欧几里得空间中的复杂操作。核空间可以利用拓扑光滑流形进行描述。例如，正定核，也被称为Graßmann流形上的Graßmann核，可以用来将流形映射到希尔伯特空间[[56](#bib.bib56)]。Zhang等人[[57](#bib.bib57)]
    设计了一种基于典型相关性的新的Graßmann核，以更准确地区分不同类别。Liu等人[[58](#bib.bib58)] 设计了用于线性子空间、协方差矩阵和高斯分布的RBF核，以优化在黎曼流形上的情感视频识别。Hariri等人[[59](#bib.bib59)]
    定义了一种基于SPD协方差矩阵的核，用于指示两个面部图像的相似性，以进行人脸匹配。
- en: Kernel space constructed on nonlinear data helps learn the inherent manifold
    structure. Yuan et al. [[60](#bib.bib60)] combined manifold kernel space with
    deep learning architecture for scene recognition. To preserve the geometric structure
    of input scene images and achieve a greater representational ability, [[60](#bib.bib60)]
    defines a low-level feature layer $X$ and a hidden manifold kernel space $Y$ as
    a base unit. Moreover, the deep architecture is unit-by-unit and $Y_{k}$ serves
    as the input of another base unit to generate the next hidden space $Y_{k+1}$.
    Comparative experiments evaluate the performance of the manifold regularized deep
    network on the large-scale scene data set.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 基于非线性数据构建的核空间有助于学习固有的流形结构。Yuan等人[[60](#bib.bib60)] 将流形核空间与深度学习架构结合用于场景识别。为了保持输入场景图像的几何结构并实现更强的表示能力，[[60](#bib.bib60)]
    定义了低级特征层 $X$ 和一个隐藏的流形核空间 $Y$ 作为基础单元。此外，深度架构是逐单元构建的，$Y_{k}$ 作为另一个基础单元的输入，用于生成下一个隐藏空间
    $Y_{k+1}$。比较实验评估了流形正则化深度网络在大规模场景数据集上的性能。
- en: Ozay et al. [[49](#bib.bib49)] considered the kernel estimation problem in CNNs
    as an optimization on embedded or immersed submanifolds of kernels. [[49](#bib.bib49)]
    explores geometric properties of convolution kernel space in CNNs and reveals
    that different kernel normalization methods induce different geometric properties.
    For example, the orthonormal normalization manner implies Stiefel manifold, while
    kernels normalized with the unit-norm reside on the sphere manifold. Furthermore,
    [[49](#bib.bib49)] proposes an SGD algorithm for optimization on kernel submanifolds.
    Experiments carried out on three kernel submanifolds confirm that the above approach
    can boost the performance of traditional CNN training.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Ozay等人[[49](#bib.bib49)] 将CNN中的核估计问题视为对嵌入或浸入的核子流形的优化。[[49](#bib.bib49)] 探索了CNN中卷积核空间的几何性质，并揭示了不同的核归一化方法会引入不同的几何特性。例如，正交归一化方式意味着Stiefel流形，而归一化为单位范数的核则位于球面流形上。此外，[[49](#bib.bib49)]
    提出了一个针对核子流形的SGD优化算法。对三个核子流形进行的实验确认了上述方法可以提升传统CNN训练的性能。
- en: 'Geometric Regularization. Regularization acts as the penalty term of the optimization
    function. It is used to impose restrictions on the parameters of the optimization
    function. The commonly used geometric regularization is the orthogonal constraint,
    aiming to restrict parameters to be on the orthogonal manifold. Recall orthogonal
    matrices $W^{T}W=I$ introduced in Section [2.3](#S2.SS3 "2.3 Manifold Examples
    ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold"), orthogonal regularization
    methods are roughly divided into hard orthogonality as'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;W^{T}W-I\&#124;_{F}^{2}$ |  | (41) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
- en: and soft orthogonality as
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lambda\&#124;W^{T}W-I\&#124;_{F}^{2}$ |  | (42) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
- en: where $\|\cdot\|_{F}$ indicates the Frobenius norm and $\lambda$ represents
    a relaxation coefficient. Based on the soft orthogonality, we can achieve double
    soft orthogonality as
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lambda(\&#124;W^{T}W-I\&#124;_{F}^{2}+\&#124;WW^{T}-I\&#124;_{F}^{2}).$
    |  | (43) |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
- en: Based on the observation that the kernel orthogonality is necessary but insufficient
    for the orthogonal convolution, Wang et al. [[6](#bib.bib6)] proposed an approach
    where orthogonality constraints directly regularize a convolution layer. During
    training, the convolution filter $K$ is transformed into a Doubly Block-Toeplitz
    (DBT) matrix and the spectrum is regularized to be uniform, which requires row
    or column orthogonality. The orthogonality constraint on the DBT matrix helps
    relieve exploding and vanishing gradient problems, making the training more stable.
    Moreover, a number of experiments show that it can achieve amazing performance
    such as stronger robustness and better generalization.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Bansal et al. [[12](#bib.bib12)] observed that orthogonality can stabilize the
    energy distribution of activations within CNNs and enhance the efficiency of training.
    [[12](#bib.bib12)] compares different orthogonality regularizers, e.g. soft orthogonality,
    double soft orthogonality and mutual coherence regularization that lowers the
    column correlation as much as possible to enforce orthogonality. Meanwhile, [[12](#bib.bib12)]
    designs a novel orthogonality regularizer named Spectral Restricted Isometry Property
    Regularization, which focuses on minimizing the spectral norm of $W^{T}W-I$. Remarkable
    experimental results suggest that regarding orthogonality regularizations as standard
    tools for training deep CNNs offers better accuracy and stablity.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: In order to estimate human face poses under challenging circumtances such as
    complex background or various orientations, Hong et al. [[61](#bib.bib61)] proposed
    manifold regularized convolutional layers (MRCL) to enhance the nonlinear locality
    constraints of CNN parameters. With MRCL being on top of traditional CNN’s pooling
    and activation operations, a low-rank manifold structure of latent data can be
    recovered for better optimization. By employing multitask learning with low-rank
    learning, multimodal of different data representations can be combined to predicate
    face postures. Comparative experiments validate the benefit of imposing manifold
    regularization to traditional convolutional layers.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在复杂背景或各种方向等挑战性情况下估计人脸姿势，洪等人[[61](#bib.bib61)]提出了流形正则化卷积层（MRCL），以增强CNN参数的非线性局部约束。MRCL位于传统CNN的池化和激活操作之上，可以恢复潜在数据的低秩流形结构以实现更好的优化。通过采用低秩学习的多任务学习，可以将不同数据表示的多模态信息结合起来预测面部姿态。对比实验验证了对传统卷积层施加流形正则化的好处。
- en: Roufosse et al. [[62](#bib.bib62)] proposed a spectral unsupervised functional
    map network (SURFMNet) where the matching network from one shape to another is
    constrained to the orthogonal manifold. SURFMNet computes correspondences across
    3D shapes using unsupervised learning, i.e., building shape correspondences without
    ground truth. Solid experimental results support the consistent superiority of
    SURFMNet compared to state-of-the-art unsupervised shape matching methods. Experimental
    results also show that SURFMNet is comparable to supervised ones.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Roufosse等人[[62](#bib.bib62)]提出了一种谱无监督功能图网络（SURFMNet），其中形状匹配网络从一个形状到另一个形状的匹配被约束到正交流形。SURFMNet使用无监督学习计算3D形状之间的对应关系，即在没有真实数据的情况下建立形状对应关系。实验证据支持SURFMNet在与最先进的无监督形状匹配方法相比具有一致的优势。实验结果还显示SURFMNet与监督方法相当。
- en: Different from existing methods that shallowly learn Lie group features, Huang
    et al. [[63](#bib.bib63)] incorporated a Lie group structure to parameter matrices
    in the deep human action recognition network. The proposed skeleton-based human
    model $(V,E)$ is a binary relation, where $V$ represents a set of vertexes that
    consists of body joints $(v_{1},\dots,v_{N})$ and $E$ represents a set of edges
    that consists of body bones $(e_{1},\dots,e_{M})$. The rotation matrix is represented
    by the axis-angle model based on the skeleton and forms the special orthogonal
    group. To preserve the Lie group structure of the input rotation matrix, the above
    human action recognition network is optimized on the Lie group manifold and mapped
    to a tangent space for the final classification.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于现有的浅层学习Lie群特征的方法，黄等人[[63](#bib.bib63)]在深度人体动作识别网络中引入了Lie群结构到参数矩阵。所提议的基于骨架的人体模型$(V,E)$是一个二元关系，其中$V$代表由身体关节$(v_{1},\dots,v_{N})$组成的顶点集合，而$E$代表由身体骨骼$(e_{1},\dots,e_{M})$组成的边集合。旋转矩阵通过基于骨架的轴角模型表示，形成了特殊的正交群。为了保留输入旋转矩阵的Lie群结构，上述人体动作识别网络在Lie群流形上进行优化，并映射到切空间进行最终分类。
- en: Similar to the above action recognition network [[63](#bib.bib63)], Chen et
    al. [[64](#bib.bib64)] put forward a deep manifold learning (DML) framework to
    learn manifold information and deep representations of action videos. [[64](#bib.bib64)]
    studies that leveraging geometry information in deep learning contributes to high
    accuracy and efficiency for action recognition. To extract more expressing features,
    the DML framework applies a manifold regularizer on the previous layer, label
    information and manifold parameters. Furthermore, adapting the DML framework to
    restricted Boltzmann machine can relieve the overfitting problem and improve the
    recognition accuracy.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于上述动作识别网络[[63](#bib.bib63)]，陈等人[[64](#bib.bib64)]提出了一种深度流形学习（DML）框架，以学习动作视频的流形信息和深度表示。[[64](#bib.bib64)]研究表明，利用深度学习中的几何信息有助于提高动作识别的准确性和效率。为了提取更多表达特征，DML框架在前一层、标签信息和流形参数上应用了流形正则化器。此外，将DML框架适应于限制玻尔兹曼机可以缓解过拟合问题并提高识别准确性。
- en: Quasi-CNN Architecture. Kernel methods and orthogonal regularization do not
    change fundamental CNN components (e.g., convolution and pooling operations).
    Another method of applying geometric optimization to deep learning is to mimic
    traditional CNN architecture and establish a new architecture suitable for the
    manifold structure. In this article, the above architecture is named as quasi-CNN
    architecture. Convolution and activation layers are rebuilt to induce geometric
    optimization in the quasi-CNN architecture. To achieve this goal, parameters in
    the quasi-CNN architecture are designed to reside on the compact Stiefel manifold.
    For a more intuitive explanation, this article takes the deep SPD matrix network
    (SPDNet) [[50](#bib.bib50)] and deep Graßmann neural network architecture (GrNet)
    [[51](#bib.bib51)] as examples.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Let $X$ be the input, and $W$ be the transformation parameter on the compact
    Stiefel manifold. First, SPDNet is designed for optimization on the SPD manifold.
    Bilinear mapping (BiMap) layer $f_{b}=WXW^{T}$ plays the role of convolution layers
    in traditional CNNs. Based on the eigenvalue decomposition $X$ = $U\Sigma U$,
    eigenvalue rectification (ReEig) layers $f_{r}=Umax(\epsilon I,\Sigma)U^{T}$ are
    designed to replace nonlinear activation layers and $\epsilon$ is the activation
    threshold. SPDNet designs the eigenvalue logarithm (LogEig) layer to flatten the
    Riemannian manifold to a flat space where classical Euclidean computations can
    be applied. GrNet is designed for optimization along the orthonormal manifold.
    Full rank mapping (FRMap) layers $f_{fr}=WX$ in GrNet replace the convolution
    layer in traditional CNNs. Inspired by the QR decomposition $X=QR$ where $Q$ is
    orthonormal, GrNet designs re-orthonormalization (ReOrth) layer $f_{fo}=XR^{-1}=Q$
    to achieve an orthonormal output. Unlike the LogEig layer in SPDnet, GrNet uses
    inner product $XX^{T}$ to reduce the manifold to a flat Euclidean space. After
    pooling operations on the resulting Euclidean data, GrNet designs orthonormal
    mapping (OrthMap) layer $f_{om}=U_{1:q}$ to transform the output matrix back to
    the orthonormal manifold, where $U_{1:q}$ denotes the first $q$ largest eigenvectors
    achieved by the eigenvalue decomposition.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Geometric RNN
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RNNs are designed to process sequential data since they can capture spatial
    and temporal dependencies between the sequential input. Therefore, RNN can be
    applied in tasks such as speech recognition, text prediction, and machine translation.
    Given an input sequence $X_{\tau}={x_{1},x_{2},\cdots,x_{\tau}}$ ($x_{i}\in\mathbb{R}^{n}$)
    with length $\tau$, a basic RNN framework is aimed to generate the output sequence
    $Y_{\tau}={y_{1},y_{2},\cdots,y_{\tau}}$ ($y_{i}\in\mathbb{R}^{p}$). With hidden
    state $h$ passed recurrently into the model at each time step, output predictions
    $o_{i}\in\mathbb{R}^{p}$ of the RNN are computed as follows [[65](#bib.bib65)]:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle h_{i}$ | $\displaystyle=\sigma(Ux_{i}+Wh_{i-1}+b),$ |  |
    (44) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle o_{i}$ | $\displaystyle=Vh_{i}+c,$ |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: where $U\in\mathbb{R}^{m\times n}$ is the input weight matrix, $W\in\mathbb{R}^{m\times
    m}$ is the recurrent weight matrix, $h_{i-1}\in\mathbb{R}^{m}$ is the previous
    hidden state, $b\in\mathbb{R}^{m}$ is the input bias, $\sigma(\cdot)$ is a pointwise
    nonlinearity function, $h_{i}\in\mathbb{R}^{m}$ is the current hidden state, $V\in\mathbb{R}^{p\times
    m}$ is the output weight matrix, and $c\in\mathbb{R}^{p}$ is the output bias.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Orthogonal RNN (ORNN)
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Denote $\mathcal{L}$ as the objective function to be minimized, the gradient
    of the loss function for the hidden state is computed as:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial h_{i}}$ | $\displaystyle=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\cdot\frac{\partial h_{\tau}}{\partial h_{i}}=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\cdot\prod_{j=i}^{\tau-1}\frac{\partial h_{j+1}}{\partial h_{j}}=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}(\prod_{j=i}^{\tau-1}D_{j+1}W^{T}),$ |  | (45) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
- en: 'where $D_{j+1}\in\mathbb{R}^{m\times m}$ is a diagonal matrix, whose entries
    consist of the derivate of the activation function. The pointwise non-linearity
    function $\sigma(\cdot)$ in Equation ([44](#S4.E44 "In 4.2 Geometric RNN ‣ 4 Applications
    in Deep Learning ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) is suggested to be a rectified linear
    unit (ReLU) function [[66](#bib.bib66), [67](#bib.bib67)], whose output has a
    minimum value of $0$. The input $D_{j+1}$ has at least one non-zero entry of the
    derivative value for all $j$. Taking the Euclidean $l_{2}-norm$ to both sides
    of Equation ([45](#S4.E45 "In 4.2.1 Orthogonal RNN (ORNN) ‣ 4.2 Geometric RNN
    ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")), we have:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;\frac{\partial\mathcal{L}}{\partial h_{i}}\right\&#124;_{2}$
    | $\displaystyle\leqslant(\prod_{j=i}^{\tau-1}\left\&#124;D_{j+1}W^{T}\right\&#124;_{2})\left\&#124;\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\right\&#124;_{2}=(\prod_{j=i}^{\tau-1}\left\&#124;W\right\&#124;_{2})\left\&#124;\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\right\&#124;_{2}$ |  | (46) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
- en: 'If $\|W\|_{2}$ is greater than one, $\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ grows exponentially as the increase of $\tau$. As a result,
    the norm of the gradient in Equation ([46](#S4.E46 "In 4.2.1 Orthogonal RNN (ORNN)
    ‣ 4.2 Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold"))
    disclosures the well-known gradient exploding problem that hinders the RNN from
    training [[68](#bib.bib68)]. If $\|W\|_{2}$ is smaller than one, $\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ declines exponentially as the increase of $\tau$, which leads
    to gradient vanishing problems [[68](#bib.bib68)].'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 $\|W\|_{2}$ 大于一，$\left\|\frac{\partial\mathcal{L}}{\partial h_{i}}\right\|_{2}$
    会随着 $\tau$ 的增加而指数级增长。因此，方程中 $\left\|\frac{\partial\mathcal{L}}{\partial h_{i}}\right\|_{2}$
    的范数揭示了著名的梯度爆炸问题，这会阻碍 RNN 的训练 [[68](#bib.bib68)]。如果 $\|W\|_{2}$ 小于一，$\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ 会随着 $\tau$ 的增加而指数级下降，这导致梯度消失问题 [[68](#bib.bib68)]。
- en: 'A recent line of ORNNs imposes the orthogonal constraint on the hidden-to-hidden
    transformation of RNN. The recurrent weight transformation matrix $W$ is restricted
    to be on the orthogonal manifold. Let $A$ be an orthogonal matrix, for each vector
    $X$, its norm after orthogonal transformation is:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 最近一系列的 ORNN 对 RNN 的隐藏到隐藏变换施加了正交约束。递归权重变换矩阵 $W$ 被限制在正交流形上。设 $A$ 为一个正交矩阵，对于每个向量
    $X$，其经过正交变换后的范数为：
- en: '|  | $(AX)^{T}(AX)=X^{T}A^{T}AX=X^{T}X,$ |  | (47) |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '|  | $(AX)^{T}(AX)=X^{T}A^{T}AX=X^{T}X,$ |  | (47) |'
- en: 'which means that orthogonal transformations do not change the norm of the original
    vector. As a result, $\left\|\frac{\partial\mathcal{L}}{\partial h_{i}}\right\|_{2}$
    can remain invariant in ORNN when the transformation matrix $W$ in Equation ([46](#S4.E46
    "In 4.2.1 Orthogonal RNN (ORNN) ‣ 4.2 Geometric RNN ‣ 4 Applications in Deep Learning
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")) is orthogonal. Therefore, the exploding and vanishing gradient
    problem of RNN can be alleviated. Moreover, orthogonal constraints can be generalized
    to unitary constraints in the complex domain.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着正交变换不会改变原始向量的范数。因此，当方程中的变换矩阵 $W$ ([46](#S4.E46 "在 4.2.1 正交 RNN (ORNN) ‣
    4.2 几何 RNN ‣ 4 深度学习中的应用 ‣ 深度学习几何优化的调查：从欧几里得空间到黎曼流形")) 是正交的时，$\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ 可以在 ORNN 中保持不变。因此，RNN 的梯度爆炸和消失问题可以得到缓解。此外，正交约束可以在复数域中推广到单位ary
    约束。
- en: 4.2.2 Recent Advances of ORNN
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 ORNN 的最新进展
- en: uRNN [[14](#bib.bib14)] constructs a large unitary matrix by simple parametric
    unitary matrices, i.e., the unitary hidden-to-hidden matrix $W$ is composed as
    follows,
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: uRNN [[14](#bib.bib14)] 通过简单的参数单位ary 矩阵构造了一个大型单位ary 矩阵，即，单位ary 隐藏到隐藏矩阵 $W$ 由以下部分组成，
- en: '|  | $\displaystyle W=D_{3}R_{2}F^{-1}D_{2}\Pi R_{1}FD_{1},$ |  | (48) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle W=D_{3}R_{2}F^{-1}D_{2}\Pi R_{1}FD_{1},$ |  | (48) |'
- en: 'where $D$ is a diagonal matrix whose diagonal element $D_{j,j}=e^{iw_{j}}$
    is defined by the imaginary unit $i$ and parameters $w_{j}\in\mathbb{R}$, $R=I-\
    2\frac{vv^{\ast}}{\|v\|^{2}}$ is a reflection matrix with the complex vector $v\in\mathbb{C}^{n}$,
    $\Pi$ is a fixed random index permutation matrix, and $F$ and $F^{-1}$ are the
    Fourier and inverse Fourier transforms. In the matrix construction strategy like
    Equation ([48](#S4.E48 "In 4.2.2 Recent Advances of ORNN ‣ 4.2 Geometric RNN ‣
    4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")), the number of parameters,
    memory, and computational overhead increase slowly at approximately linear speeds.
    Therefore, the training cost of large hidden layers can be reduced. In uRNN, a
    variation of the nonlinear activation ReLU named modReLU has been proposed to
    maintain the phase of complex-valued hidden states:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D$ 是一个对角矩阵，其对角元素 $D_{j,j}=e^{iw_{j}}$ 由虚数单位 $i$ 和参数 $w_{j}\in\mathbb{R}$
    定义，$R=I-\ 2\frac{vv^{\ast}}{\|v\|^{2}}$ 是一个反射矩阵，$v\in\mathbb{C}^{n}$ 是复数向量，$\Pi$
    是一个固定的随机索引置换矩阵，$F$ 和 $F^{-1}$ 分别是傅里叶变换和反傅里叶变换。在类似于方程中的矩阵构造策略中 ([48](#S4.E48 "在
    4.2.2 ORNN 的最新进展 ‣ 4.2 几何 RNN ‣ 4 深度学习中的应用 ‣ 深度学习几何优化的调查：从欧几里得空间到黎曼流形"))，参数、内存和计算开销的增加速度大致为线性。因此，可以降低大隐藏层的训练成本。在
    uRNN 中，提出了一种名为 modReLU 的非线性激活函数 ReLU 的变体，以保持复数值隐藏状态的相位：
- en: '|  | $\sigma_{modReLU}(z)=\left\{\begin{aligned} &amp;(&#124;z&#124;+b)\frac{z}{&#124;z&#124;},\quad&amp;if\
    &#124;z&#124;+b\geq 0\\ &amp;0,\quad&amp;if\ &#124;z&#124;+b\leq 0\end{aligned}\right.$
    |  | (49) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sigma_{modReLU}(z)=\left\{\begin{aligned} &amp;(&#124;z&#124;+b)\frac{z}{&#124;z&#124;},\quad&amp;如果\
    &#124;z&#124;+b\geq 0\\ &amp;0,\quad&amp;如果\ &#124;z&#124;+b\leq 0\end{aligned}\right.$
    |  | (49) |'
- en: where $b$ $\in\mathbb{R}$ is a bias parameter. uRNN defines a matrix $U$ to
    map complex-valued hidden state $h_{t}$ to real-valued output for prediction.
    The corresponding loss function is calculated as follows
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b$ $\in\mathbb{R}$ 是一个偏置参数。uRNN 定义一个矩阵 $U$ 将复值隐藏状态 $h_{t}$ 映射到实值输出以进行预测。相应的损失函数计算如下：
- en: '|  | $o_{t}=U\left(\begin{aligned} Re(h_{t})\\ Im(h_{t})\end{aligned}\right)+b_{o},$
    |  | (50) |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '|  | $o_{t}=U\left(\begin{aligned} Re(h_{t})\\ Im(h_{t})\end{aligned}\right)+b_{o},$
    |  | (50) |'
- en: where $b_{o}$ is the output bias, $Re(h_{t})$ and $Im(h_{t})$ represent the
    real and imaginary part of $h_{t}$ respectively.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $b_{o}$ 是输出偏置，$Re(h_{t})$ 和 $Im(h_{t})$ 分别表示 $h_{t}$ 的实部和虚部。
- en: 'However, Wisdom et al. [[69](#bib.bib69)] noticed that the unitary parameter
    construction of Equation ([48](#S4.E48 "In 4.2.2 Recent Advances of ORNN ‣ 4.2
    Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")) cannot cover
    all $N\times N$ unitary matrices for $N>7$, i.e., at least one $N\times N$ unitary
    matrix cannot be represented in the form of Equation ([48](#S4.E48 "In 4.2.2 Recent
    Advances of ORNN ‣ 4.2 Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")). To address this problem, [[69](#bib.bib69)] designs a method to measure
    the representation capacity of the structured $N\times N$ unitary matrix. [[69](#bib.bib69)]
    comes up with a perspective that the unitary matrices parameterized by $P$ real-valued
    parameters for $P\geq N^{2}$ is full-capacity, which means that it can cover all
    $N\times N$ unitary matrices.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Wisdom 等人 [[69](#bib.bib69)] 注意到方程 ([48](#S4.E48 "在 4.2.2 近期进展的 ORNN ‣ 4.2
    几何 RNN ‣ 4 深度学习中的应用 ‣ 从欧几里得空间到黎曼流形的几何优化调查")) 的单位参数构造不能覆盖所有 $N\times N$ 单位矩阵，对于
    $N>7$ 的情况，即至少存在一个 $N\times N$ 单位矩阵不能以方程 ([48](#S4.E48 "在 4.2.2 近期进展的 ORNN ‣ 4.2
    几何 RNN ‣ 4 深度学习中的应用 ‣ 从欧几里得空间到黎曼流形的几何优化调查")) 的形式表示。为了解决这个问题，[[69](#bib.bib69)]
    设计了一种方法来测量结构化 $N\times N$ 单位矩阵的表示能力。[[69](#bib.bib69)] 提出了一个观点，即由 $P$ 个实值参数参数化的单位矩阵对于
    $P\geq N^{2}$ 是全容量的，这意味着它可以覆盖所有 $N\times N$ 单位矩阵。
- en: Unlike generating compound orthogonal matrices with simple ones, the Lie exponential
    map can achieve orthogonal constraint on the hidden-to-hidden transformation [[13](#bib.bib13)].
    The connected subjective exponential mapping $exp:\mathfrak{g}$ $\rightarrow$
    G on the special orthogonal group is defined as
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与生成复合正交矩阵不同，李代数指数映射可以实现隐藏到隐藏变换的正交约束 [[13](#bib.bib13)]。特殊正交群上的连通主观指数映射 $exp:\mathfrak{g}$
    $\rightarrow$ G 定义为：
- en: '|  | $\displaystyle exp(A):=I\ +\ A+\ \frac{1}{2}A^{2}+\ \dots.$ |  | (51)
    |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle exp(A):=I\ +\ A+\ \frac{1}{2}A^{2}+\ \dots.$ |  | (51)
    |'
- en: Since it is a subjection, for each hidden-to-hidden transformation matrix $W$
    belonging to the special orthogonal group or unitary group, there must exist a
    skew-symmetric (or skew-Hermitian matrix) $A$ that satisfies $exp(A)=W$. Therefore,
    the hidden-to-hidden transformation $h_{t+1}=\sigma(Wh_{t}\ +Tx_{t+1})$ is equivalent
    to $h_{t+1}=\sigma(exp(A)h_{t}\ +Tx_{t+1})$. That is, the optimization on the
    orthogonal or unitary manifold can be transformed to the optimization in Euclidean
    space. Consequently, classic gradient descent optimizers such as Adam can be applied
    to minimize the loss function as well as satisfying orthogonal constraints. As
    a result, the Lie exponential map can achieve both cheap computation overhead
    and the mitigation of gradient exploding and vanishing problems.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个主观问题，对于每一个属于特殊正交群或单位群的隐藏到隐藏的变换矩阵 $W$，必须存在一个满足 $exp(A)=W$ 的反对称（或反厄米矩阵）$A$。因此，隐藏到隐藏的变换
    $h_{t+1}=\sigma(Wh_{t}\ +Tx_{t+1})$ 等价于 $h_{t+1}=\sigma(exp(A)h_{t}\ +Tx_{t+1})$。也就是说，正交或单位流形上的优化可以转化为欧几里得空间中的优化。因此，经典的梯度下降优化器如
    Adam 可以应用于最小化损失函数，同时满足正交约束。因此，李代数指数映射可以实现低计算开销并缓解梯度爆炸和消失问题。
- en: Another method [[70](#bib.bib70)], which is based on the Lie group, defines
    a basis $\{T_{j}\}_{j=\{1,\cdots,n^{2}\}}$ and coefficients $\{\lambda_{j}\}_{j=\{1,\cdots,n^{2}\}}$
    to construct the element $L\in\mathfrak{u}(n)$ as follows,
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L=\Sigma_{j=1}^{n^{2}}\lambda_{j}T_{j}.$ |  | (52) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
- en: 'By using exponential mapping, the element $U$ of corresponding unitary Lie
    group $U(n)$ can be represented as:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $U=exp(L)=exp(\Sigma_{j=1}^{n^{2}}\lambda_{j}T_{j}).$ |  | (53) |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
- en: Furthermore, Hyland et al. [[70](#bib.bib70)] offered an argument that the above
    parameterization helps generalize unitary RNN to arbitrary unitary matrices and
    figure out long-memory tasks.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning orthogonal filters in deep neural networks (DNN) can be formulated
    as an optimization problem over multiple dependent Stiefel manifolds (OMDSM) [[71](#bib.bib71)].
    The orthogonal linear module can substitute standard linear module in DNNs to
    stabilize the distributions of activation and regularize networks. Let $W_{k}$
    and $b_{k}$ be learnable weight matrix and bias, parameter $\theta$ be $\{W_{k},b_{k}|k=1,2,\dots
    K\}$, the deep neural network can be represented as $f(x,\theta):x\rightarrow\hat{y}$,
    where $x$ is the input feature, and $\hat{y}$ is the output prediction of DNN.
    The loss function is often designed as the discrepancy between label $y$ and prediction
    values: $\mathcal{L}(y,f(x,\theta))$. Finally, the optimization problem is formulated
    as'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta^{\ast}=argmin_{\theta}\mathbb{E}_{(x,y)\in D}[\mathcal{L}(y,f(x,\theta))].$
    |  | (54) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
- en: OMDSM trains DNN with orthogonal weight matrix $W_{k}$ in each layer. Thus,
    the optimization problem is reformulated as
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta^{\ast}=argmin_{\theta}\mathbb{E}_{(x,y)\in D}[\mathcal{L}(y,f(x,\theta))]$
    |  | (55) |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle s.t.\ W_{k}\in\mathbb{O}_{k}^{n_{k}\times d_{k}},k=1,2,\dots
    K,$ |  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
- en: where the matrix family $\mathbb{O}_{k}^{n_{k}\times d_{k}}=\{W_{k}\in\mathbb{R}^{n_{k}\times
    d_{k}}|W_{k}W_{k}^{T}=I\}$ is composed of multiple real Stiefel manifolds, which
    is an embedded sub-manifold of $\mathbb{R}^{n_{k}\times d_{k}}$. Each independent
    orthogonal filter $W\in\mathbb{R}^{n\times d}$ is given by the proxy parameter
    $V\in\mathbb{R}^{n\times d}$ as
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W=PV,$ |  | (56) |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
- en: 'where $n$ is the number of output channels, $d$ is the number of input channels,
    and $P\in\mathbb{R}^{n\times n}$ is the coefficient of the linear transformation.
    Firstly, $V$ is centered by $V_{C}$:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V_{C}=V-c{1_{d}^{T}},$ |  | (57) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
- en: 'where $c=\frac{1}{d}V1_{d}$ and $1_{d}$ is the $d$-dimension vector with all
    ones. Moreover, the eigenvalues $\wedge$ and eigenvectors $D$ of the covariance
    matrix $V_{C}{V_{C}}^{T}$ are used to construct $P$:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P=D\wedge^{-1/2}D^{T}.$ |  | (58) |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
- en: Finally, $W$ is formulated as
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W=D\wedge^{-1/2}D^{T}V_{C}.$ |  | (59) |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
- en: Research has been conducted on exploring the influence of soft orthogonal constraints
    [[72](#bib.bib72)]. By allowing the diagonal elements of $S$ to float around 1,
    the orthogonal transformation matrix $W$ is relaxed as
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W=USV^{T},$ |  | (60) |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
- en: where $U$ and $V$ are strict orthogonal matrices.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: The above methods are mainly subject to $O(n^{3})$ time complexity or dependent
    on complex matrices [[73](#bib.bib73)]. It is discovered that orthogonal matrices
    $W\subseteq O(2n)$ with doubled hidden size, can substitute complex or unitary
    matrices in $\mathbb{C}^{n\times n}$. Inspired by the above discovery, [[73](#bib.bib73)]
    proposed to utilize Householder matrices to achieve parametrization of orthogonal
    transition matrices. As a result, complex matrices are unneeded and time complexity
    is reduced, while the effect is similar to the unitary constraint.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: The norm-keeping property of orthogonal matrices may make ORNN have difficulty
    paying little attention to extraneous information [[74](#bib.bib74)]. To relieve
    this problem, Jing et al. [[74](#bib.bib74)] put forward the gated orthogonal
    recurrent unit (GORU) to be unconcerned with irrelevant or noise information while
    learning long-term dependencies. By adding the gating mechanism, experiment results
    demonstrate that GORU outperforms the unitary RNN on natural language processing
    tasks such as question answering tasks, together with long-term dependency tasks
    such as denoising and copying tasks.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: In summary, uRNN [[14](#bib.bib14)] parameterizes the unitary hidden-to-hidden
    matrix by composing simple unitary matrices. However, the above parameterization
    cannot cover all $N\times N$ unitary matrices. To make up for that, full-capacity
    uRNN [[69](#bib.bib69)] is put forward. Unlike uRNN, expRNN [[13](#bib.bib13)]
    exploits the exponential map to achieve orthogonal constraints more easily. Furthermore,
    OMDSM innovatively uses re-parameterization to optimize DNN over multiple dependent
    Stiefel manifolds instead of one manifold [[71](#bib.bib71)]. Moreover, research
    has explored whether and how the hard orthogonal constraints on RNN can be relaxed
    [[72](#bib.bib72)]. By creatively introducing the householder matrix, the considerable
    time complexity of parameterizing unitary matrices can be mitigated [[73](#bib.bib73)].
    Last but not least, GORU [[74](#bib.bib74)] designs a forget gate, so that ORNN
    can pay little attention to extraneous information.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Geometric GNN
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GNN can be used to construct a learning network based on irregular graphs. Each
    graph is represented by vertexes and edges, which describes the relationship between
    vertexes. GNN encodes vertexes as feature vectors and models edges as a relationship
    matrix between vertexes. In GNN, graph convolution is performed between the relationship
    matrix and the feature matrix. Therefore, GNN can take advantage of the graph
    structure and update the feature information of each vertex iteratively. Endowing
    Eucludiean GNN with hyperbolic geometry can make it superior in capturing graph
    structure [[52](#bib.bib52)]. Recently, plenty of geometric GNN research has investigated
    how to incorporate GNN with hyperbolic manifold to benefit from a neighborhood
    with a highly organized structure.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: To make full use of the rich geometric information in the graph, geometry interaction
    learning (GIL) [[53](#bib.bib53)] incorporates Euclidean space with hyperbolic
    geometry by exponential and logarithmic transformations. Moreover, learnable message
    passing parameters are optimized on the $M\ddot{o}bius$ manifold. To allow each
    node to determinate the importance of each geometry space freely, the GIL framework
    employs a flexible dual-space to model both low-dimensional regular data and complex
    hierarchical structures. A broad spectrum of experiments show that the GIL method
    is adaptative to node classification and link prediction tasks.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Observing that GCN cannot cope with changes in static structure information,
    Liu et al. [[75](#bib.bib75)] put forward a manifold regularized dynamic graph
    convolutional network (MRDGCN), which integrated manifold regularization into
    GCN to model dynamic structure information. MRDGCN automatically updates the structure
    information before convengence, which makes up for GCN’s inability to remain optimal
    in pace with the learning process. Considerable comparative experiments on human
    activity datasets and citation network datasets evaluate that MRDGCN outperforms
    GCN and other semi-supervised learning methods.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Geometric Optimization for Other Deep Learning Methods
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Robust Time Series Prediction. Considering that noises and outliers are inevitable
    and important for system modeling, Feng et al. [[76](#bib.bib76)] put forward
    a robust manifold broad learning system (RM-BLS) for time series prediction with
    large-scale noisy disturbations. RM-BLS applies low-rank constraint so that features
    spoiled by perturbations can be abandoned by feature selection. Furthermore, RM-BLS
    can also abandon features that are not satisfied to low-dimensional manifold embedding.
    In addition to the low-rank manifold, [[76](#bib.bib76)] also considers Stiefel
    manifold optimization and satisfies orthogonal constraints by Cayley transformation
    and curvilinear search algorithm.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: Medical Reconstruction. Geometric optimization have played an essential role
    in the medical field, such as magnetic resonance imaging (MRI) for cardiac diagnosis.
    Dynamic MR can be optimized on a low-rank tensor manifold [[77](#bib.bib77)] to
    seize the powerful temporal connection between dynamic signals. Moreover, the
    iterative reconstruction process is flattened to a neural network for acceleration,
    called dubbed Manifold-Net. To recover free breathing and ungated cardiac MRI
    data, Biswas et al. [[78](#bib.bib78)] creatively combined CNN with smoothness
    regularization on manifolds (SToRM) prior. The Laplacian matrix $L$ in SToRM $tr(X^{T}LX)$
    is defined on the manifold to model similarities between data beyond the ambient
    space. To utilize the manifold structure and patient-specific information, data
    denoizing based on CNN and SToRM together with conjugate gradients (CG) step take
    place alternatively. Experiments confirm that combining CNN with SToRM leads to
    a fast and high quality reconstruction of MRI data even when the down sampling
    frequency is less than 8.2s of acquisition time per slice.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning. To maximize the utilization of finite computing resources,
    transfer learning aims to reuse the neural network, which is trained for task
    $A$, to address a similar task $B$. Knowledge distillation (KD) is intended to
    transfer model knowledge from a well-trained model (teacher) to a compact model
    (student) with soft labels. Zhang et al. [[79](#bib.bib79)] devised an end-to-end
    deep manifold-to-manifold transforming network (DMT-Net) for discriminative feature
    learning. However, reconstructing a more discriminative SPD manifold from the
    original one is challenging. DMT-Net designs a local SPD convolutional layer and
    the non-linear SPD activation layer to deal with it. Huang et al. [[80](#bib.bib80)]
    designed a manifold-to-manifold transformation matrix $W$ and constrained the
    optimization to reside on the SPD manifold. Moreover, the intra-class and inter-class
    dissimilarity graphs are built under $W$. Hence, they can represent local geometry
    structures and learn the discriminative feature of SPD data.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: Optimal Transport. Optimal transport aims to measure the distance between two
    probability distributions by using transport plan $\Gamma$ and cost matrix $C$
    , i.e.,
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\Gamma\in\Pi(\mu_{1},\mu_{2})}trace(\Gamma^{T}C),$ |  | (61) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
- en: where $\Pi(\mu_{1},\mu_{2})$ consists of joint distributions with marginals
    $\mu_{1}$ and $\mu_{2}$. Supposing $\mu_{1}$ has $m$ points and $\mu_{2}$ has
    $n$ points, the size of both $\Gamma$ and $C$ is $m\times n$. There are works
    that have explored the application of geometric optimization in optimal transport
    problems [[81](#bib.bib81)]. By using the Riemannian gradient descent (RGD) algorithm,
    [[82](#bib.bib82)] explored how to convert optimal transport problems with different
    regularizations to the optimization problem on the coupling matrix manifold (CMM).
    To clarify the geometry optimization process, [[82](#bib.bib82)] took classic
    optimal transport problems (e.g., the entropy-regularized [[83](#bib.bib83)] and
    power-regularized optimal transport problems [[84](#bib.bib84)]) as an example.
    Observing that the constrained set $\Pi(\mu_{1},\mu_{2})$ has a differentiable
    manifold structure, [[85](#bib.bib85)] and [[86](#bib.bib86)] solved the optimal
    transport problem on a generalized doubly stochastic manifold, broadening the
    application of manifold geometry in non-linear optimal transport problems. In
    addition to general problems, [[86](#bib.bib86)] discusses how to adapt the above
    geometric optimization framework to particular ones, such as problems with sparse
    optimal transport map and problems of how to learn multiple transport plans simultaneously.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Robots. Bayesian optimization is an important technology for robots since it
    is effective in solving optimization problems such as controller tuning, policy
    adaptation, and robot design. Bayesian optimization is based on the Gaussian Process
    that relies on domain knowledge exploration. Therefore, geometry-aware Bayesian
    optimization emerges as a promising paradigm that can incorporate domain geometry
    into the optimization algorithm. There are many commonly used kernels in Gaussian
    Process, among which Matérn kernel is used to study geometry-aware Gaussian process
    and Bayesian optimization. Euclidean Matérn kernel is defined as follows,
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $K(x,x^{\prime})=exp(-\frac{\&#124;x-x^{\prime}\&#124;^{2}}{2\sigma^{2}}),$
    |  | (62) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
- en: where $\sigma$ is a free parameter. Matérn kernel is a commonly used kernel
    function when constructing stationary Gaussian process. Borovitskiy et al. [[87](#bib.bib87)]
    pointed out that generalizing the Matérn kernel to the Riemannian manifold merely
    by replacing Euclidean norms $\|x-x^{\prime}\|^{2}$ with geodesic distances $d_{g}(x-x^{\prime})$
    could not produce a well-defined kernel function. To construct the Riemannian
    Matérn kernel defined by stochastic partial differential equations, Borovitskiy
    et al. proposed to obtain Laplace–Beltrami eigenpairs for the specific manifold
    and approximate the infinite sum, which forms the basis for geometry-aware Bayesian
    optimization on robotics. However, the above method suffers from two problems
    [[88](#bib.bib88)], i.e., i) the amount of computation increases exponentially
    with the manifold dimension; and ii) such method is inapplicable to non-compact
    manifolds. To address these problems, Jaquier et al. [[88](#bib.bib88)] observed
    a general expression of Matérn kernels, which is helpful to generalize them to
    the torus and sphere manifold. More importantly, Matérn kernels can be generalized
    to non-compact manifolds (e.g., SPD matrix manifold and the Hyperbolic space)
    by using the general expression.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Continual learning. Continual learning aims to remember and use the experience
    of previous tasks to learn new tasks, which raises requirements for the memory
    ability of neural networks. Chaudhry et al. [[89](#bib.bib89)] proposed to achieve
    the purpose of continual learning on the low-rank orthogonal manifold. The core
    idea of this method is to project the gradient into disjoint low-rank orthogonal
    subspace by introducing task-specific projection matrix in the last second layer,
    which can make the gradient between different tasks orthogonal and alleviate catastrophic
    forgetting. The concept of gradient orthogonality was first proposed in [[90](#bib.bib90)].
    The essential reason for catastrophic forgetting is that learning new tasks will
    affect the parameters learned on the old tasks. Updating parameters in the direction
    orthogonal to the gradient of the old tasks can not only learn new tasks but also
    keep the loss of the old tasks, which alleviates catastrophic forgetting. In the
    deep neural network, the chain derivation process can be approximately regarded
    as the linear transformation of the gradient, which will destroy the orthogonality
    of the gradient of the earlier layers and lead to catastrophic forgetting. To
    ensure the orthogonality of the gradient between different tasks, [[89](#bib.bib89)]
    constrains parameters on the Stiefel manifold, making this linear transformation
    an orthogonal transformation.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: 5 Toolbox
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The success of the Tensorflow platform and PyTorch framework in deep learning
    shows that toolboxes can conveniently help build neural networks. There are valuable
    toolboxes designed for quickly setting up manifolds optimization. Manopt [[91](#bib.bib91)],
    Pymanopt [[92](#bib.bib92)], McTorch [[93](#bib.bib93)], and Geomstats [[94](#bib.bib94)]
    are classic toolboxes that implement manifold geometries and optimization algorithms.
    Moreover, they are user-friendly and time-saving. Table [2](#S5.T2 "Table 2 ‣
    5 Toolbox ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold") compares these toolboxes from the aspect of applicable
    manifolds and geometry operations.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Manopt, which is built on Matlab, is a helpful tool to handle a variety of
    geometry constraints (e.g., different manifold structures introduced in  [2.3](#S2.SS3
    "2.3 Manifold Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")).
    A Riemannian optimization in Manopt [[91](#bib.bib91)] is designed as a problem
    including manifold structures that the search space is confined to. The cost function,
    or optimization object, is included in the above optimization problem as well.
    If needed, a problem structure can also cover derivatives of the objective function.
    In Manopt, solvers are functions that give a general implementation to Riemannian
    optimization algorithms, including steepest-descent, conjugate-gradient, and Riemannian
    trust-regions algorithms. Since solvers in Manopt is designed to minimize the
    cost function, the cost function should be multiplied by a negative one if it
    is a maximization problem.'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: Pymanopt [[92](#bib.bib92)] extends Manopt to python. Similar to the usage of
    Manopt in Matlab, a Riemannian optimization in Pymanopt should be initialized
    with a predefined manifold and cost function. Equipped with different solvers,
    the optimization process and result can be diverse. Pymanopt covers all sorts
    of smooth manifolds such as the oblique manifold, sphere manifold, and Graßmann
    manifold. Numerable optimization algorithms are included as solvers, for instance,
    trust-regions, conjugate-gradient, and steepest-descent algorithms are contained
    by Pymanopt.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Manopt and Pymanopt are limited to shallow learning optimizations and are not
    applicable to deep learning optimizations. To fill the deficiency of Manopt and
    Pymanopt, McTorch has been implemented by extending Pytorch [[93](#bib.bib93)],
    a handy framework for deep learning. As a result, it implements a general solution
    for deep learning optimizations on the manifold. Unlike Manopt and Pymanopt, Riemannian
    optimization in McTorch does not need to define problems, manifolds, and solvers.
    Similar to Pytorch, Riemannian optimization in McTorch only needs to define modules
    and optimizers such as Adam. Network modules inherited from $torch.nn.module$
    initialize layers with manifolds and forward functions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Geoopt [[95](#bib.bib95)], which is implemented on top of Pytorch, has a cheaper
    infrastructure cost than McTorch. Extended from $torch.nn.Module.parameters$,
    Geoopt supports tensors and parameters on the manifold. Moreover, Geoopt provides
    Riemannian optimizers, for instance, $RiemannianSGD$ and $RiemannianAdam$ are
    available and inherited from $torch.optim.SGD$ and $torch.optim.Adam$, respectively
    [[95](#bib.bib95)].
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Another toolbox, Geomstats, is composed of two core modules, i.e., geometry
    and learning [[94](#bib.bib94)]. The former implements Riemannian metrics, including
    geodesic distance. The latter implements statistics and learning algorithms inherited
    from Scikit-Learn classes such as *K-Means* and PCA. Compared with Geomstats,
    other toolboxes mentioned are less modular and lack statistical learning algorithms.
    Taking clustering, one of the classic statistical learning problems, as an example,
    Geomstats encapsulates the class *Online K-Means* with the parameter *metric*.
    To perform clustering operation, users only need to initialize the Riemannian
    metric and call *fit* function of class *Online K-Means* as they do in Scikit-Learn,
    which is easy and convenient.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
- en: TheanoGeometry [[96](#bib.bib96)] uses Theano, a python-based and research-oriented
    framework, to implement differential geometry and non-linear statistics problems.
    TheanoGeometry outperforms other manifold toolboxes since it can handle symbolic
    calculations. Thus, Theano code can be generated from symbolic expression directly,
    where non-linear symbolic statistics can be optimized with a trivial amount of
    code. TheanoGeometry goes further beyond efficient symbolic computation. It implements
    Riemannian geometry such as geodesic equations, parallel transport, and curvature
    with automatic differentiation features [[97](#bib.bib97)].
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Toolboxes Comparison in Terms of Manifolds and Geometry'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: '| Toolboxes | Manifolds | Geometry |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
- en: '| Manopt [[91](#bib.bib91)] | Euclidean manifold, symmetric matrices, sphere,
    complex circle, SO (n), Stiefel, Graßmannian, oblique manifold, SPD (n), fixed-rank
    PSD matrices | Exponential and logarithmic maps, tangent space projector, retraction,
    vector transport, egrad2rgrad, ehess2rhess, vector, metric, distance, norm |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
- en: '| Pymanopt [[92](#bib.bib92)] | Same as Manopt | Same as Manopt |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
- en: '| McTorch [[93](#bib.bib93)] | Stiefel, SPD (n) | Same as Manopt |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
- en: '| Geoopt [[95](#bib.bib95)] | Euclidean manifold, sphere, Stiefel, Poincaré
    ball | Same as Manopt |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
- en: '| Geomstats [[94](#bib.bib94)] | Euclidean manifold, Minkowski and hyperbolic
    space, sphere, SO (n), SE (n), GL (n), Stiefel, Graßmannian, SPD (n), discretized
    curves, Landmarks | Exponential and logarithmic maps, parallel transport, inner
    product, distance, norm, Levi-Civita conne- ction, geodesics, invariant metrics
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
- en: '| TheanoGeometry [[96](#bib.bib96)] | Sphere, ellipsoid, SPD (n), Landmarks,
    GL (n), SO (n), SE (n) | Inner product, exponential and logarithmic maps, parallel
    transport, Christoffel symbols, Riemann, Ricci and scalar curvature, geodesics,
    Fréchet mean |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
- en: 6 Performance Evaluation
  id: totrans-254
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold"),
     [5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold"), [6](#S6.T6 "Table
    6 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold"), [7](#S6.T7 "Table 7 ‣ 6 Performance
    Evaluation ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold"), [8](#S6.T8 "Table 8 ‣ 6 Performance Evaluation
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold") compare the performance of aforementioned geometric optimization
    methods on various visual tasks (e.g., character recognition, emotion recognition,
    act recognition, and scene recognition tasks). Each image dataset used in different
    visual tasks is summarized in Table [3](#S6.T3 "Table 3 ‣ 6 Performance Evaluation
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold").'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Datasets for Different Visual Tasks'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '| Vision Task | Dataset | Total Samples | Categories | Image Size |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
- en: '| Character Recognition | MNIST[[98](#bib.bib98)] | 70000 | 10 | 32 $\times$
    32 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
- en: '| Emotion Recognition | AFEW [[99](#bib.bib99)] | 1345 | 7 | 400 $\times$ 400
    |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
- en: '| NABU3DFE [[100](#bib.bib100)] | 2500 | 6 | NA |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
- en: '| Bosphorus dataset [[101](#bib.bib101)] | 4666 | 6 | NA |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
- en: '| Action Recognition | HDM05 [[98](#bib.bib98)] | 18000 | 130 | 93 $\times$93
    |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
- en: '| Face Verification | PaSC [[102](#bib.bib102)] | 12529 | NA | $401\times 401$
    |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
- en: '| Scene Recognition | Scene15 [[103](#bib.bib103)] | NA | 15 | $300\times 250$
    |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
- en: '| Eight sports event categories[[104](#bib.bib104)] | NA | 8 | NA |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
- en: '| SUN [[105](#bib.bib105), [106](#bib.bib106)] | 899 | NA | NA |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    shows that GORU [[74](#bib.bib74)] outperforms other ORNNs on the MNIST dataset.
    GORU adds a forget gate, which enables ORNN to filter out irrelevant information.
    Taking advantage of the surjective exponential map, expRNN [[13](#bib.bib13)]
    realizes orthogonal parameterization with a more straightforward way. Unlike expRNN,
    uRNN [[14](#bib.bib14)] uses simple unitary matrices to construct the unitary
    hidden-to-hidden matrix. However, such matrix construction method fails to represent
    all $N\times N$ unitary matrices. Therefore, Scott Wisdom et al. [[69](#bib.bib69)]
    proposed full-capacity uRNN to overcome that bottleneck of uRNN. Using regularization
    terms to realize orthogonal parameterization, soRNN [[72](#bib.bib72)] explores
    the effect of soft orthogonal constraints on RNN. ORNN [[73](#bib.bib73)] exploits
    the householder matrix to enforce an orthogonal constraint on RNN, which mitigates
    the considerable time complexity of unitary matrices. Table [4](#S6.T4 "Table
    4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold") shows that combining the forget
    gate, or noise filter, with ORNN improves the performance of ORNN.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison Results of Character Recognition'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Accuracy |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
- en: '| MNIST [[98](#bib.bib98)] | uRNN [[14](#bib.bib14)] | 97.6% |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
- en: '| full-capacity uRNN [[69](#bib.bib69)] | 96.9% |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
- en: '| expRNN [[13](#bib.bib13)] | 98.7% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
- en: '| soRNN [[72](#bib.bib72)] | 97.3% |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
- en: '| ORNN [[73](#bib.bib73)] | 97.2% |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
- en: '| GORU [[74](#bib.bib74)] | 98.9% |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Comparison Results of Emotion Recognition'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Accuracy |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
- en: '| AFEW [[99](#bib.bib99)] | STM-ExpLet [[107](#bib.bib107)] | 31.73% |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
- en: '| RSR-SPDML [[32](#bib.bib32)] | 30.12% |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
- en: '| DeepO2P [[108](#bib.bib108)] | 28.54% |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
- en: '| DCC [[109](#bib.bib109)] | 25.78% |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
- en: '| GDA [[56](#bib.bib56)] | 29.11% |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
- en: '| GGDA [[56](#bib.bib56)] | 29.45% |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
- en: '| PML [[110](#bib.bib110)] | 28.98% |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
- en: '| SPDNet [[50](#bib.bib50)] | 34.23% |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
- en: '| GrNet [[51](#bib.bib51)] | 34.23% |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
- en: '| BU-3DFE [[100](#bib.bib100)] | Tree-PNN [[111](#bib.bib111)] | 93.23% |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
- en: '| Berretti et al. [[112](#bib.bib112)] | 77.53% |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
- en: '| Huynh et al. [[113](#bib.bib113)] | 92.73% |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
- en: '| Azazi et al. [[114](#bib.bib114)] | 85.71% |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
- en: '| Hariri et al. [[59](#bib.bib59)] | 93.50% |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
- en: '|  | CSLBP [[115](#bib.bib115)] | 76.98 % |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
- en: '| CLBP [[116](#bib.bib116)] | 76.56% |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
- en: '| Bosphorus | ZernikeMoments [[117](#bib.bib117)] | 60.53% |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
- en: '|   [[101](#bib.bib101)] | Azazi et al. [[114](#bib.bib114)] | 84.10% |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
- en: '|  | Hariri et al. [[59](#bib.bib59)] | 90.01% |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
- en: 'Table [5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    shows that SPDNet [[50](#bib.bib50)] and GrNet [[51](#bib.bib51)] can achieve
    better classification results than state-of-the-art methods on AFEW dataset [[99](#bib.bib99)].
    The following methods for comparison are shallow learning methods applying manifold
    structure: Expressionlets on Spatio-Temporal Manifold (STM-ExpLet) [[107](#bib.bib107)],
    Riemannian Sparse Representation combining with Manifold Learning on the manifold
    of SPD matrices (RSR-SPDML) [[32](#bib.bib32)], Discriminative Canonical Correlations
    (DCC) [[109](#bib.bib109)], Graßmann Discriminant Analysis (GDA) [[56](#bib.bib56)],
    Grassmannian Graph-Embedding Discriminant Analysis (GGDA) [[118](#bib.bib118)],
    and Projection Metric Learning (PML) [[110](#bib.bib110)]. Deep Second-order Pooling
    (DeepO2P) [[108](#bib.bib108)] is a traditional CNN model using the standard optimization
    method. SPDNet exploits the Stiefel manifold parameterization by BiMap layers
    and introduces non-linearity into the network by ReEig layers. Experiments prove
    that using the manifold geometry in deep learning optimization can improve network
    performance. The LogEig layer is crucial to Riemannian computing and contributes
    to the emotion classification success of SPDNet. The success of GrNet shows that
    optimizing on the Graßmann manifold and building a geometry-aware deep learning
    network is significant for learning representative features and classifying emotions
    with a relatively high level of accuracy.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    presents that the manifold-based classification method proposed by Hariri et al.
    [[59](#bib.bib59)] achieves the highest precision on BU-3DFE and Bosphorus datasets.
    Hariri et al. used a Graph-Matching kernel and classified facial expression data
    with SPD covariance descriptors. It outperforms Tree-PNN [[111](#bib.bib111)]
    and XP Huynh [[113](#bib.bib113)] on the BU-3DFE dataset by a narrow margin, and
    the latter two methods use traditional CNN. The manifold-based method proposed
    by Hariri et al. greatly exceeds the methods proposed by Stefano Berretti [[112](#bib.bib112)]
    and Amal Azazi [[114](#bib.bib114)] by approximately 15% and 8% on BU-3DFE dataset.
    In particular, the latter two methods apply SIFT and Speed Up Robust Features
    descriptors. On the Bosphorus dataset, the classification accuracy of Hariri et
    al.’s method [[59](#bib.bib59)] is almost far higher than all state-of-the-art
    methods. For example, it is even 30% better than the ZernikeMoments [[117](#bib.bib117)].
    The low-accuracy methods use local features rather than SPD covariance matrices.
    Overall, these results indicate that using geometry constraints is vital for feature
    representation and emotion recognition.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [6](#S6.T6 "Table 6 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    shows that SPDNet achieves the highest accuracy on the action recognition task,
    followed by GrNet. As Table [7](#S6.T7 "Table 7 ‣ 6 Performance Evaluation ‣ A
    Survey of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold") shows, SPDNet and GrNet outperform state-of-the-art methods on the
    face recognition task. The eigenvalue decomposition in SPDNet introduces non-linearity
    and the QR decomposition in GrNet performs re-orthonormalization, both of which
    contribute to the classification accuracy. Therefore, using matrix decomposition
    is vital for exploring manifold constrained parameters. The success of the deep
    manifold network on the action recognition and face recognition task shows that
    optimizing deep learning on the manifold helps learn favorable features and classify
    human actions better.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Comparison Results of Action Recognition'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Accuracy |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
- en: '| HDM05 [[98](#bib.bib98)] | RSR-SPDML [[32](#bib.bib32)] | 48.01% |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
- en: '| DCC [[109](#bib.bib109)] | 41.74% |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
- en: '| GDA [[56](#bib.bib56)] | 46.25% |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
- en: '| GGDA [[118](#bib.bib118)] | 46.87% |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
- en: '| PML [[110](#bib.bib110)] | 47.25% |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
- en: '| SPDNet [[50](#bib.bib50)] | 61.45% |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
- en: '| GrNet [[51](#bib.bib51)] | 59.23% |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Comparison Results of Face Recognition'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Accuracy |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
- en: '| Method | PaSC1 [[102](#bib.bib102)] | PaSC2 [[102](#bib.bib102)] |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
- en: '| VGGDeepFace [[119](#bib.bib119)] | 78.82% | 68.24% |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
- en: '| DeepO2P [[108](#bib.bib108)] | 68.76% | 60.14% |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
- en: '| DCC [[109](#bib.bib109)] | 75.83% | 67.04% |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
- en: '| GDA [[56](#bib.bib56)] | 71.38% | 67.49% |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
- en: '| GGDA [[118](#bib.bib118)] | 66.71% | 68.41% |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
- en: '| PML [[110](#bib.bib110)] | 73.45% | 68.32% |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
- en: '| SPDNet [[50](#bib.bib50)] | 80.12% | 72.83% |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
- en: '| GrNet [[51](#bib.bib51)] | 80.52% | 72.76% |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
- en: 'As shown in Table [8](#S6.T8 "Table 8 ‣ 6 Performance Evaluation ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold"), Scene Recognition by Manifold Regularized Deep Learning Architecture
    (SRMR) [[60](#bib.bib60)] outperforms state-of-the-art non-manifold methods on
    all three scene recognition datasets. Lazebnik et al. [[120](#bib.bib120)] partitioned
    images into fine subregions for image matching. Dixit et al. [[121](#bib.bib121)]
    formulated Bayesian adaptation for scene image classification. Kwitt et al. [[122](#bib.bib122)]
    recognized scene images on the statistical (semantic) manifold. From the perspective
    of information geometry, they can consider the parameter vectors as Riemannian
    manifolds. Goh et al. [[123](#bib.bib123)] used SIFT descriptors and represented
    vectorially for image recognition. Li et al. [[104](#bib.bib104)] interpreted
    the semantic components of images. Wu and Rehg [[124](#bib.bib124)] used the Histogram
    Intersection Kernel (HIK) for sports game classification. Donahue et al. [[124](#bib.bib124)]
    used extracted features for novel generic tasks. SRMR’s incredible success on
    scene recognition tasks shows that manifold regularizations are significant for
    improving the classification accuracy of deep learning.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparison Results of Scene Recognition'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Accuracy |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
- en: '| Scene15 [[103](#bib.bib103)] | Lazebnik et al. [[120](#bib.bib120)] | 81.2%
    |'
  id: totrans-323
  prefs: []
  type: TYPE_TB
- en: '| Dixit et al. [[121](#bib.bib121)] | 82.3% |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
- en: '| Kwitt et al. [[122](#bib.bib122)] | 85.4% |'
  id: totrans-325
  prefs: []
  type: TYPE_TB
- en: '| Goh et al. [[123](#bib.bib123)] | 85.4% |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
- en: '| SRMR [[60](#bib.bib60)] | 86.9% |'
  id: totrans-327
  prefs: []
  type: TYPE_TB
- en: '| Eight sports event categories[[104](#bib.bib104)] | Li et al. [[104](#bib.bib104)]
    | 73.4% |'
  id: totrans-328
  prefs: []
  type: TYPE_TB
- en: '| Kwitt et al. [[122](#bib.bib122)] | 83.0% |'
  id: totrans-329
  prefs: []
  type: TYPE_TB
- en: '| Wu and Rehg [[124](#bib.bib124)] | 84.3% |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
- en: '| SRMR [[60](#bib.bib60)] | 86.1% |'
  id: totrans-331
  prefs: []
  type: TYPE_TB
- en: '|  | Xiao et al. [[105](#bib.bib105)] | 27.2% |'
  id: totrans-332
  prefs: []
  type: TYPE_TB
- en: '| SUN | Kwitt et al. [[122](#bib.bib122)] | 28.9% |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)] | Donahue et al. [[125](#bib.bib125)] | 30.14% |'
  id: totrans-334
  prefs: []
  type: TYPE_TB
- en: '|  | SRMR [[60](#bib.bib60)] | 30.3% |'
  id: totrans-335
  prefs: []
  type: TYPE_TB
- en: 'Experimental results vary with different network architecture settings for
    the same manifold constrained method. For example, SPDNet [[50](#bib.bib50)] has
    four different architecture configurations: i) SPDNet-0BiRe without using blocks
    of BiMap/ReEig, ii) SPDNet-1BiRe using $1$ block of BiMap/ReEig, iii) SPDNet-2BiRe
    using $2$ blocks of BiMap/ReEig, and iv) SPDNet-3BiRe using $3$ blocks of BiMap/ReEig.
    GrNet [[51](#bib.bib51)] has three different configurations: i) GrNet-0Block without
    using blocks of Projection-Pooling, ii) GrNet-1Block using $1$ block of Projection-Pooling,
    and iii) GrNet-2Block using $2$ blocks of Projection-Pooling. These methods studied
    how different architecture settings affected classification accuracy. Note that
    our article follows the raw settings reported from corresponding articles. On
    that account, this article did not present classification accuracy under different
    architecture configurations.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions and Future Work
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, a survey on recent advances in applying geometric optimization
    to deep learning is presented. This article reviewed progress of optimizing deep
    learning networks on manifolds according to the classification of deep learning
    backbones (e.g., CNN, RNN, and GNN). In particular, this article discussed the
    theory and toolboxes for geometric optimization. Although geometric optimization
    brings various advantages to deep learning methods, it still suffers from the
    following challenges.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset-Oriented Geometric Optimization. Various methods (e.g., uRNN [[14](#bib.bib14)]
    and Cheap Orthogonal Constraints in Neural Networks [[13](#bib.bib13)]) utilize
    small image datasets such as MNIST handwritten digits to validate the effectiveness
    of geometric optimization. Whether geometric optimization can achieve good performance
    on enormous and complicated datasets such as Penn Tree Bank (PTB) needs further
    research. This prompts researchers to use more challenging datasets to verify
    the performance of deep learning techniques after applying geometric optimization.
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model-Oriented Geometric Optimization. Although optimizing deep learning networks
    such as CNN and RNN on the Riemannian manifold has been proven successful, geometric
    optimization has not been applied to all deep learning methods. For example, there
    is a lack of research in optimizing reinforcement learning and federated learning
    on manifolds, which is crucial in automatic control and privacy protection. This
    forces researchers to further explore the potential and benefit of optimizing
    more deep learning networks from a geometric perspective.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manifold-Oriented Geometric Optimization. Manifold geometry plays an important
    role in geometric optimization and different manifolds have different applications.
    For instance, the orthogonal manifold can be used to alleviate feature redundancy
    and oblique manifold can be utilized for optimizing dictionary learning. However,
    applications of certain manifolds such as centered matrix manifold remain blank
    in the literature. This motivates researchers to exploit and use manifold structures
    for geometric optimization applications as much as possible.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This article demonstrated that geometric optimization can grasp advantage of
    the geometry information of search space, speed up the optimization process, and
    mitigate gradient explosion and vanishing problems. However, considering unexplored
    deep learning methods such as reinforcement learning, together with unused manifold
    structures such as centered matrix manifold, it is still a huge challenge to push
    the boundaries of geometric optimization in deep learning.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep
    learning. 2016.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Raphael JL Townshend, Stephan Eismann, Andrew M Watkins, Ramya Rangan,
    Maria Karelina, Rhiju Das, and Ron O Dror. Geometric deep learning of rna structure.
    Science, 373(6558):1047–1051, 2021.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional
    neural networks. In International Conference on Machine Learning (ICML), pages
    6105–6114, 2019.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, and David Barber.
    Wider and deeper, cheaper and faster: Tensorized lstms for sequence learning.
    In Advances in Neural Information Processing Systems, pages 1–11, 2017.'
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] David Rolnick and Max Tegmark. The power of deeper networks for expressing
    natural functions. In International Conference on Learning Representations (ICLR),
    2018.'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu. Orthogonal
    convolutional neural networks. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 11505–11515, 2020.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Storath and A. Weinmann. Variational regularization of inverse problems
    for manifold-valued data. Information and Inference: A Journal of the IMA, 10(1):195–230,
    2021.'
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms
    on matrix manifolds. 2008.'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Nicolas Boumal. An introduction to optimization on smooth manifolds. Available
    online, Aug 2020.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Jiang Hu, Xin Liu, Zai-Wen Wen, and Ya-Xiang Yuan. A brief introduction
    to manifold optimization. Journal of the Operations Research Society of China,
    8(2):199–248, 2020.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Xian Wei. Learning Image and Video Representations Based on Sparsity Priors.
    2017.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from
    orthogonality regularizations in training deep cnns. In Proceedings of the International
    Conference on Neural Information Processing Systems, pages 4266–4276, 2018.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Mario Lezcano-Casado and David Martınez-Rubio. Cheap orthogonal constraints
    in neural networks: A simple parametrization of the orthogonal and unitary group.
    In International Conference on Machine Learning (ICML), pages 3794–3803, 2019.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent
    neural networks. In International Conference on Machine Learning (ICML), pages
    1120–1128, 2016.'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Yanhong Fei, Yingjie Liu, Xian Wei, and Mingsong Chen. O-vit: Orthogonal
    vision transformer. arXiv preprint arXiv:2201.12133, 2022.'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Zhou, M. N. Do, and J Kovačević. Ieee transactions on image processing
    1 special paraunitary matrices, cayley transform, and multidimensional orthogonal
    filter banks. IEEE Transactions on Image Processing, 14(6):760, 2008.'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] P Rodríguez, J Gonzàlez, G. Cucurull, J. M. Gonfaus, and X. Roca. Regularizing
    cnns with locally constrained decorrelations. 2017.'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Jorge Nocedal and Stephen Wright. Numerical optimization. 2006.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] David G Luenberger, Yinyu Ye, et al. Linear and nonlinear programming,
    volume 2. 1984.'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Daniel Gabay. Minimizing a differentiable function over a differential
    manifold. Journal of Optimization Theory and Applications, 37(2):177–219, 1982.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Roger W Brockett. Differential geometry and the design of gradient algorithms.
    In Proc. Symp. Pure Math., AMS, pages 69–92, 1993.'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Alan Edelman, Tomás A Arias, and Steven T Smith. The geometry of algorithms
    with orthogonality constraints. SIAM journal on Matrix Analysis and Applications,
    20(2):303–353, 1998.'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Simon Hawe, Matthias Seibert, and Martin Kleinsteuber. Separable dictionary
    learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), pages 438–445, June 2013.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Xian Wei, Hao Shen, and Martin Kleinsteuber. Trace quotient meets sparsity:
    A method for learning low dimensional image representations. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5268–5277,
    2016.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] P-A Absil, Christopher G Baker, and Kyle A Gallivan. Trust-region methods
    on riemannian manifolds. Foundations of Computational Mathematics, 7(3):303–330,
    2007.'
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Jean-Pierre Dedieu, Pierre Priouret, and Gregorio Malajovich. Newton’s
    method on riemannian manifolds: covariant alpha theory. IMA Journal of Numerical
    Analysis, 23(3):395–419, 2003.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Xian Wei, Yuanxiang Li, Hao Shen, Fang Chen, Martin Kleinsteuber, and
    Zhongfeng Wang. Dynamical textures modeling via joint video dictionary learning.
    IEEE Transactions on Image Processing, 26(6):2929–2943, 2017.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Simon Alois Hawe. Learning sparse data models via geometric optimization
    with applications to image processing. PhD thesis, Technische Universität München,
    2013.'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Kumar, Z. Mhammedi, and M. Harandi. Geometry aware constrained optimization
    techniques for deep learning. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 4460–4469, 2018.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] John M Lee. Smooth manifolds. In Introduction to Smooth Manifolds, pages
    1–31\. Springer, 2013.'
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Diego Tosato, Michela Farenzena, Mauro Spera, Vittorio Murino, and Marco
    Cristani. Multi-class classification on riemannian manifolds for video surveillance.
    In ECCV, pages 378–391, 2010.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Mehrtash T Harandi, Mathieu Salzmann, and Richard Hartley. From manifold
    to manifold: Geometry-aware dimensionality reduction for spd matrices. In European
    Conference on Computer Vision (ECCV), pages 17–32, 2014.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Zhiwu Huang, Ruiping Wang, Shiguang Shan, Xianqiu Li, and Xilin Chen.
    Log-euclidean metric learning on symmetric positive definite manifold with application
    to image set classification. In Proceedings of International Conference on Machine
    Learning (ICMR), volume 37, pages 720–729, 2015.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Jieping Ye and Qi Li. Lda/qr: an efficient and effective dimension reduction
    algorithm and its theoretical foundation. Pattern Recognition, 37(4):851–854,
    2004.'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] John NR Jeffers. Two case studies in the application of principal component
    analysis. Journal of the Royal Statistical Society: Series C (Applied Statistics),
    16(3):225–236, 1967.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis.
    Chemometrics and Intelligent Laboratory Systems, 2(1-3):37–52, 1987.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Hui Zou and Lingzhou Xue. A selective overview of sparse principal component
    analysis. Proceedings of the IEEE, 106(8):1311–1320, 2018.'
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Frank Rehm, Frank Klawonn, and Rudolf Kruse. Mds polar: A new approach
    for dimension reduction to visualize high dimensional data. In International Symposium
    on Intelligent Data Analysis, pages 316–327, 2005.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Andreas Buja, Deborah F Swayne, Michael L Littman, Nathaniel Dean, Heike
    Hofmann, and Lisha Chen. Data visualization with multidimensional scaling. Journal
    of Computational and Graphical Statistics, 17(2):444–472, 2008.'
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Mingyu Fan, Hong Qiao, Bo Zhang, and Xiaoqin Zhang. Isometric multi-manifold
    learning for feature extraction. In International Conference on Data Mining, pages
    241–250. IEEE, 2012.'
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Yepeng Ni, Jianping Chai, Yan Wang, and Weidong Fang. A fast radio map
    construction method merging self-adaptive local linear embedding (lle) and graph-based
    label propagation in wlan fingerprint localization systems. Sensors, 20(3):767,
    2020.'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Bo Li, Yan-Rui Li, and Xiao-Long Zhang. A survey on laplacian eigenmaps
    based manifold learning methods. Neurocomputing, 335:336–351, 2019.'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Rong Wang, Feiping Nie, Richang Hong, Xiaojun Chang, Xiaojun Yang, and
    Weizhong Yu. Fast and orthogonal locality preserving projections for dimensionality
    reduction. IEEE Transactions on Image Processing, 26(10):5019–5030, 2017.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Effrosini Kokiopoulou, Jie Chen, and Yousef Saad. Trace optimization and
    eigenproblems in dimension reduction methods. Numerical Linear Algebra with Applications,
    18(3):565–602, 2011.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Albert Tarantola. Inverse problem theory and methods for model parameter
    estimation. 2005.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Simon Hawe, Martin Kleinsteuber, and Klaus Diepold. Analysis operator
    learning and its application to image reconstruction. IEEE Transactions on Image
    Processing, 22(6):2138–2150, 2013.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] W. P. Krijnen. Positive loadings and factor correlations from positive
    covariance matrices. Psychometrika, 69(4):655–660, 2004.'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Pavan Turaga and Rama Chellappa. Locally time-invariant models of human
    activities using trajectories on the grassmannian. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR), pages 2435–2441,
    2009.'
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Ozay and Takayuki Okatani. Optimization on submanifolds of convolution
    kernels in cnns. ArXiv, abs/1610.07008:arXiv–1610, 2016.'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Z. Huang and L. Gool. A riemannian network for spd matrix learning. In
    Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI), pages
    2036–2042, 2017.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Z. Huang, Jiqing Wu, and L. Gool. Building deep networks on grassmann
    manifolds. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
    pages 3279–3286, 2018.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks.
    Advances in Neural Information Processing Systems, 32:8230–8241, 2019.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Shichao Zhu, Shirui Pan, Chuan Zhou, Jia Wu, Yanan Cao, and Bin Wang.
    Graph geometry interaction learning. In Advances in Neural Information Processing
    Systems, volume 33, pages 7548–7558, 2020.'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Rahul Chauhan, Kamal Kumar Ghanshala, and RC Joshi. Convolutional neural
    network (cnn) for image detection and recognition. In International Conference
    on Secure Cyber Computing and Communication (ICSCCC), pages 278–282\. IEEE, 2018.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Aliasghar Mortazi and Ulas Bagci. Automatically designing cnn architectures
    for medical image segmentation. In International Workshop on Machine Learning
    in Medical Imaging, pages 98–106, 2018.'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Jihun Hamm and Daniel D Lee. Grassmann discriminant analysis: a unifying
    view on subspace-based learning. In Proceedings of the 25th International Conference
    on Machine Learning (ICML), pages 376–383, 2008.'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Lei Zhang, Xiantong Zhen, Ling Shao, and Jingkuan Song. Learning match
    kernels on grassmann manifolds for action recognition. IEEE Transactions on Image
    Processing, 28(1):205–215, 2019.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Mengyi Liu, Ruiping Wang, Shaoxin Li, Shiguang Shan, Zhiwu Huang, and
    Xilin Chen. Combining multiple kernel methods on riemannian manifold for emotion
    recognition in the wild. In Proceedings of International Conference on multimodal
    interaction, pages 494–501, 2014.'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Walid Hariri and Nadir Farah. Efficient graph-based kernel using covariance
    descriptors for 3d facial expression classification. In Proceedings of International
    Conference on Intelligent Systems and Pattern Recognition, pages 7–11, 2020.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Yuan Yuan, Lichao Mou, and Xiaoqiang Lu. Scene recognition by manifold
    regularized deep learning architecture. IEEE Transactions on Neural Networks and
    Learning Systems, 26(10):2222–2233, 2015.'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Chaoqun Hong, Jun Yu, Jian Zhang, Xiongnan Jin, and Kyong-Ho Lee. Multimodal
    face-pose estimation with multitask manifold deep learning. IEEE Transactions
    on Industrial Informatics, 15(7):3952–3961, 2018.'
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Jean-Michel Roufosse, Abhishek Sharma, and Maks Ovsjanikov. Unsupervised
    deep learning for structured shape matching. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV), pages 1617–1627, 2019.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. Deep learning
    on lie groups for skeleton-based action recognition. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR), pages 6099–6108,
    2017.'
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Xin Chen, Jian Weng, Wei Lu, Jiaming Xu, and Jiasi Weng. Deep manifold
    learning combined with convolutional neural networks for action recognition. IEEE
    Transactions on Neural Networks and Learning Systems, 29(9):3938–3952, 2017.'
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Razvan Pascanu, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. How
    to construct deep recurrent neural networks. ArXiv, abs/1312.6026:arXiv–1312,
    2014.'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier
    neural networks. In Proceedings of International Conference on Artificial Intelligence
    and Statistics, pages 315–323, 2011.'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted
    boltzmann machines. In Proceedings of the International Conference on International
    Conference on Machine Learning, page 807–814, 2010.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Sekitoshi Kanai, Yasuhiro Fujiwara, and Sotetsu Iwamura. Preventing gradient
    explosions in gated recurrent units. In Proceedings of the 31st International
    Conference on Neural Information Processing Systems, pages 435–444, 2017.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas.
    Full-capacity unitary recurrent neural networks. 10 2016.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Stephanie L Hyland and Gunnar Rätsch. Learning unitary operators with
    help from u(n). In Proceedings of the Thirty-First AAAI Conference on Artificial
    Intelligence, pages 2050–2058, 2017.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, Yongliang Wang, and Bo Li.
    Orthogonal weight normalization: Solution to optimization over multiple dependent
    stiefel manifolds in deep neural networks. In Proceedings of the AAAI Conference
    on Artificial Intelligence (AAAI), pages 3271–3278, 2018.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality
    and learning recurrent networks with long term dependencies. In International
    Conference on Machine Learning (ICML), pages 3570–3578, 2017.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey.
    Efficient orthogonal parametrisation of recurrent neural networks using householder
    reflections. In International Conference on Machine Learning (ICML), pages 2401–2409\.
    PMLR, 2017.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin
    Soljacic, and Yoshua Bengio. Gated orthogonal recurrent units: On learning to
    forget. Neural Computation, 31(4):765–783, 2019.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Weifeng Liu, Sichao Fu, Yicong Zhou, Zheng-Jun Zha, and Liqiang Nie. Human
    activity recognition by manifold regularization based dynamic graph convolutional
    networks. Neurocomputing, 444:217–225, 2021.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Shoubo Feng, Weijie Ren, Min Han, and Yen Wei Chen. Robust manifold broad
    learning system for large-scale noisy chaotic time series prediction: A perturbation
    perspective. Neural Networks, 117:179–190, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Ziwen Ke, Zhuoxu Cui, Wenqi Huang, Jing Cheng, Seng Jia, Haifeng Wang,
    Xin Liu, Hairong Zheng, Leslie Ying, Yanjie Zhu, and Dong Liang. Deep manifold
    learning for dynamic mr imaging. ArXiv, abs/2104.01102:arXiv–2104, 2021.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Sampurna Biswas, Hemant K Aggarwal, and Mathews Jacob. Dynamic mri using
    model-based deep learning and storm priors: Modl-storm. Magnetic Resonance in
    Medicine, 82(1):485–494, 2019.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Tong Zhang, Wenming Zheng, Zhen Cui, and Chaolong Li. Deep manifold-to-manifold
    transforming network. In 2018 25th IEEE International Conference on Image Processing
    (ICIP), pages 4098–4102, 2018.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Zhiwu Huang, Ruiping Wang, Xianqiu Li, Wenxian Liu, Shiguang Shan, Luc
    Van Gool, and Xilin Chen. Geometry-aware similarity learning on spd manifolds
    for visual recognition. IEEE Transactions on Circuits and Systems for Video Technology,
    28(10):2513–2523, 2018.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] L. Ambrosio, Italien, and Scuola Normale Superiore). A survey on monge’s
    optimal transport problem. 2009.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Dai Shi, Junbin Gao, Xia Hong, S. T. Boris Choy, and Zhiyong Wang. Coupling
    matrix manifolds assisted optimization for optimal transport problems. Machine
    Learning, 110(3):533–558, 2021.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] B. K. Abid and R. M. Gower. Greedy stochastic algorithms for entropy-regularized
    optimal transport problems. 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Arnaud Dessein, Nicolas Papadakis, and Jean Luc Rouas. Regularized optimal
    transport and the rot mover’s distance. Journal of Machine Learning Research,
    19, 2016.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] B. Mishra, Ntvs Dev, H. Kasai, and P. Jawanpuria. Manifold optimization
    for optimal transport. 2021.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Bamdev Mishra, N T V Satyadev, Hiroyuki Kasai, and Pratik Jawanpuria.
    Manifold optimization for non-linear optimal transport problems. 2021.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Deisenroth (he/him).
    Matérn gaussian processes on riemannian manifolds. In Advances in Neural Information
    Processing Systems, volume 33, pages 12426–12437, 2020.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Noémie Jaquier, Viacheslav Borovitskiy, Andrei Smolensky, Alexander Terenin,
    Tamim Asfour, and Leonel Dario Rozo. Geometry-aware bayesian optimization in robotics
    using riemannian matérn kernels. In CoRL, 2021.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, and Philip Torr. Continual
    learning in low-rank orthogonal subspaces. Advances in Neural Information Processing
    Systems, 33:9900–9911, 2020.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continual learning of
    context-dependent processing in neural networks. Nature Machine Intelligence,
    1(8):364–372, 2019.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Nicolas Boumal, Bamdev Mishra, Pierre Antoine Absil, and Rodolphe J Sepulchre.
    Manopt, a matlab toolbox for optimization on manifolds. The Journal of Machine
    Learning Research, 15(1):1455–1459, 2014.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Niklas Koep and Sebastian Weichwald. Pymanopt: A python toolbox for optimization
    on manifolds using automatic differentiation. Journal of Machine Learning Research,
    17:1–5, 2016.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Mayank Meghwanshi, Pratik Jawanpuria, Anoop Kunchukuttan, Hiroyuki Kasai,
    and Bamdev Mishra. Mctorch, a manifold optimization library for deep learning.
    Technical report, arXiv preprint arXiv:1810.01811, 2018.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Nina Miolane, Nicolas Guigui, Alice Le Brigant, Johan Mathe, Benjamin
    Hou, Yann Thanwerdas, Stefan Heyder, Olivier Peltre, Niklas Koep, Hadi Zaatiti,
    et al. Geomstats: A python package for riemannian geometry in machine learning.
    Journal of Machine Learning Research, 21(223):1–9, 2020.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Max Kochurov, Rasul Karimov, and Sergei Kozlukov. Geoopt: Riemannian optimization
    in pytorch. ArXiv, abs/2005.02819:arXiv–2005, 2020.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Kühnel and Stefan Sommer. Computational anatomy in theano. In Graphs in
    Biomedical Image Analysis, Computational Anatomy and Imaging Genetics, pages 4098–4102,
    2017.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Line Kühnel, Stefan Sommer, and Alexis Arnaudon. Differential geometry
    and stochastic dynamics with deep learning numerics. Applied Mathematics and Computation,
    356:411–437, 2019.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Müller, T. Röder, M. Clausen, B. Eberhardt, B. Krüger, and A. Weber.
    Documentation mocap database hdm05. Technical Report CG-2007-2, Universität Bonn,
    June 2007.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Abhinav Dhall, Roland Goecke, Jyoti Joshi, Karan Sikka, and Tom Gedeon.
    Emotion recognition in the wild challenge 2014: Baseline, data and protocol. In
    Proceedings of the 16th International Conference on Multimodal Interaction, pages
    461–466, 2014.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang, and M.J. Rosato. A 3d facial
    expression database for facial behavior research. In 7th International Conference
    on Automatic Face and Gesture Recognition (FGR06), pages 211–216, 2006.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Arman Savran, Neşe Alyüz, Hamdi Dibeklioğlu, Oya Çeliktutan, Berk Gökberk,
    Bülent Sankur, and Lale Akarun. Bosphorus database for 3d face analysis. In European
    Workshop on Biometrics and Identity Management, pages 47–56, 2008.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J Ross Beveridge, P Jonathon Phillips, David S Bolme, Bruce A Draper,
    Geof H Givens, Yui Man Lui, Mohammad Nayeem Teli, Hao Zhang, W Todd Scruggs, Kevin W
    Bowyer, et al. The challenge of face recognition from digital point-and-shoot
    cameras. In International Conference on Biometrics: Theory, Applications and Systems
    (BTAS), pages 1–8\. IEEE, 2013.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Li Fei-Fei and Pietro Perona. A bayesian hierarchical model for learning
    natural scene categories. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), volume 2, pages 524–531, 2005.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Li-Jia Li and Li Fei-Fei. What, where and who? classifying events by
    scene and object recognition. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 1–8\. IEEE, 2007.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio
    Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010
    IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages
    3485–3492, 2010.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude
    Oliva. Sun database: Exploring a large collection of scene categories. International
    Journal of Computer Vision, 119(1):3–22, 2016.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Mengyi Liu, Shiguang Shan, Ruiping Wang, and Xilin Chen. Learning expressionlets
    on spatio-temporal manifold for dynamic facial expression recognition. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
    1749–1756, 2014.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix backpropagation
    for deep networks with structured layers. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 2965–2973, 2015.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Tae-Kyun Kim, Josef Kittler, and Roberto Cipolla. Discriminative learning
    and recognition of image set classes using canonical correlations. IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 29(6):1005–1018, 2007.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Zhiwu Huang, Ruiping Wang, Shiguang Shan, and Xilin Chen. Projection
    metric learning on grassmann manifold with application to video based face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pages 140–149, 2015.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Hamit Soyel and Hasan Demirel. Optimal feature selection for 3d facial
    expression recognition using coarse-to-fine classification. Turkish Journal of
    Electrical Engineering and Computer Sciences, 18(6):1031–1040, 2010.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Stefano Berretti, Alberto Del Bimbo, Pietro Pala, Boulbaba Ben Amor,
    and Mohamed Daoudi. A set of selected sift features for 3d facial expression recognition.
    In International Conference on Pattern Recognition, pages 4125–4128\. IEEE, 2010.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Xuan-Phung Huynh, Tien-Duc Tran, and Yong-Guk Kim. Convolutional neural
    network models for facial expression recognition using bu-3dfe database. In Information
    Science and Applications (ICISA) 2016, pages 441–450\. 2016.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Amal Azazi, Syaheerah Lebai Lutfi, Ibrahim Venkat, and Fernando Fernández-Martínez.
    Towards a robust affect recognition: Automatic facial expression recognition in
    3d faces. Expert Systems with Applications, 42(6):3056–3066, 2015.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Soon-Yong Chun, Chan-Su Lee, and Sang-Heon Lee. Facial expression recognition
    using extended local binary patterns of 3d curvature. In Multimedia and Ubiquitous
    Engineering, pages 1005–1012. 2013.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Yiding Wang, Meng Meng, and Qingkai Zhen. Learning encoded facial curvature
    information for 3d facial emotion recognition. In 2013 7th International Conference
    on Image and Graphics, pages 529–532, 2013.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Nicholas Vretos, Nikos Nikolaidis, and Ioannis Pitas. 3d facial expression
    recognition using zernike moments on depth images. In 2011 18th IEEE International
    Conference on Image Processing, pages 773–776, 2011.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Jihun Hamm and Daniel D Lee. Extended grassmann kernels for subspace-based
    learning. In Advances in Neural Information Processing Systems (NIPS), pages 601–608,
    2009.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition.
    2015.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond bags of features:
    Spatial pyramid matching for recognizing natural scene categories. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2,
    pages 2169–2178\. IEEE, 2006.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Mandar Dixit, Nikhil Rasiwasia, and Nuno Vasconcelos. Adapted gaussian
    models for image classification. In CVPR, pages 937–943\. IEEE, 2011.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Roland Kwitt, Nuno Vasconcelos, and Nikhil Rasiwasia. Scene recognition
    on the semantic manifold. In European Conference on Computer Vision (ECCV), pages
    359–372, 2012.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Hanlin Goh, Nicolas Thome, Matthieu Cord, and Joo-Hwee Lim. Learning
    deep hierarchical visual feature coding. IEEE Transactions on Neural Networks
    and Learning Systems, 25(12):2212–2225, 2014.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Jianxin Wu and James M Rehg. Beyond the euclidean distance: Creating
    effective visual codebooks using the histogram intersection kernel. In IEEE International
    Conference on Computer Vision, pages 630–637, 2009.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang,
    Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature
    for generic visual recognition. In International Conference on Machine Learning
    (ICML), pages 647–655, 2014.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
