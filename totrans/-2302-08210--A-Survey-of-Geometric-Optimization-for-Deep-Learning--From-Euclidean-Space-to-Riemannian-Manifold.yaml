- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:41:53'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2302.08210] A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.08210](https://ar5iv.labs.arxiv.org/html/2302.08210)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'A Survey of Geometric Optimization for Deep Learning: From Euclidean Space
    to Riemannian Manifold'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yanhong Fei, Xian Wei, Yingjie Liu, Zhengyu Li, Mingsong Chen
  prefs: []
  type: TYPE_NORMAL
- en: Software Engineering Institute, East China Normal University
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Although Deep Learning (DL) has achieved success in complex Artificial Intelligence
    (AI) tasks, it suffers from various notorious problems (e.g., feature redundancy,
    and vanishing or exploding gradients), since updating parameters in Euclidean
    space cannot fully exploit the geometric structure of the solution space. As a
    promising alternative solution, Riemannian-based DL uses geometric optimization
    to update parameters on Riemannian manifolds and can leverage the underlying geometric
    information. Accordingly, this article presents a comprehensive survey of applying
    geometric optimization in DL. At first, this article introduces the basic procedure
    of the geometric optimization, including various geometric optimizers and some
    concepts of Riemannian manifold. Subsequently, this article investigates the application
    of geometric optimization in different DL networks in various AI tasks, e.g.,
    convolution neural network, recurrent neural network, transfer learning, and optimal
    transport. Additionally, typical public toolboxes that implement optimization
    on manifold are also discussed. Finally, this article makes a performance comparison
    between different deep geometric optimization methods under image recognition
    scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With increasing computing power, deep neural networks optimized in Euclidean
    space have achieved remarkable success from computer vision to natural language
    processing (e.g., autonomous driving and protein structure prediction) [[1](#bib.bib1),
    [2](#bib.bib2)]. However, to fully exploit the valuable information hidden in
    the data, most deep learning models tend to increase the capacity of their networks,
    either by widening the existing layers or by adding more layers [[3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5)]. For example, models often contain hundreds of
    convolution and pooling layers with various activation functions and multiple
    fully connected layers, producing millions or billions of parameters during training.
    These massive parameters associated with complex model architectures challenge
    the optimization of deep learning networks. As an alternative paradigm, optimization
    on the Riemannian manifold exploits hidden valuable information by utilizing geometric
    properties of parameters, rather than increasing the network capacity. Therefore,
    geometric optimization can alleviate over-parameterization and feature redundancy
    problems. For example, deep learning models trained on the orthogonal manifold
    have less correlated parameters, making features much less redundant [[6](#bib.bib6)].
  prefs: []
  type: TYPE_NORMAL
- en: The optimization objective in most deep learning methods can be formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in\mathcal{D}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}),\ s.t.\ C(\boldsymbol{\theta}),$ |  | (1)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{D}$ denotes the predefined admissible search space, $f$ denotes
    a real-value optimization function (e.g., loss function) to be minimized by trainable
    parameters $\boldsymbol{\theta}$, and $C(\boldsymbol{\theta})$ represents constraints
    (e.g., orthogonality [[6](#bib.bib6)] and unit row sums [[7](#bib.bib7)]) that
    $\boldsymbol{\theta}$ is subject to. Most deep learning methods define the search
    space $\mathcal{D}$ as the Euclidean space. However, parameters satisfying constraints
    are on the manifold, which is a low dimensional subspace and only occupies a small
    part of Euclidean space. Therefore, to eliminate constraints and reduce parameters,
    geometric optimization [[8](#bib.bib8), [9](#bib.bib9), [10](#bib.bib10), [11](#bib.bib11)]
    narrows the search space from Euclidean space to a smooth manifold. Hence, Equation ([1](#S1.E1
    "In 1 Introduction ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) is transformed into a differentiable
    optimization function $f:\mathcal{M}\rightarrow\mathcal{R}$ on a Riemannian manifold,
    i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname*{argmin}_{\theta\in\mathcal{M},M=\{\theta&#124;C(\theta)\}}\
    f_{\boldsymbol{\theta}}(\mathbf{x}).$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: 'As shown in Equation ([2](#S1.E2 "In 1 Introduction ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")),
    selecting a manifold composed of points that meet constraints $C(\boldsymbol{\theta})$
    in Equation ([1](#S1.E1 "In 1 Introduction ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")), a large class
    of constrained deep learning problems in Euclidean space can be optimized as unconstrained
    and convex ones on the Riemannian manifold [[10](#bib.bib10)], which helps ensure
    the convergence. For example, a typical dimension reduction problem can be defined
    as follows'
  prefs: []
  type: TYPE_NORMAL
- en: '|  |  | $\displaystyle\operatorname*{argmin}_{\boldsymbol{\theta}\in{E}}f_{\boldsymbol{\theta}}(\mathbf{x})=-tr(\boldsymbol{\theta}^{T}x^{T}x\boldsymbol{\theta}),\
    s.t.\ \ \boldsymbol{\theta}^{T}\boldsymbol{\theta}=I,$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where $E$ represents the Euclidean space, $I$ represents the identity matrix
    and parameters $\boldsymbol{\theta}$ are constrained to be orthogonal. Since all
    matrices that satisfy orthogonality compose of the Stiefel manifold , Equation ([3](#S1.E3
    "In 1 Introduction ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) can be treated as an unconstrained problem
    on the Stiefel manifold, which is a kind of Riemannian manifold.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/80fcad216bea951de0934902f9a7c3c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Comparison between geometric and Euclidean optimization path. The
    blue center point is the global optimum. The red curve describes the Riemannian
    optimization path converging upon the global optimal goal, always along a curve
    on manifolds. In contrast, the green dotted line indicates the Euclidean gradient
    descent path towards the optimal goal, taking the risk of moving off the manifold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") depicts the intuitive
    paradigm for optimization processes in arbitrary Euclidean space and on Riemannian
    manifolds. Traditional optimization methods in Euclidean space may ignore the
    advantages of applying geometric optimization strategies. For example, the latter
    can obtain richer geometric information from different unique manifold structures
    and convert constrained optimization problems into unconstrained problems. Moreover,
    geometric optimization can achieve faster convergence speed and mitigate gradient
    explosion and disappearance problems in deep learning, which will be detailed
    in Section [4](#S4 "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold"). Due to the above
    potential, geometric optimization has been applied to various deep neural networks
    in recent years, such as convolution neural network (CNN) [[12](#bib.bib12), [6](#bib.bib6),
    [13](#bib.bib13)], recurrent neural network (RNN) [[14](#bib.bib14)] and vision
    transformer (ViT) [[15](#bib.bib15)]. For instance, orthogonal parameterization
    is used in CNN to reduce filter similarities, make spectra uniform [[16](#bib.bib16)],
    and stabilize the activation distribution in different network layers [[17](#bib.bib17)].
    However, there is a lack of comprehensive surveys focused on deep learning methods
    applying geometric optimization. To explore benefits of geometric optimization,
    this article aims to give an overall review of recent advances on applying geometric
    optimization in deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e1f28b6c67d65b8163e033c412022ca7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: An overview of the central idea of this article.'
  prefs: []
  type: TYPE_NORMAL
- en: ¹¹1*Notations* In this work, vectors and matrices are denoted by bold lower
    case letters and upper case ones, respectively. Let $\mathbb{R}$ be the set of
    real numbers, $\mathbb{C}$ be the set of complex numbers and $\nabla f$ denotes
    the Euclidean gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Overview and article organization. In this article, a survey of geometric optimization
    techniques for deep learning is presented, including the theory and applications
    of geometric optimization. Figure [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold") displays an overview of the central idea of this article. Since the
    optimization theory is unified and model-independent, this article illustrates
    the theory first, including various geometric gradient descent optimizers (Section [2](#S2
    "2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")). The motivation and technique
    of applying geometric optimization in classical machine learning is different
    from that of deep learning. Therefore, this article reviews how to apply geometric
    optimization to shallow learning (Section [3](#S3 "3 Applications in Classical
    Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) and deep learning (Section [4](#S4 "4
    Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) separately. In particular, this
    article investigates representative manifold optimization toolboxes (Section [5](#S5
    "5 Toolbox ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")), followed by performance comparisons of different
    geometric deep learning methods on image recognition tasks (Section [6](#S6 "6
    Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")). Finally, we conclude the article
    and highlight future challenges and research trend (Section [7](#S7 "7 Conclusions
    and Future Work ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")).'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Geometric Optimization Theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The essence of an optimization problem is to find the maximum or minimum value
    of a cost function. An unconstrained optimization problem can use conventional
    optimization methods (e.g., steepest descent method, conjugate gradient method,
    and Newton method) to find an optimal solution [[18](#bib.bib18)]. However, a
    broad range of optimization problems that occur in computer vision tasks are known
    as constrained optimization problems. In such a case, finding a closed form for
    the cost function is difficult. To use the aforementioned conventional optimization
    techniques, the constrained problem can be transformed into an unconstrained form
    by using the method of Lagrange multipliers or using a barrier penalty function
    [[18](#bib.bib18)]. However, the above methods hardly take advantage of underlying
    manifold structures. They merely treat the constrained problem as a “black box”
    and solve them by using algebraic manipulation.
  prefs: []
  type: TYPE_NORMAL
- en: As an alternative solution, geometric optimization methods are developed to
    exploit intrinsic geometric structures of objective function parameters. By utilizing
    the underlying geometry of a cost function, geometric optimization methods can
    narrow the search space of constrained optimization problems from Euclidean space
    to smooth Riemannian manifolds. Riemannian manifold has a differentiable structure
    and is equipped with smooth inner product and Riemannian gradients, which are
    different from Euclidean space and lay the foundation for geometric optimization.
    Based on the Riemannian inner product and Riemannian gradients, a broad spectrum
    of conventional optimization techniques in Euclidean space can have their counterparts
    on smooth manifolds [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [8](#bib.bib8)],
    including the steepest descent method [[19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21)],
    conjugate gradient descent method [[22](#bib.bib22), [23](#bib.bib23), [24](#bib.bib24)],
    trust-region method [[8](#bib.bib8), [25](#bib.bib25)] and Newton’s method [[26](#bib.bib26),
    [8](#bib.bib8)]. Therefore, geometric optimization methods can use Riemannian
    optimizers to find an optimal solution for objective functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following subsections, this article first illustrates the model-independent
    optimization process on the Riemannian manifold, covering basic concepts related
    to geometric optimization (Section [2.1](#S2.SS1 "2.1 Geometric Optimization Process
    on Manifolds ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")). Next, this
    article briefly introduces various Riemannian gradient descent optimizers implementing
    geometric optimization, which is a counterpart of optimizers in Euclidean space
    (Section [2.2](#S2.SS2 "2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")). Finally, this article presents a series of manifold
    structures that are commonly used in deep geometric learning methods (Section [2.3](#S2.SS3
    "2.3 Manifold Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")).'
  prefs: []
  type: TYPE_NORMAL
- en: '<svg  class="ltx_picture ltx_centering" height="181.84" overflow="visible"
    version="1.1" width="300.69"><g transform="translate(0,181.84) matrix(1 0 0 -1
    0 0) translate(112.31,0) translate(0,50.3)" fill="#000000" stroke="#000000"><g
    stroke-width="0.4pt"><g transform="matrix(1.0 0.0 0.0 1.0 18.46 -35.73)" fill="#000000"
    stroke="#000000"><foreignobject width="12.68" height="9.46" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">$\mathcal{M}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -20.13 8.7)" fill="#000000" stroke="#000000"><foreignobject width="20.42"
    height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j)}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -4.72 -9.19)" fill="#000000" stroke="#000000"><foreignobject
    width="34.24" height="12.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$T_{\bm{\Theta}^{(j)}}\mathcal{M}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -13.45 58.23)" fill="#000000" stroke="#000000"><foreignobject
    width="8.3" height="7.56" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\mathbf{H}$</foreignobject></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -2.82 119.18)" fill="#000000" stroke="#000000"><foreignobject
    width="104.84" height="15.05" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Update:
    $\bm{\Theta}^{(j)}+\gamma\mathbf{H}$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 65.85 98.24)" fill="#000000" stroke="#000000"><foreignobject width="122.53"
    height="13.86" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Retraction:
    $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g><g transform="matrix(1.0
    0.0 0.0 1.0 -78.01 -37.67)" fill="#000000" stroke="#000000"><foreignobject width="56.8"
    height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\operatorname{grad}{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 -96.65 60.31)" fill="#000000" stroke="#000000"><foreignobject
    width="44.47" height="12.66" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\nabla{f}(\bm{\Theta}^{(j)})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 44.48 41.89)" fill="#000000" stroke="#000000"><foreignobject
    width="42.49" height="11.08" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$</foreignobject></g>
    <g transform="matrix(1.0 0.0 0.0 1.0 115.06 31.02)" fill="#000000" stroke="#000000"><foreignobject
    width="30.32" height="12.36" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">$\bm{\Theta}^{(j+1)}$</foreignobject></g></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 3: The update process in geometric gradient descent algorithm. It shows
    an update from the point $\bm{\Theta}^{(j)}$ to the point $\bm{\Theta}^{(j+1)}$
    in a search direction $\mathbf{H}\in T_{\bm{\Theta}^{(j)}}\mathcal{M}$ along the
    geodesic curve $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$. Moreover, it describes
    how to approximate the geodesic $\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$
    by using the retraction $\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})$.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Geometric Optimization Process on Manifolds
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2 Geometric Optimization Theory ‣ A Survey of
    Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    depicts the update process in geometric optimization[[27](#bib.bib27)] through
    the gradient descent example. There are two nearby points $\Theta^{(j)}$ and $\Theta^{(j+1)}$
    on a manifold $\mathcal{M}$ together with the tangent space at $\Theta^{(j)}$
    (refer to the green area in Figure [3](#S2.F3 "Figure 3 ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")). Each point $\Theta$ on the manifold has its corresponding
    tangent space $T_{\Theta}\mathcal{M}$, which is a generalization of the tangent
    plane in Euclidean space and consists of all tangent vectors passing through $\Theta$
    [[28](#bib.bib28)]. Each tangent space has an inner product, which is vital for
    vector metrics such as length and angles. Inner product space further helps induce
    the concept of orthogonality, an extension of vertical in higher dimensions. A
    Riemannian gradient $grad\,f(\Theta)$ for geometric optimization is a tangent
    vector on the tangent space $T_{\Theta}\mathcal{M}$ and points to the direction
    where the cost function on the manifold ascends steepest [[28](#bib.bib28)]. Figure [3](#S2.F3
    "Figure 3 ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") shows that gradient
    $\nabla f(\Theta)$ is computed in the ambient Euclidean space. Since the manifold
    is locally homomorphic to the Euclidean space, $grad\,f(\Theta)$ can be achieved
    by projecting Euclidean gradient $\nabla f(\Theta)$ to the appropriate tangent
    space $T_{\Theta}\mathcal{M}$, i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $grad\ f(\Theta)=\Pi_{T_{\Theta}\mathcal{M}}(\nabla f(\Theta)),$ |  |
    (4) |'
  prefs: []
  type: TYPE_TB
- en: where $\Pi$ means the orthogonal projection.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a counterpart of Euclidean straight lines, a geodesic is a locally shortest
    path between two points on the manifold. Therefore, reaching the optimal goal
    along a correct geodesic is shortest. Formally, a geodesic $\Gamma_{\bm{\Theta}}(\gamma\mathbf{H})$
    is a smooth curve on the manifold, proceeding from $\Theta$ in the direction of
    tangent vector $\mathbf{H}\in T_{\Theta}\textit{M}$ with a step size of $\gamma\in\mathbb{R^{+}}$
    [[23](#bib.bib23)]. Since each tangent vector is the direction vector of a specific
    geodesic curve, it can uniquely determine a geodesic curve. In particular, the
    geodesic defined by the negative Riemannian gradient reveals the next point in
    the optimization direction. A point can be mapped from the tangent space to the
    manifold through exponential mapping. In practice, to alleviate the high computational
    cost of exponential mapping, retraction operation $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H})$
    is often used as an approximation [[29](#bib.bib29)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathfrak{R}_{\bm{\Theta}}(\gamma\mathbf{H}):T_{p}\mathcal{M}\rightarrow\mathcal{M},\
    \gamma\mathbf{H}\rightarrow\Gamma_{\bm{\Theta}}(\gamma\mathbf{H}),$ |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{H}$ denotes an opposite vector of the Riemannian gradient. Therefore,
    $\mathbf{H}$ points in the direction of the steepest descent of the optimization
    function. As a result, the optimization function will be minimized if parameter
    $\Theta$ is updated along a geodesic curve in the direction of $\mathbf{H}$. In
    summary, with a step size of $\gamma$, the optimizing process from the current
    parameter $\Theta^{(j)}$ to the next parameter $\Theta^{(j+1)}$ can be formulated
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Theta^{(j+1)}=\Gamma_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})\approx\mathfrak{R}_{\bm{\Theta}^{(j)}}(\gamma\mathbf{H})=\mathfrak{R}_{\bm{\Theta}^{(j)}}(-\gamma
    grad\,f(\Theta^{(j)})).$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: 2.2 Gradient Descent Optimizers
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Optimization problems defined in Euclidean space can be abstracted as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\min}\{f_{\boldsymbol{\theta}}(\mathbf{x}):\boldsymbol{\theta}\in\mathbb{E}\},$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\boldsymbol{\theta}$ are trainable parameters and $E$ means the Euclidean
    space. There are a variety of standard optimizers for Equation ([7](#S2.E7 "In
    2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey of
    Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")).
    The gradient descent method is a most basic optimization strategy. It can be improved
    by stochastic gradient descent (SGD), which can accelerate convergence. The other
    two typical variants of the gradient descent method are stochastic gradient descent-momentum
    (SGD-M) and root mean square prop (RMSProp). To solve valley oscillation and saddle
    point stagnation problems that SGD suffers from, SGD-M is developed to maintain
    the inertia of the previous step. According to empirical judgments of different
    parameters, RMSProp can adaptively determine the learning rate of parameters,
    i.e., parameters with low update frequency can have a larger learning rate, while
    parameters with high update frequency can reduce the step size. Let $\boldsymbol{\theta}^{(k)}$
    represent parameters at iteration $k$ and $\boldsymbol{\theta}^{(k+1)}$ represent
    parameters at iteration $k+1$, this section first explains the above Euclidean
    gradient descent optimizers and then shows how to generalize them to the Riemannian
    manifold for geometric optimization ²²2For simplicity, this paper uses ${\nabla}f$
    to denote $\frac{\partial f_{\boldsymbol{\theta}}(x)}{\partial\boldsymbol{\theta}}$
    in Section [2.2](#S2.SS2 "2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")..'
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent. The gradient descent method takes the following form
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-{\lambda}{\nabla}f(\boldsymbol{\theta}^{(k)}),$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda$ is a hyper-parameter representing the step size. The negative
    direction of the gradient ${\nabla}f(\boldsymbol{\theta}^{(k)})$ has a vital property,
    i.e., it is a descent direction of the optimization problem. Therefore, the optimization
    process is to iteratively update trainable parameters along the negative direction
    of gradient until convergence.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent (SGD). The main idea behind SGD is to use random
    mini-batches of training data to update parameters of the optimization problem,
    which inherently reduces the calculation workload. Although the parameters may
    not be updated in the direction of the steepest descent every time, the overall
    update is in the steepest descent direction through multiple rounds of updates.
    As a result, SGD can greatly speed up the optimization process.
  prefs: []
  type: TYPE_NORMAL
- en: Stochastic Gradient Descent-Momentum (SGD-M). Inspired by the concept of momentum
    in physics, SGD-M exerts the influence of the last update on the current update
    to damp oscillation and accelerate convergence. Let $m^{(k)}$ denote the update
    imposed on $\boldsymbol{\theta}^{(k-1)}$ and $\nabla f$ denote the gradient at
    time $k$, the update $m^{(k+1)}$ to be imposed on $\boldsymbol{\theta}^{(k)}$
    can be achieved as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $m^{(k+1)}=\lambda_{0}\ m^{(k)}+\lambda_{1}\nabla f,$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda_{0}$ and $\lambda_{1}$ are hyper-parameters. Sequentially, the
    parameter $\boldsymbol{\theta}^{(k)}$ is updated to $\boldsymbol{\theta}^{(k+1)}$
    by $m^{(k+1)}$ as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-m^{(k+1)}.$ |  |
    (10) |'
  prefs: []
  type: TYPE_TB
- en: Root Mean Square Prop (RMSProp). Similar to SGD-M, RMSProp considers the influence
    of the last update when calculating the upcoming update. Let $m^{(k)}$ be the
    update on the previous occasion and $\nabla f$ be the current gradient, RMSProp
    designs upcoming update $m^{(k+1)}$ as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $m^{(k+1)}=\lambda\ m^{(k)}+(1-\lambda)(\nabla f\odot\nabla f),$ |  |
    (11) |'
  prefs: []
  type: TYPE_TB
- en: where $\lambda$ is a hyper-parameter and $\odot$ denotes the $Hadamard$ product
    [[29](#bib.bib29)] which is element-wise. RMSProp updates $\boldsymbol{\theta}^{(k)}$
    to $\boldsymbol{\theta}^{(k+1)}$ in the following way, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\boldsymbol{\theta}^{(k)}-\eta\frac{\nabla
    f}{\sqrt{m^{(k+1)}+\epsilon}},$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where $\eta$ is a hyper-parameter and $\epsilon$ is positive to prevent the
    denominator from being zero. Using element-wise square root and division operation,
    RMSProp guarantees that different elements in gradient $\nabla f$ have different
    coefficients, which represent learning rates in deep learning. Therefore, RMSProp
    enables parameters to have different learning rates [[29](#bib.bib29)], which
    makes the optimization process more flexible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the aforementioned optimization process on manifolds (Section [2.1](#S2.SS1
    "2.1 Geometric Optimization Process on Manifolds ‣ 2 Geometric Optimization Theory
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")), the Euclidean gradient descent algorithm in Equation ([8](#S2.E8
    "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization Theory ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")) can be transferred to Riemannian manifolds as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\theta^{(k+1)}=\mathfrak{R}_{{\theta}^{(k)}}(-\lambda grad\,f(\theta^{(k)})),$
    |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathfrak{R}_{{\theta}^{(k)}}$ means the retraction operation at point
    ${\theta}^{(k)}$ and $grad\,f$ means the Riemannian gradient. For better understanding,
    this article takes constraint SGD-M and constraint RMSProp as an instance to explain
    how to generalize gradient descent optimizers from Euclidean space to manifolds.
    By performing orthogonal projection and retraction, other Euclidean gradient descent
    optimizers can be similarly converted to Riemannian optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Constraint SGD-M [[29](#bib.bib29)]. Constraint SGD-M is a generalization of
    SGD-M optimizer on manifolds. In the $k$-th iteration, $m^{(k)}$ denotes a tangent
    vector on the tangent space $T_{\boldsymbol{\theta}^{(k-1)}}M$ and $m^{(k+1)}$
    denotes another vector on the tangent space $T_{\boldsymbol{\theta}^{(k)}}M$.
    Since $\nabla f$ is in the surrounding Euclidean space, it needs to be orthogonally
    projected to tangent space $T_{\boldsymbol{\theta}^{(k)}}M$, i.e., the current
    Riemannian gradient $grad\,f$ is achieved as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $grad\,f=\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla f).$ |  | (14) |'
  prefs: []
  type: TYPE_TB
- en: 'The transportation from a tangent space associated with point $p$ to another
    tangent space associated with point $q$ is called parallel transportation, i.e.,
    $\Gamma_{p\rightarrow q}:\ T_{p}M\rightarrow T_{q}M$. After projecting the Euclidean
    gradient $\nabla f$ to the tangent space $T_{\boldsymbol{\theta}^{(k)}}M$ and
    transporting $m^{(k)}$ from $T_{\boldsymbol{\theta}^{(k-1)}}M$ to $T_{\boldsymbol{\theta}^{(k)}}M$,
    Equation ([9](#S2.E9 "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")) is transformed to:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $m^{(k+1)}=\lambda_{0}\,\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+\lambda_{1}\,\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f).$ |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: Based on the retraction operation, the optimization parameter $\boldsymbol{\theta}^{(k+1)}$
    can be updated from $\boldsymbol{\theta}^{(k)}$ by searching along the geodesic
    in the negative direction of $m^{(k+1)}$, i.e., the iterate optimization can be
    expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-m^{(k+1)}).$
    |  | (16) |'
  prefs: []
  type: TYPE_TB
- en: 'Constraint RMSProp [[29](#bib.bib29)]. Similar to constraint SGD-M, after transporting
    $m^{(k)}$ from tangent space $T_{\boldsymbol{\theta}^{(k-1)}}M$ to $T_{\boldsymbol{\theta}^{(k)}}M$
    and orthogonally projecting $\nabla f\odot\nabla f$ to corresponding tangent space,
    Equation ([11](#S2.E11 "In 2.2 Gradient Descent Optimizers ‣ 2 Geometric Optimization
    Theory ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold")) can be transformed into:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $m^{(k+1)}=\lambda\Gamma_{\boldsymbol{\theta}^{(k-1)}\rightarrow\boldsymbol{\theta}^{(k)}}(m^{(k)})+(1-\lambda)\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f\odot\nabla f).$ |  | (17) |'
  prefs: []
  type: TYPE_TB
- en: The parameter $\boldsymbol{\theta}^{(k+1)}$ of the optimization goal can be
    iteratively searched on the manifold with a determined direction $-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}$, that is,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\boldsymbol{\theta}^{(k+1)}=\mathfrak{R}_{\boldsymbol{\theta}^{(k)}}(-\eta\,\frac{\Pi_{T_{\boldsymbol{\theta}^{(k)}}M}(\nabla
    f)}{\sqrt{m^{(k+1)}+\epsilon}}).$ |  | (18) |'
  prefs: []
  type: TYPE_TB
- en: 2.3 Manifold Examples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Different kinds of matrix manifolds have different geometry structures and
    satisfy different constraints, bringing different advantages when applying geometric
    optimization to deep learning. For example, the oblique manifold plays a significant
    role in dictionary learning due to its property of unit-norm columns, while the
    Stiefel manifold has a positive effect on optimizing RNNs since matrices on the
    Stiefel manifold have orthogonal and uncorrelated columns, which helps alleviate
    feature abundancy problems in RNNs. Since space is limited, this section only
    presents common manifold structures³³3For more introduction on matrix manifolds,
    we refer interested readers to the website https://www.Pymanopt.org. such as Stiefel
    manifold, oblique manifold, and Graßmann manifold, all of which are widely used
    in existing geometric optimization techniques that are discussed in Section [3](#S3
    "3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") and Section [4](#S4
    "4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold").'
  prefs: []
  type: TYPE_NORMAL
- en: Product Manifold and Quotient Manifold. Let $\mathcal{A}$ and $\mathcal{B}$
    be two manifolds of dimension $d_{A}$ and $d_{B}$, for any pair of charts $(U,\phi)$
    and $(V,\varphi)$ of $\mathcal{A}$ and $\mathcal{B}$, the map $\Phi$ is defined
    on $U\times V$ by $\Phi(x,y)=(\phi(x),\varphi(y))$. It specifies a smooth product
    manifold structure on the product space $\mathcal{A}\times\mathcal{B}$. Quotient
    manifold is an abstract space with similar subsets in the same manifold. These
    subsets can be described with equivalence relationship. $\mathcal{A}$ represents
    a manifold equipped with an equivalence relation $\sim$, which satisfies three
    properties, i.e., reflexivity, symmetry and transitivity [[8](#bib.bib8)]. The
    equivalence class of one point $x$ consists of all elements that are equivalent
    to it, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $[x]:=\{y\in\mathcal{A}:y\sim x\},$ |  | (19) |'
  prefs: []
  type: TYPE_TB
- en: where $[x]$ indicates the equivalence class of $x$. The quotient of manifold
    $\mathcal{A}$ by relation $\sim$ is defined as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{A}/\sim\ :=\{[x]:x\in\mathcal{A}\},$ |  | (20) |'
  prefs: []
  type: TYPE_TB
- en: with the projection $\pi:\mathcal{A}\rightarrow\mathcal{A}/\sim$, indicated
    by $x\rightarrow[x]$. When $\pi$ is a submersion projection, and $\mathcal{A}$
    is a smooth manifold [[8](#bib.bib8), [30](#bib.bib30)], $\mathcal{A}/\sim$ admits
    a unique smooth manifold structure $B$, which is the quotient manifold of $\mathcal{A}$.
  prefs: []
  type: TYPE_NORMAL
- en: Symmetric Positive-Definite Manifold [[29](#bib.bib29)]. It consists of Symmetric
    Positive-Definite (SPD) matrices $M\in\mathbb{R}^{p\times p}$ equipped with the
    Affine Invariant Riemannian Metric (*AIRM*) as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $S_{++}^{p}\triangleq\{M\in\mathbb{R}^{p\times p}:v^{T}Mv>0,\forall v\in\mathbb{R}^{p}-\{0_{p}\}\}.$
    |  | (21) |'
  prefs: []
  type: TYPE_TB
- en: SPD manifold achieves great success in computer vision due to its powerful statistical
    representations for images and videos. For example, SPD matrices are used to construct
    region covariance matrices for pedestrian detection [[31](#bib.bib31)], joint
    covariance descriptors for action recognition [[32](#bib.bib32)], and image set
    covariance matrices for face recognition [[33](#bib.bib33)].
  prefs: []
  type: TYPE_NORMAL
- en: Stiefel Manifold [[29](#bib.bib29)]. The Stiefel manifold $St(p,n)$ is composed
    of orthogonal matrices $W\in\mathbb{R}^{n\times p}(p\leq n)$ endowed with the
    Frobenius inner product as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $St(p,n)\triangleq\{W\in\mathbb{R}^{n\times p}:W^{T}W=I_{p}\},$ |  | (22)
    |'
  prefs: []
  type: TYPE_TB
- en: where $I_{p}$ denotes $\mathbb{R}^{p\times p}$ identity matrix. The optimization
    function over the compact Stiefel manifolds has an upper bound, which allows it
    to achieve an optimal solution.
  prefs: []
  type: TYPE_NORMAL
- en: Sphere Manifold and Oblique Manifold. The set of unit Frobenius norm matrices
    of size $n\times m$ is denoted by the sphere $\mathbb{S}^{nm-1}$. It can be treated
    as a Riemannian submanifold embedded in Euclidean space $\mathbb{R}^{n\times m}$
    endowed with the usual inner product $\langle H_{1},H_{2}\rangle=\operatorname{trace}(H_{1}^{T}H_{2})$.
    The oblique manifold $\mathcal{OB}(n,m)$ is the set of matrices of size $n\times
    m$ with unit-norm columns. It has the same geometry as that of the product manifold
    of spheres $\prod_{i=0}^{m}\mathbb{S}^{n-1}$.
  prefs: []
  type: TYPE_NORMAL
- en: Graßmann Manifold [[29](#bib.bib29)]. The Graßmann manifold $\mathcal{G}(n,p)$
    embraces the set of subspaces spanned by the orthogonal matrices $X\in\mathbb{R}^{n\times
    p}(p\leq n)$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathcal{G}(n,p)\triangleq\{Span(X):X\in\mathbb{R}^{n\times p},X^{T}X=I_{p}\}.$
    |  | (23) |'
  prefs: []
  type: TYPE_TB
- en: Note that a Graßmann manifold is different from a Stiefel manifold, i.e., a
    point on the Stiefel manifold represents a basis for a subspace, whereas a point
    on the Graßmann manifold represents an entire subspace. Moreover, Graßmann manifolds
    are of linear subspaces and can be used to perform a geometry-aware dimension
    reduction.
  prefs: []
  type: TYPE_NORMAL
- en: Unitary Manifold. Unitary matrices are the extension of orthogonal matrices
    to the complex domain, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $U(n)\triangleq\{U\in\mathbb{C}^{n\times n}:U^{\ast}U=I_{n}\},$ |  | (24)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $U^{\ast}$ denotes the conjugate transpose matrix and $I_{n}$ represents
    the identity matrix of size $n\times n$. Orthogonal or unitary matrices can preserve
    norm of vectors, i.e., $\|Wh\|_{2}$ = $\|h\|_{2}$ when $W$ is an orthogonal or
    unitary matrix. Therefore, exploding and vanishing gradient problems in deep temporary
    networks can be alleviated when parameters are optimized on the orthogonal or
    unitary manifold, which will de detailed in Section [4.2](#S4.SS2 "4.2 Geometric
    RNN ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for
    Deep Learning: From Euclidean Space to Riemannian Manifold").'
  prefs: []
  type: TYPE_NORMAL
- en: Lie Group [[13](#bib.bib13)]. Lie groups are real or complex manifolds with
    group structure. There are two compact and connected Lie groups, i.e., the special
    orthogonal group formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $SO(n)=\{B\in\mathbb{R}^{n\times n}&#124;B^{T}B=I,det(B)=1\},$ |  | (25)
    |'
  prefs: []
  type: TYPE_TB
- en: and the unitary group formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $U(n)=\{B\in\mathbb{C}^{n\times n}&#124;B^{\ast}B=I\}.$ |  | (26) |'
  prefs: []
  type: TYPE_TB
- en: The tangent space at the identity element of the Lie group is called the $Lie\
    algebra$ of it. For the special orthogonal group and the unitary group, their
    Lie algebras are given by
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathfrak{so}(n)$ | $\displaystyle=\{A\in\mathbb{R}^{n\times
    n}&#124;A\ +A^{T}=0\},$ |  | (27) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathfrak{u}(n)$ | $\displaystyle=\{A\in\mathbb{C}^{n\times
    n}&#124;A\ +A^{\ast}=0\}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: $\mathfrak{so}(n)$ is known as skew-symmetric matrix, while $\mathfrak{u}(n)$
    is skew-Hermitian matrix. The Lie exponential map ($exp:\mathfrak{g}\rightarrow
    G$ where $G$ denotes the Lie Group and $\mathfrak{g}$ denotes its Lie algebra)
    on a connected, compact Lie group is surjective. Therefore, the optimization problem
    on a Lie group can be converted to the optimization problem in Euclidean space
    where Euclidean gradient descent optimizers can be directly used.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Applications in Classical Machine Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Classical machine learning methods gained achievements in solving artificial
    intelligence problems (e.g., dimension reduction, inverse problem, sparse representation,
    analysis operator learning, and temporal models). Despite the increasing computing
    power of modern computer facilities, it is still difficult to solve a large category
    of constrained classical machine learning problems in Euclidean space. To decrease
    the solving difficulty, geometric optimization focuses on the special structure
    of constrained problems and regards them as unconstrained ones on Riemannian manifolds
    [[10](#bib.bib10)].
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Dimension Reduction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'By using a mapping $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ with $l<m$, dimension
    reduction (DR) aims to find a lower-dimensional representation $y_{i}\in\mathbb{R}^{l}$
    of given data samples $x_{i}\in\mathbb{R}^{m}$. The most popular DR paradigm uses
    a linear projection while others employ a nonlinear transformation to constrain
    locality properties between data. Table [1](#S3.T1 "Table 1 ‣ 3.1 Dimension Reduction
    ‣ 3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold") summarizes main
    properties of mainstream DR approaches (e.g., linear discriminant analysis (LDA)
    [[34](#bib.bib34)], principal component analysis (PCA) [[35](#bib.bib35), [36](#bib.bib36),
    [37](#bib.bib37)], multi-dimensional scaling (MDS) [[38](#bib.bib38), [39](#bib.bib39)],
    isometric feature mapping (ISOMAP) [[40](#bib.bib40)], local linear embedding
    (LLE) [[41](#bib.bib41)], laplace eigenmaps (LE) [[42](#bib.bib42)], and locality
    preserving projections (LPP) [[43](#bib.bib43)]).'
  prefs: []
  type: TYPE_NORMAL
- en: The mapping $\mu\colon\mathbb{R}^{m}\to\mathbb{R}^{l}$ used in DR methods is
    often restricted to be an orthogonal projection, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mu(\mathbf{x}):=\mathbf{V}^{\top}\mathbf{x},$ |  | (28) |'
  prefs: []
  type: TYPE_TB
- en: where the orthogonal matrix $\mathbf{V}\in\mathbb{R}^{m\times l}$ belongs to
    the *Stiefel* manifold ${St}(l,m):=\big{\{}\mathbf{V}\in\mathbb{R}^{m\times l}|\mathbf{V}^{\top}\mathbf{V}=\mathbf{I}_{l}\big{\}}$.
    One generic algorithmic framework to find an optimal $\mathbf{V}\in St(l,m)$ can
    be formulated as a maximization problem, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname*{argmax}_{\mathbf{V}\in St(l,m)}\,\frac{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{A}\mathbf{V})}{\operatorname{tr}(\mathbf{V}^{\top}\mathbf{B}\mathbf{V})+\sigma},$
    |  | (29) |'
  prefs: []
  type: TYPE_TB
- en: 'where matrices $A,B\in\mathbb{R}^{m\times m}$ are often symmetric or positive
    definite matrices. Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) is called *trace quotient* or *trace
    ratio*. Note that constant $\sigma>0$ can prevent the denominator from being zero.
    Matrices $A$ and $B$ are constructed to measure the similarity between data points
    according to specific problems. $V$ is not unique and closely related to selected
    eigenvalues. Solutions of Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣
    3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")) are rotation
    invariant, i.e., let $\mathbf{V}^{*}\in St(l,m)$ be a solution of the problem,
    then $\mathbf{V}^{*}\boldsymbol{\Theta}$ for any orthogonal $\boldsymbol{\Theta}\in\mathbb{R}^{l\times
    l}$ is also a solution of Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣
    3 Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")). In other words,
    the solution set of Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) is the set of all $l$-dimensional
    linear subspaces in $\mathbb{R}^{m}$, which can be represented by Graßmann manifold
    , i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathfrak{Gr}(l,m):=\left\{\mathbf{V}\mathbf{V}^{\top}&#124;\mathbf{V}\in
    St(l,m)\right\}.$ |  | (30) |'
  prefs: []
  type: TYPE_TB
- en: 'As shown above, most linear DR methods begin with solving $tr(V^{T}AV)$ while
    nonlinear DR methods construct a graph by connecting nearby points, which captures
    information on the local neighborhood structure of data and forms a similar optimization
    problem. Taking the non-linear DR method LE as an example, the Laplace matrix
    associated with the neighborhood graph [[44](#bib.bib44)] can be regarded as the
    symmetric matrix $A$ in Equation ([29](#S3.E29 "In 3.1 Dimension Reduction ‣ 3
    Applications in Classical Machine Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")).'
  prefs: []
  type: TYPE_NORMAL
- en: '[b]'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Summary of Dimension Reduction Algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: '| Methods | $Linear/Non-Linear^{1}$ | $Global/Local^{2}$ | Properties |'
  prefs: []
  type: TYPE_TB
- en: '| LDA | Linear | Global | is supervised, uses prior knowledge of categories,
    is limited to Gaussian distribution samples |'
  prefs: []
  type: TYPE_TB
- en: '| PCA | Linear | Global | is unsupervised, uses orthogonal principal components
    to eliminate interactions between each components |'
  prefs: []
  type: TYPE_TB
- en: '| MDS | Non-Linear | Global | has simple calculation, preserves the data relationship
    in original space, is visualization-friendly, mistakenly assumes that each dimension has
    a same contribution |'
  prefs: []
  type: TYPE_TB
- en: '| ISOMAP | Non-Linear | Global | suits low dimensional manifolds with a flat interior
    rather than that with large internal curvature, has high computation cost |'
  prefs: []
  type: TYPE_TB
- en: '| LLE | Non-Linear | Local | suits non-closed locally linear low dimensional
    manifolds, has small computational complexity, is limited to dense uniform dataset,
    is sensitive to the number of nearest neighbor samples |'
  prefs: []
  type: TYPE_TB
- en: '| LE | Non-Linear | Local | preserves local features, is less sensitive to
    outliers and noise, has a stable embedding |'
  prefs: []
  type: TYPE_TB
- en: '| LPP | Linear | Local | is defined at any point in space, i.e., can be generalized
    to the testing set and not limited to the training set |'
  prefs: []
  type: TYPE_TB
- en: '1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Linear represents linear projection mapping, while non-linear represents non-linear
    projection mapping.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The global/local represents the geometric relationship of the input data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2 Inverse Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Aiming to explore internal patterns from phenomena [[45](#bib.bib45)], an inverse
    problem has a significant impact on practical applications. For example, the following
    practical problems can be modeled as inverse problems: i) deducing structural
    information in human body from the X-ray; and ii) infering interior appearance
    of stratigraphy from seismic wave. An inverse problem can be viewed as reconstructing
    inputs from outputs as follows,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathbf{y}=\bm{{W}}{\mathbf{x}},$ |  | (31) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{y}\in\mathbb{R}^{l}$ is the given output and $\bm{{W}}$ is a
    matrix that maps input data $\mathbf{x}$ to output data $\mathbf{y}$. The goal
    of the inverse problem in Equation ([31](#S3.E31 "In 3.2 Inverse Problem ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) is to recover $\mathbf{x}$ on the
    premise that $\mathbf{y}$ is a priori. It is challenging to get a precise solution,
    however, an approximate solution can be achieved by confining the parameter matrix
    $\bm{{W}}$ to reside on a smooth Riemannian manifold. Let the sum of elements
    in the same row of matrix $\bm{{W}}$ be exact 1, Equation ([31](#S3.E31 "In 3.2
    Inverse Problem ‣ 3 Applications in Classical Machine Learning ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold"))
    can be solved by optimization on the oblique manifold $\mathcal{M}$ where matrices
    all have unit row sums [[7](#bib.bib7)], i.e.,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\bm{{W}}\in\mathcal{M}}\left\&#124;\mathbf{y}-\bm{{W}}\mathbf{x}\right\&#124;_{2}^{2}.$
    |  | (32) |'
  prefs: []
  type: TYPE_TB
- en: 3.3 Dictionary Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As a specific inverse problem, dictionary learning has been widely used to obtain
    the most essential features of input data [[23](#bib.bib23)]. Let $X\in R_{n\times
    k}$ denote the input sample, in dictionary learning, $X$ is expanded into a linear
    combination as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X=D_{1}\phi_{1}+\cdots+D_{n}\phi_{n},$ |  | (33) |'
  prefs: []
  type: TYPE_TB
- en: 'where $D_{1},\cdots,D_{n}$ represent the most essential features to be learned
    from the input, while $\phi_{1},\cdots,\phi_{n}$ indicate combination coefficients
    of features $D_{1},\cdots,D_{n}$. Let $D\in R_{k\times n}$ indicate the dictionary
    set $\{D_{1},\cdots,D_{n}\}$ and $\Phi\in R_{n\times r}$ indicate the set $\{\phi_{1},\cdots,\phi_{n}\}$,
    Equation ([33](#S3.E33 "In 3.3 Dictionary Learning ‣ 3 Applications in Classical
    Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) can be simplified as follows,'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X=D\Phi,$ |  | (34) |'
  prefs: []
  type: TYPE_TB
- en: 'where $D$ and $\Phi$ can have various kinds of combinations. Dictionary learning
    aims to learn a $D$ that makes the coefficients $\Phi$ be zero or close to zero,
    i.e., a sparse representation of samples $X$. The dictionary $D$ and the sparse
    coefficients $\Phi$ are calculated alternately. When $\Phi$ is fixed, the dictionary
    learning part is the same as the form of Equation ([31](#S3.E31 "In 3.2 Inverse
    Problem ‣ 3 Applications in Classical Machine Learning ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")),
    which is an inverse problem of reconstructing $D$. Let $\left\|\phi\right\|_{0}$
    denote the number of entries in $\Phi$ that are different from zero, the dictionary
    $D$ is subject to $\left\|D_{1}\right\|=...=\left\|D_{n}\right\|=1$. Therefore,
    the above dictionary learning problem can be transformed to the following minimization
    problem on the oblique manifold:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\operatorname*{argmin}_{D\in\textit{OB}(k,n)}\left\&#124;X-D\Phi\right\&#124;_{2}^{2}+\lambda\left\&#124;\Phi\right\&#124;_{0}.$
    |  | (35) |'
  prefs: []
  type: TYPE_TB
- en: 3.4 Analysis Operator Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Analysis operator learning assumes that a few operators are sufficient to represent
    observed high-dimensional variables [[46](#bib.bib46)]. However, these operators
    are implicit and unobserved, for instance, store environment and service quality
    are latent operators hidden behind the observed variable “price”. The goal of
    analysis operator learning is to find out these invisible operators, since low-dimensional
    operators can simplify original high-dimensional variables.
  prefs: []
  type: TYPE_NORMAL
- en: Let $X$ be original high-dimensional variables and $F$ be latent operators with
    lower dimensions, the analysis operator learning can be generally formulated as
    follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $X=AF,$ |  | (36) |'
  prefs: []
  type: TYPE_TB
- en: where $A$ denotes the operator loading matrix, in which the element $A_{ij}$
    represents the load of variable $x_{i}$ on factor $f_{j}$. It is proved that the
    parameter $A$ can be positive [[47](#bib.bib47)], the analysis operator learning
    can therefore be converted to an optimization problem on the positive manifold
    $\mathcal{M}$ as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{A\in\mathcal{M}}\left\&#124;X-AF\right\&#124;_{2}^{2}.$ |  | (37)
    |'
  prefs: []
  type: TYPE_TB
- en: 3.5 Temporal Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The temporal probability model is composed of a transition model describing
    the state evolution over time and a sensor model describing the observation process
    [[27](#bib.bib27)]. A temporal model is helpful to cope with filtering, prediction
    and smoothing. In the transition model, next state $z_{t+1}$ is transited from
    the current state $z_{t}$, independent from previous states. Given the time-relevant
    transition probability $A(t)$, the transition process of states can be modeled
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $z_{t+1}=A(t)\cdot z_{t}+\epsilon(t),$ |  | (38) |'
  prefs: []
  type: TYPE_TB
- en: where the noise $\epsilon(t)$ follows the Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: States are invisible and a hidden state can manifest as a specific observation
    with the help of an emission probability. The current observation $x_{t}$ is only
    defined by the current state $z_{t}$, having nothing to do with previous states
    and observations. Given a time-varying emission probability $C(t)$, the observation
    process can be modeled as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $x_{t}=C(t)\cdot z_{t}+\delta(t),$ |  | (39) |'
  prefs: []
  type: TYPE_TB
- en: where the noise $\delta(t)$ follows the Gaussian distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a mixture of Equation ([38](#S3.E38 "In 3.5 Temporal Model ‣ 3 Applications
    in Classical Machine Learning ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold")) and Equation ([39](#S3.E39 "In
    3.5 Temporal Model ‣ 3 Applications in Classical Machine Learning ‣ A Survey of
    Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")),
    temporal models can be divided into hidden Markov models and linear dynamic systems.
    A hidden Markov model has discrete hidden state variables while the hidden state
    and observed variables of a linear dynamic system obey Gaussian distribution.
    Let $n$ represent the size of the temporal sequence, the expectation of observation
    sequences $E[x_{0},x_{1},x_{2}\cdots]$ can be deduced as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $[C(t),C(t)A(t),C(t)A(t)^{2}\cdots C(t)A(t)^{n-1}]\,z_{0},$ |  | (40)
    |'
  prefs: []
  type: TYPE_TB
- en: 'where $z_{0}$ is the initial hidden state. It can be considered as a sequence
    of subspaces spanned by the emission and transition matrix columns at the corresponding
    time [[48](#bib.bib48)]. As is mentioned in Section [2.3](#S2.SS3 "2.3 Manifold
    Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold"), a point on the
    Graßmann manifold is a subspace. Therefore, the temporal model can be mathematically
    optimized on the Graßmann manifold.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Applications in Deep Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the increasing attention to geometric optimization, more and more deep
    learning methods have developed to combine with it. Geometric optimization techniques
    vary with different deep learning backbones (e.g., CNN, RNN and GNN). Therefore,
    this section classifies applications in deep learning into the following categories,
    i.e., i) geometric CNN; ii) geometric RNN; iii) geometric GNN and iv) geometric
    optimization for other deep learning methods, such as transfer learning and optimal
    transport. Orthogonal manifold is widely employed in geometric CNNs to reduce
    feature redundancy. Examples include utilizing kernel orthogonality in Orthogonal
    CNNs [[6](#bib.bib6)], optimization on Submanifolds of Convolution Kernels in
    CNNs [[49](#bib.bib49)], and regularizing the convolution kernel with orthogonality
    when training deep CNNs [[12](#bib.bib12)]. In addition, geometric CNNs can leverage
    the unique structure of Stiefel manifold [[50](#bib.bib50)] and Graßmann manifold
    [[51](#bib.bib51)]. Geometric RNNs take advantage of the norm-keeping property
    of orthogonal and unitary manifolds to alleviate gradient explosion and vanishing
    problems. Examples include complex unitary matrices in Unitary Evolution Recurrent
    Neural Networks [[14](#bib.bib14)], and the special orthogonal group and unitary
    group in Cheap Orthogonal Constraints: A Simple Parameterization of the Orthogonal
    and Unitary Group [[13](#bib.bib13)]. Geometric GNNs pay much attention to hyperbolic
    manifold and extensively use it for structure capturing. Examples include the
    hyperbolic GNN [[52](#bib.bib52)] and a geometric neural network which incorporates
    Euclidean space with hyperbolic geometry [[53](#bib.bib53)].'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Geometric CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep CNN has achieved great success in various computer vision tasks, such as
    image recognition [[54](#bib.bib54)] and segmentation tasks [[55](#bib.bib55)].
    CNN can automatically learn features from large-scale data by benefiting from
    three essential structures, i.e., convolution, activation, and pooling structures
    [[10](#bib.bib10)]. Although CNNs have worked efficiently, using the entire Euclidean
    space to search optimal solutions cause problems (e.g., training instability and
    feature redundancy) that hinder the further development. To alleviate these problems,
    geometric optimization approaches optimize CNNs on the suitable Riemannian manifold
    via kernel space, geometric regularization, and quasi-CNN architectures with parameters
    on the manifold.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel Space. A low-dimensional manifold is often embedded in the high-dimensional
    Euclidean space. Kernel functions can map original features to a higher dimensional
    space. Therefore, with the help of kernel functions, computationally cheap operations
    on manifolds can represent complex operations in Euclidean space. Kernel spaces
    can be utilized and described by topological smooth manifolds. For example, positive-definite
    kernels, which are known as Graßmann kernels on the Graßmann manifold, can be
    used to map the manifold into a Hilbert space [[56](#bib.bib56)]. Zhang et al.
    [[57](#bib.bib57)] designed a new kind of Graßmann kernel based on canonical correlations
    to distinguish one class from others more accurately. Liu et al. [[58](#bib.bib58)]
    designed RBF kernels for linear subspace, covariance matrix, and Gaussian distribution
    to optimize emotion video recognition on the Riemannian manifolds. Hariri et al.
    [[59](#bib.bib59)] defined a kernel based on the SPD covariance matrix to indicate
    the similarity of two face images for face matching.
  prefs: []
  type: TYPE_NORMAL
- en: Kernel space constructed on nonlinear data helps learn the inherent manifold
    structure. Yuan et al. [[60](#bib.bib60)] combined manifold kernel space with
    deep learning architecture for scene recognition. To preserve the geometric structure
    of input scene images and achieve a greater representational ability, [[60](#bib.bib60)]
    defines a low-level feature layer $X$ and a hidden manifold kernel space $Y$ as
    a base unit. Moreover, the deep architecture is unit-by-unit and $Y_{k}$ serves
    as the input of another base unit to generate the next hidden space $Y_{k+1}$.
    Comparative experiments evaluate the performance of the manifold regularized deep
    network on the large-scale scene data set.
  prefs: []
  type: TYPE_NORMAL
- en: Ozay et al. [[49](#bib.bib49)] considered the kernel estimation problem in CNNs
    as an optimization on embedded or immersed submanifolds of kernels. [[49](#bib.bib49)]
    explores geometric properties of convolution kernel space in CNNs and reveals
    that different kernel normalization methods induce different geometric properties.
    For example, the orthonormal normalization manner implies Stiefel manifold, while
    kernels normalized with the unit-norm reside on the sphere manifold. Furthermore,
    [[49](#bib.bib49)] proposes an SGD algorithm for optimization on kernel submanifolds.
    Experiments carried out on three kernel submanifolds confirm that the above approach
    can boost the performance of traditional CNN training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Geometric Regularization. Regularization acts as the penalty term of the optimization
    function. It is used to impose restrictions on the parameters of the optimization
    function. The commonly used geometric regularization is the orthogonal constraint,
    aiming to restrict parameters to be on the orthogonal manifold. Recall orthogonal
    matrices $W^{T}W=I$ introduced in Section [2.3](#S2.SS3 "2.3 Manifold Examples
    ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold"), orthogonal regularization
    methods are roughly divided into hard orthogonality as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\&#124;W^{T}W-I\&#124;_{F}^{2}$ |  | (41) |'
  prefs: []
  type: TYPE_TB
- en: and soft orthogonality as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lambda\&#124;W^{T}W-I\&#124;_{F}^{2}$ |  | (42) |'
  prefs: []
  type: TYPE_TB
- en: where $\|\cdot\|_{F}$ indicates the Frobenius norm and $\lambda$ represents
    a relaxation coefficient. Based on the soft orthogonality, we can achieve double
    soft orthogonality as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\lambda(\&#124;W^{T}W-I\&#124;_{F}^{2}+\&#124;WW^{T}-I\&#124;_{F}^{2}).$
    |  | (43) |'
  prefs: []
  type: TYPE_TB
- en: Based on the observation that the kernel orthogonality is necessary but insufficient
    for the orthogonal convolution, Wang et al. [[6](#bib.bib6)] proposed an approach
    where orthogonality constraints directly regularize a convolution layer. During
    training, the convolution filter $K$ is transformed into a Doubly Block-Toeplitz
    (DBT) matrix and the spectrum is regularized to be uniform, which requires row
    or column orthogonality. The orthogonality constraint on the DBT matrix helps
    relieve exploding and vanishing gradient problems, making the training more stable.
    Moreover, a number of experiments show that it can achieve amazing performance
    such as stronger robustness and better generalization.
  prefs: []
  type: TYPE_NORMAL
- en: Bansal et al. [[12](#bib.bib12)] observed that orthogonality can stabilize the
    energy distribution of activations within CNNs and enhance the efficiency of training.
    [[12](#bib.bib12)] compares different orthogonality regularizers, e.g. soft orthogonality,
    double soft orthogonality and mutual coherence regularization that lowers the
    column correlation as much as possible to enforce orthogonality. Meanwhile, [[12](#bib.bib12)]
    designs a novel orthogonality regularizer named Spectral Restricted Isometry Property
    Regularization, which focuses on minimizing the spectral norm of $W^{T}W-I$. Remarkable
    experimental results suggest that regarding orthogonality regularizations as standard
    tools for training deep CNNs offers better accuracy and stablity.
  prefs: []
  type: TYPE_NORMAL
- en: In order to estimate human face poses under challenging circumtances such as
    complex background or various orientations, Hong et al. [[61](#bib.bib61)] proposed
    manifold regularized convolutional layers (MRCL) to enhance the nonlinear locality
    constraints of CNN parameters. With MRCL being on top of traditional CNN’s pooling
    and activation operations, a low-rank manifold structure of latent data can be
    recovered for better optimization. By employing multitask learning with low-rank
    learning, multimodal of different data representations can be combined to predicate
    face postures. Comparative experiments validate the benefit of imposing manifold
    regularization to traditional convolutional layers.
  prefs: []
  type: TYPE_NORMAL
- en: Roufosse et al. [[62](#bib.bib62)] proposed a spectral unsupervised functional
    map network (SURFMNet) where the matching network from one shape to another is
    constrained to the orthogonal manifold. SURFMNet computes correspondences across
    3D shapes using unsupervised learning, i.e., building shape correspondences without
    ground truth. Solid experimental results support the consistent superiority of
    SURFMNet compared to state-of-the-art unsupervised shape matching methods. Experimental
    results also show that SURFMNet is comparable to supervised ones.
  prefs: []
  type: TYPE_NORMAL
- en: Different from existing methods that shallowly learn Lie group features, Huang
    et al. [[63](#bib.bib63)] incorporated a Lie group structure to parameter matrices
    in the deep human action recognition network. The proposed skeleton-based human
    model $(V,E)$ is a binary relation, where $V$ represents a set of vertexes that
    consists of body joints $(v_{1},\dots,v_{N})$ and $E$ represents a set of edges
    that consists of body bones $(e_{1},\dots,e_{M})$. The rotation matrix is represented
    by the axis-angle model based on the skeleton and forms the special orthogonal
    group. To preserve the Lie group structure of the input rotation matrix, the above
    human action recognition network is optimized on the Lie group manifold and mapped
    to a tangent space for the final classification.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the above action recognition network [[63](#bib.bib63)], Chen et
    al. [[64](#bib.bib64)] put forward a deep manifold learning (DML) framework to
    learn manifold information and deep representations of action videos. [[64](#bib.bib64)]
    studies that leveraging geometry information in deep learning contributes to high
    accuracy and efficiency for action recognition. To extract more expressing features,
    the DML framework applies a manifold regularizer on the previous layer, label
    information and manifold parameters. Furthermore, adapting the DML framework to
    restricted Boltzmann machine can relieve the overfitting problem and improve the
    recognition accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Quasi-CNN Architecture. Kernel methods and orthogonal regularization do not
    change fundamental CNN components (e.g., convolution and pooling operations).
    Another method of applying geometric optimization to deep learning is to mimic
    traditional CNN architecture and establish a new architecture suitable for the
    manifold structure. In this article, the above architecture is named as quasi-CNN
    architecture. Convolution and activation layers are rebuilt to induce geometric
    optimization in the quasi-CNN architecture. To achieve this goal, parameters in
    the quasi-CNN architecture are designed to reside on the compact Stiefel manifold.
    For a more intuitive explanation, this article takes the deep SPD matrix network
    (SPDNet) [[50](#bib.bib50)] and deep Graßmann neural network architecture (GrNet)
    [[51](#bib.bib51)] as examples.
  prefs: []
  type: TYPE_NORMAL
- en: Let $X$ be the input, and $W$ be the transformation parameter on the compact
    Stiefel manifold. First, SPDNet is designed for optimization on the SPD manifold.
    Bilinear mapping (BiMap) layer $f_{b}=WXW^{T}$ plays the role of convolution layers
    in traditional CNNs. Based on the eigenvalue decomposition $X$ = $U\Sigma U$,
    eigenvalue rectification (ReEig) layers $f_{r}=Umax(\epsilon I,\Sigma)U^{T}$ are
    designed to replace nonlinear activation layers and $\epsilon$ is the activation
    threshold. SPDNet designs the eigenvalue logarithm (LogEig) layer to flatten the
    Riemannian manifold to a flat space where classical Euclidean computations can
    be applied. GrNet is designed for optimization along the orthonormal manifold.
    Full rank mapping (FRMap) layers $f_{fr}=WX$ in GrNet replace the convolution
    layer in traditional CNNs. Inspired by the QR decomposition $X=QR$ where $Q$ is
    orthonormal, GrNet designs re-orthonormalization (ReOrth) layer $f_{fo}=XR^{-1}=Q$
    to achieve an orthonormal output. Unlike the LogEig layer in SPDnet, GrNet uses
    inner product $XX^{T}$ to reduce the manifold to a flat Euclidean space. After
    pooling operations on the resulting Euclidean data, GrNet designs orthonormal
    mapping (OrthMap) layer $f_{om}=U_{1:q}$ to transform the output matrix back to
    the orthonormal manifold, where $U_{1:q}$ denotes the first $q$ largest eigenvectors
    achieved by the eigenvalue decomposition.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Geometric RNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'RNNs are designed to process sequential data since they can capture spatial
    and temporal dependencies between the sequential input. Therefore, RNN can be
    applied in tasks such as speech recognition, text prediction, and machine translation.
    Given an input sequence $X_{\tau}={x_{1},x_{2},\cdots,x_{\tau}}$ ($x_{i}\in\mathbb{R}^{n}$)
    with length $\tau$, a basic RNN framework is aimed to generate the output sequence
    $Y_{\tau}={y_{1},y_{2},\cdots,y_{\tau}}$ ($y_{i}\in\mathbb{R}^{p}$). With hidden
    state $h$ passed recurrently into the model at each time step, output predictions
    $o_{i}\in\mathbb{R}^{p}$ of the RNN are computed as follows [[65](#bib.bib65)]:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle h_{i}$ | $\displaystyle=\sigma(Ux_{i}+Wh_{i-1}+b),$ |  |
    (44) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle o_{i}$ | $\displaystyle=Vh_{i}+c,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $U\in\mathbb{R}^{m\times n}$ is the input weight matrix, $W\in\mathbb{R}^{m\times
    m}$ is the recurrent weight matrix, $h_{i-1}\in\mathbb{R}^{m}$ is the previous
    hidden state, $b\in\mathbb{R}^{m}$ is the input bias, $\sigma(\cdot)$ is a pointwise
    nonlinearity function, $h_{i}\in\mathbb{R}^{m}$ is the current hidden state, $V\in\mathbb{R}^{p\times
    m}$ is the output weight matrix, and $c\in\mathbb{R}^{p}$ is the output bias.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Orthogonal RNN (ORNN)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Denote $\mathcal{L}$ as the objective function to be minimized, the gradient
    of the loss function for the hidden state is computed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\frac{\partial\mathcal{L}}{\partial h_{i}}$ | $\displaystyle=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\cdot\frac{\partial h_{\tau}}{\partial h_{i}}=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\cdot\prod_{j=i}^{\tau-1}\frac{\partial h_{j+1}}{\partial h_{j}}=\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}(\prod_{j=i}^{\tau-1}D_{j+1}W^{T}),$ |  | (45) |'
  prefs: []
  type: TYPE_TB
- en: 'where $D_{j+1}\in\mathbb{R}^{m\times m}$ is a diagonal matrix, whose entries
    consist of the derivate of the activation function. The pointwise non-linearity
    function $\sigma(\cdot)$ in Equation ([44](#S4.E44 "In 4.2 Geometric RNN ‣ 4 Applications
    in Deep Learning ‣ A Survey of Geometric Optimization for Deep Learning: From
    Euclidean Space to Riemannian Manifold")) is suggested to be a rectified linear
    unit (ReLU) function [[66](#bib.bib66), [67](#bib.bib67)], whose output has a
    minimum value of $0$. The input $D_{j+1}$ has at least one non-zero entry of the
    derivative value for all $j$. Taking the Euclidean $l_{2}-norm$ to both sides
    of Equation ([45](#S4.E45 "In 4.2.1 Orthogonal RNN (ORNN) ‣ 4.2 Geometric RNN
    ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")), we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\left\&#124;\frac{\partial\mathcal{L}}{\partial h_{i}}\right\&#124;_{2}$
    | $\displaystyle\leqslant(\prod_{j=i}^{\tau-1}\left\&#124;D_{j+1}W^{T}\right\&#124;_{2})\left\&#124;\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\right\&#124;_{2}=(\prod_{j=i}^{\tau-1}\left\&#124;W\right\&#124;_{2})\left\&#124;\frac{\partial\mathcal{L}}{\partial
    h_{\tau}}\right\&#124;_{2}$ |  | (46) |'
  prefs: []
  type: TYPE_TB
- en: 'If $\|W\|_{2}$ is greater than one, $\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ grows exponentially as the increase of $\tau$. As a result,
    the norm of the gradient in Equation ([46](#S4.E46 "In 4.2.1 Orthogonal RNN (ORNN)
    ‣ 4.2 Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold"))
    disclosures the well-known gradient exploding problem that hinders the RNN from
    training [[68](#bib.bib68)]. If $\|W\|_{2}$ is smaller than one, $\left\|\frac{\partial\mathcal{L}}{\partial
    h_{i}}\right\|_{2}$ declines exponentially as the increase of $\tau$, which leads
    to gradient vanishing problems [[68](#bib.bib68)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'A recent line of ORNNs imposes the orthogonal constraint on the hidden-to-hidden
    transformation of RNN. The recurrent weight transformation matrix $W$ is restricted
    to be on the orthogonal manifold. Let $A$ be an orthogonal matrix, for each vector
    $X$, its norm after orthogonal transformation is:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $(AX)^{T}(AX)=X^{T}A^{T}AX=X^{T}X,$ |  | (47) |'
  prefs: []
  type: TYPE_TB
- en: 'which means that orthogonal transformations do not change the norm of the original
    vector. As a result, $\left\|\frac{\partial\mathcal{L}}{\partial h_{i}}\right\|_{2}$
    can remain invariant in ORNN when the transformation matrix $W$ in Equation ([46](#S4.E46
    "In 4.2.1 Orthogonal RNN (ORNN) ‣ 4.2 Geometric RNN ‣ 4 Applications in Deep Learning
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold")) is orthogonal. Therefore, the exploding and vanishing gradient
    problem of RNN can be alleviated. Moreover, orthogonal constraints can be generalized
    to unitary constraints in the complex domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Recent Advances of ORNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: uRNN [[14](#bib.bib14)] constructs a large unitary matrix by simple parametric
    unitary matrices, i.e., the unitary hidden-to-hidden matrix $W$ is composed as
    follows,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle W=D_{3}R_{2}F^{-1}D_{2}\Pi R_{1}FD_{1},$ |  | (48) |'
  prefs: []
  type: TYPE_TB
- en: 'where $D$ is a diagonal matrix whose diagonal element $D_{j,j}=e^{iw_{j}}$
    is defined by the imaginary unit $i$ and parameters $w_{j}\in\mathbb{R}$, $R=I-\
    2\frac{vv^{\ast}}{\|v\|^{2}}$ is a reflection matrix with the complex vector $v\in\mathbb{C}^{n}$,
    $\Pi$ is a fixed random index permutation matrix, and $F$ and $F^{-1}$ are the
    Fourier and inverse Fourier transforms. In the matrix construction strategy like
    Equation ([48](#S4.E48 "In 4.2.2 Recent Advances of ORNN ‣ 4.2 Geometric RNN ‣
    4 Applications in Deep Learning ‣ A Survey of Geometric Optimization for Deep
    Learning: From Euclidean Space to Riemannian Manifold")), the number of parameters,
    memory, and computational overhead increase slowly at approximately linear speeds.
    Therefore, the training cost of large hidden layers can be reduced. In uRNN, a
    variation of the nonlinear activation ReLU named modReLU has been proposed to
    maintain the phase of complex-valued hidden states:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma_{modReLU}(z)=\left\{\begin{aligned} &amp;(&#124;z&#124;+b)\frac{z}{&#124;z&#124;},\quad&amp;if\
    &#124;z&#124;+b\geq 0\\ &amp;0,\quad&amp;if\ &#124;z&#124;+b\leq 0\end{aligned}\right.$
    |  | (49) |'
  prefs: []
  type: TYPE_TB
- en: where $b$ $\in\mathbb{R}$ is a bias parameter. uRNN defines a matrix $U$ to
    map complex-valued hidden state $h_{t}$ to real-valued output for prediction.
    The corresponding loss function is calculated as follows
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $o_{t}=U\left(\begin{aligned} Re(h_{t})\\ Im(h_{t})\end{aligned}\right)+b_{o},$
    |  | (50) |'
  prefs: []
  type: TYPE_TB
- en: where $b_{o}$ is the output bias, $Re(h_{t})$ and $Im(h_{t})$ represent the
    real and imaginary part of $h_{t}$ respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, Wisdom et al. [[69](#bib.bib69)] noticed that the unitary parameter
    construction of Equation ([48](#S4.E48 "In 4.2.2 Recent Advances of ORNN ‣ 4.2
    Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold")) cannot cover
    all $N\times N$ unitary matrices for $N>7$, i.e., at least one $N\times N$ unitary
    matrix cannot be represented in the form of Equation ([48](#S4.E48 "In 4.2.2 Recent
    Advances of ORNN ‣ 4.2 Geometric RNN ‣ 4 Applications in Deep Learning ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold")). To address this problem, [[69](#bib.bib69)] designs a method to measure
    the representation capacity of the structured $N\times N$ unitary matrix. [[69](#bib.bib69)]
    comes up with a perspective that the unitary matrices parameterized by $P$ real-valued
    parameters for $P\geq N^{2}$ is full-capacity, which means that it can cover all
    $N\times N$ unitary matrices.'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike generating compound orthogonal matrices with simple ones, the Lie exponential
    map can achieve orthogonal constraint on the hidden-to-hidden transformation [[13](#bib.bib13)].
    The connected subjective exponential mapping $exp:\mathfrak{g}$ $\rightarrow$
    G on the special orthogonal group is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle exp(A):=I\ +\ A+\ \frac{1}{2}A^{2}+\ \dots.$ |  | (51)
    |'
  prefs: []
  type: TYPE_TB
- en: Since it is a subjection, for each hidden-to-hidden transformation matrix $W$
    belonging to the special orthogonal group or unitary group, there must exist a
    skew-symmetric (or skew-Hermitian matrix) $A$ that satisfies $exp(A)=W$. Therefore,
    the hidden-to-hidden transformation $h_{t+1}=\sigma(Wh_{t}\ +Tx_{t+1})$ is equivalent
    to $h_{t+1}=\sigma(exp(A)h_{t}\ +Tx_{t+1})$. That is, the optimization on the
    orthogonal or unitary manifold can be transformed to the optimization in Euclidean
    space. Consequently, classic gradient descent optimizers such as Adam can be applied
    to minimize the loss function as well as satisfying orthogonal constraints. As
    a result, the Lie exponential map can achieve both cheap computation overhead
    and the mitigation of gradient exploding and vanishing problems.
  prefs: []
  type: TYPE_NORMAL
- en: Another method [[70](#bib.bib70)], which is based on the Lie group, defines
    a basis $\{T_{j}\}_{j=\{1,\cdots,n^{2}\}}$ and coefficients $\{\lambda_{j}\}_{j=\{1,\cdots,n^{2}\}}$
    to construct the element $L\in\mathfrak{u}(n)$ as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $L=\Sigma_{j=1}^{n^{2}}\lambda_{j}T_{j}.$ |  | (52) |'
  prefs: []
  type: TYPE_TB
- en: 'By using exponential mapping, the element $U$ of corresponding unitary Lie
    group $U(n)$ can be represented as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $U=exp(L)=exp(\Sigma_{j=1}^{n^{2}}\lambda_{j}T_{j}).$ |  | (53) |'
  prefs: []
  type: TYPE_TB
- en: Furthermore, Hyland et al. [[70](#bib.bib70)] offered an argument that the above
    parameterization helps generalize unitary RNN to arbitrary unitary matrices and
    figure out long-memory tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Learning orthogonal filters in deep neural networks (DNN) can be formulated
    as an optimization problem over multiple dependent Stiefel manifolds (OMDSM) [[71](#bib.bib71)].
    The orthogonal linear module can substitute standard linear module in DNNs to
    stabilize the distributions of activation and regularize networks. Let $W_{k}$
    and $b_{k}$ be learnable weight matrix and bias, parameter $\theta$ be $\{W_{k},b_{k}|k=1,2,\dots
    K\}$, the deep neural network can be represented as $f(x,\theta):x\rightarrow\hat{y}$,
    where $x$ is the input feature, and $\hat{y}$ is the output prediction of DNN.
    The loss function is often designed as the discrepancy between label $y$ and prediction
    values: $\mathcal{L}(y,f(x,\theta))$. Finally, the optimization problem is formulated
    as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta^{\ast}=argmin_{\theta}\mathbb{E}_{(x,y)\in D}[\mathcal{L}(y,f(x,\theta))].$
    |  | (54) |'
  prefs: []
  type: TYPE_TB
- en: OMDSM trains DNN with orthogonal weight matrix $W_{k}$ in each layer. Thus,
    the optimization problem is reformulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\theta^{\ast}=argmin_{\theta}\mathbb{E}_{(x,y)\in D}[\mathcal{L}(y,f(x,\theta))]$
    |  | (55) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle s.t.\ W_{k}\in\mathbb{O}_{k}^{n_{k}\times d_{k}},k=1,2,\dots
    K,$ |  |'
  prefs: []
  type: TYPE_TB
- en: where the matrix family $\mathbb{O}_{k}^{n_{k}\times d_{k}}=\{W_{k}\in\mathbb{R}^{n_{k}\times
    d_{k}}|W_{k}W_{k}^{T}=I\}$ is composed of multiple real Stiefel manifolds, which
    is an embedded sub-manifold of $\mathbb{R}^{n_{k}\times d_{k}}$. Each independent
    orthogonal filter $W\in\mathbb{R}^{n\times d}$ is given by the proxy parameter
    $V\in\mathbb{R}^{n\times d}$ as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W=PV,$ |  | (56) |'
  prefs: []
  type: TYPE_TB
- en: 'where $n$ is the number of output channels, $d$ is the number of input channels,
    and $P\in\mathbb{R}^{n\times n}$ is the coefficient of the linear transformation.
    Firstly, $V$ is centered by $V_{C}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $V_{C}=V-c{1_{d}^{T}},$ |  | (57) |'
  prefs: []
  type: TYPE_TB
- en: 'where $c=\frac{1}{d}V1_{d}$ and $1_{d}$ is the $d$-dimension vector with all
    ones. Moreover, the eigenvalues $\wedge$ and eigenvectors $D$ of the covariance
    matrix $V_{C}{V_{C}}^{T}$ are used to construct $P$:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $P=D\wedge^{-1/2}D^{T}.$ |  | (58) |'
  prefs: []
  type: TYPE_TB
- en: Finally, $W$ is formulated as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W=D\wedge^{-1/2}D^{T}V_{C}.$ |  | (59) |'
  prefs: []
  type: TYPE_TB
- en: Research has been conducted on exploring the influence of soft orthogonal constraints
    [[72](#bib.bib72)]. By allowing the diagonal elements of $S$ to float around 1,
    the orthogonal transformation matrix $W$ is relaxed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $W=USV^{T},$ |  | (60) |'
  prefs: []
  type: TYPE_TB
- en: where $U$ and $V$ are strict orthogonal matrices.
  prefs: []
  type: TYPE_NORMAL
- en: The above methods are mainly subject to $O(n^{3})$ time complexity or dependent
    on complex matrices [[73](#bib.bib73)]. It is discovered that orthogonal matrices
    $W\subseteq O(2n)$ with doubled hidden size, can substitute complex or unitary
    matrices in $\mathbb{C}^{n\times n}$. Inspired by the above discovery, [[73](#bib.bib73)]
    proposed to utilize Householder matrices to achieve parametrization of orthogonal
    transition matrices. As a result, complex matrices are unneeded and time complexity
    is reduced, while the effect is similar to the unitary constraint.
  prefs: []
  type: TYPE_NORMAL
- en: The norm-keeping property of orthogonal matrices may make ORNN have difficulty
    paying little attention to extraneous information [[74](#bib.bib74)]. To relieve
    this problem, Jing et al. [[74](#bib.bib74)] put forward the gated orthogonal
    recurrent unit (GORU) to be unconcerned with irrelevant or noise information while
    learning long-term dependencies. By adding the gating mechanism, experiment results
    demonstrate that GORU outperforms the unitary RNN on natural language processing
    tasks such as question answering tasks, together with long-term dependency tasks
    such as denoising and copying tasks.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, uRNN [[14](#bib.bib14)] parameterizes the unitary hidden-to-hidden
    matrix by composing simple unitary matrices. However, the above parameterization
    cannot cover all $N\times N$ unitary matrices. To make up for that, full-capacity
    uRNN [[69](#bib.bib69)] is put forward. Unlike uRNN, expRNN [[13](#bib.bib13)]
    exploits the exponential map to achieve orthogonal constraints more easily. Furthermore,
    OMDSM innovatively uses re-parameterization to optimize DNN over multiple dependent
    Stiefel manifolds instead of one manifold [[71](#bib.bib71)]. Moreover, research
    has explored whether and how the hard orthogonal constraints on RNN can be relaxed
    [[72](#bib.bib72)]. By creatively introducing the householder matrix, the considerable
    time complexity of parameterizing unitary matrices can be mitigated [[73](#bib.bib73)].
    Last but not least, GORU [[74](#bib.bib74)] designs a forget gate, so that ORNN
    can pay little attention to extraneous information.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Geometric GNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: GNN can be used to construct a learning network based on irregular graphs. Each
    graph is represented by vertexes and edges, which describes the relationship between
    vertexes. GNN encodes vertexes as feature vectors and models edges as a relationship
    matrix between vertexes. In GNN, graph convolution is performed between the relationship
    matrix and the feature matrix. Therefore, GNN can take advantage of the graph
    structure and update the feature information of each vertex iteratively. Endowing
    Eucludiean GNN with hyperbolic geometry can make it superior in capturing graph
    structure [[52](#bib.bib52)]. Recently, plenty of geometric GNN research has investigated
    how to incorporate GNN with hyperbolic manifold to benefit from a neighborhood
    with a highly organized structure.
  prefs: []
  type: TYPE_NORMAL
- en: To make full use of the rich geometric information in the graph, geometry interaction
    learning (GIL) [[53](#bib.bib53)] incorporates Euclidean space with hyperbolic
    geometry by exponential and logarithmic transformations. Moreover, learnable message
    passing parameters are optimized on the $M\ddot{o}bius$ manifold. To allow each
    node to determinate the importance of each geometry space freely, the GIL framework
    employs a flexible dual-space to model both low-dimensional regular data and complex
    hierarchical structures. A broad spectrum of experiments show that the GIL method
    is adaptative to node classification and link prediction tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Observing that GCN cannot cope with changes in static structure information,
    Liu et al. [[75](#bib.bib75)] put forward a manifold regularized dynamic graph
    convolutional network (MRDGCN), which integrated manifold regularization into
    GCN to model dynamic structure information. MRDGCN automatically updates the structure
    information before convengence, which makes up for GCN’s inability to remain optimal
    in pace with the learning process. Considerable comparative experiments on human
    activity datasets and citation network datasets evaluate that MRDGCN outperforms
    GCN and other semi-supervised learning methods.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Geometric Optimization for Other Deep Learning Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Robust Time Series Prediction. Considering that noises and outliers are inevitable
    and important for system modeling, Feng et al. [[76](#bib.bib76)] put forward
    a robust manifold broad learning system (RM-BLS) for time series prediction with
    large-scale noisy disturbations. RM-BLS applies low-rank constraint so that features
    spoiled by perturbations can be abandoned by feature selection. Furthermore, RM-BLS
    can also abandon features that are not satisfied to low-dimensional manifold embedding.
    In addition to the low-rank manifold, [[76](#bib.bib76)] also considers Stiefel
    manifold optimization and satisfies orthogonal constraints by Cayley transformation
    and curvilinear search algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: Medical Reconstruction. Geometric optimization have played an essential role
    in the medical field, such as magnetic resonance imaging (MRI) for cardiac diagnosis.
    Dynamic MR can be optimized on a low-rank tensor manifold [[77](#bib.bib77)] to
    seize the powerful temporal connection between dynamic signals. Moreover, the
    iterative reconstruction process is flattened to a neural network for acceleration,
    called dubbed Manifold-Net. To recover free breathing and ungated cardiac MRI
    data, Biswas et al. [[78](#bib.bib78)] creatively combined CNN with smoothness
    regularization on manifolds (SToRM) prior. The Laplacian matrix $L$ in SToRM $tr(X^{T}LX)$
    is defined on the manifold to model similarities between data beyond the ambient
    space. To utilize the manifold structure and patient-specific information, data
    denoizing based on CNN and SToRM together with conjugate gradients (CG) step take
    place alternatively. Experiments confirm that combining CNN with SToRM leads to
    a fast and high quality reconstruction of MRI data even when the down sampling
    frequency is less than 8.2s of acquisition time per slice.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer Learning. To maximize the utilization of finite computing resources,
    transfer learning aims to reuse the neural network, which is trained for task
    $A$, to address a similar task $B$. Knowledge distillation (KD) is intended to
    transfer model knowledge from a well-trained model (teacher) to a compact model
    (student) with soft labels. Zhang et al. [[79](#bib.bib79)] devised an end-to-end
    deep manifold-to-manifold transforming network (DMT-Net) for discriminative feature
    learning. However, reconstructing a more discriminative SPD manifold from the
    original one is challenging. DMT-Net designs a local SPD convolutional layer and
    the non-linear SPD activation layer to deal with it. Huang et al. [[80](#bib.bib80)]
    designed a manifold-to-manifold transformation matrix $W$ and constrained the
    optimization to reside on the SPD manifold. Moreover, the intra-class and inter-class
    dissimilarity graphs are built under $W$. Hence, they can represent local geometry
    structures and learn the discriminative feature of SPD data.
  prefs: []
  type: TYPE_NORMAL
- en: Optimal Transport. Optimal transport aims to measure the distance between two
    probability distributions by using transport plan $\Gamma$ and cost matrix $C$
    , i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\min_{\Gamma\in\Pi(\mu_{1},\mu_{2})}trace(\Gamma^{T}C),$ |  | (61) |'
  prefs: []
  type: TYPE_TB
- en: where $\Pi(\mu_{1},\mu_{2})$ consists of joint distributions with marginals
    $\mu_{1}$ and $\mu_{2}$. Supposing $\mu_{1}$ has $m$ points and $\mu_{2}$ has
    $n$ points, the size of both $\Gamma$ and $C$ is $m\times n$. There are works
    that have explored the application of geometric optimization in optimal transport
    problems [[81](#bib.bib81)]. By using the Riemannian gradient descent (RGD) algorithm,
    [[82](#bib.bib82)] explored how to convert optimal transport problems with different
    regularizations to the optimization problem on the coupling matrix manifold (CMM).
    To clarify the geometry optimization process, [[82](#bib.bib82)] took classic
    optimal transport problems (e.g., the entropy-regularized [[83](#bib.bib83)] and
    power-regularized optimal transport problems [[84](#bib.bib84)]) as an example.
    Observing that the constrained set $\Pi(\mu_{1},\mu_{2})$ has a differentiable
    manifold structure, [[85](#bib.bib85)] and [[86](#bib.bib86)] solved the optimal
    transport problem on a generalized doubly stochastic manifold, broadening the
    application of manifold geometry in non-linear optimal transport problems. In
    addition to general problems, [[86](#bib.bib86)] discusses how to adapt the above
    geometric optimization framework to particular ones, such as problems with sparse
    optimal transport map and problems of how to learn multiple transport plans simultaneously.
  prefs: []
  type: TYPE_NORMAL
- en: Robots. Bayesian optimization is an important technology for robots since it
    is effective in solving optimization problems such as controller tuning, policy
    adaptation, and robot design. Bayesian optimization is based on the Gaussian Process
    that relies on domain knowledge exploration. Therefore, geometry-aware Bayesian
    optimization emerges as a promising paradigm that can incorporate domain geometry
    into the optimization algorithm. There are many commonly used kernels in Gaussian
    Process, among which Matérn kernel is used to study geometry-aware Gaussian process
    and Bayesian optimization. Euclidean Matérn kernel is defined as follows,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $K(x,x^{\prime})=exp(-\frac{\&#124;x-x^{\prime}\&#124;^{2}}{2\sigma^{2}}),$
    |  | (62) |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma$ is a free parameter. Matérn kernel is a commonly used kernel
    function when constructing stationary Gaussian process. Borovitskiy et al. [[87](#bib.bib87)]
    pointed out that generalizing the Matérn kernel to the Riemannian manifold merely
    by replacing Euclidean norms $\|x-x^{\prime}\|^{2}$ with geodesic distances $d_{g}(x-x^{\prime})$
    could not produce a well-defined kernel function. To construct the Riemannian
    Matérn kernel defined by stochastic partial differential equations, Borovitskiy
    et al. proposed to obtain Laplace–Beltrami eigenpairs for the specific manifold
    and approximate the infinite sum, which forms the basis for geometry-aware Bayesian
    optimization on robotics. However, the above method suffers from two problems
    [[88](#bib.bib88)], i.e., i) the amount of computation increases exponentially
    with the manifold dimension; and ii) such method is inapplicable to non-compact
    manifolds. To address these problems, Jaquier et al. [[88](#bib.bib88)] observed
    a general expression of Matérn kernels, which is helpful to generalize them to
    the torus and sphere manifold. More importantly, Matérn kernels can be generalized
    to non-compact manifolds (e.g., SPD matrix manifold and the Hyperbolic space)
    by using the general expression.
  prefs: []
  type: TYPE_NORMAL
- en: Continual learning. Continual learning aims to remember and use the experience
    of previous tasks to learn new tasks, which raises requirements for the memory
    ability of neural networks. Chaudhry et al. [[89](#bib.bib89)] proposed to achieve
    the purpose of continual learning on the low-rank orthogonal manifold. The core
    idea of this method is to project the gradient into disjoint low-rank orthogonal
    subspace by introducing task-specific projection matrix in the last second layer,
    which can make the gradient between different tasks orthogonal and alleviate catastrophic
    forgetting. The concept of gradient orthogonality was first proposed in [[90](#bib.bib90)].
    The essential reason for catastrophic forgetting is that learning new tasks will
    affect the parameters learned on the old tasks. Updating parameters in the direction
    orthogonal to the gradient of the old tasks can not only learn new tasks but also
    keep the loss of the old tasks, which alleviates catastrophic forgetting. In the
    deep neural network, the chain derivation process can be approximately regarded
    as the linear transformation of the gradient, which will destroy the orthogonality
    of the gradient of the earlier layers and lead to catastrophic forgetting. To
    ensure the orthogonality of the gradient between different tasks, [[89](#bib.bib89)]
    constrains parameters on the Stiefel manifold, making this linear transformation
    an orthogonal transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Toolbox
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The success of the Tensorflow platform and PyTorch framework in deep learning
    shows that toolboxes can conveniently help build neural networks. There are valuable
    toolboxes designed for quickly setting up manifolds optimization. Manopt [[91](#bib.bib91)],
    Pymanopt [[92](#bib.bib92)], McTorch [[93](#bib.bib93)], and Geomstats [[94](#bib.bib94)]
    are classic toolboxes that implement manifold geometries and optimization algorithms.
    Moreover, they are user-friendly and time-saving. Table [2](#S5.T2 "Table 2 ‣
    5 Toolbox ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold") compares these toolboxes from the aspect of applicable
    manifolds and geometry operations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Manopt, which is built on Matlab, is a helpful tool to handle a variety of
    geometry constraints (e.g., different manifold structures introduced in  [2.3](#S2.SS3
    "2.3 Manifold Examples ‣ 2 Geometric Optimization Theory ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")).
    A Riemannian optimization in Manopt [[91](#bib.bib91)] is designed as a problem
    including manifold structures that the search space is confined to. The cost function,
    or optimization object, is included in the above optimization problem as well.
    If needed, a problem structure can also cover derivatives of the objective function.
    In Manopt, solvers are functions that give a general implementation to Riemannian
    optimization algorithms, including steepest-descent, conjugate-gradient, and Riemannian
    trust-regions algorithms. Since solvers in Manopt is designed to minimize the
    cost function, the cost function should be multiplied by a negative one if it
    is a maximization problem.'
  prefs: []
  type: TYPE_NORMAL
- en: Pymanopt [[92](#bib.bib92)] extends Manopt to python. Similar to the usage of
    Manopt in Matlab, a Riemannian optimization in Pymanopt should be initialized
    with a predefined manifold and cost function. Equipped with different solvers,
    the optimization process and result can be diverse. Pymanopt covers all sorts
    of smooth manifolds such as the oblique manifold, sphere manifold, and Graßmann
    manifold. Numerable optimization algorithms are included as solvers, for instance,
    trust-regions, conjugate-gradient, and steepest-descent algorithms are contained
    by Pymanopt.
  prefs: []
  type: TYPE_NORMAL
- en: Manopt and Pymanopt are limited to shallow learning optimizations and are not
    applicable to deep learning optimizations. To fill the deficiency of Manopt and
    Pymanopt, McTorch has been implemented by extending Pytorch [[93](#bib.bib93)],
    a handy framework for deep learning. As a result, it implements a general solution
    for deep learning optimizations on the manifold. Unlike Manopt and Pymanopt, Riemannian
    optimization in McTorch does not need to define problems, manifolds, and solvers.
    Similar to Pytorch, Riemannian optimization in McTorch only needs to define modules
    and optimizers such as Adam. Network modules inherited from $torch.nn.module$
    initialize layers with manifolds and forward functions.
  prefs: []
  type: TYPE_NORMAL
- en: Geoopt [[95](#bib.bib95)], which is implemented on top of Pytorch, has a cheaper
    infrastructure cost than McTorch. Extended from $torch.nn.Module.parameters$,
    Geoopt supports tensors and parameters on the manifold. Moreover, Geoopt provides
    Riemannian optimizers, for instance, $RiemannianSGD$ and $RiemannianAdam$ are
    available and inherited from $torch.optim.SGD$ and $torch.optim.Adam$, respectively
    [[95](#bib.bib95)].
  prefs: []
  type: TYPE_NORMAL
- en: Another toolbox, Geomstats, is composed of two core modules, i.e., geometry
    and learning [[94](#bib.bib94)]. The former implements Riemannian metrics, including
    geodesic distance. The latter implements statistics and learning algorithms inherited
    from Scikit-Learn classes such as *K-Means* and PCA. Compared with Geomstats,
    other toolboxes mentioned are less modular and lack statistical learning algorithms.
    Taking clustering, one of the classic statistical learning problems, as an example,
    Geomstats encapsulates the class *Online K-Means* with the parameter *metric*.
    To perform clustering operation, users only need to initialize the Riemannian
    metric and call *fit* function of class *Online K-Means* as they do in Scikit-Learn,
    which is easy and convenient.
  prefs: []
  type: TYPE_NORMAL
- en: TheanoGeometry [[96](#bib.bib96)] uses Theano, a python-based and research-oriented
    framework, to implement differential geometry and non-linear statistics problems.
    TheanoGeometry outperforms other manifold toolboxes since it can handle symbolic
    calculations. Thus, Theano code can be generated from symbolic expression directly,
    where non-linear symbolic statistics can be optimized with a trivial amount of
    code. TheanoGeometry goes further beyond efficient symbolic computation. It implements
    Riemannian geometry such as geodesic equations, parallel transport, and curvature
    with automatic differentiation features [[97](#bib.bib97)].
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Toolboxes Comparison in Terms of Manifolds and Geometry'
  prefs: []
  type: TYPE_NORMAL
- en: '| Toolboxes | Manifolds | Geometry |'
  prefs: []
  type: TYPE_TB
- en: '| Manopt [[91](#bib.bib91)] | Euclidean manifold, symmetric matrices, sphere,
    complex circle, SO (n), Stiefel, Graßmannian, oblique manifold, SPD (n), fixed-rank
    PSD matrices | Exponential and logarithmic maps, tangent space projector, retraction,
    vector transport, egrad2rgrad, ehess2rhess, vector, metric, distance, norm |'
  prefs: []
  type: TYPE_TB
- en: '| Pymanopt [[92](#bib.bib92)] | Same as Manopt | Same as Manopt |'
  prefs: []
  type: TYPE_TB
- en: '| McTorch [[93](#bib.bib93)] | Stiefel, SPD (n) | Same as Manopt |'
  prefs: []
  type: TYPE_TB
- en: '| Geoopt [[95](#bib.bib95)] | Euclidean manifold, sphere, Stiefel, Poincaré
    ball | Same as Manopt |'
  prefs: []
  type: TYPE_TB
- en: '| Geomstats [[94](#bib.bib94)] | Euclidean manifold, Minkowski and hyperbolic
    space, sphere, SO (n), SE (n), GL (n), Stiefel, Graßmannian, SPD (n), discretized
    curves, Landmarks | Exponential and logarithmic maps, parallel transport, inner
    product, distance, norm, Levi-Civita conne- ction, geodesics, invariant metrics
    |'
  prefs: []
  type: TYPE_TB
- en: '| TheanoGeometry [[96](#bib.bib96)] | Sphere, ellipsoid, SPD (n), Landmarks,
    GL (n), SO (n), SE (n) | Inner product, exponential and logarithmic maps, parallel
    transport, Christoffel symbols, Riemann, Ricci and scalar curvature, geodesics,
    Fréchet mean |'
  prefs: []
  type: TYPE_TB
- en: 6 Performance Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold"),
     [5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization
    for Deep Learning: From Euclidean Space to Riemannian Manifold"), [6](#S6.T6 "Table
    6 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold"), [7](#S6.T7 "Table 7 ‣ 6 Performance
    Evaluation ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean
    Space to Riemannian Manifold"), [8](#S6.T8 "Table 8 ‣ 6 Performance Evaluation
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold") compare the performance of aforementioned geometric optimization
    methods on various visual tasks (e.g., character recognition, emotion recognition,
    act recognition, and scene recognition tasks). Each image dataset used in different
    visual tasks is summarized in Table [3](#S6.T3 "Table 3 ‣ 6 Performance Evaluation
    ‣ A Survey of Geometric Optimization for Deep Learning: From Euclidean Space to
    Riemannian Manifold").'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Datasets for Different Visual Tasks'
  prefs: []
  type: TYPE_NORMAL
- en: '| Vision Task | Dataset | Total Samples | Categories | Image Size |'
  prefs: []
  type: TYPE_TB
- en: '| Character Recognition | MNIST[[98](#bib.bib98)] | 70000 | 10 | 32 $\times$
    32 |'
  prefs: []
  type: TYPE_TB
- en: '| Emotion Recognition | AFEW [[99](#bib.bib99)] | 1345 | 7 | 400 $\times$ 400
    |'
  prefs: []
  type: TYPE_TB
- en: '| NABU3DFE [[100](#bib.bib100)] | 2500 | 6 | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Bosphorus dataset [[101](#bib.bib101)] | 4666 | 6 | NA |'
  prefs: []
  type: TYPE_TB
- en: '| Action Recognition | HDM05 [[98](#bib.bib98)] | 18000 | 130 | 93 $\times$93
    |'
  prefs: []
  type: TYPE_TB
- en: '| Face Verification | PaSC [[102](#bib.bib102)] | 12529 | NA | $401\times 401$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Scene Recognition | Scene15 [[103](#bib.bib103)] | NA | 15 | $300\times 250$
    |'
  prefs: []
  type: TYPE_TB
- en: '| Eight sports event categories[[104](#bib.bib104)] | NA | 8 | NA |'
  prefs: []
  type: TYPE_TB
- en: '| SUN [[105](#bib.bib105), [106](#bib.bib106)] | 899 | NA | NA |'
  prefs: []
  type: TYPE_TB
- en: 'Table [4](#S6.T4 "Table 4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    shows that GORU [[74](#bib.bib74)] outperforms other ORNNs on the MNIST dataset.
    GORU adds a forget gate, which enables ORNN to filter out irrelevant information.
    Taking advantage of the surjective exponential map, expRNN [[13](#bib.bib13)]
    realizes orthogonal parameterization with a more straightforward way. Unlike expRNN,
    uRNN [[14](#bib.bib14)] uses simple unitary matrices to construct the unitary
    hidden-to-hidden matrix. However, such matrix construction method fails to represent
    all $N\times N$ unitary matrices. Therefore, Scott Wisdom et al. [[69](#bib.bib69)]
    proposed full-capacity uRNN to overcome that bottleneck of uRNN. Using regularization
    terms to realize orthogonal parameterization, soRNN [[72](#bib.bib72)] explores
    the effect of soft orthogonal constraints on RNN. ORNN [[73](#bib.bib73)] exploits
    the householder matrix to enforce an orthogonal constraint on RNN, which mitigates
    the considerable time complexity of unitary matrices. Table [4](#S6.T4 "Table
    4 ‣ 6 Performance Evaluation ‣ A Survey of Geometric Optimization for Deep Learning:
    From Euclidean Space to Riemannian Manifold") shows that combining the forget
    gate, or noise filter, with ORNN improves the performance of ORNN.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 4: Comparison Results of Character Recognition'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| MNIST [[98](#bib.bib98)] | uRNN [[14](#bib.bib14)] | 97.6% |'
  prefs: []
  type: TYPE_TB
- en: '| full-capacity uRNN [[69](#bib.bib69)] | 96.9% |'
  prefs: []
  type: TYPE_TB
- en: '| expRNN [[13](#bib.bib13)] | 98.7% |'
  prefs: []
  type: TYPE_TB
- en: '| soRNN [[72](#bib.bib72)] | 97.3% |'
  prefs: []
  type: TYPE_TB
- en: '| ORNN [[73](#bib.bib73)] | 97.2% |'
  prefs: []
  type: TYPE_TB
- en: '| GORU [[74](#bib.bib74)] | 98.9% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 5: Comparison Results of Emotion Recognition'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| AFEW [[99](#bib.bib99)] | STM-ExpLet [[107](#bib.bib107)] | 31.73% |'
  prefs: []
  type: TYPE_TB
- en: '| RSR-SPDML [[32](#bib.bib32)] | 30.12% |'
  prefs: []
  type: TYPE_TB
- en: '| DeepO2P [[108](#bib.bib108)] | 28.54% |'
  prefs: []
  type: TYPE_TB
- en: '| DCC [[109](#bib.bib109)] | 25.78% |'
  prefs: []
  type: TYPE_TB
- en: '| GDA [[56](#bib.bib56)] | 29.11% |'
  prefs: []
  type: TYPE_TB
- en: '| GGDA [[56](#bib.bib56)] | 29.45% |'
  prefs: []
  type: TYPE_TB
- en: '| PML [[110](#bib.bib110)] | 28.98% |'
  prefs: []
  type: TYPE_TB
- en: '| SPDNet [[50](#bib.bib50)] | 34.23% |'
  prefs: []
  type: TYPE_TB
- en: '| GrNet [[51](#bib.bib51)] | 34.23% |'
  prefs: []
  type: TYPE_TB
- en: '| BU-3DFE [[100](#bib.bib100)] | Tree-PNN [[111](#bib.bib111)] | 93.23% |'
  prefs: []
  type: TYPE_TB
- en: '| Berretti et al. [[112](#bib.bib112)] | 77.53% |'
  prefs: []
  type: TYPE_TB
- en: '| Huynh et al. [[113](#bib.bib113)] | 92.73% |'
  prefs: []
  type: TYPE_TB
- en: '| Azazi et al. [[114](#bib.bib114)] | 85.71% |'
  prefs: []
  type: TYPE_TB
- en: '| Hariri et al. [[59](#bib.bib59)] | 93.50% |'
  prefs: []
  type: TYPE_TB
- en: '|  | CSLBP [[115](#bib.bib115)] | 76.98 % |'
  prefs: []
  type: TYPE_TB
- en: '| CLBP [[116](#bib.bib116)] | 76.56% |'
  prefs: []
  type: TYPE_TB
- en: '| Bosphorus | ZernikeMoments [[117](#bib.bib117)] | 60.53% |'
  prefs: []
  type: TYPE_TB
- en: '|   [[101](#bib.bib101)] | Azazi et al. [[114](#bib.bib114)] | 84.10% |'
  prefs: []
  type: TYPE_TB
- en: '|  | Hariri et al. [[59](#bib.bib59)] | 90.01% |'
  prefs: []
  type: TYPE_TB
- en: 'Table [5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    shows that SPDNet [[50](#bib.bib50)] and GrNet [[51](#bib.bib51)] can achieve
    better classification results than state-of-the-art methods on AFEW dataset [[99](#bib.bib99)].
    The following methods for comparison are shallow learning methods applying manifold
    structure: Expressionlets on Spatio-Temporal Manifold (STM-ExpLet) [[107](#bib.bib107)],
    Riemannian Sparse Representation combining with Manifold Learning on the manifold
    of SPD matrices (RSR-SPDML) [[32](#bib.bib32)], Discriminative Canonical Correlations
    (DCC) [[109](#bib.bib109)], Graßmann Discriminant Analysis (GDA) [[56](#bib.bib56)],
    Grassmannian Graph-Embedding Discriminant Analysis (GGDA) [[118](#bib.bib118)],
    and Projection Metric Learning (PML) [[110](#bib.bib110)]. Deep Second-order Pooling
    (DeepO2P) [[108](#bib.bib108)] is a traditional CNN model using the standard optimization
    method. SPDNet exploits the Stiefel manifold parameterization by BiMap layers
    and introduces non-linearity into the network by ReEig layers. Experiments prove
    that using the manifold geometry in deep learning optimization can improve network
    performance. The LogEig layer is crucial to Riemannian computing and contributes
    to the emotion classification success of SPDNet. The success of GrNet shows that
    optimizing on the Graßmann manifold and building a geometry-aware deep learning
    network is significant for learning representative features and classifying emotions
    with a relatively high level of accuracy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [5](#S6.T5 "Table 5 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    presents that the manifold-based classification method proposed by Hariri et al.
    [[59](#bib.bib59)] achieves the highest precision on BU-3DFE and Bosphorus datasets.
    Hariri et al. used a Graph-Matching kernel and classified facial expression data
    with SPD covariance descriptors. It outperforms Tree-PNN [[111](#bib.bib111)]
    and XP Huynh [[113](#bib.bib113)] on the BU-3DFE dataset by a narrow margin, and
    the latter two methods use traditional CNN. The manifold-based method proposed
    by Hariri et al. greatly exceeds the methods proposed by Stefano Berretti [[112](#bib.bib112)]
    and Amal Azazi [[114](#bib.bib114)] by approximately 15% and 8% on BU-3DFE dataset.
    In particular, the latter two methods apply SIFT and Speed Up Robust Features
    descriptors. On the Bosphorus dataset, the classification accuracy of Hariri et
    al.’s method [[59](#bib.bib59)] is almost far higher than all state-of-the-art
    methods. For example, it is even 30% better than the ZernikeMoments [[117](#bib.bib117)].
    The low-accuracy methods use local features rather than SPD covariance matrices.
    Overall, these results indicate that using geometry constraints is vital for feature
    representation and emotion recognition.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [6](#S6.T6 "Table 6 ‣ 6 Performance Evaluation ‣ A Survey of Geometric
    Optimization for Deep Learning: From Euclidean Space to Riemannian Manifold")
    shows that SPDNet achieves the highest accuracy on the action recognition task,
    followed by GrNet. As Table [7](#S6.T7 "Table 7 ‣ 6 Performance Evaluation ‣ A
    Survey of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold") shows, SPDNet and GrNet outperform state-of-the-art methods on the
    face recognition task. The eigenvalue decomposition in SPDNet introduces non-linearity
    and the QR decomposition in GrNet performs re-orthonormalization, both of which
    contribute to the classification accuracy. Therefore, using matrix decomposition
    is vital for exploring manifold constrained parameters. The success of the deep
    manifold network on the action recognition and face recognition task shows that
    optimizing deep learning on the manifold helps learn favorable features and classify
    human actions better.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6: Comparison Results of Action Recognition'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| HDM05 [[98](#bib.bib98)] | RSR-SPDML [[32](#bib.bib32)] | 48.01% |'
  prefs: []
  type: TYPE_TB
- en: '| DCC [[109](#bib.bib109)] | 41.74% |'
  prefs: []
  type: TYPE_TB
- en: '| GDA [[56](#bib.bib56)] | 46.25% |'
  prefs: []
  type: TYPE_TB
- en: '| GGDA [[118](#bib.bib118)] | 46.87% |'
  prefs: []
  type: TYPE_TB
- en: '| PML [[110](#bib.bib110)] | 47.25% |'
  prefs: []
  type: TYPE_TB
- en: '| SPDNet [[50](#bib.bib50)] | 61.45% |'
  prefs: []
  type: TYPE_TB
- en: '| GrNet [[51](#bib.bib51)] | 59.23% |'
  prefs: []
  type: TYPE_TB
- en: 'Table 7: Comparison Results of Face Recognition'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Method | PaSC1 [[102](#bib.bib102)] | PaSC2 [[102](#bib.bib102)] |'
  prefs: []
  type: TYPE_TB
- en: '| VGGDeepFace [[119](#bib.bib119)] | 78.82% | 68.24% |'
  prefs: []
  type: TYPE_TB
- en: '| DeepO2P [[108](#bib.bib108)] | 68.76% | 60.14% |'
  prefs: []
  type: TYPE_TB
- en: '| DCC [[109](#bib.bib109)] | 75.83% | 67.04% |'
  prefs: []
  type: TYPE_TB
- en: '| GDA [[56](#bib.bib56)] | 71.38% | 67.49% |'
  prefs: []
  type: TYPE_TB
- en: '| GGDA [[118](#bib.bib118)] | 66.71% | 68.41% |'
  prefs: []
  type: TYPE_TB
- en: '| PML [[110](#bib.bib110)] | 73.45% | 68.32% |'
  prefs: []
  type: TYPE_TB
- en: '| SPDNet [[50](#bib.bib50)] | 80.12% | 72.83% |'
  prefs: []
  type: TYPE_TB
- en: '| GrNet [[51](#bib.bib51)] | 80.52% | 72.76% |'
  prefs: []
  type: TYPE_TB
- en: 'As shown in Table [8](#S6.T8 "Table 8 ‣ 6 Performance Evaluation ‣ A Survey
    of Geometric Optimization for Deep Learning: From Euclidean Space to Riemannian
    Manifold"), Scene Recognition by Manifold Regularized Deep Learning Architecture
    (SRMR) [[60](#bib.bib60)] outperforms state-of-the-art non-manifold methods on
    all three scene recognition datasets. Lazebnik et al. [[120](#bib.bib120)] partitioned
    images into fine subregions for image matching. Dixit et al. [[121](#bib.bib121)]
    formulated Bayesian adaptation for scene image classification. Kwitt et al. [[122](#bib.bib122)]
    recognized scene images on the statistical (semantic) manifold. From the perspective
    of information geometry, they can consider the parameter vectors as Riemannian
    manifolds. Goh et al. [[123](#bib.bib123)] used SIFT descriptors and represented
    vectorially for image recognition. Li et al. [[104](#bib.bib104)] interpreted
    the semantic components of images. Wu and Rehg [[124](#bib.bib124)] used the Histogram
    Intersection Kernel (HIK) for sports game classification. Donahue et al. [[124](#bib.bib124)]
    used extracted features for novel generic tasks. SRMR’s incredible success on
    scene recognition tasks shows that manifold regularizations are significant for
    improving the classification accuracy of deep learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 8: Comparison Results of Scene Recognition'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Method | Accuracy |'
  prefs: []
  type: TYPE_TB
- en: '| Scene15 [[103](#bib.bib103)] | Lazebnik et al. [[120](#bib.bib120)] | 81.2%
    |'
  prefs: []
  type: TYPE_TB
- en: '| Dixit et al. [[121](#bib.bib121)] | 82.3% |'
  prefs: []
  type: TYPE_TB
- en: '| Kwitt et al. [[122](#bib.bib122)] | 85.4% |'
  prefs: []
  type: TYPE_TB
- en: '| Goh et al. [[123](#bib.bib123)] | 85.4% |'
  prefs: []
  type: TYPE_TB
- en: '| SRMR [[60](#bib.bib60)] | 86.9% |'
  prefs: []
  type: TYPE_TB
- en: '| Eight sports event categories[[104](#bib.bib104)] | Li et al. [[104](#bib.bib104)]
    | 73.4% |'
  prefs: []
  type: TYPE_TB
- en: '| Kwitt et al. [[122](#bib.bib122)] | 83.0% |'
  prefs: []
  type: TYPE_TB
- en: '| Wu and Rehg [[124](#bib.bib124)] | 84.3% |'
  prefs: []
  type: TYPE_TB
- en: '| SRMR [[60](#bib.bib60)] | 86.1% |'
  prefs: []
  type: TYPE_TB
- en: '|  | Xiao et al. [[105](#bib.bib105)] | 27.2% |'
  prefs: []
  type: TYPE_TB
- en: '| SUN | Kwitt et al. [[122](#bib.bib122)] | 28.9% |'
  prefs: []
  type: TYPE_TB
- en: '| [[105](#bib.bib105)] | Donahue et al. [[125](#bib.bib125)] | 30.14% |'
  prefs: []
  type: TYPE_TB
- en: '|  | SRMR [[60](#bib.bib60)] | 30.3% |'
  prefs: []
  type: TYPE_TB
- en: 'Experimental results vary with different network architecture settings for
    the same manifold constrained method. For example, SPDNet [[50](#bib.bib50)] has
    four different architecture configurations: i) SPDNet-0BiRe without using blocks
    of BiMap/ReEig, ii) SPDNet-1BiRe using $1$ block of BiMap/ReEig, iii) SPDNet-2BiRe
    using $2$ blocks of BiMap/ReEig, and iv) SPDNet-3BiRe using $3$ blocks of BiMap/ReEig.
    GrNet [[51](#bib.bib51)] has three different configurations: i) GrNet-0Block without
    using blocks of Projection-Pooling, ii) GrNet-1Block using $1$ block of Projection-Pooling,
    and iii) GrNet-2Block using $2$ blocks of Projection-Pooling. These methods studied
    how different architecture settings affected classification accuracy. Note that
    our article follows the raw settings reported from corresponding articles. On
    that account, this article did not present classification accuracy under different
    architecture configurations.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions and Future Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this article, a survey on recent advances in applying geometric optimization
    to deep learning is presented. This article reviewed progress of optimizing deep
    learning networks on manifolds according to the classification of deep learning
    backbones (e.g., CNN, RNN, and GNN). In particular, this article discussed the
    theory and toolboxes for geometric optimization. Although geometric optimization
    brings various advantages to deep learning methods, it still suffers from the
    following challenges.
  prefs: []
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Dataset-Oriented Geometric Optimization. Various methods (e.g., uRNN [[14](#bib.bib14)]
    and Cheap Orthogonal Constraints in Neural Networks [[13](#bib.bib13)]) utilize
    small image datasets such as MNIST handwritten digits to validate the effectiveness
    of geometric optimization. Whether geometric optimization can achieve good performance
    on enormous and complicated datasets such as Penn Tree Bank (PTB) needs further
    research. This prompts researchers to use more challenging datasets to verify
    the performance of deep learning techniques after applying geometric optimization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model-Oriented Geometric Optimization. Although optimizing deep learning networks
    such as CNN and RNN on the Riemannian manifold has been proven successful, geometric
    optimization has not been applied to all deep learning methods. For example, there
    is a lack of research in optimizing reinforcement learning and federated learning
    on manifolds, which is crucial in automatic control and privacy protection. This
    forces researchers to further explore the potential and benefit of optimizing
    more deep learning networks from a geometric perspective.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '-'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Manifold-Oriented Geometric Optimization. Manifold geometry plays an important
    role in geometric optimization and different manifolds have different applications.
    For instance, the orthogonal manifold can be used to alleviate feature redundancy
    and oblique manifold can be utilized for optimizing dictionary learning. However,
    applications of certain manifolds such as centered matrix manifold remain blank
    in the literature. This motivates researchers to exploit and use manifold structures
    for geometric optimization applications as much as possible.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This article demonstrated that geometric optimization can grasp advantage of
    the geometry information of search space, speed up the optimization process, and
    mitigate gradient explosion and vanishing problems. However, considering unexplored
    deep learning methods such as reinforcement learning, together with unused manifold
    structures such as centered matrix manifold, it is still a huge challenge to push
    the boundaries of geometric optimization in deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] Ian Goodfellow, Yoshua Bengio, Aaron Courville, and Yoshua Bengio. Deep
    learning. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Raphael JL Townshend, Stephan Eismann, Andrew M Watkins, Ramya Rangan,
    Maria Karelina, Rhiju Das, and Ron O Dror. Geometric deep learning of rna structure.
    Science, 373(6558):1047–1051, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional
    neural networks. In International Conference on Machine Learning (ICML), pages
    6105–6114, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] Zhen He, Shaobing Gao, Liang Xiao, Daxue Liu, Hangen He, and David Barber.
    Wider and deeper, cheaper and faster: Tensorized lstms for sequence learning.
    In Advances in Neural Information Processing Systems, pages 1–11, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] David Rolnick and Max Tegmark. The power of deeper networks for expressing
    natural functions. In International Conference on Learning Representations (ICLR),
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] Jiayun Wang, Yubei Chen, Rudrasis Chakraborty, and Stella X Yu. Orthogonal
    convolutional neural networks. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 11505–11515, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] M. Storath and A. Weinmann. Variational regularization of inverse problems
    for manifold-valued data. Information and Inference: A Journal of the IMA, 10(1):195–230,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] P-A Absil, Robert Mahony, and Rodolphe Sepulchre. Optimization algorithms
    on matrix manifolds. 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] Nicolas Boumal. An introduction to optimization on smooth manifolds. Available
    online, Aug 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] Jiang Hu, Xin Liu, Zai-Wen Wen, and Ya-Xiang Yuan. A brief introduction
    to manifold optimization. Journal of the Operations Research Society of China,
    8(2):199–248, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] Xian Wei. Learning Image and Video Representations Based on Sparsity Priors.
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Nitin Bansal, Xiaohan Chen, and Zhangyang Wang. Can we gain more from
    orthogonality regularizations in training deep cnns. In Proceedings of the International
    Conference on Neural Information Processing Systems, pages 4266–4276, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Mario Lezcano-Casado and David Martınez-Rubio. Cheap orthogonal constraints
    in neural networks: A simple parametrization of the orthogonal and unitary group.
    In International Conference on Machine Learning (ICML), pages 3794–3803, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent
    neural networks. In International Conference on Machine Learning (ICML), pages
    1120–1128, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] Yanhong Fei, Yingjie Liu, Xian Wei, and Mingsong Chen. O-vit: Orthogonal
    vision transformer. arXiv preprint arXiv:2201.12133, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] J. Zhou, M. N. Do, and J Kovačević. Ieee transactions on image processing
    1 special paraunitary matrices, cayley transform, and multidimensional orthogonal
    filter banks. IEEE Transactions on Image Processing, 14(6):760, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] P Rodríguez, J Gonzàlez, G. Cucurull, J. M. Gonfaus, and X. Roca. Regularizing
    cnns with locally constrained decorrelations. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Jorge Nocedal and Stephen Wright. Numerical optimization. 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] David G Luenberger, Yinyu Ye, et al. Linear and nonlinear programming,
    volume 2. 1984.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] Daniel Gabay. Minimizing a differentiable function over a differential
    manifold. Journal of Optimization Theory and Applications, 37(2):177–219, 1982.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] Roger W Brockett. Differential geometry and the design of gradient algorithms.
    In Proc. Symp. Pure Math., AMS, pages 69–92, 1993.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] Alan Edelman, Tomás A Arias, and Steven T Smith. The geometry of algorithms
    with orthogonality constraints. SIAM journal on Matrix Analysis and Applications,
    20(2):303–353, 1998.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] Simon Hawe, Matthias Seibert, and Martin Kleinsteuber. Separable dictionary
    learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR), pages 438–445, June 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] Xian Wei, Hao Shen, and Martin Kleinsteuber. Trace quotient meets sparsity:
    A method for learning low dimensional image representations. In Proceedings of
    the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5268–5277,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] P-A Absil, Christopher G Baker, and Kyle A Gallivan. Trust-region methods
    on riemannian manifolds. Foundations of Computational Mathematics, 7(3):303–330,
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] Jean-Pierre Dedieu, Pierre Priouret, and Gregorio Malajovich. Newton’s
    method on riemannian manifolds: covariant alpha theory. IMA Journal of Numerical
    Analysis, 23(3):395–419, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Xian Wei, Yuanxiang Li, Hao Shen, Fang Chen, Martin Kleinsteuber, and
    Zhongfeng Wang. Dynamical textures modeling via joint video dictionary learning.
    IEEE Transactions on Image Processing, 26(6):2929–2943, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Simon Alois Hawe. Learning sparse data models via geometric optimization
    with applications to image processing. PhD thesis, Technische Universität München,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] S. Kumar, Z. Mhammedi, and M. Harandi. Geometry aware constrained optimization
    techniques for deep learning. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 4460–4469, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] John M Lee. Smooth manifolds. In Introduction to Smooth Manifolds, pages
    1–31\. Springer, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] Diego Tosato, Michela Farenzena, Mauro Spera, Vittorio Murino, and Marco
    Cristani. Multi-class classification on riemannian manifolds for video surveillance.
    In ECCV, pages 378–391, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Mehrtash T Harandi, Mathieu Salzmann, and Richard Hartley. From manifold
    to manifold: Geometry-aware dimensionality reduction for spd matrices. In European
    Conference on Computer Vision (ECCV), pages 17–32, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Zhiwu Huang, Ruiping Wang, Shiguang Shan, Xianqiu Li, and Xilin Chen.
    Log-euclidean metric learning on symmetric positive definite manifold with application
    to image set classification. In Proceedings of International Conference on Machine
    Learning (ICMR), volume 37, pages 720–729, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Jieping Ye and Qi Li. Lda/qr: an efficient and effective dimension reduction
    algorithm and its theoretical foundation. Pattern Recognition, 37(4):851–854,
    2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] John NR Jeffers. Two case studies in the application of principal component
    analysis. Journal of the Royal Statistical Society: Series C (Applied Statistics),
    16(3):225–236, 1967.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] Svante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis.
    Chemometrics and Intelligent Laboratory Systems, 2(1-3):37–52, 1987.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] Hui Zou and Lingzhou Xue. A selective overview of sparse principal component
    analysis. Proceedings of the IEEE, 106(8):1311–1320, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] Frank Rehm, Frank Klawonn, and Rudolf Kruse. Mds polar: A new approach
    for dimension reduction to visualize high dimensional data. In International Symposium
    on Intelligent Data Analysis, pages 316–327, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] Andreas Buja, Deborah F Swayne, Michael L Littman, Nathaniel Dean, Heike
    Hofmann, and Lisha Chen. Data visualization with multidimensional scaling. Journal
    of Computational and Graphical Statistics, 17(2):444–472, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] Mingyu Fan, Hong Qiao, Bo Zhang, and Xiaoqin Zhang. Isometric multi-manifold
    learning for feature extraction. In International Conference on Data Mining, pages
    241–250. IEEE, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Yepeng Ni, Jianping Chai, Yan Wang, and Weidong Fang. A fast radio map
    construction method merging self-adaptive local linear embedding (lle) and graph-based
    label propagation in wlan fingerprint localization systems. Sensors, 20(3):767,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] Bo Li, Yan-Rui Li, and Xiao-Long Zhang. A survey on laplacian eigenmaps
    based manifold learning methods. Neurocomputing, 335:336–351, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] Rong Wang, Feiping Nie, Richang Hong, Xiaojun Chang, Xiaojun Yang, and
    Weizhong Yu. Fast and orthogonal locality preserving projections for dimensionality
    reduction. IEEE Transactions on Image Processing, 26(10):5019–5030, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Effrosini Kokiopoulou, Jie Chen, and Yousef Saad. Trace optimization and
    eigenproblems in dimension reduction methods. Numerical Linear Algebra with Applications,
    18(3):565–602, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] Albert Tarantola. Inverse problem theory and methods for model parameter
    estimation. 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Simon Hawe, Martin Kleinsteuber, and Klaus Diepold. Analysis operator
    learning and its application to image reconstruction. IEEE Transactions on Image
    Processing, 22(6):2138–2150, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] W. P. Krijnen. Positive loadings and factor correlations from positive
    covariance matrices. Psychometrika, 69(4):655–660, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Pavan Turaga and Rama Chellappa. Locally time-invariant models of human
    activities using trajectories on the grassmannian. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR), pages 2435–2441,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] M. Ozay and Takayuki Okatani. Optimization on submanifolds of convolution
    kernels in cnns. ArXiv, abs/1610.07008:arXiv–1610, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] Z. Huang and L. Gool. A riemannian network for spd matrix learning. In
    Proceedings of the 31st AAAI Conference on Artificial Intelligence (AAAI), pages
    2036–2042, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Z. Huang, Jiqing Wu, and L. Gool. Building deep networks on grassmann
    manifolds. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI),
    pages 3279–3286, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] Qi Liu, Maximilian Nickel, and Douwe Kiela. Hyperbolic graph neural networks.
    Advances in Neural Information Processing Systems, 32:8230–8241, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Shichao Zhu, Shirui Pan, Chuan Zhou, Jia Wu, Yanan Cao, and Bin Wang.
    Graph geometry interaction learning. In Advances in Neural Information Processing
    Systems, volume 33, pages 7548–7558, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] Rahul Chauhan, Kamal Kumar Ghanshala, and RC Joshi. Convolutional neural
    network (cnn) for image detection and recognition. In International Conference
    on Secure Cyber Computing and Communication (ICSCCC), pages 278–282\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] Aliasghar Mortazi and Ulas Bagci. Automatically designing cnn architectures
    for medical image segmentation. In International Workshop on Machine Learning
    in Medical Imaging, pages 98–106, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] Jihun Hamm and Daniel D Lee. Grassmann discriminant analysis: a unifying
    view on subspace-based learning. In Proceedings of the 25th International Conference
    on Machine Learning (ICML), pages 376–383, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] Lei Zhang, Xiantong Zhen, Ling Shao, and Jingkuan Song. Learning match
    kernels on grassmann manifolds for action recognition. IEEE Transactions on Image
    Processing, 28(1):205–215, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] Mengyi Liu, Ruiping Wang, Shaoxin Li, Shiguang Shan, Zhiwu Huang, and
    Xilin Chen. Combining multiple kernel methods on riemannian manifold for emotion
    recognition in the wild. In Proceedings of International Conference on multimodal
    interaction, pages 494–501, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Walid Hariri and Nadir Farah. Efficient graph-based kernel using covariance
    descriptors for 3d facial expression classification. In Proceedings of International
    Conference on Intelligent Systems and Pattern Recognition, pages 7–11, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Yuan Yuan, Lichao Mou, and Xiaoqiang Lu. Scene recognition by manifold
    regularized deep learning architecture. IEEE Transactions on Neural Networks and
    Learning Systems, 26(10):2222–2233, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Chaoqun Hong, Jun Yu, Jian Zhang, Xiongnan Jin, and Kyong-Ho Lee. Multimodal
    face-pose estimation with multitask manifold deep learning. IEEE Transactions
    on Industrial Informatics, 15(7):3952–3961, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] Jean-Michel Roufosse, Abhishek Sharma, and Maks Ovsjanikov. Unsupervised
    deep learning for structured shape matching. In Proceedings of the IEEE/CVF International
    Conference on Computer Vision (ICCV), pages 1617–1627, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Zhiwu Huang, Chengde Wan, Thomas Probst, and Luc Van Gool. Deep learning
    on lie groups for skeleton-based action recognition. In Proceedings of the IEEE
    Conference on Computer Vision and Pattern Recognition (CVPR), pages 6099–6108,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] Xin Chen, Jian Weng, Wei Lu, Jiaming Xu, and Jiasi Weng. Deep manifold
    learning combined with convolutional neural networks for action recognition. IEEE
    Transactions on Neural Networks and Learning Systems, 29(9):3938–3952, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] Razvan Pascanu, Çaglar Gülçehre, Kyunghyun Cho, and Yoshua Bengio. How
    to construct deep recurrent neural networks. ArXiv, abs/1312.6026:arXiv–1312,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] Xavier Glorot, Antoine Bordes, and Yoshua Bengio. Deep sparse rectifier
    neural networks. In Proceedings of International Conference on Artificial Intelligence
    and Statistics, pages 315–323, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] Vinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted
    boltzmann machines. In Proceedings of the International Conference on International
    Conference on Machine Learning, page 807–814, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] Sekitoshi Kanai, Yasuhiro Fujiwara, and Sotetsu Iwamura. Preventing gradient
    explosions in gated recurrent units. In Proceedings of the 31st International
    Conference on Neural Information Processing Systems, pages 435–444, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Scott Wisdom, Thomas Powers, John Hershey, Jonathan Le Roux, and Les Atlas.
    Full-capacity unitary recurrent neural networks. 10 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] Stephanie L Hyland and Gunnar Rätsch. Learning unitary operators with
    help from u(n). In Proceedings of the Thirty-First AAAI Conference on Artificial
    Intelligence, pages 2050–2058, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] Lei Huang, Xianglong Liu, Bo Lang, Adams Wei Yu, Yongliang Wang, and Bo Li.
    Orthogonal weight normalization: Solution to optimization over multiple dependent
    stiefel manifolds in deep neural networks. In Proceedings of the AAAI Conference
    on Artificial Intelligence (AAAI), pages 3271–3278, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] Eugene Vorontsov, Chiheb Trabelsi, Samuel Kadoury, and Chris Pal. On orthogonality
    and learning recurrent networks with long term dependencies. In International
    Conference on Machine Learning (ICML), pages 3570–3578, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey.
    Efficient orthogonal parametrisation of recurrent neural networks using householder
    reflections. In International Conference on Machine Learning (ICML), pages 2401–2409\.
    PMLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] Li Jing, Caglar Gulcehre, John Peurifoy, Yichen Shen, Max Tegmark, Marin
    Soljacic, and Yoshua Bengio. Gated orthogonal recurrent units: On learning to
    forget. Neural Computation, 31(4):765–783, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Weifeng Liu, Sichao Fu, Yicong Zhou, Zheng-Jun Zha, and Liqiang Nie. Human
    activity recognition by manifold regularization based dynamic graph convolutional
    networks. Neurocomputing, 444:217–225, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] Shoubo Feng, Weijie Ren, Min Han, and Yen Wei Chen. Robust manifold broad
    learning system for large-scale noisy chaotic time series prediction: A perturbation
    perspective. Neural Networks, 117:179–190, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] Ziwen Ke, Zhuoxu Cui, Wenqi Huang, Jing Cheng, Seng Jia, Haifeng Wang,
    Xin Liu, Hairong Zheng, Leslie Ying, Yanjie Zhu, and Dong Liang. Deep manifold
    learning for dynamic mr imaging. ArXiv, abs/2104.01102:arXiv–2104, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Sampurna Biswas, Hemant K Aggarwal, and Mathews Jacob. Dynamic mri using
    model-based deep learning and storm priors: Modl-storm. Magnetic Resonance in
    Medicine, 82(1):485–494, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] Tong Zhang, Wenming Zheng, Zhen Cui, and Chaolong Li. Deep manifold-to-manifold
    transforming network. In 2018 25th IEEE International Conference on Image Processing
    (ICIP), pages 4098–4102, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] Zhiwu Huang, Ruiping Wang, Xianqiu Li, Wenxian Liu, Shiguang Shan, Luc
    Van Gool, and Xilin Chen. Geometry-aware similarity learning on spd manifolds
    for visual recognition. IEEE Transactions on Circuits and Systems for Video Technology,
    28(10):2513–2523, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] L. Ambrosio, Italien, and Scuola Normale Superiore). A survey on monge’s
    optimal transport problem. 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] Dai Shi, Junbin Gao, Xia Hong, S. T. Boris Choy, and Zhiyong Wang. Coupling
    matrix manifolds assisted optimization for optimal transport problems. Machine
    Learning, 110(3):533–558, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] B. K. Abid and R. M. Gower. Greedy stochastic algorithms for entropy-regularized
    optimal transport problems. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Arnaud Dessein, Nicolas Papadakis, and Jean Luc Rouas. Regularized optimal
    transport and the rot mover’s distance. Journal of Machine Learning Research,
    19, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] B. Mishra, Ntvs Dev, H. Kasai, and P. Jawanpuria. Manifold optimization
    for optimal transport. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] Bamdev Mishra, N T V Satyadev, Hiroyuki Kasai, and Pratik Jawanpuria.
    Manifold optimization for non-linear optimal transport problems. 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Viacheslav Borovitskiy, Alexander Terenin, Peter Mostowsky, and Marc Deisenroth (he/him).
    Matérn gaussian processes on riemannian manifolds. In Advances in Neural Information
    Processing Systems, volume 33, pages 12426–12437, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] Noémie Jaquier, Viacheslav Borovitskiy, Andrei Smolensky, Alexander Terenin,
    Tamim Asfour, and Leonel Dario Rozo. Geometry-aware bayesian optimization in robotics
    using riemannian matérn kernels. In CoRL, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] Arslan Chaudhry, Naeemullah Khan, Puneet Dokania, and Philip Torr. Continual
    learning in low-rank orthogonal subspaces. Advances in Neural Information Processing
    Systems, 33:9900–9911, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Guanxiong Zeng, Yang Chen, Bo Cui, and Shan Yu. Continual learning of
    context-dependent processing in neural networks. Nature Machine Intelligence,
    1(8):364–372, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] Nicolas Boumal, Bamdev Mishra, Pierre Antoine Absil, and Rodolphe J Sepulchre.
    Manopt, a matlab toolbox for optimization on manifolds. The Journal of Machine
    Learning Research, 15(1):1455–1459, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Niklas Koep and Sebastian Weichwald. Pymanopt: A python toolbox for optimization
    on manifolds using automatic differentiation. Journal of Machine Learning Research,
    17:1–5, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] Mayank Meghwanshi, Pratik Jawanpuria, Anoop Kunchukuttan, Hiroyuki Kasai,
    and Bamdev Mishra. Mctorch, a manifold optimization library for deep learning.
    Technical report, arXiv preprint arXiv:1810.01811, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] Nina Miolane, Nicolas Guigui, Alice Le Brigant, Johan Mathe, Benjamin
    Hou, Yann Thanwerdas, Stefan Heyder, Olivier Peltre, Niklas Koep, Hadi Zaatiti,
    et al. Geomstats: A python package for riemannian geometry in machine learning.
    Journal of Machine Learning Research, 21(223):1–9, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] Max Kochurov, Rasul Karimov, and Sergei Kozlukov. Geoopt: Riemannian optimization
    in pytorch. ArXiv, abs/2005.02819:arXiv–2005, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] Kühnel and Stefan Sommer. Computational anatomy in theano. In Graphs in
    Biomedical Image Analysis, Computational Anatomy and Imaging Genetics, pages 4098–4102,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] Line Kühnel, Stefan Sommer, and Alexis Arnaudon. Differential geometry
    and stochastic dynamics with deep learning numerics. Applied Mathematics and Computation,
    356:411–437, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] M. Müller, T. Röder, M. Clausen, B. Eberhardt, B. Krüger, and A. Weber.
    Documentation mocap database hdm05. Technical Report CG-2007-2, Universität Bonn,
    June 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] Abhinav Dhall, Roland Goecke, Jyoti Joshi, Karan Sikka, and Tom Gedeon.
    Emotion recognition in the wild challenge 2014: Baseline, data and protocol. In
    Proceedings of the 16th International Conference on Multimodal Interaction, pages
    461–466, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] Lijun Yin, Xiaozhou Wei, Yi Sun, Jun Wang, and M.J. Rosato. A 3d facial
    expression database for facial behavior research. In 7th International Conference
    on Automatic Face and Gesture Recognition (FGR06), pages 211–216, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] Arman Savran, Neşe Alyüz, Hamdi Dibeklioğlu, Oya Çeliktutan, Berk Gökberk,
    Bülent Sankur, and Lale Akarun. Bosphorus database for 3d face analysis. In European
    Workshop on Biometrics and Identity Management, pages 47–56, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J Ross Beveridge, P Jonathon Phillips, David S Bolme, Bruce A Draper,
    Geof H Givens, Yui Man Lui, Mohammad Nayeem Teli, Hao Zhang, W Todd Scruggs, Kevin W
    Bowyer, et al. The challenge of face recognition from digital point-and-shoot
    cameras. In International Conference on Biometrics: Theory, Applications and Systems
    (BTAS), pages 1–8\. IEEE, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] Li Fei-Fei and Pietro Perona. A bayesian hierarchical model for learning
    natural scene categories. In Proceedings of the IEEE Conference on Computer Vision
    and Pattern Recognition (CVPR), volume 2, pages 524–531, 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] Li-Jia Li and Li Fei-Fei. What, where and who? classifying events by
    scene and object recognition. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), pages 1–8\. IEEE, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio
    Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010
    IEEE Computer Society Conference on Computer Vision and Pattern Recognition, pages
    3485–3492, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Jianxiong Xiao, Krista A Ehinger, James Hays, Antonio Torralba, and Aude
    Oliva. Sun database: Exploring a large collection of scene categories. International
    Journal of Computer Vision, 119(1):3–22, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Mengyi Liu, Shiguang Shan, Ruiping Wang, and Xilin Chen. Learning expressionlets
    on spatio-temporal manifold for dynamic facial expression recognition. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
    1749–1756, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] Catalin Ionescu, Orestis Vantzos, and Cristian Sminchisescu. Matrix backpropagation
    for deep networks with structured layers. In Proceedings of the IEEE International
    Conference on Computer Vision, pages 2965–2973, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Tae-Kyun Kim, Josef Kittler, and Roberto Cipolla. Discriminative learning
    and recognition of image set classes using canonical correlations. IEEE Transactions
    on Pattern Analysis and Machine Intelligence, 29(6):1005–1018, 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] Zhiwu Huang, Ruiping Wang, Shiguang Shan, and Xilin Chen. Projection
    metric learning on grassmann manifold with application to video based face recognition.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
    (CVPR), pages 140–149, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] Hamit Soyel and Hasan Demirel. Optimal feature selection for 3d facial
    expression recognition using coarse-to-fine classification. Turkish Journal of
    Electrical Engineering and Computer Sciences, 18(6):1031–1040, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Stefano Berretti, Alberto Del Bimbo, Pietro Pala, Boulbaba Ben Amor,
    and Mohamed Daoudi. A set of selected sift features for 3d facial expression recognition.
    In International Conference on Pattern Recognition, pages 4125–4128\. IEEE, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Xuan-Phung Huynh, Tien-Duc Tran, and Yong-Guk Kim. Convolutional neural
    network models for facial expression recognition using bu-3dfe database. In Information
    Science and Applications (ICISA) 2016, pages 441–450\. 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Amal Azazi, Syaheerah Lebai Lutfi, Ibrahim Venkat, and Fernando Fernández-Martínez.
    Towards a robust affect recognition: Automatic facial expression recognition in
    3d faces. Expert Systems with Applications, 42(6):3056–3066, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Soon-Yong Chun, Chan-Su Lee, and Sang-Heon Lee. Facial expression recognition
    using extended local binary patterns of 3d curvature. In Multimedia and Ubiquitous
    Engineering, pages 1005–1012. 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] Yiding Wang, Meng Meng, and Qingkai Zhen. Learning encoded facial curvature
    information for 3d facial emotion recognition. In 2013 7th International Conference
    on Image and Graphics, pages 529–532, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] Nicholas Vretos, Nikos Nikolaidis, and Ioannis Pitas. 3d facial expression
    recognition using zernike moments on depth images. In 2011 18th IEEE International
    Conference on Image Processing, pages 773–776, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] Jihun Hamm and Daniel D Lee. Extended grassmann kernels for subspace-based
    learning. In Advances in Neural Information Processing Systems (NIPS), pages 601–608,
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Omkar M Parkhi, Andrea Vedaldi, and Andrew Zisserman. Deep face recognition.
    2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] Svetlana Lazebnik, Cordelia Schmid, and Jean Ponce. Beyond bags of features:
    Spatial pyramid matching for recognizing natural scene categories. In Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume 2,
    pages 2169–2178\. IEEE, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] Mandar Dixit, Nikhil Rasiwasia, and Nuno Vasconcelos. Adapted gaussian
    models for image classification. In CVPR, pages 937–943\. IEEE, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Roland Kwitt, Nuno Vasconcelos, and Nikhil Rasiwasia. Scene recognition
    on the semantic manifold. In European Conference on Computer Vision (ECCV), pages
    359–372, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] Hanlin Goh, Nicolas Thome, Matthieu Cord, and Joo-Hwee Lim. Learning
    deep hierarchical visual feature coding. IEEE Transactions on Neural Networks
    and Learning Systems, 25(12):2212–2225, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] Jianxin Wu and James M Rehg. Beyond the euclidean distance: Creating
    effective visual codebooks using the histogram intersection kernel. In IEEE International
    Conference on Computer Vision, pages 630–637, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang,
    Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature
    for generic visual recognition. In International Conference on Machine Learning
    (ICML), pages 647–655, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
