- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:04:33'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1910.02075] The PAU Survey: Background light estimation with deep learning
    techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1910.02075](https://ar5iv.labs.arxiv.org/html/1910.02075)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'The PAU Survey: Background light estimation with deep learning techniques'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'L. Cabayol-Garcia¹, M. Eriksen¹ , A. Alarcón^(2,3), A. Amara⁴, J. Carretero¹³³footnotemark:
    3, R. Casas^(2,3), F. J. Castander^(2,3), E. Fernández¹, J. García-Bellido⁵, E. Gaztanaga^(2,3),
    H. Hoekstra⁶, R. Miquel^(1,7), C. Neissner¹³³footnotemark: 3, C. Padilla¹ E. Sánchez⁸,
    S. Serrano², I. Sevilla-Noarbe², M. Siudek¹, P. Tallada⁸³³footnotemark: 3, L. Tortorelli⁹'
  prefs: []
  type: TYPE_NORMAL
- en: ¹Institut de Física d’Altes Energies (IFAE), The Barcelona Institute of Science
    and Technology, 08193 Bellaterra (Barcelona), Spain
  prefs: []
  type: TYPE_NORMAL
- en: ²Institute of Space Sciences (ICE, CSIC), Campus UAB, Carrer de Can Magrans,
    s/n, 08193 Barcelona, Spain
  prefs: []
  type: TYPE_NORMAL
- en: ³Institut d’Estudis Espacials de Catalunya (IEEC), 08193 Barcelona, Spain
  prefs: []
  type: TYPE_NORMAL
- en: ⁴Institute of Cosmology & Gravitation, University of Portsmouth, Dennis Sciama
    Building, Burnaby Road, Portsmouth PO1 3FX, UK
  prefs: []
  type: TYPE_NORMAL
- en: ⁵Instituto de Fisica Teorica UAM/CSIC, Universidad Autonoma de Madrid, 28049
    Madrid, Spain
  prefs: []
  type: TYPE_NORMAL
- en: ⁶Leiden Observatory, Leiden University, Leiden, The Netherlands
  prefs: []
  type: TYPE_NORMAL
- en: ⁷Institució Catalana de Recerca i Estudis Avançats, E-08010 Barcelona, Spain
  prefs: []
  type: TYPE_NORMAL
- en: ⁸Centro de Investigaciones Energéticas, Medioambientales y Tecnológicas (CIEMAT),
    Madrid, Spain
  prefs: []
  type: TYPE_NORMAL
- en: '⁹Institute for Particle Physics and Astrophysics, ETH Zürich, Wolfgang-Pauli-Str.
    27, 8093 Zürich, Switzerland E-mail:lcabayol@ifae.esE-mail: eriksen@pic.esAlso
    at Port d’Informació Científica (PIC), Campus UAB, C. Albareda s/n, 08193 Bellaterra
    (Cerdanyola del Vallès), Spain(Accepted XXX. Received YYY; in original form ZZZ)'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: 'blackIn any imaging survey, measuring accurately the astronomical background
    light is crucial to obtain good photometry. This paper introduces BKGnet, a deep
    neural network to predict the background and its associated error. BKGnet has
    been developed for data from the Physics of the Accelerating Universe Survey (PAUS),
    an imaging survey using a 40 narrow-band filter camera (PAUCam). Images obtained
    with PAUCam are affected by scattered light: an optical effect consisting of light
    multiply reflected that deposits energy in specific detector regions contaminating
    the science measurements. Fortunately, scattered light is not a random effect,
    but it can be predicted and corrected for. We have found that BKGnet background
    predictions are very robust to distorting effects, while still being statistically
    accurate. On average, the use of BKGnet improves the photometric flux measurements
    by $7\%$ and up to $20\%$ at the bright end. BKGnet also removes a systematic
    trend in the background error estimation with magnitude in the $i$-band that is
    present with the current PAU data management method. With BKGnet, we reduce the
    photometric redshift outlier rate by $35\%$ for the best $20\%$ galaxies selected
    with a photometric quality parameter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'techniques: photometric – light pollution – instrumentation: photometers^†^†pubyear:
    2019^†^†pagerange: The PAU Survey: Background light estimation with deep learning
    techniques–[B](#A2 "Appendix B Variable annulus ‣ The PAU Survey: Background light
    estimation with deep learning techniques")'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackThe positions, fluxes and other properties of galaxies and stars can be
    determined by analysing images of the sky. Modern imaging surveys can cover large
    areas of sky efficiently, resulting in measurements for large numbers of galaxies
    to faint magnitudes (e.g. DES (Abbott et al., [2018](#bib.bib1))). Improving the
    accuracy of the measurements is crucial for future weak lensing surveys, e.g.
    LSST and Euclid (Ivezić et al., [2019](#bib.bib23); Laureijs et al., [2011](#bib.bib29)),
    to ensure that the results are not dominated by systematic errors.
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: 'blackFor imaging surveys, accurate flux measurements are essential: they are
    used to select samples of galaxies, or to infer their physical properties. A key
    step towards a reliable flux estimate is the determination of the background,
    which needs to be subtracted. The main source of background is the brightness
    of the night sky, which may vary due to a range of effects, such as illumination
    by the Moon, airglow and light pollution. Instrumental effects can contribute
    as well, and in this paper we focus on scattered light, which is the result of
    light deflecting from the instrument optical path appearing at a different region
    of the detector (Romanishin, [2014](#bib.bib41)).'
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackDifferent approaches have been used to estimate the sky background (Bijaoui,
    [1980](#bib.bib4); Newell, [1983](#bib.bib34)), and example implementations include
    DAOPHOT (Stetson, [1987](#bib.bib43)) and SExtractor (Bertin & Arnouts, [1996](#bib.bib3)).
    DAOPHOT measures the background as the mode of the uniformly scattered pixels
    at a certain Full Width at Half Maximum (FWHM) of the given target source. On
    the other hand, SExtractor meshes the background and reconstructs a ’background
    map’ with the background estimated at each particular mesh location. Other methods
    aim to be more robust in the presence of nearby sources. In Teeninga et al. ([2015](#bib.bib45))
    the background is estimated at a location without nearby sources, while Popowicz
    & Smolka ([2015](#bib.bib40)), is based on the removal of small objects and an
    interpolation of missing pixels. In this paper we propose a new approach based
    on a deep neural network to predict the background and its associated error.
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackOver the last few years, deep learning algorithms have resulted in revolutionary
    advances in machine learning and computer vision (Voulodimos et al., [2018](#bib.bib49)).
    Theoretical breakthroughs in training deep Neural Networks (NN) (Werbos, [1982](#bib.bib50)),
    or Convolutional Neural Networks (CNN) (LeCun et al., [1989](#bib.bib30); Lecun
    et al., [1998](#bib.bib31); Zeiler & Fergus, [2013a](#bib.bib52)) together with
    powerful and efficient parallel computing provided by Graphics Processing Units
    (GPUs) (Krizhevsky et al., [2012](#bib.bib27)) have lead to groundbreaking improvements
    across a variety of applications. The number of deep learning projects in cosmology
    is quickly increasing. This includes astronomical object classification (Carrasco-Davis
    et al., [2018](#bib.bib8); Cabayol et al., [2019](#bib.bib7)), (Cabayol et al.,
    [2019](#bib.bib7)), gravitational wave detection (George & Huerta, [2018](#bib.bib19))
    and directly constraining cosmological parameters from mass maps (Fluri et al.,
    [2018](#bib.bib17); Herbel et al., [2018a](#bib.bib20)).
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackExtracting the source photometry requires a significant amount of data
    engineering and parameter tweaking. This can be particularly challenging for noisy
    sources. Deep learning has already been successfully applied to different steps
    in source photometry extraction. Examples include point source detection (Vafaei
    Sadr et al., [2019](#bib.bib48)), cosmic ray detection (Zhang & Bloom, [2019](#bib.bib54))
    or Point Spread Function (PSF) modelling (Herbel et al., [2018b](#bib.bib21)).
    Deep learning has also been used to directly estimate photometric redshifts from
    images (D’Isanto & Polsterer, [2018](#bib.bib14); Pasquet et al., [2019](#bib.bib37)).
    These algorithms implicitly include steps for the background subtraction. Understanding
    these image processing steps can optimize the performance of e.g. redshift estimation
    and galaxy classification.
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackOur goal is to develop and test a deep learning background subtraction
    method using data from the Physics of the Accelerating Universe Survey (PAUS).
    PAUS is an imaging survey that measures high precision photo-$z$s to faint magnitudes
    ($i_{\rm AB}<22.5$), while covering a large area of sky (Martí et al., [2014](#bib.bib33)).
    This is possible thanks to the PAUCam instrument (Castander et al., [2012](#bib.bib11);
    Padilla et al., [2016](#bib.bib35), [2019](#bib.bib36)), an optical camera equipped
    with 40 narrow bands (NB) covering a wavelength range from 450nm to 850nm (Casas
    et al., [2016](#bib.bib10)). PAUS reaches a photo-$z$ precision $\sigma(z)/(1+z)\sim
    0.0035$ for the best 50% of the sample, compared to typical precision of 0.05
    for broad band measurements (Eriksen et al., [2019](#bib.bib16)). The scientific
    goals of PAUS include the measurement of intrinsic alignments of galaxies out
    to $z\sim 0.75$, the study of their spectral energy distributions (SEDs), detailed
    studies of intermediate-scale cosmic structure (Stothert et al., [2018](#bib.bib44)),
    and improvements of image simulations (Tortorelli et al., [2018](#bib.bib47)).
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackPAUS imaged the COSMOS field as a calibration area given the availability
    of spectroscopic redshifts. The PAUS photo-$z$ catalogue for the full COSMOS sample
    with $i_{\rm AB}<22.5$ contains outliers when compared to the spectroscopic redshifts.
    Some of these outliers simply arise from noisy photometry, but others are due
    to a strongly varying continuum produced by scattered light. The excess scattered
    light decreases the signal-to-noise ratio (SNR) of the photometric measurement
    and also alters the statistics of the values of the pixels from which the continuum
    is estimated. These effects can potentially bias the flux measurements.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we present BKGnet, a convolutional neural network capable of
    learning the underlying behaviour of scattered light and other distorting effects
    present in the PAUCam images. BKGnet predicts the background at the location of
    the target sources and the error associated with the background prediction. Although
    it is built to improve the PAUS photometry, BKGnet can be also be applied to other
    future imaging surveys such as LSST (Ivezić et al., [2019](#bib.bib23)) and Euclid
    (Laureijs et al., [2011](#bib.bib29)). The code is available at [https://gitlab.pic.es/pau/bkgnet](https://gitlab.pic.es/pau/bkgnet).
  prefs: []
  type: TYPE_NORMAL
- en: The structure of this paper is as follows. In section 2, we describe the PAU
    Survey and the PAUCam camera and present the modelling of scattered light using
    scattered-light templates. In section 3, we introduce the specific network we
    have developed, as well as defining the training and testing process. Sections
    4 and 5 contain the results obtained for simulated and real PAUCam images, respectively.
    In section 6, we validate the network predictions on real target locations and
    we conclude and summarise in section 7.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Modelling scattered light
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PAUCam images contain substantial amounts of scattered light, which mostly affect
    the edge regions of some CCDs. Scattered light increases the amount of background
    in the affected regions and distorts the expected statistics of the pixel values
    used to estimate the background. Therefore, the scattered light present in PAUCam
    images can lead to an incorrect estimate of the background if not properly modelled,
    thus biasing the photometry. Moreover the elevated background lowers the SNR of
    the measurements. In 2016 the PAU camera was modified in order to mitigate the
    effect of scattered light by introducing baffles on all the edges of the NB filters
    of each filter tray. Although this reduced the amount of scattered light, residuals
    still remain. In the latest COSMOS data reduction, around 8% of exposures taken
    before the camera intervention are flagged as affected by scattered light, and
    therefore dismissed. After the intervention, this number reduced to 5% of the
    exposures, such that on average $7\%$ of data in the COSMOS field are lost due
    to scattered light. In this section, we present the PAUCam scattered light model
    we are using throughout the paper.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 The PAUS observations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: PAUS has been observing since the 2015B semester and as of 2019A, PAUS has taken
    data for 160 nights. The current data covers 10 deg² of the CFHTLS fields¹¹1http://www.cfht.hawaii.edu/Science/CFHTLS_Y_WIRCam
  prefs: []
  type: TYPE_NORMAL
- en: /cfhtlsdeepwidefields.html W1, W2; 20 deg² in W3 and 2 deg² of the COSMOS field²²2http://cosmos.astro.caltech.edu/.
    The PAUS data are stored at the Port d’Informació Científica (PIC), where the
    data are processed and distributed (Tonello et al., [2019](#bib.bib46)).
  prefs: []
  type: TYPE_NORMAL
- en: In this paper we focus only on the data from the COSMOS field, which were taken
    in the semesters 2015B, 2016A, 2016B and 2017B (the low efficiency was caused
    by bad weather). The COSMOS field observations comprise a total of 9749 images,
    343 images for each NB. From these images, 4928 were taken before the camera intervention
    and 4821 after. The basic exposure times in the COSMOS field are 70, 80, 90, 110
    and 130 seconds from the bluest to the reddest filter trays.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 PAUS images processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackThe PAUCam instrument (Castander et al., [2012](#bib.bib11); Padilla et al.,
    [2016](#bib.bib35), [2019](#bib.bib36)) is an optical camera equipped with 40
    narrow bands (NB), covering a wavelength range from 450nm to 850nm (Casas et al.,
    [2016](#bib.bib10)). The NB filters have 13nm FWHM and a separation between consecutive
    bands of 10nm. The camera is also equipped with $ugrizY$ broad band filters that
    so far have been mainly used by external observers. The camera has 18 red-sensitive
    fully depleted Hamamatsu CCD detectors (Casas et al., [2012](#bib.bib9)), although
    only the 8 central CCDs are used for NB imaging. Each CCD has 4096x2048 pixels
    with a pixel scale of 0.26 arcsec/pix. The NB filter set effectively measures
    a high resolution photometric spectrum ($R\approx 50$).
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackIn this project we use images that have already been corrected for various
    instrumental effects (Serrano et al., [prep](#bib.bib42)) in the PAUS nightly
    pipeline. This pipeline performs basic instrumental de-trending processing. The
    electronic effects are corrected using a master bias, which is an observation
    with the shutter closed and zero exposure time. To correct pixel-to-pixel variations
    we use dome flats, which are obtained by imaging a uniformly illuminated screen.
    The astrometry of the narrow band images is calibrated comparing to the positions
    of GAIA DR2 stars (Gaia Collaboration et al., [2018](#bib.bib18)). The photometry
    calibration is done relative to SDSS stars (Castander et al., [prep](#bib.bib12)).
  prefs: []
  type: TYPE_NORMAL
- en: Once the images have been corrected, we perform forced photometry to extract
    the galaxy flux. The current PAUdm pipeline, similarly to DAOPHOT, predicts the
    background noise as the median of the pixels within a ring placed around the target
    source. However, this algorithm requires a (fairly) flat background for an accurate
    estimate. This assumption breaks down when either the annulus or source extraction
    regions are affected by scattered light. In addition, other sources of errors
    in the background estimation are undetected sources, cosmic rays or cross-talk.
    In order to minimize the effect of any of these artifacts, the pixels inside the
    annulus are 3-$\sigma$ clipped before computing the median.
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: 'blackThe default PAUdm radii for the annulus region are $r_{\rm in}=30$ and
    $r_{\rm out}=45$ pixels (Serrano et al., [prep](#bib.bib42)). The annulus is selected
    to be sufficiently far away from the galaxy to avoid light leaking inside the
    ring and not too far so that the background is representative. Throughout this
    paper we use the default configuration to compare this commonly used approach
    to our deep learning algorithm. However, in Appendix [B](#A2 "Appendix B Variable
    annulus ‣ The PAU Survey: Background light estimation with deep learning techniques")
    we study the effect of a variable annulus.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Scattered-light templates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/da5fe663a74a2aae12aa70660be88b5d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Images taken with the PAUCam, corresponding to the NB685 filter.
    *Left:* The first two images correspond to PAUCam images before the camera intervention.
    Notice that both exhibit the same scattered light pattern. *Right:* The two images
    on the right correspond to PAUCam images after the intervention. Again, both present
    the same scattered light pattern, but different to the first two images on the
    left. This shows the changes in scattered light patterns with the intervention.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/698fa706544376b0009f25a7669a5720.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: *Top*: Normalised background light content in each pixel as a function
    of the pixel position in the image for different images before (black dashed line)
    and after (orange solid line) the camera intervention. Each pixel value is divided
    by the mean background in the image. Regions without scattered light should fluctuate
    around unity. Regions affected by scattered light should be above unity. *Bottom*:
    Mean value of the normalised background curves considering all the images taken
    in that band, for the 40 narrow photometric bands.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [1](#S2.F1 "Figure 1 ‣ 2.3 Scattered-light templates ‣ 2 Modelling scattered
    light ‣ The PAU Survey: Background light estimation with deep learning techniques")
    shows four PAUCam images in the NB filter NB685 before the camera intervention
    (first and second images on the left) and after the camera intervention (third
    and fourth images). They show scattered light near the edges of the CCD, displaying
    a spatially varying amount of scattered light. The scattered light patterns change
    from before the intervention (two images on the left) to after (two images on
    the right). The scattered light pattern is also filter dependent. The images taken
    in each filter show their own distinctive patterns, meaning that the pattern depends
    on the filter used.'
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to quantify and model the scattered light is to create background pixel
    maps per NB. This is done with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'i. *Select images*:'
  prefs: []
  type: TYPE_NORMAL
- en: Select a group of NB images from the same bands, since they have the same scattered
    light pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'ii. *Compute median*:'
  prefs: []
  type: TYPE_NORMAL
- en: For each of the images, compute the median background level in the central regions,
    $\mu_{\rm BKG}$, which are unaffected by scattered light.
  prefs: []
  type: TYPE_NORMAL
- en: 'iii. *Estimate ratios*:'
  prefs: []
  type: TYPE_NORMAL
- en: Divide every image by its median to obtain a pixel ratio map.
  prefs: []
  type: TYPE_NORMAL
- en: 'iv. *Mask sources*:'
  prefs: []
  type: TYPE_NORMAL
- en: Mask the images sources by masking all pixels above a given pixel ratio threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'v. *Combine images*:'
  prefs: []
  type: TYPE_NORMAL
- en: Combine all individual pixels maps with a median to get a single scattered-light
    template (SLT) for all the selected images.
  prefs: []
  type: TYPE_NORMAL
- en: If the background were flat and followed Poisson statistics, all pixels in the
    ratio map should fluctuate along unity. However, if the image is affected by scattered
    light, the scattered-light templates in affected regions will have a value above
    unity. We can understand this ratio as approximately the percentage of extra light
    (scattered light) compared to the flat background. Notice that this model takes
    into account that scattered light depends on the amount of light falling on the
    CCD.
  prefs: []
  type: TYPE_NORMAL
- en: The procedure in step (v) can be written as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm SLT}(x,y)=\text{median}_{j}\Big{[}\frac{I_{j}(x,y)}{\mu_{\rm BKG}}\Big{]},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $I_{j}$ is image $j$ and the median is over the selected images (step
    i). To determine the amount of scattered light we can follow the previous procedure
    to step [iii]. This way we obtain normalised background images that should fluctuate
    around unity if they contain a flat background, but would have values above one
    if they are affected by scattered light.
  prefs: []
  type: TYPE_NORMAL
- en: 'The Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Scattered-light templates ‣ 2 Modelling
    scattered light ‣ The PAU Survey: Background light estimation with deep learning
    techniques") top panel shows some of these normalised images for the NB685 filter.
    It shows the background pixel value from side to side of the image before (black
    dashed line) and one after (orange solid line) the camera intervention. The plot
    shows an increasing background on the edges of the CCD before the camera intervention.
    After the intervention, the amount of scattered light is considerably reduced.
    Unfortunately it is still present and thus needs to be accounted for.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use all the normalised background images in a given NB to create a general
    scattered-light template for that band (also splitting before/after the intervention).
    The bottom panel in Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Scattered-light templates
    ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light estimation with
    deep learning techniques") shows the resulting mean of each scattered-light templates
    (one per band) as a function of NB. The mean of the scattered-light templates
    gives information about the amount of scattered light in a given band. We can
    clearly see the effect of the intervention on the amount of scattered light, which
    is reduced.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Scattered-light templates as scattered light correcting method
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If the scattered-light templates modelling is sufficiently accurate, it can
    be used to correct the scattered light on PAUCam images. Assuming that all the
    images from a given NB follow the same scattered light pattern scaled by the CCD
    sky background, a way of correcting scattered light would be
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\tilde{I}(x,y)=I(x,y)-(\text{SLT}(x,y)-1)\mu_{\rm BKG},$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where we subtract from a given target image ($I(x,y)$) the scattered-light templates
    scaled by the mean background of such image ($\mu_{\rm BKG}$). Notice that instead
    of subtracting the scattered-light templates, we subtract the scattered-light
    templates without the flat sky background. This way, the regions without scattered
    light are barely affected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [3](#S2.F3 "Figure 3 ‣ 2.4 Scattered-light templates as scattered light
    correcting method ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light
    estimation with deep learning techniques") shows the original CCD image (left),
    after correcting with the scattered-light templates (middle) and the scattered-light
    templates used for correction (right). Visually, the scattered light pattern in
    the original image (left) disappears after applying the scattered-light templates
    correction (middle). However, although the correction seems visually almost perfect,
    this method has a drawback. Even though scattered light follows approximately
    a pattern given a band, there might be fluctuations due to other external conditions.
    For example, the weather, Moon illumination and other observing conditions may
    induce variations between different observations in a NB. To be more precise estimating
    the correction, one should create a template per band and per night, such that
    the observing conditions are similar. However, for creating a scattered-light
    templates per night, there might be a insufficient number of images to obtain
    an accurate modelling of the correction pattern. Bright stars also contribute
    to scattered light and this cannot be corrected with the scattered-light templates.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [4](#S2.F4 "Figure 4 ‣ 2.4 Scattered-light templates as scattered light
    correcting method ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light
    estimation with deep learning techniques") shows the background level for a specific
    image in the NB685 filter before and after the correction with the scattered-light
    templates. In this case, the image is corrected without considering any split
    on night to generate the scattered-light templates. This means that all images,
    despite being observed on different night and with different observing conditions
    are used to build the scattered-light templates. The image without correction
    displays large peaks at both edges and those are clearly corrected by the scattered-light
    templates. However, both sides of the CCD still show bumps that are caused by
    scattered light residuals.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8b03bfe0cf4619190c82e515250ca97b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: *Left:* Image taken in the NB685 filter showing a scattered light
    pattern on the edges. *Middle:* Previous image corrected with the scattered light
    template. *Right:* The scattered-light template generated with equation [1](#S2.E1
    "In 2.3 Scattered-light templates ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques") considering all images
    taken the same observation night as the original image.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c115331c6522e7cb05203b1f9d783423.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Background pixel values across the image. The original image (orange
    solid line) displays high peaks on the edges caused by scattered light. After
    correcting with the scattered-light templates (dashed black line) the peaks are
    reduced, but some residuals remain. The images are in e/s.'
  prefs: []
  type: TYPE_NORMAL
- en: '3 BKGnet: A Deep Learning based method to predict the background'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section we start by describing the BKGnet architecture. We then describe
    our training and test samples and describe the training process. As a reference,
    in Appendix [A](#A1 "Appendix A Convolutional Neural Networks ‣ The PAU Survey:
    Background light estimation with deep learning techniques") we introduce the basics
    of deep learning and convolutional neural networks, together with some terminology
    definitions.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Neural network architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'BKGnet³³3https://gitlab.pic.es/pau/bkgnet is built using the PyTorch library
    (Paszke et al., [2017](#bib.bib38)). It has two main blocks: a convolutional neural
    network (CNN) and a linear neural network.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [5](#S3.F5 "Figure 5 ‣ 3.2 Data: training and test samples ‣ 3 BKGnet:
    A Deep Learning based method to predict the background ‣ The PAU Survey: Background
    light estimation with deep learning techniques") shows the BKGnet architecture.
    The CNN block handles the information coming from the image itself, as the background
    we want to recover is encoded in the pixel values. The inputs are 120x120 pixels
    stamps containing the target galaxy in the center. \textcolorblackThis choice
    for the stamp size is a compromise between having enough pixels whilst keeping
    the computing requirements (memory, GPU) within reasonable limits.'
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: 'blackAs Figure [5](#S3.F5 "Figure 5 ‣ 3.2 Data: training and test samples ‣
    3 BKGnet: A Deep Learning based method to predict the background ‣ The PAU Survey:
    Background light estimation with deep learning techniques") shows, the CNN contains
    5 blocks of convolutional layer (red layer), pooling layer (yellow layer) and
    batch normalization layer (blue layer). In each convolutional layer, the network
    learns to gradually capture different features in the image. The first layers
    learn low-level features, like edge detection, while having more layers leads
    the network to learn high-level features (Zeiler & Fergus, [2013b](#bib.bib53)).'
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackThe scattered light model depends on parameters that are not encoded in
    the stamps. These are the position of the stamp in the original image, the NB
    used to observe the galaxy and a before/after intervention flag informing the
    network when the galaxy was observed. We also include the target galaxy magnitude
    from a reference catalogue, as it contains information about the number of pixels
    that are affected by the galaxy. To help the network learning the scattered light
    patterns, the previously mentioned parameters are provided to the linear neural
    network, together with the CNN’s output. The magnitude of the galaxy and the coordinates
    of the image are added as fixed parameters per stamp.
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackThe NB filter and the intervention flag are discrete variables with forty
    possible values for the band (1-40) and two for the intervention flag (0/1). The
    combination of the two is, however, directly related to the scattered light pattern,
    and we can effectively convert these two discrete variables into a single one
    with values from 1 to 80\. We add the band and intervention information using
    an embedding. The embedding replaces each combination with 10 trainable parameters.
    Before embedding the band and intervention information, BKGnet learns to encode
    each scattered light pattern using the ten numbers that should best characterise
    the pattern. Therefore, the linear network receives the output from the convolution
    layers, two galaxy coordinates, the magnitude and ten numbers from the embedding.
  prefs: []
  type: TYPE_NORMAL
- en: '3.2 Data: training and test samples'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: BKGnet’s inputs are stamps with the target galaxy in the center. However, to
    train the network we use empty CCD positions, meaning regions where there are
    not target sources. This way, we can estimate the ground truth background value
    at the central CCD region (where there is supposed to be a target galaxy) and
    train the network to recover this value. The estimation of the true background
    values used as training sample labels is done by computing the mean background
    inside a circular aperture of a given fixed radius in the central region of the
    stamp. Therefore, these measurements have an associated uncertainty that directly
    depends on the aperture radius. Assuming that the background is purely Poissonian,
    then
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma^{2}_{\rm label}=\frac{N_{\rm a}b}{t_{\rm exp}},$ |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: where $t_{\rm exp}$ is the exposure time, $b$ is the background estimated as
    the mean of the pixels inside the aperture, i.e. the background label, and $N_{\rm
    a}$ is the number of pixels inside the circular aperture, directly related with
    the choice of aperture radius. We have fixed it to 8 pixels as a balance between
    the error of the ground truth measurement and having a precise background measurement
    in the exact galaxy location. To select empty stamps for the training sample we
    identify sources by cross-correlating the sky coordinates of a given image location
    with the sky coordinates of the sources in the COSMOS catalogue (Laigle et al.,
    [2016](#bib.bib28)).
  prefs: []
  type: TYPE_NORMAL
- en: 'In any deep learning algorithm, the training and test samples should be as
    similar as possible. In our case, our training sample does not contain target
    galaxies whereas the test sample does. We therefore add simulated galaxies at
    the center of the empty training stamps. The simulated galaxies are constructed
    with parameters based on PAUS data: the Sersic profile parameters, $r_{\rm 50}$,
    $I_{\rm 50}$ and the magnitude in the $i$-band. The Sersic profile describes the
    surface brightness profile ($I$) of a galaxy. The radius $r$ that contains 50%
    of the light intensity ($I_{\rm 50}$) is $r_{\rm 50}$. These simulated galaxies
    may differ from the real ones. \textcolorblackFor this reason, we mask the central
    16 x 16 pixels in both the training and test samples. Although the simulated galaxy
    is now masked, it is still important to include it, as for some profiles the galaxy
    light extends outside the masked region. Without the simulated galaxy, BKGnet
    fails on testing bright sources. As the label is estimated in a 8 pixels radius
    aperture, 16 x 16 pixels is the minimum area such that the network does not see
    the pixels used for the estimation of the ground truth background.'
  prefs: []
  type: TYPE_NORMAL
- en: We normalise the stamp before feeding the network. There are different ways
    of doing this. We apply a normalisation stamp by stamp, where we use the mean
    and the standard deviation of each stamp to normalise it. We have chosen this
    normalisation method as it performs better on our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We use all the PAUCam images in the COSMOS field to train and validate the
    network. We have 4928 PAUCam images before the intervention and 4821 after (see
    sec. [3.2](#S3.SS2 "3.2 Data: training and test samples ‣ 3 BKGnet: A Deep Learning
    based method to predict the background ‣ The PAU Survey: Background light estimation
    with deep learning techniques") for details). For each of them, we sample around
    40 stamps per CCD image, giving a total of around 400,000 stamps. We use 90% of
    them for training and the remaining 10% for validation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6141c8967606dd8034d09c38cf73445c.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: BKGnet scheme: The first set of layers corresponds to a Convolutional
    Neural Network to which one inputs the images. The CNN output, together with extra
    information are input to a linear neural network. The numbers on each of the convolutional
    layers represent the layer’s dimension. The first number corresponds to the number
    of channels. The second and third numbers are the dimension of the stamp in that
    layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Training process and loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Supervised deep learning algorithms are trained comparing the true value with
    the algorithm’s prediction. The agreement between the prediction and the true
    value is evaluated with a loss function. The choice of loss function depends on
    the kind of problem one is facing, (e.g. classification, regression). A typical
    loss function for classification problems is the cross-entropy loss, whereas in
    regression problems the mean squared error is commonly used. With BKGnet we want
    the network to associate an uncertainty to each prediction. In supervised deep
    learning, there are some methods based on Bayesian statistics that deal with uncertainties
    associated with the predictions (e.g. Kendall & Gal, [2017](#bib.bib24); Kendall
    et al., [2017](#bib.bib25)).
  prefs: []
  type: TYPE_NORMAL
- en: The method we use assumes that the distribution $p(\textbf{y}|f^{\textbf{w}}(\textbf{x}))$
    is Gaussian, where y are the background label values, x are the inputs and $f^{\textbf{w}}(\textbf{x})$
    are the network background predictions. Therefore, the loss function is defined
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $Loss=\>-\log{p(f^{\textbf{w}}(\textbf{x}))}\>=\>\frac{(f^{\textbf{w}}(\textbf{x})-y)^{2}}{\sigma^{2}}+2\log\sigma.$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: In this way, we train the network to provide both, the background prediction
    $f^{\textbf{w}}(\textbf{x})$ and its associated error $\sigma$. Notice that the
    second term on the right hand side prevents the network from predicting a large
    error that minimizes the first term.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the loss function in Equation [4](#S3.E4 "In 3.3 Training process and
    loss function ‣ 3 BKGnet: A Deep Learning based method to predict the background
    ‣ The PAU Survey: Background light estimation with deep learning techniques"),
    the network provides an error on the quantity $f^{\textbf{w}}(\textbf{x})-y$,
    which has an associated uncertainty $\sigma^{2}_{\rm pred}+\sigma^{2}_{\rm label}$.
    The error on the prediction is therefore'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma_{\rm pred}=\sqrt{\sigma^{2}_{\rm bkgnet}-\sigma^{2}_{\rm label}}\>,$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: 'where $\sigma^{2}_{\rm bkgnet}$ is the error provided by the network and $\sigma^{2}_{\rm
    label}$ is the error of the background label. The error of the background label
    is defined in Equation ([3](#S3.E3 "In 3.2 Data: training and test samples ‣ 3
    BKGnet: A Deep Learning based method to predict the background ‣ The PAU Survey:
    Background light estimation with deep learning techniques")).'
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: 'blackBKGnet is trained in 60 epochs with a batch size of 100 stamps using the
    ADAM optimiser (Kingma & Ba, [2014](#bib.bib26)) and a learning rate of $10^{\rm-5}$
    (see Appendix [A](#A1 "Appendix A Convolutional Neural Networks ‣ The PAU Survey:
    Background light estimation with deep learning techniques") for terminology).
    The training takes about 2 hours using an NVIDIA TITAN V GPU.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Testing BKGnet on simulations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We test the performance of BKGnet with simulated data. We study how well we
    can predict the background with the network and explore what data are needed and
    how these data need to be treated before feeding the network. Throughout the rest
    of the paper we compare theBKGnet predictions to those obtained by calculating
    the background inside an annulus around the target source before and after correcting
    the image with the scattered-light templates.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Simulated PAUCam background images
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackThe final simulated image $I_{\rm sim}(x,y)$ can be expressed as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm I_{\rm sim}}(x,y)=A\cdot\frac{t_{\rm exp}\cdot{\rm SLT}(x,y)+P(t_{\rm
    exp}\cdot{\rm SLT}(x,y))}{t_{\rm exp}},$ |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: where SLT is the scattered light template used to generate the image. In this
    way, the simulated image shows the same scattered light pattern as the PAUCam
    images. To generate the Poisson noise, we first multiply with the exposure time
    $(t_{\rm exp})$ to convert the template from e/s to electrons. Additionally, the
    template is scaled with a factor $A$ to simulate a wide range of background levels.
    Finally, we generate a realization of Poisson sky noise $P(\cdot)$ that we add
    to the image and convert it back to e/s.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [6](#S4.F6 "Figure 6 ‣ 4.1 Simulated PAUCam background images ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques") illustrates the effects of scattered light in our simulated
    stamps. On the left, we show a simulated stamp image with a flat background pattern;
    and, on the right, with a gradient on the background. This gradient is caused
    by scattered light (see Fig. [1](#S2.F1 "Figure 1 ‣ 2.3 Scattered-light templates
    ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light estimation with
    deep learning techniques") and [3](#S2.F3 "Figure 3 ‣ 2.4 Scattered-light templates
    as scattered light correcting method ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques")). Both stamps show
    a central 8x8 pixel masked region, blocking the light of the galaxy.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/db253c5eb2f4059b9e06d99f3d89c43b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Simulated stamps. On the left, a stamp with a flat background. On
    the right a stamp with a background with a gradient.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 BKGnet predictions on simulations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Throughout this section, we train and test on stamps without target galaxies
    (empty positions). This allows us to test whether it is possible to predict the
    background with this network’s assembly. We also fix the band we are testing to
    the NB685 filter after the camera intervention. This choice is a compromise between
    having a considerable amount of scattered light without being completely dominated
    by it. Before the intervention, the amount of scattered light in some of the CCD
    images is very large and might not be an adequate choice to test the network.
    On the other hand, after the intervention, some of the CCDs barely contain scattered
    light, and those would not be a good choice either. The NB685 filter contains
    a considerable amount of scattered light and therefore it is a representative
    example. We do not need to simulate all bands, as here we only want to test the
    viability of the the scattered light prediction with BKGnet and to have a better
    understanding of the network’s behaviour. To quantify the background prediction
    accuracy, we use
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma_{\rm 68}\equiv 0.5\,(b_{\rm quant}^{84.1}-b_{\rm quant}^{15.9}),$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: with quantiles set to 84.1 and 15.9 percentage values. This quantity, $\sigma_{\rm
    68}$, is equivalent to a $1\sigma$ error for a normal distribution, but it is
    less affected by outliers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/fb1b966db11a6821845a37dd15cc3921.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Relative error distributions for the BKGnet (green without coordinate
    information and orange with coordinate information) and the annulus predictions.
    $b_{\rm 0}$ is the background label and $b_{\rm pred}$ is the background prediction,
    either for the annulus or for BKGnet.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [7](#S4.F7 "Figure 7 ‣ 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques") compares the accuracy with which BKGnet predicts the background
    to the PAUS default approach. As described before, PAUS estimates the background
    computing the median inside an annulus centered on the galaxy after the pixel
    values have been $\sigma$-clipped. The plot shows the relative error distribution
    of the predictions for both methods. We have tested BKGnet with and without embedding
    the image coordinates of the galaxy. The BKGnet performance improves significantly
    with the coordinate information. This is not surprising because the amount of
    scattered light depends on the CCD position (see Section [2](#S2 "2 Modelling
    scattered light ‣ The PAU Survey: Background light estimation with deep learning
    techniques")). Although scattered light is encoded in the image, the CCD position
    also includes essential information for the prediction. The network might need
    it to create something similar to the scattered light template. BKGnet achieves
    a $\sigma_{\rm 68}$ = 0.0038 with information coming only with the stamps. Including
    the coordinate information, this improves to $\sigma_{\rm 68}$ = 0.0022\. Therefore,
    the network improves by 70% with the coordinates embedding. The default background
    estimate shows tails on both sides of the distribution, and yields $\sigma_{\rm
    68}$ = 0.0033, which means BKGnet improves the estimate by 42%.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/449f5fb086576d048b51f660863699eb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: *Left*: CCD reconstruction with the true background values used to
    train the network. We sample these background values consecutively and we reconstruct
    the original image by placing each value in the position it was sampled from.
    *Second:* Accuracy on the background prediction with BKGnet in the different image
    positions. We can see there are not spatial patterns. *Third*: Accuracy on the
    background prediction with the annulus in the different image positions. We can
    see there are not spatial patterns. *Right*: Accuracy on the background prediction
    with a kNN in the different image positions.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [8](#S4.F8 "Figure 8 ‣ 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques") shows the spatial background map (left) and the relative
    error on the prediction of this map with the annulus background predictions (third
    panel) and the BKGnet background predictions (second panel). The precision is
    lower at the edges of the CCD for the annulus-based method, where scattered light
    is present. This indicates that the tails in Figure [7](#S4.F7 "Figure 7 ‣ 4.2
    BKGnet predictions on simulations ‣ 4 Testing BKGnet on simulations ‣ The PAU
    Survey: Background light estimation with deep learning techniques") are caused
    by scattered light. On the other hand, one can see that BKGnet is able to account
    for the presence of scattered light.'
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: 'blackThe right panel of Fig. [8](#S4.F8 "Figure 8 ‣ 4.2 BKGnet predictions
    on simulations ‣ 4 Testing BKGnet on simulations ‣ The PAU Survey: Background
    light estimation with deep learning techniques") shows the background reconstruction
    using a kNN algorithm. In addition to the CNN and annulus methods, we have also
    tested the k-Nearest Neighbors (kNN) (Cover & Hart, [2006](#bib.bib13)), Support
    Vector Regression (SVR) (Drucker et al., [1996](#bib.bib15)), Random Forest (RF)
    (Breiman, [2001](#bib.bib6)) and a Neural Network (NN) techniques. We used the
    scikit-learn implementations (Pedregosa et al., [2011](#bib.bib39)) to run the
    kNN, RF and SVR algorithms. For these tests, unlike the CNN, which can handle
    images, we input the embedded information and the median pixel value. As shown
    in Figure [8](#S4.F8 "Figure 8 ‣ 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques"), the kNN measures the background with less accuracy than
    BKGnet and the annulus method in the flat background regions. The background measurements
    are biased by about 3% , which is 6 times larger than the relative errors (0.5%).
    The prediction also shows patterns on the edges with an error 6 times higher than
    those on flat regions. In contrast, for BKGnet the precision only degrades by
    a factor of 1.2 when we compare the center with the border positions. Concerning
    the other methods, the NN provides better predictions than the kNN, although it
    increases $\sigma_{68}$ by a factor of 2.5 with respect to BKGnet. It also shows
    patterns on the edges with 4 times higher errors than in flat background regions.
    Further, the errors of the RF and SVR algorithms are a factor of 6 and 4, respectively,
    higher than those of the BKGnet method, rendering these methods too imprecise.'
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackIn the following sections we test the background estimation method on real
    PAUS images. Based on the performance on simulations, we will only consider the
    BKGnet and the annulus methods.
  prefs: []
  type: TYPE_NORMAL
- en: 5 BKGnet on PAUCam images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the previous section, we have shown that BKGnet is able to accurately predict
    strongly scattered light backgrounds on simulated blank images including only
    the background. However, in real PAUCam data other complications, such as as cosmic
    rays, electronic cross-talk, read-out noise and dark current may affect the performance
    on the estimation of the background. Moreover, correlations between pixels might
    be introduced during the data reduction process. To examine the impact of these
    real-life effects we use actual PAUCam images. To assess the accuracy of our measurements,
    we test network on empty stamps, i.e. without target galaxies.
  prefs: []
  type: TYPE_NORMAL
- en: We will use all the images available in COSMOS, splitting the data into those
    obtained before the camera intervention (in 2016A) and after. This split yields
    4928 images and 4821 images for before and after the intervention, respectively.
    As these numbers are similar, we can easily balance the number of stamps before
    and after the intervention in our training sample. Although the training sample
    does not contain target galaxies in the center, sources might be placed in other
    stamp’s positions. To avoid outliers in the training set, e.g. a stamp with a
    bright star covering most of the background or a bright object too close to the
    center, we filter the training stamps based on the maximum pixel value. All stamps
    with a pixel containing more than 100,000 counts are excluded from the training
    sample.
  prefs: []
  type: TYPE_NORMAL
- en: We also exclude 40 images from each subsample before training the network. These
    80 images are not used to train the network, but are kept to test it. This is
    important, as we need to test the network on images it has never seen before.
    To generate the test set, instead of sampling randomly from the CCDs, we sample
    stamps consecutively in intervals of 60 pixels. This ensures that we test all
    CCD regions, including regions affected by scattered light.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8a4a41be1ff2a747fd1a8ef71537e500.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: $\sigma_{\rm 68}$ of the relative error in the background prediction
    for the 40 NBs. *Left*: Before the intervention. *Right*: After the camera intervention.
    In almost all cases BKGnet performs better than the default approach that employs
    an annulus to estimate the background.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques") shows the results when we use
    BKGnet to predict the background on PAUCam images in empty regions. We also show
    results when the background is estimated using an annulus, and when we first correct
    the background variations using a scattered light template (’annulus + sky’).
    Figure [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques") shows the value of $\sigma_{\rm
    68}$ (Equation ([7](#S4.E7 "In 4.2 BKGnet predictions on simulations ‣ 4 Testing
    BKGnet on simulations ‣ The PAU Survey: Background light estimation with deep
    learning techniques"))) of the relative error distribution on the prediction for
    the 40 different bands. Because we are using the relative error, the comparison
    between the results before and after the intervention is not representative, as
    the background levels are different. For instance, in the first filter tray (NB455-NB515),
    the background before the intervention is between 3 and 5 times higher than after.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We focus first on the results before the camera intervention (left panel in
    Figure [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques")). Images before the camera intervention
    contain more scattered light than those after (see Fig. [2](#S2.F2 "Figure 2 ‣
    2.3 Scattered-light templates ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques")). This makes the scattered-light
    template modelling more unstable than the modeling of images after the intervention.
    We find that correcting with the scattered-light template does not improve the
    annulus method result in every band. In the bluest NBs, i.e. those with the highest
    amount of scattered light, the scattered-light template seems to decrease the
    accuracy of the background prediction. On the other hand, BKGnet improves the
    accuracy compared to the other two methods, especially on the bluest filter tray.
    On average considering all bands, the network reduces the $\sigma_{\rm 68}$ by
    $37\%$ compared to the scattered-light template and up to 50% if we only consider
    the 8 bluest NBs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we consider the results after the camera intervention (right panel of Figure
    [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques")), we see that the scattered-light
    template improves the annulus method prediction in all the bands. This is expected
    from the top panel in Figure [2](#S2.F2 "Figure 2 ‣ 2.3 Scattered-light templates
    ‣ 2 Modelling scattered light ‣ The PAU Survey: Background light estimation with
    deep learning techniques"), which shows that scattered light trends are stable
    after the camera intervention. Before the intervention the scattered-light template
    fails in the bluer bands, which no longer happens after the camera intervention.
    Nevertheless, BKGnet performs even better: on average, after the intervention
    it achieves an 18% improvement compared to the scattered-light template correction.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | BEFORE | AFTER |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '|  | filtered | sources | filtered | sources |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Annulus | 0.011 | 0.011 | 0.014 | 0.014 |'
  prefs: []
  type: TYPE_TB
- en: '| + SLT | 0.011 | 0.011 | 0.011 | 0.013 |'
  prefs: []
  type: TYPE_TB
- en: '| BKGnet | 0.008 | 0.008 | 0.011 | 0.011 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 1: Average $\sigma_{\rm 68}$ of the relative error in the background
    prediction across all the bands for BKGnet trained before and after the camera
    intervention. We list the results for the data sets without filtering out stamps
    affected by sources (‘sources’), and if we remove these (’filtered’).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [1](#S5.T1 "Table 1 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques") lists the average value of $\sigma_{\rm
    68}$ of the relative error in the background prediction for the three methods:
    annulus, annulus + scattered-light template and BKGnet. When training and testing,
    we first exclude the stamps with a maximum pixel value above 100,000\. By doing
    this, we avoid stamps with very bright nearby sources that might bias the prediction.
    To examine the impact of this step, we also list the results when contaminated
    stamps are included (‘sources’) in Table [1](#S5.T1 "Table 1 ‣ 5 BKGnet on PAUCam
    images ‣ The PAU Survey: Background light estimation with deep learning techniques").
    These results show that the filtering does not make a difference before the intervention,
    but the performance improves somewhat for the correction that uses the scattered-light
    templates. The small difference suggests that scattered light is the main source
    of bias. For the images without bright sources taken after the intervention, BKGnet
    and the scattered-light template give the same $\sigma_{\rm 68}$. Therefore, it
    is possible that BKGnet learns the underlying behaviour of scattered light in
    a similar way as the scattered-light template. However, as the network also sees
    the stamp, the correction it infers is more flexible than applying a scattered-light
    template. This indicates that BKGnet is able to learn how to estimate the background
    in the presence of other artifacts (e.g. sources or cosmic rays).'
  prefs: []
  type: TYPE_NORMAL
- en: BKGnet also provides an estimate for the uncertainty associated with the background
    prediction. To test the accuracy of this estimate we use the empty stamps and
    study the distribution of $(b_{\rm net}-b_{\rm true})/\sigma$, where $(b_{\rm
    net}$ and $\sigma$ are the network predictions. If the errors are correct, this
    distribution should be a Gaussian with zero mean and unit variance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [10](#S5.F10 "Figure 10 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey:
    Background light estimation with deep learning techniques") shows the theoretical
    Gaussian we should recover and the measured distributions for the annulus and
    BKGnet predictions. The BKGnet results fit the theoretical Gaussian, which means
    that our errors are robust. In contrast, the annulus predictions underestimate
    the uncertainties by 47%. Therefore, BKGnet provides a more reliable estimate
    of the uncertainty in the background determination.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/01e900b4f04986f02a1ff4a31a6e6bd4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: The distribution of $(b_{\rm net}-b_{\rm 0})/\sigma$, where $b_{\rm
    net}$ is the background prediction and $b_{\rm 0}$ the true background. $\sigma$
    is the uncertainty in the prediction. We expect the distribution to be a Gaussian
    centered on zero with unit variance. We show the distribution for the annulus
    (orange) and BKGnet (blue) predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: 6 BKGnet validation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The results presented in section [5](#S5 "5 BKGnet on PAUCam images ‣ The PAU
    Survey: Background light estimation with deep learning techniques") show that,
    compared to the annulus-based methods, BKGnet yields better background estimates
    (see Fig [9](#S5.F9 "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background
    light estimation with deep learning techniques") and Tab [1](#S5.T1 "Table 1 ‣
    5 BKGnet on PAUCam images ‣ The PAU Survey: Background light estimation with deep
    learning techniques")). It also provides accurate estimates for the associated
    uncertainty. However, these tests were done on stamps without galaxies. Here we
    increase the realism of the problem and quantify the performance of BKGnet at
    galaxy positions.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Generating the PAUS catalogue with BKGnet predictions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We use BKGnet to estimate the background for galaxies in the COSMOS field. We
    compare the results to those from the PAUdm catalogue, which uses an annulus to
    determine the background. These catalogues contain around 12 million flux measurements,
    approximately half of them done on images taken before the intervention and the
    other half on images taken after the intervention. The galaxy fluxes are obtained
    subtracting the background from the PAUS raw signal measurements,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $F=S-N_{\rm a}b,$ |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: where $F$ is the net galaxy flux, $S$ is the total signal measured inside the
    aperture, $N_{\rm a}$ is the number of pixels inside the aperture and $b$ is the
    predicted background per pixel. When the background is estimated with an annulus,
    the error on the net flux is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma^{2}=(S-b)+N_{\rm a}\sigma_{\rm b}^{2}+N^{2}_{\rm a}\Big{(}\frac{\pi}{2}\Big{)}\frac{\sigma_{\rm
    b}^{2}}{N_{\rm b}},$ |  | (9) |'
  prefs: []
  type: TYPE_TB
- en: where $b$ and $\sigma_{\rm b}$ are the background and the background error in
    that region and $N_{\rm b}$ is the number of pixels inside the annulus. The $\pi/2$
    factor arises from that fact that we use the median of the pixels inside the annulus
    instead of the mean ⁴⁴4http://wise2.ipac.caltech.edu/staff/fmasci/ApPhotUncert.pdf.
  prefs: []
  type: TYPE_NORMAL
- en: For BKGnet the error on the galaxy flux is
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\sigma^{2}=(S-b)+N_{\rm a}(b+RN^{2})+N_{\rm a}^{2}\sigma_{\rm b}^{2},$
    |  | (10) |'
  prefs: []
  type: TYPE_TB
- en: where $RN$ is the read-out noise.
  prefs: []
  type: TYPE_NORMAL
- en: 'Equations [9](#S6.E9 "In 6.1 Generating the PAUS catalogue with BKGnet predictions
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques") and [10](#S6.E10 "In 6.1 Generating the PAUS catalogue with
    BKGnet predictions ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation
    with deep learning techniques") reflect the differences in the flux uncertainty
    when the background is measured with an annulus or with BKGnet. In general, there
    are three main contributors to the flux uncertainty: the uncertainty in the net
    galaxy flux, the uncertainty in the background estimate, and the uncertainty introduced
    by the background subtraction. For both background estimation methods we assume
    that the uncertainty in the net galaxy flux is captured by shot noise. For BKGnet,
    the background uncertainty is also described by shot noise (Eq. [10](#S6.E10 "In
    6.1 Generating the PAUS catalogue with BKGnet predictions ‣ 6 BKGnet validation
    ‣ The PAU Survey: Background light estimation with deep learning techniques")),
    but we add a read-out noise contribution to the background error. For the PAUdm
    measurements, the background uncertainty is given by the mean variance per pixel
    (Eq. [9](#S6.E9 "In 6.1 Generating the PAUS catalogue with BKGnet predictions
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques")). Therefore, for PAUdm, this term should also account for
    other error contributions besides shot noise. The third terms in Eqs. [9](#S6.E9
    "In 6.1 Generating the PAUS catalogue with BKGnet predictions ‣ 6 BKGnet validation
    ‣ The PAU Survey: Background light estimation with deep learning techniques")
    and [10](#S6.E10 "In 6.1 Generating the PAUS catalogue with BKGnet predictions
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques") are the contributions from background subtraction uncertainties.
    In PAUdm, this is determined by the subtraction of a background measured in the
    annulus. In contrast, in Equation [10](#S6.E10 "In 6.1 Generating the PAUS catalogue
    with BKGnet predictions ‣ 6 BKGnet validation ‣ The PAU Survey: Background light
    estimation with deep learning techniques") we use the uncertainty provided by
    the network within the aperture where the flux is estimated.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Validating the catalogues
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The estimation of the background using an annulus is a viable method when the
    background is flat. In PAUS data, scattered light only affects objects near the
    edges of the images. Hence, for most of the galaxies in PAUS data the background
    should be (approximately) flat and we should not expect large differences between
    the BKGnet and the PAUdm catalogues. Comparing the fluxes estimated with Equation
    [8](#S6.E8 "In 6.1 Generating the PAUS catalogue with BKGnet predictions ‣ 6 BKGnet
    validation ‣ The PAU Survey: Background light estimation with deep learning techniques"),
    we find a 2% difference between the two approaches. On the other hand, the uncertainties
    estimated with BKGnet (Eq. [10](#S6.E10 "In 6.1 Generating the PAUS catalogue
    with BKGnet predictions ‣ 6 BKGnet validation ‣ The PAU Survey: Background light
    estimation with deep learning techniques")) are 4% lower than for PAUdm.'
  prefs: []
  type: TYPE_NORMAL
- en: We need to determine which catalogue provides better photometry estimates. To
    do so, we use the fact that PAUCam takes multiple observations of the same object
    in all NB filters. We can compare different exposures of the same object, which
    should be comparable once the background noise is subtracted. This is formulated
    as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $D\equiv\frac{(e_{\rm 1}-e_{\rm 2})}{\sqrt{(\sigma^{2}_{\rm 1}+\sigma^{2}_{\rm
    2})}},$ |  | (11) |'
  prefs: []
  type: TYPE_TB
- en: where $e_{i}$ are different exposures of the same object and $\sigma_{i}$ the
    associated uncertainties. The distribution of $D$ should be a Gaussian with unit
    variance if the photometry is robust and the errors are properly accounted for.
    We call this the duplicates test.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e302277c8e0d0f093a83a52be45eb1f4.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: BKGnet validation with the duplicates distribution test. We plot
    the width of the distribution defined in Equation ([11](#S6.E11 "In 6.2 Validating
    the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation
    with deep learning techniques")) as a function of wavelength for the catalogue
    generated with BKGnet (black line) and the current PAUdm catalogue (orange line).
    The dashed line corresponds to the results excluding all objects flagged in PAUdm.
    The solid line includes all objects.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [11](#S6.F11 "Figure 11 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation
    ‣ The PAU Survey: Background light estimation with deep learning techniques")
    shows the results of the duplicates test as a function of wavelength. We estimate
    $\sigma_{\rm 68}[D]$ (Eq. [11](#S6.F11 "Figure 11 ‣ 6.2 Validating the catalogues
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques")) for each NB with the BKGnet (black line) and PAUdm (orange
    line) catalogues. It is possible to flag photometric outliers based on an ellipticity
    parameter to detect strongly varying backgrounds. The dashed lines in Figure [11](#S6.F11
    "Figure 11 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey:
    Background light estimation with deep learning techniques") show the results when
    we exclude such flagged objects. Dropping flagged objects does not significantly
    change the measurements for BKGnet, but we see a clear improvement for the PAUdm
    measurements. The improvement is particularly prominent for the NB755 filter (at
    7500Å), which is affected by telluric absorption of O[2] in the atmosphere. Interestingly
    BKGnet seems to know how to deal with these objects, showing that BKGnet is more
    robust towards various sources of bias, not only scattered light. When we consider
    all NBs, we find $\langle\sigma_{\rm 68}[D]\rangle=1.00$ for BKGnet, which is
    what we would expect for correct photometry. On the other hand, the current PAUdm
    catalogue yields $\langle\sigma_{\rm 68}[D]\rangle$ = 1.10, i.e. it overestimates
    the uncertainties.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The measurement uncertainties should depend on the brightness of the source.
    To explore this we show $\sigma_{\rm 68}[D]$ as a function of the Subaru $i_{\rm
    Auto}$ magnitude in Figure [12](#S6.F12 "Figure 12 ‣ 6.2 Validating the catalogues
    ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation with deep
    learning techniques"). In the PAUdm catalogue there is a strong trend with magnitude.
    At the bright end, the fluxes differ by more than 20% compared to the expectation.
    This trend disappears when we predict the background and uncertainties with BKGnet.
    To explore the origin of the trend further we used the background prediction from
    BKGnet but the errors from the annulus method. As the blue dotted line in Figure [12](#S6.F12
    "Figure 12 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey:
    Background light estimation with deep learning techniques") shows, we find the
    same trend with magnitude. This implies that it is caused by the estimated uncertainties
    for the annulus method. Moreover, the blue dotted line lies below the PAUdm line.
    The only difference between these two curves is the background value prediction
    (not the error). Therefore, the predictions with PAUdm are more accurate than
    those with the annulus method.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f4c9bba78a3121461a830d85316dcae9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: BKGnet validation with the duplicates distribution test. We plot
    the width of the distribution defined in Equation ([11](#S6.E11 "In 6.2 Validating
    the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey: Background light estimation
    with deep learning techniques")) as a function of $i_{\rm auto}$ in the Subaru
    $i$ band for the catalogue generated with BKGnet (black solid line), the current
    PAUdm catalogue (orange dashed line) and a mixed catalogue with the predictions
    from BKGnet and the errors from PAUdm (blue dotted line).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To further validate the BKGnet catalogue we run BCNz2 (Eriksen et al., [2019](#bib.bib16))
    using the fluxes determined using BKGnet. For this test, we exclude the objects
    flagged in the PAUdm catalogue, in order to use exactly the same objects as in
    (Eriksen et al., [2019](#bib.bib16)). However, as shown in Figures [11](#S6.F11
    "Figure 11 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey:
    Background light estimation with deep learning techniques") and [12](#S6.F12 "Figure
    12 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey: Background
    light estimation with deep learning techniques"), we do not need to exclude these
    objects. The photo-$z$s are compared to secure spectroscopic estimates from zCOSMOS
    DR3 (Lilly et al., [2007](#bib.bib32)) with $i_{\rm AB}<22.5$. We split the sample
    based on a quality parameter defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm Qz}\equiv\frac{\chi^{2}}{N_{\rm f}-3}\Big{(}\frac{z_{\rm quant}^{\rm
    99}-z_{\rm quant}^{\rm 1}}{\rm ODDS(\Delta z=0.01)}\Big{)},$ |  | (12) |'
  prefs: []
  type: TYPE_TB
- en: where $\chi^{2}/(n_{\rm f}-1)$ is the reduced chi-squared from the template
    fit and the $z_{\rm quant}$ are the percentiles of $(z_{\rm photo}-z_{\rm spec})/(1+z_{\rm
    spec}$). The ODDS is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '|  | ${\rm ODDS}\equiv\int_{z_{\rm b}-\Delta z}^{z_{\rm b}+\Delta z}{\rm dz\
    p(z)},$ |  | (13) |'
  prefs: []
  type: TYPE_TB
- en: where $z_{\rm b}$ is the mode of the $p(z)$ and $\Delta z$ defines a redshift
    interval around the peak. In PAUS, a galaxy is considered an outlier if
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $&#124;z_{\rm photo}-z_{\rm spec}&#124;\ /\ (1+z_{\rm spec})>0.02.$ |  |
    (14) |'
  prefs: []
  type: TYPE_TB
- en: Notice that this outlier definition is very strict. In broad band photometry,
    a common outlier definition is $|z_{\rm photo}-z_{\rm spec}|>0.15\,(1+z_{\rm spec})$,
    e.g. Ilbert et al. ([2006](#bib.bib22)); Bilicki et al. ([2018](#bib.bib5)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [2](#S6.T2 "Table 2 ‣ 6.2 Validating the catalogues ‣ 6 BKGnet validation
    ‣ The PAU Survey: Background light estimation with deep learning techniques")
    lists the outlier rate and the photometric redshift precision obtained with BCNz2
    for the two catalogues. To quantify the redshift precision we use $\sigma_{\rm
    68}$ (Eq. [7](#S4.E7 "In 4.2 BKGnet predictions on simulations ‣ 4 Testing BKGnet
    on simulations ‣ The PAU Survey: Background light estimation with deep learning
    techniques")). The photometric redshift precision does not improve significantly
    between the two catalogues, but we find a reduction in the outlier rate. If we
    consider the complete sample (100%) this improvement is small. This might be because
    in the full sample the outliers are dominated by photo-$z$ outliers, rather than
    outliers on the photometry itself. However, if we cut using the $Qz$ parameter
    to get the best 20% and 50% of the sample, we notice that the outlier rate reduces
    significantly. These outliers should be dominated by photometric outliers. For
    the best 50% of the sample we reduce the number of outliers by 25%, whereas for
    the best 20% of objects this improvement rises to a 35%. This shows once more
    that BKGnet is a statistically accurate method that is also robust.'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Outlier percentage | $10^{3}\sigma_{\rm 68}$ |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Percentage | BKGnet | PAUdm | BKGnet | PAUdm |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 20 | 3.5 | 5.4 | 2.0 | 2.1 |'
  prefs: []
  type: TYPE_TB
- en: '| 50 | 3.8 | 5.1 | 3.6 | 3.7 |'
  prefs: []
  type: TYPE_TB
- en: '| 80 | 10.4 | 11.3 | 5.8 | 6.0 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 16.7 | 17.5 | 8.4 | 8.6 |'
  prefs: []
  type: TYPE_TB
- en: 'Table 2: Photo-$z$ outlier rate and accuracy obtained with BCNz2 for the BKGnet
    and the PAUdm catalogues. The percentages correspond to the samples selected by
    the photo-$z$ quality parameters $Qz$.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Imaging surveys need accurate background subtraction methods to obtain precise
    source photometry. We have developed a deep learning method to predict the background
    for astronomical images that are affected by scattered light. The algorithm has
    been developed to predict the background on images taken with PAUCam. The edges
    of PAUCam images are affected by scattered light (see Fig. [1](#S2.F1 "Figure
    1 ‣ 2.3 Scattered-light templates ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques")), especially in the
    bluer bands. In 2016, the camera was modified to reduce the amount of scattered
    light. While the amount of scattered light decreased drastically, PAUcam images
    still contain a significant amount of scattered light (see Fig. [2](#S2.F2 "Figure
    2 ‣ 2.3 Scattered-light templates ‣ 2 Modelling scattered light ‣ The PAU Survey:
    Background light estimation with deep learning techniques")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For each band, the scattered light follows the same spatial pattern within
    the CCD and scales approximately linear with the background level. We have constructed
    scattered-light templates and background pixel maps by combining images taken
    with the same NB and normalised by their background level. These scattered-light
    templates show the scattered light variation across the CCD and can be used to
    correct for scattered light (see Fig. [3](#S2.F3 "Figure 3 ‣ 2.4 Scattered-light
    templates as scattered light correcting method ‣ 2 Modelling scattered light ‣
    The PAU Survey: Background light estimation with deep learning techniques")).
    Nevertheless, background fluctuations due to external conditions (e.g. Moon, seeing,
    airmass) can trigger differences on scattered light from night to night. To accurately
    correct scattered light with scattered-light templates, we would need to generate
    a scattered-light templates per NB and night. However, even then, fluctuations
    during the night or a small number of available images in a given band can lead
    to inaccurate corrections.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We therefore developed BKGnet, a deep learning based algorithm that predicts
    the background and its associated uncertainty behind target sources accounting
    for scattered light and other distorting effect. BKGnet consists of a Convolutional
    Neural Network followed by a linear neural network (see Fig. [5](#S3.F5 "Figure
    5 ‣ 3.2 Data: training and test samples ‣ 3 BKGnet: A Deep Learning based method
    to predict the background ‣ The PAU Survey: Background light estimation with deep
    learning techniques")). In the training set we use empty stamps, i.e. without
    a target galaxy, so that we can estimate the true background and use it for training.
    We need to simulate target galaxies in the training sample before masking the
    central region, otherwise the network fails when applied to bright and large sources.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We first tested the predictions on PAUCam empty stamps, i.e. without target
    galaxies. For data taken before the intervention, BKGnet improves over the scattered-light
    templates + annulus prediction by $37\%$. The scattered-light templates correction
    fails in many of the bands, specially on the bluer filter tray, which is affected
    the most by scattered light (left panel of Fig. [9](#S5.F9 "Figure 9 ‣ 5 BKGnet
    on PAUCam images ‣ The PAU Survey: Background light estimation with deep learning
    techniques")). For data taken after the intervention, BKGnet improves over the
    scattered-light templates + annulus prediction by $17\%$ (right panel of Fig. [9](#S5.F9
    "Figure 9 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background light estimation
    with deep learning techniques")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'BKGnet also predicts the uncertainty associated with the background prediction.
    For that, we use the log likelihood of a Gaussian centered at the background true
    value as loss function (Eq. [4](#S3.E4 "In 3.3 Training process and loss function
    ‣ 3 BKGnet: A Deep Learning based method to predict the background ‣ The PAU Survey:
    Background light estimation with deep learning techniques")). To validate BKGnet,
    we test on empty positions and estimate the difference between the prediction
    and the background level, divided by the estimated uncertainty. For the annulus
    method, we find that the errors are underestimated by 47% (Fig [10](#S5.F10 "Figure
    10 ‣ 5 BKGnet on PAUCam images ‣ The PAU Survey: Background light estimation with
    deep learning techniques")). On the other hand, with BKGnet this quantity is normally
    distributed around zero with unit variance, showing that the uncertainties are
    correctly estimated.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We generated a PAUS catalogue for the COSMOS field using BKGnet to predict
    the background. To validate the catalogue we took advantage of having multiple
    measurements of the same object. The resulting distribution of differences in
    flux measurements should be a Gaussian of unit variance (Eq. [11](#S6.E11 "In
    6.2 Validating the catalogues ‣ 6 BKGnet validation ‣ The PAU Survey: Background
    light estimation with deep learning techniques")). The results demonstrate that
    BKGnet improves the photometry with respect to the current background subtraction
    algorithm. We test the performance for the full catalogue and a catalogue where
    we exclude all objects flagged in the current catalogue version. When excluding
    flagged objects, we find very similar results with the BKGnet catalogue and the
    current catalogue. However, when testing the full catalogue, we find a large improvement
    for BKGnet. It specially improves the results in a region with high atmospheric
    absorption, demonstrating that it is more robust against sources of bias while
    still being statistically accurate. It also removes a strong systematic trend
    with $i$-band magnitude, that disappears when the uncertainties are estimated
    with the network.'
  prefs: []
  type: TYPE_NORMAL
- en: Finally, as the aim of PAUS is to provide accurate redshifts for large samples
    of galaxies, we have run the BCNz2 code using the BKGnet catalogue. BKGnet reduces
    the outlier rate by a 25% and 35% respectively for the best 50% and 20% photo-$z$
    samples, while the accuracy is not affected.
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackWith BKGnet we have optimised the background subtraction task, one of the
    image processing steps in photometric surveys that can improve the redshift estimation
    and classification of galaxies. Deep learning algorithms that predict these quantities
    directly from images have to subtract the background internally. Therefore, the
    understanding from BKGnet will also help to optimise such deep learning algorithms.
    Although the network has been tested with PAUCam images, the concept should also
    be applicable to future imaging surveys as Euclid and LSST.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Funding for PAUS has been provided by Durham University (via the ERC StG DEGAS-259586),
    ETH Zurich, Leiden University (via ERC StG ADULT-279396 and Netherlands Organisation
    for Scientific Research (NWO) Vici grant 639.043.512) and University College London.
    The PAUS participants from Spanish institutions are partially supported by MINECO
    under grants CSD2007-00060, AYA2015-71825, ESP2015-88861, FPA2015-68048, SEV-2016-0588,
    SEV-2016-0597, and MDM-2015-0509, some of which include ERDF funds from the European
    Union. IEEC and IFAE are partially funded by the CERCA program of the Generalitat
    de Catalunya. The PAU data center is hosted by the Port d’Informació Científica
    (PIC), maintained through a collaboration of CIEMAT and IFAE, with additional
    support from Universitat Autònoma de Barcelona and ERDF. CosmoHub has been developed
    by PIC and was partially funded by the "Plan Estatal de Investigación Científica
    y Técnica y de Innovación" program of the Spanish government. We gratefully acknowledge
    the support of NVIDIA Corporation with the donation of the Titan V GPU used for
    this research. This project has received funding from the European Union’s Horizon
    2020 research and innovation programme under grant agreement No 776247\. Adam
    Amara is supported by a Royal Society Wolfson Fellowship. MS has been supported
    by the National Science Centre (grant UMO-2016/23/N/ST9/02963).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Abbott et al. (2018) Abbott T. M. C., et al., 2018, [ApJS](http://dx.doi.org/10.3847/1538-4365/aae9f0),
    [239, 18](https://ui.adsabs.harvard.edu/abs/2018ApJS..239...18A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alexander et al. (2019) Alexander S., Gleyzer S., McDonough E., Toomey M. W.,
    Usai E., 2019, [p. arXiv:1909.07346](https://ui.adsabs.harvard.edu/abs/2019arXiv190907346A)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bertin & Arnouts (1996) Bertin E., Arnouts S., 1996, [Astronomy and Astrophysics
    Supplement Series](http://dx.doi.org/10.1051/aas:1996164), [117, 393](https://ui.adsabs.harvard.edu/#abs/1996A&AS..117..393B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bijaoui (1980) Bijaoui A., 1980, A&A, [84, 81](https://ui.adsabs.harvard.edu/#abs/1980A&A....84...81B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bilicki et al. (2018) Bilicki M., et al., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201731942),
    [616, A69](https://ui.adsabs.harvard.edu/abs/2018A&A...616A..69B)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breiman (2001) Breiman L., 2001, [Mach. Learn.](http://dx.doi.org/10.1023/A:1010933404324),
    45, 5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cabayol et al. (2019) Cabayol L., et al., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/sty3129),
    [483, 529](https://ui.adsabs.harvard.edu/#abs/2019MNRAS.483..529C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carrasco-Davis et al. (2018) Carrasco-Davis R., et al., 2018, [p. arXiv:1807.03869](https://ui.adsabs.harvard.edu/abs/2018arXiv180703869C)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casas et al. (2012) Casas R., et al., 2012, in High Energy, Optical, and Infrared
    Detectors for Astronomy V. p. 845326, [doi:10.1117/12.924640](http://dx.doi.org/10.1117/12.924640)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Casas et al. (2016) Casas R., et al., 2016, in Ground-based and Airborne Instrumentation
    for Astronomy VI. p. 99084K, [doi:10.1117/12.2232422](http://dx.doi.org/10.1117/12.2232422)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castander et al. (2012) Castander F. J., et al., 2012, in Ground-based and Airborne
    Instrumentation for Astronomy IV. p. 84466D, [doi:10.1117/12.926234](http://dx.doi.org/10.1117/12.926234)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castander et al. (prep) Castander F., Eriksen M., Serrano S., et al. in prep.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cover & Hart (2006) Cover T., Hart P., 2006, [IEEE Trans. Inf. Theor.](http://dx.doi.org/10.1109/TIT.1967.1053964),
    13, 21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: D’Isanto & Polsterer (2018) D’Isanto A., Polsterer K. L., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201731326),
    [609, A111](https://ui.adsabs.harvard.edu/abs/2018A&A...609A.111D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Drucker et al. (1996) Drucker H., Burges C. J. C., Kaufman L., Smola A., Vapnik
    V., 1996, Support Vector Regression Machines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eriksen et al. (2019) Eriksen M., et al., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz204),
    [484, 4200](https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.4200E)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fluri et al. (2018) Fluri J., Kacprzak T., Refregier A., Amara A., Lucchi A.,
    Hofmann T., 2018, [Phys. Rev. D](http://dx.doi.org/10.1103/PhysRevD.98.123518),
    [98, 123518](https://ui.adsabs.harvard.edu/#abs/2018PhRvD..98l3518F)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gaia Collaboration et al. (2018) Gaia Collaboration et al., 2018, [A&A](http://dx.doi.org/10.1051/0004-6361/201833051),
    [616, A1](https://ui.adsabs.harvard.edu/abs/2018A&A...616A...1G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: George & Huerta (2018) George D., Huerta E. A., 2018, [Physics Letters B](http://dx.doi.org/10.1016/j.physletb.2017.12.053),
    [778, 64](https://ui.adsabs.harvard.edu/#abs/2018PhLB..778...64G)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Herbel et al. (2018a) Herbel J., Kacprzak T., Amara A., Refregier A., Lucchi
    A., 2018a, [J. Cosmology Astropart. Phys.](http://dx.doi.org/10.1088/1475-7516/2018/07/054),
    [2018, 054](https://ui.adsabs.harvard.edu/abs/2018JCAP...07..054H)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Herbel et al. (2018b) Herbel J., Kacprzak T., Amara A., Refregier A., Lucchi
    A., 2018b, [J. Cosmology Astropart. Phys.](http://dx.doi.org/10.1088/1475-7516/2018/07/054),
    [2018, 054](https://ui.adsabs.harvard.edu/abs/2018JCAP...07..054H)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilbert et al. (2006) Ilbert O., et al., 2006, [A&A](http://dx.doi.org/10.1051/0004-6361:20065138),
    [457, 841](https://ui.adsabs.harvard.edu/abs/2006A&A...457..841I)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ivezić et al. (2019) Ivezić Ž., et al., 2019, [ApJ](http://dx.doi.org/10.3847/1538-4357/ab042c),
    [873, 111](https://ui.adsabs.harvard.edu/abs/2019ApJ...873..111I)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall & Gal (2017) Kendall A., Gal Y., 2017, [p. arXiv:1703.04977](https://ui.adsabs.harvard.edu/abs/2017arXiv170304977K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kendall et al. (2017) Kendall A., Gal Y., Cipolla R., 2017, [p. arXiv:1705.07115](https://ui.adsabs.harvard.edu/abs/2017arXiv170507115K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma & Ba (2014) Kingma D. P., Ba J., 2014, arXiv e-prints, [p. arXiv:1412.6980](https://ui.adsabs.harvard.edu/abs/2014arXiv1412.6980K)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. (2012) Krizhevsky A., Sutskever I., Hinton G. E., 2012, in
    Proceedings of the 25th International Conference on Neural Information Processing
    Systems - Volume 1. NIPS’12. Curran Associates Inc., USA, pp 1097–1105, [http://dl.acm.org/citation.cfm?id=2999134.2999257](http://dl.acm.org/citation.cfm?id=2999134.2999257)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laigle et al. (2016) Laigle C., et al., 2016, [ApJS](http://dx.doi.org/10.3847/0067-0049/224/2/24),
    [224, 24](https://ui.adsabs.harvard.edu/abs/2016ApJS..224...24L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Laureijs et al. (2011) Laureijs R., et al., 2011, [p. arXiv:1110.3193](https://ui.adsabs.harvard.edu/#abs/2011arXiv1110.3193L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (1989) LeCun Y., Boser B., Denker J. S., Henderson D., Howard R. E.,
    Hubbard W., Jackel L. D., 1989, [Neural Computation](http://dx.doi.org/10.1162/neco.1989.1.4.541),
    1, 541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lecun et al. (1998) Lecun Y., Bottou L., Bengio Y., Haffner P., 1998, [Proceedings
    of the IEEE](http://dx.doi.org/10.1109/5.726791), 86, 2278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lilly et al. (2007) Lilly S. J., et al., 2007, [ApJS](http://dx.doi.org/10.1086/516589),
    [172, 70](https://ui.adsabs.harvard.edu/abs/2007ApJS..172...70L)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Martí et al. (2014) Martí P., Miquel R., Castander F. J., Gaztañaga E., Eriksen
    M., Sánchez C., 2014, [MNRAS](http://dx.doi.org/10.1093/mnras/stu801), [442, 92](https://ui.adsabs.harvard.edu/#abs/2014MNRAS.442...92M)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Newell (1983) Newell E. B., 1983, in Astronomical Measuring Machines Workshop.
    pp 15–40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padilla et al. (2016) Padilla C., et al., 2016, in Ground-based and Airborne
    Instrumentation for Astronomy VI. p. 99080Z, [doi:10.1117/12.2231884](http://dx.doi.org/10.1117/12.2231884)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Padilla et al. (2019) Padilla C., et al., 2019, [AJ](http://dx.doi.org/10.3847/1538-3881/ab0412),
    [157, 246](https://ui.adsabs.harvard.edu/abs/2019AJ....157..246P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pasquet et al. (2019) Pasquet J., Bertin E., Treyer M., Arnouts S., Fouchez
    D., 2019, [A&A](http://dx.doi.org/10.1051/0004-6361/201833617), [621, A26](https://ui.adsabs.harvard.edu/abs/2019A&A...621A..26P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paszke et al. (2017) Paszke A., et al., 2017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pedregosa et al. (2011) Pedregosa F., et al., 2011, Journal of Machine Learning
    Research, 12, 2825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Popowicz & Smolka (2015) Popowicz A., Smolka B., 2015, [MNRAS](http://dx.doi.org/10.1093/mnras/stv1320),
    [452, 809](https://ui.adsabs.harvard.edu/#abs/2015MNRAS.452..809P)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Romanishin (2014) Romanishin W., 2014, An Introduction to Astronomical Photometry
    Using Ccds. Createspace Independent Pub, [https://books.google.es/books?id=0nbMoQEACAAJ](https://books.google.es/books?id=0nbMoQEACAAJ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Serrano et al. (prep) Serrano S., Castander F., Fernandez E., et al. in prep.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stetson (1987) Stetson P. B., 1987, [Publications of the Astronomical Society
    of the Pacific](http://dx.doi.org/10.1086/131977), [99, 191](https://ui.adsabs.harvard.edu/#abs/1987PASP...99..191S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stothert et al. (2018) Stothert L., et al., 2018, [MNRAS](http://dx.doi.org/10.1093/mnras/sty2491),
    [481, 4221](https://ui.adsabs.harvard.edu/#abs/2018MNRAS.481.4221S)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Teeninga et al. (2015) Teeninga P., Moschini U., Trager S. C., Wilkinson M.
    H. F., 2015, in 2015 IEEE International Conference on Image Processing (ICIP).
    pp 1046–1050, [doi:10.1109/ICIP.2015.7350959](http://dx.doi.org/10.1109/ICIP.2015.7350959)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tonello et al. (2019) Tonello N., et al., 2019, [Astronomy and Computing](http://dx.doi.org/10.1016/j.ascom.2019.04.002),
    [27, 171](https://ui.adsabs.harvard.edu/abs/2019A&C....27..171T)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tortorelli et al. (2018) Tortorelli L., et al., 2018, [p. arXiv:1805.05340](http://adsabs.harvard.edu/abs/2018arXiv180505340T)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vafaei Sadr et al. (2019) Vafaei Sadr A., Vos E. E., Bassett B. A., Hosenie
    Z., Oozeer N., Lochner M., 2019, [MNRAS](http://dx.doi.org/10.1093/mnras/stz131),
    [484, 2793](https://ui.adsabs.harvard.edu/abs/2019MNRAS.484.2793V)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Voulodimos et al. (2018) Voulodimos A., Doulamis N., Doulamis A., Protopapadakis
    E., 2018, [Computational Intelligence and Neuroscience](http://dx.doi.org/10.1155/2018/7068349),
    2018, 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Werbos (1982) Werbos P. J., 1982, in Drenick R. F., Kozin F., eds, System Modeling
    and Optimization. Springer Berlin Heidelberg, Berlin, Heidelberg, pp 762–770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2015) Xu B., Wang N., Chen T., Li M., 2015, [p. arXiv:1505.00853](https://ui.adsabs.harvard.edu/#abs/2015arXiv150500853X)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeiler & Fergus (2013a) Zeiler M. D., Fergus R., 2013a, [p. arXiv:1311.2901](https://ui.adsabs.harvard.edu/#abs/2013arXiv1311.2901Z)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeiler & Fergus (2013b) Zeiler M. D., Fergus R., 2013b, [p. arXiv:1311.2901](https://ui.adsabs.harvard.edu/abs/2013arXiv1311.2901Z)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang & Bloom (2019) Zhang K., Bloom J., 2019, [The Journal of Open Source Software](http://dx.doi.org/10.21105/joss.01651),
    [4, 1651](https://ui.adsabs.harvard.edu/abs/2019JOSS....4.1651Z)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Convolutional Neural Networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Machine learning methods are data analysis techniques where the algorithm learns
    from the data. In particular, one of the most popular class of algorithms are
    neural networks (Werbos, [1982](#bib.bib50)), which are designed to recognise
    patterns, usually learned from training data (supervised method). They are mainly
    used for regression and classification problems (Alexander et al., [2019](#bib.bib2)).
    Deep learning is a subset of machine learning that refers to a development of
    neural network technology, involving a large number of layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning methods, and in general any supervised machine learning method,
    model a problem by optimising a set of trainable weights that fit the data. This
    is done in three stages: forward propagation, back propagation and weight optimisation.
    The network starts with the forward propagation. At this stage, the input data
    propagates through all the network layers and then, the network gives a prediction
    for each of the input samples. After that, by comparing with the known true value,
    which is technically called label, the network estimates a prediction error with
    a given loss function. After that, back propagation takes place. Back propagation
    consists of computing the contribution of each weight on the prediction error.
    Such contributions are calculated with the partial derivative of the loss with
    respect to each of the weights. The weight optimisation is the weights correction
    based on the quantities calculated in the back propagation to reduce the error
    in the next iteration.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this work, we use a Convolutional Neural Network (CNN; Lecun et al., [1998](#bib.bib31);
    Zeiler & Fergus, [2013a](#bib.bib52)). Our network contains four differentiated
    types of layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Convolutional layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: This layer makes the network powerful in image and pattern recognition tasks.
    It has a filter, technically named kernel and is usually 2-dimensional, which
    contains a set of trainable weights used to convolve the image. The outcome of
    this layer is the input image convolved with the kernel. In a given convolutional
    layer, one can convolve the input with as many kernels as desired. Each of these
    convolutions will generate a convolved image, which we refer to as channel. All
    of them together are the input of the next layer.
  prefs: []
  type: TYPE_NORMAL
- en: '*Pooling layers*:'
  prefs: []
  type: TYPE_NORMAL
- en: This layer reduces the dimensionality of the set of convolved images. It applies
    some function (e.g. sum, mean, maximum) to a group of spatially connected pixels
    and reduces the dimensions of such group. For example, it takes 2 consecutive
    pixels and converts them to the mean of both. Although we use it to handle the
    amount of data generated after the convolutions, it also regularises the model
    to avoid learning from non-generalisable noise and details in the training data
    (also known as overfitting).
  prefs: []
  type: TYPE_NORMAL
- en: '*Fully connected layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: This layer is usually the last layer of the network. Its input is the linearised
    outcome of the previous ones (in our network, convolutions + poolings). It applies
    a linear transformation from the input to the output. The slope and bias of the
    linear transformation are the learning parameters.
  prefs: []
  type: TYPE_NORMAL
- en: '*Batch normalisation layer*:'
  prefs: []
  type: TYPE_NORMAL
- en: In this layer the network normalises the output of a previous activation layer.
    It subtracts the mean and divides by the standard deviation. Batch normalisation
    helps to increase the stability of a neural network and avoids over-fitting problems.
  prefs: []
  type: TYPE_NORMAL
- en: After each convolution and fully connected layer there is an activation function
    that transforms the outcome. Activation functions are non-linear functions that
    map the outcome of a layer to the input of the following one. An example of an
    activation function is the Rectified Linear Unit (ReLu) (Krizhevsky et al., [2012](#bib.bib27)),
    although we use a variation of this function called LeakyReLu (Xu et al., [2015](#bib.bib51)),
    with which we find better results.
  prefs: []
  type: TYPE_NORMAL
- en: Other terms that one needs to be familiar with are epoch and batch. An epoch
    is an iteration over the complete training dataset. It is common practice to avoid
    feeding the network with all the training sample together. Instead, the training
    data is divided in groups of a certain size and each of these groups is called
    batch. Feeding the network in batches helps it learn faster as in every iteration
    over a batch, it back-propagates updating all the weights. Then, instead of updating
    once per epoch, it updates as many times as there are batches. The amount of variation
    allowed per iteration is regulated by the learning rate.
  prefs: []
  type: TYPE_NORMAL
- en: Appendix B Variable annulus
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackCurrently, the PAUS photometry pipeline uses an annulus region to estimate
    the astronomical background behind a target source. The inner and outer radii
    are fixed at 30 and 45 pixels from the source, respectively. However, for each
    galaxy, one could adjust the annulus parameters to minimize the effect of the
    target galaxy flux falling inside the ring and background variations between the
    target and the annulus location.
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: blackWe can quantify the amount of extra light falling inside the annulus coming
    from the galaxy ($\Delta_{\rm F}$) and scattered light ($\Delta_{\rm B}$). For
    a flat background, $\Delta_{\rm B}$ should be independent of the annulus location.
    However, this term does depend on the annulus location when the background varies.
    On the other hand, $\Delta_{\rm F}$ is minimized by an annulus further away from
    the source and depends on the galaxy size and the PSF. We define
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\Gamma\equiv\frac{&#124;\Delta_{\rm F}+\Delta_{\rm B}&#124;}{\sigma_{\rm
    b}},$ |  | (15) |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma_{\rm b}$ is the error on the background subtraction. $\Gamma$
    measures the relative error on the background prediction due to scattered light
    and the source contribution. We use $\Gamma$ to study the effect of a variable
    annulus by minimising the quantity as a function of annulus radius.
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: 'blackWe have tested this on simulations. The background images are simulated
    as in section [3.2](#S3.SS2 "3.2 Data: training and test samples ‣ 3 BKGnet: A
    Deep Learning based method to predict the background ‣ The PAU Survey: Background
    light estimation with deep learning techniques"). The images also contain simulated
    galaxies with the same size (r[50]) and PSF distribution as observed PAUS galaxies.
    Using simulations allows us to evaluate $\Delta_{\rm B}$ on the background simulations
    and $\Delta_{\rm F}$ on the galaxy simulations. This method cannot directly be
    applied to observed images, since it requires distinguishing between the galaxy
    flux and the background. However, it shows under which conditions the annulus
    approach degrades. Our simulations only contain Poisson noise and scattered light.
    We know that real PAUCam images have a more complicated noise pattern and therefore,
    we would also expect the distributions of $\Gamma_{\rm opt}$ to shift towards
    higher values.'
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: 'blackFigure [13](#A2.F13 "Figure 13 ‣ Appendix B Variable annulus ‣ The PAU
    Survey: Background light estimation with deep learning techniques") shows the
    histogram of minimum $\Gamma$ measurements (Eq. [15](#A2.E15 "In Appendix B Variable
    annulus ‣ The PAU Survey: Background light estimation with deep learning techniques"))
    for a set of simulated galaxies. $\Gamma$ is evaluated for different annulus radii,
    moving the inner and outer radii between one and forty pixels from the source
    with fixed $r_{\rm out}-r_{\rm in}$ = 15 pixels. We split the results in galaxies
    in the center of the image (flat background, orange) and galaxies in the borders
    (scattered light, black). Both histograms show a large fraction of galaxies for
    which the annulus can predict the background very accurately (low gamma values).
    For galaxies in the center, we expect a flat background and therefore good results
    with the annulus method.'
  prefs: []
  type: TYPE_NORMAL
- en: \textcolor
  prefs: []
  type: TYPE_NORMAL
- en: 'blackThere are also many galaxies (Fig. [13](#A2.F13 "Figure 13 ‣ Appendix
    B Variable annulus ‣ The PAU Survey: Background light estimation with deep learning
    techniques")) on the border with accurate measurements. First of all, not all
    positions at the border are affected by scattered light. For those positions with
    scattered light, the annulus can be placed very close to the target galaxy if
    it is small ($r_{\rm 50}\approx$ 1 or 2 pixels). Then, the background variation
    from the target source to the annulus position would be small. Nevertheless, the
    distribution of border galaxies also shows a tail corresponding to galaxies for
    which the annulus estimation significantly degrades. In these cases, the optimal
    annulus is either too close to the source or capturing a strong background variation.
    If the background variation is very strong, the annulus will tend to get closer
    to the target source in order to minimize $\Delta_{\rm B}$. However, this is not
    possible for bright and large galaxies, since getting closer to the target increases
    $\Delta_{\rm F}$. The annulus method cannot be used to measure the background
    in large galaxies in varying background regions, since it strongly degrades.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/32332105b5537ff9df70bbdfb1b69203.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: The $\Gamma_{\rm min}$ distribution for galaxies in the border (scattered
    light affected, black) and in the center (flat background, filled orange) of the
    image.'
  prefs: []
  type: TYPE_NORMAL
