- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:50:30'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:50:30
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2110.08180] Deep Transfer Learning for Blended Source Identification in Galaxy
    Survey Data'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2110.08180] 深度迁移学习用于星系勘测数据中的混合源识别'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2110.08180](https://ar5iv.labs.arxiv.org/html/2110.08180)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2110.08180](https://ar5iv.labs.arxiv.org/html/2110.08180)
- en: '¹¹institutetext: AIM, CEA, CNRS, Université Paris-Saclay, Université Paris
    Diderot, Sorbonne Paris Cité, F-91191 Gif-sur-Yvette, France ²²institutetext:
    Université Paris-Saclay, CNRS, CEA, Astrophysique, Instrumentation et Modélisation
    de Paris-Saclay, 91191, Gif-sur-Yvette, France'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ¹¹机构：AIM, CEA, CNRS, 巴黎第萨克雷大学, 巴黎第八大学, 巴黎索邦城市大学, F-91191 Gif-sur-Yvette, 法国
    ²²机构：巴黎第萨克雷大学, CNRS, CEA, 巴黎萨克雷天体物理学、仪器与建模, 91191, Gif-sur-Yvette, 法国
- en: Deep Transfer Learning for Blended Source Identification in Galaxy Survey Data
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度迁移学习用于星系勘测数据中的混合源识别
- en: 'S. Farrens email: samuel.farrens@cea.frDeep Transfer Learning for Blended Source
    Identification in Galaxy Survey DataDeep Transfer Learning for Blended Source
    Identification in Galaxy Survey Data    A. Lacan Deep Transfer Learning for Blended
    Source Identification in Galaxy Survey DataDeep Transfer Learning for Blended
    Source Identification in Galaxy Survey Data    A. Guinot Deep Transfer Learning
    for Blended Source Identification in Galaxy Survey DataDeep Transfer Learning
    for Blended Source Identification in Galaxy Survey Data    A. Z. Vitorelli Deep
    Transfer Learning for Blended Source Identification in Galaxy Survey DataDeep
    Transfer Learning for Blended Source Identification in Galaxy Survey Data'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: S. Farrens 邮箱：samuel.farrens@cea.fr深度迁移学习用于星系勘测数据中的混合源识别深度迁移学习用于星系勘测数据中的混合源识别
       A. Lacan 深度迁移学习用于星系勘测数据中的混合源识别深度迁移学习用于星系勘测数据中的混合源识别    A. Guinot 深度迁移学习用于星系勘测数据中的混合源识别深度迁移学习用于星系勘测数据中的混合源识别
       A. Z. Vitorelli 深度迁移学习用于星系勘测数据中的混合源识别深度迁移学习用于星系勘测数据中的混合源识别
- en: We present BlendHunter, a proof-of-concept for a deep transfer learning based
    approach for the automated and robust identification of blended sources in galaxy
    survey data. We take the VGG-16 network with pre-trained convolutional layers
    and train the fully connected layers on parametric models of COSMOS images. We
    test the efficacy of the transfer learning by taking the weights learned on the
    parametric models and using them to identify blends in more realistic CFIS-like
    images. We compare the performance of this method to SEP (a Python implementation
    of SExtractor) as function of noise level and the separation between sources.
    We find that BlendHunter outperforms SEP by $\sim 15\%$ in terms of classification
    accuracy for close blends ($<10$ pixel separation between sources) regardless
    of the noise level used for training. Additionally, the method provides consistent
    results to SEP for distant blends ($\geq 10$ pixel separation between sources)
    provided the network is trained on data with a relatively close noise standard
    deviation to the target images. The code and data have been made publicly available
    to ensure the reproducibility of the results. [\faGithub](https://github.com/CosmoStat/BlendHunter)
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了BlendHunter，一个基于深度迁移学习的概念验证工具，用于自动且稳健地识别星系勘测数据中的混合源。我们采用预训练的卷积层的VGG-16网络，并在COSMOS图像的参数模型上训练全连接层。通过使用在参数模型上学到的权重来识别更真实的CFIS-like图像中的混合源，我们测试了迁移学习的有效性。我们将此方法的性能与SEP（SExtractor的Python实现）进行了比较，比较了噪声水平和源之间的分离。我们发现，BlendHunter在近混合源（源间距离<10像素）的分类准确率上比SEP提高了约15%，无论训练时使用的噪声水平如何。此外，只要网络在噪声标准差接近目标图像的数据上进行训练，该方法对远离混合源（源间距离≥10像素）也能提供与SEP一致的结果。代码和数据已公开，以确保结果的可重复性。
    [\faGithub](https://github.com/CosmoStat/BlendHunter)
- en: 'Key Words.:'
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'Techniques: image processing, Methods: numerical, Methods: data analysis, Gravitational
    lensing: weak'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 技术：图像处理，方法：数值，方法：数据分析，弱引力透镜
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: The blending of sources (i.e. the apparent overlapping of extended objects in
    2D images) has a significant impact on the measurement of the morphological and
    structural properties of galaxies, in particular for ground-based surveys. Nearby
    objects can easily be mistaken for a single source, which can lead to significant
    detection and/or measurement biases depending on the depth of the survey.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 来源的融合（即二维图像中延伸物体的明显重叠）对星系的形态学和结构特性的测量具有重要影响，特别是对地面观测调查而言。附近的物体很容易被误认为是单一来源，这可能导致显著的检测和/或测量偏差，具体取决于调查的深度。
- en: For weak gravitational lensing analysis, it is essential to understand how and
    to what degree blending impacts shear and photometric redshift measurements (Mandelbaum,
    [2018](#bib.bib22)). This problem becomes even more critical in the current epoch
    of high precision cosmology, where systematic effects need to be carefully accounted
    for. This is particularly important for reliable comparisons between late-time
    probes, like weak lensing, and early-time probes, such as the CMB.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 对于弱引力透镜分析，了解融合如何以及在何种程度上影响剪切和光度红移测量（Mandelbaum, [2018](#bib.bib22)）是至关重要的。在当前高精度宇宙学的时代，这个问题变得更加关键，需要仔细考虑系统效应。这对于可靠比较晚期探测（如弱透镜）和早期探测（如宇宙微波背景）尤为重要。
- en: 'Blended sources make up a significant fraction of the observed sources in survey
    images: $>30\%$ for the Dark Energy Survey (DES, Samuroff et al., [2018](#bib.bib29)),
    $>50\%$ (up to $>60\%$ and $>70\%$ for the Deep and UltraDeep layers, respectively)
    for Hyper Suprime-Cam (HSC, Bosch et al., [2018](#bib.bib6)) and $>60\%$ for Vera
    C. Rubin Observatory Legacy Survey of Space and Time (LSST, Sanchez et al., [2021](#bib.bib30)).
    Simply removing all the blends that have been identified would significantly reduce
    the sample size and could potentially lead to entangled biases in the shear correlation
    function (Hartlap et al., [2011](#bib.bib15)). Additional biases can be introduced
    by unidentified blends coming from sources below the detection threshold (Hoekstra
    et al., [2017](#bib.bib16); Euclid Collaboration et al., [2019](#bib.bib12)).
    It is therefore necessary to develop an appropriate procedure for dealing with
    blends in survey data.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 融合源在调查图像中占据了显著的比例：在暗能量调查（DES, Samuroff et al., [2018](#bib.bib29)）中占比$>30\%$，在Hyper
    Suprime-Cam（HSC, Bosch et al., [2018](#bib.bib6)）中占比$>50\%$（深层和超深层分别达到$>60\%$和$>70\%$），以及在Vera
    C. Rubin Observatory Legacy Survey of Space and Time（LSST, Sanchez et al., [2021](#bib.bib30)）中占比$>60\%$。仅仅去除所有已识别的融合源将显著减少样本大小，并可能导致剪切相关函数中的纠缠偏差（Hartlap
    et al., [2011](#bib.bib15)）。未识别的融合源可能来自低于检测阈值的源，这可能引入额外的偏差（Hoekstra et al., [2017](#bib.bib16);
    Euclid Collaboration et al., [2019](#bib.bib12)）。因此，开发一种适当的处理调查数据中融合源的程序是必要的。
- en: 'The process of handling blended sources in astrophysical images can be broadly
    divided into several problems: a) the detection of objects in the image, b) the
    classification of those objects as either single sources or blended sources, c)
    the segmentation of pixels from blended sources (often referred to as ‘deblending’),
    and d) the rejection of those objects that cannot be easily included in the scientific
    analysis. This paper focuses on the problem of classifying objects, already detected
    with standard source extraction software, as either blends or not. Unlike the
    problem of segmentation, which has been abundantly addressed in the literature
    (Joseph et al., [2016](#bib.bib17); Melchior et al., [2018](#bib.bib25); Reiman
    & Göhre, [2019](#bib.bib26)), little effort has been made to find reliable and
    automated methods for identifying blended sources in survey data.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 处理天体物理图像中的融合源的过程可以大致分为几个问题：a) 图像中物体的检测，b) 将这些物体分类为单一源或融合源，c) 从融合源中分割像素（通常称为“去融合”），以及
    d) 排除那些不能轻易纳入科学分析的物体。本文重点关注将已经用标准源提取软件检测到的物体分类为融合源还是非融合源的问题。与文献中已经广泛讨论的分割问题（Joseph
    et al., [2016](#bib.bib17); Melchior et al., [2018](#bib.bib25); Reiman & Göhre,
    [2019](#bib.bib26)）不同，针对调查数据中融合源的可靠和自动化识别方法的研究相对较少。
- en: Traditional methods for identifying blended sources, such as SExtractor (Bertin
    & Arnouts, [1996](#bib.bib5)), rely on fixed thresholds to detect multiple peaks
    in the light intensity profiles of the objects. While this approach may work for
    a reasonable fraction of the sources, it lacks the flexibility to handle blends
    with a bigger discrepancy in the brightness and/or size of the individual objects.
    This means that a large number of blended sources, which may have a non-negligible
    impact on the scientific analysis, could be missed.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的混合源识别方法，例如SExtractor（Bertin & Arnouts，[1996](#bib.bib5)），依赖固定阈值来检测对象光强度曲线中的多个峰值。虽然这种方法对相当一部分源可能有效，但它缺乏处理亮度和/或大小差异较大的混合源的灵活性。这意味着可能会错过大量混合源，这些源可能对科学分析产生不可忽视的影响。
- en: Machine learning techniques, in particular deep learning architectures, have
    been shown to be incredibly successful when applied to complicated classification
    problems (see e.g. Kotsiantis ([2007](#bib.bib19)), Lecun et al. ([2015](#bib.bib20)),
    and Srinivas et al. ([2016](#bib.bib34))). The effectiveness of these tools, however,
    can be difficult to gauge without reliable labelled data. In real astrophysical
    images it is not known a priori if the underlying signal for a given detection
    comes from a single object or indeed from the combination of several. This necessitates
    the use of simulated data in order to generate a reliable training set, where
    the number and diversity of overlapping sources are perfectly known. This, however,
    introduces the possibility of over-fitting the network to properties specific
    to the simulation, *e.g.* overly simplified galaxy models. This in turn can cause
    the network to be very sensitive to artefacts and more complex structures seen
    in real observations. It is extremely challenging and time consuming to attempt
    to develop simulated galaxy images that contain all of the properties expected
    in real data. Therefore, the application of networks well suited to transfer learning
    presents an interesting approach to mitigate this problem.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习技术，特别是深度学习架构，已被证明在处理复杂分类问题时非常成功（参见例如Kotsiantis（[2007](#bib.bib19)），Lecun
    et al.（[2015](#bib.bib20)），以及Srinivas et al.（[2016](#bib.bib34)））。然而，这些工具的有效性可能难以评估，特别是缺乏可靠的标注数据时。在真实的天体物理图像中，无法事先知道给定检测的基础信号是否来自单一对象或几个对象的组合。这就需要使用模拟数据以生成可靠的训练集，其中重叠源的数量和多样性是完全已知的。然而，这引入了过拟合网络到特定于模拟的属性的可能性，例如过于简化的星系模型。这反过来可能导致网络对伪影和真实观测中看到的更复杂结构非常敏感。开发包含真实数据中所有预期属性的模拟星系图像是极具挑战性且耗时的。因此，应用适合转移学习的网络是缓解此问题的一个有趣方法。
- en: 'Transfer learning, in particular deep transfer learning, is a machine learning
    approach whereby network weights obtained by training on a given data set are
    applied to another distinct but similar data set. This can help prevent over-fitting
    to the training data as well as significantly reducing the time required to apply
    the method to new data sets. Deep transfer learning has been applied to a variety
    of astrophysical applications in recent years including: the classification of
    compact star clusters (Wei et al., [2020](#bib.bib40)), the separation of low
    surface brightness galaxies from artefacts (Tanoglidis et al., [2021](#bib.bib36)),
    the classification of planetary nebulae (Awang Iskandar et al., [2020](#bib.bib3))
    and the deblending of galaxy images (Arcelin et al., [2021](#bib.bib2)).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习，特别是深度转移学习，是一种机器学习方法，其中通过在给定数据集上训练获得的网络权重应用于另一个不同但相似的数据集。这有助于防止过拟合训练数据，并显著减少将该方法应用于新数据集所需的时间。近年来，深度转移学习已被应用于各种天体物理应用，包括：紧凑星团的分类（Wei
    et al., [2020](#bib.bib40)），从伪影中分离低表面亮度星系（Tanoglidis et al., [2021](#bib.bib36)），行星状星云的分类（Awang
    Iskandar et al., [2020](#bib.bib3)）以及星系图像的去混合（Arcelin et al., [2021](#bib.bib2)）。
- en: This work introduces a proof-of-concept deep transfer learning approach for
    the automated and robust identification of blended sources in single-band galaxy
    survey data, hereafter referred to as BlendHunter (BH)¹¹1In the spirit of reproducible
    research, all code and data needed to reproduce the results in this paper have
    been made publicly available on GitHub ([https://github.com/CosmoStat/BlendHunter](https://github.com/CosmoStat/BlendHunter))
    without any restrictions.. This method incorporates a Convolutional Neural Network
    (CNN) trained on a large database of natural images and a fully connected layer
    for classification. Simple parametric models are used to train the fully connected
    layers and the learned weights are in turn used to identify blended sources in
    more realistic images.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究介绍了一种概念验证的深度迁移学习方法，用于自动化和鲁棒地识别单带星系调查数据中的混合源，以下称为BlendHunter（BH）¹¹1为了可重复研究的精神，所有代码和数据均已在GitHub上公开，供
    reproducing 论文结果 使用 ([https://github.com/CosmoStat/BlendHunter](https://github.com/CosmoStat/BlendHunter))，没有任何限制。该方法结合了在大量自然图像数据库上训练的卷积神经网络（CNN）和用于分类的全连接层。简单的参数模型用于训练全连接层，学习到的权重随后用于识别更现实图像中的混合源。
- en: This paper is organised as follows. The following section introduces the properties
    of the training and testing data. Sect. [3](#S3 "3 Deep transfer learning framework
    for blend identification ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data") presents the transfer learning approach used and how it
    was trained to identify blended galaxy images. Sect. [4](#S4 "4 Results ‣ Deep
    Transfer Learning for Blended Source Identification in Galaxy Survey Data") provides
    results on how this approach compares to the state of the art. Finally, conclusions
    are presented in Sect. [5](#S5 "5 Conclusions ‣ Deep Transfer Learning for Blended
    Source Identification in Galaxy Survey Data").
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本论文组织如下。接下来的部分介绍了训练和测试数据的属性。第[3](#S3 "3 Deep transfer learning framework for
    blend identification ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data")节介绍了使用的迁移学习方法以及如何训练它以识别混合星系图像。第[4](#S4 "4 Results ‣ Deep
    Transfer Learning for Blended Source Identification in Galaxy Survey Data")节提供了该方法与当前最先进技术的比较结果。最后，第[5](#S5
    "5 Conclusions ‣ Deep Transfer Learning for Blended Source Identification in Galaxy
    Survey Data")节总结了结论。
- en: 2 Data
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 数据
- en: Supervised machine learning approaches, such as that presented in this work,
    require accurately labelled and representative training data. These labels often
    correspond to properties that cannot be directly and/or reliably measured from
    observed data. Simulations, on the other hand, provide a controlled environment
    where these properties are known a priori.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习方法，如本研究中提出的方法，需要准确标注和具有代表性的训练数据。这些标签通常对应于从观测数据中不能直接和/或可靠测量的属性。而模拟提供了一个受控的环境，在这里这些属性是已知的。
- en: To train our network we opted to produce a set of simulated single-band (*i.e.*
    monochromatic) galaxy images from a parametric model. This allowed us to control
    the fraction of blended images in the whole sample and the separation of sources
    in the blended images. The choice of single-band images was made to restrict the
    learning to pixel features (*i.e.* no colour information), which could potentially
    be of interest for the $r$-band of the Canada-France Imaging Survey (CFIS)²²2[https://www.cfht.hawaii.edu/Science/CFIS/](https://www.cfht.hawaii.edu/Science/CFIS/),
    part of the Ultraviolet Near-Infrared Optical Northern Survey (UNIONS), or eventually
    the Euclid visible band (Cropper et al., [2012](#bib.bib8)). We additionally prepared
    a sample of realistic CFIS-like galaxy images to test the efficacy of transferring
    the learned weights to a similar but unseen data set.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的网络，我们选择了从参数模型中生成一组模拟的单带（*即* 单色）星系图像。这使我们能够控制整个样本中混合图像的比例以及混合图像中源的分离程度。选择单带图像是为了将学习限制在像素特征上（*即*
    不包含颜色信息），这可能对加拿大-法国成像调查（CFIS）²²2[https://www.cfht.hawaii.edu/Science/CFIS/](https://www.cfht.hawaii.edu/Science/CFIS/)中的$r$-带，或最终的欧几里得可见光带（Cropper
    et al., [2012](#bib.bib8)）具有潜在的兴趣。我们还准备了一组现实的CFIS类星系图像样本，以测试将学习到的权重转移到类似但未见过的数据集上的效果。
- en: This section details how the training and testing data sets were generated.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 本节详细描述了训练和测试数据集的生成方式。
- en: 2.1 Blend definition
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 混合定义
- en: In order to label our training data, we first needed to make a clear definition
    of a blend. Typically this would be some measure of amount of overlap between
    the light profiles of the individual sources that constitute the blend. For the
    purposes of this work, we assume a simple scenario in which isolated (*i.e.* un-blended)
    sources are composed of a single galaxy centred within a postage stamp of size
    $51\times 51$ pixels ($9.5\mathrm{arcsec}\times 9.5\mathrm{arcsec}$). We then
    define a blend as a postage stamp (of the same size) containing two galaxies,
    one at the centre and a secondary source at a random position. This definition
    is fairly agnostic in that we do not require any specific overlap between the
    light profiles. This choice was made in order to gauge classification accuracy
    as a function of distance between the two sources.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了标注我们的训练数据，我们首先需要对“混合”的定义进行明确。通常，这将是组成混合体的各个源的光谱轮廓之间重叠量的某种度量。对于本工作，我们假设一个简单的情境，其中孤立（*即*
    未混合）的源由一个单一的星系组成，该星系位于 $51\times 51$ 像素（$9.5\mathrm{arcsec}\times 9.5\mathrm{arcsec}$）的邮票内。然后我们将混合定义为一个包含两个星系的邮票，其中一个位于中心，另一个在随机位置。这个定义相当中立，因为我们不要求光谱轮廓之间有任何特定的重叠。这一选择是为了衡量分类准确性与两个源之间距离的关系。
- en: 'Given the size of the postage stamps, we expect that standard source extraction
    software, such as SExtractor, will be easily able to identify and separate sources
    near the borders but may struggle as the sources get closer together. To better
    highlight this point we additionally separate blends into two categories: ‘close
    blends’ and ‘distant blends’. We define close blends as postage stamps in which
    the two sources are separated by less than ten pixels and distant blends as those
    in which they are separated by ten or more pixels.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于邮票的大小，我们预计标准的源提取软件，如 SExtractor，将能够轻松识别和分离接近边界的源，但可能在源靠得更近时遇到困难。为了更好地突出这一点，我们还将混合体分为两类：“近混合体”和“远混合体”。我们将近混合体定义为两个源之间距离小于十个像素的邮票，而将远混合体定义为两个源之间距离大于或等于十个像素的邮票。
- en: To avoid any spurious correlations, blended images are produced by simulating
    the galaxies individually and then artificially combining them. For simplicity,
    we only consider blends consisting of two sources. Real images may well contain
    cases involving more than two sources as well as various artefacts that would
    make classification more complicated. We leave these issues to be addressed in
    future work and here focus on testing the applicability of transfer learning to
    our simple test case.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免任何虚假的相关性，混合图像是通过单独模拟星系然后人工组合它们产生的。为简单起见，我们仅考虑由两个源组成的混合体。真实图像可能包含涉及两个以上源的情况，以及各种伪影，这将使分类变得更加复杂。我们将这些问题留待未来的工作中解决，此处重点测试迁移学习在我们简单测试案例中的适用性。
- en: 2.2 The COSMOS catalogue
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 COSMOS 目录
- en: We use the Cosmological Evolution Survey (COSMOS, Scoville et al., [2007](#bib.bib31))
    catalogue as basis from which to derive our simplistic training data and more
    realistic testing data. COSMOS was chosen as it provides a representative sample
    of galaxies in terms of size, ellipticity, luminosity and morphology. COSMOS is
    a catalogue of Hubble Space Telescope observations of 1.64 $\mathrm{deg}^{2}$
    on the the sky with very accurate photometry and morphology. This catalogue contains
    high resolution images (0.05 arcsec/pixel) with a very small Point Spread Function
    (PSF) of 0.01 arcsec.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用宇宙学演化调查（COSMOS，Scoville 等人，[2007](#bib.bib31)）目录作为基础，以从中推导出我们的简化训练数据和更现实的测试数据。选择
    COSMOS 是因为它提供了在尺寸、椭圆率、亮度和形态学方面具有代表性的星系样本。COSMOS 是哈勃空间望远镜对天空 1.64 $\mathrm{deg}^{2}$
    区域的观测目录，具有非常准确的光度和形态学数据。该目录包含高分辨率图像（0.05 弧秒/像素），并具有非常小的点扩散函数（PSF）为 0.01 弧秒。
- en: In particular, we use a subset of this catalogue selected and processed for
    weak lensing purposes (Mandelbaum et al., [2012](#bib.bib23)). This subset of
    images have a negligible amount of noise and the PSF has been deconvolved. This
    makes it possible to re-sample the images on a larger pixel scale and to convolve
    them with a different PSF in order to mimic observations from another instrument.
    Finally, the galaxies have been fitted by either a Sérsic profile or a bulge +
    disc profile for which the bulge is represented by a De Vaucouleurs profile (de
    Vaucouleurs, [1948](#bib.bib9)) and the disc by an exponential profile (see Mandelbaum
    et al., [2012](#bib.bib23), for details).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们使用了一个为弱透镜目的而选定和处理的目录子集（Mandelbaum 等人，[2012](#bib.bib23)）。这些图像的噪声微不足道，PSF
    已被去卷积处理。这使得可以在更大的像素尺度上重新采样图像，并用不同的 PSF 卷积它们，以模拟来自另一个仪器的观察。最后，星系被拟合为 Sérsic 轮廓或一个
    bulge + disc 轮廓，其中 bulge 由 De Vaucouleurs 轮廓（de Vaucouleurs，[1948](#bib.bib9)）表示，disc
    由指数轮廓表示（有关详细信息，请参见 Mandelbaum 等人，[2012](#bib.bib23)）。
- en: The subset of processed COSMOS data has been made available through the Galsim
    software package (Rowe et al., [2015](#bib.bib27)). GalSim is widely used in the
    astrophysics community to simulate and manipulate galaxy images and was used extensively
    in several weak lensing challenges, such as GREAT3 (Mandelbaum et al., [2014](#bib.bib24)).
    The package provides all the tools necessary to work with COSMOS data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Galsim 软件包（Rowe 等人，[2015](#bib.bib27)）提供了处理后的 COSMOS 数据子集。GalSim 在天体物理学社区中被广泛用于模拟和处理星系图像，并在多个弱透镜挑战中得到了广泛应用，如
    GREAT3（Mandelbaum 等人，[2014](#bib.bib24)）。该软件包提供了处理 COSMOS 数据所需的所有工具。
- en: 2.3 Point spread function
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 点扩散函数
- en: The impulse response or PSF of an instrument encompasses all of the aberrations
    induced by the optical system along with other distortions arising from the atmosphere
    *etc.* The PSF induces blurring in observed images, which artificially increase
    the size of sources and can cause their light profiles to overlap creating blends.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 仪器的冲激响应或 PSF 包括了光学系统引起的所有像差以及大气*等*产生的其他失真。PSF 会导致观察到的图像模糊，从而人为地增大源的大小，并可能导致其光谱重叠，形成混合。
- en: We model the optical PSF as a Moffat profile with $\beta=4.765$ (Trujillo et al.,
    [2001](#bib.bib39)) and the PSF ellipticity is drawn from real optical variations
    of the Canada-France-Hawaii Telescope (CFHT) derived from real CFIS data (Guinot
    et al., [2021](#bib.bib14)). Atmospheric turbulence is modelled by a Kolmogorov
    profile (Tatarski, [2016](#bib.bib37)) with random ellipticity drawn from a Gaussian
    distribution with average $\mu=0$ and standard deviation $\sigma=0.01$. The final
    PSF is obtained by convolving the two models and has an average size of 0.65 arcsec.
    Given the relatively small size of the postage stamps, spatial variations of the
    PSF are neglected.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将光学 PSF 模型化为 Moffat 轮廓，$\beta=4.765$（Trujillo 等人，[2001](#bib.bib39)），PSF 的椭圆度来源于从实际
    CFIS 数据（Guinot 等人，[2021](#bib.bib14)）中得出的加拿大-法国-夏威夷望远镜（CFHT）的实际光学变化。大气湍流则通过 Kolmogorov
    轮廓（Tatarski，[2016](#bib.bib37)）进行建模，随机椭圆度来源于均值 $\mu=0$ 和标准差 $\sigma=0.01$ 的高斯分布。最终
    PSF 通过卷积这两个模型获得，平均大小为 0.65 角秒。鉴于邮票的相对小尺寸，PSF 的空间变化被忽略。
- en: This CFIS-like PSF model is simple but sufficiently realistic for the purposes
    of this work.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类似 CFIS 的 PSF 模型虽然简单，但足够真实，符合本工作的目的。
- en: 2.4 Parametric model training data
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 参数模型训练数据
- en: In order to keep our training samples as simple and generic as possible, we
    use a series of parametric models derived from fits to the COSMOS sample described
    in Sect. [2.2](#S2.SS2 "2.2 The COSMOS catalogue ‣ 2 Data ‣ Deep Transfer Learning
    for Blended Source Identification in Galaxy Survey Data"). This constitutes a
    range of simulated galaxies with different sizes, shapes and ellipticities. The
    parametric models are then convolved with the CFIS-like PSF described in Sect. [2.3](#S2.SS3
    "2.3 Point spread function ‣ 2 Data ‣ Deep Transfer Learning for Blended Source
    Identification in Galaxy Survey Data"). Each image corresponds to a $51\times
    51$ pixel postage stamp with a convolved galaxy model at the centre.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的训练样本尽可能简单和通用，我们使用了一系列来自 COSMOS 样本的参数模型，这些模型在第 [2.2](#S2.SS2 "2.2 The COSMOS
    catalogue ‣ 2 Data ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data") 节中描述。它们包括不同大小、形状和椭圆度的模拟星系。这些参数模型随后与第 [2.3](#S2.SS3 "2.3
    Point spread function ‣ 2 Data ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data") 节中描述的类似 CFIS 的 PSF 进行卷积。每幅图像对应一个 $51\times 51$ 像素的邮票，中间为卷积星系模型。
- en: For half of the sample a second galaxy model is placed at a random position
    within the postage stamp to produce blends according to the definition provided
    in Sect. [2.1](#S2.SS1 "2.1 Blend definition ‣ 2 Data ‣ Deep Transfer Learning
    for Blended Source Identification in Galaxy Survey Data"). We pad all of the images
    with zeros (7 pixels in every direction) to avoid issues when the secondary source
    is close to the border of the postage stamp.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于样本的一半，我们在邮票中随机位置放置第二个星系模型，以根据第[2.1节](#S2.SS1 "2.1 Blend definition ‣ 2 Data
    ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey Data")中提供的定义生成混合图像。我们用零（每个方向7像素）填充所有图像，以避免当第二个源接近邮票边缘时出现问题。
- en: The complete sample is comprised of 80 000 noiseless, padded postage stamps,
    half of which are isolated galaxies and the other half are blends.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的样本由80,000个无噪声、填充的邮票组成，其中一半是孤立的星系，另一半是混合的。
- en: The final step in producing our simulated training sets is to add Gaussian random
    noise. In order to test how sensitive blend identification is to noise, we generate
    7 different noise standard deviations ($\sigma_{\mathrm{noise}}=5,10,15,20,25,30,35$).
    We additionally created 10 realisations of the noise for each value of $\sigma_{\mathrm{noise}}$
    in order to test the stability of the training. Each realisation of each noise
    level is treated as an independent training set. In other words, we train the
    network 70 times and obtain 70 sets of weights.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 生成我们模拟训练集的最后一步是添加高斯随机噪声。为了测试混合识别对噪声的敏感度，我们生成了7种不同的噪声标准差（$\sigma_{\mathrm{noise}}=5,10,15,20,25,30,35$）。我们还为每个$\sigma_{\mathrm{noise}}$值创建了10次噪声实现，以测试训练的稳定性。每个噪声水平的每个实现都被视为独立的训练集。换句话说，我们训练网络70次，获得70组权重。
- en: 2.5 Realistic testing data
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 现实测试数据
- en: We generate a sample of realistic CFIS-like images to test our transfer learning
    approach. To do so, we take the real denoised and deconvolved COSMOS images described
    in Sect. [2.2](#S2.SS2 "2.2 The COSMOS catalogue ‣ 2 Data ‣ Deep Transfer Learning
    for Blended Source Identification in Galaxy Survey Data"), crop them to $51\times
    51$ pixel postage stamps and convolve them with the CFIS-like PSF described in
    Sect. [2.3](#S2.SS3 "2.3 Point spread function ‣ 2 Data ‣ Deep Transfer Learning
    for Blended Source Identification in Galaxy Survey Data"). The flux is rescaled
    in order to reproduce a 300s exposure at the 3.6m CFHT telescope. The images are
    re-sampled at the resolution of the CFIS survey (0.187 arcsec). Similarly to the
    training images, blends are created for half of the sample by adding a secondary
    source at a random position in the postage stamp and then all the postage stamps
    are zero padded in the same way. Finally, Gaussian noise with $\sigma_{\mathrm{noise}}=14.5$
    was added to the images to replicate the SNR of CFIS data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成了一个现实的CFIS类似图像样本来测试我们的迁移学习方法。为此，我们采用了真实的去噪和去卷积的COSMOS图像，这些图像在第[2.2节](#S2.SS2
    "2.2 The COSMOS catalogue ‣ 2 Data ‣ Deep Transfer Learning for Blended Source
    Identification in Galaxy Survey Data")中描述，将其裁剪为$51\times 51$像素的邮票，并用第[2.3节](#S2.SS3
    "2.3 Point spread function ‣ 2 Data ‣ Deep Transfer Learning for Blended Source
    Identification in Galaxy Survey Data")中描述的CFIS类似PSF进行卷积。通量被重新缩放以再现3.6米CFHT望远镜下的300秒曝光。图像以CFIS调查的分辨率（0.187弧秒）进行重采样。与训练图像类似，样本的一半通过在邮票中随机位置添加第二个源来创建混合图像，然后所有邮票都以相同方式进行零填充。最后，向图像中添加了$\sigma_{\mathrm{noise}}=14.5$的高斯噪声，以复制CFIS数据的信噪比。
- en: The final sample of $80\leavevmode\nobreak\ 000$ realistic postage stamps is
    considerably more complex than the parametric models described in Sect. [2.4](#S2.SS4
    "2.4 Parametric model training data ‣ 2 Data ‣ Deep Transfer Learning for Blended
    Source Identification in Galaxy Survey Data") and more closely approximate the
    conditions expected in real images. For example, the light profiles do not necessarily
    have central symmetry due to star forming regions or because of complex morphology.
    Out of the $80\leavevmode\nobreak\ 000$ postage stamps $4\leavevmode\nobreak\
    838$ correspond to close blends (*i.e.* the sources are less than ten pixels apart)
    and the remaining $35\leavevmode\nobreak\ 162$ correspond to distant blends.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最终样本的$80\leavevmode\nobreak\ 000$个现实邮票比第[2.4节](#S2.SS4 "2.4 Parametric model
    training data ‣ 2 Data ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data")中描述的参数模型复杂得多，更接近于真实图像中预期的条件。例如，由于恒星形成区域或复杂的形态，光谱轮廓不一定具有中心对称性。在$80\leavevmode\nobreak\
    000$个邮票中，有$4\leavevmode\nobreak\ 838$个对应于近距离混合（*即*，源之间的距离小于十个像素），其余$35\leavevmode\nobreak\
    162$个对应于远距离混合。
- en: Table [1](#S2.T1 "Table 1 ‣ 2.5 Realistic testing data ‣ 2 Data ‣ Deep Transfer
    Learning for Blended Source Identification in Galaxy Survey Data") summarises
    all of the data sets used for training and testing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S2.T1 "表 1 ‣ 2.5 现实测试数据 ‣ 2 数据 ‣ 深度迁移学习在银河测量数据中的混合源识别") 总结了用于训练和测试的所有数据集。
- en: 'Table 1: Summary of the data used for training and testing.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：用于训练和测试的数据总结。
- en: '| Data set | Parametric Training Set | Realistic Testing Set |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 数据集 | 参数训练集 | 现实测试集 |'
- en: '| --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| $\mathrm{N}_{\mathrm{isolated}}$ | 40 000 | 40 000 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| $\mathrm{N}_{\mathrm{isolated}}$ | 40 000 | 40 000 |'
- en: '| $\mathrm{N}_{\mathrm{blended}}$ | 40 000 | 40 000 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| $\mathrm{N}_{\mathrm{blended}}$ | 40 000 | 40 000 |'
- en: '| $\sigma_{\mathrm{noise}}$ | 5, 10, 15, 20, 25, 30, 35 | $14.5$ |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| $\sigma_{\mathrm{noise}}$ | 5, 10, 15, 20, 25, 30, 35 | $14.5$ |'
- en: '| $\mathrm{N}_{\mathrm{\sigma\ real}}$ | 10 | 1 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| $\mathrm{N}_{\mathrm{\sigma\ real}}$ | 10 | 1 |'
- en: $\mathrm{N}_{\mathrm{isolated}}$ is the number postage stamps containing isolated
    sources, $\mathrm{N}_{\mathrm{blended}}$ is the number of postage stamps containing
    blended sources, $\sigma_{\mathrm{noise}}$ is the amount of noise added to the
    postage stamps and $\mathrm{N}_{\mathrm{\sigma\ real}}$ is the number of realisations
    for each noise level.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathrm{N}_{\mathrm{isolated}}$ 是包含孤立源的邮票数量，$\mathrm{N}_{\mathrm{blended}}$
    是包含混合源的邮票数量，$\sigma_{\mathrm{noise}}$ 是添加到邮票中的噪声量，$\mathrm{N}_{\mathrm{\sigma\
    real}}$ 是每个噪声水平的实现次数。
- en: 3 Deep transfer learning framework for blend identification
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 混合体识别的深度迁移学习框架
- en: The objective of transfer learning is to learn a set of weights from a given
    training set and then transfer these to a related but independent problem. This
    is interesting from the perspective of blend identification given that we do not
    have a large sample of labelled data and training with a small sample of known
    blends may significantly bias the learned weights. Additionally, the use of pre-trained
    weights considerably reduces the time required to train a network, thereby making
    the application of a deep learning approach to galaxy survey data more feasible.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的目标是从给定的训练集学习一组权重，然后将这些权重转移到相关但独立的问题中。从混合体识别的角度来看，这很有趣，因为我们没有大量标记数据样本，用少量已知混合体样本进行训练可能会显著偏倚学习到的权重。此外，使用预训练权重显著减少了训练网络所需的时间，从而使深度学习方法在银河测量数据中的应用变得更加可行。
- en: For the purposes of this work we test the applicability of transfer learning
    to the problem of blend identification in a two-step process. In the first step,
    CNN weights learned from natural images are used to extract generic features from
    the galaxy images. The fully connected layers are then trained on galaxy image
    features derived from simple parametric models. The objective being to capture
    the general properties of blended images and not specific features of the individual
    galaxies. In the second step, the weights learned from simple parametric models
    are applied to more realistic CFIS-like images to test the classification accuracy.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这项工作，我们在两步过程中测试迁移学习在混合体识别问题中的适用性。在第一步中，使用从自然图像中学习到的CNN权重来提取银河图像中的通用特征。然后，使用来自简单参数模型的银河图像特征来训练全连接层。目标是捕捉混合图像的一般特征，而不是单个银河的特定特征。在第二步中，将从简单参数模型中学习到的权重应用于更现实的类似CFIS的图像，以测试分类准确性。
- en: The architecture we choose to implement our deep transfer problem was that of
    VGG-16\. For simplicity we refer to our specific VGG-16 set up for the problem
    of blend identification as BlendHunter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择实现深度迁移问题的架构是 VGG-16。为简便起见，我们将针对混合体识别问题的特定 VGG-16 设置称为 BlendHunter。
- en: 3.1 The VGG-16 network
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 VGG-16 网络
- en: VGG-16 is a deep convolutional network with 16 weight layers developed by the
    Visual Geometry Group (VGG) at the University of Oxford (Simonyan & Zisserman,
    [2014](#bib.bib33)). The network was ranked first in the ImageNet Large-Scale
    Visual Recognition Challenge (ILSVRC) in 2014 (Russakovsky et al., [2015](#bib.bib28)).
    The main feature of this architecture was the increased depth of the network compared
    to the state of the art at the time. In VGG-16, three-channel images (RGB) are
    passed through 5 blocks of convolutional layers, where each block is composed
    of increasing numbers of $3\times 3$ filters. The stride (i.e. the amount by which
    the filter is shifted) is fixed to 1, while the convolutional layer inputs are
    padded such that the spatial resolution is preserved after convolution (i.e. the
    padding is 1 pixel for $3\times 3$ filters). The blocks are separated by max-pooling
    (i.e. down-sampling) layers. Max-pooling is performed over $2\times 2$ windows
    with stride 2\. The 5 blocks of convolutional layers are followed by three fully-connected
    layers. The final layer is a soft-max layer that outputs class probabilities.
    The full network architecture used is shown in Fig. [1](#S3.F1 "Figure 1 ‣ 3.1
    The VGG-16 network ‣ 3 Deep transfer learning framework for blend identification
    ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey Data").
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-16是由牛津大学视觉几何组（VGG）开发的一个深度卷积网络，具有16层权重（Simonyan & Zisserman，[2014](#bib.bib33)）。该网络在2014年ImageNet大规模视觉识别挑战赛（ILSVRC）中排名第一（Russakovsky
    et al., [2015](#bib.bib28)）。该架构的主要特点是相较于当时的最先进技术，其网络深度的增加。在VGG-16中，三通道图像（RGB）通过5个卷积层块，每个块由数量递增的$3\times
    3$滤波器组成。步幅（即滤波器的移动量）固定为1，而卷积层输入通过填充来保持卷积后的空间分辨率（即$3\times 3$滤波器的填充为1像素）。这些块由最大池化（即下采样）层分隔。最大池化在$2\times
    2$窗口上进行，步幅为2。5个卷积层块后接三个全连接层。最终层是一个soft-max层，输出类别概率。完整的网络架构见图[1](#S3.F1 "Figure
    1 ‣ 3.1 The VGG-16 network ‣ 3 Deep transfer learning framework for blend identification
    ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey Data")。
- en: '![Refer to caption](img/059204b9192d5b3b6a59dfe3d687d80b.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/059204b9192d5b3b6a59dfe3d687d80b.png)'
- en: 'Figure 1: Visual representation of the VGG-16 network. Convolutional layers
    with ReLU activation are shown in solid blue, max pooling layers are shown in
    brick-pattern red, fully connected layers are shown as green bars and the output
    softmax layer is shown as the last box in pink.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：VGG-16网络的视觉表示。带有ReLU激活的卷积层显示为实心蓝色，最大池化层显示为砖纹红色，全连接层显示为绿色条，输出softmax层显示为最后的粉色框。
- en: The VGG-16 network was chosen for purposes of the work presented here for several
    reasons. Firstly, the network can be implemented with weights pre-trained on the
    ImageNet database (Deng et al., [2009](#bib.bib10)) in order to save computation
    time and resources. The diversity of this data set has allowed the network to
    learn a variety of generic image features, which are applicable to most image
    classification tasks. Finally, VGG-16 has already been applied to a variety of
    astrophysical applications, including galaxy morphology classification (Wu et al.,
    [2019](#bib.bib41); Zhu et al., [2019](#bib.bib42)), glitch classification in
    gravitational wave data (George et al., [2018](#bib.bib13)) and coronagraph image
    classification (Shan et al., [2020](#bib.bib32)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 选择VGG-16作为此处工作的目的有几个原因。首先，该网络可以使用在ImageNet数据库（Deng et al., [2009](#bib.bib10)）上预训练的权重，从而节省计算时间和资源。这个数据集的多样性使得网络能够学习各种通用图像特征，这些特征适用于大多数图像分类任务。最后，VGG-16已经被应用于各种天体物理学应用，包括星系形态分类（Wu
    et al., [2019](#bib.bib41); Zhu et al., [2019](#bib.bib42)）、引力波数据中的故障分类（George
    et al., [2018](#bib.bib13)）和日冕仪图像分类（Shan et al., [2020](#bib.bib32)）。
- en: The VGG-16 network was implemented in Python via the TensorFlow-Keras neural
    network API (Chollet et al., [2015](#bib.bib7); Abadi et al., [2015](#bib.bib1)),
    which includes ImageNet pre-trained weights.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: VGG-16网络通过TensorFlow-Keras神经网络API（Chollet et al., [2015](#bib.bib7); Abadi et
    al., [2015](#bib.bib1)）在Python中实现，该API包含ImageNet预训练权重。
- en: 3.2 Training and validation
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 训练与验证
- en: 'We trained BlendHunter in two phases using the 70 sets of 80 000 simulated
    images described in Sect. [2.4](#S2.SS4 "2.4 Parametric model training data ‣
    2 Data ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey
    Data"). We divided each data set in the follow way: 36 000 images were used for
    training, 36 000 for validation and the remaining 8 000 for testing.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '-   我们使用70组80,000个模拟图像（详见第2.4节）对BlendHunter进行了两阶段训练，将每个数据集划分如下：36,000张图像用于训练，36,000张用于验证，剩余的8,000张用于测试。'
- en: 3.2.1 Convolutional layers
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '-   卷积层（Convolutional layers）'
- en: In the first phase, the we simply initialise the convolutional layers (*i.e.*
    the first 13 weight layers) with the pre-trained ImageNet weights. As mentioned
    in the previous section, the purpose of using pre-trained weights is to save processing
    time. Therefore, this part of the network is not actually trained with our data
    and can be seen as a simple feature extractor.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '-   在第一阶段中，我们仅仅使用预训练的ImageNet权重初始化了卷积层（即前13个权重层）。正如前一节所述，使用预训练权重的目的是为了节省处理时间。因此，网络的这一部分实际上没有使用我们的数据进行训练，可以看作是一个简单的特征提取器。'
- en: As ImageNet weights are used, the VGG-16 network expects RGB images as inputs.
    Therefore, we rescaled the simulated images to the range $i\in\mathbb{Z}:0\leq
    i\leq 255$ and the monochromatic pixel values were repeated across the 3 RGB channels.
    The final images are saved as Portable Network Graphics (PNG) files which are
    fed into the network.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '-   由于使用了ImageNet权重，VGG-16网络期望以RGB图像作为输入。因此，我们将模拟图像重新缩放到范围$i\in\mathbb{Z}:0\leq
    i\leq 255$，并且单色像素值在3个RGB通道上重复。最终的图像保存为Portable Network Graphics（PNG）文件，然后输入到网络中。'
- en: Given the relatively small amount of training data, we additionally implemented
    data augmentation in order to obtain more diversified features from the convolutional
    layers of the network. Augmenting our simulated images simply means creating additional
    images with minor changes such as flips, translations or rotations. Specifically,
    we used the Keras image pre-processing modules to include a shear range and zoom
    range of $0.2$, as well as horizontal flipping. Note that data augmentation was
    not used on the test images.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于训练数据相对较少，我们额外实施了数据增强，以便从网络的卷积层中获得更多多样化的特征。增加我们的模拟图像简单地意味着创建具有轻微变化（如翻转、平移或旋转）的额外图像。具体而言，我们使用了Keras图像预处理模块，包括剪切范围和缩放范围为$0.2$，以及水平翻转。请注意，测试图像未使用数据增强。
- en: Fig. [2](#S3.F2 "Figure 2 ‣ 3.2.1 Convolutional layers ‣ 3.2 Training and validation
    ‣ 3 Deep transfer learning framework for blend identification ‣ Deep Transfer
    Learning for Blended Source Identification in Galaxy Survey Data") shows some
    examples of features extracted from on of the simulated images with the convolutional
    layers. The earlier convolutional blocks provide more general features while the
    later blocks provide more specific features.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '-   图2（详见第3.2.1节）显示了从一个模拟图像中提取的特征示例，使用了卷积层。早期的卷积块提供更一般的特征，而后期的块提供更具体的特征。'
- en: '![Refer to caption](img/8feb99d813596d03a61d4f70eba27a86.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/8feb99d813596d03a61d4f70eba27a86.png)'
- en: 'Figure 2: Examples of features extracted from the VGG-16 convolutional layers
    pre-trained with ImageNet weights. The leftmost panel shows an input postage stamp
    containing blended sources. The following panels show example features extracted
    from various convolution blocks.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '-   图2：从使用ImageNet权重预训练的VGG-16卷积层中提取的特征示例。最左侧面板显示包含混合源的输入邮票。接下来的面板显示从各种卷积块中提取的示例特征。'
- en: 3.2.2 Fully connected layers
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: '-   完全连接层（Fully connected layers）'
- en: In the second phase, we train the fully connected layers (*i.e.* the last 3
    weight layers) on top of the previously stored features. These layers consist
    of a layer with linear transformation, followed by a layer with dropout and finally
    a layer with Rectified Linear Unit (ReLu) activation. We chose this non-linear
    activation function, widely used with deep neural networks, because it allows
    for faster convergence. We used dropout alongside data augmentation to avoid over-fitting
    (Srivastava et al., [2014](#bib.bib35)). This is a way of cancelling some activations
    to prevent the network from learning random correlations in the images. We employed
    a dropout rate of $0.1$ for our experiments. For the training, we proceeded with
    batches of 250 images at a time over 500 epochs. An epoch corresponds to passing
    the entire training sample through the network and a batch size of 250 proved
    to be the most computationally efficient for the gradient descent.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二阶段，我们在之前存储的特征上训练完全连接层（*即*最后 3 个权重层）。这些层由一个线性变换层、一个 dropout 层和最后一个 ReLU 激活层组成。我们选择了这种广泛用于深度神经网络的非线性激活函数，因为它可以实现更快的收敛。我们使用了
    dropout 和数据增强来避免过拟合（Srivastava 等，[2014](#bib.bib35)）。这是一种取消一些激活以防止网络学习图像中的随机相关性的方式。我们在实验中使用了
    $0.1$ 的 dropout 率。训练时，我们以 250 张图像为一个批次进行，训练 500 个周期。一个周期对应于通过网络处理整个训练样本，而批次大小为
    250 被证明在梯度下降计算上最为高效。
- en: We use the binary cross entropy loss function in Eq. [1](#S3.E1 "In 3.2.2 Fully
    connected layers ‣ 3.2 Training and validation ‣ 3 Deep transfer learning framework
    for blend identification ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data")
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在公式 [1](#S3.E1 "在 3.2.2 完全连接层 ‣ 3.2 训练与验证 ‣ 3 深度迁移学习框架用于混合识别 ‣ 深度迁移学习用于银河调查数据中的混合源识别")
    中使用了二元交叉熵损失函数。
- en: '{ceqn}'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '{ceqn}'
- en: '|  | $\displaystyle\mathrm{BCE}=-\frac{1}{N}\sum_{i=1}^{N}y_{i}\cdot\log(p(y_{i}))+(1-y_{i})\cdot\log(1-p(y_{i}))$
    |  | (1) |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathrm{BCE}=-\frac{1}{N}\sum_{i=1}^{N}y_{i}\cdot\log(p(y_{i}))+(1-y_{i})\cdot\log(1-p(y_{i}))$
    |  | (1) |'
- en: where $y_{i}$ is the true label (1 for blended, 0 otherwise) for the $i^{th}$
    image and $p(y_{i})$ is the probability of the image being blended or not returned
    by the network. We selected this loss function to optimise as it is typically
    used in this type of binary classification problem.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $y_{i}$ 是第 $i^{th}$ 张图像的真实标签（混合为 1，非混合为 0），$p(y_{i})$ 是网络返回的图像被混合的概率。我们选择了这个损失函数进行优化，因为它通常用于这种类型的二元分类问题。
- en: The Adam algorithm (Kingma & Ba, [2014](#bib.bib18)) was used to minimise our
    loss function. In contrast to a classic Stochastic Gradient Descent (SGD) algorithm,
    it computes individual learning rates for each weight and is quite efficient in
    the optimisation of deep neural networks. In order to adapt the learning rate,
    Adam requires the estimations of the first (the mean) and second (the uncentred
    variance) moments of the gradient using exponentially moving averages (Eqs. [2](#S3.E2
    "In 3.2.2 Fully connected layers ‣ 3.2 Training and validation ‣ 3 Deep transfer
    learning framework for blend identification ‣ Deep Transfer Learning for Blended
    Source Identification in Galaxy Survey Data") and [3](#S3.E3 "In 3.2.2 Fully connected
    layers ‣ 3.2 Training and validation ‣ 3 Deep transfer learning framework for
    blend identification ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data"), respectively), with $g_{t}$ the objective function gradient
    at time step $t$, and $\beta_{1}$ and $\beta_{2}$ the exponential decay rates
    for the moment estimates.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用了 Adam 算法（Kingma & Ba，[2014](#bib.bib18)）来最小化我们的损失函数。与经典的随机梯度下降（SGD）算法相比，它为每个权重计算单独的学习率，并且在优化深度神经网络方面非常高效。为了调整学习率，Adam
    需要通过指数移动平均来估算梯度的第一个（均值）和第二个（非中心方差）矩量（分别见公式 [2](#S3.E2 "在 3.2.2 完全连接层 ‣ 3.2 训练与验证
    ‣ 3 深度迁移学习框架用于混合识别 ‣ 深度迁移学习用于银河调查数据中的混合源识别") 和 [3](#S3.E3 "在 3.2.2 完全连接层 ‣ 3.2
    训练与验证 ‣ 3 深度迁移学习框架用于混合识别 ‣ 深度迁移学习用于银河调查数据中的混合源识别")），其中 $g_{t}$ 是时间步 $t$ 的目标函数梯度，$\beta_{1}$
    和 $\beta_{2}$ 是矩量估计的指数衰减率。
- en: '{ceqn}'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '{ceqn}'
- en: '|  | $\displaystyle m_{t}=\beta_{1}\cdot m_{t-1}+(1-\beta_{1})\cdot g_{t}$
    |  | (2) |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle m_{t}=\beta_{1}\cdot m_{t-1}+(1-\beta_{1})\cdot g_{t}$
    |  | (2) |'
- en: '{ceqn}'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '{ceqn}'
- en: '|  | $\displaystyle v_{t}=\beta_{2}\cdot v_{t-1}+(1-\beta_{2})\cdot g_{t}^{2}$
    |  | (3) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle v_{t}=\beta_{2}\cdot v_{t-1}+(1-\beta_{2})\cdot g_{t}^{2}$
    |  | (3) |'
- en: It takes advantage of both RMSprop (Tieleman & Hinton, [2012](#bib.bib38)) and
    AdaGrad (Duchi et al., [2011](#bib.bib11)) methods. In addition to being robust
    and less time-consuming, Adam can thus be applied to a wider selection of optimisation
    problems. It also requires almost no tuning of its parameters. We set $\beta_{1}$
    and $\beta_{2}$ to their default values, respectively 0.9 and 0.999\. Finally,
    the weights are updated according to Eq. [4](#S3.E4 "In 3.2.2 Fully connected
    layers ‣ 3.2 Training and validation ‣ 3 Deep transfer learning framework for
    blend identification ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data"), where $w_{t}$ are the fully-connected network weights
    at time step $t$, $\eta$ is the step size, $\hat{m_{t}}$ and $\hat{v_{t}}$ are
    the bias corrected estimators of the first and second moments, and $\epsilon$
    a value set to $10^{-8}$ to prevent division by 0.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 它利用了 RMSprop（Tieleman & Hinton，[2012](#bib.bib38)）和 AdaGrad（Duchi et al.，[2011](#bib.bib11)）方法。除了具有鲁棒性和较少的时间消耗外，Adam
    还可以应用于更广泛的优化问题。此外，它几乎不需要调整其参数。我们将 $\beta_{1}$ 和 $\beta_{2}$ 分别设置为默认值 0.9 和 0.999。最后，权重根据
    Eq. [4](#S3.E4 "In 3.2.2 Fully connected layers ‣ 3.2 Training and validation
    ‣ 3 Deep transfer learning framework for blend identification ‣ Deep Transfer
    Learning for Blended Source Identification in Galaxy Survey Data") 更新，其中 $w_{t}$
    是时间步 $t$ 的全连接网络权重，$\eta$ 是步长，$\hat{m_{t}}$ 和 $\hat{v_{t}}$ 是一阶和二阶矩的偏差校正估计值，$\epsilon$
    是设置为 $10^{-8}$ 的值，以防止除以 0。
- en: '{ceqn}'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '{ceqn}'
- en: '|  | $\displaystyle w_{t}=w_{t-1}-\eta\cdot\frac{\hat{m_{t}}}{\sqrt{\hat{v_{t}}}+\epsilon}$
    |  | (4) |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle w_{t}=w_{t-1}-\eta\cdot\frac{\hat{m_{t}}}{\sqrt{\hat{v_{t}}}+\epsilon}$
    |  | (4) |'
- en: At the end of each epoch, both training and validation losses were computed,
    and the weights were updated every time the validation loss decreased. Training
    was stopped when the validation loss had not decreased after 10 epochs. The network
    converged after around 70 epochs on average.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个训练周期结束时，会计算训练和验证损失，并且每当验证损失减少时，都会更新权重。当验证损失在 10 个周期后没有减少时，训练被停止。网络平均在大约 70
    个周期后收敛。
- en: We began training with a learning rate of $1.10^{-3}$. Since choosing the right
    learning rate can be challenging, we decided to reduce the learning rate by a
    factor of 0.5 every time the validation loss did not decrease after 5 epochs.
    A small learning rate would make it possible to avoid big jumps in gradient descent.
    Otherwise, in this case, it could fail to converge and settle around a local minimum.
    No weight decay was implemented in this phase.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始训练时的学习率为 $1.10^{-3}$。由于选择合适的学习率可能具有挑战性，因此我们决定在验证损失在 5 个周期后未减少时，将学习率减少 0.5
    倍。较小的学习率可以避免梯度下降中的大幅跳跃。否则，在这种情况下，它可能无法收敛并停留在局部最小值。这个阶段没有实现权重衰减。
- en: Tuning deep neural networks hyperparameters with a considerable amount of parameters
    to learn can prove to be very time-consuming. This is why we focused on the hyperparameters
    that would have the most impact on the results. Several tests were made such as
    changes to the regularisation, weight initialisation, dropout rate, learning rate
    and optimiser. However, no significant improvement in accuracy was observed. Switching
    to the SGD optimiser or increasing the dropout rate led to worse performance overall.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 调整深度神经网络的超参数，尤其是学习参数的数量，可以非常耗时。因此，我们专注于对结果影响最大的超参数。进行了多个测试，例如更改正则化、权重初始化、丢弃率、学习率和优化器。然而，未观察到准确性的显著改善。切换到
    SGD 优化器或增加丢弃率导致整体性能变差。
- en: The network takes approximately 630s to train on a sample of 80 000 images using
    a standard Intel(R) Core(TM) i7-6900K CPU (3.20GHz, 32GB).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标准 Intel(R) Core(TM) i7-6900K CPU（3.20GHz，32GB），网络训练一个 80,000 张图像的样本大约需要 630
    秒。
- en: 4 Results
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 结果
- en: 4.1 SExtractor benchmark
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 SExtractor 基准测试
- en: We compare the performance of BlendHunter with SExtractor, as this is the most
    widely used tool in the community for identifying and handling blended sources
    in astronomical images. The objective being to test the reliability of our approach
    versus the state of the art. Specifically, We make use of the SExtractor Python
    wrapper SEP (Barbary, [2016](#bib.bib4)) for our tests. Note that SEP does many
    things beyond blend identification and many of these steps can not easily be isolated,
    however we tried to make the comparison as fair and consistent as possible.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将BlendHunter的性能与SExtractor进行比较，因为这是社区中最广泛使用的工具，用于识别和处理天文图像中的混合源。目标是测试我们的方法相对于现有技术的可靠性。具体来说，我们使用了SExtractor的Python包装器SEP（Barbary，[2016](#bib.bib4)）进行测试。请注意，SEP不仅仅用于混合识别，还有许多其他步骤，这些步骤不能轻易隔离，但我们尽力使比较尽可能公平和一致。
- en: 'SEP implements a multi-thresholding technique to decompose detected sources
    into sub-sources (when possible). This method takes two input parameters: the
    number of bins to decompose the light profile and the minimum contrast value between
    the main peak and a given sub-peak. The contrast is evaluated based on the flux
    of each peak (see Fig. 2 in Bertin & Arnouts ([1996](#bib.bib5))). The set of
    parameters we used for SEP is shown in Table [2](#S4.T2 "Table 2 ‣ 4.1 SExtractor
    benchmark ‣ 4 Results ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data") (all the other parameters are kept to their default values).
    The value of $0.005$ for DEBLEND_MINCONT (the minimum contrast parameter for deblending)
    may be considered a little high compared to that commonly used in the literature,
    but here we chose to favour reliable identification over increasing the number
    of blends found at the cost of also increasing the number of spurious detections.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: SEP实施了一种多阈值技术，将检测到的源分解为子源（如果可能的话）。该方法需要两个输入参数：分解光谱轮廓的箱数和主峰与给定子峰之间的最小对比度值。对比度是基于每个峰的通量进行评估的（见Bertin
    & Arnouts（[1996](#bib.bib5)）的图2）。我们为SEP使用的参数集显示在表[2](#S4.T2 "表 2 ‣ 4.1 SExtractor基准
    ‣ 4 结果 ‣ 深度迁移学习用于星系调查数据中的混合源识别")中（所有其他参数保持默认值）。DEBLEND_MINCONT的值为$0.005$（去混合的最小对比度参数），与文献中常用的值相比可能有点高，但在这里我们选择了可靠的识别，而不是增加混合源的数量，同时增加虚假检测的数量。
- en: 'Table 2: SEP parameter settings.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：SEP参数设置。
- en: '| Parameter | Value |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 值 |'
- en: '| --- | --- |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| THRESH_TYPE | RELATIVE |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| THRESH_TYPE | RELATIVE |'
- en: '| DETECT_THRESH | 1.5 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| DETECT_THRESH | 1.5 |'
- en: '| DETECT_MINAREA | 5 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| DETECT_MINAREA | 5 |'
- en: '| FILTER | Y |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| FILTER | Y |'
- en: '| FILTER_NAME | kernel_3x3.conv (default) |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| FILTER_NAME | kernel_3x3.conv (默认) |'
- en: '| DEBLEND_NTHRESH | 32 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| DEBLEND_NTHRESH | 32 |'
- en: '| DEBLEND_MINCONT | 0.005 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| DEBLEND_MINCONT | 0.005 |'
- en: Given our loose definition of blended sources (see Sect. [2.1](#S2.SS1 "2.1
    Blend definition ‣ 2 Data ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data")), we chose not to rely exclusively on the deblending flags
    provided by SEP. Instead, we also check that the sources are found at the right
    positions (within a two pixel radius) to make sure we do not extract noise features.
    Additionally, since some of the sources in the postage stamp do not technically
    overlap, when SEP correctly identifies the number of sources (*i.e.* 1 or 2) we
    take this as a correctly labelled postage stamp.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们对混合源的宽松定义（见第[2.1](#S2.SS1 "2.1 混合定义 ‣ 2 数据 ‣ 深度迁移学习用于星系调查数据中的混合源识别")节），我们选择不完全依赖于SEP提供的去混合标志。相反，我们还检查源是否位于正确的位置（在两个像素半径内），以确保我们没有提取噪声特征。此外，由于邮票中的一些源在技术上并不重叠，当SEP正确识别源的数量（*即*
    1 或 2）时，我们将其视为正确标记的邮票。
- en: 'The process for labelling postage stamps as either isolated or blended sources
    using SEP can be summarised as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SEP将邮票标记为孤立源或混合源的过程可以总结如下：
- en: 'If a single source is detected:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检测到单个源：
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the source is flagged as a blend and is at the expected position, the image
    is labelled as a blend.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果源被标记为混合且位于预期位置，则图像被标记为混合。
- en: •
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Otherwise, the image is labelled as an isolated source.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 否则，图像将被标记为孤立源。
- en: 'If two sources are detected:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检测到两个源：
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If both sources are detected at the right positions, the image is labelled as
    a blend (this stands even if one of the two sources is itself detected as a blend).
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果两个源都在正确的位置被检测到，图像被标记为混合（即使这两个源中的一个本身被检测为混合，也适用）。
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the sources are incorrectly identified, the image is labelled as an isolated
    source.
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果源被错误识别，则图像被标记为孤立源。
- en: 'If more than two sources are detected:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果检测到的源数量超过两个：
- en: •
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If at least the right number of sources are detected at their expected positions,
    the image is labelled as a blend.
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果至少在预期的位置检测到正确数量的源，则图像被标记为混合源。
- en: •
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: If the number of detected sources is larger than the number of true sources,
    the image is flagged as a SEP failure and is not included in the analysis. Note
    that this occurred for less than $1\%$ of the images.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果检测到的源数量大于实际源数量，则图像被标记为SEP失败，并且不包括在分析中。请注意，这种情况发生在不到$1\%$的图像中。
- en: '![Refer to caption](img/44d528a6784553608bedd699020a03eb.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![请参阅说明](img/44d528a6784553608bedd699020a03eb.png)'
- en: 'Figure 3: Classification accuracy of BlendHunter (black solid line) versus
    SEP (blue dashed line) as a function of separation between sources. Results are
    from the blended samples of the COSMOS parametric model testing sets. Each panel
    shows one realisation of a given noise standard deviation, $\sigma_{\mathrm{noise}}$.
    The blue shaded area shows the gain in accuracy of BlendHunter with respect to
    SEP.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：BlendHunter（黑色实线）与SEP（蓝色虚线）在源之间分离函数下的分类准确率。结果来自COSMOS参数模型测试集的混合样本。每个面板显示给定噪声标准偏差$\sigma_{\mathrm{noise}}$的一个实现。蓝色阴影区域显示了BlendHunter相对于SEP的准确率提升。
- en: 4.2 Results from parametric models
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 参数模型结果
- en: In the following, we define classification accuracy as the percentage of postage
    stamps correctly labelled as either isolated or blended sources. Here we take
    the weights obtained via the training procedure described in [3.2](#S3.SS2 "3.2
    Training and validation ‣ 3 Deep transfer learning framework for blend identification
    ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey Data")
    and apply them to the test sample of $8\leavevmode\nobreak\ 000$ parametric model
    postage stamps (see Sect. [2.4](#S2.SS4 "2.4 Parametric model training data ‣
    2 Data ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey
    Data")) to obtain classification labels.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下文中，我们将分类准确率定义为正确标记为孤立或混合源的邮票的百分比。这里我们采用通过[3.2](#S3.SS2 "3.2 Training and validation
    ‣ 3 Deep transfer learning framework for blend identification ‣ Deep Transfer
    Learning for Blended Source Identification in Galaxy Survey Data")所述训练过程获得的权重，并将其应用于$8\leavevmode\nobreak\
    000$个参数模型邮票的测试样本（参见第[2.4](#S2.SS4 "2.4 Parametric model training data ‣ 2 Data
    ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey Data")节）以获得分类标签。
- en: Fig. [3](#S4.F3 "Figure 3 ‣ 4.1 SExtractor benchmark ‣ 4 Results ‣ Deep Transfer
    Learning for Blended Source Identification in Galaxy Survey Data") shows the classification
    accuracy of BlendHunter (black solid line) versus SEP (blue dashed line) as a
    function of separation between sources. Results are from the blended samples of
    the COSMOS parametric model testing sets (*i.e.* sets of $4\leavevmode\nobreak\
    000$ postage stamps). Each panel shows one realisation of a given noise standard
    deviation, $\sigma_{\mathrm{noise}}$. The blue shaded area shows the gain in accuracy
    of BlendHunter with respect to SEP. For close blends (*i.e.* where the two sources
    are less than 10 pixels apart), BlendHunter significantly outperforms SEP showing
    gains as high as $\sim 80\%$ in classification accuracy. For distant blends, the
    two techniques appear to be consistent to within a few percent and correctly label
    the majority of the postage stamps.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图[3](#S4.F3 "Figure 3 ‣ 4.1 SExtractor benchmark ‣ 4 Results ‣ Deep Transfer
    Learning for Blended Source Identification in Galaxy Survey Data")显示了BlendHunter（黑色实线）与SEP（蓝色虚线）在源之间分离函数下的分类准确率。结果来自COSMOS参数模型测试集的混合样本（*即*
    $4\leavevmode\nobreak\ 000$邮票的集合）。每个面板显示给定噪声标准偏差$\sigma_{\mathrm{noise}}$的一个实现。蓝色阴影区域显示了BlendHunter相对于SEP的准确率提升。对于接近的混合源（*即*
    两个源之间距离小于10像素），BlendHunter显著优于SEP，分类准确率提升高达$\sim 80\%$。对于远离的混合源，这两种技术的表现一致，且正确标记了大多数邮票。
- en: In Fig. [4](#S4.F4 "Figure 4 ‣ 4.2 Results from parametric models ‣ 4 Results
    ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey Data")
    we display the overall accuracy as function of noise standard deviation for BlendHunter
    and SEP on the full parametric model testing set (*i.e.* sets of $8\leavevmode\nobreak\
    000$ postage stamps). The points are taken from the average classification accuracy
    from the ten noise realisations and the error bars from the standard deviation.
    As expected, both approaches perform almost perfectly for low noise levels and
    drop off as the noise increases. Overall the two approaches appear fairly consistent,
    however BlendHunter drops off less rapidly for higher noise levels indicating
    it may be slightly more robust in higher noise regimes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在图[4](#S4.F4 "Figure 4 ‣ 4.2 Results from parametric models ‣ 4 Results ‣ Deep
    Transfer Learning for Blended Source Identification in Galaxy Survey Data")中，我们展示了BlendHunter和SEP在整个参数模型测试集（*即*
    $8\leavevmode\nobreak\ 000$张邮票）上的噪声标准偏差的总体准确度函数。数据点来自十次噪声实现的平均分类准确度，误差条来自标准偏差。正如预期的那样，两种方法在低噪声水平下表现几乎完美，随着噪声的增加表现下降。总体而言，这两种方法表现较为一致，但BlendHunter在较高噪声水平下的下降速度较慢，表明它在高噪声环境中可能稍微更具鲁棒性。
- en: '![Refer to caption](img/d854ea22d39ca828bd2422430501fd9f.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/d854ea22d39ca828bd2422430501fd9f.png)'
- en: 'Figure 4: Overall classification accuracy of BlendHunter (black solid line)
    versus SEP (blue dashed line) with respect to $\sigma_{\mathrm{noise}}$ on the
    COSMOS parametric model testing set. The points are taken from the average accuracy
    from ten realisations of each noise level and the error bars from the standard
    deviation.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：BlendHunter（黑色实线）与SEP（蓝色虚线）在COSMOS参数模型测试集上的整体分类准确度，关于$\sigma_{\mathrm{noise}}$。数据点来自每个噪声水平的十次实现的平均准确度，误差条来自标准偏差。
- en: 4.3 Results from realistic CFIS-like images
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 逼真的CFIS-like图像结果
- en: '![Refer to caption](img/74bc0db9a6693206db8c159c454a6269.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/74bc0db9a6693206db8c159c454a6269.png)'
- en: 'Figure 5: Relative classification accuracy for BlendHunter with respect to
    SEP on the realistic CFIS-like postage stamps, with $\sigma_{\mathrm{noise}}=14.5$.
    The points are taken from the average relative classification accuracy from the
    ten training noise realisations and the error bars from the standard deviation.
    For close blends (red dot-dashed line), BlendHunter outperforms SEP for any level
    of noise in the training data.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：BlendHunter相对于SEP在逼真的CFIS-like邮票上的相对分类准确度，$\sigma_{\mathrm{noise}}=14.5$。数据点来自十次训练噪声实现的平均相对分类准确度，误差条来自标准偏差。对于接近的混合（红色点划线），BlendHunter在任何训练数据的噪声水平下均优于SEP。
- en: Here we test the weights obtained from the parametric models with various noise
    levels and apply them to the test sample of $80\leavevmode\nobreak\ 000$ realistic
    CFIS-like postage stamps (see Sect. [2.5](#S2.SS5 "2.5 Realistic testing data
    ‣ 2 Data ‣ Deep Transfer Learning for Blended Source Identification in Galaxy
    Survey Data")) to obtain classification labels.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们测试了从参数模型获得的权重在不同噪声水平下的表现，并将其应用于$80\leavevmode\nobreak\ 000$张逼真的CFIS-like邮票测试样本（见第[2.5节](#S2.SS5
    "2.5 Realistic testing data ‣ 2 Data ‣ Deep Transfer Learning for Blended Source
    Identification in Galaxy Survey Data")）以获得分类标签。
- en: 'Fig. [5](#S4.F5 "Figure 5 ‣ 4.3 Results from realistic CFIS-like images ‣ 4
    Results ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey
    Data") shows the relative classification accuracy as function of the training
    noise standard deviation (*i.e.* the noise level of the sample used for training)
    for BlendHunter with respect to SEP on the realistic CFIS-like postage stamps.
    Points show the average relative classification accuracy corresponding to the
    ten noise realisations for each training noise level and the error bars are the
    standard deviation. SEP is simply run once on the full data set. For the whole
    set of images (*i.e.* all $80\leavevmode\nobreak\ 000$ postage stamps), we can
    see that BlendHunter is able to slightly outperform SEP (only by a few percent)
    around $\sigma_{\mathrm{noise}}=14.5$, which is the true noise standard deviation
    of the CFIS-like sample. The classification accuracy then degrades if the network
    is trained on noise levels that differ significantly. In the subset of close blends
    ($4\leavevmode\nobreak\ 838$ postage stamps), BlendHunter significantly outperforms
    SEP regardless of the training conditions. This is consistent with the results
    from the parametric model data. Finally, for distant blends ($35\leavevmode\nobreak\
    162$ postage stamps), BlendHunter matches the performance of SEP when trained
    on the same noise level as the target images and under-performs by a maximum of
    $5\%$ when trained on a noise level that differs. The overall accuracy of SEP
    in each case is: $91\%$ over the whole data set, $69\%$ for close blends, and
    $94\%$ for distant blends.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 [5](#S4.F5 "Figure 5 ‣ 4.3 Results from realistic CFIS-like images ‣ 4 Results
    ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey Data")
    显示了 BlendHunter 相对于 SEP 在现实 CFIS-like 邮票上的分类精度相对于训练噪声标准偏差（*即*用于训练的样本噪声水平）的变化。点表示对应于每个训练噪声水平的十次噪声实现的平均相对分类精度，误差条是标准偏差。SEP
    只在完整数据集上运行一次。对于整个图像集（*即*所有 $80\leavevmode\nobreak\ 000$ 邮票），我们可以看到，BlendHunter
    在 $\sigma_{\mathrm{noise}}=14.5$ 时略微超越了 SEP（仅高出几个百分点），这是真实 CFIS-like 样本的噪声标准偏差。如果网络在噪声水平差异较大的情况下进行训练，分类精度会下降。在近混合的子集（$4\leavevmode\nobreak\
    838$ 邮票）中，无论训练条件如何，BlendHunter 都显著超越 SEP。这与参数模型数据的结果一致。最后，对于远距离混合（$35\leavevmode\nobreak\
    162$ 邮票），当 BlendHunter 在与目标图像相同的噪声水平上进行训练时，其性能与 SEP 相匹配，而在训练噪声水平不同的情况下最多表现为低 $5\%$。每种情况下
    SEP 的整体准确率为：整个数据集 $91\%$，近混合 $69\%$，远距离混合 $94\%$。
- en: These results indicate that BlendHunter is not overly sensitive to the training
    without perfect knowledge of the noise in the target sample. This is a strong
    indication that the transfer learning has been successful and promising for the
    prospect of applying this approach to real survey data. Additionally, the performance
    of BlendHunter is noticeably more robust for close blends, where galaxy profiles
    significantly overlap.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果表明，BlendHunter 对没有完美噪声知识的训练并不过于敏感。这强烈表明迁移学习在应用于实际调查数据的前景上取得了成功和有希望的进展。此外，BlendHunter
    在近混合中表现出明显更强的鲁棒性，其中银河系轮廓显著重叠。
- en: It should be stressed that these results are based on the relatively simple
    blend definition provided in section [2.1](#S2.SS1 "2.1 Blend definition ‣ 2 Data
    ‣ Deep Transfer Learning for Blended Source Identification in Galaxy Survey Data"),
    which does not take into account all of the complexities that could be encountered
    for real extended sources.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 应强调的是，这些结果基于第 [2.1](#S2.SS1 "2.1 Blend definition ‣ 2 Data ‣ Deep Transfer Learning
    for Blended Source Identification in Galaxy Survey Data") 节中提供的相对简单的混合定义，这并未考虑实际扩展源可能遇到的所有复杂性。
- en: We additionally examined the confusion matrices for the predictions from the
    realistic CFIS-like test set. Fig. [6](#S4.F6 "Figure 6 ‣ 4.3 Results from realistic
    CFIS-like images ‣ 4 Results ‣ Deep Transfer Learning for Blended Source Identification
    in Galaxy Survey Data") shows the fraction of blends correctly classified as blends
    (*i.e.* true positives) or isolated sources correctly classified as isolated sources
    (*i.e.* true negatives) as a function of the the training noise standard deviation.
    The first panel shows the performance of BlendHunter compared to SEP for the sample
    of close blends (green lines) along with an equivalent number (4 838) of isolated
    sources (red lines). The second panel shows the equivalent plot for distant blends
    with the remaining sample of isolated sources. The third panel shows the results
    for all 80 000 postage stamps.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还检查了来自现实 CFIS 类似测试集的预测混淆矩阵。图 [6](#S4.F6 "图 6 ‣ 4.3 现实 CFIS 类似图像的结果 ‣ 4 结果
    ‣ 深度迁移学习在银河系调查数据中的混合源识别") 显示了随着训练噪声标准差的变化，正确分类为混合源（*即* 真阳性）或正确分类为孤立源（*即* 真阴性）的混合源的比例。第一个面板显示了
    BlendHunter 相对于 SEP 对于接近混合源的样本的表现（绿色线条），以及等量的孤立源（红色线条）。第二个面板显示了远距离混合源的等效图，以及剩余的孤立源样本。第三个面板显示了所有
    80 000 个邮票的结果。
- en: The results show that BlendHunter is significantly better at identifying true
    blends than SEP regardless of the noise level used for training. This is particularly
    noticeable for the sample of close blends. BlendHunter, however, also produces
    more incorrect labels for isolated sources and this worsens as the training noise
    level increases beyond the that of the target sample.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，BlendHunter 在识别真实混合源方面显著优于 SEP，无论使用何种噪声水平进行训练。这在接近混合源的样本中尤为明显。然而，BlendHunter
    也对孤立源产生了更多的错误标签，并且这种情况随着训练噪声水平超过目标样本的噪声水平而恶化。
- en: '![Refer to caption](img/939a8ddd3498b7d8f1131aa4a9806595.png)![Refer to caption](img/fe0987328460d9e6484dd718a281d4fc.png)![Refer
    to caption](img/17bdb50779d16650ba03f474ab8c09b2.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/939a8ddd3498b7d8f1131aa4a9806595.png)![参见标题](img/fe0987328460d9e6484dd718a281d4fc.png)![参见标题](img/17bdb50779d16650ba03f474ab8c09b2.png)'
- en: 'Figure 6: Classification confusion matrices of BlendHunter (solid lines) compared
    to SEP (dot-dashed lines) for the realistic CFIS-like test data. Each panel shows
    the fraction of blends correctly classified as blends (*i.e.* true positives,
    green lines) or isolated sources correctly classified as isolated sources (*i.e.*
    true negatives, red lines) as a function of the the training noise standard deviation.
    The left panel shows the sample of close blends along with an equivalent number
    of isolated sources. The middle panel shows the sample of distant blends along
    with the remaining isolated sources. The right panel shows the results for the
    full set of postage stamps.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: BlendHunter（实线）与 SEP（点划线）在现实 CFIS 类似测试数据中的分类混淆矩阵。每个面板显示了随着训练噪声标准差变化，正确分类为混合源（*即*
    真阳性，绿色线条）或正确分类为孤立源（*即* 真阴性，红色线条）的混合源的比例。左侧面板显示了接近混合源的样本以及等量的孤立源。中间面板显示了远距离混合源的样本及剩余孤立源。右侧面板显示了完整的邮票集的结果。'
- en: 5 Conclusions
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: We have presented a proof-of-concept deep transfer learning approach for the
    automated and robust identification of blended sources in galaxy survey data called
    BlendHunter. This technique uses convolution layers pre-trained on natural images
    from ImageNet, thus significantly reducing the time required for training. The
    fully connected layers are then trained on simple parametric models derived from
    COSMOS data for various levels and realisations of Gaussian random noise.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提出了一种概念验证的深度迁移学习方法，用于自动化和鲁棒地识别银河系调查数据中的混合源，称为 BlendHunter。该技术使用在 ImageNet
    上预训练的自然图像卷积层，从而显著减少了训练所需的时间。然后，完全连接层在从 COSMOS 数据中得出的简单参数模型上进行训练，这些模型涵盖了各种水平和高斯随机噪声的实现。
- en: Comparison with the community standard SEP (a Python implementation of SExtractor)
    on 70 sets of $8\leavevmode\nobreak\ 000$ parametric models demonstrate that BlendHunter
    is significantly more sensitive to blends when the images are very noisy or when
    galaxy pairs are very close ($<10$ pixels). In the low noise, distant blend regime,
    BlendHunter appears to be consistent with SEP.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 与社区标准 SEP（SExtractor 的 Python 实现）在 70 组 $8\leavevmode\nobreak\ 000$ 参数模型上的比较表明，当图像非常嘈杂或银河对非常接近（$<10$
    像素）时，BlendHunter 对混合源的敏感性明显高于 SEP。在低噪声、远距离混合源的情况下，BlendHunter 与 SEP 的表现似乎一致。
- en: We additionally produced a sample of $80\leavevmode\nobreak\ 000$ more realistic
    CFIS-like images derived from real COSMOS data. Results demonstrate that the BlendHunter
    weights, learned on the parametric models, can successfully be transferred to
    this more complex data set and achieve $>90\%$ classification accuracy, outperforming
    SEP by a few percent on the full sample when the appropriate weights are used.
    The results also indicate that BlendHunter is capable of achieving results roughly
    consistent with SEP even when weights are used that have been trained with a significantly
    different noise standard deviation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们另外生成了一批 $80\leavevmode\nobreak\ 000$ 张更为真实的类似 CFIS 的图像，这些图像来源于真实的 COSMOS 数据。结果表明，BlendHunter
    在参数模型上学习到的权重可以成功地转移到这个更复杂的数据集上，并且在使用适当的权重时，分类准确率可达 $>90\%$，在完整样本中比 SEP 高出几个百分点。结果还表明，即使使用的权重是用具有显著不同噪声标准差的数据训练得到的，BlendHunter
    也能够取得大致与 SEP 一致的结果。
- en: BlendHunter notably outperforms SEP by $5-15\%$ for blends in which the galaxies
    are separated by less than ten pixels. This is a interesting result as these are
    precisely the cases that generate biases in galaxy shape measurements (MacCrann
    et al., [2020](#bib.bib21)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: BlendHunter 在处理星系分离少于十个像素的混合图像时，相较于 SEP 提高了 $5-15\%$ 的性能。这是一个有趣的结果，因为这些正是会在星系形状测量中产生偏差的情况（MacCrann
    等人，[2020](#bib.bib21)）。
- en: Overall the results are promising and indicate that it may be possible to adapt
    this approach to more accurately identify blended sources in real survey data.
    The next steps for moving in this direction would entail generating more realistic
    testing data that contain some artefacts and images with more than two sources.
    Further work is also required to reduce the number of false negatives, *i.e.*
    incorrect labels for isolated sources. Finally, additional tests should be performed
    to determine if and to what degree the use of pre-trained weights in the CNN layers
    can help prevent over-fitting the network to the training sets. We leave the investigation
    and implementation of these steps for future work.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来看，这些结果令人鼓舞，并且表明将这种方法应用于更准确地识别真实调查数据中的混合源是可能的。接下来的步骤将涉及生成包含一些伪影和两个以上源的图像的更真实的测试数据。此外，还需要进一步工作以减少误报数量，*即*对孤立源的错误标记。最后，还应进行额外的测试，以确定使用预训练权重的
    CNN 层是否以及在多大程度上能够帮助防止网络对训练集的过拟合。我们将这些步骤的调查和实施留待未来的工作。
- en: Acknowledgements.
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 致谢。
- en: The authors wish to acknowledge the COSMIC project funded by the CEA DRF-Impulsion
    call in 2016, the CrossDisciplinary Program on Numerical Simulation (SILICOSMIC
    project in 2018) of CEA, the French Alternative Energies and Atomic Energy Commission.
    The Euclid Collaboration, the European Space Agency and the support of the Centre
    National d’Etudes Spatiales. This work was also supported by the ANR AstroDeep
    project - grant 19-CE23-0024-01\. This work has made use of the CANDIDE Cluster
    at the Institut d’Astrophysique de Paris and made possible by grants from the
    PNCG and the DIM-ACAV. This work is based on data obtained as part of the Canada-France
    Imaging Survey, a CFHT large program of the National Research Council of Canada
    and the French Centre National de la Recherche Scientifique. Based on observations
    obtained with MegaPrime/MegaCam, a joint project of CFHT and CEA Saclay, at the
    Canada-France-Hawaii Telescope (CFHT) which is operated by the National Research
    Council (NRC) of Canada, the Institut National des Science de l’Univers (INSU)
    of the Centre National de la Recherche Scientifique (CNRS) of France, and the
    University of Hawaii. This research used the facilities of the Canadian Astronomy
    Data Centre operated by the National Research Council of Canada with the support
    of the Canadian Space Agency. AZV would like to thank LSC/PMR/EP at University
    of São Paulo for providing additional computing power. The authors also wish to
    thank Xinyu Wang and Alexandre Bruckert for initial efforts that were expanded
    upon in this work.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 作者们感谢 COSMIC 项目，该项目由 2016 年 CEA DRF-Impulsion 资助，CEA 的跨学科数值模拟计划（2018 年 SILICOSMIC
    项目），法国替代能源与原子能委员会。Euclid 合作组织、欧洲航天局以及法国国家空间研究中心的支持。该工作还得到了 ANR AstroDeep 项目 -
    资助编号 19-CE23-0024-01 的支持。此项工作利用了巴黎天文物理研究所的 CANDIDE 集群，并得到了 PNCG 和 DIM-ACAV 的资助。该工作基于加拿大-法国成像调查的数据，这是由加拿大国家研究委员会和法国国家科学研究中心的大型项目
    CFHT 提供的。基于通过加拿大-法国-夏威夷望远镜 (CFHT) 的 MegaPrime/MegaCam 观测，该望远镜由加拿大国家研究委员会 (NRC)、法国国家科学研究中心
    (CNRS) 的国家宇宙科学研究所 (INSU) 和夏威夷大学共同运营。本研究使用了由加拿大国家研究委员会在加拿大空间局支持下运营的加拿大天文学数据中心的设施。AZV
    感谢圣保罗大学 LSC/PMR/EP 提供额外的计算能力。作者们还感谢 Xinyu Wang 和 Alexandre Bruckert 对本工作的初步努力。
- en: References
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: References
- en: 'Abadi et al. (2015) Abadi, M., Agarwal, A., Barham, P., et al. 2015, TensorFlow:
    Large-Scale Machine Learning on Heterogeneous Systems, software available from
    tensorflow.org'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Abadi et al. (2015) Abadi, M., Agarwal, A., Barham, P., et al. 2015, TensorFlow:
    Large-Scale Machine Learning on Heterogeneous Systems, software available from
    tensorflow.org'
- en: Arcelin et al. (2021) Arcelin, B., Doux, C., Aubourg, E., Roucelle, C., & LSST
    Dark Energy Science Collaboration. 2021, MNRAS, 500, 531
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arcelin et al. (2021) Arcelin, B., Doux, C., Aubourg, E., Roucelle, C., & LSST
    Dark Energy Science Collaboration. 2021, MNRAS, 500, 531
- en: Awang Iskandar et al. (2020) Awang Iskandar, D. N. F., Zijlstra, A. A., McDonald,
    I., et al. 2020, Galaxies, 8, 88
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Awang Iskandar et al. (2020) Awang Iskandar, D. N. F., Zijlstra, A. A., McDonald,
    I., et al. 2020, Galaxies, 8, 88
- en: Barbary (2016) Barbary, K. 2016, Journal of Open Source Software, 1, 58
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barbary (2016) Barbary, K. 2016, Journal of Open Source Software, 1, 58
- en: Bertin & Arnouts (1996) Bertin, E. & Arnouts, S. 1996, A&AS, 117, 393
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bertin & Arnouts (1996) Bertin, E. & Arnouts, S. 1996, A&AS, 117, 393
- en: Bosch et al. (2018) Bosch, J., Armstrong, R., Bickerton, S., et al. 2018, PASJ,
    70, S5
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bosch et al. (2018) Bosch, J., Armstrong, R., Bickerton, S., et al. 2018, PASJ,
    70, S5
- en: Chollet et al. (2015) Chollet, F. et al. 2015, Keras, [https://keras.io](https://keras.io)
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chollet et al. (2015) Chollet, F. et al. 2015, Keras, [https://keras.io](https://keras.io)
- en: 'Cropper et al. (2012) Cropper, M., Cole, R., James, A., et al. 2012, in Society
    of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, Vol. 8442,
    Space Telescopes and Instrumentation 2012: Optical, Infrared, and Millimeter Wave,
    ed. M. C. Clampin, G. G. Fazio, H. A. MacEwen, & J. Oschmann, Jacobus M., 84420V'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Cropper et al. (2012) Cropper, M., Cole, R., James, A., et al. 2012, in Society
    of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, Vol. 8442,
    Space Telescopes and Instrumentation 2012: Optical, Infrared, and Millimeter Wave,
    ed. M. C. Clampin, G. G. Fazio, H. A. MacEwen, & J. Oschmann, Jacobus M., 84420V'
- en: de Vaucouleurs (1948) de Vaucouleurs, G. 1948, Annales d’Astrophysique, 11,
    247
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: de Vaucouleurs (1948) de Vaucouleurs, G. 1948, Annales d’Astrophysique, 11,
    247
- en: Deng et al. (2009) Deng, J., Dong, W., Socher, R., et al. 2009, Proc. CVPR
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng et al. (2009) Deng, J., Dong, W., Socher, R., et al. 2009, Proc. CVPR
- en: Duchi et al. (2011) Duchi, J., Hazan, E., & Singer, Y. 2011, Journal of Machine
    Learning Research, 12, 2121
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Duchi et al. (2011) Duchi, J., Hazan, E., & Singer, Y. 2011, Journal of Machine
    Learning Research, 12, 2121
- en: Euclid Collaboration et al. (2019) Euclid Collaboration, Martinet, N., Schrabback,
    T., et al. 2019, A&A, 627, A59
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Euclid Collaboration 等（2019）Euclid Collaboration, Martinet, N., Schrabback,
    T., 等 2019, 《天文学与天体物理学》，627, A59
- en: George et al. (2018) George, D., Shen, H., & Huerta, E. A. 2018, Phys. Rev. D,
    97, 101501
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: George 等（2018）George, D., Shen, H., & Huerta, E. A. 2018, 《物理评论D》，97, 101501
- en: Guinot et al. (2021) Guinot, A., Kilbinger, M., Farrens, S., et al. 2021, A&A,
    submitted
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guinot 等（2021）Guinot, A., Kilbinger, M., Farrens, S., 等 2021, 《天文学与天体物理学》，已提交
- en: Hartlap et al. (2011) Hartlap, J., Hilbert, S., Schneider, P., & Hildebrandt,
    H. 2011, A&A, 528, A51
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hartlap 等（2011）Hartlap, J., Hilbert, S., Schneider, P., & Hildebrandt, H. 2011,
    《天文学与天体物理学》，528, A51
- en: Hoekstra et al. (2017) Hoekstra, H., Viola, M., & Herbonnet, R. 2017, MNRAS,
    468, 3295
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoekstra 等（2017）Hoekstra, H., Viola, M., & Herbonnet, R. 2017, 《蒙特卡洛与天文学》，468,
    3295
- en: Joseph et al. (2016) Joseph, R., Courbin, F., & Starck, J.-L. 2016, A&A, 589,
    A2
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Joseph 等（2016）Joseph, R., Courbin, F., & Starck, J.-L. 2016, 《天文学与天体物理学》，589,
    A2
- en: 'Kingma & Ba (2014) Kingma, D. P. & Ba, J. 2014, Adam: A Method for Stochastic
    Optimization, cite arxiv:1412.6980Comment: Published as a conference paper at
    the 3rd International Conference for Learning Representations, San Diego, 2015'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kingma & Ba（2014）Kingma, D. P. & Ba, J. 2014, Adam: 一种随机优化方法, cite arxiv:1412.6980Comment:
    作为会议论文发布于2015年3rd International Conference for Learning Representations, San Diego'
- en: Kotsiantis (2007) Kotsiantis, S. 2007, 31, 249
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kotsiantis（2007）Kotsiantis, S. 2007, 31, 249
- en: Lecun et al. (2015) Lecun, Y., Bengio, Y., & Hinton, G. 2015, Nature, 521, 436
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lecun 等（2015）Lecun, Y., Bengio, Y., & Hinton, G. 2015, 《自然》，521, 436
- en: MacCrann et al. (2020) MacCrann, N., Becker, M. R., McCullough, J., et al. 2020,
    arXiv e-prints, arXiv:2012.08567
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacCrann 等（2020）MacCrann, N., Becker, M. R., McCullough, J., 等 2020, arXiv e-prints,
    arXiv:2012.08567
- en: Mandelbaum (2018) Mandelbaum, R. 2018, ARA&A, 56, 393
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandelbaum（2018）Mandelbaum, R. 2018, 《年鉴与天文学与天体物理学》，56, 393
- en: Mandelbaum et al. (2012) Mandelbaum, R., Hirata, C. M., Leauthaud, A., Massey,
    R. J., & Rhodes, J. 2012, Monthly Notices of the Royal Astronomical Society, 420,
    1518
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandelbaum 等（2012）Mandelbaum, R., Hirata, C. M., Leauthaud, A., Massey, R. J.,
    & Rhodes, J. 2012, 《皇家天文学会月刊》，420, 1518
- en: Mandelbaum et al. (2014) Mandelbaum, R., Rowe, B., Bosch, J., et al. 2014, The
    Astrophysical Journal Supplement Series, 212, 5
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mandelbaum 等（2014）Mandelbaum, R., Rowe, B., Bosch, J., 等 2014, 《天体物理学杂志补充系列》，212,
    5
- en: Melchior et al. (2018) Melchior, P., Moolekamp, F., Jerdee, M., et al. 2018,
    Astronomy and Computing, 24, 129
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Melchior 等（2018）Melchior, P., Moolekamp, F., Jerdee, M., 等 2018, 《天文学与计算》，24,
    129
- en: Reiman & Göhre (2019) Reiman, D. M. & Göhre, B. E. 2019, MNRAS, 485, 2617
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Reiman & Göhre（2019）Reiman, D. M. & Göhre, B. E. 2019, 《蒙特卡洛与天文学》，485, 2617
- en: Rowe et al. (2015) Rowe, B. T. P., Jarvis, M., Mandelbaum, R., et al. 2015,
    Astronomy and Computing, 10, 121
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rowe 等（2015）Rowe, B. T. P., Jarvis, M., Mandelbaum, R., 等 2015, 《天文学与计算》，10,
    121
- en: Russakovsky et al. (2015) Russakovsky, O., Deng, J., Su, H., et al. 2015, International
    Journal of Computer Vision, 115, 211
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russakovsky 等（2015）Russakovsky, O., Deng, J., Su, H., 等 2015, 《计算机视觉国际期刊》，115,
    211
- en: Samuroff et al. (2018) Samuroff, S., Bridle, S. L., Zuntz, J., et al. 2018,
    MNRAS, 475, 4524
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Samuroff 等（2018）Samuroff, S., Bridle, S. L., Zuntz, J., 等 2018, 《蒙特卡洛与天文学》，475,
    4524
- en: Sanchez et al. (2021) Sanchez, J., Mendoza, I., Kirkby, D. P., Burchat, P. R.,
    & LSST Dark Energy Science Collaboration. 2021, J. Cosmology Astropart. Phys.,
    2021, 043
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanchez 等（2021）Sanchez, J., Mendoza, I., Kirkby, D. P., Burchat, P. R., & LSST
    Dark Energy Science Collaboration. 2021, 《宇宙学与粒子物理学杂志》，2021, 043
- en: Scoville et al. (2007) Scoville, N., Aussel, H., Brusa, M., et al. 2007, ApJS,
    172, 1
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scoville 等（2007）Scoville, N., Aussel, H., Brusa, M., 等 2007, 《天体物理学杂志》，172,
    1
- en: Shan et al. (2020) Shan, J.-h., Feng, L., Yuan, H.-q., et al. 2020, Chinese
    Astron. Astrophys., 44, 507
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shan 等（2020）Shan, J.-h., Feng, L., Yuan, H.-q., 等 2020, 《中国天文学与天体物理学》，44, 507
- en: Simonyan & Zisserman (2014) Simonyan, K. & Zisserman, A. 2014, CoRR [arXiv:1409.1556]
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan & Zisserman（2014）Simonyan, K. & Zisserman, A. 2014, CoRR [arXiv:1409.1556]
- en: Srinivas et al. (2016) Srinivas, S., Sarvadevabhatla, R. K., Mopuri, K. R.,
    et al. 2016, Frontiers in Robotics and AI, 2, 36
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srinivas 等（2016）Srinivas, S., Sarvadevabhatla, R. K., Mopuri, K. R., 等 2016,
    《机器人与人工智能前沿》，2, 36
- en: Srivastava et al. (2014) Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever,
    I., & Salakhutdinov, R. 2014, Journal of Machine Learning Research, 15, 1929
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Srivastava 等（2014）Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I.,
    & Salakhutdinov, R. 2014, 《机器学习研究期刊》，15, 1929
- en: Tanoglidis et al. (2021) Tanoglidis, D., Ćiprijanović, A., & Drlica-Wagner,
    A. 2021, Astronomy and Computing, 35, 100469
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tanoglidis 等（2021）Tanoglidis, D., Ćiprijanović, A., & Drlica-Wagner, A. 2021,
    《天文学与计算》，35, 100469
- en: Tatarski (2016) Tatarski, V. I. 2016, Wave propagation in a turbulent medium
    (Courier Dover Publications)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tatarski（2016）Tatarski, V. I. 2016, 《湍流介质中的波传播》（Courier Dover Publications）
- en: 'Tieleman & Hinton (2012) Tieleman, T. & Hinton, G. 2012, Lecture 6.5—RmsProp:
    Divide the gradient by a running average of its recent magnitude, COURSERA: Neural
    Networks for Machine Learning'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tieleman & Hinton（2012）Tieleman, T. & Hinton, G. 2012年，第6.5讲——RmsProp：将梯度除以其最近幅度的移动平均，COURSERA：机器学习的神经网络
- en: Trujillo et al. (2001) Trujillo, I., Aguerri, J., Cepa, J., & Gutiérrez, C.
    2001, Monthly Notices of the Royal Astronomical Society, 328, 977–985
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Trujillo等（2001）Trujillo, I., Aguerri, J., Cepa, J., & Gutiérrez, C. 2001年，《皇家天文学会月刊》，328，977–985
- en: Wei et al. (2020) Wei, W., Huerta, E. A., Whitmore, B. C., et al. 2020, MNRAS,
    493, 3178
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wei等（2020）Wei, W., Huerta, E. A., Whitmore, B. C., 等。2020年，《MNRAS》，493，3178
- en: Wu et al. (2019) Wu, C., Wong, O. I., Rudnick, L., et al. 2019, MNRAS, 482,
    1211
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu等（2019）Wu, C., Wong, O. I., Rudnick, L., 等。2019年，《MNRAS》，482，1211
- en: Zhu et al. (2019) Zhu, X.-P., Dai, J.-M., Bian, C.-J., et al. 2019, Ap&SS, 364,
    55
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu等（2019）Zhu, X.-P., Dai, J.-M., Bian, C.-J., 等。2019年，《Ap&SS》，364，55
