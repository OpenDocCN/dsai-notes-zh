- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:39:57'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2305.05959] Survey of Code Search Based on Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2305.05959](https://ar5iv.labs.arxiv.org/html/2305.05959)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Survey of Code Search Based on Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Yutao Xie Peking University & International Digital Economy AcademyNo. 5 Shihua
    RoadFutian DistrictShenzhenChina518017 [yutaoxie@idea.edu.cn](mailto:yutaoxie@idea.edu.cn)
    ,  Jiayi Lin International Digital Economy AcademyNo. 5 Shihua RoadFutian DistrictShenzhenChina518017
    [jiayilin1024@gmail.com](mailto:jiayilin1024@gmail.com) ,  Hande Dong International
    Digital Economy AcademyNo. 5 Shihua RoadFutian DistrictShenzhenChina518017 [donghd66@gmail.com](mailto:donghd66@gmail.com)
    ,  Lei Zhang International Digital Economy AcademyNo. 5 Shihua RoadFutian DistrictShenzhenChina518017
    [leizhang@idea.edu.cn](mailto:leizhang@idea.edu.cn)  and  Zhonghai Wu Key Lab
    of High Confidence Software Technologies (MOE), Peking UniversityNo. 5 Yiheyuan
    RoadHaidian DistrictBeijingChina100871 [wuzh@pku.edu.cn](mailto:wuzh@pku.edu.cn)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Code writing is repetitive and predictable, inspiring us to develop various
    code intelligence techniques. This survey focuses on code search, that is, to
    retrieve code that matches a given natural language query by effectively capturing
    the semantic similarity between the query and code. Deep learning, being able
    to extract complex semantics information, has achieved great success in this field.
    Recently, various deep learning methods, such as graph neural networks and pretraining
    models, have been applied to code search with significant progress. Deep learning
    is now the leading paradigm for code search. In this survey, we provide a comprehensive
    overview of deep learning-based code search. We review the existing deep learning-based
    code search framework which maps query/code to vectors and measures their similarity.
    Furthermore, we propose a new taxonomy to illustrate the state-of-the-art deep
    learning-based code search in a three-steps process: query semantics modeling,
    code semantics modeling, and matching modeling which involves the deep learning
    model training. Finally, we suggest potential avenues for future research in this
    promising field.'
  prefs: []
  type: TYPE_NORMAL
- en: 'code search, code understanding, natural language processing, deep learning,
    pre-training^†^†journal: JACM^†^†ccs: General and reference Surveys and overviews^†^†ccs:
    Software and its engineering^†^†ccs: Software and its engineering Software notations
    and tools^†^†ccs: Software and its engineering Software creation and management^†^†copyright:
    acmlicensed^†^†journal: TOSEM^†^†journalyear: 2023^†^†journalvolume: 1^†^†journalnumber:
    1^†^†article: 1^†^†publicationmonth: 1^†^†price: 15.00^†^†doi: 10.1145/3628161'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Programming is the process of using a programming language to write code that
    performs a specific function. Despite being a creative endeavor, programming also
    exhibits repetitive and predictable attributes (Hindle et al., [2016](#bib.bib31)),
    with many codes already implemented to some extent. By analyzing historical codes,
    it is possible to anticipate the content of the code to be written. Building on
    this insight, code intelligence techniques such as code search, code completion,
    and code generation can be employed to enhance the productivity of software developers.
    In recent years, the application of deep learning in the field of code intelligence
    has led to remarkable achievements (Wang et al., [2022d](#bib.bib85), [2021a](#bib.bib89);
    Chen et al., [2021](#bib.bib11); Li et al., [2022c](#bib.bib48); Zhu et al., [2022](#bib.bib103);
    Guo et al., [2022](#bib.bib25)), thanks to its powerful representation capability
    and capacity to uncover hidden patterns. At present, a number of code intelligence
    tools leveraging deep learning, including Copilot and ChatGPT, have been developed
    and commercialized. These advanced tools enable software developers to write efficient
    and accurate code with greater ease and speed.
  prefs: []
  type: TYPE_NORMAL
- en: This survey focuses on the nl-to-code search task. Code search is one of the
    three main software engineering tasks that has remained active over the years
    (Watson et al., [2022](#bib.bib90)). While code-to-code search falls within the
    research domain of code search, its role bears a closer affinity to code clone
    detection, thus rendering it outside the scope of this survey. The goal of nl-to-code
    search (henceforth referred to as code search) is to retrieve code fragments that
    match developers’ natural language queries from a large code corpus (Stolee et al.,
    [2014](#bib.bib75)). Software development is a creative work that requires software
    developers to write code that meets product requirements. During software development,
    developers often use search engines to search for code-related information (Shuai
    et al., [2020](#bib.bib72)), such as reusable code snippets, API understanding,
    and examples of specific functionality. They seek high-quality open source code
    for reference or reuse, which enhances the productivity of development (Gharehyazie
    et al., [2017](#bib.bib19)). Developers typically use general search engines such
    as Google, Bing, and Baidu to search for target codes. However, these general
    search engines usually search in the full database and are not specifically designed
    for code-related information. Consequently, they are inefficient on the code search
    task and the search results are not satisfactory.
  prefs: []
  type: TYPE_NORMAL
- en: 'The transformative impact of deep learning on code search. Early code search
    engines primarily utilized Information Retrieval (IR) technology. They treated
    source code as text, and searched for the target code by matching keywords in
    query and code (Chatterjee et al., [2009](#bib.bib10); McMillan et al., [2011](#bib.bib59);
    Hill et al., [2014](#bib.bib30); Nie et al., [2016](#bib.bib61)). These methods
    mainly rely on the textual similarity between code and query. However, this approach
    has drawbacks since programming languages and natural languages differ greatly
    from each other, making it difficult for IR-based methods to comprehend the semantics.
    Fortunately, deep learning boasts an exceptional capability for extracting high-level
    semantic representations. As a result, compared to conventional IR-based approaches,
    code search has been improved significantly since deep learning incorporation
    (Gu et al., [2018](#bib.bib24)). Deep learning has received great attention in
    recent years in code search research. Code search is one of the first software
    engineering tasks to use deep learning techniques (Watson et al., [2022](#bib.bib90)).
    Hence, this paper primarily investigates code search methods based on deep learning.
    We provide a taxonomy which groups the current series of work into three categories:
    various techniques are employed to enhance the semantics of the query text for
    a more precise search intent (Sirres et al., [2018](#bib.bib73); Rahman and Roy,
    [2018](#bib.bib67); Rahman et al., [2019](#bib.bib68); Rahman, [2019](#bib.bib66);
    Zhang et al., [2018](#bib.bib100); Liu et al., [2019a](#bib.bib52); Cao et al.,
    [2021](#bib.bib8); Huang et al., [2019](#bib.bib35); Wu and Yang, [2019](#bib.bib91);
    Wang et al., [2022b](#bib.bib83); Eberhart and McMillan, [2022](#bib.bib17); Sivaraman
    et al., [2019](#bib.bib74)); sequence and graph-based deep learning technologies
    are introduced to model code representation and enhance code comprehension (Cambronero
    et al., [2019](#bib.bib7); Feng et al., [2020](#bib.bib18); Ling et al., [2020](#bib.bib49);
    Huang et al., [2021](#bib.bib34); Lachaux et al., [2021](#bib.bib42); Du et al.,
    [2021](#bib.bib16); Salza et al., [2023](#bib.bib70); Wang et al., [2022a](#bib.bib84);
    Chai et al., [2022](#bib.bib9); Li et al., [2022a](#bib.bib45), [b](#bib.bib46),
    [d](#bib.bib44); Guo et al., [2021](#bib.bib26); Gu et al., [2021a](#bib.bib21);
    Xu et al., [2021](#bib.bib92); Wang et al., [2021b](#bib.bib87), [2022c](#bib.bib88);
    Niu et al., [2022](#bib.bib62); Guo et al., [2022](#bib.bib25); Gu et al., [2018](#bib.bib24);
    Sachdev et al., [2018](#bib.bib69); Wan et al., [2019](#bib.bib79); Zeng et al.,
    [2023](#bib.bib99); Ling et al., [2021](#bib.bib50); Sun et al., [2022a](#bib.bib76);
    Liu et al., [2023](#bib.bib53); Yao et al., [2019](#bib.bib95); Haldar et al.,
    [2020](#bib.bib27); Wang et al., [2020](#bib.bib86); Zhao and Sun, [2020](#bib.bib102);
    Ye et al., [2020](#bib.bib97); Gu et al., [2021b](#bib.bib22); Cheng and Kuang,
    [2022](#bib.bib13); Arakelyan et al., [2022](#bib.bib3); Cai et al., [2023](#bib.bib6);
    Han et al., [2022](#bib.bib28); Ma et al., [2023](#bib.bib57)); more efficient
    training techniques are introduced or proposed, making the training of large models
    a great success (Feng et al., [2020](#bib.bib18); Guo et al., [2021](#bib.bib26);
    Wang et al., [2021b](#bib.bib87); Guo et al., [2022](#bib.bib25); Wang et al.,
    [2022c](#bib.bib88); Niu et al., [2022](#bib.bib62); Lachaux et al., [2021](#bib.bib42);
    Li et al., [2022b](#bib.bib46); Huang et al., [2021](#bib.bib34); Gu et al., [2018](#bib.bib24);
    Wan et al., [2019](#bib.bib79); Zeng et al., [2023](#bib.bib99); Ling et al.,
    [2021](#bib.bib50); Sun et al., [2022a](#bib.bib76); Xu et al., [2021](#bib.bib92);
    Ling et al., [2020](#bib.bib49); Liu et al., [2023](#bib.bib53); Li et al., [2022a](#bib.bib45),
    [d](#bib.bib44); Chen and Zhou, [2018](#bib.bib12); Chai et al., [2022](#bib.bib9);
    Wang et al., [2022a](#bib.bib84); Han et al., [2022](#bib.bib28); Yao et al.,
    [2019](#bib.bib95); Haldar et al., [2020](#bib.bib27); Wang et al., [2020](#bib.bib86);
    Zhao and Sun, [2020](#bib.bib102); Ye et al., [2020](#bib.bib97); Gu et al., [2021b](#bib.bib22);
    Bui et al., [2021](#bib.bib5); Cheng and Kuang, [2022](#bib.bib13); Arakelyan
    et al., [2022](#bib.bib3); Park et al., [2023](#bib.bib64); Cai et al., [2023](#bib.bib6);
    Ma et al., [2023](#bib.bib57); Hu et al., [2023](#bib.bib32)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Significance of this review. With numerous deep learning-based code search
    works published, especially the advent of pre-training technology in recent years,
    the field of code search has entered a new era. Despite the existence of several
    studies in the field of deep learning and code search, there has not been a thorough
    and organized examination of the correlation between them. As a result, we undertake
    a study of recent deep learning research in code search, develop a new taxonomy
    and provide insights into the future research opportunities from our discoveries.
    This review serves to fill this gap. We believe that this review will be useful
    for both relevant academic researchers and industrial practitioners as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Individuals who are new to the field of code search and wish to gain a foundational
    understanding through a review.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Individuals who already have some understanding of the field of code search,
    but lack an organized summary and classification, and are unable to establish
    a coherent knowledge structure for the field.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Individuals who are interested in learning about the latest advancements and
    state-of-the-art methods in the field of deep code search.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Individuals who aim to develop code search tools in the industry.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Individuals who are currently seeking a research direction in the field of deep
    code search.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Differences from prior reviews. At present, there have been several surveys
    on code search, but their focus is different from ours. Khalifa (Khalifa, [2019](#bib.bib41))
    discusses existing code search techniques, focusing on IR-based and deep-learning-based
    approaches, but only covers 5 relevant papers. Liu et al. (Liu et al., [2022](#bib.bib51))
    focus on publication trends, application scenarios, and evaluation metrics of
    code search. Grazia and Pradel (Grazia and Pradel, [2023](#bib.bib20)) provide
    a review of the historical development of code search and covers the various stages
    of the code search process, including query formulation, query processing, indexing,
    and ranking. However, the review lacks theoretical analysis and in-depth summaries,
    especially in terms of the relationship between query and code semantic matching
    models. Different from the previous reviews, we concentrate on code search technology
    based on deep learning. We collect and organize high-quality conference/journal
    papers published in the past 5 years and propose a new taxonomy. In this survey,
    we have discussed the following research questions (RQs):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ1. How to accurately capture the user’s query intent?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ2. How to enhance the semantic understanding of code?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ3. How to train a deep code search model?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ4. How to evaluate a code search model?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ5. What are some potential avenues for future research in this promising field?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Literatures collection process. Our aim is to equip novice researchers and
    non-experts with a quick and systematic grasp of the latest code search techniques
    and motivate their future research. Considering that Gu et al. (Gu et al., [2018](#bib.bib24))
    were the pioneers in applying deep neural networks to code search tasks in 2018,
    we establish 2018 as our reference point and adhere to the guidelines proposed
    by Kitchenham and Charters (Keele, [2007](#bib.bib40)) as well as Petersen et
    al (Petersen et al., [2015](#bib.bib65)). This allows us to perform a comprehensive
    examination of the relevant literature spanning the last six years (from 2018
    to the present). Specifically, we identified a set of search strings, namely “code
    search” and “code retrieval”, from the existing literature on code search that
    we are already known. Subsequently, capitalizing on our emphasis on the precise
    technological domain of deep learning, we broadened our search strings to encompass
    the advanced concepts of “deep code search” and “deep code retrieval”. Based on
    the aforementioned four search strings, we conducted initial manual searches on
    Google Scholar, DBLP, ACM Digital Library, and Papers With Code, resulting in
    693 relevant papers. After removing duplicate papers, we identified a total of
    431 candidate papers. Our comprehensive search was finalized on June 12, 2023,
    encompassing research published prior to this date. Subsequently, we meticulously
    applied a set of well-defined inclusion and exclusion criteria to identify the
    most relevant literature that aligns with our research topic:'
  prefs: []
  type: TYPE_NORMAL
- en: ✓
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Complete research papers published in top-ranked international academic conferences
    or journals. Specifically, venues ranked A* or A in the CORE ranking: [http://portal.core.edu.au](https://www.core.edu.au/conference-portal).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ✓
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Significantly impactful papers from workshops, arXiv, and lower-ranked venues.
    Specifically, these papers have received no less than 15 citations and are highly
    relevant to the subject of our investigation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ✓
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The paper must include a discussion or evaluation of the model on the task of
    natural language code search.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ✗
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The preprints of accepted papers are excluded.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ✗
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Journal papers that are extensions of conference proceedings are discarded.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ✗
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Non-deep learning-based code search papers are ruled out.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ✗
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papers focusing on code-to-code search are excluded.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ✗
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Papers that focus on utilizing code search models to assist other code intelligence
    tasks, such as code generation and code repair, are discarded.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Table 1. Selection of relevant papers.
  prefs: []
  type: TYPE_NORMAL
- en: '| Process | Number of papers |'
  prefs: []
  type: TYPE_TB
- en: '| Total papers retrieved | 693 |'
  prefs: []
  type: TYPE_TB
- en: '| After removing duplicate papers | 431 |'
  prefs: []
  type: TYPE_TB
- en: '| After excluding based on title, abstract, and keywords | 97 |'
  prefs: []
  type: TYPE_TB
- en: '| After excluding based on the full text | 64 |'
  prefs: []
  type: TYPE_TB
- en: Table 2. Top publication venues with at least two deep code search papers.
  prefs: []
  type: TYPE_NORMAL
- en: '| Publication venue types | Short Name | Full Name | Number of papers |'
  prefs: []
  type: TYPE_TB
- en: '| Conference | ICSE | International Conference on Software Engineering | 9
    |'
  prefs: []
  type: TYPE_TB
- en: '| EMNLP | Conference on Empirical Methods in Natural Language Processing |
    6 |'
  prefs: []
  type: TYPE_TB
- en: '| SANER | IEEE International Conference on Software Analysis, Evolution, and
    Reengineering | 5 |'
  prefs: []
  type: TYPE_TB
- en: '| ACL | Annual Meeting of the Association for Computational Linguistics | 4
    |'
  prefs: []
  type: TYPE_TB
- en: '| FSE/ESEC | ACM Joint European Software Engineering Conference and Symposium
    on the Foundations of Software Engineering | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| ASE | International Conference on Automated Software Engineering | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| TSE | IEEE Transactions on Software Engineering | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| NeurIPS | Conference on Neural Information Processing Systems | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| ICPC | IEEE International Conference on Program Comprehension | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| WWW | World Wide Web | 3 |'
  prefs: []
  type: TYPE_TB
- en: '| PLDI | ACM SIGPLAN Conference on Programming Language Design and Implementation
    | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| ICSME | International Conference on Software Maintenance and Evolution |
    2 |'
  prefs: []
  type: TYPE_TB
- en: '| Journal | TOSEM | ACM Transactions on Software Engineering and Methodology
    | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| - | Neural Networks | 2 |'
  prefs: []
  type: TYPE_TB
- en: '| Total | - | - | 50 | ![Refer to caption](img/37f461c68989e37ba608518d99705dc6.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1. Number of publications per year.
  prefs: []
  type: TYPE_NORMAL
- en: As presented in Table [1](#S1.T1 "Table 1 ‣ 1\. Introduction ‣ Survey of Code
    Search Based on Deep Learning"), through a thorough analysis of the titles, abstracts,
    and keywords of the papers, we successfully identified 97 papers. Subsequently,
    after carefully cross-checking the full texts of the remaining papers, we ultimately
    obtained a collection of 64 papers on code search leveraging deep learning techniques.
    These papers primarily encompass international top conferences and journals in
    the domains of Software Engineering, Content Retrieval, and Artificial Intelligence.
    Table [2](#S1.T2 "Table 2 ‣ 1\. Introduction ‣ Survey of Code Search Based on
    Deep Learning") presents the esteemed publication venues have published at least
    two deep code search papers. These venues collectively feature 50 papers, constituting
    78.1% of the entire obtained papers. Notably, conference proceedings accounted
    for 92% of the published works. Among these 14 venues, we observed that the top
    five most popular conferences are ICSE, EMNLP, SANER, ACL, and FSE/ESEC, while
    the top two preferred journals are TOSEM and Neural Networks. Figure [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ Survey of Code Search Based on Deep Learning")
    showcases a compelling upward trajectory in the annual publication quantity of
    relevant literature following the integration of deep learning techniques into
    the field of code search. This notable trend serves as a testament to the escalating
    attention and burgeoning interest surrounding deep code search.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions. The main contributions of this paper are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'New taxonomy. We review 64 related works on deep code search published until
    June 12, 2023, and propose a new taxonomy which groups code search based on deep
    learning into three categories: query semantics modeling, code semantics modeling,
    and matching modeling.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comprehensive review. We present a comprehensive overview of deep learning techniques
    in code search, offering in-depth descriptions of representative models, thorough
    comparisons, and summarized algorithms.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Abundant resources. We gather a vast array of code search resources, including
    large-scale training corpus, benchmark datasets, and evaluation metrics suitable
    for various scenarios, to aid in comparing deep code search models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Future directions. We highlight the limitations of current deep code search
    methods and suggest various valuable research directions, aiming to spur further
    exploration in this area.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The rest of this paper is organized as follows. Section 2 introduces the deep
    learning-based code search framework and defines the core problem of code search.
    Section 3 covers methods for enriching the semantics of natural language text
    queries. Section 4 categorizes various code representation and vectorization techniques.
    Section 5 outlines the training methods and objectives of models. Section 6 summarizes
    commonly used datasets and evaluation metrics in this field. Section 7 highlights
    potential future research directions in this promising field. Section 8 provides
    a summary of this survey.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Deep Learning-Based Code Search Framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning has seen rapid development in the field of software engineering
    in recent years, with code search being one of the most successful applications.
    Compared with traditional methods, deep code search leverages the powerful ability
    of deep neural network to extract hidden features from data, generating semantic
    representation of natural language and code for improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/32c893a26078e58b4e9288b3692c2912.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2. Code search framework based on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure [2](#S2.F2 "Figure 2 ‣ 2\. Deep Learning-Based Code Search Framework
    ‣ Survey of Code Search Based on Deep Learning") illustrates the overall framework
    of deep learning-based code search. The framework consists primarily of three
    components: encoding the query, encoding the code, and measuring their similarity.
    During training, a bimodal deep neural network is trained on a large parallel
    corpus of code and natural language to learn how to encode query and code into
    high-dimensional vectors, q and c. Subsequently, the similarity between the query
    vector q and the code vector c in a high-dimensional vector space is calculated,
    such as Cosine similarity $s(\textbf{q},\textbf{c})=\frac{\textbf{q}^{T}\cdot\textbf{c}}{\|\textbf{q}\|\cdot\|\textbf{c}\|}$
    or Euclidean distance $s(\boldsymbol{q},\boldsymbol{c})=\sqrt{\sum_{i=1}^{n}\left(q_{i}-c_{i}\right)^{2}}$.
    Among these options, Cosine similarity stands out as the widely preferred calculation
    method (Gu et al., [2018](#bib.bib24); Wan et al., [2019](#bib.bib79); Ling et al.,
    [2020](#bib.bib49), [2021](#bib.bib50); Xu et al., [2021](#bib.bib92); Li et al.,
    [2022a](#bib.bib45); Zeng et al., [2023](#bib.bib99); Hu et al., [2023](#bib.bib32)).'
  prefs: []
  type: TYPE_NORMAL
- en: To improve code search accuracy, the model should make query representation
    similar to correct code representation while distinct from incorrect code representation.
    Hence, training instances are typically structured as triplets $\left\langle q,c^{+},c^{-}\right\rangle$,
    and the model is trained to minimize the triplet sorting loss $\mathcal{L}\left(q,c^{+},c^{-}\right)=\max\left(0,\delta-s\left(q,c^{+}\right)+s\left(q,c^{-}\right)\right)$,
    where $q$ denotes the natural language query text, $c^{+}$ denotes correct code
    fragment, $c^{-}$ is a randomly selected negative sample code fragment from the
    rest of the code corpus, and $\delta$ denotes the margin that ensures $c^{+}$
    is closer to $q$ than $c^{-}$ in the vector space.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance the response efficiency of the online code search service, the trained
    model typically pre-calculates the vector representation of the candidate code
    fragments in the codebase. Upon receiving a query, the code search engine computes
    the vector representation of the query, traverses the codebase to compute the
    similarity between the query and each code fragment in the semantic space, and
    finally returns the top $k$ code fragments based on the relevance scores.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d40d292c8173835d1284d71208982247.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3. Overview of techniques for deep code search.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Figure [3](#S2.F3 "Figure 3 ‣ 2\. Deep Learning-Based Code Search
    Framework ‣ Survey of Code Search Based on Deep Learning"), deep learning-based
    code search engines mainly deal with two types of input: natural language query
    and source code. Deep learning techniques are widely employed to learn the semantic
    encoding of both inputs, especially the deep semantic information of source code.
    Following this overview, we want to categorize, analyze, and summarize 64 papers
    that are highly relevant to date and deep code search. To accomplish this objective,
    we undertook an exploration of five research questions (RQs):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ1. How to accurately capture the user’s query intent? To achieve the desired
    level of search result quality, it is crucial for the model to possess a good
    understanding of the user’s query intent. The goal of this RQ is to investigate
    how to model the representation of queries and enhance the semantic representation
    of query, thereby assisting the model in accurately capturing the user’s query
    intent.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ2. How to enhance the semantic understanding of code? The syntax of programming
    languages differs from natural language, giving rise to a semantic gap between
    them. This RQ investigates through multiple perspectives, encompassing the representation
    and vectorization of code, as well as diminishing the semantic gap between code
    and natural language through interaction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ3. How to train a deep code search model? This RQ aims to investigate how
    to pretrain code intelligence large models and how to fine-tune code search models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ4. How to evaluate a code search model? The goal of this RQ is to analyze
    code search datasets, evaluation metrics, and the selection of appropriate code
    search methods.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RQ5. What are some potential avenues for future research in this promising field?
    Through analysis of the aforementioned RQs, we have identified several limitations
    and challenges. Building upon this foundation, this RQ discusses twelve valuable
    future research directions in the field of deep code search.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the following sections, we will delve into a comprehensive and detailed discussion
    of each RQ.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Query Representation and Enhancement (RQ1)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Processing a user’s input query is the initial step in conducting a code search
    task. Accurately comprehending the search intention within the query text is crucial
    in providing satisfying results to users. This section covers the query processing
    procedure from two perspectives: modeling the query representation and enhancing
    the semantic representation of the query.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Query Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The deep learning-based code search represents queries and code fragments as
    vectors in high-dimensional space by encoding them. The query encoder and the
    code encoder can use the same or different network architectures. To derive the
    vector representation of the query, tokenization of the input query text is required.
    Tokenization involves reassembling continuous word sequences into token sequences
    based on specific criteria. As shown in Figure [3](#S2.F3 "Figure 3 ‣ 2\. Deep
    Learning-Based Code Search Framework ‣ Survey of Code Search Based on Deep Learning"),
    the developer inputs a query “get the maximum value”, and the tokenizer processes
    it into a token sequence [“get”, “the”, “maximum”, “value”], which is then looked
    up in the dictionary to get the id list [459, 448, 6098, 767] as the input for
    the query encoder. The query encoder $E_{q}$ is an embedding network that converts
    the developer-given query text into a $d$-dimensional vector representation. To
    train the query encoder, existing deep learning-based code search methods have
    tried various neural network architectures, such as RNN (Gu et al., [2018](#bib.bib24)),
    LSTM (Zeng et al., [2023](#bib.bib99)), GNN (Ling et al., [2021](#bib.bib50)),
    and transformer (Feng et al., [2020](#bib.bib18)). As collecting and annotating
    real (query, code) pairs is costly, comments are often treated as queries during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Methods For Enhancing query Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Developers often struggle to clearly articulate their target requirements when
    searching for code and prefer to use short initial queries with broad meanings,
    such as keywords splicing. In this case, the code search engine is likely to return
    many irrelevant code fragments. As a result, Developers have to waste time reviewing
    non-relevant results and adjusting queries. To address this, several studies have
    focused on enhancing query semantics to improve code search efficiency by enabling
    the code search engine to expand or reconstruct the query automatically. Table
    [3](#S3.T3 "Table 3 ‣ 3.2\. Methods For Enhancing query Representation ‣ 3\. Query
    Representation and Enhancement (RQ1) ‣ Survey of Code Search Based on Deep Learning")
    summarizes the different approaches along based on API or class name, based on
    co-occurring terms, based on commit versions, based on query generation, based
    on search logs, based on user feedback, and based on multiple perspectives. We
    will discuss them in detail below.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3. Overview of approaches for enhancing query representation.
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Work | Based on |'
  prefs: []
  type: TYPE_TB
- en: '| API or class name | co-occurring terms | commit versions | query generation
    | Others |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | NLP2API (Rahman and Roy, [2018](#bib.bib67)) | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | COCABU (Sirres et al., [2018](#bib.bib73)) | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Zhang et al. (Zhang et al., [2018](#bib.bib100)) | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | RACK (Rahman et al., [2019](#bib.bib68)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Rahman (Rahman, [2019](#bib.bib66)) |  | ✓ |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | ALICE (Sivaraman et al., [2019](#bib.bib74)) |  |  |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | NQE (Liu et al., [2019a](#bib.bib52)) | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | QESC (Huang et al., [2019](#bib.bib35)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Wu and Yang (Wu and Yang, [2019](#bib.bib91)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | SEQUER (Cao et al., [2021](#bib.bib8)) |  |  |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | QueCos (Wang et al., [2022b](#bib.bib83)) |  |  |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | ZaCQ (Eberhart and McMillan, [2022](#bib.bib17)) |  |  |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: Based on API or class name. Enhancing natural language queries with semantically
    related identifiers has significant potential. Researchers have leveraged source
    code knowledge from platforms such as Stack Overflow and Github to reformulate
    natural language queries. The commonality among query reformulation methods is
    using similarity measures to compare the words or identifiers in the query to
    words or identifiers such as API and class names in source code (Sirres et al.,
    [2018](#bib.bib73); Rahman and Roy, [2018](#bib.bib67); Rahman et al., [2019](#bib.bib68);
    Rahman, [2019](#bib.bib66); Zhang et al., [2018](#bib.bib100)). For instance,
    based on posts on Stack Overflow, Sirres et al. (Sirres et al., [2018](#bib.bib73))
    extend users’ free-form queries with API methods or class names in code snippets
    marked as acceptable. Rahman and Roy (Rahman and Roy, [2018](#bib.bib67)) propose
    automatically identifying relevant or specific API classes from Stack Overflow
    to reformulate queries. They first use pseudo-relevance feedback and term weighting
    algorithm to gather candidate API classes from Stack Overflow, and then apply
    the maximum likelihood estimation between the keywords in the query and API classes
    to rank them. Finally, the top-$k$ correlated classes are used to reformulate
    the query. Zhang et al. (Zhang et al., [2018](#bib.bib100)) apply a continuous
    bag-of-words model to learn the vector representation of API class names in the
    code corpus. These vectors are then utilized to calculate the semantic distance
    between the initial query and API class names. The most semantically relevant
    API class names are selected for query expansion.
  prefs: []
  type: TYPE_NORMAL
- en: Based on co-occurring terms. A few approaches explore term co-occurrence relationships
    when reconstructing queries. These methods identify query keywords within structured
    entities like method and field signatures and then propose their co-occurring
    terms as candidates for query reformulation. For example, Liu et al. (Liu et al.,
    [2019a](#bib.bib52)) present a model called NQE that uses an encoder-decoder architecture
    to predict co-occurring keywords with query keywords in codebase. NQE takes an
    initial query as input to obtain the corresponding encoding, and then inputs this
    encoding into a Recurrent Neural Network (RNN) decoder as the initial state. By
    collecting the output at each time step, NQE generates a sequence of method names.
    Finally, an output set of expanded keywords is obtained by splitting each method
    name in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: Based on commit versions. The code retrieved via code search engines may require
    modification prior to use, such as for compatibility with different API versions.
    A few approaches take this potential code change into account when expanding queries,
    freeing developers from manually modifying codes. Huang et al. (Huang et al.,
    [2019](#bib.bib35)) analyze the changes made to codes through extracted sequences
    from GitHub commits to determine what modifications were made and the reasons.
    This aids in inferring fine-grained changes to queries. They expand the query
    with relevant terms and eliminate irrelevant terms to minimize the negative effects
    of excessive query expansion. Wu and Yang (Wu and Yang, [2019](#bib.bib91)) analyze
    recurrent code changes within the versions history of open source projects. For
    example, if token “$a$” in the code snippet is frequently modified to token “$b$”,
    then when the target code contains token “$a$”, the token “$b$” can be used to
    expand the initial query. By expanding the query in this way, the search engine
    can retrieve the updated code snippets, avoiding developers manually updating
    query.
  prefs: []
  type: TYPE_NORMAL
- en: Based on query generation. Besides extracting relevant identifiers or keywords
    from codebases to expand queries, a few approaches aim to generate queries with
    richer semantics. Wang et al. (Wang et al., [2022b](#bib.bib83)) argue that many
    existing deep learning methods in the field overlook the knowledge gap between
    query and code description. They note that queries are typically shorter than
    the corresponding code descriptions. Moreover, using (code, description) pairs
    as training data may not generalize well to real-world user queries. To address
    these issues, they propose the model QueCos. QueCos employs LSTM with an attention
    mechanism as its architecture and uses reinforcement learning to capture the key
    semantics of a given query. It then generates a semantically enhanced query, and
    blends the results of the initial query and the semantically enhanced query using
    a mix-ranking technique to prioritize relevant code snippets. To solve the problem
    that the ground-truth answer does not have a high rank, Eberhart and McMillan
    (Eberhart and McMillan, [2022](#bib.bib17)) propose the model ZaCQ to generate
    questions that clarify the developers’ search intent. When given a code search
    query and the top-$k$ most relevant results, ZaCQ identifies ambiguous aspects
    in the query, extracts task information from attributes such as function names
    and comments, and generates a targeted clarifying question for unclear requirements.
    Eventually, it employs the feedback association algorithm to boost the ranking
    of relevant results.
  prefs: []
  type: TYPE_NORMAL
- en: Others. Rahman (Rahman, [2019](#bib.bib66)) reformulates the query from multiple
    perspectives to further enrich its semantics. He gathers search keywords from
    texts and source codes, structured entities from bug reports and documents, and
    relevant API classes from Stack Overflow Q&A posts. After that, he uses this information
    to reformulate queries and improve code search performance with appropriate term
    weighting and context awareness. To explore the search logs from Stack Overflow,
    Cao et al. (Cao et al., [2021](#bib.bib8)) provide a unique insight into query
    reformulation patterns based on large-scale real search logs from Stack Overflow.
    They build a large-scale query reconstruction corpus, incorporating both original
    queries and their corresponding reconstructed queries, to train the model. When
    given a user query, the trained model automatically generates a list of candidate
    reconstructed queries for selection. User feedback can also be actively included
    to help users refine query. Sivaraman et al. (Sivaraman et al., [2019](#bib.bib74))
    leverage user feedback to label whether the returned samples are desired or not
    desired, and then extract the logical information of the code from these positive
    and negative samples to reconstruct the query.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Current code search engines struggle with understanding natural language queries
    and only return relevant code fragments if the query includes specific identifiers
    like class and function names. However, developers may not know the relevant identifiers.
    To overcome this, data from sources such as Stack Overflow, GitHub, and codebases
    are fully mined to extract valuable information, such as code elements and term
    tokens, based on API, co-occurrence relations, search logs, and commit information.
    This information is then added to natural language queries with appropriate term
    weighting, significantly improving the representation of the query. Moreover,
    techniques exist to produce a query with a more lucid search intent by leveraging
    the extracted information, thereby enhancing search efficiency and performance.
    However, semantically related terms may not always co-occur, and simple co-occurrence
    frequencies may not be sufficient for selecting appropriate search keywords for
    query reformulation. More importantly, the success of the extended query method
    also relies on the quality of the extracted words. The crowdsourced knowledge
    from Stack Overflow may contain noise and if the words are not extracted properly,
    it can lead to excessive query expansion and negatively impact search accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg   height="105.75" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,105.75) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="78.2" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Summary
    of answers to RQ1: • Information such as API or class name, co-occurring terms,
    commit versions, search logs and user feedback are often used to reformulate the
    query. • Reformulating query based on API or class name is the most popular method
    in the past 6 years. • When reconstructing the query, it is important to avoid
    introducing noisy words that could result in excessive query expansion.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Code representation and vectorization (RQ2)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal of a code search engine is to retrieve code fragments that match the
    query semantics from the codebase. To close the semantic gap between programming
    language and natural language, deepening the understanding of code semantics and
    fostering robust interaction between code and query are essential in addition
    to improving the accuracy of query intent. In this section, we discuss the code
    encoding model, examining it through the lenses of code representation and code
    vectorization. Additionally, we discuss the crucial aspect of the interaction
    between query and code, shedding light on its significance for the comprehensive
    semantic understanding of both elements.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Code Representation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 4.1.1\. Preliminaries
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Source code is the original program text written by developers in a programming
    language, consisting of instructions and statements. It is typically compiled
    into machine language that a computer can understand and execute. During the compilation
    process from source code to machine code, various intermediate representation
    forms are generated, such as Abstract Syntax Tree (AST), Data Flow Graph (DFG),
    Control Flow Graph (CFG), and LLVM Intermediate Representation (IR). In this process,
    the compiler automatically performs program analysis techniques, including lexical,
    syntactic, and semantic analysis, to verify the correctness of the source code.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9153d070697d454960c7461d90913a99.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4. Multiple Modalities of Source Code.
  prefs: []
  type: TYPE_NORMAL
- en: Abstract Syntax Tree (AST) is a tree-like representation of the source code’s
    syntax. It consists of leaf nodes, non-leaf nodes and the edges between them,
    reflecting the source code’s rich syntax structure. For example, in Figure [4](#S4.F4
    "Figure 4 ‣ 4.1.1\. Preliminaries ‣ 4.1\. Code Representation ‣ 4\. Code representation
    and vectorization (RQ2) ‣ Survey of Code Search Based on Deep Learning")(e), the
    assignment statement “$x=0$” is represented by a non-leaf node “$assignment$”
    and three leaf nodes “$x$”, “$=$”, and “$0$” connected by directed edges. A conditional
    jump statement like $if-condition-else$ can be represented by a node with three
    branches. Typically, the standard compiler tool $tree-sitter$ ¹¹1https://github.com/tree-sitter/tree-sitter
    is commonly used to parse source code into AST.
  prefs: []
  type: TYPE_NORMAL
- en: Data Flow Graph (DFG) represents the logical relationships of the code, the
    direction of data transmission and the process of logical transformation. DFG
    abstracts the dependencies between variables in the source code, where nodes represent
    variables and edges represent the source of each variable’s value (Guo et al.,
    [2021](#bib.bib26)). As shown in Figure [4](#S4.F4 "Figure 4 ‣ 4.1.1\. Preliminaries
    ‣ 4.1\. Code Representation ‣ 4\. Code representation and vectorization (RQ2)
    ‣ Survey of Code Search Based on Deep Learning")(b), the variable $x^{11}$ returned
    by the function has two sources which can either come from $a^{10}$ or $b^{8}$
    based on the outcome of the $if$ condition. Evidently, adhering to naming conventions
    is not universal among software developers, and this can pose a challenge in comprehending
    variable semantics at times. Fortunately, DFG helps overcome the semantic deviation
    caused by inconsistent naming.
  prefs: []
  type: TYPE_NORMAL
- en: Control Flow Graph (CFG) is an abstract representation of a program’s structure,
    composed of basic blocks and directed edges between them. Each directed edge reflects
    the execution order of two basic blocks. For example, as shown in Figure [4](#S4.F4
    "Figure 4 ‣ 4.1.1\. Preliminaries ‣ 4.1\. Code Representation ‣ 4\. Code representation
    and vectorization (RQ2) ‣ Survey of Code Search Based on Deep Learning")(c), the
    outcome of an $if$ statement results in two different execution paths, namely
    “$x=a$” and “$x=b$”. CFG represents the running sequence and logical relationship
    of the code, effectively reflecting the execution semantics of the program.
  prefs: []
  type: TYPE_NORMAL
- en: Intermediate Representation (IR) is a structured and language-agnostic representation
    of the source code that captures the essential semantics and structure of the
    code, while abstracting away language-specific details. It serves as an intermediate
    step during compilation or interpretation and enables various optimizations and
    transformations to be performed on the code. For example, as shown in Figure [4](#S4.F4
    "Figure 4 ‣ 4.1.1\. Preliminaries ‣ 4.1\. Code Representation ‣ 4\. Code representation
    and vectorization (RQ2) ‣ Survey of Code Search Based on Deep Learning")(d), the
    assignment statement “$x=0$” is represented by the IR instruction `%x = alloca
    i32; store i32 0, i32 %x;` , that is, first allocate storage space for the temporary
    variable “$x$”, and then store the constant “$0$” in it. IR serves as the foundation
    for building CFG and DFG. The common IR is shared by multiple objects and can
    realize the unified representation of different programming languages.
  prefs: []
  type: TYPE_NORMAL
- en: Program Transformation (PT) denotes a code snippet that performs the same task
    as a given program. It allows for the creation of numerous versions that fulfill
    identical functional requirements. For example, Figure [4](#S4.F4 "Figure 4 ‣
    4.1.1\. Preliminaries ‣ 4.1\. Code Representation ‣ 4\. Code representation and
    vectorization (RQ2) ‣ Survey of Code Search Based on Deep Learning")(f) is another
    representation of the functionality expressed in Figure [4](#S4.F4 "Figure 4 ‣
    4.1.1\. Preliminaries ‣ 4.1\. Code Representation ‣ 4\. Code representation and
    vectorization (RQ2) ‣ Survey of Code Search Based on Deep Learning")(a).
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. Sequence
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The triumph of embedding technology in NLP has spurred researchers to investigate
    its potential use in code analysis. Presently, several studies treat source code
    as a straightforward text sequence and employ NLP techniques directly on code
    snippets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Source code Token Sequence (STS). Some approaches consider the tokenized code
    token sequence itself as the code representation and feed it into the model training
    process to learn the semantics of the code (Cambronero et al., [2019](#bib.bib7);
    Yao et al., [2019](#bib.bib95); Zhao and Sun, [2020](#bib.bib102); Ye et al.,
    [2020](#bib.bib97); Feng et al., [2020](#bib.bib18); Ling et al., [2020](#bib.bib49);
    Huang et al., [2021](#bib.bib34); Lachaux et al., [2021](#bib.bib42); Du et al.,
    [2021](#bib.bib16); Salza et al., [2023](#bib.bib70); Arakelyan et al., [2022](#bib.bib3);
    Cai et al., [2023](#bib.bib6); Wang et al., [2022a](#bib.bib84); Chai et al.,
    [2022](#bib.bib9); Li et al., [2022a](#bib.bib45), [b](#bib.bib46), [d](#bib.bib44)).
    For instance, based on the powerful learning ability of Transformer, Feng et al.
    (Feng et al., [2020](#bib.bib18)) build CodeBERT, the first large-scale program
    language pre-training model. Inspired by BERT-based NLP, they directly feed the
    token sequence of a code fragment into a multi-layer Transformer. This allows
    the model to incorporate context information and learn to identify tokens that
    are critical to the code’s semantics. Nevertheless, obtaining all the tokens of
    the source code directly through tokenization ignores syntactic details, making
    it difficult to discern whether a term originates from a variable name or a method
    call. To this end, Du et al. (Du et al., [2021](#bib.bib16)) perform data augmentation
    on code fragments from three perspectives: code structure, variable names, and
    APIs. They construct structure-centric datasets, variable-centric datasets, and
    API-centric datasets, which are respectively fed into the model for training.
    To assist the model in learning code features that are invariant across semantically
    equivalent programs, Wang et al. (Wang et al., [2022a](#bib.bib84)) develop a
    semantic-preserving transformation for code snippets. The transformation preserves
    the code’s semantics but changes its lexical appearance and syntactic structure.
    Specifically, they modify the control structure, APIs, and declarations of the
    code to enable the model to extract and learn relevant features, while preserving
    code semantics. Transformation-based approaches (such as generating semantically
    equivalent code fragments through variable renaming) often produce code with highly
    similar superficial forms, causing the model to focus on surface-level code structure
    rather than its semantic information. To mitigate this, Li et al. (Li et al.,
    [2022b](#bib.bib46)) construct positive samples using code comments and abstract
    syntax tree subtrees, encouraging the model to capture semantic information.'
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal Token Sequence (MTS). Code snippets have different information dimensions.
    A code token sequence only captures shallow features of source code, such as method
    names and code tokens, but ignores structural features like AST and CFG, which
    hold rich and well-defined source code semantics.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal learning aims to build models that can process and aggregate information
    from multiple modalities. Recently, many studies have attempted to utilize program
    analysis techniques to capture the structural and syntactic representation of
    programs. These works capture multiple code modalities to form complementary code
    representations (Haldar et al., [2020](#bib.bib27); Guo et al., [2021](#bib.bib26);
    Gu et al., [2021b](#bib.bib22), [a](#bib.bib21); Xu et al., [2021](#bib.bib92);
    Wang et al., [2021b](#bib.bib87), [2022c](#bib.bib88); Han et al., [2022](#bib.bib28);
    Niu et al., [2022](#bib.bib62); Guo et al., [2022](#bib.bib25)). For instance,
    Xu et al. (Xu et al., [2021](#bib.bib92)) extract AST from the method body as
    a structural feature and parse a code fragment into a syntax tree. The nodes in
    the tree represent various code types, such as loop structures, conditional judgment
    structures, method calls, and variable declarations. They traverse the AST through
    a breadth-first traversal strategy to obtain the AST node sequence, which they
    use for model training along with method names, API sequences, and code tokens.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance the semantic suitability of the AST tree structure for code search,
    Gu et al. (Gu et al., [2021a](#bib.bib21)) transform AST into Simplified Semantic
    Tree (SST). In contrast to AST, SST eliminates redundant nodes, enhances node
    labels, accentuates the semantic information of code snippets, and has broader
    applicability across diverse programming languages. Then they obtain tree-serialized
    representations from SST by sampling tree paths or traversing tree structures,
    which are used in multimodal learning to complement traditional source code token
    sequence representations. Similarly, Niu et al. (Niu et al., [2022](#bib.bib62))
    enrich the input representation of source code pre-trained models with simplified
    and linearized AST versions. To facilitate the transformer in encoding AST, Guo
    et al. (Guo et al., [2022](#bib.bib25)) propose a method for lossless serialization
    of AST. They transform AST into a sequential structure that maintains all tree
    information and use it as input to enhance the code representation. To provide
    complementary information for program semantic understanding, Guo et al. (Guo
    et al., [2021](#bib.bib26)) construct data flow graphs based on AST. Given a code
    fragment, they use a standard compiler to generate a AST, identify the variable
    sequence through the leaf nodes of the AST, and retain the source of each variable
    value to obtain the code’s data flow information. Compared to AST, the data flow
    information is lighter and does not bring redundant structures, making the model
    more efficient.
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. (Wang et al., [2022c](#bib.bib88)) recognize that different views
    of code offer complementary semantics. They utilize the compiler to convert the
    program into multiple views, such as AST, CFG, and equivalent programs. AST provides
    the grammatical information of the code, CFG reveals the execution information
    of the code, and different variants of the same program offer the functional information
    of the code. They use a depth-first traversal to convert a AST into a sequence
    of AST tokens, and traverse a CFG along directed edges to parse it into a sequence
    of tokens. Finally, the model learns complementary information among multiple
    views under the framework of contrastive learning.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c248151fce727429220fe5f0c57063fa.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5. Extract feature tokens from code snippet.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature Token Sequence (FTS). Gu et al. (Gu et al., [2018](#bib.bib24)) propose
    CODEnn, the first deep learning-based supervised model for code search tasks.
    Considering that the source code is not plain text, they represent it from three
    aspects: method name, API call sequence, and code tokens, as shown in Figure [5](#S4.F5
    "Figure 5 ‣ 4.1.2\. Sequence ‣ 4.1\. Code Representation ‣ 4\. Code representation
    and vectorization (RQ2) ‣ Survey of Code Search Based on Deep Learning"). They
    encode each aspect separately and then combine them into a single vector to represent
    the entire code fragment. Sachdev et al. (Sachdev et al., [2018](#bib.bib69))
    assume that source code tokens contain enough natural language information. To
    represent code information at the method level, they construct a natural language
    document by extracting method names, method calls, enumerations, string constants,
    and comments. These methods generally follow the convention of not extracting
    variable names, which can vary from one software developer to another. Cheng and
    Kuang (Cheng and Kuang, [2022](#bib.bib13)) employ a comprehensive approach to
    represent code fragments by utilizing method name, API sequence, and code tokens.
    They effectively convert each type of feature into its respective n-gram vector
    using the embedding layer. Eventually, they intelligently concatenate the three
    distinct feature vectors, resulting in the acquisition of the final code fragment
    feature matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3\. Tree/Graph
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Converting natural graph structures such as AST or CFG into sequences can result
    in loss of key structural information. To address this, some approaches try to
    directly use the graph structure as the code representation. Wan et al. (Wan et al.,
    [2019](#bib.bib79)) regard CFG as a directed graph and use Gated Graph Neural
    Network (GGNN) to obtain the vector representation of CFG. They define a graph
    as $\mathcal{G}=\{\mathcal{V},\mathcal{E}\}$, where $\mathcal{V}$ is a set of
    vertices $\left(v,\ell_{v}\right)$, $\mathcal{E}$ is a set of edges $\left(v_{i},v_{j},\ell_{e}\right)$,
    and $\ell_{v}$ and $\ell_{e}$ are the labels of vertices and edges, respectively.
    In the code retrieval scenario, each vertex represents a node in the CFG, and
    each edge signifies the control flow of the code.
  prefs: []
  type: TYPE_NORMAL
- en: The code snippets with the same functionality may have different implementations,
    while the code snippets with different code semantics may have similar structural
    features. To achieve satisfactory results in code search, Zeng et al. (Zeng et al.,
    [2023](#bib.bib99)) aim to find a more precise representation for the source code.
    They address the limitations of structural feature-based code representation methods
    by incorporating data and control dependencies. This allows semantically similar
    codes to have similar representations. To achieve this, they analyze various types
    of LLVM IR instructions, integrate data dependencies and control dependencies
    into a graph, and build a Variable-based Flow Graph (VFG). The nodes in this graph
    can be variables, opcode, or tag identifier. Considering that excessive information
    can obstruct the model from learning the fine-grained relationship between source
    code and query, they optimize VFG to minimize noise during training. Finally,
    they feed the optimized graph into a GGNN with an attention mechanism to learn
    the vector representation of the code.
  prefs: []
  type: TYPE_NORMAL
- en: With the aim of aligning the semantics of query and code, Ling et al. (Ling
    et al., [2021](#bib.bib50)) represent query text and source code as a unified
    graph structure. The program graph is generated from the AST of a code fragment.
    The AST is represented as a tuple $\langle\mathbb{N},\mathbb{T},\mathbb{X},\Delta,\phi,s\rangle$,
    where $\mathbb{N}$ denotes a set of non-terminal nodes, $\mathbb{T}$ denotes a
    set of terminal nodes, $\mathbb{X}$ represents a set of values, $\Delta:\mathbb{N}\rightarrow(\mathbb{N}\cup\mathbb{T})^{*}$
    is a function that maps a non-terminal node to its child nodes, $\phi:\mathbb{T}\rightarrow\mathbb{X}$
    is a function that maps a terminal node to an associated value, and $s$ represents
    the root node. Specifically, the program graph is composed of syntactic nodes
    (terminal and non-terminal nodes in AST) and grammatical tokens (the corresponding
    values of terminal nodes in the source code). This graph uses various types of
    edges to model syntactic and semantic relationships between nodes and tokens,
    including Child edges to link syntactic nodes in the AST, NextToken edges to connect
    each syntactic token to its subsequent in the original code, and LastLexicalUse
    edges to associate identifiers with their nearest lexical usage in the code.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Code Vectorization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once the code representation is obtained, we can embed it in a vector space.
    The choice of network architecture for encoding depends on the type of code representation
    (such as source code tokens or AST) and data structure (such as AST sequences
    or graphs). The deep neural network exhibits its excellent performance in this
    process. In this section, the code embedding network based on deep learning is
    divided into five categories: Word Embedding, Recurrent Neural Network (RNN),
    Graph Neural Network (GNN), Transformer, and mixed mode. These will be discussed
    in details below.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1\. Based on Word Embedding
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'code2vec (Alon et al., [2019](#bib.bib2)) is a seminal work in the field of
    code understanding, which realizes the representation of a code snippet as a single
    fixed-length code vector. These vectors serve as powerful tools to predict the
    semantic properties of the code snippet. Prior to this, the traditional word embedding
    algorithms include Word2Vec and GloVe. Word2Vec employs a two-layer dense neural
    network to calculate the vector representation of words. It can be trained unsupervisedly
    on large corpora, ensuring that words with common context are closely located
    in the vector space. FastText, a variant of Word2Vec, enhances word vectors with
    subword information and was once popular among researchers for code vectorization
    (Sachdev et al., [2018](#bib.bib69); Cambronero et al., [2019](#bib.bib7); Ling
    et al., [2020](#bib.bib49)). Saksham et al. (Sachdev et al., [2018](#bib.bib69))
    extract five types of information from each method: namely method name, method
    call, enumeration, string constant, and comment, to obtain the token sequence
    $c=\left\{c_{1},\ldots,c_{n}\right\}$ in the source code. Then they utilize FastText
    to calculate the vector representation of the extracted tokens sequence and obtain
    the embedding matrix $T\in\mathbb{R}^{\left|V_{c}\right|\times d}$, where $\left|V_{c}\right|$
    is the size of the extracted code vocabulary, $d$ is the dimension of selected
    token embedding, and the $k$-th row in $T$ is the vector representation of the
    $k$-th extracted word in $V_{c}$, corresponding to the set of embedding vectors
    $\left\{T\left[c_{1}\right],\ldots,T\left[c_{n}\right]\right\}$ of the code. To
    combine a set of embedding vectors of code tokens into a single vector, they apply
    a weighted sum using TF-IDF weights. TF-IDF weights are designed to enhance the
    importance of tokens that frequently appear in code snippets, and reduce the weight
    of tokens that are commonly used across all codebases. When searching for code,
    they use $T$ to calculate the embedding vector $\left\{T\left[q_{1}\right],\ldots,T\left[q_{m}\right]\right\}$
    of a given query $q=\left\{q_{1},\ldots,q_{m}\right\}$, take its average value
    to obtain the final query vector $e_{q}$, and then use FAISS to calculate the
    distance between $e_{c}$ and $e_{q}$, representing the relevance of the code fragment
    to the given query. In the second year, they further optimize the code vectorization
    process by using supervised learning to update the token matrix $T$ obtained from
    FastText and replacing the TF-IDF weight of the merged token embedding with an
    attention-based scheme (Cambronero et al., [2019](#bib.bib7)). The attention weight
    $a_{c}\in\mathbb{R}^{d}$ is a $d$-dimensional vector learned during training.
    For the code token embedding vector $\left\{T\left[c_{1}\right],\ldots,T\left[c_{n}\right]\right\}$,
    the attention weight of each token is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\alpha_{i}=\frac{\exp\left(a_{c}\cdot T\left[c_{i}\right]^{\top}\right)}{\sum_{i=1}^{n}\exp\left(a_{c}\cdot
    T\left[c_{i}\right]^{\top}\right)}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Finally, the code’s embedding vector is obtained by weighted summation of the
    attention weight $\alpha_{i}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $e_{c}=\sum_{i=1}^{n}\alpha_{i}T\left[c_{i}\right].$ |  |'
  prefs: []
  type: TYPE_TB
- en: 4.2.2\. Based on Recurrent Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Recurrent Neural Networks (RNNs) are widely used for embedding sequences in
    natural language processing. Inspired by natural language processing techniques,
    some researchers have attempted to use RNN to encode code token sequences. For
    instance, Gu et al. (Gu et al., [2018](#bib.bib24)) extract method names, API
    call sequences, and tokens from code snippets, represented as $C=[M,A,\Gamma]$,
    where $M=w_{1},\ldots,w_{N_{M}}$ is a sequence of $N_{M}$ tokens obtained by dividing
    the method name with camel case, $A=a_{1},\ldots,a_{N_{A}}$ is a sequence of $N_{A}$
    consecutive API calls, and $\Gamma=\left\{\tau_{1},\ldots,\tau_{N_{\Gamma}}\right\}$
    is a collection of tokens in code fragments. They then encode the method name
    sequence and the API sequence separately using an RNN with a max-pooling layer.
    Since the tokens do not have a strict order in the source code, they are encoded
    using a multi-layer perceptron. Finally, the three vectors are combined into a
    single vector $c$, representing the entire code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $c={LSTM}_{1}(M)+{LSTM}_{2}(A)+{MLP}(\Gamma).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Sun et al. (Sun et al., [2022a](#bib.bib76)) translate the code into a natural
    language description to obtain a translation, and apply the LSTM architecture
    to build a translation encoder. They use word embedding to map words in the translation
    sequence, $s=w_{1},\cdots,w_{N^{s}}$, to vector representations $\boldsymbol{w}_{i}=\psi\left(w_{i}\right)$.
    These vectors are then arranged in an embedding matrix $E\in\mathbb{R}^{n\times
    m}$, where $n$ is the size of the vocabulary, $m$ is the dimension of the embedding
    vector. The embedding matrix $E=\left(\psi\left(w_{1}\right),\ldots,\psi\left(w_{i}\right)\right)^{T}$
    is randomly initialized and learned together with the encoder during training.
    The translation’s vector representation $v^{s}$ is obtained from the embedding
    matrix and inputted into the translation encoder to obtain the final embedding
    vector $e^{s}$. The hidden state $h_{i}^{s}$ for the $i$-th word in $s$ is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $h_{i}^{s}={LSTM}\left(h_{i-1}^{s},\boldsymbol{w}_{i}\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'In addition, they also employ an attention mechanism to alleviate the long
    dependency problem in long text sequences. The attention weight for each word
    is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\alpha_{i}^{s}=\frac{\exp\left(f\left(h_{i}^{s}\right)\cdot u^{s}\right)}{\sum_{j=1}^{N}\exp\left(f\left(h_{j}^{s}\right)\cdot
    u^{s}\right)},$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $f(\cdot)$ represents the linear layer; $u^{s}$ represents the context
    vector, which is the high-level representation of all words in $s$. $\cdot$ represents
    the inner product of $h_{i}^{s}$ and $u^{s}$. $u^{s}$ is randomly initialized
    and jointly learned during training. The final embedding representation $e^{s}$
    of $s$ is computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $e^{s}=\sum_{j=1}^{N^{s}}\alpha_{i}^{s}\cdot h_{i}^{s}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 4.2.3\. Based on Graph Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As mentioned earlier, the code can be represented as a graph structure. This
    representation is superior to a sequence representation as it retains more information
    about the code. Some approaches use Graph Neural Networks (GNNs) to encode the
    graph structure, which is constructed based on structures such as AST and CFG
    (Zeng et al., [2023](#bib.bib99); Ling et al., [2021](#bib.bib50); Liu et al.,
    [2023](#bib.bib53)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Zeng et al. (Zeng et al., [2023](#bib.bib99)) construct a variable-based flow
    graph (VFG) based on the data dependencies and control dependencies of the source
    code. VFG is a directed graph with multiple types of edges, represented as $G=(V,E)$.
    They then use a GGNN model, which is well suited for handling graph-structured
    data, to learn the vector representation of code. Here, $V$ represents a set of
    nodes $\left(v,l_{v}\right)$, $E$ represents a set of edges $\left(v_{i},v_{j},l_{\left(v_{i},v_{j}\right)}\right)$,
    and $l_{v}$ represents the label of node $v$, which consists of variables in the
    IR instruction. $l_{\left(v_{i},v_{j}\right)}$ represents the label of the edge
    from $v_{i}$ to $v_{j}$, including data dependency and control dependency. GGNN
    learns the vector representation of $G$ through the message passing mechanism.
    In each iteration, each node $v_{i}$ receives a message $m_{t}^{v_{j}\mapsto v_{j}}=W_{l_{\left(v_{i},v_{j}\right)}}h_{v_{j}}^{t-1}$
    from its neighbor $v_{j}$, which is determined by the type of edge between them.
    GGNN then aggregates all messages $m_{t}^{i}=\sum_{v_{j}\in{Neibour}\left(v_{i}\right)}\left(m_{t}^{v_{j}\mapsto
    v_{i}}\right)$ from neighbors of $v_{i}$, and updates the embedding $h_{v_{i}}^{t}=GRU\left(m_{t}^{i},h_{v_{i}}^{t-1}\right)$
    of each node $v_{i}$ using GRU. Considering that different nodes contribute differently
    to code semantics, they use an attention mechanism to calculate the importance
    of different nodes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\alpha_{i}={sigmoid}\left(f\left(h_{v_{i}}\right)\cdot u_{vfg}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $f(\cdot)$ is a linear layer and $u_{vfg}$ represents the context vector,
    which is a high-level representation of the entire nodes in the graph, learned
    together during training. The final embedding of the entire graph is expressed
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $h_{vfg}=\sum_{v_{i}\in V}\left(\alpha_{i}h_{v_{i}}\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Given that query text has sequence characteristics as a natural language, Zeng
    et al. (Zeng et al., [2023](#bib.bib99)) use LSTM to encode a query into a vector
    space and calculate its similarity with the code’s semantic vector. Different
    from this, Ling et al. (Ling et al., [2021](#bib.bib50)) design a unified graph
    structure for both query and code. They then use RGCN to encode text graph and
    code graph, which is a variant of GNN. Given a code graph $G_{e}=\left(\mathcal{V}_{e},\mathcal{E}_{e},\mathcal{R}_{e}\right)$,
    to calculate the updated embedding vector $\mathbf{e}_{i}$ of each node $e_{i}$
    in the code graph $G_{e}$, RGCN defines the propagation model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $\mathbf{e}_{i}^{(l+1)}={ReLU}\left(W_{\Theta}^{(l)}\mathbf{e}_{i}^{(l)}+\sum_{r\in\mathcal{R}_{e}}\sum_{j\in\mathcal{N}_{i}^{r}}\frac{1}{\left&#124;\mathcal{N}_{i}^{r}\right&#124;}W_{r}^{(l)}\mathbf{e}_{j}^{(l)}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbf{e}_{i}^{(l+1)}$ represents the updated embedding vector of node
    $\mathbf{e}_{i}$ in the $(l+1)$th layer of RGCN, $\mathcal{R}_{e}$ represents
    a set of relations (that is, the types of edge), $\mathcal{N}_{i}^{r}$ is a set
    of neighbors of node $e_{i}$ under the edge type $r\in\mathcal{R}_{q}$, and $W_{\Theta}^{(l)}$
    and $W_{r}^{(l)}$ are the parameters that the RGCN model needs to learn. By encoding
    the graph structure of the code with the RGCN model, they obtain the node embedding
    $\mathbf{X}_{e}=\left\{\mathbf{e}_{j}\right\}_{j=1}^{N}\in\mathbb{R}^{(N,d)}$
    of the code graph. The embedding $\mathbf{X}_{q}=\left\{\mathbf{q}_{i}\right\}_{i=1}^{M}\in\mathbb{R}^{(M,d)}$
    of the query text graph can be obtained similarly.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4\. Based on Transformers
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In recent years, large pre-trained models have brought significant improvements
    to many NLP tasks. Many approaches train deep learning models on massive plain
    text data using self-supervised objectives, with the Transformer neural network
    architecture being the most prominent. Transformer contains multiple self-attention
    layers and can continuously learn in an end-to-end manner through gradient descent,
    because each of its components is differentiable. The success of pre-training
    models in NLP has inspired researchers to create code understanding pre-training
    models based on transformers, driving the growth of code intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Feng et al. (Feng et al., [2020](#bib.bib18)) propose CodeBERT, the first large-scale
    natural language-programming language pre-training model for multiple programming
    languages. During pre-training, they use special tokens to splice natural language
    text sequences and code token sequences, which are fed into a multi-layer Transformer-based
    CodeBERT. The model learns the semantic relationship between natural language
    and programming language through Masked Language Modeling (MLM) and Replaced Token
    Detection (RTD) tasks, ultimately yielding a general vector for code understanding
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Some methods aim to provide a more comprehensive understanding of code by feeding
    multiple modal representations of the source code into the Transformer. For instance,
    Guo et al. (Guo et al., [2021](#bib.bib26)) combine the sequence of variables
    extracted from the data flow graph with the sequence of code tokens, and feed
    both into the multi-layer transformer-based GraphCodeBERT. They then use Masked
    Language Modeling (MLM), Edge Prediction (EP) and Node Alignment (NA) tasks to
    guide the model to learn the code structure and data dependencies. Additionally,
    Guo et al. (Guo et al., [2022](#bib.bib25)) merge the serialized AST with the
    comment text sequence and feed both into the multi-layer Transformer-based UnixCoder.
    They then use Masked Language Modeling (MLM), Unidirectional Language Modeling
    (ULM), DeNoiSing (DNS), Multi-modal Contrastive Learning (MCL), and Cross-Modal
    Generation (CMG) to learn the syntactic information of the code and enhance the
    understanding of the code semantics. Wang et al. (Wang et al., [2022c](#bib.bib88))
    add not only the serialized AST, which reflects the grammatical information of
    the code, but also the CFG sequence, which reflects the logical information, and
    the token sequence of code fragments that have different implementation but the
    same semantics. All these are fed into the CODE-MVP. They then use Multi-View
    Contrastive Learning (MVCL), Fine-Grained Type Inference (FGTI), and Multi-View
    Masked Language Modeling (MMLM) tasks to help the model learn the structural information
    of the code.
  prefs: []
  type: TYPE_NORMAL
- en: To ensure accurate code search results for each query, it’s essential for the
    model to make the query and correct code vectors as close as possible in the shared
    vector space, and as far away as possible from incorrect code vectors. To achieve
    this, some researchers integrate contrastive learning into the Transformer network
    architecture and enhance the performance of code search engines by constructing
    positive and negative samples. For instance, Huang et al. (Huang et al., [2021](#bib.bib34))
    form negative sample pairs by randomly selecting the query and code within the
    same batch. Meanwhile, they generate positive sample pairs by duplicating the
    query, meaning they rephrase the query without altering its semantics. Developers
    often split a complete comment over several lines to improve readability. Inspired
    by this, Li et al. (Li et al., [2022a](#bib.bib45)) combine consecutive comment
    lines into a single line to make the most of code snippets without comments to
    form positive sample pairs for contrastive learning. When multiple modalities
    of code are available, Wang et al. (Wang et al., [2021b](#bib.bib87)) combine
    different modalities to create positive sample pairs and use both in-batch and
    cross-batch sampling methods to generate negative sample pairs.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer is favored by researchers for its general and powerful modeling
    capabilities. To improve code semantic understanding, researchers have explored
    various pre-training tasks such as MLM, RTD, EP, and FGTI, etc. to guide model
    learning and enable the model to learn the grammatical and structural information
    of the code.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.5\. Mixed Mode
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The structures used to represent different modes of the code may vary, and some
    methods choose the proper model to deal with each mode accordingly. For instance,
    Wan et al. (Wan et al., [2019](#bib.bib79)) extract AST and CFG as code representations,
    and propose Tree-LSTM for the tree structure of AST. Compared with the traditional
    LTSM unit, the Tree-LSTM unit contains multiple forgetting gates. For the directed
    graph structure of the CFG, they employ the Gated Graph Neural Network (GGNN)
    for encoding, and the Gated Recurrent Unit (GRU) for updating the hidden state
    of each vertex. Finally, they obtain the overall embedding vector of CFG by aggregating
    the embeddings of all vertices.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Interaction Between Code and Query
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The existence of a semantic gap between natural language queries and code snippets
    has posed a significant challenge. However, several methods have emerged to bridge
    this gap by effectively modeling the interaction between the two, thus enhancing
    the understanding of their respective semantics (Haldar et al., [2020](#bib.bib27);
    Xu et al., [2021](#bib.bib92); Arakelyan et al., [2022](#bib.bib3); Cheng and
    Kuang, [2022](#bib.bib13); Cai et al., [2023](#bib.bib6)). For instance, leveraging
    the widely adopted approach of calculating the overall similarity between queries
    and code, Haldar et al. (Haldar et al., [2020](#bib.bib27)) additionally introduced
    the concept of evaluating local similarity between the two components. This perspective
    provides valuable insights into the finer-grained aspects of their correlation.
    Xu et al. (Xu et al., [2021](#bib.bib92)) proposed an innovative two-stage attention
    network architecture. In the initial stage, the semantics of both the query and
    the code are extracted. Subsequently, in the second stage, a joint attention mechanism
    is employed to facilitate the interaction between the two, enabling the capture
    of their semantic relevance. This approach presents a significant advancement
    in bridging the gap between natural language queries and code snippets. Similarly,
    Cheng and Kuang (Cheng and Kuang, [2022](#bib.bib13)) combined neural information
    retrieval (IR) and semantic matching to enhance the interaction between queries
    and code. Their approach involved capturing two matching signals simultaneously.
    Firstly, neural IR captured keyword matching signals, encompassing words, terms,
    and phrases, within query-code pairs. Secondly, semantic matching employed a joint
    attention mechanism to simultaneously focus on description attention and code
    attention, thereby acquiring comprehensive semantic vector representations for
    both components. Particularly, Arakelyan et al. (Arakelyan et al., [2022](#bib.bib3))
    proposed a meticulous approach wherein they parse the query by leveraging distinct
    part-of-speech roles to decompose it into concise semantic phrases. Specifically,
    nouns and noun phrases are associated with data entities within the code, while
    verbs depict operations or transformations performed on those entities. The interaction
    between the query and code is facilitated through an entity discovery module and
    an action module. The entity discovery module receives a string referencing a
    data entity and aims to identify code tokens that exhibit strong correlation with
    the given string. The output of this module is then employed as input for the
    action module, enabling prediction of the target entity object for the intended
    operational behavior. This comprehensive methodology offers valuable insights
    into enhancing the understanding and alignment between natural language queries
    and code snippets.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Source code can be represented in various modes, including code token sequences,
    Abstract Syntax Trees (AST), Control Flow Graphs (CFG), Data Flow Graphs (DFG),
    and Program Transformation (PT). Code token sequences are commonly utilized for
    extracting textual features of the code, while AST and CFG are frequently employed
    for extracting the structural features of the code. As Table [4](#S4.T4 "Table
    4 ‣ 4.4\. Summary ‣ 4\. Code representation and vectorization (RQ2) ‣ Survey of
    Code Search Based on Deep Learning") demonstrates, the multiple modes of code
    are mainly fed into the network in two forms of data structures: sequence and
    graph. The information from these different modes can complement each other, enabling
    the model to fully grasp the semantic information of the code. Based on the type
    of input data structure, an appropriate sequence embedding model or graph embedding
    model is selected to obtain the code vector representation. As depicted in Figure
    5, the predominant choice for code representation is source code token sequence
    (STS), while Transformer architecture has emerged as the widely adopted code encoder
    of choice. Not all modalities will have equal impact on the final representation
    of the code. By aggregating embedding vectors from different modes of the source
    code, assigning attention weights, and taking the weighted sum, a more semantically
    rich code vector representation can be obtained. Furthermore, it is crucial to
    foster a fine-grained interaction between the query and code, facilitating a more
    robust learning of their intricate semantics.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 4. Overview of approaches for code representation and code vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Work | Code Representation | Code Vectorization | Interaction |'
  prefs: []
  type: TYPE_TB
- en: '| STS | MTS | FTS | Tree/Graph | Word Embedding | RNN | GNN | Transformer |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | CODEnn (Gu et al., [2018](#bib.bib24)) |  |  | ✓ |  |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | NCS (Sachdev et al., [2018](#bib.bib69)) |  |  | ✓ |  | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | UNIF (Cambronero et al., [2019](#bib.bib7)) | ✓ |  |  |  | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | MMAN (Wan et al., [2019](#bib.bib79)) | ✓ |  |  | ✓ |  | ✓ | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CoaCor (Yao et al., [2019](#bib.bib95)) | ✓ |  |  |  |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CodeBERT (Feng et al., [2020](#bib.bib18)) | ✓ |  |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | AdaCS (Ling et al., [2020](#bib.bib49)) | ✓ |  |  |  |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | MP-CAT (Haldar et al., [2020](#bib.bib27)) |  | ✓ |  |  |  | ✓ |  |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | ${TranS}^{3}$ (Wang et al., [2020](#bib.bib86)) |  |  |  | ✓ |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Zhao and Sun (Zhao and Sun, [2020](#bib.bib102)) | ✓ |  |  |  |  |
    ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CO3 (Ye et al., [2020](#bib.bib97)) | ✓ |  |  |  |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CRaDLe (Gu et al., [2021b](#bib.bib22)) |  | ✓ |  |  |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GraphCodeBERT (Guo et al., [2021](#bib.bib26)) |  | ✓ |  | ✓ |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | DGMS (Ling et al., [2021](#bib.bib50)) |  |  |  | ✓ |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CoCLR (Huang et al., [2021](#bib.bib34)) | ✓ |  |  |  |  |  |  | ✓
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | DOBF (Lachaux et al., [2021](#bib.bib42)) | ✓ |  |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Gu et al. (Gu et al., [2021a](#bib.bib21)) |  | ✓ |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | TabCS (Xu et al., [2021](#bib.bib92)) |  | ✓ |  |  |  |  |  | ✓ |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | MuCoS (Du et al., [2021](#bib.bib16)) | ✓ |  |  |  |  |  |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | SynCoBERT (Wang et al., [2021b](#bib.bib87)) |  | ✓ |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Salza et al. (Salza et al., [2023](#bib.bib70)) | ✓ |  |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | G2SC (Shi et al., [2022](#bib.bib71)) |  | ✓ |  |  |  | ✓ |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CSRS (Cheng and Kuang, [2022](#bib.bib13)) |  |  | ✓ |  | ✓ |  |  |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | TranCS (Sun et al., [2022a](#bib.bib76)) | ✓ |  |  |  |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Wang et al. (Wang et al., [2022a](#bib.bib84)) | ✓ |  |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | ${NS}^{3}$ (Arakelyan et al., [2022](#bib.bib3)) | ✓ |  |  |  |  |  |  |
    ✓ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CodeRetriever (Li et al., [2022a](#bib.bib45)) | ✓ |  |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CDCS (Chai et al., [2022](#bib.bib9)) | ✓ |  |  |  |  |  |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CSSAM (Cai et al., [2023](#bib.bib6)) | ✓ |  |  | ✓ |  | ✓ | ✓ |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | SCodeR (Li et al., [2022b](#bib.bib46)) | ✓ |  |  |  |  |  |  | ✓
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | SPT-Code (Niu et al., [2022](#bib.bib62)) |  | ✓ |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Li et al. (Li et al., [2022d](#bib.bib44)) | ✓ |  |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CTBERT (Han et al., [2022](#bib.bib28)) |  | ✓ |  |  |  |  |  | ✓
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CODE-MVP (Wang et al., [2022c](#bib.bib88)) |  | ✓ |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | UniXcoder (Guo et al., [2022](#bib.bib25)) |  | ✓ |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | deGraphCS (Zeng et al., [2023](#bib.bib99)) |  |  |  | ✓ |  |  | ✓
    |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | MulCS (Ma et al., [2023](#bib.bib57)) |  |  |  | ✓ |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | GraphSearchNet (Liu et al., [2023](#bib.bib53)) |  |  |  | ✓ |  |  |
    ✓ |  |  | ![Refer to caption](img/1c9810d15ace10937155ce4435486ef0.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Code representation
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/abcf4941f39e9a298a479fedc7e8e362.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Code vectorization
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6. Code representation and code vectorization.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg   height="149.96" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,149.96) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="122.4" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Summary
    of answers to RQ2: • An array of diverse modalities, including Abstract Syntax
    Trees (AST), Data Flow Graphs (DFG), Control Flow Graphs (CFG), Program Trees
    (PT), and other code representations, can be leveraged to effectively augment
    the model’s ability to learn the intricate semantics of the code. • When feeding
    the code into the encoder, it is common practice to represent it as a sequence
    or a graph. Among these forms, the source code token sequence (STS) is the prevailing
    choice in most scenarios. • Transformer is the most popular code encoder in the
    past 6 years. • Facilitating a fine-grained interaction between the query and
    code can enhance the model’s ability to grasp their semantics.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Training Method (RQ3)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As previously stated, the core problem of code search based on deep learning
    is to employ a model $f_{\theta}(q,c)=E_{\theta_{1}}(q)\cdot E_{\theta_{2}}(c)$
    to estimate the relevance scores of query and code, and then sort according to
    the scores to obtain the code that matches the query. Sections 3 and 4 elaborate
    on the utilization of deep learning models to obtain representations for both
    the query and code. This section provides an overview of the existing techniques
    for training the model parameter $\theta$. The section is organized as follows:
    first, the pre-training technology of the Transformer-based large model relevant
    to code intelligence is introduced, encompassing three categories: sequence tasks,
    code structure tasks, and multimodal tasks. Second, the training of the code search
    model is described. Finally, alternative training methods for other scenarios,
    such as meta-learning and data enhancement, are introduced.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Pre-training of Code Intelligence Large Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Encoding query and code using Transformer encoder is an important encoding
    strategy in code search tasks. Pre-trained Transformers have proven their effectiveness
    in many fields (Devlin et al., [2019](#bib.bib14); Liu et al., [2019b](#bib.bib54)).
    Large-scale pre-training enables the model to start from a better initial state,
    thereby facilitating efficient training and fine-tuning for downstream tasks.
    At present, there have been many works to introduce pre-training technology into
    the field of code intelligence, resulting in the design of various pre-training
    tasks. These tasks can be classified into three categories: sequence tasks, code
    structure tasks, and multi-modal matching tasks. Table [5](#S5.T5 "Table 5 ‣ 5.1\.
    Pre-training of Code Intelligence Large Models ‣ 5\. Training Method (RQ3) ‣ Survey
    of Code Search Based on Deep Learning") summarizes the approaches along these
    three dimensions. Sequence tasks treat the input as a sequence and train by using
    popular NLP tasks. Code structure tasks build upon sequences and utilize the relationships
    between tokens in a code structure graph for training. Multi-modal matching tasks
    try to introduce contrastive learning into pre-training, and use different modalities
    to construct matching positive samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5. Overview of pre-training tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Work | Sequence Tasks | Code Structure Tasks | Multimodal Matching
    Tasks |'
  prefs: []
  type: TYPE_TB
- en: '| MLM | RTD | ULM | DNS | EP | IP | ICP | MMP | MCCL | CMG |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CodeBERT (Feng et al., [2020](#bib.bib18)) | ✓ | ✓ |  |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GraphCodeBERT (Guo et al., [2021](#bib.bib26)) | ✓ |  |  |  | ✓ |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Syncobert (Wang et al., [2021b](#bib.bib87)) | ✓ |  |  |  | ✓ | ✓
    |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CODE-MVP (Wang et al., [2022c](#bib.bib88)) | ✓ |  |  |  |  | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CTBERT (Han et al., [2022](#bib.bib28)) |  |  |  |  | ✓ |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | SPT-Code (Niu et al., [2022](#bib.bib62)) |  |  |  | ✓ |  |  | ✓ |
    ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | DOBF (Lachaux et al., [2021](#bib.bib42)) |  |  |  |  |  |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | SCodeR (Li et al., [2022b](#bib.bib46)) |  |  |  |  |  |  |  |  |
    ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | UniXcoder (Guo et al., [2022](#bib.bib25)) | ✓ |  | ✓ | ✓ |  |  |  |  |  |
    ✓ |'
  prefs: []
  type: TYPE_TB
- en: 5.1.1\. Sequence Tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Sequences are the most straightforward and efficient way of representing code
    and text. Data structures like AST and CFG can be transformed into sequences through
    a serialization algorithm. There are many established pre-training tasks in NLP
    for encoding sequences in Transformer models, which can be directly introduced
    into code intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 'Masked Language Model (MLM) (Feng et al., [2020](#bib.bib18); Guo et al., [2021](#bib.bib26);
    Wang et al., [2021b](#bib.bib87); Guo et al., [2022](#bib.bib25); Wang et al.,
    [2022c](#bib.bib88)). Given a token sequence $X=\left\{[cls],x_{1},x_{2},\cdots\right.$
    $\left.,x_{N},[end]\right\}$, some tokens are randomly masked as $[mask]$ and
    a new token sequence is denoted as $X^{mask}=\left\{x_{1},x_{2},\cdots,[mask],\cdots,x_{N}\right\}$.
    $X^{mask}$ is input ted into the model, and the objective-task can restore the
    original tokens through the representation at the output layer. The loss function
    can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | ${Loss}_{MLM}=-\sum_{i}\log p\left(x_{i}\mid X^{mask}\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: MLM can be extended to the input of multiple modal splicing, that is, splicing
    query, code, and other modalities of code (such as flattened AST and CFG sequences).
    This results in a new sequence $X=\left\{[cls],x_{1}^{(1)},x_{2}^{(1)},\cdots,x_{N}^{(1)},[{sep}],x_{1}^{(2)},x_{2}^{(2)},\cdots,x_{M}^{(2)},[end]\right\}$,
    on which the MLM task is performed. In some literatures, this type of Masked Language
    Model that combines multiple modalities is referred to as Multi-Modal Masked Language
    Modeling (MMLM) (Wang et al., [2021b](#bib.bib87)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Replaced Token Detection (RTD) (Feng et al., [2020](#bib.bib18)). A token sequence
    is given, and some of its tokens are randomly replaced with others, creating a
    new token sequence, denoted as $X^{corrupt}=\left\{[cls],x_{1}^{\prime},x_{2}^{\prime},\cdots,\right.$
    $\left.x_{N}^{\prime},[end]\right\}$. The task makes predictions at the output
    layer regarding whether a token in $X^{corrupt}$ is a replacement token. The loss
    function can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | ${Loss}_{RTD}=-\sum_{i}\left(\delta(i)\log\left(i\mid X^{corrupt}\right)+(1-\delta(i))(1-\left.\log
    p\left(i\mid X^{corrupt}\right)\right)\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: When the token is not replaced (that is, $x_{i}^{\prime}=x_{i}$), $\delta(i)=1$,
    otherwise $\delta(i)=0$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unidirectional Language Modeling (ULM) (Guo et al., [2022](#bib.bib25)). ULM
    predicts the tokens in the sequence from start to end and is a widely used auxiliary
    task for training the Transformer decoder. In some models that train both the
    encoder and decoder (Guo et al., [2022](#bib.bib25)), ULM is selected to train
    the decoder. The loss function can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | ${Loss}_{ULM}=-\sum_{i}\log p\left(x_{i}\mid X[<i]\right),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $X[<i]$ represents the tokens before the $i$-th token.
  prefs: []
  type: TYPE_NORMAL
- en: 'DeNoiSing (DNS) (Guo et al., [2022](#bib.bib25); Niu et al., [2022](#bib.bib62)).
    The DeNoiSing pre-training objective uses $[mask_{i}]$ to mask multiple spans
    in the input tokens sequence to obtain $X^{mask}$, where $i$ represents the $i$-th
    masked span. All $X^{mask}$ are concatenated and represented as $Y=\left\{y_{1},y_{2},\cdots\right\}$.
    The corresponding output can be reconstructed through the encoder-decoder framework,
    and the objective function can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | ${Loss}_{DNS}=-\sum_{i}\log\left(y_{i}\mid X^{mask},Y[<i]\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 5.1.2\. Code Structure Tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The structured representation of code, such as AST and CFG, often holds a lot
    of information, such as the variable jump relationships, code execution order,
    etc. These information can be mined to design auxiliary tasks that aid in pre-training
    code intelligence models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Edge Prediction (EP) (Guo et al., [2021](#bib.bib26); Wang et al., [2021b](#bib.bib87);
    Han et al., [2022](#bib.bib28)). When encoding structured code representations
    using a Transformer, it’s common to flatten it into a sequence $X=\left\{[cls],x_{1},x_{2},\cdots,x_{N},[end]\right\}$.
    After that, the connection relationship of the nodes in the original graph/tree
    can be inherited into the tokens sequence. For example, if node $i$ points to
    node $j$ in the original graph, then token $x_{i}$ and token $x_{j}$ will inherit
    this connection, allowing it to guide the model’s pre-training. The probability
    $p\left(e_{ij}\mid X\right)$ of whether there is an edge between two tokens is
    measured by the similarity of representations $\vec{e}_{i}$ and $\vec{e}_{j}$
    of $x_{i}$ and $x_{j}$, namely $<\vec{e}_{i},\vec{e}_{j}>$. The objective function
    of EP can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | ${Loss}_{EP}=-\sum_{ij}\left(\delta\left(e_{ij}\right)\log\left(e_{ij}\mid
    X\right)+\left(1-\delta\left(e_{ij}\right)\right)(1-\left.{logp}\left(e_{ij}\mid
    X\right)\right)\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: When there is an edge between token $x_{i}$ and token $x_{j}$, $\delta\left(e_{ij}\right)=1$,
    otherwise $\delta\left(e_{ij}\right)=0$. Besides single-modal structures, cross-modal
    structures can also be used to construct edges between tokens, for example, by
    combining code sequence tokens with data flow tokens. There is a correspondence
    between them, and edges can be constructed according to the corresponding relationship.
  prefs: []
  type: TYPE_NORMAL
- en: 'Identifier Prediction (IP) (Wang et al., [2022c](#bib.bib88), [2021b](#bib.bib87)).
    The tokens in the code snippet can be classified into two types: identifiers (such
    as variable and function names, etc.) and non-identifiers (keywords that indicate
    grammatical operations). This classification can be performed through AST-based
    code analysis. The information helps construct auxiliary tasks to determine whether
    a token is an identifier. The probability $p(i\mid X)$ that a token is an identifier
    can be predicted based on its representation through a simple transformation,
    namely $\sigma\left(\vec{w}^{T}\cdot\vec{e}_{i}\right)$, where $\sigma$ is a sigmoid
    function that maps the score to 0-1. The objective function of IP can be expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (15) |  | ${Loss}_{IP}=-\sum_{i}(\delta(i)\log p(i\mid X)+(1-\delta(i))(1-\log
    p(i\mid X))).$ |  |'
  prefs: []
  type: TYPE_TB
- en: If the node is an identifier, $\delta(i)=1$; otherwise $\delta(i)=0$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Identifier Content Prediction (ICP) (Lachaux et al., [2021](#bib.bib42); Niu
    et al., [2022](#bib.bib62)). Identifiers such as function names and variable names
    carry significant information that can be considered as a general expression of
    functions. To allow the model to learn this high-level semantic information, the
    following proxy tasks can be designed: masking the identifier as $[mask]$, and
    predicting the masked content. Compared to randomly masking tokens, accurately
    masking identifiers by analyzing the code structure increases the difficulty of
    the task and makes the model learn higher-level information. When masking identifiers,
    to avoid the model dependence on the same variable name appearing in the context
    for inference, all the same tokens can be replaced by $[mask_{i}]$ at the same
    time, where $i$ represents the $i$-th replaced identifier. When actually designing
    the training task, $[mask_{i}]$ can be connected to the masked token. All masked
    information is concatenated as output and then the model is trained using the
    encoder-decoder framework. The corresponding objective function can be expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (16) |  | ${Loss}_{ICP}=-\sum_{i}\log p\left(y_{i}\mid X^{mask},y[<i]\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 5.1.3\. Multimodal Matching Tasks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: From a multi-modal viewpoint, code intelligence is a multi-modal task involving
    natural language, code sequence representation, and code structure representation.
    At present, many studies are also examining this problem from a multi-modal perspective
    and designing pre-training tasks for multi-modal matching.
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-Modal Matching Prediction (MMP) (Niu et al., [2022](#bib.bib62)). The
    multiple modalities of code can be seen as different ways of describing information
    with the same semantics. MMP splices the descriptions of these different modalities
    to obtain $M^{(1)}//M^{(2)}//\cdots//M^{(N)}$, where $M^{(i)}$ represents the
    token sequence composed of modality $i$. The number of modalities can be greater
    than or equal to 2, but attention should be paid to controlling the total amount,
    as sequences that are too long can result in a high computational complexity for
    the Transformer. Usually, $N$ is set to 2 or 3. Assuming $N=2$, the spliced sequence
    is denoted as $M_{i}^{(1)}//M_{j}^{(2)}$, where $M_{i}^{(j)}$ represents the $j$-th
    modal sequence representation of code $i$. The training objective is to determine
    if the two modalities in the spliced sequence describe the same piece of code
    information. If they do, the label is $\delta(ij)=1$, otherwise it is $\delta(ij)=0$.
    The spliced sequence can be input ted to the model, and the probability of $i=j$
    can be estimated by the representation of $[cls]$. The loss function can be expressed
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | ${Loss}_{MMP}=-\sum_{ij}\left(\delta(ij)\log p\left(cls\mid M_{i}^{(1)}//M_{j}^{(2)}\right)+(1-\delta(ij))\left(1-\log
    p\left(cls\mid M_{i}^{(1)}//M_{j}^{(2)}\right)\right)\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Multi-modal Concatenation Contrastive Learning (MCCL) (Li et al., [2022b](#bib.bib46)).
    In multi-modal contrastive learning, the model encodes information from two modalities
    and calculates similarity in the representation space, namely $s_{ij}=<E\left(M_{i}^{(1)}\right),E\left(M_{j}^{(2)}\right)>$.
    However, this approach doesn’t model the fine-grained interaction of $M_{i}^{(i)}$
    and $M_{i}^{(j)}$ at the token level. To address this limitation, the token sequences
    of the two modalities can be spliced and encoded together with a single encoder,
    namely $s_{ij}=E\left(M_{i}^{(1)}//M_{j}^{(2)}\right)$. Based on the similarity,
    the loss function is constructed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | ${Loss}_{MCCL}=-\sum_{i}\log\frac{\exp\left(s_{ii}/\tau\right)}{\sum_{j}\exp\left(s_{ij}/\tau\right)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: After training with this objective, researchers can continue to use $s_{ij}$
    to weight the samples effectively in multi-modal contrastive learning, thereby
    enhancing its performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Cross Modal Generation (CMG) (Guo et al., [2022](#bib.bib25)). CMG aims to
    predict the information of another modal sequence through the information of one
    modal sequence. For example, the comment of the code can be predicted based on
    the function body sequence. If the source modality is denoted as $X$ and the target
    modality is denoted as $Y$, the objective function can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (19) |  | ${Loss}_{CMG}=-\sum_{i}\log p\left(y_{i}\mid X,Y[<i]\right).$ |  |'
  prefs: []
  type: TYPE_TB
- en: CMG can be regarded as a generation task that utilizes multi-modal matching
    information, similar to an implicit matching task.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.4\. Summary
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section introduces ten pre-training tasks for code intelligence, covering
    code sequence, code structure, and code multimodality. These tasks allow for training
    large models using extensive amounts of unlabeled data, leading to pre-trained
    models that can excel in various downstream tasks, such as code search, with minimal
    fine-tuning efforts. In practical applications, simple training tasks like MLM
    are often highly effective and play a fundamental role. Their simplicity and effectiveness
    make them a staple in most pre-training models, such as CodeBERT (Feng et al.,
    [2020](#bib.bib18)) and GraphCodeBERT (Guo et al., [2021](#bib.bib26)). Complex
    tasks show limited improvement in the presence of simple training tasks as seen
    in some ablation experiments (Guo et al., [2022](#bib.bib25)), and they do not
    generalize well to all data and tasks (Wang et al., [2021b](#bib.bib87)). Despite
    this, the variety of tasks also expands the possibilities for pre-training code
    intelligence, increasing its potential. Furthermore, for code search, pre-training
    with multimodal contrastive learning has produced better results due to its greater
    alignment with the downstream task (Li et al., [2022a](#bib.bib45)). This confirms
    that pre-training tasks perform better when they are consistent with the downstream
    tasks (Zhang et al., [2020](#bib.bib101)).
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Code Search Model Training/Fine-tuning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The pre-training tasks for training large models for code understanding are
    described above. Pre-training helps the parameters of the model reach a better
    state, which benefits the training of downstream tasks. This process is commonly
    referred to as fine-tuning. It is important to note that pre-training goals may
    not align with the target task, but training/fine-tuning should be focused on
    the target task and evaluated using the target task’s evaluation method. Models
    that do not require pre-training, like Graph Neural Networks, also require design
    goals for optimization. For simplicity and consistency with models without pre-training,
    we refer to fine-tuning as training below.
  prefs: []
  type: TYPE_NORMAL
- en: During training, the model is guided by task-specific labels, known as supervision
    information. For code search, supervision information consists of paired (query,
    code) samples. Based on different training scenarios, we categorize existing code
    search training into discriminative model training and generative model training.
    Table [6](#S5.T6 "Table 6 ‣ 5.2.1\. Discriminative Model Training ‣ 5.2\. Code
    Search Model Training/Fine-tuning ‣ 5\. Training Method (RQ3) ‣ Survey of Code
    Search Based on Deep Learning") summarizes the approaches along these two dimensions.
    The discriminative model is the most commonly used in code search and models the
    similarity $s_{qc}=f_{\theta}(q,c)$. The generative model in code search mainly
    involves a Variational Auto-Encoder, encoding the input query/code into a distribution,
    and then decoding the distribution back to the original query/code. By designing
    a suitable training strategy for the encoder-decoder framework, the trained Variational
    Auto-Encoder model has a good generalization ability for its distribution in the
    latent space, and similarity can be calculated based on the matching of the mean
    vector in the latent space distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.1\. Discriminative Model Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'There are three types: point-level, pair-level, and sequence-level, which will
    be introduced separately.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Point-wise (Huang et al., [2021](#bib.bib34); Arakelyan et al., [2022](#bib.bib3)).
    In this task, paired (query, code) sample pairs are labeled as 1, while unpaired
    (query, code) sample pairs are labeled as 0. Therefore, the training data can
    be viewed as a set of (query, code, label) triplets, with labels being either
    1 or 0. This task is a binary classification problem, and the model can be optimized
    using a binary classification objective such as Mean Squared Error loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (20) |  | ${Loss}_{MSE}=\frac{1}{&#124;D&#124;}\sum_{(q,c)\in D}\left&#124;\hat{y}_{qc}-y_{qc}\right&#124;^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $y_{qc}$ represents the true label of the sample pair $(q,c)$, $\hat{y}_{qc}=\sigma\left(f_{\theta}(q,c)\right)$
    represents the similarity score output by the model, $\sigma$ is the sigmoid function
    to ensure that the value of $\hat{y}_{qc}$ is between 0 and 1, and $D$ represents
    the sample set. During optimization, to maintain a balance between positive and
    negative samples, it is necessary to sample an equal number of negative samples.
  prefs: []
  type: TYPE_NORMAL
- en: Table 6. Overview of code search model training/fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Work | Discriminative Model Training | Generative Model Training |
    Other |'
  prefs: []
  type: TYPE_TB
- en: '| Point-wise | Pair-wise | List-wise |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | CODEnn (Gu et al., [2018](#bib.bib24)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Chen and Zhou (Chen and Zhou, [2018](#bib.bib12)) |  |  |  | ✓ |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | MMAN (Wan et al., [2019](#bib.bib79)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CoaCor (Yao et al., [2019](#bib.bib95)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | AdaCS (Ling et al., [2020](#bib.bib49)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | MP-CAT (Haldar et al., [2020](#bib.bib27)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | ${TranS}^{3}$ (Wang et al., [2020](#bib.bib86)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Zhao and Sun (Zhao and Sun, [2020](#bib.bib102)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CO3 (Ye et al., [2020](#bib.bib97)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | DGMS (Ling et al., [2021](#bib.bib50)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CoCLR (Huang et al., [2021](#bib.bib34)) | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | TabCS (Xu et al., [2021](#bib.bib92)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CRaDLe (Gu et al., [2021b](#bib.bib22)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Corder (Bui et al., [2021](#bib.bib5)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | TranCS (Sun et al., [2022a](#bib.bib76)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CSRS (Cheng and Kuang, [2022](#bib.bib13)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | G2SC (Shi et al., [2022](#bib.bib71)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Li et al. (Li et al., [2022d](#bib.bib44)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CodeRetriever (Li et al., [2022a](#bib.bib45)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CDCS (Chai et al., [2022](#bib.bib9)) |  |  |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | ${NS}^{3}$ (Arakelyan et al., [2022](#bib.bib3)) | ✓ |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Wang et al. (Wang et al., [2022a](#bib.bib84)) |  |  |  |  | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | deGraphCS (Zeng et al., [2023](#bib.bib99)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | KeyDAC (Park et al., [2023](#bib.bib64)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | CSSAM (Cai et al., [2023](#bib.bib6)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | MulCS (Ma et al., [2023](#bib.bib57)) |  | ✓ |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | GraphSearchNet (Liu et al., [2023](#bib.bib53)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | TOSS (Hu et al., [2023](#bib.bib32)) |  |  | ✓ |  |  |'
  prefs: []
  type: TYPE_TB
- en: 'Pair-wise (Gu et al., [2018](#bib.bib24); Yao et al., [2019](#bib.bib95); Wan
    et al., [2019](#bib.bib79); Haldar et al., [2020](#bib.bib27); Ling et al., [2020](#bib.bib49);
    Zhao and Sun, [2020](#bib.bib102); Ling et al., [2021](#bib.bib50); Xu et al.,
    [2021](#bib.bib92); Bui et al., [2021](#bib.bib5); Gu et al., [2021b](#bib.bib22);
    Sun et al., [2022a](#bib.bib76); Cai et al., [2023](#bib.bib6); Ma et al., [2023](#bib.bib57);
    Zeng et al., [2023](#bib.bib99)). For the paired (query, code) positive sample
    pair, the negative sample code can be randomly selected to construct a triplet
    $\left(q,c^{+},c^{-}\right)$ called (query, positive sample code, negative sample
    code). The objective of training is to maximize the gap between positive sample
    $\left(q,c^{+}\right)$ and negative sample $\left(q,c^{-}\right)$. The loss function
    commonly used for this purpose is the Hinge loss, which can be represented as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (21) |  | $Loss_{hinge}=\sum_{\left(q,c^{+},c^{-}\right)\in D}\max\left(0,\epsilon-s_{{qc}^{+}}+s_{{qc}^{-}}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $s_{qc}=f_{\theta}(q,c)$ represents the similarity between query $q$ and
    code $c$, $\epsilon$ is a hyperparameter, and $D$ represents the sample set. For
    paired $(q,c)$ samples, negative samples $c^{-}$ are randomly selected to construct
    triplet sample $\left(q,c^{+},c^{-}\right)$.
  prefs: []
  type: TYPE_NORMAL
- en: 'List-wise (Ye et al., [2020](#bib.bib97); Wang et al., [2020](#bib.bib86);
    Cheng and Kuang, [2022](#bib.bib13); Li et al., [2022a](#bib.bib45), [d](#bib.bib44);
    Liu et al., [2023](#bib.bib53); Hu et al., [2023](#bib.bib32)). For paired (query,
    code) positive sample pairs, several negative sample codes can be randomly selected
    to construct the sequence $\left(q,c^{+},c_{1}^{-},\cdots,c_{n-1}^{-}\right)$.
    The training goal is to optimize the similarity ranking of positive sample $c^{+}$
    in the entire sequence $\left(c^{+},c_{1}^{-},\cdots,c_{n-1}^{-}\right)$ for the
    query $q$. The InfoNCE loss function can be utilized as the related loss function,
    viewing the problem as an $n$ classification task with the number of categories
    equal to the number of positive sample categories. After passing the similarity
    through softmax, the cross-entropy is used to construct the loss. The loss function
    can be expressed as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (22) |  | ${Loss}_{InfoNCE}=-\sum_{\left(q,c^{+},c_{1}^{-},\cdots,c_{n-1}^{-}\right)\in
    D}\log\left(\frac{\exp\left(s_{qc^{+}}/\tau\right)}{\exp\left(s_{qc^{+}}/\tau\right)+\sum_{j}\exp\left(s_{qc_{j}^{-}}/\tau\right)}\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tau$ is the temperature hyperparameter. In practice, the batch negative
    sampling strategy is commonly used to generate negative samples, meaning that
    for a batch of positive sample pairs, other codes within the batch are considered
    as negative samples. For instance, (Li et al., [2022a](#bib.bib45)) employs various
    strategies to sample negative samples to improve model performance, while (Li
    et al., [2022d](#bib.bib44)) improves the representation space data and generates
    more positive and negative samples.
  prefs: []
  type: TYPE_NORMAL
- en: The above three optimization objectives are all designed for discriminative
    models in supervised learning. Among them, the sequence-level optimization objective
    is currently the most popular due to its effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.2\. Generative Model Training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The Variational Auto-Encoder (VAE) map the input $\vec{x}$ to a distribution
    in the hidden space through the function $f_{\theta}(\cdot)$ (assumed to be a
    Gaussian distribution, which can be represented by a mean and variance vector),
    and then the vector obtained from sampling the distribution is then mapped to
    the input space by the function $g_{\phi}(\cdot)$, hoping to reconstruct $\vec{x}$.
    A regularization term is added to make the latent space distribution closer to
    a standard Gaussian distribution. The loss function of VAE is:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | ${Loss}_{VAE}=\mathcal{R}(\vec{x};\theta,\phi)+KL\left(q_{\theta}(\vec{z}\mid\vec{x}),\mathcal{N}(0,1)\right),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{R}(\vec{x};\theta,\phi)$ denotes the error loss for reconstructing
    $\vec{x}$ through the encoder-decoder framework, and $KL\left(q_{\theta}(\vec{z}\mid\vec{x}),\mathcal{N}(0,1)\right)$
    denotes the KL divergence of $q_{\theta}(\vec{z}\mid\vec{x})$ over $\mathcal{N}(0,1)$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4825156cab89d7d554fb76a4097e2b4f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7. Bimodal Variational Auto-Encoder.
  prefs: []
  type: TYPE_NORMAL
- en: 'The bimodal VAE, unlike general VAE, addresses the two modalities in code search
    and focuses on learning the matching relationships. The bimodal VAE has two improvements
    over the general VAE to make it more suitable for code search tasks (Chen and
    Zhou, [2018](#bib.bib12)), as shown in Figure [7](#S5.F7 "Figure 7 ‣ 5.2.2\. Generative
    Model Training ‣ 5.2\. Code Search Model Training/Fine-tuning ‣ 5\. Training Method
    (RQ3) ‣ Survey of Code Search Based on Deep Learning"). (a) Bimodal encoder-decoder
    framework: this framework inputs both the query and the code into the respective
    encoder-decoder models (the parameters of each model can be the same), and they
    operate in parallel. (b) Cross-modal regularization term: the regularization involves
    minimizing the difference in distribution between two modes of a sample pair instead
    of using the standard normal distribution regularization term. This unifies the
    query and code representations in the latent space, making the subsequent similarity
    matching more rational. The regularization term can be the KL divergence between
    the two modes or the KL divergence between one mode and the mean of both modes.
    Here, we use the KL divergence from query to code, expressed as ${KL}\left(q_{\theta}^{(1)}\left(\vec{z}_{q}\mid\vec{x}_{q}\right),q_{\theta}^{(2)}\left(\vec{z}_{\boldsymbol{c}}\mid\vec{x}_{\boldsymbol{c}}\right)\right)$,
    where $q_{\theta}^{(1)}\left(\vec{z}_{q}\mid\vec{x}_{q}\right)$ and $q_{\theta}^{(2)}\left(\vec{z}_{\boldsymbol{c}}\mid\vec{x}_{\boldsymbol{c}}\right)$
    represent the normal distribution consisting of the mean and variance of the query
    and code. The KL divergence from code to query can be obtained by reversing the
    above formula. The average distribution of queries and codes can be obtained by
    averaging their mean and variance, and the mean can also be used as a regularization
    term in the KL divergence.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2.3\. Other Training Methods
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides the pre-training and training techniques introduced above, there are
    also works exploring model training techniques from other perspectives. For example,
    meta-learning in (Chai et al., [2022](#bib.bib9)) uses a small verification set
    to improve model initialization for better performance with small data. (Wang
    et al., [2022a](#bib.bib84)) proposes data enhancement that preserves semantics
    and uses curriculum learning to control sample weight for better model convergence.
    In zero-shot settings, with no label training process, pre-training is crucial
    for developing a representation ability. Hence, the input is transformed into
    the representation space, where the resulting vectors carry semantic information.
    Decisions are made based on the similarity evaluation within this space (Guo et al.,
    [2022](#bib.bib25)).
  prefs: []
  type: TYPE_NORMAL
- en: '<svg   height="122.73" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,122.73) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="95.17" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Summary
    of answers to RQ3: • Over the past six years, the Masked Language Model (MLM)
    has emerged as the prevailing choice for pre-training tasks in code understanding.
    Despite its apparent simplicity, this task has consistently demonstrated remarkable
    effectiveness and serves as a fundamental cornerstone in achieving comprehensive
    code comprehension. • The training/fine-tuning of code search models predominantly
    adopt a discriminative model training approach, with pair-wise training being
    the most prevalent form.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Datasets and Evaluation (RQ4)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Datasets have had a significant impact on the evolution of code search technology.
    They not only serve as a mean to assess the performance of various models, but
    also play a crucial role in addressing real-world challenges faced in code search,
    thus driving the growth of this field. In this section, we present an overview
    of 12 code search datasets that have been proposed since 2018 and 8 commonly used
    metrics for evaluating code search models. To provide some guidelines for industrial
    practitioners, we further discuss how to choose proper code search approaches
    to fit their needs.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Code Search Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To drive the advancement of code search, various datasets comprising of (text,
    code) pairs have been introduced. This section summarizes the existing datasets
    on natural language code search research, including release year, data sources,
    programming languages, data statistics, dataset division, data annotation, and
    acquisition methods, as displayed in Table [7](#S6.T7 "Table 7 ‣ 6.1\. Code Search
    Datasets ‣ 6\. Datasets and Evaluation (RQ4) ‣ Survey of Code Search Based on
    Deep Learning"). In the subsequent section, we give a concise overview of some
    of the classic corpora outlined in Table [7](#S6.T7 "Table 7 ‣ 6.1\. Code Search
    Datasets ‣ 6\. Datasets and Evaluation (RQ4) ‣ Survey of Code Search Based on
    Deep Learning"). Furthermore, We conduct meticulous statistical analysis on data
    sources and programming languages. As depicted in Figure [8](#S6.F8 "Figure 8
    ‣ 6.1\. Code Search Datasets ‣ 6\. Datasets and Evaluation (RQ4) ‣ Survey of Code
    Search Based on Deep Learning"), GitHub emerges as the primary source of code
    search task datasets, with Python and Java being the predominant languages of
    interest for this task.
  prefs: []
  type: TYPE_NORMAL
- en: Table 7. Overview of existing datasets on code search.
  prefs: []
  type: TYPE_NORMAL
- en: '| Release year | Corpus | Data sources | Programming languages | Data statistics
    | Data splits | Data annotation | Acquisition methods |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | StaQC (Yao et al., [2018](#bib.bib96)) | SO | Python, SQL | 147,546
    Python (question,code) pairs; 119,519 SQL (question,code) pairs | training set,
    dev set, test set | manual, automatic | https://github.com/ LittleYUYU/StackOverflow-
    Question-Code-Dataset |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | CoNaLa (Yin et al., [2018](#bib.bib98)) | SO | Python, Java | 42 Python
    questions, 736 code blocks; 100 Java questions, 434 code blocks | training set,
    dev set, test set | manual, automatic | https://conala-corpus.github.io/ |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Gu et al. (Gu et al., [2018](#bib.bib24)) | GitHub | Java | 18,233,872
    Java methods with docstring | training set | automatic | https://github.com/ guxd/deep-code-search
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Java-50 (Gu et al., [2018](#bib.bib24)) | SO, GitHub | Java | 9,950
    Java projects; 16,262,602 Java methods; 50 queries | codebase, evaluation set
    | manual | https://github.com/ guxd/deep-code-search |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | FB-Java (Li et al., [2019](#bib.bib43)) | SO, GitHub | Java | 24,549
    repositories; 4,716,814 methods; 287 (question,answer) pairs | codebase, evaluation
    set | manual | https://github.com/ facebookresearch/Neural-Code -Search-Evaluation-Dataset
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CSN (Husain et al., [2019](#bib.bib36)) | GitHub | Python, Java, Ruby,
    Go, PHP, JavaScript | 2,326,976 (documentation,function) pairs; 4,125,470 functions
    without documentation | training set, dev set, test set, codebase | automatic
    | https://github.com /github/CodeSearchNet |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CSN-99 (Husain et al., [2019](#bib.bib36)) | Bing, GitHub | Python,
    Java, Ruby, Go, PHP, JavaScript | 99 queries; 4,026 (query,code) pairs | evaluation
    set | manual | https://github.com /github/CodeSearchNet |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | SO-DS (Heyman and Cutsem, [2020](#bib.bib29)) | SO, GitHub | Python
    | 1,113 queries; 12,137 code snippets | training set, dev set, test set | automatic
    | https://github.com/ nokia/codesearch |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CosBench (Yan et al., [2020](#bib.bib93)) | SO, GitHub | Java | 1,000
    Java projects; 475,783 Java files; 4,199,769 code snippets; 52 queries | codebase,
    evaluation set | manual | https://github.com/ BASE-LAB-SJTU/CosBench |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | WebQueryTest (Lu et al., [2021](#bib.bib55)) | Bing, GitHub | Python
    | 1,046 (web query, code) pairs | test set | manual | https://github.com/microsoft/
    CodeXGLUE/tree/main/Text-Code/ NL-code-search-WebQuery |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | AdvTest (Lu et al., [2021](#bib.bib55)) | GitHub | Python | 280,634
    (documentation, function) pairs | training set, dev set, test set | automatic
    | https://github.com/microsoft/ CodeXGLUE/tree/main/Text-Code/ NL-code-search-Adv
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CoSQA (Huang et al., [2021](#bib.bib34)) | Bing, GitHub | Python |
    20,604 (web query, code) pairs | training set, dev set, test set | manual | https://github.com/
    Jun-jie-Huang/CoCLR |'
  prefs: []
  type: TYPE_TB
- en: StaQC (Stack Overflow Question-Code pairs) (Yao et al., [2018](#bib.bib96))
    is a dataset of (question,code) pairs that has been automatically extracted from
    Stack Overflow (SO), which makes it ideal for predicting whether a code snippet
    can answer a particular question. Stack Overflow is a well-known website where
    developers can ask programming-related questions, such as “how to read a file
    in Python”. There are various user-generated solutions available on the site,
    and the “accepted” label is used to indicate the quality of these solutions. To
    construct the StaQC dataset, Yao et al. (Yao et al., [2018](#bib.bib96)) filtered
    posts in the Python and SQL domains on Stack Overflow using tags, and used a binary
    classifier to select posts that had ”how-to” type questions. They then combined
    manual labeling and automatic extraction methods to select questions and independent,
    “accepted” solutions from these posts to create (question, code) pairs. As a result,
    they obtained 147,546 Python (question, code) pairs and 119,519 SQL (question,
    code) pairs.
  prefs: []
  type: TYPE_NORMAL
- en: However, collecting questions from Stack Overflow can be a tedious and time-consuming
    process, which limits the quality and quantity of the collected corpus and may
    pose challenges for systematic comparison of various models. With the exponential
    growth of open-source software projects on GitHub, it has become the main source
    for obtaining code corpus.
  prefs: []
  type: TYPE_NORMAL
- en: CSN (CodeSearchNet) (Husain et al., [2019](#bib.bib36)) is a code search dataset
    that has been constructed using the open-source projects on GitHub. This corpus
    encompasses 6 different programming languages, including Python, Java, Go, PHP,
    JavaScript, and Ruby. In order to minimize the manual labeling effort, Husain
    et al. (Husain et al., [2019](#bib.bib36)) replaced natural language queries with
    documentations, forming pairs of (documentation, function) along with code snippets.
    To ensure the corpus’s quality, multiple filtering rules were established. This
    involved eliminating (documentation, function) pairs with fewer than 3 documentation
    tokens, functions containing less than 3 lines of code, functions with “test”
    in their names, and duplicated functions. As a result, the CSN corpus comprises
    a total of 6 million samples, including 2 million (documentation, function) pairs
    and 4 million functions without paired documents. The arrival of the CSN corpus
    presents exciting opportunities for training large models of code intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: AdvTest (Lu et al., [2021](#bib.bib55)) is designed for evaluating the generalization
    capability of code search models. It is constructed using the CodeSearchNet’s
    Python corpus. To guard against over-fitting, Lu et al. (Lu et al., [2021](#bib.bib55))
    replaced the function and variable names in the test set with special tokens (e.g.,
    “Func” in place of a function name). This helps prevent the model from over-relying
    on keyword matching during training and decrease the rate of keyword coincidence
    between query and code at the token level.
  prefs: []
  type: TYPE_NORMAL
- en: CoSQA (Huang et al., [2021](#bib.bib34)) is a unique code search dataset that
    is more representative of real-world code search scenarios compared to other datasets.
    Unlike other datasets that utilize documents, docstrings, or comments as queries,
    CoSQA is based on real user queries collected from Microsoft’s Bing search engine
    logs. To construct the CoSQA dataset, Huang et al. (Huang et al., [2021](#bib.bib34))
    filtered queries that did not contain the keyword “Python” or showed no code search
    intent. To build (query, code) pairs, Huang et al. (Huang et al., [2021](#bib.bib34))
    utilized CodeBERT to pre-select high-confidence functions from the CodeSearchNet
    Python corpus for each query. Subsequently, the annotators were tasked with determining
    whether the query was a match for the selected function. Finally, they obtained
    20,604 (web query, code) pairs that could assist the model in learning the semantic
    relationship between queries and codes in real-world scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8. Manually labeled datasets.
  prefs: []
  type: TYPE_NORMAL
- en: '| Release year | Corpus | number of languages | number of participants | type
    of participant | type of query | type of codebase |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | StaQC (Yao et al., [2018](#bib.bib96)) | 2 | 4 | undergraduate student
    | question in SO post | answer in SO post |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | CoNaLa (Yin et al., [2018](#bib.bib98)) | 2 | 5 | researcher and programmer
    | question in SO post | answer in SO post |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Java-50 (Gu et al., [2018](#bib.bib24)) | 1 | 2 | programmer | question
    in SO post | code in GitHub |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | FB-Java (Li et al., [2019](#bib.bib43)) | 1 | unknown | unknown |
    question in SO post | answer in SO post |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CSN-99 (Husain et al., [2019](#bib.bib36)) | 6 | unknown | programmer
    | query from Bing | code in GitHub |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CosBench (Yan et al., [2020](#bib.bib93)) | 1 | unknown | unknown
    | question in SO post | answer in SO post and code in GitHub |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | WebQueryTest (Lu et al., [2021](#bib.bib55)) | 1 | 13 | programmer
    | query from Bing | code in GitHub |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CoSQA (Huang et al., [2021](#bib.bib34)) | 1 | more than 100 | programmer
    | query from Bing | code in GitHub |'
  prefs: []
  type: TYPE_TB
- en: We present a detailed analysis of the manually annotated datasets in Table [8](#S6.T8
    "Table 8 ‣ 6.1\. Code Search Datasets ‣ 6\. Datasets and Evaluation (RQ4) ‣ Survey
    of Code Search Based on Deep Learning"). Our findings reveal that within these
    datasets, the queries predominantly originate from questions in Stack Overflow
    (SO) posts, closely followed by real queries entered by users in the Bing search
    engine. In comparison to source code comments, both of these queries bear a closer
    resemblance to queries encountered in real search scenarios. Notably, the primary
    source of code in the codebase is the open-source code repository, Github. Moreover,
    code embodies knowledge within the professional realm, thereby necessitating the
    involvement of programmers in the annotation process. This factor significantly
    escalates the costs associated with code annotation.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/267430d6a636f3a784867850fb135812.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Data source
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a08177de53cf047e2867da379283b5c4.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Programming language
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8. Data source and programming language of the datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the analysis above, there are three main challenges in the current
    code search datasets: (1) Inconsistency between training data and real user queries.
    The existing datasets primarily consist of (question, code) pairs, (document,
    code) pairs, and (comment, code) pairs. However, there is a discrepancy between
    the query text input by a user in the search engine and the text found in a question
    on Stack Overflow or a document/comment in the source code, leading to a poor
    performance of the trained model in real-world scenarios. (2) Scarcity of high-quality
    labeled data. Due to the high cost of code labeling, the existing datasets are
    mainly manually labeled in the evaluation set, and there is a shortage of a large
    number of manually labeled training sets, restricting the training of supervised
    learning models. (3) Limited data volume. The number of training data in existing
    datasets is limited and currently only reaches a few million, which is insufficient
    for training large-scale code understanding pre-training models. Although Markovtsev
    and Long (Markovtsev and Long, [2018](#bib.bib58)) selected 182,014 repositories
    from Github to create the first large-scale public code dataset for large-scale
    programming analysis, it is currently not accessible. In the future, obtaining
    a large-scale code corpus from Google BigQuery, which collects active data on
    GitHub including complete snapshots of over one million open source repositories
    and hundreds of millions of code submissions, may be explored.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Evaluation Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Code search evaluation datasets typically include carefully selected queries
    and annotated functions. The purpose of evaluating a code search model is to assess
    its ability to accurately retrieve relevant code snippets from the codebase in
    response to a given natural language query. The evaluation of code search models
    is largely conducted through automatic methods. Common metrics used in recent
    studies for evaluating code search models include Precision, Recall, F1-score,
    MAP, MRR, Frank, and SuccessRate.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b2baf4b95c95b4bb22170c733c3ba6de.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9. Schematic diagram of $top@k$ and $hit@k$.
  prefs: []
  type: TYPE_NORMAL
- en: Assume that a given set of queries to be executed, denoted as $Q=\left[q_{1},q_{2},\ldots,q_{n}\right]$,
    and each query is marked with its corresponding set of ground-truth answers $G$.
    As depicted in Figure [9](#S6.F9 "Figure 9 ‣ 6.2\. Evaluation Metrics ‣ 6\. Datasets
    and Evaluation (RQ4) ‣ Survey of Code Search Based on Deep Learning"), $top@k$
    represents the top-$k$ result sets returned for a particular query, and $hit@k$
    refers to the answer set among the top-$k$ results that correctly belongs to the
    ground-truth answer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Precision@k is a metric that shows the relationship between the results of
    a query and the ground-truth set. It measures, on average, how many of the top
    $k$ results returned by a query belong to the ground-truth set for a query set
    $Q$. The higher the value of $Precision@k$, the stronger the correlation between
    the returned results and the query. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (24) |  | $Precision@k=\frac{1}{&#124;Q&#124;}\sum_{i=1}^{&#124;Q&#124;}\frac{\mid
    Hit@k\left(q_{i}\right)\mid}{k}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'Recall@k indicates the average percentage of the ground-truth answer set for
    each query that is hit. The higher the $Recall@k$ value, the more ground-truth
    answers are retrieved. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (25) |  | $Recall@k=\frac{1}{&#124;Q&#124;}\sum_{i=1}^{&#124;Q&#124;}\frac{\mid
    Hit@k\left(q_{i}\right)\mid}{\left&#124;G_{i}\right&#124;}.$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'F1-Score is employed to evaluate the performance of a model when both precision
    and recall carry equal weight. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (26) |  | $F1-Score=\frac{2\cdot Precision\cdot Recall}{Precision+Recall}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'MAP@k, which stands for Mean Average Precision, reflects the average precision
    of the rankings produced by all queries. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (27) |  | $MAP@k=\frac{1}{&#124;Q&#124;}\sum_{i=1}^{&#124;Q&#124;}\frac{1}{m}\sum_{j=1}^{m}\frac{j}{{rank}\left({hit}_{j},{Top@k}\left(q_{i}\right)\right)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $m$ is $|Hit@k|$, $hit_{j}$ is an element in $Hit@k$, and ${rank}(e,l)$
    is the rank (i.e., index) of element $e$ in list $l$. When $\left|Hit@k\left(q_{i}\right)\right|=0$,
    $hit_{j}$ does not exist, and the average precision of query $q$ is 0. It is evident
    that a larger $MAP@k$ value signifies that a greater number of answers that hit
    the ground-truth are present in the top-$k$ results returned.
  prefs: []
  type: TYPE_NORMAL
- en: 'MRR@k, which stands for Mean Reciprocal Rank, indicates the average of the
    reciprocals of the rankings of all search results. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (28) |  | $MRR@k=\frac{1}{&#124;Q&#124;}\sum_{i=1}^{&#124;Q&#124;}\frac{1}{{rank}\left({hit}_{1},{Top@k}\left(q_{i}\right)\right)}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: When $\left|Hit@k\left(q_{i}\right)\right|=0$, the reciprocal of the ranking
    is 0. Typically, only the first answer that hits the ground-truth is taken into
    account when calculating $MRR@k$. The greater the $MRR@k$ value, the higher the
    ranking of the answer that hits the ground-truth in $Top@k$.
  prefs: []
  type: TYPE_NORMAL
- en: 'Frank@k represents the average rank of the first hit answer across all queries.
    It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (29) |  | $Frank@k=\frac{1}{&#124;Q&#124;}\sum_{i=1}^{&#124;Q&#124;}{rank}\left({hit}_{1}\left(q_{i}\right),{Top@k}\left(q_{i}\right)\right).$
    |  |'
  prefs: []
  type: TYPE_TB
- en: It is evident that a smaller $Frank@k$ value corresponds to a higher ranking
    of the ground-truth answer in the search results.
  prefs: []
  type: TYPE_NORMAL
- en: 'SuccessRate@k indicates the proportion of queries for which there are more
    than one ground-truth answers among the top-$k$ results returned. It is calculated
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (30) |  | $SuccessRate@k=\frac{1}{&#124;Q&#124;}\sum_{i=1}^{&#124;Q&#124;}\tau\left({Frank}_{q_{i}}\leq
    k\right),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\tau$ is an indicator function that returns 1 when the ranking of the
    hit answer for query $q_{i}$ is less than $k$ and 0 otherwise. $SuccessRate@k$
    is a crucial evaluation metric because an effective code search engine should
    enable software developers to find the desired code snippets by examining fewer
    results.
  prefs: []
  type: TYPE_NORMAL
- en: 'NDCG, which stands for Normalized Discounted Cumulative Gain, serves as a crucial
    metric to quantify the similarity between the ranking of candidate code fragments
    and the ideal ranking, placing significant emphasis on the overall ranking order
    among all candidate results. It is calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (31) |  | $NDCG=\frac{1}{&#124;Q&#124;}\sum_{i=1}^{k}\frac{2^{r_{i}}-1}{\log_{2}(i+1)},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $r_{i}$ is the relevance score of the top-$k$ results at position $i$.
  prefs: []
  type: TYPE_NORMAL
- en: Table [9](#S6.T9 "Table 9 ‣ 6.2\. Evaluation Metrics ‣ 6\. Datasets and Evaluation
    (RQ4) ‣ Survey of Code Search Based on Deep Learning") provides an in-depth analysis
    of 53 deep learning-based code search models, encompassing the evaluated datasets,
    the baseline model utilized for comparison, and the selected evaluation metrics.
    Figure [10](#S6.F10 "Figure 10 ‣ 6.2\. Evaluation Metrics ‣ 6\. Datasets and Evaluation
    (RQ4) ‣ Survey of Code Search Based on Deep Learning") highlights that the CSN
    dataset holds prominence in the code search task, while the most widely adopted
    evaluation metric is MRR@k, with a utilization rate of 90.6%.
  prefs: []
  type: TYPE_NORMAL
- en: Table 9. Deep code search models and their performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: '| Year | Models | Dataset | Baselines | Metric |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | CODEnn (Gu et al., [2018](#bib.bib24)) | Gu et al. (Gu et al., [2018](#bib.bib24))
    | CodeHow (Lv et al., [2015](#bib.bib56)) | Frank@k, Precision@k, MRR@k, SuccessRate@k
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | NCS (Sachdev et al., [2018](#bib.bib69)) | Sachdev et al. (Sachdev
    et al., [2018](#bib.bib69)) | TF-IDF, BM25 | Precision@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | NLP2API (Rahman and Roy, [2018](#bib.bib67)) | Rahman and Roy (Rahman
    and Roy, [2018](#bib.bib67)) | Rahman and Roy(Rahman and Roy, [2018](#bib.bib67))
    | Precision@k, MRR@K, MAP@K, Recall@K, Frank@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | COCABU (Sirres et al., [2018](#bib.bib73)) | Sirres et al. (Sirres
    et al., [2018](#bib.bib73)) | Codota, OpenHub | Precision@k, MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2018 | Zhang et al. (Zhang et al., [2018](#bib.bib100)) | Zhang et al. (Zhang
    et al., [2018](#bib.bib100)) | Dice, Rocchio, RSV | MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | UNIF (Cambronero et al., [2019](#bib.bib7)) | Gu et al. (Gu et al.,
    [2018](#bib.bib24)) | CODEnn (Gu et al., [2018](#bib.bib24)), NCS (Sachdev et al.,
    [2018](#bib.bib69)) | SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | MMAN (Wan et al., [2019](#bib.bib79)) | Wan et al. (Wan et al., [2019](#bib.bib79))
    | CodeHow (Lv et al., [2015](#bib.bib56)), CODEnn (Gu et al., [2018](#bib.bib24))
    | MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | RACK (Rahman et al., [2019](#bib.bib68)) | Rahman (Rahman et al.,
    [2019](#bib.bib68)) | NL Keywords (Rahman et al., [2019](#bib.bib68)) | Precision@k,
    MRR@K, MAP@K, Recall@K, NDCG, Frank@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Rahman (Rahman, [2019](#bib.bib66)) | Rahman (Rahman, [2019](#bib.bib66))
    | - | Hit@K, MAP@k, MRR@k, Recall@k, Frank@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | NQE (Liu et al., [2019a](#bib.bib52)) | Liu et al. (Liu et al., [2019a](#bib.bib52))
    | BM25, NCS (Sachdev et al., [2018](#bib.bib69)) | Precision@k, MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | QESC (Huang et al., [2019](#bib.bib35)) | Huang et al. (Huang et al.,
    [2019](#bib.bib35)) | CodeHow (Lv et al., [2015](#bib.bib56)), QECK (Nie et al.,
    [2016](#bib.bib61)) | Precision@k, NDCG |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | CoaCor (Yao et al., [2019](#bib.bib95)) | StaQC (Yao et al., [2018](#bib.bib96)),
    DEV and EVAL (Iyer et al., [2016](#bib.bib37)) | CODEnn (Gu et al., [2018](#bib.bib24)),
    CODE-NN (Iyer et al., [2016](#bib.bib37)) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2019 | Wu and Yang (Wu and Yang, [2019](#bib.bib91)) | Wu and Yang (Wu and
    Yang, [2019](#bib.bib91)) | CodeHow (Lv et al., [2015](#bib.bib56)) | Precision@k,
    NDCG |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | OCoR (Zhu et al., [2020](#bib.bib104)) | StaQC (Yao et al., [2018](#bib.bib96)),
    DEV and EVAL (Iyer et al., [2016](#bib.bib37)) | CODEnn (Gu et al., [2018](#bib.bib24)),
    CODE-NN (Iyer et al., [2016](#bib.bib37)), CoaCor (Yao et al., [2019](#bib.bib95))
    | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CodeBERT (Feng et al., [2020](#bib.bib18)) | CSN (Husain et al., [2019](#bib.bib36))
    | NBoW, 1D-CNN, biRNN, SelfAtt, RoBERTa(code) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | AdaCS (Ling et al., [2020](#bib.bib49)) | Ling et al. (Ling et al.,
    [2020](#bib.bib49)) | CodeHow (Lv et al., [2015](#bib.bib56)), CODEnn (Gu et al.,
    [2018](#bib.bib24)), BVAE (Chen and Zhou, [2018](#bib.bib12)) | Hit@K, MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | MP-CAT (Haldar et al., [2020](#bib.bib27)) | CoNaLa (Yin et al., [2018](#bib.bib98))
    | CT | Recall@k, MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | ${TranS}^{3}$ (Wang et al., [2020](#bib.bib86)) | Barone and Sennrich
    (Barone and Sennrich, [2017](#bib.bib4)) | CODEnn (Gu et al., [2018](#bib.bib24)),
    CoaCor (Yao et al., [2019](#bib.bib95)), Hybrid-DeepCom (Hu et al., [2020](#bib.bib33)),
    AutoSum (Wan et al., [2018](#bib.bib82)) | MRR@k, NDCG, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | Zhao and Sun (Zhao and Sun, [2020](#bib.bib102)) | StaQC (Yao et al.,
    [2018](#bib.bib96)) | CODEnn (Gu et al., [2018](#bib.bib24)), CoaCor (Yao et al.,
    [2019](#bib.bib95)) | MAP@k, NDCG |'
  prefs: []
  type: TYPE_TB
- en: '| 2020 | CO3 (Ye et al., [2020](#bib.bib97)) | StaQC (Yao et al., [2018](#bib.bib96))
    | CODEnn (Gu et al., [2018](#bib.bib24)), CoaCor (Yao et al., [2019](#bib.bib95))
    | MRR@k, NDCG |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | GraphCodeBERT (Guo et al., [2021](#bib.bib26)) | CSN (Husain et al.,
    [2019](#bib.bib36)) | NBoW, 1D-CNN, biRNN, SelfAtt, RoBERTa(code), CodeBERT (Feng
    et al., [2020](#bib.bib18)) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | DGMS (Ling et al., [2021](#bib.bib50)) | FB-Java (Li et al., [2019](#bib.bib43)),
    CSN-Python (Husain et al., [2019](#bib.bib36)) | NBoW, 1D-CNN, biRNN, SelfAtt,
    CODEnn (Gu et al., [2018](#bib.bib24)), UNIF (Cambronero et al., [2019](#bib.bib7)),
    CAT (Haldar et al., [2020](#bib.bib27)) | MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CoCLR (Huang et al., [2021](#bib.bib34)) | CoSQA (Huang et al., [2021](#bib.bib34)),
    WebQueryTest (Lu et al., [2021](#bib.bib55)) | RoBERTa (Liu et al., [2019b](#bib.bib54)),
    CodeBERT (Feng et al., [2020](#bib.bib18)) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | SEQUER (Cao et al., [2021](#bib.bib8)) | Cao et al. (Cao et al., [2021](#bib.bib8))
    | seq2seq (Sutskever et al., [2014](#bib.bib78)) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | DOBF (Lachaux et al., [2021](#bib.bib42)) | CSN-Python (Husain et al.,
    [2019](#bib.bib36)) | CodeBERT (Feng et al., [2020](#bib.bib18)), GraphCodeBERT
    (Guo et al., [2021](#bib.bib26)) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | CRaDLe (Gu et al., [2021b](#bib.bib22)) | CSN (Husain et al., [2019](#bib.bib36))
    | CODEnn (Gu et al., [2018](#bib.bib24)), UNIF (Cambronero et al., [2019](#bib.bib7)),
    NBoW, 1D-CNN, biRNN, SelfAtt, ConvSelfAtt | MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Corder (Bui et al., [2021](#bib.bib5)) | Gu et al. (Gu et al., [2018](#bib.bib24))
    | NBoW, biRNN, SelfAtt, TBCNN (Mou et al., [2016](#bib.bib60)), Code2vec (Alon
    et al., [2019](#bib.bib2)) | Precision@k, MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | Gu et al. (Gu et al., [2021a](#bib.bib21)) | CSN (Husain et al., [2019](#bib.bib36))
    | NBoW, 1D-CNN, biRNN, SelfAtt | MRR@k, NDCG |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | TabCS (Xu et al., [2021](#bib.bib92)) | Hu et al. (Hu et al., [2020](#bib.bib33)),
    CSN (Husain et al., [2019](#bib.bib36)) | CODEnn (Gu et al., [2018](#bib.bib24)),
    CARLCS-CNN (Shuai et al., [2020](#bib.bib72)), CARLCS-TS (Shuai et al., [2020](#bib.bib72)),
    UNIF (Cambronero et al., [2019](#bib.bib7)) | MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | MuCoS (Du et al., [2021](#bib.bib16)) | CSN (Husain et al., [2019](#bib.bib36))
    | NBoW, 1D-CNN, biRNN, SelfAtt, ConvSelfAtt, CODEnn (Gu et al., [2018](#bib.bib24)),
    CodeBERT (Feng et al., [2020](#bib.bib18)) | MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2021 | SynCoBERT (Wang et al., [2021b](#bib.bib87)) | CSN (Husain et al.,
    [2019](#bib.bib36)), AdvTest (Lu et al., [2021](#bib.bib55)) | NBow, CNN, BiRNN,
    SelfAttn, RoBERTa (Liu et al., [2019b](#bib.bib54)), RoBERTa(code), CodeBERT (Feng
    et al., [2020](#bib.bib18)), GraphCodeBERT (Guo et al., [2021](#bib.bib26)) |
    MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | TranCS (Sun et al., [2022a](#bib.bib76)) | CSN-Java (Husain et al.,
    [2019](#bib.bib36)) | CODEnn (Gu et al., [2018](#bib.bib24)), MMAN (Wan et al.,
    [2019](#bib.bib79)) | MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Wang et al. (Wang et al., [2022a](#bib.bib84)) | CSN (Husain et al.,
    [2019](#bib.bib36)) | NBoW, 1D-CNN, biRNN, SelfAtt, RoBERTa (Liu et al., [2019b](#bib.bib54)),
    RoBERTa(code), CodeBERT (Feng et al., [2020](#bib.bib18)), GraphCodeBERT (Guo
    et al., [2021](#bib.bib26)) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CodeRetriever (Li et al., [2022a](#bib.bib45)) | CSN (Husain et al.,
    [2019](#bib.bib36)), AdvTest (Lu et al., [2021](#bib.bib55)), CoSQA (Huang et al.,
    [2021](#bib.bib34)), CoNaLa (Yin et al., [2018](#bib.bib98)), SO-DS (Heyman and
    Cutsem, [2020](#bib.bib29)), StaQC (Yao et al., [2018](#bib.bib96)) | CodeBERT
    (Feng et al., [2020](#bib.bib18)), GraphCodeBERT (Guo et al., [2021](#bib.bib26)),
    SynCoBERT (Wang et al., [2021b](#bib.bib87)) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CDCS (Chai et al., [2022](#bib.bib9)) | CSN-Python (Husain et al.,
    [2019](#bib.bib36)), CSN-Java (Husain et al., [2019](#bib.bib36)), Solidity and
    SQL (Yang et al., [2021](#bib.bib94)) | Roberta (Liu et al., [2019b](#bib.bib54)),
    CodeBERT (Feng et al., [2020](#bib.bib18)) | MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CSRS (Cheng and Kuang, [2022](#bib.bib13)) | Gu et al. (Gu et al.,
    [2018](#bib.bib24)) | CODEnn (Gu et al., [2018](#bib.bib24)), CARLCS-CNN (Shuai
    et al., [2020](#bib.bib72)) | Recall@k, MRR@k, NDCG |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | ${NS}^{3}$ (Arakelyan et al., [2022](#bib.bib3)) | CSN (Husain et al.,
    [2019](#bib.bib36)), CoSQA (Huang et al., [2021](#bib.bib34)) | BM25, RoBERTa(code),
    CuBERT (Kanade et al., [2020](#bib.bib38)), CodeBERT (Feng et al., [2020](#bib.bib18)),
    GraphCodeBERT (Guo et al., [2021](#bib.bib26)) | MRR@k, Precision@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CSSAM (Cai et al., [2023](#bib.bib6)) | Hu et al. (Hu et al., [2020](#bib.bib33)),
    CSN (Husain et al., [2019](#bib.bib36)) | CodeHow (Lv et al., [2015](#bib.bib56)),
    CODEnn (Gu et al., [2018](#bib.bib24)), MP-CAT (Haldar et al., [2020](#bib.bib27)),
    TabCS (Xu et al., [2021](#bib.bib92)) | MRR@k, SuccessRate@k, NDCG |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CTBERT (Han et al., [2022](#bib.bib28)) | CSN (Husain et al., [2019](#bib.bib36)),
    AdvTest (Lu et al., [2021](#bib.bib55)) | CodeBERT (Feng et al., [2020](#bib.bib18)),
    GraphCodeBERT (Guo et al., [2021](#bib.bib26)), SynCoBERT (Wang et al., [2021b](#bib.bib87))
    | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | QueCos (Wang et al., [2022b](#bib.bib83)) | CSN-Python (Husain et al.,
    [2019](#bib.bib36)), CSN-Java (Husain et al., [2019](#bib.bib36)), Wang et al.
    (Wang et al., [2022b](#bib.bib83)) | CODEnn (Gu et al., [2018](#bib.bib24)), UNIF
    (Cambronero et al., [2019](#bib.bib7)), OCoR (Zhu et al., [2020](#bib.bib104))
    | MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | ZaCQ (Eberhart and McMillan, [2022](#bib.bib17)) | CSN (Husain et al.,
    [2019](#bib.bib36)) | V-DO, KW | MRR@k, MAP@k, NDCG |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | G2SC (Shi et al., [2022](#bib.bib71)) | CSN (Husain et al., [2019](#bib.bib36))
    | CODEnn (Gu et al., [2018](#bib.bib24)), MMAN (Wan et al., [2019](#bib.bib79)),
    CodeBERT (Feng et al., [2020](#bib.bib18)), GraphCodeBERT (Guo et al., [2021](#bib.bib26))
    | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | SPT-Code (Niu et al., [2022](#bib.bib62)) | CSN (Husain et al., [2019](#bib.bib36))
    | CNN, Bi-GRU, SelfAtt, CodeBERT (Feng et al., [2020](#bib.bib18)), GraphCodeBERT
    (Guo et al., [2021](#bib.bib26)) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | CODE-MVP (Wang et al., [2022c](#bib.bib88)) | AdvTest (Lu et al.,
    [2021](#bib.bib55)), CoNaLa (Yin et al., [2018](#bib.bib98)), CoSQA (Huang et al.,
    [2021](#bib.bib34)) | RoBERTa (Liu et al., [2019b](#bib.bib54)), CodeBERT (Feng
    et al., [2020](#bib.bib18)), GraphCodeBERT (Guo et al., [2021](#bib.bib26)), SynCoBERT
    (Wang et al., [2021b](#bib.bib87)) | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | UniXcoder (Guo et al., [2022](#bib.bib25)) | CSN (Husain et al., [2019](#bib.bib36)),
    AdvTest (Lu et al., [2021](#bib.bib55)), CoSQA (Huang et al., [2021](#bib.bib34))
    | RoBERTa (Liu et al., [2019b](#bib.bib54)), CodeBERT (Feng et al., [2020](#bib.bib18)),
    GraphCodeBERT (Guo et al., [2021](#bib.bib26)), SynCoBERT (Wang et al., [2021b](#bib.bib87))
    | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | Li et al. (Li et al., [2022d](#bib.bib44)) | CSN (Husain et al., [2019](#bib.bib36))
    | RoBERTa(code), CodeBERT (Feng et al., [2020](#bib.bib18)), GraphCodeBERT (Guo
    et al., [2021](#bib.bib26)), UniXCoder (Guo et al., [2022](#bib.bib25)) | MRR@k
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2022 | SCodeR (Li et al., [2022b](#bib.bib46)) | CSN (Husain et al., [2019](#bib.bib36)),
    AdvTest (Lu et al., [2021](#bib.bib55)), CoSQA (Huang et al., [2021](#bib.bib34))
    | CodeBERT (Feng et al., [2020](#bib.bib18)), GraphCodeBERT (Guo et al., [2021](#bib.bib26)),
    SyncoBERT (Wang et al., [2021b](#bib.bib87)), CodeRetriever (Li et al., [2022a](#bib.bib45)),
    Code-MVP (Wang et al., [2022c](#bib.bib88)), UniXcoder (Guo et al., [2022](#bib.bib25))
    | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | Salza et al. (Salza et al., [2023](#bib.bib70)) | Salza et al. (Salza
    et al., [2023](#bib.bib70)) | LUCENE, CODEnn (Gu et al., [2018](#bib.bib24)) |
    MRR@k, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | deGraphCS (Zeng et al., [2023](#bib.bib99)) | Zeng et al. (Zeng et al.,
    [2023](#bib.bib99)) | CODEnn (Gu et al., [2018](#bib.bib24)), UNIF (Cambronero
    et al., [2019](#bib.bib7)), MMAN (Wan et al., [2019](#bib.bib79)) | MRR@k, SuccessRate@k
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | GraphSearchNet (Liu et al., [2023](#bib.bib53)) | CSN-Python (Husain
    et al., [2019](#bib.bib36)), CSN-Java (Husain et al., [2019](#bib.bib36)) | NBoW,
    1D-CNN, biRNN, SelfAtt, UNIF (Cambronero et al., [2019](#bib.bib7)), CODEnn (Gu
    et al., [2018](#bib.bib24)), CARLCS-CNN (Shuai et al., [2020](#bib.bib72)), TabCS
    (Xu et al., [2021](#bib.bib92)), Coacor (Yao et al., [2019](#bib.bib95)) | MRR@k,
    NDCG, SuccessRate@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | MulCS (Ma et al., [2023](#bib.bib57)) | CSN (Husain et al., [2019](#bib.bib36)),
    C dataset (Ma et al., [2023](#bib.bib57)) | CODEnn (Gu et al., [2018](#bib.bib24)),
    TabCS (Xu et al., [2021](#bib.bib92)), NBoW, 1D-CNN, biRNN, SelfAtt | MRR@k, SuccessRate@k
    |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | KeyDAC (Park et al., [2023](#bib.bib64)) | CoSQA (Huang et al., [2021](#bib.bib34)),
    WebQueryTest (Lu et al., [2021](#bib.bib55)) | CoCLR (Huang et al., [2021](#bib.bib34))
    | MRR@k |'
  prefs: []
  type: TYPE_TB
- en: '| 2023 | TOSS (Hu et al., [2023](#bib.bib32)) | CSN (Husain et al., [2019](#bib.bib36))
    | Jaccard, BOW, TFIDF, BM25, CODEnn (Gu et al., [2018](#bib.bib24)), CodeBERT
    (Feng et al., [2020](#bib.bib18)), GraphCodeBERT (Guo et al., [2021](#bib.bib26)),
    CoCLR (Huang et al., [2021](#bib.bib34)) | MRR@k, Precision@k | ![Refer to caption](img/aaa4647cef89638c7d94677ebbd9eaae.png)'
  prefs: []
  type: TYPE_NORMAL
- en: (a) Datasets adopted by at least two papers.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/11b8b7869b620e3ef08c8251c9ce1efd.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Metrics adopted by at least two papers.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10. Datasets and metrics adopted by at least two papers.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Usage Scenarios
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The rapid evolution of code search technology has presented the industry with
    a multitude of fresh opportunities. An interesting and important question for
    industrial practitioners is how to choose proper code search approaches to fit
    their needs. This section discusses this question from three aspects: effectiveness,
    efficiency, and multi-language.'
  prefs: []
  type: TYPE_NORMAL
- en: Effectiveness. By placing a heightened emphasis on the accuracy of code search,
    industrial practitioners can leverage a suite of powerful techniques. Methods
    such as RACK (Rahman et al., [2019](#bib.bib68)), NQE (Liu et al., [2019a](#bib.bib52)),
    SEQUER (Cao et al., [2021](#bib.bib8)), and ZaCQ (Eberhart and McMillan, [2022](#bib.bib17))
    offer effective means to reconstruct queries, aiding in the clarification of user
    search intent. To deepen the understanding of code semantics, practitioners can
    employ methods such as MP-CAT (Haldar et al., [2020](#bib.bib27)), CRaDLe (Gu
    et al., [2021b](#bib.bib22)), GraphCodeBERT (Guo et al., [2021](#bib.bib26)),
    TabCS (Xu et al., [2021](#bib.bib92)), SynCoBERT (Wang et al., [2021b](#bib.bib87)),
    G2SC (Shi et al., [2022](#bib.bib71)), SPT-Code (Niu et al., [2022](#bib.bib62)),
    and UniXcoder (Guo et al., [2022](#bib.bib25)). Furthermore, methods such as MP-CAT
    (Haldar et al., [2020](#bib.bib27)), TabCS (Xu et al., [2021](#bib.bib92)), CSRS
    (Cheng and Kuang, [2022](#bib.bib13)), ${NS}^{3}$ (Arakelyan et al., [2022](#bib.bib3)),
    and CSSAM (Cai et al., [2023](#bib.bib6)) enable fine-grained interaction between
    query and code, effectively bridging the semantic gap that exists between them.
    Considering the pivotal role of the training corpus in determining the performance
    of neural code search (Sun et al., [2022b](#bib.bib77)), it is imperative for
    practitioners to prioritize the quality of their data. By meticulously cleaning
    the training corpus, practitioners can attain a high-quality dataset that fosters
    the establishment of a precise mapping from natural language to programming language.
    Furthermore, practitioners can leverage CodeRetriever (Li et al., [2022a](#bib.bib45))
    to specifically concentrate on examining the semantic distinctions between query
    and code via Contrastive Learning. This approach facilitates a detailed analysis
    of the nuanced differences, thereby enhancing the overall search accuracy. Additionally,
    the recall and rerank framework offered by TOSS (Hu et al., [2023](#bib.bib32))
    presents an invaluable opportunity to augment the accuracy of code search. By
    integrating this framework, practitioners can achieve further improvements in
    the accuracy of their products.
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency. By prioritizing search efficiency, industrial practitioners can
    leverage the offline calculation of code fragment embeddings within their codebases.
    This approach offers a time-saving alternative to the online calculation of vector
    similarity. Moreover, the utilization of CoSHC (Gu et al., [2022](#bib.bib23)),
    a code search acceleration method based on deep hashing and code classification,
    enables efficient code search while accepting a minor trade-off in precision.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-language. By emphasizing the generalization of code search tools across
    multiple programming languages, industrial practitioners can use MulCS (Ma et al.,
    [2023](#bib.bib57)) to break down the semantic barriers between different programming
    languages, which uses a unified data structure to represent multiple programming
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: '<svg   height="136.64" overflow="visible"
    version="1.1" width="600"><g transform="translate(0,136.64) matrix(1 0 0 -1 0
    0)" fill="#000000" stroke="#000000" stroke-width="0.4pt"><g fill-opacity="1.0"
    transform="matrix(1.0 0.0 0.0 1.0 21.65 13.78)"><foreignobject width="556.69"
    height="109.08" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible" color="#000000">Summary
    of answers to RQ4: • The primary source of the code in the code search datasets
    originates from the open-source code repository, Github. On the other hand, Stack
    Overflow (SO) serves as the main source of queries these the datasets. • Python
    and Java are the two most concerned programming languages for code search tasks.
    • For nearly 6 years, the CSN dataset has dominated the evaluation of code search
    tasks. • The prevalent evaluation metrics employed in code search encompass Precision@k,
    Recall@k, F1-score, MAP@k, MRR@k, Frank@k, and SuccessRate@k, with MRR@k widely
    utilized indicator among them.</foreignobject></g></g></svg>'
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Open Challenges and Future Directions (RQ5)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section addresses the outstanding challenges in the field of code search
    and identifies potential avenues for future advancements.
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Understanding code structure. The information contained in code can be divided
    into two parts: structured information determined by keywords that define the
    code’s function, and auxiliary information reflected by identifiers such as function
    and variable names, which indicate a software developer’s generalization of code
    functions. However, studies have shown that anonymizing function and variable
    names can lead to a significant drop in model performance (Guo et al., [2022](#bib.bib25)),
    indicating that current methods rely too heavily on identifier information for
    inference. Identifier content serves as a convenient shortcut for code intelligence
    tasks, but an overreliance on it may hinder the model’s ability to learn the code
    functions defined by structured information, thereby hindering its overall understanding
    of code structure. The challenge is to design a strategy that allows the model
    to better focus on and understand the code’s structural information, and to strike
    a balance between the structural information defined by keywords and the auxiliary
    information provided by identifiers during both training and inference. This is
    a direction worthy of further research. First steps toward that goal have been
    taken, e.g., by Li et al. (Li et al., [2022b](#bib.bib46)) and Wang et al. (Wang
    et al., [2022c](#bib.bib88)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Graph pre-training technology. Code can be analyzed and represented as a graph
    data structure (e.g., Data Flow Graph (Guo et al., [2021](#bib.bib26)) and Variable-based
    Flow Graph (Zeng et al., [2023](#bib.bib99))). Compared to sequential structures,
    using a graph structure to represent code provides a larger amount of information
    and higher accuracy. We believe that this graph structure encompasses valuable
    insights that can lead to breakthroughs in understanding the meaning of code.
    However, based on the results of existing research, methods utilizing Transformer-encoded
    sequences (Guo et al., [2022](#bib.bib25); Wang et al., [2021b](#bib.bib87); Li
    et al., [2022a](#bib.bib45); Wang et al., [2022c](#bib.bib88)) continue to maintain
    their lead over those based on graph neural network approaches for graph encoding
    (Zeng et al., [2023](#bib.bib99); Ling et al., [2021](#bib.bib50); Ma et al.,
    [2023](#bib.bib57); Liu et al., [2023](#bib.bib53)). The remarkable success of
    the Transformer-based methods can be largely attributed to the substantial improvement
    in model capacity achieved through pre-training techniques. We believe that the
    potential of graph-based methods in code intelligence has not been fully explored
    yet. A valuable research direction lies in exploring how to incorporate graph
    pre-training into code intelligence, aiming to fully unleash the performance of
    graph-based models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Robust optimization. Research has indicated that current code intelligence methods
    are susceptible to adversarial attacks (Li et al., [2022e](#bib.bib47)). Perturbing
    identifiers significantly decreases the model’s performance, highlighting its
    lack of robustness. Efforts have been made to enhance robustness through adversarial
    training (Wan et al., [2022a](#bib.bib80)), but further studies are required.
    At the same time, it is crucial to establish a standard evaluation method or dataset
    to evaluate the robustness of these models.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Interpretability research. Like other deep learning models, code search models
    are trained and operate in a relatively black-box manner. Interpretability research
    aims to understand the reasons behind a model’s decisions, improve our understanding
    of the model’s inference process, and identify the information that the model
    considers important. This research is crucial as it can boost user confidence
    in the model and, in case of errors, enable the identification of the cause and
    development of a solution in a timely manner. Karmakar and Robbes (Karmakar and
    Robbes, [2021](#bib.bib39)) and Wan et al. (Wan et al., [2022b](#bib.bib81)) propose
    promising first steps into this direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Comprehensive evaluation. Presently, the most efficient approach in the field
    of code search is the pre-training & fine-tuning paradigm (e.g., CodeBERT (Feng
    et al., [2020](#bib.bib18)), GraphCodeBERT (Guo et al., [2021](#bib.bib26)), Syncobert
    (Wang et al., [2021b](#bib.bib87)), and UniXcoder (Guo et al., [2022](#bib.bib25))).
    This paradigm encompasses several crucial design elements, including data preprocessing
    strategy, tokenizer, pre-training task, pre-training model & training parameters,
    fine-tuning model & training parameters, and negative sample construction strategy.
    However, there is a shortage of analytical experiments on these key elements that
    can guide practitioners in choosing the most effective training strategies for
    code intelligence. For example, Guo et al. (Guo et al., [2021](#bib.bib26)) adopt
    the parameters of CodeBERT (Feng et al., [2020](#bib.bib18)) to initialize the
    GraphCodeBERT model and proceed with continue pre-training on the CSN dataset
    using tasks like MLM. However, the performance improvement has not been validated
    to determine whether it is a result of additional iterations on the MLM task.
    Investigating the significance of these design elements will offer more direction
    to practitioners and also provide new insights for future research by uncovering
    the unique properties of code intelligence.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (6)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'High-quality datasets. At present, the most widely used datasets in code search
    come from three sources: crawling from Github (e.g., CSN (Husain et al., [2019](#bib.bib36))),
    which consists of code comments as queries and the rest as code; crawling from
    Stack Overflow (e.g., StaQC (Yao et al., [2018](#bib.bib96)) and CoNaLa (Yin et al.,
    [2018](#bib.bib98))), which consists of questions and code answers in posts; and
    actual user queries collected from search engines (e.g., CoSQA (Huang et al.,
    [2021](#bib.bib34))), with codes annotated by annotators . Each of these data
    sources has its own limitations: in the Github data, code comments are significantly
    different from actual queries; in the Stack Overflow data, the answered code is
    often not a complete function; and in the annotated data, a vast amount of background
    knowledge is required to understand the code, making it difficult to guarantee
    the scale and quality of the data. Therefore, we believe it is essential to create
    a more practical dataset for model training and evaluation. One potential solution
    is to collect online user behavior records, such as clicks and stays, which would
    require a highly performing code search engine with a large user base. Hence,
    there is a potential to use the existing model to build a preliminary code search
    engine, attract a group of users, and collect user data to create a new dataset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (7)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: More fine-grained interaction. Current methods of code search have limitations
    in their ability to model the interaction between query and code. These limitations
    stem from the fact that existing methods only model the interaction at the representation
    level (Cheng and Kuang, [2022](#bib.bib13); Xu et al., [2021](#bib.bib92)) and
    fail to consider cross-modal interaction at the token level. Additionally, these
    methods use a single level for discrimination, which limits the ability to capture
    hierarchical information. Hence, the model architecture used for code search still
    holds potential for improvement. First steps toward that goal have been taken,
    e.g., by Dong et al. (Dong et al., [2023](#bib.bib15)).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (8)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving code search efficiency. Deep learning-based code search methods have
    demonstrated promising results. However, previous approaches have predominantly
    focused on retrieval accuracy while neglecting the consideration of retrieval
    efficiency. If the code search model is intended for real-world online scenarios,
    enhancing retrieval efficiency becomes a challenge that demands immediate attention.
    Addressing this crucial concern is vital to successfully deploy and utilize the
    model in practical applications. Gu et al. (Gu et al., [2022](#bib.bib23)) propose
    promising first steps into this direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (9)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Context-related code search. The current method assumes that functions in a
    project are independent (Gu et al., [2018](#bib.bib24); Husain et al., [2019](#bib.bib36);
    Huang et al., [2021](#bib.bib34)), disregarding their relationships within the
    project. However, functions in a project are actually interdependent, with function
    calls and shared variables. To accurately model a function’s role, its context
    must be considered. Designing a method to model contextual information and efficiently
    search for functions with context information online is a valuable research direction.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (10)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Personalized code search. Software developers possess unique programming habits,
    leading to varying speeds of understanding and preferences for different forms
    of code for the same function. Consequently, code search results should be tailored
    to individual users based on their programming preferences. A potential area for
    research is the implementation of a personalized code search service.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (11)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Better code analysis tools. Different data structures like Abstract Syntax Trees,
    Control Flow Graphs, Program Dependency Graphs, and Data Flow Graphs can assist
    in comprehending the semantics of code. However, the absence of useful open source
    tools to extract these structures impedes the progress of code intelligence research.
    For instance, the extraction of program dependency graphs is currently limited
    to tools designed specifically for the Java programming language. This poses a
    hindrance to research in the field of code intelligence. Thus, developing better
    open-source code analysis tools for multiple programming languages would be a
    beneficial direction to further advance related research.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (12)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exploring new search paradigms. Recently, large-scale language models used for
    code generation have provided functionality akin to code search. For instance,
    GitHub’s Copilot tool ²²2https://github.com/features/copilot, backed by Codex
    (Chen et al., [2021](#bib.bib11)) at its core, maps natural language descriptions
    of desired functionalities to corresponding code snippets that provide those functionalities.
    Similarly, OpenAI’s ChatGPT tool ³³3https://openai.com/blog/chatgpt, powered by
    GPT-4 (OpenAI, [2023](#bib.bib63)), maps natural language descriptions of desired
    functionalities to code snippets that offer the desired functionalities, accompanied
    by explanatory natural language text to enhance developers’ understanding and
    usability. In light of the impact of these tools, it is imperative that we reconsider
    the significance and form of code search tasks. Exploring new paradigms may usher
    in a new era of transformation for code search. One promising direction worth
    exploring is cross-fertilization with other code intelligence tasks, such as leveraging
    code search results to assist code completion or code generation.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey, we have presented a comprehensive overview of deep learning-based
    methods for the code search task. We start by introducing the code search task
    and outlining the framework of deep learning-based code search method. Next, we
    detail the methods for extracting representations of query and code, respectively.
    Furthermore, we categorize many loss functions about the model training. Finally,
    we identify several open challenges and promising research directions in this
    area. We hope this survey can help both academic researchers and industry practitioners,
    and inspire more meaningful work in this field.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Alon et al. (2019) Uri Alon, Meital Zilberstein, Omer Levy, and Eran Yahav.
    2019. code2vec: learning distributed representations of code. *Proc. ACM Program.
    Lang.* 3, POPL (2019), 40:1–40:29. [https://doi.org/10.1145/3290353](https://doi.org/10.1145/3290353)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Arakelyan et al. (2022) Shushan Arakelyan, Anna Hakhverdyan, Miltiadis Allamanis,
    Luis Garcia, Christophe Hauser, and Xiang Ren. 2022. NS3: Neuro-symbolic Semantic
    Code Search. In *NeurIPS*. [http://papers.nips.cc/paper_files/paper/2022/hash/43f5f6c5cb333115914c8448b8506411-Abstract-Conference.html](http://papers.nips.cc/paper_files/paper/2022/hash/43f5f6c5cb333115914c8448b8506411-Abstract-Conference.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barone and Sennrich (2017) Antonio Valerio Miceli Barone and Rico Sennrich.
    2017. A Parallel Corpus of Python Functions and Documentation Strings for Automated
    Code Documentation and Code Generation. In *Proceedings of the Eighth International
    Joint Conference on Natural Language Processing, IJCNLP 2017, Taipei, Taiwan,
    November 27 - December 1, 2017, Volume 2: Short Papers*, Greg Kondrak and Taro
    Watanabe (Eds.). Asian Federation of Natural Language Processing, 314–319. [https://aclanthology.org/I17-2053/](https://aclanthology.org/I17-2053/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bui et al. (2021) Nghi D. Q. Bui, Yijun Yu, and Lingxiao Jiang. 2021. Self-Supervised
    Contrastive Learning for Code Retrieval and Summarization via Semantic-Preserving
    Transformations. In *SIGIR ’21: The 44th International ACM SIGIR Conference on
    Research and Development in Information Retrieval, Virtual Event, Canada, July
    11-15, 2021*, Fernando Diaz, Chirag Shah, Torsten Suel, Pablo Castells, Rosie
    Jones, and Tetsuya Sakai (Eds.). ACM, 511–521. [https://doi.org/10.1145/3404835.3462840](https://doi.org/10.1145/3404835.3462840)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cai et al. (2023) Bo Cai, Yaoxiang Yu, and Yi Hu. 2023. CSSAM: Code Search
    via Attention Matching of Code Semantics and Structures. In *IEEE International
    Conference on Software Analysis, Evolution and Reengineering, SANER 2023, Taipa,
    Macao, March 21-24, 2023*, Tao Zhang, Xin Xia, and Nicole Novielli (Eds.). IEEE,
    402–413. [https://doi.org/10.1109/SANER56733.2023.00045](https://doi.org/10.1109/SANER56733.2023.00045)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cambronero et al. (2019) José Cambronero, Hongyu Li, Seohyun Kim, Koushik Sen,
    and Satish Chandra. 2019. When deep learning met code search. In *Proceedings
    of the ACM Joint Meeting on European Software Engineering Conference and Symposium
    on the Foundations of Software Engineering, ESEC/SIGSOFT FSE 2019, Tallinn, Estonia,
    August 26-30, 2019*, Marlon Dumas, Dietmar Pfahl, Sven Apel, and Alessandra Russo
    (Eds.). ACM, 964–974. [https://doi.org/10.1145/3338906.3340458](https://doi.org/10.1145/3338906.3340458)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2021) Kaibo Cao, Chunyang Chen, Sebastian Baltes, Christoph Treude,
    and Xiang Chen. 2021. Automated Query Reformulation for Efficient Search based
    on Query Logs From Stack Overflow. In *43rd IEEE/ACM International Conference
    on Software Engineering, ICSE 2021, Madrid, Spain, 22-30 May 2021*. IEEE, 1273–1285.
    [https://doi.org/10.1109/ICSE43902.2021.00116](https://doi.org/10.1109/ICSE43902.2021.00116)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chai et al. (2022) Yitian Chai, Hongyu Zhang, Beijun Shen, and Xiaodong Gu.
    2022. Cross-Domain Deep Code Search with Meta Learning. In *44th IEEE/ACM 44th
    International Conference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA,
    May 25-27, 2022*. ACM, 487–498. [https://doi.org/10.1145/3510003.3510125](https://doi.org/10.1145/3510003.3510125)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chatterjee et al. (2009) Shaunak Chatterjee, Sudeep Juvekar, and Koushik Sen.
    2009. SNIFF: A Search Engine for Java Using Free-Form Queries. In *Fundamental
    Approaches to Software Engineering, 12th International Conference, FASE 2009,
    Held as Part of the Joint European Conferences on Theory and Practice of Software,
    ETAPS 2009, York, UK, March 22-29, 2009\. Proceedings* *(Lecture Notes in Computer
    Science, Vol. 5503)*, Marsha Chechik and Martin Wirsing (Eds.). Springer, 385–400.
    [https://doi.org/10.1007/978-3-642-00593-0_26](https://doi.org/10.1007/978-3-642-00593-0_26)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021) Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé
    de Oliveira Pinto, Jared Kaplan, Harrison Edwards, Yuri Burda, Nicholas Joseph,
    Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf,
    Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,
    Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet,
    Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth
    Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas
    Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders,
    Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan
    Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer,
    Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and
    Wojciech Zaremba. 2021. Evaluating Large Language Models Trained on Code. *CoRR*
    abs/2107.03374 (2021). arXiv:2107.03374 [https://arxiv.org/abs/2107.03374](https://arxiv.org/abs/2107.03374)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen and Zhou (2018) Qingying Chen and Minghui Zhou. 2018. A neural framework
    for retrieval and summarization of source code. In *Proceedings of the 33rd ACM/IEEE
    International Conference on Automated Software Engineering, ASE 2018, Montpellier,
    France, September 3-7, 2018*, Marianne Huchard, Christian Kästner, and Gordon
    Fraser (Eds.). ACM, 826–831. [https://doi.org/10.1145/3238147.3240471](https://doi.org/10.1145/3238147.3240471)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cheng and Kuang (2022) Yi Cheng and Li Kuang. 2022. CSRS: code search with
    relevance matching and semantic matching. In *Proceedings of the 30th IEEE/ACM
    International Conference on Program Comprehension, ICPC 2022, Virtual Event, May
    16-17, 2022*, Ayushi Rastogi, Rosalia Tufano, Gabriele Bavota, Venera Arnaoudova,
    and Sonia Haiduc (Eds.). ACM, 533–542. [https://doi.org/10.1145/3524610.3527889](https://doi.org/10.1145/3524610.3527889)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. (2019) Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina
    Toutanova. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language
    Understanding. In *Proceedings of the 2019 Conference of the North American Chapter
    of the Association for Computational Linguistics: Human Language Technologies,
    NAACL-HLT 2019, Minneapolis, MN, USA, June 2-7, 2019, Volume 1 (Long and Short
    Papers)*, Jill Burstein, Christy Doran, and Thamar Solorio (Eds.). Association
    for Computational Linguistics, 4171–4186. [https://doi.org/10.18653/v1/n19-1423](https://doi.org/10.18653/v1/n19-1423)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dong et al. (2023) Hande Dong, Jiayi Lin, Yichong Leng, Jiawei Chen, and Yutao
    Xie. 2023. Retriever and Ranker Framework with Probabilistic Hard Negative Sampling
    for Code Search. *CoRR* abs/2305.04508 (2023). [https://doi.org/10.48550/arXiv.2305.04508](https://doi.org/10.48550/arXiv.2305.04508)
    arXiv:2305.04508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Du et al. (2021) Lun Du, Xiaozhou Shi, Yanlin Wang, Ensheng Shi, Shi Han, and
    Dongmei Zhang. 2021. Is a Single Model Enough? MuCoS: A Multi-Model Ensemble Learning
    Approach for Semantic Code Search. In *CIKM ’21: The 30th ACM International Conference
    on Information and Knowledge Management, Virtual Event, Queensland, Australia,
    November 1 - 5, 2021*, Gianluca Demartini, Guido Zuccon, J. Shane Culpepper, Zi Huang,
    and Hanghang Tong (Eds.). ACM, 2994–2998. [https://doi.org/10.1145/3459637.3482127](https://doi.org/10.1145/3459637.3482127)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eberhart and McMillan (2022) Zachary Eberhart and Collin McMillan. 2022. Generating
    Clarifying Questions for Query Refinement in Source Code Search. In *IEEE International
    Conference on Software Analysis, Evolution and Reengineering, SANER 2022, Honolulu,
    HI, USA, March 15-18, 2022*. IEEE, 140–151. [https://doi.org/10.1109/SANER53432.2022.00028](https://doi.org/10.1109/SANER53432.2022.00028)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020) Zhangyin Feng, Daya Guo, Duyu Tang, Nan Duan, Xiaocheng
    Feng, Ming Gong, Linjun Shou, Bing Qin, Ting Liu, Daxin Jiang, and Ming Zhou.
    2020. CodeBERT: A Pre-Trained Model for Programming and Natural Languages. In
    *Findings of the Association for Computational Linguistics: EMNLP 2020, Online
    Event, 16-20 November 2020* *(Findings of ACL, Vol. EMNLP 2020)*, Trevor Cohn,
    Yulan He, and Yang Liu (Eds.). Association for Computational Linguistics, 1536–1547.
    [https://doi.org/10.18653/v1/2020.findings-emnlp.139](https://doi.org/10.18653/v1/2020.findings-emnlp.139)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gharehyazie et al. (2017) Mohammad Gharehyazie, Baishakhi Ray, and Vladimir
    Filkov. 2017. Some from here, some from there: cross-project code reuse in GitHub.
    In *Proceedings of the 14th International Conference on Mining Software Repositories,
    MSR 2017, Buenos Aires, Argentina, May 20-28, 2017*, Jesús M. González-Barahona,
    Abram Hindle, and Lin Tan (Eds.). IEEE Computer Society, 291–301. [https://doi.org/10.1109/MSR.2017.15](https://doi.org/10.1109/MSR.2017.15)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grazia and Pradel (2023) Luca Di Grazia and Michael Pradel. 2023. Code Search:
    A Survey of Techniques for Finding Code. *ACM Comput. Surv.* 55, 11 (2023), 220:1–220:31.
    [https://doi.org/10.1145/3565971](https://doi.org/10.1145/3565971)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2021a) Jian Gu, Zimin Chen, and Martin Monperrus. 2021a. Multimodal
    Representation for Neural Code Search. In *IEEE International Conference on Software
    Maintenance and Evolution, ICSME 2021, Luxembourg, September 27 - October 1, 2021*.
    IEEE, 483–494. [https://doi.org/10.1109/ICSME52107.2021.00049](https://doi.org/10.1109/ICSME52107.2021.00049)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2021b) Wenchao Gu, Zongjie Li, Cuiyun Gao, Chaozheng Wang, Hongyu
    Zhang, Zenglin Xu, and Michael R. Lyu. 2021b. CRaDLe: Deep code retrieval based
    on semantic Dependency Learning. *Neural Networks* 141 (2021), 385–394. [https://doi.org/10.1016/j.neunet.2021.04.019](https://doi.org/10.1016/j.neunet.2021.04.019)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gu et al. (2022) Wenchao Gu, Yanlin Wang, Lun Du, Hongyu Zhang, Shi Han, Dongmei
    Zhang, and Michael R. Lyu. 2022. Accelerating Code Search with Deep Hashing and
    Code Classification. In *Proceedings of the 60th Annual Meeting of the Association
    for Computational Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland,
    May 22-27, 2022*, Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.).
    Association for Computational Linguistics, 2534–2544. [https://doi.org/10.18653/v1/2022.acl-long.181](https://doi.org/10.18653/v1/2022.acl-long.181)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gu et al. (2018) Xiaodong Gu, Hongyu Zhang, and Sunghun Kim. 2018. Deep code
    search. In *Proceedings of the 40th International Conference on Software Engineering,
    ICSE 2018, Gothenburg, Sweden, May 27 - June 03, 2018*, Michel Chaudron, Ivica
    Crnkovic, Marsha Chechik, and Mark Harman (Eds.). ACM, 933–944. [https://doi.org/10.1145/3180155.3180167](https://doi.org/10.1145/3180155.3180167)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2022) Daya Guo, Shuai Lu, Nan Duan, Yanlin Wang, Ming Zhou, and
    Jian Yin. 2022. UniXcoder: Unified Cross-Modal Pre-training for Code Representation.
    In *Proceedings of the 60th Annual Meeting of the Association for Computational
    Linguistics (Volume 1: Long Papers), ACL 2022, Dublin, Ireland, May 22-27, 2022*,
    Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (Eds.). Association for
    Computational Linguistics, 7212–7225. [https://doi.org/10.18653/v1/2022.acl-long.499](https://doi.org/10.18653/v1/2022.acl-long.499)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2021) Daya Guo, Shuo Ren, Shuai Lu, Zhangyin Feng, Duyu Tang, Shujie
    Liu, Long Zhou, Nan Duan, Alexey Svyatkovskiy, Shengyu Fu, Michele Tufano, Shao Kun
    Deng, Colin B. Clement, Dawn Drain, Neel Sundaresan, Jian Yin, Daxin Jiang, and
    Ming Zhou. 2021. GraphCodeBERT: Pre-training Code Representations with Data Flow.
    In *9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021*. OpenReview.net. [https://openreview.net/forum?id=jLoC4ez43PZ](https://openreview.net/forum?id=jLoC4ez43PZ)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Haldar et al. (2020) Rajarshi Haldar, Lingfei Wu, Jinjun Xiong, and Julia Hockenmaier.
    2020. A Multi-Perspective Architecture for Semantic Code Search. In *Proceedings
    of the 58th Annual Meeting of the Association for Computational Linguistics, ACL
    2020, Online, July 5-10, 2020*, Dan Jurafsky, Joyce Chai, Natalie Schluter, and
    Joel R. Tetreault (Eds.). Association for Computational Linguistics, 8563–8568.
    [https://doi.org/10.18653/v1/2020.acl-main.758](https://doi.org/10.18653/v1/2020.acl-main.758)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Han et al. (2022) Hojae Han, Seung-won Hwang, Shuai Lu, Nan Duan, and Seungtaek
    Choi. 2022. Towards Compositional Generalization in Code Search. In *Proceedings
    of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022*, Yoav Goldberg, Zornitsa
    Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics, 10743–10750.
    [https://aclanthology.org/2022.emnlp-main.737](https://aclanthology.org/2022.emnlp-main.737)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heyman and Cutsem (2020) Geert Heyman and Tom Van Cutsem. 2020. Neural Code
    Search Revisited: Enhancing Code Snippet Retrieval through Natural Language Intent.
    *CoRR* abs/2008.12193 (2020). arXiv:2008.12193 [https://arxiv.org/abs/2008.12193](https://arxiv.org/abs/2008.12193)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hill et al. (2014) Emily Hill, Manuel Roldan-Vega, Jerry Alan Fails, and Greg
    Mallet. 2014. NL-based query refinement and contextualized code search results:
    A user study. In *2014 Software Evolution Week - IEEE Conference on Software Maintenance,
    Reengineering, and Reverse Engineering, CSMR-WCRE 2014, Antwerp, Belgium, February
    3-6, 2014*, Serge Demeyer, Dave W. Binkley, and Filippo Ricca (Eds.). IEEE Computer
    Society, 34–43. [https://doi.org/10.1109/CSMR-WCRE.2014.6747190](https://doi.org/10.1109/CSMR-WCRE.2014.6747190)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hindle et al. (2016) Abram Hindle, Earl T. Barr, Mark Gabel, Zhendong Su, and
    Premkumar T. Devanbu. 2016. On the naturalness of software. *Commun. ACM* 59,
    5 (2016), 122–131. [https://doi.org/10.1145/2902362](https://doi.org/10.1145/2902362)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2023) Fan Hu, Yanlin Wang, Lun Du, Xirong Li, Hongyu Zhang, Shi Han,
    and Dongmei Zhang. 2023. Revisiting Code Search in a Two-Stage Paradigm. In *Proceedings
    of the Sixteenth ACM International Conference on Web Search and Data Mining, WSDM
    2023, Singapore, 27 February 2023 - 3 March 2023*, Tat-Seng Chua, Hady W. Lauw,
    Luo Si, Evimaria Terzi, and Panayiotis Tsaparas (Eds.). ACM, 994–1002. [https://doi.org/10.1145/3539597.3570383](https://doi.org/10.1145/3539597.3570383)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2020) Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep
    code comment generation with hybrid lexical and syntactical information. *Empir.
    Softw. Eng.* 25, 3 (2020), 2179–2217. [https://doi.org/10.1007/s10664-019-09730-9](https://doi.org/10.1007/s10664-019-09730-9)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. (2021) Junjie Huang, Duyu Tang, Linjun Shou, Ming Gong, Ke Xu,
    Daxin Jiang, Ming Zhou, and Nan Duan. 2021. CoSQA: 20, 000+ Web Queries for Code
    Search and Question Answering. In *Proceedings of the 59th Annual Meeting of the
    Association for Computational Linguistics and the 11th International Joint Conference
    on Natural Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual
    Event, August 1-6, 2021*, Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli
    (Eds.). Association for Computational Linguistics, 5690–5700. [https://doi.org/10.18653/v1/2021.acl-long.442](https://doi.org/10.18653/v1/2021.acl-long.442)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2019) Qing Huang, Yang Yang, and Ming Cheng. 2019. Deep learning
    the semantics of change sequences for query expansion. *Softw. Pract. Exp.* 49,
    11 (2019), 1600–1617. [https://doi.org/10.1002/spe.2736](https://doi.org/10.1002/spe.2736)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Husain et al. (2019) Hamel Husain, Ho-Hsiang Wu, Tiferet Gazit, Miltiadis Allamanis,
    and Marc Brockschmidt. 2019. CodeSearchNet Challenge: Evaluating the State of
    Semantic Code Search. *CoRR* abs/1909.09436 (2019). arXiv:1909.09436 [http://arxiv.org/abs/1909.09436](http://arxiv.org/abs/1909.09436)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Iyer et al. (2016) Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke
    Zettlemoyer. 2016. Summarizing Source Code using a Neural Attention Model. In
    *Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics,
    ACL 2016, August 7-12, 2016, Berlin, Germany, Volume 1: Long Papers*. The Association
    for Computer Linguistics. [https://doi.org/10.18653/v1/p16-1195](https://doi.org/10.18653/v1/p16-1195)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kanade et al. (2020) Aditya Kanade, Petros Maniatis, Gogul Balakrishnan, and
    Kensen Shi. 2020. Learning and Evaluating Contextual Embedding of Source Code.
    In *Proceedings of the 37th International Conference on Machine Learning, ICML
    2020, 13-18 July 2020, Virtual Event* *(Proceedings of Machine Learning Research,
    Vol. 119)*. PMLR, 5110–5121. [http://proceedings.mlr.press/v119/kanade20a.html](http://proceedings.mlr.press/v119/kanade20a.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Karmakar and Robbes (2021) Anjan Karmakar and Romain Robbes. 2021. What do pre-trained
    code models know about code?. In *36th IEEE/ACM International Conference on Automated
    Software Engineering, ASE 2021, Melbourne, Australia, November 15-19, 2021*. IEEE,
    1332–1336. [https://doi.org/10.1109/ASE51524.2021.9678927](https://doi.org/10.1109/ASE51524.2021.9678927)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Keele (2007) Staffs Keele. 2007. *Guidelines for performing systematic literature
    reviews in software engineering*. Technical Report. Ver. 2.3 EBSE Technical Report.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khalifa (2019) Muhammad Khalifa. 2019. Semantic Source Code Search: A Study
    of the Past and a Glimpse at the Future. *CoRR* abs/1908.06738 (2019). arXiv:1908.06738
    [http://arxiv.org/abs/1908.06738](http://arxiv.org/abs/1908.06738) Withdrawn..'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lachaux et al. (2021) Marie-Anne Lachaux, Baptiste Rozière, Marc Szafraniec,
    and Guillaume Lample. 2021. DOBF: A Deobfuscation Pre-Training Objective for Programming
    Languages. In *Advances in Neural Information Processing Systems 34: Annual Conference
    on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021,
    virtual*, Marc’Aurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang,
    and Jennifer Wortman Vaughan (Eds.). 14967–14979. [https://proceedings.neurips.cc/paper/2021/hash/7d6548bdc0082aacc950ed35e91fcccb-Abstract.html](https://proceedings.neurips.cc/paper/2021/hash/7d6548bdc0082aacc950ed35e91fcccb-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2019) Hongyu Li, Seohyun Kim, and Satish Chandra. 2019. Neural Code
    Search Evaluation Dataset. *CoRR* abs/1908.09804 (2019). arXiv:1908.09804 [http://arxiv.org/abs/1908.09804](http://arxiv.org/abs/1908.09804)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022d) Haochen Li, Chunyan Miao, Cyril Leung, Yanxian Huang, Yuan
    Huang, Hongyu Zhang, and Yanlin Wang. 2022d. Exploring Representation-level Augmentation
    for Code Search. In *Proceedings of the 2022 Conference on Empirical Methods in
    Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December
    7-11, 2022*, Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association
    for Computational Linguistics, 4924–4936. [https://aclanthology.org/2022.emnlp-main.327](https://aclanthology.org/2022.emnlp-main.327)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022a) Xiaonan Li, Yeyun Gong, Yelong Shen, Xipeng Qiu, Hang Zhang,
    Bolun Yao, Weizhen Qi, Daxin Jiang, Weizhu Chen, and Nan Duan. 2022a. CodeRetriever:
    Unimodal and Bimodal Contrastive Learning. In *Proceedings of the 2022 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United
    Arab Emirates, December 7-11, 2022*. Association for Computational Linguistics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022b) Xiaonan Li, Daya Guo, Yeyun Gong, Yun Lin, Yelong Shen, Xipeng
    Qiu, Daxin Jiang, Weizhu Chen, and Nan Duan. 2022b. Soft-Labeled Contrastive Pre-Training
    for Function-Level Code Representation. In *Findings of the Association for Computational
    Linguistics: EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022*,
    Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (Eds.). Association for Computational
    Linguistics, 118–129. [https://aclanthology.org/2022.findings-emnlp.9](https://aclanthology.org/2022.findings-emnlp.9)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022e) Yiyang Li, Hongqiu Wu, and Hai Zhao. 2022e. Semantic-Preserving
    Adversarial Code Comprehension. In *Proceedings of the 29th International Conference
    on Computational Linguistics, COLING 2022, Gyeongju, Republic of Korea, October
    12-17, 2022*, Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky,
    Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji,
    Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong
    He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na (Eds.). International
    Committee on Computational Linguistics, 3017–3028. [https://aclanthology.org/2022.coling-1.267](https://aclanthology.org/2022.coling-1.267)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022c) Zhiyu Li, Shuai Lu, Daya Guo, Nan Duan, Shailesh Jannu, Grant
    Jenks, Deep Majumder, Jared Green, Alexey Svyatkovskiy, Shengyu Fu, and Neel Sundaresan.
    2022c. Automating code review activities by large-scale pre-training. In *Proceedings
    of the 30th ACM Joint European Software Engineering Conference and Symposium on
    the Foundations of Software Engineering, ESEC/FSE 2022, Singapore, Singapore,
    November 14-18, 2022*, Abhik Roychoudhury, Cristian Cadar, and Miryung Kim (Eds.).
    ACM, 1035–1047. [https://doi.org/10.1145/3540250.3549081](https://doi.org/10.1145/3540250.3549081)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ling et al. (2020) Chunyang Ling, Zeqi Lin, Yanzhen Zou, and Bing Xie. 2020.
    Adaptive Deep Code Search. In *ICPC ’20: 28th International Conference on Program
    Comprehension, Seoul, Republic of Korea, July 13-15, 2020*. ACM, 48–59. [https://doi.org/10.1145/3387904.3389278](https://doi.org/10.1145/3387904.3389278)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ling et al. (2021) Xiang Ling, Lingfei Wu, Saizhuo Wang, Gaoning Pan, Tengfei
    Ma, Fangli Xu, Alex X. Liu, Chunming Wu, and Shouling Ji. 2021. Deep Graph Matching
    and Searching for Semantic Code Retrieval. *ACM Trans. Knowl. Discov. Data* 15,
    5 (2021), 88:1–88:21. [https://doi.org/10.1145/3447571](https://doi.org/10.1145/3447571)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2022) Chao Liu, Xin Xia, David Lo, Cuiyun Gao, Xiaohu Yang, and
    John C. Grundy. 2022. Opportunities and Challenges in Code Search Tools. *ACM
    Comput. Surv.* 54, 9 (2022), 196:1–196:40. [https://doi.org/10.1145/3480027](https://doi.org/10.1145/3480027)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2019a) Jason Liu, Seohyun Kim, Vijayaraghavan Murali, Swarat Chaudhuri,
    and Satish Chandra. 2019a. Neural query expansion for code search. In *Proceedings
    of the 3rd ACM SIGPLAN International Workshop on Machine Learning and Programming
    Languages, MAPL@PLDI 2019, Phoenix, AZ, USA, June 22, 2019*, Tim Mattson, Abdullah
    Muzahid, and Armando Solar-Lezama (Eds.). ACM, 29–37. [https://doi.org/10.1145/3315508.3329975](https://doi.org/10.1145/3315508.3329975)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2023) Shangqing Liu, Xiaofei Xie, Jing Kai Siow, Lei Ma, Guozhu
    Meng, and Yang Liu. 2023. GraphSearchNet: Enhancing GNNs via Capturing Global
    Dependencies for Semantic Code Search. *IEEE Trans. Software Eng.* 49, 4 (2023),
    2839–2855. [https://doi.org/10.1109/TSE.2022.3233901](https://doi.org/10.1109/TSE.2022.3233901)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2019b) Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi,
    Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019b.
    RoBERTa: A Robustly Optimized BERT Pretraining Approach. *CoRR* abs/1907.11692
    (2019). arXiv:1907.11692 [http://arxiv.org/abs/1907.11692](http://arxiv.org/abs/1907.11692)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. (2021) Shuai Lu, Daya Guo, Shuo Ren, Junjie Huang, Alexey Svyatkovskiy,
    Ambrosio Blanco, Colin B. Clement, Dawn Drain, Daxin Jiang, Duyu Tang, Ge Li,
    Lidong Zhou, Linjun Shou, Long Zhou, Michele Tufano, Ming Gong, Ming Zhou, Nan
    Duan, Neel Sundaresan, Shao Kun Deng, Shengyu Fu, and Shujie Liu. 2021. CodeXGLUE:
    A Machine Learning Benchmark Dataset for Code Understanding and Generation. In
    *Proceedings of the Neural Information Processing Systems Track on Datasets and
    Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual*, Joaquin
    Vanschoren and Sai-Kit Yeung (Eds.). [https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c16a5320fa475530d9583c34fd356ef5-Abstract-round1.html](https://datasets-benchmarks-proceedings.neurips.cc/paper/2021/hash/c16a5320fa475530d9583c34fd356ef5-Abstract-round1.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lv et al. (2015) Fei Lv, Hongyu Zhang, Jian-Guang Lou, Shaowei Wang, Dongmei
    Zhang, and Jianjun Zhao. 2015. CodeHow: Effective Code Search Based on API Understanding
    and Extended Boolean Model (E). In *30th IEEE/ACM International Conference on
    Automated Software Engineering, ASE 2015, Lincoln, NE, USA, November 9-13, 2015*,
    Myra B. Cohen, Lars Grunske, and Michael Whalen (Eds.). IEEE Computer Society,
    260–270. [https://doi.org/10.1109/ASE.2015.42](https://doi.org/10.1109/ASE.2015.42)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ma et al. (2023) Yingwei Ma, Yue Yu, Shanshan Li, Zhouyang Jia, Jun Ma, Rulin
    Xu, Wei Dong, and Xiangke Liao. 2023. MulCS: Towards a Unified Deep Representation
    for Multilingual Code Search. In *IEEE International Conference on Software Analysis,
    Evolution and Reengineering, SANER 2023, Taipa, Macao, March 21-24, 2023*, Tao
    Zhang, Xin Xia, and Nicole Novielli (Eds.). IEEE, 120–131. [https://doi.org/10.1109/SANER56733.2023.00021](https://doi.org/10.1109/SANER56733.2023.00021)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Markovtsev and Long (2018) Vadim Markovtsev and Waren Long. 2018. Public git
    archive: a big code dataset for all. In *Proceedings of the 15th International
    Conference on Mining Software Repositories, MSR 2018, Gothenburg, Sweden, May
    28-29, 2018*, Andy Zaidman, Yasutaka Kamei, and Emily Hill (Eds.). ACM, 34–37.
    [https://doi.org/10.1145/3196398.3196464](https://doi.org/10.1145/3196398.3196464)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'McMillan et al. (2011) Collin McMillan, Mark Grechanik, Denys Poshyvanyk, Qing
    Xie, and Chen Fu. 2011. Portfolio: finding relevant functions and their usage.
    In *Proceedings of the 33rd International Conference on Software Engineering,
    ICSE 2011, Waikiki, Honolulu , HI, USA, May 21-28, 2011*, Richard N. Taylor, Harald C.
    Gall, and Nenad Medvidovic (Eds.). ACM, 111–120. [https://doi.org/10.1145/1985793.1985809](https://doi.org/10.1145/1985793.1985809)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mou et al. (2016) Lili Mou, Ge Li, Lu Zhang, Tao Wang, and Zhi Jin. 2016. Convolutional
    Neural Networks over Tree Structures for Programming Language Processing. In *Proceedings
    of the Thirtieth AAAI Conference on Artificial Intelligence, February 12-17, 2016,
    Phoenix, Arizona, USA*, Dale Schuurmans and Michael P. Wellman (Eds.). AAAI Press,
    1287–1293. [http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11775](http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/11775)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nie et al. (2016) Liming Nie, He Jiang, Zhilei Ren, Zeyi Sun, and Xiaochen Li.
    2016. Query Expansion Based on Crowd Knowledge for Code Search. *IEEE Trans. Serv.
    Comput.* 9, 5 (2016), 771–783. [https://doi.org/10.1109/TSC.2016.2560165](https://doi.org/10.1109/TSC.2016.2560165)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Niu et al. (2022) Changan Niu, Chuanyi Li, Vincent Ng, Jidong Ge, Liguo Huang,
    and Bin Luo. 2022. SPT-Code: Sequence-to-Sequence Pre-Training for Learning Source
    Code Representations. In *44th IEEE/ACM 44th International Conference on Software
    Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022*. ACM, 1–13. [https://doi.org/10.1145/3510003.3510096](https://doi.org/10.1145/3510003.3510096)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI (2023) OpenAI. 2023. GPT-4 Technical Report. *CoRR* abs/2303.08774 (2023).
    [https://doi.org/10.48550/arXiv.2303.08774](https://doi.org/10.48550/arXiv.2303.08774)
    arXiv:2303.08774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2023) Shinwoo Park, Youngwook Kim, and Yo-Sub Han. 2023. Contrastive
    Learning with Keyword-based Data Augmentation for Code Search and Code Question
    Answering. In *Proceedings of the 17th Conference of the European Chapter of the
    Association for Computational Linguistics, EACL 2023, Dubrovnik, Croatia, May
    2-6, 2023*, Andreas Vlachos and Isabelle Augenstein (Eds.). Association for Computational
    Linguistics, 3591–3601. [https://aclanthology.org/2023.eacl-main.262](https://aclanthology.org/2023.eacl-main.262)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Petersen et al. (2015) Kai Petersen, Sairam Vakkalanka, and Ludwik Kuzniarz.
    2015. Guidelines for conducting systematic mapping studies in software engineering:
    An update. *Inf. Softw. Technol.* 64 (2015), 1–18. [https://doi.org/10.1016/j.infsof.2015.03.007](https://doi.org/10.1016/j.infsof.2015.03.007)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rahman (2019) Mohammad Masudur Rahman. 2019. Supporting code search with context-aware,
    analytics-driven, effective query reformulation. In *Proceedings of the 41st International
    Conference on Software Engineering: Companion Proceedings, ICSE 2019, Montreal,
    QC, Canada, May 25-31, 2019*, Joanne M. Atlee, Tevfik Bultan, and Jon Whittle
    (Eds.). IEEE / ACM, 226–229. [https://doi.org/10.1109/ICSE-Companion.2019.00088](https://doi.org/10.1109/ICSE-Companion.2019.00088)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahman and Roy (2018) Mohammad Masudur Rahman and Chanchal K. Roy. 2018. Effective
    Reformulation of Query for Code Search Using Crowdsourced Knowledge and Extra-Large
    Data Analytics. In *2018 IEEE International Conference on Software Maintenance
    and Evolution, ICSME 2018, Madrid, Spain, September 23-29, 2018*. IEEE Computer
    Society, 473–484. [https://doi.org/10.1109/ICSME.2018.00057](https://doi.org/10.1109/ICSME.2018.00057)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rahman et al. (2019) Mohammad Masudur Rahman, Chanchal K. Roy, and David Lo.
    2019. Automatic query reformulation for code search using crowdsourced knowledge.
    *Empir. Softw. Eng.* 24, 4 (2019), 1869–1924. [https://doi.org/10.1007/s10664-018-9671-0](https://doi.org/10.1007/s10664-018-9671-0)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sachdev et al. (2018) Saksham Sachdev, Hongyu Li, Sifei Luan, Seohyun Kim,
    Koushik Sen, and Satish Chandra. 2018. Retrieval on source code: a neural code
    search. In *Proceedings of the 2nd ACM SIGPLAN International Workshop on Machine
    Learning and Programming Languages, MAPL@PLDI 2018, Philadelphia, PA, USA, June
    18-22, 2018*, Justin Gottschlich and Alvin Cheung (Eds.). ACM, 31–41. [https://doi.org/10.1145/3211346.3211353](https://doi.org/10.1145/3211346.3211353)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Salza et al. (2023) Pasquale Salza, Christoph Schwizer, Jian Gu, and Harald C.
    Gall. 2023. On the Effectiveness of Transfer Learning for Code Search. *IEEE Trans.
    Software Eng.* 49, 4 (2023), 1804–1822. [https://doi.org/10.1109/TSE.2022.3192755](https://doi.org/10.1109/TSE.2022.3192755)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shi et al. (2022) Yucen Shi, Ying Yin, Zhengkui Wang, David Lo, Tao Zhang, Xin
    Xia, Yuhai Zhao, and Bowen Xu. 2022. How to better utilize code graphs in semantic
    code search?. In *Proceedings of the 30th ACM Joint European Software Engineering
    Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE
    2022, Singapore, Singapore, November 14-18, 2022*, Abhik Roychoudhury, Cristian
    Cadar, and Miryung Kim (Eds.). ACM, 722–733. [https://doi.org/10.1145/3540250.3549087](https://doi.org/10.1145/3540250.3549087)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shuai et al. (2020) Jianhang Shuai, Ling Xu, Chao Liu, Meng Yan, Xin Xia, and
    Yan Lei. 2020. Improving Code Search with Co-Attentive Representation Learning.
    In *ICPC ’20: 28th International Conference on Program Comprehension, Seoul, Republic
    of Korea, July 13-15, 2020*. ACM, 196–207. [https://doi.org/10.1145/3387904.3389269](https://doi.org/10.1145/3387904.3389269)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sirres et al. (2018) Raphael Sirres, Tegawendé F. Bissyandé, Dongsun Kim, David
    Lo, Jacques Klein, Kisub Kim, and Yves Le Traon. 2018. Augmenting and structuring
    user queries to support efficient free-form code search. In *Proceedings of the
    40th International Conference on Software Engineering, ICSE 2018, Gothenburg,
    Sweden, May 27 - June 03, 2018*, Michel Chaudron, Ivica Crnkovic, Marsha Chechik,
    and Mark Harman (Eds.). ACM, 945. [https://doi.org/10.1145/3180155.3182513](https://doi.org/10.1145/3180155.3182513)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sivaraman et al. (2019) Aishwarya Sivaraman, Tianyi Zhang, Guy Van den Broeck,
    and Miryung Kim. 2019. Active inductive logic programming for code search. In
    *Proceedings of the 41st International Conference on Software Engineering, ICSE
    2019, Montreal, QC, Canada, May 25-31, 2019*, Joanne M. Atlee, Tevfik Bultan,
    and Jon Whittle (Eds.). IEEE / ACM, 292–303. [https://doi.org/10.1109/ICSE.2019.00044](https://doi.org/10.1109/ICSE.2019.00044)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stolee et al. (2014) Kathryn T. Stolee, Sebastian G. Elbaum, and Daniel Dobos.
    2014. Solving the Search for Source Code. *ACM Trans. Softw. Eng. Methodol.* 23,
    3 (2014), 26:1–26:45. [https://doi.org/10.1145/2581377](https://doi.org/10.1145/2581377)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022a) Weisong Sun, Chunrong Fang, Yuchen Chen, Guanhong Tao, Tingxu
    Han, and Quanjun Zhang. 2022a. Code Search based on Context-aware Code Translation.
    In *44th IEEE/ACM 44th International Conference on Software Engineering, ICSE
    2022, Pittsburgh, PA, USA, May 25-27, 2022*. ACM, 388–400. [https://doi.org/10.1145/3510003.3510140](https://doi.org/10.1145/3510003.3510140)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sun et al. (2022b) Zhensu Sun, Li Li, Yan Liu, Xiaoning Du, and Li Li. 2022b.
    On the Importance of Building High-quality Training Datasets for Neural Code Search.
    In *44th IEEE/ACM 44th International Conference on Software Engineering, ICSE
    2022, Pittsburgh, PA, USA, May 25-27, 2022*. ACM, 1609–1620. [https://doi.org/10.1145/3510003.3510160](https://doi.org/10.1145/3510003.3510160)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutskever et al. (2014) Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. 2014.
    Sequence to Sequence Learning with Neural Networks. In *Advances in Neural Information
    Processing Systems 27: Annual Conference on Neural Information Processing Systems
    2014, December 8-13 2014, Montreal, Quebec, Canada*, Zoubin Ghahramani, Max Welling,
    Corinna Cortes, Neil D. Lawrence, and Kilian Q. Weinberger (Eds.). 3104–3112.
    [https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html](https://proceedings.neurips.cc/paper/2014/hash/a14ac55a4f27472c5d894ec1c3c743d2-Abstract.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2019) Yao Wan, Jingdong Shu, Yulei Sui, Guandong Xu, Zhou Zhao,
    Jian Wu, and Philip S. Yu. 2019. Multi-modal Attention Network Learning for Semantic
    Source Code Retrieval. In *34th IEEE/ACM International Conference on Automated
    Software Engineering, ASE 2019, San Diego, CA, USA, November 11-15, 2019*. IEEE,
    13–25. [https://doi.org/10.1109/ASE.2019.00012](https://doi.org/10.1109/ASE.2019.00012)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wan et al. (2022a) Yao Wan, Shijie Zhang, Hongyu Zhang, Yulei Sui, Guandong
    Xu, Dezhong Yao, Hai Jin, and Lichao Sun. 2022a. You see what I want you to see:
    poisoning vulnerabilities in neural code search. In *Proceedings of the 30th ACM
    Joint European Software Engineering Conference and Symposium on the Foundations
    of Software Engineering, ESEC/FSE 2022, Singapore, Singapore, November 14-18,
    2022*, Abhik Roychoudhury, Cristian Cadar, and Miryung Kim (Eds.). ACM, 1233–1245.
    [https://doi.org/10.1145/3540250.3549153](https://doi.org/10.1145/3540250.3549153)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2022b) Yao Wan, Wei Zhao, Hongyu Zhang, Yulei Sui, Guandong Xu,
    and Hai Jin. 2022b. What Do They Capture? - A Structural Analysis of Pre-Trained
    Language Models for Source Code. In *44th IEEE/ACM 44th International Conference
    on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022*. ACM,
    2377–2388. [https://doi.org/10.1145/3510003.3510050](https://doi.org/10.1145/3510003.3510050)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wan et al. (2018) Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian
    Wu, and Philip S. Yu. 2018. Improving automatic source code summarization via
    deep reinforcement learning. In *Proceedings of the 33rd ACM/IEEE International
    Conference on Automated Software Engineering, ASE 2018, Montpellier, France, September
    3-7, 2018*, Marianne Huchard, Christian Kästner, and Gordon Fraser (Eds.). ACM,
    397–407. [https://doi.org/10.1145/3238147.3238206](https://doi.org/10.1145/3238147.3238206)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022b) Chaozheng Wang, Zhenhao Nong, Cuiyun Gao, Zongjie Li, Jichuan
    Zeng, Zhenchang Xing, and Yang Liu. 2022b. Enriching query semantics for code
    search with reinforcement learning. *Neural Networks* 145 (2022), 22–32. [https://doi.org/10.1016/j.neunet.2021.09.025](https://doi.org/10.1016/j.neunet.2021.09.025)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022a) Deze Wang, Zhouyang Jia, Shanshan Li, Yue Yu, Yun Xiong,
    Wei Dong, and Xiangke Liao. 2022a. Bridging Pre-trained Models and Downstream
    Tasks for Source Code Understanding. In *44th IEEE/ACM 44th International Conference
    on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27, 2022*. ACM,
    287–298. [https://doi.org/10.1145/3510003.3510062](https://doi.org/10.1145/3510003.3510062)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2022d) Wenhua Wang, Yuqun Zhang, Yulei Sui, Yao Wan, Zhou Zhao,
    Jian Wu, Philip S. Yu, and Guandong Xu. 2022d. Reinforcement-Learning-Guided Source
    Code Summarization Using Hierarchical Attention. *IEEE Trans. Software Eng.* 48,
    2 (2022), 102–119. [https://doi.org/10.1109/TSE.2020.2979701](https://doi.org/10.1109/TSE.2020.2979701)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Wenhua Wang, Yuqun Zhang, Zhengran Zeng, and Guandong Xu.
    2020. TranS^3: A Transformer-based Framework for Unifying Code Summarization and
    Code Search. *CoRR* abs/2003.03238 (2020). arXiv:2003.03238 [https://arxiv.org/abs/2003.03238](https://arxiv.org/abs/2003.03238)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021b) Xin Wang, Yasheng Wang, Fei Mi, Pingyi Zhou, Yao Wan, Xiao
    Liu, Li Li, Hao Wu, Jin Liu, and Xin Jiang. 2021b. Syncobert: Syntax-guided multi-modal
    contrastive pre-training for code representation. *arXiv preprint arXiv:2108.04556*
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022c) Xin Wang, Yasheng Wang, Yao Wan, Jiawei Wang, Pingyi Zhou,
    Li Li, Hao Wu, and Jin Liu. 2022c. CODE-MVP: Learning to Represent Source Code
    from Multiple Views with Contrastive Pre-Training. In *Findings of the Association
    for Computational Linguistics: NAACL 2022, Seattle, WA, United States, July 10-15,
    2022*, Marine Carpuat, Marie-Catherine de Marneffe, and Iván Vladimir Meza Ruíz
    (Eds.). Association for Computational Linguistics, 1066–1077. [https://doi.org/10.18653/v1/2022.findings-naacl.80](https://doi.org/10.18653/v1/2022.findings-naacl.80)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2021a) Yue Wang, Weishi Wang, Shafiq R. Joty, and Steven C. H.
    Hoi. 2021a. CodeT5: Identifier-aware Unified Pre-trained Encoder-Decoder Models
    for Code Understanding and Generation. In *Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing, EMNLP 2021, Virtual Event
    / Punta Cana, Dominican Republic, 7-11 November, 2021*, Marie-Francine Moens,
    Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih (Eds.). Association for Computational
    Linguistics, 8696–8708. [https://doi.org/10.18653/v1/2021.emnlp-main.685](https://doi.org/10.18653/v1/2021.emnlp-main.685)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Watson et al. (2022) Cody Watson, Nathan Cooper, David Nader-Palacio, Kevin
    Moran, and Denys Poshyvanyk. 2022. A Systematic Literature Review on the Use of
    Deep Learning in Software Engineering Research. *ACM Trans. Softw. Eng. Methodol.*
    31, 2 (2022), 32:1–32:58. [https://doi.org/10.1145/3485275](https://doi.org/10.1145/3485275)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu and Yang (2019) Huaiguang Wu and Yang Yang. 2019. Code Search Based on Alteration
    Intent. *IEEE Access* 7 (2019), 56796–56802. [https://doi.org/10.1109/ACCESS.2019.2913560](https://doi.org/10.1109/ACCESS.2019.2913560)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2021) Ling Xu, Huanhuan Yang, Chao Liu, Jianhang Shuai, Meng Yan,
    Yan Lei, and Zhou Xu. 2021. Two-Stage Attention-Based Model for Code Search with
    Textual and Structural Features. In *28th IEEE International Conference on Software
    Analysis, Evolution and Reengineering, SANER 2021, Honolulu, HI, USA, March 9-12,
    2021*. IEEE, 342–353. [https://doi.org/10.1109/SANER50967.2021.00039](https://doi.org/10.1109/SANER50967.2021.00039)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yan et al. (2020) Shuhan Yan, Hang Yu, Yuting Chen, Beijun Shen, and Lingxiao
    Jiang. 2020. Are the Code Snippets What We Are Searching for? A Benchmark and
    an Empirical Study on Code Search with Natural-Language Queries. In *27th IEEE
    International Conference on Software Analysis, Evolution and Reengineering, SANER
    2020, London, ON, Canada, February 18-21, 2020*, Kostas Kontogiannis, Foutse Khomh,
    Alexander Chatzigeorgiou, Marios-Eleftherios Fokaefs, and Minghui Zhou (Eds.).
    IEEE, 344–354. [https://doi.org/10.1109/SANER48275.2020.9054840](https://doi.org/10.1109/SANER48275.2020.9054840)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2021) Zhen Yang, Jacky Keung, Xiao Yu, Xiaodong Gu, Zhengyuan Wei,
    Xiaoxue Ma, and Miao Zhang. 2021. A Multi-Modal Transformer-based Code Summarization
    Approach for Smart Contracts. In *29th IEEE/ACM International Conference on Program
    Comprehension, ICPC 2021, Madrid, Spain, May 20-21, 2021*. IEEE, 1–12. [https://doi.org/10.1109/ICPC52881.2021.00010](https://doi.org/10.1109/ICPC52881.2021.00010)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2019) Ziyu Yao, Jayavardhan Reddy Peddamail, and Huan Sun. 2019.
    CoaCor: Code Annotation for Code Retrieval with Reinforcement Learning. In *The
    World Wide Web Conference, WWW 2019, San Francisco, CA, USA, May 13-17, 2019*,
    Ling Liu, Ryen W. White, Amin Mantrach, Fabrizio Silvestri, Julian J. McAuley,
    Ricardo Baeza-Yates, and Leila Zia (Eds.). ACM, 2203–2214. [https://doi.org/10.1145/3308558.3313632](https://doi.org/10.1145/3308558.3313632)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yao et al. (2018) Ziyu Yao, Daniel S. Weld, Wei-Peng Chen, and Huan Sun. 2018.
    StaQC: A Systematically Mined Question-Code Dataset from Stack Overflow. In *Proceedings
    of the 2018 World Wide Web Conference on World Wide Web, WWW 2018, Lyon, France,
    April 23-27, 2018*, Pierre-Antoine Champin, Fabien Gandon, Mounia Lalmas, and
    Panagiotis G. Ipeirotis (Eds.). ACM, 1693–1703. [https://doi.org/10.1145/3178876.3186081](https://doi.org/10.1145/3178876.3186081)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2020) Wei Ye, Rui Xie, Jinglei Zhang, Tianxiang Hu, Xiaoyin Wang,
    and Shikun Zhang. 2020. Leveraging Code Generation to Improve Code Retrieval and
    Summarization via Dual Learning. In *WWW ’20: The Web Conference 2020, Taipei,
    Taiwan, April 20-24, 2020*, Yennun Huang, Irwin King, Tie-Yan Liu, and Maarten
    van Steen (Eds.). ACM / IW3C2, 2309–2319. [https://doi.org/10.1145/3366423.3380295](https://doi.org/10.1145/3366423.3380295)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yin et al. (2018) Pengcheng Yin, Bowen Deng, Edgar Chen, Bogdan Vasilescu, and
    Graham Neubig. 2018. Learning to mine aligned code and natural language pairs
    from stack overflow. In *Proceedings of the 15th International Conference on Mining
    Software Repositories, MSR 2018, Gothenburg, Sweden, May 28-29, 2018*, Andy Zaidman,
    Yasutaka Kamei, and Emily Hill (Eds.). ACM, 476–486. [https://doi.org/10.1145/3196398.3196408](https://doi.org/10.1145/3196398.3196408)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zeng et al. (2023) Chen Zeng, Yue Yu, Shanshan Li, Xin Xia, Zhiming Wang, Mingyang
    Geng, Linxiao Bai, Wei Dong, and Xiangke Liao. 2023. deGraphCS: Embedding Variable-based
    Flow Graph for Neural Code Search. *ACM Trans. Softw. Eng. Methodol.* 32, 2 (2023),
    34:1–34:27. [https://doi.org/10.1145/3546066](https://doi.org/10.1145/3546066)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Feng Zhang, Haoran Niu, Iman Keivanloo, and Ying Zou. 2018.
    Expanding Queries for Code Search Using Semantically Related API Class-names.
    *IEEE Transactions on Software Engineering* 44, 11 (2018), 1070–1082. [https://doi.org/10.1109/TSE.2017.2750682](https://doi.org/10.1109/TSE.2017.2750682)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2020) Jingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter J.
    Liu. 2020. PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive
    Summarization. In *Proceedings of the 37th International Conference on Machine
    Learning, ICML 2020, 13-18 July 2020, Virtual Event* *(Proceedings of Machine
    Learning Research, Vol. 119)*. PMLR, 11328–11339. [http://proceedings.mlr.press/v119/zhang20ae.html](http://proceedings.mlr.press/v119/zhang20ae.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhao and Sun (2020) Jie Zhao and Huan Sun. 2020. Adversarial Training for Code
    Retrieval with Question-Description Relevance Regularization. In *Findings of
    the Association for Computational Linguistics: EMNLP 2020, Online Event, 16-20
    November 2020* *(Findings of ACL, Vol. EMNLP 2020)*, Trevor Cohn, Yulan He, and
    Yang Liu (Eds.). Association for Computational Linguistics, 4049–4059. [https://doi.org/10.18653/v1/2020.findings-emnlp.361](https://doi.org/10.18653/v1/2020.findings-emnlp.361)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2022) Ming Zhu, Karthik Suresh, and Chandan K. Reddy. 2022. Multilingual
    Code Snippets Training for Program Translation. In *Thirty-Sixth AAAI Conference
    on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative
    Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on
    Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February
    22 - March 1, 2022*. AAAI Press, 11783–11790. [https://ojs.aaai.org/index.php/AAAI/article/view/21434](https://ojs.aaai.org/index.php/AAAI/article/view/21434)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. (2020) Qihao Zhu, Zeyu Sun, Xiran Liang, Yingfei Xiong, and Lu Zhang.
    2020. OCoR: An Overlapping-Aware Code Retriever. In *35th IEEE/ACM International
    Conference on Automated Software Engineering, ASE 2020, Melbourne, Australia,
    September 21-25, 2020*. IEEE, 883–894. [https://doi.org/10.1145/3324884.3416530](https://doi.org/10.1145/3324884.3416530)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
