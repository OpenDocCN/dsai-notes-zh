- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 19:43:21'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:43:21
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2211.03959] Pretraining in Deep Reinforcement Learning: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2211.03959] 深度强化学习中的预训练：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2211.03959](https://ar5iv.labs.arxiv.org/html/2211.03959)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2211.03959](https://ar5iv.labs.arxiv.org/html/2211.03959)
- en: 'Pretraining in Deep Reinforcement Learning: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度强化学习中的预训练：综述
- en: \nameZhihui Xie \emailfffffarmer@sjtu.edu.cn
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: \name智辉 谢 \emailfffffarmer@sjtu.edu.cn
- en: \addrShanghai Jiao Tong University \AND\nameZichuan Lin \emailzichuanlin@tencent.com
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: \addr上海交通大学 \AND\name自川 林 \emailzichuanlin@tencent.com
- en: \addrTencent \AND\nameJunyou Li \emailjunyouli@tencent.com
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: \addr腾讯 \AND\name俊友 李 \emailjunyouli@tencent.com
- en: \addrTencent \AND\nameShuai Li \emailshuaili8@sjtu.edu.cn
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: \addr腾讯 \AND\name帅 李 \emailshuaili8@sjtu.edu.cn
- en: \addrShanghai Jiao Tong University \AND\nameDeheng Ye \emaildericye@tencent.com
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: \addr上海交通大学 \AND\name德恒 叶 \emaildericye@tencent.com
- en: \addrTencent
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: \addr腾讯
- en: Abstract
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: The past few years have seen rapid progress in combining reinforcement learning
    (RL) with deep learning. Various breakthroughs ranging from games to robotics
    have spurred the interest in designing sophisticated RL algorithms and systems.
    However, the prevailing workflow in RL is to learn tabula rasa, which may incur
    computational inefficiency. This precludes continuous deployment of RL algorithms
    and potentially excludes researchers without large-scale computing resources.
    In many other areas of machine learning, the pretraining paradigm has shown to
    be effective in acquiring transferable knowledge, which can be utilized for a
    variety of downstream tasks. Recently, we saw a surge of interest in Pretraining
    for Deep RL with promising results. However, much of the research has been based
    on different experimental settings. Due to the nature of RL, pretraining in this
    field is faced with unique challenges and hence requires new design principles.
    In this survey, we seek to systematically review existing works in pretraining
    for deep reinforcement learning, provide a taxonomy of these methods, discuss
    each sub-field, and bring attention to open problems and future directions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 过去几年中，强化学习（RL）与深度学习的结合取得了快速进展。从游戏到机器人领域的各种突破激发了对设计复杂RL算法和系统的兴趣。然而，当前RL的主流工作流程是从头学习，这可能导致计算效率低下。这阻碍了RL算法的持续部署，也可能排除了没有大规模计算资源的研究人员。在许多其他机器学习领域，预训练范式已被证明在获取可迁移知识方面有效，这些知识可以用于各种下游任务。最近，我们看到对深度RL预训练的兴趣激增，并取得了令人鼓舞的结果。然而，大部分研究都是基于不同的实验设置。由于RL的特性，这一领域的预训练面临独特的挑战，因此需要新的设计原则。在本综述中，我们旨在系统地回顾深度强化学习中预训练的现有工作，提供这些方法的分类，讨论各子领域，并关注未解问题和未来方向。
- en: 1 Introduction
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Reinforcement learning (RL) provides a general-purpose mathematical formalism
    for sequential decision-making (?). By utilizing RL algorithms together with deep
    neural networks, various milestones in different domains have achieved superhuman
    performances via optimizing user-specified reward functions in a data-driven manner (?, ?, ?, ?, ?, ?, ?).
    As such, we have seen a growing interest recently in this research direction.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）为序列决策提供了一种通用的数学形式(?). 通过将RL算法与深度神经网络结合，利用数据驱动的方式优化用户指定的奖励函数，在不同领域取得了超越人类的表现(?，?，?，?，?，?，?，?).
    因此，最近我们见证了这一研究方向的日益关注。
- en: However, while RL has been proven effective at solving well-specified tasks,
    the issue of sample efficiency (?) and generalization (?) still hinder its application
    to real-world problems. In RL research, a standard paradigm is to let the agent
    learn from its own or others’ collected experience, usually on a single task,
    and to optimize neural networks tabula rasa with random initializations. For humans,
    in contrast, prior knowledge about the world contributes greatly to the decision-making
    process. If the task is related to previously seen tasks, humans tend to reuse
    what has been learned to quickly adapt to a new task, without learning from exhaustive
    interactions from scratch. Therefore, as compared to humans, RL agents usually
    suffer from great data inefficiency (?) and are prone to overfitting (?).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，虽然 RL 在解决明确任务方面已被证明有效，但样本效率（?）和泛化（?）的问题仍然阻碍其在现实世界问题中的应用。在 RL 研究中，标准的范式是让智能体从自己或他人收集的经验中学习，通常是在单一任务上，并且在随机初始化的情况下优化神经网络。相比之下，人类的世界先验知识对决策过程有很大贡献。如果任务与以前见过的任务相关，人类倾向于重用已学到的知识，快速适应新任务，而不需要从头开始进行详尽的交互学习。因此，与人类相比，RL
    智能体通常遭受数据低效（?）和容易过拟合（?）的问题。
- en: 'Recent advances in other areas of machine learning, however, actively advocate
    for leveraging prior knowledge built from large-scale pretraining. By training
    on broad data at scale, large generic models, also known as foundation models (?),
    can quickly adapt to various downstream tasks. This pretrain-finetune paradigm
    has been proven effective in areas like computer vision (?, ?, ?) and natural
    language processing (?, ?). However, pretraining has not yet had a significant
    impact on the field of RL. Despite its promise, designing principles for large-scale
    RL pretraining faces challenges from many sources: 1) the diversity of domains
    and tasks; 2) the limited data sources; 3) the difficulty of fast adaptation to
    solve downstream tasks. These factors stem from the nature of RL and are inevitably
    necessary to be considered.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，机器学习其他领域的最新进展积极倡导利用从大规模预训练中获得的先验知识。通过在广泛的数据上进行大规模训练，大型通用模型，也称为基础模型（?），可以快速适应各种下游任务。这种预训练-微调的范式已在计算机视觉（?，?，?）和自然语言处理（?，?）等领域证明了其有效性。然而，预训练尚未对
    RL 领域产生显著影响。尽管有前景，设计大规模 RL 预训练的原则面临许多挑战：1) 领域和任务的多样性；2) 数据源有限；3) 快速适应以解决下游任务的困难。这些因素源于
    RL 的本质，不可避免地需要考虑。
- en: '![Refer to caption](img/6901517bf3c2e824c04e1f01c3f1757f.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6901517bf3c2e824c04e1f01c3f1757f.png)'
- en: 'Figure 1: An illustrating example of the RL pretraining pipeline.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：展示了 RL 预训练管道的示例。
- en: This survey aims to present a bird’s-eye view of current research on Pretraining
    in Deep RL. Principled pretraining in RL has a variety of potential benefits.
    First of all, the substantial computational cost incurred by RL training remains
    a hurdle for industrial applications. For example, replicating the results of
    AlphaStar (?) approximately costs millions of dollars (?). Pretraining can ameliorate
    this issue, either with pretrained world models (?) or pretrained representations (?),
    by enabling quick adaptation to solve tasks in zero or few-shot manner. Besides,
    RL is notoriously task- and domain-specific. It has already been shown that pretraining
    with massive task-agnostic data can enhance these kinds of generalizations (?).
    Finally, we believe that pretraining with proper architectures can unlock the
    power of scaling laws (?), as shown by recent success in games (?, ?). By scaling
    up general-purpose models with increased computation, we are able to further achieve
    superhuman results, as taught in the “bitter lesson” (?).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本综述旨在呈现对深度 RL 中预训练当前研究的全景视图。在 RL 中有原则的预训练具有多种潜在好处。首先，RL 训练所带来的巨大的计算成本仍然是工业应用的障碍。例如，复制
    AlphaStar（?）的结果大约需要数百万美元（?）。预训练可以缓解这个问题，无论是通过预训练的世界模型（?）还是预训练的表征（?），都能实现快速适应以零样本或少样本的方式解决任务。此外，RL
    以任务和领域特定而闻名。已经显示出，使用大量任务无关的数据进行预训练可以增强这些类型的泛化（?）。最后，我们相信，通过适当的架构进行预训练可以解锁规模法则的力量（?），正如在游戏中最近的成功（?，?）所示。通过扩大通用模型的计算规模，我们能够进一步取得超越人类的结果，如“痛苦的教训”（?）所教导的那样。
- en: Pretraining in deep RL has undergone several breakthroughs in recent years.
    Naive pretraining with expert demonstrations, using supervised learning to predict
    the actions taken by experts, has been exhibited with the famed AlphaGo (?). To
    pursue large-scale pretraining with less supervision, the field of unsupervised
    RL has been growing rapidly in recent years (?, ?), which allows the agent to
    learn from interacting with the environment in the absence of reward signals.
    In accordance with recent advances in offline RL (?), researchers further consider
    how to leverage unlabeled and sub-optimal offline data for pretraining (?, ?),
    which we term offline pretraining. The offline paradigm with task-irrelevant data
    further paves the way towards generalist pretraining, where diverse datasets from
    different tasks and modalities as well as general-purpose models with great scaling
    properties are combined to build generalist models (?, ?).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 深度强化学习中的预训练近年来经历了几个突破。使用专家演示的简单预训练，利用监督学习来预测专家采取的行动，已经在著名的AlphaGo中展示过（?）。为了追求大规模的低监督预训练，近年来无监督强化学习领域迅速发展（?,
    ?），这使得智能体可以在没有奖励信号的情况下通过与环境交互进行学习。根据最近在离线强化学习方面的进展（?），研究人员进一步考虑如何利用未标记的和次优的离线数据进行预训练（?,
    ?），我们称之为离线预训练。任务无关的数据的离线范式进一步铺平了通用预训练的道路，其中来自不同任务和模态的多样化数据集以及具有良好扩展性的通用模型被结合起来，以构建通用模型（?,
    ?）。
- en: Pretraining has the potential to play a big role for RL and this survey could
    serve as a starting point for those interested in this direction. In this paper,
    we seek to provide a systematic review of existing works in pretraining for deep
    reinforcement learning. To the best of our knowledge, it is one of the pioneering
    efforts to systematically study pretraining in deep RL.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练在强化学习中有着巨大的潜力，这项调查可以作为对这一方向感兴趣的人的起点。在本文中，我们力求系统地回顾现有的深度强化学习预训练工作。根据我们的最佳知识，这是对深度强化学习中的预训练进行系统研究的开创性努力之一。
- en: 'Following the development trend of pretraining in RL, we organize the paper
    as follows. After going through the preliminaries of reinforcement learning and
    pretraining (Section [2](#S2 "2 Preliminaries ‣ Pretraining in Deep Reinforcement
    Learning: A Survey")), we start with online pretraining in which an agent learns
    from interacting with the environment without reward signals (Section [3](#S3
    "3 Online Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey")).
    And then, we consider offline pretraining, the scenario where unlabeled training
    data is collected once with any policy (Section [4](#S4 "4 Offline Pretraining
    ‣ Pretraining in Deep Reinforcement Learning: A Survey")). In Section [5](#S5
    "5 Towards Generalist Agents with RL ‣ Pretraining in Deep Reinforcement Learning:
    A Survey"), we discuss recent advances in developing generalist agents for a variety
    of orthogonal tasks. We further discuss how to adapt to downstream RL tasks (Section [6](#S6
    "6 Task Adaptation ‣ Pretraining in Deep Reinforcement Learning: A Survey")).
    Finally, we conclude this survey together with a few prospects (Section [7](#S7
    "7 Conclusions and Future Perspectives ‣ Pretraining in Deep Reinforcement Learning:
    A Survey")).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '根据强化学习中预训练的发展趋势，我们将论文组织如下。在经过强化学习和预训练的初步介绍（第[2](#S2 "2 Preliminaries ‣ Pretraining
    in Deep Reinforcement Learning: A Survey")节）后，我们从在线预训练开始，其中一个智能体通过与环境交互来学习，而没有奖励信号（第[3](#S3
    "3 Online Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey")节）。然后，我们考虑离线预训练，即通过任何策略一次性收集未标记的训练数据的场景（第[4](#S4
    "4 Offline Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey")节）。在第[5](#S5
    "5 Towards Generalist Agents with RL ‣ Pretraining in Deep Reinforcement Learning:
    A Survey")节，我们讨论了在各种正交任务中开发通用智能体的最新进展。我们进一步讨论了如何适应下游的强化学习任务（第[6](#S6 "6 Task Adaptation
    ‣ Pretraining in Deep Reinforcement Learning: A Survey")节）。最后，我们总结了这项调查，并展望了一些前景（第[7](#S7
    "7 Conclusions and Future Perspectives ‣ Pretraining in Deep Reinforcement Learning:
    A Survey")节）。'
- en: 2 Preliminaries
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 初步介绍
- en: '|  Notation | Description |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  符号 | 描述 |'
- en: '| --- | --- |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| $\mathcal{M}$ | Markov decision process |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{M}$ | 马尔可夫决策过程 |'
- en: '| $\mathcal{S}$ | State space |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{S}$ | 状态空间 |'
- en: '| $\mathcal{A}$ | Action space |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{A}$ | 行动空间 |'
- en: '| $\mathcal{T}$ | Transition function |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{T}$ | 转移函数 |'
- en: '| $\rho_{0}$ | Initial state distribution |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| $\rho_{0}$ | 初始状态分布 |'
- en: '| $r$ | Reward function |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| $r$ | 奖励函数 |'
- en: '| $\gamma$ | Discount factor |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma$ | 折扣因子 |'
- en: '| $\mathcal{D}$ | Offline dataset |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{D}$ | 离线数据集 |'
- en: '| $\tau$ | Trajectory |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| $\tau$ | 轨迹 |'
- en: '| $Q$ | Q function |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| $Q$ | Q函数 |'
- en: '| $J$ | Expected total discounted reward function |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| $J$ | 预期总折扣奖励函数 |'
- en: '| $\theta$ | Neural network parameters |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| $\theta$ | 神经网络参数 |'
- en: '| $\phi$ | Feature encoder |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| $\phi$ | 特征编码器 |'
- en: '| $z$ | Skill latent vector |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| $z$ | 技能潜在向量 |'
- en: '| $\mathcal{Z}$ | Skill latent space |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| $\mathcal{Z}$ | 技能潜在空间 |'
- en: '| $H$ | Entropy |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| $H$ | 熵 |'
- en: '| $I$ | Mutual information |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| $I$ | 互信息 |'
- en: '|   |  |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '|   |  |'
- en: 'Table 1: Notations used in the survey.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：调查中使用的符号。
- en: 2.1 Reinforcement learning
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 强化学习
- en: Reinforcement learning considers the problem of finding a policy that interacts
    with the environment under uncertainty to maximize its collected reward. Mathematically,
    this problem can be formulated via a Markov Decision Process (MDP) defined by
    tuple ($\mathcal{S}$, $\mathcal{A}$, $\mathcal{T}$, $\rho_{0}$, $r$, $\gamma$),
    with a state space $\mathcal{S}$, an action space $\mathcal{A}$, a state transition
    distribution $\mathcal{T}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$,
    an initial state distribution $\rho_{0}:\mathcal{S}\rightarrow[0,1]$, a reward
    function $r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$, and a discount
    factor $\gamma\in(0,1)$. The objective is to find such a policy ${\pi_{\theta}\left(a|s\right)}$
    parameterized by $\theta$ that maximizes
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习考虑了在不确定性下与环境交互以最大化收集的奖励的策略问题。数学上，这个问题可以通过一个马尔可夫决策过程（MDP）来公式化，该过程由元组（$\mathcal{S}$,
    $\mathcal{A}$, $\mathcal{T}$, $\rho_{0}$, $r$, $\gamma$）定义，其中包括状态空间$\mathcal{S}$、动作空间$\mathcal{A}$、状态转移分布$\mathcal{T}:\mathcal{S}\times\mathcal{A}\times\mathcal{S}\rightarrow[0,1]$、初始状态分布$\rho_{0}:\mathcal{S}\rightarrow[0,1]$、奖励函数$r:\mathcal{S}\times\mathcal{A}\rightarrow\mathbb{R}$和折扣因子$\gamma\in(0,1)$。目标是找到一个由$\theta$参数化的策略${\pi_{\theta}\left(a|s\right)}$，使其最大化
- en: '|  | $J(\pi_{\theta})=\mathbb{E}_{\pi_{\theta},\mathcal{T},\rho_{0}}\left[\sum_{t=0}^{\infty}\gamma^{t}r\left(s_{t},a_{t}\right)\right],$
    |  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  | $J(\pi_{\theta})=\mathbb{E}_{\pi_{\theta},\mathcal{T},\rho_{0}}\left[\sum_{t=0}^{\infty}\gamma^{t}r\left(s_{t},a_{t}\right)\right],$
    |  |'
- en: 'known as the discounted returns. The notation used in the paper is summarized
    in Table [1](#S2.T1 "Table 1 ‣ 2 Preliminaries ‣ Pretraining in Deep Reinforcement
    Learning: A Survey").'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '被称为折扣回报。论文中使用的符号总结见表 [1](#S2.T1 "Table 1 ‣ 2 Preliminaries ‣ Pretraining in
    Deep Reinforcement Learning: A Survey")。'
- en: 2.2 Pretraining
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 预训练
- en: Pretraining aims at obtaining transferable knowledge from large-scale training
    data to facilitate downstream tasks. In the context of RL, transferable knowledge
    typically includes good representations that facilitate the agent to perceive
    the world (i.e., a better state space) and reusable skills from which the agent
    can quickly build complex behaviors given task descriptions (i.e., a better action
    space). Training data can be one bottleneck for effective RL pretraining. Unlike
    what we have witnessed in fields like computer vision and natural language processing
    where a wealth of unlabeled data can be collected with minimal supervision, RL
    usually requires highly task-specific reward design, which hinders scaling up
    pretraining for large-scale applications.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练的目标是从大规模训练数据中获得可迁移的知识，以促进下游任务。在强化学习的背景下，可迁移的知识通常包括有助于智能体感知世界的良好表示（即，更好的状态空间）和从中智能体可以根据任务描述迅速构建复杂行为的可重用技能（即，更好的动作空间）。训练数据可能是有效强化学习预训练的一个瓶颈。与我们在计算机视觉和自然语言处理等领域看到的情况不同，在这些领域可以收集大量的未标记数据且监督成本很低，强化学习通常需要高度特定任务的奖励设计，这阻碍了大规模应用的预训练扩展。
- en: 'Therefore, the focus of this survey is unsupervised pretraining, in which task-specific
    rewards are unavailable during pretraining but it is still allowed to learn from
    online interaction, unlabeled logged data, or task-irrelevant data from other
    modalities. We omit supervised pretraining given that with task-specific rewards
    this scenario roughly degenerates to existing RL settings (?). Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Pretraining in Deep Reinforcement Learning: A Survey")
    demonstrates an overview of the pretraining and adaptation process.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，本次调查的重点是无监督预训练，其中在预训练期间没有特定任务的奖励，但仍允许从在线交互、未标记的日志数据或来自其他模态的与任务无关的数据中学习。我们省略了监督预训练，因为有了特定任务的奖励，这种情况大致退化为现有的强化学习设置（？）。图 [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Pretraining in Deep Reinforcement Learning: A Survey")
    展示了预训练和适应过程的概述。'
- en: 'The objective is to acquire useful prior knowledge in various forms like good
    visual representations, exploratory policies ${\pi_{\theta}\left(a|s\right)}$,
    latent-conditioned policies ${\pi\left(a|s,z\right)}$, or simply logged datasets.
    Depending on what data is available during pretraining, it requires different
    considerations to obtain useful knowledge (Section [3](#S3 "3 Online Pretraining
    ‣ Pretraining in Deep Reinforcement Learning: A Survey")-[5](#S5 "5 Towards Generalist
    Agents with RL ‣ Pretraining in Deep Reinforcement Learning: A Survey")) and adapt
    it accordingly to downstream tasks (Section [6](#S6 "6 Task Adaptation ‣ Pretraining
    in Deep Reinforcement Learning: A Survey")).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是以各种形式获取有用的先验知识，如良好的视觉表示、探索策略 ${\pi_{\theta}\left(a|s\right)}$、潜在条件策略 ${\pi\left(a|s,z\right)}$，或简单的日志数据集。根据在预训练期间可用的数据，需要不同的考虑以获取有用的知识（第
    [3](#S3 "3 在线预训练 ‣ 深度强化学习中的预训练：综述")-[5](#S5 "5 追求通用智能体与强化学习 ‣ 深度强化学习中的预训练：综述")
    节）并相应地将其调整为下游任务（第 [6](#S6 "6 任务适应 ‣ 深度强化学习中的预训练：综述") 节）。
- en: '|  Type | Algorithm | Intrinsic Reward | Visual |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|  类型 | 算法 | 内在奖励 | 视觉 |'
- en: '| Curiosity-driven Exploration | ICM (?) | $r_{t}\propto\left\&#124;f\left(\phi\left(s_{t}\right),a_{t}\right)-\phi\left(s_{t+1}\right)\right\&#124;^{2}$
    | ✓ |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 好奇心驱动探索 | ICM (?) | $r_{t}\propto\left\|f\left(\phi\left(s_{t}\right),a_{t}\right)-\phi\left(s_{t+1}\right)\right\|^{2}$
    | ✓ |'
- en: '| RND (?) | $r_{t}\propto\left\&#124;f\left(\phi\left(s_{t}\right),a_{t}\right)-\phi\left(s_{t+1}\right)\right\&#124;^{2}$
    | ✓ |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| RND (?) | $r_{t}\propto\left\|f\left(\phi\left(s_{t}\right),a_{t}\right)-\phi\left(s_{t+1}\right)\right\|^{2}$
    | ✓ |'
- en: '| Disagreement (?) | $r_{t}\propto\operatorname{Var}\left(f\left(\phi\left(s_{t}\right),a_{t}\right)\right)$
    | ✓ |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| Disagreement (?) | $r_{t}\propto\operatorname{Var}\left(f\left(\phi\left(s_{t}\right),a_{t}\right)\right)$
    | ✓ |'
- en: '| Plan2Explore (?) | $r_{t}\propto\operatorname{Var}\left(f\left(\phi\left(s_{t}\right),a_{t}\right)\right)$
    | ✓ |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| Plan2Explore (?) | $r_{t}\propto\operatorname{Var}\left(f\left(\phi\left(s_{t}\right),a_{t}\right)\right)$
    | ✓ |'
- en: '| Skill Discovery | VIC (?) | $r\propto\log q\left(z\mid\phi(s_{H})\right)-\log
    p(z)$ | ✓ |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 技能发现 | VIC (?) | $r\propto\log q\left(z\mid\phi(s_{H})\right)-\log p(z)$
    | ✓ |'
- en: '| VALOR (?) | $r\propto\log q\left(z\mid s_{1:H}\right)-\log p(z)$ | ✗ |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| VALOR (?) | $r\propto\log q\left(z\mid s_{1:H}\right)-\log p(z)$ | ✗ |'
- en: '| DIAYN (?) | $r_{t}\propto\log q\left(z\mid s_{t}\right)-\log p(z)$ | ✗ |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| DIAYN (?) | $r_{t}\propto\log q\left(z\mid s_{t}\right)-\log p(z)$ | ✗ |'
- en: '| VISR (?) | $r_{t}\propto\log q\left(z\mid\phi(s_{t})\right)-\log p(z)$ |
    ✓ |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| VISR (?) | $r_{t}\propto\log q\left(z\mid\phi(s_{t})\right)-\log p(z)$ |
    ✓ |'
- en: '| DADS (?) | $r_{t}\propto\log q\left(s_{t+1}\mid s_{t},z\right)-\log q\left(s_{t+1}\mid
    s_{t}\right)$ | ✗ |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| DADS (?) | $r_{t}\propto\log q\left(s_{t+1}\mid s_{t},z\right)-\log q\left(s_{t+1}\mid
    s_{t}\right)$ | ✗ |'
- en: '| EDL (?) | $r_{t}\propto\log q\left(s_{t}\mid z\right)$ | ✗ |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| EDL (?) | $r_{t}\propto\log q\left(s_{t}\mid z\right)$ | ✗ |'
- en: '| APS (?) | $r_{t}\propto\log q\left(s_{t}\mid z\right)+\sum_{i\in\mathcal{I}_{\text{random}}}\log\left\&#124;\phi(s_{t})-h_{i}\right\&#124;$
    | ✓ |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| APS (?) | $r_{t}\propto\log q\left(s_{t}\mid z\right)+\sum_{i\in\mathcal{I}_{\text{random}}}\log\left\|
    \phi(s_{t})-h_{i}\right\|$ | ✓ |'
- en: '| HIDIO (?) | $r_{t}\propto\log q(z\mid a_{t-k+1:t},s_{t-k:t})$ | ✗ |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| HIDIO (?) | $r_{t}\propto\log q(z\mid a_{t-k+1:t},s_{t-k:t})$ | ✗ |'
- en: '| UPSIDE (?) | $r_{t}\propto\log q(z\mid s_{t})-\log p(z)$ | ✗ |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| UPSIDE (?) | $r_{t}\propto\log q(z\mid s_{t})-\log p(z)$ | ✗ |'
- en: '| LSD (?) | $r_{t}\propto\left(\phi\left(s_{t+1}\right)-\phi\left(s_{t}\right)\right)^{\top}z$
    | ✗ |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| LSD (?) | $r_{t}\propto\left(\phi\left(s_{t+1}\right)-\phi\left(s_{t}\right)\right)^{\top}z$
    | ✗ |'
- en: '| Data Coverage Maximization | CBB (?) | $r_{t}\propto\hat{N}(s_{t})^{-\frac{1}{2}}$
    | ✗ |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 数据覆盖最大化 | CBB (?) | $r_{t}\propto\hat{N}(s_{t})^{-\frac{1}{2}}$ | ✗ |'
- en: '| MaxEnt (?) | $r_{t}\propto\nabla R\left(\hat{d}_{\pi_{t}}\right)$ | ✗ |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| MaxEnt (?) | $r_{t}\propto\nabla R\left(\hat{d}_{\pi_{t}}\right)$ | ✗ |'
- en: '| SMM (?) | $r_{t}\propto\log\hat{p}(s_{t})-\log p_{\pi}(s_{t})$ | ✗ |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| SMM (?) | $r_{t}\propto\log\hat{p}(s_{t})-\log p_{\pi}(s_{t})$ | ✗ |'
- en: '| APT (?) | $r_{t}\propto\sum_{i\in\mathcal{I}_{\text{random}}}\log\left\&#124;\phi(s_{t})-h_{i}\right\&#124;$
    | ✓ |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| APT (?) | $r_{t}\propto\sum_{i\in\mathcal{I}_{\text{random}}}\log\left\|
    \phi(s_{t})-h_{i}\right\|$ | ✓ |'
- en: '| Proto-RL (?) | $r_{t}\propto\sum_{i\in\mathcal{I}_{\text{prototype}}}\log\left\&#124;\phi(s_{t})-h_{i}\right\&#124;$
    | ✓ |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| Proto-RL (?) | $r_{t}\propto\sum_{i\in\mathcal{I}_{\text{prototype}}}\log\left\|
    \phi(s_{t})-h_{i}\right\|$ | ✓ |'
- en: '| RE3 (?) | $r_{t}\propto\log\left(\left\&#124;\phi(s_{t})-\operatorname{KNN}\left(\phi(s_{t})\right)\right\&#124;+1\right)$
    | ✓ |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| RE3 (?) | $r_{t}\propto\log\left(\left\| \phi(s_{t})-\operatorname{KNN}\left(\phi(s_{t})\right)\right\|+1\right)$
    | ✓ |'
- en: '|   |  |  |  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |'
- en: 'Table 2: Categorization of representative online pretraining approaches.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：具有代表性的在线预训练方法的分类。
- en: 3 Online Pretraining
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 在线预训练
- en: Most of the previous successes in RL have been achieved given dense and well-designed
    reward functions. Despite its primacy in providing excel performances for a specific
    task, the traditional RL paradigm faces two critical challenges when scaling it
    up to large-scale pretraining. Firstly, it is notoriously easy for an RL agent
    to overfit (?). As a result, a pretrained agent trained with sophisticated task
    rewards can hardly generalize to unseen task specifications. Furthermore, it remains
    a practical challenge to design reward functions which is usually costly and requires
    expert knowledge.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以往在强化学习中的大部分成功都是在密集且设计良好的奖励函数的基础上取得的。尽管这种方法在提供特定任务的卓越表现方面具有首要地位，但传统的强化学习范式在扩展到大规模预训练时面临两个关键挑战。首先，强化学习代理容易过拟合（?）。因此，用复杂任务奖励训练的预训练代理很难推广到未见过的任务规格。此外，设计奖励函数仍然是一个实际挑战，通常成本高且需要专家知识。
- en: Online pretraining without these reward signals can potentially be a good solution
    to learning generic skills and eliminate the supervision requirement. Online pretraining
    aims at acquiring prior knowledge by interacting with the environment in the absence
    of human supervision. During the pretraining phase, the agent is allowed to interact
    with the environment for a long period without access to extrinsic rewards. When
    the environment is accessible, playing with it facilitates skill learning that
    will be useful later when a task is assigned to the agent. This solution, also
    known as unsupervised RL, has been actively studied in recent years (?, ?).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有这些奖励信号的情况下进行在线预训练可能是学习通用技能并消除监督需求的一个好解决方案。在线预训练旨在通过在没有人工监督的情况下与环境互动来获得先验知识。在预训练阶段，代理可以长时间与环境互动而不接触外在奖励。当环境可访问时，与之互动有助于技能学习，这些技能在任务分配给代理时会派上用场。这个解决方案，也称为无监督强化学习，近年来得到了积极研究（?,
    ?）。
- en: To encourage the agent to build its own knowledge without any supervision, we
    need principled mechanisms to provide the agent with intrinsic drives. Psychologists
    found that babies can discover both the tasks to be learned and the solution to
    those tasks through interacting with the environment (?). With experiences accumulated,
    they are capable of more difficult tasks later on. This motivates a wealth of
    research that studies how to build self-taught agents with intrinsic rewards (?, ?, ?).
    Intrinsic rewards, in contrast to task-specifying extrinsic rewards, refer to
    general learning signals that encourage the agent either to collect diverse experiences
    or to develop useful skills. It has been shown that pretraining an agent with
    intrinsic rewards and standard RL algorithms can lead to fast adaptation once
    the downstream task is given (?).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励代理在没有任何监督的情况下建立自己的知识，我们需要原则性机制来提供内在驱动力。心理学家发现，婴儿可以通过与环境互动发现需要学习的任务以及这些任务的解决方案（?）。随着经验的积累，他们能够处理更困难的任务。这激发了大量研究，探讨如何通过内在奖励构建自学型代理（?,
    ?, ?）。与任务特定的外在奖励相比，内在奖励指的是鼓励代理收集多样化经验或发展有用技能的一般学习信号。研究表明，使用内在奖励和标准强化学习算法对代理进行预训练，可以在下游任务给定后实现快速适应（?）。
- en: 'Based on how to design intrinsic rewards, we classify existing approaches of
    unsupervised RL into three categories¹¹1This taxonomy of unsupervised RL was originally
    proposed by ? (?).: curiosity-driven exploration, skill discovery, and maximal
    data coverage. Table [2](#S2.T2 "Table 2 ‣ 2.2 Pretraining ‣ 2 Preliminaries ‣
    Pretraining in Deep Reinforcement Learning: A Survey") presents a categorization
    of representative online pretraining algorithms together with their used intrinsic
    rewards.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '根据如何设计内在奖励，我们将现有的无监督强化学习方法分为三类¹¹1此分类法最初由?（?）提出：好奇心驱动探索、技能发现和最大数据覆盖。表[2](#S2.T2
    "Table 2 ‣ 2.2 Pretraining ‣ 2 Preliminaries ‣ Pretraining in Deep Reinforcement
    Learning: A Survey")展示了代表性在线预训练算法的分类及其使用的内在奖励。'
- en: 3.1 Curiosity-driven Exploration
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 好奇心驱动的探索
- en: '![Refer to caption](img/a16e886e5bcfb4c2c459a64aff40b1e6.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a16e886e5bcfb4c2c459a64aff40b1e6.png)'
- en: 'Figure 2: The process of computing intrinsic rewards using curiosity-driven
    exploration approaches.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：使用好奇心驱动探索方法计算内在奖励的过程。
- en: 'In the psychology of motivation, curiosity represents motivation to reduce
    uncertainty about the world (?). Inspired by this line of psychological theory,
    similar ideas have been studied to build curiosity-driven approaches for online
    pretraining. Curiosity-driven approaches seek to explore interesting states that
    can possibly bring knowledge about the environment. Intuitively, if the agent
    falls short of accurately predicting the environment, it gains knowledge by interacting
    and then reducing this part of the uncertainty. The defining characteristic of
    a curiosity-driven agent is how to compute the degree of curiosity to these interesting
    states, which directly serves as the intrinsic reward for learning. A concrete
    example is ICM (?), which applies the intrinsic reward proportional to the prediction
    error as shown in Figure [2](#S3.F2 "Figure 2 ‣ 3.1 Curiosity-driven Exploration
    ‣ 3 Online Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey"):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在动机心理学中，好奇心代表了减少对世界的不确定性的动机（?）。受到这一心理学理论的启发，类似的思想被用来构建以好奇心驱动的在线预训练方法。好奇心驱动的方法试图探索有趣的状态，这些状态可能带来关于环境的知识。直观地说，如果智能体在准确预测环境方面存在不足，它通过交互获取知识，然后减少这部分的不确定性。好奇心驱动的智能体的定义特征是如何计算对这些有趣状态的好奇心程度，这直接作为学习的内在奖励。一个具体的例子是ICM（?），它应用与预测误差成比例的内在奖励，如图 [2](#S3.F2
    "图 2 ‣ 3.1 好奇心驱动的探索 ‣ 3 在线预训练 ‣ 深度强化学习中的预训练：综述")所示：
- en: '|  | $r_{t}\propto\left\&#124;f\left(\phi\left(s_{t}\right),a_{t}\right)-\phi\left(s_{t+1}\right)\right\&#124;_{2}^{2},$
    |  |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{t}\propto\left\&#124;f\left(\phi\left(s_{t}\right),a_{t}\right)-\phi\left(s_{t+1}\right)\right\&#124;_{2}^{2},$
    |  |'
- en: where $f$ and $\phi$ represent the learned forward dynamics model and feature
    encoder, respectively.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 和 $\phi$ 分别表示学习到的前向动态模型和特征编码器。
- en: 'To measure curiosity, a broad class of approaches (?, ?) leverages this kind
    of learned dynamics models to predict future states in an auxiliary feature space.
    there are mainly two kinds of estimation: prediction error and prediction uncertainty.
    Despite that these dynamics-based approaches perform quite well across common
    scenarios, they usually suffer from action-dependent noisy TVs (?), which will
    be discussed later in Section [3.1.1](#S3.SS1.SSS1 "3.1.1 Challenges & Future
    Directions ‣ 3.1 Curiosity-driven Exploration ‣ 3 Online Pretraining ‣ Pretraining
    in Deep Reinforcement Learning: A Survey"). This deficiency encourages the following
    work to design dynamics-free curiosity estimation (?) and more sophisticated uncertainty
    estimation methods (?, ?).'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量好奇心，广泛的研究方法（?, ?）利用这种学习到的动态模型在辅助特征空间中预测未来状态。主要有两种估计：预测误差和预测不确定性。尽管这些基于动态的方法在常见场景中表现良好，但它们通常受到依赖于动作的噪声TV（?）的影响，这将在第 [3.1.1](#S3.SS1.SSS1
    "3.1.1 挑战与未来方向 ‣ 3.1 好奇心驱动的探索 ‣ 3 在线预训练 ‣ 深度强化学习中的预训练：综述")节中讨论。这一缺陷促使后续工作设计无动态的好奇心估计（?）和更复杂的不确定性估计方法（?,
    ?）。
- en: Another important design choice is associated with the feature encoder $\phi$,
    especially for high-dimensional observations. A proper feature encoder can make
    the prediction task more tractable and filter out irrelevant aspects so that the
    agent can only focus on the informative ones. Early studies (?, ?) leverage auto-encoding
    embeddings to recover the original high-dimensional inputs, but the induced feature
    space is usually too informative about irrelevant details and hence susceptible
    to noise. To address this issue, ? (?) utilize an inverse dynamics model for feature
    encoding to make sure that the agent is unaffected by nuisance factors in the
    environment. The proposed ICM shows impressive zero-shot performance in playing
    video games. ? (?) further relax the design burden by simply replacing the feature
    model with a fixed randomly initialized neural network, which is proven effective
    by a following large-scale empirical study (?). Despite that random feature encoders
    are sufficient for good performance at training, learned features (e.g., based
    on inverse dynamics) generalize better (?). Inspired by recent advances in representation
    learning, ? (?) directly link curiosity and representation learning loss by formulating
    a minimax game between a generic representation learning algorithm and a reinforcement
    learning policy.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的设计选择涉及特征编码器 $\phi$，特别是对于高维观察数据。一个合适的特征编码器可以使预测任务更易处理，并过滤掉无关方面，使代理仅关注有用的信息。早期研究（？，？）利用自编码嵌入来恢复原始高维输入，但引导的特征空间通常过于关注无关细节，因此容易受到噪声影响。为了解决这个问题，？（？）利用逆动态模型进行特征编码，以确保代理不受环境中干扰因素的影响。所提出的ICM在玩视频游戏时展示了令人印象深刻的零样本性能。？（？）通过简单地将特征模型替换为固定的随机初始化神经网络进一步减轻了设计负担，随后的大规模实证研究（？）证明了这一方法的有效性。尽管随机特征编码器在训练中足以取得良好表现，但学习的特征（例如，基于逆动态）具有更好的泛化能力（？）。受到最近表征学习进展的启发，？（？）通过将好奇心和表征学习损失直接联系起来，构建了一个泛化表征学习算法与强化学习策略之间的最小最大博弈。
- en: 3.1.1 Challenges & Future Directions
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1 挑战与未来方向
- en: This kind of approach has several deficiencies. One of the most important issues
    is how to distinguish epistemic and aleatoric uncertainty. Epistemic uncertainty
    refers to uncertainty caused by a lack of knowledge. Aleatoric uncertainty, in
    contrast, refers to the variability in the outcome due to inherently random effects.
    A concrete phenomenon in RL is the noisy TV problem (?), which refers to the cases
    where the agent gets trapped by its curiosity in highly stochastic environments.
    To mitigate this issue, some work attempts to use intrinsic rewards proportional
    to a reduction in uncertainty (?, ?). However, tractable epistemic uncertainty
    estimation in high dimension remains challenging (?) due to its sensitivity to
    imperfect data.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法存在几个缺陷。其中一个最重要的问题是如何区分认识不确定性和随机不确定性。认识不确定性指的是由于知识不足而产生的不确定性。而随机不确定性则是指由于固有的随机效应导致结果的变异。在强化学习中的一个具体现象是嘈杂的电视问题（？），即代理由于在高度随机的环境中被好奇心困住。为了减轻这个问题，一些工作尝试使用与不确定性减少成正比的内在奖励（？，？）。然而，高维度下可处理的认识不确定性估计仍然具有挑战性（？），因为它对不完美数据的敏感性。
- en: Another issue with the above approaches is that they only receive retrospective
    signals after the agent has achieved epistemic uncertainty, which might cause
    inefficiency in exploration. Based on this intuition, ? (?) design a model-based
    method that can prospectively look for uncertainty in the environment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 上述方法的另一个问题是它们仅在代理达到认识不确定性后收到回顾性信号，这可能导致探索效率低下。基于这种直觉，？（？）设计了一种基于模型的方法，能够前瞻性地寻找环境中的不确定性。
- en: 3.2 Skill Discovery
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 技能发现
- en: '![Refer to caption](img/ac01d7461732f856e962e7bd5bd1f955.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/ac01d7461732f856e962e7bd5bd1f955.png)'
- en: 'Figure 3: The process of computing intrinsic rewards using skill discovery
    approaches.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：使用技能发现方法计算内在奖励的过程。
- en: Apart from curiosity-driven approaches that tackle unsupervised RL in a model-based
    perspective, one can also consider model-free learning of primitive skills²²2In
    this work, we use skill, option, and behavior prior interchangeably. that can
    be composed to solve downstream tasks. This kind of approach is usually referred
    to as skill discovery approach. The main intuition behind this is that the learned
    skill should control which states the agent visits, which can be seen as a notion
    of empowerment.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 除了从模型驱动的角度处理无监督强化学习的好奇心驱动方法外，还可以考虑无模型学习原始技能²²2在本工作中，我们将技能、选项和行为先验互换使用。来组合以解决下游任务。这种方法通常被称为技能发现方法。其主要直觉是，学习到的技能应该控制代理访问哪些状态，这可以看作是一种赋能的概念。
- en: 'Generally speaking, the objective for skill discovery can be formalized as
    maximizing the mutual information (MI) between skill latent variable $z$ and state
    $s$:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，技能发现的目标可以形式化为最大化技能潜变量$z$和状态$s$之间的互信息（MI）：
- en: '|  | $I(s;z)=H(z)-H(z\mid s)=H(s)-H(s\mid z),$ |  | (1) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $I(s;z)=H(z)-H(z\mid s)=H(s)-H(s\mid z),$ |  | (1) |'
- en: 'where we define skills or options as the policies conditioned on $z$. There
    are two components for a skill discovery agent to determine: 1) a skill distribution
    $p(z)$; 2) a skill policy ${\pi\left(a|s,z\right)}$. Before each episode, skill
    latent $z$ is sampled from distribution $p(z)$, followed by skill ${\pi\left(a|s,z\right)}$
    to interact with the environment. Learning skills that maximize MI is a challenging
    optimization problem, upon which a variety of approaches sharing the same spirit
    have been applied.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将技能或选项定义为以$z$为条件的策略。技能发现代理需要确定两个组件：1）技能分布$p(z)$；2）技能策略${\pi\left(a|s,z\right)}$。在每个回合之前，从分布$p(z)$中采样技能潜变量$z$，然后使用技能${\pi\left(a|s,z\right)}$与环境互动。学习最大化MI的技能是一个具有挑战性的优化问题，已经应用了多种共享相同精神的方法。
- en: 'Among the existing MI-based skill discovery methods, the majority (?, ?, ?)
    apply the former form of Equation [1](#S3.E1 "In 3.2 Skill Discovery ‣ 3 Online
    Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey") with the
    following variational lower bound (?):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '在现有的基于MI的技能发现方法中，大多数 (?, ?, ?) 应用方程式 [1](#S3.E1 "在 3.2 技能发现 ‣ 3 在线预训练 ‣ 深度强化学习中的预训练：综述")
    的前一种形式，并使用以下变分下界 (?):'
- en: '|  | $\displaystyle I(s;z)$ | $\displaystyle=\mathbb{E}_{s,z\sim p(s,z)}[\log
    p(z\mid s)]-\mathbb{E}_{z\sim p(z)}[\log p(z)]$ |  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I(s;z)$ | $\displaystyle=\mathbb{E}_{s,z\sim p(s,z)}[\log
    p(z\mid s)]-\mathbb{E}_{z\sim p(z)}[\log p(z)]$ |  |'
- en: '|  |  | $\displaystyle\geq\mathbb{E}_{s,z\sim p(s,z)}\left[\log q(z\mid s)\right]-\mathbb{E}_{z\sim
    p(z)}[\log p(z)].$ |  |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\geq\mathbb{E}_{s,z\sim p(s,z)}\left[\log q(z\mid s)\right]-\mathbb{E}_{z\sim
    p(z)}[\log p(z)].$ |  |'
- en: 'In this case, a parametric model $q(z\mid s)$ is trained together with other
    variables to estimate the conditional distribution $p(z\mid s)$. Maximizing $H(z)$
    can be achieved by sampling $z$ from a learned distribution (?) or directly from
    a fixed uniform distribution (?). As shown in Figure [3](#S3.F3 "Figure 3 ‣ 3.2
    Skill Discovery ‣ 3 Online Pretraining ‣ Pretraining in Deep Reinforcement Learning:
    A Survey"), the intrinsic reward is given by $r_{t}=\log q\left(z\mid s_{t}\right)-\log
    p(z)$, upon which one can apply standard RL algorithms to learn skills.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，参数模型$q(z\mid s)$与其他变量一起训练，以估计条件分布$p(z\mid s)$。通过从学习到的分布中采样$z$ (?)或直接从固定均匀分布中采样 (?)来实现最大化$H(z)$。如图 [3](#S3.F3
    "图 3 ‣ 3.2 技能发现 ‣ 3 在线预训练 ‣ 深度强化学习中的预训练：综述")所示，内在奖励由$r_{t}=\log q\left(z\mid s_{t}\right)-\log
    p(z)$给出，可以应用标准的RL算法来学习技能。
- en: 'Another line of research (?, ?, ?, ?) considers the latter form and similarly
    derives a lower bound:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 另一类研究 (?, ?, ?, ?) 考虑了后一种形式，并类似地推导出一个下界：
- en: '|  | $\displaystyle I(s;z)$ | $\displaystyle=\mathbb{E}_{s,z\sim p(s,z)}[\log
    p(s\mid z)]-\mathbb{E}_{s\sim p(s)}[\log p(s)]$ |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I(s;z)$ | $\displaystyle=\mathbb{E}_{s,z\sim p(s,z)}[\log
    p(s\mid z)]-\mathbb{E}_{s\sim p(s)}[\log p(s)]$ |  |'
- en: '|  |  | $\displaystyle\geq\mathbb{E}_{s,z\sim p(s,z)}\left[\log q(s\mid z)\right]-\mathbb{E}_{s\sim
    p(s)}[\log p(s)].$ |  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\geq\mathbb{E}_{s,z\sim p(s,z)}\left[\log q(s\mid z)\right]-\mathbb{E}_{s\sim
    p(s)}[\log p(s)].$ |  |'
- en: 'In this formulation, maximizing the state entropy $H(s)$ encourages exploration
    while minimizing the conditional entropy results in directed behaviors. The difficulty
    lies in the density estimation of $s$, especially for high-dimensional state spaces.
    A common practice is to maximize $H(s)$ via maximum entropy estimation (?, ?, ?),
    which will be elaborated more in Section [3.3](#S3.SS3 "3.3 Data Coverage Maximization
    ‣ 3 Online Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey").'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种公式化的表达中，最大化状态熵 $H(s)$ 以鼓励探索，同时最小化条件熵则会导致有针对性的行为。困难在于对 $s$ 的密度估计，尤其是对于高维状态空间。一种常见的做法是通过最大熵估计来最大化
    $H(s)$（?, ?, ?），这将在第 [3.3](#S3.SS3 "3.3 数据覆盖最大化 ‣ 3 在线预训练 ‣ 深度强化学习中的预训练：综述") 节中详细阐述。
- en: 'Although different work uses slightly different approaches to optimize Equation [1](#S3.E1
    "In 3.2 Skill Discovery ‣ 3 Online Pretraining ‣ Pretraining in Deep Reinforcement
    Learning: A Survey"), it could be more important to decide other design factors
    when using skill discovery for online pretraining. For instance, while most studies
    consider the episodic setting, some efforts have been made to extend MI-based
    skill discovery to non-episodic settings (?, ?). It is also promising to consider
    a curriculum with an increasing number of skills to learn (?). Several other factors
    are also worth mentioning, such as whether skill latent $z$ is discrete (?) or
    continuous (?), whether the reward signals are dense (?) or sparse (?), and whether
    it works for image-based observations (?).'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不同的工作使用略有不同的方法来优化方程 [1](#S3.E1 "在 3.2 技能发现 ‣ 3 在线预训练 ‣ 深度强化学习中的预训练：综述")，但在使用技能发现进行在线预训练时，决定其他设计因素可能更为重要。例如，虽然大多数研究考虑了情节设置，但也有一些努力将基于
    MI 的技能发现扩展到非情节设置（?, ?）。考虑到一个具有不断增加技能数目的课程也是有前景的（?）。还有几个其他因素值得提及，例如技能潜在变量 $z$ 是离散的（?）还是连续的（?），奖励信号是密集的（?）还是稀疏的（?），以及它是否适用于基于图像的观察（?）。
- en: Skill discovery can be also reinterpreted as goal-conditioned policy learning,
    where $z$ as self-generated and abstract goal is sampled from a distribution instead
    of provided by the task. One can also consider generating concrete goals in a
    self-supervised manner (?, ?) and derive a goal-conditioned reward function similarly
    from MI maximization. DISCERN (?) designs a non-parametric approach for goal sampling,
    maintaining a buffer of past observations that drifts as the agent collects new
    experiences. Skew-Fit (?) instead learns a maximum entropy goal distribution by
    increasing the entropy of a generative model in an iterative manner. ? (?) provide
    a more formal connection mainly from the perspective of goal-conditioned RL. We
    refer the interested reader to ? (?) for further discussion.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 技能发现也可以重新解释为目标条件的策略学习，其中 $z$ 作为自生成的抽象目标，从分布中采样，而不是由任务提供。还可以考虑以自监督的方式生成具体目标（?,
    ?），并类似地从 MI 最大化中推导出一个目标条件的奖励函数。DISCERN (?) 设计了一种非参数方法用于目标采样，保持一个随着代理收集新经验而漂移的过去观察的缓冲区。Skew-Fit
    (?) 则通过迭代方式增加生成模型的熵来学习最大熵目标分布。? (?) 从目标条件强化学习的角度提供了更正式的联系。我们建议感兴趣的读者参考 ? (?) 以获取更多讨论。
- en: 3.2.1 Challenges & Future Directions
  id: totrans-111
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1 挑战与未来方向
- en: A major issue for MI-based skill discovery approaches is that the objective
    does not necessarily lead to strong state coverage as one can maximize $I(s;z)$
    even with the smallest state variations (?, ?). This lack of coverage can greatly
    limit their applicability to downstream tasks with complex environments (?). To
    resolve this issue, some existing work explicitly uses $x$-$y$ coordinates as
    features to enforce state coverage induced by skills (?, ?). It is also explored
    to separate the learning process to first maximize $H(s)$ via maximum entropy
    estimation, followed by behavior learning (?, ?).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 MI 的技能发现方法的一个主要问题是，目标不一定会导致强大的状态覆盖，因为即使在最小的状态变化下也可以最大化 $I(s;z)$（?, ?）。这种覆盖不足会大大限制它们在复杂环境下的下游任务中的适用性（?）。为了解决这个问题，一些现有的工作明确使用
    $x$-$y$ 坐标作为特征来强制执行由技能引起的状态覆盖（?, ?）。也有探索将学习过程分为两个阶段，首先通过最大熵估计最大化 $H(s)$，然后进行行为学习（?,
    ?）。
- en: Moreover, it is empirically shown that skill discovery methods underperform
    other kinds of online pretraining methods, which may be due to restricted skill
    spaces (?). This calls attention to dissecting what skills are learned. In order
    to live up to their full potential, the discovered skills must strike a balance
    between generality (i.e., the applicability to a large variety of downstream tasks)
    and specificity (i.e., the quality of being useful to induce specific behaviors) (?).
    It is also desired to avoid learning trivial skills (?, ?).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，经验上显示，技能发现方法的表现不如其他类型的在线预训练方法，这可能是由于技能空间的限制 (?)。这引起了对所学技能进行剖析的关注。为了发挥其全部潜力，发现的技能必须在通用性（即，适用于各种下游任务的能力）和特异性（即，对特定行为产生有用性的质量）之间取得平衡 (?)。还希望避免学习琐碎的技能 (?, ?)。
- en: 3.3 Data Coverage Maximization
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 数据覆盖最大化
- en: Previously we have discussed how to obtain knowledge or skills, measured by
    the agent’s own capability, from unsupervised interaction. Albeit indirectly related
    to the agent’s ability, data diversity induced by online pretraining plays an
    essential role in deciding how well the agent obtains prior knowledge. In the
    field of supervised learning, recent advances have shown that diverse data can
    enhance out-of-distribution generalization (?) and robustness (?). Another supporting
    evidence is that most of the famed datasets are large and diverse (?, ?). Motivated
    by the above considerations, it is desired to use data coverage maximization,
    usually measured by state visitation, as an objective to stimulate unsupervised
    learning.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论了如何从无监督交互中获得知识或技能，这些知识或技能通过代理的自身能力进行衡量。尽管与代理的能力间接相关，但在线预训练引起的数据多样性在决定代理获得先验知识的效果方面扮演了重要角色。在监督学习领域，近期的研究显示，多样的数据可以增强对分布外数据的泛化能力 (?)
    和鲁棒性 (?)。另一个支持证据是大多数知名数据集都是大型且多样的 (?, ?)。基于上述考虑，使用数据覆盖最大化（通常通过状态访问来衡量）作为目标来刺激无监督学习是期望的。
- en: 3.3.1 Count-based Exploration
  id: totrans-116
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1 基于计数的探索
- en: 'The first category of data coverage maximization is count-based exploration.
    Count-based exploration methods directly use visit counts to guide the agent towards
    underexplored states (?, ?). For tabular MDPs, Model-based Interval Estimation
    with Exploration Bonuses (?) provably turn state-action $N(s,a)$ counts into an
    exploration bonus reward:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 数据覆盖最大化的第一类是基于计数的探索。基于计数的探索方法直接使用访问计数来引导代理到未充分探索的状态 (?, ?)。对于表格型MDP，带有探索奖励的模型基区间估计 (?)
    证明将状态-动作 $N(s,a)$ 计数转换为探索奖励：
- en: '|  | $r_{t}\propto N(s_{t},a_{t})^{-1/2}.$ |  | (2) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $r_{t}\propto N(s_{t},a_{t})^{-1/2}.$ |  | (2) |'
- en: 'Built on Equation [2](#S3.E2 "In 3.3.1 Count-based Exploration ‣ 3.3 Data Coverage
    Maximization ‣ 3 Online Pretraining ‣ Pretraining in Deep Reinforcement Learning:
    A Survey"), a series of work has studied how to tractably generalize count bonuses
    to high-dimensional state spaces (?, ?, ?). To approximate these counts in high
    dimensions, ? (?) introduce pseudo-counts derived from a density model. Specifically,
    the pseudo-count is defined as:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 基于方程式 [2](#S3.E2 "在3.3.1 基于计数的探索 ‣ 3.3 数据覆盖最大化 ‣ 3 在线预训练 ‣ 深度强化学习中的预训练：综述")，一系列研究探讨了如何将计数奖励有效地推广到高维状态空间 (?, ?, ?)。为了在高维空间中近似这些计数，? (?)
    引入了从密度模型派生的伪计数。具体地，伪计数定义为：
- en: '|  | $\hat{N}(s)=\frac{\rho_{t}(s)\left(1-\rho_{t}^{\prime}(s)\right)}{\rho_{t}^{\prime}(s)-\rho_{t}(s)},$
    |  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{N}(s)=\frac{\rho_{t}(s)\left(1-\rho_{t}^{\prime}(s)\right)}{\rho_{t}^{\prime}(s)-\rho_{t}(s)},$
    |  |'
- en: 'where $\rho$ is a density model over state space $\mathcal{S}$, $\rho_{t}(s)$
    is the density assigned to $s$ after training on a sequence of states $s_{1},\ldots,s_{t}$,
    and $\rho_{t}^{\prime}(s)$ is the density of $s$ if $\rho$ were to be trained
    on $s$ one additional time. Based on similar ideas, it has been shown that a better
    density model (?) or a hash function (?, ?) for computing state statistics can
    further improve performance. Besides, a self-supervised inverse dynamics model
    as discussed in Section [3.1](#S3.SS1 "3.1 Curiosity-driven Exploration ‣ 3 Online
    Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey") can also
    be used to bias the count-based bonuses towards what the agent can control (?).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\rho$ 是状态空间$\mathcal{S}$上的密度模型，$\rho_{t}(s)$是在对一系列状态$s_{1},\ldots,s_{t}$进行训练后的$s$的密度，$\rho_{t}^{\prime}(s)$是如果$\rho$额外对$s$进行一次训练时的$s$的密度。基于类似的想法，已经显示更好的密度模型（？）或用于计算状态统计的哈希函数（？，？）可以进一步提高性能。此外，前文第[3.1节](#S3.SS1
    "3.1 Curiosity-driven Exploration ‣ 3 Online Pretraining ‣ Pretraining in Deep
    Reinforcement Learning: A Survey")讨论的自监督逆动态模型也可以用来将基于计数的奖励偏向于代理可以控制的部分（？）。'
- en: 3.3.2 Entropy Maximization
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2 熵最大化
- en: 'To encourage novel state visitation, an alternative objective is to directly
    maximize the entropy of state visitation distribution $d_{\pi}$ induced by policy
    ${\pi_{\theta}\left(a|s\right)}$:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了鼓励新颖状态的访问，另一种目标是直接最大化由策略${\pi_{\theta}\left(a|s\right)}$诱导的状态访问分布$d_{\pi}$的熵：
- en: '|  | $\pi^{*}\in\underset{\pi\in\Pi}{\arg\max}H\left(d_{\pi}\right),$ |  |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\pi^{*}\in\underset{\pi\in\Pi}{\arg\max}H\left(d_{\pi}\right),$ |  |'
- en: where $H(\cdot)$ can be Shannon entropy (?, ?, ?), Rényi entropy (?), or geometry-aware
    entropy (?). The state distribution $d_{\pi}$ can either be a discounted distribution (?),
    a marginal distribution (?), or a stationary distribution (?).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$H(\cdot)$ 可以是香农熵（？，？，？）、雷尼熵（？）或几何感知熵（？）。状态分布$d_{\pi}$ 可以是折扣分布（？）、边际分布（？）或稳态分布（？）。
- en: Albeit compelling, the objective relies on maximizing state entropy, which is
    notoriously hard to estimate and optimize. ? (?) contribute a provably efficient
    algorithm in the tabular setting using the conditional gradient method (?) to
    avoid direct optimization. ? (?) propose a similar approach that can be viewed
    from the perspective of state marginal matching between the state distribution
    and a given target distribution (e.g., a uniform distribution). Both ? (?) and
    ? (?) propose to learn a mixture of policies that maximizes the induced state
    entropy in an iterative manner. While impressive, these parametric approaches
    struggle to scale up to high dimensional spaces. To address this issue, ? (?)
    instead optimize a non-parametric, particle-based estimate of state distribution
    entropy (?), but restrict its use to state-based tasks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管目标具有说服力，但该目标依赖于最大化状态熵，这 notoriously 难以估计和优化。？（？）在表格设置中贡献了一个经过证明的高效算法，使用条件梯度方法（？）避免直接优化。？（？）提出了一种类似的方法，可以从状态分布与给定目标分布（例如均匀分布）之间的状态边际匹配的角度来理解。这两者（？（？）和？（？））建议以迭代方式学习最大化诱导状态熵的策略混合。尽管令人印象深刻，这些参数化方法在高维空间中扩展面临困难。为了解决这个问题，？（？）改为优化非参数粒子型状态分布熵估计（？），但将其应用限制在状态相关任务上。
- en: For unsupervised online pretraining with visual observations, entropy maximization
    becomes more tricky as exploration is now inextricably intertwined with representation
    learning. This leads to a chicken-and-egg problem (?, ?), where learning useful
    representations requires diverse data, while effective exploration can only be
    achieved with good representations. Based on particle-based entropy estimators,
    several approaches successfully apply entropy maximization in image-based tasks
    with self-supervised representations learned by inverse dynamics prediction (?),
    contrastive learning (?, ?), or the information bottleneck (?).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于视觉观察的无监督在线预训练，熵最大化变得更加棘手，因为探索现在与表征学习密不可分。这导致了一个“鸡与蛋”问题（？，？），其中学习有用的表征需要多样的数据，而有效的探索只能通过良好的表征实现。基于粒子型熵估计器，一些方法成功地在图像任务中应用了熵最大化，这些方法通过逆动态预测（？）、对比学习（？，？）或信息瓶颈（？）学习自监督表征。
- en: 3.3.3 Challenges & Future Directions
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3 挑战与未来方向
- en: Although count-based approaches are shown effective for exploration, it has
    been shown in previous work (?) that they usually suffer from detachment, in which
    the agent loses track of interesting areas to explore, and derailment, in which
    the exploratory mechanism prevents it from returning to previously visited states.
    Count-based approaches also tend to be short-sighted, driving the agent to get
    stuck in local minima (?).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管基于计数的方法在探索中显示出有效性，但先前的工作 (?) 已表明，它们通常存在分离问题，即智能体失去对有趣领域的追踪，以及脱轨问题，即探索机制阻止其返回到先前访问的状态。基于计数的方法也往往目光短浅，使得智能体陷入局部最小值 (?)。
- en: When applying state entropy maximization approaches for pretraining, it is worth
    pointing out that many of them aim at maximizing the entropy of all states visited
    during the process, and hence the final policy is not necessarily exploratory (?).
    It has also been shown theoretically that the class of Markovian policies is insufficient
    for the maximum state entropy objective, while non-Markovian policies are essential
    to guarantee good exploration.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用状态熵最大化方法进行预训练时，值得指出的是，许多方法的目标是最大化过程中的所有状态的熵，因此最终策略不一定是探索性的 (?). 理论上也已证明，马尔可夫策略类对于最大状态熵目标是不足的，而非马尔可夫策略对于确保良好的探索是必不可少的。
- en: Instead of learning an exploratory policy, another line of research considers
    collecting unlabeled records as a prerequisite for offline RL (?, ?), which is
    an interesting direction for understanding and utilizing task-agnostic agents.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个研究方向则考虑将收集未标记记录作为离线 RL (?, ?) 的前提，这是一种有趣的方向，用于理解和利用任务无关的智能体。
- en: 4 Offline Pretraining
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 离线预训练
- en: '|  Type | Algorithm | Objective | Visual | Expert Data |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 算法 | 目标 | 视觉 | 专家数据 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Skill Extraction | SPiRL (?) | Variational Auto-encoder | ✓ | ✗ |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 技能提取 | SPiRL (?) | 变分自编码器 | ✓ | ✗ |'
- en: '| OPAL (?) | Variational Auto-encoder | ✗ | ✓ |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| OPAL (?) | 变分自编码器 | ✗ | ✓ |'
- en: '| Parrot (?) | Normalizing Flow | ✓ | ✓ |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 鹦鹉 (?) | 归一化流 | ✓ | ✓ |'
- en: '| SkiLD (?) | Variational Auto-encoder | ✗ | ✗ |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| SkiLD (?) | 变分自编码器 | ✗ | ✗ |'
- en: '| TRIAL (?) | Energy-based Model | ✗ | ✓ |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| TRIAL (?) | 基于能量的模型 | ✗ | ✓ |'
- en: '| FIST (?) | Variational Auto-encoder | ✗ | ✓ |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| FIST (?) | 变分自编码器 | ✗ | ✓ |'
- en: '| Representation Learning | World Model (?) | Reconstruction | ✓ | ✗ |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 表示学习 | 世界模型 (?) | 重建 | ✓ | ✗ |'
- en: '| ST-DIM (?) | Forward Pixel Prediction | ✓ | ✗ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| ST-DIM (?) | 前向像素预测 | ✓ | ✗ |'
- en: '| ATC (?) | Forward Dynamics Modeling | ✓ | ✓ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| ATC (?) | 前向动力学建模 | ✓ | ✓ |'
- en: '| SGI (?) | Forward Dynamics Modeling | ✓ | ✗ |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| SGI (?) | 前向动力学建模 | ✓ | ✗ |'
- en: '| Markov (?) | Inverse Dynamics Modeling | ✓ | ✗ |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 马尔可夫 (?) | 逆向动力学建模 | ✓ | ✗ |'
- en: '|   |  |  |  |  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |  |  |'
- en: 'Table 3: Categorization of representative offline pretraining approaches.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3：代表性离线预训练方法的分类。
- en: Despite its attractive effectiveness of learning without human supervision,
    online pretraining is still limited for large-scale applications. Eventually,
    it is difficult to reconcile online interaction with the need to train on large
    and diverse datasets (?). To address this issue, it is desired to decouple data
    collection and pretraining and directly leverage historical data collected from
    other agents or humans.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在线预训练在无需人工监督的学习方面表现出吸引力，但它在大规模应用中仍然有限。最终，难以协调在线交互与在大规模、多样化数据集上进行训练的需求 (?).
    为了解决这个问题，希望将数据收集与预训练解耦，并直接利用从其他智能体或人类那里收集的历史数据。
- en: A feasible solution is offline RL (?, ?), which has been gaining attention recently.
    Offline RL aims to obtain a reward-maximizing policy purely from offline data.
    A fundamental challenge of offline RL is the distributional shift, which refers
    to the distribution discrepancy between training data and those seen during testing.
    Existing offline RL approaches focus on how to address this challenge when using
    function approximation. For instance, policy constraint approaches (?, ?) explicitly
    require the learned policy to avoid taking unseen actions in the dataset. Value
    regularization methods (?) alleviate the overestimation problem of value functions
    by fitting them to some forms of lower bounds. However, it remains under-explored
    whether policies trained offline can generalize to new contexts unseen in the
    offline dataset (?).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可行的解决方案是离线强化学习（`?`，`?`），这在最近引起了关注。离线强化学习旨在仅从离线数据中获得最大化奖励的策略。离线强化学习的一个基本挑战是分布偏移，即训练数据与测试数据之间的分布差异。现有的离线强化学习方法集中于如何在使用函数逼近时解决这一挑战。例如，策略约束方法（`?`，`?`）明确要求学习的策略避免在数据集中采取未见过的动作。价值正则化方法（`?`）通过将价值函数拟合到某些形式的下界来缓解价值函数的高估问题。然而，尚未充分探讨离线训练的策略是否能够在离线数据集中未见过的新背景中进行泛化（`?`）。
- en: Another scenario is offline-to-online RL (?, ?, ?, ?), where offline RL is used
    for pretraining, followed by online finetuning. It has been shown in this scenario
    that offline RL can accelerate online RL (?). However, both offline RL and offline-to-online
    RL require the offline experience to be annotated with rewards, which are challenging
    to provide for large real-world datasets (?).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个场景是离线到在线的强化学习（`?`，`?`，`?`，`?`），其中离线强化学习用于预训练，随后进行在线微调。研究表明，在这种情况下，离线强化学习可以加速在线强化学习（`?`）。然而，离线强化学习和离线到在线强化学习都需要离线经验用奖励进行标注，而对于大规模真实世界数据集，这一点较难提供（`?`）。
- en: 'A compelling alternative direction for leveraging offline data is to sidestep
    policy learning, but instead learn prior knowledge that is beneficial for downstream
    tasks in terms of convergence speed or final performances. What is more intriguing,
    if our model were able to utilize data without human supervision, it could potentially
    benefit from web-scale data for decision-making. We refer to this setting as offline
    pretraining, where the agent can extract important information (e.g., good representations
    and behavior priors) from offline data. In Table [3](#S4.T3 "Table 3 ‣ 4 Offline
    Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey"), we categorize
    existing offline pretraining approaches as well as summarize each approach’s key
    properties.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '利用离线数据的一个有力的替代方向是绕过策略学习，而是学习对下游任务有益的先验知识，这些知识在收敛速度或最终性能方面有所帮助。更有趣的是，如果我们的模型能够利用没有人工监督的数据，它可能会从大规模的网络数据中受益用于决策。我们将这种设置称为离线预训练，其中代理可以从离线数据中提取重要信息（例如良好的表征和行为先验）。在表 [3](#S4.T3
    "Table 3 ‣ 4 Offline Pretraining ‣ Pretraining in Deep Reinforcement Learning:
    A Survey")中，我们对现有的离线预训练方法进行了分类，并总结了每种方法的关键特性。'
- en: 4.1 Skill Extraction
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 技能提取
- en: Learning useful behaviors from offline data has a long history (?, ?). When
    the offline data comes from expert demonstrations, it is straightforward to pretrain
    policies via imitation learning (?, ?, ?), which is often used in real-world applications
    like robotic manipulation (?, ?) and self-driving (?). However, imitation learning
    approaches often assume that the training data contains complete solutions. They
    therefore usually fall short of obtaining good policies when demonstrations are
    collected from a series of sources.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从离线数据中学习有用的行为有着悠久的历史（`?`，`?`）。当离线数据来自专家演示时，通过模仿学习（`?`，`?`，`?`）进行策略预训练是直接的，这在机器人操作（`?`，`?`）和自动驾驶（`?`）等真实世界应用中经常使用。然而，模仿学习方法通常假设训练数据包含完整的解决方案。因此，当演示数据来自多个来源时，它们通常无法获得良好的策略。
- en: 'An alternative solution is to learn useful behavior priors from offline data (?, ?, ?),
    similar to what we have discussed in Section [3.2](#S3.SS2 "3.2 Skill Discovery
    ‣ 3 Online Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey").
    Compared with its online counterpart, offline skill extraction assumes a fixed
    set of trajectories. These approaches learn a spectrum of behavior policies conditioned
    on latent $z$, which provide a more compact action space for learning high-level
    policies that can quickly adapt to downstream tasks. Specifically, temporal skill
    extraction (?) for few-shot imitation (?) and RL (?, ?, ?) considers how to distill
    offline trajectories into primitive policies ${\pi\left(a|s,z\right)}$, where
    $z\in\mathcal{Z}$ denotes a skill latent learned via unsupervised learning. By
    leveraging stochastic latent variable models, we aim at learning a skill latent
    $z_{i}\in\mathcal{Z}$ for a sequence of state-action pairs $\left\{s_{t},a_{t},\ldots,s_{t+H-1},a_{t+H-1}\right\}$,
    where $H$ is a fixed horizon or a variable one (?, ?). For example, ? (?) propose
    the following auto-encoding objective to learn primitive skills:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种解决方案是从离线数据中学习有用的行为先验 (？，？，？)，类似于我们在第 [3.2](#S3.SS2 "3.2 Skill Discovery
    ‣ 3 Online Pretraining ‣ Pretraining in Deep Reinforcement Learning: A Survey")
    节中讨论的内容。与在线对应方法相比，离线技能提取假设一组固定的轨迹。这些方法学习条件于潜变量 $z$ 的行为策略的范围，为学习能够迅速适应下游任务的高层次策略提供了更紧凑的动作空间。具体来说，少样本模仿
    (？) 和强化学习 (？，？，？) 的时间技能提取 (？) 考虑如何将离线轨迹提炼为原始策略 ${\pi\left(a|s,z\right)}$，其中 $z\in\mathcal{Z}$
    表示通过无监督学习获得的技能潜变量。通过利用随机潜变量模型，我们旨在为一系列状态-动作对 $\left\{s_{t},a_{t},\ldots,s_{t+H-1},a_{t+H-1}\right\}$
    学习一个技能潜变量 $z_{i}\in\mathcal{Z}$，其中 $H$ 是固定视野或可变视野 (？，？)。例如，? (？) 提出了以下自编码目标来学习原始技能：'
- en: '|  |  | $\displaystyle\min_{\theta,\phi,\omega}J(\theta,\phi,\omega)=\mathbb{E}_{\tau\sim\mathcal{D},z\sim
    q_{\phi}(z\mid\tau)}\left[-\sum_{t=0}^{H-1}\log\pi_{\theta}\left(a_{t}\mid s_{t},z\right)\right]$
    |  |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\min_{\theta,\phi,\omega}J(\theta,\phi,\omega)=\mathbb{E}_{\tau\sim\mathcal{D},z\sim
    q_{\phi}(z\mid\tau)}\left[-\sum_{t=0}^{H-1}\log\pi_{\theta}\left(a_{t}\mid s_{t},z\right)\right]$
    |  |'
- en: '|  |  | $\displaystyle\text{ s.t. }\mathbb{E}_{\tau\sim\mathcal{D}}\left[\mathrm{D}_{\mathrm{KL}}\left(q_{\phi}(z\mid\tau)\&#124;\rho_{\omega}\left(z\mid
    s_{0}\right)\right)\right]\leq\epsilon_{\mathrm{KL}},$ |  |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\text{ 满足 }\mathbb{E}_{\tau\sim\mathcal{D}}\left[\mathrm{D}_{\mathrm{KL}}\left(q_{\phi}(z\mid\tau)\&#124;\rho_{\omega}\left(z\mid
    s_{0}\right)\right)\right]\leq\epsilon_{\mathrm{KL}},$ |  |'
- en: 'where $q_{\phi}(z\mid\tau)$ encodes the trajectory $\tau$ into skill latent
    $z$ and skill policy ${\pi\left(a|s,z\right)}$ serves as a decoder to translate
    skill latent $z$ into action sequences. To transfer skills into downstream tasks,
    it is feasible to learn a hierarchical policy that generates high-level behaviors
    with $\pi(z\mid s)$ trained on downstream tasks (?), which will be elaborated
    in Secition [6.2](#S6.SS2 "6.2 Policy Transfer ‣ 6 Task Adaptation ‣ Pretraining
    in Deep Reinforcement Learning: A Survey").'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $q_{\phi}(z\mid\tau)$ 将轨迹 $\tau$ 编码为技能潜变量 $z$，技能策略 ${\pi\left(a|s,z\right)}$
    作为解码器将技能潜变量 $z$ 转换为动作序列。为了将技能转移到下游任务中，可以学习一个层次化策略，该策略通过在下游任务上训练的 $\pi(z\mid s)$
    生成高层次行为（?），这将在第 [6.2](#S6.SS2 "6.2 Policy Transfer ‣ 6 Task Adaptation ‣ Pretraining
    in Deep Reinforcement Learning: A Survey") 节中详细阐述。'
- en: Various latent variable models have been used for pretraining behavior priors.
    For instance, variational auto-encoders (?) are widely considered (?, ?, ?). Following
    work (?, ?) also explores normalizing flow (?) and energy-based models (?) to
    learn action priors.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 各种潜变量模型已用于预训练行为先验。例如，变分自编码器 (？) 被广泛认为 (？，？，？)。后续工作 (？，？) 还探索了归一化流 (？) 和基于能量的模型
    (？) 来学习动作先验。
- en: The scenario of pretraining behavior priors also bears resemblance to few-shot
    imitation learning (?, ?). However, for few-shot imitation learning, it is often
    assumed that expert data is collected from a single behavior policy. Furthermore,
    due to error accumulation (?), few-shot imitation learning is often limited to
    short-horizon problems (?). In this regard, learning behavior priors from diverse
    and sub-optimal data appears to be a promising direction.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练行为先验的场景也与少样本模仿学习 (？，？) 类似。然而，对于少样本模仿学习，通常假设专家数据来自单一行为策略。此外，由于误差累积 (？)，少样本模仿学习通常限制于短视野问题
    (？)。在这方面，从多样化和次优数据中学习行为先验似乎是一个有前途的方向。
- en: 4.1.1 Challenges & Future Directions
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 挑战与未来方向
- en: Despite its potential to extract useful primitive skills, it is still challenging
    to pretrain on highly sub-optimal offline data containing random actions (?).
    Besides, RL with learned skills does not usually generalize to downstream tasks
    efficiently, requiring millions of online interactions to converge (?). A possible
    solution is to combine with successor features (?, ?) for fast task inference.
    However, strategies that directly use the pretrained policies for exploitation
    may result in sub-optimal solutions in such a scenario (?).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它有可能提取有用的原始技能，但在高度次优的离线数据上进行预训练仍然具有挑战性，其中包含随机动作（?）。此外，使用学习到的技能的强化学习通常不能有效地推广到下游任务，需要数百万次在线交互才能收敛（?）。一种可能的解决方案是结合后继特征（?,
    ?）以实现快速任务推断。然而，在这种情况下，直接使用预训练策略进行利用可能导致次优解（?）。
- en: 4.2 Representation Learning
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 表示学习
- en: '|  Type | Sufficiency | Compactness |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 充分性 | 紧凑性 |'
- en: '| --- | --- | --- |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Reconstruction | $\bigstar\bigstar\bigstar$ | $\bigstar$ |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 重建 | $\bigstar\bigstar\bigstar$ | $\bigstar$ |'
- en: '| Forward Pixel Prediction | $\bigstar\bigstar\bigstar$ | $\bigstar$ |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 前向像素预测 | $\bigstar\bigstar\bigstar$ | $\bigstar$ |'
- en: '| Forward Dynamics Modeling | $\bigstar\bigstar$ | $\bigstar\bigstar$ |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 前向动力学建模 | $\bigstar\bigstar$ | $\bigstar\bigstar$ |'
- en: '| Inverse Dynamics Modeling | $\bigstar$ | $\bigstar\bigstar\bigstar$ |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 逆向动力学建模 | $\bigstar$ | $\bigstar\bigstar\bigstar$ |'
- en: '|   |  |  |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|   |  |  |'
- en: 'Table 4: Comparison between different representation learning approaches.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4：不同表示学习方法的比较。
- en: 'While pretraining behavior priors focus on reducing the complexity of the action
    space, there exists another line of work that aims to pretrain good state representations
    from offline data to promote transfer. If the agent effectively reduces the representation
    gap between the learned state representations and the ground-truth endogenous
    states, it can better focus on factors that are essential for control. Table [4](#S4.T4
    "Table 4 ‣ 4.2 Representation Learning ‣ 4 Offline Pretraining ‣ Pretraining in
    Deep Reinforcement Learning: A Survey") compares different kinds of representation
    learning objectives in terms of sufficiency (i.e., whether the representations
    contain sufficient state information) and compactness (i.e., whether the representations
    discard irrelevant information).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然预训练行为先验侧重于减少动作空间的复杂性，但还有另一类工作旨在从离线数据中预训练良好的状态表示，以促进迁移。如果智能体能够有效地减少学习到的状态表示与真实内生状态之间的表示差距，它可以更好地关注对控制至关重要的因素。表[4](#S4.T4
    "Table 4 ‣ 4.2 Representation Learning ‣ 4 Offline Pretraining ‣ Pretraining in
    Deep Reinforcement Learning: A Survey")比较了不同类型的表示学习目标在充分性（即，表示是否包含足够的状态信息）和紧凑性（即，表示是否丢弃了不相关的信息）方面的表现。'
- en: Learning good state representations for RL is a mature research area with a
    range of tools (?, ?, ?). Traditionally, the problem is formulated to group states
    into clusters based on certain properties (?). Existing representation learning
    approaches generally propose some predictive properties that the desired representations
    have, with regard to states, actions, and rewards across different time-steps.
    One of the most representative concepts is bisimulation (?, ?), which originally
    requires two equivalent states to have the same reward and equivalent distributions
    over the next bisimilar states. The objective turns out to be very restrictive
    and is further relaxed by following work (?, ?) with a defined pseudo-metric space
    to measure behavioral similarity. Despite their recent advances (?, ?, ?) in effective
    representation learning using deep neural networks, bisimulation methods fail
    to provide good abstraction when the rewards are sparse or even absent. In this
    case, solely relying on a forward model can lead to representation collapse (?).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为强化学习学习良好的状态表示是一个成熟的研究领域，拥有一系列工具（?, ?, ?）。传统上，这个问题被形式化为根据某些属性将状态分组（?）。现有的表示学习方法通常提出一些期望表示所具有的预测特性，涉及不同时间步的状态、动作和奖励。其中一个最具代表性的概念是双模拟（?,
    ?），其最初要求两个等价状态具有相同的奖励，并且在下一个双模拟状态上具有等效的分布。这个目标被证明是非常严格的，并在后续工作（?, ?）中通过定义的伪度量空间来测量行为相似性进一步放宽。尽管在使用深度神经网络进行有效表示学习方面取得了最近的进展（?,
    ?, ?），双模拟方法在奖励稀疏或甚至缺失的情况下未能提供良好的抽象。在这种情况下，仅仅依靠前向模型可能导致表示崩溃（?）。
- en: To alleviate representation collapse, one can instead set the targets to pixel
    observations. This includes reconstruction-based approaches (?, ?) and those based
    on pixel prediction (?, ?). Reconstruction-based approaches typically train an
    auto-encoder on image observations to learn a low-dimensional representation,
    using which a policy is learned subsequently. Approaches based on pixel prediction
    force the representations to contain sufficient information about future pixel
    observations. Despite these learned representations preserving sufficient information
    about the observation, it lacks compactness and does not guarantee to capture
    of useful information for the control task.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了缓解表示崩溃，可以将目标设置为像素观察。这包括基于重建的方法 (?, ?) 和基于像素预测的方法 (?, ?)。基于重建的方法通常在图像观察上训练自编码器，以学习低维表示，随后学习一个策略。基于像素预测的方法则迫使表示包含足够的信息关于未来像素观察。尽管这些学习到的表示保留了足够的观察信息，但缺乏紧凑性，并且无法保证捕捉到控制任务的有用信息。
- en: Instead of predicting the future, it is also beneficial to model the inverse
    dynamics of the system (?, ?). Inverse dynamics modeling learns a representation
    that is predictive of the action taken between a pair of consecutive states. It
    has been shown that the learned representation can filter out all uncontrollable
    aspects of the observations (?). However, it can also wrongly ignore controllable
    information and cause over-abstraction over the state space (?, ?).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 除了预测未来，建模系统的逆动力学 (?, ?) 也是有益的。逆动力学建模学习一种在一对连续状态之间的行动可预测的表示。研究表明，学习到的表示可以过滤掉观察中的所有不可控方面 (?)。然而，它也可能错误地忽略可控信息，导致对状态空间的过度抽象 (?, ?)。
- en: 'With the rise of self-supervised learning developed for CV and NLP, a natural
    direction is to adapt these task-agnostic techniques to RL. For instance, a large
    body of works has explored contrastive learning (?) as an effective framework
    to learn good representations (?, ?, ?, ?). Contrastive learning typically uses
    the InfoNCE loss (?) to maximize mutual information between two variables:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自监督学习在计算机视觉（CV）和自然语言处理（NLP）中的兴起，一个自然的方向是将这些任务无关的技术应用于强化学习（RL）。例如，大量的研究探讨了对比学习 (?) 作为一种有效的框架来学习良好的表示 (?, ?, ?, ?)。对比学习通常使用InfoNCE损失 (?) 来最大化两个变量之间的互信息：
- en: '|  | $\mathcal{L}_{\text{InfoNCE }}=\mathbb{E}\left[\log\frac{\exp\left(f\left(x_{i},y_{i}\right)\right)}{\frac{1}{K}\sum_{j=1}^{K}\exp\left(f\left(x_{i},y_{j}\right)\right)}\right],$
    |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{InfoNCE }}=\mathbb{E}\left[\log\frac{\exp\left(f\left(x_{i},y_{i}\right)\right)}{\frac{1}{K}\sum_{j=1}^{K}\exp\left(f\left(x_{i},y_{j}\right)\right)}\right],$
    |  |'
- en: where $f$ is a bilinear function $f\left(x_{i},y_{i}\right)=\phi(x_{i})^{\top}W\phi(y_{i})$
    with learned parameter $W\in\mathbb{R}^{n\times n}$ and $K$ is the number of negative
    samples. These approaches usually incorporate temporal information, aiming to
    distinguish between sequential and non-sequential states (?, ?). Following works (?, ?)
    further consider bootstrapped latent representations (?) that get rid of negative
    samples.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f$ 是一个双线性函数 $f\left(x_{i},y_{i}\right)=\phi(x_{i})^{\top}W\phi(y_{i})$，具有学习参数
    $W\in\mathbb{R}^{n\times n}$，$K$ 是负样本的数量。这些方法通常结合时间信息，旨在区分顺序状态和非顺序状态 (?, ?)。后续工作 (?, ?) 进一步考虑了自举潜在表示 (?)，以摆脱负样本。
- en: Aside from the above representation learning objectives, some other work considers
    imposing Lipschitz smoothness (?, ?), kinematic inseparability (?), or the Markov
    property (?). It has also been shown that a combined objective can also lead to
    better performance (?).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述表示学习目标，一些其他工作考虑了施加Lipschitz平滑 (?, ?), 运动学不可分离性 (?), 或Markov性质 (?)。还表明，结合目标也可以带来更好的性能 (?)。
- en: 4.2.1 Challenges & Future Directions
  id: totrans-179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 挑战与未来方向
- en: 'While unsupervised representations have been shown to bring significant improvements
    to downstream tasks, the absence of reward signals typically leads the pretrained
    encoder to focus on task-irrelevant features instead of task-relevant ones in
    visually complex environments (?). To alleviate this issue, one might incorporate
    additional inductive bias (?) or labeled data that are cheaper to obtain. We will
    discuss the latter solution in Section [5](#S5 "5 Towards Generalist Agents with
    RL ‣ Pretraining in Deep Reinforcement Learning: A Survey").'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管无监督表示已显示出对下游任务带来了显著的改进，但奖励信号的缺失通常导致预训练编码器在视觉复杂环境中关注与任务无关的特征，而不是与任务相关的特征（？）。为了解决这个问题，可以结合额外的归纳偏差（？）或更便宜获得的标注数据。我们将在第[5](#S5
    "5 Towards Generalist Agents with RL ‣ Pretraining in Deep Reinforcement Learning:
    A Survey")节讨论后者的解决方案。'
- en: Another challenge for unsupervised representation learning is how to measure
    its effectiveness without access to downstream tasks. Such evaluation is beneficial
    because it can provide a proxy metric to predict performance and promote a deeper
    understanding of the semantic meanings of pretrained representations. To achieve
    this, it is desired to analyze these representations with probing techniques and
    determine which properties they encode. Although previous work has made efforts
    in this direction (?), it remains unclear what properties are most indispensable
    for pretrained representations.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督表示学习的另一个挑战是如何在没有下游任务的情况下衡量其效果。这种评估是有益的，因为它可以提供一个预测性能的代理度量，并促进对预训练表示的语义含义的更深入理解。为此，期望利用探测技术分析这些表示，并确定它们编码了哪些属性。尽管以前的工作在这方面做出了努力（？），但仍不清楚哪些属性对预训练表示最为不可或缺。
- en: 5 Towards Generalist Agents with RL
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 朝着通用智能体的强化学习
- en: So far we have discussed online and offline scenarios that are generally restricted
    to a single modality and single environment. Recently, there is a surge of interest
    in building a single generalist model (?, ?, ?) to handle tasks in different environments
    across different modalities. To enable the agent to learn from and adapt to various
    open-ended tasks, it is desired to leverage considerable prior knowledge in different
    forms such as visual perception and language understanding. Intuitively, the aim
    is to bridge the worlds of RL and other fields of machine learning, combining
    previous success together to build a large decision-making model capable of a
    diverse set of tasks. In this section, we look at various considerations for handling
    data and tasks from different modalities to acquire useful prior knowledge.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了通常限制于单一模态和单一环境的在线和离线场景。最近，人们对构建一个能够处理不同环境中不同模态任务的单一通用模型（？，？，？）产生了浓厚的兴趣。为了使智能体能够从各种开放任务中学习和适应，期望利用不同形式的丰富先验知识，如视觉感知和语言理解。从直观上讲，目标是桥接强化学习与其他机器学习领域的世界，将以往的成功结合起来，构建一个能够处理多样任务的大型决策模型。在这一部分，我们将探讨处理来自不同模态的数据和任务的各种考虑因素，以获取有用的先验知识。
- en: 5.1 Visual Pretraining
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 视觉预训练
- en: Perception is an unavoidable prerequisite for real-world applications. With
    an increased number of image-based decision-making tasks, pretrained visual encoders
    that were exposed to a wide distribution of images can provide RL agents with
    robust and resilient representations as a basis to learn optimal policies.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 感知是实际应用中不可避免的前提。随着基于图像的决策任务的增加，接触到广泛图像分布的预训练视觉编码器可以为强化学习智能体提供强健而稳健的表示，作为学习最优策略的基础。
- en: The field of computer vision has seen tremendous progress in pretraining visual
    encoders from large-scale image datasets (?) and video corpora (?). Given that
    these data are cheap to access, several works have explored the use of pretrained
    visual encoders on large-scale image datasets as means to improve the generalization
    and sample efficiency of RL agents. ? (?) equip standard deep RL algorithms with
    ResNet encoders pretrained on ImageNet and observe that the pretrained representations
    lead to impressive performances in Adroit (?) but struggle in the DeepMind control
    suite (?) due to large domain gap. ? (?) further investigate various design choices
    including datasets, augmentations, and layers, and report positive results on
    all four considered control tasks. ? (?) conduct a large-scale study on how different
    properties of pretrained VAE-based embeddings affect out-of-distribution generalization,
    concluding that some of them (e.g., the GS metric (?)) can be good proxy metrics
    to predict generalization performance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉领域在从大规模图像数据集 (?) 和视频语料库 (?) 中预训练视觉编码器方面取得了巨大进展。鉴于这些数据易于获取，多个研究探索了在大规模图像数据集上使用预训练视觉编码器作为提升RL代理的泛化能力和样本效率的手段。? (?)
    使标准深度RL算法配备了在ImageNet上预训练的ResNet编码器，观察到预训练的表示在Adroit (?) 中表现出色，但在DeepMind控制套件 (?)
    中由于较大的领域差距而表现不佳。? (?) 进一步调查了包括数据集、增强和层在内的各种设计选择，并报告了所有四个控制任务上的积极结果。? (?) 进行了一项大规模研究，探讨了预训练的VAE基础嵌入的不同属性如何影响分布外泛化，结论是其中一些（例如GS指标 (?)）可以作为预测泛化性能的良好代理指标。
- en: Instead of extracting visual information from static image datasets, another
    intriguing direction is to capture temporal relations from unlabeled videos. ? (?)
    design a self-supervised approach to learning temporal variance and multi-view
    invariance on multi-view video data. ? (?) empirically find that, without exploiting
    temporal information, in-the-wild images collected from YouTube or Egocentric
    videos lead to better self-supervised representations for manipulation tasks that
    ImageNet images. ? (?) introduce a two-phase learning framework, which first learns
    useful representations via generative pretraining on videos and then uses the
    pretrained model for learning action-conditional world models. ? (?) successfully
    extract behavioral priors from internet-scale videos with an inverse dynamics
    model to uncover the underlying actions followed by behavior cloning, finding
    that the pretrained model exhibits impressive zero-shot capabilities and finetuning
    results for playing Minecraft. ? (?) also leverage inverse dynamics models to
    predict action labels from action-free videos, upon which a new contrastive learning
    framework is proposed to pretrain action-conditioned policies.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 相较于从静态图像数据集中提取视觉信息，另一个引人注目的方向是从未标记的视频中捕捉时间关系。? (?) 设计了一种自监督的方法，用于在多视角视频数据上学习时间变化和多视角不变性。? (?)
    实证发现，在不利用时间信息的情况下，从YouTube或自视角视频中收集的自然图像能在操作任务上产生比ImageNet图像更好的自监督表示。? (?) 引入了一种两阶段学习框架，首先通过对视频的生成预训练学习有用的表示，然后利用预训练模型学习基于动作的世界模型。? (?)
    成功地从互联网规模的视频中提取行为先验，使用逆动力学模型揭示潜在动作，并通过行为克隆发现预训练模型在玩Minecraft时表现出令人印象深刻的零-shot能力和微调结果。? (?)
    还利用逆动力学模型从无动作视频中预测动作标签，并提出了一种新的对比学习框架以预训练基于动作的策略。
- en: 5.2 Natural Language Pretraining
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 自然语言预训练
- en: Human beings are not only able to perceive the visual world through their eyes,
    but understand high-level natural language instructions and ground the rich knowledge
    from texts to complete tasks. In this vein, there has been a long history of how
    to connect language and actions (?, ?). Especially due to the rapid development
    of large language models (LLMs) (?, ?) that exhibit great capability of encoding
    semantic knowledge, it appears to be a promising direction to leverage advanced
    LLMs as generic computation engines to facilitate decision making (?).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 人类不仅能够通过眼睛感知视觉世界，还能够理解高级自然语言指令，并将文本中的丰富知识应用于完成任务。在这一方面，如何连接语言和动作有着悠久的历史 (?, ?)。尤其由于大型语言模型
    (LLMs) (?, ?) 的快速发展，这些模型展现了编码语义知识的巨大能力，利用先进的LLMs作为通用计算引擎以促进决策制定似乎是一个有前景的方向 (?)。
- en: 5.2.1 Language-conditioned Policy Learning
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1 基于语言的策略学习
- en: To extract and harness the knowledge of well-informed pretrained LLMs, a feasible
    solution is to condition the policies on text descriptions processed by LLMs.
    This kind of language-conditioned policy learning could be extremely useful for
    robotic tasks where high-level language instructions are available. For example,
    ? (?) use pretrained LLMs to split high-level instructions into sub-tasks via
    prompt engineering for grounding value functions in real-world robotic tasks.
    ? (?) further enable grounded closed-loop feedback generated by additional perception
    models as the source of corrections for LLMs’ predictions. ? (?) instead consider
    effective exploration in 3D environments, showing that pretrained representations
    from vision-language models (?) form a semantically meaningful state space for
    curiosity-driven intrinsic rewards. ? (?) also connect reward specification to
    vision-language supervision, introducing a framework that leverages text descriptions
    and pixel observations to produce reward signals.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取和利用预训练LLMs的知识，一种可行的解决方案是根据LLMs处理的文本描述来调整策略。这种基于语言的策略学习对于可以提供高级语言指令的机器人任务极为有用。例如，？（？）利用预训练LLMs通过提示工程将高级指令拆分为子任务，用于在现实世界的机器人任务中接地价值函数。？（？）进一步通过额外的感知模型生成的接地闭环反馈来作为LLMs预测的纠正来源。？（？）则考虑了在3D环境中有效的探索，展示了来自视觉语言模型（？）的预训练表示形成了一个语义上有意义的状态空间，用于好奇心驱动的内在奖励。？（？）还将奖励规范与视觉语言监督相结合，引入了一个利用文本描述和像素观察来产生奖励信号的框架。
- en: 5.2.2 Policy Initialization
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2 策略初始化
- en: Recent advances bridge the gap between reinforcement learning and sequential
    modeling (?, ?, ?, ?), opening up opportunities to borrow sequential models to
    RL tasks. Despite the clear distinction, pretrained LLMs could arguably provide
    reusable knowledge via weight initialization. ? (?) investigate whether pretrained
    LLMs can provide good weight initialization for Transformer-based offline RL models,
    and conclude with very positive results. ? (?) also demonstrate that pretrained
    LLMs can be used to initialize policies and facilitate behavior cloning as well
    as online reinforcement learning for embodied tasks. They also suggest using sequential
    input representations and fintuning the pretrained weights for better generalization.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最近的进展弥合了强化学习与序列建模之间的差距（？，？，？，？），为将序列模型借用于RL任务开辟了机会。尽管存在明显的区别，但预训练的LLMs可以通过权重初始化提供可重用的知识。？（？）研究了预训练的LLMs是否能为基于Transformer的离线RL模型提供良好的权重初始化，并得出了非常积极的结果。？（？）还展示了预训练的LLMs可以用来初始化策略，并促进行为克隆以及针对具身任务的在线强化学习。他们还建议使用序列输入表示，并对预训练权重进行微调，以实现更好的泛化。
- en: 5.3 Multi-task and Multi-modal Pretraining
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 多任务和多模态预训练
- en: With recent advances in building powerful sequence models to handle different
    modalities and tasks (?, ?, ?), the wave of using large general-propose models (?)
    has been sweeping through the field of supervised learning. The key ingredient
    is Transformer (?), a highly capable neural architecture built on the self-attention
    mechanism (?) that excels at capturing long-range dependencies in sequential data.
    Due to its strong generality where various tasks in different domains can be formulated
    as sequence modeling, Transformer is believed to be a unified architecture for
    developing foundation models (?).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 随着在处理不同模态和任务的强大序列模型的最新进展（？，？，？），使用大型通用模型（？）的潮流已经席卷了监督学习领域。关键成分是Transformer（？），一种基于自注意力机制（？）的高度有效的神经架构，擅长捕捉序列数据中的长程依赖关系。由于其强大的通用性，能够将不同领域的各种任务公式化为序列建模，因此Transformer被认为是开发基础模型（？）的统一架构。
- en: Recently, Transformer-based architectures have also been extended to the field
    of offline RL (?, ?) and then online RL (?), in which the agent is trained auto-regressively
    in a supervised manner via likelihood maximization. This opens up the possibility
    of replicating previous success achieved with Transformer in the field of supervised
    learning. Specifically, it is expected that by combining large-scale data, open-ended
    objectives, and Transformer-based architectures, we are ready to build general-purpose
    decision-making agents that are capable of various downstream tasks in different
    environments.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，基于Transformer的架构也被扩展到了离线RL（？，？）以及在线RL（？）领域，其中智能体通过最大化似然的监督方式进行自回归训练。这为在监督学习领域复制Transformer取得的成功开辟了可能性。具体来说，预计通过结合大规模数据、开放式目标和基于Transformer的架构，我们已经准备好构建能够在不同环境中处理各种下游任务的通用决策智能体。
- en: Pioneering work in this direction is Gato (?), a generalist agent trained on
    various tasks from control environments, vision datasets, and language datasets
    in a supervised manner. To handle multi-task and multi-modal data, Gato uses demonstrations
    as prompt sequences (?) at inference time. ? (?) extend Decision Transformer (?)
    to train a generalist agent called Multi-Game DT that can play 41 Atari games
    simultaneously. Both Gato and Multi-Game DT show impressive scaling law properties.
    ? (?) make use of large-scale multi-modal data from YouTube videos, Wikipedia
    pages, and Reddit posts to train an agent able to solve various tasks in Minecraft.
    To provide dense reward signals, a pretrained vision-language model based on CLIP (?)
    is introduced as a proxy of human evaluation.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方向的开创性工作是Gato（？），这是一个在控制环境、视觉数据集和语言数据集上以监督方式训练的通用智能体。为了处理多任务和多模态数据，Gato在推理时使用演示作为提示序列（？）。？（？）扩展了Decision
    Transformer（？）以训练一个名为Multi-Game DT的通用智能体，能够同时玩41款Atari游戏。Gato和Multi-Game DT都展示了令人印象深刻的扩展规律特性。？（？）利用来自YouTube视频、Wikipedia页面和Reddit帖子的多模态大规模数据来训练一个能够在Minecraft中解决各种任务的智能体。为了提供密集的奖励信号，引入了基于CLIP（？）的预训练视觉语言模型作为人类评估的代理。
- en: 5.4 Challenges & Future Directions
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 挑战与未来方向
- en: In spite of some promising results, how generalist models benefit from multi-modal
    and multi-task data remains unclear. More specifically, these models might suffer
    from detrimental gradient interference (?) between modalities and tasks due to
    the incurred optimization challenges. To mitigate this issue, it is desired to
    incorporate more analysis tools for optimization landscapes (?) and gradients (?)
    to tease out the precise principles.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管取得了一些有前景的结果，但通用模型如何从多模态和多任务数据中受益仍不清楚。更具体地说，这些模型可能会因为引发的优化挑战而遭受模态和任务之间有害的梯度干扰（？）。为了缓解这个问题，期望结合更多优化景观（？）和梯度（？）分析工具，以揭示精确的原则。
- en: Another compelling direction is to compose separate pretrained models (e.g.,
    GPT-3 (?) and CLIP (?)) together. By leveraging expert knowledge from different
    models, this kind of framework can solve complex multi-modal tasks (?).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个引人注目的方向是将不同的预训练模型（例如GPT-3（？）和CLIP（？））组合在一起。通过利用来自不同模型的专家知识，这种框架可以解决复杂的多模态任务（？）。
- en: 6 Task Adaptation
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 任务适应
- en: While pretraining on unsupervised experiences can result in rich transferable
    knowledge, it remains challenging to adapt the knowledge to downstream tasks in
    which reward signals are exposed. In this section, we discuss briefly various
    considerations for downstream task adaptation. We limit the scope to online adaptation,
    while adaptation with offline RL or imitation learning is also feasible (?).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在无监督经验上的预训练可以产生丰富的可迁移知识，但将这些知识适应于奖励信号暴露的下游任务仍然具有挑战性。在本节中，我们简要讨论了下游任务适应的各种考虑因素。我们将范围限制在在线适应，而离线RL或模仿学习的适应也是可行的（？）。
- en: In online task adaptation, a pretrained model is given, which can be composed
    of various components such as policies and representations, together with a target
    MDP that can interact with. Given that pretraining could result in different forms
    of knowledge, it brings difficulties to designing principled adaptation techniques.
    Nevertheless, considerable efforts have been made to study this aspect.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线任务适应中，给定一个预训练模型，该模型可以由各种组件组成，如策略和表示，并且有一个可以进行交互的目标MDP。鉴于预训练可能产生不同形式的知识，这给设计有原则的适应技术带来了困难。然而，已经投入了相当大的努力来研究这一方面。
- en: 6.1 Representation Transfer
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 表示迁移
- en: In the field of supervised learning, recent advances (?, ?, ?) have demonstrated
    that good representations can be pretrained on large-scale unlabeled dataset,
    as evidenced by their impressive downstream performances. The most common practice
    is to freeze the weights of the pretrained feature encoder and train a randomly
    initialized task-specific network on top of that during adaptation. The success
    of this paradigm is essentially based on the promise that related tasks can usually
    be solved using similar representations.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习领域，最近的进展 (?, ?, ?) 证明了良好的表征可以在大规模无标注数据集上进行预训练，这从其令人印象深刻的下游表现中得到了体现。最常见的做法是冻结预训练特征编码器的权重，并在此基础上训练一个随机初始化的任务特定网络进行适应。这一范式的成功本质上基于相关任务通常可以使用类似的表征来解决。
- en: For RL, it has been shown that directly reusing pretrained task-agnostic representations
    can significantly improve sample efficiency on downstream tasks. For instance,
    ? (?) conduct experiments on the Atari 100K benchmark and find that frozen representations
    pretrained on exploratory offline data already form a basis of data-efficient
    RL. This success also extends to the cases where domain discrepancy exists between
    upstream and downstream tasks (?, ?). However, the issue of negative transfer
    in the face of domain discrepancy might be exacerbated for RL due to its complexity (?).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 对于强化学习（RL），已经证明直接重用预训练的任务无关表征可以显著提高下游任务的样本效率。例如，? (?) 在 Atari 100K 基准测试中进行实验，发现基于探索性离线数据预训练的冻结表征已经构成了数据高效
    RL 的基础。这一成功也延伸到了上游和下游任务之间存在领域差异的情况 (?, ?)。然而，由于 RL 的复杂性，领域差异下的负迁移问题可能会被放大 (?)。
- en: 'When adapting to tasks that have the same environment dynamics as that of the
    upstream task(s), successor features (?) can be a powerful tool to aid task adaptation.
    The framework of successor features is based on the following decomposition of
    reward functions:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在适应具有与上游任务相同环境动态的任务时，后继特征 (?) 可以成为辅助任务适应的强大工具。后继特征的框架基于以下奖励函数的分解：
- en: '|  | $r\left(s,a,s^{\prime}\right)=\phi\left(s,a,s^{\prime}\right)^{\top}w,$
    |  | (3) |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|  | $r\left(s,a,s^{\prime}\right)=\phi\left(s,a,s^{\prime}\right)^{\top}w,$
    |  | (3) |'
- en: 'where $\phi\left(s,a,s^{\prime}\right)\in\mathbb{R}^{d}$ represents features
    of transition $\left(s,a,s^{\prime}\right)$ and $w\in\mathbb{R}^{d}$ encodes reward-specifying
    weights. This leads to a representation of the value function that decouples the
    dynamics of the environment from the rewards:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi\left(s,a,s^{\prime}\right)\in\mathbb{R}^{d}$ 代表转移 $\left(s,a,s^{\prime}\right)$
    的特征，$w\in\mathbb{R}^{d}$ 编码了奖励指定的权重。这导致了将环境的动态与奖励解耦的价值函数的表征：
- en: '|  | $Q^{\pi}(s,a)=\mathbb{E}_{s_{t}=s,a_{t}=a}\left[\sum_{i=t}^{\infty}\gamma^{i-t}\phi\left(s_{i+1},a_{i+1},s_{i+1}^{\prime}\right)\right]^{\top}w=\psi^{\pi}(s,a)^{\top}w,$
    |  |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '|  | $Q^{\pi}(s,a)=\mathbb{E}_{s_{t}=s,a_{t}=a}\left[\sum_{i=t}^{\infty}\gamma^{i-t}\phi\left(s_{i+1},a_{i+1},s_{i+1}^{\prime}\right)\right]^{\top}w=\psi^{\pi}(s,a)^{\top}w,$
    |  |'
- en: 'where we call $\psi^{\pi}(s,a)$ the successor features of $(s,a)$ under $\pi$.
    Intuitively, $\psi^{\pi}$ summarizes the dynamics induced by $\pi$ and has been
    studied within the framework of online pretraining (?, ?) by combining with skill
    discovery approaches to implicitly learn controllable successor features $\psi^{\pi}(s,a)$.
    Given a learned $\psi^{\pi}(s,a)$, the problem of task adaptation reduces to a
    linear regression derived from Equation [3](#S6.E3 "In 6.1 Representation Transfer
    ‣ 6 Task Adaptation ‣ Pretraining in Deep Reinforcement Learning: A Survey").'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '其中我们将 $\psi^{\pi}(s,a)$ 称为 $\pi$ 下 $(s,a)$ 的后继特征。从直观上看，$\psi^{\pi}$ 总结了 $\pi$
    诱导的动态，并已在在线预训练的框架内 (?, ?) 结合技能发现方法来隐式学习可控的后继特征 $\psi^{\pi}(s,a)$。给定学习到的 $\psi^{\pi}(s,a)$，任务适应的问题减少为从方程
    [3](#S6.E3 "In 6.1 Representation Transfer ‣ 6 Task Adaptation ‣ Pretraining in
    Deep Reinforcement Learning: A Survey") 推导出的线性回归问题。'
- en: 6.2 Policy Transfer
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 策略迁移
- en: A compelling alternative for task adaptation is to transfer learned behaviors.
    As discussed in previous sections, existing work has explored how to pretrain
    primitive skills that can be reused to face new tasks or a single exploratory
    policy that facilitates exploration at the beginning of task adaptation. The differences
    in pretrained behaviors result in different adaptation strategies.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 任务适应的一个有力替代方法是迁移学习到的行为。如前面章节讨论的，现有工作探索了如何预训练原始技能，这些技能可以重用于面对新任务，或者单一探索性策略，这在任务适应开始时促进探索。预训练行为的差异导致了不同的适应策略。
- en: To achieve high rewards on the downstream task with skill-conditioned policy
    ${\pi\left(a|s,z\right)}$, a straightforward strategy is to simply choose the
    skill $z$ with the best outcome and further enhance it with finetuning. However,
    a single best-performing skill can not fulfill its potential. To better combine
    diverse skills for task solving, one can view them from the perspective of hierarchical
    RL (?, ?). In hierarchical RL, the decision-making task is typically decomposed
    into a two-level hierarchy, where a meta-controller ${\pi(z\mid s)}$ decides which
    low-level policy to use for task solving, depending on the current state. This
    hierarchical scheme is agnostic to how the low-level policies are learned. Therefore,
    it is sufficient to train a meta-controller on top of the discovered skills, which
    has been proven effective for few-shot adaptation (?) and zero-shot adaptation (?).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在下游任务中实现高奖励，使用技能条件策略 ${\pi\left(a|s,z\right)}，一种直接的策略是简单地选择具有最佳结果的技能 $z$，并通过微调进一步增强它。然而，单一最佳技能不能充分发挥其潜力。为了更好地结合多样技能解决任务，可以从层次强化学习的角度来看待这些技能 (?, ?)。在层次强化学习中，决策任务通常被分解为两级层次结构，其中一个元控制器
    ${\pi(z\mid s)}$ 根据当前状态决定使用哪个低级策略来解决任务。这种层次结构对低级策略的学习方式是不可知的。因此，在发现的技能上训练一个元控制器是足够的，这在少样本适应 (?)
    和零样本适应 (?) 中已被证明是有效的。
- en: Exploratory policies, as another form of prior knowledge, benefit downstream
    tasks in a different way. Due to the importance of exploration, exploratory policies
    can provide good initialization for the agent to gather diverse experiences and
    reach high-rewarding states. For example, ? (?) validate the effectiveness of
    transferring exploratory policies trained by curiosity-driven approaches, in particular
    for domains that require structured exploration.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性策略作为另一种形式的先验知识，以不同的方式促进下游任务。由于探索的重要性，探索性策略可以为代理提供良好的初始化，以收集多样化的经验并达到高奖励状态。例如，? (?)
    验证了通过好奇心驱动方法训练的探索性策略的转移效果，特别是在需要结构化探索的领域。
- en: 'While it is always feasible to finetune pretrained policies, considerations
    should be taken in order to prevent catastrophic forgetting when learning in the
    downstream task. Catastrophic forgetting refers to the tendency of neural networks
    to disregard their previously obtained knowledge when new information is acquired.
    To mitigate this issue, one might apply knowledge distillation-like regularization
    together with RL objectives (?):'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然对预训练策略进行微调始终是可行的，但在下游任务中学习时应考虑防止灾难性遗忘。灾难性遗忘指的是神经网络在获得新信息时忽视之前获得的知识。为了缓解这个问题，可以应用类似知识蒸馏的正则化与强化学习目标 (?)
    一起使用：
- en: '|  | $\mathcal{L}_{\text{KD}}=H\left(\hat{\pi}(a\mid s)\&#124;\pi_{\theta}(a\mid
    s)\right),$ |  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{\text{KD}}=H\left(\hat{\pi}(a\mid s)\&#124;\pi_{\theta}(a\mid
    s)\right),$ |  |'
- en: where $H$ is cross entropy and $\hat{\pi}$ is the teacher policy. We refer the
    reader to ? (?) for more discussions on catastrophic forgetting in reinforcement
    learning.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $H$ 是交叉熵，$\hat{\pi}$ 是教师策略。我们请读者参考 ? (?) 以获取更多关于强化学习中灾难性遗忘的讨论。
- en: 6.3 Challenges & Future Directions
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3 挑战与未来方向
- en: Parameter Efficiency.
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 参数效率。
- en: Despite that existing pretrained models for RL have much fewer parameters as
    compared with those in the field of supervised learning, the issue of parameter
    efficiency is still important with the ever-increasing number of model parameters.
    More concretely, it is desired to design parameter-efficient transfer learning
    that updates only a small fraction of parameters while keeping most of the pretrained
    parameters intact. It has been actively studied in natural language processing (?)
    with solutions like adding small neural modules as adapters (?) and prepending
    learnable prefix tokens as soft prompts (?). Built on these techniques, several
    efforts have been made to enable parameter-efficient transfer with prompting (?, ?),
    which we believe has a large room to improve with tailored methods.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管现有的强化学习预训练模型与监督学习领域的模型相比，参数要少得多，但随着模型参数数量的不断增加，参数效率的问题仍然很重要。更具体地说，希望设计一种参数高效的迁移学习，只更新少量参数，同时保持大部分预训练参数不变。这在自然语言处理 (?)
    中得到了积极研究，解决方案包括添加小型神经模块作为适配器 (?) 和在前面添加可学习的前缀标记作为软提示 (?). 基于这些技术，已经有多个努力尝试通过提示实现参数高效迁移 (?, ?)，我们相信这种方法还有很大的改进空间。
- en: Domain adaptation.
  id: totrans-222
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 领域适应。
- en: In this section, we mainly consider task adaptation where unseen tasks are given
    in the same environment. A more challenging but practical scenario is domain adaptation.
    In domain adaptation, there exist environmental shifts between the upstream and
    downstream tasks. Despite that these environmental shifts are commonly seen in
    real-world applications, it remains a challenging problem to transfer across different
    domains (?, ?). However, we believe that this direction will rapidly evolve by
    bringing related techniques from supervised learning to reinforcement learning.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们主要考虑任务适应，其中未见过的任务在相同环境中给出。一个更具挑战性但实用的场景是领域适应。在领域适应中，上游任务和下游任务之间存在环境变化。尽管这些环境变化在现实应用中很常见，但在不同领域之间进行迁移仍然是一个挑战性的问题 (?, ?)。然而，我们相信这一方向将通过将监督学习的相关技术引入强化学习而迅速发展。
- en: Continually-developed models.
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 持续发展的模型。
- en: For practical applications, we can take a step forward and consider building
    large pretrained models continually to support added features (e.g., a modified
    action space, more powerful architectures, etc). While such consideration was
    already underway during the development of large-scale RL models (?), it requires
    a more principled way of combining updates into RL models. We refer the reader
    to recent work in this direction in the field of supervised learning (?) and reinforcement
    learning (?, ?).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实际应用，我们可以更进一步，考虑持续构建大型预训练模型以支持新增特性（例如，修改后的动作空间、更强大的架构等）。虽然在大规模 RL 模型 (?) 的开发过程中已经开始考虑这一点，但将更新结合到
    RL 模型中需要一种更为原则化的方法。我们推荐读者参考在监督学习 (?) 和强化学习 (?, ?) 领域中的相关最新研究。
- en: 7 Conclusions and Future Perspectives
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 结论与未来展望
- en: In this section, we conclude this survey and highlight several future directions
    which we believe will be important topics for future work.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们总结了本调查并强调了几个我们认为将成为未来工作重要主题的未来方向。
- en: This paper introduces pretraining in deep RL by discussing recent trends to
    obtain general prior knowledge for decision-making. In contrast to its supervised
    learning counterpart, pretraining faces a variety of challenges unique to RL.
    In this survey, we present several promising research directions to tackle these
    challenges and we believe this field will evolve rapidly in the coming years.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本文通过讨论获得决策制定的一般先验知识的最新趋势，引入了深度 RL 中的预训练。与监督学习中的对应方法相比，预训练面临着许多独特的挑战。在这项调查中，我们提出了几个有前景的研究方向来应对这些挑战，我们相信这一领域将在未来几年迅速发展。
- en: There are still several open questions that are important and remain to be addressed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然存在几个重要的未解问题，需要进一步解决。
- en: Benchmarks and evaluation metrics.
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基准和评估指标。
- en: Evaluation serves as a means for comparing various methods and driving further
    improvements. In the field of natural language processing, GLUE (?) is a widely
    used benchmark to evaluate the performance of models across various natural language
    understanding tasks. Recently, there has been a surge of research on improving
    evaluation for RL in terms of evaluation metrics (?) and benchmark datasets (?).
    To the best of our knowledge, URLB (?) is the only benchmark for pretraining in
    deep RL. It presents a unified evaluation protocol for online pretraining based
    on the DeepMind control suite (?). However, a principled evaluation framework
    for offline pretraining and generalist pretraining is still missing. We expect
    existing offline RL benchmarks like D4RL (?) and RL Unplugged (?) can serve as
    the basis for developing pretraining benchmarks, but more challenging tasks should
    better illustrate the value of pretraining.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 评估作为比较各种方法和推动进一步改进的手段。在自然语言处理领域，GLUE (?) 是一个广泛使用的基准，用于评估模型在各种自然语言理解任务中的表现。最近，关于改进强化学习的评估方面，包括评估指标 (?)
    和基准数据集 (?) 的研究激增。根据我们所知，URLB (?) 是唯一一个用于深度强化学习预训练的基准，它提供了一个基于 DeepMind 控制套件 (?)
    的统一评估协议。然而，离线预训练和通用预训练的原则性评估框架仍然缺失。我们期望现有的离线 RL 基准如 D4RL (?) 和 RL Unplugged (?)
    能够作为开发预训练基准的基础，但更具挑战性的任务应能更好地展示预训练的价值。
- en: Architecture.
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 架构。
- en: As discussed in previous sections, there has been a surge of leveraging large
    transformers for RL tasks. We expect other recent advances in model architecture
    can bring more improvements. For example, ? (?) learn large sparse models with
    a mixture of experts that simultaneously handle images and text with modality-agnostic
    routing. This has the promise to solve complex tasks at scale. Besides, one can
    also rethink existing architectures that have the potential to support large-scale
    pretraining (e.g., Progress Neural Networks (?)).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: Multi-agent RL.
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Multi-agent RL (?) is an important sub-field of RL. Extending existing pretraining
    techniques to the multi-agent scenario is non-trivial. Multi-agent RL typically
    requires socially desirable behaviors (?) and representations (?). To the best
    of our knowledge, ? (?) present the only effort in pretraining multi-agent RL
    with supervision. How to enable unsupervised pretraining for multi-agent RL remains
    unclear, which we believe is a promising research direction.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Theoretical results.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: For RL, the significant gap between theory and practice has been a long-standing
    problem, and bringing large-scale pretraining to RL may exacerbate this even further.
    Fortunately, recent theoretical studies have made efforts in terms of representation
    transfer (?) and skill-conditioned policy transfer (?). Increasing focus on theoretical
    results is likely to have profound effects on the development of more advanced
    pretraining methods.‘
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Abu-El-Haija et al. Abu-El-Haija, S., Kothari, N., Lee, J., Natsev, P., Toderici,
    G., Varadarajan, B., and Vijayanarasimhan, S. (2016). Youtube-8m: A large-scale
    video classification benchmark. CoRR, abs/1609.08675. [http://arxiv.org/abs/1609.08675](http://arxiv.org/abs/1609.08675).'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achiam et al. Achiam, J., Edwards, H., Amodei, D., and Abbeel, P. (2018). Variational
    option discovery algorithms. CoRR, abs/1807.10299. [http://arxiv.org/abs/1807.10299](http://arxiv.org/abs/1807.10299).
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agakov Agakov, D. B. F. (2004). The im algorithm: a variational approach to
    information maximization. Advances in neural information processing systems, 16(320),
    201.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. Agarwal, A., Song, Y., Sun, W., Wang, K., Wang, M., and Zhang,
    X. (2022). Provable benefits of representational transfer in reinforcement learning.
    CoRR, abs/2205.14571. DOI: 10.48550/arXiv.2205.14571.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. Agarwal, R., Machado, M. C., Castro, P. S., and Bellemare, M. G.
    (2021). Contrastive behavioral similarity embeddings for generalization in reinforcement
    learning. In 9th International Conference on Learning Representations, ICLR 2021,
    Virtual Event, Austria, May 3-7, 2021. OpenReview.net. [https://openreview.net/forum?id=qda7-sVg84](https://openreview.net/forum?id=qda7-sVg84).
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Agarwal et al. Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A., and Bellemare,
    M. G. (2022). Reincarnating reinforcement learning: Reusing prior computation
    to accelerate progress.. DOI: 10.48550/ARXIV.2206.01626.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. Agarwal, R., Schwarzer, M., Castro, P. S., Courville, A. C.,
    and Bellemare, M. (2021). Deep reinforcement learning at the edge of the statistical
    precipice. In Ranzato, M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan,
    J. W. (Eds.), Advances in Neural Information Processing Systems, Vol. 34, pp. 29304–29320\.
    Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2021/file/f514cec81cb148559cf475e7426eed5e-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/f514cec81cb148559cf475e7426eed5e-Paper.pdf).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ahn et al. Ahn, M., Brohan, A., Brown, N., Chebotar, Y., Cortes, O., David,
    B., Finn, C., Gopalakrishnan, K., Hausman, K., Herzog, A., et al. (2022). Do as
    i can, not as i say: Grounding language in robotic affordances. ArXiv preprint,
    abs/2204.01691. [https://arxiv.org/abs/2204.01691](https://arxiv.org/abs/2204.01691).'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ajay et al. Ajay, A., Kumar, A., Agrawal, P., Levine, S., and Nachum, O. (2021).
    OPAL: offline primitive discovery for accelerating offline reinforcement learning.
    In 9th International Conference on Learning Representations, ICLR 2021, Virtual
    Event, Austria, May 3-7, 2021. OpenReview.net. [https://openreview.net/forum?id=V69LGwJ0lIN](https://openreview.net/forum?id=V69LGwJ0lIN).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Akkaya et al. Akkaya, I., Andrychowicz, M., Chociej, M., Litwin, M., McGrew,
    B., Petron, A., Paino, A., Plappert, M., Powell, G., Ribas, R., et al. (2019).
    Solving rubik’s cube with a robot hand. ArXiv preprint, abs/1910.07113. [https://arxiv.org/abs/1910.07113](https://arxiv.org/abs/1910.07113).
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allen et al. Allen, C., Parikh, N., Gottesman, O., and Konidaris, G. (2021).
    Learning markov state abstractions for deep reinforcement learning. Advances in
    Neural Information Processing Systems, 34, 8229–8241.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Anand et al. Anand, A., Racah, E., Ozair, S., Bengio, Y., Côté, M., and Hjelm,
    R. D. (2019). Unsupervised state representation learning in atari. In Wallach,
    H. M., Larochelle, H., Beygelzimer, A., d’Alché-Buc, F., Fox, E. B., and Garnett,
    R. (Eds.), Advances in Neural Information Processing Systems 32: Annual Conference
    on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019,
    Vancouver, BC, Canada, pp. 8766–8779. [https://proceedings.neurips.cc/paper/2019/hash/6fb52e71b837628ac16539c1ff911667-Abstract.html](https://proceedings.neurips.cc/paper/2019/hash/6fb52e71b837628ac16539c1ff911667-Abstract.html).'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Argall et al. Argall, B. D., Chernova, S., Veloso, M., and Browning, B. (2009).
    A survey of robot learning from demonstration. Robotics and Autonomous Systems,
    57(5), 469–483. DOI: https://doi.org/10.1016/j.robot.2008.10.024.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Badia et al. Badia, A. P., Sprechmann, P., Vitvitskyi, A., Guo, D., Piot, B.,
    Kapturowski, S., Tieleman, O., Arjovsky, M., Pritzel, A., Bolt, A., and Blundell,
    C. (2020). Never give up: Learning directed exploration strategies. In 8th International
    Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April
    26-30, 2020. OpenReview.net. [https://openreview.net/forum?id=Sye57xStvB](https://openreview.net/forum?id=Sye57xStvB).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. Bahdanau, D., Cho, K., and Bengio, Y. (2015). Neural machine
    translation by jointly learning to align and translate. In Bengio, Y.,  and LeCun,
    Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 2015,
    San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473).
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Baker et al. Baker, B., Akkaya, I., Zhokhov, P., Huizinga, J., Tang, J., Ecoffet,
    A., Houghton, B., Sampedro, R., and Clune, J. (2022). Video pretraining (vpt):
    Learning to act by watching unlabeled online videos. ArXiv preprint, abs/2206.11795.
    [https://arxiv.org/abs/2206.11795](https://arxiv.org/abs/2206.11795).'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barreto et al. Barreto, A., Dabney, W., Munos, R., Hunt, J. J., Schaul, T.,
    Silver, D., and van Hasselt, H. (2017). Successor features for transfer in reinforcement
    learning. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R.,
    Vishwanathan, S. V. N., and Garnett, R. (Eds.), Advances in Neural Information
    Processing Systems 30: Annual Conference on Neural Information Processing Systems
    2017, December 4-9, 2017, Long Beach, CA, USA, pp. 4055–4065. [https://proceedings.neurips.cc/paper/2017/hash/350db081a661525235354dd3e19b8c05-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/350db081a661525235354dd3e19b8c05-Abstract.html).'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Barto and Mahadevan Barto, A. G.,  and Mahadevan, S. (2003). Recent advances
    in hierarchical reinforcement learning. Discret. Event Dyn. Syst., 13(1-2), 41–77.
    DOI: 10.1023/A:1022140919877.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baumli et al. Baumli, K., Warde-Farley, D., Hansen, S., and Mnih, V. (2021).
    Relative variational intrinsic control. Proceedings of the AAAI Conference on
    Artificial Intelligence, 35(8), 6732–6740. DOI: 10.1609/aaai.v35i8.16832.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bellemare et al. Bellemare, M. G., Srinivasan, S., Ostrovski, G., Schaul, T.,
    Saxton, D., and Munos, R. (2016). Unifying count-based exploration and intrinsic
    motivation. In Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett,
    R. (Eds.), Advances in Neural Information Processing Systems 29: Annual Conference
    on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona,
    Spain, pp. 1471–1479. [https://proceedings.neurips.cc/paper/2016/hash/afda332245e2af431fb7b672a68b659d-Abstract.html](https://proceedings.neurips.cc/paper/2016/hash/afda332245e2af431fb7b672a68b659d-Abstract.html).'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Berner et al. Berner, C., Brockman, G., Chan, B., Cheung, V., Debiak, P., Dennison,
    C., Farhi, D., Fischer, Q., Hashme, S., Hesse, C., Józefowicz, R., Gray, S., Olsson,
    C., Pachocki, J., Petrov, M., de Oliveira Pinto, H. P., Raiman, J., Salimans,
    T., Schlatter, J., Schneider, J., Sidor, S., Sutskever, I., Tang, J., Wolski,
    F., and Zhang, S. (2019). Dota 2 with large scale deep reinforcement learning.
    CoRR, abs/1912.06680. [http://arxiv.org/abs/1912.06680](http://arxiv.org/abs/1912.06680).
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bommasani et al. Bommasani, R., Hudson, D. A., Adeli, E., Altman, R., Arora,
    S., von Arx, S., Bernstein, M. S., Bohg, J., Bosselut, A., Brunskill, E., et al.
    (2021). On the opportunities and risks of foundation models. ArXiv preprint, abs/2108.07258.
    [https://arxiv.org/abs/2108.07258](https://arxiv.org/abs/2108.07258).
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Brown et al. Brown, T. B., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal,
    P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss,
    A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D. M., Wu, J.,
    Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B.,
    Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei,
    D. (2020). Language models are few-shot learners. In Larochelle, H., Ranzato,
    M., Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual. [https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html).'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burda et al. Burda, Y., Edwards, H., Pathak, D., Storkey, A. J., Darrell, T.,
    and Efros, A. A. (2019a). Large-scale study of curiosity-driven learning. In 7th
    International Conference on Learning Representations, ICLR 2019, New Orleans,
    LA, USA, May 6-9, 2019. OpenReview.net. [https://openreview.net/forum?id=rJNwDjAqYX](https://openreview.net/forum?id=rJNwDjAqYX).
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Burda et al. Burda, Y., Edwards, H., Storkey, A. J., and Klimov, O. (2019b).
    Exploration by random network distillation. In 7th International Conference on
    Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net.
    [https://openreview.net/forum?id=H1lJJnR5Ym](https://openreview.net/forum?id=H1lJJnR5Ym).
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Campos et al. Campos, V., Sprechmann, P., Hansen, S. S., Barreto, A., Kapturowski,
    S., Vitvitskyi, A., Badia, A. P., and Blundell, C. (2021). Beyond fine-tuning:
    Transferring behavior in reinforcement learning. In ICML 2021 Workshop on Unsupervised
    Reinforcement Learning. [https://openreview.net/forum?id=4NUhTHom2HZ](https://openreview.net/forum?id=4NUhTHom2HZ).'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Campos et al. Campos, V., Trott, A., Xiong, C., Socher, R., Giró-i-Nieto, X.,
    and Torres, J. (2020). Explore, discover and learn: Unsupervised discovery of
    state-covering skills. In Proceedings of the 37th International Conference on
    Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings
    of Machine Learning Research, pp. 1317–1327. PMLR. [http://proceedings.mlr.press/v119/campos20a.html](http://proceedings.mlr.press/v119/campos20a.html).'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Castro and Precup Castro, P. S.,  and Precup, D. (2010). Using bisimulation
    for policy transfer in mdps. In Fox, M.,  and Poole, D. (Eds.), Proceedings of
    the Twenty-Fourth AAAI Conference on Artificial Intelligence, AAAI 2010, Atlanta,
    Georgia, USA, July 11-15, 2010. AAAI Press. [http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1907](http://www.aaai.org/ocs/index.php/AAAI/AAAI10/paper/view/1907).
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chebotar et al. Chebotar, Y., Hausman, K., Lu, Y., Xiao, T., Kalashnikov, D.,
    Varley, J., Irpan, A., Eysenbach, B., Julian, R., Finn, C., and Levine, S. (2021).
    Actionable models: Unsupervised offline reinforcement learning of robotic skills.
    In Meila, M.,  and Zhang, T. (Eds.), Proceedings of the 38th International Conference
    on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings
    of Machine Learning Research, pp. 1518–1528\. PMLR. [http://proceedings.mlr.press/v139/chebotar21a.html](http://proceedings.mlr.press/v139/chebotar21a.html).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin,
    M., Abbeel, P., Srinivas, A., and Mordatch, I. (2021a). Decision transformer:
    Reinforcement learning via sequence modeling. Advances in neural information processing
    systems, 34, 15084–15097.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. Chen, S., Zhu, M., Ye, D., Zhang, W., Fu, Q., and Yang, W. (2021b).
    Which heroes to pick? learning to draft in moba games with neural networks and
    tree search. IEEE Transactions on Games, 13(4), 410–421.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. Chen, T., Kornblith, S., Norouzi, M., and Hinton, G. E. (2020).
    A simple framework for contrastive learning of visual representations. In Proceedings
    of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
    2020, Virtual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 1597–1607.
    PMLR. [http://proceedings.mlr.press/v119/chen20j.html](http://proceedings.mlr.press/v119/chen20j.html).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choi et al. Choi, J., Sharma, A., Lee, H., Levine, S., and Gu, S. S. (2021).
    Variational empowerment as representation learning for goal-conditioned reinforcement
    learning. In Meila, M.,  and Zhang, T. (Eds.), Proceedings of the 38th International
    Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139
    of Proceedings of Machine Learning Research, pp. 1953–1963\. PMLR. [http://proceedings.mlr.press/v139/choi21b.html](http://proceedings.mlr.press/v139/choi21b.html).
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chowdhery et al. Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
    G., Roberts, A., Barham, P., Chung, H. W., Sutton, C., Gehrmann, S., et al. (2022).
    Palm: Scaling language modeling with pathways. ArXiv preprint, abs/2204.02311.
    [https://arxiv.org/abs/2204.02311](https://arxiv.org/abs/2204.02311).'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Christiano et al. Christiano, P. F., Shah, Z., Mordatch, I., Schneider, J.,
    Blackwell, T., Tobin, J., Abbeel, P., and Zaremba, W. (2016). Transfer from simulation
    to real world through learning deep inverse dynamics model. CoRR, abs/1610.03518.
    [http://arxiv.org/abs/1610.03518](http://arxiv.org/abs/1610.03518).
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cobbe et al. Cobbe, K., Hesse, C., Hilton, J., and Schulman, J. (2020). Leveraging
    procedural generation to benchmark reinforcement learning. In III, H. D.,  and Singh,
    A. (Eds.), Proceedings of the 37th International Conference on Machine Learning,
    Vol. 119 of Proceedings of Machine Learning Research, pp. 2048–2056. PMLR. [https://proceedings.mlr.press/v119/cobbe20a.html](https://proceedings.mlr.press/v119/cobbe20a.html).
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Codevilla et al. Codevilla, F., Santana, E., Lopez, A. M., and Gaidon, A. (2019).
    Exploring the limitations of behavior cloning for autonomous driving. In Proceedings
    of the IEEE/CVF International Conference on Computer Vision (ICCV).
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Colas et al. Colas, C., Karch, T., Sigaud, O., and Oudeyer, P.-Y. (2022). Autotelic
    agents with intrinsically motivated goal-conditioned reinforcement learning: A
    short survey. J. Artif. Int. Res., 74. DOI: 10.1613/jair.1.13554.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dance et al. Dance, C. R., Perez, J., and Cachet, T. (2021). Demonstration-conditioned
    reinforcement learning for few-shot imitation. In Meila, M.,  and Zhang, T. (Eds.),
    Proceedings of the 38th International Conference on Machine Learning, Vol. 139
    of Proceedings of Machine Learning Research, pp. 2376–2387. PMLR. [https://proceedings.mlr.press/v139/dance21a.html](https://proceedings.mlr.press/v139/dance21a.html).
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Li, F. (2009).
    Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society
    Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June
    2009, Miami, Florida, USA, pp. 248–255\. IEEE Computer Society. DOI: 10.1109/CVPR.2009.5206848.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Devlin et al. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019).
    BERT: Pre-training of deep bidirectional transformers for language understanding.
    In Proceedings of the 2019 Conference of the North American Chapter of the Association
    for Computational Linguistics: Human Language Technologies, Volume 1 (Long and
    Short Papers), pp. 4171–4186, Minneapolis, Minnesota. Association for Computational
    Linguistics. DOI: 10.18653/v1/N19-1423.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dinh et al. Dinh, L., Sohl-Dickstein, J., and Bengio, S. (2017). Density estimation
    using real NVP. In 5th International Conference on Learning Representations, ICLR
    2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net.
    [https://openreview.net/forum?id=HkpbnH9lx](https://openreview.net/forum?id=HkpbnH9lx).
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dittadi et al. Dittadi, A., Träuble, F., Locatello, F., Wuthrich, M., Agrawal,
    V., Winther, O., Bauer, S., and Schölkopf, B. (2021). On the transfer of disentangled
    representations in realistic settings. In 9th International Conference on Learning
    Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
    [https://openreview.net/forum?id=8VXvj1QNRl1](https://openreview.net/forum?id=8VXvj1QNRl1).
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du et al. Du, Y., Gan, C., and Isola, P. (2021). Curious representation learning
    for embodied intelligence. In Proceedings of the IEEE/CVF International Conference
    on Computer Vision, pp. 10408–10417.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ecoffet et al. Ecoffet, A., Huizinga, J., Lehman, J., Stanley, K. O., and Clune,
    J. (2021). First return, then explore. Nature, 590(7847), 580–586.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Efroni et al. Efroni, Y., Misra, D., Krishnamurthy, A., Agarwal, A., and Langford,
    J. (2021). Provable RL with exogenous distractors via multistep inverse dynamics.
    CoRR, abs/2110.08847. [https://arxiv.org/abs/2110.08847](https://arxiv.org/abs/2110.08847).
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eysenbach et al. Eysenbach, B., Chaudhari, S., Asawa, S., Levine, S., and Salakhutdinov,
    R. (2021). Off-dynamics reinforcement learning: Training for transfer with domain
    classifiers. In 9th International Conference on Learning Representations, ICLR
    2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. [https://openreview.net/forum?id=eqBwg3AcIAK](https://openreview.net/forum?id=eqBwg3AcIAK).'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Eysenbach et al. Eysenbach, B., Gupta, A., Ibarz, J., and Levine, S. (2019).
    Diversity is all you need: Learning skills without a reward function. In 7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019. OpenReview.net. [https://openreview.net/forum?id=SJx63jRqFm](https://openreview.net/forum?id=SJx63jRqFm).'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eysenbach et al. Eysenbach, B., Salakhutdinov, R., and Levine, S. (2022). The
    information geometry of unsupervised reinforcement learning. In The Tenth International
    Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29,
    2022. OpenReview.net. [https://openreview.net/forum?id=3wU2UX0voE](https://openreview.net/forum?id=3wU2UX0voE).
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H.,
    Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A. (2022). Minedojo: Building
    open-ended embodied agents with internet-scale knowledge. ArXiv preprint, abs/2206.08853.
    [https://arxiv.org/abs/2206.08853](https://arxiv.org/abs/2206.08853).'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ferns et al. Ferns, N., Panangaden, P., and Precup, D. (2004). Metrics for finite
    markov decision processes.. In UAI, Vol. 4, pp. 162–169.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Florence et al. Florence, P., Lynch, C., Zeng, A., Ramirez, O. A., Wahid, A.,
    Downs, L., Wong, A., Lee, J., Mordatch, I., and Tompson, J. (2022). Implicit behavioral
    cloning. In Conference on Robot Learning, pp. 158–168\. PMLR.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Frank and Wolfe Frank, M.,  and Wolfe, P. (1956). An algorithm for quadratic
    programming. Naval Research Logistics Quarterly, 3(1-2), 95–110. DOI: https://doi.org/10.1002/nav.3800030109.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fu et al. Fu, J., Kumar, A., Nachum, O., Tucker, G., and Levine, S. (2020).
    D4RL: datasets for deep data-driven reinforcement learning. CoRR, abs/2004.07219.
    [https://arxiv.org/abs/2004.07219](https://arxiv.org/abs/2004.07219).'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furuta et al. Furuta, H., Matsuo, Y., and Gu, S. S. (2022). Generalized decision
    transformer for offline hindsight information matching. In International Conference
    on Learning Representations. [https://openreview.net/forum?id=CAjxVodl_v](https://openreview.net/forum?id=CAjxVodl_v).
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gehring et al. Gehring, J., Synnaeve, G., Krause, A., and Usunier, N. (2021).
    Hierarchical skills for efficient exploration. Advances in Neural Information
    Processing Systems, 34, 11553–11564.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gelada et al. Gelada, C., Kumar, S., Buckman, J., Nachum, O., and Bellemare,
    M. G. (2019). Deepmdp: Learning continuous latent space models for representation
    learning. In Chaudhuri, K.,  and Salakhutdinov, R. (Eds.), Proceedings of the
    36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019,
    Long Beach, California, USA, Vol. 97 of Proceedings of Machine Learning Research,
    pp. 2170–2179\. PMLR. [http://proceedings.mlr.press/v97/gelada19a.html](http://proceedings.mlr.press/v97/gelada19a.html).'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Givan et al. Givan, R., Dean, T., and Greig, M. (2003). Equivalence notions
    and model minimization in markov decision processes. Artificial Intelligence,
    147(1-2), 163–223.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow and Vinyals Goodfellow, I. J.,  and Vinyals, O. (2015). Qualitatively
    characterizing neural network optimization problems. In Bengio, Y.,  and LeCun,
    Y. (Eds.), 3rd International Conference on Learning Representations, ICLR 2015,
    San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings. [http://arxiv.org/abs/1412.6544](http://arxiv.org/abs/1412.6544).
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gregor et al. Gregor, K., Rezende, D. J., and Wierstra, D. (2016). Variational
    intrinsic control. ArXiv preprint, abs/1611.07507. [https://arxiv.org/abs/1611.07507](https://arxiv.org/abs/1611.07507).
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grill et al. Grill, J., Strub, F., Altché, F., Tallec, C., Richemond, P. H.,
    Buchatskaya, E., Doersch, C., Pires, B. Á., Guo, Z., Azar, M. G., Piot, B., Kavukcuoglu,
    K., Munos, R., and Valko, M. (2020). Bootstrap your own latent - A new approach
    to self-supervised learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan,
    M., and Lin, H. (Eds.), Advances in Neural Information Processing Systems 33:
    Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020,
    December 6-12, 2020, virtual. [https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/f3ada80d5c4ee70142b17b8192b2958e-Abstract.html).'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gulcehre et al. Gulcehre, C., Wang, Z., Novikov, A., Paine, T., Gómez, S.,
    Zolna, K., Agarwal, R., Merel, J. S., Mankowitz, D. J., Paduraru, C., Dulac-Arnold,
    G., Li, J., Norouzi, M., Hoffman, M., Heess, N., and de Freitas, N. (2020). Rl
    unplugged: A suite of benchmarks for offline reinforcement learning. In Larochelle,
    H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances in Neural
    Information Processing Systems, Vol. 33, pp. 7248–7259\. Curran Associates, Inc.
    [https://proceedings.neurips.cc/paper/2020/file/51200d29d1fc15f5a71c1dab4bb54f7c-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/51200d29d1fc15f5a71c1dab4bb54f7c-Paper.pdf).'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. Guo, Z. D., Azar, M. G., Saade, A., Thakoor, S., Piot, B., Pires,
    B. A., Valko, M., Mesnard, T., Lattimore, T., and Munos, R. (2021). Geometric
    entropic exploration. ArXiv preprint, abs/2101.02055. [https://arxiv.org/abs/2101.02055](https://arxiv.org/abs/2101.02055).
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gupta et al. Gupta, A., Kumar, V., Lynch, C., Levine, S., and Hausman, K. (2020).
    Relay policy learning: Solving long-horizon tasks via imitation and reinforcement
    learning. In Kaelbling, L. P., Kragic, D., and Sugiura, K. (Eds.), Proceedings
    of the Conference on Robot Learning, Vol. 100 of Proceedings of Machine Learning
    Research, pp. 1025–1037\. PMLR. [https://proceedings.mlr.press/v100/gupta20a.html](https://proceedings.mlr.press/v100/gupta20a.html).'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gutmann and Hyvärinen Gutmann, M.,  and Hyvärinen, A. (2010). Noise-contrastive
    estimation: A new estimation principle for unnormalized statistical models. In
    Teh, Y. W.,  and Titterington, M. (Eds.), Proceedings of the Thirteenth International
    Conference on Artificial Intelligence and Statistics, Vol. 9 of Proceedings of
    Machine Learning Research, pp. 297–304, Chia Laguna Resort, Sardinia, Italy. PMLR.
    [https://proceedings.mlr.press/v9/gutmann10a.html](https://proceedings.mlr.press/v9/gutmann10a.html).'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ha and Schmidhuber Ha, D.,  and Schmidhuber, J. (2018). World models. CoRR,
    abs/1803.10122. [http://arxiv.org/abs/1803.10122](http://arxiv.org/abs/1803.10122).
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Haber et al. Haber, N., Mrowca, D., Wang, S., Li, F., and Yamins, D. L. (2018).
    Learning to play with intrinsically-motivated, self-aware agents. In Bengio, S.,
    Wallach, H. M., Larochelle, H., Grauman, K., Cesa-Bianchi, N., and Garnett, R.
    (Eds.), Advances in Neural Information Processing Systems 31: Annual Conference
    on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018,
    Montréal, Canada, pp. 8398–8409. [https://proceedings.neurips.cc/paper/2018/hash/71e63ef5b7249cfc60852f0e0f5bf4c8-Abstract.html](https://proceedings.neurips.cc/paper/2018/hash/71e63ef5b7249cfc60852f0e0f5bf4c8-Abstract.html).'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hakhamaneshi et al. Hakhamaneshi, K., Zhao, R., Zhan, A., Abbeel, P., and Laskin,
    M. (2022). Hierarchical few-shot imitation with skill transition models. In International
    Conference on Learning Representations. [https://openreview.net/forum?id=xKZ4K0lTj_](https://openreview.net/forum?id=xKZ4K0lTj_).
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hansen et al. Hansen, S., Dabney, W., Barreto, A., Warde-Farley, D., de Wiele,
    T. V., and Mnih, V. (2020). Fast task inference with variational intrinsic successor
    features. In 8th International Conference on Learning Representations, ICLR 2020,
    Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. [https://openreview.net/forum?id=BJeAHkrYDS](https://openreview.net/forum?id=BJeAHkrYDS).
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hazan et al. Hazan, E., Kakade, S. M., Singh, K., and Soest, A. V. (2019). Provably
    efficient maximum entropy exploration. In Chaudhuri, K.,  and Salakhutdinov, R.
    (Eds.), Proceedings of the 36th International Conference on Machine Learning,
    ICML 2019, 9-15 June 2019, Long Beach, California, USA, Vol. 97 of Proceedings
    of Machine Learning Research, pp. 2681–2691\. PMLR. [http://proceedings.mlr.press/v97/hazan19a.html](http://proceedings.mlr.press/v97/hazan19a.html).
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. He, J., Zhou, C., Ma, X., Berg-Kirkpatrick, T., and Neubig, G. (2022).
    Towards a unified view of parameter-efficient transfer learning. In The Tenth
    International Conference on Learning Representations, ICLR 2022, Virtual Event,
    April 25-29, 2022. OpenReview.net. [https://openreview.net/forum?id=0RDcd5Axok](https://openreview.net/forum?id=0RDcd5Axok).
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. He, K., Fan, H., Wu, Y., Xie, S., and Girshick, R. B. (2020). Momentum
    contrast for unsupervised visual representation learning. In 2020 IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June
    13-19, 2020, pp. 9726–9735\. IEEE. DOI: 10.1109/CVPR42600.2020.00975.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. Hendrycks, D., Liu, X., Wallace, E., Dziedzic, A., Krishnan,
    R., and Song, D. (2020a). Pretrained transformers improve out-of-distribution
    robustness. In Proceedings of the 58th Annual Meeting of the Association for Computational
    Linguistics, pp. 2744–2751, Online. Association for Computational Linguistics.
    DOI: 10.18653/v1/2020.acl-main.244.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hendrycks et al. Hendrycks, D., Mu, N., Cubuk, E. D., Zoph, B., Gilmer, J.,
    and Lakshminarayanan, B. (2020b). Augmix: A simple data processing method to improve
    robustness and uncertainty. In 8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. [https://openreview.net/forum?id=S1gmrxHFvB](https://openreview.net/forum?id=S1gmrxHFvB).'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Houlsby et al. Houlsby, N., Giurgiu, A., Jastrzebski, S., Morrone, B., De Laroussilhe,
    Q., Gesmundo, A., Attariyan, M., and Gelly, S. (2019). Parameter-efficient transfer
    learning for NLP. In Chaudhuri, K.,  and Salakhutdinov, R. (Eds.), Proceedings
    of the 36th International Conference on Machine Learning, Vol. 97 of Proceedings
    of Machine Learning Research, pp. 2790–2799\. PMLR. [https://proceedings.mlr.press/v97/houlsby19a.html](https://proceedings.mlr.press/v97/houlsby19a.html).
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Houthooft et al. Houthooft, R., Chen, X., Duan, Y., Schulman, J., Turck, F. D.,
    and Abbeel, P. (2016). VIME: variational information maximizing exploration. In
    Lee, D. D., Sugiyama, M., von Luxburg, U., Guyon, I., and Garnett, R. (Eds.),
    Advances in Neural Information Processing Systems 29: Annual Conference on Neural
    Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain, pp. 1109–1117.
    [https://proceedings.neurips.cc/paper/2016/hash/abd815286ba1007abfbb8415b83ae2cf-Abstract.html](https://proceedings.neurips.cc/paper/2016/hash/abd815286ba1007abfbb8415b83ae2cf-Abstract.html).'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. Huang, B., Feng, F., Lu, C., Magliacane, S., and Zhang, K. (2022a).
    Adarl: What, where, and how to adapt in transfer reinforcement learning. In The
    Tenth International Conference on Learning Representations, ICLR 2022, Virtual
    Event, April 25-29, 2022. OpenReview.net. [https://openreview.net/forum?id=8H5bpVwvt5](https://openreview.net/forum?id=8H5bpVwvt5).'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang et al. Huang, W., Xia, F., Xiao, T., Chan, H., Liang, J., Florence, P.,
    Zeng, A., Tompson, J., Mordatch, I., Chebotar, Y., et al. (2022b). Inner monologue:
    Embodied reasoning through planning with language models. ArXiv preprint, abs/2207.05608.
    [https://arxiv.org/abs/2207.05608](https://arxiv.org/abs/2207.05608).'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hüllermeier and Waegeman Hüllermeier, E.,  and Waegeman, W. (2021). Aleatoric
    and epistemic uncertainty in machine learning: An introduction to concepts and
    methods. Machine Learning, 110(3), 457–506.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jaegle et al. Jaegle, A., Borgeaud, S., Alayrac, J.-B., Doersch, C., Ionescu,
    C., Ding, D., Koppula, S., Zoran, D., Brock, A., Shelhamer, E., Henaff, O. J.,
    Botvinick, M., Zisserman, A., Vinyals, O., and Carreira, J. (2022). Perceiver
    IO: A general architecture for structured inputs & outputs. In International Conference
    on Learning Representations. [https://openreview.net/forum?id=fILj7WpI-g](https://openreview.net/forum?id=fILj7WpI-g).'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Janner et al. Janner, M., Li, Q., and Levine, S. (2021). Offline reinforcement
    learning as one big sequence modeling problem. Advances in neural information
    processing systems, 34, 1273–1286.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Janny et al. Janny, S., Baradel, F., Neverova, N., Nadri, M., Mori, G., and Wolf,
    C. (2022). Filtered-cophy: Unsupervised learning of counterfactual physics in
    pixel space. In The Tenth International Conference on Learning Representations,
    ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. [https://openreview.net/forum?id=1L0C5ROtFp](https://openreview.net/forum?id=1L0C5ROtFp).'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jin et al. Jin, C., Liu, Q., and Miryoosefi, S. (2021). Bellman eluder dimension:
    New rich classes of rl problems, and sample-efficient algorithms. In Ranzato,
    M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (Eds.), Advances
    in Neural Information Processing Systems, Vol. 34, pp. 13406–13418\. Curran Associates,
    Inc. [https://proceedings.neurips.cc/paper/2021/file/6f5e4e86a87220e5d361ad82f1ebc335-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/6f5e4e86a87220e5d361ad82f1ebc335-Paper.pdf).'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaiser et al. Kaiser, L., Babaeizadeh, M., Milos, P., Osinski, B., Campbell,
    R. H., Czechowski, K., Erhan, D., Finn, C., Kozakowski, P., Levine, S., Mohiuddin,
    A., Sepassi, R., Tucker, G., and Michalewski, H. (2020). Model based reinforcement
    learning for atari. In 8th International Conference on Learning Representations,
    ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. [https://openreview.net/forum?id=S1xCPJHtDB](https://openreview.net/forum?id=S1xCPJHtDB).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kamienny et al. Kamienny, P.-A., Tarbouriech, J., Lazaric, A., and Denoyer,
    L. (2022). Direct then diffuse: Incremental unsupervised skill discovery for state
    covering and goal reaching. In International Conference on Learning Representations.
    [https://openreview.net/forum?id=25kzAhUB1lz](https://openreview.net/forum?id=25kzAhUB1lz).'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaplan et al. Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B., Chess,
    B., Child, R., Gray, S., Radford, A., Wu, J., and Amodei, D. (2020). Scaling laws
    for neural language models. CoRR, abs/2001.08361. [https://arxiv.org/abs/2001.08361](https://arxiv.org/abs/2001.08361).
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kapturowski et al. Kapturowski, S., Campos, V., Jiang, R., Rakićević, N., van
    Hasselt, H., Blundell, C., and Badia, A. P. (2022). Human-level atari 200x faster..
    DOI: 10.48550/ARXIV.2209.07550.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khetarpal et al. Khetarpal, K., Riemer, M., Rish, I., and Precup, D. (2020).
    Towards continual reinforcement learning: A review and perspectives. CoRR, abs/2012.13490.
    [https://arxiv.org/abs/2012.13490](https://arxiv.org/abs/2012.13490).'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kingma and Welling Kingma, D. P.,  and Welling, M. (2014). Auto-encoding variational
    bayes. In Bengio, Y.,  and LeCun, Y. (Eds.), 2nd International Conference on Learning
    Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track
    Proceedings. [http://arxiv.org/abs/1312.6114](http://arxiv.org/abs/1312.6114).
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kipf et al. Kipf, T., Li, Y., Dai, H., Zambaldi, V. F., Sanchez-Gonzalez, A.,
    Grefenstette, E., Kohli, P., and Battaglia, P. W. (2019). Compile: Compositional
    imitation learning and execution. In Chaudhuri, K.,  and Salakhutdinov, R. (Eds.),
    Proceedings of the 36th International Conference on Machine Learning, ICML 2019,
    9-15 June 2019, Long Beach, California, USA, Vol. 97 of Proceedings of Machine
    Learning Research, pp. 3418–3428\. PMLR. [http://proceedings.mlr.press/v97/kipf19a.html](http://proceedings.mlr.press/v97/kipf19a.html).'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kirk et al. Kirk, R., Zhang, A., Grefenstette, E., and Rocktäschel, T. (2021).
    A survey of generalisation in deep reinforcement learning. CoRR, abs/2111.09794.
    [https://arxiv.org/abs/2111.09794](https://arxiv.org/abs/2111.09794).
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kollar et al. Kollar, T., Tellex, S., Roy, D., and Roy, N. (2010). Toward understanding
    natural language directions. In 2010 5th ACM/IEEE International Conference on
    Human-Robot Interaction (HRI), pp. 259–266. DOI: 10.1109/HRI.2010.5453186.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kontoyiannis et al. Kontoyiannis, I., Algoet, P., Suhov, Y., and Wyner, A. (1998).
    Nonparametric entropy estimation for stationary processes and random fields, with
    applications to english text. IEEE Transactions on Information Theory, 44(3),
    1319–1327. DOI: 10.1109/18.669425.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kostrikov et al. Kostrikov, I., Nair, A., and Levine, S. (2022). Offline reinforcement
    learning with implicit q-learning. In The Tenth International Conference on Learning
    Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net.
    [https://openreview.net/forum?id=68n2s9ZJWF8](https://openreview.net/forum?id=68n2s9ZJWF8).
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kulkarni et al. Kulkarni, T. D., Narasimhan, K., Saeedi, A., and Tenenbaum,
    J. (2016). Hierarchical deep reinforcement learning: Integrating temporal abstraction
    and intrinsic motivation. In Lee, D., Sugiyama, M., Luxburg, U., Guyon, I., and Garnett,
    R. (Eds.), Advances in Neural Information Processing Systems, Vol. 29\. Curran
    Associates, Inc. [https://proceedings.neurips.cc/paper/2016/file/f442d33fa06832082290ad8544a8da27-Paper.pdf](https://proceedings.neurips.cc/paper/2016/file/f442d33fa06832082290ad8544a8da27-Paper.pdf).'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. Kumar, A., Fu, J., Soh, M., Tucker, G., and Levine, S. (2019).
    Stabilizing off-policy q-learning via bootstrapping error reduction. In Wallach,
    H., Larochelle, H., Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R.
    (Eds.), Advances in Neural Information Processing Systems, Vol. 32\. Curran Associates,
    Inc. [https://proceedings.neurips.cc/paper/2019/file/c2073ffa77b5357a498057413bb09d3a-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/c2073ffa77b5357a498057413bb09d3a-Paper.pdf).
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kumar et al. Kumar, A., Zhou, A., Tucker, G., and Levine, S. (2020). Conservative
    q-learning for offline reinforcement learning. In Larochelle, H., Ranzato, M.,
    Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances in Neural Information Processing
    Systems, Vol. 33, pp. 1179–1191\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/0d2b2061826a5df3221116a5085a6052-Paper.pdf).
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lambert et al. Lambert, N., Wulfmeier, M., Whitney, W., Byravan, A., Bloesch,
    M., Dasagi, V., Hertweck, T., and Riedmiller, M. (2022). The challenges of exploration
    for offline reinforcement learning. ArXiv preprint, abs/2201.11861. [https://arxiv.org/abs/2201.11861](https://arxiv.org/abs/2201.11861).
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lange et al. Lange, S., Gabel, T., and Riedmiller, M. (2012). Batch reinforcement
    learning. In Reinforcement learning, pp. 45–73\. Springer.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lange and Riedmiller Lange, S.,  and Riedmiller, M. A. (2010). Deep auto-encoder
    neural networks in reinforcement learning. In International Joint Conference on
    Neural Networks, IJCNN 2010, Barcelona, Spain, 18-23 July, 2010, pp. 1–8\. IEEE.
    DOI: 10.1109/IJCNN.2010.5596468.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Larsen and Skou Larsen, K. G.,  and Skou, A. (1991). Bisimulation through probabilistic
    testing. Information and computation, 94(1), 1–28.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laskin et al. Laskin, M., Liu, H., Peng, X. B., Yarats, D., Rajeswaran, A.,
    and Abbeel, P. (2022). CIC: contrastive intrinsic control for unsupervised skill
    discovery. CoRR, abs/2202.00161. [https://arxiv.org/abs/2202.00161](https://arxiv.org/abs/2202.00161).'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laskin et al. Laskin, M., Srinivas, A., and Abbeel, P. (2020). CURL: contrastive
    unsupervised representations for reinforcement learning. In Proceedings of the
    37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020,
    Virtual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 5639–5650.
    PMLR. [http://proceedings.mlr.press/v119/laskin20a.html](http://proceedings.mlr.press/v119/laskin20a.html).'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Laskin et al. Laskin, M., Yarats, D., Liu, H., Lee, K., Zhan, A., Lu, K., Cang,
    C., Pinto, L., and Abbeel, P. (2021). Urlb: Unsupervised reinforcement learning
    benchmark. ArXiv preprint, abs/2110.15191. [https://arxiv.org/abs/2110.15191](https://arxiv.org/abs/2110.15191).'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. LeCun, Y., Chopra, S., Hadsell, R., Ranzato, M., and Huang, F.
    (2006). A tutorial on energy-based learning. Predicting structured data, 1(0).
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. Lee, K.-H., Nachum, O., Yang, M., Lee, L., Freeman, D., Xu, W., Guadarrama,
    S., Fischer, I., Jang, E., Michalewski, H., et al. (2022). Multi-game decision
    transformers. ArXiv preprint, abs/2205.15241. [https://arxiv.org/abs/2205.15241](https://arxiv.org/abs/2205.15241).
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. Lee, L., Eysenbach, B., Parisotto, E., Xing, E., Levine, S., and Salakhutdinov,
    R. (2019). Efficient exploration via state marginal matching. ArXiv preprint,
    abs/1906.05274. [https://arxiv.org/abs/1906.05274](https://arxiv.org/abs/1906.05274).
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. Lee, S., Seo, Y., Lee, K., Abbeel, P., and Shin, J. (2022). Offline-to-online
    reinforcement learning via balanced replay and pessimistic q-ensemble. In Conference
    on Robot Learning, pp. 1702–1712\. PMLR.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lester et al. Lester, B., Al-Rfou, R., and Constant, N. (2021). The power of
    scale for parameter-efficient prompt tuning. In Proceedings of the 2021 Conference
    on Empirical Methods in Natural Language Processing, pp. 3045–3059, Online and
    Punta Cana, Dominican Republic. Association for Computational Linguistics. DOI: 10.18653/v1/2021.emnlp-main.243.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levine Levine, S. (2021). Understanding the world through action. In 5th Annual
    Conference on Robot Learning, Blue Sky Submission Track. [https://openreview.net/forum?id=L55-yn1iwrm](https://openreview.net/forum?id=L55-yn1iwrm).
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Levine et al. Levine, S., Kumar, A., Tucker, G., and Fu, J. (2020). Offline
    reinforcement learning: Tutorial, review, and perspectives on open problems. ArXiv
    preprint, abs/2005.01643. [https://arxiv.org/abs/2005.01643](https://arxiv.org/abs/2005.01643).'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. Li, C., Wang, T., Wu, C., Zhao, Q., Yang, J., and Zhang, C. (2021).
    Celebrating diversity in shared multi-agent reinforcement learning. In Ranzato,
    M., Beygelzimer, A., Dauphin, Y., Liang, P., and Vaughan, J. W. (Eds.), Advances
    in Neural Information Processing Systems, Vol. 34, pp. 3991–4002\. Curran Associates,
    Inc. [https://proceedings.neurips.cc/paper/2021/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf](https://proceedings.neurips.cc/paper/2021/file/20aee3a5f4643755a79ee5f6a73050ac-Paper.pdf).
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. Li, L., Walsh, T. J., and Littman, M. L. (2006). Towards a unified
    theory of state abstraction for mdps.. In AI&M.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. Li, S., Du, Y., Tenenbaum, J. B., Torralba, A., and Mordatch, I. (2022a).
    Composing ensembles of pre-trained models via iterative consensus. CoRR, abs/2210.11522.
    DOI: 10.48550/arXiv.2210.11522.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. Li, S., Puig, X., Du, Y., Wang, C., Akyurek, E., Torralba, A., Andreas,
    J., and Mordatch, I. (2022b). Pre-trained language models for interactive decision-making.
    ArXiv preprint, abs/2202.01771. [https://arxiv.org/abs/2202.01771](https://arxiv.org/abs/2202.01771).
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Abbeel Liu, H.,  and Abbeel, P. (2021a). APS: active pretraining with
    successor features. In Meila, M.,  and Zhang, T. (Eds.), Proceedings of the 38th
    International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual
    Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 6736–6747\. PMLR.
    [http://proceedings.mlr.press/v139/liu21b.html](http://proceedings.mlr.press/v139/liu21b.html).'
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu and Abbeel Liu, H.,  and Abbeel, P. (2021b). Behavior from the void: Unsupervised
    active pre-training. Advances in Neural Information Processing Systems, 34, 18459–18473.'
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. Lu, J., Batra, D., Parikh, D., and Lee, S. (2019). Vilbert: Pretraining
    task-agnostic visiolinguistic representations for vision-and-language tasks. In
    Wallach, H., Larochelle, H., Beygelzimer, A., d''Alché-Buc, F., Fox, E., and Garnett,
    R. (Eds.), Advances in Neural Information Processing Systems, Vol. 32\. Curran
    Associates, Inc. [https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/c74d97b01eae257e44aa9d5bade97baf-Paper.pdf).'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. Lu, K., Grover, A., Abbeel, P., and Mordatch, I. (2021a). Pretrained
    transformers as universal computation engines. ArXiv preprint, abs/2103.05247.
    [https://arxiv.org/abs/2103.05247](https://arxiv.org/abs/2103.05247).
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu et al. Lu, K., Grover, A., Abbeel, P., and Mordatch, I. (2021b). Reset-free
    lifelong learning with skill-space planning. In International Conference on Learning
    Representations. [https://openreview.net/forum?id=HIGSa_3kOx3](https://openreview.net/forum?id=HIGSa_3kOx3).
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lu et al. Lu, Y., Hausman, K., Chebotar, Y., Yan, M., Jang, E., Herzog, A.,
    Xiao, T., Irpan, A., Khansari, M., Kalashnikov, D., and Levine, S. (2022). Aw-opt:
    Learning robotic skills with imitation and reinforcement at scale. In Faust, A.,
    Hsu, D., and Neumann, G. (Eds.), Proceedings of the 5th Conference on Robot Learning,
    Vol. 164 of Proceedings of Machine Learning Research, pp. 1078–1088\. PMLR. [https://proceedings.mlr.press/v164/lu22a.html](https://proceedings.mlr.press/v164/lu22a.html).'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lynch et al. Lynch, C., Khansari, M., Xiao, T., Kumar, V., Tompson, J., Levine,
    S., and Sermanet, P. (2020). Learning latent plans from play. In Conference on
    robot learning, pp. 1113–1132\. PMLR.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahmoudieh et al. Mahmoudieh, P., Pathak, D., and Darrell, T. (2022). Zero-shot
    reward specification via grounded natural language. In Chaudhuri, K., Jegelka,
    S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (Eds.), Proceedings of the
    39th International Conference on Machine Learning, Vol. 162 of Proceedings of
    Machine Learning Research, pp. 14743–14752\. PMLR. [https://proceedings.mlr.press/v162/mahmoudieh22a.html](https://proceedings.mlr.press/v162/mahmoudieh22a.html).
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mavor-Parker et al. Mavor-Parker, A., Young, K., Barry, C., and Griffin, L.
    (2022). How to stay curious while avoiding noisy tvs using aleatoric uncertainty
    estimation. In International Conference on Machine Learning, pp. 15220–15240\.
    PMLR.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mazoure et al. Mazoure, B., des Combes, R. T., Doan, T., Bachman, P., and Hjelm,
    R. D. (2020). Deep reinforcement and infomax learning. In Larochelle, H., Ranzato,
    M., Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual. [https://proceedings.neurips.cc/paper/2020/hash/26588e932c7ccfa1df309280702fe1b5-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/26588e932c7ccfa1df309280702fe1b5-Abstract.html).'
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Meng et al. Meng, L., Wen, M., Yang, Y., Le, C., Li, X., Zhang, W., Wen, Y.,
    Zhang, H., Wang, J., and Xu, B. (2021). Offline pre-trained multi-agent decision
    transformer: One big sequence model tackles all SMAC tasks. CoRR, abs/2112.02845.
    [https://arxiv.org/abs/2112.02845](https://arxiv.org/abs/2112.02845).'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Misra et al. Misra, D., Henaff, M., Krishnamurthy, A., and Langford, J. (2020).
    Kinematic state abstraction and provably efficient rich-observation reinforcement
    learning. In Proceedings of the 37th International Conference on Machine Learning,
    ICML 2020, 13-18 July 2020, Virtual Event, Vol. 119 of Proceedings of Machine
    Learning Research, pp. 6961–6971. PMLR. [http://proceedings.mlr.press/v119/misra20a.html](http://proceedings.mlr.press/v119/misra20a.html).
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mustafa et al. Mustafa, B., Riquelme, C., Puigcerver, J., Jenatton, R., and Houlsby,
    N. (2022). Multimodal contrastive learning with limoe: the language-image mixture
    of experts. CoRR, abs/2206.02770. DOI: 10.48550/arXiv.2206.02770.'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mutti et al. Mutti, M., Pratissoli, L., and Restelli, M. (2020). A policy gradient
    method for task-agnostic exploration. In 4th Lifelong Machine Learning Workshop
    at ICML 2020. [https://openreview.net/forum?id=d9j_RNHtQEo](https://openreview.net/forum?id=d9j_RNHtQEo).
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nachum et al. Nachum, O., Gu, S., Lee, H., and Levine, S. (2019). Near-optimal
    representation learning for hierarchical reinforcement learning. In 7th International
    Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9,
    2019. OpenReview.net. [https://openreview.net/forum?id=H1emus0qF7](https://openreview.net/forum?id=H1emus0qF7).
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Nair et al. Nair, A., Dalal, M., Gupta, A., and Levine, S. (2020). Accelerating
    online reinforcement learning with offline datasets. ArXiv preprint, abs/2006.09359.
    [https://arxiv.org/abs/2006.09359](https://arxiv.org/abs/2006.09359).
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ndousse et al. Ndousse, K. K., Eck, D., Levine, S., and Jaques, N. (2021). Emergent
    social learning via multi-agent reinforcement learning. In Meila, M.,  and Zhang,
    T. (Eds.), Proceedings of the 38th International Conference on Machine Learning,
    Vol. 139 of Proceedings of Machine Learning Research, pp. 7991–8004. PMLR. [https://proceedings.mlr.press/v139/ndousse21a.html](https://proceedings.mlr.press/v139/ndousse21a.html).
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ostrovski et al. Ostrovski, G., Bellemare, M. G., van den Oord, A., and Munos,
    R. (2017). Count-based exploration with neural density models. In Precup, D., 
    and Teh, Y. W. (Eds.), Proceedings of the 34th International Conference on Machine
    Learning, ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, Vol. 70 of Proceedings
    of Machine Learning Research, pp. 2721–2730\. PMLR. [http://proceedings.mlr.press/v70/ostrovski17a.html](http://proceedings.mlr.press/v70/ostrovski17a.html).
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oudeyer et al. Oudeyer, P.-Y., Kaplan, F., and Hafner, V. V. (2007). Intrinsic
    motivation systems for autonomous mental development. IEEE transactions on evolutionary
    computation, 11(2), 265–286.
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parisi et al. Parisi, S., Rajeswaran, A., Purushwalkam, S., and Gupta, A. (2022).
    The unsurprising effectiveness of pre-trained vision models for control. In Chaudhuri,
    K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato, S. (Eds.), Proceedings
    of the 39th International Conference on Machine Learning, Vol. 162 of Proceedings
    of Machine Learning Research, pp. 17359–17371\. PMLR. [https://proceedings.mlr.press/v162/parisi22a.html](https://proceedings.mlr.press/v162/parisi22a.html).
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. Park, S., Choi, J., Kim, J., Lee, H., and Kim, G. (2022). Lipschitz-constrained
    unsupervised skill discovery. In International Conference on Learning Representations.
    [https://openreview.net/forum?id=BGvt0ghNgA](https://openreview.net/forum?id=BGvt0ghNgA).
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pathak et al. Pathak, D., Agrawal, P., Efros, A. A., and Darrell, T. (2017).
    Curiosity-driven exploration by self-supervised prediction. In Precup, D.,  and Teh,
    Y. W. (Eds.), Proceedings of the 34th International Conference on Machine Learning,
    ICML 2017, Sydney, NSW, Australia, 6-11 August 2017, Vol. 70 of Proceedings of
    Machine Learning Research, pp. 2778–2787\. PMLR. [http://proceedings.mlr.press/v70/pathak17a.html](http://proceedings.mlr.press/v70/pathak17a.html).
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pathak et al. Pathak, D., Gandhi, D., and Gupta, A. (2019). Self-supervised
    exploration via disagreement. In Chaudhuri, K.,  and Salakhutdinov, R. (Eds.),
    Proceedings of the 36th International Conference on Machine Learning, ICML 2019,
    9-15 June 2019, Long Beach, California, USA, Vol. 97 of Proceedings of Machine
    Learning Research, pp. 5062–5071\. PMLR. [http://proceedings.mlr.press/v97/pathak19a.html](http://proceedings.mlr.press/v97/pathak19a.html).
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pertsch et al. Pertsch, K., Lee, Y., and Lim, J. (2021a). Accelerating reinforcement
    learning with learned skill priors. In Kober, J., Ramos, F., and Tomlin, C. (Eds.),
    Proceedings of the 2020 Conference on Robot Learning, Vol. 155 of Proceedings
    of Machine Learning Research, pp. 188–204\. PMLR. [https://proceedings.mlr.press/v155/pertsch21a.html](https://proceedings.mlr.press/v155/pertsch21a.html).
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pertsch et al. Pertsch, K., Lee, Y., Wu, Y., and Lim, J. J. (2021b). Demonstration-guided
    reinforcement learning with learned skills. In 5th Annual Conference on Robot
    Learning. [https://openreview.net/forum?id=JSC4KMlENqF](https://openreview.net/forum?id=JSC4KMlENqF).
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pomerleau Pomerleau, D. A. (1988). Alvinn: An autonomous land vehicle in a
    neural network. Advances in neural information processing systems, 1.'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pong et al. Pong, V., Dalal, M., Lin, S., Nair, A., Bahl, S., and Levine, S.
    (2020). Skew-fit: State-covering self-supervised reinforcement learning. In Proceedings
    of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July
    2020, Virtual Event, Vol. 119 of Proceedings of Machine Learning Research, pp. 7783–7792.
    PMLR. [http://proceedings.mlr.press/v119/pong20a.html](http://proceedings.mlr.press/v119/pong20a.html).'
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Poole et al. Poole, B., Ozair, S., Van Den Oord, A., Alemi, A., and Tucker,
    G. (2019). On variational bounds of mutual information. In Chaudhuri, K.,  and Salakhutdinov,
    R. (Eds.), Proceedings of the 36th International Conference on Machine Learning,
    Vol. 97 of Proceedings of Machine Learning Research, pp. 5171–5180\. PMLR. [https://proceedings.mlr.press/v97/poole19a.html](https://proceedings.mlr.press/v97/poole19a.html).
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal,
    S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G., and Sutskever,
    I. (2021). Learning transferable visual models from natural language supervision.
    In Meila, M.,  and Zhang, T. (Eds.), Proceedings of the 38th International Conference
    on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings
    of Machine Learning Research, pp. 8748–8763\. PMLR. [http://proceedings.mlr.press/v139/radford21a.html](http://proceedings.mlr.press/v139/radford21a.html).
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Raffel Raffel, C. (2021). A call to build models like we build open-source software..
    [https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html](https://colinraffel.com/blog/a-call-to-build-models-like-we-build-open-source-software.html).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rajeswaran et al. Rajeswaran, A., Kumar, V., Gupta, A., Vezzani, G., Schulman,
    J., Todorov, E., and Levine, S. (2018). Learning complex dexterous manipulation
    with deep reinforcement learning and demonstrations. In Kress-Gazit, H., Srinivasa,
    S. S., Howard, T., and Atanasov, N. (Eds.), Robotics: Science and Systems XIV,
    Carnegie Mellon University, Pittsburgh, Pennsylvania, USA, June 26-30, 2018. DOI: 10.15607/RSS.2018.XIV.049.'
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rakelly et al. Rakelly, K., Gupta, A., Florensa, C., and Levine, S. (2021).
    Which mutual-information representation learning objectives are sufficient for
    control?. Advances in Neural Information Processing Systems, 34, 26345–26357.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rashid et al. Rashid, T., Peng, B., Boehmer, W., and Whiteson, S. (2020). Optimistic
    exploration even with a pessimistic initialisation. In 8th International Conference
    on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
    OpenReview.net. [https://openreview.net/forum?id=r1xGP6VYwH](https://openreview.net/forum?id=r1xGP6VYwH).
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reed et al. Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov,
    A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al.
    (2022). A generalist agent. ArXiv preprint, abs/2205.06175. [https://arxiv.org/abs/2205.06175](https://arxiv.org/abs/2205.06175).
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reid et al. Reid, M., Yamada, Y., and Gu, S. S. (2022). Can wikipedia help offline
    reinforcement learning?. ArXiv preprint, abs/2201.12122. [https://arxiv.org/abs/2201.12122](https://arxiv.org/abs/2201.12122).
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ross et al. Ross, S., Gordon, G., and Bagnell, D. (2011). A reduction of imitation
    learning and structured prediction to no-regret online learning. In Gordon, G.,
    Dunson, D., and Dudík, M. (Eds.), Proceedings of the Fourteenth International
    Conference on Artificial Intelligence and Statistics, Vol. 15 of Proceedings of
    Machine Learning Research, pp. 627–635, Fort Lauderdale, FL, USA. PMLR. [https://proceedings.mlr.press/v15/ross11a.html](https://proceedings.mlr.press/v15/ross11a.html).
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rusu et al. Rusu, A. A., Rabinowitz, N. C., Desjardins, G., Soyer, H., Kirkpatrick,
    J., Kavukcuoglu, K., Pascanu, R., and Hadsell, R. (2016). Progressive neural networks.
    CoRR, abs/1606.04671. [http://arxiv.org/abs/1606.04671](http://arxiv.org/abs/1606.04671).
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmidhuber Schmidhuber, J. (1991). Curious model-building control systems.
    In Proc. international joint conference on neural networks, pp. 1458–1463.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schmitt et al. Schmitt, S., Hudson, J. J., Zídek, A., Osindero, S., Doersch,
    C., Czarnecki, W. M., Leibo, J. Z., Küttler, H., Zisserman, A., Simonyan, K.,
    and Eslami, S. M. A. (2018). Kickstarting deep reinforcement learning. CoRR, abs/1803.03835.
    [http://arxiv.org/abs/1803.03835](http://arxiv.org/abs/1803.03835).
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwarzer et al. Schwarzer, M., Anand, A., Goel, R., Hjelm, R. D., Courville,
    A. C., and Bachman, P. (2021a). Data-efficient reinforcement learning with self-predictive
    representations. In 9th International Conference on Learning Representations,
    ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. [https://openreview.net/forum?id=uCQfPZwRaUu](https://openreview.net/forum?id=uCQfPZwRaUu).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schwarzer et al. Schwarzer, M., Rajkumar, N., Noukhovitch, M., Anand, A., Charlin,
    L., Hjelm, R. D., Bachman, P., and Courville, A. C. (2021b). Pretraining representations
    for data-efficient reinforcement learning. Advances in Neural Information Processing
    Systems, 34, 12686–12699.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sekar et al. Sekar, R., Rybkin, O., Daniilidis, K., Abbeel, P., Hafner, D.,
    and Pathak, D. (2020). Planning to explore via self-supervised world models. In
    Proceedings of the 37th International Conference on Machine Learning, ICML 2020,
    13-18 July 2020, Virtual Event, Vol. 119 of Proceedings of Machine Learning Research,
    pp. 8583–8592. PMLR. [http://proceedings.mlr.press/v119/sekar20a.html](http://proceedings.mlr.press/v119/sekar20a.html).
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seo et al. Seo, Y., Chen, L., Shin, J., Lee, H., Abbeel, P., and Lee, K. (2021).
    State entropy maximization with random encoders for efficient exploration. In
    Meila, M.,  and Zhang, T. (Eds.), Proceedings of the 38th International Conference
    on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings
    of Machine Learning Research, pp. 9443–9454\. PMLR. [http://proceedings.mlr.press/v139/seo21a.html](http://proceedings.mlr.press/v139/seo21a.html).
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Seo et al. Seo, Y., Lee, K., James, S., and Abbeel, P. (2022). Reinforcement
    learning with action-free pre-training from videos. ArXiv preprint, abs/2203.13880.
    [https://arxiv.org/abs/2203.13880](https://arxiv.org/abs/2203.13880).
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sermanet et al. Sermanet, P., Lynch, C., Chebotar, Y., Hsu, J., Jang, E., Schaal,
    S., Levine, S., and Brain, G. (2018). Time-contrastive networks: Self-supervised
    learning from video. In 2018 IEEE international conference on robotics and automation
    (ICRA), pp. 1134–1141\. IEEE.'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shah and Kumar Shah, R. M.,  and Kumar, V. (2021). RRL: resnet as representation
    for reinforcement learning. In Meila, M.,  and Zhang, T. (Eds.), Proceedings of
    the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021,
    Virtual Event, Vol. 139 of Proceedings of Machine Learning Research, pp. 9465–9476\.
    PMLR. [http://proceedings.mlr.press/v139/shah21a.html](http://proceedings.mlr.press/v139/shah21a.html).'
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shankar and Gupta Shankar, T.,  and Gupta, A. (2020). Learning robot skills
    with temporal variational inference. In III, H. D.,  and Singh, A. (Eds.), Proceedings
    of the 37th International Conference on Machine Learning, Vol. 119 of Proceedings
    of Machine Learning Research, pp. 8624–8633. PMLR. [https://proceedings.mlr.press/v119/shankar20b.html](https://proceedings.mlr.press/v119/shankar20b.html).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shankar et al. Shankar, T., Tulsiani, S., Pinto, L., and Gupta, A. (2020). Discovering
    motor programs by recomposing demonstrations. In 8th International Conference
    on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
    OpenReview.net. [https://openreview.net/forum?id=rkgHY0NYwr](https://openreview.net/forum?id=rkgHY0NYwr).
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sharma et al. Sharma, A., Gu, S., Levine, S., Kumar, V., and Hausman, K. (2020).
    Dynamics-aware unsupervised discovery of skills. In 8th International Conference
    on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.
    OpenReview.net. [https://openreview.net/forum?id=HJgLZR4KvH](https://openreview.net/forum?id=HJgLZR4KvH).
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Siegel et al. Siegel, N., Springenberg, J. T., Berkenkamp, F., Abdolmaleki,
    A., Neunert, M., Lampe, T., Hafner, R., Heess, N., and Riedmiller, M. (2020).
    Keep doing what worked: Behavior modelling priors for offline reinforcement learning.
    In International Conference on Learning Representations. [https://openreview.net/forum?id=rke7geHtwH](https://openreview.net/forum?id=rke7geHtwH).'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silver et al. Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., Van
    Den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot,
    M., et al. (2016). Mastering the game of go with deep neural networks and tree
    search. nature, 529(7587), 484–489.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silvia Silvia, P. J. (2012). Curiosity and Motivation. In The Oxford Handbook
    of Human Motivation. Oxford University Press. DOI: 10.1093/oxfordhb/9780195399820.013.0010.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Singh et al. Singh, A., Liu, H., Zhou, G., Yu, A., Rhinehart, N., and Levine,
    S. (2021). Parrot: Data-driven behavioral priors for reinforcement learning. In
    9th International Conference on Learning Representations, ICLR 2021, Virtual Event,
    Austria, May 3-7, 2021. OpenReview.net. [https://openreview.net/forum?id=Ysuv-WOFeKR](https://openreview.net/forum?id=Ysuv-WOFeKR).'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Singh et al. Singh, S. P., Barto, A. G., and Chentanez, N. (2004). Intrinsically
    motivated reinforcement learning. In Advances in Neural Information Processing
    Systems 17 [Neural Information Processing Systems, NIPS 2004, December 13-18,
    2004, Vancouver, British Columbia, Canada], pp. 1281–1288. [https://proceedings.neurips.cc/paper/2004/hash/4be5a36cbaca8ab9d2066debfe4e65c1-Abstract.html](https://proceedings.neurips.cc/paper/2004/hash/4be5a36cbaca8ab9d2066debfe4e65c1-Abstract.html).
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Smith and Gasser Smith, L.,  and Gasser, M. (2005). The development of embodied
    cognition: Six lessons from babies. Artificial life, 11(1-2), 13–29.'
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Srinivas and Abbeel Srinivas, A.,  and Abbeel, P. (2021). Unsupervised learning
    for reinforcement learning.. [https://icml.cc/media/icml-2021/Slides/10843_QHaHBNU.pdf](https://icml.cc/media/icml-2021/Slides/10843_QHaHBNU.pdf).
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stadie et al. Stadie, B. C., Levine, S., and Abbeel, P. (2015). Incentivizing
    exploration in reinforcement learning with deep predictive models. ArXiv preprint,
    abs/1507.00814. [https://arxiv.org/abs/1507.00814](https://arxiv.org/abs/1507.00814).
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stooke et al. Stooke, A., Lee, K., Abbeel, P., and Laskin, M. (2021). Decoupling
    representation learning from reinforcement learning. In Meila, M.,  and Zhang,
    T. (Eds.), Proceedings of the 38th International Conference on Machine Learning,
    ICML 2021, 18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine
    Learning Research, pp. 9870–9879\. PMLR. [http://proceedings.mlr.press/v139/stooke21a.html](http://proceedings.mlr.press/v139/stooke21a.html).
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Strehl and Littman Strehl, A. L.,  and Littman, M. L. (2008). An analysis of
    model-based interval estimation for markov decision processes. Journal of Computer
    and System Sciences, 74(8), 1309–1331.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sutton Sutton, R. (2019). The bitter lesson. Incomplete Ideas (blog), 13, 12.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sutton and Barto Sutton, R. S.,  and Barto, A. G. (2018). Reinforcement learning:
    An introduction. MIT press.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tam et al. Tam, A. C., Rabinowitz, N. C., Lampinen, A. K., Roy, N. A., Chan,
    S. C., Strouse, D., Wang, J. X., Banino, A., and Hill, F. (2022). Semantic exploration
    from language abstractions and pretrained representations. ArXiv preprint, abs/2204.05080.
    [https://arxiv.org/abs/2204.05080](https://arxiv.org/abs/2204.05080).
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. Tang, H., Houthooft, R., Foote, D., Stooke, A., Chen, X., Duan,
    Y., Schulman, J., Turck, F. D., and Abbeel, P. (2017). #exploration: A study of
    count-based exploration for deep reinforcement learning. In Guyon, I., von Luxburg,
    U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett,
    R. (Eds.), Advances in Neural Information Processing Systems 30: Annual Conference
    on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach,
    CA, USA, pp. 2753–2762. [https://proceedings.neurips.cc/paper/2017/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html](https://proceedings.neurips.cc/paper/2017/hash/3a20f62a0af1aa152670bab3c602feed-Abstract.html).'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tao et al. Tao, R. Y., Francois-Lavet, V., and Pineau, J. (2020). Novelty search
    in representational space for sample efficient exploration. In Larochelle, H.,
    Ranzato, M., Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual. [https://proceedings.neurips.cc/paper/2020/hash/5ca41a86596a5ed567d15af0be224952-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/5ca41a86596a5ed567d15af0be224952-Abstract.html).'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tarbouriech and Lazaric Tarbouriech, J.,  and Lazaric, A. (2019). Active exploration
    in markov decision processes. In Chaudhuri, K.,  and Sugiyama, M. (Eds.), The
    22nd International Conference on Artificial Intelligence and Statistics, AISTATS
    2019, 16-18 April 2019, Naha, Okinawa, Japan, Vol. 89 of Proceedings of Machine
    Learning Research, pp. 974–982\. PMLR. [http://proceedings.mlr.press/v89/tarbouriech19a.html](http://proceedings.mlr.press/v89/tarbouriech19a.html).
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tassa et al. Tassa, Y., Doron, Y., Muldal, A., Erez, T., Li, Y., de Las Casas,
    D., Budden, D., Abdolmaleki, A., Merel, J., Lefrancq, A., Lillicrap, T. P., and Riedmiller,
    M. A. (2018). Deepmind control suite. CoRR, abs/1801.00690. [http://arxiv.org/abs/1801.00690](http://arxiv.org/abs/1801.00690).
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tellex et al. Tellex, S., Kollar, T., Dickerson, S., Walter, M., Banerjee, A.,
    Teller, S., and Roy, N. (2011). Understanding natural language commands for robotic
    navigation and mobile manipulation. Proceedings of the AAAI Conference on Artificial
    Intelligence, 25(1), 1507–1514. DOI: 10.1609/aaai.v25i1.7979.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Träuble et al. Träuble, F., Dittadi, A., Wuthrich, M., Widmaier, F., Gehler,
    P. V., Winther, O., Locatello, F., Bachem, O., Schölkopf, B., and Bauer, S. (2022).
    The role of pretrained representations for the OOD generalization of RL agents.
    In International Conference on Learning Representations. [https://openreview.net/forum?id=8eb12UQYxrG](https://openreview.net/forum?id=8eb12UQYxrG).
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: van den Oord et al. van den Oord, A., Li, Y., and Vinyals, O. (2018). Representation
    learning with contrastive predictive coding. CoRR, abs/1807.03748. [http://arxiv.org/abs/1807.03748](http://arxiv.org/abs/1807.03748).
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L.,
    Gomez, A. N., Kaiser, L. u., and Polosukhin, I. (2017). Attention is all you need.
    In Guyon, I., Luxburg, U. V., Bengio, S., Wallach, H., Fergus, R., Vishwanathan,
    S., and Garnett, R. (Eds.), Advances in Neural Information Processing Systems,
    Vol. 30\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf](https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf).
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vinyals et al. Vinyals, O., Babuschkin, I., Czarnecki, W. M., Mathieu, M., Dudzik,
    A., Chung, J., Choi, D. H., Powell, R., Ewalds, T., Georgiev, P., et al. (2019).
    Grandmaster level in starcraft ii using multi-agent reinforcement learning. Nature,
    575(7782), 350–354.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. Wang, A., Singh, A., Michael, J., Hill, F., Levy, O., and Bowman,
    S. R. (2019). GLUE: A multi-task benchmark and analysis platform for natural language
    understanding. In 7th International Conference on Learning Representations, ICLR
    2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. [https://openreview.net/forum?id=rJ4km2R5t7](https://openreview.net/forum?id=rJ4km2R5t7).'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal,
    K., Mohammed, O. K., Singhal, S., Som, S., and Wei, F. (2022). Image as a foreign
    language: Beit pretraining for all vision and vision-language tasks. CoRR, abs/2208.10442.
    DOI: 10.48550/arXiv.2208.10442.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Warde-Farley et al. Warde-Farley, D., de Wiele, T. V., Kulkarni, T. D., Ionescu,
    C., Hansen, S., and Mnih, V. (2019). Unsupervised control through non-parametric
    discriminative rewards. In 7th International Conference on Learning Representations,
    ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net. [https://openreview.net/forum?id=r1eVMnA9K7](https://openreview.net/forum?id=r1eVMnA9K7).
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Watter et al. Watter, M., Springenberg, J., Boedecker, J., and Riedmiller,
    M. (2015). Embed to control: A locally linear latent dynamics model for control
    from raw images. In Cortes, C., Lawrence, N., Lee, D., Sugiyama, M., and Garnett,
    R. (Eds.), Advances in Neural Information Processing Systems, Vol. 28\. Curran
    Associates, Inc. [https://proceedings.neurips.cc/paper/2015/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf](https://proceedings.neurips.cc/paper/2015/file/a1afc58c6ca9540d057299ec3016d726-Paper.pdf).'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wei et al. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B.,
    Du, N., Dai, A. M., and Le, Q. V. (2022). Finetuned language models are zero-shot
    learners. In The Tenth International Conference on Learning Representations, ICLR
    2022, Virtual Event, April 25-29, 2022. OpenReview.net. [https://openreview.net/forum?id=gEZrGCozdqR](https://openreview.net/forum?id=gEZrGCozdqR).
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xiao et al. Xiao, T., Radosavovic, I., Darrell, T., and Malik, J. (2022). Masked
    visual pre-training for motor control. ArXiv preprint, abs/2203.06173. [https://arxiv.org/abs/2203.06173](https://arxiv.org/abs/2203.06173).
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. Xu, K., Verma, S., Finn, C., and Levine, S. (2020). Continual learning
    of control primitives : Skill discovery via reset-games. In Larochelle, H., Ranzato,
    M., Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances in Neural Information
    Processing Systems, Vol. 33, pp. 4999–5010\. Curran Associates, Inc. [https://proceedings.neurips.cc/paper/2020/file/3472ab80b6dff70c54758fd6dfc800c2-Paper.pdf](https://proceedings.neurips.cc/paper/2020/file/3472ab80b6dff70c54758fd6dfc800c2-Paper.pdf).'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. Xu, M., Shen, Y., Zhang, S., Lu, Y., Zhao, D., Tenenbaum, B. J., and Gan,
    C. (2022). Prompting decision transformer for few-shot policy generalization.
    In Thirty-ninth International Conference on Machine Learning.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yamada et al. Yamada, J., Pertsch, K., Gunjal, A., and Lim, J. J. (2022). Task-induced
    representation learning. In The Tenth International Conference on Learning Representations,
    ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net. [https://openreview.net/forum?id=OzyXtIZAzFv](https://openreview.net/forum?id=OzyXtIZAzFv).
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. Yang, M., Levine, S., and Nachum, O. (2022). TRAIL: Near-optimal
    imitation learning with suboptimal data. In International Conference on Learning
    Representations. [https://openreview.net/forum?id=6q_2b6u0BnJ](https://openreview.net/forum?id=6q_2b6u0BnJ).'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang and Nachum Yang, M.,  and Nachum, O. (2021). Representation matters: Offline
    pretraining for sequential decision making. In Meila, M.,  and Zhang, T. (Eds.),
    Proceedings of the 38th International Conference on Machine Learning, ICML 2021,
    18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine Learning Research,
    pp. 11784–11794\. PMLR. [http://proceedings.mlr.press/v139/yang21h.html](http://proceedings.mlr.press/v139/yang21h.html).'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yarats et al. Yarats, D., Brandfonbrener, D., Liu, H., Laskin, M., Abbeel,
    P., Lazaric, A., and Pinto, L. (2022). Don’t change the algorithm, change the
    data: Exploratory data for offline reinforcement learning. In ICLR 2022 Workshop
    on Generalizable Policy Learning in Physical World. [https://openreview.net/forum?id=Su-zh4a41Z5](https://openreview.net/forum?id=Su-zh4a41Z5).'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yarats et al. Yarats, D., Fergus, R., Lazaric, A., and Pinto, L. (2021). Reinforcement
    learning with prototypical representations. In Meila, M.,  and Zhang, T. (Eds.),
    Proceedings of the 38th International Conference on Machine Learning, ICML 2021,
    18-24 July 2021, Virtual Event, Vol. 139 of Proceedings of Machine Learning Research,
    pp. 11920–11931\. PMLR. [http://proceedings.mlr.press/v139/yarats21a.html](http://proceedings.mlr.press/v139/yarats21a.html).
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. Ye, D., Chen, G., Zhang, W., Chen, S., Yuan, B., Liu, B., Chen, J.,
    Liu, Z., Qiu, F., Yu, H., Yin, Y., Shi, B., Wang, L., Shi, T., Fu, Q., Yang, W.,
    Huang, L., and Liu, W. (2020). Towards playing full MOBA games with deep reinforcement
    learning. In Larochelle, H., Ranzato, M., Hadsell, R., Balcan, M., and Lin, H.
    (Eds.), Advances in Neural Information Processing Systems 33: Annual Conference
    on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020,
    virtual. [https://proceedings.neurips.cc/paper/2020/hash/06d5ae105ea1bea4d800bc96491876e9-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/06d5ae105ea1bea4d800bc96491876e9-Abstract.html).'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. Ye, D., Chen, G., Zhao, P., Qiu, F., Yuan, B., Zhang, W., Chen, S.,
    Sun, M., Li, X., Li, S., Liang, J., Lian, Z., Shi, B., Wang, L., Shi, T., Fu,
    Q., Yang, W., and Huang, L. (2022). Supervised learning achieves human-level performance
    in MOBA games: A case study of honor of kings. IEEE Trans. Neural Networks Learn.
    Syst., 33(3), 908–918. DOI: 10.1109/TNNLS.2020.3029475.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ye et al. Ye, D., Liu, Z., Sun, M., Shi, B., Zhao, P., Wu, H., Yu, H., Yang,
    S., Wu, X., Guo, Q., Chen, Q., Yin, Y., Zhang, H., Shi, T., Wang, L., Fu, Q.,
    Yang, W., and Huang, L. (2020). Mastering complex control in MOBA games with deep
    reinforcement learning. In The Thirty-Fourth AAAI Conference on Artificial Intelligence,
    AAAI 2020, The Thirty-Second Innovative Applications of Artificial Intelligence
    Conference, IAAI 2020, The Tenth AAAI Symposium on Educational Advances in Artificial
    Intelligence, EAAI 2020, New York, NY, USA, February 7-12, 2020, pp. 6672–6679\.
    AAAI Press. [https://ojs.aaai.org/index.php/AAAI/article/view/6144](https://ojs.aaai.org/index.php/AAAI/article/view/6144).
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. Yu, T., Kumar, S., Gupta, A., Levine, S., Hausman, K., and Finn,
    C. (2020). Gradient surgery for multi-task learning. In Larochelle, H., Ranzato,
    M., Hadsell, R., Balcan, M., and Lin, H. (Eds.), Advances in Neural Information
    Processing Systems 33: Annual Conference on Neural Information Processing Systems
    2020, NeurIPS 2020, December 6-12, 2020, virtual. [https://proceedings.neurips.cc/paper/2020/hash/3fe78a8acf5fda99de95303940a2420c-Abstract.html](https://proceedings.neurips.cc/paper/2020/hash/3fe78a8acf5fda99de95303940a2420c-Abstract.html).'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhan et al. Zhan, A., Zhao, P., Pinto, L., Abbeel, P., and Laskin, M. (2020).
    A framework for efficient robotic manipulation. ArXiv preprint, abs/2012.07975.
    [https://arxiv.org/abs/2012.07975](https://arxiv.org/abs/2012.07975).
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, A., Ballas, N., and Pineau, J. (2018). A dissection of overfitting
    and generalization in continuous reinforcement learning. CoRR, abs/1806.07937.
    [http://arxiv.org/abs/1806.07937](http://arxiv.org/abs/1806.07937).
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, A., McAllister, R. T., Calandra, R., Gal, Y., and Levine,
    S. (2021a). Learning invariant representations for reinforcement learning without
    reconstruction. In 9th International Conference on Learning Representations, ICLR
    2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. [https://openreview.net/forum?id=-2FCwDKRREu](https://openreview.net/forum?id=-2FCwDKRREu).
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, C., Cai, Y., Huang, L., and Li, J. (2021b). Exploration
    by maximizing rényi entropy for reward-free rl framework. In Proceedings of the
    AAAI Conference on Artificial Intelligence, pp. 10859–10867.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, J., Yu, H., and Xu, W. (2021c). Hierarchical reinforcement
    learning by discovering intrinsic options. In 9th International Conference on
    Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net.
    [https://openreview.net/forum?id=r-gPPHEjpmw](https://openreview.net/forum?id=r-gPPHEjpmw).
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. Zhang, K., Yang, Z., and Basar, T. (2019). Multi-agent reinforcement
    learning: A selective overview of theories and algorithms. CoRR, abs/1911.10635.
    [http://arxiv.org/abs/1911.10635](http://arxiv.org/abs/1911.10635).'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. Zhang, Q., Peng, Z., and Zhou, B. (2022). Learning to drive by
    watching youtube videos: Action-conditioned contrastive policy pretraining.. DOI: 10.48550/ARXIV.2204.02393.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, T., McCarthy, Z., Jow, O., Lee, D., Chen, X., Goldberg,
    K., and Abbeel, P. (2018). Deep imitation learning for complex manipulation tasks
    from virtual reality teleoperation. In 2018 IEEE International Conference on Robotics
    and Automation (ICRA), pp. 5628–5635. DOI: 10.1109/ICRA.2018.8461249.
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. Zhang, W., GX-Chen, A., Sobal, V., LeCun, Y., and Carion, N. (2022a).
    Light-weight probing of unsupervised representations for reinforcement learning.
    CoRR, abs/2208.12345. DOI: 10.48550/arXiv.2208.12345.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. Zhang, X., Song, Y., Uehara, M., Wang, M., Agarwal, A., and Sun,
    W. (2022b). Efficient reinforcement learning in block MDPs: A model-free representation
    learning approach. In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu,
    G., and Sabato, S. (Eds.), Proceedings of the 39th International Conference on
    Machine Learning, Vol. 162 of Proceedings of Machine Learning Research, pp. 26517–26547\.
    PMLR. [https://proceedings.mlr.press/v162/zhang22aa.html](https://proceedings.mlr.press/v162/zhang22aa.html).'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zheng et al. Zheng, Q., Zhang, A., and Grover, A. (2022). Online decision transformer.
    In Chaudhuri, K., Jegelka, S., Song, L., Szepesvari, C., Niu, G., and Sabato,
    S. (Eds.), Proceedings of the 39th International Conference on Machine Learning,
    Vol. 162 of Proceedings of Machine Learning Research, pp. 27042–27059\. PMLR.
    [https://proceedings.mlr.press/v162/zheng22c.html](https://proceedings.mlr.press/v162/zheng22c.html).
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhu et al. Zhu, Y., Wang, Z., Merel, J., Rusu, A., Erez, T., Cabi, S., Tunyasuvunakool,
    S., KramÃ¡r, J., Hadsell, R., de Freitas, N., and Heess, N. (2018). Reinforcement
    and imitation learning for diverse visuomotor skills. In Proceedings of Robotics:
    Science and Systems, Pittsburgh, Pennsylvania. DOI: 10.15607/RSS.2018.XIV.009.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
