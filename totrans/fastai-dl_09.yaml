- en: 'Deep Learning 2: Part 2 Lesson 9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习2：第2部分第9课
- en: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b)
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 原文：[https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b](https://medium.com/@hiromi_suenaga/deep-learning-2-part-2-lesson-9-5f0cf9e4bb5b)
- en: '*My personal notes from* [*fast.ai course*](http://www.fast.ai/)*. These notes
    will continue to be updated and improved as I continue to review the course to
    “really” understand it. Much appreciation to* [*Jeremy*](https://twitter.com/jeremyphoward)
    *and* [*Rachel*](https://twitter.com/math_rachel) *who gave me this opportunity
    to learn.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*我从* [*fast.ai课程*](http://www.fast.ai/) *中的个人笔记。这些笔记将在我继续复习课程以“真正”理解它的过程中继续更新和改进。非常感谢*
    [*Jeremy*](https://twitter.com/jeremyphoward) *和* [*Rachel*](https://twitter.com/math_rachel)
    *给了我这个学习的机会。*'
- en: Links
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 链接
- en: '[**Forum**](http://forums.fast.ai/t/part-2-lesson-9-in-class/14028/1) **/**
    [**Video**](https://youtu.be/0frKXR-2PBY)'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '[**论坛**](http://forums.fast.ai/t/part-2-lesson-9-in-class/14028/1) **/** [**视频**](https://youtu.be/0frKXR-2PBY)'
- en: Review
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回顾
- en: 'From Last week:'
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上周的内容：
- en: Pathlib; JSON
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pathlib；JSON
- en: Dictionary comprehensions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字典推导
- en: Defaultdict
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Defaultdict
- en: How to jump around fastai source
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在fastai源代码中跳转
- en: matplotlib OO API
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: matplotlib OO API
- en: Lambda functions
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lambda函数
- en: Bounding box coordinates
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界框坐标
- en: Custom head; bounding box regression
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义头部；边界框回归
- en: 'From Part 1:'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 来自第1部分：
- en: How to view model inputs from a DataLoader
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何查看DataLoader中的模型输入
- en: How to view model outputs
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何查看模型输出
- en: Data Augmentation and Bounding Box [[2:58](https://youtu.be/0frKXR-2PBY?t=2m58s)]
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据增强和边界框[[2:58](https://youtu.be/0frKXR-2PBY?t=2m58s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb)'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/pascal.ipynb)'
- en: '**Awkward rough edges of fastai:**'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**fastai的尴尬问题：**'
- en: A *classifier* is anything with dependent variable is categorical or binomial.
    As opposed to *regression* which is anything with dependent variable is continuous.
    Naming is a little confusing but will be sorted out in future. Here, `continuous`
    is `True` because our dependent variable is the coordinates of bounding box —
    hence this is actually a regressor data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '*分类器*是任何具有分类或二元因变量的东西。与*回归*相对，回归是任何具有连续因变量的东西。命名有点混乱，但将在未来得到解决。在这里，`continuous`是`True`，因为我们的因变量是边界框的坐标
    — 因此这实际上是一个回归器数据。'
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s create some data augmentation [[4:40](https://youtu.be/0frKXR-2PBY?t=4m40s)]
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 让我们创建一些数据增强[[4:40](https://youtu.be/0frKXR-2PBY?t=4m40s)]
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Normally, we use these shortcuts Jeremy created for us, but they are simply
    lists of random augmentations. But you can easily create your own (most if not
    all of them start with “Random”).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们使用Jeremy为我们创建的这些快捷方式，但它们只是随机增强的列表。但您可以轻松创建自己的（大多数，如果不是全部，都以“Random”开头）。
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As you can see, the image gets rotated and lighting varies, but bounding box
    is *not moving* and is *in a wrong spot* [[6:17](https://youtu.be/0frKXR-2PBY?t=6m17s)].
    This is the problem with data augmentations when your dependent variable is pixel
    values or in some way connected to the independent variable — they need to be
    augmented together. As you can see in the bounding box coordinates `[ 115\. 63\.
    240\. 311.]` , our image is 224 by 224 — so it is neither scaled nor cropped.
    The dependent variable needs to go through all the geometric transformation as
    the independent variables.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，图像会旋转并且光照会变化，但边界框*不会移动*，而且*位置不正确*[[6:17](https://youtu.be/0frKXR-2PBY?t=6m17s)]。这是数据增强的问题，当您的因变量是像素值或以某种方式与自变量相关联时，它们需要一起增强。如您在边界框坐标`[
    115. 63. 240. 311.]`中所看到的，我们的图像是224乘以224 — 因此它既没有缩放也没有裁剪。因变量需要经历所有几何变换，就像自变量一样。
- en: 'To do this [[7:10](https://youtu.be/0frKXR-2PBY?t=7m10s)], every transformation
    has an optional `tfm_y` parameter:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行此操作[[7:10](https://youtu.be/0frKXR-2PBY?t=7m10s)]，每个转换都有一个可选的`tfm_y`参数：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`TrmType.COORD` indicates that the *y* value represents coordinate. This needs
    to be added to all the augmentations as well as `tfms_from_model` which is responsible
    for cropping, zooming, resizing, padding, etc.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '`TrmType.COORD`表示*y*值表示坐标。这需要添加到所有增强以及`tfms_from_model`中，后者负责裁剪、缩放、调整大小、填充等。'
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, the bounding box moves with the image and is in the right spot. You may
    notice that sometimes it looks odd like the middle on in the bottom row. This
    is the constraint of the information we have. If the object occupied the corners
    of the original bounding box, your new bounding box needs to be bigger after the
    image rotates. So you must **be careful of not doing too higher rotations with
    bounding boxes** because there is not enough information for them to stay accurate.
    If we were doing polygons or segmentations, we would not have this problem.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，边界框随图像移动并位于正确位置。您可能会注意到有时看起来像底部行中间的那个奇怪。这是我们拥有的信息的限制。如果对象占据原始边界框的角落，那么在图像旋转后，您的新边界框需要更大。因此，您必须**小心不要对边界框进行过高的旋转**，因为没有足够的信息使它们保持准确。如果我们正在进行多边形或分割，我们将不会遇到这个问题。
- en: This why the box gets bigger
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么框变大了
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: So here, we do maximum of 3 degree rotation to avoid this problem [[9:14](https://youtu.be/0frKXR-2PBY?t=9m14s)].
    It also only rotates half of the time (`p=0.5`).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在这里，我们最多进行3度旋转，以避免这个问题[[9:14](https://youtu.be/0frKXR-2PBY?t=9m14s)]。它也只有一半的时间旋转（`p=0.5`）。
- en: custom_head [[9:34](https://youtu.be/0frKXR-2PBY?t=9m34s)]
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: custom_head[[9:34](https://youtu.be/0frKXR-2PBY?t=9m34s)]
- en: '`learn.summary()` will run a small batch of data through a model and prints
    out the size of tensors at every layer. As you can see, right before the `Flatten`
    layer, the tensor has the shape of 512 by 7 by 7\. So if it were a rank 1 tensor
    (i.e. a single vector) its length will be 25088 (512 * 7 * 7)and that is why our
    custom header’s input size is 25088\. Output size is 4 since it is the bounding
    box coordinates.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`learn.summary()`将通过模型运行一小批数据，并打印出每一层张量的大小。正如您所看到的，在`Flatten`层之前，张量的形状为512乘以7乘以7。因此，如果它是一个秩为1的张量（即一个单一向量），其长度将为25088（512
    * 7 * 7），这就是为什么我们自定义标题的输入大小为25088。输出大小为4，因为它是边界框坐标。'
- en: '[PRE6]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Single object detection [[10:35](https://youtu.be/0frKXR-2PBY?t=10m35s)]
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单个对象检测[[10:35](https://youtu.be/0frKXR-2PBY?t=10m35s)]
- en: Let’s combine the two to create something that can classify and localize the
    largest object in each image.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这两者结合起来，创建一个可以对每个图像中最大的对象进行分类和定位的东西。
- en: 'There are 3 things that we need to do to train a neural network:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络有3件事情我们需要做：
- en: Data
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据
- en: Architecture
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 架构
- en: Loss Function
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 损失函数
- en: 1\. Providing Data
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1. 提供数据
- en: We need a `ModelData` object whose independent variable is the images, and dependent
    variable is a tuple of bounding box coordinates and class label. There are several
    ways to do this, but here is a particularly lazy and convinient way Jeremy came
    up with is to create two `ModelData` objects representing the two different dependent
    variables we want (one with bounding boxes coordinates, one with classes).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个`ModelData`对象，其独立变量是图像，依赖变量是一个包含边界框坐标和类别标签的元组。有几种方法可以做到这一点，但这里是Jeremy想出的一个特别懒惰和方便的方法，即创建两个代表我们想要的两个不同依赖变量的`ModelData`对象（一个带有边界框坐标，一个带有类别）。
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'A dataset can be anything with `__len__` and `__getitem__`. Here''s a dataset
    that adds a 2nd label to an existing dataset:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以是任何具有`__len__`和`__getitem__`的东西。这里有一个数据集，它向现有数据集添加了第二个标签：
- en: '[PRE8]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`ds` : contains both independent and dependent variables'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ds`：包含独立和依赖变量'
- en: '`y2` : contains the additional dependent variables'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y2`：包含额外的依赖变量'
- en: '`(x, (y,self.y2[i]))` : `__getitem___` returns an independent variable and
    the combination of two dependent variables.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`(x, (y,self.y2[i]))`：`__getitem___`返回一个独立变量和两个依赖变量的组合。'
- en: We’ll use it to add the classes to the bounding boxes labels.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用它来将类别添加到边界框标签中。
- en: '[PRE9]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here is an example dependent variable:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个例子的依赖变量：
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can replace the dataloaders’ datasets with these new ones.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用这些新的数据集替换数据加载器的数据集。
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have to `denorm`alize the images from the dataloader before they can be plotted.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须在绘图之前从数据加载器中对图像进行`denorm`alize。
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 2\. Choosing Architecture [[13:54](https://youtu.be/0frKXR-2PBY?t=13m54s)]
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2. 选择架构[[13:54](https://youtu.be/0frKXR-2PBY?t=13m54s)]
- en: The architecture will be the same as the one we used for the classifier and
    bounding box regression, but we will just combine them. In other words, if we
    have `c` classes, then the number of activations we need in the final layer is
    4 plus `c`. 4 for bounding box coordinates and `c` probabilities (one per class).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 架构将与我们用于分类器和边界框回归的相同，但我们将它们结合起来。换句话说，如果我们有`c`个类别，那么最终层中所需的激活数量是4加上`c`。4用于边界框坐标和`c`个概率（每个类别一个）。
- en: We’ll use an extra linear layer this time, plus some dropout, to help us train
    a more flexible model. In general, we want our custom head to be capable of solving
    the problem on its own if the pre-trained backbone it is connected to is appropriate.
    So in this case, we are trying to do quite a bit — classifier and bounding box
    regression, so just the single linear layer does not seem enough. If you were
    wondering why there is no `BatchNorm1d` after the first `ReLU` , ResNet backbone
    already has `BatchNorm1d` as its final layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 这次我们将使用额外的线性层，再加上一些dropout，来帮助我们训练一个更灵活的模型。一般来说，如果预训练的主干适合，我们希望我们的自定义头部能够独立解决问题。因此，在这种情况下，我们尝试做了很多事情——分类器和边界框回归，所以单个线性层似乎不够。如果你想知道为什么第一个`ReLU`后面没有`BatchNorm1d`，那是因为ResNet主干已经有`BatchNorm1d`作为最后一层。
- en: '[PRE13]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 3\. Loss Function [[15:46](https://youtu.be/0frKXR-2PBY?t=15m46s)]
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3. 损失函数[[15:46](https://youtu.be/0frKXR-2PBY?t=15m46s)]
- en: The loss function needs to look at these `4 + len(cats)` activations and decide
    if they are good — whether these numbers accurately reflect the position and class
    of the largest object in the image. We know how to do this. For the first 4 activations,
    we will use L1Loss just like we did before (L1Loss is like a Mean Squared Error
    — instead of sum of squared errors, it uses sum of absolute values). For rest
    of the activations, we can use cross entropy loss.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数需要查看这些`4 + len(cats)`激活，并决定它们是否良好——这些数字是否准确反映了图像中最大对象的位置和类别。我们知道如何做到这一点。对于前4个激活，我们将像以前一样使用L1Loss（L1Loss类似于均方误差——它使用绝对值的和，而不是平方误差的和）。对于其余的激活，我们可以使用交叉熵损失。
- en: '[PRE14]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`input` : activations'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input`：激活'
- en: '`target` : ground truth'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`target`：真实值'
- en: '`bb_t,c_t = target` : Our custom dataset returns a tuple containing bounding
    box coordinates and classes. This assignment will destructure them.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bb_t,c_t = target`：我们的自定义数据集返回一个包含边界框坐标和类别的元组。这个赋值将对它们进行解构。'
- en: '`bb_i,c_i = input[:, :4], input[:, 4:]` : the first `:` is for the batch dimension.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bb_i,c_i = input[:, :4], input[:, 4:]`：第一个`:`是用于批处理维度。'
- en: '`b_i = F.sigmoid(bb_i)*224` : we know our image is 224 by 224\. `Sigmoid` will
    force it to be between 0 and 1, and multiply it by 224 to help our neural net
    to be in the range of what it has to be.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`b_i = F.sigmoid(bb_i)*224`：我们知道我们的图像是224x224。`Sigmoid`将强制它在0和1之间，并将其乘以224，以帮助我们的神经网络处于必须的范围内。'
- en: '**Question:** As a general rule, is it better to put BatchNorm before or after
    ReLU [[18:02](https://youtu.be/0frKXR-2PBY?t=18m2s)]? Jeremy would suggest to
    put it after a ReLU because BathNorm is meant to move towards zero-mean one-standard
    deviation. So if you put ReLU right after it, you are truncating it at zero so
    there is no way to create negative numbers. But if you put ReLU then BatchNorm,
    it does have that ability and gives slightly better results. Having said that,
    it is not too big of a deal either way. You see during this part of the course,
    most of the time, Jeremy does ReLU then BatchNorm but sometimes does the opposite
    when he wants to be consistent with the paper.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题：**一般规则是，在ReLU之前还是之后放置BatchNorm更好[[18:02](https://youtu.be/0frKXR-2PBY?t=18m2s)]？Jeremy建议在ReLU之后放置BatchNorm，因为BatchNorm旨在朝着零均值一标准差移动。因此，如果你在它之后放置ReLU，你就在零处截断它，所以没有办法创建负数。但如果你先放ReLU再放BatchNorm，它确实有这个能力，并且会给出稍微更好的结果。话虽如此，无论哪种方式都不是太大的问题。你会在这门课程的这部分看到，大多数时候Jeremy会先ReLU再BatchNorm，但有时会相反，当他想要与论文保持一致时。'
- en: '**Question**: What is the intuition behind using dropout after a BatchNorm?
    Doesn’t BatchNorm already do a good job of regularizing [[19:12](https://youtu.be/0frKXR-2PBY?t=19m12s)]?
    BatchNorm does an okay job of regularizing but if you think back to part 1 when
    we discussed a list of things we do to avoid overfitting and adding BatchNorm
    is one of them as is data augmentation. But it’s perfectly possible that you’ll
    still be overfitting. One nice thing about dropout is that is it has a parameter
    to say how much to drop out. Parameters are great specifically parameters that
    decide how much to regularize because it lets you build a nice big over parameterized
    model and then decide on how much to regularize it. Jeremy tends to always put
    in a drop out starting with `p=0` and then as he adds regularization, he can just
    change the dropout parameter without worrying about if he saved a model he want
    to be able to load it back, but if he had dropout layers in one but no in another,
    it will not load anymore. So this way, it stays consistent.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：在BatchNorm之后使用dropout的直觉是什么？BatchNorm不是已经很好地进行了正则化吗[[19:12](https://youtu.be/0frKXR-2PBY?t=19m12s)]？BatchNorm做正则化的效果还可以，但是如果回想第1部分，我们讨论过避免过拟合的一系列方法，添加BatchNorm是其中之一，数据增强也是其中之一。但是仍然有可能过拟合。关于dropout的一个好处是它有一个参数来指定要丢弃多少。参数非常好，特别是决定要进行多少正则化，因为它让你可以构建一个很大的超参数化模型，然后决定要进行多少正则化。Jeremy倾向于总是从`p=0`开始添加dropout，然后随着添加正则化，他可以只需更改dropout参数，而不必担心是否保存了一个模型，他希望能够重新加载它，但如果一个中有dropout层而另一个中没有，它将无法加载。这样，保持一致性。'
- en: 'Now we have out inputs and targets, we can calculate the L1 loss and add the
    cross entropy [[20:39](https://youtu.be/0frKXR-2PBY?t=20m39s)]:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了输入和目标，我们可以计算L1损失并添加交叉熵[[20:39](https://youtu.be/0frKXR-2PBY?t=20m39s)]：
- en: '`F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t)*20`'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`F.l1_loss(bb_i, bb_t) + F.cross_entropy(c_i, c_t)*20`'
- en: This is our loss function. Cross entropy and L1 loss may be of wildly different
    scales — in which case in the loss function, the larger one is going to dominate.
    In this case, Jeremy printed out the values and found out that if we multiply
    cross entropy by 20 that makes them about the same scale.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的损失函数。交叉熵和L1损失可能处于非常不同的尺度——在这种情况下，较大的那个将占主导地位。在这种情况下，Jeremy打印出值并发现如果我们将交叉熵乘以20，它们就会大致处于相同的尺度。
- en: '[PRE15]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It is nice to print out information as you train, so we grabbed L1 loss and
    added it as metrics.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时打印信息是很好的，所以我们抓取了L1损失并将其添加为指标。
- en: '[PRE16]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: A detection accuracy is in the low 80’s which is the same as what it was before.
    This is not surprising because ResNet was designed to do classification so we
    wouldn’t expect to be able to improve things in such a simple way. It certainly
    wasn’t designed to do bounding box regression. It was explicitly actually designed
    in such a way to not care about geometry — it takes the last 7 by 7 grid of activations
    and averages them all together throwing away all the information about where everything
    came from.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 检测准确率在80%左右，与之前相同。这并不令人惊讶，因为ResNet是设计用于分类的，所以我们不会指望能够以这种简单的方式改进事情。它确实不是设计用于边界框回归的。实际上，它是明确设计成不关心几何形状的——它取最后的7x7激活网格并将它们全部平均在一起，丢弃了所有关于每个位置的信息。
- en: Interestingly, when we do accuracy (classification) and bounding box at the
    same time, the L1 seems a little bit better than when we just do bounding box
    regression [[22:46](https://youtu.be/0frKXR-2PBY?t=22m46s)]. If that is counterintuitive
    to you, then this would be one of the main things to think about after this lesson
    since it is a really important idea. The idea is this — figuring out what the
    main object in an image is, is kind of the hard part. Then figuring out exactly
    where the bounding box is and what class it is is the easy part in a way. So when
    you have a single network that’s both saying what is the object and where is the
    object, it’s going to share all the computation about finding the object. And
    all that shared computation is very efficient. When we back propagate the errors
    in the class and in the place, that’s all the information that is going to help
    the computation around finding the biggest object. So anytime you have multiple
    tasks which share some concept of what those tasks would need to do to complete
    their work, it is very likely they should share at least some layers of the network
    together. Later today, we will look at a model where most of the layers are shared
    except for the last one.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，当我们同时进行准确性（分类）和边界框时，L1似乎比我们只进行边界框回归时要好一点[[22:46](https://youtu.be/0frKXR-2PBY?t=22m46s)]。如果这对你来说是违反直觉的，那么这将是本课后需要考虑的主要问题之一，因为这是一个非常重要的想法。这个想法是——找出图像中的主要对象是比较困难的部分。然后确定边界框的确切位置和类别是一种简单的方式。因此，当你有一个同时指出对象是什么和对象在哪里的单个网络时，它将共享所有关于找到对象的计算。所有这些共享的计算非常高效。当我们反向传播类别和位置的错误时，所有这些信息都将帮助计算找到最大对象的周围。因此，每当你有多个任务共享某些概念，这些任务需要完成它们的工作，它们很可能应该至少共享网络的一些层。今天晚些时候，我们将看一个模型，其中大部分层都是共享的，除了最后一层。
- en: Here are the result [[24:34](https://youtu.be/0frKXR-2PBY?t=24m34s)]. As before,
    it does a good job when there is single major object in the image.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果[[24:34](https://youtu.be/0frKXR-2PBY?t=24m34s)]。与以前一样，在图像中有单个主要对象时表现良好。
- en: Multi label classification [[25:29](https://youtu.be/0frKXR-2PBY?t=25m29s)]
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多标签分类[[25:29](https://youtu.be/0frKXR-2PBY?t=25m29s)]
- en: '[Notebook](https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '[笔记本](https://github.com/fastai/fastai/blob/master/courses/dl2/pascal-multi.ipynb)'
- en: 'We want to keep building models that are slightly more complex than the last
    model so that if something stops working, we know exactly where it broke. Here
    are functions from the previous notebook:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望继续构建比上一个模型稍微复杂的模型，这样如果某些东西停止工作，我们就知道出了什么问题。以下是上一个笔记本中的函数：
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Setup
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Multi class [[26:12](https://youtu.be/0frKXR-2PBY?t=26m12s)]
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多类别[[26:12](https://youtu.be/0frKXR-2PBY?t=26m12s)]
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: One of the students pointed out that by using Pandas, we can do things much
    simpler than using `collections.defaultdict` and shared [this gist](https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed).
    The more you get to know Pandas, the more often you realize it is a good way to
    solve lots of different problems.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个学生指出，通过使用Pandas，我们可以比使用`collections.defaultdict`更简单地完成一些事情，并分享了这个[gist](https://gist.github.com/binga/1bc4ebe5e41f670f5954d2ffa9d6c0ed)。您越了解Pandas，就越会意识到它是解决许多不同问题的好方法。
- en: '**Question**: When you are incrementally building on top of smaller models,
    do you reuse them as pre-trained weights? or do you toss it away then retrain
    from scratch [[27:11](https://youtu.be/0frKXR-2PBY?t=27m11s)]? When Jeremy is
    figuring stuff out as he goes like this, he would generally lean towards tossing
    away because reusing pre-trained weights introduces unnecessary complexities.
    However, if he is trying to get to a point where he can train on really big images,
    he will generally start on much smaller and often re-use these weights.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：当您在较小的模型基础上逐步构建时，您是否重复使用它们作为预训练权重？还是将其丢弃然后从头开始重新训练？当Jeremy像这样逐步弄清楚事情时，他通常倾向于丢弃，因为重用预训练权重会引入不必要的复杂性。但是，如果他试图达到一个可以在非常大的图像上训练的点，他通常会从更小的模型开始，并经常重用这些权重。'
- en: '[PRE20]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Multi-class classification is pretty straight forward [[28:28](https://youtu.be/0frKXR-2PBY?t=28m28s)].
    One minor tweak is the use of `set` in this line so that each object type appear
    once.:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 多类别分类非常直接。在这一行中使用`set`的一个小调整，以便每种对象类型只出现一次。
- en: '[PRE21]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: SSD and YOLO [[29:10](https://youtu.be/0frKXR-2PBY?t=29m10s)]
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SSD和YOLO
- en: We have an input image that goes through a conv net which outputs a vector of
    size `4+c` where `c=len(cats)` . This gives us an object detector for a single
    largest object. Let’s now create one that finds 16 objects. The obvious way to
    do this would be to take the last linear layer and rather than having `4+c` outputs,
    we could have `16x(4+c)` outputs. This gives us 16 sets of class probabilities
    and 16 sets of bounding box coordinates. Then we would just need a loss function
    that will check whether those 16 sets of bounding boxes correctly represented
    the up to 16 objects in the image (we will go into the loss function later).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个输入图像通过卷积网络，输出大小为`4+c`的向量，其中`c=len(cats)`。这为我们提供了一个用于单个最大对象的对象检测器。现在让我们创建一个可以找到16个对象的检测器。显而易见的方法是取最后一个线性层，而不是有`4+c`个输出，我们可以有`16x(4+c)`个输出。这为我们提供了16组类别概率和16组边界框坐标。然后我们只需要一个损失函数，检查这16组边界框是否正确表示了图像中的最多16个对象（我们将在后面讨论损失函数）。
- en: The second way to do this is rather than using `nn.linear`, what if instead,
    we took from our ResNet convolutional backbone and added an `nn.Conv2d` with stride
    2 [[31:32](https://youtu.be/0frKXR-2PBY?t=31m32s)]? This will give us a `4x4x[#
    of filters]` tensor — here let’s make it `4x4x(4+c)` so that we get a tensor where
    the number of elements is exactly equal to the number of elements we wanted. Now
    if we created a loss function that took a `4x4x(4+c)` tensor and and mapped it
    to 16 objects in the image and checked whether each one was correctly represented
    by these `4+c` activations, this would work as well. It turns out, both of these
    approaches are actually used [[33:48](https://youtu.be/0frKXR-2PBY?t=33m48s)].
    The approach where the output is one big long vector from a fully connected linear
    layer is used by a class of models known as [YOLO (You Only Look Once)](https://arxiv.org/abs/1506.02640),
    where else, the approach of the convolutional activations is used by models which
    started with something called [SSD (Single Shot Detector)](https://arxiv.org/abs/1512.02325).
    Since these things came out very similar times in late 2015, things are very much
    moved towards SSD. So the point where this morning, [YOLO version 3](https://pjreddie.com/media/files/papers/YOLOv3.pdf)
    came out and is now doing SSD, so that’s what we are going to do. We will also
    learn about why this makes more sense as well.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是，与其使用`nn.linear`，不如从我们的ResNet卷积主干中取出并添加一个带有步幅2的`nn.Conv2d`？这将给我们一个`4x4x[#
    of filters]`张量 - 这里让我们将其设为`4x4x(4+c)`，以便得到一个元素数量与我们想要的元素数量完全相等的张量。现在，如果我们创建一个损失函数，接受一个`4x4x(4+c)`张量，并将其映射到图像中的16个对象，并检查每个对象是否由这些`4+c`激活正确表示，这也可以起作用。事实证明，这两种方法实际上都被使用。从一个完全连接的线性层输出一个很长的向量的方法被一类模型使用，这类模型被称为[YOLO（You
    Only Look Once）](https://arxiv.org/abs/1506.02640)，而卷积激活的方法被一些从[SSD（Single Shot
    Detector）](https://arxiv.org/abs/1512.02325)开始的模型使用。由于这些东西在2015年末几乎同时出现，事情在很大程度上朝着SSD发展。所以今天早上，[YOLO版本3](https://pjreddie.com/media/files/papers/YOLOv3.pdf)发布了，现在正在使用SSD，这就是我们要做的。我们还将了解为什么这样做更有意义。
- en: Anchor boxes [[35:04](https://youtu.be/0frKXR-2PBY?t=35m04s)]
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 锚框
- en: 'Let’s imagine that we had another `Conv2d(stride=2)` then we would have `2x2x(4+c)`
    tensor. Basically, it is creating a grid that looks something like this:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有另一个`Conv2d(stride=2)`，那么我们将有一个`2x2x(4+c)`张量。基本上，它创建了一个看起来像这样的网格：
- en: This is how the geometry of the activations of the second extra convolutional
    stride 2 layer are. Remember, stride 2 convolution does the same thing to the
    geometry of the activations as a stride 1 convolution followed by maxpooling assuming
    the padding is ok.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二个额外的卷积步幅2层激活的几何形状。请记住，步幅2卷积对激活的几何形状做的事情与步幅1卷积后跟着最大池化假设填充正常的激活几何形状是一样的。
- en: Let’s talk about what we might do here [[36:09](https://youtu.be/0frKXR-2PBY?t=36m9s)].
    We want each of these grid cell to be responsible for finding the largest object
    in that part of the image.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈我们可能在这里做什么。我们希望每个网格单元负责查找图像该部分中最大的对象。
- en: Receptive Field [[37:20](https://youtu.be/0frKXR-2PBY?t=37m20s)]
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感受野
- en: Why do we care about the idea that we would like each convolutional grid cell
    to be responsible for finding things that are in the corresponding part of the
    image? The reason is because of something called the receptive field of that convolutional
    grid cell. The basic idea is that throughout your convolutional layers, every
    piece of those tensors has a receptive field which means which part of the input
    image was responsible for calculating that cell. Like all things in life, the
    easiest way to see this is with Excel [[38:01](https://youtu.be/0frKXR-2PBY?t=38m1s)].
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们关心每个卷积网格单元负责找到图像相应部分中的事物的想法？原因是因为有一个叫做卷积网格单元的感受野。基本思想是，在您的卷积层中，这些张量的每一部分都有一个感受野，这意味着负责计算该单元的输入图像的哪个部分。就像生活中的所有事物一样，最容易通过Excel来看到这一点[[38:01](https://youtu.be/0frKXR-2PBY?t=38m1s)]。
- en: Take a single activation (in this case in the maxpool layer)and let’s see where
    it came from [[38:45](https://youtu.be/0frKXR-2PBY?t=38m45s)]. In excel you can
    do Formulas → Trace Precedents. Tracing all the way back to the input layer, you
    can see that it came from this 6 x 6 portion of the image (as well as filters).
    What is more, the middle portion has lots of weights coming out of where else
    cells in the outside only have one weight coming out. So we call this 6 x 6 cells
    the receptive field of the one activation we picked.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 取一个激活（在这种情况下是在最大池层）并看看它来自哪里[[38:45](https://youtu.be/0frKXR-2PBY?t=38m45s)]。在Excel中，您可以执行公式
    → 跟踪前导。一直追溯到输入层，您可以看到它来自图像的这个6 x 6部分（以及滤波器）。更重要的是，中间部分有很多权重从外部的细胞中出来，而外部的细胞只有一个权重出来。所以我们称这6
    x 6个单元格为我们选择的一个激活的感受野。
- en: 3x3 convolution with opacity 15% — clearly the center of the box has more dependencies
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 3x3 卷积，不透明度为15% —— 明显地，盒子的中心有更多的依赖关系
- en: Note that the receptive field is not just saying it’s this box but also that
    the center of the box has more dependencies [[40:27](https://youtu.be/0frKXR-2PBY?t=40m27s)]
    This is a critically important concept when it comes to understanding architectures
    and understanding why conv nets work the way they do.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，感受野不仅仅是说这是一个盒子，而且盒子的中心有更多的依赖关系[[40:27](https://youtu.be/0frKXR-2PBY?t=40m27s)]，当涉及到理解架构以及理解为什么卷积网络工作方式时，这是一个至关重要的概念。
- en: Architecture [[41:18](https://youtu.be/0frKXR-2PBY?t=41m18s)]
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 架构 [[41:18](https://youtu.be/0frKXR-2PBY?t=41m18s)]
- en: The architecture is, we will have a ResNet backbone followed by one or more
    2D convolutions (one for now) which is going to give us a `4x4` grid.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 架构是，我们将有一个ResNet主干，后面跟着一个或多个2D卷积（现在只有一个），这将给我们一个`4x4`的网格。
- en: '[PRE22]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**SSD_Head**'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '**SSD_Head**'
- en: We start with ReLU and dropout
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从ReLU和dropout开始
- en: Then stride 1 convolution. The reason we start with a stride 1 convolution is
    because that does not change the geometry at all — it just lets us add an extra
    layer of calculation. It lets us create not just a linear layer but now we have
    a little mini neural network in our custom head. `StdConv` is defined above —
    it does convolution, ReLU, BatchNorm, and dropout. Most research code you see
    won’t define a class like this, instead they write the entire thing again and
    again. Don’t be like that. Duplicate code leads to errors and poor understanding.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后是步幅为1的卷积。我们从步幅为1的卷积开始的原因是因为这不会改变几何形状 —— 它只让我们增加一层额外的计算。它让我们不仅可以创建一个线性层，而且现在我们的自定义头部中有一个小型神经网络。`StdConv`在上面定义了
    —— 它执行卷积、ReLU、BatchNorm和dropout。您看到的大多数研究代码不会像这样定义一个类，而是一遍又一遍地写整个代码。不要这样做。重复的代码会导致错误和理解不足。
- en: Stride 2 convolution [[44:56](https://youtu.be/0frKXR-2PBY?t=44m56s)]
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 步幅为2的卷积 [[44:56](https://youtu.be/0frKXR-2PBY?t=44m56s)]
- en: At the end, the output of step 3 is `4x4` which gets passed to `OutConv`. `OutConv`
    has two separate convolutional layers each of which is stride 1 so it is not changing
    the geometry of the input. One of them is of length of the number of classes (ignore
    `k` for now and `+1` is for “background” — i.e. no object was detected), the other’s
    length is 4\. Rather than having a single conv layer that outputs `4+c`, let’s
    have two conv layers and return their outputs in a list. This allows these layers
    to specialize just a little bit. We talked about this idea that when you have
    multiple tasks, they can share layers, but they do not have to share all the layers.
    In this case, our two tasks of creating a classifier and creating and creating
    bounding box regression share every single layers except the very last one.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，步骤3的输出是`4x4`，传递给`OutConv`。`OutConv`有两个单独的卷积层，每个都是步幅为1，因此不会改变输入的几何形状。其中一个的长度是类别数（现在忽略`k`和`+1`是为了“背景”
    —— 即没有检测到对象），另一个的长度是4。与其有一个输出`4+c`的单个卷积层，不如有两个卷积层并将它们的输出返回到列表中。这使得这些层可以稍微专门化。我们谈到了这样一个想法，当您有多个任务时，它们可以共享层，但它们不必共享所有层。在这种情况下，我们的两个任务是创建一个分类器和创建和创建边界框回归，除了最后一个层外，它们共享每一个层。
- en: At the end, we flatten out the convolution because Jeremy wrote the loss function
    to expect flattened out tensor, but we could totally rewrite it to not do that.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们展平卷积，因为Jeremy编写的损失函数期望展平的张量，但我们完全可以重写它以不这样做。
- en: '[Fastai Coding Style](https://github.com/fastai/fastai/blob/master/docs/style.md)
    [[42:58](https://youtu.be/0frKXR-2PBY?t=42m58s)]'
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '[Fastai编码风格](https://github.com/fastai/fastai/blob/master/docs/style.md) [[42:58](https://youtu.be/0frKXR-2PBY?t=42m58s)]'
- en: The first draft was released this week. It is very heavily orient towards the
    idea of expository programming which is the idea that programming code should
    be something that you can use to explain an idea, ideally as readily as mathematical
    notation, to somebody that understands your coding method. The idea goes back
    a very long way, but it was best described in the Turing Award lecture of 1979
    by probably Jeremy’s greatest computer science hero Ken Iverson. He had been working
    on it since well before 1964 but 1964 was the first example of this approach of
    programming he released which is called APL and 25 years later, he won the Turing
    Award. He then passed on the baton to his son Eric Iverson. Fastai style guide
    is an attempt at taking some of these ideas.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 第一版本本周发布。它非常重视阐述性编程的概念，即编程代码应该是您可以用来解释一个想法的东西，理想情况下，可以像数学符号一样容易地向理解您编码方法的人解释。这个想法已经存在很长时间了，但最好的描述是杰里米最崇拜的计算机科学英雄肯·艾弗森在1979年的图灵奖演讲中描述的。他在1964年之前就一直在研究这个问题，但1964年是他发布这种编程方法的第一个例子，称为APL，25年后，他获得了图灵奖。然后他把接力棒传给了他的儿子埃里克·艾弗森。Fastai风格指南是对这些想法的一种尝试。
- en: Loss Function [[47:44](https://youtu.be/0frKXR-2PBY?t=47m44s)]
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 损失函数[[47:44](https://youtu.be/0frKXR-2PBY?t=47m44s)]
- en: The loss function needs to look at each of these 16 sets of activations, each
    of which has four bounding box coordinates and `c+1` class probabilities and decide
    if those activations are close or far away from the object which is the closest
    to this grid cell in the image. If nothing is there, then whether it is predicting
    background correctly. That turns out to be very hard to do.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数需要查看这16组激活中的每一组，每组都有四个边界框坐标和`c+1`类概率，并决定这些激活是否接近或远离图像中与该网格单元最接近的对象。如果没有任何东西，那么它是否正确地预测了背景。这是非常难做到的。
- en: Matching Problem [[48:43](https://youtu.be/0frKXR-2PBY?t=48m43s)]
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 匹配问题[[48:43](https://youtu.be/0frKXR-2PBY?t=48m43s)]
- en: The loss function needs to take each of the objects in the image and match them
    to one of these convolutional grid cells to say “this grid cell is responsible
    for this particular object” so then it can go ahead and say “okay, how close are
    the 4 coordinates and how close are the class probabilities.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 损失函数需要将图像中的每个对象与这些卷积网格单元中的一个进行匹配，以便说“这个网格单元负责这个特定对象”，然后它可以继续说“好的，这4个坐标有多接近，类概率有多接近”。
- en: 'Here is our goal [[49:56](https://youtu.be/0frKXR-2PBY?t=49m56s)]:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的目标[[49:56](https://youtu.be/0frKXR-2PBY?t=49m56s)]：
- en: Our dependent variable looks like the one on the left, and our final convolutional
    layer is going to be `4x4x(c+1)` in this case `c=20`. We then flatten that out
    into a vector. Our goal is to come up with a function which takes in a dependent
    variable and also some particular set of activations that ended up coming out
    of the model and returns a higher number if these activations are not a good reflection
    of the ground truth bounding boxes; or a lower number if it is a good reflection.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的因变量看起来像左边的那个，我们最终的卷积层将是`4x4x(c+1)`，在这种情况下`c=20`。然后我们将其展平为一个向量。我们的目标是设计一个函数，该函数接受一个因变量和模型输出的一些特定激活，并在这些激活不是地面真实边界框的良好反映时返回更高的数字；或者如果是一个好的反映，则返回更低的数字。
- en: Testing [[51:58](https://youtu.be/0frKXR-2PBY?t=51m58s)]
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 测试[[51:58](https://youtu.be/0frKXR-2PBY?t=51m58s)]
- en: '[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Make sure these shapes make sense. Let’s now look at the ground truth `y` [[53:24](https://youtu.be/0frKXR-2PBY?t=53m24s)]:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 确保这些形状是合理的。现在让我们看看地面真实`y`[[53:24](https://youtu.be/0frKXR-2PBY?t=53m24s)]：
- en: '[PRE24]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note that bounding box coordinates have been scaled to between 0 and 1 — basically
    we are treating the image as being 1x1, so they are relative to the size of the
    image.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，边界框坐标已缩放到0和1之间 - 基本上我们将图像视为1x1，因此它们是相对于图像大小的。
- en: 'We already have `show_ground_truth` function. This `torch_gt` (gt: ground truth)
    function simply converts tensors into numpy array.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了`show_ground_truth`函数。这个`torch_gt`（gt：地面真相）函数简单地将张量转换为numpy数组。
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The above is a ground truth. Here is our `4x4` grid cells from our final convolutional
    layer [[54:44](https://youtu.be/0frKXR-2PBY?t=54m44s)]:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以上是一个地面真相。这是我们最终卷积层的`4x4`网格单元[[54:44](https://youtu.be/0frKXR-2PBY?t=54m44s)]：
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Each of these square boxes, different papers call them different things. The
    three terms you’ll hear are: anchor boxes, prior boxes, or default boxes. We will
    stick with the term anchor boxes.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个正方形框，不同的论文称其为不同的东西。您将听到的三个术语是：锚框、先验框或默认框。我们将坚持使用术语锚框。
- en: What we are going to do for this loss function is we are going to go through
    a matching problem where we are going to take every one of these 16 boxes and
    see which one of these three ground truth objects has the highest amount of overlap
    with a given square [[55:21](https://youtu.be/0frKXR-2PBY?t=55m21s)]. To do this,
    we have to have some way of measuring amount of overlap and a standard function
    for this is called [Jaccard index](https://en.wikipedia.org/wiki/Jaccard_index)
    (IoU).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个损失函数，我们将通过一个匹配问题，看看这16个框中的每一个与给定正方形中的这三个地面真实对象哪一个有最高的重叠量。为了做到这一点，我们必须有一种衡量重叠量的方法，这种标准函数称为Jaccard指数（IoU）。
- en: We are going to go through and find the Jaccard overlap for each one of the
    three objects versus each of the 16 anchor boxes [[57:11](https://youtu.be/0frKXR-2PBY?t=57m11s)].
    That is going to give us a `3x16` matrix.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将逐个查看这三个对象与每个16个锚框的Jaccard重叠[[57:11](https://youtu.be/0frKXR-2PBY?t=57m11s)]。这将给我们一个`3x16`矩阵。
- en: 'Here are the *coordinates* of all of our anchor boxes (centers, height, width):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们所有锚框（中心、高度、宽度）的*坐标*：
- en: '[PRE27]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here are the amount of overlap between 3 ground truth objects and 16 anchor
    boxes:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这是3个地面真实对象和16个锚框之间的重叠量：
- en: '[PRE28]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'What we could do now is we could take the max of dimension 1 (row-wise) which
    will tell us for each ground truth object, what the maximum amount that overlaps
    with some grid cell as well as the index:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以取维度1（按行）的最大值，这将告诉我们每个地面真实对象的最大重叠量以及索引：
- en: '[PRE29]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will also going to look at max over a dimension 0 (column-wise) which will
    tell us what is the maximum amount of overlap for each grid cell across all of
    the ground truth objects [[59:08](https://youtu.be/0frKXR-2PBY?t=59m8s)]:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将查看维度0（按列）的最大值，这将告诉我们每个网格单元与所有地面真实对象之间的最大重叠量是多少：
- en: '[PRE30]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: What is particularly interesting here is that it tells us for every grid cell
    what is the index of the ground truth object which overlaps with it the most.
    Zero is a bit overloaded here — zero could either mean the amount of overlap was
    zero or its largest overlap is with object index zero. It is going to turn out
    not to matter but just FYI.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这里特别有趣的是，它告诉我们每个网格单元与之重叠最多的地面真实对象的索引是什么。零在这里有点过载 - 零可能意味着重叠量为零，也可能意味着它与对象索引零的重叠最大。这将被证明并不重要，但只是供参考。
- en: There is a function called `map_to_ground_truth` which we will not worry about
    for now [[59:57](https://youtu.be/0frKXR-2PBY?t=59m57s)]. It is super simple code
    but it is slightly awkward to think about. Basically what it does is it combines
    these two sets of overlaps in a way described in the SSD paper to assign every
    anchor box to a ground truth object. The way it assign that is each of the three
    (row-wise max) gets assigned as is. For the rest of the anchor boxes, they get
    assigned to anything which they have an overlap of at least 0.5 with (column-wise).
    If neither applies, it is considered to be a cell which contains background.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个名为`map_to_ground_truth`的函数，我们现在不用担心。这是非常简单的代码，但稍微难以理解。基本上它的作用是以SSD论文中描述的方式将这两组重叠组合起来，将每个锚框分配给一个地面真实对象。它的分配方式是每个三个（按行最大）都被分配为是。对于其余的锚框，它们被分配给它们与至少0.5重叠的任何东西（按列）。如果两者都不适用，则被视为包含背景的单元格。
- en: '[PRE31]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now you can see a list of all the assignments [[1:01:05](https://youtu.be/0frKXR-2PBY?t=1h1m5s)].
    Anywhere that has `gt_overlap < 0.5` gets assigned background. The three row-wise
    max anchor box has high number to force the assignments. Now we can combine these
    values to classes:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您可以看到所有分配的列表。任何`gt_overlap < 0.5`的地方都被分配为背景。三行最大锚框具有较高的数字以强制分配。现在我们可以将这些值组合到类别中：
- en: '[PRE32]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then add a threshold and finally comes up with the three classes that are being
    predicted:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 然后添加一个阈值，最后得出正在预测的三个类：
- en: '[PRE33]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'And here are what each of these anchor boxes is meant to be predicting:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是每个锚框预测的含义：
- en: '[PRE34]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'So that was the matching stage [[1:02:29](https://youtu.be/0frKXR-2PBY?t=1h2m29s)].
    For L1 loss, we can:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 那就是匹配阶段。对于L1损失，我们可以：
- en: take the activations which matched (`pos_idx = [11, 13, 14]`)
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取匹配的激活（`pos_idx = [11, 13, 14]`）
- en: subtract from those the ground truth bounding boxes
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从中减去地面真实边界框
- en: take the absolute value of the difference
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取差的绝对值
- en: take the mean of that.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取平均值。
- en: For classifications, we can just do a cross entropy
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类，我们可以做一个交叉熵
- en: '[PRE35]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We will end up with 16 predicted bounding boxes, most of them will be background.
    If you are wondering what it predicts in terms of bounding box of background,
    the answer is it totally ignores it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最终我们将得到16个预测的边界框，其中大多数将是背景。如果您想知道它在背景边界框方面的预测是什么，答案是它完全忽略了它。
- en: '[PRE36]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Tweak 1\. How do we interpret the activations [[1:04:16](https://youtu.be/0frKXR-2PBY?t=1h4m16s)]?
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调1.我们如何解释激活？
- en: 'The way we interpret the activation is defined here:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们解释激活的方式在这里定义：
- en: '[PRE37]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We grab the activations, we stick them through `tanh` (remember `tanh` is the
    same shape as sigmoid except it is scaled to be between -1 and 1) which forces
    it to be within that range. We then grab the actual position of the anchor boxes,
    and we will move them around according to the value of the activations divided
    by two (`actn_bbs[:,:2]/2`). In other words, each predicted bounding box can be
    moved by up to 50% of a grid size from where its default position is. Ditto for
    its height and width — it can be up to twice as big or half as big as its default
    size.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们抓取激活，将它们通过`tanh`（记住`tanh`与sigmoid形状相同，只是缩放到-1和1之间）强制使其在该范围内。然后我们抓取锚框的实际位置，并根据激活值除以二（`actn_bbs[:,:2]/2`）将它们移动。换句话说，每个预测的边界框可以从其默认位置最多移动一个网格大小的50%。高度和宽度也是如此
    - 它可以是默认大小的两倍大或一半小。
- en: Tweak 2\. We actually use binary cross entropy loss instead of cross entropy
    [[1:05:36](https://youtu.be/0frKXR-2PBY?t=1h5m36s)]
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 微调2.我们实际上使用二元交叉熵损失而不是交叉熵
- en: '[PRE38]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Binary cross entropy is what we normally use for multi-label classification.
    Like in the planet satellite competition, each satellite image could have multiple
    things. If it has multiple things in it, you cannot use softmax because softmax
    really encourages just one thing to have the high number. In our case, each anchor
    box can only have one object associated with it, so it is not for that reason
    that we are avoiding softmax. It is something else — which is it is possible for
    an anchor box to have nothing associated with it. There are two ways to handle
    this idea of “background”; one would be to say background is just a class, so
    let’s use softmax and just treat background as one of the classes that the softmax
    could predict. A lot of people have done it this way. But that is a really hard
    thing to ask neural network to do [[1:06:52](https://youtu.be/0frKXR-2PBY?t=1h5m52s)]
    — it is basically asking whether this grid cell does not have any of the 20 objects
    that I am interested with Jaccard overlap of more than 0.5\. It is a really hard
    to thing to put into a single computation. On the other hand, what if we just
    asked for each class; “is it a motorbike?” “is it a bus?”, “ is it a person?”
    etc and if all the answer is no, consider that background. That is the way we
    do it here. It is not that we can have multiple true labels, but we can have zero.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 二元交叉熵是我们通常用于多标签分类的。就像在行星卫星竞赛中，每个卫星图像可能有多个物体。如果它有多个物体，你不能使用softmax，因为softmax真的鼓励只有一个物体有高的数字。在我们的情况下，每个锚框只能与一个物体相关联，所以我们避免使用softmax并不是因为这个原因。还有其他原因——即一个锚框可能没有任何与之相关联的物体。处理这种“背景”的想法有两种方法；一种是说背景只是一个类，所以让我们使用softmax，将背景视为softmax可以预测的类之一。很多人都是这样做的。但这是一个非常困难的事情要求神经网络做[[1:06:52](https://youtu.be/0frKXR-2PBY?t=1h5m52s)]
    — 基本上是在问这个网格单元是否没有我感兴趣的20个物体中的任何一个，Jaccard重叠大于0.5。这是一个非常难以放入单个计算中的事情。另一方面，如果我们只问每个类；“这是摩托车吗？”“这是公共汽车吗？”“这是一个人吗？”等等，如果所有的答案都是否定的，那就认为是背景。这就是我们在这里做的方式。并不是我们可以有多个真实标签，而是我们可以有零个。
- en: 'In `forward` :'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在`forward`中：
- en: First we take the one hot embedding of the target (at this stage, we do have
    the idea of background)
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先我们获取目标的one hot编码（在这个阶段，我们已经有了背景的概念）
- en: Then we remove the background column (the last one) which results in a vector
    either of all zeros or one one.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们移除背景列（最后一列），结果是一个全为零或全为一的向量。
- en: Use binary cross-entropy predictions.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二元交叉熵预测。
- en: This is a minor tweak, but it is the kind of minor tweak that Jeremy wants you
    to think about and understand because it makes a really big difference to your
    training and when there is some increment over a previous paper, it would be something
    like this [[1:08:25](https://youtu.be/0frKXR-2PBY?t=1h8m25s)]. It is important
    to understand what this is doing and more importantly why.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个小的调整，但这是Jeremy希望你考虑和理解的小调整，因为它对你的训练有很大的影响，当有一些对以前论文的增量时，会是这样的[[1:08:25](https://youtu.be/0frKXR-2PBY?t=1h8m25s)]。重要的是要理解这是在做什么，更重要的是为什么。
- en: 'So now we have [[1:09:39](https://youtu.be/0frKXR-2PBY?t=1h9m39s)]:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有[[1:09:39](https://youtu.be/0frKXR-2PBY?t=1h9m39s)]：
- en: A custom loss function
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个自定义损失函数
- en: A way to calculate Jaccard index
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算Jaccard指数的方法
- en: A way to convert activations to bounding box
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将激活转换为边界框的方法
- en: A way to map anchor boxes to ground truth
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将锚框映射到地面真实的方法
- en: Now all it’s left is SSD loss function.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的就是SSD损失函数。
- en: SSD Loss Function [[1:09:55](https://youtu.be/0frKXR-2PBY?t=1h9m55s)]
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SSD损失函数[[1:09:55](https://youtu.be/0frKXR-2PBY?t=1h9m55s)]
- en: '[PRE39]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The `ssd_loss` function which is what we set as the criteria, it loops through
    each image in the mini-batch and call `ssd_1_loss` function (i.e. SSD loss for
    one image).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssd_loss`函数是我们设置的标准，它循环遍历每个小批量中的图像，并调用`ssd_1_loss`函数（即一个图像的SSD损失）。'
- en: '`ssd_1_loss` is where it is all happening. It begins by de-structuring `bbox`
    and `clas`. Let’s take a closer look at `get_y` [[1:10:38](https://youtu.be/0frKXR-2PBY?t=1h10m38s)]:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`ssd_1_loss`是所有操作发生的地方。它从`bbox`和`clas`开始解构。让我们更仔细地看一下`get_y`[[1:10:38](https://youtu.be/0frKXR-2PBY?t=1h10m38s)]：'
- en: '[PRE40]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: A lot of code you find on the internet does not work with mini-batches. It only
    does one thing at a time which we don’t want. In this case, all these functions
    (`get_y`, `actn_to_bb`, `map_to_ground_truth`) is working on, not exactly a mini-batch
    at a time, but a whole bunch of ground truth objects at a time. The data loader
    is being fed a mini-batch at a time to do the convolutional layers. Because we
    can have *different numbers of ground truth objects in each image* but a tensor
    has to be the strict rectangular shape, fastai automatically pads it with zeros
    (any target values that are shorter) [[1:11:08](https://youtu.be/0frKXR-2PBY?t=1h11m8s)].
    This was something that was added recently and super handy, but that does mean
    that you then have to make sure that you get rid of those zeros. So `get_y` gets
    rid of any of the bounding boxes that are just padding.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 你在互联网上找到的很多代码都不能用于小批量。它一次只能做一件事，而我们不想要这样。在这种情况下，所有这些函数（`get_y`、`actn_to_bb`、`map_to_ground_truth`）都是在一次处理，不完全是一个小批量，而是一次处理一堆地面真实对象。数据加载器每次被馈送一个小批量以执行卷积层。因为我们可以在每个图像中有*不同数量的地面真实对象*，但张量必须是严格的矩形形状，fastai会自动用零填充它（任何较短的目标值）[[1:11:08](https://youtu.be/0frKXR-2PBY?t=1h11m8s)]。这是最近添加的一个功能，非常方便，但这意味着你必须确保去掉这些零。因此，`get_y`会去掉任何只是填充的边界框。
- en: Get rid of the padding
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 去掉填充
- en: Turn the activations to bounding boxes
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将激活转换为边界框
- en: Do the Jaccard
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算Jaccard指数
- en: Do map_to_ground_truth
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行地面真实的映射
- en: Check that there is an overlap greater than something around 0.4~0.5 (different
    papers use different values for this)
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查是否有大约0.4~0.5的重叠（不同的论文使用不同的值）
- en: Find the indices of things that matched
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到匹配的索引
- en: Assign background class for the ones that did not match
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为那些不匹配的分配背景类
- en: Then finally get L1 loss for the localization, binary cross entropy loss for
    the classification, and return them which gets added in `ssd_loss`
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后最终得到定位的L1损失，分类的二元交叉熵损失，并将它们返回，加入`ssd_loss`
- en: Training [[1:12:47](https://youtu.be/0frKXR-2PBY?t=1h12m47s)]
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: '[PRE41]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[PRE43]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Result [[1:13:16](https://youtu.be/0frKXR-2PBY?t=1h13m16s)]
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结果
- en: In practice, we want to remove the background and also add some threshold for
    probabilities, but it is on the right track. The potted plant image, the result
    is not surprising as all of our anchor boxes were small (4x4 grid). To go from
    here to something that is going to be more accurate, all we are going to do is
    to create way more anchor boxes.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们希望去除背景，并为概率添加一些阈值，但这是正确的方向。盆栽植物图像，结果并不令人惊讶，因为我们所有的锚盒都很小（4x4网格）。要从这里走向更准确的东西，我们要做的就是创建更多的锚盒。
- en: '**Question**: For the multi-label classification, why aren’t we multiplying
    the categorical loss by a constant like we did before [[1:15:20](https://youtu.be/0frKXR-2PBY?t=1h15m20s)]?
    Great question. It is because later on it will turn out we do not need to.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 问题：对于多标签分类，为什么我们不像以前那样将分类损失乘以一个常数？很好的问题。因为后来会发现我们不需要这样做。
- en: More anchors! [[1:14:47](https://youtu.be/0frKXR-2PBY?t=1h14m47s)]
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多的锚点！
- en: 'There are 3 ways to do this:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 有3种方法可以做到这一点：
- en: 'Create anchor boxes of different sizes (zoom):'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建不同尺寸的锚盒（缩放）：
- en: From left (1x1, 2x2, 4x4 grids of anchor boxes). Notice that some of the anchor
    box is bigger than the original image.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 从左边（1x1、2x2、4x4的锚盒网格）。注意一些锚盒比原始图像大。
- en: '2\. Create anchor boxes of different aspect ratios:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 创建不同长宽比的锚盒：
- en: '3\. Use more convolutional layers as sources of anchor boxes (the boxes are
    randomly jittered so that we can see ones that are overlapping [[1:16:28](https://youtu.be/0frKXR-2PBY?t=1h16m28s)]):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 使用更多卷积层作为锚盒的来源（盒子被随机抖动，以便我们可以看到重叠的盒子）：
- en: 'Combining these approaches, you can create lots of anchor boxes (Jeremy said
    he wouldn’t print it, but here it is):'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 结合这些方法，你可以创建很多锚盒（Jeremy说他不会打印出来，但这里有）：
- en: '[PRE44]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '`anchors` : middle and height, width'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`锚点`：中间和高度，宽度'
- en: '`anchor_cnr` : top left and bottom right corners'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '`anchor_cnr`：左上角和右下角'
- en: Review of key concept [[1:18:00](https://youtu.be/0frKXR-2PBY?t=1h18m)]
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键概念回顾
- en: We have a vector of ground truth (sets of 4 bounding box coordinates and a class)
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个地面真相的向量（一组4个边界框坐标和一个类）
- en: We have a neural net that takes some input and spits out some output activations
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有一个神经网络，它接受一些输入并输出一些输出激活
- en: Compare the activations and the ground truth, calculate a loss, find the derivative
    of that, and adjust weights according to the derivative times a learning rate.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较激活和地面真相，计算损失，找到该导数的导数，并根据导数乘以学习率调整权重。
- en: We need a loss function that can take ground truth and activation and spit out
    a number that says how good these activations are. To do this, we need to take
    each one of `m` ground truth objects and decide which set of `(4+c)` activations
    is responsible for that object [[1:21:58](https://youtu.be/0frKXR-2PBY?t=1h21m58s)]
    — which one we should be comparing to decide whether the class is correct and
    bounding box is close or not (matching problem).
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要一个损失函数，可以接受地面真相和激活，并输出一个数字，表示这些激活有多好。为了做到这一点，我们需要考虑每一个`m`个地面真相对象，并决定哪组`(4+c)`激活负责该对象
    — 我们应该比较哪一个来决定类是否正确，边界框是否接近（匹配问题）。
- en: Since we are using SSD approach, so it is not arbitrary which ones we match
    up [[1:23:18](https://youtu.be/0frKXR-2PBY?t=1h23m18s)]. We want to match up the
    set of activations whose receptive field has the maximum density from where the
    real object is.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们使用SSD方法，所以我们匹配的对象并不是任意的。我们希望匹配的是接收域密度最大的激活集，从真实对象所在的地方。
- en: The loss function needs to be some consistent task. If in the first image, the
    top left object corresponds with the first 4+c activations, and in the second
    image, we threw things around and suddenly it’s now going with the last 4+c activations,
    the neural net doesn’t know what to learn.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失函数需要是一些一致的任务。如果在第一幅图像中，左上角的对象对应于前4+c个激活，并且在第二幅图像中，我们把事物扔来扔去，突然它现在与最后的4+c个激活一起，神经网络就不知道要学习什么。
- en: Once matching problem is resolved, the rest is just the same as the single object
    detection.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦匹配问题解决了，其余的就和单个对象检测一样。
- en: 'Architectures:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 架构：
- en: YOLO — the last layer is fully connected (no concept of geometry)
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO — 最后一层是全连接的（没有几何概念）
- en: SSD — the last layer is convolutional
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSD — 最后一层是卷积
- en: k (zooms x ratios)[[1:29:39](https://youtu.be/0frKXR-2PBY?t=1h29m39s)]
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k（缩放x比率）
- en: For every grid cell which can be different sizes, we can have different orientations
    and zooms representing different anchor boxes which are just like conceptual ideas
    that every one of anchor boxes is associated with one set of `4+c` activations
    in our model. So however many anchor boxes we have, we need to have that times
    `(4+c)` activations. That does not mean that each convolutional layer needs that
    many activations. Because 4x4 convolutional layer already has 16 sets of activations,
    the 2x2 layer has 4 sets of activations, and finally 1x1 has one set. So we basically
    get 1 + 4 + 16 for free. So we only needs to know `k` where `k` is the number
    of zooms by the number of aspect ratios. Where else, the grids, we will get for
    free through our architecture.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个可能具有不同大小的网格单元，我们可以有不同的方向和缩放，代表不同的锚框，这些锚框就像是每个锚框都与我们模型中的一个`4+c`激活集相关联的概念性想法。因此，无论我们有多少个锚框，我们都需要有那么多次`(4+c)`激活。这并不意味着每个卷积层都需要那么多激活。因为4x4卷积层已经有16组激活，2x2层有4组激活，最后1x1层有一组激活。所以我们基本上可以免费获得1
    + 4 + 16。因此，我们只需要知道`k`，其中`k`是缩放数乘以宽高比数。而网格，我们将通过我们的架构免费获得。
- en: Model Architecture [[1:31:10](https://youtu.be/0frKXR-2PBY?t=1h31m10s)]
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型架构
- en: '[PRE45]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The model is nearly identical to what we had before. But we have a number of
    stride 2 convolutions which is going to take us through to 4x4, 2x2, and 1x1 (each
    stride 2 convolution halves our grid size in both directions).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 模型几乎与之前的模型相同。但我们有许多步长为2的卷积，这将带我们到4x4、2x2和1x1（每个步长为2的卷积都会将我们的网格大小在两个方向上减半）。
- en: After we do our first convolution to get to 4x4, we will grab a set of outputs
    from that because we want to save away the 4x4 anchors.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们进行第一次卷积以达到4x4后，我们将从中获取一组输出，因为我们想要保存4x4的锚点。
- en: Once we get to 2x2, we grab another set of now 2x2 anchors
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们到达2x2，我们再抓取一组2x2的锚点
- en: Then finally we get to 1x1
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后我们到达1x1
- en: We then concatenate them all together, which gives us the correct number of
    activations (one activation for every anchor box).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将它们全部连接在一起，这给我们正确数量的激活（每个锚框一个激活）。
- en: Training [[1:32:50](https://youtu.be/0frKXR-2PBY?t=1h32m50s)]
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练
- en: '[PRE46]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Here, we printed out those detections with at least probability of `0.2` . Some
    of them look pretty hopeful but others not so much.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们打印出那些至少概率为`0.2`的检测结果。有些看起来很有希望，但有些则不太好。
- en: History of object detection [[1:33:43](https://youtu.be/0frKXR-2PBY?t=1h33m43s)]
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 目标检测的历史
- en: '[Scalable Object Detection using Deep Neural Networks](https://arxiv.org/abs/1312.2249)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 使用深度神经网络的可扩展目标检测
- en: When people refer to the multi-box method, they are talking about this paper.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当人们提到多框法时，他们指的是这篇论文。
- en: This was the paper that came up with the idea that we can have a loss function
    that has this matching process and then you can use that to do object detection.
    So everything since that time has been trying to figure out how to make this better.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这篇论文提出了一个损失函数的想法，该函数具有匹配过程，然后可以用来进行目标检测。因此，自那时以来，一切都在尝试找出如何使其更好。
- en: '[Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks](https://arxiv.org/abs/1506.01497)'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 实时目标检测与区域提议网络
- en: In parallel, Ross Girshick was going down a totally different direction. He
    had these two-stage process where the first stage used the classical computer
    vision approaches to find edges and changes of gradients to guess which parts
    of the image may represent distinct objects. Then fit each of those into a convolutional
    neural network which was basically designed to figure out if that is the kind
    of object we are interested in.
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，Ross Girshick正在走一条完全不同的方向。他有这两个阶段的过程，第一阶段使用经典的计算机视觉方法来找到边缘和梯度变化，猜测图像的哪些部分可能代表不同的对象。然后将每个对象放入一个卷积神经网络中，这个网络基本上是设计用来确定我们感兴趣的对象的类型。
- en: R-CNN and Fast R-CNN are hybrid of traditional computer vision and deep learning.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R-CNN和Fast R-CNN是传统计算机视觉和深度学习的混合体。
- en: 'What Ross and his team then did was they took the multibox idea and replaced
    the traditional non-deep learning computer vision part of their two stage process
    with the conv net. So now they have two conv nets: one for region proposals (all
    of the things that might be objects) and the second part was the same as his earlier
    work.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ross和他的团队接着做的是，他们采用了多框法的思想，用卷积网络替换了他们两阶段过程中传统的非深度学习计算机视觉部分。现在他们有两个卷积网络：一个用于区域提议（可能是对象的所有东西），第二部分与他之前的工作相同。
- en: '[You Only Look Once: Unified, Real-Time Object Detection](https://arxiv.org/abs/1506.02640)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 统一、实时目标检测
- en: '[SSD: Single Shot MultiBox Detector](https://arxiv.org/abs/1512.02325)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 单次多框检测器（SSD）
- en: At similar time these paper came out. Both of these did something pretty cool
    which is they achieved similar performance as the Faster R-CNN but with 1 stage.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在同一时间，这些论文出现了。这两篇论文做了一些非常酷的事情，就是他们实现了与Faster R-CNN相似的性能，但只用了1个阶段。
- en: They took the multibox idea and they tried to figure out how to deal with messy
    outputs. The basic ideas were to use, for example, hard negative mining where
    they would go through and find all of the matches that did not look that good
    and throw them away, use very tricky and complex data augmentation methods, and
    all kind of hackery. But they got them to work pretty well.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们采用了多框法，并试图找出如何处理混乱的输出。基本思想是使用，例如，硬负样本挖掘，他们会遍历所有看起来不太好的匹配项并将其丢弃，使用非常棘手和复杂的数据增强方法，以及各种技巧。但他们让它们运行得相当不错。
- en: '[Focal Loss for Dense Object Detection](https://arxiv.org/abs/1708.02002) (RetinaNet)'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 密集目标检测的焦点损失（RetinaNet）
- en: Then something really cool happened late last year which is this thing called
    focal loss.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后去年年底发生了一件非常酷的事情，那就是焦点损失。
- en: They actually realized why this messy thing wasn’t working. When we look at
    an image, there are 3 different granularities of convolutional grid (4x4, 2x2,
    1x1) [[1:37:28](https://youtu.be/0frKXR-2PBY?t=1h37m28s)]. The 1x1 is quite likely
    to have a reasonable overlap with some object because most photos have some kind
    of main subject. On the other hand, in the 4x4 grid cells, the most of 16 anchor
    boxes are not going to have a much of an overlap with anything. So if somebody
    was to say to you “$20 bet, what do you reckon this little clip is?” and you are
    not sure, you will say “background” because most of the time, it is the background.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们实际上意识到为什么这个混乱的东西不起作用。当我们查看图像时，有3种不同的卷积网格粒度（4x4、2x2、1x1）。1x1很可能与某个对象有合理的重叠，因为大多数照片都有某种主题。另一方面，在4x4网格单元中，大多数16个锚框不会与任何东西有太多重叠。因此，如果有人对你说“20美元赌注，你认为这个小片段是什么？”而你不确定，你会说“背景”，因为大多数时候，它是背景。
- en: '**Question**: I understand why we have a 4x4 grid of receptive fields with
    1 anchor box each to coarsely localize objects in the image. But what I think
    I’m missing is why we need multiple receptive fields at different sizes. The first
    version already included 16 receptive fields, each with a single anchor box associated.
    With the additions, there are now many more anchor boxes to consider. Is this
    because you constrained how much a receptive field could move or scale from its
    original size? Or is there another reason? [[1:38:47](https://youtu.be/0frKXR-2PBY?t=1h38m47s)]
    It is kind of backwards. The reason Jeremy did the constraining was because he
    knew he was going to be adding more boxes later. But really, the reason is that
    the Jaccard overlap between one of those 4x4 grid cells and a picture where a
    single object that takes up most of the image is never going to be 0.5\. The intersection
    is much smaller than the union because the object is too big. So for this general
    idea to work where we are saying you are responsible for something that you have
    better than 50% overlap with, we need anchor boxes which will on a regular basis
    have a 50% or higher overlap which means we need to have a variety of sizes, shapes,
    and scales. This all happens in the loss function. The vast majority of the interesting
    stuff in all of the object detection is the loss function.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**问题**：我理解为什么我们在图像中有一个4x4网格的感受野，每个都有一个锚框来粗略定位对象。但我觉得我不明白的是为什么我们需要不同尺寸的多个感受野。第一个版本已经包括了16个感受野，每个都有一个关联的单个锚框。通过添加，现在有更多的锚框要考虑。这是因为您限制了感受野可以从其原始大小移动或缩放的程度吗？还是有其他原因？这有点反向。Jeremy做约束的原因是因为他知道他以后会添加更多的框。但实际上，原因是4x4网格单元之一与占据图像大部分的单个对象的图像之间的Jaccard重叠永远不会达到0.5。交集远小于并集，因为对象太大。因此，为了使这个一般想法起作用，我们说你负责的东西与之有50%以上的重叠，我们需要锚框，这些锚框将定期具有50%或更高的重叠，这意味着我们需要具有各种大小、形状和比例的锚框。所有这些都发生在损失函数中。所有目标检测中大部分有趣的东西都在损失函数中。'
- en: Focal Loss [[1:40:38](https://youtu.be/0frKXR-2PBY?t=1h40m38s)]
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 焦点损失
- en: The key thing is this very first picture. The blue line is the binary cross
    entropy loss. If the answer is not a motorbike [[1:41:46](https://youtu.be/0frKXR-2PBY?t=1h41m46s)],
    and I said “I think it’s not a motorbike and I am 60% sure” with the blue line,
    the loss is still about 0.5 which is pretty bad. So if we want to get our loss
    down, then for all these things which are actually back ground, we have to be
    saying “I am sure that is background”, “I am sure it’s not a motorbike, or a bus,
    or a person” — because if I don’t say we are sure it is not any of these things,
    then we still get loss.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 关键是这张第一张图片。蓝线是二元交叉熵损失。如果答案不是摩托车，我说“我认为这不是摩托车，我有60%的把握”用蓝线，损失仍然约为0.5，这相当糟糕。所以，如果我们想降低损失，那么对于所有这些实际上是背景的东西，我们必须说“我确定那是背景”，“我确定这不是摩托车，公共汽车或人”
    — 因为如果我不说我们确定它不是这些东西中的任何一个，那么我们仍然会有损失。
- en: That is why the motorbike example did not work [[1:42:39](https://youtu.be/0frKXR-2PBY?t=1h42m39s)].
    Because even when it gets to lower right corner and it wants to say “I think it’s
    a motorbike”, there is no payoff for it to say so. If it is wrong, it gets killed.
    And the vast majority of the time, it is background. Even if it is not background,
    it is not enough just to say “it’s not background” — you have to say which of
    the 20 things it is.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么摩托车的例子不起作用。因为即使它到达右下角并想说“我认为这是一辆摩托车”，也没有回报。如果错了，就会被淘汰。而且大多数时候，它是背景。即使不是背景，仅仅说“这不是背景”是不够的
    — 你必须说它是20件事物中的哪一个。
- en: So the trick is to trying to find a different loss function [[1:44:00](https://youtu.be/0frKXR-2PBY?t=1h44m)]
    that looks more like the purple line. Focal loss is literally just a scaled cross
    entropy loss. Now if we say “I’m .6 sure it’s not a motorbike” then the loss function
    will say “good for you! no worries” [[1:44:42](https://youtu.be/0frKXR-2PBY?t=1h44m42s)].
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 所以诀窍是尝试找到一个更像紫线的不同损失函数。焦点损失实际上只是一个缩放的交叉熵损失。现在如果我们说“我有60%的把握这不是摩托车”，那么损失函数会说“干得好！没问题”。
- en: The actual contribution of this paper is to add `(1 − pt)^γ` to the start of
    the equation [[1:45:06](https://youtu.be/0frKXR-2PBY?t=1h45m6s)] which sounds
    like nothing but actually people have been trying to figure out this problem for
    years. When you come across a paper like this which is game-changing, you shouldn’t
    assume you are going to have to write thousands of lines of code. Very often it
    is one line of code, or the change of a single constant, or adding log to a single
    place.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文的实际贡献是在方程的开头添加`(1 − pt)^γ`，听起来像无关紧要的事情，但实际上人们多年来一直在努力解决这个问题。当你遇到这样一个改变游戏规则的论文时，不要假设你将不得不编写成千上万行的代码。很多时候只是一行代码，或者改变一个常数，或者在一个地方添加对数。
- en: 'A couple of terrific things about this paper [[1:46:08](https://youtu.be/0frKXR-2PBY?t=1h46m8s)]:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这篇论文的一些了不起的事情[[1:46:08](https://youtu.be/0frKXR-2PBY?t=1h46m8s)]：
- en: Equations are written in a simple manner
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程式以简单的方式编写
- en: They “refactor”
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 他们“重构”
- en: 'Implementing Focal Loss [[1:49:27](https://youtu.be/0frKXR-2PBY?t=1h49m27s)]:'
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现焦点损失[[1:49:27](https://youtu.be/0frKXR-2PBY?t=1h49m27s)]：
- en: 'Remember, -log(pt) is the cross entropy loss and focal loss is just a scaled
    version. When we defined the binomial cross entropy loss, you may have noticed
    that there was a weight which by default was none:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，-log(pt)是交叉熵损失，焦点损失只是一个缩放版本。当我们定义二项式交叉熵损失时，您可能已经注意到默认情况下没有权重：
- en: '[PRE48]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'When you call `F.binary_cross_entropy_with_logits`, you can pass in the weight.
    Since we just wanted to multiply a cross entropy by something, we can just define
    `get_weight`. Here is the entirety of focal loss [[1:50:23](https://youtu.be/0frKXR-2PBY?t=1h50m23s)]:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当您调用`F.binary_cross_entropy_with_logits`时，可以传入权重。由于我们只想将交叉熵乘以某个值，我们可以定义`get_weight`。这是焦点损失的全部内容[[1:50:23](https://youtu.be/0frKXR-2PBY?t=1h50m23s)]：
- en: '[PRE49]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'If you were wondering why alpha and gamma are 0.25 and 2, here is another excellent
    thing about this paper, because they tried lots of different values and found
    that these work well:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想知道为什么alpha和gamma是0.25和2，这篇论文的另一个优点是，因为他们尝试了许多不同的值，并发现这些值效果很好：
- en: Training [[1:51:25](https://youtu.be/0frKXR-2PBY?t=1h51m25s)]
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练[[1:51:25](https://youtu.be/0frKXR-2PBY?t=1h51m25s)]
- en: '[PRE50]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This time things are looking quite a bit better. So our last step, for now,
    is to basically figure out how to pull out just the interesting ones.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这次情况看起来好多了。因此，我们现在的最后一步是基本上弄清楚如何只提取感兴趣的部分。
- en: Non Maximum Suppression [[1:52:15](https://youtu.be/0frKXR-2PBY?t=1h52m15s)]
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非极大值抑制[[1:52:15](https://youtu.be/0frKXR-2PBY?t=1h52m15s)]
- en: All we are going to do is we are going to go through every pair of these bounding
    boxes and if they overlap by more than some amount, say 0.5, using Jaccard and
    they are both predicting the same class, we are going to assume they are the same
    thing and we are going to pick the one with higher `p` value.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要做的就是遍历每对这些边界框，如果它们重叠超过一定数量，比如0.5，使用Jaccard并且它们都预测相同的类别，我们将假设它们是相同的东西，并且我们将选择具有更高`p`值的那个。
- en: It is really boring code, Jeremy didn’t write it himself and copied somebody
    else’s. No reason particularly to go through it.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 这是非常无聊的代码，Jeremy自己没有写，而是复制了别人的。没有特别的原因要去研究它。
- en: '[PRE52]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: There are some things still to fix here [[1:53:43](https://youtu.be/0frKXR-2PBY?t=1h53m43s)].
    The trick will be to use something called feature pyramid. That is what we are
    going to do in lesson 14.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有一些需要修复的地方[[1:53:43](https://youtu.be/0frKXR-2PBY?t=1h53m43s)]。技巧将是使用称为特征金字塔的东西。这就是我们将在第14课中做的事情。
- en: Talking a little more about SSD paper [[1:54:03](https://youtu.be/0frKXR-2PBY?t=1h54m3s)]
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于SSD论文的讨论[[1:54:03](https://youtu.be/0frKXR-2PBY?t=1h54m3s)]
- en: When this paper came out, Jeremy was excited because this and YOLO were the
    first kind of single-pass good quality object detection method that come along.
    There has been this continuous repetition of history in the deep learning world
    which is things that involve multiple passes of multiple different pieces, over
    time, particularly where they involve some non-deep learning pieces (like R-CNN
    did), over time, they always get turned into a single end-to-end deep learning
    model. So I tend to ignore them until that happens because that’s the point where
    people have figured out how to show this as a deep learning model, as soon as
    they do that they generally end up something much faster and much more accurate.
    So SSD and YOLO were really important.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 当这篇论文出来时，Jeremy很兴奋，因为这和YOLO是第一种单次通过的高质量目标检测方法。在深度学习世界中存在这种连续的历史重复，即涉及多次通过多个不同部分的事物，特别是当它们涉及一些非深度学习部分（如R-CNN）时，随着时间的推移，它们总是被转化为单一的端到端深度学习模型。因此，我倾向于忽略它们，直到发生这种情况，因为那是人们已经找到如何将其展示为深度学习模型的时候，一旦他们这样做，它们通常会变得更快更准确。因此，SSD和YOLO非常重要。
- en: The model is 4 paragraphs. Papers are really concise which means you need to
    read them pretty carefully. Partly, though, you need to know which bits to read
    carefully. The bits where they say “here we are going to prove the error bounds
    on this model,” you could ignore that because you don’t care about proving error
    bounds. But the bit which says here is what the model is, you need to read real
    carefully.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型有4段。论文非常简洁，这意味着您需要非常仔细地阅读它们。但部分原因是，您需要知道哪些部分需要仔细阅读。当他们说“在这里我们将证明该模型的误差界限”时，您可以忽略，因为您不关心证明误差界限。但是当他们说这就是模型时，您需要仔细阅读。
- en: Jeremy reads a section **2.1 Model** [[1:56:37](https://youtu.be/0frKXR-2PBY?t=1h56m37s)]
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy阅读了一个部分**2.1模型**[[1:56:37](https://youtu.be/0frKXR-2PBY?t=1h56m37s)]
- en: If you jump straight in and read a paper like this, these 4 paragraphs would
    probably make no sense. But now that we’ve gone through it, you read those and
    hopefully thinking “oh that’s just what Jeremy said, only they sad it better than
    Jeremy and less words [[2:00:37](https://youtu.be/0frKXR-2PBY?t=2h37s)]. If you
    start to read a paper and go “what the heck”, the trick is to then start reading
    back over the citations.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您直接阅读这样的论文，这4段可能毫无意义。但是现在我们已经阅读过了，您阅读这些内容时，希望会想到“哦，这就是Jeremy说的，只是他们比Jeremy说得更好，用词更少[[2:00:37](https://youtu.be/0frKXR-2PBY?t=2h37s)]。如果您开始阅读一篇论文并说“到底是什么”，那么技巧就是开始回顾引文。
- en: Jeremy reads **Matching strategy** and **Training objective** (a.k.a. Loss function)[[2:01:44](https://youtu.be/0frKXR-2PBY?t=2h1m44s)]
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: Jeremy阅读**匹配策略**和**训练目标**（也称为损失函数）[[2:01:44](https://youtu.be/0frKXR-2PBY?t=2h1m44s)]
- en: Some paper tips [[2:02:34](https://youtu.be/0frKXR-2PBY?t=2h2m34s)]
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一些论文提示[[2:02:34](https://youtu.be/0frKXR-2PBY?t=2h2m34s)]
- en: '[Scalable Object Detection using Deep Neural Networks](https://arxiv.org/pdf/1312.2249.pdf)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[使用深度神经网络的可扩展目标检测](https://arxiv.org/pdf/1312.2249.pdf)'
- en: “Training objective” is loss function
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “训练目标”是损失函数
- en: Double bars and two 2’s like this means Mean Squared Error
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双条和两个2，像这样表示均方误差
- en: 'log(c) and log(1-c), and x and (1-x) they are all the pieces for binary cross
    entropy:'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: log(c)和log(1-c)，以及x和(1-x)它们都是二元交叉熵的组成部分。
- en: This week, go through the code and go through the paper and see what is going
    on. Remember what Jeremy did to make it easier for you was he took that loss function,
    he copied it into a cell and split it up so that each bit was in a separate cell.
    Then after every sell, he printed or plotted that value. Hopefully this is a good
    starting point.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这周，浏览代码和论文，看看发生了什么。记住Jeremy为了让你更容易理解，他将损失函数复制到一个单元格中，并将其拆分，使每个部分都在单独的单元格中。然后在每次出售后，他打印或绘制该值。希望这是一个好的起点。
