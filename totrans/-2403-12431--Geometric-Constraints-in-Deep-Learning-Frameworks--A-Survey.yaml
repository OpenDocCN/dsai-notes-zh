- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:33:52'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:33:52
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2403.12431] Geometric Constraints in Deep Learning Frameworks: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2403.12431] 深度学习框架中的几何约束：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2403.12431](https://ar5iv.labs.arxiv.org/html/2403.12431)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2403.12431](https://ar5iv.labs.arxiv.org/html/2403.12431)
- en: 'Geometric Constraints in Deep Learning Frameworks: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习框架中的几何约束：综述
- en: Vibhas K Vats, David J Crandall
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Vibhas K Vats, David J Crandall
- en: Indiana University Bloomington
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 印第安纳大学布卢明顿
- en: '{vkvats, djcran}@iu.edu'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '{vkvats, djcran}@iu.edu'
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Stereophotogrammetry is an emerging technique of scene understanding. Its origins
    go back to at least the 1800s when people first started to investigate using photographs
    to measure the physical properties of the world. Since then, thousands of approaches
    have been explored. The classic geometric techniques of Shape from Stereo is built
    on using geometry to define constraints on scene and camera geometry and then
    solving the non-linear systems of equations. More recent work has taken an entirely
    different approach, using end-to-end deep learning without any attempt to explicitly
    model the geometry. In this survey, we explore the overlap for geometric-based
    and deep learning-based frameworks. We compare and contrast geometry enforcing
    constraints integrated into a deep learning framework for depth estimation or
    other closely related problems. We present a new taxonomy for prevalent geometry
    enforcing constraints used in modern deep learning frameworks. We also present
    insightful observations and potential future research directions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 立体摄影测量是一种新兴的场景理解技术。它的起源可以追溯到至少1800年代，当时人们首次开始研究利用照片测量世界的物理属性。从那时起，已经探索了数千种方法。经典的立体几何技术建立在使用几何学定义场景和相机几何约束的基础上，然后求解非线性方程系统。更近期的工作采用了完全不同的方法，使用端到端的深度学习，而没有尝试明确建模几何。在这篇综述中，我们探讨了几何基础和深度学习基础框架的重叠。我们比较和对比了集成到深度学习框架中的几何约束，用于深度估计或其他密切相关的问题。我们提出了一种新的分类法，用于现代深度学习框架中常见的几何约束。我们还提出了有见地的观察和潜在的未来研究方向。
- en: '*K*eywords Depth Estimation  $\cdot$ Monocular  $\cdot$ Stereo  $\cdot$ Multi-view
    Stereo  $\cdot$ Geometric Constraints  $\cdot$ Stereophotogrammetry  $\cdot$ Scene
    understanding  $\cdot$ self-supervised Depth Estimation  $\cdot$ Photometric Consistency
     $\cdot$ Smoothness  $\cdot$ Geometric Representations  $\cdot$ Structural Consistency'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '*关键字* 深度估计  $\cdot$ 单目  $\cdot$ 立体  $\cdot$ 多视角立体  $\cdot$ 几何约束  $\cdot$ 立体摄影测量
     $\cdot$ 场景理解  $\cdot$ 自监督深度估计  $\cdot$ 光度一致性  $\cdot$ 光滑性  $\cdot$ 几何表示  $\cdot$
    结构一致性'
- en: 1 Introduction
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 介绍
- en: '![Refer to caption](img/d3116ca853e1b16a91818f1fe0496c6e.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![参见图例](img/d3116ca853e1b16a91818f1fe0496c6e.png)'
- en: 'Figure 1: Overview of geometric constraints concepts covered in the survey
    paper. We discuss geometric constraints used in deep learning-based depth estimation
    and other closely related frameworks.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：调查论文中涵盖的几何约束概念概述。我们讨论了在深度学习基础的深度估计和其他密切相关框架中使用的几何约束。
- en: Traditional stereo or multi-view stereo (MVS) depth estimation methods rely
    on solving for photometric and geometric consistency constraints across view(s)
    for consistent depth estimation [[1](#bib.bib1), [2](#bib.bib2), [3](#bib.bib3),
    [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7), [8](#bib.bib8)].
    With the phenomenal rise of deep learning frameworks [[9](#bib.bib9)], like Convolutional
    neural networks (CNNs) [[10](#bib.bib10)], Recurrent neural networks (RNNs) [[11](#bib.bib11)],
    and Vision transformers (ViTs) [[12](#bib.bib12)], which can extract deep local
    and high-level features, the the requirement to apply photometric and geometric
    consistency constraints has significantly reduced, especially in supervised depth
    estimation methods. Deeper features significantly improved feature matching leading
    to a huge improvement in depth estimates [[13](#bib.bib13), [14](#bib.bib14),
    [15](#bib.bib15), [16](#bib.bib16)]. The application of geometric constraints
    remains limited to the use of plane-sweep algorithm [[17](#bib.bib17)] form majority
    of supervised stereo and MVS methods.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的立体或多视角立体（MVS）深度估计方法依赖于解决视图间的光度和几何一致性约束，以实现一致的深度估计[[1](#bib.bib1), [2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8)]。随着深度学习框架[[9](#bib.bib9)]的飞速发展，如卷积神经网络（CNNs）[[10](#bib.bib10)]、递归神经网络（RNNs）[[11](#bib.bib11)]和视觉变换器（ViTs）[[12](#bib.bib12)]，它们可以提取深层的局部和高级特征，光度和几何一致性约束的需求显著减少，特别是在监督深度估计方法中。更深的特征显著改善了特征匹配，从而大幅提升了深度估计[[13](#bib.bib13),
    [14](#bib.bib14), [15](#bib.bib15), [16](#bib.bib16)]。几何约束的应用仍然局限于平面扫描算法[[17](#bib.bib17)]，这是大多数监督立体和MVS方法的主要形式。
- en: 'In a typical supervised stereo or MVS depth estimation framework, the plane-sweep
    algorithm is applied to create a matching (cost) volume, which is then aggregated
    based on a metric. The aggregated volume – cost volume, is then regularized using
    3D-CNNs or RNNs to produce a coherent estimate. The lack of ground truth in unsupervised/self-supervised
    depth estimation methods do not allow such freedom. Photometric and geometric
    consistency constraints remain a key part of unsupervised frameworks [[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]. Some
    of the other closely related problems, like structure from motion [[23](#bib.bib23),
    [24](#bib.bib24)], video-depth estimation [[25](#bib.bib25), [26](#bib.bib26)],
    semantic-segmentation [[27](#bib.bib27), [28](#bib.bib28), [29](#bib.bib29), [18](#bib.bib18)],
    and monocular depth estimation [[30](#bib.bib30), [31](#bib.bib31)], also apply
    various geometric constraints for a consistent result. In this survey, we focus
    on all such methods that integrate photometric or geometric constraints in deep
    learning-based frameworks and are closely related to depth estimation problem.
    Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey") shows the collection of such geometric constraints and
    their associated problems, that are covered in this survey. We discuss the theory
    and mathematical formulation of all these concepts and present a carefully crafted
    taxonomy, see Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey").'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '在典型的监督立体或MVS深度估计框架中，平面扫描算法用于创建一个匹配（成本）体积，然后根据度量进行聚合。聚合后的体积——成本体积，接着使用3D-CNN或RNN进行正则化，以产生一致的估计。无监督/自监督深度估计方法中缺乏地面真值，这限制了这种自由度。光度和几何一致性约束仍然是无监督框架的关键部分[[18](#bib.bib18),
    [19](#bib.bib19), [20](#bib.bib20), [21](#bib.bib21), [22](#bib.bib22)]。一些其他密切相关的问题，如运动重建[[23](#bib.bib23),
    [24](#bib.bib24)]、视频深度估计[[25](#bib.bib25), [26](#bib.bib26)]、语义分割[[27](#bib.bib27),
    [28](#bib.bib28), [29](#bib.bib29), [18](#bib.bib18)]和单目深度估计[[30](#bib.bib30),
    [31](#bib.bib31)]，也应用了各种几何约束以获得一致的结果。在本次调查中，我们专注于所有在深度学习框架中集成光度或几何约束的方法，并且与深度估计问题密切相关。图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")展示了在本次调查中涵盖的几何约束及其相关问题。我们讨论了所有这些概念的理论和数学公式，并提供了精心制作的分类法，参见图[2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")。'
- en: '![Refer to caption](img/99bf324908b72b7c412f25dabf8cab2b.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/99bf324908b72b7c412f25dabf8cab2b.png)'
- en: 'Figure 2: Our taxonomy of current geometric constraints used in deep learning-based
    depth estimation and closely related frameworks.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：我们当前在基于深度学习的深度估计和密切相关框架中使用的几何约束分类法。
- en: 'Throughout this paper, we keep our focus on geometry-enforcing concepts used
    across different problems that either do depth estimation or are closely related
    to depth estimation problems. We only discuss the specific concepts used and their
    relevance to stereo or MVS depth estimation frameworks. This survey is organized
    in $8$ sections. Starting from Sec. [2](#S2 "2 Plane Sweep Algorithm ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey"), we discuss the broad classification
    of geometric constraints presented in our taxonomy show in Fig. [2](#S1.F2 "Figure
    2 ‣ 1 Introduction ‣ Geometric Constraints in Deep Learning Frameworks: A Survey").
    For most of the sections, we first describe the a most common mathematical formulation
    of the concept that covers the majority of the methods and then, we describe different
    modifications applied to it by specific methods. Sec. [2](#S2 "2 Plane Sweep Algorithm
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") describes the
    traditional plane sweep algorithm and its variants. Sec. [3](#S3 "3 Cross-View
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") focuses
    on all such geometric constraints that use alternate view(s) for enforcing consistency
    (cross-view consistency). Sec. [4](#S4 "4 Geometry Preserving Constraints ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey") delves into geometric constraints
    that enforce structural consistency between a reference image and a target image
    to preserve the structural integrity of the scenes. Sec. [5](#S5 "5 Normal-Depth
    Orthogonal Constraint ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")
    focuses on the orthogonal relation between depth and surface normal to guide geometric
    consistency. Sec. [6](#S6 "6 Attention Meets Geometry ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") discusses the integration of geometric
    constraints in attention mechanism and Sec. [7](#S7 "7 Learning Geometric Representations
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") presents the methods
    to enforce geometry-based representation learning in deep neural networks. We
    present our conclusion in Sec. [8](#S8 "8 Conclusion ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey").'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们将重点放在不同问题中使用的几何约束概念，这些问题涉及深度估计或与深度估计问题紧密相关。我们仅讨论所用的具体概念及其与立体视觉或MVS深度估计框架的相关性。本文分为$8$个部分组织。自第[2](#S2
    "2 平面扫描算法 ‣ 深度学习框架中的几何约束：综述")节开始，我们讨论我们分类法中展示的几何约束的广泛分类，如图[2](#S1.F2 "图 2 ‣ 1
    介绍 ‣ 深度学习框架中的几何约束：综述")所示。在大多数部分中，我们首先描述覆盖大多数方法的最常见数学公式，然后描述具体方法对其进行的不同修改。第[2](#S2
    "2 平面扫描算法 ‣ 深度学习框架中的几何约束：综述")节描述了传统的平面扫描算法及其变体。第[3](#S3 "3 交叉视图约束 ‣ 深度学习框架中的几何约束：综述")节重点介绍所有使用备用视图进行一致性约束的几何约束（交叉视图一致性）。第[4](#S4
    "4 几何保持约束 ‣ 深度学习框架中的几何约束：综述")节深入探讨强制参考图像与目标图像之间结构一致性的几何约束，以保持场景的结构完整性。第[5](#S5
    "5 法线-深度正交约束 ‣ 深度学习框架中的几何约束：综述")节重点关注深度与表面法线之间的正交关系，以指导几何一致性。第[6](#S6 "6 注意力与几何
    ‣ 深度学习框架中的几何约束：综述")节讨论几何约束在注意力机制中的整合，第[7](#S7 "7 学习几何表示 ‣ 深度学习框架中的几何约束：综述")节介绍在深度神经网络中强制几何表示学习的方法。我们在第[8](#S8
    "8 结论 ‣ 深度学习框架中的几何约束：综述")节中提出我们的结论。
- en: 2 Plane Sweep Algorithm
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 平面扫描算法
- en: '![Refer to caption](img/2a34e68401a830b22e1c86a7abf6dce1.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/2a34e68401a830b22e1c86a7abf6dce1.png)'
- en: 'Figure 3: Epipolar geometry, Left: epipolar line segment corresponding to one
    ray (Fig. reused from [[8](#bib.bib8)]), Center: corresponding set of epipolar
    lines and their epipolar plane (Fig. reused from [[8](#bib.bib8)]), $R$, $t$ are
    the rotation and translation parameters, and $l_{i}$ is the epipolar line segment.
    Right: illustration of plane-sweep algorithm (Fig. reused from[[32](#bib.bib32)])'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：极几何，左：对应于一条光线的极线段（图自[[8](#bib.bib8)]），中：对应的极线集合及其极面（图自[[8](#bib.bib8)]），$R$、$t$
    是旋转和位移参数，$l_{i}$ 是极线段。右：平面扫描算法的示意图（图自[[32](#bib.bib32)]）
- en: Stereophotogrammetry is the process of estimating the 3D coordinates of points
    on an object by utilizing measurements from two or more images of the object taken
    from different positions [[33](#bib.bib33)]. This involves stereo matching where
    two or more images are used to find matching pixels in the images and convert
    their 2D positions into 3D depths [[8](#bib.bib8)]. The process of finding matching
    pixels is based on the geometry of stereo matching (epipolar geometry), i.e the
    process of computing the range of possible locations of a pixel from an image
    that might appear in another image. In this section, first, we discuss the epipolar
    geometry for two rectified images and then describe a general resampling algorithm
    , plane sweep, that can be used to perform multi-image stereo matching with arbitrary
    camera configuration.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 立体摄影测量是通过利用从不同位置拍摄的两个或多个对象图像的测量来估计对象上点的3D坐标的过程 [[33](#bib.bib33)]。这涉及到立体匹配，其中使用两个或多个图像来找到图像中的匹配像素，并将其2D位置转换为3D深度
    [[8](#bib.bib8)]。找到匹配像素的过程基于立体匹配的几何学（极几何），即计算一个图像中的像素可能出现在另一个图像中的范围。在本节中，我们首先讨论两个校正图像的极几何，然后描述一种通用的重采样算法，平面扫掠，用于执行具有任意相机配置的多图像立体匹配。
- en: 'Fig. [3](#S2.F3 "Figure 3 ‣ 2 Plane Sweep Algorithm ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") (left) shows the epipolar constraints
    of how a pixel in one image $x_{0}$ projects to an epipolar line segment in the
    other image. The line segment is bounded by $p_{\infty}$ – projection of the original
    viewing ray, and $c_{0}$ – projection of the original camera center into the second
    camera, called epipole $e_{1}$. The projections of the epipolar line in the second
    image back into the first image give us another line segment bounded by corresponding
    epipole $e_{0}$. Extension of these two line segments to infinity gives us a pair
    of corresponding epipolar lines, Fig. [3](#S2.F3 "Figure 3 ‣ 2 Plane Sweep Algorithm
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")(center), that
    are the intersection of the two image planes, epipolar planes, that passes through
    both camera centers $c_{0}$ and $c_{1}$. $p$ is the point of interest [[6](#bib.bib6),
    [7](#bib.bib7)].'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [3](#S2.F3 "Figure 3 ‣ 2 Plane Sweep Algorithm ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey")（左）显示了一个图像中的像素$x_{0}$如何投影到另一个图像中的极线段。该线段由$p_{\infty}$—原始视线的投影和$c_{0}$—原始相机中心在第二个相机中的投影（称为极点$e_{1}$）界定。第二个图像中极线的投影回到第一个图像中，给我们另一个由对应极点$e_{0}$界定的线段。这两个线段扩展到无穷大，给我们一对对应的极线，图
    [3](#S2.F3 "Figure 3 ‣ 2 Plane Sweep Algorithm ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey")（中），这是通过两个相机中心$c_{0}$和$c_{1}$的两个图像平面、极平面的交集。$p$是感兴趣的点
    [[6](#bib.bib6), [7](#bib.bib7)]。'
- en: 'Multi-image stereo reconstruction is a process to recover the 3D scene structure
    from multiple overlapping images with known intrinsic ($K$) and extrinsic ($E$)
    camera parameters [[17](#bib.bib17)]. More precisely, source images are projected
    to fronto-parallel planes of the reference camera frustum, and the photoconsistency
    of these images is analyzed, see Fig. [3](#S2.F3 "Figure 3 ‣ 2 Plane Sweep Algorithm
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") (right). This
    process is commonly known as plane-sweep approach [[17](#bib.bib17), [34](#bib.bib34),
    [35](#bib.bib35), [32](#bib.bib32)]'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '多图像立体重建是从多个重叠图像中恢复3D场景结构的过程，已知内参($K$)和外参($E$)相机参数 [[17](#bib.bib17)]。更准确地说，源图像被投影到参考相机棱锥的前向平面中，并分析这些图像的光度一致性，见图
    [3](#S2.F3 "Figure 3 ‣ 2 Plane Sweep Algorithm ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey")（右）。这个过程通常被称为平面扫掠方法 [[17](#bib.bib17), [34](#bib.bib34),
    [35](#bib.bib35), [32](#bib.bib32)]。'
- en: 'The plane-sweep method is based on the premise that areas of space where several
    image features viewing rays intersect are likely to be the 3D location of observed
    scene features. In this method, a single plane partitioned into cells is swept
    through the volume of space along a line perpendicular to the plane [[17](#bib.bib17)],
    perpendicular to reference camera frustum as shown in Fig. [3](#S2.F3 "Figure
    3 ‣ 2 Plane Sweep Algorithm ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") (right). At each position of the plane along the sweeping path, the
    number of intersecting viewing rays is tallied. This is done by back-projecting
    features from each source image onto the sweeping plane and noting the features
    that fall within some threshold of back-projected point position. Cells with the
    largest tallies are hypothesized as the location of 3D scene features, corresponding
    depth hypothesis in the case of depth maps, are selected [[32](#bib.bib32), [17](#bib.bib17)].'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 平面扫描方法基于这样一个前提：多个图像特征视线交叉的空间区域很可能是观察场景特征的3D位置。在该方法中，一个分成单元的平面沿着垂直于平面的线 [[17](#bib.bib17)]
    扫描空间体积，垂直于参考相机视锥体，如图 [3](#S2.F3 "图 3 ‣ 2 平面扫描算法 ‣ 深度学习框架中的几何约束")（右图）所示。在平面沿扫描路径的每个位置，交叉视线的数量被统计。通过将每个源图像的特征反投影到扫描平面上，并记录落在反投影点位置某个阈值范围内的特征来完成这一过程。具有最大统计量的单元被假设为3D场景特征的位置，对于深度图像，选择相应的深度假设
    [[32](#bib.bib32), [17](#bib.bib17)]。
- en: The plane-sweep method can directly estimate the disparity (for stereo) or depth
    values (for MVS), but modern deep learning-based frameworks use it to create matching
    volume corresponding to each source image and the reference image. The matching
    volume is aggregated based on a metric to create a cost volume. The cost volume
    is then regularized either by 3D-CNNs [[13](#bib.bib13), [14](#bib.bib14), [16](#bib.bib16),
    [15](#bib.bib15)] or by RNNs [[36](#bib.bib36), [37](#bib.bib37)]. The metric
    used for cost volume aggregation can vary from method to method. For example,
    Huang et al. [[38](#bib.bib38)] compute pairwise matching costs between a reference
    image and neighboring source images and fuse them with max-pooling. MVSNet [[13](#bib.bib13)],
    R-MVSNet [[36](#bib.bib36)] and CasMVSNet [[14](#bib.bib14)] computes the variance
    between all encoded features warped onto each sweep plane and regularizes it with
    3D-U-Net [[39](#bib.bib39)]. TransMVSNet [[15](#bib.bib15)] computes similarity-based
    cost volume for regularization. Yang et al. [[40](#bib.bib40)] reuse cost volume
    from the previous stage, along with the partial cost volume of the current stage
    in the multi-stage MVS framework. They create a pyramidal structure in cost volume
    creation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 平面扫描方法可以直接估计视差（用于立体视觉）或深度值（用于多视角立体），但现代基于深度学习的框架使用它来创建与每个源图像和参考图像对应的匹配体积。匹配体积基于某种度量聚合，以创建成本体积。成本体积然后通过3D-CNN
    [[13](#bib.bib13), [14](#bib.bib14), [16](#bib.bib16), [15](#bib.bib15)] 或 RNN
    [[36](#bib.bib36), [37](#bib.bib37)] 进行正则化。用于成本体积聚合的度量可以因方法而异。例如，Huang等人 [[38](#bib.bib38)]
    计算参考图像与邻近源图像之间的成对匹配成本，并通过最大池化融合它们。MVSNet [[13](#bib.bib13)], R-MVSNet [[36](#bib.bib36)]
    和 CasMVSNet [[14](#bib.bib14)] 计算所有编码特征在每个扫描平面上的方差，并使用3D-U-Net [[39](#bib.bib39)]
    进行正则化。TransMVSNet [[15](#bib.bib15)] 计算基于相似度的成本体积进行正则化。Yang等人 [[40](#bib.bib40)]
    在多阶段MVS框架中重复使用前一阶段的成本体积以及当前阶段的部分成本体积。他们在成本体积创建中创建了一个金字塔结构。
- en: Plane sweeping typically assumes surfaces to be fronto-parallel, which causes
    ambiguity in matching where slanted surfaces are involved like urban scenes [[41](#bib.bib41)].
    Gallup et al. [[41](#bib.bib41)] propose to perform multiple plane sweeps, where
    each plane sweep is intended to take care of planar surfaces with a particular
    normal. The proposed method is applied in three steps, first, the surface normals
    of the ground and facade planes are identified by analyzing their 3D points obtained
    through the sparse structure from motion. Then, a plane sweep for each surface
    normal is applied, resulting in multiple depth hypotheses for each pixel in the
    final depth map. Finally, the best depth/normal combination for each pixel is
    selected based on a cost or by multi-label graph cut method [[5](#bib.bib5), [42](#bib.bib42),
    [43](#bib.bib43)].
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 平面扫描通常假设表面为前平行，这会导致在涉及斜面（如城市场景）时匹配模糊[[41](#bib.bib41)]。Gallup等人[[41](#bib.bib41)]建议执行多次平面扫描，每次平面扫描旨在处理具有特定法向量的平面表面。该方法分三步进行：首先，通过分析通过稀疏运动结构获得的3D点来识别地面和立面平面的表面法向量。然后，对每个表面法向量应用平面扫描，从而在最终深度图中为每个像素生成多个深度假设。最后，根据成本或通过多标签图割方法[[5](#bib.bib5),
    [42](#bib.bib42), [43](#bib.bib43)]选择每个像素的最佳深度/法向组合。
- en: 3 Cross-View Constraints
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 跨视图约束
- en: Cross-view constraints are applied to a scene with more than one view. It can
    be applied to stereo (two views per scene) and MVS ($N>2$ view per scene) frameworks
    by projecting one view, either reference or source view, to the other view or
    vice-versa. Once projected to the other view, various constraints, like, photometric
    consistency, depth flow consistency, and view synthesis consistency can be utilized.
    An alternate way of utilizing cross-view constraints is to use forward-backward
    reprojection, where one view is projected to the other view and then it is back-projected
    to the first view to check the geometrical consistency of the scene. In this section,
    we discuss all such approaches that use cross-view consistency constraints in
    end-to-end deep learning-based frameworks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 跨视图约束应用于具有多个视图的场景。它可以应用于立体（每个场景两个视图）和MVS（每个场景`$N>2$`视图）框架，通过将一个视图（参考或源视图）投影到另一个视图，或反之。投影到另一个视图后，可以利用各种约束，如光度一致性、深度流一致性和视图合成一致性。另一种利用跨视图约束的方法是使用前向-后向重投影，其中一个视图被投影到另一个视图，然后回投影到第一个视图，以检查场景的几何一致性。在本节中，我们讨论所有在端到端深度学习框架中使用跨视图一致性约束的方法。
- en: 3.1 Photometric Consistency
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 光度一致性
- en: 'Photometric consistency minimizes the difference between a real image and a
    synthesized image from other views. The real and the synthesized images are denoted
    as reference ($I_{ref}$) and source ($I_{src}$) views in MVS, left ($I_{L}$) and
    right ($I_{R}$) or vice-versa in stereo problems. For video depth estimation,
    next frame ($I_{+}$) is compared with the current frame ($I_{0}$). The synthesized
    images are warped into real image views using intrinsic ($K$) and extrinsic ($E$)
    parameters. The warping process brings both real and synthesized images in the
    same camera view. The photometric loss is calculated as per Eqs. [1](#S3.E1 "In
    3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey")- [3](#S3.E3 "In 3.1 Photometric Consistency
    ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey"). Here, we use notations related to MVS problem formulation.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 光度一致性最小化真实图像和从其他视图合成图像之间的差异。在MVS中，真实图像和合成图像分别表示为参考视图（`$I_{ref}$`）和源视图（`$I_{src}$`），在立体问题中则为左视图（`$I_{L}$`）和右视图（`$I_{R}$`）或反之。对于视频深度估计，下一帧（`$I_{+}$`）与当前帧（`$I_{0}$`）进行比较。合成图像通过内参（`$K$`）和外参（`$E$`）参数被变换到真实图像视图中。这一变换过程将真实图像和合成图像带入同一相机视图中。光度损失按公式[1](#S3.E1
    "在3.1光度一致性 ‣ 3跨视图约束 ‣ 深度学习框架中的几何约束：综述")- [3](#S3.E3 "在3.1光度一致性 ‣ 3跨视图约束 ‣ 深度学习框架中的几何约束：综述")计算。在这里，我们使用与MVS问题公式相关的符号。
- en: 'Two main variations of photometric loss are, pixel photometric loss and gradient
    photometric loss. As the name suggests, pixel photometric loss is the comparison
    between pixel values of these images, and gradient photometric loss is the comparison
    of the gradients of these images. Sometimes, pixel photometric loss and gradient
    photometric loss are combined for a more robust form of photometric loss, called
    robust photometric loss. Eqs. [1](#S3.E1 "In 3.1 Photometric Consistency ‣ 3 Cross-View
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"), [2](#S3.E2
    "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") and [3](#S3.E3 "In 3.1 Photometric Consistency
    ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") shows the pixel, gradient, and robust formulation of photometric loss
    for MVS methods.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 光度损失的两个主要变种是像素光度损失和梯度光度损失。顾名思义，像素光度损失是这些图像的像素值之间的比较，而梯度光度损失是这些图像的梯度之间的比较。有时，像素光度损失和梯度光度损失会结合在一起，形成一种更鲁棒的光度损失形式，称为鲁棒光度损失。公式
    [1](#S3.E1 "在 3.1 光度一致性 ‣ 3 视角约束 ‣ 深度学习框架中的几何约束：综述")、[2](#S3.E2 "在 3.1 光度一致性 ‣
    3 视角约束 ‣ 深度学习框架中的几何约束：综述") 和 [3](#S3.E3 "在 3.1 光度一致性 ‣ 3 视角约束 ‣ 深度学习框架中的几何约束：综述")
    展示了 MVS 方法的像素、梯度和鲁棒光度损失的公式。
- en: '|  | $\displaystyle\mathcal{L}_{photo_{pixel}}$ | $\displaystyle=\frac{&#124;&#124;(I_{ref}-\hat{I}_{src\rightarrow
    ref})\odot M&#124;&#124;_{l_{i}}}{&#124;&#124;M&#124;&#124;_{1}}$ |  | (1) |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{photo_{pixel}}$ | $\displaystyle=\frac{&#124;&#124;(I_{ref}-\hat{I}_{src\rightarrow
    ref})\odot M&#124;&#124;_{l_{i}}}{&#124;&#124;M&#124;&#124;_{1}}$ |  | (1) |'
- en: '|  | $\displaystyle\mathcal{L}_{photo_{grad}}$ | $\displaystyle=\frac{&#124;&#124;(\triangledown
    I_{ref}-\triangledown\hat{I}_{src\rightarrow ref})\odot M&#124;&#124;_{l_{i}}}{&#124;&#124;M&#124;&#124;_{1}}$
    |  | (2) |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{photo_{grad}}$ | $\displaystyle=\frac{&#124;&#124;(\triangledown
    I_{ref}-\triangledown\hat{I}_{src\rightarrow ref})\odot M&#124;&#124;_{l_{i}}}{&#124;&#124;M&#124;&#124;_{1}}$
    |  | (2) |'
- en: '|  | $\displaystyle\mathcal{L}_{photo_{robust}}$ | $\displaystyle=\lambda_{1}.\mathcal{L}_{photo_{pixel}}+\lambda_{2}.\mathcal{L}_{photo_{grad}}$
    |  | (3) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{photo_{robust}}$ | $\displaystyle=\lambda_{1}.\mathcal{L}_{photo_{pixel}}+\lambda_{2}.\mathcal{L}_{photo_{grad}}$
    |  | (3) |'
- en: where, $l_{i}$ denotes $L_{1}$ or $L_{2}$ norm, $M$ denotes the mask, and $\lambda_{1},\lambda_{2}$
    denotes scaling factor for pixel and gradient photometric losses, respectively.
    $\odot$ denotes pixel-wise multiplication.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$l_{i}$ 表示 $L_{1}$ 或 $L_{2}$ 范数，$M$ 表示掩膜，而 $\lambda_{1},\lambda_{2}$ 分别表示像素和梯度光度损失的缩放因子。$\odot$
    表示逐像素相乘。
- en: 'There can be different ways of formulating Eqs [1](#S3.E1 "In 3.1 Photometric
    Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey"), [2](#S3.E2 "In 3.1 Photometric Consistency ‣ 3 Cross-View
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") and
    [3](#S3.E3 "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey") based on the choice of view
    to be warped. Most methods [[44](#bib.bib44), [45](#bib.bib45), [46](#bib.bib46),
    [22](#bib.bib22), [47](#bib.bib47), [48](#bib.bib48)] warp source views to the
    reference view called source-reference warp, as shown in Eqs [1](#S3.E1 "In 3.1
    Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey"), [2](#S3.E2 "In 3.1 Photometric Consistency
    ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") and [3](#S3.E3 "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"). But some methods
    warp the reference view to source views (reference-source warp) for estimating
    the photometric loss [[49](#bib.bib49)], replace src with ref and vice-versa in
    Eqs [1](#S3.E1 "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey"), [2](#S3.E2 "In 3.1 Photometric
    Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey") and [3](#S3.E3 "In 3.1 Photometric Consistency ‣ 3 Cross-View
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") to
    generate such a formulation. Other methods like, [[50](#bib.bib50)] use patch-wise
    photometric consistency to estimate the photometric loss. We highlight all such
    variations of photometric consistency formulation in the next few paragraphs.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '公式 [1](#S3.E1 "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey")、[2](#S3.E2 "In 3.1
    Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey") 和 [3](#S3.E3 "In 3.1 Photometric Consistency
    ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") 的公式化方法可能有所不同，取决于选择的视图进行变形。大多数方法 [[44](#bib.bib44), [45](#bib.bib45),
    [46](#bib.bib46), [22](#bib.bib22), [47](#bib.bib47), [48](#bib.bib48)] 将源视图变形为参考视图，称为源-参考变形，如公式
    [1](#S3.E1 "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey")、[2](#S3.E2 "In 3.1 Photometric
    Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey") 和 [3](#S3.E3 "In 3.1 Photometric Consistency ‣ 3 Cross-View
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") 所示。但有些方法将参考视图变形为源视图（参考-源变形）以估计光度损失
    [[49](#bib.bib49)]，在公式 [1](#S3.E1 "In 3.1 Photometric Consistency ‣ 3 Cross-View
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")、[2](#S3.E2
    "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") 和 [3](#S3.E3 "In 3.1 Photometric Consistency
    ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") 中用 ref 替换 src 以及 vice-versa 以生成这样的公式。其他方法如 [[50](#bib.bib50)] 使用基于补丁的光度一致性来估计光度损失。我们将在接下来的几段中突出所有这些光度一致性公式的变体。'
- en: We start with pixel photometric loss and its variations. Mallick et al. [[44](#bib.bib44)]
    use pixel formulation of photometric loss between reference and source views to
    enforce geometric consistency in adaptive learning approach for self-supervised
    MVS pipeline. Zhao et al. [[45](#bib.bib45)] use a similar formulation of pixel
    photometric loss in self-supervised monocular depth estimation problem to promote
    cross-view geometric consistency. Xu et al. [[51](#bib.bib51)] use a slightly
    different formulation of pixel photometric loss based on the condition that a
    pixel in one view finds a valid pixel in another view for semi-supervised MVS
    pipeline. The following equation represents this formulation
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从像素光度损失及其变体开始。Mallick 等人 [[44](#bib.bib44)] 使用参考视图和源视图之间的像素光度损失公式，以在自监督 MVS
    管道的自适应学习方法中强制几何一致性。赵等人 [[45](#bib.bib45)] 在自监督单目深度估计问题中使用类似的像素光度损失公式，以促进跨视图的几何一致性。徐等人
    [[51](#bib.bib51)] 使用了一种稍有不同的像素光度损失公式，该公式基于一个视图中的像素在另一个视图中找到有效像素的条件，用于半监督 MVS
    管道。以下方程表示这一公式
- en: '|  | $\mathcal{L}_{photo_{pixel}}=\frac{\Phi(1\leq\hat{p}_{i}\leq[H,W])&#124;&#124;I_{ref}(p_{i})-\hat{I}_{src\rightarrow
    ref}(p_{i})&#124;&#124;_{L_{2}}}{\Phi(1\leq\hat{p}_{i}\leq[H,W])}$ |  | (4) |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{photo_{pixel}}=\frac{\Phi(1\leq\hat{p}_{i}\leq[H,W])&#124;&#124;I_{ref}(p_{i})-\hat{I}_{src\rightarrow
    ref}(p_{i})&#124;&#124;_{L_{2}}}{\Phi(1\leq\hat{p}_{i}\leq[H,W])}$ |  | (4) |'
- en: where, $p_{i}$ denotes each pixel, $\Phi(1\leq\hat{p}_{i}\leq[H,W])$ indicates
    whether the current pixels $p_{i}$ can find a valid pixel $\hat{p}_{i}$ in other
    source view. $H,W$ denotes the height and width of the image, respectively.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$p_{i}$表示每个像素，$\Phi(1\leq\hat{p}_{i}\leq[H,W])$表示当前像素$p_{i}$是否能在其他源视图中找到一个有效像素$\hat{p}_{i}$。$H,W$分别表示图像的高度和宽度。
- en: Li et al. [[49](#bib.bib49)] use a slightly different formulation of pixel photometric
    loss by warping reference view to source views, instead of warping source view
    to the reference view, to calculate the $L_{1}$ distance between reference-source
    depth maps.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: Li等人[[49](#bib.bib49)]使用了略微不同的像素光度损失公式，通过将参考视图变形到源视图，而不是将源视图变形到参考视图，来计算参考-源深度图之间的$L_{1}$距离。
- en: '|  | $\mathcal{L}_{photo_{pixel}}=\frac{1}{N-1}\sum^{N}_{i=1}&#124;I_{src_{i}}-\hat{I}_{ref\rightarrow
    src_{i}}&#124;$ |  | (5) |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{photo_{pixel}}=\frac{1}{N-1}\sum^{N}_{i=1}&#124;I_{src_{i}}-\hat{I}_{ref\rightarrow
    src_{i}}&#124;$ |  | (5) |'
- en: where $N$ is the total number of source views.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$N$是源视图的总数。
- en: All the above-mentioned methods use pixel-wise warping operations to estimate
    photometric error. Yu et al. [[52](#bib.bib52)] propose patch-based warping of
    the extracted key points. It uses the point selection strategy from Direct Sparse
    Odometry (DSO) [[53](#bib.bib53)] and defines a support domain $\Omega(P_{i})$
    over each point $P_{i}$’s local window. The photometric error is then estimated
    over each support domain $\Omega(P_{i})$, instead of a single point. It is called
    patch photometric consistency. Dong and Yao [[50](#bib.bib50)] applied a similar
    approach to estimate patch photometric error. Unlike [[52](#bib.bib52)], which
    uses DSO to extract key points, it uses each pixel as a key point. It defines
    a $m^{2}$ sized square patch centering on the pixel $P$ as $\Omega(P)$. The local
    patch $\Omega(P)$ is small so that it can be treated as a plane [[52](#bib.bib52)]
    and it assumes that the patch shares the same depth as the center pixel. This
    patch is warped from the source view to the reference view and $L_{1}$ difference
    between pixel values is estimated as $\mathcal{L}_{photo_{patch}}$, given as
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述方法使用逐像素的变形操作来估计光度误差。Yu等人[[52](#bib.bib52)]提出了基于补丁的关键点提取变形方法。它使用来自Direct
    Sparse Odometry (DSO) [[53](#bib.bib53)] 的点选择策略，并在每个点$P_{i}$的局部窗口上定义一个支持域$\Omega(P_{i})$。然后在每个支持域$\Omega(P_{i})$上估计光度误差，而不是单一的点。这被称为补丁光度一致性。Dong和Yao
    [[50](#bib.bib50)] 采用了类似的方法来估计补丁光度误差。与[[52](#bib.bib52)]使用DSO提取关键点不同，它使用每个像素作为关键点。它定义了一个以像素$P$为中心的$m^{2}$大小的方形补丁为$\Omega(P)$。局部补丁$\Omega(P)$很小，因此可以视为平面[[52](#bib.bib52)]，并假设该补丁与中心像素具有相同的深度。该补丁从源视图变形到参考视图，并估计像素值之间的$L_{1}$差异，作为$\mathcal{L}_{photo_{patch}}$，表示为
- en: '|  | $\mathcal{L}_{photo_{patch}}=\frac{1}{N-1}\sum^{N}_{i=1}&#124;I_{ref}-\hat{I}_{src\rightarrow
    ref}&#124;\odot M_{ref}$ |  | (6) |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{photo_{patch}}=\frac{1}{N-1}\sum^{N}_{i=1}&#124;I_{ref}-\hat{I}_{src\rightarrow
    ref}&#124;\odot M_{ref}$ |  | (6) |'
- en: where $N$ indicates the number of source views and $M_{ref}$ indicates the reference
    view mask. $\odot$ denotes element-wise multiplication operation.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 其中$N$表示源视图的数量，$M_{ref}$表示参考视图掩码。$\odot$表示逐元素乘法操作。
- en: 'While pixel photometric consistency is significantly used to achieve better
    geometric consistency across views, their performance is susceptible to changes
    in lighting conditions. Change in lighting conditions makes enforcing pixel-level
    consistency difficult, but image gradients are more invariant to such changes.
    Many methods employ gradient photometric loss alongside pixel photometric loss.
    Since the addition of the gradient term makes the photometric loss more robust,
    it is called robust photometric loss. MVS methods like [[46](#bib.bib46), [22](#bib.bib22),
    [47](#bib.bib47), [48](#bib.bib48), [18](#bib.bib18)] use this formulation, Eq.
    [3](#S3.E3 "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey"), during end-to-end training.
    $\lambda_{1}$ and $\lambda_{2}$ are the tunable hyper-parameters in Eq. [3](#S3.E3
    "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey").'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管像素光度一致性在跨视角实现更好的几何一致性方面得到了广泛应用，但其性能容易受到光照条件变化的影响。光照条件的变化使得强制执行像素级一致性变得困难，但图像梯度对这些变化更具不变性。许多方法在像素光度损失的基础上同时采用梯度光度损失。由于梯度项的加入使得光度损失更为鲁棒，因此称为鲁棒光度损失。像[[46](#bib.bib46)、[22](#bib.bib22)、[47](#bib.bib47)、[48](#bib.bib48)、[18](#bib.bib18)]这样的MVS方法在端到端训练过程中使用这种公式，即公式[3](#S3.E3
    "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey")。$\lambda_{1}$ 和 $\lambda_{2}$ 是公式[3](#S3.E3
    "In 3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey")中的可调超参数。'
- en: While most mentioned MVS methods use asymmetric pipeline – estimating only the
    reference depth map using both the reference and source RGB images, Dai et al.
    [[54](#bib.bib54)] use a symmetric pipeline for MVS, i.e. the network predicts
    depth maps of all views simultaneously. With $N$ depth estimates, one per view,
    the method uses a bidirectional calculation of photometric consistency between
    each pair of views, called cross-view consistency loss. They do not use robust
    formulations for cross-view consistency.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数提到的MVS方法使用的是非对称管道——仅使用参考和源RGB图像来估计参考深度图，但Dai等人[[54](#bib.bib54)]使用了对称管道进行MVS，即网络同时预测所有视角的深度图。通过每个视角一个的
    $N$ 个深度估计，该方法使用每对视角之间的光度一致性的双向计算，称为视角一致性损失。他们不使用用于视角一致性的鲁棒公式。
- en: 3.2 Geometric Consistency
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 几何一致性
- en: 'Just like photometric consistency, geometric consistency also involves cross-view
    consistency checks with projections. For photometric consistency, one view, either
    reference or source, is warped to another view to calculate the consistency error.
    Geometric consistency employ forward-backward reprojection (FBR) to estimate the
    error. FBR involves a series of projections of the reference view to estimate
    the geometric error, see Alg. [1](#alg1 "Algorithm 1 ‣ 3.2 Geometric Consistency
    ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey"). First, we project the reference image ($I_{r}$) to the source view
    ($I_{r\rightarrow s}$), then, we remap the projected reference view $I_{r\rightarrow
    s}$ to generate $I_{s_{remap}}$ and finally, we back-project $I_{s_{remap}}$ to
    the reference view to obtain $I_{r\rightleftarrows s}$. $I_{r\rightleftarrows
    s}$ is then compared with original $I_{r}$ to estimate the photometric error.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '就像光度一致性一样，几何一致性也涉及到通过投影进行的视角一致性检查。对于光度一致性，一个视角，无论是参考视角还是源视角，都被扭曲到另一个视角以计算一致性误差。几何一致性使用前向-后向重投影（FBR）来估计误差。FBR包括一系列对参考视角的投影以估计几何误差，见算法[1](#alg1
    "Algorithm 1 ‣ 3.2 Geometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey")。首先，我们将参考图像（$I_{r}$）投影到源视角（$I_{r\rightarrow
    s}$），然后，我们将投影后的参考视角 $I_{r\rightarrow s}$ 重新映射以生成 $I_{s_{remap}}$，最后，我们将 $I_{s_{remap}}$
    反投影到参考视角以获得 $I_{r\rightleftarrows s}$。接着，将 $I_{r\rightleftarrows s}$ 与原始的 $I_{r}$
    进行比较，以估计光度误差。'
- en: 'Inputs: $I_{r},c_{s},I_{s},c_{s}$$K_{r},E_{r}\leftarrow c_{r}$; $K_{s},E_{s}\leftarrow
    c_{s}$$I_{(r\rightarrow s)}\leftarrow K_{s}\cdot E_{s}\cdot E_{r}^{-1}\cdot K_{r}^{-1}\cdot
    I_{r}$ $\triangleright$ Project$X_{I_{(r\rightarrow s)}},Y_{I_{(r\rightarrow s)}}\leftarrow
    I_{(r\rightarrow s)}$$I_{s_{remap}}\leftarrow REMAP(I_{s},X_{I_{(r\rightarrow
    s)}},Y_{I_{(r\rightarrow s)}})$ $\triangleright$ Remap$I_{r\rightleftarrows s}\leftarrow
    K_{r}\cdot E_{r}\cdot E_{s}^{-1}\cdot K_{s}^{-1}\cdot I_{s_{remap}}$ $\triangleright$
    Back project'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：$I_{r},c_{s},I_{s},c_{s}$$K_{r},E_{r}\leftarrow c_{r}$; $K_{s},E_{s}\leftarrow
    c_{s}$$I_{(r\rightarrow s)}\leftarrow K_{s}\cdot E_{s}\cdot E_{r}^{-1}\cdot K_{r}^{-1}\cdot
    I_{r}$ $\triangleright$ 投影$X_{I_{(r\rightarrow s)}},Y_{I_{(r\rightarrow s)}}\leftarrow
    I_{(r\rightarrow s)}$$I_{s_{remap}}\leftarrow REMAP(I_{s},X_{I_{(r\rightarrow
    s)}},Y_{I_{(r\rightarrow s)}})$ $\triangleright$ 重新映射$I_{r\rightleftarrows s}\leftarrow
    K_{r}\cdot E_{r}\cdot E_{s}^{-1}\cdot K_{s}^{-1}\cdot I_{s_{remap}}$ $\triangleright$
    反投影
- en: Algorithm 1 Forward Backward Reprojection (FBR)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 算法 1 前向后向重投影（FBR）
- en: Dong and Yao [[50](#bib.bib50)] use cross-view geometric consistency by applying
    FBR in the pixel domain in an unsupervised MVS pipeline. Once the FBR steps are
    done, the actual pixel values between the original reference image $I_{r}$ and
    back-projected reference images $I_{r\rightleftarrows s}$ are used to check the
    geometrical consistency of the depth estimates. It diminishes the matching ambiguity
    between reference and source views. The following equation shows its mathematical
    formulation
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Dong 和 Yao [[50](#bib.bib50)] 通过在像素域中应用 FBR 来使用跨视图几何一致性，进行无监督 MVS 流程。一旦完成 FBR
    步骤，实际像素值在原始参考图像 $I_{r}$ 和反投影参考图像 $I_{r\rightleftarrows s}$ 之间用于检查深度估计的几何一致性。它减少了参考视图和源视图之间的匹配歧义。以下方程展示了其数学公式
- en: '|  | $\mathcal{L}_{geometric}=\frac{1}{N}\sum^{N}_{i=1}&#124;I_{ref}-\hat{I}^{i}_{ref\rightleftarrows
    src}&#124;\odot M_{ref}$ |  | (7) |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{geometric}=\frac{1}{N}\sum^{N}_{i=1}\left|I_{ref}-\hat{I}^{i}_{ref\rightleftarrows
    src}\right|\odot M_{ref}$ |  | (7) |'
- en: where $N$ indicates the number of source views and $M_{ref}$ denotes the reference
    view mask. $\odot$ is the element-wise multiplication operation. Sometimes,
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $N$ 表示源视图的数量，$M_{ref}$ 表示参考视图掩模。$\odot$ 是逐元素乘法操作。有时，
- en: The Geometric consistency can be extended to the 3D coordinates of the camera
    using the concept of back-projection [[55](#bib.bib55), [23](#bib.bib23)]. Chen
    et al. [[23](#bib.bib23)] apply this modified geometric consistency in 3D space
    for a video depth estimation problem, called 3D geometric consistency. For a given
    source image pixel $P_{src}$ and corresponding reference image pixel $P_{ref}$,
    their 3D coordinates are obtained by back projection. The inconsistency between
    the estimates of the same 3D point is then estimated and used as a penalty. The
    loss value represents the 3D discrepancy of predictions from two views. [[55](#bib.bib55)]
    use a similar formulation to overcome the deficiencies of photometric loss in
    a self-supervised monocular depth estimation framework.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 几何一致性可以使用反投影的概念扩展到相机的 3D 坐标 [[55](#bib.bib55), [23](#bib.bib23)]。Chen 等人 [[23](#bib.bib23)]
    将这种修改后的几何一致性应用于 3D 空间，用于视频深度估计问题，称为 3D 几何一致性。对于给定的源图像像素 $P_{src}$ 和对应的参考图像像素 $P_{ref}$，它们的
    3D 坐标通过反投影获得。然后估计相同 3D 点的估计值之间的不一致性，并将其作为惩罚。损失值表示来自两个视图的预测的 3D 差异。[[55](#bib.bib55)]
    使用类似的公式来克服自监督单目深度估计框架中光度损失的缺陷。
- en: 3.3 Cross-View Depth-Flow Consistency
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 跨视图深度流一致性
- en: '![Refer to caption](img/201b85a46c9bbda3b1903b2e8edb87d8.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/201b85a46c9bbda3b1903b2e8edb87d8.png)'
- en: 'Figure 4: Intuition of Depth2Flow module. The relative motion of moving camera
    can be viewed as a moving object with a still camera to estimate optical flow.
    Figure reused from [[46](#bib.bib46)]'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：Depth2Flow 模块的直观图。移动相机的相对运动可以视为一个移动物体与静止相机来估计光流。图来自 [[46](#bib.bib46)]
- en: Depth-flow estimations are typically used in optical flow problems [[56](#bib.bib56),
    [57](#bib.bib57)]. But it can easily be adapted in MVS problems by estimating
    flow from estimated depth maps as well as from input RGB images and comparing
    them. Xu et al. [[46](#bib.bib46)] propose a novel flow-depth consistency loss
    to regularize the ambiguous supervision in the foreground of depth maps. Estimation
    of flow-depth consistency loss requires two modules for an MVS method, RGB2Flow
    and Depth2Flow. As the name suggests, the Depth2Flow module transforms the estimated
    depth maps to virtual optical flow between the reference and arbitrary source
    view and the RGB2Flow module uses [[56](#bib.bib56)] to predict the optical flow
    from the corresponding reference-source pairs. The two flows predicted should
    be consistent with each other.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 深度流估计通常用于光流问题[[56](#bib.bib56), [57](#bib.bib57)]。但是，也可以通过从估计深度图以及输入的RGB图像估算流并进行比较来轻松地应用于MVS问题。Xu等人[[46](#bib.bib46)]提出了一种新的流深度一致性损失，以规范深度图前景中的模糊监督。流深度一致性损失的估计需要MVS方法的两个模块，RGB2Flow和Depth2Flow。顾名思义，Depth2Flow模块将估计的深度图转换为参考和任意源视图之间的虚拟光流，RGB2Flow模块使用[[56](#bib.bib56)]从相应的参考源对预测出光流。预测的两个流应该是彼此一致的。
- en: Depth2Flow module
  id: totrans-63
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Depth2Flow模块
- en: 'In an MVS system, cameras move around the object to collect multi-view images.
    If we consider the relative motion between the object and the camera, the camera
    can be assumed to be fixed and the object can be assumed to be in motion towards
    the virtually still camera, see Fig. [4](#S3.F4 "Figure 4 ‣ 3.3 Cross-View Depth-Flow
    Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey"). This virtual motion can be represented as a dense 3D optical
    flow and it should be consistent with the 3D correspondence in real MVS systems.
    The virtual flow for a pixel $p_{i}$ can be defined as'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '在MVS系统中，摄像机围绕物体移动以收集多角度图像。如果考虑物体和摄像机之间的相对运动，摄像机可以被认为是固定的，而物体可以被认为是朝着几乎静止的摄像机运动，参见图[4](#S3.F4
    "Figure 4 ‣ 3.3 Cross-View Depth-Flow Consistency ‣ 3 Cross-View Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey")。这种虚拟运动可以表示为密集的3D光流，并且应该与真实MVS系统中的3D对应一致。像素$p_{i}$的虚拟流可以定义为'
- en: '|  | $\hat{F}_{r\rightarrow s}=Norm[K_{s}.E_{s}(K_{r}.T_{r})^{-1}I_{r}(p_{i})]-p_{i}$
    |  | (8) |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|  | $\hat{F}_{r\rightarrow s}=Norm[K_{s}.E_{s}(K_{r}.T_{r})^{-1}I_{r}(p_{i})]-p_{i}$
    |  | (8) |'
- en: RGB2Flow module
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: RGB2Flow模块
- en: To estimate the optical flow from RGB input images, a pre-trained model can
    be utilized. Xu et al. [[46](#bib.bib46)] use [[56](#bib.bib56)] to estimate the
    forward-flow, $F_{r\rightarrow s}$ and backward-flow $F_{s\rightarrow r}$. All
    two-view pairs combined with a reference view and a source view produce $F_{r\rightarrow
    s}$ and $F_{s\rightarrow r}$.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要从RGB输入图像中估算光流，可以利用预训练模型。Xu等人[[46](#bib.bib46)]使用[[56](#bib.bib56)]来估计前向光流$F_{r\rightarrow
    s}$和后向光流$F_{s\rightarrow r}$。所有参考视图和源视图组合的双视图配对都产生$F_{r\rightarrow s}$和$F_{s\rightarrow
    r$。
- en: To estimate the flow-depth consistency loss, we check the consistency of both
    $F_{r\rightarrow s}$ and $F_{s\rightarrow r}$ with the virtual flow $\hat{F}_{r\rightarrow
    s}$. For effective learning, the occluded parts are masked out with mask $M_{r\rightarrow
    s}$. The error is given as
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估算流深度一致性损失，我们检查$F_{r\rightarrow s}$和$F_{s\rightarrow r}$与虚拟流$\hat{F}_{r\rightarrow
    s}$的一致性。为了有效学习，遮挡部分使用遮罩$M_{r\rightarrow s}$掩盖。误差被给出为
- en: '|  | $\displaystyle M_{r\rightarrow s}$ | $\displaystyle=\{&#124;F_{r\rightarrow
    s}+F_{s\rightarrow r}&#124;>\epsilon\}$ |  | (9) |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle M_{r\rightarrow s}$ | $\displaystyle=\{&#124;F_{r\rightarrow
    s}+F_{s\rightarrow r}&#124;>\epsilon\}$ |  | (9) |'
- en: '|  | $\displaystyle\mathcal{L}_{flow}$ | $\displaystyle=\sum^{HW}_{i=1}\genfrac{}{}{0.0pt}{}{min}{2\leq
    s_{j}\leq V}\frac{&#124;&#124;F_{r\rightarrow s_{j}}(p_{i})-\hat{F}_{r\rightarrow
    s_{j}}(p_{i}).M(p_{i})&#124;&#124;_{2}}{\sum^{HW}_{i=1}M_{r\rightarrow s_{j}}(p_{i})}$
    |  | (10) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{flow}$ | $\displaystyle=\sum^{HW}_{i=1}\genfrac{}{}{0.0pt}{}{min}{2\leq
    s_{j}\leq V}\frac{&#124;&#124;F_{r\rightarrow s_{j}}(p_{i})-\hat{F}_{r\rightarrow
    s_{j}}(p_{i}).M(p_{i})&#124;&#124;_{2}}{\sum^{HW}_{i=1}M_{r\rightarrow s_{j}}(p_{i})}$
    |  | (10) |'
- en: 'where $\epsilon$ is threshold set to 0.5, $H$ and $W$ are the height and width
    of the image. Instead of averaging the difference between $F_{r\rightarrow s}$
    and $\hat{F}_{r\rightarrow s}$ on all source views, a minimum error is estimated
    at each pixel, see Eq [10](#S3.E10 "In RGB2Flow module ‣ 3.3 Cross-View Depth-Flow
    Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey"). Godard et al. [[58](#bib.bib58)] introduced the minimum
    error calculation to reject occluded pixels in depth estimation. Both the modules,
    Depth2Flow and RGB2Flow are fully differentiable and can be used in end-to-end
    training setups.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\epsilon$ 是设定为0.5的阈值，$H$ 和 $W$ 是图像的高度和宽度。不是对所有源视图的 $F_{r\rightarrow s}$
    和 $\hat{F}_{r\rightarrow s}$ 之间的差异进行平均，而是对每个像素估计最小误差，见公式[10](#S3.E10 "In RGB2Flow
    module ‣ 3.3 Cross-View Depth-Flow Consistency ‣ 3 Cross-View Constraints ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey")。Godard等人[[58](#bib.bib58)]引入了最小误差计算来拒绝深度估计中的遮挡像素。Depth2Flow和RGB2Flow两个模块都是完全可微的，并且可以用于端到端训练设置。'
- en: Chen et al. [[59](#bib.bib59)] use point-flow information to refine the estimated
    depth map in an MVS framework. Using extracted features from the input images
    and the previous stage estimated depth map, they generate a feature augmented
    point cloud and use point-flow module to learn the depth residual to refine the
    previously estimated depth map. The point-flow module estimates the displacement
    of a point to the ground truth surface along the reference camera direction by
    observing the neighboring points from all views.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 陈等人[[59](#bib.bib59)]在多视角立体视觉框架中使用点流信息来优化估计的深度图。通过使用输入图像和前一阶段估计的深度图中提取的特征，他们生成了一个特征增强的点云，并利用点流模块学习深度残差以优化之前估计的深度图。点流模块通过观察所有视角的邻近点，沿参考相机方向估计点到真实表面的位移。
- en: 3.4 View Synthesis Consistency
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4 视图合成一致性
- en: '![Refer to caption](img/a6c9d16bf47e6e15e410b274ae94ec02.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/a6c9d16bf47e6e15e410b274ae94ec02.png)'
- en: 'Figure 5: (a) Shows steps of view synthesis using RGB image and its depth map.
    (b) shows a bilinear interpolation step for adjusting values to pixel space. R
    and t are rotation and translation parameters. Figure reused from [[20](#bib.bib20)]'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：（a）展示了使用RGB图像及其深度图的视图合成步骤。（b）展示了调整像素空间值的双线性插值步骤。R 和 t 是旋转和位移参数。图像重用自[[20](#bib.bib20)]
- en: 'Most of the monocular, stereo, and MVS depth estimation frameworks use ground
    truth as a supervision signal. While these frameworks may utilize the additional
    source view images in the pipeline, they always estimate only one depth map, the
    reference view depth map. Estimation and use of only one depth map may not provide
    enough geometrical information for consistent estimation. To address this gap,
    many methods [[31](#bib.bib31), [20](#bib.bib20), [49](#bib.bib49), [54](#bib.bib54)]
    synthesis additional view (see Fig. [5](#S3.F5 "Figure 5 ‣ 3.4 View Synthesis
    Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey")), either depth map or RGB image (commonly referred to as
    target view), using camera parameters and reference view information. This additional
    view, when included in the training framework, provides additional geometric consistency
    information during the learning process. In this section, we discuss methods that
    utilize view synthesis in a depth estimation framework.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '大多数单目、立体和多视角深度估计框架使用真实深度作为监督信号。尽管这些框架可能在管道中利用额外的源视图图像，但它们总是仅估计一个深度图，即参考视图深度图。仅估计一个深度图可能无法提供足够的几何信息以保证一致的估计。为了解决这个问题，许多方法[[31](#bib.bib31),
    [20](#bib.bib20), [49](#bib.bib49), [54](#bib.bib54)]合成额外的视图（见图[5](#S3.F5 "Figure
    5 ‣ 3.4 View Synthesis Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey")），无论是深度图还是RGB图像（通常称为目标视图），使用相机参数和参考视图信息。这些额外的视图在训练框架中提供了额外的几何一致性信息。在这一节中，我们讨论了在深度估计框架中利用视图合成的方法。'
- en: Bauer et al. [[31](#bib.bib31)] use view synthesis in a monocular depth estimation
    framework. They apply two networks, depth network (DepNet) and synthesis network
    (SynNet) in a series of operations to enforce geometric constraints with view
    synthesis. First, the RGB input (source view) is used in DepNet to generate a
    corresponding depth estimate. The estimated depth map is projected to a target
    view and using SynNet the holes in the target view are filled. Finally, the synthesized
    RGB target view is used as input to DepNet to estimate its depth map. This ensures
    that the DepNet learns to estimate geometrically consistent depth estimates of
    both the source and the synthesized target view. They use $L_{1}$ loss to enforce
    consistency.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Bauer 等人 [[31](#bib.bib31)] 在单目深度估计框架中使用视图合成。他们在一系列操作中应用两个网络，深度网络（DepNet）和合成网络（SynNet），以通过视图合成来强制实施几何约束。首先，RGB
    输入（源视图）被用于 DepNet 生成相应的深度估计。估计的深度图被投影到目标视图上，然后使用 SynNet 填补目标视图中的空洞。最后，合成的 RGB
    目标视图被用作 DepNet 的输入，以估计其深度图。这确保了 DepNet 学会对源视图和合成的目标视图进行几何一致的深度估计。他们使用 $L_{1}$
    损失来强制一致性。
- en: Yang et al. [[20](#bib.bib20)] also synthesize RGB target view to improve geometric
    consistency in video depth estimation. With estimated pixel matching pairs between
    source ($I_{s}$) and target views ($I_{t}$), they synthesis a target view $\hat{I}_{s}$
    using the source view, camera parameters, and bilinear interpolation [[19](#bib.bib19)].
    To handle occlusion and movement of objects, an explainability mask ($M_{s}$)
    is applied during the loss calculation given as
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Yang 等人 [[20](#bib.bib20)] 也合成 RGB 目标视图，以提高视频深度估计中的几何一致性。通过源视图 ($I_{s}$) 和目标视图
    ($I_{t}$) 之间的像素匹配对，他们使用源视图、相机参数和双线性插值 [[19](#bib.bib19)] 合成目标视图 $\hat{I}_{s}$。为了处理遮挡和物体移动，在损失计算过程中应用了一个解释性掩码
    ($M_{s}$)，其公式为
- en: '|  | $\mathcal{L}_{ViewSynthesis}=\sum^{S}_{s=1}&#124;I_{t}-\hat{I}_{s}&#124;\odot
    M_{s}$ |  | (11) |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{ViewSynthesis}=\sum^{S}_{s=1}\|I_{t}-\hat{I}_{s}\|\odot
    M_{s}$ |  | (11) |'
- en: where $s$ is the source views.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $s$ 是源视图。
- en: 'Typically, MVS methods regularize a cost volume and build using multiple source
    views, to estimate the reference view depth map. Li et al. [[49](#bib.bib49)]
    argue that by estimating only the reference view depth map from the cost volume,
    an MVS method underutilizes the information present in the cost volume. They propose
    a reasonable module to synthesize source depths from the cost volume by projecting
    the pixels in reference view at different depth hypotheses to the source views.
    They use robust photometric consistency (Sec. [3.1](#S3.SS1 "3.1 Photometric Consistency
    ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")) to estimate the view synthesis loss.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，MVS 方法对成本体积进行正则化，并使用多个源视图来估计参考视图的深度图。Li 等人 [[49](#bib.bib49)] 认为，仅从成本体积中估计参考视图深度图，MVS
    方法会低效利用成本体积中存在的信息。他们提出了一个合理的模块，通过将参考视图中不同深度假设下的像素投影到源视图中，来合成源深度。他们使用鲁棒的光度一致性（参见
    [3.1](#S3.SS1 "3.1 光度一致性 ‣ 3 视图间约束 ‣ 深度学习框架中的几何约束：综述")）来估计视图合成损失。
- en: 'Dai et al. [[54](#bib.bib54)] synthesize source view depth maps using the reference
    view depth maps in the MVS pipeline for additional geometric consistency. For
    each pair of reference-source views, they calculate the bidirectional error, i.e.
    $\mathcal{L}_{r\rightarrow s}$ and $\mathcal{L}_{s\rightarrow r}$ along with $\mathcal{L}_{smooth_{\triangledown}}$
    and $\mathcal{L}_{smooth_{\triangledown^{2}}}$ (Sec. [4.2](#S4.SS2 "4.2 Edge-Aware
    Smoothness Constraint ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey")) to estimate $\mathcal{L}_{ViewSynthesis}$.
    They use a robust formulation of photometric consistency loss (Sec. [3.1](#S3.SS1
    "3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey")) to estimate $\mathcal{L}_{r\rightarrow
    s}$ and $\mathcal{L}_{s\rightarrow r}$ and add structural similarity (SSIM) error
    term (Sec. [4.1](#S4.SS1 "4.1 Structural Similarity Index Measurement ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey")) in the loss.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: Dai 等人 [[54](#bib.bib54)] 使用 MVS 管道中的参考视图深度图合成源视图深度图，以实现额外的几何一致性。对于每对参考视图-源视图，他们计算双向误差，即
    $\mathcal{L}_{r\rightarrow s}$ 和 $\mathcal{L}_{s\rightarrow r}$，以及 $\mathcal{L}_{smooth_{\triangledown}}$
    和 $\mathcal{L}_{smooth_{\triangledown^{2}}$（第 [4.2](#S4.SS2 "4.2 边缘感知平滑约束 ‣ 4
    几何保持约束 ‣ 深度学习框架中的几何约束：综述")节），以估计 $\mathcal{L}_{ViewSynthesis}$。他们使用了稳健的光度一致性损失公式（第
    [3.1](#S3.SS1 "3.1 光度一致性 ‣ 3 跨视图约束 ‣ 深度学习框架中的几何约束：综述")节）来估计 $\mathcal{L}_{r\rightarrow
    s}$ 和 $\mathcal{L}_{s\rightarrow r}$，并在损失中加入结构相似性（SSIM）误差项（第 [4.1](#S4.SS1 "4.1
    结构相似性指数测量 ‣ 4 几何保持约束 ‣ 深度学习框架中的几何约束：综述")节）。
- en: '|  | $\displaystyle\mathcal{L}_{ViewSynthesis}^{r\rightarrow s}$ | $\displaystyle=(\mathcal{L}_{r\rightarrow
    s}+\mathcal{L}_{s\rightarrow r})+(\mathcal{L}^{r}_{smooth_{\triangledown}}+\mathcal{L}^{r}_{smooth_{\triangledown^{2}}})$
    |  | (12) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{ViewSynthesis}^{r\rightarrow s}$ | $\displaystyle=(\mathcal{L}_{r\rightarrow
    s}+\mathcal{L}_{s\rightarrow r})+(\mathcal{L}^{r}_{smooth_{\triangledown}}+\mathcal{L}^{r}_{smooth_{\triangledown^{2}}})$
    |  | (12) |'
- en: '|  | $\displaystyle\mathcal{L}_{r\rightarrow s}$ | $\displaystyle=\mathcal{L}_{photo_{robust}}+\mathcal{L}^{r}_{SSIM}$
    |  | (13) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{r\rightarrow s}$ | $\displaystyle=\mathcal{L}_{photo_{robust}}+\mathcal{L}^{r}_{SSIM}$
    |  | (13) |'
- en: '|  | $\displaystyle\mathcal{L}_{s\rightarrow r}$ | $\displaystyle=\mathcal{L}_{photo_{robust}}+\mathcal{L}^{s}_{SSIM}$
    |  | (14) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{s\rightarrow r}$ | $\displaystyle=\mathcal{L}_{photo_{robust}}+\mathcal{L}^{s}_{SSIM}$
    |  | (14) |'
- en: 'where $r,s$ are the reference and the source views. We describe the other loss
    terms in Sec. [4](#S4 "4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey").'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r,s$ 是参考视图和源视图。我们在第 [4](#S4 "4 几何保持约束 ‣ 深度学习框架中的几何约束：综述")节中描述了其他损失项。
- en: 4 Geometry Preserving Constraints
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 几何保持约束
- en: Apart from utilizing cross-view consistency constraints, there are different
    ways of enforcing structural consistency in a depth estimation pipeline. In this
    section, we discuss all such approaches that utilize alternative methods of enforcing
    geometric constraints. We have classified these methods into four broader categories,
    i.e. structural similarity index measurement (SSIM), edge-aware smoothness constraints,
    consistency regularization, and planar consistency. We discuss each of these approaches
    in detail and highlight the research works that utilize these methods in their
    pipeline.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用跨视图一致性约束之外，还有不同的方法来强制执行深度估计管道中的结构一致性。在本节中，我们讨论了所有利用替代方法来强制执行几何约束的方法。我们将这些方法分为四个更广泛的类别，即结构相似性指数测量（SSIM）、边缘感知平滑约束、一致性正则化和平面一致性。我们详细讨论了这些方法，并突出了在其管道中利用这些方法的研究工作。
- en: 4.1 Structural Similarity Index Measurement
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 结构相似性指数测量
- en: '![Refer to caption](img/f3f2b9ca416ffa83d02c4ef5bd01a5f7.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f3f2b9ca416ffa83d02c4ef5bd01a5f7.png)'
- en: 'Figure 6: SSIM measurement system as described in [[60](#bib.bib60)]. Figure
    reused from [[60](#bib.bib60)]'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '图 6: 如 [[60](#bib.bib60)] 所描述的 SSIM 测量系统。图表重用自 [[60](#bib.bib60)]'
- en: Objective image quality metrics are roughly classified into three categories
    based on the availability of distortion-free (original or reference) images. The
    metric is called full-reference when the complete reference image is known, it
    is called no-reference when the reference image is not available and it is called
    reduced-reference when the reference image is partially available. Eskicioglu
    and Fisher [[61](#bib.bib61)] discuss several such image quality metrics and their
    performance.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 客观图像质量指标大致分为三类，根据失真图像（原始或参考）的可用性进行分类。当完整的参考图像已知时，度量被称为全参考；当参考图像不可用时，称为无参考；当部分可用时，称为减少参考。Eskicioglu和Fisher[[61](#bib.bib61)]讨论了几种这样的图像质量度量及其性能。
- en: Structural similarity index measurement (SSIM) is a full-reference image quality
    assessment technique [[60](#bib.bib60)]. Its assessment is based on the degradation
    of structural information between the reference and the noisy image. Specifically,
    it compares local patterns of pixel intensities that have been normalized for
    luminance and contrast. Luminance of a surface is the product of its illumination
    and reflectance, but the structure of the object is independent of illumination.
    SSIM separates the influence of illumination to analyze the structural information
    in an image.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 结构相似性指数测量（SSIM）是一种全参考图像质量评估技术[[60](#bib.bib60)]。它的评估是基于参考图像和噪音图像之间结构信息的退化而进行的。具体而言，它比较了已经归一化为亮度和对比度的像素强度的局部模式。表面的亮度是其照明和反射的乘积，但物体的结构独立于照明。SSIM分离了照明的影响以分析图像中的结构信息。
- en: 'Wang et al. [[60](#bib.bib60)] define structural information as the attributes
    that represent the structural information of objects in an image, independent
    of average luminance and contrast. As shown in Fig. [6](#S4.F6 "Figure 6 ‣ 4.1
    Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey"), given two aligned
    non-negative signals (images), X and Y, one of which is assumed to be the reference
    quality, we can have a quantitative measurement of the second signal with SSIM.
    Three separate tasks are considered for structural similarity measurement: luminance,
    contrast, and structure [[60](#bib.bib60)].'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 王等人[[60](#bib.bib60)]将结构信息定义为代表图像中物体结构信息的属性，独立于平均亮度和对比度。如图[6](#S4.F6 "图6 ‣ 4.1
    结构相似性指数测量 ‣ 4 保持几何约束 ‣ 深度学习框架中的几何约束：调查")所示，假设有两个对齐的非负信号（图像），X和Y，其中一个被假定为参考质量，我们可以用SSIM对第二个信号进行定量测量。结构相似性测量考虑了三个分开的任务：亮度、对比度和结构[[60](#bib.bib60)]。
- en: '|  |  | $\displaystyle\mu_{x}=\frac{1}{N}\sum^{N}_{i=1}x_{i};\mu_{y}=\frac{1}{N}\sum^{N}_{i=1}y_{i}$
    |  | (15) |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\mu_{x}=\frac{1}{N}\sum^{N}_{i=1}x_{i};\mu_{y}=\frac{1}{N}\sum^{N}_{i=1}y_{i}$
    |  | (15) |'
- en: '|  |  | $\displaystyle\sigma_{x}=(\frac{1}{N-1}\sum^{N}_{i=1}(x_{i}-\mu_{x})^{2})^{\frac{1}{2}};\sigma_{y}=(\frac{1}{N-1}\sum^{N}_{i=1}(y_{i}-\mu_{y})^{2})^{\frac{1}{2}}$
    |  | (16) |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\sigma_{x}=(\frac{1}{N-1}\sum^{N}_{i=1}(x_{i}-\mu_{x})^{2})^{\frac{1}{2}};\sigma_{y}=(\frac{1}{N-1}\sum^{N}_{i=1}(y_{i}-\mu_{y})^{2})^{\frac{1}{2}}$
    |  | (16) |'
- en: '|  |  | $\displaystyle\sigma_{xy}=\frac{1}{N-1}\sum^{N}_{i=1}(x_{i}-\mu_{x})(y_{i}-\mu_{y})$
    |  | (17) |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle\sigma_{xy}=\frac{1}{N-1}\sum^{N}_{i=1}(x_{i}-\mu_{x})(y_{i}-\mu_{y})$
    |  | (17) |'
- en: 'The SSIM formula is given in Eq. [18](#S4.E18 "In 4.1 Structural Similarity
    Index Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"), which is a scaled product of the three
    components discussed above. The luminance comparison function $l(X,Y)$ is a function
    of mean intensities $\mu_{x}$ and $\mu_{y}$ as shown in Eq. [19](#S4.E19 "In 4.1
    Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey"). The contrast comparison
    $c(X,Y)$ is a function of $\sigma_{x}$ and $\sigma_{y}$ as shown in Eq. [20](#S4.E20
    "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") and the structure
    comparison measure $s(X,Y)$ is a function of correlation coefficient $\sigma_{xy}$,
    $\sigma_{x}$ and $\sigma_{y}$ as shown in Eq. [21](#S4.E21 "In 4.1 Structural
    Similarity Index Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"). $L$ is the dynamic range of the pixel
    values (255 for grayscale image). The mean, standard deviation, and correlation
    coefficient of the signals are calculated using Eqs. [15](#S4.E15 "In 4.1 Structural
    Similarity Index Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"), [16](#S4.E16 "In 4.1 Structural Similarity
    Index Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"), and [17](#S4.E17 "In 4.1 Structural Similarity
    Index Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"). Where $N$ is the total number of pixels
    in an image.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: SSIM公式在公式 [18](#S4.E18 "在4.1结构相似性指数测量 ‣ 4几何保持约束 ‣ 深度学习框架中的几何约束：综述") 中给出，它是上述三个组件的缩放乘积。亮度比较函数
    $l(X,Y)$ 是均值强度 $\mu_{x}$ 和 $\mu_{y}$ 的函数，如公式 [19](#S4.E19 "在4.1结构相似性指数测量 ‣ 4几何保持约束
    ‣ 深度学习框架中的几何约束：综述") 所示。对比度比较 $c(X,Y)$ 是 $\sigma_{x}$ 和 $\sigma_{y}$ 的函数，如公式 [20](#S4.E20
    "在4.1结构相似性指数测量 ‣ 4几何保持约束 ‣ 深度学习框架中的几何约束：综述") 所示，结构比较度量 $s(X,Y)$ 是相关系数 $\sigma_{xy}$、$\sigma_{x}$
    和 $\sigma_{y}$ 的函数，如公式 [21](#S4.E21 "在4.1结构相似性指数测量 ‣ 4几何保持约束 ‣ 深度学习框架中的几何约束：综述")
    所示。$L$ 是像素值的动态范围（灰度图像为255）。信号的均值、标准差和相关系数是通过公式 [15](#S4.E15 "在4.1结构相似性指数测量 ‣ 4几何保持约束
    ‣ 深度学习框架中的几何约束：综述")、 [16](#S4.E16 "在4.1结构相似性指数测量 ‣ 4几何保持约束 ‣ 深度学习框架中的几何约束：综述")
    和 [17](#S4.E17 "在4.1结构相似性指数测量 ‣ 4几何保持约束 ‣ 深度学习框架中的几何约束：综述") 计算的。$N$ 是图像中像素的总数。
- en: '|  | $\displaystyle SSIM(X,Y)$ | $\displaystyle=[l(X,Y)]^{\alpha}.[c(X,Y)]^{\beta}.[s(X,Y)]^{\gamma};\alpha>0,\beta>0,\gamma>0$
    |  | (18) |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle SSIM(X,Y)$ | $\displaystyle=[l(X,Y)]^{\alpha}.[c(X,Y)]^{\beta}.[s(X,Y)]^{\gamma};\alpha>0,\beta>0,\gamma>0$
    |  | (18) |'
- en: '|  | $\displaystyle l(X,Y)$ | $\displaystyle=\frac{2\mu_{x}\mu_{y}+c_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+c_{1}};c_{1}=(K_{1}L)^{2},K_{1}\ll
    1$ |  | (19) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle l(X,Y)$ | $\displaystyle=\frac{2\mu_{x}\mu_{y}+c_{1}}{\mu_{x}^{2}+\mu_{y}^{2}+c_{1}};c_{1}=(K_{1}L)^{2},K_{1}\ll
    1$ |  | (19) |'
- en: '|  | $\displaystyle c(X,Y)$ | $\displaystyle=\frac{2\sigma_{x}\sigma_{y}+c_{2}}{\sigma^{2}_{x}+\sigma^{2}_{y}+c_{2}};c_{2}=(K_{2}L)^{2},K_{2}\ll
    1$ |  | (20) |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle c(X,Y)$ | $\displaystyle=\frac{2\sigma_{x}\sigma_{y}+c_{2}}{\sigma^{2}_{x}+\sigma^{2}_{y}+c_{2}};c_{2}=(K_{2}L)^{2},K_{2}\ll
    1$ |  | (20) |'
- en: '|  | $\displaystyle s(X,Y)$ | $\displaystyle=\frac{\sigma_{xy}+c_{3}}{\sigma_{x}\sigma_{y}+c_{3}},c_{3}=\frac{c_{2}}{2}$
    |  | (21) |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle s(X,Y)$ | $\displaystyle=\frac{\sigma_{xy}+c_{3}}{\sigma_{x}\sigma_{y}+c_{3}},c_{3}=\frac{c_{2}}{2}$
    |  | (21) |'
- en: SSIM index should be applied locally rather than globally for the following
    reasons. First, image statistical features are highly spatially non-stationary.
    Second, image distortions may be space-variant, and third, local quality measurement
    delivers more information about the quality degradation by providing a spatially
    varying quality map of the image.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: SSIM指数应该在局部而非全局应用，原因如下。首先，图像统计特征在空间上高度非平稳。其次，图像失真可能是空间变异的。第三，局部质量测量通过提供图像的空间变化质量图，能更好地反映质量退化的信息。
- en: Wang et al. [[60](#bib.bib60)] defined $SSIM$ index for two signals in the same
    domain, i.e. it can estimate the structural similarity between two RGB or grayscale
    images or between two depth maps or two patches. In an end-to-end framework, where
    we minimized the loss, we want to maximize $SSIM$ for better result. Since $SSIM$
    is upper bound to $1$, we can instead minimize $1-SSIM$. This formulation takes
    a general form of
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 [[60](#bib.bib60)] 为相同领域中的两个信号定义了 $SSIM$ 指数，即它可以估计两个 RGB 或灰度图像之间，或两个深度图之间，或两个补丁之间的结构相似性。在一个端到端框架中，我们在最小化损失时，希望最大化
    $SSIM$ 以获得更好的结果。由于 $SSIM$ 的上界为 $1$，我们可以改为最小化 $1-SSIM$。该公式的一般形式为
- en: '|  | $\mathcal{L}_{SSIM}=\frac{1}{N}\sum^{N}_{i=1}\frac{1-SSIM(X,Y)}{K}\odot
    M$ |  | (22) |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{SSIM}=\frac{1}{N}\sum^{N}_{i=1}\frac{1-SSIM(X,Y)}{K}\odot
    M$ |  | (22) |'
- en: in an end-to-end depth estimation framework. Where $N$ is the number of source
    views, $X$ and $Y$ are the two signals to be compared, preferably in the same
    domain. $M$ is the mask to handle occlusion and $K$ is a constant.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个端到端的深度估计框架中，其中 $N$ 是源视图的数量，$X$ 和 $Y$ 是两个需要比较的信号，最好在相同的领域中。$M$ 是处理遮挡的掩码，而
    $K$ 是一个常数。
- en: 'Zhao et al. [[62](#bib.bib62)] use this formulation with no mask and $K=1$
    for image restoration problems. It calculates the means and standard deviations
    (Eqs. [15](#S4.E15 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey"), [16](#S4.E16 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey") and [17](#S4.E17 "In 4.1 Structural Similarity Index Measurement ‣ 4
    Geometry Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")) using Gaussian filter with standard deviation $\sigma_{G}$. The choice
    of $\sigma_{G}$ impacts the quality of the processed results. With smaller values
    of $\sigma_{G}$ the network loses the ability to preserve local structures, introducing
    artifacts in the image, and for larger $\sigma_{G}$, the network preserves noises
    around the edges. Instead of finetuning the value of $\sigma_{G}$, Zhao et al.
    [[62](#bib.bib62)] propose a multi-scale formulation of SSIM (MS-SSIM), where
    all results from the variations of $\sigma_{G}$ are multiplied together.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 'Zhao 等人 [[62](#bib.bib62)] 在图像恢复问题中使用这种公式，不使用掩码且 $K=1$。它使用标准差为 $\sigma_{G}$
    的高斯滤波器计算均值和标准差（Eqs. [15](#S4.E15 "In 4.1 Structural Similarity Index Measurement
    ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")、[16](#S4.E16 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey") 和 [17](#S4.E17 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey")）。$\sigma_{G}$ 的选择会影响处理结果的质量。$\sigma_{G}$ 较小值时，网络丧失保留局部结构的能力，图像中会出现伪影；而
    $\sigma_{G}$ 较大时，网络会保留边缘周围的噪声。Zhao 等人 [[62](#bib.bib62)] 提出了 SSIM 的多尺度公式（MS-SSIM），其中所有
    $\sigma_{G}$ 变化的结果相乘在一起。'
- en: Inherently, both MS-SSIM and SSIM are not particularly sensitive to the change
    of brightness or shift of colors. However, they preserve the contrast in high-frequency
    regions. $L_{1}$ loss, on the other hand, preserves colors and luminance - an
    error weighted equally regardless of the local structure - but does not produce
    the same impact for contrast. For best impact, Zhao et al. [[62](#bib.bib62)]
    combines both these terms, $\mathcal{L}^{mix}_{SSIM}$, as follows
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，MS-SSIM 和 SSIM 对亮度变化或颜色偏移不是特别敏感。然而，它们保留了高频区域的对比度。另一方面，$L_{1}$ 损失保留了颜色和亮度——无论局部结构如何，误差的加权是相同的——但对比度的影响不同。为了最佳效果，Zhao
    等人 [[62](#bib.bib62)] 将这两个项 $\mathcal{L}^{mix}_{SSIM}$ 结合如下
- en: '|  | $\mathcal{L}^{mix}_{SSIM}=\alpha.\mathcal{L}_{SSIM}+(1-\alpha).\mathcal{L}^{l_{i}}$
    |  | (23) |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}^{mix}_{SSIM}=\alpha.\mathcal{L}_{SSIM}+(1-\alpha).\mathcal{L}^{l_{i}}$
    |  | (23) |'
- en: where, $\alpha$ is a tunable hyper-parameter and $l_{i}$ is $L_{1}$ or $L_{2}$
    norm.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\alpha$ 是一个可调的超参数，而 $l_{i}$ 是 $L_{1}$ 或 $L_{2}$ 范数。
- en: 'Mallick et al. [[44](#bib.bib44)] use simplest form of $\mathcal{L}_{SSIM}$
    (Eq. [22](#S4.E22 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey")) in a self-supervised MVS framework with no mask $M$, $X=I_{ref}$, $Y=\hat{I}_{src}$
    and $K=1$. Huang et al. [[22](#bib.bib22)] use $\mathcal{L}_{SSIM}$ (Eq. [22](#S4.E22
    "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")) with $X=I_{ref}$,
    $Y=\hat{I}_{src}$ and $K=2$ in unsupervised MVS framework. Mahjourian et al. [[25](#bib.bib25)]
    also use it in unsupervised MVS framework with $K=1$ (Eq. [22](#S4.E22 "In 4.1
    Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey")), $c_{1}={0.01}^{2}$
    and $c_{2}={0.03}^{2}$ (Eqs. [19](#S4.E19 "In 4.1 Structural Similarity Index
    Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey") and [20](#S4.E20 "In 4.1 Structural Similarity
    Index Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey")). Khot et al. [[48](#bib.bib48)] use the
    same formulation as [[25](#bib.bib25)] in an unsupervised MVS framework. Li et
    al. [[49](#bib.bib49)] use bidirectional calculation, forward ($\mathcal{L}^{forward}_{SSIM}$)
    with $X=I_{ref}$, $Y=\hat{I}_{src}$ (Source view projected to reference view)
    and backward ($\mathcal{L}^{backward}_{SSIM}$) with $X=\hat{I}_{ref}$, $Y=I_{src}$
    (reference view projected to source view), with no mask $M$ and $K=1$, Eq. [22](#S4.E22
    "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"). The final $\mathcal{L}_{SSIM}=\mathcal{L}^{forward}_{SSIM}+\mathcal{L}^{backward}_{SSIM}$'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: Mallick 等人 [[44](#bib.bib44)] 在一个自监督 MVS 框架中使用了最简单形式的 $\mathcal{L}_{SSIM}$（公式
    [22](#S4.E22 "在 4.1 结构相似性指数测量 ‣ 4 几何保持约束 ‣ 深度学习框架中的几何约束：综述")），其中没有掩模 $M$，$X=I_{ref}$，$Y=\hat{I}_{src}$
    和 $K=1$。Huang 等人 [[22](#bib.bib22)] 在无监督 MVS 框架中使用了 $\mathcal{L}_{SSIM}$（公式 [22](#S4.E22
    "在 4.1 结构相似性指数测量 ‣ 4 几何保持约束 ‣ 深度学习框架中的几何约束：综述")），其中 $X=I_{ref}$，$Y=\hat{I}_{src}$
    和 $K=2$。Mahjourian 等人 [[25](#bib.bib25)] 也在无监督 MVS 框架中使用了它，$K=1$（公式 [22](#S4.E22
    "在 4.1 结构相似性指数测量 ‣ 4 几何保持约束 ‣ 深度学习框架中的几何约束：综述")），$c_{1}={0.01}^{2}$ 和 $c_{2}={0.03}^{2}$（公式
    [19](#S4.E19 "在 4.1 结构相似性指数测量 ‣ 4 几何保持约束 ‣ 深度学习框架中的几何约束：综述") 和 [20](#S4.E20 "在
    4.1 结构相似性指数测量 ‣ 4 几何保持约束 ‣ 深度学习框架中的几何约束：综述")）。Khot 等人 [[48](#bib.bib48)] 在一个无监督
    MVS 框架中使用了与 [[25](#bib.bib25)] 相同的公式。Li 等人 [[49](#bib.bib49)] 使用了双向计算，前向 ($\mathcal{L}^{forward}_{SSIM}$)
    与 $X=I_{ref}$，$Y=\hat{I}_{src}$（源视图投影到参考视图），以及后向 ($\mathcal{L}^{backward}_{SSIM}$)
    与 $X=\hat{I}_{ref}$，$Y=I_{src}$（参考视图投影到源视图），没有掩模 $M$ 和 $K=1$，公式 [22](#S4.E22 "在
    4.1 结构相似性指数测量 ‣ 4 几何保持约束 ‣ 深度学习框架中的几何约束：综述")。最终的 $\mathcal{L}_{SSIM}=\mathcal{L}^{forward}_{SSIM}+\mathcal{L}^{backward}_{SSIM}$
- en: 'As explained earlier, the $\mathcal{L}_{SSIM}$ is usually combined with a uniformly
    weighted loss function like $L_{1}$ and $L_{2}$, to choose the best of both the
    individual loss terms. The combined formulation is shown in Eq. [23](#S4.E23 "In
    4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"). Dai et al. [[54](#bib.bib54)]
    and Yu et al. [[52](#bib.bib52)] use Eq. [23](#S4.E23 "In 4.1 Structural Similarity
    Index Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") formulation with $\alpha=0.85$ in MVS
    framework. This formulation finds widespread use in monocular depth estimation
    problems. Monocular methods like, [[63](#bib.bib63), [64](#bib.bib64)] uses Eq.
    [23](#S4.E23 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") with
    $\alpha=0.85$ with target and novel view synthesized depth maps as $X$ and $Y$.
    Godard et al. [[26](#bib.bib26)] use $\mathcal{L}^{mix}_{SSIM}$ (Eq. [23](#S4.E23
    "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")) between input
    RGB image and reconstructed new view RGB image with $\alpha=0.85$ to enforce structural
    similarity. Zhao et al. [[65](#bib.bib65)] with symmetric domain adaptation, real-to-synthetic
    and synthetic-to-real, apply Eq. [23](#S4.E23 "In 4.1 Structural Similarity Index
    Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey") both ways to enforce structural similarity during
    the transition from one domain to the other domain.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，$\mathcal{L}_{SSIM}$ 通常与均匀加权的损失函数如 $L_{1}$ 和 $L_{2}$ 结合，以选择单个损失项中的最佳。组合公式如公式
    [23](#S4.E23 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") 所示。Dai
    等人 [[54](#bib.bib54)] 和 Yu 等人 [[52](#bib.bib52)] 在 MVS 框架中使用了 $\alpha=0.85$ 的公式
    [23](#S4.E23 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")。该公式在单目深度估计问题中得到了广泛应用。像
    [[63](#bib.bib63), [64](#bib.bib64)] 这样的单目方法使用公式 [23](#S4.E23 "In 4.1 Structural
    Similarity Index Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey")，其中 $\alpha=0.85$，目标和新颖视图合成的深度图作为 $X$ 和
    $Y$。Godard 等人 [[26](#bib.bib26)] 使用 $\mathcal{L}^{mix}_{SSIM}$ (公式 [23](#S4.E23
    "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")) 在输入 RGB 图像和重建的新视图
    RGB 图像之间，以 $\alpha=0.85$ 强化结构相似性。Zhao 等人 [[65](#bib.bib65)] 在对称领域适应、从真实到合成和从合成到真实的过程中，应用公式
    [23](#S4.E23 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") 来强化结构相似性，以应对从一个领域到另一个领域的过渡。'
- en: 'Chen et al. [[23](#bib.bib23)] use $\mathcal{L}^{mix}_{SSIM}$, Eq. [23](#S4.E23
    "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"), with slight modification
    to make it adaptive loss in optical flow estimation problem. For scene structures
    that can not be explained by the global rigid motion, it adapts to a more flexible
    optical flow estimation by updating the channel parameters only for the configurations
    that closely explain the displacement. It is represented as the minimum error
    between the optical flow and the rigid motion displacements, it is formulated
    as'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 'Chen 等人 [[23](#bib.bib23)] 使用了 $\mathcal{L}^{mix}_{SSIM}$，公式 [23](#S4.E23 "In
    4.1 Structural Similarity Index Measurement ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")，稍作修改以使其在光流估计问题中适应性强。对于无法用全局刚性运动解释的场景结构，通过仅更新紧密解释位移的配置的通道参数来适应更灵活的光流估计。它表现为光流与刚性运动位移之间的最小误差，其公式为'
- en: '|  | $L^{adaptive}_{SSIM}=min\{\mathcal{L}^{mix}_{SSIM_{flow}},\mathcal{L}^{mix}_{SSIM_{rigid}}\}$
    |  | (24) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '|  | $L^{adaptive}_{SSIM}=min\{\mathcal{L}^{mix}_{SSIM_{flow}},\mathcal{L}^{mix}_{SSIM_{rigid}}\}$
    |  | (24) |'
- en: 'where it uses $\alpha=0.85$ in Eq. [23](#S4.E23 "In 4.1 Structural Similarity
    Index Measurement ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"). We only discuss the $\mathcal{L}^{mix}_{SSIM}$
    formulation here and refer to [[23](#bib.bib23)] for other optical flow related
    details.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '在公式 [23](#S4.E23 "In 4.1 Structural Similarity Index Measurement ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey") 中，使用了 $\alpha=0.85$。我们在这里只讨论 $\mathcal{L}^{mix}_{SSIM}$ 的公式，其他光流相关细节请参考
    [[23](#bib.bib23)]。'
- en: 4.2 Edge-Aware Smoothness Constraint
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 边缘感知平滑约束
- en: The smoothness constraint finds its origin in the optical flow estimation problem.
    It was first applied by Uras et al. [[66](#bib.bib66)] to estimate consistent
    optical flow from two images. Brox et al. [[67](#bib.bib67)] further explained
    the concept under three assumptions for the optical flow framework, i.e. the gray
    value constancy assumption, the gradient constancy assumption, and the smoothness
    assumption.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 平滑性约束起源于光流估计问题。最初由Uras等人应用于从两幅图像中估计一致的光流[[66](#bib.bib66)]。Brox等人[[67](#bib.bib67)]在光流框架下进一步解释了这一概念，提出了三种假设，即灰度值恒定假设、梯度恒定假设和光滑性假设。
- en: '|  | $\displaystyle I(x,y,t)$ | $\displaystyle=I(x+u,y+v,t+1)$ |  | (25) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle I(x,y,t)$ | $\displaystyle=I(x+u,y+v,t+1)$ |  | (25) |'
- en: '|  | $\displaystyle\triangledown I(x,y,t)$ | $\displaystyle=\triangledown I(x+u,y+v,t+1)$
    |  | (26) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\triangledown I(x,y,t)$ | $\displaystyle=\triangledown I(x+u,y+v,t+1)$
    |  | (26) |'
- en: '|  | $\displaystyle E_{smooth}(u,v)$ | $\displaystyle=\int\psi(&#124;\triangledown
    u&#124;^{2}+&#124;\triangledown v&#124;^{2})dx$ |  | (27) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle E_{smooth}(u,v)$ | $\displaystyle=\int\psi(&#124;\triangledown
    u&#124;^{2}+&#124;\triangledown v&#124;^{2})dx$ |  | (27) |'
- en: 'Since the beginning of the optical flow estimation problem, it has been assumed
    that the gray value of a pixel does not change on displacement under a given lighting
    condition, Eq. [25](#S4.E25 "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey") [[67](#bib.bib67)]. But brightness changes in a natural scene all the
    time. Therefore, it is considered useful to allow small variations in gray values
    but finds a different criterion that remains relatively invariant under gray value
    changes, i.e. the gradient constancy was assumed under displacement, Eq. [26](#S4.E26
    "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey"). This brought about
    the third assumption, the smoothness of the flow field. While discontinuities
    are assumed to be present at the boundaries of the object in the scene, a piecewise
    smoothness can be assumed in the flow field [[67](#bib.bib67)]. To achieve this
    smoothness in flow estimation a penalty on the flow field was applied as shown
    in Eq. [27](#S4.E27 "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey").'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '自光流估计问题开始以来，假设在给定的光照条件下，像素的灰度值在位移时不会变化，见方程[25](#S4.E25 "In 4.2 Edge-Aware Smoothness
    Constraint ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey")[[67](#bib.bib67)]。但在自然场景中亮度总是变化。因此，允许灰度值的小幅变化被认为是有用的，但找到一个在灰度值变化下相对不变的标准，即在位移下假设梯度恒定，见方程[26](#S4.E26
    "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey")。这引入了第三个假设，即流场的平滑性。尽管假设在场景中物体的边界处存在不连续性，但可以在流场中假设分段光滑性[[67](#bib.bib67)]。为了在流估计中实现这种光滑性，对流场施加了惩罚，如方程[27](#S4.E27
    "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey")所示。'
- en: 'In the optical flow framework, objects are assumed to be moving with a fixed
    camera, in an MVS framework, the objects are assumed to be fixed and the camera
    moves concerning a fixed point. The relative motion of an object can be viewed
    as a moving camera to pose it as an MVS problem, see Fig. [4](#S3.F4 "Figure 4
    ‣ 3.3 Cross-View Depth-Flow Consistency ‣ 3 Cross-View Constraints ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey"). With this assumption, the
    smoothness constraint can be applied to the depth estimation problem. Initially,
    only the first-order smoothness constraints were used in the depth estimation
    framework [[27](#bib.bib27), [19](#bib.bib19), [26](#bib.bib26), [64](#bib.bib64),
    [25](#bib.bib25), [52](#bib.bib52)]. After Yang et al. [[20](#bib.bib20)] used
    second-order smoothness constraint for regularization was combined with the first-order
    smoothness constraint in subsequent works [[54](#bib.bib54), [22](#bib.bib22),
    [50](#bib.bib50), [68](#bib.bib68)].'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在光流框架中，假设物体在固定相机下运动，而在 MVS 框架中，假设物体是固定的，且相机相对于固定点移动。物体的相对运动可以被视为一个移动的相机，将其看作
    MVS 问题，见图 [4](#S3.F4 "图 4 ‣ 3.3 跨视图深度-流动一致性 ‣ 3 跨视图约束 ‣ 深度学习框架中的几何约束：一项调查")。在这个假设下，平滑性约束可以应用于深度估计问题。最初，深度估计框架中仅使用了第一阶平滑性约束
    [[27](#bib.bib27), [19](#bib.bib19), [26](#bib.bib26), [64](#bib.bib64), [25](#bib.bib25),
    [52](#bib.bib52)]。在 Yang 等人 [[20](#bib.bib20)] 使用第二阶平滑性约束进行正则化后，第二阶平滑性约束与第一阶平滑性约束结合在后续工作中使用
    [[54](#bib.bib54), [22](#bib.bib22), [50](#bib.bib50), [68](#bib.bib68)]。
- en: '|  | $\displaystyle\mathcal{L}_{smooth_{\triangledown}}$ | $\displaystyle=\sum&#124;&#124;\partial_{x}\hat{D}&#124;&#124;.e^{-&#124;&#124;\partial_{x}I&#124;&#124;}+&#124;&#124;\partial_{y}\hat{D}&#124;&#124;.e^{-&#124;&#124;\partial_{y}I&#124;&#124;}$
    |  | (28) |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{smooth_{\triangledown}}$ | $\displaystyle=\sum&#124;&#124;\partial_{x}\hat{D}&#124;&#124;.e^{-&#124;&#124;\partial_{x}I&#124;&#124;}+&#124;&#124;\partial_{y}\hat{D}&#124;&#124;.e^{-&#124;&#124;\partial_{y}I&#124;&#124;}$
    |  | (28) |'
- en: '|  | $\displaystyle\mathcal{L}_{smooth_{\triangledown^{2}}}$ | $\displaystyle=\sum&#124;&#124;\partial^{2}_{x}\hat{D}&#124;&#124;.e^{-&#124;&#124;\partial^{2}_{x}I&#124;&#124;}+&#124;&#124;\partial^{2}_{y}\hat{D}&#124;&#124;.e^{-&#124;&#124;\partial^{2}_{y}I&#124;&#124;}$
    |  | (29) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{smooth_{\triangledown^{2}}}$ | $\displaystyle=\sum&#124;&#124;\partial^{2}_{x}\hat{D}&#124;&#124;.e^{-&#124;&#124;\partial^{2}_{x}I&#124;&#124;}+&#124;&#124;\partial^{2}_{y}\hat{D}&#124;&#124;.e^{-&#124;&#124;\partial^{2}_{y}I&#124;&#124;}$
    |  | (29) |'
- en: '|  | $\displaystyle\mathcal{L}_{smooth}$ | $\displaystyle=\alpha.\mathcal{L}_{smooth_{\triangledown}}+\beta.\mathcal{L}_{smooth_{\triangledown^{2}}}$
    |  | (30) |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{smooth}$ | $\displaystyle=\alpha.\mathcal{L}_{smooth_{\triangledown}}+\beta.\mathcal{L}_{smooth_{\triangledown^{2}}}$
    |  | (30) |'
- en: 'In the depth estimation framework, smoothness constraint is applied between
    the gradient of input images $(I)$ and the estimated depth maps $(\hat{D})$. Eqs.
    [28](#S4.E28 "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") and
    [29](#S4.E29 "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") represents
    the first $(\mathcal{L}_{smooth_{\triangledown}})$ and second $(\mathcal{L}_{smooth_{\triangledown^{2}}})$
    order smoothness constraints. Eq. [30](#S4.E30 "In 4.2 Edge-Aware Smoothness Constraint
    ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") shows the combined formulation with $\alpha>0$ and $\beta>0$ as a scaling
    factor.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度估计框架中，平滑性约束应用于输入图像 $(I)$ 的梯度和估计的深度图 $(\hat{D})$ 之间。方程 [28](#S4.E28 "在 4.2
    边缘感知平滑约束 ‣ 4 几何保持约束 ‣ 深度学习框架中的几何约束：一项调查") 和 [29](#S4.E29 "在 4.2 边缘感知平滑约束 ‣ 4 几何保持约束
    ‣ 深度学习框架中的几何约束：一项调查") 代表了第一阶 $(\mathcal{L}_{smooth_{\triangledown}})$ 和第二阶 $(\mathcal{L}_{smooth_{\triangledown^{2}}})$
    平滑性约束。方程 [30](#S4.E30 "在 4.2 边缘感知平滑约束 ‣ 4 几何保持约束 ‣ 深度学习框架中的几何约束：一项调查") 显示了结合 $\alpha>0$
    和 $\beta>0$ 作为缩放因子的公式。
- en: 'Garg et al. [[19](#bib.bib19)] use the first order formulation with $L_{2}$
    norm as a regularization term to achieve smoothness in estimation. Mahjourian
    et al. [[25](#bib.bib25)] use the first order formulation for monocular video
    depth estimation problem. First-order smoothness constraint is actively used in
    self-supervised/unsupervised MVS frameworks [[49](#bib.bib49), [52](#bib.bib52),
    [47](#bib.bib47), [48](#bib.bib48), [44](#bib.bib44)]. Zhao et al. [[65](#bib.bib65)]
    with its symmetric domain adaptation for monocular depth estimation uses first-order
    constraint in both domains. Other monocular depth estimation methods [[26](#bib.bib26),
    [64](#bib.bib64), [69](#bib.bib69), [63](#bib.bib63)] also apply the first-order
    smoothness constraint as defined in Eq. [28](#S4.E28 "In 4.2 Edge-Aware Smoothness
    Constraint ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey").'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'Garg等人[[19](#bib.bib19)]使用**一阶**形式和$L_{2}$范数作为正则化项以实现平滑性。Mahjourian等人[[25](#bib.bib25)]在单目视频深度估计问题中使用**一阶**形式。**一阶**平滑约束在自监督/无监督MVS框架中被广泛使用[[49](#bib.bib49),
    [52](#bib.bib52), [47](#bib.bib47), [48](#bib.bib48), [44](#bib.bib44)]。Zhao等人[[65](#bib.bib65)]在其用于单目深度估计的对称域适应中在两个领域中都使用**一阶**约束。其他单目深度估计方法[[26](#bib.bib26),
    [64](#bib.bib64), [69](#bib.bib69), [63](#bib.bib63)]也应用了**一阶**平滑约束，如Eq. [28](#S4.E28
    "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey")所定义。'
- en: 'Yang et al. [[20](#bib.bib20)] only uses second order formulation, Eq. [29](#S4.E29
    "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey") as a regularization
    term in monocular video-depth estimation framework. Learning from it, more recent
    MVS frameworks combine both the first-order and the second-order formulation [[22](#bib.bib22),
    [54](#bib.bib54), [50](#bib.bib50)] as shown in Eq. [30](#S4.E30 "In 4.2 Edge-Aware
    Smoothness Constraint ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"). Inspired from [[26](#bib.bib26)] and
    [[69](#bib.bib69)], Yang et al. [[68](#bib.bib68)] apply the combined edge-aware
    smoothness constraint, Eq. [30](#S4.E30 "In 4.2 Edge-Aware Smoothness Constraint
    ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey"), in monocular endoscopy problem. All these methods use $\alpha=\beta=1$
    in Eq. [30](#S4.E30 "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey").'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 'Yang等人[[20](#bib.bib20)]仅使用二阶形式，参见Eq. [29](#S4.E29 "In 4.2 Edge-Aware Smoothness
    Constraint ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey")，作为单目视频深度估计框架中的正则化项。由此学习到的经验，近年来的MVS框架结合了**一阶**和**二阶**形式[[22](#bib.bib22),
    [54](#bib.bib54), [50](#bib.bib50)]，如Eq. [30](#S4.E30 "In 4.2 Edge-Aware Smoothness
    Constraint ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey")所示。受到[[26](#bib.bib26)]和[[69](#bib.bib69)]的启发，Yang等人[[68](#bib.bib68)]在单目内窥镜问题中应用了组合的边缘感知平滑约束，参见Eq.
    [30](#S4.E30 "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")。所有这些方法在Eq.
    [30](#S4.E30 "In 4.2 Edge-Aware Smoothness Constraint ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")中使用了$\alpha=\beta=1$。'
- en: 4.3 Consistency Regularization
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 一致性正则化
- en: 'Deep learning-based frameworks inherently suffer from over-parameterization
    problems. One of the most efficient methods to counter it is to regularize the
    loss function. Photometric consistency, Sec. [3.1](#S3.SS1 "3.1 Photometric Consistency
    ‣ 3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey"), which enforces geometrical consistency at the pixel level is highly
    susceptible to change in lighting conditions. Many MVS methods employ different
    consistency regularization techniques to effectively handle this problem [[20](#bib.bib20),
    [19](#bib.bib19), [51](#bib.bib51)].'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '基于深度学习的框架本质上存在过度参数化问题。应对这一问题的最有效方法之一是对损失函数进行正则化。光度一致性，参见[3.1](#S3.SS1 "3.1
    Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey")，在像素级强制几何一致性，但对光照条件的变化高度敏感。许多MVS方法采用不同的一致性正则化技术来有效处理这一问题[[20](#bib.bib20),
    [19](#bib.bib19), [51](#bib.bib51)]。'
- en: 'As discussed in Sec [4.2](#S4.SS2 "4.2 Edge-Aware Smoothness Constraint ‣ 4
    Geometry Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey"), first-order and second-order gradients are often used for this task
    [[20](#bib.bib20), [19](#bib.bib19)]. Garg et al. [[19](#bib.bib19)] argue that
    the photometric loss is non-informative in homogeneous regions of a scene, which
    leads to multiple warps generating similar disparity outcomes. It uses $L_{2}$
    regularization (Eq. [31](#S4.E31 "In 4.3 Consistency Regularization ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey")) on the disparity discontinuities as a prior. It also recommends the
    use of other robust penalty functions used in [[67](#bib.bib67), [70](#bib.bib70)]
    as an alternative regularization term. Yang et al. [[20](#bib.bib20)] use a spatial
    smoothness penalty with $L_{1}$ norm of second-order gradient of depth, Eq. [32](#S4.E32
    "In 4.3 Consistency Regularization ‣ 4 Geometry Preserving Constraints ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey"). It encourages depth values
    to align in the planar surface when no image gradient appears.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[4.2节](#S4.SS2 "4.2 边缘感知平滑约束 ‣ 4 保几何约束 ‣ 深度学习框架中的几何约束：综述")中讨论的，通常使用一阶和二阶梯度来完成这项任务[[20](#bib.bib20),
    [19](#bib.bib19)]。Garg等人[[19](#bib.bib19)]认为，在场景的同质区域中，光度损失信息不足，这导致多个变形生成类似的视差结果。它对视差不连续性使用$L_{2}$正则化（公式[31](#S4.E31
    "在4.3 一致性正则化 ‣ 4 保几何约束 ‣ 深度学习框架中的几何约束：综述")）作为先验。它还建议使用[[67](#bib.bib67), [70](#bib.bib70)]中使用的其他鲁棒惩罚函数作为替代正则化项。Yang等人[[20](#bib.bib20)]使用具有$L_{1}$范数的深度二阶梯度的空间平滑惩罚，公式[32](#S4.E32
    "在4.3 一致性正则化 ‣ 4 保几何约束 ‣ 深度学习框架中的几何约束：综述")。当图像梯度不存在时，它鼓励深度值在平面表面上对齐。
- en: 'Xu et al. [[51](#bib.bib51)] apply consistency regularization in the semi-supervised
    MVS method. The proposed regularization minimizes the Kullback-Leibler (KL) divergence
    between the predicted distributions of augmented $(\hat{PV})$ and non-augmented
    $(PV)$ samples. With the $K$ depth hypothesis, the probability volume $PV$, of
    size $H\times W\times K$, is separated into $K$ categories of $HW$ logits. Eq.
    [33](#S4.E33 "In 4.3 Consistency Regularization ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") shows the formulation
    of the regularizer, where $p_{i}$ represents a pixel coordinate.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Xu等人[[51](#bib.bib51)]在半监督MVS方法中应用了一致性正则化。所提出的正则化最小化了增强$(\hat{PV})$和未增强$(PV)$样本预测分布之间的Kullback-Leibler（KL）散度。在$K$深度假设下，大小为$H\times
    W\times K$的概率体积$PV$被分成$K$类$HW$对数值。公式[33](#S4.E33 "在4.3 一致性正则化 ‣ 4 保几何约束 ‣ 深度学习框架中的几何约束：综述")展示了正则化器的公式，其中$p_{i}$表示一个像素坐标。
- en: '|  | $\displaystyle\mathcal{L}_{\triangledown}$ | $\displaystyle=&#124;&#124;\triangledown\hat{D}&#124;&#124;^{2}$
    |  | (31) |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\triangledown}$ | $\displaystyle=&#124;&#124;\triangledown\hat{D}&#124;&#124;^{2}$
    |  | (31) |'
- en: '|  | $\displaystyle\mathcal{L}_{\triangledown^{2}}$ | $\displaystyle=\sum_{d\in
    x,y}&#124;\triangledown^{2}_{d}\hat{D}&#124;.e^{-\alpha&#124;\triangledown I&#124;};\alpha>0$
    |  | (32) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{\triangledown^{2}}$ | $\displaystyle=\sum_{d\in
    x,y}&#124;\triangledown^{2}_{d}\hat{D}&#124;.e^{-\alpha&#124;\triangledown I&#124;};\alpha>0$
    |  | (32) |'
- en: '|  | $\displaystyle\mathcal{L}_{KL}$ | $\displaystyle=\frac{1}{HW}\sum^{HW}_{i=1}KL(PV_{p_{i}},\hat{PV}_{p_{i}})$
    |  | (33) |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{KL}$ | $\displaystyle=\frac{1}{HW}\sum^{HW}_{i=1}KL(PV_{p_{i}},\hat{PV}_{p_{i}})$
    |  | (33) |'
- en: 4.4 Structural Consistency in 3D Space
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 三维空间中的结构一致性
- en: '![Refer to caption](img/f9041cfae273698779d2d6bc3527aae3.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/f9041cfae273698779d2d6bc3527aae3.png)'
- en: 'Figure 7: Left: Example of an input image, their corresponding key points,
    superpixels and piece-wise planar regions obtained from large superpixels (Fig.
    reused from [[52](#bib.bib52)]). Right: Point cloud matching process and approximate
    gradients for the top view of a car (Fig. reused from [[25](#bib.bib25)]).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：左侧：输入图像示例，其对应的关键点、超像素和从大超像素中获得的逐片平面区域（图自[[52](#bib.bib52)]）。右侧：点云匹配过程及汽车俯视图的近似梯度（图自[[25](#bib.bib25)]）。
- en: Structural consistency is not limited to the 2D image plane, it can easily be
    extended to camera 3D space or 3D point clouds. In this section, we discuss two
    such methods [[52](#bib.bib52), [25](#bib.bib25)] that uses structural consistency
    in 3D space alongside other geometric constraints in an end-to-end framework.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 结构一致性不限于二维图像平面，它可以很容易地扩展到相机三维空间或三维点云。在这一部分，我们讨论了两种方法 [[52](#bib.bib52), [25](#bib.bib25)]，这些方法在三维空间中利用结构一致性，并结合其他几何约束在端到端框架中进行应用。
- en: 4.4.1 Planar Consistency
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1 平面一致性
- en: Planar consistency [[52](#bib.bib52)] is based on the assumption that most of
    the homogeneous-color regions in an indoor scene are planar regions and a continuous
    depth can be assumed for such regions. Extraction of such piece-wise planar regions
    is a three-step process. Given an input image $I$, first, the key points are extracted.
    The key points in the input image are then used to extract superpixels. Finally,
    a segmentation algorithm is used to apply a greedy approach to segment areas with
    low gradients to produce more planar regions. There are many ways to extract key
    points and superpixels, [[52](#bib.bib52)] uses Direct Sparse Odometry [[53](#bib.bib53)]
    to extract key points and Felzenszwalb superpixel segmentation [[71](#bib.bib71)]
    for superpixels and planar regions segmentation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 平面一致性 [[52](#bib.bib52)] 基于这样一个假设：在室内场景中，大多数同质颜色区域都是平面区域，并且可以假设这些区域的深度是连续的。提取这些分段平面区域的过程分为三步。给定一幅输入图像
    $I$，首先，提取关键点。然后，使用输入图像中的关键点来提取超像素。最后，使用分割算法应用贪婪方法对低梯度区域进行分割，以产生更多的平面区域。提取关键点和超像素的方法有很多种，[[52](#bib.bib52)]
    使用了直接稀疏视觉里程计 [[53](#bib.bib53)] 来提取关键点，以及 Felzenszwalb 超像素分割 [[71](#bib.bib71)]
    来进行超像素和平面区域的分割。
- en: 'Left side images in Fig. [7](#S4.F7 "Figure 7 ‣ 4.4 Structural Consistency
    in 3D Space ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey") show the steps of obtaining planar regions in
    two indoor scenes. For an image $I$, after extracting the superpixels, a threshold
    is applied to only keep regions with larger than $1000$ pixels. It is assumed
    that most planar regions occupy large pixel areas. With the extracted super pixels
    $SPP_{m}$ and its corresponding depth $D(p_{n})$, we first back project all points
    $p_{n}$ into 3D space ($p^{3D}_{n}$), Eq [34](#S4.E34 "In 4.4.1 Planar Consistency
    ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey"). Using the plane
    parameter $A_{m}$ of $SPP_{m}$, the plane is defined in 3D space, Eq. [35](#S4.E35
    "In 4.4.1 Planar Consistency ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey"). $A_{m}$ is calculated using two matrices, $Y_{m}=[1,...,1]^{T}$ and
    $P_{n}=[p^{3D}_{1},...,p^{3D}_{n}]$ as shown in Eq. [36](#S4.E36 "In 4.4.1 Planar
    Consistency ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"), where $E$ is
    an identity matrix and $\epsilon$ is a for numerical stability. With planar parameters,
    a fitted planar depth for each pixel in all superpixels can be retrieved to estimate
    the planar loss $\mathcal{L}_{planar}$ as shown in Eq. [38](#S4.E38 "In 4.4.1
    Planar Consistency ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry Preserving
    Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '图 [7](#S4.F7 "Figure 7 ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey") 左侧图像展示了在两个室内场景中获取平面区域的步骤。对于一幅图像 $I$，在提取超像素后，应用一个阈值，仅保留像素数大于 $1000$ 的区域。假设大多数平面区域占据较大的像素区域。使用提取的超像素
    $SPP_{m}$ 及其对应的深度 $D(p_{n})$，我们首先将所有点 $p_{n}$ 反向投影到三维空间 ($p^{3D}_{n}$)，见 Eq [34](#S4.E34
    "In 4.4.1 Planar Consistency ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey")。利用 $SPP_{m}$ 的平面参数 $A_{m}$，在三维空间中定义该平面，见 Eq. [35](#S4.E35 "In 4.4.1 Planar
    Consistency ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")。$A_{m}$ 通过两个矩阵计算得出，$Y_{m}=[1,...,1]^{T}$
    和 $P_{n}=[p^{3D}_{1},...,p^{3D}_{n}]$，如 Eq. [36](#S4.E36 "In 4.4.1 Planar Consistency
    ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry Preserving Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey") 所示，其中 $E$ 是单位矩阵，$\epsilon$
    是数值稳定性因子。通过平面参数，可以为所有超像素中的每个像素检索拟合的平面深度，以估计平面损失 $\mathcal{L}_{planar}$，见 Eq. [38](#S4.E38
    "In 4.4.1 Planar Consistency ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey")'
- en: '|  | $\displaystyle p^{3D}_{n}$ | $\displaystyle=D(p_{n})K^{-1}p_{n},p_{n}\subseteq
    SPP_{m}$ |  | (34) |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p^{3D}_{n}$ | $\displaystyle=D(p_{n})K^{-1}p_{n},p_{n}\subseteq
    SPP_{m}$ |  | (34) |'
- en: '|  | $\displaystyle A^{T}_{m}p^{3D}_{n}$ | $\displaystyle=1$ |  | (35) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle A^{T}_{m}p^{3D}_{n}$ | $\displaystyle=1$ |  | (35) |'
- en: '|  | $\displaystyle P_{n}A_{m}$ | $\displaystyle=Y_{m};A_{m}=(P^{T}_{n}P_{n}+\epsilon
    E)^{-1}P^{T}_{n}Y_{m}$ |  | (36) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P_{n}A_{m}$ | $\displaystyle=Y_{m};A_{m}=(P^{T}_{n}P_{n}+\epsilon
    E)^{-1}P^{T}_{n}Y_{m}$ |  | (36) |'
- en: '|  | $\displaystyle D^{\prime}(p_{n})$ | $\displaystyle=(A^{T}_{m}K^{-1}p_{n})^{-1}$
    |  | (37) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle D^{\prime}(p_{n})$ | $\displaystyle=(A^{T}_{m}K^{-1}p_{n})^{-1}$
    |  | (37) |'
- en: '|  | $\displaystyle\mathcal{L}_{planar}$ | $\displaystyle=\sum^{M}_{m=1}\sum^{N}_{n=1}&#124;D(p_{n})-D^{\prime}(p_{n})&#124;$
    |  | (38) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{planar}$ | $\displaystyle=\sum^{M}_{m=1}\sum^{N}_{n=1}&#124;D(p_{n})-D^{\prime}(p_{n})&#124;$
    |  | (38) |'
- en: 4.4.2 Point Cloud Alignment
  id: totrans-148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2 点云对齐
- en: Mahjourian et al. [[25](#bib.bib25)] use another approach to align the 3D point
    clouds of two consecutive frame ($Q_{t-1},Q_{t}$) in video depth estimation pipeline.
    It directly compares the estimated point cloud associated with respective frames
    ($\hat{Q}_{t-1}$ and $\hat{Q}_{t}$), i.e. compare $\hat{Q}_{t-1}$ to $Q_{t-1}$
    or $\hat{Q}_{t}$ to $Q_{t}$ using well know rigid registration methods, Iterative
    Closest Point (ICP) [[72](#bib.bib72), [73](#bib.bib73), [74](#bib.bib74)], that
    computes a transformation to minimize the point-to-point distance between two
    point clouds. It alternates between computing correspondences between 3D points
    and best-fit transformation between the two point clouds. For each iteration,
    it recomputes the correspondence with the previous iteration’s transformation
    applied.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Mahjourian 等人[[25](#bib.bib25)]使用另一种方法对齐视频深度估计流程中的两个连续帧的3D点云($Q_{t-1},Q_{t}$)。它直接比较与各自帧相关的估计点云($\hat{Q}_{t-1}$和$\hat{Q}_{t}$)，即使用众所周知的刚性配准方法，将$\hat{Q}_{t-1}$与$Q_{t-1}$或$\hat{Q}_{t}$与$Q_{t}$进行比较，采用迭代最近点（ICP）[[72](#bib.bib72),
    [73](#bib.bib73), [74](#bib.bib74)]，计算一个变换以最小化两个点云之间的点对点距离。它在计算3D点之间的对应关系和两个点云之间的最佳配准变换之间交替进行。每次迭代时，它会重新计算与上一迭代的变换应用的对应关系。
- en: 'ICP is not differentiable, but its gradients can be approximated using the
    products it computes as part of the algorithm, allowing back-propagation. It takes
    two point clouds $A$ and $B$ as input and produces two outputs. First is the best-fit
    transformation $T^{\prime}$ which minimizes the distance between the transformed
    points in $A$ and $B$, and second is the resudual $r^{ij}$, Eq. [39](#S4.E39 "In
    4.4.2 Point Cloud Alignment ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey"), which reflects the residual distances between corresponding points after
    ICP’s minimization. The loss, $\mathcal{L}_{3DAlignment}$, is given as in Eq.
    [40](#S4.E40 "In 4.4.2 Point Cloud Alignment ‣ 4.4 Structural Consistency in 3D
    Space ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey").'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 'ICP不可微分，但其梯度可以通过算法中计算的产品近似，从而允许反向传播。它以两个点云$A$和$B$为输入，并生成两个输出。第一个是最佳配准变换$T^{\prime}$，它最小化了$A$和$B$中变换点之间的距离，第二个是残差$r^{ij}$，公式[39](#S4.E39
    "In 4.4.2 Point Cloud Alignment ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey")，它反映了ICP最小化后的对应点之间的残差距离。损失$\mathcal{L}_{3DAlignment}$如公式[40](#S4.E40 "In
    4.4.2 Point Cloud Alignment ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry
    Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks: A
    Survey")给出。'
- en: '|  | $\displaystyle r^{ij}$ | $\displaystyle=A^{ij}-T^{\prime-1}.B^{c(ij)}$
    |  | (39) |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle r^{ij}$ | $\displaystyle=A^{ij}-T^{\prime-1}.B^{c(ij)}$
    |  | (39) |'
- en: '|  | $\displaystyle\mathcal{L}_{3DAlignment}$ | $\displaystyle=&#124;&#124;T^{\prime}-I&#124;&#124;_{1}+&#124;&#124;r&#124;&#124;_{1};I=1$
    |  | (40) |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{3DAlignment}$ | $\displaystyle=&#124;&#124;T^{\prime}-I&#124;&#124;_{1}+&#124;&#124;r&#124;&#124;_{1};I=1$
    |  | (40) |'
- en: 'For each frame $t$ of the video, if the alignment of the estimate is not perfect,
    the ICP algorithm produces a transformation $T^{\prime}_{t}$ and $r_{t}$, which
    can be used to adjust the estimates towards initial alignment [[25](#bib.bib25)].
    Right side of Fig. [7](#S4.F7 "Figure 7 ‣ 4.4 Structural Consistency in 3D Space
    ‣ 4 Geometry Preserving Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") shows the steps of alignment loss.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '对于视频的每一帧$t$，如果估计的对齐不完美，ICP算法会生成一个变换$T^{\prime}_{t}$和$r_{t}$，这些可以用来调整估计向初始对齐[[25](#bib.bib25)]。图[7](#S4.F7
    "Figure 7 ‣ 4.4 Structural Consistency in 3D Space ‣ 4 Geometry Preserving Constraints
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")的右侧显示了对齐损失的步骤。'
- en: 5 Normal-Depth Orthogonal Constraint
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 法线-深度正交约束
- en: '![Refer to caption](img/735f9d76eec5c61e7c0ab9d7b565ca4d.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/735f9d76eec5c61e7c0ab9d7b565ca4d.png)'
- en: 'Figure 8: Left: estimation of normal from depth. Right: estimation of depth
    from normal. Figure reused from [[22](#bib.bib22)]'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8：左侧：从深度估计法线。右侧：从法线估计深度。图源自 [[22](#bib.bib22)]
- en: Surface normal is an important ’local’ feature of 3D point-cloud of a scene,
    which can provide promising 3D geometric cues to estimate geometrically consistent
    depth maps. In the 3D world coordinate system, the vector connecting two pixels
    in the same plane should be orthogonal to their direction of normal. Enforcing
    normal-depth orthogonal constraint tends to improve depth estimates in 3D space
    [[75](#bib.bib75), [27](#bib.bib27), [20](#bib.bib20)].
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 表面法线是场景 3D 点云的一个重要“局部”特征，它可以提供有前景的 3D 几何线索来估计几何上一致的深度图。在 3D 世界坐标系中，同一平面上两个像素之间的向量应该与它们的法线方向正交。强制实施法线-深度正交约束有助于改善
    3D 空间中的深度估计 [[75](#bib.bib75), [27](#bib.bib27), [20](#bib.bib20)]。
- en: 'Depth to normal:'
  id: totrans-158
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 深度到法线：
- en: 'Given depth $D_{i}$, to estimate the normal of each central pixel $p_{i}$,
    Fig. [8](#S5.F8 "Figure 8 ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") (left), we need to consider the neighboring
    pixels, $p_{neighbors}$. The Fig. [8](#S5.F8 "Figure 8 ‣ 5 Normal-Depth Orthogonal
    Constraint ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") (left)
    shows $8$ neighbor convention to compute the normal of the central pixel. Two
    neighbors , $p_{ix}$ and $p_{iy}$ are selected from $p_{neighbors}$ for each central
    pixel $p_{i}$ with depth value as $D_{i}$ and camera intrinsics $K$ to estimate
    normal $\tilde{N_{i}}$. First, we project the depth in 3D space and then, use
    cross product between vector $\overrightarrow{P_{i}P_{ix}}$ and $\overrightarrow{P_{i}P_{iy}}$
    to estimate the normal, Eq. [42](#S5.E42 "In Depth to normal: ‣ 5 Normal-Depth
    Orthogonal Constraint ‣ Geometric Constraints in Deep Learning Frameworks: A Survey").
    With $8$ such estimates of normal $\tilde{N_{i}}$ for each central pixel, we estimate
    the final normal $N_{i}$ as the mean value of all the estimates using Eq. [43](#S5.E43
    "In Depth to normal: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey").'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 给定深度 $D_{i}$，为了估计每个中央像素 $p_{i}$ 的法线，如图 [8](#S5.F8 "图 8 ‣ 5 法线-深度正交约束 ‣ 深度学习框架中的几何约束：综述")（左侧）所示，我们需要考虑邻近像素
    $p_{neighbors}$。图 [8](#S5.F8 "图 8 ‣ 5 法线-深度正交约束 ‣ 深度学习框架中的几何约束：综述")（左侧）显示了 $8$
    个邻居的常规，以计算中央像素的法线。两个邻居 $p_{ix}$ 和 $p_{iy}$ 从 $p_{neighbors}$ 中选择，以每个中央像素 $p_{i}$
    的深度值 $D_{i}$ 和相机内参 $K$ 来估计法线 $\tilde{N_{i}}$。首先，我们在 3D 空间中投影深度，然后使用向量 $\overrightarrow{P_{i}P_{ix}}$
    和 $\overrightarrow{P_{i}P_{iy}}$ 之间的叉积来估计法线，如公式 [42](#S5.E42 "从深度到法线： ‣ 5 法线-深度正交约束
    ‣ 深度学习框架中的几何约束：综述") 所示。对于每个中央像素，利用 $8$ 个这样的法线估计 $\tilde{N_{i}}$，我们使用公式 [43](#S5.E43
    "从深度到法线： ‣ 5 法线-深度正交约束 ‣ 深度学习框架中的几何约束：综述") 计算最终的法线 $N_{i}$，即所有估计值的平均值。
- en: '|  | $\displaystyle P_{i}$ | $\displaystyle=K^{-1}D_{i}p_{i}$ |  | (41) |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle P_{i}$ | $\displaystyle=K^{-1}D_{i}p_{i}$ |  | (41) |'
- en: '|  | $\displaystyle\tilde{N_{i}}$ | $\displaystyle=\overrightarrow{P_{i}P_{ix}}\times\overrightarrow{P_{i}P_{iy}}$
    |  | (42) |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{N_{i}}$ | $\displaystyle=\overrightarrow{P_{i}P_{ix}}\times\overrightarrow{P_{i}P_{iy}}$
    |  | (42) |'
- en: '|  | $\displaystyle N_{i}$ | $\displaystyle=\frac{1}{8}\sum^{8}_{i=1}(\tilde{N_{i}})$
    |  | (43) |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle N_{i}$ | $\displaystyle=\frac{1}{8}\sum^{8}_{i=1}(\tilde{N_{i}})$
    |  | (43) |'
- en: 'Normal to depth:'
  id: totrans-163
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 法线与深度：
- en: 'Many methods [[22](#bib.bib22), [20](#bib.bib20), [76](#bib.bib76)] use normal
    to depth estimation to refine the depth values $D_{i}$ using the orthogonal relationship.
    For each pixel $p_{i}(x_{i},y_{i})$, the depth of each neighbor $p_{neighbor}$
    should be refined. The corresponding 3D points are, $P_{i}$ and $P_{neighbor}$
    and central pixel $P_{i}$’s normal $\overrightarrow{N_{i}}(n_{x},n_{y},n_{z})$.
    The depth of $P_{i}$ is $D_{i}$ and $P_{neighbor}$ is $D_{neighbor}$. Using the
    orthogonal relations $\overrightarrow{N_{i}}\perp\overrightarrow{P_{i}P_{neighbor}}$,
    we can write the Eq. [44](#S5.E44 "In Normal to depth: ‣ 5 Normal-Depth Orthogonal
    Constraint ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"). With
    depth estimates coming from eight neighbors, we need a method for weighting these
    values to incorporate discontinuity of normal in some edges or irregular surfaces.
    Generally, the weight $w_{i}$ is inferred from the reference image $I_{i}$, making
    depth more conforming to geometric consistency. The value of $w_{i}$ depends on
    the gradient between $p_{i}$ and $p_{neighbor}$, Eq. [45](#S5.E45 "In Normal to
    depth: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey"). The bigger values of gradient represent the less
    reliability of the refined depth. Given the eight neighbors, the final refined
    depth $\tilde{D}_{neighbor}$ is a weighted sum of eight different directions as
    shown in Eq. [46](#S5.E46 "In Normal to depth: ‣ 5 Normal-Depth Orthogonal Constraint
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") [[22](#bib.bib22)].
    This is the outcome of the regularization in 3D space, improving the accuracy
    and continuity of the estimated depth maps.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 许多方法[[22](#bib.bib22), [20](#bib.bib20), [76](#bib.bib76)] 使用法线到深度的估计来通过正交关系来优化深度值
    $D_{i}$。对于每个像素 $p_{i}(x_{i},y_{i})$，每个邻域 $p_{neighbor}$ 的深度需要被优化。对应的 3D 点为 $P_{i}$
    和 $P_{neighbor}$，以及中心像素 $P_{i}$ 的法线 $\overrightarrow{N_{i}}(n_{x},n_{y},n_{z})$。$P_{i}$
    的深度为 $D_{i}$，$P_{neighbor}$ 的深度为 $D_{neighbor}$。利用正交关系 $\overrightarrow{N_{i}}\perp\overrightarrow{P_{i}P_{neighbor}}$，我们可以写出方程
    [44](#S5.E44 "在法线到深度： ‣ 5 法线-深度正交约束 ‣ 深度学习框架中的几何约束：综述")。通过来自八个邻域的深度估计，我们需要一种方法来加权这些值，以考虑一些边缘或不规则表面上的法线不连续性。通常，权重
    $w_{i}$ 是从参考图像 $I_{i}$ 中推断出来的，使得深度更符合几何一致性。$w_{i}$ 的值取决于 $p_{i}$ 和 $p_{neighbor}$
    之间的梯度，方程 [45](#S5.E45 "在法线到深度： ‣ 5 法线-深度正交约束 ‣ 深度学习框架中的几何约束：综述")。梯度的值越大，优化后的深度的可靠性就越低。给定八个邻域，最终优化的深度
    $\tilde{D}_{neighbor}$ 是八个不同方向的加权和，如方程 [46](#S5.E46 "在法线到深度： ‣ 5 法线-深度正交约束 ‣ 深度学习框架中的几何约束：综述")
    [[22](#bib.bib22)] 所示。这是 3D 空间中正则化的结果，提升了估计深度图的准确性和连续性。
- en: '|  | <math   alttext="\displaystyle[K^{-1}D_{i}p_{i}-K^{-1}D_{neighbor}P_{neighbor}]\begin{bmatrix}n_{x}\\
    n_{y}\\'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\displaystyle[K^{-1}D_{i}p_{i}-K^{-1}D_{neighbor}P_{neighbor}]\begin{bmatrix}n_{x}\\
    n_{y}\\'
- en: n_{z}\\
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: n_{z}\\
- en: \end{bmatrix}" display="inline"><semantics ><mrow ><mrow  ><mo stretchy="false"  >[</mo><mrow
    ><mrow ><msup  ><mi >K</mi><mrow ><mo >−</mo><mn >1</mn></mrow></msup><mo lspace="0em"
    rspace="0em" >​</mo><msub ><mi  >D</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >p</mi><mi >i</mi></msub></mrow><mo >−</mo><mrow  ><msup ><mi >K</mi><mrow
    ><mo  >−</mo><mn >1</mn></mrow></msup><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >D</mi><mrow ><mi  >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >h</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >b</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >r</mi></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >P</mi><mrow
    ><mi  >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >h</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >b</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >r</mi></mrow></msub></mrow></mrow><mo stretchy="false"  >]</mo></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo >[</mo><mtable rowspacing="0pt" ><mtr  ><mtd ><msub
    ><mi  >n</mi><mi >x</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >n</mi><mi
    >y</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >n</mi><mi >z</mi></msub></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><csymbol cd="latexml" >delimited-[]</csymbol><apply ><apply  ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐾</ci><apply ><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐷</ci><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑝</ci><ci >𝑖</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐾</ci><apply ><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐷</ci><apply ><ci  >𝑛</ci><ci >𝑒</ci><ci >𝑖</ci><ci >𝑔</ci><ci >ℎ</ci><ci >𝑏</ci><ci
    >𝑜</ci><ci >𝑟</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑃</ci><apply ><ci >𝑛</ci><ci >𝑒</ci><ci >𝑖</ci><ci >𝑔</ci><ci >ℎ</ci><ci >𝑏</ci><ci
    >𝑜</ci><ci >𝑟</ci></apply></apply></apply></apply></apply><apply ><csymbol cd="latexml"
    >matrix</csymbol><matrix ><matrixrow  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑛</ci><ci >𝑥</ci></apply></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑛</ci><ci >𝑦</ci></apply></matrixrow><matrixrow ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑛</ci><ci >𝑧</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle[K^{-1}D_{i}p_{i}-K^{-1}D_{neighbor}P_{neighbor}]\begin{bmatrix}n_{x}\\
    n_{y}\\ n_{z}\\ \end{bmatrix}</annotation></semantics></math> | $\displaystyle=0$
    |  | (44) |
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: \end{bmatrix}" display="inline"><semantics ><mrow ><mrow  ><mo stretchy="false"  >[</mo><mrow
    ><mrow ><msup  ><mi >K</mi><mrow ><mo >−</mo><mn >1</mn></mrow></msup><mo lspace="0em"
    rspace="0em" >​</mo><msub ><mi  >D</mi><mi >i</mi></msub><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >p</mi><mi >i</mi></msub></mrow><mo >−</mo><mrow  ><msup ><mi >K</mi><mrow
    ><mo  >−</mo><mn >1</mn></mrow></msup><mo lspace="0em" rspace="0em"  >​</mo><msub
    ><mi >D</mi><mrow ><mi  >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >g</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >h</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >b</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >r</mi></mrow></msub><mo lspace="0em" rspace="0em"  >​</mo><msub ><mi >P</mi><mrow
    ><mi  >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >e</mi><mo lspace="0em"
    rspace="0em"  >​</mo><mi >i</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >g</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >h</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >b</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >o</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >r</mi></mrow></msub></mrow></mrow><mo stretchy="false"  >]</mo></mrow><mo lspace="0em"
    rspace="0em"  >​</mo><mrow ><mo >[</mo><mtable rowspacing="0pt" ><mtr  ><mtd ><msub
    ><mi  >n</mi><mi >x</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >n</mi><mi
    >y</mi></msub></mtd></mtr><mtr ><mtd ><msub  ><mi >n</mi><mi >z</mi></msub></mtd></mtr></mtable><mo
    >]</mo></mrow></mrow><annotation-xml encoding="MathML-Content" ><apply  ><apply
    ><csymbol cd="latexml" >delimited-[]</csymbol><apply ><apply  ><apply ><csymbol
    cd="ambiguous" >superscript</csymbol><ci >𝐾</ci><apply ><cn type="integer" >1</cn></apply></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝐷</ci><ci >𝑖</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><ci >𝑝</ci><ci >𝑖</ci></apply></apply><apply
    ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >𝐾</ci><apply ><cn
    type="integer" >1</cn></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝐷</ci><apply ><ci  >𝑛</ci><ci >𝑒</ci><ci >𝑖</ci><ci >𝑔</ci><ci >ℎ</ci><ci >𝑏</ci><ci
    >𝑜</ci><ci >𝑟</ci></apply></apply><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑃</ci><apply ><ci >𝑛</ci><ci >𝑒</ci><ci >𝑖</ci><ci >𝑔</ci><ci >ℎ</ci><ci >𝑏</ci><ci
    >𝑜</ci><ci >𝑟</ci></apply></apply></apply></apply></apply><apply ><csymbol cd="latexml"
    >matrix</csymbol><matrix ><matrixrow  ><apply ><csymbol cd="ambiguous" >subscript</csymbol><ci
    >𝑛</ci><ci >𝑥</ci></apply></matrixrow><matrixrow ><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >𝑛</ci><ci >𝑦</ci></apply></matrixrow><matrixrow ><apply  ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝑛</ci><ci >𝑧</ci></apply></matrixrow></matrix></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\displaystyle[K^{-1}D_{i}p_{i}-K^{-1}D_{neighbor}P_{neighbor}]\begin{bmatrix}n_{x}\\
    n_{y}\\ n_{z}\\ \end{bmatrix}</annotation></semantics></math> | $\displaystyle=0$
    |  | (44) |
- en: '|  | $\displaystyle w_{i}$ | $\displaystyle=e^{-\alpha&#124;\triangledown I_{i}&#124;}$
    |  | (45) |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle w_{i}$ | $\displaystyle=e^{-\alpha\mid\triangledown I_{i}\mid}$
    |  | (45) |'
- en: '|  | $\displaystyle\tilde{D}_{neighbor}$ | $\displaystyle=\sum^{8}_{i=1}\frac{w_{i}}{\sum^{8}_{i=1}w_{i}}D_{neighbor}$
    |  | (46) |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\tilde{D}_{neighbor}$ | $\displaystyle=\sum^{8}_{i=1}\frac{w_{i}}{\sum^{8}_{i=1}w_{i}}D_{neighbor}$
    |  | (46) |'
- en: 'Yang et al [[20](#bib.bib20)] use a similar formulation as shown in Eqs. [44](#S5.E44
    "In Normal to depth: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"), [45](#S5.E45 "In Normal to depth: ‣ 5
    Normal-Depth Orthogonal Constraint ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey"), and [46](#S5.E46 "In Normal to depth: ‣ 5 Normal-Depth Orthogonal
    Constraint ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") to
    enforce geometric consistency in unsupervised video depth estimation problems.
    Wang et al. [[75](#bib.bib75)] use the orthogonal compatibility principle to bring
    consistency in the normal directions of two pixels falling on the same plane.
    Eigen and Fergus [[27](#bib.bib27)] use a single multiscale CNN to estimate depth,
    surface normal, and semantic labeling. For surface normal estimation at each pixel,
    they predict the $x,y$ and $z$ components for each pixel [[77](#bib.bib77)] and
    employ elementwise loss comparison with dot product as shown in Eq. [47](#S5.E47
    "In Normal to depth: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"), where $N$ is the valid pixels, $n$ and
    $\hat{n}$ are ground truth and the predicted normal at each pixel.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 'Yang 等人 [[20](#bib.bib20)] 使用与方程 [44](#S5.E44 "In Normal to depth: ‣ 5 Normal-Depth
    Orthogonal Constraint ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")、[45](#S5.E45
    "In Normal to depth: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") 和 [46](#S5.E46 "In Normal to depth: ‣
    5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey") 中所示的类似公式来强制在无监督视频深度估计问题中的几何一致性。Wang 等人 [[75](#bib.bib75)]
    使用正交兼容性原理来确保落在同一平面上的两个像素的法线方向的一致性。Eigen 和 Fergus [[27](#bib.bib27)] 使用单个多尺度卷积神经网络（CNN）来估计深度、表面法线和语义标注。对于每个像素的表面法线估计，他们预测每个像素的
    $x,y$ 和 $z$ 组件 [[77](#bib.bib77)]，并使用元素级别的损失比较和点积，如方程 [47](#S5.E47 "In Normal
    to depth: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey") 中所示，其中 $N$ 是有效像素，$n$ 和 $\hat{n}$ 是每个像素的真实法线和预测法线。'
- en: '|  | $\mathcal{L}_{normal}=-\frac{1}{N}\sum_{i}\hat{n}_{i}.n_{i}$ |  | (47)
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{normal}=-\frac{1}{N}\sum_{i}\hat{n}_{i}.n_{i}$ |  | (47)
    |'
- en: Qi et al. [[78](#bib.bib78)] also employ depth-to-normal and normal-to-depth
    networks to regularize the depth estimate in 3D space following [[79](#bib.bib79)],
    but they do not use 8-neighbor-based calculation. Instead, they use a distance-based
    selection of neighboring pixels are given as
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Qi 等人 [[78](#bib.bib78)] 也使用深度到法线和法线到深度的网络来规范三维空间中的深度估计，参考 [[79](#bib.bib79)]，但他们不使用基于8邻域的计算。相反，他们使用基于距离的邻域像素选择，定义如下：
- en: '|  | $N_{i}=\{(x_{j},y_{j},z_{j})&#124;&#124;u_{i}-u_{j}&#124;<\beta,&#124;v_{i}-v_{j}&#124;<\beta,&#124;z_{i}-z_{j}&#124;<\gamma
    z_{i}\}$ |  | (48) |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | $N_{i}=\{(x_{j},y_{j},z_{j})\mid\mid u_{i}-u_{j}\mid<\beta,\mid v_{i}-v_{j}\mid<\beta,\mid
    z_{i}-z_{j}\mid<\gamma z_{i}\}$ |  | (48) |'
- en: where, $u_{i},v_{i}$ are the 2D coordinates, $(x_{i},y_{i},z_{i})$ are the 3D
    coordinates, $\beta$ and $\gamma$ are hyper-parameters controlling the size of
    the neighborhood along $x-y$ and depth axes respectively. They use $L_{2}$ norm
    between the ground truth normal and the estimated normal as the loss $\mathcal{L}_{normal}$
    in end-to-end deep learning framework.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$u_{i},v_{i}$ 是二维坐标，$(x_{i},y_{i},z_{i})$ 是三维坐标，$\beta$ 和 $\gamma$ 是控制$x-y$平面和深度轴方向邻域大小的超参数。他们在端到端深度学习框架中使用地面真实法线和估计法线之间的$L_{2}$范数作为损失$\mathcal{L}_{normal}$。
- en: 'Hu et al. [[69](#bib.bib69)] use ground truth and estimated depth map gradients
    to measure the angle between their surface normals, Eqs. [49](#S5.E49 "In Normal
    to depth: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey") and [50](#S5.E50 "In Normal to depth: ‣ 5 Normal-Depth
    Orthogonal Constraint ‣ Geometric Constraints in Deep Learning Frameworks: A Survey").
    The loss, $\mathcal{L}_{normal}$ is estimated as per Eq. [51](#S5.E51 "In Normal
    to depth: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey"), where $\langle.,.\rangle$ denotes the inner product
    of vectors. This loss is sensitive to small depth structures [[69](#bib.bib69)].
    Yang et al. [[68](#bib.bib68)] use the same method of estimating normal for monocular
    depth estimation problems in Endoscopy applications.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 'Hu 等人 [[69](#bib.bib69)] 使用真实深度和估计的深度图梯度来测量它们的表面法线之间的角度，公式 [49](#S5.E49 "In
    Normal to depth: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") 和 [50](#S5.E50 "In Normal to depth: ‣
    5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey")。损失 $\mathcal{L}_{normal}$ 根据公式 [51](#S5.E51 "In Normal
    to depth: ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey") 进行估计，其中 $\langle.,.\rangle$ 表示向量的内积。该损失对小的深度结构敏感
    [[69](#bib.bib69)]。Yang 等人 [[68](#bib.bib68)] 使用相同的方法来估计单目深度估计问题中的法线，用于内窥镜应用。'
- en: '|  | $\displaystyle n^{\hat{d}}_{i}$ | $\displaystyle=[-\triangledown_{x}\hat{d}_{i},-\triangledown_{y}\hat{d}_{i},1]^{T}$
    |  | (49) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle n^{\hat{d}}_{i}$ | $\displaystyle=[-\triangledown_{x}\hat{d}_{i},-\triangledown_{y}\hat{d}_{i},1]^{T}$
    |  | (49) |'
- en: '|  | $\displaystyle n^{d}_{i}$ | $\displaystyle=[-\triangledown_{x}d_{i},-\triangledown_{y}d_{i},1]^{T}$
    |  | (50) |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle n^{d}_{i}$ | $\displaystyle=[-\triangledown_{x}d_{i},-\triangledown_{y}d_{i},1]^{T}$
    |  | (50) |'
- en: '|  | $\displaystyle\mathcal{L}_{normal}$ | $\displaystyle=\frac{1}{N}\sum^{N}_{i=1}\left(1-\frac{\langle
    n^{\hat{d}}_{i},n^{d}_{i}\rangle}{\sqrt{\langle n^{\hat{d}}_{i},n^{\hat{d}}_{i}\rangle}\sqrt{\langle
    n^{d}_{i},n^{d}_{i}\rangle}}\right)$ |  | (51) |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{normal}$ | $\displaystyle=\frac{1}{N}\sum^{N}_{i=1}\left(1-\frac{\langle
    n^{\hat{d}}_{i},n^{d}_{i}\rangle}{\sqrt{\langle n^{\hat{d}}_{i},n^{\hat{d}}_{i}\rangle}\sqrt{\langle
    n^{d}_{i},n^{d}_{i}\rangle}}\right)$ |  | (51) |'
- en: There are several advantages of estimating normal in a depth estimation framework,
    like, it provides an explicit understanding of normal during the learning process,
    it provides higher-order interaction between the estimated depth and ground truths
    and it also provides flexibility to integrate additional operations, e.g. Manhattan
    assumption, over normal [[20](#bib.bib20)]. It also has certain disadvantages,
    like, as it is prone to noise in the ground truth depth maps or the estimated
    depth maps and it only considers local information for its estimation which may
    not align with the global structure.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度估计框架中估计法线有几个优点，比如，它在学习过程中提供了对法线的明确理解，它提供了估计深度和真实值之间的高阶交互，并且它还提供了将额外操作（例如，Manhattan
    假设）集成到法线上的灵活性 [[20](#bib.bib20)]。它也有一些缺点，例如，它容易受到真实深度图或估计深度图中的噪声影响，并且它仅考虑局部信息进行估计，这可能与全局结构不一致。
- en: '|  | $\mathcal{L}_{Normal_{robust}}=\frac{1}{N}\left(\sum^{N}_{i=1}&#124;&#124;\hat{n}-n_{i}&#124;&#124;_{1}\right)$
    |  | (52) |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{Normal_{robust}}=\frac{1}{N}\left(\sum^{N}_{i=1}&#124;&#124;\hat{n}-n_{i}&#124;&#124;_{1}\right)$
    |  | (52) |'
- en: 'To enforce robust higher-order geometric supervision in 3D space, Yin et al.
    [[76](#bib.bib76)] propose virtual normal (VN) estimation. VN can establish 3D
    geometric connections between regions in a much larger range. To estimate VN,
    $N$ group points from the depth map, with three points in each group, are sampled.
    The selected point has to be non-colinear. The three points establish a plane
    and the corresponding normal is estimated. Similarly, ground truth normal $(n_{i})$
    are estimated and compared with the normal corresponding to the estimated depth
    maps $(\hat{n}_{i})$ as shown in Eq. [52](#S5.E52 "In Normal to depth: ‣ 5 Normal-Depth
    Orthogonal Constraint ‣ Geometric Constraints in Deep Learning Frameworks: A Survey").
    Naderi et al. [[30](#bib.bib30)] use a similar formulation in a monocular depth
    estimation problem to enforce higher-order robust geometric constraints for depth
    estimation.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在3D空间中施加强健的高阶几何监督，Yin等人[[76](#bib.bib76)] 提出了虚拟法线（VN）估计。VN可以在更大范围内建立3D几何连接。为了估计VN，从深度图中采样$N$组点，每组三个点。所选点必须是非共线的。这三个点建立一个平面，并估计相应的法线。类似地，地面真实法线$(n_{i})$被估计并与对应于估计深度图的法线$(\hat{n}_{i})$进行比较，如方程[52](#S5.E52
    "在法线到深度：‣ 5 法线-深度正交约束 ‣ 深度学习框架中的几何约束：调查")所示。Naderi等人[[30](#bib.bib30)] 在单目深度估计问题中使用了类似的公式，以施加高阶强健几何约束进行深度估计。
- en: 'Normal-depth joint learning approach:'
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 普通深度联合学习方法：
- en: '![Refer to caption](img/0b094730f554049ae9765ec9e7bec2ed.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0b094730f554049ae9765ec9e7bec2ed.png)'
- en: 'Figure 9: Normal-depth joint learning approached. Depth and normal are estimated
    using the same feature cost volume [[80](#bib.bib80)]. The above diagram is a
    slight modification from Kusupati et al [[80](#bib.bib80)] to only show the joint
    estimation setup.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：法线-深度联合学习方法。深度和法线使用相同的特征代价体积进行估计[[80](#bib.bib80)]。上图是Kusupati等人[[80](#bib.bib80)]略微修改的版本，仅展示联合估计设置。
- en: 'Kusupati et al. [[80](#bib.bib80)] develop a normal-assisted depth estimation
    algorithm that couples the learning of multi-view normal estimation and multi-view
    depth estimation process. It utilizes feature cost volume to estimate the depth
    map and the normal. A cost volume provides better structural representation to
    facilitate better learning on the image features for estimating the underlying
    surface normal. Specifically, Kusupati et el. [[80](#bib.bib80)] estimates two
    depth maps $Z_{1}$ and $Z_{2}$ using 3D-CNNs, see Fig. [9](#S5.F9 "Figure 9 ‣
    Normal-depth joint learning approach: ‣ 5 Normal-Depth Orthogonal Constraint ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey"), and utilizes a
    7-layered CNN (NNet, Fig. [9](#S5.F9 "Figure 9 ‣ Normal-depth joint learning approach:
    ‣ 5 Normal-Depth Orthogonal Constraint ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey")) to estimate normal. The world coordinate volume is concatenated
    with the initial feature volume to provide a feature slice to NNet as input. NNet
    predicts normal for each feature slice, which is later averaged to get the final
    normal map.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Kusupati等人[[80](#bib.bib80)] 开发了一种法线辅助深度估计算法，该算法将多视角法线估计和多视角深度估计过程结合起来。它利用特征代价体积来估计深度图和法线。代价体积提供了更好的结构表示，以便在图像特征上进行更好的学习，从而估计底层表面法线。具体来说，Kusupati等人[[80](#bib.bib80)]
    使用3D-CNNs估计两个深度图$Z_{1}$和$Z_{2}$，见图[9](#S5.F9 "图9 ‣ 法线-深度联合学习方法：‣ 5 法线-深度正交约束 ‣
    深度学习框架中的几何约束：调查")，并利用一个7层的CNN（NNet，图[9](#S5.F9 "图9 ‣ 法线-深度联合学习方法：‣ 5 法线-深度正交约束
    ‣ 深度学习框架中的几何约束：调查")）来估计法线。世界坐标体积与初始特征体积连接，以提供特征切片作为NNet的输入。NNet预测每个特征切片的法线，随后取平均得到最终的法线图。
- en: 6 Attention Meets Geometry
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 注意力与几何的结合
- en: 'A transformer with a self-attention mechanism is introduced by Dosovitski et
    al. [[12](#bib.bib12)] in vision domain. It can learn long-range global-contextual
    representation by dynamically shifting its attention within the image. The inputs
    to the attention module are usually named query (Q), key (K), and value (V). Q
    retrieves information from V based on the attention weights, Eq. [53](#S6.E53
    "In 6 Attention Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey"), where $\mathcal{A}(.)$ is a function that produces similarity score
    as attention weight between feature embeddings for aggregation.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 'Dosovitski 等人 [[12](#bib.bib12)] 在视觉领域引入了一种具有自注意力机制的变换器。它可以通过动态调整其在图像内的注意力来学习长距离的全局上下文表示。注意力模块的输入通常被称为查询（Q）、键（K）和值（V）。Q
    根据注意力权重从 V 中检索信息，方程 [53](#S6.E53 "In 6 Attention Meets Geometry ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") 中，$\mathcal{A}(.)$ 是一个生成相似度分数作为特征嵌入聚合的注意力权重的函数。'
- en: The performance of stereo or MVS depth estimation methods depends on finding
    dense correspondence between reference images and source images. Recently, Sun
    et al. [[81](#bib.bib81)] showed that features extracted using a transformer model
    with self- and cross-attention can produce significantly improved correspondences
    as compared to the features extracted using convolutional neural networks. These
    attention mechanisms are designed to pay attention to the contextual information
    and not to geometry-based information. Recently, a few methods have modified these
    attention mechanisms to consider geometric information while calculating the attention
    weight [[82](#bib.bib82), [30](#bib.bib30), [83](#bib.bib83), [84](#bib.bib84)].
    In this section, we discuss such methods and their approach to include geometric
    information in attention.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 立体或多视图深度估计方法的性能取决于在参考图像和源图像之间找到密集的对应关系。最近，Sun 等人 [[81](#bib.bib81)] 表明，使用自注意力和交叉注意力的变换器模型提取的特征比使用卷积神经网络提取的特征能显著改善对应关系。这些注意力机制旨在关注上下文信息，而非基于几何的信息。近期，一些方法已修改这些注意力机制，以在计算注意力权重时考虑几何信息
    [[82](#bib.bib82), [30](#bib.bib30), [83](#bib.bib83), [84](#bib.bib84)]。在本节中，我们将探讨这些方法及其将几何信息纳入注意力的方式。
- en: '|  | $\displaystyle Attention(Q,K,V)$ | $\displaystyle=\mathcal{A}(Q,K)V$ |  |
    (53) |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle Attention(Q,K,V)$ | $\displaystyle=\mathcal{A}(Q,K)V$ |  |
    (53) |'
- en: '|  | $\displaystyle p_{i}=K^{-1}(d_{i}.C_{i});p_{j}$ | $\displaystyle=K^{-1}(d_{j}.C_{j})$
    |  | (54) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle p_{i}=K^{-1}(d_{i}.C_{i});p_{j}$ | $\displaystyle=K^{-1}(d_{j}.C_{j})$
    |  | (54) |'
- en: '|  | $\displaystyle\mathcal{A}^{spatial}_{i,j}$ | $\displaystyle=exp\left(-\frac{&#124;&#124;P_{i}-P_{j}&#124;&#124;_{2}}{\sigma}\right)$
    |  | (55) |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{A}^{spatial}_{i,j}$ | $\displaystyle=exp\left(-\frac{&#124;&#124;P_{i}-P_{j}&#124;&#124;_{2}}{\sigma}\right)$
    |  | (55) |'
- en: '|  | $\displaystyle\mathcal{A}^{temporal}_{i,j}$ | $\displaystyle=Softmax_{j}(F^{q^{T}}_{i}F^{k}_{j})$
    |  | (56) |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{A}^{temporal}_{i,j}$ | $\displaystyle=Softmax_{j}(F^{q^{T}}_{i}F^{k}_{j})$
    |  | (56) |'
- en: 'Ruhkamp et al. [[82](#bib.bib82)] use geometry to guide spatial-temporal attention
    to guide self-supervised monocular depth estimation method from videos. They propose
    a spatial-attention layer with 3D spatial awareness by exploiting a coarse predicted
    initial depth estimate. With known intrinsic camera parameter K and pair of coordinates
    ($C_{i},C_{j}$) along with their depth estimates $(d_{i},d_{j})$, they back-project
    the depth values into 3D space using Eq. [54](#S6.E54 "In 6 Attention Meets Geometry
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"). The 3D space-aware
    spatial attention is then calculated as per Eq. [55](#S6.E55 "In 6 Attention Meets
    Geometry ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"), where
    $P_{i},P_{j}$ are treated as K and Q, respectively. They use Eq. [56](#S6.E56
    "In 6 Attention Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") to estimate the temporal attention for aggregation. The unique formulation
    of the spatial-temporal attention model can explicitly correlate geometrically
    meaningful and spatially coherent features for dense correspondence [[82](#bib.bib82)].'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 'Ruhkamp 等人[[82](#bib.bib82)] 使用几何来指导空间-时间注意力，以指导自监督单目深度估计方法从视频中提取。他们通过利用粗略预测的初始深度估计，提出了一种具有3D空间感知的空间注意力层。通过已知的内在相机参数
    K 和一对坐标 ($C_{i},C_{j}$) 以及它们的深度估计 $(d_{i},d_{j})$，他们使用方程[54](#S6.E54 "In 6 Attention
    Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")
    将深度值反投影到3D空间中。然后，根据方程[55](#S6.E55 "In 6 Attention Meets Geometry ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") 计算3D空间感知的空间注意力，其中 $P_{i},P_{j}$ 被视为 K
    和 Q。通过方程[56](#S6.E56 "In 6 Attention Meets Geometry ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey") 估计时间注意力以进行聚合。空间-时间注意力模型的独特公式可以显式地将几何上有意义和空间上连贯的特征关联起来，以实现密集匹配[[82](#bib.bib82)]。'
- en: '![Refer to caption](img/0b7394666c1ff849107d45574e81a8ec.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/0b7394666c1ff849107d45574e81a8ec.png)'
- en: 'Figure 10: Adaptive Geometric Attention. Figure reused from [[30](#bib.bib30)]'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：自适应几何注意力。图像重用自[[30](#bib.bib30)]
- en: 'Naderi et al. [[30](#bib.bib30)] propose adaptive geometric attention (AGA)
    for monocular depth estimation problems with encoder-decoder architecture. They
    apply the AGA module in the decoding step utilizing both low-level $(F_{L})$ and
    high-level $(F_{h})$ features. Fig. [10](#S6.F10 "Figure 10 ‣ 6 Attention Meets
    Geometry ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") shows
    the process of calculating AGA. The first row of operation in Fig. [10](#S6.F10
    "Figure 10 ‣ 6 Attention Meets Geometry ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey") shows the steps to calculate channel-attention $(\mathcal{CA})$,
    which produces $1\times 1\times C$ shape attention map and is multiplied with
    the $F_{l}$. Rest, two rows of calculation shows two spatial attention $(\mathcal{SA})$
    calculation which is equivalent to Eq. [57](#S6.E57 "In 6 Attention Meets Geometry
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"). The final aggregated
    feature output $(F_{o}ut)$ is estimated using Eq. [60](#S6.E60 "In 6 Attention
    Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks: A Survey"),
    where $\mathcal{SA}_{1}$ is added and $\mathcal{SA}_{2}$ is multiplied with $F_{L}$.
    $f_{1}(.)$ and $f_{2}(.)$ are introduced to enhance the sensitivity to any non-zero
    correlation between $F_{l}$ and $F_{h}$, Eq. [58](#S6.E58 "In 6 Attention Meets
    Geometry ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") is the
    with no-enhanced sensitivity, whereas Eq. [59](#S6.E59 "In 6 Attention Meets Geometry
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") shows the formulation
    of sensitivity enhanced attention map.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 'Naderi 等人[[30](#bib.bib30)] 提出了用于单目深度估计问题的自适应几何注意力（AGA），采用编码器-解码器架构。他们在解码步骤中应用
    AGA 模块，利用低级 $(F_{L})$ 和高级 $(F_{h})$ 特征。图[10](#S6.F10 "Figure 10 ‣ 6 Attention
    Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")
    展示了计算 AGA 的过程。图[10](#S6.F10 "Figure 10 ‣ 6 Attention Meets Geometry ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey") 的第一行操作展示了计算通道注意力 $(\mathcal{CA})$
    的步骤，生成 $1\times 1\times C$ 形状的注意力图，并与 $F_{l}$ 相乘。其余的两行计算展示了两个空间注意力 $(\mathcal{SA})$
    的计算，相当于方程[57](#S6.E57 "In 6 Attention Meets Geometry ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey")。最终的聚合特征输出 $(F_{o}ut)$ 使用方程[60](#S6.E60 "In
    6 Attention Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") 进行估计，其中 $\mathcal{SA}_{1}$ 被加上，$\mathcal{SA}_{2}$ 与 $F_{L}$ 相乘。引入 $f_{1}(.)$
    和 $f_{2}(.)$ 来增强对 $F_{l}$ 和 $F_{h}$ 之间任何非零相关性的敏感性，方程[58](#S6.E58 "In 6 Attention
    Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")
    是没有增强敏感性的，而方程[59](#S6.E59 "In 6 Attention Meets Geometry ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey") 展示了增强敏感性的注意力图的公式。'
- en: '|  | $\displaystyle\mathcal{SA}_{i}$ | $\displaystyle=&#124;Cosine_{similarity}(E_{l,i},E_{h,i})&#124;,i=1,2$
    |  | (57) |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{SA}_{i}$ | $\displaystyle=&#124;Cosine_{similarity}(E_{l,i},E_{h,i})&#124;,i=1,2$
    |  | (57) |'
- en: '|  | $\displaystyle f(\mathcal{SA})$ | $\displaystyle=\mathcal{SA}$ |  | (58)
    |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(\mathcal{SA})$ | $\displaystyle=\mathcal{SA}$ |  | (58)
    |'
- en: '|  | $\displaystyle f(\mathcal{SA})$ | $\displaystyle=\mathcal{SA}.\,exp(\mathcal{SA})$
    |  | (59) |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle f(\mathcal{SA})$ | $\displaystyle=\mathcal{SA}.\,exp(\mathcal{SA})$
    |  | (59) |'
- en: '|  | $\displaystyle F_{out}$ | $\displaystyle=[f_{1}(\mathcal{SA}_{1})+f_{2}(\mathcal{SA}_{2})\times\mathcal{CA}]\times
    F_{l}+F_{h}$ |  | (60) |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle F_{out}$ | $\displaystyle=[f_{1}(\mathcal{SA}_{1})+f_{2}(\mathcal{SA}_{2})\times\mathcal{CA}]\times
    F_{l}+F_{h}$ |  | (60) |'
- en: Zhu et al. [[83](#bib.bib83)] use two types of transformer modules to extract
    geometry-aware features in an MVS pipeline, a global-context transformer module
    and a 3D geometry transformer module. The global-context transformer module extracts
    3D-consistent reference features $(T_{r})$ which is then used as input to the
    3D-geometry transformer module to facilitate cross-view attention. $(T_{r})$ is
    used to generate K and V to enhance interaction between reference and source view
    features for obtaining dense correspondence.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: Zhu 等人 [[83](#bib.bib83)] 在 MVS 流程中使用了两种类型的变换器模块来提取几何感知特征，即全局上下文变换器模块和 3D 几何变换器模块。全局上下文变换器模块提取
    3D 一致的参考特征 $(T_{r})$，然后将其作为输入传递给 3D 几何变换器模块，以促进跨视图注意力。$(T_{r})$ 被用来生成 K 和 V，以增强参考视图和源视图特征之间的交互，从而获得密集的对应关系。
- en: 'Guo et al. [[84](#bib.bib84)] use a geometrically aware attention mechanism
    for image captioning tasks. Unlike other methods discussed above, they do not
    modify the attention mechanism in itself but add a bias term which makes the feature
    extraction biased towards specific content. Their attention module is similar
    to [[12](#bib.bib12)] apart from the added bias term in score $E$ calculation,
    Eq. [61](#S6.E61 "In 6 Attention Meets Geometry ‣ Geometric Constraints in Deep
    Learning Frameworks: A Survey"), where $G_{ij}$ is the relative geometry feature
    between two objects $i$ and $j$. There are two terms in score $E$, the left one
    is content-based weights and the right one is geometric bias. They propose three
    different ways of applying geometric bias. content-independent bias (Eq. [62](#S6.E62
    "In 6 Attention Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")) assumes static geometric bias, i.e. same geometric bias is applied
    to all the K-Q pairs. The query-dependent bias provides geometric information
    based on the type of query ([63](#S6.E63 "In 6 Attention Meets Geometry ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey")) and key-dependent bias provides
    geometric information based on the clues present in keys (Eq. [64](#S6.E64 "In
    6 Attention Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")).'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 'Guo 等人 [[84](#bib.bib84)] 为图像标注任务使用了几何感知的注意力机制。与上述其他方法不同的是，他们并没有修改注意力机制本身，而是添加了一个偏置项，使得特征提取倾向于特定内容。他们的注意力模块与
    [[12](#bib.bib12)] 相似，除了在分数 $E$ 计算中添加的偏置项，公式 [61](#S6.E61 "In 6 Attention Meets
    Geometry ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")，其中 $G_{ij}$
    是两个对象 $i$ 和 $j$ 之间的相对几何特征。分数 $E$ 中有两个项，左边是基于内容的权重，右边是几何偏置。他们提出了三种不同的几何偏置应用方式。内容无关偏置（公式
    [62](#S6.E62 "In 6 Attention Meets Geometry ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey")）假设静态几何偏置，即相同的几何偏置应用于所有 K-Q 对。查询依赖偏置根据查询的类型提供几何信息（[63](#S6.E63
    "In 6 Attention Meets Geometry ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")），键依赖偏置根据键中存在的线索提供几何信息（公式 [64](#S6.E64 "In 6 Attention Meets Geometry
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")）。'
- en: '|  | $\displaystyle E$ | $\displaystyle=QK^{T}+\phi(Q^{\prime},K^{\prime},G)$
    |  | (61) |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle E$ | $\displaystyle=QK^{T}+\phi(Q^{\prime},K^{\prime},G)$
    |  | (61) |'
- en: '|  | $\displaystyle\phi^{independent}_{ij}$ | $\displaystyle=ReLU(w^{T}_{g}G_{ij})$
    |  | (62) |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi^{independent}_{ij}$ | $\displaystyle=ReLU(w^{T}_{g}G_{ij})$
    |  | (62) |'
- en: '|  | $\displaystyle\phi^{query}_{ij}$ | $\displaystyle=Q^{{}^{\prime}T}_{g}G_{ij}$
    |  | (63) |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi^{query}_{ij}$ | $\displaystyle=Q^{{}^{\prime}T}_{g}G_{ij}$
    |  | (63) |'
- en: '|  | $\displaystyle\phi^{key}_{ij}$ | $\displaystyle=K^{{}^{\prime}T}_{g}G_{ij}$
    |  | (64) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\phi^{key}_{ij}$ | $\displaystyle=K^{{}^{\prime}T}_{g}G_{ij}$
    |  | (64) |'
- en: 7 Learning Geometric Representations
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 学习几何表示
- en: Apart from utilizing direct methods of enforcing geometric constraints or providing
    geometric guidance or exploiting orthogonal relations between depth and normal,
    there are some indirect ways of learning geometrically and structurally consistent
    representations. For example, features with high-level of semantic information
    is more likely to retain structural consistency of objects compared to features
    with low-level of semantic information, pseudo-label generation purely on the
    basis of geometric consistency can be utilized for self-supervision, more robust
    feature representation can be learned using suitable data-augmentation techniques,
    attaching semantic segmentation information of objects or using co-segmentation
    can also provide suitable clues for the structural consistency of objects, contrastive
    learning with positive and negative pairs can guide a model to learn better representation
    which are geometrically sharp and consistent. We discuss all such methods in this
    section.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 除了利用直接的方法来强制几何约束或提供几何指导，或利用深度和法线之间的正交关系外，还有一些间接的方法来学习几何和结构上连贯的表示。例如，与低层语义信息的特征相比，高层语义信息的特征更可能保留对象的结构一致性。基于几何一致性生成伪标签可以用于自我监督，使用合适的数据增强技术可以学习到更稳健的特征表示，附加对象的语义分割信息或使用共同分割也可以提供对象结构一致性的适当线索。对比学习通过正负对可以引导模型学习更好的表示，这些表示在几何上更清晰且一致。我们将在本节中讨论所有这些方法。
- en: 7.1 High-Level Feature Alignment Constraints
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 高层特征对齐约束
- en: In deep learning-based frameworks for depth estimation, the quality of the extracted
    features directly impacts the quality of depth estimates. The poor quality of
    extracted features can greatly impact the local as well as global structural pattern.
    One way to handle this problem is by guiding the extracted features with better
    representation from an auxiliary pre-trained network. While the integrated feature
    extraction network in the depth estimation pipeline can learn useful features,
    it still lags in learning higher-level representations compared to a network explicitly
    designed to learn deep high-level representations like, VGG [[85](#bib.bib85)],
    Inception [[86](#bib.bib86)], ResNet [[87](#bib.bib87)] etc. To enforce the feature
    alignment constraint, Johnson et al. [[88](#bib.bib88)] propose two loss functions,
    feature reconstruction loss and style reconstruction loss.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于深度学习的深度估计框架中，提取特征的质量直接影响深度估计的质量。提取特征的质量差可能会严重影响局部以及全局的结构模式。解决这一问题的一种方法是通过辅助预训练网络提供更好的表示来指导提取的特征。虽然深度估计管道中的集成特征提取网络能够学习有用的特征，但与像VGG
    [[85](#bib.bib85)]、Inception [[86](#bib.bib86)]、ResNet [[87](#bib.bib87)]等明确设计用于学习深层高层表示的网络相比，它在学习高层表示方面仍然有所不足。为了强制执行特征对齐约束，Johnson等人[[88](#bib.bib88)]提出了两种损失函数：特征重建损失和风格重建损失。
- en: 'Feature reconstruction loss, Eq. [66](#S7.E66 "In 7.1 High-Level Feature Alignment
    Constraints ‣ 7 Learning Geometric Representations ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey"), encourages the model to generate source
    features similar to the target features at various stages of the network [[88](#bib.bib88)].
    Minimizing feature reconstruction loss for early layers improves local visual
    as well as structural patterns, while minimizing it for higher layers improves
    overall structural patterns [[88](#bib.bib88)]. Feature reconstruction loss fails
    to preserve color and texture, which is handled by style reconstruction loss [[88](#bib.bib88)].'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重建损失，公式[66](#S7.E66 "在7.1高层特征对齐约束 ‣ 7 学习几何表示 ‣ 深度学习框架中的几何约束：综述")，鼓励模型在网络的各个阶段生成与目标特征相似的源特征[[88](#bib.bib88)]。对于早期层次，最小化特征重建损失可以改善局部视觉以及结构模式，而对于高层次，最小化它可以改善整体结构模式[[88](#bib.bib88)]。特征重建损失未能保留颜色和纹理，这由风格重建损失[[88](#bib.bib88)]来处理。
- en: '|  | $\displaystyle\mathcal{L}_{feature}$ | $\displaystyle=&#124;&#124;F_{target}-F_{source}&#124;&#124;_{L_{i}}$
    |  | (66) |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{feature}$ | $\displaystyle=&#124;&#124;F_{target}-F_{source}&#124;&#124;_{L_{i}}$
    |  | (66) |'
- en: '|  | $\displaystyle\mathcal{L}_{feature}$ | $\displaystyle=\frac{1}{N}\sum\left(F_{ref}-\hat{F}_{src}\right).M_{ref}$
    |  | (67) |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\mathcal{L}_{feature}$ | $\displaystyle=\frac{1}{N}\sum\left(F_{ref}-\hat{F}_{src}\right).M_{ref}$
    |  | (67) |'
- en: 'Eq. [66](#S7.E66 "In 7.1 High-Level Feature Alignment Constraints ‣ 7 Learning
    Geometric Representations ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") shows the generalized formulation of feature reconstruction loss, where
    $F_{target}$ is the target feature, $F_{source}$ is the source feature and $L_{i}$
    denotes $L_{1}$ or $L_{2}$ norm. A similar formulation is adopted by Huang et
    al. [[22](#bib.bib22)], dubbed as feature-wise loss, in an unsupervised MVS framework.
    Using a pre-trained VGG-16 network, high-level semantically rich feature is extracted
    at layers 8, 15, and 22 for both the reference $(F_{ref})$ and the source $(F_{src})$
    images. The features from the source images are warped to the reference view $(\hat{F}_{src})$
    using camera parameters and used in feature reconstruction loss as shown in Eq.
    [67](#S7.E67 "In 7.1 High-Level Feature Alignment Constraints ‣ 7 Learning Geometric
    Representations ‣ Geometric Constraints in Deep Learning Frameworks: A Survey").
    $M$ is the reference view mask to handle occlusion and $N$ is the number of source
    view images. Dong et al. [[50](#bib.bib50)] use similar formulation and extract
    feature from $8^{th}$ and $15^{th}$ layers'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '方程 [66](#S7.E66 "In 7.1 High-Level Feature Alignment Constraints ‣ 7 Learning
    Geometric Representations ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey") 展示了特征重建损失的广义公式，其中 $F_{target}$ 是目标特征，$F_{source}$ 是源特征，$L_{i}$ 表示 $L_{1}$
    或 $L_{2}$ 范数。Huang 等人 [[22](#bib.bib22)] 采用了类似的公式，称为特征级损失，用于无监督的 MVS 框架。利用预训练的
    VGG-16 网络，在层 8、15 和 22 提取高层语义丰富的特征，用于参考图像 $(F_{ref})$ 和源图像 $(F_{src})$。源图像的特征通过相机参数被变换到参考视图
    $(\hat{F}_{src})$，并用于特征重建损失，如方程 [67](#S7.E67 "In 7.1 High-Level Feature Alignment
    Constraints ‣ 7 Learning Geometric Representations ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey") 所示。$M$ 是参考视图掩码，用于处理遮挡，$N$ 是源视图图像的数量。Dong
    等人 [[50](#bib.bib50)] 使用了类似的公式，并从第 $8$ 层和第 $15$ 层提取特征。'
- en: Applying feature alignment loss helps the model extract high-level semantically
    rich features for depth estimation, it can also be used to generate geometrically
    consistent and style-conforming new RGB image, which is then used as input to
    the depth estimation network. Zhao et al. [[65](#bib.bib65)] and Xu et al. [[51](#bib.bib51)]
    use such an approach to fill generate synthetic input images that are geometrically
    consistent across views and close to the original data distribution.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 应用特征对齐损失有助于模型提取用于深度估计的高层次语义丰富特征，它还可以用于生成几何一致且符合风格的新 RGB 图像，然后将其作为输入传递给深度估计网络。Zhao
    等人 [[65](#bib.bib65)] 和 Xu 等人 [[51](#bib.bib51)] 使用这种方法生成几何上一致并接近原始数据分布的合成输入图像。
- en: Zhao et al. [[65](#bib.bib65)] generate synthetic RGB images, they apply feature
    reconstruction and style reconstruction loss at final image resolution. They learn
    bidirectional translators from source to target, $G_{s2t}$ and target to source
    $G_{t2s}$ to bridge the gap between the source domain (synthetic) $X_{s}$ and
    the target domain (real) $X_{t}$. Specifically, image $x_{s}$ is sequentially
    fed to $G_{s2t}$ and $G_{t2s}$ generating a reconstruction of $x_{s}$ and vice-versa
    for $x_{t}$. These are then compared to the original input as follows
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人 [[65](#bib.bib65)] 生成合成 RGB 图像时，在最终图像分辨率下应用特征重建和风格重建损失。他们学习从源到目标的双向翻译器
    $G_{s2t}$ 和从目标到源的 $G_{t2s}$，以弥合源领域（合成）$X_{s}$ 和目标领域（真实）$X_{t}$ 之间的差距。具体而言，图像 $x_{s}$
    被顺序输入到 $G_{s2t}$ 和 $G_{t2s}$ 生成对 $x_{s}$ 的重建，$x_{t}$ 也同样处理。然后，将这些与原始输入进行比较，如下所示：
- en: '|  | $\mathcal{L}_{Feature_{cycle}}=&#124;&#124;G_{t2s}(G_{s2t}(x_{s}))-x_{s}&#124;&#124;_{1}+&#124;&#124;G_{s2t}(G_{t2s}(x_{t}))-x_{t}&#124;&#124;_{1}$
    |  | (68) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{Feature_{cycle}}=&#124;&#124;G_{t2s}(G_{s2t}(x_{s}))-x_{s}&#124;&#124;_{1}+&#124;&#124;G_{s2t}(G_{t2s}(x_{t}))-x_{t}&#124;&#124;_{1}$
    |  | (68) |'
- en: 'While Zhao et al. [[65](#bib.bib65)] apply $\mathcal{L}_{feature_{cycle}}$
    at RGB image level, Xu et al. [[51](#bib.bib51)] use feature and style reconstruction
    loss in semi-supervised MVS framework. They use a geometry-preserving module to
    generate geometry and style-conforming RGB images using a labeled real image.
    They employ a geometry geometry-preserving module to generate unlabeled RGB images
    which are later used as input to estimate depth maps. The geometry-preserving
    module includes a spatial propagation network (SPN) with two branches - propagation
    network and guidance network. The labeled image $(I_{l})$ is used as input to
    the guidance network to generate an alternate view RGB image $(I_{g})$. This RGB
    image is used as input to the depth estimation pipeline to generate a corresponding
    depth map $D_{g}$. $D_{g}$ is then warped to the original view ($\hat{D}_{g}$)
    to compare with the ground truth depth map $D_{l}$. Eq. [69](#S7.E69 "In 7.1 High-Level
    Feature Alignment Constraints ‣ 7 Learning Geometric Representations ‣ Geometric
    Constraints in Deep Learning Frameworks: A Survey") shows the formulation of the
    loss function used in [[51](#bib.bib51)]. With this setup, they use labeled images
    to generate geometrically conforming alternate view unlabeled images and use them
    in the depth estimation pipeline without having to create new labeled data.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然赵等人[[65](#bib.bib65)]在RGB图像级别应用$\mathcal{L}_{feature_{cycle}}$，但徐等人[[51](#bib.bib51)]在半监督MVS框架中使用特征和风格重建损失。他们使用一个几何保留模块，通过标记的真实图像生成符合几何和风格的RGB图像。他们使用几何保留模块生成未标记的RGB图像，这些图像随后作为输入用于深度图估计。几何保留模块包括一个具有两个分支的空间传播网络（SPN）——传播网络和引导网络。标记的图像$(I_{l})$被用作引导网络的输入，以生成替代视图的RGB图像$(I_{g})$。该RGB图像用作深度估计管道的输入，以生成相应的深度图$D_{g}$。然后将$D_{g}$扭曲到原始视图（$\hat{D}_{g}$）以与真实深度图$D_{l}$进行比较。公式[69](#S7.E69
    "在7.1 高级特征对齐约束 ‣ 7 学习几何表示 ‣ 深度学习框架中的几何约束：综述")显示了[[51](#bib.bib51)]中使用的损失函数的公式。在这种设置下，他们使用标记的图像生成几何符合的替代视图未标记图像，并在深度估计管道中使用这些图像，而无需创建新的标记数据。
- en: '|  | $\mathcal{L}_{feature_{style}}=&#124;&#124;D_{l}-\hat{D}_{g}&#124;&#124;^{2}_{2}$
    |  | (69) |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{feature_{style}}=&#124;&#124;D_{l}-\hat{D}_{g}&#124;&#124;^{2}_{2}$
    |  | (69) |'
- en: 7.2 Pseudo-Label Generation with Cross-View Consistency
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 使用跨视图一致性的伪标签生成
- en: '![Refer to caption](img/cf15c0b989a784917123968bd709391d.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/cf15c0b989a784917123968bd709391d.png)'
- en: 'Figure 11: Pseudo-label generation method of [[21](#bib.bib21)]. SPSR is Screened
    Poisson Surface Reconstruction method [[89](#bib.bib89)]. Figure inspired from
    [[21](#bib.bib21)]'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：[[21](#bib.bib21)]的伪标签生成方法。SPSR是筛选的泊松表面重建方法[[89](#bib.bib89)]。图灵感来源于[[21](#bib.bib21)]
- en: In self-supervised MVS frameworks, one of the effective methods of applying
    geometric constraints is by generating pseudo-labels. Generating pseudo-labels
    requires application of cross-view consistency constraints, which encourages the
    MVS framework to be geometrically consistent during training and evaluation [[21](#bib.bib21),
    [47](#bib.bib47)]. Since the model learns with self-supervision, it also helps
    with the challenging task of collecting multi-view ground truth data. In this
    section, we discuss three methods of generating pseudo-labels for self-supervision,
    labels from high-resolution training images [[21](#bib.bib21)], sparse pseudo-label
    generation [[47](#bib.bib47)] and semi-dense pseudo-label generation [[47](#bib.bib47)].
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在自监督多视图立体视觉（MVS）框架中，应用几何约束的一种有效方法是生成伪标签。生成伪标签需要应用视图一致性约束，这促使MVS框架在训练和评估期间保持几何一致性[[21](#bib.bib21)，[47](#bib.bib47)]。由于模型通过自监督进行学习，这也有助于解决收集多视角真实数据的挑战。在这一部分，我们讨论了生成自监督伪标签的三种方法：来自高分辨率训练图像的标签[[21](#bib.bib21)]、稀疏伪标签生成[[47](#bib.bib47)]和半稠密伪标签生成[[47](#bib.bib47)]。
- en: 'Yang et al. [[21](#bib.bib21)] apply pseudo-label learning in four steps. First,
    they estimate the depth map based on photometric consistency, Sec. [3.1](#S3.SS1
    "3.1 Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints
    in Deep Learning Frameworks: A Survey"), using a coarse network (low-resolution
    network). With the initial pseudo-label in hand, they apply a two-step iterative
    self-training to refine these pseudo-labels, see Fig. [11](#S7.F11 "Figure 11
    ‣ 7.2 Pseudo-Label Generation with Cross-View Consistency ‣ 7 Learning Geometric
    Representations ‣ Geometric Constraints in Deep Learning Frameworks: A Survey").
    They utilize fine-network (high-resolution network) to refine the initial coarse
    pseudo-labels utilizing more discriminative features from high-resolution images.
    The fine-network estimates high-resolution labels which are then filtered with
    a cross-view depth consistency check, Sec. [3](#S3 "3 Cross-View Constraints ‣
    Geometric Constraints in Deep Learning Frameworks: A Survey"), utilizing depth
    re-projection error to measure pseudo-label consistency. Finally, the high-resolution
    filtered pseudo-labels from $N$ different views are fused using a multi-view fusion
    method. It generates a more complete point cloud of the scene. The point cloud
    is then rendered to generate cross-view geometrically consistent new pseudo-labels
    to guide the coarse network depth estimation pipeline.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '杨等人[[21](#bib.bib21)]在四个步骤中应用伪标签学习。首先，他们基于光度一致性估计深度图，参见第[3.1](#S3.SS1 "3.1
    Photometric Consistency ‣ 3 Cross-View Constraints ‣ Geometric Constraints in
    Deep Learning Frameworks: A Survey")节，使用一个粗网络（低分辨率网络）。有了初始伪标签后，他们应用了两步迭代自训练来优化这些伪标签，见图[11](#S7.F11
    "Figure 11 ‣ 7.2 Pseudo-Label Generation with Cross-View Consistency ‣ 7 Learning
    Geometric Representations ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")。他们利用精细网络（高分辨率网络）来优化初始粗伪标签，利用来自高分辨率图像的更多区分特征。精细网络估计高分辨率标签，然后通过交叉视图深度一致性检查来过滤这些标签，参见第[3](#S3
    "3 Cross-View Constraints ‣ Geometric Constraints in Deep Learning Frameworks:
    A Survey")节，利用深度重投影误差来测量伪标签一致性。最后，来自$N$个不同视角的高分辨率过滤伪标签通过多视角融合方法融合，生成场景的更完整点云。然后渲染点云生成交叉视图几何一致的新伪标签，以指导粗网络深度估计管道。'
- en: Liu et al. [[47](#bib.bib47)] use two geometric prior-guided pseudo-label generation
    methods, sparse and semi-dense pseudo-label. For sparse label generation, they
    use a pre-trained Structure from Motion framework (SfM) [[90](#bib.bib90)] to
    generate a sparse point cloud. This sparse point cloud is then projected to generate
    sparse depth pseudo-labels. Since, the sparse point cloud can provide very limited
    supervision, they use a traditional MVS framework that utilizes geometric and
    photometric consistency to estimate preliminary depth maps, like COLMAP [[91](#bib.bib91)].
    The preliminary depth map then undergoes cross-view geometric consistency checking
    process to eliminate any outliers. This filtered depth map is then used as a final
    pseudo-label for learning.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人[[47](#bib.bib47)]使用了两种几何先验引导的伪标签生成方法：稀疏伪标签和半稠密伪标签。对于稀疏标签生成，他们使用一个预训练的运动结构框架（SfM）[[90](#bib.bib90)]生成稀疏点云。然后将这些稀疏点云投影以生成稀疏深度伪标签。由于稀疏点云只能提供非常有限的监督，他们使用传统的MVS框架，利用几何和光度一致性来估计初步深度图，比如COLMAP[[91](#bib.bib91)]。初步深度图经过交叉视图几何一致性检查过程以消除任何离群点。这种过滤后的深度图随后被用作学习的最终伪标签。
- en: 7.3 Data-Augmentation for Geometric Robustness
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3 数据增强用于几何鲁棒性
- en: Deep-learning frameworks can always do better with more data [[92](#bib.bib92)],
    but collecting data for stereo or MVS setup is a difficult task. Applying data
    augmentation to these frameworks naturally makes sense, but it is not as easy
    to implement. The natural color fluctuation, occlusion, and geometric distortions
    in augmented images disturbs the color constancy of images, affecting the effective
    feature matching and hence, the performance of the whole depth estimation pipeline
    [[18](#bib.bib18)]. Because of these limitations, it has seldom been applied in
    either supervised [[15](#bib.bib15), [13](#bib.bib13), [14](#bib.bib14)] or unsupervised
    [[48](#bib.bib48), [54](#bib.bib54), [22](#bib.bib22)] MVS methods.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习框架在拥有更多数据时总能表现得更好[[92](#bib.bib92)]，但收集用于立体或多视角立体（MVS）设置的数据是一项困难的任务。将数据增强应用于这些框架自然是有意义的，但实施起来并不容易。增强图像中的自然色彩波动、遮挡和几何失真干扰了图像的色彩恒定性，影响了有效特征匹配，从而影响整个深度估计管道的性能[[18](#bib.bib18)]。由于这些限制，数据增强在有监督[[15](#bib.bib15),
    [13](#bib.bib13), [14](#bib.bib14)]或无监督[[48](#bib.bib48), [54](#bib.bib54), [22](#bib.bib22)]
    MVS 方法中很少被应用。
- en: Keeping these limitations in mind, Garg et al. [[19](#bib.bib19)] use three
    data-augmentation techniques in an unsupervised stereo depth estimation problem.
    They use color change – scalar multiplication of color channels by a factor $c\in[0.9,1.1]$,
    scale and crop – the input image is scaled by $2\times$ factor and then randomly
    cropped to match the original input size and left-right flip – wherein the left
    and right images are flipped horizontally and swapped to get a new training pair.
    These three simple augmentations lead to $8\times$ increase in data and improved
    localization of object edges in the depth estimates.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 牢记这些限制，Garg 等人 [[19](#bib.bib19)] 在无监督立体深度估计问题中使用了三种数据增强技术。他们使用了颜色变化——通过因子 $c\in[0.9,1.1]$
    对颜色通道进行标量乘法，缩放和裁剪——将输入图像按 $2\times$ 因子缩放，然后随机裁剪以匹配原始输入大小，以及左右翻转——将左右图像水平翻转并交换以获得新的训练对。这三种简单的增强方法导致数据量增加了
    $8\times$，并改善了深度估计中物体边缘的定位。
- en: '|  | $\mathcal{L}_{augmentation}=\frac{1}{M}\sum&#124;&#124;\hat{D}_{aug}-\hat{D}_{non-aug}&#124;&#124;_{2}\odot
    M_{non-aug}$ |  | (70) |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{augmentation}=\frac{1}{M}\sum&#124;&#124;\hat{D}_{aug}-\hat{D}_{non-aug}&#124;&#124;_{2}\odot
    M_{non-aug}$ |  | (70) |'
- en: 'Xu et al. [[18](#bib.bib18)] propose using data augmentation as a regularization
    technique. Instead of optimizing the regular loss function with ground truth depth
    estimates, they propose data-augmentation consistency loss by contrasting data
    augmentation depth estimates with non-augmented depth estimates. Specifically,
    given the non-augmented input images $I_{non-aug}$ and augmented input images
    $I_{aug}$ of the same view, the difference between the estimated augmented ($\hat{D}_{aug}$)
    and non-augmented ($\hat{D}_{non-aug}$) depth maps are minimized. Eq. [70](#S7.E70
    "In 7.3 Data-Augmentation for Geometric Robustness ‣ 7 Learning Geometric Representations
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey") shows the mathematical
    formulation of data augmentation consistency loss, $\mathcal{L}_{augmentation}$,
    used in [[18](#bib.bib18)]. Where $M_{non-aug}$ denotes an unoccluded mask under
    data-augmentation transformation. Xu et al. [[18](#bib.bib18)] cross-view masking
    augmentation to simulate occlusion hallucination in multi-view situations by randomly
    generating a binary crop mask to block out some regions on reference view. The
    mask is then projected to other views to mask out corresponding areas in source
    views. They also used gamma correction to adjust the illuminance of images, random
    color jitter, random blur and random noise addition in the input images. All these
    data-augmentation methods do not affect camera parameters. In Sec. TODO, we present
    a potential set of transformations that can be utilized for data augmentation
    without impacting the camera parameters in MVS frameworks.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: Xu 等人 [[18](#bib.bib18)] 提出了将数据增强作为正则化技术。与其优化使用真实深度估计的常规损失函数，他们提出通过对比数据增强深度估计与非增强深度估计来实现数据增强一致性损失。具体来说，给定同一视角的非增强输入图像
    $I_{non-aug}$ 和增强输入图像 $I_{aug}$，最小化估计的增强 ($\hat{D}_{aug}$) 和非增强 ($\hat{D}_{non-aug}$)
    深度图之间的差异。公式 [70](#S7.E70 "在 7.3 数据增强以增强几何鲁棒性 ‣ 7 学习几何表示 ‣ 深度学习框架中的几何约束：综述") 显示了数据增强一致性损失
    $\mathcal{L}_{augmentation}$ 的数学公式，该公式在 [[18](#bib.bib18)] 中使用。其中 $M_{non-aug}$
    表示数据增强变换下的未遮挡掩膜。Xu 等人 [[18](#bib.bib18)] 通过随机生成二进制裁剪掩膜来模拟多视图情况下的遮挡幻觉，从而进行交视遮挡增强。掩膜随后被投影到其他视图中，以遮挡源视图中对应的区域。他们还使用了伽玛校正来调整图像的亮度，随机颜色抖动、随机模糊和随机噪声添加到输入图像中。所有这些数据增强方法都不会影响相机参数。在
    Sec. TODO 中，我们展示了一组潜在的转换，这些转换可以用于数据增强而不影响 MVS 框架中的相机参数。
- en: 7.4 Semantic Information for Structural Consistency
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4 结构一致性的语义信息
- en: Humans can perform stereophotogrammetry well in ambiguous areas by exploiting
    more cues such as global perception of foreground and background, relative scaling,
    and semantic consistency of individual objects. Deep learning-based frameworks,
    operating on color constancy hypothesis [[18](#bib.bib18)], generally provide
    a superior performance as compared to traditional MVS algorithms, but both methods
    fail at featureless regions or at any such regions with different lighting conditions,
    reflections, or noises, color constancy ambiguity. Direct application of geometric
    and photometric constraints in such regions is not helpful, but high-level semantic
    segmentation clues can help these models in such regions. Semantic segmentation
    clues for a given scene can provide abstract matching clues and also act as structural
    priors for depth estimation [[18](#bib.bib18)]. In this section, we explore such
    depth estimation methods that include semantic clues in their pipeline.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 人类可以通过利用更多的线索，如前景和背景的全球感知、相对缩放和单个对象的语义一致性，在模糊区域有效地执行立体光测量。基于深度学习的框架，操作在颜色恒常性假设上[[18](#bib.bib18)]，通常提供比传统多视角立体算法更优越的性能，但两种方法在无特征区域或任何具有不同光照条件、反射或噪声、颜色恒常性歧义的区域都无效。在这些区域直接应用几何和光度约束没有帮助，但高级语义分割线索可以帮助这些模型。在给定场景中的语义分割线索可以提供抽象匹配线索，也可以作为深度估计的结构先验[[18](#bib.bib18)]。在本节中，我们深入探讨了在其管道中包含语义线索的深度估计方法。
- en: Inspired by Cheng et al. [[24](#bib.bib24)], which incorporate semantic segmentation
    information to learn optical flow from video, Yang et al. [[29](#bib.bib29)] incorporate
    semantic feature embedding and regularize semantic cues as the loss term to improve
    disparity learning in stereo problem. Semantic feature embedding is a concatenation
    of three types of features, left-image features, left-right correlation features,
    and left-image semantic features. In addition to image and correlation features,
    semantic features provide more consistent representations of featureless regions,
    which helps solve the disparity problem. They also regularize the semantic cues
    loss term by warping the right image segmentation map to the left view and comparing
    it with the left image segmentation ground truth. Minimizing the semantic cues
    loss term improves its consistency in the end-to-end learning process. Dovesi
    et al. [[28](#bib.bib28)] also employ semantic segmentation networks in coarse-to-fine
    design and utilize additional information in stereo-matching problems.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 受到 Cheng 等人[[24](#bib.bib24)] 的启发，他们结合语义分割信息以从视频中学习光流，Yang 等人[[29](#bib.bib29)]
    将语义特征嵌入并将语义线索正规化为损失项，以改进立体问题中的视差学习。语义特征嵌入是三种特征的串联：左图像特征、左-右关联特征和左图像语义特征。除了图像和关联特征外，语义特征提供了对无特征区域的更一致的表示，这有助于解决视差问题。他们还通过将右图像分割图变形到左视图并与左图像分割地面真值进行比较来正规化语义线索损失项。最小化语义线索损失项在端到端学习过程中提高了其一致性。Dovesi
    等人[[28](#bib.bib28)] 也在粗到细的设计中使用语义分割网络，并在立体匹配问题中利用额外的信息。
- en: Another way of utilizing semantic information for geometric and structural consistency
    is through co-segmentation. Co-segmentation method aims to predict foreground
    pixels of the common object to give an image collection [[93](#bib.bib93)]. Inspired
    by Casser et al. [[94](#bib.bib94)], which applied co-segmentation to learn semantic
    information in unsupervised monocular depth ego-motion learning problem, Xu et
    al. [[18](#bib.bib18)] apply co-segmentation on multi-view pairs to exploit the
    common semantics. They adopt non-negative matrix factorization (NMF) [[95](#bib.bib95)]
    to excavate the common semantic clusters among multi-view images during the learning
    process. NMF is applied to the activations of a pre-trained layer [[96](#bib.bib96)]
    to find semantic correspondences across images. We refer to Ding et al. [[95](#bib.bib95)]
    for more details on NMF. The consistency of the co-segmentation map can be expanded
    across multiple views by warping it to other views. The semantic consistency loss
    is calculated as per pixel cross-entropy loss between the warped segmentation
    map ($\hat{S}_{i}$ and the ground truth labels converted from the reference segmentation
    map $S$ as follows
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种利用语义信息实现几何和结构一致性的方法是通过共分割。共分割方法旨在预测图像集合中公共对象的前景像素[[93](#bib.bib93)]。受 Casser
    等人[[94](#bib.bib94)] 的启发，后者将共分割应用于无监督单目深度自运动学习问题，Xu 等人[[18](#bib.bib18)] 将共分割应用于多视角对，以挖掘共同的语义。他们采用非负矩阵分解
    (NMF) [[95](#bib.bib95)] 来挖掘多视图图像中的共同语义簇。在学习过程中，NMF 被应用于预训练层的激活值[[96](#bib.bib96)]，以找到图像之间的语义对应关系。有关
    NMF 的更多细节，请参见 Ding 等人[[95](#bib.bib95)]。共分割图的连贯性可以通过将其变形到其他视角来扩展。语义一致性损失按像素交叉熵损失计算，公式如下：
- en: '|  | $\mathcal{L}_{semantic}=-\sum^{N}_{i=2}\left[\frac{1}{&#124;&#124;M_{i}&#124;&#124;_{1}}\sum^{HW}_{j=1}f(S_{j})log(\hat{s}_{i,j})M_{i,j}\right]$
    |  | (71) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '|  | $\mathcal{L}_{semantic}=-\sum^{N}_{i=2}\left[\frac{1}{&#124;&#124;M_{i}&#124;&#124;_{1}}\sum^{HW}_{j=1}f(S_{j})log(\hat{s}_{i,j})M_{i,j}\right]$
    |  | (71) |'
- en: where $f(S_{j})=onehot(arg\,max(S_{j}))$ and $M_{i}$ is the binary mask indicating
    valid pixels from the $i^{th}$ view to the reference view.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(S_{j})=onehot(arg\,max(S_{j}))$ 和 $M_{i}$ 是二值掩膜，表示从第 $i^{th}$ 视角到参考视角的有效像素。
- en: 7.5 Geometric Representation Learning with Contrastive Loss
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5 使用对比损失进行几何表示学习
- en: Contrastive learning [[97](#bib.bib97)] learns object representations by enforcing
    the attractive force to positive pair and the repulsive pair to negative pair
    [[98](#bib.bib98)]. This form of representation learning has not been explored
    much in a depth estimation problem. There is only a handful of research works
    that use contrastive learning for depth estimation [[99](#bib.bib99), [100](#bib.bib100),
    [98](#bib.bib98)]. Fan et al. [[100](#bib.bib100)] use contrastive learning to
    pay more attention to depth distribution and improve the overall depth estimation
    process by adopting a non-overlapping window-based contrastive learning approach.
    Lee et al. [[99](#bib.bib99)] use contrastive learning to disentangle the camera
    and object motion. While these methods use contrastive learning for estimating
    depth maps none of them use contrastive learning to promote the geometric representation
    of objects.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 对比学习[[97](#bib.bib97)]通过对正样本对施加吸引力，对负样本对施加排斥力来学习对象表示[[98](#bib.bib98)]。这种表示学习形式在深度估计问题中尚未深入探索。目前只有少数研究工作使用对比学习进行深度估计[[99](#bib.bib99),
    [100](#bib.bib100), [98](#bib.bib98)]。Fan等人[[100](#bib.bib100)]利用对比学习更多地关注深度分布，并通过采用非重叠窗口的对比学习方法来改善整体深度估计过程。Lee等人[[99](#bib.bib99)]使用对比学习来解开相机和对象的运动。虽然这些方法使用对比学习来估计深度图，但没有一个方法利用对比学习来促进对象的几何表示。
- en: '![Refer to caption](img/9f6bc86c22940d034c8abd8b395f5958.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/9f6bc86c22940d034c8abd8b395f5958.png)'
- en: 'Figure 12: Contrastive learning approach to promote geometric representation.
    Figure reused from [[98](#bib.bib98)]'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12：对比学习方法以促进几何表示。图像重用自[[98](#bib.bib98)]
- en: 'Shim and Kim [[98](#bib.bib98)] focus on learning geometric representation
    for depth estimation problems using contrastive learning. They utilize Sobel kernel
    and Canny edge binary mask [[101](#bib.bib101)] to generate gradient fields of
    the image as the positive and negative pairs, see Fig. [12](#S7.F12 "Figure 12
    ‣ 7.5 Geometric Representation Learning with Contrastive Loss ‣ 7 Learning Geometric
    Representations ‣ Geometric Constraints in Deep Learning Frameworks: A Survey").
    To estimate the gradient field $G$ ($+$ for positive and $-$ for negative examples)
    of input image pairs $\mathcal{I}_{q}$ (query image) and $\mathcal{I}_{-}$ (other
    image), they modify the Canny detector to extract the magnitude of the dominant
    gradient as well as its location to adjust the gradient field according to its
    edge dominance. The adopted process can be mathematically formulated as shown
    in Eq. [72](#S7.E72 "In 7.5 Geometric Representation Learning with Contrastive
    Loss ‣ 7 Learning Geometric Representations ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey"), where $||E||$ and $B_{Canny}$ denote the magnitude of
    the gradient from the Sobel operator and the binary mask of the Canny detector.
    $\odot$ is element-wise multiplication.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 'Shim 和 Kim [[98](#bib.bib98)] 专注于使用对比学习进行深度估计问题的几何表示学习。他们利用 Sobel 核心和 Canny
    边缘二值掩模 [[101](#bib.bib101)] 生成图像的梯度场作为正负对，请参见图 [12](#S7.F12 "Figure 12 ‣ 7.5 Geometric
    Representation Learning with Contrastive Loss ‣ 7 Learning Geometric Representations
    ‣ Geometric Constraints in Deep Learning Frameworks: A Survey")。为了估计输入图像对 $\mathcal{I}_{q}$（查询图像）和
    $\mathcal{I}_{-}$（其他图像）的梯度场 $G$（正例用 $+$，负例用 $-$），他们修改了 Canny 检测器，以提取主导梯度的大小及其位置，并根据边缘主导性调整梯度场。所采用的过程可以数学上公式化，如公式
    [72](#S7.E72 "In 7.5 Geometric Representation Learning with Contrastive Loss ‣
    7 Learning Geometric Representations ‣ Geometric Constraints in Deep Learning
    Frameworks: A Survey") 所示，其中 $||E||$ 和 $B_{Canny}$ 表示 Sobel 操作符的梯度大小和 Canny 检测器的二值掩模。$\odot$
    是逐元素乘法。'
- en: '|  | <math   alttext="\begin{split}\mathcal{I}&amp;\in\mathbb{R}^{h\times w},\triangledown\mathcal{I}_{x},\triangledown\mathcal{I}_{y}\in\mathbb{R}^{h\times
    w},\\ &#124;&#124;E&#124;&#124;&amp;=\sqrt{\mathcal{I}^{2}_{x}+\mathcal{I}^{2}_{y}},\\'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | <math   alttext="\begin{split}\mathcal{I}&amp;\in\mathbb{R}^{h\times w},\triangledown\mathcal{I}_{x},\triangledown\mathcal{I}_{y}\in\mathbb{R}^{h\times
    w},\\ &#124;&#124;E&#124;&#124;&amp;=\sqrt{\mathcal{I}^{2}_{x}+\mathcal{I}^{2}_{y}},\\'
- en: G&amp;=B_{Canny}\odot&#124;&#124;E&#124;&#124;\end{split}" display="block"><semantics
    ><mtable columnspacing="0pt" displaystyle="true" rowspacing="0pt" ><mtr ><mtd
    columnalign="right" ><mi >ℐ</mi></mtd><mtd columnalign="left" ><mrow ><mrow ><mrow
    ><mo >∈</mo><mrow ><msup ><mi >ℝ</mi><mrow  ><mi >h</mi><mo lspace="0.222em" rspace="0.222em"  >×</mo><mi
    >w</mi></mrow></msup><mo >,</mo><mrow ><mi mathvariant="normal"  >▽</mi><mo lspace="0em"
    rspace="0em"  >​</mo><msub ><mi >ℐ</mi><mi >x</mi></msub></mrow></mrow></mrow><mo
    >,</mo><mrow ><mrow ><mi mathvariant="normal" >▽</mi><mo lspace="0em" rspace="0em"
    >​</mo><msub ><mi >ℐ</mi><mi >y</mi></msub></mrow><mo >∈</mo><msup ><mi >ℝ</mi><mrow
    ><mi >h</mi><mo lspace="0.222em" rspace="0.222em" >×</mo><mi >w</mi></mrow></msup></mrow></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="right" ><mrow ><mo stretchy="false"
    >‖</mo><mi  >E</mi><mo stretchy="false"  >‖</mo></mrow></mtd><mtd columnalign="left"
    ><mrow ><mrow ><mo  >=</mo><msqrt ><mrow ><msubsup ><mi >ℐ</mi><mi >x</mi><mn
    >2</mn></msubsup><mo >+</mo><msubsup ><mi >ℐ</mi><mi >y</mi><mn >2</mn></msubsup></mrow></msqrt></mrow><mo
    >,</mo></mrow></mtd></mtr><mtr ><mtd  columnalign="right" ><mi >G</mi></mtd><mtd
    columnalign="left" ><mrow ><mo >=</mo><mrow ><msub ><mi >B</mi><mrow ><mi  >C</mi><mo
    lspace="0em" rspace="0em"  >​</mo><mi >a</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi >n</mi><mo lspace="0em" rspace="0em"  >​</mo><mi
    >y</mi></mrow></msub><mo lspace="0.222em" rspace="0.222em"  >⊙</mo><mrow ><mo
    stretchy="false" >‖</mo><mi  >E</mi><mo stretchy="false"  >‖</mo></mrow></mrow></mrow></mtd></mtr></mtable><annotation-xml
    encoding="MathML-Content" ><apply ><csymbol cd="ambiguous" >formulae-sequence</csymbol><apply
    ><ci >ℐ</ci><list ><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci >ℝ</ci><apply  ><ci
    >ℎ</ci><ci >𝑤</ci></apply></apply><apply ><ci >▽</ci><apply ><csymbol cd="ambiguous"
    >subscript</csymbol><ci >ℐ</ci><ci  >𝑥</ci></apply></apply></list></apply><apply
    ><csymbol cd="ambiguous"  >formulae-sequence</csymbol><apply ><apply ><ci >▽</ci><apply
    ><csymbol cd="ambiguous"  >subscript</csymbol><ci >ℐ</ci><ci >𝑦</ci></apply></apply><apply
    ><csymbol cd="ambiguous"  >superscript</csymbol><ci >ℝ</ci><apply ><ci >ℎ</ci><ci
    >𝑤</ci></apply></apply></apply><apply ><csymbol cd="ambiguous"  >formulae-sequence</csymbol><apply
    ><apply ><csymbol cd="latexml"  >norm</csymbol><ci >𝐸</ci></apply><apply ><apply
    ><apply ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous"
    >superscript</csymbol><ci >ℐ</ci><cn type="integer" >2</cn></apply><ci >𝑥</ci></apply><apply
    ><csymbol cd="ambiguous" >subscript</csymbol><apply ><csymbol cd="ambiguous" >superscript</csymbol><ci
    >ℐ</ci><cn type="integer" >2</cn></apply><ci >𝑦</ci></apply></apply></apply></apply><apply
    ><ci >𝐺</ci><apply ><csymbol cd="latexml"  >direct-product</csymbol><apply ><csymbol
    cd="ambiguous"  >subscript</csymbol><ci >𝐵</ci><apply ><ci >𝐶</ci><ci >𝑎</ci><ci
    >𝑛</ci><ci >𝑛</ci><ci >𝑦</ci></apply></apply><apply ><csymbol cd="latexml" >norm</csymbol><ci
    >𝐸</ci></apply></apply></apply></apply></apply></apply></annotation-xml><annotation
    encoding="application/x-tex" >\begin{split}\mathcal{I}&\in\mathbb{R}^{h\times
    w},\triangledown\mathcal{I}_{x},\triangledown\mathcal{I}_{y}\in\mathbb{R}^{h\times
    w},\\ &#124;&#124;E&#124;&#124;&=\sqrt{\mathcal{I}^{2}_{x}+\mathcal{I}^{2}_{y}},\\
    G&=B_{Canny}\odot&#124;&#124;E&#124;&#124;\end{split}</annotation></semantics></math>
    |  | (72) |
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{I} \in \mathbb{R}^{h \times w}, \nabla \mathcal{I}_{x}, \nabla \mathcal{I}_{
- en: The network is pre-trained with contrastive loss to learn the geometric representation
    $\mathcal{Z}$ of the images. This learned representation is further compressed
    by a 2-layer fully connected network with ReLU non-linear activation to a feature
    space $\mathcal{H}$. The projected latent vector $h$ of the positive and the negative
    pairs are used to estimate the contrastive loss.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通过对比损失进行预训练，以学习图像的几何表示$\mathcal{Z}$。这个学习到的表示进一步通过一个具有ReLU非线性激活的2层全连接网络压缩到特征空间$\mathcal{H}$。正负对的投影潜在向量$h$用于估计对比损失。
- en: 8 Conclusion
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 结论
- en: The instrumental progress in deep learning technologies has immensely benefited
    the depth estimation frameworks. It has enabled the extraction of high-level representations
    from input images for enhanced stereo matching. However, it has also limited the
    use of modeling explicit photometric and geometric constraints in the learning
    process. Most supervised stereo and MVS methods focus on better feature extraction,
    and enhanced feature matching through attention mechanism but apply a plane-sweep
    algorithm as the only geometric constraint. They largely depend on the quality
    of ground truth to learn about geometric and structural consistency in the learning
    process. In this review, we have comprehensively reviewed geometric concepts in
    depth estimation and its closely related domains that can be coupled with deep
    learning frameworks to enforce geometric and structural consistency in the learning
    process. explicitly modeling geometric constraints, along with the supervision
    signal, can enforce structural reasoning, occlusion reasoning, and cross-view
    consistency in a depth estimation framework. We believe this review will provide
    a good reference for readers and researchers to explore the integration of geometric
    constraints in deep learning frameworks.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术的工具性进展极大地推动了深度估计框架的发展。它使得从输入图像中提取高层次表示成为可能，从而增强了立体匹配。然而，这也限制了在学习过程中建模显式光度和几何约束的使用。大多数监督立体视觉和MVS方法侧重于更好的特征提取和通过注意机制增强特征匹配，但将平面扫描算法作为唯一的几何约束。它们在学习过程中大多依赖于地面真相的质量来学习几何和结构一致性。在这篇综述中，我们全面回顾了深度估计中的几何概念及其密切相关的领域，这些领域可以与深度学习框架结合，以在学习过程中强制实施几何和结构一致性。显式建模几何约束以及监督信号可以在深度估计框架中强制进行结构推理、遮挡推理和视图间一致性。我们相信这篇综述将为读者和研究人员提供一个很好的参考，以探索在深度学习框架中集成几何约束。
- en: References
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview
    stereopsis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 32:1362–1376,
    2010.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] 古川泰和让·庞斯. 精确、密集且鲁棒的多视图立体视觉. 《IEEE模式分析与机器智能学报》，32:1362–1376，2010年。'
- en: '[2] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel
    multiview stereopsis by surface normal diffusion. In 2015 IEEE International Conference
    on Computer Vision (ICCV), pages 873–881, 2015.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] 西尔瓦诺·加利亚尼、卡特琳·拉辛格和孔拉德·辛德勒. 通过表面法线扩散进行大规模并行多视图立体视觉. 见于2015年IEEE国际计算机视觉会议（ICCV），第873–881页，2015年。'
- en: '[3] Engin Tola, Christoph Strecha, and Pascal Fua. Efficient large scale multi-view
    stereo for ultra high resolution image sets. Machine Vision and Applications,
    23, 09 2011.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] 恩金·托拉、克里斯托夫·斯特雷查和帕斯卡尔·弗亚. 高效的大规模多视图立体视觉用于超高分辨率图像集. 《机器视觉与应用》，23，2011年9月。'
- en: '[4] Johannes L. Schönberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys.
    Pixelwise view selection for unstructured multi-view stereo. In Bastian Leibe,
    Jiri Matas, Nicu Sebe, and Max Welling, editors, Computer Vision – ECCV 2016,
    pages 501–518, Cham, 2016\. Springer International Publishing.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] 约翰内斯·L·舍恩贝格、恩梁·郑、扬·迈克尔·弗拉姆和马克·波莱费斯. 针对无结构多视图立体视觉的逐像素视图选择. 见于巴斯蒂安·莱布、吉里·马塔斯、尼库·塞贝和马克斯·韦林主编的《计算机视觉
    – ECCV 2016》，第501–518页，尚，2016年。斯普林格国际出版公司。'
- en: '[5] Yuri Boykov, Olga Veksler, and Ramin Zabih. Fast approximate energy minimization
    via graph cuts. IEEE Transactions on pattern analysis and machine intelligence,
    23:1222–1239, 2001.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] 尤里·博伊科夫、奥尔加·维克斯勒和拉敏·扎比赫. 通过图割进行快速近似能量最小化. 《IEEE模式分析与机器智能学报》，23:1222–1239，2001年。'
- en: '[6] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer
    vision. Cambridge university press, 2003.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] 理查德·哈特利和安德鲁·齐瑟曼. 《计算机视觉中的多视图几何学》。剑桥大学出版社，2003年。'
- en: '[7] Quang-Tuan Luong and OD Faugeras. The geometry of multiple images. MIT
    Press, Boston, 2(3):4–5, 2001.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] 阮光全和OD·福热拉斯. 多图像的几何学. MIT出版社，波士顿，2(3):4–5，2001年。'
- en: '[8] Richard Szeliski. Computer vision: algorithms and applications. Springer
    Nature, 2022.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] 理查德·斯泽利斯基。计算机视觉：算法与应用。Springer Nature，2022年。'
- en: '[9] Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. Deep learning. nature,
    521(7553):436–444, 2015.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] 杨·勒库恩、约书亚·本吉奥和杰弗里·辛顿。深度学习。《自然》，521(7553):436–444，2015年。'
- en: '[10] Yann LeCun, Yoshua Bengio, et al. Convolutional networks for images, speech,
    and time series. The handbook of brain theory and neural networks, 3361(10):1995,
    1995.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] 杨·勒库恩、约书亚·本吉奥等。用于图像、语音和时间序列的卷积网络。《大脑理论与神经网络手册》，3361(10):1995，1995年。'
- en: '[11] Pankaj Malhotra, Lovekesh Vig, Gautam Shroff, Puneet Agarwal, et al. Long
    short term memory networks for anomaly detection in time series. In Esann, volume
    2015, page 89, 2015.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] 潘卡吉·马尔霍特拉、洛维克什·维格、戈塔姆·什罗夫、普尼特·阿格瓦尔等。用于时间序列异常检测的长短期记忆网络。发表于 Esann，第 2015
    卷，页面 89，2015年。'
- en: '[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn,
    Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold,
    Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition
    at scale. arXiv preprint arXiv:2010.11929, 2020.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] 阿列克谢·多索维茨基、卢卡斯·贝耶、亚历山大·科列斯尼科夫、迪尔克·魏森博恩、肖华·翟、托马斯·安特尔蒂纳、穆斯塔法·德赫赫尼、马蒂亚斯·门德尔、乔治·海戈尔德、希尔万·吉利等。图像值
    16x16 个单词：大规模图像识别的变压器。arXiv 预印本 arXiv:2010.11929，2020年。'
- en: '[13] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet: Depth
    inference for unstructured multi-view stereo. In Proceedings of the European conference
    on computer vision (ECCV), pages 767–783, 2018.'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] 姚瑶、罗子欣、李诗伟、方天和全龙。MVSNet：非结构化多视图立体的深度推断。发表于欧洲计算机视觉会议 (ECCV) 论文集，页面 767–783，2018年。'
- en: '[14] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping
    Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching.
    In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    pages 2495–2504, 2020.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] 顾晓东、范志文、朱思瑜、戴作卓、谭飞通和谭平。高分辨率多视图立体和立体匹配的级联代价体。发表于 IEEE/CVF 计算机视觉与模式识别会议论文集，页面
    2495–2504，2020年。'
- en: '[15] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang
    Wang, and Xiao Liu. Transmvsnet: Global context-aware multi-view stereo network
    with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 8585–8594, 2022.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] 丁逸康、袁文韬、朱青田、张浩天、刘向月、王元江和刘潇。Transmvsnet: 基于变压器的全球上下文感知多视图立体网络。发表于 IEEE/CVF
    计算机视觉与模式识别会议论文集，页面 8585–8594，2022年。'
- en: '[16] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy,
    Abraham Bachrach, and Adam Bry. End-to-end learning of geometry and context for
    deep stereo regression. In Proceedings of the IEEE international conference on
    computer vision, pages 66–75, 2017.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] 亚历克斯·肯德尔、哈伊克·马尔蒂罗斯扬、苏米特罗·达斯古普塔、彼得·亨利、瑞安·肯尼迪、亚伯拉罕·巴赫拉赫和亚当·布赖。深度立体回归的几何与上下文端到端学习。发表于
    IEEE 国际计算机视觉会议论文集，页面 66–75，2017年。'
- en: '[17] Robert T Collins. A space-sweep approach to true multi-image matching.
    In Proceedings CVPR IEEE computer society conference on computer vision and pattern
    recognition, pages 358–363\. Ieee, 1996.'
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] 罗伯特·T·柯林斯。基于空间扫掠的真实多图像匹配方法。发表于 CVPR IEEE 计算机学会计算机视觉与模式识别会议论文集，页面 358–363。IEEE，1996年。'
- en: '[18] Hongbin Xu, Zhipeng Zhou, Yu Qiao, Wenxiong Kang, and Qiuxia Wu. Self-supervised
    multi-view stereo via effective co-segmentation and data-augmentation. In Proceedings
    of the AAAI Conference on Artificial Intelligence, volume 35, pages 3030–3038,
    2021.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] 徐洪斌、周志鹏、游巧、康文雄和吴秋霞。自监督多视图立体通过有效的共同分割和数据增强。发表于 AAAI 人工智能会议论文集，第 35 卷，页面
    3030–3038，2021年。'
- en: '[19] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid. Unsupervised
    cnn for single view depth estimation: Geometry to the rescue. In Computer Vision–ECCV
    2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
    Proceedings, Part VIII 14, pages 740–756\. Springer, 2016.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] 拉维·加尔格、维贾伊·库马尔·BG、古斯塔沃·卡内罗和伊恩·里德。用于单视图深度估计的无监督 CNN：几何的拯救。发表于计算机视觉–ECCV
    2016：第十四届欧洲会议，荷兰阿姆斯特丹，2016年10月11-14日，论文集，第八部分 14，页面 740–756。Springer，2016年。'
- en: '[20] Zhenheng Yang, Peng Wang, Wei Xu, Liang Zhao, and Ramakant Nevatia. Unsupervised
    learning of geometry with edge-aware depth-normal consistency. arXiv preprint
    arXiv:1711.03665, 2017.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] 杨振恒、王鹏、徐伟、赵亮和拉马坎特·内瓦蒂亚。基于边缘感知深度-法线一致性的几何无监督学习。arXiv 预印本 arXiv:1711.03665，2017年。'
- en: '[21] Jiayu Yang, Jose M Alvarez, and Miaomiao Liu. Self-supervised learning
    of depth inference for multi-view stereo. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 7526–7534, 2021.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] Jiayu Yang, Jose M Alvarez 和 Miaomiao Liu. 多视角立体的深度推断自监督学习。载于《IEEE/CVF计算机视觉与模式识别会议论文集》，页码
    7526–7534，2021年。'
- en: '[22] Baichuan Huang, Hongwei Yi, Can Huang, Yijia He, Jingbin Liu, and Xiao
    Liu. M3vsnet: Unsupervised multi-metric multi-view stereo network. In 2021 IEEE
    International Conference on Image Processing (ICIP), pages 3163–3167\. IEEE, 2021.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] Baichuan Huang, Hongwei Yi, Can Huang, Yijia He, Jingbin Liu 和 Xiao Liu.
    M3vsnet: 无监督多度量多视角立体网络。载于2021年IEEE国际图像处理会议（ICIP），页码 3163–3167。IEEE，2021年。'
- en: '[23] Yuhua Chen, Cordelia Schmid, and Cristian Sminchisescu. Self-supervised
    learning with geometric constraints in monocular video: Connecting flow, depth,
    and camera. In Proceedings of the IEEE/CVF International Conference on Computer
    Vision, pages 7063–7072, 2019.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] Yuhua Chen, Cordelia Schmid 和 Cristian Sminchisescu. 使用几何约束进行自监督学习的单目视频：连接光流、深度和相机。载于《IEEE/CVF国际计算机视觉会议论文集》，页码
    7063–7072，2019年。'
- en: '[24] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang, and Ming-Hsuan Yang. Segflow:
    Joint learning for video object segmentation and optical flow. In Proceedings
    of the IEEE international conference on computer vision, pages 686–695, 2017.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] Jingchun Cheng, Yi-Hsuan Tsai, Shengjin Wang 和 Ming-Hsuan Yang. Segflow:
    视频对象分割与光流的联合学习。载于《IEEE国际计算机视觉会议论文集》，页码 686–695，2017年。'
- en: '[25] Reza Mahjourian, Martin Wicke, and Anelia Angelova. Unsupervised learning
    of depth and ego-motion from monocular video using 3d geometric constraints. In
    Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 5667–5675, 2018.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] Reza Mahjourian, Martin Wicke 和 Anelia Angelova. 使用三维几何约束从单目视频中进行无监督深度和自我运动学习。载于《IEEE计算机视觉与模式识别会议论文集》，页码
    5667–5675，2018年。'
- en: '[26] Clément Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular
    depth estimation with left-right consistency. In Proceedings of the IEEE conference
    on computer vision and pattern recognition, pages 270–279, 2017.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] Clément Godard, Oisin Mac Aodha 和 Gabriel J Brostow. 带有左右一致性的无监督单目深度估计。载于《IEEE计算机视觉与模式识别会议论文集》，页码
    270–279，2017年。'
- en: '[27] David Eigen and Rob Fergus. Predicting depth, surface normals and semantic
    labels with a common multi-scale convolutional architecture. In Proceedings of
    the IEEE international conference on computer vision, pages 2650–2658, 2015.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] David Eigen 和 Rob Fergus. 使用共同的多尺度卷积架构预测深度、表面法线和语义标签。载于《IEEE国际计算机视觉会议论文集》，页码
    2650–2658，2015年。'
- en: '[28] Pier Luigi Dovesi, Matteo Poggi, Lorenzo Andraghetti, Miquel Martí, Hedvig
    Kjellström, Alessandro Pieropan, and Stefano Mattoccia. Real-time semantic stereo
    matching. In 2020 IEEE international conference on robotics and automation (ICRA),
    pages 10780–10787\. IEEE, 2020.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] Pier Luigi Dovesi, Matteo Poggi, Lorenzo Andraghetti, Miquel Martí, Hedvig
    Kjellström, Alessandro Pieropan 和 Stefano Mattoccia. 实时语义立体匹配。载于2020年IEEE国际机器人与自动化会议（ICRA），页码
    10780–10787。IEEE，2020年。'
- en: '[29] Guorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong Deng, and Jiaya Jia.
    Segstereo: Exploiting semantic information for disparity estimation. In Proceedings
    of the European conference on computer vision (ECCV), pages 636–651, 2018.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] Guorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong Deng 和 Jiaya Jia.
    Segstereo: 利用语义信息进行视差估计。载于欧洲计算机视觉会议（ECCV），页码 636–651，2018年。'
- en: '[30] Taher Naderi, Amir Sadovnik, Jason Hayward, and Hairong Qi. Monocular
    depth estimation with adaptive geometric attention. In Proceedings of the IEEE/CVF
    Winter Conference on Applications of Computer Vision, pages 944–954, 2022.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] Taher Naderi, Amir Sadovnik, Jason Hayward 和 Hairong Qi. 带有自适应几何注意力的单目深度估计。载于《IEEE/CVF冬季计算机视觉应用会议论文集》，页码
    944–954，2022年。'
- en: '[31] Zuria Bauer, Zuoyue Li, Sergio Orts-Escolano, Miguel Cazorla, Marc Pollefeys,
    and Martin R Oswald. Nvs-monodepth: Improving monocular depth prediction with
    novel view synthesis. In 2021 International Conference on 3D Vision (3DV), pages
    848–858\. IEEE, 2021.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] Zuria Bauer, Zuoyue Li, Sergio Orts-Escolano, Miguel Cazorla, Marc Pollefeys
    和 Martin R Oswald. Nvs-monodepth: 通过新颖视图合成改善单目深度预测。载于2021年国际3D视觉会议（3DV），页码 848–858。IEEE，2021年。'
- en: '[32] Qingtian Zhu, Chen Min, Zizhuang Wei, Yisong Chen, and Guoping Wang. Deep
    learning for multi-view stereo via plane sweep: A survey. arXiv preprint arXiv:2106.15328,
    2021.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] Qingtian Zhu, Chen Min, Zizhuang Wei, Yisong Chen 和 Guoping Wang. 通过平面扫描的深度学习多视角立体：综述。arXiv预印本
    arXiv:2106.15328，2021年。'
- en: '[33] N.A. Wikipedia article: Photogrammetry. In Wikipedia, Accessed: 2023-10-12.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] N.A. 维基百科条目：摄影测量。载于维基百科，访问时间：2023-10-12。'
- en: '[34] Richard Szeliski and Polina Golland. Stereo matching with transparency
    and matting. In Sixth International Conference on Computer Vision (IEEE Cat. No.
    98CH36271), pages 517–524\. IEEE, 1998.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] Richard Szeliski 和 Polina Golland. 透明度和抠图的立体匹配. 在第六届国际计算机视觉会议 (IEEE Cat.
    No. 98CH36271), 页517–524\. IEEE, 1998.'
- en: '[35] Hideo Saito and Takeo Kanade. Shape reconstruction in projective grid
    space from large number of images. In Proceedings. 1999 IEEE Computer Society
    Conference on Computer Vision and Pattern Recognition (Cat. No PR00149), volume 2,
    pages 49–54\. IEEE, 1999.'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] Hideo Saito 和 Takeo Kanade. 从大量图像中在投影网格空间进行形状重建. 在第1999年IEEE计算机学会计算机视觉与模式识别会议论文集中
    (Cat. No PR00149), 第二卷, 页49–54\. IEEE, 1999.'
- en: '[36] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan.
    Recurrent mvsnet for high-resolution multi-view stereo depth inference. Computer
    Vision and Pattern Recognition (CVPR), 2019.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, 和 Long Quan. 高分辨率多视角立体深度推断的递归mvsnet.
    计算机视觉与模式识别会议 (CVPR), 2019.'
- en: '[37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, et al.
    Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048,
    2023.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho,
    Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, 等. Rwkv:
    为变换器时代重新发明rnn. arXiv 预印本 arXiv:2305.13048, 2023.'
- en: '[38] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, and Jia-Bin
    Huang. Deepmvs: Learning multi-view stereopsis. In Proceedings of the IEEE Conference
    on Computer Vision and Pattern Recognition, pages 2821–2830, 2018.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] Po-Han Huang, Kevin Matzen, Johannes Kopf, Narendra Ahuja, 和 Jia-Bin Huang.
    Deepmvs: 学习多视角立体视觉. 在 IEEE 计算机视觉与模式识别会议论文集中，页2821–2830, 2018.'
- en: '[39] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional
    networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted
    Intervention–MICCAI 2015: 18th International Conference, Munich, Germany, October
    5-9, 2015, Proceedings, Part III 18, pages 234–241\. Springer, 2015.'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] Olaf Ronneberger, Philipp Fischer, 和 Thomas Brox. U-net: 用于生物医学图像分割的卷积网络.
    在医学图像计算与计算机辅助干预–MICCAI 2015: 第18届国际会议, 慕尼黑, 德国, 2015年10月5-9日, 论文集, 第三部分 18, 页234–241\.
    Springer, 2015.'
- en: '[40] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid
    based depth inference for multi-view stereo. In Proceedings of the IEEE/CVF Conference
    on Computer Vision and Pattern Recognition, pages 4877–4886, 2020.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] Jiayu Yang, Wei Mao, Jose M Alvarez, 和 Miaomiao Liu. 基于成本体积金字塔的多视角立体深度推断.
    在 IEEE/CVF 计算机视觉与模式识别会议论文集中，页4877–4886, 2020.'
- en: '[41] David Gallup, Jan-Michael Frahm, Philippos Mordohai, Qingxiong Yang, and
    Marc Pollefeys. Real-time plane-sweeping stereo with multiple sweeping directions.
    In 2007 IEEE Conference on Computer Vision and Pattern Recognition, pages 1–8\.
    IEEE, 2007.'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] David Gallup, Jan-Michael Frahm, Philippos Mordohai, Qingxiong Yang, 和
    Marc Pollefeys. 实时平面扫掠立体视觉与多扫掠方向. 在 2007 IEEE 计算机视觉与模式识别会议, 页1–8\. IEEE, 2007.'
- en: '[42] Dorothy M Greig, Bruce T Porteous, and Allan H Seheult. Exact maximum
    a posteriori estimation for binary images. Journal of the Royal Statistical Society
    Series B: Statistical Methodology, 51:271–279, 1989.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] Dorothy M Greig, Bruce T Porteous, 和 Allan H Seheult. 二值图像的精确最大后验估计. 《皇家统计学会B系列：统计方法论》,
    51:271–279, 1989.'
- en: '[43] Christopher M Bishop and Nasser M Nasrabadi. Pattern recognition and machine
    learning, volume 4. Springer, 2006.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] Christopher M Bishop 和 Nasser M Nasrabadi. 模式识别与机器学习, 第4卷. Springer, 2006.'
- en: '[44] Arijit Mallick, Jörg Stückler, and Hendrik Lensch. Learning to adapt multi-view
    stereo by self-supervision. arXiv preprint arXiv:2009.13278, 2020.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] Arijit Mallick, Jörg Stückler, 和 Hendrik Lensch. 通过自我监督学习适应多视角立体视觉. arXiv
    预印本 arXiv:2009.13278, 2020.'
- en: '[45] Haimei Zhao, Jing Zhang, Zhuo Chen, Bo Yuan, and Dacheng Tao. On robust
    cross-view consistency in self-supervised monocular depth estimation. arXiv preprint
    arXiv:2209.08747, 2022.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] Haimei Zhao, Jing Zhang, Zhuo Chen, Bo Yuan, 和 Dacheng Tao. 自我监督单目深度估计中的稳健跨视图一致性.
    arXiv 预印本 arXiv:2209.08747, 2022.'
- en: '[46] Hongbin Xu, Zhipeng Zhou, Yali Wang, Wenxiong Kang, Baigui Sun, Hao Li,
    and Yu Qiao. Digging into uncertainty in self-supervised multi-view stereo. In
    Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
    6078–6087, 2021.'
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] Hongbin Xu, Zhipeng Zhou, Yali Wang, Wenxiong Kang, Baigui Sun, Hao Li,
    和 Yu Qiao. 探索自我监督多视角立体视觉中的不确定性. 在 IEEE/CVF 国际计算机视觉会议论文集中，页6078–6087, 2021.'
- en: '[47] Liman Liu, Fenghao Zhang, Wanjuan Su, Yuhang Qi, and Wenbing Tao. Geometric
    prior-guided self-supervised learning for multi-view stereo. Remote Sensing, 15(8):2109,
    2023.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] 利曼·刘、丰浩·张、万娟·苏、宇航·齐和文炳·陶。几何先验引导的自监督多视角立体学习。遥感，15(8):2109，2023年。'
- en: '[48] Tejas Khot, Shubham Agrawal, Shubham Tulsiani, Christoph Mertz, Simon
    Lucey, and Martial Hebert. Learning unsupervised multi-view stereopsis via robust
    photometric consistency. arXiv preprint arXiv:1905.02706, 2019.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] 特贾斯·科特、舒布汉·阿格拉瓦尔、舒布汉·图尔西亚尼、克里斯托弗·梅尔茨、西蒙·卢西和马歇尔·赫伯特。通过鲁棒光度一致性学习无监督多视角立体视觉。arXiv预印本
    arXiv:1905.02706，2019年。'
- en: '[49] Jingliang Li, Zhengda Lu, Yiqun Wang, Ying Wang, and Jun Xiao. Ds-mvsnet:
    Unsupervised multi-view stereo via depth synthesis. In Proceedings of the 30th
    ACM International Conference on Multimedia, pages 5593–5601, 2022.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] 静良·李、郑达·卢、义群·王、英·王和军·肖。Ds-mvsnet：通过深度合成的无监督多视角立体视觉。载于第30届ACM国际多媒体会议论文集，第5593–5601页，2022年。'
- en: '[50] Haonan Dong and Jian Yao. Patchmvsnet: Patch-wise unsupervised multi-view
    stereo for weakly-textured surface reconstruction. arXiv preprint arXiv:2203.02156,
    2022.'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] 浩南·董和剑·姚。Patchmvsnet：用于弱纹理表面重建的补丁级无监督多视角立体视觉。arXiv预印本 arXiv:2203.02156，2022年。'
- en: '[51] Hongbin Xu, Zhipeng Zhou, Weitao Chen, Baigui Sun, Hao Li, and Wenxiong
    Kang. Semi-supervised deep multi-view stereo. arXiv preprint arXiv:2207.11699,
    2022.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] 鸿斌·徐、志鹏·周、伟涛·陈、百贵·孙、浩·李和文雄·康。半监督深度多视角立体视觉。arXiv预印本 arXiv:2207.11699，2022年。'
- en: '[52] Zehao Yu, Lei Jin, and Shenghua Gao. P 2 net: Patch-match and plane-regularization
    for unsupervised indoor depth estimation. In European Conference on Computer Vision,
    pages 206–222\. Springer, 2020.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] 泽浩·余、雷·金和胜华·高。P 2 net：用于无监督室内深度估计的补丁匹配和平面正则化。载于欧洲计算机视觉会议论文集，第206–222页。Springer，2020年。'
- en: '[53] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry.
    IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(3):611–625,
    2018.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] 雅各布·恩格尔、弗拉德伦·科尔顿和丹尼尔·克雷默斯。直接稀疏视觉里程计。IEEE模式分析与机器智能汇刊，40(3):611–625，2018年。'
- en: '[54] Yuchao Dai, Zhidong Zhu, Zhibo Rao, and Bo Li. Mvs2: Deep unsupervised
    multi-view stereo with multi-view symmetry. In 2019 International Conference on
    3D Vision (3DV), pages 1–8\. Ieee, 2019.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] 雨超·戴、志东·朱、志博·饶和博·李。Mvs2：具有多视角对称的深度无监督多视角立体视觉。载于2019年国际3D视觉会议（3DV）论文集，第1–8页。IEEE，2019年。'
- en: '[55] Shu Chen, Zhengdong Pu, Xiang Fan, and Beiji Zou. Fixing defect of photometric
    loss for self-supervised monocular depth estimation. IEEE Transactions on Circuits
    and Systems for Video Technology, 32(3):1328–1338, 2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] 舒·陈、郑东·蒲、向·范和贝极·邹。修复自监督单目深度估计中的光度损失缺陷。IEEE电路与系统视频技术汇刊，32(3):1328–1338，2021年。'
- en: '[56] Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai,
    Donghao Luo, Chengjie Wang, Jilin Li, and Feiyue Huang. Learning by analogy: Reliable
    supervision from transformations for unsupervised optical flow estimation. In
    2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages
    6488–6497, 2020.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] 梁刘、江宁·张、瑞飞·贺、勇·刘、亚彪·王、英·泰、东浩·罗、程杰·王、吉林·李和飞跃·黄。类比学习：来自变换的可靠监督用于无监督光流估计。载于2020
    IEEE/CVF计算机视觉与模式识别会议（CVPR），第6488–6497页，2020年。'
- en: '[57] Simon Meister, Junhwa Hur, and Stefan Roth. Unflow: Unsupervised learning
    of optical flow with a bidirectional census loss. In Proceedings of the AAAI conference
    on artificial intelligence, volume 32, 2018.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] 西蒙·迈斯特、俊华·赫和斯特凡·罗斯。Unflow：具有双向普查损失的无监督光流学习。载于AAAI人工智能会议论文集，第32卷，2018年。'
- en: '[58] Clément Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow.
    Digging into self-supervised monocular depth estimation. In Proceedings of the
    IEEE/CVF international conference on computer vision, pages 3828–3838, 2019.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] 克莱门特·戈达、奥辛·麦克·奥达、迈克尔·弗曼和加布里埃尔·J·布罗斯托。深入探讨自监督单目深度估计。载于IEEE/CVF国际计算机视觉会议论文集，第3828–3838页，2019年。'
- en: '[59] Rui Chen, Songfang Han, Jing Xu, and Hao Su. Point-based multi-view stereo
    network. In Proceedings of the IEEE/CVF international conference on computer vision,
    pages 1538–1547, 2019.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] 瑞·陈、松芳·韩、静·徐和浩·苏。基于点的多视角立体网络。载于IEEE/CVF国际计算机视觉会议论文集，第1538–1547页，2019年。'
- en: '[60] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image
    quality assessment: from error visibility to structural similarity. IEEE transactions
    on image processing, 13(4):600–612, 2004.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] 周·王、艾伦·C·博维克、哈米德·R·谢赫和埃罗·P·西蒙切利。图像质量评估：从错误可见性到结构相似性。IEEE图像处理汇刊，13(4):600–612，2004年。'
- en: '[61] A.M. Eskicioglu and P.S. Fisher. Image quality measures and their performance.
    IEEE Transactions on Communications, 43(12):2959–2965, 1995.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] A.M. 埃斯基乔格鲁和 P.S. 菲舍尔。图像质量测量及其性能。IEEE通信学报，43(12)：2959–2965，1995年。'
- en: '[62] Hang Zhao, Orazio Gallo, Iuri Frosio, and Jan Kautz. Loss functions for
    neural networks for image processing. arXiv preprint arXiv:1511.08861, 2015.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] 赵航、奥拉齐奥·加洛、伊乌里·弗罗西奥和简·考茨。图像处理神经网络的损失函数。arXiv预印本arXiv:1511.08861，2015年。'
- en: '[63] Kaixuan Wang, Yao Chen, Hengkai Guo, Linfu Wen, and Shaojie Shen. Geometric
    pretraining for monocular depth estimation. In 2020 IEEE International Conference
    on Robotics and Automation (ICRA), pages 4782–4788\. IEEE, 2020.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] 王凯轩、陈耀、郭恒凯、温林富和沈少杰。单目深度估计的几何预训练。在2020年IEEE国际机器人与自动化会议（ICRA）上，第4782–4788页。IEEE，2020年。'
- en: '[64] Zhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth,
    optical flow and camera pose. In Proceedings of the IEEE Conference on Computer
    Vision and Pattern Recognition (CVPR), June 2018.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] 尹志超和石建平。Geonet：密集深度、光流和相机姿态的无监督学习。在IEEE计算机视觉与模式识别会议（CVPR）论文集中，2018年6月。'
- en: '[65] Shanshan Zhao, Huan Fu, Mingming Gong, and Dacheng Tao. Geometry-aware
    symmetric domain adaptation for monocular depth estimation. In Proceedings of
    the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9788–9798,
    2019.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] 赵姗姗、傅欢、龚名名和陶大成。面向单目深度估计的几何感知对称领域适应。在IEEE/CVF计算机视觉与模式识别会议论文集中，第9788–9798页，2019年。'
- en: '[66] Sergio Uras, Federico Girosi, Alessandro Verri, and Vincent Torre. A computational
    approach to motion perception. Biological cybernetics, 60:79–87, 1988.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] 塞尔吉奥·乌拉斯、费德里科·吉奥罗西、亚历山德罗·维里和文森特·托雷。运动感知的计算方法。生物控制论，60：79–87，1988年。'
- en: '[67] Thomas Brox, Andrés Bruhn, Nils Papenberg, and Joachim Weickert. High
    accuracy optical flow estimation based on a theory for warping. In Computer Vision-ECCV
    2004: 8th European Conference on Computer Vision, Prague, Czech Republic, May
    11-14, 2004\. Proceedings, Part IV 8, pages 25–36\. Springer, 2004.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] 托马斯·布罗克斯、安德烈斯·布鲁恩、尼尔斯·帕彭贝格和约阿希姆·维克特。基于变形理论的高精度光流估计。在计算机视觉-ECCV 2004：第8届欧洲计算机视觉会议，捷克布拉格，2004年5月11-14日。论文集，第IV部分，第8卷，第25–36页。施普林格，2004年。'
- en: '[68] Yongming Yang, Shuwei Shao, Tao Yang, Peng Wang, Zhuo Yang, Chengdong
    Wu, and Hao Liu. A geometry-aware deep network for depth estimation in monocular
    endoscopy. Engineering Applications of Artificial Intelligence, 122:105989, 2023.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] 杨永明、邵舒伟、杨涛、王鹏、杨卓和吴承栋。用于单目内窥镜深度估计的几何感知深度网络。人工智能工程应用，122：105989，2023年。'
- en: '[69] Junjie Hu, Mete Ozay, Yan Zhang, and Takayuki Okatani. Revisiting single
    image depth estimation: Toward higher resolution maps with accurate object boundaries.
    In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages
    1043–1051, 2019.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] 胡俊杰、梅特·奥扎伊、张燕和冈田高行。重新审视单幅图像深度估计：朝着更高分辨率的地图与准确的物体边界前进。在2019年IEEE计算机视觉应用冬季会议（WACV）上，第1043–1051页，2019年。'
- en: '[70] Christopher Zach, Thomas Pock, and Horst Bischof. A duality based approach
    for realtime tv-l 1 optical flow. In Pattern Recognition: 29th DAGM Symposium,
    Heidelberg, Germany, September 12-14, 2007\. Proceedings 29, pages 214–223\. Springer,
    2007.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] 克里斯托弗·扎克、托马斯·波克和霍斯特·比绍夫。基于对偶性的实时TV-L1光流方法。在模式识别：第29届DAGM研讨会，德国海德堡，2007年9月12-14日。论文集29，第214–223页。施普林格，2007年。'
- en: '[71] Pedro F Felzenszwalb and Daniel P Huttenlocher. Efficient graph-based
    image segmentation. International journal of computer vision, 59:167–181, 2004.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] Pedro F. 菲尔岑斯瓦尔布和丹尼尔 P. 胡滕洛赫。基于图的高效图像分割。国际计算机视觉杂志，59：167–181，2004年。'
- en: '[72] Paul J Besl and Neil D McKay. Method for registration of 3-d shapes. In
    Sensor fusion IV: control paradigms and data structures, volume 1611, pages 586–606\.
    Spie, 1992.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] Paul J. 贝斯尔和尼尔 D. 麦凯。3-D形状注册方法。在传感器融合IV：控制范式和数据结构，第1611卷，第586–606页。SPIE，1992年。'
- en: '[73] Y. Chen and G. Medioni. Object modeling by registration of multiple range
    images. In Proceedings. 1991 IEEE International Conference on Robotics and Automation,
    pages 2724–2729 vol.3, 1991.'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] Y. 陈和 G. 美迪奥尼。通过注册多个深度图像进行对象建模。在1991年IEEE国际机器人与自动化会议论文集中，第2724–2729页，第3卷，1991年。'
- en: '[74] S. Rusinkiewicz and M. Levoy. Efficient variants of the icp algorithm.
    In Proceedings Third International Conference on 3-D Digital Imaging and Modeling,
    pages 145–152, 2001.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] S. 鲁辛基维奇和 M. 利沃伊。ICP算法的高效变体。在第三届国际3-D数字成像与建模会议论文集中，第145–152页，2001年。'
- en: '[75] Peng Wang, Xiaohui Shen, Bryan Russell, Scott Cohen, Brian Price, and
    Alan L Yuille. Surge: Surface regularized geometry estimation from a single image.
    Advances in Neural Information Processing Systems, 29, 2016.'
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Peng Wang、Xiaohui Shen、Bryan Russell、Scott Cohen、Brian Price 和 Alan L
    Yuille。Surge：从单幅图像中表面正则化几何估计。收录于《神经信息处理系统进展》，第29卷，2016年。'
- en: '[76] Wei Yin, Yifan Liu, Chunhua Shen, and Youliang Yan. Enforcing geometric
    constraints of virtual normal for depth prediction. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pages 5684–5693, 2019.'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] Wei Yin、Yifan Liu、Chunhua Shen 和 Youliang Yan。强化虚拟法线的几何约束以进行深度预测。收录于《IEEE/CVF国际计算机视觉会议论文集》，第5684–5693页，2019年。'
- en: '[77] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor
    segmentation and support inference from rgbd images. In Computer Vision–ECCV 2012:
    12th European Conference on Computer Vision, Florence, Italy, October 7-13, 2012,
    Proceedings, Part V 12, pages 746–760\. Springer, 2012.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] Nathan Silberman、Derek Hoiem、Pushmeet Kohli 和 Rob Fergus。室内分割与基于RGBD图像的支持推断。收录于《计算机视觉–ECCV
    2012：第12届欧洲计算机视觉会议，意大利佛罗伦萨，2012年10月7-13日，会议论文集，第五部分 12》，第746–760页。Springer，2012年。'
- en: '[78] Xiaojuan Qi, Renjie Liao, Zhengzhe Liu, Raquel Urtasun, and Jiaya Jia.
    Geonet: Geometric neural network for joint depth and surface normal estimation.
    In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
    pages 283–291, 2018.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] Xiaojuan Qi、Renjie Liao、Zhengzhe Liu、Raquel Urtasun 和 Jiaya Jia。GeoNet：用于联合深度和表面法线估计的几何神经网络。收录于《IEEE计算机视觉与模式识别会议论文集》，第283–291页，2018年。'
- en: '[79] David F Fouhey, Abhinav Gupta, and Martial Hebert. Data-driven 3d primitives
    for single image understanding. In Proceedings of the IEEE International Conference
    on Computer Vision, pages 3392–3399, 2013.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] David F Fouhey、Abhinav Gupta 和 Martial Hebert。基于数据的三维原语用于单幅图像理解。收录于《IEEE国际计算机视觉会议论文集》，第3392–3399页，2013年。'
- en: '[80] Uday Kusupati, Shuo Cheng, Rui Chen, and Hao Su. Normal assisted stereo
    depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 2189–2199, 2020.'
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] Uday Kusupati、Shuo Cheng、Rui Chen 和 Hao Su。法线辅助的立体深度估计。收录于《IEEE/CVF计算机视觉与模式识别会议论文集》，第2189–2199页，2020年。'
- en: '[81] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr:
    Detector-free local feature matching with transformers. In Proceedings of the
    IEEE/CVF conference on computer vision and pattern recognition, pages 8922–8931,
    2021.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] Jiaming Sun、Zehong Shen、Yuang Wang、Hujun Bao 和 Xiaowei Zhou。Loftr：基于变换器的无检测器局部特征匹配。收录于《IEEE/CVF计算机视觉与模式识别会议论文集》，第8922–8931页，2021年。'
- en: '[82] Patrick Ruhkamp, Daoyi Gao, Hanzhi Chen, Nassir Navab, and Beniamin Busam.
    Attention meets geometry: Geometry guided spatial-temporal attention for consistent
    self-supervised monocular depth estimation. In 2021 International Conference on
    3D Vision (3DV), pages 837–847\. IEEE, 2021.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] Patrick Ruhkamp、Daoyi Gao、Hanzhi Chen、Nassir Navab 和 Beniamin Busam。注意力遇到几何：几何引导的时空注意力用于一致的自监督单目深度估计。收录于《2021国际三维视觉会议（3DV）》，第837–847页。IEEE，2021年。'
- en: '[83] Jie Zhu, Bo Peng, Wanqing Li, Haifeng Shen, Zhe Zhang, and Jianjun Lei.
    Multi-view stereo with transformer. arXiv preprint arXiv:2112.00336, 2021.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Jie Zhu、Bo Peng、Wanqing Li、Haifeng Shen、Zhe Zhang 和 Jianjun Lei。具有变换器的多视角立体视觉。arXiv预印本
    arXiv:2112.00336，2021年。'
- en: '[84] Longteng Guo, Jing Liu, Xinxin Zhu, Peng Yao, Shichen Lu, and Hanqing
    Lu. Normalized and geometry-aware self-attention network for image captioning.
    In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition,
    pages 10327–10336, 2020.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] Longteng Guo、Jing Liu、Xinxin Zhu、Peng Yao、Shichen Lu 和 Hanqing Lu。用于图像描述的归一化和几何感知自注意网络。收录于《IEEE/CVF计算机视觉与模式识别会议论文集》，第10327–10336页，2020年。'
- en: '[85] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks
    for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] Karen Simonyan 和 Andrew Zisserman。用于大规模图像识别的非常深度卷积网络。arXiv预印本 arXiv:1409.1556，2014年。'
- en: '[86] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander Alemi.
    Inception-v4, inception-resnet and the impact of residual connections on learning.
    In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] Christian Szegedy、Sergey Ioffe、Vincent Vanhoucke 和 Alexander Alemi。Inception-v4、Inception-ResNet
    以及残差连接对学习的影响。收录于《AAAI人工智能会议论文集》，第31卷，2017年。'
- en: '[87] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning
    for image recognition. In Proceedings of the IEEE conference on computer vision
    and pattern recognition, pages 770–778, 2016.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] Kaiming He、Xiangyu Zhang、Shaoqing Ren 和 Jian Sun。用于图像识别的深度残差学习。收录于《IEEE计算机视觉与模式识别会议论文集》，第770–778页，2016年。'
- en: '[88] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for
    real-time style transfer and super-resolution. In Computer Vision–ECCV 2016: 14th
    European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings,
    Part II 14, pages 694–711\. Springer, 2016.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] Justin Johnson, Alexandre Alahi 和 Li Fei-Fei. 实时风格迁移和超分辨率的感知损失. 在计算机视觉–ECCV
    2016：第 14 届欧洲会议, 荷兰阿姆斯特丹, 2016 年 10 月 11-14 日, 论文集, 第二部分 14, 第 694–711 页\. Springer,
    2016.'
- en: '[89] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction.
    ACM Transactions on Graphics (ToG), 32(3):1–13, 2013.'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] Michael Kazhdan 和 Hugues Hoppe. 屏幕化泊松表面重建. ACM 图形学交易 (ToG), 32(3):1–13,
    2013.'
- en: '[90] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited.
    In Proceedings of the IEEE conference on computer vision and pattern recognition,
    pages 4104–4113, 2016.'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] Johannes L Schonberger 和 Jan-Michael Frahm. 运动结构重访. 在 IEEE 计算机视觉与模式识别会议论文集中,
    第 4104–4113 页, 2016.'
- en: '[91] Johannes L Schönberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys.
    Pixelwise view selection for unstructured multi-view stereo. In Computer Vision–ECCV
    2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016,
    Proceedings, Part III 14, pages 501–518\. Springer, 2016.'
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] Johannes L Schönberger, Enliang Zheng, Jan-Michael Frahm 和 Marc Pollefeys.
    用于非结构化多视图立体的逐像素视图选择. 在计算机视觉–ECCV 2016：第 14 届欧洲会议, 荷兰阿姆斯特丹, 2016 年 10 月 11-14 日,
    论文集, 第三部分 14, 第 501–518 页\. Springer, 2016.'
- en: '[92] Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation
    for deep learning. Journal of big data, 6(1):1–48, 2019.'
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] Connor Shorten 和 Taghi M Khoshgoftaar. 深度学习图像数据增强的调查. 大数据杂志, 6(1):1–48,
    2019.'
- en: '[93] Armand Joulin, Francis Bach, and Jean Ponce. Multi-class cosegmentation.
    In 2012 IEEE conference on computer vision and pattern recognition, pages 542–549\.
    IEEE, 2012.'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] Armand Joulin, Francis Bach 和 Jean Ponce. 多类别共同分割. 在 2012 年 IEEE 计算机视觉与模式识别会议中,
    第 542–549 页\. IEEE, 2012.'
- en: '[94] Vincent Casser, Soeren Pirk, Reza Mahjourian, and Anelia Angelova. Unsupervised
    monocular depth and ego-motion learning with structure and semantics. In Proceedings
    of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops,
    pages 0–0, 2019.'
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] Vincent Casser, Soeren Pirk, Reza Mahjourian 和 Anelia Angelova. 使用结构和语义的无监督单目深度和自我运动学习.
    在 IEEE/CVF 计算机视觉与模式识别会议研讨会论文集中, 第 0–0 页, 2019.'
- en: '[95] Chris Ding, Xiaofeng He, and Horst D Simon. On the equivalence of nonnegative
    matrix factorization and spectral clustering. In Proceedings of the 2005 SIAM
    international conference on data mining, pages 606–610\. SIAM, 2005.'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] Chris Ding, Xiaofeng He 和 Horst D Simon. 非负矩阵分解与谱聚类的等效性. 在 2005 年 SIAM
    国际数据挖掘会议论文集中, 第 606–610 页\. SIAM, 2005.'
- en: '[96] Edo Collins, Radhakrishna Achanta, and Sabine Susstrunk. Deep feature
    factorization for concept discovery. In Proceedings of the European Conference
    on Computer Vision (ECCV), pages 336–352, 2018.'
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] Edo Collins, Radhakrishna Achanta 和 Sabine Susstrunk. 用于概念发现的深度特征分解. 在欧洲计算机视觉会议（ECCV）论文集中,
    第 336–352 页, 2018.'
- en: '[97] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by
    learning an invariant mapping. In 2006 IEEE computer society conference on computer
    vision and pattern recognition (CVPR’06), volume 2, pages 1735–1742\. IEEE, 2006.'
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] Raia Hadsell, Sumit Chopra 和 Yann LeCun. 通过学习不变映射进行降维. 在 2006 年 IEEE 计算机学会计算机视觉与模式识别会议（CVPR''06）,
    第 2 卷, 第 1735–1742 页\. IEEE, 2006.'
- en: '[98] Dongseok Shim and H Jin Kim. Learning a geometric representation for data-efficient
    depth estimation via gradient field and contrastive loss. In 2021 IEEE International
    Conference on Robotics and Automation (ICRA), pages 13634–13640\. IEEE, 2021.'
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] Dongseok Shim 和 H Jin Kim. 通过梯度场和对比损失学习数据高效的几何表示用于深度估计. 在 2021 年 IEEE
    国际机器人与自动化会议（ICRA）论文集中, 第 13634–13640 页\. IEEE, 2021.'
- en: '[99] Seokju Lee, Francois Rameau, Fei Pan, and In So Kweon. Attentive and contrastive
    learning for joint depth and motion field estimation. In Proceedings of the IEEE/CVF
    International Conference on Computer Vision, pages 4862–4871, 2021.'
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] Seokju Lee, Francois Rameau, Fei Pan 和 In So Kweon. 用于联合深度和运动场估计的注意力和对比学习.
    在 IEEE/CVF 国际计算机视觉会议论文集中, 第 4862–4871 页, 2021.'
- en: '[100] Rizhao Fan, Matteo Poggi, and Stefano Mattoccia. Contrastive learning
    for depth prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision
    and Pattern Recognition, pages 3225–3236, 2023.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] Rizhao Fan, Matteo Poggi 和 Stefano Mattoccia. 用于深度预测的对比学习. 在 IEEE/CVF
    计算机视觉与模式识别会议论文集中, 第 3225–3236 页, 2023.'
- en: '[101] John Canny. A computational approach to edge detection. IEEE Transactions
    on pattern analysis and machine intelligence, 6:679–698, 1986.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] 约翰·坎尼。边缘检测的计算方法。IEEE模式分析与机器智能交易，6：679-698，1986。'
