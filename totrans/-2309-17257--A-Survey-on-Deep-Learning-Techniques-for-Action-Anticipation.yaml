- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:36:55'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2309.17257] A Survey on Deep Learning Techniques for Action Anticipation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2309.17257](https://ar5iv.labs.arxiv.org/html/2309.17257)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Deep Learning Techniques for Action Anticipation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zeyun Zhong, Manuel Martin, Michael Voit, Juergen Gall, Jürgen Beyerer Zeyun
    Zhong, Jürgen Beyerer are with the Karlsruhe Institute of Technology (KIT), Germany.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: firstname.lastname@kit.edu'
  prefs: []
  type: TYPE_NORMAL
- en: Zeyun Zhong, Manuel Martin, Michael Voit, Jürgen Beyerer are with the Fraunhofer
    Institute of Optronics, System Technologies and Image Exploitation IOSB, Germany.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: firstname.lastname@iosb.fraunhofer.de'
  prefs: []
  type: TYPE_NORMAL
- en: Juergen Gall is with the University of Bonn and the Lamarr Institute for Machine
    Learning and Artificial Intelligence, Germany.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: gall@iai.uni-bonn.de'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The ability to anticipate possible future human actions is essential for a wide
    range of applications, including autonomous driving and human-robot interaction.
    Consequently, numerous methods have been introduced for action anticipation in
    recent years, with deep learning-based approaches being particularly popular.
    In this work, we review the recent advances of action anticipation algorithms
    with a particular focus on daily-living scenarios. Additionally, we classify these
    methods according to their primary contributions and summarize them in tabular
    form, allowing readers to grasp the details at a glance. Furthermore, we delve
    into the common evaluation metrics and datasets used for action anticipation and
    provide future directions with systematical discussions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: action anticipation, activities of daily living, video understanding, deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compared to human action recognition and early action recognition, where the
    entire or part of an action is observable, action anticipation aims to predict
    a future action without observing any part of it, as displayed in Figure [1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    Anticipating possible future daily-living actions is one of the most important
    tasks for human machine cooperation and robotic assistance, e.g., to offer a hand
    at the right time or to generate a proactive dialog to provide more natural interactions.
  prefs: []
  type: TYPE_NORMAL
- en: As the future actions are often not deterministic, this tends to be significantly
    more challenging than the traditional action recognition task, where today’s well-honed
    discriminative models [[1](#bib.bib1), [2](#bib.bib2)] perform very well. The
    predictability of actions varies based on the nature of the tasks involved, as
    shown in Fig. [2](#S1.F2 "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning
    Techniques for Action Anticipation"). In the realm of predefined processes with
    less variability, such as industrial processes and medical operations [[3](#bib.bib3)],
    actions tend to be highly predictable due to the strict adherence to established
    protocols and guidelines. On the other hand, predefined processes with large variability,
    like cooking, introduce a degree of unpredictability as they involve subjective
    decision-making and varying ingredients, which can influence the final outcome.
    Actions stimulated by the environment or other people, exemplified by opening
    the door when the doorbell rings, exhibit moderate predictability, as they are
    influenced by external cues, but the responses are often ingrained through habitual
    patterns. Lastly, spontaneous behavior represents the highest level of unpredictability,
    as it arises without explicit stimuli or established patterns, making it challenging
    to anticipate.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, the computer vision research community has shown significant interest
    in addressing this challenging task. In line with action recognition, anticipation
    approaches began with predictions based on a single video frame [[4](#bib.bib4)]
    and tended to use longer temporal context in more recent works [[5](#bib.bib5),
    [6](#bib.bib6)]. In addition to utilizing a longer action history, many approaches
    are exploring the incorporation of multiple modalities beyond the raw RGB video
    frames to further improve the anticipatory ability. These include optical flow,
    which contains motion information, as well as objects present in the scene, among
    others.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1718b2ac9b69fea627b01a544512e523.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: The action anticipation task aims to anticipate future actions before
    they happen, whereas action recognition and early action recognition require the
    observation of complete and partial actions, respectively. The example frames
    are from the EpicKitchens [[7](#bib.bib7)] dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e089cbdaf73eb5c7e4bf5ea15f99fd0b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Predictability of actions.'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the aforementioned action recognition and early action recognition
    task, there are several other tasks in the literature that are related to action
    anticipation, such as action segmentation [[8](#bib.bib8)], temporal/spatio-temporal
    action detection [[9](#bib.bib9)], video prediction [[10](#bib.bib10)], trajectory
    prediction [[11](#bib.bib11)], and motion prediction [[12](#bib.bib12)]. The definitions
    of action anticipation and its related tasks in computer vision are listed in
    Table [I](#S1.T1 "TABLE I ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques
    for Action Anticipation"). In contrast to action anticipation, the related tasks
    are concerned with recognizing and understanding actions that have already occurred
    or are in progress. Despite these differences, these tasks complement each other,
    with action recognition and localization providing foundational components for
    action anticipation.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Definition of action anticipation and its related tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Task | Definition |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Action Anticipation | Anticipates one or multiple future actions before they
    happen. |'
  prefs: []
  type: TYPE_TB
- en: '| Action Recognition | Categorizes actions in a video or image sequence given
    the full observation. |'
  prefs: []
  type: TYPE_TB
- en: '| Early Action Recognition | Categorizes actions in a video or image sequence
    given the partial observation. |'
  prefs: []
  type: TYPE_TB
- en: '| Action Segmentation | Categorizes actions for every frame of the video. |'
  prefs: []
  type: TYPE_TB
- en: '| Temporal Action Detection | Identifies and localizes actions within a video
    by predicting the start and end times of actions. |'
  prefs: []
  type: TYPE_TB
- en: '| Spatio-temporal Action Detection | Identifies and localizes actions in both
    spatial and temporal dimensions within a video. |'
  prefs: []
  type: TYPE_TB
- en: '| Video Prediction | Predicts future frames given a set of video frames. |'
  prefs: []
  type: TYPE_TB
- en: '| Trajectory Prediction | Estimates the future trajectory or path of objects
    or agents in a scene. |'
  prefs: []
  type: TYPE_TB
- en: '| Motion Prediction | Predicts changes in human pose. |'
  prefs: []
  type: TYPE_TB
- en: 1.1 Application Domains
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While exploring the plausible future is well studied in other fields, such as
    weather forecasting [[13](#bib.bib13)] and stock price prediction [[14](#bib.bib14)],
    researchers have only recently become heavily active in exploring solutions in
    the computer vision community. As such, action anticipation approaches have been
    applied for diverse applications across various domains in recent years, such
    as industrial applications [[15](#bib.bib15)], robotics [[16](#bib.bib16)], advanced
    driver assistance systems [[17](#bib.bib17)], autonomous driving [[18](#bib.bib18)],
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: In the industrial domain like robotized warehouses, action anticipation approaches
    can be used to infer the intention of workers to improve efficiency and safety [[15](#bib.bib15),
    [19](#bib.bib19)]. In the field of human-robot-interaction, in contrast to passive
    service, e.g., providing service only after being spoken to, robots are expected
    to provide proactive service that initiates an interaction at an early stage.
    In this context, the ability of accurately predicting the start of an interaction [[20](#bib.bib20)]
    or the object that the user is going to interact with [[16](#bib.bib16), [21](#bib.bib21),
    [22](#bib.bib22)] enables robots to respond naturally and intuitively to human
    actions, improving collaboration. Anticipation can also find application in generating
    notifications for our daily routines, such as reminding us to turn off lights
    before leaving a room or the stove after cooking [[23](#bib.bib23)]. Moreover,
    the safety of users, especially care recipients, stands to be significantly improved
    if robots can assess the risk of the current environment [[24](#bib.bib24)]. In
    driving scenarios, anticipating a dangerous maneuver beforehand can alert drivers
    before they perform the maneuver and can also give driver assistance systems more
    time to avoid or prepare for the danger [[17](#bib.bib17)]. In autonomous driving,
    intelligent vehicles rely on predictive abilities to anticipate the actions of
    other road users in urban environments, particularly pedestrians at crosswalks [[18](#bib.bib18)]
    or potential traffic accidents [[25](#bib.bib25)], ensuring safe and efficient
    navigation.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Review Scope and Terminology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are several surveys in the literature that discuss action anticipation
    as part of their scope [[26](#bib.bib26), [27](#bib.bib27), [28](#bib.bib28),
    [29](#bib.bib29), [30](#bib.bib30), [31](#bib.bib31)]. However, these surveys
    do not provide a full overview of the action anticipation literature. While [[27](#bib.bib27),
    [28](#bib.bib28), [30](#bib.bib30)] consider several topics and action anticipation
    is only briefly discussed among other topics, [[29](#bib.bib29), [31](#bib.bib31)]
    focus on the specific challenges of egocentric vision. [[26](#bib.bib26)], being
    one of the earlier works, primarily delves into methods that are not deep learning-based.
    In this survey, our objective is to review and classify the research field of
    action anticipation in general. This includes an overview of the current state-of-the-art
    approaches as well as datasets and evaluation metrics. We conclude our review
    by identifying future research challenges and by summarizing our findings.
  prefs: []
  type: TYPE_NORMAL
- en: Although classical learning approaches, such as models based on the Markovian
    assumption, including Probabilistic Suffix Tree (PST) [[32](#bib.bib32), [33](#bib.bib33)],
    Hidden Markov Model (HMM) [[17](#bib.bib17)], Conditional Random Fields (CRFs) [[16](#bib.bib16)],
    Markov Decision Process (MDP) [[34](#bib.bib34)], and other statistical methods [[35](#bib.bib35),
    [36](#bib.bib36)], have been widely used in the literature, we put our focus on
    deep learning techniques and how they have been extended or applied to action
    anticipation, leaving the classical approaches outside the scope of the present
    review. In this context, the terms action anticipation, action prediction, and
    action forecasting are used interchangeably. Note that we do not explicitly differentiate
    between the terms action and activity in this survey, since these two terms are
    commonly used interchangeably in the literature.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Problem Statement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Section [2.1](#S2.SS1 "2.1 Video feature encoding ‣ 2 Problem Statement
    ‣ A Survey on Deep Learning Techniques for Action Anticipation"), we introduce
    video feature encoding, a foundational step for action anticipation. Based on
    the predicted time horizon, action anticipation approaches can be grouped into
    two categories: short-term anticipation approaches and long-term anticipation
    approaches. Short-term approaches usually operate on subsymbolic sensory data
    and predict actions a few seconds into the future. Conversely, long-term approaches
    may utilize action history with a higher abstraction level to predict a sequence
    of future actions (with their durations) up to several minutes into the future.
    Below, we present the detailed task definitions commonly used in the literature
    for both categories in Section [2.2](#S2.SS2 "2.2 Short-term anticipation ‣ 2
    Problem Statement ‣ A Survey on Deep Learning Techniques for Action Anticipation")
    and Section [2.3](#S2.SS3 "2.3 Long-term anticipation ‣ 2 Problem Statement ‣
    A Survey on Deep Learning Techniques for Action Anticipation"), respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Video feature encoding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Untrimmed videos are typically used to evaluate action anticipation approaches.
    They can be as long as several minutes, and contain several actions. It is therefore
    difficult to directly input the entire video to a visual encoder for feature extraction
    due to the limits of computational resources. A common strategy for video representation
    is to partition the video into equally sized temporal intervals called snippets,
    and then apply a pre-trained feature extractor over each snippet.
  prefs: []
  type: TYPE_NORMAL
- en: More formally, let the observed video segment be denoted by $V$ that contains
    $T$ frames $\{x_{1},x_{2},\dotsc,x_{T}\}$, corresponding to $i$ actions $\{a^{1},a^{2},\dotsc,a^{i}\}$,
    where $i$ is usually much smaller than $T$. Then, the video segment is broken
    down into $N$ snippets {$V_{1},V_{2},\dotsc,V_{N}$}, with each snippet $V_{k}$
    containing $n$ video frames $\{x_{k1},x_{k2},\dotsc,x_{kn}\}$, where the exact
    number of frames $n$ depends on the frame rate of the videos and the used feature
    extractor. Afterwards, a feature extractor is applied on these snippets to extract
    a sequence of video representations {$z_{1},z_{2},\dotsc,z_{N}$}.
  prefs: []
  type: TYPE_NORMAL
- en: Common extractors range from frame-level spatial models, such as VGG16 [[37](#bib.bib37)],
    ResNet50 [[38](#bib.bib38)], TSN [[39](#bib.bib39)] and ViT [[40](#bib.bib40)],
    to snippet-level spatiotemporal models, such as I3D [[41](#bib.bib41)], Two-stream [[42](#bib.bib42)],
    R(2+1)D [[43](#bib.bib43)], MViT [[44](#bib.bib44)], and Swin [[2](#bib.bib2)].
    The features extracted from pre-trained encoders, which are typically trained
    for trimmed action recognition tasks, are not necessarily suitable for action
    anticipation.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Short-term anticipation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1065cb18dfae6d2bf3ab2111dcfad2c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Category of the action anticipation task. Short-term anticipation
    aims to predict actions at several future time steps, whereas the long-term task
    aims to predict subsequent actions (along with their durations) without adhering
    to fixed time steps.'
  prefs: []
  type: TYPE_NORMAL
- en: Most short-term anticipation approaches follow the setup defined in [[4](#bib.bib4),
    [45](#bib.bib45), [7](#bib.bib7)]. Approaches tackling the short-term anticipation
    task typically operate on the sequence of video representations extracted by a
    pre-trained feature extractor, as introduced in Section [2.1](#S2.SS1 "2.1 Video
    feature encoding ‣ 2 Problem Statement ‣ A Survey on Deep Learning Techniques
    for Action Anticipation"). Taking these representations with the same temporal
    spacing as input, short-term approaches predict a few representations $M$ time
    steps into the future $\{\hat{z}_{N+1},\hat{z}_{N+2},\dotsc,\hat{z}_{N+M}\}$,
    which are then classified into actions $\{\hat{a}_{N+1},\hat{a}_{N+2},\dotsc,\hat{a}_{N+M}\}$,
    as illustrated in Figure [3](#S2.F3 "Figure 3 ‣ 2.2 Short-term anticipation ‣
    2 Problem Statement ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    The number of time steps into the future $M$ depends on the anticipation protocol
    and the temporal spacing of the video representations. For instance, $M$ would
    be 4 if the temporal spacing were set at 0.25 seconds and the chosen protocol
    aimed to predict an action in 1 second.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Long-term anticipation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Parallel research addresses the long-term anticipation task [[46](#bib.bib46),
    [47](#bib.bib47)]. The goal is to anticipate the category (and the duration) of
    future actions for a given time horizon, which can extend up to several minutes,
    as illustrated in Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Short-term anticipation ‣ 2
    Problem Statement ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    In contrast to short-term approaches, long-term approaches are not confined to
    using subsymbolic video representations as their input. Certain methods leverage
    past actions, represented as $\{a^{1},a^{2},\dotsc,a^{i}\}$, to directly forecast
    a sequence of future actions, $\{\hat{a}^{i+1},\hat{a}^{i+2},\dotsc,\hat{a}^{i+j}\}$ [[46](#bib.bib46),
    [48](#bib.bib48), [49](#bib.bib49)]. These past actions could either be based
    on actual observations or derived from an action segmentation technique [[50](#bib.bib50),
    [51](#bib.bib51)]. Approaches that tackle long-term anticipation typically rely
    on fully-labeled data, i.e., sequences labeled with all future actions and their
    durations. Notably, some methods operate within a weakly supervised setting, using
    only a few fully labeled sequences and primarily sequences where only the next
    action is labeled [[52](#bib.bib52)].
  prefs: []
  type: TYPE_NORMAL
- en: 3 Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the predictive capability of machines could enable many real-world
    applications in robotics and manufacturing, developing an algorithm to anticipate
    the future action of the user is challenging. Humans rely on extensive knowledge
    accumulated over their lifetime to infer what will happen next. However, how machines
    can gain such knowledge remains an open question. In this section, we describe
    the methods that are trying to tackle such a challenging task. To our knowledge,
    the current literature does not provide a specific taxonomy to classify action
    anticipation models. In this review, we therefore classify the existing methods
    into five groups, according to the specific problem they addressed, as shown in
    Fig. [4](#S3.F4 "Figure 4 ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for
    Action Anticipation").
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/4a4a8401c8e7ca435429e8292b5db551.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Classification of action anticipation methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging the inherent temporal dynamics of videos as a learning signal, some
    methods build self-supervised frameworks to train the anticipation models (Section [3.1](#S3.SS1
    "3.1 Leveraging temporal dynamics as a learning signal ‣ 3 Methods ‣ A Survey
    on Deep Learning Techniques for Action Anticipation")). Early methods anticipate
    future actions based on the representation of a single past frame, ignoring all
    temporal dynamics. To accurately predict the actions in the future, the evolution
    of the past actions should be analyzed and summarized. Consequently, recent approaches
    use various architectures to encode the long-term history, including recurrent
    and transformer-based models (Section [3.2](#S3.SS2 "3.2 Utilizing long-term temporal
    history ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation")).
    Some methods attempt to derive and utilize additional modalities¹¹1We refer to
    both different sensory data (e.g., RGB and audio) and different feature representations
    (e.g., RGB and optical flow) as modalities for the sake of emphasizing the difference
    in data representations. to improve the anticipative performance, such as presence
    of objects in the scene, object bounding boxes, and optical flow, as they may
    complement the raw RGB video frames. In addition to the data acquired from the
    input video, other information sources, e.g., personalization, the topology of
    the action sequences, or the underlying intention, may also contribute to improved
    future prediction (Section [3.3](#S3.SS3 "3.3 Narrowing anticipation space by
    using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")). As the vast majority of action anticipation models
    are deterministic, they do not deal with the uncertainty of the future. To address
    this issue, several authors proposed modeling uncertainty for future prediction
    (Section [3.4](#S3.SS4 "3.4 Incorporating uncertainty ‣ 3 Methods ‣ A Survey on
    Deep Learning Techniques for Action Anticipation")). Most action anticipation
    methods in the literature operate on the representation-level, i.e., future video
    representations are first anticipated and then categorized to actions, as illustrated
    in the upper part of Fig. [3](#S2.F3 "Figure 3 ‣ 2.2 Short-term anticipation ‣
    2 Problem Statement ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    In contrast, some methods predict future actions directly based on the observed
    action sequences (Section [3.5](#S3.SS5 "3.5 Modeling at a more conceptual level
    ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation")),
    which may provide a clean analysis of action dependencies and better explainability.
  prefs: []
  type: TYPE_NORMAL
- en: Table [II](#S3.T2 "TABLE II ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation") shows an overview of the most relevant methods, ordered
    in chronological order. We characterize these approaches by their neural network
    architecture, datasets used for training and testing, and technical details, including
    modalities fed into the individual model, the fusion strategy (if any), incorporation
    of external knowledge, the abstraction level on which a prediction is made, and
    whether the model is addressing the long-term or short-term anticipation tasks
    introduced in Section [2](#S2 "2 Problem Statement ‣ A Survey on Deep Learning
    Techniques for Action Anticipation"). Although most short-term methods can perform
    long-term anticipation by iteratively applying the prediction module, we characterize
    each method based on the experiments performed in the original work only. Moreover,
    we provide the information whether the model was trained end-to-end, or if it
    relied on pre-extracted features. Learning from features allows to save computational
    resources during training. On the other hand, the ability to learn or fine-tune
    a task-speciﬁc representation end-to-end may be beneficial depending on the specific
    task. Furthermore, pre-extracted features are not available in a real-life streaming
    scenario, so models trained from features should also consider the impact of feature
    extraction at application time. Note that the taxonomy is not mutually exclusive,
    as some methods can be classified into several categories since they address multiple
    goals. For instance, [[48](#bib.bib48), [53](#bib.bib53)] address both action-level
    prediction and prediction uncertainty. We specify the category of these models
    according to their main contribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Summary of most relevant action anticipation models. The following
    abbreviations are used in the table. Architecture: Transformer (TF), Non-local
    Block (NLB). Modalities: Objects (O), Bounding Boxes (BB), Motion (M), Action
    Classes (A), Audio (Au), Gaze (G), Hand Mask (H), Human-Object-Interaction (HOI),
    Time (T). Fusion: Score Fusion (score), Late feature fusion (late), Mid-level
    feature fusion (mid). EK: external knowledge. LT: Long-term anticipation. E2E:
    End-to-end. Because of space constraints, we have omitted the inclusion of object
    detectors such as Fast R-CNN [[54](#bib.bib54)] and Mask R-CNN [[55](#bib.bib55)]
    as feature extractors for approaches that rely on object or human-object interaction
    modalities.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Feature extractor | Future predictor | Datasets | Details
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| Modalities | Fusion | EK | Abst. | LT | E2E |'
  prefs: []
  type: TYPE_TB
- en: '| Leveraging temporal dynamics as a learning signal |'
  prefs: []
  type: TYPE_TB
- en: '| Vondrick et al. [[4](#bib.bib4)] | 2016 | AlexNet[[56](#bib.bib56)] | MLP
    | [[57](#bib.bib57), [58](#bib.bib58), [59](#bib.bib59)] | RGB | – | ✗ | feat.
    | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Zeng et al. [[60](#bib.bib60)] | 2017 | ResNet[[38](#bib.bib38)] | MLP |
    [[57](#bib.bib57), [59](#bib.bib59)] | RGB | – | ✗ | feat. | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| RED [[45](#bib.bib45)] | 2017 | VGG[[37](#bib.bib37)],TS[[61](#bib.bib61)],
    | LSTM | [[62](#bib.bib62), [57](#bib.bib57), [59](#bib.bib59)] | RGB,M | mid
    | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhong et al. [[63](#bib.bib63)] | 2018 | ResNet [[38](#bib.bib38)] | LSTM
    | [[57](#bib.bib57), [59](#bib.bib59)] | RGB | – | ✗ | feat. | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Tran et al. [[64](#bib.bib64)] | 2021 | I3D [[41](#bib.bib41)] | – | [[7](#bib.bib7)]
    | RGB | – | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Fernando et al. [[65](#bib.bib65)] | 2021 | R(2+1)D [[43](#bib.bib43)] |
    FC | [[7](#bib.bib7), [66](#bib.bib66)] | RGB,O,M | score | ✗ | feat.,act. | ✗
    | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| AVT [[67](#bib.bib67)] | 2021 | TSN [[39](#bib.bib39)],ViT [[40](#bib.bib40)]
    | TF | [[7](#bib.bib7), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)]
    | RGB,O | score | ✗ | feat. | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| DCR [[71](#bib.bib71)] | 2022 | TSN [[39](#bib.bib39)],TSM [[72](#bib.bib72)]
    | LSTM,TF | [[7](#bib.bib7), [68](#bib.bib68), [69](#bib.bib69), [70](#bib.bib70)]
    | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | MViT [[44](#bib.bib44)] | TF | [[7](#bib.bib7),
    [68](#bib.bib68), [69](#bib.bib69)] | RGB | – | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Utilizing long-term temporal history |'
  prefs: []
  type: TYPE_TB
- en: '| Gammulle et al. [[74](#bib.bib74)] | 2019 | ResNet [[38](#bib.bib38)] | LSTM
    | [[70](#bib.bib70), [66](#bib.bib66)] | RGB,A | mid | ✗ | feat. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TRN [[75](#bib.bib75)] | 2019 | TS [[61](#bib.bib61)] | LSTM | [[59](#bib.bib59),
    [62](#bib.bib62)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | TSN [[39](#bib.bib39)],I3D [[41](#bib.bib41)]
    | NLB,LSTM | [[7](#bib.bib7), [70](#bib.bib70), [66](#bib.bib66)] | RGB,O,BB,M
    | score | ✗ | feat. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TTPP [[76](#bib.bib76)] | 2020 | VGG[[37](#bib.bib37)],TS[[61](#bib.bib61)]
    | TF,MLP | [[62](#bib.bib62), [57](#bib.bib57), [59](#bib.bib59)] | RGB,M | mid
    | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| LAP [[77](#bib.bib77)] | 2020 | TS [[61](#bib.bib61)] | GRU | [[59](#bib.bib59),
    [62](#bib.bib62)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ImagineRNN [[78](#bib.bib78)] | 2021 | TSN [[39](#bib.bib39)] | LSTM | [[7](#bib.bib7),
    [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| LSTR [[79](#bib.bib79)] | 2021 | TS [[61](#bib.bib61)] | TF | [[59](#bib.bib59),
    [62](#bib.bib62)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| OadTR [[80](#bib.bib80)] | 2021 | TS [[61](#bib.bib61)] | TF | [[62](#bib.bib62),
    [59](#bib.bib59)] | RGB,M | mid | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| HRO [[81](#bib.bib81)] | 2022 | TSN [[39](#bib.bib39)] | GRU | [[7](#bib.bib7),
    [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | MViT [[44](#bib.bib44)] | – | [[68](#bib.bib68)]
    | RGB | – | ✗ | feat. | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| FUTR [[82](#bib.bib82)] | 2022 | I3D [[41](#bib.bib41)] | TF | [[70](#bib.bib70),
    [66](#bib.bib66)] | RGB,M | mid | ✗ | feat. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | TS [[61](#bib.bib61)],TSN [[39](#bib.bib39)]
    | TF | [[59](#bib.bib59), [68](#bib.bib68)] | RGB,M | mid | ✗ | feat. | ✗ | ✗
    |'
  prefs: []
  type: TYPE_TB
- en: '| Multi-modal fusion |'
  prefs: []
  type: TYPE_TB
- en: '| Zhou et al. [[84](#bib.bib84)] | 2015 | VGG [[37](#bib.bib37)],TS [[42](#bib.bib42)]
    | MLP | [[85](#bib.bib85), [84](#bib.bib84)] | RGB,O,M | late | ✗ | feat. | ✗
    | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Mahmud et al. [[86](#bib.bib86)] | 2017 | C3D [[87](#bib.bib87)] | LSTM,MLP
    | [[88](#bib.bib88), [89](#bib.bib89)] | O,M | late | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Shen et al. [[90](#bib.bib90)] | 2018 | [[91](#bib.bib91)],SSD [[92](#bib.bib92)]
    | LSTM | [[93](#bib.bib93), [94](#bib.bib94)] | O,BB,G,H | late | ✗ | feat. |
    ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Liang et al. [[95](#bib.bib95)] | 2019 | [[96](#bib.bib96)] | LSTM | [[97](#bib.bib97)]
    | RGB,HOI | late | ✗ | feat. | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| RULSTM [[98](#bib.bib98)] | 2019 | TSN [[39](#bib.bib39)] | LSTM | [[7](#bib.bib7),
    [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| FHOI [[99](#bib.bib99)] | 2020 | I3D [[41](#bib.bib41)],CSN [[100](#bib.bib100)]
    | FC | [[7](#bib.bib7), [69](#bib.bib69)] | RGB,O | score | ✗ | feat. | ✗ | ✓
    |'
  prefs: []
  type: TYPE_TB
- en: '| Ego-OMG [[101](#bib.bib101)] | 2021 | CSN [[87](#bib.bib87)],GCN [[102](#bib.bib102)]
    | LSTM | [[7](#bib.bib7)] | RGB | score | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Zatsarynna et al. [[103](#bib.bib103)] | 2021 | TSN [[39](#bib.bib39)] |
    TCN | [[7](#bib.bib7), [68](#bib.bib68)] | RGB,O,M | late | ✗ | feat. | ✗ | ✗
    |'
  prefs: []
  type: TYPE_TB
- en: '| Roy et al. [[104](#bib.bib104)] | 2021 | I3D [[41](#bib.bib41)] | TF | [[7](#bib.bib7),
    [70](#bib.bib70), [66](#bib.bib66)] | RGB,HOI,M | score | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| AFFT [[105](#bib.bib105)] | 2023 | TSN [[39](#bib.bib39)],Swin [[106](#bib.bib106)]
    | TF | [[68](#bib.bib68), [69](#bib.bib69)] | RGB,O,M,Au | mid | ✗ | feat. | ✗
    | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Conditioning on extra information |'
  prefs: []
  type: TYPE_TB
- en: '| S-RNN [[107](#bib.bib107)] | 2016 | [[108](#bib.bib108)] | GNN,LSTM | [[108](#bib.bib108)]
    | RGB,O,BB | mid | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| DR²N [[109](#bib.bib109)] | 2019 | S3D [[110](#bib.bib110)] | GNN,GRU | [[111](#bib.bib111)]
    | RGB | – | ✗ | feat.,act. | ✗ | ✓ |'
  prefs: []
  type: TYPE_TB
- en: '| Farha et al. [[112](#bib.bib112)] | 2020 | I3D [[41](#bib.bib41)] | TCN,GRU
    | [[70](#bib.bib70), [66](#bib.bib66)] | RGB,M | mid | ✗ | feat. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Camporese et al. [[113](#bib.bib113)] | 2020 | TSN [[39](#bib.bib39)] | LSTM
    | [[7](#bib.bib7), [69](#bib.bib69)] | RGB,O,M | score | ✓ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| RESTEP [[114](#bib.bib114)] | 2021 | I3D [[41](#bib.bib41)] | ConvGRU | [[111](#bib.bib111)]
    | RGB | – | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| A-ACT [[115](#bib.bib115)] | 2022 | TSN [[39](#bib.bib39)],I3D [[41](#bib.bib41)]
    | TF,MLP | [[66](#bib.bib66), [70](#bib.bib70), [7](#bib.bib7)] | RGB,O,M | score
    | ✗ | feat.,act. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Abst. goal [[116](#bib.bib116)] | 2022 | TSN [[39](#bib.bib39)] | GRU | [[7](#bib.bib7),
    [68](#bib.bib68), [69](#bib.bib69)] | RGB,O,M | score | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Anticipatr [[117](#bib.bib117)] | 2022 | I3D [[41](#bib.bib41)] | TF | [[70](#bib.bib70),
    [66](#bib.bib66), [69](#bib.bib69), [7](#bib.bib7)] | RGB | – | ✗ | feat. | ✓
    | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Mascaro et al. [[118](#bib.bib118)] | 2023 | SlowFast [[1](#bib.bib1)] |
    MLP,VAE,TF | [[47](#bib.bib47)] | RGB | – | ✗ | feat. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| AntGPT [[119](#bib.bib119)] | 2023 | ViT [[40](#bib.bib40)] | TF | [[47](#bib.bib47),
    [7](#bib.bib7), [69](#bib.bib69)] | RGB | – | ✓ | act. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Incorporating uncertainty |'
  prefs: []
  type: TYPE_TB
- en: '| Schydlo et al. [[21](#bib.bib21)] | 2018 | [[120](#bib.bib120)] | LSTM |
    [[108](#bib.bib108)] | P | – | ✗ | feat. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Farha et al. [[48](#bib.bib48)] | 2019 | – | GRU | [[70](#bib.bib70), [66](#bib.bib66)]
    | A,T | mid | ✗ | act. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| APP-VAE [[53](#bib.bib53)] | 2019 | – | LSTM,VAE | [[66](#bib.bib66), [121](#bib.bib121)]
    | A,T | mid | ✗ | act. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Ng et al. [[122](#bib.bib122)] | 2020 | I3D [[41](#bib.bib41)] | GRU | [[123](#bib.bib123),
    [66](#bib.bib66), [89](#bib.bib89), [70](#bib.bib70)] | RGB | – | ✗ | feat. |
    ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| AGG [[124](#bib.bib124)] | 2020 | – | TCN,MLP,GAN | [[123](#bib.bib123),
    [70](#bib.bib70), [121](#bib.bib121)] | A | – | ✗ | act. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[49](#bib.bib49)] | 2020 | – | LSTM,GAN | [[66](#bib.bib66),
    [70](#bib.bib70), [7](#bib.bib7)] | A,T | mid | ✗ | act. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Modeling at a more conceptual level |'
  prefs: []
  type: TYPE_TB
- en: '| Farha et al. [[46](#bib.bib46)] | 2018 | – | CNN,GRU | [[70](#bib.bib70),
    [66](#bib.bib66)] | A,T | mid | ✗ | act. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Miech et al. [[125](#bib.bib125)] | 2019 | R(2+1)D [[43](#bib.bib43)] | FC
    | [[7](#bib.bib7), [66](#bib.bib66), [126](#bib.bib126)] | RGB | – | ✗ | feat.,act.
    | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Ke et al. [[127](#bib.bib127)] | 2019 | I3D [[41](#bib.bib41)] | TCN | [[7](#bib.bib7),
    [70](#bib.bib70), [66](#bib.bib66)] | RGB,T | mid | ✗ | act. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| Zhang et al. [[128](#bib.bib128)] | 2020 | TSN [[39](#bib.bib39)] | LSTM,MLP
    | [[7](#bib.bib7)] | RGB,O,M | score | ✗ | feat.,act. | ✗ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: '| ProActive [[129](#bib.bib129)] | 2022 | – | TF | [[66](#bib.bib66), [126](#bib.bib126),
    [121](#bib.bib121)] | A,T | mid | ✓ | act. | ✓ | ✗ |'
  prefs: []
  type: TYPE_TB
- en: 3.1 Leveraging temporal dynamics as a learning signal
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Utilizing temporal dynamics via a self-supervised loss. One promising way to
    equip machines with the ability to infer future user actions is to use the abundantly
    available unlabeled videos on the Internet. These videos are economically feasible
    to obtain at massive scales and contain rich signals, particularly the inherent
    temporal ordering of frames. Some models leverage the temporal ordering and are
    trained by predicting the next video frame or motion image [[130](#bib.bib130)].
    However, the pixel space is high dimensional and extremely variable [[10](#bib.bib10)],
    which can pose additional challenges to the anticipation task. Consequently, many
    approaches consider the visual representation of future frames as their prediction
    target and learn to anticipate future representations by maximizing the correlation [[65](#bib.bib65),
    [131](#bib.bib131)] or minimizing the distance between predicted and actual future
    frame representations, e.g., in the form of a contrastive loss [[132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136),
    [137](#bib.bib137)] or a simple MSE loss [[4](#bib.bib4), [45](#bib.bib45), [63](#bib.bib63),
    [67](#bib.bib67)].
  prefs: []
  type: TYPE_NORMAL
- en: Video representations capture the semantic information about actions and can
    be automatically computed using a pre-trained feature extractor, making them scalable
    to unlabeled videos. Building upon this idea, an initial work [[4](#bib.bib4)]
    built a deep regression network that takes a single frame as input and anticipates
    the future frame representation of a pre-trained AlexNet [[56](#bib.bib56)] (last
    hidden layer (fc7)). A single frame, however, does not provide much information
    about the past. To tackle this limitation, Zhong and Zheng [[63](#bib.bib63)]
    developed a two-stream network comprising a spatial stream and a temporal stream,
    following the learning objective from [[4](#bib.bib4)] (i.e., an MSE loss). While
    the spatial stream encoded the spatial context of the recent frame using a ResNet [[38](#bib.bib38)],
    the temporal stream summarized the historical information using a Long Short-Term
    Memory unit (LSTM [[138](#bib.bib138)]). To address the limited long-term modeling
    ability of LSTMs, the authors proposed employing a compact representation computed
    by a fully connected layer over all hidden states of past time steps. With the
    encoded spatial and temporal context, a fully connected layer was then employed
    to predict future representations. Through the incorporation of temporal information,
    this approach surpassed [[4](#bib.bib4)].
  prefs: []
  type: TYPE_NORMAL
- en: Another line of research is based on contrastive learning [[132](#bib.bib132),
    [133](#bib.bib133), [134](#bib.bib134), [135](#bib.bib135), [136](#bib.bib136)].
    The essence of the contrastive objective is to bring positive pairs closer together
    while distancing them from a large set of negative pairs. The definition of positive
    and negative pairs varies across these works. The Contrastive Predictive Coding
    (CPC) [[132](#bib.bib132)] and Dense Predictive Coding (DPC) [[133](#bib.bib133)]
    approaches share a common conceptual foundation, where their learning objectives
    were to predict coarse clip-level and fine-grained spatiotemporal region representations
    of future clips, given an observed sequence of clips for context, respectively.
    Han et al. [[134](#bib.bib134)] extended the principles of DPC by introducing
    a memory bank composed of learnable vectors to account for the non-deterministic
    nature of predicting the future. Suris et al. [[135](#bib.bib135)] integrated
    the hyperbolic geometry, specifically the Poincaré ball model, into the DPC framework
    to discern which features can be predicted from the data. Hyperbolic geometry,
    akin to a continuous tree [[139](#bib.bib139)], encodes hierarchical structures
    and facilitates predictions at multiple levels of abstraction. In cases of high
    confidence, specific actions are predicted, while in uncertain scenarios, higher-level
    abstractions are chosen. Zatsarynna et al. [[136](#bib.bib136)] proposed a composite
    loss for unintentional action prediction, which consists of a temporal contrastive
    loss that encourages proximity of neighboring video clips in the embedding space,
    and a pair-wise ordering loss that accounts for the relative order of video clips.
  prefs: []
  type: TYPE_NORMAL
- en: 'In contrast to aforementioned approaches, where the classifier is trainable
    while the main part of the models remains fixed after pre-training, some approaches
    have demonstrated the effectiveness of fine-tuning on the target dataset for action
    anticipation [[45](#bib.bib45), [71](#bib.bib71), [131](#bib.bib131), [137](#bib.bib137)],
    especially for datasets where positive video segments containing ongoing actions
    are a very small part of the entire dataset. For instance, Gao et al. [[45](#bib.bib45)]
    proposed an encoder-decoder network (RED) based on LSTMs. Taking continuous steps
    of historical visual representations as input, the network output a sequence of
    anticipated future representations, which were then processed by a classification
    network for action classification. The architecture was trained in a two-stage
    process. In the first stage, self-supervised training using an MSE loss was performed
    on all training videos in the dataset. In the second stage, the initialized network
    was optimized with full supervision on positive samples from the videos. Moreover,
    the authors introduced a reinforcement learning module to optimize the network
    on the sequence level. For further details, please refer to Section [3.3.2](#S3.SS3.SSS2
    "3.3.2 Conditioning on extra information ‣ 3.3 Narrowing anticipation space by
    using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation"). Similarly, Xu et al. [[71](#bib.bib71)] performed order-aware
    pre-training for the adopted transformer-based reasoning model [[140](#bib.bib140)],
    which proved beneficial for the final anticipation performance. Specifically,
    the observed video segment was sent into the transformer without positional encoding.
    The cosine similarity between transformer output tokens and the positional encoding
    was then computed and transformed into a probability using Softmax. To supervise
    the training, the authors proposed a similarity based on Gaussian affinity [[141](#bib.bib141)].
    The similarity $s_{p,q}$ of positional encoding at time $p$ and the frame feature
    at time $q$ is measured as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $s_{p,q}=\mathrm{exp}\biggl{(}-\frac{(p-q)^{2}}{\sigma^{2}}\biggr{)},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\sigma$ represents the bandwidth of the Gaussian and is typically set
    to a constant empirically.
  prefs: []
  type: TYPE_NORMAL
- en: Some approaches chose to incorporate a self-supervised loss in the training
    objective, often combined with a cross-entropy loss for action classification.
    Tran et al. [[64](#bib.bib64)] proposed supervising the training of the anticipation
    model by minimizing the distance between past and future feature maps, along with
    a cross-entropy loss. To address dynamics in videos, the authors introduced an
    attentional pooling operator to transform feature maps to location-agnostic representations
    for computing the squared loss. Fernando and Herath [[65](#bib.bib65)] proposed
    three novel similarity measures to supervise the training, maximizing the correlation
    between the predicted and actual future representations. They argued that commonly
    used similarity measures such as L2 distance and cosine similarity are not ideal,
    as they are either unbounded or discard the magnitude. Girdhar and Grauman [[67](#bib.bib67)]
    proposed a GPT2-based causal anticipation model [[142](#bib.bib142)] and employed
    the MSE loss for supervising intermediate future predictions. In contrast to predicting
    the next frame feature using the causal attention mask as done in [[67](#bib.bib67)],
    Girase et al. [[73](#bib.bib73)] proposed using a generalized form of masking-based
    self-supervision to predict shuffled future features, allowing for exponentially
    more variations than the vanilla causal masking scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the underlying dynamics. Some approaches relied on the adversarial
    training framework [[143](#bib.bib143)] to learn the underlying dynamics of videos [[60](#bib.bib60),
    [144](#bib.bib144), [124](#bib.bib124), [49](#bib.bib49)]. For instance, Zeng et al. [[60](#bib.bib60)]
    formulated the anticipation task as an inverse reinforcement learning (IRL) problem,
    where the goal was to imitate the behavior of natural sequences that are treated
    as expert demonstrations. More specifically, they leveraged the Generative Adversarial
    Imitation Learning framework [[145](#bib.bib145)] to bypass the exhaustive state-action
    pair visits, since both the state (frame) and action (transition) space were high-dimensional
    and continuous. In this framework, a discriminator, which aimed to distinguish
    the generated sequence from expert’s, and a generator (policy), guided by the
    discriminator to move toward expert-like regions, were jointly optimized during
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Utilizing long-term temporal history
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most actions exhibit dependencies on preceding actions. These preceding trigger
    actions may not necessarily be confined to the immediate past but can extend back
    to the long-term history. Consequently, many methods aim to exploit the information
    contained in the long-term history.
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent networks. Having an internal memory, recurrent networks, including
    LSTMs [[138](#bib.bib138)] and GRUs [[146](#bib.bib146)], are often adopted for
    exploiting the sequential action history [[45](#bib.bib45), [98](#bib.bib98),
    [75](#bib.bib75), [77](#bib.bib77)]. Qi et al. [[147](#bib.bib147)] employed GRUs
    for a recursive sequence prediction task. To tackle the challenge of error accumulation,
    they proposed a self-regulated learning framework that regulated the intermediate
    representation using a contrastive loss and a dynamic reweighing mechanism to
    attend to informative frames in the observed content. Inspired by the human visual
    cognitive system, where individuals often deduce current actions by envisioning
    future scenes concurrently [[148](#bib.bib148), [149](#bib.bib149)], Xu et al. [[75](#bib.bib75)]
    proposed the temporal recurrent network (TRN) to simultaneously perform online
    action detection and anticipation of the immediate future. This predicted future
    data was then harnessed to enhance the present action detection. While initially
    crafted for online action detection, TRN’s predictive capabilities led to its
    application in action anticipation. This concept of incorporating predicted future
    information for improving detection performance has further been followed by later
    works [[77](#bib.bib77), [80](#bib.bib80)]. Instead of using simply fixed future
    temporal ranges as in TRN, Qu et al. [[77](#bib.bib77)] argued that the optimal
    supplementary features should be obtained from distinct temporal ranges. They
    introduced an adaptive feature sampling strategy. More specifically, they initially
    inferred the distribution of the ongoing action progression by leveraging past
    frame representations. To sample a progression, they employed the Gumbel-Softmax
    trick [[150](#bib.bib150)], a technique that emulates one-hot vectors derived
    from categorical distributions. Subsequently, an equidistant sampling strategy
    was employed to acquire the desired supplementary features, encompassing both
    preceding and anticipated future features. The size of the sampling range was
    contingent upon the current progression of actions.
  prefs: []
  type: TYPE_NORMAL
- en: Incorporating an external memory module. It is well known that recurrent networks
    struggle to model long-term dependencies due to their sequential nature. To enhance
    the long-term modeling ability of common recurrent architectures such as LSTMs,
    external memory modules were incorporated in [[74](#bib.bib74), [81](#bib.bib81)].
    Gammulle et al. [[74](#bib.bib74)] proposed using external neural memory networks
    to store the knowledge of the whole dataset. Specifically, they made use of two
    types of historical information, the observed frames and the corresponding action
    labels. When an input stimulus, i.e., a query vector generated by a read operation
    based on the input was present, the memory network generated an output based on
    the knowledge that persisted in the memory and updated itself with a write operation.
    By iteratively feeding the concatenated sequential outputs of each separate memory
    network of two information sources to an LSTM, a sequence of future action predictions
    was generated. Unlike in [[74](#bib.bib74)], the external memory module in Liu
    and Lam [[81](#bib.bib81)] (HRO) was fixed, i.e., it will not be updated when
    processing a sequence during inference. Furthermore, the memory module in HRO
    was used for predicting the representation at the next time step, rather than
    enhancing the representation at the current time step. To further regularize the
    learning process and improve the anticipation performance, the authors employed
    a contrastive learning paradigm, inspired by [[78](#bib.bib78)], and a transition
    layer to bridge the gap between the semantics of the past and the future.
  prefs: []
  type: TYPE_NORMAL
- en: Most anticipation models are designed to expect pre-extracted features as inputs,
    in order to leverage a longer temporal history while limiting computational cost
    during training. However, this also eliminates the benefit of fine-tuning a task-specific
    representation compared to end-to-end approaches, where feature extractors are
    also optimized during back-propagation. To train a fully end-to-end model and
    benefit from a longer action history without hitting the computation or memory
    bottlenecks, Wu et al. [[6](#bib.bib6)] proposed processing videos in an online
    fashion and caching memory. Videos were divided into consecutive clips and processed
    sequentially. Intermediate results, i.e., the key and value vectors of earlier
    time steps, were cached in the memory module, so that the query vector at the
    current time step had access not only to the recent past frames within the same
    clip but also to the past frames in earlier clips. This design differs from the
    aforementioned memory-based methods in that it does not contain any learnable
    parameters and thus no knowledge about the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Attending the temporal history. In tackling the challenge of sequentially modeling
    recurrent architectures, numerous approaches have emerged, aiming to encapsulate
    the action history using a parallel methodology. This involves processing all
    past frames simultaneously rather than sequentially. Notably, the attention mechanism [[140](#bib.bib140)]
    has garnered substantial interest within these approaches, leveraging its widespread
    application and remarkable performance in both natural language processing and
    computer vision domains. Wang et al. [[76](#bib.bib76)] proposed a transformer-style
    architecture which generated a query vector from the last observed frame feature,
    and key and value vectors from all other past frame features. The query vector
    extracted relevant information from the past history based on the attention score,
    forming an aggregated representation. A prediction module consisting of MLPs was
    then leveraged to iteratively predict future features and actions based on the
    aggregated representation. Sener et al. [[5](#bib.bib5)] represented recent observation
    with different temporal extent and long-range past history at various granularities.
    Non-local blocks [[151](#bib.bib151), [152](#bib.bib152)] were used to aggregate
    short-term (recent) and long-term (spanning) features via attention. Each recent
    feature was coupled with all spanning representations. Outputs were combined via
    score fusion, i.e., averaging multiple predictions, to predict the final result.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reducing complexity of self-attention. The self-attention operation in transformer
    models calculates the dot product between each pair of tokens in the input sequence,
    whose complexity is $O(N^{2})$, where $N$ is the length of the sequence. Recently,
    there has been a targeted focus to reduce the complexity to $O(N)$ in large methods
    so that transformer models can scale to long sequences. Low-rank approximations
    factorize the attention matrix into two lower-rank matrices [[153](#bib.bib153),
    [154](#bib.bib154)]. Query-based cross-attention mechanisms compress longer-term
    input into a fixed-size representation [[155](#bib.bib155), [156](#bib.bib156)].
    Following the idea of query-based cross-attention design, Xu et al. [[79](#bib.bib79)]
    proposed a two-stage memory compression design to compress and abstract the long-term
    memory into a latent representation of fixed length, which has empirically been
    found to lead to better performance. Moreover, the authors employed a short window
    of recent frames to perform self-attention and cross-attention operations on the
    extracted long-term memory. In case of action anticipation, the short-term memory
    was concatenated with a sequence of learnable tokens, responsible for future predictions.
    Following Xu et al. [[79](#bib.bib79)], Zhao and Krähenbühl [[83](#bib.bib83)]
    reformulated the cross-attention in the first memory compression stage through
    the lens of a kernel [[157](#bib.bib157)]. In this reformulation, the attention
    can be seen as applying a kernel smoother to the inputs, where the kernel scores
    represent input similarities. They applied two kinds of temporal smoothing kernel:
    a box kernel and a Laplace kernel. The resulting streaming attention reuses much
    of the computation from frame to frame, and only has a constant caching and computing
    overhead.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Narrowing anticipation space by using additional information
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: While vision based systems are the de-facto standard for action recognition
    and anticipation [[98](#bib.bib98), [67](#bib.bib67)], using other additional
    supporting input modalities like optical flow features [[39](#bib.bib39), [41](#bib.bib41),
    [158](#bib.bib158)] or knowledge about objects and sounds in the scene [[159](#bib.bib159),
    [158](#bib.bib158), [105](#bib.bib105)] has shown to be beneficial [[160](#bib.bib160)].
    To properly fuse the different modalities, various fusion strategies have been
    exploited for action anticipation in recent years, which are described in Section [3.3.1](#S3.SS3.SSS1
    "3.3.1 Multi-modal fusion ‣ 3.3 Narrowing anticipation space by using additional
    information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    Moreover, additional useful information such as personalization (Section [3.3.2](#S3.SS3.SSS2
    "3.3.2 Conditioning on extra information ‣ 3.3 Narrowing anticipation space by
    using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")) could also be incorporated in the system to further
    support anticipation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1 Multi-modal fusion
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Input modalities. While action anticipation methods typically take the original
    video frames as the default input modality, many methods leverage higher level
    modalities from the original RGB frames, such as optical flow [[161](#bib.bib161)],
    object features (usually depending on an object detector such as [[54](#bib.bib54),
    [55](#bib.bib55)]), human-object-interaction [[162](#bib.bib162), [104](#bib.bib104),
    [163](#bib.bib163)], and human-scene-interaction [[95](#bib.bib95)]. For egocentric
    videos, the camera wearer’s trajectory [[164](#bib.bib164)], hand trajectory [[99](#bib.bib99)],
    eye gaze [[69](#bib.bib69)], and environment affordance [[165](#bib.bib165)] have
    been additionally used.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/afb7690c3e9f4c828d7c5ac9df9a3264.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Score fusion.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6559f16b0cc5d677fbe8496a420dc9df.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) Late feature fusion.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f767661504ed5f0da194f9d15dbdbefb.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Mid-level feature fusion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 5: Three typical fusion strategies for multi-modal action anticipation.
    Feature extractors are omitted for brevity. Examples from the EpicKitchens [[7](#bib.bib7)]
    dataset and AFFT [[105](#bib.bib105)].'
  prefs: []
  type: TYPE_NORMAL
- en: For instance, Shen et al. [[90](#bib.bib90)] proposed using gaze to extract
    events, as gaze moving in/out of a certain object most probably indicates the
    occurrence/ending of a certain activity, and constructed a two-stream network
    driven by the gaze events. The asynchronous stream modeled inter-relationship
    between different events with a Hawkes process model [[166](#bib.bib166)], whereas
    the synchronous stream extracted frame-wise hand and gaze features which were
    temporally weighted by an attention module based on the gaze events. Liu et al. [[99](#bib.bib99)]
    explicitly incorporated intentional hand movements as an anticipatory representation
    of actions. They jointly modeled and predicted hand trajectories, interaction
    hotspots and labels of future actions. Similarly, Dessalene et al. [[101](#bib.bib101)]
    exploited hand and object segmentation masks and used the time to contact between
    hands and objects as a representation for the anticipation task. Long-term temporal
    semantic relations between actions were modeled using a graph convolutional network
    (GCN [[102](#bib.bib102)]) and an LSTM. Future actions were predicted based on
    the graph representations along with appearance features. Roy and Fernando [[104](#bib.bib104)]
    introduced human-object interaction as an anticipatory representation and represented
    it by cross-correlation of visual features of every pairwise human-object pairs
    in the scene. A standard encoder-decoder transformer was then leveraged for temporal
    feature aggregation and future action generation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Score fusion. Consistent with most multi-modal action recognition models [[39](#bib.bib39),
    [41](#bib.bib41)], anticipation models typically use score fusion, as displayed
    in Fig. [5(a)](#S3.F5.sf1 "In Figure 5 ‣ 3.3.1 Multi-modal fusion ‣ 3.3 Narrowing
    anticipation space by using additional information ‣ 3 Methods ‣ A Survey on Deep
    Learning Techniques for Action Anticipation"), to fuse different modalities. While
    averaging using fixed weights, including simple averaging [[5](#bib.bib5), [104](#bib.bib104)]
    and weighted averaging [[67](#bib.bib67)], showed already superior results over
    the uni-modal baseline, Furnari et al. [[98](#bib.bib98)] showed that assigning
    each modality with dynamical importance for the final prediction is particularly
    beneficial for anticipating egocentric actions. They employed a two-LSTMs-based
    encoder-decoder architecture (RU-LSTM) for individual modalities. Taking three
    distinct types of information into account: appearance, motion, and object presence
    in the scene, they built three branches and proposed a fusion module consisting
    of fully connected layers to generate attention scores for each modality. It is
    noteworthy that the authors also published the pre-extracted features²²2[https://github.com/fpv-iplab/rulstm](https://github.com/fpv-iplab/rulstm)
    of Epic-Kitchens [[7](#bib.bib7), [167](#bib.bib167)] and EGTEA Gaze+ [[69](#bib.bib69)],
    which were used in many approaches on action anticipation [[5](#bib.bib5), [103](#bib.bib103),
    [105](#bib.bib105)]. Osman et al. [[168](#bib.bib168)] further extended this work
    by incorporating a second stream operating at different frame rates, inspired
    by [[1](#bib.bib1)].'
  prefs: []
  type: TYPE_NORMAL
- en: Feature fusion. There exists another line of multi-modal work which focuses
    on feature fusion, i.e., modalities get fused before decisions are made, including
    late fusion [[103](#bib.bib103), [86](#bib.bib86)] and mid-level fusion [[105](#bib.bib105),
    [82](#bib.bib82), [48](#bib.bib48)]. Similar to score-fusion-based methods, approaches
    based on a late fusion strategy first anticipate representations containing future
    action patterns, and then fuse representations of different modalities with a
    specific fusion module (refer to Fig. [5(b)](#S3.F5.sf2 "In Figure 5 ‣ 3.3.1 Multi-modal
    fusion ‣ 3.3 Narrowing anticipation space by using additional information ‣ 3
    Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation")). Mahmud et al. [[86](#bib.bib86)]
    proposed a network consisting of three branches to jointly learn both the future
    action label and the starting time using object features and motion-based action
    features. To align the three branches, a fully-connected layer was added on top
    of the concatenated outputs of all branches. Building upon the work of Furnari et al. [[98](#bib.bib98)],
    Zatsarynna et al. [[103](#bib.bib103)] considered the same three modalities in
    their work. They used temporal convolutional networks (TCN [[169](#bib.bib169)])
    to predict future representations independently for each input modality. Subsequently,
    the three independent representations were concatenated and fused via fully-connected
    layers. Final predictions were made based on the resulting fused representation.
  prefs: []
  type: TYPE_NORMAL
- en: Unlike the late fusion strategy used in [[103](#bib.bib103)], mid-level strategies
    usually first fuse different modalities without explicitly taking the temporal
    information into account, and then anticipate the next action with an additional
    anticipation module, as shown in Fig. [5(c)](#S3.F5.sf3 "In Figure 5 ‣ 3.3.1 Multi-modal
    fusion ‣ 3.3 Narrowing anticipation space by using additional information ‣ 3
    Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation"). While
    aggregating the concatenated multi-modal features with a fully-connected layer [[82](#bib.bib82),
    [48](#bib.bib48)] is a standard feature fusion strategy, Zhong et al. [[105](#bib.bib105)]
    built a transformer-based fusion module with a modality-agnostic token, similar
    to the [cls] token in vision transformers (ViT [[40](#bib.bib40)]). They achieved
    superior results compared to a score-fusion-based counterpart. In addition to
    the three modalities presented by Furnari et al. [[98](#bib.bib98)], i.e., appearance,
    motion, and object presence, they also incorporated an audio modality. However,
    it was demonstrated that audio provides comparatively less informative cues compared
    to the visual modalities. To predict the next action, the GPT-based anticipation
    module from [[67](#bib.bib67)] was used.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2 Conditioning on extra information
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Personalization. Zhou and Berg [[84](#bib.bib84)] explored two simple tasks
    related to temporal prediction in egocentric videos of everyday activities, pairwise
    temporal ordering and future video selection. They showed that personalization
    to a particular individual or environment provides significantly increased performance.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction time. As the recurrent approaches suffer from increasing inference
    time and error accumulation when predicting longer action sequences, some methods
    relied on parallel decoding by learning a sequence of action queries [[80](#bib.bib80),
    [82](#bib.bib82), [117](#bib.bib117)]. However, the number of predictable future
    actions was also limited to the number of action queries used in the training
    process. Ke et al. [[127](#bib.bib127)] took another approach and chose to condition
    on a time variable representing the prediction time. Specifically, they transformed
    the prediction time to a time representation, and concatenated it with the original
    inputs forming time-conditioned observations. Their model was therefore capable
    of anticipating a future action at arbitrary and variable time horizons in a one-shot
    fashion. Building upon a similar concept, Anticipatr [[117](#bib.bib117)] utilized
    a linear layer to convert learnable action queries, along with the anticipation
    duration, into time-conditioned queries. It employed a two-stage learning approach.
    In the first stage, a segment encoder is trained to predict the set of future
    action labels. In the second stage, a video encoder and an anticipation decoder
    were trained for making the final decision based on the input of the segment encoder.
    Anticipatr demonstrated state-of-the-art performance across multiple benchmarks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sequence-level information. A natural expectation of action anticipation is
    to make the correct anticipation as early as possible. However, a cross-entropy
    loss would not capture sequence-level distinctions, as it is calculated at each
    step to output higher confident scores on the ground truth category, and no sequence-level
    information is involved. Gao et al. [[45](#bib.bib45)] therefore proposed a reinforcement
    learning module to give high rewards to sequences that correspond to early correct
    predictions. Following a similar idea, Ng and Fernando [[122](#bib.bib122)] modified
    the cross-entropy loss between the prediction and the ground truth, taking the
    following considerations into account: 1) shorter observations contain less information
    and should therefore contribute less to the overall loss, and 2) a correct predicted
    action in the near future is important for predictions far into the future and
    should be emphasized by the loss function. Empirical results demonstrated the
    efficacy of the modified loss. Drawing inspiration from the human ability to infer
    the past based on the future, certain approaches [[112](#bib.bib112), [115](#bib.bib115)]
    have introduced a cycle consistency module. This module predicts past activities
    using the projected future and has demonstrated improved outcomes in comparison
    to its counterpart lacking the consistency module. Similarly, Fosco et al. [[170](#bib.bib170)]
    introduced the Event Transition Matrix (ETM) to incorporate statistical regularities
    into models. The ETM is computed from action labels in an untrimmed video dataset
    and represents the likelihood that a particular action is preceded or followed
    by any other action in the set. By incorporating ETM, the model can directly leverage
    this explicit representation of event transition likelihoods without having to
    learn it from scratch.'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic meaning of actions. An important aspect to consider when anticipating
    human actions is the future’s uncertainty since multiple actions may occur. However,
    most approaches used one-hot encoded labels to supervise the learning process,
    which may prevent the models from capturing the actual data distribution. Addressing
    this issue, Camporese et al. [[113](#bib.bib113)] proposed injecting semantic
    meaning of actions, e.g., in form of the similarity of word embeddings like GloVe [[171](#bib.bib171)],
    into the model via label smoothing techniques, which broadens the set of possible
    futures.
  prefs: []
  type: TYPE_NORMAL
- en: Spatio-temporal graph structure. When humans interact with objects, the interactions
    take place in both space and time. Modeling such a spatio-temporal graph structure,
    where the nodes of the graph typically represent the problem components and the
    edges capture their spatio-temporal interactions, has attracted much attention
    in recent years [[107](#bib.bib107), [162](#bib.bib162), [109](#bib.bib109)].
    While the concept is clear, the way to construct the graph and to update the node
    features is challenging. Jain et al. [[107](#bib.bib107)] employed multiple LSTMs
    for updating corresponding nodes or edges based on the concatenated message features.
    Qi et al. [[162](#bib.bib162)] updated the node features based on the sum of weighted
    messages, where the weights, i.e., the adjacency matrix, were also estimated and
    updated during optimization. Similar to [[162](#bib.bib162)], Sun et al. [[109](#bib.bib109)]
    estimated attention scores during optimization to aggregate all messages within
    a time step for the virtual nodes similar to a Graph Attention Network [[172](#bib.bib172)],
    which were then used to update node features in the next step. To anticipate the
    actor’s behavior, a GRU [[146](#bib.bib146)] was used for temporal modeling. By
    doing so, the spatial and temporal dependencies were isolated from each other.
    Li et al. [[114](#bib.bib114)] argued that such a setting may not be able to fully
    capture the spatial and temporal information due to the ignorance of their co-occurrence
    and strong correlations. Addressing this issue, they proposed a spatio-temporal
    relational network (STRN) by broadening the Convolutional Gated Recurrent Units
    (ConvGRU) [[173](#bib.bib173)], which can capture the spatial along with the temporal
    context.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/1d6690c1de057364e67bbbe782156c26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Abstraction level of activities.'
  prefs: []
  type: TYPE_NORMAL
- en: The underlying intention. To get something done, humans perform a sequence of
    actions dictated by an intention [[174](#bib.bib174)]. It is therefore advantageous
    to understand human intention to anticipate how a person would act in the future.
    Following this idea, some works [[175](#bib.bib175), [116](#bib.bib116), [118](#bib.bib118),
    [119](#bib.bib119)] built a top-down framework (refer to Fig. [6](#S3.F6 "Figure
    6 ‣ 3.3.2 Conditioning on extra information ‣ 3.3 Narrowing anticipation space
    by using additional information ‣ 3 Methods ‣ A Survey on Deep Learning Techniques
    for Action Anticipation")) to explicitly infer the intention and use the intention
    to constrain the variability of future actions. Specifically, Roy and Fernando [[175](#bib.bib175)]
    used a stacked LSTM to derive the latent goal from the observed visual representations,
    and exploited the concepts of goal closeness and goal consistency to guide the
    predictions. Goal closeness suggests actions should align with the latent goal,
    while goal consistency maintains this alignment over a sequence. Mascaro et al. [[118](#bib.bib118)]
    proposed estimating human intention with an MLP Mixer [[176](#bib.bib176)]. By
    taking the past actions, the estimated intention, as well as a latent vector sampled
    from a latent distribution based on the reparameterization trick from VAE as input,
    a transformer decoder was used to generate a sequence of future actions. Zhao et al. [[119](#bib.bib119)]
    proposed first recognizing human actions and then feeding the recognized actions
    as discretized labels to the ChatGPT [[177](#bib.bib177)] to get both the goal
    and future actions. Zatsarynna and Gall [[178](#bib.bib178)] integrated goal estimation
    into a multi-task framework, demonstrating improved performance compared to the
    counterpart that lacks goal estimation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4 Incorporating uncertainty
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Predicting future actions is inherently uncertain. Given an observed video segment
    containing an ongoing action, multiple actions could be possible to be the next
    action following the observed one. This uncertainty becomes even larger if we
    are going to predict far into the future. Therefore, it may be beneficial to model
    the underlying uncertainty, allowing to capture different possible future actions.
    However, action prediction is mostly treated as a classification problem and optimized
    using the cross-entropy loss, suffering from overly high resemblance to dominant
    ground truth while suppressing other reasonable possibilities [[179](#bib.bib179)].
    Moreover, approaches that are optimized using the mean square error tend to produce
    the mean of the modes [[4](#bib.bib4), [180](#bib.bib180)]. Therefore, some approaches
    output multiple possible future actions to be able to capture the underlying uncertainty.
  prefs: []
  type: TYPE_NORMAL
- en: 'Generating multiple outputs with multiple rules. To address the non-deterministic
    nature of future prediction, Vondrick et al. [[4](#bib.bib4)] extended their regression
    network to support multiple outputs by training a mixture of $K$ networks, where
    each mixture was trained to predict one of the modes in the future. As only one
    of the possible futures is provided in the dataset and it is unknown to which
    of the $K$ mixtures each data sample belongs, they overcame these issues by alternating
    between two steps: 1) optimizing the network while keeping the mixture assignment
    fixed, and 2) re-estimating the mixture assignment with the new network weights.
    During inference, the most likely action class is selected by marginalizing over
    the probability distributions of all modes. Following a similar idea, Piergiovanni et al. [[124](#bib.bib124)]
    proposed a differentiable grammar model which learns a set of production rules,
    being able to generate multiple candidate future sequences that follow a similar
    distribution of sequences seen during training. To avoid enumerating all possible
    rules which have exponential growth in time, they introduced adversarial learning
    for the grammar model, allowing for much more memory- and computationally-efficient
    learning without such enumeration.'
  prefs: []
  type: TYPE_NORMAL
- en: Generating multiple outputs by sampling from the learned distribution. Schydlo et al. [[21](#bib.bib21)]
    took human body pose features observed over three time steps as input into an
    encoder-decoder LSTM network to generate future action sequences. By sampling
    from the action probability distribution at each time step and using beam search,
    they made a final selection from a pool of action candidates during inference.
    Similarly, Farha and Gall [[48](#bib.bib48)] introduced a framework that predicts
    all subsequent actions and corresponding durations in a stochastic manner. In
    their framework, an action model similar to the one proposed in [[46](#bib.bib46)]
    and a time model were trained to predict the probability distribution of the future
    action label and duration, respectively. While the action model is trained using
    the cross-entropy loss, the durations were modeled with a Gaussian distribution
    and optimized with the negative log likelihood. At test time, a future action
    label and its duration were sampled from the learned distributions. Long-term
    predictions were achieved by feeding the predicted action segment to the model
    recursively.
  prefs: []
  type: TYPE_NORMAL
- en: Zhao and Wildes [[49](#bib.bib49)] addressed the same task, i.e., joint anticipation
    of long-term action labels and their corresponding times, by using Conditional
    Adversarial Generative Networks. Unlike Farha and Gall [[48](#bib.bib48)], they
    treated both action labels and time as discrete data, formatting them as one-hot
    vectors. This approach was inspired by research on recommendation systems, which
    found that modeling time with a categorical representation and optimizing with
    cross-entropy consistently outperformed the use of real-valued modeling [[181](#bib.bib181)].
    After projecting these vectors into higher-dimensional continuous spaces and concatenating
    them, they were fed into a seq2seq generator [[182](#bib.bib182)] to compute future
    action labels and their corresponding times. To enable differentiable sampling
    and generate future sequences with both quality and diversity during training,
    the Gumbel-Softmax relaxation technique [[150](#bib.bib150)], which mimics one-hot
    vectors from categorical distributions, and a normalized distance regularizer [[183](#bib.bib183)],
    which encourages diversity, were adopted. A ConvNet classifier was used as the
    discriminator to enable adversarial training of the generator.
  prefs: []
  type: TYPE_NORMAL
- en: Mehrasa et al. [[53](#bib.bib53)] proposed using a recurrent variational auto-encoder [[184](#bib.bib184)]
    (VAE) to capture the distribution over the times and categories of action sequences.
    To address the limitation that a fixed prior distribution of the latent variable
    (typically $\mathcal{N}(0,I)$ in VAE models) might ignore temporal dependencies
    present between actions, the authors developed a time-varying prior. At test time,
    a latent code was sampled from the learned prior distribution, and this code was
    then used to infer the probability distributions of the action class and its corresponding
    time.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5 Modeling at a more conceptual level
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Most action anticipation methods introduced in previous sections operate on
    the feature-level, i.e., the anticipation module receives a single or a sequence
    of feature vectors representing the observed video, and then predicts the future
    feature vectors that are further categorized to actions by a classifier. Some
    approaches go in the other direction. They first recognize the ongoing action(s)
    and then infer future actions directly. The intuition behind these approaches
    is also straightforward. As some actions are dependent on specific trigger actions,
    we can directly predict the next action, if one of the trigger actions has been
    observed. For example, if we have observed the action take glass, it is more likely
    to see an future action drink water. Instead of processing on feature-level, these
    approaches utilize action-level observations and model action dependencies directly.
  prefs: []
  type: TYPE_NORMAL
- en: Markov assumption. To this end, some methods make a Markov assumption on the
    sequence of performed actions and use a linear layer to model the transition between
    the past and the next action [[125](#bib.bib125), [128](#bib.bib128), [65](#bib.bib65)].
    The linear weights of the transition model can be interpreted as conveying the
    importance of each past action class for the anticipation. By analyzing the linear
    weights and the confidence vector of the current action, we can easily interpret
    which action class is most responsible for the anticipation and diagnose the source
    of false predictions, i.e., whether the false prediction is due to the recognition
    model or due to the learned transitional model.
  prefs: []
  type: TYPE_NORMAL
- en: Long-term anticipation. While the history of feature vectors contains rich semantic
    information, the action-level event history provides a different perspective yet
    perhaps a better analysis of the evolution of past actions, allowing to create
    a model for predicting future events occurring farther into the future [[74](#bib.bib74),
    [53](#bib.bib53)]. Consequently, many approaches utilize such action-level history
    to perform long-term anticipation [[46](#bib.bib46), [49](#bib.bib49), [53](#bib.bib53)].
    For instance, Farha et al. [[46](#bib.bib46)] introduced two methods for long-term
    action anticipation. One was based on an RNN model, which output the remaining
    length of the current action, the next action class, and its length. The long-term
    prediction was conducted recursively, i.e., observations were combined with the
    current prediction to produce the next prediction. Another method was based on
    a CNN model, which output a sequence of future actions in the form of a matrix
    in one single step.
  prefs: []
  type: TYPE_NORMAL
- en: Treating actions as events. A majority of data generated via human activities,
    e.g., running, cooking, etc., can be represented as a sequence of actions over
    a continuous time. These actions vary in their start and completion times depending
    on the user and the surrounding environment. Therefore, these continuous-time
    action sequences can be viewed as sparse events (action categories and their temporal
    occurrence), and modeled with classical event modeling methods such as temporal
    point processes (TPPs) [[185](#bib.bib185), [166](#bib.bib166)]. TPPs have shown
    a significant promise in modeling a variety of continuous-time sequences in healthcare [[186](#bib.bib186)],
    finance [[187](#bib.bib187)], education [[188](#bib.bib188)], and social networks [[189](#bib.bib189)].
  prefs: []
  type: TYPE_NORMAL
- en: 'In recent years, there are also some attempts to apply TPPs to action prediction.
    An initial effort in this direction [[35](#bib.bib35)] adopted the Poisson process [[185](#bib.bib185)]
    as a key technique for activity inter-arrival time modeling. Notice that inter-arrival
    time differs from the absolute activity occurrence time in that it stands for
    the time duration between consecutive activities. To support a stochastic modeling
    of time, the authors pre-defined a Gaussian Process prior for the intensity function
    and learned its parameters, i.e., the mean and variance for certain actions and
    times, from training data for every action. As TPPs can also be described using
    a counting process $C(t)$, the physical meaning of the intensity function is the
    rate of event arrivals:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{P}\bigl{(}dC(t)=1\bigr{)}=\lambda^{*}(t),$ |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: where $dC(t)=C(t+dt)-C(t)$ and ^∗ denotes a dependence on history. Then, they
    performed testing through importance sampling. As opposed to the constant intensity
    function in a Poisson process, an extension of the initial work calculated the
    intensity parameter dynamically from past observations [[53](#bib.bib53)]. Their
    model also supports stochastic time and action predictions via sampling latent
    variables through a Variational Auto-Encoder (VAE). Gupta and Bedathur [[129](#bib.bib129)]
    proposed learning the distribution of actions in a continuous-time action sequence
    using temporal normalizing flows (NF) [[190](#bib.bib190), [191](#bib.bib191)]
    conditioned on the dynamics of the sequence and the action features, e.g., minimum
    completion time. Such a flow-based formulation facilitates closed-form and faster
    sampling as well as more accurate predictions than the intensity-based models [[191](#bib.bib191),
    [192](#bib.bib192)]. In addition to action prediction, the authors also addressed
    goal prediction and sequence generation given the goal.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Evaluation Datasets and Metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Datasets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The algorithms for action anticipation are evaluated on a wide range of datasets
    that are temporally labeled with the corresponding action. To better differentiate
    current datasets, we consider several characteristics as follows.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE III: Representative benchmark datasets for human action anticipation.
    The following abbreviations are used in the table. Type: Activities of daily living
    (ADL). Sensor modality: Depth (D), Au (Audio). Comp. act.: Composite activities.
    Conc. act.: Concurrent activities. Spont.: Spontaneity.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Dataset | Year | Type | View | Multi-view | Comp. act. | Conc. act. | Spont.
    | Hours | Segments | Classes | Sensor modality |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| TV-I [[57](#bib.bib57)] | 2010 | Movie | Shooting | ✗ | ✗ | ✗ | Medium |
    – | 300 | 4 | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| 50Salads [[70](#bib.bib70)] | 2013 | Cooking | Top-view | ✗ | ✓ | ✗ | Medium
    | 4.5 | 966 | 17 | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| CAD-120 [[108](#bib.bib108)] | 2013 | ADL | Shooting | ✗ | ✓ | ✗ | Low |
    0.57 | 120 | 12 | RGB, D |'
  prefs: []
  type: TYPE_TB
- en: '| Breakfast [[66](#bib.bib66)] | 2014 | Cooking | Shooting | ✓ | ✓ | ✗ | Medium
    | 77 | 11.2K | 48 | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| THUMOS14 [[59](#bib.bib59)] | 2014 | Web | Shooting | ✗ | ✗ | ✗ | Medium
    | 20 | 6.3K | 20 | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| Charades [[123](#bib.bib123)] | 2016 | ADL | Shooting | ✗ | ✗ | ✓ | Low |
    82.3 | 66.5K | 157 | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| TVSeries [[62](#bib.bib62)] | 2016 | Movie | Shooting | ✗ | ✗ | ✗ | Medium
    | 16 | 6.2K | 30 | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| EGTEA Gaze+[[69](#bib.bib69)] | 2018 | Cooking | Egocentric | ✗ | ✗ | ✗ |
    Medium | 28 | 10.3K | 106 | RGB, Gaze |'
  prefs: []
  type: TYPE_TB
- en: '| EpicKitchens-50[[7](#bib.bib7)] | 2018 | Cooking | Egocentric | ✗ | ✗ | Few
    | High | 55 | 39.6K | 2,513 | RGB, Au |'
  prefs: []
  type: TYPE_TB
- en: '| AVA [[111](#bib.bib111)] | 2018 | Movie | Shooting | ✗ | ✗ | ✓ | Medium |
    107.5 | 38.6K | 60 | RGB |'
  prefs: []
  type: TYPE_TB
- en: '| EpicKitchens-100 [[68](#bib.bib68)] | 2020 | Cooking | Egocentric | ✗ | ✗
    | ✓ | High | 100 | 90K | 3,807 | RGB, Au |'
  prefs: []
  type: TYPE_TB
- en: '| Ego4D [[47](#bib.bib47)] | 2022 | ADL | Egocentric | ✗ | ✗ | ✓ | High | 243
    | – | 4,756 | RGB, Au |'
  prefs: []
  type: TYPE_TB
- en: '| Assembly101 [[193](#bib.bib193)] | 2022 | ADL | Multi-view | ✓ | ✓ | ✗ |
    Medium | 513 | 1M | 1,380 | RGB, Gray, Pose |'
  prefs: []
  type: TYPE_TB
- en: 'Spontaneous behavior: Activities in real-world scenarios are performed naturally,
    yet many existing datasets involve subjects following some scripts or certain
    activities, which may oversimplify the reality.'
  prefs: []
  type: TYPE_NORMAL
- en: 'View-point: Although certain datasets, such as EpicKichens [[7](#bib.bib7),
    [68](#bib.bib68)] and Ego4D [[47](#bib.bib47)], provide recordings from an egocentric
    viewpoint, the majority of the utilized action anticipation datasets are derived
    from a third-person perspective where the subjects are typically centrally positioned.
    Conversely, datasets captured by automated monitoring systems employing fixed
    cameras may exhibit subjects that are off-center, occluded, or partially outside
    the field of view. Given the need for methods to exhibit robustness in the face
    of viewpoint variations, we also provide the information if samples captured from
    diverse perspectives are included within benchmark datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Composite activities: Certain elementary activities can compose complex activities,
    forming a scenario as illustrated in Fig. [6](#S3.F6 "Figure 6 ‣ 3.3.2 Conditioning
    on extra information ‣ 3.3 Narrowing anticipation space by using additional information
    ‣ 3 Methods ‣ A Survey on Deep Learning Techniques for Action Anticipation").
    For instance, cutting bread, spreading butter, and eating at a table may indicate
    a having breakfast scenario. It is therefore advantageous to include these composite
    activities in the dataset as well.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Concurrent activities: Simultaneous performance of activities, such as reading
    and listening to music, is common in daily life. Incorporating concurrent activities
    within the dataset enhances its reflection of real-world scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: Table [III](#S4.T3 "TABLE III ‣ 4.1 Datasets ‣ 4 Evaluation Datasets and Metrics
    ‣ A Survey on Deep Learning Techniques for Action Anticipation") presents a comprehensive
    overview, comparing the essential attributes of prominent public untrimmed video
    datasets commonly employed for assessing action anticipation methods. This comparison
    encompasses the aforementioned criteria in conjunction with dataset sizes. Subsequently,
    we provide an in-depth exploration of these datasets, delving into their specific
    details.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Web & Movie datasets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Several widely used benchmark datasets are collected from web or movies [[57](#bib.bib57),
    [59](#bib.bib59), [62](#bib.bib62), [111](#bib.bib111)]. The TV-Interaction dataset [[57](#bib.bib57)]
    contains 300 videos clips with human interactions. These videos are categorized
    into 4 interaction categories: handshake, high five, hug, and kiss, and annotated
    with the upper body of people, discrete head orientation and interaction. THUMOS14 [[59](#bib.bib59)]
    is a popular benchmark for temporal action detection and anticipation [[45](#bib.bib45),
    [76](#bib.bib76), [80](#bib.bib80), [83](#bib.bib83)]. It contains over 20 hours
    of sport videos annotated with 20 actions. Since the training set contains only
    trimmed videos that cannot be used to train action anticipation models, prior
    works usually use the validation set (including 3K action instances in 200 untrimmed
    videos) for training and the test set (including 3.3K action instances in 213
    untrimmed videos) for evaluation. TVSeries [[62](#bib.bib62)] contains video footage
    from six popular TV series, about 150 minutes for each and about 16 hours in total.
    The dataset totally includes 30 realistic, everyday actions (e.g., pick up, open
    door, drink, etc.), and every action occurs at least 50 times in the dataset.
    TVSeries contains many unconstrained perspectives and a wide variety of backgrounds.
    AVA [[111](#bib.bib111)] provides audio-visual annotations for about 15 minute
    long movie clips. The AVA Action subset contains 430 videos split into 235 for
    training, 64 for validation, and 131 for testing, covering 60 atomic action classes.
    The videos are annotated in a 1 second interval.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Cooking activities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One popular category of datasets in this domain contains videos of cooking
    activities: Epic-Kitchens [[7](#bib.bib7), [167](#bib.bib167)], EGTEA Gaze+ [[69](#bib.bib69)],
    Breakfast [[66](#bib.bib66)], and 50Salads [[70](#bib.bib70)]. Among these, Epic-Kitchens
    and EGTEA Gaze+ are egocentric (first-person) datasets. The videos are recorded
    with a wearable camera that captures the scene directly in front of the user,
    in which only hands are visible in the center of the camera view. EpicKitchens-100
    consists of 700 long unscripted videos of cooking activities totalling 100 hours.
    It contains 90K action annotations, 97 verbs, and 300 nouns. Considering all unique
    (verb, noun) pairs in the public training set yields 3,806 unique actions. EpicKitchens-55
    is an earlier version of the EpicKitchens-100 with 39,596 segments labeled with
    125 verbs, 352 nouns, and 2,513 combinations (actions), totalling 55 hours. EGTEA
    Gaze+ contains 28 hours of videos including 10.3K action annotations, 19 verbs,
    51 nouns, and 106 unique actions.'
  prefs: []
  type: TYPE_NORMAL
- en: Breakfast [[66](#bib.bib66)], and 50Salads [[70](#bib.bib70)] contain cooking
    activities like making breakfast, or preparing salads. Subjects are asked to prepare
    a specific recipe in each video. The activities are therefore performed without
    hesitation or mistakes (reduced spontaneity). Moreover, the datasets lack the
    presence of other activities that are irrelevant to cooking, e.g. drinking water,
    but often occur in real-life. The Breakfast dataset comprises 1,712 videos of
    52 different individuals making breakfast in 18 different kitchens, totalling
    77 hours. Every video is categorized into one of the 10 activities related to
    breakfast preparation. The videos are annotated by 48 fine-grained actions. The
    50Salads dataset comprises 50 top-view videos of 25 people preparing a salad.
    The dataset contains over 4 hours of RGB-D video data, annotated with 17 fine-grained
    action labels and 3 high-level activities.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3 Other daily living activities
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: This section presents datasets of other activities of daily living (ADL) that
    are not focussed on cooking. CAD-120 [[108](#bib.bib108)] is a small-sized dataset
    (about 60K frames in total). This dataset comprises 120 RGB-D action videos covering
    12 daily activities, which are captured using the Kinect sensor. Action videos
    are performed by 4 subjects following a script in different rooms. The Charades [[123](#bib.bib123)]
    dataset is recorded by hundreds of people in their private homes following strict
    scripts. It has 7,985 videos for training and 1,863 videos for testing. The dataset
    is collected in 15 types of indoor scenes, involves interactions with 46 object
    classes and has a vocabulary of 30 verbs leading to 157 action classes. On average
    there are 6.8 actions per video.
  prefs: []
  type: TYPE_NORMAL
- en: Ego4D [[47](#bib.bib47)] is the most extensive daily-life egocentric video dataset,
    currently available to research. The forecasting benchmark from Ego4D consists
    of 120 hours of annotated videos from 53 different scenarios. The annotations
    provided contain 478 noun types and 115 verb types, with a total amount of 4756
    action classes among training and validation set. The number of training and validation
    samples has doubled in the second version.
  prefs: []
  type: TYPE_NORMAL
- en: The Assembly101 [[193](#bib.bib193)] dataset features 4321 videos of participants
    assembling and disassembling 101 toy vehicles. With no fixed guidelines, this
    led to varied action sequences, mistakes, and corrections. Assembly101, unique
    for its multi-view recordings, has 8 static and 4 egocentric cameras. It contains
    513 hours of footage with over 100K coarse and 1M fine-grained segments, categorized
    into 1380 fine-grained and 202 coarse classes. The static cameras record in RGB,
    while the egocentric ones are in monochrome.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Evluation metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Top-k accuracy is widely used to evaluate the overall performance of action
    anticipation methods. It is computed by checking if the ground truth label is
    among the top-k predictions. Accounting for the class imbalance in a long-tail
    distribution, many methods tend to use class-aware measures such as Class Mean
    Top-k Accuracy to evaluate the performance. Note that some works [[194](#bib.bib194),
    [98](#bib.bib98)] refer to this metric as Mean Top-k Recall. In the case of long-term
    anticipation, where predictions for a large number of frames are made, some works [[46](#bib.bib46),
    [127](#bib.bib127), [5](#bib.bib5), [82](#bib.bib82)] report the class mean top-1
    accuracy of the predicted frames (Mean over Classes (MoC)) as the evaluation metric.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of multiple step predictions, the Mean Average Precision (mAP) [[121](#bib.bib121)]
    and its augmented version Calibrated Average Precision (cAP) [[62](#bib.bib62)]
    are widely used. After average precision (AP) is calculated for each action class
    based on the following equation,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\mathrm{AP}=\frac{\sum_{p}\mathrm{Prec}(p)\times\mathrm{I}(p)}{\sum\mathrm{TP}},$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: 'where I($p$) is equal to 1 if frame $p$ is a true positive (TP) and 0 otherwise,
    the mean average precision is then computed by taking the mean over all action
    classes. Calibrated average precision is an augmented version of the mAP, accounting
    for the class imbalance phenomenon. It uses a parameter $w$, which is the ratio
    between negative frames and positive frames in the calculation of precision, so
    that the average precision is calculated as if there were an equal amount of positive
    and negative frames:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\mathrm{cPrec}$ | $\displaystyle=\frac{\mathrm{TP}}{\mathrm{TP}+\frac{\mathrm{FP}}{w}}$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: '|  | $\displaystyle\mathrm{cAP}$ | $\displaystyle=\frac{\sum_{p}\mathrm{cPrec}(p)\times\mathrm{I}(p)}{\sum\mathrm{TP}}.$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: As treating predictions for each future time-step independently when calculating
    accuracy does not account for the sequential nature of the prediction task where
    the order of predictions is important, Edit Distance (ED), computed as the Damerau-Levenshtein
    distance [[195](#bib.bib195), [196](#bib.bib196)], is employed to evaluate the
    predicted action sequences [[47](#bib.bib47)]. The goal of this measure is to
    assess performance in a way which is robust to some error in the predicted order
    of future actions. A predicted verb/noun is considered ”correct” if it matches
    the ground truth verb label at a specific timestep. The allowed operations to
    compute the edit distance are insertions, deletions, substitutions and transpositions
    of any two predicted actions. The lower the ED is the more similar are the anticipated
    sequences to the ground-truth.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Benchmark Protocols and Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.1 Short-term anticipation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE IV: Comparison of the state-of-the-art methods addressing predictions
    at multiple timestamps on TVSeries [[62](#bib.bib62)] (mean cAP %) and THUMOS-14 [[59](#bib.bib59)]
    (mAP %). The optimal performance in each column within every block is indicated
    in bold, while the supreme overall performance in each column is further denoted
    by an underscore.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Back- bone | Pre-train | TVSeries | THUMOS14 |'
  prefs: []
  type: TYPE_TB
- en: '| 1.0 | 2.0 | Avg. | 1.0 | 2.0 | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| ED [[45](#bib.bib45)] | 2017 | VGG | IN1K | 68.8 | 66.7 | 68.7 | – | – |
    – |'
  prefs: []
  type: TYPE_TB
- en: '| RED [[45](#bib.bib45)] | 2017 | 70.2 | 66.8 | 69.4 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| TTPP[[76](#bib.bib76)] | 2020 | 71.6 | 69.3 | 71.3 | – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| ED [[45](#bib.bib45)] | 2017 | TS | ANet1.3 | 74.6 | 71.0 | 74.5 | 36.8 |
    31.6 | 36.6 |'
  prefs: []
  type: TYPE_TB
- en: '| RED [[45](#bib.bib45)] | 2017 | 75.5 | 71.2 | 75.1 | 37.5 | 32.1 | 37.5 |'
  prefs: []
  type: TYPE_TB
- en: '| TRN [[75](#bib.bib75)] | 2019 | 75.9 | 72.3 | 75.7 | 39.1 | 34.3 | 38.9 |'
  prefs: []
  type: TYPE_TB
- en: '| TTPP[[76](#bib.bib76)] | 2020 | 77.6 | 74.9 | 77.9 | 41.0 | 37.3 | 40.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LAP [[77](#bib.bib77)] | 2020 | 78.9 | 75.5 | 78.7 | 43.2 | 37.0 | 42.6 |'
  prefs: []
  type: TYPE_TB
- en: '| OadTR[[80](#bib.bib80)] | 2021 | 78.2 | 74.3 | 77.8 | 46.8 | 41.1 | 45.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| LSTR [[79](#bib.bib79)] | 2021 | – | – | 80.8 | – | – | 50.1 |'
  prefs: []
  type: TYPE_TB
- en: '| TeSTra [[83](#bib.bib83)] | 2022 |  |  | – | – | – | 55.7 | 47.8 | 55.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TTPP [[76](#bib.bib76)] | 2020 |  |  | – | – | – | 43.6 | 38.7 | 42.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LSTR [[79](#bib.bib79)] | 2021 |  |  | – | – | – | 53.3 | 45.7 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '| OadTR[[80](#bib.bib80)] | 2021 | TS | K400 | 80.1 | 75.7 | 79.1 | 54.6 |
    46.8 | 53.5 |'
  prefs: []
  type: TYPE_TB
- en: '| TeSTra [[83](#bib.bib83)] | 2022 |  |  | – | – | – | 57.4 | 48.9 | 56.8 |'
  prefs: []
  type: TYPE_TB
- en: 'Short-term action anticipation can be broadly classified into two types: future
    action predictions at multiple timestamps and single future action prediction.
    The former frequently employs datasets that incorporate third-person perspectives,
    such as TVSeries [[62](#bib.bib62)] and THUMOS14 [[59](#bib.bib59)] (Section [5.1.1](#S5.SS1.SSS1
    "5.1.1 TVSeries and THUMOS14 ‣ 5.1 Short-term anticipation ‣ 5 Benchmark Protocols
    and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation")),
    while the latter is typically evaluated using egocentric datasets such as EpicKitchens [[68](#bib.bib68)]
    (Section [5.1.2](#S5.SS1.SSS2 "5.1.2 EpicKitchens-100 ‣ 5.1 Short-term anticipation
    ‣ 5 Benchmark Protocols and Results ‣ A Survey on Deep Learning Techniques for
    Action Anticipation")).'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.1 TVSeries and THUMOS14
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Table [IV](#S5.T4 "TABLE IV ‣ 5.1 Short-term anticipation ‣ 5 Benchmark Protocols
    and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation") showcases
    the performance of approaches that address predictions at multiple timestamps,
    with results detailed at times 1.0 and 2.0 seconds. Comprehensive results spanning
    0.25 to 2.0 seconds can be found in the supplementary material. The chosen evaluation
    metrics for TVSeries and THUMOS14 are mean calibrated average precision (mcAP)
    and mean average precision (mAP), respectively, as detailed in Section [4.2](#S4.SS2
    "4.2 Evluation metrics ‣ 4 Evaluation Datasets and Metrics ‣ A Survey on Deep
    Learning Techniques for Action Anticipation"). The backbones adopted by the methods
    range extensively from pre-trained ImageNet-1K (IN1K) models like VGG-16 [[37](#bib.bib37)]
    to two-stream video models [[61](#bib.bib61)] (TS), pre-trained either on ActivityNet [[126](#bib.bib126)]
    (ANet1.3) or Kinetics [[41](#bib.bib41)] (K400). Within these two-stream models,
    the spatial and temporal subnetworks typically employ ResNet [[38](#bib.bib38)]
    and BN-Inception [[197](#bib.bib197)] separately.
  prefs: []
  type: TYPE_NORMAL
- en: Considering results on TVSeries, TTPP [[76](#bib.bib76)], which utilizes VGG-16
    as the backbone, offers the best performance, boasting an average mcAP of 71.3%.
    When transitioning to two-stream models pre-trained on ActivityNet, performance
    generally improves. For instance, LSTR [[79](#bib.bib79)], which only provides
    an average score, surpasses others with an mcAP of 80.8%. Pre-training with a
    more extensive dataset such as Kinetics enhances performance further, as demonstrated
    by OadTR [[80](#bib.bib80)], which reports a 1.3% improvement in the average score
    ($77.8\%\rightarrow 79.1\%$). Parallel observations can be made on THUMOS14\.
    The state-of-the-art method TeSTra [[83](#bib.bib83)] achieves an average mAP
    of 55.3%, when pre-trained on ActivityNet, and exhibits an increase of 1.5% when
    Kinetics is used for pre-training. Additionally, we observe that transformer-based
    approaches, such as OadTR, LSTR, and TeSTra, demonstrate superior results compared
    to non-transformer-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1.2 EpicKitchens-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'TABLE V: Comparison of state-of-the-art methods on the validation and test
    set of EpicKitchens-100 [[68](#bib.bib68)] in terms of mean top-5 recall (%).
    The optimal performance in each column within every block is indicated in bold.
    Additional modalities to the RGB modality: Objects (O), Bounding Boxes (BB), Motion
    (M), Audio (Au).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Year | Addl. modality | Backbone | Init. | E2E | Overall |'
  prefs: []
  type: TYPE_TB
- en: '| Verb | Noun | Act. |'
  prefs: []
  type: TYPE_TB
- en: '| Val | RULSTM [[98](#bib.bib98)] | 2019 | – | TSN | IN1K | ✗ | 27.5 | 29.0
    | 13.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | – | TSN | IN1K | ✗ | 24.2 | 29.8 | 13.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| AVT [[67](#bib.bib67)] | 2021 | – | TSN | IN1K | ✗ | 27.2 | 30.7 | 13.6 |'
  prefs: []
  type: TYPE_TB
- en: '| AVT [[67](#bib.bib67)] | 2021 | – | ViT-B | IN21K | ✓ | 30.2 | 31.7 | 14.9
    |'
  prefs: []
  type: TYPE_TB
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | – | MViT-B | K400 | ✓ | 32.8 | 33.2 | 15.1
    |'
  prefs: []
  type: TYPE_TB
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | – | MViT-L | K700 | ✓ | 32.2 | 37.0 | 17.7
    |'
  prefs: []
  type: TYPE_TB
- en: '| DCR [[71](#bib.bib71)] | 2022 | – | TSM | K400 | ✗ | 32.6 | 32.7 | 16.1 |'
  prefs: []
  type: TYPE_TB
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | – | TSN | IN1K | ✗ | 26.8 | 36.2 | 17.0
    |'
  prefs: []
  type: TYPE_TB
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K400 | ✗ | 33.3 | 35.5
    | 17.6 |'
  prefs: []
  type: TYPE_TB
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K700 | ✗ | 33.7 | 37.1
    | 18.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | RULSTM [[98](#bib.bib98)] | 2019 | O,M | TSN | IN1K | ✗ | 27.8 | 30.8
    | 14.0 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TempAgg [[5](#bib.bib5)] | 2020 | O,M,BB | TSN | IN1K | ✗ | 23.2 | 31.4
    | 14.7 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AVT [[67](#bib.bib67)] | 2021 | O | ViT-B | IN21K | ✓ | 28.2 | 32.0 |
    15.9 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DCR [[71](#bib.bib71)] | 2022 | O | TSM | K400 | ✗ | – | – | 18.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TeSTra [[83](#bib.bib83)] | 2022 | M | TSN | IN1K | ✗ | 30.8 | 35.8 |
    17.6 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AFFT [[105](#bib.bib105)] | 2023 | O,M | TSN | IN1K | ✗ | 21.3 | 32.7
    | 16.4 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AFFT [[105](#bib.bib105)] | 2023 | O,M,Au | Swin-B | K400 | ✗ | 22.8 |
    34.6 | 18.5 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | RULSTM [[98](#bib.bib98)] | 2019 | O,M | TSN | IN1K | ✗ | 25.3 | 26.7
    | 11.2 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | O,M,BB | TSN | IN1K | ✗ | 21.8 | 30.6 |
    12.6 |'
  prefs: []
  type: TYPE_TB
- en: '| AVT [[67](#bib.bib67)] | 2021 | O | ViT-B | IN21K | ✓ | 25.6 | 28.8 | 12.6
    |'
  prefs: []
  type: TYPE_TB
- en: '| TCN-TBN [[103](#bib.bib103)] | 2021 | O,M | TBN | IN1K | ✗ | 21.5 | 26.8
    | 11.0 |'
  prefs: []
  type: TYPE_TB
- en: '| Abst. goal [[116](#bib.bib116)] | 2022 | O,M | TSN | IN1K | ✗ | 31.4 | 30.1
    | 14.3 |'
  prefs: []
  type: TYPE_TB
- en: '| AFFT [[105](#bib.bib105)] | 2023 | O,M,Au | Swin-B | K400 | ✗ | 20.7 | 31.8
    | 14.9 |'
  prefs: []
  type: TYPE_TB
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K400 | ✗ | 27.3 | 32.8
    | 14.0 |'
  prefs: []
  type: TYPE_TB
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | – | MViT-B | K700 | ✗ | 27.4 | 34.0
    | 14.7 |'
  prefs: []
  type: TYPE_TB
- en: Table [V](#S5.T5 "TABLE V ‣ 5.1.2 EpicKitchens-100 ‣ 5.1 Short-term anticipation
    ‣ 5 Benchmark Protocols and Results ‣ A Survey on Deep Learning Techniques for
    Action Anticipation") provides a detailed study of various state-of-the-art methods
    on the validation and test sets of EpicKitchens-100 [[68](#bib.bib68)]. It shows
    the overall results, while results for unseen kitchen and tail classes are available
    in the supplementary material. Each of these benchmarks is evaluated based on
    the top-5 recall for verbs, nouns, and actions across all classes. The primary
    metric used to rank the methods is accentuated. If the anticipation approach employs
    a backbone fine-tuned for feature extraction, it is marked with a check mark in
    the column titled E2E in Table [V](#S5.T5 "TABLE V ‣ 5.1.2 EpicKitchens-100 ‣
    5.1 Short-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A Survey on
    Deep Learning Techniques for Action Anticipation"). We also report the modalities
    used by each method, providing a comprehensive analysis of the potential gains
    of multi-modal methods and facilitating a fair comparison across different approaches.
  prefs: []
  type: TYPE_NORMAL
- en: When examining the validation set, RAFTformer and MeMViT show the best overall
    performance with a single RGB modality. Specifically, RAFTformer, equipped with
    the MViT-B backbone and initialized with the Kinetics-700 [[198](#bib.bib198)]
    (K700) dataset, exhibits superior performance. Several modalities are typically
    employed for egocentric vision following [[98](#bib.bib98)], including RGB, object
    presence (O), and optical flow (M). Some methods extend this set of modalities
    to include additional features like interacting hand-object bounding boxes (BB) [[5](#bib.bib5)]
    and audio (Au) [[105](#bib.bib105)]. Multi-modal incorporation proves to be highly
    effective for action anticipation. The fusion of these different modalities plays
    a significant role, as seen in the case of AFFT [[105](#bib.bib105)], which surpasses
    the earlier state-of-the-art, RULSTM [[98](#bib.bib98)], significantly on the
    validation set using the exact same features ($14.0\rightarrow 16.4$). Regarding
    backbones, Transformer-based models like ViT, MViT, and Swin consistently outperform
    models such as TSN or TSM [[72](#bib.bib72)], particularly when additional modalities
    are used, and end-to-end training is applied. For the initialization datasets,
    larger datasets like ImageNet-21K [[199](#bib.bib199)] (IN21K), Kinetics-400 (K400),
    Kinetics-700 (K700) generally deliver better performance than those initialized
    with the smaller ImageNet-1K (IN1K) dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of the test set, only peer-reviewed results from approaches using
    standard training sets are included. Results originating from the EpicKitchens-Challenge
    that lack peer-reviewed validation are excluded. The challenge submissions often
    use ensembles of various methods or combine the training and validation set for
    training. AFFT with the Swin-B backbone achieves the highest score, while the
    RAFTformer model shows similar performance using only RGB as modality.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Long-term anticipation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'TABLE VI: Benchmark of long-term action anticipation on Breakfast [[66](#bib.bib66)]
    and 50Salads [[70](#bib.bib70)] in terms of mean over classes (%). For details
    on the font coding, please refer to Table [IV](#S5.T4 "TABLE IV ‣ 5.1 Short-term
    anticipation ‣ 5 Benchmark Protocols and Results ‣ A Survey on Deep Learning Techniques
    for Action Anticipation").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input Type | Backbone | Methods | Year | Breakfast $\beta$ ($\alpha$ = 0.3)
    | 50Salads $\beta$ ($\alpha$ = 0.3) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 0.2 | 0.3 | 0.5 | 0.1 | 0.2 | 0.3 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GT. label | – | RNN [[46](#bib.bib46)] | 2018 | 61.45 | 50.25 | 44.90 | 41.75
    | 44.19 | 29.51 | 19.96 | 10.38 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN [[46](#bib.bib46)] | 2018 | 60.32 | 50.14 | 45.18 | 40.51 | 37.36 | 24.78
    | 20.78 | 14.05 |'
  prefs: []
  type: TYPE_TB
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 65.95 | 55.94 | 49.14 | 44.23 |
    46.40 | 34.80 | 25.24 | 13.84 |'
  prefs: []
  type: TYPE_TB
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 51.25 | 42.94 | 38.33 | 33.07 | 33.15
    | 24.65 | 18.84 | 14.34 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 67.40 | 56.10 | 47.40 | 41.50 | 44.80 |
    32.70 | 23.50 | 15.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[49](#bib.bib49)] (avg.) | 2020 | 74.14 | 71.32 | 65.30 | 52.38
    | 46.13 | 36.37 | 33.10 | 19.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Pred. label | Fisher | RNN [[46](#bib.bib46)] | 2018 | 21.64 | 20.02 | 19.73
    | 19.21 | 30.77 | 17.19 | 14.79 | 9.77 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN [[46](#bib.bib46)] | 2018 | 22.44 | 20.12 | 19.69 | 18.76 | 29.14 | 20.14
    | 17.46 | 10.86 |'
  prefs: []
  type: TYPE_TB
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 19.14 | 17.18 | 17.38 | 14.98 | 28.04
    | 17.95 | 14.77 | 12.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 22.75 | 20.44 | 19.64 | 19.75 |
    35.12 | 27.05 | 22.05 | 15.59 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 23.00 | 20.00 | 19.90 | 18.60 | 32.30 |
    25.50 | 22.70 | 17.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Piergiovanni et al. [[124](#bib.bib124)] | 2020 | - | - | - | - | 40.70 |
    40.10 | 26.40 | 19.20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 39.50 | 34.10 | 31.00 | 27.90
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Features | Fisher | CNN [[46](#bib.bib46)] | 2018 | 17.72 | 16.87 | 15.48
    | 14.09 | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 19.50 | 17.00 | 15.60 | 15.10 | 30.60 |
    22.50 | 19.10 | 11.20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 30.40 | 26.30 | 23.80 | 21.20
    | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '|  | Cycle Cons[[112](#bib.bib112)] | 2020 | 29.66 | 27.37 | 25.58 | 25.20
    | 34.39 | 23.70 | 18.95 | 15.89 |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-ACT [[115](#bib.bib115)] | 2022 | 30.80 | 28.30 | 26.10 | 25.80 | 35.70
    | 25.30 | 20.10 | 16.30 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FUTR [[82](#bib.bib82)] | 2022 | 32.27 | 29.88 | 27.49 | 25.87 | 35.15
    | 24.86 | 24.22 | 15.26 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Anticipatr [[117](#bib.bib117)] | 2022 | 39.90 | 35.70 | 32.10 | 29.40
    | 42.80 | 42.30 | 28.50 | 23.60 |'
  prefs: []
  type: TYPE_TB
- en: The current long-term action anticipation approaches are primarily evaluated
    on the Breakfast [[66](#bib.bib66)] and 50Salads [[70](#bib.bib70)] datasets using
    mean over classes accuracy (see Sec. [4.2](#S4.SS2 "4.2 Evluation metrics ‣ 4
    Evaluation Datasets and Metrics ‣ A Survey on Deep Learning Techniques for Action
    Anticipation")), averaged over future timestamps within a defined anticipation
    duration. Actions are typically predicted after observing the first part ($\alpha$)
    of a video, with benchmarks from[[46](#bib.bib46)] setting $\alpha$ at 0.2 or
    0.3\. Predictions then span segments $\beta$ of the entire video, with $\beta=\{0.1,0.2,0.3,0.5\}$.
    Due to space limits, only results with $\alpha=0.3$ are shown in Table [VI](#S5.T6
    "TABLE VI ‣ 5.2 Long-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A
    Survey on Deep Learning Techniques for Action Anticipation"). Complete results
    are in the supplementary material. Notably, the evaluation protocol of Anticipatr
    differs slightly, as they assessed predictions over $\beta$ portions of the remaining
    video segments. We therefore mark their results in Table [VI](#S5.T6 "TABLE VI
    ‣ 5.2 Long-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A Survey on
    Deep Learning Techniques for Action Anticipation") with gray font. Additionally,
    we observe that the results of FUTR [[82](#bib.bib82)] on 50Salads deviate slightly
    from those reported on the official GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'Three primary types of inputs are commonly utilized: ground truth video segmentations,
    frame-level features such as Fisher vectors [[200](#bib.bib200)] or I3D features [[41](#bib.bib41)],
    and video segmentations inferred from these features through segmentation models
    such as [[50](#bib.bib50)]. Using ground truth annotations facilitates a comprehensive
    analysis of the prediction capabilities of anticipation models. However, employing
    raw features or predicted segmentations reveals the real performance, and thus
    introduces more challenges as potential errors are carried forward into future
    predictions.'
  prefs: []
  type: TYPE_NORMAL
- en: Findings from the Breakfast dataset reveal that methods using ground truth action
    observations as input deliver superior results compared to those utilizing features
    or predicted video segmentations. This suggests that accurately recognizing past
    actions is crucial for future action anticipation. Furthermore, models using I3D
    features exhibit significantly better outcomes than those leveraging Fisher vectors,
    indicating the beneficial role of strong backbones. Different from Breakfast,
    on the 50Salads dataset, the drop in accuracy of models using segmentation predictions
    compared to those using perfect observations is surprisingly small. This can be
    partially attributed to the strong performance of the segmentation decoder [[50](#bib.bib50)]
    and the pronounced inter-class dependencies of the 50Salads dataset, which facilitates
    the learning of coherent action sequences [[46](#bib.bib46)].
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, earlier approaches that are based on the predicted video segmentations
    considerably outperform the direct prediction from features. For instance, Temporal
    Agg. [[5](#bib.bib5)] gains increases of up to +7% on Breakfast using I3D features
    and +6% on 50Salads using Fisher vectors with $\alpha=0.3$ and $\beta=0.5$. Direct
    future prediction from features poses a challenge as it requires the model to
    recognize observed actions and extract relevant information for future anticipation
    simultaneously. Conversely, approaches based on predicted video segmentations
    decouple these tasks, employing a powerful decoding model to recognize observed
    actions and confining the future predictor to capture context over action classes
    rather than frame-wise features. However, separating the understanding of the
    past and the anticipation of future also leads to several disadvantages. First,
    the temporal action segmentation model is not optimized for the anticipation task.
    Second, any mistakes in the temporal action segmentation will be propagated and
    affect the anticipation results. Finally, since ground truth action labels are
    one-hotted vectors, they contain less information than the spatiotemporal features [[112](#bib.bib112)].
    When the semantic information contained in the features is effectively utilized,
    approaches that directly make predictions from these features may yield better
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Research Challenges and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Despite the abundance of current action anticipation approaches and the substantial
    advancements that have been achieved in this field, there is still potential for
    enhancing state-of-the-art algorithms. In the following discussion, we address
    the research challenges and suggest several promising directions for future research.
  prefs: []
  type: TYPE_NORMAL
- en: 'Expanding dataset coverage: Current anticipation methods are mainly trained
    and evaluated on datasets with a very restricted scene context and perspective,
    such as Breakfast [[66](#bib.bib66)] and EpicKitchens [[68](#bib.bib68)]. The
    limitations of these datasets restrict the ability of these methods to generalize
    to other scenarios. To make the anticipation methods applicable in our everyday
    life, the development of more comprehensive datasets is crucial. The optimal datasets
    should cover a wide range of daily life scenarios and include diverse actions
    across different cultures, geographies, and social contexts. While Ego4D [[47](#bib.bib47)]
    is a very good example for diversity, it is limited to egocentric views. Studying
    anticipation across different tasks and geographic locations is a promising research
    directions, which has recently been studied in the context of egocentric action
    recognition [[201](#bib.bib201)]. Additionally, more complex scenarios involving
    interactions with various objects or activities with long-term dependencies should
    be included. Since actions do not occur in isolation but are influenced by the
    environment in which they take place, the context, such as location, time, or
    objects present, should also be provided along with the publication of the dataset,
    and be exploited by the anticipation methods. However, labeling a dataset can
    be a labor-intensive process, especially for action anticipation datasets that
    contain natural, untrimmed, long videos. In response to this challenge, unsupervised
    approaches, not necessarily restricted to short-term anticipation, may present
    a promising direction. Furthermore, synthetic datasets, like those generated from
    games [[202](#bib.bib202)], could be of interest.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Exploiting language models: Many actions follow a specific order (see Fig. [2](#S1.F2
    "Figure 2 ‣ 1 Introduction ‣ A Survey on Deep Learning Techniques for Action Anticipation")).
    If we identify the preceding actions, we could predict the subsequent action without
    needing to predict the next frame feature and classify it as an action. Furthermore,
    we often adhere to daily routines. For instance, we might wake up at 7 o’clock,
    head to the washroom, and then prepare and eat breakfast. Learning such routines
    or action patterns is akin to our predictive abilities based on the experiences
    we have gathered since childhood. In this context, transferring the knowledge
    harnessed by large language models (LLMs) like ChatGPT [[177](#bib.bib177)] to
    action anticipation is a fascinating avenue to explore, as LLMs are trained on
    extensive text corpora which encapsulate such data, and demonstrate remarkable
    zero-shot capability in effectively tackling multiple NLP or vision-centric tasks [[203](#bib.bib203),
    [119](#bib.bib119)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Personalization: Current approaches are subject-agnostic, treating all individuals
    as having uniform preferences and behaviors. However, it is evident that each
    individual possesses unique tendencies, preferences, and patterns of behavior.
    Adapting anticipation models to individuals is another promising research direction.
    In particular, if the personal context, e.g., on vacation, is known.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Addressing uncertainty with probabilistic generative models: Generative models,
    including GANs [[143](#bib.bib143)], VAEs [[184](#bib.bib184)], Normalizing Flows [[190](#bib.bib190)],
    and Diffusion Models [[204](#bib.bib204), [205](#bib.bib205)], offer a promising
    avenue for addressing uncertainty in action anticipation. These models possess
    the capability to generate diverse potential future actions given a current context,
    which can help to provide a better understanding of the range of possible actions.
    Methods for quantifying and incorporating this uncertainty into the decision-making
    process could be explored in the future.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Real-time action anticipation: Although vision-based forecasting systems are
    typically designed for real-time deployment on autonomous entities such as self-driving
    cars and robots, they are predominantly evaluated in offline settings where inference
    latency is overlooked. To facilitate real-world applications, anticipation methods
    should be optimized and assessed within an online setting where latency plays
    a critical role [[206](#bib.bib206), [207](#bib.bib207), [73](#bib.bib73)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-person action anticipation: Existing methods mainly focus on single-person
    scenarios, but many real-world situations involve multiple people. Multi-person
    action forecasting in a video sequence has therefore surged as an intriguing topic.
    To address this, object and person detection can be incorporated, following the
    works of [[109](#bib.bib109), [114](#bib.bib114)].'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this survey, we provide a systematic overview of over 50 action anticipation
    methods pertinent to daily-living scenarios. We examined these methods from three
    perspectives: the specific research question each method addresses, a description
    of each method, and its contributions over previous work. We introduced a taxonomy
    to organize the different methods based on their main contributions. Moreover,
    we have provided a comparative summary of these methods in a tabular format to
    allow readers to identify low-level details at a glance. We have also introduced
    commonly used datasets and metrics, and presented results on several standard
    benchmarks. Lastly, we have offered insights in the form of future research directions.
    In conclusion, action anticipation is a captivating and relatively recent research
    topic that is garnering increasing attention within the community and holds value
    for numerous intelligent decision-making systems. While considerable progress
    has been made, there remains a vast scope for improvement in action anticipation
    using deep learning techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by the JuBot project which was made possible by funding
    from the Carl-Zeiss-Foundation. Juergen Gall has been supported by the Deutsche
    Forschungsgemeinschaft (DFG, German Research Foundation) GA 1927/4-2 (FOR 2535
    Anticipating Human Behavior) and the ERC Consolidator Grant FORHUE (101044724).
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] C. Feichtenhofer, H. Fan, J. Malik, and K. He, “Slowfast networks for video
    recognition,” in *ICCV*, 2019, pp. 6202–6211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] Z. Liu, J. Ning, Y. Cao, Y. Wei, Z. Zhang, S. Lin, and H. Hu, “Video swin
    transformer,” in *CVPR*, 2022, pp. 3202–3211.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] P. Philipp, *Über die Formalisierung und Analyse medizinischer Prozesse
    im Kontext von Expertenwissen und künstlicher Intelligenz*.   KIT Scientific Publishing,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] C. Vondrick, H. Pirsiavash, and A. Torralba, “Anticipating visual representations
    from unlabeled video,” in *CVPR*, 2016, pp. 98–106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] F. Sener, D. Singhania, and A. Yao, “Temporal Aggregate Representations
    for Long-Range Video Understanding,” in *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] C.-Y. Wu, Y. Li, K. Mangalam, H. Fan, B. Xiong, J. Malik, and C. Feichtenhofer,
    “MeMViT: Memory-Augmented Multiscale Vision Transformer for Efficient Long-Term
    Video Recognition,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *et al.*, “Scaling egocentric vision:
    The epic-kitchens dataset,” in *ECCV*, 2018, pp. 720–736.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] G. Ding, F. Sener, and A. Yao, “Temporal action segmentation: An analysis
    of modern technique,” *arXiv preprint arXiv:2210.10352*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] E. Vahdani and Y. Tian, “Deep learning-based action detection in untrimmed
    videos: A survey,” *TPAMI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] S. Oprea, P. Martinez-Gonzalez, A. Garcia-Garcia, J. A. Castro-Vargas,
    S. Orts-Escolano, J. Garcia-Rodriguez, and A. Argyros, “A Review on Deep Learning
    Techniques for Video Prediction,” *TPAMI*, no. 6, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. Rudenko, L. Palmieri, M. Herman, K. M. Kitani, D. M. Gavrila, and K. O.
    Arras, “Human motion trajectory prediction: A survey,” *The International Journal
    of Robotics Research*, vol. 39, no. 8, pp. 895–935, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] K. Lyu, H. Chen, Z. Liu, B. Zhang, and R. Wang, “3d human motion prediction:
    A survey,” *Neurocomputing*, vol. 489, pp. 345–365, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] X. Shi, Z. Chen, H. Wang, D.-Y. Yeung, W.-K. Wong, and W.-c. Woo, “Convolutional
    lstm network: A machine learning approach for precipitation nowcasting,” in *NeurIPS*,
    vol. 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] J. K. MacKie-Mason, A. V. Osepayshvili, D. M. Reeves, and M. P. Wellman,
    “Price prediction strategies for market-based scheduling,” in *International Conference
    on Automated Planning and Scheduling*, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] T. Petković, D. Puljiz, I. Marković, and B. Hein, “Human Intention Estimation
    based on Hidden Markov Model Motion Validation for Safe Flexible Robotized Warehouses,”
    *Robotics and Computer-Integrated Manufacturing*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] H. S. Koppula and A. Saxena, “Anticipating Human Activities Using Object
    Affordances for Reactive Robotic Response,” *TPAMI*, no. 1, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] A. Jain, H. S. Koppula, B. Raghavan, S. Soh, and A. Saxena, “Car that
    Knows Before You Do: Anticipating Maneuvers via Learning Temporal Driving Models,”
    in *ICCV*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] A. Rasouli, I. Kotseruba, and J. K. Tsotsos, “Pedestrian Action Anticipation
    using Contextual Feature Fusion in Stacked RNNs,” in *BMVC*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] E. Alati, L. Mauro, V. Ntouskos, and F. Pirri, “Help by predicting what
    to do,” in *ICIP*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] K. Ito, Q. Kong, S. Horiguchi, T. Sumiyoshi, and K. Nagamatsu, “Anticipating
    the Start of User Interaction for Service Robot in the Wild,” in *ICRA*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] P. Schydlo, M. Rakovic, L. Jamone, and J. Santos-Victor, “Anticipation
    in human-robot cooperation: A recurrent neural network approach for multiple action
    sequences prediction,” in *ICRA*, 2018, pp. 5909–5914.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] C.-M. Huang, S. Andrist, A. Sauppé, and B. Mutlu, “Using gaze patterns
    to predict task intent in collaboration,” *Frontiers in psychology*, vol. 6, p.
    1049, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] B. Soran, A. Farhadi, and L. Shapiro, “Generating notifications for missing
    actions: Don’t forget to turn the lights off!” in *ICCV*, 2015, pp. 4669–4677.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] K.-H. Zeng, S.-H. Chou, F.-H. Chan, J. C. Niebles, and M. Sun, “Agent-Centric
    Risk Assessment: Accident Anticipation and Risky Region Localization,” in *CVPR*,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] T. Suzuki, H. Kataoka, Y. Aoki, and Y. Satoh, “Anticipating traffic accidents
    with adaptive loss and large-scale incident db,” in *CVPR*, 2018, pp. 3521–3529.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] N. P. Trong, H. Nguyen, K. Kazunori, and B. Le Hoai, “A comprehensive
    survey on human activity prediction,” in *ICCSA*, 2017, pp. 411–425.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] A. Rasouli, “Deep learning for vision-based prediction: A survey,” *arXiv
    preprint arXiv:2007.00095*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] Y. Kong and Y. Fu, “Human action recognition and prediction: A survey,”
    *IJCV*, vol. 130, no. 5, pp. 1366–1401, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] I. Rodin, A. Furnari, D. Mavroeidis, and G. M. Farinella, “Predicting
    the future from first person (egocentric) vision: A survey,” *Computer Vision
    and Image Understanding*, vol. 211, p. 103252, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] X. Hu, J. Dai, M. Li, C. Peng, Y. Li, and S. Du, “Online human action
    detection and anticipation in videos: A survey,” *Neurocomputing*, vol. 491, pp.
    395–413, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] C. Plizzari, G. Goletto, A. Furnari, S. Bansal, F. Ragusa, G. M. Farinella,
    D. Damen, and T. Tommasi, “An outlook into the future of egocentric vision,” *arXiv
    preprint arXiv:2308.07123*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] K. Li, J. Hu, and Y. Fu, “Modeling Complex Temporal Composition of Actionlets
    for Activity Prediction,” in *ECCV*, vol. 7572, 2012, pp. 286–299.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] K. Li and Y. Fu, “Prediction of Human Activity by Discovering Temporal
    Sequence Patterns,” *TPAMI*, vol. 36, no. 8, pp. 1644–1657, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] N. Rhinehart and K. M. Kitani, “First-person activity forecasting with
    online inverse reinforcement learning,” in *ICCV*, 2017, pp. 3696–3705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] T. Mahmud, M. Hasan, A. Chakraborty, and A. K. Roy-Chowdhury, “A poisson
    process model for activity forecasting,” in *ICIP*, 2016, pp. 3339–3343.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] S. Qi, S. Huang, P. Wei, and S.-C. Zhu, “Predicting Human Activities Using
    Stochastic Grammar,” in *ICCV*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale
    image recognition,” in *ICLR*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for image
    recognition,” in *CVPR*, 2016, pp. 770–778.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] L. Wang, Y. Xiong, Z. Wang, Y. Qiao, D. Lin, X. Tang, and L. Van Gool,
    “Temporal Segment Networks: Towards Good Practices for Deep Action Recognition,”
    in *ECCV*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby,
    “An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale,”
    in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Carreira and A. Zisserman, “Quo Vadis, Action Recognition? A New Model
    and the Kinetics Dataset,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] K. Simonyan and A. Zisserman, “Two-stream convolutional networks for action
    recognition in videos,” in *NeurIPS*, vol. 27, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] D. Tran, H. Wang, L. Torresani, J. Ray, Y. LeCun, and M. Paluri, “A closer
    look at spatiotemporal convolutions for action recognition,” in *CVPR*, 2018,
    pp. 6450–6459.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] Y. Li, C.-Y. Wu, H. Fan, K. Mangalam, B. Xiong, J. Malik, and C. Feichtenhofer,
    “Mvitv2: Improved multiscale vision transformers for classification and detection,”
    in *CVPR*, 2022, pp. 4804–4814.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Gao, Z. Yang, and R. Nevatia, “RED: Reinforced Encoder-Decoder Networks
    for Action Anticipation,” in *BMVC*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] Y. A. Farha, A. Richard, and J. Gall, “When will you do what? - Anticipating
    Temporal Occurrences of Activities,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] K. Grauman, A. Westbury, E. Byrne, Z. Chavis, A. Furnari, R. Girdhar *et al.*,
    “Ego4D: Around the World in 3,000 Hours of Egocentric Video,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] Y. Abu Farha and J. Gall, “Uncertainty-Aware Anticipation of Activities,”
    in *ICCV Workshop*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] H. Zhao and R. P. Wildes, “On diverse asynchronous activity anticipation,”
    in *ECCV*, 2020, pp. 781–799.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] A. Richard, H. Kuehne, and J. Gall, “Weakly supervised action learning
    with rnn based fine-to-coarse modeling,” in *CVPR*, 2017, pp. 754–763.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] Y. A. Farha and J. Gall, “Ms-tcn: Multi-stage temporal convolutional network
    for action segmentation,” in *CVPR*, 2019, pp. 3575–3584.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] H. Zhang, F. Chen, and A. Yao, “Weakly-supervised dense action anticipation,”
    in *BMVC*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] N. Mehrasa, A. A. Jyothi, T. Durand, J. He, L. Sigal, and G. Mori, “A
    Variational Auto-Encoder Model for Stochastic Point Processes,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] S. Ren, K. He, R. Girshick, and J. Sun, “Faster r-cnn: Towards real-time
    object detection with region proposal networks,” in *NeurIPS*, vol. 28, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] K. He, G. Gkioxari, P. Dollár, and R. Girshick, “Mask r-cnn,” in *ICCV*,
    2017, pp. 2961–2969.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] A. Krizhevsky, I. Sutskever, and G. E. Hinton, “Imagenet classification
    with deep convolutional neural networks,” in *NeurIPS*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] A. Patron-Perez, M. Marszalek, A. Zisserman, and I. Reid, “High five:
    Recognising human interactions in tv shows.” in *BMVC*, vol. 1, no. 2, 2010, p. 33.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] H. Pirsiavash and D. Ramanan, “Detecting activities of daily living in
    first-person camera views,” in *CVPR*, 2012, pp. 2847–2854.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] Y.-G. Jiang, J. Liu, A. R. Zamir, G. Toderici, I. Laptev, M. Shah, and
    R. Sukthankar, “Thumos challenge: Action recognition with a large number of classes,”
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] K.-H. Zeng, W. B. Shen, D.-A. Huang, M. Sun, and J. Carlos Niebles, “Visual
    forecasting by imitating dynamics in natural sequences,” in *ICCV*, 2017, pp.
    2999–3008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] Y. Xiong, L. Wang, Z. Wang, B. Zhang, H. Song, W. Li, D. Lin, Y. Qiao,
    L. Van Gool, and X. Tang, “Cuhk & ethz & siat submission to activitynet challenge
    2016,” *arXiv preprint arXiv:1608.00797*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] R. D. Geest, E. Gavves, A. Ghodrati, Z. Li, C. Snoek, and T. Tuytelaars,
    “Online action detection,” in *ECCV*, 2016, pp. 269–284.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] Y. Zhong and W.-S. Zheng, “Unsupervised learning for forecasting action
    representations,” in *ICIP*, 2018, pp. 1073–1077.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] V. Tran, Y. Wang, Z. Zhang, and M. Hoai, “Knowledge distillation for human
    action anticipation,” in *ICIP*, 2021, pp. 2518–2522.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] B. Fernando and S. Herath, “Anticipating human actions by correlating
    past with the future with Jaccard similarity measures,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] H. Kuehne, A. Arslan, and T. Serre, “The Language of Actions: Recovering
    the Syntax and Semantics of Goal-Directed Human Activities,” in *CVPR*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. Girdhar and K. Grauman, “Anticipative Video Transformer,” in *ICCV*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] D. Damen, H. Doughty, G. M. Farinella, A. Furnari, E. Kazakos, J. Ma,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *et al.*, “Rescaling egocentric
    vision: Collection, pipeline and challenges for epic-kitchens-100,” *IJCV*, pp.
    1–23, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Y. Li, M. Liu, and J. M. Rehg, “In the eye of beholder: Joint learning
    of gaze and actions in first person video,” in *ECCV*, 2018, pp. 619–635.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] S. Stein and S. J. McKenna, “Combining embedded accelerometers with computer
    vision for recognizing food preparation activities,” in *UbiComp*, 2013, pp. 729–738.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] X. Xu, Y.-L. Li, and C. Lu, “Learning To Anticipate Future With Dynamic
    Context Removal,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] J. Lin, C. Gan, and S. Han, “Tsm: Temporal shift module for efficient
    video understanding,” in *ICCV*, 2019, pp. 7083–7093.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] H. Girase, N. Agarwal, C. Choi, and K. Mangalam, “Latency matters: Real-time
    action forecasting transformer,” in *CVPR*, 2023, pp. 18 759–18 769.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Forecasting future
    action sequences with neural memory networks,” in *BMVC*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] M. Xu, M. Gao, Y.-T. Chen, L. S. Davis, and D. J. Crandall, “Temporal
    recurrent networks for online action detection,” in *ICCV*, 2019, pp. 5532–5541.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] W. Wang, X. Peng, Y. Su, Y. Qiao, and J. Cheng, “TTPP: Temporal Transformer
    with Progressive Prediction for Efficient Action Anticipation,” *Neurocomputing*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] S. Qu, G. Chen, D. Xu, J. Dong, F. Lu, and A. Knoll, “Lap-net: Adaptive
    features sampling via learning action progression for online action detection,”
    *arXiv preprint arXiv:2011.07915*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Y. Wu, L. Zhu, X. Wang, Y. Yang, and F. Wu, “Learning to Anticipate Egocentric
    Actions by Imagination,” *TIP*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] M. Xu, Y. Xiong, H. Chen, X. Li, W. Xia, Z. Tu, and S. Soatto, “Long short-term
    transformer for online action detection,” in *NeurIPS*, vol. 34, 2021, pp. 1086–1099.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] X. Wang, S. Zhang, Z. Qing, Y. Shao, Z. Zuo, C. Gao, and N. Sang, “Oadtr:
    Online action detection with transformers,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] T. Liu and K.-M. Lam, “A Hybrid Egocentric Activity Anticipation Framework
    via Memory-Augmented Recurrent and One-shot Representation Forecasting,” in *CVPR*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] D. Gong, J. Lee, M. Kim, S. J. Ha, and M. Cho, “Future Transformer for
    Long-term Action Anticipation,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Zhao and P. Krähenbühl, “Real-time online video detection with temporal
    smoothing transformers,” in *ECCV*, 2022, pp. 485–502.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] Y. Zhou and T. L. Berg, “Temporal perception and prediction in ego-centric
    video,” in *ICCV*, 2015, pp. 4498–4506.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] K. Soomro, A. R. Zamir, and M. Shah, “Ucf101: A dataset of 101 human actions
    classes from videos in the wild,” *arXiv preprint arXiv:1212.0402*, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] T. Mahmud, M. Hasan, and A. K. Roy-Chowdhury, “Joint prediction of activity
    labels and starting times in untrimmed videos,” in *ICCV*, 2017, pp. 5773–5782.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learning
    spatiotemporal features with 3d convolutional networks,” in *ICCV*, 2015, pp.
    4489–4497.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] S. Oh, A. Hoogs, A. Perera, N. Cuntoor, C.-C. Chen, J. T. Lee, S. Mukherjee,
    J. Aggarwal, H. Lee, L. Davis *et al.*, “A large-scale benchmark dataset for event
    recognition in surveillance video,” in *CVPR*, 2011, pp. 3153–3160.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] M. Rohrbach, S. Amin, M. Andriluka, and B. Schiele, “A database for fine
    grained activity detection of cooking activities,” in *CVPR*, 2012, pp. 1194–1201.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] Y. Shen, B. Ni, Z. Li, and N. Zhuang, “Egocentric activity prediction
    via event modulated attention,” in *ECCV*, 2018, pp. 197–212.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] X. Zhu, X. Jia, and K.-Y. K. Wong, “Pixel-level hand detection with shape-aware
    structured forests,” in *ACCV*, 2015, pp. 64–78.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] W. Liu, D. Anguelov, D. Erhan, C. Szegedy, S. Reed, C.-Y. Fu, and A. C.
    Berg, “Ssd: Single shot multibox detector,” in *ECCV*, 2016, pp. 21–37.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] A. Fathi, X. Ren, and J. M. Rehg, “Learning to recognize objects in egocentric
    activities,” in *CVPR*, 2011, pp. 3281–3288.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Fathi, Y. Li, and J. M. Rehg, “Learning to recognize daily actions
    using gaze,” in *ECCV*, 2012, pp. 314–327.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] J. Liang, L. Jiang, J. C. Niebles, A. G. Hauptmann, and L. Fei-Fei, “Peeking
    Into the Future: Predicting Future Person Activities and Locations in Videos,”
    in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] L.-C. Chen, Y. Zhu, G. Papandreou, F. Schroff, and H. Adam, “Encoder-decoder
    with atrous separable convolution for semantic image segmentation,” in *ECCV*,
    2018, pp. 801–818.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] G. Awad, A. A. Butt, K. Curtis, Y. Lee, J. Fiscus, A. Godil, D. Joy, A. Delgado,
    A. F. Smeaton, Y. Graham *et al.*, “Trecvid 2018: Benchmarking video activity
    detection, video captioning and matching, video storytelling linking and video
    search,” in *TRECVID*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] A. Furnari and G. Farinella, “What Would You Expect? Anticipating Egocentric
    Actions With Rolling-Unrolling LSTMs and Modality Attention,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] M. Liu, S. Tang, Y. Li, and J. M. Rehg, “Forecasting Human-Object Interaction:
    Joint Prediction of Motor Attention and Actions in First Person Video,” in *ECCV*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] D. Tran, H. Wang, L. Torresani, and M. Feiszli, “Video classification
    with channel-separated convolutional networks,” in *ICCV*, 2019, pp. 5552–5561.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] E. Dessalene, C. Devaraj, M. Maynord, C. Fermuller, and Y. Aloimonos,
    “Forecasting action through contact representations from first person video,”
    *TPAMI*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] T. N. Kipf and M. Welling, “Semi-supervised classification with graph
    convolutional networks,” in *ICLR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] O. Zatsarynna, Y. A. Farha, and J. Gall, “Multi-Modal Temporal Convolutional
    Network for Anticipating Actions in Egocentric Videos,” in *CVPR Workshop*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] D. Roy and B. Fernando, “Action Anticipation Using Pairwise Human-Object
    Interactions and Transformers,” *TIP*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] Z. Zhong, D. Schneider, M. Voit, R. Stiefelhagen, and J. Beyerer, “Anticipative
    feature fusion transformer for multi-modal action anticipation,” in *WACV*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo,
    “Swin transformer: Hierarchical vision transformer using shifted windows,” in
    *CVPR*, 2021, pp. 10 012–10 022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena, “Structural-rnn: Deep
    learning on spatio-temporal graphs,” in *CVPR*, 2016, pp. 5308–5317.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] H. S. Koppula, R. Gupta, and A. Saxena, “Learning human activities and
    object affordances from rgb-d videos,” *IJRR*, vol. 32, no. 8, pp. 951–970, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] C. Sun, A. Shrivastava, C. Vondrick, R. Sukthankar, K. Murphy, and C. Schmid,
    “Relational Action Forecasting,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] S. Xie, C. Sun, J. Huang, Z. Tu, and K. Murphy, “Rethinking spatiotemporal
    feature learning for video understanding,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] C. Gu, C. Sun, D. A. Ross, C. Vondrick, C. Pantofaru, Y. Li, S. Vijayanarasimhan,
    G. Toderici, S. Ricco, R. Sukthankar *et al.*, “Ava: A video dataset of spatio-temporally
    localized atomic visual actions,” in *CVPR*, 2018, pp. 6047–6056.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] Y. Abu Farha, Q. Ke, B. Schiele, and J. Gall, “Long-term anticipation
    of activities with cycle consistency,” in *GCPR*, 2020, pp. 159–173.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] G. Camporese, P. Coscia, A. Furnari, G. M. Farinella, and L. Ballan,
    “Knowledge distillation for action anticipation via label smoothing,” in *ICPR*,
    2020, pp. 3312–3319.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Y. Li, P. Wang, and C.-Y. Chan, “Restep into the future: relational spatio-temporal
    learning for multi-person action forecasting,” *IEEE Transactions on Multimedia*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] A. Gupta, J. Liu, L. Bo, A. K. Roy-Chowdhury, and T. Mei, “A-act: Action
    anticipation through cycle transformations,” *arXiv preprint arXiv:2204.00942*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] D. Roy and B. Fernando, “Predicting the next action by modeling the abstract
    goal,” *arXiv preprint arXiv:2209.05044*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] M. Nawhal, A. A. Jyothi, and G. Mori, “Rethinking learning approaches
    for long-term action anticipation,” in *ECCV*, 2022, pp. 558–576.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] E. V. Mascaró, H. Ahn, and D. Lee, “Intention-conditioned long-term human
    egocentric action anticipation,” in *WACV*, 2023, pp. 6048–6057.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] Q. Zhao, C. Zhang, S. Wang, C. Fu, N. Agarwal, K. Lee, and C. Sun, “Antgpt:
    Can large language models help long-term action anticipation from videos?” *arXiv
    preprint arXiv:2307.16368*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Sung, C. Ponce, B. Selman, and A. Saxena, “Unstructured human activity
    detection from rgbd images,” in *ICRA*, 2012, pp. 842–849.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and L. Fei-Fei,
    “Every moment counts: Dense detailed labeling of actions in complex videos,” *IJCV*,
    vol. 126, no. 2, pp. 375–389, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] Y. B. Ng and B. Fernando, “Forecasting future action sequences with attention:
    a new approach to weakly supervised action forecasting,” *TIP*, vol. 29, pp. 8880–8891,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] G. A. Sigurdsson, G. Varol, X. Wang, A. Farhadi, I. Laptev, and A. Gupta,
    “Hollywood in homes: Crowdsourcing data collection for activity understanding,”
    in *ECCV*, 2016, pp. 510–526.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] A. Piergiovanni, A. Angelova, A. Toshev, and M. S. Ryoo, “Adversarial
    generative grammars for human activity prediction,” in *ECCV*, 2020, pp. 507–523.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] A. Miech, I. Laptev, J. Sivic, H. Wang, L. Torresani, and D. Tran, “Leveraging
    the Present to Anticipate the Future in Videos,” in *CVPR Workshop*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] F. Caba Heilbron, V. Escorcia, B. Ghanem, and J. Carlos Niebles, “Activitynet:
    A large-scale video benchmark for human activity understanding,” in *CVPR*, 2015,
    pp. 961–970.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] Q. Ke, M. Fritz, and B. Schiele, “Time-Conditioned Action Anticipation
    in One Shot,” in *CVPR*, Jun. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] T. Zhang, W. Min, Y. Zhu, Y. Rui, and S. Jiang, “An egocentric action
    anticipation framework via fusing intuition and analysis,” in *ACMMM*, 2020, pp.
    402–410.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] V. Gupta and S. Bedathur, “Proactive: Self-attentive temporal point process
    flows for activity sequences,” in *KDD*, 2022, pp. 496–504.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] C. Rodriguez, B. Fernando, and H. Li, “Action anticipation by predicting
    future dynamic images,” in *ECCV Workshop*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] I. Teeti, R. S. Bhargav, V. Singh, A. Bradley, B. Banerjee, and F. Cuzzolin,
    “Temporal dino: A self-supervised video strategy to enhance action prediction,”
    *arXiv preprint arXiv:2308.04589*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] A. v. d. Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive
    predictive coding,” *arXiv preprint arXiv:1807.03748*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] T. Han, W. Xie, and A. Zisserman, “Video representation learning by dense
    predictive coding,” in *ICCV Workshop*, 2019, pp. 0–0.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] ——, “Memory-augmented dense predictive coding for video representation
    learning,” in *ECCV*, 2020, pp. 312–329.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] D. Surís, R. Liu, and C. Vondrick, “Learning the predictability of the
    future,” in *CVPR*, 2021, pp. 12 607–12 617.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] O. Zatsarynna, Y. A. Farha, and J. Gall, “Self-supervised learning for
    unintentional action prediction,” in *GCPR*, 2022, pp. 429–444.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] R. Tan, M. De Lange, M. Iuzzolino, B. A. Plummer, K. Saenko, K. Ridgeway,
    and L. Torresani, “Multiscale video pretraining for long-term activity forecasting,”
    *arXiv preprint arXiv:2307.12854*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] S. Hochreiter and J. Schmidhuber, “Long short-term memory,” *Neural computation*,
    vol. 9, no. 8, pp. 1735–1780, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] M. Nickel and D. Kiela, “Poincaré embeddings for learning hierarchical
    representations,” *NeurIPS*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is All you Need,” in *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] M. Hayat, S. Khan, S. W. Zamir, J. Shen, and L. Shao, “Gaussian affinity
    for max-margin class imbalanced learning,” in *ICCV*, 2019, pp. 6469–6479.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever *et al.*,
    “Language models are unsupervised multitask learners,” *OpenAI blog*, vol. 1,
    no. 8, p. 9, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
    A. Courville, and Y. Bengio, “Generative adversarial nets,” in *NeurIPS*, vol. 27,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] H. Gammulle, S. Denman, S. Sridharan, and C. Fookes, “Predicting the
    future: A jointly learnt model for action anticipation,” in *ICCV*, 2019, pp.
    5562–5571.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] J. Ho and S. Ermon, “Generative adversarial imitation learning,” in *NeurIPS*,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] J. Chung, C. Gulcehre, K. Cho, and Y. Bengio, “Empirical evaluation of
    gated recurrent neural networks on sequence modeling,” *arXiv preprint arXiv:1412.3555*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Z. Qi, S. Wang, C. Su, L. Su, Q. Huang, and Q. Tian, “Self-regulated
    learning for egocentric video activity anticipation,” *TPAMI*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] A. Bubic, D. Y. Von Cramon, and R. I. Schubotz, “Prediction, cognition
    and the brain,” *Frontiers in human neuroscience*, vol. 4, p. 1094, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] A. Clark, “Whatever next? predictive brains, situated agents, and the
    future of cognitive science,” *Behavioral and brain sciences*, vol. 36, no. 3,
    pp. 181–204, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] E. Jang, S. Gu, and B. Poole, “Categorical reparameterization with gumbel-softmax,”
    in *ICLR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural networks,”
    in *CVPR*, 2018, pp. 7794–7803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] C.-Y. Wu, C. Feichtenhofer, H. Fan, K. He, P. Krahenbuhl, and R. Girshick,
    “Long-term feature banks for detailed video understanding,” in *CVPR*, 2019, pp.
    284–293.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] K. Choromanski, V. Likhosherstov, D. Dohan, X. Song, A. Gane, T. Sarlos,
    P. Hawkins, J. Davis, A. Mohiuddin, L. Kaiser *et al.*, “Rethinking attention
    with performers,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] S. Wang, B. Z. Li, M. Khabsa, H. Fang, and H. Ma, “Linformer: Self-attention
    with linear complexity,” *arXiv preprint arXiv:2006.04768*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] J. W. Rae, A. Potapenko, S. M. Jayakumar, and T. P. Lillicrap, “Compressive
    transformers for long-range sequence modelling,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] A. Jaegle, F. Gimeno, A. Brock, O. Vinyals, A. Zisserman, and J. Carreira,
    “Perceiver: General perception with iterative attention,” in *ICML*, 2021, pp.
    4651–4664.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] Y.-H. H. Tsai, S. Bai, M. Yamada, L.-P. Morency, and R. Salakhutdinov,
    “Transformer dissection: An unified understanding for transformer’s attention
    via the lens of kernel,” in *EMNLP*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] E. Kazakos, A. Nagrani, A. Zisserman, and D. Damen, “EPIC-Fusion: Audio-Visual
    Temporal Binding for Egocentric Action Recognition,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] A. Furnari, S. Battiato, K. Grauman, and G. M. Farinella, “Next-active-object
    prediction from egocentric videos,” *Journal of Visual Communication and Image
    Representation*, pp. 401–411, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] Z. Sun, Q. Ke, H. Rahmani, M. Bennamoun, G. Wang, and J. Liu, “Human
    action recognition from various data modalities: A review,” *TPAMI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] C. Zach, T. Pock, and H. Bischof, “A duality based approach for realtime
    tv-l 1 optical flow,” in *Joint pattern recognition symposium*, 2007, pp. 214–223.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] S. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu, “Learning Human-Object
    Interactions by Graph Parsing Neural Networks,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] D. Roy, R. Rajendiran, and B. Fernando, “Interaction visual transformer
    for egocentric action anticipation,” *arXiv preprint arXiv:2211.14154*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] H. S. Park, J.-J. Hwang, Y. Niu, and J. Shi, “Egocentric future localization,”
    in *CVPR*, 2016, pp. 4697–4705.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] T. Nagarajan, Y. Li, C. Feichtenhofer, and K. Grauman, “Ego-topo: Environment
    affordances from egocentric video,” in *CVPR*, 2020, pp. 163–172.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] A. G. Hawkes, “Spectra of some self-exciting and mutually exciting point
    processes,” *Biometrika*, vol. 58, no. 1, pp. 83–90, 1971.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] D. Damen, H. Doughty, G. M. Farinella, S. Fidler, A. Furnari, E. Kazakos,
    D. Moltisanti, J. Munro, T. Perrett, W. Price *et al.*, “The epic-kitchens dataset:
    Collection, challenges and baselines,” *TPAMI*, vol. 43, no. 11, pp. 4125–4141,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] N. Osman, G. Camporese, P. Coscia, and L. Ballan, “Slowfast rolling-unrolling
    lstms for action anticipation in egocentric videos,” in *ICCV Workshop*, 2021,
    pp. 3437–3445.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager, “Temporal
    convolutional networks for action segmentation and detection,” in *CVPR*, 2017,
    pp. 156–165.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] C. L. Fosco, S. Jin, E. Josephs, and A. Oliva, “Leveraging temporal context
    in low representational power regimes,” in *CVPR*, June 2023, pp. 10 693–10 703.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] J. Pennington, R. Socher, and C. D. Manning, “Glove: Global vectors for
    word representation,” in *EMNLP*, 2014, pp. 1532–1543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] P. Veličković, G. Cucurull, A. Casanova, A. Romero, P. Lio, and Y. Bengio,
    “Graph attention networks,” in *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] N. Ballas, L. Yao, C. Pal, and A. Courville, “Delving deeper into convolutional
    networks for learning video representations,” in *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] A. W. Kruglanski and E. Szumowska, “Habitual behavior is goal-driven,”
    *Perspectives on Psychological Science*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] D. Roy and B. Fernando, “Action anticipation using latent goal learning,”
    in *WACV*, 2022, pp. 2745–2753.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] I. O. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Unterthiner,
    J. Yung, A. Steiner, D. Keysers, J. Uszkoreit *et al.*, “Mlp-mixer: An all-mlp
    architecture for vision,” in *NeurIPS*, vol. 34, 2021, pp. 24 261–24 272.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] OpenAI, “Chatgpt: Optimizing language models for dialogue,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] O. Zatsarynna and J. Gall, “Action anticipation with goal consistency,”
    in *ICIP*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] B. Dai, S. Fidler, R. Urtasun, and D. Lin, “Towards diverse and natural
    image descriptions via a conditional gan,” in *ICCV*, 2017, pp. 2970–2979.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] M. Mathieu, C. Couprie, and Y. LeCun, “Deep multi-scale video prediction
    beyond mean square error,” in *ICLR*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Y. Li, N. Du, and S. Bengio, “Time-dependent representation for neural
    event sequence prediction,” *arXiv preprint arXiv:1708.00065*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] I. Sutskever, O. Vinyals, and Q. V. Le, “Sequence to sequence learning
    with neural networks,” in *NeurIPS*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] D. Yang, S. Hong, Y. Jang, T. Zhao, and H. Lee, “Diversity-sensitive
    conditional generative adversarial networks,” in *ICLR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” in *ICLR*,
    2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] J. F. C. Kingman, *Poisson processes*.   Clarendon Press, 1992, vol. 3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] M.-A. Rizoiu, L. Xie, S. Sanner, M. Cebrian, H. Yu, and P. Van Hentenryck,
    “Expecting to be hip: Hawkes intensity processes for social media popularity,”
    in *WWW*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] E. Bacry, I. Mastromatteo, and J.-F. Muzy, “Hawkes processes in finance,”
    *Market Microstructure and Liquidity*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] M. Yao, S. Zhao, S. Sahebi, and R. Feyzi Behnagh, “Stimuli-sensitive
    hawkes processes for personalized student procrastination modeling,” in *WWW*,
    2021, pp. 1562–1573.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] Q. Zhao, M. A. Erdogdu, H. Y. He, A. Rajaraman, and J. Leskovec, “Seismic:
    A self-exciting point process model for predicting tweet popularity,” in *KDD*,
    2015, pp. 1513–1522.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] D. Rezende and S. Mohamed, “Variational inference with normalizing flows,”
    in *ICML*, 2015, pp. 1530–1538.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] N. Mehrasa, R. Deng, M. O. Ahmed, B. Chang, J. He, T. Durand, M. Brubaker,
    and G. Mori, “Point process flows,” *arXiv preprint arXiv:1910.08281*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] O. Shchur, M. Biloš, and S. Günnemann, “Intensity-free learning of temporal
    point processes,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] F. Sener, D. Chatterjee, D. Shelepov, K. He, D. Singhania, R. Wang, and
    A. Yao, “Assembly101: A large-scale multi-view video dataset for understanding
    procedural activities,” in *CVPR*, 2022, pp. 21 096–21 106.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] A. Furnari, S. Battiato, and G. M. Farinella, “Leveraging Uncertainty
    to Rethink Loss Functions and Evaluation Measures for Egocentric Action Anticipation,”
    in *ECCV Workshop*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] F. J. Damerau, “A technique for computer detection and correction of
    spelling errors,” *Communications of the ACM*, vol. 7, no. 3, pp. 171–176, 1964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] V. I. Levenshtein *et al.*, “Binary codes capable of correcting deletions,
    insertions, and reversals,” in *Soviet physics doklady*, vol. 10, no. 8, 1966,
    pp. 707–710.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] S. Ioffe and C. Szegedy, “Batch normalization: Accelerating deep network
    training by reducing internal covariate shift,” in *ICML*, 2015, pp. 448–456.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] J. Carreira, E. Noland, C. Hillier, and A. Zisserman, “A short note on
    the kinetics-700 human action dataset,” *arXiv preprint arXiv:1907.06987*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Imagenet:
    A large-scale hierarchical image database,” in *CVPR*, 2009, pp. 248–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] [https://bitbucket.org/doneata/fv4a/src/master/](https://bitbucket.org/doneata/fv4a/src/master/).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] C. Plizzari, T. Perrett, B. Caputo, and D. Damen, “What can a cook in
    italy teach a mechanic in india? action recognition generalisation over scenarios
    and locations,” in *ICCV*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] A. Roitberg, D. Schneider, A. Djamal, C. Seibold, S. Reiß, and R. Stiefelhagen,
    “Let’s play for action: Recognizing activities of daily living by learning from
    life simulation video games,” in *IROS*, 2021, pp. 8563–8569.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] G. Chen, Y.-D. Zheng, J. Wang, J. Xu, Y. Huang, J. Pan, Y. Wang, Y. Wang,
    Y. Qiao, T. Lu *et al.*, “Videollm: Modeling video sequence with large language
    models,” *arXiv preprint arXiv:2305.13292*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    in *NeurIPS*, vol. 33, 2020, pp. 6840–6851.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] D. Liu, Q. Li, A. Dinh, T. Jiang, M. Shah, and C. Xu, “Diffusion action
    segmentation,” in *ICCV*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] M. Li, Y.-X. Wang, and D. Ramanan, “Towards streaming perception,” in
    *ECCV*, 2020, pp. 473–488.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] A. Furnari and G. M. Farinella, “Towards streaming egocentric action
    anticipation,” in *ICPR*, 2022, pp. 1250–1257.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '| ![[Uncaptioned image]](img/96f59c6a2a2d68aa1bc64965825f1bfa.png) | Zeyun
    Zhong received the B.Eng. degree in process-, energy- and environmental engineering
    from Hannover University of Applied Sciences and Arts, in 2018, and the M.Sc.
    in mechatronics from Leibniz University Hannover, in 2021\. He is currently working
    toward the Ph.D. degree at the Department of Informatics, Karlsruhe Institute
    of Technology. His main research interests include deep learning, video understanding,
    action recognition, and anticipation. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/7e8c9902cc6fdc1c7c94a8c2c8c46165.png) | Manuel
    Martin studied computer science at the Karlsruhe Institute of Technology (KIT)
    obtaining his diploma in 2013\. He received his Ph.D. from KIT in 2023 for his
    work on 3D human body pose estimation and activity recognition of drivers in automated
    vehicles. He is now senior scientist in the group Perceptual User Interfaces at
    Fraunhofer IOSB, Karlsruhe, continuing his work on machine learning based occupant
    monitoring systems. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/96be0b1cd4bb95cb6dea06869f3284c0.png) | Michael
    Voit studied computer science at the former University of Karlsruhe (TH), now
    Karlsruhe Institute of Technology (KIT), obtaining his diploma in 2004\. He received
    his Ph.D. from KIT in 2012 for estimating the visual focus of attention of people
    by monitoring them with multiple overhead cameras distributed in a room. He is
    now head of the research group Perceptual User Interfaces at Fraunhofer IOSB,
    Karlsruhe, continuing his work on camera-based human machine interaction techniques.
    |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/da15fc854403b99bf8d38575f65d72f2.png) | Juergen
    Gall obtained his B.Sc. and his Masters degree in mathematics from the University
    of Wales Swansea (2004) and from the University of Mannheim (2005). In 2009, he
    obtained a Ph.D. in computer science from the Saarland University and the Max
    Planck Institut für Informatik. He was a postdoctoral researcher at the Computer
    Vision Laboratory, ETH Zurich, from 2009 until 2012 and senior research scientist
    at the Max Planck Institute for Intelligent Systems in Tübingen from 2012 until
    2013\. Since 2013, he is professor at the University of Bonn and member of the
    Lamarr Institute for Machine Learning and Artificial Intelligence. |'
  prefs: []
  type: TYPE_TB
- en: '| ![[Uncaptioned image]](img/8df0840672c864ec74942e018bf6891c.png) | Jürgen
    Beyerer studied electrical engineering at the University of Karlsruhe, obtaining
    his diploma in 1989\. In 1994, he obtained his Ph.D. from the Institute for Measurement
    and Control Systems (MRT) at the same university. Post his doctorate, he established
    a research group on automatic visual inspection and image processing at MRT. From
    1999 to 2004, he led the Hottinger Systems GmbH in Mannheim and served as the
    deputy managing director for Hottinger Maschinenbau GmbH. Since 2004, he is professor
    at the Karlsruhe Institute of Technology and concurrently the head of the Fraunhofer
    Institute of Optronics, System Technologies and Image Exploitation (IOSB). |'
  prefs: []
  type: TYPE_TB
- en: 8 Supplementary Material
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While Tables [IV](#S5.T4 "TABLE IV ‣ 5.1 Short-term anticipation ‣ 5 Benchmark
    Protocols and Results ‣ A Survey on Deep Learning Techniques for Action Anticipation")-[VI](#S5.T6
    "TABLE VI ‣ 5.2 Long-term anticipation ‣ 5 Benchmark Protocols and Results ‣ A
    Survey on Deep Learning Techniques for Action Anticipation") show the results
    for a subset of the evaluation protocols, we provide the comprehensive results
    in Tables [VII](#S8.T7 "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep
    Learning Techniques for Action Anticipation")-[XI](#S8.T11 "TABLE XI ‣ 8 Supplementary
    Material ‣ A Survey on Deep Learning Techniques for Action Anticipation"). These
    results encompass both short-term and long-term action anticipation, covering
    both third-person and egocentric perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VII: Comparison of the state-of-the-art methods on TVSeries [[62](#bib.bib62)]
    in terms of mean cAP (%). The optimal performance in each column within every
    block is indicated in bold, while the supreme overall performance in each column
    is further denoted by an underscore.'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Backbone | Pre-train | mcAP@Time predicted into the future
    (seconds) |  |'
  prefs: []
  type: TYPE_TB
- en: '| 0.25 | 0.5 | 0.75 | 1.0 | 1.25 | 1.5 | 1.75 | 2.0 | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| ED [[45](#bib.bib45)] | 2017 | VGG | IN1K | 71.0 | 70.6 | 69.9 | 68.8 | 68.0
    | 67.4 | 67.0 | 66.7 | 68.7 |'
  prefs: []
  type: TYPE_TB
- en: '| RED [[45](#bib.bib45)] | 2017 | 71.2 | 71.0 | 70.6 | 70.2 | 69.2 | 68.5 |
    67.5 | 66.8 | 69.4 |'
  prefs: []
  type: TYPE_TB
- en: '| TTPP[[76](#bib.bib76)] | 2020 | 72.7 | 72.3 | 71.9 | 71.6 | 71.3 | 70.9 |
    69.9 | 69.3 | 71.3 |'
  prefs: []
  type: TYPE_TB
- en: '| ED [[45](#bib.bib45)] | 2017 | TS | ANet1.3 | 78.5 | 78.0 | 76.3 | 74.6 |
    73.7 | 72.7 | 71.7 | 71.0 | 74.5 |'
  prefs: []
  type: TYPE_TB
- en: '| RED [[45](#bib.bib45)] | 2017 | 79.2 | 78.7 | 77.1 | 75.5 | 74.2 | 73.0 |
    72.0 | 71.2 | 75.1 |'
  prefs: []
  type: TYPE_TB
- en: '| TRN [[75](#bib.bib75)] | 2019 | 79.9 | 78.4 | 77.1 | 75.9 | 74.9 | 73.9 |
    73.0 | 72.3 | 75.7 |'
  prefs: []
  type: TYPE_TB
- en: '| TTPP[[76](#bib.bib76)] | 2020 | 81.2 | 80.3 | 79.3 | 77.6 | 76.9 | 76.7 |
    76.0 | 74.9 | 77.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LAP [[77](#bib.bib77)] | 2020 | 82.6 | 81.3 | 80.0 | 78.9 | 77.9 | 77.1 |
    76.3 | 75.5 | 78.7 |'
  prefs: []
  type: TYPE_TB
- en: '| OadTR[[80](#bib.bib80)] | 2021 | 81.9 | 80.6 | 79.4 | 78.2 | 77.1 | 76.0
    | 75.2 | 74.3 | 77.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LSTR [[79](#bib.bib79)] | 2021 | – | – | – | – | – | – | – | – | 80.8 |'
  prefs: []
  type: TYPE_TB
- en: '| OadTR[[80](#bib.bib80)] | 2021 | TS | K400 | 84.1 | 82.6 | 81.3 | 80.1 |
    78.9 | 77.7 | 76.7 | 75.7 | 79.1 |'
  prefs: []
  type: TYPE_TB
- en: TVSeries [[62](#bib.bib62)] & THUMOS14 [[59](#bib.bib59)]. TVSeries contains
    six popular TV series including 30 realistic, everyday actions. THUMOS14 contains
    sport videos depicting 20 actions. Both datasets employ an anticipation protocol
    to predict actions at various future timestamps, ranging from 0.25 to 2.0 seconds.
    For evaluation, mean calibrated average precision (mcAP) is used for TVSeries
    and mean average precision (mAP) for THUMOS14\. Comprehensive benchmark results
    and average performance across all timestamps can be found in Table [VII](#S8.T7
    "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for
    Action Anticipation") and Table [VIII](#S8.T8 "TABLE VIII ‣ 8 Supplementary Material
    ‣ A Survey on Deep Learning Techniques for Action Anticipation"), respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'EpicKitchens-100 [[68](#bib.bib68)] consists of long unscripted videos of cooking
    activities totalling 100 hours. Table [IX](#S8.T9 "TABLE IX ‣ 8 Supplementary
    Material ‣ A Survey on Deep Learning Techniques for Action Anticipation") outlines
    three performance benchmarks: overall, unseen kitchen, and tail classes. Each
    of these benchmarks is evaluated based on the top-5 recall for verbs, nouns, and
    actions across all classes. The primary metric used to rank the methods is accentuated.
    If the anticipation approach employs a backbone fine-tuned for feature extraction,
    it is marked with a check mark in the column titled E2E.'
  prefs: []
  type: TYPE_NORMAL
- en: Breakfast [[66](#bib.bib66)] & 50Salads [[70](#bib.bib70)]. The Breakfast dataset
    features videos of various individuals preparing breakfast in distinct kitchens,
    while the 50Salads dataset consists of top-view RGB-D video footage capturing
    individuals making salads. Evaluation of these datasets is conducted using mean-over-classes
    accuracy, averaged across future timestamps within a specified anticipation duration.
    For evaluation, a fraction ($\alpha$) of a complete video is observed, with the
    objective of predicting actions in the subsequent video. Typically, the observation
    ratio $\alpha$ is set to 0.2 or 0.3\. Action predictions are then made in $\beta$
    segments of the full video, where $\beta$ can be $\{0.1,0.2,0.3,0.5\}$. It is
    important to note that the evaluation protocol of Anticipatr deviates from previous
    works. Instead of the typical approach, they measure the predictions at $\beta$
    of the residual video segments. We thus show the results of Anticipatr in Table [X](#S8.T10
    "TABLE X ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for
    Action Anticipation") and Table [XI](#S8.T11 "TABLE XI ‣ 8 Supplementary Material
    ‣ A Survey on Deep Learning Techniques for Action Anticipation") in gray font
    since the results are not comparable to previous works.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE VIII: Comparison of the state-of-the-art methods on THUMOS14 [[59](#bib.bib59)]
    in terms of mAP (%). For details on the font coding, please refer to Table [VII](#S8.T7
    "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep Learning Techniques for
    Action Anticipation").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Method | Year | Backbone | Pre-train | mAP@Time predicted into the future
    (seconds) |  |'
  prefs: []
  type: TYPE_TB
- en: '| 0.25 | 0.5 | 0.75 | 1.0 | 1.25 | 1.5 | 1.75 | 2.0 | Avg. |'
  prefs: []
  type: TYPE_TB
- en: '| ED [[45](#bib.bib45)] | 2017 | TS | ANet1.3 | 43.8 | 40.9 | 38.7 | 36.8 |
    34.6 | 33.9 | 32.5 | 31.6 | 36.6 |'
  prefs: []
  type: TYPE_TB
- en: '| RED [[45](#bib.bib45)] | 2017 | 45.3 | 42.1 | 39.6 | 37.5 | 35.8 | 34.4 |
    33.2 | 32.1 | 37.5 |'
  prefs: []
  type: TYPE_TB
- en: '| TRN [[75](#bib.bib75)] | 2019 | 45.1 | 42.4 | 40.7 | 39.1 | 37.7 | 36.4 |
    35.3 | 34.3 | 38.9 |'
  prefs: []
  type: TYPE_TB
- en: '| TTPP [[76](#bib.bib76)] | 2020 | 45.9 | 43.7 | 42.4 | 41.0 | 39.9 | 39.4
    | 37.9 | 37.3 | 40.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LAP [[77](#bib.bib77)] | 2020 | 49.0 | 47.4 | 45.3 | 43.2 | 41.3 | 39.7 |
    38.3 | 37.0 | 42.6 |'
  prefs: []
  type: TYPE_TB
- en: '| OadTR [[80](#bib.bib80)] | 2021 | 50.2 | 49.3 | 48.1 | 46.8 | 45.3 | 43.9
    | 42.4 | 41.1 | 45.9 |'
  prefs: []
  type: TYPE_TB
- en: '| LSTR [[79](#bib.bib79)] | 2021 | – | – | – | – | – | – | – | – | 50.1 |'
  prefs: []
  type: TYPE_TB
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | 64.7 | 61.8 | 58.7 | 55.7 | 53.2 | 51.1
    | 49.2 | 47.8 | 55.3 |'
  prefs: []
  type: TYPE_TB
- en: '| TTPP [[76](#bib.bib76)] | 2020 | TS | K400 | 46.8 | 45.5 | 44.6 | 43.6 |
    41.9 | 41.1 | 40.4 | 38.7 | 42.8 |'
  prefs: []
  type: TYPE_TB
- en: '| LSTR [[79](#bib.bib79)] | 2021 | 60.4 | 58.6 | 56.0 | 53.3 | 50.9 | 48.9
    | 47.1 | 45.7 | 52.6 |'
  prefs: []
  type: TYPE_TB
- en: '| OadTR [[80](#bib.bib80)] | 2021 | 59.8 | 58.5 | 56.6 | 54.6 | 52.6 | 50.5
    | 48.6 | 46.8 | 53.5 |'
  prefs: []
  type: TYPE_TB
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | 66.2 | 63.5 | 60.5 | 57.4 | 54.8 | 52.6
    | 50.5 | 48.9 | 56.8 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE IX: Comparison of state-of-the-art methods on the validation and test
    set of EpicKitchens-100 in terms of mean top-5 recall (%). The optimal performance
    in each column within every block is indicated in bold. Modalities: Objects (O),
    Bounding Boxes (BB), Motion (M), Audio (Au).'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | Method | Year | Modality | Backbone | Init. | E2E | Overall | Unseen Kitchen
    | Tail Classes |'
  prefs: []
  type: TYPE_TB
- en: '| Verb | Noun | Act. |  Verb | Noun | Act. |  Verb | Noun | Act. |'
  prefs: []
  type: TYPE_TB
- en: '| Val | RULSTM [[98](#bib.bib98)] | 2019 | RGB | TSN | IN1K | ✗ | 27.5 | 29.0
    | 13.3 |  – | – | – |  – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | RGB | TSN | IN1K | ✗ | 24.2 | 29.8 | 13.0
    |  27.0 | 23.0 | 12.2 |  16.2 | 22.9 | 10.4 |'
  prefs: []
  type: TYPE_TB
- en: '| AVT [[67](#bib.bib67)] | 2021 | RGB | TSN | IN1K | ✗ | 27.2 | 30.7 | 13.6
    |  – | – | – |  – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| AVT [[67](#bib.bib67)] | 2021 | RGB | ViT-B | IN21K | ✓ | 30.2 | 31.7 | 14.9
    |  – | – | – |  – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | RGB | MViT-B | K400 | ✓ | 32.8 | 33.2 |
    15.1 |  27.5 | 21.7 | 9.8 |  26.3 | 27.4 | 13.2 |'
  prefs: []
  type: TYPE_TB
- en: '| MeMViT [[6](#bib.bib6)] | 2022 | RGB | MViT-L | K700 | ✓ | 32.2 | 37.0 |
    17.7 |  28.6 | 27.4 | 15.2 |  25.3 | 31.0 | 15.5 |'
  prefs: []
  type: TYPE_TB
- en: '| DCR [[71](#bib.bib71)] | 2022 | RGB | TSM | K400 | ✗ | 32.6 | 32.7 | 16.1
    |  – | – | – |  – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| TeSTra [[83](#bib.bib83)] | 2022 | RGB | TSN | IN1K | ✗ | 26.8 | 36.2 | 17.0
    |  27.1 | 30.1 | 13.3 |  19.3 | 28.6 | 13.7 |'
  prefs: []
  type: TYPE_TB
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K400 + IN1K | ✗ | 33.3
    | 35.5 | 17.6 |  – | – | – |  – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K700 | ✗ | 33.7 | 37.1
    | 18.0 |  – | – | – |  – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '|  | RULSTM [[98](#bib.bib98)] | 2019 | RGB,O,M | TSN | IN1K | ✗ | 27.8 | 30.8
    | 14.0 |  28.8 | 27.2 | 14.2 |  19.8 | 22.0 | 11.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TempAgg [[5](#bib.bib5)] | 2020 | RGB,O,M,BB | TSN | IN1K | ✗ | 23.2 |
    31.4 | 14.7 |  28.0 | 26.2 | 14.5 |  14.5 | 22.5 | 11.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AVT [[67](#bib.bib67)] | 2021 | RGB,O | ViT-B | IN21K | ✓ | 28.2 | 32.0
    | 15.9 |  29.5 | 23.9 | 11.9 |  21.1 | 25.8 | 14.1 |'
  prefs: []
  type: TYPE_TB
- en: '|  | DCR [[71](#bib.bib71)] | 2022 | RGB,O | TSM | K400 | ✗ | – | – | 18.3
    |  – | – | 14.7 |  – | – | 15.8 |'
  prefs: []
  type: TYPE_TB
- en: '|  | TeSTra [[83](#bib.bib83)] | 2022 | RGB,M | TSN | IN1K | ✗ | 30.8 | 35.8
    | 17.6 |  29.6 | 26.0 | 12.8 |  23.2 | 29.2 | 14.2 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AFFT [[105](#bib.bib105)] | 2023 | RGB,O,M | TSN | IN1K | ✗ | 21.3 | 32.7
    | 16.4 |  24.1 | 25.5 | 13.6 |  13.2 | 25.8 | 14.3 |'
  prefs: []
  type: TYPE_TB
- en: '|  | AFFT [[105](#bib.bib105)] | 2023 | RGB,O,M,Au | Swin-B | K400 | ✗ | 22.8
    | 34.6 | 18.5 |  24.8 | 26.4 | 15.5 |  15.0 | 27.7 | 16.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Test | RULSTM [[98](#bib.bib98)] | 2019 | RGB,O,M | TSN | IN1K | ✗ | 25.3
    | 26.7 | 11.2 |  19.4 | 26.9 | 9.7 |  17.6 | 16.0 | 7.9 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | RGB,O,M,BB | TSN | IN1K | ✗ | 21.8 | 30.6
    | 12.6 |  17.9 | 27.0 | 10.5 |  13.6 | 20.6 | 8.9 |'
  prefs: []
  type: TYPE_TB
- en: '| AVT [[67](#bib.bib67)] | 2021 | RGB,O | ViT-B | IN21K | ✓ | 25.6 | 28.8 |
    12.6 |  20.9 | 22.3 | 8.8 |  19.0 | 22.0 | 10.1 |'
  prefs: []
  type: TYPE_TB
- en: '| TCN-TBN [[103](#bib.bib103)] | 2021 | RGB,O,M | TBN | IN1K | ✗ | 21.5 | 26.8
    | 11.0 |  20.8 | 28.3 | 12.2 |  13.2 | 15.4 | 7.2 |'
  prefs: []
  type: TYPE_TB
- en: '| Abst. goal [[116](#bib.bib116)] | 2022 | RGB,O,M | TSN | IN1K | ✗ | 31.4
    | 30.1 | 14.3 |  31.4 | 35.6 | 17.3 |  – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| AFFT [[105](#bib.bib105)] | 2023 | RGB,O,M,Au | Swin-B | K400 | ✗ | 20.7
    | 31.8 | 14.9 |  16.2 | 27.7 | 12.1 |  13.4 | 23.8 | 11.8 |'
  prefs: []
  type: TYPE_TB
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K400 + IN1K | ✗ | 27.3
    | 32.8 | 14.0 |  – | – | – |  – | – | – |'
  prefs: []
  type: TYPE_TB
- en: '| RAFTformer [[73](#bib.bib73)] | 2023 | RGB | MViT-B | K700 | ✗ | 27.4 | 34.0
    | 14.7 |  – | – | – |  – | – | – |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE X: Benchmark of long-term action anticipation on Breakfast [[66](#bib.bib66)]
    in terms of mean over classes (%). For details on the font coding, please refer
    to Table [VII](#S8.T7 "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep
    Learning Techniques for Action Anticipation").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input Type | Backbone | Methods | Year | $\beta$ ($\alpha$ = 0.2) | $\beta$
    ($\alpha$ = 0.3) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 0.2 | 0.3 | 0.5 | 0.1 | 0.2 | 0.3 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GT. label | – | RNN [[46](#bib.bib46)] | 2018 | 60.35 | 50.44 | 45.28 | 40.42
    | 61.45 | 50.25 | 44.90 | 41.75 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN [[46](#bib.bib46)] | 2018 | 57.97 | 49.12 | 44.03 | 39.26 | 60.32 | 50.14
    | 45.18 | 40.51 |'
  prefs: []
  type: TYPE_TB
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 64.46 | 56.27 | 50.15 | 43.99 |
    65.95 | 55.94 | 49.14 | 44.23 |'
  prefs: []
  type: TYPE_TB
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 50.39 | 41.71 | 37.79 | 32.78 | 51.25
    | 42.94 | 38.33 | 33.07 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 65.50 | 55.50 | 46.80 | 40.10 | 67.40 |
    56.10 | 47.40 | 41.50 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[49](#bib.bib49)] (avg.) | 2020 | 72.22 | 62.40 | 56.22 | 45.95
    | 74.14 | 71.32 | 65.30 | 52.38 |'
  prefs: []
  type: TYPE_TB
- en: '| Pred. label | Fisher | RNN [[46](#bib.bib46)] | 2018 | 18.11 | 17.20 | 15.94
    | 15.81 | 21.64 | 20.02 | 19.73 | 19.21 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN [[46](#bib.bib46)] | 2018 | 17.90 | 16.35 | 15.37 | 14.54 | 22.44 | 20.12
    | 19.69 | 18.76 |'
  prefs: []
  type: TYPE_TB
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 15.69 | 14.00 | 13.30 | 12.95 | 19.14
    | 17.18 | 17.38 | 14.98 |'
  prefs: []
  type: TYPE_TB
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 18.41 | 17.21 | 16.42 | 15.84 |
    22.75 | 20.44 | 19.64 | 19.75 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 18.80 | 16.90 | 16.50 | 15.40 | 23.00 |
    20.00 | 19.90 | 18.60 |'
  prefs: []
  type: TYPE_TB
- en: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 37.40 | 31.20 | 30.00 | 26.10
    | 39.50 | 34.10 | 31.00 | 27.90 |'
  prefs: []
  type: TYPE_TB
- en: '| Features | Fisher | CNN [[46](#bib.bib46)] | 2018 | 12.78 | 11.62 | 11.21
    | 10.27 | 17.72 | 16.87 | 15.48 | 14.09 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 15.60 | 13.10 | 12.10 | 11.10 | 19.50 |
    17.00 | 15.60 | 15.10 |'
  prefs: []
  type: TYPE_TB
- en: '|  | I3D | TempAgg [[5](#bib.bib5)] | 2020 | 24.20 | 21.10 | 20.00 | 18.10
    | 30.40 | 26.30 | 23.80 | 21.20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Cycle Cons[[112](#bib.bib112)] | 2020 | 25.88 | 23.42 | 22.42 | 21.54
    | 29.66 | 27.37 | 25.58 | 25.20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-ACT [[115](#bib.bib115)] | 2022 | 26.70 | 24.30 | 23.20 | 21.70 | 30.80
    | 28.30 | 26.10 | 25.80 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FUTR [[82](#bib.bib82)] | 2022 | 27.70 | 24.55 | 22.83 | 22.04 | 32.27
    | 29.88 | 27.49 | 25.87 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Anticipatr [[117](#bib.bib117)] | 2022 | 37.40 | 32.00 | 30.30 | 28.60
    | 39.90 | 35.70 | 32.10 | 29.40 |'
  prefs: []
  type: TYPE_TB
- en: 'TABLE XI: Benchmark of long-term action anticipation on 50 Salads [[70](#bib.bib70)]
    in terms of mean over classes (%). For details on the font coding, please refer
    to Table [VII](#S8.T7 "TABLE VII ‣ 8 Supplementary Material ‣ A Survey on Deep
    Learning Techniques for Action Anticipation").'
  prefs: []
  type: TYPE_NORMAL
- en: '| Input Type | Backbone | Methods | Year | $\beta$ ($\alpha$ = 0.2) | $\beta$
    ($\alpha$ = 0.3) |'
  prefs: []
  type: TYPE_TB
- en: '| 0.1 | 0.2 | 0.3 | 0.5 | 0.1 | 0.2 | 0.3 | 0.5 |'
  prefs: []
  type: TYPE_TB
- en: '| GT. label | – | RNN [[46](#bib.bib46)] | 2018 | 42.30 | 31.19 | 25.22 | 16.82
    | 44.19 | 29.51 | 19.96 | 10.38 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN [[46](#bib.bib46)] | 2018 | 36.08 | 27.62 | 21.43 | 15.48 | 37.36 | 24.78
    | 20.78 | 14.05 |'
  prefs: []
  type: TYPE_TB
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 45.12 | 33.23 | 27.59 | 17.27 |
    46.40 | 34.80 | 25.24 | 13.84 |'
  prefs: []
  type: TYPE_TB
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 34.95 | 28.05 | 24.08 | 15.41 | 33.15
    | 24.65 | 18.84 | 14.34 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 47.20 | 34.60 | 30.50 | 19.10 | 44.80 |
    32.70 | 23.50 | 15.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Zhao et al. [[49](#bib.bib49)] (avg.) | 2020 | 46.63 | 35.62 | 31.91 | 21.37
    | 46.13 | 36.37 | 33.10 | 19.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Pred. label | Fisher | RNN [[46](#bib.bib46)] | 2018 | 30.06 | 25.43 | 18.74
    | 13.49 | 30.77 | 17.19 | 14.79 | 9.77 |'
  prefs: []
  type: TYPE_TB
- en: '| CNN [[46](#bib.bib46)] | 2018 | 21.24 | 19.03 | 15.98 | 9.87 | 29.14 | 20.14
    | 17.46 | 10.86 |'
  prefs: []
  type: TYPE_TB
- en: '| UAAA [[48](#bib.bib48)] (avg.) | 2019 | 23.56 | 19.48 | 18.01 | 12.78 | 28.04
    | 17.95 | 14.77 | 12.06 |'
  prefs: []
  type: TYPE_TB
- en: '| Time-Cond. [[127](#bib.bib127)] | 2019 | 32.51 | 27.61 | 21.26 | 15.99 |
    35.12 | 27.05 | 22.05 | 15.59 |'
  prefs: []
  type: TYPE_TB
- en: '| TempAgg [[5](#bib.bib5)] | 2020 | 32.70 | 26.30 | 21.90 | 15.60 | 32.30 |
    25.50 | 22.70 | 17.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Piergiovanni et al. [[124](#bib.bib124)] | 2020 | 40.40 | 33.70 | 25.40 |
    20.90 | 40.70 | 40.10 | 26.40 | 19.20 |'
  prefs: []
  type: TYPE_TB
- en: '| Features | Fisher | TempAgg [[5](#bib.bib5)] | 2020 | 25.50 | 19.90 | 18.20
    | 15.10 | 30.60 | 22.50 | 19.10 | 11.20 |'
  prefs: []
  type: TYPE_TB
- en: '|  | I3D | Cycle Cons. [[112](#bib.bib112)] | 2020 | 34.76 | 28.41 | 21.82
    | 15.25 | 34.39 | 23.70 | 18.95 | 15.89 |'
  prefs: []
  type: TYPE_TB
- en: '|  | A-ACT [[115](#bib.bib115)] | 2022 | 35.40 | 29.60 | 22.50 | 16.10 | 35.70
    | 25.30 | 20.10 | 16.30 |'
  prefs: []
  type: TYPE_TB
- en: '|  | FUTR [[82](#bib.bib82)] | 2022 | 39.55 | 27.54 | 23.31 | 17.77 | 35.15
    | 24.86 | 24.22 | 15.26 |'
  prefs: []
  type: TYPE_TB
- en: '|  | Anticipatr [[117](#bib.bib117)] | 2022 | 41.10 | 35.00 | 27.60 | 27.30
    | 42.80 | 42.30 | 28.50 | 23.60 |'
  prefs: []
  type: TYPE_TB
