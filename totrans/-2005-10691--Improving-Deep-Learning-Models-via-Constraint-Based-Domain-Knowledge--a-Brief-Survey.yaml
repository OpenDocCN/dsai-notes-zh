- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:01:10'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 20:01:10'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2005.10691] Improving Deep Learning Models via Constraint-Based Domain Knowledge:
    a Brief Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2005.10691] 通过基于约束的领域知识改进深度学习模型：简要综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2005.10691](https://ar5iv.labs.arxiv.org/html/2005.10691)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2005.10691](https://ar5iv.labs.arxiv.org/html/2005.10691)
- en: 'Improving Deep Learning Models via Constraint-Based Domain Knowledge: a Brief
    Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过基于约束的领域知识改进深度学习模型：简要综述
- en: Andrea Borghesi, Federico Baldo, Michela Milano
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 安德烈亚·博尔赫西，费德里科·巴尔多，米凯拉·米兰诺
- en: DISI, University of Bologna
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 博洛尼亚大学 DISI
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep Learning (DL) models proved themselves to perform extremely well on a wide
    variety of learning tasks, as they can learn useful patterns from large data sets.
    However, purely data-driven models might struggle when very difficult functions
    need to be learned or when there is not enough available training data. Fortunately,
    in many domains prior information can be retrieved and used to boost the performance
    of DL models.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）模型在广泛的学习任务中表现出色，因为它们可以从大数据集中学习有用的模式。然而，当需要学习非常困难的函数或没有足够的训练数据时，纯粹的数据驱动模型可能会遇到困难。幸运的是，在许多领域，可以提取并利用先验信息来提升DL模型的性能。
- en: 'This paper presents a first survey of the approaches devised to integrate domain
    knowledge , expressed in the form of constraints, in DL learning models to improve
    their performance, in particular targeting deep neural networks. We identify five
    (non-mutually exclusive) categories that encompass the main approaches to inject
    domain knowledge: 1) acting on the features space, 2) modifications to the hypothesis
    space, 3) data augmentation, 4) regularization schemes, 5) constrained learning.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本文介绍了首次综述了将领域知识（以约束形式表达）整合到DL学习模型中的方法，以提高其性能，特别是针对深度神经网络。我们识别了五种（非相互排斥的）类别，涵盖了注入领域知识的主要方法：1）作用于特征空间，2）对假设空间的修改，3）数据增强，4）正则化方案，5）约束学习。
- en: 1 Introduction
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: A vast array of Deep Learning (DL) approaches have been proven successful in
    many different learning tasks in recent years. One of the key strength of DL models
    is their ability to automatically learn a *representation* of the features composing
    a data set. Deep Neural Networks (DNNs) represent the foremost and widely spread
    class of DL models. Broadly speaking, DNNs are *sub-symbolic* ML approaches that
    are very good at extracting the useful information contained in large data sets.
    One of the advantages of DL techniques is that, in general, they do not rely on
    stringent assumptions on the distribution of the underlying data and on the function
    to be learned or approximated. This allows them to be applied in many different
    areas with very good results, without significant changes to the DNNs’ structure
    and training algorithm.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，各种深度学习（DL）方法在许多不同的学习任务中已被证明成功。DL模型的一个关键优势是它们能够自动学习数据集中组成特征的*表示*。深度神经网络（DNNs）是最重要和广泛应用的DL模型类别。广义而言，DNNs是*子符号*的机器学习方法，非常擅长从大型数据集中提取有用信息。DL技术的一个优点是，它们通常不依赖于对底层数据分布和待学习或近似的函数的严格假设。这使得它们可以在许多不同的领域中取得非常好的结果，而无需对DNNs的结构和训练算法进行重大修改。
- en: However, there are contexts where purely data-driven models are not an ideal
    fit, for example when scarce data of good quality and very difficult learning
    tasks. In such situations, a great boost in the performance of neural networks
    (and ML models in general) can be obtained through the exploitation of domain
    knowledge, e.g. problem-specific information that can be used to improve the DL
    model and/or simplify the training process (for instance, structured data, knowledge
    about the data generation process, domain experts experience, etc). Hence, it
    makes sense to take advantage of domain information to improve the performance
    of DL models, so that they do not have to start from scratch while dealing with
    difficult learning tasks. In other words, *why learn again something that you
    already know?*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，纯粹的数据驱动模型并不是理想的选择，例如当数据稀缺且学习任务非常困难时。在这种情况下，通过利用领域知识（例如可以用来改进DL模型和/或简化训练过程的特定问题信息，如结构化数据、关于数据生成过程的知识、领域专家经验等），可以显著提高神经网络（以及机器学习模型）的性能。因此，利用领域信息来提高DL模型的性能是有意义的，这样它们在处理困难学习任务时就不必从头开始。换句话说，*为何再学习你已经知道的东西？*
- en: In general, the integration of domain knowledge in DL models is a multi-faceted
    topic that has been extensively explored from multiple angles, with a wide range
    of approaches dealing with different types of domain knowledge and a very large
    number of different target DL models to be boosted. In this paper we do not claim
    to provide a comprehensive and exhaustive overview of all the methodologies proposed
    in the literature (for a broad overview [[32](#bib.bib32)] provide a great taxonomy),
    but we want to focus on a particular class of techniques for injecting prior information
    in DL models. Specifically, we consider prior knowledge that can be expressed
    in the form of constraints and as target models to be improved with this information
    we restrict our interest to DNNs. We take into account constraints of different
    nature, ranging from first-order and propositional logic predicates to linear
    and non-linear equations. In the scope of this paper the constraints can represent
    relationships between the input features, relationships between input and output
    features, bounds on the output variables. We consider both hard constraints (which
    set conditions that must be satisfied) and soft constraints (which set conditions
    with an associated penalty, in case they are not satisfied).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，领域知识在深度学习模型中的集成是一个多方面的话题，已从多个角度进行了广泛的探索，涉及不同类型的领域知识和大量不同的目标深度学习模型。在本文中，我们并不声称提供文献中所有方法论的全面和详尽的概述（有关广泛概述，[[32](#bib.bib32)]
    提供了很好的分类），而是希望专注于一种特定的技术类别，用于将先验信息注入深度学习模型。具体而言，我们考虑可以用约束形式表示的先验知识，以及要用这些信息改进的目标模型，我们将兴趣限制在深度神经网络（DNNs）上。我们考虑不同性质的约束，从一阶和命题逻辑谓词到线性和非线性方程。在本文的范围内，这些约束可以表示输入特征之间的关系、输入与输出特征之间的关系、输出变量的界限。我们考虑硬约束（设定必须满足的条件）和软约束（设定条件并附加惩罚，如果未满足的情况下）。
- en: 'The paper is structured as follows. Section [2](#S2 "2 Domain Knowledge Injection
    Approaches ‣ Improving Deep Learning Models via Constraint-Based Domain Knowledge:
    a Brief Survey") is the core of the survey and presents research items from the
    literature for the injection of domain knowledge (expressed as constraints) in
    DNNs, grouping the related works in five macro-areas. Section [3](#S3 "3 Related
    Areas ‣ Improving Deep Learning Models via Constraint-Based Domain Knowledge:
    a Brief Survey") highlights analogies between the surveyed papers and other, closely
    related, fields within the DL area. Section [4](#S4 "4 Observations & Remarks
    ‣ Improving Deep Learning Models via Constraint-Based Domain Knowledge: a Brief
    Survey") discusses common trends among the injection approaches and provides observations
    and insights. Finally, Section [5](#S5 "5 Conclusion ‣ Improving Deep Learning
    Models via Constraint-Based Domain Knowledge: a Brief Survey") summarizes and
    concludes the paper.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '论文结构如下。第[2](#S2 "2 Domain Knowledge Injection Approaches ‣ Improving Deep Learning
    Models via Constraint-Based Domain Knowledge: a Brief Survey")节是调查的核心，介绍了文献中关于将领域知识（以约束形式表达）注入深度神经网络（DNNs）的研究项目，并将相关工作分为五个宏观领域。第[3](#S3
    "3 Related Areas ‣ Improving Deep Learning Models via Constraint-Based Domain
    Knowledge: a Brief Survey")节强调了调查论文与深度学习领域内其他相关领域之间的类比。第[4](#S4 "4 Observations
    & Remarks ‣ Improving Deep Learning Models via Constraint-Based Domain Knowledge:
    a Brief Survey")节讨论了注入方法的共同趋势，并提供了观察和见解。最后，第[5](#S5 "5 Conclusion ‣ Improving
    Deep Learning Models via Constraint-Based Domain Knowledge: a Brief Survey")节总结并结束了本文。'
- en: 2 Domain Knowledge Injection Approaches
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 领域知识注入方法
- en: 'In this section we discuss recent methods for injecting prior information in
    DNNs. We assume to have a learning task where the learner is trying to approximate
    a function $f^{*}$ that maps an input $X$ to an output $y$; the training set is
    composed by examples $(x_{i},y_{i})$. We consider domain knowledge that can be
    expressed as a set of constraints, or predicates, $\pi$. Domain knowledge can
    be represented by algebraic equations, such as linear and non-linear equations,
    equality and inequality constraints, logic formulas. These predicates can be applied
    to different groups of variables: 1) they can involve only the input features
    $X$, e.g. it is known that the input features of well-formed data instances share
    particular properties, or are linked by a set of precise relations; 2) the constraints
    can concern only the output variables $y$, e.g. the output of the network must
    fall within a determined range; 3) the condition can involve both input $x_{i}$
    and output $y_{i}$, for instance, the real function to be approximated $f$ could
    be monotonic ($x_{1}\leq x_{2}\rightarrow y_{1}\leq y_{2}$). As a particular case
    of constraint-based information we also include domain knowledge expressed in
    the form of graphs (e.g. knowledge graphs), as graphs can be decomposed as collections
    of simpler constraints encapsulating the relations between nodes and edges. With
    these assumptions, the constraint-expressed domain knowledge can be integrated
    in the DNNs in multiple ways. We classify literature works in five branches, based
    on the injection mechanism: 1) feature space, 2) hypothesis space, 3) data augmentation,
    4) regularization schemes, 5) constraints learning. Each subsection is devoted
    to a branch.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们探讨了在深度神经网络（DNNs）中注入先验信息的最新方法。我们假设有一个学习任务，其中学习者试图逼近一个将输入$X$映射到输出$y$的函数$f^{*}$；训练集由例子$(x_{i},y_{i})$组成。我们考虑可以用一组约束或谓词$\pi$表达的领域知识。领域知识可以通过代数方程表示，如线性和非线性方程、等式和不等式约束、逻辑公式。这些谓词可以应用于不同的变量组：1）它们可能只涉及输入特征$X$，例如已知良好数据实例的输入特征共享特定属性，或通过一组精确的关系关联；2）约束可能只涉及输出变量$y$，例如网络的输出必须在特定范围内；3）条件可以涉及输入$x_{i}$和输出$y_{i}$，例如，待逼近的真实函数$f$可能是单调的（$x_{1}\leq
    x_{2}\rightarrow y_{1}\leq y_{2}$）。作为基于约束的信息的一个特殊情况，我们还包括以图的形式表达的领域知识（例如知识图谱），因为图可以被分解为包含节点和边之间关系的更简单约束的集合。基于这些假设，可以通过多种方式将约束表达的领域知识集成到DNNs中。我们将文献中的工作分为五个分支，基于注入机制：1）特征空间，2）假设空间，3）数据增强，4）正则化方案，5）约束学习。每个子部分都专门讨论一个分支。
- en: 2.1 Feature Space
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 特征空间
- en: DL models’ performance strongly depends on the quality of the available training
    data, either labeled or not; the features in the data define the *feature space*
    whose implicit information is extracted by DNNs. The shape of the feature space
    is a critical issue for the performance of DL models and the ease of their training.
    For instance, *feature engineering* is a common method for improving the accuracy
    of purely data-driven ML models by selecting useful features and/or transforming
    the original ones to facilitate the learner’s task. In general, this is a difficult
    problem and requires much effort, both from system expert and ML practitioners [[14](#bib.bib14)].
    In recent years, several research avenues studied the possibility to automatically
    explore the feature space in order to extract only the most relevant features,
    with most of the approaches belonging to the AutoML area [[26](#bib.bib26)] and
    reinforcement learning [[15](#bib.bib15)]. The majority of current feature engineering
    methods aim at selecting the optimal features for a specific learning task and
    generally tend to reduce the feature space, by selecting only the most relevant
    features (feature extraction or feature compression). Furthermore, these methods
    are generally purely data-driven and require large amounts of data; although they
    aim at exploring the feature space in an efficient way, when the number of features
    is large this becomes a non trivial problem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: DL模型的表现高度依赖于可用训练数据的质量，无论是标记的还是未标记的；数据中的特征定义了*特征空间*，其隐含信息由DNN提取。特征空间的形状对DL模型的表现和训练的难易程度是一个关键问题。例如，*特征工程*是一种常见方法，通过选择有用特征和/或转换原始特征来提高纯数据驱动ML模型的准确性，以促进学习者的任务。一般而言，这是一个困难的问题，需要系统专家和ML从业者付出大量努力[[14](#bib.bib14)]。近年来，几条研究路径探讨了自动探索特征空间以提取最相关特征的可能性，大多数方法属于AutoML领域[[26](#bib.bib26)]和强化学习[[15](#bib.bib15)]。目前大多数特征工程方法旨在为特定学习任务选择最佳特征，通常倾向于通过选择最相关的特征（特征提取或特征压缩）来减少特征空间。此外，这些方法通常是纯数据驱动的，且需要大量数据；尽管它们旨在以高效的方式探索特征空间，但当特征数量很大时，这变成了一个非平凡的问题。
- en: A relatively unexplored direction is the use of domain knowledge to create novel
    features that render *explicit* the information hidden in the raw data. This is
    a form of feature space extension with the purpose of highlighting the prior knowledge
    embedded in the original features but not easily extractable by a neural network.
    In practice, the approaches proposed in this area work on the original input features
    $X$ to generate an extended feature set $X^{\prime}=X\cup\{x_{j}\}$, where $\{x_{j}\},j\in
    1,..N_{j}$ is the set composed by the additional features ($N_{j}$ is the number
    of added features). The new features $x_{j}$ are computed as combination (linear
    or non-linear equations) of the original ones, depending on the domain constraints.
    For example, [[1](#bib.bib1)] enrich the feature space using the domain-specific
    information encoded by knowledge graphs. The relationships explicitly described
    by the knowledge graph represent constraints (soft and hard) among the original
    input features and can be used to create additional features, which then improve
    the accuracy of a supervised DNN. In a similar fashion [[23](#bib.bib23)] increase
    the feature space with domain-based features and boost the performance of a DL
    model. They do not use a knowledge graph to obtain the additional features but
    rather an ensemble of decision trees solving a classification task on a sub-set
    of the data devoted to the training of the DL model. Each tree learns domain-specific
    information and partakes to the final prediction through its own score; these
    scores are then added as additional features to the training set. Another method
    to incorporate domain knowledge by enriching the feature space is discussed by
    [[3](#bib.bib3)], which study the improvement of a DNN for the prediction of soccer
    match outcomes obtained through the addition of domain-inspired features. The
    training set is a time series composed of matches among two different teams and
    related outcomes; their approach consists in adding a set of novel features that
    encapsulate i) the rating of the teams involved in the match and ii) the results
    obtained in the last $k$ matches by each team. These novel features are encoded
    as a set of linear and non-linear equations applied to the original input $X$.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相对未被探索的方向是利用领域知识创建新特征，从而*明确*原始数据中隐藏的信息。这是一种特征空间扩展的形式，目的是突出原始特征中嵌入的先验知识，但神经网络不容易提取。在实际应用中，该领域提出的方法在原始输入特征
    $X$ 上工作，生成一个扩展的特征集 $X^{\prime}=X\cup\{x_{j}\}$，其中 $\{x_{j}\},j\in 1,..N_{j}$ 是由额外特征组成的集合（$N_{j}$
    是添加的特征数量）。新特征 $x_{j}$ 的计算是原始特征的组合（线性或非线性方程），这取决于领域约束。例如，[[1](#bib.bib1)] 使用由知识图谱编码的领域特定信息来丰富特征空间。知识图谱中明确描述的关系表示原始输入特征之间的约束（软约束和硬约束），这些关系可以用来创建附加特征，从而提高监督DNN的准确性。类似地，[[23](#bib.bib23)]
    通过领域基础特征来增加特征空间，提升DL模型的性能。他们没有使用知识图谱来获取额外特征，而是使用决策树集群解决一个子集数据的分类任务，这些数据用于DL模型的训练。每棵树学习领域特定的信息，并通过其自身的分数参与最终预测；这些分数随后作为附加特征添加到训练集中。另一个通过丰富特征空间来整合领域知识的方法由
    [[3](#bib.bib3)] 讨论，该方法研究了通过添加领域启发特征来改进DNN在预测足球比赛结果中的表现。训练集是由两支不同球队的比赛及其相关结果组成的时间序列；他们的方法包括添加一组新特征，涵盖
    i) 比赛中涉及的球队的评级和 ii) 每支球队在最近 $k$ 场比赛中取得的结果。这些新特征被编码为应用于原始输入 $X$ 的一组线性和非线性方程。
- en: 2.2 Hypothesis Space
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 假设空间
- en: A DNN can be characterized by its position in the so called *hypothesis space*,
    namely the multi-dimensional space covering its structure and its hyperparameters.
    The architecture of a NN is a very relevant factor for determining its performance
    on different learning tasks. The hypothesis space has been explored with implicit
    guidance provided by domain knowledge for many years, as attested by the introduction
    of convolutional networks, whose structure is based on the locality assumption
    (e.g. pixels close to each other in an image are related). Implicit knowledge
    about temporal locality has also lead to a wide range of architectures targeted
    at handling time series and sequences, for instance recurrent NNs (RNNs), Long-Short
    Term Memory NNs (LSTMs), Temporal Convolutional Networks [[2](#bib.bib2)].
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: DNN可以通过其在所谓的*假设空间*中的位置来特征化，即覆盖其结构和超参数的多维空间。神经网络的架构是决定其在不同学习任务中性能的一个非常重要的因素。假设空间多年来一直在领域知识的隐式指导下进行探索，这一点可以通过卷积网络的引入得到证明，卷积网络的结构基于局部性假设（例如，图像中彼此接近的像素是相关的）。对时间局部性的隐性知识也导致了针对处理时间序列和序列的广泛架构，例如递归神经网络（RNNs）、长短期记忆网络（LSTMs）、时间卷积网络[[2](#bib.bib2)]。
- en: In more recent years, a remarkable research effort has been devoted to exploring
    DNN architectures optimized for circumstances where the domain knowledge can be
    expressed in the form of graphs, precisely with a DL model called *Graph Neural
    Network* (GNNs) [[27](#bib.bib27)]; *Graph Convolutional Networks* (GCNN) [[16](#bib.bib16)]
    were introduced as well to exploit the same type of graph-expressible prior information.
    GNNs have been used in several fields [[37](#bib.bib37)], owning to their capability
    to deal with data whose structure can be described via graphs, thanks to a generalization
    in the spectral domain of the convolutional layers found in many deep learning
    networks. GCNNs most common applications involve semi-supervised classification
    tasks, with the goal of predicting the class of unlabeled nodes in a graph – a
    case of graph learning. Using prior information expressible as graph has been
    proposed also to devise other types of NNs, namely networks whose structure resembles
    the LSTM’s one but where the nodes connections are determined by the prior information [[20](#bib.bib20)].
    Similarly, [[13](#bib.bib13)] combine the temporal structure of RNNs with spatial-based
    information, namely the domain information is encoded as sequences of actions
    each one characterized by a time and a position in space, which are then represented
    through an extended RNN. As mentioned earlier, domain knowledge represented as
    a graph can be expressed by sets of constraints, which encode the relationships
    between the nodes through the edges. Currently, the vast majority of information
    injection methods in this area (and all those listed in this section) operate
    on the input features $X$, which can be cast as concepts and related connections
    within the graph structure.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 最近几年，显著的研究努力集中在探索针对可以用图的形式表达领域知识的情况的深度神经网络（DNN）架构上，特别是利用一种被称为*图神经网络*（GNNs）的深度学习模型[[27](#bib.bib27)]；*图卷积网络*（GCNN）[[16](#bib.bib16)]也被引入以利用相同类型的图可表达先验信息。由于GCNN能够处理数据，其结构可以通过图来描述，这得益于在许多深度学习网络中卷积层在谱域的推广，GNNs已被广泛应用于多个领域[[37](#bib.bib37)]。GCNNs最常见的应用涉及半监督分类任务，目的是预测图中未标记节点的类别——这是一种图学习的情况。利用可以表达为图的先验信息也被提议用于设计其他类型的神经网络，即那些结构类似于LSTM的网络，但节点连接由先验信息决定[[20](#bib.bib20)]。类似地，[[13](#bib.bib13)]将RNNs的时间结构与基于空间的信息结合起来，即领域信息被编码为每个动作的序列，每个动作都有时间和空间位置，然后通过扩展的RNN进行表示。如前所述，表示为图的领域知识可以通过一组约束来表达，这些约束通过边缘编码节点之间的关系。目前，该领域绝大多数信息注入方法（以及本节列出的所有方法）都在输入特征$X$上操作，这些特征可以被看作是图结构中的概念和相关连接。
- en: 2.3 Data Augmentation
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 数据增强
- en: Besides working on the feature and the hypothesis space, another mechanism to
    infuse domain knowledge in DNNs regards the training data, mainly in the forms
    of creating *ex-novo* entire training sets or adding new examples to existing
    ones following criteria defined by the domain knowledge (for instances, examples
    respecting certain relationships among the input features). We refer to these
    methodologies with the term *data augmentation*. Data augmentation approaches
    based on prior information (constraints) have been started to be explored in recent
    years, especially to cope with data sets of limited size and the related issue
    of poor generalization performance [[12](#bib.bib12)]. Data augmentation techniques
    have a strong history of success in the context of image-based learning tasks
    (e.g. image classification) [[33](#bib.bib33)]. For image classification tasks,
    the training set can be augmented by applying a plethora of transformations to
    the images in the original training set, thus feeding the NN to be trained with
    a more varied set of examples. The selection of the best transformations to apply
    to augment the available data is a process that is typically guided by information
    obtained via domain experts. In particular, constraint-based domain information
    can be used to augment the available data, e.g. linear and non-linear functions
    such as rotation, distortion, flipping, etc. For instance, [[4](#bib.bib4)] propose
    a data augmentation technique to train a LSTM model for classifying chemical molecules.
    Each molecule can be described as a combination of its composing elements, encoded
    as a concatenation of strings. A single molecule has multiple possible encoding
    strings; the authors propose to expand the training set by enumerating the chemically
    allowed combinations for each data point/molecule. The enumeration takes place
    via a heuristic algorithm that enforces on the generated examples the same chemical
    properties of the original data points; these properties are encoded as a collection
    of constraints (linear combinations representing admissible chemical properties)
    among the input features (the concatenated strings). In this case the constraints
    involve both input features $x_{i}$ and the output $y_{i}$, as the relationships
    among $X$ are used to generate novel data points with the same output value (the
    label).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在特征和假设空间上进行工作，另一个将领域知识注入深度神经网络（DNN）的机制涉及训练数据，主要表现为创建*全新*的整个训练集或根据领域知识定义的标准（例如，遵循输入特征之间某些关系的示例）向现有数据集中添加新示例。我们将这些方法称为*数据增强*。基于先前信息（约束）的数据增强方法近年来开始被探索，特别是为了应对数据集规模有限及相关的泛化性能差的问题[[12](#bib.bib12)]。数据增强技术在基于图像的学习任务（例如图像分类）中有着成功的历史[[33](#bib.bib33)]。对于图像分类任务，可以通过对原始训练集中的图像应用各种变换来增强训练集，从而为神经网络提供更为多样化的示例。选择最佳的变换方法来增强现有数据通常是由领域专家提供的信息指导的。特别是，基于约束的领域信息可以用来增强现有数据，例如旋转、扭曲、翻转等线性和非线性函数。例如，[[4](#bib.bib4)]提出了一种数据增强技术来训练LSTM模型以分类化学分子。每个分子可以描述为其组成元素的组合，编码为字符串的串联。单个分子有多种可能的编码字符串；作者建议通过枚举每个数据点/分子的化学允许组合来扩展训练集。枚举通过启发式算法进行，该算法对生成的示例强加了与原始数据点相同的化学性质；这些性质被编码为输入特征（串联字符串）之间的约束集合（表示可接受的化学性质的线性组合）。在这种情况下，约束涉及输入特征$x_{i}$和输出$y_{i}$，因为$X$之间的关系用于生成具有相同输出值（标签）的新数据点。
- en: '[[24](#bib.bib24)] propose a different methodology for data set augmentation:
    they consider *feature side-information*, domain knowledge describing feature
    properties and/or feature relations. The feature side-information are expressed
    as a matrix whose rows represent the prior information associated to each feature;
    a similarity function is introduced to compute the pairwise similarity of different
    features. An augmented training data point is obtained from an original example
    by applying a transformation that preserves the associated label and perturbs
    the values of similar features. Again, the focus is on constraints among input
    features $x_{i}$. Similarly, [[31](#bib.bib31)] devise a data augmentation method
    based on similarity score among features, for the purpose of improving the results
    of a CNNs used for sentiment analysis. In this case the original data set is composed
    by labeled sentences (the training examples); each sentence can be decomposed
    in a set of sentiment terms, i.e. the features. The author introduce a similarity
    measure for the sentiments and then propose an algorithm (based on quadratic programming)
    to generate similar sentences from the original ones, based on the sentiment similarity
    score; the augmented similar examples are annotated with the same label as the
    corresponding original training points.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '[[24](#bib.bib24)] 提出了一个不同的数据集增强方法：他们考虑了*特征侧信息*，即描述特征属性和/或特征关系的领域知识。特征侧信息以矩阵的形式表示，其中的行代表与每个特征相关的先验信息；引入了相似性函数来计算不同特征之间的成对相似性。通过应用一种保留相关标签并扰动相似特征值的变换，从原始示例中获得一个增强的训练数据点。再次强调，重点在于输入特征
    $x_{i}$ 之间的约束。类似地，[[31](#bib.bib31)] 设计了一种基于特征之间相似度的增强方法，旨在提高用于情感分析的 CNN 的结果。在这种情况下，原始数据集由标记句子（训练示例）组成；每个句子可以分解为一组情感词，即特征。作者引入了一种情感相似度度量，然后提出了一种基于二次规划的算法，从原始句子中生成相似句子，基于情感相似度分数；增强的相似示例标注为与相应原始训练点相同的标签。'
- en: 2.4 Regularization Schemes
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4 正则化方案
- en: 'Regularization is a widely known method to avoid overfitting in machine learning
    models, but it can also be exploited to inject domain knowledge. To do so, some
    form of prior insight, i.e constraints, is translated into a regularization term
    able to measure the level of consistency with the domain knowledge and guide the
    loss optimization process. More in general, the loss function can be defined as
    follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化是避免机器学习模型过拟合的广泛使用的方法，但它也可以用来注入领域知识。为此，一些形式的先验知识，即约束，被转换为能够测量与领域知识一致性的正则化项，并指导损失优化过程。更一般地，损失函数可以定义如下：
- en: '|  | $\sum_{i=1}^{N}\lambda_{\tau}L(f(x_{i}),y_{i})+\lambda_{\pi}L_{\pi}(f(x_{i}))$
    |  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|  | $\sum_{i=1}^{N}\lambda_{\tau}L(f(x_{i}),y_{i})+\lambda_{\pi}L_{\pi}(f(x_{i}))$
    |  |'
- en: where $L$ represents the true loss, e.g. MSE, while $L_{\pi}$ represents the
    regularization term given a set of constraints $\pi$; these terms are weighted,
    respectively, with $\lambda_{\tau}$ and $\lambda_{\pi}$; the choice of these weight
    parameters is non-trivial and might consistently affect the final outcome ([[10](#bib.bib10)]
    address this problem exploiting *lagrangian duality*). In the last few years many
    authors worked on this class of methods proposing a variety of approaches to the
    problem; for instance, [[25](#bib.bib25)], developed a DNN, called *domain adapted
    neural network* (DANN), which exploits a mathematical formulation of the constraints
    , namely approximation and monotonicity constraints, whose degree of satisfaction
    is used as regularization term. A more general approach is introduced by [[8](#bib.bib8)]
    with *Semantic Based Regularization* (SBR), which provides a set of rules to translate
    first-order logic formulas into fuzzy constraints, then used as penalty factors
    in the loss function. This is achieved by introducing a *t-norm* function, which
    can be defined in different ways, according to the desired interpretation of the
    domain constraints involved in the regularization, allowing for an adaptable tool.
    [[22](#bib.bib22)] present a strongly related method denoted *LYRICS*, a framework
    that introduces a declarative language to express the domain knowledge and enforces
    it using SBR on top of DNNs, allowing a very flexible representation of the constraints
    and the prior information. A stochastic approach with semantic loss is proposed
    by [[35](#bib.bib35)], which use a regularization term given by the probability
    of generating a state satisfying the domain-based constraints; during the training
    process, the presence of states not satisfying the desired constraints is penalized
    acting on the loss function. [[29](#bib.bib29)] propose a SBR-inspired technique
    for injecting domain constraints in a DNN used to extend a partial variable assignment
    for the Partial Latin Square problem, specifically finding feasible solutions
    that respect domain constraints.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $L$ 代表真实的损失，例如 MSE，而 $L_{\pi}$ 代表在一组约束 $\pi$ 下的正则化项；这些项分别用 $\lambda_{\tau}$
    和 $\lambda_{\pi}$ 加权；这些权重参数的选择并非简单且可能持续影响最终结果（[[10](#bib.bib10)] 利用 *拉格朗日对偶性*
    解决了这个问题）。在过去几年中，许多作者在这类方法上进行研究，提出了各种解决问题的方法；例如，[[25](#bib.bib25)] 开发了一个叫做 *领域自适应神经网络*（DANN）的
    DNN，利用约束的数学公式，即逼近和单调性约束，其满足程度用作正则化项。[[8](#bib.bib8)] 提出的 *基于语义的正则化*（SBR）方法是一种更通用的方式，它提供了一套规则，将一阶逻辑公式转化为模糊约束，然后作为损失函数中的惩罚因子。这通过引入一个
    *t-范数* 函数来实现，该函数可以通过不同的方式定义，以适应正则化中涉及的领域约束的解释，从而提供了一个可调整的工具。[[22](#bib.bib22)]
    提出了一种高度相关的方法，称为 *LYRICS*，这是一个引入声明性语言以表达领域知识的框架，并在 DNN 上应用 SBR，从而允许对约束和先验信息进行非常灵活的表示。[[35](#bib.bib35)]
    提出了一个带有语义损失的随机方法，该方法使用一个正则化项，该项由生成满足领域约束的状态的概率给出；在训练过程中，不满足所需约束的状态会受到惩罚，作用于损失函数。[[29](#bib.bib29)]
    提出了一个受 SBR 启发的技术，用于在 DNN 中注入领域约束，扩展部分变量分配以解决部分拉丁方问题，具体是找到符合领域约束的可行解。
- en: 2.5 Learning with Constraints
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5 在约束条件下学习
- en: 'DL models are often employed in specific tasks, e.g. to find solutions to optimization
    problems. Usually these approaches follow a two-step process where: first the
    model is trained with the observed data and then used to approximate an aspect
    of the optimization problem, e.g. the cost function. In these cases, the model’s
    accuracy is not the only way to measure the performance of the approach, as task
    related metrics might be more relevant. Lately a new paradigm emerged, namely
    *decision-focused learning* (or *end-to-end learning*), where DNNs are trained
    to directly produce good results for the end goal of the whole task. In this context,
    prior information for the specific optimization task is introduced to guide the
    training of the DNNs used to approximate the needed functions. This class of approaches
    shares some aspects with the regularization ones, as the penalty terms in the
    loss function can be applied also for end-to-end learning. However, since in this
    case the DNNs are used within larger optimization models, the mechanism to influence
    their behaviour is different. In general, the DNNs for decision-focused learning
    produce a proxy (intermediate) solution for the target task, given a fixed parametrization
    of the network, then optimized by the loss function; however, as the solution
    space might be not continuous, differentiability issues might arise, which are
    in general solved through transformations.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）模型通常用于特定任务，例如解决优化问题。这些方法通常遵循两个步骤的过程：首先使用观察到的数据训练模型，然后用来近似优化问题的某个方面，例如成本函数。在这些情况下，模型的准确性并不是衡量方法性能的唯一标准，因为任务相关的指标可能更为重要。最近出现了一种新的范式，即*决策聚焦学习*（或*端到端学习*），其中深度神经网络（DNN）被训练以直接产生整体任务的良好结果。在这种情况下，引入了针对特定优化任务的先验信息，以指导DNN的训练，用于近似所需的函数。这类方法与正则化方法有一些相似之处，因为损失函数中的惩罚项也可以应用于端到端学习。然而，由于在这种情况下DNN被用于更大的优化模型中，影响其行为的机制有所不同。一般来说，决策聚焦学习的DNN为目标任务生成代理（中间）解，给定网络的固定参数化，然后通过损失函数进行优化；然而，由于解空间可能不连续，可能会出现可微性问题，这通常通过变换解决。
- en: 'A first example of this method can be seen in [[34](#bib.bib34)], where it
    is applied to combinatorial optimization with the introduction of a continuous
    relaxation of the proxy solution (which is discrete), namely a *convex hull*;
    the loss is then computed by combining the gradient with respect to the decision
    variables and the gradient of the model with respect to its own parameters. A
    slightly different contribution is given by [[9](#bib.bib9)], which apply end-to-end
    learning on a stochastic optimization problem; however, in this approach the focus
    is on the training process, where the gradient descent updates the network parameters
    using two functions: one computing the number of constraint violations and one
    represented by a classic loss function. If the proxy solution does not satisfy
    the constraints, the parameters are updated using the first function, otherwise
    using log-likelihood loss.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的第一个例子可以在[[34](#bib.bib34)]中看到，其中它被应用于组合优化，通过引入代理解的连续放松（代理解是离散的），即*凸包*；然后通过将关于决策变量的梯度与模型自身参数的梯度相结合来计算损失。[[9](#bib.bib9)]提供了一个略有不同的贡献，它将端到端学习应用于随机优化问题；然而，在这种方法中，重点在于训练过程，其中梯度下降使用两个函数来更新网络参数：一个计算约束违反次数，另一个由经典损失函数表示。如果代理解不满足约束，则使用第一个函数更新参数，否则使用对数似然损失。
- en: '[[17](#bib.bib17)] offer a totally different method to tackle problems where
    prior knowledge is expressed with first-order logic formulas; in this case the
    prior information is embedded directly in the DNN, where nodes, called *named
    neurons*, are labeled to mimic the logic elements in the formula; these are used
    to build *constrained neural layers*, which produce their output according to
    the truth value of each named neuron in the layer.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '[[17](#bib.bib17)]提供了一种完全不同的方法来处理先验知识通过一阶逻辑公式表达的问题；在这种情况下，先验信息直接嵌入到DNN中，其中节点称为*命名神经元*，被标记以模仿公式中的逻辑元素；这些节点用于构建*约束神经层*，根据层中每个命名神经元的真值来生成输出。'
- en: 'Recently, [[19](#bib.bib19)] introduced an approach similar to the end-to-end
    learning paradigm, but more general, called *DL2*. DL2 is a framework for explicitly
    embedding constraints in DNNs, specifically, it allows to translate logical formulas
    into loss functions, i.e. by defining recursively the corresponding mathematical
    equation for each term in the formula; the training is then carried out using
    projected gradient descent. The model is also provided with a SQL-like query language,
    which allows to interrogate the network on a specific input, in order to: check
    if the constraints are satisfied and train the model for input outside of the
    set of observed data; basically, the network can be challenged with queries that
    help improve its accuracy beyond the data available for the training – this method
    is called *global training*.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，[[19](#bib.bib19)]引入了一种类似于端到端学习范式的方法，但更通用，称为*DL2*。DL2是一个显式嵌入约束到DNN中的框架，具体来说，它允许将逻辑公式转换为损失函数，即通过递归定义公式中每个项的相应数学方程；训练则使用投影梯度下降进行。该模型还提供了类似SQL的查询语言，允许对特定输入进行网络查询，以便：检查约束是否得到满足，并为观测数据集之外的输入训练模型；基本上，网络可以通过查询来挑战，以帮助提高其准确性超出训练数据——这种方法称为*全球训练*。
- en: 3 Related Areas
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 相关领域
- en: 'Another field where prior information has been exploited to improve performance
    is *reinforcement learning*. In this area as well the knowledge domain information
    can be used to improve the models and/or transform very difficult problems into
    more tractable ones; for example, by creating good initial conditions for the
    training algorithm, thus decreasing the number of required training examples and
    therefore providing a *warm start* for the reinforcement learning process [[28](#bib.bib28)].
    Prior information has been successfully applied with remarkable benefits in various
    context well suited for reinforcement learning [[18](#bib.bib18)]. However, an
    important aspect has to be considered while injecting domain knowledge in this
    setting: uncertainty in the domain knowledge can greatly hinder the learning process
    if wrong decisions (caused by uncertain or incomplete information on the system
    state) are taken at the beginning of the learning phase [[30](#bib.bib30)]. Another
    area where domain knowledge has been shown to be beneficial is the initialization
    of the weights of a DNN. For instance, [[36](#bib.bib36)] describe a semi-supervised
    pre-training strategy for DL models to predict the behaviour of industrial processes;
    the pre-training is based on domain information regarding the chemical properties
    and relations among the data set features. With a similar strategy [[11](#bib.bib11)]
    exploit semantic-based knowledge to address an artificial task unsolvable by standard
    ML techniques; the key idea is to provide “hints” to the learner about appropriate
    intermediate concepts.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个利用先验信息来提升性能的领域是*强化学习*。在这个领域，知识领域信息也可以用来改进模型和/或将非常困难的问题转化为更易处理的问题；例如，通过为训练算法创建良好的初始条件，从而减少所需的训练样本数量，并因此为强化学习过程提供*温暖的起点*[[28](#bib.bib28)]。先验信息在各种适合强化学习的背景下已成功应用，带来了显著的好处[[18](#bib.bib18)]。然而，在这种设置中注入领域知识时必须考虑一个重要的方面：领域知识中的不确定性如果在学习阶段初期做出错误的决策（由系统状态的某些不确定或不完整的信息引起）可能会严重阻碍学习过程[[30](#bib.bib30)]。另一个领域是领域知识在深度神经网络（DNN）权重初始化中的好处。例如，[[36](#bib.bib36)]描述了一种针对工业过程行为预测的半监督预训练策略；该预训练基于关于化学性质和数据集特征之间关系的领域信息。类似地，[[11](#bib.bib11)]利用基于语义的知识来解决标准机器学习技术无法解决的人工任务；关键思想是向学习者提供关于适当中间概念的“提示”。
- en: 4 Observations & Remarks
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 观察与备注
- en: In this section we will discuss some common traits spanning over all the knowledge
    injection methodologies described previously and present some insights.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论之前描述的所有知识注入方法的一些共同特征，并提供一些见解。
- en: Feature manipulation
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征操作
- en: 'The features composing the data are often the target of the techniques described
    in Sec.  [2](#S2 "2 Domain Knowledge Injection Approaches ‣ Improving Deep Learning
    Models via Constraint-Based Domain Knowledge: a Brief Survey"), as they offer
    a practical mechanism to inject prior information in a variety of DL models. This
    method is especially effective when the knowledge can be expressed in the form
    of relationships among input features, which can then be used either to augment
    the number of training examples (the relations provide a guide to generate novel
    valid examples starting from the original ones) or to create new, more informative
    features that should highlight implicit information already available in the data
    but hard to extract. An interesting thing to be noted is the fact that acting
    on the feature space does not necessarily require information about labels $y_{i}$,
    as the domain knowledge might involve only constraints among the $x_{i}$ features;
    this suggests that this type of approach is especially well suited for unsupervised
    learning tasks. These approaches go against the most prevailing DL research direction
    in recent years, namely the development of purely data-driven models that extract
    all the hidden information contained in the feature space without any help. Clearly,
    having powerful and general models capable of good performance regardless of the
    domain information is a very important purpose, nevertheless we reckon that there
    exist many contexts that could benefit from the exploitation of domain-derived
    prior information, and this research direction is worthy to be explored in future
    works.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '组成数据的特征通常是第[2](#S2 "2 Domain Knowledge Injection Approaches ‣ Improving Deep
    Learning Models via Constraint-Based Domain Knowledge: a Brief Survey")节中描述的技术的目标，因为它们提供了一种实用的机制，用于在各种深度学习模型中注入先验信息。当知识能够以输入特征之间关系的形式表达时，这种方法尤其有效，这些关系可以用来增加训练样本的数量（这些关系提供了从原始样本生成新有效样本的指导）或创建新的、更具信息性的特征，这些特征可以突出数据中已经存在但难以提取的隐含信息。值得注意的一点是，作用于特征空间并不一定需要关于标签$y_{i}$的信息，因为领域知识可能仅涉及$x_{i}$特征之间的约束；这表明这种方法特别适合无监督学习任务。这些方法与近年来最主要的深度学习研究方向相对立，即开发完全数据驱动的模型，这些模型在没有任何帮助的情况下提取特征空间中隐藏的所有信息。显然，拥有能够在不考虑领域信息的情况下实现良好性能的强大通用模型是一个非常重要的目标，然而我们认为，存在许多上下文可以从领域衍生的先验信息中受益，这一研究方向值得在未来的工作中进一步探索。'
- en: Accuracy VS Optimization
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精度与优化
- en: A partially unexplored area, in our opinion, is the trade-off between the accuracy
    of the DL models (e.g. high accuracy for classification tasks and low error for
    regression ones) and the satisfaction of the constraints imposed to the models.
    For instance, we already stated that in *end-to-end* learning the goal is to optimize
    the neural network given a specific task, therefore we might not observe an improvement
    in the accuracy of the model itself (e.g. Mean Average Error), but rather an improvement
    over a specific task. This is also the case for regularization schemes, where
    the resulting models have outputs more consistent with the prior knowledge, but
    with no increase in accuracy in mere terms of prediction. This is an aspect that
    is implicit in the desire to minimize a objective function composed by multiple
    terms that do not point towards the same direction. Generally speaking, the majority
    of these approaches opt for “soft” constraints on the output of the neural network,
    that is the constraints are not enforced strictly (as in the case of hard constraints)
    but rather the optimization tries to balance the diverse terms. This problem has
    been only partially studied on DNNs [[21](#bib.bib21), [7](#bib.bib7)], although
    in the optimization area it is well known that minimizing an objective function
    with multiple terms yields poor convergence properties, mainly because the optimizer
    is likely to focus on one term of the objective function while ignoring the remaining
    ones; furthermore, enforcing multiple constraints of different natures means that
    the terms risk not to be commensurate. Finally, an additional complicating factor
    in these methods is the selection of adequate weights for the various terms (model
    accuracy and constraints-based terms).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为一个部分尚未探索的领域是深度学习模型（例如分类任务的高准确性和回归任务的低误差）准确性与模型约束满足之间的权衡。例如，我们已经提到在*端到端*学习中，目标是优化神经网络以完成特定任务，因此我们可能不会观察到模型本身准确性的提升（例如均值绝对误差），而是对特定任务的改善。这也适用于正则化方案，其中结果模型的输出与先验知识更一致，但预测准确性并没有提升。这是一个隐含在最小化由多个不指向相同方向的项组成的目标函数的愿望中的方面。一般来说，大多数这些方法选择对神经网络输出施加“软”约束，即约束不会被严格执行（如硬约束），而是优化尝试平衡不同的项。这个问题在DNN上只得到部分研究[[21](#bib.bib21),
    [7](#bib.bib7)]，尽管在优化领域中已知，最小化一个包含多个项的目标函数会导致较差的收敛特性，主要因为优化器可能会专注于目标函数的一个项而忽略其他项；此外，施加多种不同性质的约束意味着这些项可能不具有可比性。最后，这些方法中的一个额外复杂因素是为各种项（模型准确性和基于约束的项）选择适当的权重。
- en: Small data sets
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 小数据集
- en: A non-negligible problem for DL approaches is the requirement of large amounts
    of data, preferably labelled. In many scenarios, this does not happen, thus the
    training of deep models is hindered. Injecting domain knowledge can boost the
    performance of DL models when training data is scarce. For this purpose, the most
    obvious candidates are techniques which augment the available data, but regularization
    schemes and feature engineering approaches can provide benefits as well, by, respectively,
    “guiding” the training process of the DNN and simplifying the learning task thanks
    to the additional features added to the raw data. In general, techniques to integrate
    prior information can be extremely useful in the context of *active learning*
    and other settings where the dearth of data cannot be bypassed. Nowadays, the
    majority of approaches for active learning are not guided with domain knowledge
    whilst there are potentially big benefits in exploiting such knowledge, for instance
    to drive the selection of new instances to be evaluated, an exploration that is
    currently guided by domain-agnostic strategies based on measures such as information
    gain. Some recent works did preliminary work towards this direction. For instance,
    [[6](#bib.bib6)] propose a combination of active and end-to-end learning, by embedding
    a DNN within an optimization model for floating point variables precision tuning.
    As the training set is relatively small, the DNN starts with inaccurate predictions;
    the author use active learning to iteratively improve the DNN by retraining it
    on new examples, namely the solutions of the optimization model, directly depending
    on the domain knowledge.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 对于深度学习方法而言，一个不可忽视的问题是需要大量的数据，最好是带标签的数据。在许多情况下，这种数据并不存在，因此深度模型的训练受到阻碍。注入领域知识可以在训练数据稀缺时提升深度学习模型的性能。为此，最明显的选择是那些扩充现有数据的技术，但正则化方案和特征工程方法也能提供帮助，前者通过“引导”DNN的训练过程，后者通过简化学习任务和将附加特征添加到原始数据中而带来好处。一般来说，在*主动学习*以及其他无法绕过数据不足的场景中，整合先验信息的技术可以非常有用。如今，大多数主动学习的方法并未借助领域知识，而利用这些知识可能带来很大好处，例如驱动新实例的选择，这一探索目前是由基于信息增益等度量的领域无关策略来指导的。一些近期的研究朝着这个方向做了初步工作。例如，[[6](#bib.bib6)]
    提出了主动学习和端到端学习的组合，通过将DNN嵌入浮点变量精度调优的优化模型中。由于训练集相对较小，DNN初期预测不准确；作者利用主动学习，通过在新示例上重新训练DNN来迭代改进它，这些示例即为优化模型的解，直接依赖于领域知识。
- en: Evaluation metrics
  id: totrans-47
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 评估指标
- en: A partially unexplored issue in the domain information injection area regards
    the best metric to evaluate the performance of the injection mechanism. Broadly
    speaking, most common DL techniques are measured on the basis of a single metric
    such as accuracy (classification tasks) or Mean Average Error or Mean Squared
    Error (for regression tasks). However, these metrics are not the fairest ones
    when one has to judge the benefits of injecting domain knowledge. For instance,
    a common aspect of regularization schemes and methodologies that embed constraints
    in the NNs is that their goal is not to simply reduce the prediction error or
    increase the model’s accuracy, but rather to obtain NNs whose output respects
    some desirable proprieties (e.g. monotonicity) or where certain relationships
    between input and output need to hold. In practice, this means that there are
    no established and straightforward methods to measure the improvements of knowledge
    injection methods, as the for each domain different authors chose different evaluation
    metrics. In many cases the authors compare their injection methods to standard
    DL models using specific test sets, carefully crafted for the particular task.
    For example, models enforcing an output with no violations of a particular constraint
    can be trained on data sets that contain instances violating the constraints but
    tested exclusively on data sets with no violations. This is acceptable in order
    to perform a fair comparison but with an increased risk of creating artificial
    experimental settings. This lack of homogeneity is not a trivial issue and it
    complicates the comparison of different techniques. A set of common benchmarks
    and key performance indicators would greatly benefit this research area.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在领域信息注入领域，一个尚未完全探索的问题是评估注入机制性能的最佳指标。一般来说，大多数常见的深度学习技术都是基于单一指标来进行测量，如准确率（分类任务）或平均绝对误差或均方误差（回归任务）。然而，当需要评估注入领域知识的好处时，这些指标并不是最公平的。例如，常规化方案和将约束嵌入神经网络的方法的一个共同点是，它们的目标不仅仅是减少预测误差或提高模型的准确性，而是获得输出符合某些期望特性（例如单调性）或输入与输出之间需要保持某些关系的神经网络。在实践中，这意味着没有建立且直接的方法来衡量知识注入方法的改进，因为每个领域的不同作者选择了不同的评估指标。在许多情况下，作者将他们的注入方法与标准深度学习模型进行比较，使用为特定任务精心设计的测试集。例如，强制输出不违反特定约束的模型可以在包含违反约束实例的数据集上进行训练，但仅在没有违反的数据集上进行测试。这在进行公平比较时是可以接受的，但增加了创建人工实验设置的风险。这种缺乏同质性的问题并不简单，它使得不同技术之间的比较变得复杂。制定一套共同的基准和关键性能指标将极大地有利于这一研究领域。
- en: Towards a unified framework
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 朝着统一框架的方向
- en: As an overall remark, it can be noted that the various approaches discussed
    previously operate in relative isolation. This is understandable and can be partially
    explained by the fact that domain injection techniques are, by definition, domain
    specific. This issue is exacerbated by the large number of possible injection
    mechanisms and targets. This leads to a lack of a common perspective and makes
    the comparison of different injection approaches harder; however, some recent
    attempts have been made, see for example [[5](#bib.bib5)], where multiple knowledge
    injection techniques are employed to boost a DNN dealing with a complex learning
    task. We believe that a unified methodology, or framework, for injecting domain-derived
    constraints would be a great step for advancing the research progress in this
    area. Such unified framework would require a common language to express the domain
    knowledge; it should be a language with expressive power (many different types
    of relationships and concepts should be allowed to be represented) and flexible,
    that is capable to describe information stemming from very diverse domains. We
    reckon that a language based on constraints and logic predicates could be a very
    apt choice for this task, especially thanks to paradigms such as constraint programming
    and mixed integer and linear programming, which have been proven to be capable
    of handling a wide range of domain-specific challenges. The next step would be
    deciding the best information injection mechanism for the desired task; for a
    detailed answer follow-up studies need to be conducted, but some guidelines can
    be already provided at this stage. First, when the available data is scarce, data
    augmentation and feature space manipulation techniques are to be preferred, as
    they allow to increase the training set size by exploiting known relationships
    between input set features $x_{i}$, as well as relations between input features
    and output features $y_{i}$. Secondly, if the information contained in the input
    data is hidden and not easy to be extracted, ad-hoc DNNs architectures can be
    extremely helpful – e.g. acting on the hypothesis space – as the domain expert
    knowledge can be directly injected in the neural network structure from its design
    to the training algorithm. Finally, regularization methods and explicit constraints
    learning in the neural network are very effective strategies for end-to-end learning
    and, in general, for more complex learning tasks where the model accuracy is not
    the exclusive performance metrics. In this case, careful attention should be employed
    to choose the right trade-off between the optimization problem and the pure minimization
    of the DNN’s loss function. This is a non trivial issue, as it complicates the
    actual development of regularization and constraints learning techniques, together
    with the fact that they are extremely task-specific (e.g. a set of weights for
    one context would be not well suited to other ones); the practitioner implementing
    these approaches will be faced with steeper challenges compared, for example,
    to data augmentation approaches, which also tend to be more transferable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，可以注意到之前讨论的各种方法在相对孤立的情况下运作。这是可以理解的，部分原因在于领域注入技术本质上是领域特定的。这个问题由于可能的注入机制和目标的数量庞大而加剧。这导致了缺乏共同的视角，使得比较不同的注入方法变得更加困难；然而，最近已进行了一些尝试，例如[[5](#bib.bib5)]，其中采用了多种知识注入技术来提升处理复杂学习任务的DNN。我们认为，一个统一的方法论或框架来注入领域衍生的约束将是推进该领域研究进展的重要一步。这样的统一框架需要一个共同的语言来表达领域知识；它应该是一个具有表现力（允许表示多种不同类型的关系和概念）和灵活的语言，能够描述来自非常多样化领域的信息。我们认为基于约束和逻辑谓词的语言可能是非常合适的选择，特别是得益于诸如约束编程和混合整数与线性编程等范式，这些方法已被证明能够处理广泛的领域特定挑战。下一步将是决定最佳的信息注入机制；要得到详细的答案需要进行后续研究，但在这一阶段可以提供一些指导。首先，当可用数据稀少时，应优先考虑数据增强和特征空间操控技术，因为它们通过利用输入集特征$x_{i}$之间的已知关系以及输入特征和输出特征$y_{i}$之间的关系来增加训练集的规模。其次，如果输入数据中的信息隐藏且不易提取，特定的DNN架构可以极其有用——例如，作用于假设空间——因为领域专家知识可以直接注入到神经网络结构中，从设计到训练算法。最后，正则化方法和神经网络中的显式约束学习是端到端学习以及在一般情况下，更复杂学习任务中非常有效的策略，其中模型的准确性不是唯一的性能指标。在这种情况下，应谨慎选择优化问题和DNN损失函数纯最小化之间的正确权衡。这是一个非平凡的问题，因为它使得正则化和约束学习技术的实际开发变得复杂，加上这些技术非常特定于任务（例如，一种上下文的权重集不适合其他上下文）；实施这些方法的从业者将面临比数据增强方法更为陡峭的挑战，而数据增强方法通常也更具可迁移性。
- en: 5 Conclusion
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 结论
- en: The integration of domain knowledge expressible in the form of constraints into
    deep neural networks is a wide research area that have seen increasing research
    interest in recent years. This topic has been tackled from different angles by
    a variety of approaches, typically in relative isolation, a fact that probably
    hindered research breakthrough. In this paper, we have provided a first cross-disciplinary
    attempt at classifying existing approaches, identifying the main classes of techniques
    for domain knowledge injection, and highlighting connections with related fields
    from the DL area. Moreover, we identified a series of common trends and issues
    that have been addressed and open challenges that still need to be tackled, with
    the hope of providing useful insights and a guidance for future research efforts.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将以约束形式表达的领域知识整合到深度神经网络中是一个广泛的研究领域，近年来引起了越来越多的研究兴趣。这个话题从不同角度被各种方法探讨，通常是相对孤立的，这可能阻碍了研究突破。本文提供了一个跨学科的首次尝试，对现有方法进行分类，识别领域知识注入的主要技术类别，并突出与深度学习领域相关的联系。此外，我们识别了一系列已解决的共同趋势和问题以及仍需解决的挑战，希望为未来的研究工作提供有用的见解和指导。
- en: Acknowledgments
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work has been partially supported by European H2020 FET project OPRECOMP
    (g.a. 732631) and ICT project AI4EU (g.a. 825619).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本工作部分得到了欧洲 H2020 FET 项目 OPRECOMP（g.a. 732631）和 ICT 项目 AI4EU（g.a. 825619）的支持。
- en: References
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Atzmueller and E. Sternberg. Mixed-initiative feature engineering using
    knowledge graphs. In Proceedings of the Knowledge Capture Conference, pages 1–4,
    2017.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Atzmueller 和 E. Sternberg。使用知识图谱的混合主动特征工程。发表于知识捕捉会议论文集，第 1–4 页, 2017。'
- en: '[2] S. Bai, J. Z. Kolter, and V. Koltun. An empirical evaluation of generic
    convolutional and recurrent networks for sequence modeling. arXiv preprint arXiv:1803.01271,
    2018.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Bai, J. Z. Kolter, 和 V. Koltun。通用卷积和递归网络用于序列建模的实证评估。arXiv 预印本 arXiv:1803.01271,
    2018。'
- en: '[3] D. Berrar, P. Lopes, and W. Dubitzky. Incorporating domain knowledge in
    machine learning for soccer outcome prediction. Machine Learning, 108(1):97–126,
    2019.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] D. Berrar, P. Lopes, 和 W. Dubitzky。将领域知识融入足球结果预测的机器学习中。机器学习, 108(1):97–126,
    2019。'
- en: '[4] E. J. Bjerrum. Smiles enumeration as data augmentation for neural network
    modeling of molecules. arXiv preprint arXiv:1703.07076, 2017.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] E. J. Bjerrum. 将笑脸枚举作为神经网络分子建模的数据增强。arXiv 预印本 arXiv:1703.07076, 2017。'
- en: '[5] A. Borghesi, F. Baldo, M. Lombardi, and M. Milano. Injective domain knowledge
    in neural networks for transprecision computing. arXiv preprint arXiv:2002.10214,
    2020.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. Borghesi, F. Baldo, M. Lombardi, 和 M. Milano。在神经网络中注入领域知识以进行精度计算。arXiv
    预印本 arXiv:2002.10214, 2020。'
- en: '[6] A. Borghesi, G. Tagliavini, and et al. Combining learning and optimization
    for transprecision computing. In Proceedings of the 17th ACM International Conference
    on Computing Frontiers, 2020.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] A. Borghesi, G. Tagliavini, 等。结合学习与优化进行精度计算。发表于第 17 届 ACM 国际计算前沿会议论文集,
    2020。'
- en: '[7] F. De Tassis, M. Lombardi, and M. Milano. Teaching the old dog new tricks:
    Supervised learning with constraints. arXiv preprint arXiv:2002.10766, 2020.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] F. De Tassis, M. Lombardi, 和 M. Milano。教老狗学新把戏：带约束的监督学习。arXiv 预印本 arXiv:2002.10766,
    2020。'
- en: '[8] M. Diligenti, M. Gori, and C. Sacca. Semantic-based regularization for
    learning and inference. Artificial Intelligence, 244:143–165, 2017.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] M. Diligenti, M. Gori, 和 C. Sacca。基于语义的正则化用于学习和推断。人工智能, 244:143–165, 2017。'
- en: '[9] P. Donti, B. Amos, and J. Z. Kolter. Task-based end-to-end model learning
    in stochastic optimization. In Advances in Neural Information Processing Systems,
    pages 5484–5494, 2017.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] P. Donti, B. Amos, 和 J. Z. Kolter。基于任务的端到端模型学习在随机优化中的应用。发表于神经信息处理系统进展,
    第 5484–5494 页, 2017。'
- en: '[10] F. Fioretto, T. W. Mak, and et al. A lagrangian dual framework for deep
    neural networks with constraints. arXiv preprint arXiv:2001.09394, 2020.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] F. Fioretto, T. W. Mak, 等。带约束的深度神经网络的拉格朗日对偶框架。arXiv 预印本 arXiv:2001.09394,
    2020。'
- en: '[11] Ç. Gülçehre and Y. Bengio. Knowledge matters: Importance of prior information
    for optimization. The Journal of Machine Learning Research, 17(1):226–257, 2016.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] Ç. Gülçehre 和 Y. Bengio。知识很重要：优化中先验信息的重要性。机器学习研究期刊, 17(1):226–257, 2016。'
- en: '[12] A. Hernández-García and P. König. Data augmentation instead of explicit
    regularization. arXiv preprint arXiv:1806.03852, 2018.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] A. Hernández-García 和 P. König。数据增强代替显式正则化。arXiv 预印本 arXiv:1806.03852,
    2018。'
- en: '[13] A. Jain, A. R. Zamir, S. Savarese, and A. Saxena. Structural-rnn: Deep
    learning on spatio-temporal graphs. In Proceedings of the ieee conference on computer
    vision and pattern recognition, pages 5308–5317, 2016.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] A. Jain、A. R. Zamir、S. Savarese 和 A. Saxena. Structural-rnn：在时空图上的深度学习。在
    IEEE 计算机视觉与模式识别会议论文集，页面 5308–5317, 2016.'
- en: '[14] S. Khalid, T. Khalil, and S. Nasreen. A survey of feature selection and
    feature extraction techniques in machine learning. In 2014 Science and Information
    Conference, pages 372–378. IEEE, 2014.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] S. Khalid、T. Khalil 和 S. Nasreen. 机器学习中特征选择和特征提取技术的综述。在 2014 科学与信息会议，页面
    372–378。IEEE, 2014.'
- en: '[15] U. Khurana, H. Samulowitz, and D. Turaga. Feature engineering for predictive
    modeling using reinforcement learning. In Thirty-Second AAAI Conference on Artificial
    Intelligence, 2018.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] U. Khurana、H. Samulowitz 和 D. Turaga. 使用强化学习进行预测建模的特征工程。在第三十二届 AAAI 人工智能会议，2018.'
- en: '[16] T. N. Kipf and M. Welling. Semi-Supervised Classification with Graph Convolutional
    Networks. In Proceedings of the 5th International Conference on Learning Representations,
    ICLR ’17, 2017.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] T. N. Kipf 和 M. Welling. 基于图卷积网络的半监督分类。在第五届国际学习表示会议，ICLR ’17, 2017.'
- en: '[17] T. Li and V. Srikumar. Augmenting neural networks with first-order logic.
    arXiv preprint arXiv:1906.06298, 2019.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] T. Li 和 V. Srikumar. 用一阶逻辑增强神经网络。arXiv 预印本 arXiv:1906.06298, 2019.'
- en: '[18] J. Luketina, N. Nardelli, and et al. A survey of reinforcement learning
    informed by natural language. arXiv preprint arXiv:1906.03926, 2019.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Luketina、N. Nardelli 等. 一种由自然语言启发的强化学习综述。arXiv 预印本 arXiv:1906.03926,
    2019.'
- en: '[19] D. D.-C. T. G. C. Z. M. V. Marc Fischer, Mislav Balunovic. Dl2: Training
    and querying neural networks with logic. In International Conference on Machine
    Learning, 2019.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] D. D.-C. T. G. C. Z. M. V. Marc Fischer、Mislav Balunovic. Dl2：用逻辑训练和查询神经网络。在国际机器学习会议，2019.'
- en: '[20] K. Marino, R. Salakhutdinov, and A. Gupta. The more you know: Using knowledge
    graphs for image classification. arXiv preprint arXiv:1612.04844, 2016.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] K. Marino、R. Salakhutdinov 和 A. Gupta. 知识越多：利用知识图谱进行图像分类。arXiv 预印本 arXiv:1612.04844,
    2016.'
- en: '[21] P. Márquez-Neila, M. Salzmann, and P. Fua. Imposing hard constraints on
    deep networks: Promises and limitations. arXiv preprint arXiv:1706.02025, 2017.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] P. Márquez-Neila、M. Salzmann 和 P. Fua. 对深度网络施加硬性约束：承诺与局限性。arXiv 预印本 arXiv:1706.02025,
    2017.'
- en: '[22] G. Marra, F. Giannini, M. Diligenti, and M. Gori. Lyrics: a general interface
    layer to integrate logic inference and deep learning. In ECML-PKDD 2019, 2019.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] G. Marra、F. Giannini、M. Diligenti 和 M. Gori. Lyrics：一个将逻辑推理和深度学习集成的通用接口层。在
    ECML-PKDD 2019，2019.'
- en: '[23] R. Miao, Z. Yang, and V. Gavrishchaka. Leveraging domain-expert knowledge,
    boosting and deep learning for identification of rare and complex states. In Journal
    of Physics: Conference Series, volume 1207, page 012016\. IOP Publishing, 2019.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] R. Miao、Z. Yang 和 V. Gavrishchaka. 利用领域专家知识、提升和深度学习来识别稀有和复杂状态。在《物理学期刊：会议系列》，卷
    1207，页码 012016。IOP Publishing, 2019.'
- en: '[24] A. Mollaysa, A. Kalousis, E. Bruno, and M. Diephuis. Learning to augment
    with feature side-information. In Asian Conference on Machine Learning, pages
    173–187, 2019.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] A. Mollaysa、A. Kalousis、E. Bruno 和 M. Diephuis. 学习利用特征侧信息进行增强。在亚洲机器学习会议，页面
    173–187, 2019.'
- en: '[25] N. Muralidhar, M. Islam, and et al. Incorporating prior domain knowledge
    into deep neural networks. In 2018 IEEE International Conference on Big Data (Big
    Data). IEEE, 2018.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] N. Muralidhar、M. Islam 等. 将先验领域知识融入深度神经网络。在 2018 IEEE 国际大数据会议（Big Data）。IEEE,
    2018.'
- en: '[26] F. Nargesian, H. Samulowitz, U. Khurana, E. B. Khalil, and D. S. Turaga.
    Learning feature engineering for classification. In IJCAI, pages 2529–2535, 2017.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] F. Nargesian、H. Samulowitz、U. Khurana、E. B. Khalil 和 D. S. Turaga. 学习特征工程用于分类。在
    IJCAI，页面 2529–2535, 2017.'
- en: '[27] F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini.
    The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61–80,
    2008.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] F. Scarselli、M. Gori、A. C. Tsoi、M. Hagenbuchner 和 G. Monfardini. 图神经网络模型。IEEE
    神经网络学报，20(1)：61–80, 2008.'
- en: '[28] A. Silva and M. Gombolay. Prolonets: Neural-encoding human experts’ domain
    knowledge to warm start reinforcement learning. arXiv preprint arXiv:1902.06007,
    2019.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] A. Silva 和 M. Gombolay. Prolonets：神经编码人类专家的领域知识以启动强化学习。arXiv 预印本 arXiv:1902.06007,
    2019.'
- en: '[29] M. Silvestri, M. Lombardi, and M. Milano. Injecting domain knowledge in
    neural networks: a controlled experiment on a constrained problem. arXiv preprint
    arXiv:2002.10742, 2020.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] M. Silvestri、M. Lombardi 和 M. Milano. 将领域知识注入神经网络：对受限问题的受控实验。arXiv 预印本
    arXiv:2002.10742, 2020.'
- en: '[30] K. Terashima and J. Murata. A study on use of prior information for acceleration
    of reinforcement learning. In SICE Annual Conference 2011, pages 537–543\. IEEE,
    2011.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] K. Terashima 和 J. Murata. 关于利用先验信息加速强化学习的研究。发表于《SICE年会2011》，第537–543页。IEEE，2011年。'
- en: '[31] K. Vo, D. Pham, M. Nguyen, T. Mai, and T. Quan. Combination of domain
    knowledge and deep learning for sentiment analysis. In International Workshop
    on Multi-disciplinary Trends in Artificial Intelligence, pages 162–173\. Springer,
    2017.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] K. Vo, D. Pham, M. Nguyen, T. Mai, 和 T. Quan. 领域知识与深度学习相结合用于情感分析。发表于《人工智能跨学科趋势国际研讨会》，第162–173页。Springer，2017年。'
- en: '[32] L. von Rueden, S. Mayer, and et al. Informed machine learning – a taxonomy
    and survey of integrating knowledge into learning systems. arXiv preprint arXiv:1903.12394v2,
    2020.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] L. von Rueden, S. Mayer, 等. 有信息的机器学习 – 整合知识到学习系统的分类与综述。arXiv预印本 arXiv:1903.12394v2，2020年。'
- en: '[33] J. Wang and L. Perez. The effectiveness of data augmentation in image
    classification using deep learning. Convolutional Neural Networks Vis. Recognit,
    page 11, 2017.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] J. Wang 和 L. Perez. 使用深度学习进行图像分类的数据增强效果。《卷积神经网络视觉识别》，第11页，2017年。'
- en: '[34] B. Wilder, B. Dilkina, and M. Tambe. Melding the data-decisions pipeline:
    Decision-focused learning for combinatorial optimization. In Proceedings of the
    AAAI Conference on Artificial Intelligence, volume 33, pages 1658–1665, 2019.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] B. Wilder, B. Dilkina, 和 M. Tambe. 数据决策管道的融合：针对组合优化的决策导向学习。发表于《AAAI人工智能会议论文集》，第33卷，第1658–1665页，2019年。'
- en: '[35] J. Xu, Z. Zhang, and et al. A semantic loss function for deep learning
    with symbolic knowledge. In Proceedings of the 35th ICML, 2018.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] J. Xu, Z. Zhang, 等. 带符号知识的深度学习语义损失函数。发表于第35届ICML会议论文集，2018年。'
- en: '[36] X. Yuan, C. Ou, Y. Wang, C. Yang, and W. Gui. A novel semi-supervised
    pre-training strategy for deep networks and its application for quality variable
    prediction in industrial processes. Chemical Engineering Science, page 115509,
    2020.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] X. Yuan, C. Ou, Y. Wang, C. Yang, 和 W. Gui. 一种新型半监督预训练策略用于深度网络及其在工业过程质量变量预测中的应用。《化学工程科学》，第115509页，2020年。'
- en: '[37] S. Zhang, H. Tong, J. Xu, and R. Maciejewski. Graph convolutional networks:
    a comprehensive review. Computational Social Networks, 6(1):11, 2019.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] S. Zhang, H. Tong, J. Xu, 和 R. Maciejewski. 图卷积网络：全面综述。《计算社会网络》，6(1):11，2019年。'
