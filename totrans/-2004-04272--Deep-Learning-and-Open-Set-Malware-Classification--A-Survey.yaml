- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:01:37'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:01:37
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2004.04272] Deep Learning and Open Set Malware Classification: A Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2004.04272] 深度学习与开放集恶意软件分类：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2004.04272](https://ar5iv.labs.arxiv.org/html/2004.04272)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2004.04272](https://ar5iv.labs.arxiv.org/html/2004.04272)
- en: 'Deep Learning and Open Set Malware Classification: A Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习与开放集恶意软件分类：综述
- en: Jingyun Jia School of Computing, Florida Institute of Technology150 West University
    BlvdMelbourne, FL 32901 [jiaj2018@my.fit.edu](mailto:jiaj2018@my.fit.edu)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Jingyun Jia 计算机学院，佛罗里达理工学院150 West University BlvdMelbourne, FL 32901 [jiaj2018@my.fit.edu](mailto:jiaj2018@my.fit.edu)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: As the Internet is growing rapidly these years, the variant of malicious software,
    which often referred to as malware, has become one of the major and serious threats
    to Internet users. The dramatic increase of malware has led to a research area
    of not only using cutting edge machine learning techniques classify malware into
    their known families, moreover, recognize the unknown ones, which can be related
    to Open Set Recognition (OSR) problem in machine learning. Recent machine learning
    works have shed light on Open Set Recognition (OSR) from different scenarios.
    Under the situation of missing unknown training samples, the OSR system should
    not only correctly classify the known classes, but also recognize the unknown
    class. This survey provides an overview of different deep learning techniques,
    a discussion of OSR and graph representation solutions and an introduction of
    malware classification systems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着互联网近年来的快速发展，恶意软件的变种，通常称为恶意软件，已成为互联网用户面临的主要和严重威胁之一。恶意软件的剧增促使了一个研究领域，即不仅使用前沿的机器学习技术将恶意软件分类到其已知的家族中，还识别未知的恶意软件，这与机器学习中的开放集识别（OSR）问题有关。近期的机器学习研究从不同场景中揭示了开放集识别（OSR）。在缺失未知训练样本的情况下，OSR系统不仅应能正确分类已知类别，还需识别未知类别。本综述提供了不同深度学习技术的概述，讨论了OSR和图表示解决方案，并介绍了恶意软件分类系统。
- en: 'Deep Learning, Open Set Recognition, Graph Representation, Malware Classification^†^†copyright:
    none'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习、开放集识别、图表示、恶意软件分类^†^†版权所有：无
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 介绍
- en: 'Malware, software that ”deliberately fulfills the harmful intent of an attacker”
    (Bayer et al., [2006](#bib.bib4)) nowadays come in a wide range of variations
    and multiple families. They have become one of the most terrible and major security
    threats of the Internet today. Instead of using traditional defenses, which typically
    use signature-based methods. There is an active research area that using machine
    learning-based techniques to solve the problem in both sides:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意软件，即“故意实现攻击者有害意图的”软件（Bayer 等，[2006](#bib.bib4)），如今有广泛的变种和多个家族。它们已成为当今互联网中最可怕和主要的安全威胁之一。除了使用通常基于特征的方法的传统防御措施外，还有一个活跃的研究领域，使用基于机器学习的技术来解决两方面的问题：
- en: (1)
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Classify known malware into their families, which turns out to be normal multi-classification
  id: totrans-14
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将已知恶意软件分类到其家族中，这实际上是普通的多类分类
- en: (2)
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Recognize unknown malware. i.e. malware that are not present in the training
    set but appear in the test set.
  id: totrans-16
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 识别未知恶意软件，即在训练集中不存在但出现在测试集中的恶意软件。
- en: One solution for malware classification is to convert it to function call graphs
    (FCG), then classify them into families according to the representation of FCG
    as in (Hassen and Chan, [2017](#bib.bib20)). Graphs are an important data structure
    in machine learning tasks, and the challenge is to find a way to represent graphs.
    Traditionally, the feature extraction relied on user-defined heuristics. And recent
    researches have been focus on using deep learning to automatically learn to encode
    graph structure into low-dimensional embedding.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一种恶意软件分类的解决方案是将其转换为函数调用图（FCG），然后根据FCG的表示将其分类到不同的家族中，如（Hassen 和 Chan，[2017](#bib.bib20)）。图是机器学习任务中的一个重要数据结构，挑战在于找到一种表示图的方法。传统上，特征提取依赖于用户定义的启发式方法。近年来的研究则集中于利用深度学习自动学习将图结构编码为低维嵌入。
- en: Another problem is it is less likely to label all the classes in training samples
    for the fast-developing diverse of malware families, the second item has become
    even more important daily. The related open set recognition (OSR) should also
    be able to handle those unlabeled ones. Traditional classification techniques
    focus on problems with labeled classes. While OSR pays attention to unknown classes.
    It requires the classifier accurately classify known classes, meanwhile identify
    unknown classes. Meanwhile, deep learning based OSR solutions have become a flourish
    research area in recent years.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是，由于恶意软件家族的快速多样化，标记训练样本中的所有类别的可能性较小，因此第二项问题变得越来越重要。相关的开放集识别（OSR）也应该能够处理那些未标记的样本。传统的分类技术专注于标记类的问题，而OSR则关注未知类别。它要求分类器能够准确地分类已知类别，同时识别未知类别。与此同时，基于深度学习的OSR解决方案近年来已经成为一个繁荣的研究领域。
- en: In this survey, we will first review basic deep learning techniques. Then a
    brief categorization of current OSR techniques will be given in section 3\. In
    section 4, we will cover methods learning graph representations, followed by an
    introduction to state-of-art malware classification techniques in section 5\.
    And finally, section 6 will conclude.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这项调查中，我们将首先回顾基本的深度学习技术。然后在第3节中简要分类当前的OSR技术。在第4节中，我们将介绍学习图表示的方法，接着在第5节中介绍最新的恶意软件分类技术。最后，第6节将作总结。
- en: 2\. Deep learning basics
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 深度学习基础
- en: As conventional machine-learning techniques were limited in their ability to
    process natural data in their raw form, hence required expertise in feature engineering
    (LeCun et al., [2015](#bib.bib29)). Deep Learning was introduced to discover intricate
    structures in high-dimension data, which requires litter engineering by hand.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于传统的机器学习技术在处理原始自然数据时存在限制，因此需要在特征工程方面具备专业知识（LeCun et al., [2015](#bib.bib29)）。深度学习被引入以发现高维数据中的复杂结构，这需要较少的手工工程。
- en: In the following subsections, we will give an overview of different network
    architectures in different areas in recent years.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将概述近年来不同领域的不同网络架构。
- en: '![Refer to caption](img/9b2ad45c800f1cbd5e3b28e139110b45.png)\Description'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/9b2ad45c800f1cbd5e3b28e139110b45.png)\描述'
- en: CNNs
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs
- en: Figure 1\. A CNN sequence to classify handwritten digits ([Saha](#bib.bib45),
    ([2018](#bib.bib45)))
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. 一个CNN序列用于分类手写数字（[Saha](#bib.bib45)，([2018](#bib.bib45)))
- en: 2.1\. Convolutional neural networks
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 卷积神经网络
- en: 'As a popular architecture of Deep Neural Networks, Convolutional Neural Networks
    (CNNs) has achieved good performance in the Computer Vision area. As Figure [1](#S2.F1
    "Figure 1 ‣ 2\. Deep learning basics ‣ Deep Learning and Open Set Malware Classification:
    A Survey"), a typical CNN usually consists of “Input Layer”, “Convolutional Layer”,
    “Pooling Layer” and “Output Layer”. The convolutional and pooling layer can be
    repeated several times. In most cases, the Relu function is used as activation
    in the convolutional layer and Max-Pooling is used in the pooling layer. During
    the learning process, filters will be learned and feature maps will then be generated,
    which is the output of representation learning. The output is usually followed
    by a fully connected network for classification problems. The architectures could
    be different in various aspects: layers and connections ((Cao et al., [2015](#bib.bib6))
    (He et al., [2016](#bib.bib23))), loss functions ((Wen et al., [2016](#bib.bib54))
    (He et al., [2018](#bib.bib24)) (Deng et al., [2019](#bib.bib11))), etc.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种流行的深度神经网络架构，卷积神经网络（CNNs）在计算机视觉领域取得了良好的表现。如图[1](#S2.F1 "图 1 ‣ 2\. 深度学习基础
    ‣ 深度学习与开放集恶意软件分类：综述")所示，典型的CNN通常由“输入层”、“卷积层”、“池化层”和“输出层”组成。卷积层和池化层可以重复多次。在大多数情况下，卷积层中使用Relu函数作为激活函数，池化层中使用Max-Pooling。在学习过程中，过滤器将被学习并生成特征图，这是表征学习的输出。输出通常会跟随一个全连接网络以解决分类问题。架构在各个方面可能有所不同：层次和连接（(Cao
    et al., [2015](#bib.bib6)) (He et al., [2016](#bib.bib23)))，损失函数（(Wen et al.,
    [2016](#bib.bib54)) (He et al., [2018](#bib.bib24)) (Deng et al., [2019](#bib.bib11)))等。
- en: 2.1.1\. Feedback Network
  id: totrans-28
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.1\. 反馈网络
- en: Cao et al. ([2015](#bib.bib6)) proposed Feedback Network to develop a computational
    feedback mechanism which can help better visualized and understand how deep neural
    network works, and capture visual attention on expected objects, even in images
    with cluttered background and multiple objects.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Cao 等人 ([2015](#bib.bib6)) 提出了反馈网络，开发了一种计算反馈机制，帮助更好地可视化和理解深度神经网络的工作方式，并在背景混乱和物体多样的图像中捕捉对预期对象的视觉注意力。
- en: 'Feedback Network introduced a feedback layer. The feedback layer contains another
    set of binary neuron activation variables $Z\in\{0,1\}$. The feedback layer is
    stacked upon each ReLU layer, and they compose a hybrid control unit to active
    neuron response in both bottom-up and top-down manners: Bottom-Up Inherent the
    selectivity from ReLU layers, and the dominant features will be passed to upper
    layers; Top-Down is controlled by Feedback Layers, which propagate the high-level
    semantics and global information back to image representations. Only those gates
    related to particular target neurons are activated.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 反馈网络引入了一个反馈层。反馈层包含另一组二进制神经元激活变量 $Z\in\{0,1\}$。反馈层堆叠在每个 ReLU 层上，它们组成了一个混合控制单元，以自下而上和自上而下的方式激活神经元响应：自下而上的方式继承了
    ReLU 层的选择性，主要特征将传递到上层；自上而下的方式由反馈层控制，将高层次的语义和全局信息传回图像表示。只有与特定目标神经元相关的门会被激活。
- en: 2.1.2\. Center loss
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.2\. 中心损失
- en: 'Wen et al. proposed center loss as a new supervision signal (objective function)
    for face recognition tasks in (Wen et al., [2016](#bib.bib54)). To separate the
    features of different classes to achieve better performance in classification
    tasks, center loss tries to minimize the variation of intra-class. Let $\mathbf{c}_{y_{i}}$
    denotes the center of the embeddings of $y_{i}$th class, the loss function looks
    like:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Wen 等人提出了中心损失作为面部识别任务的新监督信号（目标函数）（Wen et al., [2016](#bib.bib54)）。为了将不同类别的特征分开以实现更好的分类性能，中心损失试图最小化类内的变化。令
    $\mathbf{c}_{y_{i}}$ 表示 $y_{i}$ 类的嵌入中心，损失函数如下：
- en: '| (1) |  | $\mathcal{L}_{C}=\frac{1}{2}\sum_{i=1}^{m}\&#124;\mathbf{x}_{i}-\mathbf{c}_{y_{i}}\&#124;_{2}^{2},$
    |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\mathcal{L}_{C}=\frac{1}{2}\sum_{i=1}^{m}\&#124;\mathbf{x}_{i}-\mathbf{c}_{y_{i}}\&#124;_{2}^{2},$
    |  |'
- en: To make the computation more efficient, center loss uses a mini-batch updating
    method and the centers are updated by the features mean of the corresponding classes
    after each iteration. The paper showed that under the joint supervision of softmax
    loss and center loss, CNN can obtain inter-class dispensation and intra-class
    compactness as much as possible.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高计算效率，中心损失使用了一个小批量更新方法，并在每次迭代后通过相应类别特征的均值来更新中心。论文显示，在 softmax 损失和中心损失的联合监督下，CNN
    可以尽可能地获得类间分散和类内紧凑。
- en: 2.1.3\. Triplet-center loss
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.3\. 三元组-中心损失
- en: Inspired by triplet loss and center loss, He et al. introduced triplet-center
    loss to further enhance the discriminative power of the features, as to 2D object
    recognition algorithms in (He et al., [2018](#bib.bib24)). Triplet-loss intends
    to find an embedding space where the distances between different classes are greater
    than those form the same classes. Center loss tries to find embedding spaces where
    the deep learned features from the same class more compact and closer to the corresponding
    center. Similarly, instead of comparing the distances of each two instances in
    triplet loss, triplet loss computes the distances of instance and class center.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 受到三元组损失和中心损失的启发，He 等人引入了三元组-中心损失，以进一步增强特征的判别能力，应用于 2D 物体识别算法（He et al., [2018](#bib.bib24)）。三元组损失旨在找到一个嵌入空间，使不同类别之间的距离大于同一类别之间的距离。中心损失则试图找到一个嵌入空间，使来自同一类别的深度学习特征更加紧凑，并更接近相应的中心。类似地，三元组损失通过计算实例与类别中心的距离，而不是比较每两个实例之间的距离。
- en: '| (2) |  | $\mathcal{L}_{tc}=\sum_{i=1}^{M}\max\left(D(f_{i},C_{y_{i}})+m-\min_{j\neq
    y_{i}}D(f_{i},c_{j}),0\right),$ |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $\mathcal{L}_{tc}=\sum_{i=1}^{M}\max\left(D(f_{i},C_{y_{i}})+m-\min_{j\neq
    y_{i}}D(f_{i},c_{j}),0\right),$ |  |'
- en: where $D()$ is a distance function and $m$ is margin value. By setting up a
    margin value, the loss function ensures different classes be pushed by at least
    $m$ distance away.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $D()$ 是一个距离函数，$m$ 是边际值。通过设置边际值，损失函数确保不同类别之间至少有 $m$ 的距离。
- en: 2.1.4\. Arcface
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.4\. Arcface
- en: In (Deng et al., [2019](#bib.bib11)), Deng et al. proposed an additive angular
    margin loss (ArcFace) to obtain highly discriminative features for face recognition.
    Based on classic softmax loss,
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在（Deng 等人，[2019](#bib.bib11)）中，Deng 等人提出了一种附加角度边际损失（ArcFace），以获得高度可区分的特征用于人脸识别。基于经典的
    softmax 损失，
- en: '| (3) |  | $\mathcal{L}_{\text{softmax}}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{e^{W_{y_{i}}^{T}x_{i}+b_{y_{i}}}}{\sum_{j=1}^{n}e^{W_{j}^{T}x_{i}+b_{j}}},$
    |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\mathcal{L}_{\text{softmax}}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{e^{W_{y_{i}}^{T}x_{i}+b_{y_{i}}}}{\sum_{j=1}^{n}e^{W_{j}^{T}x_{i}+b_{j}}},$
    |  |'
- en: 'where $x_{i}$ denotes the embedding of the $i$th sample. After normalizing
    $x$ and $W$, ArcFace adds an additive angular margin penalty $m$ between $x_{i}$
    and $W_{y_{i}}$ to simultaneously enhance the intra-class compactness and inter-class
    discrepancy:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $x_{i}$ 表示第 $i$ 个样本的嵌入。ArcFace 在归一化 $x$ 和 $W$ 后，向 $x_{i}$ 和 $W_{y_{i}}$ 之间添加了一个附加的角度边际惩罚
    $m$，以同时增强类内紧凑性和类间差异：
- en: '| (4) |  | $\mathcal{L}_{\text{Arc}}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{e^{s(\cos(\theta_{y_{i}}+m))}}{e^{s(\cos(\theta_{y_{i}}+m))}+\sum_{j=1,j\neq
    y_{i}}^{n}e^{s\cos\theta_{j}}}$ |  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\mathcal{L}_{\text{Arc}}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{e^{s(\cos(\theta_{y_{i}}+m))}}{e^{s(\cos(\theta_{y_{i}}+m))}+\sum_{j=1,j\neq
    y_{i}}^{n}e^{s\cos\theta_{j}}}$ |  |'
- en: The paper shows that ArcFace has a better geometric attribute as the angular
    margin has the exact correspondence to the geodesic distance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 论文显示，ArcFace 具有更好的几何属性，因为角度边际与测地距离具有精确的对应关系。
- en: 2.1.5\. ResNets
  id: totrans-45
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.1.5\. ResNets
- en: 'The training of deeper neural networks is facing degradation problems: with
    the network depth increasing, accuracy gets saturated and then degrades rapidly.
    The degradation problem indicates that not all systems are similarly easy to optimize.
    Under the hypothesis that it is easier to optimize the residual mapping than to
    optimize the original unreferenced mapping, He et al. ([2016](#bib.bib23)) presented
    a residual learning framework called ResNets to ease the training of the network.
    ResNets consists of residual blocks as Figure [2](#S2.F2 "Figure 2 ‣ 2.1.5\. ResNets
    ‣ 2.1\. Convolutional neural networks ‣ 2\. Deep learning basics ‣ Deep Learning
    and Open Set Malware Classification: A Survey"). Instead of hoping each few stacked
    layers directly fit a desired underlying mapping, ResNets explicitly let these
    layers fit a residual mapping.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络的训练面临退化问题：随着网络深度的增加，准确率会达到饱和点并迅速下降。退化问题表明并非所有系统都同样容易优化。根据假设，优化残差映射比优化原始未参考映射更容易，He
    等人（[2016](#bib.bib23)）提出了一种名为 ResNets 的残差学习框架，以简化网络的训练。ResNets 由残差块组成，如图 [2](#S2.F2
    "图 2 ‣ 2.1.5\. ResNets ‣ 2.1\. 卷积神经网络 ‣ 2\. 深度学习基础 ‣ 深度学习与开放集恶意软件分类：综述") 所示。ResNets
    明确地让这些层拟合残差映射，而不是希望每几层直接拟合所需的基础映射。
- en: '![Refer to caption](img/ff1b181bfaf970f23043f75f1c1f96b6.png)\Description'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '![参考说明](img/ff1b181bfaf970f23043f75f1c1f96b6.png)\描述'
- en: resNet
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: resNet
- en: Figure 2\. A building block of resNet ([He et al.](#bib.bib23), ([2016](#bib.bib23)))
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. resNet 的一个构建块 ([He 等人](#bib.bib23), ([2016](#bib.bib23)))
- en: Specifically, instead fitting the desired underlying mapping as $H(x)$, ResNet
    makes stacked nonlinear layers fit another mapping of $\mathcal{F}(x):=H(x)-x$.
    Then original mapping is recast into $\mathcal{F}(x)+x$. In an extreme case, if
    an identity mapping were optimal, it would be easier to push the residual to zero
    than to fit an identity mapping.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，ResNet 不是拟合所需的基础映射 $H(x)$，而是让堆叠的非线性层拟合另一个映射 $\mathcal{F}(x):=H(x)-x$。然后原始映射被重塑为
    $\mathcal{F}(x)+x$。在极端情况下，如果恒等映射是最优的，那么将残差推向零会比拟合恒等映射更容易。
- en: '![Refer to caption](img/3e88067395562d82641c4e483eb629aa.png)\Description'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '![参考说明](img/3e88067395562d82641c4e483eb629aa.png)\描述'
- en: RNNs
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: Figure 3\. A standard RNN contains a single layer ([Olah](#bib.bib38), ([2015](#bib.bib38)))
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 标准 RNN 包含一个单层 ([Olah](#bib.bib38), ([2015](#bib.bib38)))
- en: 2.2\. Recurrent neural networks
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2\. 循环神经网络
- en: 'Another type of popular architecture of Deep Neural Networks is Recurrent Neural
    Networks (RNNs), which involves sequential inputs, such as speech and language.
    RNNs process an input sequence one element at a time, also maintain a state vector
    that implicitly contains all the historical information. An unfold RNN (Figure
    [3](#S2.F3 "Figure 3 ‣ 2.1.5\. ResNets ‣ 2.1\. Convolutional neural networks ‣
    2\. Deep learning basics ‣ Deep Learning and Open Set Malware Classification:
    A Survey")) could be considered as a deep multi-layer network. Just like CNNs,
    there are multiple variants for RNNs as well. Particularly, it has been widely
    used in machine translation tasks ((Cho et al., [2014](#bib.bib9)) (Bahdanau et al.,
    [2015](#bib.bib3)) (Luong et al., [2015](#bib.bib33))).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种流行的深度神经网络架构是递归神经网络（RNNs），它涉及顺序输入，如语音和语言。RNNs 一次处理一个元素的输入序列，同时维护一个隐含包含所有历史信息的状态向量。展开的
    RNN（图 [3](#S2.F3 "图 3 ‣ 2.1.5\. ResNets ‣ 2.1\. 卷积神经网络 ‣ 2\. 深度学习基础 ‣ 深度学习与开放集恶意软件分类：综述")）可以被视为一个深度多层网络。就像
    CNNs 一样，RNNs 也有多个变种。特别是，它在机器翻译任务中被广泛使用 ((Cho et al., [2014](#bib.bib9)) (Bahdanau
    et al., [2015](#bib.bib3)) (Luong et al., [2015](#bib.bib33)))。
- en: '![Refer to caption](img/14648ea3bc8f23656cd2651a9ed3edda.png)\Description'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![参考标题](img/14648ea3bc8f23656cd2651a9ed3edda.png)\描述'
- en: LSTMs
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: LSTMs
- en: Figure 4\. An LSTM contains four interacting layers ([Olah](#bib.bib38), ([2015](#bib.bib38)))
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. LSTM 包含四个相互作用的层 ([Olah](#bib.bib38)，([2015](#bib.bib38)))
- en: 2.2.1\. LSTM
  id: totrans-59
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.1\. LSTM
- en: 'As it is not applicable to store information for very long, “Long Short Term
    Memory” (LSTM) was proposed to solve the problem. [Chen](#bib.bib8) gave a gentle
    tutorial on basics of backpropagation in recurrent neural networks (RNN) and long
    short-term memory (LSTM) in (Chen, [2016](#bib.bib8)). LSTM (Figure [4](#S2.F4
    "Figure 4 ‣ 2.2\. Recurrent neural networks ‣ 2\. Deep learning basics ‣ Deep
    Learning and Open Set Malware Classification: A Survey")) includes four gates:
    input modulation gate, input gate, forget gate (Gers et al. ([1999](#bib.bib16)))
    and output gate along with their corresponding weights. LSTM also contains a special
    unit called memory cell act like an accumulator or a gated leaky neuron. Meanwhile,
    There are other augment RNNs with a memory module such as “Neural Turing Machine”
    and “memory networks”. These models are being used for tasks need reasoning and
    symbol manipulation.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无法长期存储信息，因此提出了“长短期记忆”（LSTM）以解决这一问题。[Chen](#bib.bib8) 在 (Chen, [2016](#bib.bib8))
    中提供了关于递归神经网络（RNN）和长短期记忆（LSTM）基本概念的简明教程。LSTM（图 [4](#S2.F4 "图 4 ‣ 2.2\. 递归神经网络 ‣
    2\. 深度学习基础 ‣ 深度学习与开放集恶意软件分类：综述")）包括四个门：输入调制门、输入门、遗忘门（Gers et al. ([1999](#bib.bib16)))
    和输出门以及它们相应的权重。LSTM 还包含一个称为记忆单元的特殊单元，类似于累加器或带门控的漏电神经元。同时，还有其他增强型 RNN，带有记忆模块，如“神经图灵机”和“记忆网络”。这些模型用于需要推理和符号操作的任务。
- en: '![Refer to caption](img/59c1482cfe224dfac1b38fecadaf19a3.png)\Description'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![参考标题](img/59c1482cfe224dfac1b38fecadaf19a3.png)\描述'
- en: rnn_en_de
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: rnn_en_de
- en: Figure 5\. An illustration of the RNN Encoder-Decoder ([Cho et al.](#bib.bib9),
    ([2014](#bib.bib9)))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. RNN 编码器-解码器的示意图 ([Cho et al.](#bib.bib9)，([2014](#bib.bib9)))
- en: 2.2.2\. RNN encoder-dencoder
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.2\. RNN 编码器-解码器
- en: 'Cho et al. ([2014](#bib.bib9)) proposed a neural network architecture called
    RNN Encoder-Decoder (Figure [5](#S2.F5 "Figure 5 ‣ 2.2.1\. LSTM ‣ 2.2\. Recurrent
    neural networks ‣ 2\. Deep learning basics ‣ Deep Learning and Open Set Malware
    Classification: A Survey")), which can be to used as additional features in statistical
    machine translation (SMT) system to generate a target sequence, also can be used
    to score a given pair of input and output sequence. The architecture learns to
    encode a variable-length sequence into a fixed-length vector representation and
    to decode a given fixed-length vector representation back into a variable-length
    sequence. The encoder is an RNN that reads each symbol of an input sequence x
    sequentially. The decoder of the proposed model is another RNN that is trained
    to generate the output sequence by predicting the next symbol given the hidden
    state.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Cho et al. ([2014](#bib.bib9)) 提出了一个称为 RNN 编码器-解码器的神经网络架构（图 [5](#S2.F5 "图 5
    ‣ 2.2.1\. LSTM ‣ 2.2\. 递归神经网络 ‣ 2\. 深度学习基础 ‣ 深度学习与开放集恶意软件分类：综述")），该架构可以作为统计机器翻译（SMT）系统中的附加特征，用于生成目标序列，也可以用于评分给定的输入和输出序列对。该架构学会将可变长度的序列编码为固定长度的向量表示，并将给定的固定长度向量表示解码为可变长度的序列。编码器是一个
    RNN，它顺序读取输入序列 x 的每个符号。所提模型的解码器是另一个 RNN，经过训练以生成输出序列，通过预测给定隐藏状态下的下一个符号。
- en: In addition to a novel model architecture, the paper also proposed a variant
    of LSTM, which includes an update gate and a reset gate. The update gate selects
    whether the hidden state is to be updated with a new hidden state while the reset
    gate decides whether the previous hidden state is ignored.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 除了新的模型架构，论文还提出了一种 LSTM 的变体，其中包括一个更新门和一个重置门。更新门决定是否用新的隐藏状态更新隐藏状态，而重置门决定是否忽略之前的隐藏状态。
- en: '![Refer to caption](img/dd0b4da57130584a9354da48a2237111.png)\Description'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/dd0b4da57130584a9354da48a2237111.png)\说明'
- en: rnnsearch
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: rnnsearch
- en: Figure 6\. An illustration of the RNNsearch ([Bahdanau et al.](#bib.bib3), ([2015](#bib.bib3)))
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6\. RNNsearch 的示意图 ([Bahdanau 等人](#bib.bib3), ([2015](#bib.bib3)))
- en: 2.2.3\. RNNsearch
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.3\. RNNsearch
- en: 'In (Bahdanau et al., [2015](#bib.bib3)), [Bahdanau et al.](#bib.bib3) proposed
    a new architecture for machine translation model by adding an alignment model
    to basic RNN Encoder-Decoder. Just like a traditional machine translation model,
    the proposed architecture consists of an encoder and a decoder. The encoder reads
    the input sentence, then converts into a vector. And the decoder emulates searching
    through a source sentence during decoding a translation. As Figure [6](#S2.F6
    "Figure 6 ‣ 2.2.2\. RNN encoder-dencoder ‣ 2.2\. Recurrent neural networks ‣ 2\.
    Deep learning basics ‣ Deep Learning and Open Set Malware Classification: A Survey"),
    the align model learns the weights $\alpha_{ij}$ of each annotation $h_{j}$ scoring
    how well the inputs around the position $j$ and output at position $i$ match.
    The score is based on the RNN hidden state $s_{i-1}$ and the $j$th annotation
    $h_{j}$ of the input sentence.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在（Bahdanau 等人，[2015](#bib.bib3)）中，[Bahdanau 等人](#bib.bib3)通过向基础 RNN 编码器-解码器添加对齐模型，提出了一种新的机器翻译模型架构。与传统的机器翻译模型类似，所提架构包括一个编码器和一个解码器。编码器读取输入句子，然后将其转换为一个向量。解码器在翻译过程中模拟搜索源句子。如图
    [6](#S2.F6 "图 6 ‣ 2.2.2\. RNN 编码器-解码器 ‣ 2.2\. 循环神经网络 ‣ 2\. 深度学习基础 ‣ 深度学习与开放集恶意软件分类：综述")
    所示，对齐模型学习每个注释 $h_{j}$ 的权重 $\alpha_{ij}$，以评分输入位置 $j$ 附近的输入与位置 $i$ 的输出匹配的程度。该评分基于
    RNN 隐藏状态 $s_{i-1}$ 和输入句子的第 $j$ 个注释 $h_{j}$。
- en: 2.2.4\. Attentional mechanism
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.2.4\. 注意力机制
- en: 'In (Luong et al., [2015](#bib.bib33)), [Luong et al.](#bib.bib33) examined
    two classes of attentional mechanism to better improve neural machine translation
    (NMT): a global approach which always attends to all source words and a local
    one that only looks at a subset of source words at a time. Based on LSTM, they
    introduced a variable-length alignment vector for two kinds of attentional mechanisms.
    The global attention model is based on the global context, and the size of the
    alignment vector equals the number of time steps on the source site. While the
    local attention model is based on a window context, where the size of the alignment
    vector equals to window size.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在（Luong 等人，[2015](#bib.bib33)）中，[Luong 等人](#bib.bib33)研究了两类注意力机制，以更好地改进神经机器翻译（NMT）：一种是全局方法，它始终关注所有源语言词汇；另一种是局部方法，它只关注一次源语言词汇的一个子集。基于
    LSTM，他们为这两种注意力机制引入了变长对齐向量。全局注意力模型基于全局上下文，对齐向量的大小等于源语言站点上的时间步数。而局部注意力模型则基于窗口上下文，对齐向量的大小等于窗口大小。
- en: '![Refer to caption](img/41385c0b65b46740844fd54c298cea51.png)\Description'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/41385c0b65b46740844fd54c298cea51.png)\说明'
- en: GANs
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: GANs
- en: Figure 7\. Overview of the framework of GANs ([Silva](#bib.bib51), ([2018](#bib.bib51)))
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7\. GAN 框架概述 ([Silva](#bib.bib51), ([2018](#bib.bib51)))
- en: 2.3\. Generative adversarial networks
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3\. 生成对抗网络
- en: 'Deep learning has achieved great performance in supervised learning in discriminative
    models. However, deep generative models have had less of impact as:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在判别模型的监督学习中取得了显著的性能。然而，深度生成模型的影响较小，例如：
- en: •
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It is difficult to approximate the computations in maximum likelihood estimation
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在最大似然估计中近似计算是困难的
- en: •
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It is difficult to leverage the benefits of piecewise linear units in the generative
    context”
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在生成上下文中利用分段线性单元的好处是困难的
- en: 'Goodfellow et al. ([2014](#bib.bib17)) proposed a new generative model: generative
    adversarial nets (GANs) to avoid these difficulties. The proposed GANs architecture
    includes two components: a generator $G$ and a discriminator $D$. To learn the
    distribution $P_{g}$ over given data $x$. GANs define a prior on input noise variables
    $p_{\mathbf{z}}(\mathbf{z})$. The framework of GANs corresponds to a “min-max
    two-player game” (discriminator vs. generator) with value function $V(G,D)$:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Goodfellow等人（[2014](#bib.bib17)）提出了一种新的生成模型：生成对抗网络（GANs），以避免这些困难。提出的GANs架构包括两个组件：生成器$G$和判别器$D$。以学习给定数据$x$上的分布$P_{g}$。GANs在输入噪声变量上定义了一个先验$p_{\mathbf{z}}(\mathbf{z})$。GANs的框架对应于一个“最小-最大双人游戏”（判别器对生成器）与价值函数$V(G,D)$：
- en: '| (5) |  | $\min_{G}\max_{D}V(D,G)=\mathbb{E}_{\mathbf{x}\sim p_{\text{data}}(\mathbf{x})}[\log
    D(\mathbf{x})]+\mathbb{E}_{\mathbf{z}\sim p_{\mathbf{z}}(\mathbf{z})}[\log(1-D(G(\mathbf{z})))]$
    |  |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\min_{G}\max_{D}V(D,G)=\mathbb{E}_{\mathbf{x}\sim p_{\text{data}}(\mathbf{x})}[\log
    D(\mathbf{x})]+\mathbb{E}_{\mathbf{z}\sim p_{\mathbf{z}}(\mathbf{z})}[\log(1-D(G(\mathbf{z})))]$
    |  |'
- en: Generator generates noise samples from a prior distribution and discriminator
    represents the probability of the data come from the target dataset rather than
    a generator. Hence the target is to train the discriminator $D$ to maximize the
    probability of assigning the correct label to both training examples and samples
    from $G$, meanwhile train the generator $G$ to minimize $\log(1-D(G(\mathbf{z})))$,
    i.e. generating samples alike examples to “fool” the discriminator. In practice,
    the procedure optimizes $D$ $k$ steps and one step of $G$.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器从先验分布生成噪声样本，而判别器表示数据来自目标数据集的概率而非生成器。因此，目标是训练判别器$D$以最大化对训练样本和来自$G$的样本正确标签的概率，同时训练生成器$G$以最小化$\log(1-D(G(\mathbf{z})))$，即生成类似于示例的样本以“欺骗”判别器。实际上，该过程优化$D$
    $k$步和$G$的一步。
- en: 2.3.1\. DCGANs
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1\. DCGANs
- en: 'Radford et al. ([2016](#bib.bib41)) proposed deep convolution generative adversarial
    networks (DCGANs) to bridge the gap between the supervised learning and unsupervised
    learning in CNNs, which makes GANs more stable. The architecture guidelines for
    stable Deep Convolutional GANs:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: Radford等人（[2016](#bib.bib41)）提出了深度卷积生成对抗网络（DCGANs），以弥合CNN中监督学习和无监督学习之间的差距，使GANs更稳定。稳定深度卷积GANs的架构指南：
- en: •
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Replace any pooling layers with stridden convolutions (discriminator) and fractional-strided
    convolutions (generator).
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将任何池化层替换为步幅卷积（判别器）和分数步幅卷积（生成器）。
- en: •
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Use batch norm in both the generator and the discriminator.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在生成器和判别器中使用批量归一化。
- en: •
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Remove fully connected hidden layers for deeper architectures.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于更深的架构，移除全连接隐藏层。
- en: •
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Use ReLU activation in the generator for all layers except for the output, which
    uses Tanh.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在生成器中使用ReLU激活函数，除了输出层使用Tanh。
- en: •
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Use LeakyReLU activation in the discriminator for all layers.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在判别器的所有层中使用LeakyReLU激活函数。
- en: 2.3.2\. AAE
  id: totrans-98
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. AAE
- en: 'Makhzani et al. ([2015](#bib.bib34)) proposed a new inference algorithm Adversarial
    Autoencoder (AAE), which uses the GANs framework which could better deal with
    applications such as semi-supervised classification, disentangling style and content
    of images, unsupervised clustering, dimensionality reduction, and data visualization.
    The algorithm aims to find a representation for graphs that follows a certain
    type of distribution. And it consists of two phases: the reconstruction phase
    and the regularization phase. In the reconstruction phase, encoder and decoder
    are updated to minimize reconstruction error. In the regularization phase, the
    discriminator is updated to distinguish true prior samples from generated samples,
    and the generator is updated to fool the discriminator. Reconstruction phase and
    regularization phase are referred to as the generator and discriminator in GANs.
    And the method could be used in semi-supervised learning and unsupervised clustering.
    For semi-supervised learning, there is a semi-supervised classification phase
    besides the reconstruction phase and regularization phase. And labeled data would
    be trained at this stage. which is an aggregated categorical distribution. The
    architecture of unsupervised clustering is similar to semi-supervised learning,
    the difference is that the semi-supervised classification stage is removed and
    thus no longer train the network on any labeled mini-batch.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Makhzani et al. ([2015](#bib.bib34)) 提出了一个新的推断算法 Adversarial Autoencoder (AAE)，它使用
    GANs 框架，能够更好地处理半监督分类、图像风格与内容的解耦、无监督聚类、降维和数据可视化等应用。该算法旨在为图形找到遵循某种类型分布的表示。它包括两个阶段：重建阶段和正则化阶段。在重建阶段，编码器和解码器被更新以最小化重建误差。在正则化阶段，判别器被更新以区分真实的先验样本和生成的样本，而生成器则被更新以欺骗判别器。重建阶段和正则化阶段在
    GANs 中被称为生成器和判别器。该方法可以用于半监督学习和无监督聚类。对于半监督学习，除了重建阶段和正则化阶段外，还有一个半监督分类阶段，并且在这一阶段将训练标记数据。无监督聚类的架构类似于半监督学习，不同之处在于去除了半监督分类阶段，因此不再对任何标记的小批量数据进行训练。
- en: 2.4\. Representation learning
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4\. 表示学习
- en: Representation learning allows a machine to be fed with raw data and to automatically
    discover the representations (embeddings) needed for detection or classification
    (LeCun et al., [2015](#bib.bib29)). Those raw data could be images, videos, texts,
    etc. An image comes in the form of an array of pixel values and texts come in
    the form of word sequences. Motivated by different objectives, a set of representative
    features would be generated through deep neural networks.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表示学习允许机器接收原始数据，并自动发现用于检测或分类所需的表示（嵌入）（LeCun et al., [2015](#bib.bib29)）。这些原始数据可以是图像、视频、文本等。图像以像素值数组的形式存在，文本以单词序列的形式存在。根据不同的目标，通过深度神经网络生成一组代表性特征。
- en: 2.4.1\. Skip-gram
  id: totrans-102
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.1\. Skip-gram
- en: 'Skip-gram model has achieved good performance in learning high-quality vector
    representations of words from large amounts of unstructured text data, which doesn’t
    require dense matrix multiplications. The training objective of the Skip-gram
    model is to find word representations that are useful for predicting the surrounding
    words in a sentence or a document. Given a sequence of training words $\mathfrak{w}_{1},\mathfrak{w}_{2},\mathfrak{w}_{3},...,\mathfrak{w}_{T}$,
    the objective of the Skip-gram model is to maximize the average log probability:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Skip-gram 模型在从大量非结构化文本数据中学习高质量单词向量表示方面表现良好，它不需要密集矩阵乘法。Skip-gram 模型的训练目标是找到对预测句子或文档中的周围单词有用的单词表示。给定一个训练单词序列
    $\mathfrak{w}_{1},\mathfrak{w}_{2},\mathfrak{w}_{3},...,\mathfrak{w}_{T}$，Skip-gram
    模型的目标是最大化平均对数概率：
- en: '| (6) |  | $\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\leq j\leq c,j\neq 0}\log p(\mathfrak{w}_{t+j}&#124;\mathfrak{w}_{t}),$
    |  |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\leq j\leq c,j\neq 0}\log p(\mathfrak{w}_{t+j}&#124;\mathfrak{w}_{t}),$
    |  |'
- en: 'where $c$ is the size of training context. The basic Skip-gram formulation
    defines $p(\mathfrak{w}_{t+j}|\mathfrak{w}_{t})$ using the softmax function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $c$ 是训练上下文的大小。基本的 Skip-gram 公式通过 softmax 函数定义 $p(\mathfrak{w}_{t+j}|\mathfrak{w}_{t})$：
- en: '| (7) |  | $p(\mathfrak{w}_{\mathit{O}}&#124;\mathfrak{w}_{\mathit{I}})=\frac{\exp(\mathfrak{v}^{\prime\top}_{\mathfrak{w}_{\mathit{O}}}\mathfrak{v}_{\mathfrak{w}_{\mathit{I}}})}{\sum_{\mathfrak{w}=1}^{W}\exp(\mathfrak{v}^{\prime\top}_{\mathfrak{w}}\mathfrak{v}_{\mathfrak{w}_{\mathit{I}}})}.$
    |  |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $p(\mathfrak{w}_{\mathit{O}}|\mathfrak{w}_{\mathit{I}})=\frac{\exp(\mathfrak{v}^{\prime\top}_{\mathfrak{w}_{\mathit{O}}}\mathfrak{v}_{\mathfrak{w}_{\mathit{I}}})}{\sum_{\mathfrak{w}=1}^{W}\exp(\mathfrak{v}^{\prime\top}_{\mathfrak{w}}\mathfrak{v}_{\mathfrak{w}_{\mathit{I}}})}.$
    |  |'
- en: 'where $\mathfrak{v}_{\mathfrak{w}}$ and $\mathfrak{v}^{\prime}_{\mathfrak{w}}$
    are the ”input” and ”output” vector representations of $\mathfrak{w}$, and $W$
    is the number of words in the vocabulary. Based on the skip-gram algorithm. Mikolov
    et al. ([2013](#bib.bib36)) presents some extensions to improve its performance:
    Hierarchical softmax, negative sampling and subsampling. It shows that the word
    vectors can be meaningful combined using just simple vector addition. Specifically,
    hierarchical softmax uses a binary tree to present the output layer rather than
    a plat output of all the words for output dimension reduction to make the computation
    more efficient. An alternative to hierarchical softmax is negative sampling, inspired
    by Noise Contrastive Estimation (NCE). The basic idea is to sample one “accurate”
    data and $k$ noise data, the objective is to maximize their conditional log-likelihood:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathfrak{v}_{\mathfrak{w}}$ 和 $\mathfrak{v}^{\prime}_{\mathfrak{w}}$ 是词
    $\mathfrak{w}$ 的“输入”和“输出”向量表示，$W$ 是词汇表中的词数。基于 skip-gram 算法。Mikolov 等人 ([2013](#bib.bib36))
    提出了一些扩展以改善其性能：层次 softmax、负采样和子采样。研究表明，词向量可以通过简单的向量加法有意义地组合在一起。具体而言，层次 softmax
    使用二叉树来表示输出层，而不是将所有词的平面输出用于输出维度的减少，以提高计算效率。层次 softmax 的替代方案是负采样，灵感来自噪声对比估计 (NCE)。基本思想是抽样一个“准确”的数据和
    $k$ 个噪声数据，目标是最大化它们的条件对数似然：
- en: '| (8) |  | $\log\sigma(\mathfrak{v}^{\prime\top}_{\mathfrak{w}_{\mathit{O}}}\mathfrak{v}_{\mathfrak{w}_{\mathit{I}}})+\sum_{i=1}^{k}\mathbb{E}_{\mathfrak{w}_{i}\sim
    P_{n}(\mathfrak{w})}\left[\log\sigma(-v^{\prime\top}_{\mathfrak{w}_{i}}v_{\mathfrak{w}_{\mathit{I}}})\right],$
    |  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\log\sigma(\mathfrak{v}^{\prime\top}_{\mathfrak{w}_{\mathit{O}}}\mathfrak{v}_{\mathfrak{w}_{\mathit{I}}})+\sum_{i=1}^{k}\mathbb{E}_{\mathfrak{w}_{i}\sim
    P_{n}(\mathfrak{w})}\left[\log\sigma(-v^{\prime\top}_{\mathfrak{w}_{i}}v_{\mathfrak{w}_{\mathit{I}}})\right],$
    |  |'
- en: 'The objective of NCE is used to replace every $\log P(\mathfrak{w}_{\mathit{O}}|\mathfrak{w}_{\mathit{I}})$
    term in the Skip-gram objective, and the task is to distinguish the target word
    $\mathfrak{w}_{\mathit{O}}$ from draws from the noise distribution $P_{n}(\mathfrak{w})$
    using logistic regression, where there are $k$ negative samples for each data
    sample. The paper also suggested a simple subsampling approach to address the
    imbalance issue between the rare and frequent words: each word $w_{i}$ in the
    training set is discarded with probability computed by the formula'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: NCE 的目标是用来替代 Skip-gram 目标中的每一个 $\log P(\mathfrak{w}_{\mathit{O}}|\mathfrak{w}_{\mathit{I}})$
    项，任务是通过逻辑回归将目标词 $\mathfrak{w}_{\mathit{O}}$ 与从噪声分布 $P_{n}(\mathfrak{w})$ 中抽样的词区分开来，其中每个数据样本有
    $k$ 个负样本。论文还建议了一种简单的子采样方法来解决稀有词和频繁词之间的不平衡问题：训练集中每个词 $w_{i}$ 以根据公式计算的概率被丢弃。
- en: '| (9) |  | $P(w_{i})=1-\sqrt{\frac{t}{f(w_{i})}},$ |  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $P(w_{i})=1-\sqrt{\frac{t}{f(w_{i})}},$ |  |'
- en: where $f(w_{i})$ is the frequent of word $w_{i}$ and $t$ is threshold.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(w_{i})$ 是词 $w_{i}$ 的频率，$t$ 是阈值。
- en: 2.4.2\. GloVe
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.4.2\. GloVe
- en: 'To better deal with word representations: word analogy, word similarity, and
    named entity recognition tasks, Pennington et al. ([2014](#bib.bib39)) constructs
    a new model GloVe (for Global Vectors), which can capture the global corpus statistics.
    GloVe combines count-based methods and prediction-based methods for the unsupervised
    learning of word representations, proposing a new cost function'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地处理词表示：词类比、词相似度和命名实体识别任务，Pennington 等人 ([2014](#bib.bib39)) 构建了一个新的模型 GloVe（全局向量），它可以捕捉全局语料库统计信息。GloVe
    结合了基于计数的方法和基于预测的方法进行无监督的词表示学习，提出了一个新的成本函数。
- en: '| (10) |  | $J=\sum_{i,j=1}^{V}f(X_{ij})(W_{i}^{T}\tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log
    X_{ij})^{2},$ |  |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $J=\sum_{i,j=1}^{V}f(X_{ij})(W_{i}^{T}\tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log
    X_{ij})^{2},$ |  |'
- en: where $V$ is the size of the vocabulary, $f(X_{ij})$ is a weighting function.
    $w$ are word vectors and $\tilde{w}$ are separate context word vector as training
    multiple instances of the network and then combining the results can help reduce
    overfitting and noise and generally improve results. $b$ and $\tilde{b}$ are corresponding
    bias for $w$ and $\tilde{w}$.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $V$ 是词汇表的大小，$f(X_{ij})$ 是加权函数。$w$ 是词向量，$\tilde{w}$ 是单独的上下文词向量，通过训练多个网络实例然后合并结果，可以帮助减少过拟合和噪声，并通常改善结果。$b$
    和 $\tilde{b}$ 是对应于 $w$ 和 $\tilde{w}$ 的偏置。
- en: 2.5\. Meta-learning and interpretability
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5\. 元学习与可解释性
- en: '![Refer to caption](img/6ee868a868b63a113795d13e936aaeb1.png)\Description'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见标题](img/6ee868a868b63a113795d13e936aaeb1.png)\描述'
- en: MAML
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: MAML
- en: Figure 8\. Diagram of the MAML ([Finn et al.](#bib.bib13), ([2017](#bib.bib13)))
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图8\. MAML的示意图 ([Finn et al.](#bib.bib13), ([2017](#bib.bib13)))
- en: 2.5.1\. Meta-Learning
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.1\. 元学习
- en: 'Deep neural networks generally perform poorly on few-shot learning tasks as
    a classifier has to quickly generalize after seeing very few examples from each
    class. Ravi and Larochelle ([2016](#bib.bib42)) proposes an LSTM based meta-learner
    model to learn the exact optimization algorithm used to train another learner
    neural network classifier in the few-shot regime. The meta-learner captures both
    short-term knowledge within a task and long-term knowledge common among all the
    tasks. Also, Finn et al. ([2017](#bib.bib13)) proposed an algorithm called model-agnostic
    meta-learning (MAML) for meta-learning which is compatible with any model trained
    with gradient descent and different learning problems such as classification,
    regression and reinforcement learning. The meta-learning is to prepare the model
    for fast adaption. In general, it consists of two steps:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络在少样本学习任务中的表现通常较差，因为分类器在看到每个类别的很少样本后需要迅速进行泛化。Ravi 和 Larochelle ([2016](#bib.bib42))
    提出了一个基于 LSTM 的元学习者模型，用于学习在少样本环境下训练另一个学习者神经网络分类器所使用的优化算法。元学习者捕捉任务中的短期知识以及所有任务中共同的长期知识。此外，Finn
    等人 ([2017](#bib.bib13)) 提出了一个名为模型无关元学习（MAML）的算法，适用于任何使用梯度下降进行训练的模型，以及分类、回归和强化学习等不同学习问题。元学习旨在为快速适应做准备。一般来说，它包括两个步骤：
- en: (1)
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: sample batch of tasks to learn the gradient update for each of them, then combine
    their results;
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对每个任务进行样本批次以学习梯度更新，然后将结果合并；
- en: (2)
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: take the result of step 1 as a starting point when learning a specific task.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在学习特定任务时，以第1步的结果作为起点。
- en: 'The diagram looks like Figure [8](#S2.F8 "Figure 8 ‣ 2.5\. Meta-learning and
    interpretability ‣ 2\. Deep learning basics ‣ Deep Learning and Open Set Malware
    Classification: A Survey"), it optimizes for a representation $\theta$ that can
    quickly adapt to new tasks.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '该图类似于图 [8](#S2.F8 "Figure 8 ‣ 2.5\. Meta-learning and interpretability ‣ 2\.
    Deep learning basics ‣ Deep Learning and Open Set Malware Classification: A Survey")，它优化一个表示
    $\theta$，以便能快速适应新任务。'
- en: 2.5.2\. LIME
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.2\. LIME
- en: As machine learning techniques are rapidly developed these days, there are plenty
    of models remain mostly black box. To make the predictions more interpretable
    to non-expertises despite which model it is (model-agnostic), so that people can
    make better decisions, Ribeiro et al. ([2016](#bib.bib44)) proposed a method -
    Local Interpretable Model-agnostic Explanations (LIME) to identify an interpretable
    model over interpretable presentation that is locally faithful to the classifier.
    It introduced sparse line explanations, weighing similarity between instance and
    its interpretable version with their distance. The paper also suggested a submodular
    pick algorithm (SP-LIME) to better select instances by picking the most important
    features based on the explanation matrix learned in LIME.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习技术的快速发展，目前许多模型仍然大多是黑箱。为了使预测对非专家更具可解释性，无论模型是什么（模型无关），以便人们能够做出更好的决策，Ribeiro
    等人 ([2016](#bib.bib44)) 提出了局部可解释模型无关解释（LIME）的方法，以识别在分类器局部忠实的可解释模型。它引入了稀疏的线性解释，通过实例及其可解释版本之间的相似性和距离进行加权。论文还提出了一种子模块选择算法（SP-LIME），通过基于在
    LIME 中学到的解释矩阵选择最重要的特征来更好地选择实例。
- en: 2.5.3\. Large-Scale evolution
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.5.3\. 大规模演化
- en: To minimize human participation in neural network design, Real et al. (Real
    et al., [2017](#bib.bib43)) employed evolutionary algorithms to discover network
    architectures automatically. The evolutionary algorithm uses the evolutionary
    algorithm to select the best of a pair to be a parent during tournament selection.
    Using pairwise comparisons instead of whole population operations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化人工参与神经网络设计，Real et al. (Real et al., [2017](#bib.bib43)) 使用进化算法自动发现网络架构。进化算法使用进化算法在锦标赛选择过程中选择一对中的最佳作为父母。使用成对比较，而不是整个种群操作。
- en: In the proposed method, individual architectures are encoded as a graph. In
    the graph, the vertices represent rank-3 tensors or activations. The graph’s edges
    represent identity connections or convolutions and contain the mutable numerical
    parameters defining the convolution’s properties. A child is similar but not identical
    to the parent because of the action of a mutation. Mutation operations include
    ”ALTER-LEARNING-RATE”, ”RESET-WEIGHTS”, ”INSERT-CONVOLUTION”, ”REMOVE-CONVOLUTION”,
    etc.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在提出的方法中，单个架构被编码为图。在图中，顶点代表三阶张量或激活值。图的边表示身份连接或卷积，并包含定义卷积属性的可变数值参数。由于突变的作用，子图与父图类似但不完全相同。突变操作包括“ALTER-LEARNING-RATE”、“RESET-WEIGHTS”、“INSERT-CONVOLUTION”、“REMOVE-CONVOLUTION”等。
- en: 3\. Open Set Recognition
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 开放集识别
- en: 'As deep learning has achieved great success in object classification, it is
    less likely to label all the classes in training samples, and the unlabeled class,
    so-called open-set becomes a problem. Different from a traditional close-set problem,
    which only requires correctly classify the labeled data, open set recognition
    (OSR) should also handle those unlabeled ones. [Geng et al.](#bib.bib15) stated
    four categories of recognition problems as follows (Geng et al., [2018](#bib.bib15)):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 随着深度学习在物体分类上取得了巨大成功，将所有类别标记在训练样本中变得不太可能，而未标记的类别，即所谓的开放集，成为了一个问题。与传统的闭集问题不同，闭集问题只需要正确分类标记的数据，而开放集识别（OSR）还需要处理那些未标记的数据。[Geng
    et al.](#bib.bib15) 指出了四类识别问题，如下所示 (Geng et al., [2018](#bib.bib15))：
- en: (1)
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'known known classes: labeled distinctive positive classes, available in training
    samples;'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已知已知类别：标记的独特正类，在训练样本中可用；
- en: (2)
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'known unknown classes: labeled negative classes, available in training samples;'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 已知未知类别：标记的负类，在训练样本中可用；
- en: (3)
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'unknown known classes: training samples not available, but having some side-information
    such as semantic/attribute information'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未知已知类别：训练样本不可用，但有一些侧面信息，如语义/属性信息
- en: (4)
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'unknown unknown classes: neither training samples nor side-information available,
    completely unseen.'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 未知未知类别：既没有训练样本也没有侧面信息，完全未见过。
- en: 'Traditional classification techniques focus on problems with labeled classes,
    which include known known classes and known unknown classes. While open set recognition
    (OSR) pays attention to the later ones: unknown known classes and unknown unknown
    classes. It requires the classifier accurately classify known known classes, meanwhile
    identify unknown unknown classes.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 传统分类技术关注于标记类别的问题，包括已知已知类别和已知未知类别。而开放集识别（OSR）关注于后者：未知已知类别和未知未知类别。它要求分类器准确分类已知已知类别，同时识别未知未知类别。
- en: 'In general, the techniques can be categorized into three classes according
    to the training set compositions as Table [1](#S3.T1 "Table 1 ‣ 3\. Open Set Recognition
    ‣ Deep Learning and Open Set Malware Classification: A Survey").'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，技术可以根据训练集的组成分为三类，如表 [1](#S3.T1 "表 1 ‣ 3\. 开放集识别 ‣ 深度学习与开放集恶意软件分类：综述")。
- en: Table 1\. OSR Techniques Categorization
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 开放集识别技术分类
- en: '| Training Set | papers |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 训练集 | 论文 |'
- en: '| --- | --- |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Borrowing Additional Data | (Shu et al., [2018b](#bib.bib49)) (Saito et al.,
    [2018](#bib.bib46)) (Shu et al., [2018a](#bib.bib50)) (Hendrycks et al., [2019](#bib.bib25))
    (Dhamija et al., [2018](#bib.bib12)) (Perera and Patel, [2019](#bib.bib40)) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 借用额外数据 | (Shu et al., [2018b](#bib.bib49)) (Saito et al., [2018](#bib.bib46))
    (Shu et al., [2018a](#bib.bib50)) (Hendrycks et al., [2019](#bib.bib25)) (Dhamija
    et al., [2018](#bib.bib12)) (Perera and Patel, [2019](#bib.bib40)) |'
- en: '| Generating additional data | (Jo et al., [2018](#bib.bib26)) (Neal et al.,
    [2018](#bib.bib37)) (Ge et al., [2017](#bib.bib14)) (Yu et al., [2017](#bib.bib56))
    (Lee et al., [2018](#bib.bib30)) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 生成额外数据 | (Jo et al., [2018](#bib.bib26)) (Neal et al., [2018](#bib.bib37))
    (Ge et al., [2017](#bib.bib14)) (Yu et al., [2017](#bib.bib56)) (Lee et al., [2018](#bib.bib30))
    |'
- en: '| No Additional Data | (Bendale and Boult, [2016](#bib.bib5)) (Hassen and Chan,
    [2018a](#bib.bib21)) (Júnior et al., [2017](#bib.bib27)) (Mao et al., [2018](#bib.bib35))
    (Wang et al., [2018](#bib.bib53)) (Schultheiss et al., [2017](#bib.bib47)) (Zhang
    and Patel, [2016](#bib.bib57)) (Liang et al., [2018](#bib.bib32)) (Shu et al.,
    [2017](#bib.bib48)) |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 无额外数据 | (Bendale 和 Boult, [2016](#bib.bib5)) (Hassen 和 Chan, [2018a](#bib.bib21))
    (Júnior 等人, [2017](#bib.bib27)) (Mao 等人, [2018](#bib.bib35)) (Wang 等人, [2018](#bib.bib53))
    (Schultheiss 等人, [2017](#bib.bib47)) (Zhang 和 Patel, [2016](#bib.bib57)) (Liang
    等人, [2018](#bib.bib32)) (Shu 等人, [2017](#bib.bib48)) |'
- en: 3.1\. Borrowing additional data
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 借用额外数据
- en: To better discriminate known class and unknown class, some techniques introduce
    unlabeled data in training((Shu et al., [2018b](#bib.bib49)) (Saito et al., [2018](#bib.bib46))).
    In addition, (Shu et al., [2018a](#bib.bib50)) indicates several manually annotations
    for unknown classes are required in their workflow.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地区分已知类别和未知类别，一些技术在训练中引入了未标记数据（Shu 等人，[2018b](#bib.bib49)）（Saito 等人，[2018](#bib.bib46)）。此外，（Shu
    等人，[2018a](#bib.bib50)）指出，他们的工作流程中需要若干手动标注的未知类别。
- en: 3.1.1\. Open set domain adaptation by backpropagation
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. 通过反向传播进行开放集领域适应
- en: Saito et al. ([2018](#bib.bib46)) proposed a method which marks unlabeled target
    samples as unknown, then mixes them with labeled source samples together to train
    a feature generator and a classifier. The classifier attempts to make a boundary
    between source and target samples whereas the generator attempts to make target
    samples far from the boundary. The idea is to extract feature which separates
    known and unknown samples. According to the feature generator, the test data either
    would be aligned to known classes or rejected as an unknown class.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Saito 等人 ([2018](#bib.bib46)) 提出了一种方法，该方法将未标记的目标样本标记为未知，然后将它们与标记的源样本混合，以训练特征生成器和分类器。分类器试图在源样本和目标样本之间建立边界，而生成器则试图使目标样本远离边界。其思想是提取出能够区分已知和未知样本的特征。根据特征生成器，测试数据要么与已知类别对齐，要么被拒绝为未知类别。
- en: 3.1.2\. Unseen class discovery in open-world classification
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 开放世界分类中的未见类别发现
- en: '![Refer to caption](img/618808dfbbda1e9e726f3761520ce849.png)\Description'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/618808dfbbda1e9e726f3761520ce849.png)\Description'
- en: OCN+PCN+HC
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: OCN+PCN+HC
- en: Figure 9\. Overall framework of OCN+PCN+HC ([Shu et al.](#bib.bib49), 2018)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9\. OCN+PCN+HC 的总体框架 ([Shu 等人](#bib.bib49), 2018)
- en: 'Shu et al. ([2018b](#bib.bib49)) introduced a framework to solve the open set
    problem, which involves unlabeled data as an autoencoder network to avoid overfitting.
    Besides autoencoder, it contains another two networks in the training process
    - an Open Classification Network (OCN), a Pairwise Classification Network (PCN).
    Only OCN participants in the testing phase, which predicts test dataset including
    unlabeled examples from both seen and unseen classes. Then it follows the clustering
    phase, based on the results of the predictions of PCN, they used hierarchical
    clustering (bottom-up/ merge) to cluster rejected examples clusters. Overall framework
    as Figure [9](#S3.F9 "Figure 9 ‣ 3.1.2\. Unseen class discovery in open-world
    classification ‣ 3.1\. Borrowing additional data ‣ 3\. Open Set Recognition ‣
    Deep Learning and Open Set Malware Classification: A Survey").'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Shu 等人 ([2018b](#bib.bib49)) 提出了一个解决开放集问题的框架，该框架涉及未标记的数据作为自编码器网络，以避免过拟合。除了自编码器外，它在训练过程中还包含另外两个网络——开放分类网络（OCN）和成对分类网络（PCN）。只有
    OCN 参与测试阶段，预测测试数据集，包括来自已见和未见类别的未标记示例。然后进入聚类阶段，基于 PCN 的预测结果，他们使用层次聚类（自下而上/合并）对被拒绝示例进行聚类。总体框架见图
    [9](#S3.F9 "图 9 ‣ 3.1.2\. 开放世界分类中的未见类别发现 ‣ 3.1\. 借用额外数据 ‣ 3\. 开放集识别 ‣ 深度学习与开放集恶意软件分类：综述")。
- en: 3.1.3\. ODN
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.3\. ODN
- en: 'Manual labeled unknown data is used in Open Deep Network (ODN) proposed by
    Shu et al. ([2018a](#bib.bib50)). It needs several manually annotations. Specifically,
    it added another new column corresponding to the unknown category to the weight
    matrix and initialized it as $\mathfrak{w}_{N+1}$:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 手动标记的未知数据在 Shu 等人 ([2018a](#bib.bib50)) 提出的开放深度网络（ODN）中使用。它需要若干手动标注。具体来说，它在权重矩阵中增加了一个新列以对应未知类别，并将其初始化为
    $\mathfrak{w}_{N+1}$：
- en: '| (11) |  | $\mathfrak{w}_{N+1}=\alpha\frac{1}{N}\sum_{n=1}^{N}\mathfrak{w}_{n}+\beta\frac{1}{M}\sum_{m=1}^{M}\mathfrak{w}_{m},$
    |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\mathfrak{w}_{N+1}=\alpha\frac{1}{N}\sum_{n=1}^{N}\mathfrak{w}_{n}+\beta\frac{1}{M}\sum_{m=1}^{M}\mathfrak{w}_{m},$
    |  |'
- en: where $\mathfrak{w}_{n}$ is the weight column if the known $n$th category. In
    addition, as the similar categories should play a more critical role in the initialization
    of $\mathfrak{w}_{m}$, ODN added another term $\frac{1}{M}\sum_{m=1}^{M}\mathfrak{w}_{m}$
    to emphase the similar known categories. The $\mathfrak{w}_{m}$ is the weight
    columns of $M$ highest activation values. The $\mathfrak{w}_{N+1}$ is concatenated
    to the transfer weight $W$ to support the new category. And this initialization
    method is called Emphasis Initialization.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathfrak{w}_{n}$ 是已知 $n$ 类别的权重列。此外，由于相似类别在 $\mathfrak{w}_{m}$ 的初始化中应发挥更关键的作用，ODN
    添加了另一个项 $\frac{1}{M}\sum_{m=1}^{M}\mathfrak{w}_{m}$ 来强调相似的已知类别。$\mathfrak{w}_{m}$
    是 $M$ 个最高激活值的权重列。$\mathfrak{w}_{N+1}$ 被连接到转移权重 $W$ 上以支持新类别。这个初始化方法被称为强调初始化。
- en: 'ODN also introduces multi-class triplet thresholds to identify new categories:
    accept threshold, reject threshold and distance-threshold. Specifically, a sample
    would be accepted as a labeled class if and only if the index of its top confidence
    value is greater than the acceptable threshold. A sample would be considered as
    unknown if all the confidence values are below the rejected threshold. For samples
    between accept threshold and reject threshold, they would also be accepted as
    a labeled class if the distance between top and second maximal confidence values
    is large than the distance-threshold.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ODN 还引入了多类三元组阈值来识别新类别：接受阈值、拒绝阈值和距离阈值。具体来说，只有当样本的最高置信值的索引大于接受阈值时，该样本才会被接受为标记类别。如果所有置信值都低于拒绝阈值，则该样本会被视为未知。如果样本的置信值介于接受阈值和拒绝阈值之间，且最高和第二高置信值之间的距离大于距离阈值，则该样本也会被接受为标记类别。
- en: 3.1.4\. Outlier Exposure
  id: totrans-164
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.4\. 异常值暴露
- en: 'Hendrycks et al. ([2019](#bib.bib25)) proposed Outlier Exposure(OE) to distinguish
    between anomalous and in-distribution examples. OE borrowed data from other datasets
    to be “out-of-distribution” (OOD), denoted as $\mathcal{D}_{out}$. Meanwhile target
    samples as “in-distribution”, marked as $\mathcal{D}_{in}$. Then the model is
    trained to discover signals and learn heuristics to detect” which dataset a query
    is sampled from. Given a model $f$ and the original learning objective $\mathcal{L}$,
    the objective function of OE looks like:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Hendrycks 等人（[2019](#bib.bib25)）提出了 Outlier Exposure（OE），用于区分异常样本和分布内样本。OE 从其他数据集中借用数据作为“分布外”（OOD），记为
    $\mathcal{D}_{out}$。同时，将目标样本视为“分布内”，标记为 $\mathcal{D}_{in}$。然后，模型被训练以发现信号并学习启发式方法来检测查询样本来自哪个数据集。给定模型
    $f$ 和原始学习目标 $\mathcal{L}$，OE 的目标函数如下：
- en: '| (12) |  | $\mathbb{E}_{(x,y)\sim\mathcal{D}_{in}}[\mathcal{L}(f(x),y)+\lambda\mathbb{E}_{x^{\prime}\sim\mathcal{D}_{in}^{out}}[\mathcal{L}_{OE}(f(x^{\prime}),f(x),y)]]$
    |  |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $\mathbb{E}_{(x,y)\sim\mathcal{D}_{in}}[\mathcal{L}(f(x),y)+\lambda\mathbb{E}_{x^{\prime}\sim\mathcal{D}_{in}^{out}}[\mathcal{L}_{OE}(f(x^{\prime}),f(x),y)]]$
    |  |'
- en: $\mathcal{D}_{out}^{OE}$ is an outlier exposure dataset. The equation indicates
    the model tries to minimize the objective L for data from “in-distribution” ($\mathcal{L}$)
    and “out-of-distribution” ($\mathcal{L}_{OE}$). The paper also used the maximum
    softmax probability baseline detector (cross-entropy) for $\mathcal{L}_{OE}$.
    And when labels are not available, $\mathcal{L}_{OE}$ was set to a margin ranking
    loss on the log probabilities $f(x^{\prime})$ and $f(x)$. However, the performance
    of this method depends on the chosen OOD dataset.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: $\mathcal{D}_{out}^{OE}$ 是一个异常值暴露数据集。该方程表明，模型尝试最小化来自“分布内”（$\mathcal{L}$）和“分布外”（$\mathcal{L}_{OE}$）的数据的目标
    L。论文还使用了最大 softmax 概率基线检测器（交叉熵）来处理 $\mathcal{L}_{OE}$。当标签不可用时，$\mathcal{L}_{OE}$
    被设置为 log 概率 $f(x^{\prime})$ 和 $f(x)$ 上的边际排名损失。然而，该方法的性能依赖于选择的 OOD 数据集。
- en: 3.1.5\. Objectosphere Loss
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.5\. Objectosphere 损失
- en: 'Dhamija et al. ([2018](#bib.bib12)) proposed Entropic Open-Set and Objectoshere
    losses for open set recognition, which trained networks using negative samples
    from some classes. The method reduced the deep feature magnitude and maximize
    entropy of the softmax scores of unknown sample to separate them from known samples.
    The idea of Entropic Open-Set is to maximum entropy when an input is unknown.
    Formally, let $S_{c}(x)$ denotes the softmax score of sample $x$ from known class
    $c$, the Entropic Open-Set Loss $J_{E}$ can be defined as:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Dhamija 等人（[2018](#bib.bib12)）提出了用于开放集识别的 Entropic Open-Set 和 Objectoshere 损失，该方法使用来自某些类别的负样本训练网络。该方法减少了深度特征的幅度，并最大化了未知样本的
    softmax 分数的熵，以将其与已知样本分开。Entropic Open-Set 的思想是当输入未知时最大化熵。形式上，令 $S_{c}(x)$ 表示来自已知类别
    $c$ 的样本 $x$ 的 softmax 分数，Entropic Open-Set 损失 $J_{E}$ 可以定义为：
- en: '| (13) |  | $J_{E}(x)=\begin{cases}-\log S_{c}(x)&amp;\text{if $x\in\mathcal{D}^{\prime}_{c}$
    is from class $c$}\\ -\frac{1}{C}\sum_{c=1}^{C}\log S_{c}(x)&amp;\text{if $x\in\mathcal{D}^{\prime}_{b}$
    is from class $c$}\end{cases}$ |  |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $J_{E}(x)=\begin{cases}-\log S_{c}(x)&amp;\text{如果 $x\in\mathcal{D}^{\prime}_{c}$
    属于类别 $c$}\\ -\frac{1}{C}\sum_{c=1}^{C}\log S_{c}(x)&amp;\text{如果 $x\in\mathcal{D}^{\prime}_{b}$
    属于类别 $c$}\end{cases}$ |  |'
- en: 'where $\mathcal{D}^{\prime}_{b}$ denotes out of distribution samples. To further
    separate known and unknown samples, the paper pushed known samples into the “objectosphere”
    where they have large feature magnitude and low entropy, so-called Objectosphere
    Loss, which is calculated as:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{D}^{\prime}_{b}$ 表示分布外样本。为了进一步区分已知和未知样本，论文将已知样本推入“对象球体”，在那里它们具有大的特征幅度和低熵，即所谓的
    Objectosphere Loss，其计算方法为：
- en: '| (14) |  | $J_{R}=J_{E}+\lambda\begin{cases}\max(\xi-\&#124;F(x)\&#124;,0)^{2}&amp;\text{if
    $x\in\mathcal{D}^{\prime}_{c}$}\\ \&#124;F(x)\&#124;^{2}&amp;\text{if $x\in\mathcal{D}^{\prime}_{b}$}\end{cases}$
    |  |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $J_{R}=J_{E}+\lambda\begin{cases}\max(\xi-\&#124;F(x)\&#124;,0)^{2}&amp;\text{如果
    $x\in\mathcal{D}^{\prime}_{c}$}\\ \&#124;F(x)\&#124;^{2}&amp;\text{如果 $x\in\mathcal{D}^{\prime}_{b}$}\end{cases}$
    |  |'
- en: where $F(x)$ is the deep feature vector, and Objectosphere Loss penalizes the
    known classes if their feature magnitude is inside the boundary of the Objectosphere
    $\xi$ and unknown classes if their magnitude is greater than zero.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $F(x)$ 是深度特征向量，而 Objectosphere Loss 会对已知类别进行惩罚，如果它们的特征幅度在 Objectosphere $\xi$
    的边界内，对未知类别进行惩罚，如果它们的幅度大于零。
- en: 3.1.6\. DOC
  id: totrans-174
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.6\. DOC
- en: 'Perera and Patel ([2019](#bib.bib40)) proposed a deep learning-based solution
    for one-class classification (DOC) feature extraction. The objective of one-class
    classification is to recognize normal class and abnormal class using only samples
    from normal class and there are different strategies to solve a classification
    problem. The proposed accept two inputs: one from the target dataset, one from
    the reference dataset, and produces two losses through a pre-trained reference
    network and a secondary network. The reference dataset is the dataset used to
    train the reference network, and the target dataset contains samples of the class
    for which one-class learning is used for. During training, two image batches,
    each from the reference dataset and the target dataset are simultaneously fed
    into the input layers of the reference network and secondary network. At the end
    of the forward pass, the reference network generates a descriptiveness loss ($l_{D}$),
    which is the same as the cross-entropy loss, and the secondary network generates
    compactness loss ($l_{C}$). The composite loss ($l$) of the network is defined
    as:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Perera 和 Patel ([2019](#bib.bib40)) 提出了一个基于深度学习的一类分类 (DOC) 特征提取解决方案。一类分类的目标是仅使用正常类别的样本来识别正常类别和异常类别，并且有不同的策略来解决分类问题。提出的方法接受两个输入：一个来自目标数据集，一个来自参考数据集，并通过一个预训练的参考网络和一个次级网络产生两个损失。参考数据集是用来训练参考网络的数据集，目标数据集包含用于一类学习的类别样本。在训练过程中，来自参考数据集和目标数据集的两个图像批次同时输入到参考网络和次级网络的输入层。在前向传递结束时，参考网络生成描述性损失
    ($l_{D}$)，这与交叉熵损失相同，次级网络生成紧凑性损失 ($l_{C}$)。网络的复合损失 ($l$) 定义为：
- en: '| (15) |  | $l(r,t)=l_{D}(r&#124;W)+\lambda l_{C}(t&#124;W),$ |  |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| (15) |  | $l(r,t)=l_{D}(r&#124;W)+\lambda l_{C}(t&#124;W),$ |  |'
- en: where $r$ and $t$ are the training data from reference dataset and target dataset
    respectively, $W$ is the shared weights of both networks, and $\lambda$ is a constant.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $r$ 和 $t$ 分别是参考数据集和目标数据集中的训练数据，$W$ 是两个网络的共享权重，$\lambda$ 是一个常数。
- en: 'The overview of the proposed method looks like figure [10](#S3.F10 "Figure
    10 ‣ 3.1.6\. DOC ‣ 3.1\. Borrowing additional data ‣ 3\. Open Set Recognition
    ‣ Deep Learning and Open Set Malware Classification: A Survey"), where $g$ is
    feature extraction networks and $h_{c}$ is classification networks. The compactness
    loss is to assess the compactness of the class under consideration in the learned
    feature space. The descriptiveness loss was assessed by an external multi-class
    dataset.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的算法概览如图 [10](#S3.F10 "图 10 ‣ 3.1.6\. DOC ‣ 3.1\. 借用额外数据 ‣ 3\. 开放集识别 ‣ 深度学习与开放集恶意软件分类：综述")
    所示，其中 $g$ 是特征提取网络，$h_{c}$ 是分类网络。紧凑性损失用于评估在学习的特征空间中考虑类别的紧凑性。描述性损失是通过外部多类数据集进行评估的。
- en: '![Refer to caption](img/58396cf57701c6cfe934790869a29582.png)\Description'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/58396cf57701c6cfe934790869a29582.png)\描述'
- en: OC
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: OC
- en: Figure 10\. Overview of the Deep Feature for One-Class Classification framework
    ([Perera and Patel](#bib.bib40), ([2019](#bib.bib40)))
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10\. 一类分类框架的深度特征概述 ([Perera and Patel](#bib.bib40)，([2019](#bib.bib40)))
- en: Discussion
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论
- en: All the above methods borrow some dataset as unknown class during training,
    (Shu et al., [2018b](#bib.bib49)) borrows target samples (test set) as unknown
    classes and utilizes adversarial learning. A classifier is trained to make a boundary
    between the source and the target samples whereas a generator is trained to make
    target samples far from the boundary. (Saito et al., [2018](#bib.bib46)) also
    borrows unlabeled examples from the dataset, then used and Auto-encoder to learn
    the representations. (Shu et al., [2018a](#bib.bib50)) uses several manually annotations
    during Emphasis Initialization. (Hendrycks et al., [2019](#bib.bib25)) introduced
    outlier exposure datasets on top of in-distribution datasets. (Dhamija et al.,
    [2018](#bib.bib12)) also introduces unknown datasets, meanwhile utilizes the differences
    of feature magnitudes between known and unknown samples as part of the objective
    function. Different from the multi-class classification problems, (Perera and
    Patel, [2019](#bib.bib40)) presents a one-class classification problem from anomaly
    detection, with additional reference dataset for transfer learning. In general,
    borrowing and annotating additional data s OSR an easier problem. However, the
    retrieval and selection of additional datasets would be another problem.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 所有上述方法在训练过程中借用了某些数据集作为未知类别，（Shu 等人，[2018b](#bib.bib49)）借用了目标样本（测试集）作为未知类别，并利用对抗学习。一个分类器被训练以在源样本和目标样本之间做出边界，而生成器被训练以使目标样本远离边界。（Saito
    等人，[2018](#bib.bib46)）也从数据集中借用了未标记的示例，然后使用自编码器来学习表示。（Shu 等人，[2018a](#bib.bib50)）在强调初始化期间使用了多个手动注释。（Hendrycks
    等人，[2019](#bib.bib25)）在分布内数据集的基础上引入了离群点暴露数据集。（Dhamija 等人，[2018](#bib.bib12)）也引入了未知数据集，同时利用已知样本和未知样本之间的特征幅度差异作为目标函数的一部分。不同于多类分类问题，（Perera
    和 Patel，[2019](#bib.bib40)）从异常检测中提出了一类分类问题，并提供了用于迁移学习的额外参考数据集。一般来说，借用和注释额外数据使
    OSR 成为一个更简单的问题。然而，额外数据集的检索和选择将是另一个问题。
- en: 3.2\. Generating additional data
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 生成额外数据
- en: As adversarial learning has achieved great access such as GANs, there are ideas
    use GANs generating unknown samples before training.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对抗学习（如 GANs）取得的巨大成功，出现了一些想法，使用 GAN 在训练前生成未知样本。
- en: 3.2.1\. G-OpenMax
  id: totrans-186
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. G-OpenMax
- en: 'Ge et al. ([2017](#bib.bib14)) designed a networks based on OpenMax and GANs.
    Their approach provided explicit modeling and decision score for novel category
    image synthesis. The method proposed has two stages as well as OpenMax: pre-Network
    training and score calibration. During the pre-Network training stage, different
    with OpenMax, it first generates some unknown class samples (synthetic samples)
    then sends them along with known samples into networks for training. A modified
    conditional GAN is employed in G-OpenMax to sythesize unknown classes. In conditional
    GAN, random noise is fed to the generator $G$ with a one-hot vector $c\in c_{i,...,k}$,
    which represents a desired class. Meanwhile, the discriminator $D$ learns faster
    if the input image is supplied together with the class it belongs to. Thus, the
    optimization of a conditional GAN with class labels can be formulated as:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Ge 等人 ([2017](#bib.bib14)) 设计了基于 OpenMax 和 GAN 的网络。他们的方法提供了新类别图像合成的明确建模和决策评分。所提方法包括两个阶段以及
    OpenMax：预网络训练和评分校准。在预网络训练阶段，与 OpenMax 不同的是，它首先生成一些未知类别样本（合成样本），然后将这些样本与已知样本一起输入网络进行训练。在
    G-OpenMax 中使用了修改版的条件 GAN 来合成未知类别。在条件 GAN 中，随机噪声被输入生成器 $G$ 和一个一热向量 $c\in c_{i,...,k}$，该向量代表一个期望的类别。同时，如果输入图像与其所属类别一起提供，鉴别器
    $D$ 学习得更快。因此，带有类别标签的条件 GAN 的优化可以表述为：
- en: '| (16) |  | $\displaystyle\min_{\phi}\max_{\theta}$ | $\displaystyle=E_{x,c\sim
    P_{data}}[\log D_{\theta}(x,c)]$ |  |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| (16) |  | $\displaystyle\min_{\phi}\max_{\theta}$ | $\displaystyle=E_{x,c\sim
    P_{data}}[\log D_{\theta}(x,c)]$ |  |'
- en: '|  |  | $\displaystyle+E_{z\sim P_{z},c\sim P_{c}}[\log(1-D_{\theta}(G_{\phi}(z,c),c))],$
    |  |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '|  |  | $\displaystyle+E_{z\sim P_{z},c\sim P_{c}}[\log(1-D_{\theta}(G_{\phi}(z,c),c))],$
    |  |'
- en: where $\phi$ and $\theta$ are trainable parameters for $G_{\phi}$ and $D_{\theta}$,
    the generator inputs $z$ and $c$ are the latent variables drawn from their prior
    distribution $P(z)$ and $P(c)$. For each generated sample, if the class with the
    highest value is different from the pre-trained classifier, it will be marked
    as ”unknown”. Finally, a final classifier provides an explicit estimated probability
    for generated unknown classes.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\phi$ 和 $\theta$ 是 $G_{\phi}$ 和 $D_{\theta}$ 的可训练参数，生成器的输入 $z$ 和 $c$ 是从它们的先验分布
    $P(z)$ 和 $P(c)$ 中抽取的潜在变量。对于每个生成的样本，如果具有最高值的类别与预训练分类器不同，则会被标记为“未知”。最后，最终分类器提供了对生成的未知类别的显式估计概率。
- en: 3.2.2\. Adversarial sample generation
  id: totrans-191
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 对抗样本生成
- en: 'Yu et al. ([2017](#bib.bib56)) proposed Adversarial Sample Generation (ASG)
    as a data augmentation technique for the OSR problem. The idea is to generate
    some points closed to but different from the training instances as unknown labels,
    then straightforward to train an open-category classifier to identify seen from
    unseen. Moreover, ASG also generates ”unknown” samples, which are close to ”known”
    samples. Different from the GANs min-max strategy, ASG generated samples based
    on distances and distributions, the generated unknown samples are:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Yu 等人 ([2017](#bib.bib56)) 提出了对抗样本生成 (ASG) 作为 OSR 问题的数据增强技术。其思想是生成一些接近但不同于训练实例的未知标签点，然后直接训练一个开放类别分类器，以区分已见和未见。此外，ASG
    还生成接近“已知”样本的“未知”样本。与 GANs 的最小-最大策略不同，ASG 生成的样本基于距离和分布，这些生成的未知样本为：
- en: (1)
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: close to the seen class data
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接近已见类别数据
- en: (2)
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: scattered around the known/unknown boundary
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在已知/未知边界周围散布
- en: 3.2.3\. Counterfactual image generation
  id: totrans-197
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 反事实图像生成
- en: 'Different from standard GANs, Neal et al. ([2018](#bib.bib37)) proposed a dataset
    augmentation technique called counterfactual image generation, which adopts an
    encoder-decoder architecture to generate synthetic images closed to the real image
    but not in any known classes, then take them as unknown class. The architecture
    consists of three components:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准 GANs 不同，Neal 等人 ([2018](#bib.bib37)) 提出了一个名为反事实图像生成的数据集增强技术，它采用编码器-解码器架构来生成接近真实图像但不属于任何已知类别的合成图像，然后将其视为未知类别。该架构包括三个组件：
- en: •
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'An encoder network: maps from images to a latent space.'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个编码器网络：将图像映射到潜在空间。
- en: •
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A generator: maps from latent space back to an image.'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个生成器：将潜在空间映射回图像。
- en: •
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'A discriminator: discriminates generated images from real images.'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个判别器：区分生成的图像和真实图像。
- en: 3.2.4\. GAN-MDFM
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. GAN-MDFM
- en: Jo et al. ([2018](#bib.bib26)) presented a new method to generate fake data
    for unknown unknowns. They proposed Marginal Denoising Autoencoder (MDAE) technique,
    which models the noise distribution of known classes in feature spaces with a
    margin is introduced to generate data similar to known classes but not the same
    ones. The model contains a classifier, a generator, and an autoencoder. The classifier
    calculated the entropy of membership probability instead of discriminating generated
    data from real data explicitly. Then, a threshold is used here to identify unknown
    classes. The generator modeled the distribution $m$ away from the known classes.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Jo 等人 ([2018](#bib.bib26)) 提出了生成未知未知数据的新方法。他们提出了边际去噪自编码器 (MDAE) 技术，该技术通过引入边际来建模已知类别在特征空间中的噪声分布，以生成与已知类别相似但不相同的数据。该模型包含一个分类器、一个生成器和一个自编码器。分类器计算成员概率的熵，而不是明确区分生成数据和真实数据。然后，使用阈值来识别未知类别。生成器对远离已知类别的分布
    $m$ 进行了建模。
- en: 3.2.5\. Confident classifier
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5\. 自信分类器
- en: In order to distinguish in-distribution and out-of-distribution samples, Lee
    et al. ([2018](#bib.bib30)) suggested two additional terms added to the original
    cross-entropy loss, where the first one (confident loss) forces out-of-distribution
    samples less confident by the classifier while the second one (adversarial generator)
    is for generating most effective training samples for the first one. Specifically,
    the proposed confident loss is to minimize the Kullback-Leibler (KL) divergence
    from the predictive distribution on out-of-distribution samples to the uniform
    one in order to achieve less confident predictions for samples from out-of-distribution.
    Meanwhile in- and out-of-distributions are expected to be more separable. Then,
    an adversarial generator is introduced to generate the most effective out-of-distribution
    samples. Unlike the original generative adversarial network(GAN), which generates
    samples similar to in-distribution samples, the proposed generator generates ”boundary”
    samples in the low-density area of in-distribution acting as out-of-distribution
    samples. Finally, a joint training scheme was designed to minimize both loss functions
    alternatively. Finally, the paper showed that the proposed GAN implicitly encourages
    training a more confident classifier.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 为了区分分布内样本和分布外样本，Lee et al. ([2018](#bib.bib30)) 提出了在原始交叉熵损失中添加两个额外项，其中第一个项（置信损失）使分布外样本对分类器的信心降低，而第二个项（对抗生成器）用于生成最有效的训练样本以供第一个项使用。具体而言，所提出的置信损失旨在最小化从分布外样本的预测分布到均匀分布的Kullback-Leibler（KL）散度，以实现对分布外样本的更低置信度预测。同时，期望分布内和分布外样本更加可分。然后，引入对抗生成器生成最有效的分布外样本。与原始生成对抗网络（GAN）不同，后者生成类似于分布内样本的样本，所提出的生成器生成分布内低密度区域中的“边界”样本，充当分布外样本。最后，设计了一个联合训练方案来交替最小化这两个损失函数。最终，论文显示所提出的GAN隐式地鼓励训练一个更有信心的分类器。
- en: Discussion
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论
- en: Instead of borrowing data from other datasets, generating additional data methods
    generate unknown samples from the knowns. Most data generation methods are based
    on GANs. (Ge et al., [2017](#bib.bib14)) introduces a conditional GAN to generate
    some unknown samples followed by OpenMax open set classifier. (Yu et al., [2017](#bib.bib56))
    also uses the min-max strategy from GANs, generating data around the decision
    boundary between known and unknown samples as unknown. (Neal et al., [2018](#bib.bib37))
    adds another encoder network to traditional GANs to map from images to a latent
    space. (Jo et al., [2018](#bib.bib26)) generates unknown samples by marginal denoising
    autoencoder that provided a target distribution which is $m$ away from the distribution
    of known samples. (Lee et al., [2018](#bib.bib30)) generates ”boundary” samples
    in the low-density area of in-distribution acting as unknown samples, and jointly
    trains confident classifier and adversarial generator to make both models improve
    each other. Generating unknown samples for the OSR problem has achieved great
    performance, meanwhile, it requires more complex network architectures.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 与其从其他数据集中借用数据，不如生成额外数据的方法从已知样本中生成未知样本。大多数数据生成方法基于GANs。（Ge et al., [2017](#bib.bib14)）引入了一种条件GAN来生成一些未知样本，随后由OpenMax开放集分类器处理。（Yu
    et al., [2017](#bib.bib56)）也使用了来自GANs的最小-最大策略，在已知和未知样本之间的决策边界周围生成数据作为未知样本。（Neal
    et al., [2018](#bib.bib37)）在传统的GANs中添加了另一个编码器网络，以将图像映射到潜在空间。（Jo et al., [2018](#bib.bib26)）通过边际去噪自编码器生成未知样本，该编码器提供了一个目标分布，该分布与已知样本的分布相差$m$。（Lee
    et al., [2018](#bib.bib30)）在分布内低密度区域生成“边界”样本，充当未知样本，并联合训练置信分类器和对抗生成器，使两个模型相互提升。生成未知样本以解决OSR问题已经取得了很好的性能，同时，它需要更复杂的网络架构。
- en: '![Refer to caption](img/5b6460b9a0915591b815facc955152c2.png)\Description'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![参考说明](img/5b6460b9a0915591b815facc955152c2.png)\描述'
- en: ii-loss
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ii-loss
- en: Figure 11\. Network Architecture of ii-loss ([Hassen and Chan](#bib.bib21),
    2018)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图11\. ii-loss的网络架构（[Hassen and Chan](#bib.bib21), 2018）
- en: 3.3\. No additional data
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 无需额外数据
- en: The OSR techniques not requiring additional data in training can be divided
    to DNN-based ((Bendale and Boult, [2016](#bib.bib5)) (Hassen and Chan, [2018a](#bib.bib21))
    (Mao et al., [2018](#bib.bib35)) (Zhang and Patel, [2016](#bib.bib57)) (Liang
    et al., [2018](#bib.bib32))) and traditional ML-based ((Júnior et al., [2017](#bib.bib27))).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要额外训练数据的OSR技术可以分为基于DNN的（(Bendale and Boult, [2016](#bib.bib5)) (Hassen and
    Chan, [2018a](#bib.bib21)) (Mao et al., [2018](#bib.bib35)) (Zhang and Patel,
    [2016](#bib.bib57)) (Liang et al., [2018](#bib.bib32))) 和传统的基于ML的（(Júnior et al.,
    [2017](#bib.bib27))）。
- en: 3.3.1\. Extreme Value Signatures
  id: totrans-216
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.1\. 极值签名
- en: Schultheiss et al. ([2017](#bib.bib47)) investigated class-specific activation
    patterns to leverage CNNs to novelty detection tasks. They introduced “extreme
    value signature”, which specifies which dimensions of deep neural activations
    have the largest value. They also assumed that a semantic category can be described
    by its signature. Thereby, a test example will be considered as novel if it is
    different from the extreme-value signatures of all known categories, They applied
    extreme value signatures on the top of existing models, which allow to “upgrade”
    arbitrary classification networks to jointly estimate novelty and class membership.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Schultheiss 等人 ([2017](#bib.bib47)) 研究了类别特定的激活模式，以利用 CNN 进行新颖性检测任务。他们引入了“极值签名”，该签名指定了深度神经激活中哪个维度的值最大。他们还假设一个语义类别可以通过其签名来描述。因此，如果测试样本与所有已知类别的极值签名不同，则将其视为新颖的。他们在现有模型的基础上应用了极值签名，从而允许“升级”任意分类网络，以共同估计新颖性和类别成员资格。
- en: 3.3.2\. OpenMax
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.2\. OpenMax
- en: Bendale and Boult ([2016](#bib.bib5)) proposed OpenMax which replaces the softmax
    layer in DNNs with an OpenMax layer, and the model estimates the probability of
    an input being from an unknown class. The model adopts the Extreme Value Theory
    (EVT) meta-recognition calibration in the penultimate layer of the networks. For
    each instance, the activation vector is revised to the sum of the product of its
    distance to the mean activation vectors (MAV) of each class. Further, it redistributes
    values of activation vector acting as activation for unknown class. Finally, the
    new redistributed activation vectors are used for computing the probabilities
    of both known and unknown classes.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Bendale 和 Boult ([2016](#bib.bib5)) 提出了 OpenMax，它用 OpenMax 层替代了深度神经网络中的 softmax
    层，模型估计输入属于未知类别的概率。该模型在网络的倒数第二层采用了极值理论（EVT）元识别校准。对于每个实例，激活向量被修正为其与每个类别的均值激活向量（MAV）距离的乘积之和。此外，它重新分配激活向量的值，将其作为未知类别的激活。最后，新的重新分配的激活向量用于计算已知类别和未知类别的概率。
- en: 3.3.3\. ii-loss
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.3\. ii-loss
- en: 'Hassen and Chan ([2018a](#bib.bib21)) proposed a distance-based loss function
    in DNNs in order to learn the representation for open set recognition. The idea
    is to maximize the distance between different classes (inter-class separation)
    and minimize distance of an instance from its class mean (intra-class spread).
    So that in the learned representation, instances from the same class are close
    to each other while those from different classes are further apart. More formally,
    let $\overrightarrow{z_{i}}$ be the the projection (embedding) of the input vector
    $\overrightarrow{x_{i}}$ of instance $i$. The intra class spread is measured as
    the average distance of instances from their class means:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: Hassen 和 Chan ([2018a](#bib.bib21)) 提出了基于距离的损失函数，用于学习开放集识别的表示。其思路是最大化不同类别之间的距离（类别间分离度），最小化实例与其类别均值之间的距离（类别内分布）。这样，在学习到的表示中，同一类别的实例彼此接近，而不同类别的实例则距离较远。更正式地，设
    $\overrightarrow{z_{i}}$ 为实例 $i$ 的输入向量 $\overrightarrow{x_{i}}$ 的投影（嵌入）。类别内分布度被测量为实例与其类别均值之间的平均距离：
- en: '| (17) |  | $intra\_spread=\frac{1}{N}\sum_{j=1}^{K}\sum_{i=1}^{C_{j}}\&#124;\overrightarrow{\mu_{j}}-\overrightarrow{z_{i}}\&#124;_{2}^{2},$
    |  |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| (17) |  | $intra\_spread=\frac{1}{N}\sum_{j=1}^{K}\sum_{i=1}^{C_{j}}\&#124;\overrightarrow{\mu_{j}}-\overrightarrow{z_{i}}\&#124;_{2}^{2},$
    |  |'
- en: 'where $|C_{j}|$ is the number of training instances in class $C_{j}$, $N$ is
    the number of training instances, and $\mu_{j}$ is the mean of class $C_{j}$.
    Meanwhile, the inter class separation is measured as the closest two class means
    among all the $K$ known classes:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $|C_{j}|$ 是类别 $C_{j}$ 中训练实例的数量，$N$ 是训练实例的数量，而 $\mu_{j}$ 是类别 $C_{j}$ 的均值。同时，类别间分离度被测量为所有
    $K$ 个已知类别中最接近的两个类别均值：
- en: '| (18) |  | $inter\_separation=\min_{\begin{subarray}{c}1\leq m\leq K\\ m+1\leq
    n\leq K\end{subarray}}\&#124;\overrightarrow{\mu_{m}}-\overrightarrow{\mu_{n}}\&#124;_{2}^{2}$
    |  |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| (18) |  | $inter\_separation=\min_{\begin{subarray}{c}1\leq m\leq K\\ m+1\leq
    n\leq K\end{subarray}}\&#124;\overrightarrow{\mu_{m}}-\overrightarrow{\mu_{n}}\&#124;_{2}^{2}$
    |  |'
- en: 'The proposed ii-loss minimizes the intra-class spread and maximizes inter-class
    separation:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的 ii-loss 最小化了类别内分布度并最大化了类别间分离度：
- en: '| (19) |  | $ii\_loss=intra\_spread-inter\_separation$ |  |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| (19) |  | $ii\_loss=intra\_spread-inter\_separation$ |  |'
- en: 'So that the distance between an instance and the closest known class mean can
    be used as a criterion of unknown class. i.e. if the distance above some threshold,
    the instance then be recognized as an unknown class. The network architecture
    as Figure [11](#S3.F11 "Figure 11 ‣ 3.2.5\. Confident classifier ‣ 3.2\. Generating
    additional data ‣ 3\. Open Set Recognition ‣ Deep Learning and Open Set Malware
    Classification: A Survey").'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '这样实例与最近已知类别均值之间的距离可以用作未知类别的标准。即，如果距离超过某个阈值，则该实例被识别为未知类别。网络结构如图 [11](#S3.F11
    "Figure 11 ‣ 3.2.5\. Confident classifier ‣ 3.2\. Generating additional data ‣
    3\. Open Set Recognition ‣ Deep Learning and Open Set Malware Classification:
    A Survey")。'
- en: 3.3.4\. Distribution networks
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.4\. Distribution networks
- en: Mao et al. ([2018](#bib.bib35)) assumed that through a certain mapping, all
    the classes followed different Gaussian distributions. They proposed a distributions
    parameters transfer strategy to detect and model the unknown classes through estimating
    those of known classes. Formally, let $\boldsymbol{z}_{i}^{k}$ denotes the embedding
    of $\boldsymbol{x}_{i}^{k}$, they assume samples from class $k$ follow a probability
    distribution $p_{k}(\boldsymbol{z};\boldsymbol{\Theta}_{k})$ with learnable parameters
    $\boldsymbol{\Theta}_{k}$ in the latent space. For class $k$, the log-likelihood
    is
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Mao 等人 ([2018](#bib.bib35)) 假设通过某种映射，所有类别遵循不同的高斯分布。他们提出了一种分布参数迁移策略，通过估计已知类别的参数来检测和建模未知类别。正式地，设
    $\boldsymbol{z}_{i}^{k}$ 表示 $\boldsymbol{x}_{i}^{k}$ 的嵌入，他们假设来自类别 $k$ 的样本在潜在空间中遵循一个带有可学习参数
    $\boldsymbol{\Theta}_{k}$ 的概率分布 $p_{k}(\boldsymbol{z};\boldsymbol{\Theta}_{k})$。对于类别
    $k$，对数似然为
- en: '| (20) |  | $\log\mathcal{L}_{k}(\boldsymbol{\Theta}_{k},\boldsymbol{W})=\sum_{i=1}^{n_{k}}\log
    p_{k}(\boldsymbol{z}_{i}^{k};\boldsymbol{\Theta}_{k})$ |  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| (20) |  | $\log\mathcal{L}_{k}(\boldsymbol{\Theta}_{k},\boldsymbol{W})=\sum_{i=1}^{n_{k}}\log
    p_{k}(\boldsymbol{z}_{i}^{k};\boldsymbol{\Theta}_{k})$ |  |'
- en: The training objective is to make samples more likely to belong to their labeled
    class. i.e, maximize the log-likelihood of each class with respect to their samples.
    Hence the negative mean log-likelihood is used as a loss function in the proposed
    distribution networks.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 训练目标是使样本更可能属于其标记类别。即，最大化每个类别相对于其样本的对数似然。因此，负均值对数似然被用作提议分布网络中的损失函数。
- en: '| (21) |  | $J(\boldsymbol{W},\boldsymbol{\Theta})=-\sum_{k=1}^{l}\frac{1}{n_{k}}\log\mathcal{L}_{k}(\boldsymbol{\Theta}_{k},\boldsymbol{W})$
    |  |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| (21) |  | $J(\boldsymbol{W},\boldsymbol{\Theta})=-\sum_{k=1}^{l}\frac{1}{n_{k}}\log\mathcal{L}_{k}(\boldsymbol{\Theta}_{k},\boldsymbol{W})$
    |  |'
- en: The method can not only detect novel samples but also differentiate and model
    unknown classes, hence discover new patterns or even new knowledge in the real
    world.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法不仅可以检测新样本，还可以区分和建模未知类别，从而发现现实世界中的新模式甚至新知识。
- en: 3.3.5\. OSNN
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.5\. OSNN
- en: Besides DNNs, Júnior et al. ([2017](#bib.bib27)) introduced OSNN as an extension
    for the traditional machine learning technique - Nearest Neighbors(NN) classifier.
    It applies the Nearest Neighbors Distance Ratio (NNDR) technique as a threshold
    on the ratio of similarity scores. Specifically, it measures the ratio of the
    distances between a sample and its nearest neighbors in two different known classes.
    And assign the sample to one of the class if the ratio is below a certain threshold.
    And samples who are ambiguous between classes (ratio above a certain threshold)
    and those faraway from any unknown class are classified as unknown.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 DNNs，Júnior 等人 ([2017](#bib.bib27)) 将 OSNN 作为传统机器学习技术 - 最近邻（NN）分类器的扩展。他们将最近邻距离比率（NNDR）技术应用于相似度评分的比率上。具体而言，它测量样本与两个不同已知类别的最近邻之间的距离比率。如果比率低于某个阈值，则将样本分配给其中一个类别。那些在类别之间模糊（比率高于某个阈值）和那些远离任何未知类别的样本被分类为未知。
- en: 3.3.6\. RLCN
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.6\. RLCN
- en: 'Wang et al. ([2018](#bib.bib53)) proposed a pairwise-constraint loss (PCL)
    function to achieve “intra-class compactness” and “inter-class separation” in
    order to address OSR problem. They also developed a two-channel co-representation
    framework to detect novel class over time. In addition to which, they added a
    Frobenius regularization term to avoid over-fitting. Their model also applied
    binary classification error(BCE) at the final output layer to form the entire
    loss function. Moreover, they applied temperature scaling and t distribution assumptions
    to find the optimal threshold, which requires fewer parameters. The two-channel
    co-representation framework looks like figure [12](#S3.F12 "Figure 12 ‣ 3.3.6\.
    RLCN ‣ 3.3\. No additional data ‣ 3\. Open Set Recognition ‣ Deep Learning and
    Open Set Malware Classification: A Survey").'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '王等人 ([2018](#bib.bib53)) 提出了一个成对约束损失（PCL）函数，以实现“类内紧凑性”和“类间分离”，以解决开放集识别（OSR）问题。他们还开发了一个双通道共同表示框架，以随时间检测新类别。此外，他们添加了一个
    Frobenius 正则化项来避免过拟合。其模型还在最终输出层应用了二元分类误差（BCE）来形成整个损失函数。此外，他们应用了温度缩放和 t 分布假设来找到最佳阈值，这需要更少的参数。双通道共同表示框架如图
    [12](#S3.F12 "Figure 12 ‣ 3.3.6\. RLCN ‣ 3.3\. No additional data ‣ 3\. Open Set
    Recognition ‣ Deep Learning and Open Set Malware Classification: A Survey") 所示。'
- en: '![Refer to caption](img/d0b72036faa288fb7dbfe7973fa60421.png)\Description'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/d0b72036faa288fb7dbfe7973fa60421.png)\描述'
- en: RLCN
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: RLCN
- en: Figure 12\. Overview of the RLCN Framework ([Wang et al.](#bib.bib53), ([2018](#bib.bib53)))
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12\. RLCN 框架概述 ([王等人](#bib.bib53), ([2018](#bib.bib53)))
- en: 3.3.7\. SROSR
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.7\. SROSR
- en: 'Zhang and Patel ([2016](#bib.bib57)) proposed a generalized Sparse Recognition
    based Classification (SRC) algorithm for open set recognition in. The algorithm
    uses class reconstruction errors for classification. It models the tail of those
    two error distributions using the statistical Extreme Value Theory (EVT), then
    simplifies the open set recognition problem into a set of hypothesis testing problems.
    Figure [13](#S3.F13 "Figure 13 ‣ 3.3.7\. SROSR ‣ 3.3\. No additional data ‣ 3\.
    Open Set Recognition ‣ Deep Learning and Open Set Malware Classification: A Survey")
    gives an overview of the proposed SROSR algorithm.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '张和帕特尔 ([2016](#bib.bib57)) 提出了一个通用的稀疏识别分类（SRC）算法，用于开放集识别。该算法使用类重建误差进行分类。它利用统计极值理论（EVT）对这两个误差分布的尾部进行建模，然后将开放集识别问题简化为一组假设检验问题。图
    [13](#S3.F13 "Figure 13 ‣ 3.3.7\. SROSR ‣ 3.3\. No additional data ‣ 3\. Open
    Set Recognition ‣ Deep Learning and Open Set Malware Classification: A Survey")
    概述了所提出的 SROSR 算法。'
- en: '![Refer to caption](img/a689915c1037bfe1ab4069b1d302b778.png)\Description'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/a689915c1037bfe1ab4069b1d302b778.png)\描述'
- en: SROSR
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: SROSR
- en: Figure 13\. Overview of the SROSR framework ([Zhang and Patel](#bib.bib57),
    ([2016](#bib.bib57)))
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13\. SROSR 框架概述 ([张和帕特尔](#bib.bib57), ([2016](#bib.bib57)))
- en: The algorithm consists of two main stages. In the first stage, given training
    samples, SROSR models tail part of the matched reconstruction error distribution
    and the sum of non-matched reconstruction error using the EVT. In the second stage,
    the modeled distributions and the matched and the non-matched reconstruction errors
    are used to calculate the confidence scores for test samples. Then these scores
    are fused to obtain the final score for recognition.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法包括两个主要阶段。在第一个阶段，给定训练样本，SROSR 模型使用 EVT 对匹配重建误差分布的尾部和非匹配重建误差的总和进行建模。在第二个阶段，使用建模的分布以及匹配和非匹配重建误差来计算测试样本的置信度分数。然后，将这些分数融合以获得最终的识别分数。
- en: 3.3.8\. ODIN
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.8\. ODIN
- en: 'Liang et al. ([2018](#bib.bib32)) proposed ODIN, an out-of-distribution detector,
    for solving the problem of detecting out-of-distribution images in neural networks.
    The proposed method does not require any change to a pre-trained neural network.
    The detector is based on two components: temperature scaling and input pre-processing.
    Specifically, ODIN set a temperature scaling parameter in original softmax output
    for each class like:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 梁等人 ([2018](#bib.bib32)) 提出了 ODIN，一种用于解决神经网络中检测分布外图像问题的检测器。该方法无需对预训练的神经网络进行任何更改。检测器基于两个组件：温度缩放和输入预处理。具体来说，ODIN
    在原始 softmax 输出中为每个类别设置了一个温度缩放参数，如下所示：
- en: '| (22) |  | $S_{i}(\mathbf{x};T)=\frac{\exp(f_{i}(\mathbf{x}/T))}{\sum_{j=1}^{N}\exp(f_{j}(\mathbf{x})/T)}$
    |  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| (22) |  | $S_{i}(\mathbf{x};T)=\frac{\exp(f_{i}(\mathbf{x}/T))}{\sum_{j=1}^{N}\exp(f_{j}(\mathbf{x})/T)}$
    |  |'
- en: ODIN used the maximum softmax probability softmax score, the temperature scaling
    can push the softmax scores of in- and out-of-distribution images further apart
    from each other, making the out-of-distribution images distinguishable. Meanwhile,
    small perturbations were added to the input during pre-processing to make in-
    distribution images and out-of-distribution images more separable.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ODIN 使用了最大 softmax 概率分数，温度缩放可以将分布内和分布外图像的 softmax 分数进一步拉开，使得分布外图像更加可区分。同时，在预处理过程中添加了小的扰动，以使分布内图像和分布外图像更易于分离。
- en: 3.3.9\. DOC
  id: totrans-251
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.3.9\. DOC
- en: 'To address open classification problem, [Shu et al.](#bib.bib48) proposed Deep
    Open Classification (DOC) method in (Shu et al., [2017](#bib.bib48)). DOC builds
    a multi-class classifier with a 1-vs-rest final layer of sigmoids rather than
    softmax to reduce the open space risk as Figure [14](#S3.F14 "Figure 14 ‣ 3.3.9\.
    DOC ‣ 3.3\. No additional data ‣ 3\. Open Set Recognition ‣ Deep Learning and
    Open Set Malware Classification: A Survey"). Specifically, the 1-vs-rest layer
    contains one sigmoid function for each class. And the objective function is the
    summation of all log loss of the sigmoid functions:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '为了解决开放分类问题，[Shu 等](#bib.bib48)在 (Shu et al., [2017](#bib.bib48)) 中提出了深度开放分类（DOC）方法。DOC
    构建了一个多类别分类器，其最终层采用 1 对多的 sigmoid 而非 softmax，以降低开放空间风险，如图 [14](#S3.F14 "Figure
    14 ‣ 3.3.9\. DOC ‣ 3.3\. No additional data ‣ 3\. Open Set Recognition ‣ Deep
    Learning and Open Set Malware Classification: A Survey") 所示。具体来说，1 对多层为每个类别包含一个
    sigmoid 函数。目标函数是所有 sigmoid 函数的对数损失的总和：'
- en: '| (23) |  | $Loss=\sum_{i=1}^{m}\sum_{j=1}^{n}-\mathbb{I}(y_{j}=l_{i})\log
    p(y_{j}=l_{i})-\mathbb{I}(y_{j}\neq l_{i})\log(1-p(y_{j}=l_{i}))$ |  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| (23) |  | $Loss=\sum_{i=1}^{m}\sum_{j=1}^{n}-\mathbb{I}(y_{j}=l_{i})\log
    p(y_{j}=l_{i})-\mathbb{I}(y_{j}\neq l_{i})\log(1-p(y_{j}=l_{i}))$ |  |'
- en: where $\mathbb{I}$ is the indicator function and $p(y_{j}=l_{i})=Sigmoid(d_{j,i})$
    is the probability output from $i$th sigmoid function ($i$th class) on the $j$th
    document’s $i$th dimension of $d$.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathbb{I}$ 是指示函数，$p(y_{j}=l_{i})=Sigmoid(d_{j,i})$ 是来自 $i$th sigmoid 函数（$i$
    类）在第 $j$ 个文档的第 $i$ 维 $d$ 的概率输出。
- en: '![Refer to caption](img/0578644e2cf74eca9427d51680a5d9b8.png)\Description'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/0578644e2cf74eca9427d51680a5d9b8.png)\描述'
- en: DOC
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: DOC
- en: Figure 14\. Overview of DOC framework ([Shu et al.](#bib.bib48), ([2017](#bib.bib48)))
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14\. DOC 框架概述 ([Shu 等](#bib.bib48), ([2017](#bib.bib48)))
- en: DOC also borrows the idea of outlier detection in statistics to reduce the open
    space risk further for rejection by tightening the decision boundaries of sigmoid
    functions with Gaussian fitting. It fits the predicted probability for all training
    data of each class, then estimates the standard deviation to find the classification
    thresholds for each different class.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: DOC 还借鉴了统计学中的离群点检测思想，通过对 sigmoid 函数进行高斯拟合来进一步减少开放空间风险。它为每个类别的所有训练数据拟合预测概率，然后估计标准差以找到每个不同类别的分类阈值。
- en: Discussion
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论
- en: 'The above papers manage to solve the OSR problems without additional datasets,
    and some of them adopt similar ideas as in Table [2](#S3.T2 "Table 2 ‣ 3.3.9\.
    DOC ‣ 3.3\. No additional data ‣ 3\. Open Set Recognition ‣ Deep Learning and
    Open Set Malware Classification: A Survey"). (Schultheiss et al., [2017](#bib.bib47)),
    (Bendale and Boult, [2016](#bib.bib5)) and (Zhang and Patel, [2016](#bib.bib57))
    utilize EVT to distinguish unknown class and known classes. (Hassen and Chan,
    [2018a](#bib.bib21)) and (Wang et al., [2018](#bib.bib53)) design different distance-based
    loss functions to achieve “intra-class compactness” and “inter-class separation”.
    Some technologies are applied in different ways: (Wang et al., [2018](#bib.bib53))
    uses temperature scaling to find the threshold of outliers, while (Liang et al.,
    [2018](#bib.bib32)) uses temperature scaling in softmax output. (Mao et al., [2018](#bib.bib35))
    assumes all the classes followed different Gaussian distributions, while (Shu
    et al., [2017](#bib.bib48)) tightens the decision boundaries of sigmoid functions
    with Gaussian fitting. In general, not using an additional dataset requires the
    networks generating more precise representations for known classes. Other than
    DNN, (Júnior et al., [2017](#bib.bib27)) introduces an extension for the Nearest
    Neighbors classifier.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '上述论文成功地解决了无需额外数据集的开放集识别问题，其中一些论文采用了与表[2](#S3.T2 "Table 2 ‣ 3.3.9\. DOC ‣ 3.3\.
    No additional data ‣ 3\. Open Set Recognition ‣ Deep Learning and Open Set Malware
    Classification: A Survey")相似的思路。 (Schultheiss等人，[2017](#bib.bib47))，(Bendale和Boult，[2016](#bib.bib5))
    和 (Zhang和Patel，[2016](#bib.bib57)) 利用EVT来区分未知类别和已知类别。 (Hassen和Chan，[2018a](#bib.bib21))
    和 (Wang等人，[2018](#bib.bib53)) 设计了不同的基于距离的损失函数，以实现“类内紧凑性”和“类间分离”。一些技术以不同的方式应用：(Wang等人，[2018](#bib.bib53))
    使用温度缩放来找到异常值的阈值，而 (Liang等人，[2018](#bib.bib32)) 在softmax输出中使用温度缩放。 (Mao等人，[2018](#bib.bib35))
    假设所有类别遵循不同的高斯分布，而 (Shu等人，[2017](#bib.bib48)) 通过高斯拟合来收紧sigmoid函数的决策边界。一般来说，不使用额外数据集要求网络为已知类别生成更精确的表示。除了DNN，(Júnior等人，[2017](#bib.bib27))
    为最近邻分类器引入了一个扩展。'
- en: Table 2\. Similarities and Differences of OSR Techniques without Additional
    Data
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 表2\. 无需额外数据的开放集识别技术的相似性和差异
- en: '| Ideas | (Schultheiss et al., [2017](#bib.bib47)) | (Bendale and Boult, [2016](#bib.bib5))
    | (Hassen and Chan, [2018a](#bib.bib21)) | (Mao et al., [2018](#bib.bib35)) |
    (Júnior et al., [2017](#bib.bib27)) | (Wang et al., [2018](#bib.bib53)) | (Zhang
    and Patel, [2016](#bib.bib57)) | (Liang et al., [2018](#bib.bib32)) | (Shu et al.,
    [2017](#bib.bib48)) |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 思路 | (Schultheiss等人，[2017](#bib.bib47)) | (Bendale和Boult，[2016](#bib.bib5))
    | (Hassen和Chan，[2018a](#bib.bib21)) | (Mao等人，[2018](#bib.bib35)) | (Júnior等人，[2017](#bib.bib27))
    | (Wang等人，[2018](#bib.bib53)) | (Zhang和Patel，[2016](#bib.bib57)) | (Liang等人，[2018](#bib.bib32))
    | (Shu等人，[2017](#bib.bib48)) |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| DNN | x | x | x | x |  | x | x | x | x |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| DNN | x | x | x | x |  | x | x | x | x |'
- en: '| EVT | x | x |  |  |  |  | x |  |  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| EVT | x | x |  |  |  |  | x |  |  |'
- en: '| SRC |  |  |  |  |  |  | x |  |  |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| SRC |  |  |  |  |  |  | x |  |  |'
- en: '| Distance-based activation vector |  | x |  |  |  |  |  |  |  |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 基于距离的激活向量 |  | x |  |  |  |  |  |  |  |'
- en: '| Distance-based loss function |  |  | x |  |  | x |  |  |  |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 基于距离的损失函数 |  |  | x |  |  | x |  |  |  |'
- en: '| Gaussian distribution |  |  |  | x |  |  |  |  | x |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 高斯分布 |  |  |  | x |  |  |  |  | x |'
- en: '| Temperature scaling |  |  |  |  |  | x |  | x |  |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 温度缩放 |  |  |  |  |  | x |  | x |  |'
- en: '| Input perturbations |  |  |  |  |  |  |  | x |  |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 输入扰动 |  |  |  |  |  |  |  | x |  |'
- en: '| 1-vs-rest |  |  |  |  |  |  |  |  | x |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 一对其余 |  |  |  |  |  |  |  |  | x |'
- en: '| Nearest neighbors |  |  |  |  | x |  |  |  |  |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 最近邻 |  |  |  |  | x |  |  |  |  |'
- en: 4\. Learning graph representation
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 学习图表示
- en: Hamilton et al. ([2017b](#bib.bib19)) provided a review of techniques in representation
    learning on graphs, which including matrix factorization-based methods, random-walk
    based algorithms ad graph network.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Hamilton等人 ([2017b](#bib.bib19)) 提供了有关图表示学习技术的综述，包括基于矩阵分解的方法、随机游走算法和图网络。
- en: The paper introduced methods for vertex embedding and subgraph embedding. The
    vertex embedding can be viewed as encoding nodes into a latent space from an encoder-decoder
    perspective. The goal of subgraph embedding is to encode a set of nodes and edges,
    which is a continuous vector representation.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这篇论文介绍了顶点嵌入和子图嵌入的方法。顶点嵌入可以被视为从编码器-解码器的角度将节点编码到潜在空间中。子图嵌入的目标是对一组节点和边进行编码，这是一个连续的向量表示。
- en: 4.1\. Vertex embedding
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 顶点嵌入
- en: 'Vertex embedding can be organized as an encoder-decoder framework. An encoder
    maps each node to a low-dimensional vector or embedding. And decoder decodes structural
    information about the graph from the learned embeddings. Adopting the encoder-decoder
    perspective, there are four methodological components for the various node embedding
    methods (Hamilton et al., [2017b](#bib.bib19)):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 顶点嵌入可以组织为编码器-解码器框架。编码器将每个节点映射到低维向量或嵌入。解码器从学习到的嵌入中解码图的结构信息。采用编码器-解码器视角，有四个方法论组件用于各种节点嵌入方法（Hamilton
    等，[2017b](#bib.bib19)）：
- en: •
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A pairwise similarity function, which measures the similarity between nodes
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个成对相似度函数，用于衡量节点之间的相似度
- en: •
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: An encoder function, which generates the node embeddings
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个编码器函数，用于生成节点嵌入
- en: •
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A decoder function, which reconstructs pairwise similarity values from the generated
    embeddings
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个解码器函数，用于从生成的嵌入中重建成对相似度值
- en: •
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: A loss function, which determines how the quality of the pairwise reconstructions
    is evaluated to train the model
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个损失函数，用于确定如何评估成对重建的质量以训练模型
- en: The majority of node embedding algorithm reply on shallow embedding, whose encoder
    function just maps nodes to vector embedding. However, these shallow embedding
    vectors have some drawbacks.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数节点嵌入算法依赖于浅层嵌入，其编码器函数仅将节点映射到向量嵌入。然而，这些浅层嵌入向量有一些缺点。
- en: •
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: No parameters are shared between nodes in the encoder, which makes it computationally
    inefficient.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在编码器中，节点之间没有共享参数，这使得计算效率低下。
- en: •
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Shallow embedding fails to leverage node attributes during encoding.
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 浅层嵌入在编码过程中未能利用节点属性。
- en: •
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Shallow embedding can only generate embeddings for nodes that were present during
    the training phase, cannot generate embeddings for previously unseen nodes without
    additional rounds of optimization.
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 浅层嵌入只能为训练阶段存在的节点生成嵌入，不能为以前未见过的节点生成嵌入，除非进行额外的优化。
- en: Recently, several deep neural network-based approaches have been proposed to
    address the above issues. They used autoencoders to compress information about
    a node’s local neighborhood.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，提出了几种基于深度神经网络的方法来解决上述问题。他们使用自编码器来压缩关于节点局部邻域的信息。
- en: 4.1.1\. GCN
  id: totrans-295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. GCN
- en: In the work of Graph Convolutional Networks (GCN) (Kipf and Welling, [2017](#bib.bib28)),
    [Kipf and Welling](#bib.bib28) presented encoded the graph structure directly
    using a neural network model and trained on a supervised target. The adjacency
    matrix of the graph will then allow the model to distribute gradient information
    from the supervised loss and will enable it to learn representations of nodes
    both with and without labels.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在图卷积网络（GCN）的工作中（Kipf 和 Welling，[2017](#bib.bib28)），[Kipf 和 Welling](#bib.bib28)
    提出了直接使用神经网络模型对图结构进行编码，并在监督目标上进行训练。图的邻接矩阵将使模型能够从监督损失中分配梯度信息，并使其能够学习有标签和无标签节点的表示。
- en: 'The paper first introduces a simple and well-behaved layer-wise propagation
    rule for neural network models which operate directly on graphs as:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 该论文首先介绍了一种简单且行为良好的逐层传播规则，用于直接在图上操作的神经网络模型，如下所示：
- en: '| (24) |  | $H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}\right)$
    |  |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| (24) |  | $H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}\right)$
    |  |'
- en: 'Where $A$ is the adjacency matrix of the undirected graph, $I_{N}$ is identity
    matrix. $\tilde{A}$ is the adjacency matrix with added self-connections with added
    self-connections. $\tilde{D}_{ii}=\sum_{j}\tilde{A}_{ij}$ and $W^{(l)}$ is a layer-specific
    trainable weight matrix. $\sigma(\cdot)$ denotes an activation function and $H^{l}\in\mathbf{R}^{N\times
    D}$ is the matrix of activation functions in the $l^{th}$ layer. Considering a
    two-layer GCN as semi-supervised node classificaiton example. The pre-processing
    step calculates $\hat{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$,
    then the forward model takes the simple form:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $A$ 是无向图的邻接矩阵，$I_{N}$ 是单位矩阵。$\tilde{A}$ 是添加了自连接的邻接矩阵。$\tilde{D}_{ii}=\sum_{j}\tilde{A}_{ij}$
    和 $W^{(l)}$ 是层特定的可训练权重矩阵。$\sigma(\cdot)$ 表示激活函数，$H^{l}\in\mathbf{R}^{N\times D}$
    是第 $l^{th}$ 层的激活函数矩阵。考虑一个两层 GCN 作为半监督节点分类示例。预处理步骤计算 $\hat{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$，然后前向模型取简单形式：
- en: '| (25) |  | $Z=f(X,A)=\text{softmax}(\hat{A}\text{ ReLU}(\hat{A}XW^{(0)})W^{(1)})$
    |  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| (25) |  | $Z=f(X,A)=\text{softmax}(\hat{A}\text{ ReLU}(\hat{A}XW^{(0)})W^{(1)})$
    |  |'
- en: 'Here, $W^{(0)}\in\mathbf{R}^{C\times H}$ is an input-to-hidden weight matrix
    for a hidden layer with $H$ feature maps. $W^{(1)}\in\mathbf{R}^{H\times F}$ is
    a hidden-to-output weight matrix. For semi-supervised multi-class classification,
    the cross entropy error is evaluated over all labeled examples:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，$W^{(0)}\in\mathbf{R}^{C\times H}$ 是隐藏层的输入到隐藏权重矩阵，$H$ 是特征图的数量。$W^{(1)}\in\mathbf{R}^{H\times
    F}$ 是隐藏到输出的权重矩阵。对于半监督多类分类，交叉熵误差在所有标记样本上进行评估：
- en: '| (26) |  | $\mathcal{L}=-\sum_{l\in\mathcal{Y}_{L}}\sum_{f=1}^{F}Y_{lf}\ln
    Z_{lf}$ |  |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| (26) |  | $\mathcal{L}=-\sum_{l\in\mathcal{Y}_{L}}\sum_{f=1}^{F}Y_{lf}\ln
    Z_{lf}$ |  |'
- en: where $\mathcal{Y}_{L}$ is the set of node indices that have labels.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{Y}_{L}$ 是具有标签的节点索引集。
- en: '![Refer to caption](img/7c17a63af33573989c0ac222818b834e.png)\Description'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '![参考说明](img/7c17a63af33573989c0ac222818b834e.png)\描述'
- en: graphSAGE
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: graphSAGE
- en: Figure 15\. Visual illustration of the GraphSAGE sample and aggregate approach
    ([Hamilton et al.](#bib.bib18), ([2017a](#bib.bib18)))
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15\. GraphSAGE样本和聚合方法的视觉说明 ([Hamilton et al.](#bib.bib18), ([2017a](#bib.bib18)))
- en: 4.1.2\. GraphSAGE
  id: totrans-307
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. GraphSAGE
- en: 'Hamilton et al. presented GraphSAGE in (Hamilton et al., [2017a](#bib.bib18)),
    a general inductive framework that leverages node feature information to efficiently
    generate node embeddings for previously unseen data. GraphSAGE can learn a function
    that generates embeddings by sampling and aggregating features from a node’s local
    neighborhood as Figure [15](#S4.F15 "Figure 15 ‣ 4.1.1\. GCN ‣ 4.1\. Vertex embedding
    ‣ 4\. Learning graph representation ‣ Deep Learning and Open Set Malware Classification:
    A Survey"). Instead of training individual embeddings for each node, a set of
    aggregator functions are learned to aggregate feature information from a node’s
    local neighborhood from a different number of hops away from a given node, for
    example, for aggregator function $k$ we have:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: Hamilton 等人 (Hamilton et al., [2017a](#bib.bib18)) 提出了 GraphSAGE，这是一种通用的归纳框架，利用节点特征信息来高效生成对以前未见数据的节点嵌入。GraphSAGE
    可以学习一个函数，通过从节点的局部邻域采样和聚合特征来生成嵌入，如图 [15](#S4.F15 "图 15 ‣ 4.1.1\. GCN ‣ 4.1\. 顶点嵌入
    ‣ 4\. 学习图表示 ‣ 深度学习和开放集恶意软件分类：综述")。GraphSAGE 不是为每个节点训练单独的嵌入，而是学习一组聚合函数，从距离给定节点不同跳数的局部邻域中聚合特征信息。例如，对于聚合函数
    $k$，我们有：
- en: '| (27) |  | $h_{\mathcal{N}(v)}^{k}\leftarrow\text{AGGREGATE}_{k}(\{h_{u}^{k-1},\forall
    u\in\mathcal{N}(v)\}),$ |  |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| (27) |  | $h_{\mathcal{N}(v)}^{k}\leftarrow\text{AGGREGATE}_{k}(\{h_{u}^{k-1},\forall
    u\in\mathcal{N}(v)\}),$ |  |'
- en: 'where $h$ is representation vector, $v$ is input node, $\mathcal{N}$ is neighborhood
    function. GraphSAGE then concatenates the node’s current representation, $h_{v}^{k-1}$,
    with the aggregated neighborhood vector. $h_{\mathcal{N}(v)}^{k-1}$, and this
    concatenated vector is fed through a fully connected layer with nonlinear activation
    function $\sigma$ like:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $h$ 是表示向量，$v$ 是输入节点，$\mathcal{N}$ 是邻域函数。GraphSAGE 然后将节点的当前表示 $h_{v}^{k-1}$
    与聚合的邻域向量 $h_{\mathcal{N}(v)}^{k-1}$ 进行连接，这个连接后的向量通过具有非线性激活函数 $\sigma$ 的全连接层：
- en: '| (28) |  | $h_{v}^{k}\leftarrow\sigma\left(\mathbf{W}^{k}\cdot\text{CONCAT}(h_{v}^{k-1},h_{\mathcal{N}(v)}^{k})\right)$
    |  |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| (28) |  | $h_{v}^{k}\leftarrow\sigma\left(\mathbf{W}^{k}\cdot\text{CONCAT}(h_{v}^{k-1},h_{\mathcal{N}(v)}^{k})\right)$
    |  |'
- en: The learned aggregation functions are then applied to the entire unseen nodes
    to generate embeddings during the test phase.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的聚合函数随后应用于整个未见节点，在测试阶段生成嵌入。
- en: 4.1.3\. LINE
  id: totrans-313
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3\. LINE
- en: 'Tang et al. proposed a method for Large-scale Information Network Embedding:
    Line in (Tang et al., [2015](#bib.bib52)), which is suitable for undirected, directed
    and/or weighted networks. The model optimizes an objective which preserves both
    the local and global network structures. The paper explores both first-order and
    second-order proximity between the vertices. Most existing graph embedding are
    designed to preserve first-order proximity, which is presented by observed links
    like vertex 6 and 7 in Figure [16](#S4.F16 "Figure 16 ‣ 4.1.3\. LINE ‣ 4.1\. Vertex
    embedding ‣ 4\. Learning graph representation ‣ Deep Learning and Open Set Malware
    Classification: A Survey"), the objective function to preserve first-order proximity
    looks like:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: Tang 等人 (Tang et al., [2015](#bib.bib52)) 提出了一种用于大规模信息网络嵌入的方法：LINE，这种方法适用于无向、有向和/或加权网络。该模型优化一个目标，既保留局部结构又保留全局结构。论文探索了顶点之间的一阶和二阶接近度。大多数现有的图嵌入方法设计用于保留一阶接近度，这由图
    [16](#S4.F16 "图 16 ‣ 4.1.3\. LINE ‣ 4.1\. 顶点嵌入 ‣ 4\. 学习图表示 ‣ 深度学习和开放集恶意软件分类：综述")
    中观察到的链接表示，如顶点 6 和 7，一阶接近度的目标函数如下：
- en: '| (29) |  | $O_{1}=-\sum_{(i,j)\in E}w_{ij}\log p_{1}(v_{i},v_{j}),$ |  |'
  id: totrans-315
  prefs: []
  type: TYPE_TB
  zh: '| (29) |  | $O_{1}=-\sum_{(i,j)\in E}w_{ij}\log p_{1}(v_{i},v_{j}),$ |  |'
- en: 'where $p_{1}$ the joint probability between two vetices and is only valid for
    undirected edge $(i,j)$. Besides, LINE explores the second-order proximity between
    the vertices, which is not determined through the observed tie strength but through
    the shared neighborhood structures of the vertices, such as vertex 5 and 6 should
    also be placed close as they share similar neighbors. In second-order proximity,
    each vertex is treated as a specific ”context” and vertices with similar distributions
    over the ”contexts” are assumed to be similar. To preserve the second-order proximity,
    LINE minimize the following objective function:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{1}$ 是两个顶点之间的联合概率，仅对无向边 $(i,j)$ 有效。此外，LINE 探索顶点之间的二阶接近性，这种接近性不是通过观察到的联系强度来确定的，而是通过顶点的共享邻域结构来确定，例如，顶点
    5 和 6 也应该放置得很近，因为它们共享相似的邻居。在二阶接近性中，每个顶点被视为一个特定的“上下文”，而具有相似“上下文”分布的顶点被认为是相似的。为了保持二阶接近性，LINE
    最小化以下目标函数：
- en: '| (30) |  | $O_{2}=-\sum_{(i,j)\in E}w_{ij}\log p_{2}(v_{j}&#124;v_{i}),$ |  |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '| (30) |  | $O_{2}=-\sum_{(i,j)\in E}w_{ij}\log p_{2}(v_{j}|v_{i}),$ |  |'
- en: where $p_{2}$ is defined as the probability of ”context” $v_{j}$ generated by
    vertex $v_{i}$ for each directed edge $(i,j)$.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $p_{2}$ 定义为顶点 $v_{i}$ 为每个有向边 $(i,j)$ 生成的“上下文” $v_{j}$ 的概率。
- en: The functions preserved first-order proximity and second-order proximity are
    trained separately and the embeddings trained by two methods are concatenated
    for each vertex.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 保留一阶接近性和二阶接近性的函数分别训练，通过两种方法训练的嵌入对每个顶点进行连接。
- en: '![Refer to caption](img/c4717a4a313663f17e30e6b0ecb80d52.png)\Description'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/c4717a4a313663f17e30e6b0ecb80d52.png)\说明'
- en: Line
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: Line
- en: Figure 16\. A toy example of information network ([Tang et al.](#bib.bib52),
    ([2015](#bib.bib52)))
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16\. 信息网络的玩具示例 ([Tang et al.](#bib.bib52), ([2015](#bib.bib52)))
- en: 4.1.4\. JK-Net
  id: totrans-323
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.4\. JK-Net
- en: In order to overcome the limitations of neighborhood aggregation schemes, Xu
    et. al proposed Jumping Knowledge (JK) Networks strategy in (Xu et al., [2018](#bib.bib55))
    that flexibly leverages different neighborhood ranges to enable better structure-aware
    representation for each node. This architecture selectively combines different
    aggregations at the last layer, i.e., the representations “jump” to the last layer.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服邻域聚合方案的局限性，Xu 等人在 (Xu et al., [2018](#bib.bib55)) 中提出了跳跃知识 (JK) 网络策略，该策略灵活地利用不同的邻域范围，为每个节点提供更好的结构感知表示。该架构在最后一层选择性地结合不同的聚合，即表示“跳跃”到最后一层。
- en: '![Refer to caption](img/8aaa08e70777c61632a48e91347041ab.png)\Description'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/8aaa08e70777c61632a48e91347041ab.png)\说明'
- en: JK-Net
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: JK-Net
- en: Figure 17\. Illustration of a 4-layer JK-Net. N.A. stands for neighborhood aggregation
    ([Xu et al.](#bib.bib55), ([2018](#bib.bib55)))
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17\. 4 层 JK-Net 的示意图。N.A. 代表邻域聚合 ([Xu et al.](#bib.bib55), ([2018](#bib.bib55)))
- en: 'The main idea of JK-Net is illustrated as Figure [17](#S4.F17 "Figure 17 ‣
    4.1.4\. JK-Net ‣ 4.1\. Vertex embedding ‣ 4\. Learning graph representation ‣
    Deep Learning and Open Set Malware Classification: A Survey"): as in common neighborhood
    aggregation networks, each layer increases the size of the influence distribution
    by aggregating neighborhoods from the previous layer. At the last layer, for each
    node, JK-Net selects from all of those intermediate representations (which “jump”
    to the last layer), potentially combining a few. If this is done independently
    for each node, then the model can adapt the effective neighborhood size for each
    node as needed, resulting in exactly the desired adaptivity. As a more general
    framework, JK-Net admits general layer-wise aggregation models and enable better
    structure-aware representations on graphs with complex structures.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: JK-Net 的主要思想如图 [17](#S4.F17 "图 17 ‣ 4.1.4\. JK-Net ‣ 4.1\. 顶点嵌入 ‣ 4\. 学习图表示
    ‣ 深度学习与开放集恶意软件分类：综述") 所示：与常见的邻域聚合网络类似，每一层通过聚合前一层的邻域来增加影响分布的范围。在最后一层，对于每个节点，JK-Net
    从所有这些中间表示中进行选择（这些表示“跳跃”到最后一层），并可能结合几个。如果对每个节点独立执行此操作，那么模型可以根据需要调整每个节点的有效邻域大小，从而实现精确的自适应性。作为一个更通用的框架，JK-Net
    接受通用的逐层聚合模型，并在具有复杂结构的图上提供更好的结构感知表示。
- en: '![Refer to caption](img/c878d05f3a357bae69807b43cf22f9aa.png)\Description'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '![参见说明](img/c878d05f3a357bae69807b43cf22f9aa.png)\说明'
- en: DNGR
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: DNGR
- en: 'Figure 18\. Main components of DNGR: random surfing, PPMI and SDAE ([Cao et al.](#bib.bib7),
    ([2016](#bib.bib7)))'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18\. DNGR 的主要组件：随机游走、PPMI 和 SDAE ([Cao et al.](#bib.bib7), ([2016](#bib.bib7)))
- en: 4.1.5\. DNGR
  id: totrans-332
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.5\. DNGR
- en: 'In (Cao et al., [2016](#bib.bib7)), Cao et al. adopted a random surfing model
    to capture graph structural information directly instead of using a sampling-based
    method. As illustrated in Figure [18](#S4.F18 "Figure 18 ‣ 4.1.4\. JK-Net ‣ 4.1\.
    Vertex embedding ‣ 4\. Learning graph representation ‣ Deep Learning and Open
    Set Malware Classification: A Survey"), the proposed DNGR model contains three
    major components: random surfing, calculation of PPMI matrix and feature reduction
    by SDAE. The random surfing model is motivated by the PageRank model and is used
    to capture graph structural information and generate a probabilistic co-occurrence
    matrix.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '在 (Cao 等，[2016](#bib.bib7)) 中，Cao 等人采用随机漫游模型直接捕捉图的结构信息，而不是使用基于采样的方法。如图 [18](#S4.F18
    "Figure 18 ‣ 4.1.4\. JK-Net ‣ 4.1\. Vertex embedding ‣ 4\. Learning graph representation
    ‣ Deep Learning and Open Set Malware Classification: A Survey") 所示，提出的 DNGR 模型包含三个主要组成部分：随机漫游、PPMI
    矩阵计算和通过 SDAE 的特征降维。随机漫游模型受 PageRank 模型的启发，用于捕捉图的结构信息并生成概率共现矩阵。'
- en: Random surfing first randomly orders the vertices in a graph and assume there
    is a transition matrix that captures the transition probabilities between different
    vertices. The proposed random surfing model allows contextual information to be
    weighted differently based on their distance to target. The generated co-occurrence
    matrix then used to calculate PPMI matrix (an improvement for pointwise mutual
    information PMI, details in (Levy and Goldberg, [2014](#bib.bib31))). Next, as
    high dimensional input data often contain redundant information and noise stacked
    denoising autoencoder (SDAE) is used to enhance the robustness of DNN, denoising
    autoencoder partially corrupt the input data before taking the training step.
    Specifically, it corrupts each input sample x randomly by assigning some of the
    entries in the vector to 0 with a certain probability.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 随机漫游首先随机排列图中的顶点，并假设存在一个捕捉不同顶点之间过渡概率的转移矩阵。提出的随机漫游模型允许根据与目标的距离对上下文信息赋予不同的权重。生成的共现矩阵随后用于计算
    PPMI 矩阵（点对点互信息 PMI 的改进，详见 (Levy 和 Goldberg，[2014](#bib.bib31)））。接下来，由于高维输入数据通常包含冗余信息和噪声，堆叠去噪自编码器（SDAE）用于增强
    DNN 的鲁棒性，去噪自编码器在进行训练步骤之前部分地破坏输入数据。具体来说，它通过以一定概率将向量中的部分条目分配为 0 来随机破坏每个输入样本 x。
- en: 4.2\. Subgraph embedding
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 子图嵌入
- en: The goal of embedding subgraphs is to encode a set of nodes and edges into a
    low-dimensional vector embedding. Representation learning on subgraphs is closely
    related to the design of graph kernels, which define a distance measure between
    subgraphs. According to (Hamilton et al., [2017b](#bib.bib19)), some subgraph
    embedding techniques use the convolutional neighborhood aggregation idea to generate
    embeddings for nodes then use additional modules to aggregate sets of node embeddings
    to subgraph, such as sum-based approaches, graph-coarsening approaches. Besides,
    there is some related work on ”graph neural networks” (GNN). Instead of aggregating
    information from neighbors, GNN uses backpropagation ”passing information” between
    nodes.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入子图的目标是将一组节点和边编码成低维向量嵌入。子图的表示学习与图核的设计密切相关，图核定义了子图之间的距离度量。根据 (Hamilton 等，[2017b](#bib.bib19))，一些子图嵌入技术使用卷积邻域聚合的思想生成节点的嵌入，然后使用额外的模块将一组节点嵌入聚合到子图中，例如基于和的方法、图粗化方法。此外，还有一些关于“图神经网络”（GNN）的相关工作。GNN
    不通过邻居信息进行聚合，而是使用反向传播“传递信息”在节点之间。
- en: 5\. Malware Classification
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 恶意软件分类
- en: 5.0.1\. FCG
  id: totrans-338
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.0.1\. FCG
- en: 'Hassen and Chan ([2017](#bib.bib20)) proposed a linear time function call graph
    representation (FCG) vector representation. It starts with an FCG extraction module,
    which is a directed graph representation of code where the vertices of the graph
    correspond to functions and the directed edges represent the caller-callee relation
    between the function nodes. This module takes disassembled malware binaries and
    extract FCG representations. Thus they presented the caller-callee relation between
    functions as directed, unweighted edges. The next module is the function clustering.
    The algorithm used minhash to approximate Jaccard Index, to cluster functions
    of the given graph. The following module is vector extraction. The algorithm extracted
    vector representation from an FCG labeled using the cluster-ids. The representation
    consists of two parts, vertex weight, and edge weight. The vertex weight specifies
    the number of vertices in each cluster for that FCG and the edge weight describes
    the number of times an edge is found from one cluster to another cluster. The
    example workflow looks like figure [19](#S5.F19 "Figure 19 ‣ 5.0.1\. FCG ‣ 5\.
    Malware Classification ‣ Deep Learning and Open Set Malware Classification: A
    Survey").'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: Hassen 和 Chan ([2017](#bib.bib20)) 提出了一个线性时间函数调用图表示（FCG）向量表示。它以 FCG 提取模块开始，该模块是代码的有向图表示，其中图的顶点对应于函数，定向边表示函数节点之间的调用-被调用关系。该模块处理反汇编的恶意软件二进制文件并提取
    FCG 表示。因此，他们将函数之间的调用-被调用关系呈现为有向、无权重的边。下一个模块是函数聚类。所用算法使用 minhash 来近似 Jaccard 指数，以对给定图的函数进行聚类。接下来的模块是向量提取。该算法从标记有聚类
    ID 的 FCG 中提取向量表示。该表示由两个部分组成：顶点权重和边权重。顶点权重指定了每个 FCG 中每个聚类的顶点数量，边权重描述了从一个聚类到另一个聚类的边出现的次数。示例工作流程如图
    [19](#S5.F19 "图 19 ‣ 5.0.1\. FCG ‣ 5\. 恶意软件分类 ‣ 深度学习和开放集恶意软件分类：综述") 所示。
- en: '![Refer to caption](img/5a47301c420802724296e6ca69de3d81.png)\Description'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: '![参考图示](img/5a47301c420802724296e6ca69de3d81.png)\描述'
- en: fcg
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: fcg
- en: Figure 19\. FCG Example ([Hassen and Chan](#bib.bib22), 2018)
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19\. FCG 示例 ([Hassen 和 Chan](#bib.bib22)，2018)
- en: 5.0.2\. COW, COW PC
  id: totrans-343
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.0.2\. COW，COW PC
- en: 'Based on the work in (Hassen and Chan, [2017](#bib.bib20)), Hassen and Chan
    ([2018b](#bib.bib22)) further introduced two new features: $P_{max}$, which is
    the maximum predicted class probability for one instance:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 基于（Hassen 和 Chan，[2017](#bib.bib20)）的工作，Hassen 和 Chan ([2018b](#bib.bib22))
    进一步介绍了两个新特征：$P_{max}$，即一个实例的最大预测类别概率：
- en: '| (31) |  | $P_{max}=\max_{c\in C^{k}}Pr(y_{i}=c&#124;\vec{x_{i}})$ |  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| (31) |  | $P_{max}=\max_{c\in C^{k}}Pr(y_{i}=c&#124;\vec{x_{i}})$ |  |'
- en: 'And the entropy for probability distribution over classes:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 类别概率分布的熵：
- en: '| (32) |  | $entropy(p)=-\sum_{j}^{&#124;C^{k}&#124;}p_{j}logp_{j}$ |  |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| (32) |  | $entropy(p)=-\sum_{j}^{&#124;C^{k}&#124;}p_{j}logp_{j}$ |  |'
- en: 'The paper also introduced two algorithms: Classification in an Open World (COW)
    and COW PC. Both consist of two classifiers: outlier detector $M_{outlier}$ and
    multi-class classifier. The difference is in COW, the outlier detector was trained
    by all the classes. And during testing, test data will go through outlier detector
    first, if it is recognized as not outlier, then it will be sent in a multi-class
    classifier. While COW PC has a class-specific outlier detector, i.e. each class
    has its own outlier detector. The test data will come through a multi-class classifier
    first, then will be sent into the corresponding outlier detector afterward.'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 论文还介绍了两种算法：开放世界分类（COW）和 COW PC。这两种算法都由两个分类器组成：异常值检测器 $M_{outlier}$ 和多类分类器。不同之处在于，COW
    中的异常值检测器是通过所有类别进行训练的。在测试过程中，测试数据会首先经过异常值检测器，如果被识别为非异常值，则会传递给多类分类器。而 COW PC 拥有特定于类别的异常值检测器，即每个类别都有其自己的异常值检测器。测试数据会首先通过多类分类器，然后再发送到对应的异常值检测器。
- en: 5.0.3\. Random projections
  id: totrans-349
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.0.3\. 随机投影
- en: Malware classifiers often use sparse binary features, and there can be hundreds
    of millions of potential features. In (Dahl et al., [2013](#bib.bib10)), Dahl
    et al. used random projections to reduce the dimensionality of the original input
    space of neural networks. They first extracted three types of features including
    null-terminated patterns observed in the process’ memory, tri-grams of system
    API calls, and distinct combinations of a single system API call and one input
    parameter, next performed feature selection, ended with generating over 179 thousand
    sparse binary features. To make the problem more manageable, they projected each
    input vector into a much lower dimensional space using a sparse project matrix
    with entries sample iid from a distribution over ${0,1,-1}$. Entries of 1 and
    -1 are equiprobable and $P(R_{ij}=0)=1-\frac{1}{\sqrt{d}}$, where $d$ is the original
    input dimensionality. The lower-dimensional data then serves as input to the neural
    network.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意软件分类器通常使用稀疏的二进制特征，可能有数亿个潜在特征。在(Dahl 等，[2013](#bib.bib10))中，Dahl 等使用随机投影来减少神经网络原始输入空间的维度。他们首先提取了三种类型的特征，包括在进程内存中观察到的以空字符终止的模式、系统
    API 调用的三元组，以及单个系统 API 调用和一个输入参数的不同组合，然后进行特征选择，最终生成了超过 179,000 个稀疏二进制特征。为了使问题更易处理，他们使用稀疏投影矩阵将每个输入向量投影到更低维空间，其中条目是从
    ${0,1,-1}$ 的分布中独立同分布抽样的。1 和 -1 的出现概率相等，且 $P(R_{ij}=0)=1-\frac{1}{\sqrt{d}}$，其中
    $d$ 是原始输入维度。然后，低维数据作为神经网络的输入。
- en: 6\. Conclusions
  id: totrans-351
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: We provide a brief introduction of several deep neural network structures, and
    an overview of existing OSR, a discussion on learning graph representation and
    malware classification in this survey. It can be seen that those topics are advancing
    and profiting from each other in different areas. Also, despite the achieved great
    success, there are still serious challenges and great potential for them.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简要介绍了几种深度神经网络结构，并概述了现有的开放集识别（OSR），讨论了图表示学习和恶意软件分类。可以看出，这些主题在不同领域中相互推动并获益。尽管取得了很大成功，但仍面临严重挑战，并具有巨大的潜力。
- en: References
  id: totrans-353
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1)
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1)
- en: DBL (2018) 2018. *6th International Conference on Learning Representations,
    ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.
    OpenReview.net. [https://openreview.net/group?id=ICLR.cc/2018/Conference](https://openreview.net/group?id=ICLR.cc/2018/Conference)
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBL (2018) 2018. *第六届学习表征国际会议，ICLR 2018，加拿大温哥华，2018年4月30日 - 5月3日，会议论文集*。OpenReview.net。
    [https://openreview.net/group?id=ICLR.cc/2018/Conference](https://openreview.net/group?id=ICLR.cc/2018/Conference)
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
    Neural Machine Translation by Jointly Learning to Align and Translate. In *3rd
    International Conference on Learning Representations, ICLR 2015, San Diego, CA,
    USA, May 7-9, 2015, Conference Track Proceedings*, Yoshua Bengio and Yann LeCun
    (Eds.). [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bahdanau 等 (2015) Dzmitry Bahdanau, Kyunghyun Cho 和 Yoshua Bengio. 2015. 通过联合学习对齐和翻译的神经机器翻译。在*第三届学习表征国际会议，ICLR
    2015，加州圣地亚哥，美国，2015年5月7-9日，会议论文集*中，Yoshua Bengio 和 Yann LeCun (编辑)。 [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)
- en: Bayer et al. (2006) Ulrich Bayer, Andreas Moser, Christopher Kruegel, and Engin
    Kirda. 2006. Dynamic analysis of malicious code. *Journal in Computer Virology*
    2, 1 (2006), 67–77.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bayer 等 (2006) Ulrich Bayer, Andreas Moser, Christopher Kruegel 和 Engin Kirda.
    2006. 恶意代码的动态分析。*计算机病毒学期刊* 2, 1 (2006), 67–77。
- en: Bendale and Boult (2016) Abhijit Bendale and Terrance E Boult. 2016. Towards
    open set deep networks. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 1563–1572.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bendale 和 Boult (2016) Abhijit Bendale 和 Terrance E Boult. 2016. 向开放集深度网络迈进。在*IEEE计算机视觉与模式识别会议论文集*中。1563–1572。
- en: 'Cao et al. (2015) Chunshui Cao, Xianming Liu, Yi Yang, Yinan Yu, Jiang Wang,
    Zilei Wang, Yongzhen Huang, Liang Wang, Chang Huang, Wei Xu, et al. 2015. Look
    and think twice: Capturing top-down visual attention with feedback convolutional
    neural networks. In *Proceedings of the IEEE International Conference on Computer
    Vision*. 2956–2964.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等 (2015) Chunshui Cao, Xianming Liu, Yi Yang, Yinan Yu, Jiang Wang, Zilei
    Wang, Yongzhen Huang, Liang Wang, Chang Huang, Wei Xu 等。2015. 看两次再想：通过反馈卷积神经网络捕捉自上而下的视觉注意力。在*IEEE国际计算机视觉会议论文集*中。2956–2964。
- en: Cao et al. (2016) Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural
    networks for learning graph representations. In *Thirtieth AAAI Conference on
    Artificial Intelligence*.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等 (2016) Shaosheng Cao, Wei Lu 和 Qiongkai Xu. 2016. 用于学习图表示的深度神经网络。在*第三十届AAAI人工智能会议*中。
- en: Chen (2016) Gang Chen. 2016. A gentle tutorial of recurrent neural network with
    error backpropagation. *arXiv preprint arXiv:1610.02583* (2016).
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen（2016）Gang Chen。2016。带有误差反向传播的递归神经网络简明教程。*arXiv预印本 arXiv:1610.02583*（2016）。
- en: Cho et al. (2014) Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
    Representations using RNN Encoder-Decoder for Statistical Machine Translation.
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT,
    a Special Interest Group of the ACL*, Alessandro Moschitti, Bo Pang, and Walter
    Daelemans (Eds.). ACL, 1724–1734. [https://www.aclweb.org/anthology/D14-1179/](https://www.aclweb.org/anthology/D14-1179/)
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho等（2014）Kyunghyun Cho、Bart van Merrienboer、Çaglar Gülçehre、Dzmitry Bahdanau、Fethi
    Bougares、Holger Schwenk和Yoshua Bengio。2014。使用RNN编码器-解码器进行统计机器翻译的短语表示学习。见于*2014年自然语言处理经验方法会议论文集，EMNLP
    2014，2014年10月25-29日，卡塔尔多哈，ACL特别兴趣组SIGDAT会议*，Alessandro Moschitti、Bo Pang和Walter
    Daelemans（编）。ACL，1724–1734。 [https://www.aclweb.org/anthology/D14-1179/](https://www.aclweb.org/anthology/D14-1179/)
- en: Dahl et al. (2013) George E Dahl, Jack W Stokes, Li Deng, and Dong Yu. 2013.
    Large-scale malware classification using random projections and neural networks.
    In *2013 IEEE International Conference on Acoustics, Speech and Signal Processing*.
    IEEE, 3422–3426.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dahl等（2013）George E Dahl、Jack W Stokes、Li Deng和Dong Yu。2013。使用随机投影和神经网络的大规模恶意软件分类。见于*2013年IEEE声学、语音和信号处理国际会议*。IEEE，3422–3426。
- en: 'Deng et al. (2019) Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.
    2019. Arcface: Additive angular margin loss for deep face recognition. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 4690–4699.'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Deng等（2019）Jiankang Deng、Jia Guo、Niannan Xue和Stefanos Zafeiriou。2019。Arcface：深度人脸识别的加性角度边际损失。见于*IEEE计算机视觉与模式识别会议论文集*。4690–4699。
- en: Dhamija et al. (2018) Akshay Raj Dhamija, Manuel Günther, and Terrance Boult.
    2018. Reducing network agnostophobia. In *Advances in Neural Information Processing
    Systems*. 9157–9168.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dhamija等（2018）Akshay Raj Dhamija、Manuel Günther和Terrance Boult。2018。减少网络不可知症。见于*神经信息处理系统进展*。9157–9168。
- en: Finn et al. (2017) Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *Proceedings of the 34th
    International Conference on Machine Learning-Volume 70*. JMLR. org, 1126–1135.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Finn等（2017）Chelsea Finn、Pieter Abbeel和Sergey Levine。2017。用于深度网络快速适应的模型无关元学习。见于*第34届国际机器学习大会论文集-第70卷*。JMLR.org，1126–1135。
- en: Ge et al. (2017) Zongyuan Ge, Sergey Demyanov, and Rahil Garnavi. 2017. Generative
    OpenMax for Multi-Class Open Set Classification. In *British Machine Vision Conference
    2017, BMVC 2017, London, UK, September 4-7, 2017*. BMVA Press. [https://www.dropbox.com/s/7hlycn200phni9j/0104.pdf?dl=1](https://www.dropbox.com/s/7hlycn200phni9j/0104.pdf?dl=1)
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ge等（2017）Zongyuan Ge、Sergey Demyanov和Rahil Garnavi。2017。用于多类开放集分类的生成OpenMax。见于*2017年英国机器视觉会议，BMVC
    2017，伦敦，英国，2017年9月4-7日*。BMVA Press。 [https://www.dropbox.com/s/7hlycn200phni9j/0104.pdf?dl=1](https://www.dropbox.com/s/7hlycn200phni9j/0104.pdf?dl=1)
- en: 'Geng et al. (2018) Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. 2018.
    Recent Advances in Open Set Recognition: A Survey. *arXiv preprint arXiv:1811.08581*
    (2018).'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geng等（2018）Chuanxing Geng、Sheng-jun Huang和Songcan Chen。2018。开放集识别的最新进展：一项综述。*arXiv预印本
    arXiv:1811.08581*（2018）。
- en: 'Gers et al. (1999) Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. 1999.
    Learning to forget: Continual prediction with LSTM. (1999).'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gers等（1999）Felix A Gers、Jürgen Schmidhuber和Fred Cummins。1999。学习遗忘：使用LSTM的持续预测。（1999）。
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. In *Advances in neural information processing systems*.
    2672–2680.
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow等（2014）Ian Goodfellow、Jean Pouget-Abadie、Mehdi Mirza、Bing Xu、David
    Warde-Farley、Sherjil Ozair、Aaron Courville和Yoshua Bengio。2014。生成对抗网络。见于*神经信息处理系统进展*。2672–2680。
- en: Hamilton et al. (2017a) Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017a.
    Inductive representation learning on large graphs. In *Advances in Neural Information
    Processing Systems*. 1024–1034.
  id: totrans-371
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton等（2017a）Will Hamilton、Zhitao Ying和Jure Leskovec。2017a。大规模图上的归纳表示学习。见于*神经信息处理系统进展*。1024–1034。
- en: 'Hamilton et al. (2017b) William L. Hamilton, Rex Ying, and Jure Leskovec. 2017b.
    Representation Learning on Graphs: Methods and Applications. *IEEE Data Eng. Bull.*
    40, 3 (2017), 52–74. [http://sites.computer.org/debull/A17sept/p52.pdf](http://sites.computer.org/debull/A17sept/p52.pdf)'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hamilton 等 (2017b) William L. Hamilton, Rex Ying 和 Jure Leskovec. 2017b. 图上的表示学习：方法与应用。*IEEE数据工程快报*
    40, 3 (2017), 52–74. [http://sites.computer.org/debull/A17sept/p52.pdf](http://sites.computer.org/debull/A17sept/p52.pdf)
- en: Hassen and Chan (2017) Mehadi Hassen and Philip K Chan. 2017. Scalable function
    call graph-based malware classification. In *Proceedings of the Seventh ACM on
    Conference on Data and Application Security and Privacy*. ACM, 239–248.
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassen 和 Chan (2017) Mehadi Hassen 和 Philip K Chan. 2017. 可扩展的基于函数调用图的恶意软件分类。在
    *第七届ACM数据与应用安全与隐私会议* 上。ACM, 239–248.
- en: Hassen and Chan (2018a) Mehadi Hassen and Philip K Chan. 2018a. Learning a neural-network-based
    representation for open set recognition. *arXiv preprint arXiv:1802.04365* (2018).
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassen 和 Chan (2018a) Mehadi Hassen 和 Philip K Chan. 2018a. 为开放集识别学习基于神经网络的表示。*arXiv预印本
    arXiv:1802.04365* (2018).
- en: 'Hassen and Chan (2018b) Mehadi Hassen and Philip K Chan. 2018b. Learning to
    Identify Known and Unknown Classes: A Case Study in Open World Malware Classification.
    In *The Thirty-First International Flairs Conference*.'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hassen 和 Chan (2018b) Mehadi Hassen 和 Philip K Chan. 2018b. 学习识别已知和未知类别：一个开放世界恶意软件分类的案例研究。在
    *第31届国际FLAIRS会议* 上。
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren 和 Jian Sun. 2016. 用于图像识别的深度残差学习。在
    *IEEE计算机视觉与模式识别会议* 上。770–778.
- en: He et al. (2018) Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai.
    2018. Triplet-center loss for multi-view 3d object retrieval. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 1945–1954.
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等 (2018) Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai 和 Xiang Bai. 2018.
    多视图三维物体检索的三重中心损失。在 *IEEE计算机视觉与模式识别会议* 上。1945–1954.
- en: Hendrycks et al. (2019) Dan Hendrycks, Mantas Mazeika, and Thomas G. Dietterich.
    2019. Deep Anomaly Detection with Outlier Exposure. In *7th International Conference
    on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*.
    OpenReview.net. [https://openreview.net/forum?id=HyxCxhRcY7](https://openreview.net/forum?id=HyxCxhRcY7)
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hendrycks 等 (2019) Dan Hendrycks, Mantas Mazeika 和 Thomas G. Dietterich. 2019.
    具有异常点暴露的深度异常检测。在 *第七届国际表示学习会议, ICLR 2019, 美国新奥尔良, 2019年5月6-9日* 上。OpenReview.net.
    [https://openreview.net/forum?id=HyxCxhRcY7](https://openreview.net/forum?id=HyxCxhRcY7)
- en: Jo et al. (2018) Inhyuk Jo, Jungtaek Kim, Hyohyeong Kang, Yong-Deok Kim, and
    Seungjin Choi. 2018. Open Set Recognition by Regularising Classifier with Fake
    Data Generated by Generative Adversarial Networks. In *2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2686–2690.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jo 等 (2018) Inhyuk Jo, Jungtaek Kim, Hyohyeong Kang, Yong-Deok Kim 和 Seungjin
    Choi. 2018. 通过生成对抗网络生成的假数据来规范化分类器的开放集识别。在 *2018 IEEE国际声学、语音和信号处理会议 (ICASSP)* 上。IEEE,
    2686–2690.
- en: Júnior et al. (2017) Pedro R Mendes Júnior, Roberto M de Souza, Rafael de O
    Werneck, Bernardo V Stein, Daniel V Pazinato, Waldir R de Almeida, Otávio AB Penatti,
    Ricardo da S Torres, and Anderson Rocha. 2017. Nearest neighbors distance ratio
    open-set classifier. *Machine Learning* 106, 3 (2017), 359–386.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Júnior 等 (2017) Pedro R Mendes Júnior, Roberto M de Souza, Rafael de O Werneck,
    Bernardo V Stein, Daniel V Pazinato, Waldir R de Almeida, Otávio AB Penatti, Ricardo
    da S Torres 和 Anderson Rocha. 2017. 最近邻距离比开放集分类器。*机器学习* 106, 3 (2017), 359–386.
- en: Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised
    Classification with Graph Convolutional Networks. In *5th International Conference
    on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
    Track Proceedings*. OpenReview.net. [https://openreview.net/forum?id=SJU4ayYgl](https://openreview.net/forum?id=SJU4ayYgl)
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kipf 和 Welling (2017) Thomas N. Kipf 和 Max Welling. 2017. 使用图卷积网络的半监督分类。在 *第五届国际表示学习会议,
    ICLR 2017, 法国图伦, 2017年4月24-26日, 会议论文集* 上。OpenReview.net. [https://openreview.net/forum?id=SJU4ayYgl](https://openreview.net/forum?id=SJU4ayYgl)
- en: LeCun et al. (2015) Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep
    learning. *nature* 521, 7553 (2015), 436–444.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等 (2015) Yann LeCun, Yoshua Bengio 和 Geoffrey Hinton. 2015. 深度学习。*nature*
    521, 7553 (2015), 436–444.
- en: Lee et al. (2018) Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. 2018.
    Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples,
    See DBL ([2018](#bib.bib2)). [https://openreview.net/forum?id=ryiAv2xAZ](https://openreview.net/forum?id=ryiAv2xAZ)
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lee等（2018）Kimin Lee, Honglak Lee, Kibok Lee, 和 Jinwoo Shin。2018年。用于检测分布外样本的信心校准分类器训练，见DBL（[2018](#bib.bib2)）。[https://openreview.net/forum?id=ryiAv2xAZ](https://openreview.net/forum?id=ryiAv2xAZ)
- en: Levy and Goldberg (2014) Omer Levy and Yoav Goldberg. 2014. Neural word embedding
    as implicit matrix factorization. In *Advances in neural information processing
    systems*. 2177–2185.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Levy和Goldberg（2014）Omer Levy 和 Yoav Goldberg。2014年。作为隐式矩阵分解的神经词嵌入。在*神经信息处理系统进展*。2177–2185。
- en: Liang et al. (2018) Shiyu Liang, Yixuan Li, and R. Srikant. 2018. Enhancing
    The Reliability of Out-of-distribution Image Detection in Neural Networks, See
    DBL ([2018](#bib.bib2)). [https://openreview.net/forum?id=H1VGkIxRZ](https://openreview.net/forum?id=H1VGkIxRZ)
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang等（2018）Shiyu Liang, Yixuan Li, 和 R. Srikant。2018年。增强神经网络中分布外图像检测的可靠性，见DBL（[2018](#bib.bib2)）。[https://openreview.net/forum?id=H1VGkIxRZ](https://openreview.net/forum?id=H1VGkIxRZ)
- en: Luong et al. (2015) Thang Luong, Hieu Pham, and Christopher D. Manning. 2015.
    Effective Approaches to Attention-based Neural Machine Translation. In *Proceedings
    of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2015, Lisbon, Portugal, September 17-21, 2015*, Lluís Màrquez, Chris Callison-Burch,
    Jian Su, Daniele Pighin, and Yuval Marton (Eds.). The Association for Computational
    Linguistics, 1412–1421. [https://www.aclweb.org/anthology/D15-1166/](https://www.aclweb.org/anthology/D15-1166/)
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luong等（2015）Thang Luong, Hieu Pham, 和 Christopher D. Manning。2015年。基于注意力的神经机器翻译的有效方法。在*2015年自然语言处理经验方法会议（EMNLP
    2015），葡萄牙里斯本，2015年9月17-21日*，Lluís Màrquez, Chris Callison-Burch, Jian Su, Daniele
    Pighin, 和 Yuval Marton（编）。计算语言学协会，1412–1421。[https://www.aclweb.org/anthology/D15-1166/](https://www.aclweb.org/anthology/D15-1166/)
- en: Makhzani et al. (2015) Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian
    Goodfellow, and Brendan Frey. 2015. Adversarial autoencoders. *arXiv preprint
    arXiv:1511.05644* (2015).
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Makhzani等（2015）Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian Goodfellow,
    和 Brendan Frey。2015年。对抗自编码器。*arXiv预印本 arXiv:1511.05644*（2015）。
- en: Mao et al. (2018) Chengsheng Mao, Liang Yao, and Yuan Luo. 2018. Distribution
    Networks for Open Set Learning. arXiv:cs.LG/1809.08106
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao等（2018）Chengsheng Mao, Liang Yao, 和 Yuan Luo。2018年。用于开放集学习的分布网络。arXiv:cs.LG/1809.08106
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed representations of words and phrases and their
    compositionality. In *Advances in neural information processing systems*. 3111–3119.
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mikolov等（2013）Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, 和 Jeff
    Dean。2013年。词和短语的分布式表示及其组合性。在*神经信息处理系统进展*。3111–3119。
- en: Neal et al. (2018) Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong,
    and Fuxin Li. 2018. Open set learning with counterfactual images. In *Proceedings
    of the European Conference on Computer Vision (ECCV)*. 613–628.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Neal等（2018）Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong, 和 Fuxin
    Li。2018年。使用反事实图像的开放集学习。在*欧洲计算机视觉会议（ECCV）论文集*。613–628。
- en: Olah (2015) Christopher Olah. 2015. Understanding LSTM Networks. [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olah（2015）Christopher Olah。2015年。理解LSTM网络。[https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher
    Manning. 2014. Glove: Global vectors for word representation. In *Proceedings
    of the 2014 conference on empirical methods in natural language processing (EMNLP)*.
    1532–1543.'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pennington等（2014）Jeffrey Pennington, Richard Socher, 和 Christopher Manning。2014年。GloVe:
    全局词向量表示。在*2014年自然语言处理经验方法会议（EMNLP）论文集*。1532–1543。'
- en: Perera and Patel (2019) Pramuditha Perera and Vishal M Patel. 2019. Learning
    deep features for one-class classification. *IEEE Transactions on Image Processing*
    (2019).
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Perera 和 Patel（2019）Pramuditha Perera 和 Vishal M Patel。2019年。用于单类分类的深度特征学习。*IEEE图像处理学报*（2019）。
- en: Radford et al. (2016) Alec Radford, Luke Metz, and Soumith Chintala. 2016. Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks.
    In *4th International Conference on Learning Representations, ICLR 2016, San Juan,
    Puerto Rico, May 2-4, 2016, Conference Track Proceedings*, Yoshua Bengio and Yann
    LeCun (Eds.). [http://arxiv.org/abs/1511.06434](http://arxiv.org/abs/1511.06434)
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Radford et al. (2016) Alec Radford, Luke Metz, 和 Soumith Chintala. 2016. 利用深度卷积生成对抗网络进行无监督表示学习.
    见于 *第四届国际表示学习会议，ICLR 2016，波多黎各圣胡安，2016年5月2-4日，会议论文集*，Yoshua Bengio 和 Yann LeCun
    (编辑)。 [http://arxiv.org/abs/1511.06434](http://arxiv.org/abs/1511.06434)
- en: Ravi and Larochelle (2016) Sachin Ravi and Hugo Larochelle. 2016. Optimization
    as a model for few-shot learning. (2016).
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ravi and Larochelle (2016) Sachin Ravi 和 Hugo Larochelle. 2016. 作为少量样本学习模型的优化.
    (2016).
- en: Real et al. (2017) Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena,
    Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. 2017. Large-scale
    evolution of image classifiers. In *Proceedings of the 34th International Conference
    on Machine Learning-Volume 70*. JMLR. org, 2902–2911.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Real et al. (2017) Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena,
    Yutaka Leon Suematsu, Jie Tan, Quoc V Le, 和 Alexey Kurakin. 2017. 大规模图像分类器的进化.
    见于 *第34届国际机器学习会议论文集-第70卷*。JMLR.org, 2902–2911.
- en: 'Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016. Why should i trust you?: Explaining the predictions of any classifier. In
    *Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery
    and data mining*. ACM, 1135–1144.'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, 和 Carlos Guestrin.
    2016. 我为什么要相信你？：解释任何分类器的预测. 见于 *第22届ACM SIGKDD国际知识发现与数据挖掘会议论文集*。ACM, 1135–1144.
- en: Saha (2018) Sumit Saha. 2018. A Comprehensive Guide to Convolutional Neural
    Networks-the ELI5 way. [https://bit.ly/2Oa2YCh](https://bit.ly/2Oa2YCh)
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saha (2018) Sumit Saha. 2018. 卷积神经网络全面指南—ELI5方式. [https://bit.ly/2Oa2YCh](https://bit.ly/2Oa2YCh)
- en: Saito et al. (2018) Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya
    Harada. 2018. Open set domain adaptation by backpropagation. In *Proceedings of
    the European Conference on Computer Vision (ECCV)*. 153–168.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Saito et al. (2018) Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, 和 Tatsuya
    Harada. 2018. 通过反向传播进行开放集领域适应. 见于 *欧洲计算机视觉会议论文集 (ECCV)*。153–168.
- en: 'Schultheiss et al. (2017) Alexander Schultheiss, Christoph Käding, Alexander
    Freytag, and Joachim Denzler. 2017. Finding the unknown: Novelty detection with
    extreme value signatures of deep neural activations. In *German Conference on
    Pattern Recognition*. Springer, 226–238.'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schultheiss et al. (2017) Alexander Schultheiss, Christoph Käding, Alexander
    Freytag, 和 Joachim Denzler. 2017. 发现未知：使用深度神经网络激活的极值签名进行新颖性检测. 见于 *德国模式识别会议*。Springer,
    226–238.
- en: 'Shu et al. (2017) Lei Shu, Hu Xu, and Bing Liu. 2017. DOC: Deep Open Classification
    of Text Documents. In *Proceedings of the 2017 Conference on Empirical Methods
    in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11,
    2017*, Martha Palmer, Rebecca Hwa, and Sebastian Riedel (Eds.). Association for
    Computational Linguistics, 2911–2916. [https://www.aclweb.org/anthology/D17-1314/](https://www.aclweb.org/anthology/D17-1314/)'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu et al. (2017) Lei Shu, Hu Xu, 和 Bing Liu. 2017. DOC：文本文件的深度开放分类. 见于 *2017年自然语言处理经验方法会议论文集，EMNLP
    2017，丹麦哥本哈根，2017年9月9-11日*，Martha Palmer, Rebecca Hwa, 和 Sebastian Riedel (编辑)。计算语言学协会,
    2911–2916. [https://www.aclweb.org/anthology/D17-1314/](https://www.aclweb.org/anthology/D17-1314/)
- en: Shu et al. (2018b) Lei Shu, Hu Xu, and Bing Liu. 2018b. Unseen class discovery
    in open-world classification. *arXiv preprint arXiv:1801.05609* (2018).
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu et al. (2018b) Lei Shu, Hu Xu, 和 Bing Liu. 2018b. 开放世界分类中的未见类别发现. *arXiv
    预印本 arXiv:1801.05609* (2018).
- en: 'Shu et al. (2018a) Yu Shu, Yemin Shi, Yaowei Wang, Yixiong Zou, Qingsheng Yuan,
    and Yonghong Tian. 2018a. ODN: Opening the Deep Network for Open-Set Action Recognition.
    In *2018 IEEE International Conference on Multimedia and Expo (ICME)*. IEEE, 1–6.'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shu et al. (2018a) Yu Shu, Yemin Shi, Yaowei Wang, Yixiong Zou, Qingsheng Yuan,
    和 Yonghong Tian. 2018a. ODN：为开放集动作识别开启深度网络. 见于 *2018 IEEE国际多媒体与博览会 (ICME)*。IEEE,
    1–6.
- en: Silva (2018) Thalles Silva. 2018. An intuitive introduction to Generative Adversarial
    Networks (GANs). [https://bit.ly/34dgUB2](https://bit.ly/34dgUB2)
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Silva (2018) Thalles Silva. 2018. 生成对抗网络 (GANs) 的直观介绍. [https://bit.ly/34dgUB2](https://bit.ly/34dgUB2)
- en: 'Tang et al. (2015) Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and
    Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In *Proceedings
    of the 24th international conference on world wide web*. International World Wide
    Web Conferences Steering Committee, 1067–1077.'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tang等人（2015）Jian Tang、Meng Qu、Mingzhe Wang、Ming Zhang、Jun Yan和Qiaozhu Mei。2015。Line：大规模信息网络嵌入。在*第24届国际世界宽网会议论文集*中。国际万维网会议指导委员会，1067–1077。
- en: Wang et al. (2018) Zhuoyi Wang, Zelun Kong, Hemeng Tao, Swarup Chandra, and
    Latifur Khan. 2018. Co-Representation Learning For Classification and Novel Class
    Detection via Deep Networks. *arXiv preprint arXiv:1811.05141* (2018).
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang等人（2018）Zhuoyi Wang、Zelun Kong、Hemeng Tao、Swarup Chandra和Latifur Khan。2018。通过深度网络进行分类和新类别检测的共表示学习。*arXiv预印本
    arXiv:1811.05141*（2018）。
- en: Wen et al. (2016) Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. 2016.
    A discriminative feature learning approach for deep face recognition. In *European
    conference on computer vision*. Springer, 499–515.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen等人（2016）Yandong Wen、Kaipeng Zhang、Zhifeng Li和Yu Qiao。2016。用于深度人脸识别的辨别特征学习方法。在*欧洲计算机视觉会议*中。Springer，499–515。
- en: Xu et al. (2018) Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
    Kawarabayashi, and Stefanie Jegelka. 2018. Representation Learning on Graphs with
    Jumping Knowledge Networks. In *Proceedings of the 35th International Conference
    on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,
    2018* *(Proceedings of Machine Learning Research)*, Jennifer G. Dy and Andreas
    Krause (Eds.), Vol. 80. PMLR, 5449–5458. [http://proceedings.mlr.press/v80/xu18c.html](http://proceedings.mlr.press/v80/xu18c.html)
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu等人（2018）Keyulu Xu、Chengtao Li、Yonglong Tian、Tomohiro Sonobe、Ken-ichi Kawarabayashi和Stefanie
    Jegelka。2018。具有跳跃知识网络的图表示学习。在*第35届国际机器学习大会，ICML 2018，斯德哥尔摩，瑞典，2018年7月10-15日* *(机器学习研究论文集)*中，Jennifer
    G. Dy和Andreas Krause（编辑），第80卷。PMLR，5449–5458。 [http://proceedings.mlr.press/v80/xu18c.html](http://proceedings.mlr.press/v80/xu18c.html)
- en: Yu et al. (2017) Yang Yu, Wei-Yang Qu, Nan Li, and Zimin Guo. 2017. Open Category
    Classification by Adversarial Sample Generation. In *Proceedings of the Twenty-Sixth
    International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne,
    Australia, August 19-25, 2017*, Carles Sierra (Ed.). ijcai.org, 3357–3363. [https://doi.org/10.24963/ijcai.2017/469](https://doi.org/10.24963/ijcai.2017/469)
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu等人（2017）Yang Yu、Wei-Yang Qu、Nan Li和Zimin Guo。2017。通过对抗样本生成进行开放类别分类。在*第26届国际联合人工智能会议，IJCAI
    2017，澳大利亚墨尔本，2017年8月19-25日*，Carles Sierra（编辑）。ijcai.org，3357–3363。 [https://doi.org/10.24963/ijcai.2017/469](https://doi.org/10.24963/ijcai.2017/469)
- en: Zhang and Patel (2016) He Zhang and Vishal M Patel. 2016. Sparse representation-based
    open set recognition. *IEEE transactions on pattern analysis and machine intelligence*
    39, 8 (2016), 1690–1696.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang和Patel（2016）He Zhang和Vishal M Patel。2016。基于稀疏表示的开放集识别。*IEEE模式分析与机器智能学报*
    39，第8期（2016），1690–1696。
