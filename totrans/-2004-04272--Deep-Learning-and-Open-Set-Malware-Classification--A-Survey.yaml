- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:01:37'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2004.04272] Deep Learning and Open Set Malware Classification: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2004.04272](https://ar5iv.labs.arxiv.org/html/2004.04272)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Learning and Open Set Malware Classification: A Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Jingyun Jia School of Computing, Florida Institute of Technology150 West University
    BlvdMelbourne, FL 32901 [jiaj2018@my.fit.edu](mailto:jiaj2018@my.fit.edu)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As the Internet is growing rapidly these years, the variant of malicious software,
    which often referred to as malware, has become one of the major and serious threats
    to Internet users. The dramatic increase of malware has led to a research area
    of not only using cutting edge machine learning techniques classify malware into
    their known families, moreover, recognize the unknown ones, which can be related
    to Open Set Recognition (OSR) problem in machine learning. Recent machine learning
    works have shed light on Open Set Recognition (OSR) from different scenarios.
    Under the situation of missing unknown training samples, the OSR system should
    not only correctly classify the known classes, but also recognize the unknown
    class. This survey provides an overview of different deep learning techniques,
    a discussion of OSR and graph representation solutions and an introduction of
    malware classification systems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep Learning, Open Set Recognition, Graph Representation, Malware Classification^†^†copyright:
    none'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Malware, software that ”deliberately fulfills the harmful intent of an attacker”
    (Bayer et al., [2006](#bib.bib4)) nowadays come in a wide range of variations
    and multiple families. They have become one of the most terrible and major security
    threats of the Internet today. Instead of using traditional defenses, which typically
    use signature-based methods. There is an active research area that using machine
    learning-based techniques to solve the problem in both sides:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Classify known malware into their families, which turns out to be normal multi-classification
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Recognize unknown malware. i.e. malware that are not present in the training
    set but appear in the test set.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One solution for malware classification is to convert it to function call graphs
    (FCG), then classify them into families according to the representation of FCG
    as in (Hassen and Chan, [2017](#bib.bib20)). Graphs are an important data structure
    in machine learning tasks, and the challenge is to find a way to represent graphs.
    Traditionally, the feature extraction relied on user-defined heuristics. And recent
    researches have been focus on using deep learning to automatically learn to encode
    graph structure into low-dimensional embedding.
  prefs: []
  type: TYPE_NORMAL
- en: Another problem is it is less likely to label all the classes in training samples
    for the fast-developing diverse of malware families, the second item has become
    even more important daily. The related open set recognition (OSR) should also
    be able to handle those unlabeled ones. Traditional classification techniques
    focus on problems with labeled classes. While OSR pays attention to unknown classes.
    It requires the classifier accurately classify known classes, meanwhile identify
    unknown classes. Meanwhile, deep learning based OSR solutions have become a flourish
    research area in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: In this survey, we will first review basic deep learning techniques. Then a
    brief categorization of current OSR techniques will be given in section 3\. In
    section 4, we will cover methods learning graph representations, followed by an
    introduction to state-of-art malware classification techniques in section 5\.
    And finally, section 6 will conclude.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Deep learning basics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As conventional machine-learning techniques were limited in their ability to
    process natural data in their raw form, hence required expertise in feature engineering
    (LeCun et al., [2015](#bib.bib29)). Deep Learning was introduced to discover intricate
    structures in high-dimension data, which requires litter engineering by hand.
  prefs: []
  type: TYPE_NORMAL
- en: In the following subsections, we will give an overview of different network
    architectures in different areas in recent years.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9b2ad45c800f1cbd5e3b28e139110b45.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: CNNs
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. A CNN sequence to classify handwritten digits ([Saha](#bib.bib45),
    ([2018](#bib.bib45)))
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Convolutional neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As a popular architecture of Deep Neural Networks, Convolutional Neural Networks
    (CNNs) has achieved good performance in the Computer Vision area. As Figure [1](#S2.F1
    "Figure 1 ‣ 2\. Deep learning basics ‣ Deep Learning and Open Set Malware Classification:
    A Survey"), a typical CNN usually consists of “Input Layer”, “Convolutional Layer”,
    “Pooling Layer” and “Output Layer”. The convolutional and pooling layer can be
    repeated several times. In most cases, the Relu function is used as activation
    in the convolutional layer and Max-Pooling is used in the pooling layer. During
    the learning process, filters will be learned and feature maps will then be generated,
    which is the output of representation learning. The output is usually followed
    by a fully connected network for classification problems. The architectures could
    be different in various aspects: layers and connections ((Cao et al., [2015](#bib.bib6))
    (He et al., [2016](#bib.bib23))), loss functions ((Wen et al., [2016](#bib.bib54))
    (He et al., [2018](#bib.bib24)) (Deng et al., [2019](#bib.bib11))), etc.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1\. Feedback Network
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cao et al. ([2015](#bib.bib6)) proposed Feedback Network to develop a computational
    feedback mechanism which can help better visualized and understand how deep neural
    network works, and capture visual attention on expected objects, even in images
    with cluttered background and multiple objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feedback Network introduced a feedback layer. The feedback layer contains another
    set of binary neuron activation variables $Z\in\{0,1\}$. The feedback layer is
    stacked upon each ReLU layer, and they compose a hybrid control unit to active
    neuron response in both bottom-up and top-down manners: Bottom-Up Inherent the
    selectivity from ReLU layers, and the dominant features will be passed to upper
    layers; Top-Down is controlled by Feedback Layers, which propagate the high-level
    semantics and global information back to image representations. Only those gates
    related to particular target neurons are activated.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2\. Center loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Wen et al. proposed center loss as a new supervision signal (objective function)
    for face recognition tasks in (Wen et al., [2016](#bib.bib54)). To separate the
    features of different classes to achieve better performance in classification
    tasks, center loss tries to minimize the variation of intra-class. Let $\mathbf{c}_{y_{i}}$
    denotes the center of the embeddings of $y_{i}$th class, the loss function looks
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\mathcal{L}_{C}=\frac{1}{2}\sum_{i=1}^{m}\&#124;\mathbf{x}_{i}-\mathbf{c}_{y_{i}}\&#124;_{2}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: To make the computation more efficient, center loss uses a mini-batch updating
    method and the centers are updated by the features mean of the corresponding classes
    after each iteration. The paper showed that under the joint supervision of softmax
    loss and center loss, CNN can obtain inter-class dispensation and intra-class
    compactness as much as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3\. Triplet-center loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Inspired by triplet loss and center loss, He et al. introduced triplet-center
    loss to further enhance the discriminative power of the features, as to 2D object
    recognition algorithms in (He et al., [2018](#bib.bib24)). Triplet-loss intends
    to find an embedding space where the distances between different classes are greater
    than those form the same classes. Center loss tries to find embedding spaces where
    the deep learned features from the same class more compact and closer to the corresponding
    center. Similarly, instead of comparing the distances of each two instances in
    triplet loss, triplet loss computes the distances of instance and class center.
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\mathcal{L}_{tc}=\sum_{i=1}^{M}\max\left(D(f_{i},C_{y_{i}})+m-\min_{j\neq
    y_{i}}D(f_{i},c_{j}),0\right),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $D()$ is a distance function and $m$ is margin value. By setting up a
    margin value, the loss function ensures different classes be pushed by at least
    $m$ distance away.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.4\. Arcface
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In (Deng et al., [2019](#bib.bib11)), Deng et al. proposed an additive angular
    margin loss (ArcFace) to obtain highly discriminative features for face recognition.
    Based on classic softmax loss,
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $\mathcal{L}_{\text{softmax}}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{e^{W_{y_{i}}^{T}x_{i}+b_{y_{i}}}}{\sum_{j=1}^{n}e^{W_{j}^{T}x_{i}+b_{j}}},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $x_{i}$ denotes the embedding of the $i$th sample. After normalizing
    $x$ and $W$, ArcFace adds an additive angular margin penalty $m$ between $x_{i}$
    and $W_{y_{i}}$ to simultaneously enhance the intra-class compactness and inter-class
    discrepancy:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\mathcal{L}_{\text{Arc}}=-\frac{1}{N}\sum_{i=1}^{N}\log\frac{e^{s(\cos(\theta_{y_{i}}+m))}}{e^{s(\cos(\theta_{y_{i}}+m))}+\sum_{j=1,j\neq
    y_{i}}^{n}e^{s\cos\theta_{j}}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: The paper shows that ArcFace has a better geometric attribute as the angular
    margin has the exact correspondence to the geodesic distance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.5\. ResNets
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The training of deeper neural networks is facing degradation problems: with
    the network depth increasing, accuracy gets saturated and then degrades rapidly.
    The degradation problem indicates that not all systems are similarly easy to optimize.
    Under the hypothesis that it is easier to optimize the residual mapping than to
    optimize the original unreferenced mapping, He et al. ([2016](#bib.bib23)) presented
    a residual learning framework called ResNets to ease the training of the network.
    ResNets consists of residual blocks as Figure [2](#S2.F2 "Figure 2 ‣ 2.1.5\. ResNets
    ‣ 2.1\. Convolutional neural networks ‣ 2\. Deep learning basics ‣ Deep Learning
    and Open Set Malware Classification: A Survey"). Instead of hoping each few stacked
    layers directly fit a desired underlying mapping, ResNets explicitly let these
    layers fit a residual mapping.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/ff1b181bfaf970f23043f75f1c1f96b6.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: resNet
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2\. A building block of resNet ([He et al.](#bib.bib23), ([2016](#bib.bib23)))
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, instead fitting the desired underlying mapping as $H(x)$, ResNet
    makes stacked nonlinear layers fit another mapping of $\mathcal{F}(x):=H(x)-x$.
    Then original mapping is recast into $\mathcal{F}(x)+x$. In an extreme case, if
    an identity mapping were optimal, it would be easier to push the residual to zero
    than to fit an identity mapping.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e88067395562d82641c4e483eb629aa.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs
  prefs: []
  type: TYPE_NORMAL
- en: Figure 3\. A standard RNN contains a single layer ([Olah](#bib.bib38), ([2015](#bib.bib38)))
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Recurrent neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another type of popular architecture of Deep Neural Networks is Recurrent Neural
    Networks (RNNs), which involves sequential inputs, such as speech and language.
    RNNs process an input sequence one element at a time, also maintain a state vector
    that implicitly contains all the historical information. An unfold RNN (Figure
    [3](#S2.F3 "Figure 3 ‣ 2.1.5\. ResNets ‣ 2.1\. Convolutional neural networks ‣
    2\. Deep learning basics ‣ Deep Learning and Open Set Malware Classification:
    A Survey")) could be considered as a deep multi-layer network. Just like CNNs,
    there are multiple variants for RNNs as well. Particularly, it has been widely
    used in machine translation tasks ((Cho et al., [2014](#bib.bib9)) (Bahdanau et al.,
    [2015](#bib.bib3)) (Luong et al., [2015](#bib.bib33))).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/14648ea3bc8f23656cd2651a9ed3edda.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: LSTMs
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4\. An LSTM contains four interacting layers ([Olah](#bib.bib38), ([2015](#bib.bib38)))
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. LSTM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As it is not applicable to store information for very long, “Long Short Term
    Memory” (LSTM) was proposed to solve the problem. [Chen](#bib.bib8) gave a gentle
    tutorial on basics of backpropagation in recurrent neural networks (RNN) and long
    short-term memory (LSTM) in (Chen, [2016](#bib.bib8)). LSTM (Figure [4](#S2.F4
    "Figure 4 ‣ 2.2\. Recurrent neural networks ‣ 2\. Deep learning basics ‣ Deep
    Learning and Open Set Malware Classification: A Survey")) includes four gates:
    input modulation gate, input gate, forget gate (Gers et al. ([1999](#bib.bib16)))
    and output gate along with their corresponding weights. LSTM also contains a special
    unit called memory cell act like an accumulator or a gated leaky neuron. Meanwhile,
    There are other augment RNNs with a memory module such as “Neural Turing Machine”
    and “memory networks”. These models are being used for tasks need reasoning and
    symbol manipulation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/59c1482cfe224dfac1b38fecadaf19a3.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: rnn_en_de
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5\. An illustration of the RNN Encoder-Decoder ([Cho et al.](#bib.bib9),
    ([2014](#bib.bib9)))
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2\. RNN encoder-dencoder
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Cho et al. ([2014](#bib.bib9)) proposed a neural network architecture called
    RNN Encoder-Decoder (Figure [5](#S2.F5 "Figure 5 ‣ 2.2.1\. LSTM ‣ 2.2\. Recurrent
    neural networks ‣ 2\. Deep learning basics ‣ Deep Learning and Open Set Malware
    Classification: A Survey")), which can be to used as additional features in statistical
    machine translation (SMT) system to generate a target sequence, also can be used
    to score a given pair of input and output sequence. The architecture learns to
    encode a variable-length sequence into a fixed-length vector representation and
    to decode a given fixed-length vector representation back into a variable-length
    sequence. The encoder is an RNN that reads each symbol of an input sequence x
    sequentially. The decoder of the proposed model is another RNN that is trained
    to generate the output sequence by predicting the next symbol given the hidden
    state.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition to a novel model architecture, the paper also proposed a variant
    of LSTM, which includes an update gate and a reset gate. The update gate selects
    whether the hidden state is to be updated with a new hidden state while the reset
    gate decides whether the previous hidden state is ignored.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/dd0b4da57130584a9354da48a2237111.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: rnnsearch
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6\. An illustration of the RNNsearch ([Bahdanau et al.](#bib.bib3), ([2015](#bib.bib3)))
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. RNNsearch
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In (Bahdanau et al., [2015](#bib.bib3)), [Bahdanau et al.](#bib.bib3) proposed
    a new architecture for machine translation model by adding an alignment model
    to basic RNN Encoder-Decoder. Just like a traditional machine translation model,
    the proposed architecture consists of an encoder and a decoder. The encoder reads
    the input sentence, then converts into a vector. And the decoder emulates searching
    through a source sentence during decoding a translation. As Figure [6](#S2.F6
    "Figure 6 ‣ 2.2.2\. RNN encoder-dencoder ‣ 2.2\. Recurrent neural networks ‣ 2\.
    Deep learning basics ‣ Deep Learning and Open Set Malware Classification: A Survey"),
    the align model learns the weights $\alpha_{ij}$ of each annotation $h_{j}$ scoring
    how well the inputs around the position $j$ and output at position $i$ match.
    The score is based on the RNN hidden state $s_{i-1}$ and the $j$th annotation
    $h_{j}$ of the input sentence.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.4\. Attentional mechanism
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In (Luong et al., [2015](#bib.bib33)), [Luong et al.](#bib.bib33) examined
    two classes of attentional mechanism to better improve neural machine translation
    (NMT): a global approach which always attends to all source words and a local
    one that only looks at a subset of source words at a time. Based on LSTM, they
    introduced a variable-length alignment vector for two kinds of attentional mechanisms.
    The global attention model is based on the global context, and the size of the
    alignment vector equals the number of time steps on the source site. While the
    local attention model is based on a window context, where the size of the alignment
    vector equals to window size.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/41385c0b65b46740844fd54c298cea51.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: GANs
  prefs: []
  type: TYPE_NORMAL
- en: Figure 7\. Overview of the framework of GANs ([Silva](#bib.bib51), ([2018](#bib.bib51)))
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Generative adversarial networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Deep learning has achieved great performance in supervised learning in discriminative
    models. However, deep generative models have had less of impact as:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is difficult to approximate the computations in maximum likelihood estimation
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is difficult to leverage the benefits of piecewise linear units in the generative
    context”
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Goodfellow et al. ([2014](#bib.bib17)) proposed a new generative model: generative
    adversarial nets (GANs) to avoid these difficulties. The proposed GANs architecture
    includes two components: a generator $G$ and a discriminator $D$. To learn the
    distribution $P_{g}$ over given data $x$. GANs define a prior on input noise variables
    $p_{\mathbf{z}}(\mathbf{z})$. The framework of GANs corresponds to a “min-max
    two-player game” (discriminator vs. generator) with value function $V(G,D)$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $\min_{G}\max_{D}V(D,G)=\mathbb{E}_{\mathbf{x}\sim p_{\text{data}}(\mathbf{x})}[\log
    D(\mathbf{x})]+\mathbb{E}_{\mathbf{z}\sim p_{\mathbf{z}}(\mathbf{z})}[\log(1-D(G(\mathbf{z})))]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: Generator generates noise samples from a prior distribution and discriminator
    represents the probability of the data come from the target dataset rather than
    a generator. Hence the target is to train the discriminator $D$ to maximize the
    probability of assigning the correct label to both training examples and samples
    from $G$, meanwhile train the generator $G$ to minimize $\log(1-D(G(\mathbf{z})))$,
    i.e. generating samples alike examples to “fool” the discriminator. In practice,
    the procedure optimizes $D$ $k$ steps and one step of $G$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1\. DCGANs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Radford et al. ([2016](#bib.bib41)) proposed deep convolution generative adversarial
    networks (DCGANs) to bridge the gap between the supervised learning and unsupervised
    learning in CNNs, which makes GANs more stable. The architecture guidelines for
    stable Deep Convolutional GANs:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Replace any pooling layers with stridden convolutions (discriminator) and fractional-strided
    convolutions (generator).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use batch norm in both the generator and the discriminator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove fully connected hidden layers for deeper architectures.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use ReLU activation in the generator for all layers except for the output, which
    uses Tanh.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use LeakyReLU activation in the discriminator for all layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 2.3.2\. AAE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Makhzani et al. ([2015](#bib.bib34)) proposed a new inference algorithm Adversarial
    Autoencoder (AAE), which uses the GANs framework which could better deal with
    applications such as semi-supervised classification, disentangling style and content
    of images, unsupervised clustering, dimensionality reduction, and data visualization.
    The algorithm aims to find a representation for graphs that follows a certain
    type of distribution. And it consists of two phases: the reconstruction phase
    and the regularization phase. In the reconstruction phase, encoder and decoder
    are updated to minimize reconstruction error. In the regularization phase, the
    discriminator is updated to distinguish true prior samples from generated samples,
    and the generator is updated to fool the discriminator. Reconstruction phase and
    regularization phase are referred to as the generator and discriminator in GANs.
    And the method could be used in semi-supervised learning and unsupervised clustering.
    For semi-supervised learning, there is a semi-supervised classification phase
    besides the reconstruction phase and regularization phase. And labeled data would
    be trained at this stage. which is an aggregated categorical distribution. The
    architecture of unsupervised clustering is similar to semi-supervised learning,
    the difference is that the semi-supervised classification stage is removed and
    thus no longer train the network on any labeled mini-batch.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.4\. Representation learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Representation learning allows a machine to be fed with raw data and to automatically
    discover the representations (embeddings) needed for detection or classification
    (LeCun et al., [2015](#bib.bib29)). Those raw data could be images, videos, texts,
    etc. An image comes in the form of an array of pixel values and texts come in
    the form of word sequences. Motivated by different objectives, a set of representative
    features would be generated through deep neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.1\. Skip-gram
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Skip-gram model has achieved good performance in learning high-quality vector
    representations of words from large amounts of unstructured text data, which doesn’t
    require dense matrix multiplications. The training objective of the Skip-gram
    model is to find word representations that are useful for predicting the surrounding
    words in a sentence or a document. Given a sequence of training words $\mathfrak{w}_{1},\mathfrak{w}_{2},\mathfrak{w}_{3},...,\mathfrak{w}_{T}$,
    the objective of the Skip-gram model is to maximize the average log probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\frac{1}{T}\sum_{t=1}^{T}\sum_{-c\leq j\leq c,j\neq 0}\log p(\mathfrak{w}_{t+j}&#124;\mathfrak{w}_{t}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $c$ is the size of training context. The basic Skip-gram formulation
    defines $p(\mathfrak{w}_{t+j}|\mathfrak{w}_{t})$ using the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $p(\mathfrak{w}_{\mathit{O}}&#124;\mathfrak{w}_{\mathit{I}})=\frac{\exp(\mathfrak{v}^{\prime\top}_{\mathfrak{w}_{\mathit{O}}}\mathfrak{v}_{\mathfrak{w}_{\mathit{I}}})}{\sum_{\mathfrak{w}=1}^{W}\exp(\mathfrak{v}^{\prime\top}_{\mathfrak{w}}\mathfrak{v}_{\mathfrak{w}_{\mathit{I}}})}.$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathfrak{v}_{\mathfrak{w}}$ and $\mathfrak{v}^{\prime}_{\mathfrak{w}}$
    are the ”input” and ”output” vector representations of $\mathfrak{w}$, and $W$
    is the number of words in the vocabulary. Based on the skip-gram algorithm. Mikolov
    et al. ([2013](#bib.bib36)) presents some extensions to improve its performance:
    Hierarchical softmax, negative sampling and subsampling. It shows that the word
    vectors can be meaningful combined using just simple vector addition. Specifically,
    hierarchical softmax uses a binary tree to present the output layer rather than
    a plat output of all the words for output dimension reduction to make the computation
    more efficient. An alternative to hierarchical softmax is negative sampling, inspired
    by Noise Contrastive Estimation (NCE). The basic idea is to sample one “accurate”
    data and $k$ noise data, the objective is to maximize their conditional log-likelihood:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\log\sigma(\mathfrak{v}^{\prime\top}_{\mathfrak{w}_{\mathit{O}}}\mathfrak{v}_{\mathfrak{w}_{\mathit{I}}})+\sum_{i=1}^{k}\mathbb{E}_{\mathfrak{w}_{i}\sim
    P_{n}(\mathfrak{w})}\left[\log\sigma(-v^{\prime\top}_{\mathfrak{w}_{i}}v_{\mathfrak{w}_{\mathit{I}}})\right],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The objective of NCE is used to replace every $\log P(\mathfrak{w}_{\mathit{O}}|\mathfrak{w}_{\mathit{I}})$
    term in the Skip-gram objective, and the task is to distinguish the target word
    $\mathfrak{w}_{\mathit{O}}$ from draws from the noise distribution $P_{n}(\mathfrak{w})$
    using logistic regression, where there are $k$ negative samples for each data
    sample. The paper also suggested a simple subsampling approach to address the
    imbalance issue between the rare and frequent words: each word $w_{i}$ in the
    training set is discarded with probability computed by the formula'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $P(w_{i})=1-\sqrt{\frac{t}{f(w_{i})}},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $f(w_{i})$ is the frequent of word $w_{i}$ and $t$ is threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4.2\. GloVe
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To better deal with word representations: word analogy, word similarity, and
    named entity recognition tasks, Pennington et al. ([2014](#bib.bib39)) constructs
    a new model GloVe (for Global Vectors), which can capture the global corpus statistics.
    GloVe combines count-based methods and prediction-based methods for the unsupervised
    learning of word representations, proposing a new cost function'
  prefs: []
  type: TYPE_NORMAL
- en: '| (10) |  | $J=\sum_{i,j=1}^{V}f(X_{ij})(W_{i}^{T}\tilde{w}_{j}+b_{i}+\tilde{b}_{j}-\log
    X_{ij})^{2},$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $V$ is the size of the vocabulary, $f(X_{ij})$ is a weighting function.
    $w$ are word vectors and $\tilde{w}$ are separate context word vector as training
    multiple instances of the network and then combining the results can help reduce
    overfitting and noise and generally improve results. $b$ and $\tilde{b}$ are corresponding
    bias for $w$ and $\tilde{w}$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5\. Meta-learning and interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6ee868a868b63a113795d13e936aaeb1.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: MAML
  prefs: []
  type: TYPE_NORMAL
- en: Figure 8\. Diagram of the MAML ([Finn et al.](#bib.bib13), ([2017](#bib.bib13)))
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.1\. Meta-Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Deep neural networks generally perform poorly on few-shot learning tasks as
    a classifier has to quickly generalize after seeing very few examples from each
    class. Ravi and Larochelle ([2016](#bib.bib42)) proposes an LSTM based meta-learner
    model to learn the exact optimization algorithm used to train another learner
    neural network classifier in the few-shot regime. The meta-learner captures both
    short-term knowledge within a task and long-term knowledge common among all the
    tasks. Also, Finn et al. ([2017](#bib.bib13)) proposed an algorithm called model-agnostic
    meta-learning (MAML) for meta-learning which is compatible with any model trained
    with gradient descent and different learning problems such as classification,
    regression and reinforcement learning. The meta-learning is to prepare the model
    for fast adaption. In general, it consists of two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: sample batch of tasks to learn the gradient update for each of them, then combine
    their results;
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: take the result of step 1 as a starting point when learning a specific task.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The diagram looks like Figure [8](#S2.F8 "Figure 8 ‣ 2.5\. Meta-learning and
    interpretability ‣ 2\. Deep learning basics ‣ Deep Learning and Open Set Malware
    Classification: A Survey"), it optimizes for a representation $\theta$ that can
    quickly adapt to new tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.2\. LIME
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As machine learning techniques are rapidly developed these days, there are plenty
    of models remain mostly black box. To make the predictions more interpretable
    to non-expertises despite which model it is (model-agnostic), so that people can
    make better decisions, Ribeiro et al. ([2016](#bib.bib44)) proposed a method -
    Local Interpretable Model-agnostic Explanations (LIME) to identify an interpretable
    model over interpretable presentation that is locally faithful to the classifier.
    It introduced sparse line explanations, weighing similarity between instance and
    its interpretable version with their distance. The paper also suggested a submodular
    pick algorithm (SP-LIME) to better select instances by picking the most important
    features based on the explanation matrix learned in LIME.
  prefs: []
  type: TYPE_NORMAL
- en: 2.5.3\. Large-Scale evolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To minimize human participation in neural network design, Real et al. (Real
    et al., [2017](#bib.bib43)) employed evolutionary algorithms to discover network
    architectures automatically. The evolutionary algorithm uses the evolutionary
    algorithm to select the best of a pair to be a parent during tournament selection.
    Using pairwise comparisons instead of whole population operations.
  prefs: []
  type: TYPE_NORMAL
- en: In the proposed method, individual architectures are encoded as a graph. In
    the graph, the vertices represent rank-3 tensors or activations. The graph’s edges
    represent identity connections or convolutions and contain the mutable numerical
    parameters defining the convolution’s properties. A child is similar but not identical
    to the parent because of the action of a mutation. Mutation operations include
    ”ALTER-LEARNING-RATE”, ”RESET-WEIGHTS”, ”INSERT-CONVOLUTION”, ”REMOVE-CONVOLUTION”,
    etc.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Open Set Recognition
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As deep learning has achieved great success in object classification, it is
    less likely to label all the classes in training samples, and the unlabeled class,
    so-called open-set becomes a problem. Different from a traditional close-set problem,
    which only requires correctly classify the labeled data, open set recognition
    (OSR) should also handle those unlabeled ones. [Geng et al.](#bib.bib15) stated
    four categories of recognition problems as follows (Geng et al., [2018](#bib.bib15)):'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'known known classes: labeled distinctive positive classes, available in training
    samples;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'known unknown classes: labeled negative classes, available in training samples;'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'unknown known classes: training samples not available, but having some side-information
    such as semantic/attribute information'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'unknown unknown classes: neither training samples nor side-information available,
    completely unseen.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Traditional classification techniques focus on problems with labeled classes,
    which include known known classes and known unknown classes. While open set recognition
    (OSR) pays attention to the later ones: unknown known classes and unknown unknown
    classes. It requires the classifier accurately classify known known classes, meanwhile
    identify unknown unknown classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In general, the techniques can be categorized into three classes according
    to the training set compositions as Table [1](#S3.T1 "Table 1 ‣ 3\. Open Set Recognition
    ‣ Deep Learning and Open Set Malware Classification: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. OSR Techniques Categorization
  prefs: []
  type: TYPE_NORMAL
- en: '| Training Set | papers |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Borrowing Additional Data | (Shu et al., [2018b](#bib.bib49)) (Saito et al.,
    [2018](#bib.bib46)) (Shu et al., [2018a](#bib.bib50)) (Hendrycks et al., [2019](#bib.bib25))
    (Dhamija et al., [2018](#bib.bib12)) (Perera and Patel, [2019](#bib.bib40)) |'
  prefs: []
  type: TYPE_TB
- en: '| Generating additional data | (Jo et al., [2018](#bib.bib26)) (Neal et al.,
    [2018](#bib.bib37)) (Ge et al., [2017](#bib.bib14)) (Yu et al., [2017](#bib.bib56))
    (Lee et al., [2018](#bib.bib30)) |'
  prefs: []
  type: TYPE_TB
- en: '| No Additional Data | (Bendale and Boult, [2016](#bib.bib5)) (Hassen and Chan,
    [2018a](#bib.bib21)) (Júnior et al., [2017](#bib.bib27)) (Mao et al., [2018](#bib.bib35))
    (Wang et al., [2018](#bib.bib53)) (Schultheiss et al., [2017](#bib.bib47)) (Zhang
    and Patel, [2016](#bib.bib57)) (Liang et al., [2018](#bib.bib32)) (Shu et al.,
    [2017](#bib.bib48)) |'
  prefs: []
  type: TYPE_TB
- en: 3.1\. Borrowing additional data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To better discriminate known class and unknown class, some techniques introduce
    unlabeled data in training((Shu et al., [2018b](#bib.bib49)) (Saito et al., [2018](#bib.bib46))).
    In addition, (Shu et al., [2018a](#bib.bib50)) indicates several manually annotations
    for unknown classes are required in their workflow.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.1\. Open set domain adaptation by backpropagation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Saito et al. ([2018](#bib.bib46)) proposed a method which marks unlabeled target
    samples as unknown, then mixes them with labeled source samples together to train
    a feature generator and a classifier. The classifier attempts to make a boundary
    between source and target samples whereas the generator attempts to make target
    samples far from the boundary. The idea is to extract feature which separates
    known and unknown samples. According to the feature generator, the test data either
    would be aligned to known classes or rejected as an unknown class.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.2\. Unseen class discovery in open-world classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '![Refer to caption](img/618808dfbbda1e9e726f3761520ce849.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: OCN+PCN+HC
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9\. Overall framework of OCN+PCN+HC ([Shu et al.](#bib.bib49), 2018)
  prefs: []
  type: TYPE_NORMAL
- en: 'Shu et al. ([2018b](#bib.bib49)) introduced a framework to solve the open set
    problem, which involves unlabeled data as an autoencoder network to avoid overfitting.
    Besides autoencoder, it contains another two networks in the training process
    - an Open Classification Network (OCN), a Pairwise Classification Network (PCN).
    Only OCN participants in the testing phase, which predicts test dataset including
    unlabeled examples from both seen and unseen classes. Then it follows the clustering
    phase, based on the results of the predictions of PCN, they used hierarchical
    clustering (bottom-up/ merge) to cluster rejected examples clusters. Overall framework
    as Figure [9](#S3.F9 "Figure 9 ‣ 3.1.2\. Unseen class discovery in open-world
    classification ‣ 3.1\. Borrowing additional data ‣ 3\. Open Set Recognition ‣
    Deep Learning and Open Set Malware Classification: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.3\. ODN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Manual labeled unknown data is used in Open Deep Network (ODN) proposed by
    Shu et al. ([2018a](#bib.bib50)). It needs several manually annotations. Specifically,
    it added another new column corresponding to the unknown category to the weight
    matrix and initialized it as $\mathfrak{w}_{N+1}$:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (11) |  | $\mathfrak{w}_{N+1}=\alpha\frac{1}{N}\sum_{n=1}^{N}\mathfrak{w}_{n}+\beta\frac{1}{M}\sum_{m=1}^{M}\mathfrak{w}_{m},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathfrak{w}_{n}$ is the weight column if the known $n$th category. In
    addition, as the similar categories should play a more critical role in the initialization
    of $\mathfrak{w}_{m}$, ODN added another term $\frac{1}{M}\sum_{m=1}^{M}\mathfrak{w}_{m}$
    to emphase the similar known categories. The $\mathfrak{w}_{m}$ is the weight
    columns of $M$ highest activation values. The $\mathfrak{w}_{N+1}$ is concatenated
    to the transfer weight $W$ to support the new category. And this initialization
    method is called Emphasis Initialization.
  prefs: []
  type: TYPE_NORMAL
- en: 'ODN also introduces multi-class triplet thresholds to identify new categories:
    accept threshold, reject threshold and distance-threshold. Specifically, a sample
    would be accepted as a labeled class if and only if the index of its top confidence
    value is greater than the acceptable threshold. A sample would be considered as
    unknown if all the confidence values are below the rejected threshold. For samples
    between accept threshold and reject threshold, they would also be accepted as
    a labeled class if the distance between top and second maximal confidence values
    is large than the distance-threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.4\. Outlier Exposure
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Hendrycks et al. ([2019](#bib.bib25)) proposed Outlier Exposure(OE) to distinguish
    between anomalous and in-distribution examples. OE borrowed data from other datasets
    to be “out-of-distribution” (OOD), denoted as $\mathcal{D}_{out}$. Meanwhile target
    samples as “in-distribution”, marked as $\mathcal{D}_{in}$. Then the model is
    trained to discover signals and learn heuristics to detect” which dataset a query
    is sampled from. Given a model $f$ and the original learning objective $\mathcal{L}$,
    the objective function of OE looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $\mathbb{E}_{(x,y)\sim\mathcal{D}_{in}}[\mathcal{L}(f(x),y)+\lambda\mathbb{E}_{x^{\prime}\sim\mathcal{D}_{in}^{out}}[\mathcal{L}_{OE}(f(x^{\prime}),f(x),y)]]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: $\mathcal{D}_{out}^{OE}$ is an outlier exposure dataset. The equation indicates
    the model tries to minimize the objective L for data from “in-distribution” ($\mathcal{L}$)
    and “out-of-distribution” ($\mathcal{L}_{OE}$). The paper also used the maximum
    softmax probability baseline detector (cross-entropy) for $\mathcal{L}_{OE}$.
    And when labels are not available, $\mathcal{L}_{OE}$ was set to a margin ranking
    loss on the log probabilities $f(x^{\prime})$ and $f(x)$. However, the performance
    of this method depends on the chosen OOD dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.5\. Objectosphere Loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Dhamija et al. ([2018](#bib.bib12)) proposed Entropic Open-Set and Objectoshere
    losses for open set recognition, which trained networks using negative samples
    from some classes. The method reduced the deep feature magnitude and maximize
    entropy of the softmax scores of unknown sample to separate them from known samples.
    The idea of Entropic Open-Set is to maximum entropy when an input is unknown.
    Formally, let $S_{c}(x)$ denotes the softmax score of sample $x$ from known class
    $c$, the Entropic Open-Set Loss $J_{E}$ can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $J_{E}(x)=\begin{cases}-\log S_{c}(x)&amp;\text{if $x\in\mathcal{D}^{\prime}_{c}$
    is from class $c$}\\ -\frac{1}{C}\sum_{c=1}^{C}\log S_{c}(x)&amp;\text{if $x\in\mathcal{D}^{\prime}_{b}$
    is from class $c$}\end{cases}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{D}^{\prime}_{b}$ denotes out of distribution samples. To further
    separate known and unknown samples, the paper pushed known samples into the “objectosphere”
    where they have large feature magnitude and low entropy, so-called Objectosphere
    Loss, which is calculated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $J_{R}=J_{E}+\lambda\begin{cases}\max(\xi-\&#124;F(x)\&#124;,0)^{2}&amp;\text{if
    $x\in\mathcal{D}^{\prime}_{c}$}\\ \&#124;F(x)\&#124;^{2}&amp;\text{if $x\in\mathcal{D}^{\prime}_{b}$}\end{cases}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $F(x)$ is the deep feature vector, and Objectosphere Loss penalizes the
    known classes if their feature magnitude is inside the boundary of the Objectosphere
    $\xi$ and unknown classes if their magnitude is greater than zero.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1.6\. DOC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Perera and Patel ([2019](#bib.bib40)) proposed a deep learning-based solution
    for one-class classification (DOC) feature extraction. The objective of one-class
    classification is to recognize normal class and abnormal class using only samples
    from normal class and there are different strategies to solve a classification
    problem. The proposed accept two inputs: one from the target dataset, one from
    the reference dataset, and produces two losses through a pre-trained reference
    network and a secondary network. The reference dataset is the dataset used to
    train the reference network, and the target dataset contains samples of the class
    for which one-class learning is used for. During training, two image batches,
    each from the reference dataset and the target dataset are simultaneously fed
    into the input layers of the reference network and secondary network. At the end
    of the forward pass, the reference network generates a descriptiveness loss ($l_{D}$),
    which is the same as the cross-entropy loss, and the secondary network generates
    compactness loss ($l_{C}$). The composite loss ($l$) of the network is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (15) |  | $l(r,t)=l_{D}(r&#124;W)+\lambda l_{C}(t&#124;W),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $r$ and $t$ are the training data from reference dataset and target dataset
    respectively, $W$ is the shared weights of both networks, and $\lambda$ is a constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'The overview of the proposed method looks like figure [10](#S3.F10 "Figure
    10 ‣ 3.1.6\. DOC ‣ 3.1\. Borrowing additional data ‣ 3\. Open Set Recognition
    ‣ Deep Learning and Open Set Malware Classification: A Survey"), where $g$ is
    feature extraction networks and $h_{c}$ is classification networks. The compactness
    loss is to assess the compactness of the class under consideration in the learned
    feature space. The descriptiveness loss was assessed by an external multi-class
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58396cf57701c6cfe934790869a29582.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: OC
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10\. Overview of the Deep Feature for One-Class Classification framework
    ([Perera and Patel](#bib.bib40), ([2019](#bib.bib40)))
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs: []
  type: TYPE_NORMAL
- en: All the above methods borrow some dataset as unknown class during training,
    (Shu et al., [2018b](#bib.bib49)) borrows target samples (test set) as unknown
    classes and utilizes adversarial learning. A classifier is trained to make a boundary
    between the source and the target samples whereas a generator is trained to make
    target samples far from the boundary. (Saito et al., [2018](#bib.bib46)) also
    borrows unlabeled examples from the dataset, then used and Auto-encoder to learn
    the representations. (Shu et al., [2018a](#bib.bib50)) uses several manually annotations
    during Emphasis Initialization. (Hendrycks et al., [2019](#bib.bib25)) introduced
    outlier exposure datasets on top of in-distribution datasets. (Dhamija et al.,
    [2018](#bib.bib12)) also introduces unknown datasets, meanwhile utilizes the differences
    of feature magnitudes between known and unknown samples as part of the objective
    function. Different from the multi-class classification problems, (Perera and
    Patel, [2019](#bib.bib40)) presents a one-class classification problem from anomaly
    detection, with additional reference dataset for transfer learning. In general,
    borrowing and annotating additional data s OSR an easier problem. However, the
    retrieval and selection of additional datasets would be another problem.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Generating additional data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As adversarial learning has achieved great access such as GANs, there are ideas
    use GANs generating unknown samples before training.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.1\. G-OpenMax
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Ge et al. ([2017](#bib.bib14)) designed a networks based on OpenMax and GANs.
    Their approach provided explicit modeling and decision score for novel category
    image synthesis. The method proposed has two stages as well as OpenMax: pre-Network
    training and score calibration. During the pre-Network training stage, different
    with OpenMax, it first generates some unknown class samples (synthetic samples)
    then sends them along with known samples into networks for training. A modified
    conditional GAN is employed in G-OpenMax to sythesize unknown classes. In conditional
    GAN, random noise is fed to the generator $G$ with a one-hot vector $c\in c_{i,...,k}$,
    which represents a desired class. Meanwhile, the discriminator $D$ learns faster
    if the input image is supplied together with the class it belongs to. Thus, the
    optimization of a conditional GAN with class labels can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (16) |  | $\displaystyle\min_{\phi}\max_{\theta}$ | $\displaystyle=E_{x,c\sim
    P_{data}}[\log D_{\theta}(x,c)]$ |  |'
  prefs: []
  type: TYPE_TB
- en: '|  |  | $\displaystyle+E_{z\sim P_{z},c\sim P_{c}}[\log(1-D_{\theta}(G_{\phi}(z,c),c))],$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $\phi$ and $\theta$ are trainable parameters for $G_{\phi}$ and $D_{\theta}$,
    the generator inputs $z$ and $c$ are the latent variables drawn from their prior
    distribution $P(z)$ and $P(c)$. For each generated sample, if the class with the
    highest value is different from the pre-trained classifier, it will be marked
    as ”unknown”. Finally, a final classifier provides an explicit estimated probability
    for generated unknown classes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2\. Adversarial sample generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Yu et al. ([2017](#bib.bib56)) proposed Adversarial Sample Generation (ASG)
    as a data augmentation technique for the OSR problem. The idea is to generate
    some points closed to but different from the training instances as unknown labels,
    then straightforward to train an open-category classifier to identify seen from
    unseen. Moreover, ASG also generates ”unknown” samples, which are close to ”known”
    samples. Different from the GANs min-max strategy, ASG generated samples based
    on distances and distributions, the generated unknown samples are:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: close to the seen class data
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: scattered around the known/unknown boundary
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2.3\. Counterfactual image generation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Different from standard GANs, Neal et al. ([2018](#bib.bib37)) proposed a dataset
    augmentation technique called counterfactual image generation, which adopts an
    encoder-decoder architecture to generate synthetic images closed to the real image
    but not in any known classes, then take them as unknown class. The architecture
    consists of three components:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An encoder network: maps from images to a latent space.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A generator: maps from latent space back to an image.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A discriminator: discriminates generated images from real images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 3.2.4\. GAN-MDFM
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Jo et al. ([2018](#bib.bib26)) presented a new method to generate fake data
    for unknown unknowns. They proposed Marginal Denoising Autoencoder (MDAE) technique,
    which models the noise distribution of known classes in feature spaces with a
    margin is introduced to generate data similar to known classes but not the same
    ones. The model contains a classifier, a generator, and an autoencoder. The classifier
    calculated the entropy of membership probability instead of discriminating generated
    data from real data explicitly. Then, a threshold is used here to identify unknown
    classes. The generator modeled the distribution $m$ away from the known classes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.5\. Confident classifier
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to distinguish in-distribution and out-of-distribution samples, Lee
    et al. ([2018](#bib.bib30)) suggested two additional terms added to the original
    cross-entropy loss, where the first one (confident loss) forces out-of-distribution
    samples less confident by the classifier while the second one (adversarial generator)
    is for generating most effective training samples for the first one. Specifically,
    the proposed confident loss is to minimize the Kullback-Leibler (KL) divergence
    from the predictive distribution on out-of-distribution samples to the uniform
    one in order to achieve less confident predictions for samples from out-of-distribution.
    Meanwhile in- and out-of-distributions are expected to be more separable. Then,
    an adversarial generator is introduced to generate the most effective out-of-distribution
    samples. Unlike the original generative adversarial network(GAN), which generates
    samples similar to in-distribution samples, the proposed generator generates ”boundary”
    samples in the low-density area of in-distribution acting as out-of-distribution
    samples. Finally, a joint training scheme was designed to minimize both loss functions
    alternatively. Finally, the paper showed that the proposed GAN implicitly encourages
    training a more confident classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs: []
  type: TYPE_NORMAL
- en: Instead of borrowing data from other datasets, generating additional data methods
    generate unknown samples from the knowns. Most data generation methods are based
    on GANs. (Ge et al., [2017](#bib.bib14)) introduces a conditional GAN to generate
    some unknown samples followed by OpenMax open set classifier. (Yu et al., [2017](#bib.bib56))
    also uses the min-max strategy from GANs, generating data around the decision
    boundary between known and unknown samples as unknown. (Neal et al., [2018](#bib.bib37))
    adds another encoder network to traditional GANs to map from images to a latent
    space. (Jo et al., [2018](#bib.bib26)) generates unknown samples by marginal denoising
    autoencoder that provided a target distribution which is $m$ away from the distribution
    of known samples. (Lee et al., [2018](#bib.bib30)) generates ”boundary” samples
    in the low-density area of in-distribution acting as unknown samples, and jointly
    trains confident classifier and adversarial generator to make both models improve
    each other. Generating unknown samples for the OSR problem has achieved great
    performance, meanwhile, it requires more complex network architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5b6460b9a0915591b815facc955152c2.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: ii-loss
  prefs: []
  type: TYPE_NORMAL
- en: Figure 11\. Network Architecture of ii-loss ([Hassen and Chan](#bib.bib21),
    2018)
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. No additional data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The OSR techniques not requiring additional data in training can be divided
    to DNN-based ((Bendale and Boult, [2016](#bib.bib5)) (Hassen and Chan, [2018a](#bib.bib21))
    (Mao et al., [2018](#bib.bib35)) (Zhang and Patel, [2016](#bib.bib57)) (Liang
    et al., [2018](#bib.bib32))) and traditional ML-based ((Júnior et al., [2017](#bib.bib27))).
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.1\. Extreme Value Signatures
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Schultheiss et al. ([2017](#bib.bib47)) investigated class-specific activation
    patterns to leverage CNNs to novelty detection tasks. They introduced “extreme
    value signature”, which specifies which dimensions of deep neural activations
    have the largest value. They also assumed that a semantic category can be described
    by its signature. Thereby, a test example will be considered as novel if it is
    different from the extreme-value signatures of all known categories, They applied
    extreme value signatures on the top of existing models, which allow to “upgrade”
    arbitrary classification networks to jointly estimate novelty and class membership.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.2\. OpenMax
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Bendale and Boult ([2016](#bib.bib5)) proposed OpenMax which replaces the softmax
    layer in DNNs with an OpenMax layer, and the model estimates the probability of
    an input being from an unknown class. The model adopts the Extreme Value Theory
    (EVT) meta-recognition calibration in the penultimate layer of the networks. For
    each instance, the activation vector is revised to the sum of the product of its
    distance to the mean activation vectors (MAV) of each class. Further, it redistributes
    values of activation vector acting as activation for unknown class. Finally, the
    new redistributed activation vectors are used for computing the probabilities
    of both known and unknown classes.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.3\. ii-loss
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Hassen and Chan ([2018a](#bib.bib21)) proposed a distance-based loss function
    in DNNs in order to learn the representation for open set recognition. The idea
    is to maximize the distance between different classes (inter-class separation)
    and minimize distance of an instance from its class mean (intra-class spread).
    So that in the learned representation, instances from the same class are close
    to each other while those from different classes are further apart. More formally,
    let $\overrightarrow{z_{i}}$ be the the projection (embedding) of the input vector
    $\overrightarrow{x_{i}}$ of instance $i$. The intra class spread is measured as
    the average distance of instances from their class means:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (17) |  | $intra\_spread=\frac{1}{N}\sum_{j=1}^{K}\sum_{i=1}^{C_{j}}\&#124;\overrightarrow{\mu_{j}}-\overrightarrow{z_{i}}\&#124;_{2}^{2},$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $|C_{j}|$ is the number of training instances in class $C_{j}$, $N$ is
    the number of training instances, and $\mu_{j}$ is the mean of class $C_{j}$.
    Meanwhile, the inter class separation is measured as the closest two class means
    among all the $K$ known classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (18) |  | $inter\_separation=\min_{\begin{subarray}{c}1\leq m\leq K\\ m+1\leq
    n\leq K\end{subarray}}\&#124;\overrightarrow{\mu_{m}}-\overrightarrow{\mu_{n}}\&#124;_{2}^{2}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'The proposed ii-loss minimizes the intra-class spread and maximizes inter-class
    separation:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (19) |  | $ii\_loss=intra\_spread-inter\_separation$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'So that the distance between an instance and the closest known class mean can
    be used as a criterion of unknown class. i.e. if the distance above some threshold,
    the instance then be recognized as an unknown class. The network architecture
    as Figure [11](#S3.F11 "Figure 11 ‣ 3.2.5\. Confident classifier ‣ 3.2\. Generating
    additional data ‣ 3\. Open Set Recognition ‣ Deep Learning and Open Set Malware
    Classification: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.4\. Distribution networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Mao et al. ([2018](#bib.bib35)) assumed that through a certain mapping, all
    the classes followed different Gaussian distributions. They proposed a distributions
    parameters transfer strategy to detect and model the unknown classes through estimating
    those of known classes. Formally, let $\boldsymbol{z}_{i}^{k}$ denotes the embedding
    of $\boldsymbol{x}_{i}^{k}$, they assume samples from class $k$ follow a probability
    distribution $p_{k}(\boldsymbol{z};\boldsymbol{\Theta}_{k})$ with learnable parameters
    $\boldsymbol{\Theta}_{k}$ in the latent space. For class $k$, the log-likelihood
    is
  prefs: []
  type: TYPE_NORMAL
- en: '| (20) |  | $\log\mathcal{L}_{k}(\boldsymbol{\Theta}_{k},\boldsymbol{W})=\sum_{i=1}^{n_{k}}\log
    p_{k}(\boldsymbol{z}_{i}^{k};\boldsymbol{\Theta}_{k})$ |  |'
  prefs: []
  type: TYPE_TB
- en: The training objective is to make samples more likely to belong to their labeled
    class. i.e, maximize the log-likelihood of each class with respect to their samples.
    Hence the negative mean log-likelihood is used as a loss function in the proposed
    distribution networks.
  prefs: []
  type: TYPE_NORMAL
- en: '| (21) |  | $J(\boldsymbol{W},\boldsymbol{\Theta})=-\sum_{k=1}^{l}\frac{1}{n_{k}}\log\mathcal{L}_{k}(\boldsymbol{\Theta}_{k},\boldsymbol{W})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The method can not only detect novel samples but also differentiate and model
    unknown classes, hence discover new patterns or even new knowledge in the real
    world.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.5\. OSNN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Besides DNNs, Júnior et al. ([2017](#bib.bib27)) introduced OSNN as an extension
    for the traditional machine learning technique - Nearest Neighbors(NN) classifier.
    It applies the Nearest Neighbors Distance Ratio (NNDR) technique as a threshold
    on the ratio of similarity scores. Specifically, it measures the ratio of the
    distances between a sample and its nearest neighbors in two different known classes.
    And assign the sample to one of the class if the ratio is below a certain threshold.
    And samples who are ambiguous between classes (ratio above a certain threshold)
    and those faraway from any unknown class are classified as unknown.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.6\. RLCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Wang et al. ([2018](#bib.bib53)) proposed a pairwise-constraint loss (PCL)
    function to achieve “intra-class compactness” and “inter-class separation” in
    order to address OSR problem. They also developed a two-channel co-representation
    framework to detect novel class over time. In addition to which, they added a
    Frobenius regularization term to avoid over-fitting. Their model also applied
    binary classification error(BCE) at the final output layer to form the entire
    loss function. Moreover, they applied temperature scaling and t distribution assumptions
    to find the optimal threshold, which requires fewer parameters. The two-channel
    co-representation framework looks like figure [12](#S3.F12 "Figure 12 ‣ 3.3.6\.
    RLCN ‣ 3.3\. No additional data ‣ 3\. Open Set Recognition ‣ Deep Learning and
    Open Set Malware Classification: A Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d0b72036faa288fb7dbfe7973fa60421.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: RLCN
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12\. Overview of the RLCN Framework ([Wang et al.](#bib.bib53), ([2018](#bib.bib53)))
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.7\. SROSR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Zhang and Patel ([2016](#bib.bib57)) proposed a generalized Sparse Recognition
    based Classification (SRC) algorithm for open set recognition in. The algorithm
    uses class reconstruction errors for classification. It models the tail of those
    two error distributions using the statistical Extreme Value Theory (EVT), then
    simplifies the open set recognition problem into a set of hypothesis testing problems.
    Figure [13](#S3.F13 "Figure 13 ‣ 3.3.7\. SROSR ‣ 3.3\. No additional data ‣ 3\.
    Open Set Recognition ‣ Deep Learning and Open Set Malware Classification: A Survey")
    gives an overview of the proposed SROSR algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/a689915c1037bfe1ab4069b1d302b778.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: SROSR
  prefs: []
  type: TYPE_NORMAL
- en: Figure 13\. Overview of the SROSR framework ([Zhang and Patel](#bib.bib57),
    ([2016](#bib.bib57)))
  prefs: []
  type: TYPE_NORMAL
- en: The algorithm consists of two main stages. In the first stage, given training
    samples, SROSR models tail part of the matched reconstruction error distribution
    and the sum of non-matched reconstruction error using the EVT. In the second stage,
    the modeled distributions and the matched and the non-matched reconstruction errors
    are used to calculate the confidence scores for test samples. Then these scores
    are fused to obtain the final score for recognition.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.8\. ODIN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Liang et al. ([2018](#bib.bib32)) proposed ODIN, an out-of-distribution detector,
    for solving the problem of detecting out-of-distribution images in neural networks.
    The proposed method does not require any change to a pre-trained neural network.
    The detector is based on two components: temperature scaling and input pre-processing.
    Specifically, ODIN set a temperature scaling parameter in original softmax output
    for each class like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (22) |  | $S_{i}(\mathbf{x};T)=\frac{\exp(f_{i}(\mathbf{x}/T))}{\sum_{j=1}^{N}\exp(f_{j}(\mathbf{x})/T)}$
    |  |'
  prefs: []
  type: TYPE_TB
- en: ODIN used the maximum softmax probability softmax score, the temperature scaling
    can push the softmax scores of in- and out-of-distribution images further apart
    from each other, making the out-of-distribution images distinguishable. Meanwhile,
    small perturbations were added to the input during pre-processing to make in-
    distribution images and out-of-distribution images more separable.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3.9\. DOC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To address open classification problem, [Shu et al.](#bib.bib48) proposed Deep
    Open Classification (DOC) method in (Shu et al., [2017](#bib.bib48)). DOC builds
    a multi-class classifier with a 1-vs-rest final layer of sigmoids rather than
    softmax to reduce the open space risk as Figure [14](#S3.F14 "Figure 14 ‣ 3.3.9\.
    DOC ‣ 3.3\. No additional data ‣ 3\. Open Set Recognition ‣ Deep Learning and
    Open Set Malware Classification: A Survey"). Specifically, the 1-vs-rest layer
    contains one sigmoid function for each class. And the objective function is the
    summation of all log loss of the sigmoid functions:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (23) |  | $Loss=\sum_{i=1}^{m}\sum_{j=1}^{n}-\mathbb{I}(y_{j}=l_{i})\log
    p(y_{j}=l_{i})-\mathbb{I}(y_{j}\neq l_{i})\log(1-p(y_{j}=l_{i}))$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathbb{I}$ is the indicator function and $p(y_{j}=l_{i})=Sigmoid(d_{j,i})$
    is the probability output from $i$th sigmoid function ($i$th class) on the $j$th
    document’s $i$th dimension of $d$.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0578644e2cf74eca9427d51680a5d9b8.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: DOC
  prefs: []
  type: TYPE_NORMAL
- en: Figure 14\. Overview of DOC framework ([Shu et al.](#bib.bib48), ([2017](#bib.bib48)))
  prefs: []
  type: TYPE_NORMAL
- en: DOC also borrows the idea of outlier detection in statistics to reduce the open
    space risk further for rejection by tightening the decision boundaries of sigmoid
    functions with Gaussian fitting. It fits the predicted probability for all training
    data of each class, then estimates the standard deviation to find the classification
    thresholds for each different class.
  prefs: []
  type: TYPE_NORMAL
- en: Discussion
  prefs: []
  type: TYPE_NORMAL
- en: 'The above papers manage to solve the OSR problems without additional datasets,
    and some of them adopt similar ideas as in Table [2](#S3.T2 "Table 2 ‣ 3.3.9\.
    DOC ‣ 3.3\. No additional data ‣ 3\. Open Set Recognition ‣ Deep Learning and
    Open Set Malware Classification: A Survey"). (Schultheiss et al., [2017](#bib.bib47)),
    (Bendale and Boult, [2016](#bib.bib5)) and (Zhang and Patel, [2016](#bib.bib57))
    utilize EVT to distinguish unknown class and known classes. (Hassen and Chan,
    [2018a](#bib.bib21)) and (Wang et al., [2018](#bib.bib53)) design different distance-based
    loss functions to achieve “intra-class compactness” and “inter-class separation”.
    Some technologies are applied in different ways: (Wang et al., [2018](#bib.bib53))
    uses temperature scaling to find the threshold of outliers, while (Liang et al.,
    [2018](#bib.bib32)) uses temperature scaling in softmax output. (Mao et al., [2018](#bib.bib35))
    assumes all the classes followed different Gaussian distributions, while (Shu
    et al., [2017](#bib.bib48)) tightens the decision boundaries of sigmoid functions
    with Gaussian fitting. In general, not using an additional dataset requires the
    networks generating more precise representations for known classes. Other than
    DNN, (Júnior et al., [2017](#bib.bib27)) introduces an extension for the Nearest
    Neighbors classifier.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Similarities and Differences of OSR Techniques without Additional
    Data
  prefs: []
  type: TYPE_NORMAL
- en: '| Ideas | (Schultheiss et al., [2017](#bib.bib47)) | (Bendale and Boult, [2016](#bib.bib5))
    | (Hassen and Chan, [2018a](#bib.bib21)) | (Mao et al., [2018](#bib.bib35)) |
    (Júnior et al., [2017](#bib.bib27)) | (Wang et al., [2018](#bib.bib53)) | (Zhang
    and Patel, [2016](#bib.bib57)) | (Liang et al., [2018](#bib.bib32)) | (Shu et al.,
    [2017](#bib.bib48)) |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| DNN | x | x | x | x |  | x | x | x | x |'
  prefs: []
  type: TYPE_TB
- en: '| EVT | x | x |  |  |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| SRC |  |  |  |  |  |  | x |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Distance-based activation vector |  | x |  |  |  |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Distance-based loss function |  |  | x |  |  | x |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: '| Gaussian distribution |  |  |  | x |  |  |  |  | x |'
  prefs: []
  type: TYPE_TB
- en: '| Temperature scaling |  |  |  |  |  | x |  | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| Input perturbations |  |  |  |  |  |  |  | x |  |'
  prefs: []
  type: TYPE_TB
- en: '| 1-vs-rest |  |  |  |  |  |  |  |  | x |'
  prefs: []
  type: TYPE_TB
- en: '| Nearest neighbors |  |  |  |  | x |  |  |  |  |'
  prefs: []
  type: TYPE_TB
- en: 4\. Learning graph representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hamilton et al. ([2017b](#bib.bib19)) provided a review of techniques in representation
    learning on graphs, which including matrix factorization-based methods, random-walk
    based algorithms ad graph network.
  prefs: []
  type: TYPE_NORMAL
- en: The paper introduced methods for vertex embedding and subgraph embedding. The
    vertex embedding can be viewed as encoding nodes into a latent space from an encoder-decoder
    perspective. The goal of subgraph embedding is to encode a set of nodes and edges,
    which is a continuous vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Vertex embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Vertex embedding can be organized as an encoder-decoder framework. An encoder
    maps each node to a low-dimensional vector or embedding. And decoder decodes structural
    information about the graph from the learned embeddings. Adopting the encoder-decoder
    perspective, there are four methodological components for the various node embedding
    methods (Hamilton et al., [2017b](#bib.bib19)):'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A pairwise similarity function, which measures the similarity between nodes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An encoder function, which generates the node embeddings
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A decoder function, which reconstructs pairwise similarity values from the generated
    embeddings
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A loss function, which determines how the quality of the pairwise reconstructions
    is evaluated to train the model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The majority of node embedding algorithm reply on shallow embedding, whose encoder
    function just maps nodes to vector embedding. However, these shallow embedding
    vectors have some drawbacks.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: No parameters are shared between nodes in the encoder, which makes it computationally
    inefficient.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shallow embedding fails to leverage node attributes during encoding.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shallow embedding can only generate embeddings for nodes that were present during
    the training phase, cannot generate embeddings for previously unseen nodes without
    additional rounds of optimization.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Recently, several deep neural network-based approaches have been proposed to
    address the above issues. They used autoencoders to compress information about
    a node’s local neighborhood.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1\. GCN
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In the work of Graph Convolutional Networks (GCN) (Kipf and Welling, [2017](#bib.bib28)),
    [Kipf and Welling](#bib.bib28) presented encoded the graph structure directly
    using a neural network model and trained on a supervised target. The adjacency
    matrix of the graph will then allow the model to distribute gradient information
    from the supervised loss and will enable it to learn representations of nodes
    both with and without labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper first introduces a simple and well-behaved layer-wise propagation
    rule for neural network models which operate directly on graphs as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (24) |  | $H^{(l+1)}=\sigma\left(\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}H^{(l)}W^{(l)}\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Where $A$ is the adjacency matrix of the undirected graph, $I_{N}$ is identity
    matrix. $\tilde{A}$ is the adjacency matrix with added self-connections with added
    self-connections. $\tilde{D}_{ii}=\sum_{j}\tilde{A}_{ij}$ and $W^{(l)}$ is a layer-specific
    trainable weight matrix. $\sigma(\cdot)$ denotes an activation function and $H^{l}\in\mathbf{R}^{N\times
    D}$ is the matrix of activation functions in the $l^{th}$ layer. Considering a
    two-layer GCN as semi-supervised node classificaiton example. The pre-processing
    step calculates $\hat{A}=\tilde{D}^{-\frac{1}{2}}\tilde{A}\tilde{D}^{-\frac{1}{2}}$,
    then the forward model takes the simple form:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (25) |  | $Z=f(X,A)=\text{softmax}(\hat{A}\text{ ReLU}(\hat{A}XW^{(0)})W^{(1)})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'Here, $W^{(0)}\in\mathbf{R}^{C\times H}$ is an input-to-hidden weight matrix
    for a hidden layer with $H$ feature maps. $W^{(1)}\in\mathbf{R}^{H\times F}$ is
    a hidden-to-output weight matrix. For semi-supervised multi-class classification,
    the cross entropy error is evaluated over all labeled examples:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (26) |  | $\mathcal{L}=-\sum_{l\in\mathcal{Y}_{L}}\sum_{f=1}^{F}Y_{lf}\ln
    Z_{lf}$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{Y}_{L}$ is the set of node indices that have labels.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/7c17a63af33573989c0ac222818b834e.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: graphSAGE
  prefs: []
  type: TYPE_NORMAL
- en: Figure 15\. Visual illustration of the GraphSAGE sample and aggregate approach
    ([Hamilton et al.](#bib.bib18), ([2017a](#bib.bib18)))
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2\. GraphSAGE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Hamilton et al. presented GraphSAGE in (Hamilton et al., [2017a](#bib.bib18)),
    a general inductive framework that leverages node feature information to efficiently
    generate node embeddings for previously unseen data. GraphSAGE can learn a function
    that generates embeddings by sampling and aggregating features from a node’s local
    neighborhood as Figure [15](#S4.F15 "Figure 15 ‣ 4.1.1\. GCN ‣ 4.1\. Vertex embedding
    ‣ 4\. Learning graph representation ‣ Deep Learning and Open Set Malware Classification:
    A Survey"). Instead of training individual embeddings for each node, a set of
    aggregator functions are learned to aggregate feature information from a node’s
    local neighborhood from a different number of hops away from a given node, for
    example, for aggregator function $k$ we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (27) |  | $h_{\mathcal{N}(v)}^{k}\leftarrow\text{AGGREGATE}_{k}(\{h_{u}^{k-1},\forall
    u\in\mathcal{N}(v)\}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $h$ is representation vector, $v$ is input node, $\mathcal{N}$ is neighborhood
    function. GraphSAGE then concatenates the node’s current representation, $h_{v}^{k-1}$,
    with the aggregated neighborhood vector. $h_{\mathcal{N}(v)}^{k-1}$, and this
    concatenated vector is fed through a fully connected layer with nonlinear activation
    function $\sigma$ like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (28) |  | $h_{v}^{k}\leftarrow\sigma\left(\mathbf{W}^{k}\cdot\text{CONCAT}(h_{v}^{k-1},h_{\mathcal{N}(v)}^{k})\right)$
    |  |'
  prefs: []
  type: TYPE_TB
- en: The learned aggregation functions are then applied to the entire unseen nodes
    to generate embeddings during the test phase.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.3\. LINE
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Tang et al. proposed a method for Large-scale Information Network Embedding:
    Line in (Tang et al., [2015](#bib.bib52)), which is suitable for undirected, directed
    and/or weighted networks. The model optimizes an objective which preserves both
    the local and global network structures. The paper explores both first-order and
    second-order proximity between the vertices. Most existing graph embedding are
    designed to preserve first-order proximity, which is presented by observed links
    like vertex 6 and 7 in Figure [16](#S4.F16 "Figure 16 ‣ 4.1.3\. LINE ‣ 4.1\. Vertex
    embedding ‣ 4\. Learning graph representation ‣ Deep Learning and Open Set Malware
    Classification: A Survey"), the objective function to preserve first-order proximity
    looks like:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (29) |  | $O_{1}=-\sum_{(i,j)\in E}w_{ij}\log p_{1}(v_{i},v_{j}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $p_{1}$ the joint probability between two vetices and is only valid for
    undirected edge $(i,j)$. Besides, LINE explores the second-order proximity between
    the vertices, which is not determined through the observed tie strength but through
    the shared neighborhood structures of the vertices, such as vertex 5 and 6 should
    also be placed close as they share similar neighbors. In second-order proximity,
    each vertex is treated as a specific ”context” and vertices with similar distributions
    over the ”contexts” are assumed to be similar. To preserve the second-order proximity,
    LINE minimize the following objective function:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (30) |  | $O_{2}=-\sum_{(i,j)\in E}w_{ij}\log p_{2}(v_{j}&#124;v_{i}),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $p_{2}$ is defined as the probability of ”context” $v_{j}$ generated by
    vertex $v_{i}$ for each directed edge $(i,j)$.
  prefs: []
  type: TYPE_NORMAL
- en: The functions preserved first-order proximity and second-order proximity are
    trained separately and the embeddings trained by two methods are concatenated
    for each vertex.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c4717a4a313663f17e30e6b0ecb80d52.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: Line
  prefs: []
  type: TYPE_NORMAL
- en: Figure 16\. A toy example of information network ([Tang et al.](#bib.bib52),
    ([2015](#bib.bib52)))
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.4\. JK-Net
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In order to overcome the limitations of neighborhood aggregation schemes, Xu
    et. al proposed Jumping Knowledge (JK) Networks strategy in (Xu et al., [2018](#bib.bib55))
    that flexibly leverages different neighborhood ranges to enable better structure-aware
    representation for each node. This architecture selectively combines different
    aggregations at the last layer, i.e., the representations “jump” to the last layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8aaa08e70777c61632a48e91347041ab.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: JK-Net
  prefs: []
  type: TYPE_NORMAL
- en: Figure 17\. Illustration of a 4-layer JK-Net. N.A. stands for neighborhood aggregation
    ([Xu et al.](#bib.bib55), ([2018](#bib.bib55)))
  prefs: []
  type: TYPE_NORMAL
- en: 'The main idea of JK-Net is illustrated as Figure [17](#S4.F17 "Figure 17 ‣
    4.1.4\. JK-Net ‣ 4.1\. Vertex embedding ‣ 4\. Learning graph representation ‣
    Deep Learning and Open Set Malware Classification: A Survey"): as in common neighborhood
    aggregation networks, each layer increases the size of the influence distribution
    by aggregating neighborhoods from the previous layer. At the last layer, for each
    node, JK-Net selects from all of those intermediate representations (which “jump”
    to the last layer), potentially combining a few. If this is done independently
    for each node, then the model can adapt the effective neighborhood size for each
    node as needed, resulting in exactly the desired adaptivity. As a more general
    framework, JK-Net admits general layer-wise aggregation models and enable better
    structure-aware representations on graphs with complex structures.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c878d05f3a357bae69807b43cf22f9aa.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: DNGR
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18\. Main components of DNGR: random surfing, PPMI and SDAE ([Cao et al.](#bib.bib7),
    ([2016](#bib.bib7)))'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.5\. DNGR
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In (Cao et al., [2016](#bib.bib7)), Cao et al. adopted a random surfing model
    to capture graph structural information directly instead of using a sampling-based
    method. As illustrated in Figure [18](#S4.F18 "Figure 18 ‣ 4.1.4\. JK-Net ‣ 4.1\.
    Vertex embedding ‣ 4\. Learning graph representation ‣ Deep Learning and Open
    Set Malware Classification: A Survey"), the proposed DNGR model contains three
    major components: random surfing, calculation of PPMI matrix and feature reduction
    by SDAE. The random surfing model is motivated by the PageRank model and is used
    to capture graph structural information and generate a probabilistic co-occurrence
    matrix.'
  prefs: []
  type: TYPE_NORMAL
- en: Random surfing first randomly orders the vertices in a graph and assume there
    is a transition matrix that captures the transition probabilities between different
    vertices. The proposed random surfing model allows contextual information to be
    weighted differently based on their distance to target. The generated co-occurrence
    matrix then used to calculate PPMI matrix (an improvement for pointwise mutual
    information PMI, details in (Levy and Goldberg, [2014](#bib.bib31))). Next, as
    high dimensional input data often contain redundant information and noise stacked
    denoising autoencoder (SDAE) is used to enhance the robustness of DNN, denoising
    autoencoder partially corrupt the input data before taking the training step.
    Specifically, it corrupts each input sample x randomly by assigning some of the
    entries in the vector to 0 with a certain probability.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Subgraph embedding
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of embedding subgraphs is to encode a set of nodes and edges into a
    low-dimensional vector embedding. Representation learning on subgraphs is closely
    related to the design of graph kernels, which define a distance measure between
    subgraphs. According to (Hamilton et al., [2017b](#bib.bib19)), some subgraph
    embedding techniques use the convolutional neighborhood aggregation idea to generate
    embeddings for nodes then use additional modules to aggregate sets of node embeddings
    to subgraph, such as sum-based approaches, graph-coarsening approaches. Besides,
    there is some related work on ”graph neural networks” (GNN). Instead of aggregating
    information from neighbors, GNN uses backpropagation ”passing information” between
    nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Malware Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 5.0.1\. FCG
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Hassen and Chan ([2017](#bib.bib20)) proposed a linear time function call graph
    representation (FCG) vector representation. It starts with an FCG extraction module,
    which is a directed graph representation of code where the vertices of the graph
    correspond to functions and the directed edges represent the caller-callee relation
    between the function nodes. This module takes disassembled malware binaries and
    extract FCG representations. Thus they presented the caller-callee relation between
    functions as directed, unweighted edges. The next module is the function clustering.
    The algorithm used minhash to approximate Jaccard Index, to cluster functions
    of the given graph. The following module is vector extraction. The algorithm extracted
    vector representation from an FCG labeled using the cluster-ids. The representation
    consists of two parts, vertex weight, and edge weight. The vertex weight specifies
    the number of vertices in each cluster for that FCG and the edge weight describes
    the number of times an edge is found from one cluster to another cluster. The
    example workflow looks like figure [19](#S5.F19 "Figure 19 ‣ 5.0.1\. FCG ‣ 5\.
    Malware Classification ‣ Deep Learning and Open Set Malware Classification: A
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/5a47301c420802724296e6ca69de3d81.png)\Description'
  prefs: []
  type: TYPE_NORMAL
- en: fcg
  prefs: []
  type: TYPE_NORMAL
- en: Figure 19\. FCG Example ([Hassen and Chan](#bib.bib22), 2018)
  prefs: []
  type: TYPE_NORMAL
- en: 5.0.2\. COW, COW PC
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Based on the work in (Hassen and Chan, [2017](#bib.bib20)), Hassen and Chan
    ([2018b](#bib.bib22)) further introduced two new features: $P_{max}$, which is
    the maximum predicted class probability for one instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (31) |  | $P_{max}=\max_{c\in C^{k}}Pr(y_{i}=c&#124;\vec{x_{i}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'And the entropy for probability distribution over classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (32) |  | $entropy(p)=-\sum_{j}^{&#124;C^{k}&#124;}p_{j}logp_{j}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The paper also introduced two algorithms: Classification in an Open World (COW)
    and COW PC. Both consist of two classifiers: outlier detector $M_{outlier}$ and
    multi-class classifier. The difference is in COW, the outlier detector was trained
    by all the classes. And during testing, test data will go through outlier detector
    first, if it is recognized as not outlier, then it will be sent in a multi-class
    classifier. While COW PC has a class-specific outlier detector, i.e. each class
    has its own outlier detector. The test data will come through a multi-class classifier
    first, then will be sent into the corresponding outlier detector afterward.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.0.3\. Random projections
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Malware classifiers often use sparse binary features, and there can be hundreds
    of millions of potential features. In (Dahl et al., [2013](#bib.bib10)), Dahl
    et al. used random projections to reduce the dimensionality of the original input
    space of neural networks. They first extracted three types of features including
    null-terminated patterns observed in the process’ memory, tri-grams of system
    API calls, and distinct combinations of a single system API call and one input
    parameter, next performed feature selection, ended with generating over 179 thousand
    sparse binary features. To make the problem more manageable, they projected each
    input vector into a much lower dimensional space using a sparse project matrix
    with entries sample iid from a distribution over ${0,1,-1}$. Entries of 1 and
    -1 are equiprobable and $P(R_{ij}=0)=1-\frac{1}{\sqrt{d}}$, where $d$ is the original
    input dimensionality. The lower-dimensional data then serves as input to the neural
    network.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We provide a brief introduction of several deep neural network structures, and
    an overview of existing OSR, a discussion on learning graph representation and
    malware classification in this survey. It can be seen that those topics are advancing
    and profiting from each other in different areas. Also, despite the achieved great
    success, there are still serious challenges and great potential for them.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: DBL (2018) 2018. *6th International Conference on Learning Representations,
    ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings*.
    OpenReview.net. [https://openreview.net/group?id=ICLR.cc/2018/Conference](https://openreview.net/group?id=ICLR.cc/2018/Conference)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bahdanau et al. (2015) Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. 2015.
    Neural Machine Translation by Jointly Learning to Align and Translate. In *3rd
    International Conference on Learning Representations, ICLR 2015, San Diego, CA,
    USA, May 7-9, 2015, Conference Track Proceedings*, Yoshua Bengio and Yann LeCun
    (Eds.). [http://arxiv.org/abs/1409.0473](http://arxiv.org/abs/1409.0473)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bayer et al. (2006) Ulrich Bayer, Andreas Moser, Christopher Kruegel, and Engin
    Kirda. 2006. Dynamic analysis of malicious code. *Journal in Computer Virology*
    2, 1 (2006), 67–77.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bendale and Boult (2016) Abhijit Bendale and Terrance E Boult. 2016. Towards
    open set deep networks. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition*. 1563–1572.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cao et al. (2015) Chunshui Cao, Xianming Liu, Yi Yang, Yinan Yu, Jiang Wang,
    Zilei Wang, Yongzhen Huang, Liang Wang, Chang Huang, Wei Xu, et al. 2015. Look
    and think twice: Capturing top-down visual attention with feedback convolutional
    neural networks. In *Proceedings of the IEEE International Conference on Computer
    Vision*. 2956–2964.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2016) Shaosheng Cao, Wei Lu, and Qiongkai Xu. 2016. Deep neural
    networks for learning graph representations. In *Thirtieth AAAI Conference on
    Artificial Intelligence*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen (2016) Gang Chen. 2016. A gentle tutorial of recurrent neural network with
    error backpropagation. *arXiv preprint arXiv:1610.02583* (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cho et al. (2014) Kyunghyun Cho, Bart van Merrienboer, Çaglar Gülçehre, Dzmitry
    Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. 2014. Learning Phrase
    Representations using RNN Encoder-Decoder for Statistical Machine Translation.
    In *Proceedings of the 2014 Conference on Empirical Methods in Natural Language
    Processing, EMNLP 2014, October 25-29, 2014, Doha, Qatar, A meeting of SIGDAT,
    a Special Interest Group of the ACL*, Alessandro Moschitti, Bo Pang, and Walter
    Daelemans (Eds.). ACL, 1724–1734. [https://www.aclweb.org/anthology/D14-1179/](https://www.aclweb.org/anthology/D14-1179/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dahl et al. (2013) George E Dahl, Jack W Stokes, Li Deng, and Dong Yu. 2013.
    Large-scale malware classification using random projections and neural networks.
    In *2013 IEEE International Conference on Acoustics, Speech and Signal Processing*.
    IEEE, 3422–3426.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Deng et al. (2019) Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou.
    2019. Arcface: Additive angular margin loss for deep face recognition. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 4690–4699.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dhamija et al. (2018) Akshay Raj Dhamija, Manuel Günther, and Terrance Boult.
    2018. Reducing network agnostophobia. In *Advances in Neural Information Processing
    Systems*. 9157–9168.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finn et al. (2017) Chelsea Finn, Pieter Abbeel, and Sergey Levine. 2017. Model-agnostic
    meta-learning for fast adaptation of deep networks. In *Proceedings of the 34th
    International Conference on Machine Learning-Volume 70*. JMLR. org, 1126–1135.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ge et al. (2017) Zongyuan Ge, Sergey Demyanov, and Rahil Garnavi. 2017. Generative
    OpenMax for Multi-Class Open Set Classification. In *British Machine Vision Conference
    2017, BMVC 2017, London, UK, September 4-7, 2017*. BMVA Press. [https://www.dropbox.com/s/7hlycn200phni9j/0104.pdf?dl=1](https://www.dropbox.com/s/7hlycn200phni9j/0104.pdf?dl=1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geng et al. (2018) Chuanxing Geng, Sheng-jun Huang, and Songcan Chen. 2018.
    Recent Advances in Open Set Recognition: A Survey. *arXiv preprint arXiv:1811.08581*
    (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gers et al. (1999) Felix A Gers, Jürgen Schmidhuber, and Fred Cummins. 1999.
    Learning to forget: Continual prediction with LSTM. (1999).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2014) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing
    Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. 2014.
    Generative adversarial nets. In *Advances in neural information processing systems*.
    2672–2680.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hamilton et al. (2017a) Will Hamilton, Zhitao Ying, and Jure Leskovec. 2017a.
    Inductive representation learning on large graphs. In *Advances in Neural Information
    Processing Systems*. 1024–1034.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hamilton et al. (2017b) William L. Hamilton, Rex Ying, and Jure Leskovec. 2017b.
    Representation Learning on Graphs: Methods and Applications. *IEEE Data Eng. Bull.*
    40, 3 (2017), 52–74. [http://sites.computer.org/debull/A17sept/p52.pdf](http://sites.computer.org/debull/A17sept/p52.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassen and Chan (2017) Mehadi Hassen and Philip K Chan. 2017. Scalable function
    call graph-based malware classification. In *Proceedings of the Seventh ACM on
    Conference on Data and Application Security and Privacy*. ACM, 239–248.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hassen and Chan (2018a) Mehadi Hassen and Philip K Chan. 2018a. Learning a neural-network-based
    representation for open set recognition. *arXiv preprint arXiv:1802.04365* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hassen and Chan (2018b) Mehadi Hassen and Philip K Chan. 2018b. Learning to
    Identify Known and Unknown Classes: A Case Study in Open World Malware Classification.
    In *The Thirty-First International Flairs Conference*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2016) Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016.
    Deep residual learning for image recognition. In *Proceedings of the IEEE conference
    on computer vision and pattern recognition*. 770–778.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. (2018) Xinwei He, Yang Zhou, Zhichao Zhou, Song Bai, and Xiang Bai.
    2018. Triplet-center loss for multi-view 3d object retrieval. In *Proceedings
    of the IEEE Conference on Computer Vision and Pattern Recognition*. 1945–1954.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hendrycks et al. (2019) Dan Hendrycks, Mantas Mazeika, and Thomas G. Dietterich.
    2019. Deep Anomaly Detection with Outlier Exposure. In *7th International Conference
    on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019*.
    OpenReview.net. [https://openreview.net/forum?id=HyxCxhRcY7](https://openreview.net/forum?id=HyxCxhRcY7)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jo et al. (2018) Inhyuk Jo, Jungtaek Kim, Hyohyeong Kang, Yong-Deok Kim, and
    Seungjin Choi. 2018. Open Set Recognition by Regularising Classifier with Fake
    Data Generated by Generative Adversarial Networks. In *2018 IEEE International
    Conference on Acoustics, Speech and Signal Processing (ICASSP)*. IEEE, 2686–2690.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Júnior et al. (2017) Pedro R Mendes Júnior, Roberto M de Souza, Rafael de O
    Werneck, Bernardo V Stein, Daniel V Pazinato, Waldir R de Almeida, Otávio AB Penatti,
    Ricardo da S Torres, and Anderson Rocha. 2017. Nearest neighbors distance ratio
    open-set classifier. *Machine Learning* 106, 3 (2017), 359–386.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kipf and Welling (2017) Thomas N. Kipf and Max Welling. 2017. Semi-Supervised
    Classification with Graph Convolutional Networks. In *5th International Conference
    on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference
    Track Proceedings*. OpenReview.net. [https://openreview.net/forum?id=SJU4ayYgl](https://openreview.net/forum?id=SJU4ayYgl)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. (2015) Yann LeCun, Yoshua Bengio, and Geoffrey Hinton. 2015. Deep
    learning. *nature* 521, 7553 (2015), 436–444.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lee et al. (2018) Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. 2018.
    Training Confidence-calibrated Classifiers for Detecting Out-of-Distribution Samples,
    See DBL ([2018](#bib.bib2)). [https://openreview.net/forum?id=ryiAv2xAZ](https://openreview.net/forum?id=ryiAv2xAZ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levy and Goldberg (2014) Omer Levy and Yoav Goldberg. 2014. Neural word embedding
    as implicit matrix factorization. In *Advances in neural information processing
    systems*. 2177–2185.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2018) Shiyu Liang, Yixuan Li, and R. Srikant. 2018. Enhancing
    The Reliability of Out-of-distribution Image Detection in Neural Networks, See
    DBL ([2018](#bib.bib2)). [https://openreview.net/forum?id=H1VGkIxRZ](https://openreview.net/forum?id=H1VGkIxRZ)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luong et al. (2015) Thang Luong, Hieu Pham, and Christopher D. Manning. 2015.
    Effective Approaches to Attention-based Neural Machine Translation. In *Proceedings
    of the 2015 Conference on Empirical Methods in Natural Language Processing, EMNLP
    2015, Lisbon, Portugal, September 17-21, 2015*, Lluís Màrquez, Chris Callison-Burch,
    Jian Su, Daniele Pighin, and Yuval Marton (Eds.). The Association for Computational
    Linguistics, 1412–1421. [https://www.aclweb.org/anthology/D15-1166/](https://www.aclweb.org/anthology/D15-1166/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Makhzani et al. (2015) Alireza Makhzani, Jonathon Shlens, Navdeep Jaitly, Ian
    Goodfellow, and Brendan Frey. 2015. Adversarial autoencoders. *arXiv preprint
    arXiv:1511.05644* (2015).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mao et al. (2018) Chengsheng Mao, Liang Yao, and Yuan Luo. 2018. Distribution
    Networks for Open Set Learning. arXiv:cs.LG/1809.08106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov et al. (2013) Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado,
    and Jeff Dean. 2013. Distributed representations of words and phrases and their
    compositionality. In *Advances in neural information processing systems*. 3111–3119.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neal et al. (2018) Lawrence Neal, Matthew Olson, Xiaoli Fern, Weng-Keen Wong,
    and Fuxin Li. 2018. Open set learning with counterfactual images. In *Proceedings
    of the European Conference on Computer Vision (ECCV)*. 613–628.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Olah (2015) Christopher Olah. 2015. Understanding LSTM Networks. [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington et al. (2014) Jeffrey Pennington, Richard Socher, and Christopher
    Manning. 2014. Glove: Global vectors for word representation. In *Proceedings
    of the 2014 conference on empirical methods in natural language processing (EMNLP)*.
    1532–1543.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Perera and Patel (2019) Pramuditha Perera and Vishal M Patel. 2019. Learning
    deep features for one-class classification. *IEEE Transactions on Image Processing*
    (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford et al. (2016) Alec Radford, Luke Metz, and Soumith Chintala. 2016. Unsupervised
    Representation Learning with Deep Convolutional Generative Adversarial Networks.
    In *4th International Conference on Learning Representations, ICLR 2016, San Juan,
    Puerto Rico, May 2-4, 2016, Conference Track Proceedings*, Yoshua Bengio and Yann
    LeCun (Eds.). [http://arxiv.org/abs/1511.06434](http://arxiv.org/abs/1511.06434)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ravi and Larochelle (2016) Sachin Ravi and Hugo Larochelle. 2016. Optimization
    as a model for few-shot learning. (2016).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real et al. (2017) Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena,
    Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. 2017. Large-scale
    evolution of image classifiers. In *Proceedings of the 34th International Conference
    on Machine Learning-Volume 70*. JMLR. org, 2902–2911.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro et al. (2016) Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016. Why should i trust you?: Explaining the predictions of any classifier. In
    *Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery
    and data mining*. ACM, 1135–1144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saha (2018) Sumit Saha. 2018. A Comprehensive Guide to Convolutional Neural
    Networks-the ELI5 way. [https://bit.ly/2Oa2YCh](https://bit.ly/2Oa2YCh)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saito et al. (2018) Kuniaki Saito, Shohei Yamamoto, Yoshitaka Ushiku, and Tatsuya
    Harada. 2018. Open set domain adaptation by backpropagation. In *Proceedings of
    the European Conference on Computer Vision (ECCV)*. 153–168.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schultheiss et al. (2017) Alexander Schultheiss, Christoph Käding, Alexander
    Freytag, and Joachim Denzler. 2017. Finding the unknown: Novelty detection with
    extreme value signatures of deep neural activations. In *German Conference on
    Pattern Recognition*. Springer, 226–238.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shu et al. (2017) Lei Shu, Hu Xu, and Bing Liu. 2017. DOC: Deep Open Classification
    of Text Documents. In *Proceedings of the 2017 Conference on Empirical Methods
    in Natural Language Processing, EMNLP 2017, Copenhagen, Denmark, September 9-11,
    2017*, Martha Palmer, Rebecca Hwa, and Sebastian Riedel (Eds.). Association for
    Computational Linguistics, 2911–2916. [https://www.aclweb.org/anthology/D17-1314/](https://www.aclweb.org/anthology/D17-1314/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shu et al. (2018b) Lei Shu, Hu Xu, and Bing Liu. 2018b. Unseen class discovery
    in open-world classification. *arXiv preprint arXiv:1801.05609* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shu et al. (2018a) Yu Shu, Yemin Shi, Yaowei Wang, Yixiong Zou, Qingsheng Yuan,
    and Yonghong Tian. 2018a. ODN: Opening the Deep Network for Open-Set Action Recognition.
    In *2018 IEEE International Conference on Multimedia and Expo (ICME)*. IEEE, 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Silva (2018) Thalles Silva. 2018. An intuitive introduction to Generative Adversarial
    Networks (GANs). [https://bit.ly/34dgUB2](https://bit.ly/34dgUB2)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tang et al. (2015) Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and
    Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In *Proceedings
    of the 24th international conference on world wide web*. International World Wide
    Web Conferences Steering Committee, 1067–1077.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Zhuoyi Wang, Zelun Kong, Hemeng Tao, Swarup Chandra, and
    Latifur Khan. 2018. Co-Representation Learning For Classification and Novel Class
    Detection via Deep Networks. *arXiv preprint arXiv:1811.05141* (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wen et al. (2016) Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. 2016.
    A discriminative feature learning approach for deep face recognition. In *European
    conference on computer vision*. Springer, 499–515.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2018) Keyulu Xu, Chengtao Li, Yonglong Tian, Tomohiro Sonobe, Ken-ichi
    Kawarabayashi, and Stefanie Jegelka. 2018. Representation Learning on Graphs with
    Jumping Knowledge Networks. In *Proceedings of the 35th International Conference
    on Machine Learning, ICML 2018, Stockholmsmässan, Stockholm, Sweden, July 10-15,
    2018* *(Proceedings of Machine Learning Research)*, Jennifer G. Dy and Andreas
    Krause (Eds.), Vol. 80. PMLR, 5449–5458. [http://proceedings.mlr.press/v80/xu18c.html](http://proceedings.mlr.press/v80/xu18c.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2017) Yang Yu, Wei-Yang Qu, Nan Li, and Zimin Guo. 2017. Open Category
    Classification by Adversarial Sample Generation. In *Proceedings of the Twenty-Sixth
    International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne,
    Australia, August 19-25, 2017*, Carles Sierra (Ed.). ijcai.org, 3357–3363. [https://doi.org/10.24963/ijcai.2017/469](https://doi.org/10.24963/ijcai.2017/469)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang and Patel (2016) He Zhang and Vishal M Patel. 2016. Sparse representation-based
    open set recognition. *IEEE transactions on pattern analysis and machine intelligence*
    39, 8 (2016), 1690–1696.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
