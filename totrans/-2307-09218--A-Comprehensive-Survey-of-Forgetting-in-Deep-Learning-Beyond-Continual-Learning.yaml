- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:37:54'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2307.09218] A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual
    Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2307.09218](https://ar5iv.labs.arxiv.org/html/2307.09218)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Comprehensive Survey of Forgetting in
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep Learning Beyond Continual Learning
  prefs: []
  type: TYPE_NORMAL
- en: Zhenyi Wang, Enneng Yang, Li Shen, Heng Huang Zhenyi Wang and Heng Huang are
    with the Department of Computer Science, University of Maryland, College Park,
    MD 20742, USA.
  prefs: []
  type: TYPE_NORMAL
- en: 'E-mail: wangzhenyineu@gmail.com; heng@umd.edu Enneng Yang is with Northeastern
    University, China. E-mail: ennengyang@stumail.neu.edu.cn Li Shen is with JD Explore
    Academy, China. E-mail: mathshenli@gmail.com. Manuscript received July 15, 2023;
    revised July 15, 2023.'
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Forgetting refers to the loss or deterioration of previously acquired information
    or knowledge. While the existing surveys on forgetting have primarily focused
    on continual learning, forgetting is a prevalent phenomenon observed in various
    other research domains within deep learning. Forgetting manifests in research
    fields such as generative models due to generator shifts, and federated learning
    due to heterogeneous data distributions across clients. Addressing forgetting
    encompasses several challenges, including balancing the retention of old task
    knowledge with fast learning of new tasks, managing task interference with conflicting
    goals, and preventing privacy leakage, etc. Moreover, most existing surveys on
    continual learning implicitly assume that forgetting is always harmful. In contrast,
    our survey argues that forgetting is a double-edged sword and can be beneficial
    and desirable in certain cases, such as privacy-preserving scenarios. By exploring
    forgetting in a broader context, we aim to present a more nuanced understanding
    of this phenomenon and highlight its potential advantages. Through this comprehensive
    survey, we aspire to uncover potential solutions by drawing upon ideas and approaches
    from various fields that have dealt with forgetting. By examining forgetting beyond
    its conventional boundaries, in future work, we hope to encourage the development
    of novel strategies for mitigating, harnessing, or even embracing forgetting in
    real applications. A comprehensive list of papers about forgetting in various
    research fields is available at [https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning](https://github.com/EnnengYang/Awesome-Forgetting-in-Deep-Learning).
  prefs: []
  type: TYPE_NORMAL
- en: 'Index Terms:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Beneficial Forgetting, Harmful Forgetting, Memorization, Distribution Shift,
    Cross-Disciplinary Research
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Forgetting [[1](#bib.bib1)] refers to the phenomenon where previously acquired
    information or knowledge in a machine learning system degrades over time. In the
    early days of neural networks, the focus was primarily on training models on static
    datasets. Forgetting was not a significant concern in these settings since the
    models were trained and evaluated on fixed datasets. The concept of catastrophic
    forgetting was first formally introduced by McCloskey and Cohen [[1](#bib.bib1)].
    They demonstrated that neural networks when trained sequentially on different
    tasks, tend to forget previously learned tasks when new tasks are learned. This
    observation highlighted the need for addressing forgetting in sequential learning
    scenarios. Later, addressing the issue of forgetting was formalized as continual
    learning (CL). Nowadays, forgetting has garnered significant attention not only
    within the CL domain but also in the broader machine learning community, which
    has evolved into a fundamental problem in the field of machine learning as a whole.
  prefs: []
  type: TYPE_NORMAL
- en: Existing surveys on forgetting have primarily focused on CL [[2](#bib.bib2),
    [3](#bib.bib3), [4](#bib.bib4), [5](#bib.bib5), [6](#bib.bib6), [7](#bib.bib7),
    [8](#bib.bib8), [9](#bib.bib9)]. However, these surveys tend to concentrate solely
    on the harmful effects of forgetting and lack a comprehensive discussion on the
    topic. In contrast, our survey aims to provide a more holistic understanding of
    forgetting. We highlight its dual nature as a double-edged sword, emphasizing
    both its benefits and harms. Additionally, our survey extends beyond the scope
    of CL and covers the forgetting issue in various other domains, including foundation
    models, domain adaptation, meta-learning, test-time adaptation, generative models,
    reinforcement learning and federated learning. By doing so, we offer a comprehensive
    examination of forgetting that encompasses a broader range of contexts and applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE I: Harmful Forgetting: Comparisons among different problem settings.'
  prefs: []
  type: TYPE_NORMAL
- en: Problem Setting Goal Source of Forgetting Continual Learning learn non-stationary
    data distribution without forgetting previous knowledge data-distribution shift
    during training Foundation Model unsupervised learning on large-scale unlabeled
    data data-distribution shift in pre-training, fine-tuning Domain Adaptation adapt
    to target domain while maintaining performance on source domain target domain
    sequentially shift over time Test-time Adaptation mitigate the distribution gap
    between training and testing adaptation to the test data distribution during testing
    Meta Learning learn adaptable knowledge to new tasks incrementally meta-learn
    new classes / task-distribution shift Generative Model learn a generator to approximate
    real data distribution generator shift / data-distribution shift Reinforcement
    Learning maximize accumulate rewards state, action, reward and state transition
    dynamics shift Federated Learning decentralized training without sharing data
    model average; non-i.i.d data; data-distribution shift
  prefs: []
  type: TYPE_NORMAL
- en: 'TABLE II: Beneficial Forgetting: Comparisons among different problem settings.'
  prefs: []
  type: TYPE_NORMAL
- en: Problem Setting Goal Mitigate Overfitting mitigate memorization of training
    data through selective forgetting Debias and Forget Irrelevant Information forget
    biased information to achieve better performance or remove irrelevant information
    to learn new tasks Machine Unlearning forget some specified training data to protect
    user privacy
  prefs: []
  type: TYPE_NORMAL
- en: 'In this survey, we classify forgetting in machine learning into two categories:
    harmful forgetting and beneficial forgetting, based on the specific application
    scenarios. Harmful forgetting occurs when we desire the machine learning model
    to retain previously learned knowledge while adapting to new tasks, domains, or
    environments. In such scenarios, it is crucial to prevent and mitigate knowledge
    forgetting. Conversely, there are many cases where beneficial forgetting becomes
    necessary. For example: (1) Overfitting to the training data hinders generalization.
    (2) Irrelevant and noisy information impedes the model’s ability to effectively
    learn new tasks and knowledge. (3) Pre-trained model contains private information
    that could potentially lead to privacy leakage. In these situations, forgetting
    becomes desirable as it serves several important purposes. Firstly, forgetting
    can mitigate overfitting, as it allows the model to forget irrelevant details
    and focus on the most pertinent patterns in the training data. Additionally, by
    discarding unnecessary information, forgetting facilitates the learning of new
    knowledge, as the model can make better use of its capacity to acquire and adapt
    to novel information. Lastly, forgetting helps protect privacy by discarding sensitive
    user information, ensuring that such data is not retained in the model’s memory.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1 Harmful Forgetting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Harmful forgetting has been observed not only in CL but also in various other
    research areas, including foundation model, domain adaptation, meta-learning,
    test-time adaptation, generative model, reinforcement learning and federated learning.
    While existing surveys have predominantly focused on forgetting in the context
    of CL, they often overlook a comprehensive examination of these other related
    research areas. This survey aims to fill this gap by offering an overview of forgetting
    in different learning scenarios, encompassing the aforementioned research areas.
  prefs: []
  type: TYPE_NORMAL
- en: 'Forgetting in these research fields can be attributed to various factors. In
    the context of continual learning, forgetting occurs due to the shift in data
    distribution across different tasks. In meta-learning, forgetting is a consequence
    of the shift in task distribution. In federated learning, forgetting is caused
    by the heterogeneity of data distribution among different clients, commonly known
    as client drift. In domain adaptation, forgetting happens because of domain shift.
    In test-time adaptation, forgetting is a result of adapting to the test data distribution
    during testing. In generative models, forgetting occurs due to the shift in the
    generator over time or when learning non-stationary data distribution. In reinforcement
    learning, forgetting can occur as a result of shifts in state, action, reward,
    and state transition dynamics over time. These changes in the underlying factors
    of the environment can lead to the loss or alteration of previously learned knowledge
    in the reinforcement learning process. In the case of foundation models, forgetting
    can be attributed to three different reasons: fine-tuning forgetting, incremental
    streaming data pre-training, and the utilization of foundation models for downstream
    CL tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate clarity and comparison of various settings related to forgetting,
    we present a comprehensive analysis of harmful forgetting in Table [I](#S1.T1
    "TABLE I ‣ 1 Introduction ‣ A Comprehensive Survey of Forgetting in Deep Learning
    Beyond Continual Learning"), highlighting the distinctions among different settings.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2 Beneficial Forgetting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While the prevailing belief in most existing works is that forgetting is harmful,
    we have come to recognize that forgetting is a double-edged sword. There are many
    instances where it is advantageous to forget certain knowledge within learned
    neural networks. Intentional forgetting proves beneficial in several scenarios:
    (1) selective forgetting could help mitigate overfitting; (2) to enhance model
    generalization or facilitate learning of new tasks/knowledge, it is imperative
    to eliminate biased or irrelevant information from previously learned knowledge;
    and (3) machine unlearning, which prevents data privacy leakage.'
  prefs: []
  type: TYPE_NORMAL
- en: First, overfitting has remained a fundamental challenge in machine learning,
    as it arises when a model excessively memorizes the training data but struggles
    to generalize effectively to new, unseen test data. To improve generalization,
    it is crucial for the model to avoid the mere memorization of training data and
    instead should prioritize learning the true underlying relationship between the
    input data and corresponding labels. One important technique to enhance generalization
    is selective forgetting, which plays a significant role in removing irrelevant
    or noisy information learned from the training data. By selectively discarding
    such irrelevant details, the model can focus on the most pertinent patterns and
    features, leading to improved generalization performance on unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: Second, when the goal is to learn new tasks or knowledge, the previously acquired
    knowledge may not always be relevant or beneficial for improving future learning
    on new information. When the model holds on to outdated or unrelated knowledge,
    it can hinder its ability to effectively learn and generalize from new data. In
    such situations, it becomes necessary to discard irrelevant information that is
    still retained in the model’s memory. By freeing up capacity within the model,
    it becomes more receptive and adaptive to acquiring new knowledge. The process
    of discarding irrelevant information is crucial for preventing interference between
    old and new knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, machine learning model users may request the removal of not only their
    training data from the database but also any traces within the pre-trained model
    itself, exercising their Right to Be Forgotten [[10](#bib.bib10)]. To address
    this concern, researchers have explored the concept of machine unlearning, which
    allows for the intentional forgetting of undesirable private training data. Furthermore,
    certain privacy attacks exploit the memorization effect of machine learning models
    to extract private information from pre-trained models. Membership inference attacks
    [[11](#bib.bib11)], for example, can determine whether a data point belongs to
    the training data associated with a pre-trained model. These privacy attacks can
    be successful in practice due to the memorization effect of neural networks. In
    such cases, intentional forgetting of private knowledge becomes beneficial in
    protecting privacy and preventing information leakage.
  prefs: []
  type: TYPE_NORMAL
- en: To facilitate comparisons, we also provide a comparative analysis in Table [II](#S1.T2
    "TABLE II ‣ 1 Introduction ‣ A Comprehensive Survey of Forgetting in Deep Learning
    Beyond Continual Learning") for beneficial forgetting, encompassing the above
    mentioned diverse settings for reference.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3 Challenges in Addressing Forgetting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Addressing forgetting faces numerous challenges that vary across different
    research fields. These challenges include:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Data Availability: Data availability is a significant challenge in various
    scenarios and greatly complicates the task of addressing forgetting. On one hand,
    the availability of previous task data may be limited due to storage constraints
    or data privacy concerns when learning new tasks. This challenge is prevalent
    in continual learning, meta-learning, domain adaptation, generative model and
    reinforcement learning, where access to past task data is crucial for mitigating
    forgetting and leveraging prior knowledge. On the other hand, some scenarios prohibit
    the use of raw data. For instance, in federated learning, only the parameters
    of a pre-trained model are transmitted to a central server, without sharing the
    underlying training data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Resource Constraints: Resource-limited environments, such as those with constraints
    on memory and computation, present challenges in effectively addressing forgetting.
    In online continual learning and meta-learning, where data or tasks are typically
    processed only once, these challenges are particularly pronounced. In addition,
    the learning agent has limited access to past data and experiences, which restricts
    the opportunities to reinforce previously learned tasks. This limited exposure
    to past data makes it difficult to retain knowledge and effectively mitigate forgetting.
    Furthermore, online learning often operates in resource-constrained environments
    with limited memory or computation capabilities. These constraints pose additional
    hurdles for addressing forgetting in an online setting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adaption to New Environments/Distribution: In various domains such as continual
    learning, foundation model, reinforcement learning, domain adaptation, test-time
    adaptation, meta-learning, and generative model, the target environment or data
    distribution can change over time. It becomes necessary for the learning agent
    to adapt to these new environments or scenarios. This adaptation can occur either
    during the training phase or during the testing phase. However, the forgetting
    challenge arises when the learning agent adapts to new scenarios and environments.
    The agent tends to lose previously acquired knowledge or performance on earlier
    tasks due to the shift in data distribution.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Task Interference/Inconsistency: Conflicting and incompatible goals among different
    tasks can lead to task interference, posing challenges in preventing forgetting.
    This issue is observed in various contexts, including continual learning and federated
    learning. In continual learning, sequentially observed tasks may have conflicting
    goals, making it difficult for the network to balance its performance across multiple
    tasks. As the network learns new tasks, the interference from these conflicting
    goals can exacerbate the forgetting problem. Similarly, in federated learning,
    models trained on different clients can exhibit inconsistencies [[12](#bib.bib12)]
    due to the heterogeneous data distribution across clients. These inconsistencies
    can lead to task or client interference, further aggravating the forgetting issue.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Privacy-Leakage Prevention: In certain scenarios, retaining old knowledge can
    raise privacy concerns as it may inadvertently expose private information. Consequently,
    it is essential to address these privacy risks and prevent the unintended disclosure
    of sensitive data. In this context, the objective shifts towards forgetting or
    erasing the traces of training data rather than memorizing them, thereby safeguarding
    user privacy. This specific challenge is encountered in the field of machine unlearning
    [[13](#bib.bib13)], which focuses on developing techniques to effectively forget
    or remove the training data traces from machine learning models.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.4 Survey Scope, Contributions and Organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Survey Scope Our primary objective is to provide a comprehensive overview of
    forgetting in the main research directions within the aforementioned fields. These
    fields have been chosen as representative research directions where forgetting
    plays a significant role. By covering these areas, we aim to shed light on the
    existence and impact of forgetting in these research domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our contributions can be summarized into three fold:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We provide a more systematic survey on CL compared to existing surveys. Our
    survey includes a more systematic categorization of CL problem settings and methods,
    offering a more thorough overview of the field.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to CL, our survey extends its scope to encompass forgetting in other
    research fields such as foundation model, meta-learning, domain adaptation, test
    time adaptation, generative model, federated learning, reinforcement learning
    and machine unlearning. This broader coverage provides a comprehensive understanding
    of forgetting across various research fields.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Our survey, in contrast to existing surveys on CL and forgetting, reveals that
    forgetting can be viewed as a double-edged sword. While it is often seen as a
    challenge, we highlight that forgetting also has desirable implications in privacy-preserving
    scenarios.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Organization The structure of this paper is as follows. In Sections [2](#S2
    "2 Forgetting in Continual Learning ‣ A Comprehensive Survey of Forgetting in
    Deep Learning Beyond Continual Learning")-[9](#S9 "9 Forgetting in Federated Learning
    ‣ A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning"),
    we provide a comprehensive survey on the phenomenon of harmful forgetting in various
    machine learning domains. These include continual learning, foundation model,
    domain adaptation, test-time adaptation, meta-learning, generative model, reinforcement
    learning, and federated learning. Each section explores the occurrence and impact
    of forgetting within these specific fields. In Section [10](#S10 "10 Beneficial
    Forgetting ‣ A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual
    Learning"), we delve into the concept of beneficial forgetting and its role in
    enhancing model generalization performance and facilitating machine unlearning.
    This section highlights the positive aspects of forgetting in specific learning
    scenarios. In Section [11](#S11 "11 Discussion and Future Prospect ‣ A Comprehensive
    Survey of Forgetting in Deep Learning Beyond Continual Learning"), we present
    the current research trends and offer insights into the potential future developments
    in the field.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Forgetting in Continual Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TABLE III: Content outline in CL. Based on different problem setting categorization
    criteria, the CL setting can be classified into various scenarios, as presented
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: Section Problem Setting Categorization Criterion Section [2.1](#S2.SS1 "2.1
    Task-Aware and Task-Free CL ‣ 2 Forgetting in Continual Learning ‣ A Comprehensive
    Survey of Forgetting in Deep Learning Beyond Continual Learning") Task-aware and
    Task-free CL whether explicit task splits/information are available or not during
    training Section [2.2](#S2.SS2 "2.2 Online CL ‣ 2 Forgetting in Continual Learning
    ‣ A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning")
    Online CL the model processes the data in a single pass or multiple passes Section
    [2.3](#S2.SS3 "2.3 Semi-supervised, Few-shot and Unsupervised CL ‣ 2 Forgetting
    in Continual Learning ‣ A Comprehensive Survey of Forgetting in Deep Learning
    Beyond Continual Learning") Semi-supervised, Few-shot and Unsupervised CL the
    amount of labeled data used in CL
  prefs: []
  type: TYPE_NORMAL
- en: The goal of continual learning (CL) is to learn on a sequence of tasks ${\mathcal{T}}_{1},{\mathcal{T}}_{2},\cdots,{\mathcal{T}}_{N}$
    without forgetting the knowledge on previous tasks. It can be formulated with
    the following optimization objective. Suppose when learning task $t$, the goal
    is to minimize the risk on all the seen tasks so far, i.e.,
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}}({\bm{\theta}}_{t})=\sum_{t=1}^{N}{\mathbb{E}}_{({\bm{x}},y)\sim{\mathcal{D}}_{{\mathcal{T}}_{t}}}{\mathcal{L}}_{{\bm{\theta}}_{t}}({\bm{x}},y),$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where ${\bm{\theta}}_{t}$ are the parameters when learning task $t$, and ${\mathcal{D}}_{{\mathcal{T}}_{t}}$
    represents the training data associated with task $t$.
  prefs: []
  type: TYPE_NORMAL
- en: The CL problem can be categorized in several different ways. Firstly, according
    to whether explicit task splits/information are available or not during training,
    CL can be divided into task-aware and task-free scenarios. Task-aware can be further
    classified into task/domain/class incremental learning. Addressing forgetting
    in task-aware CL is relatively straightforward due to the availability of task
    information. This task information can be leveraged to design and implement various
    strategies to mitigate forgetting. With knowledge of the specific tasks involved,
    CL learner can utilize task-specific cues or labels to guide its learning process
    and manage forgetting. However, addressing forgetting in task-free CL is more
    challenging. In task-free CL, there are no explicit task splits or task-specific
    information available to the learning system. As a result, the learning system
    must autonomously identify and adapt to changes or shifts in the data distribution
    without any task-specific cues or labels. This requires the development of robust
    and adaptive mechanisms that can detect and respond to changes in the data distribution,
    effectively managing forgetting in the absence of explicit task information.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, depending on whether the model processes the data in a single pass
    or multiple passes, CL can be categorized as online and offline CL. Offline CL
    has been extensively studied due to its availability of abundant computing and
    storage resources. However, online CL presents unique challenges. In online CL,
    the agent has limited access to past data and experiences, which restricts the
    opportunities to revisit and reinforce previously learned tasks. This limited
    exposure to past data makes it challenging to retain knowledge and effectively
    mitigate forgetting. Furthermore, online learning often operates in resource-constrained
    environments with limited memory or processing capabilities. These resource limitations
    pose additional hurdles for addressing forgetting in online CL. Storing and processing
    large amounts of data becomes more difficult due to the restricted resources,
    impeding the development and implementation of effective strategies to mitigate
    forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, according to the amount of labeled data used in CL, they could be categorized
    into supervised, semi-supervised, few-shot, and unsupervised CL. Supervised CL
    is generally considered the easiest case since the availability of labeled data
    provides clear task boundaries and evaluation signals. However, challenges arise
    in other forms of CL. For semi-supervised CL: the challenge lies in selecting
    useful knowledge from unlabeled data to mitigate forgetting. Not all unlabeled
    data may be beneficial for addressing forgetting, making the selection process
    challenging. In few-shot CL: with only a limited number of labeled data points
    available for learning, few-shot CL poses additional challenges. The scarcity
    of labeled data requires the learning agent to effectively utilize the available
    information to minimize forgetting and adapt to new tasks. In the case of unsupervised
    CL: unsupervised CL is the most challenging due to the absence of explicit task
    boundaries. Defining when a new task begins and differentiating it from previous
    tasks becomes difficult. The lack of task boundaries complicates the identification
    and management of forgetting, as the learning agent needs to adapt to new data
    streams without clear task transitions. Furthermore, the absence of labeled data
    in unsupervised CL results in a scarcity of feedback and evaluation signals for
    measuring forgetting. This absence of explicit task labels or ground truth information
    makes it challenging to quantify the extent of forgetting and evaluate the performance
    of unsupervised CL algorithms.'
  prefs: []
  type: TYPE_NORMAL
- en: Below, we present the details of each problem setting and its corresponding
    related works. To make content organization clear, we provide a Table [III](#S2.T3
    "TABLE III ‣ 2 Forgetting in Continual Learning ‣ A Comprehensive Survey of Forgetting
    in Deep Learning Beyond Continual Learning") to summarize the problem setting
    categorization in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Task-Aware and Task-Free CL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.1.1 Task-aware CL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Task-aware CL focuses on addressing scenarios where explicit task definitions,
    such as task IDs or labels, are available during the CL process. The three most
    common CL scenarios within task-aware settings are task-incremental learning,
    domain-incremental learning, and class-incremental learning [[3](#bib.bib3)].
    In domain-incremental learning, tasks sequentially arrive with the same label
    space but different input data distributions. This means that the tasks share
    a common set of labels or categories, but the distribution of the input data may
    vary across tasks. Task-incremental learning refers to the scenario where tasks
    arrive sequentially, and each task has its own disjoint label space. During testing,
    the presence of explicit task identification is considered, allowing the model
    to identify the specific task at hand. Class-incremental learning is similar to
    task-incremental learning but without explicit task identification during testing.
    Instead, the model needs to incrementally learn new classes without forgetting
    previously learned classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem Setup: We consider the standard CL problem of learning a sequence of
    $N$ tasks denoted as ${\mathcal{D}}^{tr}=\{{\mathcal{D}}_{1}^{tr},{\mathcal{D}}_{2}^{tr},\cdots,{\mathcal{D}}_{N}^{tr}\}$.
    The training data of $k$-th task ${\mathcal{D}}_{k}^{tr}$ consists of a set of
    triplets $\{({\bm{x}}_{i}^{k},y_{i}^{k},{\mathcal{T}}_{k})_{i=1}^{n_{k}}\}$, where
    ${\bm{x}}_{i}^{k}$ is the $i$-th task data example, $y_{i}^{k}$ is the data label
    associated with ${\bm{x}}_{i}^{k}$, and ${\mathcal{T}}_{k}$ is the task identifier.
    The goal is to learn a neural network with parameters ${\bm{\theta}}$, i.e., $f_{{\bm{\theta}}}$,
    on the training task sequence ${\mathcal{D}}^{tr}$ so that it performs well on
    the test set of all the learned tasks ${\mathcal{D}}^{te}=\{{\mathcal{D}}_{1}^{te},{\mathcal{D}}_{2}^{te},\cdots,{\mathcal{D}}_{N}^{te}\}$
    without forgetting the knowledge of previous tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/8975b93df69e70404b27cc051cb231be.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Categorization of existing continual learning approach.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing methods on task-aware CL have explored five main branches: memory-based,
    architecture-based, regularization-based, subspace-based, and Bayesian-based methods.
    An overall framework depicting these branches is provided in Figure [1](#S2.F1
    "Figure 1 ‣ 2.1.1 Task-aware CL ‣ 2.1 Task-Aware and Task-Free CL ‣ 2 Forgetting
    in Continual Learning ‣ A Comprehensive Survey of Forgetting in Deep Learning
    Beyond Continual Learning"). For a more comprehensive understanding of the methods
    within each category, please refer to Appendix [A.1](#A1.SS1 "A.1 Task-aware CL
    ‣ Appendix A Continual Learning ‣ A Comprehensive Survey of Forgetting in Deep
    Learning Beyond Continual Learning"), where we provide detailed descriptions.
    Below, we provide a brief overview of each class method.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory-based Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Memory-based method keeps a *memory buffer* that stores the data examples from
    previous tasks and replays those examples during learning new tasks. It can be
    further categorized into: raw memory replay; memory sample selection; generative
    replay; and compressed memory replay. Next, we discuss each direction in detail.
    (1) Raw Sample Replay: These methods randomly save a small amount of raw data
    from previous tasks and train the model together with the new task data. When
    the new task updates the model, the old task data is used as a constraint [[14](#bib.bib14),
    [15](#bib.bib15)] or directly mixed with the new data to form a batch [[16](#bib.bib16)]
    to update the model, thereby alleviating forgetting. (2) Memory Sample Selection:
    Randomly selecting samples for replay ignores the amount of information in each
    sample, which can lead to suboptimal performance [[17](#bib.bib17), [18](#bib.bib18)].
    Therefore, heuristic selection selects samples to be stored according to certain
    rules. For example, select the representative sample closest to the cluster center [[19](#bib.bib19)],
    the samples with higher diversity [[20](#bib.bib20), [21](#bib.bib21)], or the
    difficult sample closer to the decision boundary [[22](#bib.bib22), [23](#bib.bib23)].
    (3) Generative Replay: When privacy concerns restrict the storage of raw memory
    data, generative replay provides an alternative approach in CL to replay previous
    task data. The main concept behind generative replay is to train a generative
    model capable of capturing and remembering the data distribution from previous
    tasks. The representative works in this line involve using different generative
    models, including GAN-based [[24](#bib.bib24), [25](#bib.bib25)], AutoEncoder-based [[26](#bib.bib26)],
    Diffusion-based [[27](#bib.bib27)], and Model-inversion [[28](#bib.bib28)]. (4)
    Compressed Memory Replay: In scenarios with strict storage constraints on edge
    devices, memory efficiency becomes a critical consideration. Different strategies
    have been proposed to improve memory efficiency in CL learning. For example, storing
    feature representations [[29](#bib.bib29), [30](#bib.bib30)] or low-fidelity images [[31](#bib.bib31),
    [32](#bib.bib32)] instead of original images, or learning a set of condensed images [[33](#bib.bib33),
    [34](#bib.bib34)] using dataset distillation [[35](#bib.bib35)].'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture-based Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Architecture-based methods in CL [[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]
    involve updating the network architecture during the learning process to retain
    previously acquired knowledge. These methods aim to adapt the model’s architecture
    to acquire new tasks while preserving the knowledge from previous tasks. Based
    on whether the model parameters expand with the number of tasks, architecture-based
    methods can be categorized into two types: fixed-capacity and capacity-increasing
    methods. (1) Fixed-Capacity: In these methods, the amount of CL model’s parameters
    does not increase with the number of tasks, and each task selects a sub-network
    from the CL model to achieve knowledge transfer and reduce the forgetting caused
    by sub-network updates. Common subnetwork selection techniques include masking [[39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)], and pruning [[42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)]. (2) Capacity-Increasing: As the number of tasks increases,
    fixed-capacity CL models may face limitations in accommodating new tasks. To overcome
    this challenge, dynamic capacity methods are proposed [[36](#bib.bib36), [38](#bib.bib38),
    [45](#bib.bib45), [46](#bib.bib46)]. These methods ensure that old tasks are not
    forgotten and adapt to new tasks by introducing new task-specific parameters for
    each new task, while freezing parameters related to old tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization-based Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'These methods in CL involve the addition of regularization loss terms to the
    training objective to prevent forgetting previously learned knowledge [[47](#bib.bib47),
    [48](#bib.bib48), [49](#bib.bib49)]. It can be further divided into two subcategories:
    penalizing important parameter updates and knowledge distillation using a previous
    model as a teacher. (1) Penalize Parameter Updates: These methods use the Fisher
    information matrix [[47](#bib.bib47)], the cumulative update amount of parameters [[50](#bib.bib50)],
    etc. as a measure of the importance of old task parameters. On the one hand, when
    new tasks update important parameters, a large penalty is imposed in order to
    keep the knowledge of old tasks from being forgotten. On the other hand, imposing
    a small penalty on unimportant parameter updates can learn new task’s knowledge [[48](#bib.bib48),
    [51](#bib.bib51), [22](#bib.bib22)]. (2) Knowledge-Distillation-Based: Inspired
    by knowledge distillation [[52](#bib.bib52)], several methods in CL incorporate
    a distillation loss between the network of the previous task (referred to as the
    teacher) and the network of the current task (referred to as the student) to mitigate
    forgetting [[53](#bib.bib53), [54](#bib.bib54), [55](#bib.bib55)]. It should be
    mentioned that the ideal scenario would involve using raw data from old tasks
    to extract the knowledge of the teacher model and refine it into a student model.
    However, accessing raw data of old tasks is often not feasible due to data privacy
    concerns. Therefore, existing methods utilize proxy data, such as new task data [[53](#bib.bib53)]
    or large-scale unlabeled data [[56](#bib.bib56)], as a substitute for distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: Subspace-based Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Subspace-based methods in CL aim to address the issue of interference between
    multiple tasks by conducting learning in separate and disjoint subspaces, thus
    reducing old task forgetting. Subspace-based methods can be categorized into two
    types based on how the subspaces are constructed: orthogonal gradient subspace
    and orthogonal feature subspace methods. (1) Orthogonal Gradient Subspace: These
    methods require that the parameter update direction of the new task is orthogonal
    to the gradient subspace of the old tasks [[57](#bib.bib57), [58](#bib.bib58),
    [59](#bib.bib59)], ensuring minimal interference between tasks. (2) Orthogonal
    Feature Subspace: Similarly, these require that the parameter update direction
    of the new task is orthogonal to the subspace spanned by the input (feature) of
    the old tasks [[60](#bib.bib60), [61](#bib.bib61), [62](#bib.bib62), [63](#bib.bib63)].'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'These Bayesian approaches offer effective strategies to mitigate forgetting
    by incorporating uncertainty estimation and regularization techniques, thereby
    enhancing the adaptability of the learning process. Bayesian methods can be classified
    into three categories: methods that constrain the update of weight parameter distributions,
    methods that constrain the update in function space, and methods that dynamically
    grow the CL model architecture in an adaptive and Bayesian manner. Specifically,
    (1) Weight Space Regularization: These methods model the parameter update uncertainty
    and enforce the model parameter (weight space) distribution when learning the
    new task is close to that of all the previously learned tasks, including [[64](#bib.bib64),
    [65](#bib.bib65), [66](#bib.bib66), [67](#bib.bib67), [68](#bib.bib68), [69](#bib.bib69)].
    (2) Function Space Regularization: Different from weight space regularization
    which constrains the weight update, the function space regularization regulates
    the CL function update in the function space. They achieve this goal by enforcing
    the posterior distribution over the function space [[70](#bib.bib70)], constraining
    neural network predictions [[71](#bib.bib71)], modeling the cross-task covariances
    [[72](#bib.bib72)] or sequential function-space variational inference [[73](#bib.bib73)].
    (3) Bayesian Architecture Expansion: Bayesian architecture growing methods employ
    a probabilistic and Bayesian approach to dynamically expand the CL model. By leveraging
    Bayesian principles, these methods enable the CL model to incrementally grow and
    adapt to new tasks or data while preserving previously learned knowledge. This
    probabilistic framework facilitates the flexible and principled expansion of the
    model’s architecture, allowing it to accommodate increasing complexity and variability
    in the learning process, including [[74](#bib.bib74), [75](#bib.bib75)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2 Task-free CL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Task-free CL refers to a specific scenario that the learning system does not
    have access to any explicit task information. Unlike the task-aware CL setting,
    where a sequence of tasks is defined, task-free CL aims to perform adaptation
    without explicit task boundaries or labels. The system needs to adapt and generalize
    its knowledge over time, continually updating its model or representation to accommodate
    new information while retaining previously learned knowledge. The absence of explicit
    task information in task-free CL poses significantly more challenges, as the learning
    system must autonomously identify and adapt to changes or shifts in the data distribution
    without any task-specific cues or labels. Existing approaches for task-free CL
    can be categorized into two classes: memory-based methods and network expansion-based
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory-based method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Memory-based methods [[20](#bib.bib20), [23](#bib.bib23), [76](#bib.bib76),
    [15](#bib.bib15)] involve storing a small subset of previous data and replaying
    them alongside new mini-batch data. MIR [[23](#bib.bib23)] selects and replays
    samples that are most prone to interference. This selective replay aims to prioritize
    samples that are most relevant for retaining previously learned knowledge. Building
    upon MIR, GEN-MIR [[23](#bib.bib23)] incorporates generative models to synthesize
    memory examples during replay. GSS [[20](#bib.bib20)] focuses on storing diverse
    examples. GMED [[77](#bib.bib77)], proposes a method for editing the memory examples
    to promote forgetting and discourage memorization. While GMED focuses on editing
    memory examples, Wang et al. [[78](#bib.bib78)] propose a Distributionally Robust
    Optimization framework that considers population- and distribution-level evolution
    to address memory overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Expansion-based method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: In the context of architecture expansion-based methods, several approaches have
    been proposed to address the forgetting issue and facilitate continual adaptation.
    CN-DPM [[79](#bib.bib79)] introduces a method that expands the network structure
    based on the Dirichlet process mixture model. This approach allows for the automatic
    expansion of the network to accommodate new data distributions or concepts while
    preserving previously learned knowledge. VariGrow [[80](#bib.bib80)] proposes
    a variational architecture growing method based on Bayesian novelty to mitigate
    forgetting. This method leverages Bayesian techniques to identify novel information
    and dynamically expand the network architecture to accommodate new knowledge.
    ODDL [[81](#bib.bib81)] proposes a dynamical architecture expansion method based
    on estimating the discrepancy between the probabilistic representation of the
    memory buffer data and the accumulated knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Online CL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.2.1 Approaches for Online CL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The online CL setting presents additional challenges compared to offline CL.
    In online CL, the learner is only allowed to process the data for each task once
     [[5](#bib.bib5)]. Existing works addressing forgetting in online CL are mainly
    based on rehearsal replay [[23](#bib.bib23), [20](#bib.bib20), [82](#bib.bib82),
    [83](#bib.bib83)]. MIR [[23](#bib.bib23)] suggests replaying the samples that
    exhibit the maximum increase in loss. OCS [[84](#bib.bib84)] proposes to select
    samples with high affinity for old tasks. DVC [[85](#bib.bib85)] introduces a
    technique that involves selecting samples whose gradients are most interfered
    with new incoming samples to store in memory buffer. ASER [[82](#bib.bib82)] introduces
    an adversarial Shapley value scoring method that assigns scores to memory data
    samples. These scores are used to evaluate the contribution of memory samples
    to the performance of forgetting. La-MAML [[86](#bib.bib86)] utilizes a meta-learning
    algorithm to tackle online CL by leveraging a small episodic memory. GPS [[87](#bib.bib87)]
    formulates the memory construction problem in experience replay as a combinatorial
    optimization problem, and simulates the forgetting mode of the current task by
    creating future pseudo-tasks. However, in cases where memory constraints are stringent,
    replay-based online CL approaches have limited effectiveness. Some studies have
    proposed the utilization of regularization-based strategies to prevent forgetting [[88](#bib.bib88),
    [89](#bib.bib89)].
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.2 Imbalanced Class Issue in Online CL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The presence of imbalanced data streams in online CL has drawn significant
    attention, primarily due to its prevalence in real-world application scenarios [[90](#bib.bib90),
    [91](#bib.bib91), [92](#bib.bib92), [93](#bib.bib93)]. Addressing the issue of
    class imbalance can be approached through two main strategies: (1) learning a
    model that effectively balances the learning of both old and new classes during
    the training phase, or (2) employing post-processing techniques to calibrate the
    biases inherent in the model.'
  prefs: []
  type: TYPE_NORMAL
- en: Balance Learning Between New and Old Classes
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Balancing in the training phase involves heuristically selecting a balanced
    memory to tune the model [[90](#bib.bib90), [91](#bib.bib91), [94](#bib.bib94),
    [95](#bib.bib95)]. Chrysakis et al. [[94](#bib.bib94)] propose class-balancing
    reservoir sampling (CBRS) to tackle this issue. PRS [[95](#bib.bib95)] suggests
    a partitioning reservoir sampling strategy to address this issue. Kim et al. [[96](#bib.bib96)]
    introduces a stochastic information-theoretic reservoir sampler to select memory
    points from the imbalanced data stream. E2E [[90](#bib.bib90)] proposes to alleviate
    the imbalance problem by adopting a balanced fine-tuning strategy at the end of
    each incremental stage. GDumb [[91](#bib.bib91)] found that the downsampling strategy
    can well solve the problem of imbalance between old and new classes.
  prefs: []
  type: TYPE_NORMAL
- en: Post-Processing Calibration Techniques
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Post-processing calibration methods perform bias calibration on the classifier
    of the model during the inference phase [[92](#bib.bib92), [97](#bib.bib97), [98](#bib.bib98)].
    BiC [[92](#bib.bib92)] introduces a two-stage training where they perform the
    main training in the first stage, followed by a linear transformation to mitigate
    bias in the second stage. WA [[98](#bib.bib98)] reduces the imbalance between
    old and new classes by aligning the logits output from the model on the old and
    new classes. OBC [[99](#bib.bib99)] provides both theoretical and empirical explanations
    of how replay can introduce a bias towards the most recently observed data stream.
    They address this issue by modifying the model’s output layer, aiming to mitigate
    the effects of this online bias.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Semi-supervised, Few-shot and Unsupervised CL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 2.3.1 Semi-supervised CL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Semi-supervised CL is an extension of traditional CL that allows each task to
    incorporate unlabeled data as well.
  prefs: []
  type: TYPE_NORMAL
- en: Existing works on semi-supervised CL, mainly include generative replay [[100](#bib.bib100),
    [101](#bib.bib101)] and distillation [[56](#bib.bib56), [102](#bib.bib102)] to
    avoid forgetting. Specifically, ORDisCo [[101](#bib.bib101)] maintains a relatively
    constant-sized network, and it simultaneously trains a classifier and a conditional
    GAN, and learns the classifier by replaying data sampled from the GAN in an online
    fashion. SDSL [[100](#bib.bib100)] is also based on the generation-replay framework.
    GD [[56](#bib.bib56)] and DistillMatch [[102](#bib.bib102)] are distillation-based
    approaches. DistillMatch performs knowledge distillation by assigning pseudo-labels
    and data augmentation to the unlabeled data to reduce forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2 Few-shot CL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Few-shot CL refers to the scenario where a model needs to learn new tasks with
    only a limited number of labeled examples per task while retaining knowledge from
    previously encountered tasks. The challenge lies in effectively leveraging the
    limited labeled data and previously learned knowledge to adapt to new tasks while
    avoiding forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to traditional CL, few-shot CL faces the challenge of overfitting
    due to the limited number of examples available per task [[103](#bib.bib103),
    [104](#bib.bib104)]. To tackle the forgetting problem in few-shot CL, existing
    approaches employ various techniques, including metric learning, meta-learning,
    and parameter regularization. Due to limited pages, we provide details of these
    methods in Appendix. [A.2](#A1.SS2 "A.2 Few-shot CL ‣ Appendix A Continual Learning
    ‣ A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning").
    Below, we briefly explain each method: (1) Metric Learning-Based: These methods
    perform classification by class prototypes. To avoid forgetting, the prototype
    of the new class should be separable from the old class [[105](#bib.bib105), [106](#bib.bib106),
    [107](#bib.bib107)], and the prototype of the old class should not change drastically
    during the adjustment process of the new class [[107](#bib.bib107), [103](#bib.bib103)].
    (2) Meta-Learning-Based: These methods simulate the inference phase during training
    so that CL models can quickly adapt to unseen new classes to solve few-shot CL.
    For example, LIMIT [[108](#bib.bib108)] and MetaFSCIL [[109](#bib.bib109)] split
    the base task into multiple ’fake’-incremental tasks, so that the model has the
    learning ability of few-shot CL tasks. By reducing the loss associated with the
    meta-objective, they minimize forgetting of the old tasks. (3) Parameter Regularization-Based:
    These methods employ various strategies to address the forgetting problem by penalizing
    parameter updates that are important for old tasks [[110](#bib.bib110), [111](#bib.bib111)].'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3 Unsupervised CL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Unsupervised CL [[112](#bib.bib112), [113](#bib.bib113)] is a rapidly growing
    research area that emphasizes learning from unlabeled data alone. Unlike traditional
    supervised CL relying on labeled data, unsupervised CL explores techniques that
    enable learning and adaptation using only unlabeled data.
  prefs: []
  type: TYPE_NORMAL
- en: Existing unsupervised CL methods mainly rely on representation-based contrastive
    learning techniques [[112](#bib.bib112), [114](#bib.bib114), [113](#bib.bib113),
    [115](#bib.bib115)]. CURL [[112](#bib.bib112)] is the first offline continual
    unsupervised representation learning framework with unknown task labels and boundaries.
    Co2l [[114](#bib.bib114)] finds that self-supervised loss is generally more robust
    to forgetting than cross-entropy loss in CL. LUMP [[113](#bib.bib113)] observes
    that unsupervised CL models have a flatter loss landscape than supervised CL models,
    and additionally, it performs Mixup [[116](#bib.bib116)] between old task samples
    and new task samples to reduce forgetting. Prob [[115](#bib.bib115)] revisits
    the phenomenon of representational forgetting in both supervised and unsupervised
    CL settings, and shows that using observed accuracy to measure forgetting is a
    misleading metric.
  prefs: []
  type: TYPE_NORMAL
- en: 2.4 Theoretical Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The theoretical analysis of CL is quite a few. Pentina et al. [[117](#bib.bib117)]
    provide a generalization bound in the PAC-Bayesian framework for CL. Karakida
    et al. [[118](#bib.bib118)] conduct a theoretical analysis of the generalization
    performance within a solvable case of CL. They utilized a statistical mechanical
    analysis of kernel ridge-less regression to provide insights into the understanding
    of the generalization capabilities in CL scenarios. Kim et al. [[119](#bib.bib119)]
    study class-incremental learning and provides a theoretical justification for
    decomposing the problem into task-id prediction and within-task prediction. Evron
    et al. [[120](#bib.bib120)] theoretically study the CL on a sequence of separable
    linear classification tasks with binary classes. Peng et al. [[121](#bib.bib121)]
    propose Ideal Continual Learner (ICL), which unifies multiple existing well-established
    CL solutions, and gives the generalization boundary of ICL.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Forgetting in Foundation Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The forgetting in foundation models manifests in several distinct directions.
    Firstly, when fine-tuning a foundation model, there is a tendency to forget the
    pre-trained knowledge, resulting in sub-optimal performance on downstream tasks.
    This implies that the model may not effectively leverage the general knowledge
    it acquired during pre-training, leading to a decrease in overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, foundation models are typically trained on a dataset for a single
    pass  [[122](#bib.bib122), [123](#bib.bib123)], resulting in two types of forgetting.
    Firstly, in the case of streaming data, the challenge lies in retaining previous
    pre-trained knowledge as unlabeled data arrives sequentially [[124](#bib.bib124)].
    This type of forgetting is undesirable as it can hinder the model’s ability to
    leverage prior knowledge effectively. Conversely, the earlier examples encountered
    during pre-training may be overwritten or forgotten more quickly than the later
    examples. While this characteristic of forgetting can be seen as a disadvantage
    in some contexts, it can be advantageous in privacy-preserving scenarios. By discarding
    or attenuating sensitive information from the initial training examples, the model
    can enhance privacy protection. Therefore, foundation models exhibit both challenges
    and potential benefits associated with forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, foundation models have the ability to extract features with a strong
    feature extractor, which makes foundation models increasingly popular for CL approaches,
    aiming to achieve excellent performance across multiple tasks without forgetting
    previously learned knowledge. By leveraging the powerful feature extraction capabilities
    of foundation models, researchers have explored new avenues for advancing CL techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Below, we will delve into each research direction in more detail, examining
    the challenges and opportunities they present in the context of foundation models.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Forgetting in Fine-Tuning Foundation Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Fine-tuning the foundation model could achieve impressive performance on downstream
    tasks. However, fine-tuning a foundation model can result in the forgetting of
    pre-trained knowledge, which may lead to sub-optimal performance on downstream
    tasks. Forgetting occurs when the target model deviates significantly from the
    pre-trained model during the fine-tuning process  [[125](#bib.bib125)]. This deviation
    increases the likelihood of overfitting to a small fine-tuning set  [[126](#bib.bib126)],
    which can contribute to forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: There are several simple and effective strategies to mitigate forgetting during
    the fine-tuning process. These include techniques such as learning rate decreasing [[126](#bib.bib126)],
    weight decay [[127](#bib.bib127), [128](#bib.bib128)], and Mixout regularization [[125](#bib.bib125)].
    Furthermore, Fatemi et al. [[129](#bib.bib129)] find that in the study of mitigating
    the gender bias of the pre-trained language model, the pre-trained knowledge will
    be forgotten when the small neutral data is fine-tuned, which will hurt the downstream
    task performance. Dong et al. [[130](#bib.bib130)] observe that adversarial fine-tuning
    of pre-trained language models is prone to severe catastrophic forgetting, causing
    the loss of previously captured general and robust linguistic features. To address
    these issues, they propose a Robust Informative Fine-Tuning method from an information-theoretical
    perspective. In addition, an approach called Recall and Learn, proposed in Chen
    et al. [[131](#bib.bib131)], addresses the forgetting issue by utilizing Pretraining
    Simulation and Objective Shifting. This approach enables multi-task fine-tuning
    without relying on the data from the pretraining tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Forgetting in One-Epoch Pre-training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 3.2.1 Forgetting Previous Knowledge in Pre-training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Foundation model is typically trained using self-supervised learning, and self-supervised
    learning with streaming data has become a significant research area due to the
    storage and time costs associated with storing and training on large amounts of
    unlabeled data. In this context, Hu et al.[[132](#bib.bib132)] propose a sequential
    training method for self-supervised learning, demonstrating that self-supervised
    learning exhibits less forgetting compared to its supervised learning counterpart.
    Similarly, Purushwalkam et al.[[133](#bib.bib133)] perform self-supervised learning
    on a continuous non-iid data stream and introduce a minimum redundancy (MinRed)
    buffer approach to mitigate catastrophic forgetting. Furthermore, Lin et al. [[134](#bib.bib134)]
    develop a rehearsal-based framework that incorporates their proposed sampling
    strategies and self-supervised knowledge distillation to address the forgetting
    problem in streaming self-supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2.2 Beneficial Forgetting in Pre-training
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The memorization and forgetting of large foundation model have attracted much
    attention, some emerging work [[124](#bib.bib124)] studies the connection between
    the memorization of training data and different types of forgetting. Foundation
    models display two seemingly conflicting occurrences: the memorization of training
    data and different types of forgetting. Memorization refers to models excessively
    fitting to specific training examples, making them vulnerable to privacy leakage,
    e.g., inferring whether a data point belong to the training data associated with
    the pre-trained model. Some researches have shown that this phenomenon of leakage
    personally identifiable information in language model [[135](#bib.bib135), [136](#bib.bib136)],
    which is undesible in practice. On the other hand, forgetting involves the gradual
    loss of information about examples encountered early in training. The study in
    [[124](#bib.bib124)] establishes a connection between these phenomena and introduce
    a method to quantify the degree to which models "forget" specific details of training
    examples. This leads to reduced susceptibility to privacy attacks on examples
    that have not been recently encountered. Different from CL, which measures the
    forgetting as apposed to retain the knowledge from previously learned knowledge.
    In contrast, the study conducted by [[124](#bib.bib124)] examines a task-specific
    model and investigates the degree of forgetting exhibited towards specific training
    examples.'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 CL in Foundation Model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, researchers have explored the use of foundation models to tackle continual
    learning (CL) problems. CL based on foundation or pre-trained models has shown
    promise, particularly in the domain of natural language processing (NLP). Studies
    such as [[137](#bib.bib137), [138](#bib.bib138)] delve into the effective application
    of pre-trained models in CL for NLP tasks. Furthermore, [[139](#bib.bib139)] demonstrates
    the capability of fine-tuned pre-trained language models to act as continual learners.
  prefs: []
  type: TYPE_NORMAL
- en: Given the remarkable success of the Transformer model  [[140](#bib.bib140)]
    in computer vision tasks  [[141](#bib.bib141)], many studies in the field have
    started exploring the application of CL using foundational model. Ostapenko et
    al. [[142](#bib.bib142)] study the efficacy of pre-trained vision models as a
    foundation for downstream CL tasks. Mehta et al. [[143](#bib.bib143)] explain
    why pretrained models are more helpful in mitigating forgetting from a loss landscape
    perspective. They find that pretrained models drive weights to converge to wider
    minima, which usually indicates better generalization. Ramasesh et al. [[144](#bib.bib144)]
    observe that pretrained models are more resistant to forgetting than randomly
    initialized models, and this capability increases with the size of pretrained
    model and data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to the aforementioned studies, many researchers have explored the
    fine-tuning of pre-trained models to enhance their adaptation to downstream tasks
    in CL. These efforts primarily revolve around parameter-efficient fine-tuning
    techniques, such as Adapters [[145](#bib.bib145), [146](#bib.bib146)] and Prompts [[147](#bib.bib147),
    [148](#bib.bib148), [149](#bib.bib149), [150](#bib.bib150)]. (1) Adapter-based
    methods: ADAM [[145](#bib.bib145)] demonstrates that utilizing a frozen base model
    to generate generalizable embeddings and setting classifier weights as prototype
    features can outperform state-of-the-art CL methods. Additionally, performing
    model adaptation on downstream tasks further enhances performance. ADA [[146](#bib.bib146)],
    on the other hand, adopts a different approach by learning a single adapter for
    each new task instead of adjusting the entire CL model. The new adapter is then
    fused with the existing adapter to maintain a fixed capacity. (2) Prompts-based
    methods: L2P [[147](#bib.bib147)] introduces a dynamic instance-level learning
    approach to determine the matched expert-prompt for each new task, while freezing
    the pre-trained model. DualPrompt [[148](#bib.bib148)] builds upon L2P by appending
    complementary prompts (shared general-prompt and matched expert-prompt) on a pre-trained
    backbone. S-Prompt [[149](#bib.bib149)] focuses on exemplar-free domain-incremental
    learning. It independently learns prompts for each domain and stores them in a
    pool to prevent forgetting. Progressive Prompts [[150](#bib.bib150)] also learns
    a prompt for each new task, but it freezes the prompts of old tasks and incorporates
    them into the new task to encourage the transfer of knowledge in a forward manner.'
  prefs: []
  type: TYPE_NORMAL
- en: 4 Forgetting in Domain Adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of domain adaptation is to transfer knowledge from a source domain
    to a target domain. A domain represents the joint distribution of the input space
    ${\mathcal{X}}$ and the output space ${\mathcal{Y}}$. Specifically, the source
    domain is defined as ${\mathcal{P}}^{S}({\bm{x}},y)$, where ${\bm{x}}$ belongs
    to the input space ${\mathcal{X}}^{S}$ and $y$ belongs to the output space ${\mathcal{Y}}^{S}$.
    Similarly, the target domain is defined as ${\mathcal{P}}^{T}({\bm{x}},y)$, where
    ${\bm{x}}$ belongs to the input space ${\mathcal{X}}^{T}$ and $y$ belongs to the
    output space ${\mathcal{Y}}^{T}$.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the context of continual domain adaptation (CDA) [[151](#bib.bib151)], the
    focus is primarily on the covariate shift setting. Covariate shift refers to a
    situation where the distribution of input data, ${\mathcal{X}}$, differs between
    the source and target domains, while the conditional distribution of the output,
    ${\mathcal{Y}}$, remains the same. This setting assumes that the relationship
    between inputs and outputs remains consistent across domains, but the distributions
    of the input data vary. This is formally defined as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{P}}^{S}(X={\bm{x}})\neq{\mathcal{P}}^{T}(X={\bm{x}}),{\mathcal{P}}^{S}(y&#124;X={\bm{x}})={\mathcal{P}}^{T}(y&#124;X={\bm{x}}).$
    |  | (2) |'
  prefs: []
  type: TYPE_TB
- en: CDA and traditional CL have distinct characteristics and goals. On the one hand,
    CDA differs from traditional CL in terms of the availability of source domain
    data for transferring knowledge across the target domain sequence. In CDA, the
    source domain data is accessible, and the objective is to adapt the model from
    the source domain to the target domain, leveraging the available source domain
    data. However, the target domain may only provide unlabeled data, requiring the
    model to adapt to the new domain without explicit supervision. On the other hand,
    traditional CL aims to learn and adapt the model to a sequence of tasks without
    accessing previous task-specific labeled data. Labeled data is typically provided
    for each task in traditional CL.
  prefs: []
  type: TYPE_NORMAL
- en: 'Problem Setup: Suppose we have a pre-trained model $f_{{\bm{\theta}}}$ that
    has been trained on a set of source domain data ${\mathcal{P}}^{S}({\bm{x}},y)$,
    where ${\bm{x}}$ belongs to the source domain input space ${\mathcal{X}}^{S}$
    and $y$ belongs to the source domain label space ${\mathcal{Y}}^{S}$. Additionally,
    we have a sequence of evolving target distributions ${\mathcal{P}}_{t}^{T}({\bm{x}},y)$,
    where ${\bm{x}}$ belongs to the input space ${\mathcal{X}}^{T}_{t}$ of the target
    domain $t$. $y$ belongs to the label space ${\mathcal{Y}}^{T}_{t}$ of the target
    domain $t$. $t$ represents the domain index ranging from 1 to $N$. The objective
    of CDA, as proposed in [[151](#bib.bib151)], is to train $f_{{\bm{\theta}}}$ in
    such a way that it performs well on all the domains ${\mathcal{P}}_{t}^{T}({\bm{x}},y)$
    in the evolving target domain sequence, defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\min_{{\bm{\theta}}}\mathbb{E}_{t\in[1,\ldots,N]}\mathbb{E}_{{\bm{x}}^{T}\sim
    P^{T}_{t},{\bm{x}}^{S}\sim P^{S}}\mathcal{L}\left(f_{{\bm{\theta}}}({\bm{x}}^{T}),f_{{\bm{\theta}}}({\bm{x}}^{S})\right).$
    |  | (3) |'
  prefs: []
  type: TYPE_TB
- en: When dealing with new target domains, it is important to note that their data
    distributions differ from that of the source domain. As a result, adapting the
    model to these new domains can inadvertently result in forgetting the knowledge
    acquired from the previous domains. To mitigate this issue, various approaches
    have been developed to tackle the problem of catastrophic forgetting in domain
    adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: When the source domain data is available, most of the works avoid forgetting
    by replaying the source domain data [[152](#bib.bib152), [153](#bib.bib153), [154](#bib.bib154)],
    and a few works are based on regularization [[155](#bib.bib155)], or meta-learning [[156](#bib.bib156)].
    First, Replay-based methods are effective in preventing forgetting by incorporating
    data or knowledge from previous domains. One approach, CUA [[152](#bib.bib152)],
    addresses the issue by randomly selecting samples from previous domains and storing
    them in a memory buffer. Another method called UCL-GV [[157](#bib.bib157)] utilizes
    a First-In, First-Out (FIFO) buffer to replay episodic memory. AuCID [[153](#bib.bib153)]
    tackles the problem of continual unsupervised domain adaptation by consolidating
    the learned internal distribution. It achieves this by storing a fixed number
    of confident samples for each class per domain, which are later replayed during
    the adaptation process. Then, GRCL [[155](#bib.bib155)] utilizes the gradient
    direction of samples from the previous domain as a regularization term. This constraint
    ensures that the model can be updated with new target domain data without negatively
    affecting the performance of the previous domain. Finally, Meta-DR [[156](#bib.bib156)]
    proposes a meta-learning and domain randomization approach to mitigate forgetting
    and retain knowledge from previous domains during CDA.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, few works have focused on source-free approaches [[158](#bib.bib158)]
    that aim to protect the privacy of the source domain data, which is often inaccessible
    in many scenarios [[159](#bib.bib159), [160](#bib.bib160)]. CoSDA [[159](#bib.bib159)]
    introduces a knowledge distillation method that employs a dual-speed teacher-student
    structure. The slow-updating teacher preserves the long-term knowledge of previous
    domains, while the fast-updating student quickly adapts to the target domain.
    C-SUDA [[160](#bib.bib160)] achieves continual adaptation by synthesizing source-style
    images to avoid forgetting the source domain.
  prefs: []
  type: TYPE_NORMAL
- en: 5 Forgetting in Test Time Adaptation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Test time adaptation (TTA) refers to the process of adapting a pre-trained model
    on-the-fly to unlabeled test data during inference or testing [[161](#bib.bib161),
    [162](#bib.bib162), [163](#bib.bib163), [164](#bib.bib164), [165](#bib.bib165)].
    Unlike domain adaptation, test time adaptation occurs during the deployment phase
    rather than during the training phase. This approach allows the model to adapt
    to specific instances or conditions encountered at test time to improve the model
    generalization performance. TTA can involve updating certain parameters or features
    of the model based on additional information or feedback obtained during inference.
  prefs: []
  type: TYPE_NORMAL
- en: In traditional machine learning scenarios, during testing, it is typically assumed
    that the test data ${\mathcal{D}}_{test}$ follows the same distribution as the
    training data. However, in real-world applications, it is common for the test
    data distribution to deviate from the training data distribution. To address the
    distribution shift between the training and testing phases, TTA is employed. TTA
    involves adapting the pre-trained model on the unlabeled testing data ${\bm{x}}$
    using an unsupervised adaptation loss function. This adaptation aims to minimize
    the loss function ${\mathcal{L}}({\bm{x}},{\bm{\theta}})$ with respect to the
    parameters ${\bm{\theta}}$. It is important to note that ${\bm{x}}$ is sampled
    from the testing dataset, ${\mathcal{D}}_{test}$. Subsequently, the adapted model
    utilizes the updated parameters to make predictions for the test input ${\bm{x}}$.
    This allows the model to account for the distribution shift between the training
    and testing phases, and hopefully improve its performance on the test data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing Works: The Tent method [[161](#bib.bib161)] presents a test-time entropy
    minimization approach to enhance model generalization. This method focuses on
    minimizing the entropy of model predictions on test data, thereby improving the
    model’s ability to generalize to unseen examples. Another approach, known as MECTA
    [[166](#bib.bib166)], aims to improve the memory efficiency of test time adaptation.
    MECTA proposes techniques to adapt the model during testing while optimizing memory
    usage, ensuring efficient and effective adaptation to the test data distribution.
    In a different vein, MEMO [[167](#bib.bib167)] proposes a method that applies
    various data augmentations to a test data point. Subsequently, all model parameters
    are adapted by minimizing the entropy of the model’s output distribution across
    the augmented samples.'
  prefs: []
  type: TYPE_NORMAL
- en: When a pre-trained model is adapted to new unlabeled test data, the model shifts
    to the new data, potentially causing it to forget crucial information previously
    learned from the source domain data. This phenomenon can result in a substantial
    loss of knowledge and adversely affect the model’s overall performance [[162](#bib.bib162),
    [168](#bib.bib168)]. To address this issue, existing approaches primarily adopt
    two strategies.
  prefs: []
  type: TYPE_NORMAL
- en: Firstly, one intuitive approach to prevent the model from forgetting the knowledge
    acquired from the source domain is to employ a two-step process. Initially, the
    model trained on the source data is frozen. Subsequently, new learnable parameters
    are introduced to adapt the model to the test-time data [[169](#bib.bib169), [170](#bib.bib170)].
    For instance, VDP [[169](#bib.bib169)] prevents forgetting by freezing the source
    domain model parameters and instead learns a set of visual prompts tailored to
    the test data. These prompts help the model adapt effectively to the target domain.
    Similarly, EcoTTA [[170](#bib.bib170)] freezes the pre-trained network from the
    source domain and introduces a lightweight meta-network to facilitate adaptation
    to the target domain while retaining the valuable source domain knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: Secondly, another common approach to prevent forgetting in TTA is by constraining
    the update of important parameters, thereby avoiding the introduction of new parameters.
    Tent [[161](#bib.bib161)] specifically focuses on preserving the previous knowledge
    by updating only the BatchNorm layer in the network. On the other hand, CoTTA [[168](#bib.bib168)]
    proposes a different strategy. In each iteration of the adaptation process, CoTTA
    randomly restores the weights of certain neurons to the weights that were originally
    trained in the source domain. This restoration mechanism helps in retaining the
    knowledge acquired from the source domain, preventing it from being forgotten
    during the adaptation to the target domain. Other approaches in TTA employ techniques
    similar to regularization-based approaches commonly used in traditional CL. These
    methods penalize the updating of parameters that are deemed important to the source
    data during the adaptation process [[171](#bib.bib171), [162](#bib.bib162), [172](#bib.bib172)].
    For instance, EATA  [[162](#bib.bib162)] calculates the importance using the Fisher
    information matrix and utilizes this measure as a penalty when updating parameters
    during adaptation.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Forgetting in Meta-Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Meta-learning, also known as learning to learn, focuses on developing algorithms
    and models that can learn from previous learning experiences to improve their
    ability to learn new tasks or adapt to new domains more efficiently and effectively.
    In meta-learning, the goal is to enable a learning system, often referred to as
    the meta-learner or the meta-model, to acquire general knowledge or "meta-knowledge"
    from a set of related learning tasks or domains. This meta-knowledge is then leveraged
    to facilitate faster learning, better generalization, and improved adaptation
    to new, unseen tasks or domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Formally, let’s consider a distribution of tasks, denoted as $P({\mathcal{T}})$.
    For a specific task ${\mathcal{T}}_{t}$, which consists of a training dataset
    ${\mathcal{D}}_{\text{train}}$ and a validation dataset ${\mathcal{D}}_{\text{val}}$,
    sampled from the task distribution $P({\mathcal{T}})$. The loss function for task
    ${\mathcal{T}}_{t}$ with meta-parameters ${\bm{\theta}}$ is defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{L}}({\mathcal{T}}_{t})=\log P({\mathcal{D}}_{\text{val}}&#124;{\mathcal{D}}_{\text{train}};{\bm{\theta}}).$
    |  | (4) |'
  prefs: []
  type: TYPE_TB
- en: 'The objective of meta-learning is to optimize the meta loss function, given
    by:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small\min_{{\bm{\theta}}}\mathbb{E}_{{\mathcal{T}}_{t}\sim P({\mathcal{T}})}{\mathcal{L}}({\mathcal{T}}_{t}).$
    |  | (5) |'
  prefs: []
  type: TYPE_TB
- en: In other words, the aim is to find the optimal meta-parameters ${\bm{\theta}}$
    that minimize the expected loss across tasks, where tasks are sampled from the
    task distribution $P({\mathcal{T}})$.
  prefs: []
  type: TYPE_NORMAL
- en: However, forgetting can still occur in the context of meta-learning, and it
    can be classified into two distinct research directions. The first research direction
    focuses on Incremental Few-Shot Learning (IFSL), where the objective is to meta-learn
    new classes in addition to the pre-trained base classes. In this scenario, forgetting
    arises from the loss of information related to the pre-trained base classes. The
    challenge lies in retaining the knowledge of both the base classes and the newly
    introduced classes during the learning process. The second research direction
    deals with Continual Meta-Learning, where the agent encounters non-stationary
    task distributions over time while learning new tasks. Unlike IFSL, the goal here
    is not to remember the specific base classes. Instead, the objective is to retain
    the meta-knowledge acquired from previous task distributions. We will present
    the details of each direction in the following.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Incremental Few-Shot Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Incremental few-shot learning (IFSL) [[173](#bib.bib173), [174](#bib.bib174)]
    focuses on the challenge of learning new categories with limited labeled data
    while retaining knowledge about previously learned categories. In this scenario,
    a standard classification network has previously undergone training to recognize
    a predefined set of base classes. After that, the focus is on incorporating additional
    novel classes, each accompanied by only a small number of labeled examples. Subsequently,
    the model is tested on its classification performance, considering both the base
    and novel classes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing Works: Gidaris et al. [[173](#bib.bib173)] propose the IFSL problem
    and an attention-based solution to mitigate the forgetting in IFSL. The Attention
    Attractor Network, proposed by Ren et al. [[174](#bib.bib174)], is an alternative
    approach where the per-episode training objective during the incremental meta-learning
    stage is regulated using an attention mechanism to attend the set of base classes.
    In contrast to previous approaches that extract a fixed representation for each
    task, XtarNet [[175](#bib.bib175)] emphasizes the extraction of task-adaptive
    representations by combining novel and base features to enhance the adaptability
    of the representations. Shi et al. [[176](#bib.bib176)] suggest putting more effort
    into the base classifier pretraining stage rather than the later few-shot learning
    stage. As a result, they propose to seek flat local minima of the base classifier
    training objective function and subsequently fine-tune the model parameters within
    that flat region when faced with new tasks. In addition, C-FSCIL [[106](#bib.bib106)]
    incorporates a trainable fixed-size fully connected layer and a rewritable dynamically
    growing memory buffer to mitigate forgetting. This memory buffer can store a vector
    for each class encountered up to that point in the learning process.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Continual Meta-Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The goal of continual meta-learning (CML) is to address the challenge of forgetting
    in non-stationary task distributions. Traditional meta-learning approaches typically
    focus on a single task distribution. However, CML extends this concept to handle
    a sequence of task distributions, denoted as $P_{1}({\mathcal{T}}),P_{2}({\mathcal{T}}),\cdots,P_{N}({\mathcal{T}})$.
    In CML, the objective is to develop meta-learning algorithms that can effectively
    adapt and generalize to new task distributions as they arise over time. These
    task distributions can represent different environments, domains, or contexts.
    It aims to mitigate the forgetting of previously learned task distributions while
    efficiently adapting to new tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Existing Works: Online meta-learning (OML) [[177](#bib.bib177)] is a framework
    that assumes tasks arrive sequentially and aims to improve performance on future
    tasks. Jerfel et al. [[178](#bib.bib178)] extended the Model-Agnostic Meta-Learning
    [[179](#bib.bib179)] approach and utilized Dirichlet process mixtures to group
    similar training tasks together. However, this method is not scalable to large-scale
    non-stationary distributions due to the requirement of independent parameters
    for each component. Yap et al. [[180](#bib.bib180)] proposed an approach to model
    the posterior distribution of meta-parameters using Laplace approximation [[64](#bib.bib64)].
    Zhang et al. [[181](#bib.bib181)] further extended this framework by employing
    a dynamical mixture model to learn the distribution of meta-parameters instead
    of a single distribution. Additionally, they used structural variational inference
    techniques to infer latent variables in the model. Wang et al. [[182](#bib.bib182),
    [183](#bib.bib183), [184](#bib.bib184)] introduced a large-scale benchmark for
    sequential domain meta-learning. They proposed different settings, including supervised
    learning, imbalanced domains, and semi-supervised settings, to evaluate the performance
    of various methods in sequential domain meta-learning.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Forgetting in Generative Model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The goal of a generative model is to learn a generator that can generate samples
    from a target distribution. In the context of generative models, research related
    to forgetting can be categorized into two main categories: (1) GAN training itself
    can be viewed as CL; (2) lifelong learning generative model on non-stationary
    distributions.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 GAN Training is a Continual Learning Problem
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Thanh-Tung et al. [[185](#bib.bib185)] approach GAN training as a CL problem.
    They consider the discriminator as learning a sequence of tasks, with each task
    representing the data distribution generated by a specific generator. To address
    the issue of catastrophic forgetting, they propose the use of momentum or gradient
    penalties on the discriminator. By incorporating these techniques, they aim to
    prevent forgetting of previously learned tasks, leading to improved convergence
    and reduced mode collapse in GAN training. Similarly, Liang et al. [[186](#bib.bib186)]
    adopt Elastic Weight Consolidation (EWC) [[47](#bib.bib47)] or Synaptic Intelligence
    (SI) [[48](#bib.bib48)] to mitigate forgetting in the discriminator during GAN
    training.
  prefs: []
  type: TYPE_NORMAL
- en: One application of viewing GAN training as CL is in the context of data-free
    knowledge distillation (DFKD) [[187](#bib.bib187)]. DFKD aims to extract a compact
    student model from a pretrained teacher model when the original training data
    of the teacher model is unavailable. The basic approach in DFKD involves using
    a generative model to reconstruct or generate hard examples as input to the pretrained
    model, thereby distilling knowledge from the teacher model. However, forgetting
    poses a significant challenge in DFKD. Forgetting occurs in DFKD due to the non-stationary
    distribution of pseudo-samples generated by the pseudo-data generator, as the
    generator evolves over time. To mitigate this forgetting issue, Binici et al.
    [[188](#bib.bib188)] propose the use of a memory buffer that dynamically collects
    generated samples over time. By preserving past samples, the memory buffer helps
    to alleviate forgetting in DFKD. Binici et al. [[189](#bib.bib189)] utilize generative
    replay, which involves remembering previous data distributions, to further mitigate
    forgetting in DFKD. Another approach to address forgetting in DFKD is MAD [[190](#bib.bib190)]
    proposed by Do et al, which maintains an exponential moving average of the generator,
    preventing drastic changes in the generated data distribution and preserving previous
    knowledge. Patel et al. [[191](#bib.bib191)] propose a meta-learning-inspired
    framework to tackle the forgetting issue in DFKD.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Lifelong Learning of Generative Models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Lifelong learning of generative models seeks to enable generative models to
    generate data for new tasks while preserving the ability to generate data for
    previously learned tasks without forgetting. The goal is to develop generative
    models that can continually generate high-quality samples for both new and previously
    encountered tasks. The proposed approaches in lifelong learning of generative
    models encompass both Generative Adversarial Networks (GAN)-based and Variational
    Autoencoders (VAE)-based methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the realm of GAN-based approaches, Zhai et al. [[192](#bib.bib192), [193](#bib.bib193)]
    have proposed lifelong GAN, which allows for conditioned image generation while
    avoiding the problem of forgetting previously acquired knowledge. In the context
    of VAE-based approaches, Ramapuram et al. [[194](#bib.bib194)] introduce a teacher-student
    architecture. Furthermore, Ye et al. propose network expansion [[195](#bib.bib195)]
    and dynamic optimal transport formulation [[196](#bib.bib196)] to address the
    continual VAE.
  prefs: []
  type: TYPE_NORMAL
- en: 'Intentional Forgetting in Diffusion Model: The advancement of text-to-image
    diffusion models [[197](#bib.bib197), [198](#bib.bib198)] has sparked significant
    concerns regarding data privacy, copyright infringement, and safety related to
    generative models, primarily because of the memorization effect observed in large
    generative models. This effect leads to the learning and generation of unauthorized
    and potentially harmful content, contributing to the creation and dissemination
    of unregulated material. To address these concerns, the Forget-Me-Not approach
    [[199](#bib.bib199)] and Selective Amnesia (SA) [[200](#bib.bib200)] have emerged,
    aiming to mitigate the presence of potentially harmful or unauthorized content.
    These approaches achieve this by leveraging forgetting mechanisms to eliminate
    unwanted information, thereby promoting diversity in generated outputs and protecting
    data privacy.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Forgetting in Reinforcement Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While most existing CL methods primarily tackle the issue of forgetting in image
    classification, it is important to note that forgetting also widely occurs in
    reinforcement learning (RL), known as continual RL. Addressing catastrophic forgetting
    in RL is vital for the advancement of intelligent agents that can continuously
    learn and adapt to new tasks and environments [[201](#bib.bib201)].
  prefs: []
  type: TYPE_NORMAL
- en: Standard RL formulation could be defined as the following. We denote ${\mathcal{S}}$
    as the state space, ${\mathcal{A}}$ as the action space, and a reward function
    is $r:{\mathcal{S}}\times{\mathcal{A}}\rightarrow R$. At each time step $t$, the
    agent sample action from a policy function which output the optimal action or
    the distributions over the action space. The deterministic policy takes the current
    state $s$ as input, and outputs the action according to $a_{t}=\mu(s_{t})$. A
    stochastic policy takes the state $s_{t}$ as input, outputs the optimal action
    distribution according to $a_{t}\sim\pi(\cdot|s_{t})$. Then, the state transition
    function takes the state $s_{t}$ and $a_{t}$ as input and outputs the next action
    either deterministically $s_{t+1}=f(s_{t},a_{t})$ or stochastically $s_{t+1}\sim
    p(\cdot|s_{t},a_{t})$. The goal of RL agent is to accumulate as much reward as
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'Following [[201](#bib.bib201)], the general continual RL can be formulated
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 8.1
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '(General Continual RL): Given a state space ${\mathcal{S}}$, action space ${\mathcal{A}}$
    and observation space ${\mathcal{O}}$. A reward function is $r:{\mathcal{S}}\times{\mathcal{A}}\rightarrow
    R$; A transition function is $p:{\mathcal{S}}\times{\mathcal{A}}\rightarrow{\mathcal{S}}$;
    An observation function is $x:{\mathcal{S}}\rightarrow{\mathcal{O}}$. The general
    continual RL can be formulated as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small{\mathcal{M}}\overset{def}{=}\langle{\mathcal{S}}(t),{\mathcal{A}}(t),r(t),p(t),x(t),{\mathcal{O}}(t)\rangle.$
    |  | (6) |'
  prefs: []
  type: TYPE_TB
- en: Definition [8.1](#S8.Thmtheorem1 "Definition 8.1 ‣ 8 Forgetting in Reinforcement
    Learning ‣ A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual
    Learning") highlights that in continual RL, various components such as the state,
    action, reward, observation, and more, undergo changes over time. This emphasizes
    the dynamic nature of the RL process in continual settings.
  prefs: []
  type: TYPE_NORMAL
- en: 'Continual RL approach. The existing continual RL methods can be categorized
    into four main groups: (1) regularization-based methods. These approaches employ
    techniques such as knowledge distillation to alleviate forgetting [[202](#bib.bib202)],
    (2) rehearsal-based methods. These methods utilize rehearsal or experience replay
    to mitigate forgetting [[203](#bib.bib203)], (3) architecture-based methods. These
    approaches focus on learning a shared structure, such as network modularity or
    composition, to facilitate continual learning [[204](#bib.bib204)] and (4) meta-learning-based
    methods [[179](#bib.bib179)].'
  prefs: []
  type: TYPE_NORMAL
- en: 9 Forgetting in Federated Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Federated learning (FL) is a decentralized machine learning approach where the
    training process takes place on local devices or edge servers instead of a centralized
    server. In federated learning, instead of sending raw data to a central server,
    the model is distributed to multiple client devices or servers. Each client device
    performs training on its local data, and only the model updates are sent back
    to the central server. The central server aggregates these updates from multiple
    clients to update the global model. This collaborative learning process enables
    privacy preservation as the raw data remains on the local devices, reducing the
    risks associated with data sharing.
  prefs: []
  type: TYPE_NORMAL
- en: We can classify the forgetting issue in federated learning (FL) into two branches.
    The first branch pertains to the forgetting problem caused by the inherent non-IID
    (not identically and independently distributed) data among different clients participating
    in FL. In this scenario, each client’s data distribution may vary significantly,
    leading to challenges in preserving previously learned knowledge when aggregating
    model updates from multiple clients. The second branch addresses the issue of
    continual learning within each individual client in the federated learning process,
    which results in forgetting at the overall FL level. This branch is referred to
    as federated continual learning (FCL) [[205](#bib.bib205)]. FCL involves the continual
    learning and adaptation of models on each client while participating in federated
    learning, potentially leading to forgetting of previously learned knowledge at
    the global FL level. We will provide a detailed explanation for each research
    direction in the following.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1 Forgetting Due to Non-IID Data in FL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The problem of client drift [[206](#bib.bib206)] in federated learning pertains
    to the scenario where there is a substantial variation in the data distribution
    across individual clients. In federated learning, each client conducts local training
    using its own data, and subsequently, the model updates are aggregated to construct
    a global model. However, when there are significant variations in the data distribution
    among clients, it leads to each client updating its model in different directions,
    ultimately resulting in a drift among the client models. This drift can lead to
    performance degradation or instability in the global model. In other words, forgetting
    in federated learning occurs during the model averaging process on the server
    side and is primarily driven by the inconsistency of models among clients [[12](#bib.bib12)].
    This inconsistency arises due to the presence of heterogeneous data distributions
    across clients.
  prefs: []
  type: TYPE_NORMAL
- en: Shoham et al. [[207](#bib.bib207)] provide an interpretation of the client drift
    issue in FL by relating it to the concept of forgetting in continual learning.
    They propose a regularization-based approach specifically designed to address
    client drift in federated learning settings, particularly in the non-IID data
    setting. Their method aims to mitigate the effects of forgetting and preserve
    previously learned knowledge during the training process in federated learning.
    Subsequently, multiple methods have been proposed to alleviate the issue of forgetting
    by integrating penalty or regularization terms that account for client model shifts
     [[208](#bib.bib208), [209](#bib.bib209), [210](#bib.bib210)]. FCCL [[208](#bib.bib208)]
    utilizes two teachers, the optimal model pre-trained on the client’s private data
    and the model after the server’s collaborative update, and applies knowledge distillation
    to constrain the client model update, thereby alleviating forgetting. FedReg [[209](#bib.bib209)]
    addresses forgetting by using generated pseudo data to regularize the parameter
    update during client training, noting that solving forgetting in federated learning
    can enhance the convergence speed of the algorithm. FedNTD [[210](#bib.bib210)]
    employs knowledge distillation to mitigate knowledge forgetting, but focuses only
    on distilling mispredicted classes while disregarding well-predicted ones. Additionally,
    inspired by the GEM [[14](#bib.bib14)] and OGD [[57](#bib.bib57)], GradMA [[211](#bib.bib211)]
    employs gradient information from the previous local model and the centralized
    model to restrict the gradient direction of the local model update. By rethinking
    the design of the models used in federated learning, Qu et al. [[212](#bib.bib212)]
    propose architectural strategies to mitigate forgetting and improve the overall
    performance and stability of federated learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2 Federated Continual Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In traditional federated learning, the focus is primarily on aggregating model
    updates from different clients without considering the long-term retention of
    knowledge across multiple training rounds. However, in scenarios where clients
    encounter non-stationary distribution, it becomes necessary to incorporate continual
    learning techniques to avoid catastrophic forgetting and retain knowledge from
    previous training rounds. In standard CL, an agent learns a sequence of tasks,
    denoted as ${{\mathcal{T}}_{1},\cdots,{\mathcal{T}}_{N}}$. However, in federated
    continual learning (FCL), each client learns its own private task sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'FCL is more complicated than traditional FL or CL because non-iid and catastrophic
    forgetting problems need to be solved simultaneously. FedWeIT [[205](#bib.bib205)]
    first formally introduced the FCL setting from the perspective of CL. Solving
    forgetting in FCL is very challenging due to two reasons: (1) non-stationary data
    distribution in each client; and (2) model average in the server. Existing methods
    of addressing forgetting can be divided into three classes. (1) Parameter Isolation
    Method [[205](#bib.bib205)]: FedWeIT [[205](#bib.bib205)] decomposes the parameters
    of each client into global and sparse local task adaptation parameters to reduce
    inter-client interference and thus alleviate forgetting. (2) Replay-based Method [[213](#bib.bib213),
    [214](#bib.bib214)]: inspired by traditional memory-based CL, some methods address
    the forgetting problem by replaying the client’s old data. (3) Knowledge Distillation
    Method [[215](#bib.bib215)]: CFeD [[215](#bib.bib215)] assumes that there is an
    unlabeled wild dataset in the clients and server. It proposes a distillation-based
    method that utilizes an unlabeled surrogate dataset to aggregate clients and avoids
    forgetting by rehearsing old data.'
  prefs: []
  type: TYPE_NORMAL
- en: 10 Beneficial Forgetting
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Forgetting is not always harmful; in fact, intentional forgetting can prove
    beneficial in many cases. In this section, we explore the concept of beneficial
    forgetting within various learning scenarios. We begin by discussing selective
    forgetting and its positive impact on mitigating overfitting, as outlined in Section
    [10.1](#S10.SS1 "10.1 Combat Overfitting Through Forgetting ‣ 10 Beneficial Forgetting
    ‣ A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning").
    Next, we highlight the importance of discarding old, irrelevant knowledge when
    acquiring new knowledge, which we explore in Section [10.2](#S10.SS2 "10.2 Learning
    New Knowledge Through Forgetting Previous Knowledge ‣ 10 Beneficial Forgetting
    ‣ A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning").
    Additionally, we delve into the field of machine unlearning in Section [10.3](#S10.SS3
    "10.3 Machine Unlearning ‣ 10 Beneficial Forgetting ‣ A Comprehensive Survey of
    Forgetting in Deep Learning Beyond Continual Learning"), which specifically focuses
    on the task of erasing private user data from pre-trained models. This area of
    research aims to address privacy concerns by effectively removing sensitive information
    from models that have been trained on user data. By discussing machine unlearning,
    we shed light on the techniques and strategies employed to achieve secure and
    privacy-preserving machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 10.1 Combat Overfitting Through Forgetting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Overfitting in neural networks occurs when the model excessively memorizes the
    training data, leading to poor generalization. To address overfitting, it is necessary
    to selectively forget irrelevant or noisy information [[216](#bib.bib216)]. One
    strategy to mitigate overfitting is selective forgetting, as proposed by Shibata
    et al. [[217](#bib.bib217)]. This approach involves identifying and discarding
    less relevant or noisy information from the training data, enabling the model
    to focus on the most important patterns and improve its ability to generalize
    to unseen data. Current methodologies, including techniques such as $l_{1}$ normalization,
    feature selection, and early stopping, can be considered as forms of selective
    forgetting. These methods play an important role in removing less informative
    components from the model’s memory. By applying $l_{1}$ normalization, the model’s
    parameters are encouraged to become sparse, effectively forgetting less relevant
    features. Feature selection techniques focus on identifying and retaining only
    the most informative features while discarding the rest. Additionally, early stopping
    halts the training process when the model’s performance on a validation set starts
    to deteriorate, preventing further memorization of noise or irrelevant patterns.
    Furthermore, Nikishin et al. [[218](#bib.bib218)] overcome the overfitting to
    early experiences in reinforcement learning by resetting the last few layers of
    the RL agent. All of these methods contribute to selective forgetting by discarding
    less informative components and promoting the retention of more relevant information
    in the model’s memory.
  prefs: []
  type: TYPE_NORMAL
- en: In the context of learning from noisy labeled data, the memorization and overfitting
    to noisy labeled data can detrimentally impact the model’s generalization performance.
    SIGUA [[219](#bib.bib219)] proposes a method to selectively forget the undesired
    memorization of noisy labeled data, reinforcing the desired memorization and improving
    the model’s overall performance.
  prefs: []
  type: TYPE_NORMAL
- en: Adversarial forgetting, as described in the work by Jaiswal et al. [[220](#bib.bib220)],
    offers a mechanism to mitigate overfitting to unwanted and nuisance factors present
    in the training data. This approach involves selectively forgetting irrelevant
    information to promote the learning of invariant representations, thus reducing
    the impact of irrelevant factors on the model’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: 10.2 Learning New Knowledge Through Forgetting Previous Knowledge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Numerous studies in psychology and neuroscience have revealed the interconnected
    nature of forgetting and learning [[221](#bib.bib221)]. In the field of machine
    learning, the Learn to Forget approach proposed by Baik et al. [[222](#bib.bib222)]
    highlights that not all prior knowledge acquired through meta-learning is beneficial
    for learning new tasks. They propose selectively forgetting certain aspects of
    prior knowledge to facilitate faster learning of new tasks. In a different line
    of research, Zhou et al. [[223](#bib.bib223)] introduce a forget-and-relearn paradigm.
    They show that adding a forgetting step can improve the generalization and effectiveness
    of model relearning. Another approach, Learning Not to Learn (LNL) [[224](#bib.bib224)],
    focuses on forgetting biased information. LNL aims to minimize the mutual information
    between feature embeddings and bias, thereby enhancing performance during test
    time by reducing the influence of biased or irrelevant information. Bevan et al.
    [[225](#bib.bib225)] leverage the concept of forgetting biased information for
    Melanoma Classification. They explore the idea of selectively forgetting certain
    biased information that might be present in the dataset to improve the accuracy
    and effectiveness of melanoma classification models. Chen et al. [[226](#bib.bib226)]
    delve into the learn-forget paradigm in the context of optimal task selection
    for meta-learning. Their research investigates how the selective learning and
    forgetting of tasks can contribute to achieving optimal task selection in meta-learning
    scenarios. Moreover, in the context of continual learning, Wang et al. [[227](#bib.bib227)]
    identify that when old knowledge interferes with the learning of new tasks, accurate
    preservation of the old knowledge can exacerbate the interference. To address
    this issue, they propose an active forgetting mechanism that selectively discards
    old knowledge that hinders the learning of new tasks. By doing so, they demonstrate
    the potential benefits of active forgetting in CL.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3 Machine Unlearning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 10.3.1 Problem Overview
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Machine unlearning [[13](#bib.bib13), [228](#bib.bib228), [229](#bib.bib229)],
    a recent area of research, addresses the need to forget previously learned training
    data in order to protect user data privacy and aligns with privacy regulations
    such as the European Union’s General Data Protection Regulation [[230](#bib.bib230)]
    and the Right to Be Forgotten [[10](#bib.bib10)]. These regulations require companies
    and organizations to provide users with the ability to remove their data under
    specific circumstances.
  prefs: []
  type: TYPE_NORMAL
- en: 'Interestingly, machine unlearning demonstrates two distinct phenomena related
    to forgetting. On one hand, it involves forgetting specific training data memorized
    by the pre-trained model. The problem of machine unlearning can be categorized
    into two main types: exact unlearning and approximate unlearning. In Section [10.3.2](#S10.SS3.SSS2
    "10.3.2 Exact Unlearning ‣ 10.3 Machine Unlearning ‣ 10 Beneficial Forgetting
    ‣ A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning"),
    we will delve into the details of exact unlearning, while in Section [10.3.3](#S10.SS3.SSS3
    "10.3.3 Approximate Unlearning ‣ 10.3 Machine Unlearning ‣ 10 Beneficial Forgetting
    ‣ A Comprehensive Survey of Forgetting in Deep Learning Beyond Continual Learning"),
    we will focus on the concept of approximate unlearning. On the other hand, machine
    unlearning can lead to catastrophic forgetting of knowledge on data other than
    the targeted forgotten training set. The gradual updating of the pre-trained model
    to achieve unlearning is responsible for this second type of forgetting. In Section
    [10.3.4](#S10.SS3.SSS4 "10.3.4 Catastrophic Forgetting Other Normal Examples ‣
    10.3 Machine Unlearning ‣ 10 Beneficial Forgetting ‣ A Comprehensive Survey of
    Forgetting in Deep Learning Beyond Continual Learning"), we will provide a comprehensive
    exploration of catastrophic forgetting in machine unlearning and its implications.'
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.2 Exact Unlearning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Exact unlearning refers to the scenario where the distribution of the model
    obtained by training on the remaining dataset is identical to the distribution
    of the unlearned model. Let $P({\mathbb{A}}({\mathcal{D}}))$ denote the model
    distribution trained on dataset ${\mathcal{D}}$ with algorithm ${\mathbb{A}}$.
    Let ${\mathcal{F}}$ denote the dataset to be removed in the pre-trained model.
    $P({\mathbb{A}}({\mathcal{D}}\backslash{\mathcal{F}}))$ denote the model distribution
    by training on the remaining dataset ${\mathcal{D}}\backslash{\mathcal{F}}$. $P({\mathcal{U}}({\mathcal{D}},{\mathcal{F}},{\mathbb{A}}({\mathcal{D}})))$
    denote the unlearned model distribution. This concept can be defined as follows.
  prefs: []
  type: TYPE_NORMAL
- en: Definition 10.1 (exact unlearning)
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'For a learning algorithm ${\mathbb{A}}$, a dataset ${\mathcal{D}}$ and a dataset
    ${\mathcal{F}}$ to be forgotten, the exact unlearning ${\mathcal{U}}$ can be defined
    as following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small P({\mathbb{A}}({\mathcal{D}}\backslash{\mathcal{F}}))=P({\mathcal{U}}({\mathcal{D}},{\mathcal{F}},{\mathbb{A}}({\mathcal{D}}))).$
    |  | (7) |'
  prefs: []
  type: TYPE_TB
- en: One straightforward approach to achieve exact unlearning of targeted training
    data from a machine learning model is to retrain the model using the remaining
    dataset as defined earlier. This method effectively removes all information associated
    with the deletion set. However, this approach can be computationally expensive,
    especially in the case of large-scale pre-trained models and datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Existing works. To address the above challenges, DeltaGrad [[231](#bib.bib231)]
    cache the model parameters and gradients at each training iteration during the
    learning process to speed up the retraining on the remaining dataset. SISA [[228](#bib.bib228)]
    involves partitioning the dataset into distinct shards or subsets and maintaining
    multiple independent models by training the model on each subset. During inference,
    the predictions of these individual models are combined or aggregated. Upon receiving
    deletion requests, SISA approach retraining process focuses solely on the constituent
    model that originally trained on the subset containing this data point. This methodology
    ensures the elimination of all information associated with the removed data points.
    Sekhari et al. [[232](#bib.bib232)] explores on machine unlearning with a focus
    on population risk minimization instead of previous work focus on empirical risk
    minimization. Ullah et al. [[233](#bib.bib233)] further propose an exact unlearning
    method based on the notion of total variation (TV) stability. ARCANE [[234](#bib.bib234)]
    is an approach that leverages ensemble learning to address the retraining cost
    associated with unlearning. Instead of performing a full retraining process, ARCANE
    transforms it into multiple one-class classification tasks. By doing so, it reduces
    the computational and resource requirements while still maintaining the desired
    model performance.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.3 Approximate Unlearning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As discussed above, exact unlearning can be computationally and memory intensive,
    especially in the case of large-scale pre-trained models and datasets. Additionally,
    it poses a challenge to validate whether the distribution of the model after unlearning
    matches that of a fully retrained model using the remaining dataset. As a result,
    recent research has embraced the concept of approximate unlearning [[235](#bib.bib235)].
    This concept allows for a more efficient and feasible approach to unlearning,
    offering a balance between computational resources and the desired level of forgetting.
    The concept of "approximate unlearning" is defined in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Definition 10.2 (approximate unlearning [[235](#bib.bib235)])
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Given a $\epsilon$, for a learning algorithm ${\mathbb{A}}$, a dataset ${\mathcal{D}}$
    and a dataset ${\mathcal{F}}$ to be forgotten, the unlearning algorithm ${\mathcal{U}}$
    performs $\epsilon$ certified removal to remove the influence of ${\bm{z}}$ defined
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\small log&#124;&#124;P({\mathbb{A}}({\mathcal{D}}\backslash{\bm{z}}))-P({\mathcal{U}}({\mathcal{D}},{\bm{z}},{\mathbb{A}}({\mathcal{D}})))&#124;&#124;\leq\epsilon.$
    |  | (8) |'
  prefs: []
  type: TYPE_TB
- en: Intuitively, the definition of approximate unlearning strives to minimize the
    disparity between the parameter distribution of the unlearned model and the model
    obtained through full retraining on the remaining dataset. Instead of enforcing
    an exact match, approximate unlearning relaxes the requirement by focusing on
    bringing their distributions closer together. This relaxation allows for a more
    practical approach to unlearning, acknowledging that achieving an exact replication
    of the fully trained model on the remaining dataset may not always be feasible
    or necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Existing works. Certified Removal [[235](#bib.bib235)] introduces a theoretical
    framework for approximate unlearning. It offers a compelling assurance that a
    model, from which specific data has been removed, is indistinguishable from a
    model that has never encountered that data in the first place. This theoretical
    guarantee provides a strong foundation for the effectiveness of the unlearning
    process. By ensuring that the model’s behavior remains consistent with a model
    that lacks knowledge of the removed data, Certified Removal offers a high level
    of confidence in the privacy and security of the unlearning procedure. The scrubbing
    procedure [[236](#bib.bib236)] offers an alternative framework for machine unlearning
    by leveraging a generalized and weaker form of Differential Privacy. It modifies
    neural network weights to eliminate or "scrub" information related to specific
    training data. Variational Bayesian Unlearning (VBU) [[237](#bib.bib237)] is a
    probabilistic framework for approximate unlearning. It aims to minimize the KL-divergence
    between the approximate posterior of model parameters obtained through direct
    unlearning and the exact posterior from retraining the model with full data. VBU
    strikes a balance between completely forgetting the erased data and retaining
    essential knowledge captured by the model’s posterior belief when trained on the
    full data. In a subsequent study, L-CODEC [[238](#bib.bib238)] is an approximate
    unlearning method that identifies a specific subset of model parameters with the
    highest semantic overlap on an individual sample level. This subset of parameters
    is then utilized in the context of machine unlearning to achieve more efficient
    and targeted unlearning.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.4 Catastrophic Forgetting Other Normal Examples
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Catastrophic forgetting in machine unlearning refers to the unintended decrease
    in the log posterior probability of the remaining dataset, denoted as $\small
    P({\bm{\theta}}\mid{\mathcal{D}}\backslash{\mathcal{F}})$, as a result of unlearning
    [[239](#bib.bib239)]. This phenomenon leads to a degradation in the performance
    of the unlearned model on data other than the targeted samples, which is undesirable.
    The significance of catastrophic forgetting increases as more data is unlearned.
    It is important to note that this concept differs from the objective of machine
    unlearning, which focuses solely on forgetting specific samples. In the UNLEARN
    method proposed by [[239](#bib.bib239)], the update magnitude of new parameter
    values to the old values is constrained, and a memory buffer is maintained to
    store previous examples. The Forsaken method [[240](#bib.bib240)] introduces a
    dynamic gradient penalty term to restrict parameter changes on normal data, effectively
    mitigating the issue of catastrophic forgetting. A Bayesian unlearning framework
    proposed by [[237](#bib.bib237)] provides a natural trade-off interpretation between
    fully unlearning from erased data and retaining posterior beliefs by learning
    on the entire dataset. Additionally, scrubbing procedure [[236](#bib.bib236)]
    minimizes loss on the remaining data while maximizing forgetting on the target
    forgetting dataset by introducing random noise to the model weights, allowing
    for forgetting on the specified dataset while retaining knowledge on the remaining
    data. Furthermore, PUMA [[241](#bib.bib241)], aims to mitigate the potential negative
    impact caused by the removal of targeted training data. PUMA achieves this by
    optimally reweighting the remaining data, ensuring that the model’s performance
    is not excessively affected when unlearning specific samples.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is evident that we cannot simply erase an excessive number of data
    points, as doing so would eventually lead to a decrease in performance on the
    test set. In a study by Sekhari et al. [[232](#bib.bib232)], the authors conduct
    a theoretical analysis to determine the maximum number of samples that can be
    deleted from a pre-trained model without compromising the performance on the test
    set. This analysis provides insights into the trade-off between machine unlearning
    on a specific subset of training set and maintaining the generalization performance
    on test set.
  prefs: []
  type: TYPE_NORMAL
- en: 10.3.5 Application of Machine Unlearning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Machine unlearning techniques find application in various domains, including
    backdoor attack defense and defending against membership inference attacks. In
    the context of backdoor attack defense [[242](#bib.bib242)], machine unlearning
    can be used to erase the backdoor trigger that has been injected into a trained
    model. By selectively unlearning the specific information related to the backdoor
    trigger, the model can be cleansed of its malicious behavior. Similarly, machine
    unlearning can also serve as a defense mechanism [[241](#bib.bib241), [235](#bib.bib235)]
    against membership inference attacks. By removing the specific training data from
    a pre-trained model, the traces or characteristics of these data are eliminated,
    making it harder for attackers to infer membership or extract sensitive information.
  prefs: []
  type: TYPE_NORMAL
- en: 11 Discussion and Future Prospect
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 11.1 Summary and Research Trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Cross Discipline Research About Forgetting: Cross-disciplinary research is
    vital for advancing machine learning disciplines. Various fields offer valuable
    insights and innovations through cross-disciplinary research. This exploration
    encompasses both the application of techniques from CL to address challenges in
    other research domains and applications of techniques from various fields to solve
    problems in CL. These interdisciplinary research efforts pave the way for the
    advancements in different research areas. In future work, we anticipate that actively
    promoting collaboration and facilitating idea exchange across diverse research
    areas will be crucial in overcoming disciplinary barriers.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intentional Forgetting in Machine Learning: Intentional forgetting has emerged
    as a promising approach to enhance model performance and address data privacy
    concerns in recent studies. On one hand, intentional forgetting proves valuable
    in eliminating biased and irrelevant information during model training, thereby
    mitigating overfitting and improving generalization. On the other hand, in the
    context of machine unlearning, forgetting plays a crucial role in removing private
    data to safeguard data privacy.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Growing Trend for Theoretical Analysis: Recently, there is an emerging trend
    towards conducting theoretical analyses to delve deeper into the understanding
    and analysis of forgetting across diverse research fields. These theoretical analyses
    aim to provide more valuable insights and uncover fundamental principles that
    govern forgetting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Addressing Forgetting with Foundation Models: The foundation model has demonstrated
    promising potential in tackling forgetting across various research fields, making
    it an active and promising area of research.'
  prefs: []
  type: TYPE_NORMAL
- en: 11.2 Open Research Questions and Future Prospect
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the following, we prospect several potential research directions for future
    study.
  prefs: []
  type: TYPE_NORMAL
- en: 'In-depth Theoretical Analysis of Forgetting: The majority of existing methods
    are empirical in nature, lacking theoretical guarantees and comprehensive analysis.
    Therefore, there is a pressing need for more thorough theoretical analysis across
    different learning domains. Future research efforts should aim to bridge this
    gap by incorporating rigorous theoretical analysis of forgetting.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Trustworthiness of Forgetting: Most existing studies on forgetting primarily
    concentrate on its impact on model performance while overlooking other crucial
    aspects such as robustness, fairness, transparency, etc. To bridge this research
    gap, future investigations should dedicate more efforts to analyze and understand
    the effects of forgetting on these trustworthiness properties.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Proper Trade-off Between Memorization and Forgetting: Striking a balance between
    remembering previous knowledge and protecting privacy poses a critical research
    problem. Remembering more previous knowledge can increase the risk of memorizing
    private information, potentially compromising privacy. Conversely, prioritizing
    privacy protection may result in sacrificing performance on previous tasks. Achieving
    an appropriate equilibrium between memorization and forgetting is crucial to address
    this challenge effectively. Future research should focus on developing methodologies
    and techniques that enable effective management of knowledge retention while ensuring
    privacy preservation, thereby striking the optimal balance between the two.'
  prefs: []
  type: TYPE_NORMAL
- en: 12 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Forgetting is a prevalent phenomenon across various machine learning fields,
    driven by different factors. This survey aims to offer a comprehensive examination
    of the forgetting issue in diverse machine learning domains, such as continual
    learning, foundation model, meta-learning, domain adaptation, test time adaptation,
    generative model, federated learning, reinforcement learning and machine unlearning.
    Our survey aims to provide a thorough overview and understanding of the research
    progress on forgetting, with the intention of inspiring future investigations
    into intriguing and innovative research directions and methods. While existing
    surveys on continual learning often emphasize the harmful aspects of forgetting,
    our survey argues that forgetting can also be beneficial. We present application
    scenarios where selective forgetting improves generalization performance and learns
    new tasks, and intentional forgetting of memorized examples helps safeguard the
    privacy of machine learning systems.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. McCloskey and N. J. Cohen, “Catastrophic interference in connectionist
    networks: The sequential learning problem,” *Psychology of learning and motivation*,
    1989.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] G. I. Parisi, R. Kemker, J. L. Part, C. Kanan, and S. Wermter, “Continual
    lifelong learning with neural networks: A review,” *Neural Networks*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] G. M. van de Ven and A. S. Tolias, “Three scenarios for continual learning,”
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] M. De Lange, R. Aljundi, M. Masana, S. Parisot, X. Jia, A. Leonardis, G. Slabaugh,
    and T. Tuytelaars, “A continual learning survey: Defying forgetting in classification
    tasks,” *TPAMI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] Z. Mai, R. Li, J. Jeong, D. Quispe, H. Kim, and S. Sanner, “Online continual
    learning in image classification: An empirical survey,” *Neurocomputing*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] M. Masana, X. Liu, B. Twardowski, M. Menta, A. D. Bagdanov, and J. van de
    Weijer, “Class-incremental learning: survey and performance evaluation on image
    classification,” *TPAMI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D.-W. Zhou, Q.-W. Wang, Z.-H. Qi, H.-J. Ye, D.-C. Zhan, and Z. Liu, “Deep
    class-incremental learning: A survey,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] J. A. Mendez and E. Eaton, “How to reuse and compose knowledge for a lifetime
    of tasks: A survey on continual learning and functional composition,” *Transactions
    on Machine Learning Research*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] L. Wang, X. Zhang, H. Su, and J. Zhu, “A comprehensive survey of continual
    learning: Theory, method and application,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] A. Ginart, M. Guan, G. Valiant, and J. Y. Zou, “Making ai forget you:
    Data deletion in machine learning,” *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership inference
    attacks against machine learning models,” in *SP*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] Y. Shi, L. Shen, K. Wei, Y. Sun, B. Yuan, X. Wang, and D. Tao, “Improving
    the model consistency of decentralized federated learning,” *ICML*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] Y. Cao and J. Yang, “Towards making systems forget with machine unlearning,”
    in *2015 IEEE symposium on security and privacy*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. Lopez-Paz and M. Ranzato, “Gradient episodic memory for continual learning,”
    *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] A. Chaudhry, M. Ranzato, M. Rohrbach, and M. Elhoseiny, “Efficient lifelong
    learning with a-gem,” *ICLR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] P. Buzzega, M. Boschini, A. Porrello, D. Abati, and S. Calderara, “Dark
    experience for general continual learning: a strong, simple baseline,” *NeurIPS*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] R. Tiwari, K. Killamsetty, R. Iyer, and P. Shenoy, “Gcr: Gradient coreset
    based replay buffer selection for continual learning,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] Z. Sun, Y. Mu, and G. Hua, “Regularizing second-order influences for continual
    learning,” in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] S.-A. Rebuffi, A. Kolesnikov, G. Sperl, and C. H. Lampert, “icarl: Incremental
    classifier and representation learning,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] R. Aljundi, M. Lin, B. Goujaud, and Y. Bengio, “Gradient based sample
    selection for online continual learning,” in *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] J. Bang, H. Kim, Y. Yoo, J.-W. Ha, and J. Choi, “Rainbow memory: Continual
    learning with a memory of diverse samples,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] A. Chaudhry, P. K. Dokania, T. Ajanthan, and P. H. Torr, “Riemannian walk
    for incremental learning: Understanding forgetting and intransigence,” in *ECCV*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] R. Aljundi, E. Belilovsky, T. Tuytelaars, L. Charlin, M. Caccia, M. Lin,
    and L. Page-Caccia, “Online continual learning with maximal interfered retrieval,”
    *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] W. Chenshen, L. Herranz, L. Xialei *et al.*, “Memory replay gans: Learning
    to generate images from new categories without forgetting,” in *NeurIPS*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] Y. Xiang, Y. Fu, P. Ji, and H. Huang, “Incremental learning using conditional
    adversarial networks,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] R. Kemker and C. Kanan, “Fearnet: Brain-inspired model for incremental
    learning,” in *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] Q. Jodelet, X. Liu, Y. J. Phua, and T. Murata, “Class-incremental learning
    using diffusion model for distillation and replay,” *CoRR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] J. Smith, Y.-C. Hsu, J. Balloch, Y. Shen, H. Jin, and Z. Kira, “Always
    be dreaming: A new approach for data-free class-incremental learning,” in *ICCV*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] G. M. Van de Ven, H. T. Siegelmann, and A. S. Tolias, “Brain-inspired
    replay for continual learning with artificial neural networks,” *Nature communications*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] K. Jeeveswaran, P. Bhat, B. Zonooz, and E. Arani, “Birt: Bio-inspired
    replay in vision transformers for continual learning,” *ICML*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] L. Wang, X. Zhang, K. Yang, L. Yu, C. Li, L. HONG, S. Zhang, Z. Li, Y. Zhong,
    and J. Zhu, “Memory replay with data compression for continual learning,” in *ICLR*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] Z. Luo, Y. Liu, B. Schiele, and Q. Sun, “Class-incremental exemplar compression
    for class-incremental learning,” in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] Y. Liu, Y. Su, A. Liu, B. Schiele, and Q. Sun, “Mnemonics training: Multi-class
    incremental learning without forgetting,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] Z. Deng and O. Russakovsky, “Remember the past: Distilling datasets into
    addressable memories for neural networks,” in *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] G. Cazenavette, T. Wang, A. Torralba, A. A. Efros, and J.-Y. Zhu, “Dataset
    distillation by matching training trajectories,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] A. A. Rusu, N. C. Rabinowitz, G. Desjardins, H. Soyer, J. Kirkpatrick,
    K. Kavukcuoglu, R. Pascanu, and R. Hadsell, “Progressive neural networks,” 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] C. Fernando, D. Banarse, C. Blundell, Y. Zwols, D. Ha, A. A. Rusu, A. Pritzel,
    and D. Wierstra, “Pathnet: Evolution channels gradient descent in super neural
    networks,” *arXiv preprint arXiv:1701.08734*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] J. Yoon, E. Yang, J. Lee, and S. J. Hwang, “Lifelong learning with dynamically
    expandable networks,” *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] J. Serrá, D. Surís, M. Miron, and A. Karatzoglou, “Overcoming catastrophic
    forgetting with hard attention to the task,” in *ICML*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] G. Bellec, D. Kappel, W. Maass, and R. Legenstein, “Deep rewiring: Training
    very sparse deep networks,” in *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] Z. Wang, Z. Zhan, Y. Gong, G. Yuan, W. Niu, T. Jian, B. Ren, S. Ioannidis,
    Y. Wang, and J. Dy, “Sparcl: Sparse continual learning on the edge,” in *NeurIPS*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] A. Mallya, D. Davis, and S. Lazebnik, “Piggyback: Adapting a single network
    to multiple tasks by learning to mask weights,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] A. Mallya and S. Lazebnik, “Packnet: Adding multiple tasks to a single
    network by iterative pruning,” in *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] C.-Y. Hung, C.-H. Tu, C.-E. Wu, C.-H. Chen, Y.-M. Chan, and C.-S. Chen,
    “Compacting, picking and growing for unforgetting continual learning,” *NeurIPS*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] X. Li, Y. Zhou, T. Wu, R. Socher, and C. Xiong, “Learn to grow: A continual
    structure learning framework for overcoming catastrophic forgetting,” in *ICML*,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] S. Yan, J. Xie, and X. He, “Der: Dynamically expandable representation
    for class incremental learning,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] J. Kirkpatrick, R. Pascanu, N. Rabinowitz, J. Veness, G. Desjardins, A. A.
    Rusu, K. Milan, J. Quan, T. Ramalho, A. Grabska-Barwinska, D. Hassabis, C. Clopath,
    D. Kumaran, and R. Hadsell, “Overcoming catastrophic forgetting in neural networks,”
    *Proceedings of the national academy of sciences*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] F. Zenke, B. Poole, and S. Ganguli, “Continual learning through synaptic
    intelligence,” in *ICML*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] J. von Oswald, C. Henning, J. Sacramento, and B. F. Grewe, “Continual
    learning with hypernetworks,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] R. Aljundi, F. Babiloni, M. Elhoseiny, M. Rohrbach, and T. Tuytelaars,
    “Memory aware synapses: Learning what (not) to forget,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] H. Ahn, S. Cha, D. Lee, and T. Moon, “Uncertainty-based continual learning
    with adaptive regularization,” *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] G. Hinton, O. Vinyals, and J. Dean, “Distilling the knowledge in a neural
    network,” *arXiv preprint arXiv:1503.02531*, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] Z. Li and D. Hoiem, “Learning without forgetting,” *TPAMI*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] P. Dhar, R. V. Singh, K.-C. Peng, Z. Wu, and R. Chellappa, “Learning without
    memorizing,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] I. Fostiropoulos, J. Zhu, and L. Itti, “Batch model consolidation: A multi-task
    model consolidation framework,” in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] K. Lee, K. Lee, J. Shin, and H. Lee, “Overcoming catastrophic forgetting
    with unlabeled data in the wild,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] M. Farajtabar, N. Azizan, A. Mott, and A. Li, “Orthogonal gradient descent
    for continual learning,” 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] T. Doan, M. A. Bennani, B. Mazoure, G. Rabusseau, and P. Alquier, “A theoretical
    analysis of catastrophic forgetting through the NTK overlap matrix,” in *AISTATS*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] A. Chaudhry, N. Khan, P. K. Dokania, and P. H. Torr, “Continual learning
    in low-rank orthogonal subspaces,” in *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] G. Zeng, Y. Chen, B. Cui, and S. Yu, “Continual learning of context-dependent
    processing in neural networks,” *Nature Machine Intelligence*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] G. Saha, I. Garg, and K. Roy, “Gradient projection memory for continual
    learning,” *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] D. DENG, G. Chen, J. HAO, Q. Wang, and P.-A. Heng, “Flattening sharpness
    for dynamic gradient projection memory benefits continual learning,” in *NeurIPS*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] S. Lin, L. Yang, D. Fan, and J. Zhang, “TRGP: Trust region gradient projection
    for continual learning,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] C. V. Nguyen, Y. Li, T. D. Bui, and R. E. Turner, “Variational continual
    learning,” *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S.-W. Lee, J.-H. Kim, J. Jun, J.-W. Ha, and B.-T. Zhang, “Overcoming catastrophic
    forgetting by incremental moment matching,” *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] T. Adel, H. Zhao, and R. E. Turner, “Continual learning with adaptive
    weights (claw),” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] R. Kurle, B. Cseke, A. Klushyn, P. van der Smagt, and S. Günnemann, “Continual
    learning with bayesian neural networks for non-stationary data,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] C. Henning, M. Cervera, F. D’Angelo, J. V. Oswald, R. Traber, B. Ehret,
    S. Kobayashi, B. F. Grewe, and J. Sacramento, “Posterior meta-replay for continual
    learning,” in *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] T.-C. Kao, K. Jensen, G. van de Ven, A. Bernacchia, and G. Hennequin,
    “Natural continual learning: success is a journey, not (just) a destination,”
    *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] P. Pan, S. Swaroop, A. Immer, R. Eschenhagen, R. Turner, and M. E. E.
    Khan, “Continual deep learning by functional regularisation of memorable past,”
    *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] M. K. Titsias, J. Schwarz, A. G. d. G. Matthews, R. Pascanu, and Y. W.
    Teh, “Functional regularisation for continual learning with gaussian processes,”
    in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] S. Kapoor, T. Karaletsos, and T. D. Bui, “Variational auto-regressive
    gaussian processes for continual learning,” in *ICML*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] T. G. Rudner, F. B. Smith, Q. Feng, Y. W. Teh, and Y. Gal, “Continual
    learning via sequential function-space variational inference,” in *ICML*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] A. Kumar, S. Chatterjee, and P. Rai, “Bayesian structural adaptation for
    continual learning,” in *ICML*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] N. Mehta, K. Liang, V. K. Verma, and L. Carin, “Continual learning using
    a bayesian nonparametric dictionary of weight factors,” in *AISTATS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] A. Chaudhry, M. Rohrbach, M. Elhoseiny, T. Ajanthan, P. K. Dokania, P. H. S.
    Torr, and M. Ranzato, “Continual learning with tiny episodic memories,” 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] X. Jin, A. Sadhu, J. Du, and X. Ren, “Gradient-based editing of memory
    examples for online task-free continual learning,” *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] Z. Wang, L. Shen, L. Fang, Q. Suo, T. Duan, and M. Gao, “Improving task-free
    continual learning by distributionally robust memory evolution,” in *ICML*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] S. Lee, J. Ha, D. Zhang, and G. Kim, “A neural dirichlet process mixture
    model for task-free continual learning,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] R. Ardywibowo, Z. Huo, Z. Wang, B. J. Mortazavi, S. Huang, and X. Qian,
    “VariGrow: Variational architecture growing for task-agnostic continual learning
    based on Bayesian novelty,” in *ICML*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] F. Ye and A. G. Bors, “Task-free continual learning via online discrepancy
    distance learning,” in *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] D. Shim, Z. Mai, J. Jeong, S. Sanner, H. Kim, and J. Jang, “Online class-incremental
    continual learning with adversarial shapley value,” in *AAAI*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Y. Guo, B. Liu, and D. Zhao, “Online continual learning through mutual
    information maximization,” in *ICML*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] J. Yoon, D. Madaan, E. Yang, and S. J. Hwang, “Online coreset selection
    for rehearsal-based continual learning,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] Y. Gu, X. Yang, K. Wei, and C. Deng, “Not just selection, but exploration:
    Online class-incremental continual learning via dual view consistency,” in *CVPR*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] G. Gupta, K. Yadav, and L. Paull, “La-maml: Look-ahead meta learning for
    continual learning,” in *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] Y. Liu, W. Zhu, and S. Ren, “Navigating memory construction by global
    pseudo-task simulation for continual learning,” in *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] E. Fini, S. Lathuiliere, E. Sangineto, M. Nabi, and E. Ricci, “Online
    continual learning under extreme memory constraints,” in *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] J. Schwarz, J. Luketina, W. M. Czarnecki, A. Grabska-Barwinska, Y. W.
    Teh, R. Pascanu, and R. Hadsell, “Progress and compress: A scalable framework
    for continual learning,” in *ICML*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] F. M. Castro, M. J. Marín-Jiménez, N. Guil, C. Schmid, and K. Alahari,
    “End-to-end incremental learning,” in *ECCV*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] A. Prabhu, P. H. Torr, and P. K. Dokania, “Gdumb: A simple approach that
    questions our progress in continual learning,” in *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] Y. Wu, Y. Chen, L. Wang, Y. Ye, Z. Liu, Y. Guo, and Y. Fu, “Large scale
    incremental learning,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] H. Ahn, J. Kwak, S. Lim, H. Bang, H. Kim, and T. Moon, “Ss-il: Separated
    softmax for incremental learning,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Chrysakis and M.-F. Moens, “Online continual learning from imbalanced
    data,” in *ICML*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] C. D. Kim, J. Jeong, and G. Kim, “Imbalanced continual learning with partitioning
    reservoir sampling,” in *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] S. Sun, D. Calandriello, H. Hu, A. Li, and M. Titsias, “Information-theoretic
    online memory selection for continual learning,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] E. Belouadah and A. Popescu, “Il2m: Class incremental learning with dual
    memory,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] B. Zhao, X. Xiao, G. Gan, B. Zhang, and S.-T. Xia, “Maintaining discrimination
    and fairness in class incremental learning,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] A. Chrysakis and M.-F. Moens, “Online bias correction for task-free continual
    learning,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] W. Ren, P. Wang, X. Li, C. E. Hughes, and Y. Fu, “Semi-supervised drifted
    stream learning with short lookback,” in *SIGKDD*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] L. Wang, K. Yang, C. Li, L. Hong, Z. Li, and J. Zhu, “Ordisco: Effective
    and efficient usage of incremental unlabeled data for semi-supervised continual
    learning,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] J. Smith, J. Balloch, Y.-C. Hsu, and Z. Kira, “Memory-efficient semi-supervised
    continual learning: The world is its own replay buffer,” in *IJCNN*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] B. Yang, M. Lin, Y. Zhang, B. Liu, X. Liang, R. Ji, and Q. Ye, “Dynamic
    support network for few-shot class incremental learning,” *TPAMI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] X. Tao, X. Hong, X. Chang, S. Dong, X. Wei, and Y. Gong, “Few-shot class-incremental
    learning,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] K. Zhu, Y. Cao, W. Zhai, J. Cheng, and Z.-J. Zha, “Self-promoted prototype
    refinement for few-shot class-incremental learning,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] M. Hersche, G. Karunaratne, G. Cherubini, L. Benini, A. Sebastian, and
    A. Rahimi, “Constrained few-shot class-incremental learning,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] Y. Yang, H. Yuan, X. Li, Z. Lin, P. Torr, and D. Tao, “Neural collapse
    inspired feature-classifier alignment for few-shot class-incremental learning,”
    in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] D.-W. Zhou, H.-J. Ye, L. Ma, D. Xie, S. Pu, and D.-C. Zhan, “Few-shot
    class-incremental learning by sampling multi-phase tasks,” *TPAMI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] Z. Chi, L. Gu, H. Liu, Y. Wang, Y. Yu, and J. Tang, “Metafscil: a meta-learning
    approach for few-shot class incremental learning,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] P. Mazumder, P. Singh, and P. Rai, “Few-shot lifelong learning,” in *AAAI*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] D.-Y. Kim, D.-J. Han, J. Seo, and J. Moon, “Warping the space: Weight
    space rotation for class-incremental few-shot learning,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] D. Rao, F. Visin, A. A. Rusu, Y. W. Teh, R. Pascanu, and R. Hadsell,
    “Continual unsupervised representation learning,” in *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] D. Madaan, J. Yoon, Y. Li, Y. Liu, and S. J. Hwang, “Representational
    continuity for unsupervised continual learning,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] H. Cha, J. Lee, and J. Shin, “Co2l: Contrastive continual learning,”
    in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] M. Davari, N. Asadi, S. Mudur, R. Aljundi, and E. Belilovsky, “Probing
    representation forgetting in supervised and unsupervised continual learning,”
    in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] H. Zhang, M. Cissé, Y. N. Dauphin, and D. Lopez-Paz, “mixup: Beyond empirical
    risk minimization,” in *ICLR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. Pentina and C. Lampert, “A pac-bayesian bound for lifelong learning,”
    in *ICML*, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] R. Karakida and S. Akaho, “Learning curves for continual learning in
    neural networks: Self-knowledge transfer and forgetting,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] G. Kim, C. Xiao, T. Konishi, Z. Ke, and B. Liu, “A theoretical study
    on solving continual learning,” in *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] I. Evron, E. Moroshko, G. Buzaglo, M. Khriesh, B. Marjieh, N. Srebro,
    and D. Soudry, “Continual learning in linear classification on separable data,”
    in *ICML*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] L. Peng, P. V. Giampouras, and R. Vidal, “The ideal continual learner:
    An agent that never forgets,” in *ICML*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] A. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts,
    P. Barham, H. W. Chung, C. Sutton, S. Gehrmann *et al.*, “Palm: Scaling language
    modeling with pathways,” *arXiv preprint arXiv:2204.02311*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix,
    B. Rozière, N. Goyal, E. Hambro, F. Azhar *et al.*, “Llama: Open and efficient
    foundation language models,” *arXiv preprint arXiv:2302.13971*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] M. Jagielski, O. Thakkar, F. Tramer, D. Ippolito, K. Lee, N. Carlini,
    E. Wallace, S. Song, A. G. Thakurta, N. Papernot, and C. Zhang, “Measuring forgetting
    of memorized training examples,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] C. Lee, K. Cho, and W. Kang, “Mixout: Effective regularization to finetune
    large-scale pretrained language models,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] J. Howard and S. Ruder, “Universal language model fine-tuning for text
    classification,” in *ACL*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] C. Chelba and A. Acero, “Adaptation of maximum entropy capitalizer: Little
    data can help a lot,” *Computer Speech & Language*, 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] T. Zhang, F. Wu, A. Katiyar, K. Q. Weinberger, and Y. Artzi, “Revisiting
    few-sample bert fine-tuning,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] Z. Fatemi, C. Xing, W. Liu, and C. Xiong, “Improving gender fairness
    of pre-trained language models without catastrophic forgetting,” in *ACL*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] X. Dong, A. T. Luu, M. Lin, S. Yan, and H. Zhang, “How should pre-trained
    language models be fine-tuned towards adversarial robustness?” *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] S. Chen, Y. Hou, Y. Cui, W. Che, T. Liu, and X. Yu, “Recall and learn:
    Fine-tuning deep pretrained language models with less forgetting,” in *EMNLP*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] D. Hu, S. Yan, Q. Lu, L. HONG, H. Hu, Y. Zhang, Z. Li, X. Wang, and J. Feng,
    “How well does self-supervised pre-training perform with streaming data?” in *ICLR*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] S. Purushwalkam, P. Morgado, and A. Gupta, “The challenges of continuous
    self-supervised learning,” in *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] Z. Lin, Y. Wang, and H. Lin, “Continual contrastive learning for image
    classification,” in *ICME*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] N. Lukas, A. Salem, R. Sim, S. Tople, L. Wutschitz, and S. Zanella-Béguelin,
    “Analyzing leakage of personally identifiable information in language models,”
    *S&P*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] N. Carlini, D. Ippolito, M. Jagielski, K. Lee, F. Tramer, and C. Zhang,
    “Quantifying memorization across neural language models,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] Z. Ke, B. Liu, N. Ma, H. Xu, and L. Shu, “Achieving forgetting prevention
    and knowledge transfer in continual learning,” *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] T. Wu, M. Caccia, Z. Li, Y.-F. Li, G. Qi, and G. Haffari, “Pretrained
    language model in continual learning: A comparative study,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] T. Scialom, T. Chakrabarty, and S. Muresan, “Fine-tuned language models
    are continual learners,” in *EMNLP*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    Ł. Kaiser, and I. Polosukhin, “Attention is all you need,” *NeurIPS*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner,
    M. Dehghani, M. Minderer, G. Heigold, S. Gelly *et al.*, “An image is worth 16x16
    words: Transformers for image recognition at scale,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] O. Ostapenko, T. Lesort, P. Rodríguez, M. R. Arefin, A. Douillard, I. Rish,
    and L. Charlin, “Continual learning with foundation models: An empirical study
    of latent replay,” in *Conference on Lifelong Learning Agents*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] S. V. Mehta, D. Patil, S. Chandar, and E. Strubell, “An empirical investigation
    of the role of pre-training in lifelong learning,” *ICML*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] V. V. Ramasesh, A. Lewkowycz, and E. Dyer, “Effect of scale on catastrophic
    forgetting in neural networks,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] D.-W. Zhou, H.-J. Ye, D.-C. Zhan, and Z. Liu, “Revisiting class-incremental
    learning with pre-trained models: Generalizability and adaptivity are all you
    need,” *arXiv preprint arXiv:2303.07338*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] B. Ermis, G. Zappella, M. Wistuba, A. Rawal, and C. Archambeau, “Memory
    efficient continual learning with transformers,” in *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] Z. Wang, Z. Zhang, C.-Y. Lee, H. Zhang, R. Sun, X. Ren, G. Su, V. Perot,
    J. Dy, and T. Pfister, “Learning to prompt for continual learning,” in *CVPR*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] Z. Wang, Z. Zhang, S. Ebrahimi, R. Sun, H. Zhang, C. Lee, X. Ren, G. Su,
    V. Perot, J. G. Dy, and T. Pfister, “Dualprompt: Complementary prompting for rehearsal-free
    continual learning,” in *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] Y. Wang, Z. Huang, and X. Hong, “S-prompts learning with pre-trained
    transformers: An occam’s razor for domain incremental learning,” in *NeurIPS*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] A. Razdaibiedina, Y. Mao, R. Hou, M. Khabsa, M. Lewis, and A. Almahairi,
    “Progressive prompts: Continual learning for language models,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] H. Liu, M. Long, J. Wang, and Y. Wang, “Learning to adapt to evolving
    domains,” *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] A. Bobu, E. Tzeng, J. Hoffman, and T. Darrell, “Adapting to continuously
    shifting domains,” 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] M. Rostami, “Lifelong domain adaptation via consolidated internal distribution,”
    in *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] T. Panagiotakopoulos, P. L. Dovesi, L. Härenstam-Nielsen, and M. Poggi,
    “Online domain adaptation for semantic segmentation in ever-changing conditions,”
    in *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] S. Tang, P. Su, D. Chen, and W. Ouyang, “Gradient regularized contrastive
    learning for continual domain adaptation,” in *AAAI*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] R. Volpi, D. Larlus, and G. Rogez, “Continual adaptation of visual representations
    via domain randomization and meta-learning,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] A. M. N. Taufique, C. S. Jahan, and A. Savakis, “Unsupervised continual
    learning for gradually varying domains,” in *CVPR Workshops*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] S. Yang, Y. Wang, J. Van De Weijer, L. Herranz, and S. Jui, “Generalized
    source-free domain adaptation,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] H. Feng, Z. Yang, H. Chen, T. Pang, C. Du, M. Zhu, W. Chen, and S. Yan,
    “Cosda: Continual source-free domain adaptation,” *arXiv preprint arXiv:2304.06627*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] W. Ahmed, P. Morerio, and V. Murino, “Continual source-free unsupervised
    domain adaptation,” *arXiv preprint arXiv:2304.07374*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] D. Wang, E. Shelhamer, S. Liu, B. Olshausen, and T. Darrell, “Tent: Fully
    test-time adaptation by entropy minimization,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] S. Niu, J. Wu, Y. Zhang, Y. Chen, S. Zheng, P. Zhao, and M. Tan, “Efficient
    test-time model adaptation without forgetting,” in *ICML*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] T. Gong, J. Jeong, T. Kim, Y. Kim, J. Shin, and S.-J. Lee, “Note: Robust
    continual test-time adaptation against temporal correlation,” *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] S. Niu, J. Wu, Y. Zhang, Z. Wen, Y. Chen, P. Zhao, and M. Tan, “Towards
    stable test-time adaptation in dynamic wild world,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] J. Liang, R. He, and T. Tan, “A comprehensive survey on test-time adaptation
    under distribution shifts,” 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] J. Hong, L. Lyu, J. Zhou, and M. Spranger, “MECTA: Memory-economic continual
    test-time model adaptation,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] M. M. Zhang, S. Levine, and C. Finn, “MEMO: Test time robustness via
    adaptation and augmentation,” in *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] Q. Wang, O. Fink, L. Van Gool, and D. Dai, “Continual test-time domain
    adaptation,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] Y. Gan, X. Ma, Y. Lou, Y. Bai, R. Zhang, N. Shi, and L. Luo, “Decorate
    the newcomers: Visual domain prompt for continual test time adaptation,” *AAAI*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] J. Song, J. Lee, I. S. Kweon, and S. Choi, “Ecotta: Memory-efficient
    continual test-time adaptation via self-distilled regularization,” in *CVPR*,
    2023, pp. 11 920–11 929.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] S. Choi, S. Yang, S. Choi, and S. Yun, “Improving test-time adaptation
    via shift-agnostic weight regularization and nearest source prototypes,” in *ECCV*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] D. Brahma and P. Rai, “A probabilistic framework for lifelong test-time
    adaptation,” in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] S. Gidaris and N. Komodakis, “Dynamic few-shot visual learning without
    forgetting,” *CVPR*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] M. Ren, R. Liao, E. Fetaya, and R. S. Zemel, “Incremental few-shot learning
    with attention attractor networks,” *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] S. W. Yoon, D.-Y. Kim, J. Seo, and J. Moon, “Xtarnet: Learning to extract
    task-adaptive representation for incremental few-shot learning,” *ICML*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] G. SHI, J. Chen, W. Zhang, L.-M. Zhan, and X.-M. Wu, “Overcoming catastrophic
    forgetting in incremental few-shot learning by finding flat minima,” in *NeurIPS*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] C. Finn, A. Rajeswaran, S. Kakade, and S. Levine, “Online meta-learning,”
    in *ICML*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] G. Jerfel, E. Grant, T. L. Griffiths, and K. Heller, “Reconciling meta-learning
    and continual learning with online mixtures of tasks,” *NeurIPS*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] C. Finn, P. Abbeel, and S. Levine, “Model-agnostic meta-learning for
    fast adaptation of deep networks,” *ICML*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] P. Yap, H. Ritter, and D. Barber, “Addressing catastrophic forgetting
    in few-shot problems,” in *ICML*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] Q. Zhang, J. Fang, Z. Meng, S. Liang, and E. Yilmaz, “Variational continual
    bayesian meta-learning,” in *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] Z. Wang, L. Shen, T. Duan, D. Zhan, L. Fang, and M. Gao, “Learning to
    learn and remember super long multi-domain task sequence,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] Z. Wang, T. Duan, L. Fang, Q. Suo, and M. Gao, “Meta learning on a sequence
    of imbalanced domains with difficulty awareness,” in *ICCV*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] Z. Wang, L. Shen, L. Fang, Q. Suo, D. Zhan, T. Duan, and M. Gao, “Meta-learning
    with less forgetting on large-scale non-stationary task distributions,” in *ECCV*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] H. Thanh-Tung and T. Tran, “On catastrophic forgetting and mode collapse
    in generative adversarial networks,” in *IJCNN*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] K. J. Liang, C. Li, G. Wang, and L. Carin, “Generative adversarial network
    training is a continual learning problem,” *arXiv preprint arXiv:1811.11083*,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] H. Chen, Y. Wang, C. Xu, Z. Yang, C. Liu, B. Shi, C. Xu, C. Xu, and Q. Tian,
    “Data-free learning of student networks,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] K. Binici, N. T. Pham, T. Mitra, and K. Leman, “Preventing catastrophic
    forgetting and distribution mismatch in knowledge distillation via synthetic data,”
    in *WACV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] K. Binici, S. Aggarwal, N. T. Pham, K. Leman, and T. Mitra, “Robust and
    resource-efficient data-free knowledge distillation by generative pseudo replay,”
    in *AAAI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] K. Do, T. H. Le, D. Nguyen, D. Nguyen, H. Harikumar, T. Tran, S. Rana,
    and S. Venkatesh, “Momentum adversarial distillation: Handling large distribution
    shifts in data-free knowledge distillation,” *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] G. Patel, K. R. Mopuri, and Q. Qiu, “Learning to retain while acquiring:
    Combating distribution-shift in adversarial data-free knowledge distillation,”
    in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] M. Zhai, L. Chen, F. Tung, J. He, M. Nawhal, and G. Mori, “Lifelong gan:
    Continual learning for conditional image generation,” in *ICCV*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] M. Zhai, L. Chen, and G. Mori, “Hyper-lifelonggan: Scalable lifelong
    learning for image conditioned generation,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] J. Ramapuram, M. Gregorova, and A. Kalousis, “Lifelong generative modeling,”
    *Neurocomputing*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] F. Ye and A. G. Bors, “Lifelong mixture of variational autoencoders,”
    *TNNLS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] ——, “Continual variational autoencoder learning via online cooperative
    memorization,” in *ECCV*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] J. Ho, A. Jain, and P. Abbeel, “Denoising diffusion probabilistic models,”
    *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] C. Saharia, W. Chan, S. Saxena, L. Li, J. Whang, E. L. Denton, K. Ghasemipour,
    R. Gontijo Lopes, B. Karagol Ayan, T. Salimans *et al.*, “Photorealistic text-to-image
    diffusion models with deep language understanding,” *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] E. Zhang, K. Wang, X. Xu, Z. Wang, and H. Shi, “Forget-me-not: Learning
    to forget in text-to-image diffusion models,” *arXiv preprint arXiv:2303.17591*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] A. Heng and H. Soh, “Selective amnesia: A continual learning approach
    to forgetting in deep generative models,” *arXiv preprint arXiv:2305.10120*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] K. Khetarpal, M. Riemer, I. Rish, and D. Precup, “Towards continual reinforcement
    learning: A review and perspectives,” *Journal of Artificial Intelligence Research*,
    vol. 75, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] M. Igl, G. Farquhar, J. Luketina, W. Boehmer, and S. Whiteson, “Transient
    non-stationarity and generalisation in deep reinforcement learning,” in *ICLR*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Z. Daniels, A. Raghavan, J. Hostetler, A. Rahman, I. Sur, M. Piacentino,
    and A. Divakaran, “Model-free generative replay for lifelong reinforcement learning:
    Application to starcraft-2,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] J.-B. Gaya, T. Doan, L. Caccia, L. Soulier, L. Denoyer, and R. Raileanu,
    “Building a subspace of policies for scalable continual learning,” in *ICLR*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] J. Yoon, W. Jeong, G. Lee, E. Yang, and S. J. Hwang, “Federated continual
    learning with weighted inter-client transfer,” in *ICML*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] S. P. Karimireddy, S. Kale, M. Mohri, S. Reddi, S. Stich, and A. T. Suresh,
    “Scaffold: Stochastic controlled averaging for federated learning,” in *ICML*,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] N. Shoham, T. Avidor, A. Keren, N. Israel, D. Benditkis, L. Mor-Yosef,
    and I. Zeitak, “Overcoming forgetting in federated learning on non-iid data,”
    *NeurIPS Workshop*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] W. Huang, M. Ye, and B. Du, “Learn from others and be yourself in heterogeneous
    federated learning,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] C. Xu, Z. Hong, M. Huang, and T. Jiang, “Acceleration of federated learning
    with alleviated forgetting in local training,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] G. Lee, M. Jeong, Y. Shin, S. Bae, and S.-Y. Yun, “Preservation of the
    global knowledge by not-true distillation in federated learning,” *NeurIPS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] K. Luo, X. Li, Y. Lan, and M. Gao, “Gradma: A gradient-memory-based accelerated
    federated learning with alleviated catastrophic forgetting,” in *CVPR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] L. Qu, Y. Zhou, P. P. Liang, Y. Xia, F. Wang, E. Adeli, L. Fei-Fei, and
    D. Rubin, “Rethinking architecture design for tackling data heterogeneity in federated
    learning,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] J. Dong, L. Wang, Z. Fang, G. Sun, S. Xu, X. Wang, and Q. Zhu, “Federated
    class-incremental learning,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] D. Qi, H. Zhao, and S. Li, “Better generative replay for continual federated
    learning,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] Y. Ma, Z. Xie, J. Wang, K. Chen, and L. Shou, “Continual federated learning
    based on knowledge distillation,” in *IJCAI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] K. Audhkhasi, G. Saon, Z. Tüske, B. Kingsbury, and M. Picheny, “Forget
    a bit to learn better: Soft forgetting for ctc-based automatic speech recognition.”
    in *Interspeech*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] T. Shibata, G. Irie, D. Ikami, and Y. Mitsuzumi, “Learning with selective
    forgetting.” in *IJCAI*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] E. Nikishin, M. Schwarzer, P. D’Oro, P.-L. Bacon, and A. Courville, “The
    primacy bias in deep reinforcement learning,” in *ICML*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] B. Han, G. Niu, X. Yu, Q. Yao, M. Xu, I. Tsang, and M. Sugiyama, “Sigua:
    Forgetting may make learning with noisy labels more robust,” in *ICML*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] A. Jaiswal, D. Moyer, G. Ver Steeg, W. AbdAlmageed, and P. Natarajan,
    “Invariant representations through adversarial forgetting,” in *AAAI*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] L. Gravitz, “The importance of forgetting,” *Nature*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] S. Baik, S. Hong, and K. M. Lee, “Learning to forget for meta-learning,”
    in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] H. Zhou, A. Vani, H. Larochelle, and A. Courville, “Fortuitous forgetting
    in connectionist networks,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] B. Kim, H. Kim, K. Kim, S. Kim, and J. Kim, “Learning not to learn: Training
    deep neural networks with biased data,” in *CVPR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] P. J. Bevan and A. Atapour-Abarghouei, “Skin deep unlearning: Artefact
    and instrument debiasing in the context of melanoma classification,” in *ICML*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] Y. Chen, S. Zhang, and B. K. H. Low, “Near-optimal task selection for
    meta-learning with mutual information and online variational bayesian unlearning,”
    in *AISTATS*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] L. Wang, M. Zhang, Z. Jia, Q. Li, C. Bao, K. Ma, J. Zhu, and Y. Zhong,
    “Afec: Active forgetting of negative transfer in continual learning,” *NeurIPS*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers,
    B. Zhang, D. Lie, and N. Papernot, “Machine unlearning,” in *SSP*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] T. T. Nguyen, T. T. Huynh, P. L. Nguyen, A. W.-C. Liew, H. Yin, and Q. V. H.
    Nguyen, “A survey of machine unlearning,” 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] A. Mantelero, “The eu proposal for a general data protection regulation
    and the roots of the ‘right to be forgotten’,” *Computer Law & Security Review*,
    2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] Y. Wu, E. Dobriban, and S. Davidson, “DeltaGrad: Rapid retraining of
    machine learning models,” in *ICML*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] A. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh, “Remember what you
    want to forget: Algorithms for machine unlearning,” *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] E. Ullah, T. Mai, A. Rao, R. A. Rossi, and R. Arora, “Machine unlearning
    via algorithmic stability,” in *COLT*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] H. Yan, X. Li, Z. Guo, H. Li, F. Li, and X. Lin, “Arcane: An efficient
    architecture for exact machine unlearning,” in *IJCAI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] C. Guo, T. Goldstein, A. Hannun, and L. van der Maaten, “Certified data
    removal from machine learning models,” in *ICML*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] A. Golatkar, A. Achille, and S. Soatto, “Eternal sunshine of the spotless
    net: Selective forgetting in deep networks,” in *CVPR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] Q. P. Nguyen, B. K. H. Low, and P. Jaillet, “Variational bayesian unlearning,”
    *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] R. Mehta, S. Pal, V. Singh, and S. N. Ravi, “Deep unlearning via randomized
    conditionally independent hessians,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] M. Du, Z. Chen, C. Liu, R. Oak, and D. Song, “Lifelong anomaly detection
    through unlearning,” in *SIGSAC*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Z. Ma, Y. Liu, X. Liu, J. Liu, J. Ma, and K. Ren, “Learn to forget: Machine
    unlearning via neuron masking,” *IEEE Transactions on Dependable and Secure Computing*,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] G. Wu, M. Hashemi, and C. Srinivasa, “Puma: Performance unchanged model
    augmentation for training data removal,” in *AAAI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] Y. Liu, M. Fan, C. Chen, X. Liu, Z. Ma, L. Wang, and J. Ma, “Backdoor
    defense with machine unlearning,” in *IEEE INFOCOM 2022*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] H. Shin, J. K. Lee, J. Kim, and J. Kim, “Continual learning with deep
    generative replay,” *NeurIPS*, vol. 30, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] M. Riemer, I. Cases, R. Ajemian, M. Liu, I. Rish, Y. Tu, and G. Tesauro,
    “Learning to learn without forgetting by maximizing transfer and minimizing interference,”
    *ICLR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] Y. Liu, B. Schiele, and Q. Sun, “Rmm: Reinforced memory management for
    class-incremental learning,” *NeurIPS*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] F.-K. Sun, C.-H. Ho, and H.-Y. Lee, “Lamol: Language modeling for lifelong
    language learning,” in *ICLR*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] A. Iscen, J. Zhang, S. Lazebnik, and C. Schmid, “Memory-efficient incremental
    learning through feature adaptation,” in *ECCV*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] F. Zhu, X.-Y. Zhang, C. Wang, F. Yin, and C.-L. Liu, “Prototype augmentation
    and self-supervision for incremental learning,” in *CVPR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] K. Zhu, W. Zhai, Y. Cao, J. Luo, and Z. Zha, “Self-sustaining representation
    expansion for non-exemplar class-incremental learning,” in *CVPR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] B. Zhao, K. R. Mopuri, and H. Bilen, “Dataset condensation with gradient
    matching,” in *ICLR*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] M. Wortsman, V. Ramanujan, R. Liu, A. Kembhavi, M. Rastegari, J. Yosinski,
    and A. Farhadi, “Supermasks in superposition,” *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] T. Konishi, M. Kurokawa, C. Ono, Z. Ke, G. Kim, and B. Liu, “Parameter-level
    soft-masking for continual learning,” *ICML*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] H. Kang, R. J. L. Mina, S. R. H. Madjid, J. Yoon, M. Hasegawa-Johnson,
    S. J. Hwang, and C. D. Yoo, “Forget-free continual learning with winning subnetworks,”
    in *ICML*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] J. Frankle and M. Carbin, “The lottery ticket hypothesis: Finding sparse,
    trainable neural networks,” in *ICLR*, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] M. B. Gurbuz and C. Dovrolis, “Nispa: Neuro-inspired stability-plasticity
    adaptation for continual learning in sparse networks,” in *ICML*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] R. Aljundi, P. Chakravarty, and T. Tuytelaars, “Expert gate: Lifelong
    learning with a network of experts,” in *CVPR*, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] D.-W. Zhou, Q.-W. Wang, H.-J. Ye, and D.-C. Zhan, “A model or 603 exemplars:
    Towards memory-efficient class-incremental learning,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] Z. Hu, Y. Li, J. Lyu, D. Gao, and N. Vasconcelos, “Dense network expansion
    for class incremental learning,” in *CVPR*, 2023, pp. 11 858–11 867.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Y. Wen, D. Tran, and J. Ba, “Batchensemble: an alternative approach to
    efficient ensemble and lifelong learning,” in *ICLR*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] S. Jung, H. Ahn, S. Cha, and T. Moon, “Continual learning with node-importance
    based adaptive group sparse regularization,” *NeurIPS*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] Y. Guo, W. Hu, D. Zhao, and B. Liu, “Adaptive orthogonal projection for
    batch and online continual learning,” in *AAAI*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] A. F. Akyürek, E. Akyürek, D. Wijaya, and J. Andreas, “Subspace regularizers
    for few-shot class incremental learning,” in *ICLR*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Appendix A Continual Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A.1 Task-aware CL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Existing works proposed for solving task-aware CL have five major research
    branches: memory-based, architecture-based, regularization-based, subspace-based,
    and Bayesian-based CL methods. We give an overall framework in Fig. [1](#S2.F1
    "Figure 1 ‣ 2.1.1 Task-aware CL ‣ 2.1 Task-Aware and Task-Free CL ‣ 2 Forgetting
    in Continual Learning ‣ A Comprehensive Survey of Forgetting in Deep Learning
    Beyond Continual Learning"). We present the details of each class method in the
    following.'
  prefs: []
  type: TYPE_NORMAL
- en: Memory-based Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Memory-based method keeps a *memory buffer* that stores the data examples from
    previous tasks and replay those examples during learning new tasks, representative
    works include [[243](#bib.bib243), [15](#bib.bib15), [244](#bib.bib244), [76](#bib.bib76),
    [23](#bib.bib23)]. It can be further categorized into: (1) raw memory replay;
    (2) memory sample selection; (3) generative replay; (4) compressed memory replay.
    Next, we discuss each direction in detail.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Raw Sample Replay: These methods save a small amount of data from previous
    tasks and trains the model with the new task. A key issue in this type of approach
    is how to select raw samples to store in the memory buffer. Current research works
    contain random selection [[76](#bib.bib76)] and selection based on heuristic rules [[22](#bib.bib22),
    [23](#bib.bib23)]. The random selection method randomly saves some raw samples
    into the memory, and performs replay [[76](#bib.bib76)] or constraints [[14](#bib.bib14),
    [15](#bib.bib15)] when learning a new task. For example, GEM [[14](#bib.bib14)]
    and A-GEM [[15](#bib.bib15)] use the memory buffer data losses as inequality constraints
    to constrain their increase but allow their decrease to mitigate forgetting. Meta
    Experience Replay (MER) [[244](#bib.bib244)] uses meta-learning to encourage information
    transfer from previous tasks and minimize interference. DER++ [[16](#bib.bib16)]
    found that replaying old data with soft labels is significantly better than one-hot
    labels.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory Sample Selection: These simplest methods to mitigate forgetting is to
    randomly store some examples for replay. However, this may lead to sub-optimal
    performance since random selection overlooks the informativeness of each sample.
    The heuristic selection selects samples for storage based on certain rules. e.g.,
    iCaRL [[19](#bib.bib19)] selects the samples closest to each category cluster
    center. Some works choose the samples closest to the decision boundary of each
    class [[22](#bib.bib22), [23](#bib.bib23)] or make the sample diversity in memory
    the highest [[20](#bib.bib20), [21](#bib.bib21)]. GCR [[17](#bib.bib17)] selects
    the corset that best represents the gradients produced by past data. Sun et al. [[18](#bib.bib18)]
    model and regularize the influence of each selected example on the future. In
    addition, the above schemes of static memory allocation (fixed total capacity
    or storing a specific number of samples per class) may also lead to suboptimal
    performance. RMM [[245](#bib.bib245)] proposes a reinforcement learning based
    strategy to automatically allocate capacity for old and new classes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Generative Replay: When privacy concerns restrict the storage of raw memory
    data, generative replay provides an alternative approach in CL to replay previous
    task data. The main concept behind generative replay is to train a generative
    model capable of capturing and remembering the data distribution from previous
    tasks. Instead of storing raw samples, the generative model is used to generate
    synthetic samples representative of the old tasks  [[246](#bib.bib246)]. The representative
    works in this line involve using different generative models, include GAN-based
    methods [[24](#bib.bib24), [25](#bib.bib25)], AutoEncoder-based methods [[26](#bib.bib26)],
    Diffusion-based model [[27](#bib.bib27)], and Model-inversion [[28](#bib.bib28)].'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compressed Memory Replay: In scenarios where storage constraints are stringent
    on edge devices, memory efficiency becomes a crucial consideration. Several research
    efforts have aimed to improve memory efficiency in CL by employing different strategies.
    For example, instead of directly storing high-resolution images, one approach
    is to store feature representations  [[29](#bib.bib29), [247](#bib.bib247), [30](#bib.bib30)]
    or feature prototype extracted from the image  [[248](#bib.bib248), [249](#bib.bib249)],
    or compress the high-fidelity image into a low-fidelity image (JPEG) for storage [[31](#bib.bib31),
    [32](#bib.bib32)]. Recently, dataset condensation has emerged as a promising approach
    for compressing and condensing datasets, as demonstrated by works such as [[250](#bib.bib250),
    [35](#bib.bib35)]. Drawing inspiration from data distillation techniques, many
    CL methods adopt the strategy of storing condensed samples from old tasks in memory
    for replay  [[33](#bib.bib33), [34](#bib.bib34)].'
  prefs: []
  type: TYPE_NORMAL
- en: Architecture-based Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Architecture-based methods in CL [[36](#bib.bib36), [37](#bib.bib37), [38](#bib.bib38)]
    involve updating the network architecture during the learning process to retain
    previously acquired knowledge. These methods aim to adapt the model’s architecture
    to accommodate new tasks while preserving the knowledge from previous tasks. Based
    on whether the model parameters expand with the number of tasks, architecture-based
    methods can be categorized into two types: fixed-capacity and capacity-increasing
    methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Fixed-Capacity: In these methods, the amount of CL model’s parameters does
    not increase with the number of tasks, and each task selects a sub-network from
    the CL model to achieve knowledge transfer and reduce the forgetting caused by
    sub-network updates. Common subnetwork selection techniques include masking [[39](#bib.bib39),
    [40](#bib.bib40), [41](#bib.bib41)], and pruning [[42](#bib.bib42), [43](#bib.bib43),
    [44](#bib.bib44)]. On the one hand, the mask-based method isolates the update
    of important neurons or parameters of old tasks during the backpropagation of
    the new task, thereby alleviating the forgetting of old tasks. A representative
    method, HAT [[39](#bib.bib39)] proposes a task-based binary/hard attention mask
    mechanism on neurons instead of parameters. Unlike HAT, which requires task identification
    during testing, SupSup [[251](#bib.bib251)] proposes a gradient-based optimization
    strategy, which can automatically infer tasks and find the learned optimal mask
    during testing. In addition, different from the above-mentioned hard mask methods
    that monopolize the sub-network, which hinders knowledge transfer and has the
    problem of excessive consumption of network capacity by old tasks, SPG [[252](#bib.bib252)]
    proposes a soft mask mechanism that uses the importance of old task parameters
    as a mask to constrain the back-propagation gradient flow updates to important
    parameters. On the other hand, similar to the mask mechanism, the pruning-based
    method freezes important parameters to avoid forgetting after the old tasks are
    learned, and reinitializes unimportant parameters to learn new tasks. PackNet [[43](#bib.bib43)]
    iteration performs pruning of a well-trained base network and maintains the binary
    sparse mask. Similarly, WSN [[253](#bib.bib253)] selects an optimal sub-network
    (winning ticket) for each task based on the Lottery Ticket Hypothesis [[254](#bib.bib254)].
    Finally, unlike all the above methods that start masking or pruning from a densely
    connected network, although NISPA [[255](#bib.bib255)] also fixes the total number
    of neurons, it only starts learning tasks from a sparsely connected network, and
    then continuously increases the connections between neurons to learn new tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Capacity-Increasing: a fixed capacity CL model may face limitations in accommodating
    new tasks as the number of tasks increases. This can restrict the plasticity of
    new tasks due to insufficient remaining network capacity. To overcome this challenge,
    dynamic capacity methods have been proposed. These methods augment the CL model
    by introducing task-specific new parameters for each new task while freezing the
    parameters associated with old tasks. By doing so, dynamic capacity methods aim
    to prevent forgetting while allowing the model to adapt and learn new tasks effectively [[36](#bib.bib36),
    [38](#bib.bib38), [45](#bib.bib45), [46](#bib.bib46)]. For example, PNN [[36](#bib.bib36)]
    employs an independent network branch for each task, where the input of the new
    task branch is the output from the old task branch, enabling knowledge transfer
    between tasks. ExpertGate [[256](#bib.bib256)], MEMO [[257](#bib.bib257)], and
    DNE [[258](#bib.bib258)] also add task-specialized experts/blocks/layers for each
    new task, respectively. BatchEnsemble [[259](#bib.bib259)] freezes the backbone
    network after learning the first task, and learns two independent rank-one matrices
    for each new task. However, it also limits knowledge transfer between tasks starting
    from the second task.'
  prefs: []
  type: TYPE_NORMAL
- en: Regularization-based Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Regularization-based methods in CL involve the addition of regularization loss
    terms to the training objective to prevent forgetting of previously learned knowledge
    [[47](#bib.bib47), [48](#bib.bib48), [49](#bib.bib49)]. These methods aim to constrain
    the model’s parameter updates during the learning process to ensure that valuable
    information from past tasks is retained. Regularization-based methods can be further
    divided into two subcategories: penalizing important parameter updates and knowledge
    distillation using a previous model as a teacher.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Penalize Parameter Updates: Some works alleviate forgetting by penalizing parameter
    updates that are important for old tasks. For example, EWC [[47](#bib.bib47)]
    calculates regularization terms by approximating Fisher information matrix (FIM).
    MAS [[50](#bib.bib50)] takes the cumulative update of the parameters as a penalty
    term. SI [[48](#bib.bib48)] computes parameter importance using the path integral
    of gradient vector fields during parameter updating. Rwalk [[22](#bib.bib22)]
    can be seen as a generalized version of SI and EWC++. UCL [[51](#bib.bib51)] proposes
    an uncertain regularization based on the Bayesian online learning framework. AGS-CL [[260](#bib.bib260)]
    proposes two group sparsity penalties based on node importance as regularization
    terms.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowledge-Distillation-Based: Inspired by knowledge distillation [[52](#bib.bib52)],
    several methods in CL incorporate a distillation loss between the network of the
    previous task (referred to as the teacher) and the network of the current task
    (referred to as the student) to mitigate forgetting. One method that follows this
    approach is Learning without Forgetting (LwF) [[53](#bib.bib53)], which considers
    the soft targets generated by the teacher network as additional learning objectives
    for the student model. By treating the soft targets as valuable information, LwF
    enables the current model to learn from the knowledge of the previous model, reducing
    the risk of forgetting. Building upon LwF, LwM [[54](#bib.bib54)] leverages the
    attention mechanism of the previous network to guide the training of the current
    network. The attention of the teacher network helps the student model focus on
    relevant features and retain important knowledge from past tasks. Another method,
    BMC [[55](#bib.bib55)], takes a different approach by training multiple expert
    models simultaneously on disjoint sets of tasks. These expert models are then
    aggregated through knowledge distillation, which involves transferring knowledge
    from the expert models to a single student model. On the other hand, in distillation-based
    CL methods, the ideal scenario would involve using the raw data of old tasks to
    extract the knowledge of the teacher model and distill it to the student model.
    However, accessing the raw data of old tasks is often not feasible due to data
    privacy concerns. Consequently, existing approaches utilize proxy data as a substitute
    for distillation. One such method is LwF [[53](#bib.bib53)], which uses the current
    task data as a proxy for distillation. Another approach, GD [[56](#bib.bib56)],
    utilizes large-scale unlabeled data from the wild as a proxy for distillation.'
  prefs: []
  type: TYPE_NORMAL
- en: Subspace-based Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Subspace-based methods in CL aim to address the issue of interference between
    multiple tasks by conducting learning in separate and disjoint subspaces. Subspace-based
    methods can be categorized into two types based on how the subspaces are constructed:
    orthogonal gradient subspace and orthogonal feature subspace methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Orthogonal Gradient Subspace: These methods require that the parameter update
    direction of the new task is orthogonal to the gradient subspace of the old tasks,
    ensuring minimal interference between tasks. OGD [[57](#bib.bib57)] proposes updating
    the CL model in a gradient subspace orthogonal to the subspace of the old tasks.
    PCAOGD [[58](#bib.bib58)] is an extension of OGD that only stores the top principal
    components of gradients for each task. ORTHOG-SUBSPACE [[59](#bib.bib59)] learns
    different tasks in different orthogonal (low-rank) subspaces to minimize interference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Orthogonal Feature Subspace: These require that the parameter update direction
    of the new task is orthogonal to the subspace crossed by the input (feature) of
    the old task. OWM [[60](#bib.bib60)] proposes that when learning a new task, network
    parameters are only updated in a direction orthogonal to the subspace spanned
    by the inputs of all previous tasks. AOP [[261](#bib.bib261)] solves the problem
    of inaccurate input space estimation in OWM. GPM [[61](#bib.bib61)] proposes to
    construct subspace for past tasks, the network updates the network parameters
    taking gradient steps in the orthogonal direction to the gradient subspaces that
    are important for the past learned tasks. Deng et al.[[62](#bib.bib62)] propose
    a Flattening Sharpness for Dynamic Gradient Projection Memory (FS-DGPM) with soft
    weight on the basis of GPM to improve new task learning. TRGP [[63](#bib.bib63)]
    introduces a trust-region variant of GPM to tackle the challenge of balancing
    the learning of new tasks with preserving the knowledge of old tasks.'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian Method
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: 'Bayesian methods provide a principled probabilistic framework for addressing
    CF and can be classified into three categories: (1) methods that constrain the
    update of weight parameter distributions, (2) methods that constrain the update
    in function space, and (3) methods that dynamically grow the CL model architecture
    in an adaptive and Bayesian manner. These Bayesian approaches offer effective
    strategies to mitigate CF by incorporating uncertainty estimation and regularization
    techniques, thereby enhancing the adaptability of the learning process. In the
    following paragraphs, we provide a detailed discussion of each research direction.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weight Space Regularization: Weight space regularization based methods model
    the parameter update uncertainty and enforce the model parameter (weight space)
    distribution when learning the new task is close to that of all the previously
    learned tasks, including [[64](#bib.bib64), [65](#bib.bib65), [66](#bib.bib66)].
    Gaussian Residual Scoring (GRS) [[67](#bib.bib67)] extends Bayesian neural network
    to the non-stationary streaming data. Different from previous work, which updates
    the parameter posterior distribution recursively over the task sequence, posterior
    meta-replay [[68](#bib.bib68)] which learns the task-specific posteriors via a
    single shared meta-model, improving the flexibility in representing the current
    task and previous tasks. Natural Continual Learning (NCL) [[69](#bib.bib69)] combines
    Bayesian inference with gradient projection methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Function Space Regularization: Different from weight space regularization which
    constrains the weight update, the function space regularization regulates the
    CL function update in the function space. Functional Regularisation of Memorable
    Past (FROMP) [[70](#bib.bib70)] enforce the posterior distribution over function
    space (instead of weight space) of the new task to be close to that of the all
    the previously learned task with Gaussian process. In contrast to previous works
    that primarily emphasize either parameter or function space regularization, Functional
    Regularized Continual Learning (FRCL) [[71](#bib.bib71)] takes a different approach
    by focusing on constraining neural network predictions to prevent significant
    deviations from solutions to previous tasks. Variational Auto-Regressive Gaussian
    Processes (VAR-GPs) [[72](#bib.bib72)] further proposes a framework through a
    principled posterior updating mechanism to naturally model the cross-task covariances.
    Sequential function-space variational inference (S-FSVI) [[73](#bib.bib73)] presents
    a framework for addressing CL by formulating it as sequential function-space variational
    inference. This approach offers several advantages, including the ability to employ
    more flexible variational distributions and more effective regularization techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Bayesian Architecture Expansion: Bayesian architecture growing methods employ
    a probabilistic and Bayesian approach to dynamically expand the CL model. By leveraging
    Bayesian principles, these methods enable the CL model to incrementally grow and
    adapt to new tasks or data while preserving previously learned knowledge. This
    probabilistic framework facilitates the flexible and principled expansion of the
    model’s architecture, allowing it to accommodate increasing complexity and variability
    in the learning process. Kumar et al. [[74](#bib.bib74)] introduce a principled
    framework for CL that incorporates both variational Bayes and a nonparametric
    Bayesian modeling paradigm. Their approach enables the continual learning of neural
    network structures, offering a systematic methodology for adapting and expanding
    the network architecture over time. Moreover, the work by Mehta et al. [[75](#bib.bib75)]
    introduces IBP-WF, a method that leverages a principled Bayesian nonparametric
    approach, to dynamically expand the network architecture based on the task complexity.
    This approach allows the model to scale and accommodate increasing demands and
    intricacies of the learning tasks, ensuring efficient and effective adaptation
    throughout the CL process.'
  prefs: []
  type: TYPE_NORMAL
- en: A.2 Few-shot CL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to tackle the forgetting problem within the context of few-shot CL,
    existing approaches employ various techniques, including metric learning, meta-learning,
    and parameter regularization. These techniques will be elaborated upon in the
    following.
  prefs: []
  type: TYPE_NORMAL
- en: Metric Learning-Based
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These methods perform classification by class prototypes. To avoid forgetting,
    the prototype of the new class should be separable from the old class [[105](#bib.bib105)],
    and the prototype of the old class should not change drastically during the adjustment
    process of the new class [[107](#bib.bib107)]. TOPIC [[104](#bib.bib104)] first
    defines the benchmark-setting of few-shot class-incremental learning (FSCIL),
    which represents the topological structure of different classes in the feature
    space, and alleviates the forgetting of old classes by keeping the topology stable.
    To alleviate forgetting, DSN [[103](#bib.bib103)] estimates the distribution of
    classes in the current task. In a new task, feature vectors are sampled from old
    class distributions for replay. Another two approaches, C-FSCIL  [[106](#bib.bib106)]
    and FSCIL [[107](#bib.bib107)] try to reduce interference between prototypes.
    C-FSCIL maps input images to quasi-orthogonal prototypes to minimize task interference.
    Also, FSCIL pre-assigns and fixes the feature prototype classifier, and trains
    a projection layer to project the sample features of each class onto the assigned
    prototype, thereby avoiding the interference between classes and the problem of
    forgetting.
  prefs: []
  type: TYPE_NORMAL
- en: Meta-Learning-Based
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These methods simulate the inference phase during training so that CL models
    can quickly adapt to unseen new classes to solve few-shot CL. LIMIT [[108](#bib.bib108)]
    and MetaFSCIL [[109](#bib.bib109)] split the base task into multiple ’fake’-incremental
    tasks, so that the model has the learning ability of FSCIL tasks. Specifically,
    MetaFSCIL directly considers the adaptability of new tasks and the stability of
    old tasks as its primary objectives for meta-learning. When MetaFSCIL trains on
    a new class, it evaluates the performance on all encountered classes as a meta-objective.
    By reducing the loss associated with the meta-objective, MetaFSCIL minimizes forgetting
    of the old tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Parameter Regularization-Based
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: These methods employ various strategies to address the forgetting problem by
    penalizing parameter updates that are important for old tasks. For example, FSLL [[110](#bib.bib110)]
    and WaRP [[111](#bib.bib111)] adopt an approach where certain crucial parameters
    are frozen during training, while the remaining parameters are fine-tuned specifically
    for the few-shot task. Another approach, Subspace Regularization [[262](#bib.bib262)],
    introduces a subspace regularization that encourages the weights of new classes
    to be close to the subspace spanned by the weights of existing classes.
  prefs: []
  type: TYPE_NORMAL
