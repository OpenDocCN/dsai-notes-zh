- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:38:41'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:38:41
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2306.15110] Deep Transfer Learning for Intelligent Vehicle Perception: a Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2306.15110] 智能车辆感知的深度迁移学习：一项综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.15110](https://ar5iv.labs.arxiv.org/html/2306.15110)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2306.15110](https://ar5iv.labs.arxiv.org/html/2306.15110)
- en: 'Deep Transfer Learning for Intelligent Vehicle Perception: a Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 智能车辆感知的深度迁移学习：一项综述
- en: Xinyu Liu¹, Jinlong Li¹, Jin Ma¹, Huiming Sun¹, Zhigang Xu²,
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 刘欣宇¹，李金龙¹，马进¹，孙辉明¹，许志刚²，
- en: Tianyun Zhang¹, Hongkai Yu¹ ¹ Department of Electrical Engineering and Computer
    Science, Cleveland State University, Cleveland, OH 44115, USA
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 张天云¹，余宏凯¹ ¹ 电子工程与计算机科学系，克利夫兰州立大学，克利夫兰，OH 44115，美国
- en: ² School of Information Engineering, Chang’an University, Xi’an 710064, China
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: ² 信息工程学院，长安大学，西安 710064，中国
- en: Abstract
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning-based intelligent vehicle perception has been developing prominently
    in recent years to provide a reliable source for motion planning and decision
    making in autonomous driving. A large number of powerful deep learning-based methods
    can achieve excellent performance in solving various perception problems of autonomous
    driving. However, these deep learning methods still have several limitations,
    for example, the assumption that lab-training (source domain) and real-testing
    (target domain) data follow the same feature distribution may not be practical
    in the real world. There is often a dramatic domain gap between them in many real-world
    cases. As a solution to this challenge, deep transfer learning can handle situations
    excellently by transferring the knowledge from one domain to another. Deep transfer
    learning aims to improve task performance in a new domain by leveraging the knowledge
    of similar tasks learned in another domain before. Nevertheless, there are currently
    no survey papers on the topic of deep transfer learning for intelligent vehicle
    perception. To the best of our knowledge, this paper represents the first comprehensive
    survey on the topic of the deep transfer learning for intelligent vehicle perception.
    This paper discusses the domain gaps related to the differences of sensor, data,
    and model for the intelligent vehicle perception. The recent applications, challenges,
    future researches in intelligent vehicle perception are also explored.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的智能车辆感知近年来得到了显著的发展，为自动驾驶中的运动规划和决策提供了可靠的来源。大量强大的深度学习方法可以在解决自动驾驶的各种感知问题上取得优异的表现。然而，这些深度学习方法仍然存在一些局限性，例如，实验室训练（源领域）和实际测试（目标领域）数据假设遵循相同的特征分布在实际中可能并不切实际。在许多实际案例中，它们之间通常存在显著的领域差距。作为应对这一挑战的解决方案，深度迁移学习通过将知识从一个领域转移到另一个领域来处理这些情况。深度迁移学习旨在通过利用在另一个领域之前学习到的类似任务的知识来提高新领域中的任务表现。然而，目前尚无关于智能车辆感知的深度迁移学习的综述论文。根据我们所知，这篇论文代表了关于智能车辆感知的深度迁移学习的首个全面综述。本文讨论了与智能车辆感知相关的传感器、数据和模型的领域差距，还探讨了智能车辆感知的最新应用、挑战和未来研究。
- en: 'keywords:'
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 关键词：
- en: 'deep transfer learning, domain gap, intelligent vehicle perception, autonomous
    driving^†^†journal: Green Energy and Intelligent Transportation'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度迁移学习，领域差距，智能车辆感知，自动驾驶^†^†期刊：绿色能源与智能交通
- en: 1 Introduction
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: 'In recent years, perception has been viewed as a critical component in intelligent
    vehicles for precise localization, safe motion planning, and robust control Li
    et al. ([2020a](#bib.bib69)), Yurtsever et al. ([2020](#bib.bib171)), Huang and
    Chen ([2020](#bib.bib56)). The perception system provides intelligent vehicles
    with immediate environmental information about surrounding pedestrians, vehicles,
    traffic signs, and other items and helps to avoid possible collisions. In this
    paper, the terminology “perception” is mainly focused on the detection and segmentation
    tasks by ignoring the tracking and trajectory prediction tasks. This focus is
    because of two reasons: (1) Some previous intelligent vehicle research works mainly
    use detection and/or segmentation tasks to describe the terminology “perception”
    for intelligent vehicles Van Brummelen et al. ([2018](#bib.bib136)), Xu et al.
    ([2022b](#bib.bib159)); (2) Many downstream tasks, such as tracking, trajectory
    prediction, and behavior prediction, are dependent on the accurate detection and/or
    segmentation first.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，感知被视为智能车辆在精确定位、安全运动规划和强健控制中的关键组成部分，Li 等人（[2020a](#bib.bib69)），Yurtsever
    等人（[2020](#bib.bib171)），Huang 和 Chen（[2020](#bib.bib56)）。感知系统为智能车辆提供关于周围行人、车辆、交通标志及其他物品的即时环境信息，并帮助避免可能的碰撞。本文中的“感知”一词主要关注检测和分割任务，而忽略了跟踪和轨迹预测任务。这种关注的原因有两个：（1）一些之前的智能车辆研究工作主要使用检测和/或分割任务来描述“感知”这一术语，Van
    Brummelen 等人（[2018](#bib.bib136)），Xu 等人（[2022b](#bib.bib159)）；（2）许多下游任务，如跟踪、轨迹预测和行为预测，首先依赖于准确的检测和/或分割。
- en: 'The perception tasks play an indispensable role in intelligent vehicles and
    autonomous driving Arnold et al. ([2019](#bib.bib5)). Recently, the deep learning
    methods have gained significant traction in the intelligent vehicle perception
    and have achieved great successes Grigorescu et al. ([2020](#bib.bib42)), Wen
    and Jo ([2022](#bib.bib150)), Chen et al. ([2022](#bib.bib18)). For example, as
    shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Transfer Learning for
    Intelligent Vehicle Perception: a Survey"), four important applications have been
    actively studied and have gained significant advancements in recent years. (1)
    2D object detection is a prominent task which aims at recognizing and localizing
    objects within the images or videos. The main goal is to develop algorithms or
    models that can automatically detect objects and accurately outline their boundaries.
    (2) 3D object detection is a crucial task that focuses on discerning and precisely
    localizing objects within a three-dimensional space. 3D detection includes estimating
    the position of an object and orientation within a comprehensive 3D coordinate
    system. (3) Semantic segmentation is a pixel-level image analysis task where each
    pixel in an image is assigned a semantic label. The goal is to partition the image
    into coherent regions or segments based on the objects or classes they belong
    to. (4) Instance segmentation combines the object detection and semantic segmentation
    task. The objective is to detect and segment individual objects within an image,
    providing a unique label and pixel-level mask to each instance.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '感知任务在智能车辆和自动驾驶中发挥着不可或缺的作用，Arnold 等人（[2019](#bib.bib5)）。最近，深度学习方法在智能车辆感知中获得了显著关注，并取得了巨大成功，Grigorescu
    等人（[2020](#bib.bib42)），Wen 和 Jo（[2022](#bib.bib150)），Chen 等人（[2022](#bib.bib18)）。例如，如图
    [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Transfer Learning for Intelligent
    Vehicle Perception: a Survey") 所示，四个重要应用已被积极研究，并在近年来取得了显著进展。（1）2D 目标检测是一项显著的任务，旨在识别和定位图像或视频中的物体。主要目标是开发能够自动检测物体并准确勾画其边界的算法或模型。（2）3D
    目标检测是一项关键任务，重点在于辨别和精确定位三维空间中的物体。3D 检测包括估计物体的位置和在全面的 3D 坐标系统中的方向。（3）语义分割是一个像素级的图像分析任务，其中图像中的每个像素都被分配一个语义标签。目标是根据物体或类别将图像划分为一致的区域或分段。（4）实例分割结合了目标检测和语义分割任务。目标是在图像中检测和分割单个物体，为每个实例提供独特的标签和像素级的掩码。'
- en: 'However, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Transfer
    Learning for Intelligent Vehicle Perception: a Survey"), there are lots of complex
    cases where the deep learning methods might fail in the real world. For example,
    a deep learning based vehicle detection model pre-trained on clear weather data
    might be then tested in the foggy weather, low-illumination (like night), occlusion,
    or different data source (like simulation) conditions, leading to a large performance
    drop. This degradation is influenced by the domain gap (shift) between diverse
    driving environments Hnewa and Radha ([2020](#bib.bib48)), Mirza et al. ([2022](#bib.bib98)),
    Mohammed et al. ([2020](#bib.bib100)), e.g., different weather, illumination,
    occlusion, data source conditions. Moreover, different types and settings of the
    sensors Rist et al. ([2019](#bib.bib115)) installed on vehicles and various deep
    learning model structures Xu et al. ([2023a](#bib.bib160)) Khalil and Mouftah
    ([2022](#bib.bib60)) during the Vehicle-to-Vehicle (V2V) cooperative perception
    might result in the domain gap as well.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，如图[1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Transfer Learning for Intelligent
    Vehicle Perception: a Survey")所示，在现实世界中存在许多复杂情况，深度学习方法可能会失败。例如，一个基于深度学习的车辆检测模型在清晰天气数据上进行预训练后，可能会在雾霾天气、低光照（如夜晚）、遮挡或不同数据源（如仿真）条件下进行测试，导致性能大幅下降。这种性能下降受到多种驾驶环境之间的领域差距（偏移）的影响，Hnewa和Radha（[2020](#bib.bib48)）、Mirza等（[2022](#bib.bib98)）、Mohammed等（[2020](#bib.bib100)）等研究指出，例如不同的天气、光照、遮挡和数据源条件。此外，车辆上安装的不同类型和设置的传感器（Rist等，[2019](#bib.bib115)）以及各种深度学习模型结构（Xu等，[2023a](#bib.bib160)）Khalil和Mouftah（[2022](#bib.bib60)）在车辆间（V2V）协同感知过程中，也可能导致领域差距。'
- en: '![Refer to caption](img/3e463d3f80547b14a213941ac1a2fd66.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/3e463d3f80547b14a213941ac1a2fd66.png)'
- en: 'Figure 1: Illustration of Challenges and Applications of Intelligent Vehicle
    Perception. Transfer Learning (TL) methods can be applied to reduce the domain
    gaps by sensor difference, data difference, and model difference.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：智能车辆感知的挑战和应用的示意图。迁移学习（TL）方法可以应用于减少传感器差异、数据差异和模型差异带来的领域差距。
- en: 'The above mentioned performance drop for intelligent vehicle perception because
    of the domain gap can be relieved via the Transfer Learning (TL) Zhuang et al.
    ([2020](#bib.bib187)) methods. The TL techniques include two goals: (1) fully
    using the prior knowledge obtained from the source domain to guide the inference
    in the related target domain, (2) largely reducing the feature distribution discrepancy
    caused by the domain gap. Due to these two goals, the performance for deep learning
    based intelligent vehicle perception systems in related but different domains
    can be enhanced. The deep learning model’s generalization capability can be improved
    for the intelligent vehicle perception under different challenging scenarios as
    shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Transfer Learning for
    Intelligent Vehicle Perception: a Survey").'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '由于领域差距导致的智能车辆感知性能下降可以通过迁移学习（TL）方法（Zhuang等，[2020](#bib.bib187)）得到缓解。TL技术包括两个目标：（1）充分利用从源领域获得的先验知识来指导在相关目标领域的推断，（2）大幅减少因领域差距造成的特征分布差异。由于这两个目标，基于深度学习的智能车辆感知系统在相关但不同领域的性能可以得到提升。深度学习模型的泛化能力可以在不同挑战性场景下得到改善，如图[1](#S1.F1
    "Figure 1 ‣ 1 Introduction ‣ Deep Transfer Learning for Intelligent Vehicle Perception:
    a Survey")所示。'
- en: 'In this paper, we focus on the transfer learning methods for the intelligent
    vehicle perception in the deep learning era. This paper first reviews the related
    tasks and benchmark datasets for intelligent vehicle perception, and then classifies
    the domain gaps to three differences of sensor, data, and model during the vehicle
    driving. Next, we carefully review about 150 related published papers of deep
    transfer learning since the deep learning research is started, then we classify
    the deep learning based transfer learning methods into four types: (1) Supervised
    TL, (2) Unsupervised TL, (3) Weakly-and-semi Supervised TL, and (4) Domain Generalization.
    For the first three types, the transfer learning is implemented from one source
    domain to one target domain, where our classification depends on whether the target
    domain has labeled data or not. For the last type, the transfer learning is conducted
    from one source domain to multiple target domains for the generalization in many
    seen or unseen driving scenarios. In addition, several subdivisions of each type
    of transfer learning methods are also reviewed and analyzed in this survey.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 本文重点关注深度学习时代智能车辆感知的迁移学习方法。本文首先回顾了智能车辆感知的相关任务和基准数据集，然后将领域差距分为传感器、数据和模型三种差异。接下来，我们仔细回顾了约150篇深度迁移学习相关的发表论文，并将基于深度学习的迁移学习方法分类为四种类型：（1）有监督迁移学习，（2）无监督迁移学习，（3）弱监督和半监督迁移学习，以及（4）领域泛化。前面三种类型的迁移学习是从一个源领域到一个目标领域进行，我们的分类取决于目标领域是否有标注数据。最后一种类型的迁移学习是从一个源领域到多个目标领域，以实现许多已知或未知驾驶场景的泛化。此外，本文还对每种迁移学习方法的若干子分类进行了回顾和分析。
- en: The contributions of this paper can be outlined as follows.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本文的贡献可以概括如下。
- en: '1.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: To the best of our knowledge, this paper is the first in-depth survey on the
    topic of the deep transfer learning for intelligent vehicle perception.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 据我们所知，本文是关于智能车辆感知的深度迁移学习主题的首个深入调查。
- en: '2.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: This paper summarizes the domain gap for intelligent vehicle perception into
    three types (differences of sensor, data, model) and gives detailed explanations
    to the related tasks and benchmark datasets.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本文将智能车辆感知领域的差距总结为三种类型（传感器差异、数据差异、模型差异），并对相关任务和基准数据集进行了详细解释。
- en: '3.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: After reviewing about 150 related published papers, we classify the deep transfer
    learning methods for intelligent vehicle perception into four types and explain
    each of them in details.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在回顾了约150篇相关的发表论文后，我们将智能车辆感知的深度迁移学习方法分类为四种类型，并详细解释了每一种方法。
- en: 'The subsequent sections of this paper are structured as follows. Section [2](#S2
    "2 Intelligent Vehicle Perception ‣ Deep Transfer Learning for Intelligent Vehicle
    Perception: a Survey") provides a general overview about the related tasks and
    benchmark datasets for intelligent vehicle perception. Section [3](#S3 "3 Domain
    Distribution Discrepancy ‣ Deep Transfer Learning for Intelligent Vehicle Perception:
    a Survey") presents the domain distribution discrepancy and the three kinds of
    domain gap. Section [4](#S4 "4 Deep Transfer Learning Methodology ‣ Deep Transfer
    Learning for Intelligent Vehicle Perception: a Survey") details the different
    methodologies of the deep transfer learning techniques. Section [5](#S5 "5 Challenges
    and Future Research ‣ Deep Transfer Learning for Intelligent Vehicle Perception:
    a Survey") explains the challenges and future research, followed by a conclusion
    in Section [6](#S6 "6 Conclusion ‣ Deep Transfer Learning for Intelligent Vehicle
    Perception: a Survey").'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '本文的后续章节结构如下。第[2](#S2 "2 Intelligent Vehicle Perception ‣ Deep Transfer Learning
    for Intelligent Vehicle Perception: a Survey")节提供了关于智能车辆感知相关任务和基准数据集的一般概述。第[3](#S3
    "3 Domain Distribution Discrepancy ‣ Deep Transfer Learning for Intelligent Vehicle
    Perception: a Survey")节介绍了领域分布差异和三种领域差距。第[4](#S4 "4 Deep Transfer Learning Methodology
    ‣ Deep Transfer Learning for Intelligent Vehicle Perception: a Survey")节详细描述了深度迁移学习技术的不同方法。第[5](#S5
    "5 Challenges and Future Research ‣ Deep Transfer Learning for Intelligent Vehicle
    Perception: a Survey")节解释了挑战和未来研究，最后在第[6](#S6 "6 Conclusion ‣ Deep Transfer Learning
    for Intelligent Vehicle Perception: a Survey")节中做出总结。'
- en: 2 Intelligent Vehicle Perception
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 智能车辆感知
- en: For intelligent vehicles or autonomous driving, perception plays a crucial role
    in receiving data from sensors and extracting meaningful information from the
    surrounding environment, so as to make meaningful decisions for the precise motion
    planning by identifying obstacles, traffic signs/markers, and available driving
    areas Li et al. ([2023e](#bib.bib81)). Two types of mainstream sensors (Camera,
    LiDAR) are widely used in self-driving or intelligent driving vehicles Cao et al.
    ([2019](#bib.bib14)) Fadadu et al. ([2022](#bib.bib28)) Liu et al. ([2021b](#bib.bib88)) Liu
    et al. ([2022b](#bib.bib89)) Gholamhosseinian and Seitz ([2021](#bib.bib37)),
    Yu et al. ([2022](#bib.bib169)). These sensors installed on vehicles are utilized
    for the intelligent vehicle perception tasks.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于智能车辆或自动驾驶，感知在从传感器接收数据并提取周围环境中的有意义信息中发挥了至关重要的作用，从而通过识别障碍物、交通标志/标记和可用驾驶区域来做出有意义的决策，以进行精确的运动规划 Li
    et al. ([2023e](#bib.bib81))。两种主流传感器（相机、激光雷达）在自动驾驶或智能驾驶车辆中得到广泛使用 Cao et al. ([2019](#bib.bib14)) Fadadu
    et al. ([2022](#bib.bib28)) Liu et al. ([2021b](#bib.bib88)) Liu et al. ([2022b](#bib.bib89)) Gholamhosseinian
    and Seitz ([2021](#bib.bib37)), Yu et al. ([2022](#bib.bib169))。这些安装在车辆上的传感器用于智能车辆的感知任务。
- en: The intelligent vehicle perception tasks include discovering the surrounding
    vehicles and pedestrians, recognizing traffic signs and markers, finding the driving
    areas (e.g., road regions), and so on. In the real world, sometimes the objects
    may be similar to each other or the background, and the challenging scenarios
    (e.g., diverse weather, dark illumination) might affect the performance of sensors,
    making the perception tasks even more difficult Hnewa and Radha ([2020](#bib.bib48)),
    Li et al. ([2023c](#bib.bib75)). This paper groups these intelligent vehicle perception
    tasks into two classes (Object Detection, Semantic/Instance Segmentation) and
    further discusses these challenges for intelligent vehicle perception in the real
    world.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 智能车辆感知任务包括发现周围的车辆和行人，识别交通标志和标记，寻找驾驶区域（例如，道路区域）等。在现实世界中，有时物体可能与彼此或背景相似，并且挑战性的场景（例如，多样的天气、黑暗的照明）可能会影响传感器的性能，使得感知任务更加困难 Hnewa
    and Radha ([2020](#bib.bib48)), Li et al. ([2023c](#bib.bib75))。本文将这些智能车辆感知任务分为两个类别（物体检测、语义/实例分割），并进一步讨论了这些挑战。
- en: 2.1 Object Detection
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1 物体检测
- en: To achieve autonomous driving safely and successfully, it is necessary to have
    a reliable object detection system. Considering the complex road conditions, it
    is essential to detect (localize and recognize) other vehicles, pedestrians, and
    obstacles to prevent potential accidents. However, detecting objects in urban
    areas is challenging due to the diverse types of objects and unknown road situations Arnold
    et al. ([2019](#bib.bib5)), Feng et al. ([2020](#bib.bib29)).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安全和成功地实现自动驾驶，必须拥有一个可靠的物体检测系统。考虑到复杂的道路条件，检测（定位和识别）其他车辆、行人和障碍物对于防止潜在事故至关重要。然而，由于物体种类繁多和道路情况未知，在城市地区进行物体检测是具有挑战性的 Arnold
    et al. ([2019](#bib.bib5)), Feng et al. ([2020](#bib.bib29))。
- en: '2D Object Detection: By only using the relatively cheap camera sensor(s), deep
    learning models can be easily applied to efficiently detect (localize and recognize)
    the surrounding objects from the 2D image data Yeong et al. ([2021](#bib.bib166)).
    The output will be the identified 2D bounding boxes (2D coordinates) with the
    recognized object classes for the surrounding objects on each camera image, with
    a real-time or near real-time inference speed. However, 2D object detection alone
    can only provide the object’s position on a 2D plane, which does not provide enough
    information Wang et al. ([2019b](#bib.bib144)), e.g., object depth, object 3D
    size.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 2D 物体检测：仅使用相对便宜的相机传感器，深度学习模型可以被轻松应用于高效地检测（定位和识别）来自 2D 图像数据的周围物体 Yeong et al.
    ([2021](#bib.bib166))。输出将是每个相机图像上识别的 2D 边界框（2D 坐标）和识别的物体类别，具有实时或接近实时的推理速度。然而，单独的
    2D 物体检测只能提供物体在 2D 平面上的位置，这并不提供足够的信息 Wang et al. ([2019b](#bib.bib144))，例如物体深度、物体
    3D 尺寸。
- en: '3D Object Detection: Considering the limitations of 2D object detection, the
    object 3D information might equip the intelligent vehicle with the capability
    to more robustly and accurately perceive and recognize surrounding objects. The
    output will be the identified 3D bounding boxes (3D coordinates) with the recognized
    object classes for the surrounding objects, with a reasonable inference time.
    Because the images of camera sensors and the point clouds of LiDAR sensors could
    provide the depth cues, the 3D object detection task could be achieved via three
    sensor settings: (1) Camera only Wang et al. ([2023a](#bib.bib145)), (2) LiDAR
    only Xu et al. ([2023b](#bib.bib161)) Xu et al. ([2022b](#bib.bib159)) Li et al.
    ([2023a](#bib.bib73)), (3) Camera + LiDAR Zhao et al. ([2020](#bib.bib178)).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 3D目标检测：考虑到2D目标检测的局限性，物体的3D信息可能使智能车辆更稳健、准确地感知和识别周围物体。输出将是识别的3D边界框（3D坐标）和识别的物体类别，具有合理的推理时间。由于摄像头图像和激光雷达点云可以提供深度线索，3D目标检测任务可以通过以下三种传感器设置来实现：（1）仅摄像头 Wang
    et al. ([2023a](#bib.bib145))，（2）仅激光雷达 Xu et al. ([2023b](#bib.bib161)) Xu et al.
    ([2022b](#bib.bib159)) Li et al. ([2023a](#bib.bib73))，（3）摄像头 + 激光雷达 Zhao et al.
    ([2020](#bib.bib178))。
- en: 2.2 Semantic/Instance Segmentation
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2 语义/实例分割
- en: 'Different with the object detection task, the segmentation task not only discovers
    the object regions but also give the pixel-level labels (masks) for everything
    (object and background) in the driving scenarios. For the intelligent vehicle
    perception, the segmentation task can be classified into two types: Semantic Segmentation,
    Instance Segmentation.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 与目标检测任务不同，分割任务不仅发现物体区域，还为驾驶场景中的所有内容（包括物体和背景）提供像素级标签（掩码）。对于智能车辆感知，分割任务可以分为两种类型：**语义分割**和**实例分割**。
- en: 'Semantic Segmentation: Semantic segmentation involves the assignment of a semantic
    label to every pixel within an image, such as “road”, “vehicle” or “pedestrian”,
    “traffic sign”, and so on. This technique enables the intelligent vehicle to perceive
    the surrounding environment and understand the scene more comprehensively Feng
    et al. ([2020](#bib.bib29)), Mo et al. ([2022](#bib.bib99)). The identification
    of specific regions within an image can aid the self-driving vehicles in making
    informed decisions, e.g., determining where the driving road region is.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割：语义分割涉及为图像中的每个像素分配语义标签，如“道路”、“车辆”或“行人”、“交通标志”等。这项技术使智能车辆能够感知周围环境并更全面地理解场景 Feng
    et al. ([2020](#bib.bib29))，Mo et al. ([2022](#bib.bib99))。识别图像中特定区域可以帮助自动驾驶车辆做出明智的决策，例如确定行驶道路区域的位置。
- en: 'Instance Segmentation: Instance segmentation outputs the boundaries (pixel-level
    masks) of each object and assigns a unique label to each discovered object Zhou
    et al. ([2020a](#bib.bib182)), which seems like a integration of object detection
    and semantic segmentation. It is particularly useful for identifying the shape,
    location, and number of surrounding objects in autonomous driving Rashed et al.
    ([2021](#bib.bib113)), Ko et al. ([2021](#bib.bib62)).'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割：实例分割输出每个物体的边界（像素级掩码），并为每个发现的物体分配一个唯一标签 Zhou et al. ([2020a](#bib.bib182))，这似乎是目标检测和语义分割的结合。它特别有助于识别自动驾驶中周围物体的形状、位置和数量 Rashed
    et al. ([2021](#bib.bib113))，Ko et al. ([2021](#bib.bib62))。
- en: Semantic segmentation captures the overall scene structure, while instance segmentation
    enables a more fine-grained understanding of objects and their boundaries. Different
    from semantic segmentation which only classifies images into meaningful semantic
    regions, instance segmentation provides more precise analysis by supplying a separate
    semantic mask (with identity) for every object instance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割捕捉整体场景结构，而实例分割则实现了对物体及其边界的更细粒度理解。不同于只将图像分类为有意义语义区域的语义分割，实例分割通过为每个物体实例提供单独的语义掩码（带有身份）来提供更精确的分析。
- en: 2.3 Benchmark Dataset
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3 基准数据集
- en: There might be many different sensors installed on the intelligent vehicles,
    such as Camera, LiDAR, Radar, Near-Infrared Sensors, Ultrasonic Sensors, and so
    on. For a clear description, we focus on introducing the two widely-used main
    sensors (Camera and LiDAR) in this paper, related to the image data by Camera
    and the point cloud data by LiDAR on the intelligent vehicles.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 智能车辆上可能安装了许多不同的传感器，如摄像头、激光雷达、雷达、近红外传感器、超声波传感器等。为了清晰描述，本文重点介绍两种广泛使用的主要传感器（摄像头和激光雷达），即智能车辆上的摄像头图像数据和激光雷达点云数据。
- en: 'Camera data: The 3-channel color images in Green, Red, Blue primary colors
    of light (i.e., RGB images) are commonly acquired by monocular or multiple cameras,
    which are simple and reliable sensors that closely resemble human eyes Feng et al.
    ([2020](#bib.bib29)). One of the main benefits of RGB cameras is their high resolution
    and relatively low cost. However, their performance can deteriorate significantly
    under the challenging weather and illumination conditions Feng et al. ([2021](#bib.bib30)).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 相机数据：绿色、红色、蓝色三通道颜色图像（即RGB图像）通常由单目或多目相机获取，这些相机是简单而可靠的传感器，类似于人眼 Feng等 ([2020](#bib.bib29))。RGB相机的主要优点之一是其高分辨率和相对较低的成本。然而，在具有挑战性的天气和光照条件下，它们的性能可能会显著下降 Feng等
    ([2021](#bib.bib30))。
- en: 'LiDAR data: Unlike cameras, laser sensors offer direct and precise 3D information,
    making it easier to extract object candidates and aiding in the classification
    task by providing 3D shape information. LiDAR, also known as light detection and
    ranging, is a sensor technology that is capable of detecting targets in all lighting
    conditions and creating a distance map of the targets with high spatial coverage Li
    and Ibanez-Guzman ([2020](#bib.bib76)), Li et al. ([2020b](#bib.bib77)). LiDAR
    could work in some challenging weather and dark illumination scenarios, but it
    is quite expensive with high cost. Its high cost is a major obstacle to wider
    adoption Li et al. ([2020b](#bib.bib77)) Pham et al. ([2020](#bib.bib109)).'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: LiDAR数据：与相机不同，激光传感器提供直接且精确的3D信息，使得提取物体候选区域更加容易，并通过提供3D形状信息来帮助分类任务。LiDAR，也称为光探测和测距，是一种能够在所有光照条件下探测目标并创建具有高空间覆盖的目标距离图的传感器技术 Li和Ibanez-Guzman
    ([2020](#bib.bib76))，Li等 ([2020b](#bib.bib77))。LiDAR可以在一些具有挑战性的天气和低光照环境下工作，但其成本较高。高成本是其更广泛应用的主要障碍 Li等
    ([2020b](#bib.bib77))，Pham等 ([2020](#bib.bib109))。
- en: 'Benchmark for 2D Object Detection: KITTI Geiger et al. ([2013](#bib.bib35)),
    Cityscapes Cordts et al. ([2016](#bib.bib25)), SIM10k Johnson-Roberson et al.
    ([2017](#bib.bib59)), Foggy Cityscapes Sakaridis et al. ([2018](#bib.bib118)),
    Syn2Real-D Peng et al. ([2018](#bib.bib108)), BDD100k Yu et al. ([2018](#bib.bib168)),
    GTA5 Richter et al. ([2016](#bib.bib114)), nuScenes Caesar et al. ([2020](#bib.bib13)),
    Waymo Open Sun et al. ([2020](#bib.bib132)), A*3D Pham et al. ([2020](#bib.bib109)),
    ApolloScape Huang et al. ([2018](#bib.bib55)), Ford Agarwal et al. ([2020](#bib.bib2)),
    A2D2 Geyer et al. ([2020](#bib.bib36)), ONCE Mao et al. ([2021](#bib.bib95)),
    and Automine Li et al. ([2022c](#bib.bib78)). Let us give two examples: (1) KITTI Geiger
    et al. ([2013](#bib.bib35)) is a widely utilized dataset for autonomous driving,
    encompassing camera images, LiDAR point clouds, and ground-truth 3D bounding boxes.
    It comprises 7,481 training frames and 7,518 test frames, accompanied by sensor
    calibration data and annotated 3D bounding boxes around objects of interest (2)
    SIM10k Johnson-Roberson et al. ([2017](#bib.bib59)) is a synthetic dataset derived
    from the Grand Theft Auto V (GTA-V) computer game. It contains 10,000 images capturing
    driving street scenes, accompanied by bounding box annotations specifically for
    cars.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 2D物体检测基准：KITTI Geiger等 ([2013](#bib.bib35))，Cityscapes Cordts等 ([2016](#bib.bib25))，SIM10k Johnson-Roberson等
    ([2017](#bib.bib59))，Foggy Cityscapes Sakaridis等 ([2018](#bib.bib118))，Syn2Real-D Peng等
    ([2018](#bib.bib108))，BDD100k Yu等 ([2018](#bib.bib168))，GTA5 Richter等 ([2016](#bib.bib114))，nuScenes Caesar等
    ([2020](#bib.bib13))，Waymo Open Sun等 ([2020](#bib.bib132))，A*3D Pham等 ([2020](#bib.bib109))，ApolloScape Huang等
    ([2018](#bib.bib55))，Ford Agarwal等 ([2020](#bib.bib2))，A2D2 Geyer等 ([2020](#bib.bib36))，ONCE Mao等
    ([2021](#bib.bib95))，以及Automine Li等 ([2022c](#bib.bib78))。举两个例子：（1）KITTI Geiger等
    ([2013](#bib.bib35)) 是一个广泛使用的自动驾驶数据集，包括相机图像、LiDAR点云和真实3D边界框。它包含7,481个训练帧和7,518个测试帧，附带传感器校准数据和围绕感兴趣物体的标注3D边界框。（2）SIM10k Johnson-Roberson等
    ([2017](#bib.bib59)) 是从《侠盗猎车手V》（GTA-V）计算机游戏中生成的合成数据集。它包含10,000张捕捉驾驶街景的图像，并附有专门针对汽车的边界框标注。
- en: 'Benchmark for 3D Object Detection: KITTI Geiger et al. ([2013](#bib.bib35)),
    Cityscapes Cordts et al. ([2016](#bib.bib25)), Foggy Cityscapes Sakaridis et al.
    ([2018](#bib.bib118)), GTA5-LiDAR Wu et al. ([2019](#bib.bib153)), nuScenes Caesar
    et al. ([2020](#bib.bib13)), Waymo Open Sun et al. ([2020](#bib.bib132)), A*3D Pham
    et al. ([2020](#bib.bib109)), ApolloScape Huang et al. ([2018](#bib.bib55)), Ford Agarwal
    et al. ([2020](#bib.bib2)), A2D2 Geyer et al. ([2020](#bib.bib36)), ONCE Mao et al.
    ([2021](#bib.bib95)), Automine Li et al. ([2022c](#bib.bib78)), OPV2V Xu et al.
    ([2022b](#bib.bib159)) and V2V4Real Xu et al. ([2023b](#bib.bib161)). Here we
    explain two examples: (1) The Waymo Open Dataset Sun et al. ([2020](#bib.bib132))
    is a Camera+LiDAR dataset, which consists of 1,000 driving sequences containing
    798 scenes allocated for training and 202 scenes allocated for validation. The
    evaluation metrics employed are Average Precision(AP) and Average Precision with
    Heading (APH) information, which consider the weighted average precision and the
    heading accuracy, respectively. The metrics are computed based on the 3D Intersection
    Over Union (IoU) threshold of 0.7 for vehicles and 0.5 for others. (2) V2V4Real
    dataset Xu et al. ([2023b](#bib.bib161)) is designed for the connected vehicles
    based cooperative perception of autonomous driving using V2V (Vehicle-to-Vehicle)
    communication, encompassing a driving area spanning 410 km. It includes a substantial
    collection of real-world data, such as 20,000 LiDAR frames, 40,000 RGB frames,
    240,000 annotated 3D bounding boxes for 5 specific classes, and comprehensive
    High-Definition (HD) Maps containing all driving routes.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 3D目标检测基准：KITTI Geiger 等 ([2013](#bib.bib35))，Cityscapes Cordts 等 ([2016](#bib.bib25))，Foggy
    Cityscapes Sakaridis 等 ([2018](#bib.bib118))，GTA5-LiDAR Wu 等 ([2019](#bib.bib153))，nuScenes Caesar
    等 ([2020](#bib.bib13))，Waymo Open Sun 等 ([2020](#bib.bib132))，A*3D Pham 等 ([2020](#bib.bib109))，ApolloScape Huang
    等 ([2018](#bib.bib55))，Ford Agarwal 等 ([2020](#bib.bib2))，A2D2 Geyer 等 ([2020](#bib.bib36))，ONCE Mao
    等 ([2021](#bib.bib95))，Automine Li 等 ([2022c](#bib.bib78))，OPV2V Xu 等 ([2022b](#bib.bib159))
    和 V2V4Real Xu 等 ([2023b](#bib.bib161))。以下是两个例子：(1) Waymo Open Dataset Sun 等 ([2020](#bib.bib132))
    是一个Camera+LiDAR数据集，包含1,000个驾驶序列，其中798个场景用于训练，202个场景用于验证。使用的评估指标是平均精度(AP)和带有朝向信息的平均精度(APH)，分别考虑加权平均精度和朝向准确性。指标的计算基于车辆的3D交集并集(IoU)阈值为0.7，其他对象为0.5。(2)
    V2V4Real 数据集 Xu 等 ([2023b](#bib.bib161)) 旨在通过V2V（车辆对车辆）通信实现自动驾驶的协作感知，涵盖了410公里的驾驶区域。它包括大量的真实世界数据，如20,000个LiDAR帧，40,000个RGB帧，240,000个特定类别的标注3D边界框，以及包含所有驾驶路线的全面高清(HD)地图。
- en: 'Benchmark for Semantic Segmentation: KITTI Geiger et al. ([2013](#bib.bib35)),
    Cityscapes Cordts et al. ([2016](#bib.bib25)), ApolloScape Huang et al. ([2018](#bib.bib55)),
    BDD100k Yu et al. ([2018](#bib.bib168)), and A2D2 Geyer et al. ([2020](#bib.bib36)).
    Two examples are given here: (1) Cityscapes Cordts et al. ([2016](#bib.bib25))
    is a semantic urban scene dataset under driving scenarios. It includes semantic
    and instance segmentation annotations. The dataset comprises 2,975 training images
    with a resolution of 2048 × 1024, along with an additional set of 500 validation
    images. (2) ApolloScape Huang et al. ([2018](#bib.bib55)) contains a large collection
    of sequentially recorded 140,000 camera images with pixel-level semantic annotations
    in different driving conditions. It comprises 40,960 training images and 8,327
    validation images. In addition to the semantic annotations, the dataset also includes
    pose information relative to static background point clouds.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分割基准：KITTI Geiger 等 ([2013](#bib.bib35))，Cityscapes Cordts 等 ([2016](#bib.bib25))，ApolloScape Huang
    等 ([2018](#bib.bib55))，BDD100k Yu 等 ([2018](#bib.bib168)) 和 A2D2 Geyer 等 ([2020](#bib.bib36))。这里给出两个例子：(1)
    Cityscapes Cordts 等 ([2016](#bib.bib25)) 是一个针对驾驶场景的语义城市场景数据集。它包括语义和实例分割注释。数据集包含2,975张分辨率为2048
    × 1024的训练图像，以及另外500张验证图像。(2) ApolloScape Huang 等 ([2018](#bib.bib55)) 包含了大量按顺序记录的14万张摄像头图像，并在不同驾驶条件下具有像素级语义注释。它包括40,960张训练图像和8,327张验证图像。除了语义注释，数据集还包括相对于静态背景点云的姿态信息。
- en: 'Benchmark for Instance Segmentation: Cityscapes Cordts et al. ([2016](#bib.bib25)),
    nuScenes Caesar et al. ([2020](#bib.bib13)), BDD100k Yu et al. ([2018](#bib.bib168)),
    and KITTI-360 Liao et al. ([2022](#bib.bib83)). Let us give two examples: (1)
    BDD100k Yu et al. ([2018](#bib.bib168)) includes 100,000 images with a resolution
    of 1280 × 720, and it has training, testing and validation sets. Among them, annotated
    70,000 images are for training and annotated 10,000 images are designed for validation.
    This dataset contains six various weather conditions, six distinct scenes, three
    different parts of a day, and ten object categories with bounding box annotations.
    (2) nuScenes Caesar et al. ([2020](#bib.bib13)) is an autonomous driving dataset
    comprising 1,000 driving scenes. In addition to the scene annotations, nuScenes
    also provides High-Definition (HD) semantic maps, offering insights into 11 distinct
    semantic classes. This dataset encompasses 700 scenes for training, 150 scenes
    for validation, and another 150 scenes for testing. The data collection process
    involved the utilization of six cameras and a 32-beam LiDAR system, while the
    annotations cover 10 objects within a complete 360-degree field of view.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 实例分割的基准：Cityscapes Cordts 等人 ([2016](#bib.bib25))、nuScenes Caesar 等人 ([2020](#bib.bib13))、BDD100k
    Yu 等人 ([2018](#bib.bib168)) 和 KITTI-360 Liao 等人 ([2022](#bib.bib83))。让我们举两个例子：（1）BDD100k
    Yu 等人 ([2018](#bib.bib168)) 包含 100,000 张分辨率为 1280 × 720 的图像，其中包含训练集、测试集和验证集。其中特注释的
    70,000 张图像用于训练，10,000 张图像用于验证。该数据集包含六种不同的天气条件、六种不同的场景、一天中的三个不同时间段以及十个带有边界框注释的物体类别。（2）nuScenes
    Caesar 等人 ([2020](#bib.bib13)) 是一个包含 1,000 个驾驶场景的自动驾驶数据集。除了场景注释外，nuScenes 还提供高清
    (HD) 语义地图，提供对 11 种不同语义类别的见解。该数据集包括 700 个训练场景、150 个验证场景和另外 150 个测试场景。数据收集过程中使用了六个摄像头和一个
    32 梁激光雷达系统，注释覆盖了一个完整 360 度视场内的 10 个物体。
- en: 'The Table [1](#S2.T1 "Table 1 ‣ 2.3 Benchmark Dataset ‣ 2 Intelligent Vehicle
    Perception ‣ Deep Transfer Learning for Intelligent Vehicle Perception: a Survey")
    summarizes the current widely-used benchmark dataset details for the intelligent
    vehicle perception tasks, including the image resolution, image numbers, LiDAR
    frame numbers, task types, real or synthetic information of each benchmark dataset.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 [1](#S2.T1 "表 1 ‣ 2.3 基准数据集 ‣ 2 智能车辆感知 ‣ 智能车辆感知的深度迁移学习：综述") 总结了目前广泛使用的智能车辆感知任务基准数据集的详细信息，包括图像分辨率、图像数量、激光雷达帧数量、任务类型、每个基准数据集的真实或合成信息。
- en: 'Table 1: Benchmark Datasets for Intelligent Vehicle Perception. D: object detection
    in 2D or 3D, S: semantic or instance segmentation, Syn: synthetic data, R: real
    data.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1：智能车辆感知的基准数据集。D：2D 或 3D 目标检测，S：语义或实例分割，Syn：合成数据，R：真实数据。
- en: 'Benchmark Image Resolution Image # LiDAR Frame # Tasks Real/Syn KITTI Geiger
    et al. ([2013](#bib.bib35)) 1,392×512 15K 1.3M D, S R Cityscapes Cordts et al.
    ([2016](#bib.bib25)) 2,048×1,024 25K - D, S R SIM10k Johnson-Roberson et al. ([2017](#bib.bib59))
    1,914×1,052 10K - D Syn Foggy Cityscapes Sakaridis et al. ([2018](#bib.bib118))
    2,048×1,024 3,475 - D, S Syn Syn2Real-D Peng et al. ([2018](#bib.bib108)) - 248K
    - D Syn, R BDD100K Yu et al. ([2018](#bib.bib168)) 1,280×720 8K - D, S R GTA Richter
    et al. ([2016](#bib.bib114)) 1,914×1,052 24,966 - S Syn GTA-LiDAR Wu et al. ([2019](#bib.bib153))
    64×512 100K - S Syn H3D Patil et al. ([2019](#bib.bib107)) 1,920×1,200 27,721
    - D R nuScenes Caesar et al. ([2020](#bib.bib13)) 1,600×900 40K - D R Waymo Open Sun
    et al. ([2020](#bib.bib132)) 1,920×1,280 200K - D R ApolloCar3D Song et al. ([2019b](#bib.bib130))
    3,384×2,710 5,277 - S R A*3D Pham et al. ([2020](#bib.bib109)) 2,048×1,536 39K
    39,179 D R ApolloScape Huang et al. ([2018](#bib.bib55)) 3,384×2,710 143,906 -
    S R SYNTHIA Ros et al. ([2016](#bib.bib116)) 960×720 13.4K - S Syn Lyft Level
    5 Houston et al. ([2021](#bib.bib52)) - 55K - S R Ford Agarwal et al. ([2020](#bib.bib2))
    - 200K - D R A2D2 Geyer et al. ([2020](#bib.bib36)) 1,928×1,208 12K D, S R ONCE Mao
    et al. ([2021](#bib.bib95)) 1,920×1,020 1M - D R AutoMine Li et al. ([2022c](#bib.bib78))
    2,048×1,536 18K - D R OPV2V Xu et al. ([2022b](#bib.bib159)) 800×600 44K 11K D
    Syn V2V4Real Xu et al. ([2023b](#bib.bib161)) 2,064×1,544 40K 20K D R ![Refer
    to caption](img/285c969079e44cf0b1a34c43f78c4186.png)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基准图像分辨率 图像数量 LiDAR 帧 数量 任务 真实/合成 KITTI Geiger et al. ([2013](#bib.bib35)) 1,392×512
    15K 1.3M D, S R Cityscapes Cordts et al. ([2016](#bib.bib25)) 2,048×1,024 25K
    - D, S R SIM10k Johnson-Roberson et al. ([2017](#bib.bib59)) 1,914×1,052 10K -
    D 合成 Foggy Cityscapes Sakaridis et al. ([2018](#bib.bib118)) 2,048×1,024 3,475
    - D, S 合成 Syn2Real-D Peng et al. ([2018](#bib.bib108)) - 248K - D 合成, R BDD100K
    Yu et al. ([2018](#bib.bib168)) 1,280×720 8K - D, S R GTA Richter et al. ([2016](#bib.bib114))
    1,914×1,052 24,966 - S 合成 GTA-LiDAR Wu et al. ([2019](#bib.bib153)) 64×512 100K
    - S 合成 H3D Patil et al. ([2019](#bib.bib107)) 1,920×1,200 27,721 - D R nuScenes
    Caesar et al. ([2020](#bib.bib13)) 1,600×900 40K - D R Waymo Open Sun et al. ([2020](#bib.bib132))
    1,920×1,280 200K - D R ApolloCar3D Song et al. ([2019b](#bib.bib130)) 3,384×2,710
    5,277 - S R A*3D Pham et al. ([2020](#bib.bib109)) 2,048×1,536 39K 39,179 D R
    ApolloScape Huang et al. ([2018](#bib.bib55)) 3,384×2,710 143,906 - S R SYNTHIA
    Ros et al. ([2016](#bib.bib116)) 960×720 13.4K - S 合成 Lyft Level 5 Houston et
    al. ([2021](#bib.bib52)) - 55K - S R Ford Agarwal et al. ([2020](#bib.bib2)) -
    200K - D R A2D2 Geyer et al. ([2020](#bib.bib36)) 1,928×1,208 12K D, S R ONCE
    Mao et al. ([2021](#bib.bib95)) 1,920×1,020 1M - D R AutoMine Li et al. ([2022c](#bib.bib78))
    2,048×1,536 18K - D R OPV2V Xu et al. ([2022b](#bib.bib159)) 800×600 44K 11K D
    合成 V2V4Real Xu et al. ([2023b](#bib.bib161)) 2,064×1,544 40K 20K D R ![参见图注](img/285c969079e44cf0b1a34c43f78c4186.png)
- en: 'Figure 2: Distribution Pie Charts of the Benchmark Datasets for Intelligent
    Vehicle Perception: (a) sensor type, (b) task, (c) data source. Note: D: object
    detection in 2D or 3D, S: semantic or instance segmentation, Syn: synthetic data,
    R: real data.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2: 智能车辆感知基准数据集的分布饼图： (a) 传感器类型，(b) 任务，(c) 数据来源。 注：D：2D或3D物体检测，S：语义或实例分割，合成：合成数据，R：真实数据。'
- en: 3 Domain Distribution Discrepancy
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 域分布差异
- en: 'Despite the remarkable achievements of the intelligent vehicle perception algorithms
    on benchmark datasets, there are still significant challenges in the real world
    due to the large variations in the sensor types and settings, data in diverse
    style, environment, weather and illumination, trained epoch, and architecture Li
    et al. ([2022b](#bib.bib71)), Feng et al. ([2021](#bib.bib30)), Schutera et al.
    ([2020](#bib.bib123)), Song et al. ([2023](#bib.bib131)). Based on these observations,
    we divide the domain distribution discrepancy for intelligent vehicle perception
    into three types: sensor difference, data difference, and model difference, as
    shown in Table [2](#S3.T2 "Table 2 ‣ 3.3 Model Difference ‣ 3 Domain Distribution
    Discrepancy ‣ Deep Transfer Learning for Intelligent Vehicle Perception: a Survey").'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管智能车辆感知算法在基准数据集上取得了显著成就，但由于传感器类型和设置、数据风格、环境、天气和光照的巨大差异、训练周期和架构 Li et al. ([2022b](#bib.bib71))，Feng
    et al. ([2021](#bib.bib30))，Schutera et al. ([2020](#bib.bib123))，Song et al.
    ([2023](#bib.bib131))，现实世界中仍面临重大挑战。 基于这些观察，我们将智能车辆感知的域分布差异分为三种类型：传感器差异、数据差异和模型差异，如表
    [2](#S3.T2 "表 2 ‣ 3.3 模型差异 ‣ 3 域分布差异 ‣ 智能车辆感知的深度迁移学习：综述") 所示。
- en: 3.1 Sensor Difference
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1 传感器差异
- en: First of all, the domain gap shows up when the sensors are different in types
    and settings as described in Chen et al. ([2023](#bib.bib19)), Wang et al. ([2023a](#bib.bib145)),
    Li et al. ([2022d](#bib.bib80)). Let us explain the sensor difference for camera
    and LiDAR separately. The camera sensor is cheap but not robust to different types
    and settings, for example, angle difference from horizontal to oblique Rist et al.
    ([2019](#bib.bib115)), placement dissimilarity from front view to rear view Alonso
    et al. ([2020](#bib.bib3)), image resolution diversity Carranza-García et al.
    ([2020](#bib.bib15)), and so on. The LiDAR sensors might also have different types
    and settings, for example, different laser beam numbers Yi et al. ([2021](#bib.bib167)),
    various LiDAR equipment from different companies Xu et al. ([2023a](#bib.bib160)),
    LiDAR placement dissimilarity Hu et al. ([2022a](#bib.bib53)), and so on. The
    problem is quite similar to the setting of other sensor types. Taking Radar as
    an example, variations in Radar resolutions, field-of-view, and noise characteristics
    can result in diverse data distributions. These real-world challenges due to the
    sensor difference may generate the heterogeneous feature distribution between
    different domains Triess et al. ([2021](#bib.bib134)), Zhou et al. ([2022b](#bib.bib184)),
    Chakeri et al. ([2021](#bib.bib17)).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如Chen et al. ([2023](#bib.bib19))、Wang et al. ([2023a](#bib.bib145))、Li et
    al. ([2022d](#bib.bib80))所述，当传感器在类型和设置上存在差异时，领域间的差距会显现。让我们分别解释相机和激光雷达的传感器差异。相机传感器便宜，但对于不同类型和设置的适应性较差，例如，水平到倾斜角度的差异 Rist
    et al. ([2019](#bib.bib115))、从前视到后视的放置差异 Alonso et al. ([2020](#bib.bib3))、图像分辨率的多样性 Carranza-García
    et al. ([2020](#bib.bib15))，等等。激光雷达传感器也可能具有不同的类型和设置，例如，激光束数量的差异 Yi et al. ([2021](#bib.bib167))、来自不同公司的各种激光雷达设备 Xu
    et al. ([2023a](#bib.bib160))、激光雷达的放置差异 Hu et al. ([2022a](#bib.bib53))，等等。这一问题与其他传感器类型的设置类似。以雷达为例，雷达分辨率、视场和噪声特性等的变化可能导致数据分布的多样化。这些由于传感器差异而产生的现实挑战可能导致不同领域间的特征分布异质性 Triess
    et al. ([2021](#bib.bib134))、Zhou et al. ([2022b](#bib.bib184))、Chakeri et al.
    ([2021](#bib.bib17))。
- en: 3.2 Data Difference
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2 数据差异
- en: 'In addition, the domain gap exists when the data itself is different in style
    and format as explained in Gao et al. ([2021](#bib.bib32)), Triess et al. ([2021](#bib.bib134)).
    The data collected by the sensors in different situations might result in the
    heterogeneous data distribution between different domains. See the following four
    common examples for intelligent vehicles. (1) Diverse Weather: foggy, rainy, snowy,
    sunny, etc Miglani and Kumar ([2019](#bib.bib97)), Xu et al. ([2021](#bib.bib157)),
    Mirza et al. ([2022](#bib.bib98)), Bogdoll et al. ([2022](#bib.bib11)), Li et al.
    ([2023c](#bib.bib75)). (2) Various Illumination: daytime, nighttime, tunnel, etc Wu
    et al. ([2021](#bib.bib154)). (3) Occlusion: objects or parts of objects are obscured
    or hidden from the sensors’ view due to obstacles, other vehicles, or environmental
    conditions Qian et al. ([2022](#bib.bib111)), Ruan et al. ([2023](#bib.bib117)).
    (4) Different Data Source: differences between synthetic data (computer game data:
    SYNTHIA Ros et al. ([2016](#bib.bib116)), GTA5 Richter et al. ([2016](#bib.bib114)))
    and real-world data Li et al. ([2023a](#bib.bib73)), and the changes between the
    data collected in different urban or highway environments Shenaj et al. ([2023](#bib.bib126)).'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当数据本身在风格和格式上存在差异时，领域间的差距也会存在，如Gao et al. ([2021](#bib.bib32))、Triess et al.
    ([2021](#bib.bib134))所解释的。不同情况传感器收集的数据可能导致不同领域间的数据分布异质性。以下是智能车辆的四个常见例子。 (1) 多样的天气：雾天、雨天、雪天、晴天等 Miglani
    and Kumar ([2019](#bib.bib97))、Xu et al. ([2021](#bib.bib157))、Mirza et al. ([2022](#bib.bib98))、Bogdoll
    et al. ([2022](#bib.bib11))、Li et al. ([2023c](#bib.bib75))。 (2) 不同的光照：白天、夜晚、隧道等 Wu
    et al. ([2021](#bib.bib154))。 (3) 遮挡：由于障碍物、其他车辆或环境条件，物体或物体部分被传感器视野遮挡或隐藏 Qian et
    al. ([2022](#bib.bib111))、Ruan et al. ([2023](#bib.bib117))。 (4) 不同的数据来源：合成数据（计算机游戏数据：SYNTHIA Ros
    et al. ([2016](#bib.bib116))、GTA5 Richter et al. ([2016](#bib.bib114)))和真实世界数据 Li
    et al. ([2023a](#bib.bib73))之间的差异，以及在不同城市或高速公路环境中收集的数据的变化 Shenaj et al. ([2023](#bib.bib126))。
- en: 3.3 Model Difference
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3 模型差异
- en: Finally, the model difference is not common but it is also one possible reason
    for the domain gap in intelligent vehicle perception. When the perception architecture
    is diverse, the model is obviously different Xu et al. ([2023a](#bib.bib160)),
    for example, from PointPillar architecture Lang et al. ([2019](#bib.bib66)) to
    SECOND architecture Yan et al. ([2018](#bib.bib162)). When the perception architecture
    is the same, the domain gap may still exist because of heterogeneous configurations,
    like training epochs, resolution, number of convolution layers, and hyperparameters Xu
    et al. ([2023a](#bib.bib160)). Because of these diverse situations, the features
    extracted from them might have a domain shift, as described in Xu et al. ([2023a](#bib.bib160)),
    leading to the heterogeneous feature distribution between different domains.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，模型差异虽然不常见，但它也是智能车辆感知中领域差距的一个可能原因。当感知体系结构多样时，模型明显不同 Xu et al. ([2023a](#bib.bib160))，例如，从
    PointPillar 体系结构 Lang et al. ([2019](#bib.bib66)) 到 SECOND 体系结构 Yan et al. ([2018](#bib.bib162))。当感知体系结构相同时，领域差距仍可能存在，因为配置异质，如训练周期、分辨率、卷积层数量和超参数 Xu
    et al. ([2023a](#bib.bib160))。由于这些不同的情况，从中提取的特征可能会出现领域迁移，正如 Xu et al. ([2023a](#bib.bib160))
    中所述，导致不同领域之间特征分布的异质性。
- en: 'Table 2: Domain Distribution Discrepancy with Three Types of Differences for
    Intelligent Vehicle Perception: sensor, data and model. “$\rightarrow$” means
    the model training with the left data and testing on the right data.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2：智能车辆感知的三种差异类型的领域分布差异：传感器、数据和模型。“$\rightarrow$” 表示模型在左侧数据上训练并在右侧数据上测试。
- en: Types Differences Examples Sensor Difference Setup 64-beam LiDAR $\rightarrow$
    32-beam LiDAR Yi et al. ([2021](#bib.bib167)) Placement Front $\rightarrow$ Rear Feng
    et al. ([2020](#bib.bib29)) Angle Horizontal $\rightarrow$ Oblique Rist et al.
    ([2019](#bib.bib115)) Data Difference Data Source GTA5 $\rightarrow$ Cityscapes Murez
    et al. ([2018](#bib.bib101)) Occlusion OPV2V $\rightarrow$ V2V4Real Xu et al.
    ([2023b](#bib.bib161)) Weather Cityscapes $\rightarrow$ Foggy Cityscapes Li et al.
    ([2023c](#bib.bib75)) Illumination Cityscapes $\rightarrow$ Dark Zurich Wu et al.
    ([2021](#bib.bib154)) Model Difference Configuration Hyperparameter 1 $\rightarrow$
    Hyperparameter 2  Xu et al. ([2023a](#bib.bib160)) Architecture PointPillars $\rightarrow$
    SECOND Xu et al. ([2023a](#bib.bib160))
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 差异 示例 传感器差异 设置 64-beam LiDAR $\rightarrow$ 32-beam LiDAR  Yi et al. ([2021](#bib.bib167))
    放置 前 $\rightarrow$ 后 Feng et al. ([2020](#bib.bib29)) 角度 水平 $\rightarrow$ 倾斜 Rist
    et al. ([2019](#bib.bib115)) 数据差异 数据来源 GTA5 $\rightarrow$ Cityscapes Murez et
    al. ([2018](#bib.bib101)) 遮挡 OPV2V $\rightarrow$ V2V4Real Xu et al. ([2023b](#bib.bib161))
    天气 Cityscapes $\rightarrow$ Foggy Cityscapes Li et al. ([2023c](#bib.bib75)) 照明
    Cityscapes $\rightarrow$ Dark Zurich Wu et al. ([2021](#bib.bib154)) 模型差异 配置 超参数
    1 $\rightarrow$ 超参数 2  Xu et al. ([2023a](#bib.bib160)) 体系结构 PointPillars $\rightarrow$
    SECOND Xu et al. ([2023a](#bib.bib160))
- en: 4 Deep Transfer Learning Methodology
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 深度迁移学习方法论
- en: With the rapid advancement of autonomous driving techniques, there is now an
    abundance of driving scene images available. Deep learning methods are booming
    in the application of autonomous driving with high performance of perception.
    This paper is focused on the transfer learning methods for the intelligent vehicle
    perception in the deep learning era.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 随着自动驾驶技术的快速发展，现在有大量的驾驶场景图像可用。深度学习方法在自动驾驶应用中表现出色，感知性能高。本论文重点介绍深度学习时代智能车辆感知的迁移学习方法。
- en: 'Transfer Learning (TL) is a machine learning method to largely apply the knowledge
    acquired from one task or domain to another related task or domain Zhuang et al.
    ([2020](#bib.bib187)). This paper classifies the deep transfer learning into several
    main types: Supervised TL, Unsupervised TL, Weakly-and-semi Supervised TL, Domain
    Generalization. The chronological overview of the transfer learning research development
    in the deep learning era is shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4 Deep Transfer
    Learning Methodology ‣ Deep Transfer Learning for Intelligent Vehicle Perception:
    a Survey").'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习（TL）是一种机器学习方法，旨在将从一个任务或领域获得的知识大规模应用到另一个相关任务或领域 Zhuang et al. ([2020](#bib.bib187))。本文将深度迁移学习分为几种主要类型：有监督迁移学习、无监督迁移学习、弱监督和半监督迁移学习、领域泛化。深度学习时代迁移学习研究的发展时间顺序概述见图 [3](#S4.F3
    "图 3 ‣ 4 深度迁移学习方法论 ‣ 深度迁移学习在智能车辆感知中的应用：一项综述")。
- en: '![Refer to caption](img/6bdc5e2afa6b9686047878899c11c1bc.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6bdc5e2afa6b9686047878899c11c1bc.png)'
- en: 'Figure 3: The Chronological Overview of Transfer Learning Research in the Deep
    Learning Era.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：深度学习时代迁移学习研究的时间顺序概述。
- en: 4.1 Supervised TL
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 监督迁移学习
- en: In the transfer learning research, the source domain normally has the manually
    annotated ground truth. If the target domain also has the manually annotated ground
    truth, the machine learning technique that transfers knowledge from a labeled
    source domain to the labeled target domain is named as Supervised TL Drews et al.
    ([2017](#bib.bib27)), Yu et al. ([2018](#bib.bib168)), Zhou et al. ([2019](#bib.bib181)).
    Gathering such manually annotated data requires substantial human involvement,
    which is labor-intensive and time-consuming Carvalho et al. ([2015](#bib.bib16)).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习研究中，源领域通常有手动标注的真实值。如果目标领域也有手动标注的真实值，则将知识从标记的源领域迁移到标记的目标领域的机器学习技术称为监督迁移学习，Drews
    等人 ([2017](#bib.bib27))，Yu 等人 ([2018](#bib.bib168))，Zhou 等人 ([2019](#bib.bib181))
    有相关研究。收集这些手动标注的数据需要大量的人力，劳动密集且耗时，Carvalho 等人 ([2015](#bib.bib16)) 指出。
- en: We divide the Supervise TL methods into Fine-tuning and Knowledge distillation
    via teacher-student network in this paper.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 本文将监督迁移学习方法分为微调和通过教师-学生网络的知识蒸馏。
- en: 4.1.1 Fine-tuning
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 微调
- en: 'Fine-tuning is a common technique in the transfer learning Guo et al. ([2019b](#bib.bib45)),
    Li and Zhang ([2021](#bib.bib68)), Hu et al. ([2022a](#bib.bib53)), which has
    been widely used in intelligent vehicle perception Wang et al. ([2019b](#bib.bib144)),
    Luo et al. ([2021](#bib.bib93)), Liang et al. ([2022](#bib.bib82)), Xu et al.
    ([2019](#bib.bib156)), Doan et al. ([2019](#bib.bib26)). Fine-tuning takes an
    existing neural network model pre-trained on a source domain dataset and further
    trains it on a new target domain dataset. By the fine-tuning, the knowledge learned
    from the source domain can be leveraged to improve the performance on the target
    domain. It is worth mentioning that fine-tuning a pre-trained neural network model
    could obtain better performance than directly training from scratch. Typically,
    the pre-trained neural network model is trained on a large-scale dataset, enabling
    to acquire the knowledge from a wide range. The learning rate of fine-tuning on
    the target domain is relatively small as a fine adjustment for the neural network
    model pre-trained on source domain. The fine-tuning methods could be roughly classified
    into two types: (1) Whole Fine-tuning: it trains all the layers of the whole neural
    network model. (2) Partial Fine-tuning: it allows us to train only the some interested
    layers of the pre-trained neural network while keeping the some layers frozen.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 微调是一种常见的迁移学习技术，Guo 等人 ([2019b](#bib.bib45))，Li 和 Zhang ([2021](#bib.bib68))，Hu
    等人 ([2022a](#bib.bib53)) 提出了该技术，它已被广泛应用于智能车辆感知领域，Wang 等人 ([2019b](#bib.bib144))，Luo
    等人 ([2021](#bib.bib93))，Liang 等人 ([2022](#bib.bib82))，Xu 等人 ([2019](#bib.bib156))，Doan
    等人 ([2019](#bib.bib26)) 也有相关研究。微调利用在源领域数据集上预训练的现有神经网络模型，并在新的目标领域数据集上进一步训练。通过微调，可以利用源领域学习到的知识来提高目标领域的表现。值得一提的是，微调一个预训练的神经网络模型通常能获得比从头开始训练更好的性能。通常，预训练的神经网络模型是在大规模数据集上训练的，能够从广泛的范围中获取知识。目标领域的微调学习率相对较小，作为对在源领域上预训练的神经网络模型的微调。微调方法大致可以分为两类：(1)
    整体微调：训练整个神经网络模型的所有层。(2) 部分微调：只训练预训练神经网络模型中的某些感兴趣的层，同时保持某些层冻结。
- en: 'Whole Fine-tuning: All the layers of the entire neural network model are fine-tuned
    to obtain the spatial–temporal interactions Ye et al. ([2021](#bib.bib165)) among
    autonomous vehicles and the 3D perception in autonomous driving Sautier et al.
    ([2022](#bib.bib120)).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 整体微调：对整个神经网络模型的所有层进行微调，以获得自主车辆之间的时空交互，Ye 等人 ([2021](#bib.bib165))，以及在自主驾驶中的
    3D 感知，Sautier 等人 ([2022](#bib.bib120))。
- en: 'Partial Fine-tuning: Guo et al. ([2018](#bib.bib43)) only fine-tunes the encoder-decoder
    based semantic segmentation model, by fixing a pre-trained sub-network to ensure
    the multi-class boundary constraint.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 部分微调：Guo 等人 ([2018](#bib.bib43)) 仅对基于编码器-解码器的语义分割模型进行微调，通过固定预训练的子网络来确保多类别边界约束。
- en: 'In-depth Discussion: (1) The fine-tuning based transfer learning is a simple
    but effective way to transfer the knowledge gained from the pre-training on source
    domain to enhance the performance on the target domain with less data and computational
    resources than training from scratch. The training iterations are typically fewer
    when we fine-tune a pre-trained model, compared to training a machine learning
    model from scratch. Because the pre-trained model already has the prior knowledge
    of the pre-training datasets (normally large), the fine-tuning process requires
    less labeled data for continually training the model to achieve the outstanding
    performance. When the pre-trained model is trained on similar data in source domain,
    fine-tuning will generate improved performance on the target domain. (2) However,
    as a supervised method, fine-tuning requires the manually annotated ground truth
    on the target domain, which might be not available in some real-world applications.
    In addition, fine-tuning might suffer from model forgetting, where the model may
    miss some knowledge about the pre-trained task in source domain when adapting
    to the new task in target domain. Furthermore, small target-domain dataset might
    result in the model overfitting during the fine-tuning.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 深入讨论：（1）基于微调的迁移学习是一种简单但有效的方法，将从源领域的预训练中获得的知识转移到目标领域，以提高在目标领域上的性能，所需的数据和计算资源少于从头开始训练。当我们微调一个预训练模型时，训练迭代通常比从头开始训练机器学习模型要少。因为预训练模型已经拥有预训练数据集的先验知识（通常很大），所以微调过程需要较少的标记数据来继续训练模型以实现优异的性能。当预训练模型在源领域的类似数据上进行训练时，微调将在目标领域上生成改进的性能。（2）然而，作为一种监督方法，微调需要在目标领域上进行人工标注的真实数据，这在一些实际应用中可能不可用。此外，微调可能会遭遇模型遗忘问题，即模型在适应目标领域的新任务时，可能会丢失一些关于源领域预训练任务的知识。此外，小的目标领域数据集可能导致模型在微调过程中出现过拟合。
- en: 4.1.2 Knowledge Distillation
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2 知识蒸馏
- en: Knowledge distillation Hinton et al. ([2015](#bib.bib47)), Gou et al. ([2021](#bib.bib40)),
    Wang and Yoon ([2021](#bib.bib140)), Chen et al. ([2021a](#bib.bib20)), Xie and
    Du ([2022](#bib.bib155)), Beyer et al. ([2022](#bib.bib8)) is an advanced technique
    in deep learning, which is also referred to as teacher-student learning, where
    a student neural network is trained on target domain to imitate the knowledge
    of a teacher neural network trained on source domain. Knowledge distillation has
    been widely utilized in intelligent vehicle perception Kothandaraman et al. ([2021](#bib.bib63)),
    Gao et al. ([2022](#bib.bib33)), Hou et al. ([2022](#bib.bib51)), Yang et al.
    ([2022](#bib.bib163)), Sautier et al. ([2022](#bib.bib120)).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏 Hinton et al. ([2015](#bib.bib47)), Gou et al. ([2021](#bib.bib40)), Wang
    and Yoon ([2021](#bib.bib140)), Chen et al. ([2021a](#bib.bib20)), Xie and Du
    ([2022](#bib.bib155)), Beyer et al. ([2022](#bib.bib8)) 是深度学习中的一种先进技术，也称为教师-学生学习，其中一个学生神经网络在目标领域上进行训练，以模仿在源领域上训练的教师神经网络的知识。知识蒸馏已广泛应用于智能车辆感知 Kothandaraman
    et al. ([2021](#bib.bib63)), Gao et al. ([2022](#bib.bib33)), Hou et al. ([2022](#bib.bib51)),
    Yang et al. ([2022](#bib.bib163)), Sautier et al. ([2022](#bib.bib120))。
- en: Knowledge distillation could be beneficial to model generalization, model compression,
    model transferability. It improves the model generalization so that the student
    network can generalize better on unseen examples, especially in scenarios with
    limited training data. It allows to compress a large teacher model into a smaller
    student model. It enables the knowledge transferability from the teacher model
    (source domain) to the student model (target domain) even with different deep
    learning architectures. The teacher network is typically trained on a large-scale
    dataset for the next knowledge transferability to the student network, however
    the large-scale dataset might be not available in the source domain of some intelligent
    vehicle perception tasks. Inspired by Lan and Tian ([2022](#bib.bib65)), the knowledge
    distillation methods could be roughly classified into the following two types.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 知识蒸馏有助于模型泛化、模型压缩和模型迁移能力。它改善了模型的泛化能力，使学生网络能够在未见过的样本上表现更好，特别是在训练数据有限的情况下。它允许将一个大型教师模型压缩成一个较小的学生模型。即使在不同的深度学习架构下，它也能够实现从教师模型（源领域）到学生模型（目标领域）的知识迁移。教师网络通常在大规模数据集上进行训练，以便将知识迁移到学生网络，但在某些智能车辆感知任务的源领域中，可能没有大规模数据集。受到 Lan
    and Tian ([2022](#bib.bib65)) 的启发，知识蒸馏方法大致可以分为以下两类。
- en: 'Response Knowledge Distillation: It focuses on the final output layer of the
    teacher model so as to teach a student model to mimic its predictions. The core
    concept is to use a loss function called the distillation loss, which measures
    the difference between the output activations of the student and teacher models.
    By minimizing this loss during training, the student model gradually improves
    its ability to generate predictions that closely resemble those of the teacher
    model. Gao et al. ([2022](#bib.bib33)) proposes the cross-domain correlation distillation
    loss to transfer knowledge from daytime to nighttime domains, thereby improving
    nighttime semantic segmentation performance.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 响应知识蒸馏：它专注于教师模型的最终输出层，以教会学生模型模仿其预测。核心概念是使用一种叫做蒸馏损失的损失函数，来衡量学生模型和教师模型输出激活之间的差异。通过在训练过程中最小化这种损失，学生模型逐渐提高其生成与教师模型预测结果相似的预测的能力。Gao
    等人（[2022](#bib.bib33)）提出了跨域相关蒸馏损失，以将知识从白天域转移到夜晚域，从而提高夜晚语义分割性能。
- en: 'Intermediate Knowledge Distillation: It focuses on aligning the intermediate
    representations of the teacher and student models. The intermediate layers learn
    to recognize and distinguish specific features in the data, and this knowledge
    distilled in teacher network can be leveraged to train the student model effectively.
    Hou et al. ([2022](#bib.bib51)) proposes an approach of transferring distilled
    knowledge from a larger source teacher model to a smaller target student network
    to conduct LiDAR semantic segmentation. Specifically, the intermediate Point-to-Voxel
    Knowledge Distillation approach is utilized to transfer latent knowledge from
    both point level and voxel level to complement sparse supervision signals.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 中间知识蒸馏：它专注于对齐教师模型和学生模型的中间表示。中间层学习识别和区分数据中的特定特征，而这些在教师网络中提取的知识可以用来有效地训练学生模型。Hou
    等人（[2022](#bib.bib51)）提出了一种将从较大源教师模型中提取的知识转移到较小目标学生网络以进行 LiDAR 语义分割的方法。具体而言，使用中间点对体素知识蒸馏方法将点级和体素级的潜在知识转移，以补充稀疏的监督信号。
- en: 'In-depth Discussion: (1) By the knowledge transfer from the teacher model to
    student model, knowledge distillation allows for model compression, improved generalization,
    regularization to prevent overfitting, and ensemble effect mimic. A smaller student
    model can be trained to approximate the performance of a larger computationally
    expensive teacher model, leading to the model compression for the student model.
    By learning from the guidance provided by the teacher model, the student model
    will inherit the teacher’s generalization abilities to unseen examples, ensuring
    better generalization of the student model. The guidance by the teacher model
    is able to act as a form of regularization that prevents overfitting of the student
    model. Knowledge distillation can also be combined with traditional model training
    to mimic the ensemble effect of multiple models. (2) However, the success of knowledge
    distillation relies on a well-trained high-performance teacher model first. Training
    a larger teacher model can be computationally expensive, and the large-scale dataset
    might be not available in the source domain of some intelligent vehicle perception
    tasks. If the teacher model lacks sufficient recognition capacity (not well-optimized),
    then the knowledge transferred to the student model may be not strong enough.
    In some complex cases of intelligent vehicle perception tasks, knowledge distillation
    might struggle to improve the student model’s performance when the data distribution
    difference between source and target domains is quite large.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 深入讨论：（1）通过将知识从教师模型转移到学生模型，知识蒸馏可以实现模型压缩、提高泛化能力、防止过拟合的正则化效果以及模仿集成效果。一个较小的学生模型可以被训练来近似较大、计算开销较大的教师模型的性能，从而实现对学生模型的模型压缩。通过从教师模型提供的指导中学习，学生模型将继承教师模型对未见示例的泛化能力，确保学生模型的更好泛化。教师模型的指导能够作为一种正则化形式，防止学生模型的过拟合。知识蒸馏还可以与传统模型训练相结合，以模仿多个模型的集成效果。（2）然而，知识蒸馏的成功首先依赖于经过良好训练的高性能教师模型。训练一个较大的教师模型可能计算开销很大，并且在某些智能车辆感知任务的源领域中，可能没有大规模的数据集。如果教师模型缺乏足够的识别能力（未经过良好优化），那么转移到学生模型的知识可能不够强。对于某些复杂的智能车辆感知任务，当源领域和目标领域之间的数据分布差异很大时，知识蒸馏可能难以提高学生模型的性能。
- en: 4.2 Unsupervised TL
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 无监督迁移学习
- en: In the intelligent vehicle perception, data labeling is a time-consuming and
    labor-intensive process in real-world scenarios. Generally, supervised algorithms
    struggle when there is a scarcity of labeled data in the source domains Niu et al.
    ([2020](#bib.bib103)), Pan and Yang ([2010](#bib.bib105)). To overcome these challenges,
    Unsupervised Transfer Learning (TL) has emerged as a promising approach for addressing
    such specific cases in the intelligent vehicle perception tasks. Unsupervised
    TL refers to a scenario where there is unlabeled target data besides labeled data
    available in source domain. Unsupervised TL approaches offer promising solutions
    to overcome the limitations of limited labeled data availability, enabling more
    efficient and effective perception in intelligent vehicles.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能车辆感知中，数据标注是现实世界场景下耗时且劳动密集的过程。一般而言，当源领域中标记数据稀缺时，监督算法会遇到困难 Niu et al. ([2020](#bib.bib103)),
    Pan and Yang ([2010](#bib.bib105))。为了解决这些挑战，无监督迁移学习（TL）作为一种有前途的方法出现，以应对智能车辆感知任务中的具体情况。无监督
    TL 指的是除了源领域中有标记数据外，还有未标记的目标数据的情境。无监督 TL 方法为克服标记数据有限性的限制提供了有前途的解决方案，从而实现更高效和有效的智能车辆感知。
- en: 4.2.1 Image-to-image Transfer
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1 图像到图像迁移
- en: Image-to-image transfer, also known as image-to-image translation, is a computer
    vision task that involves converting an input image to a different domain. It
    aims to establish a learned correspondence between two visual domains, where the
    input image originates from the source domain, while the desired output image
    that resembles the target domain. The goal is to generate a corresponding image
    with similar style of the target domain and simultaneously preserve the relevant
    characteristics and semantic contents of the input image. It has found extensive
    application in the field of autonomous driving as well as intelligent transportation
    systems, including semantic segmentation Murez et al. ([2018](#bib.bib101)), Pizzati
    et al. ([2020](#bib.bib110)), lane recognition Hou et al. ([2019](#bib.bib50)),
    Liu et al. ([2021a](#bib.bib85)), data augmentation Zhang et al. ([2022](#bib.bib176)),
    Yang et al. ([2020](#bib.bib164)) Mușat et al. ([2021](#bib.bib102)) and object
    detection Schutera et al. ([2020](#bib.bib123)), Li et al. ([2021](#bib.bib72),
    [2022b](#bib.bib71)), Shan et al. ([2019](#bib.bib124)).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到图像迁移，也称为图像到图像翻译，是一种计算机视觉任务，涉及将输入图像转换到不同的领域。它旨在建立两个视觉领域之间的学习对应关系，其中输入图像来自源领域，而所需的输出图像类似于目标领域。目标是生成一个与目标领域风格相似的对应图像，同时保持输入图像的相关特征和语义内容。它在自动驾驶和智能交通系统领域有广泛的应用，包括语义分割
    Murez et al. ([2018](#bib.bib101)), Pizzati et al. ([2020](#bib.bib110)), 车道识别
    Hou et al. ([2019](#bib.bib50)), Liu et al. ([2021a](#bib.bib85)), 数据增强 Zhang
    et al. ([2022](#bib.bib176)), Yang et al. ([2020](#bib.bib164)) Mușat et al. ([2021](#bib.bib102))
    和目标检测 Schutera et al. ([2020](#bib.bib123)), Li et al. ([2021](#bib.bib72), [2022b](#bib.bib71)),
    Shan et al. ([2019](#bib.bib124))。
- en: 'Image-to-image transfer neural networks are commonly implemented using two
    different approaches: (1) Paired Image-to-Image Transfer and (2) Unpaired Image-to-Image
    Transfer. The first approach utilizes generative adversarial networks trained
    on paired images Wang et al. ([2018](#bib.bib143)). This type of network learns
    a mapping that transforms an input image from its original domain to desired output
    domain Isola et al. ([2017](#bib.bib58)). The second approach addresses scenarios
    where unpaired images are used to establish a more general framework Zhu et al.
    ([2017](#bib.bib186)), Park et al. ([2020](#bib.bib106)), inspiring the unsupervised
    image-to-image translation methods Liu et al. ([2017](#bib.bib86)), Baek et al.
    ([2021](#bib.bib6)).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图像到图像的迁移神经网络通常有两种不同的实现方式：（1）配对图像到图像迁移和（2）非配对图像到图像迁移。第一种方法利用了在配对图像上训练的生成对抗网络
    Wang et al. ([2018](#bib.bib143))。这种类型的网络学习一种映射，将输入图像从其原始领域转换到所需的输出领域 Isola et
    al. ([2017](#bib.bib58))。第二种方法处理那些使用非配对图像来建立更一般框架的情境 Zhu et al. ([2017](#bib.bib186)),
    Park et al. ([2020](#bib.bib106))，这激发了无监督的图像到图像翻译方法 Liu et al. ([2017](#bib.bib86)),
    Baek et al. ([2021](#bib.bib6))。
- en: 'Paired Image-to-Image Transfer: Isola et al. ([2017](#bib.bib58)) investigated
    the utilization of conditional Generative Adversarial Networks (GAN) namely pix2pix
    for paired image-to-image translation tasks Hao et al. ([2019](#bib.bib46)). The
    GAN with condition learns a generative model of data but with the added condition
    of an input image to produce a corresponding output image. This approach strives
    to produce plausible images in target domain. The adversarial loss is utilized
    to train a Generator Network which is updated using $l_{1}$ loss, which quantifies
    the disparity between the generated image as well as predicted output. By incorporating
    additional loss, the Generator Network can produce plausible translations of the
    source images. Conversely, the Discriminator Network is designed to perform generated
    image classification. With the paired training data, these methods could translate
    the image of similar styles in different domains.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 配对图像到图像的转换：Isola 等人 ([2017](#bib.bib58)) 研究了条件生成对抗网络（GAN）的应用，即 pix2pix，用于配对图像到图像的转换任务
    Hao 等人 ([2019](#bib.bib46))。带条件的 GAN 学习数据的生成模型，但附加了输入图像的条件以生成相应的输出图像。这种方法力求在目标领域生成逼真的图像。对抗损失用于训练生成器网络，该网络通过
    $l_{1}$ 损失进行更新，$l_{1}$ 损失量化生成图像与预测输出之间的差异。通过结合额外的损失，生成器网络能够生成源图像的逼真翻译。相反，判别器网络则被设计用于执行生成图像分类。通过配对训练数据，这些方法可以在不同领域中翻译相似风格的图像。
- en: 'Unpaired Image-to-Image Transfer: Cycle-consistency GAN (CycleGAN) Zhu et al.
    ([2017](#bib.bib186)) is a type of GAN model that enables image translation between
    unpaired datasets Mușat et al. ([2021](#bib.bib102)), Uricar et al. ([2021](#bib.bib135)),
    Shan et al. ([2019](#bib.bib124)), Liu et al. ([2022a](#bib.bib87)). The training
    process of a CycleGAN involves optimizing two generators and two discriminators
    simultaneously. One generator is responsible for learning the mapping function
    $G$ from domain $X$ to $Y$, while the other generator $F$ learns the mapping from
    domain $Y$ to $X$. Both $G$ and $F$ are trained simultaneously, incorporating
    a cycle consistency loss that enforces the cycle consistency to ensure that $F(G(x))\approx
    x$ and $G(F(y))\approx y$. This loss combined with adversarial losses on domains
    $X$ and $Y$ yields objective for unpaired image-to-image translation. Unpaired
    Image-to-Image Transfer release the requirement of paired training data, which
    is more general in the real-world applications of intelligent vehicle perception.
    By incorporating adversarial losses on domains $X$ and $Y$, the objective for
    unpaired image-to-image translation is obtained. Unpaired Image-to-Image Transfer
    release the need for paired training data, making them more general in real-life
    applications of intelligent vehicle perception.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 无配对图像到图像的转换：Cycle-consistency GAN（CycleGAN）Zhu 等人 ([2017](#bib.bib186)) 是一种允许在无配对数据集之间进行图像翻译的
    GAN 模型 Mușat 等人 ([2021](#bib.bib102))、Uricar 等人 ([2021](#bib.bib135))、Shan 等人
    ([2019](#bib.bib124))、Liu 等人 ([2022a](#bib.bib87))。CycleGAN 的训练过程涉及同时优化两个生成器和两个判别器。一个生成器负责学习从领域
    $X$ 到 $Y$ 的映射函数 $G$，而另一个生成器 $F$ 学习从领域 $Y$ 到 $X$ 的映射。$G$ 和 $F$ 是同时训练的，通过一个循环一致性损失来强制循环一致性，确保
    $F(G(x))\approx x$ 和 $G(F(y))\approx y$。这个损失结合了领域 $X$ 和 $Y$ 上的对抗损失，得出了无配对图像到图像转换的目标。无配对图像到图像的转换释放了对配对训练数据的要求，这在智能车辆感知的实际应用中更为通用。通过结合领域
    $X$ 和 $Y$ 上的对抗损失，得到了无配对图像到图像转换的目标。无配对图像到图像的转换消除了对配对训练数据的需求，使其在智能车辆感知的现实应用中更加普遍。
- en: 'In-depth Discussion: (1) Image-to-image transfer can be used as data augmentation,
    data distribution style transfer. Because of the limited data of source or target
    domain, image-to-image transfer methods could generate diverse fake examples as
    data augmentation to improve the robustness and generalization during the machine
    learning model training. In addition, image-to-image transfer enables the translation
    between source and target domains, to reduce the data distribution style difference
    in intelligent vehicle perception. (2) However, there are some limitations for
    the image-to-image transfer methods when applying to the real-world intelligent
    vehicle perception. When the transfer involves complex or ambiguous patterns in
    autonomous driving, image-to-image transfer models might produce translations
    with low realism or fidelity. For the Paired Image-to-Image Transfer methods,
    in practical applications of intelligent vehicle perception, the requirement for
    paired training data poses disadvantages. For the Unpaired Image-to-Image Transfer
    methods, they rely on task-specific and predefined similarity functions between
    inputs and outputs and do not consider the reliability and robustness of the translation
    frameworks, which might be disrupted by the perturbations added to input and targeted
    images. This issue is particularly crucial for autonomous driving.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 深入讨论：（1）图像到图像的转换可以用作数据增强、数据分布风格转换。由于源域或目标域的数据有限，图像到图像的转换方法可以生成多样化的假例作为数据增强，从而提高机器学习模型训练过程中的鲁棒性和泛化能力。此外，图像到图像的转换还能够在源域和目标域之间进行转换，以减少智能车辆感知中的数据分布风格差异。（2）然而，将图像到图像的转换方法应用于实际智能车辆感知时存在一些局限性。当转换涉及自动驾驶中的复杂或模糊模式时，图像到图像的转换模型可能会产生低现实性或低逼真的转换。对于配对图像到图像的转换方法，在智能车辆感知的实际应用中，对配对训练数据的需求带来了缺点。对于非配对图像到图像的转换方法，它们依赖于任务特定的和预定义的输入与输出之间的相似性函数，并且没有考虑转换框架的可靠性和鲁棒性，这可能会受到添加到输入和目标图像中的扰动的干扰。这一问题对于自动驾驶尤其重要。
- en: 4.2.2 Adversarial Learning
  id: totrans-91
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2 对抗学习
- en: 'Adversarial learning refers to a machine learning technique that involves training
    two neural networks in a competitive manner, which is initially introduced in
    the context of Generative Adversarial Networks (GAN) by Goodfellow et al. ([2020](#bib.bib39))
    and also mentioned in Gradient Reversal Layer (GRL) framework Ganin and Lempitsky
    ([2015](#bib.bib31)), and provides a promising approach for generating target-similar
    samples at the pixel-level or target-similar representations at the feature-level
    by training robust deep neural networks. It has become popular for addressing
    transfer learning challenges by minimizing the domain discrepancy using adversarial
    objectives, such as fooling a domain discriminator/classifier. During training,
    the feature extractor and the domain discriminator are engaged in an adversarial
    game. The feature extractor tries to produce representations that confuse the
    domain discriminator, making it difficult for the discriminator to differentiate
    between the domains. Meanwhile, the objective of the domain discriminator is to
    correctly classify the samples into their respective domains. This adversarial
    process encourages the learning of domain-invariant features by the feature extractor,
    thereby minimizing the differences between domains. By minimizing the domain disparities
    through adversarial learning, the model learns representations that capture the
    underlying domain-invariant information shared across domains. This approach helps
    to address TL challenges by effectively reducing the disparities between two different
    domains, improving the model’s generalization capabilities across different domains.
    The adversarial learning based transfer learning methods for intelligent vehicle
    perception consists of two types: GRL based Methods and GAN based Methods.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 敌对学习是一种机器学习技术，涉及以竞争方式训练两个神经网络，在生成对抗网络（GAN）上首次由Goodfellow等人（[2020](#bib.bib39)）引入，并在梯度反转层（GRL）框架Ganin和Lempitsky（[2015](#bib.bib31)）中提到。它提供了一种有希望的方法，通过训练稳健的深度神经网络，在像素级别生成目标相似样本或在特征级别生成目标相似表示。通过最小化对抗目标，例如欺骗域鉴别器/分类器，它已经成为解决迁移学习挑战的热门方法之一。在训练过程中，特征提取器和域鉴别器参与一种对抗性博弈。特征提取器试图生成使域鉴别器混淆的表示，使得鉴别器很难区分不同的域。同时，域鉴别器的目标是将样本正确分类到它们各自的域中。这种对抗过程通过特征提取器学习域不变特征，从而最小化域之间的差异。通过对抗学习来最小化域的差异，模型学习到了捕捉跨域共享的底层域不变信息的表示。这种方法有助于通过有效降低两个不同域之间的差异来解决迁移学习挑战，从而提高模型在不同域上的泛化能力。基于对抗学习的智能车辆感知的迁移学习方法包括两种类型：基于GRl的方法和基于GAN的方法。
- en: 'GRL based Methods: Domain adaptation in different vehicle perception domains
    can be achieved through the addition of a Gradient Reversal Layer (GRL) to the
    deep learning architecture Xu et al. ([2023a](#bib.bib160)), Li et al. ([2023c](#bib.bib75)).
    The mechanism of domain adversarial embedding involves using a discriminator with
    a GRL to differentiate between samples from two domains. The discriminator is
    a binary classifier, while the GRL can reverse the training gradient in the back
    propagation of feature extraction. Both the discriminator and the GRL work together
    to align the feature distributions across different domains. It is worth mentioning
    that the GRL only comes into effect during the backpropagation phase and does
    not affect the forward propagation process Ganin and Lempitsky ([2015](#bib.bib31)).
    Let us give a detailed example for better understanding. Li et al. ([2023c](#bib.bib75))
    introduces a new framework for domain adaptive object detection in autonomous
    driving during challenging foggy weather. The approach addresses the domain gap
    between clear and foggy weather in vehicle driving by incorporating image-level
    and object-level adaptation techniques, which aim to minimize differences in object
    appearance and image style. Additionally, a novel Adversarial Gradient Reversal
    Layer (AdvGRL) has been proposed to enable adversarial mining for difficult examples
    along with domain adaptation.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GRL的方法：在不同的车辆感知领域中，可以通过在深度学习架构中添加一个梯度反转层（GRL）来实现领域适应（Xu et al. ([2023a](#bib.bib160))，Li
    et al. ([2023c](#bib.bib75))）。领域对抗嵌入的机制涉及使用带有GRL的鉴别器来区分来自两个领域的样本。鉴别器是一个二分类器，而GRL可以在特征提取的反向传播中反转训练梯度。鉴别器和GRL共同作用以对齐不同领域的特征分布。值得一提的是，GRL仅在反向传播阶段生效，不影响前向传播过程（Ganin
    and Lempitsky ([2015](#bib.bib31))）。为了更好地理解，我们给出一个详细的示例。Li et al. ([2023c](#bib.bib75))
    介绍了一个用于在挑战性的雾霾天气中进行领域自适应物体检测的新框架。该方法通过结合图像级和对象级的适应技术，解决了车辆驾驶中的晴天和雾霾天气之间的领域差距，旨在最小化对象外观和图像风格的差异。此外，提出了一种新型的对抗梯度反转层（AdvGRL），以便在进行领域适应的同时进行对抗性挖掘，以处理困难的样本。
- en: 'GAN based Methods: GAN Goodfellow et al. ([2020](#bib.bib39)), Song et al.
    ([2020](#bib.bib129)) is a popular deep learning framework that can be used to
    teach a model to capture the distribution patterns present within the training
    data, enabling the generation of new data from that same distribution. A GAN consists
    of two separate models, namely the generator $G$ and the discriminator $D$. The
    generator $G$’s job is to create “fake” images that resemble the training images
    so as to confuse the discriminator $D$. The applications of GAN in autonomous
    driving have been recently explored owing to its remarkable progress in generating
    realistic images. Specifically, GAN has been leveraged to generate image or subspace
    feature undistinguished by domain classifier based discriminator, for example,
    GAN could generate aligned/similar features between clear weather and foggy weather Li
    et al. ([2023c](#bib.bib75), [2022a](#bib.bib70)), between synthetic game data
    and real-world data Biasetton et al. ([2019](#bib.bib9)), Zhang et al. ([2021b](#bib.bib175)),
    between daytime data and nighttime data Wang et al. ([2022a](#bib.bib138)), Li
    et al. ([2022a](#bib.bib70)). Let us give a detailed example for better understanding.
    Hoffman et al. ([2018](#bib.bib49)) proposes a domain adaptation model which combines
    generative image space alignment, latent feature space alignment, and the vehicle
    perception task. By considering the vehicle perception task (semantic segmentation
    of urban driving scenes), the image-level features, latent features, and the task-related
    semantic features are aligned across different domains by an adversarial learning
    via a GAN-based framework.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于GAN的方法：GAN（Goodfellow et al. ([2020](#bib.bib39))，Song et al. ([2020](#bib.bib129))）是一种流行的深度学习框架，可以用于训练模型捕捉训练数据中存在的分布模式，从而生成来自相同分布的新数据。GAN包括两个独立的模型，即生成器$G$和判别器$D$。生成器$G$的任务是创建“伪造”图像，使其类似于训练图像，以混淆判别器$D$。由于生成逼真图像的显著进展，GAN在自动驾驶中的应用最近得到了探索。具体来说，GAN被用于生成未被领域分类器基于判别器区分的图像或子空间特征，例如，GAN可以生成晴天与雾天之间对齐/相似的特征 Li
    et al. ([2023c](#bib.bib75)，[2022a](#bib.bib70))，合成游戏数据与现实世界数据之间的特征 Biasetton
    et al. ([2019](#bib.bib9))，Zhang et al. ([2021b](#bib.bib175))，白天数据与夜晚数据之间的特征 Wang
    et al. ([2022a](#bib.bib138))，Li et al. ([2022a](#bib.bib70))。我们给出一个详细的例子以便更好地理解。Hoffman
    et al. ([2018](#bib.bib49))提出了一种领域适应模型，该模型结合了生成图像空间对齐、潜在特征空间对齐和车辆感知任务。通过考虑车辆感知任务（城市驾驶场景的语义分割），图像级特征、潜在特征和与任务相关的语义特征通过基于GAN的框架在不同领域间对齐。
- en: 'In-depth Discussion: (1) The GRL based Methods rely on minimizing the domain
    distribution discrepancy through a gradient reversal in the back propagation of
    feature extraction to confuse the domain discriminator. In contrast, the GAN based
    Methods focus on training the Generator Network and Discriminator Network alternately
    using a Min-Max adversarial loss function, with the goal of acquiring domain-invariant
    features. In these two ways, the feature distribution between source and target
    domains can be aligned to reduce domain gap for transfer learning. (2) However,
    training the adversarial learning model is sometimes difficult. For the GRL based
    Methods, the simple gradient reversal might be not powerful enough to well minimize
    the domain distribution discrepancy in complex driving scenarios, leading to the
    local optimal solution. For the GAN based Methods, finding the optimized balance
    between the generator $G$ and discriminator $D$ can be difficult and the convergence
    may not always be guaranteed when training the GAN model, due to its hyperparameter
    sensitivity, data quality, and data diversity.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 深入讨论：（1）基于GRL的方法依赖于通过在特征提取的反向传播中进行梯度反转来最小化领域分布差异，以混淆领域判别器。相比之下，基于GAN的方法则专注于交替训练生成器网络和判别器网络，使用最小-最大对抗损失函数，以获得领域不变特征。通过这两种方式，可以对齐源领域和目标领域之间的特征分布，以减少迁移学习中的领域差距。（2）然而，训练对抗学习模型有时是困难的。对于基于GRL的方法，简单的梯度反转可能不足以在复杂的驾驶场景中有效地最小化领域分布差异，从而导致局部最优解。对于基于GAN的方法，由于其超参数敏感性、数据质量和数据多样性，找到生成器$G$和判别器$D$之间的优化平衡可能很困难，并且在训练GAN模型时，收敛性可能无法始终得到保证。
- en: 4.2.3 Feature Alignment
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3 特征对齐
- en: To minimize the domain distribution discrepancy, the objective of feature alignment
    in transfer learning is to discover an aligned feature representation from multiple
    domains. Typically, the feature distribution difference between different domains
    can be defined as loss functions during the deep neural network training, so minimizing
    the loss functions of the feature distribution difference across multiple domains
    will reduce the domain gap.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化领域分布的差异，迁移学习中的特征对齐目标是从多个领域中发现一个对齐的特征表示。通常，领域之间的特征分布差异可以在深度神经网络训练过程中定义为损失函数，因此最小化跨多个领域的特征分布差异的损失函数将缩小领域间的差距。
- en: 'Feature alignment-based transfer learning can be classified into two main categories:
    Subspace Feature Alignment, and Attention-guided Feature Alignment.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征对齐的迁移学习可以分为两大类：子空间特征对齐和基于注意力的特征对齐。
- en: 'Subspace Feature Alignment: By projecting the features from different domains
    to a lower-dimensional subspace, several metrics to describe the distance of feature
    distribution across source and target domains can be defined as the loss functions
    in the deep learning framework. Minimizing these metric distances (loss functions)
    will align the features of different domains in the subspace. The widely used
    metric to describe the feature distribution distances are Principal Component
    Analysis (PCA) projected subspace feature distance Song et al. ([2019a](#bib.bib128)),
    Maximum Mean Discrepancy (MMD) Borgwardt et al. ([2006](#bib.bib12)), Kullback–Leibler
    Divergence  Zhang et al. ([2018](#bib.bib177)), Gram Matrix Guo et al. ([2019a](#bib.bib44)),
    Multi-Kernel MMD Gretton et al. ([2012](#bib.bib41)), Long et al. ([2015](#bib.bib91)),
    Joint MMD Long et al. ([2017](#bib.bib92)), Wasserstein distance Arjovsky et al.
    ([2017](#bib.bib4)), etc. For example, let us take a close look at the definition
    of the MMD metric, which is formulated as'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 子空间特征对齐：通过将来自不同领域的特征投影到低维子空间中，可以在深度学习框架中将描述源领域和目标领域之间特征分布距离的几个度量定义为损失函数。最小化这些度量距离（损失函数）将使不同领域的特征在子空间中对齐。常用的描述特征分布距离的度量包括主成分分析（PCA）投影子空间特征距离
    Song et al. ([2019a](#bib.bib128))、最大均值差异（MMD） Borgwardt et al. ([2006](#bib.bib12))、Kullback–Leibler
    散度 Zhang et al. ([2018](#bib.bib177))、Gram 矩阵 Guo et al. ([2019a](#bib.bib44))、多核
    MMD Gretton et al. ([2012](#bib.bib41))、Long et al. ([2015](#bib.bib91))、联合 MMD
    Long et al. ([2017](#bib.bib92))、Wasserstein 距离 Arjovsky et al. ([2017](#bib.bib4))
    等。例如，我们可以详细看看 MMD 度量的定义，其公式为
- en: '|  | $MMD(\mathcal{X}_{s},\mathcal{X}_{t})=\lVert\frac{1}{n_{s}}\sum_{i=1}^{n_{s}}k(\mathbf{x}_{i}^{s})-\frac{1}{n_{t}}\sum_{j=1}^{n_{t}}k(\mathbf{x}_{j}^{t})\rVert_{H},$
    |  | (1) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | $MMD(\mathcal{X}_{s},\mathcal{X}_{t})=\lVert\frac{1}{n_{s}}\sum_{i=1}^{n_{s}}k(\mathbf{x}_{i}^{s})-\frac{1}{n_{t}}\sum_{j=1}^{n_{t}}k(\mathbf{x}_{j}^{t})\rVert_{H},$
    |  | (1) |'
- en: where $\mathcal{X}_{s}$ and $\mathcal{X}_{t}$ denote the sets of samples obtained
    from the source and target domains, $\mathbf{x}_{i}^{s}$ and $\mathbf{x}_{j}^{t}$
    are individual samples from the respective domains, and $n_{s}$ and $n_{t}$ denote
    the sample sizes of the source and target domains respectively, $k$ denotes the
    kernel functions, and $H$ indicates the Reproducing Kernel Hilbert Space (RKHS).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\mathcal{X}_{s}$ 和 $\mathcal{X}_{t}$ 表示从源领域和目标领域获得的样本集合，$\mathbf{x}_{i}^{s}$
    和 $\mathbf{x}_{j}^{t}$ 是来自各自领域的个体样本，$n_{s}$ 和 $n_{t}$ 分别表示源领域和目标领域的样本大小，$k$ 表示核函数，$H$
    表示再生核希尔伯特空间（RKHS）。
- en: 'Attention-guided Feature Alignment: Taking inspiration from the attention mechanism Zhou
    et al. ([2016](#bib.bib180)), Vaswani et al. ([2017](#bib.bib137)), the most informative
    components of specific importance can be focused for the intelligent vehicle perception.
    The deep learning frameworks can first extract the attention maps, then the distance
    of attention maps between two domains can be defined as loss function to be minimized
    during the neural network training Zhou et al. ([2020b](#bib.bib185)), Zagoruyko
    and Komodakis ([2016](#bib.bib172)). By employing this approach, it becomes possible
    to align the feature distribution across both the source and target domains via
    the attention map consistency constraint. For example, in Cho et al. ([2023](#bib.bib22)),
    the relation-aware knowledge captured by multiple detection heads can be transferred
    using a specially designed attention head loss for the improved LiDAR-based 3D
    object detection in the context of autonomous driving.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 注意引导特征对齐：受注意力机制的启发 Zhou et al. ([2016](#bib.bib180))，Vaswani et al. ([2017](#bib.bib137))，可以将特定重要性的最具信息成分集中用于智能车辆感知。深度学习框架首先可以提取注意力图，然后定义两个领域之间注意力图的距离作为损失函数，在神经网络训练过程中最小化 Zhou
    et al. ([2020b](#bib.bib185))，Zagoruyko 和 Komodakis ([2016](#bib.bib172))。通过采用这种方法，可以通过注意力图一致性约束对源领域和目标领域的特征分布进行对齐。例如，在 Cho
    et al. ([2023](#bib.bib22))中，多个检测头捕捉的关系感知知识可以通过专门设计的注意力头损失进行转移，以改进自动驾驶背景下基于 LiDAR
    的 3D 目标检测。
- en: 'In-depth Discussion: (1) The Subspace Feature Alignment methods focus on aligning
    the feature distribution in the lower-dimensional subspace representation by using
    different metrics of distribution distances. The Attention-guided Feature Alignment
    methods use the attention mechanism to extract the attention maps first and then
    enforce the attention maps from multiple domains to be the same. In these two
    ways, feature alignment facilitates the model to adapt its knowledge learned from
    the source domain to the target domain, so the model can generalize better to
    the target domain. (2) However, several important settings in the feature alignment
    methods are still open questions. Let us give some examples. How to discover the
    most representative feature subspaces or attention maps does not have a widely-accepted
    common sense in the diverse deep neural network architectures. The feature distribution
    distance metrics (like MMD) are good but may still ignore some important domain
    information. How to balance the loss weights of the feature alignment and the
    original task related loss for intelligent vehicle perception is still challenging
    during the model training.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 深入讨论： (1) 子空间特征对齐方法专注于通过使用不同的分布距离度量来对齐低维子空间表示中的特征分布。注意引导特征对齐方法首先使用注意力机制提取注意力图，然后强制来自多个领域的注意力图保持一致。这两种方法通过特征对齐有助于模型将从源领域学习到的知识适应到目标领域，从而使模型能够更好地推广到目标领域。
    (2) 然而，特征对齐方法中的几个重要设置仍然是开放问题。举几个例子。如何发现最具代表性的特征子空间或注意力图，在多样的深度神经网络架构中没有广泛接受的常识。特征分布距离度量（如
    MMD）很好，但可能仍然忽略一些重要的领域信息。在智能车辆感知的模型训练过程中，如何平衡特征对齐损失和原始任务相关损失的权重仍然具有挑战性。
- en: 4.2.4 Self-learning
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4 自我学习
- en: Autonomous vehicles continuously collect unlabeled data during their operation,
    creating an opportunity for self-learning Liu et al. ([2021c](#bib.bib90)), Zhang
    et al. ([2021a](#bib.bib173)), Kumar et al. ([2021](#bib.bib64)), Luo et al. ([2021](#bib.bib93)),
    Ziegler and Asano ([2022](#bib.bib188)), which offers a promising approach to
    reduce the reliance on labeled data and enhance model flexibility. Given the absence
    of labeled data in target domain using Unsupervised TL, the self-learning methods
    use the additional cues to evaluate the neural network prediction in an unsupervised
    setting, so some prediction results with high confidence are used as the pseudo-labels
    in the further training or testing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 自动驾驶车辆在操作过程中持续收集未标记数据，为自我学习创造了机会 Liu et al. ([2021c](#bib.bib90))，Zhang et al.
    ([2021a](#bib.bib173))，Kumar et al. ([2021](#bib.bib64))，Luo et al. ([2021](#bib.bib93))，Ziegler
    和 Asano ([2022](#bib.bib188))，这为减少对标记数据的依赖和增强模型灵活性提供了有前途的方法。由于目标领域缺乏标记数据，使用无监督迁移学习，自我学习方法利用额外线索在无监督环境中评估神经网络预测，因此一些高置信度的预测结果被用作进一步训练或测试中的伪标签。
- en: The following shows some representative examples of self-learning methods for
    the Unsupervised TL based intelligent vehicle perception. The entropy based uncertainty
    can be used to define the hardness of a specific training sample so as to implement
    an easy-to-hard curriculum learning for semantic segmentation Pan et al. ([2020](#bib.bib104)).
    Wang et al. ([2021a](#bib.bib142)) utilizes self-supervised learning to enhance
    the semantic segmentation performance by using depth estimation as guidance to
    overcome the domain gap between the source and target domains. They explicitly
    capture the correlation between task features and use target depth estimation
    to enhance target semantic predictions. The adaptation difficulty, as inferred
    from depth information, is subsequently utilized to enhance the quality of pseudo-labels
    for target semantic segmentation. Shin et al. ([2022](#bib.bib127)) proposes a
    multi-modal extension of test-time adaptation in the context of 3D semantic segmentation.
    To improve the unstable performance of models at test time, they design both intra-modal
    and inter-modal modules together to acquire more dependable self-learning signals
    of pseudo-labels. Zhang et al. ([2021a](#bib.bib173)) utilizes the multiple classifiers
    with attention heads to evaluate the uncertainty associated with the pseudo-labels.
    The panoramic pseudo-labels with high confidences are then used to improve the
    panoramic semantic segmentation prediction in an iterative fashion.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了一些代表性的自学习方法，用于基于无监督迁移学习的智能车辆感知。基于熵的不确定性可以用来定义特定训练样本的难度，从而实现一种由易到难的课程学习方法用于语义分割 Pan
    等人（[2020](#bib.bib104)）。Wang 等人（[2021a](#bib.bib142)）利用自监督学习，通过使用深度估计作为指导，增强语义分割性能，以克服源领域和目标领域之间的领域差距。他们明确捕捉任务特征之间的相关性，并使用目标深度估计来增强目标语义预测。随后，从深度信息中推断出的适应难度被用来提高目标语义分割的伪标签质量。Shin
    等人（[2022](#bib.bib127)）提出了一种在3D语义分割背景下测试时间适应的多模态扩展。为了改善模型在测试时的不稳定性能，他们设计了内模态和跨模态模块，以获得更可靠的伪标签自学习信号。Zhang
    等人（[2021a](#bib.bib173)）利用多个带有注意力头的分类器来评估与伪标签相关的不确定性。然后使用高置信度的全景伪标签以迭代的方式来改进全景语义分割预测。
- en: 'In-depth Discussion: (1) Self-learning bridges the gap between supervised and
    unsupervised learning, which combines the advantages of the both methods by using
    labeled data for training and unlabeled data for further refinement. By leveraging
    self-learning in autonomous driving, the need for extensive manual annotation
    of data is reduced, enabling more cost-effective and efficient training of models.
    The iterative process of utilizing high-confidence identified samples and generating
    pseudo-labels facilitates promising methods using unlabeled data in target domain.
    (2) However, the robustness and convergence of the self-learning methods is still
    an open question for the reliable intelligent vehicle perception. If the initial
    model is not sufficiently confident and makes mistake predictions on unlabeled
    data, such errors might be propagated through iterations, leading to poor performance
    or hard convergence.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 深入讨论：（1）自学习弥补了监督学习和无监督学习之间的差距，通过使用带标签的数据进行训练以及使用未标记的数据进行进一步的优化，结合了两种方法的优点。通过在自动驾驶中利用自学习，减少了大量手动标注数据的需求，使模型训练更加成本有效和高效。利用高置信度识别样本和生成伪标签的迭代过程促进了在目标领域使用未标记数据的有前景的方法。（2）然而，自学习方法的鲁棒性和收敛性仍然是可靠智能车辆感知中的一个悬而未决的问题。如果初始模型不够自信，并且对未标记数据做出错误预测，这些错误可能会通过迭代传播，导致性能下降或困难的收敛。
- en: 4.3 Weakly-and-semi Supervised TL
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3 弱监督和半监督迁移学习
- en: 'Although impressive results have been achieved by unsupervised TL methods,
    the domain gap cannot be completely eliminated due to the lack of supervision
    on the target domain. There is still a relative performance gap compared with
    supervised TL methods. Another way in addressing the domain gap is by using the
    weakly-and-semi supervised learning method that utilizes both weakly labeled and
    some labeled/unlabeled data in target domain. Based on the available supervision,
    the weakly-and-semi supervised transfer learning methods could be roughly classified
    into two types: Weakly-Supervised TL: There are only weakly supervised labels
    in the target domain. Semi-Supervised TL: There are only semi-supervised labels
    in the target domain, including some labeled data and the remaining unlabeled
    data on target domain.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无监督迁移学习方法取得了令人印象深刻的结果，但由于缺乏目标领域的监督，领域间的差距无法完全消除。与监督迁移学习方法相比，仍然存在相对的性能差距。解决领域差距的另一种方法是使用弱监督和半监督学习方法，这些方法利用了目标领域中的弱标记数据和一些标记/未标记数据。根据可用的监督，弱监督和半监督迁移学习方法大致可以分为两类：弱监督迁移学习（Weakly-Supervised
    TL）：目标领域中只有弱监督标签。半监督迁移学习（Semi-Supervised TL）：目标领域中只有半监督标签，包括一些标记数据和剩余的未标记数据。
- en: 'Weakly-Supervised TL: Theories of weakly supervised learning have been applied
    in autonomous driving Barnes et al. ([2017](#bib.bib7)), Gojcic et al. ([2021](#bib.bib38)),
    such as object detection, semantic segmentation, and instance segmentation. The
    transfer learning techniques can be applied simultaneously with the weakly supervised
    learning. For example, when an instance-level task only has image-level annotations
    in target domain but with instance-level annotations in source domain, the pseudo
    annotations can be predicted Inoue et al. ([2018](#bib.bib57)) for the object
    detection task. Given a source domain (synthetic data) with pixel/object- level
    labels, a target domain (real-world scenes) might only have object-level labels,
    where the pixel-level and object-level domain classifiers can be used in transfer
    learning to learn domain-invariant features for the semantic segmentation task
    in driving scenes Wang et al. ([2019a](#bib.bib141)).'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 弱监督迁移学习（Weakly-Supervised TL）：弱监督学习理论已应用于自动驾驶，如Barnes等（[2017](#bib.bib7)）、Gojcic等（[2021](#bib.bib38)），例如目标检测、语义分割和实例分割。迁移学习技术可以与弱监督学习同时应用。例如，当实例级任务在目标领域只有图像级标注而在源领域有实例级标注时，可以预测伪标注，如Inoue等（[2018](#bib.bib57)）用于目标检测任务。给定一个具有像素/对象级标签的源领域（合成数据），目标领域（现实场景）可能只有对象级标签，此时可以在迁移学习中使用像素级和对象级领域分类器，以学习领域不变特征，用于驾驶场景中的语义分割任务，如Wang等（[2019a](#bib.bib141)）。
- en: 'Semi-Supervised TL: There are three types of training data (labeled source
    data, labeled target data, and unlabeled target data) in the semi-supervised TL
    setting Wang et al. ([2020](#bib.bib147)), Chen et al. ([2021b](#bib.bib21)),
    Wang et al. ([2023b](#bib.bib146)). The key point for improving semi-supervised
    TL is to effectively use available unlabeled data from target domain and limited
    labeled data from different domains. For example, Wang et al. ([2020](#bib.bib147))
    aligns feature distribution across two domains by introducing an extra semantic-level
    adaptation module, which leverages a few labeled images from the target domain
    to supervise the segmentation and feature adaptation tasks. Other works focus
    on generating pseudo labels for unlabeled target data by using labeled source
    data and labeled target data. For example, Wang et al. ([2023b](#bib.bib146))
    solves this problem by two-stage learning that includes inter-domain adaptation
    stage and intra-domain generalization stage. While Chen et al. ([2021b](#bib.bib21))
    uses the domain-mixed teacher models and knowledge distillation to train a good
    student model, then the good student model will generate pseudo labels for the
    next round of teacher model training.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督TL：半监督TL设置中有三种类型的训练数据（标记源数据，标记目标数据和未标记目标数据）（Wang et al. [2020](#bib.bib147)，Chen
    et al. [2021b](#bib.bib21)，Wang et al. [2023b](#bib.bib146)）。改进半监督TL的关键点是有效利用目标领域的可用未标记数据和不同领域的有限标记数据。例如，Wang
    et al. [2020](#bib.bib147)通过引入额外的语义级适应模块，在两个领域之间对特征分布进行对齐，这个模块利用了目标域的少量标记图像来监督分割和特征适应任务。其他作品则侧重于使用标记的源数据和标记的目标数据为未标记的目标数据生成伪标签。例如，Wang
    et al. [2023b](#bib.bib146)通过两阶段学习解决了这个问题，包括域间适应阶段和域内泛化阶段。而Chen et al. [2021b](#bib.bib21)则使用了域混合的教师模型和知识蒸馏来训练一个良好的学生模型，然后良好的学生模型将为下一轮的教师模型训练生成伪标签。
- en: 'In-depth Discussion: (1) By involving some supervisions in the target domain,
    the weakly-and-semi supervised learning methods could achieve a better performance
    than the unsupervised TL methods. Weakly-Supervised TL can be more data-efficient
    as it leverages weakly labeled data which is often easier and cheaper to obtain,
    reducing the need for extensive manual labeling. Semi-Supervised TL can improve
    the model’s generalization to unseen data and reduce overfitting on the limited
    labeled data. Semi-supervised methods augment the labeled dataset with unlabeled
    data, providing the model with additional training examples and increasing data
    diversity. Similar to weakly supervised methods, semi-supervised methods can be
    valuable for domain adaptation tasks, when limited labeled data is available in
    the target domain. (2) While various methods have been proposed for weakly-and-semi
    supervised transfer learning, how to leverage the unlabeled target data with the
    help of available labeled data under different complex situations is still challenging.
    The performance of weakly-and-semi supervised transfer learning methods still
    perform worse than the supervised transfer learning methods, which indicates that
    some local optimal solutions are achieved only. How to make a further improvement
    to overcome the algorithm performance boundary is an open question now. Maybe
    some prior knowledge of human driving can be leveraged to advance the algorithm
    performance.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 深入讨论：（1）通过在目标领域中加入一些监督，弱监督和半监督学习方法可以比无监督TL方法取得更好的性能。弱监督TL可以更加节约数据，因为它利用了通常更容易和更便宜获得的弱标记数据，减少了对大量手动标注的需求。半监督TL可以提高模型对未知数据的泛化能力，并减少对有限标记数据的过拟合。半监督方法通过未标记数据增加了标记数据集，为模型提供了更多的训练示例，增加了数据的多样性。与弱监督方法类似，半监督方法对于域适应任务时有限标记数据在目标域中可用时非常有价值。（2）尽管已经提出了各种方法用于弱监督和半监督迁移学习，但在不同复杂情况下如何利用可用的标记数据来利用未标记的目标数据仍然具有挑战性。弱监督和半监督迁移学习方法的性能仍然低于监督迁移学习方法，这表明只实现了一些局部最优解。如何进一步改进以克服算法性能边界是一个现在的开放问题。也许可以利用人类驾驶的一些先验知识来提升算法的性能。
- en: 4.4 Domain Generalization
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4 领域通用化
- en: Domain Generalization (DG) for intelligent vehicle perception offers a solution
    to the challenge of enhancing the resilience of deep neural networks against arbitrary
    unseen driving scenes Zhou et al. ([2022a](#bib.bib183)). Unlike Domain Adaptation
    (DA), DG methods typically focus on learning a shared representation across multiple
    source domains. This approach aims to enhance the model ability to generalize
    across various domains, enabling it to perform well in an unknown target domain
    of driving. Nevertheless, the collection of multi-domain datasets is a laborious
    and costly endeavor, and the efficacy of DG methods is significantly influenced
    by the quantity of source datasets Wang et al. ([2022b](#bib.bib139)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 用于智能车辆感知的域泛化（DG）提供了增强深度神经网络对任意未见驾驶场景的鲁棒性的解决方案 Zhou 等（[2022a](#bib.bib183)）。与领域适应（DA）不同，DG
    方法通常专注于在多个源领域之间学习共享表示。这种方法旨在提高模型在各种领域间的泛化能力，使其能够在未知的驾驶目标领域中表现良好。然而，收集多领域数据集是一项费时且昂贵的工作，并且
    DG 方法的效果受到源数据集数量的显著影响 Wang 等（[2022b](#bib.bib139)）。
- en: 'The concept of domain generalization (DG) has emerged as a solution to address
    the lack of target data in domain gap Blanchard et al. ([2011](#bib.bib10)) Wang
    et al. ([2022b](#bib.bib139)). The primary distinction between DA and DG lies
    in the fact that DG does not require access to the target domain during the training
    phase. DG aims to develop a model by using data from one or multiple related but
    distinct source domains to generate any out-of-distribution target domain data Shen
    et al. ([2021](#bib.bib125)). The existing methods for DG can be divided into
    two main groups according to the number of source domains: Multi-source DG and
    Single-source DG.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 域泛化（DG）的概念已经成为解决领域间差距中缺乏目标数据问题的一个方案 Blanchard 等（[2011](#bib.bib10)）Wang 等（[2022b](#bib.bib139)）。DA
    和 DG 之间的主要区别在于 DG 在训练阶段不需要访问目标领域。DG 旨在通过使用一个或多个相关但不同的源领域的数据来开发一个模型，以生成任何分布外的目标领域数据
    Shen 等（[2021](#bib.bib125)）。现有的 DG 方法可以根据源领域的数量分为两个主要组：多源 DG 和单源 DG。
- en: 'Multi-source DG: Its primary motivation is to utilize data from multiple sources
    to learn representations that are invariant to different marginal distributions Wilson
    and Cook ([2020](#bib.bib151)) Luo et al. ([2022](#bib.bib94)) Zhao et al. ([2022](#bib.bib179)).
    Due to the absence of target data, it is challenging for a model trained on a
    single source to achieve generalization effectively. By leveraging multiple domains,
    a model can discover stable patterns across the source domains, leading to better
    generalization results on unseen domains. The underlying concept behind this category
    is to minimize the difference between the representations of various source domains,
    thus learn domain-invariant representations Yue et al. ([2019](#bib.bib170)),
    Hu et al. ([2022b](#bib.bib54)), Xu et al. ([2022a](#bib.bib158)), Li et al. ([2022b](#bib.bib71)),
    Choi et al. ([2021](#bib.bib23)), Lin et al. ([2021](#bib.bib84)), Acuna et al.
    ([2021](#bib.bib1)).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 多源 DG：其主要动机是利用来自多个来源的数据来学习对不同边际分布不变的表示 Wilson 和 Cook（[2020](#bib.bib151)）Luo
    等（[2022](#bib.bib94)）Zhao 等（[2022](#bib.bib179)）。由于缺乏目标数据，单一来源训练的模型很难有效地实现泛化。通过利用多个领域，模型可以发现源领域之间的稳定模式，从而在未见领域上取得更好的泛化结果。这一类别的基本概念是最小化不同源领域表示之间的差异，从而学习领域不变的表示
    Yue 等（[2019](#bib.bib170)），Hu 等（[2022b](#bib.bib54)），Xu 等（[2022a](#bib.bib158)），Li
    等（[2022b](#bib.bib71)），Choi 等（[2021](#bib.bib23)），Lin 等（[2021](#bib.bib84)），Acuna
    等（[2021](#bib.bib1)）。
- en: 'Single-source DG: It assumes that the training data is homogeneous, which is
    sampled from a single domain Qiao et al. ([2020](#bib.bib112)), Wang et al. ([2021b](#bib.bib148)).
    Single-source DG methods revolve around data augmentation, and they aim to create
    samples that are out of the domain and utilize them to train the network in conjunction
    with the source samples, enhancing the generalization capability Li et al. ([2023d](#bib.bib79)),
    Lehner et al. ([2022](#bib.bib67)), Hu et al. ([2022b](#bib.bib54)), Khosravian
    et al. ([2021](#bib.bib61)), Chuah et al. ([2022](#bib.bib24)), Sanchez et al.
    ([2022](#bib.bib119)), Zhang et al. ([2020](#bib.bib174)), Wu and Deng ([2022](#bib.bib152)).
    Although single-source DG methods are not robust as multi-source domain method
    due to the limited information from source domain, they do not rely on domain
    identity labels for learning, which makes them applicable to both single-source
    and multi-source scenarios.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 单源领域泛化（DG）：它假设训练数据是同质的，样本来自单一领域 Qiao 等人（[2020](#bib.bib112)），Wang 等人（[2021b](#bib.bib148)）。单源领域泛化方法围绕数据增强展开，旨在创建超出领域的样本，并将这些样本与源样本一起用于训练网络，从而提高泛化能力
    Li 等人（[2023d](#bib.bib79)），Lehner 等人（[2022](#bib.bib67)），Hu 等人（[2022b](#bib.bib54)），Khosravian
    等人（[2021](#bib.bib61)），Chuah 等人（[2022](#bib.bib24)），Sanchez 等人（[2022](#bib.bib119)），Zhang
    等人（[2020](#bib.bib174)），Wu 和 Deng（[2022](#bib.bib152)）。虽然单源领域泛化方法由于源领域信息有限而不如多源领域方法鲁棒，但它们不依赖于领域身份标签进行学习，使其适用于单源和多源场景。
- en: 'In-depth Discussion: (1) By learning domain-invariant features plus data augmentation
    or random feature generalization, domain generalization aims to enhance the model’s
    ability to be generalized to new unseen data. This can be beneficial for intelligent
    vehicle perception because many complex driving scenarios in the real world are
    not seen before. By incorporating Multi-source DG and Single-source DG, they can
    reduce the data bias in the existing training data, so the perception model can
    be more robust to new unseen driving scenarios. It is impossible to collect all
    the driving data in the real world, so domain generalization potentially saves
    time and labor cost with improved model robustness. (2) However, training a perception
    model with the domain generalization capability is more complex than traditional
    domain-specific transfer learning methods, as it requires handling the generalization
    of domain-invariant features for either single source or multiple sources. When
    training on multiple source domains, the model might suffer from the data imbalance
    across different domains, leading to biased learning towards some dominant domains.
    This issue is similar for the class imbalance problem in a single source domain,
    posing challenges to the domain generalization methods.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 深入讨论：（1）通过学习领域不变特征加上数据增强或随机特征泛化，领域泛化旨在提高模型对新未见数据的泛化能力。这对智能车辆感知有益，因为现实世界中的许多复杂驾驶场景之前未曾见过。通过结合多源领域泛化和单源领域泛化，可以减少现有训练数据中的数据偏差，从而使感知模型对新未见的驾驶场景更为鲁棒。由于无法收集现实世界中的所有驾驶数据，领域泛化有可能节省时间和劳动成本，同时提高模型鲁棒性。（2）然而，训练具备领域泛化能力的感知模型比传统的领域特定迁移学习方法更复杂，因为它需要处理单源或多源的领域不变特征的泛化。在多个源领域进行训练时，模型可能会遭遇不同领域之间的数据不平衡问题，导致对一些主导领域的学习偏向。这一问题与单源领域中的类别不平衡问题类似，对领域泛化方法提出了挑战。
- en: 'Table 3: Uniqueness of Deep Transfer Learning (TL) Methods for Intelligent
    Vehicle Perception.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：智能车辆感知中深度迁移学习（TL）方法的独特性。
- en: Types Methodologies Uniqueness Supervised TL Fine-tuning Continuous learning
    from pre-trained model Knowledge distillation Knowledge transfer from teacher
    to student model Unsupervised TL Image-to-image transfer Pixel-level mapping or
    translation Adversarial learning Training two models adversarially Feature alignment
    Aligning features in different domains Self-learning Learning with pseudo-labels
    for refinement Weakly-and-semi Supervised TL Weakly-Supervised TL Learning with
    weakly-labeled data Semi-Supervised TL Learning with partially-labeled data Domain
    Generalization Multi-source DG Generalization to unseen data by multiple source
    domains Single-source DG Generalization to unseen data by single source domain
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 类型 方法论 独特性 监督迁移学习 微调 从预训练模型中持续学习 知识蒸馏 从教师模型到学生模型的知识迁移 无监督迁移学习 图像到图像迁移 像素级映射或翻译
    对抗学习 训练两个对抗的模型 特征对齐 对齐不同领域的特征 自学习 使用伪标签进行学习和优化 弱监督和半监督迁移学习 弱监督迁移学习 使用弱标记数据进行学习
    半监督迁移学习 使用部分标记数据进行学习 领域泛化 多源领域泛化 通过多个源领域对未知数据进行泛化 单源领域泛化 通过单一源领域对未知数据进行泛化
- en: 4.5 Uniqueness of Methodology
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5 方法论的独特性
- en: 'The Table [3](#S4.T3 "Table 3 ‣ 4.4 Domain Generalization ‣ 4 Deep Transfer
    Learning Methodology ‣ Deep Transfer Learning for Intelligent Vehicle Perception:
    a Survey") summarizes the uniqueness of the above classified deep transfer learning
    methods for intelligent vehicle perception, including the methods of Supervise
    TL, Unsupervised TL, Weakly-and-semi Supervised TL, and Domain Generalization.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [3](#S4.T3 "Table 3 ‣ 4.4 Domain Generalization ‣ 4 Deep Transfer Learning
    Methodology ‣ Deep Transfer Learning for Intelligent Vehicle Perception: a Survey")
    总结了上述分类的智能车辆感知深度迁移学习方法的独特性，包括监督迁移学习（Supervise TL）、无监督迁移学习（Unsupervised TL）、弱监督和半监督迁移学习（Weakly-and-semi
    Supervised TL）以及领域泛化（Domain Generalization）的方法。'
- en: 5 Challenges and Future Research
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 挑战与未来研究
- en: This section outlines the main challenges of the deep transfer learning for
    the current intelligent vehicle perception and the related future research directions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本节概述了当前智能车辆感知领域深度迁移学习面临的主要挑战以及相关的未来研究方向。
- en: '1.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1.'
- en: 'Sensor Robustness: The current camera and LiDAR sensors are not robust enough
    in the extreme driving scenarios, like diverse weather, dark illumination, various
    environments. In addition, for the V2V cooperative perception, the V2V communication
    sensors might have the issues of lossy communication Li et al. ([2023b](#bib.bib74))
    due to the fast speed, obstacles, etc Schlager et al. ([2022a](#bib.bib121)).
    Future Research: More future research can be focused on improving the sensor robustness,
    for example, the camera and LiDAR sensors in diverse weather, dark illumination,
    various environments, and the communication sensors in the V2V system Tahir et al.
    ([2021](#bib.bib133)). For example, more advanced sensors with robust lens coatings,
    self-cleaning mechanisms, better LiDAR reflection can be studied to compensate
    for distortions in adverse weather. More advanced V2V communication systems with
    less communication delay can be investigated, anticipating the future growth of
    autonomous connected vehicles.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 传感器鲁棒性：当前的摄像头和激光雷达传感器在极端驾驶场景中，如多变的天气、低光照、各种环境下，鲁棒性不足。此外，对于V2V协作感知，V2V通信传感器可能因高速、障碍物等原因出现丢包通信问题
    Li et al. ([2023b](#bib.bib74))，Schlager et al. ([2022a](#bib.bib121))。未来研究：可以更多地关注提高传感器鲁棒性，例如，在多变天气、低光照、各种环境中的摄像头和激光雷达传感器，以及V2V系统中的通信传感器
    Tahir et al. ([2021](#bib.bib133))。例如，可以研究具有坚固镜头涂层、自清洁机制、更佳激光雷达反射的高级传感器，以补偿恶劣天气下的失真。还可以研究具有更少通信延迟的先进V2V通信系统，以应对未来自动驾驶连接车辆的增长。
- en: '2.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2.'
- en: 'Methodology Limitation: The current unsupervised transfer learning methods
    are worse than the supervised transfer learning methods with a relative performance
    insufficiency. In addition, how to fully utilize the knowledge of the source domain
    and the human prior cognition and experience is still a question to be answered.
    How to effectively use the weakly and partially labeled data is still a open question.
    Future Research: Researchers could make efforts to develop more advanced deep
    transfer learning methods in the future, for example, largely reducing the performance
    disparity between unsupervised and supervised approaches, incorporating the Vehicle-to-Everything
    (V2X) techniques to communicate with connected vehicles and smart infrastructures
    to overcome the occlusion challenges, involving the Large Language Models, like
    ChatGPT Gao et al. ([2023](#bib.bib34)), to better simulate the human cognition
    and knowledge so as to guide the transfer learning methods, accurately self-learning
    the unlabeled data, effectively and efficiently using the weakly and partially
    supervised data.'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 方法论局限性：目前的无监督迁移学习方法相较于有监督迁移学习方法表现较差，存在相对性能不足的问题。此外，如何充分利用源领域的知识以及人类的先验认知和经验仍然是一个待解答的问题。如何有效利用弱标记和部分标记的数据仍然是一个未解之谜。未来研究：研究人员可以努力在未来开发更先进的深度迁移学习方法，例如，大幅减少无监督和有监督方法之间的性能差距，将车对一切（V2X）技术应用于与联网车辆和智能基础设施的通信，以克服遮挡挑战，引入大型语言模型，如ChatGPT
    Gao et al.（[2023](#bib.bib34)），以更好地模拟人类认知和知识，从而指导迁移学习方法，准确自我学习未标记的数据，及有效利用弱标记和部分监督的数据。
- en: '3.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3.'
- en: 'Realism of Synthetic Data: By eliminating the need for manual annotation, the
    synthetic data generated by computer game engines is quite helpful to improve
    the training data size, but it still has significant differences with the real-world
    data in styles, lighting conditions, viewpoints, and vehicle behaviors, etc. Future
    Research: The realism of the synthetic data can be improved by more advanced computer
    game engines in the future. The customized synthetic data can be better simulated
    via a digital twin simulation system Wang et al. ([2023c](#bib.bib149)). For example,
    researchers can engage in the development of dynamic and interactive virtual environments
    within gaming engines. Through the simulation of authentic variations in illuminating,
    weather conditions, and object interactions, synthetic data will be able to emulate
    the complexities of real-world scenarios. This process could create more robust
    and transferable machine learning models. Moreover, constructing digital twin
    models based on real-world data offers the opportunity to generate highly personalized
    and precise synthetic data.'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 合成数据的现实性：通过消除手动标注的需求，由计算机游戏引擎生成的合成数据在提高训练数据量方面非常有帮助，但与真实世界数据在风格、光照条件、视角和车辆行为等方面仍存在显著差异。未来研究：未来可以通过更先进的计算机游戏引擎来提升合成数据的现实性。通过数字双胞胎模拟系统Wang
    et al.（[2023c](#bib.bib149)）可以更好地模拟定制的合成数据。例如，研究人员可以参与开发游戏引擎中的动态和互动虚拟环境。通过模拟真实的光照、天气条件和物体交互，合成数据将能够模仿现实世界场景的复杂性。这一过程可能会创造出更强大且可转移的机器学习模型。此外，基于真实世界数据构建数字双胞胎模型提供了生成高度个性化和精确合成数据的机会。
- en: '4.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4.'
- en: 'Scarcity of Annotated Benchmarks in Complex Scenarios: There are infinite complex
    scenarios in the real-world driving, but the current benchmark datasets in the
    complex driving scenarios are still limited. For example, the Foggy Cityscapes
    dataset Sakaridis et al. ([2018](#bib.bib118)) only has 2,975 training images
    during the foggy weather, whose small size poses a clear hurdle for the accurate
    perception of the intelligent vehicle in the foggy weather. Future Research: We
    expect that more high-quality benchmark datasets in complex driving scenarios
    could be collected and publicized in the future. We also encourage more advanced
    physical models to simulate the benchmark data (Camera, LiDAR) in complex driving
    scenarios in the future, such as simulating the fog, rain, snow, lighting changes,
    etc. For example, future research may focus on collecting large-scale real-world
    benchmark datasets (Camera, LiDAR) that cover a wide range of complex driving
    scenarios with challenging urban or highway environments, dense traffic, pedestrian
    crossings, various traffic patterns and different road geometries.'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 复杂场景中标注基准数据的稀缺：现实世界驾驶中存在无限复杂的场景，但目前复杂驾驶场景中的基准数据集仍然有限。例如，Foggy Cityscapes数据集 Sakaridis等人（[2018](#bib.bib118)）仅在雾天时包含2,975张训练图像，其小规模对智能车辆在雾天中的准确感知构成明显障碍。未来研究：我们期望未来能够收集并公开更多高质量的复杂驾驶场景基准数据集。我们还鼓励在未来使用更先进的物理模型来模拟复杂驾驶场景中的基准数据（相机、LiDAR），例如模拟雾、雨、雪、光照变化等。例如，未来的研究可能会集中于收集覆盖各种复杂驾驶场景的大规模真实世界基准数据集（相机、LiDAR），包括具有挑战性的城市或高速公路环境、密集的交通、行人过路、各种交通模式和不同的道路几何。
- en: '5.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5.'
- en: 'International Standards for Hardware Sensors: The hardware sensors might be
    provided from multiple companies of different countries, but there are no unified
    international standards for the hardware sensors for intelligent vehicle perception.
    For example, the different hardware sensor types and settings will enlarge the
    domain gap in different environments. Future Research: We hope that the multiple
    companies of different countries can collaborate together to promote the international
    standards for hardware sensors in the future, including the types, settings, parameters
    of the hardware sensors in different driving environments Schlager et al. ([2022b](#bib.bib122)),
    Masmoudi et al. ([2021](#bib.bib96)). Compatible hardware sensor standards would
    enable the exchange and sharing of data across different intelligent vehicle platforms,
    allowing for improved decision-making and safety on the roads.'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 硬件传感器的国际标准：硬件传感器可能来自多个不同国家的公司，但智能车辆感知的硬件传感器尚无统一的国际标准。例如，不同的硬件传感器类型和设置将扩大不同环境中的领域差距。未来研究：我们希望未来不同国家的多家公司能够共同合作，推动硬件传感器的国际标准，包括不同驾驶环境中的硬件传感器的类型、设置和参数 Schlager等人（[2022b](#bib.bib122)），Masmoudi等人（[2021](#bib.bib96)）。兼容的硬件传感器标准将使不同智能车辆平台之间的数据交换和共享成为可能，从而提高道路上的决策和安全。
- en: '6.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '6.'
- en: 'International Standards for Software Packages: The software package might be
    provided from multiple companies of different countries as well, but there are
    no unified international standards for the software packes for intelligent vehicle
    perception. For example, sharing the features of models trained in different epochs,
    e.g., from different companies, will result in the performance drop in V2V cooperative
    perception Xu et al. ([2023a](#bib.bib160)). Future Research: The multiple companies
    of different countries are expected to collaborate together to promote the international
    standards for software packages in the future, including the deep learning model
    architectures and frameworks, hyper parameters, privacy and safety preservation,
    etc. For example, the accepted software package standards can reduce the problems
    in the V2V data sharing of the connected intelligent vehicles among different
    companies with guidelines.'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 软件包的国际标准：软件包可能来自多个不同国家的公司，但智能车辆感知的软件包尚未有统一的国际标准。例如，不同公司在不同训练时期的模型特征共享，将导致V2V协同感知的性能下降 Xu等人（[2023a](#bib.bib160)）。未来研究：预计未来不同国家的多家公司将共同合作，推动软件包的国际标准，包括深度学习模型架构和框架、超参数、隐私和安全保护等。例如，接受的软件包标准可以减少不同公司之间的连接智能车辆V2V数据共享中的问题，并提供指导。
- en: 6 Conclusion
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 结论
- en: In this survey paper, we presented a comprehensive review of deep transfer learning
    for intelligent vehicle perception. We reviewed the perception tasks and the related
    benchmark datasets and then divided the domain distribution discrepancy of the
    intelligent vehicle perception in the real world into sensor, data, and model
    differences. Then, we provided clearly classified and summarized definition and
    description of numerous representative deep transfer learning approaches and related
    works in intelligent vehicle perception. Through our intensive analysis and review,
    we have identified several potential challenges and directions for future research.
    Overall, this survey paper aims to make contributions to introduce and explain
    the deep transfer learning techniques for intelligent vehicle perception, offering
    invaluable insights and directions for the future research.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇综述论文中，我们对智能车辆感知的深度迁移学习进行了全面的回顾。我们回顾了感知任务及相关的基准数据集，然后将智能车辆感知在现实世界中的领域分布差异划分为传感器、数据和模型差异。接着，我们对许多代表性的深度迁移学习方法及其在智能车辆感知中的相关工作进行了清晰的分类和总结。通过我们的深入分析和回顾，我们确定了几个潜在的挑战和未来研究的方向。总体而言，这篇综述论文旨在介绍和解释智能车辆感知的深度迁移学习技术，为未来的研究提供宝贵的见解和方向。
- en: 7 CRediT authorship contribution
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 CRediT 作者贡献
- en: 'Xinyu Liu: Conceptualization, Methodology, Original draft preparation. Jinlong
    Li: Methodology, Original draft preparation. Jin Ma: Methodology, Investigation.
    Huiming Sun: Methodology, Investigation. Zhigang Xu: Review & editing. Tianyun
    Zhang: Review & editing. Hongkai Yu: Methodology guidance, Supervision, Review
    & editing.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 'Xinyu Liu: 概念化，方法论，原稿准备。Jinlong Li: 方法论，原稿准备。Jin Ma: 方法论，调查。Huiming Sun: 方法论，调查。Zhigang
    Xu: 审阅与编辑。Tianyun Zhang: 审阅与编辑。Hongkai Yu: 方法论指导，监督，审阅与编辑。'
- en: 8 Declaration of Competing Interest
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 竞争利益声明
- en: The authors declare that they have no known competing financial interests or
    personal relationships that could have appeared to influence the work reported
    in this paper.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 作者声明他们没有已知的竞争财务利益或个人关系，这些可能会影响本文所报告的工作。
- en: 9 Acknowledgement
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9 致谢
- en: This work was supported by NSF 2215388 and CSU FRD grants.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这项工作得到了 NSF 2215388 和 CSU FRD 资助的支持。
- en: References
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Acuna et al. (2021) D. Acuna, J. Philion, and S. Fidler. Towards optimal strategies
    for training self-driving perception models in simulation. *Advances in Neural
    Information Processing Systems*, 34:1686–1699, 2021.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Acuna 等人 (2021) D. Acuna, J. Philion, 和 S. Fidler. 朝着最佳策略训练自驾驶感知模型的模拟。*神经信息处理系统进展*,
    34:1686–1699, 2021。
- en: Agarwal et al. (2020) S. Agarwal, A. Vora, G. Pandey, W. Williams, H. Kourous,
    and J. McBride. Ford multi-av seasonal dataset. *The International Journal of
    Robotics Research*, 39(12):1367–1376, 2020.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Agarwal 等人 (2020) S. Agarwal, A. Vora, G. Pandey, W. Williams, H. Kourous, 和
    J. McBride. 福特多季节数据集。*国际机器人研究杂志*, 39(12):1367–1376, 2020。
- en: Alonso et al. (2020) I. Alonso, L. Riazuelo, L. Montesano, and A. C. Murillo.
    Domain adaptation in lidar semantic segmentation by aligning class distributions.
    *arXiv preprint arXiv:2010.12239*, 2020.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Alonso 等人 (2020) I. Alonso, L. Riazuelo, L. Montesano, 和 A. C. Murillo. 通过对齐类分布进行激光雷达语义分割的领域适应。*arXiv
    预印本 arXiv:2010.12239*, 2020。
- en: Arjovsky et al. (2017) M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein
    generative adversarial networks. In *International Conference on Machine Learning*,
    pages 214–223\. PMLR, 2017.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arjovsky 等人 (2017) M. Arjovsky, S. Chintala, 和 L. Bottou. Wasserstein 生成对抗网络。在
    *国际机器学习大会*，页码 214–223\. PMLR, 2017。
- en: Arnold et al. (2019) E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby,
    and A. Mouzakitis. A survey on 3d object detection methods for autonomous driving
    applications. *IEEE Transactions on Intelligent Transportation Systems*, 20(10):3782–3795,
    2019.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Arnold 等人 (2019) E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby,
    和 A. Mouzakitis. 关于自动驾驶应用的 3D 对象检测方法的调查。*IEEE 智能交通系统汇刊*, 20(10):3782–3795, 2019。
- en: Baek et al. (2021) K. Baek, Y. Choi, Y. Uh, J. Yoo, and H. Shim. Rethinking
    the truly unsupervised image-to-image translation. In *IEEE/CVF International
    Conference on Computer Vision*, pages 14154–14163, 2021.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Baek 等人 (2021) K. Baek, Y. Choi, Y. Uh, J. Yoo, 和 H. Shim. 重新思考真正的无监督图像到图像翻译。在
    *IEEE/CVF 国际计算机视觉会议*，页码 14154–14163, 2021。
- en: 'Barnes et al. (2017) D. Barnes, W. Maddern, and I. Posner. Find your own way:
    Weakly-supervised segmentation of path proposals for urban autonomy. In *IEEE
    International Conference on Robotics and Automation*, pages 203–210\. IEEE, 2017.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Barnes 等（2017）D. Barnes, W. Maddern 和 I. Posner。找到自己的道路：城市自治路径提议的弱监督分割。见 *IEEE
    国际机器人与自动化会议*，第203–210页，2017年。
- en: 'Beyer et al. (2022) L. Beyer, X. Zhai, A. Royer, L. Markeeva, R. Anil, and
    A. Kolesnikov. Knowledge distillation: A good teacher is patient and consistent.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 10925–10934,
    2022.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Beyer 等（2022）L. Beyer, X. Zhai, A. Royer, L. Markeeva, R. Anil 和 A. Kolesnikov。知识蒸馏：一个好的教师是耐心和一致的。见
    *IEEE/CVF 计算机视觉与模式识别会议*，第10925–10934页，2022年。
- en: Biasetton et al. (2019) M. Biasetton, U. Michieli, G. Agresti, and P. Zanuttigh.
    Unsupervised domain adaptation for semantic segmentation of urban scenes. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops*, pages 0–0, 2019.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Biasetton 等（2019）M. Biasetton, U. Michieli, G. Agresti 和 P. Zanuttigh。城市场景的语义分割的无监督领域适应。见
    *IEEE/CVF 计算机视觉与模式识别会议工作坊*，第0–0页，2019年。
- en: Blanchard et al. (2011) G. Blanchard, G. Lee, and C. Scott. Generalizing from
    several related classification tasks to a new unlabeled sample. *Advances in Neural
    Information Processing Systems*, 24, 2011.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Blanchard 等（2011）G. Blanchard, G. Lee 和 C. Scott。从多个相关分类任务推广到新的未标记样本。*神经信息处理系统进展*，24，2011年。
- en: 'Bogdoll et al. (2022) D. Bogdoll, M. Nitsche, and J. M. Zöllner. Anomaly detection
    in autonomous driving: A survey. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pages 4488–4499, 2022.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bogdoll 等（2022）D. Bogdoll, M. Nitsche 和 J. M. Zöllner。在自动驾驶中的异常检测：综述。见 *IEEE/CVF
    计算机视觉与模式识别会议*，第4488–4499页，2022年。
- en: Borgwardt et al. (2006) K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel,
    B. Schölkopf, and A. J. Smola. Integrating structured biological data by kernel
    maximum mean discrepancy. *Bioinformatics*, 22(14):e49–e57, 2006.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Borgwardt 等（2006）K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel, B.
    Schölkopf 和 A. J. Smola。通过核最大均值差异整合结构化生物数据。*生物信息学*，22(14):e49–e57，2006年。
- en: 'Caesar et al. (2020) H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong,
    Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom. nuscenes: A multimodal
    dataset for autonomous driving. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pages 11621–11631, 2020.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caesar 等（2020）H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu,
    A. Krishnan, Y. Pan, G. Baldan 和 O. Beijbom。nuscenes：一个用于自动驾驶的多模态数据集。见 *IEEE/CVF
    计算机视觉与模式识别会议*，第11621–11631页，2020年。
- en: Cao et al. (2019) Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A.
    Chen, K. Fu, and Z. M. Mao. Adversarial sensor attack on lidar-based perception
    in autonomous driving. In *ACM SIGSAC Conference on Computer and Communications
    Security*, pages 2267–2281, 2019.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cao 等（2019）Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A. Chen,
    K. Fu 和 Z. M. Mao。在自动驾驶中的基于 lidar 的感知的对抗性传感器攻击。见 *ACM SIGSAC 计算机与通信安全会议*，第2267–2281页，2019年。
- en: Carranza-García et al. (2020) M. Carranza-García, J. Torres-Mateo, P. Lara-Benítez,
    and J. García-Gutiérrez. On the performance of one-stage and two-stage object
    detectors in autonomous vehicles using camera data. *Remote Sensing*, 13(1):89,
    2020.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carranza-García 等（2020）M. Carranza-García, J. Torres-Mateo, P. Lara-Benítez
    和 J. García-Gutiérrez。在使用摄像头数据的自动驾驶中单阶段和双阶段目标检测器的性能。*遥感*，13(1):89，2020年。
- en: 'Carvalho et al. (2015) A. Carvalho, S. Lefévre, G. Schildbach, J. Kong, and
    F. Borrelli. Automated driving: The role of forecasts and uncertainty—a control
    perspective. *European Journal of Control*, 24:14–32, 2015.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Carvalho 等（2015）A. Carvalho, S. Lefévre, G. Schildbach, J. Kong 和 F. Borrelli。自动驾驶：预测和不确定性的作用——控制视角。*欧洲控制杂志*，24:14–32，2015年。
- en: Chakeri et al. (2021) A. Chakeri, X. Wang, Q. Goss, M. I. Akbas, and L. G. Jaimes.
    A platform-based incentive mechanism for autonomous vehicle crowdsensing. *IEEE
    Open Journal of Intelligent Transportation Systems*, 2:13–23, 2021.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chakeri 等（2021）A. Chakeri, X. Wang, Q. Goss, M. I. Akbas 和 L. G. Jaimes。一个基于平台的自主车辆众测激励机制。*IEEE
    智能交通系统开放期刊*，2:13–23，2021年。
- en: 'Chen et al. (2022) L. Chen, Y. Li, C. Huang, B. Li, Y. Xing, D. Tian, L. Li,
    Z. Hu, X. Na, Z. Li, et al. Milestones in autonomous driving and intelligent vehicles:
    Survey of surveys. *IEEE Transactions on Intelligent Vehicles*, 8(2):1046–1056,
    2022.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2022）L. Chen, Y. Li, C. Huang, B. Li, Y. Xing, D. Tian, L. Li, Z. Hu,
    X. Na, Z. Li 等。自动驾驶和智能车辆的里程碑：综述的综述。*IEEE 智能车辆学报*，8(2):1046–1056，2022年。
- en: 'Chen et al. (2023) L. Chen, Y. Li, C. Huang, Y. Xing, D. Tian, L. Li, Z. Hu,
    S. Teng, C. Lv, J. Wang, et al. Milestones in autonomous driving and intelligent
    vehicles—part 1: Control, computing system design, communication, hd map, testing,
    and human behaviors. *IEEE Transactions on Systems, Man, and Cybernetics: Systems*,
    2023.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2023）L. Chen, Y. Li, C. Huang, Y. Xing, D. Tian, L. Li, Z. Hu, S. Teng,
    C. Lv, J. Wang 等. 自主驾驶和智能车辆的里程碑—第一部分：控制、计算系统设计、通信、高精度地图、测试和人类行为。*IEEE 系统、人类与控制系统汇刊*，2023。
- en: Chen et al. (2021a) P. Chen, S. Liu, H. Zhao, and J. Jia. Distilling knowledge
    via knowledge review. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 5008–5017, 2021a.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021a）P. Chen, S. Liu, H. Zhao, 和 J. Jia. 通过知识回顾提取知识。在 *IEEE/CVF 计算机视觉与模式识别会议*
    上，页码 5008–5017，2021a。
- en: Chen et al. (2021b) S. Chen, X. Jia, J. He, Y. Shi, and J. Liu. Semi-supervised
    domain adaptation based on dual-level domain mixing for semantic segmentation.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11018–11027,
    2021b.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等（2021b）S. Chen, X. Jia, J. He, Y. Shi, 和 J. Liu. 基于双层领域混合的半监督领域适应用于语义分割。在
    *IEEE/CVF 计算机视觉与模式识别会议* 上，页码 11018–11027，2021b。
- en: 'Cho et al. (2023) H. Cho, J. Choi, G. Baek, and W. Hwang. itkd: Interchange
    transfer-based knowledge distillation for 3d object detection. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 13540–13549, 2023.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cho 等（2023）H. Cho, J. Choi, G. Baek, 和 W. Hwang. itkd：基于交换传输的知识蒸馏用于 3D 物体检测。在
    *IEEE/CVF 计算机视觉与模式识别会议* 上，页码 13540–13549，2023。
- en: 'Choi et al. (2021) S. Choi, S. Jung, H. Yun, J. T. Kim, S. Kim, and J. Choo.
    Robustnet: Improving domain generalization in urban-scene segmentation via instance
    selective whitening. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 11580–11590, 2021.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Choi 等（2021）S. Choi, S. Jung, H. Yun, J. T. Kim, S. Kim, 和 J. Choo. Robustnet：通过实例选择性白化提高城市场景分割中的领域泛化能力。在
    *IEEE/CVF 计算机视觉与模式识别会议* 上，页码 11580–11590，2021。
- en: 'Chuah et al. (2022) W. Chuah, R. Tennakoon, R. Hoseinnezhad, A. Bab-Hadiashar,
    and D. Suter. Itsa: An information-theoretic approach to automatic shortcut avoidance
    and domain generalization in stereo matching networks. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 13022–13032, 2022.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chuah 等（2022）W. Chuah, R. Tennakoon, R. Hoseinnezhad, A. Bab-Hadiashar, 和 D.
    Suter. ITSA：一种信息论方法用于自动规避捷径和立体匹配网络中的领域泛化。在 *IEEE/CVF 计算机视觉与模式识别会议* 上，页码 13022–13032，2022。
- en: Cordts et al. (2016) M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
    R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic
    urban scene understanding. In *IEEE Conference on Computer Vision and Pattern
    Recognition*, pages 3213–3223, 2016.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cordts 等（2016）M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler, R. Benenson,
    U. Franke, S. Roth, 和 B. Schiele. 城市景观数据集用于语义城市场景理解。在 *IEEE 计算机视觉与模式识别会议* 上，页码
    3213–3223，2016。
- en: Doan et al. (2019) A.-D. Doan, Y. Latif, T.-J. Chin, Y. Liu, T.-T. Do, and I. Reid.
    Scalable place recognition under appearance change for autonomous driving. In
    *IEEE/CVF International Conference on Computer Vision*, pages 9319–9328, 2019.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Doan 等（2019）A.-D. Doan, Y. Latif, T.-J. Chin, Y. Liu, T.-T. Do, 和 I. Reid. 可扩展的自适应驾驶场景下的外观变化下的地标识别。在
    *IEEE/CVF 国际计算机视觉会议* 上，页码 9319–9328，2019。
- en: 'Drews et al. (2017) P. Drews, G. Williams, B. Goldfain, E. A. Theodorou, and
    J. M. Rehg. Aggressive deep driving: Combining convolutional neural networks and
    model predictive control. In *Conference on Robot Learning*, pages 133–142\. PMLR,
    2017.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Drews 等（2017）P. Drews, G. Williams, B. Goldfain, E. A. Theodorou, 和 J. M. Rehg.
    激进深度驾驶：结合卷积神经网络和模型预测控制。在 *机器人学习会议* 上，页码 133–142。PMLR，2017。
- en: Fadadu et al. (2022) S. Fadadu, S. Pandey, D. Hegde, Y. Shi, F.-C. Chou, N. Djuric,
    and C. Vallespi-Gonzalez. Multi-view fusion of sensor data for improved perception
    and prediction in autonomous driving. In *IEEE/CVF Winter Conference on Applications
    of Computer Vision*, pages 2349–2357, 2022.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fadadu 等（2022）S. Fadadu, S. Pandey, D. Hegde, Y. Shi, F.-C. Chou, N. Djuric,
    和 C. Vallespi-Gonzalez. 多视角传感器数据融合以改进自主驾驶中的感知和预测。在 *IEEE/CVF 冬季计算机视觉应用会议* 上，页码
    2349–2357，2022。
- en: 'Feng et al. (2020) D. Feng, C. Haase-Schütz, L. Rosenbaum, H. Hertlein, C. Glaeser,
    F. Timm, W. Wiesbeck, and K. Dietmayer. Deep multi-modal object detection and
    semantic segmentation for autonomous driving: Datasets, methods, and challenges.
    *IEEE Transactions on Intelligent Transportation Systems*, 22(3):1341–1360, 2020.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等（2020）D. Feng, C. Haase-Schütz, L. Rosenbaum, H. Hertlein, C. Glaeser,
    F. Timm, W. Wiesbeck, 和 K. Dietmayer. 深度多模态物体检测和语义分割用于自主驾驶：数据集、方法和挑战。*IEEE 智能交通系统汇刊*，22(3)：1341–1360，2020。
- en: Feng et al. (2021) D. Feng, A. Harakeh, S. L. Waslander, and K. Dietmayer. A
    review and comparative study on probabilistic object detection in autonomous driving.
    *IEEE Transactions on Intelligent Transportation Systems*, 23(8):9961–9980, 2021.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Feng 等人 (2021) D. Feng, A. Harakeh, S. L. Waslander, 和 K. Dietmayer. 自动驾驶中概率性物体检测的综述与比较研究。*IEEE
    智能交通系统汇刊*, 23(8):9961–9980, 2021。
- en: Ganin and Lempitsky (2015) Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
    by backpropagation. In *International Conference on Machine Learning*, pages 1180–1189\.
    PMLR, 2015.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ganin 和 Lempitsky (2015) Y. Ganin 和 V. Lempitsky. 通过反向传播进行无监督领域适应。*国际机器学习会议*,
    页码 1180–1189。PMLR, 2015。
- en: Gao et al. (2021) B. Gao, Y. Pan, C. Li, S. Geng, and H. Zhao. Are we hungry
    for 3d lidar data for semantic segmentation? a survey of datasets and methods.
    *IEEE Transactions on Intelligent Transportation Systems*, 23(7):6063–6081, 2021.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2021) B. Gao, Y. Pan, C. Li, S. Geng, 和 H. Zhao. 我们是否渴望用于语义分割的3D激光雷达数据？数据集和方法的综述。*IEEE
    智能交通系统汇刊*, 23(7):6063–6081, 2021。
- en: Gao et al. (2022) H. Gao, J. Guo, G. Wang, and Q. Zhang. Cross-domain correlation
    distillation for unsupervised domain adaptation in nighttime semantic segmentation.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 9913–9923,
    2022.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2022) H. Gao, J. Guo, G. Wang, 和 Q. Zhang. 用于夜间语义分割的跨域相关蒸馏。*IEEE/CVF
    计算机视觉与模式识别会议*, 页码 9913–9923, 2022。
- en: Gao et al. (2023) Y. Gao, W. Tong, E. Q. Wu, W. Chen, G. Zhu, and F.-Y. Wang.
    Chat with chatgpt on interactive engines for intelligent driving. *IEEE Transactions
    on Intelligent Vehicles*, 2023.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gao 等人 (2023) Y. Gao, W. Tong, E. Q. Wu, W. Chen, G. Zhu, 和 F.-Y. Wang. 与 ChatGPT
    讨论智能驾驶的互动引擎。*IEEE 智能车辆汇刊*, 2023。
- en: 'Geiger et al. (2013) A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision
    meets robotics: The kitti dataset. *The International Journal of Robotics Research*,
    32(11):1231–1237, 2013.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geiger 等人 (2013) A. Geiger, P. Lenz, C. Stiller, 和 R. Urtasun. 视觉遇见机器人：KITTI
    数据集。*国际机器人研究期刊*, 32(11):1231–1237, 2013。
- en: 'Geyer et al. (2020) J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh,
    A. S. Chung, L. Hauswald, V. H. Pham, M. Mühlegg, S. Dorn, et al. A2d2: Audi autonomous
    driving dataset. *arXiv preprint arXiv:2004.06320*, 2020.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Geyer 等人 (2020) J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh, A.
    S. Chung, L. Hauswald, V. H. Pham, M. Mühlegg, S. Dorn, 等人. A2D2：奥迪自动驾驶数据集。*arXiv
    预印本 arXiv:2004.06320*, 2020。
- en: 'Gholamhosseinian and Seitz (2021) A. Gholamhosseinian and J. Seitz. Vehicle
    classification in intelligent transport systems: An overview, methods and software
    perspective. *IEEE Open Journal of Intelligent Transportation Systems*, 2:173–194,
    2021.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gholamhosseinian 和 Seitz (2021) A. Gholamhosseinian 和 J. Seitz. 智能交通系统中的车辆分类：概述、方法和软件视角。*IEEE
    智能交通系统开放期刊*, 2:173–194, 2021。
- en: Gojcic et al. (2021) Z. Gojcic, O. Litany, A. Wieser, L. J. Guibas, and T. Birdal.
    Weakly supervised learning of rigid 3d scene flow. In *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pages 5692–5703, 2021.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gojcic 等人 (2021) Z. Gojcic, O. Litany, A. Wieser, L. J. Guibas, 和 T. Birdal.
    强监督学习的刚性3D场景流。*IEEE/CVF 计算机视觉与模式识别会议*, 页码 5692–5703, 2021。
- en: Goodfellow et al. (2020) I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. *Communications
    of the ACM*, 63(11):139–144, 2020.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow 等人 (2020) I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, 和 Y. Bengio. 生成对抗网络。*ACM 通讯*, 63(11):139–144, 2020。
- en: 'Gou et al. (2021) J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation:
    A survey. *International Journal of Computer Vision*, 129:1789–1819, 2021.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gou 等人 (2021) J. Gou, B. Yu, S. J. Maybank, 和 D. Tao. 知识蒸馏：综述。*计算机视觉国际期刊*, 129:1789–1819,
    2021。
- en: Gretton et al. (2012) A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan,
    M. Pontil, K. Fukumizu, and B. K. Sriperumbudur. Optimal kernel choice for large-scale
    two-sample tests. *Advances in Neural Information Processing Systems*, 25, 2012.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gretton 等人 (2012) A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan,
    M. Pontil, K. Fukumizu, 和 B. K. Sriperumbudur. 大规模两样本测试的最佳核选择。*神经信息处理系统进展*, 25,
    2012。
- en: Grigorescu et al. (2020) S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu.
    A survey of deep learning techniques for autonomous driving. *Journal of Field
    Robotics*, 37(3):362–386, 2020.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Grigorescu 等人 (2020) S. Grigorescu, B. Trasnea, T. Cocias, 和 G. Macesanu. 自动驾驶的深度学习技术综述。*领域机器人学期刊*,
    37(3):362–386, 2020。
- en: Guo et al. (2018) D. Guo, L. Zhu, Y. Lu, H. Yu, and S. Wang. Small object sensitive
    segmentation of urban street scene with spatial adjacency between object classes.
    *IEEE Transactions on Image Processing*, 28(6):2643–2653, 2018.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人（2018）D. Guo, L. Zhu, Y. Lu, H. Yu, 和 S. Wang。城市街景中小物体的敏感分割与物体类别之间的空间邻接。*IEEE图像处理汇刊*，28(6):2643–2653，2018。
- en: Guo et al. (2019a) D. Guo, Y. Pei, K. Zheng, H. Yu, Y. Lu, and S. Wang. Degraded
    image semantic segmentation with dense-gram networks. *IEEE Transactions on Image
    Processing*, 29:782–795, 2019a.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人（2019a）D. Guo, Y. Pei, K. Zheng, H. Yu, Y. Lu, 和 S. Wang。使用密集-gram网络进行退化图像语义分割。*IEEE图像处理汇刊*，29:782–795，2019a。
- en: 'Guo et al. (2019b) Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. Feris.
    Spottune: transfer learning through adaptive fine-tuning. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 4805–4814, 2019b.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Guo等人（2019b）Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, 和 R. Feris。Spottune：通过自适应微调进行迁移学习。见*IEEE/CVF计算机视觉与模式识别会议*，第4805–4814页，2019b。
- en: Hao et al. (2019) Z. Hao, S. You, Y. Li, K. Li, and F. Lu. Learning from synthetic
    photorealistic raindrop for single image raindrop removal. In *IEEE/CVF International
    Conference on Computer Vision Workshops*, pages 0–0, 2019.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hao等人（2019）Z. Hao, S. You, Y. Li, K. Li, 和 F. Lu。学习合成光现实雨滴用于单幅图像雨滴去除。见*IEEE/CVF国际计算机视觉大会研讨会*，第0–0页，2019。
- en: Hinton et al. (2015) G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
    in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton等人（2015）G. Hinton, O. Vinyals, 和 J. Dean。提取神经网络中的知识。*arXiv预印本 arXiv:1503.02531*，2015。
- en: 'Hnewa and Radha (2020) M. Hnewa and H. Radha. Object detection under rainy
    conditions for autonomous vehicles: A review of state-of-the-art and emerging
    techniques. *IEEE Signal Processing Magazine*, 38(1):53–67, 2020.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hnewa和Radha（2020）M. Hnewa 和 H. Radha。雨天条件下的物体检测：最新技术与新兴技术的综述。*IEEE信号处理杂志*，38(1):53–67，2020。
- en: 'Hoffman et al. (2018) J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko,
    A. Efros, and T. Darrell. Cycada: Cycle-consistent adversarial domain adaptation.
    In *International Conference on Machine Learning*, pages 1989–1998\. Pmlr, 2018.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hoffman等人（2018）J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko,
    A. Efros, 和 T. Darrell。Cycada：循环一致的对抗性领域适应。见*国际机器学习大会*，第1989–1998页。Pmlr，2018。
- en: Hou et al. (2019) Y. Hou, Z. Ma, C. Liu, and C. C. Loy. Learning lightweight
    lane detection cnns by self attention distillation. In *IEEE/CVF International
    Conference on Computer Vision*, pages 1013–1021, 2019.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou等人（2019）Y. Hou, Z. Ma, C. Liu, 和 C. C. Loy。通过自注意力蒸馏学习轻量级车道检测CNN。见*IEEE/CVF国际计算机视觉大会*，第1013–1021页，2019。
- en: Hou et al. (2022) Y. Hou, X. Zhu, Y. Ma, C. C. Loy, and Y. Li. Point-to-voxel
    knowledge distillation for lidar semantic segmentation. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 8479–8488, 2022.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hou等人（2022）Y. Hou, X. Zhu, Y. Ma, C. C. Loy, 和 Y. Li。点到体素知识蒸馏用于激光雷达语义分割。见*IEEE/CVF计算机视觉与模式识别会议*，第8479–8488页，2022。
- en: 'Houston et al. (2021) J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, L. Chen,
    A. Jain, S. Omari, V. Iglovikov, and P. Ondruska. One thousand and one hours:
    Self-driving motion prediction dataset. In *Conference on Robot Learning*, pages
    409–418\. PMLR, 2021.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Houston等人（2021）J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, L. Chen, A. Jain,
    S. Omari, V. Iglovikov, 和 P. Ondruska。一千零一小时：自驾驶运动预测数据集。见*机器人学习会议*，第409–418页。PMLR，2021。
- en: Hu et al. (2022a) H. Hu, Z. Liu, S. Chitlangia, A. Agnihotri, and D. Zhao. Investigating
    the impact of multi-lidar placement on object detection for autonomous driving.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 2550–2559,
    2022a.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等人（2022a）H. Hu, Z. Liu, S. Chitlangia, A. Agnihotri, 和 D. Zhao。研究多激光雷达布置对自动驾驶物体检测的影响。见*IEEE/CVF计算机视觉与模式识别会议*，第2550–2559页，2022a。
- en: Hu et al. (2022b) Y. Hu, X. Jia, M. Tomizuka, and W. Zhan. Causal-based time
    series domain generalization for vehicle intention prediction. In *International
    Conference on Robotics and Automation*, pages 7806–7813\. IEEE, 2022b.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu等人（2022b）Y. Hu, X. Jia, M. Tomizuka, 和 W. Zhan。基于因果的时间序列领域泛化用于车辆意图预测。见*国际机器人与自动化大会*，第7806–7813页。IEEE，2022b。
- en: Huang et al. (2018) X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin,
    and R. Yang. The apolloscape dataset for autonomous driving. In *IEEE Conference
    on Computer Vision and Pattern Recognition Workshops*, pages 954–960, 2018.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang等人（2018）X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin,
    和 R. Yang。ApolloScape数据集用于自动驾驶。见*IEEE计算机视觉与模式识别研讨会*，第954–960页，2018。
- en: 'Huang and Chen (2020) Y. Huang and Y. Chen. Autonomous driving with deep learning:
    A survey of state-of-art technologies. *arXiv preprint arXiv:2006.06091*, 2020.'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 和 Chen (2020) Y. Huang 和 Y. Chen. 深度学习下的自动驾驶：最前沿技术的综述。*arXiv 预印本 arXiv:2006.06091*，2020
    年。
- en: Inoue et al. (2018) N. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa. Cross-domain
    weakly-supervised object detection through progressive domain adaptation. In *IEEE
    Conference on Computer Vision and Pattern Recognition*, pages 5001–5009, 2018.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inoue 等 (2018) N. Inoue, R. Furuta, T. Yamasaki 和 K. Aizawa. 通过渐进领域适应的跨域弱监督目标检测。见于
    *IEEE 计算机视觉与模式识别会议*，第 5001–5009 页，2018 年。
- en: Isola et al. (2017) P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
    translation with conditional adversarial networks. In *IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 1125–1134, 2017.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isola 等 (2017) P. Isola, J.-Y. Zhu, T. Zhou 和 A. A. Efros. 基于条件对抗网络的图像到图像翻译。见于
    *IEEE 计算机视觉与模式识别会议*，第 1125–1134 页，2017 年。
- en: 'Johnson-Roberson et al. (2017) M. Johnson-Roberson, C. Barto, R. Mehta, S. N.
    Sridhar, K. Rosaen, and R. Vasudevan. Driving in the matrix: Can virtual worlds
    replace human-generated annotations for real world tasks? In *IEEE International
    Conference on Robotics and Automation*, pages 746–753\. IEEE, 2017.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Johnson-Roberson 等 (2017) M. Johnson-Roberson, C. Barto, R. Mehta, S. N. Sridhar,
    K. Rosaen 和 R. Vasudevan. 在矩阵中驾驶：虚拟世界能否替代人为生成的实际任务标注？见于 *IEEE 国际机器人与自动化会议*，第 746–753
    页，IEEE，2017 年。
- en: 'Khalil and Mouftah (2022) Y. H. Khalil and H. T. Mouftah. Licanet: Further
    enhancement of joint perception and motion prediction based on multi-modal fusion.
    *IEEE Open Journal of Intelligent Transportation Systems*, 3:222–235, 2022.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khalil 和 Mouftah (2022) Y. H. Khalil 和 H. T. Mouftah. Licanet：基于多模态融合的联合感知和运动预测的进一步增强。*IEEE
    智能交通系统开放期刊*，3:222–235，2022 年。
- en: Khosravian et al. (2021) A. Khosravian, A. Amirkhani, H. Kashiani, and M. Masih-Tehrani.
    Generalizing state-of-the-art object detectors for autonomous vehicles in unseen
    environments. *Expert Systems with Applications*, 183:115417, 2021.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Khosravian 等 (2021) A. Khosravian, A. Amirkhani, H. Kashiani 和 M. Masih-Tehrani.
    将最前沿目标检测器推广到未知环境中的自动驾驶。*应用专家系统*，183:115417，2021 年。
- en: Ko et al. (2021) Y. Ko, Y. Lee, S. Azam, F. Munir, M. Jeon, and W. Pedrycz.
    Key points estimation and point instance segmentation approach for lane detection.
    *IEEE Transactions on Intelligent Transportation Systems*, 23(7):8949–8958, 2021.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ko 等 (2021) Y. Ko, Y. Lee, S. Azam, F. Munir, M. Jeon 和 W. Pedrycz. 用于车道检测的关键点估计和点实例分割方法。*IEEE
    智能交通系统汇刊*，23(7):8949–8958，2021 年。
- en: Kothandaraman et al. (2021) D. Kothandaraman, A. Nambiar, and A. Mittal. Domain
    adaptive knowledge distillation for driving scene semantic segmentation. In *IEEE/CVF
    Winter Conference on Applications of Computer Vision*, pages 134–143, 2021.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kothandaraman 等 (2021) D. Kothandaraman, A. Nambiar 和 A. Mittal. 领域自适应知识蒸馏用于驾驶场景语义分割。见于
    *IEEE/CVF 冬季计算机视觉应用会议*，第 134–143 页，2021 年。
- en: 'Kumar et al. (2021) V. R. Kumar, M. Klingner, S. Yogamani, S. Milz, T. Fingscheidt,
    and P. Mader. Syndistnet: Self-supervised monocular fisheye camera distance estimation
    synergized with semantic segmentation for autonomous driving. In *IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pages 61–71, 2021.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等 (2021) V. R. Kumar, M. Klingner, S. Yogamani, S. Milz, T. Fingscheidt
    和 P. Mader. Syndistnet：自监督单目鱼眼相机距离估计与语义分割协同用于自动驾驶。见于 *IEEE/CVF 冬季计算机视觉应用会议*，第
    61–71 页，2021 年。
- en: Lan and Tian (2022) Q. Lan and Q. Tian. Instance, scale, and teacher adaptive
    knowledge distillation for visual detection in autonomous driving. *IEEE Transactions
    on Intelligent Vehicles*, 8(3):2358–2370, 2022.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lan 和 Tian (2022) Q. Lan 和 Q. Tian. 针对自动驾驶视觉检测的实例、尺度和教师自适应知识蒸馏。*IEEE 智能车辆汇刊*，8(3):2358–2370，2022
    年。
- en: 'Lang et al. (2019) A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom.
    Pointpillars: Fast encoders for object detection from point clouds. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pages 12697–12705, 2019.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lang 等 (2019) A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang 和 O. Beijbom.
    Pointpillars：用于从点云中进行目标检测的快速编码器。见于 *IEEE/CVF 计算机视觉与模式识别会议*，第 12697–12705 页，2019
    年。
- en: 'Lehner et al. (2022) A. Lehner, S. Gasperini, A. Marcos-Ramiro, M. Schmidt,
    M.-A. N. Mahani, N. Navab, B. Busam, and F. Tombari. 3d-vfield: Adversarial augmentation
    of point clouds for domain generalization in 3d object detection. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pages 17295–17304, 2022.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lehner 等 (2022) A. Lehner, S. Gasperini, A. Marcos-Ramiro, M. Schmidt, M.-A.
    N. Mahani, N. Navab, B. Busam 和 F. Tombari. 3d-vfield：用于 3d 物体检测的点云对抗性增强。见于 *IEEE/CVF
    计算机视觉与模式识别会议*，第 17295–17304 页，2022 年。
- en: Li and Zhang (2021) D. Li and H. Zhang. Improved regularization and robustness
    for fine-tuning in neural networks. *Advances in Neural Information Processing
    Systems*, 34:27249–27262, 2021.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Zhang（2021）D. Li 和 H. Zhang。用于神经网络微调的改进正则化和鲁棒性。*神经信息处理系统进展*，34：27249–27262，2021。
- en: Li et al. (2020a) D. Li, L. Deng, and Z. Cai. Intelligent vehicle network system
    and smart city management based on genetic algorithms and image perception. *Mechanical
    Systems and Signal Processing*, 141:106623, 2020a.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020a）D. Li, L. Deng, 和 Z. Cai。基于遗传算法和图像感知的智能车辆网络系统与智慧城市管理。*机械系统与信号处理*，141：106623，2020a。
- en: Li et al. (2022a) G. Li, Z. Ji, and X. Qu. Stepwise domain adaptation (sda)
    for object detection in autonomous vehicles using an adaptive centernet. *IEEE
    Transactions on Intelligent Transportation Systems*, 23(10):17729–17743, 2022a.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022a）G. Li, Z. Ji, 和 X. Qu。使用自适应 CenterNet 的逐步领域适配（sda）用于自动驾驶中的目标检测。*IEEE
    智能交通系统汇刊*，23(10)：17729–17743，2022a。
- en: 'Li et al. (2022b) G. Li, Z. Ji, X. Qu, R. Zhou, and D. Cao. Cross-domain object
    detection for autonomous driving: A stepwise domain adaptative yolo approach.
    *IEEE Transactions on Intelligent Vehicles*, 7(3):603–615, 2022b.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022b）G. Li, Z. Ji, X. Qu, R. Zhou, 和 D. Cao。自动驾驶的跨领域目标检测：一种逐步领域适应的 YOLO
    方法。*IEEE 智能车辆汇刊*，7(3)：603–615，2022b。
- en: 'Li et al. (2021) J. Li, Z. Xu, L. Fu, X. Zhou, and H. Yu. Domain adaptation
    from daytime to nighttime: A situation-sensitive vehicle detection and traffic
    flow parameter estimation framework. *Transportation Research Part C: Emerging
    Technologies*, 124:102946, 2021.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2021）J. Li, Z. Xu, L. Fu, X. Zhou, 和 H. Yu。从白天到夜晚的领域适配：一个情境敏感的车辆检测和交通流量参数估计框架。*交通研究
    C 部分：新兴技术*，124：102946，2021。
- en: 'Li et al. (2023a) J. Li, R. Xu, X. Liu, B. Li, Q. Zou, J. Ma, and H. Yu. S2r-vit
    for multi-agent cooperative perception: Bridging the gap from simulation to reality.
    *arXiv preprint arXiv:2307.07935*, 2023a.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023a）J. Li, R. Xu, X. Liu, B. Li, Q. Zou, J. Ma, 和 H. Yu。用于多智能体协同感知的
    S2R-ViT：从模拟到现实的桥梁。*arXiv 预印本 arXiv:2307.07935*，2023a。
- en: Li et al. (2023b) J. Li, R. Xu, X. Liu, J. Ma, Z. Chi, J. Ma, and H. Yu. Learning
    for vehicle-to-vehicle cooperative perception under lossy communication. *IEEE
    Transactions on Intelligent Vehicles*, 2023b.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023b）J. Li, R. Xu, X. Liu, J. Ma, Z. Chi, J. Ma, 和 H. Yu。在丢包通信下的车辆对车辆协同感知学习。*IEEE
    智能车辆汇刊*，2023b。
- en: Li et al. (2023c) J. Li, R. Xu, J. Ma, Q. Zou, J. Ma, and H. Yu. Domain adaptive
    object detection for autonomous driving under foggy weather. In *IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pages 612–622, 2023c.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023c）J. Li, R. Xu, J. Ma, Q. Zou, J. Ma, 和 H. Yu。在雾天条件下用于自动驾驶的领域适配目标检测。在
    *IEEE/CVF 计算机视觉应用冬季会议*，页码 612–622，2023c。
- en: 'Li and Ibanez-Guzman (2020) Y. Li and J. Ibanez-Guzman. Lidar for autonomous
    driving: The principles, challenges, and trends for automotive lidar and perception
    systems. *IEEE Signal Processing Magazine*, 37(4):50–61, 2020.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 和 Ibanez-Guzman（2020）Y. Li 和 J. Ibanez-Guzman。用于自动驾驶的激光雷达：汽车激光雷达和感知系统的原理、挑战和趋势。*IEEE
    信号处理杂志*，37(4)：50–61，2020。
- en: 'Li et al. (2020b) Y. Li, L. Ma, Z. Zhong, F. Liu, M. A. Chapman, D. Cao, and
    J. Li. Deep learning for lidar point clouds in autonomous driving: A review. *IEEE
    Transactions on Neural Networks and Learning Systems*, 32(8):3412–3432, 2020b.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2020b）Y. Li, L. Ma, Z. Zhong, F. Liu, M. A. Chapman, D. Cao, 和 J. Li。用于自动驾驶的激光雷达点云深度学习：综述。*IEEE
    神经网络与学习系统汇刊*，32(8)：3412–3432，2020b。
- en: 'Li et al. (2022c) Y. Li, Z. Li, S. Teng, Y. Zhang, Y. Zhou, Y. Zhu, D. Cao,
    B. Tian, Y. Ai, Z. Xuanyuan, et al. Automine: An unmanned mine dataset. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pages 21308–21317, 2022c.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022c）Y. Li, Z. Li, S. Teng, Y. Zhang, Y. Zhou, Y. Zhu, D. Cao, B. Tian,
    Y. Ai, Z. Xuanyuan 等人。Automine：一个无人矿山数据集。在 *IEEE/CVF 计算机视觉与模式识别大会*，页码 21308–21317，2022c。
- en: Li et al. (2023d) Y. Li, D. Zhang, M. Keuper, and A. Khoreva. Intra-source style
    augmentation for improved domain generalization. In *IEEE/CVF Winter Conference
    on Applications of Computer Vision*, pages 509–519, 2023d.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2023d）Y. Li, D. Zhang, M. Keuper, 和 A. Khoreva。用于改善领域泛化的源内风格增强。在 *IEEE/CVF
    计算机视觉应用冬季会议*，页码 509–519，2023d。
- en: Li et al. (2022d) Z. Li, Y. Du, M. Zhu, S. Zhou, and L. Zhang. A survey of 3d
    object detection algorithms for intelligent vehicles development. *Artificial
    Life and Robotics*, pages 1–8, 2022d.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li 等人（2022d）Z. Li, Y. Du, M. Zhu, S. Zhou, 和 L. Zhang。智能车辆开发中 3D 目标检测算法的综述。*人工生命与机器人*，页码
    1–8，2022d。
- en: 'Li et al. (2023e) Z. Li, C. Gong, Y. Lin, G. Li, X. Wang, C. Lu, M. Wang, S. Chen,
    and J. Gong. Continual driver behaviour learning for connected vehicles and intelligent
    transportation systems: Framework, survey and challenges. *Green Energy and Intelligent
    Transportation*, page 100103, 2023e.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Li et al. (2023e) Z. Li, C. Gong, Y. Lin, G. Li, X. Wang, C. Lu, M. Wang, S.
    Chen, 和 J. Gong. 连续驾驶行为学习用于联网车辆和智能交通系统：框架、调查与挑战。*绿色能源与智能交通*，第100103页，2023e。
- en: Liang et al. (2022) X. Liang, Y. Liu, T. Chen, M. Liu, and Q. Yang. Federated
    transfer reinforcement learning for autonomous driving. In *Federated and Transfer
    Learning*, pages 357–371\. Springer, 2022.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liang et al. (2022) X. Liang, Y. Liu, T. Chen, M. Liu, 和 Q. Yang. 联邦迁移强化学习用于自动驾驶。在
    *联邦与迁移学习*，第357–371页。Springer，2022。
- en: 'Liao et al. (2022) Y. Liao, J. Xie, and A. Geiger. Kitti-360: A novel dataset
    and benchmarks for urban scene understanding in 2d and 3d. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2022.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liao et al. (2022) Y. Liao, J. Xie, 和 A. Geiger. Kitti-360：用于城市场景理解的2D和3D新数据集及基准。*IEEE模式分析与机器智能汇刊*，2022。
- en: Lin et al. (2021) C. Lin, Z. Yuan, S. Zhao, P. Sun, C. Wang, and J. Cai. Domain-invariant
    disentangled network for generalizable object detection. In *IEEE/CVF International
    Conference on Computer Vision*, pages 8771–8780, 2021.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lin et al. (2021) C. Lin, Z. Yuan, S. Zhao, P. Sun, C. Wang, 和 J. Cai. 域不变解耦网络用于通用目标检测。在
    *IEEE/CVF国际计算机视觉大会*，第8771–8780页，2021。
- en: 'Liu et al. (2021a) L. Liu, X. Chen, S. Zhu, and P. Tan. Condlanenet: a top-to-down
    lane detection framework based on conditional convolution. In *IEEE/CVF International
    Conference on Computer Vision*, pages 3773–3782, 2021a.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021a) L. Liu, X. Chen, S. Zhu, 和 P. Tan. Condlanenet：基于条件卷积的自顶向下车道检测框架。在
    *IEEE/CVF国际计算机视觉大会*，第3773–3782页，2021a。
- en: Liu et al. (2017) M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image
    translation networks. *Advances in neural information processing systems*, 30,
    2017.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2017) M.-Y. Liu, T. Breuel, 和 J. Kautz. 无监督图像到图像翻译网络。*神经信息处理系统进展*，30，2017。
- en: 'Liu et al. (2022a) P. Liu, C. Zhang, H. Qi, G. Wang, and H. Zheng. Multi-attention
    densenet: A scattering medium imaging optimization framework for visual data pre-processing
    of autonomous driving systems. *IEEE Transactions on Intelligent Transportation
    Systems*, 23(12):25396–25407, 2022a.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022a) P. Liu, C. Zhang, H. Qi, G. Wang, 和 H. Zheng. 多注意力密集网络：用于自动驾驶系统视觉数据预处理的散射介质成像优化框架。*IEEE智能交通系统汇刊*，23(12):25396–25407，2022a。
- en: Liu et al. (2021b) W. Liu, X. Xia, L. Xiong, Y. Lu, L. Gao, and Z. Yu. Automated
    vehicle sideslip angle estimation considering signal measurement characteristic.
    *IEEE Sensors Journal*, 21(19):21675–21687, 2021b.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021b) W. Liu, X. Xia, L. Xiong, Y. Lu, L. Gao, 和 Z. Yu. 考虑信号测量特性的自动化车辆侧滑角估计。*IEEE传感器期刊*，21(19):21675–21687，2021b。
- en: 'Liu et al. (2022b) W. Liu, K. Quijano, and M. M. Crawford. Yolov5-tassel: detecting
    tassels in rgb uav imagery with improved yolov5 based on transfer learning. *IEEE
    Selected Topics in Applied Earth Observations and Remote Sensing*, 15:8085–8094,
    2022b.'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2022b) W. Liu, K. Quijano, 和 M. M. Crawford. Yolov5-tassel：基于迁移学习的改进yolov5检测RGB无人机图像中的穗子。*IEEE应用地球观测与遥感精选主题*，15:8085–8094，2022b。
- en: Liu et al. (2021c) Y. Liu, W. Zhang, and J. Wang. Source-free domain adaptation
    for semantic segmentation. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 1215–1224, 2021c.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu et al. (2021c) Y. Liu, W. Zhang, 和 J. Wang. 无源域适应用于语义分割。在 *IEEE/CVF计算机视觉与模式识别会议*，第1215–1224页，2021c。
- en: Long et al. (2015) M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable
    features with deep adaptation networks. In *International Conference on Machine
    Learning*, pages 97–105\. PMLR, 2015.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. (2015) M. Long, Y. Cao, J. Wang, 和 M. Jordan. 使用深度适应网络学习可迁移特征。在
    *国际机器学习大会*，第97–105页。PMLR，2015。
- en: Long et al. (2017) M. Long, H. Zhu, J. Wang, and M. I. Jordan. Deep transfer
    learning with joint adaptation networks. In *International Conference on Machine
    Learning*, pages 2208–2217\. PMLR, 2017.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Long et al. (2017) M. Long, H. Zhu, J. Wang, 和 M. I. Jordan. 具有联合适应网络的深度迁移学习。在
    *国际机器学习大会*，第2208–2217页。PMLR，2017。
- en: Luo et al. (2021) C. Luo, X. Yang, and A. Yuille. Self-supervised pillar motion
    learning for autonomous driving. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pages 3183–3192, 2021.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo et al. (2021) C. Luo, X. Yang, 和 A. Yuille. 自监督柱状运动学习用于自动驾驶。在 *IEEE/CVF计算机视觉与模式识别会议*，第3183–3192页，2021。
- en: Luo et al. (2022) X. Luo, J. Zhang, K. Yang, A. Roitberg, K. Peng, and R. Stiefelhagen.
    Towards robust semantic segmentation of accident scenes via multi-source mixed
    sampling and meta-learning. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 4429–4439, 2022.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Luo等人（2022）X. Luo, J. Zhang, K. Yang, A. Roitberg, K. Peng, 和 R. Stiefelhagen.
    通过多源混合采样和元学习实现事故场景的鲁棒语义分割。发表于*IEEE/CVF计算机视觉与模式识别会议*，第4429–4439页，2022年。
- en: 'Mao et al. (2021) J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li,
    C. Ye, W. Zhang, Z. Li, et al. One million scenes for autonomous driving: Once
    dataset. *arXiv preprint arXiv:2106.11037*, 2021.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mao等人（2021）J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li, C.
    Ye, W. Zhang, Z. Li, 等人. 一百万个用于自动驾驶的场景：一次数据集。*arXiv预印本 arXiv:2106.11037*，2021年。
- en: Masmoudi et al. (2021) M. Masmoudi, H. Friji, H. Ghazzai, and Y. Massoud. A
    reinforcement learning framework for video frame-based autonomous car-following.
    *IEEE Open Journal of Intelligent Transportation Systems*, 2:111–127, 2021.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Masmoudi等人（2021）M. Masmoudi, H. Friji, H. Ghazzai, 和 Y. Massoud. 基于视频帧的自动驾驶跟车的强化学习框架。*IEEE智能交通系统开放期刊*，2:111–127，2021年。
- en: 'Miglani and Kumar (2019) A. Miglani and N. Kumar. Deep learning models for
    traffic flow prediction in autonomous vehicles: A review, solutions, and challenges.
    *Vehicular Communications*, 20:100184, 2019.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Miglani和Kumar（2019）A. Miglani 和 N. Kumar. 自动驾驶车辆的交通流预测深度学习模型：综述、解决方案与挑战。*车辆通信*，20:100184，2019年。
- en: Mirza et al. (2022) M. J. Mirza, M. Masana, H. Possegger, and H. Bischof. An
    efficient domain-incremental learning approach to drive in all weather conditions.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3001–3011,
    2022.
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mirza等人（2022）M. J. Mirza, M. Masana, H. Possegger, 和 H. Bischof. 一种高效的领域增量学习方法，适用于所有天气条件下的驾驶。发表于*IEEE/CVF计算机视觉与模式识别会议*，第3001–3011页，2022年。
- en: Mo et al. (2022) Y. Mo, Y. Wu, X. Yang, F. Liu, and Y. Liao. Review the state-of-the-art
    technologies of semantic segmentation based on deep learning. *Neurocomputing*,
    493:626–646, 2022.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mo等人（2022）Y. Mo, Y. Wu, X. Yang, F. Liu, 和 Y. Liao. 基于深度学习的语义分割前沿技术综述。*神经计算*，493:626–646，2022年。
- en: 'Mohammed et al. (2020) A. S. Mohammed, A. Amamou, F. K. Ayevide, S. Kelouwani,
    K. Agbossou, and N. Zioui. The perception system of intelligent ground vehicles
    in all weather conditions: A systematic literature review. *Sensors*, 20(22):6532,
    2020.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mohammed等人（2020）A. S. Mohammed, A. Amamou, F. K. Ayevide, S. Kelouwani, K. Agbossou,
    和 N. Zioui. 智能地面车辆在所有天气条件下的感知系统：系统文献综述。*传感器*，20(22):6532，2020年。
- en: Murez et al. (2018) Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and K. Kim.
    Image to image translation for domain adaptation. In *IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 4500–4509, 2018.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Murez等人（2018）Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, 和 K. Kim. 领域适应的图像到图像翻译。发表于*IEEE计算机视觉与模式识别会议*，第4500–4509页，2018年。
- en: 'Mușat et al. (2021) V. Mușat, I. Fursa, P. Newman, F. Cuzzolin, and A. Bradley.
    Multi-weather city: Adverse weather stacking for autonomous driving. In *IEEE/CVF
    International Conference on Computer Vision*, pages 2906–2915, 2021.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mușat等人（2021）V. Mușat, I. Fursa, P. Newman, F. Cuzzolin, 和 A. Bradley. 多天气城市：用于自动驾驶的恶劣天气堆叠。发表于*IEEE/CVF国际计算机视觉会议*，第2906–2915页，2021年。
- en: Niu et al. (2020) S. Niu, Y. Liu, J. Wang, and H. Song. A decade survey of transfer
    learning (2010–2020). *IEEE Transactions on Artificial Intelligence*, 1(2):151–166,
    2020.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Niu等人（2020）S. Niu, Y. Liu, J. Wang, 和 H. Song. 迁移学习的十年综述（2010–2020）。*IEEE人工智能学报*，1(2):151–166，2020年。
- en: Pan et al. (2020) F. Pan, I. Shin, F. Rameau, S. Lee, and I. S. Kweon. Unsupervised
    intra-domain adaptation for semantic segmentation through self-supervision. In
    *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3764–3773,
    2020.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan等人（2020）F. Pan, I. Shin, F. Rameau, S. Lee, 和 I. S. Kweon. 通过自监督进行语义分割的无监督域内适应。发表于*IEEE/CVF计算机视觉与模式识别会议*，第3764–3773页，2020年。
- en: Pan and Yang (2010) S. J. Pan and Q. Yang. A survey on transfer learning. *IEEE
    Transactions on Knowledge and Data Engineering*, 22(10):1345–1359, 2010.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pan和Yang（2010）S. J. Pan 和 Q. Yang. 迁移学习综述。*IEEE知识与数据工程学报*，22(10):1345–1359，2010年。
- en: Park et al. (2020) T. Park, A. A. Efros, R. Zhang, and J.-Y. Zhu. Contrastive
    learning for unpaired image-to-image translation. In *Computer Vision–16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16*, pages 319–345\.
    Springer, 2020.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Park等人（2020）T. Park, A. A. Efros, R. Zhang, 和 J.-Y. Zhu. 无配对图像到图像翻译的对比学习。发表于*计算机视觉–第16届欧洲会议，英国格拉斯哥，2020年8月23–28日，论文集，第九部分16*，第319–345页。Springer，2020年。
- en: Patil et al. (2019) A. Patil, S. Malla, H. Gang, and Y.-T. Chen. The h3d dataset
    for full-surround 3d multi-object detection and tracking in crowded urban scenes.
    In *International Conference on Robotics and Automation*, pages 9552–9557\. IEEE,
    2019.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Patil 等人 (2019) A. Patil, S. Malla, H. Gang, 和 Y.-T. Chen. 用于拥挤城市场景中全环绕3D多目标检测和跟踪的H3D数据集。见
    *国际机器人与自动化会议*，页码 9552–9557。IEEE，2019。
- en: 'Peng et al. (2018) X. Peng, B. Usman, K. Saito, N. Kaushik, J. Hoffman, and
    K. Saenko. Syn2real: A new benchmark forsynthetic-to-real visual domain adaptation.
    *arXiv preprint arXiv:1806.09755*, 2018.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Peng 等人 (2018) X. Peng, B. Usman, K. Saito, N. Kaushik, J. Hoffman, 和 K. Saenko.
    Syn2real：一个新的合成到真实视觉领域适应基准。*arXiv 预印本 arXiv:1806.09755*，2018。
- en: 'Pham et al. (2020) Q.-H. Pham, P. Sevestre, R. S. Pahwa, H. Zhan, C. H. Pang,
    Y. Chen, A. Mustafa, V. Chandrasekhar, and J. Lin. A 3d dataset: Towards autonomous
    driving in challenging environments. In *IEEE International Conference on Robotics
    and Automation*, pages 2267–2273\. IEEE, 2020.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pham 等人 (2020) Q.-H. Pham, P. Sevestre, R. S. Pahwa, H. Zhan, C. H. Pang, Y.
    Chen, A. Mustafa, V. Chandrasekhar, 和 J. Lin. 一个3D数据集：面向挑战环境中的自动驾驶。见 *IEEE 国际机器人与自动化会议*，页码
    2267–2273。IEEE，2020。
- en: Pizzati et al. (2020) F. Pizzati, R. d. Charette, M. Zaccaria, and P. Cerri.
    Domain bridge for unpaired image-to-image translation and unsupervised domain
    adaptation. In *IEEE/CVF Winter Conference on Applications of Computer Vision*,
    pages 2990–2998, 2020.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pizzati 等人 (2020) F. Pizzati, R. d. Charette, M. Zaccaria, 和 P. Cerri. 用于未配对图像到图像翻译和无监督领域适应的领域桥。见
    *IEEE/CVF 冬季计算机视觉应用会议*，页码 2990–2998，2020。
- en: 'Qian et al. (2022) R. Qian, X. Lai, and X. Li. 3d object detection for autonomous
    driving: A survey. *Pattern Recognition*, 130:108796, 2022.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qian 等人 (2022) R. Qian, X. Lai, 和 X. Li. 自动驾驶中的3D目标检测：综述。*模式识别*，130:108796，2022。
- en: Qiao et al. (2020) F. Qiao, L. Zhao, and X. Peng. Learning to learn single domain
    generalization. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 12556–12565, 2020.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiao 等人 (2020) F. Qiao, L. Zhao, 和 X. Peng. 学习学习单一领域泛化。见 *IEEE/CVF 计算机视觉与模式识别大会*，页码
    12556–12565，2020。
- en: 'Rashed et al. (2021) H. Rashed, E. Mohamed, G. Sistu, V. R. Kumar, C. Eising,
    A. El-Sallab, and S. Yogamani. Generalized object detection on fisheye cameras
    for autonomous driving: Dataset, representations and baseline. In *IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pages 2272–2280, 2021.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rashed 等人 (2021) H. Rashed, E. Mohamed, G. Sistu, V. R. Kumar, C. Eising, A.
    El-Sallab, 和 S. Yogamani. 在鱼眼相机上的广义目标检测用于自动驾驶：数据集、表示和基线。见 *IEEE/CVF 冬季计算机视觉应用会议*，页码
    2272–2280，2021。
- en: 'Richter et al. (2016) S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing
    for data: Ground truth from computer games. In *Computer Vision–14th European
    Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part
    II 14*, pages 102–118. Springer, 2016.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Richter 等人 (2016) S. R. Richter, V. Vineet, S. Roth, 和 V. Koltun. 为数据而玩：来自计算机游戏的真实数据。见
    *计算机视觉—第14届欧洲会议，荷兰阿姆斯特丹，2016年10月11-14日，论文集，第二部分 14*，页码 102–118。Springer，2016。
- en: Rist et al. (2019) C. B. Rist, M. Enzweiler, and D. M. Gavrila. Cross-sensor
    deep domain adaptation for lidar detection and segmentation. In *IEEE Intelligent
    Vehicles Symposium*, pages 1535–1542. IEEE, 2019.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Rist 等人 (2019) C. B. Rist, M. Enzweiler, 和 D. M. Gavrila. 用于激光雷达检测和分割的跨传感器深度领域适应。见
    *IEEE 智能车辆研讨会*，页码 1535–1542。IEEE，2019。
- en: 'Ros et al. (2016) G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
    Lopez. The synthia dataset: A large collection of synthetic images for semantic
    segmentation of urban scenes. In *IEEE Conference on Computer Vision and Pattern
    Recognition*, pages 3234–3243, 2016.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ros 等人 (2016) G. Ros, L. Sellart, J. Materzynska, D. Vazquez, 和 A. M. Lopez.
    SynthIA 数据集：一个用于城市场景语义分割的大型合成图像集合。见 *IEEE 计算机视觉与模式识别会议*，页码 3234–3243，2016。
- en: Ruan et al. (2023) J. Ruan, H. Cui, Y. Huang, T. Li, C. Wu, and K. Zhang. A
    review of occluded objects detection in real complex scenarios for autonomous
    driving. *Green Energy and Intelligent Transportation*, page 100092, 2023.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ruan 等人 (2023) J. Ruan, H. Cui, Y. Huang, T. Li, C. Wu, 和 K. Zhang. 关于自动驾驶中真实复杂场景下遮挡物体检测的综述。*绿色能源与智能交通*，页码
    100092，2023。
- en: Sakaridis et al. (2018) C. Sakaridis, D. Dai, and L. Van Gool. Semantic foggy
    scene understanding with synthetic data. *International Journal of Computer Vision*,
    126:973–992, 2018.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sakaridis 等人 (2018) C. Sakaridis, D. Dai, 和 L. Van Gool. 使用合成数据进行语义雾景理解。*国际计算机视觉杂志*，126:973–992，2018。
- en: Sanchez et al. (2022) J. Sanchez, J.-E. Deschaud, and F. Goulette. Domain generalization
    of 3d semantic segmentation in autonomous driving. *arXiv preprint arXiv:2212.04245*,
    2022.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sanchez 等人 (2022) J. Sanchez, J.-E. Deschaud, 和 F. Goulette. 自动驾驶中3D语义分割的领域泛化。*arXiv
    预印本 arXiv:2212.04245*，2022。
- en: Sautier et al. (2022) C. Sautier, G. Puy, S. Gidaris, A. Boulch, A. Bursuc,
    and R. Marlet. Image-to-lidar self-supervised distillation for autonomous driving
    data. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    9891–9901, 2022.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sautier 等人（2022）C. Sautier, G. Puy, S. Gidaris, A. Boulch, A. Bursuc 和 R. Marlet.
    图像到激光雷达的自监督蒸馏用于自主驾驶数据。在 *IEEE/CVF 计算机视觉与模式识别大会*，页码 9891–9901，2022。
- en: 'Schlager et al. (2022a) B. Schlager, T. Goelles, M. Behmer, S. Muckenhuber,
    J. Payer, and D. Watzenig. Automotive lidar and vibration: Resonance, inertial
    measurement unit, and effects on the point cloud. *IEEE Open Journal of Intelligent
    Transportation Systems*, 3:426–434, 2022a.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schlager 等人（2022a）B. Schlager, T. Goelles, M. Behmer, S. Muckenhuber, J. Payer
    和 D. Watzenig. 汽车激光雷达和振动：共振、惯性测量单元及其对点云的影响。*IEEE 开放智能交通系统期刊*，3:426–434，2022a。
- en: 'Schlager et al. (2022b) B. Schlager, T. Goelles, S. Muckenhuber, and D. Watzenig.
    Contaminations on lidar sensor covers: Performance degradation including fault
    detection and modeling as potential applications. *IEEE Open Journal of Intelligent
    Transportation Systems*, 3:738–747, 2022b.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schlager 等人（2022b）B. Schlager, T. Goelles, S. Muckenhuber 和 D. Watzenig. 激光雷达传感器覆盖物上的污染：性能退化，包括故障检测和建模作为潜在应用。*IEEE
    开放智能交通系统期刊*，3:738–747，2022b。
- en: 'Schutera et al. (2020) M. Schutera, M. Hussein, J. Abhau, R. Mikut, and M. Reischl.
    Night-to-day: Online image-to-image translation for object detection within autonomous
    driving by night. *IEEE Transactions on Intelligent Vehicles*, 6(3):480–489, 2020.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Schutera 等人（2020）M. Schutera, M. Hussein, J. Abhau, R. Mikut 和 M. Reischl. 从夜晚到白天：夜间自主驾驶中的物体检测的在线图像到图像翻译。*IEEE
    智能车辆学报*，6(3):480–489，2020。
- en: Shan et al. (2019) Y. Shan, W. F. Lu, and C. M. Chew. Pixel and feature level
    based domain adaptation for object detection in autonomous driving. *Neurocomputing*,
    367:31–38, 2019.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shan 等人（2019）Y. Shan, W. F. Lu 和 C. M. Chew. 基于像素和特征级的领域适应用于自主驾驶中的目标检测。*神经计算*，367:31–38，2019。
- en: 'Shen et al. (2021) Z. Shen, J. Liu, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui.
    Towards out-of-distribution generalization: A survey. *arXiv preprint arXiv:2108.13624*,
    2021.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shen 等人（2021）Z. Shen, J. Liu, Y. He, X. Zhang, R. Xu, H. Yu 和 P. Cui. 面向分布外泛化：综述。*arXiv
    预印本 arXiv:2108.13624*，2021。
- en: 'Shenaj et al. (2023) D. Shenaj, E. Fanì, M. Toldo, D. Caldarola, A. Tavera,
    U. Michieli, M. Ciccone, P. Zanuttigh, and B. Caputo. Learning across domains
    and devices: Style-driven source-free domain adaptation in clustered federated
    learning. In *IEEE/CVF Winter Conference on Applications of Computer Vision*,
    pages 444–454, 2023.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shenaj 等人（2023）D. Shenaj, E. Fanì, M. Toldo, D. Caldarola, A. Tavera, U. Michieli,
    M. Ciccone, P. Zanuttigh 和 B. Caputo. 跨领域和设备的学习：集群联邦学习中的风格驱动的无源领域适应。在 *IEEE/CVF
    计算机视觉应用冬季会议*，页码 444–454，2023。
- en: 'Shin et al. (2022) I. Shin, Y.-H. Tsai, B. Zhuang, S. Schulter, B. Liu, S. Garg,
    I. S. Kweon, and K.-J. Yoon. Mm-tta: multi-modal test-time adaptation for 3d semantic
    segmentation. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 16928–16937, 2022.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Shin 等人（2022）I. Shin, Y.-H. Tsai, B. Zhuang, S. Schulter, B. Liu, S. Garg, I.
    S. Kweon 和 K.-J. Yoon. Mm-tta：用于 3d 语义分割的多模态测试时间适应。在 *IEEE/CVF 计算机视觉与模式识别大会*，页码
    16928–16937，2022。
- en: Song et al. (2019a) S. Song, H. Yu, Z. Miao, Q. Zhang, Y. Lin, and S. Wang.
    Domain adaptation for convolutional neural networks-based remote sensing scene
    classification. *IEEE Geoscience and Remote Sensing Letters*, 16(8):1324–1328,
    2019a.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人（2019a）S. Song, H. Yu, Z. Miao, Q. Zhang, Y. Lin 和 S. Wang. 基于卷积神经网络的遥感场景分类的领域适应。*IEEE
    地球科学与遥感快报*，16(8):1324–1328，2019a。
- en: Song et al. (2020) S. Song, H. Yu, Z. Miao, J. Fang, K. Zheng, C. Ma, and S. Wang.
    Multi-spectral salient object detection by adversarial domain adaptation. In *AAAI
    Conference on Artificial Intelligence*, volume 34, pages 12023–12030, 2020.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人（2020）S. Song, H. Yu, Z. Miao, J. Fang, K. Zheng, C. Ma 和 S. Wang. 通过对抗性领域适应进行多光谱显著目标检测。在
    *AAAI 人工智能会议*，第 34 卷，页码 12023–12030，2020。
- en: 'Song et al. (2019b) X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su,
    H. Li, and R. Yang. Apollocar3d: A large 3d car instance understanding benchmark
    for autonomous driving. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 5452–5462, 2019b.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人（2019b）X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su, H.
    Li 和 R. Yang. Apollocar3d：一个用于自主驾驶的大型 3d 车辆实例理解基准。在 *IEEE/CVF 计算机视觉与模式识别大会*，页码
    5452–5462，2019b。
- en: 'Song et al. (2023) Z. Song, Z. He, X. Li, Q. Ma, R. Ming, Z. Mao, H. Pei, L. Peng,
    J. Hu, D. Yao, et al. Synthetic datasets for autonomous driving: A survey. *arXiv
    preprint arXiv:2304.12205*, 2023.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Song 等人（2023）Z. Song, Z. He, X. Li, Q. Ma, R. Ming, Z. Mao, H. Pei, L. Peng,
    J. Hu, D. Yao 等人. 自主驾驶的合成数据集：综述。*arXiv 预印本 arXiv:2304.12205*，2023。
- en: 'Sun et al. (2020) P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik,
    P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, et al. Scalability in perception
    for autonomous driving: Waymo open dataset. In *IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 2446–2454, 2020.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sun et al. (2020) P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik,
    P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, 等. 自主驾驶中的感知可扩展性：Waymo 开放数据集。在 *IEEE/CVF
    计算机视觉与模式识别会议*，第 2446–2454 页，2020。
- en: Tahir et al. (2021) M. N. Tahir, K. Mäenpää, T. Sukuvaara, and P. Leviäkangas.
    Deployment and analysis of cooperative intelligent transport system pilot service
    alerts in real environment. *IEEE Open Journal of Intelligent Transportation Systems*,
    2:140–148, 2021.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Tahir et al. (2021) M. N. Tahir, K. Mäenpää, T. Sukuvaara, 和 P. Leviäkangas.
    在真实环境中部署和分析协作智能交通系统试点服务警报。*IEEE 智能交通系统开放期刊*，2:140–148，2021。
- en: Triess et al. (2021) L. T. Triess, M. Dreissig, C. B. Rist, and J. M. Zöllner.
    A survey on deep domain adaptation for lidar perception. In *IEEE Intelligent
    Vehicles Symposium Workshops*, pages 350–357\. IEEE, 2021.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Triess et al. (2021) L. T. Triess, M. Dreissig, C. B. Rist, 和 J. M. Zöllner.
    关于激光雷达感知的深度领域适应调查。在 *IEEE 智能车辆研讨会工作坊*，第 350–357 页，2021。
- en: 'Uricar et al. (2021) M. Uricar, G. Sistu, H. Rashed, A. Vobecky, V. R. Kumar,
    P. Krizek, F. Burger, and S. Yogamani. Let’s get dirty: Gan based data augmentation
    for camera lens soiling detection in autonomous driving. In *IEEE/CVF Winter Conference
    on Applications of Computer Vision*, pages 766–775, 2021.'
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Uricar et al. (2021) M. Uricar, G. Sistu, H. Rashed, A. Vobecky, V. R. Kumar,
    P. Krizek, F. Burger, 和 S. Yogamani. 让我们变脏吧：基于 GAN 的数据增强用于自主驾驶中的摄像头镜头污垢检测。在 *IEEE/CVF
    计算机视觉应用冬季会议*，第 766–775 页，2021。
- en: 'Van Brummelen et al. (2018) J. Van Brummelen, M. O’Brien, D. Gruyer, and H. Najjaran.
    Autonomous vehicle perception: The technology of today and tomorrow. *Transportation
    Research Part C: Emerging Technologies*, 89:384–406, 2018.'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Van Brummelen et al. (2018) J. Van Brummelen, M. O’Brien, D. Gruyer, 和 H. Najjaran.
    自主车辆感知：当今和未来的技术。*交通研究 C 部分：新兴技术*，89:384–406，2018。
- en: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. *Advances
    in Neural Information Processing Systems*, 30, 2017.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, 和 I. Polosukhin. 注意力机制即你所需的一切。*神经信息处理系统进展*，30，2017。
- en: 'Wang et al. (2022a) H. Wang, Y. Chen, Y. Cai, L. Chen, Y. Li, M. A. Sotelo,
    and Z. Li. Sfnet-n: An improved sfnet algorithm for semantic segmentation of low-light
    autonomous driving road scenes. *IEEE Transactions on Intelligent Transportation
    Systems*, 23(11):21405–21417, 2022a.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022a) H. Wang, Y. Chen, Y. Cai, L. Chen, Y. Li, M. A. Sotelo,
    和 Z. Li. Sfnet-n：一种改进的 sfnet 算法用于低光自主驾驶道路场景的语义分割。*IEEE 智能交通系统汇刊*，23(11):21405–21417，2022a。
- en: 'Wang et al. (2022b) J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen,
    W. Zeng, and P. Yu. Generalizing to unseen domains: A survey on domain generalization.
    *IEEE Transactions on Knowledge and Data Engineering*, 2022b.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2022b) J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen,
    W. Zeng, 和 P. Yu. 泛化到未见领域：领域泛化的调查。*IEEE 知识与数据工程汇刊*，2022b。
- en: 'Wang and Yoon (2021) L. Wang and K.-J. Yoon. Knowledge distillation and student-teacher
    learning for visual intelligence: A review and new outlooks. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2021.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang and Yoon (2021) L. Wang 和 K.-J. Yoon. 知识蒸馏和学生-教师学习在视觉智能中的应用：回顾与新展望。*IEEE
    模式分析与机器智能汇刊*，2021。
- en: Wang et al. (2019a) Q. Wang, J. Gao, and X. Li. Weakly supervised adversarial
    domain adaptation for semantic segmentation in urban scenes. *IEEE Transactions
    on Image Processing*, 28(9):4376–4386, 2019a.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019a) Q. Wang, J. Gao, 和 X. Li. 用于城市场景语义分割的弱监督对抗性领域适应。*IEEE 图像处理汇刊*，28(9):4376–4386，2019a。
- en: Wang et al. (2021a) Q. Wang, D. Dai, L. Hoyer, L. Van Gool, and O. Fink. Domain
    adaptive semantic segmentation with self-supervised depth estimation. In *IEEE/CVF
    International Conference on Computer Vision*, pages 8515–8525, 2021a.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021a) Q. Wang, D. Dai, L. Hoyer, L. Van Gool, 和 O. Fink. 具有自监督深度估计的领域自适应语义分割。在
    *IEEE/CVF 国际计算机视觉大会*，第 8515–8525 页，2021a。
- en: Wang et al. (2018) T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro.
    High-resolution image synthesis and semantic manipulation with conditional gans.
    In *IEEE Conference on Computer Vision and Pattern Recognition*, pages 8798–8807,
    2018.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2018) T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, 和 B.
    Catanzaro. 使用条件 GAN 的高分辨率图像合成与语义操作。在 *IEEE 计算机视觉与模式识别会议*，第 8798–8807 页，2018。
- en: 'Wang et al. (2019b) Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell,
    and K. Q. Weinberger. Pseudo-lidar from visual depth estimation: Bridging the
    gap in 3d object detection for autonomous driving. In *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pages 8445–8453, 2019b.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2019b) Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell,
    and K. Q. Weinberger. 从视觉深度估计中生成伪激光雷达：弥合自动驾驶3D目标检测的差距。发表于*IEEE/CVF计算机视觉与模式识别会议*，页码8445–8453，2019b年。
- en: 'Wang et al. (2023a) Y. Wang, Q. Mao, H. Zhu, J. Deng, Y. Zhang, J. Ji, H. Li,
    and Y. Zhang. Multi-modal 3d object detection in autonomous driving: a survey.
    *International Journal of Computer Vision*, pages 1–31, 2023a.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023a) Y. Wang, Q. Mao, H. Zhu, J. Deng, Y. Zhang, J. Ji, H. Li,
    and Y. Zhang. 自动驾驶中的多模态3D目标检测：综述。*计算机视觉国际期刊*，页码1–31，2023a年。
- en: 'Wang et al. (2023b) Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, and J. Shen.
    Ssda3d: Semi-supervised domain adaptation for 3d object detection from point cloud.
    In *AAAI Conference on Artificial Intelligence*, volume 37, pages 2707–2715, 2023b.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023b) Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, and J. Shen.
    Ssda3d：用于点云的3D目标检测的半监督领域适应。发表于*AAAI人工智能会议*，第37卷，页码2707–2715，2023b年。
- en: 'Wang et al. (2020) Z. Wang, Y. Wei, R. Feris, J. Xiong, W.-M. Hwu, T. S. Huang,
    and H. Shi. Alleviating semantic-level shift: A semi-supervised domain adaptation
    method for semantic segmentation. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition Workshops*, pages 936–937, 2020.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2020) Z. Wang, Y. Wei, R. Feris, J. Xiong, W.-M. Hwu, T. S. Huang,
    and H. Shi. 缓解语义级别的偏移：一种用于语义分割的半监督领域适应方法。发表于*IEEE/CVF计算机视觉与模式识别会议研讨会*，页码936–937，2020年。
- en: Wang et al. (2021b) Z. Wang, Y. Luo, R. Qiu, Z. Huang, and M. Baktashmotlagh.
    Learning to diversify for single domain generalization. In *IEEE/CVF International
    Conference on Computer Vision*, pages 834–843, 2021b.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2021b) Z. Wang, Y. Luo, R. Qiu, Z. Huang, and M. Baktashmotlagh.
    学习多样化以实现单领域泛化。发表于*IEEE/CVF国际计算机视觉会议*，页码834–843，2021b年。
- en: 'Wang et al. (2023c) Z. Wang, C. Lv, and F.-Y. Wang. A new era of intelligent
    vehicles and intelligent transportation systems: Digital twins and parallel intelligence.
    *IEEE Transactions on Intelligent Vehicles*, 2023c.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang et al. (2023c) Z. Wang, C. Lv, and F.-Y. Wang. 智能车辆与智能交通系统的新时代：数字双胞胎与并行智能。*IEEE智能车辆汇刊*，2023c年。
- en: 'Wen and Jo (2022) L.-H. Wen and K.-H. Jo. Deep learning-based perception systems
    for autonomous driving: A comprehensive survey. *Neurocomputing*, 2022.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wen and Jo (2022) L.-H. Wen and K.-H. Jo. 基于深度学习的自动驾驶感知系统：全面综述。*神经计算*，2022年。
- en: Wilson and Cook (2020) G. Wilson and D. J. Cook. A survey of unsupervised deep
    domain adaptation. *ACM Transactions on Intelligent Systems and Technology*, 11(5):1–46,
    2020.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wilson and Cook (2020) G. Wilson and D. J. Cook. 无监督深度领域适应的综述。*ACM智能系统与技术汇刊*，第11卷第5期：1–46，2020年。
- en: Wu and Deng (2022) A. Wu and C. Deng. Single-domain generalized object detection
    in urban scene via cyclic-disentangled self-distillation. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 847–856, 2022.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu and Deng (2022) A. Wu and C. Deng. 通过循环解缠自蒸馏进行城市场景中的单领域广义目标检测。发表于*IEEE/CVF计算机视觉与模式识别会议*，页码847–856，2022年。
- en: 'Wu et al. (2019) B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer. Squeezesegv2:
    Improved model structure and unsupervised domain adaptation for road-object segmentation
    from a lidar point cloud. In *International Conference on Robotics and Automation*,
    pages 4376–4382\. IEEE, 2019.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2019) B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer. Squeezesegv2：改进的模型结构和无监督领域适应用于从激光雷达点云中进行道路目标分割。发表于*国际机器人与自动化会议*，页码4376–4382。IEEE，2019年。
- en: 'Wu et al. (2021) X. Wu, Z. Wu, H. Guo, L. Ju, and S. Wang. Dannet: A one-stage
    domain adaptation network for unsupervised nighttime semantic segmentation. In
    *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 15769–15778,
    2021.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu et al. (2021) X. Wu, Z. Wu, H. Guo, L. Ju, and S. Wang. Dannet：一种用于无监督夜间语义分割的单阶段领域适应网络。发表于*IEEE/CVF计算机视觉与模式识别会议*，页码15769–15778，2021年。
- en: Xie and Du (2022) P. Xie and X. Du. Performance-aware mutual knowledge distillation
    for improving neural architecture search. In *IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 11922–11932, 2022.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xie and Du (2022) P. Xie and X. Du. 性能感知的互知识蒸馏以改进神经结构搜索。发表于*IEEE/CVF计算机视觉与模式识别会议*，页码11922–11932，2022年。
- en: Xu et al. (2019) J. Xu, Y. Nie, P. Wang, and A. M. López. Training a binary
    weight object detector by knowledge transfer for autonomous driving. In *International
    Conference on Robotics and Automation*, pages 2379–2384\. IEEE, 2019.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2019) J. Xu, Y. Nie, P. Wang, 和 A. M. López. 通过知识转移训练二进制权重目标检测器用于自动驾驶。收录于
    *国际机器人与自动化会议*，页码 2379–2384\. IEEE，2019。
- en: 'Xu et al. (2021) Q. Xu, Y. Zhou, W. Wang, C. R. Qi, and D. Anguelov. Spg: Unsupervised
    domain adaptation for 3d object detection via semantic point generation. In *IEEE/CVF
    International Conference on Computer Vision*, pages 15446–15456, 2021.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2021) Q. Xu, Y. Zhou, W. Wang, C. R. Qi, 和 D. Anguelov. Spg: 通过语义点生成进行
    3D 目标检测的无监督领域适配。收录于 *IEEE/CVF 国际计算机视觉会议*，页码 15446–15456，2021。'
- en: 'Xu et al. (2022a) Q. Xu, L. Yao, Z. Jiang, G. Jiang, W. Chu, W. Han, W. Zhang,
    C. Wang, and Y. Tai. Dirl: Domain-invariant representation learning for generalizable
    semantic segmentation. In *AAAI Conference on Artificial Intelligence*, volume 36,
    pages 2884–2892, 2022a.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2022a) Q. Xu, L. Yao, Z. Jiang, G. Jiang, W. Chu, W. Han, W. Zhang,
    C. Wang, 和 Y. Tai. Dirl: 用于可泛化语义分割的领域不变表示学习。收录于 *AAAI 人工智能会议*，卷 36，页码 2884–2892，2022a。'
- en: 'Xu et al. (2022b) R. Xu, H. Xiang, X. Xia, X. Han, J. Li, and J. Ma. Opv2v:
    An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle
    communication. In *International Conference on Robotics and Automation*, pages
    2583–2589\. IEEE, 2022b.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2022b) R. Xu, H. Xiang, X. Xia, X. Han, J. Li, 和 J. Ma. Opv2v: 一个用于车与车通信感知的开放基准数据集和融合管道。收录于
    *国际机器人与自动化会议*，页码 2583–2589\. IEEE，2022b。'
- en: Xu et al. (2023a) R. Xu, J. Li, X. Dong, H. Yu, and J. Ma. Bridging the domain
    gap for multi-agent perception. *IEEE International Conference on Robotics and
    Automation*, 2023a.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xu et al. (2023a) R. Xu, J. Li, X. Dong, H. Yu, 和 J. Ma. 为多智能体感知弥合领域差距。*IEEE
    国际机器人与自动化会议*，2023a。
- en: 'Xu et al. (2023b) R. Xu, X. Xia, J. Li, H. Li, S. Zhang, Z. Tu, Z. Meng, H. Xiang,
    X. Dong, R. Song, et al. V2v4real: A real-world large-scale dataset for vehicle-to-vehicle
    cooperative perception. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 13712–13722, 2023b.'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Xu et al. (2023b) R. Xu, X. Xia, J. Li, H. Li, S. Zhang, Z. Tu, Z. Meng, H.
    Xiang, X. Dong, R. Song, 等. V2v4real: 一个用于车与车协作感知的真实大规模数据集。收录于 *IEEE/CVF 计算机视觉与模式识别会议*，页码
    13712–13722，2023b。'
- en: 'Yan et al. (2018) Y. Yan, Y. Mao, and B. Li. Second: Sparsely embedded convolutional
    detection. *Sensors*, 18(10):3337, 2018.'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yan et al. (2018) Y. Yan, Y. Mao, 和 B. Li. Second: 稀疏嵌入卷积检测。*传感器*，18(10):3337，2018。'
- en: Yang et al. (2022) C. Yang, H. Zhou, Z. An, X. Jiang, Y. Xu, and Q. Zhang. Cross-image
    relational knowledge distillation for semantic segmentation. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 12319–12328, 2022.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang et al. (2022) C. Yang, H. Zhou, Z. An, X. Jiang, Y. Xu, 和 Q. Zhang. 跨图像关系知识蒸馏用于语义分割。收录于
    *IEEE/CVF 计算机视觉与模式识别会议*，页码 12319–12328，2022。
- en: 'Yang et al. (2020) Z. Yang, Y. Chai, D. Anguelov, Y. Zhou, P. Sun, D. Erhan,
    S. Rafferty, and H. Kretzschmar. Surfelgan: Synthesizing realistic sensor data
    for autonomous driving. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 11118–11127, 2020.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yang et al. (2020) Z. Yang, Y. Chai, D. Anguelov, Y. Zhou, P. Sun, D. Erhan,
    S. Rafferty, 和 H. Kretzschmar. Surfelgan: 为自动驾驶合成真实感传感器数据。收录于 *IEEE/CVF 计算机视觉与模式识别会议*，页码
    11118–11127，2020。'
- en: 'Ye et al. (2021) L. Ye, Z. Wang, X. Chen, J. Wang, K. Wu, and K. Lu. Gsan:
    Graph self-attention network for learning spatial–temporal interaction representation
    in autonomous driving. *IEEE Internet of Things Journal*, 9(12):9190–9204, 2021.'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Ye et al. (2021) L. Ye, Z. Wang, X. Chen, J. Wang, K. Wu, 和 K. Lu. Gsan: 用于学习自动驾驶中的空间–时间交互表示的图自注意网络。*IEEE
    物联网期刊*，9(12):9190–9204，2021。'
- en: 'Yeong et al. (2021) D. J. Yeong, G. Velasco-Hernandez, J. Barry, and J. Walsh.
    Sensor and sensor fusion technology in autonomous vehicles: A review. *Sensors*,
    21(6):2140, 2021.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yeong et al. (2021) D. J. Yeong, G. Velasco-Hernandez, J. Barry, 和 J. Walsh.
    自动驾驶中的传感器与传感器融合技术：综述。*传感器*，21(6):2140，2021。
- en: 'Yi et al. (2021) L. Yi, B. Gong, and T. Funkhouser. Complete & label: A domain
    adaptation approach to semantic segmentation of lidar point clouds. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pages 15363–15373, 2021.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yi et al. (2021) L. Yi, B. Gong, 和 T. Funkhouser. Complete & label: 一种用于激光雷达点云语义分割的领域适配方法。收录于
    *IEEE/CVF 计算机视觉与模式识别会议*，页码 15363–15373，2021。'
- en: 'Yu et al. (2018) F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and
    T. Darrell. Bdd100k: A diverse driving video database with scalable annotation
    tooling. *arXiv preprint arXiv:1805.04687*, 2(5):6, 2018.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Yu et al. (2018) F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, 和 T.
    Darrell. Bdd100k: 一个具有可扩展注释工具的多样化驾驶视频数据库。*arXiv 预印本 arXiv:1805.04687*，2(5):6，2018。'
- en: Yu et al. (2022) G. Yu, H. Li, Y. Wang, P. Chen, and B. Zhou. A review on cooperative
    perception and control supported infrastructure-vehicle system. *Green Energy
    and Intelligent Transportation*, page 100023, 2022.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yu et al. (2022) G. Yu, H. Li, Y. Wang, P. Chen, 和 B. Zhou. 支持基础设施-车辆系统的协同感知与控制综述。*绿色能源与智能交通*，页码100023，2022。
- en: 'Yue et al. (2019) X. Yue, Y. Zhang, S. Zhao, A. Sangiovanni-Vincentelli, K. Keutzer,
    and B. Gong. Domain randomization and pyramid consistency: Simulation-to-real
    generalization without accessing target domain data. In *IEEE/CVF International
    Conference on Computer Vision*, pages 2100–2110, 2019.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yue et al. (2019) X. Yue, Y. Zhang, S. Zhao, A. Sangiovanni-Vincentelli, K.
    Keutzer, 和 B. Gong. 域随机化与金字塔一致性：无需访问目标领域数据的模拟到现实泛化。在*IEEE/CVF 国际计算机视觉会议*上，页码2100–2110，2019。
- en: 'Yurtsever et al. (2020) E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda.
    A survey of autonomous driving: Common practices and emerging technologies. *IEEE
    Access*, 8:58443–58469, 2020.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yurtsever et al. (2020) E. Yurtsever, J. Lambert, A. Carballo, 和 K. Takeda.
    自动驾驶的调查：常见实践与新兴技术。*IEEE Access*，8:58443–58469，2020。
- en: 'Zagoruyko and Komodakis (2016) S. Zagoruyko and N. Komodakis. Paying more attention
    to attention: Improving the performance of convolutional neural networks via attention
    transfer. In *International Conference on Learning Representations*, 2016.'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zagoruyko 和 Komodakis (2016) S. Zagoruyko 和 N. Komodakis. 关注更多的关注：通过注意力转移提高卷积神经网络的性能。在*国际学习表征会议*，2016。
- en: 'Zhang et al. (2021a) J. Zhang, C. Ma, K. Yang, A. Roitberg, K. Peng, and R. Stiefelhagen.
    Transfer beyond the field of view: Dense panoramic semantic segmentation via unsupervised
    domain adaptation. *IEEE Transactions on Intelligent Transportation Systems*,
    23(7):9478–9491, 2021a.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021a) J. Zhang, C. Ma, K. Yang, A. Roitberg, K. Peng, 和 R. Stiefelhagen.
    超越视野范围的转移：通过无监督领域适应实现密集全景语义分割。*IEEE 智能交通系统汇刊*，23(7):9478–9491，2021a。
- en: Zhang et al. (2020) W. Zhang, Z. Wang, and C. C. Loy. Exploring data augmentation
    for multi-modality 3d object detection. *arXiv preprint arXiv:2012.12741*, 2020.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2020) W. Zhang, Z. Wang, 和 C. C. Loy. 探索多模态3D物体检测的数据增强。*arXiv
    预印本 arXiv:2012.12741*，2020。
- en: Zhang et al. (2021b) X. Zhang, H. Zhang, J. Lu, L. Shao, and J. Yang. Target-targeted
    domain adaptation for unsupervised semantic segmentation. In *IEEE International
    Conference on Robotics and Automation*, pages 13560–13566\. IEEE, 2021b.
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2021b) X. Zhang, H. Zhang, J. Lu, L. Shao, 和 J. Yang. 针对目标的无监督语义分割领域适应。在*IEEE
    国际机器人与自动化会议*上，页码13560–13566。IEEE，2021b。
- en: 'Zhang et al. (2022) X. Zhang, N. Tseng, A. Syed, R. Bhasin, and N. Jaipuria.
    Simbar: Single image-based scene relighting for effective data augmentation for
    automated driving vision tasks. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pages 3718–3728, 2022.'
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2022) X. Zhang, N. Tseng, A. Syed, R. Bhasin, 和 N. Jaipuria. Simbar：基于单图像的场景重光照，用于自动驾驶视觉任务的有效数据增强。在*IEEE/CVF
    计算机视觉与模式识别会议*上，页码3718–3728，2022。
- en: Zhang et al. (2018) Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. Deep mutual
    learning. In *IEEE Conference on Computer Vision and Pattern Recognition*, pages
    4320–4328, 2018.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang et al. (2018) Y. Zhang, T. Xiang, T. M. Hospedales, 和 H. Lu. 深度互学习。在*IEEE
    计算机视觉与模式识别会议*上，页码4320–4328，2018。
- en: Zhao et al. (2020) X. Zhao, P. Sun, Z. Xu, H. Min, and H. Yu. Fusion of 3d lidar
    and camera data for object detection in autonomous vehicle applications. *IEEE
    Sensors Journal*, 20(9):4901–4913, 2020.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2020) X. Zhao, P. Sun, Z. Xu, H. Min, 和 H. Yu. 3D 激光雷达与相机数据的融合，用于自动驾驶应用中的物体检测。*IEEE
    传感器杂志*，20(9):4901–4913，2020。
- en: Zhao et al. (2022) Y. Zhao, Z. Zhong, N. Zhao, N. Sebe, and G. H. Lee. Style-hallucinated
    dual consistency learning for domain generalized semantic segmentation. In *Computer
    Vision–17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXVIII*, pages 535–552\. Springer, 2022.
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhao et al. (2022) Y. Zhao, Z. Zhong, N. Zhao, N. Sebe, 和 G. H. Lee. 风格幻觉双一致性学习用于领域泛化语义分割。在*计算机视觉–第17届欧洲会议，特拉维夫，以色列，2022年10月23日至27日，论文集，第XXVIII部分*，页码535–552。Springer，2022。
- en: Zhou et al. (2016) B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba.
    Learning deep features for discriminative localization. In *IEEE Conference on
    Computer Vision and Pattern Recognition*, pages 2921–2929, 2016.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou et al. (2016) B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, 和 A. Torralba.
    学习深度特征用于判别定位。在*IEEE 计算机视觉与模式识别会议*上，页码2921–2929，2016。
- en: Zhou et al. (2019) D. Zhou, Z. Ma, and J. Sun. Autonomous vehicles’ turning
    motion planning for conflict areas at mixed-flow intersections. *IEEE Transactions
    on Intelligent Vehicles*, 5(2):204–216, 2019.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2019）D. Zhou, Z. Ma, 和 J. Sun. 自动驾驶车辆在混合流量交叉口冲突区域的转向运动规划。*IEEE 智能车辆学报*，5(2)：204–216，2019。
- en: Zhou et al. (2020a) D. Zhou, J. Fang, X. Song, L. Liu, J. Yin, Y. Dai, H. Li,
    and R. Yang. Joint 3d instance segmentation and object detection for autonomous
    driving. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 1839–1849, 2020a.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2020a）D. Zhou, J. Fang, X. Song, L. Liu, J. Yin, Y. Dai, H. Li, 和 R.
    Yang. 联合 3D 实例分割与目标检测用于自动驾驶。见于 *IEEE/CVF 计算机视觉与模式识别大会*，第 1839–1849 页，2020a。
- en: 'Zhou et al. (2022a) K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy. Domain
    generalization: A survey. *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2022a.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2022a）K. Zhou, Z. Liu, Y. Qiao, T. Xiang, 和 C. C. Loy. 领域泛化：综述。*IEEE
    模式分析与机器智能学报*，2022a。
- en: 'Zhou et al. (2022b) Y. Zhou, L. Liu, H. Zhao, M. López-Benítez, L. Yu, and
    Y. Yue. Towards deep radar perception for autonomous driving: Datasets, methods,
    and challenges. *Sensors*, 22(11):4208, 2022b.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2022b）Y. Zhou, L. Liu, H. Zhao, M. López-Benítez, L. Yu, 和 Y. Yue. 面向自动驾驶的深度雷达感知：数据集、方法与挑战。*传感器*，22(11)：4208，2022b。
- en: Zhou et al. (2020b) Z. Zhou, Z. Wang, H. Lu, S. Wang, and M. Sun. Multi-type
    self-attention guided degraded saliency detection. In *AAAI Conference on Artificial
    Intelligence*, volume 34, pages 13082–13089, 2020b.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等（2020b）Z. Zhou, Z. Wang, H. Lu, S. Wang, 和 M. Sun. 多类型自注意力引导的退化显著性检测。见于
    *AAAI 人工智能大会*，第 34 卷，第 13082–13089 页，2020b。
- en: Zhu et al. (2017) J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image
    translation using cycle-consistent adversarial networks. In *IEEE International
    Conference on Computer Vision*, pages 2223–2232, 2017.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhu 等（2017）J.-Y. Zhu, T. Park, P. Isola, 和 A. A. Efros. 使用循环一致对抗网络的无配对图像到图像转换。见于
    *IEEE 国际计算机视觉大会*，第 2223–2232 页，2017。
- en: Zhuang et al. (2020) F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong,
    and Q. He. A comprehensive survey on transfer learning. *IEEE*, 109(1):43–76,
    2020.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhuang 等（2020）F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, 和
    Q. He. 转移学习的综合综述。*IEEE*，109(1)：43–76，2020。
- en: Ziegler and Asano (2022) A. Ziegler and Y. M. Asano. Self-supervised learning
    of object parts for semantic segmentation. In *IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 14502–14511, 2022.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ziegler 和 Asano（2022）A. Ziegler 和 Y. M. Asano. 自监督学习的对象部分用于语义分割。见于 *IEEE/CVF
    计算机视觉与模式识别大会*，第 14502–14511 页，2022。
