- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:38:41'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2306.15110] Deep Transfer Learning for Intelligent Vehicle Perception: a Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2306.15110](https://ar5iv.labs.arxiv.org/html/2306.15110)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Deep Transfer Learning for Intelligent Vehicle Perception: a Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Xinyu Liu¹, Jinlong Li¹, Jin Ma¹, Huiming Sun¹, Zhigang Xu²,
  prefs: []
  type: TYPE_NORMAL
- en: Tianyun Zhang¹, Hongkai Yu¹ ¹ Department of Electrical Engineering and Computer
    Science, Cleveland State University, Cleveland, OH 44115, USA
  prefs: []
  type: TYPE_NORMAL
- en: ² School of Information Engineering, Chang’an University, Xi’an 710064, China
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Deep learning-based intelligent vehicle perception has been developing prominently
    in recent years to provide a reliable source for motion planning and decision
    making in autonomous driving. A large number of powerful deep learning-based methods
    can achieve excellent performance in solving various perception problems of autonomous
    driving. However, these deep learning methods still have several limitations,
    for example, the assumption that lab-training (source domain) and real-testing
    (target domain) data follow the same feature distribution may not be practical
    in the real world. There is often a dramatic domain gap between them in many real-world
    cases. As a solution to this challenge, deep transfer learning can handle situations
    excellently by transferring the knowledge from one domain to another. Deep transfer
    learning aims to improve task performance in a new domain by leveraging the knowledge
    of similar tasks learned in another domain before. Nevertheless, there are currently
    no survey papers on the topic of deep transfer learning for intelligent vehicle
    perception. To the best of our knowledge, this paper represents the first comprehensive
    survey on the topic of the deep transfer learning for intelligent vehicle perception.
    This paper discusses the domain gaps related to the differences of sensor, data,
    and model for the intelligent vehicle perception. The recent applications, challenges,
    future researches in intelligent vehicle perception are also explored.
  prefs: []
  type: TYPE_NORMAL
- en: 'keywords:'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'deep transfer learning, domain gap, intelligent vehicle perception, autonomous
    driving^†^†journal: Green Energy and Intelligent Transportation'
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In recent years, perception has been viewed as a critical component in intelligent
    vehicles for precise localization, safe motion planning, and robust control Li
    et al. ([2020a](#bib.bib69)), Yurtsever et al. ([2020](#bib.bib171)), Huang and
    Chen ([2020](#bib.bib56)). The perception system provides intelligent vehicles
    with immediate environmental information about surrounding pedestrians, vehicles,
    traffic signs, and other items and helps to avoid possible collisions. In this
    paper, the terminology “perception” is mainly focused on the detection and segmentation
    tasks by ignoring the tracking and trajectory prediction tasks. This focus is
    because of two reasons: (1) Some previous intelligent vehicle research works mainly
    use detection and/or segmentation tasks to describe the terminology “perception”
    for intelligent vehicles Van Brummelen et al. ([2018](#bib.bib136)), Xu et al.
    ([2022b](#bib.bib159)); (2) Many downstream tasks, such as tracking, trajectory
    prediction, and behavior prediction, are dependent on the accurate detection and/or
    segmentation first.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The perception tasks play an indispensable role in intelligent vehicles and
    autonomous driving Arnold et al. ([2019](#bib.bib5)). Recently, the deep learning
    methods have gained significant traction in the intelligent vehicle perception
    and have achieved great successes Grigorescu et al. ([2020](#bib.bib42)), Wen
    and Jo ([2022](#bib.bib150)), Chen et al. ([2022](#bib.bib18)). For example, as
    shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Transfer Learning for
    Intelligent Vehicle Perception: a Survey"), four important applications have been
    actively studied and have gained significant advancements in recent years. (1)
    2D object detection is a prominent task which aims at recognizing and localizing
    objects within the images or videos. The main goal is to develop algorithms or
    models that can automatically detect objects and accurately outline their boundaries.
    (2) 3D object detection is a crucial task that focuses on discerning and precisely
    localizing objects within a three-dimensional space. 3D detection includes estimating
    the position of an object and orientation within a comprehensive 3D coordinate
    system. (3) Semantic segmentation is a pixel-level image analysis task where each
    pixel in an image is assigned a semantic label. The goal is to partition the image
    into coherent regions or segments based on the objects or classes they belong
    to. (4) Instance segmentation combines the object detection and semantic segmentation
    task. The objective is to detect and segment individual objects within an image,
    providing a unique label and pixel-level mask to each instance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'However, as shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Transfer
    Learning for Intelligent Vehicle Perception: a Survey"), there are lots of complex
    cases where the deep learning methods might fail in the real world. For example,
    a deep learning based vehicle detection model pre-trained on clear weather data
    might be then tested in the foggy weather, low-illumination (like night), occlusion,
    or different data source (like simulation) conditions, leading to a large performance
    drop. This degradation is influenced by the domain gap (shift) between diverse
    driving environments Hnewa and Radha ([2020](#bib.bib48)), Mirza et al. ([2022](#bib.bib98)),
    Mohammed et al. ([2020](#bib.bib100)), e.g., different weather, illumination,
    occlusion, data source conditions. Moreover, different types and settings of the
    sensors Rist et al. ([2019](#bib.bib115)) installed on vehicles and various deep
    learning model structures Xu et al. ([2023a](#bib.bib160)) Khalil and Mouftah
    ([2022](#bib.bib60)) during the Vehicle-to-Vehicle (V2V) cooperative perception
    might result in the domain gap as well.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/3e463d3f80547b14a213941ac1a2fd66.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Illustration of Challenges and Applications of Intelligent Vehicle
    Perception. Transfer Learning (TL) methods can be applied to reduce the domain
    gaps by sensor difference, data difference, and model difference.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The above mentioned performance drop for intelligent vehicle perception because
    of the domain gap can be relieved via the Transfer Learning (TL) Zhuang et al.
    ([2020](#bib.bib187)) methods. The TL techniques include two goals: (1) fully
    using the prior knowledge obtained from the source domain to guide the inference
    in the related target domain, (2) largely reducing the feature distribution discrepancy
    caused by the domain gap. Due to these two goals, the performance for deep learning
    based intelligent vehicle perception systems in related but different domains
    can be enhanced. The deep learning model’s generalization capability can be improved
    for the intelligent vehicle perception under different challenging scenarios as
    shown in Fig. [1](#S1.F1 "Figure 1 ‣ 1 Introduction ‣ Deep Transfer Learning for
    Intelligent Vehicle Perception: a Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we focus on the transfer learning methods for the intelligent
    vehicle perception in the deep learning era. This paper first reviews the related
    tasks and benchmark datasets for intelligent vehicle perception, and then classifies
    the domain gaps to three differences of sensor, data, and model during the vehicle
    driving. Next, we carefully review about 150 related published papers of deep
    transfer learning since the deep learning research is started, then we classify
    the deep learning based transfer learning methods into four types: (1) Supervised
    TL, (2) Unsupervised TL, (3) Weakly-and-semi Supervised TL, and (4) Domain Generalization.
    For the first three types, the transfer learning is implemented from one source
    domain to one target domain, where our classification depends on whether the target
    domain has labeled data or not. For the last type, the transfer learning is conducted
    from one source domain to multiple target domains for the generalization in many
    seen or unseen driving scenarios. In addition, several subdivisions of each type
    of transfer learning methods are also reviewed and analyzed in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: The contributions of this paper can be outlined as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To the best of our knowledge, this paper is the first in-depth survey on the
    topic of the deep transfer learning for intelligent vehicle perception.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This paper summarizes the domain gap for intelligent vehicle perception into
    three types (differences of sensor, data, model) and gives detailed explanations
    to the related tasks and benchmark datasets.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After reviewing about 150 related published papers, we classify the deep transfer
    learning methods for intelligent vehicle perception into four types and explain
    each of them in details.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The subsequent sections of this paper are structured as follows. Section [2](#S2
    "2 Intelligent Vehicle Perception ‣ Deep Transfer Learning for Intelligent Vehicle
    Perception: a Survey") provides a general overview about the related tasks and
    benchmark datasets for intelligent vehicle perception. Section [3](#S3 "3 Domain
    Distribution Discrepancy ‣ Deep Transfer Learning for Intelligent Vehicle Perception:
    a Survey") presents the domain distribution discrepancy and the three kinds of
    domain gap. Section [4](#S4 "4 Deep Transfer Learning Methodology ‣ Deep Transfer
    Learning for Intelligent Vehicle Perception: a Survey") details the different
    methodologies of the deep transfer learning techniques. Section [5](#S5 "5 Challenges
    and Future Research ‣ Deep Transfer Learning for Intelligent Vehicle Perception:
    a Survey") explains the challenges and future research, followed by a conclusion
    in Section [6](#S6 "6 Conclusion ‣ Deep Transfer Learning for Intelligent Vehicle
    Perception: a Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 2 Intelligent Vehicle Perception
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For intelligent vehicles or autonomous driving, perception plays a crucial role
    in receiving data from sensors and extracting meaningful information from the
    surrounding environment, so as to make meaningful decisions for the precise motion
    planning by identifying obstacles, traffic signs/markers, and available driving
    areas Li et al. ([2023e](#bib.bib81)). Two types of mainstream sensors (Camera,
    LiDAR) are widely used in self-driving or intelligent driving vehicles Cao et al.
    ([2019](#bib.bib14)) Fadadu et al. ([2022](#bib.bib28)) Liu et al. ([2021b](#bib.bib88)) Liu
    et al. ([2022b](#bib.bib89)) Gholamhosseinian and Seitz ([2021](#bib.bib37)),
    Yu et al. ([2022](#bib.bib169)). These sensors installed on vehicles are utilized
    for the intelligent vehicle perception tasks.
  prefs: []
  type: TYPE_NORMAL
- en: The intelligent vehicle perception tasks include discovering the surrounding
    vehicles and pedestrians, recognizing traffic signs and markers, finding the driving
    areas (e.g., road regions), and so on. In the real world, sometimes the objects
    may be similar to each other or the background, and the challenging scenarios
    (e.g., diverse weather, dark illumination) might affect the performance of sensors,
    making the perception tasks even more difficult Hnewa and Radha ([2020](#bib.bib48)),
    Li et al. ([2023c](#bib.bib75)). This paper groups these intelligent vehicle perception
    tasks into two classes (Object Detection, Semantic/Instance Segmentation) and
    further discusses these challenges for intelligent vehicle perception in the real
    world.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1 Object Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To achieve autonomous driving safely and successfully, it is necessary to have
    a reliable object detection system. Considering the complex road conditions, it
    is essential to detect (localize and recognize) other vehicles, pedestrians, and
    obstacles to prevent potential accidents. However, detecting objects in urban
    areas is challenging due to the diverse types of objects and unknown road situations Arnold
    et al. ([2019](#bib.bib5)), Feng et al. ([2020](#bib.bib29)).
  prefs: []
  type: TYPE_NORMAL
- en: '2D Object Detection: By only using the relatively cheap camera sensor(s), deep
    learning models can be easily applied to efficiently detect (localize and recognize)
    the surrounding objects from the 2D image data Yeong et al. ([2021](#bib.bib166)).
    The output will be the identified 2D bounding boxes (2D coordinates) with the
    recognized object classes for the surrounding objects on each camera image, with
    a real-time or near real-time inference speed. However, 2D object detection alone
    can only provide the object’s position on a 2D plane, which does not provide enough
    information Wang et al. ([2019b](#bib.bib144)), e.g., object depth, object 3D
    size.'
  prefs: []
  type: TYPE_NORMAL
- en: '3D Object Detection: Considering the limitations of 2D object detection, the
    object 3D information might equip the intelligent vehicle with the capability
    to more robustly and accurately perceive and recognize surrounding objects. The
    output will be the identified 3D bounding boxes (3D coordinates) with the recognized
    object classes for the surrounding objects, with a reasonable inference time.
    Because the images of camera sensors and the point clouds of LiDAR sensors could
    provide the depth cues, the 3D object detection task could be achieved via three
    sensor settings: (1) Camera only Wang et al. ([2023a](#bib.bib145)), (2) LiDAR
    only Xu et al. ([2023b](#bib.bib161)) Xu et al. ([2022b](#bib.bib159)) Li et al.
    ([2023a](#bib.bib73)), (3) Camera + LiDAR Zhao et al. ([2020](#bib.bib178)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2.2 Semantic/Instance Segmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Different with the object detection task, the segmentation task not only discovers
    the object regions but also give the pixel-level labels (masks) for everything
    (object and background) in the driving scenarios. For the intelligent vehicle
    perception, the segmentation task can be classified into two types: Semantic Segmentation,
    Instance Segmentation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Semantic Segmentation: Semantic segmentation involves the assignment of a semantic
    label to every pixel within an image, such as “road”, “vehicle” or “pedestrian”,
    “traffic sign”, and so on. This technique enables the intelligent vehicle to perceive
    the surrounding environment and understand the scene more comprehensively Feng
    et al. ([2020](#bib.bib29)), Mo et al. ([2022](#bib.bib99)). The identification
    of specific regions within an image can aid the self-driving vehicles in making
    informed decisions, e.g., determining where the driving road region is.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Instance Segmentation: Instance segmentation outputs the boundaries (pixel-level
    masks) of each object and assigns a unique label to each discovered object Zhou
    et al. ([2020a](#bib.bib182)), which seems like a integration of object detection
    and semantic segmentation. It is particularly useful for identifying the shape,
    location, and number of surrounding objects in autonomous driving Rashed et al.
    ([2021](#bib.bib113)), Ko et al. ([2021](#bib.bib62)).'
  prefs: []
  type: TYPE_NORMAL
- en: Semantic segmentation captures the overall scene structure, while instance segmentation
    enables a more fine-grained understanding of objects and their boundaries. Different
    from semantic segmentation which only classifies images into meaningful semantic
    regions, instance segmentation provides more precise analysis by supplying a separate
    semantic mask (with identity) for every object instance.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3 Benchmark Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There might be many different sensors installed on the intelligent vehicles,
    such as Camera, LiDAR, Radar, Near-Infrared Sensors, Ultrasonic Sensors, and so
    on. For a clear description, we focus on introducing the two widely-used main
    sensors (Camera and LiDAR) in this paper, related to the image data by Camera
    and the point cloud data by LiDAR on the intelligent vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: 'Camera data: The 3-channel color images in Green, Red, Blue primary colors
    of light (i.e., RGB images) are commonly acquired by monocular or multiple cameras,
    which are simple and reliable sensors that closely resemble human eyes Feng et al.
    ([2020](#bib.bib29)). One of the main benefits of RGB cameras is their high resolution
    and relatively low cost. However, their performance can deteriorate significantly
    under the challenging weather and illumination conditions Feng et al. ([2021](#bib.bib30)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'LiDAR data: Unlike cameras, laser sensors offer direct and precise 3D information,
    making it easier to extract object candidates and aiding in the classification
    task by providing 3D shape information. LiDAR, also known as light detection and
    ranging, is a sensor technology that is capable of detecting targets in all lighting
    conditions and creating a distance map of the targets with high spatial coverage Li
    and Ibanez-Guzman ([2020](#bib.bib76)), Li et al. ([2020b](#bib.bib77)). LiDAR
    could work in some challenging weather and dark illumination scenarios, but it
    is quite expensive with high cost. Its high cost is a major obstacle to wider
    adoption Li et al. ([2020b](#bib.bib77)) Pham et al. ([2020](#bib.bib109)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark for 2D Object Detection: KITTI Geiger et al. ([2013](#bib.bib35)),
    Cityscapes Cordts et al. ([2016](#bib.bib25)), SIM10k Johnson-Roberson et al.
    ([2017](#bib.bib59)), Foggy Cityscapes Sakaridis et al. ([2018](#bib.bib118)),
    Syn2Real-D Peng et al. ([2018](#bib.bib108)), BDD100k Yu et al. ([2018](#bib.bib168)),
    GTA5 Richter et al. ([2016](#bib.bib114)), nuScenes Caesar et al. ([2020](#bib.bib13)),
    Waymo Open Sun et al. ([2020](#bib.bib132)), A*3D Pham et al. ([2020](#bib.bib109)),
    ApolloScape Huang et al. ([2018](#bib.bib55)), Ford Agarwal et al. ([2020](#bib.bib2)),
    A2D2 Geyer et al. ([2020](#bib.bib36)), ONCE Mao et al. ([2021](#bib.bib95)),
    and Automine Li et al. ([2022c](#bib.bib78)). Let us give two examples: (1) KITTI Geiger
    et al. ([2013](#bib.bib35)) is a widely utilized dataset for autonomous driving,
    encompassing camera images, LiDAR point clouds, and ground-truth 3D bounding boxes.
    It comprises 7,481 training frames and 7,518 test frames, accompanied by sensor
    calibration data and annotated 3D bounding boxes around objects of interest (2)
    SIM10k Johnson-Roberson et al. ([2017](#bib.bib59)) is a synthetic dataset derived
    from the Grand Theft Auto V (GTA-V) computer game. It contains 10,000 images capturing
    driving street scenes, accompanied by bounding box annotations specifically for
    cars.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark for 3D Object Detection: KITTI Geiger et al. ([2013](#bib.bib35)),
    Cityscapes Cordts et al. ([2016](#bib.bib25)), Foggy Cityscapes Sakaridis et al.
    ([2018](#bib.bib118)), GTA5-LiDAR Wu et al. ([2019](#bib.bib153)), nuScenes Caesar
    et al. ([2020](#bib.bib13)), Waymo Open Sun et al. ([2020](#bib.bib132)), A*3D Pham
    et al. ([2020](#bib.bib109)), ApolloScape Huang et al. ([2018](#bib.bib55)), Ford Agarwal
    et al. ([2020](#bib.bib2)), A2D2 Geyer et al. ([2020](#bib.bib36)), ONCE Mao et al.
    ([2021](#bib.bib95)), Automine Li et al. ([2022c](#bib.bib78)), OPV2V Xu et al.
    ([2022b](#bib.bib159)) and V2V4Real Xu et al. ([2023b](#bib.bib161)). Here we
    explain two examples: (1) The Waymo Open Dataset Sun et al. ([2020](#bib.bib132))
    is a Camera+LiDAR dataset, which consists of 1,000 driving sequences containing
    798 scenes allocated for training and 202 scenes allocated for validation. The
    evaluation metrics employed are Average Precision(AP) and Average Precision with
    Heading (APH) information, which consider the weighted average precision and the
    heading accuracy, respectively. The metrics are computed based on the 3D Intersection
    Over Union (IoU) threshold of 0.7 for vehicles and 0.5 for others. (2) V2V4Real
    dataset Xu et al. ([2023b](#bib.bib161)) is designed for the connected vehicles
    based cooperative perception of autonomous driving using V2V (Vehicle-to-Vehicle)
    communication, encompassing a driving area spanning 410 km. It includes a substantial
    collection of real-world data, such as 20,000 LiDAR frames, 40,000 RGB frames,
    240,000 annotated 3D bounding boxes for 5 specific classes, and comprehensive
    High-Definition (HD) Maps containing all driving routes.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark for Semantic Segmentation: KITTI Geiger et al. ([2013](#bib.bib35)),
    Cityscapes Cordts et al. ([2016](#bib.bib25)), ApolloScape Huang et al. ([2018](#bib.bib55)),
    BDD100k Yu et al. ([2018](#bib.bib168)), and A2D2 Geyer et al. ([2020](#bib.bib36)).
    Two examples are given here: (1) Cityscapes Cordts et al. ([2016](#bib.bib25))
    is a semantic urban scene dataset under driving scenarios. It includes semantic
    and instance segmentation annotations. The dataset comprises 2,975 training images
    with a resolution of 2048 × 1024, along with an additional set of 500 validation
    images. (2) ApolloScape Huang et al. ([2018](#bib.bib55)) contains a large collection
    of sequentially recorded 140,000 camera images with pixel-level semantic annotations
    in different driving conditions. It comprises 40,960 training images and 8,327
    validation images. In addition to the semantic annotations, the dataset also includes
    pose information relative to static background point clouds.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark for Instance Segmentation: Cityscapes Cordts et al. ([2016](#bib.bib25)),
    nuScenes Caesar et al. ([2020](#bib.bib13)), BDD100k Yu et al. ([2018](#bib.bib168)),
    and KITTI-360 Liao et al. ([2022](#bib.bib83)). Let us give two examples: (1)
    BDD100k Yu et al. ([2018](#bib.bib168)) includes 100,000 images with a resolution
    of 1280 × 720, and it has training, testing and validation sets. Among them, annotated
    70,000 images are for training and annotated 10,000 images are designed for validation.
    This dataset contains six various weather conditions, six distinct scenes, three
    different parts of a day, and ten object categories with bounding box annotations.
    (2) nuScenes Caesar et al. ([2020](#bib.bib13)) is an autonomous driving dataset
    comprising 1,000 driving scenes. In addition to the scene annotations, nuScenes
    also provides High-Definition (HD) semantic maps, offering insights into 11 distinct
    semantic classes. This dataset encompasses 700 scenes for training, 150 scenes
    for validation, and another 150 scenes for testing. The data collection process
    involved the utilization of six cameras and a 32-beam LiDAR system, while the
    annotations cover 10 objects within a complete 360-degree field of view.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Table [1](#S2.T1 "Table 1 ‣ 2.3 Benchmark Dataset ‣ 2 Intelligent Vehicle
    Perception ‣ Deep Transfer Learning for Intelligent Vehicle Perception: a Survey")
    summarizes the current widely-used benchmark dataset details for the intelligent
    vehicle perception tasks, including the image resolution, image numbers, LiDAR
    frame numbers, task types, real or synthetic information of each benchmark dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 1: Benchmark Datasets for Intelligent Vehicle Perception. D: object detection
    in 2D or 3D, S: semantic or instance segmentation, Syn: synthetic data, R: real
    data.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Benchmark Image Resolution Image # LiDAR Frame # Tasks Real/Syn KITTI Geiger
    et al. ([2013](#bib.bib35)) 1,392×512 15K 1.3M D, S R Cityscapes Cordts et al.
    ([2016](#bib.bib25)) 2,048×1,024 25K - D, S R SIM10k Johnson-Roberson et al. ([2017](#bib.bib59))
    1,914×1,052 10K - D Syn Foggy Cityscapes Sakaridis et al. ([2018](#bib.bib118))
    2,048×1,024 3,475 - D, S Syn Syn2Real-D Peng et al. ([2018](#bib.bib108)) - 248K
    - D Syn, R BDD100K Yu et al. ([2018](#bib.bib168)) 1,280×720 8K - D, S R GTA Richter
    et al. ([2016](#bib.bib114)) 1,914×1,052 24,966 - S Syn GTA-LiDAR Wu et al. ([2019](#bib.bib153))
    64×512 100K - S Syn H3D Patil et al. ([2019](#bib.bib107)) 1,920×1,200 27,721
    - D R nuScenes Caesar et al. ([2020](#bib.bib13)) 1,600×900 40K - D R Waymo Open Sun
    et al. ([2020](#bib.bib132)) 1,920×1,280 200K - D R ApolloCar3D Song et al. ([2019b](#bib.bib130))
    3,384×2,710 5,277 - S R A*3D Pham et al. ([2020](#bib.bib109)) 2,048×1,536 39K
    39,179 D R ApolloScape Huang et al. ([2018](#bib.bib55)) 3,384×2,710 143,906 -
    S R SYNTHIA Ros et al. ([2016](#bib.bib116)) 960×720 13.4K - S Syn Lyft Level
    5 Houston et al. ([2021](#bib.bib52)) - 55K - S R Ford Agarwal et al. ([2020](#bib.bib2))
    - 200K - D R A2D2 Geyer et al. ([2020](#bib.bib36)) 1,928×1,208 12K D, S R ONCE Mao
    et al. ([2021](#bib.bib95)) 1,920×1,020 1M - D R AutoMine Li et al. ([2022c](#bib.bib78))
    2,048×1,536 18K - D R OPV2V Xu et al. ([2022b](#bib.bib159)) 800×600 44K 11K D
    Syn V2V4Real Xu et al. ([2023b](#bib.bib161)) 2,064×1,544 40K 20K D R ![Refer
    to caption](img/285c969079e44cf0b1a34c43f78c4186.png)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2: Distribution Pie Charts of the Benchmark Datasets for Intelligent
    Vehicle Perception: (a) sensor type, (b) task, (c) data source. Note: D: object
    detection in 2D or 3D, S: semantic or instance segmentation, Syn: synthetic data,
    R: real data.'
  prefs: []
  type: TYPE_NORMAL
- en: 3 Domain Distribution Discrepancy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Despite the remarkable achievements of the intelligent vehicle perception algorithms
    on benchmark datasets, there are still significant challenges in the real world
    due to the large variations in the sensor types and settings, data in diverse
    style, environment, weather and illumination, trained epoch, and architecture Li
    et al. ([2022b](#bib.bib71)), Feng et al. ([2021](#bib.bib30)), Schutera et al.
    ([2020](#bib.bib123)), Song et al. ([2023](#bib.bib131)). Based on these observations,
    we divide the domain distribution discrepancy for intelligent vehicle perception
    into three types: sensor difference, data difference, and model difference, as
    shown in Table [2](#S3.T2 "Table 2 ‣ 3.3 Model Difference ‣ 3 Domain Distribution
    Discrepancy ‣ Deep Transfer Learning for Intelligent Vehicle Perception: a Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 3.1 Sensor Difference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: First of all, the domain gap shows up when the sensors are different in types
    and settings as described in Chen et al. ([2023](#bib.bib19)), Wang et al. ([2023a](#bib.bib145)),
    Li et al. ([2022d](#bib.bib80)). Let us explain the sensor difference for camera
    and LiDAR separately. The camera sensor is cheap but not robust to different types
    and settings, for example, angle difference from horizontal to oblique Rist et al.
    ([2019](#bib.bib115)), placement dissimilarity from front view to rear view Alonso
    et al. ([2020](#bib.bib3)), image resolution diversity Carranza-García et al.
    ([2020](#bib.bib15)), and so on. The LiDAR sensors might also have different types
    and settings, for example, different laser beam numbers Yi et al. ([2021](#bib.bib167)),
    various LiDAR equipment from different companies Xu et al. ([2023a](#bib.bib160)),
    LiDAR placement dissimilarity Hu et al. ([2022a](#bib.bib53)), and so on. The
    problem is quite similar to the setting of other sensor types. Taking Radar as
    an example, variations in Radar resolutions, field-of-view, and noise characteristics
    can result in diverse data distributions. These real-world challenges due to the
    sensor difference may generate the heterogeneous feature distribution between
    different domains Triess et al. ([2021](#bib.bib134)), Zhou et al. ([2022b](#bib.bib184)),
    Chakeri et al. ([2021](#bib.bib17)).
  prefs: []
  type: TYPE_NORMAL
- en: 3.2 Data Difference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In addition, the domain gap exists when the data itself is different in style
    and format as explained in Gao et al. ([2021](#bib.bib32)), Triess et al. ([2021](#bib.bib134)).
    The data collected by the sensors in different situations might result in the
    heterogeneous data distribution between different domains. See the following four
    common examples for intelligent vehicles. (1) Diverse Weather: foggy, rainy, snowy,
    sunny, etc Miglani and Kumar ([2019](#bib.bib97)), Xu et al. ([2021](#bib.bib157)),
    Mirza et al. ([2022](#bib.bib98)), Bogdoll et al. ([2022](#bib.bib11)), Li et al.
    ([2023c](#bib.bib75)). (2) Various Illumination: daytime, nighttime, tunnel, etc Wu
    et al. ([2021](#bib.bib154)). (3) Occlusion: objects or parts of objects are obscured
    or hidden from the sensors’ view due to obstacles, other vehicles, or environmental
    conditions Qian et al. ([2022](#bib.bib111)), Ruan et al. ([2023](#bib.bib117)).
    (4) Different Data Source: differences between synthetic data (computer game data:
    SYNTHIA Ros et al. ([2016](#bib.bib116)), GTA5 Richter et al. ([2016](#bib.bib114)))
    and real-world data Li et al. ([2023a](#bib.bib73)), and the changes between the
    data collected in different urban or highway environments Shenaj et al. ([2023](#bib.bib126)).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.3 Model Difference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, the model difference is not common but it is also one possible reason
    for the domain gap in intelligent vehicle perception. When the perception architecture
    is diverse, the model is obviously different Xu et al. ([2023a](#bib.bib160)),
    for example, from PointPillar architecture Lang et al. ([2019](#bib.bib66)) to
    SECOND architecture Yan et al. ([2018](#bib.bib162)). When the perception architecture
    is the same, the domain gap may still exist because of heterogeneous configurations,
    like training epochs, resolution, number of convolution layers, and hyperparameters Xu
    et al. ([2023a](#bib.bib160)). Because of these diverse situations, the features
    extracted from them might have a domain shift, as described in Xu et al. ([2023a](#bib.bib160)),
    leading to the heterogeneous feature distribution between different domains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 2: Domain Distribution Discrepancy with Three Types of Differences for
    Intelligent Vehicle Perception: sensor, data and model. “$\rightarrow$” means
    the model training with the left data and testing on the right data.'
  prefs: []
  type: TYPE_NORMAL
- en: Types Differences Examples Sensor Difference Setup 64-beam LiDAR $\rightarrow$
    32-beam LiDAR Yi et al. ([2021](#bib.bib167)) Placement Front $\rightarrow$ Rear Feng
    et al. ([2020](#bib.bib29)) Angle Horizontal $\rightarrow$ Oblique Rist et al.
    ([2019](#bib.bib115)) Data Difference Data Source GTA5 $\rightarrow$ Cityscapes Murez
    et al. ([2018](#bib.bib101)) Occlusion OPV2V $\rightarrow$ V2V4Real Xu et al.
    ([2023b](#bib.bib161)) Weather Cityscapes $\rightarrow$ Foggy Cityscapes Li et al.
    ([2023c](#bib.bib75)) Illumination Cityscapes $\rightarrow$ Dark Zurich Wu et al.
    ([2021](#bib.bib154)) Model Difference Configuration Hyperparameter 1 $\rightarrow$
    Hyperparameter 2  Xu et al. ([2023a](#bib.bib160)) Architecture PointPillars $\rightarrow$
    SECOND Xu et al. ([2023a](#bib.bib160))
  prefs: []
  type: TYPE_NORMAL
- en: 4 Deep Transfer Learning Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: With the rapid advancement of autonomous driving techniques, there is now an
    abundance of driving scene images available. Deep learning methods are booming
    in the application of autonomous driving with high performance of perception.
    This paper is focused on the transfer learning methods for the intelligent vehicle
    perception in the deep learning era.
  prefs: []
  type: TYPE_NORMAL
- en: 'Transfer Learning (TL) is a machine learning method to largely apply the knowledge
    acquired from one task or domain to another related task or domain Zhuang et al.
    ([2020](#bib.bib187)). This paper classifies the deep transfer learning into several
    main types: Supervised TL, Unsupervised TL, Weakly-and-semi Supervised TL, Domain
    Generalization. The chronological overview of the transfer learning research development
    in the deep learning era is shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4 Deep Transfer
    Learning Methodology ‣ Deep Transfer Learning for Intelligent Vehicle Perception:
    a Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6bdc5e2afa6b9686047878899c11c1bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: The Chronological Overview of Transfer Learning Research in the Deep
    Learning Era.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1 Supervised TL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the transfer learning research, the source domain normally has the manually
    annotated ground truth. If the target domain also has the manually annotated ground
    truth, the machine learning technique that transfers knowledge from a labeled
    source domain to the labeled target domain is named as Supervised TL Drews et al.
    ([2017](#bib.bib27)), Yu et al. ([2018](#bib.bib168)), Zhou et al. ([2019](#bib.bib181)).
    Gathering such manually annotated data requires substantial human involvement,
    which is labor-intensive and time-consuming Carvalho et al. ([2015](#bib.bib16)).
  prefs: []
  type: TYPE_NORMAL
- en: We divide the Supervise TL methods into Fine-tuning and Knowledge distillation
    via teacher-student network in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.1 Fine-tuning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Fine-tuning is a common technique in the transfer learning Guo et al. ([2019b](#bib.bib45)),
    Li and Zhang ([2021](#bib.bib68)), Hu et al. ([2022a](#bib.bib53)), which has
    been widely used in intelligent vehicle perception Wang et al. ([2019b](#bib.bib144)),
    Luo et al. ([2021](#bib.bib93)), Liang et al. ([2022](#bib.bib82)), Xu et al.
    ([2019](#bib.bib156)), Doan et al. ([2019](#bib.bib26)). Fine-tuning takes an
    existing neural network model pre-trained on a source domain dataset and further
    trains it on a new target domain dataset. By the fine-tuning, the knowledge learned
    from the source domain can be leveraged to improve the performance on the target
    domain. It is worth mentioning that fine-tuning a pre-trained neural network model
    could obtain better performance than directly training from scratch. Typically,
    the pre-trained neural network model is trained on a large-scale dataset, enabling
    to acquire the knowledge from a wide range. The learning rate of fine-tuning on
    the target domain is relatively small as a fine adjustment for the neural network
    model pre-trained on source domain. The fine-tuning methods could be roughly classified
    into two types: (1) Whole Fine-tuning: it trains all the layers of the whole neural
    network model. (2) Partial Fine-tuning: it allows us to train only the some interested
    layers of the pre-trained neural network while keeping the some layers frozen.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Whole Fine-tuning: All the layers of the entire neural network model are fine-tuned
    to obtain the spatial–temporal interactions Ye et al. ([2021](#bib.bib165)) among
    autonomous vehicles and the 3D perception in autonomous driving Sautier et al.
    ([2022](#bib.bib120)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Partial Fine-tuning: Guo et al. ([2018](#bib.bib43)) only fine-tunes the encoder-decoder
    based semantic segmentation model, by fixing a pre-trained sub-network to ensure
    the multi-class boundary constraint.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In-depth Discussion: (1) The fine-tuning based transfer learning is a simple
    but effective way to transfer the knowledge gained from the pre-training on source
    domain to enhance the performance on the target domain with less data and computational
    resources than training from scratch. The training iterations are typically fewer
    when we fine-tune a pre-trained model, compared to training a machine learning
    model from scratch. Because the pre-trained model already has the prior knowledge
    of the pre-training datasets (normally large), the fine-tuning process requires
    less labeled data for continually training the model to achieve the outstanding
    performance. When the pre-trained model is trained on similar data in source domain,
    fine-tuning will generate improved performance on the target domain. (2) However,
    as a supervised method, fine-tuning requires the manually annotated ground truth
    on the target domain, which might be not available in some real-world applications.
    In addition, fine-tuning might suffer from model forgetting, where the model may
    miss some knowledge about the pre-trained task in source domain when adapting
    to the new task in target domain. Furthermore, small target-domain dataset might
    result in the model overfitting during the fine-tuning.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.1.2 Knowledge Distillation
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Knowledge distillation Hinton et al. ([2015](#bib.bib47)), Gou et al. ([2021](#bib.bib40)),
    Wang and Yoon ([2021](#bib.bib140)), Chen et al. ([2021a](#bib.bib20)), Xie and
    Du ([2022](#bib.bib155)), Beyer et al. ([2022](#bib.bib8)) is an advanced technique
    in deep learning, which is also referred to as teacher-student learning, where
    a student neural network is trained on target domain to imitate the knowledge
    of a teacher neural network trained on source domain. Knowledge distillation has
    been widely utilized in intelligent vehicle perception Kothandaraman et al. ([2021](#bib.bib63)),
    Gao et al. ([2022](#bib.bib33)), Hou et al. ([2022](#bib.bib51)), Yang et al.
    ([2022](#bib.bib163)), Sautier et al. ([2022](#bib.bib120)).
  prefs: []
  type: TYPE_NORMAL
- en: Knowledge distillation could be beneficial to model generalization, model compression,
    model transferability. It improves the model generalization so that the student
    network can generalize better on unseen examples, especially in scenarios with
    limited training data. It allows to compress a large teacher model into a smaller
    student model. It enables the knowledge transferability from the teacher model
    (source domain) to the student model (target domain) even with different deep
    learning architectures. The teacher network is typically trained on a large-scale
    dataset for the next knowledge transferability to the student network, however
    the large-scale dataset might be not available in the source domain of some intelligent
    vehicle perception tasks. Inspired by Lan and Tian ([2022](#bib.bib65)), the knowledge
    distillation methods could be roughly classified into the following two types.
  prefs: []
  type: TYPE_NORMAL
- en: 'Response Knowledge Distillation: It focuses on the final output layer of the
    teacher model so as to teach a student model to mimic its predictions. The core
    concept is to use a loss function called the distillation loss, which measures
    the difference between the output activations of the student and teacher models.
    By minimizing this loss during training, the student model gradually improves
    its ability to generate predictions that closely resemble those of the teacher
    model. Gao et al. ([2022](#bib.bib33)) proposes the cross-domain correlation distillation
    loss to transfer knowledge from daytime to nighttime domains, thereby improving
    nighttime semantic segmentation performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Intermediate Knowledge Distillation: It focuses on aligning the intermediate
    representations of the teacher and student models. The intermediate layers learn
    to recognize and distinguish specific features in the data, and this knowledge
    distilled in teacher network can be leveraged to train the student model effectively.
    Hou et al. ([2022](#bib.bib51)) proposes an approach of transferring distilled
    knowledge from a larger source teacher model to a smaller target student network
    to conduct LiDAR semantic segmentation. Specifically, the intermediate Point-to-Voxel
    Knowledge Distillation approach is utilized to transfer latent knowledge from
    both point level and voxel level to complement sparse supervision signals.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In-depth Discussion: (1) By the knowledge transfer from the teacher model to
    student model, knowledge distillation allows for model compression, improved generalization,
    regularization to prevent overfitting, and ensemble effect mimic. A smaller student
    model can be trained to approximate the performance of a larger computationally
    expensive teacher model, leading to the model compression for the student model.
    By learning from the guidance provided by the teacher model, the student model
    will inherit the teacher’s generalization abilities to unseen examples, ensuring
    better generalization of the student model. The guidance by the teacher model
    is able to act as a form of regularization that prevents overfitting of the student
    model. Knowledge distillation can also be combined with traditional model training
    to mimic the ensemble effect of multiple models. (2) However, the success of knowledge
    distillation relies on a well-trained high-performance teacher model first. Training
    a larger teacher model can be computationally expensive, and the large-scale dataset
    might be not available in the source domain of some intelligent vehicle perception
    tasks. If the teacher model lacks sufficient recognition capacity (not well-optimized),
    then the knowledge transferred to the student model may be not strong enough.
    In some complex cases of intelligent vehicle perception tasks, knowledge distillation
    might struggle to improve the student model’s performance when the data distribution
    difference between source and target domains is quite large.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Unsupervised TL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the intelligent vehicle perception, data labeling is a time-consuming and
    labor-intensive process in real-world scenarios. Generally, supervised algorithms
    struggle when there is a scarcity of labeled data in the source domains Niu et al.
    ([2020](#bib.bib103)), Pan and Yang ([2010](#bib.bib105)). To overcome these challenges,
    Unsupervised Transfer Learning (TL) has emerged as a promising approach for addressing
    such specific cases in the intelligent vehicle perception tasks. Unsupervised
    TL refers to a scenario where there is unlabeled target data besides labeled data
    available in source domain. Unsupervised TL approaches offer promising solutions
    to overcome the limitations of limited labeled data availability, enabling more
    efficient and effective perception in intelligent vehicles.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.1 Image-to-image Transfer
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image-to-image transfer, also known as image-to-image translation, is a computer
    vision task that involves converting an input image to a different domain. It
    aims to establish a learned correspondence between two visual domains, where the
    input image originates from the source domain, while the desired output image
    that resembles the target domain. The goal is to generate a corresponding image
    with similar style of the target domain and simultaneously preserve the relevant
    characteristics and semantic contents of the input image. It has found extensive
    application in the field of autonomous driving as well as intelligent transportation
    systems, including semantic segmentation Murez et al. ([2018](#bib.bib101)), Pizzati
    et al. ([2020](#bib.bib110)), lane recognition Hou et al. ([2019](#bib.bib50)),
    Liu et al. ([2021a](#bib.bib85)), data augmentation Zhang et al. ([2022](#bib.bib176)),
    Yang et al. ([2020](#bib.bib164)) Mușat et al. ([2021](#bib.bib102)) and object
    detection Schutera et al. ([2020](#bib.bib123)), Li et al. ([2021](#bib.bib72),
    [2022b](#bib.bib71)), Shan et al. ([2019](#bib.bib124)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Image-to-image transfer neural networks are commonly implemented using two
    different approaches: (1) Paired Image-to-Image Transfer and (2) Unpaired Image-to-Image
    Transfer. The first approach utilizes generative adversarial networks trained
    on paired images Wang et al. ([2018](#bib.bib143)). This type of network learns
    a mapping that transforms an input image from its original domain to desired output
    domain Isola et al. ([2017](#bib.bib58)). The second approach addresses scenarios
    where unpaired images are used to establish a more general framework Zhu et al.
    ([2017](#bib.bib186)), Park et al. ([2020](#bib.bib106)), inspiring the unsupervised
    image-to-image translation methods Liu et al. ([2017](#bib.bib86)), Baek et al.
    ([2021](#bib.bib6)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Paired Image-to-Image Transfer: Isola et al. ([2017](#bib.bib58)) investigated
    the utilization of conditional Generative Adversarial Networks (GAN) namely pix2pix
    for paired image-to-image translation tasks Hao et al. ([2019](#bib.bib46)). The
    GAN with condition learns a generative model of data but with the added condition
    of an input image to produce a corresponding output image. This approach strives
    to produce plausible images in target domain. The adversarial loss is utilized
    to train a Generator Network which is updated using $l_{1}$ loss, which quantifies
    the disparity between the generated image as well as predicted output. By incorporating
    additional loss, the Generator Network can produce plausible translations of the
    source images. Conversely, the Discriminator Network is designed to perform generated
    image classification. With the paired training data, these methods could translate
    the image of similar styles in different domains.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Unpaired Image-to-Image Transfer: Cycle-consistency GAN (CycleGAN) Zhu et al.
    ([2017](#bib.bib186)) is a type of GAN model that enables image translation between
    unpaired datasets Mușat et al. ([2021](#bib.bib102)), Uricar et al. ([2021](#bib.bib135)),
    Shan et al. ([2019](#bib.bib124)), Liu et al. ([2022a](#bib.bib87)). The training
    process of a CycleGAN involves optimizing two generators and two discriminators
    simultaneously. One generator is responsible for learning the mapping function
    $G$ from domain $X$ to $Y$, while the other generator $F$ learns the mapping from
    domain $Y$ to $X$. Both $G$ and $F$ are trained simultaneously, incorporating
    a cycle consistency loss that enforces the cycle consistency to ensure that $F(G(x))\approx
    x$ and $G(F(y))\approx y$. This loss combined with adversarial losses on domains
    $X$ and $Y$ yields objective for unpaired image-to-image translation. Unpaired
    Image-to-Image Transfer release the requirement of paired training data, which
    is more general in the real-world applications of intelligent vehicle perception.
    By incorporating adversarial losses on domains $X$ and $Y$, the objective for
    unpaired image-to-image translation is obtained. Unpaired Image-to-Image Transfer
    release the need for paired training data, making them more general in real-life
    applications of intelligent vehicle perception.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In-depth Discussion: (1) Image-to-image transfer can be used as data augmentation,
    data distribution style transfer. Because of the limited data of source or target
    domain, image-to-image transfer methods could generate diverse fake examples as
    data augmentation to improve the robustness and generalization during the machine
    learning model training. In addition, image-to-image transfer enables the translation
    between source and target domains, to reduce the data distribution style difference
    in intelligent vehicle perception. (2) However, there are some limitations for
    the image-to-image transfer methods when applying to the real-world intelligent
    vehicle perception. When the transfer involves complex or ambiguous patterns in
    autonomous driving, image-to-image transfer models might produce translations
    with low realism or fidelity. For the Paired Image-to-Image Transfer methods,
    in practical applications of intelligent vehicle perception, the requirement for
    paired training data poses disadvantages. For the Unpaired Image-to-Image Transfer
    methods, they rely on task-specific and predefined similarity functions between
    inputs and outputs and do not consider the reliability and robustness of the translation
    frameworks, which might be disrupted by the perturbations added to input and targeted
    images. This issue is particularly crucial for autonomous driving.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.2 Adversarial Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Adversarial learning refers to a machine learning technique that involves training
    two neural networks in a competitive manner, which is initially introduced in
    the context of Generative Adversarial Networks (GAN) by Goodfellow et al. ([2020](#bib.bib39))
    and also mentioned in Gradient Reversal Layer (GRL) framework Ganin and Lempitsky
    ([2015](#bib.bib31)), and provides a promising approach for generating target-similar
    samples at the pixel-level or target-similar representations at the feature-level
    by training robust deep neural networks. It has become popular for addressing
    transfer learning challenges by minimizing the domain discrepancy using adversarial
    objectives, such as fooling a domain discriminator/classifier. During training,
    the feature extractor and the domain discriminator are engaged in an adversarial
    game. The feature extractor tries to produce representations that confuse the
    domain discriminator, making it difficult for the discriminator to differentiate
    between the domains. Meanwhile, the objective of the domain discriminator is to
    correctly classify the samples into their respective domains. This adversarial
    process encourages the learning of domain-invariant features by the feature extractor,
    thereby minimizing the differences between domains. By minimizing the domain disparities
    through adversarial learning, the model learns representations that capture the
    underlying domain-invariant information shared across domains. This approach helps
    to address TL challenges by effectively reducing the disparities between two different
    domains, improving the model’s generalization capabilities across different domains.
    The adversarial learning based transfer learning methods for intelligent vehicle
    perception consists of two types: GRL based Methods and GAN based Methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GRL based Methods: Domain adaptation in different vehicle perception domains
    can be achieved through the addition of a Gradient Reversal Layer (GRL) to the
    deep learning architecture Xu et al. ([2023a](#bib.bib160)), Li et al. ([2023c](#bib.bib75)).
    The mechanism of domain adversarial embedding involves using a discriminator with
    a GRL to differentiate between samples from two domains. The discriminator is
    a binary classifier, while the GRL can reverse the training gradient in the back
    propagation of feature extraction. Both the discriminator and the GRL work together
    to align the feature distributions across different domains. It is worth mentioning
    that the GRL only comes into effect during the backpropagation phase and does
    not affect the forward propagation process Ganin and Lempitsky ([2015](#bib.bib31)).
    Let us give a detailed example for better understanding. Li et al. ([2023c](#bib.bib75))
    introduces a new framework for domain adaptive object detection in autonomous
    driving during challenging foggy weather. The approach addresses the domain gap
    between clear and foggy weather in vehicle driving by incorporating image-level
    and object-level adaptation techniques, which aim to minimize differences in object
    appearance and image style. Additionally, a novel Adversarial Gradient Reversal
    Layer (AdvGRL) has been proposed to enable adversarial mining for difficult examples
    along with domain adaptation.'
  prefs: []
  type: TYPE_NORMAL
- en: 'GAN based Methods: GAN Goodfellow et al. ([2020](#bib.bib39)), Song et al.
    ([2020](#bib.bib129)) is a popular deep learning framework that can be used to
    teach a model to capture the distribution patterns present within the training
    data, enabling the generation of new data from that same distribution. A GAN consists
    of two separate models, namely the generator $G$ and the discriminator $D$. The
    generator $G$’s job is to create “fake” images that resemble the training images
    so as to confuse the discriminator $D$. The applications of GAN in autonomous
    driving have been recently explored owing to its remarkable progress in generating
    realistic images. Specifically, GAN has been leveraged to generate image or subspace
    feature undistinguished by domain classifier based discriminator, for example,
    GAN could generate aligned/similar features between clear weather and foggy weather Li
    et al. ([2023c](#bib.bib75), [2022a](#bib.bib70)), between synthetic game data
    and real-world data Biasetton et al. ([2019](#bib.bib9)), Zhang et al. ([2021b](#bib.bib175)),
    between daytime data and nighttime data Wang et al. ([2022a](#bib.bib138)), Li
    et al. ([2022a](#bib.bib70)). Let us give a detailed example for better understanding.
    Hoffman et al. ([2018](#bib.bib49)) proposes a domain adaptation model which combines
    generative image space alignment, latent feature space alignment, and the vehicle
    perception task. By considering the vehicle perception task (semantic segmentation
    of urban driving scenes), the image-level features, latent features, and the task-related
    semantic features are aligned across different domains by an adversarial learning
    via a GAN-based framework.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In-depth Discussion: (1) The GRL based Methods rely on minimizing the domain
    distribution discrepancy through a gradient reversal in the back propagation of
    feature extraction to confuse the domain discriminator. In contrast, the GAN based
    Methods focus on training the Generator Network and Discriminator Network alternately
    using a Min-Max adversarial loss function, with the goal of acquiring domain-invariant
    features. In these two ways, the feature distribution between source and target
    domains can be aligned to reduce domain gap for transfer learning. (2) However,
    training the adversarial learning model is sometimes difficult. For the GRL based
    Methods, the simple gradient reversal might be not powerful enough to well minimize
    the domain distribution discrepancy in complex driving scenarios, leading to the
    local optimal solution. For the GAN based Methods, finding the optimized balance
    between the generator $G$ and discriminator $D$ can be difficult and the convergence
    may not always be guaranteed when training the GAN model, due to its hyperparameter
    sensitivity, data quality, and data diversity.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.3 Feature Alignment
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To minimize the domain distribution discrepancy, the objective of feature alignment
    in transfer learning is to discover an aligned feature representation from multiple
    domains. Typically, the feature distribution difference between different domains
    can be defined as loss functions during the deep neural network training, so minimizing
    the loss functions of the feature distribution difference across multiple domains
    will reduce the domain gap.
  prefs: []
  type: TYPE_NORMAL
- en: 'Feature alignment-based transfer learning can be classified into two main categories:
    Subspace Feature Alignment, and Attention-guided Feature Alignment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Subspace Feature Alignment: By projecting the features from different domains
    to a lower-dimensional subspace, several metrics to describe the distance of feature
    distribution across source and target domains can be defined as the loss functions
    in the deep learning framework. Minimizing these metric distances (loss functions)
    will align the features of different domains in the subspace. The widely used
    metric to describe the feature distribution distances are Principal Component
    Analysis (PCA) projected subspace feature distance Song et al. ([2019a](#bib.bib128)),
    Maximum Mean Discrepancy (MMD) Borgwardt et al. ([2006](#bib.bib12)), Kullback–Leibler
    Divergence  Zhang et al. ([2018](#bib.bib177)), Gram Matrix Guo et al. ([2019a](#bib.bib44)),
    Multi-Kernel MMD Gretton et al. ([2012](#bib.bib41)), Long et al. ([2015](#bib.bib91)),
    Joint MMD Long et al. ([2017](#bib.bib92)), Wasserstein distance Arjovsky et al.
    ([2017](#bib.bib4)), etc. For example, let us take a close look at the definition
    of the MMD metric, which is formulated as'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $MMD(\mathcal{X}_{s},\mathcal{X}_{t})=\lVert\frac{1}{n_{s}}\sum_{i=1}^{n_{s}}k(\mathbf{x}_{i}^{s})-\frac{1}{n_{t}}\sum_{j=1}^{n_{t}}k(\mathbf{x}_{j}^{t})\rVert_{H},$
    |  | (1) |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{X}_{s}$ and $\mathcal{X}_{t}$ denote the sets of samples obtained
    from the source and target domains, $\mathbf{x}_{i}^{s}$ and $\mathbf{x}_{j}^{t}$
    are individual samples from the respective domains, and $n_{s}$ and $n_{t}$ denote
    the sample sizes of the source and target domains respectively, $k$ denotes the
    kernel functions, and $H$ indicates the Reproducing Kernel Hilbert Space (RKHS).
  prefs: []
  type: TYPE_NORMAL
- en: 'Attention-guided Feature Alignment: Taking inspiration from the attention mechanism Zhou
    et al. ([2016](#bib.bib180)), Vaswani et al. ([2017](#bib.bib137)), the most informative
    components of specific importance can be focused for the intelligent vehicle perception.
    The deep learning frameworks can first extract the attention maps, then the distance
    of attention maps between two domains can be defined as loss function to be minimized
    during the neural network training Zhou et al. ([2020b](#bib.bib185)), Zagoruyko
    and Komodakis ([2016](#bib.bib172)). By employing this approach, it becomes possible
    to align the feature distribution across both the source and target domains via
    the attention map consistency constraint. For example, in Cho et al. ([2023](#bib.bib22)),
    the relation-aware knowledge captured by multiple detection heads can be transferred
    using a specially designed attention head loss for the improved LiDAR-based 3D
    object detection in the context of autonomous driving.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In-depth Discussion: (1) The Subspace Feature Alignment methods focus on aligning
    the feature distribution in the lower-dimensional subspace representation by using
    different metrics of distribution distances. The Attention-guided Feature Alignment
    methods use the attention mechanism to extract the attention maps first and then
    enforce the attention maps from multiple domains to be the same. In these two
    ways, feature alignment facilitates the model to adapt its knowledge learned from
    the source domain to the target domain, so the model can generalize better to
    the target domain. (2) However, several important settings in the feature alignment
    methods are still open questions. Let us give some examples. How to discover the
    most representative feature subspaces or attention maps does not have a widely-accepted
    common sense in the diverse deep neural network architectures. The feature distribution
    distance metrics (like MMD) are good but may still ignore some important domain
    information. How to balance the loss weights of the feature alignment and the
    original task related loss for intelligent vehicle perception is still challenging
    during the model training.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.2.4 Self-learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Autonomous vehicles continuously collect unlabeled data during their operation,
    creating an opportunity for self-learning Liu et al. ([2021c](#bib.bib90)), Zhang
    et al. ([2021a](#bib.bib173)), Kumar et al. ([2021](#bib.bib64)), Luo et al. ([2021](#bib.bib93)),
    Ziegler and Asano ([2022](#bib.bib188)), which offers a promising approach to
    reduce the reliance on labeled data and enhance model flexibility. Given the absence
    of labeled data in target domain using Unsupervised TL, the self-learning methods
    use the additional cues to evaluate the neural network prediction in an unsupervised
    setting, so some prediction results with high confidence are used as the pseudo-labels
    in the further training or testing.
  prefs: []
  type: TYPE_NORMAL
- en: The following shows some representative examples of self-learning methods for
    the Unsupervised TL based intelligent vehicle perception. The entropy based uncertainty
    can be used to define the hardness of a specific training sample so as to implement
    an easy-to-hard curriculum learning for semantic segmentation Pan et al. ([2020](#bib.bib104)).
    Wang et al. ([2021a](#bib.bib142)) utilizes self-supervised learning to enhance
    the semantic segmentation performance by using depth estimation as guidance to
    overcome the domain gap between the source and target domains. They explicitly
    capture the correlation between task features and use target depth estimation
    to enhance target semantic predictions. The adaptation difficulty, as inferred
    from depth information, is subsequently utilized to enhance the quality of pseudo-labels
    for target semantic segmentation. Shin et al. ([2022](#bib.bib127)) proposes a
    multi-modal extension of test-time adaptation in the context of 3D semantic segmentation.
    To improve the unstable performance of models at test time, they design both intra-modal
    and inter-modal modules together to acquire more dependable self-learning signals
    of pseudo-labels. Zhang et al. ([2021a](#bib.bib173)) utilizes the multiple classifiers
    with attention heads to evaluate the uncertainty associated with the pseudo-labels.
    The panoramic pseudo-labels with high confidences are then used to improve the
    panoramic semantic segmentation prediction in an iterative fashion.
  prefs: []
  type: TYPE_NORMAL
- en: 'In-depth Discussion: (1) Self-learning bridges the gap between supervised and
    unsupervised learning, which combines the advantages of the both methods by using
    labeled data for training and unlabeled data for further refinement. By leveraging
    self-learning in autonomous driving, the need for extensive manual annotation
    of data is reduced, enabling more cost-effective and efficient training of models.
    The iterative process of utilizing high-confidence identified samples and generating
    pseudo-labels facilitates promising methods using unlabeled data in target domain.
    (2) However, the robustness and convergence of the self-learning methods is still
    an open question for the reliable intelligent vehicle perception. If the initial
    model is not sufficiently confident and makes mistake predictions on unlabeled
    data, such errors might be propagated through iterations, leading to poor performance
    or hard convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.3 Weakly-and-semi Supervised TL
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although impressive results have been achieved by unsupervised TL methods,
    the domain gap cannot be completely eliminated due to the lack of supervision
    on the target domain. There is still a relative performance gap compared with
    supervised TL methods. Another way in addressing the domain gap is by using the
    weakly-and-semi supervised learning method that utilizes both weakly labeled and
    some labeled/unlabeled data in target domain. Based on the available supervision,
    the weakly-and-semi supervised transfer learning methods could be roughly classified
    into two types: Weakly-Supervised TL: There are only weakly supervised labels
    in the target domain. Semi-Supervised TL: There are only semi-supervised labels
    in the target domain, including some labeled data and the remaining unlabeled
    data on target domain.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Weakly-Supervised TL: Theories of weakly supervised learning have been applied
    in autonomous driving Barnes et al. ([2017](#bib.bib7)), Gojcic et al. ([2021](#bib.bib38)),
    such as object detection, semantic segmentation, and instance segmentation. The
    transfer learning techniques can be applied simultaneously with the weakly supervised
    learning. For example, when an instance-level task only has image-level annotations
    in target domain but with instance-level annotations in source domain, the pseudo
    annotations can be predicted Inoue et al. ([2018](#bib.bib57)) for the object
    detection task. Given a source domain (synthetic data) with pixel/object- level
    labels, a target domain (real-world scenes) might only have object-level labels,
    where the pixel-level and object-level domain classifiers can be used in transfer
    learning to learn domain-invariant features for the semantic segmentation task
    in driving scenes Wang et al. ([2019a](#bib.bib141)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Semi-Supervised TL: There are three types of training data (labeled source
    data, labeled target data, and unlabeled target data) in the semi-supervised TL
    setting Wang et al. ([2020](#bib.bib147)), Chen et al. ([2021b](#bib.bib21)),
    Wang et al. ([2023b](#bib.bib146)). The key point for improving semi-supervised
    TL is to effectively use available unlabeled data from target domain and limited
    labeled data from different domains. For example, Wang et al. ([2020](#bib.bib147))
    aligns feature distribution across two domains by introducing an extra semantic-level
    adaptation module, which leverages a few labeled images from the target domain
    to supervise the segmentation and feature adaptation tasks. Other works focus
    on generating pseudo labels for unlabeled target data by using labeled source
    data and labeled target data. For example, Wang et al. ([2023b](#bib.bib146))
    solves this problem by two-stage learning that includes inter-domain adaptation
    stage and intra-domain generalization stage. While Chen et al. ([2021b](#bib.bib21))
    uses the domain-mixed teacher models and knowledge distillation to train a good
    student model, then the good student model will generate pseudo labels for the
    next round of teacher model training.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In-depth Discussion: (1) By involving some supervisions in the target domain,
    the weakly-and-semi supervised learning methods could achieve a better performance
    than the unsupervised TL methods. Weakly-Supervised TL can be more data-efficient
    as it leverages weakly labeled data which is often easier and cheaper to obtain,
    reducing the need for extensive manual labeling. Semi-Supervised TL can improve
    the model’s generalization to unseen data and reduce overfitting on the limited
    labeled data. Semi-supervised methods augment the labeled dataset with unlabeled
    data, providing the model with additional training examples and increasing data
    diversity. Similar to weakly supervised methods, semi-supervised methods can be
    valuable for domain adaptation tasks, when limited labeled data is available in
    the target domain. (2) While various methods have been proposed for weakly-and-semi
    supervised transfer learning, how to leverage the unlabeled target data with the
    help of available labeled data under different complex situations is still challenging.
    The performance of weakly-and-semi supervised transfer learning methods still
    perform worse than the supervised transfer learning methods, which indicates that
    some local optimal solutions are achieved only. How to make a further improvement
    to overcome the algorithm performance boundary is an open question now. Maybe
    some prior knowledge of human driving can be leveraged to advance the algorithm
    performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 4.4 Domain Generalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Domain Generalization (DG) for intelligent vehicle perception offers a solution
    to the challenge of enhancing the resilience of deep neural networks against arbitrary
    unseen driving scenes Zhou et al. ([2022a](#bib.bib183)). Unlike Domain Adaptation
    (DA), DG methods typically focus on learning a shared representation across multiple
    source domains. This approach aims to enhance the model ability to generalize
    across various domains, enabling it to perform well in an unknown target domain
    of driving. Nevertheless, the collection of multi-domain datasets is a laborious
    and costly endeavor, and the efficacy of DG methods is significantly influenced
    by the quantity of source datasets Wang et al. ([2022b](#bib.bib139)).
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of domain generalization (DG) has emerged as a solution to address
    the lack of target data in domain gap Blanchard et al. ([2011](#bib.bib10)) Wang
    et al. ([2022b](#bib.bib139)). The primary distinction between DA and DG lies
    in the fact that DG does not require access to the target domain during the training
    phase. DG aims to develop a model by using data from one or multiple related but
    distinct source domains to generate any out-of-distribution target domain data Shen
    et al. ([2021](#bib.bib125)). The existing methods for DG can be divided into
    two main groups according to the number of source domains: Multi-source DG and
    Single-source DG.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Multi-source DG: Its primary motivation is to utilize data from multiple sources
    to learn representations that are invariant to different marginal distributions Wilson
    and Cook ([2020](#bib.bib151)) Luo et al. ([2022](#bib.bib94)) Zhao et al. ([2022](#bib.bib179)).
    Due to the absence of target data, it is challenging for a model trained on a
    single source to achieve generalization effectively. By leveraging multiple domains,
    a model can discover stable patterns across the source domains, leading to better
    generalization results on unseen domains. The underlying concept behind this category
    is to minimize the difference between the representations of various source domains,
    thus learn domain-invariant representations Yue et al. ([2019](#bib.bib170)),
    Hu et al. ([2022b](#bib.bib54)), Xu et al. ([2022a](#bib.bib158)), Li et al. ([2022b](#bib.bib71)),
    Choi et al. ([2021](#bib.bib23)), Lin et al. ([2021](#bib.bib84)), Acuna et al.
    ([2021](#bib.bib1)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Single-source DG: It assumes that the training data is homogeneous, which is
    sampled from a single domain Qiao et al. ([2020](#bib.bib112)), Wang et al. ([2021b](#bib.bib148)).
    Single-source DG methods revolve around data augmentation, and they aim to create
    samples that are out of the domain and utilize them to train the network in conjunction
    with the source samples, enhancing the generalization capability Li et al. ([2023d](#bib.bib79)),
    Lehner et al. ([2022](#bib.bib67)), Hu et al. ([2022b](#bib.bib54)), Khosravian
    et al. ([2021](#bib.bib61)), Chuah et al. ([2022](#bib.bib24)), Sanchez et al.
    ([2022](#bib.bib119)), Zhang et al. ([2020](#bib.bib174)), Wu and Deng ([2022](#bib.bib152)).
    Although single-source DG methods are not robust as multi-source domain method
    due to the limited information from source domain, they do not rely on domain
    identity labels for learning, which makes them applicable to both single-source
    and multi-source scenarios.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In-depth Discussion: (1) By learning domain-invariant features plus data augmentation
    or random feature generalization, domain generalization aims to enhance the model’s
    ability to be generalized to new unseen data. This can be beneficial for intelligent
    vehicle perception because many complex driving scenarios in the real world are
    not seen before. By incorporating Multi-source DG and Single-source DG, they can
    reduce the data bias in the existing training data, so the perception model can
    be more robust to new unseen driving scenarios. It is impossible to collect all
    the driving data in the real world, so domain generalization potentially saves
    time and labor cost with improved model robustness. (2) However, training a perception
    model with the domain generalization capability is more complex than traditional
    domain-specific transfer learning methods, as it requires handling the generalization
    of domain-invariant features for either single source or multiple sources. When
    training on multiple source domains, the model might suffer from the data imbalance
    across different domains, leading to biased learning towards some dominant domains.
    This issue is similar for the class imbalance problem in a single source domain,
    posing challenges to the domain generalization methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 3: Uniqueness of Deep Transfer Learning (TL) Methods for Intelligent
    Vehicle Perception.'
  prefs: []
  type: TYPE_NORMAL
- en: Types Methodologies Uniqueness Supervised TL Fine-tuning Continuous learning
    from pre-trained model Knowledge distillation Knowledge transfer from teacher
    to student model Unsupervised TL Image-to-image transfer Pixel-level mapping or
    translation Adversarial learning Training two models adversarially Feature alignment
    Aligning features in different domains Self-learning Learning with pseudo-labels
    for refinement Weakly-and-semi Supervised TL Weakly-Supervised TL Learning with
    weakly-labeled data Semi-Supervised TL Learning with partially-labeled data Domain
    Generalization Multi-source DG Generalization to unseen data by multiple source
    domains Single-source DG Generalization to unseen data by single source domain
  prefs: []
  type: TYPE_NORMAL
- en: 4.5 Uniqueness of Methodology
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Table [3](#S4.T3 "Table 3 ‣ 4.4 Domain Generalization ‣ 4 Deep Transfer
    Learning Methodology ‣ Deep Transfer Learning for Intelligent Vehicle Perception:
    a Survey") summarizes the uniqueness of the above classified deep transfer learning
    methods for intelligent vehicle perception, including the methods of Supervise
    TL, Unsupervised TL, Weakly-and-semi Supervised TL, and Domain Generalization.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Challenges and Future Research
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This section outlines the main challenges of the deep transfer learning for
    the current intelligent vehicle perception and the related future research directions.
  prefs: []
  type: TYPE_NORMAL
- en: '1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sensor Robustness: The current camera and LiDAR sensors are not robust enough
    in the extreme driving scenarios, like diverse weather, dark illumination, various
    environments. In addition, for the V2V cooperative perception, the V2V communication
    sensors might have the issues of lossy communication Li et al. ([2023b](#bib.bib74))
    due to the fast speed, obstacles, etc Schlager et al. ([2022a](#bib.bib121)).
    Future Research: More future research can be focused on improving the sensor robustness,
    for example, the camera and LiDAR sensors in diverse weather, dark illumination,
    various environments, and the communication sensors in the V2V system Tahir et al.
    ([2021](#bib.bib133)). For example, more advanced sensors with robust lens coatings,
    self-cleaning mechanisms, better LiDAR reflection can be studied to compensate
    for distortions in adverse weather. More advanced V2V communication systems with
    less communication delay can be investigated, anticipating the future growth of
    autonomous connected vehicles.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Methodology Limitation: The current unsupervised transfer learning methods
    are worse than the supervised transfer learning methods with a relative performance
    insufficiency. In addition, how to fully utilize the knowledge of the source domain
    and the human prior cognition and experience is still a question to be answered.
    How to effectively use the weakly and partially labeled data is still a open question.
    Future Research: Researchers could make efforts to develop more advanced deep
    transfer learning methods in the future, for example, largely reducing the performance
    disparity between unsupervised and supervised approaches, incorporating the Vehicle-to-Everything
    (V2X) techniques to communicate with connected vehicles and smart infrastructures
    to overcome the occlusion challenges, involving the Large Language Models, like
    ChatGPT Gao et al. ([2023](#bib.bib34)), to better simulate the human cognition
    and knowledge so as to guide the transfer learning methods, accurately self-learning
    the unlabeled data, effectively and efficiently using the weakly and partially
    supervised data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '3.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Realism of Synthetic Data: By eliminating the need for manual annotation, the
    synthetic data generated by computer game engines is quite helpful to improve
    the training data size, but it still has significant differences with the real-world
    data in styles, lighting conditions, viewpoints, and vehicle behaviors, etc. Future
    Research: The realism of the synthetic data can be improved by more advanced computer
    game engines in the future. The customized synthetic data can be better simulated
    via a digital twin simulation system Wang et al. ([2023c](#bib.bib149)). For example,
    researchers can engage in the development of dynamic and interactive virtual environments
    within gaming engines. Through the simulation of authentic variations in illuminating,
    weather conditions, and object interactions, synthetic data will be able to emulate
    the complexities of real-world scenarios. This process could create more robust
    and transferable machine learning models. Moreover, constructing digital twin
    models based on real-world data offers the opportunity to generate highly personalized
    and precise synthetic data.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '4.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Scarcity of Annotated Benchmarks in Complex Scenarios: There are infinite complex
    scenarios in the real-world driving, but the current benchmark datasets in the
    complex driving scenarios are still limited. For example, the Foggy Cityscapes
    dataset Sakaridis et al. ([2018](#bib.bib118)) only has 2,975 training images
    during the foggy weather, whose small size poses a clear hurdle for the accurate
    perception of the intelligent vehicle in the foggy weather. Future Research: We
    expect that more high-quality benchmark datasets in complex driving scenarios
    could be collected and publicized in the future. We also encourage more advanced
    physical models to simulate the benchmark data (Camera, LiDAR) in complex driving
    scenarios in the future, such as simulating the fog, rain, snow, lighting changes,
    etc. For example, future research may focus on collecting large-scale real-world
    benchmark datasets (Camera, LiDAR) that cover a wide range of complex driving
    scenarios with challenging urban or highway environments, dense traffic, pedestrian
    crossings, various traffic patterns and different road geometries.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '5.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'International Standards for Hardware Sensors: The hardware sensors might be
    provided from multiple companies of different countries, but there are no unified
    international standards for the hardware sensors for intelligent vehicle perception.
    For example, the different hardware sensor types and settings will enlarge the
    domain gap in different environments. Future Research: We hope that the multiple
    companies of different countries can collaborate together to promote the international
    standards for hardware sensors in the future, including the types, settings, parameters
    of the hardware sensors in different driving environments Schlager et al. ([2022b](#bib.bib122)),
    Masmoudi et al. ([2021](#bib.bib96)). Compatible hardware sensor standards would
    enable the exchange and sharing of data across different intelligent vehicle platforms,
    allowing for improved decision-making and safety on the roads.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'International Standards for Software Packages: The software package might be
    provided from multiple companies of different countries as well, but there are
    no unified international standards for the software packes for intelligent vehicle
    perception. For example, sharing the features of models trained in different epochs,
    e.g., from different companies, will result in the performance drop in V2V cooperative
    perception Xu et al. ([2023a](#bib.bib160)). Future Research: The multiple companies
    of different countries are expected to collaborate together to promote the international
    standards for software packages in the future, including the deep learning model
    architectures and frameworks, hyper parameters, privacy and safety preservation,
    etc. For example, the accepted software package standards can reduce the problems
    in the V2V data sharing of the connected intelligent vehicles among different
    companies with guidelines.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this survey paper, we presented a comprehensive review of deep transfer learning
    for intelligent vehicle perception. We reviewed the perception tasks and the related
    benchmark datasets and then divided the domain distribution discrepancy of the
    intelligent vehicle perception in the real world into sensor, data, and model
    differences. Then, we provided clearly classified and summarized definition and
    description of numerous representative deep transfer learning approaches and related
    works in intelligent vehicle perception. Through our intensive analysis and review,
    we have identified several potential challenges and directions for future research.
    Overall, this survey paper aims to make contributions to introduce and explain
    the deep transfer learning techniques for intelligent vehicle perception, offering
    invaluable insights and directions for the future research.
  prefs: []
  type: TYPE_NORMAL
- en: 7 CRediT authorship contribution
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Xinyu Liu: Conceptualization, Methodology, Original draft preparation. Jinlong
    Li: Methodology, Original draft preparation. Jin Ma: Methodology, Investigation.
    Huiming Sun: Methodology, Investigation. Zhigang Xu: Review & editing. Tianyun
    Zhang: Review & editing. Hongkai Yu: Methodology guidance, Supervision, Review
    & editing.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Declaration of Competing Interest
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The authors declare that they have no known competing financial interests or
    personal relationships that could have appeared to influence the work reported
    in this paper.
  prefs: []
  type: TYPE_NORMAL
- en: 9 Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work was supported by NSF 2215388 and CSU FRD grants.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Acuna et al. (2021) D. Acuna, J. Philion, and S. Fidler. Towards optimal strategies
    for training self-driving perception models in simulation. *Advances in Neural
    Information Processing Systems*, 34:1686–1699, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agarwal et al. (2020) S. Agarwal, A. Vora, G. Pandey, W. Williams, H. Kourous,
    and J. McBride. Ford multi-av seasonal dataset. *The International Journal of
    Robotics Research*, 39(12):1367–1376, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alonso et al. (2020) I. Alonso, L. Riazuelo, L. Montesano, and A. C. Murillo.
    Domain adaptation in lidar semantic segmentation by aligning class distributions.
    *arXiv preprint arXiv:2010.12239*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arjovsky et al. (2017) M. Arjovsky, S. Chintala, and L. Bottou. Wasserstein
    generative adversarial networks. In *International Conference on Machine Learning*,
    pages 214–223\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Arnold et al. (2019) E. Arnold, O. Y. Al-Jarrah, M. Dianati, S. Fallah, D. Oxtoby,
    and A. Mouzakitis. A survey on 3d object detection methods for autonomous driving
    applications. *IEEE Transactions on Intelligent Transportation Systems*, 20(10):3782–3795,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Baek et al. (2021) K. Baek, Y. Choi, Y. Uh, J. Yoo, and H. Shim. Rethinking
    the truly unsupervised image-to-image translation. In *IEEE/CVF International
    Conference on Computer Vision*, pages 14154–14163, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Barnes et al. (2017) D. Barnes, W. Maddern, and I. Posner. Find your own way:
    Weakly-supervised segmentation of path proposals for urban autonomy. In *IEEE
    International Conference on Robotics and Automation*, pages 203–210\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Beyer et al. (2022) L. Beyer, X. Zhai, A. Royer, L. Markeeva, R. Anil, and
    A. Kolesnikov. Knowledge distillation: A good teacher is patient and consistent.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 10925–10934,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Biasetton et al. (2019) M. Biasetton, U. Michieli, G. Agresti, and P. Zanuttigh.
    Unsupervised domain adaptation for semantic segmentation of urban scenes. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition Workshops*, pages 0–0, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Blanchard et al. (2011) G. Blanchard, G. Lee, and C. Scott. Generalizing from
    several related classification tasks to a new unlabeled sample. *Advances in Neural
    Information Processing Systems*, 24, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bogdoll et al. (2022) D. Bogdoll, M. Nitsche, and J. M. Zöllner. Anomaly detection
    in autonomous driving: A survey. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pages 4488–4499, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Borgwardt et al. (2006) K. M. Borgwardt, A. Gretton, M. J. Rasch, H.-P. Kriegel,
    B. Schölkopf, and A. J. Smola. Integrating structured biological data by kernel
    maximum mean discrepancy. *Bioinformatics*, 22(14):e49–e57, 2006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Caesar et al. (2020) H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong,
    Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom. nuscenes: A multimodal
    dataset for autonomous driving. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pages 11621–11631, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2019) Y. Cao, C. Xiao, B. Cyr, Y. Zhou, W. Park, S. Rampazzi, Q. A.
    Chen, K. Fu, and Z. M. Mao. Adversarial sensor attack on lidar-based perception
    in autonomous driving. In *ACM SIGSAC Conference on Computer and Communications
    Security*, pages 2267–2281, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Carranza-García et al. (2020) M. Carranza-García, J. Torres-Mateo, P. Lara-Benítez,
    and J. García-Gutiérrez. On the performance of one-stage and two-stage object
    detectors in autonomous vehicles using camera data. *Remote Sensing*, 13(1):89,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Carvalho et al. (2015) A. Carvalho, S. Lefévre, G. Schildbach, J. Kong, and
    F. Borrelli. Automated driving: The role of forecasts and uncertainty—a control
    perspective. *European Journal of Control*, 24:14–32, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chakeri et al. (2021) A. Chakeri, X. Wang, Q. Goss, M. I. Akbas, and L. G. Jaimes.
    A platform-based incentive mechanism for autonomous vehicle crowdsensing. *IEEE
    Open Journal of Intelligent Transportation Systems*, 2:13–23, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022) L. Chen, Y. Li, C. Huang, B. Li, Y. Xing, D. Tian, L. Li,
    Z. Hu, X. Na, Z. Li, et al. Milestones in autonomous driving and intelligent vehicles:
    Survey of surveys. *IEEE Transactions on Intelligent Vehicles*, 8(2):1046–1056,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2023) L. Chen, Y. Li, C. Huang, Y. Xing, D. Tian, L. Li, Z. Hu,
    S. Teng, C. Lv, J. Wang, et al. Milestones in autonomous driving and intelligent
    vehicles—part 1: Control, computing system design, communication, hd map, testing,
    and human behaviors. *IEEE Transactions on Systems, Man, and Cybernetics: Systems*,
    2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021a) P. Chen, S. Liu, H. Zhao, and J. Jia. Distilling knowledge
    via knowledge review. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 5008–5017, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2021b) S. Chen, X. Jia, J. He, Y. Shi, and J. Liu. Semi-supervised
    domain adaptation based on dual-level domain mixing for semantic segmentation.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 11018–11027,
    2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Cho et al. (2023) H. Cho, J. Choi, G. Baek, and W. Hwang. itkd: Interchange
    transfer-based knowledge distillation for 3d object detection. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 13540–13549, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Choi et al. (2021) S. Choi, S. Jung, H. Yun, J. T. Kim, S. Kim, and J. Choo.
    Robustnet: Improving domain generalization in urban-scene segmentation via instance
    selective whitening. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 11580–11590, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chuah et al. (2022) W. Chuah, R. Tennakoon, R. Hoseinnezhad, A. Bab-Hadiashar,
    and D. Suter. Itsa: An information-theoretic approach to automatic shortcut avoidance
    and domain generalization in stereo matching networks. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 13022–13032, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cordts et al. (2016) M. Cordts, M. Omran, S. Ramos, T. Rehfeld, M. Enzweiler,
    R. Benenson, U. Franke, S. Roth, and B. Schiele. The cityscapes dataset for semantic
    urban scene understanding. In *IEEE Conference on Computer Vision and Pattern
    Recognition*, pages 3213–3223, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Doan et al. (2019) A.-D. Doan, Y. Latif, T.-J. Chin, Y. Liu, T.-T. Do, and I. Reid.
    Scalable place recognition under appearance change for autonomous driving. In
    *IEEE/CVF International Conference on Computer Vision*, pages 9319–9328, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Drews et al. (2017) P. Drews, G. Williams, B. Goldfain, E. A. Theodorou, and
    J. M. Rehg. Aggressive deep driving: Combining convolutional neural networks and
    model predictive control. In *Conference on Robot Learning*, pages 133–142\. PMLR,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fadadu et al. (2022) S. Fadadu, S. Pandey, D. Hegde, Y. Shi, F.-C. Chou, N. Djuric,
    and C. Vallespi-Gonzalez. Multi-view fusion of sensor data for improved perception
    and prediction in autonomous driving. In *IEEE/CVF Winter Conference on Applications
    of Computer Vision*, pages 2349–2357, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Feng et al. (2020) D. Feng, C. Haase-Schütz, L. Rosenbaum, H. Hertlein, C. Glaeser,
    F. Timm, W. Wiesbeck, and K. Dietmayer. Deep multi-modal object detection and
    semantic segmentation for autonomous driving: Datasets, methods, and challenges.
    *IEEE Transactions on Intelligent Transportation Systems*, 22(3):1341–1360, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2021) D. Feng, A. Harakeh, S. L. Waslander, and K. Dietmayer. A
    review and comparative study on probabilistic object detection in autonomous driving.
    *IEEE Transactions on Intelligent Transportation Systems*, 23(8):9961–9980, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ganin and Lempitsky (2015) Y. Ganin and V. Lempitsky. Unsupervised domain adaptation
    by backpropagation. In *International Conference on Machine Learning*, pages 1180–1189\.
    PMLR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2021) B. Gao, Y. Pan, C. Li, S. Geng, and H. Zhao. Are we hungry
    for 3d lidar data for semantic segmentation? a survey of datasets and methods.
    *IEEE Transactions on Intelligent Transportation Systems*, 23(7):6063–6081, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2022) H. Gao, J. Guo, G. Wang, and Q. Zhang. Cross-domain correlation
    distillation for unsupervised domain adaptation in nighttime semantic segmentation.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 9913–9923,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gao et al. (2023) Y. Gao, W. Tong, E. Q. Wu, W. Chen, G. Zhu, and F.-Y. Wang.
    Chat with chatgpt on interactive engines for intelligent driving. *IEEE Transactions
    on Intelligent Vehicles*, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geiger et al. (2013) A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision
    meets robotics: The kitti dataset. *The International Journal of Robotics Research*,
    32(11):1231–1237, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Geyer et al. (2020) J. Geyer, Y. Kassahun, M. Mahmudi, X. Ricou, R. Durgesh,
    A. S. Chung, L. Hauswald, V. H. Pham, M. Mühlegg, S. Dorn, et al. A2d2: Audi autonomous
    driving dataset. *arXiv preprint arXiv:2004.06320*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gholamhosseinian and Seitz (2021) A. Gholamhosseinian and J. Seitz. Vehicle
    classification in intelligent transport systems: An overview, methods and software
    perspective. *IEEE Open Journal of Intelligent Transportation Systems*, 2:173–194,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gojcic et al. (2021) Z. Gojcic, O. Litany, A. Wieser, L. J. Guibas, and T. Birdal.
    Weakly supervised learning of rigid 3d scene flow. In *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pages 5692–5703, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow et al. (2020) I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley,
    S. Ozair, A. Courville, and Y. Bengio. Generative adversarial networks. *Communications
    of the ACM*, 63(11):139–144, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gou et al. (2021) J. Gou, B. Yu, S. J. Maybank, and D. Tao. Knowledge distillation:
    A survey. *International Journal of Computer Vision*, 129:1789–1819, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gretton et al. (2012) A. Gretton, D. Sejdinovic, H. Strathmann, S. Balakrishnan,
    M. Pontil, K. Fukumizu, and B. K. Sriperumbudur. Optimal kernel choice for large-scale
    two-sample tests. *Advances in Neural Information Processing Systems*, 25, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grigorescu et al. (2020) S. Grigorescu, B. Trasnea, T. Cocias, and G. Macesanu.
    A survey of deep learning techniques for autonomous driving. *Journal of Field
    Robotics*, 37(3):362–386, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2018) D. Guo, L. Zhu, Y. Lu, H. Yu, and S. Wang. Small object sensitive
    segmentation of urban street scene with spatial adjacency between object classes.
    *IEEE Transactions on Image Processing*, 28(6):2643–2653, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Guo et al. (2019a) D. Guo, Y. Pei, K. Zheng, H. Yu, Y. Lu, and S. Wang. Degraded
    image semantic segmentation with dense-gram networks. *IEEE Transactions on Image
    Processing*, 29:782–795, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guo et al. (2019b) Y. Guo, H. Shi, A. Kumar, K. Grauman, T. Rosing, and R. Feris.
    Spottune: transfer learning through adaptive fine-tuning. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 4805–4814, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hao et al. (2019) Z. Hao, S. You, Y. Li, K. Li, and F. Lu. Learning from synthetic
    photorealistic raindrop for single image raindrop removal. In *IEEE/CVF International
    Conference on Computer Vision Workshops*, pages 0–0, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hinton et al. (2015) G. Hinton, O. Vinyals, and J. Dean. Distilling the knowledge
    in a neural network. *arXiv preprint arXiv:1503.02531*, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hnewa and Radha (2020) M. Hnewa and H. Radha. Object detection under rainy
    conditions for autonomous vehicles: A review of state-of-the-art and emerging
    techniques. *IEEE Signal Processing Magazine*, 38(1):53–67, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoffman et al. (2018) J. Hoffman, E. Tzeng, T. Park, J.-Y. Zhu, P. Isola, K. Saenko,
    A. Efros, and T. Darrell. Cycada: Cycle-consistent adversarial domain adaptation.
    In *International Conference on Machine Learning*, pages 1989–1998\. Pmlr, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. (2019) Y. Hou, Z. Ma, C. Liu, and C. C. Loy. Learning lightweight
    lane detection cnns by self attention distillation. In *IEEE/CVF International
    Conference on Computer Vision*, pages 1013–1021, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hou et al. (2022) Y. Hou, X. Zhu, Y. Ma, C. C. Loy, and Y. Li. Point-to-voxel
    knowledge distillation for lidar semantic segmentation. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 8479–8488, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Houston et al. (2021) J. Houston, G. Zuidhof, L. Bergamini, Y. Ye, L. Chen,
    A. Jain, S. Omari, V. Iglovikov, and P. Ondruska. One thousand and one hours:
    Self-driving motion prediction dataset. In *Conference on Robot Learning*, pages
    409–418\. PMLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2022a) H. Hu, Z. Liu, S. Chitlangia, A. Agnihotri, and D. Zhao. Investigating
    the impact of multi-lidar placement on object detection for autonomous driving.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 2550–2559,
    2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. (2022b) Y. Hu, X. Jia, M. Tomizuka, and W. Zhan. Causal-based time
    series domain generalization for vehicle intention prediction. In *International
    Conference on Robotics and Automation*, pages 7806–7813\. IEEE, 2022b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. (2018) X. Huang, X. Cheng, Q. Geng, B. Cao, D. Zhou, P. Wang, Y. Lin,
    and R. Yang. The apolloscape dataset for autonomous driving. In *IEEE Conference
    on Computer Vision and Pattern Recognition Workshops*, pages 954–960, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Huang and Chen (2020) Y. Huang and Y. Chen. Autonomous driving with deep learning:
    A survey of state-of-art technologies. *arXiv preprint arXiv:2006.06091*, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Inoue et al. (2018) N. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa. Cross-domain
    weakly-supervised object detection through progressive domain adaptation. In *IEEE
    Conference on Computer Vision and Pattern Recognition*, pages 5001–5009, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Isola et al. (2017) P. Isola, J.-Y. Zhu, T. Zhou, and A. A. Efros. Image-to-image
    translation with conditional adversarial networks. In *IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 1125–1134, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Johnson-Roberson et al. (2017) M. Johnson-Roberson, C. Barto, R. Mehta, S. N.
    Sridhar, K. Rosaen, and R. Vasudevan. Driving in the matrix: Can virtual worlds
    replace human-generated annotations for real world tasks? In *IEEE International
    Conference on Robotics and Automation*, pages 746–753\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Khalil and Mouftah (2022) Y. H. Khalil and H. T. Mouftah. Licanet: Further
    enhancement of joint perception and motion prediction based on multi-modal fusion.
    *IEEE Open Journal of Intelligent Transportation Systems*, 3:222–235, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khosravian et al. (2021) A. Khosravian, A. Amirkhani, H. Kashiani, and M. Masih-Tehrani.
    Generalizing state-of-the-art object detectors for autonomous vehicles in unseen
    environments. *Expert Systems with Applications*, 183:115417, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ko et al. (2021) Y. Ko, Y. Lee, S. Azam, F. Munir, M. Jeon, and W. Pedrycz.
    Key points estimation and point instance segmentation approach for lane detection.
    *IEEE Transactions on Intelligent Transportation Systems*, 23(7):8949–8958, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kothandaraman et al. (2021) D. Kothandaraman, A. Nambiar, and A. Mittal. Domain
    adaptive knowledge distillation for driving scene semantic segmentation. In *IEEE/CVF
    Winter Conference on Applications of Computer Vision*, pages 134–143, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar et al. (2021) V. R. Kumar, M. Klingner, S. Yogamani, S. Milz, T. Fingscheidt,
    and P. Mader. Syndistnet: Self-supervised monocular fisheye camera distance estimation
    synergized with semantic segmentation for autonomous driving. In *IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pages 61–71, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lan and Tian (2022) Q. Lan and Q. Tian. Instance, scale, and teacher adaptive
    knowledge distillation for visual detection in autonomous driving. *IEEE Transactions
    on Intelligent Vehicles*, 8(3):2358–2370, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lang et al. (2019) A. H. Lang, S. Vora, H. Caesar, L. Zhou, J. Yang, and O. Beijbom.
    Pointpillars: Fast encoders for object detection from point clouds. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pages 12697–12705, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lehner et al. (2022) A. Lehner, S. Gasperini, A. Marcos-Ramiro, M. Schmidt,
    M.-A. N. Mahani, N. Navab, B. Busam, and F. Tombari. 3d-vfield: Adversarial augmentation
    of point clouds for domain generalization in 3d object detection. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pages 17295–17304, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li and Zhang (2021) D. Li and H. Zhang. Improved regularization and robustness
    for fine-tuning in neural networks. *Advances in Neural Information Processing
    Systems*, 34:27249–27262, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2020a) D. Li, L. Deng, and Z. Cai. Intelligent vehicle network system
    and smart city management based on genetic algorithms and image perception. *Mechanical
    Systems and Signal Processing*, 141:106623, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022a) G. Li, Z. Ji, and X. Qu. Stepwise domain adaptation (sda)
    for object detection in autonomous vehicles using an adaptive centernet. *IEEE
    Transactions on Intelligent Transportation Systems*, 23(10):17729–17743, 2022a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022b) G. Li, Z. Ji, X. Qu, R. Zhou, and D. Cao. Cross-domain object
    detection for autonomous driving: A stepwise domain adaptative yolo approach.
    *IEEE Transactions on Intelligent Vehicles*, 7(3):603–615, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2021) J. Li, Z. Xu, L. Fu, X. Zhou, and H. Yu. Domain adaptation
    from daytime to nighttime: A situation-sensitive vehicle detection and traffic
    flow parameter estimation framework. *Transportation Research Part C: Emerging
    Technologies*, 124:102946, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023a) J. Li, R. Xu, X. Liu, B. Li, Q. Zou, J. Ma, and H. Yu. S2r-vit
    for multi-agent cooperative perception: Bridging the gap from simulation to reality.
    *arXiv preprint arXiv:2307.07935*, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023b) J. Li, R. Xu, X. Liu, J. Ma, Z. Chi, J. Ma, and H. Yu. Learning
    for vehicle-to-vehicle cooperative perception under lossy communication. *IEEE
    Transactions on Intelligent Vehicles*, 2023b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023c) J. Li, R. Xu, J. Ma, Q. Zou, J. Ma, and H. Yu. Domain adaptive
    object detection for autonomous driving under foggy weather. In *IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pages 612–622, 2023c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li and Ibanez-Guzman (2020) Y. Li and J. Ibanez-Guzman. Lidar for autonomous
    driving: The principles, challenges, and trends for automotive lidar and perception
    systems. *IEEE Signal Processing Magazine*, 37(4):50–61, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2020b) Y. Li, L. Ma, Z. Zhong, F. Liu, M. A. Chapman, D. Cao, and
    J. Li. Deep learning for lidar point clouds in autonomous driving: A review. *IEEE
    Transactions on Neural Networks and Learning Systems*, 32(8):3412–3432, 2020b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2022c) Y. Li, Z. Li, S. Teng, Y. Zhang, Y. Zhou, Y. Zhu, D. Cao,
    B. Tian, Y. Ai, Z. Xuanyuan, et al. Automine: An unmanned mine dataset. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pages 21308–21317, 2022c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2023d) Y. Li, D. Zhang, M. Keuper, and A. Khoreva. Intra-source style
    augmentation for improved domain generalization. In *IEEE/CVF Winter Conference
    on Applications of Computer Vision*, pages 509–519, 2023d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2022d) Z. Li, Y. Du, M. Zhu, S. Zhou, and L. Zhang. A survey of 3d
    object detection algorithms for intelligent vehicles development. *Artificial
    Life and Robotics*, pages 1–8, 2022d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2023e) Z. Li, C. Gong, Y. Lin, G. Li, X. Wang, C. Lu, M. Wang, S. Chen,
    and J. Gong. Continual driver behaviour learning for connected vehicles and intelligent
    transportation systems: Framework, survey and challenges. *Green Energy and Intelligent
    Transportation*, page 100103, 2023e.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liang et al. (2022) X. Liang, Y. Liu, T. Chen, M. Liu, and Q. Yang. Federated
    transfer reinforcement learning for autonomous driving. In *Federated and Transfer
    Learning*, pages 357–371\. Springer, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liao et al. (2022) Y. Liao, J. Xie, and A. Geiger. Kitti-360: A novel dataset
    and benchmarks for urban scene understanding in 2d and 3d. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lin et al. (2021) C. Lin, Z. Yuan, S. Zhao, P. Sun, C. Wang, and J. Cai. Domain-invariant
    disentangled network for generalizable object detection. In *IEEE/CVF International
    Conference on Computer Vision*, pages 8771–8780, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2021a) L. Liu, X. Chen, S. Zhu, and P. Tan. Condlanenet: a top-to-down
    lane detection framework based on conditional convolution. In *IEEE/CVF International
    Conference on Computer Vision*, pages 3773–3782, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2017) M.-Y. Liu, T. Breuel, and J. Kautz. Unsupervised image-to-image
    translation networks. *Advances in neural information processing systems*, 30,
    2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022a) P. Liu, C. Zhang, H. Qi, G. Wang, and H. Zheng. Multi-attention
    densenet: A scattering medium imaging optimization framework for visual data pre-processing
    of autonomous driving systems. *IEEE Transactions on Intelligent Transportation
    Systems*, 23(12):25396–25407, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021b) W. Liu, X. Xia, L. Xiong, Y. Lu, L. Gao, and Z. Yu. Automated
    vehicle sideslip angle estimation considering signal measurement characteristic.
    *IEEE Sensors Journal*, 21(19):21675–21687, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Liu et al. (2022b) W. Liu, K. Quijano, and M. M. Crawford. Yolov5-tassel: detecting
    tassels in rgb uav imagery with improved yolov5 based on transfer learning. *IEEE
    Selected Topics in Applied Earth Observations and Remote Sensing*, 15:8085–8094,
    2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2021c) Y. Liu, W. Zhang, and J. Wang. Source-free domain adaptation
    for semantic segmentation. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 1215–1224, 2021c.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2015) M. Long, Y. Cao, J. Wang, and M. Jordan. Learning transferable
    features with deep adaptation networks. In *International Conference on Machine
    Learning*, pages 97–105\. PMLR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long et al. (2017) M. Long, H. Zhu, J. Wang, and M. I. Jordan. Deep transfer
    learning with joint adaptation networks. In *International Conference on Machine
    Learning*, pages 2208–2217\. PMLR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2021) C. Luo, X. Yang, and A. Yuille. Self-supervised pillar motion
    learning for autonomous driving. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pages 3183–3192, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Luo et al. (2022) X. Luo, J. Zhang, K. Yang, A. Roitberg, K. Peng, and R. Stiefelhagen.
    Towards robust semantic segmentation of accident scenes via multi-source mixed
    sampling and meta-learning. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 4429–4439, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mao et al. (2021) J. Mao, M. Niu, C. Jiang, H. Liang, J. Chen, X. Liang, Y. Li,
    C. Ye, W. Zhang, Z. Li, et al. One million scenes for autonomous driving: Once
    dataset. *arXiv preprint arXiv:2106.11037*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Masmoudi et al. (2021) M. Masmoudi, H. Friji, H. Ghazzai, and Y. Massoud. A
    reinforcement learning framework for video frame-based autonomous car-following.
    *IEEE Open Journal of Intelligent Transportation Systems*, 2:111–127, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miglani and Kumar (2019) A. Miglani and N. Kumar. Deep learning models for
    traffic flow prediction in autonomous vehicles: A review, solutions, and challenges.
    *Vehicular Communications*, 20:100184, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mirza et al. (2022) M. J. Mirza, M. Masana, H. Possegger, and H. Bischof. An
    efficient domain-incremental learning approach to drive in all weather conditions.
    In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3001–3011,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mo et al. (2022) Y. Mo, Y. Wu, X. Yang, F. Liu, and Y. Liao. Review the state-of-the-art
    technologies of semantic segmentation based on deep learning. *Neurocomputing*,
    493:626–646, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mohammed et al. (2020) A. S. Mohammed, A. Amamou, F. K. Ayevide, S. Kelouwani,
    K. Agbossou, and N. Zioui. The perception system of intelligent ground vehicles
    in all weather conditions: A systematic literature review. *Sensors*, 20(22):6532,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Murez et al. (2018) Z. Murez, S. Kolouri, D. Kriegman, R. Ramamoorthi, and K. Kim.
    Image to image translation for domain adaptation. In *IEEE Conference on Computer
    Vision and Pattern Recognition*, pages 4500–4509, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mușat et al. (2021) V. Mușat, I. Fursa, P. Newman, F. Cuzzolin, and A. Bradley.
    Multi-weather city: Adverse weather stacking for autonomous driving. In *IEEE/CVF
    International Conference on Computer Vision*, pages 2906–2915, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Niu et al. (2020) S. Niu, Y. Liu, J. Wang, and H. Song. A decade survey of transfer
    learning (2010–2020). *IEEE Transactions on Artificial Intelligence*, 1(2):151–166,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan et al. (2020) F. Pan, I. Shin, F. Rameau, S. Lee, and I. S. Kweon. Unsupervised
    intra-domain adaptation for semantic segmentation through self-supervision. In
    *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 3764–3773,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pan and Yang (2010) S. J. Pan and Q. Yang. A survey on transfer learning. *IEEE
    Transactions on Knowledge and Data Engineering*, 22(10):1345–1359, 2010.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2020) T. Park, A. A. Efros, R. Zhang, and J.-Y. Zhu. Contrastive
    learning for unpaired image-to-image translation. In *Computer Vision–16th European
    Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part IX 16*, pages 319–345\.
    Springer, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Patil et al. (2019) A. Patil, S. Malla, H. Gang, and Y.-T. Chen. The h3d dataset
    for full-surround 3d multi-object detection and tracking in crowded urban scenes.
    In *International Conference on Robotics and Automation*, pages 9552–9557\. IEEE,
    2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Peng et al. (2018) X. Peng, B. Usman, K. Saito, N. Kaushik, J. Hoffman, and
    K. Saenko. Syn2real: A new benchmark forsynthetic-to-real visual domain adaptation.
    *arXiv preprint arXiv:1806.09755*, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pham et al. (2020) Q.-H. Pham, P. Sevestre, R. S. Pahwa, H. Zhan, C. H. Pang,
    Y. Chen, A. Mustafa, V. Chandrasekhar, and J. Lin. A 3d dataset: Towards autonomous
    driving in challenging environments. In *IEEE International Conference on Robotics
    and Automation*, pages 2267–2273\. IEEE, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pizzati et al. (2020) F. Pizzati, R. d. Charette, M. Zaccaria, and P. Cerri.
    Domain bridge for unpaired image-to-image translation and unsupervised domain
    adaptation. In *IEEE/CVF Winter Conference on Applications of Computer Vision*,
    pages 2990–2998, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qian et al. (2022) R. Qian, X. Lai, and X. Li. 3d object detection for autonomous
    driving: A survey. *Pattern Recognition*, 130:108796, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Qiao et al. (2020) F. Qiao, L. Zhao, and X. Peng. Learning to learn single domain
    generalization. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 12556–12565, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rashed et al. (2021) H. Rashed, E. Mohamed, G. Sistu, V. R. Kumar, C. Eising,
    A. El-Sallab, and S. Yogamani. Generalized object detection on fisheye cameras
    for autonomous driving: Dataset, representations and baseline. In *IEEE/CVF Winter
    Conference on Applications of Computer Vision*, pages 2272–2280, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Richter et al. (2016) S. R. Richter, V. Vineet, S. Roth, and V. Koltun. Playing
    for data: Ground truth from computer games. In *Computer Vision–14th European
    Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part
    II 14*, pages 102–118. Springer, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rist et al. (2019) C. B. Rist, M. Enzweiler, and D. M. Gavrila. Cross-sensor
    deep domain adaptation for lidar detection and segmentation. In *IEEE Intelligent
    Vehicles Symposium*, pages 1535–1542. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ros et al. (2016) G. Ros, L. Sellart, J. Materzynska, D. Vazquez, and A. M.
    Lopez. The synthia dataset: A large collection of synthetic images for semantic
    segmentation of urban scenes. In *IEEE Conference on Computer Vision and Pattern
    Recognition*, pages 3234–3243, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruan et al. (2023) J. Ruan, H. Cui, Y. Huang, T. Li, C. Wu, and K. Zhang. A
    review of occluded objects detection in real complex scenarios for autonomous
    driving. *Green Energy and Intelligent Transportation*, page 100092, 2023.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sakaridis et al. (2018) C. Sakaridis, D. Dai, and L. Van Gool. Semantic foggy
    scene understanding with synthetic data. *International Journal of Computer Vision*,
    126:973–992, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sanchez et al. (2022) J. Sanchez, J.-E. Deschaud, and F. Goulette. Domain generalization
    of 3d semantic segmentation in autonomous driving. *arXiv preprint arXiv:2212.04245*,
    2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sautier et al. (2022) C. Sautier, G. Puy, S. Gidaris, A. Boulch, A. Bursuc,
    and R. Marlet. Image-to-lidar self-supervised distillation for autonomous driving
    data. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages
    9891–9901, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schlager et al. (2022a) B. Schlager, T. Goelles, M. Behmer, S. Muckenhuber,
    J. Payer, and D. Watzenig. Automotive lidar and vibration: Resonance, inertial
    measurement unit, and effects on the point cloud. *IEEE Open Journal of Intelligent
    Transportation Systems*, 3:426–434, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schlager et al. (2022b) B. Schlager, T. Goelles, S. Muckenhuber, and D. Watzenig.
    Contaminations on lidar sensor covers: Performance degradation including fault
    detection and modeling as potential applications. *IEEE Open Journal of Intelligent
    Transportation Systems*, 3:738–747, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schutera et al. (2020) M. Schutera, M. Hussein, J. Abhau, R. Mikut, and M. Reischl.
    Night-to-day: Online image-to-image translation for object detection within autonomous
    driving by night. *IEEE Transactions on Intelligent Vehicles*, 6(3):480–489, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shan et al. (2019) Y. Shan, W. F. Lu, and C. M. Chew. Pixel and feature level
    based domain adaptation for object detection in autonomous driving. *Neurocomputing*,
    367:31–38, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen et al. (2021) Z. Shen, J. Liu, Y. He, X. Zhang, R. Xu, H. Yu, and P. Cui.
    Towards out-of-distribution generalization: A survey. *arXiv preprint arXiv:2108.13624*,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shenaj et al. (2023) D. Shenaj, E. Fanì, M. Toldo, D. Caldarola, A. Tavera,
    U. Michieli, M. Ciccone, P. Zanuttigh, and B. Caputo. Learning across domains
    and devices: Style-driven source-free domain adaptation in clustered federated
    learning. In *IEEE/CVF Winter Conference on Applications of Computer Vision*,
    pages 444–454, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shin et al. (2022) I. Shin, Y.-H. Tsai, B. Zhuang, S. Schulter, B. Liu, S. Garg,
    I. S. Kweon, and K.-J. Yoon. Mm-tta: multi-modal test-time adaptation for 3d semantic
    segmentation. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 16928–16937, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2019a) S. Song, H. Yu, Z. Miao, Q. Zhang, Y. Lin, and S. Wang.
    Domain adaptation for convolutional neural networks-based remote sensing scene
    classification. *IEEE Geoscience and Remote Sensing Letters*, 16(8):1324–1328,
    2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Song et al. (2020) S. Song, H. Yu, Z. Miao, J. Fang, K. Zheng, C. Ma, and S. Wang.
    Multi-spectral salient object detection by adversarial domain adaptation. In *AAAI
    Conference on Artificial Intelligence*, volume 34, pages 12023–12030, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2019b) X. Song, P. Wang, D. Zhou, R. Zhu, C. Guan, Y. Dai, H. Su,
    H. Li, and R. Yang. Apollocar3d: A large 3d car instance understanding benchmark
    for autonomous driving. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 5452–5462, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Song et al. (2023) Z. Song, Z. He, X. Li, Q. Ma, R. Ming, Z. Mao, H. Pei, L. Peng,
    J. Hu, D. Yao, et al. Synthetic datasets for autonomous driving: A survey. *arXiv
    preprint arXiv:2304.12205*, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun et al. (2020) P. Sun, H. Kretzschmar, X. Dotiwalla, A. Chouard, V. Patnaik,
    P. Tsui, J. Guo, Y. Zhou, Y. Chai, B. Caine, et al. Scalability in perception
    for autonomous driving: Waymo open dataset. In *IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 2446–2454, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tahir et al. (2021) M. N. Tahir, K. Mäenpää, T. Sukuvaara, and P. Leviäkangas.
    Deployment and analysis of cooperative intelligent transport system pilot service
    alerts in real environment. *IEEE Open Journal of Intelligent Transportation Systems*,
    2:140–148, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Triess et al. (2021) L. T. Triess, M. Dreissig, C. B. Rist, and J. M. Zöllner.
    A survey on deep domain adaptation for lidar perception. In *IEEE Intelligent
    Vehicles Symposium Workshops*, pages 350–357\. IEEE, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Uricar et al. (2021) M. Uricar, G. Sistu, H. Rashed, A. Vobecky, V. R. Kumar,
    P. Krizek, F. Burger, and S. Yogamani. Let’s get dirty: Gan based data augmentation
    for camera lens soiling detection in autonomous driving. In *IEEE/CVF Winter Conference
    on Applications of Computer Vision*, pages 766–775, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Van Brummelen et al. (2018) J. Van Brummelen, M. O’Brien, D. Gruyer, and H. Najjaran.
    Autonomous vehicle perception: The technology of today and tomorrow. *Transportation
    Research Part C: Emerging Technologies*, 89:384–406, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vaswani et al. (2017) A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
    A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. *Advances
    in Neural Information Processing Systems*, 30, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022a) H. Wang, Y. Chen, Y. Cai, L. Chen, Y. Li, M. A. Sotelo,
    and Z. Li. Sfnet-n: An improved sfnet algorithm for semantic segmentation of low-light
    autonomous driving road scenes. *IEEE Transactions on Intelligent Transportation
    Systems*, 23(11):21405–21417, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2022b) J. Wang, C. Lan, C. Liu, Y. Ouyang, T. Qin, W. Lu, Y. Chen,
    W. Zeng, and P. Yu. Generalizing to unseen domains: A survey on domain generalization.
    *IEEE Transactions on Knowledge and Data Engineering*, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang and Yoon (2021) L. Wang and K.-J. Yoon. Knowledge distillation and student-teacher
    learning for visual intelligence: A review and new outlooks. *IEEE Transactions
    on Pattern Analysis and Machine Intelligence*, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2019a) Q. Wang, J. Gao, and X. Li. Weakly supervised adversarial
    domain adaptation for semantic segmentation in urban scenes. *IEEE Transactions
    on Image Processing*, 28(9):4376–4386, 2019a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021a) Q. Wang, D. Dai, L. Hoyer, L. Van Gool, and O. Fink. Domain
    adaptive semantic segmentation with self-supervised depth estimation. In *IEEE/CVF
    International Conference on Computer Vision*, pages 8515–8525, 2021a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) T.-C. Wang, M.-Y. Liu, J.-Y. Zhu, A. Tao, J. Kautz, and B. Catanzaro.
    High-resolution image synthesis and semantic manipulation with conditional gans.
    In *IEEE Conference on Computer Vision and Pattern Recognition*, pages 8798–8807,
    2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2019b) Y. Wang, W.-L. Chao, D. Garg, B. Hariharan, M. Campbell,
    and K. Q. Weinberger. Pseudo-lidar from visual depth estimation: Bridging the
    gap in 3d object detection for autonomous driving. In *IEEE/CVF Conference on
    Computer Vision and Pattern Recognition*, pages 8445–8453, 2019b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023a) Y. Wang, Q. Mao, H. Zhu, J. Deng, Y. Zhang, J. Ji, H. Li,
    and Y. Zhang. Multi-modal 3d object detection in autonomous driving: a survey.
    *International Journal of Computer Vision*, pages 1–31, 2023a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023b) Y. Wang, J. Yin, W. Li, P. Frossard, R. Yang, and J. Shen.
    Ssda3d: Semi-supervised domain adaptation for 3d object detection from point cloud.
    In *AAAI Conference on Artificial Intelligence*, volume 37, pages 2707–2715, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2020) Z. Wang, Y. Wei, R. Feris, J. Xiong, W.-M. Hwu, T. S. Huang,
    and H. Shi. Alleviating semantic-level shift: A semi-supervised domain adaptation
    method for semantic segmentation. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition Workshops*, pages 936–937, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2021b) Z. Wang, Y. Luo, R. Qiu, Z. Huang, and M. Baktashmotlagh.
    Learning to diversify for single domain generalization. In *IEEE/CVF International
    Conference on Computer Vision*, pages 834–843, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023c) Z. Wang, C. Lv, and F.-Y. Wang. A new era of intelligent
    vehicles and intelligent transportation systems: Digital twins and parallel intelligence.
    *IEEE Transactions on Intelligent Vehicles*, 2023c.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen and Jo (2022) L.-H. Wen and K.-H. Jo. Deep learning-based perception systems
    for autonomous driving: A comprehensive survey. *Neurocomputing*, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wilson and Cook (2020) G. Wilson and D. J. Cook. A survey of unsupervised deep
    domain adaptation. *ACM Transactions on Intelligent Systems and Technology*, 11(5):1–46,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu and Deng (2022) A. Wu and C. Deng. Single-domain generalized object detection
    in urban scene via cyclic-disentangled self-distillation. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 847–856, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2019) B. Wu, X. Zhou, S. Zhao, X. Yue, and K. Keutzer. Squeezesegv2:
    Improved model structure and unsupervised domain adaptation for road-object segmentation
    from a lidar point cloud. In *International Conference on Robotics and Automation*,
    pages 4376–4382\. IEEE, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021) X. Wu, Z. Wu, H. Guo, L. Ju, and S. Wang. Dannet: A one-stage
    domain adaptation network for unsupervised nighttime semantic segmentation. In
    *IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 15769–15778,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xie and Du (2022) P. Xie and X. Du. Performance-aware mutual knowledge distillation
    for improving neural architecture search. In *IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 11922–11932, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2019) J. Xu, Y. Nie, P. Wang, and A. M. López. Training a binary
    weight object detector by knowledge transfer for autonomous driving. In *International
    Conference on Robotics and Automation*, pages 2379–2384\. IEEE, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2021) Q. Xu, Y. Zhou, W. Wang, C. R. Qi, and D. Anguelov. Spg: Unsupervised
    domain adaptation for 3d object detection via semantic point generation. In *IEEE/CVF
    International Conference on Computer Vision*, pages 15446–15456, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2022a) Q. Xu, L. Yao, Z. Jiang, G. Jiang, W. Chu, W. Han, W. Zhang,
    C. Wang, and Y. Tai. Dirl: Domain-invariant representation learning for generalizable
    semantic segmentation. In *AAAI Conference on Artificial Intelligence*, volume 36,
    pages 2884–2892, 2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2022b) R. Xu, H. Xiang, X. Xia, X. Han, J. Li, and J. Ma. Opv2v:
    An open benchmark dataset and fusion pipeline for perception with vehicle-to-vehicle
    communication. In *International Conference on Robotics and Automation*, pages
    2583–2589\. IEEE, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2023a) R. Xu, J. Li, X. Dong, H. Yu, and J. Ma. Bridging the domain
    gap for multi-agent perception. *IEEE International Conference on Robotics and
    Automation*, 2023a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Xu et al. (2023b) R. Xu, X. Xia, J. Li, H. Li, S. Zhang, Z. Tu, Z. Meng, H. Xiang,
    X. Dong, R. Song, et al. V2v4real: A real-world large-scale dataset for vehicle-to-vehicle
    cooperative perception. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 13712–13722, 2023b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yan et al. (2018) Y. Yan, Y. Mao, and B. Li. Second: Sparsely embedded convolutional
    detection. *Sensors*, 18(10):3337, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022) C. Yang, H. Zhou, Z. An, X. Jiang, Y. Xu, and Q. Zhang. Cross-image
    relational knowledge distillation for semantic segmentation. In *IEEE/CVF Conference
    on Computer Vision and Pattern Recognition*, pages 12319–12328, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2020) Z. Yang, Y. Chai, D. Anguelov, Y. Zhou, P. Sun, D. Erhan,
    S. Rafferty, and H. Kretzschmar. Surfelgan: Synthesizing realistic sensor data
    for autonomous driving. In *IEEE/CVF Conference on Computer Vision and Pattern
    Recognition*, pages 11118–11127, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ye et al. (2021) L. Ye, Z. Wang, X. Chen, J. Wang, K. Wu, and K. Lu. Gsan:
    Graph self-attention network for learning spatial–temporal interaction representation
    in autonomous driving. *IEEE Internet of Things Journal*, 9(12):9190–9204, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yeong et al. (2021) D. J. Yeong, G. Velasco-Hernandez, J. Barry, and J. Walsh.
    Sensor and sensor fusion technology in autonomous vehicles: A review. *Sensors*,
    21(6):2140, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yi et al. (2021) L. Yi, B. Gong, and T. Funkhouser. Complete & label: A domain
    adaptation approach to semantic segmentation of lidar point clouds. In *IEEE/CVF
    Conference on Computer Vision and Pattern Recognition*, pages 15363–15373, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yu et al. (2018) F. Yu, W. Xian, Y. Chen, F. Liu, M. Liao, V. Madhavan, and
    T. Darrell. Bdd100k: A diverse driving video database with scalable annotation
    tooling. *arXiv preprint arXiv:1805.04687*, 2(5):6, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yu et al. (2022) G. Yu, H. Li, Y. Wang, P. Chen, and B. Zhou. A review on cooperative
    perception and control supported infrastructure-vehicle system. *Green Energy
    and Intelligent Transportation*, page 100023, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yue et al. (2019) X. Yue, Y. Zhang, S. Zhao, A. Sangiovanni-Vincentelli, K. Keutzer,
    and B. Gong. Domain randomization and pyramid consistency: Simulation-to-real
    generalization without accessing target domain data. In *IEEE/CVF International
    Conference on Computer Vision*, pages 2100–2110, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yurtsever et al. (2020) E. Yurtsever, J. Lambert, A. Carballo, and K. Takeda.
    A survey of autonomous driving: Common practices and emerging technologies. *IEEE
    Access*, 8:58443–58469, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zagoruyko and Komodakis (2016) S. Zagoruyko and N. Komodakis. Paying more attention
    to attention: Improving the performance of convolutional neural networks via attention
    transfer. In *International Conference on Learning Representations*, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2021a) J. Zhang, C. Ma, K. Yang, A. Roitberg, K. Peng, and R. Stiefelhagen.
    Transfer beyond the field of view: Dense panoramic semantic segmentation via unsupervised
    domain adaptation. *IEEE Transactions on Intelligent Transportation Systems*,
    23(7):9478–9491, 2021a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2020) W. Zhang, Z. Wang, and C. C. Loy. Exploring data augmentation
    for multi-modality 3d object detection. *arXiv preprint arXiv:2012.12741*, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2021b) X. Zhang, H. Zhang, J. Lu, L. Shao, and J. Yang. Target-targeted
    domain adaptation for unsupervised semantic segmentation. In *IEEE International
    Conference on Robotics and Automation*, pages 13560–13566\. IEEE, 2021b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022) X. Zhang, N. Tseng, A. Syed, R. Bhasin, and N. Jaipuria.
    Simbar: Single image-based scene relighting for effective data augmentation for
    automated driving vision tasks. In *IEEE/CVF Conference on Computer Vision and
    Pattern Recognition*, pages 3718–3728, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2018) Y. Zhang, T. Xiang, T. M. Hospedales, and H. Lu. Deep mutual
    learning. In *IEEE Conference on Computer Vision and Pattern Recognition*, pages
    4320–4328, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2020) X. Zhao, P. Sun, Z. Xu, H. Min, and H. Yu. Fusion of 3d lidar
    and camera data for object detection in autonomous vehicle applications. *IEEE
    Sensors Journal*, 20(9):4901–4913, 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022) Y. Zhao, Z. Zhong, N. Zhao, N. Sebe, and G. H. Lee. Style-hallucinated
    dual consistency learning for domain generalized semantic segmentation. In *Computer
    Vision–17th European Conference, Tel Aviv, Israel, October 23–27, 2022, Proceedings,
    Part XXVIII*, pages 535–552\. Springer, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2016) B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba.
    Learning deep features for discriminative localization. In *IEEE Conference on
    Computer Vision and Pattern Recognition*, pages 2921–2929, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2019) D. Zhou, Z. Ma, and J. Sun. Autonomous vehicles’ turning
    motion planning for conflict areas at mixed-flow intersections. *IEEE Transactions
    on Intelligent Vehicles*, 5(2):204–216, 2019.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020a) D. Zhou, J. Fang, X. Song, L. Liu, J. Yin, Y. Dai, H. Li,
    and R. Yang. Joint 3d instance segmentation and object detection for autonomous
    driving. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition*,
    pages 1839–1849, 2020a.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2022a) K. Zhou, Z. Liu, Y. Qiao, T. Xiang, and C. C. Loy. Domain
    generalization: A survey. *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    2022a.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2022b) Y. Zhou, L. Liu, H. Zhao, M. López-Benítez, L. Yu, and
    Y. Yue. Towards deep radar perception for autonomous driving: Datasets, methods,
    and challenges. *Sensors*, 22(11):4208, 2022b.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2020b) Z. Zhou, Z. Wang, H. Lu, S. Wang, and M. Sun. Multi-type
    self-attention guided degraded saliency detection. In *AAAI Conference on Artificial
    Intelligence*, volume 34, pages 13082–13089, 2020b.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhu et al. (2017) J.-Y. Zhu, T. Park, P. Isola, and A. A. Efros. Unpaired image-to-image
    translation using cycle-consistent adversarial networks. In *IEEE International
    Conference on Computer Vision*, pages 2223–2232, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhuang et al. (2020) F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong,
    and Q. He. A comprehensive survey on transfer learning. *IEEE*, 109(1):43–76,
    2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ziegler and Asano (2022) A. Ziegler and Y. M. Asano. Self-supervised learning
    of object parts for semantic segmentation. In *IEEE/CVF Conference on Computer
    Vision and Pattern Recognition*, pages 14502–14511, 2022.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
