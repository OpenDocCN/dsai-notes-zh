- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:44:49'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2208.10413] On Deep Learning in Password Guessing, a Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.10413](https://ar5iv.labs.arxiv.org/html/2208.10413)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: On Deep Learning in Password Guessing, a Survey
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fangyi Yu [fangyi.yu@ontariotechu.net](mailto:fangyi.yu@ontariotechu.net) Ontario
    Tech University2000 Simcoe St NOshawaOntarioCanadaL1G 0C5
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The security of passwords is dependent on a thorough understanding of the strategies
    used by attackers. Unfortunately, real-world adversaries use pragmatic guessing
    tactics like dictionary attacks, which are difficult to simulate in password security
    research. Dictionary attacks must be carefully configured and modified to be representative
    of the actual threat. This approach, however, needs domain-specific knowledge
    and expertise that are difficult to duplicate. This paper compares various deep
    learning-based password guessing approaches that do not require domain knowledge
    or assumptions about users’ password structures and combinations. The involved
    model categories are [Recurrent Neural Networks (RNN)](#glo.acronym.rnn), [Generative
    Adversarial Networks (GANs)](#glo.acronym.gan), autoencoder and attention mechanisms.
    Additionally, we proposed a promising research experimental design on using variations
    of [Improved Training of Wasserstein GANs (IWGAN)](#glo.acronym.iwgan) on password
    guessing under non-targeted offline attacks. Using these advanced strategies,
    we can enhance password security and create more accurate and efficient [Password
    Strength Meters (PSMs)](#glo.acronym.psm).
  prefs: []
  type: TYPE_NORMAL
- en: Authentication, Deep Learning, Generative Adversarial Learning
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Passwords have dominated the authentication system for decades, despite their
    security flaws compared to competing techniques such as cognitive authentication
    and hardware tokens. The irreplaceable is primarily due to its incomparable deployability
    and usability([bonneau2012quest,](#bib.bib2) ). However, the security of user-selected
    passwords continues to be a significant concern. According to research examining
    susceptible behaviours that affect password crackability([adams1999users,](#bib.bib1)
    ), there are three types of user actions that result in the creation of insecure
    passwords: 1\. Users often use basic terms in passwords and perform simple string
    transformations to comply with websites’ password creation policies([narayanan2005fast,](#bib.bib20)
    ). 2\. Password reuse is prevalent since the typical user has more than 20 accounts,
    and establishing unique passwords and remembering them for each account is too
    time-consuming([stobert2014password,](#bib.bib27) ). According to S.Pearman et
    al., 40% of users repeat their passwords([pearman2017let,](#bib.bib23) ). 3\.
    Users prefer to use simple-to-remember passwords that include personal information
    such as their birth date and their pets’ name. All of these behaviours expose
    the user-created passwords to attacks. Additionally, the recent large-scale leakage
    of passwords on multiple platforms across the world (listed in Table[2](#S2.T2
    "Table 2 ‣ 2.3.3\. Attention-based models ‣ 2.3\. Deep Learning-base models ‣
    2\. Background and Related Work ‣ On Deep Learning in Password Guessing, a Survey"))
    raises the alarm for researchers.'
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Offline attacks and Online attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Password guessing attacks fall into two categories: offline and online. Offline
    attacks occur when attackers get cryptographic hashes of certain users’ passwords
    and attempt to recover them by guessing and testing many passwords. The primary
    objective is to determine the difficulty of cracking a genuine user’s password,
    or the strength of a user-created password, by producing a list of password guesses
    and checking for the possibility of the genuine user’s password’s occurrence.
    Offline attacks are only considered when the following conditions are met: An
    attack gains access to the system and extracts the password file, all while remaining
    unnoticed. Moreover, the file’s salting and hashing must be done appropriately.
    Otherwise, an offline assault is either ineffective (the attacker may get credentials
    directly without requiring guesses, or an online approach is more effective),
    or impossible([florencio2014administrator,](#bib.bib7) ).'
  prefs: []
  type: TYPE_NORMAL
- en: An online attack occurs when an attacker makes password attempts against users
    using a web interface or an application. This situation is more constrained for
    attackers since most authentication systems automatically freeze accounts after
    several unsuccessful attempts. Therefore, attackers must guess users’ passwords
    successfully within the allotted number of tries, which is the primary difficulty
    of online password guessing. According to Florencio et al. ([florencio2014administrator,](#bib.bib7)
    ), $10^{6}$ is a reasonable upper limit for the number of online guesses a secure
    password must survive, while the number of offline guesses is difficult to quantify
    considering the attacker’s possible usage of unlimited computers each calculating
    hashes thousands of times quicker than the target site’s backend server.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Targeted attacks and non-targeted attacks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Targeted guessing attacks occur when attackers attempt to break users’ passwords
    using their knowledge on users, specifically their [Personally Identifiable Information
    (PII)](#glo.acronym.pii), such as name, birth date, anniversary, home address
    etc,. This is a considerable concern when [PII](#glo.acronym.pii) becomes more
    accessible as a result of constant data breaches. On the contrary, non-targeted
    attacks do not assume the users’ identities. Be aware that the majority of individual
    accounts are not deserving of concentrated attention. Only significant or vital
    accounts relating to critical work, finances, or documents demanding a high level
    of security may be considered([florencio2014administrator,](#bib.bib7) ).
  prefs: []
  type: TYPE_NORMAL
- en: This paper primarily focus on utilizing deep learning to increase passwords
    guessability under the offline and non-targeted scenario.
  prefs: []
  type: TYPE_NORMAL
- en: 1.3\. Contributions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are the primary contributions of this paper:'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It provides an in-depth comparison of deep learning techniques used for untargeted
    offline password guessing.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It identifies the open challenges and possible directions for future study in
    this topic.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It proposes a viable research approach for password guessing using [GANs](#glo.acronym.gan)
    and a feasible experimental design.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The rest of the paper is divided into the following sections: Section 2 discusses
    three approaches for password guessing: rule-based, probability-based, and deep
    learning-based models. The third section discusses methodology, datasets, and
    experimental design, and section 4 discusses unresolved issues and future work.
    The last part contains abbreviations and a bibliography.'
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Background and Related Work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The three predominant ways of password guessing are rule-based, probability-based,
    and deep learning-based.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1\. Rule-based models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The large amount of stolen passwords simplifies the process of collecting password
    patterns. Following that, other candidate passwords may be produced using these
    password patterns as guidelines. Hashcat¹¹1https://hashcat.net/wiki/ and John
    the Ripper ²²2https://www.openwall.com/john/are two popular open-source password
    guessing programs that use rule-based password guessing. They provide a variety
    of ways for cracking passwords, including dictionary attacks, brute-force attacks,
    and rule-based attacks. Among all these types, the rule-based one is the fastest,
    and HashCat is the market leader in terms of speed, hash function compatibility,
    updates, and community support([hranicky2019distributed,](#bib.bib12) ). However,
    rule-based systems create passwords solely based on pre-existing rules, and developing
    rules requires domain expertise. Once rules are defined, passwords that violate
    those restrictions will not be identified.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Probability-base models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Apart from rule-based password guessing models, conventional password guessing
    models are mostly probability-based, with two notable approaches being Markov
    Models and [Probabilistic Context-Free Grammar (PCFG)](#glo.acronym.pcfg). Markov
    Models are built on the assumption that all critical password features can be
    specified in n-grams. Its central principle is to predict the next character based
    on the preceding character([narayanan2005fast,](#bib.bib20) ). [PCFG](#glo.acronym.pcfg)
    examines the grammatical structures (combinations of special characters, digits,
    and alphanumerical sequences) in disclosed passwords and generates the distribution
    probability, after which it uses the distribution probability to produce password
    candidates([weir2009password,](#bib.bib33) ).
  prefs: []
  type: TYPE_NORMAL
- en: 2.3\. Deep Learning-base models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike rule-based or probability-based password guessing tools, deep learning-based
    methods make no assumptions about the password structure. Deep neural network-generated
    password samples are not constrained to a particular subset of the password space.
    Rather than that, neural networks can autonomously encode a broad range of password
    information beyond the capabilities of human-generated rules and Markovian password-generating
    methods.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.1\. Recurrent Neural Networks
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '[RNN](#glo.acronym.rnn) are neural networks in which inputs are processed sequentially
    and restored using internal memory. They are often employed to solve sequential
    tasks such as language translation, natural language processing, voice recognition,
    and image recognition. Due to the fact that the vanilla [RNN](#glo.acronym.rnn)
    architecture is incapable of processing long-term dependencies due to the vanishing
    gradient issue([pascanu2013difficulty,](#bib.bib21) ), therefore, [Long Short
    Term Memory Networks (LSTM)](#glo.acronym.lstm) was designed to tackle the problem.
    The [LSTM](#glo.acronym.lstm) makes use of the gating mechanism depicted in Figure[1](#S2.F1
    "Figure 1 ‣ 2.3.1\. Recurrent Neural Networks ‣ 2.3\. Deep Learning-base models
    ‣ 2\. Background and Related Work ‣ On Deep Learning in Password Guessing, a Survey")
    to retain information in memory for long periods of time([hochreiter1997long,](#bib.bib11)
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/52161e0135b719f55e09a4e02217cbb8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. The Gating Mechanism in [LSTM](#glo.acronym.lstm).
  prefs: []
  type: TYPE_NORMAL
- en: To the best of our knowledge, Melicher et al.([melicher2016fast,](#bib.bib17)
    ) were the first to utilize [RNN](#glo.acronym.rnn) to extract and predict password
    features. They kept their model, named [Fast, Lean, Accurate (FLA)](#glo.acronym.fla),
    as lightweight as possible in order to integrate it into local browsers for proactive
    password verification. Three [LSTM](#glo.acronym.lstm) layers and two highly connected
    layers comprise the proposed Neural Network. Various strategies for training neural
    networks on passwords were used. It was proven that employing transfer learning([yosinski2014transferable,](#bib.bib36)
    ) significantly improves guessing efficacy, however, adding natural language dictionaries
    to the training set and tutoring had little impact. Consequently, they discovered
    that Neural Networks are superior at guessing passwords when the number of guesses
    is increased and when more complicated or longer password policies are targeted.
    Nevertheless, because of the Markovian nature of [FLA](#glo.acronym.fla)’s password
    generation process, any password feature that is not included within the scope
    of an n-gram may be omitted from encoding([hitaj2019passgan,](#bib.bib10) ).
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al.([zhang2018password,](#bib.bib37) ) presented [Structure Partition
    and Bilstm Recurrent Neural Network (SPRNN)](#glo.acronym.sprnn), a hybrid password
    attack technique based on structural partitioning and [Bidirectional Long Short-term
    Memory (BiLSTM)](#glo.acronym.bilstm). The [PCFG](#glo.acronym.pcfg) is used for
    structure partitioning, which seeks to structure the password training set to
    learn users’ habit of password construction and generate a collection of basic
    structures and string dictionaries ordered by likelihood. The [BiLSTM](#glo.acronym.bilstm)
    was then trained using the string dictionary produced by [PCFG](#glo.acronym.pcfg).
    They compared SPRNN’s performance to probability-based approaches (Markov Models
    and [PCFG](#glo.acronym.pcfg)) on both cross-site (model trained and tested on
    various datasets) and one-site (model trained and tested on subsets of the same
    dataset) scenarios. [SPRNN](#glo.acronym.sprnn) outperforms the other two models
    in all circumstances, albeit it performs worse cross-site than one-site.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on Zhang et al.’s work([zhang2018password,](#bib.bib37) ), the same year,
    Liu et al.([liu2018genpass,](#bib.bib16) ) also developed a hybrid model named
    GENPass that can be generalized to cross-sites attacks. The model preprocesses
    a password by encoding it into a series of units that are then given tags based
    on [PCFG](#glo.acronym.pcfg) (e.g., ’password123’ can be separated into two units:
    ’D8’ and ’L3’). After that, [LSTM](#glo.acronym.lstm) is used to create passwords.
    Additionally, they built a [Convolutional Neural Networks (CNN)](#glo.acronym.cnn)
    classifier to determine which wordlist the password is most likely to originate
    from. The results indicate that GENPass can achieve the same degree of security
    as the [LSTM](#glo.acronym.lstm) model alone in a one-site test while generating
    passwords with a substantially lower rank. GENPass enhanced the matching rate
    by 16 to 30% when compared to [LSTM](#glo.acronym.lstm) alone in the cross-site
    test.'
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.2\. Autoencoders
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Autoencoders are any model architecture that is composed of two submodules:
    an encoder and a decoder. The encoder is responsible for learning the representation
    of the source text at each time step and generating a latent representation of
    the whole source sentence, which the decoder uses as an input to build a meaningful
    output of the original phrase. Typically, autoencoders are employed to deal with
    sequential data and various NLP tasks, such as machine translation, text summarization,
    and question answering. [RNN](#glo.acronym.rnn) and [CNN](#glo.acronym.cnn) are
    often used as encoder and decoder components, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: Pasquini et al.([pasquini2021improving,](#bib.bib22) ) applied this strategy
    to a dataset containing leaked passwords, using [GANs](#glo.acronym.gan) and Wasserstein
    Autoencoders (WAEs) to develop a suitable representation of the observed password
    distribution rather than directly predicting it. Their methodology, called [Dynamic
    Password Guessing (DPG)](#glo.acronym.dpg), can guess passwords that are unique
    to the password set. and they are the first to apply completely unsupervised representation
    learning to the area of password guessing.
  prefs: []
  type: TYPE_NORMAL
- en: 2.3.3\. Attention-based models
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: When we use the phrase ”Attention” in English, we mean concentrating our focus
    on something and paying closer attention. The Attention mechanism in Deep Learning
    is based on this principle, and it prioritizes certain tokens (words, letters,
    and phrases) while processing text inputs. This, intuitively, aids the model in
    gaining a better knowledge of the textual structure (e.g., grammar, semantic meaning,
    word structure, and so on) and hence improve text classification, generation and
    interpretability. In language models, attention mechanisms are often utilized
    in combination with [RNN](#glo.acronym.rnn) and [CNN](#glo.acronym.cnn). However,
    even with [LSTM](#glo.acronym.lstm), these models cannot manage lengthy dependencies
    since transforming the whole source sentence to a fixed-length context vector
    is challenging when the source sentence is too long. As a result, Transformers([vaswani2017attention,](#bib.bib31)
    ) were invented that were built just on Attention, without convolution or recurrent
    layers. [Bidirectional Encoder Representations from Transformers (BERT)](#glo.acronym.bert)
    ([devlin2018bert,](#bib.bib6) ), ELMO([peters2018deep,](#bib.bib24) ), and GPT([radford2018improving,](#bib.bib26)
    ) are all well-known instances of attention-based applications built on top of
    Transformers.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. A comparison for Deep Learning Models used for Password Guessing.
  prefs: []
  type: TYPE_NORMAL
- en: '| Category | Methods | Models used | Year |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Autoencoders | [DPG](#glo.acronym.dpg) ([pasquini2021improving,](#bib.bib22)
    ) | WAE, [GANs](#glo.acronym.gan) | 2021 |'
  prefs: []
  type: TYPE_TB
- en: '| [GANs](#glo.acronym.gan) | REDPACK([nam2020generating,](#bib.bib19) ) | [IWGAN](#glo.acronym.iwgan),
    RaGAN, HashCat, [PCFG](#glo.acronym.pcfg) | 2020 |'
  prefs: []
  type: TYPE_TB
- en: '| [GANs](#glo.acronym.gan) | PassGAN([hitaj2019passgan,](#bib.bib10) ) | [IWGAN](#glo.acronym.iwgan)
    | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| Attention | Language Model([li2019password,](#bib.bib15) ) | [BERT](#glo.acronym.bert),
    [LSTM](#glo.acronym.lstm) | 2019 |'
  prefs: []
  type: TYPE_TB
- en: '| [RNN](#glo.acronym.rnn) | GENPass([liu2018genpass,](#bib.bib16) ) | [PCFG](#glo.acronym.pcfg),
    [LSTM](#glo.acronym.lstm), [CNN](#glo.acronym.cnn) | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| [RNN](#glo.acronym.rnn) | SPRNN([zhang2018password,](#bib.bib37) ) | [PCFG](#glo.acronym.pcfg),
    [BiLSTM](#glo.acronym.bilstm) | 2018 |'
  prefs: []
  type: TYPE_TB
- en: '| [RNN](#glo.acronym.rnn) | [FLA](#glo.acronym.fla) ([melicher2016fast,](#bib.bib17)
    ) | [LSTM](#glo.acronym.lstm) | 2016 |'
  prefs: []
  type: TYPE_TB
- en: Li et al.([li2019password,](#bib.bib15) ) proposed a curated Deep Neural Network
    architecture consisted of five [LSTM](#glo.acronym.lstm) layers and an output
    layer, and then tutored and improved the created model using [BERT](#glo.acronym.bert).
    They proved that the tutoring process by [BERT](#glo.acronym.bert) can help increase
    the model performance significantly.
  prefs: []
  type: TYPE_NORMAL
- en: A comparison of prior published deep learning-based password guessing tools
    is illustrated in Table[1](#S2.T1 "Table 1 ‣ 2.3.3\. Attention-based models ‣
    2.3\. Deep Learning-base models ‣ 2\. Background and Related Work ‣ On Deep Learning
    in Password Guessing, a Survey").
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Datasets used for training and evaluating deep learning models. Size
    is the number of passwords in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '| Name | Size | Brief Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Yahoo | $4.4\times 10^{5}$ | A web services provider. |'
  prefs: []
  type: TYPE_TB
- en: '| phpBB | $3\times 10^{5}$ | A software website. |'
  prefs: []
  type: TYPE_TB
- en: '| RockYou | $1.4\times 10^{7}$ | A gaming platform. |'
  prefs: []
  type: TYPE_TB
- en: '| Myspace | $5.5\times 10^{4}$ | A social networking platform. |'
  prefs: []
  type: TYPE_TB
- en: '| SkullSecurityComp | $6.7\times 10^{6}$ | Compilations of passwords lists.
    |'
  prefs: []
  type: TYPE_TB
- en: '| LinkedIn | $1.3\times 10^{6}$ | A social online platform. |'
  prefs: []
  type: TYPE_TB
- en: 3\. Methodology
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 3.1\. Problem Statement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is a consensus in academia that password strength estimates based on entropy,
    and marginal guesswork analysis([pliam2000incomparability,](#bib.bib25) ) are
    insufficient. Rather than that, it is more suitable to quantify it in terms of
    the number of attempts required to find a password collision using the most efficient
    adversarial attacks([dell2015monte,](#bib.bib5) ). Thus, password guessing attacks
    can be used for password security analysis and be developed into [PSMs](#glo.acronym.psm)
    that can be integrated into an intelligent user interface to encourage users to
    establish stronger passwords. I will concentrate my study on utilizing [GANs](#glo.acronym.gan)
    to complete the password guessing task.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. [GANs](#glo.acronym.gan)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Unlike the previously described deep learning-based algorithms commonly employed
    in Natural Language Processing tasks, [GANs](#glo.acronym.gan) ([goodfellow2014generative,](#bib.bib8)
    ) has been used to construct simulations of pictures, texts, and voice across
    all domains. Behind the scenes, [GANs](#glo.acronym.gan) consists of two sub-modules:
    a discriminator (D) and a generator (G), both of which are built of deep learning
    neural networks. G accepts noise or random features as input; learns the probability
    of the input’s features; and generates fake data that follows the distribution
    of the input data. While D makes every effort to discriminate between actual samples
    and those created artificially by G by estimating the conditional probability
    of an example being false (or real) given a set of inputs (or features). The model
    architecture diagram is illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.2\. GANs
    ‣ 3\. Methodology ‣ On Deep Learning in Password Guessing, a Survey"). This cat-and-mouse
    game compels D to extract necessary information in training data; this information
    assists G in precisely replicating the original data distribution. D and G compete
    against one another during the training phase, which progressively improves their
    performance with each iteration. Typically, proper gradient descent and regularization
    techniques must be used to accelerate the whole process. More formally, the optimization
    problem solved by [GANs](#glo.acronym.gan) can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | $\displaystyle\min_{\theta_{G}}\max_{\theta_{D}}\sum_{i=1}^{n}logf(x_{i};\theta_{D})+\sum_{i=1}^{n}log(1-f(g(z_{j};\theta_{G});\theta_{D}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $f(x;\theta_{D})$ and $g(z_{j};\theta_{G})$ represents the discriminator
    D and the generator G respectively.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/14f16859dbfb19f81caa7ba0a5319372.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Model architecture diagram.
  prefs: []
  type: TYPE_NORMAL
- en: The optimization demonstrates the min-max game between D and G. After the initial
    [GANs](#glo.acronym.gan) work was published in 2014, several enhancements were
    made, and Hitaj et al.([hitaj2019passgan,](#bib.bib10) ) leveraged the [IWGAN](#glo.acronym.iwgan)
    ([gulrajani2017improved,](#bib.bib9) ) to apply [GANs](#glo.acronym.gan) on password
    guessing, which is the first in literature. They trained the discriminator using
    a collection of leaked passwords (actual samples). Each iteration brings the generator’s
    output closer to the distribution of genuine passwords, increasing the likelihood
    of matching real-world users’ passwords. Consequently, PassGAN outperformed current
    rule-based password guessing tools and state-of-the-art machine learning password
    guessing technologies ([FLA](#glo.acronym.fla)) after sufficient passwords were
    generated ($10^{9}$). They matched 51% - 73% of passwords when combining PassGAN
    with HashCat, compared to 17.67% when using HashCat alone and 21% when using PassGAN
    alone. One disadvantage of PassGAN is that it has intrinsic training instability
    due to the final softmax activation function in the generator, which may easily
    result in the network being vulnerable to vanishing gradients, lowering the guessing
    accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Following the publication of PassGAN in 2019, other researchers saw the possibilities
    of using [GANs](#glo.acronym.gan) for password guessing, and more refinements
    have been done on top of PassGAN. In 2020, Nam et al.([nam2020generating,](#bib.bib19)
    ) developed REDPACK that employs a variant of [GANs](#glo.acronym.gan) in conjunction
    with various password generation models for improved cracking performance. They
    suggested rPassGAN in their prior study, which enhanced PassGAN by altering its
    fundamental Neural Network architecture. More precisely, they employed [RNN](#glo.acronym.rnn)
    in PassGAN instead of ResNet in PassGAN’s original paper. However, during rPassGAN’s
    training process, it became unstable at times, and REDPACK introduced the RaSGGAN-GP
    cost function to stabilize the training process. Nam et al. also introduced a
    selection phase to REDPACK, during which the password candidates are generated
    using several password generators (Hashcat, acpcfg, and rPassGAN). The discriminator
    then determines the chance of each generator’s password candidates being realistic
    and sends the candidates with the greatest probability to password cracking tools
    such as HashCat. We regard PassGAN to be a good representation of [GANs](#glo.acronym.gan)-based
    password guessing tools, and PassGAN enhancements will be the focus of my study.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table [2](#S2.T2 "Table 2 ‣ 2.3.3\. Attention-based models ‣ 2.3\. Deep Learning-base
    models ‣ 2\. Background and Related Work ‣ On Deep Learning in Password Guessing,
    a Survey") summarizes the publicly accessible datasets that are often used in
    password guessing. While one disadvantage of prior work on the dataset is that
    most studies did not include the critical nature of password policy. In reality,
    most websites impose password policy restrictions to encourage users to generate
    strong passwords. For example, create a password with at least one special symbol
    and eight characters in both lowercase and uppercase([bonneau2010password,](#bib.bib3)
    ). In my research, I will consider the password policy when preprocessing datasets
    to imitate real-world password creation circumstances more accurately.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset that I can use for training and evaluation:[Weakpass](https://weakpass.com/wordlist/1920)
    (Contains only passwords that are suitable for password policy, length more than
    8, with 2133708093 passwords) and the smaller dataset from [GitHub](https://github.com/danielmiessler/SecLists/tree/master/Passwords/Leaked-Databases).
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Experimental design
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Tensorflow or Pytorch serves as the primary framework for the experiments. Reproducing
    PassGAN’s work([hitaj2019passgan,](#bib.bib10) ) is the first step in my research.
    However, there are limitations of PassGAN and to address which and improve PassGAN
    will be the focus of my work. The vanilla [GANs](#glo.acronym.gan) stays unstable
    throughout the training process because the discriminator’s steep gradient space,
    which results in mode collapse during the generator’s training phase([thanh2019improving,](#bib.bib30)
    ). As a consequence, the generator is prone to deceive the discriminator before
    mastering the art of creating more realistic passwords. Although the [IWGAN](#glo.acronym.iwgan)
    utilized in PassGAN uses the Wasserstein Loss function, which, in comparison to
    the Binary Cross Entropy Loss used in the vanilla [GANs](#glo.acronym.gan), aids
    in the resolution of mode collapse and vanishing gradient difficulties in some
    degree. However, in order to train [GANs](#glo.acronym.gan) using Wasserstein-loss,
    the discriminator must be 1-Lipschitz continuous, which means that the gradient’s
    norm should be at most 1 at every point. While [IWGAN](#glo.acronym.iwgan) used
    a gradient penalty to guarantee 1-Lipschitz continuity, Wu et al.([GNGAN_2021_ICCV,](#bib.bib35)
    ) proved that ”the Lipschitz constant of a layer-wise Lipschitz constraint network
    is upper-bounded by any of its k-layer subnetworks.” Thus, I will explore alternative
    techniques aimed at fulfilling the Lipschitz constraint while applying Wasserstein
    loss or stabilizing the training process by further regularization and normalizing
    on the discriminator([GNGAN_2021_ICCV,](#bib.bib35) ; [gulrajani2017improved,](#bib.bib9)
    ; [jabbar2021survey,](#bib.bib13) ; [thanh2019improving,](#bib.bib30) ; [wei2018improving,](#bib.bib32)
    ; [wu2018wasserstein,](#bib.bib34) ; [terjek2019adversarial,](#bib.bib29) ; [jiang2018computation,](#bib.bib14)
    ). By comparing the matching result of the testing set to the synthesized set,
    we can determine if the model we build can outperform PassGAN.
  prefs: []
  type: TYPE_NORMAL
- en: Table 3\. Unique password distribution in the Rockyou dataset
  prefs: []
  type: TYPE_NORMAL
- en: '| Length | Frequency |'
  prefs: []
  type: TYPE_TB
- en: '| $<8$ | 15.55% |'
  prefs: []
  type: TYPE_TB
- en: '| 8-10 | 38.16% |'
  prefs: []
  type: TYPE_TB
- en: '| 10-12 | 29.32% |'
  prefs: []
  type: TYPE_TB
- en: '| 12-16 | 14.18% |'
  prefs: []
  type: TYPE_TB
- en: '| $>=16$ | 2.80% |'
  prefs: []
  type: TYPE_TB
- en: '| Total | $1.43*10^{7}$ |'
  prefs: []
  type: TYPE_TB
- en: The datasets listed in Table[2](#S2.T2 "Table 2 ‣ 2.3.3\. Attention-based models
    ‣ 2.3\. Deep Learning-base models ‣ 2\. Background and Related Work ‣ On Deep
    Learning in Password Guessing, a Survey") will be used to train and test my curated
    model. Small, medium and big password candidate files will be produced to evaluate
    the model’s guessing capability. The big one may be up to 6.5 GB in size and include
    over $8\times 10^{9}$ unique passwords. To train a deep learning model and to
    produce such an enormous dataset requires substantial computational resources.
    A Tesla P100 GPU or above is required to reduce training and generation time.
    A system with at least 16 GB of RAM is required to prevent the process from being
    killed due to running out of memory while reading the created file to calculate
    guessing accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6149ac5a3cb26a65ef125d85f5802cfd.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Data Flow Diagram.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the work is based on the assumption that the training set and testing
    set has the same distribution, therefore by simulating samples from the training
    set, the produced samples can substantially approximating the test set. The two
    sets are produced by dividing leaked passwords to 4:1 in portion, and no duplicate
    exist in the two sets. By shuffling before splitting, we assume that the two sets
    have the same distribution. The dataflow diagram is illustrated in Figure[3](#S3.F3
    "Figure 3 ‣ 3.4\. Experimental design ‣ 3\. Methodology ‣ On Deep Learning in
    Password Guessing, a Survey").
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Experimental results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: I used Rockyou Dataset to train the [IWGAN](#glo.acronym.iwgan) model, the dataset
    it preprocessed so that all passwords are unique, there is no password both exiting
    in the testing set and training set. A distribution of the passwords based on
    length is illustrated in Table[3](#S3.T3 "Table 3 ‣ 3.4\. Experimental design
    ‣ 3\. Methodology ‣ On Deep Learning in Password Guessing, a Survey"). There are
    two minor modifications in my experiment and Hitaj et al. ([hitaj2019passgan,](#bib.bib10)
    )’s. First, when preprocessing the dataset, I restrict all passwords to 8 to 12
    in length, which is more in line with the modern password creating policy. While
    Hitaj et al. limited all passwords to less than 10\. However, in the real world
    scenario, a password with less than eight characters is regarded as weak and even
    a rule-based or probability-based password guessing tool can easily guess them.
    In my work, I want to demonstrate the capability of deep learning-based password
    guessing tools against other kinds, so the password policy restriction is considered.
    Second, I modified the activation function in the last layer of the Generator
    to Tanh instead of Softmax, and this made the model convergence faster.
  prefs: []
  type: TYPE_NORMAL
- en: The training and testing set each have $8.44*10^{6}$ and $2.11*10^{6}$ respectively.
    The model was set to train for 200000 iterations as stated in the origin PassGAN’s
    work. But after 120000 iterations, the loss of both discriminator and generator
    became plateau, and it took a Tesla P100 GPU 12 hours to train the dataset for
    75000 iterations. By generating fake passwords using the 120000 checkpoints, compared
    the generated password with the testing dataset, the matching accuracy is 3.99%
    when generating $9.3*10^{7}$ unique passwords; and when generating $10^{8}$ passwords.
    Compared with Hitaj et al’s work,
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Discussion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Password Guessing with Deep Learning is a relatively new area, with the first
    related paper published in 2016([melicher2016fast,](#bib.bib17) ) using [LSTM](#glo.acronym.lstm)
    and first [GANs](#glo.acronym.gan) related model published in 2019([hitaj2019passgan,](#bib.bib10)
    ). With continuous advancements in machine learning and deep learning methods,
    bringing these advancements and breakthroughs to the password guessing domain
    is an exciting research topic. Nonetheless, several issues persist. I will introduce
    the open problems in password guessing and the associated future research possibilities
    in this section.
  prefs: []
  type: TYPE_NORMAL
- en: 5.1\. Open problems
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Insufficiency of training resources. The majority of password guessing tools
    are trained and tested using leaked datasets shown in Table[2](#S2.T2 "Table 2
    ‣ 2.3.3\. Attention-based models ‣ 2.3\. Deep Learning-base models ‣ 2\. Background
    and Related Work ‣ On Deep Learning in Password Guessing, a Survey"). While it
    is straightforward to compare the performance of each model, the generalizability
    of the models cannot be guaranteed.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The rate of successfully guessing the passwords is still low even using a combination
    of three types of models (rule-based, probability-based and deep learning-based).
    On the one-site password guessing problem, the best guessing accuracy is 51.8%
    after $10^{10}$ guesses([pasquini2021improving,](#bib.bib22) ). For cross-cite
    guessing, the best performance is 23% after $10^{12}$ guesses([liu2018genpass,](#bib.bib16)
    ). There is much space for improvement in the password guessing problem in terms
    of success rate.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The distribution or pattern of passwords disclosed five years ago may change
    from the passwords used today, yet most suggested models train and test on the
    same dataset, which is impractical in the real world. However, if the training
    and testing sets have different distributions, the model may experience concept
    drift. Even Liu et al.([liu2018genpass,](#bib.bib16) )’s hybrid model can successfully
    guess 23% of passwords correctly after generating $10^{12}$ passwords. The model’s
    capacity to generalize effectively to a different dataset than the testing set
    is critical because assessing cross-site is more realistic and requires further
    study.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The majority of research in the literature made no distinction between various
    password policies. However, most websites demand a mix of numeric characters,
    special symbols, and special characters. Prior study([tan2020practical,](#bib.bib28)
    ) has shown that varying password regulations, such as minimum-length and minimum-class
    requirements, can have a considerable influence on both usability and security.
    As a result, it is desirable to test password guessing tools against a variety
    of password policies.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 5.2\. Future work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the provided deep learning algorithms are appropriate for password
    guessing attacks; the suggested models are somewhat simplistic. With advancements
    in the natural language processing and the computer vision fields, [BERT](#glo.acronym.bert),
    variants of [GANs](#glo.acronym.gan), the self-attention mechanism([choromanski2020rethinking,](#bib.bib4)
    ), and autoencoders have all seen significant improvements, and these newly invented
    techniques and architectures can be used to improve password guessing performance.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[GANs](#glo.acronym.gan) can also be utilized in targeted assaults. Once the
    attackers get access to the users’ [PII](#glo.acronym.pii), they may condition
    [GANs](#glo.acronym.gan) ([mirza2014conditional,](#bib.bib18) ) on the precise
    terms associated with the users’ [PII](#glo.acronym.pii), causing the generator
    to pay more attention to the region of the search space containing these keywords.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The estimations of Deep Learning models can be used to develop trustworthy
    [PSMs](#glo.acronym.psm) that can assist users in creating passwords that are
    resistant to guessing attacks. The challenges are: to develop an application capable
    of storing and querying the compressed neural network and an interactive [PSMs](#glo.acronym.psm)
    user interface that can prompt users to build more secure passwords; convert deep
    learning results to interpretable rank as password strength. One solution is to
    store [GANs](#glo.acronym.gan) generated samples by different attempts in dictionaries
    and search for which dictionary the user-created password belongs to. For example
    if the created password first exists in the dictionary with 10000 generated passwords,
    then the strength is assigned to 4, if it first appears in the generated $10^{9}$
    dictionary, then the strength is assigned to 9\. The higher number the more secure
    as it needs exponentially more computation to guess the password correctly.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To overcome the paucity of training and testing resources, researchers might
    train and test models using custom or site-specific data. For example, passwords
    generated during user studies to evaluate different password usability and security
    experiments may be utilized in our purpose.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When reviewing novel password guessing methods, it is necessary to consider
    password policies. Additionally, transfer learning may improve models’ performance
    when there is a shortage of training data for a particular policy. For example,
    we may train a pre-trained model using generic passwords, freeze the network’s
    bottom layers and retrain the model using only passwords that adhere to specific
    criteria. Melicher et al.([melicher2016fast,](#bib.bib17) ) employed transfer
    learning to train models for certain password restrictions. However, the choices
    are not exhaustive.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 6\. Conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, I compared the commonly deployed rule-based, probability-based
    password guessing tools with deep learning-based approaches from a high level,
    and introduced the deep learning techniques used in the password guessing area,
    including [RNN](#glo.acronym.rnn), autoencoder, [GANs](#glo.acronym.gan) and the
    Attention mechanism. A research idea involving using a [GANs](#glo.acronym.gan)-based
    model on password guessing attacks is proposed, along with the experimental methodology
    and resources required.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning has made tremendous strides in recent years and will continue
    to do so in the near future. Deep learning application scenarios abound in our
    daily lives. Password guessing is a novel and exciting setting that will undoubtedly
    benefit from deep learning’s growth. I hope that with the attention of researchers,
    the issues raised in this work will find promising solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Appendix
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Please see Figure[4](#S7.F4 "Figure 4 ‣ 7\. Appendix ‣ On Deep Learning in Password
    Guessing, a Survey") for the mind map for the password guessing attacks.
  prefs: []
  type: TYPE_NORMAL
- en: <svg id="S7.F4.pic1" class="ltx_picture" height="689.7" overflow="visible" version="1.1"
    width="644.71"><g transform="translate(0,689.7) matrix(1 0 0 -1 0 0) translate(320.62,0)
    translate(0,302.79)" fill="#000000" stroke="#000000" stroke-width="1.2pt"><g transform="matrix(1.0
    0.0 0.0 1.0 -68.9 4.38)" fill="#000000" stroke="#000000"><foreignobject width="137.8"
    height="28.14" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Password
    Guessing Attacks <g fill="#FFCCCC" stroke="#FFCCCC"><path d="M -44.29 -153.43
    C -44.29 -128.97 -64.12 -109.14 -88.58 -109.14 C -113.04 -109.14 -132.87 -128.97
    -132.87 -153.43 C -132.87 -177.89 -113.04 -197.72 -88.58 -197.72 C -64.12 -197.72
    -44.29 -177.89 -44.29 -153.43 Z M -88.58 -153.43"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -127.95 -157.65)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="10.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Types</foreignobject></g>
    <g fill="#FFCCCC"><path d="M -39.37 -68.19 M -13.67 -77.54 C -31.9 -74.33 -48.42
    -64.79 -60.32 -50.61 C -47.67 -65.69 -52.57 -83.3 -62.41 -100.35 L -55.7 -104.22
    C -45.86 -87.18 -33.06 -74.13 -13.67 -77.54 Z M -52.01 -97.83 L -55.7 -104.22
    L -62.41 -100.35 L -58.72 -93.95 Z M -62.41 -100.35 M -54.65 -124.96 C -61.35
    -116.98 -70.64 -111.62 -80.89 -109.81 C -69.99 -111.73 -64.26 -103.54 -58.72 -93.95
    L -52.01 -97.83 C -57.54 -107.42 -61.77 -116.48 -54.65 -124.96 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -168.22 -184 C -168.22 -164.97 -183.64
    -149.55 -202.67 -149.55 C -221.69 -149.55 -237.12 -164.97 -237.12 -184 C -237.12
    -203.03 -221.69 -218.45 -202.67 -218.45 C -183.64 -218.45 -168.22 -203.03 -168.22
    -184 Z M -202.67 -184"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -232.19
    -181.54)" fill="#000000" stroke="#000000"><foreignobject width="59.06" height="24.29"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Online and Offline</foreignobject></g>
    <g fill="#FFE6E6"><path d="M -131.36 -164.89 M -124.86 -178.83 C -130.84 -170.3
    -133.61 -159.94 -132.71 -149.57 C -133.67 -160.6 -142.84 -164.85 -153.54 -167.71
    L -151.98 -173.54 C -141.28 -170.67 -131.22 -169.76 -124.86 -178.83 Z M -151.97
    -173.54 L -151.98 -173.54 L -153.54 -167.71 L -153.53 -167.71 Z M -153.54 -167.71
    M -168.35 -187 C -167.64 -178.93 -169.8 -170.87 -174.45 -164.24 C -169.51 -171.29
    -161.85 -169.94 -153.53 -167.71 L -151.97 -173.54 C -160.29 -175.77 -167.6 -178.42
    -168.35 -187 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -109.78 -255.72 C -109.78 -234.81 -126.73 -217.85 -147.64 -217.85 C -168.55
    -217.85 -185.5 -234.81 -185.5 -255.72 C -185.5 -276.63 -168.55 -293.58 -147.64
    -293.58 C -126.73 -293.58 -109.78 -276.63 -109.78 -255.72 Z M -147.64 -255.72"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -177.17 -244.95)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="40.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Targeted
    and Non-Targeted</foreignobject></g> <g fill="#FFE6E6"><path d="M -110.73 -191.79
    M -96.27 -197.05 C -106.53 -195.24 -115.82 -189.88 -122.51 -181.9 C -115.39 -190.38
    -119.13 -199.72 -124.67 -209.31 L -118.93 -212.62 C -113.4 -203.03 -107.18 -195.13
    -96.27 -197.05 Z M -116.37 -208.19 L -118.93 -212.62 L -124.67 -209.31 L -122.11
    -204.87 Z M -124.67 -209.31 M -118.63 -231.38 C -124.35 -224.56 -132.3 -219.97
    -141.06 -218.43 C -131.74 -220.07 -126.84 -213.07 -122.11 -204.87 L -116.37 -208.19
    C -121.1 -216.39 -124.72 -224.13 -118.63 -231.38 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -23.56 -267.51 C -23.56 -248.49 -38.99
    -233.07 -58.01 -233.07 C -77.04 -233.07 -92.46 -248.49 -92.46 -267.51 C -92.46
    -286.54 -77.04 -301.96 -58.01 -301.96 C -38.99 -301.96 -23.56 -286.54 -23.56 -267.51
    Z M -58.01 -267.51"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -87.54 -264.13)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="26.15" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Static and Dynamic</foreignobject></g> <g fill="#FFE6E6"><path
    d="M -77.12 -196.21 M -63.18 -189.71 C -71.71 -195.68 -82.07 -198.46 -92.44 -197.55
    C -81.41 -198.52 -77.16 -207.69 -74.3 -218.38 L -68.48 -216.82 C -71.34 -206.13
    -72.25 -196.06 -63.18 -189.71 Z M -68.48 -216.82 L -68.48 -216.82 L -74.3 -218.38
    L -74.3 -218.38 Z M -74.3 -218.38 M -55.01 -233.2 C -63.08 -232.49 -71.14 -234.65
    -77.77 -239.29 C -70.72 -234.36 -72.07 -226.7 -74.3 -218.38 L -68.48 -216.82 C
    -66.25 -225.14 -63.59 -232.45 -55.01 -233.2 Z" style="stroke:none"></path></g><g
    fill="#FFCCCC" stroke="#FFCCCC"><path d="M 221.46 0 C 221.46 24.46 201.63 44.29
    177.17 44.29 C 152.7 44.29 132.87 24.46 132.87 0 C 132.87 -24.46 152.7 -44.29
    177.17 -44.29 C 201.63 -44.29 221.46 -24.46 221.46 0 Z M 177.17 0"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 137.8 -4.84)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Intention</foreignobject></g>
    <g fill="#FFCCCC"><path d="M 78.74 0 M 73.99 26.93 C 80.32 9.54 80.32 -9.54 73.99
    -26.93 C 80.72 -8.43 98.43 -3.88 118.11 -3.88 L 118.11 3.88 C 98.43 3.88 80.72
    8.43 73.99 26.93 Z M 118.11 -3.88 h -7.38 v 7.75 h 7.38 Z M 135.54 15.15 C 131.98
    5.36 131.98 -5.36 135.54 -15.15 C 131.76 -4.74 121.8 -3.88 110.73 -3.88 L 110.73
    3.88 C 121.8 3.88 131.76 4.74 135.54 15.15 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M 323.26 -45.2 C 323.26 -24.78 306.71
    -8.22 286.28 -8.22 C 265.86 -8.22 249.31 -24.78 249.31 -45.2 C 249.31 -65.62 265.86
    -82.18 286.28 -82.18 C 306.71 -82.18 323.26 -65.62 323.26 -45.2 Z M 286.28 -45.2"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 256.76 -35.9)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="37.97" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Password
    Strength Meters</foreignobject></g> <g fill="#FFE6E6"><path d="M 218.08 -16.95
    M 221.41 -1.93 C 220.96 -12.33 216.86 -22.25 209.82 -29.92 C 217.3 -21.76 227.08
    -24.18 237.31 -28.41 L 239.78 -22.43 C 229.55 -18.2 220.93 -12.99 221.41 -1.93
    Z M 236.28 -20.98 L 239.78 -22.43 L 237.31 -28.41 L 233.8 -26.96 Z M 237.31 -28.41
    M 259.02 -20.22 C 253.15 -26.63 249.72 -34.9 249.34 -43.58 C 249.74 -34.35 242.34
    -30.5 233.8 -26.96 L 236.28 -20.98 C 244.82 -24.52 252.77 -27.03 259.02 -20.22
    Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path d="M
    320.73 45.2 C 320.73 64.22 305.31 79.65 286.28 79.65 C 267.26 79.65 251.84 64.22
    251.84 45.2 C 251.84 26.17 267.26 10.75 286.28 10.75 C 305.31 10.75 320.73 26.17
    320.73 45.2 Z M 286.28 45.2"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 256.76
    47.51)" fill="#000000" stroke="#000000"><foreignobject width="59.06" height="24"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Honeyword Generation</foreignobject></g>
    <g fill="#FFE6E6"><path d="M 218.08 16.95 M 209.82 29.92 C 216.86 22.25 220.96
    12.33 221.41 1.93 C 220.93 12.99 229.47 18.4 239.7 22.64 L 237.39 28.21 C 227.16
    23.97 217.3 21.76 209.82 29.92 Z M 237.39 28.21 L 237.39 28.21 L 239.7 22.64 L
    239.7 22.64 Z M 239.7 22.64 M 251.87 43.7 C 252.22 35.6 255.41 27.9 260.88 21.92
    C 255.07 28.27 247.65 25.93 239.7 22.64 L 237.39 28.21 C 245.35 31.5 252.24 35.09
    251.87 43.7 Z" style="stroke:none"></path></g><g fill="#FFCCCC" stroke="#FFCCCC"><path
    d="M -44.29 153.43 C -44.29 177.89 -64.12 197.72 -88.58 197.72 C -113.04 197.72
    -132.87 177.89 -132.87 153.43 C -132.87 128.97 -113.04 109.14 -88.58 109.14 C
    -64.12 109.14 -44.29 128.97 -44.29 153.43 Z M -88.58 153.43"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -127.95 149.28)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Techniques</foreignobject></g>
    <g fill="#FFCCCC"><path d="M -39.37 68.19 M -60.32 50.61 C -48.42 64.79 -31.9
    74.33 -13.67 77.54 C -33.06 74.13 -45.86 87.18 -55.7 104.22 L -62.41 100.35 C
    -52.57 83.3 -47.67 65.69 -60.32 50.61 Z M -58.72 93.95 L -62.41 100.35 L -55.7
    104.22 L -52.01 97.83 Z M -55.7 104.22 M -80.89 109.81 C -70.64 111.62 -61.35
    116.98 -54.65 124.96 C -61.77 116.48 -57.54 107.42 -52.01 97.83 L -58.72 93.95
    C -64.26 103.54 -69.99 111.73 -80.89 109.81 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -23.56 267.51 C -23.56 286.54 -38.99
    301.96 -58.01 301.96 C -77.04 301.96 -92.46 286.54 -92.46 267.51 C -92.46 248.49
    -77.04 233.07 -58.01 233.07 C -38.99 233.07 -23.56 248.49 -23.56 267.51 Z M -58.01
    267.51"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -87.54 262.67)" fill="#000000"
    stroke="#000000"><foreignobject width="59.06" height="9.69" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Rule-based</foreignobject></g> <g fill="#FFE6E6"><path
    d="M -77.12 196.21 M -92.44 197.55 C -82.07 198.46 -71.71 195.68 -63.18 189.71
    C -72.25 196.06 -71.34 206.13 -68.48 216.82 L -74.3 218.38 C -77.16 207.69 -81.41
    198.52 -92.44 197.55 Z M -74.3 218.38 L -74.3 218.38 L -68.48 216.82 L -68.48
    216.82 Z M -68.48 216.82 M -77.77 239.29 C -71.14 234.65 -63.08 232.49 -55.01
    233.2 C -63.59 232.45 -66.25 225.14 -68.48 216.82 L -74.3 218.38 C -72.07 226.7
    -70.72 234.36 -77.77 239.29 Z" style="stroke:none"></path></g><g fill="#FFE6E6"
    stroke="#FFE6E6"><path d="M 11.87 349.34 C 11.87 361.84 1.73 371.98 -10.77 371.98
    C -23.27 371.98 -33.41 361.84 -33.41 349.34 C -33.41 336.84 -23.27 326.7 -10.77
    326.7 C 1.73 326.7 11.87 336.84 11.87 349.34 Z M -10.77 349.34"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.46 344.5)" fill="#000000" stroke="#000000"><foreignobject
    width="39.37" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">HashCat([hranicky2019distributed,](#bib.bib12)
    )</foreignobject></g> <g fill="#FFE6E6"><path d="M -40.79 297.35 M -52.03 301.44
    C -44.06 300.03 -36.83 295.86 -31.62 289.66 C -37.16 296.25 -34.77 303.82 -30.46
    311.27 L -33.89 313.25 C -38.2 305.8 -43.55 299.94 -52.03 301.44 Z M -29.46 320.93
    L -33.89 313.25 L -30.46 311.27 L -26.03 318.95 Z M -30.46 311.27 M -28.11 334.79
    C -24.69 330.71 -19.94 327.97 -14.7 327.05 C -20.27 328.03 -23.2 323.85 -26.03
    318.95 L -29.46 320.93 C -26.63 325.83 -24.47 330.46 -28.11 334.79 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -33.93 362 C -33.93 375.3 -44.71 386.08
    -58.01 386.08 C -71.31 386.08 -82.09 375.3 -82.09 362 C -82.09 348.7 -71.31 337.92
    -58.01 337.92 C -44.71 337.92 -33.93 348.7 -33.93 362 Z M -58.01 362"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -77.7 362.78)" fill="#000000" stroke="#000000"><foreignobject
    width="39.37" height="20.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">John
    the Ripper</foreignobject></g> <g fill="#FFE6E6"><path d="M -58.01 301.96 M -69.8
    299.89 C -62.18 302.66 -53.84 302.66 -46.23 299.89 C -54.32 302.83 -55.91 310.58
    -55.91 319.19 L -60.12 319.19 C -60.12 310.58 -61.7 302.83 -69.8 299.89 Z M -60.12
    325.88 L -60.12 319.19 L -55.91 319.19 L -55.91 325.88 Z M -55.91 319.19 M -66.25
    339.38 C -60.93 337.44 -55.1 337.44 -49.78 339.38 C -55.43 337.32 -55.91 331.9
    -55.91 325.88 L -60.12 325.88 C -60.12 331.9 -60.59 337.32 -66.25 339.38 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -113.19 255.72 C -113.19 274.74 -128.61
    290.17 -147.64 290.17 C -166.66 290.17 -182.09 274.74 -182.09 255.72 C -182.09
    236.69 -166.66 221.27 -147.64 221.27 C -128.61 221.27 -113.19 236.69 -113.19 255.72
    Z M -147.64 255.72"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -177.17 258.18)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="24.29" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Probability-based</foreignobject></g> <g fill="#FFE6E6"><path
    d="M -110.73 191.79 M -122.51 181.9 C -115.82 189.88 -106.53 195.24 -96.27 197.05
    C -107.18 195.13 -113.65 202.88 -119.19 212.47 L -124.41 209.46 C -118.88 199.87
    -115.39 190.38 -122.51 181.9 Z M -124.41 209.46 L -124.41 209.46 L -119.19 212.47
    L -119.19 212.47 Z M -119.19 212.47 M -141.65 221.79 C -133.68 223.2 -126.45 227.37
    -121.25 233.57 C -126.78 226.97 -123.5 219.93 -119.19 212.47 L -124.41 209.46
    C -128.72 216.92 -133.17 223.28 -141.65 221.79 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -149.46 346.99 C -149.46 359.49 -159.59
    369.62 -172.09 369.62 C -184.6 369.62 -194.73 359.49 -194.73 346.99 C -194.73
    334.48 -184.6 324.35 -172.09 324.35 C -159.59 324.35 -149.46 334.48 -149.46 346.99
    Z M -172.09 346.99"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -191.78 342.14)"
    fill="#000000" stroke="#000000"><foreignobject width="39.37" height="9.69" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PCFG([weir2009password,](#bib.bib33) )</foreignobject></g>
    <g fill="#FFE6E6"><path d="M -156.55 288.99 M -167.4 283.94 C -160.76 288.58 -152.7
    290.74 -144.64 290.03 C -153.21 290.79 -156.87 297.82 -159.1 306.14 L -162.93
    305.12 C -160.7 296.8 -160.34 288.88 -167.4 283.94 Z M -165.22 313.67 L -162.93
    305.12 L -159.1 306.14 L -161.39 314.7 Z M -159.1 306.14 M -174.07 324.43 C -168.76
    323.97 -163.47 325.39 -159.11 328.44 C -163.74 325.19 -162.86 320.16 -161.39 314.7
    L -165.22 313.67 C -166.68 319.14 -168.43 323.94 -174.07 324.43 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -189.71 322.53 C -189.71 336.19 -200.79
    347.27 -214.45 347.27 C -228.11 347.27 -239.19 336.19 -239.19 322.53 C -239.19
    308.87 -228.11 297.79 -214.45 297.79 C -200.79 297.79 -189.71 308.87 -189.71 322.53
    Z M -214.45 322.53"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -234.13 324.61)"
    fill="#000000" stroke="#000000"><foreignobject width="39.37" height="23.52" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Markov Models([narayanan2005fast,](#bib.bib20)
    )</foreignobject></g> <g fill="#FFE6E6"><path d="M -172 280.08 M -178.86 270.28
    C -175.44 277.62 -169.54 283.52 -162.2 286.94 C -170 283.3 -176.56 287.7 -182.65
    293.79 L -185.71 290.73 C -179.62 284.64 -175.22 278.08 -178.86 270.28 Z M -189.74
    294.76 L -185.71 290.73 L -182.65 293.79 L -186.68 297.82 Z M -182.65 293.79 M
    -203.99 300.11 C -198.72 302.56 -194.48 306.8 -192.03 312.07 C -194.64 306.47
    -191.05 302.19 -186.68 297.82 L -189.74 294.76 C -194.11 299.13 -198.39 302.72
    -203.99 300.11 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -168.22 184 C -168.22 203.03 -183.64 218.45 -202.67 218.45 C -221.69 218.45
    -237.12 203.03 -237.12 184 C -237.12 164.97 -221.69 149.55 -202.67 149.55 C -183.64
    149.55 -168.22 164.97 -168.22 184 Z M -202.67 184"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -232.19 187.54)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="26.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Deep Learning-based</foreignobject></g>
    <g fill="#FFE6E6"><path d="M -131.36 164.89 M -132.71 149.57 C -133.61 159.94
    -130.84 170.3 -124.86 178.83 C -131.22 169.76 -141.28 170.67 -151.98 173.54 L
    -153.54 167.71 C -142.84 164.85 -133.67 160.6 -132.71 149.57 Z M -153.53 167.71
    L -153.54 167.71 L -151.98 173.54 L -151.97 173.54 Z M -151.98 173.54 M -174.45
    164.24 C -169.8 170.87 -167.64 178.93 -168.35 187 C -167.6 178.42 -160.29 175.77
    -151.97 173.54 L -153.53 167.71 C -161.85 169.94 -169.51 171.29 -174.45 164.24
    Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path d="M
    -225.17 265.83 C -225.17 279.49 -236.25 290.57 -249.91 290.57 C -263.58 290.57
    -274.65 279.49 -274.65 265.83 C -274.65 252.16 -263.58 241.08 -249.91 241.08 C
    -236.25 241.08 -225.17 252.16 -225.17 265.83 Z M -249.91 265.83"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -269.6 267.9)" fill="#000000" stroke="#000000"><foreignobject
    width="39.37" height="23.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RNN/LSTM
    ([melicher2016fast,](#bib.bib17) ; [zhang2018password,](#bib.bib37) ; [liu2018genpass,](#bib.bib16)
    )</foreignobject></g> <g fill="#FFE6E6"><path d="M -219.89 213.83 M -229.06 206.14
    C -223.85 212.35 -216.63 216.52 -208.65 217.92 C -217.13 216.43 -222.32 222.37
    -226.63 229.83 L -230.38 227.67 C -226.07 220.21 -223.52 212.74 -229.06 206.14
    Z M -233.23 232.6 L -230.38 227.67 L -226.63 229.83 L -229.48 234.77 Z M -226.63
    229.83 M -245.61 241.46 C -239.89 242.47 -234.7 245.47 -230.96 249.92 C -234.93
    245.19 -232.57 240.13 -229.48 234.77 L -233.23 232.6 C -236.32 237.96 -239.52
    242.54 -245.61 241.46 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -259.75 231.24 C -259.75 244.91 -270.83 255.98 -284.49 255.98 C -298.16 255.98
    -309.24 244.91 -309.24 231.24 C -309.24 217.58 -298.16 206.5 -284.49 206.5 C -270.83
    206.5 -259.75 217.58 -259.75 231.24 Z M -284.49 231.24"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -304.18 233.32)" fill="#000000" stroke="#000000"><foreignobject width="39.37"
    height="23.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Autoencoder
    ([nam2020generating,](#bib.bib19) )</foreignobject></g> <g fill="#FFE6E6"><path
    d="M -232.5 201.22 M -236.59 189.98 C -235.19 197.96 -231.01 205.18 -224.81 210.39
    C -231.41 204.85 -238.88 207.4 -246.33 211.71 L -248.5 207.96 C -241.04 203.65
    -235.1 198.46 -236.59 189.98 Z M -253.44 210.81 L -248.5 207.96 L -246.33 211.71
    L -251.27 214.56 Z M -246.33 211.71 M -268.59 212.29 C -264.14 216.03 -261.14
    221.22 -260.13 226.95 C -261.2 220.86 -256.63 217.65 -251.27 214.56 L -253.44
    210.81 C -258.8 213.9 -263.85 216.27 -268.59 212.29 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -274.52 184 C -274.52 196.5 -284.65
    206.64 -297.15 206.64 C -309.66 206.64 -319.79 196.5 -319.79 184 C -319.79 171.5
    -309.66 161.36 -297.15 161.36 C -284.65 161.36 -274.52 171.5 -274.52 184 Z M -297.15
    184"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -316.84 179.16)" fill="#000000"
    stroke="#000000"><foreignobject width="39.37" height="9.69" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">GANs ([hitaj2019passgan,](#bib.bib10) ; [pasquini2021improving,](#bib.bib22)
    )</foreignobject></g> <g fill="#FFE6E6"><path d="M -237.12 184 M -235.04 172.22
    C -237.81 179.83 -237.81 188.17 -235.04 195.78 C -237.98 187.69 -245.73 185.98
    -254.34 185.98 L -254.34 182.02 C -245.73 182.02 -237.98 180.31 -235.04 172.22
    Z M -263.2 182.02 L -254.34 182.02 L -254.34 185.98 L -263.2 185.98 Z M -254.34
    185.98 M -275.88 176.26 C -274.06 181.26 -274.06 186.74 -275.88 191.74 C -273.95
    186.42 -268.86 185.98 -263.2 185.98 L -263.2 182.02 C -268.86 182.02 -273.95 181.58
    -275.88 176.26 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -259.75 136.76 C -259.75 150.42 -270.83 161.5 -284.49 161.5 C -298.16 161.5
    -309.23 150.42 -309.23 136.76 C -309.23 123.09 -298.16 112.01 -284.49 112.01 C
    -270.83 112.01 -259.75 123.09 -259.75 136.76 Z M -284.49 136.76"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -304.18 138.83)" fill="#000000" stroke="#000000"><foreignobject
    width="39.37" height="23.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Attention-Mechanism
    ([li2019password,](#bib.bib15) )</foreignobject></g><g fill="#FFE6E6"><path d="M
    -232.5 166.77 M -224.81 157.61 C -231.01 162.82 -235.19 170.04 -236.59 178.02
    C -235.1 169.54 -241.04 164.34 -248.5 160.04 L -246.33 156.29 C -238.88 160.59
    -231.41 163.15 -224.81 157.61 Z M -251.27 153.44 L -246.33 156.29 L -248.5 160.04
    L -253.44 157.19 Z M -248.5 160.04 M -260.13 141.05 C -261.14 146.78 -264.13 151.97
    -268.59 155.71 C -263.85 151.73 -258.79 154.09 -253.44 157.19 L -251.27 153.44
    C -256.63 150.34 -261.2 147.14 -260.13 141.05 Z" style="stroke:none"></path></g>
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4\. A mind map for password guessing attacks
  prefs: []
  type: TYPE_NORMAL
- en: Glossary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Acronyms
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: BERT
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional Encoder Representations from Transformers
  prefs: []
  type: TYPE_NORMAL
- en: BiLSTM
  prefs: []
  type: TYPE_NORMAL
- en: Bidirectional Long Short-term Memory
  prefs: []
  type: TYPE_NORMAL
- en: CNN
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional Neural Networks
  prefs: []
  type: TYPE_NORMAL
- en: DPG
  prefs: []
  type: TYPE_NORMAL
- en: Dynamic Password Guessing
  prefs: []
  type: TYPE_NORMAL
- en: FLA
  prefs: []
  type: TYPE_NORMAL
- en: Fast, Lean, Accurate
  prefs: []
  type: TYPE_NORMAL
- en: GANs
  prefs: []
  type: TYPE_NORMAL
- en: Generative Adversarial Networks
  prefs: []
  type: TYPE_NORMAL
- en: IWGAN
  prefs: []
  type: TYPE_NORMAL
- en: Improved Training of Wasserstein GANs
  prefs: []
  type: TYPE_NORMAL
- en: LSTM
  prefs: []
  type: TYPE_NORMAL
- en: Long Short Term Memory Networks
  prefs: []
  type: TYPE_NORMAL
- en: PCFG
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic Context-Free Grammar
  prefs: []
  type: TYPE_NORMAL
- en: PII
  prefs: []
  type: TYPE_NORMAL
- en: Personally Identifiable Information
  prefs: []
  type: TYPE_NORMAL
- en: PSMs
  prefs: []
  type: TYPE_NORMAL
- en: Password Strength Meters
  prefs: []
  type: TYPE_NORMAL
- en: RNN
  prefs: []
  type: TYPE_NORMAL
- en: Recurrent Neural Networks
  prefs: []
  type: TYPE_NORMAL
- en: SPRNN
  prefs: []
  type: TYPE_NORMAL
- en: Structure Partition and Bilstm Recurrent Neural Network
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1) Adams, A., and Sasse, M. A. Users are not the enemy. Communications of the
    ACM 42, 12 (1999), 40–46.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(2) Bonneau, J., Herley, C., Van Oorschot, P. C., and Stajano, F. The quest
    to replace passwords: A framework for comparative evaluation of web authentication
    schemes. In 2012 IEEE Symposium on Security and Privacy (2012), IEEE, pp. 553–567.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(3) Bonneau, J., and Preibusch, S. The password thicket: Technical and market
    failures in human authentication on the web. In WEIS (2010), Citeseer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (4) Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos,
    T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., et al. Rethinking attention
    with performers. In International Conference on Learning Representations (2020).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(5) Dell’Amico, M., and Filippone, M. Monte carlo strength evaluation: Fast
    and reliable password checking. In Proceedings of the 22nd ACM SIGSAC Conference
    on Computer and Communications Security (2015), pp. 158–169.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(6) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training
    of deep bidirectional transformers for language understanding. arXiv preprint
    arXiv:1810.04805 (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (7) Florêncio, D., Herley, C., and Van Oorschot, P. C. An administrator’s guide
    to internet password research. In Proceedings of the 28th USENIX conference on
    Large Installation System Administration (2014), pp. 35–52.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (8) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. Advances
    in neural information processing systems 27 (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (9) Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.
    Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028 (2017).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(10) Hitaj, B., Gasti, P., Ateniese, G., and Perez-Cruz, F. Passgan: A deep
    learning approach for password guessing. In International Conference on Applied
    Cryptography and Network Security (2019), Springer, pp. 217–237.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (11) Hochreiter, S., and Schmidhuber, J. Long short-term memory. Neural computation
    9, 8 (1997), 1735–1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (12) Hranickỳ, R., Zobal, L., Ryšavỳ, O., and Kolář, D. Distributed password
    cracking with boinc and hashcat. Digital Investigation 30 (2019), 161–172.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(13) Jabbar, A., Li, X., and Omar, B. A survey on generative adversarial networks:
    Variants, applications, and training. ACM Computing Surveys (CSUR) 54, 8 (2021),
    1–49.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (14) Jiang, H., Chen, Z., Chen, M., Liu, F., Wang, D., and Zhao, T. On computation
    and generalization of generative adversarial networks under spectrum control.
    In International Conference on Learning Representations (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (15) Li, H., Chen, M., Yan, S., Jia, C., and Li, Z. Password guessing via neural
    language modeling. In International Conference on Machine Learning for Cyber Security
    (2019), Springer, pp. 78–93.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(16) Liu, Y., Xia, Z., Yi, P., Yao, Y., Xie, T., Wang, W., and Zhu, T. Genpass:
    A general deep learning model for password guessing with pcfg rules and adversarial
    generation. In 2018 IEEE International Conference on Communications (ICC) (2018),
    IEEE, pp. 1–6.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(17) Melicher, W., Ur, B., Segreti, S. M., Komanduri, S., Bauer, L., Christin,
    N., and Cranor, L. F. Fast, lean, and accurate: modeling password guessability
    using neural networks. In Proceedings of the 25th USENIX Conference on Security
    Symposium (2016), pp. 175–191.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (18) Mirza, M., and Osindero, S. Conditional generative adversarial nets. arXiv
    preprint arXiv:1411.1784 (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (19) Nam, S., Jeon, S., and Moon, J. Generating optimized guessing candidates
    toward better password cracking from multi-dictionaries using relativistic gan.
    Applied Sciences 10, 20 (2020), 7306.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (20) Narayanan, A., and Shmatikov, V. Fast dictionary attacks on passwords using
    time-space tradeoff. In Proceedings of the 12th ACM conference on Computer and
    communications security (2005), pp. 364–372.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (21) Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training
    recurrent neural networks. In International conference on machine learning (2013),
    PMLR, pp. 1310–1318.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (22) Pasquini, D., Gangwal, A., Ateniese, G., Bernaschi, M., and Conti, M. Improving
    password guessing via representation learning. In 2021 IEEE Symposium on Security
    and Privacy (SP) (2021), IEEE, pp. 1382–1399.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(23) Pearman, S., Thomas, J., Naeini, P. E., Habib, H., Bauer, L., Christin,
    N., Cranor, L. F., Egelman, S., and Forget, A. Let’s go in for a closer look:
    Observing passwords in their natural habitat. In Proceedings of the 2017 ACM SIGSAC
    Conference on Computer and Communications Security (2017), pp. 295–310.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (24) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,
    and Zettlemoyer, L. Deep contextualized word representations. In Proceedings of
    NAACL-HLT (2018), pp. 2227–2237.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (25) Pliam, J. O. On the incomparability of entropy and marginal guesswork in
    brute-force attacks. In International Conference on Cryptology in India (2000),
    Springer, pp. 67–79.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (26) Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving
    language understanding by generative pre-training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(27) Stobert, E., and Biddle, R. The password life cycle: user behaviour in
    managing passwords. In 10th Symposium On Usable Privacy and Security ($\{$SOUPS$\}$
    2014) (2014), pp. 243–255.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (28) Tan, J., Bauer, L., Christin, N., and Cranor, L. F. Practical recommendations
    for stronger, more usable passwords combining minimum-strength, minimum-length,
    and blocklist requirements. In Proceedings of the 2020 ACM SIGSAC Conference on
    Computer and Communications Security (2020), pp. 1407–1426.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (29) Terjék, D. Adversarial lipschitz regularization. In International Conference
    on Learning Representations (2019).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(30) Thanh-Tung, H., Venkatesh, S., and Tran, T. Improving generalization and
    stability of generative adversarial networks. In ICLR 2019: Proceedings of the
    7th International Conference on Learning Representations (2019), ICLR.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (31) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in
    neural information processing systems (2017), pp. 5998–6008.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '(32) Wei, X., Gong, B., Liu, Z., Lu, W., and Wang, L. Improving the improved
    training of wasserstein gans: A consistency term and its dual effect. In International
    Conference on Learning Representation (ICLR) (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (33) Weir, M., Aggarwal, S., De Medeiros, B., and Glodek, B. Password cracking
    using probabilistic context-free grammars. In 2009 30th IEEE Symposium on Security
    and Privacy (2009), IEEE, pp. 391–405.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (34) Wu, J., Huang, Z., Thoma, J., Acharya, D., and Van Gool, L. Wasserstein
    divergence for gans. In Proceedings of the European Conference on Computer Vision
    (ECCV) (2018), pp. 653–668.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (35) Wu, Y.-L., Shuai, H.-H., Tam, Z. R., and Chiu, H.-Y. Gradient normalization
    for generative adversarial networks. In Proceedings of the IEEE International
    Conference on Computer Vision (ICCV) (Oct 2021).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (36) Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are
    features in deep neural networks? Advances in Neural Information Processing Systems
    27 (2014), 3320–3328.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: (37) Zhang, M., Zhang, Q., Hu, X., and Liu, W. A password cracking method based
    on structure partition and bilstm recurrent neural network. In Proceedings of
    the 8th International Conference on Communication and Network Security (2018),
    pp. 79–83.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
