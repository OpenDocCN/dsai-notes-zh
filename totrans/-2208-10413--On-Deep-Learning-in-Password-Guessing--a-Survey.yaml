- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 类别：未分类
- en: 'date: 2024-09-06 19:44:49'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 19:44:49
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2208.10413] On Deep Learning in Password Guessing, a Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2208.10413] 关于密码猜测中的深度学习，综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2208.10413](https://ar5iv.labs.arxiv.org/html/2208.10413)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2208.10413](https://ar5iv.labs.arxiv.org/html/2208.10413)
- en: On Deep Learning in Password Guessing, a Survey
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关于密码猜测中的深度学习，综述
- en: Fangyi Yu [fangyi.yu@ontariotechu.net](mailto:fangyi.yu@ontariotechu.net) Ontario
    Tech University2000 Simcoe St NOshawaOntarioCanadaL1G 0C5
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 方宜余 [fangyi.yu@ontariotechu.net](mailto:fangyi.yu@ontariotechu.net) 安大略理工大学2000
    Simcoe St N Oshawa Ontario Canada L1G 0C5
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: The security of passwords is dependent on a thorough understanding of the strategies
    used by attackers. Unfortunately, real-world adversaries use pragmatic guessing
    tactics like dictionary attacks, which are difficult to simulate in password security
    research. Dictionary attacks must be carefully configured and modified to be representative
    of the actual threat. This approach, however, needs domain-specific knowledge
    and expertise that are difficult to duplicate. This paper compares various deep
    learning-based password guessing approaches that do not require domain knowledge
    or assumptions about users’ password structures and combinations. The involved
    model categories are [Recurrent Neural Networks (RNN)](#glo.acronym.rnn), [Generative
    Adversarial Networks (GANs)](#glo.acronym.gan), autoencoder and attention mechanisms.
    Additionally, we proposed a promising research experimental design on using variations
    of [Improved Training of Wasserstein GANs (IWGAN)](#glo.acronym.iwgan) on password
    guessing under non-targeted offline attacks. Using these advanced strategies,
    we can enhance password security and create more accurate and efficient [Password
    Strength Meters (PSMs)](#glo.acronym.psm).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 密码的安全性依赖于对攻击者使用策略的透彻理解。不幸的是，现实世界中的对手使用实际的猜测战术，如字典攻击，这些战术在密码安全研究中很难模拟。字典攻击必须经过仔细配置和修改，以代表实际威胁。然而，这种方法需要领域特定的知识和专业技能，这些是难以复制的。本文比较了几种基于深度学习的密码猜测方法，这些方法不需要领域知识或对用户密码结构和组合的假设。涉及的模型类别有
    [递归神经网络 (RNN)](#glo.acronym.rnn)、[生成对抗网络 (GANs)](#glo.acronym.gan)、自编码器和注意力机制。此外，我们提出了一种有前景的研究实验设计，利用
    [改进的Wasserstein GANs (IWGAN)](#glo.acronym.iwgan) 的变体进行非定向离线攻击下的密码猜测。利用这些先进策略，我们可以提高密码安全性，创建更准确、更高效的
    [密码强度测量器 (PSMs)](#glo.acronym.psm)。
- en: Authentication, Deep Learning, Generative Adversarial Learning
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 身份验证、深度学习、生成对抗学习
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'Passwords have dominated the authentication system for decades, despite their
    security flaws compared to competing techniques such as cognitive authentication
    and hardware tokens. The irreplaceable is primarily due to its incomparable deployability
    and usability([bonneau2012quest,](#bib.bib2) ). However, the security of user-selected
    passwords continues to be a significant concern. According to research examining
    susceptible behaviours that affect password crackability([adams1999users,](#bib.bib1)
    ), there are three types of user actions that result in the creation of insecure
    passwords: 1\. Users often use basic terms in passwords and perform simple string
    transformations to comply with websites’ password creation policies([narayanan2005fast,](#bib.bib20)
    ). 2\. Password reuse is prevalent since the typical user has more than 20 accounts,
    and establishing unique passwords and remembering them for each account is too
    time-consuming([stobert2014password,](#bib.bib27) ). According to S.Pearman et
    al., 40% of users repeat their passwords([pearman2017let,](#bib.bib23) ). 3\.
    Users prefer to use simple-to-remember passwords that include personal information
    such as their birth date and their pets’ name. All of these behaviours expose
    the user-created passwords to attacks. Additionally, the recent large-scale leakage
    of passwords on multiple platforms across the world (listed in Table[2](#S2.T2
    "Table 2 ‣ 2.3.3\. Attention-based models ‣ 2.3\. Deep Learning-base models ‣
    2\. Background and Related Work ‣ On Deep Learning in Password Guessing, a Survey"))
    raises the alarm for researchers.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 密码在身份验证系统中主导了数十年，尽管它们相比于认知认证和硬件令牌等竞争技术存在安全缺陷。不可替代的主要原因是其无与伦比的部署性和可用性([bonneau2012quest,](#bib.bib2)
    )。然而，用户选择的密码的安全性仍然是一个重大问题。根据研究[adams1999users,](#bib.bib1) ，影响密码破解易性的易受攻击行为有三种：1\.
    用户通常在密码中使用基本术语，并进行简单的字符串转换以符合网站的密码创建政策([narayanan2005fast,](#bib.bib20) )。2\.
    密码重复使用普遍存在，因为典型用户有超过20个账户，为每个账户建立唯一密码并记住它们过于耗时([stobert2014password,](#bib.bib27)
    )。根据S.Pearman等人的研究，40%的用户重复使用他们的密码([pearman2017let,](#bib.bib23) )。3\. 用户倾向于使用容易记住的密码，这些密码包括个人信息，如出生日期和宠物的名字。所有这些行为都使用户创建的密码容易受到攻击。此外，最近全球多个平台上大规模密码泄露（见表[2](#S2.T2
    "Table 2 ‣ 2.3.3\. Attention-based models ‣ 2.3\. Deep Learning-base models ‣
    2\. Background and Related Work ‣ On Deep Learning in Password Guessing, a Survey")）引起了研究人员的警觉。
- en: 1.1\. Offline attacks and Online attacks
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1\. 离线攻击与在线攻击
- en: 'Password guessing attacks fall into two categories: offline and online. Offline
    attacks occur when attackers get cryptographic hashes of certain users’ passwords
    and attempt to recover them by guessing and testing many passwords. The primary
    objective is to determine the difficulty of cracking a genuine user’s password,
    or the strength of a user-created password, by producing a list of password guesses
    and checking for the possibility of the genuine user’s password’s occurrence.
    Offline attacks are only considered when the following conditions are met: An
    attack gains access to the system and extracts the password file, all while remaining
    unnoticed. Moreover, the file’s salting and hashing must be done appropriately.
    Otherwise, an offline assault is either ineffective (the attacker may get credentials
    directly without requiring guesses, or an online approach is more effective),
    or impossible([florencio2014administrator,](#bib.bib7) ).'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 密码猜测攻击分为两类：离线攻击和在线攻击。离线攻击发生在攻击者获得某些用户密码的加密哈希值后，通过猜测和测试许多密码来尝试恢复它们。主要目标是确定破解真实用户密码的难度，或用户创建密码的强度，通过生成密码猜测列表并检查真实用户密码出现的可能性。只有在以下条件满足时，离线攻击才被认为是有效的：攻击者获取了系统访问权限并提取了密码文件，同时未被察觉。此外，文件的盐值和哈希必须适当地完成。否则，离线攻击要么无效（攻击者可能直接获得凭据而无需猜测，或在线方法更有效），要么不可能([florencio2014administrator,](#bib.bib7)
    )。
- en: An online attack occurs when an attacker makes password attempts against users
    using a web interface or an application. This situation is more constrained for
    attackers since most authentication systems automatically freeze accounts after
    several unsuccessful attempts. Therefore, attackers must guess users’ passwords
    successfully within the allotted number of tries, which is the primary difficulty
    of online password guessing. According to Florencio et al. ([florencio2014administrator,](#bib.bib7)
    ), $10^{6}$ is a reasonable upper limit for the number of online guesses a secure
    password must survive, while the number of offline guesses is difficult to quantify
    considering the attacker’s possible usage of unlimited computers each calculating
    hashes thousands of times quicker than the target site’s backend server.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当攻击者通过网页界面或应用程序对用户进行密码尝试时，就会发生在线攻击。这种情况对攻击者更为受限，因为大多数身份验证系统在几次失败尝试后会自动冻结账户。因此，攻击者必须在规定的尝试次数内成功猜测用户的密码，这是在线密码猜测的主要难点。根据Florencio等人的研究（[florencio2014administrator,](#bib.bib7)），$10^{6}$
    是安全密码必须经受的在线猜测次数的合理上限，而离线猜测的次数则难以量化，因为攻击者可能使用无限数量的计算机，每台计算机计算哈希的速度比目标站点的后端服务器快数千倍。
- en: 1.2\. Targeted attacks and non-targeted attacks
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 定向攻击与非定向攻击
- en: Targeted guessing attacks occur when attackers attempt to break users’ passwords
    using their knowledge on users, specifically their [Personally Identifiable Information
    (PII)](#glo.acronym.pii), such as name, birth date, anniversary, home address
    etc,. This is a considerable concern when [PII](#glo.acronym.pii) becomes more
    accessible as a result of constant data breaches. On the contrary, non-targeted
    attacks do not assume the users’ identities. Be aware that the majority of individual
    accounts are not deserving of concentrated attention. Only significant or vital
    accounts relating to critical work, finances, or documents demanding a high level
    of security may be considered([florencio2014administrator,](#bib.bib7) ).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 定向猜测攻击发生在攻击者利用对用户的了解（具体是他们的[个人身份信息 (PII)](#glo.acronym.pii)，如姓名、出生日期、纪念日、家庭地址等）试图破解用户密码时。当[PII](#glo.acronym.pii)由于不断的数据泄露而变得更加容易获得时，这是一项相当严重的担忧。相反，非定向攻击不假设用户的身份。需要注意的是，大多数个人账户并不值得集中注意。只有与关键工作、财务或需要高度安全性的文件相关的重要或关键账户才可能被考虑（[florencio2014administrator,](#bib.bib7)）。
- en: This paper primarily focus on utilizing deep learning to increase passwords
    guessability under the offline and non-targeted scenario.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本文主要集中于利用深度学习在离线和非定向场景下提高密码的可猜测性。
- en: 1.3\. Contributions
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 贡献
- en: 'The following are the primary contributions of this paper:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是本文的主要贡献：
- en: •
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It provides an in-depth comparison of deep learning techniques used for untargeted
    offline password guessing.
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提供了用于非定向离线密码猜测的深度学习技术的深入比较。
- en: •
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It identifies the open challenges and possible directions for future study in
    this topic.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确定了这一主题的开放挑战和未来研究的可能方向。
- en: •
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: It proposes a viable research approach for password guessing using [GANs](#glo.acronym.gan)
    and a feasible experimental design.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提出了使用[GANs](#glo.acronym.gan)进行密码猜测的可行研究方法和实验设计。
- en: 'The rest of the paper is divided into the following sections: Section 2 discusses
    three approaches for password guessing: rule-based, probability-based, and deep
    learning-based models. The third section discusses methodology, datasets, and
    experimental design, and section 4 discusses unresolved issues and future work.
    The last part contains abbreviations and a bibliography.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本文其余部分分为以下几个部分：第2节讨论了三种密码猜测方法：基于规则、基于概率和基于深度学习的模型。第三节讨论了方法论、数据集和实验设计，第4节讨论了未解决的问题和未来的工作。最后部分包含了缩略语和参考文献。
- en: 2\. Background and Related Work
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 背景与相关工作
- en: The three predominant ways of password guessing are rule-based, probability-based,
    and deep learning-based.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 三种主要的密码猜测方式是基于规则的、基于概率的和基于深度学习的。
- en: 2.1\. Rule-based models
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1\. 基于规则的模型
- en: The large amount of stolen passwords simplifies the process of collecting password
    patterns. Following that, other candidate passwords may be produced using these
    password patterns as guidelines. Hashcat¹¹1https://hashcat.net/wiki/ and John
    the Ripper ²²2https://www.openwall.com/john/are two popular open-source password
    guessing programs that use rule-based password guessing. They provide a variety
    of ways for cracking passwords, including dictionary attacks, brute-force attacks,
    and rule-based attacks. Among all these types, the rule-based one is the fastest,
    and HashCat is the market leader in terms of speed, hash function compatibility,
    updates, and community support([hranicky2019distributed,](#bib.bib12) ). However,
    rule-based systems create passwords solely based on pre-existing rules, and developing
    rules requires domain expertise. Once rules are defined, passwords that violate
    those restrictions will not be identified.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 大量被盗的密码简化了密码模式的收集过程。之后，可以使用这些密码模式作为指导生成其他候选密码。Hashcat¹¹1https://hashcat.net/wiki/
    和 John the Ripper ²²2https://www.openwall.com/john/ 是两个流行的开源密码猜测程序，使用基于规则的密码猜测方法。它们提供了多种破解密码的方法，包括字典攻击、暴力破解攻击和基于规则的攻击。在所有这些方法中，基于规则的方法是最快的，而
    HashCat 在速度、哈希函数兼容性、更新和社区支持方面是市场领先者（[hranicky2019distributed,](#bib.bib12)）。然而，基于规则的系统仅根据预先存在的规则生成密码，而开发规则需要领域专业知识。一旦规则被定义，违反这些限制的密码将不会被识别。
- en: 2.2\. Probability-base models
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2. 概率基础模型
- en: Apart from rule-based password guessing models, conventional password guessing
    models are mostly probability-based, with two notable approaches being Markov
    Models and [Probabilistic Context-Free Grammar (PCFG)](#glo.acronym.pcfg). Markov
    Models are built on the assumption that all critical password features can be
    specified in n-grams. Its central principle is to predict the next character based
    on the preceding character([narayanan2005fast,](#bib.bib20) ). [PCFG](#glo.acronym.pcfg)
    examines the grammatical structures (combinations of special characters, digits,
    and alphanumerical sequences) in disclosed passwords and generates the distribution
    probability, after which it uses the distribution probability to produce password
    candidates([weir2009password,](#bib.bib33) ).
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 除了基于规则的密码猜测模型，传统的密码猜测模型大多是基于概率的，其中两个显著的方法是马尔可夫模型和[概率上下文无关文法（PCFG）](#glo.acronym.pcfg)。马尔可夫模型基于这样一个假设，即所有关键的密码特征可以用n-gram来指定。其核心原理是根据前一个字符预测下一个字符（[narayanan2005fast,](#bib.bib20)）。[PCFG](#glo.acronym.pcfg)
    检查已泄露密码中的语法结构（特殊字符、数字和字母数字序列的组合），生成分布概率，然后利用这些分布概率生成密码候选（[weir2009password,](#bib.bib33)）。
- en: 2.3\. Deep Learning-base models
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3. 深度学习基础模型
- en: Unlike rule-based or probability-based password guessing tools, deep learning-based
    methods make no assumptions about the password structure. Deep neural network-generated
    password samples are not constrained to a particular subset of the password space.
    Rather than that, neural networks can autonomously encode a broad range of password
    information beyond the capabilities of human-generated rules and Markovian password-generating
    methods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于规则或概率的密码猜测工具不同，基于深度学习的方法不对密码结构做出任何假设。深度神经网络生成的密码样本不受特定密码空间子集的限制。神经网络能够自主编码超出人工生成规则和马尔可夫密码生成方法能力的广泛密码信息。
- en: 2.3.1\. Recurrent Neural Networks
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.1. 递归神经网络
- en: '[RNN](#glo.acronym.rnn) are neural networks in which inputs are processed sequentially
    and restored using internal memory. They are often employed to solve sequential
    tasks such as language translation, natural language processing, voice recognition,
    and image recognition. Due to the fact that the vanilla [RNN](#glo.acronym.rnn)
    architecture is incapable of processing long-term dependencies due to the vanishing
    gradient issue([pascanu2013difficulty,](#bib.bib21) ), therefore, [Long Short
    Term Memory Networks (LSTM)](#glo.acronym.lstm) was designed to tackle the problem.
    The [LSTM](#glo.acronym.lstm) makes use of the gating mechanism depicted in Figure[1](#S2.F1
    "Figure 1 ‣ 2.3.1\. Recurrent Neural Networks ‣ 2.3\. Deep Learning-base models
    ‣ 2\. Background and Related Work ‣ On Deep Learning in Password Guessing, a Survey")
    to retain information in memory for long periods of time([hochreiter1997long,](#bib.bib11)
    ).'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '[RNN](#glo.acronym.rnn)是输入按序列处理并使用内部记忆恢复的神经网络。它们通常用于解决语言翻译、自然语言处理、语音识别和图像识别等序列任务。由于普通[RNN](#glo.acronym.rnn)架构由于梯度消失问题（[pascanu2013difficulty](#bib.bib21)）无法处理长期依赖，因此设计了[长短期记忆网络（LSTM）](#glo.acronym.lstm)来解决这个问题。[LSTM](#glo.acronym.lstm)利用图中[1](#S2.F1
    "Figure 1 ‣ 2.3.1\. Recurrent Neural Networks ‣ 2.3\. Deep Learning-base models
    ‣ 2\. Background and Related Work ‣ On Deep Learning in Password Guessing, a Survey")所示的门控机制来在内存中保持信息较长时间（[hochreiter1997long](#bib.bib11)）。'
- en: '![Refer to caption](img/52161e0135b719f55e09a4e02217cbb8.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![参见图注](img/52161e0135b719f55e09a4e02217cbb8.png)'
- en: Figure 1\. The Gating Mechanism in [LSTM](#glo.acronym.lstm).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1\. [LSTM](#glo.acronym.lstm)中的门控机制。
- en: To the best of our knowledge, Melicher et al.([melicher2016fast,](#bib.bib17)
    ) were the first to utilize [RNN](#glo.acronym.rnn) to extract and predict password
    features. They kept their model, named [Fast, Lean, Accurate (FLA)](#glo.acronym.fla),
    as lightweight as possible in order to integrate it into local browsers for proactive
    password verification. Three [LSTM](#glo.acronym.lstm) layers and two highly connected
    layers comprise the proposed Neural Network. Various strategies for training neural
    networks on passwords were used. It was proven that employing transfer learning([yosinski2014transferable,](#bib.bib36)
    ) significantly improves guessing efficacy, however, adding natural language dictionaries
    to the training set and tutoring had little impact. Consequently, they discovered
    that Neural Networks are superior at guessing passwords when the number of guesses
    is increased and when more complicated or longer password policies are targeted.
    Nevertheless, because of the Markovian nature of [FLA](#glo.acronym.fla)’s password
    generation process, any password feature that is not included within the scope
    of an n-gram may be omitted from encoding([hitaj2019passgan,](#bib.bib10) ).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 据我们所知，Melicher等人（[melicher2016fast](#bib.bib17)）是首次利用[RNN](#glo.acronym.rnn)来提取和预测密码特征的。他们将其模型命名为[Fast,
    Lean, Accurate (FLA)](#glo.acronym.fla)，尽可能保持轻量化，以便将其集成到本地浏览器中进行主动密码验证。提议的神经网络包括三层[LSTM](#glo.acronym.lstm)层和两层高度连接的层。使用了各种针对密码的神经网络训练策略。研究证明，采用迁移学习（[yosinski2014transferable](#bib.bib36)）显著提高了猜测的有效性，然而，将自然语言词典添加到训练集和辅导中的效果却很小。因此，他们发现，当增加猜测次数以及目标更复杂或更长的密码策略时，神经网络在猜测密码方面表现更优。然而，由于[FLA](#glo.acronym.fla)的密码生成过程具有马尔可夫性质，因此任何未包含在n-gram范围内的密码特征可能会被省略编码（[hitaj2019passgan](#bib.bib10)）。
- en: Zhang et al.([zhang2018password,](#bib.bib37) ) presented [Structure Partition
    and Bilstm Recurrent Neural Network (SPRNN)](#glo.acronym.sprnn), a hybrid password
    attack technique based on structural partitioning and [Bidirectional Long Short-term
    Memory (BiLSTM)](#glo.acronym.bilstm). The [PCFG](#glo.acronym.pcfg) is used for
    structure partitioning, which seeks to structure the password training set to
    learn users’ habit of password construction and generate a collection of basic
    structures and string dictionaries ordered by likelihood. The [BiLSTM](#glo.acronym.bilstm)
    was then trained using the string dictionary produced by [PCFG](#glo.acronym.pcfg).
    They compared SPRNN’s performance to probability-based approaches (Markov Models
    and [PCFG](#glo.acronym.pcfg)) on both cross-site (model trained and tested on
    various datasets) and one-site (model trained and tested on subsets of the same
    dataset) scenarios. [SPRNN](#glo.acronym.sprnn) outperforms the other two models
    in all circumstances, albeit it performs worse cross-site than one-site.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等人（[zhang2018password](#bib.bib37)）提出了[结构分区与双向长短期记忆网络（SPRNN）](#glo.acronym.sprnn)，这是一种基于结构分区和[双向长短期记忆（BiLSTM）](#glo.acronym.bilstm)的混合密码攻击技术。[PCFG](#glo.acronym.pcfg)用于结构分区，旨在对密码训练集进行结构化，以学习用户的密码构造习惯，并生成按可能性排序的基本结构和字符串词典。[BiLSTM](#glo.acronym.bilstm)随后使用由[PCFG](#glo.acronym.pcfg)生成的字符串词典进行训练。他们将SPRNN的表现与基于概率的方法（Markov模型和[PCFG](#glo.acronym.pcfg)）在跨站点（模型在不同数据集上训练和测试）和单站点（模型在同一数据集的子集上训练和测试）场景中进行了比较。[SPRNN](#glo.acronym.sprnn)在所有情况下都优于其他两种模型，但在跨站点场景中表现不如单站点场景。
- en: 'Based on Zhang et al.’s work([zhang2018password,](#bib.bib37) ), the same year,
    Liu et al.([liu2018genpass,](#bib.bib16) ) also developed a hybrid model named
    GENPass that can be generalized to cross-sites attacks. The model preprocesses
    a password by encoding it into a series of units that are then given tags based
    on [PCFG](#glo.acronym.pcfg) (e.g., ’password123’ can be separated into two units:
    ’D8’ and ’L3’). After that, [LSTM](#glo.acronym.lstm) is used to create passwords.
    Additionally, they built a [Convolutional Neural Networks (CNN)](#glo.acronym.cnn)
    classifier to determine which wordlist the password is most likely to originate
    from. The results indicate that GENPass can achieve the same degree of security
    as the [LSTM](#glo.acronym.lstm) model alone in a one-site test while generating
    passwords with a substantially lower rank. GENPass enhanced the matching rate
    by 16 to 30% when compared to [LSTM](#glo.acronym.lstm) alone in the cross-site
    test.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Zhang等人的工作（[zhang2018password](#bib.bib37)），同年，Liu 等人（[liu2018genpass](#bib.bib16)）还开发了一种名为GENPass的混合模型，该模型可以推广到跨站点攻击。该模型通过将密码编码为一系列单元来预处理密码，然后根据[PCFG](#glo.acronym.pcfg)为这些单元打上标签（例如，’password123’可以被分为两个单元：’D8’和’L3’）。之后，[LSTM](#glo.acronym.lstm)被用来生成密码。此外，他们建立了一个[卷积神经网络（CNN）](#glo.acronym.cnn)分类器，以确定密码最有可能来源于哪个词表。结果表明，GENPass在单站点测试中可以达到与单独[LSTM](#glo.acronym.lstm)模型相同的安全程度，同时生成的密码排名显著较低。在跨站点测试中，与单独[LSTM](#glo.acronym.lstm)相比，GENPass提高了16%到30%的匹配率。
- en: 2.3.2\. Autoencoders
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.2\. 自编码器
- en: 'Autoencoders are any model architecture that is composed of two submodules:
    an encoder and a decoder. The encoder is responsible for learning the representation
    of the source text at each time step and generating a latent representation of
    the whole source sentence, which the decoder uses as an input to build a meaningful
    output of the original phrase. Typically, autoencoders are employed to deal with
    sequential data and various NLP tasks, such as machine translation, text summarization,
    and question answering. [RNN](#glo.acronym.rnn) and [CNN](#glo.acronym.cnn) are
    often used as encoder and decoder components, respectively.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种由两个子模块组成的模型架构：编码器和解码器。编码器负责在每个时间步骤学习源文本的表示，并生成整个源句子的潜在表示，解码器使用这一表示作为输入来构建原始短语的有意义输出。自编码器通常用于处理序列数据和各种自然语言处理任务，如机器翻译、文本摘要和问答。[RNN](#glo.acronym.rnn)和[CNN](#glo.acronym.cnn)常被用作编码器和解码器组件。
- en: Pasquini et al.([pasquini2021improving,](#bib.bib22) ) applied this strategy
    to a dataset containing leaked passwords, using [GANs](#glo.acronym.gan) and Wasserstein
    Autoencoders (WAEs) to develop a suitable representation of the observed password
    distribution rather than directly predicting it. Their methodology, called [Dynamic
    Password Guessing (DPG)](#glo.acronym.dpg), can guess passwords that are unique
    to the password set. and they are the first to apply completely unsupervised representation
    learning to the area of password guessing.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Pasquini 等人 ([pasquini2021improving,](#bib.bib22)) 将这一策略应用于包含泄露密码的数据集，使用 [GANs](#glo.acronym.gan)
    和 Wasserstein 自编码器 (WAEs) 来开发对观察到的密码分布的合适表示，而不是直接预测密码。他们的方法论称为 [动态密码猜测 (DPG)](#glo.acronym.dpg)，可以猜测密码集中特有的密码。他们是第一批将完全无监督的表示学习应用于密码猜测领域的研究者。
- en: 2.3.3\. Attention-based models
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 2.3.3\. 基于注意力的模型
- en: When we use the phrase ”Attention” in English, we mean concentrating our focus
    on something and paying closer attention. The Attention mechanism in Deep Learning
    is based on this principle, and it prioritizes certain tokens (words, letters,
    and phrases) while processing text inputs. This, intuitively, aids the model in
    gaining a better knowledge of the textual structure (e.g., grammar, semantic meaning,
    word structure, and so on) and hence improve text classification, generation and
    interpretability. In language models, attention mechanisms are often utilized
    in combination with [RNN](#glo.acronym.rnn) and [CNN](#glo.acronym.cnn). However,
    even with [LSTM](#glo.acronym.lstm), these models cannot manage lengthy dependencies
    since transforming the whole source sentence to a fixed-length context vector
    is challenging when the source sentence is too long. As a result, Transformers([vaswani2017attention,](#bib.bib31)
    ) were invented that were built just on Attention, without convolution or recurrent
    layers. [Bidirectional Encoder Representations from Transformers (BERT)](#glo.acronym.bert)
    ([devlin2018bert,](#bib.bib6) ), ELMO([peters2018deep,](#bib.bib24) ), and GPT([radford2018improving,](#bib.bib26)
    ) are all well-known instances of attention-based applications built on top of
    Transformers.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们用英语中的“Attention”（注意力）一词时，我们指的是集中注意力并更专注于某事。深度学习中的注意力机制基于这一原则，它在处理文本输入时优先考虑某些标记（词、字母和短语）。这直观上有助于模型更好地理解文本结构（例如，语法、语义意义、词结构等），从而改善文本分类、生成和可解释性。在语言模型中，注意力机制通常与
    [RNN](#glo.acronym.rnn) 和 [CNN](#glo.acronym.cnn) 结合使用。然而，即使是 [LSTM](#glo.acronym.lstm)，这些模型也无法处理较长的依赖关系，因为当源句子太长时，将整个源句子转换为固定长度的上下文向量是具有挑战性的。因此，基于注意力的
    Transformer ([vaswani2017attention,](#bib.bib31)) 被发明了，它们完全依赖注意力机制，没有卷积或递归层。 [双向编码器表示
    (BERT)](#glo.acronym.bert) ([devlin2018bert,](#bib.bib6))、ELMO ([peters2018deep,](#bib.bib24))
    和 GPT ([radford2018improving,](#bib.bib26)) 是基于 Transformer 的注意力应用的著名实例。
- en: Table 1\. A comparison for Deep Learning Models used for Password Guessing.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 用于密码猜测的深度学习模型比较。
- en: '| Category | Methods | Models used | Year |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 方法 | 使用的模型 | 年份 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Autoencoders | [DPG](#glo.acronym.dpg) ([pasquini2021improving,](#bib.bib22)
    ) | WAE, [GANs](#glo.acronym.gan) | 2021 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 自编码器 | [DPG](#glo.acronym.dpg) ([pasquini2021improving,](#bib.bib22)) | WAE,
    [GANs](#glo.acronym.gan) | 2021 |'
- en: '| [GANs](#glo.acronym.gan) | REDPACK([nam2020generating,](#bib.bib19) ) | [IWGAN](#glo.acronym.iwgan),
    RaGAN, HashCat, [PCFG](#glo.acronym.pcfg) | 2020 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| [GANs](#glo.acronym.gan) | REDPACK ([nam2020generating,](#bib.bib19)) | [IWGAN](#glo.acronym.iwgan),
    RaGAN, HashCat, [PCFG](#glo.acronym.pcfg) | 2020 |'
- en: '| [GANs](#glo.acronym.gan) | PassGAN([hitaj2019passgan,](#bib.bib10) ) | [IWGAN](#glo.acronym.iwgan)
    | 2019 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| [GANs](#glo.acronym.gan) | PassGAN ([hitaj2019passgan,](#bib.bib10)) | [IWGAN](#glo.acronym.iwgan)
    | 2019 |'
- en: '| Attention | Language Model([li2019password,](#bib.bib15) ) | [BERT](#glo.acronym.bert),
    [LSTM](#glo.acronym.lstm) | 2019 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 注意力 | 语言模型 ([li2019password,](#bib.bib15)) | [BERT](#glo.acronym.bert), [LSTM](#glo.acronym.lstm)
    | 2019 |'
- en: '| [RNN](#glo.acronym.rnn) | GENPass([liu2018genpass,](#bib.bib16) ) | [PCFG](#glo.acronym.pcfg),
    [LSTM](#glo.acronym.lstm), [CNN](#glo.acronym.cnn) | 2018 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| [RNN](#glo.acronym.rnn) | GENPass ([liu2018genpass,](#bib.bib16)) | [PCFG](#glo.acronym.pcfg),
    [LSTM](#glo.acronym.lstm), [CNN](#glo.acronym.cnn) | 2018 |'
- en: '| [RNN](#glo.acronym.rnn) | SPRNN([zhang2018password,](#bib.bib37) ) | [PCFG](#glo.acronym.pcfg),
    [BiLSTM](#glo.acronym.bilstm) | 2018 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| [RNN](#glo.acronym.rnn) | SPRNN ([zhang2018password,](#bib.bib37)) | [PCFG](#glo.acronym.pcfg),
    [BiLSTM](#glo.acronym.bilstm) | 2018 |'
- en: '| [RNN](#glo.acronym.rnn) | [FLA](#glo.acronym.fla) ([melicher2016fast,](#bib.bib17)
    ) | [LSTM](#glo.acronym.lstm) | 2016 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| [RNN](#glo.acronym.rnn) | [FLA](#glo.acronym.fla) ([melicher2016fast,](#bib.bib17))
    | [LSTM](#glo.acronym.lstm) | 2016 |'
- en: Li et al.([li2019password,](#bib.bib15) ) proposed a curated Deep Neural Network
    architecture consisted of five [LSTM](#glo.acronym.lstm) layers and an output
    layer, and then tutored and improved the created model using [BERT](#glo.acronym.bert).
    They proved that the tutoring process by [BERT](#glo.acronym.bert) can help increase
    the model performance significantly.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: Li 等人([li2019password,](#bib.bib15) ) 提出了一个精心设计的深度神经网络架构，包括五个 [LSTM](#glo.acronym.lstm)
    层和一个输出层，然后使用 [BERT](#glo.acronym.bert) 对创建的模型进行了辅导和改进。他们证明了通过 [BERT](#glo.acronym.bert)
    的辅导过程可以显著提高模型性能。
- en: A comparison of prior published deep learning-based password guessing tools
    is illustrated in Table[1](#S2.T1 "Table 1 ‣ 2.3.3\. Attention-based models ‣
    2.3\. Deep Learning-base models ‣ 2\. Background and Related Work ‣ On Deep Learning
    in Password Guessing, a Survey").
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 先前发布的基于深度学习的密码猜测工具的比较见于表[1](#S2.T1 "表 1 ‣ 2.3.3\. 基于注意力的模型 ‣ 2.3\. 深度学习模型 ‣
    2\. 背景与相关工作 ‣ 关于密码猜测的深度学习综述")。
- en: Table 2\. Datasets used for training and evaluating deep learning models. Size
    is the number of passwords in the dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2\. 用于训练和评估深度学习模型的数据集。大小是数据集中密码的数量。
- en: '| Name | Size | Brief Description |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 名称 | 大小 | 简要描述 |'
- en: '| --- | --- | --- |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Yahoo | $4.4\times 10^{5}$ | A web services provider. |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Yahoo | $4.4\times 10^{5}$ | 一个网络服务提供商。 |'
- en: '| phpBB | $3\times 10^{5}$ | A software website. |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| phpBB | $3\times 10^{5}$ | 一个软件网站。 |'
- en: '| RockYou | $1.4\times 10^{7}$ | A gaming platform. |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| RockYou | $1.4\times 10^{7}$ | 一个游戏平台。 |'
- en: '| Myspace | $5.5\times 10^{4}$ | A social networking platform. |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| Myspace | $5.5\times 10^{4}$ | 一个社交网络平台。 |'
- en: '| SkullSecurityComp | $6.7\times 10^{6}$ | Compilations of passwords lists.
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| SkullSecurityComp | $6.7\times 10^{6}$ | 密码列表的汇编。 |'
- en: '| LinkedIn | $1.3\times 10^{6}$ | A social online platform. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| LinkedIn | $1.3\times 10^{6}$ | 一个社交在线平台。 |'
- en: 3\. Methodology
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 方法论
- en: 3.1\. Problem Statement
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 问题陈述
- en: It is a consensus in academia that password strength estimates based on entropy,
    and marginal guesswork analysis([pliam2000incomparability,](#bib.bib25) ) are
    insufficient. Rather than that, it is more suitable to quantify it in terms of
    the number of attempts required to find a password collision using the most efficient
    adversarial attacks([dell2015monte,](#bib.bib5) ). Thus, password guessing attacks
    can be used for password security analysis and be developed into [PSMs](#glo.acronym.psm)
    that can be integrated into an intelligent user interface to encourage users to
    establish stronger passwords. I will concentrate my study on utilizing [GANs](#glo.acronym.gan)
    to complete the password guessing task.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 学术界普遍认为基于熵的密码强度估计和边际猜测分析([pliam2000incomparability,](#bib.bib25) ) 是不够的。与其这样，不如量化为使用最有效的对抗攻击([dell2015monte,](#bib.bib5)
    ) 找到密码冲突所需的尝试次数。因此，密码猜测攻击可以用于密码安全分析，并发展成可以集成到智能用户界面中的 [密码安全模型（PSMs）](#glo.acronym.psm)，以鼓励用户设置更强的密码。我将集中研究利用
    [生成对抗网络（GANs）](#glo.acronym.gan) 完成密码猜测任务。
- en: 3.2\. [GANs](#glo.acronym.gan)
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. [生成对抗网络（GANs）](#glo.acronym.gan)
- en: 'Unlike the previously described deep learning-based algorithms commonly employed
    in Natural Language Processing tasks, [GANs](#glo.acronym.gan) ([goodfellow2014generative,](#bib.bib8)
    ) has been used to construct simulations of pictures, texts, and voice across
    all domains. Behind the scenes, [GANs](#glo.acronym.gan) consists of two sub-modules:
    a discriminator (D) and a generator (G), both of which are built of deep learning
    neural networks. G accepts noise or random features as input; learns the probability
    of the input’s features; and generates fake data that follows the distribution
    of the input data. While D makes every effort to discriminate between actual samples
    and those created artificially by G by estimating the conditional probability
    of an example being false (or real) given a set of inputs (or features). The model
    architecture diagram is illustrated in Figure [2](#S3.F2 "Figure 2 ‣ 3.2\. GANs
    ‣ 3\. Methodology ‣ On Deep Learning in Password Guessing, a Survey"). This cat-and-mouse
    game compels D to extract necessary information in training data; this information
    assists G in precisely replicating the original data distribution. D and G compete
    against one another during the training phase, which progressively improves their
    performance with each iteration. Typically, proper gradient descent and regularization
    techniques must be used to accelerate the whole process. More formally, the optimization
    problem solved by [GANs](#glo.acronym.gan) can be summarized as follows:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前描述的通常应用于自然语言处理任务的基于深度学习的算法不同，[生成对抗网络（GANs）](#glo.acronym.gan) ([goodfellow2014generative,](#bib.bib8))
    已被用于构建各种领域的图片、文本和语音的模拟。在背后，[生成对抗网络（GANs）](#glo.acronym.gan) 包含两个子模块：一个判别器（D）和一个生成器（G），这两个模块都是由深度学习神经网络构建的。G
    接收噪声或随机特征作为输入；学习输入特征的概率；并生成遵循输入数据分布的假数据。与此同时，D 尽力通过估计给定一组输入（或特征）的样本为假的（或真实的）条件概率来区分实际样本和由
    G 人工创建的样本。模型架构图如图 [2](#S3.F2 "Figure 2 ‣ 3.2\. GANs ‣ 3\. Methodology ‣ On Deep
    Learning in Password Guessing, a Survey") 所示。这种猫捉老鼠的游戏迫使 D 从训练数据中提取必要的信息；这些信息帮助
    G 精确地复制原始数据分布。在训练阶段，D 和 G 相互竞争，每次迭代都会逐步提高它们的性能。通常，必须使用适当的梯度下降和正则化技术来加速整个过程。更正式地说，[生成对抗网络（GANs）](#glo.acronym.gan)
    解决的优化问题可以总结如下：
- en: '|  | $\displaystyle\min_{\theta_{G}}\max_{\theta_{D}}\sum_{i=1}^{n}logf(x_{i};\theta_{D})+\sum_{i=1}^{n}log(1-f(g(z_{j};\theta_{G});\theta_{D}))$
    |  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | $\displaystyle\min_{\theta_{G}}\max_{\theta_{D}}\sum_{i=1}^{n}logf(x_{i};\theta_{D})+\sum_{i=1}^{n}log(1-f(g(z_{j};\theta_{G});\theta_{D}))$
    |  |'
- en: where $f(x;\theta_{D})$ and $g(z_{j};\theta_{G})$ represents the discriminator
    D and the generator G respectively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $f(x;\theta_{D})$ 和 $g(z_{j};\theta_{G})$ 分别代表判别器 D 和生成器 G。
- en: '![Refer to caption](img/14f16859dbfb19f81caa7ba0a5319372.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明文字](img/14f16859dbfb19f81caa7ba0a5319372.png)'
- en: Figure 2\. Model architecture diagram.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2\. 模型架构图。
- en: The optimization demonstrates the min-max game between D and G. After the initial
    [GANs](#glo.acronym.gan) work was published in 2014, several enhancements were
    made, and Hitaj et al.([hitaj2019passgan,](#bib.bib10) ) leveraged the [IWGAN](#glo.acronym.iwgan)
    ([gulrajani2017improved,](#bib.bib9) ) to apply [GANs](#glo.acronym.gan) on password
    guessing, which is the first in literature. They trained the discriminator using
    a collection of leaked passwords (actual samples). Each iteration brings the generator’s
    output closer to the distribution of genuine passwords, increasing the likelihood
    of matching real-world users’ passwords. Consequently, PassGAN outperformed current
    rule-based password guessing tools and state-of-the-art machine learning password
    guessing technologies ([FLA](#glo.acronym.fla)) after sufficient passwords were
    generated ($10^{9}$). They matched 51% - 73% of passwords when combining PassGAN
    with HashCat, compared to 17.67% when using HashCat alone and 21% when using PassGAN
    alone. One disadvantage of PassGAN is that it has intrinsic training instability
    due to the final softmax activation function in the generator, which may easily
    result in the network being vulnerable to vanishing gradients, lowering the guessing
    accuracy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 优化展示了D和G之间的极小极大博弈。在2014年首次发布的[GANs](#glo.acronym.gan)工作之后，进行了若干改进，Hitaj等人([hitaj2019passgan,](#bib.bib10))利用了[IWGAN](#glo.acronym.iwgan)
    ([gulrajani2017improved,](#bib.bib9))将[GANs](#glo.acronym.gan)应用于密码猜测，这在文献中是首次。他们使用一组泄露密码（实际样本）训练判别器。每次迭代使生成器的输出更接近真实密码的分布，从而增加了与真实用户密码匹配的可能性。因此，在生成了足够的密码（$10^{9}$）后，PassGAN超越了当前的基于规则的密码猜测工具和最先进的机器学习密码猜测技术（[FLA](#glo.acronym.fla)）。当将PassGAN与HashCat结合使用时，它们匹配了51%
    - 73%的密码，而使用HashCat单独为17.67%，使用PassGAN单独为21%。PassGAN的一个缺点是，由于生成器中的最终softmax激活函数，训练本身不稳定，这可能使网络容易出现梯度消失问题，从而降低猜测准确性。
- en: Following the publication of PassGAN in 2019, other researchers saw the possibilities
    of using [GANs](#glo.acronym.gan) for password guessing, and more refinements
    have been done on top of PassGAN. In 2020, Nam et al.([nam2020generating,](#bib.bib19)
    ) developed REDPACK that employs a variant of [GANs](#glo.acronym.gan) in conjunction
    with various password generation models for improved cracking performance. They
    suggested rPassGAN in their prior study, which enhanced PassGAN by altering its
    fundamental Neural Network architecture. More precisely, they employed [RNN](#glo.acronym.rnn)
    in PassGAN instead of ResNet in PassGAN’s original paper. However, during rPassGAN’s
    training process, it became unstable at times, and REDPACK introduced the RaSGGAN-GP
    cost function to stabilize the training process. Nam et al. also introduced a
    selection phase to REDPACK, during which the password candidates are generated
    using several password generators (Hashcat, acpcfg, and rPassGAN). The discriminator
    then determines the chance of each generator’s password candidates being realistic
    and sends the candidates with the greatest probability to password cracking tools
    such as HashCat. We regard PassGAN to be a good representation of [GANs](#glo.acronym.gan)-based
    password guessing tools, and PassGAN enhancements will be the focus of my study.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在2019年发布PassGAN之后，其他研究人员看到了使用[GANs](#glo.acronym.gan)进行密码猜测的可能性，并在PassGAN的基础上进行了更多改进。2020年，Nam等人([nam2020generating,](#bib.bib19))开发了REDPACK，这是一种结合了不同密码生成模型的[GANs](#glo.acronym.gan)变体，以提高破解性能。他们在之前的研究中提出了rPassGAN，通过改变PassGAN的基本神经网络架构来增强PassGAN。更具体地说，他们在PassGAN中使用了[RNN](#glo.acronym.rnn)而不是PassGAN原始论文中的ResNet。然而，在rPassGAN的训练过程中，模型有时会变得不稳定，REDPACK引入了RaSGGAN-GP成本函数来稳定训练过程。Nam等人还向REDPACK中引入了选择阶段，在该阶段，密码候选项是使用几个密码生成器（Hashcat、acpcfg和rPassGAN）生成的。然后，判别器确定每个生成器生成的密码候选项的真实性，并将概率最大的候选项发送给密码破解工具如HashCat。我们认为PassGAN是基于[GANs](#glo.acronym.gan)的密码猜测工具的一个良好代表，PassGAN的改进将成为我研究的重点。
- en: 3.3\. Dataset
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 数据集
- en: Table [2](#S2.T2 "Table 2 ‣ 2.3.3\. Attention-based models ‣ 2.3\. Deep Learning-base
    models ‣ 2\. Background and Related Work ‣ On Deep Learning in Password Guessing,
    a Survey") summarizes the publicly accessible datasets that are often used in
    password guessing. While one disadvantage of prior work on the dataset is that
    most studies did not include the critical nature of password policy. In reality,
    most websites impose password policy restrictions to encourage users to generate
    strong passwords. For example, create a password with at least one special symbol
    and eight characters in both lowercase and uppercase([bonneau2010password,](#bib.bib3)
    ). In my research, I will consider the password policy when preprocessing datasets
    to imitate real-world password creation circumstances more accurately.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 表格[2](#S2.T2 "表格 2 ‣ 2.3.3. 基于注意力的模型 ‣ 2.3. 深度学习模型 ‣ 2. 背景和相关工作 ‣ 关于密码猜测的深度学习调研")
    总结了常用于密码猜测的公开数据集。以往对这些数据集的一个缺点是大多数研究没有考虑密码政策的关键性。实际上，大多数网站施加密码政策限制以鼓励用户生成强密码。例如，创建一个至少包含一个特殊符号并且包括八个字符的密码（包括小写和大写字母）（[bonneau2010password](#bib.bib3)）。在我的研究中，我将在预处理数据集时考虑密码政策，以更准确地模拟现实世界中的密码创建情况。
- en: Dataset that I can use for training and evaluation:[Weakpass](https://weakpass.com/wordlist/1920)
    (Contains only passwords that are suitable for password policy, length more than
    8, with 2133708093 passwords) and the smaller dataset from [GitHub](https://github.com/danielmiessler/SecLists/tree/master/Passwords/Leaked-Databases).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我可以用于训练和评估的数据集：[Weakpass](https://weakpass.com/wordlist/1920)（仅包含适合密码政策的密码，长度超过
    8，共有 2133708093 个密码）和来自[GitHub](https://github.com/danielmiessler/SecLists/tree/master/Passwords/Leaked-Databases)的较小数据集。
- en: 3.4\. Experimental design
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4. 实验设计
- en: Tensorflow or Pytorch serves as the primary framework for the experiments. Reproducing
    PassGAN’s work([hitaj2019passgan,](#bib.bib10) ) is the first step in my research.
    However, there are limitations of PassGAN and to address which and improve PassGAN
    will be the focus of my work. The vanilla [GANs](#glo.acronym.gan) stays unstable
    throughout the training process because the discriminator’s steep gradient space,
    which results in mode collapse during the generator’s training phase([thanh2019improving,](#bib.bib30)
    ). As a consequence, the generator is prone to deceive the discriminator before
    mastering the art of creating more realistic passwords. Although the [IWGAN](#glo.acronym.iwgan)
    utilized in PassGAN uses the Wasserstein Loss function, which, in comparison to
    the Binary Cross Entropy Loss used in the vanilla [GANs](#glo.acronym.gan), aids
    in the resolution of mode collapse and vanishing gradient difficulties in some
    degree. However, in order to train [GANs](#glo.acronym.gan) using Wasserstein-loss,
    the discriminator must be 1-Lipschitz continuous, which means that the gradient’s
    norm should be at most 1 at every point. While [IWGAN](#glo.acronym.iwgan) used
    a gradient penalty to guarantee 1-Lipschitz continuity, Wu et al.([GNGAN_2021_ICCV,](#bib.bib35)
    ) proved that ”the Lipschitz constant of a layer-wise Lipschitz constraint network
    is upper-bounded by any of its k-layer subnetworks.” Thus, I will explore alternative
    techniques aimed at fulfilling the Lipschitz constraint while applying Wasserstein
    loss or stabilizing the training process by further regularization and normalizing
    on the discriminator([GNGAN_2021_ICCV,](#bib.bib35) ; [gulrajani2017improved,](#bib.bib9)
    ; [jabbar2021survey,](#bib.bib13) ; [thanh2019improving,](#bib.bib30) ; [wei2018improving,](#bib.bib32)
    ; [wu2018wasserstein,](#bib.bib34) ; [terjek2019adversarial,](#bib.bib29) ; [jiang2018computation,](#bib.bib14)
    ). By comparing the matching result of the testing set to the synthesized set,
    we can determine if the model we build can outperform PassGAN.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Tensorflow或Pytorch作为实验的主要框架。再现PassGAN的工作([hitaj2019passgan,](#bib.bib10) )是我研究的第一步。然而，PassGAN存在一些限制，解决这些限制并改进PassGAN将是我工作的重点。基础的[GANs](#glo.acronym.gan)在整个训练过程中保持不稳定，因为判别器的陡峭梯度空间，这导致生成器训练阶段出现模式崩溃([thanh2019improving,](#bib.bib30)
    )。因此，生成器容易在掌握创建更真实密码的艺术之前欺骗判别器。尽管PassGAN中使用的[IWGAN](#glo.acronym.iwgan)利用了Wasserstein
    Loss函数，相较于基础的[GANs](#glo.acronym.gan)中使用的Binary Cross Entropy Loss，Wasserstein
    Loss在一定程度上有助于解决模式崩溃和梯度消失问题。然而，为了使用Wasserstein-loss训练[GANs](#glo.acronym.gan)，判别器必须是1-Lipschitz连续的，这意味着在每一点上梯度的范数应该至多为1。虽然[IWGAN](#glo.acronym.iwgan)使用了梯度惩罚以保证1-Lipschitz连续性，但Wu等([GNGAN_2021_ICCV,](#bib.bib35)
    )证明了“层级Lipschitz约束网络的Lipschitz常数被其任何k层子网络上界。”因此，我将探索其他技术，旨在实现Lipschitz约束，同时应用Wasserstein损失或通过进一步正则化和标准化来稳定训练过程([GNGAN_2021_ICCV,](#bib.bib35)
    ; [gulrajani2017improved,](#bib.bib9) ; [jabbar2021survey,](#bib.bib13) ; [thanh2019improving,](#bib.bib30)
    ; [wei2018improving,](#bib.bib32) ; [wu2018wasserstein,](#bib.bib34) ; [terjek2019adversarial,](#bib.bib29)
    ; [jiang2018computation,](#bib.bib14) )。通过比较测试集与合成集的匹配结果，我们可以确定我们建立的模型是否能超越PassGAN。
- en: Table 3\. Unique password distribution in the Rockyou dataset
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 表3\. Rockyou数据集中唯一密码分布
- en: '| Length | Frequency |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 长度 | 频率 |'
- en: '| $<8$ | 15.55% |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| $<8$ | 15.55% |'
- en: '| 8-10 | 38.16% |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 8-10 | 38.16% |'
- en: '| 10-12 | 29.32% |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 10-12 | 29.32% |'
- en: '| 12-16 | 14.18% |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 12-16 | 14.18% |'
- en: '| $>=16$ | 2.80% |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| $>=16$ | 2.80% |'
- en: '| Total | $1.43*10^{7}$ |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | $1.43*10^{7}$ |'
- en: The datasets listed in Table[2](#S2.T2 "Table 2 ‣ 2.3.3\. Attention-based models
    ‣ 2.3\. Deep Learning-base models ‣ 2\. Background and Related Work ‣ On Deep
    Learning in Password Guessing, a Survey") will be used to train and test my curated
    model. Small, medium and big password candidate files will be produced to evaluate
    the model’s guessing capability. The big one may be up to 6.5 GB in size and include
    over $8\times 10^{9}$ unique passwords. To train a deep learning model and to
    produce such an enormous dataset requires substantial computational resources.
    A Tesla P100 GPU or above is required to reduce training and generation time.
    A system with at least 16 GB of RAM is required to prevent the process from being
    killed due to running out of memory while reading the created file to calculate
    guessing accuracy.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表[2](#S2.T2 "表 2 ‣ 2.3.3\. 基于注意力的模型 ‣ 2.3\. 深度学习模型 ‣ 2\. 背景及相关工作 ‣ 关于深度学习在密码猜测中的应用，综述")中列出的数据集将用于训练和测试我整理的模型。将产生小型、中型和大型密码候选文件来评估模型的猜测能力。大型文件的大小可能达到6.5
    GB，并包含超过$8\times 10^{9}$个唯一密码。训练深度学习模型并生成如此巨大的数据集需要大量的计算资源。需要Tesla P100 GPU或更高版本以减少训练和生成时间。需要至少16
    GB的RAM，以防在读取创建的文件以计算猜测准确性时因内存不足而导致进程被终止。
- en: '![Refer to caption](img/6149ac5a3cb26a65ef125d85f5802cfd.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/6149ac5a3cb26a65ef125d85f5802cfd.png)'
- en: Figure 3\. Data Flow Diagram.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3\. 数据流图。
- en: Note that the work is based on the assumption that the training set and testing
    set has the same distribution, therefore by simulating samples from the training
    set, the produced samples can substantially approximating the test set. The two
    sets are produced by dividing leaked passwords to 4:1 in portion, and no duplicate
    exist in the two sets. By shuffling before splitting, we assume that the two sets
    have the same distribution. The dataflow diagram is illustrated in Figure[3](#S3.F3
    "Figure 3 ‣ 3.4\. Experimental design ‣ 3\. Methodology ‣ On Deep Learning in
    Password Guessing, a Survey").
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这项工作是基于训练集和测试集具有相同分布的假设，因此通过对训练集进行样本模拟，生成的样本可以在很大程度上接近测试集。这两个数据集是通过将泄露的密码按4:1的比例划分而成，并且两个数据集中不存在重复项。通过在拆分之前进行洗牌，我们假设这两个数据集具有相同的分布。数据流图如图[3](#S3.F3
    "图 3 ‣ 3.4\. 实验设计 ‣ 3\. 方法论 ‣ 关于深度学习在密码猜测中的应用，综述")所示。
- en: 4\. Experimental results
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 实验结果
- en: I used Rockyou Dataset to train the [IWGAN](#glo.acronym.iwgan) model, the dataset
    it preprocessed so that all passwords are unique, there is no password both exiting
    in the testing set and training set. A distribution of the passwords based on
    length is illustrated in Table[3](#S3.T3 "Table 3 ‣ 3.4\. Experimental design
    ‣ 3\. Methodology ‣ On Deep Learning in Password Guessing, a Survey"). There are
    two minor modifications in my experiment and Hitaj et al. ([hitaj2019passgan,](#bib.bib10)
    )’s. First, when preprocessing the dataset, I restrict all passwords to 8 to 12
    in length, which is more in line with the modern password creating policy. While
    Hitaj et al. limited all passwords to less than 10\. However, in the real world
    scenario, a password with less than eight characters is regarded as weak and even
    a rule-based or probability-based password guessing tool can easily guess them.
    In my work, I want to demonstrate the capability of deep learning-based password
    guessing tools against other kinds, so the password policy restriction is considered.
    Second, I modified the activation function in the last layer of the Generator
    to Tanh instead of Softmax, and this made the model convergence faster.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了Rockyou数据集来训练[IWGAN](#glo.acronym.iwgan)模型，数据集经过预处理，以确保所有密码都是唯一的，测试集和训练集中不存在重复密码。基于密码长度的分布如表[3](#S3.T3
    "表 3 ‣ 3.4\. 实验设计 ‣ 3\. 方法论 ‣ 关于深度学习在密码猜测中的应用，综述")所示。我的实验与Hitaj等人([hitaj2019passgan,](#bib.bib10))的工作有两个小的区别。首先，在预处理数据集时，我将所有密码长度限制在8到12之间，这更符合现代密码创建政策。而Hitaj等人将所有密码限制在10以下。然而，在现实世界中，少于八个字符的密码被视为弱密码，即使是基于规则或概率的密码猜测工具也可以很容易地猜出这些密码。在我的工作中，我想展示基于深度学习的密码猜测工具与其他类型工具的能力，因此考虑了密码政策限制。其次，我将生成器最后一层的激活函数从Softmax修改为Tanh，这使得模型收敛更快。
- en: The training and testing set each have $8.44*10^{6}$ and $2.11*10^{6}$ respectively.
    The model was set to train for 200000 iterations as stated in the origin PassGAN’s
    work. But after 120000 iterations, the loss of both discriminator and generator
    became plateau, and it took a Tesla P100 GPU 12 hours to train the dataset for
    75000 iterations. By generating fake passwords using the 120000 checkpoints, compared
    the generated password with the testing dataset, the matching accuracy is 3.99%
    when generating $9.3*10^{7}$ unique passwords; and when generating $10^{8}$ passwords.
    Compared with Hitaj et al’s work,
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集和测试集分别包含$8.44*10^{6}$和$2.11*10^{6}$个样本。模型设置为按原始PassGAN的工作进行200000次迭代训练。但在120000次迭代后，鉴别器和生成器的损失趋于平稳，使用Tesla
    P100 GPU训练75000次迭代需要12小时。通过使用120000次检查点生成假密码，将生成的密码与测试数据集进行比较，生成$9.3*10^{7}$个唯一密码时的匹配准确率为3.99%；生成$10^{8}$个密码时的准确率为。与Hitaj等人的工作相比，
- en: 5\. Discussion
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 讨论
- en: Password Guessing with Deep Learning is a relatively new area, with the first
    related paper published in 2016([melicher2016fast,](#bib.bib17) ) using [LSTM](#glo.acronym.lstm)
    and first [GANs](#glo.acronym.gan) related model published in 2019([hitaj2019passgan,](#bib.bib10)
    ). With continuous advancements in machine learning and deep learning methods,
    bringing these advancements and breakthroughs to the password guessing domain
    is an exciting research topic. Nonetheless, several issues persist. I will introduce
    the open problems in password guessing and the associated future research possibilities
    in this section.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 基于深度学习的密码猜测是一个相对较新的领域，相关的第一篇论文发布于2016年([melicher2016fast,](#bib.bib17))，使用了[LSTM](#glo.acronym.lstm)，而第一个[GANs](#glo.acronym.gan)相关模型发布于2019年([hitaj2019passgan,](#bib.bib10))。随着机器学习和深度学习方法的不断进步，将这些进步和突破带入密码猜测领域是一个令人兴奋的研究课题。尽管如此，仍存在一些问题。在本节中，我将介绍密码猜测中的开放问题及未来研究的可能性。
- en: 5.1\. Open problems
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 开放问题
- en: •
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Insufficiency of training resources. The majority of password guessing tools
    are trained and tested using leaked datasets shown in Table[2](#S2.T2 "Table 2
    ‣ 2.3.3\. Attention-based models ‣ 2.3\. Deep Learning-base models ‣ 2\. Background
    and Related Work ‣ On Deep Learning in Password Guessing, a Survey"). While it
    is straightforward to compare the performance of each model, the generalizability
    of the models cannot be guaranteed.
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 训练资源的不足。大多数密码猜测工具都是使用表[2](#S2.T2 "Table 2 ‣ 2.3.3\. Attention-based models ‣
    2.3\. Deep Learning-base models ‣ 2\. Background and Related Work ‣ On Deep Learning
    in Password Guessing, a Survey")中显示的泄露数据集进行训练和测试的。虽然比较每个模型的性能比较直接，但无法保证模型的泛化能力。
- en: •
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The rate of successfully guessing the passwords is still low even using a combination
    of three types of models (rule-based, probability-based and deep learning-based).
    On the one-site password guessing problem, the best guessing accuracy is 51.8%
    after $10^{10}$ guesses([pasquini2021improving,](#bib.bib22) ). For cross-cite
    guessing, the best performance is 23% after $10^{12}$ guesses([liu2018genpass,](#bib.bib16)
    ). There is much space for improvement in the password guessing problem in terms
    of success rate.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 即使使用三种模型（基于规则的、基于概率的和基于深度学习的）组合，成功猜测密码的概率仍然较低。在单站密码猜测问题上，最佳猜测准确率为51.8%，在$10^{10}$次猜测后([pasquini2021improving,](#bib.bib22))。对于跨站点猜测，最佳表现是在$10^{12}$次猜测后为23%([liu2018genpass,](#bib.bib16))。在密码猜测问题上，成功率还有很大的提升空间。
- en: •
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The distribution or pattern of passwords disclosed five years ago may change
    from the passwords used today, yet most suggested models train and test on the
    same dataset, which is impractical in the real world. However, if the training
    and testing sets have different distributions, the model may experience concept
    drift. Even Liu et al.([liu2018genpass,](#bib.bib16) )’s hybrid model can successfully
    guess 23% of passwords correctly after generating $10^{12}$ passwords. The model’s
    capacity to generalize effectively to a different dataset than the testing set
    is critical because assessing cross-site is more realistic and requires further
    study.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 五年前披露的密码分布或模式可能与今天使用的密码不同，但大多数建议的模型在相同的数据集上进行训练和测试，这在现实中是不切实际的。然而，如果训练集和测试集具有不同的分布，模型可能会经历概念漂移。即使刘等([liu2018genpass,](#bib.bib16))的混合模型在生成$10^{12}$个密码后也能成功猜测23%的密码。模型在不同于测试集的数据集上有效泛化的能力至关重要，因为评估跨站点更具现实性且需要进一步研究。
- en: •
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The majority of research in the literature made no distinction between various
    password policies. However, most websites demand a mix of numeric characters,
    special symbols, and special characters. Prior study([tan2020practical,](#bib.bib28)
    ) has shown that varying password regulations, such as minimum-length and minimum-class
    requirements, can have a considerable influence on both usability and security.
    As a result, it is desirable to test password guessing tools against a variety
    of password policies.
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 大多数文献中的研究未区分不同的密码策略。然而，大多数网站要求密码包含数字字符、特殊符号和特殊字符的混合。先前的研究([tan2020practical,](#bib.bib28)
    )表明，不同的密码规则，如最小长度和最小类别要求，可能对可用性和安全性产生显著影响。因此，测试密码猜测工具在各种密码策略下的表现是可取的。
- en: 5.2\. Future work
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 未来工作
- en: •
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Although the provided deep learning algorithms are appropriate for password
    guessing attacks; the suggested models are somewhat simplistic. With advancements
    in the natural language processing and the computer vision fields, [BERT](#glo.acronym.bert),
    variants of [GANs](#glo.acronym.gan), the self-attention mechanism([choromanski2020rethinking,](#bib.bib4)
    ), and autoencoders have all seen significant improvements, and these newly invented
    techniques and architectures can be used to improve password guessing performance.
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尽管提供的深度学习算法适用于密码猜测攻击，但建议的模型有些过于简单。随着自然语言处理和计算机视觉领域的进展，[BERT](#glo.acronym.bert)、[GANs](#glo.acronym.gan)
    的变体、自注意力机制([choromanski2020rethinking,](#bib.bib4) )以及自编码器都取得了显著的进步，这些新发明的技术和架构可以用来提高密码猜测的性能。
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: '[GANs](#glo.acronym.gan) can also be utilized in targeted assaults. Once the
    attackers get access to the users’ [PII](#glo.acronym.pii), they may condition
    [GANs](#glo.acronym.gan) ([mirza2014conditional,](#bib.bib18) ) on the precise
    terms associated with the users’ [PII](#glo.acronym.pii), causing the generator
    to pay more attention to the region of the search space containing these keywords.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[GANs](#glo.acronym.gan) 还可以用于有针对性的攻击。一旦攻击者获得用户的[PII](#glo.acronym.pii)，他们可能会根据用户[PII](#glo.acronym.pii)
    相关的特定术语来调整[GANs](#glo.acronym.gan) ([mirza2014conditional,](#bib.bib18) )，使生成器更关注包含这些关键词的搜索空间区域。'
- en: •
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'The estimations of Deep Learning models can be used to develop trustworthy
    [PSMs](#glo.acronym.psm) that can assist users in creating passwords that are
    resistant to guessing attacks. The challenges are: to develop an application capable
    of storing and querying the compressed neural network and an interactive [PSMs](#glo.acronym.psm)
    user interface that can prompt users to build more secure passwords; convert deep
    learning results to interpretable rank as password strength. One solution is to
    store [GANs](#glo.acronym.gan) generated samples by different attempts in dictionaries
    and search for which dictionary the user-created password belongs to. For example
    if the created password first exists in the dictionary with 10000 generated passwords,
    then the strength is assigned to 4, if it first appears in the generated $10^{9}$
    dictionary, then the strength is assigned to 9\. The higher number the more secure
    as it needs exponentially more computation to guess the password correctly.'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深度学习模型的估计可以用来开发可靠的[PSMs](#glo.acronym.psm)，帮助用户创建抵抗猜测攻击的密码。挑战在于：开发一个能够存储和查询压缩神经网络的应用程序，并创建一个交互式[PSMs](#glo.acronym.psm)用户界面，提示用户建立更安全的密码；将深度学习结果转换为可解释的密码强度等级。一种解决方案是将不同尝试生成的[GANs](#glo.acronym.gan)样本存储在字典中，并搜索用户创建的密码属于哪个字典。例如，如果创建的密码首次出现在包含10000个生成密码的字典中，则强度评为4；如果首次出现在包含$10^{9}$个生成密码的字典中，则强度评为9。数字越高，密码越安全，因为需要指数级更多的计算才能正确猜测密码。
- en: •
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: To overcome the paucity of training and testing resources, researchers might
    train and test models using custom or site-specific data. For example, passwords
    generated during user studies to evaluate different password usability and security
    experiments may be utilized in our purpose.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了克服训练和测试资源的不足，研究人员可以使用定制或特定站点的数据来训练和测试模型。例如，用户研究中生成的密码用于评估不同的密码可用性和安全性实验，可以用于我们的目的。
- en: •
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: When reviewing novel password guessing methods, it is necessary to consider
    password policies. Additionally, transfer learning may improve models’ performance
    when there is a shortage of training data for a particular policy. For example,
    we may train a pre-trained model using generic passwords, freeze the network’s
    bottom layers and retrain the model using only passwords that adhere to specific
    criteria. Melicher et al.([melicher2016fast,](#bib.bib17) ) employed transfer
    learning to train models for certain password restrictions. However, the choices
    are not exhaustive.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在审查新颖的密码猜测方法时，需要考虑密码策略。此外，当特定策略的训练数据不足时，迁移学习可能会提高模型的性能。例如，我们可以使用通用密码训练一个预训练模型，冻结网络的底层并仅使用符合特定标准的密码重新训练模型。Melicher等人([melicher2016fast,](#bib.bib17)
    )采用迁移学习来训练满足特定密码限制的模型。然而，选择并非穷尽。
- en: 6\. Conclusions
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 结论
- en: In this paper, I compared the commonly deployed rule-based, probability-based
    password guessing tools with deep learning-based approaches from a high level,
    and introduced the deep learning techniques used in the password guessing area,
    including [RNN](#glo.acronym.rnn), autoencoder, [GANs](#glo.acronym.gan) and the
    Attention mechanism. A research idea involving using a [GANs](#glo.acronym.gan)-based
    model on password guessing attacks is proposed, along with the experimental methodology
    and resources required.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我从高层次上比较了常见的基于规则和基于概率的密码猜测工具与基于深度学习的方法，并介绍了在密码猜测领域使用的深度学习技术，包括[RNN](#glo.acronym.rnn)、自编码器、[GANs](#glo.acronym.gan)和注意力机制。提出了一个涉及使用基于[GANs](#glo.acronym.gan)的模型进行密码猜测攻击的研究思路，以及所需的实验方法和资源。
- en: Deep learning has made tremendous strides in recent years and will continue
    to do so in the near future. Deep learning application scenarios abound in our
    daily lives. Password guessing is a novel and exciting setting that will undoubtedly
    benefit from deep learning’s growth. I hope that with the attention of researchers,
    the issues raised in this work will find promising solutions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习在近年来取得了巨大的进步，并将在不久的将来继续如此。深度学习的应用场景在我们的日常生活中随处可见。密码猜测是一个新颖而激动人心的领域，毫无疑问将从深度学习的增长中受益。我希望通过研究人员的关注，这项工作中提出的问题能够找到有希望的解决方案。
- en: 7\. Appendix
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 附录
- en: Please see Figure[4](#S7.F4 "Figure 4 ‣ 7\. Appendix ‣ On Deep Learning in Password
    Guessing, a Survey") for the mind map for the password guessing attacks.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见图[4](#S7.F4 "Figure 4 ‣ 7\. Appendix ‣ On Deep Learning in Password Guessing,
    a Survey")，查看关于密码猜测攻击的思维导图。
- en: <svg   height="689.7" overflow="visible" version="1.1" width="644.71"><g transform="translate(0,689.7)
    matrix(1 0 0 -1 0 0) translate(320.62,0) translate(0,302.79)" fill="#000000" stroke="#000000"
    stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -68.9 4.38)" fill="#000000"
    stroke="#000000"><foreignobject width="137.8" height="28.14" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Password Guessing Attacks <g fill="#FFCCCC"
    stroke="#FFCCCC"><path d="M -44.29 -153.43 C -44.29 -128.97 -64.12 -109.14 -88.58
    -109.14 C -113.04 -109.14 -132.87 -128.97 -132.87 -153.43 C -132.87 -177.89 -113.04
    -197.72 -88.58 -197.72 C -64.12 -197.72 -44.29 -177.89 -44.29 -153.43 Z M -88.58
    -153.43"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -127.95 -157.65)" fill="#000000"
    stroke="#000000"><foreignobject width="78.74" height="10.93" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Types</foreignobject></g> <g fill="#FFCCCC"><path
    d="M -39.37 -68.19 M -13.67 -77.54 C -31.9 -74.33 -48.42 -64.79 -60.32 -50.61
    C -47.67 -65.69 -52.57 -83.3 -62.41 -100.35 L -55.7 -104.22 C -45.86 -87.18 -33.06
    -74.13 -13.67 -77.54 Z M -52.01 -97.83 L -55.7 -104.22 L -62.41 -100.35 L -58.72
    -93.95 Z M -62.41 -100.35 M -54.65 -124.96 C -61.35 -116.98 -70.64 -111.62 -80.89
    -109.81 C -69.99 -111.73 -64.26 -103.54 -58.72 -93.95 L -52.01 -97.83 C -57.54
    -107.42 -61.77 -116.48 -54.65 -124.96 Z" style="stroke:none"></path></g><g fill="#FFE6E6"
    stroke="#FFE6E6"><path d="M -168.22 -184 C -168.22 -164.97 -183.64 -149.55 -202.67
    -149.55 C -221.69 -149.55 -237.12 -164.97 -237.12 -184 C -237.12 -203.03 -221.69
    -218.45 -202.67 -218.45 C -183.64 -218.45 -168.22 -203.03 -168.22 -184 Z M -202.67
    -184"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -232.19 -181.54)" fill="#000000"
    stroke="#000000"><foreignobject width="59.06" height="24.29" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Online and Offline</foreignobject></g> <g fill="#FFE6E6"><path
    d="M -131.36 -164.89 M -124.86 -178.83 C -130.84 -170.3 -133.61 -159.94 -132.71
    -149.57 C -133.67 -160.6 -142.84 -164.85 -153.54 -167.71 L -151.98 -173.54 C -141.28
    -170.67 -131.22 -169.76 -124.86 -178.83 Z M -151.97 -173.54 L -151.98 -173.54
    L -153.54 -167.71 L -153.53 -167.71 Z M -153.54 -167.71 M -168.35 -187 C -167.64
    -178.93 -169.8 -170.87 -174.45 -164.24 C -169.51 -171.29 -161.85 -169.94 -153.53
    -167.71 L -151.97 -173.54 C -160.29 -175.77 -167.6 -178.42 -168.35 -187 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -109.78 -255.72 C -109.78 -234.81 -126.73
    -217.85 -147.64 -217.85 C -168.55 -217.85 -185.5 -234.81 -185.5 -255.72 C -185.5
    -276.63 -168.55 -293.58 -147.64 -293.58 C -126.73 -293.58 -109.78 -276.63 -109.78
    -255.72 Z M -147.64 -255.72"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -177.17
    -244.95)" fill="#000000" stroke="#000000"><foreignobject width="59.06" height="40.9"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Targeted and Non-Targeted</foreignobject></g>
    <g fill="#FFE6E6"><path d="M -110.73 -191.79 M -96.27 -197.05 C -106.53 -195.24
    -115.82 -189.88 -122.51 -181.9 C -115.39 -190.38 -119.13 -199.72 -124.67 -209.31
    L -118.93 -212.62 C -113.4 -203.03 -107.18 -195.13 -96.27 -197.05 Z M -116.37
    -208.19 L -118.93 -212.62 L -124.67 -209.31 L -122.11 -204.87 Z M -124.67 -209.31
    M -118.63 -231.38 C -124.35 -224.56 -132.3 -219.97 -141.06 -218.43 C -131.74 -220.07
    -126.84 -213.07 -122.11 -204.87 L -116.37 -208.19 C -121.1 -216.39 -124.72 -224.13
    -118.63 -231.38 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -23.56 -267.51 C -23.56 -248.49 -38.99 -233.07 -58.01 -233.07 C -77.04 -233.07
    -92.46 -248.49 -92.46 -267.51 C -92.46 -286.54 -77.04 -301.96 -58.01 -301.96 C
    -38.99 -301.96 -23.56 -286.54 -23.56 -267.51 Z M -58.01 -267.51"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -87.54 -264.13)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="26.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Static
    and Dynamic</foreignobject></g> <g fill="#FFE6E6"><path d="M -77.12 -196.21 M
    -63.18 -189.71 C -71.71 -195.68 -82.07 -198.46 -92.44 -197.55 C -81.41 -198.52
    -77.16 -207.69 -74.3 -218.38 L -68.48 -216.82 C -71.34 -206.13 -72.25 -196.06
    -63.18 -189.71 Z M -68.48 -216.82 L -68.48 -216.82 L -74.3 -218.38 L -74.3 -218.38
    Z M -74.3 -218.38 M -55.01 -233.2 C -63.08 -232.49 -71.14 -234.65 -77.77 -239.29
    C -70.72 -234.36 -72.07 -226.7 -74.3 -218.38 L -68.48 -216.82 C -66.25 -225.14
    -63.59 -232.45 -55.01 -233.2 Z" style="stroke:none"></path></g><g fill="#FFCCCC"
    stroke="#FFCCCC"><path d="M 221.46 0 C 221.46 24.46 201.63 44.29 177.17 44.29
    C 152.7 44.29 132.87 24.46 132.87 0 C 132.87 -24.46 152.7 -44.29 177.17 -44.29
    C 201.63 -44.29 221.46 -24.46 221.46 0 Z M 177.17 0"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 137.8 -4.84)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Intention</foreignobject></g>
    <g fill="#FFCCCC"><path d="M 78.74 0 M 73.99 26.93 C 80.32 9.54 80.32 -9.54 73.99
    -26.93 C 80.72 -8.43 98.43 -3.88 118.11 -3.88 L 118.11 3.88 C 98.43 3.88 80.72
    8.43 73.99 26.93 Z M 118.11 -3.88 h -7.38 v 7.75 h 7.38 Z M 135.54 15.15 C 131.98
    5.36 131.98 -5.36 135.54 -15.15 C 131.76 -4.74 121.8 -3.88 110.73 -3.88 L 110.73
    3.88 C 121.8 3.88 131.76 4.74 135.54 15.15 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M 323.26 -45.2 C 323.26 -24.78 306.71
    -8.22 286.28 -8.22 C 265.86 -8.22 249.31 -24.78 249.31 -45.2 C 249.31 -65.62 265.86
    -82.18 286.28 -82.18 C 306.71 -82.18 323.26 -65.62 323.26 -45.2 Z M 286.28 -45.2"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 256.76 -35.9)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="37.97" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Password
    Strength Meters</foreignobject></g> <g fill="#FFE6E6"><path d="M 218.08 -16.95
    M 221.41 -1.93 C 220.96 -12.33 216.86 -22.25 209.82 -29.92 C 217.3 -21.76 227.08
    -24.18 237.31 -28.41 L 239.78 -22.43 C 229.55 -18.2 220.93 -12.99 221.41 -1.93
    Z M 236.28 -20.98 L 239.78 -22.43 L 237.31 -28.41 L 233.8 -26.96 Z M 237.31 -28.41
    M 259.02 -20.22 C 253.15 -26.63 249.72 -34.9 249.34 -43.58 C 249.74 -34.35 242.34
    -30.5 233.8 -26.96 L 236.28 -20.98 C 244.82 -24.52 252.77 -27.03 259.02 -20.22
    Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path d="M
    320.73 45.2 C 320.73 64.22 305.31 79.65 286.28 79.65 C 267.26 79.65 251.84 64.22
    251.84 45.2 C 251.84 26.17 267.26 10.75 286.28 10.75 C 305.31 10.75 320.73 26.17
    320.73 45.2 Z M 286.28 45.2"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 256.76
    47.51)" fill="#000000" stroke="#000000"><foreignobject width="59.06" height="24"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Honeyword Generation</foreignobject></g>
    <g fill="#FFE6E6"><path d="M 218.08 16.95 M 209.82 29.92 C 216.86 22.25 220.96
    12.33 221.41 1.93 C 220.93 12.99 229.47 18.4 239.7 22.64 L 237.39 28.21 C 227.16
    23.97 217.3 21.76 209.82 29.92 Z M 237.39 28.21 L 237.39 28.21 L 239.7 22.64 L
    239.7 22.64 Z M 239.7 22.64 M 251.87 43.7 C 252.22 35.6 255.41 27.9 260.88 21.92
    C 255.07 28.27 247.65 25.93 239.7 22.64 L 237.39 28.21 C 245.35 31.5 252.24 35.09
    251.87 43.7 Z" style="stroke:none"></path></g><g fill="#FFCCCC" stroke="#FFCCCC"><path
    d="M -44.29 153.43 C -44.29 177.89 -64.12 197.72 -88.58 197.72 C -113.04 197.72
    -132.87 177.89 -132.87 153.43 C -132.87 128.97 -113.04 109.14 -88.58 109.14 C
    -64.12 109.14 -44.29 128.97 -44.29 153.43 Z M -88.58 153.43"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -127.95 149.28)" fill="#000000" stroke="#000000"><foreignobject width="78.74"
    height="11.07" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Techniques</foreignobject></g>
    <g fill="#FFCCCC"><path d="M -39.37 68.19 M -60.32 50.61 C -48.42 64.79 -31.9
    74.33 -13.67 77.54 C -33.06 74.13 -45.86 87.18 -55.7 104.22 L -62.41 100.35 C
    -52.57 83.3 -47.67 65.69 -60.32 50.61 Z M -58.72 93.95 L -62.41 100.35 L -55.7
    104.22 L -52.01 97.83 Z M -55.7 104.22 M -80.89 109.81 C -70.64 111.62 -61.35
    116.98 -54.65 124.96 C -61.77 116.48 -57.54 107.42 -52.01 97.83 L -58.72 93.95
    C -64.26 103.54 -69.99 111.73 -80.89 109.81 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -23.56 267.51 C -23.56 286.54 -38.99
    301.96 -58.01 301.96 C -77.04 301.96 -92.46 286.54 -92.46 267.51 C -92.46 248.49
    -77.04 233.07 -58.01 233.07 C -38.99 233.07 -23.56 248.49 -23.56 267.51 Z M -58.01
    267.51"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -87.54 262.67)" fill="#000000"
    stroke="#000000"><foreignobject width="59.06" height="9.69" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Rule-based</foreignobject></g> <g fill="#FFE6E6"><path
    d="M -77.12 196.21 M -92.44 197.55 C -82.07 198.46 -71.71 195.68 -63.18 189.71
    C -72.25 196.06 -71.34 206.13 -68.48 216.82 L -74.3 218.38 C -77.16 207.69 -81.41
    198.52 -92.44 197.55 Z M -74.3 218.38 L -74.3 218.38 L -68.48 216.82 L -68.48
    216.82 Z M -68.48 216.82 M -77.77 239.29 C -71.14 234.65 -63.08 232.49 -55.01
    233.2 C -63.59 232.45 -66.25 225.14 -68.48 216.82 L -74.3 218.38 C -72.07 226.7
    -70.72 234.36 -77.77 239.29 Z" style="stroke:none"></path></g><g fill="#FFE6E6"
    stroke="#FFE6E6"><path d="M 11.87 349.34 C 11.87 361.84 1.73 371.98 -10.77 371.98
    C -23.27 371.98 -33.41 361.84 -33.41 349.34 C -33.41 336.84 -23.27 326.7 -10.77
    326.7 C 1.73 326.7 11.87 336.84 11.87 349.34 Z M -10.77 349.34"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -30.46 344.5)" fill="#000000" stroke="#000000"><foreignobject
    width="39.37" height="9.69" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">HashCat([hranicky2019distributed,](#bib.bib12)
    )</foreignobject></g> <g fill="#FFE6E6"><path d="M -40.79 297.35 M -52.03 301.44
    C -44.06 300.03 -36.83 295.86 -31.62 289.66 C -37.16 296.25 -34.77 303.82 -30.46
    311.27 L -33.89 313.25 C -38.2 305.8 -43.55 299.94 -52.03 301.44 Z M -29.46 320.93
    L -33.89 313.25 L -30.46 311.27 L -26.03 318.95 Z M -30.46 311.27 M -28.11 334.79
    C -24.69 330.71 -19.94 327.97 -14.7 327.05 C -20.27 328.03 -23.2 323.85 -26.03
    318.95 L -29.46 320.93 C -26.63 325.83 -24.47 330.46 -28.11 334.79 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -33.93 362 C -33.93 375.3 -44.71 386.08
    -58.01 386.08 C -71.31 386.08 -82.09 375.3 -82.09 362 C -82.09 348.7 -71.31 337.92
    -58.01 337.92 C -44.71 337.92 -33.93 348.7 -33.93 362 Z M -58.01 362"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -77.7 362.78)" fill="#000000" stroke="#000000"><foreignobject
    width="39.37" height="20.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">John
    the Ripper</foreignobject></g> <g fill="#FFE6E6"><path d="M -58.01 301.96 M -69.8
    299.89 C -62.18 302.66 -53.84 302.66 -46.23 299.89 C -54.32 302.83 -55.91 310.58
    -55.91 319.19 L -60.12 319.19 C -60.12 310.58 -61.7 302.83 -69.8 299.89 Z M -60.12
    325.88 L -60.12 319.19 L -55.91 319.19 L -55.91 325.88 Z M -55.91 319.19 M -66.25
    339.38 C -60.93 337.44 -55.1 337.44 -49.78 339.38 C -55.43 337.32 -55.91 331.9
    -55.91 325.88 L -60.12 325.88 C -60.12 331.9 -60.59 337.32 -66.25 339.38 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -113.19 255.72 C -113.19 274.74 -128.61
    290.17 -147.64 290.17 C -166.66 290.17 -182.09 274.74 -182.09 255.72 C -182.09
    236.69 -166.66 221.27 -147.64 221.27 C -128.61 221.27 -113.19 236.69 -113.19 255.72
    Z M -147.64 255.72"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -177.17 258.18)"
    fill="#000000" stroke="#000000"><foreignobject width="59.06" height="24.29" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Probability-based</foreignobject></g> <g fill="#FFE6E6"><path
    d="M -110.73 191.79 M -122.51 181.9 C -115.82 189.88 -106.53 195.24 -96.27 197.05
    C -107.18 195.13 -113.65 202.88 -119.19 212.47 L -124.41 209.46 C -118.88 199.87
    -115.39 190.38 -122.51 181.9 Z M -124.41 209.46 L -124.41 209.46 L -119.19 212.47
    L -119.19 212.47 Z M -119.19 212.47 M -141.65 221.79 C -133.68 223.2 -126.45 227.37
    -121.25 233.57 C -126.78 226.97 -123.5 219.93 -119.19 212.47 L -124.41 209.46
    C -128.72 216.92 -133.17 223.28 -141.65 221.79 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -149.46 346.99 C -149.46 359.49 -159.59
    369.62 -172.09 369.62 C -184.6 369.62 -194.73 359.49 -194.73 346.99 C -194.73
    334.48 -184.6 324.35 -172.09 324.35 C -159.59 324.35 -149.46 334.48 -149.46 346.99
    Z M -172.09 346.99"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -191.78 342.14)"
    fill="#000000" stroke="#000000"><foreignobject width="39.37" height="9.69" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">PCFG([weir2009password,](#bib.bib33) )</foreignobject></g>
    <g fill="#FFE6E6"><path d="M -156.55 288.99 M -167.4 283.94 C -160.76 288.58 -152.7
    290.74 -144.64 290.03 C -153.21 290.79 -156.87 297.82 -159.1 306.14 L -162.93
    305.12 C -160.7 296.8 -160.34 288.88 -167.4 283.94 Z M -165.22 313.67 L -162.93
    305.12 L -159.1 306.14 L -161.39 314.7 Z M -159.1 306.14 M -174.07 324.43 C -168.76
    323.97 -163.47 325.39 -159.11 328.44 C -163.74 325.19 -162.86 320.16 -161.39 314.7
    L -165.22 313.67 C -166.68 319.14 -168.43 323.94 -174.07 324.43 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -189.71 322.53 C -189.71 336.19 -200.79
    347.27 -214.45 347.27 C -228.11 347.27 -239.19 336.19 -239.19 322.53 C -239.19
    308.87 -228.11 297.79 -214.45 297.79 C -200.79 297.79 -189.71 308.87 -189.71 322.53
    Z M -214.45 322.53"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -234.13 324.61)"
    fill="#000000" stroke="#000000"><foreignobject width="39.37" height="23.52" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">Markov Models([narayanan2005fast,](#bib.bib20)
    )</foreignobject></g> <g fill="#FFE6E6"><path d="M -172 280.08 M -178.86 270.28
    C -175.44 277.62 -169.54 283.52 -162.2 286.94 C -170 283.3 -176.56 287.7 -182.65
    293.79 L -185.71 290.73 C -179.62 284.64 -175.22 278.08 -178.86 270.28 Z M -189.74
    294.76 L -185.71 290.73 L -182.65 293.79 L -186.68 297.82 Z M -182.65 293.79 M
    -203.99 300.11 C -198.72 302.56 -194.48 306.8 -192.03 312.07 C -194.64 306.47
    -191.05 302.19 -186.68 297.82 L -189.74 294.76 C -194.11 299.13 -198.39 302.72
    -203.99 300.11 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -168.22 184 C -168.22 203.03 -183.64 218.45 -202.67 218.45 C -221.69 218.45
    -237.12 203.03 -237.12 184 C -237.12 164.97 -221.69 149.55 -202.67 149.55 C -183.64
    149.55 -168.22 164.97 -168.22 184 Z M -202.67 184"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -232.19 187.54)" fill="#000000" stroke="#000000"><foreignobject width="59.06"
    height="26.44" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Deep Learning-based</foreignobject></g>
    <g fill="#FFE6E6"><path d="M -131.36 164.89 M -132.71 149.57 C -133.61 159.94
    -130.84 170.3 -124.86 178.83 C -131.22 169.76 -141.28 170.67 -151.98 173.54 L
    -153.54 167.71 C -142.84 164.85 -133.67 160.6 -132.71 149.57 Z M -153.53 167.71
    L -153.54 167.71 L -151.98 173.54 L -151.97 173.54 Z M -151.98 173.54 M -174.45
    164.24 C -169.8 170.87 -167.64 178.93 -168.35 187 C -167.6 178.42 -160.29 175.77
    -151.97 173.54 L -153.53 167.71 C -161.85 169.94 -169.51 171.29 -174.45 164.24
    Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path d="M
    -225.17 265.83 C -225.17 279.49 -236.25 290.57 -249.91 290.57 C -263.58 290.57
    -274.65 279.49 -274.65 265.83 C -274.65 252.16 -263.58 241.08 -249.91 241.08 C
    -236.25 241.08 -225.17 252.16 -225.17 265.83 Z M -249.91 265.83"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -269.6 267.9)" fill="#000000" stroke="#000000"><foreignobject
    width="39.37" height="23.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">RNN/LSTM
    ([melicher2016fast,](#bib.bib17) ; [zhang2018password,](#bib.bib37) ; [liu2018genpass,](#bib.bib16)
    )</foreignobject></g> <g fill="#FFE6E6"><path d="M -219.89 213.83 M -229.06 206.14
    C -223.85 212.35 -216.63 216.52 -208.65 217.92 C -217.13 216.43 -222.32 222.37
    -226.63 229.83 L -230.38 227.67 C -226.07 220.21 -223.52 212.74 -229.06 206.14
    Z M -233.23 232.6 L -230.38 227.67 L -226.63 229.83 L -229.48 234.77 Z M -226.63
    229.83 M -245.61 241.46 C -239.89 242.47 -234.7 245.47 -230.96 249.92 C -234.93
    245.19 -232.57 240.13 -229.48 234.77 L -233.23 232.6 C -236.32 237.96 -239.52
    242.54 -245.61 241.46 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -259.75 231.24 C -259.75 244.91 -270.83 255.98 -284.49 255.98 C -298.16 255.98
    -309.24 244.91 -309.24 231.24 C -309.24 217.58 -298.16 206.5 -284.49 206.5 C -270.83
    206.5 -259.75 217.58 -259.75 231.24 Z M -284.49 231.24"></path></g><g transform="matrix(1.0
    0.0 0.0 1.0 -304.18 233.32)" fill="#000000" stroke="#000000"><foreignobject width="39.37"
    height="23.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Autoencoder
    ([nam2020generating,](#bib.bib19) )</foreignobject></g> <g fill="#FFE6E6"><path
    d="M -232.5 201.22 M -236.59 189.98 C -235.19 197.96 -231.01 205.18 -224.81 210.39
    C -231.41 204.85 -238.88 207.4 -246.33 211.71 L -248.5 207.96 C -241.04 203.65
    -235.1 198.46 -236.59 189.98 Z M -253.44 210.81 L -248.5 207.96 L -246.33 211.71
    L -251.27 214.56 Z M -246.33 211.71 M -268.59 212.29 C -264.14 216.03 -261.14
    221.22 -260.13 226.95 C -261.2 220.86 -256.63 217.65 -251.27 214.56 L -253.44
    210.81 C -258.8 213.9 -263.85 216.27 -268.59 212.29 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -274.52 184 C -274.52 196.5 -284.65
    206.64 -297.15 206.64 C -309.66 206.64 -319.79 196.5 -319.79 184 C -319.79 171.5
    -309.66 161.36 -297.15 161.36 C -284.65 161.36 -274.52 171.5 -274.52 184 Z M -297.15
    184"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -316.84 179.16)" fill="#000000"
    stroke="#000000"><foreignobject width="39.37" height="9.69" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">GANs ([hitaj2019passgan,](#bib.bib10) ; [pasquini2021improving,](#bib.bib22)
    )</foreignobject></g> <g fill="#FFE6E6"><path d="M -237.12 184 M -235.04 172.22
    C -237.81 179.83 -237.81 188.17 -235.04 195.78 C -237.98 187.69 -245.73 185.98
    -254.34 185.98 L -254.34 182.02 C -245.73 182.02 -237.98 180.31 -235.04 172.22
    Z M -263.2 182.02 L -254.34 182.02 L -254.34 185.98 L -263.2 185.98 Z M -254.34
    185.98 M -275.88 176.26 C -274.06 181.26 -274.06 186.74 -275.88 191.74 C -273.95
    186.42 -268.86 185.98 -263.2 185.98 L -263.2 182.02 C -268.86 182.02 -273.95 181.58
    -275.88 176.26 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -259.75 136.76 C -259.75 150.42 -270.83 161.5 -284.49 161.5 C -298.16 161.5
    -309.23 150.42 -309.23 136.76 C -309.23 123.09 -298.16 112.01 -284.49 112.01 C
    -270.83 112.01 -259.75 123.09 -259.75 136.76 Z M -284.49 136.76"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -304.18 138.83)" fill="#000000" stroke="#000000"><foreignobject
    width="39.37" height="23.52" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">Attention-Mechanism
    ([li2019password,](#bib.bib15) )</foreignobject></g><g fill="#FFE6E6"><path d="M
    -232.5 166.77 M -224.81 157.61 C -231.01 162.82 -235.19 170.04 -236.59 178.02
    C -235.1 169.54 -241.04 164.34 -248.5 160.04 L -246.33 156.29 C -238.88 160.59
    -231.41 163.15 -224.81 157.61 Z M -251.27 153.44 L -246.33 156.29 L -248.5 160.04
    L -253.44 157.19 Z M -248.5 160.04 M -260.13 141.05 C -261.14 146.78 -264.13 151.97
    -268.59 155.71 C -263.85 151.73 -258.79 154.09 -253.44 157.19 L -251.27 153.44
    C -256.63 150.34 -261.2 147.14 -260.13 141.05 Z" style="stroke:none"></path></g>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <svg   height="689.7" overflow="visible" version="1.1" width="644.71"><g transform="translate(0,689.7)
    matrix(1 0 0 -1 0 0) translate(320.62,0) translate(0,302.79)" fill="#000000" stroke="#000000"
    stroke-width="1.2pt"><g transform="matrix(1.0 0.0 0.0 1.0 -68.9 4.38)" fill="#000000"
    stroke="#000000"><foreignobject width="137.8" height="28.14" transform="matrix(1
    0 0 -1 0 16.6)" overflow="visible">密码猜测攻击 <g fill="#FFCCCC" stroke="#FFCCCC"><path
    d="M -44.29 -153.43 C -44.29 -128.97 -64.12 -109.14 -88.58 -109.14 C -113.04 -109.14
    -132.87 -128.97 -132.87 -153.43 C -132.87 -177.89 -113.04 -197.72 -88.58 -197.72
    C -64.12 -197.72 -44.29 -177.89 -44.29 -153.43 Z M -88.58 -153.43"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -127.95 -157.65)" fill="#000000" stroke="#000000"><foreignobject
    width="78.74" height="10.93" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">类型</foreignobject></g>
    <g fill="#FFCCCC"><path d="M -39.37 -68.19 M -13.67 -77.54 C -31.9 -74.33 -48.42
    -64.79 -60.32 -50.61 C -47.67 -65.69 -52.57 -83.3 -62.41 -100.35 L -55.7 -104.22
    C -45.86 -87.18 -33.06 -74.13 -13.67 -77.54 Z M -52.01 -97.83 L -55.7 -104.22
    L -62.41 -100.35 L -58.72 -93.95 Z M -62.41 -100.35 M -54.65 -124.96 C -61.35
    -116.98 -70.64 -111.62 -80.89 -109.81 C -69.99 -111.73 -64.26 -103.54 -58.72 -93.95
    L -52.01 -97.83 C -57.54 -107.42 -61.77 -116.48 -54.65 -124.96 Z" style="stroke:none"></path></g><g
    fill="#FFE6E6" stroke="#FFE6E6"><path d="M -168.22 -184 C -168.22 -164.97 -183.64
    -149.55 -202.67 -149.55 C -221.69 -149.55 -237.12 -164.97 -237.12 -184 C -237.12
    -203.03 -221.69 -218.45 -202.67 -218.45 C -183.64 -218.45 -168.22 -203.03 -168.22
    -184 Z M -202.67 -184"></path></g><g transform="matrix(1.0 0.0 0.0 1.0 -232.19
    -181.54)" fill="#000000" stroke="#000000"><foreignobject width="59.06" height="24.29"
    transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">在线和离线</foreignobject></g>
    <g fill="#FFE6E6"><path d="M -131.36 -164.89 M -124.86 -178.83 C -130.84 -170.3
    -133.61 -159.94 -132.71 -149.57 C -133.67 -160.6 -142.84 -164.85 -153.54 -167.71
    L -151.98 -173.54 C -141.28 -170.67 -131.22 -169.76 -124.86 -178.83 Z M -151.97
    -173.54 L -151.98 -173.54 L -153.54 -167.71 L -153.53 -167.71 Z M -153.54 -167.71
    M -168.35 -187 C -167.64 -178.93 -169.8 -170.87 -174.45 -164.24 C -169.51 -171.29
    -161.85 -169.94 -153.53 -167.71 L -151.97 -173.54 C -160.29 -175.77 -167.6 -178.42
    -168.35 -187 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -109.78 -255.72 C -109.78 -234.81 -126.73 -217.85 -147.64 -217.85 C -168.55
    -217.85 -185.5 -234.81 -185.5 -255.72 C -185.5 -276.63 -168.55 -293.58 -147.64
    -293.58 C -126.73 -293.58 -109.78 -276.63 -109.78 -255.72 Z M -147.64 -255.72"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -177.17 -244.95)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="40.9" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">有针对性和非针对性</foreignobject></g>
    <g fill="#FFE6E6"><path d="M -110.73 -191.79 M -96.27 -197.05 C -106.53 -195.24
    -115.82 -189.88 -122.51 -181.9 C -115.39 -190.38 -119.13 -199.72 -124.67 -209.31
    L -118.93 -212.62 C -113.4 -203.03 -107.18 -195.13 -96.27 -197.05 Z M -116.37
    -208.19 L -118.93 -212.62 L -124.67 -209.31 L -122.11 -204.87 Z M -124.67 -209.31
    M -118.63 -231.38 C -124.35 -224.56 -132.3 -219.97 -141.06 -218.43 C -131.74 -220.07
    -126.84 -213.07 -122.11 -204.87 L -116.37 -208.19 C -121.1 -216.39 -124.72 -224.13
    -118.63 -231.38 Z" style="stroke:none"></path></g><g fill="#FFE6E6" stroke="#FFE6E6"><path
    d="M -23.56 -267.51 C -23.56 -248.49 -38.99 -233.07 -58.01 -233.07 C -77.04 -233.07
    -92.46 -248.49 -92.46 -267.51 C -92.46 -286.54 -77.04 -301.96 -58.01 -301.96 C
    -38.99 -301.96 -23.56 -286.54 -23.56 -267.51 Z M -58.01 -267.51"></path></g><g
    transform="matrix(1.0 0.0 0.0 1.0 -87.54 -264.13)" fill="#000000" stroke="#000000"><foreignobject
    width="59.06" height="26.15" transform="matrix(1 0 0 -1 0 16.6)" overflow="visible">静态和动态</foreignobject></g>
    <g fill="#FFE6E6"><path d="M -77.12 -196.21 M -63.18 -189.71 C -71.71 -195.68
    -82.07 -198.46 -92.44 -197.55 C -81.41 -198.52 -77.16 -207.69 -74.3 -218.38 L
    -68.48 -216.82 C -71.34 -206.13 -72.25 -196.06 -63.18 -189.71 Z M -68.48 -216.82
    L -68.48 -216.82 L -74.3 -218.38 L -74.3 -218.38 Z M -74.3 -218.38 M -55.01 -233.2
    C -63.08 -232.49 -71.14 -234.65 -77.77 -239.29 C
- en: Figure 4\. A mind map for password guessing attacks
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图4\. 密码猜测攻击思维导图
- en: Glossary
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语表
- en: Acronyms
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩略词
- en: BERT
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: BERT
- en: Bidirectional Encoder Representations from Transformers
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 双向编码器表示从变换器
- en: BiLSTM
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: BiLSTM
- en: Bidirectional Long Short-term Memory
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 双向长短期记忆
- en: CNN
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: CNN
- en: Convolutional Neural Networks
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: DPG
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: DPG
- en: Dynamic Password Guessing
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 动态密码猜测
- en: FLA
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: FLA
- en: Fast, Lean, Accurate
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 快速、精简、准确
- en: GANs
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: GANs
- en: Generative Adversarial Networks
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: IWGAN
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: IWGAN
- en: Improved Training of Wasserstein GANs
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Wasserstein GAN 的改进训练
- en: LSTM
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM
- en: Long Short Term Memory Networks
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: PCFG
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: PCFG
- en: Probabilistic Context-Free Grammar
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 概率上下文无关文法
- en: PII
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: PII
- en: Personally Identifiable Information
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 个人可识别信息
- en: PSMs
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: PSMs
- en: Password Strength Meters
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 密码强度测量器
- en: RNN
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: RNN
- en: Recurrent Neural Networks
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: SPRNN
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: SPRNN
- en: Structure Partition and Bilstm Recurrent Neural Network
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 结构分区和 Bilstm 循环神经网络
- en: References
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: (1) Adams, A., and Sasse, M. A. Users are not the enemy. Communications of the
    ACM 42, 12 (1999), 40–46.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (1) Adams, A., 和 Sasse, M. A. 用户并不是敌人。《ACM 通讯》42, 12 (1999), 40–46。
- en: '(2) Bonneau, J., Herley, C., Van Oorschot, P. C., and Stajano, F. The quest
    to replace passwords: A framework for comparative evaluation of web authentication
    schemes. In 2012 IEEE Symposium on Security and Privacy (2012), IEEE, pp. 553–567.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (2) Bonneau, J., Herley, C., Van Oorschot, P. C., 和 Stajano, F. 替代密码的探索：一种比较评估网页认证方案的框架。见2012
    IEEE 安全与隐私研讨会 (2012), IEEE, 第553–567页。
- en: '(3) Bonneau, J., and Preibusch, S. The password thicket: Technical and market
    failures in human authentication on the web. In WEIS (2010), Citeseer.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (3) Bonneau, J., 和 Preibusch, S. 密码丛林：网络上人类身份验证中的技术和市场失效。见 WEIS (2010), Citeseer。
- en: (4) Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos,
    T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., et al. Rethinking attention
    with performers. In International Conference on Learning Representations (2020).
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (4) Choromanski, K. M., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos,
    T., Hawkins, P., Davis, J. Q., Mohiuddin, A., Kaiser, L., 等. 用表演者重新思考注意力。见学习表征国际会议
    (2020)。
- en: '(5) Dell’Amico, M., and Filippone, M. Monte carlo strength evaluation: Fast
    and reliable password checking. In Proceedings of the 22nd ACM SIGSAC Conference
    on Computer and Communications Security (2015), pp. 158–169.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (5) Dell’Amico, M., 和 Filippone, M. 蒙特卡罗强度评估：快速而可靠的密码检查。见第22届 ACM SIGSAC 计算机与通信安全会议论文集
    (2015), 第158–169页。
- en: '(6) Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training
    of deep bidirectional transformers for language understanding. arXiv preprint
    arXiv:1810.04805 (2018).'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (6) Devlin, J., Chang, M.-W., Lee, K., 和 Toutanova, K. BERT：深度双向变换器的预训练用于语言理解。arXiv
    预印本 arXiv:1810.04805 (2018)。
- en: (7) Florêncio, D., Herley, C., and Van Oorschot, P. C. An administrator’s guide
    to internet password research. In Proceedings of the 28th USENIX conference on
    Large Installation System Administration (2014), pp. 35–52.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (7) Florêncio, D., Herley, C., 和 Van Oorschot, P. C. 管理员的互联网密码研究指南。见第28届 USENIX
    大型安装系统管理会议论文集 (2014), 第35–52页。
- en: (8) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A., and Bengio, Y. Generative adversarial nets. Advances
    in neural information processing systems 27 (2014).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (8) Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A., 和 Bengio, Y. 生成对抗网络。神经信息处理系统进展 27 (2014)。
- en: (9) Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., and Courville, A.
    Improved training of wasserstein gans. arXiv preprint arXiv:1704.00028 (2017).
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (9) Gulrajani, I., Ahmed, F., Arjovsky, M., Dumoulin, V., 和 Courville, A. Wasserstein
    GAN 的改进训练。arXiv 预印本 arXiv:1704.00028 (2017)。
- en: '(10) Hitaj, B., Gasti, P., Ateniese, G., and Perez-Cruz, F. Passgan: A deep
    learning approach for password guessing. In International Conference on Applied
    Cryptography and Network Security (2019), Springer, pp. 217–237.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (10) Hitaj, B., Gasti, P., Ateniese, G., 和 Perez-Cruz, F. Passgan：一种深度学习密码猜测方法。见应用密码学和网络安全国际会议
    (2019), Springer, 第217–237页。
- en: (11) Hochreiter, S., and Schmidhuber, J. Long short-term memory. Neural computation
    9, 8 (1997), 1735–1780.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (11) Hochreiter, S., 和 Schmidhuber, J. 长短期记忆。神经计算 9, 8 (1997), 1735–1780。
- en: (12) Hranickỳ, R., Zobal, L., Ryšavỳ, O., and Kolář, D. Distributed password
    cracking with boinc and hashcat. Digital Investigation 30 (2019), 161–172.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (12) Hranickỳ, R., Zobal, L., Ryšavỳ, O., 和 Kolář, D. 使用 boinc 和 hashcat 进行分布式密码破解。《数字调查》30
    (2019), 161–172。
- en: '(13) Jabbar, A., Li, X., and Omar, B. A survey on generative adversarial networks:
    Variants, applications, and training. ACM Computing Surveys (CSUR) 54, 8 (2021),
    1–49.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (13) Jabbar, A., Li, X., 和 Omar, B. 生成对抗网络的综述：变体、应用和训练。《ACM 计算调查》（CSUR）54, 8
    (2021), 1–49。
- en: (14) Jiang, H., Chen, Z., Chen, M., Liu, F., Wang, D., and Zhao, T. On computation
    and generalization of generative adversarial networks under spectrum control.
    In International Conference on Learning Representations (2018).
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (14) Jiang, H., Chen, Z., Chen, M., Liu, F., Wang, D., 和 Zhao, T. 在谱控下生成对抗网络的计算与泛化。在国际学习表征会议（2018）。
- en: (15) Li, H., Chen, M., Yan, S., Jia, C., and Li, Z. Password guessing via neural
    language modeling. In International Conference on Machine Learning for Cyber Security
    (2019), Springer, pp. 78–93.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (15) Li, H., Chen, M., Yan, S., Jia, C., 和 Li, Z. 通过神经语言建模进行密码猜测。在网络安全机器学习国际会议（2019），Springer，第78–93页。
- en: '(16) Liu, Y., Xia, Z., Yi, P., Yao, Y., Xie, T., Wang, W., and Zhu, T. Genpass:
    A general deep learning model for password guessing with pcfg rules and adversarial
    generation. In 2018 IEEE International Conference on Communications (ICC) (2018),
    IEEE, pp. 1–6.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (16) Liu, Y., Xia, Z., Yi, P., Yao, Y., Xie, T., Wang, W., 和 Zhu, T. Genpass：一种通用的深度学习模型，用于基于PCFG规则和对抗生成的密码猜测。在2018年IEEE国际通信会议（ICC）（2018），IEEE，第1–6页。
- en: '(17) Melicher, W., Ur, B., Segreti, S. M., Komanduri, S., Bauer, L., Christin,
    N., and Cranor, L. F. Fast, lean, and accurate: modeling password guessability
    using neural networks. In Proceedings of the 25th USENIX Conference on Security
    Symposium (2016), pp. 175–191.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (17) Melicher, W., Ur, B., Segreti, S. M., Komanduri, S., Bauer, L., Christin,
    N., 和 Cranor, L. F. 快速、精简且准确：使用神经网络建模密码猜测性。在第25届USENIX安全研讨会会议录（2016），第175–191页。
- en: (18) Mirza, M., and Osindero, S. Conditional generative adversarial nets. arXiv
    preprint arXiv:1411.1784 (2014).
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (18) Mirza, M., 和 Osindero, S. 条件生成对抗网络。arXiv预印本 arXiv:1411.1784（2014）。
- en: (19) Nam, S., Jeon, S., and Moon, J. Generating optimized guessing candidates
    toward better password cracking from multi-dictionaries using relativistic gan.
    Applied Sciences 10, 20 (2020), 7306.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (19) Nam, S., Jeon, S., 和 Moon, J. 使用相对生成对抗网络从多个词典生成优化的猜测候选，以改进密码破解。《应用科学》10,
    20 (2020), 7306。
- en: (20) Narayanan, A., and Shmatikov, V. Fast dictionary attacks on passwords using
    time-space tradeoff. In Proceedings of the 12th ACM conference on Computer and
    communications security (2005), pp. 364–372.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (20) Narayanan, A., 和 Shmatikov, V. 使用时间-空间权衡的快速字典攻击。在第12届ACM计算机与通信安全大会会议录（2005），第364–372页。
- en: (21) Pascanu, R., Mikolov, T., and Bengio, Y. On the difficulty of training
    recurrent neural networks. In International conference on machine learning (2013),
    PMLR, pp. 1310–1318.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (21) Pascanu, R., Mikolov, T., 和 Bengio, Y. 循环神经网络训练的难度。在国际机器学习大会（2013），PMLR，第1310–1318页。
- en: (22) Pasquini, D., Gangwal, A., Ateniese, G., Bernaschi, M., and Conti, M. Improving
    password guessing via representation learning. In 2021 IEEE Symposium on Security
    and Privacy (SP) (2021), IEEE, pp. 1382–1399.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (22) Pasquini, D., Gangwal, A., Ateniese, G., Bernaschi, M., 和 Conti, M. 通过表示学习改进密码猜测。在2021年IEEE安全与隐私研讨会（SP）（2021），IEEE，第1382–1399页。
- en: '(23) Pearman, S., Thomas, J., Naeini, P. E., Habib, H., Bauer, L., Christin,
    N., Cranor, L. F., Egelman, S., and Forget, A. Let’s go in for a closer look:
    Observing passwords in their natural habitat. In Proceedings of the 2017 ACM SIGSAC
    Conference on Computer and Communications Security (2017), pp. 295–310.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (23) Pearman, S., Thomas, J., Naeini, P. E., Habib, H., Bauer, L., Christin,
    N., Cranor, L. F., Egelman, S., 和 Forget, A. 更近一步观察：在自然环境中观察密码。在第2017年ACM SIGSAC计算机与通信安全会议录（2017），第295–310页。
- en: (24) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,
    and Zettlemoyer, L. Deep contextualized word representations. In Proceedings of
    NAACL-HLT (2018), pp. 2227–2237.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (24) Peters, M. E., Neumann, M., Iyyer, M., Gardner, M., Clark, C., Lee, K.,
    和 Zettlemoyer, L. 深度上下文化词表示。在NAACL-HLT会议录（2018），第2227–2237页。
- en: (25) Pliam, J. O. On the incomparability of entropy and marginal guesswork in
    brute-force attacks. In International Conference on Cryptology in India (2000),
    Springer, pp. 67–79.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (25) Pliam, J. O. 关于熵和边际猜测在暴力攻击中的不可比性。在印度国际密码学会议（2000），Springer，第67–79页。
- en: (26) Radford, A., Narasimhan, K., Salimans, T., and Sutskever, I. Improving
    language understanding by generative pre-training.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (26) Radford, A., Narasimhan, K., Salimans, T., 和 Sutskever, I. 通过生成预训练提高语言理解能力。
- en: '(27) Stobert, E., and Biddle, R. The password life cycle: user behaviour in
    managing passwords. In 10th Symposium On Usable Privacy and Security ($\{$SOUPS$\}$
    2014) (2014), pp. 243–255.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (27) Stobert, E., 和 Biddle, R. 密码生命周期：用户管理密码的行为。在第10届可用隐私与安全研讨会（$\{$SOUPS$\}$
    2014）（2014），第243–255页。
- en: (28) Tan, J., Bauer, L., Christin, N., and Cranor, L. F. Practical recommendations
    for stronger, more usable passwords combining minimum-strength, minimum-length,
    and blocklist requirements. In Proceedings of the 2020 ACM SIGSAC Conference on
    Computer and Communications Security (2020), pp. 1407–1426.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (28) Tan, J., Bauer, L., Christin, N., 和 Cranor, L. F. 强而易用的密码的实用建议：结合最小强度、最小长度和黑名单要求。载于2020年ACM
    SIGSAC计算机与通信安全会议论文集（2020年），第1407–1426页。
- en: (29) Terjék, D. Adversarial lipschitz regularization. In International Conference
    on Learning Representations (2019).
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (29) Terjék, D. 对抗Lipschitz正则化。载于国际学习表征会议（2019年）。
- en: '(30) Thanh-Tung, H., Venkatesh, S., and Tran, T. Improving generalization and
    stability of generative adversarial networks. In ICLR 2019: Proceedings of the
    7th International Conference on Learning Representations (2019), ICLR.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (30) Thanh-Tung, H., Venkatesh, S., 和 Tran, T. 改进生成对抗网络的泛化能力和稳定性。载于ICLR 2019：第7届国际学习表征会议论文集（2019年），ICLR。
- en: (31) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, Ł., and Polosukhin, I. Attention is all you need. In Advances in
    neural information processing systems (2017), pp. 5998–6008.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (31) Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez,
    A. N., Kaiser, Ł., 和 Polosukhin, I. 注意力机制即你所需的一切。载于神经信息处理系统进展（2017年），第5998–6008页。
- en: '(32) Wei, X., Gong, B., Liu, Z., Lu, W., and Wang, L. Improving the improved
    training of wasserstein gans: A consistency term and its dual effect. In International
    Conference on Learning Representation (ICLR) (2018).'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (32) Wei, X., Gong, B., Liu, Z., Lu, W., 和 Wang, L. 改进Wasserstein生成对抗网络的训练：一致性项及其对偶效应。载于国际学习表征会议（ICLR）（2018年）。
- en: (33) Weir, M., Aggarwal, S., De Medeiros, B., and Glodek, B. Password cracking
    using probabilistic context-free grammars. In 2009 30th IEEE Symposium on Security
    and Privacy (2009), IEEE, pp. 391–405.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (33) Weir, M., Aggarwal, S., De Medeiros, B., 和 Glodek, B. 使用概率上下文无关文法进行密码破解。载于2009年IEEE第30届安全与隐私研讨会（2009年），IEEE，第391–405页。
- en: (34) Wu, J., Huang, Z., Thoma, J., Acharya, D., and Van Gool, L. Wasserstein
    divergence for gans. In Proceedings of the European Conference on Computer Vision
    (ECCV) (2018), pp. 653–668.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (34) Wu, J., Huang, Z., Thoma, J., Acharya, D., 和 Van Gool, L. 用于生成对抗网络的Wasserstein散度。载于欧洲计算机视觉会议（ECCV）论文集（2018年），第653–668页。
- en: (35) Wu, Y.-L., Shuai, H.-H., Tam, Z. R., and Chiu, H.-Y. Gradient normalization
    for generative adversarial networks. In Proceedings of the IEEE International
    Conference on Computer Vision (ICCV) (Oct 2021).
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (35) Wu, Y.-L., Shuai, H.-H., Tam, Z. R., 和 Chiu, H.-Y. 生成对抗网络的梯度归一化。载于IEEE国际计算机视觉会议论文集（ICCV）（2021年10月）。
- en: (36) Yosinski, J., Clune, J., Bengio, Y., and Lipson, H. How transferable are
    features in deep neural networks? Advances in Neural Information Processing Systems
    27 (2014), 3320–3328.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (36) Yosinski, J., Clune, J., Bengio, Y., 和 Lipson, H. 深度神经网络中的特征可转移性如何？神经信息处理系统进展27（2014年），第3320–3328页。
- en: (37) Zhang, M., Zhang, Q., Hu, X., and Liu, W. A password cracking method based
    on structure partition and bilstm recurrent neural network. In Proceedings of
    the 8th International Conference on Communication and Network Security (2018),
    pp. 79–83.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (37) Zhang, M., Zhang, Q., Hu, X., 和 Liu, W. 基于结构划分和双向LSTM递归神经网络的密码破解方法。载于第8届国际通信与网络安全会议论文集（2018年），第79–83页。
