- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 19:42:02'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2302.02173] A Survey on Deep Learning based Time Series Analysis with Frequency
    Transformation'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2302.02173](https://ar5iv.labs.arxiv.org/html/2302.02173)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: A Survey on Deep Learning based Time Series Analysis with Frequency Transformation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Kun Yi [yikun@bit.edu.cn](mailto:yikun@bit.edu.cn) Beijing Institute of TechnologyBeijingChina
    ,  Qi Zhang [zhangqi˙cs@tongji.edu.cn](mailto:zhangqi%CB%99cs@tongji.edu.cn) Tongji
    UniversityShanghaiChina ,  Longbing Cao [longbing.cao@mq.edu.au](mailto:longbing.cao@mq.edu.au)
    Macquarie UniversitySydneyAustralia ,  Shoujin Wang, Guodong Long [shoujin.wang,guodong.long@uts.edu.au](mailto:shoujin.wang,guodong.long@uts.edu.au)
    University of Technology SydneySydneyAustralia ,  Liang Hu [milkrain@gmail.com](mailto:milkrain@gmail.com)
    Tongji UniversityShanghaiChina ,  Hui He, Zhendong Niu [hehui617,zniu@bit.edu.cn](mailto:hehui617,zniu@bit.edu.cn)
    Beijing Institute of TechnologyBeijingChina ,  Wei Fan [weifan@knights.ucf.edu](mailto:weifan@knights.ucf.edu)
    University of Central FloridaOrlandoUSA  and  Hui Xiong [xionghui@ust.hk](mailto:xionghui@ust.hk)
    Hong Kong University of Science and TechnologyGuangzhouChina(2023)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Recently, frequency transformation (FT) has been increasingly incorporated into
    deep learning models to significantly enhance state-of-the-art accuracy and efficiency
    in time series analysis. The advantages of FT, such as high efficiency and a global
    view, have been rapidly explored and exploited in various time series tasks and
    applications, demonstrating the promising potential of FT as a new deep learning
    paradigm for time series analysis. Despite the growing attention and the proliferation
    of research in this emerging field, there is currently a lack of a systematic
    review and in-depth analysis of deep learning-based time series models with FT.
    It is also unclear why FT can enhance time series analysis and what its limitations
    in the field are. To address these gaps, we present a comprehensive review that
    systematically investigates and summarizes the recent research advancements in
    deep learning-based time series analysis with FT. Specifically, we explore the
    primary approaches used in current models that incorporate FT, the types of neural
    networks that leverage FT, and the representative FT-equipped models in deep time
    series analysis. We propose a novel taxonomy to categorize the existing methods
    in this field, providing a structured overview of the diverse approaches employed
    in incorporating FT into deep learning models for time series analysis. Finally,
    we highlight the advantages and limitations of FT for time series modeling and
    identify potential future research directions that can further contribute to the
    community of time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 'time series, neural networks, frequency transformation^†^†copyright: acmcopyright^†^†journalyear:
    2023^†^†doi: XXXXXXX.XXXXXXX^†^†journal: JACM^†^†journalvolume: 37^†^†journalnumber:
    4^†^†article: 111^†^†publicationmonth: 8^†^†ccs: General and reference Surveys
    and overviews^†^†ccs: Information systems Information systems application'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Time series data is amongst the most ubiquitous data types, and has penetrated
    nearly every corner of our daily life (Dama and Sinoquet, [2021](#bib.bib14)),
    e.g., user-item interaction series in e-commerce and stock price series over time
    in finance. In recent years, time series analysis has attracted rapidly increasing
    attention from academia and industry, particularly in areas such as time series
    forecasting (Benidis et al., [2022](#bib.bib6)), anomaly detection (Darban et al.,
    [2022](#bib.bib15)), and classification (Fawaz et al., [2019b](#bib.bib21)). Time
    series analysis has played a critical role in a wide variety of real-world applications
    to address significant challenges around us long-lastingly, such as traffic monitoring (Bai
    et al., [2020](#bib.bib4)), financial analysis (Feng et al., [2019](#bib.bib22)),
    and COVID-19 prediction (Chen et al., [2022c](#bib.bib9)). However, time series
    analysis is extremely challenging due to the intricate inter-series correlations
    and intra-series dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/116fe458e06e07b4a14c496cb7ca2561.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1\. Illustration of various working mechanisms applied to time series
    data. We take an example of four variables and $T$ timestamps, as shown in the
    left portion of the figure. (a) GNN constructs a graph connecting variables for
    each timestamp. (b) Self-attention builds temporal connections for each variable.
    (c) RNN creates a recursive cycle for capturing temporal transitions. (d) TCN
    consists of a stack of causal convolutional layers over timestamps.
  prefs: []
  type: TYPE_NORMAL
- en: Previous time series models based on deep learning have been devoted to modeling
    complex intra- and inter-series dependencies in the time domain to enhance downstream
    tasks. Representative sequential models such as recurrent neural networks (RNNs) (Lai
    et al., [2018](#bib.bib31); Hundman et al., [2018](#bib.bib27)), temporal convolutional
    networks (TCNs) (Bai et al., [2018](#bib.bib5)), and attention networks (Wu et al.,
    [2021](#bib.bib57)) are utilized to capture intra-series dependencies, while convolutional
    networks such as convolutional neural networks (CNNs) (Li et al., [2018](#bib.bib32))
    and graph neural networks (GNNs) (Chen et al., [2022a](#bib.bib10)) are preferred
    to attend to inter-series correlations. Although achieving good results, those
    networks have inherent drawbacks of time-domain modeling, limiting their capabilities
    in capturing critical patterns for time series analysis. For example, GNNs are
    constructed based on variable-wise connections as illustrated in Fig. [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ A Survey on Deep Learning based Time Series Analysis
    with Frequency Transformation")(a), and the sequential models (i.e., Transformer,
    RNN, and TCN) are based on timestamp-wise connections as shown in Fig. [1](#S1.F1
    "Figure 1 ‣ 1\. Introduction ‣ A Survey on Deep Learning based Time Series Analysis
    with Frequency Transformation")(b), (c), and (d), respectively. These modelings
    consider point-wise (e.g., variable/timestamp-wise) connections and fail to attend
    to whole or sub time series. Therefore, they are usually incapable of modeling
    common but complex global patterns, such as periodic patterns of seasonality,
    in time series (Yang et al., [2022](#bib.bib61); Woo et al., [2022a](#bib.bib54)).
    These inherent drawbacks inspire researchers to address the intricate inter-series
    correlations and intra-series dependencies of time series from a different perspective.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, deep learning methods leveraging frequency transformation (FT) (Roberts
    and Mullis, [1987](#bib.bib43)), e.g., Discrete Fourier Transform (DFT) (Winograd,
    [1976](#bib.bib53)), Discrete Cosine Transform (DCT) (Ahmed et al., [1974](#bib.bib2)),
    and Discrete Wavelet Transform (DWT) (Shensa et al., [1992](#bib.bib45)), have
    gained a surge of interest within the machine learning community (Xu et al., [2020](#bib.bib58);
    Chi et al., [2020](#bib.bib12); Guibas et al., [2022](#bib.bib25); Zhou et al.,
    [2022c](#bib.bib68)). These neural models incorporating frequency transformation
    have demonstrated an efficient learning paradigm in time series analysis and achieved
    state-of-the-art performance in terms of both efficiency and effectiveness (Wu
    et al., [2021](#bib.bib57); Zhou et al., [2022b](#bib.bib70); Zhang et al., [2022b](#bib.bib63)).
    This can be attributed to the distinctive advantages of FT (see Section [6.1](#S6.SS1
    "6.1\. Advantages ‣ 6\. Summary of Frequency Transformation ‣ A Survey on Deep
    Learning based Time Series Analysis with Frequency Transformation")) that the
    frequency spectrums generated by FT contain abundant vital patterns, e.g., seasonal
    trends, and provide a global view of the characteristics of time series. In addition,
    FT facilitates obtaining multi-scale representations and multi-frequency components
    of time series for capturing informative representations and patterns. This motivates
    us to systematically summarize and analyze the advantages of FT to instruct researchers
    in this area and to deliver a comprehensive survey on the emerging area, i.e.,
    deep learning based time series analysis with FT, thereby enlightening the time
    series community. While the literature includes various studies that discuss time
    series analysis from different perspectives (Fakhrazari and Vakilzadian, [2017](#bib.bib18);
    Benidis et al., [2022](#bib.bib6); Fawaz et al., [2019a](#bib.bib20); Chen et al.,
    [2021](#bib.bib11), [2022b](#bib.bib8); Schäfer et al., [2021](#bib.bib44)), there
    remains a lack of comprehensive summaries on the topic of time series analysis
    with FT. To the best of our knowledge, there is a notable absence of such a review
    covering the latest research progress of existing neural time series models based
    on FT. Moreover, the reasons why FT can enhance the time series analysis have
    not yet been summarized, and its limitations have not been thoroughly analyzed.
    These gaps have hindered the theoretical development and practical applications
    of time series analysis with FT.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this paper, we aim to fill the aforementioned gaps by reviewing existing
    deep learning methods for time series analysis with FT. Specifically, our primary
    objective is to provide answers to four crucial perspectives: i) the strategies
    employed by current neural time series models in incorporating neural networks
    with FT; ii) the specific types of neural networks utilized in conjunction with
    FT; iii) the representative FT-equipped neural models commonly employed in time
    series applications; and iv) an exploration of the reasons behind FT to enhance
    neural models as well as an analysis of its limitations in the context of time
    series analysis. By addressing these questions, we provide valuable insights into
    the realm of neural time series analysis with FT. To our knowledge, this paper
    is the first work to comprehensively and systematically review neural time series
    analysis with FT and to propose a new taxonomy for this emerging area, as depicted
    in Fig. [2](#S1.F2 "Figure 2 ‣ 1\. Introduction ‣ A Survey on Deep Learning based
    Time Series Analysis with Frequency Transformation").'
  prefs: []
  type: TYPE_NORMAL
- en: 'The subsequent sections of this paper are structured as follows: Section [2](#S2
    "2\. Preliminaries ‣ A Survey on Deep Learning based Time Series Analysis with
    Frequency Transformation") initially presents the fundamental concepts of time
    series analysis and frequency transformation. Following that, Section [3](#S3
    "3\. Incorporation Approach ‣ A Survey on Deep Learning based Time Series Analysis
    with Frequency Transformation") summarizes existing FT-equipped models in terms
    of approaches incorporating FT to enhance the accuracy or efficiency of time series
    analysis. In Section [4](#S4 "4\. Neural Network Design ‣ A Survey on Deep Learning
    based Time Series Analysis with Frequency Transformation"), we delve into the
    practical implementations of these models and examine the types of neural works
    utilized in conjunction with FT. Subsequently, Section [5](#S5 "5\. Applications
    ‣ A Survey on Deep Learning based Time Series Analysis with Frequency Transformation")
    categorizes representative frequency-based methods based on common time series
    tasks, including forecasting, anomaly detection, and classification. In Section
    [6](#S6 "6\. Summary of Frequency Transformation ‣ A Survey on Deep Learning based
    Time Series Analysis with Frequency Transformation"), we discuss the advantages
    and limitations of the frequency domain. Finally, we enlighten new avenues of
    future directions for time series analysis in Section [7](#S7 "7\. Discussion
    for Future Opportunities ‣ A Survey on Deep Learning based Time Series Analysis
    with Frequency Transformation").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/6a4150198eadec8cad2ab9a9aae7a5b1.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. A taxonomy of deep learning based time series analysis with frequency
    transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Preliminaries
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 2.1\. Time Series Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we provide a brief introduction to the three fundamental tasks
    of time series analysis before diving into neural time series analysis with Fourier
    Transform (FT).
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.1\. Forecasting
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Time series forecasting is the task of extrapolating time series into the future (Benidis
    et al., [2022](#bib.bib6)). For a given time series $\mathbf{X}=[{X}_{1},{X}_{2},\cdots,{X}_{T}]\in\mathbb{R}^{N\times
    T}$ with $N$ series and $T$ timestamps, where ${X}_{t}\in\mathbb{R}^{N}$ denotes
    the multi-variate values of $N$ distinct series at timestamp $t$. We consider
    a time series lookback window of length-$L$ at timestamp $t$, namely $\mathbf{X}_{t}=[{X}_{t-L+1},{X}_{t-L+2},\cdots,{X}_{t}]\in\mathbb{R}^{N\times
    L}$; also, we consider a horizon window of length-$\tau$ at timestamp $t$ as the
    prediction target, denoted as $\mathbf{Y}_{t}=[{X}_{t+1},{X}_{t+2},\cdots,{X}_{t+\tau}]\in\mathbb{R}^{N\times\tau}$.
    Then the time series forecasting task is to use historical observations $\mathbf{X}_{t}$
    to predict future values $\hat{\mathbf{Y}}_{t}$ and the typical forecasting model
    $f_{\theta}$ parameterized by $\theta$ is to produce forecasting results by $\hat{\mathbf{Y}}_{t}=f_{\theta}(\mathbf{X}_{t})$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.2\. Classification
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Time series classification seeks to assign labels to each series of a dataset (Fawaz
    et al., [2019a](#bib.bib20)). Generally, for a time series dataset $D=\{(X_{1},Y_{1}),(X_{2},Y_{2}),\cdots,(X_{N},Y_{N})\}$
    where $X_{i}\in\mathbb{R}^{T}$ is a time series with $T$ timestamps and $Y_{i}$
    is its corresponding one-hot vector label. For the dataset $D$ containing $K$
    classes, $Y_{i}$ is a vector of length $K$ where each element $j\in[1,K]$ is equal
    to 1 if the class of $X_{i}$ is $j$ and 0 otherwise. Then the classification task
    is to train a classifier $f_{\theta}$ parameterized by $\theta$ on the dataset
    $D$ to map from the space of possible inputs to a probability distribution over
    the class variable labels, formulated as $Y_{i}=f_{\theta}(X_{i})$.
  prefs: []
  type: TYPE_NORMAL
- en: 2.1.3\. Anomaly Detection
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Time series anomaly detection seeks to find abnormal subsequences in a series (Chen
    et al., [2021](#bib.bib11)). The goal is to develop algorithms or models that
    can effectively distinguish between normal and anomalous behavior, thereby providing
    early detection and alerting for unusual events or behaviors in the time series
    data. Given a time series $X=[x_{1},x_{2},\cdots,x_{T}]$ with $T$ timestamps where
    $x_{i}$ represent the data point at time index $i$, the anomaly detection is to
    identify a subset of data points $X_{s}\subseteq X$ that represents the anomalous
    or abnormal instances.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2\. Frequency Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we briefly introduce commonly used frequency transformations
    that convert time-domain data into the frequency domain, including Discrete Fourier
    Transform (DFT), Discrete Cosine Transform (DCT), and Discrete Wavelet Transform
    (DWT). Additionally, we describe the convolution theorem, which is a fundamental
    property in the frequency domain.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.1\. Discrete Fourier Transform
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Discrete Fourier Transform (DFT) (Winograd, [1976](#bib.bib53)) plays an important
    role in the area of digital signal processing. Given a sequence $x[n]$ with the
    length of N, DFT converts $x[n]$ into the frequency domain by:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\mathcal{X}[k]=\sum_{n=0}^{N-1}x[n]e^{-j(2\pi/N)kn},\ s.t.,\ k=0,1,...,N-1$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $j$ is the imaginary unit and $\mathcal{X}[k]$ represents the spectrum
    of $x[n]$ at the frequency $\omega_{k}=2\pi k/N$. The spectrum $\mathcal{X}\in\mathbb{C}^{k}$
    consists of real parts $\operatorname{Re}=\sum_{n=0}^{N-1}x[n]\cos{(2\pi/N)kn}\in\mathbb{R}^{k}$
    and imaginary parts $\operatorname{Im}=-\sum_{n=0}^{N-1}x[n]\sin{(2\pi/N)kn}\in\mathbb{R}^{k}$
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $\mathcal{X}=\operatorname{Re}+j\operatorname{Im}$ |  |'
  prefs: []
  type: TYPE_TB
- en: 'The amplitude part $A$ and phase part $\theta$ of $\mathcal{X}$ is defined
    as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (3) |  | $A=\sqrt{\operatorname{Re}^{2}+\operatorname{Im}^{2}}$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (4) |  | $\theta=\arctan(\frac{\operatorname{Im}}{\operatorname{Re}})$ |  |'
  prefs: []
  type: TYPE_TB
- en: 2.2.2\. Discrete Cosine Transform
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Discrete Cosine Transform (DCT) (Ahmed et al., [1974](#bib.bib2)) has emerged
    as the de-facto image transformation in most visual systems. The most common 1-D
    DCT $C(k)$ of a data sequence $x[n]$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (5) |  | $C(k)=\alpha(k)\sum_{n=0}^{N-1}x[n]\cos\left[\frac{\pi(2n+1)k}{2N}\right]$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $k=0,1,...,N-1$, and $\alpha(k)$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\alpha(k)=\left\{\begin{matrix}\sqrt{\frac{1}{N}},for\quad k=0\\
    \sqrt{\frac{2}{N}},for\quad k\neq 0\end{matrix}\right.$ |  |'
  prefs: []
  type: TYPE_TB
- en: DCT only retains the real parts of DFT and is roughly equivalent to DFT that
    has twice its length. It often performs on real data with even symmetry or in
    some variants where the input or output data are shifted by half a sample.
  prefs: []
  type: TYPE_NORMAL
- en: 2.2.3\. Discrete Wavelet Transform
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Discrete Wavelet Transform (DWT) (Shensa et al., [1992](#bib.bib45)) has been
    shown to be an appropriate tool for time-frequency analysis. It decomposes a given
    signal into a number of sets in which each set is a time series of coefficients
    describing the time evolution of the signal in the corresponding frequency band.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a signal $x(t)$, the wavelet transform $\operatorname{WT}$ can be expressed
    as $\operatorname{WT}(a,b)=\int_{-\infty}^{\infty}x(t)\Psi_{a,b}(t)\mathrm{d}t=\left\langle
    x(t),\Psi_{a,b}(t)\right\rangle$ where $\Psi$ is the wavelet basis function. The
    basis generation can be defined by $\Psi_{a,b}(t)=\frac{1}{\sqrt{a}}\Psi\left(\frac{t-b}{a}\right)$
    where $a$ and $b$ are the scaling and translation factors respectively. $\operatorname{DWT}$
    discretizes the scale factor a and the translation factor b as $a=a_{0}^{m},b=ka_{0}^{m}b_{0},m,k\in\mathbb{Z}$.
    Typically, $a_{0}$ is set to 2, and $b_{0}$ is set to 1\. Accordingly, the $\operatorname{DWT}$
    can be defined as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\operatorname{DWT}(a,b)=a_{0}^{-m/2}\int_{-\infty}^{\infty}x(t)\Psi(a_{0}^{-m}t-kb_{0})(t)\mathrm{d}t$
    |  |'
  prefs: []
  type: TYPE_TB
- en: In contrast to $\operatorname{DFT}$ and $\operatorname{DCT}$, a wavelet transform
    has the ability to identify the locations containing observed frequency content,
    while the DFT and DCT can only extract pure frequencies from the signal. Hence,
    $\operatorname{DWT}$ can perform time-frequency analysis. In addition, $\operatorname{DWT}$
    can obtain different resolution representations (Mallat, [1989](#bib.bib36)) by
    changing the scaling and translation factors. In Table [1](#S2.T1 "Table 1 ‣ 2.2.3\.
    Discrete Wavelet Transform ‣ 2.2\. Frequency Transformation ‣ 2\. Preliminaries
    ‣ A Survey on Deep Learning based Time Series Analysis with Frequency Transformation"),
    we compare the three frequency analysis methods, including their pros and cons
    of them.
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Comparison of DFT, DCT, and DWT for time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '| FT | Basis Function | Value Type | Time-Frequency | Pros | Cons |'
  prefs: []
  type: TYPE_TB
- en: '| DFT | Sine+Cosine | Complex | No | Shift-invariant | Leakage effect Lack
    of time localization |'
  prefs: []
  type: TYPE_TB
- en: '| DCT | Cosine | Real | No | Computationally efficient | No phase information
    Lack of time localization |'
  prefs: []
  type: TYPE_TB
- en: '| DWT | Wavelet | Real | Yes | Multi-resolution analysis Localization in time
    and frequency | Computation complexity |'
  prefs: []
  type: TYPE_TB
- en: 2.2.4\. Convolution Theorem
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The convolution theorem (Soliman and Srinath, [1990](#bib.bib46)) states the
    Fourier transform of a circular convolution of two signals equals the point-wise
    product of their Fourier transforms. Given a signal $x[n]$ and a filter $h[n]$,
    the convolution theorem can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\mathcal{F}(x[n]*h[n])=\mathcal{F}(x)\mathcal{F}(h)$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $x[n]*h[n]=\sum_{m=0}^{N-1}h[m]x[(n-m)_{N}]$, $(n-m)_{N}$ denotes $(n-m)$
    modulo N, and $\mathcal{F}(x)$ and $\mathcal{F}(h)$ denote discrete Fourier transform
    of $x[n]$ and $h[n]$, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: According to the convolution theorem, the point-wise product of frequency spectrums
    of two sequences is equivalent to their circular convolution in the time domain,
    where the product with a larger receptive field of the whole sequences better
    captures the overall characteristics (e.g., periodicity) and requires less computation
    cost (Alaa et al., [2021](#bib.bib3)).
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Incorporation Approach
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we present a systematic summary and discussion of the research
    categorization and progress regarding incorporating the frequency transformation
    to enhance time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Feature Engineering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Previous works employ frequency transformation (DFT, DCT, and DWT) as feature
    engineering tools to obtain frequency domain patterns. Basically, they utilize
    frequency transformation to capture three primary types of information: periodic
    patterns, multi-scale patterns, and global dependencies.'
  prefs: []
  type: TYPE_NORMAL
- en: Periodicity
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Compared to the time domain, the frequency domain can provide vital information
    for time series, such as periodic information. Prior models take advantage of
    frequency domain information for periodic analysis and use it as an important
    complement to the time domain information. (Yang et al., [2022](#bib.bib61)) proposes
    a frequency-domain block to capture dynamic and complicated periodic patterns
    of time series data, and integrates deep learning networks with frequency patterns.
    (Zhang et al., [2022b](#bib.bib63)) utilizes a frequency domain analysis branch
    to detect complex pattern anomalies, e.g., periodic anomalies. (Woo et al., [2022a](#bib.bib54))
    learns the trend representations in the time domain, whereas the seasonal representations
    are learned by a Fourier layer in the frequency domain. (Sun and Boning, [2022](#bib.bib47))
    is a frequency domain-based neural network model that is built on top of the baseline
    model to enhance its performance. (Woo et al., [2022b](#bib.bib55)) utilized DFT
    to design a frequency attention mechanism to replace the self-attention mechanism
    to identify seasonal patterns. (Woo et al., [2023](#bib.bib56)) leverages a novel
    concatenated Fourier features module to efficiently learn high-frequency patterns
    in time series.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-Scale
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: One big challenge for time series analysis is that there are intricate entangled
    temporal dynamics among time series data. To address this challenge, some methods
    try to solve it in terms of the frequency domain. They disentangle temporal patterns
    by decomposing time series data into different frequency components. (Hu and Qi,
    [2017](#bib.bib26)) separates the memory states of RNN into different frequency
    states such that they can explicitly learn the dependencies of both the low- and
    high-frequency patterns. (Zhang et al., [2017](#bib.bib64)) explicitly decomposes
    trading patterns into various frequency components and each component models a
    particular frequency of latent trading pattern underlying the fluctuation of stock
    price. Recently, wavelet-based models have shown competitive performances since
    wavelet transform can retain both time and frequency information and obtain multi-resolution
    representations. (Wang et al., [2018](#bib.bib49)) proposes a wavelet-based neural
    network structure for building frequency-aware deep learning models for time series
    analysis. (Wen et al., [2021a](#bib.bib51)) applies maximal overlap discrete wavelet
    transform to decouple time series into multiple levels of wavelet coefficients
    and then detect single periodicity at each level. (Wang et al., [2023](#bib.bib50))
    devises a novel data-dependent wavelet attention mechanism for dynamic frequency
    analysis of non-stationary time series analysis. (Yang et al., [2023](#bib.bib59))
    proposes an end-to-end graph enhanced Wavelet learning framework for long sequence
    forecasting which utilizes DWT to represent MTS in the wavelet domain.
  prefs: []
  type: TYPE_NORMAL
- en: Global Dependencies
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Existing time domain methods construct their models based on point-wise connections
    (see Fig. [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ A Survey on Deep Learning
    based Time Series Analysis with Frequency Transformation")), which prevent them
    from capturing series-level patterns, such as overall characteristics of time
    series. By leveraging the global view property of the frequency domain, some works
    utilize frequency information to attend to series-level patterns. (Zhou et al.,
    [2022b](#bib.bib70)) combines Fourier analysis with the Transformer which helps
    the Transformer better capture the global properties of time series. (Zhang et al.,
    [2022b](#bib.bib63)) integrates the frequency domain analysis branch with the
    time domain analysis branch and detects seasonality anomalies in the frequency
    domain. Besides, some works introduce frequency domain analysis to improve neural
    networks in order to address their inherent drawbacks. Vanilla convolutions in
    modern deep networks are known to operate locally, which causes low efficacy in
    connecting two distant locations in the network. To mitigate the locality limitation
    of convolutions, (Chi et al., [2019](#bib.bib13)) converts data into the frequency
    domain and proposes spectral residual learning for achieving a fully global receptive
    field, and (Chi et al., [2020](#bib.bib12)) harnesses the Fourier spectral theory
    and designs an operation unit to leverage frequency information for enlarging
    the receptive field of vanilla convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Compression
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Previous works utilize frequency transformation to obtain sparse representations
    and remove redundant information in the frequency domain. Moreover, since noise
    signals usually appear as high frequencies, it is easy to filter out them in the
    frequency domain. For example, in (Zhou et al., [2022a](#bib.bib69)), authors
    view time series forecasting from the sequence compression perspective and apply
    Fourier analysis to keep the part of the representation related to low-frequency
    Fourier components to remove the impact of noises. (Rippel et al., [2015](#bib.bib42))
    proposes spectral pooling that performs dimensionality reduction by truncating
    the representation in the frequency domain because energy is heavily concentrated
    in the lower frequencies. (Xu et al., [2020](#bib.bib58)) proposes a learning-based
    frequency selection method to identify the trivial frequency components while
    removing redundant information.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Data Augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Recently, a few studies investigate data augmentation from a frequency domain
    perspective for time series (Wen et al., [2021b](#bib.bib52)). Since the frequency
    domain contains some vital information for time series analysis, such as periodic
    patterns, existing methods incorporate frequency domain features with time domain
    features for data augmentations with the aim of enhancing time series representations.
    For example, CoST (Woo et al., [2022a](#bib.bib54)) incorporates a novel frequency
    domain contrastive loss which encourages discriminative seasonal representations
    and sidesteps the issue of determining the period of seasonal patterns present
    in the time series data. BTSF (Yang and Hong, [2022](#bib.bib60)) fuses the temporal
    and spectral features to enhance the discriminativity and expressiveness of the
    representations. TS-TFC (Liu et al., [2023](#bib.bib34)) proposes a temporal-frequency
    co-training model for time-series semi-supervised learning, utilizing the complementary
    information from two distinct views for unlabeled data learning.
  prefs: []
  type: TYPE_NORMAL
- en: More recently, different from CoST and BTSF that apply DFT after augmenting
    samples in the time domain, one new approach named TF-C (Zhang et al., [2022a](#bib.bib65))
    introduces frequency domain augmentations that directly perturb the frequency
    spectrum. It develops frequency-based contrastive augmentation to leverage rich
    spectral information and directly perturbs the frequency spectrum to leverage
    frequency-invariance for contrastive learning. Compared to performing data augmentations
    directly in the frequency domain (e.g., TF-C), applying the FFT after augmenting
    samples in the time domain (e.g., CoST and BTSF) may lead to information loss.
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Fourier Neural Operator Learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: According to the convolution theorem, differentiation is equivalent to multiplication
    in the Fourier domain (Li et al., [2021](#bib.bib33)). This efficiency property
    makes DFT frequently used to solve differential equations.
  prefs: []
  type: TYPE_NORMAL
- en: Recently, Fourier Neural Operators (FNOs) (Li et al., [2021](#bib.bib33)), which
    is currently the most promising one of the neural operators (Kovachki et al.,
    [2021](#bib.bib30)), have been proposed as an effective framework to solve partial
    differential equations (PDEs). More recently, FNO has been introduced in time
    series forecasting. (Zhou et al., [2022b](#bib.bib70)) proposes Fourier-enhanced
    blocks and Wavelet-enhanced blocks to capture important structures in time series
    through frequency domain mapping. (Yi et al., [2022](#bib.bib62)) reformulates
    the graph convolution operator in the frequency domain and efficiently computes
    graph convolutions over a supra-graph which represents non-static correlations
    between any two variables at any two timestamps.
  prefs: []
  type: TYPE_NORMAL
- en: 4\. Neural Network Design
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we delve deeper into existing related models that utilize specific
    types of neural networks to leverage frequency information. Considering that frequency
    transformation outputs can be either complex values or real values (as shown in
    Table [1](#S2.T1 "Table 1 ‣ 2.2.3\. Discrete Wavelet Transform ‣ 2.2\. Frequency
    Transformation ‣ 2\. Preliminaries ‣ A Survey on Deep Learning based Time Series
    Analysis with Frequency Transformation")), and each value type requires distinct
    handling methods, we discuss the models from the perspectives of these two value
    types.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Complex-Value Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The DFT output values are complex and can be represented in two ways. One representation
    is through the real and imaginary parts (as shown in Equation ([2](#S2.E2 "In
    2.2.1\. Discrete Fourier Transform ‣ 2.2\. Frequency Transformation ‣ 2\. Preliminaries
    ‣ A Survey on Deep Learning based Time Series Analysis with Frequency Transformation"))),
    while the other representation is through the amplitude and phase parts (as depicted
    in Equations ([3](#S2.E3 "In 2.2.1\. Discrete Fourier Transform ‣ 2.2\. Frequency
    Transformation ‣ 2\. Preliminaries ‣ A Survey on Deep Learning based Time Series
    Analysis with Frequency Transformation")) and ([4](#S2.E4 "In 2.2.1\. Discrete
    Fourier Transform ‣ 2.2\. Frequency Transformation ‣ 2\. Preliminaries ‣ A Survey
    on Deep Learning based Time Series Analysis with Frequency Transformation"))).
    While it is possible to simplify the calculation by retaining only one part, such
    as discarding the imaginary components (Godfrey and Gashler, [2018](#bib.bib24)),
    this approach may result in information loss.
  prefs: []
  type: TYPE_NORMAL
- en: In fact, there are mainly two approaches for performing neural networks on complex
    values. One approach is to treat each part of the complex value as a feature and
    then feed them to neural networks, respectively. Afterward, the output of corresponding
    networks is combined as a complex type (e.g., like Equation ([2](#S2.E2 "In 2.2.1\.
    Discrete Fourier Transform ‣ 2.2\. Frequency Transformation ‣ 2\. Preliminaries
    ‣ A Survey on Deep Learning based Time Series Analysis with Frequency Transformation"))),
    then the inverse DFT is executed and transmitted to the time domain. For example,
    StemGNN (Cao et al., [2020](#bib.bib7)) conducts GLU (Dauphin et al., [2017](#bib.bib16))
    on real and imaginary parts, respectively, which concatenates them as a complex
    value and applies IDFT. ATFN (Yang et al., [2022](#bib.bib61)) utilizes two linear
    layers to process the amplitude part and phase part, respectively, and then combine
    them as a whole. The other one is to conduct complex multiplication in the frequency
    domain directly. For example, FEDformer (Zhou et al., [2022b](#bib.bib70)) randomly
    samples a few frequencies and conducts complex multiplication with a parameterized
    kernel incorporated with attention architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. Real-Value Data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output value type of DCT and DWT is real, hence commonly used network structures
    can be directly applied to them, such as RNN and CNN. Besides, although the output
    value type of DFT is complex, some work discards one part, such as phase part (Zhang
    et al., [2017](#bib.bib64)), and thus their network design also belongs to a real
    value network. However, except for capturing frequency patterns, in contrast to
    other network designs, one main purpose of network design for frequency-based
    models is the frequency component selection to decide which component is discriminative
    or critical.
  prefs: []
  type: TYPE_NORMAL
- en: For example, (Xu et al., [2020](#bib.bib58)) converts the input to the frequency
    domain by DCT and groups the same frequency into one channel, and then proposes
    a learning-based dynamic channel selection method to identify the trivial frequency
    components. (Qin et al., [2021](#bib.bib39)) proposes to generalize global average
    pooling to more frequency components of DCT and designs three kinds of frequency
    components selection criteria. RobustPeriod (Wen et al., [2021a](#bib.bib51))
    applies DWT to decouple time series into multiple levels of wavelet coefficients
    and then proposes a method to robustly calculate unbiased wavelet variance at
    each level and rank periodic possibilities.
  prefs: []
  type: TYPE_NORMAL
- en: 5\. Applications
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we review the representative FT-equipped neural time series
    models. We categorize them into three main applications, including forecasting,
    anomaly detection, and classification. In Table [2](#S5.T2 "Table 2 ‣ 5\. Applications
    ‣ A Survey on Deep Learning based Time Series Analysis with Frequency Transformation"),
    we further compare them from six dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Table 2\. Summary of representative FT-equipped neural models in time series
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: '| Models | Frequency Transformation | Incorporation Approach | Value Type |
    Neural Network | Application Domains | Leveraged Advantages |'
  prefs: []
  type: TYPE_TB
- en: '| SFM (Zhang et al., [2017](#bib.bib64)) | DFT | Feature engineering | Real-value
    | RNN | Forecasting | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| StemGNN (Cao et al., [2020](#bib.bib7)) | DFT | Feature engineering | Complex-value
    | GLU | Forecasting | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| Autoformer (Wu et al., [2021](#bib.bib57)) | DFT | Feature engineering |
    Complex-value | Attention Network | Forecasting | Global view Efficiency |'
  prefs: []
  type: TYPE_TB
- en: '| AFTN (Yang et al., [2022](#bib.bib61)) | DFT | Feature engineering | Complex-value
    | MLP | Forecasting | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| DEPTS (Fan et al., [2022](#bib.bib19)) | DCT | Feature engineering | Real-value
    | MLP | Forecasting | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| FEDformer (Zhou et al., [2022b](#bib.bib70)) | DFT | Feature engineering
    Operator learning | Complex-value | Attention Network | Forecasting | Global view
    Efficiency |'
  prefs: []
  type: TYPE_TB
- en: '| CoST (Woo et al., [2022a](#bib.bib54)) | DFT | Data augmentation | Complex-value
    | MLP | Forecasting | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| FiLM (Zhou et al., [2022a](#bib.bib69)) | DFT | Compression | Complex-value
    | MLP | Forecasting | Sparse Representation |'
  prefs: []
  type: TYPE_TB
- en: '| EV-FGN (Yi et al., [2022](#bib.bib62)) | DFT | Operator learning | Complex-value
    | MLP | Forecasting | Efficiency |'
  prefs: []
  type: TYPE_TB
- en: '| FreDo (Sun and Boning, [2022](#bib.bib47)) | DFT | Feature engineering |
    Complex-value | MLP | Forecasting | Decompostion |'
  prefs: []
  type: TYPE_TB
- en: '| WAVEFORM (Yang et al., [2023](#bib.bib59)) | DWT | Feature engineering |
    Real-value | GCN | Forecasting | Decompostion |'
  prefs: []
  type: TYPE_TB
- en: '| SR-CNN (Ren et al., [2019](#bib.bib40)) | DFT | Feature engineering | Real-value
    | CNN | Anomaly Detection | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| RobustTAD (Gao et al., [2020](#bib.bib23)) | DFT | Data augmentation | Complex-value
    | CNN | Anomaly Detection | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| TFAD (Zhang et al., [2022b](#bib.bib63)) | DWT | Feature engineering | Real-value
    | TCN | Anomaly Detection | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| RCF (Wang et al., [2018](#bib.bib49)) | DWT | Feature engineering | Real-value
    | CNN | Classification | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| WD (Khan and Yener, [2018](#bib.bib29)) | DWT | Feature engineering | Real-value
    | CNN | Classification | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| BTSF (Yang and Hong, [2022](#bib.bib60)) | DFT | Data augmentation | Real-value
    | CNN | Classification Forecasting | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: '| TF-C (Zhang et al., [2022a](#bib.bib65)) | DFT | Data augmentation | Real-value
    | Transformer | Classification | Decomposition |'
  prefs: []
  type: TYPE_TB
- en: 5.1\. Time Series Forecasting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time series forecasting is essential in various domains, such as decision making
    and financial analysis. Recently, some methods leverage frequency information
    to improve the accuracy or efficiency of time series forecasting. SFM (Zhang et al.,
    [2017](#bib.bib64)) decomposes the hidden states of memory cells into multiple
    frequency components and models multi-frequency trading patterns. StemGNN (Cao
    et al., [2020](#bib.bib7)) learns spectral representations which are easier to
    recognize after DFT. Autoformer (Wu et al., [2021](#bib.bib57)) leverages FFT
    to calculate auto-correlation efficiently. DEPTS (Fan et al., [2022](#bib.bib19))
    conducts DCT to extract periodic features and then applies multi-layer perceptrons
    on these features for periodicity dependencies in time series. FEDformer (Zhou
    et al., [2022b](#bib.bib70)) captures the global view of time series in the frequency
    domain. CoST (Woo et al., [2022a](#bib.bib54)) learns the seasonal representations
    in the frequency domain. FiLM (Zhou et al., [2022a](#bib.bib69)) utilizes Fourier
    analysis to keep low-frequency Fourier components.
  prefs: []
  type: TYPE_NORMAL
- en: 5.2\. Time Series Anomaly Detection
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In recent years, frequency-based models have been introduced in anomaly detection.
    SR (Ren et al., [2019](#bib.bib40)) extracts the spectral residual in the frequency
    domain for detecting the anomaly. RobustTAD (Gao et al., [2020](#bib.bib23)) explores
    the data augmentation methods in the frequency domain to further increase labeled
    data. PFT (Park et al., [2021](#bib.bib38)) proposes a partial Fourier transform
    for anomaly detection with an order of magnitude of speedup without sacrificing
    accuracy. TFAD (Zhang et al., [2022b](#bib.bib63)) takes advantage of frequency
    domain analysis for seasonality anomaly.
  prefs: []
  type: TYPE_NORMAL
- en: 5.3\. Time Series Classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Time series classification is an important and challenging problem in time series
    analysis. Recently, a few models have considered frequency domain information
    to perform this task. RCF (Wang et al., [2018](#bib.bib49)) extracts distinguishing
    features from the DWT decomposed results. WD (Khan and Yener, [2018](#bib.bib29))
    uses wavelet functions with adjustable scale parameters to learn the spectral
    decomposition directly from the signal. BTSF (Yang and Hong, [2022](#bib.bib60))
    fuses time and spectral information to enhance the discriminativity and expressiveness
    of the representations. TF-C (Zhang et al., [2022a](#bib.bib65)) develops frequency-based
    contrastive augmentation to leverage rich spectral information and explore time-frequency
    consistency in time series.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Summary of Frequency Transformation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, to investigate why FT can enhance the neural models and what
    are its limitations for time series analysis, we summarize the advantages and
    limitations of frequency transformation.
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Advantages
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Decomposition
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Frequency transformation can decompose the original time series into different
    frequency components that embody vital information of time series, such as periodic
    patterns of seasonality. In particular, DWT can decompose a time series into a
    group of sub-series with frequencies ranked from high to low and obtains multi-scale
    representations. By decomposing time series in the time domain into different
    components in the frequency domain, it is naturally helpful to figure out and
    obtain beneficial information for time series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Global View
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: According to Equations ([1](#S2.E1 "In 2.2.1\. Discrete Fourier Transform ‣
    2.2\. Frequency Transformation ‣ 2\. Preliminaries ‣ A Survey on Deep Learning
    based Time Series Analysis with Frequency Transformation")), ([5](#S2.E5 "In 2.2.2\.
    Discrete Cosine Transform ‣ 2.2\. Frequency Transformation ‣ 2\. Preliminaries
    ‣ A Survey on Deep Learning based Time Series Analysis with Frequency Transformation")),
    and ([7](#S2.E7 "In 2.2.3\. Discrete Wavelet Transform ‣ 2.2\. Frequency Transformation
    ‣ 2\. Preliminaries ‣ A Survey on Deep Learning based Time Series Analysis with
    Frequency Transformation")), a frequency spectrum is calculated through the summation
    of all signals over time. Accordingly, each spectrum element in the frequency
    domain attends to all timestamps in the time domain, illustrating that a spectrum
    has a global view of the whole sequence of time series. In addition, according
    to the convolution theorem (see Equation ([8](#S2.E8 "In 2.2.4\. Convolution Theorem
    ‣ 2.2\. Frequency Transformation ‣ 2\. Preliminaries ‣ A Survey on Deep Learning
    based Time Series Analysis with Frequency Transformation"))), the point-wise product
    of frequency spectrums also captures the global characteristics of the whole sequence,
    inspiring to parameterize global learnable filters in the frequency domain.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse Representation
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Frequency transformation enables the provision of sparse representations for
    sequences. Taking DFT as an example, a substantial number of coefficients are
    close to zero, indicating that we can employ a reduced number of coefficients
    to represent the entire sequence. In other words, the corresponding representations
    in the frequency domain have a property of energy compaction. For example, the
    important features of signals captured by a subset of DWT coefficients are typically
    much smaller than the original. Specifically, using DWT, it ends up with the same
    number of coefficients as the original signal where many of the coefficients may
    be close to zero. As a result, we can effectively represent the original signal
    using only a small number of non-zero coefficients.
  prefs: []
  type: TYPE_NORMAL
- en: Efficiency
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: As mentioned earlier, frequency transformation often leads to sparse representations,
    where a substantial number of coefficients are close to zero. Exploiting this
    sparsity allows for efficient computations by discarding or compressing the negligible
    coefficients, resulting in reduced memory requirements and faster processing.
    Moreover, according to the convolution theorem, convolution in the time domain
    corresponds to Hadamard’s point-wise product in the frequency domain, which allows
    for convolution to be calculated more efficiently in the frequency domain. Therefore,
    considering the equivalence of the convolution theorem, convolution calculated
    in the frequency domain involves significantly fewer computational operations.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Limitations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Loss of temporal information
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Frequency transformation techniques, including DFT and DCT, primarily emphasize
    capturing the frequency characteristics of a time series. While these techniques
    offer valuable insights into the frequency domain, they may overlook or inadequately
    represent temporal information. Certain temporal patterns or dynamics inherent
    in the time series may not be fully captured in the frequency domain, thereby
    limiting the comprehensive analysis and understanding of the temporal aspects (Godfrey
    and Gashler, [2018](#bib.bib24)).
  prefs: []
  type: TYPE_NORMAL
- en: Dependence on pre-defined parameters
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
- en: Frequency transformation techniques often require setting parameters, such as
    window size, sampling rate, or frequency bands. Selecting appropriate parameter
    values can be challenging, and suboptimal choices may lead to inaccurate frequency
    representations or missed important frequency components (Khan and Yener, [2018](#bib.bib29);
    Michau et al., [2022](#bib.bib37)). Accordingly, parameter tuning and optimization
    are necessary to ensure the effectiveness of frequency transformation in time
    series analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 7\. Discussion for Future Opportunities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we explore the prospects for future research in neural time
    series analysis with frequency transformation. We begin by outlining the current
    limitations of frequency transformation and propose innovative directions to overcome
    these challenges. Subsequently, we delve into open research issues and emerging
    trends in the field of time series analysis that can be addressed through the
    utilization of frequency transformations.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1\. From the Perspective of Frequency Transformation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 7.1.1\. Leveraging New Orthogonal Transform Technology
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Recent studies have shown the efficiency and effectiveness of orthogonal transform
    which serves as a plug-in operation in neural networks, including frequency analysis
    and polynomial family. Some new orthogonal transform technologies have been introduced
    in neural networks and achieved good results. For example, FiLM (Zhou et al.,
    [2022a](#bib.bib69)) exploits the Legendre projection, which is one type of orthogonal
    polynomials, to update the representation of time series. (Park et al., [2021](#bib.bib38))
    proposes Partial Fourier Transform (PFT) to reduce complexity from $O(N\log{N})$
    to $O(N+M\log{M})$ where $M\ll N$. The Fractional Fourier transform (FrFT) has
    been proven to be desirable for noise removal and can enhance the discrimination
    between anomalies and background (Tao et al., [2019](#bib.bib48)). In (Zhao et al.,
    [2022a](#bib.bib66)), authors utilize FrFT to enhance efficient feature fusion
    and comprehensive feature extraction. (Zhao et al., [2022b](#bib.bib67)) leverages
    FrFT to enable flexible extraction of global contexts and sequential spectral
    information. In the future, it would be a promising direction to incorporate more
    new orthogonal transform technologies for deep learning in time series analysis,
    such as orthogonal polynomials, DCT, and FrFT.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.2\. Integrating Frequency Transformation with Deep Learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The basis functions used in frequency transformation, such as sine, cosine,
    and wavelet functions, are fixed across different domains. As a result, the frequency
    features extracted through these basis functions are domain-invariant. In other
    words, the features are insensitive to unexpected noise or to changing conditions.
  prefs: []
  type: TYPE_NORMAL
- en: To mitigate the limitation, few previous works combine frequency transformation
    with the learning ability of neural networks. mWDN (Wang et al., [2018](#bib.bib49))
    proposes a wavelet-based neural network structure, in which all parameters can
    be fine-tuned to fit training data of different learning tasks. (Khan and Yener,
    [2018](#bib.bib29)) proposes a method to efficiently optimize the parameters of
    the spectral decomposition based on the wavelet transform in a neural network
    framework. (Michau et al., [2022](#bib.bib37)) mimics the fast DWT cascade architecture
    utilizing the deep learning framework. These methods have shown promising performances,
    and in the future, the combination of frequency transformation with deep learning
    deserves further investigation.
  prefs: []
  type: TYPE_NORMAL
- en: 7.1.3\. Jointly Learning in the Time and Frequency Domain
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The frequency domain only uses periodic components, and thus cannot accurately
    model the non-periodic aspects of a signal, such as a linear trend (Godfrey and
    Gashler, [2018](#bib.bib24)). Moreover, according to the uncertainty principle (Zhang
    et al., [2022b](#bib.bib63)), designing a model with a single structure that can
    capture the time and frequency patterns simultaneously is difficult.
  prefs: []
  type: TYPE_NORMAL
- en: As a result, in the future, an interesting direction is to take advantage of
    corresponding characteristics of learning in the time and frequency domain to
    improve the accuracy and efficiency of time series analysis. Few works have tried
    to learn representations in the time and frequency domain, respectively. For example,
    CoST (Woo et al., [2022a](#bib.bib54)) learns the trend representations in the
    time domain and the seasonal representations in the frequency domain. However,
    it only performs data augmentations in the time domain and learns time and frequency
    representations separately. More time-frequency representation learning methods
    are required in the future.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2\. From the Perspective of Time Series Analysis
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 7.2.1\. Applying Frequency Transformation to Enhance Time Series Applications
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Applying frequency transformation techniques to a wider range of time series
    applications has the potential to unlock valuable insights and enhance decision-making
    in various domains. No matter in detecting anomalies in physiological signals,
    uncovering market cycles in financial data, or identifying patterns in environmental
    parameters, frequency transformation enables a deeper understanding of complex
    temporal patterns and trends. By harnessing the power of frequency analysis, researchers
    and practitioners can uncover hidden relationships, improve forecasting accuracy,
    optimize resource management, and advance knowledge in diverse fields, ultimately
    driving innovation and enabling data-driven decision-making in a wide range of
    time series applications.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.2\. Scalability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Scalability (Keogh and Kasetty, [2003](#bib.bib28)) is a key consideration in
    time series analysis. When coupled with frequency transformation techniques, it
    offers the potential for efficient and scalable analysis of large-scale time series
    data. Frequency transformation allows for the extraction of frequency components,
    reducing the dimensionality of the data and enabling more efficient processing.
    This reduction in dimensionality can significantly improve the scalability of
    time series analysis algorithms, as it reduces computational complexity and memory
    requirements. Scalable time series analysis with frequency transformation can
    pave the way for analyzing and extracting insights from big data time series applications
    in domains such as the Internet of Things (IoT), financial markets, or sensor
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.3\. Interpretability and Explainability
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Interpretability and explainability (Ribeiro et al., [2016](#bib.bib41); Lundberg
    and Lee, [2017](#bib.bib35)) are crucial aspects of time series analysis especially
    in practical applications. Intuitively, frequency transformation capably transforms
    time series into a more intuitive and interpretable representation in the frequency
    domain, offering valuable insights into the underlying patterns and behaviors.
    The frequency components obtained from the transformation can be analyzed to understand
    the dominant frequencies, periodicities, or significant events embodied in the
    data. This not only enhances the interpretability of the analysis but also enables
    the explanation of observed phenomena and anomalies in terms of frequency patterns.
    The interpretability and explainability of time series analysis with frequency
    transformation offer valuable advantages, enabling analysts and domain experts
    to attain deeper insights and establish trust in the analysis outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 7.2.4\. Privacy-preserving
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Leveraging frequency transformation offers a powerful approach to data privacy-preserving (Dwork
    et al., [2016](#bib.bib17)) in time series analysis. By applying frequency transformation,
    time series data can be transformed into frequency domain representations without
    revealing the underlying raw data. This transformation allows for the extraction
    of frequency components and patterns while maintaining the confidentiality of
    the original information. Privacy-preserving with frequency transformation techniques
    can ensure individual privacy and data confidentiality, and enable collaborative
    analysis, data sharing, and research collaborations while mitigating privacy risks.
    This approach is particularly valuable in domains where data sensitivity is critical,
    such as healthcare, finance, or personal monitoring, allowing for the utilization
    of frequency analysis while protecting the privacy of individuals or organizations
    involved.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we provide a comprehensive survey on deep learning based time
    series analysis with frequency transformation. We organize the reviewed methods
    from the perspectives of incorporation approaches, neural network design, and
    application domains, and we summarize the advantages and limitations of frequency
    transformation for time series analysis. To the best of our knowledge, this paper
    is the first work to comprehensively and systematically review neural time series
    analysis with frequency transformation, which would greatly benefit the time series
    community. Additionally, we offer a curated collection of sources, accessible
    at [https://github.com/BIT-Yi/time_series_frequency](https://github.com/BIT-Yi/time_series_frequency),
    to further assist the research community.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ahmed et al. (1974) Nasir Ahmed, T_ Natarajan, and Kamisetty R Rao. 1974. Discrete
    cosine transform. *IEEE transactions on Computers* 100, 1 (1974), 90–93.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Alaa et al. (2021) Ahmed M. Alaa, Alex James Chan, and Mihaela van der Schaar.
    2021. Generative Time-series Modeling with Fourier Flows. In *ICLR*. OpenReview.net.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2020) Lei Bai, Lina Yao, Can Li, Xianzhi Wang, and Can Wang. 2020.
    Adaptive Graph Convolutional Recurrent Network for Traffic Forecasting. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bai et al. (2018) Shaojie Bai, J. Zico Kolter, and Vladlen Koltun. 2018. An
    Empirical Evaluation of Generic Convolutional and Recurrent Networks for Sequence
    Modeling. *CoRR* abs/1803.01271 (2018).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Benidis et al. (2022) Konstantinos Benidis, Syama Sundar Rangapuram, Valentin
    Flunkert, Yuyang Wang, Danielle Maddix, Caner Turkmen, Jan Gasthaus, Michael Bohlke-Schneider,
    David Salinas, Lorenzo Stella, Franç ois-Xavier Aubet, Laurent Callot, and Tim
    Januschowski. 2022. Deep Learning for Time Series Forecasting: Tutorial and Literature
    Survey. *Comput. Surveys* 55 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cao et al. (2020) Defu Cao, Yujing Wang, Juanyong Duan, Ce Zhang, Xia Zhu, Congrui
    Huang, Yunhai Tong, Bixiong Xu, Jing Bai, Jie Tong, and Qi Zhang. 2020. Spectral
    Temporal Graph Neural Network for Multivariate Time-series Forecasting. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022b) Irene Y. Chen, Rahul G. Krishnan, and David A. Sontag. 2022b.
    Clustering Interval-Censored Time-Series for Disease Phenotyping. In *AAAI*. AAAI
    Press, 6211–6221.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2022c) Yuzhou Chen, Ignacio Segovia-Dominguez, Baris Coskunuzer,
    and Yulia Gel. 2022c. TAMP-S2GCNets: Coupling Time-Aware Multipersistence Knowledge
    Representation with Spatio-Supra Graph Convolutional Networks for Time-Series
    Forecasting. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chen et al. (2022a) Zekai Chen, Dingshuo Chen, Xiao Zhang, Zixuan Yuan, and
    Xiuzhen Cheng. 2022a. Learning Graph Structures With Transformer for Multivariate
    Time-Series Anomaly Detection in IoT. *IEEE Internet Things J.* 9, 12 (2022),
    9179–9189.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. (2021) Zhipeng Chen, Zhang Peng, Xueqiang Zou, and Haoqi Sun. 2021.
    Deep Learning Based Anomaly Detection for Muti-dimensional Time Series: A Survey.
    In *CNCERT* *(Communications in Computer and Information Science, Vol. 1506)*.
    Springer, 71–92.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chi et al. (2020) Lu Chi, Borui Jiang, and Yadong Mu. 2020. Fast Fourier Convolution.
    In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chi et al. (2019) Lu Chi, Guiyu Tian, Yadong Mu, Lingxi Xie, and Qi Tian. 2019.
    Fast Non-Local Neural Networks with Spectral Residual Learning. In *ACM Multimedia*.
    ACM, 2142–2151.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Dama and Sinoquet (2021) Fatoumata Dama and Christine Sinoquet. 2021. Analysis
    and modeling to forecast in time series: a systematic review. *CoRR* abs/2104.00164
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Darban et al. (2022) Zahra Zamanzadeh Darban, Geoffrey I. Webb, Shirui Pan,
    Charu C. Aggarwal, and Mahsa Salehi. 2022. Deep Learning for Time Series Anomaly
    Detection: A Survey. *CoRR* abs/2211.05244 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dauphin et al. (2017) Yann N. Dauphin, Angela Fan, Michael Auli, and David Grangier.
    2017. Language Modeling with Gated Convolutional Networks. In *ICML* *(Proceedings
    of Machine Learning Research, Vol. 70)*. PMLR, 933–941.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dwork et al. (2016) Cynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam D.
    Smith. 2016. Calibrating Noise to Sensitivity in Private Data Analysis. *J. Priv.
    Confidentiality* 7, 3 (2016), 17–51.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fakhrazari and Vakilzadian (2017) Amin Fakhrazari and Hamid Vakilzadian. 2017.
    A survey on time series data mining. In *EIT*. IEEE, 476–481.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fan et al. (2022) Wei Fan, Shun Zheng, Xiaohan Yi, Wei Cao, Yanjie Fu, Jiang
    Bian, and Tie-Yan Liu. 2022. DEPTS: Deep Expansion Learning for Periodic Time
    Series Forecasting. In *ICLR*. OpenReview.net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fawaz et al. (2019a) Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber,
    Lhassane Idoumghar, and Pierre-Alain Muller. 2019a. Deep learning for time series
    classification: a review. *Data Min. Knowl. Discov.* 33, 4 (2019), 917–963.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fawaz et al. (2019b) Hassan Ismail Fawaz, Germain Forestier, Jonathan Weber,
    Lhassane Idoumghar, and Pierre-Alain Muller. 2019b. Deep learning for time series
    classification: a review. *Data Mining and Knowledge Discovery* 33 (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feng et al. (2019) Fuli Feng, Xiangnan He, Xiang Wang, Cheng Luo, Yiqun Liu,
    and Tat-Seng Chua. 2019. Temporal Relational Ranking for Stock Prediction. *ACM
    Trans. Inf. Syst.* 37, 2 (2019), 27:1–27:30.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Gao et al. (2020) Jingkun Gao, Xiaomin Song, Qingsong Wen, Pichao Wang, Liang
    Sun, and Huan Xu. 2020. RobustTAD: Robust Time Series Anomaly Detection via Decomposition
    and Convolutional Neural Networks. *CoRR* abs/2002.09545 (2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Godfrey and Gashler (2018) Luke B. Godfrey and Michael S. Gashler. 2018. Neural
    Decomposition of Time-Series Data for Effective Generalization. *IEEE Trans. Neural
    Networks Learn. Syst.* 29, 7 (2018), 2973–2985.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Guibas et al. (2022) John Guibas, Morteza Mardani, Zongyi Li, Andrew Tao, Anima
    Anandkumar, and Bryan Catanzaro. 2022. Adaptive Fourier Neural Operators: Efficient
    Token Mixers for Transformers. In *ICLR*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu and Qi (2017) Hao Hu and Guo-Jun Qi. 2017. State-Frequency Memory Recurrent
    Neural Networks. In *ICML* *(Proceedings of Machine Learning Research, Vol. 70)*.
    PMLR, 1568–1577.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hundman et al. (2018) Kyle Hundman, Valentino Constantinou, Christopher Laporte,
    Ian Colwell, and Tom Söderström. 2018. Detecting Spacecraft Anomalies Using LSTMs
    and Nonparametric Dynamic Thresholding. In *KDD*. ACM, 387–395.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keogh and Kasetty (2003) Eamonn J. Keogh and Shruti Kasetty. 2003. On the Need
    for Time Series Data Mining Benchmarks: A Survey and Empirical Demonstration.
    *Data Min. Knowl. Discov.* 7, 4 (2003), 349–371.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Khan and Yener (2018) Haidar Khan and Bülent Yener. 2018. Learning filter widths
    of spectral decompositions with wavelets. In *NeurIPS*. 4606–4617.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kovachki et al. (2021) Nikola B. Kovachki, Zongyi Li, Burigede Liu, Kamyar
    Azizzadenesheli, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar.
    2021. Neural Operator: Learning Maps Between Function Spaces. *CoRR* abs/2108.08481
    (2021).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lai et al. (2018) Guokun Lai, Wei-Cheng Chang, Yiming Yang, and Hanxiao Liu.
    2018. Modeling Long- and Short-Term Temporal Patterns with Deep Neural Networks.
    In *SIGIR*. 95–104.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Li et al. (2018) Yaguang Li, Rose Yu, Cyrus Shahabi, and Yan Liu. 2018. Diffusion
    Convolutional Recurrent Neural Network: Data-Driven Traffic Forecasting. In *ICLR
    (Poster)*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li et al. (2021) Zongyi Li, Nikola Borislavov Kovachki, Kamyar Azizzadenesheli,
    Burigede Liu, Kaushik Bhattacharya, Andrew M. Stuart, and Anima Anandkumar. 2021.
    Fourier Neural Operator for Parametric Partial Differential Equations. In *ICLR*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. (2023) Zhen Liu, Qianli Ma, Peitian Ma, and Linghao Wang. 2023. Temporal-Frequency
    Co-training for Time Series Semi-supervised Learning. In *AAAI*. AAAI Press, 8923–8931.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lundberg and Lee (2017) Scott M. Lundberg and Su-In Lee. 2017. A Unified Approach
    to Interpreting Model Predictions. In *NIPS*. 4765–4774.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Mallat (1989) Stéphane Mallat. 1989. A Theory for Multiresolution Signal Decomposition:
    The Wavelet Representation. *IEEE Trans. Pattern Anal. Mach. Intell.* 11, 7 (1989),
    674–693.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Michau et al. (2022) Gabriel Michau, Gaetan Frusque, and Olga Fink. 2022. Fully
    learnable deep wavelet transform for unsupervised monitoring of high-frequency
    time series. *Proceedings of the National Academy of Sciences* 119, 8 (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Park et al. (2021) Yong-chan Park, Jun-Gi Jang, and U Kang. 2021. Fast and Accurate
    Partial Fourier Transform for Time Series Data. In *KDD*. ACM, 1309–1318.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Qin et al. (2021) Zequn Qin, Pengyi Zhang, Fei Wu, and Xi Li. 2021. FcaNet:
    Frequency Channel Attention Networks. In *ICCV*. IEEE, 763–772.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ren et al. (2019) Hansheng Ren, Bixiong Xu, Yujing Wang, Chao Yi, Congrui Huang,
    Xiaoyu Kou, Tony Xing, Mao Yang, Jie Tong, and Qi Zhang. 2019. Time-Series Anomaly
    Detection Service at Microsoft. In *KDD*. ACM, 3009–3017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ribeiro et al. (2016) Marco Túlio Ribeiro, Sameer Singh, and Carlos Guestrin.
    2016. ”Why Should I Trust You?”: Explaining the Predictions of Any Classifier.
    In *KDD*. ACM, 1135–1144.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rippel et al. (2015) Oren Rippel, Jasper Snoek, and Ryan P. Adams. 2015. Spectral
    Representations for Convolutional Neural Networks. In *NIPS*. 2449–2457.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Roberts and Mullis (1987) Richard A Roberts and Clifford T Mullis. 1987. *Digital
    signal processing*. Addison-Wesley Longman Publishing Co., Inc.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Schäfer et al. (2021) Patrick Schäfer, Arik Ermshaus, and Ulf Leser. 2021. ClaSP
    - Time Series Segmentation. In *CIKM*. ACM, 1578–1587.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shensa et al. (1992) Mark J Shensa et al. 1992. The discrete wavelet transform:
    wedding the a trous and Mallat algorithms. *IEEE Transactions on signal processing*
    40, 10 (1992), 2464–2482.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soliman and Srinath (1990) S. S. Soliman and MD Srinath. 1990. Continuous and
    discrete signals and systems. *Prentice Hall,* (1990).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Sun and Boning (2022) Fan-Keng Sun and Duane S. Boning. 2022. FreDo: Frequency
    Domain-based Long-Term Time Series Forecasting. *CoRR* abs/2205.12301 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tao et al. (2019) Ran Tao, Xudong Zhao, Wei Li, Heng-Chao Li, and Qian Du. 2019.
    Hyperspectral Anomaly Detection by Fractional Fourier Entropy. *IEEE J. Sel. Top.
    Appl. Earth Obs. Remote. Sens.* 12, 12 (2019), 4920–4929.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang et al. (2018) Jingyuan Wang, Ze Wang, Jianfeng Li, and Junjie Wu. 2018.
    Multilevel Wavelet Decomposition Network for Interpretable Time Series Analysis.
    In *KDD*. ACM, 2437–2446.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. (2023) Jingyuan Wang, Chen Yang, Xiaohan Jiang, and Junjie Wu.
    2023. WHEN: A Wavelet-DTW Hybrid Attention Network for Heterogeneous Time Series
    Analysis. In *KDD*. ACM, 2361–2373.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. (2021a) Qingsong Wen, Kai He, Liang Sun, Yingying Zhang, Min Ke,
    and Huan Xu. 2021a. RobustPeriod: Robust Time-Frequency Mining for Multiple Periodicity
    Detection. In *SIGMOD Conference*. ACM, 2328–2337.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wen et al. (2021b) Qingsong Wen, Liang Sun, Fan Yang, Xiaomin Song, Jingkun
    Gao, Xue Wang, and Huan Xu. 2021b. Time Series Data Augmentation for Deep Learning:
    A Survey. In *IJCAI*. ijcai.org, 4653–4660.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Winograd (1976) Shmuel Winograd. 1976. On computing the discrete Fourier transform.
    *Proceedings of the National Academy of Sciences* 73, 4 (1976), 1005–1006.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Woo et al. (2022a) Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and
    Steven C. H. Hoi. 2022a. CoST: Contrastive Learning of Disentangled Seasonal-Trend
    Representations for Time Series Forecasting. In *ICLR*. OpenReview.net.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Woo et al. (2022b) Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and
    Steven C. H. Hoi. 2022b. ETSformer: Exponential Smoothing Transformers for Time-series
    Forecasting. *CoRR* abs/2202.01381 (2022).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Woo et al. (2023) Gerald Woo, Chenghao Liu, Doyen Sahoo, Akshat Kumar, and Steven
    C. H. Hoi. 2023. Learning Deep Time-index Models for Time Series Forecasting.
    In *ICML* *(Proceedings of Machine Learning Research, Vol. 202)*. PMLR, 37217–37237.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wu et al. (2021) Haixu Wu, Jiehui Xu, Jianmin Wang, and Mingsheng Long. 2021.
    Autoformer: Decomposition Transformers with Auto-Correlation for Long-Term Series
    Forecasting. In *NeurIPS*. 22419–22430.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Xu et al. (2020) Kai Xu, Minghai Qin, Fei Sun, Yuhao Wang, Yen-Kuang Chen, and
    Fengbo Ren. 2020. Learning in the Frequency Domain. In *CVPR*. 1737–1746.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Yang et al. (2023) Fuhao Yang, Xin Li, Min Wang, Hongyu Zang, Wei Pang, and
    Mingzhong Wang. 2023. WaveForM: Graph Enhanced Wavelet Learning for Long Sequence
    Forecasting of Multivariate Time Series. In *AAAI*. AAAI Press, 10754–10761.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang and Hong (2022) Ling Yang and Shenda Hong. 2022. Unsupervised Time-Series
    Representation Learning with Iterative Bilinear Temporal-Spectral Fusion. In *ICML*
    *(Proceedings of Machine Learning Research, Vol. 162)*. PMLR, 25038–25054.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. (2022) Zhangjing Yang, Weiwu Yan, Xiaolin Huang, and Lin Mei. 2022.
    Adaptive Temporal-Frequency Network for Time-Series Forecasting. *IEEE Trans.
    Knowl. Data Eng.* 34, 4 (2022), 1576–1587.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yi et al. (2022) Kun Yi, Qi Zhang, Liang Hu, Hui He, Ning An, Longbing Cao,
    and Zhendong Niu. 2022. Edge-Varying Fourier Graph Networks for Multivariate Time
    Series Forecasting. *CoRR* abs/2210.03093 (2022).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhang et al. (2022b) Chaoli Zhang, Tian Zhou, Qingsong Wen, and Liang Sun.
    2022b. TFAD: A Decomposition Time Series Anomaly Detection Architecture with Time-Frequency
    Analysis. In *CIKM*. ACM, 2497–2507.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2017) Liheng Zhang, Charu C. Aggarwal, and Guo-Jun Qi. 2017. Stock
    Price Prediction via Discovering Multi-Frequency Trading Patterns. In *KDD*. ACM,
    2141–2149.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. (2022a) Xiang Zhang, Ziyuan Zhao, Theodoros Tsiligkaridis, and
    Marinka Zitnik. 2022a. Self-Supervised Contrastive Pre-Training For Time Series
    via Time-Frequency Consistency. In *NeurIPS*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022a) Xudong Zhao, Ran Tao, Wei Li, Wilfried Philips, and Wenzhi
    Liao. 2022a. Fractional Gabor Convolutional Network for Multisource Remote Sensing
    Data Classification. *IEEE Trans. Geosci. Remote. Sens.* 60 (2022), 1–18.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhao et al. (2022b) Xudong Zhao, Mengmeng Zhang, Ran Tao, Wei Li, Wenzhi Liao,
    Lianfang Tian, and Wilfried Philips. 2022b. Fractional Fourier Image Transformer
    for Multimodal Remote Sensing Data Classification. *IEEE Trans. Neural Networks
    Learn. Syst.* (2022), 1–13.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. (2022c) Kun Zhou, Hui Yu, Wayne Xin Zhao, and Ji-Rong Wen. 2022c.
    Filter-enhanced MLP is All You Need for Sequential Recommendation. In *WWW*. 2388–2399.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2022a) Tian Zhou, Ziqing Ma, Xue Wang, Qingsong Wen, Liang Sun,
    Tao Yao, Wotao Yin, and Rong Jin. 2022a. FiLM: Frequency improved Legendre Memory
    Model for Long-term Time Series Forecasting. In *NeurIPS*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zhou et al. (2022b) Tian Zhou, Ziqing Ma, Qingsong Wen, Xue Wang, Liang Sun,
    and Rong Jin. 2022b. FEDformer: Frequency enhanced decomposed transformer for
    long-term series forecasting. In *ICML*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
