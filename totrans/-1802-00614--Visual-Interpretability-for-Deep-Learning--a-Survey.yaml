- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 分类：未分类
- en: 'date: 2024-09-06 20:08:13'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 日期：2024-09-06 20:08:13
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[1802.00614] Visual Interpretability for Deep Learning: a Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[1802.00614] 深度学习的可视解释性：综述'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1802.00614](https://ar5iv.labs.arxiv.org/html/1802.00614)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/1802.00614](https://ar5iv.labs.arxiv.org/html/1802.00614)
- en: 'Visual Interpretability for Deep Learning: a Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习的可视解释性：综述
- en: Quanshi Zhang and Song-Chun Zhu
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 张全实和朱松纯
- en: University of California, Los Angeles
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 加利福尼亚大学洛杉矶分校
- en: Abstract
  id: totrans-9
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要
- en: This paper reviews recent studies in understanding neural-network representations
    and learning neural networks with interpretable/disentangled middle-layer representations.
    Although deep neural networks have exhibited superior performance in various tasks,
    the interpretability is always the Achilles’ heel of deep neural networks. At
    present, deep neural networks obtain high discrimination power at the cost of
    low interpretability of their black-box representations. We believe that high
    model interpretability may help people to break several bottlenecks of deep learning,
    *e.g.* learning from very few annotations, learning via human-computer communications
    at the semantic level, and semantically debugging network representations. We
    focus on convolutional neural networks (CNNs), and we revisit the visualization
    of CNN representations, methods of diagnosing representations of pre-trained CNNs,
    approaches for disentangling pre-trained CNN representations, learning of CNNs
    with disentangled representations, and middle-to-end learning based on model interpretability.
    Finally, we discuss prospective trends in explainable artificial intelligence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本文回顾了最近关于理解神经网络表示和学习具有可解释/解缠结中间层表示的神经网络的研究。虽然深度神经网络在各种任务中表现出色，但可解释性始终是深度神经网络的致命弱点。目前，深度神经网络在获得高判别能力的同时，黑箱表示的可解释性较低。我们认为，高模型可解释性可能有助于突破深度学习的几个瓶颈，如*例如*从非常少的标注中学习、通过人机语义层面交流进行学习，以及对网络表示进行语义调试。我们专注于卷积神经网络（CNNs），并重新审视CNN表示的可视化、诊断预训练CNN表示的方法、解缠结预训练CNN表示的方法、学习具有解缠结表示的CNN以及基于模型可解释性的中到端学习。最后，我们讨论了可解释人工智能的前景趋势。
- en: 1 Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1 引言
- en: Convolutional neural networks (CNNs) LeCun et al. ([1998a](#bib.bib16)); Krizhevsky
    et al. ([2012](#bib.bib13)); He et al. ([2016](#bib.bib8)); Huang et al. ([2017](#bib.bib10))
    have achieved superior performance in many visual tasks, such as object classification
    and detection. However, the end-to-end learning strategy makes CNN representations
    a black box. Except for the final network output, it is difficult for people to
    understand the logic of CNN predictions hidden inside the network. In recent years,
    a growing number of researchers have realized that high model interpretability
    is of significant value in both theory and practice and have developed models
    with interpretable knowledge representations.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络（CNNs）LeCun等人 ([1998a](#bib.bib16)); Krizhevsky等人 ([2012](#bib.bib13));
    He等人 ([2016](#bib.bib8)); Huang等人 ([2017](#bib.bib10)) 在许多视觉任务中取得了优异的表现，如目标分类和检测。然而，端到端学习策略使得CNN表示成为一个黑箱。除了最终的网络输出之外，人们很难理解隐藏在网络内部的CNN预测逻辑。近年来，越来越多的研究者意识到，高模型可解释性在理论和实践中具有重要价值，并开发了具有可解释知识表示的模型。
- en: In this paper, we conduct a survey of current studies in understanding neural-network
    representations and learning neural networks with interpretable/disentangled representations.
    We can roughly define the scope of the review into the following six research
    directions.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本文对当前理解神经网络表示和学习具有可解释/解缠结表示的神经网络的研究进行了调查。我们可以大致将综述的范围定义为以下六个研究方向。
- en: •
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Visualization of CNN representations in intermediate network layers. These
    methods mainly synthesize the image that maximizes the score of a given unit in
    a pre-trained CNN or invert feature maps of a conv-layer back to the input image.
    Please see Section [2](#S2 "2 Visualization of CNN representations ‣ Visual Interpretability
    for Deep Learning: a Survey") for detailed discussion.'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CNN表示在中间网络层的可视化。这些方法主要合成最大化预训练CNN中给定单元分数的图像，或将卷积层的特征图反演回输入图像。详细讨论请参见第[2](#S2
    "2 Visualization of CNN representations ‣ Visual Interpretability for Deep Learning:
    a Survey")节。'
- en: •
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Diagnosis of CNN representations. Related studies may either diagnose a CNN’s
    feature space for different object categories or discover potential representation
    flaws in conv-layers. Please see Section [3](#S3 "3 Diagnosis of CNN representations
    ‣ Visual Interpretability for Deep Learning: a Survey") for details.'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 'CNN表示的诊断。相关研究可能会诊断CNN的特征空间以区分不同的对象类别，或发现卷积层中的潜在表示缺陷。有关详细信息，请参见第[3](#S3 "3 Diagnosis
    of CNN representations ‣ Visual Interpretability for Deep Learning: a Survey")节。'
- en: •
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Disentanglement of “the mixture of patterns” encoded in each filter of CNNs.
    These studies mainly disentangle complex representations in conv-layers and transform
    network representations into interpretable graphs. Please see Section [4](#S4
    "4 Disentangling CNN representations into explanatory graphs & decision trees
    ‣ Visual Interpretability for Deep Learning: a Survey") for details.'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '解析卷积神经网络（CNN）中每个滤波器编码的“模式混合”。这些研究主要是解开卷积层中的复杂表示，并将网络表示转化为可解释的图形。有关详细信息，请参见第[4](#S4
    "4 Disentangling CNN representations into explanatory graphs & decision trees
    ‣ Visual Interpretability for Deep Learning: a Survey")节。'
- en: •
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Building explainable models. We discuss interpretable CNNs Zhang et al. ([2017c](#bib.bib42)),
    capsule networks Sabour et al. ([2017](#bib.bib26)), interpretable R-CNNs Wu et
    al. ([2017](#bib.bib35)), and the InfoGAN Chen et al. ([2016](#bib.bib4)) in Section [5](#S5
    "5 Learning neural networks with interpretable/disentangled representations ‣
    Visual Interpretability for Deep Learning: a Survey").'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '构建可解释的模型。我们讨论了可解释的CNN Zhang等人（[2017c](#bib.bib42)）、胶囊网络 Sabour等人（[2017](#bib.bib26)）、可解释的R-CNNs Wu等人（[2017](#bib.bib35)）和InfoGAN Chen等人（[2016](#bib.bib4)），详细信息请参见第[5](#S5
    "5 Learning neural networks with interpretable/disentangled representations ‣
    Visual Interpretability for Deep Learning: a Survey")节。'
- en: •
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Semantic-level middle-to-end learning via human-computer interaction. A clear
    semantic disentanglement of CNN representations may further enable “middle-to-end”
    learning of neural networks with weak supervision. Section [7](#S7 "7 Network
    interpretability for middle-to-end learning ‣ Visual Interpretability for Deep
    Learning: a Survey") introduces methods to learn new models via human-computer
    interactions Zhang et al. ([2017b](#bib.bib41)) and active question-answering
    with very limited human supervision Zhang et al. ([2017a](#bib.bib40)).'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '通过人机交互进行的语义级中到端学习。CNN表示的清晰语义解构可能进一步使得通过弱监督进行神经网络的“中到端”学习成为可能。第[7](#S7 "7 Network
    interpretability for middle-to-end learning ‣ Visual Interpretability for Deep
    Learning: a Survey")节介绍了通过人机交互学习新模型的方法 Zhang等人（[2017b](#bib.bib41)）和在非常有限的人工监督下的主动问答 Zhang等人（[2017a](#bib.bib40)）。'
- en: Among all the above, the visualization of CNN representations is the most direct
    way to explore network representations. The network visualization also provides
    a technical foundation for many approaches to diagnosing CNN representations.
    The disentanglement of feature representations of a pre-trained CNN and the learning
    of explainable network representations present bigger challenges to state-of-the-art
    algorithms. Finally, explainable or disentangled network representations are also
    the starting point for weakly-supervised middle-to-end learning.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述所有方面中，CNN表示的可视化是探索网络表示的最直接方式。网络可视化还为诊断CNN表示的许多方法提供了技术基础。对预训练CNN的特征表示的解构和可解释网络表示的学习对最先进的算法提出了更大的挑战。最后，可解释或解构的网络表示也是弱监督中到端学习的起点。
- en: 'Values of model interpretability: The clear semantics in high conv-layers can
    help people trust a network’s prediction. As discussed in Zhang et al. ([2018b](#bib.bib44)),
    considering dataset and representation bias, a high accuracy on testing images
    still cannot ensure that a CNN will encode correct representations. For example,
    a CNN may use an unreliable context—eye features—to identify the “lipstick” attribute
    of a face image. Therefore, people usually cannot fully trust a network unless
    a CNN can semantically or visually explain its logic, *e.g.* what patterns are
    used for prediction.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可解释性的价值：高卷积层中的清晰语义可以帮助人们信任网络的预测。正如Zhang等人（[2018b](#bib.bib44)）所讨论的那样，考虑到数据集和表示偏差，即使在测试图像上具有高准确率，仍然不能确保CNN会编码正确的表示。例如，CNN可能使用不可靠的上下文——眼部特征——来识别面部图像的“口红”属性。因此，除非CNN能够在语义上或视觉上解释其逻辑，*例如*用于预测的模式，否则人们通常无法完全信任网络。
- en: In addition, the middle-to-end learning or debugging of neural networks based
    on the explainable or disentangled network representations may also significantly
    reduce the requirement for human annotation. Furthermore, based on semantic representations
    of networks, it is possible to merge multiple CNNs into a universal network (*i.e.*
    a network encoding generic knowledge representations for different tasks) at the
    semantic level in the future.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于可解释或解耦网络表示的神经网络中后期学习或调试可能会显著减少对人工标注的需求。此外，基于网络的语义表示，未来有可能在语义层面上将多个 CNN
    合并为一个通用网络（*即* 一个编码不同任务的通用知识表示的网络）。
- en: In the following sections, we review the above research directions and discuss
    the potential future of technical developments.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将回顾上述研究方向，并讨论技术发展的潜在未来。
- en: 2 Visualization of CNN representations
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 CNN 表示的可视化
- en: Visualization of filters in a CNN is the most direct way of exploring visual
    patterns hidden inside a neural unit. Different types of visualization methods
    have been developed for network visualization.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CNN 中对滤波器的可视化是探索神经单元内部隐藏视觉模式的最直接方式。已开发出不同类型的可视化方法用于网络可视化。
- en: First, gradient-based methods Zeiler and Fergus ([2014](#bib.bib38)); Mahendran
    and Vedaldi ([2015](#bib.bib20)); Simonyan et al. ([2013](#bib.bib28)); Springenberg
    et al. ([2015](#bib.bib29)) are the mainstream of network visualization. These
    methods mainly compute gradients of the score of a given CNN unit *w.r.t.* the
    input image. They use the gradients to estimate the image appearance that maximizes
    the unit score. Olah et al. ([2017](#bib.bib23)) has provided a toolbox of existing
    techniques to visualize patterns encoded in different conv-layers of a pre-trained
    CNN.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，基于梯度的方法 Zeiler 和 Fergus ([2014](#bib.bib38))；Mahendran 和 Vedaldi ([2015](#bib.bib20))；Simonyan
    等人 ([2013](#bib.bib28))；Springenberg 等人 ([2015](#bib.bib29)) 是网络可视化的主流。这些方法主要计算给定
    CNN 单元相对于输入图像的分数的梯度。它们使用这些梯度来估计最大化单元分数的图像外观。Olah 等人 ([2017](#bib.bib23)) 提供了一个工具箱，包含现有技术，用于可视化预训练
    CNN 中不同卷积层编码的模式。
- en: Second, the up-convolutional net Dosovitskiy and Brox ([2016](#bib.bib5)) is
    another typical technique to visualize CNN representations. The up-convolutional
    net inverts CNN feature maps to images. We can regard up-convolutional nets as
    a tool that indirectly illustrates the image appearance corresponding to a feature
    map, although compared to gradient-based methods, up-convolutional nets cannot
    mathematically ensure that the visualization result exactly reflects actual representations
    in the CNN. Similarly, Nguyen et al. ([2017](#bib.bib22)) has further introduced
    an additional prior, which controls the semantic meaning of the synthesized image,
    to the adversarial generative network. We can use CNN feature maps as the prior
    for visualization.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，Dosovitskiy 和 Brox ([2016](#bib.bib5)) 提出的上卷积网络是另一种典型的技术，用于可视化 CNN 表示。上卷积网络将
    CNN 特征图反转为图像。我们可以将上卷积网络视为一种工具，间接说明特征图对应的图像外观，尽管与基于梯度的方法相比，上卷积网络无法在数学上确保可视化结果完全反映
    CNN 中的实际表示。类似地，Nguyen 等人 ([2017](#bib.bib22)) 进一步向对抗生成网络引入了一个额外的先验，该先验控制合成图像的语义含义。我们可以使用
    CNN 特征图作为可视化的先验。
- en: In addition, Zhou et al. ([2015](#bib.bib46)) has proposed a method to accurately
    compute the image-resolution receptive field of neural activations in a feature
    map. The actual receptive field of neural activation is smaller than the theoretical
    receptive field computed using the filter size. The accurate estimation of the
    receptive field helps people to understand the representation of a filter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Zhou 等人 ([2015](#bib.bib46)) 提出了一个方法来准确计算特征图中神经激活的图像分辨率感受野。神经激活的实际感受野比使用滤波器大小计算的理论感受野要小。对感受野的准确估计有助于人们理解滤波器的表示。
- en: 3 Diagnosis of CNN representations
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3 CNN 表示的诊断
- en: Some methods go beyond the visualization of CNNs and diagnose CNN representations
    to obtain insight understanding of features encoded in a CNN. We roughly divide
    all relevant research into the following five directions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一些方法超越了 CNN 的可视化，通过诊断 CNN 表示来获得对 CNN 编码特征的深入理解。我们大致将所有相关研究分为以下五个方向。
- en: Studies in the first direction analyze CNN features from a global view. Szegedy
    et al. ([2014](#bib.bib31)) has explored semantic meanings of each filter. Yosinski
    et al. ([2014](#bib.bib37)) has analyzed the transferability of filter representations
    in intermediate conv-layers. Lu ([2015](#bib.bib19)); Aubry and Russell ([2015](#bib.bib1))
    have computed feature distributions of different categories/attributes in the
    feature space of a pre-trained CNN.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: The second research direction extracts image regions that directly contribute
    the network output for a label/attribute to explain CNN representations of the
    label/attribute. This is similar to the visualization of CNNs. Methods of Fong
    and Vedaldi ([2017](#bib.bib6)); Selvaraju et al. ([2017](#bib.bib27)) have been
    proposed to propagate gradients of feature maps *w.r.t.* the final loss back to
    the image plane to estimate the image regions. The LIME model proposed in Ribeiro
    et al. ([2016](#bib.bib25)) extracts image regions that are highly sensitive to
    the network output. Studies of Zintgraf et al. ([2017](#bib.bib47)); Kindermans
    et al. ([2017](#bib.bib11)); Kumar et al. ([2017](#bib.bib14)) have invented methods
    to visualize areas in the input image that contribute the most to the decision-making
    process of the CNN. Wang et al. ([2017](#bib.bib32)); Goyal et al. ([2016](#bib.bib7))
    have tried to interpret the logic for visual question-answering encoded in neural
    networks. These studies list important objects (or regions of interests) detected
    from the images and crucial words in questions as the explanation of output answers.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: The estimation of vulnerable points in the feature space of a CNN is also a
    popular direction for diagnosing network representations. Approaches of Su et
    al. ([2017](#bib.bib30)); Koh and Liang ([2017](#bib.bib12)); Szegedy et al. ([2014](#bib.bib31))
    have been developed to compute adversarial samples for a CNN. *I.e.* these studies
    aim to estimate the minimum noisy perturbation of the input image that can change
    the final prediction. In particular, influence functions proposed in Koh and Liang
    ([2017](#bib.bib12)) can be used to compute adversarial samples. The influence
    function can also provide plausible ways to create training samples to attack
    the learning of CNNs, fix the training set, and further debug representations
    of a CNN.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: The fourth research direction is to refine network representations based on
    the analysis of network feature spaces. Given a CNN pre-trained for object classification,
    Lakkaraju et al. ([2017](#bib.bib15)) has proposed a method to discover knowledge
    blind spots (unknown patterns) of the CNN in a weakly-supervised manner. This
    method grouped all sample points in the entire feature space of a CNN into thousands
    of pseudo-categories. It assumed that a well learned CNN would use the sub-space
    of each pseudo-category to exclusively represent a subset of a specific object
    class. In this way, this study randomly showed object samples within each sub-space,
    and used the sample purity in the sub-space to discover potential representation
    flaws hidden in a pre-trained CNN. To distill representations of a teacher network
    to a student network for sentiment analysis, Hu et al. ([2016](#bib.bib9)) has
    proposed using logic rules of natural languages (*e.g.* I-ORG cannot follow B-PER)
    to construct a distillation loss to supervise the knowledge distillation of neural
    networks, in order to obtain more meaningful network representations.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第四个研究方向是基于网络特征空间的分析来细化网络表示。针对一个用于对象分类的预训练 CNN，Lakkaraju 等人（[2017](#bib.bib15)）提出了一种方法，以弱监督的方式发现
    CNN 的知识盲点（未知模式）。该方法将 CNN 的整个特征空间中的所有样本点分组为数千个伪类别。它假设一个学习良好的 CNN 会使用每个伪类别的子空间来专门表示特定对象类别的子集。通过这种方式，本研究随机展示了每个子空间中的对象样本，并利用子空间中的样本纯度来发现预训练
    CNN 中隐藏的潜在表示缺陷。为了将教师网络的表示提炼到学生网络中以进行情感分析，Hu 等人（[2016](#bib.bib9)）提出使用自然语言的逻辑规则（*例如*
    I-ORG 不能跟在 B-PER 后面）来构建提炼损失，以监督神经网络的知识提炼，从而获得更有意义的网络表示。
- en: '![Refer to caption](img/830baac3c887bd8677bf3cad496d53bc.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/830baac3c887bd8677bf3cad496d53bc.png)'
- en: 'Figure 1: Biased representations in a CNN Zhang et al. ([2018b](#bib.bib44)).
    Considering potential dataset bias, a high accuracy on testing images cannot always
    ensure that a CNN learns correct representations. The CNN may use unreliable co-appearing
    contexts to make predictions. For example, people may manually modify mouth appearances
    of two faces by masking mouth regions or pasting another mouth, but such modifications
    do not significantly change prediction scores for the lipstick attribute. This
    figure shows heat maps of inference patterns of the lipstick attribute, where
    patterns with red/blue colors are positive/negative with the attribute score.
    The CNN mistakenly considers unrelated patterns as contexts to infer the lipstick.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：CNN 中的偏倚表示 Zhang 等人（[2018b](#bib.bib44)）。考虑到潜在的数据集偏差，高测试图像的准确率并不能总是确保 CNN
    学习到正确的表示。CNN 可能会使用不可靠的共同出现上下文进行预测。例如，人们可能通过遮盖嘴部区域或粘贴另一个嘴部来手动修改两个面孔的嘴部外观，但这些修改不会显著改变对口红属性的预测分数。此图显示了口红属性的推理模式热图，其中红色/蓝色的模式与属性分数为正/负。CNN
    错误地将无关的模式视为推断口红的上下文。
- en: 'Finally, Zhang et al. ([2018b](#bib.bib44)) has presented a method to discover
    potential, biased representations of a CNN. Fig. [1](#S3.F1 "Figure 1 ‣ 3 Diagnosis
    of CNN representations ‣ Visual Interpretability for Deep Learning: a Survey")
    shows biased representations of a CNN trained for the estimation of face attributes.
    When an attribute usually co-appears with specific visual features in training
    images, then the CNN may use such co-appearing features to represent the attribute.
    When the used co-appearing features are not semantically related to the target
    attribute, these features can be considered as biased representations.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '最后，Zhang 等人（[2018b](#bib.bib44)）提出了一种发现 CNN 潜在偏倚表示的方法。图 [1](#S3.F1 "Figure
    1 ‣ 3 Diagnosis of CNN representations ‣ Visual Interpretability for Deep Learning:
    a Survey") 显示了一个用于面部属性估计的 CNN 的偏倚表示。当一个属性通常与训练图像中的特定视觉特征共同出现时，CNN 可能会使用这些共同出现的特征来表示该属性。当使用的共同出现特征与目标属性在语义上无关时，这些特征可以被视为偏倚表示。'
- en: Given a pre-trained CNN (*e.g.* a CNN that was trained to estimate face attributes),
    Zhang et al. ([2018b](#bib.bib44)) required people to annotate some ground-truth
    relationships between attributes, *e.g.* the lipstick attribute is positively
    related to the heavy-makeup attribute, and is not related to the black hair attribute.
    Then, the method mined inference patterns of each attribute output from conv-layers,
    and used inference patterns to compute actual attribute relationships encoded
    in the CNN. Conflicts between the ground-truth and the mined attribute relationships
    indicated biased representations.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个预训练的CNN（例如，一个用于估计面部属性的CNN），张等人（[2018b](#bib.bib44)）要求人们注释某些属性之间的地面真实关系，例如，口红属性与浓妆属性正相关，与黑发属性无关。然后，该方法挖掘每个属性输出从卷积层中推断出的推理模式，并使用推理模式计算编码在CNN中的实际属性关系。地面真实性与挖掘属性关系之间的冲突表明偏见表示。
- en: 4 Disentangling CNN representations into explanatory graphs & decision trees
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4 将CNN表示解释为解释性图表和决策树
- en: 4.1 Disentangling CNN representations into explanatory graphs
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1 将CNN表示解释为解释性图表
- en: Compared to the visualization and diagnosis of network representations in previous
    sections, disentangling CNN features into human-interpretable graphical representations
    (namely explanatory graphs) provides a more thorough explanation of network representations.
    Zhang et al. ([2018a](#bib.bib43), [2016](#bib.bib39)) have proposed disentangling
    features in conv-layers of a pre-trained CNN and have used a graphical model to
    represent the semantic hierarchy hidden inside a CNN.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于前几节网络表示的可视化和诊断，将CNN特征解释为人类可解释的图形表示（即解释性图表），提供了对网络表示更为彻底的解释。张等人（[2018a](#bib.bib43)，[2016](#bib.bib39)）提出了在预训练的CNN的卷积层中解开特征，并使用图形模型来表示CNN内部隐藏的语义层次。
- en: '![Refer to caption](img/bc7739ae466c4915888f48986a585a26.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![见标题说明](img/bc7739ae466c4915888f48986a585a26.png)'
- en: 'Figure 2: Feature maps of a filter obtained using different input images Zhang
    et al. ([2018a](#bib.bib43)). To visualize the feature map, the method propagates
    receptive fields of activated units in the feature map back to the image plane.
    In each sub-feature, the filter is activated by various part patterns in an image.
    This makes it difficult to understand the semantic meaning of a filter.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2：使用不同输入图像获得的一个滤波器的特征图，张等人（[2018a](#bib.bib43)）。为了可视化特征图，该方法将激活单元的感受野传播回图像平面。在每个子特征中，该滤波器被图像中各种部分模式激活。这使得理解滤波器的语义含义变得困难。
- en: 'As shown in Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Disentangling CNN representations
    into explanatory graphs ‣ 4 Disentangling CNN representations into explanatory
    graphs & decision trees ‣ Visual Interpretability for Deep Learning: a Survey"),
    each filter in a high conv-layer of a CNN usually represents a mixture of patterns.
    For example, the filter may be activated by both the head and the tail parts of
    an object. Thus, to provide a global view of how visual knowledge is organized
    in a pre-trained CNN, studies of Zhang et al. ([2018a](#bib.bib43), [2016](#bib.bib39))
    aim to answer the following three questions.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [2](#S4.F2 "图 2 ‣ 4.1 将CNN表示解释为解释性图表 ‣ 4 将CNN表示解释为解释性图表和决策树 ‣ 深度学习的视觉可解释性：一项调查")
    所示，CNN 中高卷积层中的每个滤波器通常表示一种模式的混合。例如，该滤波器可能被对象的头部和尾部部分同时激活。因此，为了全面了解预训练的CNN中视觉知识的组织方式，张等人的研究（[2018a](#bib.bib43)，[2016](#bib.bib39)）旨在回答以下三个问题。
- en: •
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: How many types of visual patterns are memorized by each convolutional filter
    of the CNN (here, a visual pattern may describe a specific object part or a certain
    texture)?
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每个卷积滤波器记忆了多少种视觉模式（这里，视觉模式可以描述特定对象部分或某种纹理）？
- en: •
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Which patterns are co-activated to describe an object part?
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述一个对象部分的哪些模式同时被激活？
- en: •
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: What is the spatial relationship between two co-activated patterns?
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 两个同时激活的模式之间的空间关系是什么？
- en: '![Refer to caption](img/9f2c3c849a74a34f4c73f28017bf8acd.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![见标题说明](img/9f2c3c849a74a34f4c73f28017bf8acd.png)'
- en: 'Figure 3: Explanatory graph Zhang et al. ([2018a](#bib.bib43)). An explanatory
    graph represents the knowledge hierarchy hidden in conv-layers of a CNN. Each
    filter in a pre-trained CNN may be activated by different object parts. Zhang
    et al. ([2018a](#bib.bib43)) disentangles part patterns from each filter in an
    unsupervised manner, thereby clarifying the knowledge representation.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3：解释性图 张等人（[2018a](#bib.bib43)）。解释性图表示了隐藏在CNN卷积层中的知识层级。预训练CNN中的每个滤波器可能会被不同的物体部分激活。张等人（[2018a](#bib.bib43)）以无监督的方式将每个滤波器中的部分模式解开，从而阐明了知识表示。
- en: '![Refer to caption](img/d9a09de685a6b155715b3d7932cbcac3.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/d9a09de685a6b155715b3d7932cbcac3.png)'
- en: 'Figure 4: Image patches corresponding to different nodes in the explanatory
    graph Zhang et al. ([2018a](#bib.bib43)).'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4：与解释性图中不同节点对应的图像补丁 张等人（[2018a](#bib.bib43)）。
- en: '![Refer to caption](img/adde9a7d5c31ee64bed790466f23b815.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/adde9a7d5c31ee64bed790466f23b815.png)'
- en: 'Figure 5: Heat maps of patterns Zhang et al. ([2018a](#bib.bib43)). A heat
    map visualizes the spatial distribution of the top 50% patterns in the $L$-th
    layer of the explanatory graph with the highest inference scores.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：模式的热图 张等人（[2018a](#bib.bib43)）。热图可视化了解释性图第 $L$ 层中前50%模式的空间分布，这些模式具有最高的推断分数。
- en: '![Refer to caption](img/f2b12ca93f5f63bb95f6eff92e0a6097.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f2b12ca93f5f63bb95f6eff92e0a6097.png)'
- en: 'Figure 6: Image regions inferred by each node in an explanatory graph Zhang
    et al. ([2018a](#bib.bib43)). The method of Zhang et al. ([2018a](#bib.bib43))
    successfully disentangles object-part patterns from representations of every single
    filter.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6：解释性图中每个节点推断的图像区域 张等人（[2018a](#bib.bib43)）。张等人（[2018a](#bib.bib43)）的方法成功地将物体部分模式从每个单独滤波器的表示中解开。
- en: 'As shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4.1 Disentangling CNN representations
    into explanatory graphs ‣ 4 Disentangling CNN representations into explanatory
    graphs & decision trees ‣ Visual Interpretability for Deep Learning: a Survey"),
    the explanatory graph explains the knowledge semantic hidden inside the CNN. The
    explanatory graph disentangles the mixture of part patterns in each filter’s feature
    map of a conv-layer, and uses each graph node to represent a part.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [3](#S4.F3 "图 3 ‣ 4.1 解开CNN表示为解释性图 ‣ 4 解开CNN表示为解释性图与决策树 ‣ 深度学习的可视化解释性：综述")
    所示，解释性图解释了CNN内部隐藏的知识语义。解释性图解开了每个滤波器特征图中的部分模式混合，并使用每个图节点表示一个部分。
- en: •
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: The explanatory graph has multiple layers. Each graph layer corresponds to a
    specific conv-layer of a CNN.
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解释性图有多个层级。每个图层对应于CNN的特定卷积层。
- en: •
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Each filter in a conv-layer may represent the appearance of different object
    parts. The algorithm automatically disentangles the mixture of part patterns encoded
    in a single filter, and uses a node in the explanatory graph to represent each
    part pattern.
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 卷积层中的每个滤波器可能代表不同物体部分的外观。该算法自动解开编码在单个滤波器中的部分模式混合，并使用解释性图中的一个节点表示每个部分模式。
- en: •
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Each node in the explanatory graph consistently represents the same object part
    through different images. We can use the node to localize the corresponding part
    on the input image. To some extent, the node is robust to shape deformation and
    pose variations.
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 解释性图中的每个节点在不同图像中始终代表相同的物体部分。我们可以使用该节点在输入图像上定位相应的部分。在某种程度上，该节点对形状变形和姿势变化具有鲁棒性。
- en: •
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Each edge encodes the co-activation relationship and the spatial relationship
    between two nodes in adjacent layers.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 每条边编码了相邻层中两个节点之间的共同激活关系和空间关系。
- en: •
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: We can regard an explanatory graph as a compression of feature maps of conv-layers.
    A CNN has multiple conv-layers. Each conv-layer may have hundreds of filters,
    and each filter may produce a feature map with hundreds of neural units. We can
    use tens of thousands of nodes in the explanatory graph to represent information
    contained in all tens of millions of neural units in these feature maps, *i.e.*
    by which part patterns the feature maps are activated, and where the part patterns
    are localized in input images.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以将解释性图视为卷积层特征图的压缩。一个卷积神经网络（CNN）有多个卷积层。每个卷积层可能有数百个滤波器，每个滤波器可能生成一个包含数百个神经单元的特征图。我们可以使用解释性图中的数万个节点来表示这些特征图中所有几千万个神经单元所包含的信息，*即*特征图被激活的部分模式，以及这些部分模式在输入图像中的位置。
- en: •
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: Just like a dictionary, each input image can only trigger a small subset of
    part patterns (nodes) in the explanatory graph. Each node describes a common part
    pattern with high transferability, which is shared by hundreds or thousands of
    training images.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 就像字典一样，每张输入图像只能激发解释图中的一小部分部分模式（节点）。每个节点描述了一种具有高度可转移性的常见部分模式，这种模式被数百或数千张训练图像共享。
- en: 'Fig. [4](#S4.F4 "Figure 4 ‣ 4.1 Disentangling CNN representations into explanatory
    graphs ‣ 4 Disentangling CNN representations into explanatory graphs & decision
    trees ‣ Visual Interpretability for Deep Learning: a Survey") lists top-ranked
    image patches corresponding to different nodes in the explanatory graph. Fig. [5](#S4.F5
    "Figure 5 ‣ 4.1 Disentangling CNN representations into explanatory graphs ‣ 4
    Disentangling CNN representations into explanatory graphs & decision trees ‣ Visual
    Interpretability for Deep Learning: a Survey") visualizes the spatial distribution
    of object parts inferred by the top 50% nodes in the $L$-th layer of the explanatory
    graph with the highest inference scores. Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Disentangling
    CNN representations into explanatory graphs ‣ 4 Disentangling CNN representations
    into explanatory graphs & decision trees ‣ Visual Interpretability for Deep Learning:
    a Survey") shows object parts inferred by a single node.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '图[4](#S4.F4 "Figure 4 ‣ 4.1 Disentangling CNN representations into explanatory
    graphs ‣ 4 Disentangling CNN representations into explanatory graphs & decision
    trees ‣ Visual Interpretability for Deep Learning: a Survey") 列出了对应于解释图中不同节点的顶级图像块。图[5](#S4.F5
    "Figure 5 ‣ 4.1 Disentangling CNN representations into explanatory graphs ‣ 4
    Disentangling CNN representations into explanatory graphs & decision trees ‣ Visual
    Interpretability for Deep Learning: a Survey") 可视化了由解释图第 $L$ 层中具有最高推理分数的前 50%
    节点推断的物体部分的空间分布。图[6](#S4.F6 "Figure 6 ‣ 4.1 Disentangling CNN representations into
    explanatory graphs ‣ 4 Disentangling CNN representations into explanatory graphs
    & decision trees ‣ Visual Interpretability for Deep Learning: a Survey") 显示了由单个节点推断出的物体部分。'
- en: '4.1.1 Application: multi-shot part localization'
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1 应用：多次拍摄部分定位
- en: There are many potential applications based on the explanatory graph. For example,
    we can regard the explanatory graph as a visual dictionary of a category and transfer
    graph nodes to other applications, such as multi-shot part localization.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于解释图有许多潜在应用。例如，我们可以将解释图视为一种类别的视觉字典，并将图节点转移到其他应用中，如多次拍摄的部分定位。
- en: Given very few bounding boxes of an object part, Zhang et al. ([2018a](#bib.bib43))
    has proposed retrieving hundreds of nodes that are related to the part annotations
    from the explanatory graph, and then use the retrieved nodes to localize object
    parts in previously unseen images. Because each node in the explanatory graph
    encodes a part pattern shared by numerous training images, the retrieved nodes
    describe a general appearance of the target part without being over-fitted to
    the limited annotations of part bounding boxes. Given three annotations for each
    object part, the explanatory-graph-based method has exhibited superior performance
    of part localization and has decreased by about 1/3 localization errors *w.r.t.*
    the second-best baseline.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 给定非常少的物体部分边界框，Zhang等人 ([2018a](#bib.bib43)) 提出了从解释图中检索与部分注释相关的数百个节点，然后利用检索到的节点在以前未见过的图像中定位物体部分。由于解释图中的每个节点编码了许多训练图像共享的部分模式，检索到的节点描述了目标部分的一般外观，而不会过度拟合到有限的部分边界框注释上。对于每个物体部分给定三个注释，基于解释图的方法展示了优越的部分定位性能，相比第二好的基线减少了约
    1/3 的定位误差 *w.r.t.*。
- en: 4.2 Disentangling CNN representations into decision trees
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2 将CNN表示解缠为决策树
- en: '![Refer to caption](img/af30f70e45dc64b49a84c40439a4e994.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![参见标题](img/af30f70e45dc64b49a84c40439a4e994.png)'
- en: 'Figure 7: Decision tree that explains a CNN prediction at the semantic level Zhang
    et al. ([2018c](#bib.bib45)). A CNN is learned for object classification with
    disentangled representations in the top conv-layer, where each filter represents
    a specific object part. The decision tree encodes various decision modes hidden
    inside fully-connected layers of the CNN in a coarse-to-fine manner. Given an
    input image, the decision tree infers a parse tree (red lines) to quantitatively
    analyze rationales for the CNN prediction, *i.e.* which object parts (or filters)
    are used for prediction and how much an object part (or filter) contributes to
    the prediction.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7: 决策树在语义层面解释 CNN 预测的示意图 Zhang 等 ([2018c](#bib.bib45))。CNN 被用于对象分类，其顶层卷积层具有解耦的表示，每个滤波器代表一个特定的对象部分。决策树以粗到细的方式编码隐藏在
    CNN 全连接层中的各种决策模式。给定一个输入图像，决策树推断出一个解析树（红线），用于定量分析 CNN 预测的理由，*即* 哪些对象部分（或滤波器）用于预测，以及一个对象部分（或滤波器）对预测的贡献程度。'
- en: Zhang et al. ([2018c](#bib.bib45)) has further proposed a decision tree to encode
    decision modes in fully-connected layers. The decision tree is not designed for
    classification. Instead, the decision tree is used to quantitatively explain the
    logic for each CNN prediction. *I.e.* given an input image, we use the CNN to
    make a prediction. The decision tree tells people which filters in a conv-layer
    are used for the prediction and how much they contribute to the prediction.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Zhang 等 ([2018c](#bib.bib45)) 进一步提出了一个决策树来编码全连接层中的决策模式。该决策树并非用于分类，而是用于定量解释每个
    CNN 预测的逻辑。*即* 给定一个输入图像，我们使用 CNN 进行预测。决策树告诉人们在卷积层中哪些滤波器被用于预测，以及它们对预测的贡献程度。
- en: 'As shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.2 Disentangling CNN representations
    into decision trees ‣ 4 Disentangling CNN representations into explanatory graphs
    & decision trees ‣ Visual Interpretability for Deep Learning: a Survey"), the
    method mines potential decision modes memorized in fully-connected layers. The
    decision tree organizes these potential decision modes in a coarse-to-fine manner.
    Furthermore, this study uses the method of Zhang et al. ([2017c](#bib.bib42))
    to disentangle representations of filters in the top conv-layers, *i.e.* making
    each filter represent a specific object part. In this way, people can use the
    decision tree to explain rationales for each CNN prediction at the semantic level,
    *i.e.* which object parts are used by the CNN to make the prediction.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [7](#S4.F7 "图 7 ‣ 4.2 将 CNN 表示解耦为决策树 ‣ 4 解耦 CNN 表示为解释图 & 决策树 ‣ 深度学习的可视解释性：综述")所示，该方法挖掘了在全连接层中记忆的潜在决策模式。决策树以粗到细的方式组织这些潜在决策模式。此外，本研究采用
    Zhang 等 ([2017c](#bib.bib42)) 的方法解耦顶层卷积层中的滤波器表示，*即* 使每个滤波器代表一个特定的对象部分。通过这种方式，人们可以使用决策树在语义层面解释每个
    CNN 预测的理由，*即* 哪些对象部分被 CNN 用于预测。
- en: 5 Learning neural networks with interpretable/disentangled representations
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5 学习具有可解释/解耦表示的神经网络
- en: Almost all methods mentioned in previous sections focus on the understanding
    of a pre-trained network. In this section, we review studies of learning disentangled
    representations of neural networks, where representations in middle layers are
    no longer a black box but have clear semantic meanings. Compared to the understanding
    of pre-trained networks, learning networks with disentangled representations present
    more challenges. Up to now, only a few studies have been published in this direction.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前面章节中提到的几乎所有方法都集中于对预训练网络的理解。在本节中，我们回顾了学习解耦表示的神经网络的研究，其中中层表示不再是黑箱，而具有明确的语义意义。与对预训练网络的理解相比，学习具有解耦表示的网络带来了更多挑战。到目前为止，仅有少数研究在这一方向上发表。
- en: '![Refer to caption](img/9d6fc1980b877ba36b699c4cd5124d1a.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9d6fc1980b877ba36b699c4cd5124d1a.png)'
- en: 'Figure 8: Structures of an ordinary conv-layer and an interpretable conv-layer Zhang
    et al. ([2017c](#bib.bib42)). Green and red lines indicate the forward and backward
    propagations, respectively.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8: 普通卷积层和可解释卷积层的结构 Zhang 等 ([2017c](#bib.bib42))。绿色和红色线分别表示前向传播和反向传播。'
- en: '![Refer to caption](img/0389877cdbc570b3bd6b0fa75957f360.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/0389877cdbc570b3bd6b0fa75957f360.png)'
- en: 'Figure 9: Templates Zhang et al. ([2017c](#bib.bib42)). Each template $T_{\mu_{i}}$
    matches to a feature map when the target part mainly triggers the $i$-th unit
    in the feature map.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9: 模板 Zhang 等 ([2017c](#bib.bib42))。每个模板 $T_{\mu_{i}}$ 当目标部分主要激活特征图中的第 $i$
    个单元时与特征图匹配。'
- en: '![Refer to caption](img/2bf3c342629dd93651da494f33a2bb94.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/2bf3c342629dd93651da494f33a2bb94.png)'
- en: 'Figure 10: Visualization of interpretable filters in the top conv-layer Zhang
    et al. ([2017c](#bib.bib42)). We used Zhou et al. ([2015](#bib.bib46)) to estimate
    the image-resolution receptive field of activations in a feature map to visualize
    a filter’s semantics. An interpretable CNN usually encodes head patterns of animals
    in its top conv-layer for classification.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：张等人（[2017c](#bib.bib42)）在顶部卷积层中可解释滤波器的可视化。我们使用了周等人（[2015](#bib.bib46)）来估计特征图中激活的图像分辨率感受野，以可视化滤波器的语义。可解释的CNN通常在其顶部卷积层中编码动物的头部特征用于分类。
- en: 5.1 Interpretable convolutional neural networks
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1 可解释卷积神经网络
- en: 'As shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5 Learning neural networks with interpretable/disentangled
    representations ‣ Visual Interpretability for Deep Learning: a Survey"), Zhang
    et al. ([2017c](#bib.bib42)) has developed a method to modify an ordinary CNN
    to obtain disentangled representations in high conv-layers by adding a loss to
    each filter in the conv-layers. The loss is used to regularize the feature map
    towards the representation of a specific object part.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如图[9](#S5.F9 "图9 ‣ 5 学习具有可解释/解耦表示的神经网络 ‣ 深度学习的可视化解释：调查")所示，张等人（[2017c](#bib.bib42)）开发了一种方法，通过向卷积层中的每个滤波器添加损失，将普通CNN修改为获得高卷积层中的解耦表示。这种损失用于规范化特征图以表示特定的物体部件。
- en: 'Note that people do not need to annotate any object parts or textures to supervise
    the learning of interpretable CNNs. Instead, the loss automatically assigns an
    object part to each filter during the end-to-end learning process. As shown in
    Fig. [9](#S5.F9 "Figure 9 ‣ 5 Learning neural networks with interpretable/disentangled
    representations ‣ Visual Interpretability for Deep Learning: a Survey"), this
    method designs some templates. Each template $T_{\mu_{i}}$ is a matrix with the
    same size of feature map. $T_{\mu_{i}}$ describes the ideal distribution of activations
    for the feature map when the target part mainly triggers the $i$-th unit in the
    feature map.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，人们不需要注释任何物体部件或纹理来监督可解释CNN的学习。相反，损失在端到端学习过程中自动将物体部件分配给每个滤波器。如图[9](#S5.F9 "图9
    ‣ 5 学习具有可解释/解耦表示的神经网络 ‣ 深度学习的可视化解释：调查")所示，该方法设计了一些模板。每个模板$T_{\mu_{i}}$是一个与特征图大小相同的矩阵。$T_{\mu_{i}}$描述了当目标部件主要触发特征图中的第$i$个单元时，特征图的理想激活分布。
- en: Given the joint probability of fitting a feature map to a template, the loss
    of a filter is formulated as the mutual information between the feature map and
    the templates. This loss encourages a low entropy of inter-category activations.
    *I.e.* each filter in the conv-layer is assigned to a certain category. If the
    input image belongs to the target category, then the loss expects the filter’s
    feature map to match a template well; otherwise, the filter needs to remain inactivated.
    In addition, the loss also encourages a low entropy of spatial distributions of
    neural activations. *I.e.* when the input image belongs the target category, the
    feature map is supposed to exclusively fit a single template. In other words,
    the filter needs to activate a single location on the feature map.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 给定将特征图拟合到模板的联合概率，滤波器的损失被公式化为特征图与模板之间的互信息。这种损失鼓励低的类别间激活熵。*即*卷积层中的每个滤波器都被分配到某一类别。如果输入图像属于目标类别，则损失期望滤波器的特征图与模板匹配良好；否则，滤波器需要保持未激活。此外，损失还鼓励神经激活的空间分布熵低。*即*当输入图像属于目标类别时，特征图应独占地拟合单个模板。换句话说，滤波器需要在特征图上激活一个单一位置。
- en: This study assumes that if a filter repetitively activates various feature-map
    regions, then this filter is more likely to describe low-level textures (*e.g.*
    colors and edges), instead of high-level parts. For example, the left eye and
    the right eye may be represented by different filters, because contexts of the
    two eyes are symmetric, but not the same.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究假设，如果一个滤波器重复激活不同的特征图区域，则该滤波器更可能描述低级纹理（*例如*颜色和边缘），而不是高级部件。例如，左眼和右眼可能由不同的滤波器表示，因为两个眼睛的背景是对称的，但不完全相同。
- en: 'Fig.[10](#S5.F10 "Figure 10 ‣ 5 Learning neural networks with interpretable/disentangled
    representations ‣ Visual Interpretability for Deep Learning: a Survey") shows
    feature maps produced by different filters of an interpretable CNN. Each filter
    consistently represents the same object part through various images.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '图[10](#S5.F10 "Figure 10 ‣ 5 Learning neural networks with interpretable/disentangled
    representations ‣ Visual Interpretability for Deep Learning: a Survey")显示了由不同过滤器产生的可解释CNN的特征图。每个过滤器通过各种图像一致地表示相同的目标部件。'
- en: '![Refer to caption](img/e66ffdf4cb642db622d3bef93b6e56e1.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/e66ffdf4cb642db622d3bef93b6e56e1.png)'
- en: 'Figure 11: Detection examples of the proposed method Wu et al. ([2017](#bib.bib35)).
    In addition to predicted bounding boxes, the method also outputs the latent parse
    tree and part configurations as the qualitatively extractive rationale in detection.
    The parse trees are inferred on-the-fly in the space of latent structures, which
    follow a top-down compositional grammar of an AOG.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：Wu等人（[2017](#bib.bib35)）提出的方法的检测示例。除了预测的边界框外，该方法还输出潜在的解析树和部件配置，作为检测中定性提取的依据。解析树在潜在结构的空间中即时推断，遵循AOG的自上而下组合语法。
- en: 5.2 Interpretable R-CNN
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2 可解释的R-CNN
- en: Wu et al. ([2017](#bib.bib35)) has proposed the learning of qualitatively interpretable
    models for object detection based on the R-CNN. The objective is to unfold latent
    configurations of object parts automatically during the object-detection process.
    This method is learned without using any part annotations for supervision. Wu
    et al. ([2017](#bib.bib35)) uses a top-down hierarchical and compositional grammar,
    namely an And-Or graph (AOG), to model latent configurations of object parts.
    This method uses an AOG-based parsing operator to substitute for the RoI-Pooling
    operator used in the R-CNN. The AOG-based parsing harnesses explainable compositional
    structures of objects and maintains the discrimination power of a R-CNN. This
    idea is related to the disentanglement of the local, bottom-up, and top-down information
    components for prediction Wu et al. ([2007](#bib.bib34)); Yang et al. ([2009](#bib.bib36));
    Wu and Zhu ([2011](#bib.bib33)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: Wu等人（[2017](#bib.bib35)）提出了基于R-CNN的定性可解释模型的学习。目标是在目标检测过程中自动展开目标部件的潜在配置。该方法在学习过程中没有使用任何部件注释作为监督。Wu等人（[2017](#bib.bib35)）使用自上而下的层次化和组合语法，即And-Or图（AOG），来建模目标部件的潜在配置。该方法使用基于AOG的解析操作符来替代R-CNN中使用的RoI-Pooling操作符。基于AOG的解析利用了对象的可解释组合结构，并保持了R-CNN的判别能力。这一思想与预测中的局部、底层和自上而下信息组件的解耦相关，见Wu等人（[2007](#bib.bib34)）；Yang等人（[2009](#bib.bib36)）；Wu和Zhu（[2011](#bib.bib33)）。
- en: During the detection process, a bounding box is interpreted as the best parse
    tree derived from the AOG on-the-fly. During the learning process, a folding-unfolding
    method is used to train the AOG and R-CNN in an end-to-end manner.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在检测过程中，边界框被解释为从AOG即时推导出的最佳解析树。在学习过程中，使用折叠-展开方法以端到端的方式训练AOG和R-CNN。
- en: 'Fig. [11](#S5.F11 "Figure 11 ‣ 5.1 Interpretable convolutional neural networks
    ‣ 5 Learning neural networks with interpretable/disentangled representations ‣
    Visual Interpretability for Deep Learning: a Survey") illustrates an example of
    object detection. The proposed method detects object bounding boxes. The method
    also determines the latent parse tree and part configurations of objects as the
    qualitatively extractive rationale in detection.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '图[11](#S5.F11 "Figure 11 ‣ 5.1 Interpretable convolutional neural networks
    ‣ 5 Learning neural networks with interpretable/disentangled representations ‣
    Visual Interpretability for Deep Learning: a Survey")展示了一个目标检测的示例。该方法检测目标的边界框，同时确定目标的潜在解析树和部件配置，作为检测中的定性提取依据。'
- en: 5.3 Capsule networks
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3 胶囊网络
- en: Sabour et al. ([2017](#bib.bib26)) has designed novel neural units, namely capsules,
    in order to substitute for traditional neural units to construct a capsule network.
    Each capsule outputs an activity vector instead of a scalar. The length of the
    activity vector represents the activation strength of the capsule, and the orientation
    of the activity vector encodes instantiation parameters. Active capsules in the
    lower layer send messages to capsules in the adjacent higher layer. This method
    uses an iterative routing-by-agreement mechanism to assign higher weights with
    the low-layer capsules whose outputs better fit the instantiation parameters of
    the high-layer capsule.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Sabour 等人 ([2017](#bib.bib26)) 设计了新型神经单元，即胶囊，以替代传统的神经单元构建胶囊网络。每个胶囊输出一个活动向量而不是标量。活动向量的长度表示胶囊的激活强度，活动向量的方向编码实例化参数。下层的活跃胶囊向相邻的上层胶囊发送消息。这种方法使用迭代的基于一致性的路由机制，为输出更符合高层胶囊实例化参数的低层胶囊分配更高的权重。
- en: Experiments showed that when people trained capsule networks using the MNIST
    dataset LeCun et al. ([1998b](#bib.bib17)), a capsule encoded a specific semantic
    concept. Different dimensions of the activity vector of a capsule controlled different
    features, including 1) scale and thickness, 2) localized part, 3) stroke thickness,
    3) localized skew, and 4) width and translation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 实验表明，当人们使用 MNIST 数据集 LeCun 等人 ([1998b](#bib.bib17)) 训练胶囊网络时，一个胶囊编码了特定的语义概念。胶囊的活动向量的不同维度控制不同的特征，包括
    1) 尺寸和厚度，2) 局部部分，3) 笔划厚度，4) 局部倾斜，以及 5) 宽度和位移。
- en: 5.4 Information maximizing generative adversarial nets
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4 信息最大化生成对抗网络
- en: The information maximizing generative adversarial net Chen et al. ([2016](#bib.bib4)),
    namely InfoGAN, is an extension of the generative adversarial network. The InfoGAN
    maximizes the mutual information between certain dimensions of the latent representation
    and the image observation. The InfoGAN separates input variables of the generator
    into two types, *i.e.* the incompressible noise $z$ and the latent code $c$. This
    study aims to learn the latent code $c$ to encode certain semantic concepts in
    an unsupervised manner.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 信息最大化生成对抗网络 Chen 等人 ([2016](#bib.bib4))，即 InfoGAN，是生成对抗网络的一种扩展。InfoGAN 最大化潜在表示的某些维度与图像观察之间的互信息。InfoGAN
    将生成器的输入变量分为两种类型，即不可压缩的噪声 $z$ 和潜在代码 $c$。本研究旨在以无监督的方式学习潜在代码 $c$，以编码特定的语义概念。
- en: The InfoGAN has been trained using the MNIST dataset LeCun et al. ([1998b](#bib.bib17)),
    the CelebA dataset Liu et al. ([2015](#bib.bib18)), the SVHN dataset Netzer et
    al. ([2011](#bib.bib21)), the 3D face dataset Paysan et al. ([2009](#bib.bib24)),
    and the 3D chair dataset Aubry et al. ([2014](#bib.bib2)). Experiments have shown
    that the latent code has successfully encoded the digit type, the rotation, and
    the width of digits in the MNIST dataset, the lighting condition and the plate
    context in the SVHN dataset, the azimuth, the existence of glasses, the hairstyle,
    and the emotion in the CelebA dataset, and the width and 3D rotation in the 3D
    face and chair datasets.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: InfoGAN 已使用 MNIST 数据集 LeCun 等人 ([1998b](#bib.bib17))、CelebA 数据集 Liu 等人 ([2015](#bib.bib18))、SVHN
    数据集 Netzer 等人 ([2011](#bib.bib21))、3D 面部数据集 Paysan 等人 ([2009](#bib.bib24)) 和 3D
    椅子数据集 Aubry 等人 ([2014](#bib.bib2)) 进行训练。实验表明，潜在代码成功编码了 MNIST 数据集中数字的类型、旋转和宽度，SVHN
    数据集中的光照条件和背景，CelebA 数据集中的方位、眼镜存在、发型和情感，以及 3D 面部和椅子数据集中的宽度和 3D 旋转。
- en: 6 Evaluation metrics for network interpretability
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6 网络可解释性的评估指标
- en: Evaluation metrics for model interpretability are crucial for the development
    of explainable models. This is because unlike traditional well-defined visual
    applications (*e.g.* object detection and segmentation), network interpretability
    is more difficult to define and evaluate. The evaluation metric of network interpretability
    can help people define the concept of network interpretability and guide the development
    of learning interpretable network representations. Up to now, only very few studies
    have discussed the evaluation of network interpretability. Proposing a promising
    evaluation metric is still a big challenge to state-of-the-art algorithms. In
    this section, we simply introduce two latest evaluation metrics for the interpretability
    of CNN filters, *i.e.* the filter interpretability proposed by Bau et al. ([2017](#bib.bib3))
    and the location instability proposed by Zhang et al. ([2018a](#bib.bib43)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可解释性的评估指标对解释性模型的开发至关重要。这是因为与传统的明确定义的视觉应用（*例如*物体检测和分割）不同，网络可解释性更难以定义和评估。网络可解释性的评估指标可以帮助人们定义网络可解释性的概念，并指导学习可解释网络表示的开发。到目前为止，仅有很少的研究讨论了网络可解释性的评估。提出一个有前景的评估指标仍然是当前最先进算法面临的重大挑战。在这一部分，我们简要介绍了两种最新的卷积神经网络（CNN）滤波器可解释性评估指标，*即*
    Bau 等人提出的滤波器可解释性（[2017](#bib.bib3)）和张等人提出的位置不稳定性（[2018a](#bib.bib43)）。
- en: 6.1 Filter interpretability
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1 滤波器可解释性
- en: Bau et al. ([2017](#bib.bib3)) has defined six types of semantics for CNN filters,
    *i.e.* objects, parts, scenes, textures, materials, and colors. The evaluation
    of filter interpretability requires people to annotate these six types of semantics
    on testing images at the pixel level. The evaluation metric measures the fitness
    between the image-resolution receptive field of a filter’s neural activations¹¹1The
    method propagates the receptive field of each activated unit in a filter’s feature
    map back to the image plane as the image-resolution receptive field of a filter.
    and the pixel-level semantic annotations on the image. For example, if the receptive
    field of a filter’s neural activations usually highly overlaps with ground-truth
    image regions of a specific semantic concept through different images, then we
    can consider that the filter represents this semantic concept.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Bau 等人（[2017](#bib.bib3)）定义了 CNN 滤波器的六种语义类型，*即*对象、部件、场景、纹理、材料和颜色。滤波器可解释性的评估要求人们在测试图像的像素级别上标注这六种语义类型。评估指标衡量滤波器神经激活的图像分辨率感受野¹¹1该方法将滤波器特征图中每个激活单元的感受野传播回图像平面，以作为滤波器的图像分辨率感受野。与图像上的像素级语义标注之间的适配度。例如，如果滤波器神经激活的感受野通常与通过不同图像获得的特定语义概念的真实图像区域高度重叠，那么我们可以认为该滤波器表示了这一语义概念。
- en: For each filter $f$, this method computes its feature maps ${\bf X}=\{x=f(I)|I\in{\bf
    I}\}$ on different testing images. Then, the distribution of activation scores
    in all positions of all feature maps is computed. Bau et al. ([2017](#bib.bib3))
    set an activation threshold $T_{f}$ such that $p(x_{ij}>T_{f})=0.005$, to select
    top activations from all spatial locations $[i,j]$ of all feature maps $x\in{\bf
    X}$ as valid map regions corresponding to $f$’s semantics. Then, the method scales
    up low-resolution valid map regions to the image resolution, thereby obtaining
    the receptive field of valid activations on each image. We use $S_{f}^{I}$ to
    denote the receptive field of $f$’s valid activations *w.r.t.* the image $I$.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个滤波器 $f$，该方法计算其在不同测试图像上的特征图 ${\bf X}=\{x=f(I)|I\in{\bf I}\}$。然后，计算所有特征图所有位置的激活分数分布。Bau
    等人（[2017](#bib.bib3)）设置了一个激活阈值 $T_{f}$，使得 $p(x_{ij}>T_{f})=0.005$，从所有特征图 $x\in{\bf
    X}$ 的所有空间位置 $[i,j]$ 中选择最高激活作为与 $f$ 的语义对应的有效图区域。然后，该方法将低分辨率的有效图区域放大到图像分辨率，从而获得每个图像上有效激活的感受野。我们用
    $S_{f}^{I}$ 来表示 $f$ 相对于图像 $I$ 的有效激活的感受野。
- en: The compatibility between a filter $f$ and a specific semantic concept is reported
    as an intersection-over-union score $IoU_{f,k}^{I}\!=\!\frac{\|S_{f}^{I}\cap S_{k}^{I}\|}{\|S_{f}^{I}\cup
    S_{k}^{I}\|}$, where $S_{k}^{I}$ denotes the ground-truth mask of the $k$-th semantic
    concept on the image $I$. Given an image $I$, filter $f$ is associated with the
    $k$-th concept if $IoU_{f,k}^{I}>0.04$. The probability of the $k$-th concept
    being associated with the filter $f$ is given as $P_{f,k}={\textrm{mean}}_{I:\textrm{with
    k-th concept}}{\bf 1}(IoU_{f,k}^{I}>0.04)$. Thus, we can use $P_{f,k}$ to evaluate
    the filter interpretability of $f$.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器$f$与特定语义概念之间的兼容性通过交并比得分$IoU_{f,k}^{I}\!=\!\frac{\|S_{f}^{I}\cap S_{k}^{I}\|}{\|S_{f}^{I}\cup
    S_{k}^{I}\|}$来报告，其中$S_{k}^{I}$表示图像$I$上第$k$个语义概念的真实掩码。给定图像$I$，如果$IoU_{f,k}^{I}>0.04$，则过滤器$f$与第$k$个概念相关联。第$k$个概念与过滤器$f$相关联的概率为$P_{f,k}={\textrm{mean}}_{I:\textrm{with
    k-th concept}}{\bf 1}(IoU_{f,k}^{I}>0.04)$。因此，我们可以使用$P_{f,k}$来评估$f$的过滤器可解释性。
- en: 6.2 Location instability
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 位置不稳定性
- en: '![Refer to caption](img/86969223247d7971c08fbfaf1eead8d5.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/86969223247d7971c08fbfaf1eead8d5.png)'
- en: 'Figure 12: Notation for the computation of a filter’s location instability Zhang
    et al. ([2018a](#bib.bib43)).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：计算过滤器位置不稳定性的符号说明 Zhang等人（[2018a](#bib.bib43)）。
- en: Another evaluation metric is location instability. This metric is proposed by
    Zhang et al. ([2018a](#bib.bib43)) to evaluate the fitness between a CNN filter
    and the representation of an object part. Given an input image $I$, the CNN computes
    a feature map $x\in\mathbb{R}^{N\times N}$ of filter $f$. We can regard the unit
    $x_{i,j}$ ($1\leq i,j\leq N$) with the highest activation as the location inference
    of $f$, where $N\times N$ is referred to as the size of the feature map. We use
    $\hat{\bf p}$ to denote the image position that corresponds to the inferred feature
    map location $(i,j)$, *i.e.* the center of the unit $x_{i,j}$’s receptive field
    when we backward propagated the receptive field to the image plane. The evaluation
    assumes that if $f$ consistently represented the same object part (the object
    part may not have an explicit name according to people’s cognition) through different
    objects, then distances between the image position $\hat{\bf p}$ and some object
    landmarks should not change much among different objects. For example, if filter
    $f$ represents the shoulder, then the distance between the shoulder and the head
    should remain stable through different objects.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个评估指标是位置不稳定性。这个指标由Zhang等人（[2018a](#bib.bib43)）提出，用于评估CNN过滤器与物体部分表示之间的适配度。给定输入图像$I$，CNN计算过滤器$f$的特征图$x\in\mathbb{R}^{N\times
    N}$。我们可以将激活值最高的单位$x_{i,j}$（$1\leq i,j\leq N$）视为$f$的位置信息，其中$N\times N$表示特征图的大小。我们用$\hat{\bf
    p}$表示与推断出的特征图位置$(i,j)$对应的图像位置，*即*当我们将感受野向图像平面反向传播时，单位$x_{i,j}$的感受野中心。评估假设如果$f$在不同物体上始终表示相同的物体部分（该物体部分可能没有明确名称），那么图像位置$\hat{\bf
    p}$与一些物体地标之间的距离在不同物体中应该不会变化太大。例如，如果过滤器$f$表示肩膀，那么肩膀与头部之间的距离在不同物体中应该保持稳定。
- en: 'Therefore, people can compute the deviation of the distance between the inferred
    position $\hat{\bf p}$ and a specific ground-truth landmark among different images.
    The average deviation *w.r.t.* various landmarks can be used to evaluate the location
    instability of $f$. As shown in Fig. [12](#S6.F12 "Figure 12 ‣ 6.2 Location instability
    ‣ 6 Evaluation metrics for network interpretability ‣ Visual Interpretability
    for Deep Learning: a Survey"), let $d_{I}({\bf p}_{k},\hat{\bf p})=\frac{\|{\bf
    p}_{k}-\hat{\bf p}\|}{\sqrt{w^{2}+h^{2}}}$ denote the normalized distance between
    the inferred part and the $k$-th landmark ${\bf p}_{k}$ on image $I$. $\sqrt{w^{2}+h^{2}}$
    denotes the diagonal length of the input image. Thus, $D_{f,k}=\sqrt{{\textrm{var}}_{I}[d_{I}({\bf
    p}_{k},\hat{\bf p})]}$ is reported as the relative location deviation of filter
    $f$ *w.r.t.* the $k$-th landmark, where ${\textrm{var}}_{I}[d_{I}({\bf p}_{k},\hat{\bf
    p})]$ is referred to as the variation of the distance $d_{I}({\bf p}_{k},\hat{\bf
    p})$. Because each landmark cannot appear in all testing images, for each filter
    $f$, the metric only uses inference results with the top-$M$ highest activation
    scores on images containing the $k$-th landmark to compute $D_{f,k}$. In this
    way, the average of relative location deviations of all the filters in a conv-layer
    *w.r.t.* all landmarks, *i.e.* ${\textrm{mean}}_{f}{\textrm{mean}}_{k=1}^{K}D_{f,k}$,
    measures the location instability of a CNN, where $K$ denotes the number of landmarks.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，人们可以计算推断位置 $\hat{\bf p}$ 与不同图像中的特定地面真实标志之间的距离偏差。相对于各种标志的平均偏差 *w.r.t.* 可以用来评估
    $f$ 的位置不稳定性。如图[12](#S6.F12 "图 12 ‣ 6.2 位置不稳定性 ‣ 6 网络可解释性的评估指标 ‣ 深度学习的视觉可解释性：综述")所示，设
    $d_{I}({\bf p}_{k},\hat{\bf p})=\frac{\|{\bf p}_{k}-\hat{\bf p}\|}{\sqrt{w^{2}+h^{2}}}$
    表示推断部分与图像 $I$ 上第 $k$ 个标志 ${\bf p}_{k}$ 之间的归一化距离。$\sqrt{w^{2}+h^{2}}$ 表示输入图像的对角线长度。因此，$D_{f,k}=\sqrt{{\textrm{var}}_{I}[d_{I}({\bf
    p}_{k},\hat{\bf p})]}$ 被报告为滤波器 $f$ 相对于第 $k$ 个标志的相对位置偏差，其中 ${\textrm{var}}_{I}[d_{I}({\bf
    p}_{k},\hat{\bf p})]$ 被称为距离 $d_{I}({\bf p}_{k},\hat{\bf p})$ 的变化量。由于每个标志不能出现在所有测试图像中，对于每个滤波器
    $f$，该指标仅使用在包含第 $k$ 个标志的图像上激活分数最高的前 $M$ 个推断结果来计算 $D_{f,k}$。通过这种方式，卷积层中所有滤波器相对于所有标志的相对位置偏差的平均值，即
    ${\textrm{mean}}_{f}{\textrm{mean}}_{k=1}^{K}D_{f,k}$，测量 CNN 的位置不稳定性，其中 $K$ 表示标志的数量。
- en: 7 Network interpretability for middle-to-end learning
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7 网络可解释性：从中到端学习
- en: 'Based on studies discussed in Sections [4](#S4 "4 Disentangling CNN representations
    into explanatory graphs & decision trees ‣ Visual Interpretability for Deep Learning:
    a Survey") and [5](#S5 "5 Learning neural networks with interpretable/disentangled
    representations ‣ Visual Interpretability for Deep Learning: a Survey"), people
    may either disentangle representations of a pre-trained CNN or learn a new network
    with interpretable, disentangled representations. Such interpretable/disentangled
    network representations can further enable middle-to-end model learning at the
    semantic level without strong supervision. We briefly review two typical studies Zhang
    et al. ([2017a](#bib.bib40), [b](#bib.bib41)) of middle-to-end learning as follows.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 根据在第[4](#S4 "4 将 CNN 表示解构为解释性图形和决策树 ‣ 深度学习的视觉可解释性：综述")和第[5](#S5 "5 使用可解释/解构表示学习神经网络
    ‣ 深度学习的视觉可解释性：综述")节中讨论的研究，人们可以解构预训练 CNN 的表示，或者学习一个具有可解释、解构表示的新网络。这种可解释/解构的网络表示可以进一步支持从中到端的模型学习，在没有强监督的情况下达到语义层级。我们简要回顾了两个典型的从中到端学习的研究，Zhang
    等人 ([2017a](#bib.bib40), [b](#bib.bib41)) 如下。
- en: '![Refer to caption](img/2d418e65d11b5645365efaefcc17cfd7.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/2d418e65d11b5645365efaefcc17cfd7.png)'
- en: 'Figure 13: And-Or graph grown on a pre-trained CNN as a semantic branch Zhang
    et al. ([2017a](#bib.bib40)). The AOG associates specific CNN units with certain
    image regions. The red lines indicate the parse graph.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：在预训练 CNN 上生成的 And-Or 图作为语义分支 Zhang 等人 ([2017a](#bib.bib40))。AOG 将特定 CNN
    单元与某些图像区域关联。红色线条表示解析图。
- en: 7.1 Active question-answering for learning And-Or graphs
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1 主动问答用于学习 And-Or 图
- en: Based on the semantic And-Or representation proposed in Zhang et al. ([2016](#bib.bib39)),
    Zhang et al. ([2017a](#bib.bib40)) has developed a method to use active question-answering
    to semanticize neural patterns in conv-layers of a pre-trained CNN and build a
    model for hierarchical object understanding.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基于 Zhang 等人 ([2016](#bib.bib39)) 提出的语义 And-Or 表示，Zhang 等人 ([2017a](#bib.bib40))
    开发了一种方法，使用主动问答将预训练 CNN 的卷积层中的神经模式语义化，并建立一个用于层次化对象理解的模型。
- en: 'As shown in Fig. [13](#S7.F13 "Figure 13 ‣ 7 Network interpretability for middle-to-end
    learning ‣ Visual Interpretability for Deep Learning: a Survey"), the CNN is pre-trained
    for object classification. The method aims to extract a four-layer interpretable
    And-Or graph (AOG) to explain the semantic hierarchy hidden in a CNN. The AOG
    encodes four-layer semantics, ranging across the semantic part (OR node), part
    templates (AND nodes), latent patterns (OR nodes), and neural units (terminal
    nodes) on feature maps. In the AOG, AND nodes represent compositional regions
    of a part, and OR nodes encode a list of alternative template/deformation candidates
    for a local part. The top part node (OR node) uses its children to represent some
    template candidates for the part. Each part template (AND node) in the second
    layer uses children latent patterns to represent its constituent regions. Each
    latent pattern in the third layer (OR node) naturally corresponds to a certain
    range of units within the feature map of a filter. The latent pattern selects
    a unit within this range to account for its geometric deformation.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [13](#S7.F13 "图 13 ‣ 7 网络解释性从中间到终端学习 ‣ 深度学习的视觉解释性：综述") 所示，CNN 是针对对象分类进行预训练的。该方法旨在提取一个四层的可解释的
    And-Or 图（AOG），以解释隐藏在 CNN 中的语义层次结构。AOG 编码了四层语义，涵盖了语义部件（OR 节点）、部件模板（AND 节点）、潜在模式（OR
    节点）和特征图上的神经单元（终端节点）。在 AOG 中，AND 节点表示部件的组成区域，而 OR 节点编码了局部部件的替代模板/变形候选项列表。顶部部件节点（OR
    节点）使用其子节点表示一些部件的模板候选项。第二层中的每个部件模板（AND 节点）使用子节点潜在模式来表示其组成区域。第三层中的每个潜在模式（OR 节点）自然地对应于滤波器特征图中的某个范围的单元。潜在模式选择该范围内的一个单元，以考虑其几何变形。
- en: '![Refer to caption](img/9827bb769e9f944386dabbfcecaee160.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/9827bb769e9f944386dabbfcecaee160.png)'
- en: 'Figure 14: Illustration of the QA process Zhang et al. ([2017a](#bib.bib40)).
    (top) The method sorts and selects unexplained objects. (bottom) Questions for
    each target object.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图14：QA过程的示意图 Zhang 等人 ([2017a](#bib.bib40))。 （上）该方法对未解释的对象进行排序和选择。 （下）每个目标对象的问题。
- en: '![Refer to caption](img/bae488ede897586e633a7390dc9d7388.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![参考说明](img/bae488ede897586e633a7390dc9d7388.png)'
- en: 'Figure 15: Part localization performance on the Pascal VOC Part dataset Zhang
    et al. ([2017a](#bib.bib40)).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图15：Pascal VOC Part 数据集上的部件定位性能 Zhang 等人 ([2017a](#bib.bib40))。
- en: 'To learn an AOG, Zhang et al. ([2017a](#bib.bib40)) allows the computer to
    actively identify and ask about objects, whose neural patterns cannot be explained
    by the current AOG. As shown in Fig. [15](#S7.F15 "Figure 15 ‣ 7.1 Active question-answering
    for learning And-Or graphs ‣ 7 Network interpretability for middle-to-end learning
    ‣ Visual Interpretability for Deep Learning: a Survey"), in each step of the active
    question-answering, the current AOG is used to localize object parts among all
    the unannotated images. The method actively selects objects that cannot well fit
    the AOG, namely unexplained objects. The method predicts the potential gain of
    asking about each unexplained object, and thus determines the best sequence of
    questions (*e.g.* asking about template types and bounding boxes of unexplained
    object parts). In this way, the method uses the answers to either refine an existing
    part template or mine latent patterns for new object-part templates, to grow AOG
    branches. Fig. [15](#S7.F15 "Figure 15 ‣ 7.1 Active question-answering for learning
    And-Or graphs ‣ 7 Network interpretability for middle-to-end learning ‣ Visual
    Interpretability for Deep Learning: a Survey") compares the part-localization
    performance of different methods. The QA-based learning exhibits significantly
    higher efficiency than other baselines. The proposed method uses about 1/6–1/3
    of the part annotations for training, but achieves similar or better part-localization
    performance than fast-RCNN methods.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '为了学习AOG，张等人（[2017a](#bib.bib40)）允许计算机主动识别和询问那些当前AOG无法解释的物体的神经模式。如图[15](#S7.F15
    "Figure 15 ‣ 7.1 Active question-answering for learning And-Or graphs ‣ 7 Network
    interpretability for middle-to-end learning ‣ Visual Interpretability for Deep
    Learning: a Survey")所示，在每一步主动问答中，当前AOG用于在所有未注释的图像中定位物体部件。该方法主动选择那些无法很好适应AOG的物体，即无法解释的物体。该方法预测询问每个无法解释物体的潜在收益，从而确定最佳的提问顺序（*例如*
    询问无法解释物体部件的模板类型和边界框）。通过这种方式，该方法利用答案来修正现有的部件模板或挖掘新物体部件模板的潜在模式，以扩展AOG分支。图[15](#S7.F15
    "Figure 15 ‣ 7.1 Active question-answering for learning And-Or graphs ‣ 7 Network
    interpretability for middle-to-end learning ‣ Visual Interpretability for Deep
    Learning: a Survey")比较了不同方法的部件定位性能。基于QA的学习展示了比其他基准显著更高的效率。所提出的方法使用了大约1/6到1/3的部件注释进行训练，但达到了与fast-RCNN方法相似或更好的部件定位性能。'
- en: 7.2 Interactive manipulations of CNN patterns
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2 CNN模式的互动操控
- en: '![Refer to caption](img/c12a1436ac4bbfa58944030ace9db2ea.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/c12a1436ac4bbfa58944030ace9db2ea.png)'
- en: 'Figure 16: Visualization of patterns for the head part before and after human
    interactions Zhang et al. ([2017b](#bib.bib41)).'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：人机交互前后头部部件模式的可视化 张等人（[2017b](#bib.bib41)）。
- en: Let a CNN be pre-trained using annotations of object bounding boxes for object
    classification. Zhang et al. ([2017b](#bib.bib41)) has explored an interactive
    method to diagnose knowledge representations of a CNN, in order to transfer CNN
    patterns to model object parts. Unlike traditional end-to-end learning of CNNs
    that requires numerous training samples, this method mines object part patterns
    from the CNN in the scenario of one/multi-shot learning.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让CNN通过物体边界框的注释进行预训练以进行物体分类。张等人（[2017b](#bib.bib41)）探索了一种互动方法来诊断CNN的知识表示，以便将CNN模式转移到模型物体部件上。与传统的端到端CNN学习需要大量训练样本不同，该方法在单次/多次学习的场景中从CNN中挖掘物体部件模式。
- en: 'More specifically, the method uses part annotations on very few (e.g. three)
    object images for supervision. Given a bounding-box annotation of a part, the
    proposed method first uses Zhang et al. ([2016](#bib.bib39)) to mine latent patterns,
    which are related to the annotated part, from conv-layers of the CNN. An AOG is
    used to organize all mined patterns as the representation of the target part.
    The method visualizes the mined latent patterns and asks people to remove latent
    patterns unrelated to the target part interactively. In this way, people can simply
    prune incorrect latent patterns from AOG branches to refine the AOG. Fig. [16](#S7.F16
    "Figure 16 ‣ 7.2 Interactive manipulations of CNN patterns ‣ 7 Network interpretability
    for middle-to-end learning ‣ Visual Interpretability for Deep Learning: a Survey")
    visualizes initially mined patterns and the remaining patterns after human interaction.
    With the guidance of human interactions, Zhang et al. ([2017b](#bib.bib41)) has
    exhibited superior performance of part localization.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，该方法使用极少的（例如三张）对象图像上的部分注释进行监督。给定部分的边界框注释，所提出的方法首先使用 Zhang 等人（[2016](#bib.bib39)）从
    CNN 的卷积层中挖掘与注释部分相关的潜在模式。使用 AOG 组织所有挖掘出的模式作为目标部分的表示。该方法可视化挖掘出的潜在模式，并要求人们互动地去除与目标部分无关的潜在模式。通过这种方式，人们可以简单地从
    AOG 分支中修剪不正确的潜在模式以细化 AOG。图 [16](#S7.F16 "图 16 ‣ 7.2 CNN 模式的交互操作 ‣ 7 中间到结束学习的网络可解释性
    ‣ 深度学习的可视化解释：综述") 可视化了最初挖掘的模式和人类交互后的剩余模式。在人类交互的指导下，Zhang 等人（[2017b](#bib.bib41)）展示了出色的部分定位性能。
- en: 8 Prospective trends and conclusions
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8 个前景趋势和结论
- en: In this paper, we have reviewed several research directions within the scope
    of network interpretability. Visualization of a neural unit’s patterns was the
    starting point of understanding network representations in the early years. Then,
    people gradually developed methods to analyze feature spaces of neural networks
    and diagnose potential representation flaws hidden inside neural networks. At
    present, disentangling chaotic representations of conv-layers into graphical models
    and/or symbolic logic has become an emerging research direction to open the black-box
    of neural networks. The approach for transforming a pre-trained CNN into an explanatory
    graph has been proposed and has exhibited significant efficiency in knowledge
    transfer and weakly-supervised learning.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本文中，我们回顾了网络可解释性的几个研究方向。神经单元模式的可视化是早期理解网络表示的起点。随后，人们逐渐发展了分析神经网络特征空间的方法，并诊断隐藏在神经网络内部的潜在表示缺陷。目前，将卷积层混乱表示解开为图形模型和/或符号逻辑已成为一种新兴的研究方向，旨在揭开神经网络的黑箱。已提出将预训练的
    CNN 转换为解释性图的方式，并在知识迁移和弱监督学习中展示了显著的效率。
- en: End-to-end learning interpretable neural networks, whose intermediate layers
    encode comprehensible patterns, is also a prospective trend. Interpretable CNNs
    have been developed, where each filter in high conv-layers represents a specific
    object part.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端学习的可解释神经网络，其中间层编码了可理解的模式，也是一个前景趋势。已经开发出可解释的 CNN，其中高卷积层中的每个滤波器表示特定的对象部分。
- en: Furthermore, based on interpretable representations of CNN patterns, semantic-level
    middle-to-end learning has been proposed to speed up the learning process. Compared
    to traditional end-to-end learning, middle-to-end learning allows human interactions
    to guide the learning process and can be applied with very few annotations for
    supervision.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，基于 CNN 模式的可解释表示，已经提出了语义层级的中间到结束学习方法，以加快学习过程。与传统的端到端学习相比，中间到结束学习允许人类交互引导学习过程，并且可以在仅有很少注释的情况下进行监督。
- en: In the future, we believe the middle-to-end learning will continuously be a
    fundamental research direction. In addition, based on the semantic hierarchy of
    an interpretable network, debugging CNN representations at the semantic level
    will create new visual applications.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 未来，我们相信中间到结束的学习将持续成为一个基础研究方向。此外，基于可解释网络的语义层级，在语义层面上调试 CNN 表示将创造新的视觉应用。
- en: Acknowledgement
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 致谢
- en: This work is supported by ONR MURI project N00014-16-1-2007 and DARPA XAI Award
    N66001-17-2-4029, and NSF IIS 1423305.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 本研究得到了 ONR MURI 项目 N00014-16-1-2007 和 DARPA XAI 奖 N66001-17-2-4029，以及 NSF IIS
    1423305 的资助支持。
- en: References
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: Aubry and Russell [2015] Mathieu Aubry and Bryan C. Russell. Understanding deep
    features with computer-generated imagery. In ICCV, 2015.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aubry 和 Russell [2015] Mathieu Aubry 和 Bryan C. Russell。通过计算机生成的图像理解深度特征。在 ICCV，2015。
- en: 'Aubry et al. [2014] M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic.
    Seeing 3d chairs: Exemplar part-based 2d-3d alignment using a large dataset of
    cad models. In CVPR, 2014.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aubry 等人 [2014] M. Aubry、D. Maturana、A. Efros、B. Russell 和 J. Sivic。观察 3D 椅子：利用大型
    CAD 模型数据集进行示例部分基础的 2D-3D 对齐。在 CVPR，2014。
- en: 'Bau et al. [2017] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio
    Torralba. Network dissection: Quantifying interpretability of deep visual representations.
    In CVPR, 2017.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bau 等人 [2017] David Bau、Bolei Zhou、Aditya Khosla、Aude Oliva 和 Antonio Torralba。网络剖析：量化深度视觉表示的可解释性。在
    CVPR，2017。
- en: 'Chen et al. [2016] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever,
    and Pieter Abbeel. Infogan: Interpretable representation learning by information
    maximizing generative adversarial nets. In NIPS, 2016.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Chen 等人 [2016] Xi Chen、Yan Duan、Rein Houthooft、John Schulman、Ilya Sutskever
    和 Pieter Abbeel。InfoGAN：通过信息最大化生成对抗网络进行可解释表示学习。在 NIPS，2016。
- en: Dosovitskiy and Brox [2016] Alexey Dosovitskiy and Thomas Brox. Inverting visual
    representations with convolutional networks. In CVPR, 2016.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dosovitskiy 和 Brox [2016] Alexey Dosovitskiy 和 Thomas Brox。利用卷积网络逆转视觉表示。在 CVPR，2016。
- en: Fong and Vedaldi [2017] Ruth C. Fong and Andrea Vedaldi. Interpretable explanations
    of black boxes by meaningful perturbation. In ICCV, 2017.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fong 和 Vedaldi [2017] Ruth C. Fong 和 Andrea Vedaldi。通过有意义的扰动解释黑箱模型。在 ICCV，2017。
- en: 'Goyal et al. [2016] Yash Goyal, Akrit Mohapatra, Devi Parikh, and Dhruv Batra.
    Towards transparent ai systems: Interpreting visual question answering models.
    In arXiv:1608.08974, 2016.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goyal 等人 [2016] Yash Goyal、Akrit Mohapatra、Devi Parikh 和 Dhruv Batra。迈向透明的 AI
    系统：解释视觉问答模型。在 arXiv:1608.08974，2016。
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In CVPR, 2016.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He 等人 [2016] Kaiming He、Xiangyu Zhang、Shaoqing Ren 和 Jian Sun。用于图像识别的深度残差学习。在
    CVPR，2016。
- en: Hu et al. [2016] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric P.
    Xing. Harnessing deep neural networks with logic rules. In ACL, 2016.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hu 等人 [2016] Zhiting Hu、Xuezhe Ma、Zhengzhong Liu、Eduard Hovy 和 Eric P. Xing。利用逻辑规则驾驭深度神经网络。在
    ACL，2016。
- en: Huang et al. [2017] Gao Huang, Zhuang Liu, Kilian Q. Weinberger, and Laurens
    van der Maaten. Densely connected convolutional networks. In CVPR, 2017.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Huang 等人 [2017] Gao Huang、Zhuang Liu、Kilian Q. Weinberger 和 Laurens van der
    Maaten。密集连接卷积网络。在 CVPR，2017。
- en: 'Kindermans et al. [2017] Pieter-Jan Kindermans, Kristof T. Schütt, Maximilian
    Alber, Klaus-Robert Müller, Dumitru Erhan, Been Kim, and Sven Dähne. Learning
    how to explain neural networks: Patternnet and patternattribution. arXiv: 1705.05598,
    2017.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Kindermans 等人 [2017] Pieter-Jan Kindermans、Kristof T. Schütt、Maximilian Alber、Klaus-Robert
    Müller、Dumitru Erhan、Been Kim 和 Sven Dähne。学习如何解释神经网络：PatternNet 和 PatternAttribution。在
    arXiv: 1705.05598，2017。'
- en: Koh and Liang [2017] PangWei Koh and Percy Liang. Understanding black-box predictions
    via influence functions. In ICML, 2017.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Koh 和 Liang [2017] PangWei Koh 和 Percy Liang。通过影响函数理解黑箱预测。在 ICML，2017。
- en: Krizhevsky et al. [2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
    classification with deep convolutional neural networks. In NIPS, 2012.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky 等人 [2012] A. Krizhevsky、I. Sutskever 和 G. E. Hinton。使用深度卷积神经网络进行
    ImageNet 分类。在 NIPS，2012。
- en: 'Kumar et al. [2017] Devinder Kumar, Alexander Wong, and Graham W. Taylor. Explaining
    the unexplained: A class-enhanced attentive response (clear) approach to understanding
    deep neural networks. In CVPR Workshop on Explainable Computer Vision and Job
    Candidate Screening Competition, 2017.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kumar 等人 [2017] Devinder Kumar、Alexander Wong 和 Graham W. Taylor。解释未解释的：一种基于类别增强的注意响应（CLEAR）方法来理解深度神经网络。在
    CVPR 解释性计算机视觉和求职候选人筛选竞赛工作坊，2017。
- en: 'Lakkaraju et al. [2017] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Eric
    Horvitz. Identifying unknown unknowns in the open world: Representations and policies
    for guided exploration. In AAAI, 2017.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lakkaraju 等人 [2017] Himabindu Lakkaraju、Ece Kamar、Rich Caruana 和 Eric Horvitz。识别开放世界中的未知未知：用于引导探索的表示和策略。在
    AAAI，2017。
- en: LeCun et al. [1998a] Yann LeCun, Lèon Bottou, Yoshua Bengio, and Patrick Haffner.
    Gradient-based learning applied to document recognition. In Proceedings of the
    IEEE, 1998.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 [1998a] Yann LeCun、Lèon Bottou、Yoshua Bengio 和 Patrick Haffner。应用于文档识别的基于梯度的学习。在
    IEEE 会议录，1998。
- en: LeCun et al. [1998b] Yann LeCun, Corinna Cortes, and Christopher JC Burges.
    The mnist database of handwritten digits. Technical report, 1998.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeCun 等人 [1998b] Yann LeCun、Corinna Cortes 和 Christopher JC Burges。MNIST 手写数字数据库。技术报告，1998。
- en: Liu et al. [2015] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep
    learning face attributes in the wild. In ICCV, 2015.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Liu 等人 [2015] Ziwei Liu、Ping Luo、Xiaogang Wang 和 Xiaoou Tang。深度学习在实际环境中的人脸属性。在
    ICCV，2015年。
- en: Lu [2015] Yao Lu. Unsupervised learning on neural network outputs. In arXiv:1506.00990v9,
    2015.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lu [2015] Yao Lu。神经网络输出的无监督学习。在 arXiv:1506.00990v9，2015年。
- en: Mahendran and Vedaldi [2015] Aravindh Mahendran and Andrea Vedaldi. Understanding
    deep image representations by inverting them. In CVPR, 2015.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Mahendran 和 Vedaldi [2015] Aravindh Mahendran 和 Andrea Vedaldi。通过反转理解深度图像表示。在
    CVPR，2015年。
- en: Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco,
    Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature
    learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,
    2011.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Netzer 等人 [2011] Yuval Netzer、Tao Wang、Adam Coates、Alessandro Bissacco、Bo Wu
    和 Andrew Y. Ng。通过无监督特征学习读取自然图像中的数字。在 NIPS 深度学习和无监督特征学习研讨会，2011年。
- en: 'Nguyen et al. [2017] Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy,
    and Jason Yosinski. Plug & play generative networks: Conditional iterative generation
    of images in latent space. CVPR, 2017.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nguyen 等人 [2017] Anh Nguyen、Jeff Clune、Yoshua Bengio、Alexey Dosovitskiy 和 Jason
    Yosinski。即插即用生成网络：在潜在空间中条件性迭代生成图像。CVPR，2017年。
- en: Olah et al. [2017] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature
    visualization. Distill, 2017. https://distill.pub/2017/feature-visualization.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Olah 等人 [2017] Chris Olah、Alexander Mordvintsev 和 Ludwig Schubert。特征可视化。Distill，2017年。https://distill.pub/2017/feature-visualization。
- en: Paysan et al. [2009] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter.
    A 3d face model for pose and illumination invariant face recognition. In AVSS,
    2009.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Paysan 等人 [2009] P. Paysan、R. Knothe、B. Amberg、S. Romdhani 和 T. Vetter。一个用于姿态和光照不变人脸识别的
    3D 人脸模型。在 AVSS，2009年。
- en: Ribeiro et al. [2016] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    “why should i trust you?” explaining the predictions of any classifier. In KDD,
    2016.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ribeiro 等人 [2016] Marco Tulio Ribeiro、Sameer Singh 和 Carlos Guestrin。“我为什么应该信任你？”解释任何分类器的预测。在
    KDD，2016年。
- en: Sabour et al. [2017] Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Dynamic
    routing between capsules. In NIPS, 2017.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sabour 等人 [2017] Sara Sabour、Nicholas Frosst 和 Geoffrey E. Hinton。胶囊之间的动态路由。在
    NIPS，2017年。
- en: 'Selvaraju et al. [2017] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek
    Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations
    from deep networks via gradient-based localization. In ICCV, 2017.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Selvaraju 等人 [2017] Ramprasaath R. Selvaraju、Michael Cogswell、Abhishek Das、Ramakrishna
    Vedantam、Devi Parikh 和 Dhruv Batra。Grad-CAM：通过基于梯度的定位从深度网络中获得视觉解释。在 ICCV，2017年。
- en: 'Simonyan et al. [2013] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
    Deep inside convolutional networks: visualising image classification models and
    saliency maps. In arXiv:1312.6034, 2013.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan 等人 [2013] Karen Simonyan、Andrea Vedaldi 和 Andrew Zisserman。深入卷积网络：可视化图像分类模型和显著性图。在
    arXiv:1312.6034，2013年。
- en: 'Springenberg et al. [2015] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas
    Brox, and Martin Riedmiller. Striving for simplicity: the all convolutional net.
    ICLR workshop, 2015.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Springenberg 等人 [2015] Jost Tobias Springenberg、Alexey Dosovitskiy、Thomas Brox
    和 Martin Riedmiller。追求简洁：全卷积网络。ICLR 研讨会，2015年。
- en: Su et al. [2017] Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi.
    One pixel attack for fooling deep neural networks. In arXiv:1710.08864, 2017.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Su 等人 [2017] Jiawei Su、Danilo Vasconcellos Vargas 和 Sakurai Kouichi。通过一个像素攻击来欺骗深度神经网络。在
    arXiv:1710.08864，2017年。
- en: Szegedy et al. [2014] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of
    neural networks. In arXiv:1312.6199, 2014.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy 等人 [2014] Christian Szegedy、Wojciech Zaremba、Ilya Sutskever、Joan Bruna、Dumitru
    Erhan、Ian Goodfellow 和 Rob Fergus。神经网络的有趣特性。在 arXiv:1312.6199，2014年。
- en: 'Wang et al. [2017] Peng Wang, Qi Wu, Chunhua Shen, and Anton van den Hengel.
    The vqa-machine: Learning how to use existing vision algorithms to answer new
    questions. In CVPR, 2017.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wang 等人 [2017] Peng Wang、Qi Wu、Chunhua Shen 和 Anton van den Hengel。VQA-Machine：学习如何利用现有视觉算法回答新问题。在
    CVPR，2017年。
- en: Wu and Zhu [2011] Tianfu Wu and Song-Chun Zhu. A numerical study of the bottom-up
    and top-down inference processes in and-or graphs. International journal of computer
    vision, 93(2):226–252, 2011.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 和 Zhu [2011] Tianfu Wu 和 Song-Chun Zhu。底向上和顶向下推理过程的数值研究。在《国际计算机视觉杂志》，93(2):226–252，2011年。
- en: Wu et al. [2007] Tian-Fu Wu, Gui-Song Xia, and Song-Chun Zhu. Compositional
    boosting for computing hierarchical image structures. In CVPR, 2007.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2007] Tian-Fu Wu、Gui-Song Xia 和 Song-Chun Zhu。用于计算层次图像结构的组合增强。在 CVPR，2007年。
- en: Wu et al. [2017] Tianfu Wu, Xilai Li, Xi Song, Wei Sun, Liang Dong, and Bo Li.
    Interpretable r-cnn. In arXiv:1711.05226, 2017.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wu 等人 [2017] Tianfu Wu, Xilai Li, Xi Song, Wei Sun, Liang Dong, 和 Bo Li. 可解释的
    R-CNN。在 arXiv:1711.05226, 2017。
- en: Yang et al. [2009] Xiong Yang, Tianfu Wu, and Song-Chun Zhu. Evaluating information
    contributions of bottom-up and top-down processes. ICCV, 2009.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yang 等人 [2009] Xiong Yang, Tianfu Wu, 和 Song-Chun Zhu. 评估自下而上和自上而下过程的信息贡献。ICCV,
    2009。
- en: Yosinski et al. [2014] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
    How transferable are features in deep neural networks? In NIPS, 2014.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Yosinski 等人 [2014] Jason Yosinski, Jeff Clune, Yoshua Bengio, 和 Hod Lipson.
    深度神经网络中的特征可转移性如何？在 NIPS, 2014。
- en: Zeiler and Fergus [2014] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding
    convolutional networks. In ECCV, 2014.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zeiler 和 Fergus [2014] Matthew D. Zeiler 和 Rob Fergus. 可视化和理解卷积网络。在 ECCV, 2014。
- en: Zhang et al. [2016] Q. Zhang, R. Cao, Y. N. Wu, and S.-C. Zhu. Growing interpretable
    part graphs on convnets via multi-shot learning. In AAAI, 2016.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2016] Q. Zhang, R. Cao, Y. N. Wu, 和 S.-C. Zhu. 通过多次学习在卷积网络上生成可解释的部分图。在
    AAAI, 2016。
- en: Zhang et al. [2017a] Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun
    Zhu. Mining object parts from cnns via active question-answering. In CVPR, 2017.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2017a] Quanshi Zhang, Ruiming Cao, Ying Nian Wu, 和 Song-Chun Zhu.
    通过主动问答挖掘 CNN 中的物体部件。在 CVPR, 2017。
- en: Zhang et al. [2017b] Quanshi Zhang, Ruiming Cao, Shengming Zhang, Mark Edmonds,
    Ying Nian Wu, and Song-Chun Zhu. Interactively transferring cnn patterns for part
    localization. In arXiv:1708.01783, 2017.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2017b] Quanshi Zhang, Ruiming Cao, Shengming Zhang, Mark Edmonds,
    Ying Nian Wu, 和 Song-Chun Zhu. 交互式转移 CNN 模式以进行部分定位。在 arXiv:1708.01783, 2017。
- en: Zhang et al. [2017c] Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable
    convolutional neural network. In arXiv:1710.00935, 2017.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2017c] Quanshi Zhang, Ying Nian Wu, 和 Song-Chun Zhu. 可解释的卷积神经网络。在
    arXiv:1710.00935, 2017。
- en: Zhang et al. [2018a] Q. Zhang, R. Cao, F. Shi, Y.N. Wu, and S.-C. Zhu. Interpreting
    cnn knowledge via an explanatory graph. In AAAI, 2018.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2018a] Q. Zhang, R. Cao, F. Shi, Y.N. Wu, 和 S.-C. Zhu. 通过解释性图解释 CNN
    知识。在 AAAI, 2018。
- en: Zhang et al. [2018b] Q. Zhang, W. Wang, and S.-C. Zhu. Examining cnn representations
    with respect to dataset bias. In AAAI, 2018.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2018b] Q. Zhang, W. Wang, 和 S.-C. Zhu. 检查 CNN 表示与数据集偏见的关系。在 AAAI,
    2018。
- en: Zhang et al. [2018c] Quanshi Zhang, Yu Yang, Ying Nian Wu, and Song-Chun Zhu.
    Interpreting cnns via decision trees. arXiv:1802.00121, 2018.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhang 等人 [2018c] Quanshi Zhang, Yu Yang, Ying Nian Wu, 和 Song-Chun Zhu. 通过决策树解释
    CNN。在 arXiv:1802.00121, 2018。
- en: Zhou et al. [2015] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. Object detectors emerge in deep scene cnns. In ICRL, 2015.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zhou 等人 [2015] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, 和 Antonio
    Torralba. 物体检测器在深度场景 CNN 中出现。在 ICRL, 2015。
- en: 'Zintgraf et al. [2017] Luisa M Zintgraf, Taco S Cohen Tameem Adel, and Max
    Welling. Visualizing deep neural network decisions: prediction difference analysis.
    ICLR, 2017.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Zintgraf 等人 [2017] Luisa M Zintgraf, Taco S Cohen, Tameem Adel, 和 Max Welling.
    可视化深度神经网络决策：预测差异分析。在 ICLR, 2017。
