- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:08:13'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[1802.00614] Visual Interpretability for Deep Learning: a Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/1802.00614](https://ar5iv.labs.arxiv.org/html/1802.00614)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Visual Interpretability for Deep Learning: a Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Quanshi Zhang and Song-Chun Zhu
  prefs: []
  type: TYPE_NORMAL
- en: University of California, Los Angeles
  prefs: []
  type: TYPE_NORMAL
- en: Abstract
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This paper reviews recent studies in understanding neural-network representations
    and learning neural networks with interpretable/disentangled middle-layer representations.
    Although deep neural networks have exhibited superior performance in various tasks,
    the interpretability is always the Achilles’ heel of deep neural networks. At
    present, deep neural networks obtain high discrimination power at the cost of
    low interpretability of their black-box representations. We believe that high
    model interpretability may help people to break several bottlenecks of deep learning,
    *e.g.* learning from very few annotations, learning via human-computer communications
    at the semantic level, and semantically debugging network representations. We
    focus on convolutional neural networks (CNNs), and we revisit the visualization
    of CNN representations, methods of diagnosing representations of pre-trained CNNs,
    approaches for disentangling pre-trained CNN representations, learning of CNNs
    with disentangled representations, and middle-to-end learning based on model interpretability.
    Finally, we discuss prospective trends in explainable artificial intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: 1 Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Convolutional neural networks (CNNs) LeCun et al. ([1998a](#bib.bib16)); Krizhevsky
    et al. ([2012](#bib.bib13)); He et al. ([2016](#bib.bib8)); Huang et al. ([2017](#bib.bib10))
    have achieved superior performance in many visual tasks, such as object classification
    and detection. However, the end-to-end learning strategy makes CNN representations
    a black box. Except for the final network output, it is difficult for people to
    understand the logic of CNN predictions hidden inside the network. In recent years,
    a growing number of researchers have realized that high model interpretability
    is of significant value in both theory and practice and have developed models
    with interpretable knowledge representations.
  prefs: []
  type: TYPE_NORMAL
- en: In this paper, we conduct a survey of current studies in understanding neural-network
    representations and learning neural networks with interpretable/disentangled representations.
    We can roughly define the scope of the review into the following six research
    directions.
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Visualization of CNN representations in intermediate network layers. These
    methods mainly synthesize the image that maximizes the score of a given unit in
    a pre-trained CNN or invert feature maps of a conv-layer back to the input image.
    Please see Section [2](#S2 "2 Visualization of CNN representations ‣ Visual Interpretability
    for Deep Learning: a Survey") for detailed discussion.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Diagnosis of CNN representations. Related studies may either diagnose a CNN’s
    feature space for different object categories or discover potential representation
    flaws in conv-layers. Please see Section [3](#S3 "3 Diagnosis of CNN representations
    ‣ Visual Interpretability for Deep Learning: a Survey") for details.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Disentanglement of “the mixture of patterns” encoded in each filter of CNNs.
    These studies mainly disentangle complex representations in conv-layers and transform
    network representations into interpretable graphs. Please see Section [4](#S4
    "4 Disentangling CNN representations into explanatory graphs & decision trees
    ‣ Visual Interpretability for Deep Learning: a Survey") for details.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building explainable models. We discuss interpretable CNNs Zhang et al. ([2017c](#bib.bib42)),
    capsule networks Sabour et al. ([2017](#bib.bib26)), interpretable R-CNNs Wu et
    al. ([2017](#bib.bib35)), and the InfoGAN Chen et al. ([2016](#bib.bib4)) in Section [5](#S5
    "5 Learning neural networks with interpretable/disentangled representations ‣
    Visual Interpretability for Deep Learning: a Survey").'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Semantic-level middle-to-end learning via human-computer interaction. A clear
    semantic disentanglement of CNN representations may further enable “middle-to-end”
    learning of neural networks with weak supervision. Section [7](#S7 "7 Network
    interpretability for middle-to-end learning ‣ Visual Interpretability for Deep
    Learning: a Survey") introduces methods to learn new models via human-computer
    interactions Zhang et al. ([2017b](#bib.bib41)) and active question-answering
    with very limited human supervision Zhang et al. ([2017a](#bib.bib40)).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Among all the above, the visualization of CNN representations is the most direct
    way to explore network representations. The network visualization also provides
    a technical foundation for many approaches to diagnosing CNN representations.
    The disentanglement of feature representations of a pre-trained CNN and the learning
    of explainable network representations present bigger challenges to state-of-the-art
    algorithms. Finally, explainable or disentangled network representations are also
    the starting point for weakly-supervised middle-to-end learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Values of model interpretability: The clear semantics in high conv-layers can
    help people trust a network’s prediction. As discussed in Zhang et al. ([2018b](#bib.bib44)),
    considering dataset and representation bias, a high accuracy on testing images
    still cannot ensure that a CNN will encode correct representations. For example,
    a CNN may use an unreliable context—eye features—to identify the “lipstick” attribute
    of a face image. Therefore, people usually cannot fully trust a network unless
    a CNN can semantically or visually explain its logic, *e.g.* what patterns are
    used for prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the middle-to-end learning or debugging of neural networks based
    on the explainable or disentangled network representations may also significantly
    reduce the requirement for human annotation. Furthermore, based on semantic representations
    of networks, it is possible to merge multiple CNNs into a universal network (*i.e.*
    a network encoding generic knowledge representations for different tasks) at the
    semantic level in the future.
  prefs: []
  type: TYPE_NORMAL
- en: In the following sections, we review the above research directions and discuss
    the potential future of technical developments.
  prefs: []
  type: TYPE_NORMAL
- en: 2 Visualization of CNN representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Visualization of filters in a CNN is the most direct way of exploring visual
    patterns hidden inside a neural unit. Different types of visualization methods
    have been developed for network visualization.
  prefs: []
  type: TYPE_NORMAL
- en: First, gradient-based methods Zeiler and Fergus ([2014](#bib.bib38)); Mahendran
    and Vedaldi ([2015](#bib.bib20)); Simonyan et al. ([2013](#bib.bib28)); Springenberg
    et al. ([2015](#bib.bib29)) are the mainstream of network visualization. These
    methods mainly compute gradients of the score of a given CNN unit *w.r.t.* the
    input image. They use the gradients to estimate the image appearance that maximizes
    the unit score. Olah et al. ([2017](#bib.bib23)) has provided a toolbox of existing
    techniques to visualize patterns encoded in different conv-layers of a pre-trained
    CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Second, the up-convolutional net Dosovitskiy and Brox ([2016](#bib.bib5)) is
    another typical technique to visualize CNN representations. The up-convolutional
    net inverts CNN feature maps to images. We can regard up-convolutional nets as
    a tool that indirectly illustrates the image appearance corresponding to a feature
    map, although compared to gradient-based methods, up-convolutional nets cannot
    mathematically ensure that the visualization result exactly reflects actual representations
    in the CNN. Similarly, Nguyen et al. ([2017](#bib.bib22)) has further introduced
    an additional prior, which controls the semantic meaning of the synthesized image,
    to the adversarial generative network. We can use CNN feature maps as the prior
    for visualization.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Zhou et al. ([2015](#bib.bib46)) has proposed a method to accurately
    compute the image-resolution receptive field of neural activations in a feature
    map. The actual receptive field of neural activation is smaller than the theoretical
    receptive field computed using the filter size. The accurate estimation of the
    receptive field helps people to understand the representation of a filter.
  prefs: []
  type: TYPE_NORMAL
- en: 3 Diagnosis of CNN representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some methods go beyond the visualization of CNNs and diagnose CNN representations
    to obtain insight understanding of features encoded in a CNN. We roughly divide
    all relevant research into the following five directions.
  prefs: []
  type: TYPE_NORMAL
- en: Studies in the first direction analyze CNN features from a global view. Szegedy
    et al. ([2014](#bib.bib31)) has explored semantic meanings of each filter. Yosinski
    et al. ([2014](#bib.bib37)) has analyzed the transferability of filter representations
    in intermediate conv-layers. Lu ([2015](#bib.bib19)); Aubry and Russell ([2015](#bib.bib1))
    have computed feature distributions of different categories/attributes in the
    feature space of a pre-trained CNN.
  prefs: []
  type: TYPE_NORMAL
- en: The second research direction extracts image regions that directly contribute
    the network output for a label/attribute to explain CNN representations of the
    label/attribute. This is similar to the visualization of CNNs. Methods of Fong
    and Vedaldi ([2017](#bib.bib6)); Selvaraju et al. ([2017](#bib.bib27)) have been
    proposed to propagate gradients of feature maps *w.r.t.* the final loss back to
    the image plane to estimate the image regions. The LIME model proposed in Ribeiro
    et al. ([2016](#bib.bib25)) extracts image regions that are highly sensitive to
    the network output. Studies of Zintgraf et al. ([2017](#bib.bib47)); Kindermans
    et al. ([2017](#bib.bib11)); Kumar et al. ([2017](#bib.bib14)) have invented methods
    to visualize areas in the input image that contribute the most to the decision-making
    process of the CNN. Wang et al. ([2017](#bib.bib32)); Goyal et al. ([2016](#bib.bib7))
    have tried to interpret the logic for visual question-answering encoded in neural
    networks. These studies list important objects (or regions of interests) detected
    from the images and crucial words in questions as the explanation of output answers.
  prefs: []
  type: TYPE_NORMAL
- en: The estimation of vulnerable points in the feature space of a CNN is also a
    popular direction for diagnosing network representations. Approaches of Su et
    al. ([2017](#bib.bib30)); Koh and Liang ([2017](#bib.bib12)); Szegedy et al. ([2014](#bib.bib31))
    have been developed to compute adversarial samples for a CNN. *I.e.* these studies
    aim to estimate the minimum noisy perturbation of the input image that can change
    the final prediction. In particular, influence functions proposed in Koh and Liang
    ([2017](#bib.bib12)) can be used to compute adversarial samples. The influence
    function can also provide plausible ways to create training samples to attack
    the learning of CNNs, fix the training set, and further debug representations
    of a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: The fourth research direction is to refine network representations based on
    the analysis of network feature spaces. Given a CNN pre-trained for object classification,
    Lakkaraju et al. ([2017](#bib.bib15)) has proposed a method to discover knowledge
    blind spots (unknown patterns) of the CNN in a weakly-supervised manner. This
    method grouped all sample points in the entire feature space of a CNN into thousands
    of pseudo-categories. It assumed that a well learned CNN would use the sub-space
    of each pseudo-category to exclusively represent a subset of a specific object
    class. In this way, this study randomly showed object samples within each sub-space,
    and used the sample purity in the sub-space to discover potential representation
    flaws hidden in a pre-trained CNN. To distill representations of a teacher network
    to a student network for sentiment analysis, Hu et al. ([2016](#bib.bib9)) has
    proposed using logic rules of natural languages (*e.g.* I-ORG cannot follow B-PER)
    to construct a distillation loss to supervise the knowledge distillation of neural
    networks, in order to obtain more meaningful network representations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/830baac3c887bd8677bf3cad496d53bc.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 1: Biased representations in a CNN Zhang et al. ([2018b](#bib.bib44)).
    Considering potential dataset bias, a high accuracy on testing images cannot always
    ensure that a CNN learns correct representations. The CNN may use unreliable co-appearing
    contexts to make predictions. For example, people may manually modify mouth appearances
    of two faces by masking mouth regions or pasting another mouth, but such modifications
    do not significantly change prediction scores for the lipstick attribute. This
    figure shows heat maps of inference patterns of the lipstick attribute, where
    patterns with red/blue colors are positive/negative with the attribute score.
    The CNN mistakenly considers unrelated patterns as contexts to infer the lipstick.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, Zhang et al. ([2018b](#bib.bib44)) has presented a method to discover
    potential, biased representations of a CNN. Fig. [1](#S3.F1 "Figure 1 ‣ 3 Diagnosis
    of CNN representations ‣ Visual Interpretability for Deep Learning: a Survey")
    shows biased representations of a CNN trained for the estimation of face attributes.
    When an attribute usually co-appears with specific visual features in training
    images, then the CNN may use such co-appearing features to represent the attribute.
    When the used co-appearing features are not semantically related to the target
    attribute, these features can be considered as biased representations.'
  prefs: []
  type: TYPE_NORMAL
- en: Given a pre-trained CNN (*e.g.* a CNN that was trained to estimate face attributes),
    Zhang et al. ([2018b](#bib.bib44)) required people to annotate some ground-truth
    relationships between attributes, *e.g.* the lipstick attribute is positively
    related to the heavy-makeup attribute, and is not related to the black hair attribute.
    Then, the method mined inference patterns of each attribute output from conv-layers,
    and used inference patterns to compute actual attribute relationships encoded
    in the CNN. Conflicts between the ground-truth and the mined attribute relationships
    indicated biased representations.
  prefs: []
  type: TYPE_NORMAL
- en: 4 Disentangling CNN representations into explanatory graphs & decision trees
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 4.1 Disentangling CNN representations into explanatory graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Compared to the visualization and diagnosis of network representations in previous
    sections, disentangling CNN features into human-interpretable graphical representations
    (namely explanatory graphs) provides a more thorough explanation of network representations.
    Zhang et al. ([2018a](#bib.bib43), [2016](#bib.bib39)) have proposed disentangling
    features in conv-layers of a pre-trained CNN and have used a graphical model to
    represent the semantic hierarchy hidden inside a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bc7739ae466c4915888f48986a585a26.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2: Feature maps of a filter obtained using different input images Zhang
    et al. ([2018a](#bib.bib43)). To visualize the feature map, the method propagates
    receptive fields of activated units in the feature map back to the image plane.
    In each sub-feature, the filter is activated by various part patterns in an image.
    This makes it difficult to understand the semantic meaning of a filter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [2](#S4.F2 "Figure 2 ‣ 4.1 Disentangling CNN representations
    into explanatory graphs ‣ 4 Disentangling CNN representations into explanatory
    graphs & decision trees ‣ Visual Interpretability for Deep Learning: a Survey"),
    each filter in a high conv-layer of a CNN usually represents a mixture of patterns.
    For example, the filter may be activated by both the head and the tail parts of
    an object. Thus, to provide a global view of how visual knowledge is organized
    in a pre-trained CNN, studies of Zhang et al. ([2018a](#bib.bib43), [2016](#bib.bib39))
    aim to answer the following three questions.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How many types of visual patterns are memorized by each convolutional filter
    of the CNN (here, a visual pattern may describe a specific object part or a certain
    texture)?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Which patterns are co-activated to describe an object part?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What is the spatial relationship between two co-activated patterns?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9f2c3c849a74a34f4c73f28017bf8acd.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 3: Explanatory graph Zhang et al. ([2018a](#bib.bib43)). An explanatory
    graph represents the knowledge hierarchy hidden in conv-layers of a CNN. Each
    filter in a pre-trained CNN may be activated by different object parts. Zhang
    et al. ([2018a](#bib.bib43)) disentangles part patterns from each filter in an
    unsupervised manner, thereby clarifying the knowledge representation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/d9a09de685a6b155715b3d7932cbcac3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4: Image patches corresponding to different nodes in the explanatory
    graph Zhang et al. ([2018a](#bib.bib43)).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/adde9a7d5c31ee64bed790466f23b815.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 5: Heat maps of patterns Zhang et al. ([2018a](#bib.bib43)). A heat
    map visualizes the spatial distribution of the top 50% patterns in the $L$-th
    layer of the explanatory graph with the highest inference scores.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f2b12ca93f5f63bb95f6eff92e0a6097.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6: Image regions inferred by each node in an explanatory graph Zhang
    et al. ([2018a](#bib.bib43)). The method of Zhang et al. ([2018a](#bib.bib43))
    successfully disentangles object-part patterns from representations of every single
    filter.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [3](#S4.F3 "Figure 3 ‣ 4.1 Disentangling CNN representations
    into explanatory graphs ‣ 4 Disentangling CNN representations into explanatory
    graphs & decision trees ‣ Visual Interpretability for Deep Learning: a Survey"),
    the explanatory graph explains the knowledge semantic hidden inside the CNN. The
    explanatory graph disentangles the mixture of part patterns in each filter’s feature
    map of a conv-layer, and uses each graph node to represent a part.'
  prefs: []
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The explanatory graph has multiple layers. Each graph layer corresponds to a
    specific conv-layer of a CNN.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each filter in a conv-layer may represent the appearance of different object
    parts. The algorithm automatically disentangles the mixture of part patterns encoded
    in a single filter, and uses a node in the explanatory graph to represent each
    part pattern.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each node in the explanatory graph consistently represents the same object part
    through different images. We can use the node to localize the corresponding part
    on the input image. To some extent, the node is robust to shape deformation and
    pose variations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each edge encodes the co-activation relationship and the spatial relationship
    between two nodes in adjacent layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can regard an explanatory graph as a compression of feature maps of conv-layers.
    A CNN has multiple conv-layers. Each conv-layer may have hundreds of filters,
    and each filter may produce a feature map with hundreds of neural units. We can
    use tens of thousands of nodes in the explanatory graph to represent information
    contained in all tens of millions of neural units in these feature maps, *i.e.*
    by which part patterns the feature maps are activated, and where the part patterns
    are localized in input images.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just like a dictionary, each input image can only trigger a small subset of
    part patterns (nodes) in the explanatory graph. Each node describes a common part
    pattern with high transferability, which is shared by hundreds or thousands of
    training images.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Fig. [4](#S4.F4 "Figure 4 ‣ 4.1 Disentangling CNN representations into explanatory
    graphs ‣ 4 Disentangling CNN representations into explanatory graphs & decision
    trees ‣ Visual Interpretability for Deep Learning: a Survey") lists top-ranked
    image patches corresponding to different nodes in the explanatory graph. Fig. [5](#S4.F5
    "Figure 5 ‣ 4.1 Disentangling CNN representations into explanatory graphs ‣ 4
    Disentangling CNN representations into explanatory graphs & decision trees ‣ Visual
    Interpretability for Deep Learning: a Survey") visualizes the spatial distribution
    of object parts inferred by the top 50% nodes in the $L$-th layer of the explanatory
    graph with the highest inference scores. Fig. [6](#S4.F6 "Figure 6 ‣ 4.1 Disentangling
    CNN representations into explanatory graphs ‣ 4 Disentangling CNN representations
    into explanatory graphs & decision trees ‣ Visual Interpretability for Deep Learning:
    a Survey") shows object parts inferred by a single node.'
  prefs: []
  type: TYPE_NORMAL
- en: '4.1.1 Application: multi-shot part localization'
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: There are many potential applications based on the explanatory graph. For example,
    we can regard the explanatory graph as a visual dictionary of a category and transfer
    graph nodes to other applications, such as multi-shot part localization.
  prefs: []
  type: TYPE_NORMAL
- en: Given very few bounding boxes of an object part, Zhang et al. ([2018a](#bib.bib43))
    has proposed retrieving hundreds of nodes that are related to the part annotations
    from the explanatory graph, and then use the retrieved nodes to localize object
    parts in previously unseen images. Because each node in the explanatory graph
    encodes a part pattern shared by numerous training images, the retrieved nodes
    describe a general appearance of the target part without being over-fitted to
    the limited annotations of part bounding boxes. Given three annotations for each
    object part, the explanatory-graph-based method has exhibited superior performance
    of part localization and has decreased by about 1/3 localization errors *w.r.t.*
    the second-best baseline.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2 Disentangling CNN representations into decision trees
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/af30f70e45dc64b49a84c40439a4e994.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7: Decision tree that explains a CNN prediction at the semantic level Zhang
    et al. ([2018c](#bib.bib45)). A CNN is learned for object classification with
    disentangled representations in the top conv-layer, where each filter represents
    a specific object part. The decision tree encodes various decision modes hidden
    inside fully-connected layers of the CNN in a coarse-to-fine manner. Given an
    input image, the decision tree infers a parse tree (red lines) to quantitatively
    analyze rationales for the CNN prediction, *i.e.* which object parts (or filters)
    are used for prediction and how much an object part (or filter) contributes to
    the prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: Zhang et al. ([2018c](#bib.bib45)) has further proposed a decision tree to encode
    decision modes in fully-connected layers. The decision tree is not designed for
    classification. Instead, the decision tree is used to quantitatively explain the
    logic for each CNN prediction. *I.e.* given an input image, we use the CNN to
    make a prediction. The decision tree tells people which filters in a conv-layer
    are used for the prediction and how much they contribute to the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [7](#S4.F7 "Figure 7 ‣ 4.2 Disentangling CNN representations
    into decision trees ‣ 4 Disentangling CNN representations into explanatory graphs
    & decision trees ‣ Visual Interpretability for Deep Learning: a Survey"), the
    method mines potential decision modes memorized in fully-connected layers. The
    decision tree organizes these potential decision modes in a coarse-to-fine manner.
    Furthermore, this study uses the method of Zhang et al. ([2017c](#bib.bib42))
    to disentangle representations of filters in the top conv-layers, *i.e.* making
    each filter represent a specific object part. In this way, people can use the
    decision tree to explain rationales for each CNN prediction at the semantic level,
    *i.e.* which object parts are used by the CNN to make the prediction.'
  prefs: []
  type: TYPE_NORMAL
- en: 5 Learning neural networks with interpretable/disentangled representations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Almost all methods mentioned in previous sections focus on the understanding
    of a pre-trained network. In this section, we review studies of learning disentangled
    representations of neural networks, where representations in middle layers are
    no longer a black box but have clear semantic meanings. Compared to the understanding
    of pre-trained networks, learning networks with disentangled representations present
    more challenges. Up to now, only a few studies have been published in this direction.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9d6fc1980b877ba36b699c4cd5124d1a.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8: Structures of an ordinary conv-layer and an interpretable conv-layer Zhang
    et al. ([2017c](#bib.bib42)). Green and red lines indicate the forward and backward
    propagations, respectively.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0389877cdbc570b3bd6b0fa75957f360.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9: Templates Zhang et al. ([2017c](#bib.bib42)). Each template $T_{\mu_{i}}$
    matches to a feature map when the target part mainly triggers the $i$-th unit
    in the feature map.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2bf3c342629dd93651da494f33a2bb94.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10: Visualization of interpretable filters in the top conv-layer Zhang
    et al. ([2017c](#bib.bib42)). We used Zhou et al. ([2015](#bib.bib46)) to estimate
    the image-resolution receptive field of activations in a feature map to visualize
    a filter’s semantics. An interpretable CNN usually encodes head patterns of animals
    in its top conv-layer for classification.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.1 Interpretable convolutional neural networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As shown in Fig. [9](#S5.F9 "Figure 9 ‣ 5 Learning neural networks with interpretable/disentangled
    representations ‣ Visual Interpretability for Deep Learning: a Survey"), Zhang
    et al. ([2017c](#bib.bib42)) has developed a method to modify an ordinary CNN
    to obtain disentangled representations in high conv-layers by adding a loss to
    each filter in the conv-layers. The loss is used to regularize the feature map
    towards the representation of a specific object part.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that people do not need to annotate any object parts or textures to supervise
    the learning of interpretable CNNs. Instead, the loss automatically assigns an
    object part to each filter during the end-to-end learning process. As shown in
    Fig. [9](#S5.F9 "Figure 9 ‣ 5 Learning neural networks with interpretable/disentangled
    representations ‣ Visual Interpretability for Deep Learning: a Survey"), this
    method designs some templates. Each template $T_{\mu_{i}}$ is a matrix with the
    same size of feature map. $T_{\mu_{i}}$ describes the ideal distribution of activations
    for the feature map when the target part mainly triggers the $i$-th unit in the
    feature map.'
  prefs: []
  type: TYPE_NORMAL
- en: Given the joint probability of fitting a feature map to a template, the loss
    of a filter is formulated as the mutual information between the feature map and
    the templates. This loss encourages a low entropy of inter-category activations.
    *I.e.* each filter in the conv-layer is assigned to a certain category. If the
    input image belongs to the target category, then the loss expects the filter’s
    feature map to match a template well; otherwise, the filter needs to remain inactivated.
    In addition, the loss also encourages a low entropy of spatial distributions of
    neural activations. *I.e.* when the input image belongs the target category, the
    feature map is supposed to exclusively fit a single template. In other words,
    the filter needs to activate a single location on the feature map.
  prefs: []
  type: TYPE_NORMAL
- en: This study assumes that if a filter repetitively activates various feature-map
    regions, then this filter is more likely to describe low-level textures (*e.g.*
    colors and edges), instead of high-level parts. For example, the left eye and
    the right eye may be represented by different filters, because contexts of the
    two eyes are symmetric, but not the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig.[10](#S5.F10 "Figure 10 ‣ 5 Learning neural networks with interpretable/disentangled
    representations ‣ Visual Interpretability for Deep Learning: a Survey") shows
    feature maps produced by different filters of an interpretable CNN. Each filter
    consistently represents the same object part through various images.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e66ffdf4cb642db622d3bef93b6e56e1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 11: Detection examples of the proposed method Wu et al. ([2017](#bib.bib35)).
    In addition to predicted bounding boxes, the method also outputs the latent parse
    tree and part configurations as the qualitatively extractive rationale in detection.
    The parse trees are inferred on-the-fly in the space of latent structures, which
    follow a top-down compositional grammar of an AOG.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.2 Interpretable R-CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Wu et al. ([2017](#bib.bib35)) has proposed the learning of qualitatively interpretable
    models for object detection based on the R-CNN. The objective is to unfold latent
    configurations of object parts automatically during the object-detection process.
    This method is learned without using any part annotations for supervision. Wu
    et al. ([2017](#bib.bib35)) uses a top-down hierarchical and compositional grammar,
    namely an And-Or graph (AOG), to model latent configurations of object parts.
    This method uses an AOG-based parsing operator to substitute for the RoI-Pooling
    operator used in the R-CNN. The AOG-based parsing harnesses explainable compositional
    structures of objects and maintains the discrimination power of a R-CNN. This
    idea is related to the disentanglement of the local, bottom-up, and top-down information
    components for prediction Wu et al. ([2007](#bib.bib34)); Yang et al. ([2009](#bib.bib36));
    Wu and Zhu ([2011](#bib.bib33)).
  prefs: []
  type: TYPE_NORMAL
- en: During the detection process, a bounding box is interpreted as the best parse
    tree derived from the AOG on-the-fly. During the learning process, a folding-unfolding
    method is used to train the AOG and R-CNN in an end-to-end manner.
  prefs: []
  type: TYPE_NORMAL
- en: 'Fig. [11](#S5.F11 "Figure 11 ‣ 5.1 Interpretable convolutional neural networks
    ‣ 5 Learning neural networks with interpretable/disentangled representations ‣
    Visual Interpretability for Deep Learning: a Survey") illustrates an example of
    object detection. The proposed method detects object bounding boxes. The method
    also determines the latent parse tree and part configurations of objects as the
    qualitatively extractive rationale in detection.'
  prefs: []
  type: TYPE_NORMAL
- en: 5.3 Capsule networks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sabour et al. ([2017](#bib.bib26)) has designed novel neural units, namely capsules,
    in order to substitute for traditional neural units to construct a capsule network.
    Each capsule outputs an activity vector instead of a scalar. The length of the
    activity vector represents the activation strength of the capsule, and the orientation
    of the activity vector encodes instantiation parameters. Active capsules in the
    lower layer send messages to capsules in the adjacent higher layer. This method
    uses an iterative routing-by-agreement mechanism to assign higher weights with
    the low-layer capsules whose outputs better fit the instantiation parameters of
    the high-layer capsule.
  prefs: []
  type: TYPE_NORMAL
- en: Experiments showed that when people trained capsule networks using the MNIST
    dataset LeCun et al. ([1998b](#bib.bib17)), a capsule encoded a specific semantic
    concept. Different dimensions of the activity vector of a capsule controlled different
    features, including 1) scale and thickness, 2) localized part, 3) stroke thickness,
    3) localized skew, and 4) width and translation.
  prefs: []
  type: TYPE_NORMAL
- en: 5.4 Information maximizing generative adversarial nets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The information maximizing generative adversarial net Chen et al. ([2016](#bib.bib4)),
    namely InfoGAN, is an extension of the generative adversarial network. The InfoGAN
    maximizes the mutual information between certain dimensions of the latent representation
    and the image observation. The InfoGAN separates input variables of the generator
    into two types, *i.e.* the incompressible noise $z$ and the latent code $c$. This
    study aims to learn the latent code $c$ to encode certain semantic concepts in
    an unsupervised manner.
  prefs: []
  type: TYPE_NORMAL
- en: The InfoGAN has been trained using the MNIST dataset LeCun et al. ([1998b](#bib.bib17)),
    the CelebA dataset Liu et al. ([2015](#bib.bib18)), the SVHN dataset Netzer et
    al. ([2011](#bib.bib21)), the 3D face dataset Paysan et al. ([2009](#bib.bib24)),
    and the 3D chair dataset Aubry et al. ([2014](#bib.bib2)). Experiments have shown
    that the latent code has successfully encoded the digit type, the rotation, and
    the width of digits in the MNIST dataset, the lighting condition and the plate
    context in the SVHN dataset, the azimuth, the existence of glasses, the hairstyle,
    and the emotion in the CelebA dataset, and the width and 3D rotation in the 3D
    face and chair datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 6 Evaluation metrics for network interpretability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Evaluation metrics for model interpretability are crucial for the development
    of explainable models. This is because unlike traditional well-defined visual
    applications (*e.g.* object detection and segmentation), network interpretability
    is more difficult to define and evaluate. The evaluation metric of network interpretability
    can help people define the concept of network interpretability and guide the development
    of learning interpretable network representations. Up to now, only very few studies
    have discussed the evaluation of network interpretability. Proposing a promising
    evaluation metric is still a big challenge to state-of-the-art algorithms. In
    this section, we simply introduce two latest evaluation metrics for the interpretability
    of CNN filters, *i.e.* the filter interpretability proposed by Bau et al. ([2017](#bib.bib3))
    and the location instability proposed by Zhang et al. ([2018a](#bib.bib43)).
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Filter interpretability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bau et al. ([2017](#bib.bib3)) has defined six types of semantics for CNN filters,
    *i.e.* objects, parts, scenes, textures, materials, and colors. The evaluation
    of filter interpretability requires people to annotate these six types of semantics
    on testing images at the pixel level. The evaluation metric measures the fitness
    between the image-resolution receptive field of a filter’s neural activations¹¹1The
    method propagates the receptive field of each activated unit in a filter’s feature
    map back to the image plane as the image-resolution receptive field of a filter.
    and the pixel-level semantic annotations on the image. For example, if the receptive
    field of a filter’s neural activations usually highly overlaps with ground-truth
    image regions of a specific semantic concept through different images, then we
    can consider that the filter represents this semantic concept.
  prefs: []
  type: TYPE_NORMAL
- en: For each filter $f$, this method computes its feature maps ${\bf X}=\{x=f(I)|I\in{\bf
    I}\}$ on different testing images. Then, the distribution of activation scores
    in all positions of all feature maps is computed. Bau et al. ([2017](#bib.bib3))
    set an activation threshold $T_{f}$ such that $p(x_{ij}>T_{f})=0.005$, to select
    top activations from all spatial locations $[i,j]$ of all feature maps $x\in{\bf
    X}$ as valid map regions corresponding to $f$’s semantics. Then, the method scales
    up low-resolution valid map regions to the image resolution, thereby obtaining
    the receptive field of valid activations on each image. We use $S_{f}^{I}$ to
    denote the receptive field of $f$’s valid activations *w.r.t.* the image $I$.
  prefs: []
  type: TYPE_NORMAL
- en: The compatibility between a filter $f$ and a specific semantic concept is reported
    as an intersection-over-union score $IoU_{f,k}^{I}\!=\!\frac{\|S_{f}^{I}\cap S_{k}^{I}\|}{\|S_{f}^{I}\cup
    S_{k}^{I}\|}$, where $S_{k}^{I}$ denotes the ground-truth mask of the $k$-th semantic
    concept on the image $I$. Given an image $I$, filter $f$ is associated with the
    $k$-th concept if $IoU_{f,k}^{I}>0.04$. The probability of the $k$-th concept
    being associated with the filter $f$ is given as $P_{f,k}={\textrm{mean}}_{I:\textrm{with
    k-th concept}}{\bf 1}(IoU_{f,k}^{I}>0.04)$. Thus, we can use $P_{f,k}$ to evaluate
    the filter interpretability of $f$.
  prefs: []
  type: TYPE_NORMAL
- en: 6.2 Location instability
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/86969223247d7971c08fbfaf1eead8d5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12: Notation for the computation of a filter’s location instability Zhang
    et al. ([2018a](#bib.bib43)).'
  prefs: []
  type: TYPE_NORMAL
- en: Another evaluation metric is location instability. This metric is proposed by
    Zhang et al. ([2018a](#bib.bib43)) to evaluate the fitness between a CNN filter
    and the representation of an object part. Given an input image $I$, the CNN computes
    a feature map $x\in\mathbb{R}^{N\times N}$ of filter $f$. We can regard the unit
    $x_{i,j}$ ($1\leq i,j\leq N$) with the highest activation as the location inference
    of $f$, where $N\times N$ is referred to as the size of the feature map. We use
    $\hat{\bf p}$ to denote the image position that corresponds to the inferred feature
    map location $(i,j)$, *i.e.* the center of the unit $x_{i,j}$’s receptive field
    when we backward propagated the receptive field to the image plane. The evaluation
    assumes that if $f$ consistently represented the same object part (the object
    part may not have an explicit name according to people’s cognition) through different
    objects, then distances between the image position $\hat{\bf p}$ and some object
    landmarks should not change much among different objects. For example, if filter
    $f$ represents the shoulder, then the distance between the shoulder and the head
    should remain stable through different objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, people can compute the deviation of the distance between the inferred
    position $\hat{\bf p}$ and a specific ground-truth landmark among different images.
    The average deviation *w.r.t.* various landmarks can be used to evaluate the location
    instability of $f$. As shown in Fig. [12](#S6.F12 "Figure 12 ‣ 6.2 Location instability
    ‣ 6 Evaluation metrics for network interpretability ‣ Visual Interpretability
    for Deep Learning: a Survey"), let $d_{I}({\bf p}_{k},\hat{\bf p})=\frac{\|{\bf
    p}_{k}-\hat{\bf p}\|}{\sqrt{w^{2}+h^{2}}}$ denote the normalized distance between
    the inferred part and the $k$-th landmark ${\bf p}_{k}$ on image $I$. $\sqrt{w^{2}+h^{2}}$
    denotes the diagonal length of the input image. Thus, $D_{f,k}=\sqrt{{\textrm{var}}_{I}[d_{I}({\bf
    p}_{k},\hat{\bf p})]}$ is reported as the relative location deviation of filter
    $f$ *w.r.t.* the $k$-th landmark, where ${\textrm{var}}_{I}[d_{I}({\bf p}_{k},\hat{\bf
    p})]$ is referred to as the variation of the distance $d_{I}({\bf p}_{k},\hat{\bf
    p})$. Because each landmark cannot appear in all testing images, for each filter
    $f$, the metric only uses inference results with the top-$M$ highest activation
    scores on images containing the $k$-th landmark to compute $D_{f,k}$. In this
    way, the average of relative location deviations of all the filters in a conv-layer
    *w.r.t.* all landmarks, *i.e.* ${\textrm{mean}}_{f}{\textrm{mean}}_{k=1}^{K}D_{f,k}$,
    measures the location instability of a CNN, where $K$ denotes the number of landmarks.'
  prefs: []
  type: TYPE_NORMAL
- en: 7 Network interpretability for middle-to-end learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Based on studies discussed in Sections [4](#S4 "4 Disentangling CNN representations
    into explanatory graphs & decision trees ‣ Visual Interpretability for Deep Learning:
    a Survey") and [5](#S5 "5 Learning neural networks with interpretable/disentangled
    representations ‣ Visual Interpretability for Deep Learning: a Survey"), people
    may either disentangle representations of a pre-trained CNN or learn a new network
    with interpretable, disentangled representations. Such interpretable/disentangled
    network representations can further enable middle-to-end model learning at the
    semantic level without strong supervision. We briefly review two typical studies Zhang
    et al. ([2017a](#bib.bib40), [b](#bib.bib41)) of middle-to-end learning as follows.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/2d418e65d11b5645365efaefcc17cfd7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13: And-Or graph grown on a pre-trained CNN as a semantic branch Zhang
    et al. ([2017a](#bib.bib40)). The AOG associates specific CNN units with certain
    image regions. The red lines indicate the parse graph.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.1 Active question-answering for learning And-Or graphs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Based on the semantic And-Or representation proposed in Zhang et al. ([2016](#bib.bib39)),
    Zhang et al. ([2017a](#bib.bib40)) has developed a method to use active question-answering
    to semanticize neural patterns in conv-layers of a pre-trained CNN and build a
    model for hierarchical object understanding.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in Fig. [13](#S7.F13 "Figure 13 ‣ 7 Network interpretability for middle-to-end
    learning ‣ Visual Interpretability for Deep Learning: a Survey"), the CNN is pre-trained
    for object classification. The method aims to extract a four-layer interpretable
    And-Or graph (AOG) to explain the semantic hierarchy hidden in a CNN. The AOG
    encodes four-layer semantics, ranging across the semantic part (OR node), part
    templates (AND nodes), latent patterns (OR nodes), and neural units (terminal
    nodes) on feature maps. In the AOG, AND nodes represent compositional regions
    of a part, and OR nodes encode a list of alternative template/deformation candidates
    for a local part. The top part node (OR node) uses its children to represent some
    template candidates for the part. Each part template (AND node) in the second
    layer uses children latent patterns to represent its constituent regions. Each
    latent pattern in the third layer (OR node) naturally corresponds to a certain
    range of units within the feature map of a filter. The latent pattern selects
    a unit within this range to account for its geometric deformation.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/9827bb769e9f944386dabbfcecaee160.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14: Illustration of the QA process Zhang et al. ([2017a](#bib.bib40)).
    (top) The method sorts and selects unexplained objects. (bottom) Questions for
    each target object.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/bae488ede897586e633a7390dc9d7388.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 15: Part localization performance on the Pascal VOC Part dataset Zhang
    et al. ([2017a](#bib.bib40)).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To learn an AOG, Zhang et al. ([2017a](#bib.bib40)) allows the computer to
    actively identify and ask about objects, whose neural patterns cannot be explained
    by the current AOG. As shown in Fig. [15](#S7.F15 "Figure 15 ‣ 7.1 Active question-answering
    for learning And-Or graphs ‣ 7 Network interpretability for middle-to-end learning
    ‣ Visual Interpretability for Deep Learning: a Survey"), in each step of the active
    question-answering, the current AOG is used to localize object parts among all
    the unannotated images. The method actively selects objects that cannot well fit
    the AOG, namely unexplained objects. The method predicts the potential gain of
    asking about each unexplained object, and thus determines the best sequence of
    questions (*e.g.* asking about template types and bounding boxes of unexplained
    object parts). In this way, the method uses the answers to either refine an existing
    part template or mine latent patterns for new object-part templates, to grow AOG
    branches. Fig. [15](#S7.F15 "Figure 15 ‣ 7.1 Active question-answering for learning
    And-Or graphs ‣ 7 Network interpretability for middle-to-end learning ‣ Visual
    Interpretability for Deep Learning: a Survey") compares the part-localization
    performance of different methods. The QA-based learning exhibits significantly
    higher efficiency than other baselines. The proposed method uses about 1/6–1/3
    of the part annotations for training, but achieves similar or better part-localization
    performance than fast-RCNN methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 7.2 Interactive manipulations of CNN patterns
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '![Refer to caption](img/c12a1436ac4bbfa58944030ace9db2ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16: Visualization of patterns for the head part before and after human
    interactions Zhang et al. ([2017b](#bib.bib41)).'
  prefs: []
  type: TYPE_NORMAL
- en: Let a CNN be pre-trained using annotations of object bounding boxes for object
    classification. Zhang et al. ([2017b](#bib.bib41)) has explored an interactive
    method to diagnose knowledge representations of a CNN, in order to transfer CNN
    patterns to model object parts. Unlike traditional end-to-end learning of CNNs
    that requires numerous training samples, this method mines object part patterns
    from the CNN in the scenario of one/multi-shot learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'More specifically, the method uses part annotations on very few (e.g. three)
    object images for supervision. Given a bounding-box annotation of a part, the
    proposed method first uses Zhang et al. ([2016](#bib.bib39)) to mine latent patterns,
    which are related to the annotated part, from conv-layers of the CNN. An AOG is
    used to organize all mined patterns as the representation of the target part.
    The method visualizes the mined latent patterns and asks people to remove latent
    patterns unrelated to the target part interactively. In this way, people can simply
    prune incorrect latent patterns from AOG branches to refine the AOG. Fig. [16](#S7.F16
    "Figure 16 ‣ 7.2 Interactive manipulations of CNN patterns ‣ 7 Network interpretability
    for middle-to-end learning ‣ Visual Interpretability for Deep Learning: a Survey")
    visualizes initially mined patterns and the remaining patterns after human interaction.
    With the guidance of human interactions, Zhang et al. ([2017b](#bib.bib41)) has
    exhibited superior performance of part localization.'
  prefs: []
  type: TYPE_NORMAL
- en: 8 Prospective trends and conclusions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this paper, we have reviewed several research directions within the scope
    of network interpretability. Visualization of a neural unit’s patterns was the
    starting point of understanding network representations in the early years. Then,
    people gradually developed methods to analyze feature spaces of neural networks
    and diagnose potential representation flaws hidden inside neural networks. At
    present, disentangling chaotic representations of conv-layers into graphical models
    and/or symbolic logic has become an emerging research direction to open the black-box
    of neural networks. The approach for transforming a pre-trained CNN into an explanatory
    graph has been proposed and has exhibited significant efficiency in knowledge
    transfer and weakly-supervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: End-to-end learning interpretable neural networks, whose intermediate layers
    encode comprehensible patterns, is also a prospective trend. Interpretable CNNs
    have been developed, where each filter in high conv-layers represents a specific
    object part.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, based on interpretable representations of CNN patterns, semantic-level
    middle-to-end learning has been proposed to speed up the learning process. Compared
    to traditional end-to-end learning, middle-to-end learning allows human interactions
    to guide the learning process and can be applied with very few annotations for
    supervision.
  prefs: []
  type: TYPE_NORMAL
- en: In the future, we believe the middle-to-end learning will continuously be a
    fundamental research direction. In addition, based on the semantic hierarchy of
    an interpretable network, debugging CNN representations at the semantic level
    will create new visual applications.
  prefs: []
  type: TYPE_NORMAL
- en: Acknowledgement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This work is supported by ONR MURI project N00014-16-1-2007 and DARPA XAI Award
    N66001-17-2-4029, and NSF IIS 1423305.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Aubry and Russell [2015] Mathieu Aubry and Bryan C. Russell. Understanding deep
    features with computer-generated imagery. In ICCV, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Aubry et al. [2014] M. Aubry, D. Maturana, A. Efros, B. Russell, and J. Sivic.
    Seeing 3d chairs: Exemplar part-based 2d-3d alignment using a large dataset of
    cad models. In CVPR, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bau et al. [2017] David Bau, Bolei Zhou, Aditya Khosla, Aude Oliva, and Antonio
    Torralba. Network dissection: Quantifying interpretability of deep visual representations.
    In CVPR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Chen et al. [2016] Xi Chen, Yan Duan, Rein Houthooft, John Schulman, Ilya Sutskever,
    and Pieter Abbeel. Infogan: Interpretable representation learning by information
    maximizing generative adversarial nets. In NIPS, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dosovitskiy and Brox [2016] Alexey Dosovitskiy and Thomas Brox. Inverting visual
    representations with convolutional networks. In CVPR, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fong and Vedaldi [2017] Ruth C. Fong and Andrea Vedaldi. Interpretable explanations
    of black boxes by meaningful perturbation. In ICCV, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goyal et al. [2016] Yash Goyal, Akrit Mohapatra, Devi Parikh, and Dhruv Batra.
    Towards transparent ai systems: Interpreting visual question answering models.
    In arXiv:1608.08974, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He et al. [2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep
    residual learning for image recognition. In CVPR, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hu et al. [2016] Zhiting Hu, Xuezhe Ma, Zhengzhong Liu, Eduard Hovy, and Eric P.
    Xing. Harnessing deep neural networks with logic rules. In ACL, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Huang et al. [2017] Gao Huang, Zhuang Liu, Kilian Q. Weinberger, and Laurens
    van der Maaten. Densely connected convolutional networks. In CVPR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kindermans et al. [2017] Pieter-Jan Kindermans, Kristof T. Schütt, Maximilian
    Alber, Klaus-Robert Müller, Dumitru Erhan, Been Kim, and Sven Dähne. Learning
    how to explain neural networks: Patternnet and patternattribution. arXiv: 1705.05598,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Koh and Liang [2017] PangWei Koh and Percy Liang. Understanding black-box predictions
    via influence functions. In ICML, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky et al. [2012] A. Krizhevsky, I. Sutskever, and G. E. Hinton. Imagenet
    classification with deep convolutional neural networks. In NIPS, 2012.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kumar et al. [2017] Devinder Kumar, Alexander Wong, and Graham W. Taylor. Explaining
    the unexplained: A class-enhanced attentive response (clear) approach to understanding
    deep neural networks. In CVPR Workshop on Explainable Computer Vision and Job
    Candidate Screening Competition, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Lakkaraju et al. [2017] Himabindu Lakkaraju, Ece Kamar, Rich Caruana, and Eric
    Horvitz. Identifying unknown unknowns in the open world: Representations and policies
    for guided exploration. In AAAI, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [1998a] Yann LeCun, Lèon Bottou, Yoshua Bengio, and Patrick Haffner.
    Gradient-based learning applied to document recognition. In Proceedings of the
    IEEE, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LeCun et al. [1998b] Yann LeCun, Corinna Cortes, and Christopher JC Burges.
    The mnist database of handwritten digits. Technical report, 1998.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liu et al. [2015] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep
    learning face attributes in the wild. In ICCV, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lu [2015] Yao Lu. Unsupervised learning on neural network outputs. In arXiv:1506.00990v9,
    2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mahendran and Vedaldi [2015] Aravindh Mahendran and Andrea Vedaldi. Understanding
    deep image representations by inverting them. In CVPR, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Netzer et al. [2011] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco,
    Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature
    learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning,
    2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Nguyen et al. [2017] Anh Nguyen, Jeff Clune, Yoshua Bengio, Alexey Dosovitskiy,
    and Jason Yosinski. Plug & play generative networks: Conditional iterative generation
    of images in latent space. CVPR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Olah et al. [2017] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. Feature
    visualization. Distill, 2017. https://distill.pub/2017/feature-visualization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paysan et al. [2009] P. Paysan, R. Knothe, B. Amberg, S. Romdhani, and T. Vetter.
    A 3d face model for pose and illumination invariant face recognition. In AVSS,
    2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ribeiro et al. [2016] Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin.
    “why should i trust you?” explaining the predictions of any classifier. In KDD,
    2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sabour et al. [2017] Sara Sabour, Nicholas Frosst, and Geoffrey E. Hinton. Dynamic
    routing between capsules. In NIPS, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Selvaraju et al. [2017] Ramprasaath R. Selvaraju, Michael Cogswell, Abhishek
    Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations
    from deep networks via gradient-based localization. In ICCV, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Simonyan et al. [2013] Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman.
    Deep inside convolutional networks: visualising image classification models and
    saliency maps. In arXiv:1312.6034, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Springenberg et al. [2015] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas
    Brox, and Martin Riedmiller. Striving for simplicity: the all convolutional net.
    ICLR workshop, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Su et al. [2017] Jiawei Su, Danilo Vasconcellos Vargas, and Sakurai Kouichi.
    One pixel attack for fooling deep neural networks. In arXiv:1710.08864, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Szegedy et al. [2014] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
    Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of
    neural networks. In arXiv:1312.6199, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang et al. [2017] Peng Wang, Qi Wu, Chunhua Shen, and Anton van den Hengel.
    The vqa-machine: Learning how to use existing vision algorithms to answer new
    questions. In CVPR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu and Zhu [2011] Tianfu Wu and Song-Chun Zhu. A numerical study of the bottom-up
    and top-down inference processes in and-or graphs. International journal of computer
    vision, 93(2):226–252, 2011.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2007] Tian-Fu Wu, Gui-Song Xia, and Song-Chun Zhu. Compositional
    boosting for computing hierarchical image structures. In CVPR, 2007.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wu et al. [2017] Tianfu Wu, Xilai Li, Xi Song, Wei Sun, Liang Dong, and Bo Li.
    Interpretable r-cnn. In arXiv:1711.05226, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yang et al. [2009] Xiong Yang, Tianfu Wu, and Song-Chun Zhu. Evaluating information
    contributions of bottom-up and top-down processes. ICCV, 2009.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yosinski et al. [2014] Jason Yosinski, Jeff Clune, Yoshua Bengio, and Hod Lipson.
    How transferable are features in deep neural networks? In NIPS, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zeiler and Fergus [2014] Matthew D. Zeiler and Rob Fergus. Visualizing and understanding
    convolutional networks. In ECCV, 2014.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2016] Q. Zhang, R. Cao, Y. N. Wu, and S.-C. Zhu. Growing interpretable
    part graphs on convnets via multi-shot learning. In AAAI, 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2017a] Quanshi Zhang, Ruiming Cao, Ying Nian Wu, and Song-Chun
    Zhu. Mining object parts from cnns via active question-answering. In CVPR, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2017b] Quanshi Zhang, Ruiming Cao, Shengming Zhang, Mark Edmonds,
    Ying Nian Wu, and Song-Chun Zhu. Interactively transferring cnn patterns for part
    localization. In arXiv:1708.01783, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2017c] Quanshi Zhang, Ying Nian Wu, and Song-Chun Zhu. Interpretable
    convolutional neural network. In arXiv:1710.00935, 2017.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2018a] Q. Zhang, R. Cao, F. Shi, Y.N. Wu, and S.-C. Zhu. Interpreting
    cnn knowledge via an explanatory graph. In AAAI, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2018b] Q. Zhang, W. Wang, and S.-C. Zhu. Examining cnn representations
    with respect to dataset bias. In AAAI, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang et al. [2018c] Quanshi Zhang, Yu Yang, Ying Nian Wu, and Song-Chun Zhu.
    Interpreting cnns via decision trees. arXiv:1802.00121, 2018.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhou et al. [2015] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and
    Antonio Torralba. Object detectors emerge in deep scene cnns. In ICRL, 2015.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Zintgraf et al. [2017] Luisa M Zintgraf, Taco S Cohen Tameem Adel, and Max
    Welling. Visualizing deep neural network decisions: prediction difference analysis.
    ICLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
