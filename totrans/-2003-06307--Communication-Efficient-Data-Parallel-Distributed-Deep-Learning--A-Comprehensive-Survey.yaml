- en: <!--yml
  prefs: []
  type: TYPE_NORMAL
- en: 'category: 未分类'
  prefs: []
  type: TYPE_NORMAL
- en: 'date: 2024-09-06 20:02:04'
  prefs: []
  type: TYPE_NORMAL
- en: -->
  prefs: []
  type: TYPE_NORMAL
- en: '[2003.06307] Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2003.06307](https://ar5iv.labs.arxiv.org/html/2003.06307)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Zhenheng Tang Hong Kong Baptist University [zhtang@comp.hkbu.edu.hk](mailto:zhtang@comp.hkbu.edu.hk)
    ,  Shaohuai Shi Harbin Institute of Technology, Shenzhen [shaohuais@hit.edu.cn](mailto:shaohuais@hit.edu.cn)
    ,  Wei Wang The Hong Kong University of Science and Technology [weiwa@cse.ust.hk](mailto:weiwa@cse.ust.hk)
    ,  Bo Li The Hong Kong University of Science and Technology [bli@cse.ust.hk](mailto:bli@cse.ust.hk)
     and  Xiaowen Chu The Hong Kong University of Science and Technology (Guangzhou)
    [xwchu@ust.hk](mailto:xwchu@ust.hk)
  prefs: []
  type: TYPE_NORMAL
- en: Abstract.
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Distributed deep learning (DL) has become prevalent in recent years to reduce
    training time by leveraging multiple computing devices (e.g., GPUs/TPUs) due to
    larger models and datasets. However, system scalability is limited by communication
    becoming the performance bottleneck. Addressing this communication issue has become
    a prominent research topic. In this paper, we provide a comprehensive survey of
    the communication-efficient distributed training algorithms, focusing on both
    system-level and algorithmic-level optimizations. We first propose a taxonomy
    of data-parallel distributed training algorithms that incorporates four primary
    dimensions: communication synchronization, system architectures, compression techniques,
    and parallelism of communication and computing tasks. We then investigate state-of-the-art
    studies that address problems in these four dimensions. We also compare the convergence
    rates of different algorithms to understand their convergence speed. Additionally,
    we conduct extensive experiments to empirically compare the convergence performance
    of various mainstream distributed training algorithms. Based on our system-level
    communication cost analysis, theoretical and experimental convergence speed comparison,
    we provide readers with an understanding of which algorithms are more efficient
    under specific distributed environments. Our research also extrapolates potential
    directions for further optimizations.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed Deep Learning, Efficient Communication^†^†ccs: Computing methodologies Distributed
    algorithms^†^†ccs: General and reference Surveys and overviews^†^†ccs: Computing
    methodologies Neural networks^†^†ccs: Computing methodologies Parallel algorithms'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Introduction
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning (DL) has made significant progress in recent years. Researchers
    and engineers have applied DL technologies to tackle intricate problems across
    various fields, including but not limited to computer vision ([resnet,](#bib.bib67)
    ), natural language processing ([attention,](#bib.bib213) ; [bert,](#bib.bib46)
    ; [yang2020survey,](#bib.bib237) ), speech recognition ([deepspeech,](#bib.bib9)
    ) and many others. DL typically involves increased sizes of training datasets
    and model parameters in deep neural networks (DNNs) to enhance the predictive
    performance in different applications, such as accuracy in classification tasks ([ml1991,](#bib.bib150)
    ; [Russakovsky2015ILS28465472846559,](#bib.bib157) ; [8237359,](#bib.bib194) ).
    However, as data size and model complexity increase, the training process becomes
    exceedingly computationally intensive and time-consuming. For example, training
    a state-of-the-art ResNet-50 ([resnet,](#bib.bib67) ) model (in 90 epochs) on
    the ImageNet dataset ([Imagenet,](#bib.bib45) ) using a latest Nvidia Tesla V100
    GPU requires approximately two days ([wang2019performance,](#bib.bib224) ). As
    Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey") shows, however, the
    development speed of GPU FLOPs and memory cannot catch up the development of newly
    large neural networks, like GPT-3 ([gpt3,](#bib.bib24) ), GShard ([lepikhin2021gshard,](#bib.bib100)
    ) and Baidu RecSys ([zhao2020distributed,](#bib.bib261) ). In addition, hyper-parameter
    tuning is necessary to achieve satisfactory results for certain tasks, which further
    demands significant time and financial investment.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/0b659eb861584b896afc5a4c91b01a74.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) Memory of GPUs and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/f0e406dcdcc99107a6873c7905029f99.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) FLOPS of GPU and FLOPs of training models.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 1\. The trends of GPU and neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'To mitigate the time-consuming training process, two approaches have gained
    traction: (1) maximizing the utilization of a single accelerator’s computing power
    by implementing highly optimized software ([chetlur2014cudnn,](#bib.bib34) ; [shi2016benchmarking,](#bib.bib178)
    ; [xu2017performance,](#bib.bib233) ; [sze2017efficient,](#bib.bib199) ; [yan2020optimizing,](#bib.bib235)
    ), and (2) employing distributed training ([padam,](#bib.bib35) ; [disml,](#bib.bib230)
    ; [sparknet,](#bib.bib132) ; [PSGD,](#bib.bib263) ) to accelerate the training
    process by making use of multiple processors, including CPUs ([you2018imagenet,](#bib.bib240)
    ), GPUs ([goyal2017accurate,](#bib.bib56) ; [shi2018performance,](#bib.bib175)
    ; [jia2018highly,](#bib.bib81) ) and TPUs ([you2019large,](#bib.bib239) ). Intuitively,
    multiple processors working collaboratively in training one model can reduce the
    overall training time. However, the communication cost between processors typically
    restricts system scalability ([shi2018performance,](#bib.bib175) ). For example,
    when deploying the high-speed computing devices (e.g., Nvidia A100/H100 GPUs)
    with low-speed interconnects (e.g., PCIe or 10GbE) to collaboratively train a
    deep model, where the computation-to-communication ratio is low ([shi2020quantitative,](#bib.bib173)
    ), using multiple processors could result in very low hardware utilization ([DGC,](#bib.bib113)
    ). And the collaborative training between different geographically distributed
    GPUs suffers from higher communication costs, because the wide-area networks have
    lower communication bandwidth ([tang2022gossipfl,](#bib.bib205) ). Therefore,
    different parallel algorithms should be carefully designed to leverage the computing
    power of distributed clusters.'
  prefs: []
  type: TYPE_NORMAL
- en: Data parallelism ([dean2012large,](#bib.bib43) ; [goyal2017accurate,](#bib.bib56)
    ) is a widely employed distributed training technique. This approach involves
    replicating the model parameters to all computing workers. During a single iteration,
    each worker computes the local gradient or model updates by sampling different
    mini-batches of data. Workers then exchanges the results with the other workers.
    Following this, aggregation and broadcast operations are executed to obtain the
    new global model.
  prefs: []
  type: TYPE_NORMAL
- en: Model parallelism ([dean2012large,](#bib.bib43) ; [3154842,](#bib.bib121) ;
    [Mirhoseini2017,](#bib.bib128) ) is another distributed training approach that
    involves partitioning model parameters among multiple computing workers. Every
    worker holds different parameters or layers of the model (i.e., a subset of $x$).
    In this paper, we focus mainly on techniques associated with data parallelism.
    Note that some of them could be used in parallel to model parallelism.
  prefs: []
  type: TYPE_NORMAL
- en: The term of pipeline parallelism ([shi2018adag,](#bib.bib176) ; [zhang2017poseidon,](#bib.bib248)
    ; [MLSYS2022_cedebb6e,](#bib.bib2) ; [harlap2018pipedream,](#bib.bib64) ; [huang2019gpipe,](#bib.bib75)
    ; [shoeybi2019megatron,](#bib.bib183) ; [li2021chimera,](#bib.bib104) ) can refer
    to different techniques under different contexts. In the context of data parallelism,
    pipelining means to execute the computational tasks and communication tasks simultaneously
    so as to reduce the iteration time. In the context of model parallelism, it refers
    to the technique that workers are assigned different stages of model training
    to execute, and intermediate results are transmitted between workers to hide some
    training time. In this paper, we mainly focus on the pipelining techniques deployed
    in data parallelism, which is used to reduce the communication overhead.
  prefs: []
  type: TYPE_NORMAL
- en: This survey provides a comprehensive overview and taxonomy of research works
    that aim to enhance communication efficiency in distributed training. We deconstruct
    the distributed training framework into several orthogonal components, including
    communication synchronization, system architectures, compression techniques and
    parallelism of communication and computation. Additionally, we offer an overview
    of convergence analysis of these algorithms, examining the trade-off between communication
    efficiency and convergence. Furthermore, we develop a benchmark framework of mainstream
    distributed training algorithms, including different synchronization schemes,
    communication topology, compression algorithms, and various numbers of workers,
    providing the practical reference to readers. Through this survey, we hope that
    readers can gain insight into the advancements made in this field and be inspired
    to develop new efficient distributed training algorithms and frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: 1.1\. Related Work
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There exist several surveys providing an introduction and review to distributed
    machine or deep learning algorithms. Peteiro-Barral et al. ([Peteiro2013,](#bib.bib142)
    ) introduced distributed machine learning algorithms for big data. Xing et al. ([XING2016179,](#bib.bib231)
    ) mainly focused on different synchronization schemes, scheduling, and balancing
    workloads and communication typologies. Ben-Nun et al. ([10.1145/3320060,](#bib.bib15)
    ) focused on DNN operators and approaches for parallelism. Guo et al. ([DBLP:abs-1808-04752,](#bib.bib60)
    ) gave a thorough review of different quantized neural networks. Meanwhile, Zhang
    et al. ([8644613,](#bib.bib259) ) provided a brief overview of large-scale distributed
    DL systems, including parallelism, parameter server architectures, synchronization
    schemes, related applications, and platforms.
  prefs: []
  type: TYPE_NORMAL
- en: Our article, in contrast to these previous surveys, concentrates specifically
    on communication-efficient distributed DL training. We present a detailed discussion
    of communication compression techniques that have not been fully demystified in
    the previous surveys. Additionally, we provide a quick review of auxiliary technologies
    and offer a comparison of convergence bounds. Moreover, we conduct extensive experiments
    to compare the performance of different mainstream distributed training algorithms,
    which provides readers with an empirical understanding of these algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 1.2\. Organization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This survey aims to provide a comprehensive analysis of communication-efficient
    distributed training algorithms with data parallelism, focusing on four key aspects:
    communication synchronization, system architectures, compression techniques, and
    scheduling methods. The rest of the paper is organized as follows. In Section
    [2](#S2 "2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey"), we illustrate the key issues
    of distributed training and propose a taxonomy of related research to summarize
    existing methods. We discuss the synchronous and asynchronous frameworks in Section
    [3](#S3 "3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"), followed by an overview
    of the system architectures that support gradient/model aggregation in Section
    [4](#S4 "4\. Centralized/Decentralized framework ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"). In Section [5](#S5
    "5\. Quantization methods ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey") and Section [6](#S6 "6\. Sparsification
    Methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"), we introduce the techniques in reducing the communication traffic with
    gradient/model compression. The scheduling methods are introduced in Section [7](#S7
    "7\. Scheduling of Communication and Computing ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"). We summarize the
    theoretical convergence bounds in Section [8](#S8 "8\. Convergence Analysis ‣
    Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"). Additionally, we present some auxiliary tricks to train deep models
    with communication-efficient algorithms in Section [9](#S9 "9\. Auxiliary Technologies
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"). Finally, we conclude the paper in Section [10](#S10 "10\. Conclusion
    and Future Directions ‣ Communication-Efficient Data Parallel Distributed Deep
    Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: 1.3\. Benchmark Framework and Experiment Configuration
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Our benchmark framework is built upon FedML ([chaoyanghe2020fedml,](#bib.bib66)
    ) and MPI ([mpi4py,](#bib.bib42) ). Our framework provides users with flexible
    and scalable application programming interfaces (APIs) for testing and developing
    novel distributed training algorithms. Multiple algorithms have already been developed
    with distinct synchronous schemes, communication topologies, and compression techniques.
    These algorithms can be combined to create new ones that are mostly independent
    of each other.
  prefs: []
  type: TYPE_NORMAL
- en: We conduct experiments of different distributed training algorithms on two typical
    DL tasks. One is the image classification on CIFAR-10 ([krizhevsky2010cifar,](#bib.bib95)
    ) using ResNet-20 ([resnet,](#bib.bib67) ). The other is Shakespeare, a natural
    language processing task based on the dataset obtained from The Complete Works
    of William Shakespeare. The model used for Shakespeare is a stacked character-level
    LSTM language model, proposed by ([FederatedLearning,](#bib.bib125) ). We conduct
    experiments with different workers ($4\sim 32$) to assess the scalability of different
    algorithms. Each worker is equipped with a NVIDIA RTX 2080 Ti and Pytorch version
    is V1.7\. Note that the hardware platform doesn’t affect the convergence performance
    and model test accuracy. The results of the experiments are presented at the end
    of each chapter, providing the readers with an accurate and practical understanding
    of the algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Taxonomy of Distributed DL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The mathematical formulation of training DL models can be defined as an optimization
    problem
  prefs: []
  type: TYPE_NORMAL
- en: '| (1) |  | $\min\limits_{\mathbf{x}\in\mathbb{R}^{N}}f(\mathbf{x}):=\mathbb{E}_{\xi_{i}\sim\mathcal{D}}F(\mathbf{x};\xi_{i}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the random variable $\xi_{i}$ follows a probability distribution $\mathcal{D}$,
    which denotes the data samples from a given dataset, $\mathbf{x}$ represents all
    the parameters of the model, $N$ is the number of parameters, and $F:\mathbb{R}^{N}\to\mathbb{R}$
    denotes the objective function with respect to $\mathbf{x}$ and $\xi_{i}$, $f$
    the expectation of $F$.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-based optimization algorithms are commonly used in DL. Due to the high
    computational complexity of second-order gradient descent methods ([martens2015optimizing,](#bib.bib120)
    ; [shi2021accelerating,](#bib.bib180) ; [zhang2023eva,](#bib.bib251) ) with DNNs,
    the first-order gradient descent methods, particularly the stochastic gradient
    descent (SGD) with mini-batch¹¹1To simplify, throughout this paper, we use SGD
    to denote the gradient descent with mini-batch which includes the cases of one
    sample and more than one samples in each training iteration. and its variants
    (e.g., Adam), are commonly used. The update rule of single-device SGD is as follows.
  prefs: []
  type: TYPE_NORMAL
- en: '| (2) |  | $G_{t}(\mathbf{x}_{t})={\nabla}F_{t}(\mathbf{x}_{t};\xi_{t})$ |  |'
  prefs: []
  type: TYPE_TB
- en: '| (3) |  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}G_{t}(\mathbf{x}_{t}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathbf{x}_{t}\in\mathbb{R}^{N}$ is the $N$-dimensional model parameter
    at iteration $t$, $\xi_{t}$ is a randomly sampled mini-batch of data, and $\gamma$
    is the learning rate (or step size). SGD is an iterative algorithm and has been
    proved that it can solve ([1](#S2.E1 "In 2\. Taxonomy of Distributed DL ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")) under the assumptions
    that $f_{s}(\mathbf{x})$ is non-convex and is with L-Lipschitzian gradients ([Bottou2016OptimizationMF,](#bib.bib20)
    ). The iterative process generally contains several steps: 1) It samples a mini-batch
    of data $\xi_{t}$. 2) It performs the feed-forward computations to evaluate the
    objective function $F_{t}(\mathbf{x}_{t};\xi_{t})$). 3) It performs backward propagation
    to calculate the gradients, ${\nabla}F_{t}(\mathbf{x}_{t};\xi_{t})$ with respect
    to model parameters. 4) Finally, it updates model parameters by Eq. ([3](#S2.E3
    "In 2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Distributed training modifies the above four basic single-device SGD procedures
    into distributed versions. We firstly describe the procedures of the widely used
    distributed training algorithm, bulk synchronous parallel SGD (BSP-SGD) ([BSP7917379181,](#bib.bib211)
    ), and then elucidate how other algorithms differ from it. In BSP-SGD, each worker
    (indexed as $i$) has an identical global model (e.g.., downloads from the Parameter
    Server (PS)), and then processes a different subset of data (i.e., $\xi_{i,t}$)
    to independently compute gradients (${\nabla}F(\mathbf{x}_{t};\xi_{i,t})$). These
    computations are performed simultaneously among all workers. After computing gradients,
    workers aggregate them through the PS or an all-reduce operation, with a synchronization
    to update the model parameters, and then proceed to the next iteration. The update
    rule of BSP-SGD can be formulated as:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (4) |  | $\displaystyle G_{i,t}(\mathbf{x}_{t})={\nabla}F_{i,t}(\mathbf{x}_{t};\xi_{i,t}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (5) |  | $\displaystyle\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}\frac{1}{n}\sum_{i=1}^{n}G_{i,t}(\mathbf{x}_{t}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where $G_{i,t}(\mathbf{x}_{t})$ represents the gradient of $F_{i,t}(\mathbf{x}_{t})$
    of worker $i$ at iteration $t$, $n$ the number of workers.
  prefs: []
  type: TYPE_NORMAL
- en: 'We summarize and classify algorithms that improve BSP-SGD in four orthogonal
    dimensions shown in Table [1](#S2.T1 "Table 1 ‣ 2\. Taxonomy of Distributed DL
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"). Furthermore, we provide an overview of communication-efficient algorithms
    for distributed DL in Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of Distributed
    DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"), which highlights the main techniques employed in each dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Flexible synchronization aims to relax the strict synchronization constraints
    of BSP to reduce the impact of synchronization and the number of communications
    in the same period (Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of Distributed DL
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey").➀ and Section [3](#S3 "3\. Synchronous/asynchronous framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Different system architectures propose changing the communication topology
    to avoid the communication congestion of the PS and workers. (Fig. [2](#S2.F2
    "Figure 2 ‣ 2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey").➃ and Section [4](#S4 "4\.
    Centralized/Decentralized framework ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compression techniques explore to compress the communication data, thereby
    reduce communication traffic and communication time. (Fig. [2](#S2.F2 "Figure
    2 ‣ 2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey").➁, Section [5](#S5 "5\. Quantization methods
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey") and [6](#S6 "6\. Sparsification Methods ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The parallelism of communication and computing seeks to hide the communication
    time to achieve shorter iteration time. (Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy
    of Distributed DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey").➂ and Section [7](#S7 "7\. Scheduling of Communication
    and Computing ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey"))'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The communication protocols (Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of Distributed
    DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey").➄) and the network topology (Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy
    of Distributed DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey").➅) are also important factors that influence the communication
    efficiency at the hardware-level. We mainly discuss the algorithm-level methods
    and protocols in this survey.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 1\. Taxonomy of Distributed SGD
  prefs: []
  type: TYPE_NORMAL
- en: '| Dimension | Method | Characteristic |'
  prefs: []
  type: TYPE_TB
- en: '| Communication Synchronization | Synchronous | Frequent communications and
    synchronization |'
  prefs: []
  type: TYPE_TB
- en: '| Stale-Synchronous | Trade-off between Synchronous and Asynchronous |'
  prefs: []
  type: TYPE_TB
- en: '| Asynchronous | No need of synchronization |'
  prefs: []
  type: TYPE_TB
- en: '| Local SGD | Less frequent communications |'
  prefs: []
  type: TYPE_TB
- en: '| System Architectures | Parameter-Server | Centralized topology |'
  prefs: []
  type: TYPE_TB
- en: '| All-Reduce | Decentralized topology and collective communication |'
  prefs: []
  type: TYPE_TB
- en: '| Gossip | Decentralized topology and peer-to-peer communication |'
  prefs: []
  type: TYPE_TB
- en: '| Compression Techniques | Quantization | Communicate low-precision parameters
    |'
  prefs: []
  type: TYPE_TB
- en: '| Sparsification | Communicate selected parameters |'
  prefs: []
  type: TYPE_TB
- en: '| Parallelism of Communication and Computing | Pipelining | Hide the communication
    or computation time |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduling | Dynamically schedule the computing and communication tasks |'
  prefs: []
  type: TYPE_TB
- en: '![Refer to caption](img/e07e88b2da1c9d8b6dc0e0445ce9df03.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2\. Overview of data-parallel distributed deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 3\. Synchronous/asynchronous framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the data parallel distributed DL, synchronization in BSP-SGD refers to the
    process in which all workers should be synchronized to complete the transmission
    of all parameters or gradients before proceeding to the next training round. Flexible
    synchronization, which relaxes the strict synchronization of BSP-SGD, affects
    not only communication traffic but also the performance and convergence of model
    training. Therefore, there is a trade-off between communication traffic and convergence.
    Moreover, different synchronization schemes can be combined with different architectures.
    In this section, we describe four representative synchronization schemes under
    the PS architecture, which has the most extensive range of applications. The timeline
    of the four different schemes is shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3\. Synchronous/asynchronous
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/eb078b211869f8c01693086080789688.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3\. Comparison of training with single device and multiple devices under
    different communication synchronization with the PS architecture.
  prefs: []
  type: TYPE_NORMAL
- en: 3.1\. Synchronous Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The classical synchronous framework, namely BSP, as mentioned in section [2](#S2
    "2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey"), involves the following steps: 1) Data
    loading; 2) Feed-forward computations; 3) Backward propagation computations; 4)
    Gradient aggregation with a barrier; and 5) Model updating with aggregated gradients.
    The step (4) requires synchronization of all workers, leading to the straggler
    problem, where few slow workers significantly affect the system throughput ([AsynDisADMM,](#bib.bib253)
    ). Additionally, the aggregation of all gradients leads to high communication
    costs, severely limiting the scalability of the system.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the parameter server (PS) architecture (§[4.1](#S4.SS1 "4.1\. Parameter
    Server ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey")), the synchronous
    barrier is enforced until all workers finish transmitting their parameters to
    the parameter servers ([Krizhevsky2014OneWT,](#bib.bib94) ; [Bradley2011,](#bib.bib23)
    ; [552669,](#bib.bib93) ; [375451,](#bib.bib28) ). Similarly, in the All-Reduce
    architecture (§[4.2](#S4.SS2 "4.2\. All-Reduce ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey")), synchronization requires that all workers wait for the
    completion of the all-reduce operation (i.e., gradient aggregation), ensuring
    that all workers have the same updated global model ([Zou2014MarianaTD,](#bib.bib264)
    ; [642949,](#bib.bib25) ; [DGC,](#bib.bib113) ). In contrast, for the decentralized
    architecture (§[4.3](#S4.SS3 "4.3\. Gossip ‣ 4\. Centralized/Decentralized framework
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")), synchronization involves waiting for the completion of communication.
    Unlike in All-Reduce, workers in the decentralized architecture do not necessarily
    maintain the same model ([CanDecent,](#bib.bib108) ; [Ram2008DistributedSS,](#bib.bib149)
    ).'
  prefs: []
  type: TYPE_NORMAL
- en: 3.2\. Stale-synchronous Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The stale-synchronous parallel (SSP) framework ([MoreEffDMLviaStaleSync,](#bib.bib69)
    ) aims to mitigate the straggler problem with relaxed synchronization. Specifically,
    SSP allows faster workers to perform more updates than slower ones, reducing the
    waiting time of faster workers, as illustrated in Fig. [3](#S3.F3 "Figure 3 ‣
    3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey"). However, to maintain model
    consistency and ensure convergence, SSP imposes a staleness bounded barrier that
    limits the iteration gap between the fastest and slowest workers. For a maximum
    staleness bound $s$, the update formula of worker $i$ at iteration $t+1$ is modified
    to'
  prefs: []
  type: TYPE_NORMAL
- en: '| (6) |  | $\displaystyle\mathbf{x}_{i,t+1}=\mathbf{x}_{0}-{\gamma}\sum_{k=1}^{t-s-1}\sum_{j=1}^{n}G_{j,k}(\mathbf{x}_{j,k})-{\gamma}\sum_{k=t-s}^{t}G_{i,k}(\mathbf{x}_{i,k})-{\gamma}\sum_{(j,k)\in\mathcal{S}_{i,t+1}}G_{j,k}(\mathbf{x}_{j,k}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $\mathcal{S}_{i,t+1}$ is some subset of the updates from other workers
    during period $\left[t-s,t\right]$, and $n$ represents the number of workers.
    The historical updates added to $\mathbf{x}_{i,t+1}$ consist of three terms: pre-window,
    read-my-writes, in-window updates, represented by second, third and forth terms
    in the right side in equation ([6](#S3.E6 "In 3.2\. Stale-synchronous Framework
    ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey")). The pre-window update represents
    the synchronized gradients which are obtained from all workers, the read-my-writes
    update is local gradients, the in-window update is the gradients from other workers.
    Less $s$ means more timely gradient synchronization but less communication efficiency.
    Determining a proper $s$ is challenging to achieve good end-to-end training performance ([MoreEffDMLviaStaleSync,](#bib.bib69)
    ). To this end, instead of setting a staleness bound, Chen et al. ([RevistSynSGD,](#bib.bib31)
    ) proposed the backup worker scheme. Specifically, a subset of workers (called
    backup workers) are used to compute the mini-batch gradient. The PS updates parameters
    without waiting for all gradients. The gradients from slowest workers are dropped
    directly.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, there is a congestion problem in SSP with parameter servers. Chen
    et al. ([8737587,](#bib.bib30) ) proposed a Round-Robin Synchronization Parallel
    (R²SP) method to address it. R²SP staggers worker updates throughout the training
    process and coordinates workers to update gradients in a fixed round-robin order
    to evenly distribute the communication load and reduce congestion.
  prefs: []
  type: TYPE_NORMAL
- en: 3.3\. Asynchronous Framework
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The asynchronous parallel SGD (ASP-SGD) framework allows the server to update
    the global model with the updates from a part of workers instead of all workers ([hogwild,](#bib.bib137)
    ; [muli2013,](#bib.bib118) ; [Tamingwild,](#bib.bib158) ; [AsynDisMLspar,](#bib.bib58)
    ; [NEURIPS2022_029df12a,](#bib.bib129) ). ASP-SGD enables more independent updates
    of the nodes and reduces one-round data transmission during communication between
    the workers and the PS as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3\. Synchronous/asynchronous
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey"). In ASP-SGD, each worker sends its gradients to the PS
    after gradient calculation. Then the PS updates the global model without waiting
    for the other workers. Thus, asynchronous frameworks get rid of straggler problems.
    Note that ASP-SGD is not friendly to the All-Reduce architecture. The update formula
    of ASP-SGD can be summarized as'
  prefs: []
  type: TYPE_NORMAL
- en: '| (7) |  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}\sum_{i=1}^{n}G_{i,t-\tau_{i,k}}(\mathbf{x}_{i,t-\tau_{k,i}}),$
    |  |'
  prefs: []
  type: TYPE_TB
- en: where the $\tau_{k,i}$ represents the period between when worker $i$ calculates
    the gradient and when the server conducts SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed Alternating Direction Method of Multipliers (D-ADMM) ([Dadmm,](#bib.bib133)
    ; [convergeceAsynDisADMM,](#bib.bib226) ) is an early work that proposed asynchronous
    updating on different parameters of an optimization problem. However, it requires
    the maintenance of a global clock, and each group of workers in D-ADMM needs to
    be aware of each other’s progress. Moreover, D-ADMM heavily depends on the network
    topology and requires a central node to keep the global model while the central
    node still needs to wait for all worker nodes to finish their tasks, which is
    akin to the synchronous framework. Moreover,  ([Dadmm,](#bib.bib133) ; [convergeceAsynDisADMM,](#bib.bib226)
    ) do not consider the optimization of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: To address these limitations, Zhang et al.  ([AsynDisADMM,](#bib.bib253) ) proposed
    an asynchronous distributed ADMM using the star topology and the PS architecture.
    Although asynchronous training has been demonstrated to be faster than synchronous
    training in the absence of slow workers, it tend to have inferior convergence
    performance than synchronous optimization. Thus,  ([AsynDisADMM,](#bib.bib253)
    ) employed partial barrier and bounded delay as two conditions to control the
    asynchrony, trying to obtain similar convergence of the synchronous optimization.
  prefs: []
  type: TYPE_NORMAL
- en: 'DistBelief ([dean2012large,](#bib.bib43) ) is an early asynchronous framework
    that is capable of harnessing computing clusters with thousands of machines for
    large-scale model training. It proposes Downpour SGD, an ASP-SGD optimization
    method, which involves multiple workers processing data in parallel to compute
    their own updates and communicating with the PS. Later, Li et al. ([communicationDisMLwithPS,](#bib.bib102)
    ) proposed an efficient algorithm named Delayed Block Proximal Gradient Method.
    In this algorithm, only a block of parameters is asynchronously updated per iteration.
    Consequently, only a portion of the parameters is required to be transmitted between
    the master and workers, and the waiting is not necessary. To further reduce the
    communication costs, Grishchenko et al. ([AsynDisMLspar,](#bib.bib58) ) developed
    an asynchronous distributed algorithm that incorporates sparsification of upward
    communications (workers-to-master). Sparsification (§[6](#S6 "6\. Sparsification
    Methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")) is implemented by uniformly sampling a selection of local update entries
    to enhance communication efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Asynchronous frameworks provide better system performance by addressing straggler
    problems. Nevertheless, asynchronous algorithms suffer from inferior convergence
    performance. Consequently, synchronous SGD remains the state-of-the-art method
    in the data center setting if workers have uniform hardware and workloads ([RevistSynSGD,](#bib.bib31)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: 3.4\. Local SGD
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Local-SGD ([Zhang2014DSM,](#bib.bib246) ; [bijral2016data,](#bib.bib19) ; [Zhang2016ParallelSW,](#bib.bib249)
    ; [NIPS2019_9288,](#bib.bib62) ; [McDonald2010DistributedTS,](#bib.bib123) ; [NIPS2009_3881,](#bib.bib122)
    ; [6853589,](#bib.bib256) ; [Zhang2015DLE296,](#bib.bib254) ; [Yu2018ParallelRS,](#bib.bib243)
    ; [spiridonoff2021communicationefficient,](#bib.bib186) ) is another set of algorithms
    that rely on strict synchronization but permit flexible communication frequencies.
    In Local-SGD, each worker independently executes several or more iterations before
    averaging all local models to obtain the most recent global model. Model Average ([FederatedLearning,](#bib.bib125)
    ) is a similar approach that performs several local iterations and synchronizing
    the model. The procedure of Local-SGD can be formalized as
  prefs: []
  type: TYPE_NORMAL
- en: '| (8) |  | $\mathbf{x}_{i,t+1}=\left\{\begin{array}[]{ll}\mathbf{x}_{i,t}-{\gamma}G_{i,t}(\mathbf{x}_{i,t}),&amp;\text{if}\
    t+1\not\in\mathcal{I}_{T}\\ \mathbf{x}_{i,t}-{\gamma}\frac{1}{n}\sum_{i=1}^{n}G_{i,t}(\mathbf{x}_{i,t}),&amp;\text{if}\
    t+1\in\mathcal{I}_{T}\end{array}\right.$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $\mathcal{I}_{T}$ represents the synchronization timestamps.
  prefs: []
  type: TYPE_NORMAL
- en: 'While Local-SGD allows for flexible communication frequency to reduce the overall
    communication overhead, reducing it excessively can lead to a decline in convergence
    performance. Consequently, Local-SGD often requires an increased number of training
    iterations to attain comparable model accuracy to that of BSP-SGD, thereby potentially
    slowing down the training process. Therefore, it is important to find the optimal
    communication period that strikes a balance between communication overhead and
    convergence performance. To improve the performance of Local-SGD, Yu et al. ([pmlrv97yu19d,](#bib.bib242)
    ) combined distributed momentum SGD and Local-SGD achieving a linear speedup in
    training. Jiang et al. ([AlinearSpeedupAnalysis,](#bib.bib82) ) reduced communication
    complexity by exploiting the quantization method (§[5](#S5 "5\. Quantization methods
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")) in combination with Local SGD.'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing batch sizes can also reduce the number of iterations needed to reduce
    communication data. CR-PSGD-Catalyst ([pmlrv97yu19c,](#bib.bib241) ) proposed
    to dynamically increase batch sizes after each training iteration, while guaranteeing
    the same convergence rate of SSP. However, large-batch SGD can lead to decreased
    generalization performance  ([chen2016scalable,](#bib.bib32) ; [NIPS2019_8452,](#bib.bib44)
    ; [Hoffer2017TLG32947713294936,](#bib.bib71) ; [DBLPconficlrKeskarMNST17,](#bib.bib88)
    ; [DBLPjournalscorrabs181103600,](#bib.bib168) ). To address this issue, Lin et
    al. ([Lin2018DontUL,](#bib.bib111) ) proposed post-local SGD to allow one to scale
    the training onto much more parallel computing devices. This algorithm divides
    the whole training process into two phases, using mini-batch SGD in the first
    phase and Local-SGD in the second phase.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lazy Aggregation (LAG) ([LAG,](#bib.bib33) ; [LENA,](#bib.bib184) ) delays
    the gradient uploading if the new gradient of a worker is similar with the old
    gradient of the last iteration. And the server will use the old gradient of the
    worker $i$ as the new gradient to conduct aggregation. This can be seen as a special
    Local-SGD, as it skips the communication round. The 3PC ([3PC,](#bib.bib154) )
    designs a novel compressor which unifies the error-feedback (§[9](#S9 "9\. Auxiliary
    Technologies ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey")) and LAG together, and obtains higher compression ratio
    and faster convergence.'
  prefs: []
  type: TYPE_NORMAL
- en: FedAvg ([FederatedLearning,](#bib.bib125) ; [tang2023fedml,](#bib.bib203) ;
    [tang2022gossipfl,](#bib.bib205) ; [tang2022virtual,](#bib.bib206) ) is another
    representative Local-SGD algorithm, which is a fundamental method in federated
    learning ([FederatedLearning,](#bib.bib125) ; [kairouz2019advances,](#bib.bib85)
    ). In FedAvg, the server randomly selects a part of clients to conduct local training
    at each communication round. Different from setting a fixed number of iterations
    in Local-SGD, clients in FedAvg conduct local training on all local samples for
    one epoch or several epochs. Thus, the number of iterations in FedAvg actually
    is proportional to the size of local datasets. After local training, the server
    collects local models for aggregation.
  prefs: []
  type: TYPE_NORMAL
- en: 3.5\. Experimental Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Table 2\. Test accuracy [%] comparison of different communication synchronization
    schemes.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | # of Worker | Resnet20 | RNN |'
  prefs: []
  type: TYPE_TB
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| BSP-SGD | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28 | 47.70
    |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
  prefs: []
  type: TYPE_TB
- en: '| ASP-SGD | 4 | 88.27 | 92.27 | 90.84 | 85.97 | 53.73 | 50.18 | 52.39 | 43.81
    |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 0.00 | 0.00 | 0.00 | 10.00 | 53.79 | 55.34 | 3.75 | 3.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD $\tau=2$ | 4 | 84.04 | 88.97 | 92.01 | 88.86 | 55.03 | 54.94 |
    52.87 | 46.72 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 65.41 | 84.96 | 89.35 | 90.11 | 54.11 | 51.72 | 52.30 | 46.17 |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD $\tau=4$ | 4 | 83.85 | 89.61 | 90.90 | 90.00 | 51.92 | 55.05 |
    56.14 | 47.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 65.55 | 85.09 | 89.44 | 89.85 | 54.36 | 51.72 | 53.40 | 47.15 |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD $\tau=8$ | 4 | 84.58 | 89.32 | 91.20 | 90.86 | 55.25 | 55.19 |
    53.35 | 48.55 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.48 | 84.82 | 89.41 | 90.09 | 54.07 | 51.72 | 54.91 | 47.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD $\tau=16$ | 4 | 84.02 | 89.25 | 90.99 | 90.56 | 55.32 | 55.41 |
    53.43 | 48.84 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.74 | 84.69 | 89.57 | 90.15 | 53.85 | 51.72 | 56.69 | 46.93 |'
  prefs: []
  type: TYPE_TB
- en: '| FedAvg | 4 | 62.41 | 84.37 | 89.81 | 90.10 | 52.20 | 55.23 | 54.91 | 54.92
    |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 40.23 | 64.42 | 83.45 | 86.65 | 28.77 | 43.70 | 51.91 | 52.02 |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Note: $\tau$ means the number of local iterations of Local SGD.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Table 3\. Test accuracy [%] comparison of different communication synchronization
    schemes.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | # of Worker | Resnet20 | RNN |'
  prefs: []
  type: TYPE_TB
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| BSP-SGD | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28 | 47.70
    |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 78.66 | 88.04 | 90.53 | 90.48 | 52.92 | 54.89 | 54.63 | 47.80 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 72.51 | 87.03 | 90.04 | 90.02 | 54.79 | 53.72 | 52.99 | 47.75 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
  prefs: []
  type: TYPE_TB
- en: '| BSP-EFTopK | 4 | 83.78 | 88.91 | 90.90 | 89.28 | 51.73 | 55.36 | 55.74 |
    47.66 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 79.76 | 88.25 | 89.84 | 90.09 | 52.64 | 54.89 | 54.48 | 46.65 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 72.23 | 87.01 | 89.80 | 88.97 | 54.70 | 53.60 | 52.86 | 47.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.52 | 84.33 | 88.08 | 88.69 | 53.98 | 51.62 | 52.14 | 46.88 |'
  prefs: []
  type: TYPE_TB
- en: '| ASP-SGD | 4 | 88.27 | 92.27 | 90.84 | 85.97 | 53.73 | 50.18 | 52.39 | 43.81
    |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 88.15 | 90.49 | 89.67 | 10.00 | 53.72 | 43.69 | 52.94 | 44.34 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 85.69 | 85.74 | 81.47 | 10.00 | 54.01 | 37.22 | 51.60 | 6.24 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 0.00 | 0.00 | 0.00 | 10.00 | 53.79 | 55.34 | 3.75 | 3.04 |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD $\tau=4$ | 4 | 83.85 | 89.61 | 90.90 | 90.00 | 51.92 | 55.05 |
    56.14 | 47.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 79.06 | 88.42 | 89.95 | 90.72 | 53.24 | 54.71 | 55.11 | 47.76 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 72.92 | 84.98 | 86.89 | 90.58 | 55.12 | 53.45 | 53.33 | 47.22 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 65.55 | 85.09 | 89.44 | 89.85 | 54.36 | 51.72 | 53.40 | 47.15 |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD $\tau=4$ TopK | 4 | 60.73 | 81.18 | 85.20 | 83.83 | 50.54 | 50.78
    | 52.64 | 50.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 8 | 54.43 | 76.96 | 85.00 | 84.78 | 48.01 | 49.13 | 52.28 | 48.89 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 46.33 | 70.29 | 83.54 | 83.38 | 44.48 | 47.47 | 51.58 | 47.47 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 39.75 | 62.86 | 79.11 | 81.55 | 40.75 | 44.62 | 50.40 | 46.64 |'
  prefs: []
  type: TYPE_TB
- en: Table 4\. Mean and STD. of the test accuracy [%] of some experiments with 3
    different random seeds.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algo | Resnet20 |'
  prefs: []
  type: TYPE_TB
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 |'
  prefs: []
  type: TYPE_TB
- en: '| BSP-SGD | $65.43\pm 0.54$ | $85.09\pm 0.11$ | $89.15\pm 0.14$ | 89.28$\pm
    0.74$ |'
  prefs: []
  type: TYPE_TB
- en: '| BSP-EFTopK | $64.62\pm 0.80$ | $84.18\pm 0.24$ | $87.81\pm 0.23$ | 88.14$\pm
    0.44$ |'
  prefs: []
  type: TYPE_TB
- en: '| DP-SGD (Gossip) | $64.86\pm 0.76$ | $84.83\pm 0.23$ | $88.83\pm 0.11$ | 89.11$\pm$
    0.45 |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD $\tau=4$ | $65.28\pm 0.67$ | $84.78\pm 0.31$ | $89.10\pm 0.24$
    | 89.49$\pm$ 0.49 |'
  prefs: []
  type: TYPE_TB
- en: '| FedAvg | $39.94\pm 0.47$ | $64.03\pm 0.81$ | $83.6\pm 0.11$ | 86.61 $\pm$0.25
    |'
  prefs: []
  type: TYPE_TB
- en: 'Different synchronization schemes have different properties of communication
    efficiency and convergence rate, some works compare few schemes ([Lin2020Dont,](#bib.bib112)
    ; [NEURIPS2022_029df12a,](#bib.bib129) ; [jiang2022pisces,](#bib.bib83) ). However,
    to the best our knowledge, there is no works benchmark all of them together in
    unified experiment settings. We jointly evaluated the performance of BSP-SGD,
    ASP-SGD, Local-SGD and FedAvg with unified dataset settings and hyper-parameters.
    Experiment settings are provided in Section [1.3](#S1.SS3 "1.3\. Benchmark Framework
    and Experiment Configuration ‣ 1\. Introduction ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"). For Local-SGD, we
    compare different number of local iterations, including 2, 4, 8, and 16\. Our
    results, as summarized in Table [2](#S3.T2 "Table 2 ‣ 3.5\. Experimental Comparison
    ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey"), indicate that BSP-SGD, ASP-SGD,
    Local-SGD and FedAvg finally obtain the similar test accuracy. And the performance
    of Local-SGD is not very sensitive to the local iterations $\tau$, suggesting
    that the communication overheads can be reduced by skipping some communication
    rounds without sacrificing the model performance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Different number of workers significantly influence the convergence of algorithms.
    Table [3](#S3.T3 "Table 3 ‣ 3.5\. Experimental Comparison ‣ 3\. Synchronous/asynchronous
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey") shows the influence of different workers (4, 8, 16, and
    32) to different algorithms. For both two datasets and all algorithms, to achieve
    the same test accuracy, higher learning rates are required when the number of
    workers is increased, as it is similar to higher batch size ([you2019large,](#bib.bib239)
    ). However, for most algorithms, increasing the number of workers can lead to
    a degradation in the final accuracy due to the well-known generalization problem
    of large batch SGD ([Hoffer2017TLG32947713294936,](#bib.bib71) ; [DBLPconficlrKeskarMNST17,](#bib.bib88)
    ), which is a significant bottleneck for distributed training scalability.'
  prefs: []
  type: TYPE_NORMAL
- en: ASP-SGD suffers from a larger drop in the test accuracy, and even diverges when
    the number of workers increases to 32\. This phenomenon is attributed to the higher
    degree of staleness with larger worker numbers. Additionally, the experimental
    results suggest that ASP-SGD may require a lower learning rate compared to other
    algorithms with the same number of workers. This could be because excessively
    moving towards a stale gradient direction could be futile and biased.
  prefs: []
  type: TYPE_NORMAL
- en: 'In conclusion, while efficient communication algorithms reduce the exchange
    of information compared to BSP-SGD, they may compromise the convergence performance
    of the model. Therefore, the benefits of communication-efficient algorithms come
    at a cost. To provide a better understanding of the varying impacts of these methods,
    we present a comparison in Table [5](#S3.T5 "Table 5 ‣ 3.5\. Experimental Comparison
    ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Table 5\. Influences of different combinations of architectures and synchronization
    schemes
  prefs: []
  type: TYPE_NORMAL
- en: '| Architecture | Synchronization | Model Consistency | Communication Frequency
    | Communication Congestion | Convergence |'
  prefs: []
  type: TYPE_TB
- en: '| PS | BSP-SGD | high | high | high | stable |'
  prefs: []
  type: TYPE_TB
- en: '| SSP-SGD | normal | high | normal | normal |'
  prefs: []
  type: TYPE_TB
- en: '| ASP-SGD | low | high | low | unstable |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD | normal | low | high | unstable |'
  prefs: []
  type: TYPE_TB
- en: '| All-Reduce | BSP-SGD | high | high | low | easy |'
  prefs: []
  type: TYPE_TB
- en: '| SSP-SGD | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ASP-SGD | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD | normal | low | low | stable |'
  prefs: []
  type: TYPE_TB
- en: '| Gossip | BSP-SGD | low | high | low | stable |'
  prefs: []
  type: TYPE_TB
- en: '| SSP-SGD | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ASP-SGD | low | high | low | unstable |'
  prefs: []
  type: TYPE_TB
- en: '| Local-SGD | low | low | low | stable |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notes: Here the different levels only represent a relative description comparing
    with the other methods. The model consistency measures how local models are different
    from others, the communication frequency measures how frequently workers communicate
    with others, and communication congestion measures how heavy the traffic of the
    central node is.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 4\. Centralized/Decentralized framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Various system architectures have been proposed to support efficient data communication
    among workers in distributed DL. The choice of the most appropriate architecture
    depends on the available hardware resources. Regarding how to average model parameters/gradients
    among distributed workers, system architectures can be categorized into three
    types: Parameter Server (PS), All-Reduce, and Gossip as shown in Fig. [4](#S4.F4
    "Figure 4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/49f575988e16ded5af29ca8b5aa4f00e.png)'
  prefs: []
  type: TYPE_IMG
- en: (a) PS architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/b975de9291f3d8881282118e2a72e1c0.png)'
  prefs: []
  type: TYPE_IMG
- en: (b) All-Reduce architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/e2fe5f9cfe335e4168bbc100874f2ee9.png)'
  prefs: []
  type: TYPE_IMG
- en: (c) Gossip architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4\. Three different system architectures for model/gradient aggregation.
    In (a) and (b), workers synchronize trained models and conduct local training
    with the synchronized model. In (c), workers communicate model parameters with
    some neighbors, and start local training with heterogeneous models parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 4.1\. Parameter Server
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Parameter Server (PS) architecture is shown in Fig. [4](#S4.F4 "Figure
    4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey")(a). Servers are responsible
    for storing the newest global model, and gathering the updating information from
    the workers. Workers independently load the dataset, pull the newest global model
    from the server, and then compute the updates, which will be transmitted to the
    servers ([PSforDisML,](#bib.bib103) ; [ScalDMLwithPS,](#bib.bib101) ). The PS
    architecture is referred to as the centralized framework, commonly used in distributed
    training  ([PSforDisML,](#bib.bib103) ; [ScalDMLwithPS,](#bib.bib101) ; [dean2012large,](#bib.bib43)
    ; [MoreEffDMLviaStaleSync,](#bib.bib69) ; [communicationDisMLwithPS,](#bib.bib102)
    ; [Ooi2015SD,](#bib.bib139) ). The major issue with this architecture is the communication
    congestion in the server ([Zhang2015DLE296,](#bib.bib254) ; [HowToScaleDDL,](#bib.bib140)
    ) as it requires to extensively communicate with all workers.'
  prefs: []
  type: TYPE_NORMAL
- en: Early PS based distributed machine learning methods  ([ScalDMLwithPS,](#bib.bib101)
    ; [DisGraphlab,](#bib.bib115) ; [AsystemLarScalSupML,](#bib.bib1) ; [AnArchforParallel,](#bib.bib185)
    ; [MoreEffDMLviaStaleSync,](#bib.bib69) ; [PSforDisML,](#bib.bib103) ) primarily
    focused on how to implement distributed machine learning at the system level to
    support a vast number of model parameters. In the pioneering framework DistBelief ([dean2012large,](#bib.bib43)
    ), Dean et al. successfully trained the model with billions of parameters on an
    extremely large-scale cluster. Initially, for data parallel distributed machine
    learning algorithms, numerous works focused on CPU clusters  ([Ahmed2012,](#bib.bib3)
    ; [pmlrv32ahn14,](#bib.bib4) ; [Cui2014EBS,](#bib.bib38) ; [Cui2014IP67,](#bib.bib39)
    ; [Power2010PBF,](#bib.bib143) ; [7816979,](#bib.bib193) ). Later, Cui et al. ([Cui2016,](#bib.bib40)
    ) proposed GeePS, a PS architecture to scale deep learning models training across
    numerous GPUs, addressing the challenge of limited GPU memory when training large
    deep models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Subsequently, researchers focused on developing communication-efficient methods
    to address the communication bottleneck in PS-based distributed machine learning.
    In an early work by Li et al. ([communicationDisMLwithPS,](#bib.bib102) ), controllable
    synchronization and user-definable filters were used to reduce data communication
    volumes. Later, numerous methods were proposed to achieve efficient communication
    with data compression like sparsification ([DorefaNet,](#bib.bib262) ; [1bit,](#bib.bib167)
    ; [scalableDisDNN,](#bib.bib190) ; [QSGD,](#bib.bib7) ; [ECQSGD,](#bib.bib229)
    ; [TernGrad,](#bib.bib227) ; [signSGD,](#bib.bib17) ; [EFsignSGD,](#bib.bib86)
    ) (§[6](#S6 "6\. Sparsification Methods ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey")) and quantization ([SparOLviaTrunc,](#bib.bib97)
    ; [FLStrategy,](#bib.bib91) ; [GradSparforDisOptim,](#bib.bib225) ; [CommQuantforDataPara,](#bib.bib48)
    ; [DGC,](#bib.bib113) ; [zhang2017poseidon,](#bib.bib248) ) (§[5](#S5 "5\. Quantization
    methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, Wang et al.  ([icdcsWangWL19,](#bib.bib219) ) proposed a novel
    measurement to determine the relevance of a worker. They measured the relevance
    by comparing the local update with the global update from the previous iteration.
    If the relevance exceeds a threshold, this update is considered as relevant and
    transmitted to the server. Another approach to reducing communication overhead
    is to use programmable switches ([In-network,](#bib.bib160) ). By having workers
    connected to the same switch transmit their model updates over the network and
    complete the aggregation in the network, communication overheads between machines
    can be reduced. However, this approach presents challenges such as limited computation
    and storage of switches, as well as the packet loss. Specialized algorithms need
    to be designed to address these issues.
  prefs: []
  type: TYPE_NORMAL
- en: 4.2\. All-Reduce
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To avoid the communication bottleneck in the PS, practitioners and researchers
    turned to the All-Reduce architecture for gradient aggregation without central
    servers ([awan2017s,](#bib.bib11) ; [chu2017efficient,](#bib.bib36) ; [wang2019impact,](#bib.bib222)
    ; [goyal2017accurate,](#bib.bib56) ; [jia2018highly,](#bib.bib81) ). As depicted
    in Fig. [4](#S4.F4 "Figure 4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")(b), all workers
    communicate with each other without a central node to obtain the gradients from
    all the other workers. The aggregated gradients are used to update their local
    models, thereby achieving consistency with other workers (the initialized models
    in different workers are identical). It is a model-centralized topology since
    there is a consistent global model attained through synchronization, which allows
    the updating equation ([5](#S2.E5 "In 2\. Taxonomy of Distributed DL ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")) (BSP-SGD) to
    remain unchanged. However, it is not suitable for asynchronous communication due
    to the collective communication nature of All-Reduce. While it is difficult to
    apply All-Reduce in the asynchronous part of SSP, it is easily applicable in the
    synchronous part of SSP.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The high-performance computing community has a long history of system optimizations
    for the All-Reduce collective, as various approaches proposed to improve its performance
     ([OptimizationofCollective,](#bib.bib148) ; [Thakur2005,](#bib.bib207) ; [Hoefler:2010,](#bib.bib70)
    ; [sanders2009two,](#bib.bib159) ). Some popular All-Reduce algorithms with different
    latency and bandwidth costs for an $N$-dimensional vector and an $n$-node cluster
    are summarized in Table [6](#S4.T6 "Table 6 ‣ 4.2\. All-Reduce ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey"). The communication cost of sending or receiving an $N$-dimensional
    vector is modeled as $\alpha+\beta N$  ([sarvotham2001connection,](#bib.bib161)
    ; [Shi2018MGWFBPED,](#bib.bib170) ).'
  prefs: []
  type: TYPE_NORMAL
- en: Table 6\. Communication costs of some representative All-Reduce algorithms
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | Latency | Bandwidth |'
  prefs: []
  type: TYPE_TB
- en: '| Binary tree | $2\alpha\log n$ | $2\beta(\log n)N$ |'
  prefs: []
  type: TYPE_TB
- en: '| Recursive doubling | $\alpha\log n$ | $\beta(\log n)N$ |'
  prefs: []
  type: TYPE_TB
- en: '| Ring | $2(n-1)\alpha$ | $\frac{2(n-1)}{n}\beta N$ |'
  prefs: []
  type: TYPE_TB
- en: 'Table [6](#S4.T6 "Table 6 ‣ 4.2\. All-Reduce ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey") summarizes various All-Reduce algorithms, which have been
    optimized by the high-performance computing community over time ([OptimizationofCollective,](#bib.bib148)
    ; [Thakur2005,](#bib.bib207) ; [Hoefler:2010,](#bib.bib70) ; [sanders2009two,](#bib.bib159)
    ). Among these algorithms, the ring-based All-Reduce is widely used in distributed
    DL due to its bandwidth optimality (e.g., Gloo²²2[https://github.com/facebookincubator/gloo](https://github.com/facebookincubator/gloo)
    and earlier versions of NCCL³³3[https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl).).
    However, the latency of the ring-based All-Reduce is linearly proportional to
    the number of workers, leading to high communication costs when scaling to large-scale
    clusters ([you2017scaling,](#bib.bib238) ; [Shi2018MGWFBPED,](#bib.bib170) ; [shi2021mgj,](#bib.bib171)
    ; [shi2021accelerating,](#bib.bib180) ). To address this issue, recent updates
    of NCCL (started from version 2.4)⁴⁴4[https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/](https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/)
    have integrated the double binary trees ([sanders2009two,](#bib.bib159) ) to perform
    an all-reduction to achieve full bandwidth and a logarithmic latency. The double
    binary trees require the whole message to be broken down into multiple blocks
    and the workers to be formed as a binary tree so that different blocks can be
    executed in parallel. Therefore, for some small messages or small-scale clusters,
    recursive doubling or ring-based algorithms would be better.'
  prefs: []
  type: TYPE_NORMAL
- en: To further reduce the latency term in All-Reduce while preserving the bandwidth
    optimality, the hierarchical All-Reduce algorithms  ([goyal2017accurate,](#bib.bib56)
    ; [jia2018highly,](#bib.bib81) ; [ueno2019exhaustive,](#bib.bib209) ) were also
    proposed, which can reduce the latency cost several times (related to the number
    of hierarchies). 2D-Torus All-Reduce ([mikami2018massively,](#bib.bib127) ; [jouppi2017datacenter,](#bib.bib84)
    ) can also massively reduce the communication latency using a 2D-Torus topology
    network. Under different topology networks (e.g., BCube  ([guo2009bcube,](#bib.bib59)
    )), it is also important to carefully design the All-Reduce algorithms to achieve
    lower latency and higher bandwidth. Wang et al.  ([wang2018bml,](#bib.bib221)
    ) proposed BML for the BCube topology to achieve efficient communication. Some
    topology-aware algorithms (e.g., BLink  ([wang2020blink,](#bib.bib214) ) and PLink ([luo2020plink,](#bib.bib116)
    )) were designed to be adaptive to distributed environments to highly utilize
    the network bandwidth with low latency.
  prefs: []
  type: TYPE_NORMAL
- en: 4.3\. Gossip
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Gossip architecture has emerged as a mechanism for inter-worker communication
    and is employed to solve the distributed average problem ([gossip1,](#bib.bib87)
    ; [gossip2,](#bib.bib114) ; [gossip3,](#bib.bib22) ; [gossip4,](#bib.bib37) ).
    Researchers have exploited this architecture to improve BSP-SGD  ([DisSubGradMultiOptim,](#bib.bib134)
    ; [AsynDisOptimRandADMM,](#bib.bib77) ; [OptimDisOptim,](#bib.bib166) ; [OptimNonsmoothDisOptim,](#bib.bib164)
    ; [DualApprochforOptim,](#bib.bib210) ; [MultiagentMirrorDescentDenct,](#bib.bib147)
    ; [CommEffDenctStoc,](#bib.bib96) ; [CanDecent,](#bib.bib108) ; [DecentTrainingoverDecentData,](#bib.bib202)
    ; [StocGradPush,](#bib.bib10) ; [CommCompforDecent,](#bib.bib201) ).
  prefs: []
  type: TYPE_NORMAL
- en: 'As illustrated in Fig. [4](#S4.F4 "Figure 4 ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey")(c), the gossip architecture does not require global models
    or parameter servers (represented by different colors of local models). Instead,
    each worker communicates updates with their neighbors (also named peers). Workers
    exchange messages via edges chosen in each iteration (the blues ones in Fig. [4](#S4.F4
    "Figure 4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")(c)), which can
    be described as a communication graph. Workers are not required to communicate
    with all others, reducing communication costs. During training, the algorithm
    does not guarantee parameter consistency across all workers after each communication,
    but guarantees it (i.e., consensus) at the end of the algorithm. This means that
    local models are different during every iteration. It should be noted that in
    asynchronous and SSP with the PS architecture, although local models are also
    different, a global model is still maintained.'
  prefs: []
  type: TYPE_NORMAL
- en: Similar to the All-Reduce architecture, gossip architecture offers the benefit
    of having no master node, thus eliminating communication issues of the central
    server ([CommCompforDecent,](#bib.bib201) ). Compared to All-Reduce architecture,
    the gossip architecture is more fault-tolerant to the worker failures and it can
    also be theoretically guaranteed ([CanDecent,](#bib.bib108) ) with less communication
    costs than PS or All-Reduce.
  prefs: []
  type: TYPE_NORMAL
- en: In the gossip architecture, the key issue is ensuring the attainment of identical
    model parameters across all workers. This problem is referred to as “consensus”
    and has been studied extensively in the literature  ([7942055DistributedLinearized,](#bib.bib12)
    ; [1498447Gossipalgorithms,](#bib.bib21) ; [Carli2010,](#bib.bib26) ; [4497789Randomizedconsensus,](#bib.bib52)
    ; [4118472ConsensusandCooperation,](#bib.bib138) ; [4434671Adistributedconsensus,](#bib.bib165)
    ). Formally, consensus is a state of all workers attaining a same opinion in common.
    In the gossip distributed SGD, the consensus is the status that all workers have
    the identical model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: However, on one hand, achieving the “consensus” incurs the difficulty in designing
    a method that enables efficient communication, high convergence rate, and consensus
    simultaneously. To address these issues, model compression can be combined with
    decentralized learning algorithms to reduce communication costs  ([CommCompforDecent,](#bib.bib201)
    ; [DecentStocOptimAndGossip,](#bib.bib90) ; [NIPS2018_7705,](#bib.bib68) ; [tang2020communication,](#bib.bib204)
    ). On the other hand, the Gossip architecture is limited to using symmetric communication,
    which inherently requires deadlock-avoidance and more synchronization, resulting
    in slower and more sensitivity to stragglers. Assran et al. ([StocGradPush,](#bib.bib10)
    ) proposed a solution that combines Stochastic Gradient Push (SGP) ([SGPDirected,](#bib.bib135)
    ) with PUSHSUM ([8340193,](#bib.bib136) ; [gossip1,](#bib.bib87) ). The PUSHSUM
    provides an approximation of the distributed averaging, while the SGP makes each
    worker only send the gradient to its out-neighbors without waiting for the response
    from these neighbors. Thus, the overall system throughput is improved.
  prefs: []
  type: TYPE_NORMAL
- en: 4.4\. Experimental Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct experiments to compare BSP-SGD⁵⁵5From the algorithmic view, the
    BSP-SGD algorithm under the All-Reduce architecture is identical to that under
    the PS architecture. Thus, we choose one to compare the convergence performance.
    and synchronous data-parallel SGD (DP-SGD) with the Gossip architecture. Table [7](#S4.T7
    "Table 7 ‣ 4.4\. Experimental Comparison ‣ 4\. Centralized/Decentralized framework
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey") indicates that BSP-SGD and DP-SGD (Gossip) can achieve similar performance.
    DP-SGD (Gossip) exhibits a decrease in performance as the number of workers increases,
    consistent with the results presented in Table [3](#S3.T3 "Table 3 ‣ 3.5\. Experimental
    Comparison ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Table 7\. Test accuracy [%] comparison of different communication architectures.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algorithm | # of Worker | Resnet20 | RNN |'
  prefs: []
  type: TYPE_TB
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| BSP-SGD (PS) | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28
    | 47.70 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
  prefs: []
  type: TYPE_TB
- en: '| DP-SGD (Gossip) | 4 | 84.60 | 89.38 | 91.08 | 91.01 | 55.41 | 55.19 | 52.92
    | 48.13 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.20 | 85.12 | 88.99 | 89.71 | 53.55 | 51.71 | 53.77 | 46.50 |'
  prefs: []
  type: TYPE_TB
- en: 5\. Quantization methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compression methods aim to reduce communication data by compressing gradients
    or model parameters that need to be transmitted between workers or servers. However,
    most of these methods use lossy compression, which prevents the receiver from
    fully recovering the original gradients or model parameters. As a result, convergence
    may be impacted due to the reduced amount of information available. It has been
    an active research direction to reduce the communication traffic with little impact
    on the convergence.
  prefs: []
  type: TYPE_NORMAL
- en: '![Refer to caption](img/58928c883b79d2ce172a2143bc953be5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5\. Comparison of Quantization and Sparsification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Quantization is a one of popular compression schemes that uses lower bits to
    represent data originally represented by 32 bits on each dimension of the transmitted
    gradient. As shown in Fig. [5](#S5.F5 "Figure 5 ‣ 5\. Quantization methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey"), the element
    in the quantized gradient is coded by fewer bits, resulting in low-precision gradients
    after quantization. Low-precision gradients are beneficial to DL as higher-speed
    calculations and lower memory are needed for training DNNs on CPUs and GPUs. Many
    researchers have studied DL convergence under low-precision gradients with different
    quantization methods ([ImproSpeedNN,](#bib.bib212) ; [DLwithlimited,](#bib.bib61)
    ; [ProbroundingNN,](#bib.bib72) ; [DorefaNet,](#bib.bib262) ; [Hubara2017,](#bib.bib76)
    ). The process of quantized SGD can be formulated by as following equations:'
  prefs: []
  type: TYPE_NORMAL
- en: '| (9) |  | $G_{i,t}^{quant}(\mathbf{x}_{t})=Quant(G_{i,t}(\mathbf{x}_{t})+\delta_{i,t}(\mathbf{x}_{t}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (10) |  | $\delta_{i,t}(\mathbf{x}_{t})=G_{i,t}(\mathbf{x}_{t})-{Quant}^{-1}(G_{i,t}^{quant}(\mathbf{x}_{t}))$
    |  |'
  prefs: []
  type: TYPE_TB
- en: '| (11) |  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}\frac{1}{n}\sum_{i=1}^{n}G_{i,t}^{quant}(\mathbf{x}_{t})$
    |  |'
  prefs: []
  type: TYPE_TB
- en: 'where $Quant(\cdot)$ denotes the quantization function that encodes gradients,
    and $Quant(\cdot)$ the unquantizer that decodes quantized gradients. This approach
    reduces communication costs as $G_{i,t}^{quant}(\mathbf{x_{t}})$, compared to
    the approach described in Eq. ([4](#S2.E4 "In 2\. Taxonomy of Distributed DL ‣
    Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")).'
  prefs: []
  type: TYPE_NORMAL
- en: In  ([DisMeanEst,](#bib.bib198) ) and ([DisMeanEst,](#bib.bib198) ), the communication
    of gradients in BSP-SGD actually is regarded as a distributed mean estimation
    problem ([DisMeanEst,](#bib.bib198) ; [RandDisMean,](#bib.bib92) ). They  ([DisMeanEst,](#bib.bib198)
    ; [RandDisMean,](#bib.bib92) ) analyzed the communication-efficient algorithms
    for distributed mean estimation. They used the Mean Squared Error (MSE) to measure
    how accurate the quantization methods are. And then, they proposed coding strategies
    to achieve the best MSE for a given communication cost, considering that increasing
    the number of quantization levels increases the communication cost. To reduce
    the communication cost, Suresh et al. ([DisMeanEst,](#bib.bib198) ) proposed two
    methods, Stochastic Rotated Quantization (SRQ) and Variable Length Coding (VLC).
    In SRQ, all clients and the central server generate a global random rotation matrix
    and then try to find an orthogonal matrix $\mathbb{R}$ that achieves low MSE.
    The VLC uses arithmetic of Huffman Coding corresponding to the number of times
    each quantized value has appeared.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve a higher compression ratio, Sei et al. ([1bit,](#bib.bib167) ) proposed
    1-bit SGD to reduce the transmitted data volumes, and successfully trained the
    deep model on traditional speech applications with a 10$\times$ speedup. They
    reduced each gradient element to one bit and quantified a gradient $G_{i,t}(\mathbf{x}_{t})$
    while keeping the quantization error $\delta_{i,t}(\mathbf{x}_{t})$ not large
    at the same time. The key idea of 1-bit quantization is also used in ([scalableDisDNN,](#bib.bib190)
    ). Different from 1-bit SGD, Strom et al. ([scalableDisDNN,](#bib.bib190) ) chose
    a fixed threshold, and then encoded the gradient elements higher than $T$ with
    the value $1$, those less than $–T$ with value 0\. The absolute value of a gradient
    element less than $T$ would not be sent, which is similar to the sparsification
    methods that will be discussed in §[6](#S6 "6\. Sparsification Methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: Some researchers further proposed adaptive quantization methods  ([7544448,](#bib.bib144)
    ; [7526802,](#bib.bib124) ) that incorporate inexact proximal gradients. However,
    these methods lack empirical validation with respect to deep learning models.
    To achieve both communication efficiency and good optimization convergence, Alistarh
    et al. ([QSGD,](#bib.bib7) ) presented a family of algorithms using quantization
    rather than just one quantization method. This family of quantization algorithms,
    named Quantized SGD (QSGD), allows for the trade-off between the number of communicated
    bits and the variance of the compressed gradient. They evaluated the performance
    of QSGD by training DNNs on ImageNet ([Imagenet,](#bib.bib45) ) and CIFAR-10 datasets,
    achieving accuracy rates comparable to those of original SGD. Specifically, QSGD
    exploits a probabilistic approach to quantify a vector. Given any vector $\mathbf{v}\in\mathbb{R}^{N}$,
    every $j$-th element $Quant_{s}({v}_{j})$ of quantization gradient $Quant_{s}(\mathbf{v})$
    is quantized by $Quant_{s}(\cdot)$, corresponding to the element ${v}_{j}$ of
    the original gradient $\mathbf{v}$. The stochastic quantization function is defined
    as
  prefs: []
  type: TYPE_NORMAL
- en: '| (12) |  | $Quant_{s}(v_{j})=\left\&#124;\mathbf{v}\right\&#124;_{2}\cdot
    sgn(v_{j})\cdot\psi_{j}(\mathbf{v},s),$ |  |'
  prefs: []
  type: TYPE_TB
- en: where $sgn(v_{j})\in\left\{-1,+1\right\}$ denotes the sign of $v_{j}$ and $sgn(0)=1$.
    $\psi_{j}(\mathbf{v},s)$ is defined as
  prefs: []
  type: TYPE_NORMAL
- en: '| (13) |  | $\psi_{j}(\mathbf{v},s)=\left\{\begin{array}[]{ll}l/s&amp;\text{with
    probability}\ 1-p(\frac{\left&#124;v_{j}\right&#124;}{\left\&#124;\mathbf{v}\right\&#124;_{2}},s),\\
    (l+1)/s&amp;\text{otherwise}\end{array}\right.$ |  |'
  prefs: []
  type: TYPE_TB
- en: $l$ is an integer such that $\left|v_{i}\right|/\left\|\mathbf{v}\right\|_{2}\in\left[l/s,(l+1)/s\right]$
    s.t. $0<l<s$, and $p(a,s)=as-l$ for any $a\in\left[0,1\right]$. This is called
    Standard (random) dithering in  ([Horvath2019NaturalCF,](#bib.bib73) ), and is
    used in PCM coding  ([6773262,](#bib.bib54) ; [1057702,](#bib.bib155) ). Note
    that if $\mathbf{v}=\mathbf{0}$, the algorithm sets $Quant_{s}(\mathbf{v},s)=\mathbf{0}$.
    The expectation $\psi_{j}(\mathbf{v},s)$ satisfies $\mathbb{E}\left[\psi_{j}(\mathbf{v},s)\right]=\left|v_{i}\right|/\left\|\mathbf{v}\right\|_{2}$.
    Then it is obvious that QSGD can assure that the quantized gradient is the unbiased
    estimation of the original vector, i.e., $\mathbb{E}\left[Quant_{s}(\mathbf{v})\right]=\mathbf{v}$,
    and facilitates convergence of training.
  prefs: []
  type: TYPE_NORMAL
- en: Wen et al. ([wen2017terngrad,](#bib.bib228) ) proposed Terngrad, which differs
    from the Parameter Server architecture. Each worker stores a copy of parameters
    locally, and the communication of parameters in the floating-point form is changed
    to the transfer of quantized gradients. This results in smaller server-to-worker
    traffic since only the quantized gradients are pulled from servers. To minimize
    the number of levels when workers are transferring different scalars, Terngrad
    presented Scalar Sharing, which selects the maximum scalar among all scalars and
    shares it across all workers. However, in a large deep model, a maximum element
    could be significantly larger than most gradients, which could weaken the approximation.
    To address this issue, Terngrad proposed layer-wise ternarizing and Gradient Clipping.
    Layer-wise ternarizing utilizes the layer-wise scalar in each layer instead of
    a large global maximum scalar. And A series of works ([faghri2020adaptive,](#bib.bib51)
    ; [jhunjhunwala2021adaptive,](#bib.bib80) ; [ElasticQuant,](#bib.bib192) ) proposed
    to utilize adaptive quantization bits, to enhance the convergence with compressed
    gradients.
  prefs: []
  type: TYPE_NORMAL
- en: Sign-SGD is another kind of quantization method ([signSGD,](#bib.bib17) ). In
    Sign-SGD, every worker quantifies the gradient to a binary value, which is the
    sign of each coordinate of the gradient vector. Bernstein et al.  ([signSGDwithVote,](#bib.bib18)
    ) provided an extensive theoretical analysis of Sign-SGD for non-convex optimization.
    They proved that when gradients are as dense or denser than stochasticity and
    curvature, Sign-SGD can converge with a theoretical rate. They also proposed a
    new algorithm named Sign-SGD with a majority vote. After workers exchange the
    sign of their gradient vector to a server, the overall update is decided by a
    majority vote. Mishchenko et al.  ([DisLearningDIANA,](#bib.bib130) ) introduced
    a novel method DIANA, which extends the methods of ([QSGD,](#bib.bib7) ; [TernGrad,](#bib.bib227)
    ) by splitting the whole gradient vector into some sub-vectors. And then, they
    quantified each sub-vector individually.
  prefs: []
  type: TYPE_NORMAL
- en: 6\. Sparsification Methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Quantization methods are limited in their compression rates, with a maximum
    compression rate of 32$\times$ achievable over commonly used SGD with single-precision
    floating-point arithmetic. However, reducing the number of transmitted elements
    can increase the compression rate even further. A set of algorithms that follow
    this approach are known as sparsification methods, where only a subset of elements
    are selected and transmitted ([meProp,](#bib.bib197) ; [ZipML,](#bib.bib247) ;
    [EffiUseofLimitMemory,](#bib.bib49) ; [NIPS2019_9610,](#bib.bib14) ; [10.1145/3452296.3472904,](#bib.bib53)
    ; [li2022on,](#bib.bib106) ; [shi2021towards,](#bib.bib182) ; [li2022near,](#bib.bib105)
    ; [zhang2023evaluation,](#bib.bib252) ).
  prefs: []
  type: TYPE_NORMAL
- en: 'The key idea of sparsification is that only “significant” gradients are essential
    for the SGD update to ensure the convergence of the training process ([DGC,](#bib.bib113)
    ). As illustrated in Fig. [5](#S5.F5 "Figure 5 ‣ 5\. Quantization methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey"), a substantial
    proportion of the coordinates in the gradient vector can be zeroed-out such that
    the zero-valued elements are no need to transmit. Gradient sparsification is a
    more aggressive compression method than quantization, enabling much more significant
    reductions in communication traffic.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The antecedents of sparsification methods can be traced back to the Truncated
    Gradient method introduced by Langford et al. ([SparOLviaTrunc,](#bib.bib97) )
    in the context of online learning algorithms. Truncated Gradient addresses memory
    space and computation constraints by inducing sparsity in the gradient. Rather
    than directly setting small coordinates to zero, Truncated Gradient keeps large
    coordinates with their original values and discards small coordinates that fall
    below a threshold. This method was the first sparsification technique developed
    for large-scale learning. After that, there are extensive studies to further improve
    sparsification in distributed DL. The sparsification methods can be broadly classified
    into four main types: coordinate descent, random sparsification, deterministic
    sparsification, and proximal methods.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1\. Random Sparsification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Random sparsification randomly selects a subset of entries from the gradient
    vector for communication and updating. This approach is also referred to as random-$k$,
    where $k$ denotes the number of selected elements.
  prefs: []
  type: TYPE_NORMAL
- en: In ([FLStrategy,](#bib.bib91) ), Konecny et al. proposed Random Mask and Subsampling.
    In Random Mask, a pre-defined random sparsity pattern, to convert the update parameters
    $\mathbf{H}$ into a sparse matrix $\hat{\mathbf{H}}$. This random pattern is regenerated
    in each training iteration. In addition, it can be initialized independently by
    each client or created by the server and then distributed to the workers. In the
    former case, each worker must transmit both the indices and values of the non-zero
    entries of $\mathbf{H}$. In the latter case, workers need only to send the values
    of the non-zero entries of $\mathbf{H}$, because all indices of non-zero entries
    in every worker are the same as others. In Subsampling, unlike Random Mask, the
    sparse communication matrix $\hat{\mathbf{H}}$ is scaled to ensure that $\mathbf{E}(\hat{\mathbf{H}})=\mathbf{H}$,
    making it an unbiased estimator of the true average.
  prefs: []
  type: TYPE_NORMAL
- en: 'In  ([GradSparforDisOptim,](#bib.bib225) ), Wangni et al. proposed to randomly
    drop out coordinates with a probability vector $\mathbf{p}$ and amplifying the
    non-zero coordinates from $g_{j}$ to $g_{j}/p_{j}$. Formally, if one wants to
    compress original vector $\mathbf{g}=\left[g_{1},g_{2},\cdots,g_{N}\right]$, given
    a probability vector $\mathbf{p}=\left[p_{1},p_{2},\cdots,p_{N}\right]$, and the
    final sparsified vector is $\mathbf{Q}_{spar}(\mathbf{g})=\left[Z_{1}\frac{g_{1}}{p_{1}},Z_{2}\frac{g_{2}}{p_{2}},\cdots,Z_{N}\frac{g_{N}}{p_{N}}\right]$,
    in which $Z_{i}$ represents selector, i.e. 0 or 1\. Each item $\mathbf{Q}_{spar}(\mathbf{g})_{i}$
    has the unbiased expectation: $\mathbb{E}\left[\mathbf{Q}_{spar}(\mathbf{g})_{i}\right]=p_{i}\times\frac{g_{i}}{p_{i}}+(1-p_{i})\times
    0=g_{i}$, similar to ([QSGD,](#bib.bib7) ; [FLStrategy,](#bib.bib91) ).'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2\. Deterministic Sparsification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Different from Random Sparsification, the sparse properties are guaranteed
    by Deterministic Sparsification ([TonotopicANN,](#bib.bib189) ; [SparConnec,](#bib.bib191)
    ), in which most weights of DNNs can be close to zero. Due to the sparse weights,
    most of the gradients in each iteration are also around zero so that the zeroed
    gradients are no need for communication to reduce the communication traffic. There
    are mainly two ways to sparsify the gradients: Fixed Threshold and Adaptive Threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.1\. Fixed Threshold
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Strom et al. ([scalableDisDNN,](#bib.bib190) ) introduced a new method to solve
    the communication bottleneck problem. In this algorithm, those gradient elements
    with absolute values less than a pre-defined threshold will be discarded. Because
    not all gradient elements are transmitted, the server must know which gradients
    to be transmitted and the indices of them. Strom et al. constructed key-value
    maps where keys are the indices and values are the corresponding gradient elements.
    The main drawback of the fixed threshold method is that it is non-trivial to choose
    a suitable threshold for different deep models or different iterations. Even worse,
    when Error Feedback (§[9](#S9 "9\. Auxiliary Technologies ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")) techniques
    are used, the fixed threshold method may result in the transmission of a large
    number of gradients.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.2.2\. Adaptive Threshold
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To address the problem of Fixed Threshold, Top-$k$ sparsification algorithms ([SparSGDwithMemory,](#bib.bib188)
    ; [ConvSparGrad,](#bib.bib6) ; [DGC,](#bib.bib113) ; [shi2020layer,](#bib.bib174)
    ; [shi2021towards,](#bib.bib182) ) select the top-$k$ gradients (in terms of absolute
    values) at each iteration.
  prefs: []
  type: TYPE_NORMAL
- en: Dryden et al. ([CommQuantforDataPara,](#bib.bib48) ) proposed an adaptive threshold,
    which uses a fixed proportion $\pi$ to indicate the proportion of gradient elements
    to transmit. In every iteration, the algorithm determines a positive threshold
    and a negative one to satisfy the desired proportion. This method is able to make
    sure that the compressing ratio will not change during the training process. Despite
    that this technique requires the sorting of the whole gradient vector elements
    and costs extra computing time, it still reduces the wall-lock time a lot. Aji
    & Heafield ([SparCommforDisGD,](#bib.bib5) ) proposed another adaptive threshold
    method that uses a single threshold to drop all gradients whose absolute value
    is smaller than the threshold. However, parameters and their corresponding gradients
    may have varying scales across different parts of the model. As comparing all
    gradients with one global threshold may result in the loss of some small-scale
    gradients, Aji & Heafield exploited layer normalization  ([LayNorm,](#bib.bib13)
    ) to make a global threshold work.
  prefs: []
  type: TYPE_NORMAL
- en: To account for local gradient activity, Chen et al. ([Adacomp,](#bib.bib29)
    ) proposed a novel gradient compression scheme, AdaComp, which can self-adapt
    the compression rate. They showed that most of the gradient compression techniques
    do not work well for convolutional layers, as different types of neural layers,
    mini-batch sizes, and other factors may affect the compression rate. AdaComp automatically
    determines the sparsification level, adapting to all variations.
  prefs: []
  type: TYPE_NORMAL
- en: Scattler et al.  ([sparsebinarycompression,](#bib.bib163) ) combined sparsification
    and quantization methods to propose Sparse Binary Compression (SBC), a new compression
    algorithm. SBC discards elements of low absolute value, averages positive and
    negative values to obtain positive and negative means $\mu^{+}$ and $\mu^{-}$
    respectively, and discards negative elements if $\mu^{+}$ is greater than $\mu^{-}$,
    and sets all positive elements to $\mu^{+}$ and vice versa. SBC then quantizes
    the non-zero elements, further reducing the communication cost by a factor of
    $\times 3$. Following SBC, Sattler et al.  ([SparseTernaryCompressionSTC,](#bib.bib162)
    ) further exploited the combination of top-$k$ sparsification and Ternary quantization
    to develop a new method named Sparse Ternary Compression (STC). Unlike SBC, STC
    is particularly suitable for the Federated Learning ([kairouz2019advances,](#bib.bib85)
    ). Experimental results in their paper demonstrate that sparsification methods
    achieve better convergence than quantization methods.
  prefs: []
  type: TYPE_NORMAL
- en: In many distributed training algorithms, workers pull the latest update from
    the PS before training to make their models not deviate too much from the global
    model. However, in most top-$k$ sparsification methods, the size of the latest
    update depends on the number of workers, and many indices of chosen elements of
    a gradient vector differ among workers. As a result, when all gradients are collected
    together, the elements of the global gradient vector increase almost linearly
    with the number of workers, leading to ineffective sparsity of master-to-workers
    communication. To address this problem, Shi et al. ([GTopk,](#bib.bib179) ) proposed
    a novel sparsification method gTop-$k$. After aggregating all gradients, gTop-$k$
    sparsifies the global gradient vector once again, reducing the master-to-workers
    communication load and attaining convergence simultaneously ([ijcai2019473,](#bib.bib181)
    ). Furthermore, by exploiting the properties of gradients that are empirically
    proved to follow bell-shaped distributions ([shi2019understanding,](#bib.bib169)
    ; [shi2021towards,](#bib.bib182) ), the computing-efficient approximation of Top-$k$
    can be further exploited. Adaptive selection of the number of gradients or model
    parameters for communication can also help reduce overall training time ([han2020adaptive,](#bib.bib63)
    ; [9834260,](#bib.bib236) ; [pmlr-v151-wang22e,](#bib.bib223) ; [9721697,](#bib.bib258)
    ; shi2020layerwise).
  prefs: []
  type: TYPE_NORMAL
- en: The compression operation itself may need the computation costs. This is often
    overlooked by much of previous works. Some statistical approaches ([m2021efficient,](#bib.bib117)
    ; [shi2019understanding,](#bib.bib169) ; [shi2021towards,](#bib.bib182) ) are
    also typically required to estimate an accurate threshold for compressing gradients
    with linear computation complexity in the size of model parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 6.3\. Coordinate Descent
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Coordinate descent is a kind of optimization method  ([Passcode,](#bib.bib74)
    ; [AsynCoordD,](#bib.bib196) ; [blockCoordD,](#bib.bib234) ; [ConverBlockCoordD,](#bib.bib257)
    ; [DisBlockCoordD,](#bib.bib119) ; [ProxBlockCoordD,](#bib.bib98) ; [Qu2015QRD2969239,](#bib.bib146)
    ) that splits all variables into multiple blocks, and then updates one block while
    fixing the remaining blocks. In an iteration, all blocks will be updated one by
    one  ([blockCoordD,](#bib.bib234) ). Despite the success of gradient-based methods,
    they may still suffer from the vanishing gradient problem for training deep neural
    networks  ([DL,](#bib.bib55) ). Gradient-free methods have been recently proposed
    to address the vanishing gradient problem, including BCD methods ([ProxBlockCoordD,](#bib.bib98)
    ; [ConverBlockCoordD,](#bib.bib257) ). In distributed DL scenarios, BCD can be
    easily implemented in a distributed and parallel manner ([DisBlockCoordD,](#bib.bib119)
    ). BCD has a property that only a subset of variables would be updated in each
    round, similar to the sparsification of distributed SGD.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lau et al. ([ProxBlockCoordD,](#bib.bib98) ) proposed an efficient BCD algorithm
    for DL and provided its convergence analysis. They highlighted three major advantages
    of BCD: (1) higher efficiency per epoch compared to SGD at early stages; (2) good
    scalability; and (3) gradient-free optimization. In ([GlobalconverBlockCoordD,](#bib.bib245)
    ), Zeng et al. presented a general methodology to establish provable convergence
    guarantees when using BCD in DL.'
  prefs: []
  type: TYPE_NORMAL
- en: Mishchenko et al.  ([Mishchenko201999OP,](#bib.bib131) ) developed a new algorithm
    named Independent Block Coordinate Descent (IBCD), which allows each worker to
    sample an independent subset of blocks. They proved that the optimal number of
    blocks to be updated by each of $n$ units in every iteration is equal to $m$,
    where $m$ is the total number of blocks. Specifically, this means that when $n=100$
    parallel units are used, 99% of work is a waste of time.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4\. Proximal Methods
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Proximal methods involve two kinds of sparsity-inducing regularization in learning
    models and solving the optimization problem with proximal-based algorithms. These
    methods are utilized for sparsity learning to reduce the number of parameters
    in deep learning models. Additionally, the communication of distributed deep learning
    can benefit from the sparsity. $L0$-and $L1$-norms of parameters are commonly
    used for these methods  ([CommEffiDisSpar,](#bib.bib152) ; [AsynCoordD,](#bib.bib196)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: Grishchenko et al. ([AsynDisMLspar,](#bib.bib58) ) firstly combined the Proximal
    method, coordinate descent, and random sparsification together. The workers compute
    a local update using a randomly selected subset of coordinates, while the master
    aggregates and averages all updates from workers and computes the proximity operator
    of the regularizer at the averaged updates.
  prefs: []
  type: TYPE_NORMAL
- en: To reduce communication overhead, Tsuzuku et al.  ([VarianceGradCompression,](#bib.bib208)
    ) developed a novel method that sends only gradient elements with small enough
    variance during training. They observed that some gradient elements are ambiguous,
    with low-amplitude but high-variance values, which may lead to futile updates
    to the global model. By controlling hyperparameters, their algorithm achieves
    high-rate sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: 6.5\. Experimental Comparison
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We conduct experiments of different compression schemes in the context of BSP,
    Local SGD, FedAVG and DPSGD algorithms. The compression schemes include QSGD quantization,
    Topk and error-feedback (EF) TopK sparsification. The overall experiment configuration
    is described in detail in Section [1.3](#S1.SS3 "1.3\. Benchmark Framework and
    Experiment Configuration ‣ 1\. Introduction ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey"). The results are presented
    in Table [8](#S6.T8 "Table 8 ‣ 6.5\. Experimental Comparison ‣ 6\. Sparsification
    Methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey").'
  prefs: []
  type: TYPE_NORMAL
- en: As the compression ratio increased, almost algorithms suffered from varying
    degrees of performance drop. In the case of TopK compression, the Local SGD suffers
    less performance drop than BSP. Intuitively, we suppose the reason is that the
    Local SGD transmits the model weights rather than the gradients, which makes workers
    still can update all parameters. Interestingly, results show that the EF-TopK
    can successfully converge well even at extremely high compression ratio, i.e.
    1000\. This is because EF-TopK communicates parameters that are left by compression
    in the future iterations. Thus, all parameters of the model can still be updated
    finally.
  prefs: []
  type: TYPE_NORMAL
- en: Table 8\. Experiments of comparing test accuracy of different communication
    compression algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: '| Algo | Compress Ratio | Client Num | Resnet20 | RNN |'
  prefs: []
  type: TYPE_TB
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
  prefs: []
  type: TYPE_TB
- en: '| BSP | None | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28 |
    47.70 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
  prefs: []
  type: TYPE_TB
- en: '| BSP quant | 2 | 4 | 84.28 | 89.33 | 90.52 | 0.31 | 55.19 | 55.06 | 52.87
    | 48.13 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.65 | 85.12 | 89.34 | 89.95 | 53.75 | 51.73 | 52.26 | 54.67 |'
  prefs: []
  type: TYPE_TB
- en: '| 16 | 4 | 80.31 | 83.45 | 86.44 | 0.15 | 48.03 | 42.39 | 12.80 | 13.51 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.34 | 83.56 | 85.37 | 84.41 | 51.84 | 52.62 | 0.62 | 8.68 |'
  prefs: []
  type: TYPE_TB
- en: '| BSP Topk | 10 | 4 | 76.68 | 86.58 | 88.30 | 88.29 | 53.53 | 52.72 | 50.73
    | 46.02 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 55.61 | 78.88 | 86.75 | 85.89 | 51.18 | 48.68 | 54.28 | 54.62 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 4 | 59.47 | 77.20 | 76.94 | 76.16 | 34.00 | 48.00 | 46.08 | 40.83 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 39.59 | 62.80 | 77.66 | 75.88 | 41.74 | 44.33 | 48.93 | 33.41 |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 4 | 43.11 | 59.52 | 62.66 | 58.48 | 39.17 | 39.89 | 38.17 | 34.19
    |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 29.43 | 45.04 | 61.98 | 56.99 | 30.70 | 36.74 | 39.40 | 39.81 |'
  prefs: []
  type: TYPE_TB
- en: '| BSP eftopk | 10 | 4 | 83.74 | 89.16 | 90.47 | 90.48 | 55.25 | 55.05 | 53.07
    | 47.94 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.74 | 84.80 | 88.65 | 88.94 | 54.00 | 51.58 | 52.18 | 46.60 |'
  prefs: []
  type: TYPE_TB
- en: '| 100 | 4 | 83.78 | 88.91 | 90.90 | 89.28 | 51.73 | 55.36 | 55.74 | 47.66 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.52 | 84.33 | 88.08 | 88.69 | 53.98 | 51.62 | 52.14 | 46.88 |'
  prefs: []
  type: TYPE_TB
- en: '| 1000 | 4 | 84.02 | 88.94 | 90.88 | 89.89 | 55.29 | 55.20 | 53.36 | 53.55
    |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 62.82 | 83.56 | 87.76 | 86.95 | 52.35 | 54.68 | 52.87 | 46.27 |'
  prefs: []
  type: TYPE_TB
- en: '| Local SGD $\tau=2$ | None | 4 | 84.04 | 88.97 | 92.01 | 88.86 | 55.03 | 54.94
    | 52.87 | 46.72 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 65.41 | 84.96 | 89.35 | 90.11 | 54.11 | 51.72 | 52.30 | 46.17 |'
  prefs: []
  type: TYPE_TB
- en: '| Local SGD $\tau=2$ topk | 100 | 4 | 61.04 | 81.45 | 87.26 | 87.40 | 49.23
    | 50.84 | 50.89 | 49.70 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 39.67 | 63.12 | 81.22 | 82.09 | 40.88 | 44.39 | 50.79 | 46.54 |'
  prefs: []
  type: TYPE_TB
- en: '| Local SGD $\tau=4$ | None | 4 | 83.85 | 89.61 | 90.90 | 90.00 | 51.92 | 55.05
    | 56.14 | 47.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 65.55 | 85.09 | 89.44 | 89.85 | 54.36 | 51.72 | 53.40 | 47.15 |'
  prefs: []
  type: TYPE_TB
- en: '| Local SGD $\tau=4$ topk | 100 | 4 | 60.73 | 81.18 | 85.20 | 83.83 | 50.54
    | 50.78 | 52.64 | 50.25 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 39.75 | 62.86 | 79.11 | 81.55 | 40.75 | 44.62 | 50.40 | 46.64 |'
  prefs: []
  type: TYPE_TB
- en: '| Local SGD $\tau=8$ | None | 4 | 84.58 | 89.32 | 91.20 | 90.86 | 55.25 | 55.19
    | 53.35 | 48.55 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.48 | 84.82 | 89.41 | 90.09 | 54.07 | 51.72 | 54.91 | 47.30 |'
  prefs: []
  type: TYPE_TB
- en: '| Local SGD $\tau=8$ topk | 100 | 4 | 61.82 | 82.19 | 85.46 | 84.77 | 49.02
    | 50.53 | 51.25 | 50.32 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 39.71 | 61.93 | 77.55 | 78.73 | 39.43 | 44.81 | 49.83 | 46.10 |'
  prefs: []
  type: TYPE_TB
- en: '| Local SGD $\tau=16$ | None | 4 | 84.02 | 89.25 | 90.99 | 90.56 | 55.32 |
    55.41 | 53.43 | 48.84 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.74 | 84.69 | 89.57 | 90.15 | 53.85 | 51.72 | 56.69 | 46.93 |'
  prefs: []
  type: TYPE_TB
- en: '| Local SGD $\tau=16$ topk | 100 | 4 | 61.13 | 81.60 | 84.68 | 83.27 | 49.27
    | 50.30 | 50.60 | 49.36 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 39.31 | 61.62 | 71.72 | 76.07 | 40.06 | 44.44 | 49.06 | 46.08 |'
  prefs: []
  type: TYPE_TB
- en: '| FedAvg | None | 4 | 62.41 | 84.37 | 89.81 | 90.10 | 52.20 | 55.23 | 54.91
    | 54.92 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 40.23 | 64.42 | 83.45 | 86.65 | 28.77 | 43.70 | 51.91 | 52.02 |'
  prefs: []
  type: TYPE_TB
- en: '| FedAvg topk | 4 | 4 | 59.26 | 82.23 | 89.00 | 89.84 | 51.08 | 54.88 | 55.06
    | 54.85 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 38.36 | 60.63 | 82.15 | 85.14 | 27.90 | 42.26 | 50.29 | 51.32 |'
  prefs: []
  type: TYPE_TB
- en: '| DPSGD | None | 4 | 84.60 | 89.38 | 91.08 | 91.01 | 55.41 | 55.19 | 52.92
    | 48.13 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.20 | 85.12 | 88.99 | 89.71 | 53.55 | 51.71 | 53.77 | 46.50 |'
  prefs: []
  type: TYPE_TB
- en: '| DCD-PSGD | 4 | 4 | 68.39 | 86.32 | 89.79 | 90.33 | 54.70 | 55.13 | 55.45
    | 54.07 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 44.66 | 70.57 | 85.78 | 87.17 | 37.73 | 47.26 | 54.38 | 51.83 |'
  prefs: []
  type: TYPE_TB
- en: '| CHOCO-SGD | 100 | 4 | 84.25 | 88.82 | 91.27 | 88.77 | 55.21 | 55.19 | 52.73
    | 48.10 |'
  prefs: []
  type: TYPE_TB
- en: '| 32 | 64.52 | 84.71 | 89.00 | 89.00 | 53.16 | 51.75 | 54.49 | 47.15 |'
  prefs: []
  type: TYPE_TB
- en: 7\. Scheduling of Communication and Computing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: DL models have a layer-wise structure that allows communication and computation
    tasks to be carried out in parallel during training ([zhang2017poseidon,](#bib.bib248)
    ). The parallelism of communication and computing can effectively hide the communication
    time and reduce the overall training time. The communication and computation tasks
    can be formulated into a general directed acyclic graph (DAG) ([shi2018adag,](#bib.bib176)
    ), pipelining or scheduling can be achieved to minimize iteration time.
  prefs: []
  type: TYPE_NORMAL
- en: In 2017, wait-free backward propagation (WFBP) ([zhang2017poseidon,](#bib.bib248)
    ; [awan2017s,](#bib.bib11) ) was proposed to pipeline the gradient communication
    of layer $l$ and the gradient computation of layer $l-1$, as they are independent.
    WFBP is naively supported in current DL frameworks (e.g., TensorFlow, PyTorch,
    Horovod, etc.). However, for small tensors of gradients, the latency term (startup
    time) can dominate the communication cost particularly on extremely large-scale
    clusters. To address the problem, merged-gradient (or tensor fusion) techniques
    (e.g., MG-WFBP ([Shi2018MGWFBPED,](#bib.bib170) ; [shi2021mgj,](#bib.bib171) ))
    are proposed to alleviate the negative impact of the startup time. For layer-wise
    gradient sparsification ([shi2020layer,](#bib.bib174) ; [shi2020communication,](#bib.bib177)
    ), three types of tasks (gradient computation, gradient sparsification, and gradient
    communication) can be pipelined. However, for the large tensors, the long communication
    time can cause their previous small tensors to wait. To minimize the waiting time
    of different tasks, communication and computation tasks can be scheduled by changing
    their execution order. Some studies ([jayarajan2018priority,](#bib.bib79) ; [hashemi2018tictac,](#bib.bib65)
    ; [harlap2018pipedream,](#bib.bib64) ; [peng2019generic,](#bib.bib141) ; [zhang2023accelerating,](#bib.bib250)
    ; [shi2023pipemoe,](#bib.bib172) ) have proposed scheduling communication tasks
    and computation tasks by changing the order of execution. Peng et al. ([peng2019generic,](#bib.bib141)
    ) proposed tensor partitioning to communication scheduling (even feed-forward
    computations can be paralleled with communications) to further reduce the communication
    cost. To prevent multiple communication tasks from affecting training performance,
    Wang et al. ([wang2020contention,](#bib.bib220) ) proposed communication contention
    aware scheduling of multiple deep learning training jobs on GPU clusters. The
    All-Reduce operation can be decoupled to two continuous operations such that they
    are possible to be overlapped with feed-forward and backward computations ([zhang2023accelerating,](#bib.bib250)
    ).
  prefs: []
  type: TYPE_NORMAL
- en: DL frameworks commonly employ DAGs to schedule compute operations. However,
    this approach presents challenges in managing gradient communication between workers ([276984,](#bib.bib156)
    ). Specifically, if each worker simply uses blocking All-Reduce to communicate
    gradients based on their production order, the resulting mismatch in the orders
    of produced gradients between workers can lead to issues such as deadlock, data
    corruption, or communication inefficiency ([276984,](#bib.bib156) ). To mitigate
    such issues, it is necessary to schedule a global order for collective communication
    of tensors ([276984,](#bib.bib156) ). Notably, a recent benchmark study ([MLSYS2022_cedebb6e,](#bib.bib2)
    ) compares various compression methods with and without overlapping, and demonstrates
    that the latter approach can significantly reduce training time.
  prefs: []
  type: TYPE_NORMAL
- en: 8\. Convergence Analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are some commonly used assumptions for the convergence analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Lipschitzian continuous gradient: All function $f_{i}(\cdot)$ have $L$-Lipschitzian
    gradients: $||\nabla f_{i}(\mathbf{x})-\nabla f_{i}(\mathbf{y})||\leq L\left\|\mathbf{x}-\mathbf{y}\right\|,\
    \forall\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}.$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unbiased stochastic gradient: The gradient calculated at every iteration provides
    an unbiased estimation of the gradient of $f_{i}(\mathbf{x})$: $G_{i}(\mathbf{x}):=\mathbb{E}_{\xi\sim\mathcal{D}_{i}}\nabla
    F_{i}(\mathbf{x};\xi)=\nabla f_{i}(\mathbf{x}),\ \forall\mathbf{x}\in\mathbb{R}^{n},$
    in which the $\mathcal{D}_{i}$ is the data distribution on $i$-th worker.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bounded variance: The variance of the stochastic gradient is bounded: $\mathbb{E}_{\xi\sim\mathcal{D}_{i}}\|\nabla
    F_{i}(\mathbf{x};\xi)-\nabla f_{i}(\mathbf{x})\|^{2}\leq\sigma^{2},\forall i,\forall\mathbf{x}\in\mathbb{R}^{n}.$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Bounded stochastic gradient: The second moment of the stochastic gradients
    is bounded: $\mathbb{E}_{\xi\sim\mathcal{D}_{i}}\|\nabla F_{i}(\mathbf{x};\xi)\|^{2}\leq
    M^{2},\forall i,\forall\mathbf{x}\in\mathbb{R}^{n}.$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (5)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '$\mu$-Strongly convex function: $f(\mathbf{x})\geq f(\mathbf{y})+\left\langle\nabla
    f(\mathbf{y}),\mathbf{x}-\mathbf{y}\right\rangle+\frac{\mu}{2}\left\|\mathbf{x}-\mathbf{y}\right\|^{2}.$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the gossip (peer-to-peer) algorithms, there are some extra assumptions
     ([CommCompforDecent,](#bib.bib201) ):'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Symmetric doubly stochastic matrix: The communication topology among workers
    is represented by a weighted matrix $W$ that is a real symmetric doubly stochastic
    matrix satisfying $W=W^{T}$ and $W\mathbf{1}=W$.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Spectral gap: Given the symmetric doubly stochastic matrix $W$, the spectral
    gap is defined as $\rho:=\text{max}\left\{\|\lambda_{2}(W)\|,\|\lambda_{n}(W)\|\right\}$
    where $\lambda_{2}(W)$ represent the second largest eigenvalues of $W$. The condition
    $\rho<1$ must hold.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For the compression methods, there are also some assumptions:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '$k$-contraction  ([SparSGDwithMemory,](#bib.bib188) ): For a parameter $0<d<n$,
    a $k$-contraction operator is a (possibly randomized) operator $C(\cdot)$: $\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$
    that satisfies the contraction property: $\mathbb{E}\left\|\mathbf{x}-C(\mathbf{x})\right\|^{2}\leq\left(1-\frac{d}{n}\right)\left\|\mathbf{x}\right\|^{2},\forall\mathbf{x}\in\mathbb{R}^{n}.$'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Unbiased compression  ([CommCompforDecent,](#bib.bib201) ): The stochastic
    compression operator $C(\cdot)$ is unbiased for any $\mathbf{x}$: $\mathbb{E}[C(\mathbf{x})]=\mathbf{x},$
    and the compression operators are independent on different workers or at different
    iterations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 8.1\. Centralized Architecture (PS or All-Reduce)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 8.1.1\. BSP-SGD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: PS and All-Reduce architectures have the same iteration equations because the
    All-Reduce algorithm only changes the way of implementing global synchronization.
    Therefore, the convergence analysis of BSP-SGD with the PS architecture can be
    applied to the All-Reduce architecture.
  prefs: []
  type: TYPE_NORMAL
- en: For quantization methods, Christopher et al. ([Tamingwild,](#bib.bib158) ) provided
    a convergence analysis with the martingale-based approach under both convex and
    non-convex objectives. In the case of QSGD, Alistarh et al.  ([QSGD,](#bib.bib7)
    ) not only proposed a family of quantization algorithms, but also provided a convergence
    analysis. They proved that QSGD can achieve convergence both for convex and non-convex
    objectives. They also proved that QSGD satisfies $\frac{1}{L}\mathbb{E}\left[\left\|\nabla
    f(\mathbf{x})\right\|^{2}_{2}\right]\leq O\left(\frac{\sqrt{L(f(\mathbf{x}_{1})-f^{*})}}{T}+\frac{min(n/s,\sqrt{n}/s)B}{L}\right)$,
    where $L$ represents Lipschitzian constant, $s$ and $B$ are hyper-parameters.
  prefs: []
  type: TYPE_NORMAL
- en: When implementing error accumulation in quantization methods, the variance bound
    of quantized gradients exceed that of QSGD ([ECQSGD,](#bib.bib229) ). Wu et al. ([ECQSGD,](#bib.bib229)
    ) provided a convergence analysis in this scenario, but their analysis is limited
    by the requirement of unbiased gradient compression.
  prefs: []
  type: TYPE_NORMAL
- en: In the case of sparsification methods, Alistarh et al.  ([ConvSparGrad,](#bib.bib6)
    ) theoretically proved that the Top-$k$ algorithm can achieve good convergence
    even with biased estimation and non-convex objectives. This analysis is subject
    to deceleration proportional to $k$. Extending the convergence analysis of Top-$k$
    methods to a more general range of sparsification methods, such as random-$k$
    or $k$-sparsification methods, Stich et al. ([SparSGDwithMemory,](#bib.bib188)
    ) proved that this scheme preserves the same order of convergence rate as vanilla
    SGD, i.e. $O\left(\frac{G^{2}}{\mu T}\right)$. Shi et al. ([ijcai2019473,](#bib.bib181)
    ) analyzed the gTop-k sparsification method ([GTopk,](#bib.bib179) ) theoretically.
    They proved that gTop-k S-SGD provides convergence guarantees for non-convex problems
    and has the same theoretical convergence rate as the mini-batch SGD.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.2\. SSP/Asynchronous
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Ho et al. ([MoreEffDMLviaStaleSync,](#bib.bib69) ) have established $O(1/\sqrt{T})$
    time for SGD under SSP. Lian et al. ([10.5555/2969442.2969545,](#bib.bib107) )
    proposed an ergodic convergence rate $O(1/\sqrt{T})$ and proved that the linear
    speedup can be achieved if the number of workers is bounded by $\sqrt{T}$. Alistarh
    et al.  ([Alistarh2018TheCO,](#bib.bib8) ) provided the convergence bounds for
    lock-free SGD. Moreover, they exhibit a fundamental trade-off between the delay
    of the system and the convergence rate. Zhang et al. ([zhang2018taming,](#bib.bib255)
    ) also provided a convergence rate of asynchronous SGD under a non-convex object
    function and established a unifying condition for asynchronous SGD.
  prefs: []
  type: TYPE_NORMAL
- en: 8.1.3\. Local SGD
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Stich ([Sebastian2019,](#bib.bib187) ) and Yu et al.  ([Yu2018ParallelRS,](#bib.bib243)
    ) have provided a concise convergence analysis for Local-SGD, demonstrating that
    this method has convergence guarantees with reducing communication costs. In ([NIPS2019_9288,](#bib.bib62)
    ), Haddadpour et al. strengthened the convergence analysis for Local-SGD and showed
    that it can be far less expensive and more generally applicable than the current
    theory suggests. They proved that for loss functions that satisfy the Polyak-Łojasiewicz
    condition, $O((pT)1/3)$ rounds of communication suffice to achieve linear speedup
    with an error of $O(1/pT)$.
  prefs: []
  type: TYPE_NORMAL
- en: Patel and Dieuleveut ([NIPS2019_9512,](#bib.bib47) ) proposed a non-asymptotic
    error analysis that enables the comparison to one-shot averaging and mini-batch
    averaging, providing adaptive lower bounds on the communication frequency. They
    showed that Local-SGD can reduce communication by a factor of $O(\frac{\sqrt{T}}{N^{3/2}})$.
    Artin et. al.  ([spiridonoff2021communicationefficient,](#bib.bib186) ) provided
    an interesting convergence analysis of Local-SGD under a strong convexity assumption.
    They proved that Local-SGD only needs $O(N)$ communication rounds.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2\. Decentralized Architecture (Gossip Communication)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In contrast to the centralized architecture, the gossip architecture involves
    each worker possessing an individual model. Consequently, the convergence analysis
    of the gossip architecture differs from that of the centralized architecture.
    In order to ensure consensus in both the convergence analysis and algorithm design
    of the gossip architecture, certain considerations must be made.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.1\. BSP
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Lian et al. ([CanDecent,](#bib.bib108) ) were the first to provide a theoretical
    analysis demonstrating that decentralized SGD algorithms for both convex and non-convex
    objectives can be faster than centralized SGD with less communication on the busiest
    node. They proved the convergence rate of decentralized SGD is $O\left(\frac{1}{T}+\frac{1}{\sqrt{nT}}\right)$,
    where $T$ and $n$ represents the number of iterations and workers respectively.
    When $T$ is sufficiently large, the $\frac{1}{\sqrt{nK}}$ term becomes dominant.
    In this scenario, the convergence rate can be approximated as $\frac{1}{\sqrt{nK}}$,
    indicating linear speedup achieved with the number of workers.
  prefs: []
  type: TYPE_NORMAL
- en: Considering communication compression, Tang et al.  ([CommCompforDecent,](#bib.bib201)
    ) proposed two algorithms, ECD-PSGD and DCD-PSGD, with detailed convergence analysis
    on both convex and non-convex problems. The convergence rate of DCD-PSGD is $O\left(\frac{\sigma}{\sqrt{nT}}+\frac{\zeta^{\frac{2}{3}}}{T^{\frac{2}{3}}}+\frac{1}{T}\right)$,
    in which $\sigma$ is the variance of stochastic gradient estimator, $\zeta$ is
    the variance of gradient divergence between single worker and all workers. For
    ECD-PSGD, they proved its convergence rate is $O\left(\frac{\sigma\left(1+\frac{{\tilde{\sigma}}^{2}logT}{n}\right)}{\sqrt{nT}}+\frac{\zeta^{\frac{2}{3}}\left(1+\frac{{\tilde{\sigma}}^{2}logT}{n}\right)}{T^{\frac{2}{3}}}+\frac{1}{T}+\frac{{\tilde{\sigma}}^{2}logT}{T}\right)$,
    which is slightly worse than DCD-PSGD, due to the extra terms $O\left(\frac{\sigma{\tilde{\sigma}}^{2}logT}{n\sqrt{nT}}+\frac{\zeta^{\frac{2}{3}}{\tilde{\sigma}}^{2}logT}{nT^{\frac{2}{3}}}+\frac{{\tilde{\sigma}}^{2}logT}{T}\right)$.
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that the convergence analysis of ECD-PSGD and DCD-PSGD is
    restricted to unbiased compression operators. Koloskova et al.  ([DecentStocOptimAndGossip,](#bib.bib90)
    ) proposed CHOCO-SGD, the first method that is capable of handling biased compression
    operators. For convex objectives, their algorithm was found to converge with a
    rate of $O\left(1/(nT)+1/(T\delta^{2}\omega)\right)$, where $\delta$ denotes the
    eigenvalue gap of the gossip matrix and $\omega\leq 1$ represents the compression
    ratio. Furthermore, Koloskova et al. ([Koloskova2019DecentralizedDL,](#bib.bib89)
    ) demonstrated that CHOCO-SGD ([DecentStocOptimAndGossip,](#bib.bib90) ) can achieve
    convergence at a rate of $O(1/\sqrt{nT}+n/\left(\rho^{4}\sigma^{2}T\right))$ for
    non-convex smooth functions, where $n$ denotes the number of nodes, $T$ the number
    of iterations, $\rho$ the spectral gap of the mixing matrix and $\sigma$ the compression
    ratio. Moreover, they proved that CHOCO-SGD can converge with arbitrarily high
    compression operators and achieve linear speedup.
  prefs: []
  type: TYPE_NORMAL
- en: 8.2.2\. Asynchronous
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Jeff et al. ([DBLP:journals/corr/abs-1803-05880,](#bib.bib41) ) provided a sketch
    of proof of convergence without the convergence rate. Lian et al.  ([pmlr-v80-lian18a,](#bib.bib109)
    ) proposed a theoretical analysis of asynchronous gossip SGD with the non-convex
    objective function, which converges with the $O(1/\sqrt{T})$ rate and has linear
    speedup with respect to the number of workers. Assran et al. ([StocGradPush,](#bib.bib10)
    ) provided theoretical guarantees for another asynchronous gossip SGD algorithm
    that achieves similar convergence of ([pmlr-v80-lian18a,](#bib.bib109) ).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table [9](#S8.T9 "Table 9 ‣ 8.2.2\. Asynchronous ‣ 8.2\. Decentralized Architecture
    (Gossip Communication) ‣ 8\. Convergence Analysis ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey") provides a comparison
    of different algorithms. Most algorithms show $O(\frac{1}{T})$ convergence rate
    for convex object functions and $O(\frac{1}{T})$ for non-convex object functions.
    However, the communication costs vary depending on the architecture and algorithm
    used.'
  prefs: []
  type: TYPE_NORMAL
- en: Table 9\. Summary of distributed learning algorithms
  prefs: []
  type: TYPE_NORMAL
- en: '| Arch. | Comm. | Compression | Communication Cost in Big $O$ | Convergence
    in Big $O$ | References of Method |'
  prefs: []
  type: TYPE_TB
- en: '| Server | Workers | convex | non-convex |'
  prefs: []
  type: TYPE_TB
- en: '| PS | BSP | Null | $O(32nNT)$ | $O(32NT)$ | $O(\frac{1}{T})$ ([Bottou2016OptimizationMF,](#bib.bib20)
    ) | $O(\frac{1}{\sqrt{T}})$ ([Bottou2016OptimizationMF,](#bib.bib20) ) |  ([Krizhevsky2014OneWT,](#bib.bib94)
    ; [Bradley2011,](#bib.bib23) ; [552669,](#bib.bib93) ; [375451,](#bib.bib28) )
    |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | $O(32nNT)$ | $O(bNT)$ | $O(\frac{1}{T})$ ([DisLearningDIANA,](#bib.bib130)
    )  ([AlinearSpeedupAnalysis,](#bib.bib82) ) | $O(\frac{1}{\sqrt{T}})$  ([AlinearSpeedupAnalysis,](#bib.bib82)
    ; [QSGD,](#bib.bib7) ) |  ([3LC,](#bib.bib110) )  ([FLStrategy,](#bib.bib91) ;
    [DisLearningDIANA,](#bib.bib130) ; [signSGDwithVote,](#bib.bib18) ; [TernGrad,](#bib.bib227)
    ; [Horvath2019NaturalCF,](#bib.bib73) )  ([signSGD,](#bib.bib17) )  ([ATOMO,](#bib.bib216)
    ; [NIPS2019_8598,](#bib.bib195) ; [Mishchenko201999OP,](#bib.bib131) ; [QSGD,](#bib.bib7)
    ; [scalableDisDNN,](#bib.bib190) ; [CommQuantforDataPara,](#bib.bib48) ; [EFsignSGD,](#bib.bib86)
    ) |'
  prefs: []
  type: TYPE_TB
- en: '| Spars. | $O(32nNT)$ | $O(k(\log N+32)T)$ | $O(\frac{1}{T})$ ([NIPS2019_9473,](#bib.bib78)
    ; [SparSGDwithMemory,](#bib.bib188) ) | $O(\frac{1}{\sqrt{T}})$  ([ConvSparGrad,](#bib.bib6)
    ; [ijcai2019473,](#bib.bib181) ) |  ([3LC,](#bib.bib110) ; [SparCommforDisGD,](#bib.bib5)
    )  ([FLStrategy,](#bib.bib91) ; [NIPS2019_9473,](#bib.bib78) ; [Horvath2019NaturalCF,](#bib.bib73)
    ; [ATOMO,](#bib.bib216) )  ([GradSparforDisOptim,](#bib.bib225) ; [DGC,](#bib.bib113)
    ; [CommQuantforDataPara,](#bib.bib48) ; [Zhao2019GlobalMC,](#bib.bib260) ; [GTopk,](#bib.bib179)
    ; [VarianceGradCompression,](#bib.bib208) ) |'
  prefs: []
  type: TYPE_TB
- en: '| SSP | Null | $O(32N\sum_{i}^{n}T_{i})$ | $O(32NT_{i})$ | - | $O(\frac{1}{\sqrt{T}})$
     ([MoreEffDMLviaStaleSync,](#bib.bib69) ) |  ([RevistSynSGD,](#bib.bib31) ; [MoreEffDMLviaStaleSync,](#bib.bib69)
    ) |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Spars. | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| ASP | Null | $O(32N\sum_{i}^{n}T_{i})$ | $O(32NT_{i})$ | $O(\frac{1}{T})$ ([Sebastian2019,](#bib.bib187)
    ; [10.5555/2969442.2969545,](#bib.bib107) ) | $O(\frac{1}{\sqrt{T}})$  ([zhang2018taming,](#bib.bib255)
    ) |  ([Sebastian2019,](#bib.bib187) ; [dean2012large,](#bib.bib43) ; [Meng2016AAS3060832,](#bib.bib126)
    ) |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | $O(32N\sum_{i}^{n}T_{i})$ | $O(bNT_{i})$ | $O(\frac{1}{T})$ ([NIPS2019_8694,](#bib.bib244)
    ) | $O(\frac{1}{\sqrt{T}})$ ([QSGD,](#bib.bib7) ; [1bit,](#bib.bib167) ) |  ([NIPS2019_8694,](#bib.bib244)
    ; [QSGD,](#bib.bib7) ; [1bit,](#bib.bib167) ; [NIPS2019_9610,](#bib.bib14) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Spars. | $O(32N\sum_{i}^{n}T_{i})$ | $O(k(\log N+32)T_{i})$ | $O(\frac{1}{T})$ ([GradSparforDisOptim,](#bib.bib225)
    ) | - |  ([GradSparforDisOptim,](#bib.bib225) ; [AsynDisMLspar,](#bib.bib58) ;
    [Meng2016AAS3060832,](#bib.bib126) ; [NIPS2019_9610,](#bib.bib14) ) |'
  prefs: []
  type: TYPE_TB
- en: '| L-SGD | Null | $O(32N\frac{T}{\tau})$ | $O(32N\frac{T}{\tau})$ | $O(\frac{1}{T})$ ([Sebastian2019,](#bib.bib187)
    ) | $O(\frac{1}{\sqrt{T}})$  ([NIPS2019_9288,](#bib.bib62) ; [pmlrv97yu19d,](#bib.bib242)
    ; [pmlrv97yu19c,](#bib.bib241) ) |  ([PSGD,](#bib.bib263) ; [Sebastian2019,](#bib.bib187)
    )  ([NIPS2019_9288,](#bib.bib62) ; [Lin2018DontUL,](#bib.bib111) ; [pmlrv97yu19d,](#bib.bib242)
    ; [pmlrv97yu19c,](#bib.bib241) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | $O(32N\frac{T}{\tau})$ | $O(bN\frac{T}{\tau})$ | - | $O(\frac{1}{\sqrt{T}})$ ([AlinearSpeedupAnalysis,](#bib.bib82)
    ) |  ([SparseTernaryCompressionSTC,](#bib.bib162) ; [AlinearSpeedupAnalysis,](#bib.bib82)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Spars. | $O(32N\frac{T}{\tau})$ | $O(k(\log N+32)\frac{T}{\tau})$ | - | -
    |  ([SparseTernaryCompressionSTC,](#bib.bib162) ; [NIPS2019_9610,](#bib.bib14)
    ) |'
  prefs: []
  type: TYPE_TB
- en: '| A.R. | BSP | Null | - | $O(32NT)$ | $O(\frac{1}{T})$ ([Bottou2016OptimizationMF,](#bib.bib20)
    ) | $O(\frac{1}{\sqrt{T}})$ ([Bottou2016OptimizationMF,](#bib.bib20) ) |  ([Krizhevsky2014OneWT,](#bib.bib94)
    ; [Bradley2011,](#bib.bib23) ; [552669,](#bib.bib93) ; [375451,](#bib.bib28) )
    |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | - | $O(bNT)$ | $O(\frac{1}{T})$ ([DisLearningDIANA,](#bib.bib130)
    )  ([AlinearSpeedupAnalysis,](#bib.bib82) ) | $O(\frac{1}{\sqrt{T}})$  ([AlinearSpeedupAnalysis,](#bib.bib82)
    ; [QSGD,](#bib.bib7) ) |  ([3LC,](#bib.bib110) )  ([FLStrategy,](#bib.bib91) ;
    [DisLearningDIANA,](#bib.bib130) ; [signSGDwithVote,](#bib.bib18) ; [TernGrad,](#bib.bib227)
    ; [Horvath2019NaturalCF,](#bib.bib73) )  ([signSGD,](#bib.bib17) )  ([ATOMO,](#bib.bib216)
    ; [NIPS2019_8598,](#bib.bib195) ; [Mishchenko201999OP,](#bib.bib131) ; [QSGD,](#bib.bib7)
    ; [scalableDisDNN,](#bib.bib190) ; [CommQuantforDataPara,](#bib.bib48) ; [EFsignSGD,](#bib.bib86)
    ) |'
  prefs: []
  type: TYPE_TB
- en: '| Spars. | - | $O(kn(\log N+32)T)$ | $O(\frac{1}{T})$ ([NIPS2019_9473,](#bib.bib78)
    ; [SparSGDwithMemory,](#bib.bib188) ) | $O(\frac{1}{\sqrt{T}})$  ([ConvSparGrad,](#bib.bib6)
    ; [ijcai2019473,](#bib.bib181) ) |  ([3LC,](#bib.bib110) ; [SparCommforDisGD,](#bib.bib5)
    )  ([FLStrategy,](#bib.bib91) ; [NIPS2019_9473,](#bib.bib78) ; [Horvath2019NaturalCF,](#bib.bib73)
    ; [ATOMO,](#bib.bib216) )  ([GradSparforDisOptim,](#bib.bib225) ; [DGC,](#bib.bib113)
    ; [CommQuantforDataPara,](#bib.bib48) ; [Zhao2019GlobalMC,](#bib.bib260) ; [GTopk,](#bib.bib179)
    ; [VarianceGradCompression,](#bib.bib208) ) |'
  prefs: []
  type: TYPE_TB
- en: '| L-SGD | Null | - | $O(32N\frac{T}{\tau})$ | $O(\frac{1}{T})$ ([Sebastian2019,](#bib.bib187)
    ) | $O(\frac{1}{\sqrt{T}})$  ([NIPS2019_9288,](#bib.bib62) ; [pmlrv97yu19d,](#bib.bib242)
    ; [pmlrv97yu19c,](#bib.bib241) ) |  ([PSGD,](#bib.bib263) ; [Sebastian2019,](#bib.bib187)
    )  ([NIPS2019_9288,](#bib.bib62) ; [pmlrv97yu19d,](#bib.bib242) ; [Lin2018DontUL,](#bib.bib111)
    ; [pmlrv97yu19d,](#bib.bib242) ; [pmlrv97yu19c,](#bib.bib241) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | - | $O(bN\frac{T}{\tau})$ | - | $O(\frac{1}{\sqrt{T}})$ ([AlinearSpeedupAnalysis,](#bib.bib82)
    ) |  ([SparseTernaryCompressionSTC,](#bib.bib162) ; [AlinearSpeedupAnalysis,](#bib.bib82)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Spars. | - | $O(kn(\log N+32)\frac{T}{\tau})$ | - | - |  ([SparseTernaryCompressionSTC,](#bib.bib162)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Gossip | BSP | Null | - | $O(32Nn_{peers}T)$ |  | $O(\frac{1}{\sqrt{T}})$ ([CanDecent,](#bib.bib108)
    ) |  ([CanDecent,](#bib.bib108) ; [StocGradPush,](#bib.bib10) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | - | $O(nbN_{peers}T)$ | - | $O(\frac{1}{\sqrt{T}})$ ([ECQSGD,](#bib.bib229)
    ) |  ([CommCompforDecent,](#bib.bib201) ; [DecentStocOptimAndGossip,](#bib.bib90)
    ; [7544448,](#bib.bib144) ; [NIPS2018_7705,](#bib.bib68) ; [ECQSGD,](#bib.bib229)
    ; [NIPS2019_9047,](#bib.bib151) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Spars. | - | $O(k(\log N+32)n_{peers}T)$ | - | - |  ([DecentStocOptimAndGossip,](#bib.bib90)
    ; [tang2020communication,](#bib.bib204) ) |'
  prefs: []
  type: TYPE_TB
- en: '| ASP | Null | - | $O(32Nn_{peers}T_{i})$ | - | $O(\frac{1}{\sqrt{T}})$  ([pmlr-v80-lian18a,](#bib.bib109)
    ) |  ([pmlr-v80-lian18a,](#bib.bib109) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Spars. | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| L-SGD | Null | - | $O(32Nn_{peers}\frac{T}{\tau})$ | $O(\frac{1}{T})$  ([Jianyu180807576,](#bib.bib218)
    ) | $O(\frac{1}{\sqrt{T}})$  ([Jianyu180807576,](#bib.bib218) ) |  ([Jianyu180807576,](#bib.bib218)
    ) |'
  prefs: []
  type: TYPE_TB
- en: '| Quant. | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Spars. | - | - | - | - | - |'
  prefs: []
  type: TYPE_TB
- en: '| Pipelining |  ([zhang2017poseidon,](#bib.bib248) ; [awan2017s,](#bib.bib11)
    ; [Shi2018MGWFBPED,](#bib.bib170) ; [shi2020layer,](#bib.bib174) ; [shi2020communication,](#bib.bib177)
    ; [aritra2020discrepancy,](#bib.bib50) ) |'
  prefs: []
  type: TYPE_TB
- en: '| Scheduling |  ([Shi2018MGWFBPED,](#bib.bib170) ; [hashemi2018tictac,](#bib.bib65)
    ; [harlap2018pipedream,](#bib.bib64) ; [peng2019generic,](#bib.bib141) ; [jayarajan2018priority,](#bib.bib79)
    ; [DBLP:journals/corr/abs-1810-08313,](#bib.bib217) ; [8884800,](#bib.bib99) )
    |'
  prefs: []
  type: TYPE_TB
- en: •
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Notes: 1) The “Arch.” indicates the architecture supported in the original
    paper, “A.R.” represents All-Reduce, “Quant.” quantization and “Spars.” sparsification.
    The “Comm.” column indicates the communication scheme which includes “BSP” (Bulk
    Synchronous Parallel), “SSP” (Stale Synchronous Parallel), “ASP” (ASynchronous
    Parallel), and “L-SGD”(Local SGD), and $\tau$ in “L-SGD” means the local steps.
    2) Some methods use both compression techniques and both Asyc and Local SGD. 3)
    Some methods also optimize download communication, we have listed them together
    with upload communication. 4) The communication complexity and convergence rate
    of some paper maybe different, we just list out the common ones. 5) Pipelining
    and scheduling can be used in many methods, so we only list methods that uses
    pipeline without classifying it into any category.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 9\. Auxiliary Technologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Communication compression methods can achieve convergence and reduce communication
    loads with the aid of auxiliary technologies.
  prefs: []
  type: TYPE_NORMAL
- en: 9.1\. Error Accumulation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In 1-bit SGD  ([1bit,](#bib.bib167) ), the current quantization error is added
    to the next mini-batch gradient before quantization in the next iteration through
    error accumulation. This error-feedback mechanism ensures that all gradients are
    eventually added up into the model with full accuracy, although with some delays.
    This process is described by Eq. ([9](#S5.E9 "In 5\. Quantization methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")), ([10](#S5.E10
    "In 5\. Quantization methods ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey")) and ([11](#S5.E11 "In 5\. Quantization
    methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")). Karimireddy et al.  ([EFsignSGD,](#bib.bib86) ) proposed EF-SIGNSGD
    (SIGNSGD with Error-Feedback), which also uses error accumulation by storing the
    error locally and adding it to the next step.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Numerous formulations of error accumulation have been proposed in the literature
     ([3LC,](#bib.bib110) ; [SparCommforDisGD,](#bib.bib5) ; [sparsebinarycompression,](#bib.bib163)
    ; [Adacomp,](#bib.bib29) ; [CommQuantforDataPara,](#bib.bib48) ; [DisLearningDIANA,](#bib.bib130)
    ; [ECQSGD,](#bib.bib229) ; [EFsignSGD,](#bib.bib86) ; [SparseTernaryCompressionSTC,](#bib.bib162)
    ; [scalableDisDNN,](#bib.bib190) ; [SparCommforDisGD,](#bib.bib5) ; [VarianceGradCompression,](#bib.bib208)
    ; [DEF,](#bib.bib232) ; [9442310,](#bib.bib215) ; [pmlr-v139-tang21a,](#bib.bib200)
    ; [EF21,](#bib.bib153) ). In summary, error accumulation involves incorporating
    the quantization error into the subsequent gradient computations, which improves
    the accuracy of the final model. This technique can be formulated as following
    steps: (1) gradient compression: $C_{i,t}=Sparse(v_{i,t-1}+\nabla_{i,t})$; (2)
    error accumulation $v_{i,t}=\nabla_{i,t}-C_{i,t}$; (3) update weight: $x_{t+1}=x_{t}-\gamma\frac{1}{n}\sum_{i=1}^{n}C_{i,t}$,
    where $C_{i,t}$ represents the updates which are compressed by any compression
    method $Sparse(\cdot)$, $\nabla$ the gradient, at $t$-th iteration and in worker
    $i$.'
  prefs: []
  type: TYPE_NORMAL
- en: Wu et al. ([ECQSGD,](#bib.bib229) ) proposed ECQ-SGD (Error Compensated Quantized
    SGD). This method differs from other compression techniques as it considers not
    only the compression error in the current iteration but also all previous errors
    by accumulating them. Tang et al. proposed 1-bit Adam ([pmlr-v139-tang21a,](#bib.bib200)
    ), which incorporates error compensation into distributed Adam with 1-bit compression.
  prefs: []
  type: TYPE_NORMAL
- en: 9.2\. Momentum Correction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Lin et al. ([DGC,](#bib.bib113) ) proposed the Momentum Correction technique
    to assist DGC in utilizing Momentum SGD. This approach involves the application
    of vanilla momentum SGD ([vanillaMomentum,](#bib.bib145) ) and error accumulation
    to the sparse gradient. The update process is as follows: (1) momentum accumulation:
    $u_{i,t}=mu_{i,t-1}+\nabla_{i,t}$; (2) error accumulation $v_{i,t}=v_{i,t-1}+u_{i,t}$;
    (3) update weight: $x_{t+1}=x_{t}-\gamma\sum_{i=1}^{n}sparse(v_{i,t})$, where
    $m$ denotes the coefficient of momentum and $u_{i,t}$ denotes the momentum at
    $t$-th iteration on worker $i$. Momentum Correction has also been exploited in
     ([sparsebinarycompression,](#bib.bib163) ; [DisLearningDIANA,](#bib.bib130) ;
    [SparseTernaryCompressionSTC,](#bib.bib162) ).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Zhao et al. ([Zhao2019GlobalMC,](#bib.bib260) ) proposed Global Momentum Compression,
    of which the update process becomes: (1) momentum accumulation: $u_{i,t}=\nabla_{i,t}-m(x_{t}-x_{t-1})$;
    (2) error accumulation: $v_{i,t}=v_{i,t-1}+u_{i,t}-sparse(v_{i,t-1}+u_{i,t})$;
    (3) update weight: $x_{t+1}=x_{t}-\gamma\sum_{i=1}^{n}sparse(v_{i,t-1}+u_{i,t})$.
    Thus, the gradient and them momentum are both compressed.'
  prefs: []
  type: TYPE_NORMAL
- en: 9.3\. Low-rank Decomposition
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The low-rand decomposition of matrix means to decompose a large matrix into
    the multiplication of small matrices. Thus, senders can reduce communication costs
    by sending smaller matrices, and recovering original matrix on receivers.
  prefs: []
  type: TYPE_NORMAL
- en: Wang et al. ([ATOMO,](#bib.bib216) ) developed a new method named Atomic Sparsification
    (ATOMO). They demonstrated that gradient sparsification and quantization are parts
    of a general approach that sparsifies the gradients in some atomic decomposition,
    such as entry-wise methods like QSGD, singular value decomposition (SVD), Fourier
    decomposition, etc. ATOMO aims to minimize the variance of the sparsified gradient
    that is sparse on the atomic basis and maintain it as an unbiased estimator of
    the original gradient. They illustrate that the 1-bit QSGD and TernGrad are special
    cases of ATOMO. Furthermore, they improved ATOMO with SVD, named as Spectral-ATOMO.
    In their experiments, Spectral-ATOMO reduces the training time by a factor of
    $2\times$ and $3\times$, compared to QSGD and TernGrad, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Ivkin et al.  ([NIPS2019_9473,](#bib.bib78) ) exploited a technique that is
    widely adopted in distributed systems, Count Sketch  ([Charikar2002FFI,](#bib.bib27)
    ). It compresses a gradient vector $G$ into a sketch $S(G)$ of size $O(1/\epsilon\log
    n)$., which can approximate every coordinate of $G$ and the $l_{2}$ norm of the
    entire gradient. Every worker sends this sketched gradient to the server, and
    the server recovers the $d$ largest coordinates of the sum of gradients and then
    performs the update.
  prefs: []
  type: TYPE_NORMAL
- en: 9.4\. Local Gradient Clipping
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Gradient Clipping  ([GradientClipping,](#bib.bib16) ) is a widely used method
    in vanilla SGD to avoid the exploding gradient problem. This technique involves
    clipping all gradients that have values exceeding a user-defined threshold. For
    the BSP-SGD with gradient sparsification, Lin et al. ([DGC,](#bib.bib113) ) modified
    it as Local Gradient Clipping, which is performed before adding the error accumulation
    term and the gradient in current iteration. The $k$-th worker has a threshold
    $thr\left(G_{k}\right)$ for its local gradient $G_{k}$, and the aggregation of
    gradients has a threshold $thr\left(G\right)$ for the global gradient $G:=\sum_{k=1}^{N}G_{k}$.
  prefs: []
  type: TYPE_NORMAL
- en: If we assume that all $N$ workers have an independent and identically distributed
    (IID) gradient distribution with variance $\sigma^{2}$, then the aggregation of
    all gradients have the variance $N\sigma^{2}$. So there are
  prefs: []
  type: TYPE_NORMAL
- en: '| (14) |  | $E\left[\left\&#124;G_{k}\right\&#124;_{2}\right]\approx\sigma,\
    E\left[\left\&#124;G\right\&#124;_{2}\right]\approx N^{1/2}\sigma.$ |  |'
  prefs: []
  type: TYPE_TB
- en: Local Gradient Clipping can restore the original variance of the aggregation
    models by adjusting the clipping threshold with respect to the number of workers.
  prefs: []
  type: TYPE_NORMAL
- en: 9.5\. Warm-up Training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Lin et al.([DGC,](#bib.bib113) ) utilized Warm-up Training ([imagenet1hour,](#bib.bib57)
    ) in DGC to overcome the problem of rapidly varying neural network behavior in
    the first few epochs of training when the gradient values are excessively large.
    This technique involves dividing the training process into two periods: the warm-up
    period and the normal training period. During the warm-up period, the algorithm
    trains the model using a less aggressive learning rate and less aggressive gradient
    sparsity. This reduces the number of extreme gradients being delayed. In addition,
    the gradient sparsity increases exponentially from a small value to the final
    value. Subsequently, the algorithm trains the model using high sparsity and a
    decreasing learning rate, similar to vanilla SGD.'
  prefs: []
  type: TYPE_NORMAL
- en: 10\. Conclusion and Future Directions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this survey, we provided a comprehensive introduction to the communication-efficient
    distributed DL algorithms. We summarized the taxonomy of distributed DL and classified
    the communication-efficient distributed training algorithms into four main dimensions:
    1) synchronous schemes, 2) system architectures, 3) compression techniques, and
    4) parallelism of communication and computing tasks. For each dimension, related
    techniques that address communication problems were introduced comprehensively.
    Furthermore, we provided a review of convergence bounds of different algorithms
    and some auxiliary techniques that help accelerate the training speed.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Below, we summarize some challenges and future directions:'
  prefs: []
  type: TYPE_NORMAL
- en: (1)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Will current communication-efficient methods work in training foundation models?
    With current rapid developments of large language models, efficient training methods
    are crucially important for developing new techniques, democratizing them and
    energy saving. Thus, verifying current and designing new communication-efficient
    distributed training methods will be valuable.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (2)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Higher compression level. Is it possible to implement an extremely high compression
    level without sacrificing training performance? While the current quantization
    method can reduce data size by a factor of 32 and sparsification by a factor of
    100-1000, achieving a higher compression ratio while maintaining model accuracy
    remains a challenging question.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (3)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adaptive Compression. Gradient/model compression can reduce communication size
    and time. However, achieving a very high compression ratio typically requires
    a larger number of iterations to reach the target optimization error, making it
    challenging to balance compression ratio and convergence speed. Can different
    compression ratios be set for different layers/tensors in a deep model or for
    peers with varying network bandwidth to achieve optimal system throughput?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: (4)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fault-tolerant algorithms. While the algorithm runs smoothly in a stable computing
    cluster, uncertainty arises when a large number of heterogeneous devices are used
    to train deep models. Factors such as severe stragglers, network congestion, and
    worker failure may cause interference. Developing more fault-tolerant algorithms
    is an important direction for increasing the reliability of training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
    G. Irving, M. Isard, M. Kudlur, et al. Tensorflow: A system for large-scale machine
    learning. In OSDI, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[2] S. Agarwal, H. Wang, S. Venkataraman, and D. Papailiopoulos. On the utility
    of gradient compression in distributed training systems. In MLSys, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[3] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. J. Smola. Scalable
    inference in latent variable models. In WSDM, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[4] S. Ahn, B. Shahbaba, and M. Welling. Distributed stochastic gradient mcmc.
    In ICML, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[5] A. F. Aji and K. Heafield. Sparse communication for distributed gradient
    descent. In EMNLP, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[6] D. Alistarh, T. Hoefler, M. Johansson, S. Khirirat, N. Konstantinov, and
    C. Renggli. The convergence of sparsified gradient methods. In NeurIPS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[7] D. Alistarh, J. Li, R. Tomioka, and M. Vojnovic. Qsgd: Randomized quantization
    for communication-optimal stochastic gradient descent. ArXiv, abs/1610.02132,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[8] D. Alistarh, C. D. Sa, and N. Konstantinov. The convergence of stochastic
    gradient descent in asynchronous shared memory. In PODC ’18, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[9] D. Amodei, S. Ananthanarayanan, R. Anubhai, et al. Deep speech 2: End-to-end
    speech recognition in english and mandarin. In ICML, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[10] M. Assran, N. Loizou, N. Ballas, and M. Rabbat. Stochastic gradient push
    for distributed deep learning. In ICML, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[11] A. A. Awan, K. Hamidouche, J. M. Hashmi, and D. K. Panda. S-caffe: Co-designing
    mpi runtimes and caffe for scalable deep learning on modern GPU clusters. In Acm
    Sigplan Notices, volume 52, pages 193–205\. ACM, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[12] N. S. Aybat, Z. Wang, T. Lin, and S. Ma. Distributed linearized alternating
    direction method of multipliers for composite convex consensus optimization. IEEE
    Transactions on Automatic Control, 63(1):5–20, Jan 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[13] J. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. ArXiv, abs/1607.06450,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[14] D. Basu, D. Data, C. Karakus, and S. Diggavi. Qsparse-local-SGD: Distributed
    SGD with quantization, sparsification and local computations. In NeurIPS. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[15] T. Ben-Nun and T. Hoefler. Demystifying parallel and distributed deep
    learning: An in-depth concurrency analysis. ACM Comput. Surv., 52(4), Aug. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[16] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies
    with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166,
    March 1994.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[17] J. Bernstein, Y. Wang, K. Azizzadenesheli, and A. Anandkumar. SIGNSGD:
    compressed optimisation for non-convex problems. In ICML, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[18] J. Bernstein, J. Zhao, K. Azizzadenesheli, and A. Anandkumar. signsgd
    with majority vote is communication efficient and byzantine fault tolerant. ArXiv,
    abs/1810.05291, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[19] A. S. Bijral, A. D. Sarwate, and N. Srebro. On data dependence in distributed
    stochastic optimization. arXiv: Optimization and Control, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[20] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale
    machine learning. SIAM Review, 60:223–311, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[21] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. Gossip algorithms: design,
    analysis and applications. In Proceedings IEEE 24th Annual Joint Conference of
    the IEEE Computer and Communications Societies., 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[22] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. Randomized gossip algorithms.
    IEEE/ACM Trans. Netw., 14(SI):2508–2530, June 2006.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[23] J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. Parallel coordinate
    descent for l1-regularized loss minimization. In ICML, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[24] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners.
    arXiv preprint arXiv:2005.14165, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[25] J. Bruck, Ching-Tien Ho, S. Kipnis, E. Upfal, and D. Weathersby. Efficient
    algorithms for all-to-all communications in multiport message-passing systems.
    IEEE TPDS, 8(11):1143–1156, Nov 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[26] R. Carli, F. Fagnani, P. Frasca, and S. Zampieri. Gossip consensus algorithms
    via quantized communication. Automatica, 46(1):70–80, Jan. 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[27] M. Charikar, K. Chen, and M. Farach-Colton. Finding frequent items in
    data streams. In Proceedings of the 29th International Colloquium on Automata,
    Languages and Programming, ICALP ’02, 2002.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[28] T. Cheatham, A. Fahmy, D. C. Stefanescu, and L. G. Valiant. Bulk synchronous
    parallel computing-a paradigm for transportable software. In Proceedings of the
    Twenty-Eighth Annual Hawaii International Conference on System Sciences, volume 2,
    pages 268–275 vol.2, Jan 1995.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[29] C. Chen, J. Choi, D. Brand, A. Agrawal, W. Zhang, and K. Gopalakrishnan.
    Adacomp : Adaptive residual gradient compression for data-parallel distributed
    training. In AAAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[30] C. Chen, W. Wang, and B. Li. Round-robin synchronization: Mitigating communication
    bottlenecks in parameter servers. In IEEE INFOCOM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[31] J. Chen, R. Monga, S. Bengio, and R. Jozefowicz. Revisiting distributed
    synchronous sgd. In ICLR Workshop Track, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[32] K. Chen and Q. Huo. Scalable training of deep learning machines by incremental
    block training with intra-block parallel optimization and blockwise model-update
    filtering. In ICASSP-2016, March 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[33] T. Chen, G. Giannakis, T. Sun, and W. Yin. Lag: Lazily aggregated gradient
    for communication-efficient distributed learning. In NeurIPS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[34] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
    and E. Shelhamer. cuDNN: Efficient primitives for deep learning. arXiv preprint
    arXiv:1410.0759, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[35] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. Project adam:
    Building an efficient and scalable deep learning training system. In OSDI, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[36] C.-H. Chu, X. Lu, A. A. Awan, H. Subramoni, J. Hashmi, B. Elton, and D. K.
    Panda. Efficient and scalable multi-source streaming broadcast on GPU clusters
    for deep learning. In ICPP, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[37] I. Colin, A. Bellet, J. Salmon, and S. Clémençon. Gossip dual averaging
    for decentralized optimization of pairwise functions. In ICML, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[38] H. Cui, J. Cipar, Q. Ho, J. K. Kim, S. Lee, A. Kumar, J. Wei, W. Dai,
    G. R. Ganger, P. B. Gibbons, G. A. Gibson, and E. P. Xing. Exploiting bounded
    staleness to speed up big data analytics. In Proceedings of the 2014 USENIX Conference
    on USENIX Annual Technical Conference, USENIX ATC’14, pages 37–48, Berkeley, CA,
    USA, 2014. USENIX Association.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[39] H. Cui, A. Tumanov, J. Wei, L. Xu, W. Dai, J. Haber-Kucharsky, Q. Ho,
    G. R. Ganger, P. B. Gibbons, G. A. Gibson, and E. P. Xing. Exploiting iterative-ness
    for parallel ml computations. In ACM SOCC, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[40] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and E. P. Xing. Geeps:
    Scalable deep learning on distributed gpus with a gpu-specialized parameter server.
    In EuroSys, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[41] J. Daily, A. Vishnu, C. Siegel, T. Warfel, and V. Amatya. Gossipgrad:
    Scalable deep learning using gossip communication based asynchronous gradient
    descent. CoRR, abs/1803.05880, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[42] L. Dalcín, R. Paz, M. Storti, and J. D’Elía. Mpi for python: Performance
    improvements and mpi-2 extensions. Journal of Parallel and Distributed Computing,
    2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[43] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato,
    A. Senior, P. Tucker, K. Yang, et al. Large scale distributed deep networks. In
    NeurIPS, 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[44] A. Defazio and L. Bottou. On the ineffectiveness of variance reduced optimization
    for deep learning. In NeurIPS. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[45] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet:
    A large-scale hierarchical image database. In CVPR, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[46] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of
    deep bidirectional transformers for language understanding. In NAACL, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[47] A. Dieuleveut and K. K. Patel. Communication trade-offs for local-sgd
    with large step size. In NeurIPS. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[48] N. Dryden, S. A. Jacobs, T. Moon, and B. Van Essen. Communication quantization
    for data-parallel training of deep neural networks. In Proceedings of the Workshop
    on MLHPC, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[49] C. Dünner, T. P. Parnell, and M. Jaggi. Efficient use of limited-memory
    accelerators for linear learning on heterogeneous systems. In NeurIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[50] A. Dutta, E. H. Bergou, A. M. Abdelmoniem, C.-Y. Ho, A. N. Sahu, M. Canini,
    and P. Kalnis. On the discrepancy between the theoretical analysis and practical
    implementations of compressed communication for distributed deep learning. In
    AAAI, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[51] F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, and A. Ramezani-Kebrya.
    Adaptive gradient quantization for data-parallel sgd. 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[52] F. Fagnani and S. Zampieri. Randomized consensus algorithms over large
    scale networks. JSAC, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[53] J. Fei, C.-Y. Ho, A. N. Sahu, M. Canini, and A. Sapio. Efficient sparse
    collective communication and its application to accelerate distributed deep learning.
    In ACM SIGCOMM, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[54] W. M. Goodall. Television by pulse code modulation. The Bell System Technical
    Journal, 30(1):33–49, Jan 1951.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[55] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. The MIT Press,
    2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[56] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
    A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: Training imagenet
    in 1 hour. arXiv preprint arXiv:1706.02677, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[57] P. Goyal, P. Dollár, R. B. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
    A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: Training imagenet
    in 1 hour. ArXiv, abs/1706.02677, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[58] D. Grishchenko, F. Iutzeler, J. Malick, and M.-R. Amini. Asynchronous
    distributed learning with sparse communications and identification. ArXiv, abs/1812.03871,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[59] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, and
    S. Lu. BCube: a high performance, server-centric network architecture for modular
    data centers. In ACM SIGCOMM, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[60] Y. Guo. A survey on methods and theories of quantized neural networks.
    CoRR, abs/1808.04752, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[61] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan. Deep learning
    with limited numerical precision. In ICML, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[62] F. Haddadpour, M. M. Kamani, M. Mahdavi, and V. Cadambe. Local sgd with
    periodic averaging: Tighter analysis and adaptive synchronization. In NeurIPS,
    pages 11080–11092\. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[63] P. Han, S. Wang, and K. K. Leung. Adaptive gradient sparsification for
    efficient federated learning: An online learning approach. arXiv preprint arXiv:2001.04756,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[64] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger,
    and P. Gibbons. Pipedream: Fast and efficient pipeline parallel DNN training.
    arXiv preprint arXiv:1806.03377, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[65] S. H. Hashemi, S. A. Jyothi, and R. H. Campbell. Tictac: Accelerating
    distributed deep learning with communication scheduling. In In Proceedings of
    Systems and Machine Learning (SysML), 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[66] C. He, S. Li, J. So, M. Zhang, H. Wang, X. Wang, P. Vepakomma, A. Singh,
    H. Qiu, L. Shen, P. Zhao, Y. Kang, Y. Liu, R. Raskar, Q. Yang, M. Annavaram, and
    S. Avestimehr. Fedml: A research library and benchmark for federated machine learning.
    arXiv preprint arXiv:2007.13518, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[67] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
    recognition. In CVPR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[68] L. He, A. Bian, and M. Jaggi. Cola: Decentralized linear learning. In
    S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
    editors, NeurIPS, pages 4536–4546\. 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[69] Q. Ho, J. Cipar, H. Cui, J. K. Kim, S. Lee, P. B. Gibbons, G. A. Gibson,
    G. R. Ganger, and E. P. Xing. More effective distributed ml via a stale synchronous
    parallel parameter server. In NeurIPS, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[70] T. Hoefler, W. Gropp, R. Thakur, and J. L. Träff. Toward performance models
    of mpi implementations for understanding application scaling issues. In EuroMPI’10,
    2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[71] E. Hoffer, I. Hubara, and D. Soudry. Train longer, generalize better:
    Closing the generalization gap in large batch training of neural networks. In
    NeurIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[72] M. Höhfeld and S. E. Fahlman. Probabilistic rounding in neural network
    learning with limited precision. Neurocomputing, 4:291–299, 1992.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[73] S. Horvath, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, and P. Richtárik.
    Natural compression for distributed deep learning. ArXiv, abs/1905.10988, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[74] C.-J. Hsieh, H.-F. Yu, and I. S. Dhillon. Passcode: Parallel asynchronous
    stochastic dual co-ordinate descent. In ICML, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[75] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam,
    Q. V. Le, Y. Wu, et al. Gpipe: Efficient training of giant neural networks using
    pipeline parallelism. NeurIPS, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[76] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Quantized
    neural networks: Training neural networks with low precision weights and activations.
    J. Mach. Learn. Res., 18(1):6869–6898, Jan. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[77] F. Iutzeler, P. Bianchi, P. Ciblat, and W. Hachem. Asynchronous distributed
    optimization using a randomized alternating direction method of multipliers. In
    52nd IEEE Conference on Decision and Control, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[78] N. Ivkin, D. Rothchild, E. Ullah, V. braverman, I. Stoica, and R. Arora.
    Communication-efficient distributed sgd with sketching. In NeurIPS. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[79] A. Jayarajan, J. Wei, G. Gibson, A. Fedorova, and G. Pekhimenko. Priority-based
    parameter propagation for distributed dnn training. In In Proceedings of Systems
    and Machine Learning (SysML), 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[80] D. Jhunjhunwala, A. Gadhikar, G. Joshi, and Y. C. Eldar. Adaptive quantization
    of model updates for communication-efficient federated learning. In ICASSP. IEEE,
    2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[81] X. Jia, S. Song, S. Shi, W. He, Y. Wang, H. Rong, F. Zhou, L. Xie, Z. Guo,
    Y. Yang, L. Yu, T. Chen, G. Hu, and X. Chu. Highly scalable deep learning training
    system with mixed-precision: Training ImageNet in four minutes. In Proc. of Workshop
    on Systems for ML and Open Source Software, collocated with NeurIPS 2018, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[82] P. Jiang and G. Agrawal. A linear speedup analysis of distributed deep
    learning with sparse and quantized communication. In NeurIPS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[83] Z. Jiang, W. Wang, B. Li, and B. Li. Pisces: efficient federated learning
    via guided asynchronous training. In SoCC, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[84] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
    S. Bates, S. Bhatia, N. Boden, A. Borchers, et al. In-datacenter performance analysis
    of a tensor processing unit. In ISCA, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[85] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
    K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al. Advances and open problems
    in federated learning. arXiv preprint arXiv:1912.04977, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[86] S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi. Error feedback
    fixes SignSGD and other gradient compression schemes. In ICML, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[87] D. Kempe, A. Dobra, and J. Gehrke. Gossip-based computation of aggregate
    information. In 44th Annual IEEE Symposium on Foundations of Computer Science,
    2003\. Proceedings., pages 482–491, Oct 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[88] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang.
    On large-batch training for deep learning: Generalization gap and sharp minima.
    In ICLR, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[89] A. Koloskova, T. Lin, S. U. Stich, and M. Jaggi. Decentralized deep learning
    with arbitrary communication compression. ArXiv, abs/1907.09356, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[90] A. Koloskova, S. Stich, and M. Jaggi. Decentralized stochastic optimization
    and gossip algorithms with compressed communication. In ICML, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[91] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and D. Bacon.
    Federated learning: Strategies for improving communication efficiency. In NeurIPS
    Workshop on Private Multi-Party Machine Learning, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[92] J. Konečný and P. Richtárik. Randomized distributed mean estimation: Accuracy
    vs. communication. Frontiers in Applied Mathematics and Statistics, 4:62, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[93] R. Krizanc and A. Saarimaki. Bulk synchronous parallel: practical experience
    with a model for parallel computing. In Proceedings of the 1996 Conference on
    Parallel Architectures and Compilation Technique, pages 208–217, Oct 1996.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[94] A. Krizhevsky. One weird trick for parallelizing convolutional neural
    networks. ArXiv, abs/1404.5997, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[95] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian institute for
    advanced research). URL http://www.cs.toronto.edu/kriz/cifar.html, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[96] G. Lan, S. Lee, and Y. Zhou. Communication-efficient algorithms for decentralized
    and stochastic optimization. CoRR, abs/1701.03961, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[97] J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated
    gradient. J. Mach. Learn. Res., 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[98] T. T. Lau, J. Zeng, B. Wu, and Y. Yao. A proximal block coordinate descent
    algorithm for deep neural network training. In ICLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[99] W. Lee, Y. Lee, J. S. Jeong, G. Yu, J. Y. Kim, H. J. Park, B. Jeon, W. Song,
    G. Kim, M. Weimer, B. Cho, and B. Chun. Automating system configuration of distributed
    machine learning. In ICDCS, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[100] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer,
    and Z. Chen. {GS}hard: Scaling giant models with conditional computation and automatic
    sharding. In ICLR, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[101] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski,
    J. Long, E. J. Shekita, and B.-Y. Su. Scaling distributed machine learning with
    the parameter server. In OSDI, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[102] M. Li, D. G. Andersen, A. Smola, and K. Yu. Communication efficient distributed
    machine learning with the parameter server. In NeurIPS, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[103] M. Li, L. Zhou, Z. Yang, A. Q. Li, F. Xia, D. G. Andersen, and A. J.
    Smola. Parameter server for distributed machine learning. In In Big Learning NeurIPS
    Workshop, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[104] S. Li and T. Hoefler. Chimera: efficiently training large-scale neural
    networks with bidirectional pipelines. In SC, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[105] S. Li and T. Hoefler. Near-optimal sparse allreduce for distributed deep
    learning. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice
    of Parallel Programming, pages 135–149, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[106] X. Li, B. Karimi, and P. Li. On distributed adaptive optimization with
    gradient compression. In ICLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[107] X. Lian, Y. Huang, Y. Li, and J. Liu. Asynchronous parallel stochastic
    gradient for nonconvex optimization. In NeurIPS, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[108] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized
    algorithms outperform centralized algorithms? a case study for decentralized parallel
    stochastic gradient descent. In NeurIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[109] X. Lian, W. Zhang, C. Zhang, and J. Liu. Asynchronous decentralized parallel
    stochastic gradient descent. In ICML, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[110] H. Lim, D. G. Andersen, and M. Kaminsky. 3lc: Lightweight and effective
    traffic compression for distributed machine learning. ArXiv, abs/1802.07389, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[111] T. Lin, S. U. Stich, and M. Jaggi. Don’t use large mini-batches, use
    local sgd. ArXiv, abs/1808.07217, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[112] T. Lin, S. U. Stich, K. K. Patel, and M. Jaggi. Don’t use large mini-batches,
    use local sgd. In ICLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[113] Y. Lin, S. Han, H. Mao, Y. Wang, and B. Dally. Deep gradient compression:
    Reducing the communication bandwidth for distributed training. In ICLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[114] Lin Xiao and S. Boyd. Fast linear iterations for distributed averaging.
    In 42nd IEEE International Conference on Decision and Control, 2003.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[115] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, and J. M. Hellerstein.
    Distributed graphlab: A framework for machine learning and data mining in the
    cloud. Proc. VLDB Endow., 5(8):716–727, Apr. 2012.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[116] L. Luo, P. West, J. Nelson, A. Krishnamurthy, and L. Ceze. PLink: Efficient
    cloud-based training with topology-aware dynamic hierarchical aggregation. In
    MLSys, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[117] A. M Abdelmoniem, A. Elzanaty, M.-S. Alouini, and M. Canini. An efficient
    statistical-based gradient compression technique for distributed training systems.
    MLSys, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[118] D. G. A. M. Li and A. Smola. Distributed delayed proximal gradient methods.
    In In NeurIPS Workshop on Optimization for Machine Learning, Lake Tahoe, CA, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[119] D. Mahajan, S. S. Keerthi, and S. Sundararajan. A distributed block coordinate
    descent method for training l1regularized linear classifiers. J. Mach. Learn.
    Res., 18(1):3167–3201, Jan. 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[120] J. Martens and R. Grosse. Optimizing neural networks with kronecker-factored
    approximate curvature. In International conference on machine learning, pages
    2408–2417\. PMLR, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[121] R. Mayer, C. Mayer, and L. Laich. The tensorflow partitioning and scheduling
    problem: It’s the critical path! In Proceedings of the 1st Workshop on Distributed
    Infrastructures for Deep Learning, DIDL ’17, New York, NY, USA, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[122] R. Mcdonald, M. Mohri, N. Silberman, D. Walker, and G. S. Mann. Efficient
    large-scale distributed training of conditional maximum entropy models. In NeurIPS.
    2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[123] R. T. McDonald, K. B. Hall, and G. Mann. Distributed training strategies
    for the structured perceptron. In HLT-NAACL, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[124] N. McGlohon and S. Patterson. Distributed semi-stochastic optimization
    with quantization refinement. In 2016 American Control Conference (ACC), pages
    7159–7164, July 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[125] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient
    learning of deep networks from decentralized data. In AISTATS, pages 1273–1282,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[126] Q. Meng, W. Chen, J. Yu, T. Wang, Z.-M. Ma, and T.-Y. Liu. Asynchronous
    accelerated stochastic gradient descent. In IJCAI, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[127] H. Mikami, H. Suganuma, Y. Tanaka, Y. Kageyama, et al. Massively distributed
    SGD: ImageNet/ResNet-50 training in a flash. arXiv preprint arXiv:1811.05233,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[128] A. Mirhoseini, H. Pham, Q. V. Le, B. Steiner, R. Larsen, Y. Zhou, N. Kumar,
    M. Norouzi, S. Bengio, and J. Dean. Device placement optimization with reinforcement
    learning. In ICML, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[129] K. Mishchenko, F. Bach, M. Even, and B. E. Woodworth. Asynchronous sgd
    beats minibatch sgd under arbitrary delays. In NeurIPS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[130] K. Mishchenko, E. A. Gorbunov, M. Takác, and P. Richtárik. Distributed
    learning with compressed gradient differences. ArXiv, abs/1901.09269, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[131] K. Mishchenko, F. Hanzely, and P. Richtárik. 99% of parallel optimization
    is inevitably a waste of time. ArXiv, abs/1901.09437, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[132] P. Moritz, R. Nishihara, I. Stoica, and M. I. Jordan. Sparknet: Training
    deep networks in spark. In ICLR, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[133] J. F. C. Mota, J. M. F. Xavier, P. M. Q. Aguiar, and M. Püschel. D-admm:
    A communication-efficient distributed algorithm for separable optimization. IEEE
    Transactions on Signal Processing, 61(10):2718–2723, May 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[134] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent
    optimization. IEEE Transactions on Automatic Control, 54(1):48–61, Jan 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[135] A. Nedić and A. Olshevsky. Stochastic gradient-push for strongly convex
    functions on time-varying directed graphs. IEEE Transactions on Automatic Control,
    61(12):3936–3947, Dec 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[136] A. Nedić, A. Olshevsky, and M. G. Rabbat. Network topology and communication-computation
    tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953–976,
    May 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[137] F. Niu, B. Recht, C. Re, and S. J. Wright. Hogwild!: A lock-free approach
    to parallelizing stochastic gradient descent. In NeurIPS, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[138] R. Olfati-Saber, J. A. Fax, and R. M. Murray. Consensus and cooperation
    in networked multi-agent systems. Proceedings of the IEEE, 95(1):215–233, Jan
    2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[139] B. C. Ooi, K.-L. Tan, S. Wang, W. Wang, Q. Cai, G. Chen, J. Gao, Z. Luo,
    A. K. Tung, Y. Wang, Z. Xie, M. Zhang, and K. Zheng. Singa: A distributed deep
    learning platform. In ACM International Conference on Multimedia, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[140] F. N. I. P. H. Jin, Q. Yuan and K. Keutzer. How to scale distributed
    deep learning? In ML Systems Workshop at NeurIPS, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[141] Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, and C. Guo. A
    generic communication scheduler for distributed DNN training acceleration. In
    Proceedings of the 27th ACM Symposium on Operating Systems Principles, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[142] D. Peteiro-Barral and B. Guijarro-Berdiñas. A survey of methods for distributed
    machine learning. Progress in Artificial Intelligence, 2:1–11, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[143] R. Power and J. Li. Piccolo: Building fast, distributed programs with
    partitioned tables. In OSDI, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[144] Y. Pu, M. N. Zeilinger, and C. N. Jones. Quantization design for distributed
    optimization. IEEE Transactions on Automatic Control, 62(5):2107–2120, May 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[145] N. Qian. On the momentum term in gradient descent learning algorithms.
    Neural Netw., 12(1):145–151, Jan. 1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[146] Z. Qu, P. Richtárik, and T. Zhang. Quartz: Randomized dual coordinate
    ascent with arbitrary sampling. In NeurIPS, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[147] M. Rabbat. Multi-agent mirror descent for decentralized stochastic optimization.
    In 2015 IEEE 6th International Workshop on CAMSAP, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[148] R. Rabenseifner. Optimization of collective reduction operations. In
    M. Bubak, G. D. van Albada, P. M. A. Sloot, and J. Dongarra, editors, Computational
    Science - ICCS 2004, 2004.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[149] S. S. Ram, A. Nedic, and V. V. Veeravalli. Distributed stochastic subgradient
    projection algorithms for convex optimization. Journal of Optimization Theory
    and Applications, 147:516–545, 2008.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[150] T. Reinartz. Focusing Solutions for Data Mining: Analytical Studies and
    Experimental Results in Real-world Domains. Springer-Verlag, Berlin, Heidelberg,
    1999.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[151] A. Reisizadeh, H. Taheri, A. Mokhtari, H. Hassani, and R. Pedarsani.
    Robust and communication-efficient collaborative learning. In NeurIPS. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[152] J. Ren, X. Li, and J. Haupt. Communication-efficient distributed optimization
    for sparse learning via two-way truncation. In 2017 IEEE 7th International Workshop
    on CAMSAP, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[153] P. Richtárik, I. Sokolov, and I. Fatkhullin. Ef21: A new, simpler, theoretically
    better, and practically faster error feedback. NeurIPS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[154] P. Richtarik, I. Sokolov, E. Gasanov, I. Fatkhullin, Z. Li, and E. Gorbunov.
    3PC: Three point compressors for communication-efficient distributed training
    and a better theory for lazy aggregation. In ICML, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[155] L. Roberts. Picture coding using pseudo-random noise. IRE Transactions
    on Information Theory, 8(2):145–154, February 1962.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[156] J. Romero, J. Yin, N. Laanait, B. Xie, M. T. Young, S. Treichler, V. Starchenko,
    A. Borisevich, A. Sergeev, and M. Matheson. Accelerating collective communication
    in data parallel training across deep learning frameworks. In NSDI, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[157] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large
    scale visual recognition challenge. Int. J. Comput. Vision, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[158] C. D. Sa, C. Zhang, K. Olukotun, and C. Ré. Taming the wild: A unified
    analysis of hogwild-style algorithms. NeurIPS, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[159] P. Sanders, J. Speck, and J. L. Träff. Two-tree algorithms for full bandwidth
    broadcast, reduction and scan. Parallel Computing, 35(12):581–594, 2009.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[160] A. Sapio, M. Canini, C.-Y. Ho, J. Nelson, P. Kalnis, C. Kim, A. Krishnamurthy,
    M. Moshref, D. Ports, and P. Richtarik. Scaling distributed machine learning with
    In-Network aggregation. In NSDI 21, pages 785–808, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[161] S. Sarvotham, R. Riedi, and R. Baraniuk. Connection-level analysis and
    modeling of network traffic. In Proceedings of ACM SIGCOMM Workshop on IMW. ACM,
    2001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[162] F. Sattler, S. Wiedemann, K. Müller, and W. Samek. Robust and communication-efficient
    federated learning from non-iid data. CoRR, abs/1903.02891, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[163] F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek. Sparse binary compression:
    Towards distributed deep learning with minimal communication. IJCN, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[164] K. Scaman, F. Bach, S. Bubeck, Y. T. Lee, and L. Massoulié. Optimal algorithms
    for non-smooth distributed optimization in networks. In NeurIPS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[165] L. Schenato and G. Gamba. A distributed consensus protocol for clock
    synchronization in wireless sensor network. In 2007 46th IEEE Conference on Decision
    and Control, pages 2289–2294, Dec 2007.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[166] K. Seaman, F. Bach, S. Bubeck, Y. T. Lee, and L. Massoulié. Optimal algorithms
    for smooth and strongly convex distributed optimization in networks. In ICML,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[167] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient
    descent and its application to data-parallel distributed training of speech DNNs.
    In INTERSPEECH, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[168] C. J. Shallue, J. Lee, J. M. Antognini, J. Sohl-Dickstein, R. Frostig,
    and G. E. Dahl. Measuring the effects of data parallelism on neural network training.
    CoRR, abs/1811.03600, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[169] S. Shi, X. Chu, K. C. Cheung, and S. See. Understanding top-k sparsification
    in distributed deep learning. arXiv preprint arXiv:1911.08772, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[170] S. Shi, X. Chu, and B. Li. MG-WFBP: Efficient data communication for
    distributed synchronous SGD algorithms. In IEEE INFOCOM, pages 172–180\. IEEE,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[171] S. Shi, X. Chu, and B. Li. MG-WFBP: Merging gradients wisely for efficient
    communication in distributed deep learning. TPDS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[172] S. Shi, X. Pan, X. Chu, and B. Li. PipeMoE: Accelerating mixture-of-experts
    through adaptive pipelining. In IEEE INFOCOM, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[173] S. Shi, Z. Tang, X. Chu, C. Liu, W. Wang, and B. Li. A quantitative survey
    of communication optimizations in distributed deep learning. IEEE Network, 35(3):230–237,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[174] S. Shi, Z. Tang, Q. Wang, K. Zhao, and X. Chu. Layer-wise adaptive gradient
    sparsification for distributed deep learning with convergence guarantees. In ECAI,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[175] S. Shi, Q. Wang, and X. Chu. Performance modeling and evaluation of distributed
    deep learning frameworks on gpus. In 2018 IEEE 4th Intl Conf on Big Data Intelligence
    and Computing, pages 949–957\. IEEE, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[176] S. Shi, Q. Wang, X. Chu, and B. Li. A DAG model of synchronous stochastic
    gradient descent in distributed deep learning. In ICPADS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[177] S. Shi, Q. Wang, X. Chu, B. Li, Y. Qin, R. Liu, and X. Zhao. Communication-efficient
    distributed deep learning with merged gradient sparsification on gpus. In IEEE
    INFOCOM, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[178] S. Shi, Q. Wang, P. Xu, and X. Chu. Benchmarking state-of-the-art deep
    learning software tools. In 2016 7th International Conference on Cloud Computing
    and Big Data (CCBD), pages 99–104\. IEEE, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[179] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y. Wang, X. Huang, and X. Chu. A distributed
    synchronous SGD algorithm with global top-k sparsification for low bandwidth networks.
    In ICDCS, pages 2238–2247, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[180] S. Shi, L. Zhang, and B. Li. Accelerating distributed k-fac with smart
    parallelism of computing and communication tasks. In ICDCS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[181] S. Shi, K. Zhao, Q. Wang, Z. Tang, and X. Chu. A convergence analysis
    of distributed SGD with communication-efficient gradient sparsification. In IJCAI,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[182] S. Shi, X. Zhou, S. Song, X. Wang, Z. Zhu, X. Huang, X. Jiang, F. Zhou,
    Z. Guo, L. Xie, et al. Towards scalable distributed training of deep learning
    on public cloud clusters. MLSys, 3:401–412, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[183] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro.
    Megatron-lm: Training multi-billion parameter language models using model parallelism.
    arXiv preprint arXiv:1909.08053, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[184] H. Shokri Ghadikolaei, S. Stich, and M. Jaggi. Lena: Communication-efficient
    distributed learning with self-triggered gradient uploads. In AISTATS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[185] A. Smola and S. Narayanamurthy. An architecture for parallel topic models.
    In Proc. VLDB Endow., 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[186] A. Spiridonoff, A. Olshevsky, and I. Paschalidis. Communication-efficient
    SGD: From local SGD to one-shot averaging. In A. Beygelzimer, Y. Dauphin, P. Liang,
    and J. W. Vaughan, editors, NeurIPS, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[187] S. U. Stich. Local SGD converges fast and communicates little. In ICLR,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[188] S. U. Stich, J.-B. Cordonnier, and M. Jaggi. Sparsified SGD with memory.
    In NeurIPS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[189] N. Strom. A tonotopic artificial neural network architecture for phoneme
    probability estimation. In 1997 IEEE Workshop on Automatic Speech Recognition
    and Understanding Proceedings, pages 156–163, Dec 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[190] N. Strom. Scalable distributed DNN training using commodity GPU cloud
    computing. In INTERSPEECH, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[191] N. Ström. Sparse connection and pruning in large dynamic artificial neural
    networks. In EUROSPEECH, 1997.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[192] B. Sudharsan, D. Sheth, S. Arya, F. Rollo, P. Yadav, P. Patel, J. G.
    Breslin, and M. I. Ali. Elasticl: Elastic quantization for communication efficient
    collaborative learning in iot. In SenSys, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[193] N. Sukhija, M. Tatineni, N. Brown, M. V. Moer, P. Rodriguez, and S. Callicott.
    Topic modeling and visualization for big data in social sciences. In 2016 Intl
    IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing,
    Scalable Computing and Communications, Cloud and Big Data Computing, Internet
    of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld), 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[194] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable
    effectiveness of data in deep learning era. In ICCV, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[195] J. Sun, T. Chen, G. Giannakis, and Z. Yang. Communication-efficient distributed
    learning via lazily aggregated quantized gradients. In NeurIPS. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[196] T. Sun, R. Hannah, and W. Yin. Asynchronous coordinate descent under
    more realistic assumption. In NeurIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[197] X. Sun, X. Ren, S. Ma, and H. Wang. meProp: Sparsified back propagation
    for accelerated deep learning with reduced overfitting. In ICML, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[198] A. T. Suresh, F. X. Yu, S. Kumar, and H. B. McMahan. Distributed mean
    estimation with limited communication. In ICML, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[199] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer. Efficient processing
    of deep neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[200] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu,
    C. Zhang, and Y. He. 1-bit adam: Communication efficient large-scale training
    with adam’s convergence speed. In ICML, 2021.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[201] H. Tang, S. Gan, C. Zhang, T. Zhang, and J. Liu. Communication compression
    for decentralized training. In NeurIPS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[202] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu. Decentralized training
    over decentralized data. In ICML, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[203] Z. Tang, X. Chu, R. Y. Ran, S. Lee, S. Shi, Y. Zhang, Y. Wang, A. Q.
    Liang, S. Avestimehr, and C. He. Fedml parrot: A scalable federated learning system
    via heterogeneity-aware scheduling on sequential and hierarchical training. arXiv
    preprint arXiv:2303.01778, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[204] Z. Tang, S. Shi, and X. Chu. Communication-efficient decentralized learning
    with sparsification and adaptive peer selection. arXiv preprint arXiv:2002.09692,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[205] Z. Tang, S. Shi, B. Li, and X. Chu. Gossipfl: A decentralized federated
    learning framework with sparsified and adaptive communication. IEEE Transactions
    on Parallel and Distributed Systems, 34(3):909–922, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[206] Z. Tang, Y. Zhang, S. Shi, X. He, B. Han, and X. Chu. Virtual homogeneity
    learning: Defending against data heterogeneity in federated learning. In International
    Conference on Machine Learning, pages 21111–21132\. PMLR, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[207] R. Thakur, R. Rabenseifner, and W. Gropp. Optimization of collective
    communication operations in mpich. Int. J. High Perform. Comput. Appl., 19(1):49–66,
    Feb. 2005.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[208] Y. Tsuzuku, H. Imachi, and T. Akiba. Variance-based gradient compression
    for efficient distributed deep learning. In ICLR, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[209] Y. Ueno and R. Yokota. Exhaustive study of hierarchical allreduce patterns
    for large messages between gpus. In Proceedings of the 18th IEEE/ACM International
    Symposium on Cluster, Cloud and Grid Computing, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[210] C. A. Uribe, S. Lee, A. Gasnikov, and A. Nedic. A dual approach for optimal
    algorithms in distributed optimization over networks. CoRR, abs/1809.00710, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[211] L. G. Valiant. A bridging model for parallel computation. Commun. ACM,
    33(8):103–111, Aug. 1990.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[212] V. Vanhoucke, A. Senior, and M. Z. Mao. Improving the speed of neural
    networks on cpus. In Deep Learning and Unsupervised Feature Learning Workshop,
    NeurIPS 2011, 2011.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[213] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin. Attention is all you need. In NeurIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[214] G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and
    I. Stoica. Blink: Fast and generic collectives for distributed ML. In MLSys, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[215] H. Wang, S. Guo, Z. Qu, R. Li, and Z. Liu. Error-compensated sparsification
    for communication-efficient decentralized training in edge environment. IEEE TPDS,
    2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[216] H. Wang, S. Sievert, Z. Charles, S. Liu, S. Wright, and D. Papailiopoulos.
    Atomo: Communication-efficient learning via atomic sparsification. In NeurIPS,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[217] J. Wang and G. Joshi. Adaptive communication strategies to achieve the
    best error-runtime trade-off in local-update SGD. Proc. of Workshop on Systems
    for ML and Open Source Software, collocated with NeurIPS 2018, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[218] J. Wang and G. Joshi. Cooperative SGD: A unified framework for the design
    and analysis of communication-efficient SGD algorithms. CoRR, abs/1808.07576,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[219] L. Wang, W. Wang, and B. Li. CMFL: mitigating communication overhead
    for federated learning. In ICDCS, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[220] Q. Wang, S. Shi, C. Wang, and X. Chu. Communication contention aware
    scheduling of multiple deep learning training jobs. arXiv preprint arXiv:2002.10105,
    2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[221] S. Wang, D. Li, Y. Cheng, J. Geng, Y. Wang, S. Wang, S.-T. Xia, and J. Wu.
    BML: A high-performance, low-cost gradient synchronization algorithm for DML training.
    In NeurIPS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[222] S. Wang, D. Li, J. Geng, Y. Gu, and Y. Cheng. Impact of network topology
    on the performance of dml: Theoretical analysis and practical factors. In IEEE
    INFOCOM, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[223] Y. Wang, L. Lin, and J. Chen. Communication-compressed adaptive gradient
    method for distributed nonconvex optimization. In AISTATS, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[224] Y. Wang, Q. Wang, S. Shi, X. He, Z. Tang, K. Zhao, and X. Chu. Benchmarking
    the performance and power of AI accelerators for AI training. arXiv preprint arXiv:1909.06842,
    2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[225] J. Wangni, J. Wang, J. Liu, and T. Zhang. Gradient sparsification for
    communication-efficient distributed optimization. In NeurIPS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[226] E. Wei and A. E. Ozdaglar. On the o(1=k) convergence of asynchronous
    distributed alternating direction method of multipliers. In 2013 IEEE Global Conference
    on Signal and Information Processing, pages 551–554, Dec 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[227] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad:
    Ternary gradients to reduce communication in distributed deep learning. In NeurIPS,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[228] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad:
    Ternary gradients to reduce communication in distributed deep learning. In NeurIPS,
    2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[229] J. Wu, W. Huang, J. Huang, and T. Zhang. Error compensated quantized
    SGD and its applications to large-scale distributed optimization. In ICML, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[230] E. P. Xing, Q. Ho, W. Dai, J. K. Kim, J. Wei, S. Lee, X. Zheng, P. Xie,
    A. Kumar, and Y. Yu. Petuum: A new platform for distributed machine learning on
    big data. IEEE Transactions on Big Data, 1(2):49–67, June 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[231] E. P. Xing, Q. Ho, P. Xie, and D. Wei. Strategies and principles of distributed
    machine learning on big data. Engineering, 2(2):179 – 195, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[232] A. Xu and H. Huang. Detached error feedback for distributed SGD with
    random sparsification. In ICML, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[233] P. Xu, S. Shi, and X. Chu. Performance evaluation of deep learning tools
    in docker containers. In 2017 3rd International Conference on Big Data Computing
    and Communications (BIGCOM), pages 395–403\. IEEE, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[234] Y. Xu and W. Yin. A block coordinate descent method for regularized multiconvex
    optimization with applications to nonnegative tensor factorization and completion.
    SIAM J. Imaging Sciences, 6:1758–1789, 2013.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[235] D. Yan, W. Wang, and X. Chu. Optimizing batched winograd convolution
    on GPUs. In Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice
    of Parallel Programming, pages 32–44, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[236] G. Yan, T. Li, S.-L. Huang, T. Lan, and L. Song. Ac-sgd: Adaptively compressed
    sgd for communication-efficient distributed learning. IEEE JSAC, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[237] S. Yang, Y. Wang, and X. Chu. A survey of deep learning techniques for
    neural machine translation. arXiv preprint arXiv:2002.07526, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[238] Y. You, A. Buluç, and J. Demmel. Scaling deep learning on gpu and knights
    landing clusters. In Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis, pages 1–12, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[239] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
    J. Demmel, K. Keutzer, and C.-J. Hsieh. Large batch optimization for deep learning:
    Training BERT in 76 minutes. In ICLR, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[240] Y. You, Z. Zhang, C.-J. Hsieh, J. Demmel, and K. Keutzer. Imagenet training
    in minutes. In Proceedings of the 47th International Conference on Parallel Processing,
    page 1\. ACM, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[241] H. Yu and R. Jin. On the computation and communication complexity of
    parallel SGD with dynamic batch sizes for stochastic non-convex optimization.
    In ICML, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[242] H. Yu, R. Jin, and S. Yang. On the linear speedup analysis of communication
    efficient momentum SGD for distributed non-convex optimization. In ICML, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[243] H. Yu, S. X. Yang, and S. Zhu. Parallel restarted sgd with faster convergence
    and less communication: Demystifying why model averaging works for deep learning.
    In AAAI, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[244] Y. Yu, J. Wu, and L. Huang. Double quantization for communication-efficient
    distributed optimization. In NeurIPS. 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[245] J. Zeng, T. T.-K. Lau, S. Lin, and Y. Yao. Global convergence of block
    coordinate descent in deep learning. In ICML, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[246] C. Zhang and C. Ré. Dimmwitted: A study of main-memory statistical analytics.
    Proc. VLDB Endow., 7(12):1283–1294, Aug. 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[247] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang. ZipML: Training
    linear models with end-to-end low precision, and a little bit of deep learning.
    In ICML, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[248] H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie,
    and E. P. Xing. Poseidon: An efficient communication architecture for distributed
    deep learning on GPU clusters. In USENIX ATC, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[249] J. Zhang, C. D. Sa, I. Mitliagkas, and C. Ré. Parallel sgd: When does
    averaging help? ArXiv, abs/1606.07365, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[250] L. Zhang, S. Shi, X. Chu, W. Wang, B. Li, and C. Liu. Accelerating distributed
    deep learning with fine-grained all-reduce pipelining. In IEEE ICDCS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[251] L. Zhang, S. Shi, and B. Li. Eva: Practical second-order optimization
    with kronecker-vectorized approximation. In The Eleventh International Conference
    on Learning Representations, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[252] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Evaluation and optimization
    of gradient compression for distributed deep learning. In IEEE ICDCS, 2023.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[253] R. Zhang and J. T. Kwok. Asynchronous distributed admm for consensus
    optimization. In ICML, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[254] S. Zhang, A. Choromanska, and Y. LeCun. Deep learning with elastic averaging
    sgd. In NeurIPS, 2015.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[255] X. Zhang, J. Liu, and Z. Zhu. Taming convergence for asynchronous stochastic
    gradient descent with unbounded delay in non-convex learning. arXiv preprint arXiv:1805.09470,
    2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[256] X. Zhang, J. Trmal, D. Povey, and S. Khudanpur. Improving deep neural
    network acoustic models using generalized maxout networks. In ICASSP, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[257] Z. Zhang and M. Brand. Convergent block coordinate descent for training
    tikhonov regularized deep neural networks. In NeurIPS, 2017.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[258] Z. Zhang and C. Wang. Mipd: An adaptive gradient sparsification framework
    for distributed dnns training. IEEE TPDS, pages 3053–3066, 2022.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[259] Z. Zhang, L. Yin, Y. Peng, and D. Li. A quick survey on large scale distributed
    deep learning systems. In ICPADS, 2018.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[260] S.-Y. Zhao, Y. Xie, H. Gao, and W.-J. Li. Global momentum compression
    for sparse communication in distributed sgd. ArXiv, abs/1905.12948, 2019.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[261] W. Zhao, D. Xie, R. Jia, Y. Qian, R. Ding, M. Sun, and P. Li. Distributed
    hierarchical gpu parameter server for massive scale deep learning ads systems.
    MLSys, 2020.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[262] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training
    low bitwidth convolutional neural networks with low bitwidth gradients. arXiv
    preprint arXiv:1606.06160, 2016.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[263] M. A. Zinkevich, M. Weimer, A. Smola, and L. Li. Parallelized stochastic
    gradient descent. In NeurIPS, 2010.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[264] Y. Zou, X. Jin, Y. Li, Z. Guo, E. Wang, and B. Xiao. Mariana: Tencent
    deep learning platform and its applications. PVLDB, 7:1772–1777, 2014.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
