- en: <!--yml
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: <!--yml
- en: 'category: 未分类'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 'category: 未分类'
- en: 'date: 2024-09-06 20:02:04'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 'date: 2024-09-06 20:02:04'
- en: -->
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: -->
- en: '[2003.06307] Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey'
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[2003.06307] 通信高效的数据并行分布式深度学习：全面调查'
- en: 来源：[https://ar5iv.labs.arxiv.org/html/2003.06307](https://ar5iv.labs.arxiv.org/html/2003.06307)
  id: totrans-5
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 来源：[https://ar5iv.labs.arxiv.org/html/2003.06307](https://ar5iv.labs.arxiv.org/html/2003.06307)
- en: 'Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通信高效的数据并行分布式深度学习：全面调查
- en: Zhenheng Tang Hong Kong Baptist University [zhtang@comp.hkbu.edu.hk](mailto:zhtang@comp.hkbu.edu.hk)
    ,  Shaohuai Shi Harbin Institute of Technology, Shenzhen [shaohuais@hit.edu.cn](mailto:shaohuais@hit.edu.cn)
    ,  Wei Wang The Hong Kong University of Science and Technology [weiwa@cse.ust.hk](mailto:weiwa@cse.ust.hk)
    ,  Bo Li The Hong Kong University of Science and Technology [bli@cse.ust.hk](mailto:bli@cse.ust.hk)
     and  Xiaowen Chu The Hong Kong University of Science and Technology (Guangzhou)
    [xwchu@ust.hk](mailto:xwchu@ust.hk)
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Zhenheng Tang 香港浸会大学 [zhtang@comp.hkbu.edu.hk](mailto:zhtang@comp.hkbu.edu.hk)，Shaohuai
    Shi 哈尔滨工业大学深圳 [shaohuais@hit.edu.cn](mailto:shaohuais@hit.edu.cn)，Wei Wang 香港科技大学
    [weiwa@cse.ust.hk](mailto:weiwa@cse.ust.hk)，Bo Li 香港科技大学 [bli@cse.ust.hk](mailto:bli@cse.ust.hk)
    和 Xiaowen Chu 香港科技大学（广州） [xwchu@ust.hk](mailto:xwchu@ust.hk)
- en: Abstract.
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 摘要。
- en: 'Distributed deep learning (DL) has become prevalent in recent years to reduce
    training time by leveraging multiple computing devices (e.g., GPUs/TPUs) due to
    larger models and datasets. However, system scalability is limited by communication
    becoming the performance bottleneck. Addressing this communication issue has become
    a prominent research topic. In this paper, we provide a comprehensive survey of
    the communication-efficient distributed training algorithms, focusing on both
    system-level and algorithmic-level optimizations. We first propose a taxonomy
    of data-parallel distributed training algorithms that incorporates four primary
    dimensions: communication synchronization, system architectures, compression techniques,
    and parallelism of communication and computing tasks. We then investigate state-of-the-art
    studies that address problems in these four dimensions. We also compare the convergence
    rates of different algorithms to understand their convergence speed. Additionally,
    we conduct extensive experiments to empirically compare the convergence performance
    of various mainstream distributed training algorithms. Based on our system-level
    communication cost analysis, theoretical and experimental convergence speed comparison,
    we provide readers with an understanding of which algorithms are more efficient
    under specific distributed environments. Our research also extrapolates potential
    directions for further optimizations.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式深度学习（DL）近年来变得越来越普及，利用多个计算设备（如GPU/TPU）来减少训练时间，这是由于模型和数据集的规模扩大。然而，系统的可扩展性受到通信成为性能瓶颈的限制。解决这一通信问题已成为一个重要的研究课题。本文提供了关于通信高效的分布式训练算法的全面调查，重点关注系统级和算法级优化。我们首先提出了一个数据并行分布式训练算法的分类法，该分类法包括四个主要维度：通信同步、系统架构、压缩技术以及通信和计算任务的并行性。然后，我们调查了针对这些四个维度问题的最新研究。我们还比较了不同算法的收敛速度，以了解其收敛速度。此外，我们进行了大量实验，实证比较了各种主流分布式训练算法的收敛性能。基于我们的系统级通信成本分析、理论和实验收敛速度比较，我们为读者提供了在特定分布式环境下哪些算法更高效的理解。我们的研究还推测了进一步优化的潜在方向。
- en: 'Distributed Deep Learning, Efficient Communication^†^†ccs: Computing methodologies Distributed
    algorithms^†^†ccs: General and reference Surveys and overviews^†^†ccs: Computing
    methodologies Neural networks^†^†ccs: Computing methodologies Parallel algorithms'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 'Distributed Deep Learning, Efficient Communication^†^†ccs: Computing methodologies Distributed
    algorithms^†^†ccs: General and reference Surveys and overviews^†^†ccs: Computing
    methodologies Neural networks^†^†ccs: Computing methodologies Parallel algorithms'
- en: 1\. Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1\. 引言
- en: 'Deep learning (DL) has made significant progress in recent years. Researchers
    and engineers have applied DL technologies to tackle intricate problems across
    various fields, including but not limited to computer vision ([resnet,](#bib.bib67)
    ), natural language processing ([attention,](#bib.bib213) ; [bert,](#bib.bib46)
    ; [yang2020survey,](#bib.bib237) ), speech recognition ([deepspeech,](#bib.bib9)
    ) and many others. DL typically involves increased sizes of training datasets
    and model parameters in deep neural networks (DNNs) to enhance the predictive
    performance in different applications, such as accuracy in classification tasks ([ml1991,](#bib.bib150)
    ; [Russakovsky2015ILS28465472846559,](#bib.bib157) ; [8237359,](#bib.bib194) ).
    However, as data size and model complexity increase, the training process becomes
    exceedingly computationally intensive and time-consuming. For example, training
    a state-of-the-art ResNet-50 ([resnet,](#bib.bib67) ) model (in 90 epochs) on
    the ImageNet dataset ([Imagenet,](#bib.bib45) ) using a latest Nvidia Tesla V100
    GPU requires approximately two days ([wang2019performance,](#bib.bib224) ). As
    Figure [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey") shows, however, the
    development speed of GPU FLOPs and memory cannot catch up the development of newly
    large neural networks, like GPT-3 ([gpt3,](#bib.bib24) ), GShard ([lepikhin2021gshard,](#bib.bib100)
    ) and Baidu RecSys ([zhao2020distributed,](#bib.bib261) ). In addition, hyper-parameter
    tuning is necessary to achieve satisfactory results for certain tasks, which further
    demands significant time and financial investment.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '深度学习（DL）近年来取得了显著进展。研究人员和工程师们已经将DL技术应用于解决各个领域的复杂问题，包括但不限于计算机视觉 ([resnet,](#bib.bib67)
    )，自然语言处理 ([attention,](#bib.bib213) ; [bert,](#bib.bib46) ; [yang2020survey,](#bib.bib237)
    )，语音识别 ([deepspeech,](#bib.bib9) )，以及其他许多领域。DL通常涉及到深度神经网络（DNNs）中训练数据集和模型参数的增加，以提高在不同应用中的预测性能，例如分类任务中的准确率 ([ml1991,](#bib.bib150)
    ; [Russakovsky2015ILS28465472846559,](#bib.bib157) ; [8237359,](#bib.bib194) )。然而，随着数据规模和模型复杂性的增加，训练过程变得极为计算密集且耗时。例如，使用最新的Nvidia
    Tesla V100 GPU在ImageNet数据集上训练一个最先进的ResNet-50 ([resnet,](#bib.bib67) )模型（90个epochs）大约需要两天的时间 ([wang2019performance,](#bib.bib224)
    )。然而，如图 [1](#S1.F1 "Figure 1 ‣ 1\. Introduction ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey") 所示，GPU FLOPs和内存的发展速度无法跟上新型大型神经网络的发展，例如GPT-3 ([gpt3,](#bib.bib24)
    )，GShard ([lepikhin2021gshard,](#bib.bib100) )和百度RecSys ([zhao2020distributed,](#bib.bib261)
    )。此外，为了在某些任务中取得令人满意的结果，超参数调优是必要的，这进一步需要大量的时间和财政投入。'
- en: '![Refer to caption](img/0b659eb861584b896afc5a4c91b01a74.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/0b659eb861584b896afc5a4c91b01a74.png)'
- en: (a) Memory of GPUs and neural networks.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: (a) GPU和神经网络的内存。
- en: '![Refer to caption](img/f0e406dcdcc99107a6873c7905029f99.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/f0e406dcdcc99107a6873c7905029f99.png)'
- en: (b) FLOPS of GPU and FLOPs of training models.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: (b) GPU的FLOPS和训练模型的FLOPs。
- en: Figure 1\. The trends of GPU and neural networks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1. GPU和神经网络的趋势。
- en: 'To mitigate the time-consuming training process, two approaches have gained
    traction: (1) maximizing the utilization of a single accelerator’s computing power
    by implementing highly optimized software ([chetlur2014cudnn,](#bib.bib34) ; [shi2016benchmarking,](#bib.bib178)
    ; [xu2017performance,](#bib.bib233) ; [sze2017efficient,](#bib.bib199) ; [yan2020optimizing,](#bib.bib235)
    ), and (2) employing distributed training ([padam,](#bib.bib35) ; [disml,](#bib.bib230)
    ; [sparknet,](#bib.bib132) ; [PSGD,](#bib.bib263) ) to accelerate the training
    process by making use of multiple processors, including CPUs ([you2018imagenet,](#bib.bib240)
    ), GPUs ([goyal2017accurate,](#bib.bib56) ; [shi2018performance,](#bib.bib175)
    ; [jia2018highly,](#bib.bib81) ) and TPUs ([you2019large,](#bib.bib239) ). Intuitively,
    multiple processors working collaboratively in training one model can reduce the
    overall training time. However, the communication cost between processors typically
    restricts system scalability ([shi2018performance,](#bib.bib175) ). For example,
    when deploying the high-speed computing devices (e.g., Nvidia A100/H100 GPUs)
    with low-speed interconnects (e.g., PCIe or 10GbE) to collaboratively train a
    deep model, where the computation-to-communication ratio is low ([shi2020quantitative,](#bib.bib173)
    ), using multiple processors could result in very low hardware utilization ([DGC,](#bib.bib113)
    ). And the collaborative training between different geographically distributed
    GPUs suffers from higher communication costs, because the wide-area networks have
    lower communication bandwidth ([tang2022gossipfl,](#bib.bib205) ). Therefore,
    different parallel algorithms should be carefully designed to leverage the computing
    power of distributed clusters.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻耗时的训练过程，已经有两种方法受到关注：（1）通过实现高度优化的软件来最大化单个加速器计算能力的利用（[chetlur2014cudnn,](#bib.bib34)
    ; [shi2016benchmarking,](#bib.bib178) ; [xu2017performance,](#bib.bib233) ; [sze2017efficient,](#bib.bib199)
    ; [yan2020optimizing,](#bib.bib235) ），以及（2）采用分布式训练（[padam,](#bib.bib35) ; [disml,](#bib.bib230)
    ; [sparknet,](#bib.bib132) ; [PSGD,](#bib.bib263) ）通过利用多个处理器（包括CPU（[you2018imagenet,](#bib.bib240)
    ），GPU（[goyal2017accurate,](#bib.bib56) ; [shi2018performance,](#bib.bib175) ;
    [jia2018highly,](#bib.bib81) ）和TPU（[you2019large,](#bib.bib239) ）来加速训练过程。直观地，多个处理器在训练一个模型时协同工作可以减少总体训练时间。然而，处理器之间的通信成本通常限制了系统的可扩展性（[shi2018performance,](#bib.bib175)
    ）。例如，当使用高速计算设备（例如，Nvidia A100/H100 GPU）与低速互连（例如，PCIe 或 10GbE）来协同训练深度模型时，当计算与通信的比例较低（[shi2020quantitative,](#bib.bib173)
    ）时，使用多个处理器可能导致硬件利用率非常低（[DGC,](#bib.bib113) ）。而不同地理位置分布的GPU之间的协作训练会受到更高的通信成本影响，因为广域网络的通信带宽较低（[tang2022gossipfl,](#bib.bib205)
    ）。因此，应仔细设计不同的并行算法以充分利用分布式集群的计算能力。
- en: Data parallelism ([dean2012large,](#bib.bib43) ; [goyal2017accurate,](#bib.bib56)
    ) is a widely employed distributed training technique. This approach involves
    replicating the model parameters to all computing workers. During a single iteration,
    each worker computes the local gradient or model updates by sampling different
    mini-batches of data. Workers then exchanges the results with the other workers.
    Following this, aggregation and broadcast operations are executed to obtain the
    new global model.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行性（[dean2012large,](#bib.bib43) ; [goyal2017accurate,](#bib.bib56) ）是一种广泛使用的分布式训练技术。这种方法涉及将模型参数复制到所有计算工作节点。在单次迭代中，每个工作节点通过对不同的小批量数据进行采样来计算本地梯度或模型更新。然后，工作节点将结果与其他节点交换。接下来，执行聚合和广播操作以获取新的全局模型。
- en: Model parallelism ([dean2012large,](#bib.bib43) ; [3154842,](#bib.bib121) ;
    [Mirhoseini2017,](#bib.bib128) ) is another distributed training approach that
    involves partitioning model parameters among multiple computing workers. Every
    worker holds different parameters or layers of the model (i.e., a subset of $x$).
    In this paper, we focus mainly on techniques associated with data parallelism.
    Note that some of them could be used in parallel to model parallelism.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 模型并行性（[dean2012large,](#bib.bib43) ; [3154842,](#bib.bib121) ; [Mirhoseini2017,](#bib.bib128)
    ）是另一种分布式训练方法，涉及将模型参数划分到多个计算工作节点中。每个工作节点持有模型的不同参数或层（即，$x$的一个子集）。在本文中，我们主要关注与数据并行性相关的技术。注意，其中一些技术也可以与模型并行性并行使用。
- en: The term of pipeline parallelism ([shi2018adag,](#bib.bib176) ; [zhang2017poseidon,](#bib.bib248)
    ; [MLSYS2022_cedebb6e,](#bib.bib2) ; [harlap2018pipedream,](#bib.bib64) ; [huang2019gpipe,](#bib.bib75)
    ; [shoeybi2019megatron,](#bib.bib183) ; [li2021chimera,](#bib.bib104) ) can refer
    to different techniques under different contexts. In the context of data parallelism,
    pipelining means to execute the computational tasks and communication tasks simultaneously
    so as to reduce the iteration time. In the context of model parallelism, it refers
    to the technique that workers are assigned different stages of model training
    to execute, and intermediate results are transmitted between workers to hide some
    training time. In this paper, we mainly focus on the pipelining techniques deployed
    in data parallelism, which is used to reduce the communication overhead.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**Pipeline parallelism** ([shi2018adag,](#bib.bib176); [zhang2017poseidon,](#bib.bib248);
    [MLSYS2022_cedebb6e,](#bib.bib2); [harlap2018pipedream,](#bib.bib64); [huang2019gpipe,](#bib.bib75);
    [shoeybi2019megatron,](#bib.bib183); [li2021chimera,](#bib.bib104)) 这一术语在不同的背景下可以指不同的技术。在数据并行的背景下，流水线指的是同时执行计算任务和通信任务，以减少迭代时间。在模型并行的背景下，它指的是将不同阶段的模型训练任务分配给不同的工作节点，并在节点之间传输中间结果以隐藏部分训练时间。本文主要关注于数据并行中的流水线技术，这些技术用于减少通信开销。'
- en: This survey provides a comprehensive overview and taxonomy of research works
    that aim to enhance communication efficiency in distributed training. We deconstruct
    the distributed training framework into several orthogonal components, including
    communication synchronization, system architectures, compression techniques and
    parallelism of communication and computation. Additionally, we offer an overview
    of convergence analysis of these algorithms, examining the trade-off between communication
    efficiency and convergence. Furthermore, we develop a benchmark framework of mainstream
    distributed training algorithms, including different synchronization schemes,
    communication topology, compression algorithms, and various numbers of workers,
    providing the practical reference to readers. Through this survey, we hope that
    readers can gain insight into the advancements made in this field and be inspired
    to develop new efficient distributed training algorithms and frameworks.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本次调查提供了一个关于提高分布式训练中通信效率的研究工作的全面概述和分类。我们将分布式训练框架解构为几个正交组件，包括通信同步、系统架构、压缩技术以及通信和计算的并行性。此外，我们还提供了这些算法收敛性分析的概述，考察了通信效率与收敛性之间的权衡。进一步地，我们开发了主流分布式训练算法的基准框架，包括不同的同步方案、通信拓扑、压缩算法以及不同数量的工作节点，为读者提供实际参考。通过这项调查，我们希望读者能够深入了解该领域的进展，并受到启发，开发出新的高效分布式训练算法和框架。
- en: 1.1\. Related Work
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1. 相关工作
- en: There exist several surveys providing an introduction and review to distributed
    machine or deep learning algorithms. Peteiro-Barral et al. ([Peteiro2013,](#bib.bib142)
    ) introduced distributed machine learning algorithms for big data. Xing et al. ([XING2016179,](#bib.bib231)
    ) mainly focused on different synchronization schemes, scheduling, and balancing
    workloads and communication typologies. Ben-Nun et al. ([10.1145/3320060,](#bib.bib15)
    ) focused on DNN operators and approaches for parallelism. Guo et al. ([DBLP:abs-1808-04752,](#bib.bib60)
    ) gave a thorough review of different quantized neural networks. Meanwhile, Zhang
    et al. ([8644613,](#bib.bib259) ) provided a brief overview of large-scale distributed
    DL systems, including parallelism, parameter server architectures, synchronization
    schemes, related applications, and platforms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多项调查提供了关于分布式机器学习或深度学习算法的介绍和评审。Peteiro-Barral 等人 ([Peteiro2013,](#bib.bib142))
    介绍了针对大数据的分布式机器学习算法。Xing 等人 ([XING2016179,](#bib.bib231)) 主要关注不同的同步方案、调度以及负载均衡和通信类型。Ben-Nun
    等人 ([10.1145/3320060,](#bib.bib15)) 关注了 DNN 操作符和并行方法。Guo 等人 ([DBLP:abs-1808-04752,](#bib.bib60))
    对不同的量化神经网络进行了详细评审。同时，Zhang 等人 ([8644613,](#bib.bib259)) 提供了大规模分布式 DL 系统的简要概述，包括并行性、参数服务器架构、同步方案、相关应用和平台。
- en: Our article, in contrast to these previous surveys, concentrates specifically
    on communication-efficient distributed DL training. We present a detailed discussion
    of communication compression techniques that have not been fully demystified in
    the previous surveys. Additionally, we provide a quick review of auxiliary technologies
    and offer a comparison of convergence bounds. Moreover, we conduct extensive experiments
    to compare the performance of different mainstream distributed training algorithms,
    which provides readers with an empirical understanding of these algorithms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于这些早期的调查，我们的文章专注于通信高效的分布式深度学习训练。我们详细讨论了在先前调查中未被完全揭示的通信压缩技术。此外，我们提供了辅助技术的快速回顾，并比较了收敛界限。此外，我们进行了一系列广泛的实验，以比较不同主流分布式训练算法的性能，为读者提供对这些算法的实证理解。
- en: 1.2\. Organization
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2\. 组织结构
- en: 'This survey aims to provide a comprehensive analysis of communication-efficient
    distributed training algorithms with data parallelism, focusing on four key aspects:
    communication synchronization, system architectures, compression techniques, and
    scheduling methods. The rest of the paper is organized as follows. In Section
    [2](#S2 "2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey"), we illustrate the key issues
    of distributed training and propose a taxonomy of related research to summarize
    existing methods. We discuss the synchronous and asynchronous frameworks in Section
    [3](#S3 "3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"), followed by an overview
    of the system architectures that support gradient/model aggregation in Section
    [4](#S4 "4\. Centralized/Decentralized framework ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"). In Section [5](#S5
    "5\. Quantization methods ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey") and Section [6](#S6 "6\. Sparsification
    Methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"), we introduce the techniques in reducing the communication traffic with
    gradient/model compression. The scheduling methods are introduced in Section [7](#S7
    "7\. Scheduling of Communication and Computing ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"). We summarize the
    theoretical convergence bounds in Section [8](#S8 "8\. Convergence Analysis ‣
    Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"). Additionally, we present some auxiliary tricks to train deep models
    with communication-efficient algorithms in Section [9](#S9 "9\. Auxiliary Technologies
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"). Finally, we conclude the paper in Section [10](#S10 "10\. Conclusion
    and Future Directions ‣ Communication-Efficient Data Parallel Distributed Deep
    Learning: A Comprehensive Survey").'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 本调查旨在提供对具有数据并行性的通信高效分布式训练算法的全面分析，重点关注四个关键方面：通信同步、系统架构、压缩技术和调度方法。本文其余部分的组织结构如下。在[第2节](#S2
    "2\. 分布式深度学习分类 ‣ 通信高效的数据并行分布式深度学习：全面调查")中，我们阐述了分布式训练的关键问题，并提出了相关研究的分类，以总结现有方法。在[第3节](#S3
    "3\. 同步/异步框架 ‣ 通信高效的数据并行分布式深度学习：全面调查")中，我们讨论了同步和异步框架，接着在[第4节](#S4 "4\. 集中/分散框架
    ‣ 通信高效的数据并行分布式深度学习：全面调查")中概述了支持梯度/模型聚合的系统架构。在[第5节](#S5 "5\. 量化方法 ‣ 通信高效的数据并行分布式深度学习：全面调查")和[第6节](#S6
    "6\. 稀疏化方法 ‣ 通信高效的数据并行分布式深度学习：全面调查")中，我们介绍了减少通信流量的技术，包括梯度/模型压缩。调度方法在[第7节](#S7
    "7\. 通信与计算调度 ‣ 通信高效的数据并行分布式深度学习：全面调查")中介绍。我们在[第8节](#S8 "8\. 收敛分析 ‣ 通信高效的数据并行分布式深度学习：全面调查")总结了理论收敛界限。此外，我们在[第9节](#S9
    "9\. 辅助技术 ‣ 通信高效的数据并行分布式深度学习：全面调查")中展示了一些使用通信高效算法训练深度模型的辅助技巧。最后，我们在[第10节](#S10
    "10\. 结论与未来方向 ‣ 通信高效的数据并行分布式深度学习：全面调查")中总结了论文内容。
- en: 1.3\. Benchmark Framework and Experiment Configuration
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.3\. 基准框架与实验配置
- en: Our benchmark framework is built upon FedML ([chaoyanghe2020fedml,](#bib.bib66)
    ) and MPI ([mpi4py,](#bib.bib42) ). Our framework provides users with flexible
    and scalable application programming interfaces (APIs) for testing and developing
    novel distributed training algorithms. Multiple algorithms have already been developed
    with distinct synchronous schemes, communication topologies, and compression techniques.
    These algorithms can be combined to create new ones that are mostly independent
    of each other.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的基准框架建立在 FedML（[chaoyanghe2020fedml](#bib.bib66) ）和 MPI（[mpi4py](#bib.bib42)
    ）之上。我们的框架为用户提供了灵活和可扩展的应用程序接口（API），用于测试和开发新的分布式训练算法。已经开发了多种具有不同同步方案、通信拓扑和压缩技术的算法。这些算法可以组合成新的算法，且彼此大多是独立的。
- en: We conduct experiments of different distributed training algorithms on two typical
    DL tasks. One is the image classification on CIFAR-10 ([krizhevsky2010cifar,](#bib.bib95)
    ) using ResNet-20 ([resnet,](#bib.bib67) ). The other is Shakespeare, a natural
    language processing task based on the dataset obtained from The Complete Works
    of William Shakespeare. The model used for Shakespeare is a stacked character-level
    LSTM language model, proposed by ([FederatedLearning,](#bib.bib125) ). We conduct
    experiments with different workers ($4\sim 32$) to assess the scalability of different
    algorithms. Each worker is equipped with a NVIDIA RTX 2080 Ti and Pytorch version
    is V1.7\. Note that the hardware platform doesn’t affect the convergence performance
    and model test accuracy. The results of the experiments are presented at the end
    of each chapter, providing the readers with an accurate and practical understanding
    of the algorithms.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在两个典型的深度学习任务上进行不同分布式训练算法的实验。一个是使用 ResNet-20 的 CIFAR-10 图像分类任务（[krizhevsky2010cifar](#bib.bib95)）。另一个是莎士比亚任务，这是一个基于《威廉·莎士比亚全集》数据集的自然语言处理任务。莎士比亚任务中使用的模型是一个堆叠的字符级
    LSTM 语言模型，由 ([FederatedLearning](#bib.bib125) ) 提出。我们对不同数量的工作节点（$4\sim 32$）进行了实验，以评估不同算法的可扩展性。每个工作节点配备了
    NVIDIA RTX 2080 Ti，Pytorch 版本为 V1.7。请注意，硬件平台不会影响收敛性能和模型测试准确性。实验结果在每章末尾呈现，为读者提供了对算法的准确和实际的理解。
- en: 2\. Taxonomy of Distributed DL
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2\. 分布式深度学习的分类
- en: The mathematical formulation of training DL models can be defined as an optimization
    problem
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 训练深度学习模型的数学公式可以定义为一个优化问题
- en: '| (1) |  | $\min\limits_{\mathbf{x}\in\mathbb{R}^{N}}f(\mathbf{x}):=\mathbb{E}_{\xi_{i}\sim\mathcal{D}}F(\mathbf{x};\xi_{i}),$
    |  |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| (1) |  | $\min\limits_{\mathbf{x}\in\mathbb{R}^{N}}f(\mathbf{x}):=\mathbb{E}_{\xi_{i}\sim\mathcal{D}}F(\mathbf{x};\xi_{i}),$
    |  |'
- en: where the random variable $\xi_{i}$ follows a probability distribution $\mathcal{D}$,
    which denotes the data samples from a given dataset, $\mathbf{x}$ represents all
    the parameters of the model, $N$ is the number of parameters, and $F:\mathbb{R}^{N}\to\mathbb{R}$
    denotes the objective function with respect to $\mathbf{x}$ and $\xi_{i}$, $f$
    the expectation of $F$.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 其中随机变量 $\xi_{i}$ 遵循概率分布 $\mathcal{D}$，表示来自给定数据集的数据样本，$\mathbf{x}$ 表示模型的所有参数，$N$
    是参数的数量，$F:\mathbb{R}^{N}\to\mathbb{R}$ 表示关于 $\mathbf{x}$ 和 $\xi_{i}$ 的目标函数，$f$
    是 $F$ 的期望值。
- en: Gradient-based optimization algorithms are commonly used in DL. Due to the high
    computational complexity of second-order gradient descent methods ([martens2015optimizing,](#bib.bib120)
    ; [shi2021accelerating,](#bib.bib180) ; [zhang2023eva,](#bib.bib251) ) with DNNs,
    the first-order gradient descent methods, particularly the stochastic gradient
    descent (SGD) with mini-batch¹¹1To simplify, throughout this paper, we use SGD
    to denote the gradient descent with mini-batch which includes the cases of one
    sample and more than one samples in each training iteration. and its variants
    (e.g., Adam), are commonly used. The update rule of single-device SGD is as follows.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 基于梯度的优化算法在深度学习中被广泛使用。由于二阶梯度下降方法（[martens2015optimizing](#bib.bib120) ; [shi2021accelerating](#bib.bib180)
    ; [zhang2023eva](#bib.bib251) ）在深度神经网络中计算复杂度较高，通常使用一阶梯度下降方法，特别是带有小批量的随机梯度下降（SGD）及其变体（例如
    Adam）。单设备 SGD 的更新规则如下。
- en: '| (2) |  | $G_{t}(\mathbf{x}_{t})={\nabla}F_{t}(\mathbf{x}_{t};\xi_{t})$ |  |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| (2) |  | $G_{t}(\mathbf{x}_{t})={\nabla}F_{t}(\mathbf{x}_{t};\xi_{t})$ |  |'
- en: '| (3) |  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}G_{t}(\mathbf{x}_{t}),$
    |  |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| (3) |  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}G_{t}(\mathbf{x}_{t}),$
    |  |'
- en: 'where $\mathbf{x}_{t}\in\mathbb{R}^{N}$ is the $N$-dimensional model parameter
    at iteration $t$, $\xi_{t}$ is a randomly sampled mini-batch of data, and $\gamma$
    is the learning rate (or step size). SGD is an iterative algorithm and has been
    proved that it can solve ([1](#S2.E1 "In 2\. Taxonomy of Distributed DL ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")) under the assumptions
    that $f_{s}(\mathbf{x})$ is non-convex and is with L-Lipschitzian gradients ([Bottou2016OptimizationMF,](#bib.bib20)
    ). The iterative process generally contains several steps: 1) It samples a mini-batch
    of data $\xi_{t}$. 2) It performs the feed-forward computations to evaluate the
    objective function $F_{t}(\mathbf{x}_{t};\xi_{t})$). 3) It performs backward propagation
    to calculate the gradients, ${\nabla}F_{t}(\mathbf{x}_{t};\xi_{t})$ with respect
    to model parameters. 4) Finally, it updates model parameters by Eq. ([3](#S2.E3
    "In 2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey")).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '其中 $\mathbf{x}_{t}\in\mathbb{R}^{N}$ 是迭代 $t$ 时的 $N$ 维模型参数，$\xi_{t}$ 是随机采样的小批量数据，$\gamma$
    是学习率（或步长）。SGD 是一种迭代算法，已经证明它可以在假设 $f_{s}(\mathbf{x})$ 是非凸的且具有 L-利普希茨梯度的情况下解决问题
    ([1](#S2.E1 "In 2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"))。迭代过程通常包含几个步骤：1)
    采样一个小批量的数据 $\xi_{t}$。2) 执行前向计算以评估目标函数 $F_{t}(\mathbf{x}_{t};\xi_{t})$。3) 执行反向传播以计算相对于模型参数的梯度
    ${\nabla}F_{t}(\mathbf{x}_{t};\xi_{t})$。4) 最后，通过方程 ([3](#S2.E3 "In 2\. Taxonomy
    of Distributed DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey")) 更新模型参数。'
- en: 'Distributed training modifies the above four basic single-device SGD procedures
    into distributed versions. We firstly describe the procedures of the widely used
    distributed training algorithm, bulk synchronous parallel SGD (BSP-SGD) ([BSP7917379181,](#bib.bib211)
    ), and then elucidate how other algorithms differ from it. In BSP-SGD, each worker
    (indexed as $i$) has an identical global model (e.g.., downloads from the Parameter
    Server (PS)), and then processes a different subset of data (i.e., $\xi_{i,t}$)
    to independently compute gradients (${\nabla}F(\mathbf{x}_{t};\xi_{i,t})$). These
    computations are performed simultaneously among all workers. After computing gradients,
    workers aggregate them through the PS or an all-reduce operation, with a synchronization
    to update the model parameters, and then proceed to the next iteration. The update
    rule of BSP-SGD can be formulated as:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练将上述四个基本的单设备 SGD 过程修改为分布式版本。我们首先描述了广泛使用的分布式训练算法——批量同步并行 SGD (BSP-SGD) ([BSP7917379181,](#bib.bib211)
    )，然后阐明了其他算法与其的不同。在 BSP-SGD 中，每个工作者（索引为 $i$）都有一个相同的全局模型（例如，从参数服务器 (PS) 下载），然后处理不同的数据子集（即
    $\xi_{i,t}$）以独立计算梯度 (${\nabla}F(\mathbf{x}_{t};\xi_{i,t})$)。这些计算在所有工作者之间同时进行。计算梯度后，工作者通过
    PS 或全归约操作汇总这些梯度，进行同步以更新模型参数，然后进行下一次迭代。BSP-SGD 的更新规则可以表示为：
- en: '| (4) |  | $\displaystyle G_{i,t}(\mathbf{x}_{t})={\nabla}F_{i,t}(\mathbf{x}_{t};\xi_{i,t}),$
    |  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| (4) |  | $\displaystyle G_{i,t}(\mathbf{x}_{t})={\nabla}F_{i,t}(\mathbf{x}_{t};\xi_{i,t}),$
    |  |'
- en: '| (5) |  | $\displaystyle\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}\frac{1}{n}\sum_{i=1}^{n}G_{i,t}(\mathbf{x}_{t}),$
    |  |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| (5) |  | $\displaystyle\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}\frac{1}{n}\sum_{i=1}^{n}G_{i,t}(\mathbf{x}_{t}),$
    |  |'
- en: where $G_{i,t}(\mathbf{x}_{t})$ represents the gradient of $F_{i,t}(\mathbf{x}_{t})$
    of worker $i$ at iteration $t$, $n$ the number of workers.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $G_{i,t}(\mathbf{x}_{t})$ 代表工作者 $i$ 在迭代 $t$ 时的 $F_{i,t}(\mathbf{x}_{t})$
    的梯度，$n$ 是工作者的数量。
- en: 'We summarize and classify algorithms that improve BSP-SGD in four orthogonal
    dimensions shown in Table [1](#S2.T1 "Table 1 ‣ 2\. Taxonomy of Distributed DL
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"). Furthermore, we provide an overview of communication-efficient algorithms
    for distributed DL in Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of Distributed
    DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey"), which highlights the main techniques employed in each dimension.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '我们总结并分类了在表格[1](#S2.T1 "Table 1 ‣ 2\. Taxonomy of Distributed DL ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")中显示的四个正交维度下改进BSP-SGD的算法。此外，我们在图[2](#S2.F2
    "Figure 2 ‣ 2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey")中提供了通信高效的分布式深度学习算法的概述，突出了每个维度中使用的主要技术。'
- en: (1)
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Flexible synchronization aims to relax the strict synchronization constraints
    of BSP to reduce the impact of synchronization and the number of communications
    in the same period (Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of Distributed DL
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey").➀ and Section [3](#S3 "3\. Synchronous/asynchronous framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")).'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '灵活同步旨在放宽BSP的严格同步约束，以减少同步的影响和同一时期内的通信次数（图 [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy
    of Distributed DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey").➀ 和章节 [3](#S3 "3\. Synchronous/asynchronous framework
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")）。'
- en: (2)
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Different system architectures propose changing the communication topology
    to avoid the communication congestion of the PS and workers. (Fig. [2](#S2.F2
    "Figure 2 ‣ 2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey").➃ and Section [4](#S4 "4\.
    Centralized/Decentralized framework ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey"))'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '不同的系统架构提出了更改通信拓扑以避免PS和工作节点的通信拥塞的方案。（图 [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of
    Distributed DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey").➃ 和章节 [4](#S4 "4\. Centralized/Decentralized framework
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")）'
- en: (3)
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Compression techniques explore to compress the communication data, thereby
    reduce communication traffic and communication time. (Fig. [2](#S2.F2 "Figure
    2 ‣ 2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey").➁, Section [5](#S5 "5\. Quantization methods
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey") and [6](#S6 "6\. Sparsification Methods ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"))'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '压缩技术探索压缩通信数据，从而减少通信流量和通信时间。（图 [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of Distributed
    DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey").➁，章节 [5](#S5 "5\. Quantization methods ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey") 和 [6](#S6 "6\. Sparsification
    Methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")）'
- en: (4)
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'The parallelism of communication and computing seeks to hide the communication
    time to achieve shorter iteration time. (Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy
    of Distributed DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey").➂ and Section [7](#S7 "7\. Scheduling of Communication
    and Computing ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey"))'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '通信和计算的并行性旨在隐藏通信时间，以实现更短的迭代时间。（图 [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of Distributed
    DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey").➂ 和章节 [7](#S7 "7\. Scheduling of Communication and Computing ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")）'
- en: 'The communication protocols (Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of Distributed
    DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey").➄) and the network topology (Fig. [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy
    of Distributed DL ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey").➅) are also important factors that influence the communication
    efficiency at the hardware-level. We mainly discuss the algorithm-level methods
    and protocols in this survey.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '通信协议（图 [2](#S2.F2 "Figure 2 ‣ 2\. Taxonomy of Distributed DL ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey").➄）和网络拓扑（图 [2](#S2.F2
    "Figure 2 ‣ 2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey").➅）也是影响硬件层面通信效率的重要因素。本调查主要讨论算法层面的方法和协议。'
- en: Table 1\. Taxonomy of Distributed SGD
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1\. 分布式SGD的分类
- en: '| Dimension | Method | Characteristic |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 维度 | 方法 | 特点 |'
- en: '| Communication Synchronization | Synchronous | Frequent communications and
    synchronization |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 通信同步 | 同步 | 频繁的通信和同步 |'
- en: '| Stale-Synchronous | Trade-off between Synchronous and Asynchronous |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 陈旧同步 | 同步与异步之间的折中 |'
- en: '| Asynchronous | No need of synchronization |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 异步 | 无需同步 |'
- en: '| Local SGD | Less frequent communications |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 本地SGD | 较少的通信 |'
- en: '| System Architectures | Parameter-Server | Centralized topology |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 系统架构 | 参数服务器 | 集中式拓扑 |'
- en: '| All-Reduce | Decentralized topology and collective communication |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| All-Reduce | 去中心化拓扑和集体通信 |'
- en: '| Gossip | Decentralized topology and peer-to-peer communication |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 谣言传播 | 去中心化拓扑和点对点通信 |'
- en: '| Compression Techniques | Quantization | Communicate low-precision parameters
    |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 压缩技术 | 量化 | 传输低精度参数 |'
- en: '| Sparsification | Communicate selected parameters |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏化 | 传输选定参数 |'
- en: '| Parallelism of Communication and Computing | Pipelining | Hide the communication
    or computation time |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 通信和计算的并行性 | 流水线 | 隐藏通信或计算时间 |'
- en: '| Scheduling | Dynamically schedule the computing and communication tasks |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 调度 | 动态调度计算和通信任务 |'
- en: '![Refer to caption](img/e07e88b2da1c9d8b6dc0e0445ce9df03.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e07e88b2da1c9d8b6dc0e0445ce9df03.png)'
- en: Figure 2\. Overview of data-parallel distributed deep learning.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2\. 数据并行分布式深度学习概述。
- en: 3\. Synchronous/asynchronous framework
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3\. 同步/异步框架
- en: 'In the data parallel distributed DL, synchronization in BSP-SGD refers to the
    process in which all workers should be synchronized to complete the transmission
    of all parameters or gradients before proceeding to the next training round. Flexible
    synchronization, which relaxes the strict synchronization of BSP-SGD, affects
    not only communication traffic but also the performance and convergence of model
    training. Therefore, there is a trade-off between communication traffic and convergence.
    Moreover, different synchronization schemes can be combined with different architectures.
    In this section, we describe four representative synchronization schemes under
    the PS architecture, which has the most extensive range of applications. The timeline
    of the four different schemes is shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3\. Synchronous/asynchronous
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey").'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '在数据并行分布式DL中，BSP-SGD中的同步指的是所有工作节点应当同步，以完成所有参数或梯度的传输后，才能进行下一轮训练。灵活同步放宽了BSP-SGD的严格同步，不仅影响通信流量，还影响模型训练的性能和收敛。因此，通信流量和收敛之间存在权衡。此外，不同的同步方案可以与不同的架构结合。在本节中，我们描述了在应用范围最广的PS架构下的四种代表性同步方案。这四种不同方案的时间线见图[3](#S3.F3
    "Figure 3 ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey")。'
- en: '![Refer to caption](img/eb078b211869f8c01693086080789688.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/eb078b211869f8c01693086080789688.png)'
- en: Figure 3\. Comparison of training with single device and multiple devices under
    different communication synchronization with the PS architecture.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图3\. 比较了在不同通信同步下使用单个设备和多个设备进行训练的PS架构。
- en: 3.1\. Synchronous Framework
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 同步框架
- en: 'The classical synchronous framework, namely BSP, as mentioned in section [2](#S2
    "2\. Taxonomy of Distributed DL ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey"), involves the following steps: 1) Data
    loading; 2) Feed-forward computations; 3) Backward propagation computations; 4)
    Gradient aggregation with a barrier; and 5) Model updating with aggregated gradients.
    The step (4) requires synchronization of all workers, leading to the straggler
    problem, where few slow workers significantly affect the system throughput ([AsynDisADMM,](#bib.bib253)
    ). Additionally, the aggregation of all gradients leads to high communication
    costs, severely limiting the scalability of the system.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '经典的同步框架，即BSP，如第[2](#S2 "2\. Taxonomy of Distributed DL ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")节中提到的，涉及以下步骤：1)
    数据加载；2) 前向计算；3) 反向传播计算；4) 梯度聚合并加锁；5) 使用聚合的梯度更新模型。步骤(4)需要同步所有工作节点，这会导致延迟节点问题，其中少数慢节点显著影响系统吞吐量([AsynDisADMM,](#bib.bib253)
    )。此外，所有梯度的聚合导致了高通信成本，严重限制了系统的可扩展性。'
- en: 'In the parameter server (PS) architecture (§[4.1](#S4.SS1 "4.1\. Parameter
    Server ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey")), the synchronous
    barrier is enforced until all workers finish transmitting their parameters to
    the parameter servers ([Krizhevsky2014OneWT,](#bib.bib94) ; [Bradley2011,](#bib.bib23)
    ; [552669,](#bib.bib93) ; [375451,](#bib.bib28) ). Similarly, in the All-Reduce
    architecture (§[4.2](#S4.SS2 "4.2\. All-Reduce ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey")), synchronization requires that all workers wait for the
    completion of the all-reduce operation (i.e., gradient aggregation), ensuring
    that all workers have the same updated global model ([Zou2014MarianaTD,](#bib.bib264)
    ; [642949,](#bib.bib25) ; [DGC,](#bib.bib113) ). In contrast, for the decentralized
    architecture (§[4.3](#S4.SS3 "4.3\. Gossip ‣ 4\. Centralized/Decentralized framework
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")), synchronization involves waiting for the completion of communication.
    Unlike in All-Reduce, workers in the decentralized architecture do not necessarily
    maintain the same model ([CanDecent,](#bib.bib108) ; [Ram2008DistributedSS,](#bib.bib149)
    ).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '在参数服务器（PS）架构（§[4.1](#S4.SS1 "4.1\. Parameter Server ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey")）中，同步障碍会一直强制执行，直到所有工作者完成将其参数传输到参数服务器的操作（[Krizhevsky2014OneWT](#bib.bib94)
    ; [Bradley2011](#bib.bib23) ; [552669](#bib.bib93) ; [375451](#bib.bib28)）。类似地，在
    All-Reduce 架构（§[4.2](#S4.SS2 "4.2\. All-Reduce ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey")）中，同步要求所有工作者等待 all-reduce 操作（即梯度聚合）的完成，确保所有工作者都有相同的更新全局模型（[Zou2014MarianaTD](#bib.bib264)
    ; [642949](#bib.bib25) ; [DGC](#bib.bib113)）。相反，对于去中心化架构（§[4.3](#S4.SS3 "4.3\.
    Gossip ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey")），同步涉及等待通信的完成。与 All-Reduce
    不同的是，去中心化架构中的工作者不一定维护相同的模型（[CanDecent](#bib.bib108) ; [Ram2008DistributedSS](#bib.bib149)）。'
- en: 3.2\. Stale-synchronous Framework
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 陈旧同步框架
- en: 'The stale-synchronous parallel (SSP) framework ([MoreEffDMLviaStaleSync,](#bib.bib69)
    ) aims to mitigate the straggler problem with relaxed synchronization. Specifically,
    SSP allows faster workers to perform more updates than slower ones, reducing the
    waiting time of faster workers, as illustrated in Fig. [3](#S3.F3 "Figure 3 ‣
    3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey"). However, to maintain model
    consistency and ensure convergence, SSP imposes a staleness bounded barrier that
    limits the iteration gap between the fastest and slowest workers. For a maximum
    staleness bound $s$, the update formula of worker $i$ at iteration $t+1$ is modified
    to'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '陈旧同步并行（SSP）框架（[MoreEffDMLviaStaleSync](#bib.bib69)）旨在通过放宽同步来缓解拖延者问题。具体来说，SSP
    允许较快的工作者比较慢的工作者执行更多的更新，从而减少较快工作者的等待时间，如图 [3](#S3.F3 "Figure 3 ‣ 3\. Synchronous/asynchronous
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey") 所示。然而，为了保持模型一致性并确保收敛，SSP 强加了一个陈旧度有界的障碍，限制了最快和最慢工作者之间的迭代间隔。对于最大陈旧度界限
    $s$，工作者 $i$ 在迭代 $t+1$ 的更新公式被修改为'
- en: '| (6) |  | $\displaystyle\mathbf{x}_{i,t+1}=\mathbf{x}_{0}-{\gamma}\sum_{k=1}^{t-s-1}\sum_{j=1}^{n}G_{j,k}(\mathbf{x}_{j,k})-{\gamma}\sum_{k=t-s}^{t}G_{i,k}(\mathbf{x}_{i,k})-{\gamma}\sum_{(j,k)\in\mathcal{S}_{i,t+1}}G_{j,k}(\mathbf{x}_{j,k}),$
    |  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| (6) |  | $\displaystyle\mathbf{x}_{i,t+1}=\mathbf{x}_{0}-{\gamma}\sum_{k=1}^{t-s-1}\sum_{j=1}^{n}G_{j,k}(\mathbf{x}_{j,k})-{\gamma}\sum_{k=t-s}^{t}G_{i,k}(\mathbf{x}_{i,k})-{\gamma}\sum_{(j,k)\in\mathcal{S}_{i,t+1}}G_{j,k}(\mathbf{x}_{j,k}),$
    |  |'
- en: 'where $\mathcal{S}_{i,t+1}$ is some subset of the updates from other workers
    during period $\left[t-s,t\right]$, and $n$ represents the number of workers.
    The historical updates added to $\mathbf{x}_{i,t+1}$ consist of three terms: pre-window,
    read-my-writes, in-window updates, represented by second, third and forth terms
    in the right side in equation ([6](#S3.E6 "In 3.2\. Stale-synchronous Framework
    ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey")). The pre-window update represents
    the synchronized gradients which are obtained from all workers, the read-my-writes
    update is local gradients, the in-window update is the gradients from other workers.
    Less $s$ means more timely gradient synchronization but less communication efficiency.
    Determining a proper $s$ is challenging to achieve good end-to-end training performance ([MoreEffDMLviaStaleSync,](#bib.bib69)
    ). To this end, instead of setting a staleness bound, Chen et al. ([RevistSynSGD,](#bib.bib31)
    ) proposed the backup worker scheme. Specifically, a subset of workers (called
    backup workers) are used to compute the mini-batch gradient. The PS updates parameters
    without waiting for all gradients. The gradients from slowest workers are dropped
    directly.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '其中，$\mathcal{S}_{i,t+1}$是期$\left[t-s,t\right]$期间来自其他工作节点的更新子集，$n$表示工作节点的数量。添加到$\mathbf{x}_{i,t+1}$中的历史更新包括三个部分：预窗口更新、读取自己的写入更新和窗口内更新，这些部分分别由等式右侧的第二、第三和第四项表示（见[6](#S3.E6
    "In 3.2\. Stale-synchronous Framework ‣ 3\. Synchronous/asynchronous framework
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")）。预窗口更新表示从所有工作节点获得的同步梯度，读取自己的写入更新是本地梯度，窗口内更新是来自其他工作节点的梯度。较小的$s$意味着更及时的梯度同步，但通信效率较低。确定一个合适的$s$对于实现良好的端到端训练性能具有挑战性 ([MoreEffDMLviaStaleSync,](#bib.bib69)
    )。为此，Chen等人 ([RevistSynSGD,](#bib.bib31) )提出了备份工作节点方案。具体而言，使用一部分工作节点（称为备份工作节点）来计算小批量梯度。参数服务器在不等待所有梯度的情况下更新参数。最慢工作节点的梯度直接被丢弃。'
- en: In addition, there is a congestion problem in SSP with parameter servers. Chen
    et al. ([8737587,](#bib.bib30) ) proposed a Round-Robin Synchronization Parallel
    (R²SP) method to address it. R²SP staggers worker updates throughout the training
    process and coordinates workers to update gradients in a fixed round-robin order
    to evenly distribute the communication load and reduce congestion.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，SSP在参数服务器中存在拥塞问题。Chen等人 ([8737587,](#bib.bib30) )提出了一种轮询同步并行（R²SP）方法来解决此问题。R²SP在训练过程中错开工作节点的更新，并协调工作节点按照固定的轮询顺序更新梯度，以均匀分配通信负载并减少拥塞。
- en: 3.3\. Asynchronous Framework
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3\. 异步框架
- en: 'The asynchronous parallel SGD (ASP-SGD) framework allows the server to update
    the global model with the updates from a part of workers instead of all workers ([hogwild,](#bib.bib137)
    ; [muli2013,](#bib.bib118) ; [Tamingwild,](#bib.bib158) ; [AsynDisMLspar,](#bib.bib58)
    ; [NEURIPS2022_029df12a,](#bib.bib129) ). ASP-SGD enables more independent updates
    of the nodes and reduces one-round data transmission during communication between
    the workers and the PS as shown in Fig. [3](#S3.F3 "Figure 3 ‣ 3\. Synchronous/asynchronous
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey"). In ASP-SGD, each worker sends its gradients to the PS
    after gradient calculation. Then the PS updates the global model without waiting
    for the other workers. Thus, asynchronous frameworks get rid of straggler problems.
    Note that ASP-SGD is not friendly to the All-Reduce architecture. The update formula
    of ASP-SGD can be summarized as'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '异步并行SGD（ASP-SGD）框架允许服务器用部分工作节点的更新来更新全局模型，而不是所有工作节点的更新 ([hogwild,](#bib.bib137)
    ; [muli2013,](#bib.bib118) ; [Tamingwild,](#bib.bib158) ; [AsynDisMLspar,](#bib.bib58)
    ; [NEURIPS2022_029df12a,](#bib.bib129) )。ASP-SGD实现了节点的更独立更新，并减少了工作节点与参数服务器之间的通信中的一轮数据传输，如图[3](#S3.F3
    "Figure 3 ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey")所示。在ASP-SGD中，每个工作节点在计算梯度后将其梯度发送到参数服务器。然后，参数服务器在不等待其他工作节点的情况下更新全局模型。因此，异步框架避免了滞后节点问题。需要注意的是，ASP-SGD不适用于All-Reduce架构。ASP-SGD的更新公式可以总结为'
- en: '| (7) |  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}\sum_{i=1}^{n}G_{i,t-\tau_{i,k}}(\mathbf{x}_{i,t-\tau_{k,i}}),$
    |  |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| (7) |  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}\sum_{i=1}^{n}G_{i,t-\tau_{i,k}}(\mathbf{x}_{i,t-\tau_{k,i}}),$
    |  |'
- en: where the $\tau_{k,i}$ represents the period between when worker $i$ calculates
    the gradient and when the server conducts SGD.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，$\tau_{k,i}$表示工作节点$i$计算梯度与服务器进行SGD之间的时间间隔。
- en: Distributed Alternating Direction Method of Multipliers (D-ADMM) ([Dadmm,](#bib.bib133)
    ; [convergeceAsynDisADMM,](#bib.bib226) ) is an early work that proposed asynchronous
    updating on different parameters of an optimization problem. However, it requires
    the maintenance of a global clock, and each group of workers in D-ADMM needs to
    be aware of each other’s progress. Moreover, D-ADMM heavily depends on the network
    topology and requires a central node to keep the global model while the central
    node still needs to wait for all worker nodes to finish their tasks, which is
    akin to the synchronous framework. Moreover,  ([Dadmm,](#bib.bib133) ; [convergeceAsynDisADMM,](#bib.bib226)
    ) do not consider the optimization of neural networks.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式交替方向乘子法（D-ADMM）（[Dadmm,](#bib.bib133) ; [convergeceAsynDisADMM,](#bib.bib226)
    ）是早期提出在优化问题的不同参数上进行异步更新的工作。然而，它需要维护一个全局时钟，并且D-ADMM中的每一组工作者需要了解彼此的进展。此外，D-ADMM严重依赖于网络拓扑，并需要一个中心节点来保持全局模型，而中心节点仍然需要等待所有工作节点完成任务，这类似于同步框架。此外，（[Dadmm,](#bib.bib133)
    ; [convergeceAsynDisADMM,](#bib.bib226) ）未考虑神经网络的优化。
- en: To address these limitations, Zhang et al.  ([AsynDisADMM,](#bib.bib253) ) proposed
    an asynchronous distributed ADMM using the star topology and the PS architecture.
    Although asynchronous training has been demonstrated to be faster than synchronous
    training in the absence of slow workers, it tend to have inferior convergence
    performance than synchronous optimization. Thus,  ([AsynDisADMM,](#bib.bib253)
    ) employed partial barrier and bounded delay as two conditions to control the
    asynchrony, trying to obtain similar convergence of the synchronous optimization.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些限制，张等人（[AsynDisADMM,](#bib.bib253) ）提出了一种使用星型拓扑和PS架构的异步分布式ADMM。尽管在没有慢速工作者的情况下，异步训练被证明比同步训练更快，但其收敛性能往往不如同步优化。因此，（[AsynDisADMM,](#bib.bib253)
    ）采用了部分障碍和有界延迟作为控制异步性的两个条件，试图获得类似于同步优化的收敛效果。
- en: 'DistBelief ([dean2012large,](#bib.bib43) ) is an early asynchronous framework
    that is capable of harnessing computing clusters with thousands of machines for
    large-scale model training. It proposes Downpour SGD, an ASP-SGD optimization
    method, which involves multiple workers processing data in parallel to compute
    their own updates and communicating with the PS. Later, Li et al. ([communicationDisMLwithPS,](#bib.bib102)
    ) proposed an efficient algorithm named Delayed Block Proximal Gradient Method.
    In this algorithm, only a block of parameters is asynchronously updated per iteration.
    Consequently, only a portion of the parameters is required to be transmitted between
    the master and workers, and the waiting is not necessary. To further reduce the
    communication costs, Grishchenko et al. ([AsynDisMLspar,](#bib.bib58) ) developed
    an asynchronous distributed algorithm that incorporates sparsification of upward
    communications (workers-to-master). Sparsification (§[6](#S6 "6\. Sparsification
    Methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")) is implemented by uniformly sampling a selection of local update entries
    to enhance communication efficiency.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 'DistBelief（[dean2012large,](#bib.bib43) ）是一个早期的异步框架，能够利用拥有数千台机器的计算集群进行大规模模型训练。它提出了Downpour
    SGD，这是一种ASP-SGD优化方法，涉及多个工作者并行处理数据以计算自己的更新，并与PS进行通信。后来，李等人（[communicationDisMLwithPS,](#bib.bib102)
    ）提出了一种名为延迟块近端梯度法的高效算法。在此算法中，每次迭代仅异步更新一个参数块。因此，仅需传输一部分参数在主节点和工作节点之间，且无需等待。为了进一步降低通信成本，Grishchenko等人（[AsynDisMLspar,](#bib.bib58)
    ）开发了一种异步分布式算法，该算法结合了向上传输（工作者到主节点）的稀疏化。稀疏化（§[6](#S6 "6\. Sparsification Methods
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")）通过均匀采样本地更新条目的选择来提高通信效率。'
- en: Asynchronous frameworks provide better system performance by addressing straggler
    problems. Nevertheless, asynchronous algorithms suffer from inferior convergence
    performance. Consequently, synchronous SGD remains the state-of-the-art method
    in the data center setting if workers have uniform hardware and workloads ([RevistSynSGD,](#bib.bib31)
    ).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 异步框架通过解决滞后者问题提供了更好的系统性能。然而，异步算法的收敛性能较差。因此，如果工作者具有均匀的硬件和工作负载，**同步SGD**仍然是数据中心环境中的最先进方法（[RevistSynSGD,](#bib.bib31)
    ）。
- en: 3.4\. Local SGD
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 本地SGD
- en: Local-SGD ([Zhang2014DSM,](#bib.bib246) ; [bijral2016data,](#bib.bib19) ; [Zhang2016ParallelSW,](#bib.bib249)
    ; [NIPS2019_9288,](#bib.bib62) ; [McDonald2010DistributedTS,](#bib.bib123) ; [NIPS2009_3881,](#bib.bib122)
    ; [6853589,](#bib.bib256) ; [Zhang2015DLE296,](#bib.bib254) ; [Yu2018ParallelRS,](#bib.bib243)
    ; [spiridonoff2021communicationefficient,](#bib.bib186) ) is another set of algorithms
    that rely on strict synchronization but permit flexible communication frequencies.
    In Local-SGD, each worker independently executes several or more iterations before
    averaging all local models to obtain the most recent global model. Model Average ([FederatedLearning,](#bib.bib125)
    ) is a similar approach that performs several local iterations and synchronizing
    the model. The procedure of Local-SGD can be formalized as
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Local-SGD ([Zhang2014DSM,](#bib.bib246) ; [bijral2016data,](#bib.bib19) ; [Zhang2016ParallelSW,](#bib.bib249)
    ; [NIPS2019_9288,](#bib.bib62) ; [McDonald2010DistributedTS,](#bib.bib123) ; [NIPS2009_3881,](#bib.bib122)
    ; [6853589,](#bib.bib256) ; [Zhang2015DLE296,](#bib.bib254) ; [Yu2018ParallelRS,](#bib.bib243)
    ; [spiridonoff2021communicationefficient,](#bib.bib186)) 是另一类依赖于严格同步但允许灵活通信频率的算法。在
    Local-SGD 中，每个工作节点在对所有本地模型进行平均以获得最新的全局模型之前，会独立执行若干次或更多次迭代。模型平均 ([FederatedLearning,](#bib.bib125))
    是一种类似的方法，它执行若干次本地迭代并同步模型。Local-SGD 的过程可以形式化为
- en: '| (8) |  | $\mathbf{x}_{i,t+1}=\left\{\begin{array}[]{ll}\mathbf{x}_{i,t}-{\gamma}G_{i,t}(\mathbf{x}_{i,t}),&amp;\text{if}\
    t+1\not\in\mathcal{I}_{T}\\ \mathbf{x}_{i,t}-{\gamma}\frac{1}{n}\sum_{i=1}^{n}G_{i,t}(\mathbf{x}_{i,t}),&amp;\text{if}\
    t+1\in\mathcal{I}_{T}\end{array}\right.$ |  |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| (8) |  | $\mathbf{x}_{i,t+1}=\left\{\begin{array}[]{ll}\mathbf{x}_{i,t}-{\gamma}G_{i,t}(\mathbf{x}_{i,t}),&amp;\text{如果}\
    t+1\not\in\mathcal{I}_{T}\\ \mathbf{x}_{i,t}-{\gamma}\frac{1}{n}\sum_{i=1}^{n}G_{i,t}(\mathbf{x}_{i,t}),&amp;\text{如果}\
    t+1\in\mathcal{I}_{T}\end{array}\right.$ |  |'
- en: where $\mathcal{I}_{T}$ represents the synchronization timestamps.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $\mathcal{I}_{T}$ 代表同步时间戳。
- en: 'While Local-SGD allows for flexible communication frequency to reduce the overall
    communication overhead, reducing it excessively can lead to a decline in convergence
    performance. Consequently, Local-SGD often requires an increased number of training
    iterations to attain comparable model accuracy to that of BSP-SGD, thereby potentially
    slowing down the training process. Therefore, it is important to find the optimal
    communication period that strikes a balance between communication overhead and
    convergence performance. To improve the performance of Local-SGD, Yu et al. ([pmlrv97yu19d,](#bib.bib242)
    ) combined distributed momentum SGD and Local-SGD achieving a linear speedup in
    training. Jiang et al. ([AlinearSpeedupAnalysis,](#bib.bib82) ) reduced communication
    complexity by exploiting the quantization method (§[5](#S5 "5\. Quantization methods
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")) in combination with Local SGD.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Local-SGD 允许灵活的通信频率以减少总体通信开销，但过度减少可能导致收敛性能下降。因此，Local-SGD 通常需要增加训练迭代次数，以获得与
    BSP-SGD 相当的模型准确性，从而可能会减慢训练过程。因此，找到一个在通信开销和收敛性能之间取得平衡的最佳通信周期非常重要。为了提高 Local-SGD
    的性能，Yu 等人 ([pmlrv97yu19d,](#bib.bib242)) 结合了分布式动量 SGD 和 Local-SGD，实现了训练的线性加速。Jiang
    等人 ([AlinearSpeedupAnalysis,](#bib.bib82)) 通过结合 Local SGD 和量化方法 (§[5](#S5 "5\.
    量化方法 ‣ 高效通信的数据并行分布式深度学习：全面调查")) 减少了通信复杂度。
- en: Increasing batch sizes can also reduce the number of iterations needed to reduce
    communication data. CR-PSGD-Catalyst ([pmlrv97yu19c,](#bib.bib241) ) proposed
    to dynamically increase batch sizes after each training iteration, while guaranteeing
    the same convergence rate of SSP. However, large-batch SGD can lead to decreased
    generalization performance  ([chen2016scalable,](#bib.bib32) ; [NIPS2019_8452,](#bib.bib44)
    ; [Hoffer2017TLG32947713294936,](#bib.bib71) ; [DBLPconficlrKeskarMNST17,](#bib.bib88)
    ; [DBLPjournalscorrabs181103600,](#bib.bib168) ). To address this issue, Lin et
    al. ([Lin2018DontUL,](#bib.bib111) ) proposed post-local SGD to allow one to scale
    the training onto much more parallel computing devices. This algorithm divides
    the whole training process into two phases, using mini-batch SGD in the first
    phase and Local-SGD in the second phase.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 增加批量大小也可以减少减少通信数据所需的迭代次数。CR-PSGD-Catalyst ([pmlrv97yu19c,](#bib.bib241) ) 提出了在每次训练迭代后动态增加批量大小，同时保证SSP的收敛率不变。然而，大批量SGD可能会导致泛化性能下降
    ([chen2016scalable,](#bib.bib32) ; [NIPS2019_8452,](#bib.bib44) ; [Hoffer2017TLG32947713294936,](#bib.bib71)
    ; [DBLPconficlrKeskarMNST17,](#bib.bib88) ; [DBLPjournalscorrabs181103600,](#bib.bib168)
    )。为解决这一问题，Lin等人 ([Lin2018DontUL,](#bib.bib111) ) 提出了后期Local-SGD，以允许将训练扩展到更多的并行计算设备。该算法将整个训练过程分为两个阶段，第一阶段使用迷你批量SGD，第二阶段使用Local-SGD。
- en: 'Lazy Aggregation (LAG) ([LAG,](#bib.bib33) ; [LENA,](#bib.bib184) ) delays
    the gradient uploading if the new gradient of a worker is similar with the old
    gradient of the last iteration. And the server will use the old gradient of the
    worker $i$ as the new gradient to conduct aggregation. This can be seen as a special
    Local-SGD, as it skips the communication round. The 3PC ([3PC,](#bib.bib154) )
    designs a novel compressor which unifies the error-feedback (§[9](#S9 "9\. Auxiliary
    Technologies ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey")) and LAG together, and obtains higher compression ratio
    and faster convergence.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 懒惰聚合（LAG）([LAG,](#bib.bib33) ; [LENA,](#bib.bib184) ）在工作节点的新梯度与上一次迭代的旧梯度相似时，延迟梯度上传。服务器将使用工作节点$i$的旧梯度作为新梯度进行聚合。这可以视作一种特殊的Local-SGD，因为它跳过了通信轮次。3PC
    ([3PC,](#bib.bib154) ) 设计了一种新型压缩器，将误差反馈（§[9](#S9 "9\. 辅助技术 ‣ 通信高效的数据并行分布式深度学习：全面调查")）和LAG结合起来，获得了更高的压缩比和更快的收敛速度。
- en: FedAvg ([FederatedLearning,](#bib.bib125) ; [tang2023fedml,](#bib.bib203) ;
    [tang2022gossipfl,](#bib.bib205) ; [tang2022virtual,](#bib.bib206) ) is another
    representative Local-SGD algorithm, which is a fundamental method in federated
    learning ([FederatedLearning,](#bib.bib125) ; [kairouz2019advances,](#bib.bib85)
    ). In FedAvg, the server randomly selects a part of clients to conduct local training
    at each communication round. Different from setting a fixed number of iterations
    in Local-SGD, clients in FedAvg conduct local training on all local samples for
    one epoch or several epochs. Thus, the number of iterations in FedAvg actually
    is proportional to the size of local datasets. After local training, the server
    collects local models for aggregation.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: FedAvg ([FederatedLearning,](#bib.bib125) ; [tang2023fedml,](#bib.bib203) ;
    [tang2022gossipfl,](#bib.bib205) ; [tang2022virtual,](#bib.bib206) ) 是另一种典型的Local-SGD算法，它是联邦学习
    ([FederatedLearning,](#bib.bib125) ; [kairouz2019advances,](#bib.bib85) ) 的基础方法。在FedAvg中，服务器在每次通信轮次中随机选择部分客户端进行本地训练。与Local-SGD中设置固定迭代次数不同，FedAvg中的客户端在所有本地样本上进行一个或多个epoch的本地训练。因此，FedAvg中的迭代次数实际上与本地数据集的大小成正比。在本地训练后，服务器收集本地模型进行聚合。
- en: 3.5\. Experimental Comparison
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5. 实验比较
- en: Table 2\. Test accuracy [%] comparison of different communication synchronization
    schemes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表2. 不同通信同步方案的测试准确率[%]比较。
- en: '| Algorithm | # of Worker | Resnet20 | RNN |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 工作节点数 | Resnet20 | RNN |'
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
- en: '| BSP-SGD | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28 | 47.70
    |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| BSP-SGD | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28 | 47.70
    |'
- en: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
- en: '| ASP-SGD | 4 | 88.27 | 92.27 | 90.84 | 85.97 | 53.73 | 50.18 | 52.39 | 43.81
    |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| ASP-SGD | 4 | 88.27 | 92.27 | 90.84 | 85.97 | 53.73 | 50.18 | 52.39 | 43.81
    |'
- en: '| 32 | 0.00 | 0.00 | 0.00 | 10.00 | 53.79 | 55.34 | 3.75 | 3.04 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 0.00 | 0.00 | 0.00 | 10.00 | 53.79 | 55.34 | 3.75 | 3.04 |'
- en: '| Local-SGD $\tau=2$ | 4 | 84.04 | 88.97 | 92.01 | 88.86 | 55.03 | 54.94 |
    52.87 | 46.72 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD $\tau=2$ | 4 | 84.04 | 88.97 | 92.01 | 88.86 | 55.03 | 54.94 |
    52.87 | 46.72 |'
- en: '| 32 | 65.41 | 84.96 | 89.35 | 90.11 | 54.11 | 51.72 | 52.30 | 46.17 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 65.41 | 84.96 | 89.35 | 90.11 | 54.11 | 51.72 | 52.30 | 46.17 |'
- en: '| Local-SGD $\tau=4$ | 4 | 83.85 | 89.61 | 90.90 | 90.00 | 51.92 | 55.05 |
    56.14 | 47.36 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD $\tau=4$ | 4 | 83.85 | 89.61 | 90.90 | 90.00 | 51.92 | 55.05 |
    56.14 | 47.36 |'
- en: '| 32 | 65.55 | 85.09 | 89.44 | 89.85 | 54.36 | 51.72 | 53.40 | 47.15 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 65.55 | 85.09 | 89.44 | 89.85 | 54.36 | 51.72 | 53.40 | 47.15 |'
- en: '| Local-SGD $\tau=8$ | 4 | 84.58 | 89.32 | 91.20 | 90.86 | 55.25 | 55.19 |
    53.35 | 48.55 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD $\tau=8$ | 4 | 84.58 | 89.32 | 91.20 | 90.86 | 55.25 | 55.19 |
    53.35 | 48.55 |'
- en: '| 32 | 64.48 | 84.82 | 89.41 | 90.09 | 54.07 | 51.72 | 54.91 | 47.30 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.48 | 84.82 | 89.41 | 90.09 | 54.07 | 51.72 | 54.91 | 47.30 |'
- en: '| Local-SGD $\tau=16$ | 4 | 84.02 | 89.25 | 90.99 | 90.56 | 55.32 | 55.41 |
    53.43 | 48.84 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD $\tau=16$ | 4 | 84.02 | 89.25 | 90.99 | 90.56 | 55.32 | 55.41 |
    53.43 | 48.84 |'
- en: '| 32 | 64.74 | 84.69 | 89.57 | 90.15 | 53.85 | 51.72 | 56.69 | 46.93 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.74 | 84.69 | 89.57 | 90.15 | 53.85 | 51.72 | 56.69 | 46.93 |'
- en: '| FedAvg | 4 | 62.41 | 84.37 | 89.81 | 90.10 | 52.20 | 55.23 | 54.91 | 54.92
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| FedAvg | 4 | 62.41 | 84.37 | 89.81 | 90.10 | 52.20 | 55.23 | 54.91 | 54.92
    |'
- en: '| 32 | 40.23 | 64.42 | 83.45 | 86.65 | 28.77 | 43.70 | 51.91 | 52.02 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 40.23 | 64.42 | 83.45 | 86.65 | 28.77 | 43.70 | 51.91 | 52.02 |'
- en: •
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Note: $\tau$ means the number of local iterations of Local SGD.'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意：$\tau$ 表示 Local SGD 的本地迭代次数。
- en: Table 3\. Test accuracy [%] comparison of different communication synchronization
    schemes.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3\. 不同通信同步方案的测试准确率 [%] 比较。
- en: '| Algorithm | # of Worker | Resnet20 | RNN |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 工作数量 | Resnet20 | RNN |'
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
- en: '| BSP-SGD | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28 | 47.70
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| BSP-SGD | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28 | 47.70
    |'
- en: '| 8 | 78.66 | 88.04 | 90.53 | 90.48 | 52.92 | 54.89 | 54.63 | 47.80 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 78.66 | 88.04 | 90.53 | 90.48 | 52.92 | 54.89 | 54.63 | 47.80 |'
- en: '| 16 | 72.51 | 87.03 | 90.04 | 90.02 | 54.79 | 53.72 | 52.99 | 47.75 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 72.51 | 87.03 | 90.04 | 90.02 | 54.79 | 53.72 | 52.99 | 47.75 |'
- en: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
- en: '| BSP-EFTopK | 4 | 83.78 | 88.91 | 90.90 | 89.28 | 51.73 | 55.36 | 55.74 |
    47.66 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| BSP-EFTopK | 4 | 83.78 | 88.91 | 90.90 | 89.28 | 51.73 | 55.36 | 55.74 |
    47.66 |'
- en: '| 8 | 79.76 | 88.25 | 89.84 | 90.09 | 52.64 | 54.89 | 54.48 | 46.65 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 79.76 | 88.25 | 89.84 | 90.09 | 52.64 | 54.89 | 54.48 | 46.65 |'
- en: '| 16 | 72.23 | 87.01 | 89.80 | 88.97 | 54.70 | 53.60 | 52.86 | 47.24 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 72.23 | 87.01 | 89.80 | 88.97 | 54.70 | 53.60 | 52.86 | 47.24 |'
- en: '| 32 | 64.52 | 84.33 | 88.08 | 88.69 | 53.98 | 51.62 | 52.14 | 46.88 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.52 | 84.33 | 88.08 | 88.69 | 53.98 | 51.62 | 52.14 | 46.88 |'
- en: '| ASP-SGD | 4 | 88.27 | 92.27 | 90.84 | 85.97 | 53.73 | 50.18 | 52.39 | 43.81
    |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| ASP-SGD | 4 | 88.27 | 92.27 | 90.84 | 85.97 | 53.73 | 50.18 | 52.39 | 43.81
    |'
- en: '| 8 | 88.15 | 90.49 | 89.67 | 10.00 | 53.72 | 43.69 | 52.94 | 44.34 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 88.15 | 90.49 | 89.67 | 10.00 | 53.72 | 43.69 | 52.94 | 44.34 |'
- en: '| 16 | 85.69 | 85.74 | 81.47 | 10.00 | 54.01 | 37.22 | 51.60 | 6.24 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 85.69 | 85.74 | 81.47 | 10.00 | 54.01 | 37.22 | 51.60 | 6.24 |'
- en: '| 32 | 0.00 | 0.00 | 0.00 | 10.00 | 53.79 | 55.34 | 3.75 | 3.04 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 0.00 | 0.00 | 0.00 | 10.00 | 53.79 | 55.34 | 3.75 | 3.04 |'
- en: '| Local-SGD $\tau=4$ | 4 | 83.85 | 89.61 | 90.90 | 90.00 | 51.92 | 55.05 |
    56.14 | 47.36 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD $\tau=4$ | 4 | 83.85 | 89.61 | 90.90 | 90.00 | 51.92 | 55.05 |
    56.14 | 47.36 |'
- en: '| 8 | 79.06 | 88.42 | 89.95 | 90.72 | 53.24 | 54.71 | 55.11 | 47.76 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 79.06 | 88.42 | 89.95 | 90.72 | 53.24 | 54.71 | 55.11 | 47.76 |'
- en: '| 16 | 72.92 | 84.98 | 86.89 | 90.58 | 55.12 | 53.45 | 53.33 | 47.22 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 72.92 | 84.98 | 86.89 | 90.58 | 55.12 | 53.45 | 53.33 | 47.22 |'
- en: '| 32 | 65.55 | 85.09 | 89.44 | 89.85 | 54.36 | 51.72 | 53.40 | 47.15 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 65.55 | 85.09 | 89.44 | 89.85 | 54.36 | 51.72 | 53.40 | 47.15 |'
- en: '| Local-SGD $\tau=4$ TopK | 4 | 60.73 | 81.18 | 85.20 | 83.83 | 50.54 | 50.78
    | 52.64 | 50.25 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD $\tau=4$ TopK | 4 | 60.73 | 81.18 | 85.20 | 83.83 | 50.54 | 50.78
    | 52.64 | 50.25 |'
- en: '| 8 | 54.43 | 76.96 | 85.00 | 84.78 | 48.01 | 49.13 | 52.28 | 48.89 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 54.43 | 76.96 | 85.00 | 84.78 | 48.01 | 49.13 | 52.28 | 48.89 |'
- en: '| 16 | 46.33 | 70.29 | 83.54 | 83.38 | 44.48 | 47.47 | 51.58 | 47.47 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 46.33 | 70.29 | 83.54 | 83.38 | 44.48 | 47.47 | 51.58 | 47.47 |'
- en: '| 32 | 39.75 | 62.86 | 79.11 | 81.55 | 40.75 | 44.62 | 50.40 | 46.64 |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 39.75 | 62.86 | 79.11 | 81.55 | 40.75 | 44.62 | 50.40 | 46.64 |'
- en: Table 4\. Mean and STD. of the test accuracy [%] of some experiments with 3
    different random seeds.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4\. 一些实验中使用 3 个不同随机种子的测试准确率的均值和标准差 [%]。
- en: '| Algo | Resnet20 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | Resnet20 |'
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 |'
- en: '| BSP-SGD | $65.43\pm 0.54$ | $85.09\pm 0.11$ | $89.15\pm 0.14$ | 89.28$\pm
    0.74$ |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| BSP-SGD | $65.43\pm 0.54$ | $85.09\pm 0.11$ | $89.15\pm 0.14$ | 89.28$\pm
    0.74$ |'
- en: '| BSP-EFTopK | $64.62\pm 0.80$ | $84.18\pm 0.24$ | $87.81\pm 0.23$ | 88.14$\pm
    0.44$ |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| BSP-EFTopK | $64.62\pm 0.80$ | $84.18\pm 0.24$ | $87.81\pm 0.23$ | 88.14$\pm
    0.44$ |'
- en: '| DP-SGD (Gossip) | $64.86\pm 0.76$ | $84.83\pm 0.23$ | $88.83\pm 0.11$ | 89.11$\pm$
    0.45 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| DP-SGD (Gossip) | $64.86\pm 0.76$ | $84.83\pm 0.23$ | $88.83\pm 0.11$ | 89.11$\pm$
    0.45 |'
- en: '| Local-SGD $\tau=4$ | $65.28\pm 0.67$ | $84.78\pm 0.31$ | $89.10\pm 0.24$
    | 89.49$\pm$ 0.49 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD $\tau=4$ | $65.28\pm 0.67$ | $84.78\pm 0.31$ | $89.10\pm 0.24$
    | 89.49$\pm$ 0.49 |'
- en: '| FedAvg | $39.94\pm 0.47$ | $64.03\pm 0.81$ | $83.6\pm 0.11$ | 86.61 $\pm$0.25
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| FedAvg | $39.94\pm 0.47$ | $64.03\pm 0.81$ | $83.6\pm 0.11$ | 86.61 $\pm$0.25
    |'
- en: 'Different synchronization schemes have different properties of communication
    efficiency and convergence rate, some works compare few schemes ([Lin2020Dont,](#bib.bib112)
    ; [NEURIPS2022_029df12a,](#bib.bib129) ; [jiang2022pisces,](#bib.bib83) ). However,
    to the best our knowledge, there is no works benchmark all of them together in
    unified experiment settings. We jointly evaluated the performance of BSP-SGD,
    ASP-SGD, Local-SGD and FedAvg with unified dataset settings and hyper-parameters.
    Experiment settings are provided in Section [1.3](#S1.SS3 "1.3\. Benchmark Framework
    and Experiment Configuration ‣ 1\. Introduction ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey"). For Local-SGD, we
    compare different number of local iterations, including 2, 4, 8, and 16\. Our
    results, as summarized in Table [2](#S3.T2 "Table 2 ‣ 3.5\. Experimental Comparison
    ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey"), indicate that BSP-SGD, ASP-SGD,
    Local-SGD and FedAvg finally obtain the similar test accuracy. And the performance
    of Local-SGD is not very sensitive to the local iterations $\tau$, suggesting
    that the communication overheads can be reduced by skipping some communication
    rounds without sacrificing the model performance.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '不同的同步方案具有不同的通信效率和收敛率，有些研究比较了几种方案（[Lin2020Dont](#bib.bib112)；[NEURIPS2022_029df12a](#bib.bib129)；[jiang2022pisces](#bib.bib83)）。然而，据我们所知，没有研究在统一的实验设置下对所有这些方案进行基准测试。我们在统一的数据集设置和超参数下共同评估了
    BSP-SGD、ASP-SGD、Local-SGD 和 FedAvg 的性能。实验设置见第 [1.3](#S1.SS3 "1.3\. Benchmark Framework
    and Experiment Configuration ‣ 1\. Introduction ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey") 节。对于 Local-SGD，我们比较了不同数量的本地迭代，包括
    2、4、8 和 16。我们的结果总结在表 [2](#S3.T2 "Table 2 ‣ 3.5\. Experimental Comparison ‣ 3\.
    Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey") 中，表明 BSP-SGD、ASP-SGD、Local-SGD 和 FedAvg
    最终获得了相似的测试准确度。而且 Local-SGD 对本地迭代次数 $\tau$ 的性能不太敏感，这表明通过跳过一些通信轮次可以减少通信开销，而不会牺牲模型性能。'
- en: 'Different number of workers significantly influence the convergence of algorithms.
    Table [3](#S3.T3 "Table 3 ‣ 3.5\. Experimental Comparison ‣ 3\. Synchronous/asynchronous
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey") shows the influence of different workers (4, 8, 16, and
    32) to different algorithms. For both two datasets and all algorithms, to achieve
    the same test accuracy, higher learning rates are required when the number of
    workers is increased, as it is similar to higher batch size ([you2019large,](#bib.bib239)
    ). However, for most algorithms, increasing the number of workers can lead to
    a degradation in the final accuracy due to the well-known generalization problem
    of large batch SGD ([Hoffer2017TLG32947713294936,](#bib.bib71) ; [DBLPconficlrKeskarMNST17,](#bib.bib88)
    ), which is a significant bottleneck for distributed training scalability.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '不同的工作节点数量对算法的收敛性有显著影响。表 [3](#S3.T3 "Table 3 ‣ 3.5\. Experimental Comparison
    ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey") 显示了不同工作节点（4、8、16 和 32）对不同算法的影响。对于两个数据集和所有算法，要实现相同的测试准确度，当工作节点数量增加时，需要更高的学习率，这类似于较大的批量大小（[you2019large](#bib.bib239)）。然而，对于大多数算法，增加工作节点数量可能导致最终准确度下降，这是由于大批量
    SGD 的广泛泛化问题（[Hoffer2017TLG32947713294936](#bib.bib71)；[DBLPconficlrKeskarMNST17](#bib.bib88)），这是分布式训练可扩展性的一个重要瓶颈。'
- en: ASP-SGD suffers from a larger drop in the test accuracy, and even diverges when
    the number of workers increases to 32\. This phenomenon is attributed to the higher
    degree of staleness with larger worker numbers. Additionally, the experimental
    results suggest that ASP-SGD may require a lower learning rate compared to other
    algorithms with the same number of workers. This could be because excessively
    moving towards a stale gradient direction could be futile and biased.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ASP-SGD 在测试准确度上有更大的下降，且当工作节点数量增加到 32 时甚至发生了发散。这种现象归因于随着工作节点数量增加，陈旧度更高。此外，实验结果表明，ASP-SGD
    可能需要比其他相同工作节点数量的算法更低的学习率。这可能是因为过度向陈旧梯度方向移动可能是徒劳且有偏差的。
- en: 'In conclusion, while efficient communication algorithms reduce the exchange
    of information compared to BSP-SGD, they may compromise the convergence performance
    of the model. Therefore, the benefits of communication-efficient algorithms come
    at a cost. To provide a better understanding of the varying impacts of these methods,
    we present a comparison in Table [5](#S3.T5 "Table 5 ‣ 3.5\. Experimental Comparison
    ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey").'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 总结而言，虽然高效的通信算法相比 BSP-SGD 减少了信息交换，但可能会妨碍模型的收敛性能。因此，高效通信算法的好处是有代价的。为了更好地理解这些方法的不同影响，我们在表
    [5](#S3.T5 "表 5 ‣ 3.5\. 实验比较 ‣ 3\. 同步/异步框架 ‣ 高效通信数据并行分布式深度学习：综合调查")中呈现了比较。
- en: Table 5\. Influences of different combinations of architectures and synchronization
    schemes
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5\. 不同架构和同步方案组合的影响
- en: '| Architecture | Synchronization | Model Consistency | Communication Frequency
    | Communication Congestion | Convergence |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 同步 | 模型一致性 | 通信频率 | 通信拥堵 | 收敛 |'
- en: '| PS | BSP-SGD | high | high | high | stable |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| PS | BSP-SGD | 高 | 高 | 高 | 稳定 |'
- en: '| SSP-SGD | normal | high | normal | normal |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| SSP-SGD | 正常 | 高 | 正常 | 正常 |'
- en: '| ASP-SGD | low | high | low | unstable |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| ASP-SGD | 低 | 高 | 低 | 不稳定 |'
- en: '| Local-SGD | normal | low | high | unstable |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD | 正常 | 低 | 高 | 不稳定 |'
- en: '| All-Reduce | BSP-SGD | high | high | low | easy |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| All-Reduce | BSP-SGD | 高 | 高 | 低 | 容易 |'
- en: '| SSP-SGD | - | - | - | - |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| SSP-SGD | - | - | - | - |'
- en: '| ASP-SGD | - | - | - | - |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ASP-SGD | - | - | - | - |'
- en: '| Local-SGD | normal | low | low | stable |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD | 正常 | 低 | 低 | 稳定 |'
- en: '| Gossip | BSP-SGD | low | high | low | stable |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| Gossip | BSP-SGD | 低 | 高 | 低 | 稳定 |'
- en: '| SSP-SGD | - | - | - | - |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| SSP-SGD | - | - | - | - |'
- en: '| ASP-SGD | low | high | low | unstable |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ASP-SGD | 低 | 高 | 低 | 不稳定 |'
- en: '| Local-SGD | low | low | low | stable |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| Local-SGD | 低 | 低 | 低 | 稳定 |'
- en: •
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Notes: Here the different levels only represent a relative description comparing
    with the other methods. The model consistency measures how local models are different
    from others, the communication frequency measures how frequently workers communicate
    with others, and communication congestion measures how heavy the traffic of the
    central node is.'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 备注：这里的不同级别仅表示与其他方法相比的相对描述。模型一致性度量本地模型与其他模型的差异，通信频率度量工作节点与其他节点的通信频率，而通信拥堵度量中心节点的流量负载。
- en: 4\. Centralized/Decentralized framework
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4\. 集中式/去中心化框架
- en: 'Various system architectures have been proposed to support efficient data communication
    among workers in distributed DL. The choice of the most appropriate architecture
    depends on the available hardware resources. Regarding how to average model parameters/gradients
    among distributed workers, system architectures can be categorized into three
    types: Parameter Server (PS), All-Reduce, and Gossip as shown in Fig. [4](#S4.F4
    "Figure 4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey").'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 已提出各种系统架构，以支持分布式深度学习中工作节点间的高效数据通信。最合适的架构取决于可用的硬件资源。关于如何在分布式工作节点间平均模型参数/梯度，系统架构可以分为三种类型：参数服务器
    (PS)、All-Reduce 和 Gossip，如图 [4](#S4.F4 "图 4 ‣ 4\. 集中式/去中心化框架 ‣ 高效通信数据并行分布式深度学习：综合调查")所示。
- en: '![Refer to caption](img/49f575988e16ded5af29ca8b5aa4f00e.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/49f575988e16ded5af29ca8b5aa4f00e.png)'
- en: (a) PS architecture.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: (a) PS 架构。
- en: '![Refer to caption](img/b975de9291f3d8881282118e2a72e1c0.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/b975de9291f3d8881282118e2a72e1c0.png)'
- en: (b) All-Reduce architecture.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: (b) All-Reduce 架构。
- en: '![Refer to caption](img/e2fe5f9cfe335e4168bbc100874f2ee9.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![参见说明](img/e2fe5f9cfe335e4168bbc100874f2ee9.png)'
- en: (c) Gossip architecture.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: (c) Gossip 架构。
- en: Figure 4\. Three different system architectures for model/gradient aggregation.
    In (a) and (b), workers synchronize trained models and conduct local training
    with the synchronized model. In (c), workers communicate model parameters with
    some neighbors, and start local training with heterogeneous models parameters.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4\. 三种不同的模型/梯度聚合系统架构。在 (a) 和 (b) 中，工作节点同步训练模型，并使用同步模型进行本地训练。在 (c) 中，工作节点与一些邻居通信模型参数，并开始使用异构模型参数进行本地训练。
- en: 4.1\. Parameter Server
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 参数服务器
- en: 'The Parameter Server (PS) architecture is shown in Fig. [4](#S4.F4 "Figure
    4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey")(a). Servers are responsible
    for storing the newest global model, and gathering the updating information from
    the workers. Workers independently load the dataset, pull the newest global model
    from the server, and then compute the updates, which will be transmitted to the
    servers ([PSforDisML,](#bib.bib103) ; [ScalDMLwithPS,](#bib.bib101) ). The PS
    architecture is referred to as the centralized framework, commonly used in distributed
    training  ([PSforDisML,](#bib.bib103) ; [ScalDMLwithPS,](#bib.bib101) ; [dean2012large,](#bib.bib43)
    ; [MoreEffDMLviaStaleSync,](#bib.bib69) ; [communicationDisMLwithPS,](#bib.bib102)
    ; [Ooi2015SD,](#bib.bib139) ). The major issue with this architecture is the communication
    congestion in the server ([Zhang2015DLE296,](#bib.bib254) ; [HowToScaleDDL,](#bib.bib140)
    ) as it requires to extensively communicate with all workers.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '参数服务器（PS）架构如图[4](#S4.F4 "Figure 4 ‣ 4\. Centralized/Decentralized framework
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")(a)所示。服务器负责存储最新的全局模型，并收集来自工作节点的更新信息。工作节点独立加载数据集，从服务器拉取最新的全局模型，然后计算更新，这些更新将被传输到服务器 ([PSforDisML,](#bib.bib103)
    ; [ScalDMLwithPS,](#bib.bib101) )。PS架构被称为集中式框架，通常用于分布式训练 ([PSforDisML,](#bib.bib103)
    ; [ScalDMLwithPS,](#bib.bib101) ; [dean2012large,](#bib.bib43) ; [MoreEffDMLviaStaleSync,](#bib.bib69)
    ; [communicationDisMLwithPS,](#bib.bib102) ; [Ooi2015SD,](#bib.bib139) )。这种架构的主要问题是服务器的通信拥堵 ([Zhang2015DLE296,](#bib.bib254)
    ; [HowToScaleDDL,](#bib.bib140) )，因为它需要与所有工作节点进行广泛通信。'
- en: Early PS based distributed machine learning methods  ([ScalDMLwithPS,](#bib.bib101)
    ; [DisGraphlab,](#bib.bib115) ; [AsystemLarScalSupML,](#bib.bib1) ; [AnArchforParallel,](#bib.bib185)
    ; [MoreEffDMLviaStaleSync,](#bib.bib69) ; [PSforDisML,](#bib.bib103) ) primarily
    focused on how to implement distributed machine learning at the system level to
    support a vast number of model parameters. In the pioneering framework DistBelief ([dean2012large,](#bib.bib43)
    ), Dean et al. successfully trained the model with billions of parameters on an
    extremely large-scale cluster. Initially, for data parallel distributed machine
    learning algorithms, numerous works focused on CPU clusters  ([Ahmed2012,](#bib.bib3)
    ; [pmlrv32ahn14,](#bib.bib4) ; [Cui2014EBS,](#bib.bib38) ; [Cui2014IP67,](#bib.bib39)
    ; [Power2010PBF,](#bib.bib143) ; [7816979,](#bib.bib193) ). Later, Cui et al. ([Cui2016,](#bib.bib40)
    ) proposed GeePS, a PS architecture to scale deep learning models training across
    numerous GPUs, addressing the challenge of limited GPU memory when training large
    deep models.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 早期基于PS的分布式机器学习方法 ([ScalDMLwithPS,](#bib.bib101) ; [DisGraphlab,](#bib.bib115)
    ; [AsystemLarScalSupML,](#bib.bib1) ; [AnArchforParallel,](#bib.bib185) ; [MoreEffDMLviaStaleSync,](#bib.bib69)
    ; [PSforDisML,](#bib.bib103) )主要集中在如何在系统级别实现分布式机器学习，以支持大量模型参数。在开创性的框架DistBelief ([dean2012large,](#bib.bib43)
    )中，Dean等人成功地在一个极大规模的集群上训练了拥有数十亿参数的模型。最初，对于数据并行的分布式机器学习算法，许多工作集中在CPU集群上 ([Ahmed2012,](#bib.bib3)
    ; [pmlrv32ahn14,](#bib.bib4) ; [Cui2014EBS,](#bib.bib38) ; [Cui2014IP67,](#bib.bib39)
    ; [Power2010PBF,](#bib.bib143) ; [7816979,](#bib.bib193) )。后来，Cui等人 ([Cui2016,](#bib.bib40)
    )提出了GeePS，这是一种PS架构，用于在多个GPU上扩展深度学习模型训练，解决了训练大型深度模型时GPU内存有限的问题。
- en: 'Subsequently, researchers focused on developing communication-efficient methods
    to address the communication bottleneck in PS-based distributed machine learning.
    In an early work by Li et al. ([communicationDisMLwithPS,](#bib.bib102) ), controllable
    synchronization and user-definable filters were used to reduce data communication
    volumes. Later, numerous methods were proposed to achieve efficient communication
    with data compression like sparsification ([DorefaNet,](#bib.bib262) ; [1bit,](#bib.bib167)
    ; [scalableDisDNN,](#bib.bib190) ; [QSGD,](#bib.bib7) ; [ECQSGD,](#bib.bib229)
    ; [TernGrad,](#bib.bib227) ; [signSGD,](#bib.bib17) ; [EFsignSGD,](#bib.bib86)
    ) (§[6](#S6 "6\. Sparsification Methods ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey")) and quantization ([SparOLviaTrunc,](#bib.bib97)
    ; [FLStrategy,](#bib.bib91) ; [GradSparforDisOptim,](#bib.bib225) ; [CommQuantforDataPara,](#bib.bib48)
    ; [DGC,](#bib.bib113) ; [zhang2017poseidon,](#bib.bib248) ) (§[5](#S5 "5\. Quantization
    methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 随后，研究人员专注于开发通信高效的方法来解决基于 PS 的分布式机器学习中的通信瓶颈。在 Li 等人 ([communicationDisMLwithPS](#bib.bib102))
    的早期工作中，使用了可控同步和用户可定义的过滤器来减少数据通信量。后来，提出了许多方法，通过数据压缩实现高效通信，如稀疏化 ([DorefaNet](#bib.bib262)
    ; [1bit](#bib.bib167) ; [scalableDisDNN](#bib.bib190) ; [QSGD](#bib.bib7) ; [ECQSGD](#bib.bib229)
    ; [TernGrad](#bib.bib227) ; [signSGD](#bib.bib17) ; [EFsignSGD](#bib.bib86)) (§[6](#S6
    "6\. 稀疏化方法 ‣ 通信高效的数据并行分布式深度学习：综合调查")) 和量化 ([SparOLviaTrunc](#bib.bib97) ; [FLStrategy](#bib.bib91)
    ; [GradSparforDisOptim](#bib.bib225) ; [CommQuantforDataPara](#bib.bib48) ; [DGC](#bib.bib113)
    ; [zhang2017poseidon](#bib.bib248)) (§[5](#S5 "5\. 量化方法 ‣ 通信高效的数据并行分布式深度学习：综合调查"))。
- en: In addition, Wang et al.  ([icdcsWangWL19,](#bib.bib219) ) proposed a novel
    measurement to determine the relevance of a worker. They measured the relevance
    by comparing the local update with the global update from the previous iteration.
    If the relevance exceeds a threshold, this update is considered as relevant and
    transmitted to the server. Another approach to reducing communication overhead
    is to use programmable switches ([In-network,](#bib.bib160) ). By having workers
    connected to the same switch transmit their model updates over the network and
    complete the aggregation in the network, communication overheads between machines
    can be reduced. However, this approach presents challenges such as limited computation
    and storage of switches, as well as the packet loss. Specialized algorithms need
    to be designed to address these issues.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，Wang 等人 ([icdcsWangWL19](#bib.bib219)) 提出了一个新颖的测量方法来确定工人的相关性。他们通过比较局部更新与来自上一个迭代的全局更新来测量相关性。如果相关性超过阈值，则该更新被认为是相关的，并传输到服务器。另一种减少通信开销的方法是使用可编程交换机
    ([In-network](#bib.bib160))。通过让连接到同一交换机的工人在网络上传输模型更新并在网络中完成聚合，可以减少机器之间的通信开销。然而，这种方法存在如交换机的计算和存储限制以及数据包丢失等挑战。需要设计专门的算法来解决这些问题。
- en: 4.2\. All-Reduce
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 全规约
- en: 'To avoid the communication bottleneck in the PS, practitioners and researchers
    turned to the All-Reduce architecture for gradient aggregation without central
    servers ([awan2017s,](#bib.bib11) ; [chu2017efficient,](#bib.bib36) ; [wang2019impact,](#bib.bib222)
    ; [goyal2017accurate,](#bib.bib56) ; [jia2018highly,](#bib.bib81) ). As depicted
    in Fig. [4](#S4.F4 "Figure 4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")(b), all workers
    communicate with each other without a central node to obtain the gradients from
    all the other workers. The aggregated gradients are used to update their local
    models, thereby achieving consistency with other workers (the initialized models
    in different workers are identical). It is a model-centralized topology since
    there is a consistent global model attained through synchronization, which allows
    the updating equation ([5](#S2.E5 "In 2\. Taxonomy of Distributed DL ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")) (BSP-SGD) to
    remain unchanged. However, it is not suitable for asynchronous communication due
    to the collective communication nature of All-Reduce. While it is difficult to
    apply All-Reduce in the asynchronous part of SSP, it is easily applicable in the
    synchronous part of SSP.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免 PS 中的通信瓶颈，实践者和研究人员转向 All-Reduce 架构进行梯度聚合而不使用中央服务器 ([awan2017s,](#bib.bib11)
    ; [chu2017efficient,](#bib.bib36) ; [wang2019impact,](#bib.bib222) ; [goyal2017accurate,](#bib.bib56)
    ; [jia2018highly,](#bib.bib81) )。如图 [4](#S4.F4 "图 4 ‣ 4\. 中央化/去中心化框架 ‣ 通信高效的数据并行分布式深度学习：全面调查")(b)
    所示，所有工作节点相互通信而没有中央节点，以从所有其他工作节点获取梯度。聚合后的梯度用于更新本地模型，从而实现与其他工作节点的一致性（不同工作节点中的初始化模型是相同的）。这是一个模型中心化的拓扑结构，因为通过同步获得了一个一致的全局模型，这使得更新方程
    ([5](#S2.E5 "在 2\. 分布式深度学习的分类 ‣ 通信高效的数据并行分布式深度学习：全面调查")) (BSP-SGD) 保持不变。然而，由于
    All-Reduce 的集体通信特性，它不适合异步通信。虽然在 SSP 的异步部分应用 All-Reduce 很困难，但在 SSP 的同步部分却很容易应用。
- en: 'The high-performance computing community has a long history of system optimizations
    for the All-Reduce collective, as various approaches proposed to improve its performance
     ([OptimizationofCollective,](#bib.bib148) ; [Thakur2005,](#bib.bib207) ; [Hoefler:2010,](#bib.bib70)
    ; [sanders2009two,](#bib.bib159) ). Some popular All-Reduce algorithms with different
    latency and bandwidth costs for an $N$-dimensional vector and an $n$-node cluster
    are summarized in Table [6](#S4.T6 "Table 6 ‣ 4.2\. All-Reduce ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey"). The communication cost of sending or receiving an $N$-dimensional
    vector is modeled as $\alpha+\beta N$  ([sarvotham2001connection,](#bib.bib161)
    ; [Shi2018MGWFBPED,](#bib.bib170) ).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 高性能计算社区在 All-Reduce 集体的系统优化方面有着悠久的历史，提出了各种方法来提高其性能 ([OptimizationofCollective,](#bib.bib148)
    ; [Thakur2005,](#bib.bib207) ; [Hoefler:2010,](#bib.bib70) ; [sanders2009two,](#bib.bib159)
    )。一些流行的 All-Reduce 算法在 $N$ 维向量和 $n$ 节点集群中的不同延迟和带宽成本汇总在表 [6](#S4.T6 "表 6 ‣ 4.2\.
    All-Reduce ‣ 4\. 中央化/去中心化框架 ‣ 通信高效的数据并行分布式深度学习：全面调查") 中。发送或接收 $N$ 维向量的通信成本被建模为
    $\alpha+\beta N$ ([sarvotham2001connection,](#bib.bib161) ; [Shi2018MGWFBPED,](#bib.bib170)
    )。
- en: Table 6\. Communication costs of some representative All-Reduce algorithms
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6\. 一些代表性 All-Reduce 算法的通信成本
- en: '| Algorithm | Latency | Bandwidth |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 延迟 | 带宽 |'
- en: '| Binary tree | $2\alpha\log n$ | $2\beta(\log n)N$ |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 二叉树 | $2\alpha\log n$ | $2\beta(\log n)N$ |'
- en: '| Recursive doubling | $\alpha\log n$ | $\beta(\log n)N$ |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 递归倍增 | $\alpha\log n$ | $\beta(\log n)N$ |'
- en: '| Ring | $2(n-1)\alpha$ | $\frac{2(n-1)}{n}\beta N$ |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 环形 | $2(n-1)\alpha$ | $\frac{2(n-1)}{n}\beta N$ |'
- en: 'Table [6](#S4.T6 "Table 6 ‣ 4.2\. All-Reduce ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey") summarizes various All-Reduce algorithms, which have been
    optimized by the high-performance computing community over time ([OptimizationofCollective,](#bib.bib148)
    ; [Thakur2005,](#bib.bib207) ; [Hoefler:2010,](#bib.bib70) ; [sanders2009two,](#bib.bib159)
    ). Among these algorithms, the ring-based All-Reduce is widely used in distributed
    DL due to its bandwidth optimality (e.g., Gloo²²2[https://github.com/facebookincubator/gloo](https://github.com/facebookincubator/gloo)
    and earlier versions of NCCL³³3[https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl).).
    However, the latency of the ring-based All-Reduce is linearly proportional to
    the number of workers, leading to high communication costs when scaling to large-scale
    clusters ([you2017scaling,](#bib.bib238) ; [Shi2018MGWFBPED,](#bib.bib170) ; [shi2021mgj,](#bib.bib171)
    ; [shi2021accelerating,](#bib.bib180) ). To address this issue, recent updates
    of NCCL (started from version 2.4)⁴⁴4[https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/](https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/)
    have integrated the double binary trees ([sanders2009two,](#bib.bib159) ) to perform
    an all-reduction to achieve full bandwidth and a logarithmic latency. The double
    binary trees require the whole message to be broken down into multiple blocks
    and the workers to be formed as a binary tree so that different blocks can be
    executed in parallel. Therefore, for some small messages or small-scale clusters,
    recursive doubling or ring-based algorithms would be better.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [6](#S4.T6 "Table 6 ‣ 4.2\. All-Reduce ‣ 4\. Centralized/Decentralized framework
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey") 总结了各种 All-Reduce 算法，这些算法经过高性能计算社区的优化 ([OptimizationofCollective,](#bib.bib148)
    ; [Thakur2005,](#bib.bib207) ; [Hoefler:2010,](#bib.bib70) ; [sanders2009two,](#bib.bib159))。在这些算法中，基于环的
    All-Reduce 在分布式深度学习中由于其带宽最优性被广泛使用（例如，Gloo²²2[https://github.com/facebookincubator/gloo](https://github.com/facebookincubator/gloo)
    和早期版本的 NCCL³³3[https://developer.nvidia.com/nccl](https://developer.nvidia.com/nccl)）。然而，基于环的
    All-Reduce 的延迟与工作节点的数量成线性关系，当扩展到大规模集群时会导致高通信成本 ([you2017scaling,](#bib.bib238)
    ; [Shi2018MGWFBPED,](#bib.bib170) ; [shi2021mgj,](#bib.bib171) ; [shi2021accelerating,](#bib.bib180))。为了解决这个问题，NCCL
    的最新更新（从版本 2.4 开始）⁴⁴4[https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/](https://devblogs.nvidia.com/massively-scale-deep-learning-training-nccl-2-4/)
    集成了双重二叉树 ([sanders2009two,](#bib.bib159)) 来执行全规约，以实现全面带宽和对数延迟。双重二叉树要求将整个消息分解成多个块，并将工作节点形成一个二叉树，以便不同的块可以并行执行。因此，对于一些小消息或小规模集群，递归加倍或基于环的算法可能更好。'
- en: To further reduce the latency term in All-Reduce while preserving the bandwidth
    optimality, the hierarchical All-Reduce algorithms  ([goyal2017accurate,](#bib.bib56)
    ; [jia2018highly,](#bib.bib81) ; [ueno2019exhaustive,](#bib.bib209) ) were also
    proposed, which can reduce the latency cost several times (related to the number
    of hierarchies). 2D-Torus All-Reduce ([mikami2018massively,](#bib.bib127) ; [jouppi2017datacenter,](#bib.bib84)
    ) can also massively reduce the communication latency using a 2D-Torus topology
    network. Under different topology networks (e.g., BCube  ([guo2009bcube,](#bib.bib59)
    )), it is also important to carefully design the All-Reduce algorithms to achieve
    lower latency and higher bandwidth. Wang et al.  ([wang2018bml,](#bib.bib221)
    ) proposed BML for the BCube topology to achieve efficient communication. Some
    topology-aware algorithms (e.g., BLink  ([wang2020blink,](#bib.bib214) ) and PLink ([luo2020plink,](#bib.bib116)
    )) were designed to be adaptive to distributed environments to highly utilize
    the network bandwidth with low latency.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在保持带宽最优的情况下进一步减少 All-Reduce 的延迟项，还提出了分层 All-Reduce 算法 ([goyal2017accurate,](#bib.bib56)
    ; [jia2018highly,](#bib.bib81) ; [ueno2019exhaustive,](#bib.bib209))，这些算法可以将延迟成本减少几倍（与层次数量相关）。2D-Torus
    All-Reduce ([mikami2018massively,](#bib.bib127) ; [jouppi2017datacenter,](#bib.bib84))
    也可以利用 2D-Torus 拓扑网络大幅减少通信延迟。在不同的拓扑网络下（例如，BCube ([guo2009bcube,](#bib.bib59)）），精心设计
    All-Reduce 算法以实现更低的延迟和更高的带宽也是很重要的。Wang 等 ([wang2018bml,](#bib.bib221)) 提出了针对 BCube
    拓扑的 BML 以实现高效通信。一些感知拓扑的算法（例如，BLink ([wang2020blink,](#bib.bib214)) 和 PLink ([luo2020plink,](#bib.bib116)））被设计为适应分布式环境，以高效利用网络带宽并保持低延迟。
- en: 4.3\. Gossip
  id: totrans-191
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. Gossip
- en: The Gossip architecture has emerged as a mechanism for inter-worker communication
    and is employed to solve the distributed average problem ([gossip1,](#bib.bib87)
    ; [gossip2,](#bib.bib114) ; [gossip3,](#bib.bib22) ; [gossip4,](#bib.bib37) ).
    Researchers have exploited this architecture to improve BSP-SGD  ([DisSubGradMultiOptim,](#bib.bib134)
    ; [AsynDisOptimRandADMM,](#bib.bib77) ; [OptimDisOptim,](#bib.bib166) ; [OptimNonsmoothDisOptim,](#bib.bib164)
    ; [DualApprochforOptim,](#bib.bib210) ; [MultiagentMirrorDescentDenct,](#bib.bib147)
    ; [CommEffDenctStoc,](#bib.bib96) ; [CanDecent,](#bib.bib108) ; [DecentTrainingoverDecentData,](#bib.bib202)
    ; [StocGradPush,](#bib.bib10) ; [CommCompforDecent,](#bib.bib201) ).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Gossip架构已经成为一种用于工人间通信的机制，并被用来解决分布式平均问题 ([gossip1,](#bib.bib87) ; [gossip2,](#bib.bib114)
    ; [gossip3,](#bib.bib22) ; [gossip4,](#bib.bib37) )。研究人员已经利用这一架构来改进BSP-SGD ([DisSubGradMultiOptim,](#bib.bib134)
    ; [AsynDisOptimRandADMM,](#bib.bib77) ; [OptimDisOptim,](#bib.bib166) ; [OptimNonsmoothDisOptim,](#bib.bib164)
    ; [DualApprochforOptim,](#bib.bib210) ; [MultiagentMirrorDescentDenct,](#bib.bib147)
    ; [CommEffDenctStoc,](#bib.bib96) ; [CanDecent,](#bib.bib108) ; [DecentTrainingoverDecentData,](#bib.bib202)
    ; [StocGradPush,](#bib.bib10) ; [CommCompforDecent,](#bib.bib201) )。
- en: 'As illustrated in Fig. [4](#S4.F4 "Figure 4 ‣ 4\. Centralized/Decentralized
    framework ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A
    Comprehensive Survey")(c), the gossip architecture does not require global models
    or parameter servers (represented by different colors of local models). Instead,
    each worker communicates updates with their neighbors (also named peers). Workers
    exchange messages via edges chosen in each iteration (the blues ones in Fig. [4](#S4.F4
    "Figure 4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")(c)), which can
    be described as a communication graph. Workers are not required to communicate
    with all others, reducing communication costs. During training, the algorithm
    does not guarantee parameter consistency across all workers after each communication,
    but guarantees it (i.e., consensus) at the end of the algorithm. This means that
    local models are different during every iteration. It should be noted that in
    asynchronous and SSP with the PS architecture, although local models are also
    different, a global model is still maintained.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '如图 [4](#S4.F4 "Figure 4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")(c) 所示，gossip架构不需要全局模型或参数服务器（由不同颜色的本地模型表示）。相反，每个工人都与他们的邻居（也称为对等体）通信更新。工人通过在每次迭代中选择的边（图
    [4](#S4.F4 "Figure 4 ‣ 4\. Centralized/Decentralized framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")(c) 中的蓝色边）交换消息，这可以描述为一个通信图。工人不需要与所有其他工人通信，从而减少了通信成本。在训练过程中，算法并不保证每次通信后的所有工人之间的参数一致性，但在算法结束时保证一致性（即，共识）。这意味着在每次迭代中，本地模型是不同的。需要注意的是，在使用PS架构的异步和SSP中，尽管本地模型也不同，但仍然维护一个全局模型。'
- en: Similar to the All-Reduce architecture, gossip architecture offers the benefit
    of having no master node, thus eliminating communication issues of the central
    server ([CommCompforDecent,](#bib.bib201) ). Compared to All-Reduce architecture,
    the gossip architecture is more fault-tolerant to the worker failures and it can
    also be theoretically guaranteed ([CanDecent,](#bib.bib108) ) with less communication
    costs than PS or All-Reduce.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与All-Reduce架构类似，gossip架构的一个好处是没有主节点，从而消除了中央服务器的通信问题 ([CommCompforDecent,](#bib.bib201)
    )。与All-Reduce架构相比，gossip架构对工人故障的容错性更强，并且理论上可以保证 ([CanDecent,](#bib.bib108) )，其通信成本低于PS或All-Reduce。
- en: In the gossip architecture, the key issue is ensuring the attainment of identical
    model parameters across all workers. This problem is referred to as “consensus”
    and has been studied extensively in the literature  ([7942055DistributedLinearized,](#bib.bib12)
    ; [1498447Gossipalgorithms,](#bib.bib21) ; [Carli2010,](#bib.bib26) ; [4497789Randomizedconsensus,](#bib.bib52)
    ; [4118472ConsensusandCooperation,](#bib.bib138) ; [4434671Adistributedconsensus,](#bib.bib165)
    ). Formally, consensus is a state of all workers attaining a same opinion in common.
    In the gossip distributed SGD, the consensus is the status that all workers have
    the identical model parameters.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在gossip架构中，关键问题是确保所有工人都获得相同的模型参数。这个问题被称为“共识”，在文献中已有广泛研究 ([7942055DistributedLinearized,](#bib.bib12)
    ; [1498447Gossipalgorithms,](#bib.bib21) ; [Carli2010,](#bib.bib26) ; [4497789Randomizedconsensus,](#bib.bib52)
    ; [4118472ConsensusandCooperation,](#bib.bib138) ; [4434671Adistributedconsensus,](#bib.bib165)
    )。正式来说，共识是所有工人达成相同意见的状态。在gossip分布式SGD中，共识是所有工人拥有相同的模型参数的状态。
- en: However, on one hand, achieving the “consensus” incurs the difficulty in designing
    a method that enables efficient communication, high convergence rate, and consensus
    simultaneously. To address these issues, model compression can be combined with
    decentralized learning algorithms to reduce communication costs  ([CommCompforDecent,](#bib.bib201)
    ; [DecentStocOptimAndGossip,](#bib.bib90) ; [NIPS2018_7705,](#bib.bib68) ; [tang2020communication,](#bib.bib204)
    ). On the other hand, the Gossip architecture is limited to using symmetric communication,
    which inherently requires deadlock-avoidance and more synchronization, resulting
    in slower and more sensitivity to stragglers. Assran et al. ([StocGradPush,](#bib.bib10)
    ) proposed a solution that combines Stochastic Gradient Push (SGP) ([SGPDirected,](#bib.bib135)
    ) with PUSHSUM ([8340193,](#bib.bib136) ; [gossip1,](#bib.bib87) ). The PUSHSUM
    provides an approximation of the distributed averaging, while the SGP makes each
    worker only send the gradient to its out-neighbors without waiting for the response
    from these neighbors. Thus, the overall system throughput is improved.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一方面，实现“共识”会遇到设计一种能够高效通信、高收敛速率和同时达成共识的方法的困难。为了解决这些问题，可以将模型压缩与去中心化学习算法相结合，以减少通信成本
    ([CommCompforDecent,](#bib.bib201) ; [DecentStocOptimAndGossip,](#bib.bib90) ;
    [NIPS2018_7705,](#bib.bib68) ; [tang2020communication,](#bib.bib204) )。另一方面，Gossip架构局限于使用对称通信，这本质上需要避免死锁和更多的同步，导致速度较慢且对拖延者更敏感。Assran等人
    ([StocGradPush,](#bib.bib10) ) 提出了一个解决方案，将随机梯度推送（SGP） ([SGPDirected,](#bib.bib135)
    ) 与PUSHSUM ([8340193,](#bib.bib136) ; [gossip1,](#bib.bib87) ) 相结合。PUSHSUM提供了分布式平均的近似，而SGP使每个工作节点仅将梯度发送给其邻居而无需等待这些邻居的响应。因此，整体系统吞吐量得到提高。
- en: 4.4\. Experimental Comparison
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4\. 实验比较
- en: 'We conduct experiments to compare BSP-SGD⁵⁵5From the algorithmic view, the
    BSP-SGD algorithm under the All-Reduce architecture is identical to that under
    the PS architecture. Thus, we choose one to compare the convergence performance.
    and synchronous data-parallel SGD (DP-SGD) with the Gossip architecture. Table [7](#S4.T7
    "Table 7 ‣ 4.4\. Experimental Comparison ‣ 4\. Centralized/Decentralized framework
    ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey") indicates that BSP-SGD and DP-SGD (Gossip) can achieve similar performance.
    DP-SGD (Gossip) exhibits a decrease in performance as the number of workers increases,
    consistent with the results presented in Table [3](#S3.T3 "Table 3 ‣ 3.5\. Experimental
    Comparison ‣ 3\. Synchronous/asynchronous framework ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey").'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行实验以比较BSP-SGD⁵⁵5从算法角度来看，All-Reduce架构下的BSP-SGD算法与PS架构下的算法是相同的。因此，我们选择其中之一来比较收敛性能。与Gossip架构的同步数据并行SGD
    (DP-SGD)进行比较。表[7](#S4.T7 "表 7 ‣ 4.4\. 实验比较 ‣ 4\. 集中/分散框架 ‣ 通信高效的数据并行分布式深度学习：全面调查")
    表明BSP-SGD和DP-SGD (Gossip)可以实现类似的性能。DP-SGD (Gossip)在工人数增加时性能下降，这与表[3](#S3.T3 "表
    3 ‣ 3.5\. 实验比较 ‣ 3\. 同步/异步框架 ‣ 通信高效的数据并行分布式深度学习：全面调查")中展示的结果一致。
- en: Table 7\. Test accuracy [%] comparison of different communication architectures.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 7\. 不同通信架构的测试准确率 [%] 比较。
- en: '| Algorithm | # of Worker | Resnet20 | RNN |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 工作节点数 | Resnet20 | RNN |'
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
- en: '| BSP-SGD (PS) | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28
    | 47.70 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| BSP-SGD (PS) | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28
    | 47.70 |'
- en: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
- en: '| DP-SGD (Gossip) | 4 | 84.60 | 89.38 | 91.08 | 91.01 | 55.41 | 55.19 | 52.92
    | 48.13 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| DP-SGD (Gossip) | 4 | 84.60 | 89.38 | 91.08 | 91.01 | 55.41 | 55.19 | 52.92
    | 48.13 |'
- en: '| 32 | 64.20 | 85.12 | 88.99 | 89.71 | 53.55 | 51.71 | 53.77 | 46.50 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.20 | 85.12 | 88.99 | 89.71 | 53.55 | 51.71 | 53.77 | 46.50 |'
- en: 5\. Quantization methods
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5\. 量化方法
- en: Compression methods aim to reduce communication data by compressing gradients
    or model parameters that need to be transmitted between workers or servers. However,
    most of these methods use lossy compression, which prevents the receiver from
    fully recovering the original gradients or model parameters. As a result, convergence
    may be impacted due to the reduced amount of information available. It has been
    an active research direction to reduce the communication traffic with little impact
    on the convergence.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩方法旨在通过压缩梯度或模型参数来减少通信数据，这些梯度或模型参数需要在工作节点或服务器之间传输。然而，大多数这些方法使用有损压缩，这会阻止接收方完全恢复原始梯度或模型参数。因此，由于信息量减少，收敛性可能会受到影响。减少通信流量且对收敛性影响较小已成为一个活跃的研究方向。
- en: '![Refer to caption](img/58928c883b79d2ce172a2143bc953be5.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![参考标题](img/58928c883b79d2ce172a2143bc953be5.png)'
- en: Figure 5\. Comparison of Quantization and Sparsification.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5\. 量化与稀疏化的比较。
- en: 'Quantization is a one of popular compression schemes that uses lower bits to
    represent data originally represented by 32 bits on each dimension of the transmitted
    gradient. As shown in Fig. [5](#S5.F5 "Figure 5 ‣ 5\. Quantization methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey"), the element
    in the quantized gradient is coded by fewer bits, resulting in low-precision gradients
    after quantization. Low-precision gradients are beneficial to DL as higher-speed
    calculations and lower memory are needed for training DNNs on CPUs and GPUs. Many
    researchers have studied DL convergence under low-precision gradients with different
    quantization methods ([ImproSpeedNN,](#bib.bib212) ; [DLwithlimited,](#bib.bib61)
    ; [ProbroundingNN,](#bib.bib72) ; [DorefaNet,](#bib.bib262) ; [Hubara2017,](#bib.bib76)
    ). The process of quantized SGD can be formulated by as following equations:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 量化是一种流行的压缩方案，它使用更少的位来表示原本由 32 位表示的每个维度的传输梯度。如图 [5](#S5.F5 "图 5 ‣ 5\. 量化方法 ‣
    通信高效的数据并行分布式深度学习：全面综述") 所示，量化梯度中的元素由更少的位编码，导致量化后的低精度梯度。低精度梯度有利于深度学习，因为在 CPU 和
    GPU 上训练深度神经网络时需要更高速度的计算和更少的内存。许多研究人员研究了低精度梯度下的深度学习收敛性，采用了不同的量化方法 ([ImproSpeedNN,](#bib.bib212)
    ; [DLwithlimited,](#bib.bib61) ; [ProbroundingNN,](#bib.bib72) ; [DorefaNet,](#bib.bib262)
    ; [Hubara2017,](#bib.bib76) )。量化的 SGD 过程可以通过以下方程式表示：
- en: '| (9) |  | $G_{i,t}^{quant}(\mathbf{x}_{t})=Quant(G_{i,t}(\mathbf{x}_{t})+\delta_{i,t}(\mathbf{x}_{t}))$
    |  |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| (9) |  | $G_{i,t}^{quant}(\mathbf{x}_{t})=Quant(G_{i,t}(\mathbf{x}_{t})+\delta_{i,t}(\mathbf{x}_{t}))$
    |  |'
- en: '| (10) |  | $\delta_{i,t}(\mathbf{x}_{t})=G_{i,t}(\mathbf{x}_{t})-{Quant}^{-1}(G_{i,t}^{quant}(\mathbf{x}_{t}))$
    |  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| (10) |  | $\delta_{i,t}(\mathbf{x}_{t})=G_{i,t}(\mathbf{x}_{t})-{Quant}^{-1}(G_{i,t}^{quant}(\mathbf{x}_{t}))$
    |  |'
- en: '| (11) |  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}\frac{1}{n}\sum_{i=1}^{n}G_{i,t}^{quant}(\mathbf{x}_{t})$
    |  |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| (11) |  | $\mathbf{x}_{t+1}=\mathbf{x}_{t}-{\gamma}\frac{1}{n}\sum_{i=1}^{n}G_{i,t}^{quant}(\mathbf{x}_{t})$
    |  |'
- en: 'where $Quant(\cdot)$ denotes the quantization function that encodes gradients,
    and $Quant(\cdot)$ the unquantizer that decodes quantized gradients. This approach
    reduces communication costs as $G_{i,t}^{quant}(\mathbf{x_{t}})$, compared to
    the approach described in Eq. ([4](#S2.E4 "In 2\. Taxonomy of Distributed DL ‣
    Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")).'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $Quant(\cdot)$ 表示编码梯度的量化函数，而 $Quant(\cdot)$ 表示解码量化梯度的解量化函数。与方程式中描述的方法相比，这种方法降低了通信成本，见
    Eq. ([4](#S2.E4 "在 2\. 分布式深度学习的分类 ‣ 通信高效的数据并行分布式深度学习：全面综述"))。
- en: In  ([DisMeanEst,](#bib.bib198) ) and ([DisMeanEst,](#bib.bib198) ), the communication
    of gradients in BSP-SGD actually is regarded as a distributed mean estimation
    problem ([DisMeanEst,](#bib.bib198) ; [RandDisMean,](#bib.bib92) ). They  ([DisMeanEst,](#bib.bib198)
    ; [RandDisMean,](#bib.bib92) ) analyzed the communication-efficient algorithms
    for distributed mean estimation. They used the Mean Squared Error (MSE) to measure
    how accurate the quantization methods are. And then, they proposed coding strategies
    to achieve the best MSE for a given communication cost, considering that increasing
    the number of quantization levels increases the communication cost. To reduce
    the communication cost, Suresh et al. ([DisMeanEst,](#bib.bib198) ) proposed two
    methods, Stochastic Rotated Quantization (SRQ) and Variable Length Coding (VLC).
    In SRQ, all clients and the central server generate a global random rotation matrix
    and then try to find an orthogonal matrix $\mathbb{R}$ that achieves low MSE.
    The VLC uses arithmetic of Huffman Coding corresponding to the number of times
    each quantized value has appeared.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([DisMeanEst,](#bib.bib198) ) 和 ([DisMeanEst,](#bib.bib198) ) 中，BSP-SGD中梯度的通信实际上被视为一个分布式均值估计问题
    ([DisMeanEst,](#bib.bib198) ; [RandDisMean,](#bib.bib92) )。他们 ([DisMeanEst,](#bib.bib198)
    ; [RandDisMean,](#bib.bib92) ) 分析了用于分布式均值估计的通信高效算法。他们使用均方误差 (MSE) 来衡量量化方法的准确性。随后，他们提出了编码策略，以在给定通信成本的情况下实现最佳
    MSE，考虑到增加量化级别的数量会增加通信成本。为了减少通信成本，Suresh 等人 ([DisMeanEst,](#bib.bib198) ) 提出了两种方法，随机旋转量化
    (SRQ) 和可变长度编码 (VLC)。在 SRQ 中，所有客户端和中央服务器生成一个全局随机旋转矩阵，然后尝试找到一个正交矩阵 $\mathbb{R}$，以实现较低的
    MSE。VLC 使用与每个量化值出现次数相对应的霍夫曼编码的算术运算。
- en: 'To achieve a higher compression ratio, Sei et al. ([1bit,](#bib.bib167) ) proposed
    1-bit SGD to reduce the transmitted data volumes, and successfully trained the
    deep model on traditional speech applications with a 10$\times$ speedup. They
    reduced each gradient element to one bit and quantified a gradient $G_{i,t}(\mathbf{x}_{t})$
    while keeping the quantization error $\delta_{i,t}(\mathbf{x}_{t})$ not large
    at the same time. The key idea of 1-bit quantization is also used in ([scalableDisDNN,](#bib.bib190)
    ). Different from 1-bit SGD, Strom et al. ([scalableDisDNN,](#bib.bib190) ) chose
    a fixed threshold, and then encoded the gradient elements higher than $T$ with
    the value $1$, those less than $–T$ with value 0\. The absolute value of a gradient
    element less than $T$ would not be sent, which is similar to the sparsification
    methods that will be discussed in §[6](#S6 "6\. Sparsification Methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey").'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '为了实现更高的压缩比，Sei 等人 ([1bit,](#bib.bib167) ) 提出了 1-bit SGD 以减少传输的数据量，并成功地在传统语音应用上训练了深度模型，实现了
    10$\times$ 的加速。他们将每个梯度元素减少到一位，并量化梯度 $G_{i,t}(\mathbf{x}_{t})$ 的同时保持量化误差 $\delta_{i,t}(\mathbf{x}_{t})$
    不大。1-bit 量化的关键思想也被用于 ([scalableDisDNN,](#bib.bib190) )。与 1-bit SGD 不同，Strom 等人
    ([scalableDisDNN,](#bib.bib190) ) 选择了一个固定的阈值，然后将高于 $T$ 的梯度元素编码为值 $1$，将低于 $–T$
    的编码为值 0。小于 $T$ 的梯度元素的绝对值不会被发送，这类似于将在 §[6](#S6 "6\. Sparsification Methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey") 中讨论的稀疏化方法。'
- en: Some researchers further proposed adaptive quantization methods  ([7544448,](#bib.bib144)
    ; [7526802,](#bib.bib124) ) that incorporate inexact proximal gradients. However,
    these methods lack empirical validation with respect to deep learning models.
    To achieve both communication efficiency and good optimization convergence, Alistarh
    et al. ([QSGD,](#bib.bib7) ) presented a family of algorithms using quantization
    rather than just one quantization method. This family of quantization algorithms,
    named Quantized SGD (QSGD), allows for the trade-off between the number of communicated
    bits and the variance of the compressed gradient. They evaluated the performance
    of QSGD by training DNNs on ImageNet ([Imagenet,](#bib.bib45) ) and CIFAR-10 datasets,
    achieving accuracy rates comparable to those of original SGD. Specifically, QSGD
    exploits a probabilistic approach to quantify a vector. Given any vector $\mathbf{v}\in\mathbb{R}^{N}$,
    every $j$-th element $Quant_{s}({v}_{j})$ of quantization gradient $Quant_{s}(\mathbf{v})$
    is quantized by $Quant_{s}(\cdot)$, corresponding to the element ${v}_{j}$ of
    the original gradient $\mathbf{v}$. The stochastic quantization function is defined
    as
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一些研究人员进一步提出了自适应量化方法（[7544448,](#bib.bib144) ; [7526802,](#bib.bib124)），这些方法结合了不精确的邻近梯度。然而，这些方法在深度学习模型中缺乏实证验证。为了实现通信效率和良好的优化收敛性，Alistarh
    等人（[QSGD,](#bib.bib7)）提出了一系列算法，采用量化而不仅仅是单一量化方法。这些量化算法被称为量化 SGD（QSGD），允许在通信比特数量和压缩梯度的方差之间进行权衡。他们通过在
    ImageNet（[Imagenet,](#bib.bib45)）和 CIFAR-10 数据集上训练 DNNs 来评估 QSGD 的性能，取得了与原始 SGD
    相当的准确率。具体来说，QSGD 利用概率方法对向量进行量化。给定任意向量 $\mathbf{v}\in\mathbb{R}^{N}$，量化梯度 $Quant_{s}(\mathbf{v})$
    的每个 $j$-th 元素 $Quant_{s}({v}_{j})$ 通过 $Quant_{s}(\cdot)$ 进行量化，对应于原始梯度 $\mathbf{v}$
    的元素 ${v}_{j}$。随机量化函数定义为
- en: '| (12) |  | $Quant_{s}(v_{j})=\left\&#124;\mathbf{v}\right\&#124;_{2}\cdot
    sgn(v_{j})\cdot\psi_{j}(\mathbf{v},s),$ |  |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| (12) |  | $Quant_{s}(v_{j})=\left\|\mathbf{v}\right\|_{2}\cdot sgn(v_{j})\cdot\psi_{j}(\mathbf{v},s),$
    |  |'
- en: where $sgn(v_{j})\in\left\{-1,+1\right\}$ denotes the sign of $v_{j}$ and $sgn(0)=1$.
    $\psi_{j}(\mathbf{v},s)$ is defined as
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 $sgn(v_{j})\in\left\{-1,+1\right\}$ 表示 $v_{j}$ 的符号，且 $sgn(0)=1$。$\psi_{j}(\mathbf{v},s)$
    定义为
- en: '| (13) |  | $\psi_{j}(\mathbf{v},s)=\left\{\begin{array}[]{ll}l/s&amp;\text{with
    probability}\ 1-p(\frac{\left&#124;v_{j}\right&#124;}{\left\&#124;\mathbf{v}\right\&#124;_{2}},s),\\
    (l+1)/s&amp;\text{otherwise}\end{array}\right.$ |  |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| (13) |  | $\psi_{j}(\mathbf{v},s)=\left\{\begin{array}[]{ll}l/s\ \text{的概率为}\
    1-p(\frac{\left|v_{j}\right|}{\left\|\mathbf{v}\right\|_{2}},s),\\ (l+1)/s\ \text{否则}\end{array}\right.$
    |  |'
- en: $l$ is an integer such that $\left|v_{i}\right|/\left\|\mathbf{v}\right\|_{2}\in\left[l/s,(l+1)/s\right]$
    s.t. $0<l<s$, and $p(a,s)=as-l$ for any $a\in\left[0,1\right]$. This is called
    Standard (random) dithering in  ([Horvath2019NaturalCF,](#bib.bib73) ), and is
    used in PCM coding  ([6773262,](#bib.bib54) ; [1057702,](#bib.bib155) ). Note
    that if $\mathbf{v}=\mathbf{0}$, the algorithm sets $Quant_{s}(\mathbf{v},s)=\mathbf{0}$.
    The expectation $\psi_{j}(\mathbf{v},s)$ satisfies $\mathbb{E}\left[\psi_{j}(\mathbf{v},s)\right]=\left|v_{i}\right|/\left\|\mathbf{v}\right\|_{2}$.
    Then it is obvious that QSGD can assure that the quantized gradient is the unbiased
    estimation of the original vector, i.e., $\mathbb{E}\left[Quant_{s}(\mathbf{v})\right]=\mathbf{v}$,
    and facilitates convergence of training.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: $l$ 是一个整数，使得 $\left|v_{i}\right|/\left\|\mathbf{v}\right\|_{2}\in\left[l/s,(l+1)/s\right]$，其中
    $0<l<s$，且 $p(a,s)=as-l$ 对于任何 $a\in\left[0,1\right]$。这称为标准（随机）抖动（[Horvath2019NaturalCF,](#bib.bib73)），并用于
    PCM 编码（[6773262,](#bib.bib54) ; [1057702,](#bib.bib155)）。注意，如果 $\mathbf{v}=\mathbf{0}$，算法将
    $Quant_{s}(\mathbf{v},s)=\mathbf{0}$。期望值 $\psi_{j}(\mathbf{v},s)$ 满足 $\mathbb{E}\left[\psi_{j}(\mathbf{v},s)\right]=\left|v_{i}\right|/\left\|\mathbf{v}\right\|_{2}$。因此，QSGD
    可以确保量化梯度是原始向量的无偏估计，即 $\mathbb{E}\left[Quant_{s}(\mathbf{v})\right]=\mathbf{v}$，并促进训练的收敛性。
- en: Wen et al. ([wen2017terngrad,](#bib.bib228) ) proposed Terngrad, which differs
    from the Parameter Server architecture. Each worker stores a copy of parameters
    locally, and the communication of parameters in the floating-point form is changed
    to the transfer of quantized gradients. This results in smaller server-to-worker
    traffic since only the quantized gradients are pulled from servers. To minimize
    the number of levels when workers are transferring different scalars, Terngrad
    presented Scalar Sharing, which selects the maximum scalar among all scalars and
    shares it across all workers. However, in a large deep model, a maximum element
    could be significantly larger than most gradients, which could weaken the approximation.
    To address this issue, Terngrad proposed layer-wise ternarizing and Gradient Clipping.
    Layer-wise ternarizing utilizes the layer-wise scalar in each layer instead of
    a large global maximum scalar. And A series of works ([faghri2020adaptive,](#bib.bib51)
    ; [jhunjhunwala2021adaptive,](#bib.bib80) ; [ElasticQuant,](#bib.bib192) ) proposed
    to utilize adaptive quantization bits, to enhance the convergence with compressed
    gradients.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Wen 等人（[wen2017terngrad](#bib.bib228)）提出了 Terngrad，它不同于参数服务器架构。每个工作节点在本地存储参数的副本，将参数的浮点形式传输改为量化梯度的传输。这减少了服务器到工作节点的流量，因为仅从服务器提取量化梯度。为了最小化当工作节点传输不同标量时的层级数量，Terngrad
    提出了标量共享，选择所有标量中的最大标量，并在所有工作节点间共享。然而，在大型深度模型中，最大元素可能显著大于大多数梯度，这可能削弱近似效果。为解决这一问题，Terngrad
    提出了逐层三值化和梯度剪切。逐层三值化在每层中使用逐层标量，而不是大的全局最大标量。一系列工作（[faghri2020adaptive](#bib.bib51)
    ; [jhunjhunwala2021adaptive](#bib.bib80) ; [ElasticQuant](#bib.bib192)）提议利用自适应量化位，以增强压缩梯度的收敛性。
- en: Sign-SGD is another kind of quantization method ([signSGD,](#bib.bib17) ). In
    Sign-SGD, every worker quantifies the gradient to a binary value, which is the
    sign of each coordinate of the gradient vector. Bernstein et al.  ([signSGDwithVote,](#bib.bib18)
    ) provided an extensive theoretical analysis of Sign-SGD for non-convex optimization.
    They proved that when gradients are as dense or denser than stochasticity and
    curvature, Sign-SGD can converge with a theoretical rate. They also proposed a
    new algorithm named Sign-SGD with a majority vote. After workers exchange the
    sign of their gradient vector to a server, the overall update is decided by a
    majority vote. Mishchenko et al.  ([DisLearningDIANA,](#bib.bib130) ) introduced
    a novel method DIANA, which extends the methods of ([QSGD,](#bib.bib7) ; [TernGrad,](#bib.bib227)
    ) by splitting the whole gradient vector into some sub-vectors. And then, they
    quantified each sub-vector individually.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: Sign-SGD 是另一种量化方法（[signSGD](#bib.bib17)）。在 Sign-SGD 中，每个工作节点将梯度量化为二进制值，即梯度向量每个坐标的符号。Bernstein
    等人（[signSGDwithVote](#bib.bib18)）对非凸优化中的 Sign-SGD 进行了广泛的理论分析。他们证明了当梯度的密度与随机性和曲率相当或更高时，Sign-SGD
    可以以理论速度收敛。他们还提出了一种新的算法，名为“Sign-SGD with majority vote”。在这个算法中，工作节点将梯度向量的符号交换到服务器后，整体更新由多数投票决定。Mishchenko
    等人（[DisLearningDIANA](#bib.bib130)）介绍了一种新方法 DIANA，该方法通过将整个梯度向量拆分为若干子向量，从而扩展了（[QSGD](#bib.bib7)
    ; [TernGrad](#bib.bib227)）的方法。然后，他们对每个子向量进行了单独量化。
- en: 6\. Sparsification Methods
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6\. 稀疏化方法
- en: Quantization methods are limited in their compression rates, with a maximum
    compression rate of 32$\times$ achievable over commonly used SGD with single-precision
    floating-point arithmetic. However, reducing the number of transmitted elements
    can increase the compression rate even further. A set of algorithms that follow
    this approach are known as sparsification methods, where only a subset of elements
    are selected and transmitted ([meProp,](#bib.bib197) ; [ZipML,](#bib.bib247) ;
    [EffiUseofLimitMemory,](#bib.bib49) ; [NIPS2019_9610,](#bib.bib14) ; [10.1145/3452296.3472904,](#bib.bib53)
    ; [li2022on,](#bib.bib106) ; [shi2021towards,](#bib.bib182) ; [li2022near,](#bib.bib105)
    ; [zhang2023evaluation,](#bib.bib252) ).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 量化方法在压缩率方面有限，通常情况下，32$\times$ 的最大压缩率可实现于使用单精度浮点运算的 SGD。然而，减少传输元素的数量可以进一步提高压缩率。一组遵循这种方法的算法被称为稀疏化方法，其中仅选择和传输部分元素（[meProp](#bib.bib197)
    ; [ZipML](#bib.bib247) ; [EffiUseofLimitMemory](#bib.bib49) ; [NIPS2019_9610](#bib.bib14)
    ; [10.1145/3452296.3472904](#bib.bib53) ; [li2022on](#bib.bib106) ; [shi2021towards](#bib.bib182)
    ; [li2022near](#bib.bib105) ; [zhang2023evaluation](#bib.bib252)）。
- en: 'The key idea of sparsification is that only “significant” gradients are essential
    for the SGD update to ensure the convergence of the training process ([DGC,](#bib.bib113)
    ). As illustrated in Fig. [5](#S5.F5 "Figure 5 ‣ 5\. Quantization methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey"), a substantial
    proportion of the coordinates in the gradient vector can be zeroed-out such that
    the zero-valued elements are no need to transmit. Gradient sparsification is a
    more aggressive compression method than quantization, enabling much more significant
    reductions in communication traffic.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '稀疏化的关键思想是，只有“重要的”梯度对于 SGD 更新是必需的，以确保训练过程的收敛([DGC,](#bib.bib113) )。如图 [5](#S5.F5
    "Figure 5 ‣ 5\. Quantization methods ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey") 所示，梯度向量中的大量坐标可以被置零，从而零值元素不需要传输。梯度稀疏化是一种比量化更激进的压缩方法，使通信流量大幅减少。'
- en: 'The antecedents of sparsification methods can be traced back to the Truncated
    Gradient method introduced by Langford et al. ([SparOLviaTrunc,](#bib.bib97) )
    in the context of online learning algorithms. Truncated Gradient addresses memory
    space and computation constraints by inducing sparsity in the gradient. Rather
    than directly setting small coordinates to zero, Truncated Gradient keeps large
    coordinates with their original values and discards small coordinates that fall
    below a threshold. This method was the first sparsification technique developed
    for large-scale learning. After that, there are extensive studies to further improve
    sparsification in distributed DL. The sparsification methods can be broadly classified
    into four main types: coordinate descent, random sparsification, deterministic
    sparsification, and proximal methods.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 稀疏化方法的前身可以追溯到 Langford 等人提出的 Truncated Gradient 方法([SparOLviaTrunc,](#bib.bib97)
    )，用于在线学习算法。Truncated Gradient 通过在梯度中引入稀疏性来解决内存空间和计算约束。Truncated Gradient 并不是直接将小坐标置为零，而是保留大坐标的原始值，并丢弃小于阈值的坐标。这种方法是第一个为大规模学习开发的稀疏化技术。之后，出现了大量研究进一步改进分布式深度学习中的稀疏化方法。稀疏化方法大致可分为四种主要类型：坐标下降、随机稀疏化、确定性稀疏化和邻近方法。
- en: 6.1\. Random Sparsification
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. 随机稀疏化
- en: Random sparsification randomly selects a subset of entries from the gradient
    vector for communication and updating. This approach is also referred to as random-$k$,
    where $k$ denotes the number of selected elements.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随机稀疏化从梯度向量中随机选择一部分条目进行通信和更新。这种方法也称为随机-$k$，其中 $k$ 表示选择的元素数量。
- en: In ([FLStrategy,](#bib.bib91) ), Konecny et al. proposed Random Mask and Subsampling.
    In Random Mask, a pre-defined random sparsity pattern, to convert the update parameters
    $\mathbf{H}$ into a sparse matrix $\hat{\mathbf{H}}$. This random pattern is regenerated
    in each training iteration. In addition, it can be initialized independently by
    each client or created by the server and then distributed to the workers. In the
    former case, each worker must transmit both the indices and values of the non-zero
    entries of $\mathbf{H}$. In the latter case, workers need only to send the values
    of the non-zero entries of $\mathbf{H}$, because all indices of non-zero entries
    in every worker are the same as others. In Subsampling, unlike Random Mask, the
    sparse communication matrix $\hat{\mathbf{H}}$ is scaled to ensure that $\mathbf{E}(\hat{\mathbf{H}})=\mathbf{H}$,
    making it an unbiased estimator of the true average.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在([FLStrategy,](#bib.bib91) )中，Konecny 等人提出了随机掩码和子采样。在随机掩码中，预定义的随机稀疏模式将更新参数
    $\mathbf{H}$ 转换为稀疏矩阵 $\hat{\mathbf{H}}$。这个随机模式在每次训练迭代中都会重新生成。此外，它可以由每个客户端独立初始化，也可以由服务器创建后分发给工作节点。在前一种情况下，每个工作节点必须传输
    $\mathbf{H}$ 的非零条目的索引和值。在后一种情况下，工作节点只需发送 $\mathbf{H}$ 的非零条目的值，因为每个工作节点中非零条目的所有索引都是相同的。在子采样中，与随机掩码不同，稀疏通信矩阵
    $\hat{\mathbf{H}}$ 被缩放以确保 $\mathbf{E}(\hat{\mathbf{H}})=\mathbf{H}$，使其成为真实平均值的无偏估计。
- en: 'In  ([GradSparforDisOptim,](#bib.bib225) ), Wangni et al. proposed to randomly
    drop out coordinates with a probability vector $\mathbf{p}$ and amplifying the
    non-zero coordinates from $g_{j}$ to $g_{j}/p_{j}$. Formally, if one wants to
    compress original vector $\mathbf{g}=\left[g_{1},g_{2},\cdots,g_{N}\right]$, given
    a probability vector $\mathbf{p}=\left[p_{1},p_{2},\cdots,p_{N}\right]$, and the
    final sparsified vector is $\mathbf{Q}_{spar}(\mathbf{g})=\left[Z_{1}\frac{g_{1}}{p_{1}},Z_{2}\frac{g_{2}}{p_{2}},\cdots,Z_{N}\frac{g_{N}}{p_{N}}\right]$,
    in which $Z_{i}$ represents selector, i.e. 0 or 1\. Each item $\mathbf{Q}_{spar}(\mathbf{g})_{i}$
    has the unbiased expectation: $\mathbb{E}\left[\mathbf{Q}_{spar}(\mathbf{g})_{i}\right]=p_{i}\times\frac{g_{i}}{p_{i}}+(1-p_{i})\times
    0=g_{i}$, similar to ([QSGD,](#bib.bib7) ; [FLStrategy,](#bib.bib91) ).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ([GradSparforDisOptim,](#bib.bib225)) 中，王倪等提出了通过概率向量 $\mathbf{p}$ 随机丢弃坐标，并将非零坐标从
    $g_{j}$ 放大到 $g_{j}/p_{j}$。正式地，如果要压缩原始向量 $\mathbf{g}=\left[g_{1},g_{2},\cdots,g_{N}\right]$，给定概率向量
    $\mathbf{p}=\left[p_{1},p_{2},\cdots,p_{N}\right]$，最终的稀疏向量为 $\mathbf{Q}_{spar}(\mathbf{g})=\left[Z_{1}\frac{g_{1}}{p_{1}},Z_{2}\frac{g_{2}}{p_{2}},\cdots,Z_{N}\frac{g_{N}}{p_{N}}\right]$，其中
    $Z_{i}$ 代表选择器，即 0 或 1。每个项 $\mathbf{Q}_{spar}(\mathbf{g})_{i}$ 的无偏期望为：$\mathbb{E}\left[\mathbf{Q}_{spar}(\mathbf{g})_{i}\right]=p_{i}\times\frac{g_{i}}{p_{i}}+(1-p_{i})\times
    0=g_{i}$，类似于 ([QSGD,](#bib.bib7) ; [FLStrategy,](#bib.bib91))。
- en: 6.2\. Deterministic Sparsification
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2 确定性稀疏化
- en: 'Different from Random Sparsification, the sparse properties are guaranteed
    by Deterministic Sparsification ([TonotopicANN,](#bib.bib189) ; [SparConnec,](#bib.bib191)
    ), in which most weights of DNNs can be close to zero. Due to the sparse weights,
    most of the gradients in each iteration are also around zero so that the zeroed
    gradients are no need for communication to reduce the communication traffic. There
    are mainly two ways to sparsify the gradients: Fixed Threshold and Adaptive Threshold.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 与随机稀疏化不同，通过确定性稀疏化 ([TonotopicANN,](#bib.bib189) ; [SparConnec,](#bib.bib191))
    来保证稀疏性质，其中大多数 DNN 的权重可以接近零。由于稀疏权重，每次迭代中的大多数梯度也接近零，因此不需要传输零梯度以减少通信流量。主要有两种方法来稀疏化梯度：固定阈值和自适应阈值。
- en: 6.2.1\. Fixed Threshold
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1 固定阈值
- en: 'Strom et al. ([scalableDisDNN,](#bib.bib190) ) introduced a new method to solve
    the communication bottleneck problem. In this algorithm, those gradient elements
    with absolute values less than a pre-defined threshold will be discarded. Because
    not all gradient elements are transmitted, the server must know which gradients
    to be transmitted and the indices of them. Strom et al. constructed key-value
    maps where keys are the indices and values are the corresponding gradient elements.
    The main drawback of the fixed threshold method is that it is non-trivial to choose
    a suitable threshold for different deep models or different iterations. Even worse,
    when Error Feedback (§[9](#S9 "9\. Auxiliary Technologies ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")) techniques
    are used, the fixed threshold method may result in the transmission of a large
    number of gradients.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Strom 等 ([scalableDisDNN,](#bib.bib190)) 引入了一种新方法来解决通信瓶颈问题。在这个算法中，绝对值小于预定义阈值的梯度元素将被丢弃。由于并非所有梯度元素都会被传输，服务器必须知道哪些梯度需要传输及其索引。Strom
    等构建了键值映射，其中键是索引，值是对应的梯度元素。固定阈值方法的主要缺点是为不同的深度模型或不同的迭代选择合适的阈值并非易事。更糟糕的是，当使用误差反馈
    (§[9](#S9 "9\. 辅助技术 ‣ 高效通信的数据并行分布式深度学习：综合调查")) 技术时，固定阈值方法可能导致大量梯度的传输。
- en: 6.2.2\. Adaptive Threshold
  id: totrans-236
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2 自适应阈值
- en: To address the problem of Fixed Threshold, Top-$k$ sparsification algorithms ([SparSGDwithMemory,](#bib.bib188)
    ; [ConvSparGrad,](#bib.bib6) ; [DGC,](#bib.bib113) ; [shi2020layer,](#bib.bib174)
    ; [shi2021towards,](#bib.bib182) ) select the top-$k$ gradients (in terms of absolute
    values) at each iteration.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决固定阈值的问题，Top-$k$ 稀疏化算法 ([SparSGDwithMemory,](#bib.bib188) ; [ConvSparGrad,](#bib.bib6)
    ; [DGC,](#bib.bib113) ; [shi2020layer,](#bib.bib174) ; [shi2021towards,](#bib.bib182))
    在每次迭代中选择绝对值最大的前-$k$ 个梯度。
- en: Dryden et al. ([CommQuantforDataPara,](#bib.bib48) ) proposed an adaptive threshold,
    which uses a fixed proportion $\pi$ to indicate the proportion of gradient elements
    to transmit. In every iteration, the algorithm determines a positive threshold
    and a negative one to satisfy the desired proportion. This method is able to make
    sure that the compressing ratio will not change during the training process. Despite
    that this technique requires the sorting of the whole gradient vector elements
    and costs extra computing time, it still reduces the wall-lock time a lot. Aji
    & Heafield ([SparCommforDisGD,](#bib.bib5) ) proposed another adaptive threshold
    method that uses a single threshold to drop all gradients whose absolute value
    is smaller than the threshold. However, parameters and their corresponding gradients
    may have varying scales across different parts of the model. As comparing all
    gradients with one global threshold may result in the loss of some small-scale
    gradients, Aji & Heafield exploited layer normalization  ([LayNorm,](#bib.bib13)
    ) to make a global threshold work.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Dryden 等人（[CommQuantforDataPara,](#bib.bib48)）提出了一种自适应阈值方法，该方法使用固定比例 $\pi$ 来指示传输的梯度元素比例。在每次迭代中，算法确定一个正阈值和一个负阈值，以满足所需比例。这种方法能够确保在训练过程中压缩比不会发生变化。尽管该技术需要对整个梯度向量元素进行排序，并且耗费额外的计算时间，但它仍然大大减少了壁锁时间。Aji
    和 Heafield（[SparCommforDisGD,](#bib.bib5)）提出了另一种自适应阈值方法，该方法使用单一阈值来丢弃所有绝对值小于阈值的梯度。然而，参数及其对应的梯度在模型的不同部分可能具有不同的尺度。由于将所有梯度与一个全局阈值进行比较可能会导致一些小尺度梯度的丢失，Aji
    和 Heafield 利用层归一化（[LayNorm,](#bib.bib13)）来使全局阈值有效。
- en: To account for local gradient activity, Chen et al. ([Adacomp,](#bib.bib29)
    ) proposed a novel gradient compression scheme, AdaComp, which can self-adapt
    the compression rate. They showed that most of the gradient compression techniques
    do not work well for convolutional layers, as different types of neural layers,
    mini-batch sizes, and other factors may affect the compression rate. AdaComp automatically
    determines the sparsification level, adapting to all variations.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 为了考虑局部梯度活动，Chen 等人（[Adacomp,](#bib.bib29)）提出了一种新颖的梯度压缩方案 AdaComp，该方案可以自适应压缩率。他们展示了大多数梯度压缩技术在卷积层上效果不佳，因为不同类型的神经层、小批量大小和其他因素可能影响压缩率。AdaComp
    自动确定稀疏化水平，以适应所有变化。
- en: Scattler et al.  ([sparsebinarycompression,](#bib.bib163) ) combined sparsification
    and quantization methods to propose Sparse Binary Compression (SBC), a new compression
    algorithm. SBC discards elements of low absolute value, averages positive and
    negative values to obtain positive and negative means $\mu^{+}$ and $\mu^{-}$
    respectively, and discards negative elements if $\mu^{+}$ is greater than $\mu^{-}$,
    and sets all positive elements to $\mu^{+}$ and vice versa. SBC then quantizes
    the non-zero elements, further reducing the communication cost by a factor of
    $\times 3$. Following SBC, Sattler et al.  ([SparseTernaryCompressionSTC,](#bib.bib162)
    ) further exploited the combination of top-$k$ sparsification and Ternary quantization
    to develop a new method named Sparse Ternary Compression (STC). Unlike SBC, STC
    is particularly suitable for the Federated Learning ([kairouz2019advances,](#bib.bib85)
    ). Experimental results in their paper demonstrate that sparsification methods
    achieve better convergence than quantization methods.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Scattler 等人（[sparsebinarycompression,](#bib.bib163)）将稀疏化和量化方法结合起来，提出了一种新压缩算法
    Sparse Binary Compression (SBC)。SBC 丢弃绝对值较小的元素，平均正负值以分别获得正均值 $\mu^{+}$ 和负均值 $\mu^{-}$，如果
    $\mu^{+}$ 大于 $\mu^{-}$ 则丢弃负元素，并将所有正元素设置为 $\mu^{+}$，反之亦然。SBC 然后量化非零元素，进一步将通信成本降低了
    $\times 3$。在 SBC 之后，Sattler 等人（[SparseTernaryCompressionSTC,](#bib.bib162)）进一步结合了
    top-$k$ 稀疏化和三值量化，开发了一种新方法名为 Sparse Ternary Compression (STC)。与 SBC 不同，STC 特别适用于联邦学习（[kairouz2019advances,](#bib.bib85)）。他们论文中的实验结果表明，稀疏化方法比量化方法具有更好的收敛性。
- en: In many distributed training algorithms, workers pull the latest update from
    the PS before training to make their models not deviate too much from the global
    model. However, in most top-$k$ sparsification methods, the size of the latest
    update depends on the number of workers, and many indices of chosen elements of
    a gradient vector differ among workers. As a result, when all gradients are collected
    together, the elements of the global gradient vector increase almost linearly
    with the number of workers, leading to ineffective sparsity of master-to-workers
    communication. To address this problem, Shi et al. ([GTopk,](#bib.bib179) ) proposed
    a novel sparsification method gTop-$k$. After aggregating all gradients, gTop-$k$
    sparsifies the global gradient vector once again, reducing the master-to-workers
    communication load and attaining convergence simultaneously ([ijcai2019473,](#bib.bib181)
    ). Furthermore, by exploiting the properties of gradients that are empirically
    proved to follow bell-shaped distributions ([shi2019understanding,](#bib.bib169)
    ; [shi2021towards,](#bib.bib182) ), the computing-efficient approximation of Top-$k$
    can be further exploited. Adaptive selection of the number of gradients or model
    parameters for communication can also help reduce overall training time ([han2020adaptive,](#bib.bib63)
    ; [9834260,](#bib.bib236) ; [pmlr-v151-wang22e,](#bib.bib223) ; [9721697,](#bib.bib258)
    ; shi2020layerwise).
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多分布式训练算法中，工作节点在训练之前从PS获取最新更新，以使其模型不会偏离全局模型。然而，在大多数top-$k$稀疏化方法中，最新更新的大小取决于工作节点的数量，并且梯度向量中选择的元素的索引在工作节点之间存在差异。因此，当所有梯度被收集在一起时，全球梯度向量的元素几乎线性增加，导致主节点与工作节点通信的稀疏性变得无效。为了解决这个问题，Shi等人（[GTopk](#bib.bib179)）提出了一种新颖的稀疏化方法gTop-$k$。在聚合所有梯度后，gTop-$k$再次稀疏化全局梯度向量，减少主节点与工作节点的通信负担，并同时实现收敛（[ijcai2019473](#bib.bib181)）。此外，通过利用梯度的性质，这些性质在经验上被证明遵循钟形分布（[shi2019understanding](#bib.bib169)
    ; [shi2021towards](#bib.bib182)），可以进一步利用Top-$k$的计算高效近似。自适应选择梯度或模型参数的数量进行通信也可以帮助减少整体训练时间（[han2020adaptive](#bib.bib63)
    ; [9834260](#bib.bib236) ; [pmlr-v151-wang22e](#bib.bib223) ; [9721697](#bib.bib258)
    ; shi2020layerwise）。
- en: The compression operation itself may need the computation costs. This is often
    overlooked by much of previous works. Some statistical approaches ([m2021efficient,](#bib.bib117)
    ; [shi2019understanding,](#bib.bib169) ; [shi2021towards,](#bib.bib182) ) are
    also typically required to estimate an accurate threshold for compressing gradients
    with linear computation complexity in the size of model parameters.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 压缩操作本身可能需要计算成本，这在许多之前的工作中往往被忽视。一些统计方法（[m2021efficient](#bib.bib117) ; [shi2019understanding](#bib.bib169)
    ; [shi2021towards](#bib.bib182)）也通常需要估算一个准确的阈值来压缩梯度，其线性计算复杂度与模型参数的大小相关。
- en: 6.3\. Coordinate Descent
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 坐标下降
- en: Coordinate descent is a kind of optimization method  ([Passcode,](#bib.bib74)
    ; [AsynCoordD,](#bib.bib196) ; [blockCoordD,](#bib.bib234) ; [ConverBlockCoordD,](#bib.bib257)
    ; [DisBlockCoordD,](#bib.bib119) ; [ProxBlockCoordD,](#bib.bib98) ; [Qu2015QRD2969239,](#bib.bib146)
    ) that splits all variables into multiple blocks, and then updates one block while
    fixing the remaining blocks. In an iteration, all blocks will be updated one by
    one  ([blockCoordD,](#bib.bib234) ). Despite the success of gradient-based methods,
    they may still suffer from the vanishing gradient problem for training deep neural
    networks  ([DL,](#bib.bib55) ). Gradient-free methods have been recently proposed
    to address the vanishing gradient problem, including BCD methods ([ProxBlockCoordD,](#bib.bib98)
    ; [ConverBlockCoordD,](#bib.bib257) ). In distributed DL scenarios, BCD can be
    easily implemented in a distributed and parallel manner ([DisBlockCoordD,](#bib.bib119)
    ). BCD has a property that only a subset of variables would be updated in each
    round, similar to the sparsification of distributed SGD.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 坐标下降是一种优化方法（[Passcode](#bib.bib74) ; [AsynCoordD](#bib.bib196) ; [blockCoordD](#bib.bib234)
    ; [ConverBlockCoordD](#bib.bib257) ; [DisBlockCoordD](#bib.bib119) ; [ProxBlockCoordD](#bib.bib98)
    ; [Qu2015QRD2969239](#bib.bib146)），它将所有变量分成多个块，然后在固定其余块的同时更新一个块。在一次迭代中，所有块会逐一更新（[blockCoordD](#bib.bib234)）。尽管基于梯度的方法取得了成功，但它们在训练深度神经网络时仍可能遭遇梯度消失问题（[DL](#bib.bib55)）。最近提出了无梯度方法以解决梯度消失问题，包括BCD方法（[ProxBlockCoordD](#bib.bib98)
    ; [ConverBlockCoordD](#bib.bib257)）。在分布式深度学习场景中，BCD可以以分布式和并行的方式轻松实现（[DisBlockCoordD](#bib.bib119)）。BCD的一个特点是每轮只更新变量的一个子集，这类似于分布式SGD的稀疏化。
- en: 'Lau et al. ([ProxBlockCoordD,](#bib.bib98) ) proposed an efficient BCD algorithm
    for DL and provided its convergence analysis. They highlighted three major advantages
    of BCD: (1) higher efficiency per epoch compared to SGD at early stages; (2) good
    scalability; and (3) gradient-free optimization. In ([GlobalconverBlockCoordD,](#bib.bib245)
    ), Zeng et al. presented a general methodology to establish provable convergence
    guarantees when using BCD in DL.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Lau 等人（[ProxBlockCoordD](#bib.bib98)）提出了一种高效的 BCD 算法用于深度学习，并提供了其收敛性分析。他们强调了
    BCD 的三个主要优势：（1）在早期阶段，相比于 SGD 每个周期的效率更高；（2）良好的可扩展性；（3）无梯度优化。在 ([GlobalconverBlockCoordD](#bib.bib245))
    中，Zeng 等人提出了一种通用方法来建立使用 BCD 进行深度学习时的可证明收敛保证。
- en: Mishchenko et al.  ([Mishchenko201999OP,](#bib.bib131) ) developed a new algorithm
    named Independent Block Coordinate Descent (IBCD), which allows each worker to
    sample an independent subset of blocks. They proved that the optimal number of
    blocks to be updated by each of $n$ units in every iteration is equal to $m$,
    where $m$ is the total number of blocks. Specifically, this means that when $n=100$
    parallel units are used, 99% of work is a waste of time.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Mishchenko 等人（[Mishchenko201999OP](#bib.bib131)）开发了一种名为独立块坐标下降（IBCD）的新算法，该算法允许每个工作节点对独立的块子集进行采样。他们证明了每次迭代中每个
    $n$ 个单元需要更新的最优块数等于 $m$，其中 $m$ 是块的总数。具体而言，这意味着当使用 $n=100$ 个并行单元时，99% 的工作都是浪费时间。
- en: 6.4\. Proximal Methods
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4 Proximal 方法
- en: Proximal methods involve two kinds of sparsity-inducing regularization in learning
    models and solving the optimization problem with proximal-based algorithms. These
    methods are utilized for sparsity learning to reduce the number of parameters
    in deep learning models. Additionally, the communication of distributed deep learning
    can benefit from the sparsity. $L0$-and $L1$-norms of parameters are commonly
    used for these methods  ([CommEffiDisSpar,](#bib.bib152) ; [AsynCoordD,](#bib.bib196)
    ).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Proximal 方法涉及在学习模型中引入两种稀疏性诱导的正则化，并使用基于邻近的算法解决优化问题。这些方法用于稀疏性学习，以减少深度学习模型中的参数数量。此外，分布式深度学习的通信也可以从稀疏性中受益。$L0$-和
    $L1$-范数的参数常用于这些方法（[CommEffiDisSpar](#bib.bib152)；[AsynCoordD](#bib.bib196)）。
- en: Grishchenko et al. ([AsynDisMLspar,](#bib.bib58) ) firstly combined the Proximal
    method, coordinate descent, and random sparsification together. The workers compute
    a local update using a randomly selected subset of coordinates, while the master
    aggregates and averages all updates from workers and computes the proximity operator
    of the regularizer at the averaged updates.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: Grishchenko 等人（[AsynDisMLspar](#bib.bib58)）首次将 Proximal 方法、坐标下降和随机稀疏化结合在一起。工作节点使用随机选择的坐标子集计算局部更新，而主节点聚合并平均所有工作节点的更新，并计算正则化器在平均更新中的邻近算子。
- en: To reduce communication overhead, Tsuzuku et al.  ([VarianceGradCompression,](#bib.bib208)
    ) developed a novel method that sends only gradient elements with small enough
    variance during training. They observed that some gradient elements are ambiguous,
    with low-amplitude but high-variance values, which may lead to futile updates
    to the global model. By controlling hyperparameters, their algorithm achieves
    high-rate sparsity.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减少通信开销，Tsuzuku 等人（[VarianceGradCompression](#bib.bib208)）开发了一种新颖的方法，在训练过程中只发送方差足够小的梯度元素。他们观察到一些梯度元素存在模糊情况，具有低幅度但高方差值，这可能导致对全局模型的无效更新。通过控制超参数，他们的算法实现了高比率的稀疏性。
- en: 6.5\. Experimental Comparison
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5 实验比较
- en: 'We conduct experiments of different compression schemes in the context of BSP,
    Local SGD, FedAVG and DPSGD algorithms. The compression schemes include QSGD quantization,
    Topk and error-feedback (EF) TopK sparsification. The overall experiment configuration
    is described in detail in Section [1.3](#S1.SS3 "1.3\. Benchmark Framework and
    Experiment Configuration ‣ 1\. Introduction ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey"). The results are presented
    in Table [8](#S6.T8 "Table 8 ‣ 6.5\. Experimental Comparison ‣ 6\. Sparsification
    Methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey").'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '我们在 BSP、Local SGD、FedAVG 和 DPSGD 算法的背景下进行不同压缩方案的实验。这些压缩方案包括 QSGD 量化、Topk 和误差反馈（EF）TopK
    稀疏化。整体实验配置在第 [1.3](#S1.SS3 "1.3\. Benchmark Framework and Experiment Configuration
    ‣ 1\. Introduction ‣ Communication-Efficient Data Parallel Distributed Deep Learning:
    A Comprehensive Survey") 节中详细描述。结果展示在表 [8](#S6.T8 "Table 8 ‣ 6.5\. Experimental
    Comparison ‣ 6\. Sparsification Methods ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey") 中。'
- en: As the compression ratio increased, almost algorithms suffered from varying
    degrees of performance drop. In the case of TopK compression, the Local SGD suffers
    less performance drop than BSP. Intuitively, we suppose the reason is that the
    Local SGD transmits the model weights rather than the gradients, which makes workers
    still can update all parameters. Interestingly, results show that the EF-TopK
    can successfully converge well even at extremely high compression ratio, i.e.
    1000\. This is because EF-TopK communicates parameters that are left by compression
    in the future iterations. Thus, all parameters of the model can still be updated
    finally.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 随着压缩比的增加，几乎所有算法都遭遇了不同程度的性能下降。在TopK压缩的情况下，Local SGD的性能下降比BSP小。直观上，我们认为原因是Local
    SGD传输的是模型权重而不是梯度，这使得工作节点仍然可以更新所有参数。有趣的是，结果显示EF-TopK即使在极高的压缩比下，如1000，也能成功收敛。这是因为EF-TopK在后续迭代中传输被压缩留下的参数。因此，模型的所有参数最终仍然可以更新。
- en: Table 8\. Experiments of comparing test accuracy of different communication
    compression algorithms.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 表8\. 比较不同通信压缩算法的测试准确率实验。
- en: '| Algo | Compress Ratio | Client Num | Resnet20 | RNN |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 压缩比 | 客户端数量 | Resnet20 | RNN |'
- en: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| $\gamma=$ 0.001 | $\gamma=$ 0.01 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$
    0.03 | $\gamma=$ 0.1 | $\gamma=$ 0.3 | $\gamma=$ 1.0 |'
- en: '| BSP | None | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28 |
    47.70 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| BSP | None | 4 | 84.16 | 89.70 | 91.25 | 90.36 | 52.03 | 55.24 | 56.28 |
    47.70 |'
- en: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 65.42 | 85.16 | 89.21 | 89.77 | 53.74 | 53.73 | 52.16 | 54.34 |'
- en: '| BSP quant | 2 | 4 | 84.28 | 89.33 | 90.52 | 0.31 | 55.19 | 55.06 | 52.87
    | 48.13 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| BSP quant | 2 | 4 | 84.28 | 89.33 | 90.52 | 0.31 | 55.19 | 55.06 | 52.87
    | 48.13 |'
- en: '| 32 | 64.65 | 85.12 | 89.34 | 89.95 | 53.75 | 51.73 | 52.26 | 54.67 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.65 | 85.12 | 89.34 | 89.95 | 53.75 | 51.73 | 52.26 | 54.67 |'
- en: '| 16 | 4 | 80.31 | 83.45 | 86.44 | 0.15 | 48.03 | 42.39 | 12.80 | 13.51 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 16 | 4 | 80.31 | 83.45 | 86.44 | 0.15 | 48.03 | 42.39 | 12.80 | 13.51 |'
- en: '| 32 | 64.34 | 83.56 | 85.37 | 84.41 | 51.84 | 52.62 | 0.62 | 8.68 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.34 | 83.56 | 85.37 | 84.41 | 51.84 | 52.62 | 0.62 | 8.68 |'
- en: '| BSP Topk | 10 | 4 | 76.68 | 86.58 | 88.30 | 88.29 | 53.53 | 52.72 | 50.73
    | 46.02 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| BSP Topk | 10 | 4 | 76.68 | 86.58 | 88.30 | 88.29 | 53.53 | 52.72 | 50.73
    | 46.02 |'
- en: '| 32 | 55.61 | 78.88 | 86.75 | 85.89 | 51.18 | 48.68 | 54.28 | 54.62 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 55.61 | 78.88 | 86.75 | 85.89 | 51.18 | 48.68 | 54.28 | 54.62 |'
- en: '| 100 | 4 | 59.47 | 77.20 | 76.94 | 76.16 | 34.00 | 48.00 | 46.08 | 40.83 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 4 | 59.47 | 77.20 | 76.94 | 76.16 | 34.00 | 48.00 | 46.08 | 40.83 |'
- en: '| 32 | 39.59 | 62.80 | 77.66 | 75.88 | 41.74 | 44.33 | 48.93 | 33.41 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 39.59 | 62.80 | 77.66 | 75.88 | 41.74 | 44.33 | 48.93 | 33.41 |'
- en: '| 1000 | 4 | 43.11 | 59.52 | 62.66 | 58.48 | 39.17 | 39.89 | 38.17 | 34.19
    |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 4 | 43.11 | 59.52 | 62.66 | 58.48 | 39.17 | 39.89 | 38.17 | 34.19
    |'
- en: '| 32 | 29.43 | 45.04 | 61.98 | 56.99 | 30.70 | 36.74 | 39.40 | 39.81 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 29.43 | 45.04 | 61.98 | 56.99 | 30.70 | 36.74 | 39.40 | 39.81 |'
- en: '| BSP eftopk | 10 | 4 | 83.74 | 89.16 | 90.47 | 90.48 | 55.25 | 55.05 | 53.07
    | 47.94 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| BSP eftopk | 10 | 4 | 83.74 | 89.16 | 90.47 | 90.48 | 55.25 | 55.05 | 53.07
    | 47.94 |'
- en: '| 32 | 64.74 | 84.80 | 88.65 | 88.94 | 54.00 | 51.58 | 52.18 | 46.60 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.74 | 84.80 | 88.65 | 88.94 | 54.00 | 51.58 | 52.18 | 46.60 |'
- en: '| 100 | 4 | 83.78 | 88.91 | 90.90 | 89.28 | 51.73 | 55.36 | 55.74 | 47.66 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 4 | 83.78 | 88.91 | 90.90 | 89.28 | 51.73 | 55.36 | 55.74 | 47.66 |'
- en: '| 32 | 64.52 | 84.33 | 88.08 | 88.69 | 53.98 | 51.62 | 52.14 | 46.88 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.52 | 84.33 | 88.08 | 88.69 | 53.98 | 51.62 | 52.14 | 46.88 |'
- en: '| 1000 | 4 | 84.02 | 88.94 | 90.88 | 89.89 | 55.29 | 55.20 | 53.36 | 53.55
    |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 1000 | 4 | 84.02 | 88.94 | 90.88 | 89.89 | 55.29 | 55.20 | 53.36 | 53.55
    |'
- en: '| 32 | 62.82 | 83.56 | 87.76 | 86.95 | 52.35 | 54.68 | 52.87 | 46.27 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 62.82 | 83.56 | 87.76 | 86.95 | 52.35 | 54.68 | 52.87 | 46.27 |'
- en: '| Local SGD $\tau=2$ | None | 4 | 84.04 | 88.97 | 92.01 | 88.86 | 55.03 | 54.94
    | 52.87 | 46.72 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| Local SGD $\tau=2$ | None | 4 | 84.04 | 88.97 | 92.01 | 88.86 | 55.03 | 54.94
    | 52.87 | 46.72 |'
- en: '| 32 | 65.41 | 84.96 | 89.35 | 90.11 | 54.11 | 51.72 | 52.30 | 46.17 |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 65.41 | 84.96 | 89.35 | 90.11 | 54.11 | 51.72 | 52.30 | 46.17 |'
- en: '| Local SGD $\tau=2$ topk | 100 | 4 | 61.04 | 81.45 | 87.26 | 87.40 | 49.23
    | 50.84 | 50.89 | 49.70 |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| Local SGD $\tau=2$ topk | 100 | 4 | 61.04 | 81.45 | 87.26 | 87.40 | 49.23
    | 50.84 | 50.89 | 49.70 |'
- en: '| 32 | 39.67 | 63.12 | 81.22 | 82.09 | 40.88 | 44.39 | 50.79 | 46.54 |'
  id: totrans-278
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 39.67 | 63.12 | 81.22 | 82.09 | 40.88 | 44.39 | 50.79 | 46.54 |'
- en: '| Local SGD $\tau=4$ | None | 4 | 83.85 | 89.61 | 90.90 | 90.00 | 51.92 | 55.05
    | 56.14 | 47.36 |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '| Local SGD $\tau=4$ | None | 4 | 83.85 | 89.61 | 90.90 | 90.00 | 51.92 | 55.05
    | 56.14 | 47.36 |'
- en: '| 32 | 65.55 | 85.09 | 89.44 | 89.85 | 54.36 | 51.72 | 53.40 | 47.15 |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 65.55 | 85.09 | 89.44 | 89.85 | 54.36 | 51.72 | 53.40 | 47.15 |'
- en: '| Local SGD $\tau=4$ topk | 100 | 4 | 60.73 | 81.18 | 85.20 | 83.83 | 50.54
    | 50.78 | 52.64 | 50.25 |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| Local SGD $\tau=4$ topk | 100 | 4 | 60.73 | 81.18 | 85.20 | 83.83 | 50.54
    | 50.78 | 52.64 | 50.25 |'
- en: '| 32 | 39.75 | 62.86 | 79.11 | 81.55 | 40.75 | 44.62 | 50.40 | 46.64 |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 39.75 | 62.86 | 79.11 | 81.55 | 40.75 | 44.62 | 50.40 | 46.64 |'
- en: '| Local SGD $\tau=8$ | None | 4 | 84.58 | 89.32 | 91.20 | 90.86 | 55.25 | 55.19
    | 53.35 | 48.55 |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 本地 SGD $\tau=8$ | None | 4 | 84.58 | 89.32 | 91.20 | 90.86 | 55.25 | 55.19
    | 53.35 | 48.55 |'
- en: '| 32 | 64.48 | 84.82 | 89.41 | 90.09 | 54.07 | 51.72 | 54.91 | 47.30 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.48 | 84.82 | 89.41 | 90.09 | 54.07 | 51.72 | 54.91 | 47.30 |'
- en: '| Local SGD $\tau=8$ topk | 100 | 4 | 61.82 | 82.19 | 85.46 | 84.77 | 49.02
    | 50.53 | 51.25 | 50.32 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 本地 SGD $\tau=8$ topk | 100 | 4 | 61.82 | 82.19 | 85.46 | 84.77 | 49.02 |
    50.53 | 51.25 | 50.32 |'
- en: '| 32 | 39.71 | 61.93 | 77.55 | 78.73 | 39.43 | 44.81 | 49.83 | 46.10 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 39.71 | 61.93 | 77.55 | 78.73 | 39.43 | 44.81 | 49.83 | 46.10 |'
- en: '| Local SGD $\tau=16$ | None | 4 | 84.02 | 89.25 | 90.99 | 90.56 | 55.32 |
    55.41 | 53.43 | 48.84 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 本地 SGD $\tau=16$ | None | 4 | 84.02 | 89.25 | 90.99 | 90.56 | 55.32 | 55.41
    | 53.43 | 48.84 |'
- en: '| 32 | 64.74 | 84.69 | 89.57 | 90.15 | 53.85 | 51.72 | 56.69 | 46.93 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.74 | 84.69 | 89.57 | 90.15 | 53.85 | 51.72 | 56.69 | 46.93 |'
- en: '| Local SGD $\tau=16$ topk | 100 | 4 | 61.13 | 81.60 | 84.68 | 83.27 | 49.27
    | 50.30 | 50.60 | 49.36 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 本地 SGD $\tau=16$ topk | 100 | 4 | 61.13 | 81.60 | 84.68 | 83.27 | 49.27 |
    50.30 | 50.60 | 49.36 |'
- en: '| 32 | 39.31 | 61.62 | 71.72 | 76.07 | 40.06 | 44.44 | 49.06 | 46.08 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 39.31 | 61.62 | 71.72 | 76.07 | 40.06 | 44.44 | 49.06 | 46.08 |'
- en: '| FedAvg | None | 4 | 62.41 | 84.37 | 89.81 | 90.10 | 52.20 | 55.23 | 54.91
    | 54.92 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| FedAvg | None | 4 | 62.41 | 84.37 | 89.81 | 90.10 | 52.20 | 55.23 | 54.91
    | 54.92 |'
- en: '| 32 | 40.23 | 64.42 | 83.45 | 86.65 | 28.77 | 43.70 | 51.91 | 52.02 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 40.23 | 64.42 | 83.45 | 86.65 | 28.77 | 43.70 | 51.91 | 52.02 |'
- en: '| FedAvg topk | 4 | 4 | 59.26 | 82.23 | 89.00 | 89.84 | 51.08 | 54.88 | 55.06
    | 54.85 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| FedAvg topk | 4 | 4 | 59.26 | 82.23 | 89.00 | 89.84 | 51.08 | 54.88 | 55.06
    | 54.85 |'
- en: '| 32 | 38.36 | 60.63 | 82.15 | 85.14 | 27.90 | 42.26 | 50.29 | 51.32 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 38.36 | 60.63 | 82.15 | 85.14 | 27.90 | 42.26 | 50.29 | 51.32 |'
- en: '| DPSGD | None | 4 | 84.60 | 89.38 | 91.08 | 91.01 | 55.41 | 55.19 | 52.92
    | 48.13 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| DPSGD | None | 4 | 84.60 | 89.38 | 91.08 | 91.01 | 55.41 | 55.19 | 52.92
    | 48.13 |'
- en: '| 32 | 64.20 | 85.12 | 88.99 | 89.71 | 53.55 | 51.71 | 53.77 | 46.50 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.20 | 85.12 | 88.99 | 89.71 | 53.55 | 51.71 | 53.77 | 46.50 |'
- en: '| DCD-PSGD | 4 | 4 | 68.39 | 86.32 | 89.79 | 90.33 | 54.70 | 55.13 | 55.45
    | 54.07 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| DCD-PSGD | 4 | 4 | 68.39 | 86.32 | 89.79 | 90.33 | 54.70 | 55.13 | 55.45
    | 54.07 |'
- en: '| 32 | 44.66 | 70.57 | 85.78 | 87.17 | 37.73 | 47.26 | 54.38 | 51.83 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 44.66 | 70.57 | 85.78 | 87.17 | 37.73 | 47.26 | 54.38 | 51.83 |'
- en: '| CHOCO-SGD | 100 | 4 | 84.25 | 88.82 | 91.27 | 88.77 | 55.21 | 55.19 | 52.73
    | 48.10 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| CHOCO-SGD | 100 | 4 | 84.25 | 88.82 | 91.27 | 88.77 | 55.21 | 55.19 | 52.73
    | 48.10 |'
- en: '| 32 | 64.52 | 84.71 | 89.00 | 89.00 | 53.16 | 51.75 | 54.49 | 47.15 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 32 | 64.52 | 84.71 | 89.00 | 89.00 | 53.16 | 51.75 | 54.49 | 47.15 |'
- en: 7\. Scheduling of Communication and Computing
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7\. 通信与计算调度
- en: DL models have a layer-wise structure that allows communication and computation
    tasks to be carried out in parallel during training ([zhang2017poseidon,](#bib.bib248)
    ). The parallelism of communication and computing can effectively hide the communication
    time and reduce the overall training time. The communication and computation tasks
    can be formulated into a general directed acyclic graph (DAG) ([shi2018adag,](#bib.bib176)
    ), pipelining or scheduling can be achieved to minimize iteration time.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型具有逐层结构，允许在训练过程中并行进行通信和计算任务 ([zhang2017poseidon,](#bib.bib248) )。通信与计算的并行性可以有效隐藏通信时间，减少整体训练时间。通信和计算任务可以被归纳为一个通用的有向无环图（DAG）
    ([shi2018adag,](#bib.bib176) ），可以通过流水线或调度来最小化迭代时间。
- en: In 2017, wait-free backward propagation (WFBP) ([zhang2017poseidon,](#bib.bib248)
    ; [awan2017s,](#bib.bib11) ) was proposed to pipeline the gradient communication
    of layer $l$ and the gradient computation of layer $l-1$, as they are independent.
    WFBP is naively supported in current DL frameworks (e.g., TensorFlow, PyTorch,
    Horovod, etc.). However, for small tensors of gradients, the latency term (startup
    time) can dominate the communication cost particularly on extremely large-scale
    clusters. To address the problem, merged-gradient (or tensor fusion) techniques
    (e.g., MG-WFBP ([Shi2018MGWFBPED,](#bib.bib170) ; [shi2021mgj,](#bib.bib171) ))
    are proposed to alleviate the negative impact of the startup time. For layer-wise
    gradient sparsification ([shi2020layer,](#bib.bib174) ; [shi2020communication,](#bib.bib177)
    ), three types of tasks (gradient computation, gradient sparsification, and gradient
    communication) can be pipelined. However, for the large tensors, the long communication
    time can cause their previous small tensors to wait. To minimize the waiting time
    of different tasks, communication and computation tasks can be scheduled by changing
    their execution order. Some studies ([jayarajan2018priority,](#bib.bib79) ; [hashemi2018tictac,](#bib.bib65)
    ; [harlap2018pipedream,](#bib.bib64) ; [peng2019generic,](#bib.bib141) ; [zhang2023accelerating,](#bib.bib250)
    ; [shi2023pipemoe,](#bib.bib172) ) have proposed scheduling communication tasks
    and computation tasks by changing the order of execution. Peng et al. ([peng2019generic,](#bib.bib141)
    ) proposed tensor partitioning to communication scheduling (even feed-forward
    computations can be paralleled with communications) to further reduce the communication
    cost. To prevent multiple communication tasks from affecting training performance,
    Wang et al. ([wang2020contention,](#bib.bib220) ) proposed communication contention
    aware scheduling of multiple deep learning training jobs on GPU clusters. The
    All-Reduce operation can be decoupled to two continuous operations such that they
    are possible to be overlapped with feed-forward and backward computations ([zhang2023accelerating,](#bib.bib250)
    ).
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，提出了无等待的反向传播（WFBP）（[zhang2017poseidon](#bib.bib248) ; [awan2017s](#bib.bib11)），用于将层$l$的梯度通信和层$l-1$的梯度计算进行流水化，因为它们是独立的。WFBP在当前的深度学习框架中（例如TensorFlow、PyTorch、Horovod等）得到了简单的支持。然而，对于小的梯度张量，延迟项（启动时间）可能会主导通信成本，尤其是在极大规模集群上。为了解决这个问题，提出了合并梯度（或张量融合）技术（例如MG-WFBP（[Shi2018MGWFBPED](#bib.bib170)
    ; [shi2021mgj](#bib.bib171)））来减轻启动时间的负面影响。对于逐层梯度稀疏化（[shi2020layer](#bib.bib174)
    ; [shi2020communication](#bib.bib177)），可以将梯度计算、梯度稀疏化和梯度通信这三种任务进行流水化。然而，对于大的张量，较长的通信时间可能会导致它们之前的小张量需要等待。为了最小化不同任务的等待时间，可以通过改变它们的执行顺序来调度通信和计算任务。一些研究（[jayarajan2018priority](#bib.bib79)
    ; [hashemi2018tictac](#bib.bib65) ; [harlap2018pipedream](#bib.bib64) ; [peng2019generic](#bib.bib141)
    ; [zhang2023accelerating](#bib.bib250) ; [shi2023pipemoe](#bib.bib172)）已经提出了通过改变执行顺序来调度通信任务和计算任务。Peng等人（[peng2019generic](#bib.bib141)）提出了张量分区到通信调度（即使前向计算也可以与通信并行）以进一步减少通信成本。为了防止多个通信任务影响训练性能，Wang等人（[wang2020contention](#bib.bib220)）提出了在GPU集群上对多个深度学习训练任务的通信争用感知调度。All-Reduce操作可以解耦成两个连续的操作，以便它们可以与前向和反向计算重叠（[zhang2023accelerating](#bib.bib250)）。
- en: DL frameworks commonly employ DAGs to schedule compute operations. However,
    this approach presents challenges in managing gradient communication between workers ([276984,](#bib.bib156)
    ). Specifically, if each worker simply uses blocking All-Reduce to communicate
    gradients based on their production order, the resulting mismatch in the orders
    of produced gradients between workers can lead to issues such as deadlock, data
    corruption, or communication inefficiency ([276984,](#bib.bib156) ). To mitigate
    such issues, it is necessary to schedule a global order for collective communication
    of tensors ([276984,](#bib.bib156) ). Notably, a recent benchmark study ([MLSYS2022_cedebb6e,](#bib.bib2)
    ) compares various compression methods with and without overlapping, and demonstrates
    that the latter approach can significantly reduce training time.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: DL 框架通常使用有向无环图（DAGs）来调度计算操作。然而，这种方法在管理工作节点之间的梯度通信方面存在挑战 ([276984,](#bib.bib156)
    )。具体来说，如果每个工作节点仅使用阻塞 All-Reduce 来根据其生产顺序通信梯度，则导致的梯度生产顺序不匹配可能会导致死锁、数据损坏或通信效率低下等问题 ([276984,](#bib.bib156)
    )。为了缓解这些问题，需要为张量的集体通信安排一个全局顺序 ([276984,](#bib.bib156) )。值得注意的是，最近的一项基准研究 ([MLSYS2022_cedebb6e,](#bib.bib2)
    ) 比较了有无重叠的各种压缩方法，并表明后者可以显著减少训练时间。
- en: 8\. Convergence Analysis
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8\. 收敛分析
- en: 'There are some commonly used assumptions for the convergence analysis:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 收敛分析中常用的一些假设：
- en: (1)
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Lipschitzian continuous gradient: All function $f_{i}(\cdot)$ have $L$-Lipschitzian
    gradients: $||\nabla f_{i}(\mathbf{x})-\nabla f_{i}(\mathbf{y})||\leq L\left\|\mathbf{x}-\mathbf{y}\right\|,\
    \forall\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}.$'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Lipschitz 连续梯度：所有函数 $f_{i}(\cdot)$ 具有 $L$-Lipschitz 连续梯度：$||\nabla f_{i}(\mathbf{x})-\nabla
    f_{i}(\mathbf{y})||\leq L\left\|\mathbf{x}-\mathbf{y}\right\|,\ \forall\mathbf{x},\mathbf{y}\in\mathbb{R}^{n}.$
- en: (2)
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Unbiased stochastic gradient: The gradient calculated at every iteration provides
    an unbiased estimation of the gradient of $f_{i}(\mathbf{x})$: $G_{i}(\mathbf{x}):=\mathbb{E}_{\xi\sim\mathcal{D}_{i}}\nabla
    F_{i}(\mathbf{x};\xi)=\nabla f_{i}(\mathbf{x}),\ \forall\mathbf{x}\in\mathbb{R}^{n},$
    in which the $\mathcal{D}_{i}$ is the data distribution on $i$-th worker.'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无偏随机梯度：每次迭代计算的梯度提供了 $f_{i}(\mathbf{x})$ 梯度的无偏估计：$G_{i}(\mathbf{x}):=\mathbb{E}_{\xi\sim\mathcal{D}_{i}}\nabla
    F_{i}(\mathbf{x};\xi)=\nabla f_{i}(\mathbf{x}),\ \forall\mathbf{x}\in\mathbb{R}^{n},$
    其中 $\mathcal{D}_{i}$ 是第 $i$ 个工作节点上的数据分布。
- en: (3)
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: 'Bounded variance: The variance of the stochastic gradient is bounded: $\mathbb{E}_{\xi\sim\mathcal{D}_{i}}\|\nabla
    F_{i}(\mathbf{x};\xi)-\nabla f_{i}(\mathbf{x})\|^{2}\leq\sigma^{2},\forall i,\forall\mathbf{x}\in\mathbb{R}^{n}.$'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有界方差：随机梯度的方差是有界的：$\mathbb{E}_{\xi\sim\mathcal{D}_{i}}\|\nabla F_{i}(\mathbf{x};\xi)-\nabla
    f_{i}(\mathbf{x})\|^{2}\leq\sigma^{2},\forall i,\forall\mathbf{x}\in\mathbb{R}^{n}.$
- en: (4)
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: 'Bounded stochastic gradient: The second moment of the stochastic gradients
    is bounded: $\mathbb{E}_{\xi\sim\mathcal{D}_{i}}\|\nabla F_{i}(\mathbf{x};\xi)\|^{2}\leq
    M^{2},\forall i,\forall\mathbf{x}\in\mathbb{R}^{n}.$'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 有界随机梯度：随机梯度的二阶矩是有界的：$\mathbb{E}_{\xi\sim\mathcal{D}_{i}}\|\nabla F_{i}(\mathbf{x};\xi)\|^{2}\leq
    M^{2},\forall i,\forall\mathbf{x}\in\mathbb{R}^{n}.$
- en: (5)
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (5)
- en: '$\mu$-Strongly convex function: $f(\mathbf{x})\geq f(\mathbf{y})+\left\langle\nabla
    f(\mathbf{y}),\mathbf{x}-\mathbf{y}\right\rangle+\frac{\mu}{2}\left\|\mathbf{x}-\mathbf{y}\right\|^{2}.$'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: $\mu$-强凸函数：$f(\mathbf{x})\geq f(\mathbf{y})+\left\langle\nabla f(\mathbf{y}),\mathbf{x}-\mathbf{y}\right\rangle+\frac{\mu}{2}\left\|\mathbf{x}-\mathbf{y}\right\|^{2}.$
- en: 'For the gossip (peer-to-peer) algorithms, there are some extra assumptions
     ([CommCompforDecent,](#bib.bib201) ):'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 gossip（点对点）算法，还存在一些额外的假设 ([CommCompforDecent,](#bib.bib201) )：
- en: (1)
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: 'Symmetric doubly stochastic matrix: The communication topology among workers
    is represented by a weighted matrix $W$ that is a real symmetric doubly stochastic
    matrix satisfying $W=W^{T}$ and $W\mathbf{1}=W$.'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对称双随机矩阵：工作节点之间的通信拓扑由一个加权矩阵 $W$ 表示，该矩阵是一个真实对称的双随机矩阵，满足 $W=W^{T}$ 和 $W\mathbf{1}=W$。
- en: (2)
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Spectral gap: Given the symmetric doubly stochastic matrix $W$, the spectral
    gap is defined as $\rho:=\text{max}\left\{\|\lambda_{2}(W)\|,\|\lambda_{n}(W)\|\right\}$
    where $\lambda_{2}(W)$ represent the second largest eigenvalues of $W$. The condition
    $\rho<1$ must hold.'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 谱间隙：给定对称的双随机矩阵 $W$，谱间隙定义为 $\rho:=\text{max}\left\{\|\lambda_{2}(W)\|,\|\lambda_{n}(W)\|\right\}$，其中
    $\lambda_{2}(W)$ 代表 $W$ 的第二大特征值。必须满足条件 $\rho<1$。
- en: 'For the compression methods, there are also some assumptions:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于压缩方法，也有一些假设：
- en: (1)
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: '$k$-contraction  ([SparSGDwithMemory,](#bib.bib188) ): For a parameter $0<d<n$,
    a $k$-contraction operator is a (possibly randomized) operator $C(\cdot)$: $\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$
    that satisfies the contraction property: $\mathbb{E}\left\|\mathbf{x}-C(\mathbf{x})\right\|^{2}\leq\left(1-\frac{d}{n}\right)\left\|\mathbf{x}\right\|^{2},\forall\mathbf{x}\in\mathbb{R}^{n}.$'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '$k$-收缩 ([SparSGDwithMemory,](#bib.bib188) ): 对于参数 $0<d<n$，$k$-收缩算子是一个（可能是随机的）算子
    $C(\cdot)$: $\mathbb{R}^{n}\rightarrow\mathbb{R}^{n}$，它满足收缩属性：$\mathbb{E}\left\|\mathbf{x}-C(\mathbf{x})\right\|^{2}\leq\left(1-\frac{d}{n}\right)\left\|\mathbf{x}\right\|^{2},\forall\mathbf{x}\in\mathbb{R}^{n}$。'
- en: (2)
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: 'Unbiased compression  ([CommCompforDecent,](#bib.bib201) ): The stochastic
    compression operator $C(\cdot)$ is unbiased for any $\mathbf{x}$: $\mathbb{E}[C(\mathbf{x})]=\mathbf{x},$
    and the compression operators are independent on different workers or at different
    iterations.'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '无偏压缩 ([CommCompforDecent,](#bib.bib201) ): 随机压缩算子 $C(\cdot)$ 对任何 $\mathbf{x}$
    是无偏的：$\mathbb{E}[C(\mathbf{x})]=\mathbf{x}$，并且压缩算子在不同的工作节点或不同的迭代中是独立的。'
- en: 8.1\. Centralized Architecture (PS or All-Reduce)
  id: totrans-327
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1. 中央化架构 (PS 或 All-Reduce)
- en: 8.1.1\. BSP-SGD
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.1. BSP-SGD
- en: PS and All-Reduce architectures have the same iteration equations because the
    All-Reduce algorithm only changes the way of implementing global synchronization.
    Therefore, the convergence analysis of BSP-SGD with the PS architecture can be
    applied to the All-Reduce architecture.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: PS 和 All-Reduce 架构具有相同的迭代方程，因为 All-Reduce 算法只是改变了全局同步的实现方式。因此，BSP-SGD 在 PS 架构下的收敛性分析可以应用于
    All-Reduce 架构。
- en: For quantization methods, Christopher et al. ([Tamingwild,](#bib.bib158) ) provided
    a convergence analysis with the martingale-based approach under both convex and
    non-convex objectives. In the case of QSGD, Alistarh et al.  ([QSGD,](#bib.bib7)
    ) not only proposed a family of quantization algorithms, but also provided a convergence
    analysis. They proved that QSGD can achieve convergence both for convex and non-convex
    objectives. They also proved that QSGD satisfies $\frac{1}{L}\mathbb{E}\left[\left\|\nabla
    f(\mathbf{x})\right\|^{2}_{2}\right]\leq O\left(\frac{\sqrt{L(f(\mathbf{x}_{1})-f^{*})}}{T}+\frac{min(n/s,\sqrt{n}/s)B}{L}\right)$,
    where $L$ represents Lipschitzian constant, $s$ and $B$ are hyper-parameters.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 对于量化方法，Christopher 等人 ([Tamingwild,](#bib.bib158) ) 提出了在凸和非凸目标下，基于鞅的量化方法的收敛性分析。在
    QSGD 的情况下，Alistarh 等人 ([QSGD,](#bib.bib7) ) 不仅提出了一系列量化算法，还提供了收敛性分析。他们证明了 QSGD
    可以在凸和非凸目标下实现收敛。他们还证明了 QSGD 满足 $\frac{1}{L}\mathbb{E}\left[\left\|\nabla f(\mathbf{x})\right\|^{2}_{2}\right]\leq
    O\left(\frac{\sqrt{L(f(\mathbf{x}_{1})-f^{*})}}{T}+\frac{min(n/s,\sqrt{n}/s)B}{L}\right)$，其中
    $L$ 代表 Lipschitz 常数，$s$ 和 $B$ 是超参数。
- en: When implementing error accumulation in quantization methods, the variance bound
    of quantized gradients exceed that of QSGD ([ECQSGD,](#bib.bib229) ). Wu et al. ([ECQSGD,](#bib.bib229)
    ) provided a convergence analysis in this scenario, but their analysis is limited
    by the requirement of unbiased gradient compression.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在量化方法中实现误差积累时，量化梯度的方差界限超过了 QSGD ([ECQSGD,](#bib.bib229) ) 的方差界限。Wu 等人 ([ECQSGD,](#bib.bib229)
    ) 在这种情况下提供了收敛性分析，但他们的分析受限于无偏梯度压缩的要求。
- en: In the case of sparsification methods, Alistarh et al.  ([ConvSparGrad,](#bib.bib6)
    ) theoretically proved that the Top-$k$ algorithm can achieve good convergence
    even with biased estimation and non-convex objectives. This analysis is subject
    to deceleration proportional to $k$. Extending the convergence analysis of Top-$k$
    methods to a more general range of sparsification methods, such as random-$k$
    or $k$-sparsification methods, Stich et al. ([SparSGDwithMemory,](#bib.bib188)
    ) proved that this scheme preserves the same order of convergence rate as vanilla
    SGD, i.e. $O\left(\frac{G^{2}}{\mu T}\right)$. Shi et al. ([ijcai2019473,](#bib.bib181)
    ) analyzed the gTop-k sparsification method ([GTopk,](#bib.bib179) ) theoretically.
    They proved that gTop-k S-SGD provides convergence guarantees for non-convex problems
    and has the same theoretical convergence rate as the mini-batch SGD.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在稀疏化方法的情况下，Alistarh 等人 ([ConvSparGrad,](#bib.bib6) ) 理论上证明了 Top-$k$ 算法即使在有偏估计和非凸目标下也能实现良好的收敛性。该分析的收敛速度与
    $k$ 成正比。Stich 等人 ([SparSGDwithMemory,](#bib.bib188) ) 将 Top-$k$ 方法的收敛性分析扩展到更广泛的稀疏化方法范围，例如随机-$k$
    或 $k$-稀疏化方法，他们证明了该方案保持了与原始 SGD 相同的收敛速度，即 $O\left(\frac{G^{2}}{\mu T}\right)$。Shi
    等人 ([ijcai2019473,](#bib.bib181) ) 理论上分析了 gTop-k 稀疏化方法 ([GTopk,](#bib.bib179)
    )。他们证明了 gTop-k S-SGD 为非凸问题提供了收敛保证，并具有与小批量 SGD 相同的理论收敛速率。
- en: 8.1.2\. SSP/Asynchronous
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.2. SSP/异步
- en: Ho et al. ([MoreEffDMLviaStaleSync,](#bib.bib69) ) have established $O(1/\sqrt{T})$
    time for SGD under SSP. Lian et al. ([10.5555/2969442.2969545,](#bib.bib107) )
    proposed an ergodic convergence rate $O(1/\sqrt{T})$ and proved that the linear
    speedup can be achieved if the number of workers is bounded by $\sqrt{T}$. Alistarh
    et al.  ([Alistarh2018TheCO,](#bib.bib8) ) provided the convergence bounds for
    lock-free SGD. Moreover, they exhibit a fundamental trade-off between the delay
    of the system and the convergence rate. Zhang et al. ([zhang2018taming,](#bib.bib255)
    ) also provided a convergence rate of asynchronous SGD under a non-convex object
    function and established a unifying condition for asynchronous SGD.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: Ho 等人 ([MoreEffDMLviaStaleSync,](#bib.bib69)) 已经建立了在SSP下SGD的$O(1/\sqrt{T})$时间。Lian
    等人 ([10.5555/2969442.2969545,](#bib.bib107)) 提出了一个遍历收敛率$O(1/\sqrt{T})$，并证明了如果工作者数量被限制在$\sqrt{T}$以内，则可以实现线性加速。Alistarh
    等人 ([Alistarh2018TheCO,](#bib.bib8)) 提供了无锁SGD的收敛界限。此外，他们展示了系统延迟与收敛率之间的基本权衡。Zhang
    等人 ([zhang2018taming,](#bib.bib255)) 还提供了在非凸目标函数下异步SGD的收敛率，并为异步SGD建立了一个统一的条件。
- en: 8.1.3\. Local SGD
  id: totrans-335
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.3\. 本地SGD
- en: Stich ([Sebastian2019,](#bib.bib187) ) and Yu et al.  ([Yu2018ParallelRS,](#bib.bib243)
    ) have provided a concise convergence analysis for Local-SGD, demonstrating that
    this method has convergence guarantees with reducing communication costs. In ([NIPS2019_9288,](#bib.bib62)
    ), Haddadpour et al. strengthened the convergence analysis for Local-SGD and showed
    that it can be far less expensive and more generally applicable than the current
    theory suggests. They proved that for loss functions that satisfy the Polyak-Łojasiewicz
    condition, $O((pT)1/3)$ rounds of communication suffice to achieve linear speedup
    with an error of $O(1/pT)$.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: Stich ([Sebastian2019,](#bib.bib187)) 和 Yu 等人 ([Yu2018ParallelRS,](#bib.bib243))
    提供了本地SGD的简明收敛性分析，证明了这种方法在减少通信成本方面具有收敛性保证。在 ([NIPS2019_9288,](#bib.bib62)) 中，Haddadpour
    等人加强了本地SGD的收敛性分析，并展示了它可以比当前理论建议的成本低得多且更具普遍适用性。他们证明了对于满足Polyak-Łojasiewicz条件的损失函数，$O((pT)1/3)$轮通信足以实现线性加速，误差为$O(1/pT)$。
- en: Patel and Dieuleveut ([NIPS2019_9512,](#bib.bib47) ) proposed a non-asymptotic
    error analysis that enables the comparison to one-shot averaging and mini-batch
    averaging, providing adaptive lower bounds on the communication frequency. They
    showed that Local-SGD can reduce communication by a factor of $O(\frac{\sqrt{T}}{N^{3/2}})$.
    Artin et. al.  ([spiridonoff2021communicationefficient,](#bib.bib186) ) provided
    an interesting convergence analysis of Local-SGD under a strong convexity assumption.
    They proved that Local-SGD only needs $O(N)$ communication rounds.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: Patel 和 Dieuleveut ([NIPS2019_9512,](#bib.bib47)) 提出了一个非渐近的误差分析，能够与单次平均和小批量平均进行比较，并提供了通信频率的自适应下界。他们展示了本地SGD可以将通信减少一个$O(\frac{\sqrt{T}}{N^{3/2}})$的因子。Artin
    等人 ([spiridonoff2021communicationefficient,](#bib.bib186)) 提供了在强凸性假设下本地SGD的有趣收敛性分析。他们证明了本地SGD只需要$O(N)$轮通信。
- en: 8.2\. Decentralized Architecture (Gossip Communication)
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 去中心化架构（ gossip通信）
- en: In contrast to the centralized architecture, the gossip architecture involves
    each worker possessing an individual model. Consequently, the convergence analysis
    of the gossip architecture differs from that of the centralized architecture.
    In order to ensure consensus in both the convergence analysis and algorithm design
    of the gossip architecture, certain considerations must be made.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 与集中式架构相比，gossip架构涉及每个工作者拥有一个独立的模型。因此，gossip架构的收敛性分析与集中式架构有所不同。为了确保在gossip架构的收敛性分析和算法设计中达成共识，必须做出一些考虑。
- en: 8.2.1\. BSP
  id: totrans-340
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.1\. BSP
- en: Lian et al. ([CanDecent,](#bib.bib108) ) were the first to provide a theoretical
    analysis demonstrating that decentralized SGD algorithms for both convex and non-convex
    objectives can be faster than centralized SGD with less communication on the busiest
    node. They proved the convergence rate of decentralized SGD is $O\left(\frac{1}{T}+\frac{1}{\sqrt{nT}}\right)$,
    where $T$ and $n$ represents the number of iterations and workers respectively.
    When $T$ is sufficiently large, the $\frac{1}{\sqrt{nK}}$ term becomes dominant.
    In this scenario, the convergence rate can be approximated as $\frac{1}{\sqrt{nK}}$,
    indicating linear speedup achieved with the number of workers.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: Lian 等人 ([CanDecent,](#bib.bib108)) 首次提供了理论分析，证明了对于凸和非凸目标的去中心化 SGD 算法比集中式 SGD
    在最繁忙的节点上通信更少且速度更快。他们证明去中心化 SGD 的收敛速率为 $O\left(\frac{1}{T}+\frac{1}{\sqrt{nT}}\right)$，其中
    $T$ 和 $n$ 分别表示迭代次数和工作节点数。当 $T$ 足够大时，$\frac{1}{\sqrt{nK}}$ 项变得主导。在这种情况下，收敛速率可以近似为
    $\frac{1}{\sqrt{nK}}$，表明随着工作节点数的增加可以实现线性加速。
- en: Considering communication compression, Tang et al.  ([CommCompforDecent,](#bib.bib201)
    ) proposed two algorithms, ECD-PSGD and DCD-PSGD, with detailed convergence analysis
    on both convex and non-convex problems. The convergence rate of DCD-PSGD is $O\left(\frac{\sigma}{\sqrt{nT}}+\frac{\zeta^{\frac{2}{3}}}{T^{\frac{2}{3}}}+\frac{1}{T}\right)$,
    in which $\sigma$ is the variance of stochastic gradient estimator, $\zeta$ is
    the variance of gradient divergence between single worker and all workers. For
    ECD-PSGD, they proved its convergence rate is $O\left(\frac{\sigma\left(1+\frac{{\tilde{\sigma}}^{2}logT}{n}\right)}{\sqrt{nT}}+\frac{\zeta^{\frac{2}{3}}\left(1+\frac{{\tilde{\sigma}}^{2}logT}{n}\right)}{T^{\frac{2}{3}}}+\frac{1}{T}+\frac{{\tilde{\sigma}}^{2}logT}{T}\right)$,
    which is slightly worse than DCD-PSGD, due to the extra terms $O\left(\frac{\sigma{\tilde{\sigma}}^{2}logT}{n\sqrt{nT}}+\frac{\zeta^{\frac{2}{3}}{\tilde{\sigma}}^{2}logT}{nT^{\frac{2}{3}}}+\frac{{\tilde{\sigma}}^{2}logT}{T}\right)$.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 针对通信压缩，Tang 等人 ([CommCompforDecent,](#bib.bib201)) 提出了两个算法，ECD-PSGD 和 DCD-PSGD，并对凸和非凸问题进行了详细的收敛分析。DCD-PSGD
    的收敛速率为 $O\left(\frac{\sigma}{\sqrt{nT}}+\frac{\zeta^{\frac{2}{3}}}{T^{\frac{2}{3}}}+\frac{1}{T}\right)$，其中
    $\sigma$ 是随机梯度估计的方差，$\zeta$ 是单个工作节点与所有工作节点之间梯度分歧的方差。对于 ECD-PSGD，他们证明其收敛速率为 $O\left(\frac{\sigma\left(1+\frac{{\tilde{\sigma}}^{2}logT}{n}\right)}{\sqrt{nT}}+\frac{\zeta^{\frac{2}{3}}\left(1+\frac{{\tilde{\sigma}}^{2}logT}{n}\right)}{T^{\frac{2}{3}}}+\frac{1}{T}+\frac{{\tilde{\sigma}}^{2}logT}{T}\right)$，稍逊于
    DCD-PSGD，因为多出了 $O\left(\frac{\sigma{\tilde{\sigma}}^{2}logT}{n\sqrt{nT}}+\frac{\zeta^{\frac{2}{3}}{\tilde{\sigma}}^{2}logT}{nT^{\frac{2}{3}}}+\frac{{\tilde{\sigma}}^{2}logT}{T}\right)$
    额外项。
- en: It should be noted that the convergence analysis of ECD-PSGD and DCD-PSGD is
    restricted to unbiased compression operators. Koloskova et al.  ([DecentStocOptimAndGossip,](#bib.bib90)
    ) proposed CHOCO-SGD, the first method that is capable of handling biased compression
    operators. For convex objectives, their algorithm was found to converge with a
    rate of $O\left(1/(nT)+1/(T\delta^{2}\omega)\right)$, where $\delta$ denotes the
    eigenvalue gap of the gossip matrix and $\omega\leq 1$ represents the compression
    ratio. Furthermore, Koloskova et al. ([Koloskova2019DecentralizedDL,](#bib.bib89)
    ) demonstrated that CHOCO-SGD ([DecentStocOptimAndGossip,](#bib.bib90) ) can achieve
    convergence at a rate of $O(1/\sqrt{nT}+n/\left(\rho^{4}\sigma^{2}T\right))$ for
    non-convex smooth functions, where $n$ denotes the number of nodes, $T$ the number
    of iterations, $\rho$ the spectral gap of the mixing matrix and $\sigma$ the compression
    ratio. Moreover, they proved that CHOCO-SGD can converge with arbitrarily high
    compression operators and achieve linear speedup.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，ECD-PSGD 和 DCD-PSGD 的收敛分析仅限于无偏压缩算子。Koloskova 等人 ([DecentStocOptimAndGossip,](#bib.bib90))
    提出了 CHOCO-SGD，这是第一个能够处理有偏压缩算子的方法。对于凸目标，他们的算法发现其收敛速率为 $O\left(1/(nT)+1/(T\delta^{2}\omega)\right)$，其中
    $\delta$ 表示 gossip 矩阵的特征值间隙，$\omega\leq 1$ 代表压缩比。此外，Koloskova 等人 ([Koloskova2019DecentralizedDL,](#bib.bib89))
    证明 CHOCO-SGD ([DecentStocOptimAndGossip,](#bib.bib90)) 可以以 $O(1/\sqrt{nT}+n/\left(\rho^{4}\sigma^{2}T\right))$
    的速率在非凸平滑函数上收敛，其中 $n$ 表示节点数，$T$ 为迭代次数，$\rho$ 为混合矩阵的谱间隙，$\sigma$ 为压缩比。此外，他们证明了 CHOCO-SGD
    可以在任意高压缩算子下收敛并实现线性加速。
- en: 8.2.2\. Asynchronous
  id: totrans-344
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.2.2\. 异步
- en: Jeff et al. ([DBLP:journals/corr/abs-1803-05880,](#bib.bib41) ) provided a sketch
    of proof of convergence without the convergence rate. Lian et al.  ([pmlr-v80-lian18a,](#bib.bib109)
    ) proposed a theoretical analysis of asynchronous gossip SGD with the non-convex
    objective function, which converges with the $O(1/\sqrt{T})$ rate and has linear
    speedup with respect to the number of workers. Assran et al. ([StocGradPush,](#bib.bib10)
    ) provided theoretical guarantees for another asynchronous gossip SGD algorithm
    that achieves similar convergence of ([pmlr-v80-lian18a,](#bib.bib109) ).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: Jeff 等人 ([DBLP:journals/corr/abs-1803-05880,](#bib.bib41)) 提供了一个没有收敛速度的收敛性证明的草图。Lian
    等人 ([pmlr-v80-lian18a,](#bib.bib109)) 提出了一个异步 gossip SGD 的理论分析，其具有 $O(1/\sqrt{T})$
    的收敛速度，并且相对于工作者数量具有线性加速。Assran 等人 ([StocGradPush,](#bib.bib10)) 为另一个异步 gossip SGD
    算法提供了理论保证，该算法实现了类似的收敛性 ([pmlr-v80-lian18a,](#bib.bib109))。
- en: 'Table [9](#S8.T9 "Table 9 ‣ 8.2.2\. Asynchronous ‣ 8.2\. Decentralized Architecture
    (Gossip Communication) ‣ 8\. Convergence Analysis ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey") provides a comparison
    of different algorithms. Most algorithms show $O(\frac{1}{T})$ convergence rate
    for convex object functions and $O(\frac{1}{T})$ for non-convex object functions.
    However, the communication costs vary depending on the architecture and algorithm
    used.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: '表 [9](#S8.T9 "Table 9 ‣ 8.2.2\. Asynchronous ‣ 8.2\. Decentralized Architecture
    (Gossip Communication) ‣ 8\. Convergence Analysis ‣ Communication-Efficient Data
    Parallel Distributed Deep Learning: A Comprehensive Survey") 提供了不同算法的比较。大多数算法对于凸目标函数和非凸目标函数显示
    $O(\frac{1}{T})$ 的收敛速度。然而，通信成本根据使用的架构和算法有所不同。'
- en: Table 9\. Summary of distributed learning algorithms
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9\. 分布式学习算法总结
- en: '| Arch. | Comm. | Compression | Communication Cost in Big $O$ | Convergence
    in Big $O$ | References of Method |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 架构 | 通信 | 压缩 | 大 $O$ 中的通信成本 | 大 $O$ 中的收敛性 | 方法的参考文献 |'
- en: '| Server | Workers | convex | non-convex |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 服务器 | 工作者 | 凸函数 | 非凸函数 |'
- en: '| PS | BSP | Null | $O(32nNT)$ | $O(32NT)$ | $O(\frac{1}{T})$ ([Bottou2016OptimizationMF,](#bib.bib20)
    ) | $O(\frac{1}{\sqrt{T}})$ ([Bottou2016OptimizationMF,](#bib.bib20) ) |  ([Krizhevsky2014OneWT,](#bib.bib94)
    ; [Bradley2011,](#bib.bib23) ; [552669,](#bib.bib93) ; [375451,](#bib.bib28) )
    |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| PS | BSP | 空 | $O(32nNT)$ | $O(32NT)$ | $O(\frac{1}{T})$ ([Bottou2016OptimizationMF,](#bib.bib20))
    | $O(\frac{1}{\sqrt{T}})$ ([Bottou2016OptimizationMF,](#bib.bib20)) | ([Krizhevsky2014OneWT,](#bib.bib94);
    [Bradley2011,](#bib.bib23); [552669,](#bib.bib93); [375451,](#bib.bib28)) |'
- en: '| Quant. | $O(32nNT)$ | $O(bNT)$ | $O(\frac{1}{T})$ ([DisLearningDIANA,](#bib.bib130)
    )  ([AlinearSpeedupAnalysis,](#bib.bib82) ) | $O(\frac{1}{\sqrt{T}})$  ([AlinearSpeedupAnalysis,](#bib.bib82)
    ; [QSGD,](#bib.bib7) ) |  ([3LC,](#bib.bib110) )  ([FLStrategy,](#bib.bib91) ;
    [DisLearningDIANA,](#bib.bib130) ; [signSGDwithVote,](#bib.bib18) ; [TernGrad,](#bib.bib227)
    ; [Horvath2019NaturalCF,](#bib.bib73) )  ([signSGD,](#bib.bib17) )  ([ATOMO,](#bib.bib216)
    ; [NIPS2019_8598,](#bib.bib195) ; [Mishchenko201999OP,](#bib.bib131) ; [QSGD,](#bib.bib7)
    ; [scalableDisDNN,](#bib.bib190) ; [CommQuantforDataPara,](#bib.bib48) ; [EFsignSGD,](#bib.bib86)
    ) |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | $O(32nNT)$ | $O(bNT)$ | $O(\frac{1}{T})$ ([DisLearningDIANA,](#bib.bib130))
    ([AlinearSpeedupAnalysis,](#bib.bib82)) | $O(\frac{1}{\sqrt{T}})$ ([AlinearSpeedupAnalysis,](#bib.bib82);
    [QSGD,](#bib.bib7)) | ([3LC,](#bib.bib110)) ([FLStrategy,](#bib.bib91); [DisLearningDIANA,](#bib.bib130);
    [signSGDwithVote,](#bib.bib18); [TernGrad,](#bib.bib227); [Horvath2019NaturalCF,](#bib.bib73))
    ([signSGD,](#bib.bib17)) ([ATOMO,](#bib.bib216); [NIPS2019_8598,](#bib.bib195);
    [Mishchenko201999OP,](#bib.bib131); [QSGD,](#bib.bib7); [scalableDisDNN,](#bib.bib190);
    [CommQuantforDataPara,](#bib.bib48); [EFsignSGD,](#bib.bib86)) |'
- en: '| Spars. | $O(32nNT)$ | $O(k(\log N+32)T)$ | $O(\frac{1}{T})$ ([NIPS2019_9473,](#bib.bib78)
    ; [SparSGDwithMemory,](#bib.bib188) ) | $O(\frac{1}{\sqrt{T}})$  ([ConvSparGrad,](#bib.bib6)
    ; [ijcai2019473,](#bib.bib181) ) |  ([3LC,](#bib.bib110) ; [SparCommforDisGD,](#bib.bib5)
    )  ([FLStrategy,](#bib.bib91) ; [NIPS2019_9473,](#bib.bib78) ; [Horvath2019NaturalCF,](#bib.bib73)
    ; [ATOMO,](#bib.bib216) )  ([GradSparforDisOptim,](#bib.bib225) ; [DGC,](#bib.bib113)
    ; [CommQuantforDataPara,](#bib.bib48) ; [Zhao2019GlobalMC,](#bib.bib260) ; [GTopk,](#bib.bib179)
    ; [VarianceGradCompression,](#bib.bib208) ) |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | $O(32nNT)$ | $O(k(\log N+32)T)$ | $O(\frac{1}{T})$ ([NIPS2019_9473,](#bib.bib78);
    [SparSGDwithMemory,](#bib.bib188)) | $O(\frac{1}{\sqrt{T}})$ ([ConvSparGrad,](#bib.bib6);
    [ijcai2019473,](#bib.bib181)) | ([3LC,](#bib.bib110); [SparCommforDisGD,](#bib.bib5))
    ([FLStrategy,](#bib.bib91); [NIPS2019_9473,](#bib.bib78); [Horvath2019NaturalCF,](#bib.bib73);
    [ATOMO,](#bib.bib216)) ([GradSparforDisOptim,](#bib.bib225); [DGC,](#bib.bib113);
    [CommQuantforDataPara,](#bib.bib48); [Zhao2019GlobalMC,](#bib.bib260); [GTopk,](#bib.bib179);
    [VarianceGradCompression,](#bib.bib208)) |'
- en: '| SSP | Null | $O(32N\sum_{i}^{n}T_{i})$ | $O(32NT_{i})$ | - | $O(\frac{1}{\sqrt{T}})$
     ([MoreEffDMLviaStaleSync,](#bib.bib69) ) |  ([RevistSynSGD,](#bib.bib31) ; [MoreEffDMLviaStaleSync,](#bib.bib69)
    ) |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '| SSP | Null | $O(32N\sum_{i}^{n}T_{i})$ | $O(32NT_{i})$ | - | $O(\frac{1}{\sqrt{T}})$
     ([MoreEffDMLviaStaleSync,](#bib.bib69) ) |  ([RevistSynSGD,](#bib.bib31) ; [MoreEffDMLviaStaleSync,](#bib.bib69)
    ) |'
- en: '| Quant. | - | - | - | - | - |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | - | - | - | - | - |'
- en: '| Spars. | - | - | - | - | - |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | - | - | - | - | - |'
- en: '| ASP | Null | $O(32N\sum_{i}^{n}T_{i})$ | $O(32NT_{i})$ | $O(\frac{1}{T})$ ([Sebastian2019,](#bib.bib187)
    ; [10.5555/2969442.2969545,](#bib.bib107) ) | $O(\frac{1}{\sqrt{T}})$  ([zhang2018taming,](#bib.bib255)
    ) |  ([Sebastian2019,](#bib.bib187) ; [dean2012large,](#bib.bib43) ; [Meng2016AAS3060832,](#bib.bib126)
    ) |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| ASP | Null | $O(32N\sum_{i}^{n}T_{i})$ | $O(32NT_{i})$ | $O(\frac{1}{T})$ ([Sebastian2019,](#bib.bib187)
    ; [10.5555/2969442.2969545,](#bib.bib107) ) | $O(\frac{1}{\sqrt{T}})$  ([zhang2018taming,](#bib.bib255)
    ) |  ([Sebastian2019,](#bib.bib187) ; [dean2012large,](#bib.bib43) ; [Meng2016AAS3060832,](#bib.bib126)
    ) |'
- en: '| Quant. | $O(32N\sum_{i}^{n}T_{i})$ | $O(bNT_{i})$ | $O(\frac{1}{T})$ ([NIPS2019_8694,](#bib.bib244)
    ) | $O(\frac{1}{\sqrt{T}})$ ([QSGD,](#bib.bib7) ; [1bit,](#bib.bib167) ) |  ([NIPS2019_8694,](#bib.bib244)
    ; [QSGD,](#bib.bib7) ; [1bit,](#bib.bib167) ; [NIPS2019_9610,](#bib.bib14) ) |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | $O(32N\sum_{i}^{n}T_{i})$ | $O(bNT_{i})$ | $O(\frac{1}{T})$ ([NIPS2019_8694,](#bib.bib244)
    ) | $O(\frac{1}{\sqrt{T}})$ ([QSGD,](#bib.bib7) ; [1bit,](#bib.bib167) ) |  ([NIPS2019_8694,](#bib.bib244)
    ; [QSGD,](#bib.bib7) ; [1bit,](#bib.bib167) ; [NIPS2019_9610,](#bib.bib14) ) |'
- en: '| Spars. | $O(32N\sum_{i}^{n}T_{i})$ | $O(k(\log N+32)T_{i})$ | $O(\frac{1}{T})$ ([GradSparforDisOptim,](#bib.bib225)
    ) | - |  ([GradSparforDisOptim,](#bib.bib225) ; [AsynDisMLspar,](#bib.bib58) ;
    [Meng2016AAS3060832,](#bib.bib126) ; [NIPS2019_9610,](#bib.bib14) ) |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | $O(32N\sum_{i}^{n}T_{i})$ | $O(k(\log N+32)T_{i})$ | $O(\frac{1}{T})$ ([GradSparforDisOptim,](#bib.bib225)
    ) | - |  ([GradSparforDisOptim,](#bib.bib225) ; [AsynDisMLspar,](#bib.bib58) ;
    [Meng2016AAS3060832,](#bib.bib126) ; [NIPS2019_9610,](#bib.bib14) ) |'
- en: '| L-SGD | Null | $O(32N\frac{T}{\tau})$ | $O(32N\frac{T}{\tau})$ | $O(\frac{1}{T})$ ([Sebastian2019,](#bib.bib187)
    ) | $O(\frac{1}{\sqrt{T}})$  ([NIPS2019_9288,](#bib.bib62) ; [pmlrv97yu19d,](#bib.bib242)
    ; [pmlrv97yu19c,](#bib.bib241) ) |  ([PSGD,](#bib.bib263) ; [Sebastian2019,](#bib.bib187)
    )  ([NIPS2019_9288,](#bib.bib62) ; [Lin2018DontUL,](#bib.bib111) ; [pmlrv97yu19d,](#bib.bib242)
    ; [pmlrv97yu19c,](#bib.bib241) ) |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| L-SGD | Null | $O(32N\frac{T}{\tau})$ | $O(32N\frac{T}{\tau})$ | $O(\frac{1}{T})$ ([Sebastian2019,](#bib.bib187)
    ) | $O(\frac{1}{\sqrt{T}})$  ([NIPS2019_9288,](#bib.bib62) ; [pmlrv97yu19d,](#bib.bib242)
    ; [pmlrv97yu19c,](#bib.bib241) ) |  ([PSGD,](#bib.bib263) ; [Sebastian2019,](#bib.bib187)
    )  ([NIPS2019_9288,](#bib.bib62) ; [Lin2018DontUL,](#bib.bib111) ; [pmlrv97yu19d,](#bib.bib242)
    ; [pmlrv97yu19c,](#bib.bib241) ) |'
- en: '| Quant. | $O(32N\frac{T}{\tau})$ | $O(bN\frac{T}{\tau})$ | - | $O(\frac{1}{\sqrt{T}})$ ([AlinearSpeedupAnalysis,](#bib.bib82)
    ) |  ([SparseTernaryCompressionSTC,](#bib.bib162) ; [AlinearSpeedupAnalysis,](#bib.bib82)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | $O(32N\frac{T}{\tau})$ | $O(bN\frac{T}{\tau})$ | - | $O(\frac{1}{\sqrt{T}})$ ([AlinearSpeedupAnalysis,](#bib.bib82)
    ) |  ([SparseTernaryCompressionSTC,](#bib.bib162) ; [AlinearSpeedupAnalysis,](#bib.bib82)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
- en: '| Spars. | $O(32N\frac{T}{\tau})$ | $O(k(\log N+32)\frac{T}{\tau})$ | - | -
    |  ([SparseTernaryCompressionSTC,](#bib.bib162) ; [NIPS2019_9610,](#bib.bib14)
    ) |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | $O(32N\frac{T}{\tau})$ | $O(k(\log N+32)\frac{T}{\tau})$ | - | - |  ([SparseTernaryCompressionSTC,](#bib.bib162)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
- en: '| A.R. | BSP | Null | - | $O(32NT)$ | $O(\frac{1}{T})$ ([Bottou2016OptimizationMF,](#bib.bib20)
    ) | $O(\frac{1}{\sqrt{T}})$ ([Bottou2016OptimizationMF,](#bib.bib20) ) |  ([Krizhevsky2014OneWT,](#bib.bib94)
    ; [Bradley2011,](#bib.bib23) ; [552669,](#bib.bib93) ; [375451,](#bib.bib28) )
    |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| A.R. | BSP | Null | - | $O(32NT)$ | $O(\frac{1}{T})$ ([Bottou2016OptimizationMF,](#bib.bib20)
    ) | $O(\frac{1}{\sqrt{T}})$ ([Bottou2016OptimizationMF,](#bib.bib20) ) |  ([Krizhevsky2014OneWT,](#bib.bib94)
    ; [Bradley2011,](#bib.bib23) ; [552669,](#bib.bib93) ; [375451,](#bib.bib28) )
    |'
- en: '| Quant. | - | $O(bNT)$ | $O(\frac{1}{T})$ ([DisLearningDIANA,](#bib.bib130)
    )  ([AlinearSpeedupAnalysis,](#bib.bib82) ) | $O(\frac{1}{\sqrt{T}})$  ([AlinearSpeedupAnalysis,](#bib.bib82)
    ; [QSGD,](#bib.bib7) ) |  ([3LC,](#bib.bib110) )  ([FLStrategy,](#bib.bib91) ;
    [DisLearningDIANA,](#bib.bib130) ; [signSGDwithVote,](#bib.bib18) ; [TernGrad,](#bib.bib227)
    ; [Horvath2019NaturalCF,](#bib.bib73) )  ([signSGD,](#bib.bib17) )  ([ATOMO,](#bib.bib216)
    ; [NIPS2019_8598,](#bib.bib195) ; [Mishchenko201999OP,](#bib.bib131) ; [QSGD,](#bib.bib7)
    ; [scalableDisDNN,](#bib.bib190) ; [CommQuantforDataPara,](#bib.bib48) ; [EFsignSGD,](#bib.bib86)
    ) |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | - | $O(bNT)$ | $O(\frac{1}{T})$ ([DisLearningDIANA,](#bib.bib130) )
     ([AlinearSpeedupAnalysis,](#bib.bib82) ) | $O(\frac{1}{\sqrt{T}})$  ([AlinearSpeedupAnalysis,](#bib.bib82)
    ; [QSGD,](#bib.bib7) ) |  ([3LC,](#bib.bib110) )  ([FLStrategy,](#bib.bib91) ;
    [DisLearningDIANA,](#bib.bib130) ; [signSGDwithVote,](#bib.bib18) ; [TernGrad,](#bib.bib227)
    ; [Horvath2019NaturalCF,](#bib.bib73) )  ([signSGD,](#bib.bib17) )  ([ATOMO,](#bib.bib216)
    ; [NIPS2019_8598,](#bib.bib195) ; [Mishchenko201999OP,](#bib.bib131) ; [QSGD,](#bib.bib7)
    ; [scalableDisDNN,](#bib.bib190) ; [CommQuantforDataPara,](#bib.bib48) ; [EFsignSGD,](#bib.bib86)
    ) |'
- en: '| Spars. | - | $O(kn(\log N+32)T)$ | $O(\frac{1}{T})$ ([NIPS2019_9473,](#bib.bib78)
    ; [SparSGDwithMemory,](#bib.bib188) ) | $O(\frac{1}{\sqrt{T}})$  ([ConvSparGrad,](#bib.bib6)
    ; [ijcai2019473,](#bib.bib181) ) |  ([3LC,](#bib.bib110) ; [SparCommforDisGD,](#bib.bib5)
    )  ([FLStrategy,](#bib.bib91) ; [NIPS2019_9473,](#bib.bib78) ; [Horvath2019NaturalCF,](#bib.bib73)
    ; [ATOMO,](#bib.bib216) )  ([GradSparforDisOptim,](#bib.bib225) ; [DGC,](#bib.bib113)
    ; [CommQuantforDataPara,](#bib.bib48) ; [Zhao2019GlobalMC,](#bib.bib260) ; [GTopk,](#bib.bib179)
    ; [VarianceGradCompression,](#bib.bib208) ) |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | - | $O(kn(\log N+32)T)$ | $O(\frac{1}{T})$ ([NIPS2019_9473,](#bib.bib78)
    ; [SparSGDwithMemory,](#bib.bib188) ) | $O(\frac{1}{\sqrt{T}})$  ([ConvSparGrad,](#bib.bib6)
    ; [ijcai2019473,](#bib.bib181) ) |  ([3LC,](#bib.bib110) ; [SparCommforDisGD,](#bib.bib5)
    )  ([FLStrategy,](#bib.bib91) ; [NIPS2019_9473,](#bib.bib78) ; [Horvath2019NaturalCF,](#bib.bib73)
    ; [ATOMO,](#bib.bib216) )  ([GradSparforDisOptim,](#bib.bib225) ; [DGC,](#bib.bib113)
    ; [CommQuantforDataPara,](#bib.bib48) ; [Zhao2019GlobalMC,](#bib.bib260) ; [GTopk,](#bib.bib179)
    ; [VarianceGradCompression,](#bib.bib208) ) |'
- en: '| L-SGD | Null | - | $O(32N\frac{T}{\tau})$ | $O(\frac{1}{T})$ ([Sebastian2019,](#bib.bib187)
    ) | $O(\frac{1}{\sqrt{T}})$  ([NIPS2019_9288,](#bib.bib62) ; [pmlrv97yu19d,](#bib.bib242)
    ; [pmlrv97yu19c,](#bib.bib241) ) |  ([PSGD,](#bib.bib263) ; [Sebastian2019,](#bib.bib187)
    )  ([NIPS2019_9288,](#bib.bib62) ; [pmlrv97yu19d,](#bib.bib242) ; [Lin2018DontUL,](#bib.bib111)
    ; [pmlrv97yu19d,](#bib.bib242) ; [pmlrv97yu19c,](#bib.bib241) ) |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| L-SGD | Null | - | $O(32N\frac{T}{\tau})$ | $O(\frac{1}{T})$ ([Sebastian2019,](#bib.bib187)
    ) | $O(\frac{1}{\sqrt{T}})$  ([NIPS2019_9288,](#bib.bib62) ; [pmlrv97yu19d,](#bib.bib242)
    ; [pmlrv97yu19c,](#bib.bib241) ) |  ([PSGD,](#bib.bib263) ; [Sebastian2019,](#bib.bib187)
    )  ([NIPS2019_9288,](#bib.bib62) ; [pmlrv97yu19d,](#bib.bib242) ; [Lin2018DontUL,](#bib.bib111)
    ; [pmlrv97yu19d,](#bib.bib242) ; [pmlrv97yu19c,](#bib.bib241) ) |'
- en: '| Quant. | - | $O(bN\frac{T}{\tau})$ | - | $O(\frac{1}{\sqrt{T}})$ ([AlinearSpeedupAnalysis,](#bib.bib82)
    ) |  ([SparseTernaryCompressionSTC,](#bib.bib162) ; [AlinearSpeedupAnalysis,](#bib.bib82)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | - | $O(bN\frac{T}{\tau})$ | - | $O(\frac{1}{\sqrt{T}})$ ([AlinearSpeedupAnalysis,](#bib.bib82)
    ) |  ([SparseTernaryCompressionSTC,](#bib.bib162) ; [AlinearSpeedupAnalysis,](#bib.bib82)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
- en: '| Spars. | - | $O(kn(\log N+32)\frac{T}{\tau})$ | - | - |  ([SparseTernaryCompressionSTC,](#bib.bib162)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | - | $O(kn(\log N+32)\frac{T}{\tau})$ | - | - |  ([SparseTernaryCompressionSTC,](#bib.bib162)
    ; [NIPS2019_9610,](#bib.bib14) ) |'
- en: '| Gossip | BSP | Null | - | $O(32Nn_{peers}T)$ |  | $O(\frac{1}{\sqrt{T}})$ ([CanDecent,](#bib.bib108)
    ) |  ([CanDecent,](#bib.bib108) ; [StocGradPush,](#bib.bib10) ) |'
  id: totrans-368
  prefs: []
  type: TYPE_TB
  zh: '| Gossip | BSP | Null | - | $O(32Nn_{peers}T)$ |  | $O(\frac{1}{\sqrt{T}})$ ([CanDecent,](#bib.bib108)
    ) |  ([CanDecent,](#bib.bib108) ; [StocGradPush,](#bib.bib10) ) |'
- en: '| Quant. | - | $O(nbN_{peers}T)$ | - | $O(\frac{1}{\sqrt{T}})$ ([ECQSGD,](#bib.bib229)
    ) |  ([CommCompforDecent,](#bib.bib201) ; [DecentStocOptimAndGossip,](#bib.bib90)
    ; [7544448,](#bib.bib144) ; [NIPS2018_7705,](#bib.bib68) ; [ECQSGD,](#bib.bib229)
    ; [NIPS2019_9047,](#bib.bib151) ) |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | - | $O(nbN_{peers}T)$ | - | $O(\frac{1}{\sqrt{T}})$ ([ECQSGD,](#bib.bib229)
    ) |  ([CommCompforDecent,](#bib.bib201) ; [DecentStocOptimAndGossip,](#bib.bib90)
    ; [7544448,](#bib.bib144) ; [NIPS2018_7705,](#bib.bib68) ; [ECQSGD,](#bib.bib229)
    ; [NIPS2019_9047,](#bib.bib151) ) |'
- en: '| Spars. | - | $O(k(\log N+32)n_{peers}T)$ | - | - |  ([DecentStocOptimAndGossip,](#bib.bib90)
    ; [tang2020communication,](#bib.bib204) ) |'
  id: totrans-370
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | - | $O(k(\log N+32)n_{peers}T)$ | - | - |  ([DecentStocOptimAndGossip,](#bib.bib90)
    ; [tang2020communication,](#bib.bib204) ) |'
- en: '| ASP | Null | - | $O(32Nn_{peers}T_{i})$ | - | $O(\frac{1}{\sqrt{T}})$  ([pmlr-v80-lian18a,](#bib.bib109)
    ) |  ([pmlr-v80-lian18a,](#bib.bib109) ) |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '| ASP | Null | - | $O(32Nn_{peers}T_{i})$ | - | $O(\frac{1}{\sqrt{T}})$  ([pmlr-v80-lian18a,](#bib.bib109)
    ) |  ([pmlr-v80-lian18a,](#bib.bib109) ) |'
- en: '| Quant. | - | - | - | - | - |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | - | - | - | - | - |'
- en: '| Spars. | - | - | - | - | - |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | - | - | - | - | - |'
- en: '| L-SGD | Null | - | $O(32Nn_{peers}\frac{T}{\tau})$ | $O(\frac{1}{T})$  ([Jianyu180807576,](#bib.bib218)
    ) | $O(\frac{1}{\sqrt{T}})$  ([Jianyu180807576,](#bib.bib218) ) |  ([Jianyu180807576,](#bib.bib218)
    ) |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| L-SGD | Null | - | $O(32Nn_{peers}\frac{T}{\tau})$ | $O(\frac{1}{T})$  ([Jianyu180807576,](#bib.bib218)
    ) | $O(\frac{1}{\sqrt{T}})$  ([Jianyu180807576,](#bib.bib218) ) |  ([Jianyu180807576,](#bib.bib218)
    ) |'
- en: '| Quant. | - | - | - | - | - |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| 量化 | - | - | - | - | - |'
- en: '| Spars. | - | - | - | - | - |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| 稀疏 | - | - | - | - | - |'
- en: '| Pipelining |  ([zhang2017poseidon,](#bib.bib248) ; [awan2017s,](#bib.bib11)
    ; [Shi2018MGWFBPED,](#bib.bib170) ; [shi2020layer,](#bib.bib174) ; [shi2020communication,](#bib.bib177)
    ; [aritra2020discrepancy,](#bib.bib50) ) |'
  id: totrans-377
  prefs: []
  type: TYPE_TB
  zh: '| 流水线 |  ([zhang2017poseidon,](#bib.bib248) ; [awan2017s,](#bib.bib11) ; [Shi2018MGWFBPED,](#bib.bib170)
    ; [shi2020layer,](#bib.bib174) ; [shi2020communication,](#bib.bib177) ; [aritra2020discrepancy,](#bib.bib50)
    ) |'
- en: '| Scheduling |  ([Shi2018MGWFBPED,](#bib.bib170) ; [hashemi2018tictac,](#bib.bib65)
    ; [harlap2018pipedream,](#bib.bib64) ; [peng2019generic,](#bib.bib141) ; [jayarajan2018priority,](#bib.bib79)
    ; [DBLP:journals/corr/abs-1810-08313,](#bib.bib217) ; [8884800,](#bib.bib99) )
    |'
  id: totrans-378
  prefs: []
  type: TYPE_TB
  zh: '| 调度 |  ([Shi2018MGWFBPED,](#bib.bib170) ; [hashemi2018tictac,](#bib.bib65)
    ; [harlap2018pipedream,](#bib.bib64) ; [peng2019generic,](#bib.bib141) ; [jayarajan2018priority,](#bib.bib79)
    ; [DBLP:journals/corr/abs-1810-08313,](#bib.bib217) ; [8884800,](#bib.bib99) )
    |'
- en: •
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: •
- en: 'Notes: 1) The “Arch.” indicates the architecture supported in the original
    paper, “A.R.” represents All-Reduce, “Quant.” quantization and “Spars.” sparsification.
    The “Comm.” column indicates the communication scheme which includes “BSP” (Bulk
    Synchronous Parallel), “SSP” (Stale Synchronous Parallel), “ASP” (ASynchronous
    Parallel), and “L-SGD”(Local SGD), and $\tau$ in “L-SGD” means the local steps.
    2) Some methods use both compression techniques and both Asyc and Local SGD. 3)
    Some methods also optimize download communication, we have listed them together
    with upload communication. 4) The communication complexity and convergence rate
    of some paper maybe different, we just list out the common ones. 5) Pipelining
    and scheduling can be used in many methods, so we only list methods that uses
    pipeline without classifying it into any category.'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意事项：1）“Arch.” 表示原始论文中支持的架构，“A.R.” 代表全归约，“Quant.” 代表量化，“Spars.” 代表稀疏化。“Comm.”
    列表示通信方案，包括 “BSP”（批量同步并行）、“SSP”（过时同步并行）、“ASP”（异步并行）和 “L-SGD”（局部SGD），其中“L-SGD”中的
    $\tau$ 表示本地步骤。2）一些方法使用了压缩技术以及异步和局部SGD。3）一些方法还优化了下载通信，我们将它们与上传通信一起列出。4）某些论文的通信复杂性和收敛速度可能不同，我们仅列出了常见的。5）流水线和调度可以用于许多方法，因此我们仅列出使用流水线的方法，而没有将其分类到任何类别中。
- en: 9\. Auxiliary Technologies
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9\. 辅助技术
- en: Communication compression methods can achieve convergence and reduce communication
    loads with the aid of auxiliary technologies.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 通信压缩方法可以在辅助技术的帮助下实现收敛并减少通信负载。
- en: 9.1\. Error Accumulation
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 错误积累
- en: 'In 1-bit SGD  ([1bit,](#bib.bib167) ), the current quantization error is added
    to the next mini-batch gradient before quantization in the next iteration through
    error accumulation. This error-feedback mechanism ensures that all gradients are
    eventually added up into the model with full accuracy, although with some delays.
    This process is described by Eq. ([9](#S5.E9 "In 5\. Quantization methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")), ([10](#S5.E10
    "In 5\. Quantization methods ‣ Communication-Efficient Data Parallel Distributed
    Deep Learning: A Comprehensive Survey")) and ([11](#S5.E11 "In 5\. Quantization
    methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")). Karimireddy et al.  ([EFsignSGD,](#bib.bib86) ) proposed EF-SIGNSGD
    (SIGNSGD with Error-Feedback), which also uses error accumulation by storing the
    error locally and adding it to the next step.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '在1位SGD ([1bit,](#bib.bib167)) 中，当前的量化误差在下一次迭代中通过错误积累被添加到下一个小批量梯度之前的量化中。这个误差反馈机制确保所有梯度最终以完全的准确性被加到模型中，尽管存在一些延迟。这个过程由方程
    ([9](#S5.E9 "In 5\. Quantization methods ‣ Communication-Efficient Data Parallel
    Distributed Deep Learning: A Comprehensive Survey"))、 ([10](#S5.E10 "In 5\. Quantization
    methods ‣ Communication-Efficient Data Parallel Distributed Deep Learning: A Comprehensive
    Survey")) 和 ([11](#S5.E11 "In 5\. Quantization methods ‣ Communication-Efficient
    Data Parallel Distributed Deep Learning: A Comprehensive Survey")) 描述。Karimireddy
    等人 ([EFsignSGD,](#bib.bib86)) 提出了 EF-SIGNSGD（带有错误反馈的SIGNSGD），它也通过在本地存储错误并将其添加到下一步来使用错误积累。'
- en: 'Numerous formulations of error accumulation have been proposed in the literature
     ([3LC,](#bib.bib110) ; [SparCommforDisGD,](#bib.bib5) ; [sparsebinarycompression,](#bib.bib163)
    ; [Adacomp,](#bib.bib29) ; [CommQuantforDataPara,](#bib.bib48) ; [DisLearningDIANA,](#bib.bib130)
    ; [ECQSGD,](#bib.bib229) ; [EFsignSGD,](#bib.bib86) ; [SparseTernaryCompressionSTC,](#bib.bib162)
    ; [scalableDisDNN,](#bib.bib190) ; [SparCommforDisGD,](#bib.bib5) ; [VarianceGradCompression,](#bib.bib208)
    ; [DEF,](#bib.bib232) ; [9442310,](#bib.bib215) ; [pmlr-v139-tang21a,](#bib.bib200)
    ; [EF21,](#bib.bib153) ). In summary, error accumulation involves incorporating
    the quantization error into the subsequent gradient computations, which improves
    the accuracy of the final model. This technique can be formulated as following
    steps: (1) gradient compression: $C_{i,t}=Sparse(v_{i,t-1}+\nabla_{i,t})$; (2)
    error accumulation $v_{i,t}=\nabla_{i,t}-C_{i,t}$; (3) update weight: $x_{t+1}=x_{t}-\gamma\frac{1}{n}\sum_{i=1}^{n}C_{i,t}$,
    where $C_{i,t}$ represents the updates which are compressed by any compression
    method $Sparse(\cdot)$, $\nabla$ the gradient, at $t$-th iteration and in worker
    $i$.'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中提出了许多误差累积的公式 ([3LC,](#bib.bib110) ; [SparCommforDisGD,](#bib.bib5) ; [sparsebinarycompression,](#bib.bib163)
    ; [Adacomp,](#bib.bib29) ; [CommQuantforDataPara,](#bib.bib48) ; [DisLearningDIANA,](#bib.bib130)
    ; [ECQSGD,](#bib.bib229) ; [EFsignSGD,](#bib.bib86) ; [SparseTernaryCompressionSTC,](#bib.bib162)
    ; [scalableDisDNN,](#bib.bib190) ; [SparCommforDisGD,](#bib.bib5) ; [VarianceGradCompression,](#bib.bib208)
    ; [DEF,](#bib.bib232) ; [9442310,](#bib.bib215) ; [pmlr-v139-tang21a,](#bib.bib200)
    ; [EF21,](#bib.bib153)）。总之，误差累积涉及将量化误差纳入后续梯度计算中，这提高了最终模型的准确性。该技术可以表述为以下步骤：（1）梯度压缩：$C_{i,t}=Sparse(v_{i,t-1}+\nabla_{i,t})$；（2）误差累积
    $v_{i,t}=\nabla_{i,t}-C_{i,t}$；（3）更新权重：$x_{t+1}=x_{t}-\gamma\frac{1}{n}\sum_{i=1}^{n}C_{i,t}$，其中
    $C_{i,t}$ 表示通过任意压缩方法 $Sparse(\cdot)$ 压缩的更新量，$\nabla$ 是梯度，在第 $t$ 次迭代中和工人 $i$。
- en: Wu et al. ([ECQSGD,](#bib.bib229) ) proposed ECQ-SGD (Error Compensated Quantized
    SGD). This method differs from other compression techniques as it considers not
    only the compression error in the current iteration but also all previous errors
    by accumulating them. Tang et al. proposed 1-bit Adam ([pmlr-v139-tang21a,](#bib.bib200)
    ), which incorporates error compensation into distributed Adam with 1-bit compression.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: Wu 等人 ([ECQSGD,](#bib.bib229)) 提出了 ECQ-SGD（误差补偿量化 SGD）。这种方法与其他压缩技术不同，因为它不仅考虑当前迭代中的压缩误差，还通过累积所有先前的误差。Tang
    等人提出了 1-bit Adam ([pmlr-v139-tang21a,](#bib.bib200))，将误差补偿融入分布式 Adam 并实现 1-bit
    压缩。
- en: 9.2\. Momentum Correction
  id: totrans-387
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2. 动量修正
- en: 'Lin et al. ([DGC,](#bib.bib113) ) proposed the Momentum Correction technique
    to assist DGC in utilizing Momentum SGD. This approach involves the application
    of vanilla momentum SGD ([vanillaMomentum,](#bib.bib145) ) and error accumulation
    to the sparse gradient. The update process is as follows: (1) momentum accumulation:
    $u_{i,t}=mu_{i,t-1}+\nabla_{i,t}$; (2) error accumulation $v_{i,t}=v_{i,t-1}+u_{i,t}$;
    (3) update weight: $x_{t+1}=x_{t}-\gamma\sum_{i=1}^{n}sparse(v_{i,t})$, where
    $m$ denotes the coefficient of momentum and $u_{i,t}$ denotes the momentum at
    $t$-th iteration on worker $i$. Momentum Correction has also been exploited in
     ([sparsebinarycompression,](#bib.bib163) ; [DisLearningDIANA,](#bib.bib130) ;
    [SparseTernaryCompressionSTC,](#bib.bib162) ).'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: Lin 等人 ([DGC,](#bib.bib113)) 提出了动量修正技术，以帮助 DGC 利用动量 SGD。这种方法涉及对稀疏梯度应用普通动量 SGD
    ([vanillaMomentum,](#bib.bib145)) 和误差累积。更新过程如下：（1）动量累积：$u_{i,t}=mu_{i,t-1}+\nabla_{i,t}$；（2）误差累积
    $v_{i,t}=v_{i,t-1}+u_{i,t}$；（3）更新权重：$x_{t+1}=x_{t}-\gamma\sum_{i=1}^{n}sparse(v_{i,t})$，其中
    $m$ 表示动量系数，$u_{i,t}$ 表示第 $t$ 次迭代中工人 $i$ 的动量。动量修正也被应用于 ([sparsebinarycompression,](#bib.bib163)
    ; [DisLearningDIANA,](#bib.bib130) ; [SparseTernaryCompressionSTC,](#bib.bib162))。
- en: 'Zhao et al. ([Zhao2019GlobalMC,](#bib.bib260) ) proposed Global Momentum Compression,
    of which the update process becomes: (1) momentum accumulation: $u_{i,t}=\nabla_{i,t}-m(x_{t}-x_{t-1})$;
    (2) error accumulation: $v_{i,t}=v_{i,t-1}+u_{i,t}-sparse(v_{i,t-1}+u_{i,t})$;
    (3) update weight: $x_{t+1}=x_{t}-\gamma\sum_{i=1}^{n}sparse(v_{i,t-1}+u_{i,t})$.
    Thus, the gradient and them momentum are both compressed.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: Zhao 等人 ([Zhao2019GlobalMC,](#bib.bib260)) 提出了全局动量压缩，其更新过程如下：（1）动量累积：$u_{i,t}=\nabla_{i,t}-m(x_{t}-x_{t-1})$；（2）误差累积：$v_{i,t}=v_{i,t-1}+u_{i,t}-sparse(v_{i,t-1}+u_{i,t})$；（3）更新权重：$x_{t+1}=x_{t}-\gamma\sum_{i=1}^{n}sparse(v_{i,t-1}+u_{i,t})$。因此，梯度和动量都被压缩。
- en: 9.3\. Low-rank Decomposition
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3. 低秩分解
- en: The low-rand decomposition of matrix means to decompose a large matrix into
    the multiplication of small matrices. Thus, senders can reduce communication costs
    by sending smaller matrices, and recovering original matrix on receivers.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵的低秩分解意味着将一个大矩阵分解为多个小矩阵的乘积。因此，发送者可以通过发送更小的矩阵来减少通信成本，而接收者则恢复原始矩阵。
- en: Wang et al. ([ATOMO,](#bib.bib216) ) developed a new method named Atomic Sparsification
    (ATOMO). They demonstrated that gradient sparsification and quantization are parts
    of a general approach that sparsifies the gradients in some atomic decomposition,
    such as entry-wise methods like QSGD, singular value decomposition (SVD), Fourier
    decomposition, etc. ATOMO aims to minimize the variance of the sparsified gradient
    that is sparse on the atomic basis and maintain it as an unbiased estimator of
    the original gradient. They illustrate that the 1-bit QSGD and TernGrad are special
    cases of ATOMO. Furthermore, they improved ATOMO with SVD, named as Spectral-ATOMO.
    In their experiments, Spectral-ATOMO reduces the training time by a factor of
    $2\times$ and $3\times$, compared to QSGD and TernGrad, respectively.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: Wang 等人 ([ATOMO,](#bib.bib216)) 开发了一种新方法，命名为原子稀疏化 (ATOMO)。他们展示了梯度稀疏化和量化是稀疏化梯度的通用方法中的一部分，这些方法在某些原子分解中稀疏梯度，例如
    QSGD 这样的逐项方法、奇异值分解 (SVD)、傅里叶分解等。ATOMO 旨在最小化在原子基上稀疏的稀疏梯度的方差，并保持其作为原始梯度的无偏估计量。他们指出
    1-bit QSGD 和 TernGrad 是 ATOMO 的特例。此外，他们用 SVD 改进了 ATOMO，命名为 Spectral-ATOMO。在他们的实验中，Spectral-ATOMO
    将训练时间分别缩短了 $2\times$ 和 $3\times$，与 QSGD 和 TernGrad 相比。
- en: Ivkin et al.  ([NIPS2019_9473,](#bib.bib78) ) exploited a technique that is
    widely adopted in distributed systems, Count Sketch  ([Charikar2002FFI,](#bib.bib27)
    ). It compresses a gradient vector $G$ into a sketch $S(G)$ of size $O(1/\epsilon\log
    n)$., which can approximate every coordinate of $G$ and the $l_{2}$ norm of the
    entire gradient. Every worker sends this sketched gradient to the server, and
    the server recovers the $d$ largest coordinates of the sum of gradients and then
    performs the update.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: Ivkin 等人 ([NIPS2019_9473,](#bib.bib78)) 利用了一种在分布式系统中广泛采用的技术——Count Sketch ([Charikar2002FFI,](#bib.bib27))。它将一个梯度向量
    $G$ 压缩成一个大小为 $O(1/\epsilon\log n)$ 的草图 $S(G)$，可以近似 $G$ 的每个坐标以及整个梯度的 $l_{2}$ 范数。每个工作节点将这个草图梯度发送给服务器，服务器恢复梯度和的
    $d$ 个最大坐标，然后执行更新。
- en: 9.4\. Local Gradient Clipping
  id: totrans-394
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4\. 局部梯度裁剪
- en: The Gradient Clipping  ([GradientClipping,](#bib.bib16) ) is a widely used method
    in vanilla SGD to avoid the exploding gradient problem. This technique involves
    clipping all gradients that have values exceeding a user-defined threshold. For
    the BSP-SGD with gradient sparsification, Lin et al. ([DGC,](#bib.bib113) ) modified
    it as Local Gradient Clipping, which is performed before adding the error accumulation
    term and the gradient in current iteration. The $k$-th worker has a threshold
    $thr\left(G_{k}\right)$ for its local gradient $G_{k}$, and the aggregation of
    gradients has a threshold $thr\left(G\right)$ for the global gradient $G:=\sum_{k=1}^{N}G_{k}$.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度裁剪 ([GradientClipping,](#bib.bib16)) 是在 vanilla SGD 中广泛使用的一种方法，用于避免梯度爆炸问题。该技术涉及裁剪所有超过用户定义阈值的梯度。对于具有梯度稀疏化的
    BSP-SGD，Lin 等人 ([DGC,](#bib.bib113)) 将其修改为局部梯度裁剪，在添加误差累积项和当前迭代的梯度之前进行。第 $k$ 个工作节点对其局部梯度
    $G_{k}$ 具有一个阈值 $thr\left(G_{k}\right)$，而梯度的聚合具有一个阈值 $thr\left(G\right)$，用于全局梯度
    $G:=\sum_{k=1}^{N}G_{k}$。
- en: If we assume that all $N$ workers have an independent and identically distributed
    (IID) gradient distribution with variance $\sigma^{2}$, then the aggregation of
    all gradients have the variance $N\sigma^{2}$. So there are
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们假设所有 $N$ 个工作节点具有独立同分布 (IID) 的梯度分布，方差为 $\sigma^{2}$，那么所有梯度的聚合方差为 $N\sigma^{2}$。因此，有
- en: '| (14) |  | $E\left[\left\&#124;G_{k}\right\&#124;_{2}\right]\approx\sigma,\
    E\left[\left\&#124;G\right\&#124;_{2}\right]\approx N^{1/2}\sigma.$ |  |'
  id: totrans-397
  prefs: []
  type: TYPE_TB
  zh: '| (14) |  | $E\left[\left\|G_{k}\right\|_{2}\right]\approx\sigma,\ E\left[\left\|G\right\|_{2}\right]\approx
    N^{1/2}\sigma.$ |  |'
- en: Local Gradient Clipping can restore the original variance of the aggregation
    models by adjusting the clipping threshold with respect to the number of workers.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 局部梯度裁剪可以通过根据工作节点数量调整裁剪阈值来恢复聚合模型的原始方差。
- en: 9.5\. Warm-up Training
  id: totrans-399
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5\. 预热训练
- en: 'Lin et al.([DGC,](#bib.bib113) ) utilized Warm-up Training ([imagenet1hour,](#bib.bib57)
    ) in DGC to overcome the problem of rapidly varying neural network behavior in
    the first few epochs of training when the gradient values are excessively large.
    This technique involves dividing the training process into two periods: the warm-up
    period and the normal training period. During the warm-up period, the algorithm
    trains the model using a less aggressive learning rate and less aggressive gradient
    sparsity. This reduces the number of extreme gradients being delayed. In addition,
    the gradient sparsity increases exponentially from a small value to the final
    value. Subsequently, the algorithm trains the model using high sparsity and a
    decreasing learning rate, similar to vanilla SGD.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: Lin等人([DGC,](#bib.bib113) )在DGC中利用了Warm-up Training ([imagenet1hour,](#bib.bib57)
    )以克服在训练的头几个epoch中梯度值过大时神经网络行为剧烈变化的问题。该技术将训练过程分为两个阶段：预热阶段和正常训练阶段。在预热阶段，算法使用较少激进的学习率和较少激进的梯度稀疏性来训练模型。这减少了极端梯度的延迟数量。此外，梯度稀疏性从一个小值逐渐增加到最终值。随后，算法使用高稀疏性和逐渐减少的学习率来训练模型，类似于传统的SGD。
- en: 10\. Conclusion and Future Directions
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10\. 结论与未来方向
- en: 'In this survey, we provided a comprehensive introduction to the communication-efficient
    distributed DL algorithms. We summarized the taxonomy of distributed DL and classified
    the communication-efficient distributed training algorithms into four main dimensions:
    1) synchronous schemes, 2) system architectures, 3) compression techniques, and
    4) parallelism of communication and computing tasks. For each dimension, related
    techniques that address communication problems were introduced comprehensively.
    Furthermore, we provided a review of convergence bounds of different algorithms
    and some auxiliary techniques that help accelerate the training speed.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在本调查中，我们对通信高效的分布式深度学习算法进行了全面的介绍。我们总结了分布式深度学习的分类，并将通信高效的分布式训练算法划分为四个主要维度：1) 同步方案，2)
    系统架构，3) 压缩技术，和4) 通信与计算任务的并行性。对于每个维度，我们全面介绍了相关技术，解决通信问题。此外，我们还回顾了不同算法的收敛界限和一些辅助技术，这些技术有助于加速训练速度。
- en: 'Below, we summarize some challenges and future directions:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们总结的一些挑战和未来方向：
- en: (1)
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (1)
- en: Will current communication-efficient methods work in training foundation models?
    With current rapid developments of large language models, efficient training methods
    are crucially important for developing new techniques, democratizing them and
    energy saving. Thus, verifying current and designing new communication-efficient
    distributed training methods will be valuable.
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前的通信效率方法是否适用于训练基础模型？随着大型语言模型的快速发展，高效的训练方法对开发新技术、推广技术和节能至关重要。因此，验证现有方法并设计新的通信高效的分布式训练方法将具有重要价值。
- en: (2)
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (2)
- en: Higher compression level. Is it possible to implement an extremely high compression
    level without sacrificing training performance? While the current quantization
    method can reduce data size by a factor of 32 and sparsification by a factor of
    100-1000, achieving a higher compression ratio while maintaining model accuracy
    remains a challenging question.
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 更高的压缩级别。是否可以在不牺牲训练性能的情况下实现极高的压缩级别？虽然当前的量化方法可以将数据大小减少32倍，稀疏化可以减少100-1000倍，但在保持模型准确性的同时实现更高的压缩比仍然是一个挑战性的问题。
- en: (3)
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (3)
- en: Adaptive Compression. Gradient/model compression can reduce communication size
    and time. However, achieving a very high compression ratio typically requires
    a larger number of iterations to reach the target optimization error, making it
    challenging to balance compression ratio and convergence speed. Can different
    compression ratios be set for different layers/tensors in a deep model or for
    peers with varying network bandwidth to achieve optimal system throughput?
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自适应压缩。梯度/模型压缩可以减少通信大小和时间。然而，实现非常高的压缩比通常需要更多的迭代次数才能达到目标优化误差，这使得在压缩比和收敛速度之间找到平衡具有挑战性。是否可以为深度模型中的不同层/张量或具有不同网络带宽的对等节点设置不同的压缩比，以实现最佳的系统吞吐量？
- en: (4)
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (4)
- en: Fault-tolerant algorithms. While the algorithm runs smoothly in a stable computing
    cluster, uncertainty arises when a large number of heterogeneous devices are used
    to train deep models. Factors such as severe stragglers, network congestion, and
    worker failure may cause interference. Developing more fault-tolerant algorithms
    is an important direction for increasing the reliability of training.
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**容错算法**。尽管算法在稳定的计算集群中运行顺利，但当使用大量异构设备训练深度模型时，不确定性会出现。严重的滞后、网络拥塞和工作节点故障等因素可能会导致干扰。开发更多容错算法是提高训练可靠性的一个重要方向。'
- en: References
  id: totrans-412
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考文献
- en: '[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S. Ghemawat,
    G. Irving, M. Isard, M. Kudlur, et al. Tensorflow: A system for large-scale machine
    learning. In OSDI, 2016.'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[1] M. Abadi, P. Barham, J. Chen, Z. Chen, A. Davis, J. Dean, M. Devin, S.
    Ghemawat, G. Irving, M. Isard, M. Kudlur 等。Tensorflow：一个大规模机器学习系统。见于 OSDI，2016。'
- en: '[2] S. Agarwal, H. Wang, S. Venkataraman, and D. Papailiopoulos. On the utility
    of gradient compression in distributed training systems. In MLSys, 2022.'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[2] S. Agarwal, H. Wang, S. Venkataraman, 和 D. Papailiopoulos. 在分布式训练系统中梯度压缩的效用。见于
    MLSys，2022。'
- en: '[3] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, and A. J. Smola. Scalable
    inference in latent variable models. In WSDM, 2012.'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[3] A. Ahmed, M. Aly, J. Gonzalez, S. Narayanamurthy, 和 A. J. Smola. 潜变量模型中的可扩展推断。见于
    WSDM，2012。'
- en: '[4] S. Ahn, B. Shahbaba, and M. Welling. Distributed stochastic gradient mcmc.
    In ICML, 2014.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[4] S. Ahn, B. Shahbaba, 和 M. Welling. 分布式随机梯度 MCMC。见于 ICML，2014。'
- en: '[5] A. F. Aji and K. Heafield. Sparse communication for distributed gradient
    descent. In EMNLP, 2017.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[5] A. F. Aji 和 K. Heafield. 用于分布式梯度下降的稀疏通信。见于 EMNLP，2017。'
- en: '[6] D. Alistarh, T. Hoefler, M. Johansson, S. Khirirat, N. Konstantinov, and
    C. Renggli. The convergence of sparsified gradient methods. In NeurIPS, 2018.'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[6] D. Alistarh, T. Hoefler, M. Johansson, S. Khirirat, N. Konstantinov, 和
    C. Renggli. 稀疏化梯度方法的收敛性。见于 NeurIPS，2018。'
- en: '[7] D. Alistarh, J. Li, R. Tomioka, and M. Vojnovic. Qsgd: Randomized quantization
    for communication-optimal stochastic gradient descent. ArXiv, abs/1610.02132,
    2016.'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[7] D. Alistarh, J. Li, R. Tomioka, 和 M. Vojnovic. QSGD：用于通信最优随机梯度下降的随机量化。ArXiv，abs/1610.02132，2016。'
- en: '[8] D. Alistarh, C. D. Sa, and N. Konstantinov. The convergence of stochastic
    gradient descent in asynchronous shared memory. In PODC ’18, 2018.'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[8] D. Alistarh, C. D. Sa, 和 N. Konstantinov. 异步共享内存中随机梯度下降的收敛性。见于 PODC ’18，2018。'
- en: '[9] D. Amodei, S. Ananthanarayanan, R. Anubhai, et al. Deep speech 2: End-to-end
    speech recognition in english and mandarin. In ICML, 2016.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[9] D. Amodei, S. Ananthanarayanan, R. Anubhai 等。Deep Speech 2：英语和普通话的端到端语音识别。见于
    ICML，2016。'
- en: '[10] M. Assran, N. Loizou, N. Ballas, and M. Rabbat. Stochastic gradient push
    for distributed deep learning. In ICML, 2019.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[10] M. Assran, N. Loizou, N. Ballas, 和 M. Rabbat. 用于分布式深度学习的随机梯度推送。见于 ICML，2019。'
- en: '[11] A. A. Awan, K. Hamidouche, J. M. Hashmi, and D. K. Panda. S-caffe: Co-designing
    mpi runtimes and caffe for scalable deep learning on modern GPU clusters. In Acm
    Sigplan Notices, volume 52, pages 193–205\. ACM, 2017.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[11] A. A. Awan, K. Hamidouche, J. M. Hashmi, 和 D. K. Panda. S-caffe：共设计 mpi
    运行时和 caffe 以实现现代 GPU 集群上的可扩展深度学习。见于 Acm Sigplan Notices，第 52 卷，第 193–205 页。ACM，2017。'
- en: '[12] N. S. Aybat, Z. Wang, T. Lin, and S. Ma. Distributed linearized alternating
    direction method of multipliers for composite convex consensus optimization. IEEE
    Transactions on Automatic Control, 63(1):5–20, Jan 2018.'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[12] N. S. Aybat, Z. Wang, T. Lin, 和 S. Ma. 用于复合凸共识优化的分布式线性化交替方向乘子法。IEEE Transactions
    on Automatic Control，63(1)：5–20，2018 年 1 月。'
- en: '[13] J. Ba, J. R. Kiros, and G. E. Hinton. Layer normalization. ArXiv, abs/1607.06450,
    2016.'
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[13] J. Ba, J. R. Kiros, 和 G. E. Hinton. 层归一化。ArXiv，abs/1607.06450，2016。'
- en: '[14] D. Basu, D. Data, C. Karakus, and S. Diggavi. Qsparse-local-SGD: Distributed
    SGD with quantization, sparsification and local computations. In NeurIPS. 2019.'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[14] D. Basu, D. Data, C. Karakus, 和 S. Diggavi. Qsparse-local-SGD：带有量化、稀疏化和局部计算的分布式
    SGD。见于 NeurIPS，2019。'
- en: '[15] T. Ben-Nun and T. Hoefler. Demystifying parallel and distributed deep
    learning: An in-depth concurrency analysis. ACM Comput. Surv., 52(4), Aug. 2019.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[15] T. Ben-Nun 和 T. Hoefler. 解密并行和分布式深度学习：深入的并发分析。ACM Comput. Surv.，52(4)，2019
    年 8 月。'
- en: '[16] Y. Bengio, P. Simard, and P. Frasconi. Learning long-term dependencies
    with gradient descent is difficult. IEEE Transactions on Neural Networks, 5(2):157–166,
    March 1994.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[16] Y. Bengio, P. Simard, 和 P. Frasconi. 使用梯度下降学习长期依赖关系是困难的。IEEE Transactions
    on Neural Networks，5(2)：157–166，1994 年 3 月。'
- en: '[17] J. Bernstein, Y. Wang, K. Azizzadenesheli, and A. Anandkumar. SIGNSGD:
    compressed optimisation for non-convex problems. In ICML, 2018.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[17] J. Bernstein, Y. Wang, K. Azizzadenesheli, 和 A. Anandkumar. SIGNSGD：针对非凸问题的压缩优化。见于
    ICML，2018。'
- en: '[18] J. Bernstein, J. Zhao, K. Azizzadenesheli, and A. Anandkumar. signsgd
    with majority vote is communication efficient and byzantine fault tolerant. ArXiv,
    abs/1810.05291, 2018.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[18] J. Bernstein, J. Zhao, K. Azizzadenesheli, 和 A. Anandkumar. 使用多数投票的 signsgd
    具有通信效率和拜占庭容错性。ArXiv, abs/1810.05291，2018年。'
- en: '[19] A. S. Bijral, A. D. Sarwate, and N. Srebro. On data dependence in distributed
    stochastic optimization. arXiv: Optimization and Control, 2016.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[19] A. S. Bijral, A. D. Sarwate, 和 N. Srebro. 分布式随机优化中的数据依赖性。arXiv: Optimization
    and Control, 2016年。'
- en: '[20] L. Bottou, F. E. Curtis, and J. Nocedal. Optimization methods for large-scale
    machine learning. SIAM Review, 60:223–311, 2016.'
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[20] L. Bottou, F. E. Curtis, 和 J. Nocedal. 大规模机器学习的优化方法。SIAM Review, 60:223–311,
    2016年。'
- en: '[21] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. Gossip algorithms: design,
    analysis and applications. In Proceedings IEEE 24th Annual Joint Conference of
    the IEEE Computer and Communications Societies., 2005.'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[21] S. Boyd, A. Ghosh, B. Prabhakar, 和 D. Shah. Gossip 算法：设计、分析与应用。在 IEEE
    第24届年度联合计算机与通信学会会议论文集中，2005年。'
- en: '[22] S. Boyd, A. Ghosh, B. Prabhakar, and D. Shah. Randomized gossip algorithms.
    IEEE/ACM Trans. Netw., 14(SI):2508–2530, June 2006.'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[22] S. Boyd, A. Ghosh, B. Prabhakar, 和 D. Shah. 随机化 gossip 算法。IEEE/ACM Trans.
    Netw., 14(SI):2508–2530, 2006年6月。'
- en: '[23] J. K. Bradley, A. Kyrola, D. Bickson, and C. Guestrin. Parallel coordinate
    descent for l1-regularized loss minimization. In ICML, 2011.'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[23] J. K. Bradley, A. Kyrola, D. Bickson, 和 C. Guestrin. l1-正则化损失最小化的并行坐标下降法。在
    ICML，2011年。'
- en: '[24] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, et al. Language models are few-shot learners.
    arXiv preprint arXiv:2005.14165, 2020.'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[24] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan,
    P. Shyam, G. Sastry, A. Askell, 等. 语言模型是少样本学习者。arXiv 预印本 arXiv:2005.14165，2020年。'
- en: '[25] J. Bruck, Ching-Tien Ho, S. Kipnis, E. Upfal, and D. Weathersby. Efficient
    algorithms for all-to-all communications in multiport message-passing systems.
    IEEE TPDS, 8(11):1143–1156, Nov 1997.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[25] J. Bruck, Ching-Tien Ho, S. Kipnis, E. Upfal, 和 D. Weathersby. 多端口消息传递系统中的全到全通信高效算法。IEEE
    TPDS, 8(11):1143–1156, 1997年11月。'
- en: '[26] R. Carli, F. Fagnani, P. Frasca, and S. Zampieri. Gossip consensus algorithms
    via quantized communication. Automatica, 46(1):70–80, Jan. 2010.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[26] R. Carli, F. Fagnani, P. Frasca, 和 S. Zampieri. 通过量化通信实现 gossip 一致性算法。Automatica,
    46(1):70–80, 2010年1月。'
- en: '[27] M. Charikar, K. Chen, and M. Farach-Colton. Finding frequent items in
    data streams. In Proceedings of the 29th International Colloquium on Automata,
    Languages and Programming, ICALP ’02, 2002.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[27] M. Charikar, K. Chen, 和 M. Farach-Colton. 在数据流中寻找频繁项。在第二十九届自动机、语言与编程国际研讨会论文集中，ICALP
    ’02，2002年。'
- en: '[28] T. Cheatham, A. Fahmy, D. C. Stefanescu, and L. G. Valiant. Bulk synchronous
    parallel computing-a paradigm for transportable software. In Proceedings of the
    Twenty-Eighth Annual Hawaii International Conference on System Sciences, volume 2,
    pages 268–275 vol.2, Jan 1995.'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[28] T. Cheatham, A. Fahmy, D. C. Stefanescu, 和 L. G. Valiant. 大规模同步并行计算——一种可移植软件的范式。在第二十八届夏威夷国际系统科学大会论文集中，第2卷，第268–275页，1995年1月。'
- en: '[29] C. Chen, J. Choi, D. Brand, A. Agrawal, W. Zhang, and K. Gopalakrishnan.
    Adacomp : Adaptive residual gradient compression for data-parallel distributed
    training. In AAAI, 2018.'
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[29] C. Chen, J. Choi, D. Brand, A. Agrawal, W. Zhang, 和 K. Gopalakrishnan.
    Adacomp：用于数据并行分布式训练的自适应残差梯度压缩。发表于 AAAI，2018年。'
- en: '[30] C. Chen, W. Wang, and B. Li. Round-robin synchronization: Mitigating communication
    bottlenecks in parameter servers. In IEEE INFOCOM, 2019.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[30] C. Chen, W. Wang, 和 B. Li. 循环同步：缓解参数服务器中的通信瓶颈。在 IEEE INFOCOM，2019年。'
- en: '[31] J. Chen, R. Monga, S. Bengio, and R. Jozefowicz. Revisiting distributed
    synchronous sgd. In ICLR Workshop Track, 2016.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[31] J. Chen, R. Monga, S. Bengio, 和 R. Jozefowicz. 重新审视分布式同步 sgd。在 ICLR 研讨会中，2016年。'
- en: '[32] K. Chen and Q. Huo. Scalable training of deep learning machines by incremental
    block training with intra-block parallel optimization and blockwise model-update
    filtering. In ICASSP-2016, March 2016.'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[32] K. Chen 和 Q. Huo. 通过增量块训练与块内并行优化和块级模型更新过滤实现深度学习机器的可扩展训练。在 ICASSP-2016，2016年3月。'
- en: '[33] T. Chen, G. Giannakis, T. Sun, and W. Yin. Lag: Lazily aggregated gradient
    for communication-efficient distributed learning. In NeurIPS, 2018.'
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[33] T. Chen, G. Giannakis, T. Sun, 和 W. Yin. Lag：用于通信高效的分布式学习的惰性聚合梯度。在 NeurIPS，2018年。'
- en: '[34] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
    and E. Shelhamer. cuDNN: Efficient primitives for deep learning. arXiv preprint
    arXiv:1410.0759, 2014.'
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[34] S. Chetlur, C. Woolley, P. Vandermersch, J. Cohen, J. Tran, B. Catanzaro,
    和 E. Shelhamer. cuDNN：用于深度学习的高效原语。arXiv 预印本 arXiv:1410.0759，2014年。'
- en: '[35] T. Chilimbi, Y. Suzue, J. Apacible, and K. Kalyanaraman. Project adam:
    Building an efficient and scalable deep learning training system. In OSDI, 2014.'
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[35] T. Chilimbi, Y. Suzue, J. Apacible, 和 K. Kalyanaraman. Project adam: 构建高效且可扩展的深度学习训练系统。发表于OSDI，2014年。'
- en: '[36] C.-H. Chu, X. Lu, A. A. Awan, H. Subramoni, J. Hashmi, B. Elton, and D. K.
    Panda. Efficient and scalable multi-source streaming broadcast on GPU clusters
    for deep learning. In ICPP, 2017.'
  id: totrans-448
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[36] C.-H. Chu, X. Lu, A. A. Awan, H. Subramoni, J. Hashmi, B. Elton, 和 D. K.
    Panda. 在GPU集群上高效且可扩展的多源流式广播，用于深度学习。发表于ICPP，2017年。'
- en: '[37] I. Colin, A. Bellet, J. Salmon, and S. Clémençon. Gossip dual averaging
    for decentralized optimization of pairwise functions. In ICML, 2016.'
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[37] I. Colin, A. Bellet, J. Salmon, 和 S. Clémençon. 用于去中心化优化对偶平均的Gossip方法。发表于ICML，2016年。'
- en: '[38] H. Cui, J. Cipar, Q. Ho, J. K. Kim, S. Lee, A. Kumar, J. Wei, W. Dai,
    G. R. Ganger, P. B. Gibbons, G. A. Gibson, and E. P. Xing. Exploiting bounded
    staleness to speed up big data analytics. In Proceedings of the 2014 USENIX Conference
    on USENIX Annual Technical Conference, USENIX ATC’14, pages 37–48, Berkeley, CA,
    USA, 2014. USENIX Association.'
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[38] H. Cui, J. Cipar, Q. Ho, J. K. Kim, S. Lee, A. Kumar, J. Wei, W. Dai,
    G. R. Ganger, P. B. Gibbons, G. A. Gibson, 和 E. P. Xing. 利用受限过时性加速大数据分析。发表于2014年USENIX年度技术会议，USENIX
    ATC’14，页面37–48，加州伯克利，美国，2014年。USENIX协会。'
- en: '[39] H. Cui, A. Tumanov, J. Wei, L. Xu, W. Dai, J. Haber-Kucharsky, Q. Ho,
    G. R. Ganger, P. B. Gibbons, G. A. Gibson, and E. P. Xing. Exploiting iterative-ness
    for parallel ml computations. In ACM SOCC, 2014.'
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[39] H. Cui, A. Tumanov, J. Wei, L. Xu, W. Dai, J. Haber-Kucharsky, Q. Ho,
    G. R. Ganger, P. B. Gibbons, G. A. Gibson, 和 E. P. Xing. 利用迭代特性加速并行机器学习计算。发表于ACM
    SOCC，2014年。'
- en: '[40] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, and E. P. Xing. Geeps:
    Scalable deep learning on distributed gpus with a gpu-specialized parameter server.
    In EuroSys, 2016.'
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[40] H. Cui, H. Zhang, G. R. Ganger, P. B. Gibbons, 和 E. P. Xing. Geeps: 在分布式GPU上可扩展的深度学习，利用GPU专用参数服务器。发表于EuroSys，2016年。'
- en: '[41] J. Daily, A. Vishnu, C. Siegel, T. Warfel, and V. Amatya. Gossipgrad:
    Scalable deep learning using gossip communication based asynchronous gradient
    descent. CoRR, abs/1803.05880, 2018.'
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[41] J. Daily, A. Vishnu, C. Siegel, T. Warfel, 和 V. Amatya. Gossipgrad: 利用Gossip通信的可扩展深度学习基于异步梯度下降。CoRR,
    abs/1803.05880, 2018年。'
- en: '[42] L. Dalcín, R. Paz, M. Storti, and J. D’Elía. Mpi for python: Performance
    improvements and mpi-2 extensions. Journal of Parallel and Distributed Computing,
    2008.'
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[42] L. Dalcín, R. Paz, M. Storti, 和 J. D’Elía. MPI for Python: 性能改进和MPI-2扩展。并行与分布式计算杂志，2008年。'
- en: '[43] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato,
    A. Senior, P. Tucker, K. Yang, et al. Large scale distributed deep networks. In
    NeurIPS, 2012.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[43] J. Dean, G. Corrado, R. Monga, K. Chen, M. Devin, M. Mao, M. Ranzato,
    A. Senior, P. Tucker, K. Yang, 等等. 大规模分布式深度网络。发表于NeurIPS，2012年。'
- en: '[44] A. Defazio and L. Bottou. On the ineffectiveness of variance reduced optimization
    for deep learning. In NeurIPS. 2019.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[44] A. Defazio 和 L. Bottou. 关于变差减少优化在深度学习中的低效性。发表于NeurIPS，2019年。'
- en: '[45] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei. Imagenet:
    A large-scale hierarchical image database. In CVPR, 2009.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[45] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, 和 Li Fei-Fei. Imagenet: 大规模层次图像数据库。发表于CVPR，2009年。'
- en: '[46] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of
    deep bidirectional transformers for language understanding. In NAACL, 2019.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[46] J. Devlin, M.-W. Chang, K. Lee, 和 K. Toutanova. BERT: 用于语言理解的深度双向变换器的预训练。发表于NAACL，2019年。'
- en: '[47] A. Dieuleveut and K. K. Patel. Communication trade-offs for local-sgd
    with large step size. In NeurIPS. 2019.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[47] A. Dieuleveut 和 K. K. Patel. 大步长下本地SGD的通信权衡。发表于NeurIPS，2019年。'
- en: '[48] N. Dryden, S. A. Jacobs, T. Moon, and B. Van Essen. Communication quantization
    for data-parallel training of deep neural networks. In Proceedings of the Workshop
    on MLHPC, 2016.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[48] N. Dryden, S. A. Jacobs, T. Moon, 和 B. Van Essen. 数据并行训练深度神经网络的通信量化。发表于MLHPC研讨会论文集，2016年。'
- en: '[49] C. Dünner, T. P. Parnell, and M. Jaggi. Efficient use of limited-memory
    accelerators for linear learning on heterogeneous systems. In NeurIPS, 2017.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[49] C. Dünner, T. P. Parnell, 和 M. Jaggi. 在异构系统上高效使用有限内存加速器进行线性学习。发表于NeurIPS，2017年。'
- en: '[50] A. Dutta, E. H. Bergou, A. M. Abdelmoniem, C.-Y. Ho, A. N. Sahu, M. Canini,
    and P. Kalnis. On the discrepancy between the theoretical analysis and practical
    implementations of compressed communication for distributed deep learning. In
    AAAI, 2020.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[50] A. Dutta, E. H. Bergou, A. M. Abdelmoniem, C.-Y. Ho, A. N. Sahu, M. Canini,
    和 P. Kalnis. 关于分布式深度学习中压缩通信的理论分析与实际实现之间的差异。发表于AAAI，2020年。'
- en: '[51] F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, and A. Ramezani-Kebrya.
    Adaptive gradient quantization for data-parallel sgd. 2020.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[51] F. Faghri, I. Tabrizian, I. Markov, D. Alistarh, D. M. Roy, 和 A. Ramezani-Kebrya.
    数据并行SGD的自适应梯度量化。2020。'
- en: '[52] F. Fagnani and S. Zampieri. Randomized consensus algorithms over large
    scale networks. JSAC, 2008.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[52] F. Fagnani 和 S. Zampieri. 大规模网络上的随机共识算法。JSAC, 2008。'
- en: '[53] J. Fei, C.-Y. Ho, A. N. Sahu, M. Canini, and A. Sapio. Efficient sparse
    collective communication and its application to accelerate distributed deep learning.
    In ACM SIGCOMM, 2021.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[53] J. Fei, C.-Y. Ho, A. N. Sahu, M. Canini, 和 A. Sapio. 高效稀疏集体通信及其在加速分布式深度学习中的应用。见于
    ACM SIGCOMM, 2021。'
- en: '[54] W. M. Goodall. Television by pulse code modulation. The Bell System Technical
    Journal, 30(1):33–49, Jan 1951.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[54] W. M. Goodall. 通过脉冲编码调制的电视。贝尔系统技术期刊, 30(1):33–49, 1951年1月。'
- en: '[55] I. Goodfellow, Y. Bengio, and A. Courville. Deep Learning. The MIT Press,
    2016.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[55] I. Goodfellow, Y. Bengio, 和 A. Courville. 深度学习。麻省理工学院出版社, 2016。'
- en: '[56] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
    A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: Training imagenet
    in 1 hour. arXiv preprint arXiv:1706.02677, 2017.'
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[56] P. Goyal, P. Dollár, R. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
    A. Tulloch, Y. Jia, 和 K. He. 精确的大批量SGD: 在1小时内训练ImageNet。arXiv预印本 arXiv:1706.02677,
    2017。'
- en: '[57] P. Goyal, P. Dollár, R. B. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
    A. Tulloch, Y. Jia, and K. He. Accurate, large minibatch sgd: Training imagenet
    in 1 hour. ArXiv, abs/1706.02677, 2017.'
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[57] P. Goyal, P. Dollár, R. B. Girshick, P. Noordhuis, L. Wesolowski, A. Kyrola,
    A. Tulloch, Y. Jia, 和 K. He. 精确的大批量SGD: 在1小时内训练ImageNet。ArXiv, abs/1706.02677,
    2017。'
- en: '[58] D. Grishchenko, F. Iutzeler, J. Malick, and M.-R. Amini. Asynchronous
    distributed learning with sparse communications and identification. ArXiv, abs/1812.03871,
    2018.'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[58] D. Grishchenko, F. Iutzeler, J. Malick, 和 M.-R. Amini. 带稀疏通信和识别的异步分布式学习。ArXiv,
    abs/1812.03871, 2018。'
- en: '[59] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, and
    S. Lu. BCube: a high performance, server-centric network architecture for modular
    data centers. In ACM SIGCOMM, 2009.'
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[59] C. Guo, G. Lu, D. Li, H. Wu, X. Zhang, Y. Shi, C. Tian, Y. Zhang, 和 S.
    Lu. BCube: 一种高性能、以服务器为中心的网络架构，用于模块化数据中心。见于 ACM SIGCOMM, 2009。'
- en: '[60] Y. Guo. A survey on methods and theories of quantized neural networks.
    CoRR, abs/1808.04752, 2018.'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[60] Y. Guo. 关于量化神经网络的方法与理论的调查。CoRR, abs/1808.04752, 2018。'
- en: '[61] S. Gupta, A. Agrawal, K. Gopalakrishnan, and P. Narayanan. Deep learning
    with limited numerical precision. In ICML, 2015.'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[61] S. Gupta, A. Agrawal, K. Gopalakrishnan, 和 P. Narayanan. 有限数值精度的深度学习。见于
    ICML, 2015。'
- en: '[62] F. Haddadpour, M. M. Kamani, M. Mahdavi, and V. Cadambe. Local sgd with
    periodic averaging: Tighter analysis and adaptive synchronization. In NeurIPS,
    pages 11080–11092\. 2019.'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[62] F. Haddadpour, M. M. Kamani, M. Mahdavi, 和 V. Cadambe. 带周期性平均的局部SGD: 更紧的分析和自适应同步。见于
    NeurIPS，第11080–11092页。2019。'
- en: '[63] P. Han, S. Wang, and K. K. Leung. Adaptive gradient sparsification for
    efficient federated learning: An online learning approach. arXiv preprint arXiv:2001.04756,
    2020.'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[63] P. Han, S. Wang, 和 K. K. Leung. 高效联邦学习的自适应梯度稀疏化: 一种在线学习方法。arXiv预印本 arXiv:2001.04756,
    2020。'
- en: '[64] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger,
    and P. Gibbons. Pipedream: Fast and efficient pipeline parallel DNN training.
    arXiv preprint arXiv:1806.03377, 2018.'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[64] A. Harlap, D. Narayanan, A. Phanishayee, V. Seshadri, N. Devanur, G. Ganger,
    和 P. Gibbons. Pipedream: 快速高效的管道并行DNN训练。arXiv预印本 arXiv:1806.03377, 2018。'
- en: '[65] S. H. Hashemi, S. A. Jyothi, and R. H. Campbell. Tictac: Accelerating
    distributed deep learning with communication scheduling. In In Proceedings of
    Systems and Machine Learning (SysML), 2018.'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[65] S. H. Hashemi, S. A. Jyothi, 和 R. H. Campbell. Tictac: 通过通信调度加速分布式深度学习。见于《系统与机器学习会议（SysML）》，2018。'
- en: '[66] C. He, S. Li, J. So, M. Zhang, H. Wang, X. Wang, P. Vepakomma, A. Singh,
    H. Qiu, L. Shen, P. Zhao, Y. Kang, Y. Liu, R. Raskar, Q. Yang, M. Annavaram, and
    S. Avestimehr. Fedml: A research library and benchmark for federated machine learning.
    arXiv preprint arXiv:2007.13518, 2020.'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[66] C. He, S. Li, J. So, M. Zhang, H. Wang, X. Wang, P. Vepakomma, A. Singh,
    H. Qiu, L. Shen, P. Zhao, Y. Kang, Y. Liu, R. Raskar, Q. Yang, M. Annavaram, 和
    S. Avestimehr. Fedml: 联邦机器学习的研究库和基准。arXiv预印本 arXiv:2007.13518, 2020。'
- en: '[67] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image
    recognition. In CVPR, 2016.'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[67] K. He, X. Zhang, S. Ren, 和 J. Sun. 深度残差学习用于图像识别。见于 CVPR, 2016。'
- en: '[68] L. He, A. Bian, and M. Jaggi. Cola: Decentralized linear learning. In
    S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett,
    editors, NeurIPS, pages 4536–4546\. 2018.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[68] L. He, A. Bian, 和 M. Jaggi. Cola: 去中心化线性学习。见于 S. Bengio, H. Wallach, H.
    Larochelle, K. Grauman, N. Cesa-Bianchi, 和 R. Garnett 编辑的《NeurIPS》，第4536–4546页。2018。'
- en: '[69] Q. Ho, J. Cipar, H. Cui, J. K. Kim, S. Lee, P. B. Gibbons, G. A. Gibson,
    G. R. Ganger, and E. P. Xing. More effective distributed ml via a stale synchronous
    parallel parameter server. In NeurIPS, 2013.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[69] Q. Ho, J. Cipar, H. Cui, J. K. Kim, S. Lee, P. B. Gibbons, G. A. Gibson,
    G. R. Ganger, 和 E. P. Xing. 通过陈旧同步并行参数服务器提高分布式机器学习的效果. 见于 NeurIPS, 2013.'
- en: '[70] T. Hoefler, W. Gropp, R. Thakur, and J. L. Träff. Toward performance models
    of mpi implementations for understanding application scaling issues. In EuroMPI’10,
    2010.'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[70] T. Hoefler, W. Gropp, R. Thakur, 和 J. L. Träff. 朝着 MPI 实现的性能模型迈进，以理解应用程序扩展问题.
    见于 EuroMPI’10, 2010.'
- en: '[71] E. Hoffer, I. Hubara, and D. Soudry. Train longer, generalize better:
    Closing the generalization gap in large batch training of neural networks. In
    NeurIPS, 2017.'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[71] E. Hoffer, I. Hubara, 和 D. Soudry. 训练更久，泛化更好：缩小神经网络大批量训练中的泛化差距. 见于 NeurIPS,
    2017.'
- en: '[72] M. Höhfeld and S. E. Fahlman. Probabilistic rounding in neural network
    learning with limited precision. Neurocomputing, 4:291–299, 1992.'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[72] M. Höhfeld 和 S. E. Fahlman. 在有限精度的神经网络学习中的概率舍入. Neurocomputing, 4:291–299,
    1992.'
- en: '[73] S. Horvath, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, and P. Richtárik.
    Natural compression for distributed deep learning. ArXiv, abs/1905.10988, 2019.'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[73] S. Horvath, C.-Y. Ho, L. Horvath, A. N. Sahu, M. Canini, 和 P. Richtárik.
    用于分布式深度学习的自然压缩. ArXiv, abs/1905.10988, 2019.'
- en: '[74] C.-J. Hsieh, H.-F. Yu, and I. S. Dhillon. Passcode: Parallel asynchronous
    stochastic dual co-ordinate descent. In ICML, 2015.'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[74] C.-J. Hsieh, H.-F. Yu, 和 I. S. Dhillon. Passcode：并行异步随机双重坐标下降. 见于 ICML,
    2015.'
- en: '[75] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam,
    Q. V. Le, Y. Wu, et al. Gpipe: Efficient training of giant neural networks using
    pipeline parallelism. NeurIPS, 2019.'
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[75] Y. Huang, Y. Cheng, A. Bapna, O. Firat, D. Chen, M. Chen, H. Lee, J. Ngiam,
    Q. V. Le, Y. Wu, 等. Gpipe：使用流水线并行高效训练巨型神经网络. NeurIPS, 2019.'
- en: '[76] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, and Y. Bengio. Quantized
    neural networks: Training neural networks with low precision weights and activations.
    J. Mach. Learn. Res., 18(1):6869–6898, Jan. 2017.'
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[76] I. Hubara, M. Courbariaux, D. Soudry, R. El-Yaniv, 和 Y. Bengio. 量化神经网络：使用低精度权重和激活训练神经网络.
    J. Mach. Learn. Res., 18(1):6869–6898, 2017年1月.'
- en: '[77] F. Iutzeler, P. Bianchi, P. Ciblat, and W. Hachem. Asynchronous distributed
    optimization using a randomized alternating direction method of multipliers. In
    52nd IEEE Conference on Decision and Control, 2013.'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[77] F. Iutzeler, P. Bianchi, P. Ciblat, 和 W. Hachem. 使用随机交替方向乘子法的异步分布式优化.
    见于第52届IEEE决策与控制会议, 2013.'
- en: '[78] N. Ivkin, D. Rothchild, E. Ullah, V. braverman, I. Stoica, and R. Arora.
    Communication-efficient distributed sgd with sketching. In NeurIPS. 2019.'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[78] N. Ivkin, D. Rothchild, E. Ullah, V. braverman, I. Stoica, 和 R. Arora.
    通信高效的分布式 SGD 与草图. 见于 NeurIPS, 2019.'
- en: '[79] A. Jayarajan, J. Wei, G. Gibson, A. Fedorova, and G. Pekhimenko. Priority-based
    parameter propagation for distributed dnn training. In In Proceedings of Systems
    and Machine Learning (SysML), 2018.'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[79] A. Jayarajan, J. Wei, G. Gibson, A. Fedorova, 和 G. Pekhimenko. 基于优先级的参数传播用于分布式深度神经网络训练.
    见于系统与机器学习（SysML）会议论文集, 2018.'
- en: '[80] D. Jhunjhunwala, A. Gadhikar, G. Joshi, and Y. C. Eldar. Adaptive quantization
    of model updates for communication-efficient federated learning. In ICASSP. IEEE,
    2021.'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[80] D. Jhunjhunwala, A. Gadhikar, G. Joshi, 和 Y. C. Eldar. 针对通信效率的联邦学习的自适应量化模型更新.
    见于 ICASSP. IEEE, 2021.'
- en: '[81] X. Jia, S. Song, S. Shi, W. He, Y. Wang, H. Rong, F. Zhou, L. Xie, Z. Guo,
    Y. Yang, L. Yu, T. Chen, G. Hu, and X. Chu. Highly scalable deep learning training
    system with mixed-precision: Training ImageNet in four minutes. In Proc. of Workshop
    on Systems for ML and Open Source Software, collocated with NeurIPS 2018, 2018.'
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[81] X. Jia, S. Song, S. Shi, W. He, Y. Wang, H. Rong, F. Zhou, L. Xie, Z.
    Guo, Y. Yang, L. Yu, T. Chen, G. Hu, 和 X. Chu. 高度可扩展的深度学习训练系统与混合精度：在四分钟内训练 ImageNet.
    见于系统与机器学习（SysML）研讨会论文集，与 NeurIPS 2018 同时举行, 2018.'
- en: '[82] P. Jiang and G. Agrawal. A linear speedup analysis of distributed deep
    learning with sparse and quantized communication. In NeurIPS, 2018.'
  id: totrans-494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[82] P. Jiang 和 G. Agrawal. 基于稀疏和量化通信的分布式深度学习线性加速分析. 见于 NeurIPS, 2018.'
- en: '[83] Z. Jiang, W. Wang, B. Li, and B. Li. Pisces: efficient federated learning
    via guided asynchronous training. In SoCC, 2022.'
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[83] Z. Jiang, W. Wang, B. Li, 和 B. Li. Pisces：通过引导的异步训练实现高效的联邦学习. 见于 SoCC,
    2022.'
- en: '[84] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
    S. Bates, S. Bhatia, N. Boden, A. Borchers, et al. In-datacenter performance analysis
    of a tensor processing unit. In ISCA, 2017.'
  id: totrans-496
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[84] N. P. Jouppi, C. Young, N. Patil, D. Patterson, G. Agrawal, R. Bajwa,
    S. Bates, S. Bhatia, N. Boden, A. Borchers, 等. 在数据中心内的张量处理单元性能分析. 见于 ISCA, 2017.'
- en: '[85] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
    K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, et al. Advances and open problems
    in federated learning. arXiv preprint arXiv:1912.04977, 2019.'
  id: totrans-497
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[85] P. Kairouz, H. B. McMahan, B. Avent, A. Bellet, M. Bennis, A. N. Bhagoji,
    K. Bonawitz, Z. Charles, G. Cormode, R. Cummings, 等等. 联邦学习中的进展与未解问题。arXiv 预印本
    arXiv:1912.04977, 2019。'
- en: '[86] S. P. Karimireddy, Q. Rebjock, S. Stich, and M. Jaggi. Error feedback
    fixes SignSGD and other gradient compression schemes. In ICML, 2019.'
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[86] S. P. Karimireddy, Q. Rebjock, S. Stich, 和 M. Jaggi. 错误反馈修复 SignSGD 和其他梯度压缩方案。在
    ICML，2019。'
- en: '[87] D. Kempe, A. Dobra, and J. Gehrke. Gossip-based computation of aggregate
    information. In 44th Annual IEEE Symposium on Foundations of Computer Science,
    2003\. Proceedings., pages 482–491, Oct 2003.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[87] D. Kempe, A. Dobra, 和 J. Gehrke. 基于 Gossip 的汇总信息计算。在第44届年度 IEEE 计算机科学基础研讨会，2003。会议录，第482–491页，2003年10月。'
- en: '[88] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, and P. T. P. Tang.
    On large-batch training for deep learning: Generalization gap and sharp minima.
    In ICLR, 2017.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[88] N. S. Keskar, D. Mudigere, J. Nocedal, M. Smelyanskiy, 和 P. T. P. Tang.
    大批量训练深度学习：泛化差距与锐利最小值。在 ICLR，2017。'
- en: '[89] A. Koloskova, T. Lin, S. U. Stich, and M. Jaggi. Decentralized deep learning
    with arbitrary communication compression. ArXiv, abs/1907.09356, 2019.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[89] A. Koloskova, T. Lin, S. U. Stich, 和 M. Jaggi. 带有任意通信压缩的去中心化深度学习。ArXiv,
    abs/1907.09356, 2019。'
- en: '[90] A. Koloskova, S. Stich, and M. Jaggi. Decentralized stochastic optimization
    and gossip algorithms with compressed communication. In ICML, 2019.'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[90] A. Koloskova, S. Stich, 和 M. Jaggi. 使用压缩通信的去中心化随机优化和 Gossip 算法。在 ICML，2019。'
- en: '[91] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, and D. Bacon.
    Federated learning: Strategies for improving communication efficiency. In NeurIPS
    Workshop on Private Multi-Party Machine Learning, 2016.'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[91] J. Konečný, H. B. McMahan, F. X. Yu, P. Richtarik, A. T. Suresh, 和 D.
    Bacon. 联邦学习：提高通信效率的策略。在 NeurIPS Workshop on Private Multi-Party Machine Learning，2016。'
- en: '[92] J. Konečný and P. Richtárik. Randomized distributed mean estimation: Accuracy
    vs. communication. Frontiers in Applied Mathematics and Statistics, 4:62, 2018.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[92] J. Konečný 和 P. Richtárik. 随机分布式均值估计：准确性与通信。应用数学与统计前沿，4:62，2018。'
- en: '[93] R. Krizanc and A. Saarimaki. Bulk synchronous parallel: practical experience
    with a model for parallel computing. In Proceedings of the 1996 Conference on
    Parallel Architectures and Compilation Technique, pages 208–217, Oct 1996.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[93] R. Krizanc 和 A. Saarimaki. 批量同步并行：平行计算模型的实践经验。在1996年并行架构与编译技术会议论文集，第208–217页，1996年10月。'
- en: '[94] A. Krizhevsky. One weird trick for parallelizing convolutional neural
    networks. ArXiv, abs/1404.5997, 2014.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[94] A. Krizhevsky. 平行化卷积神经网络的一个奇怪技巧。ArXiv, abs/1404.5997, 2014。'
- en: '[95] A. Krizhevsky, V. Nair, and G. Hinton. Cifar-10 (canadian institute for
    advanced research). URL http://www.cs.toronto.edu/kriz/cifar.html, 2010.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[95] A. Krizhevsky, V. Nair, 和 G. Hinton. Cifar-10（加拿大高级研究院）。网址 http://www.cs.toronto.edu/kriz/cifar.html,
    2010。'
- en: '[96] G. Lan, S. Lee, and Y. Zhou. Communication-efficient algorithms for decentralized
    and stochastic optimization. CoRR, abs/1701.03961, 2017.'
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[96] G. Lan, S. Lee, 和 Y. Zhou. 用于去中心化和随机优化的通信高效算法。CoRR, abs/1701.03961, 2017。'
- en: '[97] J. Langford, L. Li, and T. Zhang. Sparse online learning via truncated
    gradient. J. Mach. Learn. Res., 2009.'
  id: totrans-509
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[97] J. Langford, L. Li, 和 T. Zhang. 通过截断梯度进行稀疏在线学习。J. Mach. Learn. Res., 2009。'
- en: '[98] T. T. Lau, J. Zeng, B. Wu, and Y. Yao. A proximal block coordinate descent
    algorithm for deep neural network training. In ICLR, 2018.'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[98] T. T. Lau, J. Zeng, B. Wu, 和 Y. Yao. 用于深度神经网络训练的近端块坐标下降算法。在 ICLR，2018。'
- en: '[99] W. Lee, Y. Lee, J. S. Jeong, G. Yu, J. Y. Kim, H. J. Park, B. Jeon, W. Song,
    G. Kim, M. Weimer, B. Cho, and B. Chun. Automating system configuration of distributed
    machine learning. In ICDCS, 2019.'
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[99] W. Lee, Y. Lee, J. S. Jeong, G. Yu, J. Y. Kim, H. J. Park, B. Jeon, W.
    Song, G. Kim, M. Weimer, B. Cho, 和 B. Chun. 自动化分布式机器学习系统配置。在 ICDCS，2019。'
- en: '[100] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N. Shazeer,
    and Z. Chen. {GS}hard: Scaling giant models with conditional computation and automatic
    sharding. In ICLR, 2021.'
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[100] D. Lepikhin, H. Lee, Y. Xu, D. Chen, O. Firat, Y. Huang, M. Krikun, N.
    Shazeer, 和 Z. Chen. {GS}hard：通过条件计算和自动分片扩展巨型模型。在 ICLR，2021。'
- en: '[101] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski,
    J. Long, E. J. Shekita, and B.-Y. Su. Scaling distributed machine learning with
    the parameter server. In OSDI, 2014.'
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[101] M. Li, D. G. Andersen, J. W. Park, A. J. Smola, A. Ahmed, V. Josifovski,
    J. Long, E. J. Shekita, 和 B.-Y. Su. 通过参数服务器扩展分布式机器学习。在 OSDI，2014。'
- en: '[102] M. Li, D. G. Andersen, A. Smola, and K. Yu. Communication efficient distributed
    machine learning with the parameter server. In NeurIPS, 2014.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[102] M. Li, D. G. Andersen, A. Smola 和 K. Yu. 具有参数服务器的通信高效的分布式机器学习。见 NeurIPS,
    2014。'
- en: '[103] M. Li, L. Zhou, Z. Yang, A. Q. Li, F. Xia, D. G. Andersen, and A. J.
    Smola. Parameter server for distributed machine learning. In In Big Learning NeurIPS
    Workshop, 2013.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[103] M. Li, L. Zhou, Z. Yang, A. Q. Li, F. Xia, D. G. Andersen 和 A. J. Smola.
    分布式机器学习的参数服务器。见 Big Learning NeurIPS Workshop, 2013。'
- en: '[104] S. Li and T. Hoefler. Chimera: efficiently training large-scale neural
    networks with bidirectional pipelines. In SC, 2021.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[104] S. Li 和 T. Hoefler. Chimera：利用双向流水线高效训练大规模神经网络。见 SC, 2021。'
- en: '[105] S. Li and T. Hoefler. Near-optimal sparse allreduce for distributed deep
    learning. In Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice
    of Parallel Programming, pages 135–149, 2022.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[105] S. Li 和 T. Hoefler. 针对分布式深度学习的近似最优稀疏全减少。见第 27 届 ACM SIGPLAN 并行编程原则与实践研讨会论文集,
    页 135–149, 2022。'
- en: '[106] X. Li, B. Karimi, and P. Li. On distributed adaptive optimization with
    gradient compression. In ICLR, 2022.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[106] X. Li, B. Karimi 和 P. Li. 关于带梯度压缩的分布式自适应优化。见 ICLR, 2022。'
- en: '[107] X. Lian, Y. Huang, Y. Li, and J. Liu. Asynchronous parallel stochastic
    gradient for nonconvex optimization. In NeurIPS, 2015.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[107] X. Lian, Y. Huang, Y. Li 和 J. Liu. 用于非凸优化的异步并行随机梯度。见 NeurIPS, 2015。'
- en: '[108] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang, and J. Liu. Can decentralized
    algorithms outperform centralized algorithms? a case study for decentralized parallel
    stochastic gradient descent. In NeurIPS, 2017.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[108] X. Lian, C. Zhang, H. Zhang, C.-J. Hsieh, W. Zhang 和 J. Liu. 去中心化算法是否能优于中心化算法？以去中心化并行随机梯度下降法为例。见
    NeurIPS, 2017。'
- en: '[109] X. Lian, W. Zhang, C. Zhang, and J. Liu. Asynchronous decentralized parallel
    stochastic gradient descent. In ICML, 2018.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[109] X. Lian, W. Zhang, C. Zhang 和 J. Liu. 异步去中心化并行随机梯度下降法。见 ICML, 2018。'
- en: '[110] H. Lim, D. G. Andersen, and M. Kaminsky. 3lc: Lightweight and effective
    traffic compression for distributed machine learning. ArXiv, abs/1802.07389, 2019.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[110] H. Lim, D. G. Andersen 和 M. Kaminsky. 3lc：用于分布式机器学习的轻量级且高效的流量压缩。ArXiv,
    abs/1802.07389, 2019。'
- en: '[111] T. Lin, S. U. Stich, and M. Jaggi. Don’t use large mini-batches, use
    local sgd. ArXiv, abs/1808.07217, 2018.'
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[111] T. Lin, S. U. Stich 和 M. Jaggi. 不要使用大批量，使用本地 sgd。ArXiv, abs/1808.07217,
    2018。'
- en: '[112] T. Lin, S. U. Stich, K. K. Patel, and M. Jaggi. Don’t use large mini-batches,
    use local sgd. In ICLR, 2020.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[112] T. Lin, S. U. Stich, K. K. Patel 和 M. Jaggi. 不要使用大批量，使用本地 sgd。见 ICLR,
    2020。'
- en: '[113] Y. Lin, S. Han, H. Mao, Y. Wang, and B. Dally. Deep gradient compression:
    Reducing the communication bandwidth for distributed training. In ICLR, 2018.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[113] Y. Lin, S. Han, H. Mao, Y. Wang 和 B. Dally. 深度梯度压缩：减少分布式训练的通信带宽。见 ICLR,
    2018。'
- en: '[114] Lin Xiao and S. Boyd. Fast linear iterations for distributed averaging.
    In 42nd IEEE International Conference on Decision and Control, 2003.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[114] Lin Xiao 和 S. Boyd. 用于分布式平均的快速线性迭代。见第 42 届 IEEE 国际决策与控制会议, 2003。'
- en: '[115] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola, and J. M. Hellerstein.
    Distributed graphlab: A framework for machine learning and data mining in the
    cloud. Proc. VLDB Endow., 5(8):716–727, Apr. 2012.'
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[115] Y. Low, D. Bickson, J. Gonzalez, C. Guestrin, A. Kyrola 和 J. M. Hellerstein.
    分布式 GraphLab：一种用于云计算中机器学习和数据挖掘的框架。Proc. VLDB Endow., 5(8):716–727, 2012 年 4 月。'
- en: '[116] L. Luo, P. West, J. Nelson, A. Krishnamurthy, and L. Ceze. PLink: Efficient
    cloud-based training with topology-aware dynamic hierarchical aggregation. In
    MLSys, 2020.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[116] L. Luo, P. West, J. Nelson, A. Krishnamurthy 和 L. Ceze. PLink：基于云的高效训练，具有拓扑感知的动态分层聚合。见
    MLSys, 2020。'
- en: '[117] A. M Abdelmoniem, A. Elzanaty, M.-S. Alouini, and M. Canini. An efficient
    statistical-based gradient compression technique for distributed training systems.
    MLSys, 2021.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[117] A. M Abdelmoniem, A. Elzanaty, M.-S. Alouini 和 M. Canini. 一种高效的基于统计的梯度压缩技术，用于分布式训练系统。MLSys,
    2021。'
- en: '[118] D. G. A. M. Li and A. Smola. Distributed delayed proximal gradient methods.
    In In NeurIPS Workshop on Optimization for Machine Learning, Lake Tahoe, CA, 2013.'
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[118] D. G. A. M. Li 和 A. Smola. 分布式延迟近端梯度方法。见 NeurIPS 机器学习优化研讨会, Lake Tahoe,
    CA, 2013。'
- en: '[119] D. Mahajan, S. S. Keerthi, and S. Sundararajan. A distributed block coordinate
    descent method for training l1regularized linear classifiers. J. Mach. Learn.
    Res., 18(1):3167–3201, Jan. 2017.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[119] D. Mahajan, S. S. Keerthi 和 S. Sundararajan. 一种用于训练 l1 正则化线性分类器的分布式块坐标下降法。J.
    Mach. Learn. Res., 18(1):3167–3201, 2017 年 1 月。'
- en: '[120] J. Martens and R. Grosse. Optimizing neural networks with kronecker-factored
    approximate curvature. In International conference on machine learning, pages
    2408–2417\. PMLR, 2015.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[120] J. Martens 和 R. Grosse. 使用 Kronecker 分解近似曲率优化神经网络。在国际机器学习会议上，页码 2408–2417。PMLR，2015年。'
- en: '[121] R. Mayer, C. Mayer, and L. Laich. The tensorflow partitioning and scheduling
    problem: It’s the critical path! In Proceedings of the 1st Workshop on Distributed
    Infrastructures for Deep Learning, DIDL ’17, New York, NY, USA, 2017.'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[121] R. Mayer, C. Mayer, 和 L. Laich. Tensorflow 分区和调度问题：关键路径！在第1届深度学习分布式基础设施研讨会（DIDL
    ''17）上，纽约，NY，USA，2017年。'
- en: '[122] R. Mcdonald, M. Mohri, N. Silberman, D. Walker, and G. S. Mann. Efficient
    large-scale distributed training of conditional maximum entropy models. In NeurIPS.
    2009.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[122] R. Mcdonald, M. Mohri, N. Silberman, D. Walker, 和 G. S. Mann. 高效的大规模分布式条件最大熵模型训练。在
    NeurIPS 会议上，2009年。'
- en: '[123] R. T. McDonald, K. B. Hall, and G. Mann. Distributed training strategies
    for the structured perceptron. In HLT-NAACL, 2010.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123] R. T. McDonald, K. B. Hall, 和 G. Mann. 结构感知机的分布式训练策略。在 HLT-NAACL 会议上，2010年。'
- en: '[124] N. McGlohon and S. Patterson. Distributed semi-stochastic optimization
    with quantization refinement. In 2016 American Control Conference (ACC), pages
    7159–7164, July 2016.'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[124] N. McGlohon 和 S. Patterson. 带量化精化的分布式半随机优化。在 2016 年美国控制会议（ACC）上，页码 7159–7164，2016年7月。'
- en: '[125] B. McMahan, E. Moore, D. Ramage, S. Hampson, and B. A. y Arcas. Communication-efficient
    learning of deep networks from decentralized data. In AISTATS, pages 1273–1282,
    2017.'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[125] B. McMahan, E. Moore, D. Ramage, S. Hampson, 和 B. A. y Arcas. 从去中心化数据中进行通信高效的深度网络学习。在
    AISTATS 会议上，页码 1273–1282，2017年。'
- en: '[126] Q. Meng, W. Chen, J. Yu, T. Wang, Z.-M. Ma, and T.-Y. Liu. Asynchronous
    accelerated stochastic gradient descent. In IJCAI, 2016.'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[126] Q. Meng, W. Chen, J. Yu, T. Wang, Z.-M. Ma, 和 T.-Y. Liu. 异步加速随机梯度下降。在
    IJCAI 会议上，2016年。'
- en: '[127] H. Mikami, H. Suganuma, Y. Tanaka, Y. Kageyama, et al. Massively distributed
    SGD: ImageNet/ResNet-50 training in a flash. arXiv preprint arXiv:1811.05233,
    2018.'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[127] H. Mikami, H. Suganuma, Y. Tanaka, Y. Kageyama 等. 大规模分布式 SGD：瞬时 ImageNet/ResNet-50
    训练。arXiv 预印本 arXiv:1811.05233，2018年。'
- en: '[128] A. Mirhoseini, H. Pham, Q. V. Le, B. Steiner, R. Larsen, Y. Zhou, N. Kumar,
    M. Norouzi, S. Bengio, and J. Dean. Device placement optimization with reinforcement
    learning. In ICML, 2017.'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[128] A. Mirhoseini, H. Pham, Q. V. Le, B. Steiner, R. Larsen, Y. Zhou, N.
    Kumar, M. Norouzi, S. Bengio, 和 J. Dean. 使用强化学习进行设备放置优化。在 ICML 会议上，2017年。'
- en: '[129] K. Mishchenko, F. Bach, M. Even, and B. E. Woodworth. Asynchronous sgd
    beats minibatch sgd under arbitrary delays. In NeurIPS, 2022.'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[129] K. Mishchenko, F. Bach, M. Even, 和 B. E. Woodworth. 异步 SGD 在任意延迟下优于小批量
    SGD。在 NeurIPS 会议上，2022年。'
- en: '[130] K. Mishchenko, E. A. Gorbunov, M. Takác, and P. Richtárik. Distributed
    learning with compressed gradient differences. ArXiv, abs/1901.09269, 2019.'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[130] K. Mishchenko, E. A. Gorbunov, M. Takác, 和 P. Richtárik. 具有压缩梯度差异的分布式学习。ArXiv，abs/1901.09269，2019年。'
- en: '[131] K. Mishchenko, F. Hanzely, and P. Richtárik. 99% of parallel optimization
    is inevitably a waste of time. ArXiv, abs/1901.09437, 2019.'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[131] K. Mishchenko, F. Hanzely, 和 P. Richtárik. 99% 的并行优化不可避免地是时间浪费。ArXiv，abs/1901.09437，2019年。'
- en: '[132] P. Moritz, R. Nishihara, I. Stoica, and M. I. Jordan. Sparknet: Training
    deep networks in spark. In ICLR, 2016.'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[132] P. Moritz, R. Nishihara, I. Stoica, 和 M. I. Jordan. Sparknet：在 Spark
    中训练深度网络。在 ICLR 会议上，2016年。'
- en: '[133] J. F. C. Mota, J. M. F. Xavier, P. M. Q. Aguiar, and M. Püschel. D-admm:
    A communication-efficient distributed algorithm for separable optimization. IEEE
    Transactions on Signal Processing, 61(10):2718–2723, May 2013.'
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[133] J. F. C. Mota, J. M. F. Xavier, P. M. Q. Aguiar, 和 M. Püschel. D-admm：一种通信高效的分布式可分优化算法。IEEE
    信号处理学报，61(10):2718–2723，2013年5月。'
- en: '[134] A. Nedic and A. Ozdaglar. Distributed subgradient methods for multi-agent
    optimization. IEEE Transactions on Automatic Control, 54(1):48–61, Jan 2009.'
  id: totrans-546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[134] A. Nedic 和 A. Ozdaglar. 多代理优化的分布式次梯度方法。IEEE 自动控制学报，54(1):48–61，2009年1月。'
- en: '[135] A. Nedić and A. Olshevsky. Stochastic gradient-push for strongly convex
    functions on time-varying directed graphs. IEEE Transactions on Automatic Control,
    61(12):3936–3947, Dec 2016.'
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[135] A. Nedić 和 A. Olshevsky. 针对时间变化的有向图上的强凸函数的随机梯度推送。IEEE 自动控制学报，61(12):3936–3947，2016年12月。'
- en: '[136] A. Nedić, A. Olshevsky, and M. G. Rabbat. Network topology and communication-computation
    tradeoffs in decentralized optimization. Proceedings of the IEEE, 106(5):953–976,
    May 2018.'
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[136] A. Nedić, A. Olshevsky, 和 M. G. Rabbat. 网络拓扑和去中心化优化中的通信-计算权衡。IEEE 会议录，106(5):953–976，2018年5月。'
- en: '[137] F. Niu, B. Recht, C. Re, and S. J. Wright. Hogwild!: A lock-free approach
    to parallelizing stochastic gradient descent. In NeurIPS, 2011.'
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[137] F. Niu, B. Recht, C. Re 和 S. J. Wright. Hogwild!: 一种无锁的并行随机梯度下降方法。在NeurIPS，2011年。'
- en: '[138] R. Olfati-Saber, J. A. Fax, and R. M. Murray. Consensus and cooperation
    in networked multi-agent systems. Proceedings of the IEEE, 95(1):215–233, Jan
    2007.'
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[138] R. Olfati-Saber, J. A. Fax 和 R. M. Murray. 网络化多代理系统中的共识与合作。IEEE汇刊，95(1):215–233，2007年1月。'
- en: '[139] B. C. Ooi, K.-L. Tan, S. Wang, W. Wang, Q. Cai, G. Chen, J. Gao, Z. Luo,
    A. K. Tung, Y. Wang, Z. Xie, M. Zhang, and K. Zheng. Singa: A distributed deep
    learning platform. In ACM International Conference on Multimedia, 2015.'
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[139] B. C. Ooi, K.-L. Tan, S. Wang, W. Wang, Q. Cai, G. Chen, J. Gao, Z. Luo,
    A. K. Tung, Y. Wang, Z. Xie, M. Zhang 和 K. Zheng. Singa: 一个分布式深度学习平台。在ACM国际多媒体会议，2015年。'
- en: '[140] F. N. I. P. H. Jin, Q. Yuan and K. Keutzer. How to scale distributed
    deep learning? In ML Systems Workshop at NeurIPS, 2016.'
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[140] F. N. I. P. H. Jin, Q. Yuan 和 K. Keutzer. 如何扩展分布式深度学习？在NeurIPS ML系统研讨会，2016年。'
- en: '[141] Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu, and C. Guo. A
    generic communication scheduler for distributed DNN training acceleration. In
    Proceedings of the 27th ACM Symposium on Operating Systems Principles, 2019.'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[141] Y. Peng, Y. Zhu, Y. Chen, Y. Bao, B. Yi, C. Lan, C. Wu 和 C. Guo. 用于加速分布式DNN训练的通用通信调度器。在第27届ACM操作系统原理研讨会论文集中，2019年。'
- en: '[142] D. Peteiro-Barral and B. Guijarro-Berdiñas. A survey of methods for distributed
    machine learning. Progress in Artificial Intelligence, 2:1–11, 2013.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[142] D. Peteiro-Barral 和 B. Guijarro-Berdiñas. 分布式机器学习方法的综述。人工智能进展，2:1–11，2013年。'
- en: '[143] R. Power and J. Li. Piccolo: Building fast, distributed programs with
    partitioned tables. In OSDI, 2010.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[143] R. Power 和 J. Li. Piccolo: 用于构建快速分布式程序的分区表。在OSDI，2010年。'
- en: '[144] Y. Pu, M. N. Zeilinger, and C. N. Jones. Quantization design for distributed
    optimization. IEEE Transactions on Automatic Control, 62(5):2107–2120, May 2017.'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[144] Y. Pu, M. N. Zeilinger 和 C. N. Jones. 用于分布式优化的量化设计。IEEE自动控制汇刊，62(5):2107–2120，2017年5月。'
- en: '[145] N. Qian. On the momentum term in gradient descent learning algorithms.
    Neural Netw., 12(1):145–151, Jan. 1999.'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[145] N. Qian. 关于梯度下降学习算法中的动量项。神经网络，12(1):145–151，1999年1月。'
- en: '[146] Z. Qu, P. Richtárik, and T. Zhang. Quartz: Randomized dual coordinate
    ascent with arbitrary sampling. In NeurIPS, 2015.'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[146] Z. Qu, P. Richtárik 和 T. Zhang. Quartz: 随机化双重坐标上升与任意抽样。在NeurIPS，2015年。'
- en: '[147] M. Rabbat. Multi-agent mirror descent for decentralized stochastic optimization.
    In 2015 IEEE 6th International Workshop on CAMSAP, 2015.'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[147] M. Rabbat. 用于分散随机优化的多代理镜像下降方法。在2015 IEEE第6届国际CAMSAP研讨会，2015年。'
- en: '[148] R. Rabenseifner. Optimization of collective reduction operations. In
    M. Bubak, G. D. van Albada, P. M. A. Sloot, and J. Dongarra, editors, Computational
    Science - ICCS 2004, 2004.'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[148] R. Rabenseifner. 集体规约操作的优化。在M. Bubak, G. D. van Albada, P. M. A. Sloot
    和 J. Dongarra编辑的《计算科学 - ICCS 2004》中，2004年。'
- en: '[149] S. S. Ram, A. Nedic, and V. V. Veeravalli. Distributed stochastic subgradient
    projection algorithms for convex optimization. Journal of Optimization Theory
    and Applications, 147:516–545, 2008.'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[149] S. S. Ram, A. Nedic 和 V. V. Veeravalli. 用于凸优化的分布式随机子梯度投影算法。优化理论与应用期刊，147:516–545，2008年。'
- en: '[150] T. Reinartz. Focusing Solutions for Data Mining: Analytical Studies and
    Experimental Results in Real-world Domains. Springer-Verlag, Berlin, Heidelberg,
    1999.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[150] T. Reinartz. 数据挖掘的聚焦解决方案：真实世界领域的分析研究和实验结果。Springer-Verlag，柏林，海德堡，1999年。'
- en: '[151] A. Reisizadeh, H. Taheri, A. Mokhtari, H. Hassani, and R. Pedarsani.
    Robust and communication-efficient collaborative learning. In NeurIPS. 2019.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[151] A. Reisizadeh, H. Taheri, A. Mokhtari, H. Hassani 和 R. Pedarsani. 鲁棒且通信高效的协作学习。在NeurIPS，2019年。'
- en: '[152] J. Ren, X. Li, and J. Haupt. Communication-efficient distributed optimization
    for sparse learning via two-way truncation. In 2017 IEEE 7th International Workshop
    on CAMSAP, 2017.'
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[152] J. Ren, X. Li 和 J. Haupt. 通过双向截断实现稀疏学习的通信高效分布式优化。在2017 IEEE第7届国际CAMSAP研讨会，2017年。'
- en: '[153] P. Richtárik, I. Sokolov, and I. Fatkhullin. Ef21: A new, simpler, theoretically
    better, and practically faster error feedback. NeurIPS, 2021.'
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[153] P. Richtárik, I. Sokolov 和 I. Fatkhullin. Ef21: 一种新的、更简单、理论上更佳且实际更快的误差反馈。在NeurIPS，2021年。'
- en: '[154] P. Richtarik, I. Sokolov, E. Gasanov, I. Fatkhullin, Z. Li, and E. Gorbunov.
    3PC: Three point compressors for communication-efficient distributed training
    and a better theory for lazy aggregation. In ICML, 2022.'
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[154] P. Richtarik, I. Sokolov, E. Gasanov, I. Fatkhullin, Z. Li 和 E. Gorbunov.
    3PC: 用于通信高效分布式训练的三点压缩器及更好的懒惰聚合理论。在ICML，2022年。'
- en: '[155] L. Roberts. Picture coding using pseudo-random noise. IRE Transactions
    on Information Theory, 8(2):145–154, February 1962.'
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[155] L. Roberts. 使用伪随机噪声的图像编码。IRE 信息理论汇刊，8(2):145–154，1962 年 2 月。'
- en: '[156] J. Romero, J. Yin, N. Laanait, B. Xie, M. T. Young, S. Treichler, V. Starchenko,
    A. Borisevich, A. Sergeev, and M. Matheson. Accelerating collective communication
    in data parallel training across deep learning frameworks. In NSDI, 2022.'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[156] J. Romero, J. Yin, N. Laanait, B. Xie, M. T. Young, S. Treichler, V.
    Starchenko, A. Borisevich, A. Sergeev 和 M. Matheson. 加速跨深度学习框架的数据并行训练中的集体通信。发表于
    NSDI，2022 年。'
- en: '[157] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg, and L. Fei-Fei. Imagenet large
    scale visual recognition challenge. Int. J. Comput. Vision, 2015.'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[157] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Z. Huang,
    A. Karpathy, A. Khosla, M. Bernstein, A. C. Berg 和 L. Fei-Fei. ImageNet 大规模视觉识别挑战。国际计算机视觉杂志，2015
    年。'
- en: '[158] C. D. Sa, C. Zhang, K. Olukotun, and C. Ré. Taming the wild: A unified
    analysis of hogwild-style algorithms. NeurIPS, 2015.'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[158] C. D. Sa, C. Zhang, K. Olukotun 和 C. Ré. 驯服野性: 对 hogwild 风格算法的统一分析。NeurIPS，2015
    年。'
- en: '[159] P. Sanders, J. Speck, and J. L. Träff. Two-tree algorithms for full bandwidth
    broadcast, reduction and scan. Parallel Computing, 35(12):581–594, 2009.'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[159] P. Sanders, J. Speck 和 J. L. Träff. 用于全带宽广播、归约和扫描的双树算法。并行计算，35(12):581–594，2009
    年。'
- en: '[160] A. Sapio, M. Canini, C.-Y. Ho, J. Nelson, P. Kalnis, C. Kim, A. Krishnamurthy,
    M. Moshref, D. Ports, and P. Richtarik. Scaling distributed machine learning with
    In-Network aggregation. In NSDI 21, pages 785–808, 2021.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[160] A. Sapio, M. Canini, C.-Y. Ho, J. Nelson, P. Kalnis, C. Kim, A. Krishnamurthy,
    M. Moshref, D. Ports 和 P. Richtarik. 通过 In-Network 聚合扩展分布式机器学习。发表于 NSDI 21，页码
    785–808，2021 年。'
- en: '[161] S. Sarvotham, R. Riedi, and R. Baraniuk. Connection-level analysis and
    modeling of network traffic. In Proceedings of ACM SIGCOMM Workshop on IMW. ACM,
    2001.'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[161] S. Sarvotham, R. Riedi 和 R. Baraniuk. 网络流量的连接级分析和建模。发表于 ACM SIGCOMM IMW
    研讨会论文集。ACM，2001 年。'
- en: '[162] F. Sattler, S. Wiedemann, K. Müller, and W. Samek. Robust and communication-efficient
    federated learning from non-iid data. CoRR, abs/1903.02891, 2019.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[162] F. Sattler, S. Wiedemann, K. Müller 和 W. Samek. 从非独立同分布数据中进行鲁棒和通信高效的联邦学习。CoRR,
    abs/1903.02891，2019 年。'
- en: '[163] F. Sattler, S. Wiedemann, K.-R. Müller, and W. Samek. Sparse binary compression:
    Towards distributed deep learning with minimal communication. IJCN, 2018.'
  id: totrans-575
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[163] F. Sattler, S. Wiedemann, K.-R. Müller 和 W. Samek. 稀疏二进制压缩: 朝着最小通信的分布式深度学习迈进。IJCN，2018
    年。'
- en: '[164] K. Scaman, F. Bach, S. Bubeck, Y. T. Lee, and L. Massoulié. Optimal algorithms
    for non-smooth distributed optimization in networks. In NeurIPS, 2018.'
  id: totrans-576
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[164] K. Scaman, F. Bach, S. Bubeck, Y. T. Lee 和 L. Massoulié. 网络中非平滑分布式优化的最优算法。发表于
    NeurIPS，2018 年。'
- en: '[165] L. Schenato and G. Gamba. A distributed consensus protocol for clock
    synchronization in wireless sensor network. In 2007 46th IEEE Conference on Decision
    and Control, pages 2289–2294, Dec 2007.'
  id: totrans-577
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[165] L. Schenato 和 G. Gamba. 用于无线传感器网络时钟同步的分布式共识协议。发表于 2007 年第 46 届 IEEE 决策与控制会议，页码
    2289–2294，2007 年 12 月。'
- en: '[166] K. Seaman, F. Bach, S. Bubeck, Y. T. Lee, and L. Massoulié. Optimal algorithms
    for smooth and strongly convex distributed optimization in networks. In ICML,
    2017.'
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[166] K. Seaman, F. Bach, S. Bubeck, Y. T. Lee 和 L. Massoulié. 平滑和强凸分布式优化的最优算法。发表于
    ICML，2017 年。'
- en: '[167] F. Seide, H. Fu, J. Droppo, G. Li, and D. Yu. 1-bit stochastic gradient
    descent and its application to data-parallel distributed training of speech DNNs.
    In INTERSPEECH, 2014.'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[167] F. Seide, H. Fu, J. Droppo, G. Li 和 D. Yu. 1-bit 随机梯度下降及其在数据并行分布式训练中的应用。发表于
    INTERSPEECH，2014 年。'
- en: '[168] C. J. Shallue, J. Lee, J. M. Antognini, J. Sohl-Dickstein, R. Frostig,
    and G. E. Dahl. Measuring the effects of data parallelism on neural network training.
    CoRR, abs/1811.03600, 2018.'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[168] C. J. Shallue, J. Lee, J. M. Antognini, J. Sohl-Dickstein, R. Frostig
    和 G. E. Dahl. 测量数据并行性对神经网络训练的影响。CoRR, abs/1811.03600，2018 年。'
- en: '[169] S. Shi, X. Chu, K. C. Cheung, and S. See. Understanding top-k sparsification
    in distributed deep learning. arXiv preprint arXiv:1911.08772, 2019.'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[169] S. Shi, X. Chu, K. C. Cheung 和 S. See. 理解分布式深度学习中的 top-k 稀疏化。arXiv 预印本
    arXiv:1911.08772，2019 年。'
- en: '[170] S. Shi, X. Chu, and B. Li. MG-WFBP: Efficient data communication for
    distributed synchronous SGD algorithms. In IEEE INFOCOM, pages 172–180\. IEEE,
    2019.'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[170] S. Shi, X. Chu 和 B. Li. MG-WFBP: 高效的数据通信用于分布式同步 SGD 算法。发表于 IEEE INFOCOM，页码
    172–180。IEEE，2019 年。'
- en: '[171] S. Shi, X. Chu, and B. Li. MG-WFBP: Merging gradients wisely for efficient
    communication in distributed deep learning. TPDS, 2021.'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[171] S. Shi, X. Chu 和 B. Li. MG-WFBP: 明智地合并梯度以实现分布式深度学习中的高效通信。TPDS，2021 年。'
- en: '[172] S. Shi, X. Pan, X. Chu, and B. Li. PipeMoE: Accelerating mixture-of-experts
    through adaptive pipelining. In IEEE INFOCOM, 2023.'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[172] S. Shi, X. Pan, X. Chu, 和 B. Li. PipeMoE：通过自适应流水线加速专家混合。收录于 IEEE INFOCOM,
    2023。'
- en: '[173] S. Shi, Z. Tang, X. Chu, C. Liu, W. Wang, and B. Li. A quantitative survey
    of communication optimizations in distributed deep learning. IEEE Network, 35(3):230–237,
    2020.'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[173] S. Shi, Z. Tang, X. Chu, C. Liu, W. Wang, 和 B. Li. 分布式深度学习中通信优化的定量调查。IEEE
    Network, 35(3):230–237, 2020。'
- en: '[174] S. Shi, Z. Tang, Q. Wang, K. Zhao, and X. Chu. Layer-wise adaptive gradient
    sparsification for distributed deep learning with convergence guarantees. In ECAI,
    2020.'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[174] S. Shi, Z. Tang, Q. Wang, K. Zhao, 和 X. Chu. 带收敛保证的分布式深度学习层级自适应梯度稀疏化。收录于
    ECAI, 2020。'
- en: '[175] S. Shi, Q. Wang, and X. Chu. Performance modeling and evaluation of distributed
    deep learning frameworks on gpus. In 2018 IEEE 4th Intl Conf on Big Data Intelligence
    and Computing, pages 949–957\. IEEE, 2018.'
  id: totrans-587
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[175] S. Shi, Q. Wang, 和 X. Chu. 基于 GPU 的分布式深度学习框架的性能建模与评估。收录于 2018 IEEE 第四届大数据智能与计算国际会议，第949–957页。IEEE,
    2018。'
- en: '[176] S. Shi, Q. Wang, X. Chu, and B. Li. A DAG model of synchronous stochastic
    gradient descent in distributed deep learning. In ICPADS, 2018.'
  id: totrans-588
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[176] S. Shi, Q. Wang, X. Chu, 和 B. Li. 分布式深度学习中同步随机梯度下降的 DAG 模型。收录于 ICPADS,
    2018。'
- en: '[177] S. Shi, Q. Wang, X. Chu, B. Li, Y. Qin, R. Liu, and X. Zhao. Communication-efficient
    distributed deep learning with merged gradient sparsification on gpus. In IEEE
    INFOCOM, 2020.'
  id: totrans-589
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[177] S. Shi, Q. Wang, X. Chu, B. Li, Y. Qin, R. Liu, 和 X. Zhao. 具有合并梯度稀疏化的通信高效分布式深度学习，基于
    GPU。收录于 IEEE INFOCOM, 2020。'
- en: '[178] S. Shi, Q. Wang, P. Xu, and X. Chu. Benchmarking state-of-the-art deep
    learning software tools. In 2016 7th International Conference on Cloud Computing
    and Big Data (CCBD), pages 99–104\. IEEE, 2016.'
  id: totrans-590
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[178] S. Shi, Q. Wang, P. Xu, 和 X. Chu. 最先进深度学习软件工具的基准测试。收录于 2016 第七届国际云计算与大数据会议（CCBD），第99–104页。IEEE,
    2016。'
- en: '[179] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y. Wang, X. Huang, and X. Chu. A distributed
    synchronous SGD algorithm with global top-k sparsification for low bandwidth networks.
    In ICDCS, pages 2238–2247, 2019.'
  id: totrans-591
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[179] S. Shi, Q. Wang, K. Zhao, Z. Tang, Y. Wang, X. Huang, 和 X. Chu. 一种具有全局
    top-k 稀疏化的分布式同步 SGD 算法，适用于低带宽网络。收录于 ICDCS，第2238–2247页，2019。'
- en: '[180] S. Shi, L. Zhang, and B. Li. Accelerating distributed k-fac with smart
    parallelism of computing and communication tasks. In ICDCS, 2021.'
  id: totrans-592
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[180] S. Shi, L. Zhang, 和 B. Li. 通过计算和通信任务的智能并行加速分布式 k-fac。收录于 ICDCS, 2021。'
- en: '[181] S. Shi, K. Zhao, Q. Wang, Z. Tang, and X. Chu. A convergence analysis
    of distributed SGD with communication-efficient gradient sparsification. In IJCAI,
    2019.'
  id: totrans-593
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[181] S. Shi, K. Zhao, Q. Wang, Z. Tang, 和 X. Chu. 具有通信高效梯度稀疏化的分布式 SGD 收敛性分析。收录于
    IJCAI, 2019。'
- en: '[182] S. Shi, X. Zhou, S. Song, X. Wang, Z. Zhu, X. Huang, X. Jiang, F. Zhou,
    Z. Guo, L. Xie, et al. Towards scalable distributed training of deep learning
    on public cloud clusters. MLSys, 3:401–412, 2021.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[182] S. Shi, X. Zhou, S. Song, X. Wang, Z. Zhu, X. Huang, X. Jiang, F. Zhou,
    Z. Guo, L. Xie, 等. 面向公共云集群的可扩展深度学习分布式训练。MLSys, 3:401–412, 2021。'
- en: '[183] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, and B. Catanzaro.
    Megatron-lm: Training multi-billion parameter language models using model parallelism.
    arXiv preprint arXiv:1909.08053, 2019.'
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[183] M. Shoeybi, M. Patwary, R. Puri, P. LeGresley, J. Casper, 和 B. Catanzaro.
    Megatron-lm: 使用模型并行训练多十亿参数的语言模型。arXiv 预印本 arXiv:1909.08053, 2019。'
- en: '[184] H. Shokri Ghadikolaei, S. Stich, and M. Jaggi. Lena: Communication-efficient
    distributed learning with self-triggered gradient uploads. In AISTATS, 2021.'
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[184] H. Shokri Ghadikolaei, S. Stich, 和 M. Jaggi. Lena：具有自触发梯度上传的通信高效分布式学习。收录于
    AISTATS, 2021。'
- en: '[185] A. Smola and S. Narayanamurthy. An architecture for parallel topic models.
    In Proc. VLDB Endow., 2010.'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[185] A. Smola 和 S. Narayanamurthy. 并行主题模型的架构。收录于 Proc. VLDB Endow., 2010。'
- en: '[186] A. Spiridonoff, A. Olshevsky, and I. Paschalidis. Communication-efficient
    SGD: From local SGD to one-shot averaging. In A. Beygelzimer, Y. Dauphin, P. Liang,
    and J. W. Vaughan, editors, NeurIPS, 2021.'
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[186] A. Spiridonoff, A. Olshevsky, 和 I. Paschalidis. 通信高效的 SGD：从本地 SGD 到一次性平均。收录于
    A. Beygelzimer, Y. Dauphin, P. Liang, 和 J. W. Vaughan 编，NeurIPS, 2021。'
- en: '[187] S. U. Stich. Local SGD converges fast and communicates little. In ICLR,
    2019.'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[187] S. U. Stich. 本地 SGD 收敛快且通信少。收录于 ICLR, 2019。'
- en: '[188] S. U. Stich, J.-B. Cordonnier, and M. Jaggi. Sparsified SGD with memory.
    In NeurIPS, 2018.'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[188] S. U. Stich, J.-B. Cordonnier, 和 M. Jaggi. 带记忆的稀疏 SGD。收录于 NeurIPS, 2018。'
- en: '[189] N. Strom. A tonotopic artificial neural network architecture for phoneme
    probability estimation. In 1997 IEEE Workshop on Automatic Speech Recognition
    and Understanding Proceedings, pages 156–163, Dec 1997.'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[189] N. Strom. 一种用于音素概率估计的音调人工神经网络架构。发表于 1997 IEEE 自动语音识别与理解研讨会论文集，页面 156–163，1997
    年 12 月。'
- en: '[190] N. Strom. Scalable distributed DNN training using commodity GPU cloud
    computing. In INTERSPEECH, 2015.'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[190] N. Strom. 使用商品 GPU 云计算的可扩展分布式 DNN 训练。发表于 INTERSPEECH，2015 年。'
- en: '[191] N. Ström. Sparse connection and pruning in large dynamic artificial neural
    networks. In EUROSPEECH, 1997.'
  id: totrans-603
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[191] N. Ström. 大型动态人工神经网络中的稀疏连接与剪枝。发表于 EUROSPEECH，1997 年。'
- en: '[192] B. Sudharsan, D. Sheth, S. Arya, F. Rollo, P. Yadav, P. Patel, J. G.
    Breslin, and M. I. Ali. Elasticl: Elastic quantization for communication efficient
    collaborative learning in iot. In SenSys, 2021.'
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[192] B. Sudharsan, D. Sheth, S. Arya, F. Rollo, P. Yadav, P. Patel, J. G.
    Breslin, 和 M. I. Ali. Elasticl：在物联网中用于通信高效协作学习的弹性量化。发表于 SenSys，2021 年。'
- en: '[193] N. Sukhija, M. Tatineni, N. Brown, M. V. Moer, P. Rodriguez, and S. Callicott.
    Topic modeling and visualization for big data in social sciences. In 2016 Intl
    IEEE Conferences on Ubiquitous Intelligence Computing, Advanced and Trusted Computing,
    Scalable Computing and Communications, Cloud and Big Data Computing, Internet
    of People, and Smart World Congress (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld), 2016.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[193] N. Sukhija, M. Tatineni, N. Brown, M. V. Moer, P. Rodriguez, 和 S. Callicott.
    社会科学中大数据的主题建模与可视化。发表于 2016 年国际 IEEE 智能计算、先进与可信计算、可扩展计算与通信、云计算与大数据计算、互联网人民和智能世界大会
    (UIC/ATC/ScalCom/CBDCom/IoP/SmartWorld)，2016 年。'
- en: '[194] C. Sun, A. Shrivastava, S. Singh, and A. Gupta. Revisiting unreasonable
    effectiveness of data in deep learning era. In ICCV, 2017.'
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[194] C. Sun, A. Shrivastava, S. Singh, 和 A. Gupta. 重新审视深度学习时代数据的不合理有效性。发表于
    ICCV，2017 年。'
- en: '[195] J. Sun, T. Chen, G. Giannakis, and Z. Yang. Communication-efficient distributed
    learning via lazily aggregated quantized gradients. In NeurIPS. 2019.'
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[195] J. Sun, T. Chen, G. Giannakis, 和 Z. Yang. 通过懒惰聚合量化梯度的通信高效分布式学习。发表于 NeurIPS，2019
    年。'
- en: '[196] T. Sun, R. Hannah, and W. Yin. Asynchronous coordinate descent under
    more realistic assumption. In NeurIPS, 2017.'
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[196] T. Sun, R. Hannah, 和 W. Yin. 在更现实的假设下的异步坐标下降。发表于 NeurIPS，2017 年。'
- en: '[197] X. Sun, X. Ren, S. Ma, and H. Wang. meProp: Sparsified back propagation
    for accelerated deep learning with reduced overfitting. In ICML, 2017.'
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[197] X. Sun, X. Ren, S. Ma, 和 H. Wang. meProp：用于加速深度学习的稀疏反向传播，减少过拟合。发表于 ICML，2017
    年。'
- en: '[198] A. T. Suresh, F. X. Yu, S. Kumar, and H. B. McMahan. Distributed mean
    estimation with limited communication. In ICML, 2017.'
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[198] A. T. Suresh, F. X. Yu, S. Kumar, 和 H. B. McMahan. 有限通信下的分布式均值估计。发表于
    ICML，2017 年。'
- en: '[199] V. Sze, Y.-H. Chen, T.-J. Yang, and J. S. Emer. Efficient processing
    of deep neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329,
    2017.'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[199] V. Sze, Y.-H. Chen, T.-J. Yang, 和 J. S. Emer. 深度神经网络的高效处理：教程与综述。IEEE
    会议录，105(12):2295–2329，2017 年。'
- en: '[200] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu,
    C. Zhang, and Y. He. 1-bit adam: Communication efficient large-scale training
    with adam’s convergence speed. In ICML, 2021.'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[200] H. Tang, S. Gan, A. A. Awan, S. Rajbhandari, C. Li, X. Lian, J. Liu,
    C. Zhang, 和 Y. He. 1-bit adam：具有 adam 收敛速度的大规模通信高效训练。发表于 ICML，2021 年。'
- en: '[201] H. Tang, S. Gan, C. Zhang, T. Zhang, and J. Liu. Communication compression
    for decentralized training. In NeurIPS, 2018.'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[201] H. Tang, S. Gan, C. Zhang, T. Zhang, 和 J. Liu. 去中心化训练的通信压缩。发表于 NeurIPS，2018
    年。'
- en: '[202] H. Tang, X. Lian, M. Yan, C. Zhang, and J. Liu. Decentralized training
    over decentralized data. In ICML, 2018.'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[202] H. Tang, X. Lian, M. Yan, C. Zhang, 和 J. Liu. 在分散数据上进行去中心化训练。发表于 ICML，2018
    年。'
- en: '[203] Z. Tang, X. Chu, R. Y. Ran, S. Lee, S. Shi, Y. Zhang, Y. Wang, A. Q.
    Liang, S. Avestimehr, and C. He. Fedml parrot: A scalable federated learning system
    via heterogeneity-aware scheduling on sequential and hierarchical training. arXiv
    preprint arXiv:2303.01778, 2023.'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[203] Z. Tang, X. Chu, R. Y. Ran, S. Lee, S. Shi, Y. Zhang, Y. Wang, A. Q.
    Liang, S. Avestimehr, 和 C. He. Fedml parrot：通过异质性感知调度进行顺序和层级训练的可扩展联邦学习系统。arXiv
    预印本 arXiv:2303.01778，2023 年。'
- en: '[204] Z. Tang, S. Shi, and X. Chu. Communication-efficient decentralized learning
    with sparsification and adaptive peer selection. arXiv preprint arXiv:2002.09692,
    2020.'
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[204] Z. Tang, S. Shi, 和 X. Chu. 通过稀疏化和自适应对等选择的通信高效去中心化学习。arXiv 预印本 arXiv:2002.09692，2020
    年。'
- en: '[205] Z. Tang, S. Shi, B. Li, and X. Chu. Gossipfl: A decentralized federated
    learning framework with sparsified and adaptive communication. IEEE Transactions
    on Parallel and Distributed Systems, 34(3):909–922, 2022.'
  id: totrans-617
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[205] Z. Tang, S. Shi, B. Li 和 X. Chu. Gossipfl: 一种具有稀疏和自适应通信的去中心化联邦学习框架。IEEE
    并行与分布式系统汇刊，34(3):909–922，2022年。'
- en: '[206] Z. Tang, Y. Zhang, S. Shi, X. He, B. Han, and X. Chu. Virtual homogeneity
    learning: Defending against data heterogeneity in federated learning. In International
    Conference on Machine Learning, pages 21111–21132\. PMLR, 2022.'
  id: totrans-618
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[206] Z. Tang, Y. Zhang, S. Shi, X. He, B. Han 和 X. Chu. 虚拟同质性学习：应对联邦学习中的数据异质性。在国际机器学习会议上，页码21111–21132。PMLR，2022年。'
- en: '[207] R. Thakur, R. Rabenseifner, and W. Gropp. Optimization of collective
    communication operations in mpich. Int. J. High Perform. Comput. Appl., 19(1):49–66,
    Feb. 2005.'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[207] R. Thakur, R. Rabenseifner 和 W. Gropp. 在 mpich 中集体通信操作的优化。国际高性能计算应用期刊，19(1):49–66，2005年2月。'
- en: '[208] Y. Tsuzuku, H. Imachi, and T. Akiba. Variance-based gradient compression
    for efficient distributed deep learning. In ICLR, 2018.'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[208] Y. Tsuzuku, H. Imachi 和 T. Akiba. 基于方差的梯度压缩用于高效分布式深度学习。在 ICLR，2018年。'
- en: '[209] Y. Ueno and R. Yokota. Exhaustive study of hierarchical allreduce patterns
    for large messages between gpus. In Proceedings of the 18th IEEE/ACM International
    Symposium on Cluster, Cloud and Grid Computing, 2019.'
  id: totrans-621
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[209] Y. Ueno 和 R. Yokota. 大消息之间 GPU 的分层 allreduce 模式的详尽研究。在第18届 IEEE/ACM 国际集群、云计算与网格计算研讨会上，2019年。'
- en: '[210] C. A. Uribe, S. Lee, A. Gasnikov, and A. Nedic. A dual approach for optimal
    algorithms in distributed optimization over networks. CoRR, abs/1809.00710, 2018.'
  id: totrans-622
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[210] C. A. Uribe, S. Lee, A. Gasnikov 和 A. Nedic. 一种针对网络分布式优化的双重方法的最优算法。CoRR，abs/1809.00710，2018年。'
- en: '[211] L. G. Valiant. A bridging model for parallel computation. Commun. ACM,
    33(8):103–111, Aug. 1990.'
  id: totrans-623
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[211] L. G. Valiant. 并行计算的桥接模型。ACM 通讯，33(8):103–111，1990年8月。'
- en: '[212] V. Vanhoucke, A. Senior, and M. Z. Mao. Improving the speed of neural
    networks on cpus. In Deep Learning and Unsupervised Feature Learning Workshop,
    NeurIPS 2011, 2011.'
  id: totrans-624
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[212] V. Vanhoucke, A. Senior 和 M. Z. Mao. 提高神经网络在 CPU 上的速度。在深度学习和无监督特征学习研讨会，NeurIPS
    2011，2011年。'
- en: '[213] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser, and I. Polosukhin. Attention is all you need. In NeurIPS, 2017.'
  id: totrans-625
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[213] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,
    L. Kaiser 和 I. Polosukhin. 注意力即是你所需。NeurIPS，2017年。'
- en: '[214] G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur, and
    I. Stoica. Blink: Fast and generic collectives for distributed ML. In MLSys, 2020.'
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[214] G. Wang, S. Venkataraman, A. Phanishayee, J. Thelin, N. Devanur 和 I.
    Stoica. Blink: 快速且通用的分布式 ML 集体操作。在 MLSys，2020年。'
- en: '[215] H. Wang, S. Guo, Z. Qu, R. Li, and Z. Liu. Error-compensated sparsification
    for communication-efficient decentralized training in edge environment. IEEE TPDS,
    2022.'
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[215] H. Wang, S. Guo, Z. Qu, R. Li 和 Z. Liu. 针对边缘环境的通信高效去中心化训练的误差补偿稀疏化。IEEE
    TPDS，2022年。'
- en: '[216] H. Wang, S. Sievert, Z. Charles, S. Liu, S. Wright, and D. Papailiopoulos.
    Atomo: Communication-efficient learning via atomic sparsification. In NeurIPS,
    2018.'
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[216] H. Wang, S. Sievert, Z. Charles, S. Liu, S. Wright 和 D. Papailiopoulos.
    Atomo: 通过原子稀疏化实现通信高效学习。NeurIPS，2018年。'
- en: '[217] J. Wang and G. Joshi. Adaptive communication strategies to achieve the
    best error-runtime trade-off in local-update SGD. Proc. of Workshop on Systems
    for ML and Open Source Software, collocated with NeurIPS 2018, 2018.'
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[217] J. Wang 和 G. Joshi. 自适应通信策略以实现局部更新 SGD 的最佳误差-运行时权衡。在 ML 和开源软件系统研讨会的论文集中，NeurIPS
    2018，与会，2018年。'
- en: '[218] J. Wang and G. Joshi. Cooperative SGD: A unified framework for the design
    and analysis of communication-efficient SGD algorithms. CoRR, abs/1808.07576,
    2018.'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[218] J. Wang 和 G. Joshi. 协作 SGD: 设计和分析通信高效 SGD 算法的统一框架。CoRR，abs/1808.07576，2018年。'
- en: '[219] L. Wang, W. Wang, and B. Li. CMFL: mitigating communication overhead
    for federated learning. In ICDCS, 2019.'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[219] L. Wang, W. Wang 和 B. Li. CMFL: 缓解联邦学习的通信开销。在 ICDCS，2019年。'
- en: '[220] Q. Wang, S. Shi, C. Wang, and X. Chu. Communication contention aware
    scheduling of multiple deep learning training jobs. arXiv preprint arXiv:2002.10105,
    2020.'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[220] Q. Wang, S. Shi, C. Wang 和 X. Chu. 多个深度学习训练任务的通信争用感知调度。arXiv 预印本 arXiv:2002.10105，2020年。'
- en: '[221] S. Wang, D. Li, Y. Cheng, J. Geng, Y. Wang, S. Wang, S.-T. Xia, and J. Wu.
    BML: A high-performance, low-cost gradient synchronization algorithm for DML training.
    In NeurIPS, 2018.'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[221] S. Wang, D. Li, Y. Cheng, J. Geng, Y. Wang, S. Wang, S.-T. Xia 和 J. Wu.
    BML: 一种高性能、低成本的梯度同步算法用于 DML 训练。NeurIPS，2018年。'
- en: '[222] S. Wang, D. Li, J. Geng, Y. Gu, and Y. Cheng. Impact of network topology
    on the performance of dml: Theoretical analysis and practical factors. In IEEE
    INFOCOM, 2019.'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[222] S. Wang, D. Li, J. Geng, Y. Gu, 和 Y. Cheng. 网络拓扑对 DML 性能的影响：理论分析和实际因素。在
    IEEE INFOCOM, 2019。'
- en: '[223] Y. Wang, L. Lin, and J. Chen. Communication-compressed adaptive gradient
    method for distributed nonconvex optimization. In AISTATS, 2022.'
  id: totrans-635
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[223] Y. Wang, L. Lin, 和 J. Chen. 用于分布式非凸优化的通信压缩自适应梯度方法。在 AISTATS, 2022。'
- en: '[224] Y. Wang, Q. Wang, S. Shi, X. He, Z. Tang, K. Zhao, and X. Chu. Benchmarking
    the performance and power of AI accelerators for AI training. arXiv preprint arXiv:1909.06842,
    2019.'
  id: totrans-636
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[224] Y. Wang, Q. Wang, S. Shi, X. He, Z. Tang, K. Zhao, 和 X. Chu. AI 训练 AI
    加速器的性能和功耗基准测试。arXiv 预印本 arXiv:1909.06842, 2019。'
- en: '[225] J. Wangni, J. Wang, J. Liu, and T. Zhang. Gradient sparsification for
    communication-efficient distributed optimization. In NeurIPS, 2018.'
  id: totrans-637
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[225] J. Wangni, J. Wang, J. Liu, 和 T. Zhang. 通信高效的分布式优化中的梯度稀疏化。在 NeurIPS,
    2018。'
- en: '[226] E. Wei and A. E. Ozdaglar. On the o(1=k) convergence of asynchronous
    distributed alternating direction method of multipliers. In 2013 IEEE Global Conference
    on Signal and Information Processing, pages 551–554, Dec 2013.'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[226] E. Wei 和 A. E. Ozdaglar. 关于异步分布式交替方向乘子法的 O(1=k) 收敛性。在 2013 IEEE 全球信号与信息处理大会,
    页面 551–554, 2013年12月。'
- en: '[227] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad:
    Ternary gradients to reduce communication in distributed deep learning. In NeurIPS,
    2017.'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[227] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, 和 H. Li. Terngrad: 三元梯度以减少分布式深度学习中的通信。在
    NeurIPS, 2017。'
- en: '[228] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, and H. Li. Terngrad:
    Ternary gradients to reduce communication in distributed deep learning. In NeurIPS,
    2017.'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[228] W. Wen, C. Xu, F. Yan, C. Wu, Y. Wang, Y. Chen, 和 H. Li. Terngrad: 三元梯度以减少分布式深度学习中的通信。在
    NeurIPS, 2017。'
- en: '[229] J. Wu, W. Huang, J. Huang, and T. Zhang. Error compensated quantized
    SGD and its applications to large-scale distributed optimization. In ICML, 2018.'
  id: totrans-641
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[229] J. Wu, W. Huang, J. Huang, 和 T. Zhang. 错误补偿量化 SGD 及其在大规模分布式优化中的应用。在 ICML,
    2018。'
- en: '[230] E. P. Xing, Q. Ho, W. Dai, J. K. Kim, J. Wei, S. Lee, X. Zheng, P. Xie,
    A. Kumar, and Y. Yu. Petuum: A new platform for distributed machine learning on
    big data. IEEE Transactions on Big Data, 1(2):49–67, June 2015.'
  id: totrans-642
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[230] E. P. Xing, Q. Ho, W. Dai, J. K. Kim, J. Wei, S. Lee, X. Zheng, P. Xie,
    A. Kumar, 和 Y. Yu. Petuum: 一个用于大数据的分布式机器学习新平台。IEEE Transactions on Big Data, 1(2):49–67,
    2015年6月。'
- en: '[231] E. P. Xing, Q. Ho, P. Xie, and D. Wei. Strategies and principles of distributed
    machine learning on big data. Engineering, 2(2):179 – 195, 2016.'
  id: totrans-643
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[231] E. P. Xing, Q. Ho, P. Xie, 和 D. Wei. 大数据上分布式机器学习的策略和原则。Engineering, 2(2):179
    – 195, 2016。'
- en: '[232] A. Xu and H. Huang. Detached error feedback for distributed SGD with
    random sparsification. In ICML, 2022.'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[232] A. Xu 和 H. Huang. 带有随机稀疏化的分布式 SGD 的分离误差反馈。在 ICML, 2022。'
- en: '[233] P. Xu, S. Shi, and X. Chu. Performance evaluation of deep learning tools
    in docker containers. In 2017 3rd International Conference on Big Data Computing
    and Communications (BIGCOM), pages 395–403\. IEEE, 2017.'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[233] P. Xu, S. Shi, 和 X. Chu. Docker 容器中深度学习工具的性能评估。在 2017 第三届大数据计算与通信国际会议
    (BIGCOM), 页面 395–403. IEEE, 2017。'
- en: '[234] Y. Xu and W. Yin. A block coordinate descent method for regularized multiconvex
    optimization with applications to nonnegative tensor factorization and completion.
    SIAM J. Imaging Sciences, 6:1758–1789, 2013.'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[234] Y. Xu 和 W. Yin. 用于正则化多凸优化的块坐标下降方法及其在非负张量分解和补全中的应用。SIAM J. Imaging Sciences,
    6:1758–1789, 2013。'
- en: '[235] D. Yan, W. Wang, and X. Chu. Optimizing batched winograd convolution
    on GPUs. In Proceedings of the 25th ACM SIGPLAN Symposium on Principles and Practice
    of Parallel Programming, pages 32–44, 2020.'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[235] D. Yan, W. Wang, 和 X. Chu. 在 GPU 上优化批量 Winograd 卷积。在第 25 届 ACM SIGPLAN
    原则与实践并行编程研讨会论文集, 页面 32–44, 2020。'
- en: '[236] G. Yan, T. Li, S.-L. Huang, T. Lan, and L. Song. Ac-sgd: Adaptively compressed
    sgd for communication-efficient distributed learning. IEEE JSAC, 2022.'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[236] G. Yan, T. Li, S.-L. Huang, T. Lan, 和 L. Song. Ac-sgd: 自适应压缩 SGD 用于通信高效的分布式学习。IEEE
    JSAC, 2022。'
- en: '[237] S. Yang, Y. Wang, and X. Chu. A survey of deep learning techniques for
    neural machine translation. arXiv preprint arXiv:2002.07526, 2020.'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[237] S. Yang, Y. Wang, 和 X. Chu. 深度学习技术在神经机器翻译中的调查。arXiv 预印本 arXiv:2002.07526,
    2020。'
- en: '[238] Y. You, A. Buluç, and J. Demmel. Scaling deep learning on gpu and knights
    landing clusters. In Proceedings of the International Conference for High Performance
    Computing, Networking, Storage and Analysis, pages 1–12, 2017.'
  id: totrans-650
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[238] Y. You, A. Buluç, 和 J. Demmel. 在 GPU 和 Knights Landing 集群上扩展深度学习. 载于国际高性能计算、网络、存储与分析会议论文集,
    第1–12页, 2017.'
- en: '[239] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
    J. Demmel, K. Keutzer, and C.-J. Hsieh. Large batch optimization for deep learning:
    Training BERT in 76 minutes. In ICLR, 2020.'
  id: totrans-651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[239] Y. You, J. Li, S. Reddi, J. Hseu, S. Kumar, S. Bhojanapalli, X. Song,
    J. Demmel, K. Keutzer, 和 C.-J. Hsieh. 深度学习的大批量优化: 在76分钟内训练 BERT. 载于 ICLR, 2020.'
- en: '[240] Y. You, Z. Zhang, C.-J. Hsieh, J. Demmel, and K. Keutzer. Imagenet training
    in minutes. In Proceedings of the 47th International Conference on Parallel Processing,
    page 1\. ACM, 2018.'
  id: totrans-652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[240] Y. You, Z. Zhang, C.-J. Hsieh, J. Demmel, 和 K. Keutzer. ImageNet 训练仅需几分钟.
    载于第47届国际并行处理大会论文集, 第1页. ACM, 2018.'
- en: '[241] H. Yu and R. Jin. On the computation and communication complexity of
    parallel SGD with dynamic batch sizes for stochastic non-convex optimization.
    In ICML, 2019.'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[241] H. Yu 和 R. Jin. 关于具有动态批量大小的并行 SGD 的计算和通信复杂度. 载于 ICML, 2019.'
- en: '[242] H. Yu, R. Jin, and S. Yang. On the linear speedup analysis of communication
    efficient momentum SGD for distributed non-convex optimization. In ICML, 2019.'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[242] H. Yu, R. Jin, 和 S. Yang. 关于用于分布式非凸优化的通信高效动量 SGD 的线性加速分析. 载于 ICML, 2019.'
- en: '[243] H. Yu, S. X. Yang, and S. Zhu. Parallel restarted sgd with faster convergence
    and less communication: Demystifying why model averaging works for deep learning.
    In AAAI, 2018.'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[243] H. Yu, S. X. Yang, 和 S. Zhu. 并行重启 SGD 具有更快的收敛性和更少的通信: 揭示为何模型平均对深度学习有效.
    载于 AAAI, 2018.'
- en: '[244] Y. Yu, J. Wu, and L. Huang. Double quantization for communication-efficient
    distributed optimization. In NeurIPS. 2019.'
  id: totrans-656
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[244] Y. Yu, J. Wu, 和 L. Huang. 通信高效分布式优化的双重量化. 载于 NeurIPS, 2019.'
- en: '[245] J. Zeng, T. T.-K. Lau, S. Lin, and Y. Yao. Global convergence of block
    coordinate descent in deep learning. In ICML, 2019.'
  id: totrans-657
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[245] J. Zeng, T. T.-K. Lau, S. Lin, 和 Y. Yao. 深度学习中块坐标下降的全局收敛性. 载于 ICML, 2019.'
- en: '[246] C. Zhang and C. Ré. Dimmwitted: A study of main-memory statistical analytics.
    Proc. VLDB Endow., 7(12):1283–1294, Aug. 2014.'
  id: totrans-658
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[246] C. Zhang 和 C. Ré. Dimmwitted: 主存统计分析研究. Proc. VLDB Endow., 7(12):1283–1294,
    2014年8月.'
- en: '[247] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, and C. Zhang. ZipML: Training
    linear models with end-to-end low precision, and a little bit of deep learning.
    In ICML, 2017.'
  id: totrans-659
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[247] H. Zhang, J. Li, K. Kara, D. Alistarh, J. Liu, 和 C. Zhang. ZipML：使用端到端低精度训练线性模型，以及一点点深度学习.
    载于 ICML, 2017.'
- en: '[248] H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P. Xie,
    and E. P. Xing. Poseidon: An efficient communication architecture for distributed
    deep learning on GPU clusters. In USENIX ATC, 2017.'
  id: totrans-660
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[248] H. Zhang, Z. Zheng, S. Xu, W. Dai, Q. Ho, X. Liang, Z. Hu, J. Wei, P.
    Xie, 和 E. P. Xing. Poseidon: 一种高效的分布式深度学习通信架构用于 GPU 集群. 载于 USENIX ATC, 2017.'
- en: '[249] J. Zhang, C. D. Sa, I. Mitliagkas, and C. Ré. Parallel sgd: When does
    averaging help? ArXiv, abs/1606.07365, 2016.'
  id: totrans-661
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[249] J. Zhang, C. D. Sa, I. Mitliagkas, 和 C. Ré. 并行 SGD: 平均化何时有帮助? ArXiv,
    abs/1606.07365, 2016.'
- en: '[250] L. Zhang, S. Shi, X. Chu, W. Wang, B. Li, and C. Liu. Accelerating distributed
    deep learning with fine-grained all-reduce pipelining. In IEEE ICDCS, 2023.'
  id: totrans-662
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[250] L. Zhang, S. Shi, X. Chu, W. Wang, B. Li, 和 C. Liu. 通过细粒度全归约管道加速分布式深度学习.
    载于 IEEE ICDCS, 2023.'
- en: '[251] L. Zhang, S. Shi, and B. Li. Eva: Practical second-order optimization
    with kronecker-vectorized approximation. In The Eleventh International Conference
    on Learning Representations, 2023.'
  id: totrans-663
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[251] L. Zhang, S. Shi, 和 B. Li. Eva: 实用的二阶优化与 Kronecker-向量化近似. 载于第十一届国际学习表征会议,
    2023.'
- en: '[252] L. Zhang, L. Zhang, S. Shi, X. Chu, and B. Li. Evaluation and optimization
    of gradient compression for distributed deep learning. In IEEE ICDCS, 2023.'
  id: totrans-664
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[252] L. Zhang, L. Zhang, S. Shi, X. Chu, 和 B. Li. 分布式深度学习中梯度压缩的评估与优化. 载于 IEEE
    ICDCS, 2023.'
- en: '[253] R. Zhang and J. T. Kwok. Asynchronous distributed admm for consensus
    optimization. In ICML, 2014.'
  id: totrans-665
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[253] R. Zhang 和 J. T. Kwok. 异步分布式 ADMM 用于共识优化. 载于 ICML, 2014.'
- en: '[254] S. Zhang, A. Choromanska, and Y. LeCun. Deep learning with elastic averaging
    sgd. In NeurIPS, 2015.'
  id: totrans-666
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[254] S. Zhang, A. Choromanska, 和 Y. LeCun. 使用弹性平均 SGD 的深度学习. 载于 NeurIPS, 2015.'
- en: '[255] X. Zhang, J. Liu, and Z. Zhu. Taming convergence for asynchronous stochastic
    gradient descent with unbounded delay in non-convex learning. arXiv preprint arXiv:1805.09470,
    2018.'
  id: totrans-667
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[255] X. Zhang, J. Liu, 和 Z. Zhu. 驯化非凸学习中异步随机梯度下降的无限延迟收敛性. arXiv 预印本 arXiv:1805.09470,
    2018.'
- en: '[256] X. Zhang, J. Trmal, D. Povey, and S. Khudanpur. Improving deep neural
    network acoustic models using generalized maxout networks. In ICASSP, 2014.'
  id: totrans-668
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[256] X. 张，J. Trmal，D. Povey 和 S. Khudanpur。利用广义 Maxout 网络改进深度神经网络声学模型。发表于
    ICASSP，2014 年。'
- en: '[257] Z. Zhang and M. Brand. Convergent block coordinate descent for training
    tikhonov regularized deep neural networks. In NeurIPS, 2017.'
  id: totrans-669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[257] Z. 张和 M. Brand. 收敛块坐标下降用于训练 Tikhonov 正则化深度神经网络。发表于 NeurIPS，2017 年。'
- en: '[258] Z. Zhang and C. Wang. Mipd: An adaptive gradient sparsification framework
    for distributed dnns training. IEEE TPDS, pages 3053–3066, 2022.'
  id: totrans-670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[258] Z. 张和 C. 王。Mipd：一种用于分布式 DNNs 训练的自适应梯度稀疏化框架。IEEE TPDS，第 3053–3066 页，2022
    年。'
- en: '[259] Z. Zhang, L. Yin, Y. Peng, and D. Li. A quick survey on large scale distributed
    deep learning systems. In ICPADS, 2018.'
  id: totrans-671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[259] Z. 张，L. 尹，Y. 彭和 D. 李。大规模分布式深度学习系统的快速调研。发表于 ICPADS，2018 年。'
- en: '[260] S.-Y. Zhao, Y. Xie, H. Gao, and W.-J. Li. Global momentum compression
    for sparse communication in distributed sgd. ArXiv, abs/1905.12948, 2019.'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[260] S.-Y. 赵，Y. 谢，H. 高和 W.-J. 李。分布式 SGD 中的全局动量压缩。ArXiv，abs/1905.12948，2019
    年。'
- en: '[261] W. Zhao, D. Xie, R. Jia, Y. Qian, R. Ding, M. Sun, and P. Li. Distributed
    hierarchical gpu parameter server for massive scale deep learning ads systems.
    MLSys, 2020.'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[261] W. 赵，D. 谢，R. 贾，Y. 钱，R. 丁，M. 孙和 P. 李。大规模深度学习广告系统的分布式层次 GPU 参数服务器。MLSys，2020
    年。'
- en: '[262] S. Zhou, Y. Wu, Z. Ni, X. Zhou, H. Wen, and Y. Zou. Dorefa-net: Training
    low bitwidth convolutional neural networks with low bitwidth gradients. arXiv
    preprint arXiv:1606.06160, 2016.'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[262] S. 周，Y. 吴，Z. 倪，X. 周，H. 温和 Y. 邹。Dorefa-net：用低位宽梯度训练低位宽卷积神经网络。arXiv 预印本
    arXiv:1606.06160，2016 年。'
- en: '[263] M. A. Zinkevich, M. Weimer, A. Smola, and L. Li. Parallelized stochastic
    gradient descent. In NeurIPS, 2010.'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[263] M. A. Zinkevich，M. Weimer，A. Smola 和 L. 李。并行随机梯度下降。发表于 NeurIPS，2010 年。'
- en: '[264] Y. Zou, X. Jin, Y. Li, Z. Guo, E. Wang, and B. Xiao. Mariana: Tencent
    deep learning platform and its applications. PVLDB, 7:1772–1777, 2014.'
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[264] Y. 邹，X. 金，Y. 李，Z. 郭，E. 王和 B. 萧。Mariana：腾讯深度学习平台及其应用。PVLDB，7:1772–1777，2014
    年。'
